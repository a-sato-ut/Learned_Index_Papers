{
  "paperId": "76587ffeb352a80c9a2d07febf718ccd99909715",
  "title": "Generalizing Functional Error Correction for Language and Vision-Language Models",
  "abstract": "The goal of functional error correction is to preserve neural network performance when stored network weights are corrupted by noise. To achieve this goal, a selective protection (SP) scheme was proposed to optimally protect the functionally important bits in binary weight representations in a layer-dependent manner. Although it showed its effectiveness in image classification tasks on some relatively simple networks such as ResNet-18 and VGG-16, it becomes inadequate for emerging complex machine learning tasks generated from natural language processing and vision-language association domains. To solve this problem, we extend the SP scheme in three directions: task complexity, model complexity, and storage complexity. Extensions to complex natural language and vision-language tasks include text categorization and “zero-shot” textual classification of images. Extensions to more complex models with deeper block structures and attention mechanisms consist of Very Deep Convolutional Neural Network (VDCNN) and Contrastive Language-Image Pre-Training (CLIP) networks. Extensions to more complex storage configurations focus on distributed storage architectures to support model parallelism. Experimental results show that the optimized SP scheme preserves network performance in all of these settings. The results also provide insights into redundancy-performance tradeoffs, generalizability of SP across datasets and tasks, and robustness of partitioned network architectures.",
  "tldr_ja": "機能的エラー修正のための選択的保護（SP）スキームを拡張し、自然言語処理や視覚と言語の関連タスクにおける複雑なモデルやストレージ構成に対応させることで、ネットワーク性能を維持する方法を提案。実験結果は、さまざまな設定での性能保持を示し、冗長性と性能のトレードオフ、SPの一般化、分割ネットワークアーキテクチャの堅牢性に関する洞察を提供。"
}