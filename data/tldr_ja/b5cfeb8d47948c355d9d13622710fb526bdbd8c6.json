{
  "paperId": "b5cfeb8d47948c355d9d13622710fb526bdbd8c6",
  "title": "Compressive Meta-Learning",
  "abstract": "The rapid expansion in the size of new datasets has created a need for fast and efficient parameter-learning techniques. Compressive learning is a framework that enables efficient processing by using random, nonlinear features to project large-scale databases onto compact, information-preserving representations whose dimensionality is independent of the number of samples and can be easily stored, transferred, and processed. These database-level summaries are then used to decode parameters of interest from the underlying data distribution without requiring access to the original samples, offering an efficient and privacy-friendly learning framework. However, both the encoding and decoding techniques are typically randomized and data-independent, failing to exploit the underlying structure of the data. In this work, we propose a framework that meta-learns both the encoding and decoding stages of compressive learning methods by using neural networks that provide faster and more accurate systems than the current state-of-the-art approaches. To demonstrate the potential of the presented Compressive Meta-Learning framework, we explore multiple applications--including neural network-based compressive PCA, compressive ridge regression, compressive k-means, and autoencoders.",
  "tldr_ja": "新しいデータセットの急増に対応するため、効率的なパラメータ学習手法として、圧縮メタ学習フレームワークを提案。ランダムな特徴を用いて大規模データをコンパクトな表現に変換し、元のサンプルにアクセスせずにパラメータを推定する。ニューラルネットワークを活用し、従来の手法よりも高速かつ正確なエンコーディングとデコーディングを実現。複数の応用例も示す。"
}