{
  "paperId": "81f5c258beca404a70880f89d1f803f4c8063e73",
  "title": "DB-GPT: Large Language Model Meets Database",
  "abstract": "Large language models (LLMs) have shown superior performance in various areas. And LLMs have the potential to revolutionize data management by serving as the \"brain\" of next-generation database systems. However, there are several challenges that utilize LLMs to optimize databases. First, it is challenging to provide appropriate prompts (e.g., instructions and demonstration examples) to enable LLMs to understand the database optimization problems. Second, LLMs only capture the logical database characters (e.g., SQL semantics) but are not aware of physical characters (e.g., data distributions), and it requires to fine-tune LLMs to capture both physical and logical information. Third, LLMs are not well trained for databases with strict constraints (e.g., query plan equivalence) and privacy-preserving requirements, and it is challenging to train database-specific LLMs while ensuring database privacy. To overcome these challenges, this vision paper proposes a LLM-based database framework (), including automatic prompt generation, DB-specific model fine-tuning, and DB-specific model design and pre-training. Preliminary experiments show that achieves relatively good performance in database tasks like query rewrite and index tuning. The source code and datasets are available at github.com/TsinghuaDatabaseGroup/DB-GPT.",
  "tldr_ja": "DB-GPTは、大規模言語モデルを活用してデータベース管理を革新するフレームワークを提案し、プロンプト生成やモデルの微調整を通じてデータベース特有の課題に対応する。初期実験では、クエリの書き換えやインデックス調整などのタスクで良好な性能を示した。"
}