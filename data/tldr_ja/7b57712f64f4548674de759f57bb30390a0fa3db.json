{
  "paperId": "7b57712f64f4548674de759f57bb30390a0fa3db",
  "title": "Green, Yellow, Yield: End-Host Traffic Scheduling for Distributed Deep Learning with TensorLights",
  "abstract": "The recent success of Deep Learning (DL) in a board range of AI services has led to a surging amount of DL workloads in production clusters. To support DL jobs at scale, the parameter server (PS) architecture is the most popular approach for distributing the computation in a compute cluster. Concurrent DL jobs consisting of PS tasks and worker tasks are typically launched on available compute nodes by a cluster resource manager to ensure high cluster resource utilization. As a PS needs to distribute model updates to every remote worker, its communication has very large fan-out. We observe that network contention among colocated PSes would cause stragglers among workers, resulting in application performance degradation and resource under-utilization. To mitigate the straggler effect, we propose TensorLights, which introduces traffic prioritization at host NICs to manage traffic contention among PSes. We evaluate TensorLights experimentally and show that it effectively mitigates stragglers, improves the average completion time of DL applications by up to 31%, and increases resource utilization. TensorLights is highly practical as it provides benefits without needing changes to the DL software stack.",
  "tldr_ja": "TensorLightsは、分散深層学習におけるパラメータサーバー間の通信混雑を緩和し、平均完了時間を最大31%改善し、リソース利用率を向上させるトラフィック優先制御を提案する。"
}