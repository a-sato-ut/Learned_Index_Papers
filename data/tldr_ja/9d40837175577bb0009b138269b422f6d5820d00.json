{
  "paperId": "9d40837175577bb0009b138269b422f6d5820d00",
  "title": "Transformer Memory as a Differentiable Search Index",
  "abstract": "In this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.",
  "tldr_ja": "本論文では、情報検索を単一のTransformerで実現する方法を提案し、Differentiable Search Index（DSI）という新たな枠組みを導入します。DSIは、テキストクエリを関連する文書IDに直接マッピングするモデルで、パラメータのみを用いてクエリに応答し、検索プロセスを大幅に簡素化します。実験により、DSIは強力なベースラインを上回る性能を示し、ゼロショット設定でも優れた一般化能力を発揮することが確認されました。"
}