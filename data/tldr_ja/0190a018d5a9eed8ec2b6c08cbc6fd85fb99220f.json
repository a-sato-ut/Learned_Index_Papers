{
  "paperId": "0190a018d5a9eed8ec2b6c08cbc6fd85fb99220f",
  "title": "PQ Bench: Benchmarking Pruning and Quantization Techniques",
  "abstract": "Deep learning models are increasingly deployed in performance-critical applications, where computational efficiency must be balanced with accuracy. Model compression techniques such as pruning and quantization help address this challenge. However, they are often evaluated solely on accuracy, overlooking their impact on model size and inference time for a given hardware setup. To support practitioners in selecting and evaluating different model compression techniques, we introduce our benchmarking framework, PQ Bench. It pre-implements a set of popular compression techniques and automates the process of benchmarking their effects on accuracy, inference speed, and memory footprint. In our evaluation, we demonstrate the use of PQ Bench and provide key insights into the trade-offs across various models, compression strategies, and configurations.",
  "tldr_ja": "PQ Benchは、ディープラーニングモデルの圧縮手法（プルーニングと量子化）の評価フレームワークであり、精度だけでなくモデルサイズや推論時間も考慮します。人気の圧縮技術を事前実装し、効果を自動的にベンチマークすることで、実践者が異なる手法を選択・評価する際の支援を提供します。"
}