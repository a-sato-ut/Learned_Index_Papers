{
    "paperId": "ca20d8b593ce7221d1086b269745f8d5854e4262",
    "title": "Bifrost: End-to-End Evaluation and optimization of Reconfigurable DNN Accelerators",
    "year": 2022,
    "venue": "IEEE International Symposium on Performance Analysis of Systems and Software",
    "authors": [
        "Axel Stjerngren",
        "Perry Gibson",
        "José Cano"
    ],
    "doi": "10.48550/arXiv.2204.12418",
    "arxivId": "2204.12418",
    "url": "https://www.semanticscholar.org/paper/ca20d8b593ce7221d1086b269745f8d5854e4262",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2204.12418",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Reconfigurable accelerators for deep neural networks (DNNs) promise to improve performance such as inference latency. STONNE is the first cycle-accurate simulator for reconfigurable DNN inference accelerators which allows for the exploration of accelerator designs and configuration space. However, preparing models for evaluation and exploring configuration space in STONNE is a manual developer-time-consuming process, which is a barrier for research. This paper introduces Bifrost, an end-to-end framework for the evaluation and optimization of reconfigurable DNN inference accelerators. Bifrost operates as a frontend for STONNE and leverages the TVM deep learning compiler stack to parse models and automate offloading of accelerated computations. We discuss Bifrost’s advantages over STONNE and other tools, and evaluate the MAERI and SIGMA architectures using Bifrost. Additionally, Bifrost introduces a module leveraging AutoTVM to efficiently explore accelerator designs and datatlow mapping space to optimize performance. This is demonstrated by tuning the MAERI architecture and generating efficient datatlow mappings for AlexNet, obtaining an average speedup of $50\\times$ for the convolutional layers and $11\\times$ for the fully connected layers. Our code is available at www.github.com/gicLAB/bifrost.",
    "citationCount": 4,
    "referenceCount": 40
}