{
    "paperId": "95cb5128f2cb9fb7fb94f2ce62cf1fb62361cc77",
    "title": "Optimizing Memory Placement using Evolutionary Graph Reinforcement Learning",
    "year": 2020,
    "venue": "International Conference on Learning Representations",
    "authors": [
        "Shauharda Khadka",
        "Estelle Aflalo",
        "Mattias Marder",
        "Avrech Ben-David",
        "Santiago Miret",
        "Hanlin Tang",
        "Shie Mannor",
        "Tamir Hazan",
        "Somdeb Majumdar"
    ],
    "doi": null,
    "arxivId": "2007.07298",
    "url": "https://www.semanticscholar.org/paper/95cb5128f2cb9fb7fb94f2ce62cf1fb62361cc77",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "As modern neural networks have grown to billions of parameters, meeting tight latency budgets has become increasingly challenging. Approaches like compression, sparsification and network pruning have proven effective to tackle this problem - but they rely on modifications of the underlying network. In this paper, we look at a complimentary approach of optimizing how tensors are mapped to on-chip memory in an inference accelerator while leaving the network parameters untouched. Since different memory components trade off capacity for bandwidth differently, a sub-optimal mapping can result in high latency. We introduce evolutionary graph reinforcement learning (EGRL) - a method combining graph neural networks, reinforcement learning (RL) and evolutionary search - that aims to find the optimal mapping to minimize latency. Furthermore, a set of fast, stateless policies guide the evolutionary search to improve sample-efficiency. We train and validate our approach directly on the Intel NNP-I chip for inference using a batch size of 1. EGRL outperforms policy-gradient, evolutionary search and dynamic programming baselines on BERT, ResNet-101 and ResNet-50. We achieve 28-78% speed-up compared to the native NNP-I compiler on all three workloads.",
    "citationCount": 13,
    "referenceCount": 61
}