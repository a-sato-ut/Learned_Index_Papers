{
    "paperId": "a870e1a55b3307c915e1d246f207678de1a19490",
    "title": "Investigating the Efficacy of Large Language Models for Code Clone Detection",
    "year": 2024,
    "venue": "IEEE International Conference on Program Comprehension",
    "authors": [
        "Mohamad Khajezade",
        "J. Wu",
        "F. H. Fard",
        "Gema Rodríguez-Pérez",
        "M. S. Shehata"
    ],
    "doi": "10.1145/3643916.3645030",
    "arxivId": "2401.13802",
    "url": "https://www.semanticscholar.org/paper/a870e1a55b3307c915e1d246f207678de1a19490",
    "isOpenAccess": true,
    "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3643916.3645030",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are ‘generative’ tasks. However, there is limited research on the usage of LLMs for ‘non-generative’ tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect Type-4 code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We then conducted an analysis to understand the strengths and weaknesses of ChatGPT in CCD. ChatGPT surpasses the baselines in cross-language CCD attaining an F1-score of 0.877 and achieves comparable performance to fully fine-tuned models for mono-lingual CCD, with an F1-score of 0.878. Also, the prompt and the difficulty level of the problems has an impact on the performance of ChatGPT. Finally, we provide insights and future directions based on our initial analysis 1.1Our code and data is open-sourced at https://github.com/mkhfring/llm-for-ccd",
    "citationCount": 25,
    "referenceCount": 27
}