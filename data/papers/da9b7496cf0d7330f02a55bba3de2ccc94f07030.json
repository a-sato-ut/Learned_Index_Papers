{
    "paperId": "da9b7496cf0d7330f02a55bba3de2ccc94f07030",
    "title": "Input-Relational Verification of Deep Neural Networks",
    "year": 2024,
    "venue": "Proc. ACM Program. Lang.",
    "authors": [
        "Debangshu Banerjee",
        "Changming Xu",
        "Gagandeep Singh"
    ],
    "doi": "10.1145/3656377",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/da9b7496cf0d7330f02a55bba3de2ccc94f07030",
    "isOpenAccess": true,
    "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3656377",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We consider the verification of input-relational properties defined over deep neural networks (DNNs) such as robustness against universal adversarial perturbations, monotonicity, etc. Precise verification of these properties requires reasoning about multiple executions of the same DNN. We introduce a novel concept of difference tracking to compute the difference between the outputs of two executions of the same DNN at all layers. We design a new abstract domain, DiffPoly for efficient difference tracking that can scale large DNNs. DiffPoly is equipped with custom abstract transformers for common activation functions (ReLU, Tanh, Sigmoid, etc.) and affine layers and can create precise linear cross-execution constraints. We implement an input-relational verifier for DNNs called RaVeN which uses DiffPoly and linear program formulations to handle a wide range of input-relational properties. Our experimental results on challenging benchmarks show that by leveraging precise linear constraints defined over multiple executions of the DNN, RaVeN gains substantial precision over baselines on a wide range of datasets, networks, and input-relational properties.",
    "citationCount": 9,
    "referenceCount": 80
}