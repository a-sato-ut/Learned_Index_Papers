{
    "paperId": "53370d7205c92cd78c9076ab9d2d92c20aa4678b",
    "title": "Large Scale Multi-Task Bayesian Optimization with Large Language Models",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Yimeng Zeng",
        "Natalie Maus",
        "H. Jones",
        "Jeffrey Tao",
        "Fangping Wan",
        "M. Torres",
        "Cesar de la Fuente-Nunez",
        "Ryan Marcus",
        "Osbert Bastani",
        "Jacob R. Gardner"
    ],
    "doi": "10.48550/arXiv.2503.08131",
    "arxivId": "2503.08131",
    "url": "https://www.semanticscholar.org/paper/53370d7205c92cd78c9076ab9d2d92c20aa4678b",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "In multi-task Bayesian optimization, the goal is to leverage experience from optimizing existing tasks to improve the efficiency of optimizing new ones. While approaches using multi-task Gaussian processes or deep kernel transfer exist, the performance improvement is marginal when scaling beyond a moderate number of tasks. We introduce a novel approach leveraging large language models (LLMs) to learn from, and improve upon, previous optimization trajectories, scaling to approximately 1500 distinct tasks. Specifically, we propose a feedback loop in which an LLM is fine-tuned on the high quality solutions to specific tasks found by Bayesian optimization (BO). This LLM is then used to generate initialization points for future BO searches for new tasks. The trajectories of these new searches provide additional training data for fine-tuning the LLM, completing the loop. We evaluate our method on two distinct domains: database query optimization and antimicrobial peptide design. Results demonstrate that our approach creates a positive feedback loop, where the LLM's generated initializations gradually improve, leading to better optimization performance. As this feedback loop continues, we find that the LLM is eventually able to generate solutions to new tasks in just a few shots that are better than the solutions produced by\"from scratch\"by Bayesian optimization while simultaneously requiring significantly fewer oracle calls.",
    "citationCount": 0,
    "referenceCount": 54
}