{
    "paperId": "1e24215fd29c2373b31dcb7a50856aa819126f2a",
    "title": "Strategyproof Scheduling with Predictions",
    "year": 2022,
    "venue": "Information Technology Convergence and Services",
    "authors": [
        "Eric Balkanski",
        "Vasilis Gkatzelis",
        "Xizhi Tan"
    ],
    "doi": "10.48550/arXiv.2209.04058",
    "arxivId": "2209.04058",
    "url": "https://www.semanticscholar.org/paper/1e24215fd29c2373b31dcb7a50856aa819126f2a",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2209.04058",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "In their seminal paper that initiated the field of algorithmic mechanism design, \\citet{NR99} studied the problem of designing strategyproof mechanisms for scheduling jobs on unrelated machines aiming to minimize the makespan. They provided a strategyproof mechanism that achieves an $n$-approximation and they made the bold conjecture that this is the best approximation achievable by any deterministic strategyproof scheduling mechanism. After more than two decades and several efforts, $n$ remains the best known approximation and very recent work by \\citet{CKK21} has been able to prove an $\\Omega(\\sqrt{n})$ approximation lower bound for all deterministic strategyproof mechanisms. This strong negative result, however, heavily depends on the fact that the performance of these mechanisms is evaluated using worst-case analysis. To overcome such overly pessimistic, and often uninformative, worst-case bounds, a surge of recent work has focused on the ``learning-augmented framework'', whose goal is to leverage machine-learned predictions to obtain improved approximations when these predictions are accurate (consistency), while also achieving near-optimal worst-case approximations even when the predictions are arbitrarily wrong (robustness). In this work, we study the classic strategic scheduling problem of~\\citet{NR99} using the learning-augmented framework and give a deterministic polynomial-time strategyproof mechanism that is $6$-consistent and $2n$-robust. We thus achieve the ``best of both worlds'': an $O(1)$ consistency and an $O(n)$ robustness that asymptotically matches the best-known approximation. We then extend this result to provide more general worst-case approximation guarantees as a function of the prediction error. Finally, we complement our positive results by showing that any $1$-consistent deterministic strategyproof mechanism has unbounded robustness.",
    "citationCount": 29,
    "referenceCount": 29
}