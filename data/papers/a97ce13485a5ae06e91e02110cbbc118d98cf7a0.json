{
    "paperId": "a97ce13485a5ae06e91e02110cbbc118d98cf7a0",
    "title": "AlignRec: Aligning and Training in Multimodal Recommendations",
    "year": 2024,
    "venue": "International Conference on Information and Knowledge Management",
    "authors": [
        "Yifan Liu",
        "Kangning Zhang",
        "Xiangyuan Ren",
        "Yanhua Huang",
        "Jiarui Jin",
        "Yingjie Qin",
        "Ruilong Su",
        "Ruiwen Xu",
        "Weinan Zhang"
    ],
    "doi": "10.1145/3627673.3679626",
    "arxivId": "2403.12384",
    "url": "https://www.semanticscholar.org/paper/a97ce13485a5ae06e91e02110cbbc118d98cf7a0",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "With the development of multimedia systems, multimodal recommendations are playing an essential role, as they can leverage rich contexts beyond interactions. Existing methods mainly regard multimodal information as an auxiliary, using them to help learn ID features; However, there exist semantic gaps among multimodal content features and ID-based features, for which directly using multimodal information as an auxiliary would lead to misalignment in representations of users and items. In this paper, we first systematically investigate the misalignment issue in multimodal recommendations, and propose a solution named AlignRec. In AlignRec, the recommendation objective is decomposed into three alignments, namely alignment within contents, alignment between content and categorical ID, and alignment between users and items. Each alignment is characterized by a specific objective function and is integrated into our multimodal recommendation framework. To effectively train AlignRec, we propose starting from pre-training the first alignment to obtain unified multimodal features and subsequently training the following two alignments together with these features as input. As it is essential to analyze whether each multimodal feature helps in training and accelerate the iteration cycle of recommendation models, we design three new classes of metrics to evaluate intermediate performance. Our extensive experiments on three real-world datasets consistently verify the superiority of AlignRec compared to nine baselines. We also find that the multimodal features generated by AlignRec are better than currently used ones, which are to be open-sourced in our repository https://github.com/sjtulyf123/AlignRec_CIKM24.",
    "citationCount": 29,
    "referenceCount": 38
}