{
    "paperId": "bce09e2d11bc94d7acb5e7c759c331df88772dd4",
    "title": "A Low-Cost and Pages-Interrelation-Aware Attention Model for Hybrid Memory Scheduling",
    "year": 2023,
    "venue": "IEEE International Conference on Systems, Man and Cybernetics",
    "authors": [
        "Yanjie Zhen",
        "Yu Chen"
    ],
    "doi": "10.1109/SMC53992.2023.10394498",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/bce09e2d11bc94d7acb5e7c759c331df88772dd4",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Hybrid memory architecture has become an important solution to address the increasing demand for the main memory capacity of big data applications. Due to the varying properties of different components in hybrid memory, accurately predicting the hotness of pages and timely scheduling hot pages to fast memory becomes crucial for optimal performance. However, existing hybrid memory schedulers using non-intelligent policy exhibit low performance. Although schedulers employing neural models can improve performance, they suffer limitations such as long inference time and loss of interrelation between pages. This paper presents PI-Attention, a low-cost and pages-interrelation-aware attention model for hybrid memory scheduling. It addresses the limitations above by utilizing two attention modules in the page and time sequence dimensions. Our experiments show that PI-Attention brings 11.14% performance improvement and a 3.75x reduction in inference time.",
    "citationCount": 0,
    "referenceCount": 28
}