{
    "paperId": "d19156774f1675f07c74e72457483bbd0d9ef258",
    "title": "ALT: Breaking the Wall between Data Layout and Loop Optimizations for Deep Learning Compilation",
    "year": 2023,
    "venue": "European Conference on Computer Systems",
    "authors": [
        "Zhiying Xu",
        "Jiafan Xu",
        "H. Peng",
        "Wei Wang",
        "Xiaoliang Wang",
        "Hao Wan",
        "Haipeng Dai",
        "Yixu Xu",
        "Hao Cheng",
        "Kun Wang",
        "Guihai Chen"
    ],
    "doi": "10.1145/3552326.3587440",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/d19156774f1675f07c74e72457483bbd0d9ef258",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Deep learning models rely on highly optimized tensor libraries for efficient inference on heterogeneous hardware. Current deep compilers typically predetermine layouts of tensors and then optimize loops of operators. However, such unidirectional and one-off workflow strictly separates graph-level optimization and operator-level optimization into different system layers, missing opportunities for unified tuning. This paper proposes ALT, a deep compiler that performs joint graph-level layout optimization and operator-level loop optimization. ALT provides a generic transformation module to manipulate layouts and loops with easy-to-use primitive functions. ALT further integrates an auto-tuning module that jointly optimizes graph-level data layouts and operator-level loops while guaranteeing efficiency. Experimental results show that ALT significantly outperforms state-of-the-art compilers (e.g., Ansor) in terms of both single operator performance (e.g., 1.5× speedup on average) and end-to-end inference performance (e.g., 1.4× speedup on average).",
    "citationCount": 8,
    "referenceCount": 91
}