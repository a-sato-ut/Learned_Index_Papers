{
    "paperId": "825950f0fbe25e419bbdad8fa8f68733e13a5ffd",
    "title": "Block Access Pattern Discovery via Compressed Full Tensor Transformer",
    "year": 2021,
    "venue": "International Conference on Information and Knowledge Management",
    "authors": [
        "Xing Li",
        "Qiquan Shi",
        "G. Hu",
        "Lei Chen",
        "Hui-Ying Mao",
        "Yiyuan Yang",
        "Mingxuan Yuan",
        "Jia Zeng",
        "Zhuo Cheng"
    ],
    "doi": "10.1145/3459637.3482323",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/825950f0fbe25e419bbdad8fa8f68733e13a5ffd",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The discovery and prediction of block access patterns in hybrid storage systems is of crucial importance for effective tier management. Existing methods are usually based on heuristics and unable to handle complex patterns. This work newly introduces transformer to block access pattern prediction. We remark that block accesses in the tier management systems are aggregated temporally and spatially as multivariate time series of block access frequency, so the runtime requirements are relaxed, making complex models applicable for the deployment. Moreover, enormous and rarely accessed blocks in storage systems and the structure of traditional transformer models would result in millions of redundant parameters and make them impractical to be deployed. We incorporate Tensor-Train Decomposition (TTD) with transformer and propose the Compressed Full Tenor Transformer (CFTT), in which all linear layers in the vanilla transformer are replaced with tensor-train layers. Weights of input and output layers are shared to further reduce parameters and reuse knowledge implicitly. CFTT can significantly reduce the model size and computation cost, which is critical to save storage space and inference time. Extensive experiments are conducted on synthetic and real-world datasets. The results demonstrate that transformers achieve state-of-the-art performance stably in terms of top-k hit rates. Moreover, the proposed CFTT compresses transformers 16× to 461× and speeds up inference 5× without sacrificing performance on the whole, which facilitates its applications in tier management in hybrid storage systems.",
    "citationCount": 7,
    "referenceCount": 35
}