{
    "paperId": "58c4a8b7a5bcb0d45e1979ec35835726d30033c1",
    "title": "Learning to optimize by multi-gradient for multi-objective optimization",
    "year": 2023,
    "venue": "arXiv.org",
    "authors": [
        "Linxi Yang",
        "Xinmin Yang",
        "L. Tang"
    ],
    "doi": "10.48550/arXiv.2311.00559",
    "arxivId": "2311.00559",
    "url": "https://www.semanticscholar.org/paper/58c4a8b7a5bcb0d45e1979ec35835726d30033c1",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The development of artificial intelligence (AI) for science has led to the emergence of learning-based research paradigms, necessitating a compelling reevaluation of the design of multi-objective optimization (MOO) methods. The new generation MOO methods should be rooted in automated learning rather than manual design. In this paper, we introduce a new automatic learning paradigm for optimizing MOO problems, and propose a multi-gradient learning to optimize (ML2O) method, which automatically learns a generator (or mappings) from multiple gradients to update directions. As a learning-based method, ML2O acquires knowledge of local landscapes by leveraging information from the current step and incorporates global experience extracted from historical iteration trajectory data. By introducing a new guarding mechanism, we propose a guarded multi-gradient learning to optimize (GML2O) method, and prove that the iterative sequence generated by GML2O converges to a Pareto critical point. The experimental results demonstrate that our learned optimizer outperforms hand-designed competitors on training multi-task learning (MTL) neural network.",
    "citationCount": 1,
    "referenceCount": 51
}