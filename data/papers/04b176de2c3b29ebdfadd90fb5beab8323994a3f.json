{
    "paperId": "04b176de2c3b29ebdfadd90fb5beab8323994a3f",
    "title": "ED2LM: Encoder-Decoder to Language Model for Faster Document Re-ranking Inference",
    "year": 2022,
    "venue": "Findings",
    "authors": [
        "Kai Hui",
        "Honglei Zhuang",
        "Tao Chen",
        "Zhen Qin",
        "Jing Lu",
        "Dara Bahri",
        "Ji Ma",
        "Jai Gupta",
        "C. D. Santos",
        "Yi Tay",
        "Donald Metzler"
    ],
    "doi": "10.48550/arXiv.2204.11458",
    "arxivId": "2204.11458",
    "url": "https://www.semanticscholar.org/paper/04b176de2c3b29ebdfadd90fb5beab8323994a3f",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2204.11458",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "State-of-the-art neural models typically encode document-query pairs using cross-attention for re-ranking. To this end, models generally utilize an encoder-only (like BERT) paradigm or an encoder-decoder (like T5) approach. These paradigms, however, are not without flaws, i.e., running the model on all query-document pairs at inference-time incurs a significant computational cost. This paper proposes a new training and inference paradigm for re-ranking. We propose to finetune a pretrained encoder-decoder model using in the form of document to query generation. Subsequently, we show that this encoder-decoder architecture can be decomposed into a decoder-only language model during inference. This results in significant inference time speedups since the decoder-only architecture only needs to learn to interpret static encoder embeddings during inference. Our experiments show that this new paradigm achieves results that are comparable to the more expensive cross-attention ranking approaches while being up to 6.8X faster. We believe this work paves the way for more efficient neural rankers that leverage large pretrained models.",
    "citationCount": 18,
    "referenceCount": 60
}