{
    "paperId": "05912292a9472c7bf95029128582bc4d5d17b73d",
    "title": "Can Large Language Models Be Query Optimizer for Relational Databases?",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Jie Tan",
        "Kangfei Zhao",
        "Rui Li",
        "Jeffrey Xu Yu",
        "Chengzhi Piao",
        "Hong Cheng",
        "Helen M. Meng",
        "Deli Zhao",
        "Yu Rong"
    ],
    "doi": "10.48550/arXiv.2502.05562",
    "arxivId": "2502.05562",
    "url": "https://www.semanticscholar.org/paper/05912292a9472c7bf95029128582bc4d5d17b73d",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Query optimization, which finds the optimized execution plan for a given query, is a complex planning and decision-making problem within the exponentially growing plan space in database management systems (DBMS). Traditional optimizers heavily rely on a certain cost model constructed by various heuristics and empirical tuning, probably leading to generating suboptimal plans. Recent developments of Large Language Models (LLMs) have demonstrated their potential in solving complex planning and decision-making problems, such as arithmetic and programmatic tasks. In this paper, we try to explore the potential of LLMs in handling query optimization and propose a tentative LLM-based query optimizer dubbed LLM-QO, established on PostgreSQL's execution engine. In LLM-QO, we formulate query optimization in an autoregressive fashion which directly generates the execution plan without explicit plan enumeration. To investigate the essential input of LLM-QO, we design a customized data recipe named QInstruct to collect the training data from various optimizers and serialize the database's meta data, queries and corresponding plans into a textual format. Based on QInstruct, we implement a two-stage fine-tuning pipeline, Query Instruction Tuning (QIT) and Query Direct Preference Optimization (QDPO), to empower the capability of general-purpose LLMs in handling query optimization. In our experiments, LLM-QO can generate valid and high-quality plans and consistently outperforms both traditional and learned optimizers on three query workloads. Our findings verify that LLMs can be derived as query optimizers where generalization, efficiency and adaptivity deserve further research efforts.",
    "citationCount": 11,
    "referenceCount": 61
}