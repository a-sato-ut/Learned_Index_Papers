{
    "paperId": "1d5316e695b1a65315cd585128236e21412c9ac9",
    "title": "Scaling New Frontiers: Insights into Large Recommendation Models",
    "year": 2024,
    "venue": "arXiv.org",
    "authors": [
        "Wei Guo",
        "Hao Wang",
        "Luankang Zhang",
        "Jin Yao Chin",
        "Zhongzhou Liu",
        "Kai Cheng",
        "Qiushi Pan",
        "Yi Lee",
        "Wanqi Xue",
        "Tingjia Shen",
        "Kenan Song",
        "Kefan Wang",
        "Wenjia Xie",
        "Yuyang Ye",
        "Huifeng Guo",
        "Yong Liu",
        "Defu Lian",
        "Ruiming Tang",
        "Enhong Chen"
    ],
    "doi": "10.48550/arXiv.2412.00714",
    "arxivId": "2412.00714",
    "url": "https://www.semanticscholar.org/paper/1d5316e695b1a65315cd585128236e21412c9ac9",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Recommendation systems are essential for filtering data and retrieving relevant information across various applications. Recent advancements have seen these systems incorporate increasingly large embedding tables, scaling up to tens of terabytes for industrial use. However, the expansion of network parameters in traditional recommendation models has plateaued at tens of millions, limiting further benefits from increased embedding parameters. Inspired by the success of large language models (LLMs), a new approach has emerged that scales network parameters using innovative structures, enabling continued performance improvements. A significant development in this area is Meta's generative recommendation model HSTU, which illustrates the scaling laws of recommendation systems by expanding parameters to thousands of billions. This new paradigm has achieved substantial performance gains in online experiments. In this paper, we aim to enhance the understanding of scaling laws by conducting comprehensive evaluations of large recommendation models. Firstly, we investigate the scaling laws across different backbone architectures of the large recommendation models. Secondly, we conduct comprehensive ablation studies to explore the origins of these scaling laws. We then further assess the performance of HSTU, as the representative of large recommendation models, on complex user behavior modeling tasks to evaluate its applicability. Notably, we also analyze its effectiveness in ranking tasks for the first time. Finally, we offer insights into future directions for large recommendation models. Supplementary materials for our research are available on GitHub at https://github.com/USTC-StarTeam/Large-Recommendation-Models.",
    "citationCount": 22,
    "referenceCount": 129
}