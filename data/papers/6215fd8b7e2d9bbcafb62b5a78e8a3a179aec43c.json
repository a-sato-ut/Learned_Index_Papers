{
    "paperId": "6215fd8b7e2d9bbcafb62b5a78e8a3a179aec43c",
    "title": "Quantile-Based Policy Optimization for Reinforcement Learning",
    "year": 2022,
    "venue": "Online World Conference on Soft Computing in Industrial Applications",
    "authors": [
        "Jinyang Jiang",
        "Jiaqiao Hu",
        "Yijie Peng"
    ],
    "doi": "10.1109/WSC57314.2022.10015456",
    "arxivId": "2201.11463",
    "url": "https://www.semanticscholar.org/paper/6215fd8b7e2d9bbcafb62b5a78e8a3a179aec43c",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2201.11463",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Classical reinforcement learning (RL) aims to optimize the expected cumulative rewards. In this work, we consider the RL setting where the goal is to optimize the quantile of the cumulative rewards. We parameterize the policy controlling actions by neural networks and propose a novel policy gradient algorithm called Quantile-Based Policy Optimization (QPO) and its variant Quantile-Based Proximal Policy Optimization (QPPO) to solve deep RL problems with quantile objectives. QPO uses two coupled iterations running at different time scales for simultaneously estimating quantiles and policy parameters. Our numerical results demonstrate that the proposed algorithms outperform the existing baseline algorithms under the quantile criterion.",
    "citationCount": 10,
    "referenceCount": 38
}