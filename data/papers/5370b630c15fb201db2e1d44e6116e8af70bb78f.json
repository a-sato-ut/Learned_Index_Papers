{
    "paperId": "5370b630c15fb201db2e1d44e6116e8af70bb78f",
    "title": "Predictive data locality optimization for higher-order tensor computations",
    "year": 2021,
    "venue": "MAPS@PLDI",
    "authors": [
        "T. R. Patabandi",
        "Anand Venkat",
        "Abhishek Kulkarni",
        "Pushkar Ratnalikar",
        "Mary W. Hall",
        "Justin Emile Gottschlich"
    ],
    "doi": "10.1145/3460945.3464955",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/5370b630c15fb201db2e1d44e6116e8af70bb78f",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Book"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Automating locality optimization is still an open problem for compiler writers. Compiler-based approaches, guided by analytical cost models have achieved some success in matching high performance libraries on a restricted set of computations such as general matrix multiply (GEMM). On the other hand, library-based approaches may present some open scalability concerns. Recent developments in convolutional neural networks has seen an explosion of models, each with differing combinations of parameters. Manually tuning each of these configurations can take many development months. Further, these operations are called multiple times during machine learning training, which necessitates highly optimized implementations. 2D convolutional operators are unique in that they consist of 7-deep loop nests with different loops carrying reuse for different tensors, making the problem of identifying an optimal loop ordering hard. We devise a machine learning-based compiler which learns a regression model, correlating performance with the loop order. We integrate this model with other traditional compiler analysis for transformations such as loop unrolling and vectorization, relying on the MultiLevel Intermediate Representation (MLIR) compiler framework. We achieve an average speedup of 1.67x and 1.41x against oneDNN for 2D convolution forward and weight update kernels respectively. We are also at 0.88x and 0.96x the performance of oneDNNâ€™s best performing implementation which applies additional data layout transformations.",
    "citationCount": 7,
    "referenceCount": 27
}