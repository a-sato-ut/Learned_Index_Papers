{
    "paperId": "ae6311ebe9954b292f846559282c01e277e1302e",
    "title": "Learning Sparsity and Randomness for Data-driven Low Rank Approximation",
    "year": 2022,
    "venue": "arXiv.org",
    "authors": [
        "Tiejin Chen",
        "Yicheng Tao"
    ],
    "doi": "10.48550/arXiv.2212.08186",
    "arxivId": "2212.08186",
    "url": "https://www.semanticscholar.org/paper/ae6311ebe9954b292f846559282c01e277e1302e",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2212.08186",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Learning-based low rank approximation algorithms can significantly improve the performance of randomized low rank approximation with sketch matrix. With the learned value and fixed non-zero positions for sketch matrices from learning-based algorithms, these matrices can reduce the test error of low rank approximation significantly. However, there is still no good method to learn non-zero positions as well as overcome the out-of-distribution performance loss. In this work, we introduce two new methods Learning Sparsity and Learning Randomness which try to learn a better sparsity patterns and add randomness to the value of sketch matrix. These two methods can be applied with any learning-based algorithms which use sketch matrix directly. Our experiments show that these two methods can improve the performance of previous learning-based algorithm for both test error and out-of-distribution test error without adding too much complexity.",
    "citationCount": 0,
    "referenceCount": 19
}