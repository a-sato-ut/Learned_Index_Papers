{
    "paperId": "848974af53a0612ffc472772fc4c0628214cf433",
    "title": "Holistic query Approximation via RL Modeling",
    "year": 2025,
    "venue": "Proceedings of the VLDB Endowment",
    "authors": [
        "Susan B. Davidson",
        "Tova Milo",
        "Kathy Razmadze",
        "Gal Zeevi"
    ],
    "doi": "10.14778/3725688.3725695",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/848974af53a0612ffc472772fc4c0628214cf433",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "\n In data exploration, executing queries over a large database can be time-consuming. Previous work has proposed approximate query processing as a way to speed up aggregate queries in this context, but do not address non-aggregate queries. Our paper introduces a novel holistic approach to handle both types of queries by finding an optimized subset of data, referred to as an\n approximation set.\n The goal is to maximize query result quality while using a smaller set of data, thereby significantly reducing the query execution time. We formalize this problem as Holistic Approximate Query Processing and establish its NP-completeness. To tackle this, we propose an approximate solution using\n Reinforcement Learning\n , termed HARLM. While HARLM does not provide theoretical guarantees due to its reliance on Reinforcement Learning, it effectively overcomes challenges related to the large action space and the need for generalization beyond a known query workload. Experimental results on both non-aggregate and aggregate benchmarks show that HARLM significantly outperforms the baselines both in terms of accuracy (30% improvement) and efficiency (10â€“35X).\n",
    "citationCount": 0,
    "referenceCount": 48
}