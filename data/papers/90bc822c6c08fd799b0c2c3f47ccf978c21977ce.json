{
    "paperId": "90bc822c6c08fd799b0c2c3f47ccf978c21977ce",
    "title": "Learning the Positions in CountSketch",
    "year": 2020,
    "venue": "International Conference on Learning Representations",
    "authors": [
        "Yi Li",
        "Honghao Lin",
        "Simin Liu",
        "A. Vakilian",
        "David P. Woodruff"
    ],
    "doi": "10.48550/arXiv.2306.06611",
    "arxivId": "2007.09890",
    "url": "https://www.semanticscholar.org/paper/90bc822c6c08fd799b0c2c3f47ccf978c21977ce",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2306.06611",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We consider sketching algorithms which first compress data by multiplication with a random sketch matrix, and then apply the sketch to quickly solve an optimization problem, e.g., low-rank approximation and regression. In the learning-based sketching paradigm proposed by~\\cite{indyk2019learning}, the sketch matrix is found by choosing a random sparse matrix, e.g., CountSketch, and then the values of its non-zero entries are updated by running gradient descent on a training data set. Despite the growing body of work on this paradigm, a noticeable omission is that the locations of the non-zero entries of previous algorithms were fixed, and only their values were learned. In this work, we propose the first learning-based algorithms that also optimize the locations of the non-zero entries. Our first proposed algorithm is based on a greedy algorithm. However, one drawback of the greedy algorithm is its slower training time. We fix this issue and propose approaches for learning a sketching matrix for both low-rank approximation and Hessian approximation for second order optimization. The latter is helpful for a range of constrained optimization problems, such as LASSO and matrix estimation with a nuclear norm constraint. Both approaches achieve good accuracy with a fast running time. Moreover, our experiments suggest that our algorithm can still reduce the error significantly even if we only have a very limited number of training matrices.",
    "citationCount": 25,
    "referenceCount": 37
}