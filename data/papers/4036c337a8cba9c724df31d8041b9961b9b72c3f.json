{
    "paperId": "4036c337a8cba9c724df31d8041b9961b9b72c3f",
    "title": "Efficient LLM Scheduling by Learning to Rank",
    "year": 2024,
    "venue": "Neural Information Processing Systems",
    "authors": [
        "Yichao Fu",
        "Siqi Zhu",
        "Runlong Su",
        "Aurick Qiao",
        "Ion Stoica",
        "Hao Zhang"
    ],
    "doi": "10.48550/arXiv.2408.15792",
    "arxivId": "2408.15792",
    "url": "https://www.semanticscholar.org/paper/4036c337a8cba9c724df31d8041b9961b9b72c3f",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "In Large Language Model (LLM) inference, the output length of an LLM request is typically regarded as not known a priori. Consequently, most LLM serving systems employ a simple First-come-first-serve (FCFS) scheduling strategy, leading to Head-Of-Line (HOL) blocking and reduced throughput and service quality. In this paper, we reexamine this assumption -- we show that, although predicting the exact generation length of each request is infeasible, it is possible to predict the relative ranks of output lengths in a batch of requests, using learning to rank. The ranking information offers valuable guidance for scheduling requests. Building on this insight, we develop a novel scheduler for LLM inference and serving that can approximate the shortest-job-first (SJF) schedule better than existing approaches. We integrate this scheduler with the state-of-the-art LLM serving system and show significant performance improvement in several important applications: 2.8x lower latency in chatbot serving and 6.5x higher throughput in synthetic data generation. Our code is available at https://github.com/hao-ai-lab/vllm-ltr.git",
    "citationCount": 34,
    "referenceCount": 51
}