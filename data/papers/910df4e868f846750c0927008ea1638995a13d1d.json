{
    "paperId": "910df4e868f846750c0927008ea1638995a13d1d",
    "title": "Constrained Auto-Regressive Decoding Constrains Generative Retrieval",
    "year": 2025,
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "authors": [
        "Shiguang Wu",
        "Zhaochun Ren",
        "Xin Xin",
        "Jiyuan Yang",
        "Mengqi Zhang",
        "Zhumin Chen",
        "M. D. Rijke",
        "Pengjie Ren"
    ],
    "doi": "10.1145/3726302.3729934",
    "arxivId": "2504.09935",
    "url": "https://www.semanticscholar.org/paper/910df4e868f846750c0927008ea1638995a13d1d",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Generative retrieval seeks to replace traditional search index data structures with a single large-scale neural network, offering the potential for improved efficiency and seamless integration with generative large language models. As an end-to-end paradigm, generative retrieval adopts a learned differentiable search index to conduct retrieval by directly generating document identifiers through corpus-specific constrained decoding. The generalization capabilities of generative retrieval on out-of-distribution corpora have gathered significant attention. Recent advances primarily focus on the problems arising from training strategies, and addressing them through various learning techniques. However, the fundamental challenges of generalization arising from constrained auto-regressive decoding still remain unexplored and systematically understudied. In this paper, we examine the inherent limitations of constrained auto-regressive generation from two essential perspectives: constraints and beam search. We begin with the Bayes-optimal setting where the generative retrieval model exactly captures the underlying relevance distribution of all possible documents. Then we apply the model to specific corpora by simply adding corpus-specific constraints. Our main findings are two-fold: (i) For the effect of constraints, we derive a lower bound of the error, in terms of the KL divergence between the ground-truth and the model-predicted step-wise marginal distributions. This error arises due to the unawareness of future constraints during generation and is shown to depend on the average Simpson diversity index of the relevance distribution. (ii) For the beam search algorithm used during generation, we reveal that the usage of marginal distributions may not be an ideal approach. Specifically, we prove that for sparse relevance distributions, beam search can achieve perfect top-1 precision but suffer from poor top-k recall performance. To support our theoretical findings, we conduct experiments on synthetic and real-world datasets, validating the existence of the error from adding constraints and the recall performance drop due to beam search. This paper aims to improve our theoretical understanding of the generalization capabilities of the auto-regressive decoding retrieval paradigm, laying a foundation for its limitations and inspiring future advancements toward more robust and generalizable generative retrieval.",
    "citationCount": 0,
    "referenceCount": 71
}