{
    "paperId": "c1c97416e00d464603f1640c89f58e6ef65c7d8d",
    "title": "Learning a Zeroth-Order Optimizer for Fine-Tuning LLMs",
    "year": 2025,
    "venue": "",
    "authors": [
        "Kairun Zhang",
        "Haoyu Li",
        "Yanjun Zhao",
        "Yifan Sun",
        "Huan Zhang"
    ],
    "doi": null,
    "arxivId": "2510.00419",
    "url": "https://www.semanticscholar.org/paper/c1c97416e00d464603f1640c89f58e6ef65c7d8d",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": null,
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Zeroth-order optimizers have recently emerged as a practical approach for fine-tuning large language models (LLMs), significantly reducing GPU memory consumption compared to traditional first-order methods. Yet, existing zeroth-order methods rely on hand-crafted, static sampling strategies that are not adaptable to model-specific structures. To address this, we propose ZO Fine-tuner, a learning-based zeroth-order optimizer for LLMs that automatically learns efficient perturbation strategies through a compact and memory-efficient design. Crucially, our approach is motivated by the observation that only a small number of foundation models and their derivatives are widely adopted in practice. Therefore, learning the optimizer once for a given LLM and reusing it across diverse downstream tasks is both feasible and highly desirable. Accordingly, ZO Fine-tuner is designed to scale learning to learn (L2L) to the foundation-model era by supporting one-time training per LLM with minimal overhead. Experiments on 4 LLMs and 7 datasets show that ZO Fine-tuner outperforms prior zeroth-order baselines in 82.1\\% of task-model combinations, thereby demonstrating strong performance and scalability for efficient LLM fine-tuning. Our code is available at https://github.com/ASTRAL-Group/ZO_Fine_tuner.git.",
    "citationCount": 0,
    "referenceCount": 43
}