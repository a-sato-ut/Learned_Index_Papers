{
    "paperId": "d5177925836efe74c26c7118bef4c0669f3715dc",
    "title": "Demystifying Map Space Exploration for NPUs",
    "year": 2022,
    "venue": "IEEE International Symposium on Workload Characterization",
    "authors": [
        "Sheng-Chun Kao",
        "A. Parashar",
        "Po-An Tsai",
        "T. Krishna"
    ],
    "doi": "10.1109/IISWC55918.2022.00031",
    "arxivId": "2210.03731",
    "url": "https://www.semanticscholar.org/paper/d5177925836efe74c26c7118bef4c0669f3715dc",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2210.03731",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Map Space Exploration is the problem of finding optimized mappings of a Deep Neural Network (DNN) model on an accelerator. It is known to be extremely computationally expensive, and there has been active research looking at both heuristics and learning-based methods to make the problem computationally tractable. However, while there are dozens of mappers out there (all empirically claiming to find better mappings than others), the research community lacks systematic insights on how different search techniques navigate the map-space and how different mapping axes contribute to the accelerator’s performance and efficiency. Such insights are crucial to developing mapping frameworks for emerging DNNs that are increasingly irregular (due to neural architecture search) and sparse, making the corresponding map spaces much more complex. In this work, rather than proposing yet another mapper, we do a first-of-its-kind apples-to-apples comparison of search techniques leveraged by different mappers. Next, we extract the learnings from our study and propose two new techniques that can augment existing mappers — warm-start and sparsity-aware — that demonstrate speedups, scalability, and robustness across diverse DNN models1.1.Code avaliable at https://github.com/maestro-project/gamma-timeloop.",
    "citationCount": 12,
    "referenceCount": 81
}