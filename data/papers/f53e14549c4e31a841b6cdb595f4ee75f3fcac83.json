{
    "paperId": "f53e14549c4e31a841b6cdb595f4ee75f3fcac83",
    "title": "Improving Data Analytics with Fast and Adaptive Regularization",
    "year": 2021,
    "venue": "IEEE Transactions on Knowledge and Data Engineering",
    "authors": [
        "Zhaojing Luo",
        "Shaofeng Cai",
        "Gang Chen",
        "Jinyang Gao",
        "Wang-Chien Lee",
        "K. Ngiam",
        "Meihui Zhang"
    ],
    "doi": "10.1109/TKDE.2019.2916683",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/f53e14549c4e31a841b6cdb595f4ee75f3fcac83",
    "isOpenAccess": true,
    "openAccessPdf": "https://doi.org/10.1109/tkde.2019.2916683",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Medicine",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Deep Learning and Machine Learning models have recently been shown to be effective in many real world applications. While these models achieve increasingly better predictive performance, their structures have also become much more complex. A common and difficult problem for complex models is overfitting. Regularization is used to penalize the complexity of the model in order to avoid overfitting. However, in most learning frameworks, regularization function is usually set with some hyper-parameters where the best setting is difficult to find. In this paper, we propose an adaptive regularization method, as part of a large end-to-end healthcare data analytics software stack, which effectively addresses the above difficulty. First, we propose a general adaptive regularization method based on Gaussian Mixture (GM) to learn the best regularization function according to the observed parameters. Second, we develop an effective update algorithm which integrates Expectation Maximization (EM) with Stochastic Gradient Descent (SGD). Third, we design a lazy update and sparse update algorithm to reduce the computational cost by 4x and 20x, respectively. The overall regularization framework is fast, adaptive, and easy-to-use. We validate the effectiveness of our regularization method through an extensive experimental study over 14 standard benchmark datasets and three kinds of deep learning/machine learning models. The results illustrate that our proposed adaptive regularization method achieves significant improvement over state-of-the-art regularization methods.",
    "citationCount": 13,
    "referenceCount": 49
}