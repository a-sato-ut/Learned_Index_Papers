{
    "paperId": "8525342f216b969f3e7d9f9f7f081c0ac86dae99",
    "title": "DB-GPT-Hub: Towards Open Benchmarking Text-to-SQL Empowered by Large Language Models",
    "year": 2024,
    "venue": "arXiv.org",
    "authors": [
        "Fan Zhou",
        "Siqiao Xue",
        "Danrui Qi",
        "Wenhui Shi",
        "Wang Zhao",
        "Ganglin Wei",
        "Hongyang Zhang",
        "Caigai Jiang",
        "Gangwei Jiang",
        "Zhixuan Chu",
        "Faqiang Chen"
    ],
    "doi": "10.48550/arXiv.2406.11434",
    "arxivId": "2406.11434",
    "url": "https://www.semanticscholar.org/paper/8525342f216b969f3e7d9f9f7f081c0ac86dae99",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Large language models (LLMs) becomes the dominant paradigm for the challenging task of text-to-SQL. LLM-empowered text-to-SQL methods are typically categorized into prompting-based and tuning approaches. Compared to prompting-based methods, benchmarking fine-tuned LLMs for text-to-SQL is important yet under-explored, partially attributed to the prohibitively high computational cost. In this paper, we present DB-GPT-Hub, an open benchmark suite for LLM-empowered text-to-SQL, which primarily focuses on tuning LLMs at large scales. The proposed benchmark consists of: 1. a standardized and comprehensive evaluation of text-to-SQL tasks by fine-tuning medium to large-sized open LLMs; 2. a modularized and easy-to-extend codebase with mainstream LLMs and experimental scenarios supported, which prioritizes fine-tuning methods but can be easily extended to prompt-based setting. Our work investigates the potential gains and the performance boundaries of tuning approaches, compared to prompting approaches and explores optimal solutions tailored to specific scenarios. We hope DB-GPT-Hub, along with these findings, enables further research and broad applications that would otherwise be difficult owing to the absence of a dedicated open benchmark. The project code has been released at https://github.com/eosphoros-ai/DB-GPT-Hub.",
    "citationCount": 5,
    "referenceCount": 50
}