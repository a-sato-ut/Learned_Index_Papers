{
    "paperId": "d350caa058dac68e40df4958f5cc6fc4ed6ebc1c",
    "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising 1-Consistency",
    "year": 2025,
    "venue": "",
    "authors": [
        "Peng Chen",
        "Hailiang Zhao",
        "Jiaji Zhang",
        "Xueyan Tang",
        "Yixuan Wang",
        "Shuiguang Deng"
    ],
    "doi": null,
    "arxivId": "2507.16242",
    "url": "https://www.semanticscholar.org/paper/d350caa058dac68e40df4958f5cc6fc4ed6ebc1c",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": null,
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The online caching problem aims to minimize cache misses when serving a sequence of requests under a limited cache size. While naive learning-augmented caching algorithms achieve ideal $1$-consistency, they lack robustness guarantees. Existing robustification methods either sacrifice $1$-consistency or introduce excessive computational overhead. In this paper, we introduce Guard, a lightweight robustification framework that enhances the robustness of a broad class of learning-augmented caching algorithms to $2H_k + 2$, while preserving their $1$-consistency. Guard achieves the current best-known trade-off between consistency and robustness, with only $O(1)$ additional per-request overhead, thereby maintaining the original time complexity of the base algorithm. Extensive experiments across multiple real-world datasets and prediction models validate the effectiveness of Guard in practice.",
    "citationCount": 0,
    "referenceCount": 56
}