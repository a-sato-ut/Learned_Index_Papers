{
    "paperId": "ac9465cd4565164948de7e62ebd6b6e9ccbd5033",
    "title": "Changing Base Without Losing Pace: A GPU-Efficient Alternative to MatMul in DNNs",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Nir Ailon",
        "A. Bercovich",
        "Omri Weinstein"
    ],
    "doi": "10.48550/arXiv.2503.12211",
    "arxivId": "2503.12211",
    "url": "https://www.semanticscholar.org/paper/ac9465cd4565164948de7e62ebd6b6e9ccbd5033",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Modern AI relies on huge matrix multiplications (MatMuls), whose computation poses a scalability problem for inference and training. We propose an alternative, GPU native bilinear operator to MatMuls in neural networks, which offers a three-way tradeoff between: speed, accuracy and parameter count. In particular, this operator requires substantially fewer FLOPs to evaluate ($\\ll n^3$), yet increases the parameter count compared to MatMul ($\\gg n^2$). We call this operator Strassen-Tile (STL). The key idea behind STL is a local learnable change-of-basis, applied on tiles of the weight and activation matrices, followed by an element-wise product between the tiles, implemented simultaneously via MatMul. The key technical question we study is how to optimize the change-of-basis of a given layer, which is a highly non-convex problem. We show that theory-backed initializations (inspired by fast matrix and polynomial multiplication) lead to substantially better accuracy than random SGD initialization. This phenomenon motivates further algorithmic study of STL optimization in DNNs. Our experiments demonstrate that STL can approximate 4x4 MatMul of tiles while reducing FLOPs by a factor of 2.66, and can improve Imagenet-1K accuracy of SoTA T2T-ViT-7 (4.3M parameters) while lowering FLOPs. Even with non-CUDA optimized PyTorch code, STL achieves wall-clock speedups in the compute-bound regime. These results, together with its theoretical grounds, suggest STL as a promising building block for scalable and cost-efficient AI.",
    "citationCount": 3,
    "referenceCount": 72
}