{
    "paperId": "da23d3b0b8d98db8b657e260fb4be3951d635431",
    "title": "Supporting Dynamic Program Sizes in Deep Learning-Based Cost Models for Code Optimization",
    "year": 2025,
    "venue": "ACM Transactions on Architecture and Code Optimization (TACO)",
    "authors": [
        "Yacine Hakimi",
        "Riyadh Baghdadi",
        "Y. Challal"
    ],
    "doi": "10.1145/3727638",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/da23d3b0b8d98db8b657e260fb4be3951d635431",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Automatic code optimization enables developers to write high-level code relying on compilers to optimize it and generate efficient code for target hardware. State-of-the-art methods for automatic code optimization leverage deep learning to build cost models that predict the impact of code optimizations on execution time. However, these models are typically limited in terms of the size and complexity of the programs they support. This research presents a novel approach to developing deep learning-based cost models that address these limitations. Our approach introduces a new program representation that efficiently represents programs with complex structures and large sizes such as varying loop depths, buffer numbers, and dimensions. Furthermore, we propose a novel deep learning architecture, that can handle this dynamic program representation. This allows the model to work on larger and more complex programs than those it was trained on. We implemented this model in Tiramisu, a state-of-the-art compiler. Our evaluation shows that our proposed model can generalize to programs larger than those seen during training, while the original Tiramisu cost model cannot. We also show that such generality does not lead to a significant increase in our proposed modelâ€™s Mean Absolute Percentage Error or a decrease in the quality of code optimizations found when the model is used for automatic code optimization. In contrast, our proposed model on average achieves a 41.89% improvement in speed compared to the original cost model when both models are trained on the same dataset, showing better generalization over unseen programs. This is a significant advantage over previous approaches, which typically do not support program sizes beyond those seen during the training.",
    "citationCount": 0,
    "referenceCount": 47
}