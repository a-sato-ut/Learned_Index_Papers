{
    "paperId": "76587ffeb352a80c9a2d07febf718ccd99909715",
    "title": "Generalizing Functional Error Correction for Language and Vision-Language Models",
    "year": 2024,
    "venue": "International Conference on Machine Learning and Applications",
    "authors": [
        "Wenyu Peng",
        "Simeng Zheng",
        "Michael Baluja",
        "Tao Xie",
        "Anxiao Jiang",
        "Paul H. Siegel"
    ],
    "doi": "10.1109/ICMLA61862.2024.00110",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/76587ffeb352a80c9a2d07febf718ccd99909715",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The goal of functional error correction is to preserve neural network performance when stored network weights are corrupted by noise. To achieve this goal, a selective protection (SP) scheme was proposed to optimally protect the functionally important bits in binary weight representations in a layer-dependent manner. Although it showed its effectiveness in image classification tasks on some relatively simple networks such as ResNet-18 and VGG-16, it becomes inadequate for emerging complex machine learning tasks generated from natural language processing and vision-language association domains. To solve this problem, we extend the SP scheme in three directions: task complexity, model complexity, and storage complexity. Extensions to complex natural language and vision-language tasks include text categorization and “zero-shot” textual classification of images. Extensions to more complex models with deeper block structures and attention mechanisms consist of Very Deep Convolutional Neural Network (VDCNN) and Contrastive Language-Image Pre-Training (CLIP) networks. Extensions to more complex storage configurations focus on distributed storage architectures to support model parallelism. Experimental results show that the optimized SP scheme preserves network performance in all of these settings. The results also provide insights into redundancy-performance tradeoffs, generalizability of SP across datasets and tasks, and robustness of partitioned network architectures.",
    "citationCount": 0,
    "referenceCount": 25
}