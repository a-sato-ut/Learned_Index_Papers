{
    "paperId": "9b3ecfd45f63cb20b46e6c9cc7b452886f38575d",
    "title": "RAMQA: A Unified Framework for Retrieval-Augmented Multi-Modal Question Answering",
    "year": 2025,
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "authors": [
        "Yang Bai",
        "Christan Earl Grant",
        "Daisy Zhe Wang"
    ],
    "doi": "10.48550/arXiv.2501.13297",
    "arxivId": "2501.13297",
    "url": "https://www.semanticscholar.org/paper/9b3ecfd45f63cb20b46e6c9cc7b452886f38575d",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Multi-modal retrieval-augmented Question Answering (MRAQA), integrating text and images, has gained significant attention in information retrieval (IR) and natural language processing (NLP). Traditional ranking methods rely on small encoder-based language models, which are incompatible with modern decoder-based generative large language models (LLMs) that have advanced various NLP tasks. To bridge this gap, we propose RAMQA, a unified framework combining learning-to-rank methods with generative permutation-enhanced ranking techniques. We first train a pointwise multi-modal ranker using LLaVA as the backbone. Then, we apply instruction tuning to train a LLaMA model for re-ranking the top-k documents using an innovative autoregressive multi-task learning approach. Our generative ranking model generates re-ranked document IDs and specific answers from document candidates in various permutations. Experiments on two MRAQA benchmarks, WebQA and MultiModalQA, show significant improvements over strong baselines, highlighting the effectiveness of our approach. Code and data are available at: https://github.com/TonyBY/RAMQA",
    "citationCount": 2,
    "referenceCount": 54
}