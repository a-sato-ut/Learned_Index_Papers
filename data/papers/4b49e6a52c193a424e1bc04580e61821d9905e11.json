{
    "paperId": "4b49e6a52c193a424e1bc04580e61821d9905e11",
    "title": "FlexSP: Accelerating Large Language Model Training via Flexible Sequence Parallelism",
    "year": 2024,
    "venue": "International Conference on Architectural Support for Programming Languages and Operating Systems",
    "authors": [
        "Yujie Wang",
        "Shiju Wang",
        "Shenhan Zhu",
        "Fangcheng Fu",
        "Xinyi Liu",
        "Xuefeng Xiao",
        "Huixia Li",
        "Jiashi Li",
        "Faming Wu",
        "Bin Cui"
    ],
    "doi": "10.1145/3676641.3715998",
    "arxivId": "2412.01523",
    "url": "https://www.semanticscholar.org/paper/4b49e6a52c193a424e1bc04580e61821d9905e11",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2412.01523",
    "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Extending the context length (i.e., the maximum supported sequence length) of LLMs is of paramount significance. To facilitate long context training of LLMs, sequence parallelism has emerged as an essential technique, which scatters each input sequence across multiple devices and necessitates communication to process the sequence. In essence, existing sequence parallelism methods assume homogeneous sequence lengths (i.e., all input sequences are equal in length) and therefore leverages a single, static scattering strategy for all input sequences. However, in reality, the sequence lengths in LLM training corpora exhibit substantial variability, often following a long-tail distribution, which leads to workload heterogeneity. In this paper, we show that employing a single, static strategy results in inefficiency and resource under-utilization, highlighting the need for adaptive approaches to handle the heterogeneous workloads across sequences. To address this, we propose a heterogeneity-adaptive sequence parallelism method. For each training step, our approach captures the variability in sequence lengths and assigns the optimal combination of scattering strategies based on workload characteristics. We model this problem as a linear programming optimization and design an efficient and effective solver to find the optimal solution. Furthermore, we implement our method in a high-performance system that supports adaptive parallelization in distributed LLM training. Experimental results demonstrate that our system outperforms state-of-the-art training frameworks by up to 1.98x. Source code is available at https://github.com/PKU-DAIR/Hetu-Galvatron.",
    "citationCount": 12,
    "referenceCount": 54
}