{
    "paperId": "09bda461aa4911d0513e8e46dd39a4113947e450",
    "title": "Ansor : Generating High-Performance Tensor Programs for Deep Learning",
    "year": 2020,
    "venue": "USENIX Symposium on Operating Systems Design and Implementation",
    "authors": [
        "Lianmin Zheng",
        "Chengfan Jia",
        "Minmin Sun",
        "Zhao Wu",
        "Cody Hao Yu",
        "Ameer Haj-Ali",
        "Yida Wang",
        "Jun Yang",
        "Danyang Zhuo",
        "Koushik Sen",
        "Joseph E. Gonzalez",
        "Ion Stoica"
    ],
    "doi": null,
    "arxivId": "2006.06762",
    "url": "https://www.semanticscholar.org/paper/09bda461aa4911d0513e8e46dd39a4113947e450",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "High-performance tensor programs are crucial to guarantee efficient execution of deep learning models. However, obtaining performant tensor programs for different operators on various hardware platforms is notoriously difficult. Currently, deep learning systems rely on vendor-provided kernel libraries or various search strategies to get performant tensor programs. These approaches either require significant engineering efforts in developing platform-specific optimization code or fall short in finding high-performance programs due to restricted search space and ineffective exploration strategy. \nWe present Ansor, a tensor program generation framework for deep learning applications. Compared with existing search strategies, Ansor explores much more optimization combinations by sampling programs from a hierarchical representation of the search space. Ansor then fine-tunes the sampled programs with evolutionary search and a learned cost model to identify the best programs. Ansor can find high-performance programs that are outside the search space of existing state-of-the-art approaches. Besides, Ansor utilizes a scheduler to simultaneously optimize multiple subgraphs in a set of deep neural networks. Our evaluation shows that Ansor improves the execution performance of deep neural networks on the Intel CPU, ARM CPU, and NVIDIA GPU by up to $3.8\\times$, $2.6\\times$, and $1.7 \\times$, respectively.",
    "citationCount": 443,
    "referenceCount": 67
}