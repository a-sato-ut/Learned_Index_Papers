{
    "paperId": "1e94331d12051b41a8fc80948e8531acdb0613c8",
    "title": "Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Wuwei Zhang",
        "Fangcong Yin",
        "Howard Yen",
        "Danqi Chen",
        "Xi Ye"
    ],
    "doi": "10.48550/arXiv.2506.09944",
    "arxivId": "2506.09944",
    "url": "https://www.semanticscholar.org/paper/1e94331d12051b41a8fc80948e8531acdb0613c8",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Recent work has identified retrieval heads, a subset of attention heads responsible for retrieving salient information in long-context language models (LMs), as measured by their copy-paste behavior in Needlein-a-Haystack tasks. In this paper, we introduce QRHead (Query-Focused Retrieval Head), an improved set of attention heads that enhance retrieval from long context. We identify QRHead by aggregating attention scores with respect to the input query, using a handful of examples from real-world tasks (e.g., long-context QA). We further introduce QRRetriever, an efficient and effective retriever that uses the accumulated attention mass of QRHead as retrieval scores. We use QRRetriever for long-context reasoning by selecting the most relevant parts with the highest retrieval scores. On multi-hop reasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains over full context and outperforms strong dense retrievers. We also evaluate QRRetriever as a re-ranker on the BEIR benchmark and find that it achieves strong zero-shot performance, outperforming other LLM-based re-rankers such as RankGPT. Further analysis shows that both the query-context attention scoring and task selection are crucial for identifying QRHead with strong downstream utility. Overall, our work contributes a general-purpose retriever and offers interpretability insights into the long-context capabilities of LMs.",
    "citationCount": 4,
    "referenceCount": 58
}