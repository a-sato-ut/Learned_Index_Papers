{
    "paperId": "dd06b331be3e72f5e7b74275aa4065b76b4df5dc",
    "title": "Supersonic: Learning to Generate Source Code Optimizations in C/C++",
    "year": 2023,
    "venue": "IEEE Transactions on Software Engineering",
    "authors": [
        "Zimin Chen",
        "Sen Fang",
        "Monperrus Martin"
    ],
    "doi": "10.1109/TSE.2024.3423769",
    "arxivId": "2309.14846",
    "url": "https://www.semanticscholar.org/paper/dd06b331be3e72f5e7b74275aa4065b76b4df5dc",
    "isOpenAccess": true,
    "openAccessPdf": "https://doi.org/10.1109/tse.2024.3423769",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Software optimization refines programs for resource efficiency while preserving functionality. Traditionally, it is a process done by developers and compilers. This paper introduces a third option, automated optimization at the source code level. We present <sc>Supersonic</sc>, a neural approach targeting minor source code modifications for optimization. Using a seq2seq model, <sc>Supersonic</sc> is trained on C/C++ program pairs (<inline-formula><tex-math notation=\"LaTeX\">$x_{t}$</tex-math><alternatives><mml:math display=\"inline\"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href=\"chen-ieq1-3423769.gif\"/></alternatives></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\">$x_{t+1}$</tex-math><alternatives><mml:math display=\"inline\"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href=\"chen-ieq2-3423769.gif\"/></alternatives></inline-formula>), where <inline-formula><tex-math notation=\"LaTeX\">$x_{t+1}$</tex-math><alternatives><mml:math display=\"inline\"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href=\"chen-ieq3-3423769.gif\"/></alternatives></inline-formula> is an optimized version of <inline-formula><tex-math notation=\"LaTeX\">$x_{t}$</tex-math><alternatives><mml:math display=\"inline\"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href=\"chen-ieq4-3423769.gif\"/></alternatives></inline-formula>, and outputs a diff. <sc>Supersonic</sc>'s performance is benchmarked against OpenAI's GPT-3.5-Turbo and GPT-4 on competitive programming tasks. The experiments show that <sc>Supersonic</sc> not only outperforms both models on the code optimization task but also minimizes the extent of the change with a model more than 600x smaller than GPT-3.5-Turbo and 3700x smaller than GPT-4.",
    "citationCount": 23,
    "referenceCount": 62
}