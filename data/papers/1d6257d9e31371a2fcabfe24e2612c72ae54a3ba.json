{
    "paperId": "1d6257d9e31371a2fcabfe24e2612c72ae54a3ba",
    "title": "ZeroGR: A Generalizable and Scalable Framework for Zero-Shot Generative Retrieval",
    "year": 2025,
    "venue": "",
    "authors": [
        "Weiwei Sun",
        "Keyi Kong",
        "Xinyu Ma",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "M. D. Rijke",
        "Zhaochun Ren",
        "Yiming Yang"
    ],
    "doi": null,
    "arxivId": "2510.10419",
    "url": "https://www.semanticscholar.org/paper/1d6257d9e31371a2fcabfe24e2612c72ae54a3ba",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": null,
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Generative retrieval (GR) reformulates information retrieval (IR) by framing it as the generation of document identifiers (docids), thereby enabling an end-to-end optimization and seamless integration with generative language models (LMs). Despite notable progress under supervised training, GR still struggles to generalize to zero-shot IR scenarios, which are prevalent in real-world applications. To tackle this challenge, we propose \\textsc{ZeroGR}, a zero-shot generative retrieval framework that leverages natural language instructions to extend GR across a wide range of IR tasks. Specifically, \\textsc{ZeroGR} is composed of three key components: (i) an LM-based docid generator that unifies heterogeneous documents (e.g., text, tables, code) into semantically meaningful docids; (ii) an instruction-tuned query generator that generates diverse types of queries from natural language task descriptions to enhance corpus indexing; and (iii) a reverse annealing decoding strategy to balance precision and recall during docid generation. We investigate the impact of instruction fine-tuning scale and find that performance consistently improves as the number of IR tasks encountered during training increases. Empirical results on the BEIR and MAIR benchmarks demonstrate that \\textsc{ZeroGR} outperforms strong dense retrieval and generative baselines in zero-shot settings, establishing a new state-of-the-art for instruction-driven GR.",
    "citationCount": 0,
    "referenceCount": 66
}