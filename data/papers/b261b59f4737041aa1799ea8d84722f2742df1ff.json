{
    "paperId": "b261b59f4737041aa1799ea8d84722f2742df1ff",
    "title": "Geometric View of Soft Decorrelation in Self-Supervised Learning",
    "year": 2024,
    "venue": "Knowledge Discovery and Data Mining",
    "authors": [
        "Yifei Zhang",
        "Hao Zhu",
        "Zixing Song",
        "Yankai Chen",
        "Xinyu Fu",
        "Ziqiao Meng",
        "Piotr Koniusz",
        "Irwin King"
    ],
    "doi": "10.1145/3637528.3671914",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/b261b59f4737041aa1799ea8d84722f2742df1ff",
    "isOpenAccess": true,
    "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3637528.3671914",
    "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Contrastive learning, a form of Self-Supervised Learning (SSL), typically consists of an alignment term and a regularization term. The alignment term minimizes the distance between the embeddings of a positive pair, while the regularization term prevents trivial solutions and expresses prior beliefs about the embeddings. As a widely used regularization technique, soft decorrelation has been employed by several non-contrastive SSL methods to avoid trivial solutions. While the decorrelation term is designed to address the issue of dimensional collapse, we find that it fails to achieve this goal theoretically and experimentally. Based on such a finding, we extend the soft decorrelation regularization to minimize the distance between the covariance matrix and an identity matrix. We provide a new perspective on the geometric distance between positive definite matrices to investigate why the soft decorrelation cannot efficiently solve the dimensional collapse. Furthermore, we construct a family of loss functions utilizing the Bregman Matrix Divergence (BMD), with the soft decorrelation representing a specific instance within this family. We prove that a loss function (LogDet) in this family can solve the issue of dimensional collapse. Our novel loss functions based on BMD exhibit superior performance compared to the soft decorrelation and other baseline techniques, as demonstrated by experimental results on graph and image datasets.",
    "citationCount": 17,
    "referenceCount": 67
}