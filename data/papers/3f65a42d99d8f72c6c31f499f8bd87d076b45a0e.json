{
    "paperId": "3f65a42d99d8f72c6c31f499f8bd87d076b45a0e",
    "title": "Efficient Video Segmentation Models with Per-frame Inference",
    "year": 2022,
    "venue": "arXiv.org",
    "authors": [
        "Yifan Liu",
        "Chunhua Shen",
        "Changqian Yu",
        "Jingdong Wang"
    ],
    "doi": null,
    "arxivId": "2202.12427",
    "url": "https://www.semanticscholar.org/paper/3f65a42d99d8f72c6c31f499f8bd87d076b45a0e",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Most existing real-time deep models trained with each frame independently may produce inconsistent results across the temporal axis when tested on a video sequence. A few methods take the correlations in the video sequence into account, e.g., by propagating the results to the neighboring frames using optical flow, or extracting frame representations using multiframe information, which may lead to inaccurate results or unbalanced latency. In this work, we focus on improving the temporal consistency without introducing computation overhead in inference. To this end, we perform inference at each frame. Temporal consistency is achieved by learning from video frames with extra constraints during the training phase. introduced for inference. We propose several techniques to learn from the video sequence, including a temporal consistency loss and online/offline knowledge distillation methods. On the task of semantic video segmentation, weighing among accuracy, temporal smoothness, and efficiency, our proposed method outperforms keyframe based methods and a few baseline methods that are trained with each frame independently, on datasets including Cityscapes, Camvid and 300VW-Mask. We further apply our training method to video instance segmentation on YouTubeVIS and develop an application of portrait matting in video sequences, by segmenting temporally consistent instance-level trimaps across frames. Experiments show superior qualitative and quantitative results. Code is available at: https://git.io/vidseg.",
    "citationCount": 0,
    "referenceCount": 62
}