{
    "paperId": "fe0b2ebe2053dfaf61f78c4b7f9d715a0b3a746d",
    "title": "Personalization for Web-based Services using Offline Reinforcement Learning",
    "year": 2021,
    "venue": "Machine-mediated learning",
    "authors": [
        "P. Apostolopoulos",
        "Zehui Wang",
        "Hanson Wang",
        "Chad Zhou",
        "Kittipat Virochsiri",
        "Norm Zhou",
        "Igor L. Markov"
    ],
    "doi": "10.1007/s10994-024-06525-y",
    "arxivId": "2102.05612",
    "url": "https://www.semanticscholar.org/paper/fe0b2ebe2053dfaf61f78c4b7f9d715a0b3a746d",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2102.05612",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Large-scale Web-based services present opportunities for improving UI policies based on observed user interactions. We address challenges of learning such policies through model-free offline Reinforcement Learning (RL) with off-policy training. Deployed in a production system for user authentication in a major social network, it significantly improves long-term objectives. We articulate practical challenges, compare several ML techniques, provide insights on training and evaluation of RL models, and discuss generalizations.",
    "citationCount": 7,
    "referenceCount": 58
}