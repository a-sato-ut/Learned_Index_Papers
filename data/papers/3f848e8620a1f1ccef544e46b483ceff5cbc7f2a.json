{
    "paperId": "3f848e8620a1f1ccef544e46b483ceff5cbc7f2a",
    "title": "Understanding Performance of Long-Document Ranking Models through Comprehensive Evaluation and Leaderboarding",
    "year": 2022,
    "venue": "arXiv.org",
    "authors": [
        "Leonid Boytsov",
        "Tianyi Lin",
        "Fangwei Gao",
        "Yutian Zhao",
        "Jeffrey Huang",
        "Eric Nyberg"
    ],
    "doi": "10.48550/arXiv.2207.01262",
    "arxivId": "2207.01262",
    "url": "https://www.semanticscholar.org/paper/3f848e8620a1f1ccef544e46b483ceff5cbc7f2a",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2207.01262",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We evaluated 20+ Transformer models for ranking of long documents (including recent LongP models trained with FlashAttention) and compared them with a simple FirstP baseline, which applies the same model to the truncated input (at most 512 tokens). We used MS MARCO Documents v1 as a primary training set and evaluated both the zero-shot transferred and fine-tuned models. On MS MARCO, TREC DLs, and Robust04 no long-document model outperformed FirstP by more than 5% in NDCG and MRR (when averaged over all test sets). We conjectured this was not due to models' inability to process long context, but due to a positional bias of relevant passages, whose distribution was skewed towards the beginning of documents. We found direct evidence of this bias in some test sets, which motivated us to create MS MARCO FarRelevant (based on MS MARCO Passages) where the relevant passages were not present among the first 512 tokens. Unlike standard collections where we saw both little benefit from incorporating longer contexts and limited variability in model performance (within a few %), experiments on MS MARCO FarRelevant uncovered dramatic differences among models. The FirstP models performed roughly at the random-baseline level in both zero-shot and fine-tuning scenarios. Simple aggregation models including MaxP and PARADE Attention had good zero-shot accuracy, but benefited little from fine-tuning. Most other models had poor zero-shot performance (sometimes at a random baseline level), but outstripped MaxP by as much as 13-28% after fine-tuning. Thus, the positional bias not only diminishes benefits of processing longer document contexts, but also leads to model overfitting to positional bias and performing poorly in a zero-shot setting when the distribution of relevant passages changes substantially. We make our software and data available.",
    "citationCount": 9,
    "referenceCount": 89
}