{
    "paperId": "142b6ed4178b8d79068da049a0fdf692fd5e2895",
    "title": "Advancing Table Understanding of Large Language Models via Feature Re-ordering",
    "year": 2025,
    "venue": "SIGKDD Explorations",
    "authors": [
        "Guanchu Wang",
        "Yuzhong Chen",
        "Huiyuan Chen",
        "Xiran Fan",
        "Junpeng Wang",
        "Xiaoting Li",
        "Mingzhi Hu",
        "Chia-yuan Chang",
        "Xia Hu"
    ],
    "doi": "10.1145/3748239.3748248",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/142b6ed4178b8d79068da049a0fdf692fd5e2895",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Linguistics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Large Language Models (LLMs) exhibit exceptional proficiency in comprehending human language. Despite their significant success across a wide array of tasks, understanding tabular data remains a challenging task. Especially, tabular data lacks an intrinsic order of the different features (table fields), whereas LLMs take only sequential inputs. Consequently, an artificial order is imposed, the impact of which on the performance of LLMs has not yet been thoroughly investigated. Surprisingly, as discovered in this work, this artificially induced order bias dramatically influences the performance of LLMs on tasks related to tabular data. Mitigating the order bias presents a significant challenge. To address this, we propose a simple and cost-effective method, Re-Ordering Tabular feATures fOR LLM (ROTATOR-LLM), to conduct test-time compute without fine-tuning the base LLM. Aiming at optimizing the feature order of tabular data and boosting LLMs' capability to better understand the data semantics, ROTATOR-LLM re-frames the ordering problem as a feature trajectory generation task. A dynamic programming based meta-controller is trained to auto-regressively generate an individualized feature trajectory for each data instance via accumulative value estimation of the serialized feature input through the LLM's final performance metrics. Model performance is maximized by iteratively selecting features across different steps. Experimental results on multiple datasets and LLMs show close to or over 20% performance boosts via features reordered by ROTATOR-LLM against the un-ordered counterpart. Meanwhile, it outperforms stateof- the-Art tabular LLM methods with significant margin.",
    "citationCount": 0,
    "referenceCount": 43
}