{
    "paperId": "959adda6f1b874be32bea9bb0bd8f3b43856980b",
    "title": "Non-Programmers Can Label Programs Indirectly via Active Examples: A Case Study with Text-to-SQL",
    "year": 2022,
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "authors": [
        "Ruiqi Zhong",
        "Charles Burton Snell",
        "D. Klein",
        "Jason Eisner"
    ],
    "doi": "10.18653/v1/2023.emnlp-main.312",
    "arxivId": "2205.12422",
    "url": "https://www.semanticscholar.org/paper/959adda6f1b874be32bea9bb0bd8f3b43856980b",
    "isOpenAccess": true,
    "openAccessPdf": "https://aclanthology.org/2023.emnlp-main.312.pdf",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Can non-programmers annotate natural language utterances with complex programs that represent their meaning? We introduce APEL, a framework in which non-programmers select among candidate programs generated by a seed semantic parser (e.g., Codex). Since they cannot understand the candidate programs, we ask them to select indirectly by examining the programs' input-ouput examples. For each utterance, APEL actively searches for a simple input on which the candidate programs tend to produce different outputs. It then asks the non-programmers only to choose the appropriate output, thus allowing us to infer which program is correct and could be used to fine-tune the parser. As a first case study, we recruited human non-programmers to use APEL to re-annotate SPIDER, a text-to-SQL dataset. Our approach achieved the same annotation accuracy as the original expert annotators (75%) and exposed many subtle errors in the original annotations.",
    "citationCount": 10,
    "referenceCount": 60
}