{
    "paperId": "6837297f1304d415f6d1d9f745609973a2be69c0",
    "title": "Robust Knowledge Transfer in Tiered Reinforcement Learning",
    "year": 2023,
    "venue": "Neural Information Processing Systems",
    "authors": [
        "Jiawei Huang",
        "Niao He"
    ],
    "doi": "10.48550/arXiv.2302.05534",
    "arxivId": "2302.05534",
    "url": "https://www.semanticscholar.org/paper/6837297f1304d415f6d1d9f745609973a2be69c0",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2302.05534",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "In this paper, we study the Tiered Reinforcement Learning setting, a parallel transfer learning framework, where the goal is to transfer knowledge from the low-tier (source) task to the high-tier (target) task to reduce the exploration risk of the latter while solving the two tasks in parallel. Unlike previous work, we do not assume the low-tier and high-tier tasks share the same dynamics or reward functions, and focus on robust knowledge transfer without prior knowledge on the task similarity. We identify a natural and necessary condition called the ``Optimal Value Dominance'' for our objective. Under this condition, we propose novel online learning algorithms such that, for the high-tier task, it can achieve constant regret on partial states depending on the task similarity and retain near-optimal regret when the two tasks are dissimilar, while for the low-tier task, it can keep near-optimal without making sacrifice. Moreover, we further study the setting with multiple low-tier tasks, and propose a novel transfer source selection mechanism, which can ensemble the information from all low-tier tasks and allow provable benefits on a much larger state-action space.",
    "citationCount": 1,
    "referenceCount": 38
}