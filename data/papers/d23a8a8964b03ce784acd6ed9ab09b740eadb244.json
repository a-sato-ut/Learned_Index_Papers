{
    "paperId": "d23a8a8964b03ce784acd6ed9ab09b740eadb244",
    "title": "A Mathematics-Inspired Learning-to-Optimize Framework for Decentralized Optimization",
    "year": 2024,
    "venue": "",
    "authors": [
        "Yutong He",
        "Qiulin Shang",
        "Xinmeng Huang",
        "Jialin Liu",
        "Kun Yuan"
    ],
    "doi": null,
    "arxivId": "2410.01700",
    "url": "https://www.semanticscholar.org/paper/d23a8a8964b03ce784acd6ed9ab09b740eadb244",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": null,
    "s2FieldsOfStudy": [
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Most decentralized optimization algorithms are handcrafted. While endowed with strong theoretical guarantees, these algorithms generally target a broad class of problems, thereby not being adaptive or customized to specific problem features. This paper studies data-driven decentralized algorithms trained to exploit problem features to boost convergence. Existing learning-to-optimize methods typically suffer from poor generalization or prohibitively vast search spaces. In addition, the vast search space of communicating choices and final goal to reach the global solution via limited neighboring communication cast more challenges in decentralized settings. To resolve these challenges, this paper first derives the necessary conditions that successful decentralized algorithmic rules need to satisfy to achieve both optimality and consensus. Based on these conditions, we propose a novel Mathematics-inspired Learning-to-optimize framework for Decentralized optimization (MiLoDo). Empirical results demonstrate that MiLoDo-trained algorithms outperform handcrafted algorithms and exhibit strong generalizations. Algorithms learned via MiLoDo in 100 iterations perform robustly when running 100,000 iterations during inferences. Moreover, MiLoDo-trained algorithms on synthetic datasets perform well on problems involving real data, higher dimensions, and different loss functions.",
    "citationCount": 2,
    "referenceCount": 62
}