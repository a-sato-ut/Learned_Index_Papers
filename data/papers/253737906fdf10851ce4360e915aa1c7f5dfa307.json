{
    "paperId": "253737906fdf10851ce4360e915aa1c7f5dfa307",
    "title": "Computing in the Era of Large Generative Models: From Cloud-Native to AI-Native",
    "year": 2024,
    "venue": "arXiv.org",
    "authors": [
        "Yao Lu",
        "Song Bian",
        "Lequn Chen",
        "Yongjun He",
        "Yulong Hui",
        "Matthew Lentz",
        "Beibin Li",
        "Fei Liu",
        "Jialin Li",
        "Qi Liu",
        "Rui Liu",
        "Xiaoxuan Liu",
        "Lin Ma",
        "Kexin Rong",
        "Jianguo Wang",
        "Yingjun Wu",
        "Yongji Wu",
        "Huanchen Zhang",
        "Minjia Zhang",
        "Qizhen Zhang",
        "Tianyi Zhou",
        "Danyang Zhuo"
    ],
    "doi": "10.48550/arXiv.2401.12230",
    "arxivId": "2401.12230",
    "url": "https://www.semanticscholar.org/paper/253737906fdf10851ce4360e915aa1c7f5dfa307",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "In this paper, we investigate the intersection of large generative AI models and cloud-native computing architectures. Recent large models such as ChatGPT, while revolutionary in their capabilities, face challenges like escalating costs and demand for high-end GPUs. Drawing analogies between large-model-as-a-service (LMaaS) and cloud database-as-a-service (DBaaS), we describe an AI-native computing paradigm that harnesses the power of both cloud-native technologies (e.g., multi-tenancy and serverless computing) and advanced machine learning runtime (e.g., batched LoRA inference). These joint efforts aim to optimize costs-of-goods-sold (COGS) and improve resource accessibility. The journey of merging these two domains is just at the beginning and we hope to stimulate future research and development in this area.",
    "citationCount": 11,
    "referenceCount": 76
}