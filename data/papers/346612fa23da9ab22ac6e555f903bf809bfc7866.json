{
    "paperId": "346612fa23da9ab22ac6e555f903bf809bfc7866",
    "title": "S'MoRE: Structural Mixture of Residual Experts for LLM Fine-tuning",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Hanqing Zeng",
        "Yinglong Xia",
        "Zhuokai Zhao",
        "Gilbert Jiang",
        "Qiang Zhang",
        "Jiayi Liu",
        "Lizhu Zhang",
        "Xiangjun Fan",
        "Benyu Zhang"
    ],
    "doi": "10.48550/arXiv.2504.06426",
    "arxivId": "2504.06426",
    "url": "https://www.semanticscholar.org/paper/346612fa23da9ab22ac6e555f903bf809bfc7866",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Fine-tuning pre-trained large language models (LLMs) presents a dual challenge of balancing parameter efficiency and model capacity. Existing methods like low-rank adaptations (LoRA) are efficient but lack flexibility, while Mixture-of-Experts (MoE) architectures enhance model capacity at the cost of more&under-utilized parameters. To address these limitations, we propose Structural Mixture of Residual Experts (S'MoRE), a novel framework that seamlessly integrates the efficiency of LoRA with the flexibility of MoE. Specifically, S'MoRE employs hierarchical low-rank decomposition of expert weights, yielding residuals of varying orders interconnected in a multi-layer structure. By routing input tokens through sub-trees of residuals, S'MoRE emulates the capacity of many experts by instantiating and assembling just a few low-rank matrices. We craft the inter-layer propagation of S'MoRE's residuals as a special type of Graph Neural Network (GNN), and prove that under similar parameter budget, S'MoRE improves\"structural flexibility\"of traditional MoE (or Mixture-of-LoRA) by exponential order. Comprehensive theoretical analysis and empirical results demonstrate that S'MoRE achieves superior fine-tuning performance, offering a transformative approach for efficient LLM adaptation.",
    "citationCount": 0,
    "referenceCount": 47
}