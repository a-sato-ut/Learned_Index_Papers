{
    "paperId": "e688afc310c691bb9f1274375d6da6f063c9b006",
    "title": "Can Graph Reordering Speed Up Graph Neural Network Training? An Experimental Study",
    "year": 2024,
    "venue": "Proceedings of the VLDB Endowment",
    "authors": [
        "Nikolai Merkel",
        "Pierre Toussing",
        "R. Mayer",
        "Hans-Arno Jacobsen"
    ],
    "doi": "10.48550/arXiv.2409.11129",
    "arxivId": "2409.11129",
    "url": "https://www.semanticscholar.org/paper/e688afc310c691bb9f1274375d6da6f063c9b006",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "\n Graph neural networks (GNNs) are a type of neural network capable of learning on graph-structured data. However, training GNNs on large-scale graphs is challenging due to iterative aggregations of high-dimensional features from neighboring vertices within sparse graph structures combined with neural network operations. The sparsity of graphs frequently results in suboptimal memory access patterns and longer training time.\n Graph reordering\n is an optimization strategy aiming to improve the graph data layout. It has shown to be effective to speed up graph analytics workloads, but its effect on the performance of GNN training has not been investigated yet. The generalization of reordering to GNN performance is nontrivial, as multiple aspects must be considered: GNN hyper-parameters such as the number of layers, the number of hidden dimensions, and the feature size used in the GNN model, neural network operations, large intermediate vertex states, and GPU acceleration.\n \n In our work, we close this gap by performing an empirical evaluation of 12 reordering strategies in two state-of-the-art GNN systems, PyTorch Geometric and Deep Graph Library. Our results show that graph reordering is effective in reducing training time for CPU- and GPU-based training, respectively. Further, we find that GNN hyper-parameters influence the effectiveness of reordering, that reordering metrics play an important role in selecting a reordering strategy, that lightweight reordering performs better for GPU-based than for CPU-based training, and that invested reordering time can in many cases be amortized.",
    "citationCount": 4,
    "referenceCount": 78
}