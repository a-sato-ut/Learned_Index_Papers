{
    "paperId": "9c0178f59e46d52c7c41f318099581fef8c81aa0",
    "title": "ZERoTuNE: Learned Zero-Shot Cost Models for Parallelism Tuning in Stream Processing",
    "year": 2024,
    "venue": "IEEE International Conference on Data Engineering",
    "authors": [
        "Pratyush Agnihotri",
        "B. Koldehofe",
        "Paul Stiegele",
        "Roman Heinrich",
        "Carsten Binnig",
        "Manisha Luthra"
    ],
    "doi": "10.1109/ICDE60146.2024.00163",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/9c0178f59e46d52c7c41f318099581fef8c81aa0",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "This paper introduces ZEROTuNE, a novel cost model for parallel and distributed stream processing that can be used to effectively set initial parallelism degrees of streaming queries. Unlike existing models, which rely majorly on online learning statistics that are non-transferable, context-specific, and require extensive training, ZEROTuNE proposes data-efficient zero-shot learning techniques that enable very accurate cost predictions without having observed any query deployment. To overcome these challenges, we propose ZEROTuNE, a graph neural network architecture that can learn from the structural complexity of parallel distributed stream processing systems, enabling them to adapt to unseen workloads and hardware configurations. In our experiments, we show when integrating ZEROTuNE in a distributed streaming system such as Apache Flink, we can accurately set the degree of parallelism, showing an average speed-up of around 5Ã— in comparison to existing approaches.",
    "citationCount": 7,
    "referenceCount": 55
}