{
    "paperId": "eea0fc23005d4279968d8106009624b6ce74be14",
    "title": "ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification",
    "year": 2023,
    "venue": "FoMLAS@CAV",
    "authors": [
        "Marco Casadio",
        "Luca Arnaboldi",
        "M. Daggitt",
        "Omri Isac",
        "Tanvi Dinkar",
        "Daniel Kienitz",
        "Verena Rieser",
        "Ekaterina Komendantskaya"
    ],
    "doi": "10.48550/arXiv.2305.04003",
    "arxivId": "2305.04003",
    "url": "https://www.semanticscholar.org/paper/eea0fc23005d4279968d8106009624b6ce74be14",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2305.04003",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Verification of machine learning models used in Natural Language Processing (NLP) is known to be a hard problem. In particular, many known neural network verification methods that work for computer vision and other numeric datasets do not work for NLP. Here, we study technical reasons that underlie this problem. Based on this analysis, we propose practical methods and heuristics for preparing NLP datasets and models in a way that renders them amenable to state-of-the-art verification methods. We implement these methods as a Python library called ANTONIO that links to the neural network verifiers ERAN and Marabou. We perform evaluation of the tool using an NLP dataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP applications. We hope that, thanks to its general applicability, this work will open novel possibilities for including NLP verification problems into neural network verification competitions, and will popularise NLP problems within this community.",
    "citationCount": 4,
    "referenceCount": 52
}