{
    "paperId": "c1928f42ad6293562df6078b83adef1e78bd224e",
    "title": "Tango: Rethinking Quantization for Graph Neural Network Training on GPUs",
    "year": 2023,
    "venue": "International Conference for High Performance Computing, Networking, Storage and Analysis",
    "authors": [
        "Shiyang Chen",
        "Da Zheng",
        "Caiwen Ding",
        "Chengying Huan",
        "Yuede Ji",
        "Hang Liu"
    ],
    "doi": "10.1145/3581784.3607037",
    "arxivId": "2308.00890",
    "url": "https://www.semanticscholar.org/paper/c1928f42ad6293562df6078b83adef1e78bd224e",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2308.00890",
    "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Graph learning is becoming increasingly popular due to its superior performance in tackling many grand challenges. While quantization is widely used to accelerate Graph Neural Network (GNN) computation, quantized training faces remarkable roadblocks. Current quantized GNN training systems often experience longer training time than their full-precision counterparts for two reasons: (i) addressing the quantization accuracy challenge leads to excessive overhead, and (ii) the optimization potential exposed by quanti-zation is not adequately leveraged. This paper introduces Tango which re-thinks quantization challenges and opportunities for graph neural network training on GPUs with three contributions: Firstly, we introduce efficient rules to maintain accuracy during quantized GNN training. Secondly, we design and implement quantization-aware primitives and inter-primitive optimizations to speed up GNN training. Finally, we integrate Tango with the popular Deep Graph Library (DGL) system and demonstrate its superior performance over the state-of-the-art approaches on various GNN models and datasets.",
    "citationCount": 8,
    "referenceCount": 106
}