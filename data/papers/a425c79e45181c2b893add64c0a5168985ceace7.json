{
    "paperId": "a425c79e45181c2b893add64c0a5168985ceace7",
    "title": "Dividable Configuration Performance Learning",
    "year": 2024,
    "venue": "IEEE Transactions on Software Engineering",
    "authors": [
        "Jingzhi Gong",
        "Tao Chen",
        "Rami Bahsoon"
    ],
    "doi": "10.1109/TSE.2024.3491945",
    "arxivId": "2409.07629",
    "url": "https://www.semanticscholar.org/paper/a425c79e45181c2b893add64c0a5168985ceace7",
    "isOpenAccess": true,
    "openAccessPdf": "https://doi.org/10.1109/tse.2024.3491945",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Machine/deep learning models have been widely adopted to predict the configuration performance of software systems. However, a crucial yet unaddressed challenge is how to cater for the sparsity inherited from the configuration landscape: the influence of configuration options (features) and the distribution of data samples are highly sparse. In this paper, we propose a model-agnostic and sparsity-robust framework for predicting configuration performance, dubbed <monospace>DaL</monospace>, based on the new paradigm of dividable learning that builds a model via “divide-and-learn”. To handle sample sparsity, the samples from the configuration landscape are divided into distant divisions, for each of which we build a sparse local model, e.g., regularized Hierarchical Interaction Neural Network, to deal with the feature sparsity. A newly given configuration would then be assigned to the right model of division for the final prediction. Further, <monospace>DaL</monospace> adaptively determines the optimal number of divisions required for a system and sample size without any extra training or profiling. Experiment results from 12 real-world systems and five sets of training data reveal that, compared with the state-of-the-art approaches, <monospace>DaL</monospace> performs no worse than the best counterpart on 44 out of 60 cases (within which 31 cases are significantly better) with up to <inline-formula><tex-math notation=\"LaTeX\">$1.61\\times$</tex-math><alternatives><mml:math><mml:mn>1.61</mml:mn><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href=\"chen-ieq1-3491945.gif\"/></alternatives></inline-formula> improvement on accuracy; requires fewer samples to reach the same/better accuracy; and producing acceptable training overhead. In particular, the mechanism that adapted the parameter <inline-formula><tex-math notation=\"LaTeX\">$d$</tex-math><alternatives><mml:math><mml:mi>d</mml:mi></mml:math><inline-graphic xlink:href=\"chen-ieq2-3491945.gif\"/></alternatives></inline-formula> can reach the optimal value for 76.43% of the individual runs. The result also confirms that the paradigm of dividable learning is more suitable than other similar paradigms such as ensemble learning for predicting configuration performance. Practically, <monospace>DaL</monospace> considerably improves different global models when using them as the underlying local models, which further strengthens its flexibility. To promote open science, all the data, code, and supplementary materials of this work can be accessed at our repository: <uri>https://github.com/ideas-labo/DaL-ext</uri>.",
    "citationCount": 4,
    "referenceCount": 132
}