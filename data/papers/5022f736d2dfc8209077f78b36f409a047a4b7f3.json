{
    "paperId": "5022f736d2dfc8209077f78b36f409a047a4b7f3",
    "title": "Enhancing Graph Contrastive Learning with Reliable and Informative Augmentation for Recommendation",
    "year": 2024,
    "venue": "Knowledge Discovery and Data Mining",
    "authors": [
        "Bowen Zheng",
        "Junjie Zhang",
        "Hongyu Lu",
        "Yu Chen",
        "Ming Chen",
        "Wayne Xin Zhao",
        "Ji-Rong Wen"
    ],
    "doi": "10.1145/3690624.3709214",
    "arxivId": "2409.05633",
    "url": "https://www.semanticscholar.org/paper/5022f736d2dfc8209077f78b36f409a047a4b7f3",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Graph neural network (GNN) has been a powerful approach in collaborative filtering (CF) due to its ability to model high-order user-item relationships. Recently, to alleviate the data sparsity and enhance representation learning, many efforts have been conducted to integrate contrastive learning (CL) with GNNs. Despite the promising improvements, the contrastive view generation based on structure and representation perturbations in existing methods potentially disrupts the collaborative information in contrastive views, resulting in limited effectiveness of positive alignment. To overcome this issue, we propose CoGCL, a novel framework that aims to enhance graph contrastive learning by constructing contrastive views with stronger collaborative information via discrete codes. The core idea is to map users and items into discrete codes rich in collaborative information for reliable and informative contrastive view generation. To this end, we initially introduce a multi-level vector quantizer in an end-to-end manner to quantize user and item representations into discrete codes. Based on these discrete codes, we enhance the collaborative information of contrastive views by considering neighborhood structure and semantic relevance respectively. For neighborhood structure, we propose virtual neighbor augmentation by treating discrete codes as virtual neighbors, which expands an observed user-item interaction into multiple edges involving discrete codes. Regarding semantic relevance, we identify similar users/items based on shared discrete codes and interaction targets to generate the semantically relevant view. Through these strategies, we construct contrastive views with stronger collaborative information and develop a triple-view graph contrastive learning approach. Extensive experiments on four public datasets demonstrate the effectiveness of our proposed approach. Moreover, detailed analyses highlight our contribution in enhancing graph CL for recommendation. Our code is available at https://github.com/RUCAIBox/CoGCL.",
    "citationCount": 3,
    "referenceCount": 65
}