{
    "paperId": "2339e49c04aa2b5d2651439542687ebcc4549b80",
    "title": "ExRec: Experimental Framework for Reconfigurable Networks Based on Off-the-Shelf Hardware",
    "year": 2021,
    "venue": "Symposium on Architectures for Networking and Communications Systems",
    "authors": [
        "Johannes Zerwas",
        "C. Avin",
        "S. Schmid",
        "Andreas Blenk"
    ],
    "doi": "10.1145/3493425.3502748",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/2339e49c04aa2b5d2651439542687ebcc4549b80",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Book",
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "In order to meet the increasingly stringent throughput and latency requirements in datacenter networks, several innovative network architectures based on reconfigurable optical topologies have been proposed. Examples include demand-oblivious reconfigurable topologies such as RotorNet (SIGCOMM 2017), Opera (NSDI 2020), and Sirius (SIGCOMM 2021), as well as demand-aware topologies such as ProjecToR (SIGCOMM 2016). All these architectures feature attractive performance properties using specific prototypes. However, reproducing these experiments is often difficult due to missing hardware and publicly available software. This paper presents a flexible framework for reconfigurable networks based on off-the-shelf hardware, which supports experimentation and reproducibility at a small scale. We describe how our framework, ExReC, can be instantiated with different configurations, allowing us to emulate existing architectures and to study their trade-offs. Finally, we demonstrate the application of our approach to different use cases and workloads, including distributed machine learning training.",
    "citationCount": 7,
    "referenceCount": 31
}