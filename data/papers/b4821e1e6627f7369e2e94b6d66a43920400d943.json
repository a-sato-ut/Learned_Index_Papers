{
    "paperId": "b4821e1e6627f7369e2e94b6d66a43920400d943",
    "title": "Synthetic continued pretraining",
    "year": 2024,
    "venue": "International Conference on Learning Representations",
    "authors": [
        "Zitong Yang",
        "Neil Band",
        "Shuangping Li",
        "Emmanuel J. Cand√®s",
        "Tatsunori Hashimoto"
    ],
    "doi": "10.48550/arXiv.2409.07431",
    "arxivId": "2409.07431",
    "url": "https://www.semanticscholar.org/paper/b4821e1e6627f7369e2e94b6d66a43920400d943",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Pretraining on large-scale, unstructured internet text enables language models to acquire a significant amount of world knowledge. However, this knowledge acquisition is data-inefficient--to learn a given fact, models must be trained on hundreds to thousands of diverse representations of it. This poses a challenge when adapting a pretrained model to a small corpus of domain-specific documents, where each fact may appear rarely or only once. We propose to bridge this gap with synthetic continued pretraining: using the small domain-specific corpus to synthesize a large corpus more amenable to learning, and then performing continued pretraining on the synthesized corpus. We instantiate this proposal with EntiGraph, a synthetic data augmentation algorithm that extracts salient entities from the source documents and then generates diverse text by drawing connections between the sampled entities. Synthetic continued pretraining with EntiGraph enables a language model to answer questions and follow generic instructions related to the source documents without access to them. If, instead, the source documents are available at inference time, we show that the knowledge acquired through our approach compounds with retrieval-augmented generation. To better understand these results, we build a simple mathematical model of EntiGraph, and show how synthetic data augmentation can\"rearrange\"knowledge to enable more data-efficient learning.",
    "citationCount": 30,
    "referenceCount": 105
}