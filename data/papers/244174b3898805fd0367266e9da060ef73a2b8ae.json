{
    "paperId": "244174b3898805fd0367266e9da060ef73a2b8ae",
    "title": "Disentangled Pre-training for Image Matting",
    "year": 2023,
    "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
    "authors": [
        "Yan-Da Li",
        "Zilong Huang",
        "Gang Yu",
        "Ling Chen",
        "Yunchao Wei",
        "Jianbo Jiao"
    ],
    "doi": "10.1109/WACV57701.2024.00024",
    "arxivId": "2304.00784",
    "url": "https://www.semanticscholar.org/paper/244174b3898805fd0367266e9da060ef73a2b8ae",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2304.00784",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Image matting requires high-quality pixel-level human annotations to support the training of a deep model in recent literature. Whereas such annotation is costly and hard to scale, significantly holding back the development of the research. In this work, we make the first attempt towards addressing this problem, by proposing a self-supervised pretraining approach that can leverage infinite numbers of data to boost the matting performance. The pre-training task is designed in a similar manner as image matting, where random trimap and alpha matte are generated to achieve an image disentanglement objective. The pre-trained model is then used as an initialisation of the downstream matting task for fine-tuning. Extensive experimental evaluations show that the proposed approach outperforms both the state-of-the-art matting methods and other alternative self-supervised initialisation approaches by a large margin. We also show the robustness of the proposed approach over different backbone architectures. Our project page is available at https://crystraldo.github.io/dpt_mat/.",
    "citationCount": 1,
    "referenceCount": 48
}