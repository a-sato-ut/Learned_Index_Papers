{
    "paperId": "278be4609602dd1d1002775dcaf18ca05f2e3333",
    "title": "Efficient Processing of Spiking Neural Networks via Task Specialization",
    "year": 2024,
    "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence",
    "authors": [
        "Muath Abu Lebdeh",
        "K. Yıldırım",
        "Davide Brunelli"
    ],
    "doi": "10.1109/TETCI.2024.3370028",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/278be4609602dd1d1002775dcaf18ca05f2e3333",
    "isOpenAccess": true,
    "openAccessPdf": "https://ieeexplore.ieee.org/ielx7/7433297/7777658/10471594.pdf",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Spiking neural networks (SNNs) are considered as a candidate for efficient deep learning systems: these networks communicate with 0 or 1 spikes and their computations do not require the multiply operation. On the other hand, SNNs still have large memory overhead and poor utilization of the memory hierarchy; powerful SNN has large memory requirements and requires multiple inference steps with dynamic memory patterns. This paper proposes performing the image classification task as collaborative tasks of specialized SNNs. This specialization allows us to significantly reduce the number of memory operations and improve the utilization of memory hierarchy. Our results show that the proposed approach improves the energy and latency of SNNs inference by more than 10x. In addition, our work shows that designing narrow (and deep) SNNs is computationally more efficient than designing wide (and shallow) SNNs.",
    "citationCount": 1,
    "referenceCount": 39
}