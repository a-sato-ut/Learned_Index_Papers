{
    "paperId": "afbada39756b700a18cf010301f9adfa1546341b",
    "title": "RL: Generic reinforcement learning codebase in TensorFlow",
    "year": 2019,
    "venue": "Journal of Open Source Software",
    "authors": [
        "Bryan M. Li",
        "A. Cowen-Rivers",
        "Piotr Kozakowski",
        "David Tao",
        "S. Kamalakara",
        "Nitarshan Rajkumar",
        "Hariharan Sezhiyan",
        "Sicong Huang",
        "Aidan N. Gomez"
    ],
    "doi": "10.21105/joss.01524",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/afbada39756b700a18cf010301f9adfa1546341b",
    "isOpenAccess": true,
    "openAccessPdf": "https://doi.org/10.21105/joss.01524",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Vast reinforcement learning (RL) research groups, such as DeepMind and OpenAI, have their internal (private) reinforcement learning codebases, which enable quick prototyping and comparing of ideas to many state-of-the-art (SOTA) methods. We argue the five fundamental properties of a sophisticated research codebase are: modularity, reproducibility, many RL algorithms pre-implemented, speed and ease of running on different hardware/ integration with visualization packages. Currently, there does not exist any RL codebase, to the author’s knowledge, which contains all the five properties, particularly with TensorBoard logging and abstracting away cloud hardware such as TPU’s from the user. The codebase aims to help distil the best research practices into the community as well as ease the entry access and accelerate the pace of the field. More detailed documentation can be found here.",
    "citationCount": 0,
    "referenceCount": 18
}