{
    "paperId": "5097d710fa8d25c527126ab590d8793fe58aa461",
    "title": "HAKES: Scalable Vector Database for Embedding Search Service",
    "year": 2025,
    "venue": "Proceedings of the VLDB Endowment",
    "authors": [
        "Guoyu Hu",
        "Shaofeng Cai",
        "Tien Tuan Anh Dinh",
        "Zhongle Xie",
        "Cong Yue",
        "Gang Chen",
        "Bengchin Ooi"
    ],
    "doi": "10.14778/3746405.3746427",
    "arxivId": "2505.12524",
    "url": "https://www.semanticscholar.org/paper/5097d710fa8d25c527126ab590d8793fe58aa461",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Modern deep learning models capture the semantics of complex data by transforming them into high-dimensional embedding vectors. Emerging applications, such as retrieval-augmented generation, use approximate nearest neighbor (ANN) search in the embedding vector space to find similar data. Existing vector databases provide indexes for efficient ANN searches, with graph-based indexes being the most popular due to their low latency and high recall in real-world high-dimensional datasets. However, these indexes are costly to build, suffer from significant contention under concurrent read-write workloads, and scale poorly to multiple servers.\n \n Our goal is to build a vector database that achieves high throughput and high recall under concurrent read-write workloads. To this end, we first propose an ANN index with an explicit two-stage design combining a fast filter stage with highly compressed vectors and a refine stage to ensure recall, and we devise a novel lightweight machine learning technique to fine-tune the index parameters. We introduce an early termination check to dynamically adapt the search process for each query. Next, we add support for writes while maintaining search performance by decoupling the management of the learned parameters. Finally, we design HAKES, a distributed vector database that serves the new index in a disaggregated architecture. We evaluate our index and system against 12 state-of-the-art indexes and three distributed vector databases, using high-dimensional embedding datasets generated by deep learning models. The experimental results show that our index outperforms index baselines in the high recall region and under concurrent read-write workloads. Furthermore,\n HAKES\n is scalable and achieves up to 16x higher throughputs than the baselines.\n",
    "citationCount": 1,
    "referenceCount": 64
}