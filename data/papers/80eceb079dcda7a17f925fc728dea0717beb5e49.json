{
    "paperId": "80eceb079dcda7a17f925fc728dea0717beb5e49",
    "title": "PreScope: Unleashing the Power of Prefetching for Resource-Constrained MoE Inference",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Enda Yu",
        "Zhaoning Zhang",
        "Dezun Dong",
        "Yongwei Wu",
        "Xiangke Liao"
    ],
    "doi": "10.48550/arXiv.2509.23638",
    "arxivId": "2509.23638",
    "url": "https://www.semanticscholar.org/paper/80eceb079dcda7a17f925fc728dea0717beb5e49",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Mixture-of-Experts (MoE) models face memory and PCIe latency bottlenecks when deployed on commodity hardware. Offloading expert weights to CPU memory results in PCIe transfer latency that exceeds GPU computation by several folds. We present PreScope, a prediction-driven expert scheduling system that addresses three key challenges: inaccurate activation prediction, PCIe bandwidth competition, and cross-device scheduling complexity. Our solution includes: 1) Learnable Layer-Aware Predictor (LLaPor) that captures layer-specific expert activation patterns; 2) Prefetch-Aware Cross-Layer Scheduling (PreSched) that generates globally optimal plans balancing prefetching costs and loading overhead; 3) Asynchronous I/O Optimizer (AsyncIO) that decouples I/O from computation, eliminating waiting bubbles. PreScope achieves 141% higher throughput and 74.6% lower latency than state-of-the-art solutions.",
    "citationCount": 0,
    "referenceCount": 51
}