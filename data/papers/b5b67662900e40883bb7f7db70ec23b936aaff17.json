{
    "paperId": "b5b67662900e40883bb7f7db70ec23b936aaff17",
    "title": "Dual Correction Strategy for Ranking Distillation in Top-N Recommender System",
    "year": 2021,
    "venue": "International Conference on Information and Knowledge Management",
    "authors": [
        "Youngjune Lee",
        "Kee-Eung Kim"
    ],
    "doi": "10.1145/3459637.3482093",
    "arxivId": "2109.03459",
    "url": "https://www.semanticscholar.org/paper/b5b67662900e40883bb7f7db70ec23b936aaff17",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2109.03459",
    "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Knowledge Distillation (KD), which transfers the knowledge of a well-trained large model (teacher) to a small model (student), has become an important area of research for practical deployment of recommender systems. Recently, Relaxed Ranking Distillation (RRD) has shown that distilling the ranking information in the recommendation list significantly improves the performance. However, the method still has limitations in that 1) it does not fully utilize the prediction errors of the student model, which makes the training not fully efficient, and 2) it only distills the user-side ranking information, which provides an insufficient view under the sparse implicit feedback. This paper presents Dual Correction strategy for Distillation (DCD), which transfers the ranking information from the teacher model to the student model in a more efficient manner. Most importantly, DCD uses the discrepancy between the teacher model and the student model predictions to decide which knowledge to be distilled. By doing so, DCD essentially provides the learning guidance tailored to \"correcting\" what the student model has failed to accurately predict. This process is applied for transferring the ranking information from the user-side as well as the item-side to address sparse implicit user feedback. Our experiments show that the proposed method outperforms the state-of-the-art baselines, and ablation studies validate the effectiveness of each component.",
    "citationCount": 23,
    "referenceCount": 30
}