{
    "paperId": "691a9bdcc768fed662ced45742658c8224798988",
    "title": "Generative Dense Retrieval: Memory Can Be a Burden",
    "year": 2024,
    "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
    "authors": [
        "Peiwen Yuan",
        "Xinglin Wang",
        "Shaoxiong Feng",
        "Boyuan Pan",
        "Yiwei Li",
        "Heda Wang",
        "Xupeng Miao",
        "Kan Li"
    ],
    "doi": "10.48550/arXiv.2401.10487",
    "arxivId": "2401.10487",
    "url": "https://www.semanticscholar.org/paper/691a9bdcc768fed662ced45742658c8224798988",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Generative Retrieval (GR), autoregressively decoding relevant document identifiers given a query, has been shown to perform well under the setting of small-scale corpora. By memorizing the document corpus with model parameters, GR implicitly achieves deep interaction between query and document. However, such a memorizing mechanism faces three drawbacks: (1) Poor memory accuracy for fine-grained features of documents; (2) Memory confusion gets worse as the corpus size increases; (3) Huge memory update costs for new documents. To alleviate these problems, we propose the Generative Dense Retrieval (GDR) paradigm. Specifically, GDR first uses the limited memory volume to achieve inter-cluster matching from query to relevant document clusters. Memorizing-free matching mechanism from Dense Retrieval (DR) is then introduced to conduct fine-grained intra-cluster matching from clusters to relevant documents. The coarse-to-fine process maximizes the advantages of GR’s deep interaction and DR’s scalability. Besides, we design a cluster identifier constructing strategy to facilitate corpus memory and a cluster-adaptive negative sampling strategy to enhance the intra-cluster mapping ability. Empirical results show that GDR obtains an average of 3.0 R@100 improvement on NQ dataset under multiple settings and has better scalability.",
    "citationCount": 9,
    "referenceCount": 25
}