{
    "paperId": "9665ee48ddabd9ae5cfe32b3d8633ee097b533f9",
    "title": "Can Uncertainty Quantification Improve Learned Index Benefit Estimation?",
    "year": 2024,
    "venue": "IEEE Transactions on Knowledge and Data Engineering",
    "authors": [
        "Tao Yu",
        "Zhaonian Zou",
        "Hao Xiong"
    ],
    "doi": "10.1109/TKDE.2025.3591237",
    "arxivId": "2410.17748",
    "url": "https://www.semanticscholar.org/paper/9665ee48ddabd9ae5cfe32b3d8633ee097b533f9",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Index tuning is crucial for optimizing database performance by selecting optimal indexes based on workload. The key to this process lies in an accurate and efficient benefit estimator. Traditional methods relying on what-if tools often suffer from inefficiency and inaccuracy. In contrast, learning-based models provide a promising alternative but face challenges such as instability, lack of interpretability, and complex management. To overcome these limitations, we adopt a novel approach: quantifying the uncertainty in learning-based modelsâ€™ results, thereby combining the strengths of both traditional and learning-based methods for reliable index tuning. We propose Beauty, the first uncertainty-aware framework that enhances learning-based models with uncertainty quantification and uses what-if tools as a complementary mechanism to improve reliability and reduce management complexity. Specifically, we introduce a novel method that combines AutoEncoder and Monte Carlo Dropout to jointly quantify uncertainty, tailored to the characteristics of benefit estimation tasks. In experiments involving sixteen models, our approach outperformed existing uncertainty quantification methods in the majority of cases. We also conducted index tuning tests on six datasets. By applying the Beauty framework, we eliminated worst-case scenarios and more than tripled the occurrence of best-case scenarios.",
    "citationCount": 0,
    "referenceCount": 38
}