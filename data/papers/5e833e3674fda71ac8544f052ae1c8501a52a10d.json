{
    "paperId": "5e833e3674fda71ac8544f052ae1c8501a52a10d",
    "title": "Mask-Guided Matting in the Wild",
    "year": 2023,
    "venue": "Computer Vision and Pattern Recognition",
    "authors": [
        "Kwanyong Park",
        "Sanghyun Woo",
        "Seoung Wug Oh",
        "In-So Kweon",
        "Joon-Young Lee"
    ],
    "doi": "10.1109/CVPR52729.2023.00198",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/5e833e3674fda71ac8544f052ae1c8501a52a10d",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Mask-guided matting has shown great practicality compared to traditional trimap-based methods. The mask-guided approach takes an easily-obtainable coarse mask as guidance and produces an accurate alpha matte. To extend the success toward practical usage, we tackle mask-guided matting in the wild, which covers a wide range of categories in their complex context robustly. To this end, we propose a simple yet effective learning framework based on two core insights: 1) learning a generalized matting model that can better understand the given mask guidance and 2) leveraging weak supervision datasets (e.g., instance segmentation dataset) to alleviate the limited diversity and scale of existing matting datasets. Extensive experimental results on multiple benchmarks, consisting of a newly proposed synthetic benchmark (Composition-Wild) and existing natural datasets, demonstrate the superiority of the proposed method. Moreover, we provide appealing results on new practical applications (e.g., panoptic matting and mask-guided video matting), showing the great generality and potential of our model.",
    "citationCount": 15,
    "referenceCount": 55
}