{
    "paperId": "6ebae28a2d5e2ab02a4fe9644f6a0d02f2d7ab38",
    "title": "Diffusion for Natural Image Matting",
    "year": 2023,
    "venue": "arXiv.org",
    "authors": [
        "Yihan Hu",
        "Yiheng Lin",
        "Wei Wang",
        "Yao Zhao",
        "Yunchao Wei",
        "Humphrey Shi"
    ],
    "doi": "10.48550/arXiv.2312.05915",
    "arxivId": "2312.05915",
    "url": "https://www.semanticscholar.org/paper/6ebae28a2d5e2ab02a4fe9644f6a0d02f2d7ab38",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We aim to leverage diffusion to address the challenging image matting task. However, the presence of high computational overhead and the inconsistency of noise sampling between the training and inference processes pose significant obstacles to achieving this goal. In this paper, we present DiffMatte, a solution designed to effectively overcome these challenges. First, DiffMatte decouples the decoder from the intricately coupled matting network design, involving only one lightweight decoder in the iterations of the diffusion process. With such a strategy, DiffMatte mitigates the growth of computational overhead as the number of samples increases. Second, we employ a self-aligned training strategy with uniform time intervals, ensuring a consistent noise sampling between training and inference across the entire time domain. Our DiffMatte is designed with flexibility in mind and can seamlessly integrate into various modern matting architectures. Extensive experimental results demonstrate that DiffMatte not only reaches the state-of-the-art level on the Composition-1k test set, surpassing the best methods in the past by 5% and 15% in the SAD metric and MSE metric respectively, but also show stronger generalization ability in other benchmarks.",
    "citationCount": 14,
    "referenceCount": 85
}