{
    "paperId": "c5d038cfad52a2aa18d072455a2c87003f77029b",
    "title": "LoHan: Low-Cost High-Performance Framework to Fine-Tune 100B Model on a Consumer GPU",
    "year": 2024,
    "venue": "",
    "authors": [
        "Changyue Liao",
        "Mo Sun",
        "Zihan Yang",
        "Jun Xie",
        "Kaiqi Chen",
        "Binhang Yuan",
        "Fei Wu",
        "Zeke Wang"
    ],
    "doi": null,
    "arxivId": "2403.06504",
    "url": "https://www.semanticscholar.org/paper/c5d038cfad52a2aa18d072455a2c87003f77029b",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": null,
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Nowadays, AI researchers become more and more interested in fine-tuning a pre-trained LLM, whose size has grown to up to over 100B parameters, for their downstream tasks. One approach to fine-tune such huge models is to aggregate device memory from many GPUs. However, this approach introduces prohibitive costs for most data scientists with a limited budget for high-end GPU servers. In this paper, we focus on LLM fine-tuning on a single consumer-grade GPU in a commodity server with limited main memory capacity, which is accessible to most AI researchers. In such a scenario, existing offloading-based methods fail to fine-tune an LLM efficiently due to a lack of holistic intra-server tensor movement management. To this end, we present LoHan, a low-cost, high-performance deep learning training framework that enables efficient 100B-scale model fine-tuning on a commodity server with a consumer-grade GPU and limited main memory capacity. The key idea is to add holistic offloading traffic as an optimization dimension for 1)active gradient offloading, and 2)holistic traffic-aware activation swapping mechanism. The experimental results show that 1)LoHan is the first to fine-tune a 175B model on an RTX 4090 and 256 GB main memory, 2)LoHan achieves 2.32x throughput than the state-of-the-art baselines when fine-tuning a small 13B model, and 3)LoHan enables a cheap low-end consumer GPU to have higher cost-effectiveness than a DGX-A100 cluster when fine-tuning a 175B model.",
    "citationCount": 6,
    "referenceCount": 114
}