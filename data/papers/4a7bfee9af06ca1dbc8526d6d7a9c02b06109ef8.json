{
    "paperId": "4a7bfee9af06ca1dbc8526d6d7a9c02b06109ef8",
    "title": "Towards a Comprehensive Scaling Law of Mixture-of-Experts",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Guoliang Zhao",
        "Yuhan Fu",
        "Shuaipeng Li",
        "Xingwu Sun",
        "Ruobing Xie",
        "An Wang",
        "Weidong Han",
        "Zhen Yang",
        "Weixuan Sun",
        "Yudong Zhang",
        "Chengzhong Xu",
        "Di Wang",
        "Jie Jiang"
    ],
    "doi": "10.48550/arXiv.2509.23678",
    "arxivId": "2509.23678",
    "url": "https://www.semanticscholar.org/paper/4a7bfee9af06ca1dbc8526d6d7a9c02b06109ef8",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Mixture-of-Experts (MoE) models have become the consensus approach for enabling parameter-efficient scaling and cost-effective deployment in large language models. However, existing scaling laws for dense models are inapplicable to MoE models, which stems from three critical challenges: the multiplicity of influencing factors, their intricate coupling relationships and the non-monotonic nature of their performance impacts. They collectively necessitate a fine-grained investigation into MoE-specific scaling laws. In this work, we perform a systematic decomposition of MoE settings, identifying five key factors that influence model performance from both size and structural perspectives (data size ($D$), total model size ($N$), activated model size ($N_a$), number of active experts ($G$) and the ratio of shared experts ($S$)). Specifically, we design $446$ controlled experiments to characterize their marginal effects, ultimately constructing a comprehensive and precise joint MoE scaling law that considers all essential factors. Furthermore, we derive the theoretically optimal and practically efficiency-aware optimal configurations for $G$, $S$ and $N_a/N$ with detailed analyses. Our results demonstrate that the optimal settings for $G$ and $S$ are independent of both the model architecture and data size. With the scaling of $N$, the optimal activation parameter ratio of $N_a/N$ becomes sparser. Our proposed MoE scaling law could function as an accurate and insightful guidance to facilitate future MoE model design and training.",
    "citationCount": 1,
    "referenceCount": 40
}