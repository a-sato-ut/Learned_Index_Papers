{
    "paperId": "747f32ba57bdecf638a5b3d552255e82bed207c1",
    "title": "Do We Really Need Graph Convolution During Training? Light Post-Training Graph-ODE for Efficient Recommendation",
    "year": 2024,
    "venue": "International Conference on Information and Knowledge Management",
    "authors": [
        "Weizhi Zhang",
        "Liangwei Yang",
        "Zihe Song",
        "Henry Peng Zou",
        "Ke Xu",
        "Liancheng Fang",
        "Philip S. Yu"
    ],
    "doi": "10.1145/3627673.3679773",
    "arxivId": "2407.18910",
    "url": "https://www.semanticscholar.org/paper/747f32ba57bdecf638a5b3d552255e82bed207c1",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2407.18910",
    "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The efficiency and scalability of graph convolution networks (GCNs) in training recommender systems (RecSys) have been persistent concerns, hindering their deployment in real-world applications. This paper presents a critical examination of the necessity of graph convolutions during the training phase and introduces an innovative alternative: the Light Post-Training Graph Ordinary-Differential-Equation (LightGODE). Our investigation reveals that the benefits of GCNs are more pronounced during testing rather than training. Motivated by this, LightGODE utilizes a novel post-training graph convolution method that bypasses the computation-intensive message passing of GCNs and employs a non-parametric continuous graph ordinary-differential-equation (ODE) to dynamically model node representations. This approach drastically reduces training time while achieving fine-grained post-training graph convolution to avoid the distortion of the original training embedding space, termed the embedding discrepancy issue. We validate our model across several real-world datasets of different scales, demonstrating that LightGODE not only outperforms GCN-based models in terms of efficiency and effectiveness but also significantly mitigates the embedding discrepancy commonly associated with deeper graph convolution layers. Our LightGODE challenges the prevailing paradigms in RecSys training and suggests re-evaluating the role of graph convolutions, potentially guiding future developments of efficient large-scale graph-based RecSys.",
    "citationCount": 7,
    "referenceCount": 52
}