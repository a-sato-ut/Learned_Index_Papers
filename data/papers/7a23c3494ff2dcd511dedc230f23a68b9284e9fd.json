{
    "paperId": "7a23c3494ff2dcd511dedc230f23a68b9284e9fd",
    "title": "Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees",
    "year": 2023,
    "venue": "AAAI Conference on Artificial Intelligence",
    "authors": [
        "Luca Marzari",
        "Davide Corsi",
        "Enrico Marchesini",
        "A. Farinelli",
        "F. Cicalese"
    ],
    "doi": "10.48550/arXiv.2308.09842",
    "arxivId": "2308.09842",
    "url": "https://www.semanticscholar.org/paper/7a23c3494ff2dcd511dedc230f23a68b9284e9fd",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2308.09842",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Identifying safe areas is a key point to guarantee trust for systems that are based on Deep Neural Networks (DNNs). To this end, we introduce the AllDNN-Verification problem: given a safety property and a DNN, enumerate the set of all the regions of the property input domain which are safe, i.e., where the property does hold. Due to the #P-hardness of the problem, we propose an efficient approximation method called ε-ProVe. Our approach exploits a controllable underestimation of the output reachable sets obtained via statistical prediction of tolerance limits, and can provide a tight —with provable probabilistic guarantees— lower estimate of the safe areas. Our empirical evaluation on different standard benchmarks shows the scalability and effectiveness of our method, offering valuable insights for this new type of verification of DNNs.",
    "citationCount": 18,
    "referenceCount": 30
}