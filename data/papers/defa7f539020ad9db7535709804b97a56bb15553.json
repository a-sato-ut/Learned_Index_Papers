{
    "paperId": "defa7f539020ad9db7535709804b97a56bb15553",
    "title": "Stochastic Unrolled Federated Learning",
    "year": 2023,
    "venue": "arXiv.org",
    "authors": [
        "Samar Hadou",
        "Navid Naderializadeh",
        "Alejandro Ribeiro"
    ],
    "doi": "10.48550/arXiv.2305.15371",
    "arxivId": "2305.15371",
    "url": "https://www.semanticscholar.org/paper/defa7f539020ad9db7535709804b97a56bb15553",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2305.15371",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Engineering",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Algorithm unrolling has emerged as a learning-based optimization paradigm that unfolds truncated iterative algorithms in trainable neural-network optimizers. We introduce Stochastic UnRolled Federated learning (SURF), a method that expands algorithm unrolling to federated learning in order to expedite its convergence. Our proposed method tackles two challenges of this expansion, namely the need to feed whole datasets to the unrolled optimizers to find a descent direction and the decentralized nature of federated learning. We circumvent the former challenge by feeding stochastic mini-batches to each unrolled layer and imposing descent constraints to guarantee its convergence. We address the latter challenge by unfolding the distributed gradient descent (DGD) algorithm in a graph neural network (GNN)-based unrolled architecture, which preserves the decentralized nature of training in federated learning. We theoretically prove that our proposed unrolled optimizer converges to a near-optimal region infinitely often. Through extensive numerical experiments, we also demonstrate the effectiveness of the proposed framework in collaborative training of image classifiers.",
    "citationCount": 6,
    "referenceCount": 88
}