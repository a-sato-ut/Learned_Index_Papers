{
    "paperId": "0a8c440e593f60a55f0a724afc78747cf51805f4",
    "title": "Curriculum Modeling the Dependence among Targets with Multi-task Learning for Financial Marketing",
    "year": 2023,
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "authors": [
        "Yunpeng Weng",
        "Xing Tang",
        "Liang Chen",
        "Xiuqiang He"
    ],
    "doi": "10.1145/3539618.3591969",
    "arxivId": "2305.01514",
    "url": "https://www.semanticscholar.org/paper/0a8c440e593f60a55f0a724afc78747cf51805f4",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2305.01514",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Business",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Multi-task learning for various real-world applications usually involves tasks with logical sequential dependence. For example, in online marketing, the cascade behavior pattern of impression \\rightarrow click \\rightarrow conversion is usually modeled as multiple tasks in a multi-task manner, where the sequential dependence between tasks is simply connected with an explicitly defined function or implicitly transferred information in current works. These methods alleviate the data sparsity problem for long-path sequential tasks as the positive feedback becomes sparser along with the task sequence. However, the error accumulation and negative transfer will be a severe problem for downstream tasks. Especially, at the beginning stage of training, the optimization for parameters of former tasks is not converged yet, and thus the information transferred to downstream tasks is negative. In this paper, we propose a prior information merged model (PIMM), which explicitly models the logical dependence among tasks with a novel prior information merged (PIM) module for multiple sequential dependence task learning in a curriculum manner. Specifically, the PIM randomly selects the true label information or the prior task prediction with a soft sampling strategy to transfer to the downstream task during the training. Following an easy-to-difficult curriculum paradigm, we dynamically adjust the sampling probability to ensure that the downstream task will get the effective information along with the training. The offline experimental results on both public and product datasets verify that PIMM outperforms state-of-the-art baselines. Moreover, we deploy the PIMM in a large-scale FinTech platform, and the online experiments also demonstrate the effectiveness of PIMM.",
    "citationCount": 3,
    "referenceCount": 23
}