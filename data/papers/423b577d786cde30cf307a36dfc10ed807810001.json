{
    "paperId": "423b577d786cde30cf307a36dfc10ed807810001",
    "title": "Specific-Input LIME Explanations for Tabular Data Based on Deep Learning Models",
    "year": 2023,
    "venue": "Applied Sciences",
    "authors": [
        "Junkang An",
        "Yiwan Zhang",
        "I. Joe"
    ],
    "doi": "10.3390/app13158782",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/423b577d786cde30cf307a36dfc10ed807810001",
    "isOpenAccess": true,
    "openAccessPdf": "https://www.mdpi.com/2076-3417/13/15/8782/pdf?version=1690628056",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Deep learning researchers believe that as deep learning models evolve, they can perform well on many tasks. However, the complex parameters of deep learning models make it difficult for users to understand how deep learning models make predictions. In this paper, we propose the specific-input local interpretable model-agnostic explanations (LIME) model, a novel interpretable artificial intelligence (XAI) method that interprets deep learning models of tabular data. The specific-input process uses feature importance and partial dependency plots (PDPs) to select the “what” and “how”. In our experiments, we first obtain a basic interpretation of the data by simulating user behaviour. Second, we use our approach to understand “which” features deep learning models focus on and how these features affect the model’s predictions. From the experimental results, we find that this approach improves the stability of LIME interpretations, compensates for the problem of LIME only focusing on local interpretations, and achieves a balance between global and local interpretations.",
    "citationCount": 24,
    "referenceCount": 21
}