{
    "paperId": "4ed3ac2d1d6fc0bc02f739ffd79cf924e30b4019",
    "title": "Active Learning for ML Enhanced Database Systems",
    "year": 2020,
    "venue": "SIGMOD Conference",
    "authors": [
        "Lin Ma",
        "B. Ding",
        "Sudipto Das",
        "Adith Swaminathan"
    ],
    "doi": "10.1145/3318464.3389768",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/4ed3ac2d1d6fc0bc02f739ffd79cf924e30b4019",
    "isOpenAccess": true,
    "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3318464.3389768",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Recent research has shown promising results by using machine learning (ML) techniques to improve the performance of database systems, e.g., in query optimization or index recommendation. However, in many production deployments, the ML models' performance degrades significantly when the test data diverges from the data used to train these models. In this paper, we address this performance degradation by using B-instances to collect additional data during deployment. We propose an active data collection platform, ADCP, that employs active learning (AL) to gather relevant data cost-effectively. We develop a novel AL technique, Holistic Active Learner (HAL), that robustly combines multiple noisy signals for data gathering in the context of database applications. HAL applies to various ML tasks, budget sizes, cost types, and budgeting interfaces for database applications. We evaluate ADCP on both industry-standard benchmarks and real customer workloads. Our evaluation shows that, compared with other baselines, our technique improves ML models' prediction performance by up to 2x with the same cost budget. In particular, on production workloads, our technique reduces the prediction error of ML models by 75% using about 100 additionally collected queries.",
    "citationCount": 64,
    "referenceCount": 82
}