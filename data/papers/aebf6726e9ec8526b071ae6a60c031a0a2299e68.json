{
    "paperId": "aebf6726e9ec8526b071ae6a60c031a0a2299e68",
    "title": "Dissimilarity-Preserving Representation Learning for One-Class Time Series Classification",
    "year": 2023,
    "venue": "IEEE Transactions on Neural Networks and Learning Systems",
    "authors": [
        "Stefano Mauceri",
        "James Sweeney",
        "Miguel Nicolau",
        "James McDermott"
    ],
    "doi": "10.1109/TNNLS.2023.3273503",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/aebf6726e9ec8526b071ae6a60c031a0a2299e68",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Medicine",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We propose to embed time series in a latent space where pairwise Euclidean distances (EDs) between samples are equal to pairwise dissimilarities in the original space, for a given dissimilarity measure. To this end, we use auto-encoder (AE) and encoder-only neural networks to learn elastic dissimilarity measures, e.g., dynamic time warping (DTW), that are central to time series classification (Bagnall et al., 2017). The learned representations are used in the context of one-class classification (Mauceri et al., 2020) on the datasets of UCR/UEA archive (Dau et al., 2019). Using a 1-nearest neighbor (1NN) classifier, we show that learned representations allow classification performance that is close to that of raw data, but in a space of substantially lower dimensionality. This implies substantial and compelling savings in terms of computational and storage requirements for nearest neighbor time series classification.",
    "citationCount": 6,
    "referenceCount": 79
}