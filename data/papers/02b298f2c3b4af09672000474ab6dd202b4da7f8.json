{
    "paperId": "02b298f2c3b4af09672000474ab6dd202b4da7f8",
    "title": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Kyungmin Bin",
        "Seungbeom Choi",
        "Jimyoung Son",
        "Jieun Choi",
        "Daseul Bae",
        "Daehyeon Baek",
        "Kihyo Moon",
        "Minsung Jang",
        "Hyojung Lee"
    ],
    "doi": "10.48550/arXiv.2509.06261",
    "arxivId": "2509.06261",
    "url": "https://www.semanticscholar.org/paper/02b298f2c3b4af09672000474ab6dd202b4da7f8",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Recent advances in Post-Training Quantization (PTQ) techniques have significantly increased demand for serving quantized large language models (LLMs), enabling higher throughput and substantially reduced memory usage with minimal accuracy loss. Quantized models address memory constraints in LLMs and enhance GPU resource utilization through efficient GPU sharing. However, quantized models have smaller KV block sizes than non-quantized models, causing limited memory efficiency due to memory fragmentation. Also, distinct resource usage patterns between quantized and non-quantized models require efficient scheduling to maximize throughput. To address these challenges, we propose FineServe, an inference serving framework for mixed-precision LLMs. FineServe's key contributions include: (1) KV Slab, a precision-aware adaptive memory management technique dynamically allocating KV cache based on model quantization characteristics, significantly reducing GPU memory fragmentation, and (2) a two-level scheduling framework comprising a global scheduler that places models to GPUs based on request rates, latency SLOs, and memory constraints and efficiency, and a local scheduler that adaptively adjusts batch sizes according to real-time request fluctuations. Experimental results demonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x higher token generation throughput compared to the state-of-the-art GPU sharing systems.",
    "citationCount": 1,
    "referenceCount": 35
}