{
    "paperId": "d1a2c361bbf5e00fae3b702fe8ee15a5c6cf6eff",
    "title": "Multimodal AutoML via Representation Evolution",
    "year": 2022,
    "venue": "Machine Learning and Knowledge Extraction",
    "authors": [
        "Blaž Škrlj",
        "Matej Bevec",
        "Nadine Lavrac"
    ],
    "doi": "10.3390/make5010001",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/d1a2c361bbf5e00fae3b702fe8ee15a5c6cf6eff",
    "isOpenAccess": true,
    "openAccessPdf": "https://www.mdpi.com/2504-4990/5/1/1/pdf?version=1672134346",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "With the increasing amounts of available data, learning simultaneously from different types of inputs is becoming necessary to obtain robust and well-performing models. With the advent of representation learning in recent years, lower-dimensional vector-based representations have become available for both images and texts, while automating simultaneous learning from multiple modalities remains a challenging problem. This paper presents an AutoML (automated machine learning) approach to automated machine learning model configuration identification for data composed of two modalities: texts and images. The approach is based on the idea of representation evolution, the process of automatically amplifying heterogeneous representations across several modalities, optimized jointly with a collection of fast, well-regularized linear models. The proposed approach is benchmarked against 11 unimodal and multimodal (texts and images) approaches on four real-life benchmark datasets from different domains. It achieves competitive performance with minimal human effort and low computing requirements, enabling learning from multiple modalities in automated manner for a wider community of researchers.",
    "citationCount": 2,
    "referenceCount": 49
}