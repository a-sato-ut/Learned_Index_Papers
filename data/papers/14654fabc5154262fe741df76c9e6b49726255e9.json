{
    "paperId": "14654fabc5154262fe741df76c9e6b49726255e9",
    "title": "GCWSNet: Generalized Consistent Weighted Sampling for Scalable and Accurate Training of Neural Networks",
    "year": 2022,
    "venue": "International Conference on Information and Knowledge Management",
    "authors": [
        "Ping Li",
        "Weijie Zhao"
    ],
    "doi": "10.1145/3511808.3557332",
    "arxivId": "2201.02283",
    "url": "https://www.semanticscholar.org/paper/14654fabc5154262fe741df76c9e6b49726255e9",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We propose using \"powered generalized min-max'' (pGMM) hashed (linearized) via the \"generalized consistent weighted sampling'' (GCWS) for training (deep) neural networks (hence the name \"GCWSNet''). The pGMM and several related kernels were proposed in 2017. We demonstrate that pGMM hashed by GCWS provide a numerically stable scheme for applying power transformation on the original data, regardless of the magnitude of p and the data. Our experiments show that GCWSNet often improves the accuracy. It is also evident that GCWSNet converges substantially faster, reaching reasonable accuracy with merely one epoch of the training process. This property is much desired because many applications, such as advertisement click-through rate (CTR) prediction models, or data streams (i.e., data seen only once), often train just one epoch. Another beneficial side effect is that the computations of the first layer of the neural networks become additions instead of multiplications because the input data become binary and highly sparse. Empirical comparisons with (normalized) random Fourier features (NRFF) are provided. We also propose to reduce the model size of GCWSNet by count-sketch and develop the theory for analyzing the impact of using count-sketch on the accuracy of GCWS. Our analysis shows that an \"8-bit'' strategy should provide the good trade-off between accuracy and model size. There are other ways to take advantage of GCWS. For example, one can apply GCWS on the last layer to boost the accuracy of trained deep neural nets.",
    "citationCount": 11,
    "referenceCount": 63
}