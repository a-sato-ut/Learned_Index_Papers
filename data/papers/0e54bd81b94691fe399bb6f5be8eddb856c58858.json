{
    "paperId": "0e54bd81b94691fe399bb6f5be8eddb856c58858",
    "title": "Efficient Pruning via Entailment Cardinality Estimation for Fast Top-Down Logic Rule Mining",
    "year": 2025,
    "venue": "IEEE International Conference on Data Engineering",
    "authors": [
        "Ruoyu Wang",
        "R. Wong",
        "Daniel Sun"
    ],
    "doi": "10.1109/ICDE65448.2025.00030",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/0e54bd81b94691fe399bb6f5be8eddb856c58858",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The cognitive capabilities of AI rely heavily on inter-pretable approaches, where logic rules and programs have been widely employed. Mining for expressive logic rules, especially in a top-down manner, has been improved over decades. Nevertheless, the efficiency of mining algorithms does not match the volume of target datasets, such as open-domain RDF knowledge bases. Most time and space have been wasted on exploring low-quality candidates in search routines, though various pruning efforts have been made in existing techniques. Estimation of rule quality or confidence has been applied in recent rule mining systems, but the efficiency of such approaches is still limited. In this paper, we propose an efficient pruning method based on entailment cardinality estimation. This approach is derived from a similar-distribution assumption and estimates the order of possible branches in beam search at a low cost. As a result, the number of entailment computations in each iteration of top-down mining has been reduced from a polynomial to a constant. The experimental results show that the rule mining procedure has been accelerated by up to 70x when the target KB is large and the used observation ratio is small. More than 90% of memory consumption has been reduced, while the memory overhead of the estimation technique is less than 1% of the overall memory cost.",
    "citationCount": 0,
    "referenceCount": 46
}