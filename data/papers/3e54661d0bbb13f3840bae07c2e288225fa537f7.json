{
    "paperId": "3e54661d0bbb13f3840bae07c2e288225fa537f7",
    "title": "Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions",
    "year": 2023,
    "venue": "Neural Information Processing Systems",
    "authors": [
        "Tongxin Li",
        "Yiheng Lin",
        "Shaolei Ren",
        "A. Wierman"
    ],
    "doi": "10.48550/arXiv.2307.10524",
    "arxivId": "2307.10524",
    "url": "https://www.semanticscholar.org/paper/3e54661d0bbb13f3840bae07c2e288225fa537f7",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2307.10524",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We study the tradeoff between consistency and robustness in the context of a single-trajectory time-varying Markov Decision Process (MDP) with untrusted machine-learned advice. Our work departs from the typical approach of treating advice as coming from black-box sources by instead considering a setting where additional information about how the advice is generated is available. We prove a first-of-its-kind consistency and robustness tradeoff given Q-value advice under a general MDP model that includes both continuous and discrete state/action spaces. Our results highlight that utilizing Q-value advice enables dynamic pursuit of the better of machine-learned advice and a robust baseline, thus result in near-optimal performance guarantees, which provably improves what can be obtained solely with black-box advice.",
    "citationCount": 10,
    "referenceCount": 69
}