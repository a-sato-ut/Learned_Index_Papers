{
    "paperId": "747f95e945f067ffe937af9a25d7dc97efebc760",
    "title": "Inference-time sparse attention with asymmetric indexing",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Pierre-Emmanuel Mazar'e",
        "Gergely Szilvasy",
        "Maria Lomeli",
        "Francisco Massa",
        "Naila Murray",
        "Herv'e J'egou",
        "Matthijs Douze"
    ],
    "doi": "10.48550/arXiv.2502.08246",
    "arxivId": "2502.08246",
    "url": "https://www.semanticscholar.org/paper/747f95e945f067ffe937af9a25d7dc97efebc760",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Self-attention in transformer models is an incremental associative memory that maps key vectors to value vectors. One way to speed up self-attention is to employ GPU-compatible vector search algorithms based on standard partitioning methods such as k-means. However, such partitioning methods yield poor results in this context because (1) the keys and queries follow different distributions, and (2) the RoPE positional encoding hinders the bucket assignment. This paper introduces Saap (Self-Attention with Asymmetric Partitions), which overcomes these problems. It is an asymmetrical indexing technique that employs distinct partitions for keys and queries, thereby approximating self-attention with a data-adaptive sparsity pattern. It works on pretrained language models and only requires to train (offline) a small query classifier. On a long context Llama 3.1-8b model, with sequences ranging from 100k to 500k tokens, Saap typically reduces by a factor of 20 the fraction of memory that needs to be looked-up, which translates to a time saving of 60\\% when compared to FlashAttention-v2.",
    "citationCount": 4,
    "referenceCount": 57
}