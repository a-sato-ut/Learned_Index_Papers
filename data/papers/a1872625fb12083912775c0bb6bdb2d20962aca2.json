{
    "paperId": "a1872625fb12083912775c0bb6bdb2d20962aca2",
    "title": "LLMComp: A Language Modeling Paradigm for Error-Bounded Scientific Data Compression",
    "year": 2025,
    "venue": "",
    "authors": [
        "Guozhong Li",
        "Muhannad Alhumaidi",
        "Spiros Skiadopoulos",
        "Panos Kalnis"
    ],
    "doi": null,
    "arxivId": "2510.23632",
    "url": "https://www.semanticscholar.org/paper/a1872625fb12083912775c0bb6bdb2d20962aca2",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": null,
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The rapid growth of high-resolution scientific simulations and observation systems is generating massive spatiotemporal datasets, making efficient, error-bounded compression increasingly important. Meanwhile, decoder-only large language models (LLMs) have demonstrated remarkable capabilities in modeling complex sequential data. In this paper, we propose LLMCOMP, a novel lossy compression paradigm that leverages decoder-only large LLMs to model scientific data. LLMCOMP first quantizes 3D fields into discrete tokens, arranges them via Z-order curves to preserve locality, and applies coverage-guided sampling to enhance training efficiency. An autoregressive transformer is then trained with spatial-temporal embeddings to model token transitions. During compression, the model performs top-k prediction, storing only rank indices and fallback corrections to ensure strict error bounds. Experiments on multiple reanalysis datasets show that LLMCOMP consistently outperforms state-of-the-art compressors, achieving up to 30% higher compression ratios under strict error bounds. These results highlight the potential of LLMs as general-purpose compressors for high-fidelity scientific data.",
    "citationCount": 0,
    "referenceCount": 45
}