{
    "paperId": "70cb62dc393d3882a371113a13f11027713e6064",
    "title": "Adaptive Learned Bloom Filters under Incremental Workloads",
    "year": 2020,
    "venue": "COMAD/CODS",
    "authors": [
        "Arindam Bhattacharya",
        "Srikanta J. Bedathur",
        "A. Bagchi"
    ],
    "doi": "10.1145/3371158.3371171",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/70cb62dc393d3882a371113a13f11027713e6064",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Book",
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The recently proposed paradigm of learned Bloom filters (LBF) seems to offer significant advantages over traditional Bloom filters in terms of low memory footprint and overall performance as evidenced by empirical evaluations over static data. Its behavior in presence of updates to the set of keys being stored in Bloom filters is not very well understood. At the same time, maintaining the false positive rates (FPR) of traditional Bloom filters in presence of dynamics has been studied and extensions to carefully expand memory footprint of the filters without sacrificing FPR have been proposed. Building on these, we propose two distinct approaches for handling data updates encountered in practical uses of LBF: (i) CA-LBF, where we adjust the learned model (e.g., by retraining) to accommodate the new \"unseen\" data, resulting in classifier adaptive methods, and (ii) IA-LBF, where we replace the traditional Bloom filter with its adaptive version while keeping the learned model unchanged, leading to an index adaptive method. In this paper, we explore these two approaches in detail under incremental workloads, evaluating them in terms of their adaptability, memory footprint and false positive rates. Our empirical results using a variety of datasets and learned models of varying complexity show that our proposed methods' ability to handle incremental updates is quite robust.",
    "citationCount": 8,
    "referenceCount": 19
}