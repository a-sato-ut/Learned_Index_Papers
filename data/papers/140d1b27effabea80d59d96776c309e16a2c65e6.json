{
    "paperId": "140d1b27effabea80d59d96776c309e16a2c65e6",
    "title": "GPU-based Private Information Retrieval for On-Device Machine Learning Inference",
    "year": 2023,
    "venue": "International Conference on Architectural Support for Programming Languages and Operating Systems",
    "authors": [
        "Maximilian Lam",
        "Jeff Johnson",
        "Wenjie Xiong",
        "Kiwan Maeng",
        "Udit Gupta",
        "Yang Li",
        "Liangzhen Lai",
        "Ilias Leontiadis",
        "Minsoo Rhu",
        "Hsien-Hsin S. Lee",
        "V. Reddi",
        "Gu-Yeon Wei",
        "David Brooks",
        "Edward Suh"
    ],
    "doi": "10.1145/3617232.3624855",
    "arxivId": "2301.10904",
    "url": "https://www.semanticscholar.org/paper/140d1b27effabea80d59d96776c309e16a2c65e6",
    "isOpenAccess": true,
    "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3617232.3624855",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "On-device machine learning (ML) inference can enable the use of private user data on user devices without revealing them to remote servers. However, a pure on-device solution to private ML inference is impractical for many applications that rely on embedding tables that are too large to be stored on-device. In particular, recommendation models typically use multiple embedding tables each on the order of 1--10 GBs of data, making them impractical to store on-device. To overcome this barrier, we propose the use of private information retrieval (PIR) to efficiently and privately retrieve embeddings from servers without sharing any private information. As off-the-shelf PIR algorithms are usually too computationally intensive to directly use for latency-sensitive inference tasks, we 1) propose novel GPU-based acceleration of PIR, and 2) co-design PIR with the downstream ML application to obtain further speedup. Our GPU acceleration strategy improves system throughput by more than 20× over an optimized CPU PIR implementation, and our PIR-ML co-design provides an over 5× additional throughput improvement at fixed model quality. Together, for various on-device ML applications such as recommendation and language modeling, our system on a single V100 GPU can serve up to 100,000 queries per second---a > 100× throughput improvement over a CPU-based baseline---while maintaining model accuracy.",
    "citationCount": 13,
    "referenceCount": 108
}