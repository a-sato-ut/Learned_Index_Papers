{
    "paperId": "5c27b35f69e783caba3086890ee5f324e0f8fef0",
    "title": "Parsimonious Learning-Augmented Approximations for Dense Instances of NP-hard Problems",
    "year": 2024,
    "venue": "International Conference on Machine Learning",
    "authors": [
        "E. Bampis",
        "B. Escoffier",
        "Michalis Xefteris"
    ],
    "doi": "10.48550/arXiv.2402.02062",
    "arxivId": "2402.02062",
    "url": "https://www.semanticscholar.org/paper/5c27b35f69e783caba3086890ee5f324e0f8fef0",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The classical work of (Arora et al., 1999) provides a scheme that gives, for any $\\epsilon>0$, a polynomial time $1-\\epsilon$ approximation algorithm for dense instances of a family of $\\mathcal{NP}$-hard problems, such as Max-CUT and Max-$k$-SAT. In this paper we extend and speed up this scheme using a logarithmic number of one-bit predictions. We propose a learning augmented framework which aims at finding fast algorithms which guarantees approximation consistency, smoothness and robustness with respect to the prediction error. We provide such algorithms, which moreover use predictions parsimoniously, for dense instances of various optimization problems.",
    "citationCount": 4,
    "referenceCount": 43
}