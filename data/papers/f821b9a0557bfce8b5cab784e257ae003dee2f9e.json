{
    "paperId": "f821b9a0557bfce8b5cab784e257ae003dee2f9e",
    "title": "Private Distribution Learning with Public Data: The View from Sample Compression",
    "year": 2023,
    "venue": "Neural Information Processing Systems",
    "authors": [
        "Shai Ben-David",
        "A. Bie",
        "C. Canonne",
        "Gautam Kamath",
        "Vikrant Singhal"
    ],
    "doi": "10.48550/arXiv.2308.06239",
    "arxivId": "2308.06239",
    "url": "https://www.semanticscholar.org/paper/f821b9a0557bfce8b5cab784e257ae003dee2f9e",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2308.06239",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We study the problem of private distribution learning with access to public data. In this setup, which we refer to as public-private learning, the learner is given public and private samples drawn from an unknown distribution $p$ belonging to a class $\\mathcal Q$, with the goal of outputting an estimate of $p$ while adhering to privacy constraints (here, pure differential privacy) only with respect to the private samples. We show that the public-private learnability of a class $\\mathcal Q$ is connected to the existence of a sample compression scheme for $\\mathcal Q$, as well as to an intermediate notion we refer to as list learning. Leveraging this connection: (1) approximately recovers previous results on Gaussians over $\\mathbb R^d$; and (2) leads to new ones, including sample complexity upper bounds for arbitrary $k$-mixtures of Gaussians over $\\mathbb R^d$, results for agnostic and distribution-shift resistant learners, as well as closure properties for public-private learnability under taking mixtures and products of distributions. Finally, via the connection to list learning, we show that for Gaussians in $\\mathbb R^d$, at least $d$ public samples are necessary for private learnability, which is close to the known upper bound of $d+1$ public samples.",
    "citationCount": 16,
    "referenceCount": 81
}