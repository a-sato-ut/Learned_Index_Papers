{
    "paperId": "51601d554d04c89ae71efe1e66c1d67ef44bec71",
    "title": "A Closer Look at Reward Decomposition for High-Level Robotic Explanations",
    "year": 2023,
    "venue": "International Conference on Development and Learning",
    "authors": [
        "Wenhao Lu",
        "S. Magg",
        "Xufeng Zhao",
        "M. Gromniak",
        "Stefan Wermter"
    ],
    "doi": "10.1109/ICDL55364.2023.10364407",
    "arxivId": "2304.12958",
    "url": "https://www.semanticscholar.org/paper/51601d554d04c89ae71efe1e66c1d67ef44bec71",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2304.12958",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Explaining the behaviour of intelligent agents learned by reinforcement learning (RL) to humans is challenging yet crucial due to their incomprehensible proprioceptive states, variational intermediate goals, and resultant unpredictability. Moreover, one-step explanations for RL agents can be ambiguous as they fail to account for the agent's future behaviour at each transition, adding to the complexity of explaining robot actions. By leveraging abstracted actions that map to task-specific primitives, we avoid explanations on the movement level. To further improve the transparency and explainability of robotic systems, we propose an explainable Q-Map learning framework that combines reward decomposition (RD) with abstracted action spaces, allowing for non-ambiguous and high-level explanations based on object properties in the task. We demonstrate the effectiveness of our framework through quantitative and qualitative analysis of two robotic scenarios, showcasing visual and textual explanations, from output artefacts of RD explanations, that are easy for humans to comprehend. Additionally, we demonstrate the versatility of integrating these artefacts with large language models (LLMs) for reasoning and interactive querying.",
    "citationCount": 9,
    "referenceCount": 48
}