{
    "paperId": "2bb383228065a17cbb3de273d2572e2097115612",
    "title": "Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints",
    "year": 2022,
    "venue": "International Conference on Learning Representations",
    "authors": [
        "D. Mguni",
        "Aivar Sootla",
        "Juliusz Ziomek",
        "Oliver Slumbers",
        "Zipeng Dai",
        "Kun Shao",
        "J. Wang"
    ],
    "doi": "10.48550/arXiv.2205.15953",
    "arxivId": "2205.15953",
    "url": "https://www.semanticscholar.org/paper/2bb383228065a17cbb3de273d2572e2097115612",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2205.15953",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Many real-world settings involve costs for performing actions; transaction costs in financial systems and fuel costs being common examples. In these settings, performing actions at each time step quickly accumulates costs leading to vastly suboptimal outcomes. Additionally, repeatedly acting produces wear and tear and ultimately, damage. Determining \\textit{when to act} is crucial for achieving successful outcomes and yet, the challenge of efficiently \\textit{learning} to behave optimally when actions incur minimally bounded costs remains unresolved. In this paper, we introduce a reinforcement learning (RL) framework named \\textbf{L}earnable \\textbf{I}mpulse \\textbf{C}ontrol \\textbf{R}einforcement \\textbf{A}lgorithm (LICRA), for learning to optimally select both when to act and which actions to take when actions incur costs. At the core of LICRA is a nested structure that combines RL and a form of policy known as \\textit{impulse control} which learns to maximise objectives when actions incur costs. We prove that LICRA, which seamlessly adopts any RL method, converges to policies that optimally select when to perform actions and their optimal magnitudes. We then augment LICRA to handle problems in which the agent can perform at most $k<\\infty$ actions and more generally, faces a budget constraint. We show LICRA learns the optimal value function and ensures budget constraints are satisfied almost surely. We demonstrate empirically LICRA's superior performance against benchmark RL methods in OpenAI gym's \\textit{Lunar Lander} and in \\textit{Highway} environments and a variant of the Merton portfolio problem within finance.",
    "citationCount": 8,
    "referenceCount": 41
}