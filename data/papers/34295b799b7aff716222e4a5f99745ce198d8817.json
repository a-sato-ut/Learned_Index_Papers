{
    "paperId": "34295b799b7aff716222e4a5f99745ce198d8817",
    "title": "A Sublinear Adversarial Training Algorithm",
    "year": 2022,
    "venue": "International Conference on Learning Representations",
    "authors": [
        "Yeqi Gao",
        "Lianke Qin",
        "Zhao Song",
        "Yitan Wang"
    ],
    "doi": "10.48550/arXiv.2208.05395",
    "arxivId": "2208.05395",
    "url": "https://www.semanticscholar.org/paper/34295b799b7aff716222e4a5f99745ce198d8817",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2208.05395",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Adversarial training is a widely used strategy for making neural networks resistant to adversarial perturbations. For a neural network of width $m$, $n$ input training data in $d$ dimension, it takes $\\Omega(mnd)$ time cost per training iteration for the forward and backward computation. In this paper we analyze the convergence guarantee of adversarial training procedure on a two-layer neural network with shifted ReLU activation, and shows that only $o(m)$ neurons will be activated for each input data per iteration. Furthermore, we develop an algorithm for adversarial training with time cost $o(m n d)$ per iteration by applying half-space reporting data structure.",
    "citationCount": 26,
    "referenceCount": 48
}