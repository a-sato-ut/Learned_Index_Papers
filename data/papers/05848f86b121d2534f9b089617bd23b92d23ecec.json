{
    "paperId": "05848f86b121d2534f9b089617bd23b92d23ecec",
    "title": "Reinforcement Learning Interpretation Methods: A Survey",
    "year": 2020,
    "venue": "IEEE Access",
    "authors": [
        "Alnour Alharin",
        "Thanh-Nam Doan",
        "Mina Sartipi"
    ],
    "doi": "10.1109/ACCESS.2020.3023394",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/05848f86b121d2534f9b089617bd23b92d23ecec",
    "isOpenAccess": true,
    "openAccessPdf": "https://doi.org/10.1109/access.2020.3023394",
    "publicationTypes": [
        "JournalArticle",
        "Review"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Medicine",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Reinforcement Learning (RL) systems achieved outstanding performance in different domains such as Atari games, finance, healthcare, and self-driving cars. However, their black-box nature complicates their use, especially in critical applications such as healthcare. To solve this problem, researchers have proposed different approaches to interpret RL models. Some of these methods were adopted from machine learning, while others were designed specifically for RL. The main objective of this paper is to show and explain RL interpretation methods, the metrics used to classify them, and how these metrics were applied to understand the internal details of RL models. We reviewed papers that propose new RL interpretation methods, improve the old ones, or discuss the pros and cons of the existing methods.",
    "citationCount": 63,
    "referenceCount": 103
}