{
    "paperId": "06fafd17e917a6208a928681b28fa9dd3fa865e3",
    "title": "Interpretable Deep Reinforcement Learning for Green Security Games with Real-Time Information",
    "year": 2022,
    "venue": "arXiv.org",
    "authors": [
        "V. Sharma",
        "John P. Dickerson",
        "Pratap Tokekar"
    ],
    "doi": "10.48550/arXiv.2211.04987",
    "arxivId": "2211.04987",
    "url": "https://www.semanticscholar.org/paper/06fafd17e917a6208a928681b28fa9dd3fa865e3",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2211.04987",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Environmental Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Green Security Games with real-time information (GSG-I) add the real-time information about the agents' movement to the typical GSG formulation. Prior works on GSG-I have used deep reinforcement learning (DRL) to learn the best policy for the agent in such an environment without any need to store the huge number of state representations for GSG-I. However, the decision-making process of DRL methods is largely opaque, which results in a lack of trust in their predictions. To tackle this issue, we present an interpretable DRL method for GSG-I that generates visualization to explain the decisions taken by the DRL algorithm. We also show that this approach performs better and works well with a simpler training regimen compared to the existing method.",
    "citationCount": 0,
    "referenceCount": 28
}