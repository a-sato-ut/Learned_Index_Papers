{
    "paperId": "af481e05fcbec32043f06ced51d2c89e68cf524a",
    "title": "Agile Autotuning of a Transprecision Tensor Accelerator Overlay for TVM Compiler Stack",
    "year": 2020,
    "venue": "International Conference on Field-Programmable Logic and Applications",
    "authors": [
        "D. Diamantopoulos",
        "Burkhard Ringlein",
        "M. Purandare",
        "Gagandeep Singh",
        "C. Hagleitner"
    ],
    "doi": "10.1109/FPL50879.2020.00058",
    "arxivId": "2004.10854",
    "url": "https://www.semanticscholar.org/paper/af481e05fcbec32043f06ced51d2c89e68cf524a",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2004.10854",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Specialized accelerators for tensor-operations, such as blocked-matrix operations and multi-dimensional convolutions, have emerged as powerful architecture choices for high-performance Deep-Learning computing. The rapid development of frameworks, models, and precision options challenges the adaptability of such tensor-accelerators since the adaptation to new requirements incurs significant engineering costs. Programmable tensor accelerators offer a promising alternative by allowing reconfiguration of a virtual architecture that overlays on top of the physical FPGA configurable fabric. We propose an overlay (Ï„-VTA) and an optimization method guided by agile-inspired auto-tuning techniques. We achieve higher performance of up to 2.5x and faster convergence of up to 8.1x.",
    "citationCount": 8,
    "referenceCount": 33
}