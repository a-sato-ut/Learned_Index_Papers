{
    "paperId": "14bd57e87b014b4e81290f5863622b87977111ae",
    "title": "Cache Management for Mixture-of-Experts LLMs",
    "year": 2025,
    "venue": "European Conference on Parallel Processing",
    "authors": [
        "Spyros Angelopoulos",
        "L. Marchal",
        "Adrien Obrecht",
        "Bertrand Simon"
    ],
    "doi": "10.1007/978-3-031-99872-0_2",
    "arxivId": "2509.02408",
    "url": "https://www.semanticscholar.org/paper/14bd57e87b014b4e81290f5863622b87977111ae",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a variety of tasks. One of the main challenges towards the successful deployment of LLMs is memory management, since they typically involve billions of parameters. To this end, architectures based on Mixture-of-Experts have been proposed, which aim to reduce the size of the parameters that are activated when producing a token. This raises the equally critical issue of efficiently managing the limited cache of the system, in that frequently used experts should be stored in the fast cache rather than in the slower secondary memory. In this work, we introduce and study a new paging problem that models expert management optimization. Our formulation captures both the layered architecture of LLMs and the requirement that experts are cached efficiently. We first present lower bounds on the competitive ratio of both deterministic and randomized algorithms, which show that under mild assumptions, LRU-like policies have good theoretical competitive performance. We then propose a layer-based extension of LRU that is tailored to the problem at hand. Extensive simulations on both synthetic datasets and actual traces of MoE usage show that our algorithm outperforms policies for the classic paging problem, such as the standard LRU.",
    "citationCount": 0,
    "referenceCount": 28
}