{
    "paperId": "6c80e042dff64ad6fba6df856cd18e9cc6aa658a",
    "title": "Reinforcement Learning for Adaptive Mesh Refinement",
    "year": 2021,
    "venue": "International Conference on Artificial Intelligence and Statistics",
    "authors": [
        "Jiachen Yang",
        "T. Dzanic",
        "Brenden K. Petersen",
        "Junpei Kudo",
        "K. Mittal",
        "V. Tomov",
        "Jean-Sylvain Camier",
        "T. Zhao",
        "H. Zha",
        "T. Kolev",
        "Robert W. Anderson",
        "Daniel M. Faissol"
    ],
    "doi": null,
    "arxivId": "2103.01342",
    "url": "https://www.semanticscholar.org/paper/6c80e042dff64ad6fba6df856cd18e9cc6aa658a",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Large-scale finite element simulations of complex physical systems governed by partial differential equations (PDE) crucially depend on adaptive mesh refinement (AMR) to allocate computational budget to regions where higher resolution is required. Existing scalable AMR methods make heuristic refinement decisions based on instantaneous error estimation and thus do not aim for long-term optimality over an entire simulation. We propose a novel formulation of AMR as a Markov decision process and apply deep reinforcement learning (RL) to train refinement policies directly from simulation. AMR poses a new problem for RL as both the state dimension and available action set changes at every step, which we solve by proposing new policy architectures with differing generality and inductive bias. The model sizes of these policy architectures are independent of the mesh size and hence can be deployed on larger simulations than those used at train time. We demonstrate in comprehensive experiments on static function estimation and time-dependent equations that RL policies can be trained on problems without using ground truth solutions, are competitive with a widely-used error estimator, and generalize to larger, more complex, and unseen test problems.",
    "citationCount": 53,
    "referenceCount": 59
}