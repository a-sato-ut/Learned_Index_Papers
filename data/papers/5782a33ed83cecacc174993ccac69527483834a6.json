{
    "paperId": "5782a33ed83cecacc174993ccac69527483834a6",
    "title": "Algorithm-Hardware Co-Design for Ultra-Low-Power Large Language Models",
    "year": 2025,
    "venue": "International Symposium on Circuits and Systems",
    "authors": [
        "Steven Abreu",
        "Jason Eshraghian"
    ],
    "doi": "10.1109/ISCAS56072.2025.11043353",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/5782a33ed83cecacc174993ccac69527483834a6",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Review"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated unprecedented capabilities in language understanding and generation, yet their significant computational requirements pose substantial challenges for scalability and environmental sustainability. In this introductory review, we examine the algorithm-hardware co-design strategies necessary for developing ultra-low-power LLMs. We begin by reviewing model-level approaches– efficient architectures, quantization, sparsity, distillation–that reduce parameter count and memory movement. We discuss hardware-centric innovations, including event-driven neuromor-phic accelerators, near-memory computing paradigms, and spe-cialized number formats, illustrating how these platforms leverage compressed models for substantial gains in energy efficiency. Finally, we address emerging frontiers beyond digital systems that may further reduce power consumption. We offer a blueprint for enabling low-power LLMs, emphasizing the necessity of cross-disciplinary collaboration for efficient AI at scale.",
    "citationCount": 0,
    "referenceCount": 64
}