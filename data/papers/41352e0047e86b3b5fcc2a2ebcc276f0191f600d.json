{
    "paperId": "41352e0047e86b3b5fcc2a2ebcc276f0191f600d",
    "title": "Optimizing Inference Quality with SmartNIC for Recommendation System",
    "year": 2024,
    "venue": "International Workshop on Quality of Service",
    "authors": [
        "Ruixin Shi",
        "Ming Yan",
        "Jie Wu"
    ],
    "doi": "10.1109/IWQoS61813.2024.10682873",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/41352e0047e86b3b5fcc2a2ebcc276f0191f600d",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Embedding-based recommendation systems are now widely used to recommend content for users, and have strict requirements on their latency and throughput. However, the latest recommendation models often exceed GPU HBM memory capacity, and the system is often deployed separately on computing nodes for GPU calculating and Parameter Servers for embedding tablesâ€™ storage. This architecture leads to a significant amount of network I/O during the inference process and reduces GPU utilization.In this paper, we propose SmartEmb, an inference framework that accelerates the network I/O of embedding table lookups through a specialized control plane of task reordering, prefetching and cache management. We offload these control planes on SmartNIC to avoid contention with the host CPU and gain better performance. We implemented the SmartEmb prototype on BlueField-2 and evaluated its performance. Our evaluation demonstrates that compared to the Nvidia HugeCTR HPS, SmartEmb can improve the quality of service by achieving up to 217% improvement in throughput and reducing latency by up to 190% of overall embedding layer look-ups in inference scenarios.",
    "citationCount": 1,
    "referenceCount": 29
}