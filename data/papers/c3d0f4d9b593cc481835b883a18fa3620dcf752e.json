{
    "paperId": "c3d0f4d9b593cc481835b883a18fa3620dcf752e",
    "title": "ALT: Optimizing Tensor Compilation in Deep Learning Compilers with Active Learning",
    "year": 2020,
    "venue": "ICCD",
    "authors": [
        "Xi Zeng",
        "Tian Zhi",
        "Zidong Du",
        "Qi Guo",
        "Ninghui Sun",
        "Yunji Chen"
    ],
    "doi": "10.1109/ICCD50377.2020.00108",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/c3d0f4d9b593cc481835b883a18fa3620dcf752e",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Deep learning compilers serve as the central role of scheduling neural network execution. State-of-the-art method of tensor compilation in deep learning compilers requires a long time tuning, which greatly hinders the model's deployment. In this paper, we propose ALT, an active learning tuning method for tensor computation compilation. ALT leverages a sampling strategy based on active learning to find more informative samples to be labeled. The sampling strategy is performed by an active learning exploration module which mainly consists of an uncertainty predictor, which predicts uncertainty of unseen samples, and a score predictor which evaluates the current performance of the whole method. We design a novel ping-pang way of iteration between the score predictor and the uncertainty predictor. Experiments on real workloads show that ALT helps to achieve 1.93 × - 2.49 × time reduction to obtain the optimal schedule compared to state-of-the-art deep learning compilers. When tuning under the same time budget, the end to end inference time of a set of neural networks can be improved by 1.04× - 1.07×.",
    "citationCount": 9,
    "referenceCount": 42
}