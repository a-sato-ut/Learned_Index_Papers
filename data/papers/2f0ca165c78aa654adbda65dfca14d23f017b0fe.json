{
    "paperId": "2f0ca165c78aa654adbda65dfca14d23f017b0fe",
    "title": "Gensor: A Graph-Based Construction Tensor Compilation Method for Deep Learning",
    "year": 2025,
    "venue": "IEEE International Parallel and Distributed Processing Symposium",
    "authors": [
        "Hangda Liu",
        "Boyu Diao",
        "Yu Yang",
        "Wenxin Chen",
        "Xiaohui Peng",
        "Yongjun Xu"
    ],
    "doi": "10.1109/IPDPS64566.2025.00056",
    "arxivId": "2502.11407",
    "url": "https://www.semanticscholar.org/paper/2f0ca165c78aa654adbda65dfca14d23f017b0fe",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "High-performance deep learning depends on efficient tensor programs. In recent years, automatic tensor program optimization, also known as tensor compilation, has emerged as the primary approach to generating efficient tensor programs. However, how to generate kernels with higher performance in a shorter time is still the key challenge. In this paper, we present Gensor, a graph-based construction tensor compilation method for deep learning, to further improve the performance of construction tensor compilation. Unlike existing tree-based methods, Gensor abstracts construction space into a graph structure. Gensor then explores the construction space with Markov analysis. Gensor takes tensor programs as states and models scheduling primitives as transition actions between these states. Therefore, the process of tensor program construction optimization is abstracted as a graph traversal process. This approach expands the optimization space, improving operator performance while ensuring rapid optimization. Extensive experiments with typical operators demonstrate that Gensor significantly outperforms the state-of-the-art methods on GPUs for both cloud servers and edge devices. As a result, Gensor can generate operator kernels in seconds, with performance increasing by 18 % on average, reaching a maximum of 30 %. It also achieves high speedup for end-to-end models like ResNet50 and GPT-2, with an average acceleration of 20 %.",
    "citationCount": 1,
    "referenceCount": 41
}