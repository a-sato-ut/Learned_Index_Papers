{
    "paperId": "2adceb9c54d119540b70e300d17efd65c268a60f",
    "title": "DualNetIQ: Texture-Insensitive Image Quality Assessment with Dual Multi-Scale Feature Maps",
    "year": 2025,
    "venue": "Electronics",
    "authors": [
        "Adel Agamy",
        "Hossam Mady",
        "Hamada Esmaeil",
        "Abdulrahman Al Ayidh",
        "Abdelmageed Mohamed Aly",
        "Mohamed Abdel-Nasser"
    ],
    "doi": "10.3390/electronics14061169",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/2adceb9c54d119540b70e300d17efd65c268a60f",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The precise assessment of image quality that matches human perception is still a major challenge in the field of digital imaging. Digital images play a crucial role in many technological and media applications. The existing deep convolutional neural network (CNN)-based image quality assessment (IQA) methods have advanced considerably, but there remains a critical need to improve the performance of existing methods while maintaining explicit tolerance to visual texture resampling and texture similarity. This paper introduces DualNetIQ, a novel full-reference IQA method that leverages the strengths of deep learning architectures to exhibit robustness against resampling effects on visual textures. DualNetIQ includes two main stages: feature extraction from the reference and distorted images, and similarity measurement based on combining global texture and structure similarity metrics. In particular, DualNetIQ takes features from input images using a group of hybrid pre-trained multi-scale feature maps carefully chosen from VGG19 and SqueezeNet pre-trained CNN models to find differences in texture and structure between the reference image and the distorted image. The Grey Wolf Optimizer (GWO) calculates the weighted combination of global texture and structure similarity metrics to assess the similarity between reference and distorted images. The unique advantage of the proposed method is that it does not require training or fine-tuning the CNN deep learning model. Comprehensive experiments and comparisons on five databases, including various distortion types, demonstrate the superiority of the proposed method over state-of-the-art models, particularly in image quality prediction and texture similarity tasks.",
    "citationCount": 0,
    "referenceCount": 23
}