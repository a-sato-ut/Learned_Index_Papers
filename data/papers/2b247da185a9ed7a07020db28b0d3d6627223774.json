{
    "paperId": "2b247da185a9ed7a07020db28b0d3d6627223774",
    "title": "Improved Contrastive Divergence Training of Energy Based Models",
    "year": 2020,
    "venue": "International Conference on Machine Learning",
    "authors": [
        "Yilun Du",
        "Shuang Li",
        "J. Tenenbaum",
        "Igor Mordatch"
    ],
    "doi": null,
    "arxivId": "2012.01316",
    "url": "https://www.semanticscholar.org/paper/2b247da185a9ed7a07020db28b0d3d6627223774",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We propose several different techniques to improve contrastive divergence training of energy-based models (EBMs). We first show that a gradient term neglected in the popular contrastive divergence formulation is both tractable to estimate and is important to avoid training instabilities in previous models. We further highlight how data augmentation, multi-scale processing, and reservoir sampling can be used to improve model robustness and generation quality. Thirdly, we empirically evaluate stability of model architectures and show improved performance on a host of benchmarks and use cases, such as image generation, OOD detection, and compositional generation.",
    "citationCount": 157,
    "referenceCount": 75
}