{
    "paperId": "8dcd6fb14077a738592bb3a19ba54e5b1f04f47c",
    "title": "Compiling Discrete Probabilistic Programs for Vectorized Exact Inference",
    "year": 2023,
    "venue": "International Conference on Compiler Construction",
    "authors": [
        "Jingwen Pan",
        "Amir Shaikhha"
    ],
    "doi": "10.1145/3578360.3580258",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/8dcd6fb14077a738592bb3a19ba54e5b1f04f47c",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Probabilistic programming languages (PPLs) are essential for reasoning under uncertainty. Even though many real-world probabilistic programs involve discrete distributions, the state-of-the-art PPLs are suboptimal for a large class of tasks dealing with such distributions. In this paper, we propose BayesTensor, a tensor-based probabilistic programming framework. By generating tensor algebra code from probabilistic programs, BayesTensor takes advantage of the highly-tuned vectorized implementations of tensor processing frameworks. Our experiments show that BayesTensor outperforms the state-of-the-art frameworks in a variety of discrete probabilistic programs, inference over Bayesian Networks, and real-world probabilistic programs employed in data processing systems.",
    "citationCount": 3,
    "referenceCount": 59
}