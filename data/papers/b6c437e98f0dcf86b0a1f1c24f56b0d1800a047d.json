{
    "paperId": "b6c437e98f0dcf86b0a1f1c24f56b0d1800a047d",
    "title": "Merlin HugeCTR: GPU-accelerated Recommender System Training and Inference",
    "year": 2022,
    "venue": "ACM Conference on Recommender Systems",
    "authors": [
        "Zehuan Wang",
        "Yingcan Wei",
        "Minseok Lee",
        "Matthias Langer",
        "F. Yu",
        "Jie Liu",
        "Shijie Liu",
        "Daniel G. Abel",
        "Xu Guo",
        "Jianbing Dong",
        "Ji Shi",
        "Kunlun Li"
    ],
    "doi": "10.1145/3523227.3547405",
    "arxivId": "2210.08803",
    "url": "https://www.semanticscholar.org/paper/b6c437e98f0dcf86b0a1f1c24f56b0d1800a047d",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2210.08803",
    "publicationTypes": [
        "JournalArticle",
        "Book"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "In this talk, we introduce Merlin HugeCTR. Merlin HugeCTR is an open source, GPU-accelerated integration framework for click-through rate estimation. It optimizes both training and inference, whilst enabling model training at scale with model-parallel embeddings and data-parallel neural networks. In particular, Merlin HugeCTR combines a high-performance GPU embedding cache with an hierarchical storage architecture, to realize low-latency retrieval of embeddings for online model inference tasks. In the MLPerf v1.0 DLRM model training benchmark, Merlin HugeCTR achieves a speedup of up to 24.6x on a single DGX A100 (8x A100) over PyTorch on 4x4-socket CPU nodes (4x4x28 cores). Merlin HugeCTR can also take advantage of multi-node environments to accelerate training even further. Since late 2021, Merlin HugeCTR additionally features a hierarchical parameter server (HPS) and supports deployment via the NVIDIA Triton server framework, to leverage the computational capabilities of GPUs for high-speed recommendation model inference. Using this HPS, Merlin HugeCTR users can achieve a 5~62x speedup (batch size dependent) for popular recommendation models over CPU baseline implementations, and dramatically reduce their end-to-end inference latency.",
    "citationCount": 37,
    "referenceCount": 26
}