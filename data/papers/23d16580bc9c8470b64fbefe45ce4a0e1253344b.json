{
    "paperId": "23d16580bc9c8470b64fbefe45ce4a0e1253344b",
    "title": "Simulation Analysis of Multi-Objective Functions in Mobile Robot\n Navigation Based on Enhanced Deep Q-Network Algorithm",
    "year": 2024,
    "venue": "SAE technical paper series",
    "authors": [
        "Vengatesan Arumugam",
        "Vasudevan Alagumalai",
        "Sundarakannan Rajendran"
    ],
    "doi": "10.4271/2024-01-5110",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/23d16580bc9c8470b64fbefe45ce4a0e1253344b",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": null,
    "s2FieldsOfStudy": [
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "This research, path planning optimization of the deep Q-network (DQN) algorithm\n is enhanced through integration with the enhanced deep Q-network (EDQN) for\n mobile robot (MR) navigation in specific scenarios. This approach involves\n multiple objectives, such as minimizing path distance, energy consumption, and\n obstacle avoidance. The proposed algorithm has been adapted to operate MRs in\n both 10 × 10 and 15 × 15 grid-mapped environments, accommodating both static and\n dynamic settings. The main objective of the algorithm is to determine the most\n efficient, optimized path to the target destination. A learning-based MR was\n utilized to experimentally validate the EDQN methodology, confirming its\n effectiveness. For robot trajectory tasks, this research demonstrates that the\n EDQN approach enables collision avoidance, optimizes path efficiency, and\n achieves practical applicability. Training episodes were implemented over 3000\n iterations. In comparison to traditional algorithms such as A*, GA, and ACO, as\n well as deep learning algorithms (IDQN and D3QN), the simulation and real-time\n experimental results showed improved performance in both static and dynamic\n environments. The results indicated a travel time reduction to 9 s, a 14.6%\n decrease in total path distance, and a training duration reduction of 1657\n iterations compared to IDQN and D3QN.",
    "citationCount": 0,
    "referenceCount": 18
}