{
    "paperId": "da2927ba66ace7150e939b062aa1e2aa9f1bb2eb",
    "title": "Why Prototypes Collapse: Diagnosing and Preventing Partial Collapse in Prototypical Self-Supervised Learning",
    "year": 2025,
    "venue": "",
    "authors": [
        "Gabriel Y. Arteaga",
        "Marius Aasan",
        "Rwiddhi Chakraborty",
        "Martine Hjelkrem-Tan",
        "Thalles Silva",
        "Michael Kampffmeyer",
        "Ad'in Ram'irez Rivera"
    ],
    "doi": null,
    "arxivId": "2510.20108",
    "url": "https://www.semanticscholar.org/paper/da2927ba66ace7150e939b062aa1e2aa9f1bb2eb",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": null,
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Prototypical self-supervised learning methods consistently suffer from partial prototype collapse, where multiple prototypes converge to nearly identical representations. This undermines their central purpose -- providing diverse and informative targets to guide encoders toward rich representations -- and has led practitioners to over-parameterize prototype sets or add ad-hoc regularizers, which mitigate symptoms rather than address the root cause. We empirically trace the collapse to the joint optimization of encoders and prototypes, which encourages a type of shortcut learning: early in training prototypes drift toward redundant representations that minimize loss without necessarily enhancing representation diversity. To break the joint optimization, we introduce a fully decoupled training strategy that learns prototypes and encoders under separate objectives. Concretely, we model prototypes as a Gaussian mixture updated with an online EM-style procedure, independent of the encoder's loss. This simple yet principled decoupling eliminates prototype collapse without explicit regularization and yields consistently diverse prototypes and stronger downstream performance.",
    "citationCount": 0,
    "referenceCount": 55
}