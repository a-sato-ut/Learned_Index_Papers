{
    "paperId": "fce6039fb422c830f814b4be601396fce2d2db1e",
    "title": "Divergence Frontiers for Generative Models: Sample Complexity, Quantization Effects, and Frontier Integrals",
    "year": 2021,
    "venue": "Neural Information Processing Systems",
    "authors": [
        "Lang Liu",
        "Krishna Pillutla",
        "S. Welleck",
        "Sewoong Oh",
        "Yejin Choi",
        "Za√Ød Harchaoui"
    ],
    "doi": null,
    "arxivId": "2106.07898",
    "url": "https://www.semanticscholar.org/paper/fce6039fb422c830f814b4be601396fce2d2db1e",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The spectacular success of deep generative models calls for quantitative tools to measure their statistical performance. Divergence frontiers have recently been proposed as an evaluation framework for generative models, due to their ability to measure the quality-diversity trade-off inherent to deep generative modeling. We establish non-asymptotic bounds on the sample complexity of divergence frontiers. We also introduce frontier integrals which provide summary statistics of divergence frontiers. We show how smoothed estimators such as Good-Turing or Krichevsky-Trofimov can overcome the missing mass problem and lead to faster rates of convergence. We illustrate the theoretical results with numerical examples from natural language processing and computer vision.",
    "citationCount": 18,
    "referenceCount": 68
}