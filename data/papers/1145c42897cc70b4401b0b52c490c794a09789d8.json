{
    "paperId": "1145c42897cc70b4401b0b52c490c794a09789d8",
    "title": "A Survey on Knowledge Editing of Neural Networks",
    "year": 2023,
    "venue": "IEEE Transactions on Neural Networks and Learning Systems",
    "authors": [
        "Vittorio Mazzia",
        "Alessandro Pedrani",
        "Andrea Caciolai",
        "Kay Rottmann",
        "Davide Bernardi"
    ],
    "doi": "10.1109/TNNLS.2024.3498935",
    "arxivId": "2310.19704",
    "url": "https://www.semanticscholar.org/paper/1145c42897cc70b4401b0b52c490c794a09789d8",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2310.19704",
    "publicationTypes": [
        "JournalArticle",
        "Review"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Medicine",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Deep neural networks are becoming increasingly pervasive in academia and industry, matching and surpassing human performance in a wide variety of fields and related tasks. However, just as humans, even the largest artificial neural networks (ANNs) make mistakes, and once-correct predictions can become invalid as the world progresses in time. Augmenting datasets with samples that account for mistakes or up-to-date information has become a common workaround in practical applications. However, the well-known phenomenon of catastrophic forgetting poses a challenge in achieving precise changes in the implicitly memorized knowledge of neural network parameters, often requiring a full model retraining to achieve desired behaviors. That is expensive, unreliable, and incompatible with the current trend of large self-supervised pretraining, making it necessary to find more efficient and effective methods for adapting neural network models to changing data. To address this need, knowledge editing (KE) is emerging as a novel area of research that aims to enable reliable, data-efficient, and fast changes to a pretrained target model, without affecting model behaviors on previously learned tasks. In this survey, we provide a brief review of this recent artificial intelligence field of research. We first introduce the problem of editing neural networks, formalize it in a common framework and differentiate it from more notorious branches of research such as continuous learning. Next, we provide a review of the most relevant KE approaches and datasets proposed so far, grouping works under four different families: regularization techniques, meta-learning, direct model editing, and architectural strategies. Finally, we outline some intersections with other fields of research and potential directions for future works.",
    "citationCount": 32,
    "referenceCount": 134
}