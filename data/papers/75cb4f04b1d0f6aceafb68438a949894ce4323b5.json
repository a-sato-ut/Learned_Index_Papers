{
    "paperId": "75cb4f04b1d0f6aceafb68438a949894ce4323b5",
    "title": "Automatic generation of high-performance quantized machine learning kernels",
    "year": 2020,
    "venue": "IEEE/ACM International Symposium on Code Generation and Optimization",
    "authors": [
        "M. Cowan",
        "T. Moreau",
        "Tianqi Chen",
        "James Bornholt",
        "L. Ceze"
    ],
    "doi": "10.1145/3368826.3377912",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/75cb4f04b1d0f6aceafb68438a949894ce4323b5",
    "isOpenAccess": true,
    "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3368826.3377912",
    "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Quantization optimizes machine learning inference for resource constrained environments by reducing the precision of its computation. In the extreme, even single-bit computations can produce acceptable results at dramatically lower cost. But this ultra-low-precision quantization is difficult to exploit because extracting optimal performance requires hand-tuning both high-level scheduling decisions and low-level implementations. As a result, practitioners settle for a few predefined quantized kernels, sacrificing optimality and restricting their ability to adapt to new hardware. This paper presents a new automated approach to implementing quantized inference for machine learning models. We integrate the choice of how to lay out quantized values into the scheduling phase of a machine learning compiler, allowing it to be optimized in concert with tiling and parallelization decisions. After scheduling, we use program synthesis to automatically generate efficient low-level operator implementations for the desired precision and data layout. We scale up synthesis using a novel reduction sketch that exploits the structure of matrix multiplication. On a ResNet18 model, our generated code outperforms an optimized floating-point baseline by up to 3.9×, and a state-of-the-art quantized implementation by up to 16.6×.",
    "citationCount": 51,
    "referenceCount": 42
}