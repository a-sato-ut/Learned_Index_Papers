{
    "paperId": "40a9e1723aff2d12d2eed99f9eb8914e73573b90",
    "title": "Differentially Private Set Representations",
    "year": 2025,
    "venue": "Neural Information Processing Systems",
    "authors": [
        "Sarvar Patel",
        "Giuseppe Persiano",
        "Joon Young Seo",
        "Kevin Yeo"
    ],
    "doi": "10.48550/arXiv.2501.16680",
    "arxivId": "2501.16680",
    "url": "https://www.semanticscholar.org/paper/40a9e1723aff2d12d2eed99f9eb8914e73573b90",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We study the problem of differentially private (DP) mechanisms for representing sets of size $k$ from a large universe. Our first construction creates $(\\epsilon,\\delta)$-DP representations with error probability of $1/(e^\\epsilon + 1)$ using space at most $1.05 k \\epsilon \\cdot \\log(e)$ bits where the time to construct a representation is $O(k \\log(1/\\delta))$ while decoding time is $O(\\log(1/\\delta))$. We also present a second algorithm for pure $\\epsilon$-DP representations with the same error using space at most $k \\epsilon \\cdot \\log(e)$ bits, but requiring large decoding times. Our algorithms match our lower bounds on privacy-utility trade-offs (including constants but ignoring $\\delta$ factors) and we also present a new space lower bound matching our constructions up to small constant factors. To obtain our results, we design a new approach embedding sets into random linear systems deviating from most prior approaches that inject noise into non-private solutions.",
    "citationCount": 0,
    "referenceCount": 36
}