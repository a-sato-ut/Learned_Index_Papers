{
    "paperId": "2c9436969f7b4502ef8cb8993d5d6a4473a0dc83",
    "title": "Heterogeneous Acceleration Pipeline for Recommendation System Training",
    "year": 2022,
    "venue": "International Symposium on Computer Architecture",
    "authors": [
        "Muhammad Adnan",
        "Yassaman Ebrahimzadeh Maboud",
        "Divyat Mahajan",
        "Prashant J. Nair"
    ],
    "doi": "10.1109/ISCA59077.2024.00081",
    "arxivId": "2204.05436",
    "url": "https://www.semanticscholar.org/paper/2c9436969f7b4502ef8cb8993d5d6a4473a0dc83",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Recommendation models rely on deep learning networks and large embedding tables, resulting in computationally and memory-intensive processes. These models are typically trained using hybrid CPU-GPU or GPU-only configurations. The hybrid mode combines the GPU’s neural network acceleration with the CPUs’ memory storage and supply for embedding tables but may incur significant CPU-to-GPU transfer time. In contrast, the GPU-only mode utilizes High Bandwidth Memory (HBM) across multiple GPUs for storing embedding tables. However, this approach is expensive and presents scaling concerns. This paper introduces Hotline, a heterogeneous acceleration pipeline that addresses these concerns. Hotline develops a dataaware and model-aware scheduling pipeline by leveraging the insight that only a few embedding entries are frequently accessed (popular). This approach utilizes CPU main memory for nonpopular embeddings and GPUs’ HBM for popular embeddings. To achieve this, Hotline accelerator fragments a mini-batch into popular and non-popular micro-batches ($\\mu$-batches). It gathers the necessary working parameters for non-popular $\\mu$-batches from the CPU, while GPUs execute popular $\\mu$-batches. The hardware accelerator dynamically coordinates the execution of popular embeddings on GPUs and non-popular embeddings from the CPU’s main memory. Real-world datasets and models confirm Hotline’s effectiveness, reducing average end-to-end training time by $2.2 \\times$ compared to Intel-optimized CPU-GPU DLRM baseline.",
    "citationCount": 22,
    "referenceCount": 77
}