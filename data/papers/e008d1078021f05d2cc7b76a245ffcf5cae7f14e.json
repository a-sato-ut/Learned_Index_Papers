{
    "paperId": "e008d1078021f05d2cc7b76a245ffcf5cae7f14e",
    "title": "Learning from distinctive candidates to optimize reduced-precision convolution program on tensor cores",
    "year": 2022,
    "venue": "arXiv.org",
    "authors": [
        "Junkyeong Choi",
        "Hyucksung Kwon",
        "W. Lee",
        "Jungwook Choi",
        "Jieun Lim"
    ],
    "doi": null,
    "arxivId": "2202.06819",
    "url": "https://www.semanticscholar.org/paper/e008d1078021f05d2cc7b76a245ffcf5cae7f14e",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Convolution is one of the fundamental operations of deep neural networks with demanding matrix computation. In a graphic processing unit (GPU), Tensor Core is a specialized matrix processing hardware equipped with reduced-precision matrix-multiply-accumulate (MMA) instructions to increase throughput. However, it is challenging to achieve optimal performance since the best scheduling of MMA instructions varies for different convolution sizes. In particular, reduced-precision MMA requires many elements grouped as a matrix operand, seriously limiting data reuse and imposing packing and layout overhead on the schedule. This work proposes an automatic scheduling method of reduced-precision MMA for convolution operation. In this method, we devise a search space that explores the thread tile and warp sizes to increase the data reuse despite a large matrix operand of reduced-precision MMA. The search space also includes options of register-level packing and layout optimization to lesson overhead of handling reduced-precision data. Finally, we propose a search algorithm to find the best schedule by learning from the distinctive candidates. This reduced-precision MMA optimization method is evaluated on convolution operations of popular neural networks to demonstrate substantial speedup on Tensor Core compared to the state of the arts with shortened search time.",
    "citationCount": 1,
    "referenceCount": 21
}