{
    "paperId": "72a7c904ef01ad7b8d2810eda48817ef6fd152e8",
    "title": "Highly Efficient Natural Image Matting",
    "year": 2021,
    "venue": "British Machine Vision Conference",
    "authors": [
        "Yijie Zhong",
        "Bo Li",
        "Lv Tang",
        "H. Tang",
        "Shouhong Ding"
    ],
    "doi": "10.5244/c.35.433",
    "arxivId": "2110.12748",
    "url": "https://www.semanticscholar.org/paper/72a7c904ef01ad7b8d2810eda48817ef6fd152e8",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Over the last few years, deep learning based approaches have achieved outstanding improvements in natural image matting. However, there are still two drawbacks that impede the widespread application of image matting: the reliance on user-provided trimaps and the heavy model sizes. In this paper, we propose a trimap-free natural image matting method with a lightweight model. With a lightweight basic convolution block, we build a two-stages framework: Segmentation Network (SN) is designed to capture sufficient semantics and classify the pixels into unknown, foreground and background regions; Matting Refine Network (MRN) aims at capturing detailed texture information and regressing accurate alpha values. With the proposed cross-level fusion Module (CFM), SN can efficiently utilize multi-scale features with less computational cost. Efficient non-local attention module (ENA) in MRN can efficiently model the relevance between different pixels and help regress high-quality alpha values. Utilizing these techniques, we construct an extremely light-weighted model, which achieves comparable performance with ~1\\% parameters (344k) of large models on popular natural image matting benchmarks.",
    "citationCount": 20,
    "referenceCount": 55
}