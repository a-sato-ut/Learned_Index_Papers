{
    "paperId": "ee9e36c0abe8fd1c48aa73d1f83bcbb22fe3670b",
    "title": "Hyper-Tune: Towards Efficient Hyper-parameter Tuning at Scale",
    "year": 2022,
    "venue": "Proceedings of the VLDB Endowment",
    "authors": [
        "Yang Li",
        "Yu Shen",
        "Huaijun Jiang",
        "Wentao Zhang",
        "Jixiang Li",
        "Ji Liu",
        "Ce Zhang",
        "Bin Cui"
    ],
    "doi": "10.14778/3514061.3514071",
    "arxivId": "2201.06834",
    "url": "https://www.semanticscholar.org/paper/ee9e36c0abe8fd1c48aa73d1f83bcbb22fe3670b",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "\n The ever-growing demand and complexity of machine learning are putting pressure on hyper-parameter tuning systems:\n while the evaluation cost of models continues to increase, the scalability of state-of-the-arts starts to become a crucial bottleneck.\n In this paper, inspired by our experience when deploying hyper-parameter tuning in a real-world application in production and the limitations of existing systems, we propose Hyper-Tune, an efficient and robust distributed hyper-parameter tuning framework. Compared with existing systems, Hyper-Tune highlights multiple system optimizations, including (1) automatic resource allocation, (2) asynchronous scheduling, and (3) multi-fidelity optimizer. We conduct extensive evaluations on benchmark datasets and a large-scale real-world dataset in production. Empirically, with the aid of these optimizations, Hyper-Tune outperforms competitive hyper-parameter tuning systems on a wide range of scenarios, including XGBoost, CNN, RNN, and some architectural hyper-parameters for neural networks. Compared with the state-of-the-art BOHB and A-BOHB, Hyper-Tune achieves up to 11.2X and 5.1X speedups, respectively.\n",
    "citationCount": 28,
    "referenceCount": 82
}