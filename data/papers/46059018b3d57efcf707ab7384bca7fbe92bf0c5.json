{
    "paperId": "46059018b3d57efcf707ab7384bca7fbe92bf0c5",
    "title": "AutoGluon-Multimodal (AutoMM): Supercharging Multimodal AutoML with Foundation Models",
    "year": 2024,
    "venue": "AutoML",
    "authors": [
        "Zhiqiang Tang",
        "Haoyang Fang",
        "Su Zhou",
        "Taojiannan Yang",
        "Zihan Zhong",
        "Tony Hu",
        "Katrin Kirchhoff",
        "G. Karypis"
    ],
    "doi": "10.48550/arXiv.2404.16233",
    "arxivId": "2404.16233",
    "url": "https://www.semanticscholar.org/paper/46059018b3d57efcf707ab7384bca7fbe92bf0c5",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "AutoGluon-Multimodal (AutoMM) is introduced as an open-source AutoML library designed specifically for multimodal learning. Distinguished by its exceptional ease of use, AutoMM enables fine-tuning of foundation models with just three lines of code. Supporting various modalities including image, text, and tabular data, both independently and in combination, the library offers a comprehensive suite of functionalities spanning classification, regression, object detection, semantic matching, and image segmentation. Experiments across diverse datasets and tasks showcases AutoMM's superior performance in basic classification and regression tasks compared to existing AutoML tools, while also demonstrating competitive results in advanced tasks, aligning with specialized toolboxes designed for such purposes.",
    "citationCount": 25,
    "referenceCount": 128
}