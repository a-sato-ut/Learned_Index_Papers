{
    "paperId": "022f386eb66fc5532dd6f439e7a356fd33ebb9a2",
    "title": "PQCache: Product Quantization-based KVCache for Long Context LLM Inference",
    "year": 2024,
    "venue": "Proc. ACM Manag. Data",
    "authors": [
        "Hailin Zhang",
        "Xiaodong Ji",
        "Yilin Chen",
        "Fangcheng Fu",
        "Xupeng Miao",
        "Xiaonan Nie",
        "Weipeng Chen",
        "Bin Cui"
    ],
    "doi": "10.1145/3725338",
    "arxivId": "2407.12820",
    "url": "https://www.semanticscholar.org/paper/022f386eb66fc5532dd6f439e7a356fd33ebb9a2",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "As the field of Large Language Models (LLMs) continues to evolve, the context length in inference is steadily growing. Key-Value Cache (KVCache), the intermediate representations of tokens within LLM inference, has now become the primary memory bottleneck due to limited GPU memory. Current methods selectively determine suitable keys and values for self-attention computation in LLMs to address the issue. However, they either fall short in maintaining model quality or result in high serving latency. Drawing inspiration from advanced embedding retrieval techniques prevalent in the data management community, we consider the storage and retrieval of KVCache as a typical embedding retrieval problem. We propose PQCache, which employs Product Quantization (PQ) to manage KVCache, maintaining model quality while ensuring low serving latency. During the prefilling phase, we apply PQ to tokens' keys for each LLM layer and head. During the autoregressive decoding phase, we use PQ codes and centroids to approximately identify important preceding tokens, then fetch the corresponding key-value pairs for self-attention computation. Through meticulous design of overlapping and caching, we minimize any additional computation and communication overhead during both phases. Extensive experiments demonstrate that PQCache achieves both effectiveness and efficiency, with 4.60% score improvement over existing methods on InfiniteBench and low system latency in both prefilling and decoding.",
    "citationCount": 48,
    "referenceCount": 121
}