{
    "paperId": "89265449ae4a0aa96c8b9bfabb758d9d3efa11bc",
    "title": "Asymptotically Tight Approximation for Online File Caching With Delayed Hits and Bypassing",
    "year": 2025,
    "venue": "IEEE Transactions on Networking",
    "authors": [
        "Haisheng Tan",
        "Yi Wang",
        "Chi Zhang",
        "Guopeng Li",
        "Haohua Du",
        "Zhenhua Han",
        "Shaofeng H.-C. Jiang",
        "Xiangyang Li"
    ],
    "doi": "10.1109/TON.2025.3549289",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/89265449ae4a0aa96c8b9bfabb758d9d3efa11bc",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "In latency-sensitive file caching systems such as Content Delivery Networks (CDNs) and Mobile Edge Computing (MEC), the latency of fetching a missing file to the local cache can be significant. Recent studies have revealed that successive requests for the same missing file before the fetching process completes could still suffer latency (so-called delayed hits). Motivated by the practical scenarios, we study the online general file caching problem with delayed hits and bypassing, <italic>i.e.</italic>, a request may be bypassed and processed directly at the remote data center. The objective is to minimize the total request latency. We present a general reduction that turns a traditional file caching algorithm into one that can handle delayed hits. Based on this reduction, we propose an efficient online file caching algorithm, called <monospace>CaLa</monospace>, with an asymptotically tight competitive ratio as <inline-formula> <tex-math notation=\"LaTeX\">$O(Z \\log K)$ </tex-math></inline-formula>, where <italic>Z</italic> is the maximum fetching latency of any file and <italic>K</italic> is the cache size. Extensive simulations on the production data trace from Google and the Yahoo benchmark illustrate that <monospace>CaLa</monospace> can reduce the latency by up to 8.48% compared with the state-of-the-art schemes dealing with delayed hits without bypassing, and this improvement increases to 26.00% if bypassing is allowed. Furthermore, by upgrading the method for estimating filesâ€™ weights in <monospace>CaLa</monospace>, we propose <monospace>CaLa+</monospace>, which further reduces the total latency by more than 5%.",
    "citationCount": 2,
    "referenceCount": 52
}