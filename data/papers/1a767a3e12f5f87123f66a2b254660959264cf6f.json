{
    "paperId": "1a767a3e12f5f87123f66a2b254660959264cf6f",
    "title": "EARN: Efficient Inference Acceleration for LLM-based Generative Recommendation by Register Tokens",
    "year": 2025,
    "venue": "Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2",
    "authors": [
        "Chaoqun Yang",
        "Xinyu Lin",
        "Wenjie Wang",
        "Yongqi Li",
        "Teng Sun",
        "Xianjing Han",
        "Tat-Seng Chua"
    ],
    "doi": "10.1145/3711896.3736919",
    "arxivId": "2507.00715",
    "url": "https://www.semanticscholar.org/paper/1a767a3e12f5f87123f66a2b254660959264cf6f",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Large Language Model-based generative recommendation (LLMRec) has achieved notable success, but it suffers from high inference latency due to massive computational overhead and memory pressure of KV Cache. Existing KV Cache reduction methods face critical limitations: cache compression offers marginal acceleration given recommendation tasks' short decoding steps, while prompt compression risks discarding vital interaction history. Through systematic analysis of attention patterns in LLMRec, we uncover two pivotal insights: 1) layer-wise attention sparsity inversion where early layers retain dense informative patterns while later layers exhibit high redundancy, and 2) dual attention sinks phenomenon where attention scores concentrate on both head and tail tokens of input sequences. Motivated by these insights, we propose EARN, an efficient inference framework that leverages the early layers to compress information into register tokens placed at the input sequence boundaries, then focuses solely on these tokens in the subsequent layers. Extensive experiments on three datasets, two LLMRec methods and two LLM architectures demonstrate EARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction with better accuracy than the general finetuning approach. Our work bridges the efficiency-effectiveness gap in LLMRec, offering practical deployment advantages for industrial scenarios.",
    "citationCount": 1,
    "referenceCount": 42
}