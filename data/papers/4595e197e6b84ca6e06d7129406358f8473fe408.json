{
    "paperId": "4595e197e6b84ca6e06d7129406358f8473fe408",
    "title": "Nonparametric Decoding for Generative Retrieval",
    "year": 2022,
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "authors": [
        "Hyunji Lee",
        "Jaeyoung Kim",
        "Hoyeon Chang",
        "Hanseok Oh",
        "Sohee Yang",
        "Vladimir Karpukhin",
        "Yi Lu",
        "Minjoon Seo"
    ],
    "doi": "10.18653/v1/2023.findings-acl.801",
    "arxivId": "2210.02068",
    "url": "https://www.semanticscholar.org/paper/4595e197e6b84ca6e06d7129406358f8473fe408",
    "isOpenAccess": true,
    "openAccessPdf": "https://aclanthology.org/2023.findings-acl.801.pdf",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The generative retrieval model depends solely on the information encoded in its model parameters without external memory, its information capacity is limited and fixed. To overcome the limitation, we propose Nonparametric Decoding (Np Decoding) which can be applied to existing generative retrieval models. Np Decoding uses nonparametric contextualized vocab embeddings (external memory) rather than vanilla vocab embeddings as decoder vocab embeddings. By leveraging the contextualized vocab embeddings, the generative retrieval model is able to utilize both the parametric and nonparametric space. Evaluation over 9 datasets (8 single-hop and 1 multi-hop) in the document retrieval task shows that applying Np Decoding to generative retrieval models significantly improves the performance. We also show that Np Decoding is data- and parameter-efficient, and shows high performance in the zero-shot setting.",
    "citationCount": 8,
    "referenceCount": 53
}