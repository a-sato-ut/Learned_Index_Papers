{
    "paperId": "a5f2b4cf7b72294efb6363ff0755517ec5559660",
    "title": "Efficient Cache Utilization via Model-aware Data Placement for Recommendation Models",
    "year": 2021,
    "venue": "International Symposium on Memory Systems",
    "authors": [
        "M. Ibrahim",
        "Onur Kayiran",
        "Shaizeen Aga"
    ],
    "doi": "10.1145/3488423.3519317",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/a5f2b4cf7b72294efb6363ff0755517ec5559660",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Book",
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Deep neural network (DNN) based recommendation models (RMs) represent a class of critical workloads that are broadly used in social media, entertainment content, and online businesses. Given their pervasive usage, understanding the memory subsystem behavior of these models is crucial, particularly from the perspective of future memory subsystem design. To this end, in this work, we first do an in-depth memory footprint and traffic analysis of emerging RMs. We observe that emerging RMs will severely stress future (and possibly larger) caches and memories. To address this challenge, we make the key observation that a data placement strategy that is aware of the components within these models (as opposed to one that considers the entire model as a whole) stands a better chance of relieving the stress on the memory subsystem. Specifically, of the two key components of these models, namely, embedding tables and multi-layer perceptron layers, we show how we can exploit the locality of memory accesses to embedding tables to come up with a more nuanced data placement scheme. We demonstrate how our proposed data placement strategy can reduce overall memory traffic (approximately 32%) while improving performance (up to 1.99 Ã—). We argue that memory subsystems that are more amenable to residency controls stand a better chance to address the needs of emerging models.",
    "citationCount": 6,
    "referenceCount": 37
}