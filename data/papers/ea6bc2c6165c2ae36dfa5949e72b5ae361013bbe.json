{
    "paperId": "ea6bc2c6165c2ae36dfa5949e72b5ae361013bbe",
    "title": "Determining a consistent experimental setup for benchmarking and optimizing databases",
    "year": 2021,
    "venue": "GECCO Companion",
    "authors": [
        "Moisés Silva-Muñoz",
        "Gonzalo Calderon",
        "A. Franzin",
        "H. Bersini"
    ],
    "doi": "10.1145/3449726.3463180",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/ea6bc2c6165c2ae36dfa5949e72b5ae361013bbe",
    "isOpenAccess": true,
    "openAccessPdf": "https://dipot.ulb.ac.be/dspace/bitstream/2013/367320/3/SilvaCalderonFranzinBersini_IAM2021.pdf",
    "publicationTypes": [
        "JournalArticle",
        "Book"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The evaluation of the performance of an IT system is a fundamental operation in its benchmarking and optimization. However, despite the general consensus on the importance of this task, little guidance is usually provided to practitioners who need to benchmark their IT system. In particular, many works in the area of database optimization do not provide an adequate amount of information on the setup used in their experiments and analyses. In this work we report an experimental procedure that, through a sequence of experiments, analyzes the impact of various choices in the design of a database benchmark, leading to the individuation of an experimental setup that balances the consistency of the results with the time needed to obtain them. We show that the minimal experimental setup we obtain is representative also of heavier scenarios, which make it possible for the results of optimization tasks to scale.",
    "citationCount": 1,
    "referenceCount": 40
}