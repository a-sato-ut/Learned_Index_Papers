{
    "paperId": "71d7c183f05b41f91488f2d8bf4d5cc24a36072c",
    "title": "Hit the Gym: Accelerating Query Execution to Efficiently Bootstrap Behavior Models for Self-Driving Database Management Systems",
    "year": 2024,
    "venue": "Proceedings of the VLDB Endowment",
    "authors": [
        "Wan Shen Lim",
        "Lin Ma",
        "William Zhang",
        "Matthew Butrovich",
        "Samuel Arch",
        "Andrew Pavlo"
    ],
    "doi": "10.14778/3681954.3682030",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/71d7c183f05b41f91488f2d8bf4d5cc24a36072c",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Autonomous database management systems (DBMSs) aim to optimize themselves automatically without human guidance. They rely on machine learning (ML) models that predict their run-time behavior to evaluate whether a candidate configuration is beneficial without the expensive execution of queries. However, the high cost of collecting the training data to build these models makes them impractical for real-world deployments. Furthermore, these models are instance-specific and thus require retraining whenever the DBMS's environment changes. State-of-the-art methods spend over 93% of their time running queries for training versus tuning.\n To mitigate this problem, we present the Boot framework for automatically accelerating training data collection in DBMSs. Boot utilizes macro- and micro-acceleration (MMA) techniques that modify query execution semantics with approximate run-time telemetry and skip repetitive parts of the training process. To evaluate Boot, we integrated it into a database gym for PostgreSQL. Our experimental evaluation shows that Boot reduces training collection times by up to 268Ã— with modest degradation in model accuracy. These results also indicate that our MMA-based approach scales with dataset size and workload complexity.",
    "citationCount": 7,
    "referenceCount": 69
}