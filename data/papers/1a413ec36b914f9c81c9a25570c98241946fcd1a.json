{
    "paperId": "1a413ec36b914f9c81c9a25570c98241946fcd1a",
    "title": "Efficient Item ID Generation for Large-Scale LLM-based Recommendation",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Anushya Subbiah",
        "Vikram Aggarwal",
        "James Pine",
        "Steffen Rendle",
        "Krishna Sayana",
        "Kun Su"
    ],
    "doi": "10.48550/arXiv.2509.03746",
    "arxivId": "2509.03746",
    "url": "https://www.semanticscholar.org/paper/1a413ec36b914f9c81c9a25570c98241946fcd1a",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Integrating product catalogs and user behavior into LLMs can enhance recommendations with broad world knowledge, but the scale of real-world item catalogs, often containing millions of discrete item identifiers (Item IDs), poses a significant challenge. This contrasts with the smaller, tokenized text vocabularies typically used in LLMs. The predominant view within the LLM-based recommendation literature is that it is infeasible to treat item ids as a first class citizen in the LLM and instead some sort of tokenization of an item into multiple tokens is required. However, this creates a key practical bottleneck in serving these models for real-time low-latency applications. Our paper challenges this predominant practice and integrates item ids as first class citizens into the LLM. We provide simple, yet highly effective, novel training and inference modifications that enable single-token representations of items and single-step decoding. Our method shows improvements in recommendation quality (Recall and NDCG) over existing techniques on the Amazon shopping datasets while significantly improving inference efficiency by 5x-14x. Our work offers an efficiency perspective distinct from that of other popular approaches within LLM-based recommendation, potentially inspiring further research and opening up a new direction for integrating IDs into LLMs. Our code is available here https://drive.google.com/file/d/1cUMj37rV0Z1bCWMdhQ6i4q4eTRQLURtC",
    "citationCount": 0,
    "referenceCount": 38
}