{
    "paperId": "d5b4f5af3ceedf79b74ef4cdd4abc8f80a9a75e3",
    "title": "Accelerating Linear Recurrent Neural Networks for the Edge with Unstructured Sparsity",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Alessandro Pierro",
        "Steven Abreu",
        "Jonathan Timcheck",
        "Philipp Stratmann",
        "Andreas Wild",
        "S. Shrestha"
    ],
    "doi": "10.48550/arXiv.2502.01330",
    "arxivId": "2502.01330",
    "url": "https://www.semanticscholar.org/paper/d5b4f5af3ceedf79b74ef4cdd4abc8f80a9a75e3",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Linear recurrent neural networks enable powerful long-range sequence modeling with constant memory usage and time-per-token during inference. These architectures hold promise for streaming applications at the edge, but deployment in resource-constrained environments requires hardware-aware optimizations to minimize latency and energy consumption. Unstructured sparsity offers a compelling solution, enabling substantial reductions in compute and memory requirements--when accelerated by compatible hardware platforms. In this paper, we conduct a scaling study to investigate the Pareto front of performance and efficiency across inference compute budgets. We find that highly sparse linear RNNs consistently achieve better efficiency-performance trade-offs than dense baselines, with 2x less compute and 36% less memory at iso-accuracy. Our models achieve state-of-the-art results on a real-time streaming task for audio denoising. By quantizing our sparse models to fixed-point arithmetic and deploying them on the Intel Loihi 2 neuromorphic chip for real-time processing, we translate model compression into tangible gains of 42x lower latency and 149x lower energy consumption compared to a dense model on an edge GPU. Our findings showcase the transformative potential of unstructured sparsity, paving the way for highly efficient recurrent neural networks in real-world, resource-constrained environments.",
    "citationCount": 1,
    "referenceCount": 66
}