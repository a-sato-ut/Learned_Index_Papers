{
    "paperId": "2a574b943f4a86df3ca314755e7945affea030cb",
    "title": "A 1.2-fJ/MAC, 7-fJ/ReLU, 5.9-b ENOB, All-Analog Neural Network Computing Macro",
    "year": 2025,
    "venue": "Prime",
    "authors": [
        "Raphael Nägele",
        "Jakob Finkbeiner",
        "Manuel Wittlinger",
        "M. Grözing",
        "Manfred Berroth",
        "Georg Rademacher"
    ],
    "doi": "10.1109/PRIME66228.2025.11203351",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/2a574b943f4a86df3ca314755e7945affea030cb",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Artificial intelligence at the edge requires ultra energy efficient computing circuits. Computing in the analog domain can offer orders of magnitude higher energy efficiency compared to the digital domain. We present and characterize an all-analog neural layer macro to compute fully-connected neural networks without the involvement of data converters and digital signal processing in the data path. Our charge domain multiplyaccumulate (MAC) units operate with 1.2 fJ/MAC in average and the ReLU-voltage-to-time converter with $7 \\mathrm{fJ} / \\mathrm{ReLU}$. The effective resolution is 5.6 b for the weights, 4.9 b for the activations and 5.9 b for the dot product. An accurate TensorFlow model is developed for hardware-aware training.",
    "citationCount": 0,
    "referenceCount": 15
}