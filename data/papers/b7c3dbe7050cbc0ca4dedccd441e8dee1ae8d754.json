{
    "paperId": "b7c3dbe7050cbc0ca4dedccd441e8dee1ae8d754",
    "title": "Learn to optimizeâ€”a brief overview",
    "year": 2024,
    "venue": "National Science Review",
    "authors": [
        "Ke Tang",
        "Xin Yao"
    ],
    "doi": "10.1093/nsr/nwae132",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/b7c3dbe7050cbc0ca4dedccd441e8dee1ae8d754",
    "isOpenAccess": true,
    "openAccessPdf": "https://academic.oup.com/nsr/advance-article-pdf/doi/10.1093/nsr/nwae132/57141610/nwae132.pdf",
    "publicationTypes": [
        "Review",
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Medicine",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "ABSTRACT Most optimization problems of practical significance are typically solved by highly configurable parameterized algorithms. To achieve the best performance on a problem instance, a trial-and-error configuration process is required, which is very costly and even prohibitive for problems that are already computationally intensive, e.g. optimization problems associated with machine learning tasks. In the past decades, many studies have been conducted to accelerate the tedious configuration process by learning from a set of training instances. This article refers to these studies as learn to optimize and reviews the progress achieved.",
    "citationCount": 26,
    "referenceCount": 88
}