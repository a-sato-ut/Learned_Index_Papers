{
    "paperId": "760131f20fd013aca69786a4baf54e41b2e64e1f",
    "title": "Automating In-Network Machine Learning",
    "year": 2022,
    "venue": "arXiv.org",
    "authors": [
        "Changgang Zheng",
        "Mingyuan Zang",
        "Xinpeng Hong",
        "Riyad Bensoussane",
        "S. Vargaftik",
        "Y. Ben-Itzhak",
        "Noa Zilberman"
    ],
    "doi": "10.48550/arXiv.2205.08824",
    "arxivId": "2205.08824",
    "url": "https://www.semanticscholar.org/paper/760131f20fd013aca69786a4baf54e41b2e64e1f",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2205.08824",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Using programmable network devices to aid in-network machine learning has been the focus of significant research. However, most of the research was of a limited scope, providing a proof of concept or describing a closed-source algorithm. To date, no general solution has been provided for mapping machine learning algorithms to programmable network devices. In this paper, we present Planter, an open-source, modular framework for mapping trained machine learning models to programmable devices. Planter supports a wide range of machine learning models, multiple targets and can be easily extended. The evaluation of Planter compares different mapping approaches, and demonstrates the feasibility, performance, and resource efficiency for applications such as anomaly detection, financial transactions, and quality of experience. The results show that Planter-based in-network machine learning algorithms can run at line rate, have a negligible effect on latency, coexist with standard switching functionality, and have no or minor accuracy trade-offs.",
    "citationCount": 44,
    "referenceCount": 76
}