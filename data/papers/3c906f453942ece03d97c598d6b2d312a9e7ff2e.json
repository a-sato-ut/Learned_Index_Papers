{
    "paperId": "3c906f453942ece03d97c598d6b2d312a9e7ff2e",
    "title": "Iteratively Enhanced Semidefinite Relaxations for Efficient Neural Network Verification",
    "year": 2023,
    "venue": "AAAI Conference on Artificial Intelligence",
    "authors": [
        "Jianglin Lan",
        "Yang Zheng",
        "A. Lomuscio"
    ],
    "doi": "10.1609/aaai.v37i12.26744",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/3c906f453942ece03d97c598d6b2d312a9e7ff2e",
    "isOpenAccess": true,
    "openAccessPdf": "https://ojs.aaai.org/index.php/AAAI/article/download/26744/26516",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We propose an enhanced semidefinite program (SDP) relaxation to enable the tight and efficient verification of neural networks (NNs). The tightness improvement is achieved by introducing a nonlinear constraint to existing SDP relaxations previously proposed for NN verification. The efficiency of the proposal stems from the iterative nature of the proposed algorithm in that it solves the resulting non-convex SDP by recursively solving auxiliary convex layer-based SDP problems. We show formally that the solution generated by our algorithm is tighter than state-of-the-art SDP-based solutions for the problem. We also show that the solution sequence converges to the optimal solution of the non-convex enhanced SDP relaxation. The experimental results on standard benchmarks in the area show that our algorithm achieves the state-of-the-art performance whilst maintaining an acceptable computational cost.",
    "citationCount": 1,
    "referenceCount": 51
}