{
    "paperId": "5787f36ddd1cd1b652064409b0483826530ad6c3",
    "title": "Hierarchical Prediction-based Management for LMaaS Systems",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Zhihan Jiang",
        "Yujie Huang",
        "Guangba Yu",
        "Junjie Huang",
        "Jiazhen Gu",
        "Michael R. Lyu"
    ],
    "doi": "10.48550/arXiv.2504.03702",
    "arxivId": "2504.03702",
    "url": "https://www.semanticscholar.org/paper/5787f36ddd1cd1b652064409b0483826530ad6c3",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Large Language Models (LLMs) have revolutionized numerous domains, driving the rise of Language-Model-as-a-Service (LMaaS) platforms that process millions of queries daily. These platforms must minimize latency and meet Service Level Objectives (SLOs) while optimizing resource usage. However, conventional cloud service management techniques, designed for traditional workloads, are suboptimal for LMaaS due to its dynamic service workloads and variable request loads. To address this, we propose PreServe, a tailored LMaaS management framework centered on hierarchical prediction. PreServe incorporates a service workload predictor to estimate periodic token density at a coarse granularity and a novel request load predictor to assess the resource demand of individual LLM requests, enabling the construction of a load anticipator for each LLM instance. By integrating both long-term and short-term predictions, PreServe adjusts resource allocation in advance, mitigating the risks of instance under- or over-provisioning. Besides, PreServe optimizes request routing by considering both current and anticipated future instance loads, ensuring balanced load distribution across instances. Evaluations on real-world production datasets show that PreServe outperforms state-of-the-art methods, reducing tail latency by 41.3%, cutting resource consumption by 49.38%, while incurring only 0.23% additional overhead.",
    "citationCount": 0,
    "referenceCount": 102
}