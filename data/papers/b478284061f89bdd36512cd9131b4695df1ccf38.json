{
    "paperId": "b478284061f89bdd36512cd9131b4695df1ccf38",
    "title": "Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Shan Yu",
        "Jiarong Xing",
        "Yifan Qiao",
        "Mingyuan Ma",
        "Yangmin Li",
        "Yang Wang",
        "Shuo Yang",
        "Zhiqiang Xie",
        "Shiyi Cao",
        "Ke Bao",
        "Ion Stoica",
        "Harry Xu",
        "Ying Sheng"
    ],
    "doi": "10.48550/arXiv.2505.04021",
    "arxivId": "2505.04021",
    "url": "https://www.semanticscholar.org/paper/b478284061f89bdd36512cd9131b4695df1ccf38",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Serving large language models (LLMs) is expensive, especially for providers hosting many models, making cost reduction essential. The unique workload patterns of serving multiple LLMs (i.e., multi-LLM serving) create new opportunities and challenges for this task. The long-tail popularity of models and their long idle periods present opportunities to improve utilization through GPU sharing. However, existing GPU sharing systems lack the ability to adjust their resource allocation and sharing policies at runtime, making them ineffective at meeting latency service-level objectives (SLOs) under rapidly fluctuating workloads. This paper presents Prism, a multi-LLM serving system that unleashes the full potential of GPU sharing to achieve both cost efficiency and SLO attainment. At its core, Prism tackles a key limitation of existing systems$\\unicode{x2014}$the lack of $\\textit{cross-model memory coordination}$, which is essential for flexibly sharing GPU memory across models under dynamic workloads. Prism achieves this with two key designs. First, it supports on-demand memory allocation by dynamically mapping physical to virtual memory pages, allowing flexible memory redistribution among models that space- and time-share a GPU. Second, it improves memory efficiency through a two-level scheduling policy that dynamically adjusts sharing strategies based on models' runtime demands. Evaluations on real-world traces show that Prism achieves more than $2\\times$ cost savings and $3.3\\times$ SLO attainment compared to state-of-the-art systems.",
    "citationCount": 8,
    "referenceCount": 54
}