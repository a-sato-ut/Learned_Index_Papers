{
    "paperId": "7be8750e3b3d1d004b9dd91a11e0c508495969a2",
    "title": "Mirage: A Multi-Level Superoptimizer for Tensor Programs",
    "year": 2024,
    "venue": "USENIX Symposium on Operating Systems Design and Implementation",
    "authors": [
        "Mengdi Wu",
        "Xinhao Cheng",
        "Shengyu Liu",
        "Chunan Shi",
        "Jianan Ji",
        "Man Kit Ao",
        "Praveen Velliengiri",
        "Xupeng Miao",
        "Oded Padon",
        "Zhihao Jia"
    ],
    "doi": null,
    "arxivId": "2405.05751",
    "url": "https://www.semanticscholar.org/paper/7be8750e3b3d1d004b9dd91a11e0c508495969a2",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We introduce Mirage, the first multi-level superoptimizer for tensor programs. A key idea in Mirage is $\\mu$Graphs, a uniform representation of tensor programs at the kernel, thread block, and thread levels of the GPU compute hierarchy. $\\mu$Graphs enable Mirage to discover novel optimizations that combine algebraic transformations, schedule transformations, and generation of new custom kernels. To navigate the large search space, Mirage introduces a pruning technique based on abstraction that significantly reduces the search space and provides a certain optimality guarantee. To ensure that the optimized $\\mu$Graph is equivalent to the input program, Mirage introduces a probabilistic equivalence verification procedure with strong theoretical guarantees. Our evaluation shows that Mirage outperforms existing approaches by up to 3.3$\\times$ even for DNNs that are widely used and heavily optimized. Mirage is publicly available at https://github.com/mirage-project/mirage.",
    "citationCount": 12,
    "referenceCount": 51
}