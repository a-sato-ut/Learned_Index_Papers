{
    "paperId": "514b902bb2f44eb228a5c5c643cd5235409bbe5c",
    "title": "AI-Driven Performance Modeling for AI Inference Workloads",
    "year": 2022,
    "venue": "Electronics",
    "authors": [
        "Max Sponner",
        "Bernd Waschneck",
        "Akash Kumar"
    ],
    "doi": "10.3390/electronics11152316",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/514b902bb2f44eb228a5c5c643cd5235409bbe5c",
    "isOpenAccess": true,
    "openAccessPdf": "https://www.mdpi.com/2079-9292/11/15/2316/pdf?version=1658821163",
    "publicationTypes": null,
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Deep Learning (DL) is moving towards deploying workloads not only in cloud datacenters, but also to the local devices. Although these are mostly limited to inference tasks, it still widens the range of possible target architectures significantly. Additionally, these new targets usually come with drastically reduced computation performance and memory sizes compared to the traditionally used architectures—and put the key optimization focus on the efficiency as they often depend on batteries. To help developers quickly estimate the performance of a neural network during its design phase, performance models could be used. However, these models are expensive to implement as they require in-depth knowledge about the hardware architecture and the used algorithms. Although AI-based solutions exist, these either require large datasets that are difficult to collect on the low-performance targets and/or limited to a small number of target platforms and metrics. Our solution exploits the block-based structure of neural networks, as well as the high similarity in the typically used layer configurations across neural networks, enabling the training of accurate models on significantly smaller datasets. In addition, our solution is not limited to a specific architecture or metric. We showcase the feasibility of the solution on a set of seven devices from four different hardware architectures, and with up to three performance metrics per target—including the power consumption and memory footprint. Our tests have shown that the solution achieved an error of less than 1 ms (2.6%) in latency, 0.12 J (4%) in energy consumption and 11 MiB (1.5%) in memory allocation for the whole network inference prediction, while being up to five orders of magnitude faster than a benchmark.",
    "citationCount": 5,
    "referenceCount": 61
}