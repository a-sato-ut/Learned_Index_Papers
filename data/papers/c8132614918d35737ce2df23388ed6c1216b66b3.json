{
    "paperId": "c8132614918d35737ce2df23388ed6c1216b66b3",
    "title": "Cooled-KLL: Enhancing Quantile Estimation by Filtering Hot Item",
    "year": 2025,
    "venue": "Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2",
    "authors": [
        "Qilong Shi",
        "Wei Zhou",
        "Yizhuo Zheng",
        "Xinye Xu",
        "Ting Yang",
        "Yuanyuan Zhang",
        "Long Yao",
        "Yangyang Wang",
        "Mingwei Xu"
    ],
    "doi": "10.1145/3711896.3736894",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/c8132614918d35737ce2df23388ed6c1216b66b3",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Quantile estimation is critical for diverse applications, including database management and network traffic monitoring. Probabilistic quantile sketches are widely employed in practice, with the KLL sketch (introduced in 2016) being particularly notable for its theoretically space-optimal properties. However, KLL overlooks the inherent repetition of elements often present in real-world data streams. Such streams are frequently highly skewed, characterized by ''hot items''-items that appear with high frequency. The KLL sketch processes these hot items without accounting for their prevalence, resulting in suboptimal space utilization due to redundant insertions and storage. To overcome this limitation, we propose Cooled-KLL, an enhanced KLL sketch. Cooled-KLL introduces a novel ''Hot Filter'' structure that efficiently identifies and stores hot items as compact key-value pairs. This mechanism ensures that only ''cold'' (less frequent) items are subsequently processed by the core KLL sketch. Our approach significantly reduces memory consumption without compromising processing speed. Extensive experiments demonstrate that Cooled-KLL consistently outperforms five other state-of-the-art algorithms, achieving up to 2.5 orders of magnitude higher accuracy compared to the standard KLL sketch.",
    "citationCount": 0,
    "referenceCount": 51
}