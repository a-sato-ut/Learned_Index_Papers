{
    "paperId": "9d9fdc77f6931c25997b2f56bd8183499ac29a3b",
    "title": "Domain Adaptation for Dense Retrieval through Self-Supervision by Pseudo-Relevance Labeling",
    "year": 2022,
    "venue": "arXiv.org",
    "authors": [
        "Minghan Li",
        "Ã‰ric Gaussier"
    ],
    "doi": "10.48550/arXiv.2212.06552",
    "arxivId": "2212.06552",
    "url": "https://www.semanticscholar.org/paper/9d9fdc77f6931c25997b2f56bd8183499ac29a3b",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2212.06552",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Although neural information retrieval has witnessed great improvements, recent works showed that the generalization ability of dense retrieval models on target domains with different distributions is limited, which contrasts with the results obtained with interaction-based models. To address this issue, researchers have resorted to adversarial learning and query generation approaches; both approaches nevertheless resulted in limited improvements. In this paper, we propose to use a self-supervision approach in which pseudo-relevance labels are automatically generated on the target domain. To do so, we first use the standard BM25 model on the target domain to obtain a first ranking of documents, and then use the interaction-based model T53B to re-rank top documents. We further combine this approach with knowledge distillation relying on an interaction-based teacher model trained on the source domain. Our experiments reveal that pseudo-relevance labeling using T53B and the MiniLM teacher performs on average better than other approaches and helps improve the state-of-the-art query generation approach GPL when it is fine-tuned on the pseudo-relevance labeled data.",
    "citationCount": 5,
    "referenceCount": 57
}