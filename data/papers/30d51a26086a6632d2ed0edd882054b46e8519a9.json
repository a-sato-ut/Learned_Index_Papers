{
    "paperId": "30d51a26086a6632d2ed0edd882054b46e8519a9",
    "title": "Automating reinforcement learning architecture design for code optimization",
    "year": 2022,
    "venue": "International Conference on Compiler Construction",
    "authors": [
        "Huanting Wang",
        "Zhanyong Tang",
        "Chen Zhang",
        "Jiaqi Zhao",
        "Chris Cummins",
        "Hugh Leather",
        "Z. Wang"
    ],
    "doi": "10.1145/3497776.3517769",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/30d51a26086a6632d2ed0edd882054b46e8519a9",
    "isOpenAccess": true,
    "openAccessPdf": "https://eprints.whiterose.ac.uk/184257/7/main.pdf",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Reinforcement learning (RL) is emerging as a powerful technique for solving complex code optimization tasks with an ample search space. While promising, existing solutions require a painstaking manual process to tune the right task-specific RL architecture, for which compiler developers need to determine the composition of the RL exploration algorithm, its supporting components like state, reward, and transition functions, and the hyperparameters of these models. This paper introduces SuperSonic, a new open-source framework to allow compiler developers to integrate RL into compilers easily, regardless of their RL expertise. SuperSonic supports customizable RL architecture compositions to target a wide range of optimization tasks. A key feature of SuperSonic is the use of deep RL and multi-task learning techniques to develop a meta-optimizer to automatically find and tune the right RL architecture from training benchmarks. The tuned RL can then be deployed to optimize new programs. We demonstrate the efficacy and generality of SuperSonic by applying it to four code optimization problems and comparing it against eight auto-tuning frameworks. Experimental results show that SuperSonic consistently improves hand-tuned methods by delivering better overall performance, accelerating the deployment-stage search by 1.75x on average (up to 100x).",
    "citationCount": 22,
    "referenceCount": 93
}