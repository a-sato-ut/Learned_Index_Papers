{
    "paperId": "ef691c914a3a59c12fc58c671f5488883cdc8d21",
    "title": "Lightweight Alpha Matting Network Using Distillation-Based Channel Pruning",
    "year": 2022,
    "venue": "Asian Conference on Computer Vision",
    "authors": [
        "Donggeun Yoon",
        "Jinsun Park",
        "Donghyeon Cho"
    ],
    "doi": "10.48550/arXiv.2210.07760",
    "arxivId": "2210.07760",
    "url": "https://www.semanticscholar.org/paper/ef691c914a3a59c12fc58c671f5488883cdc8d21",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2210.07760",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Recently, alpha matting has received a lot of attention because of its usefulness in mobile applications such as selfies. Therefore, there has been a demand for a lightweight alpha matting model due to the limited computational resources of commercial portable devices. To this end, we suggest a distillation-based channel pruning method for the alpha matting networks. In the pruning step, we remove channels of a student network having fewer impacts on mimicking the knowledge of a teacher network. Then, the pruned lightweight student network is trained by the same distillation loss. A lightweight alpha matting model from the proposed method outperforms existing lightweight methods. To show superiority of our algorithm, we provide various quantitative and qualitative experiments with in-depth analyses. Furthermore, we demonstrate the versatility of the proposed distillation-based channel pruning method by applying it to semantic segmentation.",
    "citationCount": 1,
    "referenceCount": 55
}