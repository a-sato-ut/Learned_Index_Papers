{
    "paperId": "9a2ca811882ed7513f83014b9de4fb3b4ab218c4",
    "title": "Compositional Generalization and Decomposition in Neural Program Synthesis",
    "year": 2022,
    "venue": "arXiv.org",
    "authors": [
        "Kensen Shi",
        "Joey Hong",
        "M. Zaheer",
        "Pengcheng Yin",
        "Charles Sutton"
    ],
    "doi": "10.48550/arXiv.2204.03758",
    "arxivId": "2204.03758",
    "url": "https://www.semanticscholar.org/paper/9a2ca811882ed7513f83014b9de4fb3b4ab218c4",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2204.03758",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, what we can measure is whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks. In this paper, we focus on measuring the ability of learned program synthesizers to compositionally generalize. We first characterize several different axes along which program synthesis methods would be desired to generalize, e.g., length generalization, or the ability to combine known subroutines in new ways that do not occur in the training data. Based on this characterization, we introduce a benchmark suite of tasks to assess these abilities based on two popular existing datasets, SCAN and RobustFill. Finally, we make first attempts to improve the compositional generalization ability of Transformer models along these axes through novel attention mechanisms that draw inspiration from a human-like decomposition strategy. Empirically, we find our modified Transformer models generally perform better than natural baselines, but the tasks remain challenging.",
    "citationCount": 7,
    "referenceCount": 63
}