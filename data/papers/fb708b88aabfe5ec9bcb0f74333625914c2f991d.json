{
    "paperId": "fb708b88aabfe5ec9bcb0f74333625914c2f991d",
    "title": "Unveiling and Mitigating Adversarial Vulnerabilities in Iterative Optimizers",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Elad Sofer",
        "Tomer Shaked",
        "Caroline Chaux",
        "Nir Shlezinger"
    ],
    "doi": "10.48550/arXiv.2504.19000",
    "arxivId": "2504.19000",
    "url": "https://www.semanticscholar.org/paper/fb708b88aabfe5ec9bcb0f74333625914c2f991d",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Engineering",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Machine learning (ML) models are often sensitive to carefully crafted yet seemingly unnoticeable perturbations. Such adversarial examples are considered to be a property of ML models, often associated with their black-box operation and sensitivity to features learned from data. This work examines the adversarial sensitivity of non-learned decision rules, and particularly of iterative optimizers. Our analysis is inspired by the recent developments in deep unfolding, which cast such optimizers as ML models. We show that non-learned iterative optimizers share the sensitivity to adversarial examples of ML models, and that attacking iterative optimizers effectively alters the optimization objective surface in a manner that modifies the minima sought. We then leverage the ability to cast iteration-limited optimizers as ML models to enhance robustness via adversarial training. For a class of proximal gradient optimizers, we rigorously prove how their learning affects adversarial sensitivity. We numerically back our findings, showing the vulnerability of various optimizers, as well as the robustness induced by unfolding and adversarial training.",
    "citationCount": 2,
    "referenceCount": 55
}