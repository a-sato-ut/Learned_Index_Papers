{
    "paperId": "61c0da1586bd93c21c5c02877a5e8b232618a51c",
    "title": "Fully Automatic Neural Network Reduction for Formal Verification",
    "year": 2023,
    "venue": "Trans. Mach. Learn. Res.",
    "authors": [
        "T. Ladner",
        "M. Althoff"
    ],
    "doi": null,
    "arxivId": "2305.01932",
    "url": "https://www.semanticscholar.org/paper/61c0da1586bd93c21c5c02877a5e8b232618a51c",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Formal verification of neural networks is essential before their deployment in safety-critical applications. However, existing methods for formally verifying neural networks are not yet scalable enough to handle practical problems under strict time constraints. We address this challenge by introducing a fully automatic and sound reduction of neural networks using reachability analysis. The soundness ensures that the verification of the reduced network entails the verification of the original network. Our sound reduction approach is applicable to neural networks with any type of element-wise activation function, such as ReLU, sigmoid, and tanh. The network reduction is computed on the fly while simultaneously verifying the original network and its specification. All parameters are automatically tuned to minimize the network size without compromising verifiability. We further show the applicability of our approach to convolutional neural networks by explicitly exploiting similar neighboring pixels. Our evaluation shows that our approach reduces large neural networks to a fraction of the original number of neurons and thus shortens the verification time to a similar degree.",
    "citationCount": 6,
    "referenceCount": 68
}