{
    "paperId": "4753c543ad058554605f980699ebe1becc05b703",
    "title": "PAC-Bayesian Learning of Optimization Algorithms",
    "year": 2022,
    "venue": "International Conference on Artificial Intelligence and Statistics",
    "authors": [
        "Michael Sucker",
        "Peter Ochs"
    ],
    "doi": "10.48550/arXiv.2210.11113",
    "arxivId": "2210.11113",
    "url": "https://www.semanticscholar.org/paper/4753c543ad058554605f980699ebe1becc05b703",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2210.11113",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We apply the PAC-Bayes theory to the setting of learning-to-optimize. To the best of our knowledge, we present the first framework to learn optimization algorithms with provable generalization guarantees (PAC-bounds) and explicit trade-off between a high probability of convergence and a high convergence speed. Even in the limit case, where convergence is guaranteed, our learned optimization algorithms provably outperform related algorithms based on a (deterministic) worst-case analysis. Our results rely on PAC-Bayes bounds for general, unbounded loss-functions based on exponential families. By generalizing existing ideas, we reformulate the learning procedure into a one-dimensional minimization problem and study the possibility to find a global minimum, which enables the algorithmic realization of the learning procedure. As a proof-of-concept, we learn hyperparameters of standard optimization algorithms to empirically underline our theory.",
    "citationCount": 5,
    "referenceCount": 62
}