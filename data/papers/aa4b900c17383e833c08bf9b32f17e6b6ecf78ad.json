{
    "paperId": "aa4b900c17383e833c08bf9b32f17e6b6ecf78ad",
    "title": "Privacy-Preserving Portrait Matting",
    "year": 2021,
    "venue": "ACM Multimedia",
    "authors": [
        "Jizhizi Li",
        "Sihan Ma",
        "Jing Zhang",
        "D. Tao"
    ],
    "doi": "10.1145/3474085.3475512",
    "arxivId": "2104.14222",
    "url": "https://www.semanticscholar.org/paper/aa4b900c17383e833c08bf9b32f17e6b6ecf78ad",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2104.14222",
    "publicationTypes": [
        "JournalArticle",
        "Book"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Recently, there has been an increasing concern about the privacy issue raised by using personally identifiable information in machine learning. However, previous portrait matting methods were all based on identifiable portrait images. To fill the gap, we present P3M-10k in this paper, which is the first large-scale anonymized benchmark for Privacy-Preserving Portrait Matting. P3M-10k consists of 10,000 high-resolution face-blurred portrait images along with high-quality alpha mattes. We systematically evaluate both trimap-free and trimap-based matting methods on P3M-10k and find that existing matting methods show different generalization capabilities when following the Privacy-Preserving Training (PPT) setting, i.e., training on face-blurred images and testing on arbitrary images. To devise a better trimap-free portrait matting model, we propose P3M-Net, which leverages the power of a unified framework for both semantic perception and detail matting, and specifically emphasizes the interaction between them and the encoder to facilitate the matting process. Extensive experiments on P3M-10k demonstrate that P3M-Net outperforms the state-of-the-art methods in terms of both objective metrics and subjective visual quality. Besides, it shows good generalization capacity under the PPT setting, confirming the value of P3M-10k for facilitating future research and enabling potential real-world applications. The source code and dataset are available at https://github.com/JizhiziLi/P3M.",
    "citationCount": 69,
    "referenceCount": 48
}