{
    "paperId": "bf5d9e92e921a945d3e6652dc786c3c7d96988ae",
    "title": "Testing the Robustness of Learned Index Structures",
    "year": 2022,
    "venue": "arXiv.org",
    "authors": [
        "Matthias Bachfischer",
        "Renata Borovica-Gajic",
        "Benjamin I. P. Rubinstein"
    ],
    "doi": "10.48550/arXiv.2207.11575",
    "arxivId": "2207.11575",
    "url": "https://www.semanticscholar.org/paper/bf5d9e92e921a945d3e6652dc786c3c7d96988ae",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2207.11575",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "While early empirical evidence has supported the case for learned index structures as having favourable average-case performance, little is known about their worst-case performance. By contrast, classical structures are known to achieve optimal worst-case behaviour. This work evaluates the robustness of learned index structures in the presence of adversarial workloads. To simulate adversarial work-loads, we carry out a data poisoning attack on linear regression models that manipulates the cumulative distribution function (CDF) on which the learned index model is trained. The attack deteriorates the fit of the underlying ML model by injecting a set of poisoning keys into the training dataset, which leads to an increase in the prediction error of the model and thus deteriorates the overall performance of the learned index structure. We assess the performance of various regression methods and the learned index implementations ALEX and PGM-Index. We show that learned index structures can suffer from a significant performance deterioration of up to 20% when evaluated on poisoned vs. non-poisoned datasets.",
    "citationCount": 2,
    "referenceCount": 35
}