{
    "paperId": "8dff027b0856b1f646a6283dd330f346273b03d4",
    "title": "Overparameterized neural networks implement associative memory",
    "year": 2019,
    "venue": "Proceedings of the National Academy of Sciences of the United States of America",
    "authors": [
        "Adityanarayanan Radhakrishnan",
        "M. Belkin",
        "Caroline Uhler"
    ],
    "doi": "10.1073/pnas.2005013117",
    "arxivId": "1909.12362",
    "url": "https://www.semanticscholar.org/paper/8dff027b0856b1f646a6283dd330f346273b03d4",
    "isOpenAccess": true,
    "openAccessPdf": "https://www.pnas.org/content/pnas/117/44/27162.full.pdf",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Medicine",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Significance Development of computational models of memory is a subject of long-standing interest at the intersection of machine learning and neuroscience. Our main finding is that overparameterized neural networks trained using standard optimization methods provide a simple mechanism for implementing associative memory. Remarkably, this mechanism allows for the storage and retrieval of sequences of examples. This finding also sheds light on inductive biases in overparameterized networks: while there are many functions that can achieve zero training loss in the overparameterized regime, our result shows that increasing depth and width in neural networks leads to maps that are more contractive around training examples, thereby allowing for storage and retrieval of more training examples. Identifying computational mechanisms for memorization and retrieval of data is a long-standing problem at the intersection of machine learning and neuroscience. Our main finding is that standard overparameterized deep neural networks trained using standard optimization methods implement such a mechanism for real-valued data. We provide empirical evidence that 1) overparameterized autoencoders store training samples as attractors and thus iterating the learned map leads to sample recovery, and that 2) the same mechanism allows for encoding sequences of examples and serves as an even more efficient mechanism for memory than autoencoding. Theoretically, we prove that when trained on a single example, autoencoders store the example as an attractor. Lastly, by treating a sequence encoder as a composition of maps, we prove that sequence encoding provides a more efficient mechanism for memory than autoencoding.",
    "citationCount": 75,
    "referenceCount": 47
}