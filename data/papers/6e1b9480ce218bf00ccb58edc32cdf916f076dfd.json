{
    "paperId": "6e1b9480ce218bf00ccb58edc32cdf916f076dfd",
    "title": "Online Container Caching with Late-Warm for IoT Data Processing",
    "year": 2024,
    "venue": "IEEE International Conference on Data Engineering",
    "authors": [
        "Guopeng Li",
        "Haisheng Tan",
        "Xuan Zhang",
        "Chi Zhang",
        "Ruiting Zhou",
        "Zhenhua Han",
        "Guoliang Chen"
    ],
    "doi": "10.1109/ICDE60146.2024.00127",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/6e1b9480ce218bf00ccb58edc32cdf916f076dfd",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        },
        {
            "category": "Environmental Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Serverless edge computing is an efficient way to execute event-driven, short-duration, and bursty IoT data processing tasks on resource-limited edge servers, using on-demand resource allocation and dynamic auto-scaling. In this paradigm, function requests are handled in virtualized environments, e.g., containers. When a function request arrives online, if there is no container in memory to execute it, the serverless platform will initialize such a container with non-negligible latency, known as cold start. Otherwise, it results in a warm start with no latency in previous studies. However, based on our experiments, we find there is a remarkable third case called Late-Warm, i.e., when a request arrives during the container initializing, its latency is less than a cold start but not zero. In this paper, we study online container caching in serverless edge computing to minimize the total latency with Late-Warm and other practical issues considered. We propose OnCoLa, a novel $O(T_{c}^{3}/2K)$-competitive algorithm supporting request relaying on multiple edge servers. Here, Tc and $K$ are the maximum container cold start latency and the memory size, respectively. Experiments on Raspberry Pi and Jetson Nano with OpenFaaS and faasd using common IoT data processing tasks show that OnCoLa reduces latency by up to 21.38% compared with representative lightweight policies. Extensive simulations on two real-world traces demonstrate that OnCoLa consistently outperforms the state-of-the-art container caching algorithms and reduces the latency by 27.8%.",
    "citationCount": 1,
    "referenceCount": 79
}