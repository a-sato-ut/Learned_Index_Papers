{
    "paperId": "f915dfd0719235951ee9e46302ea81e8f05778cf",
    "title": "Memory Efficient Matting with Adaptive Token Routing",
    "year": 2024,
    "venue": "AAAI Conference on Artificial Intelligence",
    "authors": [
        "Yiheng Lin",
        "Yihan Hu",
        "Chenyi Zhang",
        "Ting Liu",
        "Xiaochao Qu",
        "Luoqi Liu",
        "Yao Zhao",
        "Yunchao Wei"
    ],
    "doi": "10.48550/arXiv.2412.10702",
    "arxivId": "2412.10702",
    "url": "https://www.semanticscholar.org/paper/f915dfd0719235951ee9e46302ea81e8f05778cf",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Transformer-based models have recently achieved outstanding performance in image matting. However, their application to high-resolution images remains challenging due to the quadratic complexity of global self-attention. To address this issue, we propose MEMatte, a memory-efficient matting framework for processing high-resolution images. MEMatte incorporates a router before each global attention block, directing informative tokens to the global attention while routing other tokens to a Lightweight Token Refinement Module (LTRM). Specifically, the router employs a local-global strategy to predict the routing probability of each token, and the LTRM utilizes efficient modules to simulate global attention. Additionally, we introduce a Batch-constrained Adaptive Token Routing (BATR) mechanism, which allows each router to dynamically route tokens based on image content and the stages of attention block in the network. Furthermore, we construct an ultra high-resolution image matting dataset, UHR-395, comprising 35,500 training images and 1,000 test images, with an average resolution of 4872 Ã— 6017. This dataset is created by compositing 395 different alpha mattes across 11 categories onto various backgrounds, all with high-quality manual annotation. Extensive experiments demonstrate that MEMatte outperforms existing methods on both high-resolution and real-world datasets, significantly reducing memory usage by approximately 88% and latency by 50% on the Composition-1K benchmark.",
    "citationCount": 0,
    "referenceCount": 31
}