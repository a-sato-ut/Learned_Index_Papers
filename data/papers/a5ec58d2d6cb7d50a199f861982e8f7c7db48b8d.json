{
    "paperId": "a5ec58d2d6cb7d50a199f861982e8f7c7db48b8d",
    "title": "Learning Robust Dense Retrieval Models from Incomplete Relevance Labels",
    "year": 2021,
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "authors": [
        "P. Prakash",
        "Julian Killingback",
        "Hamed Zamani"
    ],
    "doi": "10.1145/3404835.3463106",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/a5ec58d2d6cb7d50a199f861982e8f7c7db48b8d",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Recent deployment of efficient billion-scale approximate nearest neighbor (ANN) search algorithms on GPUs has motivated information retrieval researchers to develop neural ranking models that learn low-dimensional dense representations for queries and documents and use ANN search for retrieval. However, optimizing these dense retrieval models poses several challenges including negative sampling for (pair-wise) training. A recent model, called ANCE, successfully uses dynamic negative sampling using ANN search. This paper improves upon ANCE by proposing a robust negative sampling strategy for scenarios where the training data lacks complete relevance annotations. This is of particular importance as obtaining large-scale training data with complete relevance judgment is extremely expensive. Our model uses a small validation set with complete relevance judgments to accurately estimate a negative sampling distribution for dense retrieval models. We also explore leveraging a lexical matching signal during training and pseudo-relevance feedback during evaluation for improved performance. Our experiments on the TREC Deep Learning Track benchmarks demonstrate the effectiveness of our solutions.",
    "citationCount": 22,
    "referenceCount": 23
}