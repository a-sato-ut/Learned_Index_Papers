{
    "paperId": "88bb275161782df0d0873de01797964e6ca7b9d4",
    "title": "Software-hardware co-design for fast and scalable training of deep learning recommendation models",
    "year": 2021,
    "venue": "International Symposium on Computer Architecture",
    "authors": [
        "Dheevatsa Mudigere",
        "Y. Hao",
        "Jianyu Huang",
        "Zhihao Jia",
        "Andrew Tulloch",
        "Srinivas Sridharan",
        "Xing Liu",
        "Mustafa Ozdal",
        "Jade Nie",
        "Jongsoo Park",
        "Liangchen Luo",
        "J. Yang",
        "Leon Gao",
        "Dmytro Ivchenko",
        "Aarti Basant",
        "Yuxi Hu",
        "Jiyan Yang",
        "E. K. Ardestani",
        "Xiaodong Wang",
        "Rakesh Komuravelli",
        "Ching-Hsiang Chu",
        "Serhat Yilmaz",
        "Huayu Li",
        "Jiyuan Qian",
        "Zhuobo Feng",
        "Yi-An Ma",
        "Junjie Yang",
        "Ellie Wen",
        "Hong Li",
        "Lin Yang",
        "Chonglin Sun",
        "Whitney Zhao",
        "Dimitry Melts",
        "Krishnaveni Dhulipala",
        "Kranthi G. Kishore",
        "Tyler N. Graf",
        "Assaf Eisenman",
        "Kiran Kumar Matam",
        "Adi Gangidi",
        "Guoqiang Jerry Chen",
        "M. Krishnan",
        "A. Nayak",
        "Krishnakumar Nair",
        "Bharath Muthiah",
        "Mahmoud khorashadi",
        "P. Bhattacharya",
        "Petr Lapukhov",
        "M. Naumov",
        "A. Mathews",
        "Lin Qiao",
        "M. Smelyanskiy",
        "Bill Jia",
        "Vijay Rao"
    ],
    "doi": "10.1145/3470496.3533727",
    "arxivId": "2104.05158",
    "url": "https://www.semanticscholar.org/paper/88bb275161782df0d0873de01797964e6ca7b9d4",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2104.05158",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Deep learning recommendation models (DLRMs) have been used across many business-critical services at Meta and are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper, we present Neo, a software-hardware co-designed system for high-performance distributed training of large-scale DLRMs. Neo employs a novel 4D parallelism strategy that combines table-wise, row-wise, column-wise, and data parallelism for training massive embedding operators in DLRMs. In addition, Neo enables extremely high-performance and memory-efficient embedding computations using a variety of critical systems optimizations, including hybrid kernel fusion, software-managed caching, and quality-preserving compression. Finally, Neo is paired with ZionEX, a new hardware platform co-designed with Neo's 4D parallelism for optimizing communications for large-scale DLRM training. Our evaluation on 128 GPUs using 16 ZionEX nodes shows that Neo outperforms existing systems by up to 40Ã— for training 12-trillion-parameter DLRM models deployed in production.",
    "citationCount": 170,
    "referenceCount": 75
}