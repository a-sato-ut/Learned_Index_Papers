{
    "paperId": "16dad7e64183374af1ba3ce6db82a76299f0cfcb",
    "title": "AMAZe: A Multi-Agent Zero-shot Index Advisor for Relational Databases",
    "year": 2025,
    "venue": "",
    "authors": [
        "Zhaodonghui Li",
        "Haitao Yuan",
        "Jiachen Shi",
        "Hao Zhang",
        "Yu Rong",
        "Gao Cong"
    ],
    "doi": null,
    "arxivId": "2508.16044",
    "url": "https://www.semanticscholar.org/paper/16dad7e64183374af1ba3ce6db82a76299f0cfcb",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": null,
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Index recommendation is one of the most important problems in database management system (DBMS) optimization. Given queries and certain index-related constraints, traditional methods rely on heuristic optimization or learning-based models to select effective indexes and improve query performance. However, heuristic optimization suffers from high computation time, and learning-based models lose generalisability due to training for different workloads and database schemas. With the recent rapid development of large language models (LLMs), methods using prompt tuning have been proposed to enhance the efficiency of index selection. However, such methods still can not achieve the state-of-the-art (SOTA) results, and preparing the index selection demonstrations is also resource-intensive. To address these issues, we propose AMAZe, a zero-shot LLM-based index advisor with a multi-agent framework. We decompose the index recommendation problem into sub-steps, including planning, selection, combination, revision, and reflection. A set of LLM-embedded agents is designed to handle each one of the different sub-steps. Our method utilizes high-level agents to control the index selection process and low-level agents to select and revise indexes. Through extensive experiments, we show that our proposed AMAZe not only achieves the SOTA performance compared to the heuristic methods, but also outperforms learning-based and prompt-based methods with higher efficiency and better zero-shot inference ability.",
    "citationCount": 1,
    "referenceCount": 44
}