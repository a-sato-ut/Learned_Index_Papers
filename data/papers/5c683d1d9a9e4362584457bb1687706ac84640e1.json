{
    "paperId": "5c683d1d9a9e4362584457bb1687706ac84640e1",
    "title": "MMQ: Multimodal Mixture-of-Quantization Tokenization for Semantic ID Generation and User Behavioral Adaptation",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Yi Xu",
        "Moyu Zhang",
        "Chenxuan Li",
        "Zhihao Liao",
        "Haibo Xing",
        "Hao Deng",
        "Jinxin Hu",
        "Yu Zhang",
        "Xiaoyi Zeng",
        "Jing Zhang"
    ],
    "doi": "10.48550/arXiv.2508.15281",
    "arxivId": "2508.15281",
    "url": "https://www.semanticscholar.org/paper/5c683d1d9a9e4362584457bb1687706ac84640e1",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Recommender systems traditionally represent items using unique identifiers (ItemIDs), but this approach struggles with large, dynamic item corpora and sparse long-tail data, limiting scalability and generalization. Semantic IDs, derived from multimodal content such as text and images, offer a promising alternative by mapping items into a shared semantic space, enabling knowledge transfer and improving recommendations for new or rare items. However, existing methods face two key challenges: (1) balancing cross-modal synergy with modality-specific uniqueness, and (2) bridging the semantic-behavioral gap, where semantic representations may misalign with actual user preferences. To address these challenges, we propose Multimodal Mixture-of-Quantization (MMQ), a two-stage framework that trains a novel multimodal tokenizer. First, a shared-specific tokenizer leverages a multi-expert architecture with modality-specific and modality-shared experts, using orthogonal regularization to capture comprehensive multimodal information. Second, behavior-aware fine-tuning dynamically adapts semantic IDs to downstream recommendation objectives while preserving modality information through a multimodal reconstruction loss. Extensive offline experiments and online A/B tests demonstrate that MMQ effectively unifies multimodal synergy, specificity, and behavioral adaptation, providing a scalable and versatile solution for both generative retrieval and discriminative ranking tasks.",
    "citationCount": 1,
    "referenceCount": 65
}