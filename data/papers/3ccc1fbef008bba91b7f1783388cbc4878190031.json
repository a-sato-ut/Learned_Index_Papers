{
    "paperId": "3ccc1fbef008bba91b7f1783388cbc4878190031",
    "title": "Explainable Reinforcement Learning: A Survey and Comparative Review",
    "year": 2023,
    "venue": "ACM Computing Surveys",
    "authors": [
        "Stephanie Milani",
        "Nicholay Topin",
        "M. Veloso",
        "Fei Fang"
    ],
    "doi": "10.1145/3616864",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/3ccc1fbef008bba91b7f1783388cbc4878190031",
    "isOpenAccess": true,
    "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3616864",
    "publicationTypes": [
        "JournalArticle",
        "Review"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Explainable reinforcement learning (XRL) is an emerging subfield of explainable machine learning that has attracted considerable attention in recent years. The goal of XRL is to elucidate the decision-making process of reinforcement learning (RL) agents in sequential decision-making settings. Equipped with this information, practitioners can better understand important questions about RL agents (especially those deployed in the real world), such as what the agents will do and why. Despite increased interest, there exists a gap in the literature for organizing the plethora of papersâ€”especially in a way that centers the sequential decision-making nature of the problem. In this survey, we propose a novel taxonomy for organizing the XRL literature that prioritizes the RL setting. We propose three high-level categories: feature importance, learning process and Markov decision process, and policy-level. We overview techniques according to this taxonomy, highlighting challenges and opportunities for future work. We conclude by using these gaps to motivate and outline a roadmap for future work.",
    "citationCount": 146,
    "referenceCount": 174
}