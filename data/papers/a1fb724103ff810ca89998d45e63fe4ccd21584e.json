{
    "paperId": "a1fb724103ff810ca89998d45e63fe4ccd21584e",
    "title": "Energy-Based Models for Continual Learning",
    "year": 2020,
    "venue": "CoLLAs",
    "authors": [
        "Shuang Li",
        "Yilun Du",
        "Gido M. van de Ven",
        "A. Torralba",
        "Igor Mordatch"
    ],
    "doi": null,
    "arxivId": "2011.12216",
    "url": "https://www.semanticscholar.org/paper/a1fb724103ff810ca89998d45e63fe4ccd21584e",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We motivate Energy-Based Models (EBMs) as a promising model class for continual learning problems. Instead of tackling continual learning via the use of external memory, growing models, or regularization, EBMs have a natural way to support a dynamically-growing number of tasks or classes that causes less interference with previously learned information. We find that EBMs outperform the baseline methods by a large margin on several continual learning benchmarks. We also show that EBMs are adaptable to a more general continual learning setting where the data distribution changes without the notion of explicitly delineated tasks. These observations point towards EBMs as a class of models naturally inclined towards the continual learning regime.",
    "citationCount": 44,
    "referenceCount": 95
}