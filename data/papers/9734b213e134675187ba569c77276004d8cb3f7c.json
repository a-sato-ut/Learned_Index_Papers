{
    "paperId": "9734b213e134675187ba569c77276004d8cb3f7c",
    "title": "A sparse iteration space transformation framework for sparse tensor algebra",
    "year": 2020,
    "venue": "Proc. ACM Program. Lang.",
    "authors": [
        "Ryan Senanayake",
        "Changwan Hong",
        "Ziheng Wang",
        "Amalee Wilson",
        "Stephen Chou",
        "Shoaib Kamil",
        "Saman P. Amarasinghe",
        "Fredrik Kjolstad"
    ],
    "doi": "10.1145/3428226",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/9734b213e134675187ba569c77276004d8cb3f7c",
    "isOpenAccess": true,
    "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3428226",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We address the problem of optimizing sparse tensor algebra in a compiler and show how to define standard loop transformations---split, collapse, and reorder---on sparse iteration spaces. The key idea is to track the transformation functions that map the original iteration space to derived iteration spaces. These functions are needed by the code generator to emit code that maps coordinates between iteration spaces at runtime, since the coordinates in the sparse data structures remain in the original iteration space. We further demonstrate that derived iteration spaces can tile both the universe of coordinates and the subset of nonzero coordinates: the former is analogous to tiling dense iteration spaces, while the latter tiles sparse iteration spaces into statically load-balanced blocks of nonzeros. Tiling the space of nonzeros lets the generated code efficiently exploit heterogeneous compute resources such as threads, vector units, and GPUs. We implement these concepts by extending the sparse iteration theory implementation in the TACO system. The associated scheduling API can be used by performance engineers or it can be the target of an automatic scheduling system. We outline one heuristic autoscheduling system, but other systems are possible. Using the scheduling API, we show how to optimize mixed sparse-dense tensor algebra expressions on CPUs and GPUs. Our results show that the sparse transformations are sufficient to generate code with competitive performance to hand-optimized implementations from the literature, while generalizing to all of the tensor algebra.",
    "citationCount": 63,
    "referenceCount": 62
}