{
    "paperId": "7e71c2e64fc36a288f936c99260121ecc24be287",
    "title": "Neural Partial Differential Equations with Functional Convolution",
    "year": 2023,
    "venue": "arXiv.org",
    "authors": [
        "Z. Wu",
        "Xingzhe He",
        "Yijun Li",
        "Cheng Yang",
        "Rui Liu",
        "S. Xiong",
        "Bo Zhu"
    ],
    "doi": "10.48550/arXiv.2303.07194",
    "arxivId": "2303.07194",
    "url": "https://www.semanticscholar.org/paper/7e71c2e64fc36a288f936c99260121ecc24be287",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2303.07194",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        },
        {
            "category": "Physics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We present a lightweighted neural PDE representation to discover the hidden structure and predict the solution of different nonlinear PDEs. Our key idea is to leverage the prior of ``translational similarity'' of numerical PDE differential operators to drastically reduce the scale of learning model and training data. We implemented three central network components, including a neural functional convolution operator, a Picard forward iterative procedure, and an adjoint backward gradient calculator. Our novel paradigm fully leverages the multifaceted priors that stem from the sparse and smooth nature of the physical PDE solution manifold and the various mature numerical techniques such as adjoint solver, linearization, and iterative procedure to accelerate the computation. We demonstrate the efficacy of our method by robustly discovering the model and accurately predicting the solutions of various types of PDEs with small-scale networks and training sets. We highlight that all the PDE examples we showed were trained with up to 8 data samples and within 325 network parameters.",
    "citationCount": 1,
    "referenceCount": 40
}