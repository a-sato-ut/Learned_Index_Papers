{
    "paperId": "187eb8c9e4bf2641df7bb3563ba32fac4bbc4e14",
    "title": "Collapse-Proof Non-Contrastive Self-Supervised Learning",
    "year": 2024,
    "venue": "",
    "authors": [
        "Emanuele Sansone",
        "Tim Lebailly",
        "T. Tuytelaars"
    ],
    "doi": null,
    "arxivId": "2410.04959",
    "url": "https://www.semanticscholar.org/paper/187eb8c9e4bf2641df7bb3563ba32fac4bbc4e14",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": null,
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We present a principled and simplified design of the projector and loss function for non-contrastive self-supervised learning based on hyperdimensional computing. We theoretically demonstrate that this design introduces an inductive bias that encourages representations to be simultaneously decorrelated and clustered, without explicitly enforcing these properties. This bias provably enhances generalization and suffices to avoid known training failure modes, such as representation, dimensional, cluster, and intracluster collapses. We validate our theoretical findings on image datasets, including SVHN, CIFAR-10, CIFAR-100, and ImageNet-100. Our approach effectively combines the strengths of feature decorrelation and cluster-based self-supervised learning methods, overcoming training failure modes while achieving strong generalization in clustering and linear classification tasks.",
    "citationCount": 2,
    "referenceCount": 81
}