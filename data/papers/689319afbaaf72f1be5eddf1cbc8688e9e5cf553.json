{
    "paperId": "689319afbaaf72f1be5eddf1cbc8688e9e5cf553",
    "title": "MaGGIe: Masked Guided Gradual Human Instance Matting",
    "year": 2024,
    "venue": "Computer Vision and Pattern Recognition",
    "authors": [
        "Chuong Huynh",
        "Seoung Wug Oh",
        "Abhinav Shrivastava",
        "Joon-Young Lee"
    ],
    "doi": "10.1109/CVPR52733.2024.00371",
    "arxivId": "2404.16035",
    "url": "https://www.semanticscholar.org/paper/689319afbaaf72f1be5eddf1cbc8688e9e5cf553",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2404.16035",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Human matting is a foundation task in image and video processing where human foreground pixels are extracted from the input. Prior works either improve the accuracy by additional guidance or improve the temporal consistency of a single instance across frames. We propose a new framework MaGGIe, Masked Guided Gradual Human Instance Matting, which predicts alpha mattes progressively for each human instances while maintaining the computational cost, precision, and consistency. Our method leverages modern architectures, including transformer attention and sparse convolution, to output all instance mattes simultaneously without exploding memory and latency. Although keeping constant inference costs in the multiple-instance scenario, our framework achieves robust and versatile performance on our proposed synthesized benchmarks. With the higher quality image and video matting benchmarks, the novel multi-instance synthesis approach from publicly available sources is introduced to increase the generalization of models in real-world scenarios. Our code and datasets are available at https://maggie-matt.github.io.",
    "citationCount": 12,
    "referenceCount": 57
}