{
    "paperId": "1c37b277c0a0e3c4ecf25ac97caac13ae9b268c2",
    "title": "MoLEx: Mixture of LoRA Experts in Speech Self-Supervised Models for Audio Deepfake Detection",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Zihan Pan",
        "Sailor Hardik Bhupendra",
        "Jinyang Wu"
    ],
    "doi": "10.48550/arXiv.2509.09175",
    "arxivId": "2509.09175",
    "url": "https://www.semanticscholar.org/paper/1c37b277c0a0e3c4ecf25ac97caac13ae9b268c2",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "While self-supervised learning (SSL)-based models have boosted audio deepfake detection accuracy, fully finetuning them is computationally expensive. To address this, we propose a parameter-efficient framework that combines Low-Rank Adaptation with a Mixture-of-Experts router, called Mixture of LoRA Experts (MoLEx). It preserves pre-trained knowledge of SSL models while efficiently finetuning only selected experts, reducing training costs while maintaining robust performance. The observed utility of experts during inference shows the router reactivates the same experts for similar attacks but switches to other experts for novel spoofs, confirming MoLEx's domain-aware adaptability. MoLEx additionally offers flexibility for domain adaptation by allowing extra experts to be trained without modifying the entire model. We mainly evaluate our approach on the ASVSpoof 5 dataset and achieve the state-of-the-art (SOTA) equal error rate (EER) of 5.56% on the evaluation set without augmentation.",
    "citationCount": 1,
    "referenceCount": 61
}