{
    "paperId": "868d14ba0e41c2f912bd4548ebee22c172aa78e4",
    "title": "FSGANv2: Improved Subject Agnostic Face Swapping and Reenactment",
    "year": 2022,
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "authors": [
        "Y. Nirkin",
        "Y. Keller",
        "Tal Hassner"
    ],
    "doi": "10.1109/TPAMI.2022.3155571",
    "arxivId": "2202.12972",
    "url": "https://www.semanticscholar.org/paper/868d14ba0e41c2f912bd4548ebee22c172aa78e4",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2202.12972",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Medicine",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We present Face Swapping GAN (FSGAN) for face swapping and reenactment. Unlike previous work, we offer a subject agnostic swapping scheme that can be applied to pairs of faces without requiring training on those faces. We derive a novel iterative deep learningâ€“based approach for face reenactment which adjusts significant pose and expression variations that can be applied to a single image or a video sequence. For video sequences, we introduce a continuous interpolation of the face views based on reenactment, Delaunay Triangulation, and barycentric coordinates. Occluded face regions are handled by a face completion network. Finally, we use a face blending network for seamless blending of the two faces while preserving the target skin color and lighting conditions. This network uses a novel Poisson blending loss combining Poisson optimization with a perceptual loss. We compare our approach to existing state-of-the-art systems and show our results to be both qualitatively and quantitatively superior. This work describes extensions of the FSGAN method, proposed in an earlier conference version of our work (Nirkin et al. 2019), as well as additional experiments and results.",
    "citationCount": 48,
    "referenceCount": 83
}