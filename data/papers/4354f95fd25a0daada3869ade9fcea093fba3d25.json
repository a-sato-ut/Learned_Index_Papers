{
    "paperId": "4354f95fd25a0daada3869ade9fcea093fba3d25",
    "title": "Towards Constituting Mathematical Structures for Learning to Optimize",
    "year": 2023,
    "venue": "International Conference on Machine Learning",
    "authors": [
        "Jialin Liu",
        "Xiaohan Chen",
        "Zhangyang Wang",
        "W. Yin",
        "Hanqin Cai"
    ],
    "doi": "10.48550/arXiv.2305.18577",
    "arxivId": "2305.18577",
    "url": "https://www.semanticscholar.org/paper/4354f95fd25a0daada3869ade9fcea093fba3d25",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2305.18577",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Learning to Optimize (L2O), a technique that utilizes machine learning to learn an optimization algorithm automatically from data, has gained arising attention in recent years. A generic L2O approach parameterizes the iterative update rule and learns the update direction as a black-box network. While the generic approach is widely applicable, the learned model can overfit and may not generalize well to out-of-distribution test sets. In this paper, we derive the basic mathematical conditions that successful update rules commonly satisfy. Consequently, we propose a novel L2O model with a mathematics-inspired structure that is broadly applicable and generalized well to out-of-distribution problems. Numerical simulations validate our theoretical findings and demonstrate the superior empirical performance of the proposed L2O model.",
    "citationCount": 16,
    "referenceCount": 59
}