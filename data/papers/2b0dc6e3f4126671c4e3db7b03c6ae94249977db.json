{
    "paperId": "2b0dc6e3f4126671c4e3db7b03c6ae94249977db",
    "title": "One-Trimap Video Matting",
    "year": 2022,
    "venue": "European Conference on Computer Vision",
    "authors": [
        "Hongje Seong",
        "Seoung Wug Oh",
        "Brian L. Price",
        "Euntai Kim",
        "Joon-Young Lee"
    ],
    "doi": "10.48550/arXiv.2207.13353",
    "arxivId": "2207.13353",
    "url": "https://www.semanticscholar.org/paper/2b0dc6e3f4126671c4e3db7b03c6ae94249977db",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2207.13353",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Recent studies made great progress in video matting by extending the success of trimap-based image matting to the video domain. In this paper, we push this task toward a more practical setting and propose One-Trimap Video Matting network (OTVM) that performs video matting robustly using only one user-annotated trimap. A key of OTVM is the joint modeling of trimap propagation and alpha prediction. Starting from baseline trimap propagation and alpha prediction networks, our OTVM combines the two networks with an alpha-trimap refinement module to facilitate information flow. We also present an end-to-end training strategy to take full advantage of the joint model. Our joint modeling greatly improves the temporal stability of trimap propagation compared to the previous decoupled methods. We evaluate our model on two latest video matting benchmarks, Deep Video Matting and VideoMatting108, and outperform state-of-the-art by significant margins (MSE improvements of 56.4% and 56.7%, respectively). The source code and model are available online: https://github.com/Hongje/OTVM.",
    "citationCount": 14,
    "referenceCount": 66
}