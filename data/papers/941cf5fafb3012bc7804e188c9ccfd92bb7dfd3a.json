{
    "paperId": "941cf5fafb3012bc7804e188c9ccfd92bb7dfd3a",
    "title": "Learning-Augmented Dynamic Submodular Maximization",
    "year": 2023,
    "venue": "Neural Information Processing Systems",
    "authors": [
        "Arpit Agarwal",
        "Eric Balkanski"
    ],
    "doi": "10.48550/arXiv.2311.13006",
    "arxivId": "2311.13006",
    "url": "https://www.semanticscholar.org/paper/941cf5fafb3012bc7804e188c9ccfd92bb7dfd3a",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "In dynamic submodular maximization, the goal is to maintain a high-value solution over a sequence of element insertions and deletions with a fast update time. Motivated by large-scale applications and the fact that dynamic data often exhibits patterns, we ask the following question: can predictions be used to accelerate the update time of dynamic submodular maximization algorithms? We consider the model for dynamic algorithms with predictions where predictions regarding the insertion and deletion times of elements can be used for preprocessing. Our main result is an algorithm with an $O(poly(\\log \\eta, \\log w, \\log k))$ amortized update time over the sequence of updates that achieves a $1/2 - \\epsilon$ approximation in expectation for dynamic monotone submodular maximization under a cardinality constraint $k$, where the prediction error $\\eta$ is the number of elements that are not inserted and deleted within $w$ time steps of their predicted insertion and deletion times. This amortized update time is independent of the length of the stream and instead depends on the prediction error.",
    "citationCount": 2,
    "referenceCount": 41
}