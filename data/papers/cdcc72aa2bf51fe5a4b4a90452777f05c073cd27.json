{
    "paperId": "cdcc72aa2bf51fe5a4b4a90452777f05c073cd27",
    "title": "Leveraging Reward Consistency for Interpretable Feature Discovery in Reinforcement Learning",
    "year": 2023,
    "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems",
    "authors": [
        "Qisen Yang",
        "Huanqian Wang",
        "Mukun Tong",
        "Wenjie Shi",
        "Gao Huang",
        "Shiji Song"
    ],
    "doi": "10.1109/TSMC.2023.3312411",
    "arxivId": "2309.01458",
    "url": "https://www.semanticscholar.org/paper/cdcc72aa2bf51fe5a4b4a90452777f05c073cd27",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2309.01458",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The black-box nature of deep reinforcement learning (RL) hinders them from real-world applications. Therefore, interpreting and explaining RL agents have been active research topics in recent years. Existing methods for post-hoc explanations usually adopt the action matching principle to enable an easy understanding of vision-based RL agents. In this article, it is argued that the commonly used action matching principle is more like an explanation of deep neural networks (DNNs) than the interpretation of RL agents. It may lead to irrelevant or misplaced feature attribution when different DNNs’ outputs lead to the same rewards or different rewards result from the same outputs. Therefore, we propose to consider rewards, the essential objective of RL agents, as the essential objective of interpreting RL agents as well. To ensure reward consistency during interpretable feature discovery, a novel framework (RL interpreting RL, denoted as RL-in-RL) is proposed to solve the gradient disconnection from actions to rewards. We verify and evaluate our method on the Atari 2600 games as well as Duckietown, a challenging self-driving car simulator environment. The results show that our method manages to keep reward (or return) consistency and achieves high-quality feature attribution. Further, a series of analytical experiments validate our assumption of the action matching principle’s limitations.",
    "citationCount": 5,
    "referenceCount": 76
}