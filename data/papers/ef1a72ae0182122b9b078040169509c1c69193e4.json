{
    "paperId": "ef1a72ae0182122b9b078040169509c1c69193e4",
    "title": "Boosting Robustness of Image Matting with Context Assembling and Strong Data Augmentation",
    "year": 2022,
    "venue": "Computer Vision and Pattern Recognition",
    "authors": [
        "Yutong Dai",
        "Brian L. Price",
        "He Zhang",
        "Chunhua Shen"
    ],
    "doi": "10.1109/CVPR52688.2022.01141",
    "arxivId": "2201.06889",
    "url": "https://www.semanticscholar.org/paper/ef1a72ae0182122b9b078040169509c1c69193e4",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2201.06889",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Deep image matting methods have achieved increasingly better results on benchmarks (e.g., Composition-1k/alphamatting.com). However, the robustness, including robustness to trimaps and generalization to images from different domains, is still underexplored. Although some works propose to either refine the trimaps or adapt the algorithms to real-world images via extra data augmentation, none of them has taken both into consideration, not to mention the significant performance deterioration on benchmarks while using those data augmentation. To fill this gap, we propose an image matting method which achieves higher robustness (RMat) via multilevel context assembling and strong data augmentation targeting matting. Specifically, we first build a strong matting framework by modeling ample global information with transformer blocks in the encoder, and focusing on details in combination with convolution layers as well as a low-level feature assembling attention block in the decoder. Then, based on this strong baseline, we analyze current data augmentation and explore simple but effective strong data augmentation to boost the baseline model and contribute a more generalizable matting method. Compared with previous methods, the proposed method not only achieves state-of-the-art results on the Composition-1k benchmark (11 % improvement on SAD and 27% improvement on Grad) with smaller model size, but also shows more robust generalization results on other benchmarks, on real-world images, and also on varying coarse-to-fine trimaps with our extensive experiments.11This work was in part done when YD was an intern at Adobe and CS was with The University of Adelaide. CS is the corresponding author. Project page: https://dongdong93.github.io/RMat/.",
    "citationCount": 30,
    "referenceCount": 49
}