{
    "paperId": "d4d461288e76c7b6a36ffd6b66c26815bc7bd8e6",
    "title": "STen: Productive and Efficient Sparsity in PyTorch",
    "year": 2023,
    "venue": "arXiv.org",
    "authors": [
        "Andrei Ivanov",
        "Nikoli Dryden",
        "Tal Ben-Nun",
        "Saleh Ashkboos",
        "T. Hoefler"
    ],
    "doi": "10.48550/arXiv.2304.07613",
    "arxivId": "2304.07613",
    "url": "https://www.semanticscholar.org/paper/d4d461288e76c7b6a36ffd6b66c26815bc7bd8e6",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2304.07613",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "As deep learning models grow, sparsity is becoming an increasingly critical component of deep neural networks, enabling improved performance and reduced storage. However, existing frameworks offer poor support for sparsity. Specialized sparsity engines focus exclusively on sparse inference, while general frameworks primarily focus on sparse tensors in classical formats and neglect the broader sparsification pipeline necessary for using sparse models, especially during training. Further, existing frameworks are not easily extensible: adding a new sparse tensor format or operator is challenging and time-consuming. To address this, we propose STen, a sparsity programming model and interface for PyTorch, which incorporates sparsity layouts, operators, and sparsifiers, in an efficient, customizable, and extensible framework that supports virtually all sparsification methods. We demonstrate this by developing a high-performance grouped n:m sparsity layout for CPU inference at moderate sparsity. STen brings high performance and ease of use to the ML community, making sparsity easily accessible.",
    "citationCount": 6,
    "referenceCount": 76
}