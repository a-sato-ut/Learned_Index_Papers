{
    "paperId": "1a4fa89fd92148edcb691abecfcc06af2158e310",
    "title": "SGM-Net: Semantic Guided Matting Net",
    "year": 2022,
    "venue": "arXiv.org",
    "authors": [
        "Qing-Huang Song",
        "Wen Sun",
        "Donghan Yang",
        "Mengjie Hu",
        "Chun Liu"
    ],
    "doi": "10.48550/arXiv.2208.07496",
    "arxivId": "2208.07496",
    "url": "https://www.semanticscholar.org/paper/1a4fa89fd92148edcb691abecfcc06af2158e310",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2208.07496",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Human matting refers to extracting human parts from natural images with high quality, including human detail information such as hair, glasses, hat, etc. This technology plays an essential role in image synthesis and visual effects in the film industry. When the green screen is not available, the existing human matting methods need the help of additional inputs (such as trimap, background image, etc.), or the model with high computational cost and complex network structure, which brings great difficulties to the application of human matting in practice. To alleviate such problems, most existing methods (such as MODNet) use multi-branches to pave the way for matting through segmentation, but these methods do not make full use of the image features and only utilize the prediction results of the network as guidance information. Therefore, we propose a module to generate foreground probability map and add it to MODNet to obtain Semantic Guided Matting Net (SGM-Net). Under the condition of only one image, we can realize the human matting task. We verify our method on the P3M-10k dataset. Compared with the benchmark, our method has significantly improved in various evaluation indicators.",
    "citationCount": 2,
    "referenceCount": 48
}