{
    "paperId": "1d1f04ba91b9d4125d0e7fbf7af18c5196f9c163",
    "title": "Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Jindong Li",
        "Yali Fu",
        "Jiahong Liu",
        "Linxiao Cao",
        "Wei Ji",
        "Menglin Yang",
        "Irwin King",
        "Ming-Hsuan Yang"
    ],
    "doi": "10.48550/arXiv.2507.22920",
    "arxivId": "2507.22920",
    "url": "https://www.semanticscholar.org/paper/1d1f04ba91b9d4125d0e7fbf7af18c5196f9c163",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Review"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Linguistics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The rapid advancement of large language models (LLMs) has intensified the need for effective mechanisms to transform continuous multimodal data into discrete representations suitable for language-based processing. Discrete tokenization, with vector quantization (VQ) as a central approach, offers both computational efficiency and compatibility with LLM architectures. Despite its growing importance, there is a lack of a comprehensive survey that systematically examines VQ techniques in the context of LLM-based systems. This work fills this gap by presenting the first structured taxonomy and analysis of discrete tokenization methods designed for LLMs. We categorize 8 representative VQ variants that span classical and modern paradigms and analyze their algorithmic principles, training dynamics, and integration challenges with LLM pipelines. Beyond algorithm-level investigation, we discuss existing research in terms of classical applications without LLMs, LLM-based single-modality systems, and LLM-based multimodal systems, highlighting how quantization strategies influence alignment, reasoning, and generation performance. In addition, we identify key challenges including codebook collapse, unstable gradient estimation, and modality-specific encoding constraints. Finally, we discuss emerging research directions such as dynamic and task-adaptive quantization, unified tokenization frameworks, and biologically inspired codebook learning. This survey bridges the gap between traditional vector quantization and modern LLM applications, serving as a foundational reference for the development of efficient and generalizable multimodal systems. A continuously updated version is available at: https://github.com/jindongli-Ai/LLM-Discrete-Tokenization-Survey.",
    "citationCount": 0,
    "referenceCount": 258
}