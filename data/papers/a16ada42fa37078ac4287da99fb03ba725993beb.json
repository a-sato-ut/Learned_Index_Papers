{
    "paperId": "a16ada42fa37078ac4287da99fb03ba725993beb",
    "title": "AKG: automatic kernel generation for neural processing units using polyhedral transformations",
    "year": 2021,
    "venue": "ACM-SIGPLAN Symposium on Programming Language Design and Implementation",
    "authors": [
        "Jieyu Zhao",
        "Bojie Li",
        "Wang Nie",
        "Zhen Geng",
        "Renwei Zhang",
        "Xiong Gao",
        "Bin Cheng",
        "Chen Wu",
        "Yun Cheng",
        "Zheng Li",
        "Peng Di",
        "Kun Zhang",
        "Xuefeng Jin"
    ],
    "doi": "10.1145/3453483.3454106",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/a16ada42fa37078ac4287da99fb03ba725993beb",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Existing tensor compilers have proven their effectiveness in deploying deep neural networks on general-purpose hardware like CPU and GPU, but optimizing for neural processing units (NPUs) is still challenging due to the heterogeneous compute units and complicated memory hierarchy. In this paper, we present AKG, a tensor compiler for NPUs. AKG first lowers the tensor expression language to a polyhedral representation, which is used to automate the memory management of NPUs. Unlike existing approaches that resort to manually written schedules, AKG leverages polyhedral schedulers to perform a much wider class of transformations, and extends the semantics of the polyhedral representation to combine complex tiling techniques and hierarchical fusion strategies. We also implement the domain-specific optimization of convolution in AKG. Moreover, to achieve the optimal performance, we introduce complementary optimizations in code generation, which is followed by an auto-tuner. We conduct extensive experiments on benchmarks ranging from single operators to end-to-end networks. The experimental results show that AKG can obtain superior performance to both manual scheduling approaches and vendor provided libraries. We believe AKG will cast a light on the follow-up compiler works on NPUs.",
    "citationCount": 63,
    "referenceCount": 79
}