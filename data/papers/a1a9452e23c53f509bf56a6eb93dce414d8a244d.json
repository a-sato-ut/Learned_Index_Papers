{
    "paperId": "a1a9452e23c53f509bf56a6eb93dce414d8a244d",
    "title": "Malleus: Straggler-Resilient Hybrid Parallel Training of Large-scale Models via Malleable Data and Model Parallelization",
    "year": 2024,
    "venue": "Proc. ACM Manag. Data",
    "authors": [
        "Haoyang Li",
        "Fangcheng Fu",
        "Hao Ge",
        "Sheng Lin",
        "Xuanyu Wang",
        "Jiawen Niu",
        "Yujie Wang",
        "Hailin Zhang",
        "Xiaonan Nie",
        "Bin Cui"
    ],
    "doi": "10.1145/3725322",
    "arxivId": "2410.13333",
    "url": "https://www.semanticscholar.org/paper/a1a9452e23c53f509bf56a6eb93dce414d8a244d",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "As the scale of models and training data continues to grow, there is an expanding reliance on more GPUs to train large-scale models, which inevitably increases the likelihood of encountering dynamic stragglers that some devices lag behind in performance occasionally. However, hybrid parallel training, one of the de facto paradigms to train large models, is typically sensitive to the stragglers. This paper presents Malleus, a straggler-resilient hybrid parallel training framework for large-scale models. Malleus quantifies the stragglers at the nuanced, per-GPU granularity during training, and develops a novel planning algorithm to deduce the optimal parallelization of GPU devices, pipeline stages, model layers, and training data, maximizing training efficiency when stragglers exist. In addition, once a shift in the straggler situation is detected, Malleus adaptively adjusts the parallelization via a re-planning process, and seamlessly and efficiently migrates the model states on the fly, without sacrificing the stability of the training tasks. Empirical results on large language models with up to 110B parameters show that Malleus consistently outperforms existing parallel training frameworks under various straggler situations, delivering on average 2.63-5.28x of efficiency improvement.",
    "citationCount": 6,
    "referenceCount": 94
}