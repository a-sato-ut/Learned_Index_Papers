{
    "paperId": "9a7a9794af00bfc8017496a31d650e7c4e38e6e9",
    "title": "LXR: Learning to eXplain Recommendations",
    "year": 2025,
    "venue": "ACM Transactions on Recommender Systems",
    "authors": [
        "Liya Gurevitch",
        "Veronika Bogina",
        "Oren Barkan",
        "Yahlly Schein",
        "Yehonatan Elisha",
        "Noam Koenigstein"
    ],
    "doi": "10.1145/3732292",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/9a7a9794af00bfc8017496a31d650e7c4e38e6e9",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Recommender systems have become integral to many online services, leveraging user data to provide personalized recommendations. However, as these systems grow in complexity, understanding the rationale behind their recommendations becomes increasingly difficult. Explainable Artificial Intelligence (XAI) has emerged as a crucial field addressing this challenge, particularly in ensuring transparency and trustworthiness in automated decision-making processes. In this paper, we introduce Learning to eXplain Recommendations (LXR), a scalable, model-agnostic framework designed to generate counterfactually correct explanations for recommender systems. LXR generates explanations for recommendations produced by any differentiable recommender system. By leveraging both factual and counterfactual loss terms, LXR offers robust, accurate, and computationally efficient explanations that reflect the model’s internal decision-making process. A key feature of LXR is its focus on the factual correctness of explanations through counterfactual reasoning, bridging the gap between plausible and accurate explanations. Unlike traditional approaches that rely on exhaustive perturbations of user data, LXR uses a self-supervised learning method to generate explanations efficiently, without sacrificing accuracy. LXR operates in two stages: a pre-training step and a novel Inference-Time Fine-tuning (ITF) step that refines explanations at the individual recommendation level, significantly improving accuracy with minimal computational overhead. Additionally, LXR is applied to hybrid recommender models incorporating demographic data, demonstrating its versatility across real-world scenarios. Finally, we also showcase LXR’s ability to explain recommendations at various ranks within a user’s recommendation list. As a secondary contribution, we introduce several novel evaluation metrics, inspired by saliency maps from computer vision, to rigorously assess the counterfactual correctness of explanations in recommender systems. Our results demonstrate that LXR sets a new benchmark for explainability, providing accurate, transparent, and interpretable explanations. The code is available on our GitHub repository: https://github.com/DeltaLabTLV/LXR.",
    "citationCount": 1,
    "referenceCount": 43
}