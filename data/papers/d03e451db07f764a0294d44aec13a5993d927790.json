{
    "paperId": "d03e451db07f764a0294d44aec13a5993d927790",
    "title": "OpenML Benchmarking Suites",
    "year": 2017,
    "venue": "NeurIPS Datasets and Benchmarks",
    "authors": [
        "B. Bischl",
        "Giuseppe Casalicchio",
        "Matthias Feurer",
        "F. Hutter",
        "Michel Lang",
        "R. G. Mantovani",
        "J. N. V. Rijn",
        "J. Vanschoren"
    ],
    "doi": null,
    "arxivId": "1708.03731",
    "url": "https://www.semanticscholar.org/paper/d03e451db07f764a0294d44aec13a5993d927790",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Machine learning research depends on objectively interpretable, comparable, and reproducible algorithm benchmarks. Therefore, we advocate the use of curated, comprehensive suites of machine learning tasks to standardize the setup, execution, and reporting of benchmarks. We enable this through software tools that help to create and leverage these benchmarking suites. These are seamlessly integrated into the OpenML platform, and accessible through interfaces in Python, Java, and R. OpenML benchmarking suites are (a) easy to use through standardized data formats, APIs, and client libraries; (b) machine-readable, with extensive meta-information on the included datasets; and (c) allow benchmarks to be shared and reused in future studies. We also present a first, carefully curated and practical benchmarking suite for classification: the OpenML Curated Classification benchmarking suite 2018 (OpenML-CC18).",
    "citationCount": 181,
    "referenceCount": 76
}