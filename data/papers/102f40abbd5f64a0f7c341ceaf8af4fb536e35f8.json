{
    "paperId": "102f40abbd5f64a0f7c341ceaf8af4fb536e35f8",
    "title": "Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation",
    "year": 2020,
    "venue": "arXiv.org",
    "authors": [
        "Sebastian Hofstätter",
        "Sophia Althammer",
        "M. Schröder",
        "Mete Sertkan",
        "A. Hanbury"
    ],
    "doi": null,
    "arxivId": "2010.02666",
    "url": "https://www.semanticscholar.org/paper/102f40abbd5f64a0f7c341ceaf8af4fb536e35f8",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The latency of neural ranking models at query time is largely dependent on the architecture and deliberate choices by their designers to trade-off effectiveness for higher efficiency. This focus on low query latency of a rising number of efficient ranking architectures make them feasible for production deployment. In machine learning an increasingly common approach to close the effectiveness gap of more efficient models is to apply knowledge distillation from a large teacher model to a smaller student model. We find that different ranking architectures tend to produce output scores in different magnitudes. Based on this finding, we propose a cross-architecture training procedure with a margin focused loss (Margin-MSE), that adapts knowledge distillation to the varying score output distributions of different BERT and non-BERT ranking architectures. We apply the teachable information as additional fine-grained labels to existing training triples of the MSMARCO-Passage collection. We evaluate our procedure of distilling knowledge from state-of-the-art concatenated BERT models to four different efficient architectures (TK, ColBERT, PreTT, and a BERT CLS dot product model). We show that across our evaluated architectures our Margin-MSE knowledge distillation significantly improves their effectiveness without compromising their efficiency. To benefit the community, we publish the costly teacher-score training files in a ready-to-use package.",
    "citationCount": 255,
    "referenceCount": 47
}