{
    "paperId": "22dfd2b9b0f55fc8c0aea5ffc983b31cc14c9468",
    "title": "Interpretable Ranking with Generalized Additive Models",
    "year": 2021,
    "venue": "Web Search and Data Mining",
    "authors": [
        "Honglei Zhuang",
        "Xuanhui Wang",
        "Michael Bendersky",
        "Alexander Grushetsky",
        "Yonghui Wu",
        "Petr Mitrichev",
        "Ethan Sterling",
        "Nathan Bell",
        "Walker Ravina",
        "Hai Qian"
    ],
    "doi": "10.1145/3437963.3441796",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/22dfd2b9b0f55fc8c0aea5ffc983b31cc14c9468",
    "isOpenAccess": true,
    "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3437963.3441796",
    "publicationTypes": [
        "Book",
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Interpretability of ranking models is a crucial yet relatively under-examined research area. Recent progress on this area largely focuses on generating post-hoc explanations for existing black-box ranking models. Though promising, such post-hoc methods cannot provide sufficiently accurate explanations in general, which makes them infeasible in many high-stakes scenarios, especially the ones with legal or policy constraints. Thus, building an intrinsically interpretable ranking model with transparent, self-explainable structure becomes necessary, but this remains less explored in the learning-to-rank setting. In this paper, we lay the groundwork for intrinsically interpretable learning-to-rank by introducing generalized additive models (GAMs) into ranking tasks. Generalized additive models (GAMs) are intrinsically interpretable machine learning models and have been extensively studied on regression and classification tasks. We study how to extend GAMs into ranking models which can handle both item-level and list-level features and propose a novel formulation of ranking GAMs. To instantiate ranking GAMs, we employ neural networks instead of traditional splines or regression trees. We also show that our neural ranking GAMs can be distilled into a set of simple and compact piece-wise linear functions that are much more efficient to evaluate with little accuracy loss. We conduct experiments on three data sets and show that our proposed neural ranking GAMs can outperform other traditional GAM baselines while maintaining similar interpretability.",
    "citationCount": 37,
    "referenceCount": 62
}