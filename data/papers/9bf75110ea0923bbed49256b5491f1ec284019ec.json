{
    "paperId": "9bf75110ea0923bbed49256b5491f1ec284019ec",
    "title": "From BERT to GPT-3 Codex: Harnessing the Potential of Very Large Language Models for Data Management",
    "year": 2022,
    "venue": "Proceedings of the VLDB Endowment",
    "authors": [
        "Immanuel Trummer"
    ],
    "doi": "10.14778/3554821.3554896",
    "arxivId": "2306.09339",
    "url": "https://www.semanticscholar.org/paper/9bf75110ea0923bbed49256b5491f1ec284019ec",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2306.09339",
    "publicationTypes": [
        "JournalArticle",
        "Review"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Large language models have recently advanced the state of the art on many natural language processing benchmarks. The newest generation of models can be applied to a variety of tasks with little to no specialized training. This technology creates various opportunities for applications in the context of data management.\n The tutorial will introduce participants to basic background on language models, discuss different methods to use language models, and give an overview and short demonstration of available libraries and APIs. Models for generating natural language will be considered as well as models, such as GPT-3 Codex, which complete program code or generate code from natural language instructions. Finally, the tutorial will discuss recent research in the database community that exploits language models in the context of traditional database systems or proposes novel system architectures that are based on them.\n The tutorial is targeted at database researchers. No prior background on language models is required. The goal of the tutorial is to introduce database researchers to the latest generation of language models, and to their use cases in the domain of data management.",
    "citationCount": 35,
    "referenceCount": 101
}