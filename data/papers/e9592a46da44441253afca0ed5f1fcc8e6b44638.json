{
    "paperId": "e9592a46da44441253afca0ed5f1fcc8e6b44638",
    "title": "G-Sparse: Compiler-Driven Acceleration for Generalized Sparse Computation for Graph Neural Networks on Modern GPUs",
    "year": 2023,
    "venue": "International Conference on Parallel Architectures and Compilation Techniques",
    "authors": [
        "Yue Jin",
        "Chengying Huan",
        "Heng Zhang",
        "Yongchao Liu",
        "S. Song",
        "Rui Zhao",
        "Yao Zhang",
        "Changhua He",
        "Wenguang Chen"
    ],
    "doi": "10.1109/PACT58117.2023.00020",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/e9592a46da44441253afca0ed5f1fcc8e6b44638",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Graph Neural Network (GNN) learning over non-Euclidean graph data has recently drawn a rapid increase of interest in many domains. Generalized sparse computation is crucial for maximizing the performance of GNN learning, while most recent GNNs primarily focused on optimizing coarse-grained parallelism associated with nodes, edges, and additional feature dimensions. However, efficiently implementing generalized sparse computation is challenging. The performance optimization of generalized sparse computation lacking in-depth architecture-aware design is seldom supported by existing Domain-Specific Languages (DSLs) and is hard to be tuned by experts, which involves substantial trial and error. In this work, we propose G-Sparse, a new compiler framework that extends the popular Halide compiler to enable effective acceleration for generalized sparse computations for GNNs through compiler-driven optimizations and auto-tuning. To facilitate generalized sparse computations, G-Sparse separates algorithms from schedules and introduces several novel sparse computation optimization techniques for modern GPUs, including two-dimensional shared memory optimizations and efficient cost-driven design space exploration and auto-tuning. Extensive evaluation against highly-optimized state-of-the-art sparse computation kernels and on end-to-end GNN training and inference efficiency has demonstrated that our proposed G-Sparse achieves up to a $4.75\\times$ speedup over the state-of-the-art sparse kernels, and a training and inference speedup of $1.37\\times\\sim 2.25\\times$ over three popular GNN frameworks including GCN, GraphSAGE, and GAT. The source code of G-Sparse is publicly available at https://github.com/TuGraph-family/tugraph-db/tree/master/learn.",
    "citationCount": 2,
    "referenceCount": 49
}