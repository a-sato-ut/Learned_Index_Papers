{
    "paperId": "2c114f25982147a72a0e5f9295c87b2e2ea8ba63",
    "title": "Scalable End-to-End ML Platforms: from AutoML to Self-serve",
    "year": 2023,
    "venue": "arXiv.org",
    "authors": [
        "I. Markov",
        "P. Apostolopoulos",
        "Mia Garrard",
        "Tianyu Qie",
        "Yin Huang",
        "Tanvi Gupta",
        "Anika Li",
        "Cesar Cardoso",
        "George Han",
        "Ryan Maghsoudian",
        "Norm Zhou"
    ],
    "doi": "10.48550/arXiv.2302.14139",
    "arxivId": "2302.14139",
    "url": "https://www.semanticscholar.org/paper/2c114f25982147a72a0e5f9295c87b2e2ea8ba63",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2302.14139",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "ML platforms help enable intelligent data-driven applications and maintain them with limited engineering effort. Upon sufficiently broad adoption, such platforms reach economies of scale that bring greater component reuse while improving efficiency of system development and maintenance. For an end-to-end ML platform with broad adoption, scaling relies on pervasive ML automation and system integration to reach the quality we term self-serve that we define with ten requirements and six optional capabilities. With this in mind, we identify long-term goals for platform development, discuss related tradeoffs and future work. Our reasoning is illustrated on two commercially-deployed end-to-end ML platforms that host hundreds of real-time use cases -- one general-purpose and one specialized.",
    "citationCount": 4,
    "referenceCount": 50
}