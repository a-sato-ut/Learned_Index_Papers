{
    "paperId": "3d3b10740b0f90917b4f395eb321a6a6173ce486",
    "title": "Gradient Flow Matching for Learning Update Dynamics in Neural Network Training",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Xiao Shou",
        "Yanna Ding",
        "Jianxi Gao"
    ],
    "doi": "10.48550/arXiv.2505.20221",
    "arxivId": "2505.20221",
    "url": "https://www.semanticscholar.org/paper/3d3b10740b0f90917b4f395eb321a6a6173ce486",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Training deep neural networks remains computationally intensive due to the itera2 tive nature of gradient-based optimization. We propose Gradient Flow Matching (GFM), a continuous-time modeling framework that treats neural network training as a dynamical system governed by learned optimizer-aware vector fields. By leveraging conditional flow matching, GFM captures the underlying update rules of optimizers such as SGD, Adam, and RMSprop, enabling smooth extrapolation of weight trajectories toward convergence. Unlike black-box sequence models, GFM incorporates structural knowledge of gradient-based updates into the learning objective, facilitating accurate forecasting of final weights from partial training sequences. Empirically, GFM achieves forecasting accuracy that is competitive with Transformer-based models and significantly outperforms LSTM and other classical baselines. Furthermore, GFM generalizes across neural architectures and initializations, providing a unified framework for studying optimization dynamics and accelerating convergence prediction.",
    "citationCount": 0,
    "referenceCount": 30
}