{
    "paperId": "3722fbcee2cb4e484a3b22e9687a768f4ee409e0",
    "title": "SwizzlePerf: Hardware-Aware LLMs for GPU Kernel Performance Optimization",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Arya Tschand",
        "Muhammad A. Awad",
        "Ryan Swann",
        "Kesavan Ramakrishnan",
        "Jeffrey Ma",
        "Keith Lowery",
        "Ganesh Dasika",
        "V. Reddi"
    ],
    "doi": "10.48550/arXiv.2508.20258",
    "arxivId": "2508.20258",
    "url": "https://www.semanticscholar.org/paper/3722fbcee2cb4e484a3b22e9687a768f4ee409e0",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Large language models (LLMs) have shown progress in GPU kernel performance engineering using inefficient search-based methods that optimize around runtime. Any existing approach lacks a key characteristic that human performance engineers rely on for near-optimal utilization -- hardware-awareness. By leveraging the workload's specific memory access patterns, architecture specifications, filtered profiling logs, and reflections on historical performance, we can make software-level optimizations that are tailored to the underlying hardware. SwizzlePerf automatically generates spatial optimizations for GPU kernels on disaggregated architectures by giving LLMs explicit hardware-awareness. For a GEMM kernel, SwizzlePerf takes less than 5 minutes to generate the same hardware-specific optimal swizzling pattern that took expert performance engineers 2 weeks to find. On a suite of 10 diverse ML and Science kernels, SwizzlePerf can generate swizzling patterns for 9 of the kernels that achieve up to a 2.06x speedup and 70% improvement in L2 hit rate. This work is the first of many steps toward systematically creating hardware-aware LLM performance engineering agents.",
    "citationCount": 1,
    "referenceCount": 28
}