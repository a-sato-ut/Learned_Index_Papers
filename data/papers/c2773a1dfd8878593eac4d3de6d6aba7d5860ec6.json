{
    "paperId": "c2773a1dfd8878593eac4d3de6d6aba7d5860ec6",
    "title": "M-L2O: Towards Generalizable Learning-to-Optimize by Test-Time Fast Self-Adaptation",
    "year": 2023,
    "venue": "International Conference on Learning Representations",
    "authors": [
        "Junjie Yang",
        "Xuxi Chen",
        "Tianlong Chen",
        "Zhangyang Wang",
        "Yitao Liang"
    ],
    "doi": "10.48550/arXiv.2303.00039",
    "arxivId": "2303.00039",
    "url": "https://www.semanticscholar.org/paper/c2773a1dfd8878593eac4d3de6d6aba7d5860ec6",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2303.00039",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Learning to Optimize (L2O) has drawn increasing attention as it often remarkably accelerates the optimization procedure of complex tasks by ``overfitting\"specific task type, leading to enhanced performance compared to analytical optimizers. Generally, L2O develops a parameterized optimization method (i.e., ``optimizer\") by learning from solving sample problems. This data-driven procedure yields L2O that can efficiently solve problems similar to those seen in training, that is, drawn from the same ``task distribution\". However, such learned optimizers often struggle when new test problems come with a substantially deviation from the training task distribution. This paper investigates a potential solution to this open challenge, by meta-training an L2O optimizer that can perform fast test-time self-adaptation to an out-of-distribution task, in only a few steps. We theoretically characterize the generalization of L2O, and further show that our proposed framework (termed as M-L2O) provably facilitates rapid task adaptation by locating well-adapted initial points for the optimizer weight. Empirical observations on several classic tasks like LASSO and Quadratic, demonstrate that M-L2O converges significantly faster than vanilla L2O with only $5$ steps of adaptation, echoing our theoretical results. Codes are available in https://github.com/VITA-Group/M-L2O.",
    "citationCount": 4,
    "referenceCount": 39
}