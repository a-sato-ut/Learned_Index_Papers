{
    "paperId": "4ebb5616d8856f49320fb302dced260e79f6ea09",
    "title": "Efficient Re-ranking with Cross-encoders via Early Exit",
    "year": 2025,
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "authors": [
        "Francesco Busolin",
        "C. Lucchese",
        "F. M. Nardini",
        "Salvatore Orlando",
        "Raffaele Perego",
        "Salvatore Trani",
        "Alberto Veneri"
    ],
    "doi": "10.1145/3726302.3729962",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/4ebb5616d8856f49320fb302dced260e79f6ea09",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Pre-trained language models based on transformer networks are highly effective for document re-ranking in ad-hoc search. Among these, cross-encoders stand out for their effectiveness, as they process query-document pairs through the entire transformer network to compute ranking scores. However, this traversal is computationally expensive. To address this, prior work has explored early-exit strategies, enabling the model to terminate the traversal of query-document pairs. These techniques rely on learned classifiers, placed after each transformer block, that decide if a query-document pair can be dropped. Diverging from previous approaches, we propose Similarity-based Early Exit (SEE), a novel-non-learned-strategy that exploits the similarities between query and document token embeddings to early-terminate the inference of documents that will most likely be non-relevant to the query. Even though SEE can be used after every transformer block, we show that the best advantage is achieved when applied before the first transformer block, thus saving most of the inference cost for the query-document pairs. Reproducible experiments on 17 public datasets covering in-domain and out-of-domain evaluation show that SEE can be effectively applied to four different cross-encoders, achieving speedups of up to 3.5Ã— with a limited loss in ranking effectiveness.",
    "citationCount": 0,
    "referenceCount": 54
}