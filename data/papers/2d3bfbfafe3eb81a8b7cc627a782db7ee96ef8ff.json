{
    "paperId": "2d3bfbfafe3eb81a8b7cc627a782db7ee96ef8ff",
    "title": "LobRA: Multi-tenant Fine-tuning over Heterogeneous Data",
    "year": 2025,
    "venue": "Proceedings of the VLDB Endowment",
    "authors": [
        "Sheng Lin",
        "Fangcheng Fu",
        "Haoyang Li",
        "Hao Ge",
        "Xuanyu Wang",
        "Jiawen Niu",
        "Yaofeng Tu",
        "Bin Cui"
    ],
    "doi": "10.14778/3742728.3742752",
    "arxivId": "2509.01193",
    "url": "https://www.semanticscholar.org/paper/2d3bfbfafe3eb81a8b7cc627a782db7ee96ef8ff",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "With the breakthrough of Transformer-based pre-trained models, the demand for fine-tuning (FT) to adapt the base pre-trained models to downstream applications continues to grow, so it is essential for service providers to reduce the cost of processing FT requests. Low-rank adaption (LoRA) is a widely used FT technique that only trains small-scale adapters and keeps the base model unaltered, conveying the possibility of processing multiple FT tasks by jointly training different LoRA adapters with a shared base model.\n Nevertheless, through in-depth analysis, we reveal the efficiency of joint FT is dampened by two heterogeneity issues in the training data â€” the sequence length variation and skewness. To tackle these issues, we develop LobRA, a brand new framework that supports processing multiple FT tasks by jointly training LoRA adapters. Two innovative designs are introduced. Firstly, LobRA deploys the FT replicas (i.e., model replicas for FT) with heterogeneous resource usages and parallel configurations, matching the diverse workloads caused by the sequence length variation. Secondly, for each training step, LobRA takes account of the sequence length skewness and dispatches the training data among the heterogeneous FT replicas to achieve workload balance. We conduct experiments to assess the performance of LobRA, validating that it significantly reduces the GPU seconds required for joint FT by 45.03%-60.67%.",
    "citationCount": 2,
    "referenceCount": 75
}