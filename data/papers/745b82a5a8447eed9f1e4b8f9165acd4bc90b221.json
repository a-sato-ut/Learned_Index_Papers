{
    "paperId": "745b82a5a8447eed9f1e4b8f9165acd4bc90b221",
    "title": "ReFiNe: Recursive Field Networks for Cross-Modal Multi-Scene Representation",
    "year": 2024,
    "venue": "International Conference on Computer Graphics and Interactive Techniques",
    "authors": [
        "Sergey Zakharov",
        "Katherine Liu",
        "Adrien Gaidon",
        "Rares Ambrus"
    ],
    "doi": "10.1145/3641519.3657526",
    "arxivId": "2406.04309",
    "url": "https://www.semanticscholar.org/paper/745b82a5a8447eed9f1e4b8f9165acd4bc90b221",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2406.04309",
    "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The common trade-offs of state-of-the-art methods for multi-shape representation (a single model \"packing\" multiple objects) involve trading modeling accuracy against memory and storage. We show how to encode multiple shapes represented as continuous neural fields with a higher degree of precision than previously possible and with low memory usage. Key to our approach is a recursive hierarchical formulation that exploits object self-similarity, leading to a highly compressed and efficient shape latent space. Thanks to the recursive formulation, our method supports spatial and global-to-local latent feature fusion without needing to initialize and maintain auxiliary data structures, while still allowing for continuous field queries to enable applications such as raytracing. In experiments on a set of diverse datasets, we provide compelling qualitative results and demonstrate state-of-the-art multi-scene reconstruction and compression results with a single network per dataset.",
    "citationCount": 2,
    "referenceCount": 83
}