{
    "paperId": "cc4a30d4b2e63e316b79f2a09462530d675c067f",
    "title": "Discovering Editing Rules by Deep Reinforcement Learning",
    "year": 2023,
    "venue": "IEEE International Conference on Data Engineering",
    "authors": [
        "Yinan Mei",
        "Shaoxu Song",
        "Chenguang Fang",
        "Ziheng Wei",
        "Jingyun Fang",
        "Jiang Long"
    ],
    "doi": "10.1109/ICDE55515.2023.00034",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/cc4a30d4b2e63e316b79f2a09462530d675c067f",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Editing rules specify the conditions of applying high quality master data to repair low quality input data. Discovering editing rules, however, is challenging, since it considers not only the well curated master data but also the large-scale input data, an extremely large search space. A natural baseline, namely EnuMiner, costly enumerates the rules with possible conditions from both master and input data. Although several pruning strategies are enabled, the algorithm still takes a long time when the enumeration space is large. To avoid enumerating all candidate rules during mining, we argue to model the rule discovery process as a Markov Decision Process. Specifically, we discover editing rules by growing a rule tree where each node corresponds to a rule. The algorithm generates a new rule from the current node as a child node. We propose a reinforcement learning-based editing rule discovery algorithm, RLMiner, which trains an agent to wisely make decisions on branches when traversing the tree. Following the idea of evaluating rules, we design a reward function that is more in line with rule discovery scenarios and makes our algorithm perform effectively and efficiently. The experimental results show that our proposed RLMiner can mine high-utility editing rules like EnuMiner and scale well on the datasets with many attributes and large domains.",
    "citationCount": 1,
    "referenceCount": 40
}