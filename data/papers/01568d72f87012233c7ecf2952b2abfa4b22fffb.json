{
    "paperId": "01568d72f87012233c7ecf2952b2abfa4b22fffb",
    "title": "Protecting the Protected Group: Circumventing Harmful Fairness",
    "year": 2019,
    "venue": "AAAI Conference on Artificial Intelligence",
    "authors": [
        "Omer Ben-Porat",
        "Fedor Sandomirskiy",
        "Moshe Tennenholtz"
    ],
    "doi": "10.1609/aaai.v35i6.16654",
    "arxivId": "1905.10546",
    "url": "https://www.semanticscholar.org/paper/01568d72f87012233c7ecf2952b2abfa4b22fffb",
    "isOpenAccess": true,
    "openAccessPdf": "https://ojs.aaai.org/index.php/AAAI/article/download/16654/16461",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Economics",
            "source": "s2-fos-model"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The recent literature on fair Machine Learning manifests that the choice of fairness constraints must be driven by the utilities of the population. However, virtually all previous work makes the unrealistic assumption that the exact underlying utilities of the population (representing private tastes of individuals) are known to the regulator that imposes the fairness constraint. In this paper we initiate the discussion of the \\emph{mismatch}, the unavoidable difference between the underlying utilities of the population and the utilities assumed by the regulator. We demonstrate that the mismatch can make the disadvantaged protected group worse off after imposing the fairness constraint and provide tools to design fairness constraints that help the disadvantaged group despite the mismatch.",
    "citationCount": 18,
    "referenceCount": 54
}