{
    "paperId": "7a06ca5e148c6362d982f40388d3919ba15bf9c0",
    "title": "LLM Alignment as Retriever Optimization: An Information Retrieval Perspective",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Bowen Jin",
        "Jinsung Yoon",
        "Zhen Qin",
        "Ziqi Wang",
        "Wei Xiong",
        "Yu Meng",
        "Jiawei Han",
        "Sercan Ã–. Arik"
    ],
    "doi": "10.48550/arXiv.2502.03699",
    "arxivId": "2502.03699",
    "url": "https://www.semanticscholar.org/paper/7a06ca5e148c6362d982f40388d3919ba15bf9c0",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence with capabilities in reasoning, coding, and communication, driving innovation across industries. Their true potential depends on effective alignment to ensure correct, trustworthy and ethical behavior, addressing challenges like misinformation, hallucinations, bias and misuse. While existing Reinforcement Learning (RL)-based alignment methods are notoriously complex, direct optimization approaches offer a simpler alternative. In this work, we introduce a novel direct optimization approach for LLM alignment by drawing on established Information Retrieval (IR) principles. We present a systematic framework that bridges LLM alignment and IR methodologies, mapping LLM generation and reward models to IR's retriever-reranker paradigm. Building on this foundation, we propose LLM Alignment as Retriever Preference Optimization (LarPO), a new alignment method that enhances overall alignment quality. Extensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 % averaged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work opens new avenues for advancing LLM alignment by integrating IR foundations, offering a promising direction for future research.",
    "citationCount": 5,
    "referenceCount": 64
}