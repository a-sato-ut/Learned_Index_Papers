{
    "paperId": "8a5f154d1d5d55623ac54455142281246e298de5",
    "title": "Database Tuning using Natural Language Processing",
    "year": 2021,
    "venue": "SIGMOD record",
    "authors": [
        "Immanuel Trummer"
    ],
    "doi": "10.1145/3503780.3503788",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/8a5f154d1d5d55623ac54455142281246e298de5",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Introduction. We have seen significant advances in the state of the art in natural language processing (NLP) over the past few years [20]. These advances have been driven by new neural network architectures, in particular the Transformer model [19], as well as the successful application of transfer learning approaches to NLP [13]. Typically, training for specific NLP tasks starts from large language models that have been pre-trained on generic tasks (e.g., predicting obfuscated words in text [5]) for which large amounts of training data are available. Using such models as a starting point reduces task-specific training cost as well as the number of required training samples by orders of magnitude [7]. These advances motivate new use cases for NLP methods in the context of databases.",
    "citationCount": 4,
    "referenceCount": 21
}