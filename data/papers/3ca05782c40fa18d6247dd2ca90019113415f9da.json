{
    "paperId": "3ca05782c40fa18d6247dd2ca90019113415f9da",
    "title": "On the Use of Large Language Models for Table Tasks",
    "year": 2024,
    "venue": "International Conference on Information and Knowledge Management",
    "authors": [
        "Yuyang Dong",
        "M. Oyamada",
        "Chuan Xiao",
        "Haochen Zhang"
    ],
    "doi": "10.1145/3627673.3679100",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/3ca05782c40fa18d6247dd2ca90019113415f9da",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference",
        "Review"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The proliferation of large language models (LLMs) has catalyzed a diverse array of applications. This tutorial delves into the application of LLMs for tabular data and targets a variety of table-related tasks, such as table understanding, text-to-SQL conversion, and tabular data preprocessing. It surveys LLM solutions to these tasks in five classes, categorized by their underpinning techniques: prompting, fine-tuning, RAG, agents, and multimodal methods. It discusses how LLMs offer innovative ways to interpret, augment, query, and cleanse tabular data, featuring academic contributions and their practical use in the industrial sector. It emphasizes the versatility and effectiveness of LLMs in handling complex table tasks, showcasing their ability to improve data quality, enhance analytical capabilities, and facilitate more intuitive data interactions. By surveying different approaches, this tutorial highlights the strengths of LLMs in enriching table tasks with more accuracy and usability, setting a foundation for future research and application in data science and AI-driven analytics. Presentation slides for this tutorial will be available at: https://dongyuyang.github.io/tableLLM-tutorial/ .",
    "citationCount": 1,
    "referenceCount": 56
}