{
    "paperId": "7a4f596ccdf8f221c12f0a53f13f44371c28e7e0",
    "title": "M-DRL: Deep Reinforcement Learning Based Coflow Traffic Scheduler with MLFQ Threshold Adaption",
    "year": 2021,
    "venue": "International journal of parallel programming",
    "authors": [
        "Tianba Chen",
        "Wei Li",
        "YuKang Sun",
        "Yunchun Li"
    ],
    "doi": "10.1007/s10766-021-00711-4",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/7a4f596ccdf8f221c12f0a53f13f44371c28e7e0",
    "isOpenAccess": true,
    "openAccessPdf": "https://hal.inria.fr/hal-03768743/file/511910_1_En_7_Chapter.pdf",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The coflow scheduling in data-parallel clusters can improve application-level communication performance. The existing coflow scheduling method without prior knowledge usually uses multi-level feedback queue (MLFQ) with fixed threshold parameters, which is insensitive to coflow traffic characteristics. Manual adjustment of the threshold parameters for different application scenarios often has long optimization period and is coarse in optimization granularity. We propose M-DRL, a deep reinforcement learning based coflow traffic scheduler by dynamically setting thresholds of MLFQ to adapt to the coflow traffic characteristics, and reduces the average coflow completion time. Trace-driven simulations on the public dataset show that coflow communication stages using M-DRL complete 2.08x(6.48x) and 1.36x(1.25x) faster on average coflow completion time (95-th percentile) in comparison to per-flow fairness and Aalo, and is comparable to SEBF with prior knowledge.",
    "citationCount": 2,
    "referenceCount": 15
}