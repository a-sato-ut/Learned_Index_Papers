{
    "paperId": "970cb7b5b25da0f1f8b000add10960680fe8cd2e",
    "title": "Searching for Fast Model Families on Datacenter Accelerators",
    "year": 2021,
    "venue": "Computer Vision and Pattern Recognition",
    "authors": [
        "Sheng Li",
        "Mingxing Tan",
        "Ruoming Pang",
        "Andrew Li",
        "Liqun Cheng",
        "Quoc V. Le",
        "N. Jouppi"
    ],
    "doi": "10.1109/CVPR46437.2021.00799",
    "arxivId": "2102.05610",
    "url": "https://www.semanticscholar.org/paper/970cb7b5b25da0f1f8b000add10960680fe8cd2e",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2102.05610",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Engineering",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Neural Architecture Search (NAS), together with model scaling, has shown remarkable progress in designing high accuracy and fast convolutional architecture families. However, as neither NAS nor model scaling considers sufficient hardware architecture details, they do not take full advantage of the emerging datacenter (DC) accelerators. In this paper, we search for fast and accurate CNN model families for efficient inference on DC accelerators. We first analyze DC accelerators and find that existing CNNs suffer from insufficient operational intensity, parallelism, and execution efficiency and exhibit FLOPs-latency nonproportionality. These insights let us create a DC-accelerator-optimized search space, with space-to-depth, space-to-batch, hybrid fused convolution structures with vanilla and depthwise convolutions, and block-wise activation functions. We further propose a latency-aware compound scaling (LACS), the first multi-objective compound scaling method optimizing both accuracy and latency. Our LACS discovers that network depth should grow much faster than image size and network width, which is quite different from the observations from previous compound scaling. With the new search space and LACS, our search and scaling on datacenter accelerators results in a new model series named EfficientNet-X. EfficientNet-X is up to more than 2X faster than Efficient-Net (a model series with state-of-the-art trade-off on FLOPs and accuracy) on TPUv3 and GPUv100, with comparable accuracy. EfficientNet-X is also up to 7X faster than recent RegNet and ResNeSt on TPUv3 and GPUv100. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/tpu",
    "citationCount": 36,
    "referenceCount": 67
}