{
    "paperId": "d617f51833860dc50d202af7f80be71304b2e994",
    "title": "Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP",
    "year": 2021,
    "venue": "arXiv.org",
    "authors": [
        "Sabrina J. Mielke",
        "Zaid Alyafeai",
        "Elizabeth Salesky",
        "Colin Raffel",
        "Manan Dey",
        "Matthias Gallé",
        "Arun Raja",
        "Chenglei Si",
        "Wilson Y. Lee",
        "Benoît Sagot",
        "Samson Tan"
    ],
    "doi": null,
    "arxivId": "2112.10508",
    "url": "https://www.semanticscholar.org/paper/d617f51833860dc50d202af7f80be71304b2e994",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Review"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Linguistics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "What are the units of text that we want to model? From bytes to multi-word expressions, text can be analyzed and generated at many granularities. Until recently, most natural language processing (NLP) models operated over words, treating those as discrete and atomic tokens, but starting with byte-pair encoding (BPE), subword-based approaches have become dominant in many areas, enabling small vocabularies while still allowing for fast inference. Is the end of the road character-level model or byte-level processing? In this survey, we connect several lines of work from the pre-neural and neural era, by showing how hybrid approaches of words and characters as well as subword-based approaches based on learned segmentation have been proposed and evaluated. We conclude that there is and likely will never be a silver bullet singular solution for all applications and that thinking seriously about tokenization remains important for many applications.",
    "citationCount": 181,
    "referenceCount": 200
}