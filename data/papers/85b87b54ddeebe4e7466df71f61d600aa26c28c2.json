{
    "paperId": "85b87b54ddeebe4e7466df71f61d600aa26c28c2",
    "title": "Canonical Autoregressive Generation",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Ivi Chatzi",
        "Nina L. Corvelo Benz",
        "Stratis Tsirtsis",
        "Manuel Gomez-Rodriguez"
    ],
    "doi": "10.48550/arXiv.2506.06446",
    "arxivId": "2506.06446",
    "url": "https://www.semanticscholar.org/paper/85b87b54ddeebe4e7466df71f61d600aa26c28c2",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "State of the art large language models are trained using large amounts of tokens derived from raw text using what is called a tokenizer. Crucially, the tokenizer determines the (token) vocabulary a model will use during inference as well as, in principle, the (token) language. This is because, while the token vocabulary may allow for different tokenizations of a string, the tokenizer always maps the string to only one of these tokenizations--the canonical tokenization. However, multiple lines of empirical evidence suggest that large language models do not always generate canonical token sequences, and this comes with several negative consequences. In this work, we first show that, to generate a canonical token sequence, a model needs to generate (partial) canonical token sequences at each step of the autoregressive generation process underpinning its functioning. Building upon this theoretical result, we introduce canonical sampling, a simple and efficient sampling method that precludes a given model from generating non-canonical token sequences. Further, we also show that, in comparison with standard sampling, the distribution of token sequences generated using canonical sampling is provably closer to the true distribution of token sequences used during training.",
    "citationCount": 1,
    "referenceCount": 37
}