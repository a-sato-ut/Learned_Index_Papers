{
    "paperId": "b8c0fa6a4af6ecfe1932b374d9f60ca07f17ac0a",
    "title": "Deferred Continuous Batching in Resource-Efficient Large Language Model Serving",
    "year": 2024,
    "venue": "EuroMLSys@EuroSys",
    "authors": [
        "Yongjun He",
        "Yao Lu",
        "Gustavo Alonso"
    ],
    "doi": "10.1145/3642970.3655835",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/b8c0fa6a4af6ecfe1932b374d9f60ca07f17ac0a",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Book",
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Despite that prior work of batched inference and parameter-efficient fine-tuning techniques have reduced the resource requirements of large language models (LLMs), challenges remain in resource-constrained environments such as on-premise infrastructures to serve workload that is composed of both inference and fine-tuning jobs. Prior solutions must either pause existing jobs which causes service interruptions, or queue new jobs which results in a long delay. We present FineInfer, an efficient serving system that enables concurrent LLM fine-tuning and inference. FineInfer leverages base model multiplexing and a new task scheduling mechanism, namely deferred continuous batching, to enable iteration-level context switch and accelerate fine-tuning while offering inference latency that compromises service level agreements. Our evaluation shows that FineInfer outperforms prior solutions by up to 3x in fine-tuning latency, and 36x when the models are larger than the GPU memory.",
    "citationCount": 5,
    "referenceCount": 38
}