{
    "paperId": "92a78c0ecf1b1dcebf8d558806c24d9acae3272b",
    "title": "Deep Automatic Natural Image Matting",
    "year": 2021,
    "venue": "International Joint Conference on Artificial Intelligence",
    "authors": [
        "Jizhizi Li",
        "Jing Zhang",
        "D. Tao"
    ],
    "doi": "10.24963/ijcai.2021/111",
    "arxivId": "2107.07235",
    "url": "https://www.semanticscholar.org/paper/92a78c0ecf1b1dcebf8d558806c24d9acae3272b",
    "isOpenAccess": true,
    "openAccessPdf": "https://www.ijcai.org/proceedings/2021/0111.pdf",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Automatic image matting (AIM) refers to estimating the soft foreground from an arbitrary natural image without any auxiliary input like trimap, which is useful for image editing. Prior methods try to learn semantic features to aid the matting process while being limited to images with salient opaque foregrounds such as humans and animals. In this paper, we investigate the difficulties when extending them to natural images with salient transparent/meticulous foregrounds or non-salient foregrounds. To address the problem, a novel end-to-end matting network is proposed, which can predict a generalized trimap for any image of the above types as a unified semantic representation. Simultaneously, the learned semantic features guide the matting network to focus on the transition areas via an attention mechanism. We also construct a test set AIM-500 that contains 500 diverse natural images covering all types along with manually labeled alpha mattes, making it feasible to benchmark the generalization ability of AIM models. Results of the experiments demonstrate that our network trained on available composite matting datasets outperforms existing methods both objectively and subjectively. The source code and dataset are available at https://github.com/JizhiziLi/AIM.",
    "citationCount": 85,
    "referenceCount": 35
}