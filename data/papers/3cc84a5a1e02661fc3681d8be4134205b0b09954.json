{
    "paperId": "3cc84a5a1e02661fc3681d8be4134205b0b09954",
    "title": "DyG-Mamba: Continuous State Space Modeling on Dynamic Graphs",
    "year": 2024,
    "venue": "arXiv.org",
    "authors": [
        "Dongyuan Li",
        "Shiyin Tan",
        "Ying Zhang",
        "Ming Jin",
        "Shirui Pan",
        "Manabu Okumura",
        "Renhe Jiang"
    ],
    "doi": "10.48550/arXiv.2408.06966",
    "arxivId": "2408.06966",
    "url": "https://www.semanticscholar.org/paper/3cc84a5a1e02661fc3681d8be4134205b0b09954",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Dynamic graph learning aims to uncover evolutionary laws in real-world systems, enabling accurate social recommendation (link prediction) or early detection of cancer cells (classification). Inspired by the success of state space models, e.g., Mamba, for efficiently capturing long-term dependencies in language modeling, we propose DyG-Mamba, a new continuous state space model (SSM) for dynamic graph learning. Specifically, we first found that using inputs as control signals for SSM is not suitable for continuous-time dynamic network data with irregular sampling intervals, resulting in models being insensitive to time information and lacking generalization properties. Drawing inspiration from the Ebbinghaus forgetting curve, which suggests that memory of past events is strongly correlated with time intervals rather than specific details of the events themselves, we directly utilize irregular time spans as control signals for SSM to achieve significant robustness and generalization. Through exhaustive experiments on 12 datasets for dynamic link prediction and dynamic node classification tasks, we found that DyG-Mamba achieves state-of-the-art performance on most of the datasets, while also demonstrating significantly improved computation and memory efficiency.",
    "citationCount": 10,
    "referenceCount": 62
}