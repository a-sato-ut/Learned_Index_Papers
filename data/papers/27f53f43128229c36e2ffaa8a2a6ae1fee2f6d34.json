{
    "paperId": "27f53f43128229c36e2ffaa8a2a6ae1fee2f6d34",
    "title": "Dynamic Network Adaptation at Inference",
    "year": 2022,
    "venue": "arXiv.org",
    "authors": [
        "Daniel Mendoza",
        "Caroline Trippel"
    ],
    "doi": "10.48550/arXiv.2204.08400",
    "arxivId": "2204.08400",
    "url": "https://www.semanticscholar.org/paper/27f53f43128229c36e2ffaa8a2a6ae1fee2f6d34",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2204.08400",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Machine learning (ML) inference is a real-time workload that must comply with strict Service Level Objectives (SLOs), including latency and accuracy targets. Unfortunately, ensuring that SLOs are not violated in inference-serving systems is challenging due to inherent model accuracy-latency tradeoffs, SLO diversity across and within application domains, evolution of SLOs over time, unpredictable query patterns, and co-location interference. In this paper, we observe that neural networks exhibit high degrees of per-input activation sparsity during inference. . Thus, we propose SLO-Aware Neural Networks which dynamically drop out nodes per-inference query, thereby tuning the amount of computation performed, according to specified SLO optimization targets and machine utilization. SLO-Aware Neural Networks achieve average speedups of $1.3-56.7\\times$ with little to no accuracy loss (less than 0.3%). When accuracy constrained, SLO-Aware Neural Networks are able to serve a range of accuracy targets at low latency with the same trained model. When latency constrained, SLO-Aware Neural Networks can proactively alleviate latency degradation from co-location interference while maintaining high accuracy to meet latency constraints.",
    "citationCount": 0,
    "referenceCount": 44
}