{
    "paperId": "856db6be9523250b07ce3e44369515f6435dac2c",
    "title": "Self-Supervised Discovering of Interpretable Features for Reinforcement Learning",
    "year": 2020,
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "authors": [
        "Wenjie Shi",
        "Gao Huang",
        "Shiji Song",
        "Zhuoyuan Wang",
        "Tingyu Lin",
        "Cheng Wu"
    ],
    "doi": "10.1109/TPAMI.2020.3037898",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/856db6be9523250b07ce3e44369515f6435dac2c",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2003.07069",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Medicine",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Deep reinforcement learning (RL) has recently led to many breakthroughs on a range of complex control tasks. However, the agent’s decision-making process is generally not transparent. The lack of interpretability hinders the applicability of RL in safety-critical scenarios. While several methods have attempted to interpret vision-based RL, most come without detailed explanation for the agent’s behavior. In this paper, we propose a self-supervised interpretable framework, which can discover interpretable features to enable easy understanding of RL agents even for non-experts. Specifically, a self-supervised interpretable network (SSINet) is employed to produce fine-grained attention masks for highlighting task-relevant information, which constitutes most evidence for the agent’s decisions. We verify and evaluate our method on several Atari 2600 games as well as Duckietown, which is a challenging self-driving car simulator environment. The results show that our method renders empirical evidences about how the agent makes decisions and why the agent performs well or badly, especially when transferred to novel scenes. Overall, our method provides valuable insight into the internal decision-making process of vision-based RL. In addition, our method does not use any external labelled data, and thus demonstrates the possibility to learn high-quality mask through a self-supervised manner, which may shed light on new paradigms for label-free vision learning such as self-supervised segmentation and detection.",
    "citationCount": 52,
    "referenceCount": 61
}