{
    "paperId": "8f0a6bacd9d1902bc0741a86955f5e66d41db946",
    "title": "FINCHFS: Design of Ad-Hoc File System for I/O Heavy HPC Workloads",
    "year": 2024,
    "venue": "IEEE International Conference on Cluster Computing",
    "authors": [
        "Sohei Koyama",
        "Kohei Hiraga",
        "Osamu Tatebe"
    ],
    "doi": "10.1109/CLUSTER59578.2024.00045",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/8f0a6bacd9d1902bc0741a86955f5e66d41db946",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Although the performance improvements in parallel file systems have been significant, the rise of data science and deep learning using Python has introduced new I/O requirements. We redefine ad-hoc file systems as a complement to parallel file systems by specializing in requirements that cannot be met by improvements to parallel file systems. Ad-hoc file systems require extremely high metadata performance and the scalability to create and read large numbers of files in parallel in a single directory. Therefore, it is necessary to process a large number of small RPCs with low latency and high throughput, which cannot be achieved with existing ad-hoc file systems. In this research, we propose a new ad-hoc file system, FINCHFS, and realize the truly desired ad-hoc file system using intra-node shared-nothing architecture and number-aware hashing. The contribution of the intra-node shared-nothing architecture is the ability to run multiple iterative server processes on each node to achieve high IOPS using many-core. The contribution of number-aware hashing is to alleviate tail latency degradation due to the uneven distribution of requests to servers. In the evaluation using 64 nodes on the Pegasus supercomputer, we achieved 32.5 MIOPS in mdtest-hard-write.",
    "citationCount": 0,
    "referenceCount": 27
}