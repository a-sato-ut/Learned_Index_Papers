{
    "paperId": "aa6a298ec282629ac56d7f78fbd8374867734a9d",
    "title": "UNGER: Generative Recommendation with A Unified Code via Semantic and Collaborative Integration",
    "year": 2025,
    "venue": "ACM Transactions on Information Systems",
    "authors": [
        "Longtao Xiao",
        "Haozhao Wang",
        "Cheng Wang",
        "Linfei Ji",
        "Yifan Wang",
        "Jieming Zhu",
        "Zhenhua Dong",
        "Rui Zhang",
        "Ruixuan Li"
    ],
    "doi": "10.1145/3773771",
    "arxivId": "2502.06269",
    "url": "https://www.semanticscholar.org/paper/aa6a298ec282629ac56d7f78fbd8374867734a9d",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": null,
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "With the rise of generative paradigms, generative recommendation has garnered increasing attention. The core component is the item code, generally derived by quantizing collaborative or semantic representations to serve as candidate items identifiers in the context. However, existing methods typically construct separate codes for each modality, leading to higher computational and storage costs and hindering the integration of their complementary strengths. Considering this limitation, we seek to integrate two different modalities into a unified code, fully unleashing the potential of complementary nature among modalities. Nevertheless, the integration remains challenging: the integrated embedding obtained by the common concatenation method would lead to underutilization of collaborative knowledge, thereby resulting in limited effectiveness. To address this, we propose a novel method, named UNGER, which integrates semantic and collaborative knowledge into a unified code for generative recommendation. Specifically, we propose to adaptively learn an integrated embedding through the joint optimization of cross-modality knowledge alignment and next item prediction tasks. Subsequently, to mitigate the information loss caused by the quantization process, we introduce an intra-modality knowledge distillation task, using the integrated embeddings as supervised signals to compensate. Extensive experiments on three widely used benchmarks demonstrate the superiority of our approach compared to existing methods.",
    "citationCount": 1,
    "referenceCount": 75
}