{
    "paperId": "fe9d95a4195c3f2813f993376d917ff1392f2ff2",
    "title": "Optimistic Online Caching for Batched Requests",
    "year": 2023,
    "venue": "ICC 2023 - IEEE International Conference on Communications",
    "authors": [
        "Francescomaria Faticanti",
        "Giovanni Neglia"
    ],
    "doi": "10.1109/ICC45041.2023.10278692",
    "arxivId": "2310.01309",
    "url": "https://www.semanticscholar.org/paper/fe9d95a4195c3f2813f993376d917ff1392f2ff2",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2310.01309",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "In this paper we study online caching problems where predictions of future requests, e.g., provided by a machine learning model, are available. Typical online optimistic policies are based on the Follow-The-Regularized-Leader algorithm and have higher computational cost than classic ones like LFU, LRU, as each update of the cache state requires to solve a constrained optimization problem. In this work we analysed the behaviour of two different optimistic policies in a batched case, i.e., when the cache is updated less frequently in order to amortize the update cost over time or over multiple requests. Experimental results show that such an optimistic batched approach outperforms classical caching policies both on stationary and real traces.",
    "citationCount": 5,
    "referenceCount": 42
}