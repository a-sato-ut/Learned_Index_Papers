{
    "paperId": "933ef2a3a0a923c11c53fd62291a645204c0c1aa",
    "title": "Learning Upper Lower Value Envelopes to Shape Online RL: A Principled Approach",
    "year": 2025,
    "venue": "",
    "authors": [
        "Sebastian Reboul",
        "H'elene Halconruy",
        "Randal Douc"
    ],
    "doi": null,
    "arxivId": "2510.19528",
    "url": "https://www.semanticscholar.org/paper/933ef2a3a0a923c11c53fd62291a645204c0c1aa",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": null,
    "s2FieldsOfStudy": [
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We investigate the fundamental problem of leveraging offline data to accelerate online reinforcement learning - a direction with strong potential but limited theoretical grounding. Our study centers on how to learn and apply value envelopes within this context. To this end, we introduce a principled two-stage framework: the first stage uses offline data to derive upper and lower bounds on value functions, while the second incorporates these learned bounds into online algorithms. Our method extends prior work by decoupling the upper and lower bounds, enabling more flexible and tighter approximations. In contrast to approaches that rely on fixed shaping functions, our envelopes are data-driven and explicitly modeled as random variables, with a filtration argument ensuring independence across phases. The analysis establishes high-probability regret bounds determined by two interpretable quantities, thereby providing a formal bridge between offline pre-training and online fine-tuning. Empirical results on tabular MDPs demonstrate substantial regret reductions compared with both UCBVI and prior methods.",
    "citationCount": 0,
    "referenceCount": 41
}