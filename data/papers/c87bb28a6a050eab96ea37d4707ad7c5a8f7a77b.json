{
    "paperId": "c87bb28a6a050eab96ea37d4707ad7c5a8f7a77b",
    "title": "Learning Binarized Representations with Pseudo-positive Sample Enhancement for Efficient Graph Collaborative Filtering",
    "year": 2025,
    "venue": "ACM Trans. Inf. Syst.",
    "authors": [
        "Yankai Chen",
        "Yue Que",
        "Xinni Zhang",
        "Chen Ma",
        "Irwin King"
    ],
    "doi": "10.1145/3744239",
    "arxivId": "2506.02750",
    "url": "https://www.semanticscholar.org/paper/c87bb28a6a050eab96ea37d4707ad7c5a8f7a77b",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Learning vectorized embeddings is fundamental to many recommender systems for userâ€“item matching. To enable efficient online inference, representation binarization, which embeds latent features into compact binary sequences, has recently shown significant promise in optimizing both memory usage and computational overhead. However, existing approaches primarily focus on numerical quantization, neglecting the associated information loss, which often results in noticeable performance degradation. To address these issues, we study the problem of graph representation binarization for efficient collaborative filtering. Our findings indicate that explicitly mitigating information loss at various stages of embedding binarization has a significant positive impact on performance. Building on these insights, we propose an enhanced framework, BiGeaR++, which specifically leverages supervisory signals from pseudo-positive samples, incorporating both real item data and latent embedding samples. Compared to its predecessor BiGeaR, BiGeaR++ introduces a fine-grained inference distillation mechanism and an effective embedding sample synthesis approach. Empirical evaluations across five real-world datasets demonstrate that the new designs in BiGeaR++ work seamlessly well with other modules, delivering substantial improvements of around 1% \\(\\sim\\) 10% over BiGeaR and thus achieving state-of-the-art performance compared to the competing methods. Our implementation is available at https://github.com/QueYork/BiGeaR-SS.",
    "citationCount": 1,
    "referenceCount": 101
}