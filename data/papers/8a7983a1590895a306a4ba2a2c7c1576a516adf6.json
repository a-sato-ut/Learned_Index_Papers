{
    "paperId": "8a7983a1590895a306a4ba2a2c7c1576a516adf6",
    "title": "Policy Gradients for Probabilistic Constrained Reinforcement Learning",
    "year": 2022,
    "venue": "Annual Conference on Information Sciences and Systems",
    "authors": [
        "Weiqin Chen",
        "D. Subramanian",
        "Santiago Paternain"
    ],
    "doi": "10.1109/CISS56502.2023.10089763",
    "arxivId": "2210.00596",
    "url": "https://www.semanticscholar.org/paper/8a7983a1590895a306a4ba2a2c7c1576a516adf6",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2210.00596",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "This paper considers the problem of learning safe policies in the context of reinforcement learning (RL). In particular, we consider the notion of probabilistic safety. This is, we aim to design policies that maintain the state of the system in a safe set with high probability. This notion differs from cumulative constraints often considered in the literature. The challenge of working with probabilistic safety is the lack of expressions for their gradients. Indeed, policy optimization algorithms rely on gradients of the objective function and the constraints. To the best of our knowledge, this work is the first one providing such explicit gradient expressions for probabilistic constraints. It is worth noting that the gradient of this family of constraints can be applied to various policy-based algorithms. We demonstrate empirically that it is possible to handle probabilistic constraints in a continuous navigation problem.",
    "citationCount": 8,
    "referenceCount": 44
}