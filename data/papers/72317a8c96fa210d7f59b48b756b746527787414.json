{
    "paperId": "72317a8c96fa210d7f59b48b756b746527787414",
    "title": "WIKIR: A Python Toolkit for Building a Large-scale Wikipedia-based English Information Retrieval Dataset",
    "year": 2019,
    "venue": "International Conference on Language Resources and Evaluation",
    "authors": [
        "Jibril Frej",
        "D. Schwab",
        "J. Chevallet"
    ],
    "doi": null,
    "arxivId": "1912.01901",
    "url": "https://www.semanticscholar.org/paper/72317a8c96fa210d7f59b48b756b746527787414",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Over the past years, deep learning methods allowed for new state-of-the-art results in ad-hoc information retrieval. However such methods usually require large amounts of annotated data to be effective. Since most standard ad-hoc information retrieval datasets publicly available for academic research (e.g. Robust04, ClueWeb09) have at most 250 annotated queries, the recent deep learning models for information retrieval perform poorly on these datasets. These models (e.g. DUET, Conv-KNRM) are trained and evaluated on data collected from commercial search engines not publicly available for academic research which is a problem for reproducibility and the advancement of research. In this paper, we propose WIKIR: an open-source toolkit to automatically build large-scale English information retrieval datasets based on Wikipedia. WIKIR is publicly available on GitHub. We also provide wikIR59k: a large-scale publicly available dataset that contains 59,252 queries and 2,617,003 (query, relevant documents) pairs.",
    "citationCount": 15,
    "referenceCount": 34
}