{
    "paperId": "7fa11c51c540f6dc961c8a12b2573386570a0b27",
    "title": "Training Vision Transformers for Image Retrieval",
    "year": 2021,
    "venue": "arXiv.org",
    "authors": [
        "Alaaeldin El-Nouby",
        "N. Neverova",
        "I. Laptev",
        "Herv'e J'egou"
    ],
    "doi": null,
    "arxivId": "2102.05644",
    "url": "https://www.semanticscholar.org/paper/7fa11c51c540f6dc961c8a12b2573386570a0b27",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Transformers have shown outstanding results for natural language understanding and, more recently, for image classification. We here extend this work and propose a transformer-based approach for image retrieval: we adopt vision transformers for generating image descriptors and train the resulting model with a metric learning objective, which combines a contrastive loss with a differential entropy regularizer. Our results show consistent and significant improvements of transformers over convolution-based approaches. In particular, our method outperforms the state of the art on several public benchmarks for category-level retrieval, namely Stanford Online Product, In-Shop and CUB-200. Furthermore, our experiments on ROxford and RParis also show that, in comparable settings, transformers are competitive for particular object retrieval, especially in the regime of short vector representations and low-resolution images.",
    "citationCount": 163,
    "referenceCount": 59
}