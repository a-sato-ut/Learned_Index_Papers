{
    "paperId": "f941a10056a3fcc7ae7416a4b563a5dee25460df",
    "title": "Learning Multi-granular Quantized Embeddings for Large-Vocab Categorical Features in Recommender Systems",
    "year": 2020,
    "venue": "The Web Conference",
    "authors": [
        "Wang-Cheng Kang",
        "D. Cheng",
        "Ting Chen",
        "Xinyang Yi",
        "Dong Lin",
        "Lichan Hong",
        "Ed H. Chi"
    ],
    "doi": "10.1145/3366424.3383416",
    "arxivId": "2002.08530",
    "url": "https://www.semanticscholar.org/paper/f941a10056a3fcc7ae7416a4b563a5dee25460df",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2002.08530",
    "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Recommender system models often represent various sparse features like users, items, and categorical features via embeddings. A standard approach is to map each unique feature value to an embedding vector. The size of the produced embedding table grows linearly with the size of the vocabulary. Therefore, a large vocabulary inevitably leads to a gigantic embedding table, creating two severe problems: (i) making model serving intractable in resource-constrained environments; (ii) causing overfitting problems. In this paper, we seek to learn highly compact embeddings for large-vocab sparse features in recommender systems (recsys). First, we show that the novel Differentiable Product Quantization (DPQ) approach can generalize to recsys problems. In addition, to better handle the power-law data distribution commonly seen in recsys, we propose a Multi-Granular Quantized Embeddings (MGQE) technique which learns more compact embeddings for infrequent items. We seek to provide a new angle to improve recommendation performance with compact model sizes. Extensive experiments on three recommendation tasks and two datasets show that we can achieve on par or better performance, with only âˆ¼ 20% of the original model size.",
    "citationCount": 56,
    "referenceCount": 44
}