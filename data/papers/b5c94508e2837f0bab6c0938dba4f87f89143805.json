{
    "paperId": "b5c94508e2837f0bab6c0938dba4f87f89143805",
    "title": "A Bring-Your-Own-Model Approach for ML-Driven Storage Placement in Warehouse-Scale Computers",
    "year": 2025,
    "venue": "",
    "authors": [
        "Chenxi Yang",
        "Yan Li",
        "Martin Maas",
        "Mustafa Uysal",
        "Ubaid Ullah Hafeez",
        "Arif Merchant",
        "Richard McDougall"
    ],
    "doi": null,
    "arxivId": "2501.05651",
    "url": "https://www.semanticscholar.org/paper/b5c94508e2837f0bab6c0938dba4f87f89143805",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": null,
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Storage systems account for a major portion of the total cost of ownership (TCO) of warehouse-scale computers, and thus have a major impact on the overall system's efficiency. Machine learning (ML)-based methods for solving key problems in storage system efficiency, such as data placement, have shown significant promise. However, there are few known practical deployments of such methods. Studying this problem in the context of real-world hyperscale data centers at Google, we identify a number of challenges that we believe cause this lack of practical adoption. Specifically, prior work assumes a monolithic model that resides entirely within the storage layer, an unrealistic assumption in real-world deployments with frequently changing workloads. To address this problem, we introduce a cross-layer approach where workloads instead ''bring their own model''. This strategy moves ML out of the storage system and instead allows each workload to train its own lightweight model at the application layer, capturing the workload's specific characteristics. These small, interpretable models generate predictions that guide a co-designed scheduling heuristic at the storage layer, enabling adaptation to diverse online environments. We build a proof-of-concept of this approach in a production distributed computation framework at Google. Evaluations in a test deployment and large-scale simulation studies using production traces show improvements of as much as 3.47$\\times$ in TCO savings compared to state-of-the-art baselines.",
    "citationCount": 0,
    "referenceCount": 41
}