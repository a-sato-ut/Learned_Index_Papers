{
    "paperId": "8c05273278e0eedd2c806215aa17cfab7fc1da0c",
    "title": "From Neural Re-Ranking to Neural Ranking: Learning a Sparse Representation for Inverted Indexing",
    "year": 2018,
    "venue": "International Conference on Information and Knowledge Management",
    "authors": [
        "Hamed Zamani",
        "Mostafa Dehghani",
        "W. Bruce Croft",
        "E. Learned-Miller",
        "J. Kamps"
    ],
    "doi": "10.1145/3269206.3271800",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/8c05273278e0eedd2c806215aa17cfab7fc1da0c",
    "isOpenAccess": true,
    "openAccessPdf": "https://pure.uva.nl/ws/files/37120761/p497_zamani.pdf",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The availability of massive data and computing power allowing for effective data driven neural approaches is having a major impact on machine learning and information retrieval research, but these models have a basic problem with efficiency. Current neural ranking models are implemented as multistage rankers: for efficiency reasons, the neural model only re-ranks the top ranked documents retrieved by a first-stage efficient ranker in response to a given query. Neural ranking models learn dense representations causing essentially every query term to match every document term, making it highly inefficient or intractable to rank the whole collection. The reliance on a first stage ranker creates a dual problem: First, the interaction and combination effects are not well understood. Second, the first stage ranker serves as a \"gate-keeper\" or filter, effectively blocking the potential of neural models to uncover new relevant documents. In this work, we propose a standalone neural ranking model (SNRM) by introducing a sparsity property to learn a latent sparse representation for each query and document. This representation captures the semantic relationship between the query and documents, but is also sparse enough to enable constructing an inverted index for the whole collection. We parameterize the sparsity of the model to yield a retrieval model as efficient as conventional term based models. Our model gains in efficiency without loss of effectiveness: it not only outperforms the existing term matching baselines, but also performs similarly to the recent re-ranking based neural models with dense representations. Our model can also take advantage of pseudo-relevance feedback for further improvements. More generally, our results demonstrate the importance of sparsity in neural IR models and show that dense representations can be pruned effectively, giving new insights about essential semantic features and their distributions.",
    "citationCount": 198,
    "referenceCount": 66
}