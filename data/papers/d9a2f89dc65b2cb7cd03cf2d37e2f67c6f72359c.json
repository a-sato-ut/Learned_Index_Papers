{
    "paperId": "d9a2f89dc65b2cb7cd03cf2d37e2f67c6f72359c",
    "title": "PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels",
    "year": 2023,
    "venue": "International Conference on Machine Learning",
    "authors": [
        "Praneeth Kacham",
        "V. Mirrokni",
        "Peilin Zhong"
    ],
    "doi": null,
    "arxivId": "2310.01655",
    "url": "https://www.semanticscholar.org/paper/d9a2f89dc65b2cb7cd03cf2d37e2f67c6f72359c",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The quadratic time and memory complexity inherent to self-attention mechanisms, with respect to sequence length, presents a critical computational bottleneck in the training and deployment of large-scale Transformer-based language models. Recent theoretical results indicate the intractability of sub-quadratic softmax attention approximation under reasonable complexity assumptions. This paper addresses this challenge by first demonstrating that polynomial attention with high degree can effectively replace softmax without sacrificing model quality. Next, we develop polynomial sketching techniques from numerical linear algebra to achieve linear-time polynomial attention with approximation guarantees. Crucially, our approach achieves this speedup without requiring the sparsification of attention matrices. We also present a block-based algorithm to apply causal masking efficiently. Combining these techniques, we provide \\emph{PolySketchFormer}, a practical linear-time Transformer architecture for language modeling that offers provable guarantees. We validate PolySketchFormer empirically by training language models capable of handling long contexts. These experiments utilize both synthetic and real-world datasets (PG19, Wikipedia and C4) on Google Cloud TPUs. For context lengths of 32k and GPT-2 style models, our model achieves a 2.5-4x speedup in training compared to FlashAttention, with no observed degradation in quality across our experiments.",
    "citationCount": 15,
    "referenceCount": 51
}