{
    "paperId": "6e252ebc32f55c833b75f836fa4f0b1b931432dc",
    "title": "SVMax: A Feature Embedding Regularizer",
    "year": 2021,
    "venue": "arXiv.org",
    "authors": [
        "Ahmed Taha",
        "Alex Hanson",
        "Abhinav Shrivastava",
        "L. Davis"
    ],
    "doi": null,
    "arxivId": "2103.02770",
    "url": "https://www.semanticscholar.org/paper/6e252ebc32f55c833b75f836fa4f0b1b931432dc",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "A neural network regularizer (e.g., weight decay) boosts performance by explicitly penalizing the complexity of a network. In this paper, we penalize inferior network activations -- feature embeddings -- which in turn regularize the network's weights implicitly. We propose singular value maximization (SVMax) to learn a more uniform feature embedding. The SVMax regularizer supports both supervised and unsupervised learning. Our formulation mitigates model collapse and enables larger learning rates. We evaluate the SVMax regularizer using both retrieval and generative adversarial networks. We leverage a synthetic mixture of Gaussians dataset to evaluate SVMax in an unsupervised setting. For retrieval networks, SVMax achieves significant improvement margins across various ranking losses. Code available at https://bit.ly/3jNkgDt",
    "citationCount": 0,
    "referenceCount": 64
}