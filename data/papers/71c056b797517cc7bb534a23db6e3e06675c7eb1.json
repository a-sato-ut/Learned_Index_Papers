{
    "paperId": "71c056b797517cc7bb534a23db6e3e06675c7eb1",
    "title": "Membrane: Accelerating Database Analytics with Bank-Level DRAM-PIM Filtering",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Akhil Shekar",
        "Kevin Gaffney",
        "Martin Prammer",
        "Khyati Kiyawat",
        "Lingxi Wu",
        "Helena Caminal",
        "Zhenxing Fan",
        "Yimin Gao",
        "A. Venkat",
        "Jos'e F. Mart'inez",
        "Jignesh M. Patel",
        "Kevin Skadron"
    ],
    "doi": "10.48550/arXiv.2504.06473",
    "arxivId": "2504.06473",
    "url": "https://www.semanticscholar.org/paper/71c056b797517cc7bb534a23db6e3e06675c7eb1",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "In-memory database query processing frequently involves substantial data transfers between the CPU and memory, leading to inefficiencies due to Von Neumann bottleneck. Processing-in-Memory (PIM) architectures offer a viable solution to alleviate this bottleneck. In our study, we employ a commonly used software approach that streamlines JOIN operations into simpler selection or filtering tasks using pre-join denormalization which makes query processing workload more amenable to PIM acceleration. This research explores DRAM design landscape to evaluate how effectively these filtering tasks can be efficiently executed across DRAM hierarchy and their effect on overall application speedup. We also find that operations such as aggregates are more suitably executed on the CPU rather than PIM. Thus, we propose a cooperative query processing framework that capitalizes on both CPU and PIM strengths, where (i) the DRAM-based PIM block, with its massive parallelism, supports scan operations while (ii) CPU, with its flexible architecture, supports the rest of query execution. This allows us to utilize both PIM and CPU where appropriate and prevent dramatic changes to the overall system architecture. With these minimal modifications, our methodology enables us to faithfully perform end-to-end performance evaluations using established analytics benchmarks such as TPCH and star-schema benchmark (SSB). Our findings show that this novel mapping approach improves performance, delivering a 5.92x/6.5x speedup compared to a traditional schema and 3.03-4.05x speedup compared to a denormalized schema with 9-17% memory overhead, depending on the degree of partial denormalization. Further, we provide insights into query selectivity, memory overheads, and software optimizations in the context of PIM-based filtering, which better explain the behavior and performance of these systems across the benchmarks.",
    "citationCount": 0,
    "referenceCount": 112
}