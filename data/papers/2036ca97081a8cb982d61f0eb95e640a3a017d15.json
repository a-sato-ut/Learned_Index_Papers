{
    "paperId": "2036ca97081a8cb982d61f0eb95e640a3a017d15",
    "title": "Querying Templatized Document Collections with Large Language Models",
    "year": 2025,
    "venue": "IEEE International Conference on Data Engineering",
    "authors": [
        "Yiming Lin",
        "Madelon Hulsebos",
        "Ruiying Ma",
        "Shreya Shankar",
        "Sepanta Zeighami",
        "Aditya G. Parameswaran",
        "Eugene Wu"
    ],
    "doi": "10.1109/ICDE65448.2025.00183",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/2036ca97081a8cb982d61f0eb95e640a3a017d15",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Querying and extracting value from unstructured document collection remains a considerable challenge. While Large Language Models (LLMs) have made remarkable progress in document understanding, they fail to give high accuracy results for analytical queries on documents, and additionally incur high costs. While Retrieval-Augmented Generation (RAG) can reduce costs, accuracy degrades further. Our key insight is that documents in a collection often follow similar templates that impart a common semantic structure. We therefore introduce Zendb, a document analytics system that leverages this semantic structure, coupled with LLMs, to answer ad-hoc SQL queries on document collections. Zendb efficiently extracts semantic hierarchical structures from such templatized documents and introduces a novel query engine that leverages these structures for accurate and cost-effective query execution. Extensive experiments on three real-world document collections demonstrate ZENDB's benefits, achieving up to 31Ã— cost savings compared to LLM-based baselines, while maintaining or improving accuracy, and surpassing RAG-based baselines by up to 61% in precision and 81% in recall, at a marginally higher cost.",
    "citationCount": 0,
    "referenceCount": 52
}