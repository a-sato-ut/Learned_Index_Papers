{
    "paperId": "1e28e06aadc45b844bf321c34ccf48f17f837978",
    "title": "Prompt-Aware Scheduling for Low-Latency LLM Serving",
    "year": 2025,
    "venue": "",
    "authors": [
        "Yiheng Tao",
        "Yihe Zhang",
        "M. Dearing",
        "Xin Wang",
        "Yuping Fan",
        "Zhiling Lan"
    ],
    "doi": null,
    "arxivId": "2510.03243",
    "url": "https://www.semanticscholar.org/paper/1e28e06aadc45b844bf321c34ccf48f17f837978",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": null,
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Efficient scheduling of LLM inference tasks is essential for achieving low latency and high throughput, particularly with the growing use of reasoning-capable LLMs. Traditional strategies like First-Come-First-Serve (FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks delay shorter ones queued behind them. In this paper, we introduce PARS, a prompt-aware LLM task scheduler that improves serving efficiency by approximating shortest-job-first (SJF) scheduling through pairwise ranking with margin ranking loss. PARS focuses on impactful scheduling decisions and is seamlessly integrated into the state-of-the-art LLM serving system vLLM. It effectively predicts response-length-based task ordering, reducing latency with minimal overhead. Extensive experiments across multiple LLMs and real-world inference datasets show that PARS significantly improves performance, including for reasoning workloads. Furthermore, our cross-model evaluations demonstrate that the design generalizes well, enabling effective scheduling even when predictors are trained on different LLMs.",
    "citationCount": 0,
    "referenceCount": 48
}