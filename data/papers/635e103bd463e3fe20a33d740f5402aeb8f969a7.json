{
    "paperId": "635e103bd463e3fe20a33d740f5402aeb8f969a7",
    "title": "On Synthetic Data Strategies for Domain-Specific Generative Retrieval",
    "year": 2025,
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "authors": [
        "Haoyang Wen",
        "Jiang Guo",
        "Yi Zhang",
        "Jiarong Jiang",
        "Zhiguo Wang"
    ],
    "doi": "10.48550/arXiv.2502.17957",
    "arxivId": "2502.17957",
    "url": "https://www.semanticscholar.org/paper/635e103bd463e3fe20a33d740f5402aeb8f969a7",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "This paper investigates synthetic data generation strategies in developing generative retrieval models for domain-specific corpora, thereby addressing the scalability challenges inherent in manually annotating in-domain queries. We study the data strategies for a two-stage training framework: in the first stage, which focuses on learning to decode document identifiers from queries, we investigate LLM-generated queries across multiple granularity (e.g. chunks, sentences) and domain-relevant search constraints that can better capture nuanced relevancy signals. In the second stage, which aims to refine document ranking through preference learning, we explore the strategies for mining hard negatives based on the initial model's predictions. Experiments on public datasets over diverse domains demonstrate the effectiveness of our synthetic data generation and hard negative sampling approach.",
    "citationCount": 2,
    "referenceCount": 52
}