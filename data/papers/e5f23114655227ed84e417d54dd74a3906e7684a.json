{
    "paperId": "e5f23114655227ed84e417d54dd74a3906e7684a",
    "title": "Self-Supervised Discovering of Causal Features: Towards Interpretable Reinforcement Learning",
    "year": 2020,
    "venue": "arXiv.org",
    "authors": [
        "Wenjie Shi",
        "Zhuoyuan Wang",
        "Shiji Song",
        "Gao Huang"
    ],
    "doi": null,
    "arxivId": "2003.07069",
    "url": "https://www.semanticscholar.org/paper/e5f23114655227ed84e417d54dd74a3906e7684a",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Deep reinforcement learning (RL) has recently led to many breakthroughs on a range of complex control tasks. However, the agent's decision-making process is generally not transparent. The lack of interpretability hinders the applicability of RL in safety-critical scenarios. In this paper, we propose a self-supervised interpretable framework, which employs a self-supervised interpretable network (SSINet) to discover and locate fine-grained causal features that constitute most evidence for the agent's decisions. We verify and evaluate our method on several Atari 2600 games as well as Duckietown. The results show that our method renders causal explanations and empirical evidences about how the agent makes decisions and why the agent performs well or badly. Moreover, our method is a flexible explanatory module that can be applied to most vision-based RL agents. Overall, our method provides valuable insight into interpretable vision-based RL.",
    "citationCount": 19,
    "referenceCount": 62
}