{
    "paperId": "1c484477f5b5c520fa7b51231c02396316f2ff5e",
    "title": "Classification Acceleration via Merging Decision Trees",
    "year": 2020,
    "venue": "Foundations of Data Science Conference",
    "authors": [
        "Chenglin Fan",
        "P. Li"
    ],
    "doi": "10.1145/3412815.3416886",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/1c484477f5b5c520fa7b51231c02396316f2ff5e",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Book"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We study the problem of merging decision trees: Given k decision trees $T_1,T_2,T_3...,T_k$, we merge these trees into one super tree T with (often) much smaller size. The resultant super tree T, which is an integration of k decision trees with each leaf having a major label, can also be considered as a (lossless) compression of a random forest. For any testing instance, it is guaranteed that the tree T gives the same prediction as the random forest consisting of $T_1,T_2,T_3...,T_k$ but it saves the computational effort needed for traversing multiple trees. The proposed method is suitable for classification problems with time constraints, for example, the online classification task such that it needs to predict a label for a new instance before the next instance arrives. Experiments on five datasets confirm that the super tree T runs significantly faster than the random forest with k trees. The merging procedure also saves space needed storing those k trees, and it makes the forest model more interpretable, since naturally one tree is easier to be interpreted than k trees.",
    "citationCount": 9,
    "referenceCount": 33
}