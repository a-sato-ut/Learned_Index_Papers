{
    "paperId": "2f7e79e0fb72c75b42468c0e3facc459b682a6d4",
    "title": "Two-dimensional Sparse Parallelism for Large Scale Deep Learning Recommendation Model Training",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Xin Zhang",
        "Quanyu Zhu",
        "Liangbei Xu",
        "Zain Huda",
        "Wang Zhou",
        "Jin Fang",
        "Dennis Van Der Staay",
        "Yuxi Hu",
        "Jade Nie",
        "Jiyan Yang",
        "Chunzhi Yang"
    ],
    "doi": "10.48550/arXiv.2508.03854",
    "arxivId": "2508.03854",
    "url": "https://www.semanticscholar.org/paper/2f7e79e0fb72c75b42468c0e3facc459b682a6d4",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The increasing complexity of deep learning recommendation models (DLRM) has led to a growing need for large-scale distributed systems that can efficiently train vast amounts of data. In DLRM, the sparse embedding table is a crucial component for managing sparse categorical features. Typically, these tables in industrial DLRMs contain trillions of parameters, necessitating model parallelism strategies to address memory constraints. However, as training systems expand with massive GPUs, the traditional fully parallelism strategies for embedding table post significant scalability challenges, including imbalance and straggler issues, intensive lookup communication, and heavy embedding activation memory. To overcome these limitations, we propose a novel two-dimensional sparse parallelism approach. Rather than fully sharding tables across all GPUs, our solution introduces data parallelism on top of model parallelism. This enables efficient all-to-all communication and reduces peak memory consumption. Additionally, we have developed the momentum-scaled row-wise AdaGrad algorithm to mitigate performance losses associated with the shift in training paradigms. Our extensive experiments demonstrate that the proposed approach significantly enhances training efficiency while maintaining model performance parity. It achieves nearly linear training speed scaling up to 4K GPUs, setting a new state-of-the-art benchmark for recommendation model training.",
    "citationCount": 0,
    "referenceCount": 37
}