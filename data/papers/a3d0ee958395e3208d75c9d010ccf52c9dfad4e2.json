{
    "paperId": "a3d0ee958395e3208d75c9d010ccf52c9dfad4e2",
    "title": "Web-Scale Visual Entity Recognition: An LLM-Driven Data Approach",
    "year": 2024,
    "venue": "Neural Information Processing Systems",
    "authors": [
        "Mathilde Caron",
        "Alireza Fathi",
        "Cordelia Schmid",
        "Ahmet Iscen"
    ],
    "doi": "10.48550/arXiv.2410.23676",
    "arxivId": "2410.23676",
    "url": "https://www.semanticscholar.org/paper/a3d0ee958395e3208d75c9d010ccf52c9dfad4e2",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Web-scale visual entity recognition, the task of associating images with their corresponding entities within vast knowledge bases like Wikipedia, presents significant challenges due to the lack of clean, large-scale training data. In this paper, we propose a novel methodology to curate such a dataset, leveraging a multimodal large language model (LLM) for label verification, metadata generation, and rationale explanation. Instead of relying on the multimodal LLM to directly annotate data, which we found to be suboptimal, we prompt it to reason about potential candidate entity labels by accessing additional contextually relevant information (such as Wikipedia), resulting in more accurate annotations. We further use the multimodal LLM to enrich the dataset by generating question-answer pairs and a grounded finegrained textual description (referred to as\"rationale\") that explains the connection between images and their assigned entities. Experiments demonstrate that models trained on this automatically curated data achieve state-of-the-art performance on web-scale visual entity recognition tasks (e.g. +6.9% improvement in OVEN entity task), underscoring the importance of high-quality training data in this domain.",
    "citationCount": 2,
    "referenceCount": 63
}