{
    "paperId": "9952fe8cbd09e4fc89dc7d76595d138e36c7d7b5",
    "title": "A Systematic Evaluation of Transfer Learning and Pseudo-labeling with BERT-based Ranking Models",
    "year": 2021,
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "authors": [
        "Iurii Mokrii",
        "Leonid Boytsov",
        "Pavel Braslavski"
    ],
    "doi": "10.1145/3404835.3463093",
    "arxivId": "2103.03335",
    "url": "https://www.semanticscholar.org/paper/9952fe8cbd09e4fc89dc7d76595d138e36c7d7b5",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2103.03335",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Due to high annotation costs making the best use of existing human-created training data is an important research direction. We, therefore, carry out a systematic evaluation of transferability of BERT-based neural ranking models across five English datasets. Previous studies focused primarily on zero-shot and few-shot transfer from a large dataset to a dataset with a small number of queries. In contrast, each of our collections has a substantial number of queries, which enables a full-shot evaluation mode and improves reliability of our results. Furthermore, since source datasets licences often prohibit commercial use, we compare transfer learning to training on pseudo-labels generated by a BM25 scorer. We find that training on pseudo-labels---possibly with subsequent fine-tuning using a modest number of annotated queries---can produce a competitive or better model compared to transfer learning. Yet, it is necessary to improve the stability and/or effectiveness of the few-shot training, which, sometimes, can degrade performance of a pretrained model.",
    "citationCount": 28,
    "referenceCount": 36
}