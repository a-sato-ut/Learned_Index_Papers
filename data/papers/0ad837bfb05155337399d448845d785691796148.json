{
    "paperId": "0ad837bfb05155337399d448845d785691796148",
    "title": "Transformer-based Scalable Beamforming Optimization via Deep Residual Learning",
    "year": 2025,
    "venue": "",
    "authors": [
        "Yubo Zhang",
        "Xiaoyuan Liu",
        "Xiaodong Wang"
    ],
    "doi": null,
    "arxivId": "2510.13077",
    "url": "https://www.semanticscholar.org/paper/0ad837bfb05155337399d448845d785691796148",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": null,
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Engineering",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We develop an unsupervised deep learning framework for downlink beamforming in large-scale MU-MISO channels. The model is trained offline, allowing real-time inference through lightweight feedforward computations in dynamic communication environments. Following the learning-to-optimize (L2O) paradigm, a multi-layer Transformer iteratively refines both channel and beamformer features via residual connections. To enhance training, three strategies are introduced: (i) curriculum learning (CL) to improve early-stage convergence and avoid local optima, (ii) semi-amortized learning to refine each Transformer block with a few gradient ascent steps, and (iii) sliding-window training to stabilize optimization by training only a subset of Transformer blocks at a time. Extensive simulations show that the proposed scheme outperforms existing baselines at low-to-medium SNRs and closely approaches WMMSE performance at high SNRs, while achieving substantially faster inference than iterative and online learning approaches.",
    "citationCount": 0,
    "referenceCount": 18
}