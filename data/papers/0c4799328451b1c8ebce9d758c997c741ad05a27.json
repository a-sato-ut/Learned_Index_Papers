{
    "paperId": "0c4799328451b1c8ebce9d758c997c741ad05a27",
    "title": "Augment Online Linear Optimization with Arbitrarily Bad Machine-Learned Predictions",
    "year": 2024,
    "venue": "IEEE Conference on Computer Communications",
    "authors": [
        "Dacheng Wen",
        "Yupeng Li",
        "Francis C. M. Lau"
    ],
    "doi": "10.1109/INFOCOM52122.2024.10621317",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/0c4799328451b1c8ebce9d758c997c741ad05a27",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The online linear optimization paradigm is important to many real-world network applications as well as theoretical algorithmic studies. Recent studies have made attempts to augment online linear optimization with machine-learned predictions of the cost function that are meant to improve the performance of the algorithms. However, they fail to address the critical case in practical systems where the predictions can be arbitrarily bad. In this work, we take the first step to study the problem of online linear optimization with a dynamic number of arbitrarily bad machine-learned predictions per round and propose an algorithm termed OLOAP. Our theoretical analysis shows that, when the qualities of the predictions are satisfactory, OLOAP achieves a regret bound of O(logT), which circumvents the tight lower bound of Î©($\\sqrt T $) for the vanilla problem of online linear optimization (i.e., the one without any predictions). Meanwhile, the regret of our algorithm is never worse than O($\\sqrt T $) irrespective of the qualities of predictions. In addition, we further derive a lower bound for the regret of the studied problem, which demonstrates that OLOAP is near-optimal. We consider two important network applications and conduct extensive evaluations. Our results validate the superiority of our algorithm over state-of-the-art approaches.",
    "citationCount": 4,
    "referenceCount": 53
}