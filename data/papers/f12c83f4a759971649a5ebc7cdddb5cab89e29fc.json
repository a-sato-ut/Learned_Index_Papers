{
    "paperId": "f12c83f4a759971649a5ebc7cdddb5cab89e29fc",
    "title": "ConvMADE: Convolution Makes Cardinality Estimation Stronger",
    "year": 2023,
    "venue": "IEEE Access",
    "authors": [
        "Chao Gao",
        "Jiong Yu",
        "Zhenzhen He",
        "Xiaoqiao Xiong"
    ],
    "doi": "10.1109/ACCESS.2023.3312312",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/f12c83f4a759971649a5ebc7cdddb5cab89e29fc",
    "isOpenAccess": true,
    "openAccessPdf": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10239358.pdf",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Cardinality estimation is critical for optimizing database queries, and accurate results are essential for a good query plan. Traditional models use statistical principles but struggle with complex data associations. Learning-based methods solve these problems but need to improve accuracy and reduce parameter size, and adapt to multi-table training. Therefore, we propose the Convolutional Masked Autoencoder for Distribution Estimation(ConvMADE) model, which uses the Re-parameterization Convolution(RepConv) structure, which enhances the ability of the model to obtain data features, thereby improving the accuracy of cardinality estimation. At the same time, the DepthWise Multilayer Perceptron (DWMP) structure is added to reduce the number of model parameters, and each table is explicitly trained to improve the ability to capture multi-table data features. We compare the ConvMADE model with traditional and learning-based methods on the DMV and IMDB datasets. The results show that the performance of the ConvMADE model in both single-table and multi-table models is superior to other models, and the parameter amount of the ConvMADE model is much lower than that of the baseline model. The single table can be as low as 18% of the baseline model, the multi-table can be as low as 81%, and the multi-table average q-error is 27.2% lower than the baseline model.",
    "citationCount": 0,
    "referenceCount": 41
}