{
    "paperId": "10e907889d90fe57c45fee66cdd0ec0c2f3d773c",
    "title": "DeepVL: Dynamics and Inertial Measurements-based Deep Velocity Learning for Underwater Odometry",
    "year": 2025,
    "venue": "IEEE International Conference on Robotics and Automation",
    "authors": [
        "Mohit Singh",
        "Kostas Alexis"
    ],
    "doi": "10.1109/ICRA55743.2025.11128041",
    "arxivId": "2502.07726",
    "url": "https://www.semanticscholar.org/paper/10e907889d90fe57c45fee66cdd0ec0c2f3d773c",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        },
        {
            "category": "Environmental Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "This paper presents a learned model to predict the robot-centric velocity of an underwater robot through dynamics-aware proprioception. The method exploits a recurrent neural network using as inputs inertial cues, motor commands, and battery voltage readings alongside the hidden state of the previous time-step to output robust velocity estimates and their associated uncertainty. An ensemble of networks is utilized to enhance the velocity and uncertainty predictions. Fusing the network's outputs into an Extended Kalman Filter, alongside inertial predictions and barometer updates, the method enables long-term underwater odometry without further exteroception. Furthermore, when integrated into visual-inertial odometry, the method assists in enhanced estimation resilience when dealing with an order of magnitude fewer total features tracked (as few as 1) as compared to conventional visual-inertial systems. Tested onboard an underwater robot deployed both in a laboratory pool and the Trondheim Fjord, the method takes less than 5 ms for inference either on the CPU or the GPU of an NVIDIA Orin AGX and demonstrates less than 4% relative position error in novel trajectories during complete visual blackout, and approximately 2% relative error when a maximum of 2 visual features from a monocular camera are available.",
    "citationCount": 0,
    "referenceCount": 25
}