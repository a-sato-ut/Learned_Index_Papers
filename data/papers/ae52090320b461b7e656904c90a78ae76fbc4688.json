{
    "paperId": "ae52090320b461b7e656904c90a78ae76fbc4688",
    "title": "Flow Factorized Representation Learning",
    "year": 2023,
    "venue": "Neural Information Processing Systems",
    "authors": [
        "Yue Song",
        "Thomas Anderson Keller",
        "N. Sebe",
        "Max Welling"
    ],
    "doi": "10.48550/arXiv.2309.13167",
    "arxivId": "2309.13167",
    "url": "https://www.semanticscholar.org/paper/ae52090320b461b7e656904c90a78ae76fbc4688",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2309.13167",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "A prominent goal of representation learning research is to achieve representations which are factorized in a useful manner with respect to the ground truth factors of variation. The fields of disentangled and equivariant representation learning have approached this ideal from a range of complimentary perspectives; however, to date, most approaches have proven to either be ill-specified or insufficiently flexible to effectively separate all realistic factors of interest in a learned latent space. In this work, we propose an alternative viewpoint on such structured representation learning which we call Flow Factorized Representation Learning, and demonstrate it to learn both more efficient and more usefully structured representations than existing frameworks. Specifically, we introduce a generative model which specifies a distinct set of latent probability paths that define different input transformations. Each latent flow is generated by the gradient field of a learned potential following dynamic optimal transport. Our novel setup brings new understandings to both \\textit{disentanglement} and \\textit{equivariance}. We show that our model achieves higher likelihoods on standard representation learning benchmarks while simultaneously being closer to approximately equivariant models. Furthermore, we demonstrate that the transformations learned by our model are flexibly composable and can also extrapolate to new data, implying a degree of robustness and generalizability approaching the ultimate goal of usefully factorized representation learning.",
    "citationCount": 5,
    "referenceCount": 99
}