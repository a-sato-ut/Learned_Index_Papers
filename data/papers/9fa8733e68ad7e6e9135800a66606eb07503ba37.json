{
    "paperId": "9fa8733e68ad7e6e9135800a66606eb07503ba37",
    "title": "Deep Reinforcement Learning for Tehran Stock Trading",
    "year": 2022,
    "venue": "Journal of Novel Engineering Science and Technology",
    "authors": [
        "Neda Yousefi"
    ],
    "doi": "10.56741/jnest.v1i02.171",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/9fa8733e68ad7e6e9135800a66606eb07503ba37",
    "isOpenAccess": true,
    "openAccessPdf": "https://journal.iistr.org/index.php/JNEST/article/download/171/125",
    "publicationTypes": null,
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Business",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "One of the most interesting topics for research, as well as for making a profit, is stock trading. It is known that artificial intelligence has had a great influence on this path. A lot of research has been done to investigate the application of machine learning and deep learning methods in stock trading. Despite the large amount of research done in the field of prediction and automation trading, stock trading as a deep reinforcement-learning problem remains an open research area. The progress of reinforcement learning, as well as the intrinsic properties of reinforcement learning, make it a suitable method for market trading in theory. In this paper, single stock trading models are presented based on the fine-tuned state-of-the-art deep reinforcement learning algorithms (Deep Deterministic Policy Gradient (DDPG) and Advantage Actor Critic (A2C)). These algorithms are able to interact with the trading market and capture the financial market dynamics. The proposed models are compared, evaluated, and verified on historical stock trading data. Annualized return and Sharpe ratio have been used to evaluate the performance of proposed models. The results show that the agent designed based on both algorithms is able to make intelligent decisions on historical data. The DDPG strategy performs better than the A2C and achieves better results in terms of convergence, stability, and evaluation criteria.",
    "citationCount": 0,
    "referenceCount": 19
}