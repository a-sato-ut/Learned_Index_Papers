{
    "paperId": "e3899eaf965565f947f9fef8db696bcda2732ee2",
    "title": "Generative Benchmark Creation for Table Union Search",
    "year": 2023,
    "venue": "arXiv.org",
    "authors": [
        "Koyena Pal",
        "Aamod Khatiwada",
        "Roee Shraga",
        "Ren√©e J. Miller"
    ],
    "doi": "10.48550/arXiv.2308.03883",
    "arxivId": "2308.03883",
    "url": "https://www.semanticscholar.org/paper/e3899eaf965565f947f9fef8db696bcda2732ee2",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2308.03883",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Data management has traditionally relied on synthetic data generators to generate structured benchmarks, like the TPC suite, where we can control important parameters like data size and its distribution precisely. These benchmarks were central to the success and adoption of database management systems. But more and more, data management problems are of a semantic nature. An important example is finding tables that can be unioned. While any two tables with the same cardinality can be unioned, table union search is the problem of finding tables whose union is semantically coherent. Semantic problems cannot be benchmarked using synthetic data. Our current methods for creating benchmarks involve the manual curation and labeling of real data. These methods are not robust or scalable and perhaps more importantly, it is not clear how robust the created benchmarks are. We propose to use generative AI models to create structured data benchmarks for table union search. We present a novel method for using generative models to create tables with specified properties. Using this method, we create a new benchmark containing pairs of tables that are both unionable and non-unionable but related. We thoroughly evaluate recent existing table union search methods over existing benchmarks and our new benchmark. We also present and evaluate a new table search methods based on recent large language models over all benchmarks. We show that the new benchmark is more challenging for all methods than hand-curated benchmarks, specifically, the top-performing method achieves a Mean Average Precision of around 60%, over 30% less than its performance on existing manually created benchmarks. We examine why this is the case and show that the new benchmark permits more detailed analysis of methods, including a study of both false positives and false negatives that were not possible with existing benchmarks.",
    "citationCount": 1,
    "referenceCount": 49
}