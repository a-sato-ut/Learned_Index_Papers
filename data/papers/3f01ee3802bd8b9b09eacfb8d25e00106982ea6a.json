{
    "paperId": "3f01ee3802bd8b9b09eacfb8d25e00106982ea6a",
    "title": "Towards Global Neural Network Abstractions with Locally-Exact Reconstruction",
    "year": 2022,
    "venue": "Neural Networks",
    "authors": [
        "Edoardo Manino",
        "I. Bessa",
        "Lucas C. Cordeiro"
    ],
    "doi": "10.48550/arXiv.2210.12054",
    "arxivId": "2210.12054",
    "url": "https://www.semanticscholar.org/paper/3f01ee3802bd8b9b09eacfb8d25e00106982ea6a",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2210.12054",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Medicine",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Neural networks are a powerful class of non-linear functions. However, their black-box nature makes it difficult to explain their behaviour and certify their safety. Abstraction techniques address this challenge by transforming the neural network into a simpler, over-approximated function. Unfortunately, existing abstraction techniques are slack, which limits their applicability to small local regions of the input domain. In this paper, we propose Global Interval Neural Network Abstractions with Center-Exact Reconstruction (GINNACER). Our novel abstraction technique produces sound over-approximation bounds over the whole input domain while guaranteeing exact reconstructions for any given local input. Our experiments show that GINNACER is several orders of magnitude tighter than state-of-the-art global abstraction techniques, while being competitive with local ones.",
    "citationCount": 1,
    "referenceCount": 60
}