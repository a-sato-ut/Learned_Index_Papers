{
    "paperId": "9c8d7eab81080bd66618e02e0dd114f03f35c38f",
    "title": "H2-LLM: Hardware-Dataflow Co-Exploration for Heterogeneous Hybrid-Bonding-based Low-Batch LLM Inference",
    "year": 2025,
    "venue": "International Symposium on Computer Architecture",
    "authors": [
        "Cong Li",
        "Yihan Yin",
        "Xintong Wu",
        "Jingchen Zhu",
        "Zhutianya Gao",
        "Dimin Niu",
        "Qiang Wu",
        "Xin Si",
        "Yuan Xie",
        "Chen Zhang",
        "Guangyu Sun"
    ],
    "doi": "10.1145/3695053.3731008",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/9c8d7eab81080bd66618e02e0dd114f03f35c38f",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Low-batch large language model (LLM) inference has been extensively applied to edge-side generative tasks, such as personal chat helper, virtual assistant, reception bot, private edge server, etc. To efficiently handle both prefill and decoding stages in LLM inference, near-memory processing (NMP) enabled heterogeneous computation paradigm has been proposed. However, existing NMP designs typically embed processing engines into DRAM dies, resulting in limited computation capacity, which in turn restricts their ability to accelerate edge-side low-batch LLM inference. To tackle this problem, we propose H \\( ^\\text{2} \\)-LLM, a Hybrid-bonding-based Heterogeneous accelerator for edge-side low-batch LLM inference. To balance the trade-off between computation capacity and bandwidth intrinsic to hybrid-bonding technology, we propose H \\( ^\\text{2} \\)-LLM’s architecture and extract its architecture design space. We further propose a data-centric dataflow abstraction to fully exploit the heterogeneous architecture’s acceleration opportunities in low-batch LLM inference. Based on the whole design space, we propose a design space exploration (DSE) framework to automatically find out the optimal design. Compared with existing in-die NMP-based heterogeneous accelerators, H \\( ^\\text{2} \\)-LLM achieves 2.72 × geomean speedup and 1.48 × geomean better energy efficiency. H \\( ^\\text{2} \\)-LLM’s data-centric dataflow exploration framework is open-sourced at https://github.com/leesou/H2-LLM-ISCA-2025.",
    "citationCount": 3,
    "referenceCount": 84
}