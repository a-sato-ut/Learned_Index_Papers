{
    "paperId": "ca3f6a761ea05ff0ead43f55af339269daa48e6e",
    "title": "Tsetlin Machine-Based Image Classification FPGA Accelerator With On-Device Training",
    "year": 2025,
    "venue": "IEEE Transactions on Circuits and Systems Part 1: Regular Papers",
    "authors": [
        "S. Tunheim",
        "Lei Jiao",
        "R. Shafik",
        "Alex Yakovlev",
        "Ole-Christoffer Granmo"
    ],
    "doi": "10.1109/TCSI.2024.3519191",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/ca3f6a761ea05ff0ead43f55af339269daa48e6e",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The Tsetlin Machine (TM) is a novel machine learning algorithm that uses Tsetlin automata (TAs) to define propositional logic expressions (clauses) for classification. This paper describes a field-programmable gate array (FPGA) accelerator for image classification based on the Convolutional Coalesced Tsetlin Machine. The accelerator classifies booleanized images of <inline-formula> <tex-math notation=\"LaTeX\">$28\\times 28$ </tex-math></inline-formula> pixels into 10 classes, and is configured with 128 clauses in a highly parallel architecture. To achieve fast clause evaluation and class prediction, the TA action signals and the clause weights per class are available from registers. Full on-device training is included, and the TAs are implemented with 34 Block RAM (BRAM) instances which operate in parallel. Each BRAM is addressed by the clause number and has a 72-bit word width that supports 8 TAs. The design is implemented in a Xilinx Zynq Ultrascale+ XCZU7 FPGA. Running at 50 MHz, the accelerator core achieves 134k image classifications per second, with an energy consumption per classification of <inline-formula> <tex-math notation=\"LaTeX\">$13.3~\\mu $ </tex-math></inline-formula> J. A single training epoch of 60k samples requires a processing time of 1.5 seconds. The accelerator obtains a test accuracy of 97.6% on MNIST, 84.1% on Fashion-MNIST and 82.8% on Kuzushiji-MNIST.",
    "citationCount": 7,
    "referenceCount": 51
}