{
    "paperId": "66818a9d410216214897ef3e0bdcb732f3aaad96",
    "title": "Self-Supervised Video Similarity Learning",
    "year": 2023,
    "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
    "authors": [
        "Giorgos Kordopatis-Zilos",
        "Giorgos Tolias",
        "Christos Tzelepis",
        "I. Kompatsiaris",
        "I. Patras",
        "S. Papadopoulos"
    ],
    "doi": "10.1109/CVPRW59228.2023.00504",
    "arxivId": "2304.03378",
    "url": "https://www.semanticscholar.org/paper/66818a9d410216214897ef3e0bdcb732f3aaad96",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2304.03378",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We introduce S2VS, a video similarity learning approach with self-supervision. Self-Supervised Learning (SSL) is typically used to train deep models on a proxy task so as to have strong transferability on target tasks after fine-tuning. Here, in contrast to prior work, SSL is used to perform video similarity learning and address multiple retrieval and detection tasks at once with no use of labeled data. This is achieved by learning via instance-discrimination with task-tailored augmentations and the widely used InfoNCE loss together with an additional loss operating jointly on self-similarity and hard-negative similarity. We benchmark our method on tasks where video relevance is defined with varying granularity, ranging from video copies to videos depicting the same incident or event. We learn a single universal model that achieves state-of-the-art performance on all tasks, surpassing previously proposed methods that use labeled data. The code and pretrained models are publicly available at: https://github.com/gkordo/s2vs",
    "citationCount": 13,
    "referenceCount": 84
}