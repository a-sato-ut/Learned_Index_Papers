{
    "paperId": "81f5c258beca404a70880f89d1f803f4c8063e73",
    "title": "DB-GPT: Large Language Model Meets Database",
    "year": 2024,
    "venue": "Data Science and Engineering",
    "authors": [
        "Xuanhe Zhou",
        "Zhaoyan Sun",
        "Guoliang Li"
    ],
    "doi": "10.1007/s41019-023-00235-6",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/81f5c258beca404a70880f89d1f803f4c8063e73",
    "isOpenAccess": true,
    "openAccessPdf": "https://link.springer.com/content/pdf/10.1007/s41019-023-00235-6.pdf",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Large language models (LLMs) have shown superior performance in various areas. And LLMs have the potential to revolutionize data management by serving as the \"brain\" of next-generation database systems. However, there are several challenges that utilize LLMs to optimize databases. First, it is challenging to provide appropriate prompts (e.g., instructions and demonstration examples) to enable LLMs to understand the database optimization problems. Second, LLMs only capture the logical database characters (e.g., SQL semantics) but are not aware of physical characters (e.g., data distributions), and it requires to fine-tune LLMs to capture both physical and logical information. Third, LLMs are not well trained for databases with strict constraints (e.g., query plan equivalence) and privacy-preserving requirements, and it is challenging to train database-specific LLMs while ensuring database privacy. To overcome these challenges, this vision paper proposes a LLM-based database framework (), including automatic prompt generation, DB-specific model fine-tuning, and DB-specific model design and pre-training. Preliminary experiments show that achieves relatively good performance in database tasks like query rewrite and index tuning. The source code and datasets are available at github.com/TsinghuaDatabaseGroup/DB-GPT.",
    "citationCount": 72,
    "referenceCount": 61
}