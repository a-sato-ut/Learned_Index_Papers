{
    "paperId": "9b6dc696fe044ba20fb11bebcc34466eedd6109a",
    "title": "Flexible Multiple-Objective Reinforcement Learning for Chip Placement",
    "year": 2022,
    "venue": "arXiv.org",
    "authors": [
        "Fu-Chieh Chang",
        "Yu-Wei Tseng",
        "Ya-Wen Yu",
        "Ssu-Rui Lee",
        "Alexandru Cioba",
        "I. Tseng",
        "Da-shan Shiu",
        "Jhih-Wei Hsu",
        "Cheng-Yuan Wang",
        "Chien-Yi Yang",
        "Ren-Chu Wang",
        "Yao-Wen Chang",
        "Tai-Chen Chen",
        "Tung-Chieh Chen"
    ],
    "doi": "10.48550/arXiv.2204.06407",
    "arxivId": "2204.06407",
    "url": "https://www.semanticscholar.org/paper/9b6dc696fe044ba20fb11bebcc34466eedd6109a",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2204.06407",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Recently, successful applications of reinforcement learning to chip placement have emerged. Pretrained models are necessary to improve efficiency and effectiveness. Currently, the weights of objective metrics (e.g., wirelength, congestion, and timing) are fixed during pretraining. However, fixed-weighed models cannot generate the diversity of placements required for engineers to accommodate changing requirements as they arise. This paper proposes flexible multiple-objective reinforcement learning (MORL) to support objective functions with inference-time variable weights using just a single pretrained model. Our macro placement results show that MORL can generate the Pareto frontier of multiple objectives effectively.",
    "citationCount": 7,
    "referenceCount": 29
}