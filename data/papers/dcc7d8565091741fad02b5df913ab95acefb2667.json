{
    "paperId": "dcc7d8565091741fad02b5df913ab95acefb2667",
    "title": "Automatic Abstraction Refinement in Neural Network Verification using Sensitivity Analysis",
    "year": 2023,
    "venue": "International Conference on Hybrid Systems: Computation and Control",
    "authors": [
        "Tobias Ladner",
        "M. Althoff"
    ],
    "doi": "10.1145/3575870.3587129",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/dcc7d8565091741fad02b5df913ab95acefb2667",
    "isOpenAccess": true,
    "openAccessPdf": "https://mediatum.ub.tum.de/doc/1710698/document.pdf",
    "publicationTypes": [
        "JournalArticle",
        "Book"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The formal verification of neural networks is essential for their application in safety-critical environments. However, the set-based verification of neural networks using linear approximations often obtains overly conservative results, while nonlinear approximations quickly become computationally infeasible in deep neural networks. We address this issue for the first time by automatically balancing between precision and computation time without splitting the propagated set. Our work introduces a novel automatic abstraction refinement approach using sensitivity analysis to iteratively reduce the abstraction error at the neuron level until either the specifications are met or a maximum number of iterations is reached. Our evaluation shows that we can tightly over-approximate the output sets of deep neural networks and that our approach is up to a thousand times faster than a naive approach. We further demonstrate the applicability of our approach in closed-loop settings.",
    "citationCount": 12,
    "referenceCount": 50
}