{
    "paperId": "03caf4e98ba27c300a7ce70d544450282b15e58d",
    "title": "Pre-training Generative Recommender with Multi-Identifier Item Tokenization",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Bowen Zheng",
        "Enze Liu",
        "Zhongfu Chen",
        "Zhongrui Ma",
        "Yue Wang",
        "Wayne Xin Zhao",
        "Ji-Rong Wen"
    ],
    "doi": "10.48550/arXiv.2504.04400",
    "arxivId": "2504.04400",
    "url": "https://www.semanticscholar.org/paper/03caf4e98ba27c300a7ce70d544450282b15e58d",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Generative recommendation autoregressively generates item identifiers to recommend potential items. Existing methods typically adopt a one-to-one mapping strategy, where each item is represented by a single identifier. However, this scheme poses issues, such as suboptimal semantic modeling for low-frequency items and limited diversity in token sequence data. To overcome these limitations, we propose MTGRec, which leverages Multi-identifier item Tokenization to augment token sequence data for Generative Recommender pre-training. Our approach involves two key innovations: multi-identifier item tokenization and curriculum recommender pre-training. For multi-identifier item tokenization, we leverage the RQ-VAE as the tokenizer backbone and treat model checkpoints from adjacent training epochs as semantically relevant tokenizers. This allows each item to be associated with multiple identifiers, enabling a single user interaction sequence to be converted into several token sequences as different data groups. For curriculum recommender pre-training, we introduce a curriculum learning scheme guided by data influence estimation, dynamically adjusting the sampling probability of each data group during recommender pre-training. After pre-training, we fine-tune the model using a single tokenizer to ensure accurate item identification for recommendation. Extensive experiments on three public benchmark datasets demonstrate that MTGRec significantly outperforms both traditional and generative recommendation baselines in terms of effectiveness and scalability.",
    "citationCount": 2,
    "referenceCount": 56
}