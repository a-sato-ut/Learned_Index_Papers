{
    "paperId": "fe21b0db9233e921d2e4073fd80baeca124b265a",
    "title": "Dense Passage Retrieval: Is it Retrieving?",
    "year": 2024,
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "authors": [
        "Benjamin Z. Reichman",
        "Larry Heck"
    ],
    "doi": "10.18653/v1/2024.findings-emnlp.791",
    "arxivId": "2402.11035",
    "url": "https://www.semanticscholar.org/paper/fe21b0db9233e921d2e4073fd80baeca124b265a",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2402.11035",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Dense passage retrieval (DPR) is the first step in the retrieval augmented generation (RAG) paradigm for improving the performance of large language models (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of the embeddings between queries and relevant textual data. A deeper understanding of DPR fine-tuning will be required to fundamentally unlock the full potential of this approach. In this work, we explore DPR-trained models mechanistically by using a combination of probing, layer activation analysis, and model editing. Our experiments show that DPR training decentralizes how knowledge is stored in the network, creating multiple access pathways to the same information. We also uncover a limitation in this training style: the internal knowledge of the pre-trained model bounds what the retrieval model can retrieve. These findings suggest a few possible directions for dense retrieval: (1) expose the DPR training process to more knowledge so more can be decentralized, (2) inject facts as decentralized representations, (3) model and incorporate knowledge uncertainty in the retrieval process, and (4) directly map internal model knowledge to a knowledge base.",
    "citationCount": 10,
    "referenceCount": 45
}