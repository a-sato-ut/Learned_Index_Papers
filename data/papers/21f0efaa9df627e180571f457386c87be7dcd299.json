{
    "paperId": "21f0efaa9df627e180571f457386c87be7dcd299",
    "title": "Model-enhanced Vector Index",
    "year": 2023,
    "venue": "Neural Information Processing Systems",
    "authors": [
        "Hailin Zhang",
        "Yujing Wang",
        "Qi Chen",
        "Ruiheng Chang",
        "Ting Zhang",
        "Ziming Miao",
        "Yingyan Hou",
        "Yan Ding",
        "Xupeng Miao",
        "Haonan Wang",
        "Bochen Pang",
        "Yu-Wei Zhan",
        "Hao Sun",
        "Weiwei Deng",
        "Qi Zhang",
        "Fan Yang",
        "Xing Xie",
        "Mao Yang",
        "Bin Cui"
    ],
    "doi": "10.48550/arXiv.2309.13335",
    "arxivId": "2309.13335",
    "url": "https://www.semanticscholar.org/paper/21f0efaa9df627e180571f457386c87be7dcd299",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2309.13335",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Embedding-based retrieval methods construct vector indices to search for document representations that are most similar to the query representations. They are widely used in document retrieval due to low latency and decent recall performance. Recent research indicates that deep retrieval solutions offer better model quality, but are hindered by unacceptable serving latency and the inability to support document updates. In this paper, we aim to enhance the vector index with end-to-end deep generative models, leveraging the differentiable advantages of deep retrieval models while maintaining desirable serving efficiency. We propose Model-enhanced Vector Index (MEVI), a differentiable model-enhanced index empowered by a twin-tower representation model. MEVI leverages a Residual Quantization (RQ) codebook to bridge the sequence-to-sequence deep retrieval and embedding-based models. To substantially reduce the inference time, instead of decoding the unique document ids in long sequential steps, we first generate some semantic virtual cluster ids of candidate documents in a small number of steps, and then leverage the well-adapted embedding vectors to further perform a fine-grained search for the relevant documents in the candidate virtual clusters. We empirically show that our model achieves better performance on the commonly used academic benchmarks MSMARCO Passage and Natural Questions, with comparable serving latency to dense retrieval solutions.",
    "citationCount": 6,
    "referenceCount": 57
}