{
    "paperId": "9a97f39394e7666e077bbe977a89fd483308bb6b",
    "title": "Three Heads Are Better than One: Improving Cross-Domain NER with Progressive Decomposed Network",
    "year": 2024,
    "venue": "AAAI Conference on Artificial Intelligence",
    "authors": [
        "Xuming Hu",
        "Zhaochen Hong",
        "Yong Jiang",
        "Zhichao Lin",
        "Xiaobin Wang",
        "Pengjun Xie",
        "Philip S. Yu"
    ],
    "doi": "10.1609/aaai.v38i16.29785",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/9a97f39394e7666e077bbe977a89fd483308bb6b",
    "isOpenAccess": true,
    "openAccessPdf": "https://ojs.aaai.org/index.php/AAAI/article/download/29785/31356",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Cross-domain named entity recognition (NER) tasks encourage NER models to transfer knowledge from data-rich source domains to sparsely labeled target domains. Previous works adopt the paradigms of pre-training on the source domain followed by fine-tuning on the target domain. However, these works ignore that general labeled NER source domain data can be easily retrieved in the real world, and soliciting more source domains could bring more benefits. Unfortunately, previous paradigms cannot efficiently transfer knowledge from multiple source domains. In this work, to transfer multiple source domains' knowledge, we decouple the NER task into the pipeline tasks of mention detection and entity typing, where the mention detection unifies the training object across domains, thus providing the entity typing with higher-quality entity mentions. Additionally, we request multiple general source domain models to suggest the potential named entities for sentences in the target domain explicitly, and transfer their knowledge to the target domain models through the knowledge progressive networks implicitly. Furthermore, we propose two methods to analyze in which source domain knowledge transfer occurs, thus helping us judge which source domain brings the greatest benefit. In our experiment, we develop a Chinese cross-domain NER dataset. Our model improved the F1 score by an average of 12.50% across 8 Chinese and English datasets compared to models without source domain data.",
    "citationCount": 1,
    "referenceCount": 40
}