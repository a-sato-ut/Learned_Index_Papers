{
    "paperId": "a0f487ac7852a10769729f7a530f5fcfaf7466a0",
    "title": "Symbolic Learning to Optimize: Towards Interpretability and Scalability",
    "year": 2022,
    "venue": "International Conference on Learning Representations",
    "authors": [
        "Wenqing Zheng",
        "Tianlong Chen",
        "Ting-Kuei Hu",
        "Zhangyang Wang"
    ],
    "doi": "10.48550/arXiv.2203.06578",
    "arxivId": "2203.06578",
    "url": "https://www.semanticscholar.org/paper/a0f487ac7852a10769729f7a530f5fcfaf7466a0",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2203.06578",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Recent studies on Learning to Optimize (L2O) suggest a promising path to automating and accelerating the optimization procedure for complicated tasks. Existing L2O models parameterize optimization rules by neural networks, and learn those numerical rules via meta-training. However, they face two common pitfalls: (1) scalability: the numerical rules represented by neural networks create extra memory overhead for applying L2O models, and limit their applicability to optimizing larger tasks; (2) interpretability: it is unclear what an L2O model has learned in its black-box optimization rule, nor is it straightforward to compare different L2O models in an explainable way. To avoid both pitfalls, this paper proves the concept that we can\"kill two birds by one stone\", by introducing the powerful tool of symbolic regression to L2O. In this paper, we establish a holistic symbolic representation and analysis framework for L2O, which yields a series of insights for learnable optimizers. Leveraging our findings, we further propose a lightweight L2O model that can be meta-trained on large-scale problems and outperformed human-designed and tuned optimizers. Our work is set to supply a brand-new perspective to L2O research. Codes are available at: https://github.com/VITA-Group/Symbolic-Learning-To-Optimize.",
    "citationCount": 20,
    "referenceCount": 68
}