{
    "paperId": "e8d3b3944576d1c6722b832c4ebb9f85c6f549e1",
    "title": "OLLIE: Derivation-based Tensor Program Optimizer",
    "year": 2022,
    "venue": "arXiv.org",
    "authors": [
        "Liyan Zheng",
        "Haojie Wang",
        "Jidong Zhai",
        "Muyan Hu",
        "Zixuan Ma",
        "Tuowei Wang",
        "Shizhi Tang",
        "Lei Xie",
        "Kezhao Huang",
        "Zhihao Jia"
    ],
    "doi": "10.48550/arXiv.2208.02025",
    "arxivId": "2208.02025",
    "url": "https://www.semanticscholar.org/paper/e8d3b3944576d1c6722b832c4ebb9f85c6f549e1",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2208.02025",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Boosting the runtime performance of deep neural networks (DNNs) is critical due to their wide adoption in real-world tasks. Existing approaches to optimizing the tensor algebra expression of a DNN only consider expressions representable by a fixed set of predefined operators, missing possible optimization opportunities between general expressions. We propose OLLIE, the first derivation-based tensor program optimizer. OLLIE optimizes tensor programs by leveraging transformations between general tensor algebra expressions, enabling a significantly larger expression search space that includes those supported by prior work as special cases. OLLIE uses a hybrid derivation-based optimizer that effectively combines explorative and guided derivations to quickly discover highly optimized expressions. Evaluation on seven DNNs shows that OLLIE can outperform existing optimizers by up to 2.73$\\times$ (1.46$\\times$ on average) on an A100 GPU and up to 2.68$\\times$ (1.51$\\times$) on a V100 GPU, respectively.",
    "citationCount": 3,
    "referenceCount": 40
}