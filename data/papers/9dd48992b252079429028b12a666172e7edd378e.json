{
    "paperId": "9dd48992b252079429028b12a666172e7edd378e",
    "title": "Utilizing Dynamic Sparsity on Pretrained DETR",
    "year": 2025,
    "venue": "International Workshop on Machine Learning for Signal Processing",
    "authors": [
        "Reza Sedghi",
        "Anand Subramoney",
        "David Kappel"
    ],
    "doi": "10.1109/MLSP62443.2025.11204273",
    "arxivId": "2510.09380",
    "url": "https://www.semanticscholar.org/paper/9dd48992b252079429028b12a666172e7edd378e",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": null,
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Efficient inference with transformer-based models remains a challenge, especially in vision tasks like object detection. We analyze the inherent sparsity in the MLP layers of DETR and introduce two methods to exploit it without retraining. First, we propose Static Indicator-Based Sparsification (SIBS), a heuristic method that predicts neuron inactivity based on fixed activation patterns. While simple, SIBS offers limited gains due to the input-dependent nature of sparsity. To address this, we introduce Micro-Gated Sparsification (MGS), a lightweight gating mechanism trained on top of a pretrained DETR. MGS predicts dynamic sparsity using a small linear layer and achieves up to $85-95 \\%$ activation sparsity. Experiments on the COCO dataset show that MGS maintains or even improves performance while significantly reducing computation. Our method offers a practical, input-adaptive approach to sparsification, enabling efficient deployment of pretrained vision transformers without full model retraining.",
    "citationCount": 0,
    "referenceCount": 15
}