{
    "paperId": "d7b1afdd426b732ed00e321e576ccb39e6d44655",
    "title": "Imbalanced Learning Ensemble Optimization",
    "year": 2025,
    "venue": "Conference of the Open Innovations Association",
    "authors": [
        "M. Aliev",
        "Sergey Borisovich Muravyov"
    ],
    "doi": "10.23919/FRUCT65909.2025.11008125",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/d7b1afdd426b732ed00e321e576ccb39e6d44655",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Our world is far from being perfectly balanced, and therefore most real-world data is inherently imbalanced. To effectively work with such data, there are various approaches to imbalanced learning. However, finding the most successful model configuration for a specific dataset is a problem, even for an experienced data scientist. To solve it, there are various approaches to automated machine learning. However, their applicability to imbalanced learning problems is an open question. This paper presents an approach to the optimization search of bagging and boosting ensembles (based on re-weighting and undersampling techniques), and also provides an experimental comparison with the well-known FLAML solution. The benchmark results on data with the imbalance level of moderate and extreme demonstrated worthy competition in f1-measure and an overwhelming advantage in balanced accuracy. The optimization search time was on average several minutes longer for our solution. The proposed approach is implemented as an open source framework and can be found on Github (ImbaML).",
    "citationCount": 0,
    "referenceCount": 17
}