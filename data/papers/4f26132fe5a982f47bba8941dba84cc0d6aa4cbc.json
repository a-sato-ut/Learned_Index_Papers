{
    "paperId": "4f26132fe5a982f47bba8941dba84cc0d6aa4cbc",
    "title": "RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses",
    "year": 2022,
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "authors": [
        "Honglei Zhuang",
        "Zhen Qin",
        "R. Jagerman",
        "Kai Hui",
        "Ji Ma",
        "Jing Lu",
        "Jianmo Ni",
        "Xuanhui Wang",
        "Michael Bendersky"
    ],
    "doi": "10.1145/3539618.3592047",
    "arxivId": "2210.10634",
    "url": "https://www.semanticscholar.org/paper/4f26132fe5a982f47bba8941dba84cc0d6aa4cbc",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2210.10634",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Pretrained language models such as BERT have been shown to be exceptionally effective for text ranking. However, there are limited studies on how to leverage more powerful sequence-to-sequence models such as T5. Existing attempts usually formulate text ranking as a classification problem and rely on postprocessing to obtain a ranked list. In this paper, we propose RankT5 and study two T5-based ranking model structures, an encoder-decoder and an encoder-only one, so that they not only can directly output ranking scores for each query-document pair, but also can be fine-tuned with pairwise or listwise ranking losses to optimize ranking performance. Our experiments show that the proposed models with ranking losses can achieve substantial ranking performance gains on different public text ranking data sets. Moreover, ranking models fine-tuned with listwise ranking losses have better zero-shot ranking performance on out-of-domain data than models fine-tuned with classification losses.",
    "citationCount": 172,
    "referenceCount": 60
}