{
    "paperId": "1dcd3ce2221eff4213487249978f7ec844f1c611",
    "title": "Communication-efficient Decentralized Machine Learning over Heterogeneous Networks",
    "year": 2020,
    "venue": "IEEE International Conference on Data Engineering",
    "authors": [
        "Pan Zhou",
        "Qian Lin",
        "Dumitrel Loghin",
        "B. Ooi",
        "Yuncheng Wu",
        "Hongfang Yu"
    ],
    "doi": "10.1109/ICDE51399.2021.00040",
    "arxivId": "2009.05766",
    "url": "https://www.semanticscholar.org/paper/1dcd3ce2221eff4213487249978f7ec844f1c611",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2009.05766",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "In the last few years, distributed machine learning has been usually executed over heterogeneous networks such as a local area network within a multi-tenant cluster or a wide area network connecting data centers and edge clusters. In these heterogeneous networks, the link speeds among worker nodes vary significantly, making it challenging for state-of-the-art machine learning approaches to perform efficient training. Both centralized and decentralized training approaches suffer from low-speed links. In this paper, we propose a decentralized approach, namely NetMax, that enables worker nodes to communicate via high-speed links and, thus, significantly speed up the training process. NetMax possesses the following novel features. First, it consists of a novel consensus algorithm that allows worker nodes to train model copies on their local dataset asynchronously and exchange information via peer-to-peer communication to synchronize their local copies, instead of a central master node (i.e., parameter server). Second, each worker node selects one peer randomly with a fine-tuned probability to exchange information per iteration. In particular, peers with high-speed links are selected with high probability. Third, the probabilities of selecting peers are designed to minimize the total convergence time. Moreover, we mathematically prove the convergence of NetMax. We evaluate NetMax on heterogeneous cluster networks and show that it achieves speedups of 3.7×, 3.4×, and 1.9× in comparison with the state-of-the-art decentralized training approaches Prague, Allreduce-SGD, and AD-PSGD, respectively.",
    "citationCount": 44,
    "referenceCount": 41
}