{
    "paperId": "fc580c211689663a64f42e2ba92c864cb134ba9b",
    "title": "Graph Neural Networks for Learning Equivariant Representations of Neural Networks",
    "year": 2024,
    "venue": "International Conference on Learning Representations",
    "authors": [
        "Miltiadis Kofinas",
        "Boris Knyazev",
        "Yan Zhang",
        "Yunlu Chen",
        "G. Burghouts",
        "E. Gavves",
        "Cees G. M. Snoek",
        "David W. Zhang"
    ],
    "doi": "10.48550/arXiv.2403.12143",
    "arxivId": "2403.12143",
    "url": "https://www.semanticscholar.org/paper/fc580c211689663a64f42e2ba92c864cb134ba9b",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalization performance, and learning to optimize, while consistently outperforming state-of-the-art methods. The source code is open-sourced at https://github.com/mkofinas/neural-graphs.",
    "citationCount": 47,
    "referenceCount": 47
}