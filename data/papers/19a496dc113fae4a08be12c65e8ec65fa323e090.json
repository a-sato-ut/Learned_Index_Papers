{
    "paperId": "19a496dc113fae4a08be12c65e8ec65fa323e090",
    "title": "A Learned Performance Model for the Tensor Processing Unit",
    "year": 2020,
    "venue": "arXiv.org",
    "authors": [
        "Samuel J. Kaufman",
        "P. Phothilimthana",
        "Yanqi Zhou",
        "Mike Burrows"
    ],
    "doi": null,
    "arxivId": "2008.01040",
    "url": "https://www.semanticscholar.org/paper/19a496dc113fae4a08be12c65e8ec65fa323e090",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Accurate hardware performance models are critical to efficient code generation. They can be used by compilers to make heuristic decisions, by superoptimizers as an minimization objective, or by autotuners to find an optimal configuration of a specific program. However, they are difficult to develop because contemporary processors are complex, and the recent proliferation of deep learning accelerators has increased the development burden. We demonstrate a method of learning performance models from a corpus of tensor computation graph programs for the Tensor Processing Unit (TPU). We train a neural network over kernel-level sub-graphs from the corpus and find that the learned model is competitive to a heavily-optimized analytical cost model used in the production XLA compiler.",
    "citationCount": 8,
    "referenceCount": 34
}