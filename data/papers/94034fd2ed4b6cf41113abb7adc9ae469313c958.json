{
    "paperId": "94034fd2ed4b6cf41113abb7adc9ae469313c958",
    "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
    "year": 2024,
    "venue": "arXiv.org",
    "authors": [
        "Yizheng Huang",
        "Jimmy X. Huang"
    ],
    "doi": "10.48550/arXiv.2404.10981",
    "arxivId": "2404.10981",
    "url": "https://www.semanticscholar.org/paper/94034fd2ed4b6cf41113abb7adc9ae469313c958",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Review"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but possibly incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.",
    "citationCount": 70,
    "referenceCount": 153
}