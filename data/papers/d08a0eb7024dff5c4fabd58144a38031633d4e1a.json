{
    "paperId": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
    "title": "Benchmarking Graph Neural Networks",
    "year": 2023,
    "venue": "Journal of machine learning research",
    "authors": [
        "Vijay Prakash Dwivedi",
        "Chaitanya K. Joshi",
        "T. Laurent",
        "Yoshua Bengio",
        "X. Bresson"
    ],
    "doi": null,
    "arxivId": "2003.00982",
    "url": "https://www.semanticscholar.org/paper/d08a0eb7024dff5c4fabd58144a38031633d4e1a",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Graph neural networks (GNNs) have become the standard toolkit for analyzing and learning from data on graphs. As the field grows, it becomes critical to identify key architectures and validate new ideas that generalize to larger, more complex datasets. Unfortunately, it has been increasingly difficult to gauge the effectiveness of new models in the absence of a standardized benchmark with consistent experimental settings. In this paper, we introduce a reproducible GNN benchmarking framework, with the facility for researchers to add new models conveniently for arbitrary datasets. We demonstrate the usefulness of our framework by presenting a principled investigation into the recent Weisfeiler-Lehman GNNs (WL-GNNs) compared to message passing-based graph convolutional networks (GCNs) for a variety of graph tasks, i.e. graph regression/classification and node/link prediction, with medium-scale datasets.",
    "citationCount": 1049,
    "referenceCount": 147
}