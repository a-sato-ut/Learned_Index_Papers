{
    "paperId": "70e3fac68959e2175184bdc60e2986cf47df0071",
    "title": "EQUI-VOCAL: Synthesizing Queries for Compositional Video Events from Limited User Interactions",
    "year": 2023,
    "venue": "Proceedings of the VLDB Endowment",
    "authors": [
        "Enhao Zhang",
        "Maureen Daum",
        "Dong He",
        "M. Balazinska",
        "Brandon Haynes",
        "Ranjay Krishna"
    ],
    "doi": "10.14778/3611479.3611482",
    "arxivId": "2301.00929",
    "url": "https://www.semanticscholar.org/paper/70e3fac68959e2175184bdc60e2986cf47df0071",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We introduce EQUI-VOCAL: a new system that automatically synthesizes queries over videos from limited user interactions. The user only provides a handful of positive and negative examples of what they are looking for. EQUI-VOCAL utilizes these initial examples and additional ones collected through active learning to efficiently synthesize complex user queries. Our approach enables users to find events without database expertise, with limited labeling effort, and without declarative specifications or sketches. Core to EQUI-VOCAL's design is the use of spatio-temporal scene graphs in its data model and query language and a novel query synthesis approach that works on large and noisy video data. Our system outperforms two baseline systems---in terms of F1 score, synthesis time, and robustness to noise---and can flexibly synthesize complex queries that the baselines do not support.",
    "citationCount": 14,
    "referenceCount": 90
}