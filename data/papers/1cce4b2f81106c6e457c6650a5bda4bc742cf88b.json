{
    "paperId": "1cce4b2f81106c6e457c6650a5bda4bc742cf88b",
    "title": "Decouple knowledge from paramters for plug-and-play language modeling",
    "year": 2023,
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "authors": [
        "Xin Cheng",
        "Yankai Lin",
        "Xiuying Chen",
        "Dongyan Zhao",
        "Rui Yan"
    ],
    "doi": "10.48550/arXiv.2305.11564",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/1cce4b2f81106c6e457c6650a5bda4bc742cf88b",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2305.11564",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Pre-trained language models (PLM) have made impressive results in various NLP tasks. It has been revealed that one of the key factors to their success is the parameters of these models implicitly learn all kinds of knowledge during pre-training. However, encoding knowledge implicitly in the model parameters has two fundamental drawbacks. First, the knowledge is neither editable nor scalable once the model is trained, which is especially problematic in that knowledge is consistently evolving. Second, it lacks interpretability and prevents humans from understanding which knowledge PLM requires for a certain problem. In this paper, we introduce PlugLM, a pre-training model with differentiable plug-in memory (DPM). The key intuition is to de-couple the knowledge storage from model parameters with an editable and scalable key-value memory and leverage knowledge in an explainable manner by knowledge retrieval in the DPM. To justify this design choice, we conduct evaluations in three settings including: (1) domain adaptation. PlugLM obtains 3.95 F1 improvements across four domains on average without any in-domain pre-training. (2) knowledge update. PlugLM could absorb new knowledge in a training-free way after pre-training is done. (3) in-task knowledge learning. PlugLM could be further improved by incorporating training samples into DPM with knowledge prompting 1",
    "citationCount": 10,
    "referenceCount": 86
}