{
    "paperId": "d7fc18bd04ba7403be5d638028edba0c510a6f8d",
    "title": "Target Prompting for Information Extraction with Vision Language Model",
    "year": 2024,
    "venue": "arXiv.org",
    "authors": [
        "Dipankar Medhi"
    ],
    "doi": "10.48550/arXiv.2408.03834",
    "arxivId": "2408.03834",
    "url": "https://www.semanticscholar.org/paper/d7fc18bd04ba7403be5d638028edba0c510a6f8d",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The recent trend in the Large Vision and Language model has brought a new change in how information extraction systems are built. VLMs have set a new benchmark with their State-of-the-art techniques in understanding documents and building question-answering systems across various industries. They are significantly better at generating text from document images and providing accurate answers to questions. However, there are still some challenges in effectively utilizing these models to build a precise conversational system. General prompting techniques used with large language models are often not suitable for these specially designed vision language models. The output generated by such generic input prompts is ordinary and may contain information gaps when compared with the actual content of the document. To obtain more accurate and specific answers, a well-targeted prompt is required by the vision language model, along with the document image. In this paper, a technique is discussed called Target prompting, which focuses on explicitly targeting parts of document images and generating related answers from those specific regions only. The paper also covers the evaluation of response for each prompting technique using different user queries and input prompts.",
    "citationCount": 0,
    "referenceCount": 21
}