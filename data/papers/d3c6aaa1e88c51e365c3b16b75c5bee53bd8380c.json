{
    "paperId": "d3c6aaa1e88c51e365c3b16b75c5bee53bd8380c",
    "title": "MOPAR: A Model Partitioning Framework for Deep Learning Inference Services on Serverless Platforms",
    "year": 2024,
    "venue": "arXiv.org",
    "authors": [
        "Jiaang Duan",
        "Shiyou Qian",
        "Dingyu Yang",
        "Hanwen Hu",
        "Jian Cao",
        "Guangtao Xue"
    ],
    "doi": "10.48550/arXiv.2404.02445",
    "arxivId": "2404.02445",
    "url": "https://www.semanticscholar.org/paper/d3c6aaa1e88c51e365c3b16b75c5bee53bd8380c",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "With its elastic power and a pay-as-you-go cost model, the deployment of deep learning inference services (DLISs) on serverless platforms is emerging as a prevalent trend. However, the varying resource requirements of different layers in DL models hinder resource utilization and increase costs, when DLISs are deployed as a single function on serverless platforms. To tackle this problem, we propose a model partitioning framework called MOPAR. This work is based on the two resource usage patterns of DLISs: global differences and local similarity, due to the presence of resource dominant (RD) operators and layer stacking. Considering these patterns, MOPAR adopts a hybrid approach that initially divides the DL model vertically into multiple slices composed of similar layers to improve resource efficiency. Slices containing RD operators are further partitioned into multiple sub-slices, enabling parallel optimization to reduce inference latency. Moreover, MOPAR comprehensively employs data compression and share-memory techniques to offset the additional time introduced by communication between slices. We implement a prototype of MOPAR and evaluate its efficacy using four categories of 12 DL models on OpenFaaS and AWS Lambda. The experiment results show that MOPAR can improve the resource efficiency of DLISs by 27.62\\% on average, while reducing latency by about 5.52\\%. Furthermore, based on Lambda's pricing, the cost of running DLISs is reduced by about 2.58 $\\times$ using MOPAR.",
    "citationCount": 2,
    "referenceCount": 45
}