{
    "paperId": "25f17c2ab2bd57c482980e350e39e6aba8fc31c0",
    "title": "SparDL: Distributed Deep Learning Training with Efficient Sparse Communication",
    "year": 2023,
    "venue": "IEEE International Conference on Data Engineering",
    "authors": [
        "Minjun Zhao",
        "Yichen Yin",
        "Yuren Mao",
        "Lu Chen",
        "Yunjun Gao"
    ],
    "doi": "10.1109/ICDE60146.2024.00142",
    "arxivId": "2304.00737",
    "url": "https://www.semanticscholar.org/paper/25f17c2ab2bd57c482980e350e39e6aba8fc31c0",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2304.00737",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Top-k sparsification has recently been widely used to reduce the communication volume in distributed deep learning. However, due to the Sparse Gradient Accumulation (SGA) dilemma, the performance of top-k sparsification still has limitations. Recently, a few methods have been put forward to handle the SGA dilemma. Regrettably, even the state-of-the-art method suffers from several drawbacks, e.g., it relies on an inefficient communication algorithm and requires extra transmission steps. Motivated by the limitations of existing methods, we propose a novel efficient sparse communication framework, called SparDL. Specifically, SparDL uses the Spar-Reduce-Scatter algorithm, which is based on an efficient Reduce-Scatter model, to handle the SGA dilemma without additional communication operations. Besides, to further reduce the latency cost and improve the efficiency of SparDL, we propose the Spar-All-Gather algorithm. Moreover, we propose the global residual collection algorithm to ensure fast convergence of model training. Finally, extensive experiments are conducted to validate the superiority of SparDL.",
    "citationCount": 1,
    "referenceCount": 65
}