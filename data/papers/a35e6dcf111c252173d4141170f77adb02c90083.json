{
    "paperId": "a35e6dcf111c252173d4141170f77adb02c90083",
    "title": "Understanding User Sensemaking in Machine Learning Fairness Assessment Systems",
    "year": 2021,
    "venue": "The Web Conference",
    "authors": [
        "Ziwei Gu",
        "Jing Nathan Yan",
        "Jeffrey M. Rzeszotarski"
    ],
    "doi": "10.1145/3442381.3450092",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/a35e6dcf111c252173d4141170f77adb02c90083",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "A variety of systems have been proposed to assist users in detecting machine learning (ML) fairness issues. These systems approach bias reduction from a number of perspectives, including recommender systems, exploratory tools, and dashboards. In this paper, we seek to inform the design of these systems by examining how individuals make sense of fairness issues as they use different de-biasing affordances. In particular, we consider the tension between de-biasing recommendations which are quick but may lack nuance and ”what-if” style exploration which is time consuming but may lead to deeper understanding and transferable insights. Using logs, think-aloud data, and semi-structured interviews we find that exploratory systems promote a rich pattern of hypothesis generation and testing, while recommendations deliver quick answers which satisfy participants at the cost of reduced information exposure. We highlight design requirements and trade-offs in the design of ML fairness systems to promote accurate and explainable assessments.",
    "citationCount": 11,
    "referenceCount": 81
}