{
    "paperId": "4c731720636b0f53aa7d7ecd2acc40731218cd71",
    "title": "Enabling Efficient Large Recommendation Model Training with Near CXL Memory Processing",
    "year": 2024,
    "venue": "International Symposium on Computer Architecture",
    "authors": [
        "Haifeng Liu",
        "Long Zheng",
        "Yu Huang",
        "Jingyi Zhou",
        "Chao Liu",
        "Runze Wang",
        "Xiaofei Liao",
        "Hai Jin",
        "Jingling Xue"
    ],
    "doi": "10.1109/ISCA59077.2024.00036",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/4c731720636b0f53aa7d7ecd2acc40731218cd71",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Personalized recommendation systems have become one of the most important Internet services nowadays. A critical challenge of training and deploying the recommendation models is their high memory capacity and bandwidth demands, with the embedding layers occupying hundreds of GBs to TBs of storage. The advent of memory disaggregation technology and Compute Express Link (CXL) provides a promising solution for memory capacity scaling. However, relocating memory-intensive embedding layers to CXL memory incurs noticeable performance degradation due to its limited transmission bandwidth, which is significantly lower than the host memory bandwidth. To address this, we introduce ReCXL, a CXL memory disaggregation system that utilizes near-memory processing for scalable, efficient recommendation model training. ReCXL features a unified, hardwareefficient NMP architecture that processes the entire embedding training within CXL memory, minimizing data transfers over the bandwidth-limited CXL and enhancing internal bandwidth. To further improve the performance, ReCXL incorporates softwarehardware co-optimizations, including sophisticated dependencyfree prefetching and fine-grained update scheduling, to maximize hardware utilization. Evaluation results show that ReCXL outperforms the CPU-GPU baseline and the na√Øve CXL memory by $7.1 \\times \\sim 10.6 \\times(9.4 \\times$ on average) and $12.7 \\times \\sim 31.3 \\times(22.6 \\times$ on average), respectively.",
    "citationCount": 11,
    "referenceCount": 66
}