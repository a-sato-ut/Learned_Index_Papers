{
    "paperId": "313f77fec4a2a18e84eea1d9923bd94b732ec2b2",
    "title": "Deep Image Compositing",
    "year": 2020,
    "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
    "authors": [
        "He Zhang",
        "Jianming Zhang",
        "Federico Perazzi",
        "Zhe L. Lin",
        "Vishal M. Patel"
    ],
    "doi": "10.1109/WACV48630.2021.00041",
    "arxivId": "2011.02146",
    "url": "https://www.semanticscholar.org/paper/313f77fec4a2a18e84eea1d9923bd94b732ec2b2",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Image compositing is a task of combining regions from different images to compose a new image. A common use case is background replacement of portrait images. To obtain high quality composites, professionals typically manually perform multiple editing steps such as segmentation, matting and foreground color decontamination, which is very time consuming even with sophisticated photo editing tools. In this paper, we propose a new method which can automatically generate high-quality image compositing with-out any user input. Our method can be trained end-to-end to optimize exploitation of contextual and color information of both foreground and background images, where the com-positing quality is considered in the optimization. Specifically, inspired by Laplacian pyramid blending, a dense-connected multi-stream fusion network is proposed to effectively fuse the information from the foreground and back-ground images at different scales. In addition, we intro-duce a self-taught strategy to progressively train from easy to complex cases to mitigate the lack of training data. Experiments show that the proposed method can automatically generate high-quality composites and outperforms existing methods both qualitatively and quantitatively.",
    "citationCount": 35,
    "referenceCount": 49
}