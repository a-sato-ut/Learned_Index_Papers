{
    "paperId": "4dce1d40b89ab65ab3fc344bb449fdeeb0983b8a",
    "title": "TREC Deep Learning Track: Reusable Test Collections in the Large Data Regime",
    "year": 2021,
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "authors": [
        "Nick Craswell",
        "Bhaskar Mitra",
        "Emine Yilmaz",
        "Daniel Fernando Campos",
        "E. Voorhees",
        "I. Soboroff"
    ],
    "doi": "10.1145/3404835.3463249",
    "arxivId": "2104.09399",
    "url": "https://www.semanticscholar.org/paper/4dce1d40b89ab65ab3fc344bb449fdeeb0983b8a",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2104.09399",
    "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference",
        "Review"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The TREC Deep Learning (DL) Track studies ad hoc search in the large data regime, meaning that a large set of human-labeled training data is available. Results so far indicate that the best models with large data may be deep neural networks. This paper supports the reuse of the TREC DL test collections in three ways. First we describe the data sets in detail, documenting clearly and in one place some details that are otherwise scattered in track guidelines, overview papers and in our associated MS MARCO leaderboard pages. We intend this description to make it easy for newcomers to use the TREC DL data. Second, because there is some risk of iteration and selection bias when reusing a data set, we describe the best practices for writing a paper using TREC DL data, without overfitting. We provide some illustrative analysis. Finally we address a number of issues around the TREC DL data, including an analysis of reusability.",
    "citationCount": 59,
    "referenceCount": 34
}