{
    "paperId": "54f1431e099d45f20f4494ddb0a39512857f126c",
    "title": "PromptDSI: Prompt-Based Rehearsal-Free Continual Learning for Document Retrieval",
    "year": 2024,
    "venue": "ECML/PKDD",
    "authors": [
        "Tuan-Luc Huynh",
        "Thuy-Trang Vu",
        "Weiqing Wang",
        "Yinwei Wei",
        "Trung Le",
        "D. Gašević",
        "Yuan-Fang Li",
        "Thanh-Toan Do"
    ],
    "doi": "10.1007/978-3-032-06109-6_22",
    "arxivId": "2406.12593",
    "url": "https://www.semanticscholar.org/paper/54f1431e099d45f20f4494ddb0a39512857f126c",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Differentiable Search Index (DSI) utilizes pre-trained language models to perform indexing and document retrieval via end-to-end learning without relying on external indexes. However, DSI requires full re-training to index new documents, causing significant computational inefficiencies. Continual learning (CL) offers a solution by enabling the model to incrementally update without full re-training. Existing CL solutions in document retrieval rely on memory buffers or generative models for rehearsal, which is infeasible when accessing previous training data is restricted due to privacy concerns. To this end, we introduce PromptDSI, a prompt-based, rehearsal-free continual learning approach for document retrieval. PromptDSI follows the Prompt-based Continual Learning (PCL) framework, using learnable prompts to efficiently index new documents without accessing previous documents or queries. To improve retrieval latency, we remove the initial forward pass of PCL, which otherwise greatly increases training and inference time, with a negligible trade-off in performance. Additionally, we introduce a novel topic-aware prompt pool that employs neural topic embeddings as fixed keys, eliminating the instability of prompt key optimization while maintaining competitive performance with existing PCL prompt pools. In a challenging rehearsal-free continual learning setup, we demonstrate that PromptDSI variants outperform rehearsal-based baselines, match the strong cache-based baseline in mitigating forgetting, and significantly improving retrieval performance on new corpora.",
    "citationCount": 0,
    "referenceCount": 47
}