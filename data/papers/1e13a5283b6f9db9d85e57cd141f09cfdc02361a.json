{
    "paperId": "1e13a5283b6f9db9d85e57cd141f09cfdc02361a",
    "title": "ApproxTuner: a compiler and runtime system for adaptive approximations",
    "year": 2021,
    "venue": "ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming",
    "authors": [
        "Hashim Sharif",
        "Yifan Zhao",
        "Maria Kotsifakou",
        "Akash Kothari",
        "Ben Schreiber",
        "Elizabeth Wang",
        "Yasmin Sarita",
        "Nathan Zhao",
        "Keyur Joshi",
        "Vikram S. Adve",
        "Sasa Misailovic",
        "S. Adve"
    ],
    "doi": "10.1145/3437801.3446108",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/1e13a5283b6f9db9d85e57cd141f09cfdc02361a",
    "isOpenAccess": true,
    "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3437801.3446108",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Manually optimizing the tradeoffs between accuracy, performance and energy for resource-intensive applications with flexible accuracy or precision requirements is extremely difficult. We present ApproxTuner, an automatic framework for accuracy-aware optimization of tensor-based applications while requiring only high-level end-to-end quality specifications. ApproxTuner implements and manages approximations in algorithms, system software, and hardware. The key contribution in ApproxTuner is a novel three-phase approach to approximation-tuning that consists of development-time, install-time, and run-time phases. Our approach decouples tuning of hardware-independent and hardware-specific approximations, thus providing retargetability across devices. To enable efficient autotuning of approximation choices, we present a novel accuracy-aware tuning technique called predictive approximation-tuning, which significantly speeds up autotuning by analytically predicting the accuracy impacts of approximations. We evaluate ApproxTuner across 10 convolutional neural networks (CNNs) and a combined CNN and image processing benchmark. For the evaluated CNNs, using only hardware-independent approximation choices we achieve a mean speedup of 2.1x (max 2.7x) on a GPU, and 1.3x mean speedup (max 1.9x) on the CPU, while staying within 1 percentage point of inference accuracy loss. For two different accuracy-prediction models, ApproxTuner speeds up tuning by 12.8x and 20.4x compared to conventional empirical tuning while achieving comparable benefits.",
    "citationCount": 26,
    "referenceCount": 68
}