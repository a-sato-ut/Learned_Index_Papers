{
    "paperId": "f8411c39a7cc1c3772fec3acd4fa76ac83c6a0b5",
    "title": "Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based Pairwise Ranking with Batching and Caching",
    "year": 2025,
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "authors": [
        "Juan Wisznia",
        "Cecilia Bolanos",
        "Juan Tollo",
        "Giovanni Marraffini",
        "Agust'in Gianolini",
        "Noe Hsueh",
        "Luciano Del Corro"
    ],
    "doi": "10.48550/arXiv.2505.24643",
    "arxivId": "2505.24643",
    "url": "https://www.semanticscholar.org/paper/f8411c39a7cc1c3772fec3acd4fa76ac83c6a0b5",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We introduce a novel framework for analyzing sorting algorithms in pairwise ranking prompting (PRP), re-centering the cost model around LLM inferences rather than traditional pairwise comparisons. While classical metrics based on comparison counts have traditionally been used to gauge efficiency, our analysis reveals that expensive LLM inferences overturn these predictions; accordingly, our framework encourages strategies such as batching and caching to mitigate inference costs. We show that algorithms optimal in the classical setting can lose efficiency when LLM inferences dominate the cost under certain optimizations.",
    "citationCount": 0,
    "referenceCount": 21
}