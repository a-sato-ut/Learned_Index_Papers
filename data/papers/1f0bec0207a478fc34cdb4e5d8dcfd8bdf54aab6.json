{
    "paperId": "1f0bec0207a478fc34cdb4e5d8dcfd8bdf54aab6",
    "title": "Ravnest: Decentralized Asynchronous Training on Heterogeneous Devices",
    "year": 2024,
    "venue": "arXiv.org",
    "authors": [
        "A. Menon",
        "Unnikrishnan Menon",
        "Kailash Ahirwar"
    ],
    "doi": "10.48550/arXiv.2401.01728",
    "arxivId": "2401.01728",
    "url": "https://www.semanticscholar.org/paper/1f0bec0207a478fc34cdb4e5d8dcfd8bdf54aab6",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Modern deep learning models, growing larger and more complex, have demonstrated exceptional generalization and accuracy due to training on huge datasets. This trend is expected to continue. However, the increasing size of these models poses challenges in training, as traditional centralized methods are limited by memory constraints at such scales. This paper proposes an asynchronous decentralized training paradigm for large modern deep learning models that harnesses the compute power of regular heterogeneous PCs with limited resources connected across the internet to achieve favourable performance metrics. Ravnest facilitates decentralized training by efficiently organizing compute nodes into clusters with similar data transfer rates and compute capabilities, without necessitating that each node hosts the entire model. These clusters engage in $\\textit{Zero-Bubble Asynchronous Model Parallel}$ training, and a $\\textit{Parallel Multi-Ring All-Reduce}$ method is employed to effectively execute global parameter averaging across all clusters. We have framed our asynchronous SGD loss function as a block structured optimization problem with delayed updates and derived an optimal convergence rate of $O\\left(\\frac{1}{\\sqrt{K}}\\right)$. We further discuss linear speedup with respect to the number of participating clusters and the bound on the staleness parameter.",
    "citationCount": 2,
    "referenceCount": 57
}