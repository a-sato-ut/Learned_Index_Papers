{
    "paperId": "1e8403af2e1e7a8f803d8df9e8daac584f99c2a0",
    "title": "ATT3D: Amortized Text-to-3D Object Synthesis",
    "year": 2023,
    "venue": "IEEE International Conference on Computer Vision",
    "authors": [
        "Jonathan Lorraine",
        "Kevin Xie",
        "Xiaohui Zeng",
        "Chen-Hsuan Lin",
        "Towaki Takikawa",
        "Nicholas Sharp",
        "Tsung-Yi Lin",
        "Ming-Yu Liu",
        "S. Fidler",
        "James Lucas"
    ],
    "doi": "10.1109/ICCV51070.2023.01645",
    "arxivId": "2306.07349",
    "url": "https://www.semanticscholar.org/paper/1e8403af2e1e7a8f803d8df9e8daac584f99c2a0",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Text-to-3D modelling has seen exciting progress by combining generative text-to-image models with image-to-3D methods like Neural Radiance Fields. DreamFusion recently achieved high-quality results but requires a lengthy, per-prompt optimization to create 3D objects. To address this, we amortize optimization over text prompts by training on many prompts simultaneously with a unified model, instead of separately. With this, we share computation across a prompt set, training in less time than per-prompt optimization. Our framework – Amortized Text-to-3D (ATT3D) – enables knowledge sharing between prompts to generalize to unseen setups and smooth interpolations between text for novel assets and simple animations.",
    "citationCount": 94,
    "referenceCount": 73
}