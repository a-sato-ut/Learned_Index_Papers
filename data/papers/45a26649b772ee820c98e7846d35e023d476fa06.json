{
    "paperId": "45a26649b772ee820c98e7846d35e023d476fa06",
    "title": "SoftNeuro: Fast Deep Inference using Multi-platform Optimization",
    "year": 2021,
    "venue": "arXiv.org",
    "authors": [
        "Masaki Hilaga",
        "Yasuhiro Kuroda",
        "Hitoshi Matsuo",
        "Tatsuya Kawaguchi",
        "Gabriel Ogawa",
        "Hiroshi Miyake",
        "Yusuke Kozawa"
    ],
    "doi": null,
    "arxivId": "2110.06037",
    "url": "https://www.semanticscholar.org/paper/45a26649b772ee820c98e7846d35e023d476fa06",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Faster inference of deep learning models is highly demanded on edge devices and even servers, for both financial and environmental reasons. To address this issue, we propose SoftNeuro, a novel, high-performance inference framework with efficient performance tuning. The key idea is to separate algorithmic routines from network layers. Our framework maximizes the inference performance by profiling various routines for each layer and selecting the fastest path. To efficiently find the best path, we propose a routine-selection algorithm based on dynamic programming. Experiments show that the proposed framework achieves both fast inference and efficient tuning.",
    "citationCount": 2,
    "referenceCount": 29
}