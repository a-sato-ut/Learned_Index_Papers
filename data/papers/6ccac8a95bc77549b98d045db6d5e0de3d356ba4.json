{
    "paperId": "6ccac8a95bc77549b98d045db6d5e0de3d356ba4",
    "title": "Exploring Classic and Neural Lexical Translation Models for Information Retrieval: Interpretability, Effectiveness, and Efficiency Benefits",
    "year": 2021,
    "venue": "European Conference on Information Retrieval",
    "authors": [
        "Leonid Boytsov",
        "Zico Kolter"
    ],
    "doi": "10.1007/978-3-030-72113-8_5",
    "arxivId": "2102.06815",
    "url": "https://www.semanticscholar.org/paper/6ccac8a95bc77549b98d045db6d5e0de3d356ba4",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We study the utility of the lexical translation model (IBM Model 1) for English text retrieval, in particular, its neural variants that are trained end-to-end. We use the neural Model1 as an aggregator layer applied to context-free or contextualized query/document embeddings. This new approach to design a neural ranking system has benefits for effectiveness, efficiency, and interpretability. Specifically, we show that adding an interpretable neural Model 1 layer on top of BERT-based contextualized embeddings (1) does not decrease accuracy and/or efficiency; and (2) may overcome the limitation on the maximum sequence length of existing BERT models. The context-free neural Model 1 is less effective than a BERT-based ranking model, but it can run efficiently on a CPU (without expensive index-time precomputation or query-time operations on large tensors). Using Model 1 we produced best neural and non-neural runs on the MS MARCO document ranking leaderboard in late 2020.",
    "citationCount": 11,
    "referenceCount": 74
}