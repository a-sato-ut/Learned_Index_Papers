{
    "paperId": "77a315c3889767a4374781e75ceeb3cd751f860a",
    "title": "Performance Characterization of HTAP Workloads",
    "year": 2021,
    "venue": "IEEE International Conference on Data Engineering",
    "authors": [
        "Utku Sirin",
        "S. Dwarkadas",
        "A. Ailamaki"
    ],
    "doi": "10.1109/ICDE51399.2021.00162",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/77a315c3889767a4374781e75ceeb3cd751f860a",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Hybrid Transactional and Analytical Processing (HTAP) systems have become popular in the past decade. HTAP systems allow running transactional and analytical processing workloads on the same data and hardware. As a result, they suffer from workload interference. Despite the large body of existing work in HTAP systems and architectures, none of the existing work has systematically analyzed workload interference for HTAP systems.In this work, we characterize workload interference for HTAP systems. We show that the OLTP throughput drops by up to 42% due to sharing the hardware resources. Partitioning the last-level cache (LLC) among the OLTP and OLAP workloads can significantly improve the OLTP throughput without hurting the OLAP throughput. The OLAP throughput is significantly reduced due to sharing the data. The OLAP execution time is exponentially increased if the OLTP workload generates fresh tuples faster than the HTAP system propagates them. Therefore, in order to minimize the workload interference, HTAP systems should isolate the OLTP and OLAP workloads in the shared hardware resources and should allocate enough resources to fresh tuple propagation to propagate the fresh tuples faster than they are generated.",
    "citationCount": 14,
    "referenceCount": 25
}