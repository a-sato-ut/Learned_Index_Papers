{
    "paperId": "05affb0770bcf875f664afc757a73ad6c315732c",
    "title": "Explainable Sentiment Analysis through Counterfactual Reasoning",
    "year": 2025,
    "venue": "User Modeling, Adaptation, and Personalization",
    "authors": [
        "Simone Prete",
        "Giovanni Maria Biancofiore",
        "F. Narducci",
        "Eugenio Di Sciascio",
        "Tommaso Di Noia"
    ],
    "doi": "10.1145/3708319.3733660",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/05affb0770bcf875f664afc757a73ad6c315732c",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Book",
        "JournalArticle",
        "Review"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Sentiment Analysis (SA) has proven to be an effective tool for recognizing opinions in text. However, the mechanisms by which these models arrive at specific predictions often remain unclear. This paper explores how eXplainable Artificial Intelligence (XAI) techniques can enhance interpretability in sentiment classification. Specifically, we leverage SHAP (SHapley Additive exPlanations) and counterfactual generation to identify words influencing sentiment predictions in movie reviews. Our approach integrates a neural classifier and generates counterfactual examples to reveal how slight text modifications affect model decisions. Experimental results show that SHAP-based attribution and counterfactual analysis provide deeper insights into the linguistic factors driving sentiment classification.",
    "citationCount": 0,
    "referenceCount": 28
}