{
    "paperId": "ae6c4a522c6f04306903525467353cf7c049d641",
    "title": "TPRF: A Transformer-based Pseudo-Relevance Feedback Model for Efficient and Effective Retrieval",
    "year": 2024,
    "venue": "arXiv.org",
    "authors": [
        "Chuting Yu",
        "Hang Li",
        "Ahmed Mourad",
        "B. Koopman",
        "G. Zuccon"
    ],
    "doi": "10.48550/arXiv.2401.13509",
    "arxivId": "2401.13509",
    "url": "https://www.semanticscholar.org/paper/ae6c4a522c6f04306903525467353cf7c049d641",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "This paper considers Pseudo-Relevance Feedback (PRF) methods for dense retrievers in a resource constrained environment such as that of cheap cloud instances or embedded systems (e.g., smartphones and smartwatches), where memory and CPU are limited and GPUs are not present. For this, we propose a transformer-based PRF method (TPRF), which has a much smaller memory footprint and faster inference time compared to other deep language models that employ PRF mechanisms, with a marginal effectiveness loss. TPRF learns how to effectively combine the relevance feedback signals from dense passage representations. Specifically, TPRF provides a mechanism for modelling relationships and weights between the query and the relevance feedback signals. The method is agnostic to the specific dense representation used and thus can be generally applied to any dense retriever.",
    "citationCount": 1,
    "referenceCount": 32
}