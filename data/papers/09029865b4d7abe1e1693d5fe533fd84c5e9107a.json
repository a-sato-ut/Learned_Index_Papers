{
    "paperId": "09029865b4d7abe1e1693d5fe533fd84c5e9107a",
    "title": "Tensors: An abstraction for general data processing",
    "year": 2021,
    "venue": "Proceedings of the VLDB Endowment",
    "authors": [
        "D. Koutsoukos",
        "Supun Nakandala",
        "Konstantinos Karanasos",
        "Karla Saur",
        "G. Alonso",
        "Matteo Interlandi"
    ],
    "doi": "10.14778/3467861.3467869",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/09029865b4d7abe1e1693d5fe533fd84c5e9107a",
    "isOpenAccess": true,
    "openAccessPdf": "https://www.research-collection.ethz.ch/bitstream/20.500.11850/513207/1/3467861.3467869.pdf",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Deep Learning (DL) has created a growing demand for simpler ways to develop complex models and efficient ways to execute them. Thus, a significant effort has gone into frameworks like PyTorch or TensorFlow to support a variety of DL models and run efficiently and seamlessly over heterogeneous and distributed hardware. Since these frameworks will continue improving given the predominance of DL workloads, it is natural to ask what else can be done with them. This is not a trivial question since these frameworks are based on the efficient implementation of tensors, which are well adapted to DL but, in principle, to nothing else. In this paper we explore to what extent Tensor Computation Runtimes (TCRs) can support non-ML data processing applications, so that other use cases can take advantage of the investments made on TCRs. In particular, we are interested in graph processing and relational operators, two use cases very different from ML, in high demand, and complement quite well what TCRs can do today. Build-ing on Hummingbird, a recent platform converting traditional machine learning algorithms to tensor computations, we explore how to map selected graph processing and relational operator algorithms into tensor computations. Our vision is supported by the results: our code often outperforms custom-built C++ and CUDA kernels, while massively reducing the development effort, taking advantage of the cross-platform compilation capabilities of TCRs.",
    "citationCount": 33,
    "referenceCount": 55
}