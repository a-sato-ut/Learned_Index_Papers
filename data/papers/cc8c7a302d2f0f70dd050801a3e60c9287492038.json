{
    "paperId": "cc8c7a302d2f0f70dd050801a3e60c9287492038",
    "title": "Pearl: Automatic Code Optimization Using Deep Reinforcement Learning",
    "year": 2025,
    "venue": "International Conference on Supercomputing",
    "authors": [
        "Djamel Rassem Lamouri",
        "Iheb Nassim Aouadj",
        "Smail Kourta",
        "Riyadh Baghdadi"
    ],
    "doi": "10.1145/3721145.3725766",
    "arxivId": "2506.01880",
    "url": "https://www.semanticscholar.org/paper/cc8c7a302d2f0f70dd050801a3e60c9287492038",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Compilers are crucial in optimizing programs and accelerating their execution, particularly for compute-intensive tasks such as training deep learning models and conducting physics simulations. However, optimizing programs automatically using compilers is not trivial. Recent work has attempted to use reinforcement learning (RL) to solve this problem. It has limitations though. Current methods either do not support the optimization of general loop nests or can only be used to optimize loop nests seen during training. In this paper, we propose Pearl, a novel framework that uses deep reinforcement learning to automate compiler code optimization. It uses an RL agent to select the sequence of code optimizations a compiler should apply to make the input code run faster. This agent can optimize general loop nests (i.e., it is not domain-specific) and can generalize to programs unseen during training. To enable the optimization of general loop nests, we propose a novel representation of the action space that allows the RL agent to select on which part of the loop nest a given code optimization should be applied. One of the main challenges that hinder the development of RL agents for optimizing general loop nests is the fact that this task is data-intensive, with each experiment taking weeks. To avoid this problem and enable fast training of the proposed RL agent, we propose two methods: 1) execution time and legality check memoization; and 2) actor-critic pre-training. We implement our approach in Tiramisu, a state-of-the-art polyhedral compiler designed for accelerating compute-intensive programs. Our approach streamlines the optimization process and offers performance improvements compared to existing methods. To the best of our knowledge, Pearl is the first RL-based system to support general programs composed of loop nests manipulating tensors while still being able to generalize to programs unseen during training. It is also the first to support the class of polyhedral optimizations, a class of advanced loop nest optimizations. We evaluate Pearl on a set of benchmarks, and demonstrate competitive performance improvements over state-of-the-art compilers. Notably, Pearl achieves a geometric mean speedup of \\(2.02\\times\\) compared to Tiramisu and \\(3.36\\times\\) compared to Pluto.",
    "citationCount": 0,
    "referenceCount": 59
}