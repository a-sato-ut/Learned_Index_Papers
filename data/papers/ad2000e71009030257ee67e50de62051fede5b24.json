{
    "paperId": "ad2000e71009030257ee67e50de62051fede5b24",
    "title": "Ratel: Optimizing Holistic Data Movement to Fine-tune 100B Model on a Consumer GPU",
    "year": 2025,
    "venue": "IEEE International Conference on Data Engineering",
    "authors": [
        "Changyue Liao",
        "Mo Sun",
        "Zihan Yang",
        "Jun Xie",
        "Kaiqi Chen",
        "Binhang Yuan",
        "Fei Wu",
        "Zeke Wang"
    ],
    "doi": "10.1109/ICDE65448.2025.00029",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/ad2000e71009030257ee67e50de62051fede5b24",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Nowadays, AI researchers become more and more interested in fine-tuning a pre-trained LLM, whose size has grown to up to over 100B parameters, for their downstream tasks. One approach to fine-tune such huge models is to aggregate device memory from many GPUs. However, this approach introduces prohibitive costs for most data scientists with a limited budget for high-end GPU servers. In this paper, we focus on LLM fine-tuning on a single consumer-grade GPU in a commodity server with limited main memory capacity, which is accessible to most AI researchers. In such a scenario, existing offloading-based methods fail to fine-tune an LLM efficiently due to a lack of holistic intra-server tensor movement management. To this end, we present Ratel, a low-cost, high-performance deep learning training framework that enables efficient 100B-scale model fine-tuning on a commodity server with a consumergrade GPU and limited main memory capacity. The key idea is to add holistic offloading traffic as an optimization dimension for 1) active gradient offloading, and 2) holistic traffic-aware activation swapping mechanism. The experimental results show that 1) Ratel is the first to fine-tune a 175B model on an RTX 4090 and 256 GB main memory, 2) Ratel achieves 2.32x throughput than the state-of-the-art baselines when fine-tuning a small 13B model, and 3) Ratel enables a cheap low-end consumer GPU to have higher cost-effectiveness than a DGX-A100 cluster when fine-tuning a 175B model.",
    "citationCount": 0,
    "referenceCount": 115
}