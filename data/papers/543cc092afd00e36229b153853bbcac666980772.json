{
    "paperId": "543cc092afd00e36229b153853bbcac666980772",
    "title": "Act-With-Think: Chunk Auto-Regressive Modeling for Generative Recommendation",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Yifan Wang",
        "Weinan Gan",
        "Longtao Xiao",
        "Jieming Zhu",
        "Heng Chang",
        "Haozhao Wang",
        "Rui Zhang",
        "Zhenhua Dong",
        "Ruiming Tang",
        "Ruixuan Li"
    ],
    "doi": "10.48550/arXiv.2506.23643",
    "arxivId": "2506.23643",
    "url": "https://www.semanticscholar.org/paper/543cc092afd00e36229b153853bbcac666980772",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Generative recommendation (GR) typically encodes behavioral or semantic aspects of item information into discrete tokens, leveraging the standard autoregressive (AR) generation paradigm to make predictions. However, existing methods tend to overlook their intrinsic relationship, that is, the semantic usually provides some reasonable explainability\"$\\textbf{why}$\"for the behavior\"$\\textbf{what}$\", which may constrain the full potential of GR. To this end, we present Chunk AutoRegressive Modeling (CAR), a new generation paradigm following the decision pattern that users usually think semantic aspects of items (e.g. brand) and then take actions on target items (e.g. purchase). Our CAR, for the $\\textit{first time}$, incorporates semantics (SIDs) and behavior (UID) into a single autoregressive transformer from an ``act-with-think''dual perspective via chunk-level autoregression. Specifically, CAR packs SIDs and UID into a conceptual chunk for item unified representation, allowing each decoding step to make a holistic prediction. Experiments show that our CAR significantly outperforms existing methods based on traditional AR, improving Recall@5 by 7.93% to 22.30%. Furthermore, we verify the scaling effect between model performance and SIDs bit number, demonstrating that CAR preliminary emulates a kind of slow-thinking style mechanism akin to the reasoning processes observed in large language models (LLMs).",
    "citationCount": 1,
    "referenceCount": 40
}