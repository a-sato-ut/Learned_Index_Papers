{
    "paperId": "25893f487cba3739a7e2a962c2646f850a702e7a",
    "title": "Personalization Toolkit: Training Free Personalization of Large Vision Language Models",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Soroush Seifi",
        "Vaggelis Dorovatas",
        "Daniel Olmeda Reino",
        "Rahaf Aljundi"
    ],
    "doi": "10.48550/arXiv.2502.02452",
    "arxivId": "2502.02452",
    "url": "https://www.semanticscholar.org/paper/25893f487cba3739a7e2a962c2646f850a702e7a",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Personalization of Large Vision-Language Models (LVLMs) involves customizing models to recognize specific users and object instances, and to generate contextually tailored responses. Existing approaches typically rely on time-consuming test-time training for each user or object, making them impractical for real-world deployment, a limitation reflected in current personalization benchmarks, which are focused on object-centric, single-concept evaluations. In this paper, we present a novel training-free approach to LVLM personalization and introduce a comprehensive real-world benchmark designed to rigorously evaluate various aspects of the personalization task. Our method leverages pre-trained vision foundation models to extract distinctive features, applies retrieval-augmented generation (RAG) techniques to identify instances within visual inputs, and employs visual prompting strategies to guide model outputs. Our model-agnostic vision toolkit enables efficient and flexible multi-concept personalization across both images and videos, without any additional training. We achieve state-of-the-art results, surpassing existing training-based methods.",
    "citationCount": 0,
    "referenceCount": 41
}