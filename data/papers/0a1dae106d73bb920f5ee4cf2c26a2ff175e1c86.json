{
    "paperId": "0a1dae106d73bb920f5ee4cf2c26a2ff175e1c86",
    "title": "Generalizing Discriminative Retrieval Models using Generative Tasks",
    "year": 2021,
    "venue": "The Web Conference",
    "authors": [
        "Binsheng Liu",
        "Hamed Zamani",
        "Xiaolu Lu",
        "J. Culpepper"
    ],
    "doi": "10.1145/3442381.3449863",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/0a1dae106d73bb920f5ee4cf2c26a2ff175e1c86",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Information Retrieval has a long history of applying either discriminative or generative modeling to retrieval and ranking tasks. Recent developments in transformer architectures and multi-task learning techniques have dramatically improved our ability to train effective neural models capable of resolving a wide variety of tasks using either of these paradigms. In this paper, we propose a novel multi-task learning approach which can be used to produce more effective neural ranking models. The key idea is to improve the quality of the underlying transformer model by cross-training a retrieval task and one or more complementary language generation tasks. By targeting the training on the encoding layer in the transformer architecture, our experimental results show that the proposed multi-task learning approach consistently improves retrieval effectiveness on the targeted collection and can easily be re-targeted to new ranking tasks. We provide an in-depth analysis showing how multi-task learning modifies model behaviors, resulting in more general models.",
    "citationCount": 9,
    "referenceCount": 45
}