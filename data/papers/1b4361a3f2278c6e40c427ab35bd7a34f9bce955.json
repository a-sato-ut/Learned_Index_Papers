{
    "paperId": "1b4361a3f2278c6e40c427ab35bd7a34f9bce955",
    "title": "Low Rank Learning for Offline Query Optimization",
    "year": 2025,
    "venue": "Proc. ACM Manag. Data",
    "authors": [
        "Zixuan Yi",
        "Yao Tian",
        "Z. Ives",
        "Ryan Marcus"
    ],
    "doi": "10.1145/3725412",
    "arxivId": "2504.06399",
    "url": "https://www.semanticscholar.org/paper/1b4361a3f2278c6e40c427ab35bd7a34f9bce955",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Recent deployments of learned query optimizers use expensive neural networks and ad-hoc search policies. To address these issues, we introduce LimeQO, a framework for offline query optimization leveraging low-rank learning to efficiently explore alternative query plans with minimal resource usage. By modeling the workload as a partially observed, low-rank matrix, we predict unobserved query plan latencies using purely linear methods, significantly reducing computational overhead compared to neural networks. We formalize offline exploration as an active learning problem, and present simple heuristics that reduces a 3-hour workload to 1.5 hours after just 1.5 hours of exploration. Additionally, we propose a transductive Tree Convolutional Neural Network (TCNN) that, despite higher computational costs, achieves the same workload reduction with only 0.5 hours of exploration. Unlike previous approaches that place expensive neural networks directly in the query processing ''hot'' path, our approach offers a low-overhead solution and a no-regressions guarantee, all without making assumptions about the underlying DBMS.",
    "citationCount": 3,
    "referenceCount": 91
}