{
    "paperId": "9f9ddc24370d84bbe6640955279964ae8f8fdcea",
    "title": "Deep Non-Crossing Quantiles through the Partial Derivative",
    "year": 2022,
    "venue": "International Conference on Artificial Intelligence and Statistics",
    "authors": [
        "Axel Brando",
        "Joan Gimeno",
        "Jose A. Rodr'iguez-Serrano",
        "Jordi Vitri√†"
    ],
    "doi": null,
    "arxivId": "2201.12848",
    "url": "https://www.semanticscholar.org/paper/9f9ddc24370d84bbe6640955279964ae8f8fdcea",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Quantile Regression (QR) provides a way to approximate a single conditional quantile. To have a more informative description of the conditional distribution, QR can be merged with deep learning techniques to simultaneously estimate multiple quantiles. However, the minimisation of the QR-loss function does not guarantee non-crossing quantiles, which affects the validity of such predictions and introduces a critical issue in certain scenarios. In this article, we propose a generic deep learning algorithm for predicting an arbitrary number of quantiles that ensures the quantile monotonicity constraint up to the machine precision and maintains its modelling performance with respect to alternative models. The presented method is evaluated over several real-world datasets obtaining state-of-the-art results as well as showing that it scales to large-size data sets.",
    "citationCount": 14,
    "referenceCount": 42
}