{
    "paperId": "53242282ab2dffe8004fcb21f98e4c241c755a25",
    "title": "Mahalanobis++: Improving OOD Detection via Feature Normalization",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Maximilian Mueller",
        "Matthias Hein"
    ],
    "doi": "10.48550/arXiv.2505.18032",
    "arxivId": "2505.18032",
    "url": "https://www.semanticscholar.org/paper/53242282ab2dffe8004fcb21f98e4c241c755a25",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Detecting out-of-distribution (OOD) examples is an important task for deploying reliable machine learning models in safety-critial applications. While post-hoc methods based on the Mahalanobis distance applied to pre-logit features are among the most effective for ImageNet-scale OOD detection, their performance varies significantly across models. We connect this inconsistency to strong variations in feature norms, indicating severe violations of the Gaussian assumption underlying the Mahalanobis distance estimation. We show that simple $\\ell_2$-normalization of the features mitigates this problem effectively, aligning better with the premise of normally distributed data with shared covariance matrix. Extensive experiments on 44 models across diverse architectures and pretraining schemes show that $\\ell_2$-normalization improves the conventional Mahalanobis distance-based approaches significantly and consistently, and outperforms other recently proposed OOD detection methods.",
    "citationCount": 5,
    "referenceCount": 52
}