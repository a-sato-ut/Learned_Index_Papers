{
    "paperId": "1715ffe2135fbf1ec8dbcb4a21dfb930253f24ca",
    "title": "Towards Better Interpretability in Deep Q-Networks",
    "year": 2018,
    "venue": "AAAI Conference on Artificial Intelligence",
    "authors": [
        "Raghuram Mandyam Annasamy",
        "K. Sycara"
    ],
    "doi": "10.1609/aaai.v33i01.33014561",
    "arxivId": "1809.05630",
    "url": "https://www.semanticscholar.org/paper/1715ffe2135fbf1ec8dbcb4a21dfb930253f24ca",
    "isOpenAccess": true,
    "openAccessPdf": "https://ojs.aaai.org/index.php/AAAI/article/download/4377/4255",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Deep reinforcement learning techniques have demonstrated superior performance in a wide variety of environments. As improvements in training algorithms continue at a brisk pace, theoretical or empirical studies on understanding what these networks seem to learn, are far behind. In this paper we propose an interpretable neural network architecture for Q-learning which provides a global explanation of the modelâ€™s behavior using key-value memories, attention and reconstructible embeddings. With a directed exploration strategy, our model can reach training rewards comparable to the state-of-the-art deep Q-learning models. However, results suggest that the features extracted by the neural network are extremely shallow and subsequent testing using out-of-sample examples shows that the agent can easily overfit to trajectories seen during training.",
    "citationCount": 61,
    "referenceCount": 36
}