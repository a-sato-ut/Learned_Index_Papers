{
    "paperId": "beca17564ef9a03d42ce9db4e303689fba3ffcc1",
    "title": "Towards Autonomous Testing Agents via Conversational Large Language Models",
    "year": 2023,
    "venue": "International Conference on Automated Software Engineering",
    "authors": [
        "R. Feldt",
        "Sungmin Kang",
        "Juyeon Yoon",
        "Shin Yoo"
    ],
    "doi": "10.1109/ASE56229.2023.00148",
    "arxivId": "2306.05152",
    "url": "https://www.semanticscholar.org/paper/beca17564ef9a03d42ce9db4e303689fba3ffcc1",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2306.05152",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Software testing is an important part of the development cycle, yet it requires specialized expertise and substantial developer effort to adequately test software. Recent discoveries of the capabilities of large language models (LLMs) suggest that they can be used as automated testing assistants, and thus provide helpful information and even drive the testing process. To highlight the potential of this technology, we present a taxonomy of LLM-based testing agents based on their level of autonomy, and describe how a greater level of autonomy can benefit developers in practice. An example use of LLMs as a testing assistant is provided to demonstrate how a conversational framework for testing can help developers. This also highlights how the often criticized “hallucination” of LLMs can be beneficial for testing. We identify other tangible benefits that LLM-driven testing agents can bestow, and also discuss potential limitations.",
    "citationCount": 29,
    "referenceCount": 37
}