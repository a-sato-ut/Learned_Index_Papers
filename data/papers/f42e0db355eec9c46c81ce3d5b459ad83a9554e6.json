{
    "paperId": "f42e0db355eec9c46c81ce3d5b459ad83a9554e6",
    "title": "Unshackling Database Benchmarking from Synthetic Workloads",
    "year": 2023,
    "venue": "IEEE International Conference on Data Engineering",
    "authors": [
        "Parimarjan Negi",
        "Laurent Bindschaedler",
        "Mohammad Alizadeh",
        "Tim Kraska",
        "Jyoti Leeka",
        "Anja Gruenheid",
        "Matteo Interlandi"
    ],
    "doi": "10.1109/ICDE55515.2023.00292",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/f42e0db355eec9c46c81ce3d5b459ad83a9554e6",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Introducing new (learned) features into a DBMS requires considerable experimentation and benchmarking to avoid regressions in production (customer) workloads. Using standard benchmarks such as TPC-H and TCH-DS is common practice, but, unfortunately, these do not represent the complexity of real production workloads. To solve this problem, in this demo, we propose a technique that generates a synthetic dataset from query logs and metadataâ€”without touching the original data. The keystone of our approach is to map the data generation as a SAT problem where constraints, such as runtime cardinalities, are extracted from query logs and metadata. We show that our approach can generate representative benchmarks mirroring the performance of the original data without trading off privacy. The demo will guide the attendees through the various steps involved in the data generation and testing process.",
    "citationCount": 3,
    "referenceCount": 15
}