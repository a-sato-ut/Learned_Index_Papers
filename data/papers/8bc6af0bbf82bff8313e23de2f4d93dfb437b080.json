{
    "paperId": "8bc6af0bbf82bff8313e23de2f4d93dfb437b080",
    "title": "ATFormer: A Learned Performance Model with Transfer Learning Across Devices for Deep Learning Tensor Programs",
    "year": 2023,
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "authors": [
        "Yang Bai",
        "Wenqian Zhao",
        "Shuo Yin",
        "Zixiao Wang",
        "Bei Yu"
    ],
    "doi": "10.18653/v1/2023.emnlp-main.250",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/8bc6af0bbf82bff8313e23de2f4d93dfb437b080",
    "isOpenAccess": true,
    "openAccessPdf": "https://aclanthology.org/2023.emnlp-main.250.pdf",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The training and inference efﬁciency of ever-larger deep neural networks highly rely on the performance of tensor operators on speciﬁc hardware platforms. Therefore, a compilation-based optimization ﬂow with automatic tensor generation and parameter tuning is necessary for efﬁcient model deployment. While compilation-based methods with performance models can provide dynamic and suitable code optimization, they suffer from a large design space exploration with rough measurement accuracy and poor transferability among different hardware platforms. This paper presents ATFormer, a simple yet efﬁcient design with attention-inspired modules to accurately predict the performance of optimized operators by capturing global and long-range dependencies within a complete scheduling space. Compared with state-of-the-arts, ATFormer can predict the optimal implementation of tensor operators to reduce inference time with minimal effort on modern DNN benchmarks. Furthermore, AT-Former with pre-trained parameters can quickly adapt to different workloads and hardware via transfer learning.",
    "citationCount": 2,
    "referenceCount": 38
}