{
    "paperId": "d70f435a7215497406dea71e30b6c5b203623667",
    "title": "Rethinking Portrait Matting with Privacy Preserving",
    "year": 2022,
    "venue": "International Journal of Computer Vision",
    "authors": [
        "Sihan Ma",
        "Jizhizi Li",
        "Jing Zhang",
        "He-jun Zhang",
        "Dacheng Tao"
    ],
    "doi": "10.1007/s11263-023-01797-8",
    "arxivId": "2203.16828",
    "url": "https://www.semanticscholar.org/paper/d70f435a7215497406dea71e30b6c5b203623667",
    "isOpenAccess": true,
    "openAccessPdf": "https://link.springer.com/content/pdf/10.1007/s11263-023-01797-8.pdf",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Medicine",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Art",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Recently, there has been an increasing concern about the privacy issue raised by identifiable information in machine learning. However, previous portrait matting methods were all based on identifiable images. To fill the gap, we present P3M-10k, which is the first large-scale anonymized benchmark for Privacy-Preserving Portrait Matting (P3M). P3M-10k consists of 10,421 high resolution face-blurred portrait images along with high-quality alpha mattes, which enables us to systematically evaluate both trimap-free and trimap-based matting methods and obtain some useful findings about model generalization ability under the privacy preserving training (PPT) setting. We also present a unified matting model dubbed P3M-Net that is compatible with both CNN and transformer backbones. To further mitigate the cross-domain performance gap issue under the PPT setting, we devise a simple yet effective Copy and Paste strategy (P3M-CP), which borrows facial information from public celebrity images and directs the network to reacquire the face context at both data and feature level. Extensive experiments on P3M-10k and public benchmarks demonstrate the superiority of P3M-Net over state-of-the-art methods and the effectiveness of P3M-CP in improving the cross-domain generalization ability, implying a great significance of P3M for future research and real-world applications. The dataset, code and models are available here ( https://github.com/ViTAE-Transformer/P3M-Net ).",
    "citationCount": 29,
    "referenceCount": 65
}