{
    "paperId": "2693b67b62b92ceb751d3ec98bc7b70a250262ad",
    "title": "Reducing Data Bottlenecks in Distributed, Heterogeneous Neural Networks",
    "year": 2024,
    "venue": "International Symposium on Embedded Multicore/Many-core Systems-on-Chip",
    "authors": [
        "Ruhai Lin",
        "Rui-Jie Zhu",
        "Jason K. Eshraghian"
    ],
    "doi": "10.1109/MCSoC64144.2024.00088",
    "arxivId": "2410.09650",
    "url": "https://www.semanticscholar.org/paper/2693b67b62b92ceb751d3ec98bc7b70a250262ad",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2410.09650",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The rapid advancement of embedded multicore and many-core systems has revolutionized computing, enabling the development of high-performance, energy-efficient solutions for a wide range of applications. As models scale up in size, data movement is increasingly the bottleneck to performance. This movement of data can exist between processor and memory, or between cores and chips. This paper investigates the impact of bottleneck size, in terms of inter-chip data traffic, on the performance of deep learning models in embedded multicore and many-core systems. We conduct a systematic analysis of the relationship between bottleneck size, computational resource utilization, and model accuracy. We apply a hardware-software co-design methodology where data bottlenecks are replaced with extremely narrow layers to reduce the amount of data traffic. In effect, time-multiplexing of signals is replaced by learnable embeddings that reduce the demands on chip IOs. Our experiments on the CIFAR100 dataset demonstrate that the classification accuracy generally decreases as the bottleneck ratio increases, with shallower models experiencing a more significant drop compared to deeper models. Hardware-side evaluation reveals that higher bottleneck ratios lead to substantial reductions in data transfer volume across the layers of the neural network. Through this research, we can determine the trade-off between data transfer volume and model performance, enabling the identification of a balanced point that achieves good performance while minimizing data transfer volume. This characteristic allows for the development of efficient models that are well-suited for resource-constrained environments.",
    "citationCount": 1,
    "referenceCount": 41
}