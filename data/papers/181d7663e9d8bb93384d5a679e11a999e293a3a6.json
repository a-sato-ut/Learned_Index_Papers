{
    "paperId": "181d7663e9d8bb93384d5a679e11a999e293a3a6",
    "title": "Hardware Accelerators for Deep Learning Applications",
    "year": 2025,
    "venue": "2025 IEEE International Students' Conference on Electrical, Electronics and Computer Science (SCEECS)",
    "authors": [
        "Gaurav Tiwari",
        "Sangeeta Nakhate",
        "Alok Pathak",
        "Abhinandan Jain",
        "Shardul Penurkar"
    ],
    "doi": "10.1109/SCEECS64059.2025.10940371",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/181d7663e9d8bb93384d5a679e11a999e293a3a6",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Conference",
        "Review"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Deep neural networks (DNNs) are essential for AI applications like computer vision, medical diagnosis, and autonomous vehicles, due to their exceptional performance. However, as DNN models grow increasingly complex, they demand substantial computational power that general-purpose processors struggle to provide. To meet these needs, domain-specific hardware accelerators, including GPU-based, TPU-based, in-memory, FPGA, and ASIC-based accelerators, have been developed. GPU-based accelerators, such as those from NVIDIA, excel in parallel processing, making them ideal for large-scale computations. TPU-based accelerators, optimized by Google for machine learning, offer impressive speed and efficiency. In-memory accelerators overcome memory bottlenecks by storing data close to computation units, enhancing data access speed. FPGA-based accelerators can be highly customized for specific tasks, while ASIC-based accelerators, tailored for specific applications, deliver the highest performance and energy efficiency, albeit at a higher development cost. This paper reviews the necessity of these accelerators, their types, and their potential impact on the future of AI technology.",
    "citationCount": 1,
    "referenceCount": 34
}