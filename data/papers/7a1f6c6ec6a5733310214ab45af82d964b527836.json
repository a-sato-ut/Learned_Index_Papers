{
    "paperId": "7a1f6c6ec6a5733310214ab45af82d964b527836",
    "title": "ListConRanker: A Contrastive Text Reranker with Listwise Encoding",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Junlong Liu",
        "Yue Ma",
        "Ruihui Zhao",
        "Junhao Zheng",
        "Qianli Ma",
        "Yangyang Kang"
    ],
    "doi": "10.48550/arXiv.2501.07111",
    "arxivId": "2501.07111",
    "url": "https://www.semanticscholar.org/paper/7a1f6c6ec6a5733310214ab45af82d964b527836",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Reranker models aim to re-rank the passages based on the semantics similarity between the given query and passages, which have recently received more attention due to the wide application of the Retrieval-Augmented Generation. Most previous methods apply pointwise encoding, meaning that it can only encode the context of the query for each passage input into the model. However, for the reranker model, given a query, the comparison results between passages are even more important, which is called listwise encoding. Besides, previous models are trained using the cross-entropy loss function, which leads to issues of unsmooth gradient changes during training and low training efficiency. To address these issues, we propose a novel Listwise-encoded Contrastive text reRanker (ListConRanker). It can help the passage to be compared with other passages during the encoding process, and enhance the contrastive information between positive examples and between positive and negative examples. At the same time, we use the circle loss to train the model to increase the flexibility of gradients and solve the problem of training efficiency. Experimental results show that ListConRanker achieves state-of-the-art performance on the reranking benchmark of Chinese Massive Text Embedding Benchmark, including the cMedQA1.0, cMedQA2.0, MMarcoReranking, and T2Reranking datasets.",
    "citationCount": 2,
    "referenceCount": 35
}