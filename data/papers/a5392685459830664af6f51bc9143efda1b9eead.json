{
    "paperId": "a5392685459830664af6f51bc9143efda1b9eead",
    "title": "Object-Aware Video Matting with Cross-Frame Guidance",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Huayu Zhang",
        "Dongyue Wu",
        "Yuanjie Shao",
        "Nong Sang",
        "Changxin Gao"
    ],
    "doi": "10.48550/arXiv.2503.01262",
    "arxivId": "2503.01262",
    "url": "https://www.semanticscholar.org/paper/a5392685459830664af6f51bc9143efda1b9eead",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Recently, trimap-free methods have drawn increasing attention in human video matting due to their promising performance. Nevertheless, these methods still suffer from the lack of deterministic foreground-background cues, which impairs their ability to consistently identify and locate foreground targets over time and mine fine-grained details. In this paper, we present a trimap-free Object-Aware Video Matting (OAVM) framework, which can perceive different objects, enabling joint recognition of foreground objects and refinement of edge details. Specifically, we propose an Object-Guided Correction and Refinement (OGCR) module, which employs cross-frame guidance to aggregate object-level instance information into pixel-level detail features, thereby promoting their synergy. Furthermore, we design a Sequential Foreground Merging augmentation strategy to diversify sequential scenarios and enhance capacity of the network for object discrimination. Extensive experiments on recent widely used synthetic and real-world benchmarks demonstrate the state-of-the-art performance of our OAVM with only an initial coarse mask. The code and model will be available.",
    "citationCount": 0,
    "referenceCount": 47
}