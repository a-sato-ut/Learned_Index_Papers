{
    "paperId": "95d251c3acd6cf8203425dcb94fd5a9fd0ff0b67",
    "title": "Optimize Cardinality Estimation Model Pretraining by Simplifying the Training Datasets",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Boyang Fang"
    ],
    "doi": "10.48550/arXiv.2502.14350",
    "arxivId": "2502.14350",
    "url": "https://www.semanticscholar.org/paper/95d251c3acd6cf8203425dcb94fd5a9fd0ff0b67",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The cardinality estimation is a key aspect of query optimization research, and its performance has significantly improved with the integration of machine learning. To overcome the\"cold start\"problem or the lack of model transferability in learned cardinality estimators, some pre-training cardinality estimation models have been proposed that use learning across multiple datasets and corresponding workloads. These models typically train on a dataset created by uniformly sampling from many datasets, but this approach may not be optimal. By applying the Group Distributionally Robust Optimization (Group DRO) algorithm to training datasets, we find that some specific training datasets contribute more significantly to model performance than others. Based on this observation, we conduct extensive experiments to delve deeper into pre-training cardinality estimators. Our results show how the performance of these models can be influenced by the datasets and corresponding workloads. Finally, we introduce a simplified training dataset, which has been reduced to a fraction of the size of existing pretraining datasets. Sufficient experimental results demonstrate that the pre-trained cardinality estimator based on this simplified dataset can still achieve comparable performance to existing models in zero-shot setups.",
    "citationCount": 0,
    "referenceCount": 21
}