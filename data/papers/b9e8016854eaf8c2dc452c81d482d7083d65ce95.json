{
    "paperId": "b9e8016854eaf8c2dc452c81d482d7083d65ce95",
    "title": "Shahin: Faster Algorithms for Generating Explanations for Multiple Predictions",
    "year": 2021,
    "venue": "SIGMOD Conference",
    "authors": [
        "Sona Hasani",
        "Saravanan Thirumuruganathan",
        "Nick Koudas",
        "Gautam Das"
    ],
    "doi": "10.1145/3448016.3457332",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/b9e8016854eaf8c2dc452c81d482d7083d65ce95",
    "isOpenAccess": true,
    "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3448016.3457332",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Machine learning (ML) models have achieved widespread adoption in the last few years. Generating concise and accurate explanations often increases user trust and understanding of the model prediction. Usually, the implementations of popular explanation algorithms are highly optimized for a single prediction. In practice, explanations often have to be generated in a batch for multiple predictions at a time. To the best of our knowledge, there has been no work for efficiently generating explanations for more than one prediction. While one could use multiple machines to generate explanations in parallel, this approach is sub-optimal as it does not leverage higher-level optimizations that are available in a batch setting. We propose a principled and lightweight approach for identifying redundant computations and several effective heuristics for dramatically speeding up explanation generation. Our techniques are general and could be applied to a wide variety of perturbation based explanation algorithms. We demonstrate this over a diverse set of algorithms including, LIME, Anchor, and SHAP. Our empirical experiments show that our methods impose very little overhead and require minimal modification to the explanation algorithms. They achieve significant speedup over baseline approaches that generate explanations in a sequential manner.",
    "citationCount": 1,
    "referenceCount": 39
}