{
    "paperId": "debe88eb17215e83e0de34638c1f32ab531f6ae1",
    "title": "Mining Robust Default Configurations for Resource-constrained AutoML",
    "year": 2022,
    "venue": "arXiv.org",
    "authors": [
        "Moe Kayali",
        "Chi Wang"
    ],
    "doi": null,
    "arxivId": "2202.09927",
    "url": "https://www.semanticscholar.org/paper/debe88eb17215e83e0de34638c1f32ab531f6ae1",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Automatic machine learning (AutoML) is a key enabler of the mass deployment of the next generation of machine learning systems. A key desideratum for future ML systems is the automatic selection of models and hyperparameters. We present a novel method of selecting performant configurations for a given task by performing offline autoML and mining over a diverse set of tasks. By mining the training tasks, we can select a compact portfolio of configurations that perform well over a wide variety of tasks, as well as learn a strategy to select portfolio configurations for yet-unseen tasks. The algorithm runs in a zero-shot manner, that is without training any models online except the chosen one. In a compute- or time-constrained setting, this virtually instant selection is highly performant. Further, we show that our approach is effective for warm-starting existing autoML platforms. In both settings, we demonstrate an improvement on the state-of-the-art by testing over 62 classification and regression datasets. We also demonstrate the utility of recommending data-dependent default configurations that outperform widely used hand-crafted defaults.",
    "citationCount": 3,
    "referenceCount": 26
}