{
    "paperId": "ba5cff0a6fc057e745f1bec4e4c6b80f7c1f05d5",
    "title": "Training Deep Energy-Based Models Through Cyclical Stochastic Gradient Langevin Dynamics",
    "year": 2022,
    "venue": "2022 5th International Seminar on Research of Information Technology and Intelligent Systems (ISRITI)",
    "authors": [
        "Karimul Makhtidi",
        "A. Bustamam",
        "Risman Adnan"
    ],
    "doi": "10.1109/ISRITI56927.2022.10053014",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/ba5cff0a6fc057e745f1bec4e4c6b80f7c1f05d5",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": null,
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Non-normalized log probability is assigned to data samples by energy-based models (EBMs). Applications for this functionality include image synthesis, data denoising, sample restoration, outlier identification, and Bayesian inference. Long mixing times for high-dimensional datasets are a common problem for MCMC-based sampling techniques. Some methods for scaling MCMC-based EBM training with continuous neural networks have been proposed to obtain better sampling than previous likelihood models and approaches similar to modern GAN approaches without experiencing mode collapse. This study suggests a novel approach called Cyclical Stochastic Gradient Langevin Dynamics for enhancing MCMC sampling of EBMs due to the cost of mixing times. On the MNIST dataset, we discovered that Cyclical SGLD is faster than Stochastic Gradient Langevin Dynamics (SGLD) and even more effective and reliable at producing realistic images.",
    "citationCount": 0,
    "referenceCount": 40
}