{
    "paperId": "2b34ee1b7f4cf5e7dac38d8e66849a02d96fade3",
    "title": "Graph-based Neural Acceleration for Nonnegative Matrix Factorization",
    "year": 2022,
    "venue": "arXiv.org",
    "authors": [
        "Jens Sjölund",
        "Maria Bånkestad"
    ],
    "doi": null,
    "arxivId": "2202.00264",
    "url": "https://www.semanticscholar.org/paper/2b34ee1b7f4cf5e7dac38d8e66849a02d96fade3",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We describe a graph-based neural acceleration technique for nonnegative matrix factorization that builds upon a connection between matrices and bipartite graphs that is well-known in certain fields, e.g., sparse linear algebra, but has not yet been exploited to design graph neural networks for matrix computations. We first consider low-rank factorization more broadly and propose a graph representation of the problem suited for graph neural networks. Then, we focus on the task of nonnegative matrix factorization and propose a graph neural network that interleaves bipartite self-attention layers with updates based on the alternating direction method of multipliers. Our empirical evaluation on synthetic and two real-world datasets shows that we attain substantial acceleration, even though we only train in an unsupervised fashion on smaller synthetic instances.",
    "citationCount": 12,
    "referenceCount": 65
}