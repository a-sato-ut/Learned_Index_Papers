{
    "paperId": "279f693586830ed2199b3c916e4c3261ae2fa8e3",
    "title": "MatteFormer: Transformer-Based Image Matting via Prior-Tokens",
    "year": 2022,
    "venue": "Computer Vision and Pattern Recognition",
    "authors": [
        "Gyutae Park",
        "S. Son",
        "Jaeyoung Yoo",
        "Seho Kim",
        "Nojun Kwak"
    ],
    "doi": "10.1109/CVPR52688.2022.01140",
    "arxivId": "2203.15662",
    "url": "https://www.semanticscholar.org/paper/279f693586830ed2199b3c916e4c3261ae2fa8e3",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2203.15662",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "In this paper, we propose a transformer-based image matting model called MatteFormer, which takes full advantage of trimap information in the transformer block. Our method first introduces a prior-token which is a global representation of each trimap region (e.g. foreground, background and unknown). These prior-tokens are used as global priors and participate in the self-attention mechanism of each block. Each stage of the encoder is composed of PAST (Prior-Attentive Swin Transformer) block, which is based on the Swin Transformer block, but differs in a couple of aspects: 1) It has PA-WSA (Prior-Attentive Window Self-Attention) layer, performing self-attention not only with spatial-tokens but also with prior-tokens. 2) It has prior-memory which saves prior-tokens accumulatively from the previous blocks and transfers them to the next block. We evaluate our MatteFormer on the commonly used image matting datasets: Composition-Ik and Distinctions-646. Experiment results show that our proposed method achieves state-of-the-art performance with a large margin. Our codes are available at https://github.com/webtoon/matteformer.",
    "citationCount": 73,
    "referenceCount": 59
}