{
    "paperId": "6b39bea0ae1720dcbc1ed19ffa697114c4d356c4",
    "title": "Scalable Deep Learning on Distributed Infrastructures",
    "year": 2019,
    "venue": "ACM Computing Surveys",
    "authors": [
        "R. Mayer",
        "H. Jacobsen"
    ],
    "doi": "10.1145/3363554",
    "arxivId": "1903.11314",
    "url": "https://www.semanticscholar.org/paper/6b39bea0ae1720dcbc1ed19ffa697114c4d356c4",
    "isOpenAccess": true,
    "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3363554",
    "publicationTypes": [
        "JournalArticle",
        "Review"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Deep Learning (DL) has had an immense success in the recent past, leading to state-of-the-art results in various domains, such as image recognition and natural language processing. One of the reasons for this success is the increasing size of DL models and the proliferation of vast amounts of training data being available. To keep on improving the performance of DL, increasing the scalability of DL systems is necessary. In this survey, we perform a broad and thorough investigation on challenges, techniques and tools for scalable DL on distributed infrastructures. This incorporates infrastructures for DL, methods for parallel DL training, multi-tenant resource scheduling, and the management of training and model data. Further, we analyze and compare 11 current open-source DL frameworks and tools and investigate which of the techniques are commonly implemented in practice. Finally, we highlight future research trends in DL systems that deserve further research.",
    "citationCount": 207,
    "referenceCount": 222
}