{
    "paperId": "dd7e737cf6255310fd9ae11ecaa0eaf38b3c01df",
    "title": "InfoSteer: Steering Information Utility in Language Model Post-Training",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Chunyuan Deng",
        "Ruidi Chang",
        "Hanjie Chen"
    ],
    "doi": "10.48550/arXiv.2507.05158",
    "arxivId": "2507.05158",
    "url": "https://www.semanticscholar.org/paper/dd7e737cf6255310fd9ae11ecaa0eaf38b3c01df",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Recent advancements in language models (LMs) gradually ushered in an era where post-training is crucial. Yet, post-training approaches such as supervised fine-tuning (SFT) do not guarantee effective use of knowledge acquired during pretraining. We therefore present \\ours, a lightweight method that encourages parametric information utilization in LMs during post-training. This is achieved via treating FFN layer as associate key-value memory, and promotes the use of stored memory vectors via forward-pass interventions or regularization during backpropagation. We find this simple guidance during post-training phase delivers consistent performance improvements across diverse model families--including Qwen, Gemma and Llama-spanning over 15 downstream tasks in both ID and OOD evaluations. Beyond performance gains, we also find that steered LMs can adaptively allocate information-placing more emphasis on generating semantically meaningful tokens, while using fewer resources on simple transition ones (e.g., `,'or `and'). Our work underscores that vanilla post-training does not fully leverage pre-training potential, and steering LMs in latent representation space offers a promising approach that enhances both performance and interpretability.",
    "citationCount": 0,
    "referenceCount": 79
}