{
    "paperId": "3399ccaf78b9f2414798b4c85e68e485e847181d",
    "title": "Optimizing distributed training deployment in heterogeneous GPU clusters",
    "year": 2020,
    "venue": "Conference on Emerging Network Experiment and Technology",
    "authors": [
        "Xiaodong Yi",
        "Shiwei Zhang",
        "Ziyue Luo",
        "Guoping Long",
        "Lansong Diao",
        "Chuan Wu",
        "Zhen Zheng",
        "Jun Yang",
        "Wei Lin"
    ],
    "doi": "10.1145/3386367.3432728",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/3399ccaf78b9f2414798b4c85e68e485e847181d",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "This paper proposes HeteroG, an automatic module to accelerate deep neural network training in heterogeneous GPU clusters. To train a deep learning model with large amounts of data, distributed training using data or model parallelism has been widely adopted, mostly over homogeneous devices (GPUs, network bandwidth). Heterogeneous training environments may often exist in shared clusters with GPUs of different models purchased in different batches and network connections of different bandwidth availability (e.g., due to contention). Classic data parallelism does not work well in a heterogeneous cluster, while model-parallel training is hard to plan. HeteroG enables highly-efficient distributed training over heterogeneous devices, by automatically converting a single-GPU training model to a distributed one according to the deep learning graph and available resources. HeteroG embraces operation-level hybrid parallelism, communication architecture selection and execution scheduling, based on a carefully designed strategy framework exploiting both GNN-based learning and combinatorial optimization. We compare HeteroG with existing parallelism schemes and show that it achieves up-to 222% training speed-up. HeteroG also enables efficient training of large models over a set of heterogeneous devices where simple parallelism is infeasible.",
    "citationCount": 31,
    "referenceCount": 65
}