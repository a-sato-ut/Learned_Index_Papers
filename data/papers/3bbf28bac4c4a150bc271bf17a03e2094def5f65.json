{
    "paperId": "3bbf28bac4c4a150bc271bf17a03e2094def5f65",
    "title": "Dialog Inpainting: Turning Documents into Dialogs",
    "year": 2022,
    "venue": "International Conference on Machine Learning",
    "authors": [
        "Zhuyun Dai",
        "Arun Tejasvi Chaganty",
        "Vincent Zhao",
        "Aida Amini",
        "Q. Rashid",
        "Mike Green",
        "Kelvin Guu"
    ],
    "doi": "10.48550/arXiv.2205.09073",
    "arxivId": "2205.09073",
    "url": "https://www.semanticscholar.org/paper/3bbf28bac4c4a150bc271bf17a03e2094def5f65",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2205.09073",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Many important questions (e.g.\"How to eat healthier?\") require conversation to establish context and explore in depth. However, conversational question answering (ConvQA) systems have long been stymied by scarce training data that is expensive to collect. To address this problem, we propose a new technique for synthetically generating diverse and high-quality dialog data: dialog inpainting. Our approach takes the text of any document and transforms it into a two-person dialog between the writer and an imagined reader: we treat sentences from the article as utterances spoken by the writer, and then use a dialog inpainter to predict what the imagined reader asked or said in between each of the writer's utterances. By applying this approach to passages from Wikipedia and the web, we produce WikiDialog and WebDialog, two datasets totalling 19 million diverse information-seeking dialogs -- 1,000x larger than the largest existing ConvQA dataset. Furthermore, human raters judge the answer adequacy and conversationality of WikiDialog to be as good or better than existing manually-collected datasets. Using our inpainted data to pre-train ConvQA retrieval systems, we significantly advance state-of-the-art across three benchmarks (QReCC, OR-QuAC, TREC CAsT) yielding up to 40% relative gains on standard evaluation metrics.",
    "citationCount": 74,
    "referenceCount": 54
}