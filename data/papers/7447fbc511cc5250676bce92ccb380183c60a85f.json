{
    "paperId": "7447fbc511cc5250676bce92ccb380183c60a85f",
    "title": "Predicting memory accesses: the road to compact ML-driven prefetcher",
    "year": 2019,
    "venue": "International Symposium on Memory Systems",
    "authors": [
        "Ajitesh Srivastava",
        "Angelos Lazaris",
        "Benjamin Brooks",
        "R. Kannan",
        "V. Prasanna"
    ],
    "doi": "10.1145/3357526.3357549",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/7447fbc511cc5250676bce92ccb380183c60a85f",
    "isOpenAccess": true,
    "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3357526.3357549",
    "publicationTypes": [
        "JournalArticle",
        "Book"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "With the advent of fast processors, TPUs, accelerators, and heterogeneous architectures, computation is no longer the only bottleneck. In fact for many applications, speed of execution is limited by memory performance. To address memory performance, more accurate prefetching is necessary. While sophisticated machine learning algorithms have shown to predict memory accesses with high accuracy, they suffer with several issues that prevent them from being practical solutions as hardware prefetchers. These issues are centered around size of the model that results in high memory requirement, high latency and difficulty in online retraining. As the first step towards building ML-based prefetchers, we propose a compressed-LSTM approach for accurate memory access prediction. With a novel compression technique based on output encoding, we show that for the problem of predicting one of n memory locations, our technique results in O(n/log n) compression factor over the traditional LSTM approach. We further demonstrate through experiments on several benchmarks that the prediction accuracy drop due to compression is small and the training is fast. The actual compression obtained is of the order of 100Ã—.",
    "citationCount": 32,
    "referenceCount": 29
}