{
    "paperId": "f1f78bbb3e146874501e3c52b56cff6abf731842",
    "title": "Deep representation learning: Fundamentals, Perspectives, Applications, and Open Challenges",
    "year": 2022,
    "venue": "arXiv.org",
    "authors": [
        "K. T. Baghaei",
        "A. Payandeh",
        "Pooya Fayyazsanavi",
        "Shahram Rahimi",
        "Zhiqian Chen",
        "Somayeh Bakhtiari Ramezani"
    ],
    "doi": "10.48550/arXiv.2211.14732",
    "arxivId": "2211.14732",
    "url": "https://www.semanticscholar.org/paper/f1f78bbb3e146874501e3c52b56cff6abf731842",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2211.14732",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Machine Learning algorithms have had a profound impact on the field of computer science over the past few decades. These algorithms performance is greatly influenced by the representations that are derived from the data in the learning process. The representations learned in a successful learning process should be concise, discrete, meaningful, and able to be applied across a variety of tasks. A recent effort has been directed toward developing Deep Learning models, which have proven to be particularly effective at capturing high-dimensional, non-linear, and multi-modal characteristics. In this work, we discuss the principles and developments that have been made in the process of learning representations, and converting them into desirable applications. In addition, for each framework or model, the key issues and open challenges, as well as the advantages, are examined.",
    "citationCount": 8,
    "referenceCount": 483
}