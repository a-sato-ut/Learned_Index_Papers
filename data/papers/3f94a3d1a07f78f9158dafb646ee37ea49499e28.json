{
    "paperId": "3f94a3d1a07f78f9158dafb646ee37ea49499e28",
    "title": "Efficient Deep Learning Pipelines for Accurate Cost Estimations Over Large Scale Query Workload",
    "year": 2021,
    "venue": "SIGMOD Conference",
    "authors": [
        "Johan Kok",
        "Zhi-qiang Kang",
        "S. Tan",
        "Feng Cheng",
        "Shixuan Sun",
        "Bingsheng He"
    ],
    "doi": "10.1145/3448016.3457546",
    "arxivId": "2103.12465",
    "url": "https://www.semanticscholar.org/paper/3f94a3d1a07f78f9158dafb646ee37ea49499e28",
    "isOpenAccess": true,
    "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3448016.3457546",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The use of deep learning models for forecasting the resource consumption patterns of SQL queries have recently been a popular area of study. While these models have demonstrated promising accuracy, training them over large scale industry workloads are expensive. Space inefficiencies of encoding techniques over large numbers of queries and excessive padding used to enforce shape consistency across diverse query plans implies 1) longer model training time and 2) the need for expensive, scaled up infrastructure to support batched training. In turn, we developed Prestroid, a tree convolution based data science pipeline that accurately predicts resource consumption patterns of query traces, but at a much lower cost. We evaluated our pipeline over 19K Presto OLAP queries, on a data lake of more than 20PB of data from Grab. Experimental results imply that our pipeline outperforms benchmarks on predictive accuracy, contributing to more precise resource prediction for large-scale workloads, yet also reduces per-batch memory footprint by 13.5x and per-epoch training time by 3.45x. We demonstrate direct cost savings of up to 13.2x for large batched model training over Microsoft Azure VMs.",
    "citationCount": 26,
    "referenceCount": 40
}