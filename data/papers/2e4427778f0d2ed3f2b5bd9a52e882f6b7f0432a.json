{
    "paperId": "2e4427778f0d2ed3f2b5bd9a52e882f6b7f0432a",
    "title": "Tutorial on Amortized Optimization",
    "year": 2022,
    "venue": "Found. Trends Mach. Learn.",
    "authors": [
        "Brandon Amos"
    ],
    "doi": "10.1561/9781638282099",
    "arxivId": "2202.00665",
    "url": "https://www.semanticscholar.org/paper/2e4427778f0d2ed3f2b5bd9a52e882f6b7f0432a",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2202.00665",
    "publicationTypes": [
        "JournalArticle",
        "Review"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Optimization is a ubiquitous modeling tool and is often deployed in settings which repeatedly solve similar instances of the same problem. Amortized optimization methods use learning to predict the solutions to problems in these settings, exploiting the shared structure between similar problem instances. These methods have been crucial in variational inference and reinforcement learning and are capable of solving optimization problems many orders of magnitudes faster than traditional optimization methods that do not use amortization. This tutorial presents an introduction to the amortized optimization foundations behind these advancements and overviews their applications in variational inference, sparse coding, gradient-based meta-learning, control, reinforcement learning, convex optimization, optimal transport, and deep equilibrium networks. The source code for this tutorial is available at https://github.com/facebookresearch/amortized-optimization-tutorial.",
    "citationCount": 65,
    "referenceCount": 344
}