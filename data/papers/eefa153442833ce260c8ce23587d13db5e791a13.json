{
    "paperId": "eefa153442833ce260c8ce23587d13db5e791a13",
    "title": "Towards Online and Safe Configuration Tuning with Semi-supervised Anomaly Detection",
    "year": 2024,
    "venue": "International Conference on Information and Knowledge Management",
    "authors": [
        "Haitian Chen",
        "Xu Chen",
        "Zibo Liang",
        "Xiushi Feng",
        "Jiandong Xie",
        "Han Su",
        "Kai Zheng"
    ],
    "doi": "10.1145/3627673.3679700",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/eefa153442833ce260c8ce23587d13db5e791a13",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The performance of modern database management systems highly relies on hundreds of adjustable knobs. Traditionally, these knobs are manually adjusted by database administrators, a process that is both inefficient and ineffective for tuning large-scale databases in cloud environments. Recent research has explored the use of machine learning techniques to enable the automatic tuning of database configurations. Although most existing learning-based methods achieve satisfactory results on static workloads, they often experience performance degradation and low sampling efficiency in real-world environments. According to our study, this is primarily due to a lack of safety guarantees during the configuration sampling process. To address the aforementioned issues, we propose SafeTune, an online tuning system that adapts to dynamic workloads. Our core idea is to filter out a large number of configurations with potential risks during the configuration sampling process. We employ a two-stage filtering approach: The first stage utilizes a semi-supervised outlier ensemble with feature learning to achieve high-quality feature representation. The second stage employs a ranking-based classifier to refine the filtering process. In addition, to alleviate the cold-start problem, we leverage the historical tuning experience to provide high-quality initial samples during the initialization phase. We conducted comprehensive evaluations on static and dynamic workloads. In comparison to offline baseline methods, SafeTune reduces 95.6%-98.6% unsafe configuration suggestions. In contrast with state-of-the-art methods, SafeTune has improved cumulative performance by 10.5%-46.6% and tuning speed by 15.1%-35.4%.",
    "citationCount": 0,
    "referenceCount": 33
}