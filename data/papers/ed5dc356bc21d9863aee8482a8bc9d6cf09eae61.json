{
    "paperId": "ed5dc356bc21d9863aee8482a8bc9d6cf09eae61",
    "title": "Online RL in the programmable dataplane with OPaL",
    "year": 2021,
    "venue": "Conference on Emerging Network Experiment and Technology",
    "authors": [
        "Kyle A. Simpson",
        "Dimitrios P. Pezaros"
    ],
    "doi": "10.1145/3485983.3493345",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/ed5dc356bc21d9863aee8482a8bc9d6cf09eae61",
    "isOpenAccess": true,
    "openAccessPdf": "https://eprints.gla.ac.uk/258124/2/258124.pdf",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Reinforcement learning (RL) is a key tool in data-driven networking for learning to control systems online. While recent research has shown how to offload machine learning tasks to the dataplane (reducing processing latency), online learning remains an open challenge unless the model is moved back to a host CPU, harming latency-sensitive applications. Our poster introduces OPaL---On Path Learning---the first work to bring online reinforcement learning to the dataplane. OPaL makes online learning possible in SmartNIC/NPU hardware by returning to classical RL techniques---avoiding neural networks. This simplifies update logic, enabling online learning, and benefits well from the parallelism common to SmartNICs. We show that our implementation on Netronome SmartNIC hardware offers concrete latency improvements over host execution.",
    "citationCount": 3,
    "referenceCount": 6
}