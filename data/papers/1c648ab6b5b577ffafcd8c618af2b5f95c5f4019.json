{
    "paperId": "1c648ab6b5b577ffafcd8c618af2b5f95c5f4019",
    "title": "Scalable Expressiveness through Preprocessed Graph Perturbations",
    "year": 2024,
    "venue": "International Conference on Information and Knowledge Management",
    "authors": [
        "Danial Saber",
        "Amirali Salehi-Abari"
    ],
    "doi": "10.1145/3627673.3679993",
    "arxivId": "2406.11714",
    "url": "https://www.semanticscholar.org/paper/1c648ab6b5b577ffafcd8c618af2b5f95c5f4019",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2406.11714",
    "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Graph Neural Networks (GNNs) have emerged as the predominant method for analyzing graph-structured data. However, canonical GNNs have limited expressive power and generalization capability, thus triggering the development of more expressive yet computationally intensive methods. One such approach is to create a series of perturbed versions of input graphs and then repeatedly conduct multiple message-passing operations on all variations during training. Despite their expressive power, this approach does not scale well on larger graphs. To address this scalability issue, we introduce Scalable Expressiveness through Preprocessed Graph Perturbation (SE2P). This model offers a flexible, configurable balance between scalability and generalizability with four distinct configuration classes. Our extensive experiments demonstrate that SE2P can enhance generalizability compared to benchmarks while achieving significant speed improvements of up to 8-fold.",
    "citationCount": 2,
    "referenceCount": 70
}