{
    "paperId": "a88a1d6bbae42ca19a8cd57409cbf6a7f5983e23",
    "title": "From Principles to Applications: A Comprehensive Survey of Discrete Tokenizers in Generation, Comprehension, Recommendation, and Information Retrieval",
    "year": 2025,
    "venue": "arXiv.org",
    "authors": [
        "Jian Jia",
        "Jingtong Gao",
        "Ben Xue",
        "Junhao Wang",
        "Qingpeng Cai",
        "Quan Chen",
        "Xiangyu Zhao",
        "Peng Jiang",
        "Kun Gai"
    ],
    "doi": "10.48550/arXiv.2502.12448",
    "arxivId": "2502.12448",
    "url": "https://www.semanticscholar.org/paper/a88a1d6bbae42ca19a8cd57409cbf6a7f5983e23",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Review"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Linguistics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Discrete tokenizers have emerged as indispensable components in modern machine learning systems, particularly within the context of autoregressive modeling and large language models (LLMs). These tokenizers serve as the critical interface that transforms raw, unstructured data from diverse modalities into discrete tokens, enabling LLMs to operate effectively across a wide range of tasks. Despite their central role in generation, comprehension, and recommendation systems, a comprehensive survey dedicated to discrete tokenizers remains conspicuously absent in the literature. This paper addresses this gap by providing a systematic review of the design principles, applications, and challenges of discrete tokenizers. We begin by dissecting the sub-modules of tokenizers and systematically demonstrate their internal mechanisms to provide a comprehensive understanding of their functionality and design. Building on this foundation, we synthesize state-of-the-art methods, categorizing them into multimodal generation and comprehension tasks, and semantic tokens for personalized recommendations. Furthermore, we critically analyze the limitations of existing tokenizers and outline promising directions for future research. By presenting a unified framework for understanding discrete tokenizers, this survey aims to guide researchers and practitioners in addressing open challenges and advancing the field, ultimately contributing to the development of more robust and versatile AI systems.",
    "citationCount": 4,
    "referenceCount": 71
}