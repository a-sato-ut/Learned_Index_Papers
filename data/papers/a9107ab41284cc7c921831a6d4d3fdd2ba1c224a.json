{
    "paperId": "a9107ab41284cc7c921831a6d4d3fdd2ba1c224a",
    "title": "From Learning to Optimize to Learning Optimization Algorithms",
    "year": 2024,
    "venue": "International Conference on Artificial Intelligence and Statistics",
    "authors": [
        "Camille Castera",
        "Peter Ochs"
    ],
    "doi": "10.48550/arXiv.2405.18222",
    "arxivId": "2405.18222",
    "url": "https://www.semanticscholar.org/paper/a9107ab41284cc7c921831a6d4d3fdd2ba1c224a",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Towards designing learned optimization algorithms that are usable beyond their training setting, we identify key principles that classical algorithms obey, but have up to now, not been used for Learning to Optimize (L2O). Following these principles, we provide a general design pipeline, taking into account data, architecture and learning strategy, and thereby enabling a synergy between classical optimization and L2O, resulting in a philosophy of Learning Optimization Algorithms. As a consequence our learned algorithms perform well far beyond problems from the training distribution. We demonstrate the success of these novel principles by designing a new learning-enhanced BFGS algorithm and provide numerical experiments evidencing its adaptation to many settings at test time.",
    "citationCount": 1,
    "referenceCount": 83
}