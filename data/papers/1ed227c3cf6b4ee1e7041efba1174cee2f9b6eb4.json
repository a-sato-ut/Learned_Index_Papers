{
    "paperId": "1ed227c3cf6b4ee1e7041efba1174cee2f9b6eb4",
    "title": "Self-supervised Vision Transformers for Writer Retrieval",
    "year": 2024,
    "venue": "IEEE International Conference on Document Analysis and Recognition",
    "authors": [
        "Tim Raven",
        "Arthur Matei",
        "Gernot A. Fink"
    ],
    "doi": "10.1007/978-3-031-70536-6_23",
    "arxivId": "2409.00751",
    "url": "https://www.semanticscholar.org/paper/1ed227c3cf6b4ee1e7041efba1174cee2f9b6eb4",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "While methods based on Vision Transformers (ViT) have achieved state-of-the-art performance in many domains, they have not yet been applied successfully in the domain of writer retrieval. The field is dominated by methods using handcrafted features or features extracted from Convolutional Neural Networks. In this work, we bridge this gap and present a novel method that extracts features from a ViT and aggregates them using VLAD encoding. The model is trained in a self-supervised fashion without any need for labels. We show that extracting local foreground features is superior to using the ViT's class token in the context of writer retrieval. We evaluate our method on two historical document collections. We set a new state-at-of-art performance on the Historical-WI dataset (83.1\\% mAP), and the HisIR19 dataset (95.0\\% mAP). Additionally, we demonstrate that our ViT feature extractor can be directly applied to modern datasets such as the CVL database (98.6\\% mAP) without any fine-tuning.",
    "citationCount": 2,
    "referenceCount": 35
}