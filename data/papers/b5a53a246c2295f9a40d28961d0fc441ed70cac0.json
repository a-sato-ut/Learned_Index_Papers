{
    "paperId": "b5a53a246c2295f9a40d28961d0fc441ed70cac0",
    "title": "GIF: Generative Inspiration for Face Recognition at Scale",
    "year": 2025,
    "venue": "Computer Vision and Pattern Recognition",
    "authors": [
        "Saeed Ebrahimi",
        "Sahar Rahimi Malakshan",
        "Ali Dabouei",
        "Srinjoy Das",
        "Jeremy M. Dawson",
        "Nasser M. Nasrabadi"
    ],
    "doi": "10.1109/CVPR52734.2025.00334",
    "arxivId": "2505.03012",
    "url": "https://www.semanticscholar.org/paper/b5a53a246c2295f9a40d28961d0fc441ed70cac0",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Aiming to reduce the computational cost of Softmax in massive label space of Face Recognition (FR) benchmarks, recent studies estimate the output using a subset of identities. Although promising, the association between the computation cost and the number of identities in the dataset remains linear only with a reduced ratio. A shared characteristic among available FR methods is the employment of atomic scalar labels during training. Consequently, the input to label matching is through a dot product between the feature vector of the input and the Softmax centroids. Inspired by generative modeling, we present a simple yet effective method that substitutes scalar labels with structured identity code, i.e., a sequence of integers. Specifically, we propose a tokenization scheme that transforms atomic scalar labels into structured identity codes. Then, we train an FR backbone to predict the code for each input instead of its scalar label. As a result, the associated computational cost becomes logarithmic w.r.t. number of identities. We demonstrate the benefits of the proposed method by conducting experiments. In particular, our method outperforms its competitors by 1.52%, and 0.6% at TAR@FAR= le â€” 4 on IJB- B and IJB-C, respectively, while transforming the association between computational cost and the number of identities from linear to logarithmic. Code",
    "citationCount": 0,
    "referenceCount": 79
}