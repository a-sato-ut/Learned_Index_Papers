{
    "paperId": "95a9dc7c38de97ce916285a74a9117d3b1c7b804",
    "title": "Puppeteer: A Random Forest Based Manager for Hardware Prefetchers Across the Memory Hierarchy",
    "year": 2022,
    "venue": "ACM Transactions on Architecture and Code Optimization (TACO)",
    "authors": [
        "Furkan Eris",
        "Marcia S. Louis",
        "Kubra Eris",
        "José L. Abellán",
        "A. Joshi"
    ],
    "doi": "10.1145/3570304",
    "arxivId": "2201.12027",
    "url": "https://www.semanticscholar.org/paper/95a9dc7c38de97ce916285a74a9117d3b1c7b804",
    "isOpenAccess": true,
    "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3570304",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Over the years, processor throughput has steadily increased. However, the memory throughput has not increased at the same rate, which has led to the memory wall problem in turn increasing the gap between effective and theoretical peak processor performance. To cope with this, there has been an abundance of work in the area of data/instruction prefetcher designs. Broadly, prefetchers predict future data/instruction address accesses and proactively fetch data/instructions in the memory hierarchy with the goal of lowering data/instruction access latency. To this end, one or more prefetchers are deployed at each level of the memory hierarchy, but typically, each prefetcher gets designed in isolation without comprehensively accounting for other prefetchers in the system. As a result, individual prefetchers do not always complement each other, and that leads to lower average performance gains and/or many negative outliers. In this work, we propose Puppeteer, which is a hardware prefetcher manager that uses a suite of random forest regressors to determine at runtime which prefetcher should be ON at each level in the memory hierarchy, such that the prefetchers complement each other and we reduce the data/instruction access latency. Compared to a design with no prefetchers, using Puppeteer we improve IPC by 46.0% in 1 one-core, 25.8% in four-core, and 11.9% in eight-core processors on average across traces generated from SPEC2017, SPEC2006, and Cloud suites with ~11-KB overhead. Moreover, we also reduce the number of negative outliers by more than 89%, and the performance loss of the worst-case negative outlier from 25% to only 5% compared to the state of the art.",
    "citationCount": 8,
    "referenceCount": 79
}