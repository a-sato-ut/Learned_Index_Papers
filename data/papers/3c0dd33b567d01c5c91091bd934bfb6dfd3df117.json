{
    "paperId": "3c0dd33b567d01c5c91091bd934bfb6dfd3df117",
    "title": "Video Object Matting via Hierarchical Space-Time Semantic Guidance",
    "year": 2023,
    "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
    "authors": [
        "Yumeng Wang",
        "Bo Xu",
        "Ziwen Li",
        "Han Huang",
        "Cheng Lu",
        "Yandong Guo"
    ],
    "doi": "10.1109/WACV56688.2023.00509",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/3c0dd33b567d01c5c91091bd934bfb6dfd3df117",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Different from most existing approaches that require trimap generation for each frame, we reformulate video object matting (VOM) by introducing improved semantic guidance propagation. The proposed approach can achieve a higher degree of temporal coherence between frames with only a single coarse mask as a reference. In this paper, we adapt the hierarchical memory matching mechanism into the space-time baseline to build an efficient and robust framework for semantic guidance propagation and alpha prediction. To enhance the temporal smoothness, we also propose a cross-frame attention refinement (CFAR) module that can refine the feature representations across multiple adjacent frames (both historical and current frames) based on the spatio-temporal correlation among the cross- frame pixels. Extensive experiments demonstrate the effectiveness of hierarchical spatio-temporal semantic guidance and the cross-video-frame attention refinement module, and our model outperforms the state-of-the-art VOM methods. We also analyze the significance of different components in our model.",
    "citationCount": 3,
    "referenceCount": 35
}