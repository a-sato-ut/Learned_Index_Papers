{
    "paperId": "bdc73a627c38ef793450ee45bc077addef3b4e8e",
    "title": "MUSTACHE: Multi-Step-Ahead Predictions for Cache Eviction",
    "year": 2022,
    "venue": "arXiv.org",
    "authors": [
        "Gabriele Tolomei",
        "Lorenzo Takanen",
        "F. Pinelli"
    ],
    "doi": "10.48550/arXiv.2211.02177",
    "arxivId": "2211.02177",
    "url": "https://www.semanticscholar.org/paper/bdc73a627c38ef793450ee45bc077addef3b4e8e",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2211.02177",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "In this work, we propose MUSTACHE, a new page cache replacement algorithm whose logic is learned from observed memory access requests rather than fixed like existing policies. We formulate the page request prediction problem as a categorical time series forecasting task. Then, our method queries the learned page request forecaster to obtain the next k predicted page memory references to better approximate the optimal Bélády’s replacement algorithm. We implement several forecasting techniques using advanced deep learning architectures and integrate the best-performing one into an existing open-source cache simulator. Experiments run on benchmark datasets show that MUSTACHE outperforms the best page replacement heuristic (i.e., exact LRU), improving the cache hit ratio by 1.9% and reducing the number of reads/writes required to handle cache misses by 18.4% and 10.3%.",
    "citationCount": 0,
    "referenceCount": 20
}