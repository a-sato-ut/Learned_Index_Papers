{
    "paperId": "05cad00486c1ab88452d9b355477851e1bca9075",
    "title": "Copyright-Certified Distillation Dataset: Distilling One Million Coins into One Bitcoin with Your Private Key",
    "year": 2023,
    "venue": "AAAI Conference on Artificial Intelligence",
    "authors": [
        "Tengjun Liu",
        "Ying Chen",
        "Wanxuan Gu"
    ],
    "doi": "10.1609/aaai.v37i5.25794",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/05cad00486c1ab88452d9b355477851e1bca9075",
    "isOpenAccess": true,
    "openAccessPdf": "https://ojs.aaai.org/index.php/AAAI/article/download/25794/25566",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Law",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The rapid development of neural network dataset distillation in recent years has provided new ideas in many areas such as continuous learning, neural network architecture search and privacy preservation. Dataset distillation is a very effective method to distill large training datasets into small data, thus ensuring that the test accuracy of models trained on their synthesized small datasets matches that of models trained on the full dataset. Thus, dataset distillation itself is commercially valuable, not only for reducing training costs, but also for compressing storage costs and significantly reducing the training costs of deep learning. However, copyright protection for dataset distillation has not been proposed yet, so we propose the first method to protect intellectual property by embedding watermarks in the dataset distillation process. Our approach not only popularizes the dataset distillation technique, but also authenticates the ownership of the distilled dataset by the models trained on that distilled dataset.",
    "citationCount": 1,
    "referenceCount": 45
}