{
    "paperId": "779298d91b1c71ea48e45f7033658fee1c71c077",
    "title": "Kraken: Memory-Efficient Continual Learning for Large-Scale Real-Time Recommendations",
    "year": 2020,
    "venue": "International Conference for High Performance Computing, Networking, Storage and Analysis",
    "authors": [
        "Minhui Xie",
        "Kai Ren",
        "Youyou Lu",
        "Guangxu Yang",
        "Qingxing Xu",
        "Bihai Wu",
        "Jiazhen Lin",
        "H. Ao",
        "Wanhong Xu",
        "J. Shu"
    ],
    "doi": "10.1109/SC41405.2020.00025",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/779298d91b1c71ea48e45f7033658fee1c71c077",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Modern recommendation systems in industry often use deep learning (DL) models that achieve better model accuracy with more data and model parameters. However, current opensource DL frameworks, such as TensorFlow and PyTorch, show relatively low scalability on training recommendation models with terabytes of parameters. To efficiently learn large-scale recommendation models from data streams that generate hundreds of terabytes training data daily, we introduce a continual learning system called Kraken. Kraken contains a special parameter server implementation that dynamically adapts to the rapidly changing set of sparse features for the continual training and serving of recommendation models. Kraken provides a sparsity-aware training system that uses different learning optimizers for dense and sparse parameters to reduce memory overhead. Extensive experiments using real-world datasets confirm the effectiveness and scalability of Kraken. Kraken can benefit the accuracy of recommendation tasks with the same memory resources, or trisect the memory usage while keeping model performance.",
    "citationCount": 39,
    "referenceCount": 55
}