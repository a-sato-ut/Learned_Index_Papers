{
    "paperId": "e83d74d684aa69f474e5383d6c5af298d34e770e",
    "title": "Proximal Projection Method for Stable Linearly Constrained Optimization",
    "year": 2024,
    "venue": "",
    "authors": [
        "Howard Heaton"
    ],
    "doi": null,
    "arxivId": "2407.16998",
    "url": "https://www.semanticscholar.org/paper/e83d74d684aa69f474e5383d6c5af298d34e770e",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": null,
    "s2FieldsOfStudy": [
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Many applications using large datasets require efficient methods for minimizing a proximable convex function subject to satisfying a set of linear constraints within a specified tolerance. For this task, we present a proximal projection (PP) algorithm, which is an instance of Douglas-Rachford splitting that directly uses projections onto the set of constraints. Formal guarantees are presented to prove convergence of PP estimates to optimizers. Unlike many methods that obtain feasibility asymptotically, each PP iterate is feasible. Numerically, we show PP either matches or outperforms alternatives (e.g. linearized Bregman, primal dual hybrid gradient, proximal augmented Lagrangian, proximal gradient) on problems in basis pursuit, stable matrix completion, stable principal component pursuit, and the computation of earth mover's distances.",
    "citationCount": 0,
    "referenceCount": 55
}