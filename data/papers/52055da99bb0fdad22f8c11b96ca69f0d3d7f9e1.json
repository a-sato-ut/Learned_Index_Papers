{
    "paperId": "52055da99bb0fdad22f8c11b96ca69f0d3d7f9e1",
    "title": "Reinforced Approximate Exploratory Data Analysis",
    "year": 2022,
    "venue": "AAAI Conference on Artificial Intelligence",
    "authors": [
        "Shaddy Garg",
        "S. Mitra",
        "Tong Yu",
        "Yash Gadhia",
        "A. Kashettiwar"
    ],
    "doi": "10.48550/arXiv.2212.06225",
    "arxivId": "2212.06225",
    "url": "https://www.semanticscholar.org/paper/52055da99bb0fdad22f8c11b96ca69f0d3d7f9e1",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2212.06225",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Exploratory data analytics (EDA) is a sequential decision making process where analysts choose subsequent queries that might lead to some interesting insights based on the previous queries and corresponding results. Data processing systems often execute the queries on samples to produce results with low latency. Different downsampling strategy preserves different statistics of the data and have different magnitude of latency reductions. The optimum choice of sampling strategy often depends on the particular context of the analysis flow and the hidden intent of the analyst. In this paper, we are the first to consider the impact of sampling in interactive data exploration settings as they introduce approximation errors.\nWe propose a Deep Reinforcement Learning (DRL) based framework which can optimize the sample selection in order to keep the analysis and insight generation flow intact. Evaluations with real datasets show that our technique can preserve the original insight generation flow while improving the interaction latency, compared to baseline methods.",
    "citationCount": 6,
    "referenceCount": 52
}