{
    "paperId": "efa5f4d33ef7648890b82585823093a6a3152cfe",
    "title": "Deep Interpretable Models of Theory of Mind",
    "year": 2021,
    "venue": "IEEE International Symposium on Robot and Human Interactive Communication",
    "authors": [
        "Ini Oguntola",
        "Dana Hughes",
        "K. Sycara"
    ],
    "doi": "10.1109/RO-MAN50785.2021.9515505",
    "arxivId": "2104.02938",
    "url": "https://www.semanticscholar.org/paper/efa5f4d33ef7648890b82585823093a6a3152cfe",
    "isOpenAccess": true,
    "openAccessPdf": "https://arxiv.org/pdf/2104.02938",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "When developing AI systems that interact with humans, it is essential to design both a system that can understand humans, and a system that humans can understand. Most deep network based agent-modeling approaches are 1) not interpretable and 2) only model external behavior, ignoring internal mental states, which potentially limits their capability for assistance, interventions, discovering false beliefs, etc. To this end, we develop an interpretable modular neural framework for modeling the intentions of other observed entities. We demonstrate the efficacy of our approach with experiments on data from human participants on a search and rescue task in Minecraft, and show that incorporating interpretability can significantly increase predictive performance under the right conditions.",
    "citationCount": 31,
    "referenceCount": 39
}