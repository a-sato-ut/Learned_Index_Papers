{
    "paperId": "0c2a4ee3e6b8cf1485fb1ffee00dd4f5a9fc4bef",
    "title": "(De/Re)-Compositions Expressed Systematically via MDH-Based Schedules",
    "year": 2023,
    "venue": "International Conference on Compiler Construction",
    "authors": [
        "Ari Rasch",
        "Richard Schulze",
        "Denys Shabalin",
        "A. Elster",
        "S. Gorlatch",
        "Mary W. Hall"
    ],
    "doi": "10.1145/3578360.3580269",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/0c2a4ee3e6b8cf1485fb1ffee00dd4f5a9fc4bef",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We introduce a new scheduling language, based on the formalism of Multi-Dimensional Homomorphisms (MDH). In contrast to existing scheduling languages, our MDH-based language is designed to systematically \"de-compose\" computations for the memory and core hierarchies of architectures, and \"re-compose\" the computed intermediate results back to the final result -- we say \"(de/re)-composition\" for short. We argue that our scheduling langauge is easy to use and yet expressive enough to express well-performing (de/re)-compositions of popular related approaches, e.g., the TVM compiler, for MDH-supported computations (such as linear algebra routines and stencil computations). Moreover, our language is designed as auto-tunable, i.e., any optimization decision can optionally be left to the auto-tuning engine of our system, and our system can automatically recommend schedules for the user, based on its auto-tuning capabilities. Also, by relying on the MDH approach, we can formally guarantee the correctness of optimizations expressed in our language, thereby further enhancing user experience. Our experiments on GPU and CPU confirm that we can express optimizations that cannot be expressed straightforwardly (or at all) in TVM's scheduling language, thereby achieving higher performance than TVM, and also vendor libraries provided by NVIDIA and Intel, for time-intensive computations used in real-world deep learning neural networks.",
    "citationCount": 5,
    "referenceCount": 43
}