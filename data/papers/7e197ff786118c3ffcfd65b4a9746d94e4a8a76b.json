{
    "paperId": "7e197ff786118c3ffcfd65b4a9746d94e4a8a76b",
    "title": "Data-Free Dynamic Compression of CNNs for Tractable Efficiency",
    "year": 2023,
    "venue": "VISIGRAPP : VISAPP",
    "authors": [
        "Lukas Meiner",
        "Jens Mehnert",
        "A. Condurache"
    ],
    "doi": "10.5220/0013301000003912",
    "arxivId": "2309.17211",
    "url": "https://www.semanticscholar.org/paper/7e197ff786118c3ffcfd65b4a9746d94e4a8a76b",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "To reduce the computational cost of convolutional neural networks (CNNs) on resource-constrained devices, structured pruning approaches have shown promise in lowering floating-point operations (FLOPs) without substantial drops in accuracy. However, most methods require fine-tuning or specific training procedures to achieve a reasonable trade-off between retained accuracy and reduction in FLOPs, adding computational overhead and requiring training data to be available. To this end, we propose HASTE (Hashing for Tractable Efficiency), a data-free, plug-and-play convolution module that instantly reduces a network's test-time inference cost without training or fine-tuning. Our approach utilizes locality-sensitive hashing (LSH) to detect redundancies in the channel dimension of latent feature maps, compressing similar channels to reduce input and filter depth simultaneously, resulting in cheaper convolutions. We demonstrate our approach on the popular vision benchmarks CIFAR-10 and ImageNet, where we achieve a 46.72% reduction in FLOPs with only a 1.25% loss in accuracy by swapping the convolution modules in a ResNet34 on CIFAR-10 for our HASTE module.",
    "citationCount": 3,
    "referenceCount": 58
}