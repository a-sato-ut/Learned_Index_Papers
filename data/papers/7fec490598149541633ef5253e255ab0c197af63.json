{
    "paperId": "7fec490598149541633ef5253e255ab0c197af63",
    "title": "Still Asking: How Good Are Query Optimizers, Really?",
    "year": 2025,
    "venue": "Proceedings of the VLDB Endowment",
    "authors": [
        "Viktor Leis",
        "Andrey Gubichev",
        "Atanas Mirchev",
        "P. Boncz",
        "Alfons Kemper",
        "Thomas Neumann"
    ],
    "doi": "10.14778/3750601.3760521",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/7fec490598149541633ef5253e255ab0c197af63",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Review"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "This retrospective revisits our 2015 PVLDB paper How Good Are Query Optimizers, Really?, which challenged the prevailing notion that query optimization was a solved problem. By designing the Join Order Benchmark (JOB) and conducting a series of systematic experiments, we empirically disentangled the contributions of plan enumeration, cost modeling, and cardinality estimation. Our findings showed that cardinality estimation errors are widespread and often the dominant factor behind poor query plans, while cost models and enumeration strategies matter comparatively less. The benchmark and methodology helped refocus the community's attention on cardinality estimation and led to a resurgence of research in this area, including learned and AI-based approaches. We reflect on the role of experiments and benchmarking in database research, survey developments in query optimization over the past decade, and discuss open challenges around robustness, adaptive execution, and realistic workloads.",
    "citationCount": 0,
    "referenceCount": 49
}