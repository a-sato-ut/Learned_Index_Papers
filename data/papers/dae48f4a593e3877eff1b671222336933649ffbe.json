{
    "paperId": "dae48f4a593e3877eff1b671222336933649ffbe",
    "title": "Reachability Analysis of Neural Networks Using Mixed Monotonicity",
    "year": 2021,
    "venue": "IEEE Control Systems Letters",
    "authors": [
        "Pierre-Jean Meyer"
    ],
    "doi": "10.1109/LCSYS.2022.3182547",
    "arxivId": "2111.07683",
    "url": "https://www.semanticscholar.org/paper/dae48f4a593e3877eff1b671222336933649ffbe",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2111.07683",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Engineering",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "This letter presents a new reachability analysis approach to compute interval over-approximations of the output set of feedforward neural networks with input uncertainty. We adapt to neural networks an existing mixed-monotonicity method for the reachability analysis of dynamical systems and apply it to each partial network within the main network. This ensures that the intersection of the obtained results is the tightest interval over-approximation of the output of each layer that can be obtained using mixed-monotonicity on any partial network decomposition. Unlike other tools in the literature focusing on small classes of piecewise-affine or monotone activation functions, the main strength of our approach is its generality: it can handle neural networks with any Lipschitz-continuous activation function. In addition, the simplicity of our framework allows users to very easily add unimplemented activation functions, by simply providing the function, its derivative and the global argmin and argmax of the derivative. Our algorithm is compared to five other interval-based tools (Interval Bound Propagation, RELUVAL, NEURIFY, VERINET, CROWN) on both existing benchmarks and two sets of small and large randomly generated networks for four activation functions (ReLU, TanH, ELU, SiLU).",
    "citationCount": 9,
    "referenceCount": 23
}