{
    "paperId": "ef9555816af18ed5f0182031a3f054cebba3e3ef",
    "title": "A Learned Cache Eviction Framework with Minimal Overhead",
    "year": 2023,
    "venue": "arXiv.org",
    "authors": [
        "Dongsheng Yang",
        "Daniel S. Berger",
        "K. Li",
        "Wyatt Lloyd"
    ],
    "doi": "10.48550/arXiv.2301.11886",
    "arxivId": "2301.11886",
    "url": "https://www.semanticscholar.org/paper/ef9555816af18ed5f0182031a3f054cebba3e3ef",
    "isOpenAccess": true,
    "openAccessPdf": "http://arxiv.org/pdf/2301.11886",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Recent work shows the effectiveness of Machine Learning (ML) to reduce cache miss ratios by making better eviction decisions than heuristics. However, state-of-the-art ML caches require many predictions to make an eviction decision, making them impractical for high-throughput caching systems. This paper introduces Machine learning At the Tail (MAT), a framework to build efficient ML-based caching systems by integrating an ML module with a traditional cache system based on a heuristic algorithm. MAT treats the heuristic algorithm as a filter to receive high-quality samples to train an ML model and likely candidate objects for evictions. We evaluate MAT on 8 production workloads, spanning storage, in-memory caching, and CDNs. The simulation experiments show MAT reduces the number of costly ML predictions-per-eviction from 63 to 2, while achieving comparable miss ratios to the state-of-the-art ML cache system. We compare a MAT prototype system with an LRU-based caching system in the same setting and show that they achieve similar request rates.",
    "citationCount": 4,
    "referenceCount": 40
}