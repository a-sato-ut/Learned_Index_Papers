{
  "paperId": "69891541cdbc36461c4820be8e16556b73df8e43",
  "title": "Not Small Enough? SegPQ: A Learned Approach to Compress Product Quantization Codebooks",
  "abstract": "The rapid advancements of generative artificial intelligence (GenAI) have recently led to renewed attention towards approximate nearest neighbor (ANN) search and vector databases (VectorDB). Among various ANN methodologies, vector quantization techniques like product quantization (PQ) are widely used to generate space-efficient representations for large-scale dense vectors. However, the code-books generated by PQ often reach several gigabytes in size, making them impractical for web-scale, high-dimensional vectors in resource-constrained environments like mobile devices.\n \n In this study, we propose\n SegPQ\n , a simple yet effective framework for losslessly compressing codebooks generated by\n any\n PQ variants, enabling efficient\n in-memory\n vector search on devices with limited memory. SegPQ represents the raw PQ codewords as a trained error-bounded piecewise linear approximation model (ϵ-PLA) and pre-computed low-bit residuals. We theoretically demonstrate that, with high probability, the number of bits per compressed codeword is 1.721 + ⌈log\n 2\n ϵ\n OPT\n ⌉, where ϵ\n OPT\n is the optimal error parameter that can be determined by data characteristics. To accelerate query execution, we further design SIMD-aware query processing algorithms on compressed codebooks to fully exploit the hardware parallelism offered by modern architectures. Extensive experimental studies on real datasets showcase that, for\n 1 billion\n vectors, SegPQ reduces PQ codebook memory consumption by up to\n 4.7\n x (approx.\n 851 MB\n ) while incurring only\n 3.3%\n additional query processing overhead caused by decompression.\n",
  "tags": [
    "Compression",
    "Database",
    "Learned Index",
    "Main-memory",
    "Nearest Neighbor Search"
  ]
}