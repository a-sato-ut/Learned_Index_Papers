{
  "paperId": "86f014f66e484f477fe729023ee5d30537a299d6",
  "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
  "abstract": "The promising applications of large language models are often limited by the constrained GPU memory capacity available on edge devices. Mixture-of-Experts (MoE) models help address this issue by activating only a subset of the model's parameters during computation. This approach allows the unused parameters to be offloaded to host memory, thereby reducing the overall GPU memory demand. However, existing cache-based offloading solutions handle cache misses reactively, which significantly impacts system performance. In this paper, we introduce ProMoE, a novel proactive caching system that utilizes intermediate results to predict subsequent expert usage. By proactively fetching experts in advance, ProMoE eliminates passive cache misses, removes loading time from the critical path, and reduces the performance overhead associated with offloading. Our evaluations demonstrate that ProMoE achieves an average speedup of 2.20x (up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages, respectively, compared to existing offloading solutions.",
  "tldr": "ProMoE is a proactive caching system for MoE-based large language models that predicts expert usage to improve performance, achieving up to 3.21x speedup in prefill and 5.02x in decode stages compared to traditional methods."
}