{
  "paperId": "02f0c53f8b52ace0e26ca175d41581c5df1d9186",
  "title": "Don't Fear the REAPER: A Framework for Materializing and Reusing Deep-Learning Models",
  "abstract": "Training of Deep-Learning models represents a computationally intensive task, especially with high-dimensional and high-volume data, where training may take days or weeks. Current research mainly focuses on model materialization, workflow optimization, and lifecylce management. However, searching for a similar dataset in a dataset repository to reuse a suitable pre-built model has received little attention by the data-management community. A remarkable feature of reusing an already built model is that the iterative process of finding a suitable model is shortened, and additionally, only a small set of training data is required. Thus, transferring such knowledge has become an important issue in the Deep-Learning community. However, the relationship between the source and target task is not well understood yet. The aim of this research is to reduce training time of machine learning from a data-management perspective through model reuse, and shed some light on the above relationship in the case when reusing a model is appropriate. We propose a framework to aid data scientists in searching for a similar dataset given a new dataset and selecting a suitable model. We discuss a novel approach on how to determine similarity among datasets and how to chose an appropriate model. We are confident that with recent advances of model materialization and dataset repositories our research improves the knowledge on this topic.",
  "tldr": "This paper presents a framework to enhance the reuse of deep-learning models by aiding data scientists in finding similar datasets and selecting appropriate pre-built models, thereby reducing training time and improving understanding of the relationship between source and target tasks."
}