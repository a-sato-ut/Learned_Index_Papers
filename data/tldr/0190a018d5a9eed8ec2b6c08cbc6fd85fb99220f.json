{
  "paperId": "0190a018d5a9eed8ec2b6c08cbc6fd85fb99220f",
  "title": "PQ Bench: Benchmarking Pruning and Quantization Techniques",
  "abstract": "Deep learning models are increasingly deployed in performance-critical applications, where computational efficiency must be balanced with accuracy. Model compression techniques such as pruning and quantization help address this challenge. However, they are often evaluated solely on accuracy, overlooking their impact on model size and inference time for a given hardware setup. To support practitioners in selecting and evaluating different model compression techniques, we introduce our benchmarking framework, PQ Bench. It pre-implements a set of popular compression techniques and automates the process of benchmarking their effects on accuracy, inference speed, and memory footprint. In our evaluation, we demonstrate the use of PQ Bench and provide key insights into the trade-offs across various models, compression strategies, and configurations.",
  "tldr": "PQ Bench is a benchmarking framework designed to evaluate model compression techniques like pruning and quantization. It automates the assessment of their impact on accuracy, inference speed, and memory usage, providing insights into trade-offs for various models and configurations."
}