{
  "paperId": "e97b6af9f64f73358f706d34a7433d207f57f40e",
  "title": "Towards a Benchmark for Learned Systems",
  "abstract": "This paper aims to initiate a discussion around benchmarking data management systems with machine-learned components. Traditional benchmarks such as TPC or YCSB are insufficient to analyze and understand these learned systems because they evaluate the performance under a stable workload and data distribution. Learned systems automatically specialize and adapt database components to a changing workload, database, and execution environment, thereby making conventional metrics such as average throughput ill-suited to understand their performance fully. Moreover, the standard cost-per-performance metrics fail to account for essential trade-offs related to the training cost of models and the elimination of manual database tuning. We present several ideas for designing new benchmarks that are better suited to evaluate learned systems. The main challenges entail developing new metrics to capture the particularities of learned systems and ensuring that benchmark results remain comparable across many deployments with wide-ranging designs.",
  "tldr": "この論文は、機械学習を取り入れたデータ管理システムのベンチマークの必要性を提起し、従来のベンチマークがこれらのシステムの性能を適切に評価できない理由を説明しています。新しい評価基準の設計と、さまざまなデプロイメント間での比較可能性を確保するための課題について議論しています。"
}