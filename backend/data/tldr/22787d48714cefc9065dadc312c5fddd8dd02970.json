{
  "paperId": "22787d48714cefc9065dadc312c5fddd8dd02970",
  "title": "Discovering Data Structures: Nearest Neighbor Search and Beyond",
  "abstract": "We propose a general framework for end-to-end learning of data structures. Our framework adapts to the underlying data distribution and provides fine-grained control over query and space complexity. Crucially, the data structure is learned from scratch, and does not require careful initialization or seeding with candidate data structures/algorithms. We first apply this framework to the problem of nearest neighbor search. In several settings, we are able to reverse-engineer the learned data structures and query algorithms. For 1D nearest neighbor search, the model discovers optimal distribution (in)dependent algorithms such as binary search and variants of interpolation search. In higher dimensions, the model learns solutions that resemble k-d trees in some regimes, while in others, they have elements of locality-sensitive hashing. The model can also learn useful representations of high-dimensional data and exploit them to design effective data structures. We also adapt our framework to the problem of estimating frequencies over a data stream, and believe it could also be a powerful discovery tool for new problems.",
  "tldr": "This paper presents a framework for learning data structures from scratch, focusing on nearest neighbor search. It discovers efficient algorithms like binary search and k-d trees, adapts to data distributions, and can also estimate frequencies in data streams, offering potential for broader applications."
}