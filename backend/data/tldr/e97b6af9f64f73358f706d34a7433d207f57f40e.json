{
  "paperId": "e97b6af9f64f73358f706d34a7433d207f57f40e",
  "title": "Towards a Benchmark for Learned Systems",
  "abstract": "This paper aims to initiate a discussion around benchmarking data management systems with machine-learned components. Traditional benchmarks such as TPC or YCSB are insufficient to analyze and understand these learned systems because they evaluate the performance under a stable workload and data distribution. Learned systems automatically specialize and adapt database components to a changing workload, database, and execution environment, thereby making conventional metrics such as average throughput ill-suited to understand their performance fully. Moreover, the standard cost-per-performance metrics fail to account for essential trade-offs related to the training cost of models and the elimination of manual database tuning. We present several ideas for designing new benchmarks that are better suited to evaluate learned systems. The main challenges entail developing new metrics to capture the particularities of learned systems and ensuring that benchmark results remain comparable across many deployments with wide-ranging designs.",
  "tldr": "This paper discusses the need for new benchmarks to evaluate data management systems with machine-learned components, as traditional metrics fail to capture their adaptive nature and training costs. It proposes ideas for developing metrics that reflect these unique challenges."
}