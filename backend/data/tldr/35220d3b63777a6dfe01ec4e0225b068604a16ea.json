{
  "paperId": "35220d3b63777a6dfe01ec4e0225b068604a16ea",
  "title": "A Sampling-based Learning Framework for Big Databases",
  "abstract": "The autonomous database of the next generation aims to apply the reinforcement learning (RL) on tasks like query optimization and performance tuning with little or no human DBAs’ intervention. Despite the promise, to obtain a decent policy model in the domain of database optimization is still challenging — primarily due to the inherent computational overhead involved in the data hungry RL frameworks — in particular on large databases. In the line of mitigating this adverse effect, we propose Mirror in this work. The core to Mirror is a sampling process built in an RL framework together with a transferring process of the policy model from the sampled database to its original counterpart. While being conceptually simple, we identify that the policy transfer between databases involves heavy noise and prediction drifting that cannot be neglectable. Thereby we build a theoretical-guided sampling algorithm in Mirror assisted by a continuous fine-tuning module. The experiments on the PostgreSQL and an industry database PolarDB validate that Mirror has effectively reduced the computational cost while maintaining a satisfactory performance.",
  "tldr": "The paper presents Mirror, a sampling-based reinforcement learning framework for optimizing large databases with minimal human intervention. It addresses challenges like computational overhead and noise in policy transfer, demonstrating effectiveness in reducing costs while ensuring performance through experiments on PostgreSQL and PolarDB."
}