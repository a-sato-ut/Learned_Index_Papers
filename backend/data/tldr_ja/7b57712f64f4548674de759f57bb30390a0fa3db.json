{
  "paperId": "7b57712f64f4548674de759f57bb30390a0fa3db",
  "title": "Green, Yellow, Yield: End-Host Traffic Scheduling for Distributed Deep Learning with TensorLights",
  "abstract": "The recent success of Deep Learning (DL) in a board range of AI services has led to a surging amount of DL workloads in production clusters. To support DL jobs at scale, the parameter server (PS) architecture is the most popular approach for distributing the computation in a compute cluster. Concurrent DL jobs consisting of PS tasks and worker tasks are typically launched on available compute nodes by a cluster resource manager to ensure high cluster resource utilization. As a PS needs to distribute model updates to every remote worker, its communication has very large fan-out. We observe that network contention among colocated PSes would cause stragglers among workers, resulting in application performance degradation and resource under-utilization. To mitigate the straggler effect, we propose TensorLights, which introduces traffic prioritization at host NICs to manage traffic contention among PSes. We evaluate TensorLights experimentally and show that it effectively mitigates stragglers, improves the average completion time of DL applications by up to 31%, and increases resource utilization. TensorLights is highly practical as it provides benefits without needing changes to the DL software stack.",
  "tldr_ja": "最近の深層学習（DL）の成功により、計算クラスターでのDLワークロードが急増しています。これに対処するため、TensorLightsを提案し、ホストNICでのトラフィック優先順位付けを通じて、PS間のネットワーク競合を緩和します。実験により、TensorLightsはストラグラーを軽減し、DLアプリケーションの平均完了時間を最大31%改善し、リソース利用率を向上させることが示されました。"
}