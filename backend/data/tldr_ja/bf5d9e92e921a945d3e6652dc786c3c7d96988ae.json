{
  "paperId": "bf5d9e92e921a945d3e6652dc786c3c7d96988ae",
  "title": "Testing the Robustness of Learned Index Structures",
  "abstract": "While early empirical evidence has supported the case for learned index structures as having favourable average-case performance, little is known about their worst-case performance. By contrast, classical structures are known to achieve optimal worst-case behaviour. This work evaluates the robustness of learned index structures in the presence of adversarial workloads. To simulate adversarial work-loads, we carry out a data poisoning attack on linear regression models that manipulates the cumulative distribution function (CDF) on which the learned index model is trained. The attack deteriorates the fit of the underlying ML model by injecting a set of poisoning keys into the training dataset, which leads to an increase in the prediction error of the model and thus deteriorates the overall performance of the learned index structure. We assess the performance of various regression methods and the learned index implementations ALEX and PGM-Index. We show that learned index structures can suffer from a significant performance deterioration of up to 20% when evaluated on poisoned vs. non-poisoned datasets.",
  "tldr_ja": "本研究は、学習インデックス構造の最悪ケース性能を評価し、敵対的なワークロードに対するロバスト性を検証します。データポイズニング攻撃を用いて、学習モデルの予測誤差が最大20%悪化することを示し、ALEXやPGM-Indexなどの学習インデックスの性能低下を明らかにしました。"
}