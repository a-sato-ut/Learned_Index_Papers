{
  "paperId": "9b622d511ec39ae6cd9375f9e33789f2c5fd2233",
  "title": "LearnedCache: A Locality-Aware Collaborative Data Caching by Learning Model",
  "abstract": "High-efficiency in-memory caching databases are the key to building large-scale Internet services. Well-designed cache can reduce the pressure on network servers and the response delay of applications. The popular memory caching system Memcached and Redis have been successfully deployed by the famous Internet enterprises, such as Amazon, GitHub and Sina. The state-of-the-art solutions mainly focus on optimizing the performance by providing a suit of static caching policies for various applications. Take the popular key-value data structure caching store Redis for example, it provides LRU, LFU and random strategies. The current caching approaches are based on either access frequency or access timestamp, not considering data locality. However, data locality has been shown to have significant impacts on big data workloads, which are poorly served by current static caching approaches. In this paper, we present LearnedCache, a highly efficient inmemory caching algorithm. It significantly outperforms various replacement policies of Redis and Memcached for a variety of workloads. LearnedCache accomplishes this by leveraging a locality-aware learning model that provides the hotspot map of hot data based on both access frequency and access timestamp. Furthermore, to make the LearnedCache design light and highly efficient, LearnedCache adopts a collaborative technology: TinyLFU filters the cold request data into cache; lazily trains and learns the model for reducing the overhead of main worker; learned-index data structure for optimizing the data well balanced. Our methods will be useful for all distributed Web, file system, database and content delivery services. Compared with the various caching policies of two popular in-memory KV stores(i.e., Redis and Memcached), experimental results show that LearnedCache outperforms LRU and LFU by 8.7% and 12.6% on average (up to 13.4% and 16.5%), respectively.",
  "tldr_ja": "LearnedCacheは、データの局所性を考慮した新しいインメモリキャッシングアルゴリズムで、RedisやMemcachedの従来の置換ポリシーよりも優れた性能を発揮します。アクセス頻度とタイムスタンプを基にホットデータのマップを生成し、TinyLFU技術を用いて冷データをフィルタリングします。実験結果では、LRUやLFUよりも平均8.7%から12.6%の性能向上を示しました。"
}