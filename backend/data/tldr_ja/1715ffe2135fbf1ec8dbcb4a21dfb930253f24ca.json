{
  "paperId": "1715ffe2135fbf1ec8dbcb4a21dfb930253f24ca",
  "title": "Towards Better Interpretability in Deep Q-Networks",
  "abstract": "Deep reinforcement learning techniques have demonstrated superior performance in a wide variety of environments. As improvements in training algorithms continue at a brisk pace, theoretical or empirical studies on understanding what these networks seem to learn, are far behind. In this paper we propose an interpretable neural network architecture for Q-learning which provides a global explanation of the model’s behavior using key-value memories, attention and reconstructible embeddings. With a directed exploration strategy, our model can reach training rewards comparable to the state-of-the-art deep Q-learning models. However, results suggest that the features extracted by the neural network are extremely shallow and subsequent testing using out-of-sample examples shows that the agent can easily overfit to trajectories seen during training.",
  "tldr_ja": "本論文では、深層強化学習におけるQ学習のための解釈可能なニューラルネットワークアーキテクチャを提案します。このモデルは、キー・バリュー記憶や注意機構を用いて行動のグローバルな説明を提供し、最先端の深層Q学習モデルに匹敵する報酬を得ることができます。しかし、抽出された特徴は浅く、訓練データに対して過学習しやすいことが示されています。"
}