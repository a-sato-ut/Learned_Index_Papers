{
  "paperId": "86f014f66e484f477fe729023ee5d30537a299d6",
  "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
  "abstract": "The promising applications of large language models are often limited by the constrained GPU memory capacity available on edge devices. Mixture-of-Experts (MoE) models help address this issue by activating only a subset of the model's parameters during computation. This approach allows the unused parameters to be offloaded to host memory, thereby reducing the overall GPU memory demand. However, existing cache-based offloading solutions handle cache misses reactively, which significantly impacts system performance. In this paper, we introduce ProMoE, a novel proactive caching system that utilizes intermediate results to predict subsequent expert usage. By proactively fetching experts in advance, ProMoE eliminates passive cache misses, removes loading time from the critical path, and reduces the performance overhead associated with offloading. Our evaluations demonstrate that ProMoE achieves an average speedup of 2.20x (up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages, respectively, compared to existing offloading solutions.",
  "tldr_ja": "ProMoEは、Mixture-of-Expertsモデルの効率を向上させる新しいプロアクティブキャッシングシステムです。中間結果を利用して次の専門家の使用を予測し、事前に取得することでキャッシュミスを排除し、オフロードに伴う性能低下を軽減します。評価結果では、既存のオフロードソリューションに比べ、平均2.20倍の速度向上を実現しました。"
}