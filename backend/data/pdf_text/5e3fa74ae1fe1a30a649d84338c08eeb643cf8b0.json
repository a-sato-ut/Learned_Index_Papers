{
  "paperId": "5e3fa74ae1fe1a30a649d84338c08eeb643cf8b0",
  "title": "LIFT: Reinforcement Learning in Computer Systems by Learning From Demonstrations",
  "pdfPath": "5e3fa74ae1fe1a30a649d84338c08eeb643cf8b0.pdf",
  "text": "LIFT: Reinforcement Learning in Computer Systems\nby Learning From Demonstrations\nMichael Schaarschmidt\nUniversity of Cambridge\nmichael.schaarschmidt@cl.cam.ac.\nukAlexander Kuhnle\nUniversity of Cambridge\nalexander.kuhnle@cl.cam.ac.ukBen Ellis\nUniversity of Cambridge\nbe255@cam.ac.uk\nKai Fricke\nHelmut Schmidt University\nfricke@hsu-hh.deFelix Gessert\nBaqend\nfg@baqend.comEiko Yoneki\nUniversity of Cambridge\neiko.yoneki@cl.cam.ac.uk\nABSTRACT\nReinforcement learning approaches have long appealed to\nthe data management community due to their ability to learn\nto control dynamic behavior from raw system performance.\nRecent successes in combining deep neural networks with\nreinforcement learning have sparked significant new interest\nin this domain. However, practical solutions remain elusive\ndue to large training data requirements, algorithmic instabil-\nity, and lack of standard tools.\nIn this work, we introduce LIFT, an end-to-end software\nstack for applying deep reinforcement learning to data man-\nagement tasks. While prior work has frequently explored\napplications in simulations, LIFT centers on utilizing hu-\nman expertise to learn from demonstrations, thus lowering\nonline training times. We further introduce TensorForce, a\nTensorFlow library for applied deep reinforcement learning\nexposing a unified declarative interface to common RL algo-\nrithms, thus providing a backend to LIFT. We demonstrate\nthe utility of LIFT in two case studies in database compound\nindexing and resource management in stream processing.\nResults show LIFT controllers initialized from demonstra-\ntions can outperform human baselines and heuristics across\nlatency metrics and space usage by up to 70%.\nKEYWORDS\nACM proceedings, L ATEX, text tagging\n1 INTRODUCTION\nModel-free reinforcement learning (RL) techniques offer a\ngeneric framework for optimizing decision making from\nraw feedback signals such as system performance [ 67], thus\nnot requiring an analytical model of the system. In recent\nyears, deep reinforcement learning (DRL) approaches which\ncombine RL with deep neural networks have enjoyed suc-\ncesses in a variety of domains such as games (Go [ 66],\nAtari [ 16,49,50,74]), and applied domains such as indus-\ntrial process control [ 25] or robotic manipulation [ 71]. RLapproaches have also long appealed to computer systems\nresearchers, with experimental applications in domains such\nas adaptive routing or server resource management spanning\nback over 20 years [ 35,36,68,69]. The advent of deep RL in\nin combination with widely available deep learning frame-\nworks has renewed interest in this approach. More recent\nexamples include automated TensorFlow device placements\n[46,47], client-side bit-rate selection for video streaming\n[43], and simplified cluster scheduling [42].\nHowever, practical RL deployments in computer systems\nand data management remain difficult due to large training\ndata requirements and expensive decision evaluations (e.g.\nmultiple minutes to deploy a cluster configuration). RL algo-\nrithms also suffer from inferior predictability and stability\ncompared to simpler heuristics [ 26,41]. Consequently, proof-\nof-concept successes in simplified and highly controlled sim-\nulations have infrequently lead to practical deployments.\nNonetheless, DRL remains appealing as it combines the abil-\nity of deep neural networks to identify and combine features\nin unforeseen ways with learning from raw system feedback.\nThe long-term aim is to automate manual feature and al-\ngorithm design in computer systems and potentially learn\ncomplex behaviour outperforming manual designs.\nIn this work, we explore these limitations by outlining a\nsoftware stack for practical DRL, with focus on guiding learn-\ning via existing log data or demonstrated examples. The key\nidea of our paper is that in modern data processing engines,\nfine-granular log data can be used to extract demonstrations\nof desired dynamic configurations. Such demonstrations can\nbe used to pretrain a control model, which is subsequently\nrefined when deployed in its concrete application context.\nTo this end, we make the following contributions:\nWe present LIFT (§4), a high level framework for LearnIng\nFromTraces which provides common components to inter-\nface and map between systems and reinforcement learning,\nthus removing boilerplate code. We further introduce Ten-\nsorForce, a highly modularized DRL library focusing on a\ndeclarative API for common algorithms, which serves as anarXiv:1808.07903v1  [cs.LG]  23 Aug 2018\n\n4. Generates RL model from schema, compiles\nto TensorFlow . Initialize  model from historic traces\nSystem,\ne.g. DB serverSystem LIFT RL-API +  \nDeep Learning backend\nTrace loader1. Existing query set,  \nprior index conﬁguration\n2. Traces,  \nslow query logs:  \n[Query , Query-Plan, Execution-stats]  TensorForce\nAgent APILIFT-Controllers  \nSystem Converter  Model Converter  \nSchema  TensorFlow\n3. Load and convert between system and RL-API:  \nState: Query: Operator structure, indexed attributes  \nAction: Query-Plan: Index used by query planner  \nReward: Execution-stats: Latency  5.  Manage system and  \nRL-API  via controllers  \n6. Reﬁne model  \non live feedback  Figure 1: LIFT workflow.\nalgorithmic backend for LIFT. LIFT allows users to specify\ndata layouts of states and action spaces which are used by\nTensorForce to generate TensorFlow models for executing RL\ntasks (§4.2). In the evaluation (§6), we demonstrate the utility\nof our LIFT prototype in two experimental data management\ncase studies. First, we use LIFT to generate a controller for\nautomatic compound database indexing (§5). Indexing is an\nattractive use case for RL as the optimal index set for an\napplication depends on the complex interaction of work-\nload, query operators within each query, data distribution,\nand query planner heuristics. While analytical solutions are\ndifficult to build and vary per database and query planner,\nrich feedback from slow query logs enables RL controllers\nto identify effective solutions. Experimental results show\nthat a LIFT-controller pretrained from imperfect rule-based\ndemonstrations can be refined within few hours to outper-\nform various rule and expert baselines by up to 70%. We also\nuse LIFT to learn task parallelism configurations on Heron\n[34], a state of the art stream processing engine.\nFigure 1 illustrates LIFT’s role and components. The slow\nquery log from a database containing queries, the executed\nquery plan, and execution statistics are read into LIFT. Via\na user-defined schema and converter, LIFT interprets traces\nand/or provided rules as demonstrations to train an offline\nmodel. In the indexing case study, this is achieved by map-\nping query shape and existing indices to a state, the command\nrequired to create the index used to an action, and query per-\nformance to a reward. Traces must hence contain not only\nruntime performance but also corresponding configurations\nwhich can be used to reconstruct a command (action) leading\nto that configuration. For example, the slow query log may\ncontain the query plan including index used, and this can\nbe converted to the command creating that index. Schema\nlayouts are passed to TensorForce to generate a correspond-\ning TensorFlow graph. The states, actions, and rewards are\nthen used to train a controller model to adopt the strategy\n(e.g. hand-designed rule or expert decision) behind prior in-\ndexing. Finally, LIFT is deployed in online mode to either\nrefine indexing on an existing query set, or within a new\napplication to replace manual tuning.2 BACKGROUND\nWe give a brief introduction to RL with focus on practi-\ncal concerns. RL is not a single optimization strategy but a\nclass of methods used to solve the reinforcement learning\nproblem. Informally, RL is utilized when no supervised feed-\nback for a decision is available but reward signals indicating\nrelative performance. For example, a cluster scheduler al-\nlocating tasks to resources may receive feedback from task\ncompletion times, but not whether a scheduling decision was\noptimal.\nWe consider the classic formulation wherein an agent in-\nteracts with an environment ϵdescribed by states s∈Sand\naims to learn a policy πthat governs which action a∈A to\ntake in each state [ 67]. At each discrete time step t, the agent\ntakes an action ataccording to its policy π(a|s), transitions\ninto a new state st+1according to the environment dynam-\nics, and observes a reward rtfrom a reward function R(s,a).\nThe goal of the agent is to maximize cumulative expected re-\nwards R=E[Í\ntγtrt], where future rewards are discounted\nbyγ. State transitions and rewards are often assumed to\nbe stochastic, and to satisfy the Markov property so each\ntransition only depends on the prior state st−1.\nIn data management tasks, the state is typically repre-\nsented as a combination of the current workload and config-\nuration, embedded into a continuous vector space. To deal\nwith the resulting large state spaces and generalize from\nseen to unseen states, RL is used in conjunction with value\nfunction approximators such as neural networks where the\nexpected cumulative return from taking an action ain state\nsis estimated by a function parametrized by trainable pa-\nrametersθ(i.e. the neural network weights). Formally, the\naction-value function Qπis given as\nQπ(s,a;θ)=E[Rt|st=s,a]. (1)\nThe goal of learning is to determine the optimal Q∗(s,a)\nwhich maximizes expected returns. Concretely, when using\nQ-learning based algorithms, the neural network produces in\nits final layer one output per action representing it Q-value.\nThe resulting policy is implicitly derived by greedily select-\ning the action with the highest Q-value while occasionally\n2\n\nselecting random actions for exploration. Updates are per-\nformed by performing iteratively (over a sequence indexed\nbyi) gradient descent on the loss J(θ)i[50]:\nJi(θ)i=Es,a∼π[(yi−Q(s,a;θi))2] (2)\nwithy=R(s,a)+γmax a′Q(s′,a′;θi−1). Intuitively, this loss\nis the (squared) difference between the observed reward\nwhen taking ainsplus the discounted estimate of future\nreturns from the new state s′, and the current estimate of\nQ(s,a;θ), or in other words how much the Q-function has\nto be modified to account for observing a new reward.\nIn Deep Q-learning as introduced by Mnih et al. [ 50], ex-\nperience tuples of the form (st,at,rt,st+1)are collected and\ninserted into a replay buffer, and updates are performed\nby sampling random batches to compute gradients. Further,\nlearning is stabilized by using a separate target network to\nevaluate the Q-target y, which is only synchronized with\nthe training network with delay. In contrast, policy gra-\ndient (PG) methods directly update a parametrized policy\nfunction pi(a|s;θ)such as a Gaussian or categorical distri-\nbution. This is typically (e.g. in the classical REINFORCE\nalgorithm [ 76]) achieved by obtaining a sample estimates of\ncurrent policy performance and updating θin the direction\n∇θloдπ(at|st;θ)(Rt−bt(st)). Detailed surveys of contempo-\nrary work are given by Li and Arulkumaran et al. [6, 37].\nRL approaches remain attractive due to their theoretical\nvalue proposition to learn from raw feedback. However, de-\nspite over two decades of research on RL in computer sys-\ntems, practical applications remain difficult to realize due\nto various limitations. In the following, we discuss concrete\nissues before introducing LIFT.\n3 PRACTICAL ISSUES\nRL algorithms are known to suffer from various limitations\nwhich we highlight here in the context of data management.\nTraining data requirements. First, RL methods are noto-\nriously sample-inefficient and solving common benchmark\ntasks (e.g. Atari) in simulators can require up to 107-109prob-\nlem interactions (states) when using recent approaches [ 16].\nIn data management experiments, performing a single step\n(e.g. a scheduling decision) and observing its impact may\ntake between seconds and hours (e.g. deciding on resources\nfor a job and evaluating its runtime). Consequently, train-\ning through online interaction can be impractical for some\ntasks, and training in production systems is further unde-\nsirable as initial behavior is random to explore. A common\nstrategy to accelerate training is to train RL agents in simu-\nlation [ 42,43]. This approach enables researchers to explore\nproof-of-concept experiments but also introduces the risk\nof making unrealistic assumptions and oversimplifying the\nproblem domain, thus making successful simulation-to-realtransfer unlikely. Some research domains have access to ver-\nified simulators (e.g. network protocols) but this is not the\ncase for many ad-hoc problems in data management.\nAnother common approach is to execute online training\non a staging environment or a smaller deployment of the\nsystem. For example, in their recent work on hierarchical\ndevice placement in TensorFlow [ 46], Mirhoseini et al. report\nthat training their placement mechanism on a small scale\ndeployment for 12.5 GPU-hours saves 265 GPU hours in\nsubsequent training of a neural network. Here, RL was used\nas a direct search mechanism where the aim of training is to\nidentify a single final configuration which is not modified\nlater. Successful online training is further difficult if the goal\nof the controller is to react to unpredictable and sudden\nworkload changes. This is because training workloads may\nnot sufficiently cover the state space to generalize to drastic\nworkload changes (while exploring the state space is usually\npossible in simulation).\nHyper-parameters and algorithmic stability. DRL algo-\nrithms require more configuration and hyper-parameter\ntuning than other machine learning approaches, as users\nneed to tune neural network hyper-parameters, design of\nstates/actions and rewards, and parameters of the reinforce-\nment learning algorithm itself. A growing body of work in\nDRL attempts to address algorithmic limitations by more effi-\nciently re-using training data, reducing variance of gradient\nestimates, and parallelizing training (especially in simula-\ntions) [ 16,23,61,62]. Some of these efforts have recently\nreceived scrutiny as they have been shown difficult to re-\nproduce [ 26,41], often due to the introduction of various\nadditional hyper-parameters which again need to be tuned.\nThis is complicated by the fact that RL algorithms are of-\nten evaluated on the task they were trained on (i.e. testing\nperformance on the game the algorithm was trained on). RL\nis effectively used for optimization on a single task, and, as\nMania et al. argue [ 41], some algorithmic improvements in\nrecent work may stem from overfitting rather than funda-\nmental improvements.\nSoftware tools. The reproducibility issues of RL algorithms\nare further exacerbated by a lack of standard tools.The prac-\ntical successes of neural networks in diverse domains have\nled to the existence of widely adopted deep learning frame-\nworks such as Google’s TensorFlow [ 2], Microsoft’s CNTK\n[63], or Apache MXnet [ 12]. These libraries provide common\noperators for implementing and executing machine learning\nalgorithms while also omitting the complexity of directly\ninterfacing hardware accelerators (e.g. GPUs, FPGAs, ASICs).\nHowever, RL algorithms cannot be used with similar ease as\nexisting research code bases primarily focus on simulation\nenvironments, and thus require significant modifications to\nbe used in practical applications. We introduce our RL library\nbuilt on top of TensorFlow in section §4.2.\n3\n\nT ensorForce\nT ensorFlowLIFTAgent-system interaction,  \ndata/model layout  \nmodel lifecycle management\n(distributed)\nCPU/GPU/TPURL agent API,  \npre-implemented algorithms,\nmodular RL components  \nNumerical operations,   \nNN layers, optimisers  \ngraph compilation,  \nexecution  \nCUDA kernels,   \nEigen,  \nC/C++ op kernels  Component AbstractionsFigure 2: LIFT stack for applied RL.\nThe issues above continue to present significant obstacles\nin using RL. We investigate means to improve data efficiency\nand tooling by providing a software stack for deep RL focused\non initializing controllers from pre-existing knowledge.\n4 LIFT\n4.1 System overview\nWe begin by giving a high level overview of our framework\nbefore discussing each component in detail. Generally, we\ndistinguish between our algorithmic backend TensorForce ,\nandLIFT, a collection of services which allow RL controllers\nto be deployed in different execution contexts, which we\nexplain below (Figure 2). Frameworks such as TensorFlow [ 1]\nexpose an API primarily on the abstraction level of numerical\noperators with an increasing number of modules containing\nneural network layers, optimizers, probability distributions,\ndata set tools etc. However, currently no such modules exist\nwithin TensorFlow to expose RL functionality via similar\nAPIs. TensorForce fills this gap by providing a unified API\nto a set of standard RL algorithms on top of TensorFlow.\nThe main abstractions LIFT operates on are RL models and\nsystem models. A model maps between RL agent output to\nsystem actions (e.g. configuration changes), or from system\nmetrics to RL agent (e.g. parsing log entries to states, actions\nand rewards). LIFT’s primary purpose is to facilitate RL usage\nin new systems by providing commonly used functionality\npertaining to model serialization and evaluation, and further\nby defining system data layout and automatically mapping\nthem to the respective TensorFlow inputs and outputs. LIFT\nuses TensorForce as its backend in our example implementa-\ntion but is independent of both TensorForce and TensorFlow,\nas to be able to use any RL implementation providing a min-\nimal common API. In the following, we discuss the design\nof TensorForce.\n4.2 TensorForce\nDeep reinforcement learning is a rapidly evolving field and\nfew standards exist with regard to usage outside controlled\nsimulations. Various open source libraries such as OpenAIbaselines [ 65], Nervana coach [ 10], or Ray Rllib [ 38] exist.\nThey are tightly coupled with simulation environments such\nas OpenAI gym [ 9] which provide unified interfaces to tasks\nfor evaluating and comparing algorithms. In our experiments,\nwe have found these research frameworks to be difficult to\ndeploy in practical use cases for two additional reasons.\nFirst, open source reinforcement learning libraries fre-\nquently rely on fixed neural network architectures. For exam-\nple, the code we analyzed typically created network output\nlayers for actions based on descriptors provided by simu-\nlations only supporting restricted actions (e.g. only either\ndiscrete or continuous actions per step, but not both). Sub-\nstantial code modifications are required to support multiple\nseparate types of actions (tasks) per step. This is because the\npurpose of these reference implementations is primarily to\nreproduce research results on a particular set of benchmark\ntasks, as opposed to providing configurable, generic models.\nSecond, as discussed in §3, recent RL methods incorporate\nvarious optimization heuristics to help training efficiency and\nstability, thus increasing the number of tunable parameters.\nWe found existing code bases to attempt reducing complexity\nby hard-coding heuristics of which users may be unaware.\nFor example, one of the implementations we surveyed inter-\nnally smoothes state vectors via an exponentially moving\naverage, and clips reward values without documenting or\nexposing this feature. We hence introduce TensorForce, a\ngeneral purpose DRL library which exposes a well-defined\ndeclarative interface to creating and transparently configur-\ning state-of-the art algorithms.\nDesign. Our aim is to give a unified interface to specify a\ndecision model by describing its inputs and outputs without\nany restriction on the number and type of different inputs\n(states) or outputs (actions). Further, the specification con-\ntains the model to construct, network layers to use, and\nvarious further options to be applied such as exploration, in-\nput preprocessing (e.g. normalization or down-sampling) and\noutpost post-processing (e.g. noise), and algorithm-specific\noptions such as memory size.\nTensorForce is built on two principles: First, users should\nnot be required to modify any library code to express their\nproblem dynamics, as is often the case in current open source\ncode, thus necessitating expressive configurations. Second,\nreinforcement learning use cases may drastically differ in\ndesign, e.g. environments may present continuous learning\nor episodic problems, algorithms may use memories to incor-\nporate old experiences, or just learn from new observations.\nHowever, most of this arising complexity can be determinis-\ntically (depending on the model selected) handled internally.\nConsequently, we provide a unified API for all model and\nagent variants with just two methods at its core, one to re-\nquest new actions for given states, one to observe rewards\n4\n\nfrom tensorforce.agents import PPOAgent\n# Create a Proximal Policy Optimization agent\nagent = PPOAgent(\nstates= dict (type ='float ', shape=(10,)),\nactions= dict (\ndiscrete_action= dict (type ='int', num_actions=10),\nbinary_action= dict (type ='bool ')\n),\nnetwork=[\ndict (type ='dense ', size=64),\ndict (type ='dense ', size=64)\n],\nstep_optimizer= dict (\ntype ='adam ',\nlearning_rate=1e-4\n),\nexecution= dict (type ='single '),\nstates_preprocessing= [ dict (type ='running_standardize ')]\n)\n// Connect to a client\nclient = DBClient(host= 'localhost ',port=8080)\nwhile True:\n# Poll client for new state, get prediction, execute\naction = agent.act(state=client.get_state())\nreward = client.execute(action)\n# Observe feedback\nagent.observe(reward=reward, terminal=False)\nListing 1: Agent API example\nand notify the model of terminal states. Updates to the model\nare implicitly triggered according to configurations.\nThe advantage to our approach is that practitioners can\nexplore different RL paradigms in their applications sim-\nply by loading another configuration without the need to\nmodify application code (e.g. to explicitly trigger certain up-\ndates or model-specific events), or library code. The code is\navailable open source under https://github.com/reinforceio/\ntensorforce.\nFeatures. TensorForce implements both classical algorithms\nserving as an entry point for practitioners as well as newer\nmethods, which we briefly describe. From the family of Q-\nlearning algorithms, our library implements the original deep\nQ-learning [ 50], double deep Q-learning [ 74], normalized\nadvantage functions for continuous Q-learning [22], n-step\nQ-learning [ 48], and deep Q learning from demonstrations\nincorporating expert knowledge [27].\nFurther, we provide classic policy gradients (REINFORCE)\n[76], trust region policy optimization [ 61], and proximal\npolicy optimization (PPO) [ 62] from the spectrum of policy-\nbased methods, which all support categorical, continuous\nand bounded action spaces. It is worth pointing out that\nmany new algorithms only modify classic Q-learning or\npolicy gradients by slightly changing the loss functions, and\nimplementing them only requires a few lines of code on top\nof existing TensorForce components.Example usage. We illustrate how users might interact with\nthe API in Listing 1. Developers specify a configuration con-\ntaining at least a network specification and a description\nof states and action formats. Here, a single state with 10\ninputs and two separate actions per step, one boolean, one\ndiscrete with 10 options are required. Single-node execution\nis chosen, and incoming states are normalized via a state\npreprocessor. Crucially, a large number of commonly used\nheuristics is both optional and transparently configurable.\nNext, a PPO (a state-of-the-art policy optimization method,\ne.g. used in OpenAI’s recent work on DOTA [ 52]) agent is\ncreated using the configuration, and a client is instantiated\nto interact with an example remote system which we desire\nto control. The agent can now be used by retrieving new\nstate signals from the client, which needs to map system\nstate (e.g. load) to inputs, and requesting actions from the\nagent. The client must implement these actions by mapping\nnumerical representations such as the index of a discrete\naction to a change in the system. Finally, the agent has to\nobserve the reward to provide feedback to the agent. The\nagent will automatically trigger updates to the underlying\nTensorFlow graph based on algorithm semantics, e.g. episode\nbased, batch-based, or time-step based.\nDevelopers are thus freed from dealing with low-level\nsemantics of deep learning frameworks and can concentrate\non mapping their system to inputs, rewards and actions. By\nchanging a few lines in the configuration, algorithm, data\ncollection, learning, or neural network logic can be fine-\ntuned. Finally, the JSON configurations can be conveniently\npassed to auto-tuners for hyper-parameter optimization.\n4.3 LIFT\nLIFT uses the declarative agent API and a small set of reusable\ncomponents to realize three different execution modes which\nwe describe in this section.\nPretraining. In pretraining mode, LIFT does not interact\nwith a system but is provided with a trace data source such\nas a comma separated file, a database table, or a distributed\nfile system. LIFT parses and maps these to demonstrations\n(described in detail in section 5), creates an RL agent sup-\nporting pretraining, and imports data. It then executes and\nmonitors pretraining through evaluators, i.e. by validating\nmodel performance, and finally by serializing the model.\nAgent-driven. In agent-driven or active execution , LIFT\nalternates between interacting with the system (i.e. the envi-\nronment) and the RL agent via the TensorForce API. Here,\nexecution time is almost exclusively governed by waiting\non the environment, as we show in §6. The RL libraries we\nsurveyed typically only offer agent-driven execution (e.g.\nOpenAI baselines) where this execution is tightly coupled\nwith reinforcement learning logic. This is because training\n5\n\ncommon simulation tasks such as the Arcade Learning En-\nvironment [ 7] can be effectively parallelized to hundreds\nof instances due to marginal computational requirements\nper simulator process. These highly parallel training proce-\ndures are economically impractical for users without data\ncenter scale resources, as learning to control data processing\nsystems requires significant I/O and compute.\nEnvironment-driven. In environment-driven execution or\npassive execution , LIFT acts as a passive service as control\nflow is driven by external workload, e.g. a benchmark suite\nexecuted against a database. For example, LIFT may open up\na websocket or RPC connection to a monitoring service to\nreceive real-time performance metrics. The LIFT controller\nthen continuously maps incoming metrics to states, passes\nthem to the agent, and executes the necessary configura-\ntion changes on the system. Passive execution is primarily\nintended for deployment of trained models which can op-\ntionally perform incremental updates. All execution modes\nshare a common set of components which users need to im-\nplement for their given system to facilitate the parsing and\nserialization overhead necessary to interface a system.\nFirst, a schema is used to programmatically construct the\nlayouts of states, actions and rewards. For example, in our\ncompound indexing case study, the input size to the neural\nnetwork depends on the number of available query operators\nand unique fields in the database. In our experience, success-\nful application of RL initially requires frequent exploratory\niterations over different state and action layouts. In LIFT, this\nis reflected by users implementing multiple exchangeable\nschemas. Downstream components for the execution modes\nuse a schema to infer shape and type information.\nNext, users implement a model converter as the central\ncomponent for translating between RL model and controlled\nsystem via a small set of methods called throughout LIFT to\ni) map system output to agent states and agent actions (for\npretraining), ii) map system output to rewards, and iii) map\nagent output to system configuration changes. LIFT’s generic\ncomponents for each execution mode then use converters to\ndeserialize and parse log traces, and to perform offline (pre-\ntraining) and online (agent- or environment-driven) training.\nWe summarize the idea behind LIFT as motivated by two\nobservations. First, unlike common RL simulation tasks, con-\ntrolling data processing systems requires separation of en-\nvironment and RL agent due to different resource needs\nand communication patterns (e.g. access to system metrics\nthrough RPC or other protocols). Second, using RL in prac-\ntical contexts currently requires a large amount of boiler-\nplate code as no standard tools are available. LIFT enables\nresearchers to focus on understanding their state, action and\nreward semantics and express them in a schema and system\nmodel, which generate the respective TensorFlow graphs viathe TensorForce API. In the following section, we explain\nthe pretraining process on the indexing case study.\nImplementation. We implemented our LIFT prototype in\n≈10000 lines of Python code which includes components for\nour example case studies. In this work, no low-latency access\nis required (e.g. for learning to represent data structures as\ndescribed by Kraska et al. [ 33]) but we may implement a C++\nserving layer in future case studies.\n5 LEARNING FROM TRACES\n5.1 Problem setup\nWe now illustrate the use of LIFT in an end-to-end example\nbased on our compound database indexing application. In\ndatabase management, effective query indexing strategies\nare crucial for meeting performance objectives. Index data\nstructures can accelerate query execution times by multiple\nmagnitudes by providing fast look-ups for specific query op-\nerators such as range comparisons (B-trees) or exist queries\n(Bloom filters). A single index can span multiple attributes,\nand query planners employ a wide range of heuristics to\ncombine existing indices at runtime, e.g. by partial evalu-\nation of a compound (multi-attribute) index. Determining\noptimal indices is complicated by space usage, maintenance\ncost, and the fact that indexing decisions cannot be made\nindependently of runtime statistics, as index performance de-\npends on attribute cardinality and workload distribution. In\npractice, indices are identified using various techniques rang-\ning from offline tool-assisted analysis [ 3,11,14] to online\nand adaptive indexing strategies [ 21,24,29,55]. Managed\ndatabase-as-a-service (DBaaS) offerings sometimes offer a\nhybrid approach where indices for individual attributes are\nautomatically created but users need to manually create com-\npound indices.\nWe study MongoDB as a popular open source document\ndatabase where data is organized as nested J/BSON docu-\nments. While a large body of work exists on adaptive index-\ning strategies for relational databases and columnar stores\n[55], compound indexing in document databases has received\nless attention. Document databases are offered by all major\ncloud service providers, e.g. Microsoft’s Azure CosmosDB\noffers native MongoDB support [ 44], Amazon’s AWS offers\nDynamoDB [ 4], and Google Cloud provides Cloud Datas-\ntore [ 20]. The document database services we surveyed offer\nvarying specialized query operators, index design, and query\nplanners using different indexing heuristics. The aim of auto-\nmatic online index selection is to omit this operational task\nfrom service users. We initially focus on common query op-\nerators available in most query dialects, as we plan to extend\nour work to other database layouts and query languages.\nTable 1 gives an operator overview. In MongoDB, queries\nthemselves are nested documents.\n6\n\nTable 1: MongoDB basic operator overview.\nOperators MongoDB operator\n=,>,≥,<,≤, not in $eq,$дt,$дte,$lt,$lte,$nin\nand, or, nor, not $and,$or,$nor,$not\nlimit, sort, count count(),limit(n),sort(keys)\n5.2 Modeling indexing decisions\nThe MongoDB query planner uses a single index per query\nwith the exception of $orexpressions where each sub-\nexpression can use a separate index. An index may span\nbetween 1andkschema fields and is specified via an or-\ndered sequence of tuples (f1,s1), ..,(fn,sn)where each tuple\nconsists of a field name fiand a sort direction si(ascending\nor descending). At runtime, the optimizer will use a number\nof heuristics to determine the best index to use.\nVia index intersection, the optimizer can also partially\nutilize existing indices to resolve queries. For example, pre-\nfix intersection means that for any index sequence of length\nk, the optimizer can also use any ordered prefix of length\n1..k−1to resolve queries which do not contain all kattributes\nin the full index. Consequently, while the tuple ordering of\nthe index does not typically matter for individual queries,\nthe number of indices for the entire query set can be dras-\ntically reduced if index creation considers potential prefix\nintersections with other queries. Similarly, sort-ordering in\nindices can be used to sort query results via sort intersec-\ntion in case of matching sort patterns. For example, an index\nof the shape[(f1,ASC),(f2,DESC)]can be used to sort as-\ncending/descending and descending/ascending (i.e. inverted)\nsort patterns, but not ascending/ascending or descending/de-\nscending. Based on these indexing rules, we define the fol-\nlowing state, action, and reward model.\nStates. Identifying the correct index for a query requires\nknowledge of the query shape, e.g. its operators and re-\nquested attributes. To leverage intersection, the state must\nalso contain information on existing indices which could\nbe used to evaluate a query. We parse queries via a tree-\nwalk, strip concrete values from each sub-expression, and\nonly retain a sequence of operators and attributes. If an in-\ndex already exists on an attribute, we insert an additional\ntoken after the respective attribute to enable the agent to\nlearn about index intersection and avoid adding unnecessary\nindices. For example, consider the simple following query\ncounting entries with name \"Jane\":\ncollection.find({$eq: {name: \"Jane\"}}).count()\nAssuming an ascending index on the name field already\nexists, the tokenized query looks as follows (with EOS repre-\nsenting the end-of-sentence):[$eq name IDX_ASC count EOS]\nThese tokens are then converted to integers using a word\nembedding as commonly used in natural language processing\napplications to map a discrete set of words to a continuous\nvector space [ 45]. In practice, a maximum fixed input length\nis assumed and shorter inputs are padded with zeros.\nActions. For every query we seek to output an index (or\nnone) spanning at most kattributes where kis a small num-\nber as indices covering more than 2-4 attributes are rare in\npractice. This is also because compound indices containing\narrays, which require multi-key indices (each array element\nindexed separately), scale poorly and can slow down queries.\nAdditionally, as discussed above, index intersection makes in-\ndices order- and sort-sensitive, thus requiring to also output\na sort order per attribute in a multi-key index.\nThe action scheme should scale independently of the num-\nber of attributes in the document schema. Consider a com-\nbinatorial action model where the agent is modelled with\none explicit action per attribute, and a separate action out-\nput per possible index-key. A 3-key index task on 10 at-\ntributes would already result in thousands of action options\nper step ( 103∗3=3000 ) when including an extra action\nfor the three possible sort patterns (both ascending/descend-\ning, descending-ascending, ascending-descending). This ap-\nproach would not generalize to changing schemas or data\nsets. We propose a positional action model wherein the num-\nber of actions is linear in k. When receiving a query, we\nextract all query attributes and interpret an integer action\nas creating an index on the ithinput attribute, thus allowing\nthe agent to learn the importance of key-order for prefix\nintersection. To distinguish sort patterns, we create an extra\naction per key (one ascending, one descending with ascend-\ning default). This results in 1+2kactions for a k-key index\nwith one output for no-op.\nFigure 3 illustrates state and action parsing for k=2and a\nsimple query on name andageattributes. In the example, the\nname field is already indexed so when the query is tokenized,\na special index token ( IDX_ASC) is inserted to indicate the\nexisting index. The tokenized sequence is mapped to integers\nvia the embedding layer and passed through the network,\nwhich outputs kinteger actions. In the example, the agent\ndecides to implement one additional single-key index by\noutputting 3 and 0, where 3 implies an ascending index on\nthe second input attribute, and 0 is used for no-op if fewer\nthan kkeys are required in the index.\nRewards. The optimal indexing strategy is the minimal set\nof indices Imeeting performance level objectives such as\nmean latency or 90th and 99th latency percentiles for a set\nof queriesQ. Let t(q)be the time to execute a query q∈Q\nunder an index set Iand let m(I)be the memory usage of\nthe current index set. We set the reward r(q)as the negative\n7\n\nﬁnd({$and: [{name: {$eq: 'jane'}, {age: {$gt: 20}}]}).count()  Query:\nIndex-\ncontext:[(name, 1)]\nTokenizer:\nAgent-\nactions:[$and name_1 IDX_ASC $eq age $gt AGG_COUNT .. 0]  \nForward  \npass  Strip values, extract shape\nLook up discrete indices  \nin vocabulary\n3One action output  \nper index attribute\n0\nSystem-  \nactions:  [(age, 1)]3: Ascending index on second input attribute  \n0: no-op, no further keys requiredFigure 3: State and action parsing scheme for the in-\ndexing case study.\nweighted combination of these to allow expressing trade-offs\non memory usage against runtime requirements:\nr(q)=−ω1m(I)−ω2t(q).\n5.3 Demonstrations and Pretraining\nWe now describe the ideas behind learning from demon-\nstrations as used in LIFT. Our approach is motivated by the\nobservation that a human systems developer encountering a\ntuning problem can frequently use their expertise to come up\nwith an initial heuristic. For example, in the indexing prob-\nlem, a database expert can typically determine an effective\nconfiguration for a given application within a reasonable\ntime frame (e.g. a few hours) with access to profiling tools.\nDistilling this intuitive expertise into a fully automated ap-\nproach is difficult, and simple heuristics may perform well in\nsmall scenarios but fail at scale. Moreover, as discussed in §3,\ntraining a RL model from scratch is expensive and difficult,\nwhile refining a model pretrained from not necessarily fully\ncorrect demonstrations may be more effective. We hence\nargue for an approach that leverages pre-existing domain\nknowledge by initializing training from demonstrations.\nDemonstration data. In the indexing task, demonstrations\nmay exist in the form of:\n(1)Query logs from applications configured by a database\nadministrator where indices are assumed to be correct,\nwhere correctness implies fully meeting service level\nobjectives (not necessarily being optimal).(2)Query logs from applications where indices were cre-\nated using any heuristic understood to be sub-optimal\nand not necessarily meeting service objectives.\n(3)Queries and index pairs for which no runtime data is\navailable, e.g. procedurally generated examples with\neither manually or heuristically chosen index recom-\nmendations (both correct and imperfect).\nThe key difference between (1) and (2) is that when encoun-\ntering a query for which an imperfect demonstration was\navailable during pre-training, we do not mind testing other\nchoices while this is unnecessary if a demonstration was op-\ntimal for the query given. This confidence must be reflected\nin the pretraining procedure. Further, the difference between\n(1), (2) and (3) is that in the latter, no reward is available\nwithout creating indices and measuring queries. Note that\nthe key difference between demonstrations and simulation\nin our applications is the absence of information on system\ndynamics (i.e. state transitions).\nA simulator for query indexing would provide insights into\nhow addition and removal of an index affects performance.\nIn contrast, a demonstration extracted from the slow query\nlog of a database indicates how fast a query performed using\nthe index chosen by the query planner, but not how much\nfaster the index was versus not using an index, or a different\nindex. We make use of all demonstration types but focus\non (2) and (3), as we could not obtain existing traces from\nexpert-configured systems and thus had to manually tune\nconfigurations.\nAlgorithm. Hester et al. have described an algorithm to\nperform Deep Q-learning from such expert demonstrations\n(DQfD) using the example of Atari games [ 27]. In their work,\nan agent is trained until sufficient performance, and then\ngames played by that agent are given as demonstrations to a\nnew agent. DQFD works by assigning an ’expert margin’ to\ndemonstration actions by extending double Q-learning [ 74],\na Q-learning variant which corrects biased Q- estimates in\nthe original DQN by decoupling action selection and action\nevaluation. Specifically, the double DQN loss\nJDQ(Q)=(R(s,a)+γQ(st+1,amax\nt+1;θ′)−Q(s,a;θ))2(3)\nwhere\namax\nt+1=arдmax aQ(st+1,a;θ) (4)\nuses the target network (as explained in §2, parametrized\nbyθ′)) to evaluate the action selected using the training\nnetwork (with parameters θ). This is combined with another\nexpert loss function JE:\nJE(Q)=max\na∈A[Q(s,a)+l(s,aE,a)]−Q(s,aE) (5)\nHere, l(s,aE,a)is a function which outputs 0 for the expert\naction, and a margin value >0otherwise. We convey the\n8\n\nintuition of this loss function by recalling the action selection\nmechanism in Q-learning.\nRecall that Q(s,a,θ)=E[Rt|st=s,a], i.e. the expected\nreturns from taking a decision ain state s. At each step, the\nneural network (parameterized by θ) used to approximate\nQoutputs Q-values for all available actions and selects the\naction with the highest Q-value. By adding the expert margin\nto the loss of Q-values of incorrect actions , the agent is biased\ntowards the expert actions as a difference between expert ac-\ntions and other actions of at least the margin is enforced [ 56].\nThe DQFD-agent keeps a separate memory of these expert\ndemonstrations which are first used to pretrain the agent,\nthen combined with new online experiences at runtime so\nthat the agent keeps being ’reminded’ of demonstrations.\nWhat does the choice of l(s,aE,a)imply for imperfect or\nnoisy demonstrations? A large margin makes it difficult to\nlearn about any better actions in a given state because even\nif, via exploration, a different action is selected and yields a\nhigher return, an update may not change Q-values of better\naction beyond the margin. Second, the DQfD loss only en-\nforces a difference in Q-values between demonstrated action\nand all other actions; no assumptions are made about the\nrelationship between non-expert actions (e.g. second high-\nest, third highest Q-value). This behavior is desirable in the\nindexing example because even semantically similar indices\n(e.g. different order, partially covering same fields) can result\nin much worse performance than the demonstrated index,\nso we initially do not want to express any preference on non-\ndemonstrated indices. Consequently, we choose a very small\nmargin≤0.1which in practice results in a pre-generated\nmodel which initially only slightly favors the demonstrated\naction.\n5.4 Putting it all together.\nAlgorithm 1 shows pseudo-code for the online training pro-\ncedure. Following pre-training on the demonstration data\nset, we start LIFT in online mode, initialize an agent with\nthe demo model, and load the demo data. We then begin the\nepisodic training procedure on a new set of queries Qtest\nwe want to index. In each training episode, all indices are\nfirst removed from the database. Then, each query q(sorted\nby length to improve intersection) is tokenized and the sug-\ngested index created. Recall that the tokenization includes\nthe current index set Ifor the agent to learn the impact\nof existing indices. The size of the index set m(I)and the\nruntime of the query t(q)are used to inform the reward of\nthe agent. For direct search tasks like indexing, we keep the\nlist of index tuples associated with the highest reward during\ntraining. In the final evaluation, we recreate these indices\nand then run all queries 5 times on the full index set. ForAlgorithm 1 Online training procedure.\nInitialize aдentwith demo-model and demo-data D\nInitialize LIFT system _model ,model _converter\nLoad application queries Qtest\n// Fixed time budget or until objectives met\nfori=1,Ndo\nItest←∅, clear index set in DB\nforqinQtestdo\n// Tokenize, include existing indices\ns(q)←model _converter .to_aдent_state(q,Itest)\nindex←aдent.act(s(q))\n// Create index, execute query\nm(Itest)←system _model .act(index)\nt(q)←system _model .execute(q)\n// Compute reward from runtime and size\nrq←−ω1m(Itest)−ω2t(q)\naдent.observe(rq)\nAddindex toItest\nend for\nend for\n// Final evaluation, create best Itest:\n// Measure final size m(Itest), runQtest\ndynamic tasks where the agent is invoked repeatedly at run-\ntime, we simply export the trained model which can then be\nused to control a system.\n6 EVALUATION\n6.1 Aims\nWe evaluate our LIFT prototype through two case studies: 1)\nthe indexing case study in which we minimize latency and\nmemory usage by learning compound index combinations,\nand 2) the stream processing resource management case\nstudy in which we tune latency by setting parallelism levels\nunder a varying workload. In both case studies, we used LIFT\nto implement a controller, manage demonstration data, and\ninteract with the system. The difference is that the index-\ning task is an offline optimization (index set is determined\nonce, then deployed), while in the stream processing task\nwe use a controller at runtime to react to varying workloads.\nThe evaluation focuses on evaluating the utility of LIFT and\nTensorForce to solve data management tasks, and on un-\nderstanding the impact of learning from demonstrations to\novercome long training times.\n6.2 Compound indexing\nSetup. We evaluate the indexing task both on a real-world\ndataset (IMDB [ 30]) and using synthetic queries and data.\nThe synthetic query client is based on the YCSB benchmark\n[13]. YCSB generates keys and synthetic values to evaluate\n9\n\nPretrain\n+OnlineOnline Default Full Human Pretrain01234Mean latency (s)(a) Mean latency.\nPretrain\n+OnlineOnline Default Full Human Pretrain0246890th percentile latency (s) (b) 90th pct. latency.\nPretrain\n+OnlineOnline Default Full Human Pretrain0246899th percentile latency (s) (c) 99th pct. latency.\nPretrain\n+OnlineOnline Default Full Human Pretrain0.00.20.40.60.81.0Space used versus full index (d) Normalized index size.\n100 200 300 400 500 600 700 800 900 1000\nTraining steps0.00.20.40.60.81.0Accuracy\nTraining accuracy\non demonstrations\n(e) Pretraining accuracy.\n0 20 40 60 80 100\nOnline test episode−70−60−50−40Rewards\nPretrain+Online\nOnline (f) Online adaption rewards.\n0 5 10 15 20\nQueries sorted by latency10−410−310−210−1100101Latency (s)Online\nPretrain+Online (g) Per-query latency.\n0 5 10 15 20\nQueries sorted by latency0246Index-keys (k) per queryOnline Pretrain+Online (h) Per query index keys.\nFigure 4: Performance evaluation on the IMDB data set.\ncloud database performance via a set of common workload\nmixtures but has no provisions for complex queries. We\nimplemented a YCSB-style client and workload generator\ntargeting secondary indexing. The client is configured with a\nschema containing attribute names and types. The workload\ngenerator receives a query configuration containing valid\nquery operators, maximum allowed operator degrees, query\nheight, and distribution of aggregation operators. It can then\ngenerate a specified number of queries index suggestions\nbased on provided rules. All experiments were run on a\nvariety of commodity server class machines (e.g. 24 cores\n(Xeon E5-2600) and 198 GB RAM) and using MongoDB v3.6.4.\nQueries and demonstrations are imported into LIFT’s\npretrain-controller which instantiates a DQfD agent and\nparses queries and demonstrations to states and actions as\ndescribed before. We then run a small number of pretraining\nsteps until the agent has approximately adopted the rule.\nWe use batch sizes of 32 queries on a neural network with 1\nembedding layer and 1 dense layer with 128 neurons each.\nLearning is executed using an adaptive moment optimizer\n(Adam [ 32]) with a learning rate of 0.0005 , and an expert\nmargin of 0.1. To refine the pretrained model, we restart LIFT\nin online mode and train as described in Algorithm 1. The\nmax number of attributes per index was k=3.\nIndexing baselines. We consider human and rule-based\nbaselines due to a lack of standard tools for automatic index-\ning in document stores. The first rule-based strategy we use\nto generate demonstrations is full indexing ( Fullin the follow-\ning) wherein we simply create a compound index covering\nall fields in a query (respecting its sort order), thus ensur-\ning an index exists for every query. In the synthetic regime,where query shapes are re-sampled every experiment to eval-\nuate generalization to different query sets, human baselines\nwere uneconomical, and we experimented with other rule-\nbased baselines. Partial indexing ( Partial hereafter) attempts\nto avoid unnecessary indices by considering existing indices\non any attribute in a query, and only covering unindexed\nfields. Note that we do not claim these to be the most ef-\nfective heuristics but merely initial guidance for a model.\nWe also experimented with a rule based on operator and\nschema hints but this frequently did not perform well due to\nunforeseen edge cases. We refer to the following modes in\nthe evaluation: no indexing ( Default ), online learning from\nscratch without pretraining ( Online ), pretraining without\nonline refinement ( Pretrain , online learning following pre-\ntraining ( Pretrain+Online ), human expert ( Human and the\ntwo baselines described above.\nBasic behaviour. We first show results on the publicly avail-\nable internet movie database (IMDB) datasets [ 30]. We im-\nported datasets for titles and ratings ( title.akas ,title.basics ,\ntitle.ratings ) comprising≈10 million documents. We manu-\nally defined a representative set of 20 queries such as ’How\nmany comedies with length of 90 minutes or less were made\nbefore 2000?’. For this fixed set, we compared our method\nto human expert intuition. Using human baselines (which\nare common in deep learning tasks) in data management is\ndifficult due to inherent bias and prior knowledge on experi-\nment design. Generally, a human expert can identify effective\nindices for a small query set given unlimited trials to refine\nguesses. For a more interesting comparison, we hence de-\nvised a single-pass experiment where the expert was allowed\nto observe runtimes on the full indexing baseline and subse-\nquently tried to estimate an index per query. Figures 4a, 4b\n10\n\n104105106107108\nDocuments in collection0200400600Index creation time (s)Single key (k=1)\nCompound k=2\nCompound k=3(a) Index creation times.\nPretrain\n+ OnlineOnline Default Full Partial Pretrain0.00.20.40.60.81.0Space used versus full index (b) Index size in scale-up.\nPretrain\n+ OnlineOnline Default Full Partial Pretrain010203040Mean latency (s) (c) Test set mean latencies.\nPretrain\n+ OnlineOnline Default Full Partial Pretrain02040608010090th percentile latency (s) (d) Test set 90th pctl. latencies.\nFigure 5: Scalability generalization analysis using the synthetic query client. Learning was performed on 10 mil-\nlion documents, a new set of test queries was evaluated on 100 million documents.\nand 4c give mean, 90th and 99th latencies respectively on the\nfinal evaluation in which each query is executed 5 times (final\nresults averaged over five trainings with different random\nseeds). The combined Pretrain+Online strategy outperforms\nother methods significantly, in particular improving mean\nlatency by 57%and62%against FullandHuman respectively\nand by 74%on 99th percentile latency against both. Differ-\nences in 90th percentile latencies were within standard error.\nIn the human experiment, the expert attempted to reduce\nindices by leveraging intersection, creating only 14 indices\nversus 15 on average for Pretrain+Online . The size of the cre-\nated indices was however (marginally) bigger compared to\nPretrain+Online (as shown in Figure 4d) which achieved on\naverage 25%index size improvement against the Fullstrategy.\nNote that MongoDB always creates a default index on the _id\nattribute so the default size is not zero. We normalize sizes\nagainst the size of the full index to evaluate improvement.\nThe expert’s attempt to exploit intersection also underesti-\nmated the necessity of compound indices for some queries.\nThe outcome illustrates the difficulty of solving the task\nwithout iterative manual analysis.\nPretrain andOnline can perform similar to full indexing.\nThe performance of Pretrain (in its degree of similarity to\nFull) depends on whether pretraining is continued until the\nrule is fully adopted. We found early stopping at 70−80%\naccuracy to be effective when using our imperfect rules to\navoid overfitting (Figure 4e. Online can sometimes find good\nconfigurations but tends to perform significantly worse than\nPretrain+Online in mean reward due to random initialization,\nas seen in Figure 4f which shows reward curves (i.e. com-\nbined size and latency) and 1 σconfidence intervals over\n5 runs. We breakdown individual queries and indices for\nPretrain+Online andOnline , i.e. standard RL. In Figure 4g, we\nsort queries of one experiment by latency and show runtime\ndifferences (n.b. log scale), and in figure 4h the number of\nkeys in the index decision (0-3) for the query (stacked on\ntop of each other). Performance differences are concentrated\nin the five slowest queries with the rest being effectively in-\ndexed by both strategies. Comparing keys used per query alsoWorkload means Total time Pct.\nWaiting on system 64869 s ( ±4403 s) 97.8 %\nAgent interaction/evaluation 1446 s ( ±218 s) 2.2 %\nMean episode duration 663 s ( ±42 s) n/a\nMin episode duration 419 s ( ±88 s) n/a\nMax episode duration 880 s ( ±62 s) n/a\nPretrain+Online time to max 43044 s ( ±16106 s) n/a\nOnline time to max 19088 s ( ±15011 s) n/a\nTable 2: Wall clock times on the IMDB data set. One\nepisode refers to creating the entire application index\nset. On average, Pretrain+Online reaches its max per-\nformance much later in the experiment as it keeps im-\nproving while Online stops improving early.\nshows that Pretrain+Online created indices systematically\nspanning fewer keys than Online . This does not necessarily\nimply smaller total index size depending on the attributes\nindexed but indicates more effective intersection.\nTiming. Next, we analyze time spent in different training\nphases. Due to small neural network size, pretraining could\nbe comfortably performed within few minutes on a CPU.\nThis includes intermediate evaluations to test accuracy of\nthe model on the set of rule-based demonstrations, and iden-\ntifying conflicting rules. In table 2, we break down time spent\ninteracting with TensorForce for requesting actions/perform-\ning updates, and time spent waiting on the environment and\nevaluating indexing decisions by running queries. 97.9%of\ntime was spent waiting on the database to finish creating and\nremoving indices, and only 2.1%was spent on fetching and\nevaluating actions/queries, and updating the RL model. Pre-\ntraining is negligible compared to online evaluation times,\nso pretraining is desirable if data is available. If LIFT is used\nfor online training, employing pretraining only requires few\nextra converter methods.\nScalability. The indexing problem is complicated by step\ndurations growing with problem scale. Figure 5a shows index\ncreation times for increasing collection sizes. At 100 million\ndocuments generated from our synthetic schema, creating\n11\n\nan index set for a set of queries can take hours, resulting in\nweeks of online training. As RL algorithms struggle with data\nefficiency, we believe these scalability problems will continue\nto present obstacles for problems such as cluster scheduling.\nWe explore an approach where training is performed on a\nsmall data set of 10 million documents. Newly sampled test\nqueries are evaluated on the 100 million document collection\nwithout further refinement. Figures 5b, 5c, and 5d show index\nsize and latencies. All learned strategies created one index\nper query with query runtimes increasing corresponding\nto document count, and Pretrain+Online performing best.\nLatency metrics were dominated by a single long-running\nquery with two expensive $дtexpressions which could not be\nmeaningfully accelerated. While scalability transfer results\nshow some promise, we plan to investigate an approach\nwhere a model of the query planner is learned to be able to\nevaluate indices without needing to run them at full scale.\nWorkload means : Mean 90th 99th Norm. Size (GB)\nPretrain+Online 0.5 s 1.7 s 3.5 s 0.43\nOnline 0.55 s 2.1 s 3.5 s 0.53\nDefault 0.94 s 2.7 s 3.6 s 0.03\nFull 0.51 s 1.5 s 3.9 s 1.0\nPartial 0.96 s 3.4 s 4.4 s 0.32\nPretrain 0.59 s 2.2 s 4.1 s 1.0\nTable 3: Performance variation when sampling differ-\nent query sets per run. Min,90th,99th are referring to\naverage latencies across different query sets.\nGeneralization. Traditional database benchmarks such as\nTPC-H use fixed sets of query templates sampling different\nattribute values at runtime. This is problematic from a deep\nlearning perspective as the number of distinct query shapes\n(i.e. operator structure) is too small to evaluate generaliza-\ntion to unseen queries. Our synthetic benchmark client does\nnot only sample attribute values on fixed shapes, but also\nquery shapes. We investigate query generalization via our\nsynthetic client by sampling 5 different query sets, and re-\nporting on variation in learning performance. We insert 5\nmillion documents with 15 attributes with varying data types\n(schema details provided in appendix). Next, 10,000queries\nand rule-based demonstrations are generated using Fullin-\ndexing as the demonstration rule. We did not see improve-\nment when generating more examples, indicating these were\nsufficient to cover rule behaviour on the synthetic schema.\nWe pretrain on these queries as before, then sample 20 new\nqueries as the test set and perform online training. Table 3\ngives an overview on performance variation across query\nsets. Pretrain+Online saves more than 50%space while per-\nforming better or comparably across latency metrics. Partial\nsaves even more space but fails on improving latency. Valuesare averaged across different tasks, thus means per task are\nexpected to be different. Importantly, performance of our\napproach is not an artefact on a specific query set designed\nfor this task but generalizes.\n6.3 Stream task parallelism\nProblem setup. Distributed stream processing systems\n(DSPS) such as Storm [ 70], Heron [ 34] or Flink [ 5] are widely\nused in large scale real time processing. To this end, DSPS\nhave to meet strict service level objectives on message la-\ntency and throughput. Achieving these objectives requires\ncareful tuning of various scheduling parameters, as process-\ning instances may fail and workloads may vary with sudden\nspikes. Floratou et al. suggested the notion of self-regulating\nstream processing with Dhalion [ 17], a rule-based engine on\ntop of Heron which collect performance metrics, identifies\nsymptoms of performance problems (e.g. instance failure),\ngenerates diagnoses and tries to resolve issues by making ad-\njustments (e.g. changing packing plan). We use LIFT to learn\nto tune task parallelism in Heron using RL. Task parallelism\ncorresponds to the number of physical cores allocated to a\nspecific task in the processing topology. We use the same\n3 stage word-count topology as described in Dhalion on a\nsmall cluster using 5 machines (1 master, 4 slaves).\nModel. We again use LIFT to implement state and action\nmodels, and to interface Heron’s metric collection. For the\nstate, we use a matrix containing CPU and memory usage,\nand time spent in back-pressure (a special message used by\nHeron to indicate lack of resources on a task) for all task\ninstances. As actions, the agent outputs (integer) task par-\nallelism values for each component in the topology. The\nreward is a linear combination of normalized message laten-\ncies square roots (to smooth outliers), throughput, and the\nfraction of available instances used.\nResults. Collecting data for the stream processing task is\ndifficult as each step requires multiple minutes of collect-\ning metrics so performance can stabilize after changes, and\nupdating the topology by creating a new packing plan and\ndeploying it. Due to an outstanding issue in the scheduler, we\ndid not manage to run Dhalion itself. We could also not easily\nport its parallelism rule to LIFT because not all used metrics\nwere exposed via the Heron tracker API. For the purpose\nof this experiment, we hence collected demonstration data\nfrom a simple threshold rule. The aim of this case study is\nhence not to prove superiority over Dhalion, but evaluate if\nrule-based demonstrations can help RL in dynamic workload\nenvironments. We train and evaluate dynamic behavior by\nrandomly sampling different workload changes such as load\nmoving up and down periodically, or changing from low\nto high/high to low. Figure 6 shows results by comparing\naverage reward over the duration of the evaluation which\n12\n\n0 10 20 30 40 50−1.0−0.50.00.51.0Reward\n0 20 40\nSystem Steps2.55.07.510.012.5Split Parallelism\n0 20 40\nSystem Steps2468Count Parallelism\nOnline\n+Pretrain\nOnlineThreshold\nExpertPretrainFigure 6: Top: Rewards through varying workload.\nBottom: Task parallelisms for splitter and count bolts.\npresented the controller with all possible workloads in deter-\nministic order. Each step corresponds to about 2-4 minutes\nreal time to receive metrics and implement changes.\nWe defined an Expert configuration which had predeter-\nmined good configurations for each workload change. The\nbottom row shows how parallelism settings for both bolts\nare adjusted by the different strategies over time. The Ex-\npert systematically alternates between two configurations\nfor each component, incurring temporary low rewards upon\nchanges. The Pretrain+Online agent managed to avoid tem-\nporary reward loss by anticipating workload changes, and\nby always keeping split parallelism high as to have enough\ncapacity for changes, thus outperforming the pre-tuned Ex-\npertconfigurations slightly ( 3%. This ’anticipation effect’ is\na consequence of the agent observing regularities in how\nworkloads change. Online failed to adopt an effective strat-\negy within the same training time (1.5 days). Other methods\nperformed worse although the threshold rule-based model\ncould have been improved by manually fitting thresholds to\nworkload changes (thus being closer to Expert ).\nWe provide further analysis by comparing training re-\nwards with and without pretraining in Figure 7. Online with-\nout pretraining could on average not recover good configura-\ntions, thus most of the time being at a low reward, and only\noccasionally seeing high rewards when workloads matched\nits configuration. In contrast, Pretrain+Online achieved much\n0 100 200 300 400 500\nTraining steps−101Reward\nOnline reward Training mean\n0 100 200 300 400 500\nTraining steps−101Reward\nPretrain+Online reward Training meanFigure 7: Heron training rewards.\nhigher mean rewards as after around 100 episodes of training,\nit began to quickly recover from workload changes to reach\n(normalized) high reward regions again. Our experiments\nshow the combination of pretraining and online refinement\ncan produce effective results on dynamic workloads. A key\nquestion is if workloads in practice exhibit irregularities\nwhich are difficult to address through manual tuning. We sus-\npect the advantage of RL will increase for larger topologies\nwith many different bolt types on heterogeneous resources.\n6.4 Discussion\nLimitations. Our results indicate the potential of imper-\nfect demonstrations when applying RL to data management\ntasks, improving latency metrics by up to 70%in the IMDB\ncase study against several baselines, and outperforming the\nexpert configuration in Heron. Our experiments did not in-\nclude some subtasks which prolong training times. For ex-\nample, in the indexing task, we omitted considerations for\nindexing shards and replica sets. As RL applications in data\nmanagement move from simulation to real world, they will\nincrementally cover additional subtasks. We also showed the\ndifficulty of tackling tasks where step times increase with\nscale. Here, mechanisms such as pretraining and training\non partial tasks provide a promising direction to eventually\napply RL at data center scale.\nLearning. The algorithmic limitations of current RL algo-\nrithms continue to present significant limitations in real\nworld applications. Learning from scratch can take infeasi-\nbly long and may also be unreliable due to the stochastic\nnature of training. Further, learning a task once and deploy-\ning the resulting model in different contexts is unlikely to\nsucceed due to the sensitivity of RL algorithms [ 31]. We rely\non online refinement following the pretraining procedure\n13\n\nwhich incurs only little overhead after initial implementa-\ntion. The aim of our experiments was not to demonstrate the\nbest way to e.g. perform workload management in stream\nprocessing. Recent work has illustrated how neural networks\nstruggle with forecasting spiky workloads [ 40]. We focused\non evaluating if pretraining can help the notoriously difficult\napplication of RL to data management tasks. In summary,\nDRL remains a promising direction, as even results in this ex-\nploratory phase show the ability to learn complex behaviour\nwhich normally requires manual solution design.\n7 RELATED WORK\nRL in data management. Early work in exploring RL in\ncomputer systems can be found in routing (Q-routing) and\nprotocol optimization [ 8,35,36]. Subsequent research has\ncovered a diverse range of domains such as cloud work-\nload allocation, cluster scheduling, networking, or bitrate\nselection [ 15,42,43,68,72]. Many of these works have out-\nperformed existing approaches in simulation, but did not\ntranslate into real world deployments due to the difficul-\nties discussed elsewhere in this paper. Notably, the idea of\nusing neural networks in combination with RL in systems\ncan be found as early as 2006 in Tesauro et al.’s work on\nserver resource allocation [ 69]. The authors incorporated\npre-existing knowledge by initially bootstrapping control\nfrom a rule, whereas we use offline demonstrations to reduce\ntraining times. RL for combinatorial selection has also found\napplication in tuning compression of neural networks for\nmobile devices [ 39]. Mirhoseini et al. demonstrated how to\nuse attention-based andwe p hierarchical methods known\nfrom neural machine translation to effectively perform Ten-\nsorFlow device placements [ 46,47]. The difference to our\nwork is that each graph step only takes few seconds so on-\nline training can be performed more effectively. Sharma et al.\nused RL to learn single-key indices in relational databases,\nand simplified the problem by manually constructing fea-\ntures such as selectivity [64].\nAdaptive indexing. A large body of work exists on indexing\nstrategies which are widely used in practice. Offline indexing\nis performed using the design tuning tools provided by com-\nmercial database products which require database adminis-\ntrators to manually interact with the tool, and make ultimate\ndesign decisions [ 3,11,14]. Online indexing addresses this\nlimitation by making decisions based on continuous online\nmonitoring [ 60]. Adaptive (or holistic [ 55]) indexing (e.g. in\ncolumnar databases) enable even faster reaction to workload\nchanges by building and refining indices via lightweight in-\ncremental modifications [ 21,24,29]. A similar depth of work\nin indexing is not available for document databases, although\nmany techniques are likely transferable [ 57]. CommercialMongoDB services sometimes offer index recommendations\nbased on longer term workload patterns ([51]).\nML in databases. Recently, machine learning approaches\nhave been explored in data management. Pavlo et al. pro-\nposed the idea of a self-driving database with initial focus\non employing ML techniques for workload forecasting [ 53].\nIn subsequent work, these forecasts were evaluated on their\nability to help create SQL indices [ 40]. Their work in par-\nticular found that neural networks were not as effective in\ncapturing spiky loads as traditional time series techniques.\nOtterTune [ 73] automatically determines relevant database\nparameters to create end-to-end tuning pipelines [ 19]. BO\nis not easily applicable in problems like index selection or\ngenerally combinatorial problems as it requires a similarity\nfunction (Kernel) to interpolate a smooth objective func-\ntion between data points. Defining a custom Kernel between\ndatabases is difficult because semantically similar indices\ncan perform vastly different on a workload. Kraska et al. ex-\nplored representing the index data structure itself as a neural\nnetwork wit the aim to learn to match data distributions\nand access patterns [ 33]. Bailis et al. subsequently argued\nthat well tuned cuckoo hashing could still outperform these\nlearned indices [ 54]. We similarly argue that deep RL tech-\nniques are in an exploratory phase and cannot yet replace\nwell established tuning methods.\nLearning from demonstrations. Learning from expert\ndemonstration is a well studied notion in RL. DAGGER (for\nDataset Aggregation) is a popular approach for imitation\nlearning which requires an expert to continuously provide\nnew input [ 59]. While this is not directly compatible with\nlearning from traces, we are considering future additions\nto LIFT where a human may interactively provide demon-\nstrations between trials to further accelerate training. Other\nfamiliar approaches include behavioural cloning, recently\nalso in the context of generative adversarial models [ 28,75].\nSnorkel is a system to help generate weakly supervised data\nvia labeling functions which is conceptually similar to our\nrule-based demonstrations [ 58]. In this work, we relied on\nDQfD as a conceptually simple extension to Deep Q-learning\n[27]. Its main advantage is that the large margin function\ngives a simple way of assigning low or high confidence to\ndemonstrations via single tunable parameter, thus in practice\nalso allowing the use of imperfect demonstrations. Gao et\nal. recently suggested employing a unified objective which\nincorporates imperfect demonstrations naturally by account-\ning for uncertainty using an entropy approach [18].\n8 CONCLUSION\nIn this paper, we discuss long evaluation times, algorithmic\ninstability, and lack of widely available software as key obsta-\ncles to the applicability of RL in data management tasks. To\n14\n\nhelp address these issues, we introduce LIFT, the first end-to-\nend software stack for applying RL to data management. As\npart of LIFT, we also introduce TensorForce, a practical deep\nreinforcement learning library providing a declarative API\nto common RL algorithms. The key idea of LIFT is to help\ndevelopers leveraging existing knowledge from trace data,\nrules or any other form of demonstrations to guide model\ncreation. We demonstrate the practical potential of LIFT in\ntwo proof-of-concept case studies. If online-only training\ntakes impractically long, our results show that pretraining\ncan significantly improve final results.\nREFERENCES\n[1]Martın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng\nChen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu\nDevin, et al .2015. TensorFlow: Large-Scale Machine Learning on\nHeterogeneous Distributed Systems. arXiv preprint arXiv:1603.04467\n(2015).\n[2]Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis,\nJeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving,\nMichael Isard, et al .2016. TensorFlow: A System for Large-Scale\nMachine Learning.. In OSDI , Vol. 16. 265–283.\n[3]Sanjay Agrawal, Surajit Chaudhuri, Lubor Kollar, Arun Marathe, Vivek\nNarasayya, and Manoj Syamala. 2004. Database Tuning Advisor for\nMicrosoft SQL Server 2005. In VLDB . Very Large Data Bases Endow-\nment Inc. https://www.microsoft.com/en-us/research/publication/\ndatabase-tuning-advisor-for-microsoft-sql-server-2005/\n[4]Amazon Inc. 2018. Amazon DynamoDB. website. (2018). https:\n//aws.amazon.com/dynamodb/\n[5]Apache Foundation. 2018. Apache Flink. website. (2018). https:\n//flink.apache.org\n[6]Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and\nAnil Anthony Bharath. 2017. A Brief Survey of Deep Reinforcement\nLearning. arXiv preprint arXiv:1708.05866 (2017).\n[7]Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.\n2013. The Arcade Learning Environment: An evaluation platform for\ngeneral agents. J. Artif. Intell. Res.(JAIR) 47 (2013), 253–279.\n[8]Justin A Boyan and Michael L Littman. 1994. Packet routing in dy-\nnamically changing networks: A reinforcement learning approach.\nAdvances in neural information processing systems (1994), 671–671.\n[9]Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider,\nJohn Schulman, Jie Tang, and Wojciech Zaremba. 2016. OpenAI gym.\narXiv preprint arXiv:1606.01540 (2016).\n[10] Itai Caspi, Gal Leibovich, and Gal Novik. 2017. Reinforcement Learning\nCoach. (Dec. 2017). https://doi.org/10.5281/zenodo.1134899\n[11] Surajit Chaudhuri and Vivek Narasayya. 1998. AutoAdmin\n&Ldquo;What-if&Rdquo; Index Analysis Utility. SIGMOD Rec. 27,\n2 (June 1998), 367–378. https://doi.org/10.1145/276305.276337\n[12] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang,\nTianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet:\nA flexible and efficient machine learning library for heterogeneous\ndistributed systems. arXiv preprint arXiv:1512.01274 (2015).\n[13] Brian F. Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan,\nand Russell Sears. 2010. Benchmarking Cloud Serving Systems with\nYCSB. In Proceedings of the 1st ACM Symposium on Cloud Computing\n(SoCC ’10) . ACM, New York, NY, USA, 143–154. https://doi.org/10.\n1145/1807128.1807152\n[14] Benoit Dageville, Dinesh Das, Karl Dias, Khaled Yagoub, Mohamed\nZait, and Mohamed Ziauddin. 2004. Automatic SQL Tuning in Oracle10G. In Proceedings of the Thirtieth International Conference on Very\nLarge Data Bases - Volume 30 (VLDB ’04) . VLDB Endowment, 1098–\n1109. http://dl.acm.org/citation.cfm?id=1316689.1316784\n[15] Xavier Dutreilh, Sergey Kirgizov, Olga Melekhova, Jacques Malenfant,\nNicolas Rivierre, and Isis Truck. 2011. Using reinforcement learning\nfor autonomic resource allocation in clouds: towards a fully automated\nworkflow. In ICAS 2011, The Seventh International Conference on Auto-\nnomic and Autonomous Systems . 67–74.\n[16] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan,\nVolodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley,\nIain Dunning, Shane Legg, and Koray Kavukcuoglu. [n. d.]. IM-\nPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-\nLearner Architectures. ([n. d.]). arXiv:cs.LG/1802.01561v2\n[17] Avrilia Floratou, Ashvin Agrawal, Bill Graham, Sriram Rao, and Karthik\nRamasamy. 2017. Dhalion: self-regulating stream processing in heron.\nProceedings of the VLDB Endowment 10, 12 (2017), 1825–1836.\n[18] Yang Gao, Huazhe, Xu, Ji Lin, Fisher Yu, Sergey Levine, and Trevor Dar-\nrell. [n. d.]. Reinforcement Learning from Imperfect Demonstrations.\n([n. d.]). arXiv:cs.AI/1802.05313v1\n[19] Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski,\nJohn Karro, and D Sculley. 2017. Google Vizier: A Service for Black-Box\nOptimization. In Proceedings of the 23rd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining . ACM, 1487–1495.\n[20] Google Inc. 2018. Google Cloud Datastore. website. (2018). https:\n//cloud.google.com/datastore/\n[21] Goetz Graefe and Harumi Kuno. 2010. Self-selecting, Self-tuning, In-\ncrementally Optimized Indexes. In Proceedings of the 13th International\nConference on Extending Database Technology (EDBT ’10) . ACM, New\nYork, NY, USA, 371–381. https://doi.org/10.1145/1739041.1739087\n[22] Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine.\n2016. Continuous Deep Q-Learning with Model-based Acceleration.\n(March 2016). arXiv:1603.00748\n[23] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.\n2017. Reinforcement learning with deep energy-based policies. arXiv\npreprint arXiv:1702.08165 (2017).\n[24] Felix Halim, Stratos Idreos, Panagiotis Karras, and Roland H. C. Yap.\n2012. Stochastic Database Cracking: Towards Robust Adaptive In-\ndexing in Main-memory Column-stores. Proc. VLDB Endow. 5, 6 (Feb.\n2012), 502–513. https://doi.org/10.14778/2168651.2168652\n[25] Daniel Hein, Stefan Depeweg, Michel Tokic, Steffen Udluft, Alexander\nHentschel, Thomas A Runkler, and Volkmar Sterzing. 2017. A Bench-\nmark Environment Motivated by Industrial Control Problems. arXiv\npreprint arXiv:1709.09480 (2017).\n[26] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina\nPrecup, and David Meger. 2017. Deep reinforcement learning that\nmatters. arXiv preprint arXiv:1709.06560 (2017).\n[27] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul,\nBilal Piot, Andrew Sendonaris, Gabriel Dulac-Arnold, Ian Osband,\nJohn Agapiou, Joel Z. Leibo, and Audrunas Gruslys. 2017. Learning\nfrom Demonstrations for Real World Reinforcement Learning. CoRR\nabs/1704.03732 (2017). arXiv:1704.03732 http://arxiv.org/abs/1704.\n03732\n[28] Jonathan Ho and Stefano Ermon. [n. d.]. Generative Adversarial Imi-\ntation Learning. ([n. d.]). arXiv:cs.LG/1606.03476v1\n[29] Stratos Idreos, Stefan Manegold, Harumi Kuno, and Goetz Graefe. 2011.\nMerging What’s Cracked, Cracking What’s Merged: Adaptive Indexing\nin Main-memory Column-stores. Proc. VLDB Endow. 4, 9 (June 2011),\n586–597. https://doi.org/10.14778/2002938.2002944\n[30] imdb.com. 2018. IMDb Datasets. website. (2018). https://www.imdb.\ncom/interfaces/\n[31] Ken Kansky, Tom Silver, David A. Mély, Mohamed Eldawy, Miguel\nLázaro-Gredilla, Xinghua Lou, Nimrod Dorfman, Szymon Sidor, Scott\n15\n\nPhoenix, and Dileep George. [n. d.]. Schema Networks: Zero-shot\nTransfer with a Generative Causal Model of Intuitive Physics. ([n. d.]).\narXiv:cs.AI/1706.04317v2\n[32] Diederik Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic\nOptimization. (Dec. 2014). arXiv:1412.6980\n[33] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Poly-\nzotis. [n. d.]. The Case for Learned Index Structures. ([n. d.]).\narXiv:cs.DB/1712.01208v2\n[34] Sanjeev Kulkarni, Nikunj Bhagat, Maosong Fu, Vikas Kedigehalli,\nChristopher Kellogg, Sailesh Mittal, Jignesh M Patel, Karthik Ra-\nmasamy, and Siddarth Taneja. 2015. Twitter heron: Stream processing\nat scale. In Proceedings of the 2015 ACM SIGMOD International Confer-\nence on Management of Data . ACM, 239–250.\n[35] Shailesh Kumar and Risto Miikkulainen. 1997. Dual reinforcement\nQ-routing: An on-line adaptive routing algorithm. In Artificial neural\nnetworks in engineering .\n[36] Shailesh Kumar and Risto Miikkulainen. 1999. Confidence based dual\nreinforcement q-routing: An adaptive online network routing algo-\nrithm. In IJCAI , Vol. 99. Citeseer, 758–763.\n[37] Yuxi Li. 2017. Deep reinforcement learning: An overview. arXiv\npreprint arXiv:1701.07274 (2017).\n[38] Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox,\nJoseph Gonzalez, Ken Goldberg, and Ion Stoica. 2017. Ray RLLib:\nA Composable and Scalable Reinforcement Learning Library. arXiv\npreprint arXiv:1712.09381 (2017).\n[39] Sicong Liu, Yingyan Lin, Zimu Zhou, Kaiming Nan, Hui Liu, and\nJunzhao Du. 2018. On-Demand Deep Model Compression for Mobile\nDevices: A Usage-Driven Model Selection Framework. (2018).\n[40] Lin Ma, Dana Van Aken, Ahmed Hefny, Gustavo Mezerhane, Andrew\nPavlo, and Geoffrey J. Gordon. 2018. Query-based Workload Forecast-\ning for Self-Driving Database Management Systems. In Proceedings of\nthe 2018 International Conference on Management of Data (SIGMOD ’18) .\nACM, New York, NY, USA, 631–645. https://doi.org/10.1145/3183713.\n3196908\n[41] Horia Mania, Aurelia Guy, and Benjamin Recht. [n. d.]. Simple random\nsearch provides a competitive approach to reinforcement learning. ([n.\nd.]). arXiv:cs.LG/1803.07055v1\n[42] Hongzi Mao, Mohammad Alizadeh, Ishai Menache, and Srikanth Kan-\ndula. 2016. Resource Management with Deep Reinforcement Learning.\nInProceedings of the 15th ACM Workshop on Hot Topics in Networks .\nACM, 50–56.\n[43] Hongzi Mao, Ravi Netravali, and Mohammad Alizadeh. 2017. Neu-\nral Adaptive Video Streaming with Pensieve. In Proceedings of the\nConference of the ACM Special Interest Group on Data Communica-\ntion (SIGCOMM ’17) . ACM, New York, NY, USA, 197–210. https:\n//doi.org/10.1145/3098822.3098843\n[44] Microsoft. 2018. CosmosDB - A globally distributed database for low\nlatency and massively scalable applications, with native support for\nNoSQL. website. (2018). https://azure.microsoft.com/en-gb/services/\ncosmos-db/\n[45] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. [n. d.].\nEfficient Estimation of Word Representations in Vector Space. ([n. d.]).\narXiv:cs.CL/1301.3781v3\n[46] Azalia Mirhoseini, Anna Goldie, Hieu Pham, Benoit Steiner, Quoc V.\nLe, and Jeff Dean. 2018. Hierarchical Planning for Device Placement.\nhttps://openreview.net/pdf?id=Hkc-TeZ0W\n[47] Azalia Mirhoseini, Hieu Pham, Quoc V Le, Benoit Steiner, Rasmus\nLarsen, Yuefeng Zhou, Naveen Kumar, Mohammad Norouzi, Samy\nBengio, and Jeff Dean. 2017. Device Placement Optimization with\nReinforcement Learning. arXiv preprint arXiv:1706.04972 (2017).\n[48] Volodymyr Mnih, AdriÃă PuigdomÃĺnech Badia, Mehdi Mirza, Alex\nGraves, Timothy P. Lillicrap, Tim Harley, David Silver, and KorayKavukcuoglu. 2016. Asynchronous Methods for Deep Reinforcement\nLearning. (Feb. 2016). arXiv:1602.01783\n[49] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves,\nIoannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013.\nPlaying Atari with Deep Reinforcement Learning. arXiv preprint\narXiv:1312.5602 (2013).\n[50] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu,\nJoel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, An-\ndreas K Fidjeland, Georg Ostrovski, et al .2015. Human-level control\nthrough deep reinforcement learning. Nature 518, 7540 (2015), 529–\n533.\n[51] ObjectLabs Corporation. 2018. mLab cloud service. website. (2018).\nhttps://mlab.com\n[52] OpenAI. 2018. OpenAI Five DOTA. website. (June 2018). https:\n//blog.openai.com/openai-five/\n[53] Andrew Pavlo, Gustavo Angulo, Joy Arulraj, Haibin Lin, Jiexi Lin, Lin\nMa, Prashanth Menon, Todd C Mowry, Matthew Perron, Ian Quah,\net al. 2017. Self-Driving Database Management Systems.. In CIDR .\n[54] Peter Bailis et al. 2018. Don’t Throw Out Your Algorithms Book\nJust Yet: Classical Data Structures That Can Outperform Learned In-\ndexes. website. (Jan. 2018). https://dawn.cs.stanford.edu/2018/01/11/\nindex-baselines/\n[55] Eleni Petraki, Stratos Idreos, and Stefan Manegold. 2015. Holistic\nIndexing in Main-memory Column-stores. In Proceedings of the 2015\nACM SIGMOD International Conference on Management of Data (SIG-\nMOD ’15) . ACM, New York, NY, USA, 1153–1166. https://doi.org/10.\n1145/2723372.2723719\n[56] Bilal Piot, Matthieu Geist, and Olivier Pietquin. 2014. Boosted Bell-\nman residual minimization handling expert demonstrations. In Joint\nEuropean Conference on Machine Learning and Knowledge Discovery in\nDatabases . Springer, 549–564.\n[57] Mohiuddin Abdul Qader, Shiwen Cheng, and Vagelis Hristidis. 2018. A\nComparative Study of Secondary Indexing Techniques in LSM-based\nNoSQL Databases. In Proceedings of the 2018 International Conference\non Management of Data (SIGMOD ’18) . ACM, New York, NY, USA,\n551–566. https://doi.org/10.1145/3183713.3196900\n[58] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen\nWu, and Christopher Ré. 2017. Snorkel: Rapid training data creation\nwith weak supervision. Proceedings of the VLDB Endowment 11, 3\n(2017), 269–282.\n[59] Stephane Ross, Geoffrey J. Gordon, and J. Andrew Bagnell. [n. d.].\nA Reduction of Imitation Learning and Structured Prediction to No-\nRegret Online Learning. ([n. d.]). arXiv:cs.LG/1011.0686v3\n[60] Karl Schnaitter, Serge Abiteboul, Tova Milo, and Neoklis Polyzotis.\n2006. COLT: Continuous On-line Tuning. In Proceedings of the 2006\nACM SIGMOD International Conference on Management of Data (SIG-\nMOD ’06) . ACM, New York, NY, USA, 793–795. https://doi.org/10.\n1145/1142473.1142592\n[61] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and\nPhilipp Moritz. 2015. Trust region policy optimization. In Proceedings\nof the 32nd International Conference on Machine Learning (ICML-15) .\n1889–1897.\n[62] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and\nOleg Klimov. 2017. Proximal Policy Optimization Algorithms. arXiv\npreprint arXiv:1707.06347 (2017).\n[63] Frank Seide and Amit Agarwal. 2016. CNTK: Microsoft’s Open-Source\nDeep-Learning Toolkit. In Proceedings of the 22nd ACM SIGKDD Inter-\nnational Conference on Knowledge Discovery and Data Mining . ACM,\n2135–2135.\n[64] Ankur Sharma, Felix Martin Schuhknecht, and Jens Dit-\ntrich. [n. d.]. The Case for Automatic Database Admin-\nistration using Deep Reinforcement Learning. ([n. d.]).\n16\n\narXiv:cs.DB/http://arxiv.org/abs/1801.05643v1\n[65] Szymon Sidor and John Schulman. 2017. OpenAI Baselines. website.\n(2017). https://blog.openai.com/openai-baselines-dqn/\n[66] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre,\nGeorge Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou,\nVeda Panneershelvam, Marc Lanctot, et al .2016. Mastering the game\nof Go with deep neural networks and tree search. Nature 529, 7587\n(2016), 484–489.\n[67] Richard S Sutton and Andrew G Barto. 1998. Reinforcement learning:\nAn introduction . Vol. 1. MIT press Cambridge.\n[68] Gerald Tesauro, Rajarshi Das, Hoi Chan, Jeffrey Kephart, David Levine,\nFreeman Rawson, and Charles Lefurgy. 2007. Managing power con-\nsumption and performance of computing systems using reinforcement\nlearning. In Advances in Neural Information Processing Systems . 1497–\n1504.\n[69] G. Tesauro, N. K. Jong, R. Das, and M. N. Bennani. 2006. A Hybrid\nReinforcement Learning Approach to Autonomic Resource Allocation.\nInProceedings of the 2006 IEEE International Conference on Autonomic\nComputing (ICAC ’06) . IEEE Computer Society, Washington, DC, USA,\n65–73. https://doi.org/10.1109/ICAC.2006.1662383\n[70] The Apache Software Foundation. 2018. Apache Storm.\nhttps://storm.apache.org/. (2018).\n[71] Joshua Tobin, Wojciech Zaremba, and Pieter Abbeel. 2017. Domain\nRandomization and Generative Models for Robotic Grasping. arXiv\npreprint arXiv:1710.06425 (2017).\n[72] Asaf Valadarsky, Michael Schapira, Dafna Shahaf, and Aviv Tamar.\n2017. A Machine Learning Approach to Routing. arXiv preprint\narXiv:1708.03074 (2017).\n[73] Dana Van Aken, Andrew Pavlo, Geoffrey J Gordon, and Bohan Zhang.\n2017. Automatic Database Management System Tuning Through Large-\nscale Machine Learning. In Proceedings of the 2017 ACM International\nConference on Management of Data . ACM, 1009–1024.\n[74] H. van Hasselt, A. Guez, and D. Silver. 2015. Deep Reinforce-\nment Learning with Double Q-learning. ArXiv e-prints (Sept. 2015).\narXiv:cs.LG/1509.06461\n[75] Ziyu Wang, Josh Merel, Scott Reed, Greg Wayne, Nando de Freitas,\nand Nicolas Heess. [n. d.]. Robust Imitation of Diverse Behaviors. ([n.\nd.]). arXiv:cs.LG/1707.02747v2\n[76] Ronald J Williams. 1992. Simple statistical gradient-following algo-\nrithms for connectionist reinforcement learning. Machine learning 8,\n3-4 (1992), 229–256.\nA SYNTHETIC CLIENT\nWe proceed to describe the synthetic data layout and query\ngeneration procedure used in the scalability and generaliza-\ntion experiments. Synthetic documents each contained 15\nattributes with 6 strings, 6 integers of different ranges, 2 date\nfields, and 1 string array for full text.\nSynthetic were generated by sampling between 1 and 3\nattributes. For each attribute, a sub-expression was generated\nby sampling both a comparison operator and a value for the\nattribute, e.g.:\nsub_expr := {$gt: {\"attribute\": \"value\"}}\nSub-expressions were then concatenated by uniformly sam-\npling a logical operator, e.g.:\nexpr := {$or: [sub_expr, sub_expr, sub_expr]}Finally, we sampled an aggregation operator from a discrete\ndistribution where a limit without sort or count had 0.1 prob-\nability and sort/count 0.45 each, as sort and counts caused\nmore difficult indexing decisions. If a sort aggregation was\nsampled, sorting was requested for each attribute in the\nquery with 0.5 probability. Below is an example of a result-\ning query:\nQ := find({ '$or': [{ 'f2': {'$eq':'centimeter '}},\n{'f10': {'$gte ': {'$date ': 1394135731965}}}]})\n.limit(10)\nIn summary, our synthetic query client enables users to gen-\nerate query shapes of varying difficulty and size to investi-\ngate indexing behavior.\n17",
  "textLength": 89242
}