{
  "paperId": "5f910030bbbfe1bc4057574b5cf218b5ca9b0d79",
  "title": "Tsunami: A Learned Multi-dimensional Index for Correlated Data and Skewed Workloads",
  "pdfPath": "5f910030bbbfe1bc4057574b5cf218b5ca9b0d79.pdf",
  "text": "MIT Open Access Articles\nTsunami: a learned multi-dimensional index \nfor correlated data and skewed workloads\nThe MIT Faculty has made this article openly available. Please share\nhow this access benefits you. Your story matters.\nAs Published: 10.14778/3425879.3425880\nPublisher: VLDB Endowment\nPersistent URL: https://hdl.handle.net/1721.1/132295\nVersion: Final published version: final published article, as it appeared in a journal, conference \nproceedings, or other formally published context\nTerms of use: Creative Commons Attribution-NonCommercial-NoDerivs License\n\n\nTsunami: A Learned Multi-dimensional\nIndex for Correlated Data and Skewed Workloads\nJialin Ding, Vikram Nathan, Mohammad Alizadeh, Tim Kraska\nMassachusetts Insititute of Technology\n{jialind,vikramn,alizadeh,kraska}@mit.edu\nABSTRACT\nFiltering data based on predicates is one of the most fundamental\noperations for any modern data warehouse. Techniques to accel-\nerate the execution of filter expressions include clustered indexes,\nspecialized sort orders (e.g., Z-order), multi-dimensional indexes,\nand, for high selectivity queries, secondary indexes. However, these\nschemes are hard to tune and their performance is inconsistent. Re-\ncent work on learned multi-dimensional indexes has introduced the\nidea of automatically optimizing an index for a particular dataset\nand workload. However, the performance of that work suffers in the\npresence of correlated data and skewed query workloads, both of\nwhich are common in real applications. In this paper, we introduce\nTsunami, which addresses these limitations to achieve up to 6 Ã—faster\nquery performance and up to 8 Ã—smaller index size than existing\nlearned multi-dimensional indexes, in addition to up to 11 Ã—faster\nquery performance and 170 Ã—smaller index size than optimally-tuned\ntraditional indexes.\nPVLDB Reference Format:\nJialin Ding, Vikram Nathan, Mohammad Alizadeh, Tim Kraska. Tsunami: A\nLearned Multi-dimensional Index for Correlated Data and Skewed Workloads.\nPVLDB, 14(2): 74 - 86, 2021.\ndoi:10.14778/3425879.3425880\n1 INTRODUCTION\nFiltering through data is the foundation of any analytical database\nengine, and several advances over the past several years specifically\ntarget database filter performance. For example, column stores [ 11]\ndelay or entirely avoid accessing columns (i.e., dimensions) which\nare not relevant to a query, and they often sort the data by a single\ndimension in order to skip over records that do not match a query\nfilter over that dimension.\nIf data has to be filtered by more than one dimension, secondary\nindexes can be used. Unfortunately, their large storage overhead and\nthe latency incurred by chasing pointers make them viable only when\nthe predicate on the indexed dimension has a very high selectivity.\nAn alternative approach is to use (clustered) multi-dimensional in-\ndexes; these may be tree-based data structures (e.g., k-d trees, R-trees,\nor octrees) or a specialized sort order over multiple dimensions (e.g.,\na space-filling curve like Z-ordering or hand-picked hierarchical\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 14, No. 2 ISSN 2150-8097.\ndoi:10.14778/3425879.3425880sort). Many state-of-the-art analytical database systems use multi-\ndimensional indexes or sort orders to improve the scan performance\nof queries with predicates over several columns [1, 8, 16].\nHowever, multi-dimensional indexes have significant drawbacks.\nFirst, these techniques are hard to tune and require an admin to\ncarefully pick which dimensions to index, if any at all, and the order\nin which they are indexed. This decision must be revisited every time\nthe data or workload changes, requiring extensive manual labor to\nmaintain performance. Second, there is no single approach (even if\ntuned correctly) that dominates all others [30].\nTo address the shortcomings of traditional indexes, recent work\nhas proposed the idea of learned multi-dimensional indexes [ 9,25,30,\n44,46]. In particular, Flood [ 30] is a in-memory multi-dimensional\nindex that automatically optimizes its structure to achieve high\nperformance on a particular dataset and workload. In contrast to\ntraditional multi-dimensional indexes, such as the k-d tree, which\nare created entirely based on the data (see Fig. 1a), Flood divides\neach dimension into some number of partitions based on the ob-\nserved data and workload (see Fig. 1b, explained in detail in Â§2). The\nCartesian product of the partitions in each dimension form a grid.\nFurthermore, to reduce the index size, Flood uses models of the CDF\nof each dimension to locate the data.\nHowever, Flood faces a number of limitations in real-world sce-\nnarios. First, Floodâ€™s grid cannot efficiently adapt to skewed query\nworkloads in which query frequencies and filter selectivities vary\nacross the data space. Second, if dimensions are correlated, then\nFlood cannot maintain uniformly sized grid cells, which degrades\nperformance and memory usage.\nTo address these limitations, we propose Tsunami, an in-memory\nread-optimized learned multi-dimensional index that extends the\nideas of Flood with new data structures and optimization techniques.\nFirst, Tsunami achieves high performance on skewed query work-\nloads by using a lightweight decision tree, called a Grid Tree, to\npartition space into non-overlapping regions in a way that reduces\nquery skew. Second, Tsunami achieves high performance on corre-\nlated datasets by indexing each region using an Augmented Grid,\nwhich uses two techniquesâ€”functional mappings andconditional\nCDFs â€”to efficiently capture information about correlations.\nWhile recent work explored how correlation can be exploited to\nreduce the size of secondary indexes [ 20,45], our work goes much\nfurther. We demonstrate not only how to leverage correlation to\nachieve faster and more compact multi-dimensional indexes (in\nwhich the data is organized based on the index) rather than sec-\nondary indexes, but also how to integrate the optimization for query\nskew and data correlation into a full end-to-end solution. Tsunami\nautomatically optimizes the data storage organization as well as the\nmulti-dimensional index structure based on the data and workload.\n74\n\nFigure 1: Indexes must identify the points that fall in the green query rectangle. To do so, they scan the points in red. (a) K-d\ntree guarantees equally-sized regions but is not optimized for the workload. (b) Flood is optimized using the workload but\nits structure is not expressive enough to handle query skew, and cells are unequally sized on correlated data. (c) Tsunami is\noptimized using the workload, is adaptive to query skew, and maintains equally-sized cells within each region.\nLike Flood [ 30], Tsunami is a clustered in-memory read-optimized\nindex over an in-memory column store. In-memory stores are in-\ncreasingly popular due to lower RAM prices [ 19] and our focus on\nreads reflects the current trend towards avoiding in-place updates in\nfavor of incremental merges (e.g., RocksDB [ 37]). We envision that\nTsunami could serve as the building block for a multi-dimensional\nin-memory key-value store or be integrated into commercial in-\nmemory (offline) analytics accelerators like Oracleâ€™s Database In-\nMemory (DBIM) [35].\nIn summary, we make the following contributions:\n(1)WedesignandimplementTsunami,anin-memoryread-optimized\nlearned multi-dimensional index that self-optimizes to achieve\nhigh performance and robustness to correlated datasets and\nskewed workloads.\n(2)We introduce two data structures, the Grid Tree and the Aug-\nmented Grid, along with new optimization procedures that en-\nable Tsunami to tailor its index structure and data organization\nstrategy to handle data correlation and query skew.\n(3)We evaluate Tsunami against Flood, the original in-memory\nlearned multi-dimensional index, as well as a number of tra-\nditional non-learned indexes, on a variety of workloads over\nreal datasets. We show that Tsunami is up to 6 Ã—and 11Ã—faster\nthan Flood and the fastest optimally-tuned non-learned index,\nrespectively. Tsunami is also adaptable to workload shift, and\nscales across data size, query selectivity, and dimensionality.\nIn the remainder of this paper, we give background (Â§2), present an\noverview of Tsunami (Â§3), introduce its two core componentsâ€”Grid\nTree (Â§4) and Augmented Grid (Â§5), present experimental results (Â§6),\nreviewrelatedwork(Â§7),proposefuturework(Â§8),andconclude(Â§9).2 BACKGROUND\nTsunami is an in-memory clustered multi-dimensional index for a\nsingle table. Tsunami aims to increase the throughput performance\nof analytics queries by decreasing the time needed to filter records\nbased on range predicates. Tsunami supports queries such as:\nSELECT SUM(R.X)\nFROM MyTable\nWHERE (aâ‰¤R.Yâ‰¤b) AND (câ‰¤R.Zâ‰¤d)\nwhere SUM(R.X) can be replaced by any aggregation. Records in a\nğ‘‘-dimensional table can be represented as points in ğ‘‘-dimensional\ndata space. For the rest of this paper, we use the terms record andpoint\ninterchangeably. To place Tsunami in context, we first describe the k-\nd tree as an example of a traditional non-learned multi-dimensional\nindex, and Flood, which originally proposed the idea of learned\nin-memory multi-dimensional indexing.\n2.1 K-d Tree: A Traditional Non-Learned Index\nThe k-d tree [ 4] is a binary space-partitioning tree that recursively\nsplitsğ‘‘-dimensional space based on the median value along each\ndimension, until the number of points in each leaf region falls be-\nlow a threshold, called the page size. Fig. 1a shows a k-d tree over\n2-dimensional data that has 8 leaf regions. The points within each\nregion are stored contiguously in physical storage (e.g., a column\nstore). By construction, the leaf regions have a roughly equal number\nof points. To process a query (i.e., identify all points that match the\nqueryâ€™s filter predicates), the k-d tree traverses the tree to find all leaf\nregions that intersect the queryâ€™s filter, then scans all points within\nthose regions to identify points that match the filter predicates.\n75\n\nThe k-d tree structure is constructed based on the data distribu-\ntion but independently of the query workload. That is, regardless of\nwhether a region of the space is never queried or whether queries are\nmore selective in some dimensions than others, the k-d tree would\nstill build an index over all data points with the same page size and\nindex overhead. While other traditional multi-dimensional indexes\nsplit space in different ways [ 3,27,31,47], they all share the property\nthat the index is constructed independent of the query workload.\n2.2 Flood: A Learned Index\nIn contrast, Flood [ 30] optimizes its layout based on the workload\n(Fig. 1b). We first introduce how Flood works, then explain its two\nkey advantages over traditional indexes, then discuss its limitations.\nGiven ağ‘‘-dimensional dataset, Flood first constructs compact\nmodels of the CDF of each dimension. The choice of modeling tech-\nnique is orthogonal; Flood uses a Recursive Model Index [ 23], but\none could also use a histogram or linear regression. Flood uses these\nmodels to divide the domain of each dimension into equally-sized\npartitions : letğ‘ğ‘–be the number of partitions in each dimension\nğ‘–âˆˆ[0,ğ‘‘). Then a point whose value in dimension ğ‘–isğ‘¥is placed\ninto theâŒŠğ¶ğ·ğ¹ğ‘–(ğ‘¥)Â·ğ‘ğ‘–âŒ‹-th partition of dimension ğ‘–. This guarantees\nthat each partition in a given dimension has an equal number of\npoints. When combined, the partitions of each dimension form a\nğ‘‘-dimensional grid withË›\nğ‘–âˆˆ[0,ğ‘‘)ğ‘ğ‘–cells, which are ordered. The\npoints within each cell are stored contiguously in physical storage.\nFloodâ€™s query processing workflow has three steps, shown in\nFig. 1b: (1) Using the per-dimension CDF models, identify the range\nof intersecting partitions in each dimension, and take the Cartesian\nproduct to identify the set of intersecting cells. (2) For each intersect-\ning cell, identify the corresponding range in physical storage using\na lookup table. (3) Scan all the points within those physical storage\nranges, and identify the points that match all query filters.\n2.2.1 Floodâ€™s Strengths. Flood has two key advantages over tradi-\ntional indexes such as the k-d tree1. First, Flood can automatically\ntune its grid structure for a given query workload by adjusting the\nnumber of partitions in each dimension to maximize query perfor-\nmance. For example, in Fig. 1b, there are many queries in the upper-\nright region of the data space that have high selectivity over dimen-\nsion Y. Therefore, Floodâ€™s optimization technique will place more par-\ntitions in dimension Y than dimension X, in order to reduce the num-\nber of points those queries need to scan. In other words, Flood learns\nwhich dimensions to prioritize over others and adjusts the number\nof partitions accordingly, whereas non-learned approaches do not\ntake the workload into account and treat all dimensions equally.\nFloodâ€™s second key advantage is its CDF models. The advantage\nof indexing using compact CDF models, as opposed to a tree-based\nstructure such as a k-d tree, is lower overhead in both space and time:\nstoringğ‘‘CDF models takes much less space than storing pointers\nand boundary keys for all internal tree nodes. It is also much faster\nto identify intersecting grid cells by invoking ğ‘‘CDF models than by\npointer chasing to traverse down a tree index.\nThe combination of these two key advantages allows Flood to\noutperform non-learned indexes by up to three orders of magnitude\nwhile using up to 50Ã— smaller index size [30].\n1Floodâ€™s minor third advantage, the sort dimension, is orthogonal to our work.2.2.2 Floodâ€™s Limitations. However, Flood has two key limitations.\nFirst, Flood only optimizes for the average query, which results in\ndegraded performance when queries are not uniform. For example,\nin Fig. 1b there are a few queries in the lower-left region of the data\nspace that, unlike the many queries in the upper-right region, have\nhigh selectivity over dimension X. Since these queries are a small\nfraction of the total workload, Floodâ€™s optimization will not prioritize\ntheir performance. As a result, Flood will need to scan a large number\nof points to create the query result (red points in Fig. 1b). Floodâ€™s\nuniform grid structure can only optimize for the average selectivity\nin each dimension and is not expressive enough to optimize for both\nthe upper-right queries and lower-left queries independently. The\nworkload in Fig. 1 is an example of a skewed workload. Query skew is\ncommon in real workloads: for example, queries often hit recent data\nmore frequently than stale data, and operations monitoring systems\nonly query for health metrics that are exceedingly low or high.\nSecond, Floodâ€™s model-based indexing technique can result in\nunequally-sized cells when data is correlated. In Fig. 1b, even though\nthe CDF models guarantee that the three partitions over dimen-\nsion X have an equal number of points, as do the six partitions over\ndimension Y, the 18 grid cells are unequally sized. This degrades\nperformance and space usage (Â§5.1). Correlations are common in real\ndata: for example, the price and distance of a taxi ride are correlated,\nas are the dates on which a package is shipped and received.\nThe goal of our work, Tsunami, is to maintain the two advantages\nof Floodâ€”optimization based on the query workload and a compact/-\nfast model-based index structureâ€”while also addressing Floodâ€™s\nlimitations in the presence of data correlations and query skew.\n3 TSUNAMI DESIGN OVERVIEW\nTsunami is a learned multi-dimensional index that is robust to data\ncorrelation and query skew. We first introduce the index structure\nand how it is used to process a query. We then provide an overview\nof the offline procedures we use to automatically optimize Tsunamiâ€™s\nstructure.\nTsunami Structure. Tsunami is a composition of two indepen-\ndent data structures: the Grid Tree (Â§4) and the Augmented Grid\n(Â§5). The Grid Tree is a space-partitioning decision tree that divides\nğ‘‘-dimensional data space into some number of non-overlapping\nregions. In Fig. 1c, the Grid Tree divides data space into three regions\nby splitting on dimension X.\nWithin each region, there is an Augmented Grid. Each Augmented\nGrid indexes the points that fall in its region. In Fig. 1c, Regions 1\nand 3 each have their own Augmented Grid. Region 2 is not given\nan Augmented Grid because no queries intersect its region. An Aug-\nmented Grid is essentially a generalization of Floodâ€™s index structure\nthat uses additional techniques to capture correlations. In Fig. 1c, the\nAugmented Grids use ğ¹:ğ‘Œâ†’ğ‘‹andğ¶ğ·ğ¹(ğ‘Œ|ğ‘‹)instead of Floodâ€™s\nğ¶ğ·ğ¹(ğ‘Œ)(explained in Â§5.2).\nTsunami Query Workflow. Tsunami processes a query in three\nsteps: (1) Traverse the Grid Tree to find all regions that intersect\nthe queryâ€™s filter. (2) In each region, identify the set of intersecting\nAugmented Grid cells (Â§5), then identify the corresponding range in\nphysical storage using a lookup table. (3) Scan all the points within\nthose physical storage ranges, and identify the points that match all\nquery filters.\n76\n\nFigure 2: A single grid cannot efficiently index a skewed\nquery workload, but a combination of non-overlapping\ngrids can. We use this workload as a running example.\nTsunami Optimization. Tsunamiâ€™s offline optimization procedure\nhas two steps: (1) Optimize the Grid Tree using the full dataset and\nsample query workload (Â§4.3). (2) In each region of the optimized\nGrid Tree, construct an Augmented Grid that is optimized over only\nthe points and queries that intersect its region (Â§5.3).\nIntuitively, Tsunami separates the two concerns of query skew\nand data correlations into its two component structures, Grid Tree\nand Augmented Grid, respectively. Each structure is optimized in\na way that addresses its corresponding concern. We now describe\neach structure in detail.\n4 GRID TREE\nIn this section, we first discuss the performance challenges posed\nby skewed workloads. We then formally define query skew, and we\ndescribe Tsunamiâ€™s solution for mitigating query skew: the Grid Tree.\n4.1 Challenges of Query Skew\nA query workload is skewed if the characteristics of queries (e.g.,\nfrequency or selectivity) vary in different parts of the data space.\nFig. 2a shows an example of sales data from 2016 to 2020. Points are\nuniformly distributed in time. The query workload is composed of\ntwo distinct query â€œtypesâ€: the red queries ğ‘„ğ‘Ÿfilter uniformly over\none-year spans, whereas the green queries ğ‘„ğ‘”filter over one-month\nspans only over the last year. If we were to impose a grid over the\ndata space, we intuitively would want many partitions over the past\nyear in order to obtain finer granularity for ğ‘„ğ‘”, whereas partitions\nprior to 2019 should be more widely spaced, because ğ‘„ğ‘Ÿdoes not\nrequire much granularity in time. However, with a single grid it is\nnot possible accommodate both while maintaining an equal number\nof points in each partition (Fig. 2a).\nInstead, we can split the data space into two regions: before 2019\nand after 2019 (Fig. 2b). Each region has its own grid, and the two\ngrids are independent. The right region can therefore tailor its grid\nforğ‘„ğ‘”by creating many partitions over time. On the other hand, the\nleft region does not need to worry about ğ‘„ğ‘”at all and places few par-\ntitions over time, and can instead add more partitions over the sales\ndimension. This intuition drives our solution for tackling query skew.Table 1: Terms used to describe the Grid Tree\nTerm Description\nğ‘‘ Dimensionality of the dataset\nğ‘† ğ‘‘ -dimensional data space: [0,ğ‘‹0)Ã—Â·Â·Â·Ã—[ 0,ğ‘‹ğ‘‘âˆ’1)\nğ‘„ Set of queries\nğ‘ˆğ‘›ğ‘– ğ‘–(ğ‘,ğ‘) Uniform distribution over [ğ‘,ğ‘)in dimension ğ‘–âˆˆ[0,ğ‘‘)\nğ‘ƒğ·ğ¹ ğ‘–(ğ‘„,ğ‘,ğ‘) Empirical PDF of queries ğ‘„over[ğ‘,ğ‘)in dimension ğ‘–\nğ»ğ‘–ğ‘ ğ‘¡ ğ‘–(ğ‘„,ğ‘,ğ‘,ğ‘›)Approximate PDF of queries ğ‘„over range[ğ‘,ğ‘)\nin dimension ğ‘–using a histogram with ğ‘›bins\nğ¸ğ‘€ğ·(ğ‘ƒ1,ğ‘ƒ2) Earth Moverâ€™s Distance between distributions ğ‘ƒ1,ğ‘ƒ2\nğ‘†ğ‘˜ğ‘’ğ‘¤ ğ‘–(ğ‘„,ğ‘,ğ‘) Skew of query set ğ‘„over range[ğ‘,ğ‘)in dimension ğ‘–\nFigure 3: Query skew is computed independently for each\nquery type ( ğ‘„ğ‘”andğ‘„ğ‘Ÿ) and is defined as the statistical\ndistance between the empirical PDF of the queries and the\nuniform distribution.\n4.2 Reducing Query Skew with a Grid Tree\nWe first formally define query skew. We then describe at a high level\nhow Grid Tree tackles query skew and how to process queries using\nthe Grid Tree. We then describe how to find the optimal Grid Tree for\na given dataset and query workload. We use the terminology in Tab. 1.\n4.2.1 Definition of Query Skew. The skew of a set of queries ğ‘„with\nrespect to a range[ğ‘,ğ‘)in dimension ğ‘–is\nğ‘†ğ‘˜ğ‘’ğ‘¤ğ‘–(ğ‘„,ğ‘,ğ‘)=ğ¸ğ‘€ğ·(ğ‘ˆğ‘›ğ‘–ğ‘–(ğ‘,ğ‘),ğ‘ƒğ·ğ¹ğ‘–(ğ‘„,ğ‘,ğ‘))\nwhereğ‘ˆğ‘›ğ‘–ğ‘–(ğ‘,ğ‘)is a uniform distribution over [ğ‘,ğ‘)andğ‘ƒğ·ğ¹ğ‘–(ğ‘„,ğ‘,ğ‘)\nis the empirical PDF of queries in ğ‘„over[ğ‘,ğ‘). Each query contributes\na unit mass to the PDF, spread over its filter range in dimension ğ‘–.\nğ¸ğ‘€ğ· is the Earth Moverâ€™s Distance, which is a measure of the dis-\ntance between two probability distributions.\nFig. 3a shows the same data and workload as in Fig. 2. Fig. 3b-c\nshow the PDF of ğ‘„ğ‘”andğ‘„ğ‘Ÿ, respectively. The skew is intuitively\nvisualized (though not technically equal to) the shaded area be-\ntween the PDF and the uniform distribution. Although ğ‘„ğ‘”is highly\n77\n\nskewed over the time dimension, Fig. 3d shows that by splitting\nthe time domain at 2019, we can reduce the skew of ğ‘„ğ‘”because\nğ‘†ğ‘˜ğ‘’ğ‘¤ğ‘Œğ‘’ğ‘ğ‘Ÿ(ğ‘„ğ‘”,2016,2019) andğ‘†ğ‘˜ğ‘’ğ‘¤ğ‘Œğ‘’ğ‘ğ‘Ÿ(ğ‘„ğ‘”,2019,2020) are low.\nIn concept,ğ‘ƒğ·ğ¹ğ‘–(ğ‘„,ğ‘,ğ‘)is a continuous probability distribution.\nHowever, in practice we approximate ğ‘ƒğ·ğ¹ğ‘–(ğ‘„,ğ‘,ğ‘)using a histogram:\nwe discretize the range [ğ‘,ğ‘)intoğ‘›bins. If a query ğ‘â€™s filter range\nintersects with ğ‘šcontiguous bins, then it contributes 1/ğ‘šmass to\neach of the bins. Therefore, the total histogram mass will be |ğ‘„|. We\ncall this histogram ğ»ğ‘–ğ‘ ğ‘¡ğ‘–(ğ‘„,ğ‘,ğ‘,ğ‘›).\nIn this context, a probability distribution over a range of histogram\nbins[ğ‘¥,ğ‘¦), where 0â‰¤ğ‘¥<ğ‘¦â‰¤ğ‘›, is a(ğ‘¦âˆ’ğ‘¥)-dimensional vector. We\ncan concretely compute skew over the bins [ğ‘¥,ğ‘¦):\nğ‘ˆğ‘›ğ‘–ğ‘–(ğ‘„,ğ‘¥,ğ‘¦)[ğ‘—]=Ë\nğ‘¥â‰¤ğ‘˜<ğ‘¦ğ»ğ‘–ğ‘ ğ‘¡ğ‘–(ğ‘„,ğ‘,ğ‘,ğ‘›)[ğ‘˜]\nğ‘¦âˆ’ğ‘¥forğ‘¥â‰¤ğ‘—<ğ‘¦\nğ‘ƒğ·ğ¹ğ‘–(ğ‘„,ğ‘¥,ğ‘¦)[ğ‘—]=ğ»ğ‘–ğ‘ ğ‘¡ğ‘–(ğ‘„,ğ‘,ğ‘,ğ‘›)[ğ‘—] forğ‘¥â‰¤ğ‘—<ğ‘¦\nğ‘†ğ‘˜ğ‘’ğ‘¤ğ‘–(ğ‘„,ğ‘¥,ğ‘¦)=ğ¸ğ‘€ğ·(ğ‘ˆğ‘›ğ‘–ğ‘–(ğ‘„,ğ‘¥,ğ‘¦),ğ‘ƒğ·ğ¹ğ‘–(ğ‘„,ğ‘¥,ğ‘¦))\nWe store the bin boundaries of the histogram, so there is a simple map-\nping function from a value ğ‘to its binğ‘¥. Therefore, throughout this\nsection, we will use ğ‘†ğ‘˜ğ‘’ğ‘¤ğ‘–(ğ‘„,ğ‘,ğ‘)andğ‘†ğ‘˜ğ‘’ğ‘¤ğ‘–(ğ‘„,ğ‘¥,ğ‘¦)interchangeably.\n4.2.2 Grid Tree Design. Given a query workload that is skewed over\na data space, the aim of the Grid Tree is to divide the data space into a\nnumber of non-overlapping regions so that within each region, there\nis little query skew.\nThe Grid Tree is a space-partitioning decision tree, similar to a\nk-d tree. Each internal node of the Grid Tree divides space based on\nthe values in a particular dimension, called the split dimension ğ‘‘ğ‘ .\nUnlike a k-d tree, which is a binary tree, internal nodes of the Grid\nTree can split on more than one value. If an internal node splits on\nvaluesğ‘‰={ğ‘£1,...,ğ‘£ğ‘˜}, then the node has ğ‘˜+1children. To process\na query, we traverse the Grid Tree to find all regions that intersect\nwith the queryâ€™s filter predicates. If there is an index over the points\nin that region (e.g., an Augmented Grid), then we delegate the query\nto that index and aggregate the returned results. If there is no index\nfor the region, we simply scan all points in the region.\nNote that the Grid Tree is not meant to be an end-to-end index.\nInstead, the Grid Treeâ€™s purpose is to efficiently reduce query skew,\nwhile using low memory. This way, the user is free to use any index-\ning scheme within each region, without worrying about intra-region\nquery skew.\n4.3 Optimizing the Grid Tree\nGiven a dataset and sample query workload, our optimization goal is\nto reduce query skew as much as possible while maintaining a small\nand lightweight Grid Tree. We present the high-level optimization\nalgorithm, then dive into details. Our procedure is as follows: (1)\nGroup queries in the sample workload into some number of clusters,\nwhich we call query types (Â§4.3.1). (2) Build the Grid Tree in a greedy\nfashion. Start with a root node that is responsible for the entire data\nspaceğ‘†. Recursively, for each node ğ‘responsible for data space\nğ‘†ğ‘, pick the split dimension ğ‘‘ğ‘ âˆˆ[0,ğ‘‘)and the set of split values\nğ‘‰={ğ‘£1,...,ğ‘£ğ‘˜}that most reduce query skew (Â§4.3.2). ğ‘‘ğ‘ andğ‘‰define\nğ‘˜+1non-overlapping sub-spaces of ğ‘†ğ‘. Assign a child node to each\nof theğ‘˜+1sub-spaces and recurse for each child node. If a node\nğ‘has low query skew (Â§4.3.2), or has below a minimum thresholdnumber of intersecting points or queries, then it stops recursing and\nbecomes a leaf node, representing a region.\n4.3.1 Clustering Query Types. It is not enough to consider the query\nskew of the entire query set ğ‘„as a whole, because queries within\nthis set have different characteristics and therefore are best indexed\nin different ways. For example, we showed in Fig. 2 that ğ‘„ğ‘”andğ‘„ğ‘Ÿ\nare best indexed with different partitioning schemes. Considering\nall queries as a whole can mask the effects of skew because the skews\nof different query types can cancel each other out.\nTherefore, we cluster queries into types that have similar selectiv-\nity characteristics. First, queries that filter over different sets of di-\nmensions are automatically placed in different types. For each group\nof queries that filter over the same set of ğ‘‘â€²dimensions, we transform\neach query into a ğ‘‘â€²-dimensional embedding in which each value is\nset to the filter selectivity of the query over a particular dimension.\nWe run DBSCAN over the ğ‘‘â€²-dimensional embeddings with eps set\nto 0.2 (this worked well for all our experiments and we never tuned\nit). DBSCAN automatically determines the number of clusters. The\nchoice of clustering algorithm is orthogonal to the Grid Tree design.\nReal query workloads have patterns and can usually be divided\ninto types. For example, many analytic workloads are composed\nof query templates, for which the dimensions filtered and rough\nselectivity remains constant, but the specific predicate values vary.\nHowever, even if there are no patterns in the workload, the Grid Tree\nis still useful because there can still be query skew over a single query\ntype (i.e., query frequency varies in different parts of data space).\nFrom now on, we assume that if the query set ğ‘„is composed\nofğ‘¡query types, then we can divide ğ‘„intoğ‘¡subsetsğ‘„1,...,ğ‘„ğ‘¡. For\nexample, in Fig. 3 there are 2 types, ğ‘„ğ‘Ÿandğ‘„ğ‘”. Note that each query\ncan only belong to one query type, but queries in different types are\nallowed to overlap in data space. We now redefine skew:\nğ‘†ğ‘˜ğ‘’ğ‘¤ğ‘–(ğ‘„,ğ‘,ğ‘)=\n1â‰¤ğ‘–â‰¤ğ‘¡ğ‘†ğ‘˜ğ‘’ğ‘¤ğ‘–(ğ‘„ğ‘¡,ğ‘,ğ‘)\n4.3.2 Selecting the Split Dimension and Values. Given a Grid Tree\nnodeğ‘over a data space ğ‘†ğ‘and a set of queries ğ‘„that intersects\nwithğ‘†ğ‘, our goal is to find the split dimension ğ‘‘ğ‘ and split values\nover that dimension ğ‘‰={ğ‘£1,...,ğ‘£ğ‘˜}that achieve the largest reduction\nin query skew. For a dimension ğ‘–âˆˆ[0,ğ‘‘)and split values ğ‘‰, the\nreduction in query skew is defined as\nğ‘…ğ‘–(ğ‘„,0,ğ‘‹ğ‘‘,ğ‘‰)=ğ‘†ğ‘˜ğ‘’ğ‘¤ğ‘–(ğ‘„,0,ğ‘‹ğ‘‘)âˆ’h\nğ‘†ğ‘˜ğ‘’ğ‘¤ğ‘–(ğ‘„,0,ğ‘£1)\n+ğ‘†ğ‘˜ğ‘’ğ‘¤ğ‘–(ğ‘„,ğ‘£ğ‘˜,ğ‘‹ğ‘‘)+\n1â‰¤ğ‘–<ğ‘˜ğ‘†ğ‘˜ğ‘’ğ‘¤ğ‘–(ğ‘„,ğ‘£ğ‘–,ğ‘£ğ‘–+1)i\nNote that skew reduction is defined per dimension, not over all\nğ‘‘dimensions simultaneously. Therefore, we independently find\nthe largest skew reduction ğ‘…ğ‘šğ‘ğ‘¥ğ‘–=maxğ‘‰(ğ‘…ğ‘–)for each dimen-\nsionğ‘–âˆˆ[0,ğ‘‘)(explained next), then pick the split dimension ğ‘‘ğ‘ =\nargmaxğ‘–(ğ‘…ğ‘šğ‘ğ‘¥ğ‘–).\nFor example, in Fig. 3, ğ‘†ğ‘˜ğ‘’ğ‘¤ğ‘†ğ‘ğ‘™ğ‘’ğ‘  is already low because both query\ntypes are distributed relatively uniformly over Sales, so ğ‘…ğ‘šğ‘ğ‘¥ Sales\nis low. On the other hand, ğ‘†ğ‘˜ğ‘’ğ‘¤ğ‘Œğ‘’ğ‘ğ‘Ÿ is high. We can achieve very\nlargeğ‘…ğ‘šğ‘ğ‘¥ Yearusingğ‘‰={2019}. Therefore, we select ğ‘‘ğ‘ =Year and\nğ‘‰={2019}.\nIfmaxğ‘–(ğ‘…ğ‘šğ‘ğ‘¥ğ‘–)is below some minimum threshold (by default 5%\nof|ğ‘„|) or ifğ‘†ğ‘intersects below a minimum threshold of points or\n78\n\nFigure 4: Skew tree over the range [0,1000)with eight leaf\nnodes. The covering set that achieves lowest combined skew\nis shaded green. Based on the boundaries of the covering set,\nwe extract the split values ğ‘‰={250,375,500}.\nqueries (by default 1% of the total points or queries in the entire data\nspace), then ğ‘‘ğ‘ is rejected and ğ‘becomes a leaf Grid Tree node.\nWe now explain how to find the split values ğ‘‰that maximize ğ‘…ğ‘‘ğ‘ \nfor each candidate split dimension ğ‘‘ğ‘ âˆˆ[0,ğ‘‘). We introduce a data\nstructure called the skew tree, which is simply a tool to help find the\noptimalğ‘‰; it is never used when running queries. The skew tree is\na balanced binary tree (Fig. 4). Each node represents a range over\nthe domain of dimension ğ‘‘ğ‘ . The root node represents the entire\nrange[0,ğ‘‹ğ‘‘ğ‘ ), and every node represents the combined ranges of\nthe nodes in its subtree. A skew tree node whose range is [ğ‘,ğ‘)will\nstore the value ğ‘†ğ‘˜ğ‘’ğ‘¤ğ‘‘ğ‘ (ğ‘„,ğ‘,ğ‘). In other words, each skew tree node\nstores the query skew over the range it represents.\nCreating the skew tree requires ğ»ğ‘–ğ‘ ğ‘¡ğ‘‘ğ‘ (ğ‘„,0,ğ‘‹ğ‘‘ğ‘ ). By default,\nwe instantiate the histogram with 128 bins. Note that we are un-\nable to compute a meaningful skew over a single histogram bin:\nğ‘†ğ‘˜ğ‘’ğ‘¤ğ‘‘ğ‘ (ğ‘„,ğ‘¥,ğ‘¥+1)is always zero, because a single bin has no way\nto differentiate the uniform distribution from the query PDF. There-\nfore, the skew tree will only have 64 leaf nodes. However, if there\nare fewer than 128 unique values in dimension ğ‘‘ğ‘ , we create a bin for\neach unique value. In this case, there is truly no skew within each\nhistogram bin, so the skew tree has as many leaf nodes as unique\nvalues inğ‘‘ğ‘ , and the skew at each leaf node is 0.\nA set of skew tree nodes is called covering if their represented\nranges do not intersect and the union of their represented ranges is\n[0,ğ‘‹ğ‘‘ğ‘ ). We want to solve for the covering set with minimum com-\nbined query skew. This is simple to do via dynamic programming in\ntwo passes over the skew tree nodes: in the first pass, we start from\nthe leaf nodes and work towards the root node, and at each node we\nannotate the minimum combined query skew achievable over the\nnodeâ€™s subtree. In the second pass, we start from the root and work to-\nwards the leaves, and check if a nodeâ€™s skew is equal to the annotated\nskew: if so, the node is part of the optimal covering set. The bound-\naries between the ranges of nodes in the optimal covering set form ğ‘‰.\nAs a final step, we do a single ordered pass over all the nodes in\nthe covering set, in order of the range they represent, and merge\nnodes if the query skew of the combined node is not more than a\nconstant factor (by default, 10%) larger than the sum of the individual\nquery skews. For example, in Fig. 4 if ğ‘†ğ‘˜ğ‘’ğ‘¤ğ´(ğ‘„,0,375)<15Â·1.1, then\nthe first two nodes of the covering set would be merged, and 250\nwould be removed as a split value. This step counteracts the fact that\nthe binary tree may split at superfluous points, and it also acts as a\nregularizer that prevents too many splits.Table 2: Example skeleton over dimensions ğ‘‹,ğ‘Œ,ğ‘ , and all\nskeletons one â€œhopâ€ away. Restrictions are explained in\nÂ§5.2.1 and Â§5.2.2 (e.g., [ğ‘‹â†’ğ‘,ğ‘Œ|ğ‘‹,ğ‘]is not allowed).\nEx. skeleton[ğ‘‹,ğ‘Œ|ğ‘‹,ğ‘](i.e.,ğ¶ğ·ğ¹(ğ‘‹),ğ¶ğ·ğ¹(ğ‘Œ|ğ‘‹), andğ¶ğ·ğ¹(ğ‘))\nOne hop away[ğ‘‹,ğ‘Œ,ğ‘] [ğ‘‹,ğ‘Œ|ğ‘,ğ‘] [ğ‘‹,ğ‘Œâ†’ğ‘‹,ğ‘]\n[ğ‘‹,ğ‘Œâ†’ğ‘,ğ‘] [ğ‘‹,ğ‘Œ|ğ‘‹,ğ‘|ğ‘‹] [ğ‘‹,ğ‘Œ|ğ‘‹,ğ‘â†’ğ‘‹]\n5 AUGMENTED GRID\nIn this section, we describe the challenges posed by data correlations,\nand we introduce our solution to address those challenges: the Aug-\nmented Grid. Note that the Grid Tree (Â§4) optimizes only for query\nskew reduction, and the points within each region might still display\ncorrelation.\n5.1 Challenges of Data Correlation\nWe broadly define a pair of dimensions ğ‘‹andğ‘Œto be correlated if\nthey are not independent, i.e., if ğ¶ğ·ğ¹(ğ‘‹)â‰ ğ¶ğ·ğ¹(ğ‘‹|ğ‘Œ)and vice versa.\nIn the presence of correlated dimensions, it is not possible to impose\na grid that has equally-sized cells by partitioning each dimension\nindependently (see Fig. 1b). As a result, points will be clustered into\na relatively few number of cells, so any query that hits one of those\ncells will likely scan many more points than necessary.\nOne way to mitigate this issue is by increasing the number of par-\ntitions in each dimension, to form more fine-grained cells. However,\nincreasing the number of cells would counteract the two advantages\nof grids over trees: (1) Space overhead increases rapidly (e.g., dou-\nbling the number of partitions in each dimension increases index\nsize by 2ğ‘‘). (2) Time overhead also increases, because each cell incurs\na lookup table lookup. Therefore, simply making finer-grained grids\nis not a scalable solution to data correlations.\n5.2 A Correlation-Aware Grid\nTsunami handles data correlations while maintaining the time and\nspace advantage of grids by augmenting the basic grid structure\nwith new partitioning strategies that allow it to partition dimensions\ndependently instead of independently. We first provide a high level\ndescription of the Augmented Grid, then dive into details.\nAn Augmented Grid is a grid in which each dimension ğ‘‹âˆˆ[0,ğ‘‘)\nis divided into ğ‘ğ‘‹partitions and uses one of three possible strategies\nfor creating its partitions: (1) We can partition ğ‘‹independently of\nother dimensions, uniformly in ğ¶ğ·ğ¹(ğ‘‹). This is what Flood does for\nevery dimension. (2) We can remove ğ‘‹from the grid and transform\nquery filters over ğ‘‹into filters over some other dimension ğ‘Œâˆˆ[0,ğ‘‘)\nusing a functional mapping ğ¹:ğ‘‹â†’ğ‘Œ(Â§5.2.1). (3) We can partition ğ‘‹\ndependent on another dimension ğ‘Œâˆˆ[0,ğ‘‘), uniformly in ğ¶ğ·ğ¹(ğ‘‹|ğ‘Œ)\n(Â§5.2.2).\nA specific instantiation of partitioning strategies for all dimen-\nsions is called a skeleton. Tab. 2 shows an example. We â€œflesh outâ€\nthe skeleton by setting the number of partitions in each dimension\nto create a concrete instantiation of an Augmented Grid. Therefore,\nan Augmented Grid is uniquely defined by the combination of its\nskeletonğ‘†and number of partitions in each dimension ğ‘ƒ.\n5.2.1 Functional Mappings. A pair of dimensions ğ‘‹andğ‘Œis mono-\ntonically correlated if as values in ğ‘‹increase, values in ğ‘Œonly move\n79\n\nFigure 5: Functional mapping creates equally-sized cells and\nreducesscannedpointsfortightmonotoniccorrelations.The\nquery is in green, scanned points are red, and the mapping\nfunction is purple, with error bounds drawn as dashed lines.\nin one direction. Linear correlations are one subclass of monotonic\ncorrelations. For monotonically correlated ğ‘‹andğ‘Œ, we conceptually\ndefine a mapping function as a function ğ¹:R2â†’R2that takes a range\n[ğ‘Œğ‘šğ‘–ğ‘›,ğ‘Œğ‘šğ‘ğ‘¥]over dimension ğ‘Œand maps it to a range [ğ‘‹ğ‘šğ‘–ğ‘›,ğ‘‹ğ‘šğ‘ğ‘¥]\nover dimension ğ‘‹with the guarantee that any point whose value in\ndimensionğ‘Œis in[ğ‘Œğ‘šğ‘–ğ‘›,ğ‘Œğ‘šğ‘ğ‘¥]will have a value in dimension ğ‘‹in\n[ğ‘‹ğ‘šğ‘–ğ‘›,ğ‘‹ğ‘šğ‘ğ‘¥]. In this case, we call ğ‘Œthemapped dimension and we\ncallğ‘‹thetarget dimension. For simplicity, we place a restriction: a\ntarget dimension cannot itself be a mapped dimension. Similar ideas\nwere proposed in [20, 45].\nConcretely, we implement the mapping function as a simple linear\nregressionğ¿ğ‘…trained to predict ğ‘‹fromğ‘Œ, with lower and upper error\nboundsğ‘’ğ‘™andğ‘’ğ‘¢. Therefore, a functional mapping is encoded in four\nfloating point numbers and has negligible storage overhead. Given a\nrange[ğ‘Œğ‘šğ‘–ğ‘›,ğ‘Œğ‘šğ‘ğ‘¥], the mapping function produces ğ‘‹ğ‘šğ‘–ğ‘›=ğ‘Œğ‘šğ‘–ğ‘›âˆ’ğ‘’ğ‘™\nandğ‘‹ğ‘šğ‘ğ‘¥=ğ‘Œğ‘šğ‘ğ‘¥+ğ‘’ğ‘¢. Note that the idea of functional mappings can\ngeneralize to all monotonic correlations, as in [ 45]. However, in our\nexperience the vast majority of monotonic correlations in real data\nare linear, so we use linear regressions for simplicity.\nGiven a functional mapping, any range filter predicate (ğ‘¦0â‰¤ğ‘Œâ‰¤\nğ‘¦1)over dimension ğ‘Œcan be transformed into a semantically equiv-\nalent predicate(ğ‘¥0â‰¤ğ‘‹â‰¤ğ‘¥1)over dimension ğ‘‹, where(ğ‘¥0,ğ‘¥1)=\nğ¹(ğ‘¦0,ğ‘¦1). This gives us the opportunity to completely remove the\nmapped dimension from the ğ‘‘-dimensional grid, to obtain equally-\nsized cells. Fig. 5 demonstrates the benefits of functional mapping.\nThe grid without functional mapping has unequally-sized cells,\nwhich results in many points scanned. On the other hand, the grid\nwith functional mapping has equally-sized cells and is furthermore\nable to â€œshrinkâ€ the size of the query to a semantically equivalent\nquery by inducing a narrower filter over dimension X using the\nmapping function. This results in fewer points scanned.\n5.2.2 Conditional CDFs. Functional mappings are only useful for\ntight monotonic correlations. Otherwise, the error bounds would\nbe too large for the mapping to be useful. For loose monotonic corre-\nlations or generic correlations, we instead use conditional CDFs. For\na pair of generically correlated dimensions ğ‘‹andğ‘Œ, we partition ğ‘‹\nuniformly in ğ¶ğ·ğ¹(ğ‘‹)and we partition ğ‘Œuniformly in ğ¶ğ·ğ¹(ğ‘Œ|ğ‘‹),\nresulting in equally-sized cells. In this case, we call ğ‘‹thebase di-\nmension andğ‘Œthedependent dimension. For simplicity, we place\nFigure 6: Conditional CDFs create equally-sized cells and\nreduce scanned points for generic correlations. The query is\nin green, and scanned points are in red.\nrestrictions: a base dimension cannot itself be a mapped dimension\nor a dependent dimension.\nConcretely, if there are ğ‘ğ‘‹andğ‘ğ‘Œpartitions over ğ‘‹andğ‘Œrespec-\ntively, we implement ğ¶ğ·ğ¹(ğ‘Œ|ğ‘‹)by storingğ‘ğ‘‹histograms over ğ‘Œ,\none for each partition in ğ‘‹. When a query filters over ğ‘Œ, we first\nfind all intersecting partitions in ğ‘‹, then for each ğ‘‹partition inde-\npendently invoke ğ¶ğ·ğ¹(ğ‘Œ|ğ‘‹)to find the intersecting partitions in\nğ‘Œ. The storage overhead is proportional to ğ‘ğ‘‹ğ‘ğ‘Œ, which is minimal\ncompared to the existing overhead of the gridâ€™s lookup table, which\nis proportional toË›\nğ‘–âˆˆ[0,ğ‘‘)ğ‘ğ‘–.\nFig. 6 shows an example of using conditional CDFs. Both grids\nhaveğ‘ğ‘‹=ğ‘ğ‘Œ=4. By partitioning ğ‘Œusingğ¶ğ·ğ¹(ğ‘Œ|ğ‘‹), the grid on the\nright has staggered partition boundaries, which create equally-sized\ncells and results in fewer points scanned. Additionally, the regions\noutside the cells (shaded in gray) are guaranteed to have no points,\nwhich allows the query to avoid scanning the first and last partitions\nofğ‘‹, even though they intersect the query.\n5.3 Optimizing the Augmented Grid\nGiven a dataset and sample query workload, our optimization goal\nis to find the best Augmented Grid, i.e., the settings of the parame-\nters(ğ‘†,ğ‘ƒ)that achieves lowest average query time over the sample\nworkload, where ğ‘†is the skeleton and ğ‘ƒis the number of partitions\nin each dimension.\nThis optimization problem is challenging in two ways: (1) For a\nspecificsettingof(ğ‘†,ğ‘ƒ),wecannotknowtheaveragequerytimewith-\nout actually running the queries, which can be very time-intensive.\nTherefore, we create a cost model to predict average query time, and\nwe optimize for lowest average predicted query time (Â§5.3.1). (2) The\nsearch space over skeletons is exponentially large. For each dimen-\nsion, there are ğ‘‚(ğ‘‘)possible partitioning strategies, since there are\nup toğ‘‘âˆ’1choices for the other dimension in a functional mapping\nor conditional CDF. Therefore, the search space of skeletons has size\nğ‘‚(ğ‘‘ğ‘‘). To efficiently navigate the joint search space of (ğ‘†,ğ‘ƒ), we use\nadaptive gradient descent (Â§5.3.2).\n5.3.1 Cost Model. We use a simple analytic linear cost model to\npredict the runtime of a query ğ‘on datasetğ·and an instantiation\nof the Augmented Grid with parameters (ğ‘†,ğ‘ƒ):\nTime=ğ‘¤0(# cell ranges)+ ğ‘¤1(# scanned points)( # filtered dims)\n80\n\nWe now explain each term of this model. A set of adjacent cells in\nphysical storage is called a cell range. Instead of doing a lookup on\nthe lookup table for every intersecting cell, we only look up the first\nand last cell of a cell range. Furthermore, skipping to each new cell\nrange in physical storage likely incurs a cache miss. ğ‘¤0represents\nthe time to do a lookup and the cache miss of accessing the range\nin physical storage.\nTheğ‘¤1term models the time to scan points (e.g., all red points\nin previous figures). Since data is stored in a column store, only the\ndimensions filtered by the query need to be accessed. ğ‘¤1represents\nthe time to scan a single dimension of a single point.\nImportantly, the features of this cost model can be efficiently com-\nputed or estimated: the number of cell ranges is easily computed\nfromğ‘and(ğ‘†,ğ‘ƒ). The number of filtered dimensions is obvious from\nğ‘. The number of scanned points is estimated using ğ‘,(ğ‘†,ğ‘ƒ), and a\nsample ofğ·.\nNote that we do not model the time to actually perform the ag-\ngregation after finding the points that intersect the query rectangle.\nThis is because aggregation is a fixed cost that must be incurred\nregardless of index choice, so we ignore it when optimizing.\n5.3.2 Adaptive Gradient Descent. We find the(ğ‘†,ğ‘ƒ)that minimizes\naverage query time, as predicted by the cost model, using adaptive\ngradient descent (AGD). We first enumerate AGDâ€™s high level steps,\nthen provide details for each step. AGD is an iterative algorithm that\njointly optimizes ğ‘†andğ‘ƒ:\n(1) Using heuristics, initialize (ğ‘†0,ğ‘ƒ0).\n(2)From(ğ‘†0,ğ‘ƒ0), take a gradient descent step over ğ‘ƒ0using the cost\nmodel as the objective function, which gives us (ğ‘†0,ğ‘ƒ1).\n(3)From(ğ‘†0,ğ‘ƒ1), perform a local search over skeletons to find the\nskeletonğ‘†â€²that minimizes query time for (ğ‘†â€²,ğ‘ƒ1). Setğ‘†1=ğ‘†â€². It\nmay be that ğ‘†â€²=ğ‘†0, that is, the skeleton does not change in this\nstep.\n(4)Repeat steps 2 and 3 starting from (ğ‘†1,ğ‘ƒ1)until we reach a min-\nimum average query time.\nIn step 1, we first initialize ğ‘†, thenğ‘ƒ. We make a best guess at the\noptimal skeleton using heuristics: for each dimension ğ‘‹, use a func-\ntional mapping to dimension ğ‘Œif the error bound is below 10% of ğ‘Œâ€™s\ndomain. Else, partition using ğ¶ğ·ğ¹(ğ‘‹|ğ‘Œ)if not doing so would result\nin more than 25% of cells in the ğ‘‹ğ‘Œgrid hyperplane being empty.\nElse, partition ğ‘‹independently using ğ¶ğ·ğ¹(ğ‘‹). Given the initial ğ‘†,\nwe initialize ğ‘ƒproportionally to the average query filter selectivity\nin each grid dimension (i.e., excluding mapped dimensions).\nIn step 2, we use the insight that the cost model is relatively smooth\ninğ‘ƒ: changing the number of partitions usually smoothly increases\nor decreases the cost. Therefore, we take the numerical gradient over\nğ‘ƒat(ğ‘†,ğ‘ƒ)and take a step in the gradient direction.\nIn step 3, we take advantage of the insight that an incremental\nchange inğ‘ƒis unlikely to cause the skeleton ğ‘†â€²to differ greatly from\nğ‘†. Therefore, step 3 will only search over ğ‘†â€²that can be created by\nchanging the partitioning strategy for a single dimension in ğ‘†(e.g.,\nskeletons one â€œhopâ€ away in Tab. 2).\nWhile we could conceivably use black box optimization methods\nsuch as simulated annealing to optimize (ğ‘†,ğ‘ƒ), AGD takes advantage\nof the aforementioned insights into the behavior of the optimization\nand is therefore able to find lower-cost Augmented Grids, which we\nconfirm in Â§6.6.6 EVALUATION\nWe first describe the experimental setup and then present the re-\nsults of an in-depth experimental study that compares Tsunami with\nFlood and several other indexing methods on a variety of datasets\nand workloads. Overall, this evaluation shows that:\n(1)Tsunami is consistently the fastest index across tested datasets\nand workloads. It achieves up to 6 Ã—higher query throughput\nthan Flood and up to 11 Ã—higher query throughput than the\nfastestoptimally-tunednon-learnedindex.Furthermore,Tsunami\nhas up to 8Ã—smaller index size than Flood and up to 170 Ã—smaller\nindex size than the fastest non-learned index (Â§6.3).\n(2)Tsunami can optimize its index layout and reorganize the records\nquickly for a new query distribution, typically in under 4 minutes\nfor a 300 million record dataset (Â§6.4).\n(3)Tsunamiâ€™s performance advantage over other indexes scales\nwith dataset size, selectivity, and dimensionality (Â§6.5).\n6.1 Implementation and Setup\nWe implement Tsunami in C++ and perform optimization in Python.\nWe perform our query performance evaluation via single-threaded\nexperiments on an Ubuntu Linux machine with Intel Core i9-9900K\n3.6GHz CPU and 64GB RAM. Optimization and data sorting for index\ncreation are performed in parallel for Tsunami and all baselines.\nAll experiments use 64-bit integer-valued attributes. Any string\nvalues are dictionary encoded prior to evaluation. Floating point\nvalues are typically limited to a fixed number of decimal points (e.g.,\n2 for price values). We scale all values by the smallest power of 10\nthat converts them to integers.\nEvaluation is performed on data stored in a custom column store\nwith one scan-time optimization: if the range of data being scanned\nisexact, i.e., we are guaranteed ahead of time that all elements within\nthe range match the query filter, we skip checking each value against\nthe query filter. For common aggregations, e.g. COUNT , this removes\nunnecessary accesses to the underlying data.\nWe compare Tsunami to other solutions implemented on the same\ncolumn store, with the same optimizations, if applicable:\n(1)Clustered Single-Dimensional Index : Points are sorted by the\nmost selective dimension in the query workload. If a query filter\ncontains this dimension, we locate the endpoints using binary\nsearch. Otherwise, we perform a full scan.\n(2)TheZ-Order Index is a multidimensional index that orders points\nby their Z-value [14]; contiguous chunks are grouped into pages.\nGiven a query, the index finds the smallest and largest Z-value\ncontained in the query rectangle and iterates through each page\nwith Z-values in this range. Pages maintain min/max metadata\nper dimension to prune irrelevant pages.\n(3)The Hyperoctree [27] recursively subdivides space equally into\nhyperoctants (the ğ‘‘-dimensional analog to 2-dimensional quad-\nrants), until the number of points in each leaf is below a prede-\nfined but tunable page size.\n(4)The k-d tree [4] recursively partitions space using the median\nvalue along each dimension, until the number of points in each\nleaf falls below the page size. The dimensions are selected in a\nround robin fashion, in order of selectivity.\n(5)Flood, introduced in Â§2.2. We use the implementation of [ 30] with\ntwo changes: we use Tsunamiâ€™s cost model instead of Floodâ€™s\n81\n\nTable 3: Dataset and query characteristics.\nTPC-H. Taxi Perfmon Stocks\nrecords 300M 184M 236M 210M\nquery types 5 6 5 5\ndimensions 8 9 7 7\nsize (GB) 19.2 13.2 13.2 11.8\noriginal random-forest-based cost model, and we perform re-\nfinement using binary search instead of learned per-cell models\n(see [ 30] for details). We verified that these changes did not mean-\ningfully impact performance. Furthermore, removing per-cell\nmodels dramatically reduces Floodâ€™s index size (on average by\n20Ã—[30]), and this allows us to more directly evaluate the impact\nof design differences between Flood and Tsunami without any\nconfounding effects from implementation differences.\nThere are a number of other multi-dimensional indexing techniques,\nsuch as Grid Files [ 31], UB-tree [ 36], and Râˆ—-Tree [ 3]. We decided not\nto evaluate against these because Flood already showed consistent\nsuperiority over them [ 30]. We also do not evaluate against other\nlearned multi-dimensional indexes because they are either optimized\nfor disk [ 25,46] or optimize only based on the data distribution, not\nthe query workload [9, 44] (see Â§7).\n6.2 Datasets and Workloads\nWe evaluate indexes on three real-world and one synthetic dataset,\nsummarized in Tab. 3. Queries are synthesized for each dataset, and\ninclude a mix of range filters and equality filters. The queries for each\ndataset comes from a certain number of query types (Â§4.3.1), each\nof which answers a different analytics question, with 100 queries of\neach type. All queries perform a COUNT aggregation. Since all indexes\nmust pay the same fixed cost of aggregation, performing different\naggregations would not change the relative ordering of indexes in\nterms of query performance.\nTheTaxi dataset comes from records of yellow taxi trips in New\nYork City in 2018 and 2019 [ 32]. It includes fields capturing pick-\nup and drop-off dates/times, pick-up and drop-off locations, trip\ndistances, itemized fares, and driver-reported passenger counts.\nOur queries answer questions such as â€œHow common were single-\npassenger trips between two particular parts of Manhattan?â€ and\nâ€œWhat month of the past year saw the most short-distance trips?â€.\nQueries display skew over time (more queries over recent data), pas-\nsenger count (different query types about very low and very high\npassenger counts), and trip distance (more queries about very short\ntrip distances). Query selectivity varies from 0.25%to3.9%, with an\naverage of 1.3%.\nThe performance monitoring dataset Perfmon contains logs of\nall machines managed by a major US university over the course of\na year. It includes fields capturing log time, machine name, CPU\nusages, and system load averages. Our queries answer questions\nsuch as â€œWhen in the last month did a certain set of machines experi-\nence high load?â€. Queries display skew over time (more queries over\nrecent data) and CPU usage (more queries over high usage). Query\nselectivity varies from 0.50%to4.9%, with an average of 0.79%. The\noriginal dataset has 23.6M records, but we use a scaled dataset with\n236M records.Table 4: Index Statistics after Optimization.\nTPC-H Taxi Perfmon Stocks\nTsunami\nNum Grid Tree nodes 39 35 42 54\nGrid Tree depth 4 2 4 4\nNum leaf regions 27 31 36 39\nMin points per region 3.5M 1.9M 2.6M 2.4M\nMedian points per region 5.9M 3.3M 3.7M 3.2M\nMax points per region 10M 6.7M 26M 41M\nAvg FMs per region 0.67 0.55 0 1.1\nAvg CCDFs per region 1.3 1.9 1.75 1.8\nTotal num grid cells 1.5M 99K 80K 220K\nFlood\nNum grid cells 920K 840K 530K 250K\nTheStocks dataset consists of daily historical stock prices of over\n6000 stocks from 1970 to 2018 [ 12]. It includes fields capturing daily\nprices (open, close, adjusted close, low, and high), trading volume,\nand the date. Our queries answer questions such as â€œWhich stocks\nsaw the lowest intra-day price change while trading at high vol-\nume?â€ and â€œWhat one-year span in the past decade saw the most\nstocks close in a certain price range?â€. Queries display skew over\ntime (more queries over recent data) and volume (different query\ntypes about very low and very high volume). Query selectivity is\ntightly concentrated around 0.5%Â±0.04%. The original dataset has\n21M records, but we use a scaled dataset with 210M records.\nOur last dataset is TPC-H [42]. For our evaluation, we use only\nthe fact table, lineitem , with 300M records (scale factor 50) and\ncreate queries by using filters commonly found in the TPC-H query\nworkload. Our queries include filters over quantity, extended price,\ndiscount, tax, ship mode, ship date, commit date, and receipt date.\nThey answer questions such as â€œHow many high-priced orders in the\npast year used a significant discount?â€ and â€œHow many shipments\nby air had below ten items?â€. Query selectivity varies from 0.40%to\n0.64%, with an average of 0.54%.\n6.3 Overall Results\nFig. 7 compares Tsunami to Flood and the non-learned baselines.\nTsunami and Flood are automatically optimized for each dataset/-\nworkload. For the non-learned baselines, we tuned the page size to\nachieve best performance on each dataset/workload. Tsunami is con-\nsistently the fastest of all the indexes across datasets and workloads,\nand achieves up to 6 Ã—faster queries than Flood and up to 11 Ã—faster\nqueries than the fastest non-learned index.\nTab. 4 shows statistics of the optimized Tsunami index structure.\nThe Grid Tree depth and the number of leaf regions are relatively low,\nwhich confirms that the Grid Tree is lightweight, as desired. Because\nskew does not occur uniformly across data space, the number of\npoints in each region can vary by over an order of magnitude.\nThe Grid Tree typically has a low number of nodes (Tab. 4), so the\nvast majority of Tsunamiâ€™s index size comes from the cell lookup ta-\nbles for the Augmented Grids in each region. Tsunami often has fewer\ntotal grid cells than Flood (Tab. 4) because partitioning space via the\nGridTreegivesTsunamifine-grainedcontroloverthenumberofcells\n82\n\nFigure 7: Tsunami achieves up to 6Ã— faster queries than Flood and up to 11Ã— faster queries than the fastest non-learned index.\nFigure 8: Tsunami uses up to 8Ã— less memory than Flood and 7-170Ã— less memory than the fastest tuned non-learned index.\nto allocate in each region, whereas Flood must often over-provision\npartitions to deal with query skew (see Â§4.1). Fig. 8 shows that as a\nresult of having fewer cells, Tsunami uses up to 8 Ã—less memory than\nFlood. Furthermore, Tsunami is between 7 Ã—to 170Ã—smaller than the\nfastest optimally-tuned non-learned index across the four datasets.\n6.4 Adaptibility\nTsunami is able to quickly adapt to changes in the query work-\nload by re-optimizing its layout for the new query workload and\nre-organizing the data based on the new layout. In Fig. 9a, we simu-\nlate a scenario in which the query workload over the TPC-H dataset\nchanges at midnight: the original query workload is replaced by a\nnew workload with queries drawn from five new query types. This\ncauses performance on the learned indexes to degrade. Tsunami\n(as well as Flood) automatically detects the workload shift (see Â§8)\nand triggers a re-optimization of the index layout for the new query\nworkload. Tsunamiâ€™s re-optimization and data re-organization over\n300M rows finish within 4 minutes, and its high query performance\nis restored. This shows that Tsunami is highly adaptive for scenarios\nin which the data or workload changes infrequently (e.g., every day).\nThe non-learned indexes are not re-tuned after the workload shift, be-\ncause in practical settings, it is unlikely that a database administrator\nwill be able to manually tune the index for every workload change.\nFig. 9b shows the index creation time in detail for Tsunami and the\nbaselines. All indexes require time to sort the data based on the index\nlayout, shown as solid bars. The learned approaches additionally\nrequire time to perform optimization based on the dataset and query\nworkload, shown as the hatched bars. Even for the largest datasets,\nthe entire index creation time for Tsunami remains below 4 minutes.\nFigure 9: (a) After the query workload changes at midnight,\nTsunami re-optimizes and re-organizes within 4 minutes\nto maintain high performance. (b) Comparison of index\ncreation times (solid bars = data sorting time, hatched bars =\noptimization time).\n6.5 Scalability\nThroughout this subsection, Tsunami and Flood are re-optimized for\neach dataset/workload configuration, while the non-learned indexes\nuse the same page size and dimension ordering as they were tuned\nfor the full TPC-H dataset/workload in Â§6.3.\nNumber of Dimensions. To show how Tsunami scales with dimen-\nsions, and how correlation affects scalability, we create two groups\nof synthetic ğ‘‘-dimensional datasets with 100M records. Within each\ngroup, datasets vary by number of dimensions ( ğ‘‘âˆˆ{4,8,12,16,20}).\nDatasets in the first group show no correlation and points are sam-\npled from i.i.d. uniform distributions. For datasets in the second\ngroup, half of the dimensions have uniformly sampled values, and di-\nmensions in the other half are linearly correlated to dimensions in the\nfirst half, either strongly ( Â±1%error) or loosely (Â±10%error). For each\n83\n\nFigure 10: Tsunami continues to outperform other indexes\nat higher dimensions.\nFigur\ne 11: Tsunami maintains high performance across\ndataset sizes and query selectivities.\ndataset, we create a query workload with four query types. Earlier di-\nmensions are filtered with exponentially higher selectivity than later\ndimensions, and queries are skewed over the first four dimensions.\nFig. 10 shows that in both cases, Tsunami continues to outperform\nthe other indexes at higher dimensions. In particular, the Augmented\nGrid is able to take advantage of correlations to effectively reduce the\ndimensionality of the dataset. This helps Tsunami delay the curse of\ndimensionality: Tsunami has around the same performance on each\nğ‘‘-dimensionalcorrelateddatasetasitdoesonthe (ğ‘‘âˆ’4)-dimensional\nuncorrelated dataset.\nDataset Size. To show how Tsunami scales with dataset size, we\nsample records from the TPC-H dataset to create smaller datasets.\nWe run the same query workload as on the full dataset. Fig. 11a\nshows that across dataset sizes, Tsunami maintains its performance\nadvantage over Flood and non-learned indexes.\nQuery Selectivity. To show how Tsunami performs at different\nquery selectivities, we use the 8-dimensional synthetic dataset/-\nworkload with correlation (explained above) and scale filter ranges\nequally in each dimension in order to achieve between 0.001% and\n10% selectivity. Fig. 11b shows that Tsunami performs well at all selec-\ntivities. The relative performance benefit of Tsunami is less apparent\nat 10% selectivity because aggregation time becomes a bottleneck.\n6.6 Drill-down into Components\nFig. 12a shows the relative performance of only using the Augmented\nGrid (i.e., one Augmented Grid over the entire data space) and of\nonly using Grid Tree (i.e., with an instantiation of Flood in each\nleaf region). Grid Tree contributes the most to Tsunamiâ€™s perfor-\nmance, but Augmented Grid also boosts performance significantly\nFigur\ne 12: (a) Augmented Grid and Grid Tree both contribute\nto Tsunamiâ€™s performance. (b) Comparison of optimization\nmethods. Bars show the predicted query time according to\nour cost model. Error bars show the actual query time.\nover Flood. Grid Tree-only performs almost as well as Tsunami be-\ncause partitioning data space via the Grid Tree often already has the\nunintentional but useful side effect of mitigating data correlations.\nWe now evaluate Augmented Gridâ€™s optimization procedure,\nwhich can be broken into two independent parts: the accuracy of\nthe cost model (Â§5.3.1) and the ability of Adaptive Gradient Descent\n(Â§5.3.2) to minimize cost (i.e., average query time, predicted by the\ncost model). For each of our four datasets/workloads, we run Adap-\ntive Gradient Descent (AGD) to find a low-cost Augmented Grid\nover the entire data space. We compare with three alternative opti-\nmization methods, all using the same cost model:\n(1)Gradient Descent (GD) uses the same initial (ğ‘†0,ğ‘ƒ0)as AGD, then\nperforms gradient descent over ğ‘ƒ, without ever changing the\nskeleton.\n(2)Black Box starts with the same initial (ğ‘†0,ğ‘ƒ0)as AGD, then op-\ntimizesğ‘†andğ‘ƒaccording to the basin hopping algorithm, im-\nplemented in SciPy [38], for 50 iterations.\n(3)AGD with naive initialization (AGD-NI) sets the initial skeleton\nğ‘†0to useğ¶ğ·ğ¹(ğ‘‹)for each dimension, then runs AGD.\nFig. 12b shows the lowest cost achieved by each optimization method.\nThere are several insights. First, Black Box performs worse than the\ngradient descent variants, which implies that using domain knowl-\nedgeandheuristicstoguidethesearchprocessprovidesanadvantage.\nSecond, Adaptive Gradient Descent usually achieves only marginally\nbetter predicted query time than Gradient Descent, which implies\nthat for our tested datasets, our heuristics created a good initial\nskeletonğ‘†0. Third, Adaptive Gradient Descent is able to find a low-\ncost grid even when starting from a naive skeleton, which implies\nthat the local search over skeletons is able to effectively switch to\nbetter skeletons. For the Taxi dataset, AGD-NI is even able to find\na lower-cost configuration than AGD.\nFig. 12b additionally shows the error between the predicted query\ntime using the cost model and the actual query time when running\nthe queries of the workload. The average error of the model for all\noptimized configurations shown in Fig. 12b is only 15%.\n7 RELATED WORK\nTraditional Multi-dimensional Indexes. There is a rich corpus\nof work dedicated to multi-dimensional indexes, and many com-\nmercial database systems have turned to multi-dimensional index-\ning schemes. For example, Amazon Redshift organizes points by\n84\n\nZ-order [ 29], which maps multi-dimensional points onto a single di-\nmension for sorting [ 1,34,47]. With spatial dimensions, SQL Server\nallows Z-ordering [ 28], and IBM Informix uses an R-Tree [ 16]. Other\nmulti-dimensional indexes include K-d trees, octrees, Râˆ—trees, UB\ntrees (which also make use of the Z-order), and Grid Files [ 31], among\nmany others (see [ 33,40] for a survey). There has also been work\non automatic index selection [ 6,26,43]. However, these approaches\nmainly focus on creating secondary indexes, whereas Tsunami co-\noptimizes the index and data storage.\nLearned Indexes. Recent work by Kraska et al. [ 23] proposed the\nidea of replacing traditional database indexes with learned models\nthat predict the location of a key in a dataset. Their learned index,\ncalled the Recursive Model Index (RMI), and various improvements\non the RMI [10, 13, 15, 22, 41], only handle one-dimensional keys.\nSince then, there has been a corpus of work on extending the\nideas of the learned index to spatial and multi-dimensional data. The\nmost relevant work is Flood [ 30], described in Â§2.2. Learning has\nalso been applied to the challenge of reducing I/O cost for disk-based\nmulti-dimensional indexes. Qd-tree [ 46] uses reinforcement learn-\ning to construct a partitioning strategy that minimizes the number\nof disk-based blocks accessed by a query. LISA [ 25] is a disk-based\nlearned spatial index that achieves low storage consumption and\nI/O cost while supporting range queries, nearest neighbor queries,\nand insertions and deletions. Tsunami and these works share the\nidea that a multi-dimensional index can be instance-optimized for a\nparticular use case by learning from the dataset and query workload.\nPast work has also aimed to improve traditional indexing tech-\nniquesbylearningthedatadistribution.TheZM-index[ 44]combines\nthe standard Z-order space-filling curve [ 29] with the RMI from [ 23]\nby mapping multi-dimensional values into a single-dimensional\nspace, which is then learnable using models. The ML-index [ 9] com-\nbines the ideas of iDistance [ 18] and the RMI to support range and\nKNN queries. Unlike Tsunami, these works only learn from the data\ndistribution, not from the query workload.\nData Correlations. There is a body of work on discovering and tak-\ning advantage of column correlations. BHUNT [ 5], CORDS [ 17],\nand Pyro [ 24] automatically discover algebraic constraints, soft\nfunctional dependencies, and approximate dependencies between\ncolumns, respectively. CORADD [ 21] recommends materialized\nviews and indexes based on correlations. Correlation Map [ 20] aims\nto reduce the size of B+Tree secondary indexes by creating a mapping\nbetween correlated dimensions. Hermit [ 45] is a learned secondary\nindex that achieves low space usage by capturing monotonic correla-\ntions and outliers between dimensions. Although the functional map-\npings in the Augmented Grid are conceptually similar to Correlation\nMap and Hermit, our work is more focused on how to incorporate\ncorrelation-aware techniques into a multi-dimensional index.\nQuery Skew. The existence of query skew has been extensively re-\nported in settings where data is accessed via single-dimensional keys\n(i.e., â€œhot keysâ€) [ 2,7,48]. In particular, key-value store workloads\nat Facebook display strong key-space locality: hot keys are closely\nlocated in the key space [ 48]. Instead of relying on caches to reduce\nquery time for frequently accessed keys, Tsunami automatically\npartitions data space using the Grid Tree to account for query skew.8 FUTURE WORK\nComplex Correlations. Augmented Gridâ€™s functional mappings\nare not robust to outliers: one outlier can significantly increase the\nerror bound of the mapping. We can address this by placing outliers\nin a separate buffer, similar to Hermit [ 45]. Furthermore, Augmented\nGrid might not efficiently capture more complex correlation pat-\nterns, such as temporal/periodic patterns and correlations due to\nfunctional dependencies over more than two dimensions. To handle\nthese correlations, we intend to introduce new correlation-aware\npartitioning strategies to the Augmented Grid.\nData and Workload Shift. Tsunami can quickly adapt to work-\nload changes but does not currently have a way to detect when\nthe workload characteristics have changed sufficiently to merit re-\noptimization. To do this, Tsunami could detect when an existing\nquery type (Â§4.3.1) disappears, a new query type appears, or when the\nrelative frequencies of query types change. Tsunami could also detect\nwhen the query skew of a particular Grid Tree region has deviated\nfrom its skew after the initial optimization. Additionally, Tsunami is\ncompletely re-optimized for each new workload. However, Tsunami\ncould be incrementally adjusted, e.g. by only re-optimizing the Aug-\nmented Grids whose regions saw the most significant workload shift.\nTsunami currently only supports read-only workloads. To sup-\nport dynamic data, each leaf node in the Grid Tree could maintain a\nsibling node that acts as a delta index [ 39] in which inserts, updates,\nand deletes are buffered and periodically merged into the main node.\nPersistence Tsunamiâ€™s techniques for reducing query skew and\nhandling correlations are not restricted to in-memory scenarios and\ncould be incorporated into an index for data resident on disk or SSD,\nperhaps by combining ideas from qd-tree [46] or LISA [25].\n9 CONCLUSION\nRecent work has introduced the idea of learned multi-dimensional\nindexes, which outperform traditional multi-dimensional indexes\nby co-optimizing the index layout and data storage for a particular\ndataset and query workload. We design Tsunami, a new in-memory\nlearned multi-dimensional index that pushes the boundaries of per-\nformance by automatically adapting to data correlations and query\nskew. Tsunami introduces two modular data structuresâ€”Grid Tree\nand Augmented Gridâ€”that allow it to outperform existing learned\nmulti-dimensional indexes by up to 6 Ã—in query throughput and 8 Ã—\nin space. Our results take us one step closer towards a robust learned\nmulti-dimensional index that can serve as a building block in larger\nin-memory database systems.\nAcknowledgements. This research is supported by Google, Intel,\nand Microsoft as part of the MIT Data Systems and AI Lab (DSAIL)\nat MIT, NSF IIS 1900933, DARPA Award 16-43-D3M-FP040, and the\nMIT Air Force Artificial Intelligence Innovation Accelerator (AIIA).\nResearch was sponsored by the United States Air Force Research\nLaboratory and was accomplished under Cooperative Agreement\nNumber FA8750-19-2-1000. The views and conclusions contained in\nthis document are those of the authors and should not be interpreted\nas representing the official policies, either expressed or implied, of the\nUnited States Air Force or the U.S. Government. The U.S. Government\nis authorized to reproduce and distribute reprints for Government\npurposes notwithstanding any copyright notation herein.\n85\n\nREFERENCES\n[1]Amazon AWS. 2016. Amazon Redshift Engineeringâ€™s Advanced Table Design Play-\nbook: Compound and Interleaved Sort Keys. https://aws.amazon.com/blogs/big-\ndata/amazon-redshift-engineerings-advanced-table-design-playbook-\ncompound-and-interleaved-sort-keys/.\n[2]Berk Atikoglu, Yuehai Xu, Eitan Frachtenberg, Song Jiang, and Mike Paleczny.\n2012. Workload Analysis of a Large-Scale Key-Value Store. In Proceedings\nof the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference\non Measurement and Modeling of Computer Systems (London, England, UK)\n(SIGMETRICS â€™12). Association for Computing Machinery, New York, NY, USA,\n53â€“64. https://doi.org/10.1145/2254756.2254766\n[3]Norbert Beckmann, Hans-Peter Kriegel, Ralf Schneider, and Bernhard Seeger. 1990.\nThe R*-Tree: An Efficient and Robust Access Method for Points and Rectangles.\nSIGMOD Rec. 19, 2 (May 1990), 322â€“331. https://doi.org/10.1145/93605.98741\n[4]Jon Louis Bentley. 1975. Multidimensional Binary Search Trees Used\nfor Associative Searching. Commun. ACM 18, 9 (Sept. 1975), 509â€“517.\nhttps://doi.org/10.1145/361002.361007\n[5]Paul Brown and Peter J. Haas. 2003. BHUNT: Automatic Discovery of Fuzzy\nAlgebraic Constraints in Relational Data. In VLDB.\n[6]Surajit Chaudhuri and Vivek Narasayya. 1997. An Efficient, Cost-Driven Index\nSelection Tool for Microsoft SQL Server. In Proceedings of the VLDB Endowment.\nVLDB Endowment.\n[7]Brian F. Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell\nSears. 2010. Benchmarking Cloud Serving Systems with YCSB. In Proceedings\nof the 1st ACM Symposium on Cloud Computing (Indianapolis, Indiana, USA)\n(SoCC â€™10). Association for Computing Machinery, New York, NY, USA, 143â€“154.\nhttps://doi.org/10.1145/1807128.1807152\n[8]Databricks Engineering Blog. [n.d.]. Processing Petabytes of Data in Seconds with\nDatabricks Delta. https://databricks.com/blog/2018/07/31/processing-petabytes-\nof-data-in-seconds-with-databricks-delta.html.\n[9]Angjela Davitkova, Evica Milchevski, and Sebastian Michel. 2020. The ML-Index:\nA Multidimensional, Learned Index for Point, Range, and Nearest-Neighbor\nQueries. In 2020 Conference on Extending Database Technology (EDBT.\n[10] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned\nIndex. In Proceedings of the 2020 International Conference on Management of Data.\n[11] Mike Stonebraker et al. 2005. C-Store: A Column-oriented DBMS. In Proceedings\nof the 31st VLDB Conference. VLDB Endowment.\n[12] Evan Hallmark. 2020. Daily Historical Stock Prices (1970 - 2018).\nhttps://www.kaggle.com/ehallmar/daily-historical-stock-prices-1970-2018.\n[13] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds. PVLDB 13, 8 (2020),\n1162â€“1175. https://doi.org/10.14778/3389133.3389135\n[14] Volker Gaede and Oliver GÃ¼nther. 1998. Multidimensional access methods. ACM\nComputing Surveys (CSUR) 30 (1998), 170â€“231. Issue 2.\n[15] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim\nKraska. 2019. FITing-Tree: A Data-Aware Index Structure. In Proceedings of the\n2019 International Conference on Management of Data (Amsterdam, Netherlands)\n(SIGMOD â€™19). Association for Computing Machinery, New York, NY, USA,\n1189â€“1206. https://doi.org/10.1145/3299869.3319860\n[16] IBM. [n.d.]. The Spatial Index. https://www.ibm.com/support/knowledgecenter/\nSSGU8G_12.1.0/com.ibm.spatial.doc/ids_spat_024.htm.\n[17] Ihab F. Ilyas, Volker Markl, Peter Haas, Paul Brown, and Ashraf Aboulnaga. 2004.\nCORDS: Automatic Discovery of Correlations and Soft Functional Dependencies.\nInProceedings of the 2004 ACM SIGMOD International Conference on Management\nof Data (Paris, France) (SIGMOD â€™04). Association for Computing Machinery, New\nYork, NY, USA, 647â€“658. https://doi.org/10.1145/1007568.1007641\n[18] H. V. Jagadish, Beng Chin Ooi, Kian-Lee Tan, Cui Yu, and Rui Zhang.\n2005. IDistance: An Adaptive B+-Tree Based Indexing Method for Nearest\nNeighbor Search. ACM Trans. Database Syst. 30, 2 (June 2005), 364â€“397.\nhttps://doi.org/10.1145/1071610.1071612\n[19] Irfan Khan. 2012. Falling RAM prices drive in-memory database surge.\nhttps://www.itworld.com/article/2718428/falling-ram-prices-drive-in-\nmemory-database-surge.html.\n[20] Hideaki Kimura, George Huo, Alexander Rasin, Samuel Madden, and Stanley B.\nZdonik. 2009. Correlation Maps: A Compressed Access Method for Exploiting\nSoft Functional Dependencies. Proc. VLDB Endow. 2, 1 (Aug. 2009), 1222â€“1233.\nhttps://doi.org/10.14778/1687627.1687765\n[21] Hideaki Kimura, George Huo, Alexander Rasin, Samuel Madden, and Stanley B.\nZdonik. 2010. CORADD: Correlation Aware Database Designer for Materi-\nalized Views and Indexes. Proc. VLDB Endow. 3, 1â€“2 (Sept. 2010), 1103â€“1113.\nhttps://doi.org/10.14778/1920841.1920979\n[22] Andreas Kipf, Ryan Marcus, Alexander Renen, Mihail Stoian, Alfons Kemper, Tim\nKraska, and Thomas Neumann. 2020. RadixSpline: A Single-Pass Learned Index.\nInaiDM 2020.[23] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In Proceedings of the 2018 International\nConference on Management of Data, SIGMOD Conference 2018, Houston, TX, USA,\nJune 10-15, 2018. ACM, 489â€“504. https://doi.org/10.1145/3183713.3196909\n[24] Sebastian Kruse and Felix Naumann. 2018. Efficient Discovery of Ap-\nproximate Dependencies. Proc. VLDB Endow. 11, 7 (March 2018), 759â€“772.\nhttps://doi.org/10.14778/3192965.3192968\n[25] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. 2020. LISA: A Learned\nIndex Structure for Spatial Data. In Proceedings of the 2020 International Conference\non Management of Data.\n[26] Lin Ma, Dana Van Aken, Amed Hefny, Gustavo Mezerhane, Andrew Pavlo, and\nGeoffrey J. Gordon. 2018. Query-based Workload Forecasting for Self-Driving\nDatabase Management Systems. In SIGMOD. ACM.\n[27] Donald Meagher. 1980. Octree Encoding: A New Technique for the Representation,\nManipulation and Display of Arbitrary 3-D Objects by Computer. Technical Report.\n[28] Microsoft SQL Server. 2016. Spatial Indexes Overview. https:\n//docs.microsoft.com/en-us/sql/relational-databases/spatial/spatial-indexes-\noverview?view=sql-server-2017.\n[29] G. M. Morton. 1966. A computer Oriented Geodetic Data Base; and a New Technique\nin File Sequencing (PDF). Technical Report. IBM.\n[30] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learning\nMulti-dimensional Indexes. In Proceedings of the 2020 International Conference\non Management of Data (Portland, OR, USA) (SIGMOD â€™20). Association for Com-\nputing Machinery, New York, NY, USA. https://doi.org/10.1145/3318464.3380579\n[31] J. Nievergelt, Hans Hinterberger, and Kenneth C. Sevcik. 1984. The Grid File: An\nAdaptable, Symmetric Multikey File Structure. ACM Trans. Database Syst. 9, 1\n(March 1984), 38â€“71. https://doi.org/10.1145/348.318586\n[32] NYC Taxi & Limousine Commission. 2020. TLC Trip Record Data.\nhttps://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page.\n[33] Beng Chin Ooi, Ron Sacks-davis, and Jiawei Han. 2019. Indexing in Spatial\nDatabases.\n[34] Oracle Database Data Warehousing Guide. 2017. Attribute Clustering.\nhttps://docs.oracle.com/database/121/DWHSG/attcluster.htm.\n[35] Oracle, Inc. [n.d.]. Oracle Database In-Memory. https://www.oracle.com/\ndatabase/technologies/in-memory.html.\n[36] Frank Ramsak1, Volker Markl, Robert Fenk, Martin Zirkel, Klaus Elhardt, and\nRudolf Bayer. 2000. Integrating the UB-Tree into a Database System Kernel . In\nProceedings of the 26th International Conference on Very Large Databases. VLDB\nEndowment.\n[37] RocksDB. 2020. RocksDB. https://rocksdb.org/.\n[38] Scipy.org. [n.d.]. scipy.optimize.basinhopping. https://docs.scipy.org/doc/scipy-\n0.18.1/reference/generated/scipy.optimize.basinhopping.html.\n[39] Dennis G. Severance and Guy M. Lohman. 1976. Differential Files: Their\nApplication to the Maintenance of Large Databases. ACM Trans. Database Syst.\n1, 3 (Sept. 1976), 256â€“267. https://doi.org/10.1145/320473.320484\n[40] Hari Singh and Seema Bawa. 2017. A Survey of Traditional and MapReduceBased\nSpatial Query Processing Approaches. SIGMOD Rec. 46, 2 (Sept. 2017), 18â€“29.\nhttps://doi.org/10.1145/3137586.3137590\n[41] Chuzhe Tang, Youyun Wang, Zhiyuan Dong, Gansen Hu, Zhaoguo Wang,\nMinjie Wang, and Haibo Chen. 2020. XIndex: A Scalable Learned Index for\nMulticore Data Storage. In Proceedings of the 25th ACM SIGPLAN Symposium on\nPrinciples and Practice of Parallel Programming (San Diego, California) (PPoPP\nâ€™20). Association for Computing Machinery, New York, NY, USA, 308â€“320.\nhttps://doi.org/10.1145/3332466.3374547\n[42] TPC. 2019. TPC-H. http://www.tpc.org/tpch/.\n[43] Gary Valentin, Michael Zuliani, Daniel C. Zilio, Guy Lohman, and Alan Skelley.\n2000. DB2 Advisor: An Optimizer Smart Enough to Recommend its own Indexes.\nInProceedings of the 16th International Conference on Data Engineering. IEEE.\n[44] H. Wang, X. Fu, J. Xu, and H. Lu. 2019. Learned Index for Spatial Queries. In 2019\n20th IEEE International Conference on Mobile Data Management (MDM). 569â€“574.\n[45] Yingjun Wu, Jia Yu, Yuanyuan Tian, Richard Sidle, and Ronald Barber. 2019.\nDesigning Succinct Secondary Indexing Mechanism by Exploiting Column Corre-\nlations. In Proceedings of the 2019 International Conference on Management of Data\n(Amsterdam, Netherlands) (SIGMOD â€™19). Association for Computing Machinery,\nNew York, NY, USA, 1223â€“1240. https://doi.org/10.1145/3299869.3319861\n[46] Zongheng Yang, Badrish Chandramouli, Chi Wang, Johannes Gehrke, Yinan Li,\nUmar F. Minhas, Per-Ã…ke Larson, Donald Kossmann, and Rajeev Acharya. 2020.\nQd-tree: Learning Data Layouts for Big Data Analytics. In Proceedings of the 2020\nInternational Conference on Management of Data.\n[47] Zack Slayton. 2017. Z-Order Indexing for Multifaceted Queries in Amazon\nDynamoDB. https://aws.amazon.com/blogs/database/z-order-indexing-for-\nmultifaceted-queries-in-amazon-dynamodb-part-1/.\n[48] zhichao Cao, Siying Dong, Sagar Vemuri, and David H.C. Du. 2020. Char-\nacterizing, Modeling, and Benchmarking RocksDB Key-Value Work-\nloads at Facebook. In 18th USENIX Conference on File and Storage Tech-\nnologies (FAST 20). USENIX Association, Santa Clara, CA, 209â€“223.\nhttps://www.usenix.org/conference/fast20/presentation/cao-zhichao\n86",
  "textLength": 76808
}