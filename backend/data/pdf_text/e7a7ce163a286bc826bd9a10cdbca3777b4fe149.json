{
  "paperId": "e7a7ce163a286bc826bd9a10cdbca3777b4fe149",
  "title": "Partitioned Learned Bloom Filter",
  "pdfPath": "e7a7ce163a286bc826bd9a10cdbca3777b4fe149.pdf",
  "text": "Partitioned Learned Bloom Filters\nKapil Vaidya\nCSAIL\nMIT\nkapilv@mit.eduEric Knorr\nSchool of Engineering and Applied Sciences\nHarvard University\neric.r.knorr@gmail.com\nTim Kraska\nCSAIL\nMIT\nkraska@mit.eduMichael Mitzenmacher\nSchool of Engineering and Applied Sciences\nHarvard University\nmichaelm@eecs.harvard.edu\nAbstract\nBloom ﬁlters are space-efﬁcient probabilistic data structures that are used to test\nwhether an element is a member of a set, and may return false positives. Recently,\nvariations referred to as learned Bloom ﬁlters were developed that can provide\nimproved performance in terms of the rate of false positives, by using a learned\nmodel for the represented set. However, previous methods for learned Bloom ﬁlters\ndo not take full advantage of the learned model. Here we show how to frame the\nproblem of optimal model utilization as an optimization problem, and using our\nframework derive algorithms that can achieve near-optimal performance in many\ncases. Experimental results from both simulated and real-world datasets show\nsigniﬁcant performance improvements from our optimization approach over both\nthe original learned Bloom ﬁlter constructions and previously proposed heuristic\nimprovements.\n1 Introduction\nBloom ﬁlters are space-efﬁcient probabilistic data structures that are used to test whether an element\nis a member of a set [Bloom (1970)]. A Bloom ﬁlter compresses a given set Sinto an array of bits. A\nBloom ﬁlter may allow false positives, but will not give false negative matches, which makes them\nsuitable for numerous memory-constrained applications in networks, databases, and other systems\nareas. Indeed, there are many thousands of papers describing applications of Bloom ﬁlters [Dayan\net al. (2018), Dillinger and Manolios (2004), Broder and Mitzenmacher (2003)].\nThere exists a trade off between the false positive rate and the size of a Bloom ﬁlter (smaller false\npositive rate leads to larger Bloom ﬁlters). For a given false positive rate, there are known theoretical\nlower bounds on the space used [Pagh et al .(2005)] by the Bloom ﬁlter. However, these lower\nbounds assume the Bloom ﬁlter could store any possible set. If the data set or the membership\nqueries have speciﬁc structure, it may be possible to beat the lower bounds in practice [Mitzenmacher\n(2002), Bruck et al .(2006), Mitzenmacher et al .(2020)]. In particular, [Kraska et al .(2018)] and\n[Mitzenmacher (2019)] propose using machine learning models to reduce the space further, by using\na learned model to provide a suitable pre-ﬁlter for the membership queries. This allows one to beat\nthe space lower bounds by leveraging the context speciﬁc information present in the learned model.\nThe key idea of learned Bloom ﬁlters is that in many practical settings, given a query input, the\nlikelihood that the input is in the set Scan be deduced by some observable features which can be\ncaptured by a machine learning model. For example, a Bloom ﬁlter that represents a set of malicious\nURLs can beneﬁt from a learned model that can distinguish malicious URLs from benign URLs.\nThis model can be trained on URL features such as length of hostname, counts of special characters,arXiv:2006.03176v2  [cs.DS]  4 Oct 2020\n\netc. This approach is described in [Kraska et al .(2018)], which studies how standard index structures\ncan be improved using machine learning models; we refer to their framework as the original learned\nBloom ﬁlter, Given an input xand its features, the model outputs a score s(x)which is supposed\nto correlate with the likelihood of the input being in the set. Thus, the elements of the set, or keys,\nshould have a higher score value compared to non-keys. This model is used as a pre-ﬁlter, so when\nscores(x)of an inputxis above a pre-determined threshold t, it is directly classiﬁed as being in\nthe set. For inputs where s(x)<t, a smaller backup Bloom ﬁlter built from only keys with a score\nbelow the threshold (which are known) is used. This maintains the property that there are no false\nnegatives. The design essentially uses the model to immediately answer for inputs with high score\nwhereas the rest of the inputs are handled by the backup Bloom ﬁlter as shown in Fig.1(A). The\nthreshold value tis used to partition the space of scores into two regions, with inputs being processed\ndifferently depending on in which region its score falls. With a sufﬁciently accurate model, the size\nof the backup Bloom ﬁlter can be reduced signiﬁcantly over the size of a standard Bloom ﬁlter while\nmaintaining overall accuracy. [Kraska et al .(2018)] showed that, in some applications, even after\ntaking the size of the model into account, the learned Bloom ﬁlter can be smaller than the standard\nBloom ﬁlter for the same false positive rate.\nThe original learned Bloom ﬁlter compares the model score against a single threshold, but the\nframework has several drawbacks.\nChoosing the right threshold : The choice of threshold value for the learned Bloom ﬁlter is critical,\nbut the original design uses heuristics to determine the threshold value.\nUsing more partitions: Comparing the score value only against a single threshold value wastes\ninformation provided by the learning model. For instance, two elements x1;x2withs(x1)>>\ns(x2)> t, are treated the same way but the odds of x1being a key are much higher than for x2.\nIntuitively, we should be able to do better by partitioning the score space into more than two regions.\nOptimal Bloom ﬁlters for each region: Elements with scores above the threshold are directly\naccepted as keys. A more general design would provide backup Bloom ﬁlters in both regions and\nchoose the Bloom ﬁlter false positive rate of each region so as to optimize the space/false positive\ntrade-off as desired. The original setup can be interpreted as using a Bloom ﬁlter of size 0 and false\npositive rate of 1 above the threshold. This may not be the optimal choice; moreover, as we show,\nusing different Bloom ﬁlters for each region(as shown in Fig.1(C)) allows further gains when we\nincrease the number of partitions.\nFollow-up work by [Mitzenmacher (2019)] and [Dai and Shrivastava (2019)] improve on the original\ndesign but only address a subset of these drawbacks. In particular, [Mitzenmacher (2019)] proposes\nusing Bloom ﬁlters for both regions and provides a method to ﬁnd the optimal false positive rates for\neach Bloom ﬁlter. But [Mitzenmacher (2019)] only considers two regions and does not consider how\nto ﬁnd the optimal threshold value. [Dai and Shrivastava (2019)] propose using multiple thresholds\nto divide the space of scores into multiple regions, with a different backup Bloom ﬁlter for each\nscore region. The false positive rates for each of the backup Bloom ﬁlters and the threshold values\nare chosen using heuristics. Empirically, we found that these heuristics might perform worse than\n[Mitzenmacher (2019)] in some scenarios.\nA general design that resolves all the drawbacks would, given a target false positive rate and the\nlearned model, partition the score space into multiple regions with separate backup Bloom ﬁlters\nfor each region, and ﬁnd the optimal threshold values and false positive rates, under the goal of\nminimizing the memory usage while achieving the desired false positive rate as shown in Fig.1(C).\nIn this work, we show how to frame this problem as an optimization problem, and show that our\nresulting solution signiﬁcantly outperforms the heuristics used in previous works. Additionally,\nwe show that our maximum space saving1is linearly proportional to the KL divergence of the key\nand non-key score distributions determined by the partitions. We present a dynamic programming\nalgorithm to ﬁnd the optimal parameters (up to the discretization used for the dynamic programming)\nand demonstrate performance improvements over a synthetic dataset and two real world datasets:\nURLs and EMBER. We also show that the performance of the learned Bloom ﬁlter improves with\nincreasing number of partitions and that in practice a small number of regions ( \u00194\u00006) sufﬁces to\nget a very good performance. We refer to our approach as a partitioned learned Bloom ﬁlter (PLBF).\nExperimental results from both simulated and real-world datasets show signiﬁcant performance\n1space saved by using our approach instead of a Bloom ﬁlter\n2\n\nimprovements. We show that to achieve a false positive rate of 0:001, [Mitzenmacher (2019)] uses\n8.8x, 3.3x and 1.2x the amount of space and [Dai and Shrivastava (2019)] uses 6x, 2.5x and 1.1x the\namount of space compared to PLBF for synthetic, URLs and EMBER respectively.\n2 Background\ntk-1 ≤ s(x) ≤ tk t0 ≤ s(x) < t1 t ≤ s(x) ≤ 1 0 ≤ s(x) < t Input (x)\ns(x) > tLearned Model \nf1Bloom filter \nPositiveNegativePositives(x) ≤ tInput (x)\nLearned Model \nf1Bloom filter \nPositiveNegativet0=0≤ s(x) <t1\nfkBloom filter \nPositiveNegative\n(A)f0\nPositiveNegativeBloom filter Input (x)\ns(x) > tLearned Model \nf1Bloom filter \nPositive NegativePositives(x) ≤ ttk-1 ≤ s(x) ≤ tk=1\nf2\nPositiveNegativeBloom filter t1≤ s(x) <t2\n1.0 0.0g(x) h(x)\nh(x): Dist. of non-keys g(x): Dist. of keys \ntScores \n  s(x) =1 = 0.0 g(x) h(x)\nScores \n   s(x) t0t1t2tk-1\n101 10\nf101 1\nf1001 11\nfk011 10\nf2kthRegion \ntk1stRegion 1stRegion 2ndRegion 2ndRegion \nt1 ≤ s(x) < t2 \n(D) (E)(B) (C)\n0Input (x)\ns(x) > tLearned Model \nf0* f1Bloom filter \nPositiveNegatives(x) ≤ t\nf0Bloom filter \nPositiveNegative\n(F)\nFigure 1: (A),(B),(C) represent the original LBF, LBF with sandwiching, and PLBF designs, respectively. Each\nregion in (C) is deﬁned by score boundaries ti; ti+1and a false positive rate fiof the Bloom Filter used for that\nregion. (D),(E) show the LBF and PLBF with score space distributions. (F) represents a PLBF design equivalent\nto the sandwiching approach used in Appendix.D.1.\n2.1 Standard Bloom Filters and Related Variants\nA standard Bloom ﬁlter, as described in Bloom’s original paper [Bloom (1970)], is for a set S=\nfx1;x2;:::;xngofnkeys. It consists of an array of mbits and uses kindependent hash functions\nfh1;h2;:::hkgwith the range of each hibeing integer values between 0 and m\u00001. We assume the\nhash functions are fully random. Initially all mbits are 0. For every key x2S, array bitshi(x)are\nset to 1for alli2f1;2;:::kg.\nA membership query for yreturns thaty2Sifhi(y) = 1 for alli2f1;2;:::kgandy62Sotherwise.\nThis ensures that the Bloom ﬁlter has no false negatives but non-keys ymight result in a false positive.\nThis false positive rate depends on the space mused by the Bloom Filter. Asymptotically (for large\nm;n withm=n held constant), the false positive rate is given by\n \n1\u0000\u0012\n1\u00001\nm\u0013kn!k\n: (1)\nSee [Broder and Mitzenmacher (2003); Bose et al. (2008)] for further details.\n[Bloom (1970)] proved a space lower bound of jSj\u0002log2(1\nF)for a Bloom ﬁlter with false positive\nrateF. The standard construction uses space that is asymptotically log2e(\u00191:44)times more\nthan the lower bound. Other constructions exist, such as Cuckoo ﬁlters[Fan et al .(2014)], Morton\nﬁlters[Breslow and Jayasena (2018)], XOR ﬁlters[Graf and Lemire (2020)] and Vacuum ﬁlters[Wang\net al.(2019)]. These variants achieve slightly better space performance compared to standard Bloom\nﬁlters but still are a constant factor larger than the lower bound. [Pagh et al .(2005)] presents a Bloom\nﬁlter design that achieves this space lower bound, but it appears too complicated to use in practice.\n2.2 Learned Bloom Filter\nLearned Bloom ﬁlters make use of learned models to beat the theoretical space bounds. Given a\nlearned model that can distinguish between keys and non-keys, learned Bloom ﬁlters use it as a\n3\n\npre-ﬁlter before using backup Bloom ﬁlters. The backup Bloom ﬁlters can be any variant including\nthe standard, cuckoo, XOR ﬁlters, etc. If the size of the model is sufﬁciently small, learned models\ncan be used to enhance the performance of any Bloom ﬁlter variant.\nWe provide the framework for learned Bloom ﬁlters. We are given a set of keys S=fx1;x2;::;xng\nfrom a universe Ufor which to build a Bloom ﬁlter. We are also given a sample of the non-keys Q\nwhich is representative of the set U\u0000S. Features that can help in determining if an element is a\nmember ofSare determined. The learned model is then trained on features of set S[Qfor a binary\nclassiﬁcation task and produces a score s(x)2[0;1]. This score s(x)can be viewed (intuitively,\nnot formally) as the conﬁdence of the model that the element xis in the set S. So, a key in S\nwould ideally have a higher score value than the non-keys. An assumption in this framework is that\nthe training sample distribution needs to match or be close to the test distribution of non-keys; the\nimportance of this assumptions has been discussed at length in [Mitzenmacher (2019)]. For many\napplications, past workloads or historical data can be used to get an appropriate non-key sample.\nAs discussed above, [Kraska et al .(2018)] set a threshold tand inputs satisfying s(x)> t are\nclassiﬁed as a key. A backup Bloom ﬁlter is built for just the keys in Ssatisfyings(x)\u0014t. This\ndesign is represented in Fig.1(A). [Mitzenmacher (2019)] proposes using another Bloom ﬁlter before\nthe learned model along with a backup Bloom Filter. As the learned model is used between two\nBloom ﬁlters as shown in Fig.1(B), this is referred to as the ’sandwiching’ approach. They also\nprovide the analysis of the optimal false positive rates for a given amount of memory for the two\nBloom ﬁlters (given the false negative rate and false positive rate for the learned model, and the\ncorresponding threshold). Interestingly, the sandwiching approach and analysis can be seen as a\nspecial case of our approach and analysis, as we describe later in Appendix.D.1. [Dai and Shrivastava\n(2019)] use multiple thresholds to partition the score space into multiple regions and use a backup\nBloom ﬁlter for each score region. They propose heuristics for how to divide up the score range and\nchoose false positive rate per region.\n3 Partitioned Learned Bloom Filter (PLBF)\n3.1 Design\nAs discussed before, the general design segments the score space into multiple regions using multiple\nthresholds, as shown in Fig.1(C), and uses separate backup Bloom ﬁlters for each region. We can\nchoose different target false positive rates for each region2. The parameters associated with each\nregion are its threshold boundaries and its false positive rate. Setting good values for these parameters\nis crucial for performance. Our aim is to analyze the performance of the learned Bloom ﬁlter with\nrespect to these parameters, and ﬁnd methods to determine optimal or near-optimal parameters.\nThe following notation will be important for our analysis. Let G(t)be the fraction of keys with scores\nfalling below t. We note that since the key set is ﬁnite, G(t)goes through discrete jumps. But it is\nhelpful (particularly in our pictures) to think of G(t)as being a continuous function, corresponding\nto a cumulative probability distribution, with a corresponding “density” function g(t). For non keys,\nwe assume that queries involving non-keys come from some distribution D, and we deﬁne H(t)to be\nprobability that a non-key query from Dhas a score less than or equal to t. Note that non key query\ndistribution might be different from non key distribution. If non key queries are chosen uniformly\nat random, non key query distribution would be the same as non key distribution. We assume that\nH(t)is known in the theoretical analysis below. In practice, we expect a good approximation of H(t)\nwill be used, determined by taking samples from Dor a suitably good approximation, which may be\nbased on, for example, historical data (discussed in detail in [Mitzenmacher (2019)]). Here H(t)can\nbe viewed as a cumulative distribution function, and again in our pictures we think of it as having a\ndensityh(t). Also, note that if queries for non-keys are simply chosen uniformly at random, then\nH(t)is just the fraction of non-keys with scores below t. While our analysis holds generally, the\nexample ofH(t)being the fraction of non-keys with scores below tmay be easier to keep in mind.\nVisualization of the original learned Bloom ﬁlter in terms of these distributions is shown in Fig.1(D).\nAs we describe further below, for our partitioned learned Bloom ﬁlter, we use multiple thresholds and\na separate backup Bloom ﬁlter for each region, as show in Fig.1 (E). In what follows, we formulate\n2The different false positive rates per region can be achieved in multiple ways. Either by choosing a separate\nBloom ﬁlter per region or by having a common Bloom ﬁlter with varying number of hash functions per region.\n4\n\nthe problem of choosing thresholds and backup Bloom ﬁlter false positive rates (or equivalently,\nsizes) as an optimization problem in section 3.2. In section 3.3.1, we ﬁnd the optimal solution of a\nrelaxed problem which helps us gain some insight into the general problem. We then propose an\napproximate solution for the general problem in section 3.3.3.\nWe ﬁnd in our formulation that the resulting parameters correspond to quite natural quantities in\nterms ofGandH. Speciﬁcally, the optimal false positive rate of a region is proportional to the ratio\nof the fraction of keys to the fraction of non-keys in that region. If we think of these region-based\nfractions for keys and non-keys as probability distributions, the maximum space saving obtained is\nproportional to the KL divergence between these distributions. Hence we can optimize the thresholds\nby choosing them to maximize this divergence. We show that we can ﬁnd thresholds to maximize\nthis divergence, approximately, through dynamic programming. We also show that, naturally, this KL\ndivergence increases with more number of regions and so does the performance. In our experiments,\nwe ﬁnd a small number( \u00194\u00006) of partitions sufﬁces to get good performance.\n3.2 General Optimization Formulation\nTo formulate the overall problem as an optimization problem, we consider the variant which minimizes\nthe space used by the Bloom ﬁlters in PLBF in order to achieve an overall a target false positive rate\n(F). We could have similarly framed it as minimizing the false positive rate given a ﬁxed amount of\nspace. Here we are assuming the learned model is given.\nWe assume normalized score values in [0;1]for convenience. We have region boundaries given\nbytivalues 0 =t0\u0014t1\u0014::::tk\u00001\u0014tk= 1, with score values between [ti\u00001;ti]falling into\ntheithregion. We assume the target number of regions kis given. We denote the false positive\nrate for the Bloom ﬁlter in the ithregion byfi. We letGandHbe deﬁned as above. As state\npreviously, Fig.1 (E)corresponds to this setting, and the following optimization problem ﬁnds the\noptimal thresholds tiand the false positive rates fi:\nmin\nti;fi\u0010Pk\ni=1jSj\u0002(G(ti)\u0000G(ti\u00001))\u0002clog2\u0010\n1\nfi\u0011\u0011\n+ Size of Learned Model (2)\nconstraintsPk\ni=1(H(ti)\u0000H(ti\u00001))\u0002fi\u0014F (3)\nfi\u00141;i= 1:::k (4)\n(ti\u0000ti\u00001)\u00150;i= 1:::k;t0= 0;tk= 1 (5)\nThe minimized term (Eq.2) represents the total size of the learned Bloom ﬁlter, the size of backup\nBloom ﬁlters is obtained by summing the individual backup Bloom ﬁlter sizes. The constant cin the\nequation depends on which variant of the Bloom ﬁlter is used as the backup3; as it happens, its value\nwill not affect the optimization.\nThe ﬁrst constraint (Eq.3) ensures that the overall false positive rate stays below the target F. The\noverall false positive rate is obtained by summing the appropriately weighted rates of each region.\nThe next constraint (Eq.4) encodes the constraint that false positive rate for each region is at most 1.\nThe last set of constraints (Eq.5) ensure threshold values are increasing and cover the interval [0;1].\n3.3 Solving the Optimization Problem\n3.3.1 Solving a Relaxed Problem\nIf we remove the false positive rate constraints (Eq.4, giving fi\u00141), we obtain a relaxed problem\nshown in Eq.6. This relaxation is useful because it allows us to use the Karush-Kuhn-Tucker (KKT)\nconditions to obtain optimal fivalues in terms of the tivalues, which we used to design algorithms\n3The sizes of Bloom ﬁlter variants are proportional to jSj\u0002log2(1=f), where Sis the set it represents, and\nfis the false positive rate it achieves. See e.g. [Mitzenmacher (2019)] for related discussion. The constant c\ndepends on which type of Bloom ﬁlter is used as a backup. For example, c= log2(e)for standard Bloom ﬁlter.\n5\n\nfor ﬁnding near-optimal solutions. Throughout this section, we assume the the relaxed problem yields\na solution for the original problem; we return to this issue in subsection 3.3.3.\nmin\nti=1:::k\u00001;fi=1:::k\u0010Pk\ni=1jSj\u0002(G(ti)\u0000G(ti\u00001))\u0002clog2\u0010\n1\nfi\u0011\u0011\n+ Size of Learned Model\nconstraintsPk\ni=1(H(ti)\u0000H(ti\u00001))\u0002fi\u0014F;\n(ti\u0000ti\u00001)\u00150;i= 1:::k;t0= 0;tk= 1\n(6)\nThe optimal fivalues obtained by using the KKT conditions yield Eq.7 (as derived in Appendix.A),\ngiving the exact solution in terms of ti’s.\nfi=FG(ti)\u0000G(ti\u00001)\nH(ti)\u0000H(ti\u00001)(7)\nThe numerator G(ti)\u0000G(ti\u00001)is the fraction of keys in the ithregion and the denominator\nH(ti)\u0000H(ti\u00001)is the probability of a non-key query being in the ithregion. In intuitive terms,\nthe false positive rate for a region is proportional to the ratio of the key density (fraction of keys)\nto non-key density (fraction of non-key queries). Since we have found the optimal fiin terms\nof theti, we can replace the fiin the original problem to obtain a problem only in terms of the\nti. In what follows, we use^g(t)to represent the discrete distribution given by the kvalues of\nG(ti)\u0000G(ti\u00001)fori= 1;:::;k , and similarly we use^h(t)for the distribution corresponding to the\nH(ti)\u0000H(ti\u00001)values. Eq.8 shows the rearrangement of the minimization term(excluding model\nsize) after substitution.\nMin. Term =kX\ni=1jSj\u0002(G(ti)\u0000G(ti\u00001))\u0002clog2\u0012H(ti)\u0000H(ti\u00001)\n(G(ti)\u0000G(ti\u00001))\u0002F\u0013\n=kX\ni=1jSj\u0002(G(ti)\u0000G(ti\u00001))\u0002clog2\u00121\nF\u0013\n\u0000c\u0002jSj\u0002DKL\u0010^g(t);^h(t)\u0011(8)\nwhereDKLis the standard KL divergence for the distributions given by^g(t)and^h(t).\nEq.8 represents the space occupied by the backup Bloom ﬁlters; the total space includes this and the\nspace occupied by the learned model.\nc\u0002\u0010\njSj\u0002log2\u00001\nF\u0001\n\u0000jSj\u0002DKL\u0010^g(t);^h(t)\u0011\u0011\n+Size Of Learned Model (9)\nThe space occupied by the Bloom ﬁlter without the learned model is c\u0002jSj\u0002log2(1=F). Thus, the\nspace saved by PLBF in comparison to the normal Bloom ﬁlter is:\nc\u0002\u0010\njSj\u0002DKL\u0010^g(t);^h(t)\u0011\u0011\n\u0000Size Of Learned Model (10)\nThe space saved by PLBF is therefore linearly proportional to the KL divergence of key and non-key\ndistributions of the regions given by^g(t)and^h(t)of the regions.\nThis derivation suggests that the KL divergence might also be used as a loss function to improve the\nmodel quality. We have tested this empirically, but thus far have not seen signiﬁcant improvements\nover the MSE loss we use in our experiments; this remains an interesting issue for future work.\n3.3.2 Finding the Optimal Thresholds for Relaxed Problem\nWe have shown that, given a set of thresholds, we can ﬁnd the optimal false positive rates for the\nrelaxed problem. Here we turn to the question of ﬁnding optimal thresholds. We assume again that we\nare givenk, the number of regions desired. (We consider the importance of choosing kfurther in our\nexperimental section.) Given our results above, the optimal thresholds correspond to the points that\nmaximize the KL divergence between (^g(t);^h(t)). The KL divergence of (^g(t);^h(t))is the sum of the\ntermsgilog2gi\nhi, one term per region. (Here gi=G(ti)\u0000G(ti\u00001)andhi=H(ti)\u0000H(ti\u00001).) Note\n6\n\nthat each term depends only on the proportion of keys and non-keys in that region and is otherwise\nindependent of the other regions. This property allows a recursive deﬁnition of KL divergence that is\nsuitable for dynamic programming.\nWe divide the score space [0;1]intoNconsecutive small segments for a chosen value of N; this\nprovides us a discretization of the score space, with larger Nmore closely approximating the\nreal interval. Given k, we can ﬁnd a set of kapproximately optimal thresholds using dynamic\nprogramming, where the solution is approximate due to our discretization of the score space. Let\nDPKL(n;j)denote the maximum divergence one can get when you divide the ﬁrst nsegments into\njregions. Our approximately optimal divergence corresponds to DPKL(N;k). The idea behind\nthe algorithm is that the we can recursively deﬁne DPKL(n;j)as represented in Eq.11. Here g0;h0\nrepresent the fraction of keys and the fraction of non-key queries, respectively, in these Nsegments.\nDPKL(n;j) = max \nDPKL(n\u0000i;j\u00001) + nX\nr=ig0(r)\u0002log2\u0012Pn\nr=ig0(r)Pn\nr=ih0(r)\u0013!!\n(11)\nThe time complexity of computing DPKL(N;k)isO(N2k). One can increase the value of Nto get\nmore precision in the discretization when ﬁnding thresholds, at the cost of higher computation time.\n3.3.3 The Relaxed Problem and the General Problem\nWe can ﬁnd a near-optimal solution to the relaxed problem by ﬁrst, obtaining the threshold values\nthat maximize the divergence and then, getting the optimal fivalues using Eq.7. In many cases, the\noptimal relaxed solution will also be the optimal general solution, speciﬁcally if F\u0002(G(ti\u00001)\u0000\nG(ti))=(H(ti\u00001)\u0000H(ti))<1for alli. Hence, if we are aiming for a sufﬁciently low false positive\nrateF, solving the relaxed problem sufﬁces.\nTo solve the general problem, we need to deal with regions where fi\u00151, but we can use the relaxed\nproblem as a subroutine. First, given a ﬁxed set of tivalues for the general problem, we have an\nalgorithm (Alg.1, as discussed in Appendix.B) to ﬁnd the optimal fi’s. Brieﬂy summarized, we solve\nthe relaxed problem, and for regions with fi>1, the algorithm sets fi= 1, and then re-solves the\nrelaxed problem with these additional constraints, and does this iteratively until no region with fi>1\nremains. The problem is that we do not have the optimal set of tivalues to begin; as such, we use the\noptimaltivalues for the relaxed solution as described in Section 3.3.2. This yields a solution to the\ngeneral problem (psuedo-code in Alg.2), but we emphasize that it is not optimal in general, since we\ndid not start with the optimal ti. We expect still that it will perform very well in most cases.\nIn practice, we observe that keys are more concentrated on higher scores, and non-key queries are\nmore concentrated on lower scores. Given this property, if a region with fi= 1(no backup Bloom\nﬁlter used) exists in the optimal solution of the general problem, it will most probably be the rightmost\nregion. In particular, if (G(ti\u00001)\u0000G(ti))=(H(ti\u00001)\u0000H(ti))is increasing as ti\u00001;tiincrease – that\nis, the ratio of the fraction of keys to the fraction of non-key queries over regions is increasing – then\nindeed without loss of generality the last ( kth) region will be the only one with fk= 1. (We say only\none region because any two consecutive regions with fi= 1can be merged and an extra region can\nbe made in the remaining space which is strictly better, as adding an extra region always helps as\nshown in Appendix.D.2.) It is reasonable to believe that in practice this ratio will be increasing or\nnearly so.\nHence if we make the assumption that in the optimal solution all the regions except the last satisfy the\nfi<1constraint, then if we identify the optimal last region’s boundary, we can remove the fi\u00141\nconstraints for i6=kand apply the DP algorithm to ﬁnd near optimal ti’s. To identify the optimal\nlast region’s boundary, we simply try all possible boundaries for the kth region (details discussed\nin Appendix.C). As it involves assumptions on the behavior of GandH, we emphasize again that\nthis will not guarantee ﬁnding the optimal solution. But when the conditions are met it will lead to a\nnear-optimal solution (only near-optimal due to the discretization of the dynamic program).\n7\n\n4 Evaluation\nWe compare PLBF against the theoretically optimal Bloom ﬁlter [Bloom (1970)]4, the sandwiching\napproach [Mitzenmacher (2019)], and AdaBF [Dai and Shrivastava (2019)]. Comparisons against\nstandard Bloom ﬁlters5appear in Appendix.E.1. We excluded the original learned Bloom ﬁlter\n[Kraska et al .(2018)] as the sandwiching approach was strictly better. We include the size of the\nlearned model with the size of the learned Bloom ﬁlter. To ensure a fair comparison, we used the\noptimal Bloom ﬁlter as the backup bloom ﬁlter for all learned variants. We use 3 different datasets:\nURLs : As in previous papers [Kraska et al .(2018), Dai and Shrivastava (2019)], we used the\nURL data set, which contains 103520 (23%) malicious and 346646 (77%) are benign URLs. We\nused 17 features from these URL’s such as host name length, use of shortening, counts of special\ncharacters,etc.\nEMBER : Bloom ﬁlters are widely used to match ﬁle signatures with the virus signature database.\nEmber (Endgame Malware Benchmark for Research) [Anderson and Roth (2018)] is an open source\ncollection of 1.1M sha256 ﬁle hashes that were scanned by VirusTotal in 2017. Out of the 1.1 million\nﬁles, 400K are malicious, 400K are benign, and we ignore the remaining 300K unlabeled ﬁles. The\nfeatures of the ﬁles are already included in the dataset.\nSynthetic : An appealing scenario for our method is when the key density increases and non-key\ndensity decreases monotonically with respect to the score value. We simulate this by generating the\nkey and non-key score distribution using Zipﬁan distributions as in Fig.2(A). Since we directly work\non the score distribution, the size of the learned model for this synthetic dataset is zero.\n4.1 Overall Performance\nHere, we compare the performance of PLBF against other baselines by ﬁxing the target Fand\nmeasuring the space used by each methods. We use PLBF Alg.3 with DP algorithm discretization( N)\nset to 1000. We train the model on the entire key set and 40% of the non-key set. The thresholds and\nbackup Bloom ﬁlters are then tuned using this model with the aim of achieving the ﬁxed targetF.\nThe rest of the non-keys are used to evaluate the actual false positive rate.\nWhile any model can be used, we choose the random forest classiﬁer from sklearn [Pedregosa et al .\n([n.d.])] for its good accuracy. The F1 scores of the learned models used for synthetic, URLs and\nEMBER were 0.99, 0.97, and 0.85, respectively. We consider the size of the model to be the pickle\nﬁle size on the disk (a standard way of serializing objects in Python). We use ﬁve regions ( k= 5) for\nboth PLBF and AdaBF as this is usually enough to achieve good performance as discussed in 4.2.\nUsing higher kwould only improve our performance.\nThe results of the experiment are shown in the Fig.2(A-C) along with the distribution of the scores of\nkeys and non-keys for each dataset. As we can see from the ﬁgure, PLBF has a better Pareto curve\nthan the other baselines for all the datasets. On the synthetic dataset and URLs dataset we observe a\nsigniﬁcantly better performance. In contrast, for the EMBER dataset our performance is only slightly\nbetter indicating that the model here is not as helpful. The difference between space used by PLBF\nand optimal Bloom ﬁlter ﬁrst increases with decreasing false positive rate but converges to a constant\nvalue for all datasets, as given in Eq.10. For the same amount of space used(400Kb,500Kb,3000Kb\nspace for synthetic,URLs,EMBER, respectively), PLBF achieves 22x, 26x, and 3x smaller false\npositive rates than the sandwiching approach, and 8.5x, 9x, and 1.9x smaller false positive rates than\nAdaBF for synthetic, URLs, and EMBER, respectively. To achieve a false positive rate of 0:001, the\nsandwiching approach uses 8.8x, 3.3x, and 1.2x the amount of space and AdaBF uses 6x, 2.5x, and\n1.1x the amount of space compared to PLBF for synthetic, URLs, and EMBER datasets respectively.\n4For the space of a theoretically optimal Bloom ﬁlter, we take the standard Bloom ﬁlter of same false positive\nrate and divide it’s space used by log2e, as obtaining near-optimality in practice is difﬁcult. This uses the fact\nthat the standard Bloom ﬁlter is asymptotically log2etimes suboptimal than the optimal as discussed in Sec.2.1.\n5PLBF performs better against standard Bloom ﬁlters, as discussed in Appendix.D.3. Section 4.1 are\nconservative estimates of gains possible in practice using a PLBF.\n8\n\nNon-Key Key Key\n(C) EMBER (B) URLs (A) Synthetic \nKey Non-Key Non-Key \nBetter Better Better \nSpace Used(in Kb) Space Used(in Kb) Space Used(in Kb) False Positive Rate \nFalse Positive Rate \nFalse Positive Rate \n(D) Synthetic (E) URLs (F) EMBER Space Saved (in Kb) Space Saved (in Kb) \nSpace Saved (in Kb) \nNumber of Regions (k) Number of Regions (k) Number of Regions (k) Figure 2: FPR vs Space for the (A) Synthetic (B) URLs (C) EMBER datasets for various baselines along with\nkey and non-key score distributions. Space Saved as we increase number of regions for the (D) Synthetic (E)\nURLs (F) EMBER datasets for PLBF compared to the optimal Bloom ﬁlter\n4.2 Performance and the Number of Regions\nThe maximum space savings obtained by using PLBF is linearly proportional to the KL diver-\ngence of the distributions(Eq10) and this KL divergence strictly increases with the number of\nregions(Appendix.D.2). Fig.2(D-F) show the space saved w.r.t the optimal Bloom ﬁlter as we increase\nthe number of regions kfor a target false positive rate of 0:001. The red line in the ﬁgure shows the\nsavings when using 25regions; using more regions provides no noticeable improvement on this data.\nOur results suggest using 4-6 regions should be sufﬁcient to obtain reasonable performance. We have\nadditional experiments in Appendix.E that shows PLBF performance against standard Bloom ﬁlters\nand PLBF performance w.r.t model quality.\n5 Conclusion\nOur analysis of the partitioned learned Bloom ﬁlter provides a formal framework for improving\non learned Bloom ﬁlter performance that provides substantially better performance than previous\nheuristics. As Bloom ﬁlters are used across thousands of applications, we hope the PLBF may ﬁnd\nmany uses where the data set is amenable to a learned model.\n9\n\nReferences\nHyrum S. Anderson and Phil Roth. 2018. EMBER: An Open Dataset for Training Static PE Malware\nMachine Learning Models. arXiv:cs.CR/1804.04637\nBurton H. Bloom. 1970. Space/Time Trade-Offs in Hash Coding with Allowable Errors. Commun.\nACM 13, 7 (July 1970), 422–426. https://doi.org/10.1145/362686.362692\nProsenjit Bose, Hua Guo, Evangelos Kranakis, Anil Maheshwari, Pat Morin, Jason Morrison, Michiel\nSmid, and Yihui Tang. 2008. On the false-positive rate of Bloom ﬁlters. Inform. Process. Lett. 108,\n4 (2008), 210–213.\nAlex D. Breslow and Nuwan S. Jayasena. 2018. Morton Filters: Faster, Space-Efﬁcient Cuckoo\nFilters via Biasing, Compression, and Decoupled Logical Sparsity. Proc. VLDB Endow. 11, 9\n(May 2018), 1041–1055. https://doi.org/10.14778/3213880.3213884\nAndrei Z. Broder and Michael Mitzenmacher. 2003. Survey: Network Applications of Bloom Filters:\nA Survey. Internet Math. 1, 4 (2003), 485–509. https://doi.org/10.1080/15427951.2004.\n10129096\nJehoshua Bruck, Jie Gao, and Anxiao Jiang. 2006. Weighted Bloom ﬁlter. In Proceedings 2006\nIEEE International Symposium on Information Theory, ISIT 2006, The Westin Seattle, Seattle,\nWashington, USA, July 9-14, 2006 . IEEE, 2304–2308. https://doi.org/10.1109/ISIT.\n2006.261978\nZhenwei Dai and Anshumali Shrivastava. 2019. Adaptive Learned Bloom Filter (Ada-BF): Efﬁcient\nUtilization of the Classiﬁer. arXiv preprint arXiv:1910.09131 (2019).\nNiv Dayan, Manos Athanassoulis, and Stratos Idreos. 2018. Optimal Bloom Filters and Adaptive\nMerging for LSM-Trees. ACM Trans. Database Syst. 43, 4, Article 16 (Dec. 2018), 48 pages.\nhttps://doi.org/10.1145/3276980\nPeter C. Dillinger and Panagiotis Manolios. 2004. Bloom Filters in Probabilistic Veriﬁcation. In\nFormal Methods in Computer-Aided Design, 5th International Conference, FMCAD 2004, Austin,\nTexas, USA, November 15-17, 2004, Proceedings (Lecture Notes in Computer Science) , Alan J.\nHu and Andrew K. Martin (Eds.), V ol. 3312. Springer, 367–381. https://doi.org/10.1007/\n978-3-540-30494-4_26\nBin Fan, Dave G. Andersen, Michael Kaminsky, and Michael D. Mitzenmacher. 2014. Cuckoo Filter:\nPractically Better Than Bloom. In Proceedings of the 10th ACM International on Conference on\nEmerging Networking Experiments and Technologies (CoNEXT ’14) . Association for Computing\nMachinery, New York, NY , USA, 75–88. https://doi.org/10.1145/2674005.2674994\nThomas Mueller Graf and Daniel Lemire. 2020. Xor Filters. ACM Journal of Experimental\nAlgorithmics 25, 1 (Jul 2020), 1–16. https://doi.org/10.1145/3376122\nTim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018. The Case for\nLearned Index Structures. In Proceedings of the 2018 International Conference on Management\nof Data (SIGMOD ’18) . Association for Computing Machinery, New York, NY , USA, 489–504.\nhttps://doi.org/10.1145/3183713.3196909\nMichael Mitzenmacher. 2002. Compressed bloom ﬁlters. IEEE/ACM Trans. Netw. 10, 5 (2002),\n604–612. https://doi.org/10.1109/TNET.2002.803864\nMichael Mitzenmacher. 2019. A Model for Learned Bloom Filters, and Optimizing by Sandwiching.\nCoRR abs/1901.00902 (2019). arXiv:1901.00902 http://arxiv.org/abs/1901.00902\nMichael Mitzenmacher, Salvatore Pontarelli, and Pedro Reviriego. 2020. Adaptive Cuckoo Filters.\nJ. Exp. Algorithmics 25, 1, Article 1.1 (March 2020), 20 pages. https://doi.org/10.1145/\n3339504\nAnna Pagh, Rasmus Pagh, and S. Srinivasa Rao. 2005. An Optimal Bloom Filter Replacement. In\nProceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA ’05) .\nSociety for Industrial and Applied Mathematics, USA, 823–829.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-\nhofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and\nE. Duchesnay. [n.d.]. Scikit-learn: Machine Learning in Python. Journal of Machine Learning\nResearch 12 ([n. d.]), 2825–2830.\n10\n\nMinmei Wang, Mingxun Zhou, Shouqian Shi, and Chen Qian. 2019. Vacuum Filters: More Space-\nEfﬁcient and Faster Replacement for Bloom and Cuckoo Filters. Proc. VLDB Endow. 13, 2 (Oct.\n2019), 197–210. https://doi.org/10.14778/3364324.3364333\nA Solving the Relaxed Problem using KKT conditions\nAs mentioned in the main text, if we relax the constraint of fi\u00141, using the stationary KKT\nconditions we can obtain the optimal fivalues. Here we show this work. The appropriate Lagrangian\nequation is given in Eq.12. In this case, the KKT coniditions tell us that the optimal solution is a\nstationary point of the Lagrangian. Therefore, we ﬁnd where the derivative of the Lagrangian with\nrespect tofiis zero.\nL(ti;fi;\u0015;\u0017i) =Pk\ni=1(G(ti)\u0000G(ti\u00001))\u0002clog2\u0010\n1\nfi\u0011\n+\u0015\u0002\u0010\u0010Pk\ni=1(H(ti)\u0000H(ti\u00001))\u0002fi\u0011\n\u0000F\u0011\n+\nPk\ni=1\u0017i\u0002(ti\u00001\u0000ti)\n(12)\n@L(ti;fi;\u0015;\u0017i)\n@fi= 0 (13)\n@(G(ti)\u0000G(ti\u00001))clog2\u0010\n1\nfi\u0011\n@fi=\u0000\u0015@(H(ti)\u0000H(ti\u00001))\u0002fi\n@fi(14)\nfi=cln(2)\u0002(G(ti)\u0000G(ti\u00001))\u0002\u0015\n(H(ti)\u0000H(ti\u00001))(15)\n\u0015=F\ncln(2)\u0002Pk\ni=1(G(ti)\u0000G(ti\u00001))=F\ncln 2 (16)\nfi=(G(ti)\u0000G(ti\u00001))\u0002FPR\n(H(ti)\u0000H(ti\u00001))(17)\nEq.15 expresses fpriin terms of\u0015. Summing Eq.15 over all iand using the relationship between F\nandHwe get Eq.16. Thus the optimal fivalues turn out to be as given in Eq.17.\nB Optimal False Positive Rate for given thresholds\nWe provide the pseudocode for the algorithm to ﬁnd the optimal false positive rates if threshold values\nare provided. The corresponding optimization problem is given in Eq.18. As the boundaries for\nthe regions are already deﬁned, one only needs to ﬁnd the optimal false positive rate for the backup\nBloom ﬁlter of each region.\nmin\nfi=1:::kPk\ni=1(G(ti)\u0000G(ti\u00001))\u0002clog2(1\nfi)\nconstraintsPk\ni=1(H(ti)\u0000H(ti\u00001))\u0002fi=F\nfi\u00141i= 1:::k(18)\nAlg.1 gives the pseudocode. We ﬁrst assign false positive rates based on the relaxed problem but may\nﬁnd thatfi\u00151for some regions. For such regions, we can set fi= 1, re-solve the relaxed problem\nwith these additional constraints (that is, excluding these regions), and use the result as a solution for\nthe general problem. Some regions might again have a false positive rate above one, so we can repeat\nthe process. The algorithm stops when there is no new region with false positive rate greater than one.\nThis algorithm ﬁnds the optimal false positive rates for the regions when the thresholds are ﬁxed.\n11\n\nAlgorithm 1 Finding optimal fpr’s given thresholds\nInputG0- the array containing key density of each region\nInputH0- the array containing non-key density of each region\nInputF- target overall false positive rate\nInputk- number of regions\nOutputf- the array of false positive rate of each region\n1:procedure OPTIMAL FPR(G0;H0;F;k )\n2:Gsum 0 .sum of key density of regions with fi= 1\n3:Hsum 0 .sum of non-key density of regions with fi= 1\n4: foriin1;2;:::k do\n5:f[i] G0[i]\u0001F\nH0[i].Assign relaxed problem solution\n6: while somef[i]>1do\n7: foriin1;2;:::k do\n8: if(f[i]>1)thenf[i] 1 .Cap the false positive rate of region to one\n9:Gsum 0\n10:Hsum 0\n11: foriin1;2;:::k do\n12: if(f[i] = 1 )thenGsum Gsum+G0[i];Hsum Hsum+H0[i].Calculate key,non-key density in regions\nwith no Bloom ﬁlter( f[i] = 1 )\n13: foriin1;2;:::k do\n14: if(f[i]<1)thenf[i] =G0[i]\u0001(F\u0000Hsum)\nH0[i]\u0001(1\u0000Gsum).Modifying the fiof the regions to ensure target false positive rate is FPR\n15: returnfpr Array\nAlgorithm 2 Using relaxed solution for the general problem\nInputGdis- the array containing discretized key density of each region\nInputHdis- the array containing discretized key density of each region\nInputF- target overall false positive rate\nInputk- number of regions\nOutputt- the array of threshold boundaries of each region\nOutputf- the array of false positive rate of each region\nAlgorithm ThresMaxDivDP - DP algorithm that returns the thresholds maximizing the divergence between key and non-key distribution.\nAlgorithm CalcDensity - returns the region density given thresholds of the regions\nAlgorithm OptimalFPR - returns the optimal false positive rate of the regions given thresholds\nAlgorithm SpaceUsed - returns space used by the back-up Bloom ﬁlters given threhsolds and false positive rate per region.\n1:procedure SOLVE (Gdis;Hdis;F;k )\n2:t ThresMaxDivDP (Gdis;Hdis;k) .Getting the optimal thresholds for the relaxed problem\n3:G0;H0 CalcDensity (Gdis;Hdis;t)\n4:f=OptimalFPR (G0;H0;F;k ) .Obtaining optimal false positive rates of the general problem for given thresholds\n5:\n6: returnt,fArray\nC Algorithm for ﬁnding thresholds\nWe provide the pseudocode for the algorithm to ﬁnd the solution for the relaxed problem; Alg.3 ﬁnds\nthe thresholds and false positive rates. As we have described in the main text, this algorithm provides\nthe optimal parameter values, if (G(ti\u00001)\u0000G(ti))=(H(ti\u00001)\u0000H(ti))is monotonically increasing.\nThe idea is that only the false positive rate of the rightmost region can be one. The algorithm receives\ndiscretized key and non-key densities. The algorithm ﬁrst iterates over all the possibilities of the\nrightmost region. For each iteration, it ﬁnds the thresholds that maximize the KL divergence for\nthe rest of the array for which a dynamic programming algorithm exists. After calculating these\nthresholds, it ﬁnds the optimal false positive rate for each region using Alg.1. After calculating the\nthresholds and false positive rates, the algorithm calculates the total space used by the back-up Bloom\nﬁlters in PLBF. It then remembers the index for which the space used was minimal. The ti’s andfi’s\ncorresponding to this index are then used to build the backup Bloom ﬁlters. The worst case time\ncomplexity is then O(N3k).\nD Additional Considerations\nD.1 Sandwiching: A Special Case\nWe show here that the sandwiching approach can actually be interpreted as a special case of our\nmethod. In the sandwiching approach, the learned model is sandwiched between two Bloom ﬁlters\n12\n\nAlgorithm 3 Solving the general problem\nInputGdis- the array containing discretized key density of each region\nInputHdis- the array containing discretized key density of each region\nInputF- target overall false positive rate\nInputk- number of regions\nOutputt- the array of threshold boundaries of each region\nOutputf- the array of false positive rate of each region\nAlgorithm ThresMaxDivDP - DP algorithm that returns the thresholds maximizing the divergence between key and non-key distribution.\nAlgorithm CalcDensity - returns the region density given thresholds of the regions\nAlgorithm OptimalFPR - returns the optimal false positive rate of the regions given thresholds\n1:procedure SOLVE (Gdis;Hdis;F;k )\n2:MinSpaceUsed 1 .Stores minimum space used uptil now\n3:index \u0000 1 .Stores index corresponding to minimum space used\n4:Glast 0 .Key density of the current last region\n5:Hlast 0 .Non-key density of the current last region\n6:\n7: foriink\u00001;k;:::N\u00001do .Iterate over possibilities of last region\n8:Glast PN\nj=iGdis[j] .Calculate the key density of last region\n9:Hlast PN\nj=iHdis[j]\n10:t ThresMaxDivDp (G[1::(i\u00001)];H[1::(i\u00001)];k\u00001).Find the optimal thresholds for the rest of the array\n11:t:append (i)\n12:G0;H0 CalcDensity (Gdis;Hdis;t)\n13:f=OptimalFPR (G0;H0;F;k ) .Find optimal false positive rates for the current conﬁguration\n14: if(MinSpaceUsed< SpaceUsed (Gdis;Hdis;t;f))\n15: thenMinSpaceUsed SpaceUsed (Gdis;Hdis;t;f);index i . Remember the best performance uptil now\n16:\n17:Glast PN\nj=indexGdis[j]\n18:Hlast PN\nj=indexHdis[j]\n19:t ThresMaxDivDP (G[1::(index\u00001)];H[1::(index\u00001)];k\u00001)\n20:t:append (index )\n21:G0;H0 CalcDensity (Gdis;Hdis;t)\n22:f=OptimalFPR (G0;H0;F;k )\n23:\n24: returnt,fArray\nas shown in Fig.3(A). The input ﬁrst goes through a Bloom ﬁlter and the negatives are discarded.\nThe positives are passed through the learned model where based on their score s(x)they are either\ndirectly accepted when s(x)>tor passed through another backup Bloom ﬁlter when s(x)\u0014t. In\nour setting, we note that the pre-ﬁlter in the sandwiching approach can be merged with the backup\nﬁlters to yield backup ﬁlters with a modiﬁed false positive rate. Fig.3(B) shows what an equivalent\ndesign with modiﬁed false positive rates would look like. (Here equivalence means we obtain the\nsame false positive rate with the same bit budget; we do not consider compute time.) Thus, we see\nthat the sandwiching approach can be viewed as a special case of the PLBF with two regions.\nHowever, this also tells us we can make the PLBF more efﬁcient by using sandwiching. Speciﬁcally,\nif we ﬁnd when constructing a PLBF with kregions that fi<1for alli, we may assign f0=\nmax 1\u0014i\u0014kfi. We may then use an initial Bloom ﬁlter with false positive rate f0, and change the\ntarget false positive rates for all other intervals to fi=f0, while keeping the same bit budget. This\napproach will be somewhat more efﬁcient computationally, as we avoid computing the learned model\nfor some fraction of non-key elements.\nD.2 Performance against number of regions k\nEarlier, we saw the maximum space saved by using PLBF instead of a normal Bloom ﬁlter is linearly\nproportional to the DKL(^g(t);^h(t)). If we split any region into two regions, the overall divergence\nwould increase because sum of divergences of the two split regions is always more than the original\ndivergence, as shown in Eq.19. Eq.19 is an application of Jensen’s inequality.\n\u0010\n(g1+g2)\u0002log(g1+g2)\n(h1+h2)\u0011\n\u0014\u0010\ng1\u0002logg1\nh1\u0011\n+\u0010\ng2\u0002logg2\nh2\u0011\n(19)\nIncreasing the number of regions therefore always improves the maximum performance. We would\nhope that in practice a small number of regions kwould sufﬁce. This seems to be the the case in our\nexperience; we detail one such experiment in our evaluation(4.2).\n13\n\nfpr0\nPostiveNegativeBloom filter Input (x)\ns(x) > tLearned Model \nfpr1Bloom filter \nPostiveNegativePostives(x) ≤ tInput (x)\ns(x) > tLearned Model \nfpr0* fpr1Bloom filter \nPostiveNegatives(x) ≤ t\nfpr0Bloom filter \nPostiveNegative\n(A) (B)Figure 3: (A) represent LBF with sandwiching.(B) represents a PLBF design equivalent to the sandwiching\napproach.\nD.3 Performance using various Bloom ﬁlter variants\nWe consider how the space saved of the PLBF varies with the type of backup Bloom ﬁlter being\nused. The PLBF can use any Bloom ﬁlter variant as the backup Bloom ﬁlter. When we compare our\nperformance with a Bloom ﬁlter variant, we use that same Bloom ﬁlter variant as the backup Bloom\nﬁlter for a fair comparison.\nFirst, absolute space one can save by using a PLBF instead of a Bloom ﬁlter variant is given in Eq.10.\nThis quantity increases with increasing c6.\nThe relative space one saves by using PLBF instead of the given Bloom ﬁlter variant is shown in\nEq.20. This quantity is the ratio of the space saved by PLBF (as shown in Eq.10) divided by the space\nused by the given Bloom ﬁlter variant ( c\u0002jSj\u0002log2(1=F)) as shown in Eq.20.\n(c\u0002jSj\u0002DKL(^g(t);^h(t))\u0000Size Of Learned Model )\nc\u0002jSj\u0002log2(1=F)(20)\nCancelling the common terms we obtain the following Eq.21.\n\u0012\nDKL(^g(t);^h(t))\nlog2(1=F)\u0000Size Of Learned Model\nc\u0002jSj\u0002log2(1=F)\u0013\n(21)\nThe relative space saved, like the absolute space saved, also increases with increasing c. Thus, both\nthe relative and absolute space saved for the PLBF is higher for a standard Bloom ﬁlter ( c= 1:44)\nthan an optimal Bloom ﬁlter ( c= 1:00), and hence our experiments in Section 4.1 are conservative\nestimates of gains possible in practice using PLBF.\nE Additional Experiments\nE.1 Performance w.r.t standard Bloom ﬁlters\nEarlier, we evaluated our performance using optimal Bloom ﬁlters and here we present results using\nstandard Bloom ﬁlters. As shown in Appendix.D.3, PLBF performs better w.r.t standard Bloom\n6The sizes of standard Bloom ﬁlter variants are proportional to jSj\u0002log2(1=f), where Sis the set it\nrepresents, and fis the false positive rate it achieves. See e.g. Mitzenmacher (2019) for related discussion. The\nconstant cdepends on which type of Bloom ﬁlter is used as a backup. For example, c= log2(e)for standard\nBloom ﬁlter, c= 1:0for the optimal Bloom ﬁlter.\n14\n\nﬁlters than optimal Bloom ﬁlters. As one can see from Fig.4, PLBF performs better than the standard\nBloom ﬁlter.\nKey Key Key Non-Key Non-Key Non-Key \n(B) URLs (C) EMBER (A) Synthetic \nBetter Better Better False Positive rate \nSpace Used(in Kb) \nFalse Positive rate \nFalse Positive rate \nSpace Used(in Kb) Space Used(in Kb) \nFigure 4: FPR vs Space for the (A) Synthetic (B) URLs (C) EMBER datasets for various baselines along with\nkey and non-key score distributions.\nE.2 Performance and Model Quality\nHere we provide an experiment to see how the performance of various methods varies with the\nquality of the model. As discussed earlier, a good model will have high skew of the distributions\ngandhtowards extreme values. We therefore vary the skew parameter of the Zipﬁan distribution\nto simulate the model quality. We measure the quality of the model using the standard F1 score.\nFig.5(B) represents the space used by various methods to achieve a ﬁxed false positive rate of 0:001\nas we vary the F1 score of the model. The ﬁgure shows that as the model quality in terms of the F1\nscore increases, the space required by all the methods decreases (except for the optimal Bloom ﬁlter,\nwhich does not use a model). The space used by all the methods goes to zero as the F1 score goes to\n1, as for the synthetic dataset there is no space cost for the model. The data point corresponding to F1\nscore equal to 0.99 was used to plot Fig.2(A).\nFigure 5: Space used by various baselines as we increase F1 score for Synthetic dataset\n15",
  "textLength": 51098
}