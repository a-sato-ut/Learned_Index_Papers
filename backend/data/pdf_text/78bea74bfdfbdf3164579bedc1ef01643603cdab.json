{
  "paperId": "78bea74bfdfbdf3164579bedc1ef01643603cdab",
  "title": "Piecewise Linear Approximation in Learned Index Structures: Theoretical and Empirical Analysis",
  "pdfPath": "78bea74bfdfbdf3164579bedc1ef01643603cdab.pdf",
  "text": "arXiv:2506.20139v1  [cs.DB]  25 Jun 2025Piecewise Linear Approximation in Learned Index\nStructures: Theoretical and Empirical Analysis\nJiayong Qin∗, Xianyu Zhu†, Qiyu LiuB∗, Guangyi Zhang‡, Zhigang Cai∗, Jianwei Liao∗,\nSha Hu∗, Jingshu Peng§, Yingxia Shao¶, Lei Chen∥\n∗Southwest University,†RUC,‡SZTU,§ByteDanace,¶BUPT,∥HKUST (GZ)\n{jiayongqin1, xianyuzhuruc, qyliu.cs, guangyizhang.jan, liaotoad }@gmail.com, {czg, husha }@swu.edu.cn,\njingshu.peng@bytedance.com, shaoyx@bupt.edu.cn, leichen@cse.ust.hk\nAbstract —A growing trend in the database and system com-\nmunities is to augment conventional index structures, such as B+-\ntrees, with machine learning (ML) models. Among these, error-\nbounded Piecewise Linear Approximation ( ϵ-PLA) has emerged\nas a popular choice due to its simplicity and effectiveness. Despite\nits central role in many learned indexes, the design and analysis\nofϵ-PLA fitting algorithms remain underexplored. In this paper,\nwe revisit ϵ-PLA from both theoretical andempirical perspectives,\nwith a focus on its application in learned index structures. We\nfirst establish a fundamentally improved lower bound ofΩ(κ·ϵ2)\non the expected segment coverage for existing ϵ-PLA fitting\nalgorithms, where κis a data-dependent constant. We then\npresent a comprehensive benchmark of state-of-the-art ϵ-PLA\nalgorithms when used in different learned data structures. Our\nresults highlight key trade-offs among model accuracy, model\nsize, and query performance, providing actionable guidelines for\nthe principled design of future learned data structures.\nIndex Terms —piecewise linear approximation, learned data\nstructure, benchmark\nI. I NTRODUCTION\nAn emerging line of recent research explores augmenting\nconventional data structures, such as B+-tree and Hash table,\nwith simple machine learning (ML) models, leading to the\nconcept of learned index [1]–[3]. By exploiting data distri-\nbution characteristics, these structures optimize their internal\nlayout, achieving superior performance with both space and\ntime efficiency compared to their conventional counterparts,\nas demonstrated in recent benchmark studies [4], [5].\nRather than relying on sophisticated deep learning (DL)\nmodels, learned indexes typically favor simple ML approaches,\nsuch as linear models [6]. This is because DL models often\ndepend on heavy runtimes like Pytorch and Tensorflow, which\nare costly and less flexible for systems with stringent space\nand time requirements. Moreover, unlike many modern ML\ntasks [7], real-world datasets in learned data structure design\nare often easy to learn [5], making even simple models\nsufficiently accurate. Among various ML models, the error-\nbounded piecewise linear approximation model ( ϵ-PLA) is a\npopular choice due to its superior trade-off between its fitting\npower and inference efficiency [6], [8]–[15].\nGiven an array of sorted keysK={k1,···, kn}, the\nconventional indexing problem [16] is to construct a map-\nBCorrespondence to Dr. Qiyu Liu.\nPred IdxTrue IdxKey 1 3 4 7 10 14 23 25 28 31 46 50 54 56 59\n0 1 2 3 4 5 6 7 8 9 10 1112 13 14\n0 1 1 2 3 5 6 7 8 10 9 1112 12 13\nError 0 0 1 1 1 0 0 0 0 -1 1 0 0 1 1(a) Illustration of a toy ϵ-PLA model trained on 15 keys ( ϵ= 1).\nTo search for a query key q= 28 , the segment covering qis first\nfound to compute the predicted index for q, which is ⌊0.46·(28−\n23) + 6 .54⌋= 8. As the provided ϵ-PLA model is error-bounded,\nthe true index for q= 28 lies within the range [7,9].\nLevel[2]\nRoot Segment\nLevel[0]Level[1]\n(b) Illustration of a PGM-Index [6] which recursively constructs ϵ-\nPLA until there is a single model. Note that (s, α, β )encodes a\nlinear segment f(x) =α·(x−s) +β.\nFig. 1: Illustration of the ϵ-PLA model and learned index\nstructure based on ϵ-PLA.\nping from keys to their corresponding sorting indexes I=\n{1,···, n}, defined as f:K 7→ I . Intuitively, fcan be\nregarded as the cumulative distribution function (CDF) of\nK, scaled by a factor of data size n. As illustrated in Fig-\nure 1a, existing fitting algorithms, such as SwingFilter [17]\nand GreedyPLA [18], can find an ϵ-PLA that approximates\nfwith a maximum error bounded by ϵ. Querying an arbi-\ntrary qinvolves two steps: ❶Identify the segment ℓof the\ntrained ϵ-PLA that covers qand compute the predicted index\nℓ(q);❷Perform an exact “last-mile” search within the range\n[ℓ(q)−ϵ, ℓ(q) +ϵ]to locate qin the sorted array K. Existing\nlearned indexes employ different strategies to efficiently locate\nthe segment ℓthat covers a given query key q. For example,\nFITing-Tree [9] adopts a conventional B+-tree to index the\nsegments, while PGM-Index [6] recursively constructs ϵ-PLA\nuntil a pre-defined segment size threshold is reached.\nEfficient and effective ϵ-PLA fitting algorithms are essential\nfor building a wide range of learned indexes [6], [8]–[11]. The\nstudy of ϵ-PLA models dates back to the 1960s [19], and has\nsince been extensively explored, particularly in the context\nof time series data compression [20]. Typical ϵ-PLA fitting\n\nTABLE I: Major results and core assumptions of existing theoretical results on the segment coverage C(ϵ). In summary, we\nestablish a fundamentally improved lower bound under relaxed assumptions with broader applicability.\nExisting Work Major Result Core Assumption(s) Applicable PLA Algorithm(s)\nVLDB 2020 [6] C(ϵ)≥2ϵ N.A. Optimal ϵ-PLA\nICML 2020 [8] C(ϵ) =µ2·ϵ2/σ2i.i.d. assumptions on gaps andϵ≫σ/µ Optimal ϵ-PLA\nOurs C(ϵ)≥κϵ2i.i.d. assumptions on keys Both optimal and greedy ϵ-PLA\nalgorithms can be classified into two types: ❶Optimal ϵ-PLA ,\nwhich aims to minimize the number of segments under the\ngiven error bound ϵ. Typical works include ParaOptimal [17]\nand OptimalPLR [18]. ❷Greedy ϵ-PLA , which prioritizes\nconstruction speed by extending segments incrementally until\nthe error bound is violated, potentially at the cost of using\nmore segments. Typical algorithms are GreedyPLR [18] and\nSwingFilter [21].\nAlthough PLA algorithms have been extensively studied\nand applied for decades, their performance within the context\nof learned indexes remains largely unexplored. For example,\nwhile the PGM-Index [6] employs the optimal SwingFilter\nalgorithm [17] to construct its ϵ-PLA segments, it does not\ninvestigate how alternative PLA algorithms may impact index\nperformance. Motivated by this, in this paper, we comprehen-\nsively revisit ϵ-PLA fitting algorithms for learned indexes from\nboth theoretical andempirical perspectives.\nFrom the theoretical aspect, a key research question for an\narbitrary ϵ-PLA fitting algorithm is: How many line segments\n(or inversely, points covered by each segment) are required\nto satisfy the error bound ϵfor a given dataset? This directly\nimpacts the performance of PLA-based learned index as more\nsegments (or fewer points covered by each segment) improve\naccuracy but increase storage overhead, and vice versa. The\nPGM-Index [6] introduces the first lower bound for segment\ncoverage as 2ϵ, based on the piecewise constant function (a\nspecial case of PLA). Ferragina et al. [8] further derives the\nexpected segment coverage as a quadratic function µ2ϵ2/σ2,\nwhere µandσ2are the mean and variance of the gaps between\nthe consecutive keys, respectively. A key assumption, used\nin [8] and inherited by subsequent studies [10], [22], is that\nthei-th sorted key kiis a realization of a random process\nKi=Ki−1+Gi, where Gi’s, referred to as gaps , are i.i.d. non-\nnegative random variables.\nIn this work, we present a simple yet nontrivial analytical\nframework to re-examine the theoretical foundations of ϵ-PLA.\nRather than impractical gap-based assumptions in previous\nworks [8], [10], [22] (as detailed in Section II-C), we di-\nrectly assume keys are i.i.d. samples drawn from an arbitrary\ndistribution. Under relaxed assumptions, we derive a tighter\nlower bound of the expected coverage per segment in an ϵ-\nPLA, given by Ω(κϵ2), where κis a data-dependent constant.\nAs summarized in Table I, to the best of our knowledge, our\nlower bound on ϵ-PLA is the tightest result for both optimal\nand greedy ϵ-PLA construction algorithms.\nThe established lower bound highlights the worst-case be-\nhavior of generic ϵ-PLA fitting algorithms. Yet, it remains un-\nclear how different ϵ-PLA algorithms impact the practical per-formance of learned indexes. This raises our second research\nquestion: How do different ϵ-PLA fitting algorithms influence\nconstruction time, index size, and query efficiency? To\nanswer this, we introduce PLABench1, the first comprehensive\nbenchmark designed to systematically evaluate the role of ϵ-\nPLA fitting in learned indexes. PLABench provides unified,\noptimized implementations of a wide range of representative\nϵ-PLA algorithms, covering both optimal and greedy strate-\ngies, as well as single- and multi-threading variants, and\nintegrates them into two canonical learned index structures:\nFITing-Tree [9] and PGM-Index [6]. PLABench enables an\nin-depth analysis of the space-time trade-offs induced by\nPLA choices across diverse datasets and query workloads.\nMoreover, PLABench can be easily extended to support future\nPLA-based learned index designs, benefiting model selection\nand hyperparameter tuning.\nTo the best of our knowledge, this is the first work to revisit\nϵ-PLA fitting algorithms from both theoretical and empirical\nperspectives in the context of learned index structures. Our\ntechnical contributions are summarized as follows:\nC1: We present a simple yet effective analytical framework\nfor modeling the segment coverage of ϵ-PLA, avoiding\nimpractical assumptions commonly made in prior work.\nC2: Based on C1, we derive the tightest known lower bound\non the expected segment coverage, Ω(κ·ϵ2), for any\nsorted key set sampled from unknown distributions.\nC3: We introduce PLABench, the first comprehensive bench-\nmark for evaluating the impact of different ϵ-PLA algo-\nrithms on learned index performance, offering practical\nguidance for the design of future PLA-based learned\nindex structures.\nThe remainder of this paper is structured as follows. Sec-\ntion II overviews the basis of ϵ-PLA and existing fitting algo-\nrithms. Section III theoretically analyze the expected segment\ncoverage of existing ϵ-PLA algorithms Section IV introduces\nthe benchmark setup, and Section V presents and discusses\nthe benchmark results. Section VI surveys related works, and\nfinally, Section VII concludes the paper.\nII. P RELIMINARIES\nIn this section, we provide an overview of ϵ-PLA fitting\nalgorithms and discuss existing theoretical results. Table II\nsummarizes the major notations used hereafter.\nA. Error-bounded Piecewise Linear Approximation\nIn general, the error-bounded piecewise linear approxima-\ntion model ( ϵ-PLA) is defined as follows.\n1Our benchmark is made publicly available at https://github.com/bdhxxnix/\nPLABench.\n\nTABLE II: Summary of major notations.\nNotation Explanation\nK,I the input sorted key set and the corresponding index set\nK(i) thei-th order statistics of nrandom keys\nϵ the error bound of an ϵ-PLA model\n(si, αi, βi) the starting point, slope, and intercept of the i-th segment\nℓ(·) a specific linear segment of an ϵ-PLA model\nC(ϵ) the expected segment coverage given ϵ\nρ, γ, ξ data-dependent constants\nDefinition 1 (ϵ-PLA) .Given two monotonically increasing\nlistsX={x1,···, xn}andY={y1,···, yn}, anϵ-PLA of\nmline segments on (X,Y) ={(xi, yi)}i=1,···,nis defined as,\nf(x) =\n\nα1·(x−s1) +β1 ifx∈[s1, s2)\nα2·(x−s2) +β2 ifx∈[s2, s3)\n··· ···\nαm·(x−sm) +βm ifx∈[sm,+∞)(1)\nsuch that |f(xi)−yi| ≤ϵfor∀i= 1,2,···, n.\nWhen used for data indexing, Xis set to a collection of n\nsorted keys {k1,···, kn}andYis set to the corresponding\nindex set {1,···, n}. Then, a fitted ϵ-PLA model functions as\na conventional index structure like B+-tree.\nB.ϵ-PLA Fitting Algorithm\nThe first optimal ϵ-PLA fitting algorithm, ParaOptimal, was\nproposed by O’Rourke in the 1980s [17], aiming to minimize\nthe number of line segments while satisfying a given error\nbound ϵ. Subsequent variants, such as SlideFilter [21] and\nOptimalPLR [18], are theoretically equivalent to ParaOpti-\nmal in terms of their segmentation output. These algorithms\ntypically operate by updating convex hulls in the X × Y\nspace when processing input points in an online manner (i.e.,\n(x1, y1),(x2, y2),···). All aforementioned optimal methods\nachieve O(n)time complexity with O(n)auxiliary space\nduring a single pass over the data.\nIn addition to optimal algorithms, greedy heuristics have\nalso been explored to further reduce the space overhead from\nO(n)toO(1). Typical greedy algorithms include SwingFil-\nter [21] and GreedyPLR [18]. SwingFilter fits a segment using\nthe first point in the stream as a pivot, then incrementally\nadjusts the upper and lower slope bounds to fit subsequent\npoints within the error constraint. GreedyPLR follows a sim-\nilar strategy but selects the pivot as the intersection of two\nmaintained extreme lines, offering a more balanced slope\ninitialization. Table III summarizes existing ϵ-PLA fitting\nalgorithms.\nC. Limitations of Existing Theoretical Results\nAlthough ϵ-PLA fitting algorithms have been explored for\ndecades in various domains, understanding why these algo-\nrithms perform well and how good their solutions truly are\nremains an underexplored area. In particular, we focus on\nthesegment coverage , the average number of keys covered\nby each segment in a fitted ϵ-PLA model. As discussedTABLE III: Summary of existing ϵ-PLA fitting algorithms.\nAlgorithmTime\nComplexitySpace\nComplexityIs\nOptimal?\nParaOptimal [17] O(n) O(n) Yes\nSlideFilter [21] O(n) O(n) Yes\nOptimalPLA [18] O(n) O(n) Yes\nSwingFilter [21] O(n) O(1) No\nGreedyPLA [18] O(n) O(1) No\nin Section I, the PGM-Index [6] establishes a lower bound\nof2ϵon segment coverage based on a piecewise constant\nmodel. Furthermore, by assuming that the i-th sorted key kiis\nsampled from a random process Ki=Ki−1+Gi, where Gi’s\narei.i.d. non-negative random gaps , another recent work [8]\nderives the segment coverage ( notlower bound) as µ2ϵ2/σ2,\nwhere µandσ2are the mean and variance of Gi.\nThe above statistical assumptions are overly idealized in\npractice. First, the unbounded assumption on Kiconflicts with\nreal-world systems, where keys are typically fixed-width inte-\ngers (e.g., uint 32 or uint 64). Second, the “i.i.d.” assumption\non gaps Gidoes not hold when drawing nrandom keys from\nan arbitrary distribution and then sorting them. This is because\nconsecutive order statistics K(i−1),K(i), and K(i+1) are\ncorrelated, resulting in dependencies between adjacent gaps\nGi=K(i)−K(i−1)andGi+1=K(i+1)−K(i). Additionally,\nthe results in [8] are valid only under the condition ϵ≫σ/µ,\nwhich does not hold when ϵis small.\nThese limitations reveal a critical theoretical gap, which\nundermines the practical applicability of this fundamental\nbuilding block of learned indexes and highlights the need for\na more general and realistic theoretical study on ϵ-PLA.\nIII. L OWER BOUND OF SEGMENT COVERAGE\nIn this section, we formally prove the segment coverage of\nboth optimal and greedy ϵ-PLA algorithms, avoiding imprac-\ntical assumptions on gaps. The roadmap is given as follows:\n❶Section III-A formulates the segment coverage and intro-\nduces our improved assumption on input keys.\n❷Based on a constructive algorithm, Section III-B and\nSection III-C analyze the lower bound of segment coverage\nfor uniformly and arbitrarily distributed keys.\n❸Finally, Section III-D extends the results to existing ϵ-PLA\nfitting algorithms.\nA. Segment Coverage and Key Assumptions\nLetK={k1,···, kn}denote a set of nsorted integer\nkeys and I={1,···, n}represent the set of corresponding\nindexes. Rather than relying on previous impractical settings,\nwe directly model the input sorted keys K={k1,···, kn}\nas the realization of order statistics K(1),···, K(n)ofn\ni.i.d. random samples K1,···, Kn, drawn from an arbitrary\ndistribution with a continuous cumulative function (CDF).\nWe assume that all Ki’s are bounded within the range\n[0, ρ], where ρ= Θ( n). This assumption is reasonably made\nto ensure the slope of a line segment remains bounded.\nFor example, when indexing nkeys drawn from a standard\n\nAlgorithm 1 Fixed Range Segmentation (FRS)\nInput: nsorted keys {k1,···, kn}, an error parameter ϵ\nOutput: anϵ-PLA\nInitialize an empty segment set S← {}\nInitialize a segment ℓ(x) =n+1\nkn−k1·(x−k1) + 1\nfori= 1 tondo\nif|⌊ℓ(ki)⌋ −i|> ϵthen\nAppend current segment S←S∪ {ℓ}\nUpdate the segment ℓ(x)←n+1\nkn−k1·(x−ki) +i\nend if\nend for\nreturn S\nuniform distribution U(0, ρ), as shown later in Theorem 3,\nthe slope of the segments in ϵ-PLA should be α=n+1\nρ.\nNote that, in real-world systems, such as indexes in database\nmanagement system [4] and inverted lists in information\nretrieval systems [23], keys are typically unsigned integers\nstored as uint 32 (within [0,232−1]) or uint 64 (within\n[0,264−1]), whose upper bounds can be safely regarded as\nlarge as the data size n.\nDefinition 2 (Segment Coverage) .Given a dataset (X,Y) =\n{(xi, yi)}i=1,···,nand an error constraint ϵ(ϵ≥1), let f=\nA(X,Y)denote the ϵ-PLA learned by a fitting algorithm A,\nand let ℓ(x) =α·x+βrepresent a segment in f. The segment\ncoverage, denoted by C(ϵ), is defined as:\nC(ϵ) =E[I] =X\ni∈Ii·Pr (E1∧E2··· ∧ Ei),(2)\nwhere Eidenotes the event of |ℓ(xi)−yi| ≤ ϵ, r.v. I\nrepresents the number of consecutive occurrences of Ei, and\nI={1,2,···,|I|}is the set of all possible values of i.\nW.l.o.g., we always consider ℓto be the first segment of\nf. We further assume that |I|=o(n), i.e., limn→∞|I|=∞\nandlimn→∞|I|/n= 0, implying that the maximum segment\ncoverage is of lower order than the data size n. Note that, an\nalternative to the probability term in Equation (2) is Pr(E1∧\n···∧Ei∧Ei+1). Both formulations yield equivalent asymptotic\nbehavior in subsequent theoretical analysis. For simplicity, we\nadopt the formulation given in Equation (2) throughout the\nrest of this paper.\nRemarks. In [8], segment coverage is modeled as the mean\nexit time (MET) problem of a random process [24], [25],\nbased on i.i.d. assumptions on gaps . In contrast, here, we\nadopt a fundamentally different statistical model. To provide\nan intuitive interpretation, segment coverage can be related\nto the count of consecutive “heads” in a coin-flipping game,\nwhere each toss has varying probabilities and correlations.\nB. Segment Coverage for Uniform Keys\nWe begin by analyzing the behavior of segment coverage\nfor a simple algorithm constructed just for proof, and then\ngeneralize to practical ϵ-PLA fitting algorithms. As detailed in\nAlgorithm 1, the Fixed Range Segmentation (FRS) algorithm\ngapFig. 2: Illustration of segment coverage by constructing the\nsegment as ℓ(x) =n+1\nρ·x, where K(i−1)andK(i)are two\nconsecutive order statistics, and “gap” stands for the difference\nofK(i)andK(i−1).\nalways fixes the slope of each segment asn+1\nρ, where ρ=\nkn−k1is an estimator of the range of keys.\nTheorem 3 (Segment Coverage for Uniform Keys) .Given a\nset of sorted integers K={k1,···, kn}, w.l.o.g., suppose that\nk1,···, knis a realization of order statistics K(1),···, K(n)\nofni.i.d. random samples K1,···, Kndrawn from U(0, ρ).\nGiven an error threshold ϵ, when nis sufficiently large, the\nexpected coverage of a line segment ℓ(x) =n+1\nρ·xis given\nas follows:\nC(ϵ) = Ω( ϵ2). (3)\nProof. According to [26], the i-th order statistics U(i)of\nstandard uniform distribution (i.e., U(0,1)) follows a beta\ndistribution B(i, n+ 1−i). As K(i)=ρ·U(i), the mean\nand variance of K(i)can be derived as E[K(i)] =ρ·i\nn+1and\nVar[K(i)] =ρ2·i·(n+1−i)\n(n+1)2·(n+2). As illustrated in Figure 2, let\nδi=ℓ(K(i))−idenote the residual between the predicted\nindex for K(i)(i.e.,n+1\nρ·K(i)) and the true index i, we have\nE[δi] = 0 andVar[δi] =i·(n+1−i)\nn+2. According to Definition 2\nand Equation (2), we have:\nC(ϵ)≥|I|X\ni=1i·iY\nj=1Pr (Ej), (4)\ngiven that events Ejpositively correlates2with each other, i.e.,\nPr(E1∧ ··· ∧ Ei)≥Qi\nj=1Pr(Ej). By applying the Cheby-\nshev’s inequality, i.e., Pr(Ej) = Pr( |δj|> ϵ)≤Var[δj]/ϵ2,\nit follows that:\niY\nj=1Pr(Ej)≥iY\nj=1\u0012\n1−j·(n−j+ 1)\n(n+ 2)·ϵ2\u0013\n≈exp\n−iX\nj=1j\nϵ2\n≈exp\u0012\n−i2\n2ϵ2\u0013\n.(5)\n2Intuitively, event Eiis more likely to happen when igets larger, given\nthat the variance of δiis monotonically increasing for i= 1,···,|I|=o(n).\nWhen n→ ∞ ,Var[δi]→ ∞ , leading to Pr(Ei)→0. Thus, when Ei\noccurs, it becomes more likely that Ej<ialso occurs. A more rigorous proof\ncan be made by evaluating Pr(Ej<i|Ei), which is ignored here for brevity.\n\nCombining Equation (4) and Equation (5), the following\nlower bound holds:\nC(ϵ)≥|I|X\ni=1i·exp\u0012\n−i2\n2ϵ2\u0013\n. (6)\nFori≤ϵ, we havei2\n2ϵ2≤1\n2, soexp(−i2\n2ϵ2)≥e−1/2. As\nϵ <|I|andi·exp(−i2\n2ϵ2)is non-negative, Equation (6) can be\nfurther lower bounded as follows:\nC(ϵ)≥ϵX\ni=1i·exp\u0012\n−i2\n2ϵ2\u0013\n≥e−1/2ϵX\ni=1i\n≥e−1/2·ϵ2\n2≈0.303·ϵ2.(7)\nThus, we complete the proof of Theorem 3.\nRemarks. The theoretical results in [8] rely on an implicit\nassumption that ϵ≫σ/µ to apply central limit theorems,\nlimiting their validity for small ϵ. In contrast, our work re-\nmoves this restriction, only assuming a much weaker condition\nn′≫ϵand thereby addressing the theoretical gap for small ϵ.\nC. Segment Coverage for Arbitrary Keys\nTheorem 4 (Segment Coverage for Arbitrary Keys) .Given a\nset of sorted keys K={k1,···, kn}and an error threshold\nϵ, consider that:\nA1:K={k1,···, kn}is the realization of order statistics\nK(1),···, K(n)ofni.i.d. samples K1,···, Kidrawing\nfrom an arbitrary distribution.\nA2: The inverse cumulative function F−1(·)exists and for\narbitrary i∈ {1,···,|I|}, there exists constant γsuch\nthat|n\nρ·F−1(i\nn)−i| ≤γ.\nA3: For arbitrary i∈ {1,···,|I|}, the density function f(·)\nis continuous and non-zero at F−1(i\nn). Moreover, there\nexists a constant ξsuch that f(F−1(i\nn))≥1/ξ.\nUnder A1, A2, and A3, when nis large enough, the expected\ncoverage of a line segment ℓ(x) =n\nρ·xis given by:\nC(ϵ) = Ω\u0012√ρ·(ϵ−γ)2\nξ\u0013\n. (8)\nProof. For general distributions, when n→ ∞ , according\nto [27], the i-th order statistics K(i)asymptotically follows\na normal distribution as follows:\nK(i)d− → N \nF−1\u0012i\nn\u0013\n,i·(n−i)\nn2·[f(F−1(i\nn))]2!\n. (9)\nThus, the i-th residual δi=ℓ(K(i))−ialso exhibits an\nasymptotic normal distribution:\nδid− → N \nmi,i·(n−i)\nρ2·[f(F−1(i\nn))]2!\n, (10)\nwhere mi=n\nρ·F−1\u0000i\nn\u0001\n−i. Asmi̸= 0, different from the\nuniform case, ℓ(Ki)is no longer an unbiased estimator of thetrue index i. Under A2 and A3 as introduced in Theorem 4,\nby employing a similar technique, the segment coverage with\na revised error constraint ϵ+γis given by:\nC(ϵ+γ)≥|I|X\ni=1i·iY\nj=1Pr(|δi−mi| ≤ϵ)\n≥|I|X\ni=1i·iY\nj=1\u0012\n1−iξ2\nρϵ2\u0013\n≈|I|X\ni=1i·exp\u0012\n−i2ξ2\n2ρϵ2\u0013\n= Ω\u0012√ρ·ϵ2\nξ\u0013\n.(11)\nThus, we achieve the results in Theorem 4.\nRemarks. In both Theorem 3 and Theorem 4, the slope of\nline segments is fixed ton+1\nρ, naturally leading to the simple\nFRS algorithm described in Algorithm 1. An interesting ob-\nservation is that, despite being based on relaxed assumptions,\nFRS exhibits an interesting mathematical connection to the\nMET algorithm introduced in [8]. In MET, the slope is fixed\nto1/µandµis the mean of gaps between consecutive keys,\nwhich is conceptually equivalent to FRS as ρ=kn−k1=Pn\ni=2(ki−ki−1).\nD. Extension to Other ϵ-PLA Algorithms\nThe lower bounds in Theorem 3 and Theorem 4 naturally\nextend to the optimal ϵ-PLA fitting algorithms (e.g., ParaOp-\ntimal [17], SlideFilter [21], and OptimalPLA [18]), as these\nmethods guarantee the minimal number of segments under a\ngiven error bound. In this section, we further extend our results\nto cover greedy ϵ-PLA algorithms, such as SwingFilter [21]\nand GreedyPLA [18].\n(a) SwingFilter [21]\n (b) GreedyPLA [18]\nFig. 3: Illustration of two greedy ϵ-PLA fitting algorithms. The\npoint in red ( po) is the selected pivot point.\nAs illustrated in Figure 3, given an error bound ϵ, Swing-\nFilter picks the first point of each segment as the pivot po,\nand initializes the slope range by drawing lines from poto\nthe upper and lower bounds of the second point, denoted as\np2andp2. The slope range is then progressively refined by\nconnecting poto subsequent extreme points (e.g., p3andp3),\nuntil violating the error bound ϵ. GreedyPLA further improves\nSwingFilter by picking the midpoint of p1andp2as the pivot\nand updates the slope range similar to SwingFilter.\n\nTheorem 5 (Superiority of Greedy ϵ-PLA over FRS) .Given\na set of sorted keys K={k1,···, kn}and an error bound ϵ,\nthe segment coverage created by SwingFilter and GreedyPLA\nis always larger than that of FRS (Algorithm 1).\nProof. We prove the claim by induction. For brevity, let L(i)\ndenote the claim: if the first ipoints are covered by segments in\nboth FRS and SwingFilter, and the (i+1)-th point is covered by\nthe FRS segment, then it is also covered by the corresponding\nSwingFilter segment .\nBase case: L(1)is trivially true as SwingFilter initializes its\nsegment using the first two points.\nInductive step: Assuming L(i)holds, we then show that L(i+\n1)also holds. For SwingFilter, the upper and lower bounds of\nthe segment slope when traversing the i-th point are:\nαS= max\nj=2,...,i\u0012j−ϵ−1\nkj−k1\u0013\n,\nαS= min\nj=2,...,i\u0012j+ϵ−1\nkj−k1\u0013\n.(12)\nFor FRS, according to Algorithm 1, a fixed slope of αF=\nn+1\nkn−k1is adopted. Thus, we have:\n|αF·(kj−k1) + 1−j| ≤ϵ, j = 2, . . . , i, (13)\nwhich implies:\nj−1−ϵ\nkj−k1≤αF≤j−1 +ϵ\nkj−k1, j = 2, . . . , i. (14)\nCombining Equation (12) with Equation (14), it holds that\nαS≤αF≤αS. Then, if the (i+ 1)-th point (ki+1, i+ 1) can\nbe covered by FRS’s segment, i.e., |αF·(ki+1−k1) + 1−\n(i+ 1)| ≤ϵ, it always holds that αS·(ki+1−k1) + 1−ϵ≤\ni+ 1≤αS·(ki+1−k1) + 1 + ϵ, implying that (ki+1, i+ 1)\nis also covered by SwingFilter’s segment. Therefore, L(i+ 1)\nholds, and by induction, L(i)holds for all i.\nThus, we formally show that any points covered by a\nsegment in FRS must also be covered by the corresponding\nsegment in SwingFilter, indicating an intrinsically higher cov-\nerage. Notably, the same results hold for GreedyPLA following\na similar approach, which we omit here due to page limits.\nIV. B ENCHMARK SETUP\nThe lower bounds established in Section III fill the theoret-\nical gap in understanding the asymptotic behavior of various\nϵ-PLA algorithms under general data distribution assumptions.\nHowever, existing learned indexes such as PGM-Index [6]\ntypically adopt the OptimalPLA [17] algorithm for index con-\nstruction, leaving the practical impact of alternative PLA fitting\nalgorithms largely unexplored. To address this, we propose\nPLABench , a comprehensive and flexible ϵ-PLA benchmark\ntailored for learned index scenarios.\nA. Research Questions\nIn particular, we aim to answer the following research\nquestions through our PLABench.\nRQ1: Do the lower bounds established in Section III empir-\nically hold for existing ϵ-PLA fitting algorithms?TABLE IV: Summary of benchmark datasets.\nDataset Category #Keys Data Distribution\nfb Real 200M Higly skewed\nbooks Real 800M Mildly skewed\nosm Real 800M Spatially complex, near-uniform\nuniform Synthetic 200M Perfectly uniform\nnormal Synthetic 200M Perfectly Gaussian\nlognormal Synthetic 200M Perfectly right-skewed\nRQ2: How good are existing ϵ-PLA fitting algorithms in\npractical multi-threading environments?\nRQ3: When integrated within learned index structures, what\nare the trade-offs made by greedy ϵ-PLA algorithms\ncompared to the more widely adopted optimal ϵ-PLA\nalgorithms?\nRQ4: How does the error bound ϵaffect the space-time trade-\noffs for each approach?\nB. Baselines and Implementation Details\nWe provide optimized implementations for three represen-\ntative ϵ-PLA fitting algorithms, OptimalPLA [18], Greedy-\nPLA [18], and SwingFilter [21], covering both optimal and\ngreedy approaches. Other optimal algorithms, such as ParaOp-\ntimal [17] and SlideFilter [21], are excluded since they are\ntheoretically equivalent to OptimalPLA and produce identical\nϵ-PLA models given the same input dataset [18].\nGiven that parallel index construction is widely adopted on\nmodern computing platforms [28], we evaluate all algorithms\nunder both single-threading and multi-threading settings. For\nthe parallel implementation, we follow the strategy used in\nthe PGM-Index implementation [29], where the input key set\nKis divided into consecutive, disjoint chunks, and the ϵ-PLA\nalgorithm is applied independently to each chunk. Apparently,\nsuch an embarrassingly parallel strategy will break the opti-\nmality of the OptimalPLA algorithm, which is rarely discussed\nin existing work. Interestingly, our theoretical analysis and\nexperimental results consistently demonstrate that the number\nof additional segments introduced is upper bounded by the\nnumber of threads.\nWe provide three evaluation scenarios in order to answer\nquestions RQ1 toRQ4 .\n•Standalone Evaluation: Each ϵ-PLA fitting algorithm is\ncompared against FRS (Algorithm 1) to validate theoretical\nresults and to provide a detailed analysis of their practical\nperformance on a multi-threading platform.\n•FIT: Each ϵ-PLA algorithm is integrated into the FITing-\nTree [9], a B+-tree-like index where the last-mile tree search\nis replaced with a PLA-guided search. We implement FIT\nby plugging an ϵ-PLA algorithm into an STX B+-tree [30].\n•PGM: Each ϵ-PLA fitting algorithm is integrated into the\nPGM-Index [6], a representative learned index framework\nthat recursively applies ϵ-PLA to predict index positions.\nWe extend the original implementation of PGM-Index [29]\nto support arbitrary ϵ-PLA fitting algorithms.\nAll methods are implemented in C++ and compiled using\ng++ with the -O0 optimization flag to prevent the compiler\n\n2223242526272829210211212213\n/epsilon10.00.51.01.52.02.53.03.54.0#Segments\n×107\ny=6.4×108\n/epsilon12+ 2.7×106GreedyPLA\nOptimalPLA\nSwingFilter\nFRS(a)fb: #Segments vs. ϵ\n2223242526272829210211212213\n/epsilon10.00.20.40.60.81.01.2#Segments\n×108\ny=2.1×109\n/epsilon12+ 7.3×106GreedyPLA\nOptimalPLA\nSwingFilter\nFRS (b)books : #Segments vs. ϵ\n2223242526272829210211212213\n/epsilon10.00.20.40.60.81.01.21.41.6#Segments\n×108\ny=2.5×109\n/epsilon12+ 1.1×107GreedyPLA\nOptimalPLA\nSwingFilter\nFRS\n(c)osm : #Segments vs. ϵ\n2223242526272829210211212213\n/epsilon10.02.04.06.08.0#Segments\n×106\ny=1.5×108\n/epsilon12+ 4.6×104GreedyPLA\nOptimalPLA\nSwingFilter\nFRS (d)uniform : #Segments vs. ϵ\n2223242526272829210211212213\n/epsilon10.00.51.01.52.02.53.03.5#Segments\n×106\ny=5.7×108\n/epsilon12+ 2.3×106GreedyPLA\nOptimalPLA\nSwingFilter\nFRS\n(e)normal : #Segments vs. ϵ\n2223242526272829210211212213\n/epsilon10.00.51.01.52.02.53.03.54.0#Segments\n×107\ny=6.4×108\n/epsilon12+ 2.7×106GreedyPLA\nOptimalPLA\nSwingFilter\nFRS (f)lognormal : Segments vs. ϵ\nFig. 4: Segment count w.r.t. ϵ∈ {22,23,···,213}for different\nϵ-PLA fitting algorithms. The fitted curve for FRS is shown in\nthe figure as #Segments =a·ϵ−2+b. Note that, #segments ∝\nn/coverage.\nfrom applying vectorization optimizations to some algorithms\nwhile not others, thereby ensuring fairness and direct compa-\nrability in the performance comparison of the PLA algorithms.\nAll experiments are conducted on an Ubuntu 22.04 LTS server\nequipped with an Intel(R) Xeon(R) Gold 6430 CPU and 512\nGB of RAM.\nC. Datasets and Query Workloads\nWe adopt 3 commonly used real datasets from a recent\nlearned index benchmark SOSD [31]. fbis a set of user IDs\nrandomly sampled from Facebook [32]. books is a dataset\nof the popularity of books on Amazon. osm is a set of cell\nIDs from OpenStreetMap [33]. In addition, we also generate\nthree synthetic datasets by sampling from uniform, normal,\nand lognormal distributions. All keys in datasets are stored\nas 64-bit unsigned integers ( uint64_t in C++). Table IV\nsummarizes the statistics of evaluated datasets.\nFor the query workload used in our evaluation, we uniformly\nsample 1,000 query keys from each dataset, ensuring that\nthe same set of queries is used across all experiments. We\nfocus primarily on the segment count of the resulting ϵ-PLA\nmodel, as well as the space overhead and query time when\nintegrated with PGM-Index and FITing-Tree. Each experiment\nis repeated 10 times, and we report the average values of the\nmeasured metrics.\n2223242526272829210211212213\n/epsilon10.00.51.01.52.02.53.03.54.0Time (ms)\n×103\nGreedyPLA\nOptimalPLA\nSwingFilter\nFRS(a)fb: Build Time vs. ϵ\n2223242526272829210211212213\n/epsilon12.04.06.08.010.0Time (ms)\n×103\nGreedyPLA\nOptimalPLA\nSwingFilter\nFRS (b)books : Build Time vs. ϵ\n2223242526272829210211212213\n/epsilon10.02.04.06.08.010.012.014.016.0Time (ms)\n×103\nGreedyPLA\nOptimalPLA\nSwingFilter\nFRS\n(c)osm : Build Time vs. ϵ\n2223242526272829210211212213\n/epsilon10.20.40.60.81.0Time (ms)\n×103\nGreedyPLA\nOptimalPLA\nSwingFilter\nFRS (d)uniform : Build Time vs. ϵ\n2223242526272829210211212213\n/epsilon10.51.01.52.02.53.03.54.0Time (ms)\n×103\nGreedyPLA\nOptimalPLA\nSwingFilter\nFRS\n(e)normal : Build Time vs. ϵ\n2223242526272829210211212213\n/epsilon10.01.02.03.04.0Time (ms)\n×103\nGreedyPLA\nOptimalPLA\nSwingFilter\nFRS (f)lognormal : Build Time vs. ϵ\nFig. 5: Construction time w.r.t. ϵ∈ {22,23,···,213}for\ndifferent ϵ-PLA fitting algorithms.\nV. E XPERIMENTAL RESULTS\nIn this section, we present the benchmark results address-\ning the research questions outlined earlier. Specifically, Sec-\ntion V-A reports the standalone evaluation of ϵ-PLA fitting\nalgorithms ( RQ1 and RQ2 ). Section V-B and Section V-C\nshow the results when integrated with FITing-Tree and PGM-\nIndex, respectively ( RQ3 ). Finally, Section V-D analyzes the\nimpact of the error parameter ϵ(RQ4 ).\nA. Standalone Evaluation Results\n1)ϵ-PLA Segment Count ( RQ1 ):Figure 4 reports the\nnumber of segments produced by each ϵ-PLA fitting algo-\nrithm when varying the error bound ϵfrom 22to213. The\nexperimental results clearly show that, for FRS, the number\nof segments is inversely proportional to ϵ2across both real and\nsynthetic datasets, aligning with the lower bounds established\nin Theorem 3 and Theorem 4.\nWhen compared to other ϵ-PLA algorithms, the experi-\nmental results clearly show that the number of segments\nproduced by FRS consistently upper-bounds those generated\nby other methods. For example, on the fbdataset, FRS\nyields approximately 29.34×to41.31×more segments than\nOptimalPLA, and 24.05×to28.89×more than GreedyPLA\nand SwingFilter. These findings align with the theoretical\nanalysis in Theorem 5, which demonstrates the superiority of\nSwingFilter and GreedyPLA over FRS in terms of segment\ncoverage. In addition, an interesting observation is that the\noptimal algorithm shows a significant advantage when the\n\nerror bound ϵis small. However, when ϵexceeds 27= 128 ,\nboth optimal and greedy ϵ-PLA algorithms achieve similar\nsegment counts on both real and synthetic datasets.\nTakeaways. We empirically validate the derived bounds for\na wide range of ϵ-PLA algorithms. In practice, OptimalPLA\ncan reduce the number of segments by up to 1.4×compared\nto greedy algorithms such as SwingFilter and GreedyPLA.\nHowever, this performance gap narrows as ϵgrows; when\nϵ >128, both optimal and greedy algorithms produce a similar\nnumber of segments.\n2)ϵ-PLA Construction Time: We further evaluate the con-\nstruction time for different ϵ-PLA algorithms, and the results\nare presented in Figure 5. Although all the compared ϵ-\nPLA algorithms have a construction time complexity of O(n)\n(see Table III), their actual wall-clock time varies notably\nwith different ϵvalues. For example, when ϵis small (e.g.,\nϵ≤25), the simple FRS algorithm takes significantly longer\ntime than the much more sophisticated OptimalPLA. This is\nbecause FRS generates a large number of segments under\nsmall ϵ, and the overhead of storing these segments into\nin-memory data structures (e.g., std::vector ) dominates\nthe total construction time. When ϵincreases, the segment\ncounts produced by all algorithms converge, this overhead\ndiminishes, and FRS eventually outperforms OptimalPLA due\nto its simplicity.\nA similar trend can be observed for the other two greedy al-\ngorithms. SwingFilter, GreedyPLA, and OptimalPLA demon-\nstrate comparable performance for small ϵvalues (e.g., ϵ=\n22). However, as ϵincreases, the greedy algorithm’s efficiency\nadvantage becomes more significant, achieving up to a 2.0×\nspeedup over OptimalPLA.\nTakeaways. GreedyPLA and SwingFilter can achieve a\nsegment count comparable to OptimalPLA when the error\nbound ϵis large, while saving up to half of the construction\ntime, demonstrating strong potential for practical use. Notably,\nFRS is excluded from this discussion, as it is a constructive\nalgorithm designed only to assist analysis.\n3) Parallel ϵ-PLA Evaluation ( RQ2 ):The previous exper-\niments focus on the performance of serial ϵ-PLA algorithms\nunder single-threaded execution. We now turn to a more\npractical setting by examining their behavior in a multi-\nthreaded environment. As detailed in Section IV-B, we adopt a\nstraightforward data-parallel strategy that partitions the input\ndata and applies the PLA algorithm independently to each\npartition. This inevitably introduces additional segments due\nto data partition. To quantify the impact of multi-threading\nonϵ-PLA fitting quality, we report the increase in segment\ncount by varying the number of threads (i.e., data partitions),\nas shown in Figure 6.\nThe results clearly show that, for both optimal and greedy\nalgorithms, the number of additional segments introduced by\nmulti-threading is upper bounded by the number of threads.\nWe now provide a formal proof of this property for the optimal\nϵ-PLA algorithm.\nTheorem 6 (Upper Bound of Segment Increase for Parallel\n1357911131517192123252729\nThreads051015202530Increment of segments\nGreedyPLA\nOptimalPLA\nSwingFilter\ny = threads(a)fb: Increment of #Segments\nvs. #Tthreads\n1357911131517192123252729\nThreads051015202530Increment of segments\nGreedyPLA\nOptimalPLA\nSwingFilter\ny = threads(b)books : Increment of #Seg-\nments vs. #Threads\n1357911131517192123252729\nThreads051015202530Increment of segments\nGreedyPLA\nOptimalPLA\nSwingFilter\ny = threads\n(c) osm : Increment of #Seg-\nments vs. #Threads\n1357911131517192123252729\nThreads051015202530Increment of segments\nGreedyPLA\nOptimalPLA\nSwingFilter\ny = threads(d)uniform : Increment of #Seg-\nments vs. #Threads\n1357911131517192123252729\nThreads051015202530Increment of segments\nGreedyPLA\nOptimalPLA\nSwingFilter\ny = threads\n(e)normal : Increment of #Seg-\nments vs. #Threads\n1357911131517192123252729\nThreads051015202530Increment of segments\nGreedyPLA\nOptimalPLA\nSwingFilter\ny = threads(f) lognormal : Increment of\n#Segments vs. #Threads\nFig. 6: Increment of segment count w.r.t. the number of\nthreads. The green dashed line marks y=#Threads.\nOptimalPLA) .Given an input key set K, suppose that the\nserial OptimalPLA produces msegments over K. When using\ntthreads to partition and process the data independently, the\nparallel version produces at most m+t−1segments.\nProof. Letpdenote the number of segments produced by\nparallel OptimalPLA. Observe that any segmentation of the\nfull input sequence (including the serial OptimalPLA result)\ninduces a valid segmentation for each chunk. However, chunk-\nlocal OptimalPLA computes the minimum number of seg-\nments per chunk, so it cannot produce more than one extra\nsegment per boundary compared to the serial segmentation.\nAs there are t−1boundaries, the total increase in segment\ncount is at most t−1. Thus, p≤m+ (t−1).\nTakeaways. The empirical and theoretical analyses in this\nsection show that the impact of multi-threading on the results\nof the optimal ϵ-PLA algorithm is bounded by the number of\nthreads. In practice, the total number of segments is typically\nmuch larger than the number of threads (i.e., m≫t), making\nthis overhead negligible. Notably, while Theorem 6 relies\non the optimality guarantees of OptimalPLA, the results can\nbe extended to greedy algorithms by examining the chunk\nboundaries using a similar approach.\nB. FIT Evaluation Results ( RQ3 )\n1) FIT Index Size and Construction Time: Figure 7 reports\nthe index size of FIT constructed using different ϵ-PLA\nalgorithms, with ϵranging from 22to213. The results show\n\n2223242526272829210211212213\n/epsilon10.02.04.06.08.0Index Size\n×108\nGreedyPLA\nOptimalPLA\nSwingFilter(a)fb: Index size vs. ϵ\n2223242526272829210211212213\n/epsilon10.00.51.01.52.02.53.0Index Size\n×109\nGreedyPLA\nOptimalPLA\nSwingFilter (b)books : Index size vs. ϵ\n2223242526272829210211212213\n/epsilon10.00.51.01.52.02.53.0Index Size\n×109\nGreedyPLA\nOptimalPLA\nSwingFilter\n(c)osm : Index size vs. ϵ\n2223242526272829210211212213\n/epsilon10.00.51.01.52.02.53.0Index Size\n×108\nGreedyPLA\nOptimalPLA\nSwingFilter (d)uniform : Index size vs. ϵ\n2223242526272829210211212213\n/epsilon10.00.51.01.52.02.53.03.5Index Size\n×105\nGreedyPLA\nOptimalPLA\nSwingFilter\n(e)normal : Index size vs. ϵ\n2223242526272829210211212213\n/epsilon10.01.02.03.04.05.0Index Size\n×105\nGreedyPLA\nOptimalPLA\nSwingFilter (f)lognormal : Index size vs. ϵ\nFig. 7: Index size of FIT (FITing-Tree + ϵ-PLA) w.r.t. ϵ∈\n{22,23,···,213}for different ϵ-PLA algorithms. The fanout\nof the internal B+-tree is fixed to 16, so that each internal tree\nnode occupies 256 bytes.\nthat GreedyPLA and SwingFilter incur approximately 1.23×\nto1.38×more space overhead than OptimalPLA. This can\nbe attributed to the number of segments produced by different\nϵ-PLA algorithms (see Section V-A). Assuming the fanout of\nthe internal B+-tree is B, the total space overhead of FIT\nisO(m+m/B ), where mdenotes the number of segments\ngenerated by a given ϵ-PLA algorithm.\nWe next compare the construction time of FIT using dif-\nferent ϵ-PLA algorithms. As shown in Figure 8, although\nGreedyPLA and SwingFilter outperform OptimalPLA in the\nstandalone setting (Section V-A), they exhibit slightly longer\nconstruction times on datasets such as fb,osm , and books\nwhen ϵis small. This is because greedy methods tend to\ngenerate more segments, resulting in more leaf nodes and\ngreater overhead in building the internal B+-tree structure,\nthus increasing total construction time. As ϵincreases, the\nnumber of segments produced by greedy methods decreases,\nreducing this overhead and eventually allowing SwingFilter\nand GreedyPLA to outperform OptimalPLA in FIT. Notably,\non highly skewed datasets such as normal and lognormal ,\nGreedyPLA consistently achieves up to a 1.45×speedup over\nOptimalPLA, due to the significantly fewer segments produced\non such datasets.\nTakeaways. GreedyPLA and SwingFilter consistently gen-\nerate more segments across varying ϵ, leading to larger FIT\n2223242526272829210211212213\n/epsilon10.20.40.60.81.01.2Time (ms)\n×104\nGreedyPLA\nOptimalPLA\nSwingFilter(a)fb: Build Time vs. ϵ\n2223242526272829210211212213\n/epsilon11.02.03.04.0Time (ms)\n×104\nGreedyPLA\nOptimalPLA\nSwingFilter (b)books : Build Time vs. ϵ\n2223242526272829210211212213\n/epsilon10.51.01.52.02.53.03.54.04.5Time (ms)\n×104\nGreedyPLA\nOptimalPLA\nSwingFilter\n(c)osm : Build Time vs. ϵ\n2223242526272829210211212213\n/epsilon11.01.52.02.53.03.54.04.5Time (ms)\n×103\nGreedyPLA\nOptimalPLA\nSwingFilter (d)uniform : Build Time vs. ϵ\n2223242526272829210211212213\n/epsilon11.11.21.31.41.51.61.7Time (ms)\n×103\nGreedyPLA\nOptimalPLA\nSwingFilter\n(e)normal : Build Time vs. ϵ\n2223242526272829210211212213\n/epsilon11.11.21.31.41.51.6Time (ms)\n×103\nGreedyPLA\nOptimalPLA\nSwingFilter(f)lognormal : Build Time vs. ϵ\nFig. 8: Construction time of FIT (FITing-Tree + ϵ-PLA)\nw.r.t. ϵ∈ {22,23,···,213}for different ϵ-PLA algorithms.\nindex size and increased building time, particularly when ϵ\nis small. Therefore, for FIT, to fully leverage the efficiency\nadvantages of greedy ϵ-PLA methods, ϵshould be set to a\nrelatively large value (e.g., 25or higher).\n2) FIT Query Processing Time: We now examine the query\nprocessing performance of FITing-Trees constructed using\ndifferent ϵ-PLA algorithms. As shown in Figure 9, the query\ntime generally follows a U-shaped trend. Initially, greedy\nmethods underperform compared to OptimalPLA due to their\nhigher segment counts, which leads to a deeper internal B+-\ntree. However, as ϵincreases, the performance gap narrows.\nThis is because OptimalPLA, while globally minimizing the\nnumber of segments, often produces larger average residuals,\nresulting in higher last-mile search costs. In contrast, Greedy-\nPLA and SwingFilter generate more segments with smaller\nresiduals, leading to more accurate position predictions and\nultimately better query efficiency. Notably, a similar behavior\nis observed in PGM-Index, and we defer detailed discussions\nto Section V-C.\nC. PGM Evaluation Results ( RQ3 )\nUnlike FITing-Tree, which applies a single error parameter\nforϵ-PLA, PGM-Index employs two independent error param-\neters: one for the internal ϵ-PLA model(s) ( ϵi) and another for\nthe last-level ϵ-PLA model ( ϵℓ). For internal search settings,\nwe set 25as a threshold, meaning that when ϵi≤25we use\nlinear search and switch to binary search otherwise.\n\n2223242526272829210211212213\n/epsilon10.00.51.01.52.02.53.03.54.0Time (ns)×103\nSwing Filter\nGreedyPLA\nOptimalPLA(a)fb: Query Time vs. ϵ\n2223242526272829210211212213\n/epsilon10.01.02.03.04.05.06.0Time (ns)×103\nSwing Filter\nGreedyPLA\nOptimalPLA (b)books : Query Time vs. ϵ\n2223242526272829210211212213\n/epsilon10.01.02.03.04.05.06.0Time (ns)×103\nSwing Filter\nGreedyPLA\nOptimalPLA\n(c)osm : Query Time vs. ϵ\n2223242526272829210211212213\n/epsilon10.00.51.01.52.02.5Time (ns)×103\nSwing Filter\nGreedyPLA\nOptimalPLA (d)uniform : Query Time vs. ϵ\n2223242526272829210211212213\n/epsilon10.00.51.01.52.0Time (ns)×103\nSwing Filter\nGreedyPLA\nOptimalPLA\n(e)normal : Query Time vs. ϵ\n2223242526272829210211212213\n/epsilon10.00.20.40.60.81.01.21.4Time (ns)×103\nSwing Filter\nGreedyPLA\nOptimalPLA(f)lognormal : Query Time vs. ϵ\nFig. 9: Query processing time of FIT (FITing-Tree + ϵ-PLA)\nw.r.t. ϵ∈ {22,23,···,213}for different ϵ-PLA algorithms.\nTABLE V: Maximum average residual over all the internal\nlayers of PGM-Index when searching for a set of query keys.\nWe fix ϵiat 32 and vary ϵℓonfbdataset. Notably, the last-mile\nerror ϵℓdoesn’t affect the internal searching time.\nϵℓ OptimalPLA GreedyPLA SwingFilter\n2214.1936 13.1399 13.2728\n2313.3582 12.4875 12.7311\n2412.9545 12.1711 12.1628\n2514.3894 11.8181 11.6613\n2614.0896 11.4472 11.4814\n2715.5535 11.2243 11.0557\n2812.7284 10.5285 10.5566\n2915.0138 9.8464 10.5327\n1) PGM Index Size and Construction Time: Figure 11\nreports the index size of PGM-Index with different ϵ-PLA\nalgorithms when varying the error bound ϵ. Note that we set\nthe internal error bound ϵiand last-mile error bound ϵℓto\nbe the same as ϵ. The results clearly show the superiority\nof OptimalPLA as it guarantees the minimum number of\nsegments generated for each layer. The index size of the PGM-\nIndex constructed by SwingFilter and GreedyPLA is 1.24×\nto1.38×larger than that achieved with OptimalPLA. These\nfindings align with the results in Section V-A.\nWe then compare the construction time of the PGM-Index.\n2223242526272829210211212213\n/epsilon1/lscript(/epsilon1i= 32)0.00.51.01.52.02.5Time (ns)×103\nSwing Filter\nGreedyPLA\nOptimalPLA(a)fb: Query Time vs. ϵ\n2223242526272829210211212213\n/epsilon1/lscript(/epsilon1i= 32)0.00.51.01.52.0Time (ns)×103\nSwing Filter\nGreedyPLA\nOptimalPLA (b)books : Query Time vs. ϵ\n2223242526272829210211212213\n/epsilon1/lscript(/epsilon1i= 32)0.01.02.03.04.0Time (ns)×103\nSwing Filter\nGreedyPLA\nOptimalPLA\n(c)osm : Query Time vs. ϵ\n2223242526272829210211212213\n/epsilon1/lscript(/epsilon1i= 32)0.00.20.50.81.01.21.51.82.0Time (ns)×103\nSwing Filter\nGreedyPLA\nOptimalPLA (d)uniform : Query Time vs. ϵ\n2223242526272829210211212213\n/epsilon1/lscript(/epsilon1i= 32)0.00.51.01.52.02.53.03.5Time (ns)×103\nSwing Filter\nGreedyPLA\nOptimalPLA\n(e)normal : Query Time vs. ϵ\n2223242526272829210211212213\n/epsilon1/lscript(/epsilon1i= 32)0.00.51.01.52.02.5Time (ns)×103\nSwing Filter\nGreedyPLA\nOptimalPLA (f)lognormal : Query Time vs. ϵ\nFig. 10: Query processing time of PGM (PGM-Index + ϵ-\nPLA) w.r.t. ϵ∈ {22,23,···,213}for different ϵ-PLA fitting\nalgorithms.\nIn contrast to their performance on FITing-Tree (see Figure 7,\nFigure 8, Figure 9), greedy algorithms achieve more significant\nimprovements in index construction time when applied to\nPGM-Index. For instance, in Figure 12, on the books dataset,\nGreedyPLA provides up to 3×speedup over OptimalPLA.\nThe reason is that, unlike FITing-Tree, which invokes the ϵ-\nPLA only once for leaf-level index construction, PGM-Index\nrecursively applies ϵ-PLA throughout all levels of the index.\nAs a result, the efficiency advantage of greedy algorithms\nbecomes more pronounced in the context of PGM-Index.\nTakeaways. Greedy ϵ-PLA algorithms achieve significantly\nfaster index construction in the PGM-Index compared to the\nFITing-Tree, due to PGM-Index’s greater dependence on the\nefficiency of the ϵ-PLA algorithm. Notably, this improvement\nbecomes more pronounced on skewed datasets.\n2) PGM Query Processing Time: We now focus on the\nquery efficiency of PGM-Index based on different ϵ-PLA\nfitting methods. When searching for a key, we fix ϵi= 25\nfor simplicity and uniformity. As shown in Figure 10, there is\na U-shaped trend similar to Section V-B. On each dataset, the\nϵ-PLA methods show similar patterns, which will be discussed\nin Section V-D. However, OptimalPLA underperforms slightly\nover GreedyPLA and SwingFilter when ϵℓ=ϵi= 25,\nas shown in Table VII. This might be a counterintuitive\n\n2223242526272829210211212213\n/epsilon1i=/epsilon1/lscript=/epsilon10.00.20.50.81.01.21.51.8Index Size\n×108\nGreedyPLA\nOptimalPLA\nSwingFilter(a)fb: Index Size vs. ϵ\n2223242526272829210211212213\n/epsilon1i=/epsilon1/lscript=/epsilon10.01.02.03.04.05.06.0Index Size\n×108\nGreedyPLA\nOptimalPLA\nSwingFilter (b)books : Index Size vs. ϵ\n2223242526272829210211212213\n/epsilon1i=/epsilon1/lscript=/epsilon10.01.02.03.04.05.06.0Index Size\n×108\nGreedyPLA\nOptimalPLA\nSwingFilter\n(c)osm : Index Size vs. ϵ\n2223242526272829210211212213\n/epsilon1i=/epsilon1/lscript=/epsilon10.01.02.03.04.05.06.0Index Size\n×107\nGreedyPLA\nOptimalPLA\nSwingFilter (d)uniform : Index Size vs. ϵ\n2223242526272829210211212213\n/epsilon1i=/epsilon1/lscript=/epsilon10.01.02.03.04.05.06.07.0Index Size\n×104\nGreedyPLA\nOptimalPLA\nSwingFilter\n(e)normal : Index Size vs. ϵ\n2223242526272829210211212213\n/epsilon1i=/epsilon1/lscript=/epsilon10.00.20.40.60.81.0Index Size\n×105\nGreedyPLA\nOptimalPLA\nSwingFilter (f)lognormal : Index Size vs. ϵ\nFig. 11: Index size time of PGM (PGM-Index + ϵ-PLA)\nw.r.t. ϵ∈ {22,23,···,213}for different ϵ-PLA fitting algo-\nrithms.\nphenomenon because all the ϵ-PLA methods share almost the\nsame height and the exact searching range (i.e., same ϵiand\nϵℓ). To explain this, we calculate the average residual between\nthe predicted indexes and the real indexes within each layer.\nWe fix ϵi= 25and report the largest residual among all\nthe layers with different ϵℓin Table V. Notably, the average\nresidual of OptimalPLA is often larger than greedy methods\nover all ϵℓ, leading to a higher query latency inside internal\nsegments.\nTakeaways. While OptimalPLA minimizes the number of\nsegments, it often leads to larger intra-segment residuals\ncompared to greedy ϵ-PLA algorithms, highlighting a trade-\noff between compression efficacy and prediction precision that\ndirectly impacts the search latency of PGM-Index.\nD. Effects of Hyperparameters ( RQ4 )\n1) Evaluation Results of FIT: We first discuss how ϵaffects\nthe performance of FITing-Tree. As shown in Figure 7 and\nFigure 8, when ϵincreases, both the construction time and\nindex size decrease rapidly. This can be attributed to the\nsignificant decrease in leaf nodes, indicating that FITing-Tree\nperforms better when applying PLA algorithms that generate\nfewer segments.\n2) Evaluation Results of PGM: We now focus on the\nperformance of the PGM-Index under varying settings of the\ninternal error ϵiand last-mile error ϵℓ.\n2223242526272829210211212213\n/epsilon1i=/epsilon1/lscript=/epsilon10.50.81.01.21.51.82.02.2Time (ms)\n×103\nGreedyPLA\nOptimalPLA\nSwingFilter(a)fb: Build Time vs. ϵ\n2223242526272829210211212213\n/epsilon1i=/epsilon1/lscript=/epsilon12.03.04.05.06.07.08.09.0Time (ms)\n×103\nGreedyPLA\nOptimalPLA\nSwingFilter (b)books : Build Time vs. ϵ\n2223242526272829210211212213\n/epsilon1i=/epsilon1/lscript=/epsilon12.03.04.05.06.07.08.0Time (ms)\n×103\nGreedyPLA\nOptimalPLA\nSwingFilter\n(c)osm : Build Time vs. ϵ\n2223242526272829210211212213\n/epsilon1i=/epsilon1/lscript=/epsilon10.40.60.81.01.21.4Time (ms)\n×103\nGreedyPLA\nOptimalPLA\nSwingFilter (d)uniform : Build Time vs. ϵ\n2223242526272829210211212213\n/epsilon1i=/epsilon1/lscript=/epsilon10.40.60.81.01.21.41.6Time (ms)\n×103\nGreedyPLA\nOptimalPLA\nSwingFilter\n(e)normal : Build Time vs. ϵ\n2223242526272829210211212213\n/epsilon1i=/epsilon1/lscript=/epsilon10.40.60.81.01.21.41.6Time (ms)\n×103\nGreedyPLA\nOptimalPLA\nSwingFilter (f)lognormal : Build Time vs. ϵ\nFig. 12: Construction time of PGM (PGM-Index + ϵ-PLA)\nw.r.t. ϵ∈ {22,23,···,213}for different ϵ-PLA fitting algo-\nrithms.\nTABLE VI: Index Size, build time, and query processing time\ncomparison for PGM and FIT w.r.t. ϵ∈ {22,26}across differ-\nentϵ-PLA algorithms (OPT: OptimalPLA, GRY: GreedyPLA,\nSWF: SwingFilter) on books . For PGM, ϵiis fixed to 25and\nwe vary ϵℓ.\nϵPGM FIT\nOPT GRY SWF OPT GRY SWF\nIndex Size\n(MB)22480.85 589.01 600.47 2403.45 2943.43 3000.66\n23257.31 325.46 330.99 1286.31 1626.74 1654.36\n24121.22 165.80 168.34 606.04 828.78 841.49\n2537.61 53.25 53.41 188.01 266.22 267.03\n2612.17 16.42 16.48 60.82 82.10 82.37\nBuild Time\n(s)229.36 6.25 6.52 39.16 43.10 45.41\n237.66 4.31 4.79 21.77 21.88 25.69\n246.30 2.77 3.53 12.66 13.60 13.42\n255.38 1.98 2.71 7.47 7.35 7.62\n264.91 1.61 2.35 5.89 5.06 5.46\nQuery Time\n(µs)222.50 2.80 2.17 2.93 4.75 5.89\n232.27 2.48 2.46 3.29 3.58 5.05\n242.46 1.79 2.38 2.22 2.23 2.29\n251.99 1.98 2.27 1.97 2.13 2.64\n261.77 1.55 1.96 1.73 1.79 1.81\nFigure 11 clearly shows that the index size of PGM-Index\nis dominated by ϵℓ. The memory of the last layer takes up\n98%−99% of the total memory. The ϵican be negligible\nwhen evaluating the index size of PGM-Index.\nAs for the query time, there are three key factors: ❶the\nsearch range, determined by the internal error ϵiand the last-\nmile error ϵℓ,❷the height of the PGM-Index, and ❸the search\nmethod used to locate the target key.\n\nTABLE VII: Internal details of the PGM-Index using different ϵ-PLA methods on fb,books , and osm datasets ( ϵi=ϵℓ=ϵ).\nThe numbers of segments in the last and intermediate layers, listed from bottom to top, are reported in Segments(Last) and\nSegments(Others), respectively.\nDataset Method ϵ Height Segments (Last) Segments (Others) Memory (MB) Query Time (ns)\nfbOptimalPLA225 8338506 {317190 ,3603,12,2} 132.13 1150.21\n234 4235686 {69573 ,92,2} 65.69 1472.59\n244 2120493 {9324,3,2} 32.50 1843.41\n254 1055318 {790,4,2} 16.12 1963.18\nGreedyPLA226 10333876 {492381 ,10322 ,53,4,2} 165.35 1487.99\n235 5341466 {119665 ,293,4,2} 83.33 1620.26\n244 2695175 {18663 ,11,2} 41.42 1565.07\n254 1347023 {1696,4,2} 20.58 2047.63\nSwingFilter226 10628797 {521399 ,11554 ,64,4,2} 170.27 1332.43\n235 5431833 {124224 ,306,4,2} 84.78 1525.67\n244 2721025 {19092 ,11,2} 41.82 1533.47\n254 1354207 {1721,4,2} 20.69 1891.13\nosmOptimalPLA226 29028283 {745706 ,19720 ,586,23,2} 454.61 1371.74\n235 13385178 {169542 ,2460,45,2} 206.86 1612.50\n245 6175398 {39681 ,315,4,2} 94.84 1968.69\n254 2887532 {9726,43,2} 44.20 2980.33\nGreedyPLA227 36903916 {1203364 ,39339 ,1441,58,3,2} 582.17 1745.94\n236 17180689 {276360 ,4872,97,5,3} 266.42 1567.37\n245 7935265 {64184 ,640,11,2} 122.12 2139.56\n254 3702941 {15629 ,81,2} 56.74 2186.38\nSwingFilter227 37950515 {1270704 ,42449 ,1576,66,4,2} 598.98 2190.30\n236 17434082 {284225 ,5073,103,5,2} 270.33 1985.02\n245 7995084 {65118 ,648,12,2} 123.02 2245.46\n254 3718071 {15741 ,82,2} 56.98 2018.77\nbooksOptimalPLA225 31502388 {408728 ,1242,10,2} 487.06 1328.93\n234 16859909 {46531 ,60,2} 257.96 1992.28\n244 7943412 {4098,7,2} 121.27 1933.76\n253 2464239 {278,2} 37.61 1991.28\nGreedyPLA225 38580038 {749695 ,4072,29,2} 600.17 1380.49\n235 21321956 {99384 ,172,4,2} 326.93 1858.17\n244 10862911 {10603 ,13,2} 165.90 1523.33\n254 3489298 {638,3,2} 53.25 1977.94\nSwingFilter225 39330211 {791500 ,4463,27,2} 612.17 1845.98\n235 21683940 {791500 ,4463,27,2} 332.38 1736.26\n244 11029494 {10992 ,12,2} 168.45 1743.90\n254 3499927 {640,3,2} 53.40 2270.63\nAs shown in Figure 13 and Figure 14, when ϵiis fixed and\nϵℓincreases, query latency first decreases and then increases,\nwith a turning point at ϵℓ= 25. Before this point, increasing\nϵℓreduces the overall height of the PGM-Index, shortening\nthe search path and reducing query latency. Beyond ϵℓ= 25,\nthe index height stabilizes, and the expanding search range\nwithin segments becomes the dominant factor, resulting in the\nU-shaped trend observed in Figure 10.\nNotably, when ϵℓis fixed and ϵivaries, we use linear search\nifϵi≤25and binary search otherwise. As shown in Figure 13\nand Figure 14, query latency drops noticeably at ϵi= 26,\nhighlighting the significant performance impact of switchingsearch strategies.\n3) Comparison Between FIT and PGM: We further com-\npare the space-time trade-offs of FIT and PGM in terms of\nindex construction time, memory footprint, and query latency.\nAs shown in Table VI, the flattened structure of PGM-\nIndex [34] achieves a smaller index size and faster construction\ntime compared to FITing-Tree. Moreover, the deeper internal\nB+-tree reduces the query efficiency of FITing-Tree compared\nto PGM-Index under a small ϵ. As ϵincreases, the overall\nheight of FITing-Tree diminishes, resulting in comparable\nquerying latency to PGM-Index. However, PGM-Index ex-\nhibits higher sensitivity to the performance of ϵ-PLA meth-\n\n2223242526272829210211212213internal error /epsilon1i\n22\n23\n24\n25\n26\n27\n28\n29\n210\n211\n212\n213last-mile error /epsilon1/lscript\nOptimalPLA\n1200140016001800200022002400\nTime (ns)\n2223242526272829210211212213internal error /epsilon1i\n22\n23\n24\n25\n26\n27\n28\n29\n210\n211\n212\n213last-mile error /epsilon1/lscript\nGreedyPLA\n12001400160018002000220024002600\nTime (ns)\n2223242526272829210211212213internal error /epsilon1i\n22\n23\n24\n25\n26\n27\n28\n29\n210\n211\n212\n213last-mile error /epsilon1/lscript\nSwing Filter\n12001400160018002000220024002600\nTime (ns)fb: Query Time vs. ϵℓ&ϵi\n2223242526272829210211212213internal error /epsilon1i\n22\n23\n24\n25\n26\n27\n28\n29\n210\n211\n212\n213last-mile error /epsilon1/lscript\nOptimalPLA\n125015001750200022502500275030003250\nTime (ns)\n2223242526272829210211212213internal error /epsilon1i\n22\n23\n24\n25\n26\n27\n28\n29\n210\n211\n212\n213last-mile error /epsilon1/lscript\nGreedyPLA\n120014001600180020002200240026002800\nTime (ns)\n2223242526272829210211212213internal error /epsilon1i\n22\n23\n24\n25\n26\n27\n28\n29\n210\n211\n212\n213last-mile error /epsilon1/lscript\nSwing Filter\n140016001800200022002400\nTime (ns)\nbooks : Query Time vs. ϵℓ&ϵi\n2223242526272829210211212213internal error /epsilon1i\n22\n23\n24\n25\n26\n27\n28\n29\n210\n211\n212\n213last-mile error /epsilon1/lscript\nOptimalPLA\n1500200025003000350040004500\nTime (ns)\n2223242526272829210211212213internal error /epsilon1i\n22\n23\n24\n25\n26\n27\n28\n29\n210\n211\n212\n213last-mile error /epsilon1/lscript\nGreedyPLA\n160018002000220024002600\nTime (ns)\n2223242526272829210211212213internal error /epsilon1i\n22\n23\n24\n25\n26\n27\n28\n29\n210\n211\n212\n213last-mile error /epsilon1/lscript\nSwing Filter\n140016001800200022002400260028003000\nTime (ns)\nosm : Query Time vs. ϵℓ&ϵi\nFig. 13: Query time of PGM-Index with varying ϵℓandϵibased on different ϵ-PLA methods on fb,books ,osm datasets\nods compared to FITing-Tree. (e.g., when ϵ∈ {24,25,26},\nthe building time of PGM-Index varies more significantly\nacross different ϵ-PLA algorithms, whereas FITing-Tree shows\nsmaller fluctuations.)\nVI. R ELATED WORK\nIn this section, we survey and discuss related works from\nthree aspects: piecewise linear approximation, learned index,\nand other learned data sketches.\nPiecewise Linear Approximation. Efficient PLA models\nare critical for scientific data processing and time series\nrepresentation. Early PLA techniques primarily focused on\nminimizing the overall approximation error [35]–[38], but\noften lacked guarantees on per-point error bounds. In contrast,error-bounded PLA algorithms ( ϵ-PLA) offer stronger guaran-\ntees and typically achieve better space efficiency [39], making\nthem more suitable for applications such as learned indexes.\nBased on their design strategies, existing ϵ-PLA algorithms\ncan be broadly categorized as follows:\n❶Optimal PLA. Several works [36], [40], [41] achieve op-\ntimal segmentations for specific error metrics via dynamic\nprogramming, which takes super-linear time complexity. To\naddress scalability, ParaOptimal [17] and its variants like\nSlideFilter [21] and OptimalPLA [18] provide error-bounded\nguarantees while maintaining linear time complexity.\n❷Suboptimal PLA. Heuristic algorithms like [42]–[44] adopt\neither greedy or rule-based strategies to segment time series\ndata. More recent greedy algorithms like SwingFilter [21],\n\n2223242526272829210211212213internal error /epsilon1i\n22\n23\n24\n25\n26\n27\n28\n29\n210\n211\n212\n213last-mile error /epsilon1/lscript\nOptimalPLA\n1000120014001600180020002200\nTime (ns)\n2223242526272829210211212213internal error /epsilon1i\n22\n23\n24\n25\n26\n27\n28\n29\n210\n211\n212\n213last-mile error /epsilon1/lscript\nGreedyPLA\n8001000120014001600\nTime (ns)\n2223242526272829210211212213internal error /epsilon1i\n22\n23\n24\n25\n26\n27\n28\n29\n210\n211\n212\n213last-mile error /epsilon1/lscript\nSwing Filter\n100012001400160018002000\nTime (ns)uniform : Query Time vs. ϵℓ&ϵi\n2223242526272829210211212213internal error /epsilon1i\n22\n23\n24\n25\n26\n27\n28\n29\n210\n211\n212\n213last-mile error /epsilon1/lscript\nOptimalPLA\n8001000120014001600180020002200\nTime (ns)\n2223242526272829210211212213internal error /epsilon1i\n22\n23\n24\n25\n26\n27\n28\n29\n210\n211\n212\n213last-mile error /epsilon1/lscript\nGreedyPLA\n80010001200140016001800\nTime (ns)\n2223242526272829210211212213internal error /epsilon1i\n22\n23\n24\n25\n26\n27\n28\n29\n210\n211\n212\n213last-mile error /epsilon1/lscript\nSwing Filter\n10001500200025003000\nTime (ns)\nnormal : Query Time vs. ϵℓ&ϵi\n2223242526272829210211212213internal error /epsilon1i\n22\n23\n24\n25\n26\n27\n28\n29\n210\n211\n212\n213last-mile error /epsilon1/lscript\nOptimalPLA\n80010001200140016001800200022002400\nTime (ns)\n2223242526272829210211212213internal error /epsilon1i\n22\n23\n24\n25\n26\n27\n28\n29\n210\n211\n212\n213last-mile error /epsilon1/lscript\nGreedyPLA\n80010001200140016001800\nTime (ns)\n2223242526272829210211212213internal error /epsilon1i\n22\n23\n24\n25\n26\n27\n28\n29\n210\n211\n212\n213last-mile error /epsilon1/lscript\nSwing Filter\n800100012001400160018002000\nTime (ns)\nlognormal : Query Time vs. ϵℓ&ϵi\nFig. 14: Query time of PGM-Index with varying ϵℓandϵibased on different ϵ-PLA methods on synthetic datasets\nGreedyPLA [18], and SwingRR [45] aim to minimize the\nnumber of segments while satisfying the error constraint.\nDespite the distinction between optimal and suboptimal\nstrategies, existing work does not clearly establish which\napproach is superior in the context of learned index design.\nOur work addresses this gap by systematically evaluating their\ntrade-offs in terms of space and construction/query efficiency\nwhen integrated into learned index structures.\nLearned Index Structures. Learned indexes model the\ncumulative distribution function (CDF) over sorted keys us-\ning machine learning techniques, offering better space-time\ntrade-offs [34] than traditional tree-based indexes such as\nB+-Tree [46], FAST [47], and Wormhole [48]. RMI [1]\npioneers this paradigm with a static recursive model hierar-chy. ALEX [49] improves dynamic adaptability via a gap-\naware array, while XIndex [50] supports concurrent writes\nthrough two-phase compaction. FITing-Tree [9] replaces B+-\nTree leaves with linear segments, and FINEdex [51] adopts\na flattened layout to enhance dynamic performance. PGM-\nIndex [6] further improves space efficiency by recursively ap-\nplying ϵ-PLA to build a fully flattened index. Recent advances\nalso target hardware-specific optimizations, including GPU-\nfriendly GIndex [52], cache-aware FINEdex-Cache [53], and\ndisk-optimized structures [54], expanding the applicability of\nlearned indexes in diverse system settings.\nIn this work, we focus primarily on PGM-Index and FITing-\nTree, as they represent two distinct paradigms on learned index\ndesign: fully model-based indexing and model-augmented\n\nconventional index structures, respectively.\nOther Learned Data Structures. Beyond data indexing,\nmachine learning techniques have also been applied to enhance\na variety of data structures. Following the taxonomy proposed\nin [55], we categorize learned data structures into three groups\nbased on their application scenarios:\n❶Data-aware Hashing: Representative methods include Se-\nmantic Hashing [56] and Spectral Hashing [57], which pio-\nneered the use of learned projection vectors to replace random\nprojections in hashing.\n❷Approximate Membership: Typical examples are the\nLearned Bloom Filter [1] and its extensions [58]–[60], which\nlearn mapping functions for approximate set membership.\n❸Frequency Estimation: These methods estimate key fre-\nquencies without storing all keys explicitly. A representative\napproach is the Learned Count-Min Sketch [61], which pre-\ntrains a model to identify high-frequency items and assigns\nthem exact counters.\nGiven that ϵ-PLA serves as a simple yet powerful fitting\nmodel across a wide range of applications, it is promising\nto explore its integration with the aforementioned learned\ndata structures, potentially enabling new theoretical insights\nor empirical performance improvements. And we leave these\nfor our future work.\nVII. C ONCLUSION\nThis paper revisits error-bounded Piecewise Linear Approx-\nimation ( ϵ-PLA), a fundamental component in modern learned\nindex structures. We contribute a theoretical lower bound of\nΩ(κ·ϵ2)on segment coverage, offering a clearer understanding\nof the effectiveness of existing fitting algorithms. Comple-\nmenting this analysis, we conduct a comprehensive empirical\nstudy of state-of-the-art ϵ-PLA algorithms in two represen-\ntative learned index frameworks: PGM-Index and FITing-\nTree. Our findings uncover nuanced trade-offs between error\nbound, index size, construction cost, and query performance.\nNotably, while optimal algorithms minimize space usage,\ngreedy algorithms may outperform in query and construction\ntime under suitable error bounds. These results motivate more\nprincipled algorithm selection and parameter tuning when\nbuilding learned index structures. Beyond indexing, we believe\nϵ-PLA holds promise for broader use in other learned data\nstructures, which are left for future exploration.\nREFERENCES\n[1] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” in Proceedings of the 2018 international\nconference on management of data , 2018, pp. 489–504.\n[2] M. Mitzenmacher, “A model for learned bloom filters and optimizing\nby sandwiching,” Advances in Neural Information Processing Systems ,\nvol. 31, 2018.\n[3] J. Qi, G. Liu, C. S. Jensen, and L. Kulik, “Effectively learning spatial\nindices,” Proceedings of the VLDB Endowment , vol. 13, no. 12, pp.\n2341–2354, 2020.\n[4] R. Marcus, A. Kipf, A. van Renen, M. Stoian, S. Misra, A. Kemper,\nT. Neumann, and T. Kraska, “Benchmarking learned indexes,” Proceed-\nings of the VLDB Endowment , vol. 14, no. 1, pp. 1–13, 2020.\n[5] C. Wongkham, B. Lu, C. Liu, Z. Zhong, E. Lo, and T. Wang, “Are up-\ndatable learned indexes ready?” Proceedings of the VLDB Endowment ,\nvol. 15, no. 11, pp. 3004–3017, 2022.[6] P. Ferragina and G. Vinciguerra, “The pgm-index: a fully-dynamic com-\npressed learned index with provable worst-case bounds,” Proceedings of\nthe VLDB Endowment , vol. 13, no. 8, pp. 1162–1175, 2020.\n[7] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,” nature , vol. 521,\nno. 7553, pp. 436–444, 2015.\n[8] P. Ferragina, F. Lillo, and G. Vinciguerra, “Why are learned indexes so\neffective?” in International Conference on Machine Learning . PMLR,\n2020, pp. 3123–3132.\n[9] A. Galakatos, M. Markovitch, C. Binnig, R. Fonseca, and T. Kraska,\n“Fiting-tree: A data-aware index structure,” in Proceedings of the 2019\ninternational conference on management of data , 2019, pp. 1189–1206.\n[10] D. Chen, W. Li, Y . Li, B. Ding, K. Zeng, D. Lian, and J. Zhou, “Learned\nindex with dynamic ϵ,” in ICLR . OpenReview.net, 2023.\n[11] Y . Dai, Y . Xu, A. Ganesan, R. Alagappan, B. Kroth, A. Arpaci-Dusseau,\nand R. Arpaci-Dusseau, “From {WiscKey }to bourbon: A learned index\nfor{Log-Structured }merge trees,” in 14th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI 20) , 2020, pp.\n155–171.\n[12] Q. Liu, Y . Shen, and L. Chen, “Lhist: towards learning multi-dimensional\nhistogram for massive spatial data,” in 2021 IEEE 37th International\nConference on Data Engineering (ICDE) . IEEE, 2021, pp. 1188–1199.\n[13] ——, “HAP: an efficient hamming space index based on augmented\npigeonhole principle,” in Proceedings of the 2022 International Confer-\nence on Management of Data , 2022, pp. 917–930.\n[14] Q. Liu, Y . Luo, M. Cui, S. Han, J. Peng, J. Li, and L. Chen, “Bittuner:\nA toolbox for automatically configuring learned data compressors,” in\n2025 IEEE 41st International Conference on Data Engineering (ICDE) .\nIEEE Computer Society, 2025, pp. 4548–4551.\n[15] Q. Liu, S. Han, J. Liao, J. Li, J. Peng, J. Du, and L. Chen, “Learned\ndata compression: Challenges and opportunities for the future,” arXiv\npreprint arXiv:2412.10770 , 2024.\n[16] R. Bayer and E. McCreight, “Organization and maintenance of large\nordered indices,” in Proceedings of the 1970 ACM SIGFIDET (Now\nSIGMOD) Workshop on Data Description, Access and Control , 1970,\npp. 107–141.\n[17] J. O’Rourke, “An on-line algorithm for fitting straight lines between data\nranges,” Commun. ACM , vol. 24, no. 9, pp. 574–578, 1981.\n[18] Q. Xie, C. Pang, X. Zhou, X. Zhang, and K. Deng, “Maximum error-\nbounded piecewise linear representation for online stream approxima-\ntion,” VLDB J. , vol. 23, no. 6, pp. 915–937, 2014.\n[19] S. H. Cameron, “Piece-wise linear approximations,” Technical note\nCSTN-106, Computer Sciences Division, IIT Research Institute, Chicago,\nIL, 1966.\n[20] G. Chiarot and C. Silvestri, “Time series compression survey,” ACM\nComputing Surveys , vol. 55, no. 10, pp. 1–32, 2023.\n[21] H. Elmeleegy, A. K. Elmagarmid, E. Cecchet, W. G. Aref, and\nW. Zwaenepoel, “Online piece-wise linear approximation of numerical\nstreams with precision guarantees,” Proceedings of the VLDB Endow-\nment , vol. 2, no. 1, pp. 145–156, 2009.\n[22] A. Boffa, P. Ferragina, and G. Vinciguerra, “A learned approach to\ndesign compressed rank/select data structures,” ACM Transactions on\nAlgorithms (TALG) , vol. 18, no. 3, pp. 1–28, 2022.\n[23] G. E. Pibiri and R. Venturini, “Techniques for inverted index compres-\nsion,” ACM Computing Surveys (CSUR) , vol. 53, no. 6, pp. 1–36, 2020.\n[24] C. W. Gardiner, “Handbook of stochastic methods for physics, chemistry\nand the natural sciences,” Springer series in synergetics , 1985.\n[25] S. Redner, A guide to first-passage processes . Cambridge university\npress, 2001.\n[26] J. E. Gentle, Computational statistics . Springer, 2009, vol. 308.\n[27] M. Cardone, A. Dytso, and C. Rush, “Entropic central limit theorem\nfor order statistics,” IEEE Transactions on Information Theory , vol. 69,\nno. 4, pp. 2193–2205, 2022.\n[28] Y . Akhremtsev and P. Sanders, “Fast parallel operations on search trees,”\nin2016 IEEE 23rd International Conference on High Performance\nComputing (HiPC) . IEEE, 2016, pp. 291–300.\n[29] “PGM-Index,” https://github.com/gvinciguerra/PGM-index, accessed:\n2025-06-10.\n[30] “STX B+ Tree C++ template classes,” https://panthema.net/2007/\nstx-btree/, accessed: 2025-06-10.\n[31] R. Marcus, A. Kipf, A. van Renen, M. Stoian, S. Misra, A. Kemper,\nT. Neumann, and T. Kraska, “Benchmarking learned indexes,” Proc.\nVLDB Endow. , vol. 14, no. 1, p. 1–13, Sep. 2020.\n\n[32] P. Van Sandt, Y . Chronis, and J. M. Patel, “Efficiently searching\nin-memory sorted arrays: Revenge of the interpolation search?” in\nProceedings of the 2019 International Conference on Management\nof Data , ser. SIGMOD ’19, New York, NY , USA, 2019, p. 36–53.\n[Online]. Available: https://doi.org/10.1145/3299869.3300075\n[33] openstreetmap, “Openstreetmap,” [n.d.]. [Online]. Available: https:\n//www.openstreetmap.org/\n[34] Q. Liu, S. Han, Y . Qi, J. Peng, J. Li, L. Lin, and L. Chen, “Why are\nlearned indexes so effective but sometimes ineffective?” arXiv preprint\narXiv:2410.00846 , 2024.\n[35] U. Appel and A. V . Brandt, “Adaptive sequential segmentation of\npiecewise stationary time series,” Information sciences , vol. 29, no. 1,\npp. 27–56, 1983.\n[36] R. Bellman, “On the approximation of curves by line segments using\ndynamic programming,” Communications of the ACM , vol. 4, no. 6, p.\n284, 1961.\n[37] C. Pang, Q. Zhang, X. Zhou, D. Hansen, S. Wang, and A. Maeder,\n“Computing unrestricted synopses under maximum error bound,” Algo-\nrithmica , vol. 65, pp. 1–42, 2013.\n[38] M. Garofalakis and A. Kumar, “Wavelet synopses for general error\nmetrics,” ACM Transactions on Database Systems (TODS) , vol. 30,\nno. 4, pp. 888–928, 2005.\n[39] H. Zhao, T. Li, G. Chen, Z. Dong, M. Bo, and C. Pang, “An online pla\nalgorithm with maximum error bound for generating optimal mixed-\nsegments,” International Journal of Machine Learning and Cybernetics ,\nvol. 11, pp. 1483–1499, 2020.\n[40] E. Terzi and P. Tsaparas, “Efficient algorithms for sequence segmen-\ntation,” in Proceedings of the 2006 SIAM International Conference on\nData Mining . SIAM, 2006, pp. 316–327.\n[41] C.-J. Wu, W.-S. Zeng, and J.-M. Ho, “Optimal segmented linear re-\ngression for financial time series segmentation,” in 2021 International\nConference on Data Mining Workshops (ICDMW) , 2021, pp. 623–630.\n[42] E. Keogh, S. Chu, D. Hart, and M. Pazzani, “Segmenting time series:\nA survey and novel approach,” Data Mining in Time Series Databases ,\nvol. 57, 03 2003.\n[43] X. Liu, Z. Lin, and H. Wang, “Novel online methods for time series\nsegmentation,” IEEE Transactions on Knowledge and Data Engineering ,\nvol. 20, no. 12, pp. 1616–1626, 2008.\n[44] Y . Hu, P. Guan, P. Zhan, Y . Ding, and X. Li, “A novel segmentation and\nrepresentation approach for streaming time series,” IEEE Access , vol. 7,\npp. 184 423–184 437, 2019.\n[45] J.-W. Lin, S.-w. Liao, and F.-Y . Leu, “A novel bounded-error piece-\nwise linear approximation algorithm for streaming sensor data in edge\ncomputing,” in Advances in Intelligent Networking and Collaborative\nSystems , L. Barolli, H. Nishino, and H. Miwa, Eds. Cham: Springer\nInternational Publishing, 2020, pp. 123–132.\n[46] D. Comer, “Ubiquitous b-tree,” ACM Comput. Surv. , vol. 11, no. 2, p.\n121–137, Jun. 1979.\n[47] C. Kim, J. Chhugani, N. Satish, E. Sedlar, A. D. Nguyen, T. Kaldewey,\nV . W. Lee, S. A. Brandt, and P. Dubey, “Fast: fast architecture sensitive\ntree search on modern cpus and gpus,” in Proceedings of the 2010\nACM SIGMOD International Conference on Management of Data ,\nser. SIGMOD ’10. Association for Computing Machinery, 2010, p.\n339–350.\n[48] X. Wu, F. Ni, and S. Jiang, “Wormhole: A fast ordered index for in-\nmemory data management,” ser. EuroSys ’19. New York, NY , USA:\nAssociation for Computing Machinery, 2019.\n[49] J. Ding, U. F. Minhas, J. Yu, C. Wang, J. Do, Y . Li, H. Zhang,\nB. Chandramouli, J. Gehrke, D. Kossmann et al. , “Alex: an updatable\nadaptive learned index,” in Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data , 2020, pp. 969–984.\n[50] C. Tang, Y . Wang, Z. Dong, G. Hu, Z. Wang, M. Wang, and H. Chen,\n“Xindex: a scalable learned index for multicore data storage,” in\nProceedings of the 25th ACM SIGPLAN Symposium on Principles and\nPractice of Parallel Programming , ser. PPoPP ’20. New York, NY ,\nUSA: Association for Computing Machinery, 2020, p. 308–320.\n[51] P. Li, Y . Hua, J. Jia, and P. Zuo, “Finedex: a fine-grained learned index\nscheme for scalable and concurrent memory systems,” Proc. VLDB\nEndow. , vol. 15, no. 2, p. 321–334, Oct. 2021.\n[52] J. Liu, F. Zhang, L. Lu, C. Qi, X. Guo, D. Deng, G. Li, H. Zhang,\nJ. Zhai, H. Zhang, Y . Chen, A. Pan, and X. Du, “G-learned index:\nEnabling efficient learned index on gpu,” IEEE Transactions on Parallel\nand Distributed Systems , vol. 35, no. 6, pp. 950–967, 2024.[53] J. Zhang and Y . Gao, “Carmi: a cache-aware learned index with a cost-\nbased construction algorithm,” Proc. VLDB Endow. , vol. 15, no. 11, p.\n2679–2691, Jul. 2022.\n[54] J. Zhang, K. Su, and H. Zhang, “Making in-memory learned indexes\nefficient on disk,” Proc. ACM Manag. Data , vol. 2, no. 3, May 2024.\n[55] G. Ferragina, Paoloand Vinciguerra, Learned Data Structures . Cham:\nSpringer International Publishing, 2020, pp. 5–41.\n[56] R. Salakhutdinov and G. Hinton, “Semantic hashing,” International\nJournal of Approximate Reasoning , vol. 50, no. 7, pp. 969–978, 2009,\nspecial Section on Graphical Models and Information Retrieval.\n[57] Y . Weiss, A. Torralba, and R. Fergus, “Spectral hashing,” in Proceedings\nof the 22nd International Conference on Neural Information Processing\nSystems , ser. NIPS’08. Red Hook, NY , USA: Curran Associates Inc.,\n2008, p. 1753–1760.\n[58] M. Mitzenmacher, “A model for learned bloom filters, and optimizing\nby sandwiching,” in Proceedings of the 32nd International Conference\non Neural Information Processing Systems , ser. NIPS’18. Red Hook,\nNY , USA: Curran Associates Inc., 2018, p. 462–471.\n[59] J. Rae, S. Bartunov, and T. Lillicrap, “Meta-learning neural bloom\nfilters,” in International Conference on Machine Learning . PMLR,\n2019, pp. 5271–5280.\n[60] Q. Liu, L. Zheng, Y . Shen, and L. Chen, “Stable learned bloom filters\nfor data streams,” Proceedings of the VLDB Endowment , vol. 13, no. 12,\npp. 2355–2367, 2020.\n[61] C.-Y . Hsu, P. Indyk, D. Katabi, and A. Vakilian, “Learning-based fre-\nquency estimation algorithms.” in International Conference on Learning\nRepresentations , 2019.",
  "textLength": 77573
}