{
  "paperId": "0248c7c91681e642231749bbb807849556a3cfb7",
  "title": "Artificial Intelligence in the Low-Level Realm - A Survey",
  "pdfPath": "0248c7c91681e642231749bbb807849556a3cfb7.pdf",
  "text": "Artificial Intelligence in the Low-Level Realm – A\nSurvey \nVahid Mohammadi Safarzadeh Hamed Ghasr Loghmani\nAbstract—Resource-aware  machine  learning  has  been  a\ntrending  topic  in  recent  years,  focusing  on  making  ML\ncomputational aspects more exploitable by the edge devices in\nthe  Internet  of  Things.  This  paper  attempts  to  review  a\nconceptually  and  practically  related  area  concentrated  on\nefforts  and  challenges  for  applying  ML  in  the  operating\nsystems' main tasks in a low-resource environment. Artificial\nIntelligence has been integrated into the operating system with\napplications such as voice or image recognition. However, this\nintegration is only in user space. Here, we seek methods and\nefforts  that  exploit  AI  approaches,  specifically  machine\nlearning, in the OSes' primary responsibilities. We provide the\nimprovements that ML can bring to OS to make them more\ntrustworthy. In other words, the main question to be answered\nis how AI has played/can play a role directly in improving the\ntraditional  OS  kernel  main  tasks.  Also,  the  challenges  and\nlimitations in the way of this combination are provided.\nKeywords—Machine  Learning;  Operating  Systems;  Kernel\nSpace; Internet of Things (IoT)\nI.INTRODUCTION \nArtificial Intelligence  and  its  famous  subcategory,\nMachine  Learning,  is  the  current  trend  in science.  It has\naffected many scientific and technological applications such\nas medicine, science, and industry. However, it is interesting\nthat  OSes'  low-level  operations  as  the  basics  for  all\ncomputing  jobs  have  remained  almost  untouched  by  this\nrevolutionary science. OS algorithms seem very similar to\nthat of twenty years ago with no sign of intelligence in its\ncurrent meaning; learning and adapting. Although AI does\nnot guarantee an improvement in every application and also\nthe  idea  of  applying  it  to  any  traditional  procedure  and\nexpecting miracles seems naive, the origins of uncertainty\nand  unpredictability  that  OSes  are  facing  now  can  be\nreasonable for AI practitioners to see them as problems to be\nsolved via the available tools of AI.\nAt first sight, mechanisms such as memory management\nand process scheduling are the playgrounds that AI can help\nOS  with  its  current  procedures.  A  computer  executes\nprograms. The programs and the data they need to access\nshould be kept, at least partially, in the main memory. A\nmemory management scheme is for managing the presence\nand replacement of the data in the main memory to maximize\nthe CPU utilization and minimize the response time to users.\nFor the same purposes, the process schedulers choose one\nprocess from all ready-to-run or waiting for processes to be\nexecuted by the CPU. However, several challenges need to\nbe addressed. OSes are programs that resulted from many\nyears of development. They must remain as light-weight as\npossible while handling the most critical job in a system in\nthe most deterministic and flawless manner. Its actions also\nhave  many  parameters,  consequences,  and  players.\nTherefore,  in  OSes,  as  an  automated  and  deterministic\nmechanism,  AI  can  have  a  role  in  ways  that  do  not\ncompromise the system's correctness [1] . Here, our focus is\non the kernel  space  applications  of  AI  in  OS.  But some\napplications exploit both user and kernel space, mostly due to\nthe resource limitation in the kernel space.Algorithms used in OS design are either deterministic or\nheuristic.  A  deterministic  algorithm  produces  the  same\noutput for specific given input and in any number of runs,\ngoing through the same states. On the other hand, a heuristic\nalgorithm offers a procedure to achieve an optimal or near-\noptimal solution to an optimization problem [2] with which\nthe output may change for the same data in different runs.\nThe heuristic ones are mainly more monitored trial and error\nmethods.  For  example,  for  resource  scheduling,  these\nheuristics  are  adjusted  through  monitoring  the  changes\noccurring  in  the  system's  performance  while\nincreasing/decreasing specific resources for a process [3] .\nThe overwhelming wave of ML computational methods\nis affecting the lower level of computers. Several concepts\nsuch  as  learned  data  structures  (Indexes)  [4] ,  learned\ndatabase  systems  (for  optimizing  queries)  [5] ,  learned\nindexes  [6]–[8] and  learning  memory  hit  pattern  [9]  are\nintroduced, recently, which shows the applications of AI into\nthe low-level world of computation. The main feature of ML\nis the ability to exploit experiences of the past for future\ndecisions.\nSearching  in  configuration  spaces  and  predicting  the\nsystem's future states, timings, or sizes are jobs that OSes are\ncontinuously  performing.  These  operations  become  even\nmore crucial in applications such as storing systems, web\nservers, High-Performance Computing systems, clouds, and\nreal-time applications.\nIn this paper, along with analysing some challenges of\nusing AI to enhance OS operations, previous efforts in this\narea  are  analysed.  Linux's open-source  nature  makes  it  a\nsuitable platform to apply AI methods to an OS's different\ntasks, although the implementation is challenging, as we will\nencounter in this paper.\nThere  also  have  been  efforts  to  make  hardware\narchitecture more intelligent using ML. For example, in [9] ,\nthe  prefetching  operation  (in  which  hardware  predicts\nmemory accesses before the request by software based on\nhistory) is modelled as a sequence prediction problem. They\nreported a solution to this challenge using Long Short-Term\nMemory (LSTM) Recurrent Neural Networks. However, we\ndo not encounter such integrations in this paper.\nThe  kernel,  services,  and Shell constitute an OS. The\nkernel is the central part, and other parts can be seen as tools\nto  interact  with  the  kernel  and  send  requests  to  it.  The\nprimary responsibilities of OSes are managing software and\nhardware as well as providing system services.\nSome of these services fulfil users' (processes) requests\nsuch  as  I/O  requests,  file-system  manipulation,  or  inter-\nprocess  communications.  Others  are  for  increasing  the\nsystem's  efficiency.  These  services  are  mainly  related  to\nsharing resources among processes or threads using resource\nallocation algorithms; we mean CPU cycles, main memory,\nfile storage, or I/O devices [10] .\nAnother important job of OS (and hardware) is caching.\nWith the increasing  popularity of  cloud computing, some\napproaches towards programming have been changed. One\n\nof the most important changes was using microservices to\nwrite  applications  instead  of  the  traditional  monolithic\nmanner in which all parts of the program were in one piece.\nIn  a  microservice-based  development  style,  a  single\napplication is delivered as several small services developed\nindependently and loosely coupled. As a result, the program\nbecomes  highly  maintainable  and  testable.  Each  service\nneeds  to  communicate  with  the  user  and  other  services.\nObviously, breaking programs into microservices  provides\nnew challenges to OSes in handling the requests of these\nservices. Dealing with the massive amount of microservices\ndemands  can  not  be  done  optimally  with  traditional\noperational methods [3] .\nMachine  Learning  is  a  programming  approach  to  use\nexperiences for making better future decisions. Each learning\napplication is divided into two phases: Training to produce a\nmodel and Inference based on that model. The first phase is\nmore  resource-demanding  and  time-consuming  than  the\nsecond. A subcategory of ML, Deep Learning, has gained\nspecial  attention  in  recent  years  with  the  advent  of  new\ntechnologies and the availability of more data. Deep learning\nis  based  on  the  usage  of  artificial  neural  networks  with\nrepresentation  learning.  The  performance  of  machine\nlearning methods is heavily dependent on the choice of data\nrepresentation (or features) on which they are applied [11].\nRepresentation learning offers methods to automatically find\nthe desirable data representation (or features).\nIn this paper, we first make a connection between the\nmain  goal  of  AI  (ML)  methods  which  is  dealing  with\nuncertainties and the operating system’s tasks in which the\nOS faces unpredictable situations. Then, we analyse several\nreported efforts in the literature in integrating ML with OS.\nFinally, we encounter some challenges and possible solutions\nfor this integration. The conclusion and future of the idea of\nembedding ML into OS come in the last section. In addition,\nsince  the  subject  of  this  paper  is  somehow  an\ninterdisciplinary  topic  in  computer  science,  we  provide\nenough information and definitions to make it self-contained\nso that any expert from both domains (AI or OS) can use it\nsmoothly.\nII.ORIGINS OF UNCERTAINTY IN OPERATING SYSTEM\nOS designers try to make their systems as predictable as\npossible. By predictability, we mean the system's ability to\nevaluate  each  task's  timing  and  resource  properties.  One\nsmart idea is for OS to provide the exact amount of resources\nthat every algorithm requires when it is called to perform.\nThis behaviour will optimize the OS (server) load and reduce\nthe  energy  that  it  needs.  On  the  other  hand,  such\npredictability  may  have  security  drawbacks  because  it\nsimplifies how malware designers can communicate with OS\n[12],  [13].  Therefore,  any  approach  (including  ML\napproaches) should consider such a threat to the final system.\nSome sources of uncertainty in OSes like the uncertainty\nin reading timestamps resulted from Linux Kernel overhead,\non which the load of the system and CPU clock speed has a\ndirect impact [14]. Although such uncertainties may not be\nharmful in desktop or even traditional server usages of OSes,\ndespite the energy deficiency that they cause, they are crucial\nin real-time and embedded OSes. The majority of embedded\ndevices tend to fulfil a specific task using real-time operating\nsystems. A real-time operating system has well-defined and\nfixed  time  constraints  [15].  The  system’s  operation  is\nacceptable only if the task is accomplished within the defined\nconstraints.\nThe procedure of sharing resources in OSes during which\nso  much  interference  (allocation  and  deallocation  of\nresources)  occurs  is  another  reason  for  OSes'unpredictability. Predictable latency is an important issue in\nresource management in cloud computing [3] , [16]–[19].\nThe nature of multiprocessor architectures is associated\nwith unpredictability. Utilizing a particular type of memory\naccess  is  an  example  of  resource  sharing  that  increases\nunpredictability in an OS. There are two types of shared-\nmemory multiprocessors: Uniform Memory Access (UMA)\nand  Non-Uniform  Memory  Access  (NUMA)  [20]  .  In\nNUMA systems, each processor has special (efficient) access\nto  a  memory  section,  called  a  NUMA  node.  For  each\nprocess, memory can be allocated  locally (from  the local\nnode) to the corresponding CPU [21]. This allocation process\nis called NUMA placement. If the local node can not fulfil\nthe memory request, the kernel (Linux) will seek the remote\noptions [22] . NUMA placement is a source of uncertainty,\nand some works such as [23]  have proposed ML models to\npredict its influence on applications' performance.\nOther causes of unpredictability, as described in [20] , are\npipeline  optimization,  cache  interference,  which  increase\nmemory traffic, and the NP-hard problem of scheduling in\nassigning  tasks  to  processors.  Also,  as  mentioned  before,\nmany OS algorithms consist of heuristics. For example, when\nall the page frames in memory are allocated, the kernel must\ndecide which old pages can be freed and prevent them from\nbeing used by new processes. For this purpose, heuristics like\nthe Least Recently Used (LRU) are being used [24] . LRU is\noriginally a memory frame replacement policy (usually in\ncache)  in  which  the  least  recently  used  items  will  be\nabandoned first. The implementation of this idea might seem\nstraightforward,  while  keeping  the  track  of  least  recently\nused items may become expensive. However, still designing\nan acceptable page frame reclaiming method is a trial and\nerror  job. Thus, it becomes  a  situation where  the system\nbecomes  unpredictable;  therefore,  a  perfect  candidate  for\ngetting  help  from  AI  techniques  such  as  \"Reinforcement\nLearning (RL).” In RL, we examine how an agent can learn\nfrom success and failure, from reward and punishment [25]\n[15]. To follow a supervised learning approach, knowledge\nabout  the  correct  move  of  an  agent  in  every  step  is\nmandatory. Such information is seldom available in complex\nenvironments  like  operating  systems.  Hence,  a\nreward/punishment mechanism and a policy to maximize the\nexpected reward would be feasible for such environments.\nThe task of reinforcement learning is to use observed rewards\nto  learn  an  optimal  (or  nearly  optimal)  policy  for  the\nenvironment [15]. In the next section, we investigate how\nML can be beneficial to improve OS tasks.\nIII.WHAT IS DATA OR THE PROBLEM DEFINITION\nTo  look  at  the  operating  system  as  a  case  study  for\napplying  artificial  intelligence  and  machine  learning,  we\nmust first define the problem accordingly. One way is to look\nat  an  operating  system  as  a  dynamic  environment  that\ncontains  a  few  agents,  limitations  and  rules  [15].  In  this\npaper’s definition, the OS kernel is the agent, its effectors are\nthe responses that it gets to the system calls coming from the\nenvironment and the changes that it can cause in the policies\nand the limitations of the environment or on other agents.\nThe kernel also precepts the environment via analysing the\nreceived  system  calls  and  interrupts.  The  kernel  has\nalgorithms  to  maintain  the  environment’s  stability,\nefficiency, security and fairness for other agents: hardware\nand processes. However, the vast difference among process\nagents  sometimes  jeopardizes  the  smooth  activity  of  the\nkernel  to  manage  the  environment,  requiring  taking\nchallenging (intelligent) decisions.\n\nIV.HOW OSES BENEFIT FROM ML\nAs mentioned  above,  several  aspects  of  OSes are  the\ncandidates  to  apply  AI,  particularly  ML,  to  enhance\nperformance. Transforming OS's deterministic nature into a\nmore  flexible  and  dynamic  form  can  maximize  the\nexploitation  of  hardware  and  energy,  for  example,  by\nreducing the idle times of the CPU. One of the features of\nML  is  the  ability  to  use  previous  experiences  for  future\npredictions. In this case, OSes with a massive amount of\nprocesses (threads and services) that continuously start and\nstop  running  can  benefit  from  the  processes'  previous\nexecution  behaviour  to  manage  them  more  optimally, for\nexample, via allocating resources to them more intelligently\naccording to their patterns of usage.\nOSes, along with other systems like compilers, are full of\nheuristics  that  are  hard-coded  into  their  programs.  Such\nheuristics are written concretely and cannot adapt to changes\nin usage [26]. This adaptation requires mechanisms to use the\nprevious knowledge and patterns of users' behaviours to find\nthe best configurations for timing operations, such as thread\nscheduling, memory paging, and the frequency of flushing\nbuffer cache. Additionally, determining the optimal sizes for\ndifferent containers such as buffer cache in storage caching is\nanother  operation  that  can  be  improved  utilizing  ML.  In\naddition to learning the optimal configurations for timing and\nsizing activities, an OS can learn how to allocate memory\nspaces to applications or hard disk space to files in different\nsituations,  which  is  traditionally  done  according  to\npredefined policies. Scheduling is one of the essential tasks\nof an OS, and it also has been one of the well-researched and\nimplemented areas in Artificial Intelligence literature. The\nOS schedules the accessibility of the CPU to process threads,\nand its goal is to distribute the CPU time fairly and optimally\namong all threads running in the system. Other tasks, such as\nscheduling different network and storage queues, are of the\nsame  type.  There  are  deterministic  algorithms  defined  in\nOSes to manage the scheduling [1] . Applying AI methods to\nlearn  the  CPU  usage  pattern  by  different  processes  and\napplications,  we  may  achieve  better  and  more  adaptive\nscheduling mechanisms.\nAlso, Since working with OSes is a continuous activity\nand  is  in  direct  contact  with  humans  to  fulfil  their\ntremendous requests, online approaches seem beneficial. For\nexample, in space allocation tasks, exploiting Reinforcement\nLearning  is  a  proper  method  of  ML.  This  method  can\nalleviate  the  extra  time  and  energy  consumption,  and\nperformance  penalties  that  the  system  suffers  in  case  of\nwrong decisions [1]. In the next sections, we provide some\nefforts reported in the literature of applying ML methods to\nenhance OS tasks.\nA.ML in I/O Scheduling and Latency Management\nTo make I/O devices (e.g., disks) more compatible with\nmodern  processors,  I/O schedulers  must  be  in their  most\noptimal form to eliminate the processing requests' overhead.\nTherefore, I/O scheduling is an important task of OSes in\nwhich AI can be used. In [27] , a self-learning I/O scheduling\nscheme is proposed to classify different types of requests or\nworkloads in run-time. To make such scheduling decisions,\nin the data collection phase, they gathered request features,\nincluding  classes  and  sizes  of  the  requests,  number  of\nprocesses, and inter-request distances between current and\nprevious requests. Also, workload features, like the number\nof reads/writes, average request size, and the average number\nof processes, are collected. They found that Support Vector\nMachines  (SVM)  classification  produces  the  lightest\noverhead. The whole ML system was implemented in Linux\nkernel 2.6.13 by modifying kernel I/O schedulers. However,the details of implementation are not provided. They found\nonline learning more adaptive than offline learning.\nUsing  their  own  in-kernel  ML  library,  KMLIB,\nresearchers in [28] employed a regression model to early-\nreject I/O requests with a high chance of not meeting the\ndeadline. Learning data collected by producing random read\nand write operations with four threads on a 1 GB data set by\nrunning an I/O tester called FIO [29] . They modified the\n\"mq-deadline\" I/O scheduler in Linux Kernel 4.19.51 and\nadopted their ML library. The thresholded regression model's\naccuracy  indicating  whether  an  I/O  request  misses  the\ndeadline  or  not  was  74.62%,  which  reduced  the  overall\nlatency by 8%.\nIn a continuation of previous works like [30]–[32]  which\nwere based on some heuristics, the LinnOS introduced in\n[17] was an effort to deal with the problem of high latency in\nflash storage and SSDs by possessing a model that can learn\nthe  behaviour  of  the  storage  device.  It  can  inform  client\napplications  about  the  anticipated  per-I/O  speed  that  the\nstorage  can  provide  for  their  current  requests.  LinnOS\ncontains  a  computationally  lightweight  neural  network  to\nreduce the overheads of the learning and inferring phases.\nEach incoming request is classified either as a slow-speed or\nfast-speed.\nIn their online approach, if the latency is fast-speed, it\nwill be passed to the storage, otherwise, with a slow-speed\nlatency, the application will be informed, and the request will\nbe denied. In this case, the application tries another storage\nnode. The NN's input features extracted from the current and\nrecent I/O requests are the number of I/Os in the queue when\nthe new request arrives, the latency of the four most recently\naccomplished requests, and the number of pending I/Os at\nthe time of arrival of that 4 I/Os. The output is the inference\nof the speed of the I/Os (slow or fast). Because of using\nSSDs, data gathering was not an issue. They collected data\nduring busy hours to achieve more general and richer training\ndata. The NN had three layers with linear neurons.\nIn LinnOS, the training phase is done in the user space\nusing  TensorFlow  [33]  then  the  weights  are  sent  to  NN\nrunning in kernel space after transforming them to integer\nvalues to compensate for the lack of support for floating-\npoints in the kernel space. The NN is implemented in the\nblock layer part of the Linux Kernel and needs 68 KB of\nkernel memory.\nB.ML in Scheduling\nScheduling algorithms are  the heart  of any OS. Their\npurpose is the fairness of time and memory allocation among\nall processes while the system operates optimally, and the\nprocesses finish their jobs as soon as possible. For example,\nthis fair  distribution of time among processes  is done by\nLinux's  Completely  Fair  Scheduler  (CFS)  in  the  Linux\nkernel.  However,  CFS  has  limitations  in  multicore\nenvironments  where  it  becomes  so  complicated  [34] .\nResearchers,  in [35], used  an ML method to balance  the\nLinux  kernel  load  on  a  multiprocessor  computer.  They\nmodified a kernel function and embedded their Multi-Layer\nPerceptron (MLP) model with three layers for load balancing\ndecisions. The forward phase of the MLP was implemented\nin C and contained floating-point computations. However,\nthe data  collection phase  was  implemented  in a  two-way\nfashion  between  kernel  and  userspace  using  some  tools\nexplained in their paper. The training set contains 500,000\nrecords  resulting  from  calling  load-balancer  in  different\nlevels  of  workloads.  The  same  amount  of  data  also  was\ncollected  in  different  CPU  load  averages.  Fifteen  input\nfeatures, including the combination of Idle time of the target\nCPU, NUMA node numbers, and running time of the process\nper core, were collected. However, in addition to the extra\n\nload of the MLP, they did not see any noticeable difference\nbetween the original Linux scheduler and the ML-based one.\nThe increasing usage of Deep Learning methods raised\nconcerns  about  the  Quality  of  Services  of  DL-based\napplications.  For  example,  the  CFS  can  not  effectively\nhandle  the  resulting  excessive  memory  traffic  caused  by\nrequests for DL-based services [36] . New strategies based on\nML  should  be  considered  for  this  situation,  and  it  is\ninteresting because, here, ML can be used to enhance ML\nservices.\nMany  deep  learning  methods  and  platforms  are  using\nserver-less  computation,  where  microservices  play  the\ncomputation role. With a considerable  inclination towards\nmicroservices  on  the  cloud  in  AI-based  applications,\nparticularly  Deep  Learning  methods,  resource  scheduling\namong many micro-services, therefore the Quality of Service\non  a  server  is  becoming  more  challenging  for  traditional\nschedulers designed for OSes [3] . Here the challenge is to\nfind the  most optimal  allocation  of resources  to different\nmicro-services so that the QoS maximizes. This problem is a\nsearching problem.\nThe  resources  to  be  allocated  among  services  include\nCPU  cores,  main  memories,  cache,  and  bandwidth.  The\nnumber of micro-services fluctuates, and also, these services\nare computationally heavy and sensitive to the reduction of\nresources  during  their  run-time.  The  traditional  forms  of\nresource  management  and  scheduling  of  OSes  are  not\nadapted to the heavy computation needs of deep learning\napplications.\nThe  OSML  scheduling  mechanism,  introduced  in\n[3] attempts to reach the best QoS for microservices. It uses\nML  to  find  the  optimal  amount  of  resources  (cores  and\ncache)  that  any  micro-service  requires  (OAA:  Optimal\nAllocation Area) with which the OS understands that extra\nresource is not needed by the service. It also prevents micro-\nservices from facing a sharp reduction in their QoS due to\nloss  of  resources  pre-empted  by  CPU  for  serving  other\nmicroservices  (RCliffs:  Resource  Cliffs).  The  models  are\ndesigned on TensorFlow, and the whole mechanism runs in\nthe  userspace.  They  used  four  three-layered  Multi-Layer\nPerceptrons (called Model-A, Model-B, and their shadows)\nand a modified version of Deep Q-Network [37], [38]  (called\nModel-C). The Reinforcement Learning approach in Model-\nC was used to correct the wrong decisions in what Model-A\nand B propose for resource management. OSML acts as an\nalerting mechanism to the kernel scheduler.\nIn [39], the authors reported an experiment in which they\nexploited  ML  methods  to  predict  CPU  burst  times  for  a\nprocess.  Several  CPU-bound  programs,  including  matrix\nmultiplication, sorting programs, recursive Fibonacci number\ngenerating  programs,  and  random  number  generator\nprograms, were analysed. They added two system calls to the\nLinux  kernel  for  providing  interoperability  between  the\nkernel  space  scheduler  and  the  Decision  Tree  inference\nmechanism in the userspace. One of the system calls was\nresponsible for taking the Special Time Slice (STS) classified\nby the C4.5 decision tree and set the time_slice variable of\nthe process descriptor in the modified scheduler at the kernel\nlevel. Another system call was also used to take the \"time-\nrewarded\"  process  back  to  the  normal  condition  to  be\nallocated  the  time  slice  according  to  the  system  default\ntask_timeslice() procedure.\nC.ML in Cache Management\nThere are several caches in a computer system, such as\nCPU  cache,  web  cache,  file  system  cache.  Each  cache\nreduces the waiting time of the faster device for receiving\ndata from the slower device. Another challenging job of anOS  is  to  handle  the  caches  (in  virtual  memory  or  file\nsystems) to know what data should be cached, how much\ntime the data should remain in the cache always to maintain\nonly data on immediate demand in the cache and remove\nothers  [1][40]. Caches always try to maintain more data to\nhelp processes run faster [10] . However, the size of caches\nand the cache hit rates are essential criteria in the system's\nefficiency. Therefore, keeping the cache's size as minimal as\npossible while reducing the amount of removing operations\nand maximizing the cache hit rates are trade-offs in caching\nprocedures. Learning the patterns of data usages by different\nprocesses or users can help the OSes to pick the data better to\nbe evicted from the cache.\nHow  a  caching  procedure  operates  at the  OS level is\ncurrently  hard-coded  in  the  kernel,  based  on  predefined\nassumptions  about  data  and  users'  behaviour.  But,  the\nworkloads  in  caching  are  not  constant  and  face  many\nfluctuations.  Since  caching  occurs  in  different  computer\nsystem levels, it is regarded as a \"caching problem\" and is\nworked on in other researches as a general problem to be\nsolved.\nFor  example,  in  [40] ,  the  authors  investigated  the\nimprovement  in  online  cache  management  by  applying\nreinforcement learning based on the reward and impact that a\ncaching decision has brought to the system's performance. As\nthe  authors  reported,  along  with  a  better  adaptation  to\nworkloads, the approach can provide a higher hit rate while\nminimizing  the  memory  space  needed.  The  method  was\nimplemented using TensorFlow. Although its results may be\nconvincing for networking, it is mainly designed for user\nspace applications, and the challenges for implementing in\nkernel space must be considered.\nOther works such as [41]  also looked at this as a general\nproblem and only provided ML-based approaches from this\nperspective.  On the other  hand, some researchers  tried to\nsolve  this  problem  at  the  hardware  level  [42],  [43] .  For\nexample, in [43],   researchers used an LSTM-RNN-based\ncache replacement mechanism. Using the insights from that,\nthey could create a lighter SVM-based architecture at the\nhardware  level  to solve cache  replacement.  Their  method\ngained better performance than previous heuristic methods\nsuch as LRU. Accordingly, applying ML algorithms in OS\nlevel cache management still needs more efforts from the\nresearchers.\nD.ML in Malware Detection\nThe malicious behaviour of processes can be determined\nby monitoring how the kernel operates as a final result of\ntheir  requests.  Like  any  other  process,  the  malware  also\nutilizes  specific  system  calls.  There  are  several  works  in\nwhich ML is used to identify malware by monitoring the\nsystem calls that they use. For example, in [44] , for Android\nsystems, they used system calls of and executable file as\nfeatures since they demonstrate how program communicate\nwith the kernel. They also weighted the system calls mostly\ninvoked  by  each  application  to  boost  the  discrimination\nmethod  during  generating  their  data  set.  Several\nclassification methods were utilized to investigate whether an\nexecutable file is a malware or not. Although the approach\nshowed  a  noticeable  performance  in  identifying  malware\napps,  the  vulnerability  against  malicious  learning  data\nremains a severe challenge in this area. We will explain this\nissue in the following sections.\nAlso,  increasing  the  security  of  devices  that  are  not\nconnected to the Internet or have no definitive way to remain\nup-to-date through getting security patches is a serious issue,\nparticularly in embedded OS. Using ML as a dynamic way of\ncontrolling  the  behaviour  of  processes  and  requests  by\ndistinguishing  the  system's  expected  behaviour  from  the\n\nanomaly may help them stay secure independently. In [45] , a\nhost-based,  run-time  anomaly  detection  mechanism  for\nLinux OSes is proposed to increase the firmware's security in\nembedded systems. Their mechanism consists of three parts\nin both kernel and user spaces. Deep Learning models, in\ntheir Exein ML Engine (MLE), were  trained  to learn  the\nnormal behaviour of the systems' processes .\nThey categorized processes into several classes and put a\ntag identifying that class into the corresponding executable\nfiles during the firmware build. Then, for each type, an ML\nmodel was designed. MLE is a userspace  procedure  with\nConvolutional  Neural  Networks  trained  based  on  the\nprocesses' behaviour at the kernel level.\n\"Anomaly score\" as the model's output is the value used\nto  indicate  how  unusual  a  process  is  acting.  Suppose  a\nprocess is labelled as malicious during execution time. In that\ncase, another module, called Exein Linux Security Module\n(LSM) that works in direct contact with the kernel (kernel\nlevel), will be informed. Every system calls of a monitored\nprocess during its run-time is hooked and sent to LSM via a\nkernel  module: The  Exein  Linux Kernel  Module  (LKM).\nThen  LSM  will  extract  each  call's  data  such  as  file\ndescriptors,  name,  paths,  inode  attributes,  memory\ninformation, and permission attributes. This information will\nbe sent to the MLE to label the behaviour of the process\nagain via LKM. LSM will be informed of the MLE decision\nand  performs  accordingly  to trust  or  distrust  the  process.\nTherefore, the LKM acts as an interface between the MLE in\nuserspace and the LKM in kernel space.\nSuch a malware detection procedure adds a significant\ncomputational  load  on  the  OS,  as  the  authors  admitted.\nHowever,  this  load  is  mostly  coming  from  the  process\nmonitoring part and the information sent and received by the\nthree modules, which was alleviated by shrinking the whole\nprocess  data  to  limited  \"snapshots\"  or  only  monitoring\nprocesses that communicate with the network as the main\nentrance  for  attacks.  The  learning  phase  of  ML  is  done\noffline (before building the embedded firmware); however,\nCNN's  inference  load  added  to  the  OS  also  should  be\nconsidered.\nV.COMPLEXITIES IN USING ML IN OS\nIn this section, we encounter some challenges in using\nML  models  in  the  OS  environment.  Possible  or  tested\nsolutions in the literature also are analysed. Here, we tried to\nadd different aspects to those stated in [1] .\nA.Implementation Constraints (in Kernel Space)\nThe particular challenge in manipulating the kernel space\nmechanisms is limited access to libraries and programming\nlanguages in this space. One solution is to make a two-way\npath between the kernel and userspace to benefit from the\nuser  space's  resources,  not  an  optimal  approach.  For\nexample, The main problem with the two system calls used\nin [39] was extra transitions from user mode to kernel mode,\nand this is even more considerable if the learning method is\nonline (adaptive) and needs to be trained regularly based on\nthe  new  events  in  the  system.  The  first  system  call  was\ncreated due to the lack of enough flexibility in the kernel\nspace. It is vital to consider the overload of such system calls\nin the system's overall performance evaluation, benefiting AI.\nAnother solution is to implement the required tools in the\nkernel space. \nKMLib, the ML library introduced in [28] , contains the\nmaths functions implemented from scratch to be used in the\nkernel  space,  mainly  to  deal  with  the  inaccessibility  of\nfloating-point maths functions  at this level.  Based on the\ncommon preference of tensor computations in major deep-learning  libraries  such  as  TensorFlow  or  PyTorch  [46]  ,\nKMLib also used such an approach but obviously in a more\nconstrained  and  less  resource  consuming  manner.  KMLib\ncan  operate  in  two  modes:  kernel  mode  and  kernel-user\nmemory-mapped shared mode. They performed the floating-\npoint  calculations  in  a  code  block  started  by\nkernel_fpu_begin and ended by kernel_fpu_end macros in\nthe Kernel mode. Nevertheless, the code in this block must\nbe  small  because  of  the  overhead  that  it  adds  to  the\ncomputations.\nIn contrast to the work in [28] , researchers in [35]  took a\nfixed-point approach in which a fixed number of bits is for\nstoring the integer part, and the remaining bits are to store the\ndecimal part of the number. Computation with such numbers\nis similar to that of integers.\nOther techniques, such as the quantization approaches,\nexploited in TinyML [47] , can be considered advantageous\nfor  this  situation.  The  post-training  quantization  and\nquantization-aware-training approaches have shown practical\nbenefit for reducing TensorFlow models' size and latency for\nthe Internet of Things edge devices. Quantization consists of\ntransforming  32-bit  floating-point  to 8-bit  integers,  which\nreduces  model  size  and  latency  with  a  low  impact  on\naccuracy.  However,  such  ideas  are  more  practical  for\ninference, not for training. Training a (Deep) learning model\nis challenging in low-resource environments lacking floating-\npoint computational abilities [48], [49].\nB.Data Shortage and Data Gathering\nAnother issue in ML-based modifications in OS's low-\nlevel activities is related to data. The question is how the data\nshould be gathered to be general enough for multi-purpose\nOSes' jobs. In [39], since the goal was to predict the best STS\nfor  each  program,  84  data  instances  were  generated  by\nsetting different STSs for five programs (with CPU-bound\nprocesses) and finding the best STS the one that minimizes\nthe  TaT  (Turn-around-Time)  of  the  processes  of  each\nprogram. Each data's features are several static and dynamic\nattributes of a process, mostly the sizes of different tables\nsuch as the hash table size and the program's size. A Genetic\nand an Exhaustive search are used to find the best set of\nfeatures and the corresponding STS with which the process\nhas the most efficient execution behaviour. Also, in online\napproaches collecting data is done by watching the system's\nstate, then train a model regularly, which adds up more load\non the system as we saw in [45] .\nC.Many Parameters\nEach OS contains hundreds to thousands of processes\nrunning and many resources managed by the kernel. Every\nmodification in one part of the kernel must take into account\nother parts. For example, as in [39]  , the only evaluation\ncriteria was the reduction of the TaT of one program being\ntested. However, the consequences of such a change in other\nparts of the OS are not evaluated.\nD.Computational Needs of ML Inference and Learning\nEvery  AI  algorithm  also  takes  several  computing\nresources. In the case of learning algorithms, if we assume\nthe training phase as a separate procedure from the inference\nphase  (which is not in online learning  methods), still the\ninference  phase  takes  some  time  to  produce  the  results.\nGenerally, to achieve a more reliable and more data-oriented\napplication,  we  need  a  considerable  amount  of  data  and,\nconsequently, more complicated learning models like those\nintroduced in the Deep Learning literature, which also takes\nmore time to infer. However, other DL approaches can be\ntried, such as few-shot learning, [28]   in which the training\nand inference time and the amount of data can be reduced.\n\nE.Security Threats\nPenetrating  the  isolated  and  tightly  controlled\nenvironment  of  the  kernel  by  outside  data  motivates\nmalicious users to feed data that can mislead the learning\nmodels  and,  consequently,  jeopardize  the  OS's  logical\noperation. There are recommended solutions, such as using\nseparate  ML  models  for  different  applications.  However,\nconcerns remain for side-channel attacks [1] . Side-channel\nattacks are intimately related to patterns in the data gathered\nfrom a set of physically observable phenomena caused by the\ncomputation tasks in present microelectronic devices [50]. It\ncan roughly be considered that these attacks are about the\ncomputer  system’s  physical  implementation  rather  than\nsoftware  implementations  and  vulnerabilities.  In  [39]  as\nauthors admitted, a simple threat was that their method did\nnot take the security holes that are embedded into the kernel\nby  their  method  since  there  were  not  any  precautions  to\nprevent a malicious user from creating a process that requests\nthe maximum amount of STS (special_time_slice) for itself\nthat can have excessive access to CPU.\nWorks such as  [28] via implementing the whole ML into\nthe kernel can eliminate such threat from the outside world\nwith the penalty of more limited capabilities for making more\npowerful models. Additionally, every combination of basic\nkernel tasks with AI methods must consider the overloads\nthat extra security precautions can cause .\nF.Interpretability (Explainability)\nThe ML methods' black-box nature makes their results\nunexplainable for the end-user and the experts. Explainable\nAI  and  recently,  with  larger  and  more  complex  learning\nmodels, explainable DL, have become an important topic in\nthe literature [51], [52] . Therefore, in OS designed to have\nhigh predictability (in its traditional meaning and despite the\npotential  vulnerability)  and  every  response  is  based  on  a\ndefinitive algorithm, probing ML methods with little or no\ninterpretability requires a change of perspective.\nVI.CONCLUSION AND FUTURE\nIn  this  paper,  we  reviewed  some  recent  works  on\nintegrating ML methods into operating systems. According\nto  these  works,  we  also  mentioned  some  challenges  and\npossible solutions in reaching the idea of \"Intelligent OS.\" As\nwe listed, research is done to utilize ML to enhance the main\ntasks  of  an  OS,  such  as  process  scheduling,  memory\nallocation,  I/O  and  cache  management,  and  malware\ndetection. These methods were implemented in user or kernel\nspaces or both. In user space, designers have access to tools\nsuch  as  TensorFlow  to  train  powerful  models,  while  the\noverhead was tremendous.\nOn the other hand, in-kernel methods must cope with the\ncomputational  limitations.  Security  was  another  issue  in\nmethods that use both spaces. Security was also a problem in\nusing user' (processes') data to create a data-driven operating\nsystem because of malicious users' threat.\nNow,  the  question  is:  is  it  better  to  transform  the\noperating systems to become more acknowledging toward AI\nmethods,  for  example,  by  providing  richer  programming\nlibraries in the kernel? Or try to solve the above challenges.\nIs it worth changing OSes in this way? Or we should stick to\ntheir  deterministic  nature  and  let  the  under-world  of  the\nkernel  space  remain  as explainable  as  possible?  Learning\nmethods produce results that can not be explained, and there\nare  efforts  to  make  them  as  interpretable  as  possible  to\nbecome more reliable. Although predictability is desirable in\noperating system design, explainability and interpretability of\nOS actions are  also important. Making operating  systems\nmore thoughtful also will end in more computation for tasks\nthat are already done with trial and error but faster. Dealingwith  these  trade-offs  requires  lots  of  research  and\nimplementations.\nREFERENCES\n[1]Y. Zhang and Y. Huang, “‘Learned’ operating systems,” Oper. Syst.Rev.,  vol.  53,  no.  1,  pp.  40–45,  2019,  doi:10.1145/3352020.3352027.J.  Clerk  Maxwell,  A  Treatise  onElectricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892,pp.68–73.\n[2]Eiselt H.A., Sandblom CL. (2000) Heuristic Algorithms. In: IntegerProgramming  and  Network  Models.  Springer,  Berlin,  Heidelberg.https://doi.org/10.1007/978-3-662-04197-0_11K.  Elissa,  “Title  ofpaper if known,” unpublished.\n[3]S.  T.  Report  and  L.  Liu,  “QoS-Aware  Resource  Scheduling  forMicroservices :  A  Multi-Model  Collaborative  Learning-basedApproach,” arXiv, vol. 2020, no. 1, 2020.\n[4]P.  Ferragina  and  G.  Vinciguerra,  “Learned  Data  Structures,”  inStudies in Computational Intelligence, vol. 896, 2020, pp. 5–41.\n[5]T. Kraska et al., “SageDB: A learned database system,” 2019.\n[6]T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The casefor learned index structures,” in Proceedings of the ACM SIGMODInternational Conference  on Management of  Data, May 2018, pp.489–504, doi: 10.1145/3183713.3196909.\n[7]Y.  Wang,  C.  Tang, Z. Wang, and  H. Chen, “SIndex:  A  scalablelearned index for string keys,” in APSys 2020 - Proceedings of the2020 ACM SIGOPS Asia-Pacific Workshop on Systems, Aug. 2020,pp. 17–24, doi: 10.1145/3409963.3410496.\n[8]J. Ding et al., “ALEX: An Updatable Adaptive Learned Index,” inProceedings  of  the  ACM  SIGMOD  International  Conference  onManagement  of  Data,  Jun.  2020,  pp.  969–984,  doi:10.1145/3318464.3389711.\n[9]M.  Hashemi  et  al.,  “Learning  memory  access  patterns,”  in  35thInternational Conference on Machine Learning, ICML 2018, 2018,vol. 5, pp. 3062–3076.\n[10]A.  Silberschatz,  G.  Gagne,  and  P.  B.  Galvin,  Operating  SystemConcepts, 8th ed. Wiley Publishing, 2011\n[11]Schmidhuber, J.  (2015).  \"Deep Learning  in Neural  Networks:  AnOverview\".  Neural  Networks.  61:  85–117.  arXiv:1404.7828.  doi:10.1016/j.neunet.2014.09.003. PMID 25462637. S2CID 11715509.\n[12]R. S. D. E. Porter and D. O. M. Bishop, “The case for less predictableoperating  system  behavior,”  in  15th  Workshop  on  Hot  Topics  inOperating Systems, HotOS 2015, 2015, vol. 40, no. 4, pp. 12–17.\n[13]A. Alanwar, F. M. Anwar, J. P. Hespanha, and M. B. Srivastava,“Realizing  uncertainty-aware  timing  stack  in  embedded  operatingsystem,” in CEUR Workshop Proceedings, 2016, vol. 1697.\n[14]S.  Venkataraman, Z. Yang, M. Franklin, B. Recht, and I. Stoica,“Ernest:  Efficient performance  prediction  for  large-scale  advancedanalytics,”  in  Proceedings  of  the  13th  USENIX  Symposium  onNetworked Systems Design and Implementation, NSDI 2016, 2016,pp. 363–378.\n[15]Russel, S., & Norvig, P. (2009). Artificial Intelligence: A ModernApproach (Third). Pearson.\n[16]M. Hao, L. Toksoz, N. Li, E. E. Halim, H. Hoffmann, and H. S.Gunawi, “LinnOS: Predictability on Unpredictable Flash Storage witha Light Neural Network.”\n[17]E. Cortez, M. Russinovich, A. Bonde, M. Fontoura, A. Muzio, and R.Bianchini,  “Resource  Central:  Understanding  and  PredictingWorkloads  for  Improved  Resource  Management  in  Large  CloudPlatforms?,”  in  SOSP  2017  -  Proceedings  of  the  26th  ACMSymposium on Operating Systems Principles, 2017, pp. 153–167, doi:10.1145/3132747.3132772.\n[18]K. Ousterhout, S. Ratnasamy, C. Canel, and S. Shenker, “Monotasks:Architecting for Performance Clarity in Data Analytics Frameworks,”in  SOSP  2017  -  Proceedings  of  the  26th  ACM  Symposium  onOperating  Systems  Principles,  2017,  pp.  184–200,  doi:10.1145/3132747.3132766.\n[19]A.  Nógueira  and  M.  Calha,  “Predictability  and  efficiency  incontemporary hard RTOS for multiprocessor systems,” in Proceedings- 1st International Workshop on Cyber-Physical Systems, Networks,and  Applications,  CPSNA  2011,  Workshop  Held  During  RTCSA2011, 2011, vol. 2, pp. 3–8, doi: 10.1109/RTCSA.2011.56.\n[20]C.  Lameter,  “NUMA  (Non-Uniform  Memory  Access):  AnOverview,”  Queue,  vol.  11,  no.  7,  pp.  40–51,  2013,  doi:10.1145/2508834.2513149.\n[21]“What  is  NUMA?  —  The  Linux  Kernel  documentation.”https://www.kernel.org/doc/html/latest/vm/numa.html .\n\n[22]F. Arapidis, V. Karakostas, N. Papadopoulou, K. Nikas, G. Goumas,and N. Koziris, “Performance  prediction  of  NUMA  placement: Amachine-learning  approach,”  in  Proceedings  of  the  InternationalConference  on  Cloud  Computing  Technology  and  Science,CloudCom,  Dec.  2018,  vol.  2018-December,  pp.  296–301,  doi:10.1109/CloudCom2018.2018.00064.\n[23]“Page  Frame  Reclamation.”https://www.kernel.org/doc/gorman/html/understand/understand013.html.\n[24]“Better  caching  using  reinforcement  learning  -  UBC  Wiki.”https://wiki.ubc.ca/Better_caching_using_reinforcement_learning .\n[25]J. Dean, “Machine Learning for Systems and Systems for MachineLearning,” Learning, pp. 416–423, 2017.\n[26]L.  D.  Molesky,  K.  Ramamritham,  C.  Shen,  J.  A.  Stankovic,  G.Zlokapa,  and  I.  Science,  “Implementing  a  Predictable  Real-TimeMultiprocessor Kernel - The Spring Kernel,” Oper. Syst. Rev., no.May, pp. 1–7, 1990.\n[27]Z. Yu and B. Bhargava, “Self-learning disk scheduling,” IEEE Trans.Knowl.  Data  Eng.,  vol.  21,  no.  1,  pp.  50–65,  2009,  doi:10.1109/TKDE.2008.116.\n[28]S. Grünewälder, “KMLIB: Towards Machine Learning For OperatingSystems,” arXiv, no. 113, pp. 13–21, 2020.\n[29]J.  Axboe,  “Fio  -  Flexible  I/O  tester,”  URL  http//freecode.com/projects/fio, 2014.\n[30]T. J. Teorey and T. B. Pinkerton, “A Comparative Analysis of DiskScheduling Policies,” Commun. ACM, vol. 15, no. 3, pp. 177–184,1972, doi: 10.1145/361268.361278.\n[31]I. Ahmad, “Easy and efficient disk I/O workload characterization inVMware ESX server,” in Proceedings of the 2007 IEEE InternationalSymposium on Workload Characterization, IISWC, 2007, pp. 149–158, doi: 10.1109/IISWC.2007.4362191.\n[32]E. Varki, A. Merchant, J. Xu, and X. Qiu, “Issues and challenges inthe performance analysis of real disk arrays,” IEEE Trans. ParallelDistrib.  Syst.,  vol.  15,  no.  6,  pp.  559–574,  2004,  doi:10.1109/TPDS.2004.9.\n[33]M.  Abadi  et  al.,  “TensorFlow:  A  system  for  large-scale  machinelearning,”  in  Proceedings  of  the  12th  USENIX  Symposium  onOperating Systems Design and Implementation, OSDI 2016, 2016, pp.265–283.\n[34]J.  P.  Lozi,  B.  Lepers,  J.  Funston,  F.  Gaud,  V.  Quéma,  and  A.Fedorova, “The Linux scheduler: A decade of wasted cores,” 2016,doi: 10.1145/2901318.2901326.\n[35]J. Chen, S. S. Banerjee, Z. T. Kalbarczyk, and R. K. Iyer, “Machinelearning for load balancing in the Linux kernel,” in APSys 2020 -Proceedings of the 2020 ACM SIGOPS Asia-Pacific Workshop onSystems, Aug. 2020, pp. 67–74, doi: 10.1145/3409963.3410492.\n[36]J. Kim, P. Shin, M. Kim, and S. Hong, “Memory-Aware Fair-ShareScheduling for Improved Performance Isolation in the Linux Kernel,”IEEE  Access,  vol.  8,  pp.  98874–98886,  2020,  doi:10.1109/ACCESS.2020.2996596.[37]V. Mnih et al., “Playing Atari with Deep Reinforcement Learning,”2013,  Accessed:  Dec.  09,  2020.  [Online].  Available:http://arxiv.org/abs/1312.5602 .\n[38]V.  Mnih  et  al., “Human-level  control through deep reinforcementlearning,”  Nature,  vol.  518,  no.  7540,  pp.  529–533,  2015,  doi:10.1038/nature14236.\n[39]A. Negi, S. Member, K. K. P, and P. Kishore Kumar, “ApplyingMachine Learning Techniques to improve Linux Process Scheduling,”IEEE Reg. 10 Annu. Int. Conf. Proceedings/TENCON, vol. 2007, pp.1–6, 2005, doi: 10.1109/TENCON.2005.300837.\n[40]S.  Alabed,  “RLCache:  Automated  cache  management  usingreinforcement learning,” arXiv. 2019.\n[41]G. Vietri et al., “Driving cache replacement with ML-based LeCaR,”2018.\n[42]S. Charles, A. Ahmed, U. Y. Ogras, and P. Mishra, “Efficient cachereconfiguration  using  machine  learning  in  NoC-based  many-coreCMPs,”  ACM Trans. Des. Autom. Electron. Syst., vol. 24, no. 6,2019, doi: 10.1145/3350422.\n[43]Z. Shi, X. Huang, A. Jain, and C. Lin, “Applying deep learning to thecache  replacement  problem,”  in  Proceedings  of  the  AnnualInternational Symposium  on Microarchitecture, MICRO, 2019, pp.413–425, doi: 10.1145/3352460.3358319.\n[44]P. Vinod, A. Zemmari, and M. Conti, “A machine learning basedapproach to detect malicious android apps using discriminant systemcalls,” Futur. Gener. Comput. Syst., vol. 94, pp. 333–350, 2019, doi:10.1016/j.future.2018.11.021.\n[45]A. Vivona, A. Carminati, G. Cuozzo, G. Spagnuolo, and G. AlbertoFalcione, “E  X E I N  C O  R E A  host-based, run-time anomalydetection mechanism for Linux-based embedded systems.”\n[46]A. Paszke et al., “PyTorch: An Imperative Style, High-PerformanceDeep Learning Library,” 2019.\n[47]D.  Warden,  Pete;  Situnayake,  “TinyML,”  O’REILLY,  2019.https://learning.oreilly.com/library/view/tinyml/9781492052036/ch15.html#idm45271161180216.\n[48]N. Wang, J. Choi, D. Brand, C.-Y. Chen, and K. Gopalakrishnan,“Training Deep Neural Networks with 8-bit Floating Point Numbers,”Adv. Neural Inf. Process. Syst., vol. 2018-December, pp. 7675–7684,Dec. 2018.\n[49]S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan, “DeepLearning with Limited Numerical Precision,” 32nd Int. Conf. Mach.Learn. ICML 2015, vol. 3, pp. 1737–1746, Feb. 2015, Accessed: Feb.17, 2021. [Online]. Available: http://arxiv.org/abs/1502.02551 .\n[50]Standaert  FX.  (2010)  “Introduction  to  Side-Channel  Attacks”.  In:Verbauwhede  I.  (eds)  Secure  Integrated  Circuits  and  Systems.Integrated  Circuits  and  Systems.  Springer,  Boston,  MA.https://doi.org/10.1007/978-0-387-71829-3_2\n[51]A. Adadi and M. Berrada, “Peeking Inside the Black-Box: A Surveyon Explainable Artificial Intelligence (XAI),” IEEE Access, vol. 6,pp. 52138–52160, 2018, doi: 10.1109/ACCESS.2018.2870052.\n[52]N. Xie, G. Ras, M. van Gerven, and D. Doran, “Explainable deeplearning: A field guide for the uninitiated,” arXiv. 2020.",
  "textLength": 50980
}