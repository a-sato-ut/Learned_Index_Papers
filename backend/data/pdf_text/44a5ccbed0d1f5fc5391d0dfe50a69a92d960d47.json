{
  "paperId": "44a5ccbed0d1f5fc5391d0dfe50a69a92d960d47",
  "title": "The Potential of Learned Index Structures for Index Compression",
  "pdfPath": "44a5ccbed0d1f5fc5391d0dfe50a69a92d960d47.pdf",
  "text": "The Potential of Learned Index Structures\nfor Index Compression\nHarrie Oosterhuis\nUniversity of Amsterdam\noosterhuis@uva.nlJ. Shane Culpepper\nRMIT University\nshane.culpepper@rmit.edu.auMaarten de Rijke\nUniversity of Amsterdam\nderijke@uva.nl\nABSTRACT\nInverted indexes are vital in providing fast key-word-based search.\nFor every term in the document collection, a list of identifiers of\ndocuments in which the term appears is stored, along with auxil-\niary information such as term frequency, and position offsets. While\nvery effective, inverted indexes have large memory requirements\nfor web-sized collections. Recently, the concept of learned index\nstructures was introduced, where machine learned models replace\ncommon index structures such as B-tree-indexes, hash-indexes, and\nbloom-filters. These learned index structures require less memory,\nand can be computationally much faster than their traditional coun-\nterparts. In this paper, we consider whether such models may be\napplied to conjunctive Boolean querying. First, we investigate how\na learned model can replace document postings of an inverted index,\nand then evaluate the compromises such an approach might have.\nSecond, we evaluate the potential gains that can be achieved in terms\nof memory requirements. Our work shows that learned models have\ngreat potential in inverted indexing, and this direction seems to be\na promising area for future research.\n1 INTRODUCTION\nSearch engines make large collections of documents accessible to\nusers, who generally search for documents by posing key-word based\nqueries. For the best user experience, the user should be presented rel-\nevant results as quickly as possible. Inverted indexes allow systems\nto match documents with key-words in an efficient manner [ 15,22].\nDue to their scalability, inverted indexes form the basis of most search\nengines that cover large document collections. They store inverted\nlists of the terms contained in each document in the collection; for\na given term, an inverted list stores a list with all the documents\nin which it occurs. An important search operation performed on\nthese lists is conjunctive Boolean intersection, as it is routinely used\nin search engines for early stage retrieval [ 1,4,8] and for vertical\nsearch tasks such as product or job search [ 19]. Boolean queries are\ncomputed by intersecting the inverted lists of the query terms [ 7],\nand the result set typically includes documents that contain allof\nthe query terms, including the stopwords.\nDespite consistent advances in compressed index representations\nover the years [ 11,16,20], the cost of storing all relevant data (such\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nADCS ’18, December 11–12, 2018, Dunedin, New Zealand\n©2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-6549-9/18/12. . . $15.00\nhttps://doi.org/10.1145/3291992.3291993as stopword data) to effectively search sizable collections can be a\nbottleneck in scaling to increasingly larger collections. One inter-\nesting alternative is to use a bitvector to store the document vector\nof high frequency terms [ 9,14]. However, there are limits to what\ncompressing exact representations can achieve.\nRecently the idea of learned index structures has been proposed\nby Kraska et al .[10]. Here, machine learned models are optimized\nto replace common index structures such as B-tree-indexes, hash-\nindexes, and bloom-filters. The benefit of learned index structures\nis that they require less memory and can be evaluated substantially\nfaster than their traditional counterparts.\nIn this study we examine whether learned index structures can be\nused to reduce space in a Boolean search scenario, and investigate the\neffect this would have on exactness guarantees an index can provide.\nUsing existing document collections, we estimate the space savings\nthat such an approach could achieve. Our results show that a learned\nindex structure approach has the potential to significantly reduce\nstorage requirements, and still provide performance guarantees. The\nresearch questions we address are:\nRQ1 How might learned indexes be used to support search based\non Boolean intersection?\nRQ2 Would learned indexes provide any space benefits over current\ncompressed indexing schemes?\n2 RELATED WORK\nBoolean intersection has been a fundamental component of infor-\nmation retrieval systems for more than fifty years. In fact, early\nsearch systems were entirely reliant on Boolean retrieval models [ 5].\nIn recent years, ranked retrieval models have become more impor-\ntant, but Boolean operations are still a fundamental component in\na variety of search tasks [8, 19].\nOne important application of Boolean intersection is as either a\nfeature in multi-stage retrieval [ 12], or as a filtering stage in a multi-\nstage pipeline [ 8,17]. In all of these, the key idea is to apply expensive\nfeature extraction and machine learning models on a subset of the\nmost promising candidate documents to ensure early-precision is\nmaximized in the final result set [3, 21].\nMachine learning has been applied in early-stage retrieval to pre-\ndict the number of documents to pass through to the next stage [ 6]\nand even to predict which top- kprocessing algorithm should be used\nper query [ 13]. But in current cascaded and multi-stage retrieval mod-\nels the use of machine learning algorithms is often deferred to later\nstages of the retrieval process as traditionally such algorithms were\nnot optimized for efficiency. The recent introduction of learned index\nstructures by Kraska et al . [10] is changing that perception. They\nhave shown that common index structures such as B-tree-indexes,\nhash-indexes, and Bloom-filters can be replaced by learned models,arXiv:1811.06678v2  [cs.IR]  29 Jan 2019\n\nAlgorithm 1 The Exhaustive Iterative Approach.\n1:q1,...,qn←receive_query\n2:r←[]\n3:ford∈Ddo\n4:if∀qi∈[q1,...,qn],f(qi,d)=1then\n5: append(r,d)\n6:return r\nAlgorithm 2 The Two-Tiered Approach.\n1:q1,...,qn←receive_query\n2:l1,...,ln←truncated_lists_for_terms (q1,...,qn)\n3:L←Ðn\ni=1li\n4:r←[]\n5:ford∈Ldo\n6:if∀qi∈[q1,...,qn],f(qi,d)=1then\n7: append(r,d)\n8:return r\nAlgorithm 3 The Block Based Approach.\n1:q1,...,qn←receive_query\n2:b1,...,bn←block_lists_for_terms (q1,...,qn)\n3:B←Ñn\ni=1bi\n4:r←[]\n5:forb∈Bdo\n6:ford∈document_range_of_block (b)do\n7: if∀qi∈[q1,...,qn],f(qi,d)=1then\n8: append(r,d)\n9:return r\nwhile bringing both gains in memory and computational costs. More-\nover, by applying recursive models a learned index structure can fall-\nback on traditional structures for sub-cases where a learned model\nperforms poorly. Consequently, learned models can provide the\nsame correctness guarantees as their traditional counterparts. Given\nthe potential advantages of learned models, we explore this line of\nresearch in a constrained early-stage retrieval scenario – specifi-\ncally, can a learned indexing representation be used for conjunctive\nBoolean retrieval? If so, what are the performance implications?\n3 APPLICABILITY FOR INVERTED INDEXES\nIn this section we answer RQ1 : in what ways a learned index model\ncan support Boolean intersection based search.\nWe will consider models that act as learned Bloom-filters because\nthey have commonly been applied to conjuctive Boolean problems.\nMoreover, Kraska et al .[10] have shown that they can be applied to\nsizeable datasets and outperform traditional Bloom-filters in both\nspeed and memory requirements. However, learned index structures\nhave only been optimized for tasks involving a single set [ 10]. In\ncontrast, each document in an inverted index could be seen as an\nindividual set, thus making the problem substantially more complex.\nFor this study we will assume that a function f(t,d)∈{0,1}can\nbe learned perfectly so that for a term tand document d:\nf(t,d)=(\n1t∈d,\n0t<d.(1)In theory any deep neural network that is expressive enough could\nbe optimized for an entire document collection without any errors.\nIn practice, such a model has to be relatively sizeable and requires\na very long period of optimization. Therefore, one may choose to\ncompromise some correctness for practical reasons. In this study,\nwe will not discuss the specifics of such a model or its optimization\nand instead focus on how it could be applied.\n3.1 Exhaustive Iterative Approach\nA straightforward approach to conjunctive Boolean functions using\nthe model fwould be to iterate over the entire document collection.\nAlgorithm 1 displays what this approach could look like. It is clear\nthat, per query, there is a huge computational cost proportional to the\nnumber of documents in the collection. However, this approach guar-\nantees the correct results for conjunctive Boolean queries. Moreover,\nthe only storage it requires is for the model f, thus it can provide the\nbiggest gains in memory by completely replacing an inverted index.\nIn practice this approach will most likely be avoided because of its\ncomputational costs, yet it provides an interesting example of how\nthe storage requirements could be completely minimized.\n3.2 Two-Tiered Approach\nThe previous approach iterated over all documents in the collection,\nwhich has high computational costs. An existing method of speeding\nup retrieval is to use two-tier retrieval [ 18]. Here an index is divided in\ntwo partitions, one of which is of smaller size on which queries can be\npre-processed quickly. We also propose a two-tiered approach where\nan inverted index is split into a smaller partition with truncated lists,\nand a larger partition with the remainder of the lists. We will assume\nthat the size of the second partition is not important, but that the\ngoal is to minimize the size of the first partition. The first partition\nconsists of the inverted lists of each term but truncated to length k,\nthe remainder of each list appears in the second partition. We will\nnot make any assumptions about which parts of the lists are included\nin the truncations. Then Algorithm 2 displays how one may use the\nlearned model fto search through the first partition. This approach\nonly has to iterate over the intersection of the truncated lists, thus it is\ncomputationallymoreefficientthanthepreviousapproach.However,\nto retrieve all results the truncated lists will not always suffice. If\nallterms in a query have a document frequency greater than kthen\nresults may be missing after passing over the first partition. At this\npoint, the algorithm could fallback by also considering the second\npartition. Conversely, correctness is guaranteed if at least one query-\nterm appears in kor less documents. By applying the learned model\nfthere is no need to use the second partition here. Thus for queries\nwith at least one infrequent term the first partition and learned\nmodel are guaranteed to provide correct results. This approach may\nbe particularly advantageous when the smaller size allows the first\npartition to fit in memory components with faster access.\n3.3 Block Based Approach\nLastly, we introduce an approach inspired by existing signature files\nand partitioned approaches [ 8] in Algorithm 3. A document collec-\ntion may be partitioned into multiple blocks , each containing a subset\nof documents. For every term, a list indicating the blocks in which\ntheir matching documents appear is stored. Then the intersection\n\nFigure 1: Top: the distribution of document frequencies. Bottom: the minimum number of terms that appear at different\nfractions of the compressed inverted index. From left to right: results for the Robust, GOV2 and ClueWeb collections.\n0 100000 200000 300000 400000 500000\nDocument-frequency101103105Number of terms\n0.0 0.5 1.0 1.5 2.0\nDocument-frequency ×107101103105107Number of terms\n0 1 2 3 4\nDocument-frequency ×107102105108Number of terms\n0.0 0.2 0.4 0.6 0.8 1.0\nFraction of storage102104Number of terms\n0.0 0.2 0.4 0.6 0.8 1.0\nFraction of storage102104106Number of terms\n0.0 0.2 0.4 0.6 0.8 1.0\nFraction of storage102104106Number of terms\nof the lists for every query-term provides restricted ranges in which\nresults for a conjunctive Boolean query appear. Finally, these ranges\ncan be traversed with the learned model fto retrieve all documents\nfor the conjunctive Boolean query. The computational costs of this\napproach are limited by the size of the partitions. Reductions in\nstorage can be achieved since only a list of partitions has to be stored.\nWe note that for very infrequent terms, traditional inverted lists may\nstill be stored resulting in hybrid presentations [ 14]. In addition to\nthe storage gains, this approach still guarantees correct results for\nconjunctive Boolean queries.\nFinally, we conclude our answer to RQ1 : there are several meth-\nods by which a learned index structure could be applied to Boolean\nintersection. These approaches all make different tradeoffs between\ncomputational costs during retrieval and gains in the amount of\nstorage space required. It may depend on the requirements of an\napplication which approach is the most suitable.\n4 ESTIMATING POTENTIAL GAINS\nIn the previous section we proposed several approaches to support\nconjunctive Boolean search with learned index structures. In this sec-\ntion we will answer RQ2 by estimating the gains these approaches\ncould make in terms of storage requirements.\nFor this analysis we will consider the two-tiered approach detailed\nin Section 3.2. This approach was chosen because it appears to pro-\nduce the least storage gains, thus serving as a conservative bound,\nand, furthermore, for this approach we can accurately estimate the\ntradeoffs it makes. Three commonly used TREC document collec-\ntions are considered for this study: Robust 2005 (Newswire), GOV2,\nand ClueWeb09B.1Figure 1 displays the distribution of document-\nfrequencies in each collection. Additionally, to see the varying stor-\nage terms require, it also shows the minimum number of terms that\ncan be stored in different fractions of the compressed inverted index.\nFor this study we used OptPFOR compression [ 11]. From this figure\nit is clear that very few terms have a high document-frequency but\nthat they can require a considerable percentage of total storage cost\nin the inverted index. For instance, in every collection we see that\nless than one percent of the terms take up forty percent of storage.\nTo estimate the gains of the two-tiered approach we will use trun-\ncated lists of a fixed size kin the first partition. Thus only terms with a\n1https://trec.nist.govhigher document-frequency than kwill have truncated lists. In addi-\ntion this affects the optimization of fas it only has to consider terms\nfor which not all documents are stored. The potential gains in storage\nof the first partition are then estimated as follows: First, we compute\nthe amount of storage gained by removing the inverted lists of re-\nplacedtermsfromtheinvertedindex.Second,weestimatethestorage\nspace required by a truncated list of length k; we take the average size\nof compressed lists of the same length in the complete compressed in-\nverted index [ 11]. Then we estimate the size of the learned model fas\nlinearly proportional to the vocabulary and collection size: |T|·|D|·s,\nwhere sis an unknown positive value. Lastly, we expect that for every\nterm a bit has to be stored to indicate whether it has been replaced\nor not. By summing all these values we get the following formula\nfor the expected gain in storage; with Ras the set of terms to replace,\nthe complete set of terms T, and complete set of documents D:\ngain(R,s)=\n\"Õ\nt∈Rsize.full.list(t)−size.trunc.list(k)#\n−(|R|+|D|)·s−|T|.(2)\nTo account for the unknown value of swe compute a lower and upper\nbound by varying its value. For the upper bound, we estimate no\ncost from the model: s=0, this is the most gain this approach could\npotentially have. The lower bound is estimated with s=512bits, this\nis equivalent to the cost of storing a compressed 128unit embedding\nfor every document and for every term as well. We expect this to be\nthe worst-case scenario in terms of model size.\nFigure 2 displays the estimated bounds for varying truncated list\nsizes; in addition, it also shows the number of terms that have to be\nreplaced. For instance, on the Robust collection, a gain of at least 40%\ncan be achieved by using a truncated list of 4,000 and replacing less\nthan 4,000 term lists. Interestingly, the number of terms to replace\ngrows exponentially as the truncated list size decreases, while the po-\ntential gain increases at a much smaller rate. This further shows that\nthe highest gains can be made by replacing the most frequent terms.\nMoreover, replacing extremely rare terms could even require more\nstorage depending on the model costs. Regardless, we see that even\nwith high model costs substantial gains are possible by choosing an\nappropriate truncated list size.\nLastly, on a set of 40,000 queries from the TREC Million Query\nTrack [ 2], we verified the number of queries with results that can\nbe guaranteed correct on the first partition. With a learned model\n\nFigure 2: Top: The estimated upper and lower bounds in terms of storage space required. Bottom: The number of terms that\nneed to be replaced. From left to right: results for the Robust, GOV2 and ClueWeb09B collections.\n0 2000 4000 6000 8000 10000\nTruncated list size0.00.20.40.6Fraction of storage\n0 2000 4000 6000 8000 10000\nTruncated list size0.00.20.40.6Fraction of storage\n0 2000 4000 6000 8000 10000\nTruncated list size0.00.20.40.6Fraction of storage\n0 2000 4000 6000 8000 10000\nTruncated list size0250050007500 Number of terms\n0 2000 4000 6000 8000 10000\nTruncated list size050000100000150000200000 Number of terms\n0 2000 4000 6000 8000 10000\nTruncated list size0100000200000300000 Number of terms\nFigure 3: Percentage of queries with guaranteed correct results in the first-tier by varying truncated list sizes. From left to right:\nRobust, GOV2 and ClueWeb09B.\n0 5000 10000 15000 20000 25000 30000\nTruncated list size0.000.250.500.751.00Fraction of queries\n0 50000 100000 150000 200000 250000 300000\nTruncated list size0.000.250.500.751.00Fraction of queries\n0 50000 100000 150000 200000 250000 300000\nTruncated list size0.000.250.500.751.00Fraction of queriesWith learned model.\nWithout learned model.\na query is guaranteed correct if the list of at least one term is not\ntruncated. In contrast, without a learned model allquery-terms\nneed complete lists for guaranteed correct results. Figure 3 displays\nthe difference between the two-tiered approach with andwithout\nthe learned model. As expected, the learned model considerably\nincreases the correctness of the results in the first stage.\nFinally, we answer RQ2 positively: our results show that even the\nmost storage inefficient approach with high model costs can produce\nsubstantial reductions in storage requirements.\n5 CONCLUSION\nIn this study, we have explored how search based on Boolean inter-\nsection may benefit from the usage of learned index structures. We\nhave proposed several approaches by which a learned model can pro-\nduce substantial reductions in storage requirements. Each approach\nmakes a tradeoff between storage requirements and computational\ncosts. Our results show that even conservative estimates on the po-\ntential gains w.r.t. space benefits are considerable. We expect that\ncombining learned index structures with inverted indexes will be\na fruitful research direction in the near future.\nAcknowledgements . This work was partially supported by the\nNetherlands Organisation for Scientific Research (NWO) under\nproject nr. 612.001.551 and the Australian Research Council’s Dis-\ncovery Projects Scheme (DP170102231).\nREFERENCES\n[1]N. Asadi and J. Lin. 2013. Effectiveness/efficiency tradeoffs for candidate\ngeneration in multi-stage retrieval architectures. In Proc. SIGIR . 997–1000.\n[2]B. Carterette, V. Pavlu, H. Fang, and E. Kanoulas. 2009. Million Query Track 2009\nOverview. In Proc. TREC .[3]R.-C. Chen, L. Gallagher, R. Blanco, and J. S. Culpepper. 2017. Efficient cost-aware\ncascade ranking in multi-stage retrieval. In Proc. SIGIR . 445–454.\n[4]C. L. A. Clarke, J. S. Culpepper, and A. Moffat. 2016. Assessing efficiency–\neffectiveness tradeoffs in multi-stage retrieval systems without using relevance\njudgments. Inf. Retr. 19, 4 (2016), 351–377.\n[5]W. B. Croft, D. Metzler, and T. Strohman. 2010. Search Engines: Information\nRetrieval in Practice . Vol. 283. Addison-Wesley Reading.\n[6]J. S. Culpepper, C. L. A. Clarke, and J. Lin. 2016. Dynamic cutoff prediction in\nmulti-stage retrieval systems. In Proc. ADCS . 17–24.\n[7]J. S. Culpepper and A. Moffat. 2010. Efficient set intersection for inverted indexing.\nACM Trans. Inf. Sys. 29, 1 (2010), 1.\n[8]B. Goodwin, M. Hopcroft, D. Luu, A. Clemmer, M. Curmei, S. Elnikety, and Y. He.\n2017. BitFunnel: Revisiting signatures for search. In Proc. SIGIR . 605–614.\n[9]A. Kane and F. Tompa. 2014. Skewed partial bitvectors for list intersection. In\nProc. SIGIR . 263–272.\n[10] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. 2018. The case for learned\nindex structures. In Proc. SIGMOD . 489–504.\n[11] D. Lemire and L. Boytsov. 2015. Decoding billions of integers per second through\nvectorization. Soft. Prac. & Exp. 45, 1 (2015), 1–29.\n[12] C. Macdonald, R. L. T. Santos, I. Ounis, and B. He. 2013. About learning models\nwith multiple query-dependent features. ACM Trans. Inf. Sys. 31, 3 (2013), 11.\n[13] J. Mackenzie, J. S. Culpepper, R. Blanco, M. Crane, C. L. A. Clarke, and J. Lin. 2018.\nQuery driven algorithm selection in early stage retrieval. In Proc. WSDM . 396–404.\n[14] A. Moffat and J.S. Culpepper. 2007. Hybrid bitvector index compression. In Proc.\nADCS . 25–31.\n[15] A. Moffat and J. Zobel. 1996. Self-indexing inverted files for fast text retrieval.\nACM Trans. Inf. Sys. 14, 4 (1996), 349–379.\n[16] G. Ottaviano and R. Venturini. 2014. Partitioned Elias-Fano indexes. In Proc. SIGIR .\n273–282.\n[17] J. Pedersen. 2010. Query understanding at Bing. In SIGIR Industry Day .\n[18] C. Rossi, E.S. de Moura, A.L. Carvalho, and A.S. da Silva. 2013. Fast document-\nat-a-time query processing using two-tier indexes. In Proc. SIGIR . ACM, 183–192.\n[19] T. Russell-Rose and P. Gooch. 2018. 2dSearch: A visual approach to search strategy\nformulation. In Proc. DESIRES . 90–96.\n[20] A. Trotman. 2014. Compression, SIMD, and postings lists. In Proc. ADCS . 50–57.\n[21] L. Wang, J. Lin, and D. Metzler. 2011. A cascade ranking model for efficient ranked\nretrieval. In Proc. SIGIR . 105–114.\n[22] J. Zobel and A. Moffat. 2006. Inverted files for text search engines. ACM Comp.\nSurv. 38, 2 (2006), 6.",
  "textLength": 23076
}