{
  "paperId": "9471c0f199f15bc1711ea9654a011bb44cf7b764",
  "title": "Baihe: SysML Framework for AI-driven Databases",
  "pdfPath": "9471c0f199f15bc1711ea9654a011bb44cf7b764.pdf",
  "text": "Baihe: SysML Framework for AI-driven Databases\nAndreas Pfadler1, Rong Zhu1, Wei Chen1, Botong Huang1, Tianjing Zeng1,2,\nBolin Ding1, Jingren Zhou1\n1Alibaba Group ,2Renmin University of China\n{andreaswernerrober, red.zr, wickeychen.cw, botong.huang, zengtianjing.ztj,\nbolin.ding, jingren.zhou}@alibaba-inc.com\nABSTRACT\nWe present Baihe , a SysML Framework for AI-driven Databases. Us-\ningBaihe , an existing relational database system may be retrofitted\nto use learned components for query optimization or other com-\nmon tasks, such as e.g. learned structure for indexing. To ensure the\npracticality and real world applicability of Baihe , its high level ar-\nchitecture is based on the following requirements: separation from\nthe core system, minimal third party dependencies, Robustness,\nstability and fault tolerance, as well as stability and configurability.\nBased on the high level architecture, we then describe a concrete\nimplementation of Baihe forPostgreSQL and present example use\ncases for learned query optimizers. To serve both practitioners, as\nwell as researchers in the DB and AI4DB community Baihe for\nPostgreSQL will be released under open source license.\n1 INTRODUCTION\nLearned systems, particularly learned database systems, have been\na recent research hot spot [ 2,5,19]. As part of this trend, a consider-\nable number of researchers have proposed integration of machine\nlearning models into relational database systems in order to opti-\nmize overall system performance and cope with the ever increasing\namounts of data that are being processed in modern data intensive\napplications.\nAs part of this trend, machine learning based methods for a\nwide range of problems have been proposed. Most efforts appear\nto have been spent on problems related to query planning and\noptimization, where machine learning models may be used e.g. for\ncardinality estimation [ 2,4,16,18,20], join order selection [ 10],\ncost prediction [ 14,15] or query planner steering through hint set\nproposals [8].\nIn addition to the above, other lines of research have focused\non learned data structures [ 6] used for efficiently querying large\nindices [ 10‚Äì13], automatic database configuration tuning, accurate\nquery run time prediction and other problems [19].\nAs is the case with many publications in the machine learning\nspace, on paper the reported results do indeed look impressive\nand suggest significant performance gains, if such methods were\never deployed in the real world. Yet, as most prior research has\nbeen justified on the basis of numerical experiments, performed\noutside of real systems, there has been a striking absence of work\nfocusing on the practical deployment of ML-based methods inside\nreal world systems. The central goal of this work is to present a\nconcrete proposal on how this notable gap between theory and\npractice may be closed. For this aim, we propose here the Baihe\nSysML framework.Baihe is a general framework developed for integrating machine\nlearning models into a relational database systems, meaning that its\nfundamental architecture should provide a blueprint for other im-\nplementation projects in the AI4DB and SysML space. To exemplify\nits applicability and generate some more robust evidence for the\nactual usefulness of prior research in AI4DB, we have developed the\nBaihe extension for PostgreSQL , a widely used, highly successful\nand extensible relational database system.\nThis article is now structured as follows: In Section 2, we first\ndiscuss general challenges for the implementation of AI-driven\ndatabases and then present our long term vision for AI4DB. Based\non this, we then describe the rationale and fundamental principles\nforBaihe ‚Äôs design.\nLater on, in Section 3 we present the concrete design and features\nofBaihe in the case of PostgreSQL and further discuss implemen-\ntation specifics. After show a typical example use case for Baihe on\nquery optimizer in Section 4, we conclude this article in Section 5.\n2 BACKGROUND AND DESIGN RATIONALE\nOn a fundamental level, the nature of and requirements on a data-\nbase system appear to be at odds with the inherently stochastic\nnature of machine learning. Database developers, users, and ad-\nministrators expect rock-solid, stable and most of all deterministic\nbehavior. On the other hand, machine learning models may produce\npredictions with hard to estimate error bounds and their generaliza-\ntion ability may fail catastrophically, once the input data distribu-\ntion changes. Moreover, failures may not be readily detectable, since\nthey might only be visible through a change in some numerical\nvalue rather than a concrete and meaningful error message.\nMoreover, since training of a machine learning model generally\namounts to solving a mathematical optimization problem, often\nthrough the use of stochastic optimizers, model training is difficult\nto automate, requires close supervision by experts and may thus not\nbe fully regarded as a well defined process. Hyperparameters, useful\ntermination criteria and evaluation guidelines all need be carefully\ndeveloped on both a per-model and per-dataset basis. All of this\nmakes it very difficult to integrate automated training procedures\ninto a system with strong requirements on robustness and stability.\nAs challenging as this may sound, we still believe that the prac-\ntical combination of machine learning and databases into the next\ngeneration of database systems is not a hopeless endeavor.\nTo see this, we first note that some core components of a data-\nbase system already rely on statistical estimates. Modern query\noptimizers in particular rely on such estimates e.g. in the context\nof cardinality and cost estimation, which are in turn used for join-\norder-selection and other automated decision making. The visionarXiv:2112.14460v1  [cs.DB]  29 Dec 2021\n\nAndreas Pfadler1, Rong Zhu1, Wei Chen1, Botong Huang1, Tianjing Zeng1,2,\nBolin Ding1, Jingren Zhou1\noutlined by e.g. SageDB [ 5] has made a compelling case for us-\ning more advanced probabilistic methods - powered by machine\nlearning - to improve existing issues arising from the relatively\nsimple statistical estimations which are still part of the standard\nimplementations of today‚Äôs database systems.\nSecond, we believe that issues related to using machine learning\nenabled components inside a database system, should merely be\nregarded as additional requirements on the engineering of such\nsystems. Hence, we argue that one should in principle be able to\nsatisfactorily fulfil these requirements, ifthey are properly taken\ninto account during the system design phase and possibly enhanced\nthrough system-algorithm co-design. We hope to exemplify and\nsupport this point of view through the Baihe framework presented\nin this work.\n2.1 Learned Databases: Brief Overview\nBefore presenting the high-level architecture of Baihe and the\nrationale behind it, we first review related literature and describe\nthe current stage and a future vision for learned database systems.\nWe categorize learned databases into four levels, corresponding\nroughly both with historical development, as well as the amount of\n‚Äúintelligence‚Äù added by learned components.\n(1)Primary utilization in DBMS: Many available commer-\ncial and open-source DBMS have a long tradition of using\ncollected statistics and statistical estimators to support some\nof their data management functions. Some of these methods\nserve as an integral parts of some components. For exam-\nple, cardinality estimation ( CardEst ) consists of estimating\ndata distribution properties used later as input for query\noptimization (QO). While for instance SQL Server and Post-\ngreSQL build histograms and collect frequently occurring\nvalues for table attributes, MySQL and MariaDB apply a\nsampling strategy for CardEst . With respect to non-integral\nfunctions, basic statistical methods are used for advisory\nfunctions, such as index or view advisors, or knob tuning\nand SQL rewriters [19].\n(2)Individually learned components: With the recent in-\ncreased interest in machine learning, an increased amount of\nwork has tried to design ML-based models to replace certain\ncomponents in DBMS. In this context, the most representa-\ntive works address QO and indexing. For QO, a variety of\nsupervised and unsupervised models have be proposed for\nCardEst [2‚Äì4,16,18,20], cost estimation [ 14,15] and join\norder selection [ 10,13]. Meanwhile, a number of learned\ndata structures structures are proposed for single and multi-\ndimensional indexing [ 6,11,12]. It has been shown such ML-\nbased methods often exhibit superior performance, when\nevaluated in numerical experiments outside of actual sys-\ntems or tightly controlled experimental setups inside of real\nsystems (e.g. through injecting cardinality from an outside\nfile). However, many problems relevant to real-world de-\nployment, such as e.g. explainability vs. predictive power,\nrobustness or fault tolerance remain largely unaddressed.\n(3)Comprehensively learned modules: On top of individu-\nally learned components, current work further moves for-\nward by trying to substitute an entire functional module, e.g.the QO, executor or storage engine with a machine learning\nmodel. Some approaches combine multiple learned compo-\nnents together, learn to steer existing modules [ 8], or even\nattempt to \"learn\" an entire module in an end-to-end fash-\nion [ 9]. Although most work of this type claims to achieve\nincredible performance gains, many proposed solutions ap-\npear to be impractical for actual deployment in real-world\nDBMS. The reasons for this are two fold. First, learned mod-\nules of this type are highly task specific and data dependent,\nwhich may cause serious shortcomings, including but not\nlimited to: cold start problems, lack of generalization, tuning\ndifficulties. Second, replacing an entire module may cause\ncompatibility risks and may require substantial engineering\nefforts. Therefore, at this stage, even evaluating a learned\nmodule inside a real DBMS is a difficult task.\n(4)AI-Native databases: Some very recent work even pro-\nposes to redesign the whole architecture of DBMS to fully\nadapt AI models for data management. For example, [ 17]\nproposes a \"one model for all architecture\", which learns\na shared representation of data and query knowledge and\nthen fine-tunes smaller models for each specific task. [ 7] pro-\nposes \"AI-designed databases\", where AI is integrated into\nthe life cycle of database design, development, evaluation,\nand maintenance, hoping to provide optimal performance\nfor every scenario. Although this appears to be an intrigu-\ning vision, it is clear they are still far away from any useful\nimplementation.\nHence, regarding ‚ÄúAI for DB‚Äù, we observe that:\n1. This field is very prosperous. The research efforts range from\nindividual components to complete modules to even the whole\narchitectures, and include many scenarios (QO, indexing, execu-\ntion, storage and etc). Enough evidence has shown that AI-based\nsolutions could indeed improve database performance and have the\npotential to play significant roles in next-generation DBMS.\n2. Current work mainly focuses on ‚ÄúAI‚Äù solutions but is not\nconcerned with how to actually deploy them ‚Äúfor DB‚Äù. In addition\nto that, more realistic tests for newly proposed methods relying on\ne.g. existing extension functionalities of DBMS are rarely described\nin the literature, possibly because it requires deep expertise beyond\nthe design of ML models. Such expertise requires a comprehensive\nunderstanding of both AI and DB perspectives and a systematic\nco-design of both algorithm and systems.\nTherefore, we believe that a SysML framework such as Baihe ,\nwhich supports both evaluation and deployment of AI-driven solu-\ntions in real systems is crucial for the further development of the\nAI for DB field.\n2.2 High Level Architecture of Baihe :\nFundamental Design Choices and\nTrade-Offs\nWe now briefly describe the high level design of Baihe and discuss\nthe rationale and design philosophy behind it. We note that this\ndesign should rather be considered as a design blueprint resp. design\npattern, which may then have to be adapted to a specific host\ndatabase system. As an example we present Baihe forPostgreSQL\nin Section 3.\n\nBaihe: SysML Framework for AI-driven Databases\nTrainingEnvironmentBaiheSupportLibraryBackground Service WorkersDeployment\nModel 1\nModel 2\nModel n‚Ä¶‚Ä¶DB Host SystemBaiheExtensionIntegrationLayerDataCollectors\nTraining DataCollectSupportInferenceProcess ManagementBaiheFramework\nFigure 1: High Level View of Baihe ‚Äôs Architecture.\nThe core part of Baihe is an extension which plugs directly into\na database host system through an existing extension mechanism.\nThis extension is the central component of Baihe , as it\n(1)intercepts the query planning and execution process, so that\nindividual steps may be substituted with model inference\ncalls,\n(2)contains a clone of the host system‚Äôs planner (\"shadow plan-\nner\"), which can be conveniently extended and modified, so\nthat core functionality may be overwritten without interfer-\ning with the host system itself,\n(3) provides convenient configuration facilities,\n(4) controls collection of training data.\nMost of the above functionality is encapsulated in the Integration\nLayer component, which is the central control unit for Baihe .\nModel inference is decoupled from the extension itself, this\nmeans that for every model a new background worker process\nis used. These processes communicate with the Baihe extension\nand return results for inference requests. If the host system sup-\nports it, the control of these processes is managed using the host\nsystems process management capabilities.\nFinally, Baihe needs a support library. This component allows for\nsimple access to collected training data, such as e.g. query runtime\nstatistics or saved query plans. Furthermore, it provides function-\nality for convenient deployment of trained models into the host\nsystem.\nThe design above is based on the following high-level require-\nments:\n‚Ä¢Separation from the core system.\n‚Ä¢Minimal third party dependencies.\n‚Ä¢Robustness, stability and fault tolerance.\n‚Ä¢Usability and configurability.\nFor the remainder of the section we now describe the impact of\nthese requirements on our design in more details.\nSeparation from the Core System. Both commercial and open\nsource RDBMS have been and are being actively maintained andsupported over long periods of times. Commercial license and sup-\nport agreements may span years or even decades, so that customers\nmay receive regular updates and critical patches, especially those\naddressing security issues. In order to impact existing processes as\nlittle as possible, all while avoiding a development of an entirely\nnew database system, we have chosen to develop Baihe as an exten-\nsion on top of an existing system, so that we may keep it as separate\nfrom the host system as possible. In this way, we avoid unnecessary\ndoubling of maintenance efforts and allow for a quicker pace of\ndevelopment.\nMinimal Third Party Dependencies. Modern ML stacks are\ncharacterized by a large number of dependencies, comprising low\nlevel numerical libraries and GPU kernels, intermediate ML frame-\nwork and runtime codes (typically implemented in C++), as well\nas high level integration code typically written in Python. To train\nand use what may now even be considered relatively simple deep\nmodels, a large number of packages and libraries at these three\nlevels must be present on a host system. In practice, this typically\nleads to a high degree of maintenance efforts for both production,\nas well as test and development environments. This is particularly\nproblematic in more traditional organizations, such as financial\ninstitutions, where - due to security or legal reasons - individual\nsoftware packages may have to go through a specific vetting and\napproval process. We have thus designed Baihe such that it requires\na minimal amount of external dependencies beyond those required\nby the core system.\nRobustness, Stability and Fault Tolerance. For most mod-\nern applications databases are among the most fundamental and\nmission-critical components. It is thus imperative that additional\ndeployment of ML-based components into such systems does not\nimpact existing service level agreements or interfere with related\noperational requirements. Therefore, errors arising from e.g. model-\nbased predictions, which might influence an individual session or\nthe system as a whole should be detected and mitigated through\nfallbacks to existing core functionality.\nUsability and Configurability. To further ease the burden of\nintegration, it should be possible to control and configure Baihe\nthrough standard mechanisms offered by the host system. In the\n\nAndreas Pfadler1, Rong Zhu1, Wei Chen1, Botong Huang1, Tianjing Zeng1,2,\nBolin Ding1, Jingren Zhou1\nconcrete case of a system such as PostgreSQL this means that Baihe\nshould be able to be configured through the usual PostgreSQL\nconfiguration files, as well as provide a set of user defined functions\nas well as stored procedures, such that Baihe functionality may\nbe configured, activated or deactivated through any authorized\ncommand session and without requiring any restarts of the system\nas a whole.\nBased on the above four points we may now formulate more\nconcrete design goals with respect to the machine learning aspects.\nModel Support. As a general framework, Baihe should support\ndeploying models for a range of different tasks, such as e.g. cardi-\nnality estimation, join order selection, query run time prediction.\nFurthermore, it should support models for learned data structures,\nsuch as e.g. learned indices.\nWith respect to the models themselves, Baihe should offer sup-\nport for both neural network, resp. deep learning based, model\nfamilies, as well as more traditional ones, such as e.g. probabilis-\ntic graphical models, decision trees, random forests or gradient\nboosted trees.\nModel Training. As we have discussed in the previous section\n- despite the recent progress in AutoML [ 19] - model training still\nrequires close expert supervision supported by suitable tooling\nallowing for thorough model evaluation and rapid experimentation.\nInBaihe we thus prefer to decouple model training from the rest\nof the system as much as possible. While Baihe should still provide\nsuitable functionality for training data collection, as well as tools for\nmodel export and deployment, we believe that training itself should\nbe set up in a separate environment under control by specialist\nusers such as data scientists or machine learning engineers. Once\ntraining has achieved satisfactory progress, a model can then be\ndeployed using a well-defined deployment process.\nModel Inference and Deployment In order to avoid expen-\nsive serialization and de-serialization steps one might want to inte-\ngrate a model directly into e.g. the planner component of the host\nsystem. On the other hand, to maintain the maximal amount of\nflexibility with respect to software dependencies and computing\nresources needed for inference, one might also consider imple-\nmenting model inference in a completely separate service process\noutside of the control of the host system. We believe that both of\nthese extremes would clash with requirements on robustness and\nstability, as well as maintenability of the host system. In Baihe we\nthus choose to isolate inference in a separate process, but keep\nthis process under management by the host system. To eliminate\nthe need for expensive serialization steps we furthermore propose\nprocess co-location, so that existing shared memory facilities may\nbe used as much as possible.\nWhile current (practical) models in the SysML space, even deep\nones, can still be considered relatively light-weight [ 2,20], we\nhence believe that computational resources needed by the inference\nprocess would in general not adversely affect the core databases\nprocess on the same machine. In the long term, should there be the\nneed for computationally more expensive models to be deployed\nin the system, one could address this problem, at least in cloud\nenvironments, through on-demand attachable resources.\nAs a consequence of the co-location requirement, one could imag-\nine having to install additional packages on the machine running\nthe host system, which would be needed to run model inference (e.g.ML framework runtimes etc.). To address this issue Baihe should\nprovide proper tools allowing for exporting models such that they\ncan readily be used with as little extra dependencies as possible. To\nachieve this goal, we propose to make use of the recent advances\nin the context of ML model compilers [ 1], which make it possi-\nble to compile models together with custom CPU or GPU based\nmath kernels into highly efficient binary code, that may be accessed\nthrough a C-ABI. Nevertheless, Baihe should support a \"Develop-\nment Mode\", where models may be developed and tested in the host\nsystem without intermediate compilation and build steps.\n3 SYSTEM DESIGN AND IMPLEMENTATION\nWe have started Baihe development as a first step towards the\nlong term vision outlined in Section 2.1. As such, our main target\nforBaihe has been deployment in the widely used PostgreSQL\nDatabase System, which - thanks to its open nature and high de-\ngree of extensibility - makes for an excellent candidate system for\nretrofitting modern ML-based approaches into a real-world DB. We\nnow describe Baihe ‚Äôs design and implementation for PostgreSQL .\nBaihe forPostgreSQL consists of three components: The Baihe\nextension for PostgreSQL , a number of Baihe background worker\nprocesses and the Baihe support library. The relation between\nthese components is shown in Figure 2. We now describe these\ncomponents in more details.\nBaihe extension for PostgreSQL .This component is loaded\nupon database startup and uses the existing hooking mechanism\nofPostgreSQL in order to intercept query planning and execution.\nAll of Baihe ‚Äôs functionality is controlled through this component,\nincluding collection of training data (e.g. query runtime statistics)\nand model handling. The entry point for all of these functions is\nimplemented in the Integration Layer component. All outside\ncalls first end up in this component. Upon startup, it will first\nrequest additional shared memory from the PostgreSQL host system\nwhere it initializes control data structures needed for both the IPC\nModule andData Collection module.\nBackground Workers. Once a model is available for deploy-\nment, Baihe can be requested to begin using it. In this case it will\nuse the existing PostgreSQL functionality to start a background\nworker process which will load the deployed model from the file\nsystem and wait for incoming inference requests on a message\nqueue living in a shared memory space.\nBaihe Support Library. TheBaihe support library provides a\nhigh level Python API which serves two main purposes: Provide\naccess to training data collected by the Baihe extension and fur-\nthermore expose functionality needed to deploy models in either\nproduction or development mode. Model deployment in develop-\nment mode amounts to saving model weights and creating a Python\nmodule which can loaded by a Python interpreter embedded in a\nBaihe background worker process. In production mode a custom\nLLVM-based model compiler is used to compile the entire model\ninto a standard shared library, that is loaded dynamically by back-\nground workers.\n3.1 Baihe Integration Layer and IPC Module\nTheBaihe integration layer is the central control unit of the Baihe\nextension: It is accessed from the host system by implementing\n\nBaihe: SysML Framework for AI-driven Databases\nPostgreSQL Host SystemBaiheExtensionIntegration LayerBackendProcessExec. HookPlanner HookOther Hooks‚Ä¶‚Ä¶ShadowPlannerData Collector‚Ä¶‚Ä¶\nCollector 1\nCollector 2\nCollector n‚Ä¶‚Ä¶InferenceControl and ConfigBackground  Workers\nModel 1\nModel 2\nModel n‚Ä¶‚Ä¶IPC Module     Shared MemoryDataBufferDisk StorageTraining Data TablesTrainingEnvironmentBaiheSupportLibrarySaved ModelsCollected DataInference Message FlowControl and Config Information FlowCollected Data FlowModel Information FlowBaiheFramework for PostgreSQL\nHook Information Flow\nFigure 2: Baihe Architecture.\nsome of the hooks already defined in PostgreSQL , where it imple-\nments Baihe ‚Äôs high level logic. All functionality for communication\nwith background workers, as well as necessary process manage-\nment for background workers is encapsulated in the IPC Module.\nThe IPC module makes extensive use of Postgres core APIs used for\nmanagement of shared memory, as well as process management.\nAdditionally, the Baihe Integration Layer defines user defined\nfunctions and session variables which are necessary for controlling\nmodel handling and data collection, e.g. starting and stopping back-\nground workers, defining which models should be used in which\nsituations, as well as defining when and how training data should\nbe collected.\nShadow Planner. To allow for a maximum degree of flexibility\nwith respect to models providing input for query planning, the\nBaihe extension contains a customized duplicate of the core Post-\ngreSQL planner as a \"Shadow Planner\" component. In this way we\ncan achieve the following: First, we may freely add new hooks into\nthe planner without having to modify core source code1. Second,\nthe behavior of the planner as a whole may be adjusted and new\nideas tested without having to interfere with the core source. As an\nadditional benefit, this reduces overall compilation and build times.\n1Currently, PostgreSQL itself offers e.g. no hooks to overwrite cardinality estimation\nfor e.g. single table queries or join size estimatesCurrently, in addition to the existing hooks originating from the\noriginal PostgreSQL code, we have equipped the shadow planner\nwith the following hooks:\n‚Ä¢Cost Model: we add an additional hook per \"node\" in a query\nplan. This allows for overwriting cost predictions for such\noperations as sequential scan, index scan, nested loop join,\nhash join etc. Furthermore, we incorporate hooks for esti-\nmating costs of query predicates making use of operations\nbeyond comparison operators for numerical values. We plan\nto further support hooks for overwriting the cost estimation\nof user defined functions etc.\n‚Ä¢Cardinality Estimation: We add hooks at several levels of the\ncardinality estimation process, such as cardinality estimation\nfor a query involving a single table or a join between two\ntables.\nTo improve the interplay between hooks and planner code, we\nfurthermore design the concrete hook signatures and calling code\nwith error handling mechanisms allowing for seamless fallback to\nstandard planner behavior in the case of errors originating e.g. from\nerroneous model inference calls. We discuss how shadow planner\nand related hooks are specifically used for query optimization task\nin Section 4.\n\nAndreas Pfadler1, Rong Zhu1, Wei Chen1, Botong Huang1, Tianjing Zeng1,2,\nBolin Ding1, Jingren Zhou1\n3.2 Data Collection\nThe design of Baihe ‚Äôs data collection module borrows heavily from\nthe popular pg_stat_statements extension for PostgreSQL . How-\never, since Baihe targets data collection for training machine learn-\ning models, it takes a more dataset-centric point of view.\nMore concretely, Baihe data collection is designed around the\nnotion of \"Data Collectors\". Users may define and activate several\nData collectors at the same time, where each data collector may\nbe defined as a set of filter conditions plus a versioned data set\nidentifier. In this way users may for each Data Collector specify the\nfollowing:\n‚Ä¢Filter conditions : For which query type (SELECT, INSERT, ...)\ninvolving which tables should this data collector be applied?\n‚Ä¢Dataset identifier and version : For reproducibility a debugging\npurposes it is essential to keep track of exactly which data\nwas used to train a specific model version. Hence, any data\nset collected is identified through both a dataset identifier as\nwell as a version number.\n‚Ä¢Features : For some dataset and model combinations, only\nqueries themselves might need to be collected, while for\nothers it might be necessary to also collected generated query\nplans, together with estimated costs and actual run times\nboth on query plan, as well as plan-node level. Users may\nflexibly specify, which features should be saved by a data\ncollector\nData collection may be controlled entirely through a standard com-\nmand session. After a data collector has been defined through a call\nto aBaihe stored procedure, the data collection process itself may\nalso be started and stopped by running start and stop routines ex-\nposed from the Baihe extension through custom stored procedures.\nSee Figure 3b for an example.\nWhile a Data Collector is active, all collected data will be stored\nin shared memory.In the case of very large datasets, shared memory\ncontent may be temporarily stored on disk. Once data collection\nis stopped, a data set with incremented version identifier will be\nwritten to disk and made available in a table specified in the Data\nCollector‚Äôs configuration. Training data may then easily be accessed\nthrough SQL.\n3.3 Model Integration\nAlong the lines of our requirements on minimization of dependen-\ncies and model inference and deployment as described in Section 2,\nmodel inference takes place in background worker processes. For\nevery model registered in Baihe , a user can request Baihe to start a\nbackground worker process, which will\n(1) Load the saved model from disk\n(2) Connect to the Baihe shared memory space\n(3) Wait for incoming inference requests on a message queue.\n(4)Once an inference request is received, the background worker\nwill run the request through the loaded model and return in-\nference results (which may possible also just a flag indicating\nthat an error has occurred).\nOnce a query is submitted by a client to the corresponding Post-\ngreSQL backend process, the query planning and execution pro-\ncess will be intercepted by the Baihe extension and depending onmodel type, a number of inference requests will be sent to the\ncorrect background workers. All communication is implemented\nasynchronously, so that a backend may fall back to standard func-\ntionality in case a background worker is not available.\nOut of the Box Baihe allows for the integration of custom mod-\nels for query runtime prediction, as well as cost and cardinality\nestimation. Models of these types may be used directly without\nchanges to the Baihe extension source code. More specific types of\nmodels, requiring e.g. new hooks at certain places in the planner\ncode, may easily be supported with slight changes to the Baihe\nextension code.\nSimilar to the data collection functionality described in the previ-\nous subsection, Baihe ‚Äôs model handling facilities are controlled and\nconfigured using a number of stored procedures and user defined\nfunctions implemented in the Baihe extension. A simple usage\nexample is displayed in Figure 3a: Through a standard command\nsession users with the right permissions may request models to be\nregistered and the corresponding background workers to be started\nor stopped. This allows for model updates without having to restart\nthe entire system.\nAs mentioned previously in Section 2 Baihe ‚Äôs focus is on model\ninference only. This means that the training process itself, that is\nsolving the optimization process for a certain combination of model\nand training data, does not run in any Baihe components. Instead,\nthe usual development process can be outlined as follows:\n(1)Training data is selected and downloaded using the Baihe\nsupport library, implemented as a Python packages.\n(2)A model can then be defined and trained, preferably using\na framework supported by Baihe ‚Äôs production mode. Cur-\nrently supported frameworks are sklearn and Tensorflow.\nTraining is controlled entirely by an expert user, such as e.g.\na data scientist or machine learning engineer.\n(3)Once the model has been trained and evaluated, it may be\ndeployed using the Baihe support library.\n‚Ä¢Development mode : In this mode, the model is deployed as a\nPython model on the database servers file system, together\nwith an automatically created environment containing all\nthe model‚Äôs dependencies. A background worker then uses\nan embedded Python interpreter to access the model.\n‚Ä¢Production mode : In this mode, Baihe ‚Äôs support library is\nused to compile the model including trained parameters\ninto a shared library that is loaded dynamically by a back-\nground worker. The shared library does not depend on\nany external numerical or ML framework libraries. Model\ncode itself will be compiled together with a number of\nmath kernels (implementing e.g. matrix multiplications,\nconvolutions, etc.) into a self contained component with a\nstandardized interface.\n4 EXAMPLE USE CASE: LEARNED QUERY\nOPTIMIZER FOR POSTGRESQL\nWe describe now a typical use case of for Baihe : deploying a learned\nquery optimizer into PostgreSQL . More concretely, we discuss here\nthe following two variants:\n(1)QO with individual components: cardinality estimation and\ncost model substituted with separately trained components.\n\nBaihe: SysML Framework for AI-driven Databases\n## Define a data collector\n## Filter queries by tables and query type\nCALL DEFINE_DATA_COLLECTOR ( ‚ÄúCardEstCollector‚Äù,\n{ ‚Äútbl_users‚Äù, ‚Äútbl_items‚Äù, . . . }, { ‚ÄúSELECT‚Äù } );\n## Start data collection\nCALL START_DATA_COLLECTOR ( ‚ÄúCardEstCollector‚Äù,\n‚ÄúData_Set_1‚Äù, ‚Äútbl_training_data‚Äù );\n## Stop data collection\nCALL STOP_DATA_COLLECTOR ( ‚ÄúData_Set_1‚Äù);\n(a) Configuring data collection for a single cardinality estimation\nmodel. Only data related to SELECT queries touching certain tables\nis collected. Data collection can be started and stopped.## Model Registration\nCALL REGISTER_MODEL ( ‚ÄúMyCardEstModel‚Äù, ‚ÄúCARDEST‚Äù,\n{ ‚Äútbl_users‚Äù, ‚Äútbl_items‚Äù, . . . }, ‚Äútbl_my_cardest_model_stats‚Äù );\n## Start Background Process for Model\nCALL START_MODEL ( ‚ÄúMyCardEstModel‚Äù );\n## Stop Background Process for Model\nCALL RESET_MODEL ( ‚ÄúMyCardEstModel‚Äù );\n(b) Deploying a trained model into the system: The model is used\nonly for queries touching certain tables and maybe activated or de-\nactivated when requested by the user.\nFigure 3: Example configuration sessions for Baihe\n(2)End-2-End QO: Here, the entire query optimizer is substi-\ntuted by a trained model.\n4.1 QO with individually learned components\nCardEst models and ML-based cost models are supported out-of-the\nbox. The shadow planner in the Baihe integration layer intercepts\nall requests for a cardinality estimate for a query touching a set of\ntables ùëáwith set of query predicates ùëÑ. The tuple(ùëá, ùëÑ)is obtained\nfrom internal query parse tree structures, serialized and passed into\na trained model running in a background worker. The model then\nreturns a selectivity 0‚â§ùë†‚â§1, which is passed on to the planner.\nTraining data collection for a CardEst model depends on whether\nthe model is based on query-driven or data-driven CardEst . Specif-\nically, data-driven CardEst methods build unsupervised models\nover the tabular data, then the cardinality of any query could be\nestimated over this model. For data driven CardEst no additional\ndata collector is needed, since models may be trained simply using\n(samples of) table data and schema information provided by the\nuser (the latter being required for models supporting multi-table\nCardEst ).\nQuery-driven CardEst methods build the supervised models\nmapping featurized queries to the cardinality. For a query driven\nmodel, we first define a data collector, which, for every query, saves\nthe entire query plan, together with all statistics collected during the\nexecution (i.e. the entire output of EXPLAIN ANALYZE). Training\ncode can then load this data and convert it to the required form of\n(Subquery ,Cardinality)records needed for training.\nOnce a CardEst model has been trained it may then easily be\nregistered as a model of \"CARDEST\" type using a call to the corre-\nsponding Baihe procedure. To make the model active, a background\nworker is started using another Baihe procedure call and the ses-\nsion variable \"baihe_ce_model\" is set to the model identifier. Then,\nall subsequent queries for this session will use learned cardinality\nfrom the deployed model.\nThe process works similar in the case of cost models. Out of the\nbox, the Baihe will shadow planner intercepts all cost-estimation\ncall on a node-level (e.g. sequential scan, index scan, etc.). Then, a\nrecord depending on a variable number of features (depending onnode type), is built and sent to a cost estimation model running in\na background worker, which then returns a predicted cost in terms\nof cost units.\nTo collect training data, we register a data collector with the same\nsettings as used for query-driven CardEst . In this way, for every\nnode in a query plan we obtain all features required for training a\nmeaningful model, with the most important features being node\ntype, estimated cardinality, actual cardinality and the time needed\nto execute a node.\n4.2 End-2-End Learned QO\nSome recent work also presents methods for learning a query plan\ndirectly. Such methods take a query as input, apply a certain fea-\nturization scheme and return an entire query plan as output. For\nour example we take a closer look at two major representatives of\nthis line of work, namely NEO [ 9] and BAO [ 8] and show how they\ncould be deployed using Baihe .\nNEO applies tree convolution networks to extract features from\nstructured query plans and learns a function, called value network,\nmapping plans to execution latency. Then, a best-first search strat-\negy is used to find a near-optimal query plan as measured by the\nvalue network.\nTo deploy NEO, we first register a data collector as used for\nquery-driven CardEst in the previous subsection. Then, we a back-\nground workers which implements value network inference and\nthe best-first search strategy, respectively. During query execution,\nwe intercept the planning process at the highest level in the Baihe\nintegration layer (right after the Baihe extension is first called by\nthe host system) and forward the query to the the background\nworker, evaluates the value network worker and returns a query\nplan after running the best-first search. This plan is directly sent to\nthePostgreSQL engine for execution.\nBAO adapts a different strategy than NEO. It learns to steer but\nnot the replace the QO. Specifically, it also learns a latency predic-\ntion network which maps a query plan to its execution latency. For\neach query, it generates several plans corresponding to different\nhint sets and then selects the plan with the minimum predicted\n\nAndreas Pfadler1, Rong Zhu1, Wei Chen1, Botong Huang1, Tianjing Zeng1,2,\nBolin Ding1, Jingren Zhou1\nlatency. Hence, to deploy BAO, data collection and model deploy-\nment need to be configured in exactly the same way as BAO and\nNEO.\nNote that the above discussion only concerns the case where\nboth the BAO and NEO models have been trained to a certain point\nand then remained unchanged after deployment. However, both\nmodels have been designed to be updated in an online-manner, so\nthat they may possibly adjust to changes in the underlying data\nand workload, without having to be explicitly retrained.\nWhile online updates are not directly supported yet, we note\nthat model code running inside background workers could easily\nbe written in such a way that incoming inference request data\nmay simultaneously be used to updated the model running inside\na worker. However, we note that - at least for now - it is then\nthe responsibility of each such background worker to properly\nmanage model state, ensure that model updates don‚Äôt block future\ninference requests and deal with errors that might occur during\nonline updates.\n5 OPEN SOURCE RELEASE AND FUTURE\nPLANS\nAs development of Baihe has started only recently, it is not yet\navailable for general use. However, we plan to release a first ver-\nsion of Baihe forPostgreSQL under an open source license in the\nbeginning of 2022. This version should contain all of the essential\nfunctionality needed to build and experiment with learned query\noptimizers as described in the previous subsection. Later on, in the\nsecond half of 2022, we plan to release an extended version of Baihe\nwhich has seen first tests under real world conditions and supports\nproduction-mode deployments.\nThe reasons for this release schedule are as follows:\n‚Ä¢First, We hope to encourage community participation in the\ndevelopment of Baihe as soon as possible.\n‚Ä¢Second, We wish to serve the DB research community by\nproviding a flexible and easy to use experimental platform\nfor for future research into AI4DB, hoping to establish a\nstandardized and realistic test bed for future models and\nalgorithms.\nOverall, we hope to have provided convincing arguments for\nthe soundness and practicality of Baihe as a design blueprint. The\nongoing development of Baihe forPostgreSQL should further help\nrefining this blueprint and serve as an implementation guide for\nother database systems.\nBesides the ongoing development, there are many avenues for\nfuture work. For instance, the current version of Baihe has been\ndesigned with most applications revolving around query optimiza-\ntion. However, one could envision Baihe to be used in the context\nof learned indices, database configuration tuning or other advisory\nfunctions.\nAnother aspect that has been left out for now concerns the\ntraining process itself, as well as models which may benefit from\nonline training. Integrating training and online updates of possibly\nlarge models directly into Baihe should certainly provide for many\ninteresting system design challenges.\nFinally we note that the development of production mode de-\nployment needs a custom model compiler infrastructure, whichfurther adds to the many engineering and research challenges that\naccompany this line of work. We encourage the entire community\nto actively participate and accept some of these challenges.\nREFERENCES\n[1]Tal Ben-Nun, Maciej Besta, Simon Huber, Alexandros Nikolaos Ziogas, Daniel\nPeter, and Torsten Hoefler. 2019. A modular benchmarking infrastructure for\nhigh-performance and reproducible deep learning. In 2019 IEEE International\nParallel and Distributed Processing Symposium (IPDPS) . IEEE, 66‚Äì77.\n[2]Yuxing Han, Ziniu Wu, Peizhi Wu, Rong Zhu, Jingyi Yang, Tan Wei Liang, Kai\nZeng, Gao Cong, Yanzhao Qin, Andreas Pfadler, Zhengping Qian, Jingren Zhou,\nJiangneng Li, and Bin Cui. 2022. Cardinality Estimation in DBMS: A Comprehen-\nsive Benchmark Evaluation. VLDB (2022).\n[3] Shohedul Hasan, Saravanan Thirumuruganathan, Jees Augustine, Nick Koudas,\nand Gautam Das. 2019. Multi-attribute selectivity estimation using deep learning.\nInSIGMOD .\n[4]Benjamin Hilprecht, Andreas Schmidt, Moritz Kulessa, Alejandro Molina, Kristian\nKersting, and Carsten Binnig. 2019. DeepDB: learn from data, not from queries!.\nInPVLDB .\n[5]Tim Kraska, Mohammad Alizadeh, Alex Beutel, H Chi, Ani Kristo, Guillaume\nLeclerc, Samuel Madden, Hongzi Mao, and Vikram Nathan. 2019. Sagedb: A\nlearned database system. In CIDR .\n[6]Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In SIGMOD . 489‚Äì504.\n[7]Guoliang Li, Xuanhe Zhou, and Sihao Li. 2019. XuanYuan: An AI-Native Database.\nIEEE Data Eng. Bull. 42, 2 (2019), 70‚Äì81.\n[8]Ryan Marcus, Parimarjan Negi, Hongzi Mao, Nesime Tatbul, Mohammad Al-\nizadeh, and Tim Kraska. 2020. Bao: Learning to steer query optimizers. arXiv\npreprint arXiv:2004.03814 (2020).\n[9]Ryan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh,\nTim Kraska, Olga Papaemmanouil, and Nesime Tatbul. 2019. Neo: A learned\nquery optimizer. arXiv preprint arXiv:1904.03711 (2019).\n[10] Ryan Marcus and Olga Papaemmanouil. 2018. Deep reinforcement learning for\njoin order enumeration. In Proceedings of the First International Workshop on\nExploiting Artificial Intelligence Techniques for Data Management . 1‚Äì4.\n[11] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning Multi-dimensional Indexes. In SIGMOD . 985‚Äì1000.\n[12] R Malinga Perera, Bastian Oetomo, Benjamin IP Rubinstein, and Renata Borovica-\nGajic. 2021. No DBA? No regret! Multi-armed bandits for index tuning of\nanalytical and HTAP workloads with provable guarantees. arXiv preprint\narXiv:2108.10130 (2021).\n[13] Ibrahim Sabek and Tim Kraska. 2021. The Case for Learned In-Memory Joins.\narXiv preprint arXiv:2111.08824 (2021).\n[14] Tarique Siddiqui, Alekh Jindal, Shi Qiao, Hiren Patel, and Wangchao Le. 2020.\nCost models for big data query processing: Learning, retrofitting, and our findings.\nInProceedings of the 2020 ACM SIGMOD International Conference on Management\nof Data . 99‚Äì113.\n[15] Tin Vu, Alberto Belussi, Sara Migliorini, and Ahmed Eldawy. 2021. A Learned\nQuery Optimizer for Spatial Join. In Proceedings of the 29th International Confer-\nence on Advances in Geographic Information Systems . 458‚Äì467.\n[16] Ziniu Wu and Amir Shaikhha. 2020. BayesCard: A Unified Bayesian Framework\nfor Cardinality Estimation. arXiv preprint arXiv:2012.14743 (2020).\n[17] Ziniu Wu, Peilun Yang, Pei Yu, Rong Zhu, Yuxing Han, Yaliang Li, Defu Lian, Kai\nZeng, and Jingren Zhou. 2021. A Unified Transferable Model for ML-Enhanced\nDBMS. arXiv preprint arXiv:2105.02418 (2021).\n[18] Zongheng Yang, Eric Liang, Amog Kamsetty, Chenggang Wu, Yan Duan, Xi Chen,\nPieter Abbeel, Joseph M Hellerstein, Sanjay Krishnan, and Ion Stoica. 2019. Deep\nunsupervised cardinality estimation. PVLDB (2019).\n[19] Xuanhe Zhou, Chengliang Chai, Guoliang Li, and Ji Sun. 2020. Database Meets\nArtificial Intelligence: A Survey. IEEE Transactions on Knowledge and Data\nEngineering (2020).\n[20] Rong Zhu, Ziniu Wu, Yuxing Han, Kai Zeng, Andreas Pfadler, Zhengping Qian,\nJingren Zhou, and Bin Cui. 2021. FLAT: Fast, Lightweight and Accurate Method\nfor Cardinality Estimation. VLDB 14, 9 (2021), 1489‚Äì1502.",
  "textLength": 45929
}