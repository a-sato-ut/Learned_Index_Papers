{
  "paperId": "272b4752b4d5a0d16a5d71930f7c81489bad4b21",
  "title": "Selectivity correction with online machine learning",
  "pdfPath": "272b4752b4d5a0d16a5d71930f7c81489bad4b21.pdf",
  "text": "SELECTIVITY CORRECTION WITH ONLINE MACHINE LEARNING\nMax Halford\nIRIT Laboratory\nIMT Laboratory\nUniversity of Toulouse\nmax.halford@irit.frPhilippe Saint-Pierre\nIMT Laboratory\nUniversity of Toulouse\nphilippe.saint-pierre@math.univ-toulouse.frFrank Morvan\nIRIT Laboratory\nUniversity of Toulouse\nfrank.morvan@irit.fr\nABSTRACT\nComputer systems are full of heuristic rules which drive the decisions they make. These rules of\nthumb are designed to work well on average, but ignore speciﬁc information about the available\ncontext, and are thus sub-optimal. The emerging ﬁeld of machine learning for systems attempts\nto learn decision rules with machine learning algorithms. In the database community, many recent\nproposals have been made to improve selectivity estimation with batch machine learning methods.\nSuch methods are all batch methods which require retraining and cannot handle concept drift, such as\nworkload changes and schema modiﬁcations. We present online machine learning as an alternative\napproach. Online models learn on the ﬂy and do not require storing data, they are more lightweight\nthan batch models, and ﬁnally may adapt to concept drift. As an experiment, we teach models to\nimprove the selectivity estimates made by PostgreSQL’s cost model. Our experiments make the case\nthat simple online models are able to compete with a recently proposed deep learning method.\nKeywords Query optimisation \u0001Cost model\u0001Selectivity estimation \u0001Online machine learning \u0001Concept drift\n1 Introduction\nHeuristics are aplenty throughout computer systems. Applications possess many parameters that can be tuned in order\nto improve a measure of performance. For instance, page sizes in relational databases [ 1], cache sizes in storage\nsystems [ 2], and backoff amounts for re-transmits in networking [ 3], are all parameters that are chosen somewhat\nheuristically. A signiﬁcant amount of work has explored the idea of replacing these heuristics with rules found by\na machine learning algorithm. For instance, [ 4] proposed a reinforcement learning method for optimising device\nplacement in a heterogeneous distributed environment which outperformed human experts. Meanwhile the authors\nof [5] proposed to replace index structures such as B-trees with what they call learned indexes . What ties all these\nproposals together is that they each use machine learning to replace a heuristic method. Whereas heuristics are chosen\nbecause they work well on average, a machine learning algorithm can learn to use the available context of every speciﬁc\ncase, and may thus outperform a heuristic. Moreover, a machine learning algorithm can improve through time and\nexploit usage patterns. Another consequence is that the resulting systems may be easier to maintain because they require\nless code written by humans. This has become a prevalent research topic that has been coined machine learning for\nsystems [6].\nThis trend has began to seep into the database query optimisation community. In a relational database, a query optimiser\nis tasked with choosing an execution plan which answers a query in the least amount of time possible. The quality of a\nquery optimiser is mostly based on its ability to predict the cost of each candidate execution plan. The query optimiser\nrelies on a cost model in order to do so. Database cost models have been using heuristics that have evolved ever so\nslightly since the seminal work of [ 7]. Cost estimation – of which selectivity estimation is a sub-task – is therefore\nprone to large mistakes that deteriorate the quality of the query execution plans [ 8]. In recent years, there has been a\nregain in interest from the database community to explore the use of machine learning to improve query optimisation.\nWhile some have taken the extreme approach of learning a query optimiser from scratch [ 9,10], most have focused on\nimproving the accuracy of the cost model. They all follow the same approach, which is to predict the cost of a query\nexecution plan (QEP) by training models on a history past QEPs. Some have focused on solely predicting a query’s\nselectivity [11, 12, 13], while others directly model its cost [14, 15].arXiv:2009.09884v1  [cs.DB]  21 Sep 2020\n\nAPREPRINT - SEPTEMBER 22, 2020\nWe believe that replacing heuristics with machine learning – and in particular deep learning – is a promising solution\ntowards improving cost models. However, we posit that the methods referenced hereabove are ﬂawed in certain ways.\nFirst of all, every proposed method uses a batch perspective, whereby a model is trained on a large history of queries. A\nbatch model is static, and cannot learn from new queries without being retrained from scratch. The second issue is\nthat the proposed methods are not able to handle database schema changes as well as modiﬁcations in access patterns.\nOn the contrary, batch models assume that the queries seen during the ofﬂine training phase are representative of the\nqueries that will be seen during the online prediction phase. However, in practice, attributes and relations can be added,\nupdated, and dropped. Attribute distributions and correlations might also change through time. Additionally, query\nworkloads are susceptible to evolve. We put all of these changes through time under the umbrella term concept drift\n[16]. A batch model is able to handle concept drift by regularly being retrained. Alas, it will essentially be playing a\ncat-and-mouse game, as it is not able to constantly stay up to date. Finally, the recent trend towards deep learning isn’t\nrealistic for efﬁciency reasons; simpler methods are therefore still worth considering.\nWe propose an alternative approach based on online machine learning [17]. Under this regime, a machine learning\nmodel is able to update itself with every incoming observation. An online model has the anytime property , meaning\nthat it can produce predictions at any moment in its lifetime. Moreover, online models can adapt to concept drift. To\nshowcase the beneﬁts of online machine learning, our work in this paper focuses on the task of selectivity estimation.\nSpeciﬁcally, our goal is to correct an existing selectivity estimation module by learning to predict the mistakes it will\nmake. Our method can therefore be plugged into any selectivity estimation module. Our contributions are as follows: 1)\nwe formulate selectivity estimation as an online machine learning problem and enumerate the resulting advantages over\na batch learning approach, 2) we demonstrate how a small set of features as well as a simple model are as accurate\nand more efﬁcient than MSCN [ 12], which is a batch model based on neural networks, and 3) we show that an online\nmachine learning model is able to cope with concept drift and may thus outperform a batch model as time goes on.\n2 Background and related work\n2.1 Selectivity estimation\nWhenever a user issues a query to a database, the query optimiser is tasked with ﬁnding the optimal query execution plan\n(QEP). The total query time, as perceived by the user, is the sum of the query optimisation time and the query execution\ntime. The query optimiser thus has to compromise between ﬁnding an efﬁcient QEP and not spending too much time\ndoing so. To choose a QEP, the query optimiser enumerates a set of candidate plans, estimates the cost of each plan,\nand picks the one with the lowest cost. A QEP is a tree of physical operators (e.g. joins, scans, ﬁlters). Therefore, in\na single processor environment with no communication cost, the cost of a QEP is the sum of the cost of each of its\noperators. The cost of a physical operator is largely determined by the amount of tuples that ﬂow from the operators that\nprecede it. This amount is called the selectivity . To this end, the cost model has access to metadata that summarises the\ndistribution of the data. The issue is that storing a distribution of multiple attributes takes an amount of space that grows\nexponentially with the number of attributes. In practice, the attribute value independence (A VI) assumption is made,\nwhich enables the cost model to assume that attributes are independent with each other. Therefore, the cost model only\nhas access to individual attribute distributions. The cost model makes other simplifying assumptions, such as the join\nuniformity assumption , which states that attribute value distributions are preserved following a relational join.\nSelectivity estimation is considered to be the most important part of the cost model [ 18,8]. Alas, the simplifying\nassumptions that are usually made lead to estimates that are off by several orders of magnitude. Indeed, estimation errors\nmade early on in the QEP can grow exponentially and have devastating consequences further up the plan [ 19]. In turn,\nsaid errors will deteriorate the accuracy of the cost model [ 20], which in turn worsens the query optimiser’s performance.\nHowever, these assumptions allow the selectivity estimation process to run in a very short amount of time, which is\nof paramount importance. Therefore, a lot of research has delved into relaxing said assumptions, at as little a cost as\npossible. A ﬁrst area of focus has been on improving the accuracy of the one-dimensional attribute value distributions,\ni.e.attribute level synopses . In database cost models, histograms are ubiquitous [ 21,22]. Estimating the distribution\nof a single attribute is essentially a solved problem. However, attribute level synopses do not help whatsoever in\ncapturing dependencies between attributes, which are the Achilles Heel of cost models [ 20]. Consequently, table-level\nsynopses have been proposed. This includes extensions of histograms to multiple dimensions [ 23,24], as well as\nBayesian networks [25, 26]. Additionally, signiﬁcant efforts have been made into developing sampling methods, both\nfor single relations [ 27] and multiple relations [ 28]. However, apart from in-memory databases, sampling imposes a\nhigh computational cost and in general suffers from the empty-join problem [29].\nSelectivity estimation is a difﬁcult problem. The simplifying assumptions made by the cost model are difﬁcult to relax\nwithout storing an inordinate amount of metadata. Cost models sometimes use heuristic formulas in order to soften\nthese assumptions without introducing any additional complexity. These heuristics are intended to work in general\n2\n\nAPREPRINT - SEPTEMBER 22, 2020\ncases, but have no guarantee of being optimal for a given database and a particular workload. As an alternative, a\ngrowing body of research has explored the idea of learning to predict the selectivity estimation based on past QEP\nexecution feedback and contextual information.\n2.2 Learning to estimate selectivities\nOnce a QEP has been picked by the query optimiser, it is executed and the results are delivered to the user. When the\nexecution is ﬁnished, the true selectivity at each stage of the QEP is made available. Therefore, it is possible to measure\nthe error of each selectivity estimate every time a query is executed. Early on, the authors of [ 30] proposed to exploit the\nfeedback to tune the parameters of both univariate and multivariate query-driven histograms . Meanwhile, [ 31] were the\nﬁrst to propose an adaptive cost model which can learn from feedback for any kind of QEP. Their system, named LEO –\nfor “LEarning Optimizer” – is based on the simple idea of memorising the exact selectivities of frequently occurring\noperators. Their approach can thus be resumed with the saying “once bitten, twice shy”. Naturally, a memorising\napproach such as LEO does not help whatsoever when having to determine the selectivity of unseen queries.\nThe issue with LEO is that it uses the raw representation of a QEP. If a new QEP differs in any way from each memorised\nQEP, then LEO is essentially blind. A more sophisticated approach is to project QEPs into a new domain wherein\nsimilar QEPs have similar traits. In practical terms, the idea is to represent a QEP by a set of features . The goal is to\ndeﬁne features that are correlated with the query selectivities. A machine learning algorithm can then be taught to map\nthe features to the selectivities. In the case of selectivity estimation, linear models have been proposed [ 11], as well as\nsupport vector machines (SVM) [ 14], and neural networks [ 32]. Independently of the model choice, the challenge is\nto ﬁnd meaningful features which help in accurately predicting the selectivity of a QEP. Historically, designing good\nfeatures for machine learning purposes has been done manually by domain experts.\nIn recent times, there has been a surge of proposals that aim to automatically learn these feature via a neural network.\nThe authors of [ 12] introduced MSCN , which is a convolutional neural network (CNN) that takes as input a one-hot\nencoded version of a QEP. That is, they convert an input query to a set of three vector of 1s and 0s. Each vector\nindicates the presence (and absence) of each existing attribute, join key, and relational operator. They then stack a few\nlinear layers with rectiﬁed linear units and train the neural network to map the binary vectors to selectivities. In their\nexperiments, they manage to outperform the accuracy of PostgreSQL’s cost model by a factor of 3. In order to lower the\nhigh computational cost required by their method, [ 13] explored the idea of breaking down their single neural network\ninto many smaller ones. Each sub-network is in charge of capturing the dependencies at speciﬁc parts of the database\nschema. Indeed, they surmise that attribute dependencies between distant relations are less important than between\nadjacent relations. Other methods based on deep learning have also been proposed. For instance, [ 33] have proposed a\nspeciﬁc neural network designed to handle range predicates over numerical variables.\nOverall, learning to estimate selectivities seems to be a practical method with promising results. Machine learning\nmethods seem to regularly outperform established methods, at least according to recent research papers. This is no\nsurprise, as established methods are traditionally simplistic. The reasons why they are used in the ﬁrst place is because\nthey are very efﬁcient and do not require an expensive learning phase. Meanwhile, machine learning models imply\nheavier algorithms that require a learning phase where all the training data has to be available. Moreover, batch models\ncannot be dynamically updated. For instance, the one-hot encoding schemes from [ 12] and [ 13] cannot cope when\nan attribute is added to a relation. Finally, batch models cannot deal with concept drift. For instance, they do not\naccount for distribution modiﬁcations tuples are inserted or removed from the database, and cannot adapt when the\nquery workload evolves. Therefore, there is some room for improvement in being able to handle the never-ending\nstream of incoming queries. Because of the streaming nature of the problem, we believe that using an online model\nwhich can handle such a stream is a more adequate solution. Models that can do so are part of a sub-ﬁeld of machine\nlearning named online machine learning . We will now give an overview of the latter, before framing it in the case of\nselectivity estimation.\n3 Methodology\n3.1 The beneﬁts of online machine learning\nMachine learning involves teaching a computer program to learn to perform a task by showing it examples. In a\nregression task, such as selectivity estimation, the computer program has to learn a function f: I Rp!I R. That is, for a\ngiven set of pfeaturesx, the program is capable to output a numeric prediction ^y=f(x). The performance of fcan\nbe measured by comparing the prediction ^ywith the true outcome y, which, in a live environment, is unknown at the\ntime of making the prediction. Machine learning models traditionally operate in a batch setting, whereby a bunch of\nnfeature setsX=fx1;:::;xngandnknown outcomes Y=fy1;:::;yngare available. During the training phase,\n3\n\nAPREPRINT - SEPTEMBER 22, 2020\nthe program learns a function ffrom the so-called training set (X;Y ). During the prediction phase, the function fis\napplied to new sets of features. It is implicitly assumed that the training set and the test set possess the same statistical\ndistributions, which isn’t necessarily the case in practice. The main issue with a batch learning model is that it has to be\nretrained from scratch in order to learn from new data. Indeed, the predictive function fproduced by a batch learner\nis set in stone and cannot be updated incrementally with a new observation (x;y). However, in a live environment, a\nnever-ending stream of training data is constantly being produced. In such a scenario, fis immediately sub-optimal\nbecause it isn’t exploiting the new observations. This is even more so the case when concept drift occurs, which implies\nthat the non-stationary condition of the data distribution is being violated. In this case, the model’s performance is\nsusceptible to plummet. In practice, batch learning models are periodically retrained. The choice of the period between\ntraining sessions is a compromise between the cost of the training and the regret in terms of missed opportunity. Each\nof the methods mentioned in subsection 2.2 follow this paradigm.\nIn contrast, an online machine learning model can be updated every time a new observation arrives. It also has the\nanytime property, meaning that it can make predictions at any moment in its lifetime. An observation can be discarded\nonce it has been used to update f. This allows not having to store a training set, which simpliﬁes things from a data\nengineering point of view. Moreover, an online learning model is dynamic , in that the number of features can be altered\non-the-ﬂy. Indeed, an online learning model is resilient and will still work if features are added or are not available.\nThirdly, an online model is capable of being adaptive , meaning that it can adjust itself when concept drift occurs.\nLast but not least, the performance of the model can be measured in real-time. Indeed, a performance metric can be\nmaintained by comparing the model’s prediction with the ground truth once it is available, before updating the model.\nThis is called progressive validation [34] and allows all the data to be used both as a training set as well as an evaluation\nset. Many approaches have been proposed for online machine learning, including partition trees and linear models.\nWe refer readers to [ 17] and references therein for a comprehensive review. Online machine learning is a strong ﬁt\nfor selectivity estimation because of the streaming aspect of the problem. We will now formalise our approach and\ndescribe our methodology. We will then explain how we convert QEPs to features and provide an overview of the online\nmachine learning models we considered.\n3.2 Learning correction factors\nCost models used in modern databases are known to be highly error-prone. Speciﬁcally, they are susceptible to\nunderestimate selectivities by a signiﬁcant amount [ 20]. Practitioners resort to heuristic rules in order to alleviate this\nissue without introducing too much computational burden. For instance, take the case where multiple WHERE predicates\noccur on a single relation. The simple way is to estimate the selectivity of every predicate separately and multiply them\nwith each other afterwards. This approach is naive, as it is based on the attribute value independence assumption. In\norder to soften the latter, one of SQL Server’s heuristics is instead to take the minimum of the selectivities. By doing so,\nSQL Server assumes a worst-case scenario whereby the predicates are perfectly correlated. This heuristic is justiﬁed\nbecause of the fact that, supposedly, it works well for the workloads of SQL Server’s user base.\nWe can introduce a simple heuristic to ﬁx the underestimation of a cost model. We denote by ythe true selectivity,\nwhich is available once the query has been successfully executed. Meanwhile, ^ydenotes the estimated selectivity. For a\nworkload of nqueries, the average underestimation amount, which we denote by c, is equal to1\nnPn\ni=1yi\n^yi. We can\nreduce the amount of underestimation for future queries by multiplying their associated selectivities by c. The latter\ntherefore acts as a correction factor which can be updated every a query is processed. This heuristic rule can be reﬁned\nby increasing its granularity. For instance, we can segment queries according to the number of joins they require and\nkeep track of the multiplicative error made within each segment. Therefore, instead of having a single correction factor\nc, we would have one correction factor cjfor each number of joins j. A natural extension is to learn to predict the\ncorrection factor based on the characteristics of each particular query execution plan. This is a straightforward case of\nsupervised regression learning which can be done in an online manner.\n3.3 Extracting useful features from a plan\nA query execution plan as such cannot be processed by a machine learning model. Indeed, most models are designed\nto process numeric features. As per usual in machine learning, the goal is to ﬁnd features that are correlated with\nthe output and uncorrelated with each other. The key advantage of deep learning methods is that they do not have to\nmanually deﬁne features. Instead, they learn features by starting off from the one-hot encoded representation of the\nQEPs. As we explain in the next subsection, this is akin to using a factorisation machine [ 35]. Our feature extraction\nprocess is based on the observation that there are two types of QEPs we may encounter. On the one hand, a QEP might\nalready have been seen in the past. Indeed, it is quite common for queries to be repeated by users. Moreover, parts of\neach QEP might have been encountered in the context of other queries. This redundancy is a low-hanging fruit that is\n4\n\nAPREPRINT - SEPTEMBER 22, 2020\nthe basis of DB2’s LEO optimiser [ 31]. On the other hand, some QEPs might never have been seen before. In this case,\nmemory-based approaches such as LEO are useless. Because of this duality, we deﬁned two sets of features.\nThe ﬁrst set of features is meant to address QEPs which have never been seen before. In this case we record general\ninformation: 1) the number of joins, 2) the number of involved relations 3) the number of WHERE statements, 4) the\nmaximum number of WHERE clauses on a single relation. The second set of features is aimed at exploiting redundant\nQEPs. We calculate average selectivity estimation errors by grouping over various information from each QEP. In\nmachine learning, this is referred to as target encoding [36]. The idea is to extract information such as the relations,\njoins, attributes, attribute values, and replace them by the average of the selectivity estimation errors encountered in\nthe past. For example, if a QEP contains a ﬁlter condition on a country attribute, then we would look at the average\nselectivity estimation error of queries which involved it. We can also capture dependencies by target encoding on\ncombinations, such as on pairs of attributes. This can be done in a streaming fashion by noticing that an average can be\nupdated online: \u0016xi+1= \u0016xi+x\u0000\u0016xi\ni+1where \u0016xiis the current average and xis the new value (in our case, a selectivity).\nThe main issue of target encoding is that its outputs are somewhat unreliable early on. Indeed, if a particular attribute\nhas only been used in one particular QEP before, then the error that was made for that QEP will be the only one included\nin the average. To alleviate this issue, one may use a Bayesian average, which requires choosing a prior value and a\nweighting term in order to compute a weighted average between the observed average and the prior value.\n3.4 Choice of online learning models\nThe most common approach to online machine learning is to use differentiable models that minimise an empirical loss\nfunction. For instance, a common objective in regression is to ﬁnd the parameters \u0012of a modelmwhich minimise the\nsquared loss between the training set outputs yand the predicted outputs ^y=m(x)made over the training set inputs x:\nmin\n\u0012nX\ni=1(yi\u0000m(xi))2(1)\nIfmcan be differentiated with respect to its inputs, then for each pair (xi;yi)a gradientgican be obtained. The\ngradient can then be used to update \u0012. This is called stochastic gradient descent [37] as only one pair is being processed\nat a time. In the case of a linear regression, \u0012will correspond to the weights assigned to each input xi. Meanwhile in\nthe case of a neural network, \u0012corresponds to the weights and biases of each neuron at each layer.\nStochastic gradient descent comes in different ﬂavors. The simplest variant is to multiply the gradient by a learning\nrate\u0011and subtract the result from \u0012. The gradient provides the update direction as well as the its magnitude, whilst\nthe learning rate acts as a regulariser for the magnitude. In order to obtain convergence, a common trick is to deﬁne a\nschedule which reduces the learning rate as time goes by [ 37]. Although learning rate schedules are commonly used\nin practice, they are ill-suited for online learning. Indeed, if we expect to observe drift, then lowering the learning\nrate means that the model will become less capable of adapting. In our experiments we thus decided to use a constant\nlearning rate. We refer to [ 38] for an overview of more sophisticated ﬂavors. Note that we considered Online Newton\nStep (ONS) [ 39], which is a second-order optimisation method that approximates the Hessian of the loss function, but\ndiscarded it because of speed concerns.\nMany models can be trained with stochastic gradient descent. The simplest one is linear regression, which is just a dot\nproduct between a set of pweightswiand a set ofpinput features xi:^yi=Pp\nj=1wijxij. Although linear regression is\na simplistic method, it has the advantage of being fast and interpretable. Neural networks (including deep learning) can\nalso be trained via stochastic gradient descent, and in fact almost always are. In our experiments, we only considered the\nuse of standard feed-forward networks. These can be seen as a succession of dot-products finterleaved by non-linear\nactivation functions a(e.g., sigmoid, tanh, ReLU): ^y=a(f(a(f:::a (f(x))))) . In a neural network, the gradient can\nbe obtained by working backwards from the loss function and accumulating the individual derivatives at each step along\nthe way. This process is the well-known back-propagation algorithm. Neural networks may be preferable to linear\nregression because they take into account interactions between features.\nAnother way to take into account feature interactions is to compute polynomial combinations of said features and feed\nthese to a linear regression. For a given parameter k>1, one can thus obtainPk\ni=2pkadditional features, where pis\nthe cardinality of the features. This provides a simple way to give non-linear capacity to a linear model. The downside\nis that the number of additional features can grow extremely large, which in practice is a strong deterrent. Moreover,\nsome feature interactions might occur very rarely, which may lead to uncertain parameter estimation and over-ﬁtting.\nFactorisation machines (FM) [ 35] have been proposed as to circumvent these issues. Instead of explicitly estimating\na parameter for each combination of features, a FM stores latent parameters for each feature. To quote the original\nFM paper, “ instead of using an own model parameter wi;j2R for each interaction, the FM models the interaction by\n5\n\nAPREPRINT - SEPTEMBER 22, 2020\nfactorising it ”. Each feature xjis thus associated with a latent vector vjof lengthk. A latent vector therefore contains\nkvalues which describe the associated feature. Intuitively, features used in the same context, such as nationality\n= Japanese andcountry = Japan will have similar latent vectors. The interaction between two features is then\ncomputed as the dot product between their associated latent vector:\n^yi=w0+pX\nj=1wjxj+pX\nj=1pX\nk=i+1hvj;vkixjxk (2)\nWe can now store k\u0002pweights instead of p2. Even if two features have never observed together in a single observation,\ntheir interaction parameter can be estimated from their respective latent vectors. FMs have been used with great success\nfor high-dimensional problems such as click-through rate prediction [ 40]. They suit the selectivity estimation problem\nbecause of the high number of attributes, values, relations, and joins present in a database schema. In particular, FMs\nare an elegant way to model attribute dependencies.\nDecision trees are another class of models which are able to capture dependencies. They are based on a simple principle,\nwhich is to partition the space of observations into boxes, whereby the observations contained inside each box are\nhomogeneous. The idea is to recursively partition the data by ﬁnding split rules which minimise a heterogeneity\ncriterion, such as entropy for classiﬁcation or mean squared error for regression. Decision trees are usually trained in a\nbatch manner [ 41]. However, there have been a few proposals to train them online. The most established method is\ncalled the Hoeffding tree [ 42]. Because all the data is not available at once, the idea is to maintain summary statistics\nP(xijyik)wherexiare features and yikis a label. The summary statistics are stored inside each leaf of the tree\nand are updated each time an observation is sorted into a leaf. The tree starts off as a single leaf. Every so often, the\nsummary statistics are used to evaluate possible split rules. A leaf is split into two leaves, and thus becomes a branch,\nonce the gain from the heterogeneity criterion surpasses a certain threshold, which is called the Hoeffding bound. A\nHoeffding tree is guaranteed to ﬁnd the same structure as a batch decision tree as long the underlying data distribution\nis stationary.\nFinally, we made a minor contribution by developing our own online machine learning model. We adapted Bayesian\nlinear regression to make it robust to concept drift. In a nutshell, Bayesian linear models attach uncertainty to each\nweight. As more data arrives, they provide a mathematical framework for updating each weight whilst accounting for\nthe current uncertainty and the information brought by the new data. Although sound by design, such models assume a\nstationary environment, and do not cope well with concept drift. To circumvent this, we have introduced a variant to\nBayesian linear regression which is based on exponential moving averages. As far as we are aware, it has not been\nproposed in existing publications. However, the focus of this paper is to motivate the use of online machine learning\nmodels, and not necessarily to propose a state-of-the-art solution. Therefore, we leave the details in appendix A, and\nreserve ourselves the right to explore the matter in further depth in a subsequent publication.\n4 Evaluation\nWe evaluated our approach using the IMDb dataset from the JOB benchmark [ 8]. The IMDb dataset contains real-word\ndata pertaining to the movie industry and contains many correlations (for instance French actors usually play in French\nmovies). It contains 21 relations and weighs 3.6 GB. We simulated query workloads by sampling from the queries\nthat accompany each dataset. In the case of IMDb there are 113 available queries. For each sampled query, we asked\nPostgreSQL to generate an execution plan and sampled sub-plans from each of these execution plans. For each sub-plan,\nwe tasked each machine learning model we benchmarked to predict the correction factor we introduced in subsection\n3.2. We then multiplied each predicted correction factor with the selectivity estimate made by PostgreSQL’s cost model.\nFinally, we calculated the q-error [ 43], which measures the multiplicative difference between predictions and ground\ntruths, and has become the de facto standard for evaluating the quality of selectivity estimation methods.\nWe benchmarked the following online models: 1. Linear regression trained with SGD and a learning rate of 0.1\n2. Hoeffding tree with a patience of 200 and a maximum depth of 5 3. Feed-forward neural network with 2 hidden\nlayers of 30 neurons trained with Adam and a learning rate of 0.01. 4. FM with 10 components trained with SGD and a\nlearning rate of 0.1. We provided it with one-hot encoded versions of the used attributes, attribute values, relations,\nand joins. 5. Bayesian linear regression with \r= 0:7. As a comparison, we included the selectivity estimates made by\nPostgreSQL’s cost model. We also benchmarked the following batch learning methods: 1. Standard linear regression\nﬁtted with maximum likelihood estimation (MLE). 2. LightGBM [ 44] with default parameters, which is probably the\nbest off-the-shelf batch learning algorithm, and therefore provides an interesting reference. 3. MSCN from [ 12], which\nwe trained for 50 epochs.\n6\n\nAPREPRINT - SEPTEMBER 22, 2020\nWe trained each batch method on a 100,000 execution plans prior to conducting the benchmark. Meanwhile, the\nonline methods do not require this warm-up phase because they are trained online. The evaluation phase samples 600k\nquery execution plans and obtains a prediction from each method. In order to simulate concept drift, we clustered the\nqueries into three buckets. Queries within each bucket operate on similar relations and thus possess some similarities.\nIntuitively, a model that is trained on one bucket will have difﬁculties estimating selectivities for queries from other\nbuckets. We swapped buckets after 200k and 400k queries, therefore establishing two hard concept drifts. Figure 1\nshows a running average of the q-errors for each method along time. Online models are represented with dotted lines,\nwhilst solid lines are used for batch models. As can be expected, the batch methods initially outperform the online\nmethods because they have has a warm-up phase. However, in the online learning methods eventually outperform\ntheir batch counterparts, in particular because they are able to adapt to the hard concept drift, except in the case of the\nHoeffding tree. The best performing method is FM. Naturally, batch learning models can be retrained in order to cope\nwith concept drift, but that would require storing observations and deﬁning a training schedule, which isn’t particularly\ntrivial to put in place.\n0 100000 200000 300000 400000 500000 600000\nNumber of processed queries51015202530q-errorAverageq-error per model with hard drift\nBatch linear regression\nBayesian linear regression\nFMHoeﬀding tree\nLightGBM\nMSCNNeural network\nOnline linear regression\nPostgreSQL\nFigure 1:q-errors for each method with hard concept drift\nWe also experimented with slow concept drift. Instead of switching buckets at predetermined moments, we decide\nwhich bucket to sample from in a probabilistic manner. At every moment t, the probability of sampling from bucket bis:\nP(b;t) =exp(\u0000(t\u0000tb)2\nd)\nP\nbiexp(\u0000(t\u0000tbi)2\nd)(3)\nwheretbare predetermined values that we spread out in a uniform manner (i.e. 150k, 300k, 450k respectively for each\nbucket). Meanwhile, ddetermines the abruptness of the drift (i.e. a large dcorresponds to a harder drift). For our\nexperiments we arbitrarily chose d= 3. The results of this simulation are provided in ﬁgure 2. As can be seen, the\nperformance changes are smoother than in the hard drift case, which is possibly more representative of what may occur\nin the real world. The performances of the batch models aren’t good because they have been trained on data from the\nﬁrst bucket. The online models are all able to cope, except for the Hoeffding tree. However, the ranking of the models\nisn’t the same. Indeed, the best performing model is the Bayesian linear regression, whereas the FM model comes\nsecond. This could be due to the fact that it contains more weights that have to be modiﬁed, which is also the case for\nthe neural network.\n7\n\nAPREPRINT - SEPTEMBER 22, 2020\n0 100000 200000 300000 400000 500000 600000\nNumber of processed queries12.515.017.520.022.525.027.530.0q-errorAverageq-error per model with hard drift\nBatch linear regression\nBayesian linear regression\nFMHoeﬀding tree\nLightGBM\nMSCNNeural network\nOnline linear regression\nPostgreSQL\nFigure 2:q-errors for each method with soft concept drift\n5 Conclusion\nComputer systems, including database systems, have to make many choices that affect their performance. These choices\noften involve heuristic decision rules. For instance, database cost models use dampening tricks to soften the attribute\nvalue independence assumption. Said rules are designed to work well on average, but do not exploit the available\ncontext. Providing an automatic and principled way to learn better decision rules for improving computer systems\nis an active area of research. Recently, many have proposed to use supervised learning and reinforcement learning\nas potential solutions, under the umbrella term machine learning for systems . In the query optimisation community,\nall of these proposals function in a batch regime, whereby the model is static and has to be retrained to exploit newly\navailable information. Meanwhile, online machine learning allows to learn from a stream of data, and is thus able to\nremain up-to-date. Indeed, many computer systems are event-based applications, which means that algorithms which\nare able to process event streams might have some edge over static algorithms. This is even more so important in the\ncase where query workloads are modiﬁed or when the underlying data distribution changes.\nAs an example, in this paper we focus on the task of selectivity estimation in database cost models. Selectivity estimation\nis a difﬁcult problem whereby the cost model has to predict how many tuples will be produced a particular query\nexecution plan. There have recently been proposals that explore the use of deep learning to solve it. Said proposals\nfollow the batch paradigm, and do not offer a principled answer to take into account concept drift, such as workload\nmodiﬁcations. Instead, we advocate the use of online machine learning. Model that obey this paradigm learn on the\nﬂy and as such do not have to be retrained from scratch. An added beneﬁt is that they allow handling concept drift.\nMoreover, online models are more lightweight than their batch counterparts, which is of importance in a resource\nintensive environment, such as a database system.\nWe back-up our proposal by experimenting on the JOB benchmark. We simulate a workload by randomly sampling\nquery execution plans from PostgreSQL. We ask models to predict the selectivity of each execution plan and measure\ntheq-error. The online models are updated every time a QEP terminates. Meanwhile, the batch models are warmed-up\nprior to the workload and are thus static. Our experiments show that the online models are competitive with batch\nmodels. Moreover, online models are able to adapt when the query workload changes, be it in a hard or a soft manner.\nOn the whole, online machine learning is a promising approach to improving selectivity estimation, and remains\nsomewhat unexplored. More generally, we believe that online machine learning can and should be used for other\napplications where heuristic rules are applied and feedback is constantly streaming in.\nReferences\n[1]Pinchas Weisberg and Yair Wiseman. Using 4kb page size for virtual memory is obsolete. In 2009 IEEE\nInternational Conference on Information Reuse & Integration , pages 262–265. IEEE, 2009.\n8\n\nAPREPRINT - SEPTEMBER 22, 2020\n[2]Yongseok Oh, Jongmoo Choi, Donghee Lee, and Sam H Noh. Caching less for better performance: balancing\ncache size and update cost of ﬂash memory cache in hybrid storage systems. In FAST , volume 12, 2012.\n[3]Byung-Jae Kwak, Nah-Oak Song, and Leonard E Miller. Performance analysis of exponential backoff. IEEE/ACM\nTransactions on Networking (TON) , 13(2):343–355, 2005.\n[4]Azalia Mirhoseini, Hieu Pham, Quoc V Le, Benoit Steiner, Rasmus Larsen, Yuefeng Zhou, Naveen Kumar,\nMohammad Norouzi, Samy Bengio, and Jeff Dean. Device placement optimization with reinforcement learning.\nInProceedings of the 34th International Conference on Machine Learning-Volume 70 , pages 2430–2439. JMLR.\norg, 2017.\n[5]Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned index structures.\nInProceedings of the 2018 International Conference on Management of Data , pages 489–504. ACM, 2018.\n[6]Jeff Dean. Machine learning for systems and systems for machine learning. In Presentation at 2017 Conference\non Neural Information Processing Systems , 2017.\n[7]P Grifﬁths Selinger, Morton M Astrahan, Donald D Chamberlin, Raymond A Lorie, and Thomas G Price.\nAccess path selection in a relational database management system. In Proceedings of the 1979 ACM SIGMOD\ninternational conference on Management of data , pages 23–34. ACM, 1979.\n[8]Viktor Leis, Andrey Gubichev, Atanas Mirchev, Peter Boncz, Alfons Kemper, and Thomas Neumann. How good\nare query optimizers, really? Proceedings of the VLDB Endowment , 9(3):204–215, 2015.\n[9]Jennifer Ortiz, Magdalena Balazinska, Johannes Gehrke, and S Sathiya Keerthi. Learning state representations\nfor query optimization with deep reinforcement learning. In Proceedings of the Second Workshop on Data\nManagement for End-To-End Machine Learning , pages 1–4, 2018.\n[10] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh, Tim Kraska, Olga Papaemmanouil,\nand Nesime Tatbul. Neo: A learned query optimizer. Proceedings of the VLDB Endowment , 12(11):1705–1718,\n2019.\n[11] Chenggang Wu, Alekh Jindal, Saeed Amizadeh, Hiren Patel, Wangchao Le, Shi Qiao, and Sriram Rao. Towards a\nlearning optimizer for shared clouds. Proceedings of the VLDB Endowment , 12(3):210–222, 2018.\n[12] Andreas Kipf, Dimitri V orona, Jonas Müller, Thomas Kipf, Bernhard Radke, Viktor Leis, Peter Boncz, Thomas\nNeumann, and Alfons Kemper. Estimating cardinalities with deep sketches. In Proceedings of the 2019\nInternational Conference on Management of Data , pages 1937–1940, 2019.\n[13] Lucas Woltmann, Claudio Hartmann, Maik Thiele, Dirk Habich, and Wolfgang Lehner. Cardinality estimation\nwith local deep learning models. In Proceedings of the Second International Workshop on Exploiting Artiﬁcial\nIntelligence Techniques for Data Management , page 5. ACM, 2019.\n[14] Mert Akdere, Ugur Çetintemel, Matteo Riondato, Eli Upfal, and Stanley B Zdonik. Learning-based query\nperformance modeling and prediction. In 2012 IEEE 28th International Conference on Data Engineering , pages\n390–401. IEEE, 2012.\n[15] Ryan Marcus and Olga Papaemmanouil. Plan-structured deep neural network models for query performance\nprediction. Proceedings of the VLDB Endowment , 12(11):1733–1746, 2019.\n[16] João Gama, Indr ˙e Žliobait ˙e, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid Bouchachia. A survey on concept\ndrift adaptation. ACM computing surveys (CSUR) , 46(4):44, 2014.\n[17] Heitor Murilo Gomes, Jesse Read, Albert Bifet, Jean Paul Barddal, and João Gama. Machine learning for\nstreaming data: State of the art, challenges, and opportunities. SIGKDD Explor. Newsl. , 21(2):6–22, November\n2019.\n[18] Zhongxian Gu, Mohamed A Soliman, and Florian M Waas. Testing the accuracy of query optimizers. In\nProceedings of the Fifth International Workshop on Testing Database Systems , pages 1–6, 2012.\n[19] Yannis E Ioannidis and Stavros Christodoulakis. On the propagation of errors in the size of join results , volume 20.\nACM, 1991.\n[20] Guy Lohman. Is query optimization a “solved” problem. In Proc. Workshop on Database Query Optimization ,\nvolume 13. Oregon Graduate Center Comp. Sci. Tech. Rep, 2014.\n[21] Viswanath Poosala, Peter J Haas, Yannis E Ioannidis, and Eugene J Shekita. Improved histograms for selectivity\nestimation of range predicates. In ACM Sigmod Record , volume 25, pages 294–305. ACM, 1996.\n[22] Yannis Ioannidis. The history of histograms (abridged). In Proceedings 2003 VLDB Conference , pages 19–30.\nElsevier, 2003.\n9\n\nAPREPRINT - SEPTEMBER 22, 2020\n[23] M Muralikrishna and David J DeWitt. Equi-depth multidimensional histograms. In ACM SIGMOD Record ,\nvolume 17, pages 28–36. ACM, 1988.\n[24] Nicolas Bruno, Surajit Chaudhuri, and Luis Gravano. Stholes: a multidimensional workload-aware histogram. In\nAcm Sigmod Record , volume 30, pages 211–222. ACM, 2001.\n[25] Kostas Tzoumas, Amol Deshpande, and Christian S Jensen. Lightweight graphical models for selectivity estimation\nwithout independence assumptions. Proceedings of the VLDB Endowment , 4(11):852–863, 2011.\n[26] Max Halford, Philippe Saint-Pierre, and Franck Morvan. An approach based on bayesian networks for query\nselectivity estimation. In International Conference on Database Systems for Advanced Applications , pages 3–19.\nSpringer, 2019.\n[27] Matteo Riondato, Mert Akdere, U ˇgur Çetintemel, Stanley B Zdonik, and Eli Upfal. The vc-dimension of sql\nqueries and selectivity estimation through sampling. In Joint European Conference on Machine Learning and\nKnowledge Discovery in Databases , pages 661–676. Springer, 2011.\n[28] David Vengerov, Andre Cavalheiro Menck, Mohamed Zait, and Sunil P Chakkappen. Join size estimation subject\nto ﬁlter conditions. Proceedings of the VLDB Endowment , 8(12):1530–1541, 2015.\n[29] Surajit Chaudhuri, Rajeev Motwani, and Vivek Narasayya. On random sampling over joins. In ACM SIGMOD\nRecord , volume 28, pages 263–274. ACM, 1999.\n[30] Ashraf Aboulnaga and Surajit Chaudhuri. Self-tuning histograms: Building histograms without looking at data.\nACM SIGMOD Record , 28(2):181–192, 1999.\n[31] Michael Stillger, Guy M Lohman, V olker Markl, and Mokhtar Kandil. Leo-db2’s learning optimizer. In VLDB ,\nvolume 1, pages 19–28, 2001.\n[32] Henry Liu, Mingbin Xu, Ziting Yu, Vincent Corvinelli, and Calisto Zuzarte. Cardinality estimation using neural\nnetworks. In Proceedings of the 25th Annual International Conference on Computer Science and Software\nEngineering , pages 53–59. IBM Corp., 2015.\n[33] Anshuman Dutt, Chi Wang, Azade Nazi, Srikanth Kandula, Vivek Narasayya, and Surajit Chaudhuri. Selectivity\nestimation for range predicates using lightweight models. Proceedings of the VLDB Endowment , 12(9):1044–1057,\n2019.\n[34] Avrim Blum, Adam Kalai, and John Langford. Beating the hold-out: Bounds for k-fold and progressive cross-\nvalidation. In COLT , volume 99, pages 203–208, 1999.\n[35] Steffen Rendle. Factorization machines. In 2010 IEEE International Conference on Data Mining , pages 995–1000.\nIEEE, 2010.\n[36] Daniele Micci-Barreca. A preprocessing scheme for high-cardinality categorical attributes in classiﬁcation and\nprediction problems. ACM SIGKDD Explorations Newsletter , 3(1):27–32, 2001.\n[37] Léon Bottou. Stochastic gradient descent tricks. In Neural networks: Tricks of the trade , pages 421–436. Springer,\n2012.\n[38] Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747 ,\n2016.\n[39] Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization.\nMachine Learning , 69(2-3):169–192, 2007.\n[40] Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun, and Quan Lu. Field-weighted\nfactorization machines for click-through rate prediction in display advertising. In Proceedings of the 2018 World\nWide Web Conference , pages 1349–1357, 2018.\n[41] Leo Breiman. Classiﬁcation and regression trees . Routledge, 2017.\n[42] Pedro Domingos and Geoff Hulten. Mining high-speed data streams. In Kdd, volume 2, page 4, 2000.\n[43] Guido Moerkotte, Thomas Neumann, and Gabriele Steidl. Preventing bad plans by bounding the impact of\ncardinality estimation errors. Proceedings of the VLDB Endowment , 2(1):982–993, 2009.\n[44] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu.\nLightgbm: A highly efﬁcient gradient boosting decision tree. In Advances in Neural Information Processing\nSystems , pages 3146–3154, 2017.\n10\n\nAPREPRINT - SEPTEMBER 22, 2020\nA Drift-resilient Bayesian linear regression\nThe emphasis of this paper isn’t so much on the choice of the online machine learning models as much as this is on the\nbasic principle of using online machine learning. Therefore, we have resorted to using methods that are established in\nthe statistical learning community. However, we also stumbled on a twist to Bayesian linear regression which provided\nus with good experimental results. Bayesian modeling is a framework for mixing prior knowledge with observed\nevidence. In the case of linear regression, we can impose a prior distribution on the weights. We can denote this prior\ndistribution as p(\u0012). A typical parametrisation choice is to use a multivariate normal distribution centered in 0. In\nsuch a case, it may seem at ﬁrst that the prior is uninformative because it is vague and doesn’t contain any subjective\ninformation. However, this prior becomes very useful in a streaming context.\nA Bayesian model can be updated with observed samples. The goal is to adjust the parameters of the model according to\nthe observed information, whilst taking into account the current knowledge. In some sense, this goal is shared with that\nof online machine learning. The advantage of Bayesian modeling is that it offers update formulas which are consistent\nwith the rules of probability, and are in fact optimal under the latter. Given a new sample (xt;yt), Bayesian modeling\ngives us a mechanism for obtaining a new parameter distribution p(\u0012t+1j\u0012t;xt;yt), which is therefore conditioned on\nthe current distribution and the new sample. This probability distribution is obtained via Bayes’ rule, as so:\np(\u0012t+1j\u0012t;xt;yt) =p(ytjxt;\u0012t)\u0002p(\u0012t)\np(\u0012t;xt;yt)(4)\nThe left-hand side of the numerator is the likelihood of observing ytgiven the current parameter distribution \u0012tand the\nfeaturesxt. The right-hand side is the current parameter distribution. The denominator is the distribution, which isn’t in\nfact known. However, because it isn’t dependent on \u0012, it can be simpliﬁed depending on the chosen parametrisation.\nIn fact, the mathematical details work nicely when the prior distribution and the likelihood are said to be conjugate .\nThe latter is a mathematical term that describes the fact that two distributions can be fused into a new distribution\nwith updated parameters. When this isn’t the case, then one has to resort to approximate Bayesian inference, which\nis beyond the scope of this discussion. One way to see it is that we are interested in the “old-school” way of doing\nBayesian modeling, whereby distributions are conjugate to each other, which leads to analytical formulas that are well\nsuited to online machine learning. To keep things general, we will simply write down:\np(\u0012t+1j\u0012t;xt;yt)/p(ytjxt;\u0012t)p(\u0012t) (5)\nThe previous statement simply expresses the fact that the posterior distribution of the model parameters is proportional\nto the product of the likelihood and the prior distribution. In other words, in can be obtained using an analytical formula\nthat is speciﬁc to the chosen likelihood and prior distribution. If we’re being pragmatic, then what we’re really interested\nin is to obtain the predictive distribution , which is obtained by marginalising over the model parameters \u0012t:\np(ytjxt) =Z\np(ytjw;xt)p(w)dw (6)\nAgain, this isn’t analytically tractable, except if the likelihood and the prior are conjugate to each other. The equation\ndoes make sense though, because it expressed the fact we’re computing a weighted average of the potential yivalues\nfor each possible model parameter w, therefore accounting for our uncertainty in the weight parameters.\np(ytjxt)/p(ytjxt;\u0012t)p(\u0012t) (7)\nIn short, the predictive distribution can be obtained by mixing the predictive distribution and the current parameter\ndistribution. Again, this isn’t analytically tractable, except if the likelihood and the prior are conjugate to each other.\nFor the purpose of online machine learning, what matters is that we can update the distribution of the parameters when\na new pair (xt;yt)arrives:\np(\u0012t+1j\u0012t;xt;yt)/p(xt;ytj\u0012t)p(\u0012t) (8)\nBefore any data comes in, the model parameters follow the initial distribution we picked, which is p(\u00120). At this point,\nif we’re asked to predict y0, then it’s predictive distribution would be obtained as so:\np(y0jx0)/p(y0jx0;\u00120)p(\u00120) (9)\n11\n\nAPREPRINT - SEPTEMBER 22, 2020\nNext, once the ﬁrst observation (x0;y0)arrives, we can update the distribution of the parameters:\np(\u00121j\u00120;x0;y0)/p(x0;y0j\u00120)p(\u00120) (10)\nThe predictive distribution, given a set of features x1, is thus:\np(y1jx1)/p(y1jx1;\u00121)p(\u00121j\u00120;x0;y0)|{z }\np(\u00121)(11)\nThe previous equations expresses the fact that the prior of the weights for the current iteration is the posterior of the\nweights at the previous iteration. Once the second pair (x1;y1)is available, the distribution of the model parameters is\nupdated in the same way as before:\np(\u00122j\u00121;x1;y1)/p(y1jx1;\u00121)p(y0jx0;\u00120)p(\u00120)| {z }\np(\u00121)(12)\nWhen the pair (x2;y2)arrives, the distribution of the weights can be obtained once again:\np(\u00123j\u00122;x2;y2)/p(y2jx2;\u00122)p(y1jx1;\u00121)p(y0jx0;\u00120)p(\u00120)| {z }\np(\u00121)| {z }\np(\u00122)(13)\nBy now, it might be clear that there is recursive relationship that links each iteration: the posterior distribution at step\ntbecomes the prior distribution at step t+ 1. This simple fact is the reason why analytical Bayesian inference can\nnaturally be used as an online machine learning algorithm. Indeed, we only need to store the current distribution of the\nweights to make everything work.\nUp until now we didn’t give any useful example. We will now see how to perform linear regression by using Bayesian\ninference. In a linear regression, the model parameters \u0012tare just weights wtthat are linearly applied to a set of features\nxt:\nyt=wtx|\nt+\u000ft (14)\nEach prediction is the scalar product between pfeaturesxtandpweightswt. The trick here is that we’re going to\nassume that the noise \u000fifollows a given distribution. In particular, we will be boring and use the Gaussian ansatz, which\nimplies that the likelihood function is a Gaussian distribution:\np(ytjxt;wt) =N(wtx|\nt;\f\u00001) (15)\nChristopher Bishop calls \fthe “noise precision parameter”. In statistics, the precision is inversely related to the noise\nvariance as so: \f=1\n\u001b2. Basically, it translates our belief on how noisy the target distribution is. Both concepts coexist\nmostly because statisticians can’t agree on a common Bible. There are ways to tune this parameter automatically\nfrom the data, however for the sake of simplicity we will treat it as known constant. In any case, the appropriate prior\ndistribution for the above likelihood function is the multivariate Gaussian distribution:\np(w0) =N(m0;S0) (16)\nm0is the mean vector of the distribution while S0is its covariance matrix. Initially, their initial values will be:\nm0= (0;:::; 0) (17)\nS0=0\n@\u000b\u00001::: :::\n::: \u000b\u00001:::\n::: ::: \u000b\u000011\nA (18)\n12\n\nAPREPRINT - SEPTEMBER 22, 2020\nThe value\u000bis a hyperparameter that needs to be provided. From our experience, its inﬂuence is very small in an online\nscenario and therefore its value does not matter very much. We can now determine the posterior distribution of the\nweights:\np(wt+1jwt;xt;yt) =N(mt+1;St+1) (19)\nSt+1= (S\u00001\nt+\fx|\ntxt)\u00001(20)\nmt+1=St+1(S\u00001\ntmt+\fxtyt) (21)\nNote thatx|\ntxtis the outer product of xtwith itself. There are also a set of formulas that can be used to obtain the\npredictive distribution:\np(yt) =N(\u0016t;\u001bt) (22)\n\u0016t=wtx|\nt (23)\n\u001bt=1\n\f+xtStx|\nt (24)\nAll of the above formulas are quite common and can be found in many introductions to Bayesian inference. One of the\nissues with this formulation is that the data is assumed to be stationary. Indeed, the more data we show the model, the\nmore it will be conﬁdent about its parameter estimate. However, we could like to it to be able to be robust to concept\ndrift by forgetting the past and focusing on recent data. The solution we found was to change the update formulas of the\ncovariance matrix and the mean vector in the following manner:\nSt+1= (\rS\u00001\nt+ (1\u0000\r)\fx|\ntxt)\u00001(25)\nmt+1=St+1(\rS\u00001\ntmt+ (1\u0000\r)\fxtyt) (26)\nIn the above equations, \racts a smoothing parameter which controls how much the model “forgets” its current state\nand sticks to the new data. In the case where \r= 1, the model doesn’t learn and sticks to the prior distribution. On the\ncontrary, when \r= 0, the model memorises the latest sample and forgets what it has seen up to there. These formulas\nare very much heuristic and we have not taken the time to give them a thorough analytical treatment. As far as we can\ntell, they haven’t been used in published literature. However, they are not complex and resemble exponential weighted\nmoving averages. In practice, we have found that this formulation provides a robust off-the-shelf algorithm that works\nwell on average for many problems where concept drift occurs. In our experience, the importance of choosing a suitable\n\rdoesn’t matter very much. Indeed, we advise using \r= 0:7by default.\n13",
  "textLength": 56997
}