{
  "paperId": "1b28a7e49f718e5b5612c8fb28096bab0d69ebbd",
  "title": "Memory-Efficient RkNN Retrieval by Nonlinear k-Distance Approximation",
  "pdfPath": "1b28a7e49f718e5b5612c8fb28096bab0d69ebbd.pdf",
  "text": "Memory-Efﬁcient RkNN Retrieval by Nonlinear\nk-Distance Approximation\nSandra Obermeier\nLMU Munich\nMunich, Germany\nobermeier@dbs.iﬁ.lmu.deMax Berrendorf\nLMU Munich\nMunich, Germany\nberrendor@dbs.iﬁ.lmu.dePeer Kröger\nLMU Munich\nMunich, Germany\nkroeger@dbs.iﬁ.lmu.de\n50\n100\n150\n200\n250\nk\n0\n250\n500\n750\n1000\n1250\n1500\n1750distanceOur bounds\nReal k-distances\nFig. 1:k-distance (y-axis) for a selected point (red) in a toy\ndataset (small inset) and different values of k(x-axis). The\nk-distances exhibit non-linear behavior with steep increase at\nthe borders of dense regions.\nAbstract —The reverse k-nearest neighbor (RkNN) query is an\nestablished query type with various applications reaching from\nidentifying highly inﬂuential objects over incrementally updating\nkNN graphs to optimizing sensor communication and outlier\ndetection. State-of-the-art solutions exploit that the k-distances\nin real-world datasets often follow the power-law distribution,\nand bound them with linear lines in log-log space. In this work,\nwe investigate this assumption and uncover that it is violated in\nregions of changing density, which we show are typical for real-\nlife datasets. Towards a generic solution, we pose the estimation of\nk-distances as a regression problem. Thereby, we enable harness-\ning the power of the abundance of available Machine Learning\nmodels and proﬁting from their advancement. We propose a\nﬂexible approach which allows steering the performance-memory\nconsumption trade-off, and in particular to ﬁnd good solutions\nwith a ﬁxed memory budget crucial in the context of edge\ncomputing. Moreover, we show how to obtain and improve guar-\nanteed bounds essential to exact query processing. In experiments\non real-world datasets, we demonstrate how this framework\ncan signiﬁcantly reduce the index memory consumption, and\nstrongly reduce the candidate set size. We publish our code at\nhttps://github.com/sobermeier/nonlinear-kdist.\nIndex Terms —reverse k-nearest neighbor, index compression,\nedge computing\nI. I NTRODUCTION\nBesides range and k-nearest neighbor (kNN) queries, an-\nother frequently used query type is reverse k-nearest neighbor\n(RkNN) queries given as RkNN (q) =fo2Djq2\nkNN (o)g. Thus, it returns those objects in the databaseD, for which the query is one of the k-nearest neighbors.\nThis task is different to the kNN query, since the k-nearest\nneighbor relation is notsymmetric. Consider a toy dataset\nof numbersf1;2;4gwith the absolute difference as distance.\nThen, 2 is the nearest neighbor of 4, but not vice-versa. As\nnaïvelyO(n)kNN queries are necessary to determine the\nresult, numerous approaches for efﬁcient query processing\nand effective indexing have been proposed [1]–[5]. The query\nresult itself describes the inﬂuence set of an object and is\nhence of interest itself. However, it has further applications in\nﬁelds reaching from outlier detection [6] over clustering [7],\n[8] to incremental updates for kNN query processing in sensor\nnetworks [9]. Existing real-world applications are often limited\nby the given storage capacity, especially when considering\nmodern, embedded systems. State-of-the-art approaches are\noften situated within the ﬁlter-reﬁnement framework. Here,\na fast ﬁlter method is applied which decides for a large\nproportion of the data whether they are certainly part of the\nresult ( safe inclusion ) or certainly not part of the result ( safe\nexclusion ). Afterward, only for the few remaining data points\nfor which no decision could be taken, the expensive forward\nkNN queries are performed. Thus, research focuses mainly\non deriving new efﬁcient and effective ﬁlter methods. In this\nwork, we investigate how to derive ﬁlters using Machine\nLearning models that approximate the k-distances of each\npoint. To this end, we pose the regression problem to predict\nthek-distancenndist (p;k)of a data point given its represen-\ntationp. Moreover, we show how to adapt the principles of\nlearned index structures [10] to derive guaranteed lower and\nupper bounds. In our experiments, we demonstrate how the\napproach can be applied to several real-world datasets reducing\nthe number of candidates as well as the index size.\nIn summary, our contributions are:\n\u000fWe propose a framework to learn the distribution of k-\ndistances as a regression task given an object’s representa-\ntion. By employing powerful Machine Learning models,\nthis framework can model varying densities.\n\u000fWe derive and enhance guaranteed bounds essential for\nthe application to exact query processing within ﬁlter-\nreﬁnement.\n\u000fWe conduct a thorough empirical study to evaluate our\nproposed approach. The results show both a reduction inarXiv:2011.01773v1  [cs.DB]  3 Nov 2020\n\nindex size and in candidate set size.\nThe remainder of this paper is structured as follows: In\nSection II, we review existing work both in the ﬁeld of\nRkNN query processing as well as learned index structures and\nposition our work. In Section III, we formalize the regression\nproblem, and elaborate on how to obtain and improve guar-\nanteed lower and upper bounds for exact query processing, In\nSection IV, we present an extensive experimental evaluation\nanalyzing the trade-off between model size and candidate set\nsizes, as well as performing an ablation study to investigate\nthe effect of individual components. In Section V, we conclude\nthis paper and outline possible directions for future research.\nII. R ELATED WORK\nA. RkNN Query Processing\nIn general, there are static and dynamic RkNN query\nprocessing approaches. While the ﬁrst assume a static set of\nobjects in a database, the latter can cope with incremental\nchanges such as insertion and deletion of data points. Dynamic\napproaches can also be applied to a static database, but they\nmight lose some efﬁciency as they cannot exploit the fact that\nall objects are already known and there will be no changes.\nA common approach to applying static methods in a dynamic\nsetting is to keep an additional database that contains only the\nchanges. Query Processing is then performed on the indexed\ndatabase with index support and in a sequential search in the\ndatabase of changes. From time to time the static index is\nrebuilt to include the changes that happened so far. This is\noften a viable alternative for slowly changing databases. Thus,\nin our work, we focus only on a static environment.\nBesides, we can distinguish two variants of use-cases\nfor RkNN query processing: the mono-chromatic and the\nbi-chromatic case [11]. In the bi-chromatic case, the query\nobjects, and data objects belong to two different categories,\nsuch as service providers and customers. However, in our\nexperiments, we focus on the monochromatic version where\nthe query objects belong to the same category as the data\nobjects.\n1) R-Tree Extensions: The ﬁrst approaches on RkNN query\nprocessing were based on an extension of the R-Tree where\nobjects are saved in an R-Tree in a speciﬁc form.\nThe RNN-Tree [11] is only applicable for RkNN queries\nwithk= 1. Spheres in the form of (p;nndist (p;1))are stored\nin an R-Tree for each data point p2D, with center pand\nradius ofnndist (p;1), i.e. thek-distance for k= 1. A query\npointqonly inﬂuences data points that contain qbecause only\nthese points have qas their nearest neighbor. So for a query q\nthe centerspof all data objects are returned where the circle\n(p;nndist (p;1))containsq. Furthermore, the RNN Tree is\nnot very efﬁcient as there are many leaf accesses required for\nlocating the RNNs.\nThe RdNN-Tree [12] stores extra information about the\nnearest neighbors of the points directly in the R-tree and can\nhandle both NN queries and RNN queries. Still only k= 1\nis supported. Like the RNN-Tree, the leaf nodes store entries\nof the form (p;nndist (p;1)). The inner nodes, however, storean array of branches in the form of (ptr;Rect;max _nndist ).\nptrpoints to another node in the tree, either a leaf or an\ninner node. Rect is the minimum bounding rectangle of all\nelements stored in the sub-tree. max _nndist refers to the\nmaximum k-distance of all points pscontained in the sub-\ntree of this node. An RNN search for a query point qis\ndone as follows: For a leaf node, each point pin the node is\nexamined. If dist(p;q)\u0014nndist (p;1)thenpbelongs to the\nR1NN ofq. For an intermediate node, qis compared to the\nbranches. If dist(q;Rect )>max _nndist then the branch can\nbe pruned because all data points within the sub-tree rooted at\nthe investigated branch have a smaller distance to their nearest\nneighbor.\n2) Filter-Reﬁnement Approaches: Later, more efﬁcient ap-\nproaches based on a ﬁlter reﬁnement architecture were pro-\nposed. RkNN query processing is divided into two phases,\ntheﬁlter and the reﬁnement phase. In the ﬁlter step, we\nrun a fast procedure to exclude as many objects as possible.\nFor exact query processing we must ensure that no object\nwhich is part of the result is discarded. The remaining objects,\ncalled candidates , go through a reﬁnement phase. In the\nreﬁnement phase, for each candidate an expensive kNN query\nis performed to decide whether it is part of the result.\nTPL [13] is an algorithm which is based on the ﬁlter\nreﬁnement architecture. First, the entries of an R-Tree are\ntraversed into a heap ordering them by ascending distance to\nthe query point qbecause RNNs are likely to be near q. In\nthe ﬁlter step, the algorithm goes through the heap and uses\na concept of half-planes to prune data objects that cannot be\ncandidates. Considering the perpendicular bisector between a\nquery point qand an arbitrary data object pthat divides the\ndata space into two half-planes PLq(p;q)containingqand\nPLp(p;q)containingp. Every object, a single data object or\na minimum bounding rectangle containing many data objects,\nthat is in the PLp(p;q)cannot be an RNN of q because it is\ncloser top. This is the basic idea behind the pruning of the\nTPL algorithm. Even though originally it was proposed for\nR1NN queries the paper of [13] also presents a way to extend\nTPL for RkNN queries. The data objects determined as non-\ncandidates are not discarded immediately. They are added to a\nset calledSrfn. Data objects that cannot be pruned are added\nto the candidates set Scnd. In the reﬁnement step entries of\nthisSrfnare used to identify false hits in the Scnd.\nThe MRkNNCoP Tree [1] is another example within the\nﬁlter reﬁnement framework. Lower and upper bounds of\nthe k-distance are used as a ﬁlter. A novel generic index\nfor the RkNN search is proposed based on the ideas of\nRdNN Tree. Instead of an R-Tree, an M-Tree [14] is used\nto generalize from Euclidean vector data to metric objects.\nThe MRkNNCoP Tree introduces another improvement to the\nRdNN Tree by generalizing k. The MRkNNCoP approach\nexploits the observation that the distribution of distances for a\nspeciﬁc point in a natural dataset often follows the power-\nlaw. Thus, for each point, the lower and the upper bound\nof thek-distance are approximated by a linear line in log-\nlog space, thus requiring only two parameters per bound, i.e.\n\nfour parameters per data point. An extended M-Tree is used\nfor aggregating the maximum of all upper bounds and the\nminimum of all lower bounds for each node and all data\nobjects contained in that node.\nB. Learned Index Structure (LIS)\nThe basic idea of Learned Index Structures [10] is that\nindexes can be seen as models. More concrete, the authors\nshow how a B-Tree, a hash map, or a Bloom ﬁlter can be\nmodeled as regression, or classiﬁcation model, either mapping\na key to a position or predict its existence. For example, a\nB-Tree can be considered as a model that maps a key to the\nposition of a speciﬁc value in a key-sorted list. Furthermore, in\nthis setup only the ﬁrst key of a page is indexed for efﬁciency\nreasons. So, it is guaranteed that the key of the record at the\nposition given by the B-Tree is the ﬁrst key equal ( pos+0) or\nhigher (pos+pagesize ) than the look-up key. Hence, the B-\nTree is a model with a guaranteed minimal and maximal error.\nWithin a static setting, the minimal error and maximal error of\na prediction can also be guaranteed and thus, also the range\nof a predicted position. The minimal position is pos\u0000\u0001#\nwhere \u0001#is the minimal difference between the prediction\nand the true position and the maximal position is determined\nbypos+ \u0001\"where \u0001\"is the maximum difference between\nthe prediction and the true position. The main advantage of\nlearned indexes is that the cost of a lookup operation is reduced\nfromO(logn)toO(1), and the storage costs are reduced from\nO(n)toO(1).\n[15], [16] propose to accelerate (approximated) nearest\nneighbor search by using associative memories, and are hence\nrelated to learned index structures. During indexing time, the\ndata is partitioned into equi-sized bins. During query time,\nonly those bins are reﬁned having the largest overlap with\nthe query object. In [17], learned indexed for conjunctive\nboolean queries are considered, building upon the work of\n[10]. To this end, they propose multiple approaches each\nhaving a different trade-off between storage requirements and\ncomputational effort. The approaches are applied to term-\nqueries for a document database. Xiang et al. [18] propose the\nPavo index, which is a learned inverted index. The underlying\nhash function is replaced by a hierarchy of recurrent neural\nnetwork models. Dong et al. [19] propose Neural LSH for fast\napproximate nearest neighbor search. From the data points, a\nkNN graph is built, and a partition thereof is learned. There\nthe ﬁrst level comprises a NN predicting the partition index.\nThis partition is then further split using k-means.\nHowever, the existing approaches for (approximate) kNN\nquery processing with learned indices [15], [16], [19], are not\ndirectly efﬁciently applicable: Instead of determining the exact\nresult of the kNN query, it sufﬁces to decide whether qbelongs\nto it.\nC. LIS for RkNN\n[20] propose an approach to reduce the memory require-\nments of the MRkNNCoP tree. The parameters of the log-\nlog linear bounds of the MRkNNCoP tree are approximatedby various Machine Learning regression models. Guaranteed\nbounds are subsequently derived by using the minimum and\nmaximum training error. Despite appealing compression ratios,\ndue to the \"double approximation\" of ﬁrst approximating the\ncoefﬁcients of a model, which then, in turn, approximates\nthe k-distance, this approach inherits the limitation of MRkN-\nNCoP tree to be only able to model linear bounds in log-log\nspace and sacriﬁces some of its performance for smaller index\nsizes. We address this issue of having two approximation steps\neach with its loss of precision by combining them into a single,\ndirect approximation of the k-distances.\nIII. P ROPOSED METHOD\nWithin this paper we use the following notation: Let\n(U;dist )be a distance space, i.e. Uis an arbitrary set and\ndist:U\u0002U!Ris a distance, i.e. it fulﬁlls the following\naxioms for all x;y2U[21]:\n1) Non-Negativity: dist(x;y)\u00150\n2) Symmetry: dist(x;y) =dist(y;x)\n3)dist(x;x) = 0\nFurthermore, let D\u0012U;jUj<1be a database, k2Nand\nq2Ube a query point. By\nnndist (x;k) := arg min\nD0\u001aD\njD0j=kmax\nx02Ddist(x;x0) (1)\nwe denote the k-distance of an object x, i.e. the distance to\nitsk-nearest neighbor.\nAlgorithm 1 RkNN query processing within the ﬁlter-\nreﬁnement framework.\nfunction RNN(q,k)\nresult;candidates  ;;;\nfor allo2Ddo .Filter Step\nd[o] distance (q;o)\nupper;lower getBounds (o;k)\nifd<lower then\nresult results[fog\nelse ifd<upper then\ncandidates candidates[fog\nelse\nRejecto\nend if\nfor allo2candidates do.Reﬁnement Step\nkd nndist (o;k)\nifd[o]<kd then\nresult result[fog\nelse\nRejecto\nend if\nend for\nend for\nreturnresult\nend function\nWe brieﬂy revisit the ﬁlter-reﬁnement framework for pro-\ncessing RkNN queries. The general pipeline is given in Al-\n\nnndist(p,k) low high\nFig. 2: k-distances mapped on road networks Oldenburg (left)\nand North America (right) for k= 256 . In dense regions the\nk-distances are smaller while they are larger for sparse regions.\ngorithm 1. Given a query point qand the parameter of k,\nthe algorithm proceeds by ﬁrst screening the whole database,\nand calculating the distance between the query point, and the\nobjecto. Besides, the ﬁlter bounds are calculated, which give\nlower and upper bounds of the true k-distance. If the distance\nis smaller than the lower bound, it is also smaller than the\ntruek-distance, and hence can immediately be included in the\nresult without further veriﬁcation. If the distance is larger than\nthe upper bound, it will also be larger than the true k-distance,\nand hence can directly be discarded. For those in between, we\ncannot decide without further calculation and hence store them\ninto a set of candidates . After the ﬁlter step is ﬁnished, we\nhave a partial result, and well as a set of candidates. For this\nset of candidates we have to compute the real k-distances,\nwhich each requires an expensive forward kNN query.\nTowards these bounds, we pose the regression prediction\nof the (continuously valued) k-distance of a data point given\nits representation, i.e. we seek regression model Mwith\nparameters \u0012such thatM(x;k;\u0012)\u0019nndist (x;k). Our\nmotivation to use the object’s representation as input to our\nmodel is that the distances are also calculated based upon\nthese representations. To undermine this assumption consider\nthek-distances within a real world dataset. Figure 2 shows\nthe k-distances for k= 256 mapped onto the road networks\nof Oldenburg and North America. It can be observed that data\npoints that are close have similar k-distances and that the k-\ndistances are smaller in dense regions. We hypothesize that\nthis kind of correlation between coordinates of the data point,\nand itsk-distances can be captured by a Machine Learning\nmodel.\nDepending on the type of model we can encode different\ninductive biases. For instance, tree-based models such as\nregression trees learn a hierarchical split of the data space and\npredict separate values for each of these. Thus, they resemble\nclassical tree-based index structures, such as R-Trees. We refer\nto [10] for further explanation on how e.g. B-Trees can be\nseen as regression models. Neural networks on the other hand\nare universal function approximators, and depending on the\nactivation function, learn functions that resemble piece-wise00-10\n0-220\n0-120\n11-1-1\n-10-12\n2-201p\nk\n022122 \u0001K\n\"(p)2122\u0001D\n\"(k)\nFig. 3: Toy example to illustrate different aggregation modes.\nThe matrix shows the residuals, i.e. difference between the true\nk-distance and the prediction, for k= 1;:::; 4and 6 different\npoints. By max-aggregation over one of the axes we obtain the\nmaximum residuals which can be used for upper-bounding the\nrealk-distance with reduced storage requirements.\nlinear functions [22].\nFor a discussion about how to transform a trained Machine\nLearning model into an optimized C code, and thereby achieve\nefﬁciency, we refer to [10]. Moreover, our models do easily ﬁt\ninto RAM and are a direct mapping of the object’s representa-\ntion to thek-distance, which addresses the indexability. Thus,\nthe subsequent section focuses on how to obtain selective and\ncomplete bounds.\nA. Guaranteeing Completeness\nLetMbe a model predicting the k-distances. We calculate\nthe residual \u0001(p;k) =nndist (p;k)\u0000M(p;k)If we can\nbound the residual from above and below we can derive\nguaranteed bounds for the k-distance as well, i.e. if \u0001#(p;k)\u0014\n\u0001(p;k)\u0014\u0001\"(p;k)we havelb(p;k) :=M(p;k)+\u0001#(p;k)\u0014\nnndist (p;k)\u0014M(p;k)+\u0001\"(p;k) :=ub(p;k)These bounds\non the residual are essential as they guarantee the completeness\nof the ﬁlter step, i.e. that we do not discard any object which\nshould be part of the result. If we use the lower bound\nto include objects into the result without explicit k-distance\ncomputation, the guaranteed lower bound is also crucial for\nthe correctness of the algorithm.\nWhile numerous ways of obtaining such bounds are pos-\nsible, in this work, we investigate two simple choices: We\ncalculate the residuals for all data objects pand all values of\nk= 1;:::;k max, and ﬁxing either korpand aggregating over\nthe other parameter. Thereby, we can achieve either O(kmax)\norO(n), which may differ in their selectivity.\n1) Aggregating over p:In this case, for each ﬁxed k, we\naggregate the residual over all p02D, and hence obtain\nbounds with additional storage overhead of (2\u0001k)2O(kmax).\n\u0001D\n#(p;k) = \u0001D\n#(k) = min\np02D\u0001(p0;k) (2)\n\u0001D\n\"(p;k) = \u0001D\n\"(k) = max\np02D\u0001(p0;k) (3)\nThereby, for a ﬁxed value of k, the width of the bounds is\nequal for all points.\n\nub(p; k)\nub\u0003(p; k)ub(p; k + 1)\nlb(p; k\u00001)\nlb(p; k)lb\u0003(p; k)\nFig. 4: Visualization of restoring monotonicity for upper and\nlower bound exploiting monotonicity.\n2) Aggregating over k:Here, for each data point individu-\nally, we aggregate the residuals over all values of k. Thus, the\nbounds require an additional storage space of (2\u0001n)2O(n).\n\u0001K\n#(p;k) = \u0001K\n#(p) = min\n1\u0014k0\u0014kmax\u0001(p;k0) (4)\n\u0001K\n\"(p;k) = \u0001K\n\"(p) = max\n1\u0014k0\u0014kmax\u0001(p;k0) (5)\nThereby, for a ﬁxed point p, the width of the bounds is equal\nover all values of k.\n3) Combination of Aggregations: We can obtain guaranteed\nbounds with memory requirements of O(kmax)orO(n)by\naggregating residuals. Since both a guaranteed bounds, we\nmay do both and combine them to new improved bounds at\ncost ofO(n+kmax).\n\u0001KD\n#(p;k) = maxf\u0001K\n#(p);\u0001D\n#(k)g (6)\n\u0001KD\n\"(p;k) = minf\u0001K\n\"(p);\u0001D\n\"(k)g (7)\nB. Increasing Selectivity\nAfter obtaining the bounds, we can exploit two properties\nof the k-distances, namely that k-distances are non-negative\nand monotonous, in a parameter-free postprocessing step to\nenhance predictions and bounds.\n1) Non-Negativity: As all distances, the k-distances are\nnon-negative. Hence, it is safe to clip the predictions, as\nwell as the bounds at zero. By setting negative values to\n0 the predictions and bounds are getting closer to the true\nk-distances, which means that the bounds get tighter, and\ntherefore better candidate set sizes might be accomplished.\n2) Monotonicity: The k-distances for a speciﬁc data point\nare monotonously increasing in k. Hence, we can exploit\nthis property to derive additional bounds: Given a guaranteed\nupper bound ub(p;k)withp2Dandk2f1;:::;k maxg\none can derive further guaranteed upper bounds from k0\u0015k\nasnndist (p;k)\u0014ub(p;k0). We can combine all bounds to\nobtain a tighter bound: ub\u0003(p;k) := mink0\u0015k(ub(p;k0)). This\nconsideration analogously is valid for the lower bound. Since\nthe monotonicity restoration requires bounds for different\nvalues ofk0we need to perform additional computations here.However, many Machine Learning models support batched\nprocessing which allows for faster evaluation than doing the\npredictions sequentially.\n3) Sample Weights: Depending on the distribution of dis-\ntances for the current point to all other points, an error\nin the prediction, and the resulting looser bounds, have a\ndifferently severe impact on the candidate set size (cf. the\nintroductory example in Figure 2). Intuitively, if the point\nlies in a dense vicinity, increasing the upper bound might\ninduce a signiﬁcantly increased candidate set size compared\nto a point in a peripheral region with few points in close\ndistance. Thus, we propose to use sample weights to steer the\nmodel’s optimization to focus more on such dense regions, and\nless on remote points. Based on the observation of the effect\nof density, we may use a density measure to obtain sample\nweights.\nAlgorithm 2 Training procedure with iterative sample re-\nweighting.\nw[i;k] = 1 for alli= 1;:::;jDj,k= 1;:::;K\nfori= 1;:::;ITER do\nOptimize model parameters \u0012to minimize\nL=w[i;k]\u0001L(M(xi;k;\u0012);nndist (xi;k))\nCalculate candidate set size CSS for all xi2D,k=\n1;:::;K\nUpdate weights w[i;k] CSS (xi;k)\nend for\nHowever, since ultimately we are interested in the candidate\nset size, we can also directly use the candidate set size as\nsample weight. To this end, we propose an iterative training\nscheme, given in Algorithm 2. Starting with uniform weights,\nwe train the model, and calculate the candidate set sizes. Next,\nwe use these candidate set size as sample weight, and re-train\nthe model. Thus, the training procedure focuses stronger on\nthe loss from those data points where the candidate set size\nwas large. Here, we propose to repeat this procedure for a\nﬁxed number of steps. However, we may also run it until some\nconvergence criterion on the CSS is reached. The adaptive re-\nweighting scheme shares some similarity with e.g. AdaBoost\n[23], but does not directly use the prediction error, but instead\na different measure of interest (candidate set size) as weight.\nAlso the update is not multiplicative. We also notice that the\nsample weights may also be used to focus on certain data\npoints or values of kstronger than on others, for instance\nbecause these values are queried more frequently. We leave\nan exploration of such scenarios as future work.\nIV. E XPERIMENTS\nA. Experimental Setup\nThe experiments are implemented in a uniﬁed pipeline\nto ensure a consistent evaluation basis. For our extensive\nexperiments we use the Python library hyperopt [24] for\nhyperparameter and model search using a random search\n\nTABLE I: Datasets used for evaluation.\nDataset Short name Dimension Size\nOldenburg OL 2 6.105\nCalifornia CAL 2 21.049\nNorth America NA 2 175.814\nFastText EN EN 300 200.000\nstrategy. We implement the models using the sklearn [25] and\nPyTorch1libraries.\nFor data preprocessing, we apply dimension-wise z-score\nnormalization to the input, i.e. we subtract the mean and divide\nby the standard deviation. We account for these additional\nparameters in the model size ( O(d)). We notice that for some\nmodels we might be able to merge them with the model\nweights, e.g. for a neural network, we can merge the weights\nof the ﬁrst linear layer with the linear transformation of the\nz-score normalization to save some parameters and reduce\nthe index size. Moreover, for each kwe normalize the k-\ndistances of all points between 0 and 1. Again, we account\nfor these additional normalization parameters in the model size\n(O(kmax)).\nB. Evaluation\nFor query processing two metrics are of primary importance:\nRuntime and Memory consumption . Within ﬁlter-reﬁnement\nframeworks, the runtime is often approximated using the\ncandidate set size passed on to the reﬁnement step. Moreover,\nwe follow the argumentation of [26] that directly comparing\ntimes requires careful measurements and is of limited value,\nsince it compares implementations2instead of algorithms. The\nmemory consumption of a model is approximated by the\nnumber of its parameters since it is platform and programming\nlanguage independent.\nWe evaluate on three publicly available road network\ndatasets [27]3, as well as a high-dimensional dataset of word\nembeddings of the English language [28]4. Their statistics are\nsummarized in Table I. We compare with MRkNNCoP tree [1]\n(CoP) since it is a state-of-the-art approach based on the ﬁlter-\nreﬁnement framework. Moreover, it relies on a linear approx-\nimation in log-log space and hence serves as ablation to show\nthe effect of allowing arbitrary-shaped approximations. For the\nexperiments, the following model types are used: Decision\nTrees, GradientBoosting Regressors, AdaBoostRegressors and\nNeural Networks, with the hyperparameters optimized in the\nfollowing ranges:\n\u000fDecision Tree: max _depth2f1;2;:::; 15g,\n\u000fGradient Boosting: n_trees2 f 1;2;:::; 500g,\nmax _depth2f1;2;:::; 15g,\u00112[0:0;1:0],\n\u000fAdaBoost:n_trees2f1;2;:::; 500g,\u00112[0:0;1:0],\n\u000fNeural Network: n_layers2 f1;:::; 5g,units (i)2\nf4;:::; 300g,batch _norm2 fTrue;Falseg,\n1https://pytorch.org/\n2of the algorithm itself, but also the used libraries, operating system, . . .\n3https://www.cs.utah.edu/~lifeifei/SpatialDataset.htm\n4https://github.com/facebookresearch/MUSE\n104\n105\n106\n107\nSize [log]\n101\n102\nMean CSS [log]OL\nCAL\nNA\nEN\nFig. 5: Trade-off between mean candidate set size (CSS) and\nmodel size (both on logarithmic scale). For each dataset, we\nonly show the pareto-optimal models. In black, and the corre-\nsponding symbol, we show the performance of MRkNNCoP\ntree (CoP). For most datasets, our approach outperforms CoP\nin both, model size and CSS. For the EN dataset, we obtain\nsmaller models, but exceed the CSS of CoP.\n104\n105\n106\n107\nSize [log]\n102\n103\n104\n105\nMaximum CSS [log]OL\nCAL\nNA\nEN\nFig. 6: Maximum candidate set size (CSS) compared to\nthe model size for the same models as in Figure 5, again\nboth on a logarithmic scale. Different colors/markers indicate\ndifferent datasets, the black symbols show the performance\nof MRkNNCoP tree (CoP). For all datasets we can strongly\nimprove the maximum CSS compared to CoP, reducing the\nworst-case runtimes and thus mitigating latency spikes in\nquery processing.\nbatch _size2f26;:::; 212g,dropout _rate2[0:0;1:0],\ninput _norm2fTrue;Falseg,loss2fMAE;MSEg\nwhere\u0011denotes the learning rate. We always used four\niterations of sample re-weighting.\nC. Trade-off between Candidate Set Size and Model Size\nFor brevity, we abbreviate candidate set size with CSS\nfor the remainder of this paper. Figure 5 shows the overall\nperformance of our models for each dataset compared to\nCoP. Each marker type and color indicates a different dataset.\nThe lines indicate the skyline of our models regarding their\nachieved mean CSS and sizes, i.e. we only show the Pareto-\noptimal models, where there does not exist a model which\nis smaller and has a smaller mean CSS at the same time.\nThe black single markers stand for the results of MRkNNCoP\nfor each dataset. Both axes use a logarithmic scale. For all\nspatial datasets (OL, CAL, NA) our models outperform CoP\nin both, the mean CSS as well as the model size. For the word\n\nembeddings (EN) the mean CSS of CoP is smaller, however,\nwe can reduce upon the model size.\nIn Figure 6, we additionally show the same models, but\nnow comparing maximum CSS against model size. Here, our\nmodels improve over CoP on all datasets regarding both\nmetrics, up to several orders of magnitude. The maximum\nCSS correlates with the worst-case runtime of a query, which\nin practice may cause latency spikes in query processing.\nFrom the studied model types only decision trees and neural\nnetworks occur in the skyline of any dataset. EN’s skyline only\ncontains neural networks. For CAL and OL, the largest model\nin the skyline is a decision tree, and the rest comprises only\nneural networks. For NA, the two largest models are decision\ntrees.\nD. Ablation Study\nTo investigate the effect of our models’ components, we\nperform an ablation study. For each dataset we chose the best\nmodel according to mean candidate set size (CSS) which is\nstill smaller than the MRkNNCoP tree as the base model. For\nthis model, we study the effect of disabling sample weights,\naggregating only over k, or the points, and turning off the\nmonotonicity enhancement. We record mean and maximum\nCSS, as well as model size. The overall results are summarized\nin Table II and discussed in the following.\n1) Effect of Sample Weights: When comparing the usage\nof sample weights, we observe that the mean CSS always\nimproves for all road network datasets (OL, CAL, NA) irre-\nspective of the base conﬁguration, e.g. whether monotonicity\nrestoration is used, etc. We observe the strongest absolute\nimprovement for NA, ( K!SK,497:34!480:64). For\nthe word embedding dataset (EN), we observe the opposite\neffect: Here, it is advantageous to not use sample re-weighting,\nalthough the difference is usually small.\n2) Effect of Residual Aggregation Axes: Across all datasets,\nwe observe that aggregation over k(i.e. one ﬁxed bound\nwidth per data point p) is advantageous compared to the\naggregation of pwith one ﬁxed bound width per value of\nk. This is likely because the points reside in differently dense\nareas and sharing the same bound width over all data points\ndoes not take this into account. Since kmax\u001cn, the bounds\nobtained by aggregation over krequire more memory ( O(n))\nthan those by aggregation over p(O(k)). By combining both\nbounds we obtain a guaranteed improvement in candidate set\nsize with slightly increased index size O(n+k). Thus this\nparameter allows us to steer the trade-off between index size\nand candidate set size. When already spending nparameters\nfor bounds aggregated over k, we suggest to also combine\nthem with bounds aggregated over psince for usual values\nofkmax\u001cnthey offer a guaranteed (but potentially minor)\nimprovement at negligibly increased index size.\n3) Effect of Monotonicity Restoration: If the residuals are\naggregated only over p, the original bounds before bound\nenhancement have a ﬁxed width only dependent on k. If the\nresiduals are aggregated only over k, then the original bounds\nbefore bound enhancement even have a ﬁxed width over all n.\n50\n100\n150\n200\n250\nk\n0\n20\n40\n60\n80\n100\n120distance\noriginal lower\noriginal upper\nimproved lower\nimproved upper\nReal\nFig. 7: Qualitative example of restoring monotonicity for upper\nand lower bound on the dataset NA. The actual model bounds\n(dashed, lower: blue, upper: red), here obtained by aggregation\noverk, and the improved bounds (solid, lower: blue, upper:\nred) are displayed for each k(x-axis). The areas ﬁlled in\nthe corresponding color could be eliminated by restoring\nmonotonicity.\nHowever, there may be differences after bound enhancement,\nas it depends on the distribution of the predictions for a\nspeciﬁc data point pconcerningk, which is different for each\ndata object. From Table II we can observe that monotonicity\nrestoration has the largest effect when applied to bounds\nobtained only from aggregation over p. To better illustrate\nthis effect we show a qualitative example from the NA\ndataset in Figure 7, where restoring monotonicity improves\nthe tightness of the bounds. The real k-distances are shown as\na black dotted line. With dashed lines we show the original\nupper (red) and lower (blue) bounds predicted by the model.\nThe solid lines indicate the monotonicity bounds after bound\nenhancement. The colored areas between the original bound\nand the enhanced bound could be eliminated by restoring\nmonotonicity. As the bound width is ﬁxed over all k(since\nwe aggregated over this axis), as soon as the model prediction\nis non-monotonous, both bounds become non-monotonous as\nwell.\nV. C ONCLUSION\nIn this paper we proposed a novel approach to approximate\nthek-distances directly using a Machine Learning model in\norder to obtain ﬁlters for RkNN query processing within\nthe ﬁlter-reﬁnement framework. To this end, we formalized\nthe prediction as a regression problem and derived several\nguaranteed bounds essential to guarantee the completeness of\nthe ﬁlter step. Moreover, we showed how to further enhance\nthe bounds by restoring non-negativity and monotonicity. We\nalso proposed an iterative method to obtain sample weights to\nfurther tweak our models’ candidate set sizes. In experiments\non several real-world datasets, we showed superior perfor-\nmance to a state-of-the-art method, the MRkNNCoP tree, not\nonly in the number of candidates but also regarding the index\nsize.\nREFERENCES\n[1] E. Achtert, C. Böhm, P. Kröger, P. Kunath, A. Pryakhin, and M. Renz,\n“Efﬁcient reverse k-nearest neighbor search in arbitrary metric\n\nTABLE II: Ablation study showing the mean candidate set size ( CSS ), maximum candidate set size ( [CSS ) and the model\nsize. With Swe denote whether we use sample weights, with KandDwhether we aggregate over all kvalues, or over all\npoints (or both), and by Mwe denote whether we restore monotonicity. As base we choose the model with the best CSS\nwhich is still smaller than the MRkNNCoP tree, which is usually situated in the middle of the skyline.\ndataset OL CAL NA EN\nS K D M CSS[CSS Size CSS[CSS Size CSS[CSS Size CSS[CSS Size\nX X X X 26.29 118 24,147 31.46 168 60,185 50.04 642 482,052 163.61 1,457 423,749\nX X X 26.39 119 24,147 31.55 168 60,185 50.06 642 482,052 163.66 1,457 423,749\nX X X 28.32 209 23,635 31.89 210 59,673 50.05 642 481,540 164.37 1,457 423,237\nX X 28.34 209 23,635 31.95 210 59,673 50.06 642 481,540 164.42 1,457 423,237\nX X X 49.65 143 11,937 66.53 243 18,089 423.14 1,858 130,426 5334.82 36,625 23,749\nX X 52.37 164 11,937 68.25 262 18,089 428.59 1,939 130,426 5351.05 36,625 23,749\nX X X 27.07 117 24,147 31.80 182 60,185 51.32 594 482,052 163.34 1,350 423,749\nX X 27.18 118 24,147 31.88 186 60,185 51.34 594 482,052 163.40 1,350 423,749\nX X 29.16 194 23,635 32.36 221 59,673 51.32 594 481,540 164.03 1,350 423,237\nX 29.18 194 23,635 32.41 221 59,673 51.34 594 481,540 164.08 1,350 423,237\nX X 52.56 151 11,937 68.85 250 18,089 480.64 2,460 130,426 5201.22 37,236 23,749\nX 55.93 183 11,937 70.11 251 18,089 497.34 2,460 130,426 5215.65 37,236 23,749\nspaces,” in Proceedings of the 2006 ACM SIGMOD International\nConference on Management of Data , ser. SIGMOD ’06. New\nYork, NY , USA: ACM, 2006, pp. 515–526. [Online]. Available:\nhttp://doi.acm.org/10.1145/1142473.1142531\n[2] W. Wu, F. Yang, C.-Y . Chan, and K.-L. Tan, “Finch: Evaluating reverse\nk-nearest-neighbor queries on location data,” Proceedings of the VLDB\nEndowment , vol. 1, no. 1, pp. 1056–1067, 2008.\n[3] M. A. Cheema, X. Lin, W. Zhang, and Y . Zhang, “Inﬂuence\nzone: Efﬁciently processing reverse k nearest neighbors queries,”\ninProceedings of the 27th International Conference on Data\nEngineering, ICDE 2011, April 11-16, 2011, Hannover, Germany ,\nS. Abiteboul, K. Böhm, C. Koch, and K. Tan, Eds. IEEE\nComputer Society, 2011, pp. 577–588. [Online]. Available: https:\n//doi.org/10.1109/ICDE.2011.5767904\n[4] G. Casanova, E. Englmeier, M. E. Houle, P. Kröger, M. Nett, E. Schu-\nbert, and A. Zimek, “Dimensional testing for reverse k-nearest neighbor\nsearch,” Proceedings of the VLDB Endowment , vol. 10, no. 7, pp. 769–\n780, 2017.\n[5] S. Yang, M. A. Cheema, X. Lin, Y . Zhang, and W. Zhang, “Reverse k\nnearest neighbors queries and spatial reverse top-k queries,” The VLDB\nJournal , vol. 26, no. 2, pp. 151–176, 2017.\n[6] C. Lijun, L. Xiyin, Z. Tiejun, Z. Zhongping, and L. Aiyong, “A data\nstream outlier delection algorithm based on reverse k nearest neighbors,”\nin2010 International Symposium on Computational Intelligence and\nDesign , vol. 2. IEEE, 2010, pp. 236–239.\n[7] T. Hu, H. Xiong, W. Zhou, S. Y . Sung, and H. Luo, “Hypergraph\npartitioning for document clustering: A uniﬁed clique perspective,” in\nProceedings of the 31st annual international ACM SIGIR conference on\nResearch and development in information retrieval , 2008, pp. 871–872.\n[8] P. Pei, D. Zhang, and F. Guo, “A density-based clustering algorithm\nusing adaptive parameter k-reverse nearest neighbor,” in 2019 IEEE\nInternational Conference on Power, Intelligent Computing and Systems\n(ICPICS) , July 2019, pp. 455–458.\n[9] K. Yang and G. Chiu, “Monitoring continuous all k-nearest neighbor\nquery in mobile network environments,” Pervasive Mob. Comput. ,\nvol. 39, pp. 231–248, 2017.\n[10] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The\ncase for learned index structures,” in Proceedings of the 2018\nInternational Conference on Management of Data , ser. SIGMOD ’18.\nNew York, NY , USA: ACM, 2018, pp. 489–504. [Online]. Available:\nhttp://doi.acm.org/10.1145/3183713.3196909\n[11] F. Korn and S. Muthukrishnan, “Inﬂuence sets based on reverse\nnearest neighbor queries,” in Proceedings of the 2000 ACM SIGMOD\nInternational Conference on Management of Data , ser. SIGMOD ’00.\nNew York, NY , USA: ACM, 2000, pp. 201–212. [Online]. Available:\nhttp://doi.acm.org/10.1145/342009.335415\n[12] C. Yang and K. Lin, “An index structure for efﬁcient reverse\nnearest neighbor queries,” in Proceedings of the 17th International\nConference on Data Engineering, April 2-6, 2001, Heidelberg,\nGermany , D. Georgakopoulos and A. Buchmann, Eds. IEEE\nComputer Society, 2001, pp. 485–492. [Online]. Available: https:\n//doi.org/10.1109/ICDE.2001.914862[13] Y . Tao, D. Papadias, and X. Lian, “Reverse knn search in\narbitrary dimensionality,” in Proceedings of the Thirtieth International\nConference on Very Large Data Bases - Volume 30 , ser. VLDB\n’04. VLDB Endowment, 2004, pp. 744–755. [Online]. Available:\nhttp://dl.acm.org/citation.cfm?id=1316689.1316754\n[14] P. Ciaccia, M. Patella, and P. Zezula, “M-tree: An efﬁcient access method\nfor similarity search in metric spaces,” International conference on very\nlarge data bases (VLDB) , 08 2001.\n[15] C. Yu, V . Gripon, X. Jiang, and H. Jégou, “Neural associative memories\nas accelerators for binary vector search,” Proceedings of Cognitive , pp.\n85–89, 2015.\n[16] V . Gripon, M. Löwe, and F. Vermet, “Associative memories to accelerate\napproximate nearest neighbor search,” Applied Sciences , vol. 8, no. 9,\np. 1676, 2018.\n[17] H. Oosterhuis, J. S. Culpepper, and M. de Rijke, “The potential\nof learned index structures for index compression,” arXiv preprint\narXiv:1811.06678 , 2018.\n[18] W. Xiang, H. Zhang, R. Cui, X. Chu, K. Li, and W. Zhou, “Pavo: A\nrnn-based learned inverted index, supervised or unsupervised?” IEEE\nAccess , vol. 7, pp. 293–303, 2019.\n[19] Y . Dong, P. Indyk, I. Razenshteyn, and T. Wagner, “Learning\nspace partitions for nearest neighbor search,” in International\nConference on Learning Representations , 2020. [Online]. Available:\nhttps://openreview.net/forum?id=rkenmREFDr\n[20] M. Berrendorf, F. Borutta, and P. Kröger, “k-distance approximation\nfor memory-efﬁcient rknn retrieval,” in SISAP , ser. Lecture Notes in\nComputer Science, vol. 11807. Springer, 2019, pp. 57–71.\n[21] E. Deza and M. Deza, Dictionary of distances . North-Holland, 2006.\n[22] Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang, “The expressive power of\nneural networks: A view from the width,” in NIPS , 2017, pp. 6231–6239.\n[23] Y . Freund and R. E. Schapire, “A decision-theoretic generalization of on-\nline learning and an application to boosting,” in EuroCOLT , ser. Lecture\nNotes in Computer Science, vol. 904. Springer, 1995, pp. 23–37.\n[24] J. Bergstra, B. Komer, C. Eliasmith, D. Yamins, and D. D. Cox,\n“Hyperopt: a python library for model selection and hyperparameter\noptimization,” Computational Science & Discovery , vol. 8, no. 1,\np. 014008, jul 2015. [Online]. Available: https://doi.org/10.1088%\n2F1749-4699%2F8%2F1%2F014008\n[25] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vander-\nplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay, “Scikit-learn: Machine learning in Python,” Journal of Machine\nLearning Research , vol. 12, pp. 2825–2830, 2011.\n[26] H.-P. Kriegel, E. Schubert, and A. Zimek, “The (black) art of runtime\nevaluation: Are we comparing algorithms or implementations?” Knowl.\nInf. Syst. , vol. 52, no. 2, p. 341–378, Aug. 2017. [Online]. Available:\nhttps://doi.org/10.1007/s10115-016-1004-2\n[27] F. Li, D. Cheng, M. Hadjieleftheriou, G. Kollios, and S.-H. Teng, “On\ntrip planning queries in spatial databases,” in SSTD , 2005.\n[28] G. Lample, A. Conneau, M. Ranzato, L. Denoyer, and H. Jégou, “Word\ntranslation without parallel data,” in ICLR (Poster) . OpenReview.net,\n2018.",
  "textLength": 42856
}