{
  "paperId": "f6986f4e1d32ab2ab8f4e31a35ea8bcb561cdbec",
  "title": "On Dynamic Graph Algorithms with Predictions",
  "pdfPath": "f6986f4e1d32ab2ab8f4e31a35ea8bcb561cdbec.pdf",
  "text": "arXiv:2307.09961v2  [cs.DS]  8 Dec 2023On Dynamic Graph Algorithms with Predictions∗\nJan van den Brand\nGeorgia Institute of TechnologySebastian Forster\nUniversity of SalzburgYasamin Nazari\nVU Amsterdam\nAdam Polak\nBocconi University\nAbstract\nDynamic algorithms operate on inputs undergoing updates, e .g., insertions or deletions of\nedges or vertices. After processing each update, the algori thm has to answer queries regarding\nthe current state of the input data. We study dynamic algorit hms in the model of algorithms\nwith predictions (also known as learning-augmented algori thms). We assume the algorithm is\ngiven imperfect predictions regarding future updates, and we ask how such predictions can be\nused to improve the running time. In other words, we study the complexity of dynamic problems\nparameterized by the prediction accuracy. This can be seen a s a model interpolating between\nclassic online dynamic algorithms – which know nothing abou t future updates – and oﬄine\ndynamic algorithms with the whole update sequence known upf ront, which is similar to having\nperfect predictions. Our results give smooth tradeoﬀs betw een these two extreme settings.\nOur ﬁrst group of results is about partially dynamic problem s with edge updates. We\ngive algorithms for incremental and decremental transitiv e closure and approximate APSP that\ntake as an additional input a predicted sequence of updates ( edge insertions, or edge deletions,\nrespectively). They preprocess it in /tildewideO(n(3+ω)/2) time, and then handle updates in /tildewideO(1) worst-\ncase time and queries in /tildewideO(η2) worst-case time. Here ηis an error measure that can be bounded\nby the maximum diﬀerence between the predicted and actual in sertion (deletion) time of an\nedge, i.e., by the ℓ∞-error of the predictions.\nThe second group of results concerns fully dynamic problems with vertex updates, where\nthe algorithm has access to a predicted sequence of the next nupdates. We show how to solve\nfully dynamic triangle detection, maximum matching, singl e-source reachability, and more, in\nO(nω−1+nηi) worst-case update time.\nHere ηidenotes how much earlier the i-th update occurs than predicted.\nOur last result is a reduction that transforms a worst-case i ncremental algorithm without\npredictions into a fully dynamic algorithm which is given a p redicted deletion time for each\nelement at the time of its insertion. As a consequence we can, e.g., maintain fully dynamic\nexact APSP with such predictions in /tildewideO(n2) worst-case vertex insertion time and /tildewideO(n2(1 + ηi))\nworst-case vertex deletion time (for the prediction error ηideﬁned as above).\nOur algorithms from the ﬁrst two groups, given suﬃciently ac curate predictions, achieve\nrunning times that go below known lower bounds for classic (w ithout predictions) dynamic algo-\nrithms under the OMv Hypothesis. Moreover, our dependence o n the prediction errors (so-called\nsmoothness) is conditionally optimal, under plausible ﬁne -grained complexity assumptions, at\nleast in certain parameter regimes.\n∗This work is supported by the Austrian Science Fund (FWF): P 3 2863-N. This project has received funding from\nthe European Research Council (ERC) under the European Unio n’s Horizon 2020 research and innovation programme\n(grant agreement No 947702). Part of this work was performed when Yasamin Nazari was aﬃliated with University\nof Salzburg, and Adam Polak was aﬃliated with Max-Planck Ins titute of Informatics. The discussions leading to this\nwork was initiated at the AlgPiE 2022 workshop, organized by IGAFIT.\ni\n\nContents\n1 Introduction 1\n1.1 Our Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2 Further Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.3 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1.4 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2 Technical Overview 9\n2.1 Partially Dynamic Algorithms (Section 3) . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.2 Fully Dynamic Matrix Inverse with Predictions (Section 4). . . . . . . . . . . . . . 9\n2.3 Fully Dynamic Algorithms with Predicted Deletion Times (Section 6) . . . . . . . . 11\n3 Partially Dynamic Algorithms 12\n3.1 Upper Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.2 Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4 Dynamic Matrix Inverse with Predictions 16\n4.1 Reducing Rank-1 Updates to Column Updates . . . . . . . . . . . . . . . . . . . . . 16\n4.2 Column Updates with Predictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n4.3 Putting Everything Together . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n5 Fully Dynamic Graph Algorithms with Predictions 23\n5.1 Triangle Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n5.2 Directed Cycle Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n5.3 Single Source Reachability and Strong Connectivity . . . . . . . . . . . . . . . . . . 24\n5.4 Maximum Matching Size and Counting st-Paths . . . . . . . . . . . . . . . . . . . . 24\n6 Fully Dynamic Algorithms with Predicted Deletion Times 24\n6.1 Application to All-Pairs Shortest Paths with Vertex Upd ates . . . . . . . . . . . . . 26\n7 Acknowlegment 26\nii\n\n1 Introduction\nDynamic algorithms maintain a solution to a computational p roblem – e.g., single source distances\nin a graph – for an input that undergoes a sequence of updates – e.g., edge insertions or deletions.\nThe goal is to process such updates as eﬃciently as possible, at least faster than recomputing the\nsolution from scratch.\nThis is however not always plausible, as evidenced by numero us ﬁne-grained conditional lower\nbounds, see, e.g., [ Păt10 ,AW14 ,HKNS15 ].\nThe recent line of research on learning-augmented algorith ms provides many examples of how\nperformance of classic algorithms can be provably improved using imperfect predictions , generated,\ne.g., by machine-learning models (see surveys by Mitzenmac her and Vassilvitskii [ MV20 ,MV22 ]).\nAmong others, predictions allow us to improve competitive r atios of online algorithms (e.g., [ LV21 ,\nPSK18 ]), running times of static algorithms (e.g., [ DIL+21,CSVZ22 ]), approximation ratios of\npolynomial-time approximation algorithms (e.g., [ EFS+22,GLNS22 ,NCN23 ]). Often these im-\nprovements go beyond what is provably possible for classic a lgorithms (e.g., [ LV21 ,EFS+22] and\nmany others). In this work we ask the following natural quest ion:\nHow could we use predictions to improve dynamic algorithms?\nWe make two choices to narrow down this question. First, we fo cus on predictions about future\ninput data . This is akin to many previous results on learning-augmente d online algorithms (e.g.,\n[LV21 ,BMRS20 ]), and in contrast with settings where the predictions are a bout the output (e.g.,\n[ACE+23,DIL+21,EFS+22]). Second, we focus on improving the running time , which is the most\nstudied performance measure for dynamic algorithms.\nOnline and Oﬄine Dynamic Problems, and Conditional Lower Bo unds. Dynamic prob-\nlems are most often studied in their online variants, i.e., f uture updates are not known to the\nalgorithm, and it has to perform them one by one. On the other h and, oﬄine dynamic algorithms\n(see, e.g, [ Epp94 ,SM10 ,KL15 ,BKN19 ,PSS19 ,CGH+20]) are given a sequence of updates upfront.\nNote that the oﬄine model is equivalent to our proposed model with predictions in the case that\npredictions are perfectly accurate. That is, we study an int erpolation between oﬄine and online\ndynamic algorithms, and ask how an algorithm’s performance degrades with increasing inaccuracies\nof predictions. This interpolation question makes sense on ly for dynamic problems whose oﬄine\nvariants admit faster algorithms than their corresponding online variants do. Ideally, we would like\nto be able to say that predictions – even imperfect ones – allo w certain dynamic problems to be\nsolved faster than what is provably plausible without such p redictions in the classic online model.\nIn other words, we aim for running times going below known con ditional lower bounds.\nThere are many reductions showing hardness of dynamic probl ems under popular ﬁne-grained\ncomplexity assumptions about static problems, such as 3SUM , APSP, or CNF-SAT (see, e.g.,\n[AW14 ]). However, all these reductions share what is from our pers pective a limitation: they\nare not adaptive, they are capable of producing the whole inp ut sequence at once. Hence, they\nalready imply hardness for oﬄine variants of dynamic proble ms, and thus they cannot provide tight\nconditional lower bounds for those dynamic problems whose o ﬄine variants happen to be strictly\neasier than corresponding online variants. It is an interes ting open problem to ﬁnd a reduction\n– from a natural static problem to a natural online dynamic pr oblem – that does not have this\nlimitation [ Abb22 ].\nTo our best knowledge, the only known tool in ﬁne-grained com plexity that is capable of dis-\ntinguishing between online and oﬄine variants of dynamic pr oblems is the Online Matrix-Vector\nMultiplication (OMv) Hypothesis [ HKNS15 ], a conditional assumption about an online problem\n1\n\nitself. Therefore, in this work we focus (mostly) on problem s with known tight OMv lower bounds.\nThese lower bounds often show that recomputing from scratch is basically the best we can hope\nfor (without predictions).\nWarm-up: OMv with Predictions. Before we delve into graph problems, let us start with a\nsimple sanity check and verify that the cubic time barrier fo r OMv can be broken using predictions.\nIf it was not the case, our project would be hopeless because t hen problems with known OMv lower\nbounds would remain hard even with predictions.\nRecall that in the OMv problem we are ﬁrst given Boolean matri xM∈{0,1}n×n, and then\nwe need to answer nqueries of the form: Given vector vi∈{0,1}n, what is the Boolean product\nMv? We have to answer queries one by one, in an online fashion: we have to output Mv ibefore\nwe get to learn what vi+1is. OMv is conjectured to require cubic time, up to subpolyno mial\nfactors [ HKNS15 ]. We propose the following variant of OMv with predictions:\nOnline matrix-vector multiplication (OMv) with predictio ns\nInput oﬄine: matrix M∈{0,1}n×n;\npredicted vectors /hatwidev1,/hatwidev2, . . . , /hatwidevn∈{0,1}n.\nInput online: vectors v1, v2, . . . , v n∈{0,1}n.\nOutput: matrix-vector products Mv 1, Mv 2, . . . , Mv n∈{0,1}nover Boolean semiring.\nWe show a simple algorithm whose running time depends on the ℓ1-error of the predictions, which is\none of the standard error measures for learning-augmented a lgorithms (see, e.g., [ LV21 ,ACE+23]).\nObservation 1.1. OMv with predictions can be solved in total time O(nω+n/summationtext\ni||vi−/hatwidevi||1). More\nprecisely, preprocessing runs in O(nω)time, and i-th query requires O(n||vi−/hatwidevi||1)time.\nProof. In the preprocessing phase, compute M·[/hatwidev1,/hatwidev2, . . . , /hatwidevn] over integers, in time O(nω). Then,\nafter receiving each vector vi, compute (over integers) Mv i=M/hatwidevi+M(vi−/hatwidevi). The ﬁrst term,\nM/hatwidevican be retrieved in O(n) time because it is the i-th column of the matrix computed during\nthe preprocessing. The second term, M(vi−/hatwidevi) can be computed in time O(n||vi−/hatwidevi||1), because\n||vi−/hatwidevi||1equals the number of non-zeros of vi−/hatwidevi. Indeed,||vi−/hatwidevi||1=||/hatwidevi−vi||0, because vi\nand /hatwidevicontain only zeros and ones. /square\nNote that||vi−/hatwidevi||1/lessorequalslantn, and hence/summationtext\ni||vi−/hatwidevi||1/lessorequalslantn2, so even with arbitrarily bad predictions\nour algorithm never needs more than cubic time, i.e., it is robust in the learning-augmented termi-\nnology. On the other hand, for perfect predictions (i.e.,/summationtext\ni||vi−/hatwidevi||1= 0) we achieve the running\ntime of the best oﬄine algorithm, i.e., we are consistent . In Section 1.2we discuss these concepts\nfurther.\nMoreover, the running time of this algorithm matches two con ditional lower bounds. First,\nthe dependence on the total prediction error η=/summationtext\ni||vi−/hatwidevi||1cannot be improved from nηto\nnη0.99under the OMv hypothesis; this is because OMv reduces to OMv w ith predictions, with\nη=O(n2), by providing arbitrary (e.g., all-zero) vectors as predi ctions. Second, the nωterm\ncannot be improved under the assumption that Boolean matrix multiplication is not easier than\nthe general matrix multiplication, because even with perfe ct predictions (i.e., η= 0), solving OMv\nwith predictions entails computing a Boolean matrix produc t. These arguments, however, do not\nrule out the possibility of, e.g., an O(nω+η3/2) time algorithm, which would be a meaningful\nimprovement over the above algorithm. We ﬁnd it an interesti ng open problem to provide a ﬁne-\ngrained conditional lower bound, matching the running time of Observation 1.1, that holds already\nrestricted to instances with η= Θ( nα) for arbitrary ﬁxed α∈(0,2).\n2\n\nFinally, note that going below the cubic time barrier for OMv , even with accurate predictions,\nrequires fast matrix multiplication. Hence, it is not a surp rise that our graph algorithms discussed\nbelow – that go beyond known OMv-based lower bounds – use alge braic algorithms for matrix\nmultiplication.\n1.1 Our Results\nOur main results can be categorized into three groups that di ﬀer in the considered settings, types of\npredictions and measures of prediction errors. We discuss t he groups one by one. Table 1provides\na summary.\nPartially Dynamic Graph Problems. Our ﬁrst group of results is about partially dynamic\nproblems with edge updates. These are problems on graphs und ergoing edge insertions (incremental\nvariant), and graphs undergoing edge deletions (decrement al variant), but not both types of updates\nat the same time (this would be a fully dynamic variant, which we address later). While in general\nit is not the case that incremental and decremental variants must be equivalent, it turns out that\nall our results give the same bounds for both variants.\nOur partially dynamic (incremental and decremental) algor ithms take as an additional input\na predicted sequence of updates (edge insertions or edge del etions, respectively). To illustrate our\nsetting, we provide as an example a detailed deﬁnition of the incremental transitive closure problem,\nalso known as all-pairs reachability, in directed graphs.\nIncremental transitive closure with predictions\nInput oﬄine: predicted sequence of edge insertions /hatwidee1,/hatwidee2, . . . , /hatwideem∈E.\nInput online: sequence of interleaved updates and queries, i.e.,\n– edge insertions e1, e2, . . . , e m∈E, and\n– reachability queries ( s1, t1),(s2, t2), . . . , (sq, tq)∈V×V.\nOutput: for each query ( si, ti),\n– YES if there is a path from sitotiin the graph at the current moment,\n– NO otherwise.\nThe decremental variant is very similar; the diﬀerence is th at the predicted sequence tells the\nalgorithm in which order the edges are supposed to be deleted , not inserted, and that the online\nupdate operations are edge deletions, not edge insertions. In the incremental setting, it is natural\nto assume we start with an empty graph. However, in the decrem ental setting (also for classic\nalgorithms without predictions), the initial graph has to b e given upfront. In our setting with\npredicted sequence of updates, the initial graph can be cons idered to be given implicitly by the\nset of all edges appearing in the sequence. On the other hand, we can also think of an equivalent\ninput format in which we are ﬁrst given the initial graph G= (V, E) where each edge e∈Ecomes\nwith an additional label /hatwidet(e)∈[m] describing the predicted position of this edge in the seque nce of\ndeletions.\nThe second problem that we consider in this setting is the (1+ ǫ)-approximate all-pairs shortest\npaths (APSP) problem in unweighted directed graphs. The pro blem deﬁnition is similar to the\nabove transitive closure deﬁnition – the only diﬀerence is t hat each query ( si, ti) has to output a\nnumber in range [ d(si, ti),(1 + ǫ)·d(si, ti)], where d(si, ti) denotes the length of a shortest path\nfrom sitotiat the current moment.\nThe query times of our algorithms for these problems depend o n prediction error ηthat has\na somewhat technical deﬁnition. We discuss this in detail in Sections 2.1and3. For now let us\n3\n\nonly say that ηcan be upper bounded by the largest absolute diﬀerence betwe en the predicted and\nactual insertion (deletion) time of an edge – i.e., the ℓ∞error of the predictions – which we denote\nbyη∞def= max e∈E|t(e)−/hatwidet(e)|, where t(e) denotes the actual update time of an edge (i.e., t(ei)def=i),\nand/hatwidet(e) denotes the predicted update time of an edge (i.e., /hatwidet(/hatwideejdef=j). We discuss the choice of this\nerror measure in Section 1.2.\nOur partially dynamic algorithms with predictions are spec iﬁed in Theorem 1.2.\nTheorem 1.2 (Full details in Theorems 3.1and3.2).Each of the following dynamic graph problems:\n•incremental transitive closure,\n•decremental transitive closure,\n•incremental (1 + ǫ)-approximate unweighted all-pairs shortest paths,\n•decremental (1 + ǫ)-approximate unweighted all-pairs shortest paths\nwith a predicted sequence of edge updates can be solved by a de terministic algorithm with/tildewideO(n(3+ω)/2)\npreprocessing time,/tildewideO(1)worst-case edge update time, and/tildewideO(η2)query time, for prediction error\nη/lessorequalslantη∞.\nWithout predictions, both these problems (transitive clos ure and approximate APSP) in both\npartially dynamic variants (incremental and decremental) can be solved by algorithms with O(n3)\ntotal update time and O(1) query time [ IK83 ,Ita86 ,PvL87 ,DI00 ,DI06 ,BHS07 ,RZ08 ,Lac13 ,\nBer13 ], and under the OMv hypothesis this running time is tight (up to subpolynomial factors)\neven if one allows up to O(n1−ε) query time [ HKNS15 ]. In other words, classic algorithms without\npredictions face the cubic time barrier for these problems, and we show it can be bypassed with\nsuﬃciently accurate predictions.\nWe also show that the dependence of the running time in Theore m1.2on the prediction error\nη∞cannot be improved to strictly subquadratic under the OMv hy pothesis.\nTheorem 1.3. Unless the OMv Hypothesis fails, there is no algorithm for in cremental (decre-\nmental) transitive closure with predictions with arbitrar y polynomial prepocessing time, worst-case\nO(n1−ε)edge insertion (deletion) time, and O(η2−ε\n∞)query time, for any ε >0. This already holds\nwith respect to amortized time bounds per operation.\nFinally, we note that the algorithms of Theorem 1.2are not robust – for large enough prediction\nerrors they can be slower than the best known classic algorit hm – but they can be made robust, at\nleast in the amortized-time sense, using the black-box appr oach described in Section 1.2.\nFully Dynamic Graph Problems with Predicted Vertex Updates .Here we consider fully\ndynamic variants of various graph problems on directed grap hs with vertex updates: triangle detec-\ntion, single source reachability, strong connectivity, cy cle detection, maximum matching, number\nof vertex disjoint st-paths. For online vertex updates, there is no dynamic algorithm with O(n2−ǫ)\nupdate time for any constant ǫ > 0, conditional on the OMv hypothesis [ HKNS15 ]. All these\nproblems (except triangle detection) can also be naively re computed from scratch in/tildewideO(n2) time\n[BLN+20,CKL+22]. Thus no polynomial improvement over the trivial approach is possible for\nthese online dynamic problems. For oﬄine vertex updates, there is no dynamic algorithm with\nO(nω−1−ǫ) update time, conditional on the triangle detection hypoth esis [ AW14 ].\nOn the upper bound side, this is matched by [ SM10 ,BNS19 ] with O(nω−1) update time. Our\nTheorem 1.4provides a smooth trade-oﬀ between the online and oﬄine mode l.\nWe use ηito denote the prediction error of the i-th performed update. The prediction is a\nsequence of vertex updates. It can happen that an update occu rs earlier than initially predicted,\n4\n\ni.e. an update is moved several positions ahead in the sequen ce. When the data structure performs\nthei-th update, ηiis how many positions this update occurs too early. If this i-th update was not\npredicted at all (i.e. it doesn’t even occur in the predicted sequence), we deﬁne ηi=∞.\nTheorem 1.4 (Full details in Theorem 5.1).Fully dynamic triangle detection, single source reach-\nability, strong connectivity, directed cycle detection, m aximum matching size, number of vertex\ndisjoint st-paths, with Θ(n)vertex updates and predictions can be solved in O(nω+n/summationtext\nimin{ηi, n})\ntotal time.\nMore precisely, we have O(nω)preprocessing time and the i-th update takes O(nω−1+n·\nmin{ηi, n})time.\nWe remark that one can interpret our total time complexity as scaling with the ℓ1-norm of the\nerror. For η′\ni:= min{ηi, n},η′∈Nnwe have total time O(nω+n/bar⌈blη′/bar⌈bl1) for nupdates.\nThis result is obtained via reductions by [ San04 ,San07 ,BNS19 ], which reduce these dynamic\ngraph problems to dynamic matrix inverse. In dynamic matrix inverse, we are given a dynamic\nn×nmatrix Mand must maintain information about M−1. The case of vertex updates on graph\nreduces to row and column updates to M(i.e. updates can replace one row and column of Mat\na time). The oﬄine model with only column updates was studied previously in [ SM10 ,BNS19 ]\nand for entry updates in [ Kav14 ]. The online model with row and column updates was studied\nin [San04 ]. We construct a dynamic matrix inverse algorithm with pred ictions in Section 4which\nthen implies Theorem 1.4via reductions from [ San04 ,San07 ].\nFully Dynamic Graph Problems with Predicted Deletion Times .The last setting that\nwe study is also a fully dynamic one but with a weaker predicti on requirement than above.\nFirst, only deletions are predicted; the algorithm needs no prior knowledge of insertions. Second,\ndeletions are predicted only at the time of corresponding in sertions. In other words, compared to\na classic fully dynamic setting, the only diﬀerence is that e ach insertion comes with an additional\nnumber predicting when the currently inserted item (e.g., v ertex or edge) is going to be deleted.\nThis model is inspired by a recent result by [ PR23 ] in which they assume an oﬄine sequence\nof predicted deletions – i.e. deletion times have no error – b ut in our case we can handle deletion\nerrors. We ﬁrst extend their techniques to give the followin g result:\nTheorem 1.5. Consider a sequence of Tupdates and suppose we are given an incremental dynamic\nalgorithm with worst-case update time Γ. Assume also that at any point in time we have a prediction\non the order of deletions of current items, such that for the iinserted item the error ηiindicates\nthe number of elements predicted to be earlier than i-th item that actually arrive later ( ηi= 0if the\nprediction is correct or the element arrives later). Then we have a fully-dynamic algorithm with\nthe following guarantees:\n•An insertion update is performed in O(Γ log2T)worst-cast time.\n•The deletion of the i-th element can be performed in O((1 + ηi)·Γ log2T)worst-case time.\nWe can use this reduction, combined with an incremental APSP algorithm observed by [ Tho05 ]\nto get the following:\nTheorem 1.6. Given a weighted and directed graph undergoing online verte x insertions and pre-\ndicted vertex deletions, we can maintain exact weighted all -pairs shortest paths with the following\nguarantees:\n•An insertion update can be performed O(n2log2n)worst-cast time.\n5\n\nTable 1: Summary of our results. Each column describes one gr oup of results, built around one\ntechnique or data structure.\nTheorem 1.2 Theorem 1.4 Theorem 1.5\nSetting:\npartially dynamic fully dynamic fully dynamic\n(incremental or decremental) (insertions and deletions) ( insertions and deletions)\nType of updates:\nedge updates vertex updates vertex updates\nPredictions:\nsequence of all updates sequence of next nupdates deletion times\n(insertions or deletions) (given during insertions)\nRunning time:\npreprocessing: /tildewideO(n(3+ω)/2) preprocessing: O(nω) no preprocessing\nupdate: /tildewideO(1) update: insertion: /tildewideO(n2)\nquery: /tildewideO(η2\n∞) O(nω−1+n·min{ηi, n}) deletion: /tildewideO(ηin2)\nError measure:\nℓ∞ ℓ1 ℓ1\nApplications to graph problems:\ntransitive closure triangle detection exact APSP\n(1 + ǫ)-approximate APSP single-source reachability\nstrong connectivity\ndirected cycle detection\nmaximum matching size\n#vertex-disjoint st-paths\nMain technical tool:\nAll-Pairs Bottleneck Paths dynamic matrix inverse reducti on to incremental\n•A deletion of the i-th inserted vertex vican be performed in O(n2(ηilog2n+ 1)) worst-case\ntime, where error ηi∈[0, n]indicates how many vertices were predicted to be deleted bef orevi\nthat are actually deleted after vi.\nThis can be compared to a recent fully dynamic worst-case exa ct APSP bound of/tildewideO(n2.5)\nby [Mao23 ] improving upon a long line of work on sub-cubic update times for APSP [ Tho05 ,\nACK17 ,GWN20 ,CZ23 ]. We note that n2.5seems to be a natural barrier inherent to current\nalgorithmic approaches for this problem, but there is no kno wn conditional lower bound formalizing\nthis intuition.\n1.2 Further Discussion\nConsistency, Robustness, and Smoothness. Typically, algorithms with predictions are de-\nsigned with three goals in mind: (1) consistency , that is a near-optimal (or at least better than\nworst-case) performance when predictions are accurate; (2 )robustness , that is retaining worst-case\nguarantees of classic algorithms even when predictions are adversarial; and (3) smoothness , that\n6\n\nis a graceful degradation of algorithm’s performance with i ncreasing prediction error, providing an\ninterpolation between the former two extremes. Let us discu ss how these goals translate to our\nmodel of dynamic algorithms with predictions.\nIn this context, consistency alone is just equivalent to hav ing an oﬄine algorithm that is faster\nthan the fastest known (or, even better, fastest conditiona lly possible) online algorithm.\nRobustness can often be dealt with by black-box best-of-bot h-worlds types of arguments. It\nsometimes becomes an issues in contexts where the performan ce measure of choice is the competitive\nratio, but it is rarely an issue when we optimize the running t ime. For static algorithms, one can\njust simulate two algorithms – one with predictions, and ano ther one with best known worst-case\nguarantees – step by step, in parallel, and stop whenever one of these algorithms stops. This\napproach incurs only a factor-of-two multiplicative slowd own (which is negligible for asymptotic\ncomplexity) compared to the faster algorithm, on a per-inst ance basis. For dynamic algorithms,\nwe need a more careful approach: Whenever the currently fast er algorithm ﬁnishes processing a\nrequest, stop and return its answer; when a new request comes , resume the simulation from where it\nstopped, letting the slower algorithm possibly catch up. Th is way we can retain amortized running\ntime guarantees of the better of the two algorithms, on a per- instance basis. We remark that it\nseems challenging to have a similar black-box tool for worst -case per request guarantees.\nSmoothness is perhaps the least well deﬁned of the three term s. Intuitively, we want the\nalgorithms to tolerate as big prediction errors as possible without compromising on performance too\nmuch. For dynamic algorithms with predictions, some level o f smoothness can always be achieved\ntrivially. Assuming the best oﬄine algorithm is polynomial ly faster than the best online algorithm\n– which is anyway required to claim consistency – one can alwa ys rerun the oﬄine algorithm from\nscratch after encountering each diﬀerence between the pred icted and the actual input sequence,\nand therefore tolerate some polynomial number of errors. We achieve better smoothness than this\nbaseline benchmark by (1) incorporating error measures tha t distinguish between small errors and\nlarge errors, (2) getting better dependence on them, and (3) sometimes even showing that this\ndependence is conditionally optimal.\nPredictions, Prediction Errors, and Learnability. We note that the predictions that we use,\nand error measures that quantify predictions accuracy, are standard in the learning-augmented lit-\nerature. Predictions of the entire input sequence (Theorem 1.2) are used, e.g., for scheduling\nproblems [ BMRS20 ], predicting only a certain window of input sequence (Theor em1.4) is required\nfor learning-augmented weighted caching [ JPS22 ], and predictions of the time of the next oper-\nation concerning the current item (Theorem 1.5) is the by-now-standard setup for unweighted\ncaching [ LV21 ].\nThe most ubiquitous way of measuring how for the prediction i s from the truth is the ℓ1-\ndistance (e.g., [ LV21 ,DIL+21,ACE+23], and many more), but for certain problems (e.g., ﬂow time\nminimization with uncertain processing times [ ALT21 ,ALT22 ]) the ℓ∞-error of predictions is a\nmore natural (and sometimes even necessary) choice.\nOur prediction errors can be illustrated with an example of r oad networks: Every day the same\nroads get congested during rush hour, so one can try to predic t the updates in a dynamic road\nnetwork. However, such predictions will not be perfect, bec ause the exact order in which roads\nbecome congested may diﬀer from day to day.\nSince all our predictions are essentially permutations, th e question of when such predictions can\nbe eﬃciently learned is addressed by standard tools in the li terature [ HW09 ,KBTV22 ].\n7\n\n1.3 Related Work\nOver the past couple of years the ﬁeld of learning-augmented algorithms blossomed enormously,\nand it is implausible to list here all relevant contribution s. We refer the interested reader to survey\narticles by Mitzenmacher and Vassilvitskii [ MV20 ,MV22 ] and a website with a list of papers\nmaintained by Lindermayr and Megow [ LM22 ]. There are numerous works on using predictions for\nimproving competitive ratios of online problems (e.g., [ LV21 ,PSK18 ,BMRS20 ,ACE+23,APT22 ],\nand many, many more) and running times of static problems (e. g., [DIL+21,CSVZ22 ]).\nDynamic algorithms can be seen as a certain kind of data struc tures, and there are already\nseveral examples of learning-augmented data structures (s ee, e.g., [ KBC+18,FLV21 ,LLW22 ]), but\nthey focus primarily on index data structures, such as binar y search trees, and hence they are not\ndirectly related to our work.\nA concept related to oﬄine dynamic graph algorithms is that o f a graph timeline, as deﬁned\nby [LS13 ], in which a sequence of graphs G1, . . . , G Tis given upfront and any two subsequent\ngraphs diﬀer by only one edge being added or removed. [ KL15 ] studies in this model several types\nof undirected connectivity queries over a time range, askin g, e.g., if a path exists in at least one\ngraph in a given interval.\nFinally, there is a separate line of work on temporal graphs ( see for instance the survey [ HS12 ]),\nalso related to the oﬄine model, in which each edge is labelle d with (a collection of) time intervals\nindicating when it is available. Often the goal in this line o f work is understanding certain dynamics\non networks (like information diﬀusion or convergence to ce rtain properties), which is diﬀerent from\nour computation eﬃciency objectives in the dynamic setting s.\nConcurrent Work. In the concurrent and independent work, Liu and Srinivas [ LS23 ] consider\nthe same predicted-deletions model as in our Theorem 1.5. Their result is also based on a reduction\nfrom the fully-dynamic setting to the incremental setting. However, unlike our work, their algorithm\ndoes not directly rely on a similar reduction by [ PR23 ], whereas we use analysis of [ PR23 ] as a\nblack-box. We note that [ LS23 ] present many other applications (e.g. all-pairs max-ﬂow/ min-cut\napproximation, or uniform sparsest cut) in the predicted-d eletions model that can also be derived\nfrom Theorem 1.5. On the technical side, they also show how to handle the case w here number of\nupdates Tis not known upfront, which we do not consider.\nIn another independent work, Henzinger, Saha, Seybold, and Ye [HLS+23] initiate a systematic\nstudy of the time complexity of dynamic graph algorithms wit h predictions. While their focus\nis on conditional ﬁne-grained lower bounds, they also provi de some algorithms. In particular,\ntheir combinatorial (i.e., not using fast matrix multiplic ation) algorithms for transitive closure,\napproximate APSP, and triangle detection have bounds simil ar to our Theorems 1.2and1.4but\nwith worse preprocessing times. They also consider predict ion models with error measures very\ndiﬀerent from ours.\n1.4 Notation\nWe write O(nω) for the time complexity of multiplying two n×nmatrices, where the current best\nbound is ω <2.372 [ DWZ23 ].\nFor the matrix product of rectangular matrices, we write MM( a, b, c ) for the time complexity\nof multiplying a×bandb×cmatrices. We write Ifor the identity matrix.\nAll the graphs considered in this paper are directed.\n8\n\n2 Technical Overview\nIn this section we brieﬂy explain the main ideas behind our re sults.\n2.1 Partially Dynamic Algorithms (Section 3)\nOur algorithms for partially dynamic problems – transitive closure, approximate APSP, exact SSSP\n– use a connection between these problems and the all-pairs b ottleneck paths (APBP) problem. The\nlatter is a variant of the all-pairs shortest paths (APSP) pr oblem in which, instead of minimizing\nthe sum of edge weights, we minimize the maximum edge weight a long a path. As opposed to\nAPSP, which is conjectured to require cubic time [ RZ11 ,WW18 ], APBP can be solved in strongly\nsubcubic time [ VWY07 ], and the best known APBP algorithm runs in O(n(3+ω)/2)/lessorequalslantO(n2.687)\ntime [ DP09 ].\nTransitive Closure. First, let us explain the connection of partially dynamic tr ansitive closure\nwith APBP. If we use edge insertion times as edge weights, and solve APBP, we obtain a matrix\nBsuch that\nB[u, v] = min/braceleftbigmax\ne∈P{insertion time of e}/vextendsingle/vextendsingleP∈uv-paths/bracerightbig.\nHence, B[u, v]/lessorequalslantkif and only if there is a path from utovin the graph after the ﬁrst kinsertions.\nThis observation itself is suﬃcient to solve incremental1transitive closure in the oﬄine setting (in\nother words, with perfect predictions) faster than the OMv- based cubic time lower bound for the\nonline setting.\nLet us now explain how we handle prediction errors. After eac h edge insertion, we keep track\nof the longest preﬁx of the predicted sequence of updates tha t contains only the already inserted\nedges. Let us denote the length of this preﬁx by p. We also maintain the set of “out-of-order”\nedges Eerrthat have been already inserted but are not contained in that preﬁx. Upon receiving\na reachability query ( u, v) we construct an auxiliary graph Hon at most 2|Eerr|+ 2 nodes: the\nendpoints of edges in Eerrand nodes uandv. For every pair of nodes x, y∈H, we add an edge\n(x, y) toHif (x, y)∈Eerror ifB[x, y]/lessorequalslantp. It is easy to see that there is a path from utovinHif\nand only if there is a path from utovin the original graph. Constructing Hand ﬁnding a uv-path\ntakes time O(|Eerr|2), and we show that the number of out-of-order edges |Eerr|can be bounded\nby the ℓ∞-error of the predictions. The same approach can be adapted t o the decremental setting.\nApproximate APSP. With an O(ǫ−1logn) overhead, in addition to answering queries on whether\nthere is a path from utov, we are also able to report the length of a shortest path withi n\nup to 1 + ǫmultiplicative approximation error. Instead of the single matrix B, for every d∈\n{(1 + ǫ)0,(1 + ǫ)1, . . . , (1 + ǫ)log1+ǫ(n)}, we compute matrix B(d)of bottleneck paths with up to\ndhops. Each such matrix can be computed in O(n(3+ω)/2logd) time by repeatedly squaring the\ninput weight matrix using (min ,max)-product [ DP09 ]. Note that B(d)[u, v]/lessorequalslantkif and only if there\nis a path from utovof length at most din the graph after the ﬁrst kinsertions. Now, we can\nequip the auxiliary graph Hwith edge weights corresponding to (1 + ǫ)-approximate distances in\nthe original graph, and answer the queries by running the Dij kstra algorithm in H.\n2.2 Fully Dynamic Matrix Inverse with Predictions (Section 4)\nBy using standard reductions from dynamic graph problems to dynamic matrix inverse (see, e.g.,\n[San04 ,San07 ,BNS19 ]), Theorem 5.1reduces to maintaining the matrix inverse of some matrix\n1Or decremental! In the oﬄine variant they are equivalent.\n9\n\nMundergoing rank-1 updates, i.e. updates where we are given t wo vectors u, vand then set\nM←M+uv⊤. For these reductions, it suﬃces to return v⊤M−1after each update.\nFrom Rank-1 to Entry Updates. In general, without predictions, rank-1 updates are strict ly\nharder than entry updates (i.e. updates that change only a si ngle entry of the matrix Mat a time).\nRank-1 updates require Ω( n2) update time [ HKNS15 ], whereas entry updates can be handled in\nO(n1.406) update time [ BNS19 ].\nHowever, in the prediction setting, one can actually reduce dynamic matrix inverse with rank-1\nupdates to dynamic matrix inverse with entry updates, i.e. w e can reduce updates for general dense\nuandvto the special case where uandvare sparse. (Entry updates are just the special case where\nu, vhave one non-zero entry each.)\nLet ( u(1), v(1)), . . . , (u(n), v(n)) be the next npredicted rank-1 updates.\nThen we can describe the rank-1 updates as follows. Let UandVbe the n×nmatrices\nobtained by stacking the vectors ( u(t))t=1,...,nand ( v(t))t=1,...,nnext to each other. Let Dbe a\ndiagonal matrix that is initially all zero, and consider the matrix formula:\nf(M,U,V⊤,D) =V⊤(M+UDV⊤)−1. (1)\nHere, switching the diagonal entries of Done-by-one from 0 to 1 corresponds to adding u(t)(v(t))⊤\ntoMand then inverting the result. Thus, the task of maintaining the inverse of Msubject to rank-1\nupdates, while returning ( v(t))⊤M−1after each update, can be reduced to the task of returning\nthet-th row of f(A,U,V⊤,D) subject to entry updates to D. In [ Bra21 ], v.d.Brand has shown\nthat any dynamic matrix formula that can be written using the basic matrix operations (addition,\nsubtraction, multiplication and inversion), such as formu lafin (1), reduces to dynamic matrix\ninverse again (Lemma 4.2), while supporting the same kind of updates and queries.\nThus, we must maintain the inverse of a certain matrix that is subject to entry updates while\nsupporting queries to its rows, because the input to our form ulafonly receives entry updates and\nwe only require rows of f. Since we only need to consider entry updates, that means we c an now\nfocus only on the special case where we receive rank-1 update s where both uandvare sparse with\nonly one non-zero entry each. Only if we receive an update tha t was not predicted at all, do we\nneed to perform a rank-1 update with dense vectors.\nMatrix Inverse with Predictions. Dynamic matrix inverse in the oﬄine model where all\nupdates are given ahead of time was solved in O(nω) total time by Sankowski and Mucha [ SM10 ],\nand later generalized by v.d.Brand, Nanongkai and Saranura k [BNS19 ] to only require the sequence\nof column indices of all future updates but not the actual ent ries of the new columns.\nThese previous data structures are oﬄine, i.e. require corr ect predictions about the entire update\nsequence ahead of time and cannot support updates that diﬀer from the prediction received during\ninitialization. Building on their techniques, we construc t a dynamic algorithm with predictions\nthat is robust against inaccurate predictions.\nLetMbe the dynamic input matrix. We write M(i)for a variant of Mthat is updated only\nevery 2iiterations. So M(0)is always identical to MandM(i)is identical to Mevery 2iiterations.\nWe maintain these matrices for i= 0,1, . . . , lognin the following implicit form. That is, only\nthe matrices L(i),(R(i))⊤∈Fn×2iare stored in memory where\n(M(i))−1= (M(i+1))−1(I+L(i)R(i)).\nThe matrix ( M(logn))−1is also stored explicitly in memory. Note that maintaining ( M(logn))−1\ntakes O(nω−1) amortized time, as we recompute this inverse in O(nω) time every 2logn=nupdates.\n10\n\nVia the Woodbury matrix identity, one can show (Lemma 4.5) that the matrices L(i)andR(i)\nare of the form:\nR(i)= (V(i))⊤(M(i+1))−1,L(i)=U(i)(I+U(i)R(i))−1,\nwhere U(i),V(i)are given by the at most 2ivectors u, vof the past at most 2iupdates of the form\nuv⊤by which M(i)andM(i+1)diﬀer (these vectors will have one non-zero entry each, sinc e we\nreduced to entry updates). Here R(i)is composed of some (at most 2imany) rows of ( M(i))−1,\nbecause we have entry updates, and thus we can assume each vto be a standard unit vector.\nTherefore, we can compute L(i)andR(i)inO((Ti+ MM( n,2i,2i))/2i) amortized time, where Tiis\nthe time required to obtain the at most 2irows of ( M(i+1))−1. If our predictions are correct, then\nwhen we previously computed L(i+1),R(i+1), we could have also precomputed the required rows\nof (M(i+1))−1inO(MM( n,2i+1,2i+1)) time, which is subsumed by the time required to compute\nL(i+1),R(i+1).\nThus for correct predictions, we can assume Ti=n2i. This leads to O(/summationtext\niMM( n,2i,2i)/2i) =\nO(nω−1) amortized time per update2.\nA similar idea of maintaining O(logn) copies of matrix Mthat are updated every 2iiterations\nwas also used in [ SM10 ,BNS19 ] but they did not handle incorrect predictions eﬃciently.\nNow, observe what happens in our dynamic algorithm if a predi ction is incorrect, i.e. we perform\nan update in a column that was originally predicted to occur s ome ηiterations into the future. In\nthat case the required rows of ( M(i+1))−1might not be precomputed. However, the rows are\nprecomputed in ( M(j))−1for every j≤min{logη,logn}. We can wlog assume that the rows are\nprecomputed in ( M(logn))−1because the entire inverse is computed from scratch every niterations.\nThus the missing row must only be computed in ( M(ℓ))−1forℓ= 0,1, . . . , min{logη,logn}. So we\nobtain an additional O(/summationtextmin{logη,logn}\nℓ=0n2ℓ) =O(nmin{η, n}) cost for each update that occurs η\niterations earlier than initially predicted.\nAt last, consider what happens if we perform an update that wa s not predicted at all, i.e., we\nreceive two vectors u, vfor a rank-1 update. Since the update was not predicted, the p revious\nreduction does not hold and the vectors remain dense. If vis dense, computing the respective row\nofR(i)is not just copying a row of ( M(i+1))−1but rather it requires computing a vector-matrix\nproduct. Computing this product is done recursively, i.e.,\nv⊤(M(i))−1=v⊤(M(i+1))−1(I+L(i+1)R(i+1)) =v⊤(M(logn))−1log(n)−1/productdisplay\nj=i+1(I+L(j)R(j)),\nwhich takes O(n2) operations. Note that for i= 0, this actually computes v⊤(M(j))−1for all\nj= 0,1, . . . , lognat once within O(n2) time.\n2.3 Fully Dynamic Algorithms with Predicted Deletion Times (Section 6)\nWe also consider the fully dynamic model in which prediction s give no information about insertions\nwhatsoever but the relative ordering of deletions is predic ted – by specifying for each item, at the\ntime of its insertions, the position of its future deletion. Our algorithm is based on a result by Peng\nand Rubinstein [ PR23 ]3that gives a reduction from a fully dynamic semi-online data structure, in\nwhich the order of deletions is given exactly, to an insert-only online data structure. In particular,\nassuming that the insert-only data structure has worst-case update time Γ, their semi-online data\nstructure has update time O(Γ log T) for a sequence of Tupdates. We extend this reduction to the\n2We focus here in the outline on amortized complexity, but thi s can be made worst-case (see Section 4).\n3A reduction similar to [ PR23 ], but with only an amortized update time guarantee, was also given by [ Cha11 ].\n11\n\ncase where this order of deletions is not known exactly but it is predicted with some errors. The\nerror for the i-th element is denoted by ηi, indicating that there are ηideletions that were predicted\nto happen before the deletion of element ibut will arrive after its deletion. This error incurs an\nadditional worst-case update time overhead of roughly O(ηiΓ).\nAt a high-level, the idea of Peng and Rubinstein [ PR23 ] is that if the current list of the already\nperformed insertions happens to be in the reverse order of de letion times, then a deletion can\nbe performed in time O(Γ) by simply rewinding the computation of the most recently performed\ninsertion. Moreover, at any point in time, one can rewind som e recent insertions and then re-\ninsert these elements in a diﬀerent order, to better prepare for future deletions. Since re-ordering\nthe elements at each update would be expensive, they get an am ortized bound by maintaining a\nsequence of buckets that keep partial reverse orderings. Th e amortized bound follows by ensuring\nthat a set of O(2j) elements are re-ordered in every 2jupdates for each j= 0, . . . ,⌈logT⌉. In our\ncase, when the deletion of the i-th element arrives ηipositions earlier than predicted, we rewind\nthe computation of the last ηi+ 1 insertions, until we get to delete the correct element, in time\nO(ηiΓ), and then re-insert the ηiunnecessarily deleted elements.\n3 Partially Dynamic Algorithms\nIn this section we prove our upper bounds (Theorem 1.2) and lower bounds (Theorem 1.3) for\npartially dynamic graph problems with predictions.\nFirst, let us introduce two closely related concepts – all-p airs bottleneck paths and (min ,max)-\nproduct – that we heavily use throughout the section. In the a ll-pairs bottleneck paths (APBP)\nproblem, we are given a directed graph G= (V, E) with edge weights w:E→N, and we have to\ncompute a matrix B∈NV×Vwith\nB[u, v]def= min/braceleftbigmax\ne∈Pw(e)/vextendsingle/vextendsingleP∈uv-paths/bracerightbig.\nAPBP can be solved in O(n(3+ω)/2)/lessorequalslantO(n2.687) time [ DP09 ]. The (min ,max)-product of two n×n\nmatrices A,Bis deﬁned as ( A/oveeB)[i, j]def= min kmax{A[i, k], B[k, j]}. It can also be computed in\nO(n(3+ω)/2) time [ DP09 ]. Now, let us explain the relation between the two. Consider a directed\ngraph G= (V, E) with edge weights w:E→Z, and let Wdenote the corresponding weight matrix,\ni.e.,W[u, v] =w(u, v) if (u, v)∈E,W[u, v] = +∞if (u, v)/nelementE, and W[u, u] =−∞. Observe that,\nford∈Z+, the (min ,max)-product of dcopies of Wgives all-pairs bottleneck paths with up to d\nhops:\n(W/oveeW/ovee···/oveeW/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext /bracehtipdownright/bracehtipdownleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext /bracehtipupright\ndtimes)[u, v] = min/braceleftbigmax\ne∈Pw(e)/vextendsingle/vextendsingleP∈uv-paths ,|P|/lessorequalslantd/bracerightbig.\nSuch a product can be computed by the binary exponentiation\ninO(n(3+ω)/2logd) time. For d=n, we get exactly APBP, and the extra log nfactor can be\navoided [ VWY07 ].\n3.1 Upper Bounds\nNow we proceed to describe our partially dynamic algorithm f or (1 + ǫ)-approximate APSP with\npredictions. Since transitive closure is a strictly easier problem, Theorem 1.2will follow. We begin\nwith the incremental variant. Recall that /hatwidee1,/hatwidee2, . . . , /hatwideemdenotes the predicted sequence of updates,\nande1, e2, . . . , e mthe actual one.\n12\n\nTheorem 3.1. Incremental (1 + ǫ)-approximate all-pairs shortest paths in unweighted direc ted\ngraphs with predicted sequence of edge updates can be solved by a deterministic algorithm with\nO(n(3+ω)/2log2n)preprocessing time and O(logn)worst-case edge insertion time. Each query that\nis asked between the i-th and (i+ 1)-th insertion requires O(η2\nilog log n) =O(η2\n∞log log n)time,\nwhere ηiis the current number of edges in the graph that are not contai ned in the longest preﬁx of\nthe predicted sequence that has already been inserted, i.e. ,\nηidef=i−max/braceleftbigj|{/hatwidee1,/hatwidee2, . . . , /hatwideej}⊆{ e1, e2, . . . , e i}/bracerightbig.\nBefore proving the theorem, let us explain how the above meas ure of prediction error ηi\ncan be upper bounded by the more standard ℓ∞-error, denoted by η∞. Let Π∈Smdenote\nthe permutation of predicted insertions corresponding to t he actual sequence of insertions, i.e.,\ne1, e2, . . . , e m=/hatwideeΠ(1),/hatwideeΠ(2), . . . , /hatwideeΠ(m). With this notation, we have ηi=i−max/braceleftbigj|{1,2, . . . , j}⊆\n{Π(1),Π(2), . . . , Π(i)}. Our goal is to show that ηi/lessorequalslantη∞def= max j|j−Π(j)|, for every i∈[m]. Fix\ni∈[m], and let k= Π−1(i−ηi+ 1). Note that, by deﬁnition of ηi, it holds that i−ηi+ 1/nelement\n{Π(1),Π(2), . . . , Π(i)}. In other words, k/greaterorequalslanti+ 1. Then, η∞/greaterorequalslantk−Π(k)/greaterorequalslanti+ 1−(i−ηi+ 1) = ηi,\nas desired.\nProof of Theorem 3.1.Upon receiving the predicted sequence of edge insertions, t he algorithm\ncreates a weighted directed graph with edge set E={/hatwidee1,/hatwidee2, . . . , /hatwideem}and edge weights w:E→Z\nequal to predicted insertion times, i.e., w/parenleftbig\n/hatwideei/parenrightbig=ifor every i∈[m]. Then, for every d∈{(1 +\nǫ)0,(1 + ǫ)1, . . . , (1 + ǫ)log1+ǫ(n)}, the algorithm computes matrix B(d)of bottleneck paths with up\nto⌈d⌉hops. Observe that B(d)[u, v]/lessorequalslantkif and only if there is a path from utovof length at\nmost dusing only edges from {/hatwidee1,/hatwidee2, . . . , /hatwideek}. Computing all B(d)’s takes O(n(3+ω)/2log2n) time\nin total.\nOn top of that, the algorithm creates a dictionary data struc ture (e.g., a balanced BST) that will\nallow translating edges given as pairs of nodes to their indi ces in the predicted sequence of insertions,\nand another BST that will maintain the set Sof indices of already inserted edges (initially, S=∅.\nThis ends the preprocessing phase.\nWhen an edge ( u, v) is inserted, the algorithm ﬁrst ﬁnds its index in the predic ted sequence of\ninsertions, i.e., jsuch that /hatwideej= (u, v), and then simply adds jtoS. This takes O(logn) time.\nTo handle a query ( u, v) the algorithm proceeds as follows. Let i=|S|be the number of\ninsertions so far. The algorithm ﬁrst ﬁnds the largest preﬁx of the predicted sequence of insertions\nthat has been already inserted, i.e., the largest jsuch that{1,2, . . . , j}⊆S. This is simply the\nsmallest positive integer not in Sminus one, and it can be found in O(logn) time assuming the\nBST maintains sizes and value ranges of its subtrees. Then, t he algorithm uses the BST to list\n“out-of-order” edges Eerrthat have been already inserted but are not contained in that preﬁx –\nthese correspond to elements of Slarger than j. In other words, the current edge set of the graph\nis exactly{/hatwidee1,/hatwidee2, . . . , /hatwideej}∪Eerr. Note that|Eerr|=i−j=ηi, and Eerrcan be constructed in\nO(ηilogn) time.\nNext, the algorithm creates a directed weighted auxiliary g raph H. The nodes of Hare end-\npoints of edges in the list Eerrand nodes uandv, that is at most 2 ηi+ 2 nodes in total. The edges\nofHare of two kinds. First, there are all the edges from Eerr, each with weight 1. Second, for\nevery pair of nodes x, y∈H, the algorithm selects the smallest dsuch that B(d)[x, y]/lessorequalslantj, and, if\nsuch dexists, it adds to Hedge ( x, y) with weight d. This second type of auxiliary edges represents\npaths using only edges from {/hatwidee1,/hatwidee2, . . . , /hatwideej}, and their weights are upper bounds of the path lengths\nwithin up to (1 + ǫ) multiplicative approximation error. It takes O(η2\nilog log n) time to construct\nH.\n13\n\nFinally, the algorithm ﬁnds a (weighted) shortest path from utovinH, which takes O(η2\ni)\ntime, using the Dijkstra algorithm. Let us justify that the ( weighted) length of this path dH(u, v)\nis a correct (1 + ǫ)-approximation of the (unweighted) shortest path length i n the original graph\ndG(u, v), i.e., that dH(u, v)∈[dG(u, v),(1 + ǫ)·dG(u, v)]. Clearly, dG(u, v)/lessorequalslantdH(u, v). For the\nremaining direction, ﬁx a path of length dG(u, v) in the original graph. This path can be split into\nsegments, each being either a single out-of-order edge or a s ubpath composed only of edges from\n{/hatwidee1,/hatwidee2, . . . , /hatwideej}. Every such segment is represented by an edge in Hand the weight of this edge is\nat most (1 + ǫ) times larger than the length of the segment. Hence, dH(u, v)/lessorequalslant(1 +ǫ)·dG(u, v)./square\nNow we discuss the decremental variant, which is very simila r. The main diﬀerence is that we\nwill be looking at suﬃxes of the predicted sequence of deleti ons that were not yet deleted.\nTheorem 3.2. There is a deterministic decremental algorithm for (1 + ǫ)-approximate all-pairs\nshortest paths in unweighted directed graphs with predicte d sequence of edge deletions with O(n(3+ω)/2log2n)\npreprocessing time and O(logn)worst-case edge deletion time. Each query that is asked betw een the\ni-th and (i+ 1)-st deletion requires O(η2\nilog log n) =O(η2\n∞log log n)time, where ηiis the current\nnumber of edges in the graph that are not contained in the long est suﬃx of the predicted sequence\nthat has not yet been deleted, i.e.,\nηidef= min/braceleftbigj|{/hatwideej,/hatwideej+1, . . . , /hatwideem}∩{ e1, e2, . . . , e i}=∅/bracerightbig−i−1.\nProof. The algorithm closely mimics the incremental algorithm giv en in the proof of Theorem 3.1.\nWe highlight the diﬀerences.\nIn the preprocessing phase the algorithm also computes hop- bounded bottleneck paths B(d)’s\nforO(logn) exponentially growing hop bounds d, but the diﬀerence is that now the edge weight of\nedge /hatwideeiisw(/hatwidee)i=−i. It follows that B(d)[u, v]/lessorequalslant−kif and only if there is a path from utovof\nlength at most dusing only edges from {/hatwideek,/hatwideek+1, . . . , /hatwideem}.\nSetSstill contains indices of edges present in the graph. That is , initially S= [m], and deleting\nan edge boils down to removing its index from S.\nTo handle a query, the algorithm represents the current edge set of the graph{/hatwideej,/hatwideej+1, . . . , /hatwideem}∪\nEerr, for jas small as possible. Note that |Eerr|=ηi. The auxiliary graph Hagain contains u,\nv, and endpoints of Eerr. For ( x, y)/nelementEerr, the weight of ( x, y) in His the smallest dsuch\nthat B(d)[x, y]/lessorequalslant−j, if such dexists and otherwise edge ( x, y) is not included in H. As before,\ndH(u, v)∈[dG(u, v),(1 + ǫ)·dG(u, v)].\nObserve that, as before, ηi≤η∞. /square\n3.2 Lower Bounds\nIn this section we show that the dependence of the running tim e of our partially dynamic algorithms\non the prediction error η∞is (conditionally) optimal, at least in certain parameter r egimes.\nTheorem 1.3. Unless the OMv Hypothesis fails, there is no algorithm for in cremental (decre-\nmental) transitive closure with predictions with arbitrar y polynomial prepocessing time, worst-case\nO(n1−ε)edge insertion (deletion) time, and O(η2−ε\n∞)query time, for any ε >0. This already holds\nwith respect to amortized time bounds per operation.\nProof. Henzinger et al. [ HKNS15 ] proved that the OMv hypothesis implies that the following O uMv\nproblem also cannot be solved in O(n3−ε) time, for any ε > 0, even after arbitrary polynomial\npreprocessing time. In the OuMv problem we are ﬁrst given Boo lean matrix M∈{0,1}n×n, and\n14\n\nthen we need to answer online nqueries of the form: Given two Boolean vectors ui, vi∈{0,1}n,\nwhat is the Boolean product uMv ?\nWe show how to reduce an instance of OuMv to an instance of incr emental (decremental) tran-\nsitive closure with predictions, on a graph with O(n) nodes, with a request sequence containing\nO(n2) updates and O(n) queries, and with the maximum prediction error η∞=O(n). The reduc-\ntion itself runs in O(n2) time. Therefore, under the OMv hypothesis, it cannot hold s imultaneously\nthat the preprocessing time is polynomial in n, the update time is truly sublinear in n, and the\nquery time is truly subquadratic in η∞.\nWe ﬁrst focus on the incremental variant of the problem. We wi ll think of the reduction as an\nalgorithm solving the OuMv problem and having black-box acc ess to an algorithm for incremental\ntransitive closure with predictions. (We note that our redu ction is modelled after a similar one\nin [HKNS15 , Lemma 4.7 in the arXiv version], however we need to insert ed ges on two sides of the\ngraph in order to reduce the number of queries and get a meanin gful bound.)\nUpon receiving matrix M∈{0,1}n×n, the reduction creates a graph composed of four layers\nofnnodes each, and generates a predicted sequence of edge inser tions. Let the vertex set be\nV={a1, . . . , a n}∪{b1, . . . , b n}∪{c1, . . . , c n}∪{d1, . . . , d n}, and let EMdenote the following set of\nedges between b-nodes and c-nodes, corresponding to matrix M,\nEM={(bi, cj)|(i, j)∈[n]×[n], M[i, j] = 1}.\nThe predicted sequence of edge insertions starts with all th e edges from EM, in an arbitrary ﬁxed\norder, followed by\n(a1, b1),(c1, d1),(a1, b2),(c2, d1), . . . , (a1, bn),(cn, d1),\n(a2, b1),(c1, d2),(a2, b2),(c2, d2), . . . , (a2, bn),(cn, d2),\n. . . ,\n(an, b1),(c1, dn),(an, b2),(c2, dn), . . . , (an, bn),(cn, dn).\nThe reduction gives this sequence to the algorithm to prepro cess it, and then it inserts edges EM,\nin the same order as in the sequence. This concludes the prepr ocessing phase, and the reduction\nstarts accepting queries.\nUpon receiving a pair of vectors ui, vi∈{0,1}n×n, the reduction ﬁrst inserts edges from aito\nb-nodes that correspond to ones in ui, and from c-nodes that correspond to ones in vitodi, i.e.,\n{(ai, bj)|j∈[n], ui[j] = 1}∪{ (cj, di)|j∈[n], vi[j] = 1}.\nAt this point, the graph contains a path from aitodiif and only if uMv = 1, so the reduction asks\nreachability query ( ai, di) and returns the answer. Finally, the reduction inserts rem aining edges\nfrom aitob-nodes and from c-nodes to di, i.e.,\n{(ai, bj)|j∈[n], ui[j] = 0}∪{ (cj, di)|j∈[n], vi[j] = 0},\nand it is ready to accept the next query.\nNote that, both the predicted and actual insertion time for e dge ( ai, bj) are within the range\n[|Em|+ 2n(i−1),|Em|+ 2ni], so the prediction error for such edge is at most 2 n. The same is\ntrue for edges of the form ( cj, di), and the predicted insertion times for edges between b-nodes and\nc-nodes have no error. Hence, the maximum prediction error is η∞/lessorequalslant2n, as desired.\nThe construction proving hardness of the decremental varia nt of the problem is very similar.\nThe diﬀerence is that we start with two full bipartite clique s – one between a-nodes and b-nodes,\n15\n\nthe other between c-nodes and d-nodes – and edges EMbetween b-nodes and c-nodes. Then, in\ni-th OMv query, we ﬁrst remove edges going from aiand to dicorresponding to zeros in uiandvi,\nrespectively; after that we ask the reachability query ( ai, di), and ﬁnally we remove the remaining\nedges adjacent to aianddi, which correspond to ones in uiandvi. /square\n4 Dynamic Matrix Inverse with Predictions\nIn this section we prove Theorem 4.1which is our main algebraic data structure. In Section 5we\nuse this result together with standard reductions from [ San04 ,San07 ,BNS19 ] to obtain the graph\napplications stated in Theorem 5.1.\nTheorem 4.1. There exists a data structure with the following operations .\n•Initialize Initialize on given M∈Fn×nand a queue of nrank-1 updates. Complexity O(nω).\n•AppendUpdate Append a rank-1 update (given via two vectors u, v) at the end of the queue\ninO(n)worst-case update time.\n•PerformUpdate (η)Performs the update (i.e. M←M+uv⊤) stored at the η-th position in\nthe queue, and removes it from the queue. The data structure r eturns the rank and determinant\nofM. If the matrix is invertible, it also returns the vector v⊤M′−1(where vis the vector of\nthe performed rank-1 update and M′is the matrix Mfrom before the update). The worst-case\nupdate time is O(nω−1+ min{nη, n2}).\nThe queue must have at least nupdates at all times. The data structure is randomized and it s\noutput is correct with high probability.\nNote that here ηdescribes precisely the prediction error as described in th e introduction, i.e. the\nparameter ηdescribes how much earlier an update occurs than predicted. If all updates occur\nexactly in the sequence as predicted, we always have η= 1. If an update occurs ηiterations too\nearly, then it is stored at the η-th position in the queue.\nRemark. If the matrix is promised to stay invertible throughout all u pdates, the data structure of\nTheorem 4.1can be deterministic.\n4.1 Reducing Rank-1 Updates to Column Updates\nWe reduce the general rank-1 updates as described in Theorem 4.1to column updates which are\neasier to analyze. Here by column update, we mean an update th at changes only one column at a\ntime. Such a reduction is not possible in the general setting without predictions. Rank-1 updates\nwithout predictions require Ω( n2) update time under the OMv hypothesis, but column updates ca n\nbe performed in O(n1.529) time [ BNS19 ]. However, since we are in the oﬄine/prediction settings\nwe can reduce the rank-1 updates to column updates.\nThe idea is as follows: Given a set of npredicted rank-1 updates ( ui, vi)i=1,...,n, we can construct\nU,Vby stacking the vectors next to each other. Then performing t he rank-1 updates to some\nmatrix Mcould be phrased as follows: Let Dbe an initially all-0 matrix and consider M′:=\n(M+UDV⊤). By ﬂipping the diagonal entries of Dfrom 0 to 1, the matrix M′is precisely the\nmatrix Mafter receiving the rank-1 updates. In particular, if the ﬁr stkdiagonal entries of Dare\n1, we have M′=M+/summationtextk\ni=1uiv⊤\ni. Thus a rank-1 update to Mcan be seen as a single entry update\ntoD.\n16\n\nFurther, it was shown that maintaining the value of any matri x formula f(M1, . . . , Mk) that\nconsists only of basic matrix operations (addition, subtra ction, multiplication, inversion) can be\nreduced to a single matrix inversion. That is, we can reduce t he data structure task of maintaining\nthe value of the formula\nf(M,U,V⊤,D) = ( M+UDV⊤)−1\nsubject to entry updates to D, to a data structure that maintains the inverse of some matri x subject\nto entry updates (and column updates are just a generalizati on of entry updates.).\nOnly if a rank-1 update was not predicted does an entry update toDnot suﬃce and we must\nperform an actual rank-1 update to M.\nThe following reduction is implicit from the following lemm a by v.d.Brand [ Bra21 ].\nLemma 4.2 ([Bra21 ]).Given a matrix formula f(A1, . . . , Ak)consisting of p≥k−1matrix\noperations, there exists a block matrix Bwhere some blocks are precisely A1, . . . , Akand the inverse\nB−1contains a block that is f(A1, . . . , Ak). When each Aiis at most size n×n, then Nis of size\nat most O(pn)×O(pn). The proof is constructive and constructing Ntakes time O((pn)2).\nThis reduction from predicted rank-1 updates to column upda tes motivates the following Lemma 4.3,\nwhich can be interpreted as a restriction of Theorem 4.1to column updates.\nLemma 4.3. There exists a data structure with the following operations .\n•Initialize Initialize on given M∈Fn×nand a queue of nrank-1 updates in O(nω)time.\n•AppendUpdate Append a rank-1 update (given via two vectors u, v) at the end of the queue\ninO(n)worst-case update time.\n•PerformUpdate (η,isQuery )\nPerforms the update (i.e. M←M+uv⊤) stored at the η-th position in the queue, and removes\nit from the queue. The data structure returns the determinan t ofM, and v⊤M−1where vis\nthe vector of the performed rank-1 update.\n(We can decide to only perform a query, i.e. return these valu es but do not change M.) Worst-\ncase update time is O(nω−1+ min{nη, n2})ifvwas a standard unit-vector, otherwise it is\nO(n2).\nThe queue must have at least nupdates at all times. The data structure returns “fail” for t he ﬁrst\ntime Mbecomes singular. The data structure can no longer handle an y updates after that point.\n4.2 Column Updates with Predictions\nIn this subsection, we prove Lemma 4.3. The main intermediate result is the following Lemma 4.4.\nAt the end of this subsection we prove that Lemma 4.4implies Lemma 4.3.\nLemma 4.4. There is a data structure with the following operations:\n•Initialize Initialize on given M∈Fn×nand non-empty sets F0, . . . , F logn⊂[n]where|Fi|≤\nc·2i+1forc≥1andFi⊂Fi+1inO(nω)time. These sets are predictions for the future\ncolumn updates. Set Ficontains the predicted column indices for the next 2icolumn updates.\n•QueryAndUpdate Foru, v∈Fnreturn v⊤M−1. Optionally, we can decide to set M←\nM+uv⊤.\n17\n\nIfvis some standard unit vector ejandj∈Fifor some i, then this takes O(cnω−1+n2i)\ntime. Otherwise it takes O(cnω−1+n2)time.\nIf this is the t-th update, then for all ℓwhere 2ℓdivides tthe data structure must also receive\nnew prediction sets F1, . . . , F ℓsuch that Fi⊂Fi+1for all i= 0, . . . , lognand|Fi|⊂c·2i+1.\nThe matrix Mmust stay invertible throughout all updates.\nWe brieﬂy discuss why Lemma 4.4will imply update complexities as stated in Lemma 4.3.\nThe sets ( Fi)i=0,...,lognare the sets of possible column indices where we predict the f uture 2i\ncolumns updates to be. For instance, if we are promised a sequ ence of ncolumn updates with\nj1, . . . , j nbeing their column indices (i.e. when given a queue of update s as in Lemma 4.3), then we\ncan pick Fi={j1, . . . , j2i}fori= 0, . . . , logn. However, note that we do not need such a precise\nprediction for our data structure to work. It is enough if we h ave some O(2i)-sized prediction for\nthe future 2iupdates.4\nAssuming we have the promised sequence of ncolumn updates, but some column update hap-\npens ηiterations too early (i.e. ηas in PerformUpdate in Theorem 4.1), then we can ﬁnd\nthat column index in some Fkfork/lessorequalslantmin{1 + log η,logn}. Thus such an update takes time\nO(nω−1+n2k) =O(nω−1+ min{nη, n2}) by Lemma 4.4. Note that w.l.o.g. Flogn= [n], so for any\ncolumn update, we can always assume η/lessorequalslantn. However, if an update is not a column update (i.e. if\nwe perform a general rank-1 update), the update complexity w ill be O(n2) according to Lemma 4.4.\nThe proof of Lemma 4.4relies on the following implicit representation of a matrix inverse.\nLemma 4.5. Given M∈Fn×nand a rank- kupdate U,V∈Fn×kwe have\n(M+UV⊤)−1=M−1(I+LR),\nwhere R=V⊤M−1,L=U(I+UR)−1.\nProof. The Woodbury identity [ Woo50 ] states that\n(M+UV⊤)−1=M−1−M−1U(I+V⊤M−1U)−1V⊤M−1,\nwhich implies\n(M+UV⊤)−1=M−1(I−U(I+V⊤M−1U)−1V⊤M−1)\n=M−1(I−LR).\n/square\nProof of Lemma 4.4.We start by describing a data structure with amortized time complexity and\nwill later extend it to worst-case time.\nLetM(i)initially be the matrix M, but then we update M(i)←Monly every 2icalls to\nQueryAndUpdate . So we always have M(0)=M, but M(i)might be that status of Msome\n2icalls to QueryAndUpdate ago. Throughout all updates, we represent the inverses of th ese\nmatrices in the following implicit form\n(M(i))−1= (M(i+1))−1(I+L(i)R(i)) (2)\nwhere L(i),R(i)∈Fn×2i. We do this for i= 0, . . . , logn.\n4This will later be crucial to maintain the rank of M, because the reduction from dynamic matrix rank to dynamic\nmatrix inverse performs adaptive updates that cannot be acc urately predicted.\n18\n\nInitialization. We compute M−1and store this matrix as M(logn). Then set L(i)=R(i)= 0 for\nalli.\nUpdate. Assume we receive the t-th call to QueryAndUpdate where ℓis the largest integer such\nthat 2ℓdivides t. We start describing the calculations performed by our data structure. Afterward\nwe will analyze the complexity.\nFirst, we compute v⊤(M(0))−1where vis one of the vectors describing the rank-1 update given\nby the current call to QueryAndUpdate , as we must return this result.\nFurther, the data structure performs the following operati ons to update its internal representa-\ntion of the inverse.\nTo maintain invariant ( 2) we must update M(i)for all i/lessorequalslantℓ. Note that M(ℓ+1)was last updated\n2ℓcalls to QueryAndUpdate ago, so the diﬀerence between M(ℓ)andM(ℓ+1)are only the past\n2ℓupdates. By Lemma 4.5, we can choose L(ℓ)andR(ℓ)as follows:\nLetVbe the n×2ℓmatrix, where the columns are given by the vectors vof the past 2ℓcalls\ntoQueryAndUpdate . Then set R(ℓ)=V⊤(M(ℓ+1))−1.\nLetUbe the n×2ℓmatrix, where the columns are given by the vectors uof the past 2ℓcalls to\nQueryAndUpdate . (Though all ufor which we only performed a query but no update, we will\nset the corresponding column in Uto 0 instead.) Then set L(ℓ)=U(I+UR)−1.\nThus by Lemma 4.5we have\n(M(ℓ))−1= (M(ℓ+1))−1(I+L(ℓ)R(ℓ)).\nFori < ℓ , we set L(i)=R(i)= 0 because M(i)=M(ℓ). In summary, we still satisfy ( 2).\nComplexity. For now, let us assume that we only have column updates, i.e. w e only change\none column of M. In that case we can assume vis a standard unit vector because we are adding\nuv⊤=ue⊤\njtoMto reﬂect adding uto the j-th column of Mfor some j.\nThen computing V⊤(M(ℓ))−1is equivalent to picking 2ℓrows of ( M(ℓ))−1. Let’s assume we can\nobtain all these rows within some time Tℓ. Computing R(ℓ)now takes O(MM( n,2ℓ,2ℓ)) time. So\nmaintaining the representation ( 2) for this speciﬁc ℓtakes O((Tℓ+ MM( n,2ℓ,2ℓ))/2ℓ) amortized\ntime.\nNow assume we have perfect predictions, i.e. we whenever we u pdate any M(i), we know precisely\nwhich future 2irows will be required of ( M(i))−1. Then we could precompute these 2irows whenever\nwe update M(i). In that case, Ti=O(n2i) since we just need to read the precomputed rows from\nmemory.\nWith this motivation, we always precompute the rows of ( M(ℓ))−1with row index in Fℓwhenever\nwe update the representation ( 2). This costs an additional O(cMM( n,2ℓ,2ℓ)) that amortizes over\nthe next 2ℓupdates. (Note that computing rows of ( M(ℓ))−1requires the same rows of ( M(ℓ+1))−1\nbut by Fℓ⊂Fℓ+1these rows of ( M(ℓ+1))−1have been precomputed already.)\nIn summary, if all updates occur as predicted, i.e. all futur e 2iupdates aﬀect only the columns\nwith index in Fifor all i= 0, . . . , logn, then the amortized update time would just be\nO/parenleftbiglogn/summationdisplay\ni=0cMM( n,2i,2i)/2i)/parenrightbig=O(cMM( n,2logn,2logn)/2logn) =O(cnω−1).\nNow assume there is some error in our prediction, i.e. the col umn index jof an updated column\nis not in Fifor some i. That is an issue since we need the j-th row of ( M(i))−1but that row was\nnot precomputed. To compute this row, we must spend O(n2i) time (by dimension of L(i),R(i))\nand must also compute the j-th row of ( M(i+1))−1.\n19\n\nThus by recursion, we spend O(n2k) time, where k > i is the smallest integer such that Fk\ncontains j5, because then that row of ( M(k))−1was precomputed during a previous update. Thus\nour update time increases by an additive O(n2k).\nNow let us focus on the case where vis not a standard unit vector, i.e. we perform a general\nrank-1 update instead of a column update. In that case, compu tingv⊤(M(i))−1takes O(n2i) time\nplus the time to compute v⊤(M(i+1))−1. This leads to at most O(n2) time for v⊤(M(logn))−1.\nNote that we must also return v⊤(M(0))−1after each update. This is subsumed by the O(n2k)\nandO(n2) cost above, depending on whether vis a standard unit vector or not.\nWorst-Case. The worst-case bounds are obtained via a standard technique . The idea is as\nfollows. When constructing ( M(ℓ))−1(i.e.(L(ℓ),R(ℓ)), we spread the calculations over next 2ℓ−1\ncalls to QueryAndUpdate . Thus updating ( 2) for a speciﬁc ℓintroduces only O(MM( n,2ℓ,2ℓ)/2ℓ)\nworst-case cost per call to QueryAndUpdate .\nNote that this modiﬁcation requires us to modify the recursi on of ( 2) a bit. ( M(ℓ))−1is not\nimmediately accessible as we spread its calculation over se veral updates. So ( M(ℓ−1))−1cannot\naccess ( M(ℓ))−1yet. So instead, it will access the old variant of ( M(ℓ))−1(the one we are currently\nreplacing). This means that R(ℓ−1),L(ℓ−1)must be larger by a factor of two (2ℓcolumns instead\nof 2ℓ−1) because the old version of ( M(ℓ))−1represents the matrix Msome 2ℓupdates ago.\nThis old version of ( M(ℓ))−1is accessible when we refresh ( M(ℓ−1))−1because its computation\nwas spread over 2ℓ−1updates.\nThe same is done recursively. For any i < ℓ, where ℓis the largest integer that divides t(during\nthet-th update), we set\n(M(i))−1= (M′(i+1))−1(I+L(i)R(i)),\nwhere M′(i+1)is either the old version of M(i+1), or the current version of it, if it has ﬁnished its\ncalculation.\nSince the dimensions of the L,Rmatrices increases only by a constant factor, the time com-\nplexity also increases by only a constant factor, but it is no w worst-case. /square\nWe can now prove Lemma 4.3via Lemma 4.4.\nWe restate Lemma 4.3:\nLemma 4.3. There exists a data structure with the following operations .\n•Initialize Initialize on given M∈Fn×nand a queue of nrank-1 updates in O(nω)time.\n•AppendUpdate Append a rank-1 update (given via two vectors u, v) at the end of the queue\ninO(n)worst-case update time.\n•PerformUpdate (η,isQuery )\nPerforms the update (i.e. M←M+uv⊤) stored at the η-th position in the queue, and removes\nit from the queue. The data structure returns the determinan t ofM, and v⊤M−1where vis\nthe vector of the performed rank-1 update.\n(We can decide to only perform a query, i.e. return these valu es but do not change M.) Worst-\ncase update time is O(nω−1+ min{nη, n2})ifvwas a standard unit-vector, otherwise it is\nO(n2).\nThe queue must have at least nupdates at all times. The data structure returns “fail” for t he ﬁrst\ntime Mbecomes singular. The data structure can no longer handle an y updates after that point.\n5This parameter kcan be seen as the error in our prediction, i.e. if an update ha ppens ηiterations too early, then\nwe will have j∈Fkfork <1 + log η.\n20\n\nProof of Lemma 4.3.Let us quickly recap some terminology. If vof the rank-1 update is a standard\nunit vector, then the rank-1 update is a “column update” i.e. it changes only one column. We now\ndeﬁne “the column index of the update”, i.e. we assign some in dex to each update: If the update is a\ncolumn update, we select the index of the aﬀected column. If t he update is not a column update, we\njust assign 1 as the column index. Thus “the column index of th e update” is well-deﬁned regardless\nof whether the update is a column or general rank-1 update.\nMaintaining sets Fi.We have a queue of updates Q. This queue implies the sequence of sets\nF1, . . . , F lognrequired by the data structure from Lemma 4.4. That is, at initialization (and every\n2iupdates) Ficontains the column-indices of the next 2iupdates for each i= 0, . . . , logn.\nInitialization. We are given the initial matrix Mand a queue of updates Q. This queue implies\nthe sequence of sets F1, . . . , F lognrequired by the data structure from Lemma 4.4. We initialize\nLemma 4.4onMand ( Fi)i=0,...,logn.\nAppendUpdate. We are given a new update to append to the queue. We store this f uture\nupdate in Q.\nUpdate. We are given a queue position ηand must perform the update stored at the η-th position\nin the queue. If the update was a column update, then the colum n index of the update will be\nstored in some Fifori/lessorequalslantmin{1 + log η,logn}. Thus the update takes O(nω−1+ min{nη, n2})\noperations. If this update is not a column update, then by Lem ma4.4it takes O(n2) operations.\nThe data structure of Lemma 4.4returns v⊤M−1.\nDeterminant. We have det( M+uv⊤) = det( M)·(1+v⊤M−1u). Here v⊤M−1is given to us after\neach update, so we can maintain the determinant with an extra O(n) overhead per update, which is\nsubsumed by the cost of an update to Lemma 4.4. We must compute det( M) during initialization\nwhich takes O(nω) operation, which is also subsumed by the initialization co st of Lemma 4.4./square\n4.3 Putting Everything Together\nWe now prove Theorem 4.1using Lemma 4.3and the reduction from predicted rank-1 to column\nupdates that we outlined in Section 4.2.\nProof of Theorem 4.1.Theorem 4.1is almost the same as Lemma 4.3, except that\n• The update time O(nω−1+ min{nη, n2}) of Lemma 4.3and Theorem 4.1matches only for\ncolumn updates. Rank-1 updates are slower in Lemma 4.3with O(n2) update time.\n• Theorem 4.1works on singular matrices and can maintain the rank.\nWe here describe how to extend Lemma 4.3to obtain Theorem 4.1.\nRank-1 Update Complexity. We here describe how to reduce predicted rank-1 updates to\ncolumn updates. Using this reduction, any predicted rank-1 update can be performed via a column\nupdate, we obtain O(nω−1+ min{nη, n2}) update time for predicted rank-1 updates.\nWe can phrase the future rank-1 updates to Mas follows: Consider the formula V′⊤(M+\nUDV⊤)−1. Here U,V,V′are the vectors u, vof the nfuture rank-1 updates, but V′contains one\nextra all-0 column, and Dis a diagonal matrix. Initially Dis all-0, and then one by one we set the\ndiagonal entries to 1 to perform the queued rank 1 updates. Us ing Lemma 4.2, there is a matrix B\nthat can be used to maintain the value of this formula. Since t he formula consists of 5 operations\n(1 addition, 3 products and 1 inversion), the matrix Bfrom Lemma 4.2is of size O(n)×O(n) so\nthere will be only a constant complexity blow-up.\n21\n\nAny predicted rank-1 update requires us to just change one en try of B(i.e. the one entry of the\nblock corresponding to D) so it can be performed via a column update to BinO(nω−1+min{nη, n2})\ntime via Theorem 4.1. A rank-1 update not already stored in U,Vtakes O(n2) time (by Lemma 4.4)\nbecause we perform a typical rank-1 update to M(and thus a rank-1 update to B). After every\nupdate, we output one row of V′⊤(M+UDV⊤)−1which is contained in one row of B−1(and we\nknow which row of Bthat is, by Lemma 4.2being constructive). In case of an update not stored\ninV(and thus also not stored in V′), we set the extra 0 column of V′to the vector vof the rank-1\nupdate. This, too, takes O(n2) time.\nWe restart this dynamic algorithm every n/2 updates, thus U,Vwill contain all updates that\naccording to the queue should happen within the next n/2 iterations. This takes O(nω−1) amortized\ntime per update and can be made worst-case via standard techn iques. By doing these restarts, any\nupdates at position η/lessorequalslantn/2 within the queue are stored in U,Vand thus have update time\nO(nω−1+nη). For updates at position η > n/ 2, they might not be stored in U,Vand thus are\nperformed in O(n2) time.\nIn summary, we have O(nω−1+ min{nη, n2}) update time when performing an update stored\nat the η-th position within the queue.\nRank. Sankowski [ San07 ] showed the following lemma:\nLemma 4.6 ([San07 ]).Given M∈Fn×n, let\nN:=\nM X\nY I\nI I k\n∈F3n×3n\nwhere X,Yaren×nmatrices where each entry is a uniformly at random sampled nu mber from\nF. Matrix Ikis the partial identity where only the ﬁrst kdiagonal entries are 1and the remaining\nentries are 0. Then with probability at least 1−O(n/|F|)we have that Nis full rank if and only if\nrank( M)≥n−k. Further, if k= 0, then the top-left n×nblock of N−1is precisely M−1.\nWe can reduce maintaining the rank of Mto maintaining the determinant of N(as deﬁned in\nLemma 4.6): Initially, compute the rank of Mand let k=n−rank( M). Then run the our data\nstructure on N. With each update to M, the rank can change by at most 1. If we observe that\nthe rank decreases (because the determinant of Nbecomes 0), we unroll the last update, increase\nk, and then perform the original update again. Alternatively , if the determinant did not become 0,\nwe check if the rank increased by decreasing kby 1 (i.e. one extra update to N) and check if the\ndeterminant becomes 0 (if so, revert this update again). Not e that thus for each update to M, we\nwill perform up to 2 updates to N. Further, if we look some tupdates to Minto the future, then\nthe rank can change at most by t, so we have a range of size O(t) where the future updates to Ik\nwill occur. So while we do not know the exact location of the fu ture updates, we can still construct\nthe sets ( Fi)0≤i≤lognas required by Lemma 4.4: For any 0≤i≤logn, letFibe the column indices\nof the next 2iupdates to Mand additionally the 2 ·2icolumn indices 2 n+k+jfor−2i≤j≤2i\nrepresenting the range within which we may change the Ikblock of N.\nThe failure probability of Lemma 4.6isO(n/|F|). For most of our use-cases, we will have\n|F|= poly( n). However, if|F|is not polynomial size, we can make the failure probability s ome\nsmall n−cfor any arbitrary constant c >0 by instead using some ﬁeld extension F′of polynomial\nsize. /square\n22\n\n5 Fully Dynamic Graph Algorithms with Predictions\nIn this section we prove Theorem 1.4. We restate the result here in a more detailed way, i.e., we\ndeﬁne the diﬀerent operations of the data structure and how t he predictions are given to it:\nTheorem 5.1. There exists a fully dynamic algorithm that solves the follo wing problems under\nvertex updates with predictions: triangle detection, sing le-source reachability, strong connectivity,\ndirected cycle detection, maximum matching size, number of vertex disjoint st-paths.\nThe operations of the data structure are as follows\n•Initialize Initialize on the graph and a queue of npredicted vertex updates in O(nω)time.\n•AppendUpdate Append a vertex update at the end of the queue in O(n)worst-case time.\n•PerformUpdate (η)Performs the update stored at the t-th position in the queue, and re-\nmoves it from the queue. The worst-case update time is O(nω−1+nmin{η, n}).\nThe queue must have at least nupdates at all times.\nRemark. While we state the queue to need at least nupdates, any smaller Ω(n)also works by\nrepeating each update O(1)times.\nNote that η, the position of an update in the queue, as in Theorem 5.1matches the deﬁnition\nofηbeing an error measure of the prediction. The queue can be see n as the predicted sequence of\nupdates, and if all predictions are correct, we always perfo rm the ﬁrst update in queue, i.e. η= 1.\nIf the prediction is inaccurate and some update occurs ηiteration to early, then that update is\nstored not at the front of the queue, but at position η.\nWe now prove Theorem 5.1via several reductions. Each subsection will present one re duction\nthat solves one of the graph problems stated in Theorem 5.1by reducing it to Theorem 4.1.\n5.1 Triangle Detection\nGiven graph Gand its adjacency matrix A, the number of triangles in Gis given by/summationtext\nv(A3)v,v/3,\nbecause ( A3)v,vis the number of paths from vtovusing 3 edges, i.e. the number of triangles\ncontaining v.\nAfter performing a vertex update to some v∈V, letA′be the old adjacency matrix of Gand\nAbe the new one. Then the number of triangles in Gchanges by ( A3)v,v−(A′3)v,v. Thus we can\nmaintain the number of triangles by querying only 2 entries o fA3(one before and one after the\nupdate).\nWe can maintain A3via a matrix inverse by\n\nI A\nI A\nI A\nI\n−1\n=\nI−A A2−A3\nI−A A2\nI−A\nI\n\nSo we can solve triangle detection by running Theorem 4.1on the 3 n×3nmatrix above. Any\nvertex update to Gcorresponds to updating one row and one column of A, so can be implemented\nvia 3·2 = 6 rank-1 updates to the matrix above.\n23\n\n5.2 Directed Cycle Detection\nLemma 5.2 ([BNS19 ]).LetG= (V, E)be a directed graph and A∈Fn×ns.t. each Au,vfor\n(u, v)∈Eis picked independently and uniformly at random from F, and all other entries of Aare\n0. Then with probability at least 1−n/|F|we have det(I−A) = 1 if and only if Gis acyclic.\nWe run Theorem 4.1on matrix I−Aas in Lemma 5.2. A vertex update to Gcorresponds to\nchanging one row and column of I−Aso it can be performed with two rank-1 updates.\n5.3 Single Source Reachability and Strong Connectivity\nLemma 5.3 ([San04 ]).LetG= (V, E)be a directed graphs and A∈Fn×ns.t. each Au,vfor\n(u, v)∈Eis picked independently and uniformly at random from F, and all other entries of Aare\n0. Then for any s, t∈Vwith probability at least 1−n/|F|we have (I−A)−1\ns,t/nequal0if and only if s\ncan reach t.\nWe run Theorem 4.1on matrix I−Aas in Lemma 5.3. A vertex update to Gcorresponds to\nchanging one row and column of I−Aso it can be performed with two rank-1 updates. To return\nthe single-source reachability for some source vertex s, we read the s-th row of ( I−A)−1. This can\nbe obtained by using Theorem 4.1to query e⊤\ns(I−A)−1.\nTo solve strong connectivity, not that a graph is strongly co nnected if and only if for any one\nvertex v, every other vertex can reach vandvcan reach every other vertex. Thus we can solve\nstrong connectivity by running two data structures for sing le source reachability.\n5.4 Maximum Matching Size and Counting st-Paths\nLemma 5.4 ([Lov79 ]).Given graph G= (V, E)letA∈Fn×nbe the randomized Tutte matrix.\nThat is, for each (u, v)∈EletAu,v=−Av,ube picked independently and uniformly at random\nfromF. Then for|F|=Zp(p= poly( n)) we have w.h.p. rank( A) = 2·maximum matching size.\nWe run Theorem 4.1on the Tutte matrix Aas in Lemma 5.4. A vertex update to Gcorresponds\nto changing one row and column of Aso it can be performed with two rank-1 updates.\nCounting the number of vertex disjoint st-paths can be solved via standard reduction to maxi-\nmum bipartite matching size, see e.g. [ MVV87 ].\n6 Fully Dynamic Algorithms with Predicted Deletion Times\nWe consider the model in which insertions are arriving onlin e but deletions are based on a predicted\nsequence, which we refer to as semi-online with prediction s etting. We can extend the reduction\nof [PR23 ] that gives a reduction from a fully dynamic semi-online data structure, in which the\nsequence of deletions are oﬄine, to an insert-only data structure. In particular, assuming that the\ninsert only data structure has worst-case update time Γ, the semi-online data structure of [ PR23 ]\nhas update time O(Γ log T) for a sequence of Tupdates. We observe that an adaptation of their\nresult can be used for the predicted deletion model.\nWe start by sketching their amortized semi-online to worst- case insert-only reduction and then\nexplain how this algorithm can be adapted to handle a deletio n with error ηi. Speciﬁcally in the\nrest of this section we argue that the following theorem hold s6:\n6Note that this claim holds for the problems in which the order in which elements are added does not impact the\nstate of the problem – which holds for almost all graph proble ms studied in the dynamic algorithms literature.\n24\n\nTheorem 1.5. Consider a sequence of Tupdates and suppose we are given an incremental dynamic\nalgorithm with worst-case update time Γ. Assume also that at any point in time we have a prediction\non the order of deletions of current items, such that for the iinserted item the error ηiindicates\nthe number of elements predicted to be earlier than i-th item that actually arrive later ( ηi= 0if the\nprediction is correct or the element arrives later). Then we have a fully-dynamic algorithm with\nthe following guarantees:\n•An insertion update is performed in O(Γ log2T)worst-cast time.\n•The deletion of the i-th element can be performed in O((1 + ηi)·Γ log2T)worst-case time.\nThe key idea of the reduction in [ PR23 ] is to order the list of elements in the reverse of deletion\nat any time and then perform each deletion by rewinding the co mputation (undo the insertion) in\ntime O(Γ) for the ﬁrst element in this reversed list. Since re-orde ring the elements at each update\nis expensive, they perform an amortization that performs th e re-ordering partially for a set of O(2j)\nelements in every 2jupdates for each j= 0, . . . ,⌈logT⌉. In particular, they keep the elements in\nL=⌈logT⌉+ 1 buckets B0, B1, . . . , B ⌈logT⌉such that Bjcontains the elements indexed in range\n[2j,2j+1) in the reverse order of deletion, and hence B0contains the next deletion to be performed.\nAt a high-level, for each j= 0, . . . , L , once in every 2jupdates, the algorithm re-orders a set of\nO(2j) elements (in B0∪···∪ Bj, whose total size is a sum over geometric-sized buckets) and rewinds\nthe computation on these sets in time O(Γ·2j). The algorithm ensures that Bcontains the ﬁrst\ndeletion that needs to be performed. Hence for each j= 0, . . . , L we get an amortized update time\nofO(Γ). This amortization scheme is similar to the one we use in S ection 5, with the diﬀerence\nthat here the reverse ordered deletions in buckets are utili zed and deletion is performed by a rewind\n(undo insertion) operation, which also takes O(Γ) time in the RAM model.\nWe get a similar reduction in the predicted deletion setting using the following adaptation:\nwhen the update (deletion) of the i-th element earrives η:=ηipositions earlier than predicted,\nwe rewind the computation over all the sequence of elements i n these ηpositions until we get to\nthe correct position of this element in time O(ηΓ) and then re-insert the η−1 deleted elements\nand update the lists as we would with any other insertion. In o ther words, we can simply maintain\nthe state of the algorithm of [ PR23 ] by performing O(η) rewind operations and O(η) re-insert\noperations, each of which takes O(ηΓ) time.\nNote that if an element earrives later than predicted in the sequence, we simply igno re the\ndeletion eat the predicted arrival time and process it the actual arriv al time. This will result in\nthe next elements in the sequence being shifted earlier, and the cost will be incurred to the next\nelements in the sequence, so that when the element arrives th e ordering is corrected by those earlier\nelements. We can then run the algorithm of [ PR23 ], and note that the following invariants proven\nin [PR23 ] remain unchanged.\nFor any time t∈[T], let κ(t) denote the largest integer such that tis a multiple of 2κ(t), letEt\nbe the set of elements after the t-th update and let Et,rbe the set of relements that are deleted\nﬁrst (if there are less than relements, we set Et,r=Et). Their algorithm maintains the following\ninvariants:\nLemma 6.1 ([PR23 ]).After tupdates the algorithm maintains:\n•For each j= 0, . . . , κ (t)we have Et,2j+1⊆B0∪···∪ Bj;\n•|B0∪···∪ Bκ(t)+1|/lessorequalslantO(2κ(t)).\n25\n\nHence, after performing the O(ηΓ) rewinds and re-insertion we can use Lemma 6.1to get the\namortized update time over Tupdates. At each time t∈[T] we need to process the union of\nbuckets B0∪..∪Bκ(t), and by summing over the bucket sizes we have:\n1\nTT/summationdisplay\nt=1O(2κ(t)Γ +ηΓ) = O/parenleftbig(1\nTL/summationdisplay\nκ(t)=02κ(t)·T\n2κ(t))Γ + ηLΓ/parenrightbig=O((η+ 1)Γ log T)\nThis algorithm is then de-amortized to get a worst-case boun d with an additional log factor.\nAt a high-level, the goal is to slowly perform the longer sequ ence of re-ordering and re-inserting\noperation over the updates. The challenge is that the upcomi ng insertions interleave the scheduled\nre-orderings. To handle this they distribute and preproces s the re-ordering tasks to O(logT) threads\nthat further divide the updates into smaller geometrically decreasing sized epochs . We refer the\nreaders to [ PR23 ] for further details on the de-amortization details. They s how that we can perform\neach update (without error) in worst-case O(Γ log2T) time. Our adaptation adds O(η) rewind\noperations that take O(ηΓ) time, and introduces additional O(η) insertions, which also take in\ntotal O(Γ log2T) time, and thus the worst-case time is O(Γ log2T(η+ 1)).\n6.1 Application to All-Pairs Shortest Paths with Vertex Upd ates\nWe ﬁrst observe that we can use the reduction of [ PR23 ], combined with an incremental APSP\nalgorithm based on Floyd-Warshall, ﬁrst observed by Thorup [Tho05 ], to get a fully dynamic semi-\nonline APSP algorithm with O(n2log2n) worst-case update time. Note that here we can bound\nT≤nsince at any point we can restrict our attention to a set of at m ostnvertices inserted.\nObservation 6.2 ([Tho05 ]).Given an edge-weighted directed graph undergoing online ve rtex inser-\ntions, there is a deterministic algorithm that maintains ex act all-pairs shortest paths in this graph\nwith O(n2)worst-case update time.\nCorollary 6.3 ([PR23 ,Tho05 ]).Given an edge-weighted directed graph undergoing online ve rtex\ninsertions and oﬄine (known) vertex deletions, there is a de terministic algorithm that maintains\nexact all-pairs shortest paths in this graph with O(n2log2n)worst-case update time.\nWe can extend this to the setting in which the insertions are f ully online, but the deletions are\npredicted, with the error measure described using Theorem 1.5.\nTheorem 1.6. Given a weighted and directed graph undergoing online verte x insertions and pre-\ndicted vertex deletions, we can maintain exact weighted all -pairs shortest paths with the following\nguarantees:\n•An insertion update can be performed O(n2log2n)worst-cast time.\n•A deletion of the i-th inserted vertex vican be performed in O(n2(ηilog2n+ 1)) worst-case\ntime, where error ηi∈[0, n]indicates how many vertices were predicted to be deleted bef orevi\nthat are actually deleted after vi.\n7 Acknowlegment\nThe authors would like to thank Nicole Megow and Danupon Nano ngkai for inspiring discussions\non algorithms with predictions.\n26\n\nReferences\n[Abb22] Amir Abboud. Personal communication, 2022.\n[ACE+23] Antonios Antoniadis, Christian Coester, Marek Eliás, A dam Polak, and Bertrand Si-\nmon. Online metric algorithms with untrusted predictions. ACM Trans. Algorithms ,\n19(2):19:1–19:34, 2023. doi:10.1145/3582689 .\n[ACK17] Ittai Abraham, Shiri Chechik, and Sebastian Krinni nger. Fully dynamic all-pairs short-\nest paths with worst-case update-time revisited. In Proceedings of the Twenty-Eighth\nAnnual ACM-SIAM Symposium on Discrete Algorithms , pages 440–452. SIAM, 2017.\n[ALT21] Yossi Azar, Stefano Leonardi, and Noam Touitou. Flo w time scheduling with uncertain\nprocessing time. In STOC ’21: 53rd Annual ACM SIGACT Symposium on Theory of\nComputing , pages 1070–1080. ACM, 2021. doi:10.1145/3406325.3451023 .\n[ALT22] Yossi Azar, Stefano Leonardi, and Noam Touitou. Dis tortion-oblivious algo-\nrithms for minimizing ﬂow time. In Proceedings of the 2022 ACM-SIAM\nSymposium on Discrete Algorithms, SODA 2022 , pages 252–274. SIAM, 2022.\ndoi:10.1137/1.9781611977073.13 .\n[APT22] Yossi Azar, Debmalya Panigrahi, and Noam Touitou. O nline graph algorithms with\npredictions. In Proceedings of the 2022 ACM-SIAM Symposium on Discrete Algo rithms,\nSODA 2022 , pages 35–66. SIAM, 2022. doi:10.1137/1.9781611977073.3 .\n[AW14] Amir Abboud and Virginia Vassilevska Williams. Popu lar conjectures imply strong\nlower bounds for dynamic problems. In 55th IEEE Annual Symposium on Founda-\ntions of Computer Science, FOCS 2014 , pages 434–443. IEEE Computer Society, 2014.\ndoi:10.1109/FOCS.2014.53 .\n[Ber13] Aaron Bernstein. Maintaining shortest paths under deletions in weighted directed\ngraphs: [extended abstract]. In Symposium on Theory of Computing Conference,\nSTOC’13 , pages 725–734. ACM, 2013. doi:10.1145/2488608.2488701 .\n[BHS07] Surender Baswana, Ramesh Hariharan, and Sandeep Se n. Improved decre-\nmental algorithms for maintaining transitive closure and a ll-pairs shortest\npaths. Journal of Algorithms , 62(2):74–92, 2007. Announced at STOC 2002.\ndoi:10.1016/j.jalgor.2004.08.004 .\n[BKN19] Karl Bringmann, Marvin Künnemann, and André Nusser . Fréchet distance under trans-\nlation: Conditional hardness and an algorithm via oﬄine dyn amic grid reachability. In\nProceedings of the Thirtieth Annual ACM-SIAM Symposium on D iscrete Algorithms\n(SODA 2019) , pages 2902–2921. SIAM, 2019. doi:10.1137/1.9781611975482.180 .\n[BLN+20] Jan van den Brand, Yin Tat Lee, Danupon Nanongkai, Richar d Peng, Thatchaphol\nSaranurak, Aaron Sidford, Zhao Song, and Di Wang. Bipartite matching in nearly-\nlinear time on moderately dense graphs. In FOCS , pages 919–930. IEEE, 2020.\n[BMRS20] Étienne Bamas, Andreas Maggiori, Lars Rohwedder, and Ola Svensson.\nLearning augmented energy minimization via speed scaling. InAdvances\nin Neural Information Processing Systems 33: Annual Confer ence on Neu-\nral Information Processing Systems 2020, NeurIPS 2020 , 2020. URL:\nhttps://proceedings.neurips.cc/paper/2020/hash/af94 ed0d6f5acc95f97170e3685f16c0-Abstrac \n27\n\n[BNS19] Jan van den Brand, Danupon Nanongkai, and Thatchaph ol Saranurak. Dynamic matrix\ninverse: Improved algorithms and matching conditional low er bounds. In FOCS , pages\n456–480. IEEE Computer Society, 2019.\n[Bra21] Jan van den Brand. Unifying matrix data structures: Simplifying and speeding up\niterative algorithms. In SOSA , pages 1–13. SIAM, 2021.\n[CGH+20] Li Chen, Gramoz Goranci, Monika Henzinger, Richard Peng , and Thatchaphol Sara-\nnurak. Fast dynamic cuts, distances and eﬀective resistanc es via vertex sparsiﬁers. In\n61st IEEE Annual Symposium on Foundations of Computer Scien ce, FOCS 2020 , pages\n1135–1146. IEEE, 2020. doi:10.1109/FOCS46700.2020.00109 .\n[Cha11] Timothy M Chan. Three problems about dynamic convex hulls. In Proceedings of the\ntwenty-seventh annual symposium on Computational geometr y, pages 27–36, 2011.\n[CKL+22] Li Chen, Rasmus Kyng, Yang P. Liu, Richard Peng, Maximili an Probst Gutenberg, and\nSushant Sachdeva. Maximum ﬂow and minimum-cost ﬂow in almos t-linear time. In\nFOCS , pages 612–623. IEEE, 2022.\n[CSVZ22] Justin Y. Chen, Sandeep Silwal, Ali Vakilian, and F red Zhang. Faster fundamental graph\nalgorithms via learned predictions. In International Conference on Machine Learning,\nICML 2022 , volume 162 of Proceedings of Machine Learning Research , pages 3583–3602.\nPMLR, 2022. URL: https://proceedings.mlr.press/v162/chen22v.html .\n[CZ23] Shiri Chechik and Tianyi Zhang. Faster deterministi c worst-case fully dynamic all-pairs\nshortest paths via decremental hop-restricted shortest pa ths. In Proceedings of the 2023\nAnnual ACM-SIAM Symposium on Discrete Algorithms (SODA) , pages 87–99. SIAM,\n2023.\n[DI00] Camil Demetrescu and Giuseppe F. Italiano. Fully dyn amic transitive closure:\nBreaking through the o(n2) barrier. In Proceedings of the 41st Annual Sympo-\nsium on Foundations of Computer Science (FOCS 2000) , pages 381–389, 2000.\ndoi:10.1109/SFCS.2000.892126 .\n[DI06] Camil Demetrescu and Giuseppe F. Italiano. Fully dyn amic all pairs shortest paths\nwith real edge weights. Journal of Computer and System Sciences , 72(5):813–837, 2006.\nAnnounced at FOCS 2001. doi:10.1016/j.jcss.2005.05.005 .\n[DIL+21] Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei\nVassilvitskii. Faster matchings via learned duals. In Advances in Neural\nInformation Processing Systems 34: Annual Conference on Ne ural Informa-\ntion Processing Systems 2021, NeurIPS 2021 , pages 10393–10406, 2021. URL:\nhttps://proceedings.neurips.cc/paper/2021/hash/5616 060fb8ae85d93f334e7267307664-Abstrac \n[DP09] Ran Duan and Seth Pettie. Fast algorithms for (max, mi n)-matrix multiplica-\ntion and bottleneck shortest paths. In Proceedings of the Twentieth Annual ACM-\nSIAM Symposium on Discrete Algorithms, SODA 2009 , pages 384–391. SIAM, 2009.\ndoi:10.1137/1.9781611973068.43 .\n[DWZ23] Ran Duan, Hongxun Wu, and Renfei Zhou. Faster matrix multiplication via asymmetric\nhashing. In 64th IEEE Annual Symposium on Foundations of Computer Scien ce, FOCS\n2023. IEEE, 2023.\n28\n\n[EFS+22] Jon C. Ergun, Zhili Feng, Sandeep Silwal, David P. Woodru ﬀ, and Samson\nZhou. Learning-augmented $k$-means clustering. In The Tenth International Con-\nference on Learning Representations, ICLR 2022 . OpenReview.net, 2022. URL:\nhttps://openreview.net/forum?id=X8cLTHexYyY .\n[Epp94] David Eppstein. Oﬄine algorithms for dynamic minim um spanning tree problems. J.\nAlgorithms , 17(2):237–250, 1994. doi:10.1006/jagm.1994.1033 .\n[FLV21] Paolo Ferragina, Fabrizio Lillo, and Giorgio Vinci guerra. On the perfor-\nmance of learned data structures. Theor. Comput. Sci. , 871:107–120, 2021.\ndoi:10.1016/j.tcs.2021.04.015 .\n[GLNS22] Buddhima Gamlath, Silvio Lattanzi, Ashkan Norouz i-Fard, and Ola Svensson. Approx-\nimate cluster recovery from noisy labels. In Conference on Learning Theory , volume\n178 of Proceedings of Machine Learning Research , pages 1463–1509. PMLR, 2022. URL:\nhttps://proceedings.mlr.press/v178/gamlath22a.html .\n[GWN20] Maximilian Probst Gutenberg and Christian Wulﬀ-Ni lsen. Fully-dynamic all-pairs short-\nest paths: Improved worst-case time and space bounds. In Proceedings of the Fourteenth\nAnnual ACM-SIAM Symposium on Discrete Algorithms , pages 2562–2574. SIAM, 2020.\n[HKNS15] Monika Henzinger, Sebastian Krinninger, Danupon Nanongkai, and Thatchaphol Sara-\nnurak. Unifying and strengthening hardness for dynamic pro blems via the online\nmatrix-vector multiplication conjecture. In Proceedings of the Forty-Seventh Annual\nACM on Symposium on Theory of Computing, STOC 2015 , pages 21–30. ACM, 2015.\ndoi:10.1145/2746539.2746609 .\n[HLS+23] Monika Henzinger, Andrea Lincoln, Barna Saha, Martin P S eybold, and Christopher Ye.\nOn the complexity of algorithms with predictions for dynami c graph problems. arXiv\npreprint arXiv:2307.16771 , 2023.\n[HS12] Petter Holme and Jari Saramäki. Temporal networks. Physics reports , 519(3):97–125,\n2012.\n[HW09] David P. Helmbold and Manfred K. Warmuth. Learning pe rmutations with exponential\nweights. J. Mach. Learn. Res. , 10:1705–1736, 2009. doi:10.5555/1577069.1755841 .\n[IK83] Toshihide Ibaraki and Naoki Katoh. On-line computat ion of transitive\nclosures of graphs. Information Processing Letters , 16(2):95–97, 1983.\ndoi:10.1016/0020-0190(83)90033-9 .\n[Ita86] Giuseppe F. Italiano. Amortized eﬃciency of a path r etrieval data structure. Theoretical\nComputer Science , 48(3):273–281, 1986. doi:10.1016/0304-3975(86)90098-8 .\n[JPS22] Zhihao Jiang, Debmalya Panigrahi, and Kevin Sun. On line algorithms for\nweighted paging with predictions. ACM Trans. Algorithms , 18(4):39:1–39:27, 2022.\ndoi:10.1145/3548774 .\n[Kav14] Telikepalli Kavitha. Dynamic matrix rank with part ial lookahead. Theory Comput.\nSyst., 55(1):229–249, 2014.\n29\n\n[KBC+18] Tim Kraska, Alex Beutel, Ed H. Chi, Jeﬀrey Dean, and Neokl is Polyzotis. The\ncase for learned index structures. In Proceedings of the 2018 International Confer-\nence on Management of Data, SIGMOD Conference 2018 , pages 489–504. ACM, 2018.\ndoi:10.1145/3183713.3196909 .\n[KBTV22] Misha Khodak, Maria-Florina Balcan, Ameet Talwal kar, and Sergei Vassilvit-\nskii. Learning predictions for algorithms with prediction s. In NeurIPS , 2022. URL:\nhttp://papers.nips.cc/paper_files/paper/2022/hash/1 7061a94c3c7fda5fa24bbdd1832fa99-Abst \n[KL15] Adam Karczmarz and Jakub Lacki. Fast and simple conne ctivity in graph time-\nlines. In Algorithms and Data Structures - 14th International Sympos ium, WADS 2015 ,\nvolume 9214 of Lecture Notes in Computer Science , pages 458–469. Springer, 2015.\ndoi:10.1007/978-3-319-21840-3\\_38 .\n[Lac13] Jakub Lacki. Improved deterministic algorithms fo r decremental reachability and\nstrongly connected components. ACM Transactions on Algorithms , 9(3):27:1–27:15,\n2013. Announced at SODA 2011. doi:10.1145/2483699.2483707 .\n[LLW22] Honghao Lin, Tian Luo, and David P. Woodruﬀ. Learnin g augmented binary search\ntrees. In International Conference on Machine Learning, ICML 2022 , volume 162 of\nProceedings of Machine Learning Research , pages 13431–13440. PMLR, 2022. URL:\nhttps://proceedings.mlr.press/v162/lin22f.html .\n[LM22] Alexander Lindermayr and Nicole Megow. Algorithms w ith predictions.\nhttps://algorithms-with-predictions.github.io , 2022. Accessed 8 July 2023.\n[Lov79] László Lovász. On determinants, matchings, and ran dom algorithms. In FCT, pages\n565–574. Akademie-Verlag, Berlin, 1979.\n[LS13] Jakub Lacki and Piotr Sankowski. Reachability in gra ph timelines. In Proceedings of the\n4th conference on Innovations in Theoretical Computer Scie nce, pages 257–268, 2013.\n[LS23] Quanquan C Liu and Vaidehi Srinivas. The predicted-d eletion dynamic model: Taking\nadvantage of ml predictions, for free. arXiv preprint arXiv:2307.08890 , 2023.\n[LV21] Thodoris Lykouris and Sergei Vassilvitskii. Compet itive caching with machine learned\nadvice. J. ACM , 68(4):24:1–24:25, 2021. doi:10.1145/3447579 .\n[Mao23] Xiao Mao. Fully-dynamic all-pairs shortest paths: Likely optimal worst-case update\ntime. arXiv preprint arXiv:2306.02662 , 2023.\n[MV20] Michael Mitzenmacher and Sergei Vassilvitskii. Alg orithms with predictions. In Tim\nRoughgarden, editor, Beyond the Worst-Case Analysis of Algorithms , pages 646–662.\nCambridge University Press, 2020. doi:10.1017/9781108637435.037 .\n[MV22] Michael Mitzenmacher and Sergei Vassilvitskii. Alg orithms with predictions. Commun.\nACM , 65(7):33–35, 2022. doi:10.1145/3528087 .\n[MVV87] Ketan Mulmuley, Umesh V. Vazirani, and Vijay V. Vazi rani. Matching is as easy as\nmatrix inversion. In STOC , pages 345–354. ACM, 1987.\n30\n\n[NCN23] Thy Dinh Nguyen, Anamay Chaturvedi, and Huy L. Nguye n. Improved learning-\naugmented algorithms for k-means and k-medians clustering . In The Eleventh Inter-\nnational Conference on Learning Representations, ICLR 202 3. OpenReview.net, 2023.\nURL: https://openreview.net/pdf?id=dCSFiAl_VO3 .\n[Păt10] Mihai Pătraşcu. Towards polynomial lower bounds fo r dynamic problems. In Leonard J.\nSchulman, editor, Proceedings of the 42nd ACM Symposium on Theory of Computing ,\nSTOC 2010 , pages 603–610. ACM, 2010. doi:10.1145/1806689.1806772 .\n[PR23] Binghui Peng and Aviad Rubinstein. Fully-dynamic-t o-incremental reductions with\nknown deletion order (eg sliding window). In Symposium on Simplicity in Algorithms\n(SOSA) , pages 261–271. SIAM, 2023.\n[PSK18] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Impr oving online al-\ngorithms via ML predictions. In Advances in Neural Information Pro-\ncessing Systems 31: Annual Conference on Neural Informatio n Pro-\ncessing Systems 2018, NeurIPS 2018 , pages 9684–9693, 2018. URL:\nhttps://proceedings.neurips.cc/paper/2018/hash/73a4 27badebe0e32caa2e1fc7530b7f3-Abstrac \n[PSS19] Richard Peng, Bryce Sandlund, and Daniel Dominic Sl eator. Optimal oﬄine dynamic\n2, 3-edge/vertex connectivity. In Algorithms and Data Structures - 16th International\nSymposium, WADS 2019 , volume 11646 of Lecture Notes in Computer Science , pages\n553–565. Springer, 2019. doi:10.1007/978-3-030-24766-9\\_40 .\n[PvL87] Johannes A. La Poutré and Jan van Leeuwen. Maintenan ce of transitive closures and\ntransitive reductions of graphs. In Proceedings of the International Workshop on Graph-\nTheoretic Concepts in Computer Science Graph-Theoretic Co ncepts in Computer Sci-\nence (WG 1987) , volume 314, pages 106–120, 1987. doi:10.1007/3-540-19422-3\\_9 .\n[RZ08] Liam Roditty and Uri Zwick. Improved dynamic reachab ility algorithms for directed\ngraphs. SIAM Journal on Computing , 37(5):1455–1471, 2008. Announced at FOCS\n2002. doi:10.1137/060650271 .\n[RZ11] Liam Roditty and Uri Zwick. On dynamic shortest paths problems. Algorithmica ,\n61(2):389–401, 2011. Announced at ESA 2004. doi:10.1007/s00453-010-9401-5 .\n[San04] Piotr Sankowski. Dynamic transitive closure via dy namic matrix inverse (extended\nabstract). In FOCS , pages 509–517. IEEE Computer Society, 2004.\n[San07] Piotr Sankowski. Faster dynamic matchings and vert ex connectivity. In SODA , pages\n118–126. SIAM, 2007.\n[SM10] Piotr Sankowski and Marcin Mucha. Fast dynamic trans itive closure with lookahead.\nAlgorithmica , 56(2):180–197, 2010. doi:10.1007/s00453-008-9166-2 .\n[Tho05] Mikkel Thorup. Worst-case update times for fully-d ynamic all-pairs shortest paths. In\nProceedings of the Thirty-Seventh Annual ACM Symposium on T heory of Computing ,\nSTOC ’05, page 112–119, New York, NY, USA, 2005. Association for Computing Ma-\nchinery. doi:10.1145/1060590.1060607 .\n[VWY07] Virginia Vassilevska, Ryan Williams, and Raphael Y uster. All-pairs bottleneck\npaths for general graphs in truly sub-cubic time. In Proceedings of the 39th\n31\n\nAnnual ACM Symposium on Theory of Computing , pages 585–589. ACM, 2007.\ndoi:10.1145/1250790.1250876 .\n[Woo50] Max A Woodbury. Inverting modiﬁed matrices . Statistical Research Group, 1950.\n[WW18] Virginia Vassilevska Williams and R. Ryan Williams. Subcubic equivalences between\npath, matrix, and triangle problems. Journal of the ACM , 65(5):27:1–27:38, 2018. An-\nnounced at FOCS 2010. doi:10.1145/3186893 .\n32",
  "textLength": 107901
}