{
  "paperId": "5cf0257c3d0b9580bc3a287359328588c8f45647",
  "title": "Machine learning and high dimensional vector search",
  "pdfPath": "5cf0257c3d0b9580bc3a287359328588c8f45647.pdf",
  "text": "arXiv:2502.16931v1  [cs.LG]  24 Feb 2025Machine learning and high dimensional vector search\nMatthijs Douze, Meta FAIR\nAbstract\nMachine learning and vector search are two research topics that d eveloped in parallel in nearby\ncommunities. However, unlike many other ﬁelds related to big data, m achine learning has not\nsigniﬁcantly impacted vector search. In this opinion paper we attem pt to explain this oddity. Along\nthe way, we wander over the numerous bridges between the two ﬁe lds.\n1 Introduction\nMost high-dimensional vector search methods are based on st atistical tools, signal processing ap-\nproachesorgraphtraversal algorithms. Statistical tools includerandomprojections [15], dimensionality\nreduction (PCA and the SVD). Signal processing is employed p rimarily to compress vectors with quan-\ntization [30, 4, 22] Most recent indexing methods are rely on graphs [34, 49, 3, 11] that are built with\ngraph traversal heuristics.\nVector search (VS) is used in machine learning (ML) for train ing data deduplication [39] and\nsearching ML embeddings [28, 5]. Therefore, there are many r esearch teams around the world that are\ncompetent in both ﬁelds.\nIn their seminal work The case for learned indexing structures [32], Kraska et al. introduced a series\nof ML based algorithms to speed up classical indexing struct ures like hash tables and B-trees. These\nstructures are building blocks of databases, e.g. a B-tree c an be seen as a one dimensional VS index.\nThey hoped to open “an entirely new research direction for a d ecades old ﬁeld”.\nHowever, 7 years later, it is striking that ML has had very lit tle impact on VS. No ML based\nmethod appears in the current VS benchmarks at any scale [2, 4 7, 46]. The most advanced ML tool\nthat is widely used for VS is the k-means clustering algorith m. There is no deep learning, no use of\nhigh-capacity networks.\nIn the following, we attempt to explain why this is the case. W e work out a typical use case of\nVS (Section 2), then expose the fundamental limits of ML for V S (Section 3); in Section 4, we review\ninteresting applications in the other direction: VS for ML.\n2 Information retrieval with machine learning\nLet’s start from a typical application of vector search: ima ge retrieval. This is originally a classical ML\nproblem, and we show how it converts to VS.\nWe manage a collection of NimagesC={x1,...,x N}, whereNis large, and possibly growing.\nFrom a given query image q, the objective is to ﬁnd images in Cthat are relevant to it, for example\nbecause they represent the same object.\nN-way classiﬁer. The most straightforward ML approach for this is to train a N-way classiﬁer\nf1(q)∈[0,1]Nthat outputs a scalar score close to 1 for the images that are r elevant and close to 0\n1\n\nfor the others. The best matching item from the collection ca n be found without even accessing the\nimages after the training phase:\nI1= argmax f1(q). (1)\nThemodelcouldbeanystandardimageclassiﬁcation model, c onvolutional ortransformer-based[25,\n19]. Note that processing images is inherently slow, since t he raw pixels must be converted into\nsemantically relevant information, which requires severa l neural net layers. More importantly, the\nmodel’s size needs to be proportional to the collection size N, and it is impossible to add or remove\nimages from the collection after the model was trained.\nPairwise comparison. Another approach is to design a function f2that, given ( x,x′) returns a\nscore that is high if x′relevant to xand low otherwise. At search time, f2is used to compare the query\nqwith all the images from the collection (which must be access ible):\nI2= argmax\ni=1,...,Nf2(q,xi). (2)\nTheusuallearningmachinerycanbedeployedtotrainthisfu nction: givenatrainingsetofmatching\nand non-matching images, the model is optimized to return 0 o r 1 with, for example, a binary cross-\nentropy loss.\nThis resolves the problem of the model size, that remains ﬁxe d, and adding/removing from the\ncollection is painless. The issue with this approach is that the model f2is still expensive to evaluate,\nand querying one image requires Nevaluations (forward passes) of f2. Therefore it is not tractable\nbeyond a few hundred images.\nEmbeddings. A way to avoid this is to force f2to decompose as:\nf3(x,x′) =S(E(x),E(x′)), (3)\nwhereE, the embedder (or feature extractor) is a function that comp utes a vector in ddimensions\nfrom an image, and S:Rd×Rd→Ris a similarity function between embeddings. This is akin to\nkernel methods [6, chapter 7], and sometimes called a “two-t ower” network.\nThe feature extraction function Ecan be costly to evaluate, but it is performed beforehand, wh en\nthe image is added to the collection: ( e1,...,eN) = (E(x1),...,E(xN)) is precomputed. At search\ntime, we perform only simple vector comparisons with all N:\nI3= argmax\ni=1,...,NS(E(q),ei) (4)\nFrom a ML point of view, since we impose a constraint on the fun ctionf3, it is bound to be less\naccurate thanthefunction f2, that performsadirect comparison between items: thereis a n information\nbottleneck.\nThe functions SandEare bound by an embedding contract [20], that states that Eshould generate\nembeddings such that the comparison with Sis semantically meaningful.\nEmbedding extraction. There is a vast literature on embedding extraction for image s. It started\nin the pre-deep era [31, 40]. Later, image embedding models w ere ﬁrst sampled from some layer of\na classiﬁcation model [5], then trained speciﬁcally using a clustering supervision [9], or variants of\ncontrastive learning [16, 10, 12, 29]. The embeddings can be specialized for diﬀerent granularities of\nimage matching: from class-level via instance-level to cop y-level [7, 41].\n2\n\nEmbeddings can be extracted from other modalities as well, w ith slight variations. For text embed-\ndings (like BERT [17, 27]) the size of the embedding vector is usuallylargerthan the original text –\nembeddings are not guaranteed to compress the items they are computed from. These text embeddings\nare the basis for the retrieval augmented generation for LLM s [28]. Text and image embeddings can\nbe matched together as with the CLIP embeddings [43]. Recomm endation systems are often based on\ntwo-tower networks, with diﬀerent embedding functions for t he users and the items to recommend [38].\n3 Vector search with machine learning\nPerforming the embedding search of Equation 4 is an operatio n whose complexity is O(N×d) for most\nsimilarity functions. The research on vector search aims at reducing the runtime when Nbecomes\nlarge, either by making it sub-linear or by reducing the line ar factor. In the process, some accuracy\ncan be sacriﬁced, and the brute force search of Equation 4 bec omes an Approximate Nearest Neighbor\nSearch (ANNS) task.\nVector search is a linear classiﬁer. Let’s assume that the similarity function, in Equation (4) i s\na dot product1. In that case, the operation to be performed is a matrix-vect or multiplication between\nE(q) and the matrix that stacks embeddings [ e1,...,eN]. This is equivalent to a linear layer of size\nN×d. This means that vector search can be solved with the simples t of machine learning tools: a\nlinear classiﬁer of size N.\nThis equivalence between vector search and classiﬁcation i s used for k-NN classiﬁers [36, 10, 42]\n(but k-NN classiﬁers are less accurate than training linear classiﬁers end-to-end).\nA fundamental divergence. ANNS and ML both strive to optimize an accuracy objective. Ho w-\never, vector search starts where machine learning stops. Th e problems to handle when training a\nmachine learning model are the train-test discrepancy, ove rﬁtting, regularization, etc. Resource uti-\nlization is a second thought. Vector search does not have any of these problems, because computing\nthe perfect result is straightforward (Equation 4). The pro blem of vector search is how to compute the\nresult accurately with limited resources.\nThis explains why directly applying ML does not solve VS: as s oon as ML starts to apply operations\nat scale N, it is already slower than performing burte-force VS. This o bservation was already done\nin [32]. Their approach is to break down the indexing structu res into a distribution modeling part,\nthat can be solved by ML and the indexing structure itself, th at scales to size N.\nML for vector distribution modeling. A direct application of this approach for VS is to train a\ntransformation that maps vectors to a space that is easier to index [45]. In this approach the input\nvectors are transformed into a more uniform space while main taining neighborhood relations; a lattice\nquantizer is then used to encode the uniform vectors.\nModeling the vector distribution can be used to apply lossy c ompression to vectors, which is equiv-\nalent to quantization. The k-means algorithm is a strong bas eline for quantization because it obeys\nLloyd’s conditions and is eﬃcient. In fact, the way VQ-VAEs a re trained, with an exponential moving\naverage, is similar to online k-means [44].\nHowever, to scale it to more accurate compression, k-means m ust be applied several times (multi-\ncodebook quantization), which yields less optimal represe ntations like product quantization [30] or\nadditive quantization [4].\n1This is without loss of generality: most standard distances can be reduced to computing dot products in a transformed\nspace [20].\n3\n\nDeep learning models can be applied to improve multi-codebo ok quantization. This is done in the\nUNQ quantizer [37], where a learned transformation maps the vectors to a space where it is easier to\napply quantization. In the QINCo series of works [26, 50] the codebooks of a residual quantizer are\nadapted using a neural net to better approximate the vector d istribution. One interesting work shows\nthat, given a quantizer, it is possible to train a decoder tha t improves its accuracy [1].\nOne important family of VS methods is to partition the vector space. At search time, only a subset\nof partitions are visited. Usually this partitioning is bas ed on k-means [30]. However, the partitions\ncan also be predicted from a vector with a classiﬁer [18, 35].\nDiscussion. These approaches show that there are operating points where ML can help improving\nVS. However, in many cases, the practitioner hits hard limit s. The ﬁrst limit is that the runtime of\nthe quantization must remain below that of a brute-force vec tor search. The second limit is that often,\nincreasing the capacity of the model does not improve the ﬁt o f the vector distribution: a shallow model\n(or k-means itself) is hard to outperform with deeper models . Why this happens is an open research\nquestion.\n4 Machine learning with vector search\nWe review methods where VS is included within deep learning m odels.\nNetwork compression. In the previous section, we showed the equivalence between V S and a linear\nlayer. Therefore, it is natural to apply vector search techn iques to these layers, especially when the\nweight matrix is skinny ( N≫d).\nTechniques originally developed for VS have been applied to compressing linear layers. Product\nquantization is used to compress convolutional neural nets [23] and language models [48], with or with-\nout retraining the model (quantization-aware training or p ost-training quantization). This is especially\nuseful for large recommendation models where most of the net work parameters are in embedding ta-\nbles [38], i.e. N≫d. In some settings, a compressed linear operator can be appli ed to its input without\ndecompression [24].\nAttention operators. Large language models are built around an attention operato r that functions\nas an associative memory: a query vector is matched with embe ddings of all the previous tokens of the\nprompt. This is basically a vector search task.\nWhen the prompt size increases (the “long context” setting, N≫d), it becomes beneﬁcial to\nuse ANNS. This includes compression [21], and also non-exha ustive VS with partitions [8], random\nprojections [13] or graphs [33]. One diﬃculty of attention o perators is that the queries and database\nvectors have diﬀerent distributions. For partition-based V S, this can be addressed by training diﬀerent\npartition classiﬁers for queries and database vectors [35] .\nDiscussion. VS has a potential to be an accelerator for ML models, wheneve r the input needs to be\ncompared to a large number of vectors. The limit is that ML is t ypically run on hardware that is so\neﬃcient in doing brute-force VS (matrix multiplication) [1 4] that the size above which it makes sense\nto use ANNS is large (currently around N= 105) and growing.\n4\n\n5 Conclusion\nWe reviewed several use cases where VS and ML are interlinked . It is clear that ML cannot replace the\nVS data structures, but it can help modeling the data distrib utions. One direction that deserves more\nexploration is leverage ML to guide the construction of grap h-based indexes. In the other direction,\nthere are tasks in ML where VS can be very useful, in particula r to solve large classiﬁcation problems\nand to perform attention on long contexts.\nThe case for learned indexing structures [32] speculated that theincrease of computecapacity would\nmake ML models more amenable to indexing methods. What happe ned is rather that for many use\ncases of VS, compute has become so cheap that they can just be s olved in brute force.\nReferences\n[1] Kenza Amara, Matthijs Douze, Alexandre Sablayrolles, a nd Herv´ e J´ egou. Nearest neighbor search\nwith compact codes: A decoder perspective. In ICMR, 2022.\n[2] MartinAum¨ uller, ErikBernhardsson, andAlexanderFai thfull. Ann-benchmarks: Abenchmarking\ntool for approximate nearest neighbor algorithms. Information Systems , 87:101374, 2020.\n[3] Ilias Azizi, Karima Echihabi, and Themis Palpanas. Grap h-based vector search: An experimental\nevaluation of the state-of-the-art. arXiv preprint arXiv:2502.05575 , 2025.\n[4] Artem Babenko and Victor Lempitsky. Additive quantizat ion for extreme vector compression. In\nCVPR, 2014.\n[5] Artem Babenko, Anton Slesarev, Alexandr Chigorin, and V ictor Lempitsky. Neural codes for\nimageretrieval. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, S witzerland,\nSeptember 6-12, 2014, Proceedings, Part I 13 , pages 584–599. Springer, 2014.\n[6] Francis Bach. Learning theory from ﬁrst principles . MIT press, 2024.\n[7] Maxim Berman, Herv´ e J´ egou, AndreaVedaldi, Iasonas Ko kkinos, andMatthijs Douze. Multigrain:\na uniﬁed image embedding for classes and instances. arXiv preprint arXiv:1902.05509 , 2019.\n[8] Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R Go rmley. Unlimiformer: Long-range\ntransformers with unlimited length input. arXiv preprint arXiv:2305.01625 , 2023.\n[9] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Ma tthijs Douze. Deep clustering for\nunsupervised learning of visual features. In Proceedings of the European conference on computer\nvision (ECCV) , pages 132–149, 2018.\n[10] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´ e J´ eg ou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vis ion transformers. In Proceedings of the\nIEEE/CVF international conference on computer vision , pages 9650–9660, 2021.\n[11] Meng Chen, Kai Zhang, Zhenying He, Yinan Jing, and X Sean Wang. Roargraph: A projected\nbipartite graph for eﬃcient cross-modal approximate neare st neighbor search. arXiv preprint\narXiv:2408.08933 , 2024.\n[12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀr ey Hinton. A simple framework for\ncontrastive learning of visual representations. In International conference on machine learning ,\npages 1597–1607. PMLR, 2020.\n5\n\n[13] Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Jianyu Zha ng, Niklas Nolte, Matthijs Douze,\nLeon Bottou, Zhihao Jia, and Beidi Chen. Magicpig: Lsh sampl ing for eﬃcient llm generation. In\nArXiV, 2024.\n[14] Felix Chern, Blake Hechtman, Andy Davis, Ruiqi Guo, Dav id Majnemer, and Sanjiv Kumar.\nTpu-knn: K nearest neighbor search at peak ﬂop/s. Advances in Neural Information Processing\nSystems, 35:15489–15501, 2022.\n[15] Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S Mirrokni. Locality-sensitive hashing\nscheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on\nComputational geometry , pages 253–262, 2004.\n[16] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafei riou. Arcface: Additive angular margin\nloss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition , pages 4690–4699, 2019.\n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.\n[18] Yihe Dong, Piotr Indyk, Ilya Razenshteyn, and Tal Wagne r. Learning space partitions for nearest\nneighbor search. arXiv preprint arXiv:1901.08544 , 2019.\n[19] Alexey Dosovitskiy, LucasBeyer, AlexanderKolesniko v, Dirk Weissenborn, XiaohuaZhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg H eigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby. An image is worth 16x16 words: Transf ormers for image recognition at\nscale, 2021.\n[20] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeﬀ Joh nson, Gergely Szilvasy, Pierre-\nEmmanuel Mazar´ e, Maria Lomeli, Lucas Hosseini, and Herv´ e J´ egou. The faiss library. arXiv\npreprint arXiv:2401.08281 , 2024.\n[21] Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, E lias Frantar, Artem Babenko, and Dan\nAlistarh. Extreme compression of large language models via additive quantization. arXiv preprint\narXiv:2401.06118 , 2024.\n[22] Jianyang Gao and Cheng Long. Rabitq: Quantizing high-d imensional vectors with a theoretical\nerror bound for approximate nearest neighbor search. Proceedings of the ACM on Management of\nData, 2(3):1–27, 2024.\n[23] Song Han, Huizi Mao, and William J Dally. Deep compressi on: Compressing deep neural networks\nwith pruning, trained quantization and huﬀman coding. arXiv preprint arXiv:1510.00149 , 2015.\n[24] Zaid Harchaoui, Matthijs Douze, Mattis Paulin, Mirosl av Dudik, and J´ erˆ ome Malick. Large-scale\nimage classiﬁcation with trace-norm regularization. In 2012 IEEE conference on computer vision\nand pattern recognition , pages 3386–3393. IEEE, 2012.\n[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. D eep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pat tern recognition ,\npages 770–778, 2016.\n[26] IrisA.M.Huijben,MatthijsDouze, Matthew J.Muckley, RuudJ.G.vanSloun, andJakobVerbeek.\nResidual quantization with implicit neural codebooks. In International Conference on Machine\nLearning (ICML) , 2024.\n6\n\n[27] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Seba stian Riedel, Piotr Bojanowski, Armand\nJoulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning.\narXiv preprint arXiv:2112.09118 , 2021.\n[28] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open\ndomain question answering. arXiv preprint arXiv:2007.01282 , 2020.\n[29] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zade h, Debapriya Banerjee, and Fillia\nMakedon. A survey on contrastive self-supervised learning .Technologies , 9(1):2, 2020.\n[30] Herv´ e J´ egou, Matthijs Douze, and Cordelia Schmid. Pr oduct quantization for nearest neighbor\nsearch.PAMI, 2010.\n[31] Herv´ e J´ egou, Matthijs Douze, Cordelia Schmid, and Pa trick P´ erez. Aggregating local descriptors\ninto a compact image representation. In 2010 IEEE computer society conference on computer\nvision and pattern recognition , pages 3304–3311. IEEE, 2010.\n[32] Tim Kraska, Alex Beutel, Ed H Chi, Jeﬀrey Dean, and Neokli s Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 international conference on manage ment of data ,\npages 489–504, 2018.\n[33] Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua H an, Qianxi Zhang, Qi Chen, Chen-\ngruidong Zhang, Bailu Ding, Kai Zhang, et al. Retrievalatte ntion: Accelerating long-context llm\ninference via vector retrieval. arXiv preprint arXiv:2409.10516 , 2024.\n[34] Yu A Malkov and Dmitry A Yashunin. Eﬃcient and robust app roximate nearest neighbor search\nusing hierarchical navigable small world graphs. IEEE transactions on pattern analysis and ma-\nchine intelligence , 42(4):824–836, 2018.\n[35] Pierre-Emmanuel Mazar´ e, Gergely Szilvasy, Maria Lom eli, Francisco Massa, Naila Murray, Herv´ e\nJ´ egou, and Matthijs Douze. Inference-time sparse attenti on with asymmetric indexing, 2025.\n[36] Thomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela Csurka. Distance-based image\nclassiﬁcation: Generalizing to new classes at near-zero co st.IEEE transactions on pattern analysis\nand machine intelligence , 35(11):2624–2637, 2013.\n[37] Stanislav Morozov and Artem Babenko. Unsupervised neu ral quantization for compressed-domain\nsimilarity search. In ICCV, 2019.\n[38] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi , Jianyu Huang, Narayanan Sundara-\nman, Jongsoo Park, Xiaodong Wang, UditGupta, Carole-Jean W u, Alisson G Azzolini, et al. Deep\nlearning recommendation model for personalization and rec ommendation systems. arXiv preprint\narXiv:1906.00091 , 2019.\n[39] Maxime Oquab, Timoth´ ee Darcet, Th´ eo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov,\nPierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeld in El-Nouby, Mahmoud Assran, Nico-\nlas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, S hang-Wen Li, Ishan Misra, Michael\nRabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv´ e Jegou, Julien Mairal, Patrick Labatut,\nArmand Joulin, and Piotr Bojanowski. Dinov2: Learning robu st visual features without supervi-\nsion, 2024.\n7\n\n[40] Florent Perronnin, Yan Liu, Jorge S´ anchez, and Herv´ e Poirier. Large-scale image retrieval with\ncompressed ﬁsher vectors. In 2010 IEEE computer society conference on computer vision and\npattern recognition , pages 3384–3391. IEEE, 2010.\n[41] Ed Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Pr iya Goyal, and Matthijs Douze. A\nself-supervised descriptor for image copy detection. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 14532–14542, 2022.\n[42] Hang Qi, Matthew Brown, and David G Lowe. Low-shot learn ing with imprinted weights. In\nProceedings of the IEEE conference on computer vision and pat tern recognition , pages 5822–5830,\n2018.\n[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Rame sh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, e t al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning , pages\n8748–8763. PMLR, 2021.\n[44] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Gener ating diverse high-ﬁdelity images with\nvq-vae-2. Advances in neural information processing systems , 32, 2019.\n[45] Alexandre Sablayrolles, Matthijs Douze, Cordelia Sch mid, and Herv´ e J´ egou. Spreading vectors\nfor similarity search. ICLR, 2019.\n[46] Harsha Vardhan Simhadri, Martin Aum¨ uller, Amir Ingbe r, Matthijs Douze, George Williams,\nMagdalen Dobson Manohar, Dmitry Baranchuk, Edo Liberty, Fr ank Liu, Ben Landrum, et al.\nResults of the big ann: Neurips’23 competition. arXiv preprint arXiv:2409.17424 , 2024.\n[47] Harsha Vardhan Simhadri, George Williams, Martin Aum¨ uller, Matthijs Douze, Artem Babenko,\nDmitry Baranchuk, Qi Chen, Lucas Hosseini, Ravishankar Kri shnaswamny, Gopal Srinivasa,\net al. Results of the neurips’21 challenge on billion-scale approximate nearest neighbor search.\nInNeurIPS 2021 Competitions and Demonstrations Track , pages 177–189. PMLR, 2022.\n[48] Pierre Stock, Armand Joulin, R´ emi Gribonval, Benjami n Graham, and Herv´ e J´ egou. And the\nbit goes down: Revisiting the quantization of neural networ ks.arXiv preprint arXiv:1907.05686 ,\n2019.\n[49] Suhas Jayaram Subramanya, Rohan Kadekodi, Ravishanka r Krishaswamy, and Harsha Vardhan\nSimhadri. Diskann: Fast accurate billion-point nearest ne ighbor search on a single node. In\nNeurIPS , 2019.\n[50] Th´ eophane Vallaeys, Matthew Muckley, Jakob Verbeek, and Matthijs Douze. Qinco2: Vector\ncompression and search with improved implicit neural codeb ooks. In ICLR, 2025.\n8",
  "textLength": 24222
}