{
  "paperId": "278c713959aa19194beb27930b947c91d392809c",
  "title": "LITS: An Optimized Learned Index for Strings (An Extended Version)",
  "pdfPath": "278c713959aa19194beb27930b947c91d392809c.pdf",
  "text": "LITS: An Optimized Learned Index for Strings\n(An Extended Version)\nYifan Yang Shimin Chenâˆ—\nSKLP, ACS, Institute of Computing Technology, CAS\nUniversity of Chinese Academy of Sciences\n{yangyifan22z,chensm}@ict.ac.cn\nABSTRACT\nIndex is an important component in database systems. Learned\nindexes have been shown to outperform traditional tree-based index\nstructures for fixed-sized integer or floating point keys. However,\nthe application of the learned solution to variable-length string keys\nis under-researched. Our experiments show that existing learned\nindexes for strings fail to outperform traditional string indexes,\nsuch as HOT and ART. String keys are long and variable sized, and\noften contain skewed prefixes, which make the last-mile search\nexpensive, and adversely impact the capability of learned models\nto capture the skewed distribution of string keys.\nIn this paper, we propose a novel learned index for string keys,\nLITS ( Learned Index with Hash-enhanced Prefix Table and Sub-\ntries). We propose an optimized learned model, combining a global\nHash-enhanced Prefix Table (HPT) and a per-node local linear\nmodel to better distinguish string keys. Moreover, LITS exploits\ncompact leaf nodes and hybrid structures with a PMSS model for\nefficient point and range operations. Our experimental results using\neleven string data sets show that LITS achieves up to 2.43x and\n2.27x improvement over HOT and ART for point operations, and\nattains comparable scan performance.\n1 INTRODUCTION\nIndexes play an essential role in modern database engines to accel-\nerate transaction and query processing. Learned indexes have been\nshown to outperform traditional tree-based index structures for\nfixed-sized integer or floating point keys [ 13,15â€“17,20,21,23,24,\n27,28]. However, this is hardly the case for variable-length string\nkeys, which are common in the real world [9, 11].\nWhile learned indexes have been extensively studied for fixed-\nsized integer or floating point keys in recent years, the application\nof the learned solution to variable-length string keys is under-\nresearched with only a couple of studies [ 22,26]. We experimentally\ncompare the existing learned indexes for strings, i.e. SIndex [ 26]\nand RSS [ 22], with state-of-the-art traditional string indexes, i.e.,\nART [ 19] and HOT [ 10]. We find that existing learned indexes fail\nto outperform traditional indexes. In fact, traditional string indexes\nwin by a large margin.\nBy examining real-world string data sets, we observe two dis-\ntinct features of string keys that differ significantly from fixed-sized\ninteger or floating point keys. First, string keys are often long and\nvariable sized , making the key access and comparison more expen-\nsive. Second, string data sets see skewed prefixes among string keys.\nPopular prefixes shared by multiple strings make it difficult for\nlearned models to distinguish individual string keys.\nâˆ—Shimin Chen is the corresponding author.The two distinct features impact the tree height, the node search,\nand the last-mile search in index structures. For example, the last-\nmile search often requires expensive key comparisons, and there-\nfore should be avoided as much as possible. Recent studies attempt\nto adapt CDF models for fixed sized keys (e.g. RMI [ 17], Radix\nSpline [ 16], and piece-wise linear models) to string data sets. How-\never, the resulting learned models work poorly for capturing the\nskewed distribution of string keys, leading to large tree heights that\ndegrade index performance.\nIn this paper, we propose a novel learned index for string keys,\nLITS ( Learned Index with Hash-enhanced Prefix Table and Sub-\ntries). First, LITS employs the collision-driven design of LIPP [ 28] to\navoid the last-mile search by creating a child node to store the keys\nthat are mapped to the same slot. Second, we propose an optimized\nlearned model, combining a global Hash-enhanced Prefix Table\n(HPT) and a per-node local linear model. The HPT approximates\nthe conditional probability of the next character given a prefix in\nthe string key. Compared with existing learned models, HPT can\nbetter distinguish string keys. Third, the collision-driven design can\nresult in a large number of small leaf nodes containing two or only\na few keys. Consequently, a scan may have to traverse many small\nnodes, incurring expensive cache misses and node jump overhead.\nWe introduce the compact leaf node, which replaces a group of\nsmall nodes with a single node. Finally, we observe that trie-based\nindex, such as HOT, is very efficient for highly skewed string data\nsets. Therefore, we combine our learned index and HOT using a\nperformance model (PMSS) to determine whether a subtrie is more\nbeneficial to be used in the place of a child node.\nLITS supports common index operations on string keys, includ-\ning bulkload, search, insert, delete, update, and range scans. It is\nspecifically optimized for point operations. We conduct extensive\nexperiments using seven real-world string data sets and four syn-\nthetic data sets. Our experimental results show that LITS achieves\nup to 2.06x and 2.14x improvement over HOT and ART for point\noperations, respectively. For the scan-heavy workload, LITSâ€™s per-\nformance is comparable with HOT and better than ART.\nContributions. The contributions of the paper are as follows. First,\nwe propose a novel HPT-based CDF model that exhibits strong\ndiscriminative power for string keys. Second, we propose LITS, a\nnovel learned index for strings that exploits the HPT-based model,\ncompact leaf nodes, and hybrid structures with a PMSS model for\nefficient point and range operations. Finally, we perform extensive\nexperiments to compare our proposed LITS with five state-of-the-\nart string indexes using eleven string data sets. Our experiments\nshow that LITS achieves the overall best performance.\nOrganization. The rest of the paper is organized as follows. Sec-\ntion 2 studies the characteristics of string keys and examine existingarXiv:2407.11556v1  [cs.DB]  16 Jul 2024\n\nYifan Yang Shimin Chen\nTable 1: String data sets used in this work. (cf. Section 4.1)\nDataset Min Len Max Len Avg Len Number of Keys Total Size\naddress 4B 133B 24B 34M 802MB\ndblp 2B 255Bâ€ 76B 7M 506MB\ngeoname 2B 152B 13B 7M 106MB\nimdb 2B 106B 13B 9M 132MB\nreddit 3B 26B 11B 26M 292MB\nurl 12B 255Bâ€ 64B 63M 4.6GB\nwiki 2B 255Bâ€ 15B 43M 870MB\nemail* 11B 47B 23B 45M 1.1GB\nidcard* 18B 18B 18B 63M 1.2GB\nphone* 11B 23B 17B 50M 819MB\nrands* 2B 61B 32B 50M 1.6GB\nâ€ : The data set is processed to remove strings longer than 255B. The maximum key\nlength of the unprocessed dblp is up to 1461B.\n*: The data set is synthetically generated.\n/uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013 /uni00000014/uni00000015 /uni00000014/uni00000017 /uni00000014/uni00000019\n/uni00000053/uni00000055/uni00000048/uni00000049/uni0000004c/uni0000005b/uni00000003/uni0000004f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni0000000b/uni00000025/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003/uni00000052/uni00000049/uni00000003/uni00000047/uni0000004c/uni00000056/uni00000057/uni0000004c/uni00000051/uni00000046/uni00000057/uni00000003/uni00000053/uni00000055/uni00000048/uni00000049/uni0000004c/uni0000005b/uni00000048/uni00000056\n/uni00000044/uni00000047/uni00000047/uni00000055/uni00000048/uni00000056/uni00000056\n/uni00000047/uni00000045/uni0000004f/uni00000053\n/uni0000004a/uni00000048/uni00000052/uni00000051/uni00000044/uni00000050/uni00000048\n/uni0000004c/uni00000050/uni00000047/uni00000045\n/uni00000055/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057\n/uni00000058/uni00000055/uni0000004f\n/uni0000005a/uni0000004c/uni0000004e/uni0000004c\n/uni0000004c/uni00000051/uni00000057/uni00000048/uni0000004a/uni00000048/uni00000055\nFigure 1: Prefix skewness of string keys.\nstring indexes to motivate our study. Then, Section 3 presents the\nLITS design. Section 4 experimentally compares LITS with state-of-\nthe-art string indexes. Finally, Section 5 concludes the paper.\n2 BACKGROUND AND MOTIVATION\nLearned indexes have been shown to outperform traditional tree-\nbased index structures for fixed-sized integer or floating point\nkeys [ 13,15â€“17,20,21,23,24,27,28]. However, this is hardly the\ncase for string keys. In the following, we study the characteristics\nof string keys in Section 2.1, then examine existing index structures\noptimized for strings to motivate our study in Section 2.2.\n2.1 Characteristics of String Keys\nTable 1 summarizes 7 real-world and 4 synthetic string data sets\nused in this work. (Please see detailed description in Section 4.1.)\nFocusing on the real-world data sets, we observe two features that\nare distinct from fixed-sized integer or floating point keys.\nLong and Variable Sized Keys. Integer or floating point keys\nare typically of 4B or 8B large. In comparison, the string keys in\nthe real-world data sets are much more complex. They can vary\nfrom 2B to over 1KB. The average key length of the real-world data\nsets is from 11B to 26B, which is much longer than 4B/8B keys.\nConsequently, storing entire string keys in index (inner) nodes can\nsignificantly reduce node fanouts, degrading index performance.\nOn the other hand, storing pointers to string keys in index nodes\ncauses pointer dereferences, incurring CPU cache misses. Moreover,\nthe comparison of long keys is also more expensive.\nSkewed Prefixes. The prefixes of string keys are often quite skewed.\nFigure 1 compares the prefix skewness of the real-world string data\nsets and a uniformly generated integer data set. For each prefix\nlengthğ‘˜, we compute the ratio of distinct prefixes of a data set asthe number of distinct k-byte prefixes divided by the total number\nof keys in the data set. This ratio is between 0 and 1. If it is closer\nto 1, then the data set is more evenly distributed. If it is closer to\n0, then a few prefixes are very popular. A large number of keys\nshare the same prefixes. The data set is more skewed. In Figure 1,\nwe consider the prefix length when the ratio of a data set is over\n0.99. For the integer data set, all keys can be distinguished by four\nbytes. In contrast, all real-world string data sets have very low ra-\ntios of distinct prefixes at 4B prefixes. For reddit , the ratio reaches\n0.99 at 16B prefixes. The ratio of urlgets to 0.99 at 154B prefixes.\nConsequently, it is necessary to examine a much larger number\nof bytes for distinguishing string keys, adversely impacting the\neffectiveness of learned models in learned indexes.\n2.2 Existing Indexes Optimized for Strings\nWe focus on ordered indexes for strings in this paper. Figure 8 com-\npares the search performance of five state-of-the-art index struc-\ntures optimized for strings, including two trie-based indexes (i.e.,\nART [ 19] and HOT [ 10]), and three learned index based structures\n(i.e., SIndex [ 26], RSS [ 22], SLIPP, which is based on LIPP [ 28])1.\nFrom the figure, it is clear that existing learned indexes work poorly\ncompared to traditional trie-based indexes. In the following, we\nexamine the index design choices to understand the pros and cons\nof the existing index designs.\nIndex Performance Factors. Ordered indexes are typically or-\nganized as a tree. All the five state-of-the-art string indexes are\nessentially trees consisting of inner nodes and leaf nodes. A search\noften starts from the root of a tree, visits several inner nodes at\ndifferent tree levels, and finally reaches a leaf node in the tree.\nTherefore, the number of tree levels from the root to the leaf and\nthe search procedures at inner and leaf nodes are main factors\ninfluencing the index search performance:\nâ€¢Tree height : The (average) tree height indicates the expected\nnumber of nodes accessed by an index search. Each node access\noften incurs an expensive CPU cache miss. These cache misses\nare dependent on each other since the memory address of the\nnode at the next level is known only after searching the node\nat the previous level. Therefore, the tree height is an important\nperformance factor. We see four cases for tree heights. First,\nthe tree height is determined by (the logarithm of) the number\nof index entries (e.g., in B+-Trees). Second, the tree height is\ndetermined by the length of the index keys (e.g., in ART or\nRSS). Third, learned indexes introduce CDF models to predict\nthe key positions in a node in order to increase node sizes and\nreduce the tree height. The tree height is model-based . Finally,\nSIndex [ 26] constructs a two-level tree with a root node and a\nnumber of group nodes.\nâ€¢Node search : Search in an inner node narrows down the search\nscope to a subtree of the node. Search in a leaf node locates the\ntarget index entry. Both often follow similar search procedures.\nThere are mainly three ways to support node search. First, it\n1We do not include B+-Trees in the comparison because previous work has shown that\ntrie-based indexes significantly outperform B+-Trees for string data sets [ 10]. Recent\nwork on Extendible Radix Tree (ERT) enhances trie nodes with extendible hashing [ 25].\nWhile the idea could potentially support string keys, the original paper and its code\nfocus only on integer keys. Therefore, we do not consider ERT in this work.\n\nLITS: An Optimized Learned Index for Strings\n(An Extended Version)\nheight depends \non key length  height depends on #entries  n256 \nn48 n48 \nn16 compound node  \nfanout  up to 32 \nlinear model works poorly \nfor strings  LM \nitem array  \nLM \nsub-trie global HPT  ART  HOT  \ncostly last- mile \nsearch  RS model  \n+  \nredirector map \nK-V array  RSS SLIPP  Our solution: LITS \nLM LM LM root \ncostly  \nlast-mile \nsearch  SIndex  \ngroup pivots  \nLM LM group nodes  \nlocal \nmodel \nitem array  \nlocal \nmodel  \ncompact \nleaf tree \nheight  node \nsearch  last-mile \nsearch \nART  x âˆš n/a \nHOT  âˆš n/a \nSindex  âˆš x \nRSS âˆš âˆš x \nSLIPP  x âˆš n/a \nLITS  âˆš âˆš n/a \nx: poor; âˆš: good; âˆ’ : medium; \nn/a: no last- mile search  \nFigure 2: Comparing index structures optimized for strings.\ncan be based on key comparisons over the full keys or on part\nof the keys, e.g., binary search in a sorted B+Tree node. Second,\nit can perform an array lookup , e.g., in a common trie node,\nsuch as node256 in ART. Finally, learned indexes often conduct\nmodel-based search, which employs a learned model to predict\nthe location of the search key in the key array.\nâ€¢Last-mile search : In learned indexes, model prediction in inner\nnodes is often accurate by construction. Index keys are mapped\nto subtrees of an inner node using the associated model during\nbulkload and write operations. However, the models in leaf\nnodes may not predict the correct key positions. Therefore,\nlearned indexes often have to do extra work, a.k.a. last-mile\nsearch , to locate the key around the predicted position in leaf\nnodes. The last-mile search often performs key comparisons\n(e.g., with exponential or binary search), and can incur poor\nperformance for learned indexes with fixed-sized keys [ 27]. The\nsituation is even worse for long and variable sized keys because\nof pointer dereferences and higher key comparison costs.\nPros and Cons of Existing Indexes. We examine the three per-\nformance factors of the five string indexes. Figure 2 compares the\nfive indexes and our proposed solution.\nâ€¢ART: Adaptive Radix Tree (ART) [ 19] is a compressed trie. Each\nlevel of an ART uses a byte in the keys. Therefore, the height\nof an ART is determined by the key lengths, which can be\nquite large for string data sets. ART compresses the trie node\nto four types of nodes (i.e., Node4/16/48/256) to reflect the\neffective node fanouts. The node search in Node48 and Node256\nperforms array lookups, while Node4 and Node16 employ key\ncomparison based search. Both procedures are fast because ART\nsearches 1B in every node. There is no last-mile search.\nâ€¢HOT : Height Optimized Trie (HOT) [ 10] optimizes ART by re-\nducing the tree height. Each inner node of a HOT is a compound\nnode that represents a Patricia trie with a fanout of up to 32.\nThis is achieved by carefully storing only a subset of distinct\nkey bits (a.k.a. partial keys) in each node. As a result, HOT often\nreduces the tree height significantly for long string keys. Its\nheight can be viewed as roughly determined by the number of\nindex entries. Moreover, the node search compares partial keys\nwith efficient SIMD operations. There is no last-mile search.â€¢SIndex : SIndex [ 26] is a two-level tree consisting of a root node\nand a level of group nodes. The root node employs a piece-\nwise linear model (PLM) and divides the key space into key\ngroups. Then, each group node uses a linear model (LM) to\nlocate an index entry. Since the model prediction is not fully\naccurate, SIndex performs a last-mile search, which is a binary\nsearch within the error bound around the predicted location, in\nboth the root and the group nodes. The last-mile search incurs\nsignificant performance overhead.\nâ€¢RSS: Radix String Spline (RSS) [ 22] is a trie. Each trie node uses 8B\nor 16B of the keys, and computes a Radix Spline (RS) model [ 16]\nfor them. The RS model is a piece-wise linear model that pro-\nvides monotonic CDF prediction with a given error bound. For\nkeys dissatisfying the error bound (e.g., because of shared pre-\nfixes), RSS stores the key in the redirector map and creates a new\nchild node for the key. RSS compares 8B/16B portions of keys,\nwhich has lower cost than full key comparison. However, the\nlast-mile search is still very costly. In the search experiments,\nRSS spends over 70% of the time in the last-mile search.\nâ€¢SLIPP . LIPP [ 28] is an interesting learned index with fixed-sized\nkeys because it avoids the costly last-mile search. In each inner\nnode, LIPP trains a linear model. If multiple keys are mapped to\nthe same entry slot by the linear model, LIPP creates a new child\nnode for the collision keys. This design is known as the collision-\ndriven approach. It essentially converts the last-mile search into\na sub-tree search. We implement a variant of LIPP, called SLIPP ,\nto support string keys. At each node, SLIPP computes a numeric\nrepresentation of each string key (after excluding the common\nprefix of the keys in the node) using a straight-forward formula:\nğ‘¦=ğ‘ 1\n256+Â·Â·Â·+ğ‘ ğ‘™ğ‘’ğ‘›\n256ğ‘™ğ‘’ğ‘›, whereğ‘ ğ‘–is theğ‘–-th byte of the key. Then, it\ncomputes the linear model based on the numeric representation.\nThe other design of LIPP is kept unchanged. While SLIPP avoids\nthe last-mile search, the computed model can hardly distinguish\nkeys with skewed prefixes, resulting in many collisions and\nlarge tree heights.\nMotivation. The above discussion focuses on search, which is rep-\nresentative of point operations. The comparison of the five string\nindex structures motivate us to design an optimized learned in-\ndex for strings that avoids the last-mile search and improves the\neffectiveness of the learned models for reducing the tree height.\n\nYifan Yang Shimin Chen\nHash -enhanced \nPrefix Table \n \n \n5-bit \nreserved  3-bit \nnode  type 8-bit \nprefix len 48-bit \npointer  \nsubtrie node 64bit  \nï¼ˆHPTï¼‰ Performance Model \nfor Structure Selection  \n \n \nï¼ˆPMSSï¼‰ \nkv-entry  kv-entry  \nkv-entry  kv-entry  kv-entry  kv-entry  compact leaf node \n (up to 16 h- ptrs) \n \n header  h-ptr ... h-ptr model-based node  model-based node  model- based node (root)  \n \n \nitem item ... item # keys prefix  item array size  local model  \n000 empty slot  \n001 model -based node  \n010 compact node  \n011 subtrie  \n100 key-value entry  \nkv-entry  compact leaf node  \n16-bit \nhash code  of the key  48-bit \npointer  64bit  \nFigure 3: Overview of LITS.\n3 LITS\nWe propose LITS ( Learned Index with Hash-enhanced Prefix Table\nandSub-tries) for string keys in this section. Section 3.1 overviews\nthe structure and operations of LITS. Then, Section 3.2, 3.3, and 3.4\npresent the three main techniques of LITS, optimizing the learned\nmodel, accelerating scans with compact leaf nodes, and exploit-\ning subtries to further improve performance, respectively. Finally,\nSection 3.5 describes the time and space cost of LITS.\n3.1 Overview of LITS\nFigure 3 depicts the structures in LITS. We describe the distinct\nfeatures of LITS (highlighted in the figure) in the following.\nâ€¢Model-based node : To deal with the problem of the last-mile\nsearch, as discussed in Section 2.2, we employ the collision-\ndriven design of LIPP to avoid the last-mile search. Specifically,\na model-based node consists of a header and an item array.\nThe header contains metadata, such as the number of keys,\nthe key prefix, the size of the item array, and a local linear\nmodel. Each slot in the item array is a 64-bit pointer. We store\nadditional information in the upper bits of the pointers, which\nare otherwise unused in current machines. Keys are mapped to\nslots in the array with an optimized learned model (discussed in\nmore detail below). There are three cases. First, a slot is empty.\nThen, it contains a NULL item. Second, only a single key is\nmapped to a slot. Then, the slot holds a pointer to the key-value\nentry. Third, multiple keys are mapped to the same slot. Then,\nLITS creates a child node to store the keys to avoid the last-\nmile search. The node type field in the 64-bit item indicates the\ndifferent child node types.\nâ€¢Optimized global HPT and local models : As discussed in Sec-\ntion 2.2, due to skewed prefixes and long keys, existing learned\nmodels work poorly for string keys. This results in the large tree\nheight in SLIPP, lowering index performance. We propose an\noptimized learned model, combining a global Hash-enhanced\nPrefix Table (HPT) and a per-node local linear model to effec-\ntively distinguish string keys. (cf. Section 3.2)\nâ€¢Compact leaf node : The collision-driven design in model-based\nnodes lead to a large number of small leaf nodes that contain\ntwo or only a few kv-pointers. However, a scan has to traverse\nmany such small leaf nodes, and suffers from expensive cache\nmisses and node jump overhead. We introduce the compact leaf\nnode to make the design scan-friendly. A compact node contains\na header and an array of h-pointers sorted in the key order. Anh-pointer consists of a 16-bit computed hash of the key and a\n48-bit pointer to the kv-entry. In this way, we replace a number\nof small leaf nodes with a single compact leaf node, thereby\nreducing the number of node visits in scans. (cf. Section 3.3)\nâ€¢Subtrie node and PMSS : We call the resulting index with the above\ntechniques, LIT. Our experiments show that LIT outperforms\nall the five existing indexes for most data sets, but it is slightly\nslower than the trie-based index (i.e., HOT) for a couple of\ndata sets. Therefore, we propose to combine LIT and trie-based\nindexes. We build a performance model (i.e., PMSS) to determine\nwhether a subtrie is more beneficial to be used in the place of a\nchild node. The combined structure of LIT with subtries is our\nfinal proposed solution, LITS. (cf. Section 3.4)\nAfter overviewing the structures of LITS, we describe the common\nindex operations in the following.\nSearch. A search goes from the root node to a leaf node. Based on\nthe node type, LITS performs different search procedures.\nFirst, in a model-based node, LITS compares the common prefix\nrecorded in the node header with the search key. Most commonly,\nthe prefixes match. Then, LITS skips the prefix and uses the remain-\ning substring of the search key to predict the slot position based on\nthe global HPT and the local model. The position is between 1and\nğ¼ğ‘¡ğ‘’ğ‘šğ´ğ‘Ÿğ‘Ÿğ‘ğ‘¦ğ‘†ğ‘–ğ‘§ğ‘’âˆ’2. In rare situations, the prefixes do not match. We\npreserve the first (the last) item of the item array for the case where\nthe search key prefix is less (greater) than the recorded prefix. Then,\nLITS gets the target item according to the search key. If the item is\nNULL, the search key does not exist. Otherwise, LITS dereferences\nthe pointer to visit the child node / kv-entry.\nSecond, in a compact leaf node, LITS performs a key comparison\nbased search. It dereferences an h-pointer only if the hash of the\nsearch key matches the hash in the h-pointer.\nThird, if the node is a subtrie node, LITS calls the search proce-\ndure of the subtrie (e.g., HOT) to continue the search.\nFinally, upon reaching a kv-entry, LITS compares the search key\nwith the key in the kv-entry. The search succeeds if it is a match.\nOtherwise, the search key is not found.\nInsert/Delete/Update. An insert first performs the search for the\ninput key. If the search gets to an empty item, LITS simply inserts\nthe new kv-entry to the empty slot. If the search gets to a single-\nentry item (i.e., a pointer to a kv-entry), LITS builds a new compact\nleaf node to contain the new key and the existing key. If the search\ngets to a compact leaf node, then LITS inserts the key to the compact\nleaf node if there are less than 16 entries. When the compact node\nalready contains 16 entries, LITS performs the PMSS-based decision\n\nLITS: An Optimized Learned Index for Strings\n(An Extended Version)\nand replaces the compact node with either a model-based node or\na subtrie. If the search gets to a subtrie, then LITS calls the insert\nprocedure of the subtrie to complete the insertion.\nThe delete or update procedures work similarly. For an update,\nLITS searches the key and either modifies the value in the kv-entry\nor changes the item or h-pointer to point to a new kv-entry. For a\ndelete, LITS removes the key if it is found. This clears the single-\nentry item, or reduces the number of keys in a compact node. The\nsituation is a little complicated for subtrie deletion because HOT\ndoes not implement the delete function. We employ a delete list to\nhold the deleted keys associated with a subtrie. If the number of\ndeleted keys is beyond a predefined ratio of the keys in the subtrie,\nwe reconstruct the subtrie.\nWhen a model-based node contains too many (or too few) keys,\nLITS follows a procedure similar to LIPP to perform node resizing\noperations. Moreover, when inserting to a compact leaf node with\n16 entries, LITS uses the PMSS to determine if a model-based node\nor a subtrie should be constructed. (Please see Section 3.4 for all\ncases where PMSS-based decision is performed.) Finally, we adapt\nthe classic method of optimistic locking, which protects each node\nwith a lock and allows reading a node without locking it, to support\nconcurrent threads.\nScan. Given a begin key, a scan searches the begin key and con-\nstructs an iterator. Using the iterator, one can obtain a list of kv-\npointers sorted by the key order by repeatedly calling the iteratorâ€™s\nnext method. Internally, the scan maintains a stack of pointers to\nnodes from the root to the current leaf node. Both the items in\nmodel-based nodes and the h-pointers in compact leaf nodes are\nsorted in the key order. Therefore, the scan can easily traverse the\nnodes in the tree using the stack.\nBulkload. At the beginning, LITS samples a subset of keys and\ncompute the global HPT model. Then, LITS bulkloads the tree in\na similar fashion as LIPP. There are two main differences. First,\nfor a sub range of data, LITS chooses which node type to build.\nIf the number of keys is at most 16, then LITS builds a compact\nnode. If there are more keys, LITS uses the PMSS to choose between\na model-based node and a subtrie. Second, when constructing a\nmodel-based node, LITS uses the global HPT to compute the local\nlinear model for the keys in the node.\n3.2 Hash-enhanced Prefix Table (HPT)\nWe would like to design a good learned model to better distinguish\nstring keys. In the following, we begin by deriving a recursive\nformula for CDF computation. Using the formula, we explain why\nprevious linear models work poorly. Then, we propose our HPT-\nbased model to better approximate the CDF. Finally, we describe\nthe training and computation procedure using the HPT.\nRecursive Formula for CDF Computation. Given a string data\nset and a string ğ‘†=ğ‘ 1...ğ‘ ğ‘›, we would like to compute ğ‘ğ‘‘ğ‘“(ğ‘†). For\nbrevity of presentation, we prepend a special (non-existent) begin-\nning character ğ‘ 0to every string. Hence, ğ‘†=ğ‘ 0ğ‘ 1...ğ‘ ğ‘›. We denote the\nğ‘˜-byte prefix of the string as Pğ‘˜=ğ‘ 0ğ‘ 1...ğ‘ ğ‘˜. Therefore, ğ‘†â‰¡Pğ‘›. Then,\nwe have the following recursive formula:\nğ‘ğ‘‘ğ‘“(P0)=0\nğ‘ğ‘‘ğ‘“(Pğ‘˜+1)=ğ‘ğ‘‘ğ‘“(Pğ‘˜)+ğ‘ğ‘Ÿğ‘œğ‘(Pğ‘˜)Ã—Î£ğ‘ ğ‘˜+1âˆ’1\nğ‘=0ğ‘ğ‘Ÿğ‘œğ‘(ğ‘|Pğ‘˜)(1)Here,ğ‘ğ‘Ÿğ‘œğ‘(Pğ‘˜)represents the probability of prefix Pğ‘˜in the string\ndata set.ğ‘ğ‘Ÿğ‘œğ‘(ğ‘|Pğ‘˜)stands for the conditional probability of the\nnext character being ğ‘given the prefixPğ‘˜.\nWe can also derive a recursive formula for ğ‘ğ‘Ÿğ‘œğ‘(Pğ‘˜)as follows:\nğ‘ğ‘Ÿğ‘œğ‘(P0)=1\nğ‘ğ‘Ÿğ‘œğ‘(Pğ‘˜+1)=ğ‘ğ‘Ÿğ‘œğ‘(Pğ‘˜)Ã—ğ‘ğ‘Ÿğ‘œğ‘(ğ‘ ğ‘˜+1|Pğ‘˜)(2)\nFrom Eqn 1 and 2, it is clear that obtaining ğ‘ğ‘Ÿğ‘œğ‘(ğ‘|Pğ‘˜)for any\nprefixPğ‘˜and any character ğ‘is crucial for computing ğ‘ğ‘‘ğ‘“(ğ‘†).\nProblem of Existing Linear Models. Existing linear models pre-\ndict the position of a string ğ‘†=ğ‘ 1...ğ‘ ğ‘›as a linear function: ğ‘¦(ğ‘†)=\nğ›¼Ã—ğ‘¥+ğ›½, whereğ‘¥=Î£ğ‘š\nğ‘˜=1ğ‘ ğ‘˜\n256ğ‘˜. SLIPP computes ğ‘¥based on the full\nstring, and hence ğ‘š=ğ‘›. RSS uses an 8B or 16B portion of the keys\nin each node in the model prediction. Therefore, ğ‘š= 8 or 16 in RSS.\nWe can rewrite the formula in a recursive fashion as follows:\nğ‘¦(Pğ‘˜+1)=ğ‘¦(Pğ‘˜)+ğ›¼\n256ğ‘˜Ã—ğ‘ ğ‘˜+1\n256 (3)\nSinceğ‘¦(ğ‘†)is a scaled version of ğ‘ğ‘‘ğ‘“(ğ‘†), we can compare Eqn 1\nand 3.ğ›¼\n256ğ‘˜corresponds to a scaled version of ğ‘ğ‘Ÿğ‘œğ‘(Pğ‘˜), andğ‘ ğ‘˜+1\n256\ncorresponds to Î£ğ‘ ğ‘˜+1âˆ’1\nğ‘=0ğ‘ğ‘Ÿğ‘œğ‘(ğ‘|Pğ‘˜).ğ‘ ğ‘˜+1\n256implies that any 8-bit char-\nacter appears uniformly at random. Therefore, the existing linear\nmodels essentially assume that the distribution of the next char-\nacter following any given prefix is uniform. This estimation can\nhardly reflect the true distribution in a string data set, which often\ncontains highly skewed prefixes.\nOur Solution: HPT. We would like to better approximate the\nconditional probability ğ‘ğ‘Ÿğ‘œğ‘(ğ‘|P). Existing neural network-based\nnonlinear CDF models [ 8,14] are more accurate than linear models.\nHowever, these models are complex. The model training and model\nprediction are time consuming. It would be an over-kill for our goal\nof designing efficient string indexes.\nA naÃ¯ve idea is to record the conditional probabilities for all\npossible (prefix, character) pairs in the string data set. However,\nsuch an approach would require prohibitively large space to store\nthe conditional probabilities.\nTo reduce the space overhead, we propose the Hash-enhanced\nPrefix Table (HPT). As illustrated in Figure 4, the HPT is a ta-\nble (i.e., 2D array). For any prefix, we map the prefix to a row\nin the table using a hash function. (We set the hash of the empty\nprefix,â„ğ‘ğ‘ â„(ğ‘ 0)=0.) Then, each column corresponds to a charac-\nter in the character set. We approximate the conditional proba-\nbilityğ‘ğ‘Ÿğ‘œğ‘(ğ‘|P)with table lookups as HPT[ â„ğ‘ğ‘ â„(P)][ğ‘+1].cdf -\nHPT[â„ğ‘ğ‘ â„(P)][ğ‘].cdf. We have stored this value in the model as\nHPT[â„ğ‘ğ‘ â„(P)][ğ‘].prob. Note that HPT[ â„ğ‘ğ‘ â„(P)][ğ‘].cdf approxi-\nmatesğ‘ğ‘‘ğ‘“(ğ‘|P), which is Î£ğ‘âˆ’1\n0ğ‘ğ‘Ÿğ‘œğ‘(ğ‘–|P). This reduces the com-\nplexity for the CDF computation.\nHPT Construction. The construction of the HPT is simple. We\nrandomly sample a small fraction (e.g., 1%) of the string data set\nduring bulkloading, and compute the HPT using the sample. First,\nwe initialize the HPT table to all 0s. Second, we iterate through\nall the string keys in the sample. For each string, we extract all\n(prefixP, characterğ‘) pairs, and increment the corresponding cell\nHPT[â„ğ‘ğ‘ â„(P)][ğ‘]. After the processing, each cell contains the fre-\nquency of (â„ğ‘ğ‘ â„(P),ğ‘). Finally, we process each row in the HPT.\nWe compute the accumulate frequencies, and divide them by the\ntotal frequencies in the row to obtain ğ‘ğ‘‘ğ‘“(ğ‘|â„ğ‘ğ‘ â„(P)) . We store\n\nYifan Yang Shimin Chen\ncdf prob a b c\n0 0.16 0.02 0.18 0.16 0.34 0.66\n1 0.05 0.32 0.37 0.55 0.92 0.08\n2 0.03 0.31 0.34 0.32 0.66 0.34\n3 0.14 0.07 0.21 0.78 0.99 0.01HPT a b c\n0 0.16 0.02 0.18 0.16 0.34 0.66\n1 0.05 0.32 0.37 0.55 0.92 0.08\n2 0.03 0.31 0.34 0.32 0.66 0.34\n3 0.14 0.07 0.21 0.78 0.99 0.01a b c\n0 0.16 0.02 0.18 0.16 0.34 0.66\n1 0.05 0.32 0.37 0.55 0.92 0.08\n2 0.03 0.31 0.34 0.32 0.66 0.34\n3 0.14 0.07 0.21 0.78 0.99 0.01alphabet\nhash \nvaluesiteration 1:\ncdf += prob * 0.18\nprob*= 0.16iteration  2:\ncdf += prob * 0.14\nprob*= 0.07iteration  3:\ncdf += prob * 0.92\nprob*= 0.08\nhash(â€œâ€)=0\nHPT(0, â€˜b â€™) .cdf= 0.18\nHPT(0 , â€˜bâ€™).prob = 0.16hash(â€œbâ€)=3\nHPT(3, â€˜ aâ€™).cdf = 0.14\nHPT(3 , â€˜aâ€™).prob = 0.07hash(â€œ baâ€)=1\nHPT(1, â€˜ câ€™).cdf = 0.92\nHPT(1 , â€˜câ€™).prob = 0.08bac bac baciteration 0:\ncdf = 0\nprob = 1\nFigure 4: An illustration of the CDF computation using the HPT for string â€œbacâ€. (purple: prefix; red: current character)\nAlgorithm 1 HPT-based CDF computation.\n1:procedure GetCDF (HPT, string S)\n2: cdf = 0, prob = 1\n3: forğ‘˜=0toğ‘™ğ‘’ğ‘›(ğ‘†)âˆ’1do\n4: hashval = (k == 0 ? 0 : hash(S[0:k-1]))\n5: c = S[k]\n6: cdf += prob * HPT[hashval][c].cdf\n7: prob *= HPT[hashval][c].prob\n8: return cdf\nğ‘ğ‘‘ğ‘“(ğ‘|â„ğ‘ğ‘ â„(P)) in HPT[â„ğ‘ğ‘ â„(P)][ğ‘].cdf andğ‘ğ‘‘ğ‘“(ğ‘+1|â„ğ‘ğ‘ â„(P)) -\nğ‘ğ‘‘ğ‘“(ğ‘|â„ğ‘ğ‘ â„(P)) in HPT[â„ğ‘ğ‘ â„(P)][ğ‘].prob.\nModel Prediction. Algorithm 1 shows the computation of ğ‘ğ‘‘ğ‘“(ğ‘†)\nusing the HPT. The initialization in Line 2 corresponds to ğ‘ğ‘‘ğ‘“(P0)\nandğ‘ğ‘Ÿğ‘œğ‘(P0). Then, we use Eqn 1 (Line 6) and Eqn 2 (Line 7) to\niteratively compute the CDF and probability of the current prefix,\nrespectively. To reduce the cost of the hash computation in Line\n4, we keep an internal state and incrementally update the state\nwith the next character in the string. Then, we can compute the\nhash value of the prefix with ğ‘‚(1)cost. The loop proceeds until\nthe CDF of the string S is computed. Figure 4 shows an example\ncomputation of ğ‘ğ‘‘ğ‘“(ğ‘ğ‘ğ‘).\nLITS combines the global HPT and the per-node linear model in\nmodel prediction. In a model-based node, the predicted position for\nstringğ‘†is computed as ğ‘¦(ğ‘†)=ğ›¼Ã—ğ‘¥+ğ›½, whereğ‘¥=ğºğ‘’ğ‘¡ğ¶ğ·ğ¹ (HPT,\nğ‘†). Note that we exclude the common prefix in this computation.\nBenefits of the HPT-Based Model. First, compared to the uniform\nassumption in the existing linear models, our HPT-based model\nbetter captures the distribution of the string data set. Therefore, it\ncan distinguish string keys more effectively. Second, the hashing\ndesign in the HPT reduces the space overhead for recording the\nconditional probability distributions. One can adjust the number of\nHPT rows to balance the space cost and the estimation quality. The\nlarger the HPT table, the higher the estimation quality. However, a\nvery large HPT table not only causes significant space overhead,\nbut also incurs random memory accesses and CPU cache misses for\nHPT lookups. Therefore, we set the HPT table size (e.g., 2MB in our\nexperiments) to be small enough to fit in the CPU cache. Finally, our\ndesign is computationally efficient. The HPT construction using\na sample of string keys is simple and fast. Storing the conditional\nCDFs in the HPT reduces the cost for computing the sum term in\nEqn 1. Hence, model prediction takes ğ‘‚(ğ‘™ğ‘’ğ‘›(ğ‘†))time.\nAnalysis of HPT Accuracy. We have the following theorem for\nthe accuracy of approximating the conditional probability. (Pleaserefer to Appendix A.1 for the proof.)\nTheorem 3.1. If prefixPappearsğ‘›Ptimes in the string data set,\nand the HPT[ â„ğ‘ğ‘ â„(P)] row seesğ‘‘occurrences of other prefixes, then\n|ğ»ğ‘ƒğ‘‡[â„ğ‘ğ‘ â„(P)][ğ‘].ğ‘ğ‘Ÿğ‘œğ‘âˆ’ğ‘ğ‘Ÿğ‘œğ‘(ğ‘|P)|â‰¤1ğ‘›P\nğ‘‘+1.\nFor a popular prefix P, we expect ğ‘›Pâ‰«ğ‘‘with a reasonable\nsized HPT (e.g., 2MB). In such cases, the absolute error of the HPT\napproximation is small. Our experiments confirm this result. For\nthe string data sets in our experiments, the average absolute error\nof the conditional probability is 0.0006â€“0.006 for popular prefixes\nthat appear at least 10,000 times.\nDealing with Data Distribution Changes. If the data distribution\nchanges, HPT may become less accurate, leading to degraded index\nperformance. To handle data changes, LITS can sample the index\nperformance (e.g., for 1% of the queries). If it observes that the\nindex performance falls below a pre-defined water mark (e.g., 50%\nof the average performance after bulkloading), LITS can judiciously\nretrain the HPT model and rebuild the entire index.\n3.3 Compact Leaf Node\nThe collision-drive design in the model-based nodes avoids the last-\nmile search by creating new child nodes. However, we observe that\nit can result in small nodes with only two or a few keys, as illustrated\nin Figure 5. The figure depicts a subtree with four model-based\nnodes, i.e.,ğ‘0â€“ğ‘3. The four nodes are in three different tree levels,\nwhile the entire subtree rooted at ğ‘0contains only five kv-entries.\nThis subtree structure is sub-optimal for the following reasons. First,\nit degrades scan performance. Suppose ğ‘˜ğ‘£1â€“ğ‘˜ğ‘£5are retrieved by a\nscan operation. Then the scan has to traverse four nodes in three\nlevels, incurring expensive CPU cache misses and significant book-\nkeeping overhead for entering nodes and backtracing. Second, the\nsmall nodes tend to increase the tree height, adversely impacting\npoint operations. As shown in the example, ğ‘˜ğ‘£1â€“ğ‘˜ğ‘£4are located\ntwo levels deeper than ğ‘˜ğ‘£5. A search for ğ‘˜ğ‘£1is more costly than\nğ‘˜ğ‘£5. Finally, the small nodes increase the space cost for storing the\nper-node headers and the child node pointers.\nTo address this problem, we replace a number of small nodes\nwith a single compact leaf node. As illustrated in Figure 5, the four\nnodes are replaced with a compact leaf node, holding the five kv-\nentries. Each h-pointer stores a 16-bit hash of the key for better\nsearch performance. A search in a compact node sequentially com-\npares the hash of the search key with the hash in every h-pointer.\nOnly when there is a match does LITS dereference the pointer to\n\nLITS: An Optimized Learned Index for Strings\n(An Extended Version)\nN0: model -based node  \n head  N1 ... kv5 \nN1 \n head  N2 N3 ... \nN2 \n head  kv1 kv2 C0: compact leaf  \n head  kv1 kv2 kv3 kv4 kv5 \n16-bit hash 48-bit pointer  pointer to a model -based node  \npointer to a kv-entry  \nan empty item  \nh-pointer (hash enhanced  pointer)  h-pointer to a kv-entry  \nN3 \n head  kv3 kv4 \nFigure 5: Replacing multiple nodes with a compact leaf node.\nvisit the kv-entry. The false positive rate with the 16-bit hash is\n0.0015%. Compared to the common binary search, the h-pointer\nbased search can effectively avoid the high cost of unnecessary\nkv-entry dereference and key comparison. Moreover, the h-pointer\narray is sorted in the key order so that the scan iterator avoids the\ncost of sorting the keys in the compact node.\nWe discuss two important design choices of the compact node.\nâ€¢Size threshold ğ‘¤of compact nodes : A compact node can hold up\ntoğ‘¤keys. Ifğ‘¤is too small, compact nodes may not effectively\nreduce the small nodes in the index. On the other hand, if ğ‘¤is\ntoo large, the search performance suffers because it takes ğ‘‚(ğ‘¤)\ntime to sequentially examine the h-pointers. In Section 4, we\nstudy the impact of ğ‘¤on LITS performance and set ğ‘¤=16 based\non the experiments.\nâ€¢Method to support inserts : We consider two methods to support\ninserts. First, each compact node contains an array of ğ‘¤slots. If\nthere areğ‘˜keys, thenğ‘¤âˆ’ğ‘˜slots are empty. An insert operation\nplaces the new key into the existing array. It moves existing\nelements to keep the sort order. Second, an alternative method\nis to make the node compact. A compact node with ğ‘˜keys is\nstored in an array of ğ‘˜slots. No space is reserved for empty slots.\nThen an insert operation creates a new compact node with one\nmore slot to hold both the existing keys and the new key. Our\nexperiments find that the first method sees substantial space\nwaste because of the reserved empty slots, and both methods\nhave similar performance. Therefore, we choose the second\nmethod as the default design for the compact node.\n3.4 LIT Enhanced with Subtries\nWe call the learned index using HPT-based models and compact\nnodes, LIT ( Learned Index with Hash-enhanced Prefix Table). In\nthe following, we present a hardness metric, GPKL, for string data\nsets. We compare LIT and trie-based indexes experimentally, and\ncombine LIT with HOT using a GPKL-based performance model to\nfurther improve index performance.\nHardness of String Data Sets. Previous study on learned indexes\ndefined a hardness metric for data sets with integer or floating point\nkeys [ 27]. The metric reflects the difficulty of applying linear models\nto approximate the CDF of the data set. However, this metric cannot\nbe directly applied to string data sets because linear models hardly\ncapture the properties of string data sets, as shown in Section 3.2. In\nthis work, we propose a new hardness metric, GPKL (Group Partial\nKey Length), for strings.\nDefinition 3.1 (Common Prefix Length). The common prefix\nlength of a listLof strings, denoted as ğ‘ğ‘ğ‘™(L), is the length of the\nlongest prefix shared by all strings in L.Table 2: Impact of hardness on index performance (Mops).\nString Global Local Read-Only Write-Only\nDataset GPKL GPKL LIT HOT ART LIT HOT ART\nrands* 6.12 2.42 3.37 2.62 3.24 2.41 1.03 1.47\nreddit 8.24 3.48 3.01 1.90 2.39 1.74 1.17 1.52\ngeoname 10.36 4.75 2.88 2.27 2.27 1.62 1.26 1.45\nimdb 10.51 3.79 2.63 2.00 1.97 1.88 1.23 1.35\nphone* 10.84 4.01 2.92 2.01 2.38 1.53 1.18 1.43\naddress 12.61 6.55 2.23 2.08 1.83 1.52 0.94 1.19\nidcard* 12.89 5.04 3.19 1.92 1.62 2.01 1.03 1.02\nwiki 14.32 6.23 1.94 1.68 1.36 1.17 0.98 1.10\nemail* 15.32 5.86 1.88 1.89 1.11 1.06 0.92 1.00\ndblp 20.79 10.19 1.55 1.93 1.30 0.88 0.72 0.83\nurl 47.61 17.79 0.83 1.27 0.78 0.54 0.68 0.58\nnote: * indicates that the data set is synthetically generated.\nDefinition 3.2 (Partial Key Length). Given a sorted list L\nof strings, the partial key of the ğ‘–-th stringğ‘†ğ‘–inLis the shortest\nsubstring ofğ‘†ğ‘–that distinguishes ğ‘†ğ‘–fromğ‘†ğ‘–âˆ’1andğ‘†ğ‘–+1after removing\nthe common prefix of L. The partial key length of ğ‘†ğ‘–, denoted as\nğ‘ğ‘˜ğ‘™(L,ğ‘†ğ‘–), is the length of ğ‘†ğ‘–â€™s partial key.\nğ‘ğ‘˜ğ‘™(L,ğ‘†ğ‘–)can be computed with common prefix lengths as follows:\nğ‘ğ‘˜ğ‘™(L,ğ‘†ğ‘–)=ğ‘šğ‘ğ‘¥(ğ‘ğ‘ğ‘™({ğ‘†ğ‘–âˆ’1,ğ‘†ğ‘–}),ğ‘ğ‘ğ‘™({ğ‘†ğ‘–,ğ‘†ğ‘–+1}))+ 1âˆ’ğ‘ğ‘ğ‘™(L) (4)\nğ‘ğ‘ğ‘™({ğ‘†ğ‘,ğ‘†ğ‘})+1gives the smallest prefix length to distinguish ğ‘†ğ‘\nandğ‘†ğ‘. Hence, the max term plus 1 shows the length of the shortest\nprefix that distinguishes ğ‘†ğ‘–fromğ‘†ğ‘–âˆ’1andğ‘†ğ‘–+1. Then,ğ‘ğ‘˜ğ‘™(L,ğ‘†ğ‘–)is\nobtained by subtracting the common prefix length of all strings in\nLfrom this shortest prefix length.\nDefinition 3.3 (Group Partial Key Length). The group partial\nkey length (GPKL) of a sorted list Lof strings is the average of the\npartial key lengths of strings in L:ğ‘”ğ‘ğ‘˜ğ‘™(L)=1\n|L|Ã\nğ‘†âˆˆLğ‘ğ‘˜ğ‘™(L,ğ‘†).\nWe choose GPKL as the hardness metric for strings for the follow-\ning reasons. First, GPKL measures the difficulty of distinguishing\nkeys in a string data set. The larger the GPKL, the more key bytes\nare necessary to distinguish the strings. Therefore, the metric re-\nflects the hardness of modeling the string data set. Second, GPKL\nskips the common prefix of strings. This behavior mimics the de-\nsign of inner nodes in most string indexes, including HOT, ART,\nSindex, RSS, and LIT. Finally, GPKL can be computed efficiently by\nreading the sorted list of strings in one pass. This makes it possible\nto compute the GPKL online for structure selection decisions.\nWe define both a global GPKL and a local GPKL metric. Given a\nsorted list of strings, the global GPKL is the GPKL of the entire list.\nTo compute the local GPKL, we divide the sorted list into disjoint\nsublists containing ğ‘”consecutive strings in the list. We obtain the\nGPKL for each sublist, then compute the average of the sublist\nGPKLs as the local GPKL. We set ğ‘”=32in the following.\nImpact of Hardness on Index Performance. HOT and ART\nsignificantly outperform existing learned indexes for strings, as\nshown in Section 2.2. Hence, we are interested in comparing LIT\nwith HOT and ART. Table 2 reports the index throughput for both\na read-only workload and a write-only workload. For the read-only\nworkload, we randomly search 20 million keys after bulkloading an\nindex with all keys. For the write-only workload, we bulkload an\nindex with 50% of the keys, and then we measure the throughput\nof randomly inserting the rest of the keys into the index.\n\nYifan Yang Shimin Chen\nAs shown in Table 2, the data sets are arranged in the order of\nincreasing global GPKLs. We see that LIT achieves the best read\nthroughput for 8 data sets and the best write throughput for 10\nout of the 11 data sets. However, for the datasets with the high-\nest hardness values, trie-based indexes have higher performance.\nSpecifically, HOT has the best read performance for email ,dblp ,\nandurl, and the best write performance for url. This finding mo-\ntivates us to combine the strengths of LIT and HOT.\nPerformance Model for Structure Selection (PMSS). To com-\nbine LIT and HOT, our basic idea is to make a decision to choose\nfrom either LIT or HOT when creating a node for a subset of string\nkeys. Obviously, it would be too costly to experimentally compare\nthe two choices online. Therefore, we develop a performance model\n(PMSS) to make quick and accurate online decisions.\nThe PMSS model works as follows. We choose GPKL and the\nnumber (ğ‘›) of strings as two important metrics to characterize a\nsubset of strings. For a given index, the PMSS model provides two\nfunctions,ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘™ğ‘ğ‘¡ (ğ‘”ğ‘ğ‘˜ğ‘™,ğ‘›) andğ‘¤ğ‘Ÿğ‘–ğ‘¡ğ‘’ğ‘™ğ‘ğ‘¡ (ğ‘”ğ‘ğ‘˜ğ‘™,ğ‘›), which estimate\nthe index search latency and the index insert latency, respectively.\nA target workload is specified as containing ğ‘“ğ‘Ÿfraction of reads\nandğ‘“ğ‘¤fraction of writes, where ğ‘“ğ‘Ÿ+ğ‘“ğ‘¤=1. (Operation statistics\ncan be updated online to estimate the ğ‘“ğ‘Ÿ/ğ‘“ğ‘¤parameters.) Then,\nwe estimate the average latency of index operations in the target\nworkload as follows:\nğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¦ =ğ‘“ğ‘ŸÂ·ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘™ğ‘ğ‘¡(ğ‘”ğ‘ğ‘˜ğ‘™,ğ‘›)+ğ‘“ğ‘¤Â·ğ‘¤ğ‘Ÿğ‘–ğ‘¡ğ‘’ğ‘™ğ‘ğ‘¡(ğ‘”ğ‘ğ‘˜ğ‘™,ğ‘›)(5)\nWe estimate the latency for each index, and select the design with\nthe lowest latency for the given subset of strings. Figure 6 illustrates\nthis decision process using the PMSS.\nTo obtainğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘™ğ‘ğ‘¡ (ğ‘”ğ‘ğ‘˜ğ‘™,ğ‘›) andğ‘¤ğ‘Ÿğ‘–ğ‘¡ğ‘’ğ‘™ğ‘ğ‘¡ (ğ‘”ğ‘ğ‘˜ğ‘™,ğ‘›), we perform a\nset of offline benchmarking tests using synthetically generated data\nfor various combinations of ğ‘”ğ‘ğ‘˜ğ‘™ andğ‘›, and populate a ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘™ğ‘ğ‘¡\ntable and ağ‘¤ğ‘Ÿğ‘–ğ‘¡ğ‘’ğ‘™ğ‘ğ‘¡ table for each index (i.e., LIT and HOT). In our\nexperiments, we populate the tables for ğ‘”ğ‘ğ‘˜ğ‘™ =3, 5, ..., 21, and ğ‘›=24,\n25, ...,225. The total size of latency tables for LIT and HOT is less\nthan 10KB. Then, for a specific ( ğ‘”ğ‘ğ‘˜ğ‘™,ğ‘›), we can use the latency\ntables to easily compute ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘™ğ‘ğ‘¡ (ğ‘”ğ‘ğ‘˜ğ‘™ ,ğ‘›) andğ‘¤ğ‘Ÿğ‘–ğ‘¡ğ‘’ğ‘™ğ‘ğ‘¡ (ğ‘”ğ‘ğ‘˜ğ‘™ ,ğ‘›).\nFigure 7 displays the results of offline benchmarking tests for\nthe read-only workload. Figure 7(a) shows a heat map. We divide\ntheğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘™ğ‘ğ‘¡ (ğ‘”ğ‘ğ‘˜ğ‘™,ğ‘›) of HOT by that of LIT and then use different\ncolors to represent the speedup. The darker color shows where\nHOT wins, while the brighter color shows where LIT wins. We see\nthat for a fixed ğ‘”ğ‘ğ‘˜ğ‘™ , LIT exhibits a leading advantage as the data\nsize increases. This can be explained by Figure 7(b). As the number\n(ğ‘›) of keys increases, the height of HOT increases significantly,\nroughly following log32(ğ‘›), while the height of LIT only changes\nslightly. As a result, LIT outperforms HOT.\nOne interesting detail is how to generate a synthetic string data\nset with specific ğ‘”ğ‘ğ‘˜ğ‘™ =ğ‘™andğ‘›. First, we generate a random dictio-\nnary to contain 10000 random strings that are 2Bâ€“6B long. The\nstrings are used as prefixes. Second, we generate a set Lofğ‘›ran-\ndom strings, sort the strings, and compute the initial ğ‘”ğ‘ğ‘˜ğ‘™ 0forL,\nwhich is typically small for randomly generated strings. Third, we\nincrease the ğ‘”ğ‘ğ‘˜ğ‘™ ofLas follows. We randomly select ğ‘˜adjacent\nstringsğ‘†ğ‘1,ğ‘†ğ‘2,...,ğ‘†ğ‘ğ‘˜in the sorted list, and compute the common\nprefix length ğ‘ğ‘ğ‘™for theğ‘˜strings. Then, we randomly pick a string\nğ‘†ğ‘from the dictionary, generate a random insert position ğ‘—âˆˆ[0,\nintera ct \nintere sting  \n... ... \nintuition PMSS  n=2025 \n gpkl =3.2 \n 90%  read \n 10%  write \n  LITlat =0.9*289+ 0.1*437=303.8  \n  HOTlat=0.9 *383+ 0.1*671=411.8 \n           Decision: choose LIT  1 \nstring \nkeys \nLITreadlat (3.2,2025)=289ns  \nLITwritelat( 3.2,2025)=437ns  \nHOTreadlat( 3.2,2025)=383ns  \nHOTwritelat( 3.2,2025)=671ns  \n \n2 3 \n4 Figure 6: The decision process with PMSS.\n/uni00000015/uni00000041/uni00000018 /uni00000015/uni00000041/uni00000014/uni00000014 /uni00000015/uni00000041/uni00000014/uni0000001a /uni00000015/uni00000041/uni00000015/uni00000016\n/uni00000051/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000004e/uni00000048/uni0000005c/uni00000056/uni00000016/uni00000019/uni0000001c/uni0000002a/uni00000033/uni0000002e/uni0000002f\n/uni00000015/uni00000041/uni00000016 /uni00000015/uni00000041/uni00000014/uni00000013 /uni00000015/uni00000041/uni00000014/uni0000001a /uni00000015/uni00000041/uni00000015/uni00000017\n/uni00000051/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000004e/uni00000048/uni0000005c/uni00000056/uni00000015/uni00000017\n/uni0000002b/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002f/uni0000002c/uni00000037 /uni0000002b/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002b/uni00000032/uni00000037\n/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018\n(a) speedup of LIT over HOT (b) tree height (GPKL=5)\nFigure 7: Offline benchmark tests for read-only workload.\nğ‘ğ‘ğ‘™], and insert ğ‘†ğ‘into eachğ‘†ğ‘ğ‘–at theğ‘—-th byte. In this way, the\nğ‘”ğ‘ğ‘˜ğ‘™ ofLincreases by at mostğ‘˜Â·ğ‘™ğ‘’ğ‘›(ğ‘†ğ‘)\n|L|. We adjust the location\nof theğ‘˜strings to keep the sort order of L. Finally, we repeat the\nthird step until the ğ‘”ğ‘ğ‘˜ğ‘™ reaches the target ğ‘™.\nStructure Selection Scenarios. LITS performs PMSS-based deci-\nsions in three main scenarios: 1) Bulkload: if a node corresponds\nto over 16 kv-entries, LITS uses PMSS to decide whether to build\na subtrie or a model-based node in the bulkload operation; 2) In-\nsert into a full compact node: When an insert sees a full compact\nnode (with 16 keys), it replaces the compact node with either a\nmodel-based node or a subtrie; 3) Resize a model-based node: Like\nLIPP, LITS performs node resizing if there are too many or too few\nkeys in a model-based node ğ‘ğ‘Ÿ. The resizing process rebuilds the\nsubtree rooted at node ğ‘ğ‘Ÿ, and uses the PMSS to decide whether a\nmodel-based node or a subtrie should be constructed.\nMoreover, LITS detects the case where over 50% of the keys are\nmapped to an index slot in a model-based node. In such a case, LITS\nbuilds a subtrie for the child node corresponding to the index slot.\nThis restriction ensures that the non-subtrie part of the tree is at\nmostğ‘‚(ğ‘™ğ‘œğ‘”ğ‘)high. Note that the 50% restriction is actually quite\nweak, and it has not been triggered in our experiments.\nImplementation Consideration. It should be noted that careful\ndesign is required at the connection point of different structures\nto avoid potentially unnecessary cost. For example, the root node\nof HOT contains a single pointer to the actual first level node.\nTherefore, when creating a HOT subtrie, we do not simply set the\nchild pointer in the item to point to the root node of HOT. Instead,\nwe directly replace that item with the root node of HOT (while also\nhandling the flag bits of both LIT and HOT correctly). In this way,\nwe save a pointer dereference for accessing the HOT subtrie.\n3.5 Cost Analysis\nThe time and space cost of tries (e.g., ART [ 19] and HOT [ 10]) have\nbeen extensively measured and studied. In the following, we mainly\n\nLITS: An Optimized Learned Index for Strings\n(An Extended Version)\nsummarize the time and space cost of the non-subtrie part of LITS.\n(Please see the full description in Appendix A.3 .) Suppose there\nareğ‘keys in the index. Then, in the worst case, (1) the height of\nthe non-subtrie part of LITS is ğ‘‚(ğ‘™ğ‘œğ‘”ğ‘), (2) the search/update cost\nisğ‘‚(ğ‘™ğ‘œğ‘”ğ‘), (3) the amortized insert/delete cost is ğ‘‚(ğ‘™ğ‘œğ‘”2ğ‘), and\n(4) the space cost is ğ‘‚(ğ‘ğ‘™ğ‘œğ‘”ğ‘).\n4 EVALUATION\nIn this section, we compare the performance of LITS with state-of-\nthe-art string indexes, and study the performance benefits of our\nproposed techniques in LITS.\n4.1 Experimental Setup\nMachine Configuration. All experiments are conducted on a\nmachine equipped with two 3.4GHz Intel Xeon Platinum 8380 CPUs\n(with 40 cores / 80 threads per CPU and 60MB L3 cache) and 256GB\nmemory. The machine runs the standard Ubuntu 20.04 Linux. All\nprograms are compiled with GCC 9.4.0 using the O3 optimization\nlevel. To avoid NUMA effects, we perform the experiments using a\nsingle CPU in the machine.\nSolutions to Compare. We compare LITS with five state-of-the-art\ntraditional and learned indexes for strings:\nâ€¢ART (Adaptive Radix Tree) [19]: We find multiple ART imple-\nmentations on github, and choose the one with the highest stars\n(https://github.com/armon/libart.git). The implementation does\nnot support scans. Therefore, we add a scan procedure for ART.\nâ€¢HOT (Height Optimized Trie) [10]: We obtain the code written\nby the HOT authors (https://github.com/speedskater/hot.git).\nâ€¢SIndex [26]: We use the implementation provided by the au-\nthors (https://github.com/curtis-sun/TLI.git). SIndex requires\nall strings to be padded to a uniform length. Therefore, for\nSIndex experiments, we pad the strings in each data set to the\nlength of the longest string in the data set.\nâ€¢RSS (Radix String Spline) [22]: For RSS, we cannot find publicly\navailable code and write our own implementation in C++. We\nfollow the RSS paper to employ the two-gram compression of\nHOPE [ 29] to encode string keys. This improves RSSâ€™s search\nperformance. The reported index performance includes both\nthe encoding of the query key and the actual index operation\nin RSS. However, RSS does not support insertions because it\nstores sorted key-value data in an array, and uses the array\nindexes to indicate the key ranges in tree nodes. An insert\nwould have to change the array and update key ranges in tree\nnodes, which would be very costly. Therefore, we omit RSS for\nall experiments that perform insertions.\nâ€¢SLIPP : We obtain the LIPP [ 28] code provided the LIPP authors\n(https://github.com/Jiacheng-WU/lipp). Then, we modify LIPP\nto support strings as described in Section 2.2. We implement\nthe bulkload and the search operations. We find that SLIPP has\nmuch worse search performance than HOT, ART, and RSS. Since\nSLIPP is clearly less competitive, we choose not to implement\nthe other operations for SLIPP and omit SLIPP for the rest of\nthe experiments. The implementation is written in C++.â€¢LITS: We implement LITS in C++. The size of HPT is 2MB (with\n1024 rows, 128 columns, and 16B per cell). A compact leaf node\nhas a maximum capacity of 16 elements. The index used for\nconstructing a hybrid structure with LIT is HOT, because HOT\ndemonstrates better overall performance compared to ART.\nâ€¢Variants of LITS : To understand the benefit of our proposed\ntechniques, we also implement several variants of LITS. LIT\nis the learned index without subtries. Moreover, we change\nthe learned model in LIT and implement several LIT(model)\nvariants, as will be described in Section 4.3. Furthermore, we\nstudy the combination of LIT with different trie indexes. LITS-A\nis LIT enhanced with ART as the subtrie of choice. (LITS-H is\nLIT enhanced with HOT, which is another name for LITS.)\nAll experiments are conducted using a single thread except for the\nscalability experiments in Section 4.2. For the scalability experi-\nments, we compare LITS with the most competitive solution, HOT.\nThe HOT code supports multiple threads. We implement optimistic\nlocking for LITS to support concurrent threads.\nDatasets. We use seven real-world string data sets in our experi-\nments, as listed in Table 1. (1) address contains 34M addresses in\nthe form of unit-street-city in the US West [ 3]. (2) dblp contains\n7M paper titles in dblp [ 4]. (3) geoname contains 7M geographical\nnames, such as â€œPic des Langounellesâ€ [ 2]. (4) imdb contains 9M\nactor names in imdb [ 5]. (5) reddit contains the user names of 26M\nreddit accounts that have commented since Dec 2017 [ 1]. (6) url\ncontains 63M urls from the CommonCrawl [ 6]. An example is â€œhttp:\n//1000rosanegra.com.ar/index.htmlâ€. (7) wiki contains 43M wiki\ntitles [7]. An example is â€œ1980-81_Mersin_Idmanyurdu_seasonâ€.\nMoreover, we generate four synthetic data sets. (8) email con-\ntains 45M synthetic email addresses generated by the Faker 14.2.1\npackage using Python 3.6.9. (9) idcard contains 63M synthetic Chi-\nnese id-card numbers. A id-card number is a 18-byte string. The\nfirst 6B represents a region, such as a city or a county. The next 8B\nis the birthday in the form of â€œyyyymmddâ€. Then the remaining 4B\nassigns a unique code to distinguish ids with the same 14B prefix.\n(10)phone contains 50M synthesis phone numbers generated by\nthe Faker package. (11) rands contains 50M randomly generated\nstrings. The characters are selected uniformly from atoz.\nFor the experiments, all data sets have been processed to remove\nduplicate strings, strings containing non-ASCII characters, and\nstrings longer than 255 characters. Then, the value for each string\nkey is a randomly generated 64-bit integer.\nWorkload. We use six YCSB core workloads: A (50% read, 50%\nupdate), B (95% read, 5% update), C (100% read), D (95% latest-read,\n5% insert), E (95% short range scan, 5% insert), and F (50% read,\n50% read-modify-write) [ 12]. For all YCSB workloads except the\nread-only workload C, we bulkload the indexes with 80% of the\nkeys in a data set. For the read-only workload, we bulkload 100%\nof the keys. Then, we perform 20M random operations. Search\nkeys are randomly selected from the bulkload keys. Insert keys are\nrandomly selected from the 20% keys that are new. Update keys\nare randomly selected from the entire data set. For an existing key,\nthe entry is modified. For a new key, we will perform an insert\noperation. Unless otherwise noted, the random keys are chosen\nuniformly at random. We also perform a set of experiments where\nthe chosen keys follow the zipf distribution with zipf factor = 1.\n\nYifan Yang Shimin Chen\n/uni00000044/uni00000047/uni00000047/uni00000055/uni00000047/uni00000045/uni0000004f/uni00000053/uni00000048/uni00000050/uni00000044/uni0000004c/uni0000004f\n/uni0000004a/uni00000048/uni00000052/uni00000051/uni00000044/uni00000050/uni00000048/uni0000004c/uni00000047/uni00000046/uni00000044/uni00000055/uni00000047/uni0000004c/uni00000050/uni00000047/uni00000045/uni00000053/uni0000004b/uni00000052/uni00000051/uni00000048 /uni00000055/uni00000044/uni00000051/uni00000047/uni00000056/uni00000055/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000015/uni00000017/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000002b/uni00000032/uni00000037 /uni00000024/uni00000035/uni00000037 /uni00000036/uni0000002c/uni00000051/uni00000047/uni00000048/uni0000005b /uni00000035/uni00000036/uni00000036 /uni00000036/uni0000002f/uni0000002c/uni00000033/uni00000033 /uni0000002f/uni0000002c/uni00000037/uni00000036\n/uni00000044/uni00000047/uni00000047/uni00000055/uni00000047/uni00000045/uni0000004f/uni00000053/uni00000048/uni00000050/uni00000044/uni0000004c/uni0000004f\n/uni0000004a/uni00000048/uni00000052/uni00000051/uni00000044/uni00000050/uni00000048/uni0000004c/uni00000047/uni00000046/uni00000044/uni00000055/uni00000047/uni0000004c/uni00000050/uni00000047/uni00000045/uni00000053/uni0000004b/uni00000052/uni00000051/uni00000048 /uni00000055/uni00000044/uni00000051/uni00000047/uni00000056/uni00000055/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000015/uni00000017/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000002b/uni00000032/uni00000037 /uni00000024/uni00000035/uni00000037 /uni00000036/uni0000002c/uni00000051/uni00000047/uni00000048/uni0000005b /uni0000002f/uni0000002c/uni00000037/uni00000036\n(a) Read-only (YCSB workload C) (b) Insert-only\nFigure 8: Index performance for read-only and insert-only workloads.\n/uni00000044/uni00000047/uni00000047/uni00000055/uni00000047/uni00000045/uni0000004f/uni00000053/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000014/uni00000015/uni00000016/uni0000005a/uni00000052/uni00000055/uni0000004e/uni0000004f/uni00000052/uni00000044/uni00000047/uni00000003/uni00000024\n/uni00000044/uni00000047/uni00000047/uni00000055/uni00000047/uni00000045/uni0000004f/uni00000053/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000014/uni00000015/uni00000016/uni0000005a/uni00000052/uni00000055/uni0000004e/uni0000004f/uni00000052/uni00000044/uni00000047/uni00000003/uni00000025\n/uni00000044/uni00000047/uni00000047/uni00000055/uni00000047/uni00000045/uni0000004f/uni00000053/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000015/uni00000017/uni00000019/uni0000005a/uni00000052/uni00000055/uni0000004e/uni0000004f/uni00000052/uni00000044/uni00000047/uni00000003/uni00000027\n/uni00000044/uni00000047/uni00000047/uni00000055/uni00000047/uni00000045/uni0000004f/uni00000053/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000014/uni0000005a/uni00000052/uni00000055/uni0000004e/uni0000004f/uni00000052/uni00000044/uni00000047/uni00000003/uni00000028\n/uni00000044/uni00000047/uni00000047/uni00000055/uni00000047/uni00000045/uni0000004f/uni00000053/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000014/uni00000015/uni00000016/uni0000005a/uni00000052/uni00000055/uni0000004e/uni0000004f/uni00000052/uni00000044/uni00000047/uni00000003/uni00000029\n/uni00000044/uni00000047/uni00000047/uni00000055/uni00000047/uni00000045/uni0000004f/uni00000053/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000014/uni00000015/uni00000016/uni00000027/uni00000048/uni0000004f/uni00000048/uni00000057/uni00000048/uni00000010/uni00000052/uni00000051/uni0000004f/uni0000005c/uni0000002b/uni00000032/uni00000037 /uni00000024/uni00000035/uni00000037 /uni00000036/uni0000002c/uni00000051/uni00000047/uni00000048/uni0000005b /uni0000002f/uni0000002c/uni00000037/uni00000036/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c\nFigure 9: Index performance for YCSB workloads.\n/uni00000044/uni00000047/uni00000047/uni00000055/uni00000047/uni00000045/uni0000004f/uni00000053/uni00000048/uni00000050/uni00000044/uni0000004c/uni0000004f\n/uni0000004a/uni00000048/uni00000052/uni00000051/uni00000044/uni00000050/uni00000048/uni0000004c/uni00000047/uni00000046/uni00000044/uni00000055/uni00000047/uni0000004c/uni00000050/uni00000047/uni00000045/uni00000053/uni0000004b/uni00000052/uni00000051/uni00000048 /uni00000055/uni00000044/uni00000051/uni00000047/uni00000056/uni00000055/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000015/uni00000017/uni00000019/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000002b/uni00000032/uni00000037 /uni00000024/uni00000035/uni00000037 /uni0000002f/uni0000002c/uni00000037/uni00000036\nFigure 10: Index performance for read-only (YCSB workload\nC) with Zipf distribution.\nApart from the YCSB workloads, we test insert-only and delete-\nonly workloads. For the insert-only workload, we bulkload the\nindexes with 50% keys in a data set, then measure the performance\nof randomly inserting all the remaining keys. For the delete-only\nworkload, we bulkload the indexes with 100% keys, then measure\nthe performance of randomly deleting 50% existing keys.\n4.2 Overall Performance\nFigure 8 and 9 show the overall index performance. We report\nthe results of all 11 data sets for the read-only (YCSB workload C)\nand insert-only workloads in Figure 8. Due to space limitations,\nwe report the results of the four largest real-world data sets, i.e.,\naddress ,dblp ,url,wiki , for the YCSB workload A, B, D, E, F, and\nthe delete-only workload in Figure 9. Experiments on the other\ndata sets show similar trends.\nFigure 10 shows the performance of the read-only workload\nunder the zipf distribution with zipf factor = 1. (Please see more\nexperimental results under the zipf distribution in Appendix A.5.)\nFrom the figures, we see that LITS achieves the best performance\nfor most workloads and data sets. (LITS is slightly slower than HOT\nfor workload E on url). For the read-only workload, LITS achieves\nup to 1.93x and 2.23x improvement over HOT and ART, respectively.\nCompared to SIndex, LITS demonstrates a performance advantageTable 3: Comparing the height of index solutions.\ndata set LITS (base) LITS (hot) HOT ART SIndex RSS SLIPP\naddr 2.7 0.2 7.0 10.2 2 2.0 4.9\ndblp 2.7 1.7 6.8 14.1 2 2.2 7.3\nurl 3.0 2.1 7.8 16.1 2 3.7 9.1\nwiki 2.9 1.0 7.8 11.6 2 2.1 5.7\nof 2.26x-3.91x. LITS also exhibits excellent performance for insert\noperations. Compared to HOT, ART, and SIndex, LITS attains up to\n2.06x, 2.14x, and 5.31x improvement for the insert-only workload,\nrespectively. Similarly, LITS achieves up to 2.43x, 2.27x, and 3.99x\nimprovement over HOT, ART, and SIndex for workload A, B, D, and\nF. For the scan-heavy workload E, LITSâ€™s performance is comparable\nwith HOT, and better than ART and SIndex. Finally, the zipf results\nshow similar trends. Interestingly, under the zipf distribution, nodes\nthat contain popular keys tend to stay in the CPU cache, leading to\nhigher index performance than that with the uniform distribution.\nIndex Height. Table 3 compares the height of different indexes\nafter bulkloading. The height of LITS is composed of two parts:\nLITS (base), which is the height of the LIT structure including\nmodel-based nodes and compact leaf nodes, and LITS (hot), which\nis the height of the subtries. From Table 3, we see that the height\nof LITS is significantly smaller than HOT, ART, and SLIPP. This\npartially explains the good performance of LITS. Note that RSS\nachieves good tree heights. However, RSS suffers from expensive\nlocal search, and for popular duplicate key prefixes, it has to visit\nand compare the string keys.\nBulkload Time. The left figure in Figure 11 compares the bulkload\ntime of LITS, HOT, and ART. We bulkload all the keys in each data\nset. For HOT and ART, we sort the keys, then insert all the strings\ninto the index in the sorted order. From the figure, we see that the\nbulkload time of LITS is comparable to that of HOT.\n\nLITS: An Optimized Learned Index for Strings\n(An Extended Version)\n/uni00000044/uni00000047/uni00000047/uni00000055 /uni00000047/uni00000045/uni0000004f/uni00000053 /uni00000058/uni00000055/uni0000004f /uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000045/uni00000058/uni0000004f/uni0000004e/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni00000048/uni00000046/uni00000052/uni00000051/uni00000047/uni0000000c\n/uni00000044/uni00000047/uni00000047/uni00000055 /uni00000047/uni00000045/uni0000004f/uni00000053 /uni00000058/uni00000055/uni0000004f /uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013/uni0000001a/uni00000011/uni00000018/uni00000014/uni00000013/uni00000011/uni00000013/uni00000050/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002a/uni0000004c/uni00000025/uni0000000c/uni00000021/uni00000003/uni00000014/uni00000013/uni0000002a/uni0000004c/uni00000025/uni00000027/uni00000044/uni00000057/uni00000044 /uni0000002b/uni00000032/uni00000037 /uni00000024/uni00000035/uni00000037 /uni00000036/uni0000002c/uni00000051/uni00000047/uni00000048/uni0000005b /uni00000035/uni00000036/uni00000036 /uni0000002f/uni0000002c/uni00000037/uni00000036\nFigure 11: Bulkload time and memory space consumption\n.\n/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b\n/uni00000006/uni00000057/uni0000004b/uni00000055/uni00000048/uni00000044/uni00000047/uni00000056/uni00000013/uni00000019/uni00000014/uni00000015/uni00000014/uni0000001b/uni00000015/uni00000017\n/uni00000035/uni00000048/uni00000044/uni00000047/uni00000010/uni00000032/uni00000051/uni0000004f/uni0000005c\n/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b\n/uni00000006/uni00000057/uni0000004b/uni00000055/uni00000048/uni00000044/uni00000047/uni00000056/uni00000013/uni00000016/uni00000019/uni0000001c/uni00000014/uni00000015\n/uni0000002c/uni00000051/uni00000056/uni00000048/uni00000055/uni00000057/uni00000010/uni00000032/uni00000051/uni0000004f/uni0000005c/uni0000002b/uni00000032/uni00000037 /uni0000002f/uni0000002c/uni00000037/uni00000036/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c\nFigure 12: Scalability on the 34M address dataset.\nSpace Cost. The right figure in Figure 11 compares the space cost\nof LITS, HOT, ART, Sindex, and RSS. The figure does not display\nthe space cost of SLIPP; it exceeds 10GB in all four datasets. The\ngrey bar shows the raw data size. From the figure, we see that LITS\nconsumes lower space than ART and SIndex. SIndex consumes a lot\nof space for padding all strings to the maximal length in a data set.\nInterestingly, the read-only RSS has the lowest space cost, which is\nconsistent with the RSS paper [ 22]. (Please see the in-depth analysis\nof the space consumption of learned indexes in Appendix A.6 .)\nScalability. We compare LITS and HOT in scalability experiments.\nFor each data set, we bulkload the indexes with 50% keys in the\ndata set. Then, the insert-only workload measures the performance\nof randomly inserting the remaining 50% keys. After that, the read-\nonly workload measures the performance of 10M search operations\nfor keys randomly distributed in the data set.\nFigure 12 reports the index throughput for LITS and HOT varying\nthe number of threads for address data sets. From the figure, we see\nthat both LITS and HOT achieve nearly linear scalability. Compared\nto HOT, LITS achieves 1.19x â€“ 1.31x and 1.52x â€“ 1.64x improvement\nfor the read-only and insert-only workloads, respectively.\n4.3 Benefit of HPT\nTo understand the benefit of the HPT-based model in LITS, we\ncompare HPT with existing learned models for strings:\nâ€¢Simple Model (SM) : The simple method to calculate the CDF of a\nstring is to use the equation ğ‘¥=ğ‘1\n256+...ğ‘ğ‘›\n256ğ‘›to get a monotonic\nvalue based on the characters in the string. SM is used in SLIPP.\nâ€¢Radix Spline (RS) : RS is the default CDF model used in Radix\nString Spline (RSS) [ 22]. In each inner node of RSS, a ğ¾-byte\nsubstring of the key string is converted into an integer, and\na RS model is used to compute the CDF. We use the same\nconfiguration as the RSS paper [ 22]. In the experiments, ğ¾is\nset to 8 and the error-bound in Radix Spline is set to 127.\n/uni00000013/uni00000014\n/uni00000055/uni00000044/uni00000051/uni00000047/uni00000056\n /uni00000055/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057\n /uni0000004a/uni00000048/uni00000052/uni00000051/uni00000044/uni00000050/uni00000048\n /uni0000004c/uni00000050/uni00000047/uni00000045\n/uni00000013/uni00000014\n/uni00000053/uni0000004b/uni00000052/uni00000051/uni00000048\n /uni00000044/uni00000047/uni00000047/uni00000055/uni00000048/uni00000056/uni00000056\n /uni0000004c/uni00000047/uni00000046/uni00000044/uni00000055/uni00000047\n/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000016\n/uni0000005a/uni0000004c/uni0000004e/uni0000004c\n/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000016/uni00000013/uni00000014\n/uni00000048/uni00000050/uni00000044/uni0000004c/uni0000004f\n/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000016\n/uni00000047/uni00000045/uni0000004f/uni00000053\n/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000016\n/uni00000058/uni00000055/uni0000004f\n/uni00000036/uni00000046/uni00000044/uni0000004f/uni00000048/uni00000003/uni00000029/uni00000044/uni00000046/uni00000057/uni00000052/uni00000055/uni00000038/uni00000051/uni0000004c/uni00000054/uni00000058/uni00000048/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048\n/uni0000002b/uni00000033/uni00000037\n/uni00000036/uni00000035/uni00000030/uni0000002c\n/uni00000035/uni00000036\n/uni00000036/uni00000030Figure 13: Unique rate of learned models.\nâ€¢SRMI : SRMI is a string CDF model mentioned in the learned sort\npaper [ 18]. SRMI first converts a string into a floating point\nnumber using ğ‘¥=ğ‘1\n256+...+ğ‘ğ‘›\n256ğ‘›, then employs a two-layer\nRMI to compute the CDF from the coded floating point ğ‘¥.\nEffectiveness for Distinguishing Strings. We would like to com-\npare the effectiveness of the learned models for distinguishing\nstrings in the data sets. For this purpose, we define and measure a\nunique rate metric.\nWe use a learned model to map a set Sof unique strings to an\nitem array of size ğ‘†ğ¹Â·|S|, whereğ‘†ğ¹â‰¥1is the scale factor. In the\nideal situation, a perfect learned model will map every string in S\nto a separate location in the array. However, in the common case,\nthere can be collisions. That is, two or more strings are mapped\nto the same item. The total number of occupied item slots after\nmapping, denoted as ğ‘ğ‘¢ğ‘šğ‘‰ğ‘ğ‘™ğ‘–ğ‘‘ğ‘†ğ‘™ğ‘œğ‘¡ğ‘  , is always less than or equal\nto|S|. We defineğ‘ˆğ‘…ğ‘†ğ¹for scale factor ğ‘†ğ¹as follows:\nğ‘ˆğ‘…ğ‘†ğ¹=ğ‘ğ‘¢ğ‘šğ‘‰ğ‘ğ‘™ğ‘–ğ‘‘ğ‘†ğ‘™ğ‘œğ‘¡ğ‘ \n|S|(6)\nğ‘ˆğ‘…ğ‘†ğ¹is between 0 and 1. The larger the ğ‘ˆğ‘…ğ‘†ğ¹, the more effective\nthat the learned model distinguishes keys in the string data set.\nFigure 13 shows the unique rates of the four learned models\nvarying the scale factor from 1 to 1000 for all the 11 data sets. We\nsee that HPT achieves the best unique rate for all data sets and\nunder all the scale factors. Compared to SM, RS, and SRMI, HPT\nis more powerful in distinguishing strings. For the three data sets\nwith the highest GPKL, i.e., email ,dblp , and url, all the learned\nmodels work quite poorly. These data sets require larger number\nof bytes to discern one string from the adjacent string in the sort\norder, making it hard for the learned models to separate the strings.\nIndex Performance with Different Learned Models. Figure 14\ncompares the performance of LIT with different learned models. We\nchoose to compare LIT instead of LITS because the hybrid structure\nof LITS could mask the performance impact of the learned model.\nThe experiments run the read-only workload (YCSB workload C).\nLIT with the HPT model achieves the best index performance. It is\n1.14x â€“ 3.65x better than the second best, i.e., LIT(SRMI).\n\nYifan Yang Shimin Chen\n/uni00000044/uni00000047/uni00000047/uni00000055/uni00000047/uni00000045/uni0000004f/uni00000053/uni00000048/uni00000050/uni00000044/uni0000004c/uni0000004f\n/uni0000004a/uni00000048/uni00000052/uni00000051/uni00000044/uni00000050/uni00000048/uni0000004c/uni00000047/uni00000046/uni00000044/uni00000055/uni00000047/uni0000004c/uni00000050/uni00000047/uni00000045/uni00000053/uni0000004b/uni00000052/uni00000051/uni00000048 /uni00000055/uni00000044/uni00000051/uni00000047/uni00000056/uni00000055/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000015/uni00000017/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000002f/uni0000002c/uni00000037 /uni0000002f/uni0000002c/uni00000037/uni0000000b/uni00000036/uni00000035/uni00000030/uni0000002c/uni0000000c /uni0000002f/uni0000002c/uni00000037/uni0000000b/uni00000035/uni00000036/uni0000000c /uni0000002f/uni0000002c/uni00000037/uni0000000b/uni00000036/uni00000030/uni0000000c\nFigure 14: Index performance with different learned models.\n/uni00000044/uni00000047/uni00000047/uni00000055 /uni00000047/uni00000045/uni0000004f/uni00000053 /uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000019/uni00000014/uni00000011/uni00000015/uni00000014/uni00000011/uni0000001b/uni0000002c/uni00000051/uni00000056/uni00000048/uni00000055/uni00000057/uni00000010/uni00000032/uni00000051/uni0000004f/uni0000005c\n/uni00000044/uni00000047/uni00000047/uni00000055 /uni00000047/uni00000045/uni0000004f/uni00000053 /uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000036/uni00000046/uni00000044/uni00000051/uni00000010/uni00000032/uni00000051/uni0000004f/uni0000005c/uni0000002f/uni0000002c/uni00000037/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000026/uni00000051/uni00000052/uni00000047/uni00000048 /uni0000002f/uni0000002c/uni00000037/uni00000010/uni00000026/uni0000001b /uni0000002f/uni0000002c/uni00000037/uni00000010/uni00000026/uni00000014/uni00000019 /uni0000002f/uni0000002c/uni00000037/uni00000010/uni00000026/uni00000016/uni00000015/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c\nFigure 15: Comparing LIT with compact node designs.\nHPT Space and Time Cost. The HPT is lightweight. In our exper-\niments, it takes 2MB, which is orders of magnitude smaller than\nthe data set. HPT can easily fit into the CPU cache, and the model\nprediction using HPT is fast. It takes 20â€“50ns to compute the HPT-\nbased model for an 8-byte substring.\n4.4 Benefit of Compact Leaf Node\nWe study the benefit of the compact node in this subsection.\nPerformance Benefit. For the same reason as in Section 4.3, we\nconduct experiments using LIT rather than LITS. We compare LIT\nwithout compact nodes, and LIT with compact nodes whose size\nlimit is set to 8, 16, and 32. Figure 15 reports the insert-only and\nscan-only throughput for the four LIT variants. From the figure,\nwe see that the introduction of compact leaf nodes not only im-\nproves the performance of the scan operations but also enhances\nthe performance of the insert operations. Scan is improved due to\nthe fact that compact nodes place kv-pointers contiguously, thereby\nreducing cache misses for visiting many small nodes during the\nscan process. Insertion is improved because compact nodes tend\nto reduce the tree height and avoid extra cache misses caused by\nvisiting small nodes in deeper levels of the index.\nMoreover, we see that the scan throughput increases as the\nsize limit increases, but becomes relatively flat beyond 16. The\ninsert throughput may even suffer when the size limit exceeds 16.\nTherefore, we set the default size limit of compact nodes to 16 in\nall the other experiments.\nVariants of Compact Node Implementation. We consider two\nvariants: 1) pre-allocating 16 entries in compact nodes; 2) exploiting\nSIMD for cnode search. However, our experimental results show\nthat preallocation incurs up to 93% extra space overhead without\nsignificant performance benefits. The performance improvement\nwith SIMD is less significant since cnode search is only a small part\nof the search procedure. Please see Appendix A.7 for more details.\n/uni00000044/uni00000047/uni00000047/uni00000055 /uni00000047/uni00000045/uni0000004f/uni00000053 /uni00000058/uni00000055/uni0000004f /uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000014/uni00000015/uni00000016/uni00000035/uni00000048/uni00000044/uni00000047/uni00000010/uni00000032/uni00000051/uni0000004f/uni0000005c\n/uni00000044/uni00000047/uni00000047/uni00000055 /uni00000047/uni00000045/uni0000004f/uni00000053 /uni00000058/uni00000055/uni0000004f /uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni0000002c/uni00000051/uni00000056/uni00000048/uni00000055/uni00000057/uni00000010/uni00000032/uni00000051/uni0000004f/uni0000005c/uni0000002f/uni0000002c/uni00000037/uni00000036/uni0000000b/uni0000002f/uni0000002c/uni00000037/uni00000036/uni00000010/uni0000002b/uni0000000c /uni0000002f/uni0000002c/uni00000037/uni00000036/uni00000010/uni00000024 /uni0000002f/uni0000002c/uni00000037/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000cFigure 16: Comparing LITS-H, LITS-A, and LIT.\n4.5 Benefit of LIT Enhanced with Subtries\nThe LITS mentioned in the above subsections are all LITS-H (i.e.,\nthe hybrid structure of LIT and HOT). We prefer this combina-\ntion because the performance characteristics of LIT and HOT are\ncomplementary as shown in Figure 7. HOT performs well for data\nsets with large GPKLs and relatively small data sizes (e.g. dblp ).\nIn comparison, LIT demonstrates better performance for data sets\nwith small GPKLs and large data sizes (e.g., reddit ).\nIn this subsection, we consider the alternative design of combin-\ning LIT and ART, i.e., LITS-A, and LIT without subtries. Figure 16\ncompares the read-only and the insert-only throughput of LITS-H\n(i.e., LITS), LITS-A, and LIT. From the figure, we see that LITS-H\nachieves better performance than LIT, confirming that the hybrid\nstructure indeed improves index performance. For data sets with\nlarge GPKLs (e.g., url), LITS-H brings up to 50% improvement for\nsearch performance. Moreover, compared to LITS-A, LITS-H has\nhigher performance improvement for search. For the insert-only\nworkload, LITS-A and LITS-H show comparable performance.\nGPKL Computation Cost. In the insert-only workload, a single\ninvocation of the GPKL computation takes 0.8â€“1.7us. The total\nGPKL computation time contributes to 0.5%â€“1.3% of the total insert\ntime across all data sets.\n5 CONCLUSION\nIn conclusion, we have presented a novel string index called LITS\n(Learned Index with Hash-enhanced Prefix Table and Sub-tries).\nOur experimental results show that compared to HOT and ART,\nLITS achieves up to 2.43x and 2.27x improvement for point opera-\ntions and comparable scan performance.\nAcknowledgment. This work is partially supported by National\nKey R&D Program of China (2023YFB4503600) and Natural Science\nFoundation of China (62172390).\nREFERENCES\n[1] 2017. reddit. https://www.kaggle.com/datasets/colinmorris/reddit-usernames\n[2]2018. geoname. https://www.kaggle.com/datasets/geonames/geonames-\ndatabase\n[3] 2023. address. https://v2.openaddresses.io/batch-prod/collection-us-west.zip\n[4] 2023. dblp. https://dblp.org/xml/dblp.xml.gz\n[5] 2023. imdb. https://datasets.imdbws.com/name.basics.tsv.gz\n[6] 2023. url. https://data.commoncrawl.org/CC-MAIN-2023-14/\n[7] 2023. wiki. https://dumps.wikimedia.org/enwiki/\n[8]Domenico Amato, GiosuÃ¨ Lo Bosco, and Raffaele Giancarlo. 2023. Neural net-\nworks as building blocks for the design of efficient learned indexes. Neural\nComput. Appl. 35, 29 (2023), 21399â€“21414.\n\nLITS: An Optimized Learned Index for Strings\n(An Extended Version)\n[9] Berk Atikoglu, Yuehai Xu, Eitan Frachtenberg, Song Jiang, and Mike Paleczny.\n2012. Workload analysis of a large-scale key-value store. In ACM SIGMET-\nRICS/PERFORMANCE Joint International Conference on Measurement and Model-\ning of Computer Systems, SIGMETRICS â€™12, London, United Kingdom, June 11-15,\n2012. ACM, 53â€“64.\n[10] Robert Binna, Eva Zangerle, Martin Pichl, GÃ¼nther Specht, and Viktor Leis. 2018.\nHOT: A Height Optimized Trie Index for Main-Memory Database Systems. In\nProceedings of the 2018 International Conference on Management of Data, SIGMOD\nConference 2018, Houston, TX, USA, June 10-15, 2018 . 521â€“534.\n[11] Zhichao Cao, Siying Dong, Sagar Vemuri, and David H. C. Du. 2020. Character-\nizing, Modeling, and Benchmarking RocksDB Key-Value Workloads at Facebook.\nIn18th USENIX Conference on File and Storage Technologies, FAST 2020, Santa\nClara, CA, USA, February 24-27, 2020 . USENIX Association, 209â€“223.\n[12] Brian F. Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell\nSears. 2010. Benchmarking cloud serving systems with YCSB. In Proceedings of\nthe 1st ACM Symposium on Cloud Computing, SoCC 2010, Indianapolis, Indiana,\nUSA, June 10-11, 2010 . 143â€“154.\n[13] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid B. Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned\nIndex. In Proceedings of the 2020 International Conference on Management of Data,\nSIGMOD Conference 2020, online conference [Portland, OR, USA], June 14-19, 2020 .\n969â€“984.\n[14] Paolo Ferragina, Marco Frasca, GiosuÃ¨ Cataldo MarinÃ², and Giorgio Vinciguerra.\n2023. On Nonlinear Learned String Indexing. IEEE Access 11 (2023), 74021â€“74034.\n[15] Ali Hadian and Thomas Heinis. 2020. MADEX: Learning-augmented Algorithmic\nIndex Structures. In AIDB@VLDB 2020, 2nd International Workshop on Applied\nAI for Database Systems and Applications, Held with VLDB 2020, Monday, August\n31, 2020, Online Event / Tokyo, Japan .\n[16] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass learned\nindex. In Proceedings of the Third International Workshop on Exploiting Artificial\nIntelligence Techniques for Data Management, aiDM@SIGMOD 2020, Portland,\nOregon, USA, June 19, 2020 . 5:1â€“5:5.\n[17] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In Proceedings of the 2018 International\nConference on Management of Data, SIGMOD Conference 2018, Houston, TX, USA,\nJune 10-15, 2018 . 489â€“504.\n[18] Ani Kristo, Kapil Vaidya, Ugur Ã‡etintemel, Sanchit Misra, and Tim Kraska. 2020.\nThe Case for a Learned Sorting Algorithm. In Proceedings of the 2020 InternationalConference on Management of Data, SIGMOD Conference 2020, online conference\n[Portland, OR, USA], June 14-19, 2020 . 1001â€“1016.\n[19] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. The adaptive radix\ntree: ARTful indexing for main-memory databases. In 29th IEEE International\nConference on Data Engineering, ICDE 2013, Brisbane, Australia, April 8-12, 2013 .\n38â€“49.\n[20] Pengfei Li, Yu Hua, Jingnan Jia, and Pengfei Zuo. 2021. FINEdex: A Fine-grained\nLearned Index Scheme for Scalable and Concurrent Memory Systems. Proc. VLDB\nEndow. 15, 2 (2021), 321â€“334.\n[21] Pengfei Li, Hua Lu, Rong Zhu, Bolin Ding, Long Yang, and Gang Pan. 2023. DILI:\nA Distribution-Driven Learned Index. Proc. VLDB Endow. 16, 9 (2023), 2212â€“2224.\n[22] Benjamin Spector, Andreas Kipf, Kapil Vaidya, Chi Wang, Umar Farooq Minhas,\nand Tim Kraska. 2021. Bounding the Last Mile: Efficient Learned String Indexing.\nCoRR abs/2111.14905 (2021). arXiv:2111.14905\n[23] Mihail Stoian, Andreas Kipf, Ryan Marcus, and Tim Kraska. 2021. PLEX: Towards\nPractical Learned Indexing. CoRR abs/2108.05117 (2021). arXiv:2108.05117\n[24] Chuzhe Tang, Youyun Wang, Zhiyuan Dong, Gansen Hu, Zhaoguo Wang, Minjie\nWang, and Haibo Chen. 2020. XIndex: a scalable learned index for multicore data\nstorage. In PPoPP â€™20: 25th ACM SIGPLAN Symposium on Principles and Practice of\nParallel Programming, San Diego, California, USA, February 22-26, 2020 . 308â€“320.\n[25] Ke Wang, Guanqun Yang, Yiwei Li, Huanchen Zhang, and Mingyu Gao. 2023.\nWhen Tree Meets Hash: Reducing Random Reads for Index Structures on Persis-\ntent Memories. Proc. ACM Manag. Data 1, 1 (2023), 105:1â€“105:26.\n[26] Youyun Wang, Chuzhe Tang, Zhaoguo Wang, and Haibo Chen. 2020. SIndex: a\nscalable learned index for string keys. In APSys â€™20: 11th ACM SIGOPS Asia-Pacific\nWorkshop on Systems, Tsukuba, Japan, August 24-25, 2020 , Taesoo Kim and Patrick\nP. C. Lee (Eds.). 17â€“24.\n[27] Chaichon Wongkham, Baotong Lu, Chris Liu, Zhicong Zhong, Eric Lo, and\nTianzheng Wang. 2022. Are Updatable Learned Indexes Ready? Proc. VLDB\nEndow. 15, 11 (2022), 3004â€“3017.\n[28] Jiacheng Wu, Yong Zhang, Shimin Chen, Yu Chen, Jin Wang, and Chunxiao Xing.\n2021. Updatable Learned Index with Precise Positions. Proc. VLDB Endow. 14, 8\n(2021), 1276â€“1288.\n[29] Huanchen Zhang, Xiaoxuan Liu, David G. Andersen, Michael Kaminsky, Kim-\nberly Keeton, and Andrew Pavlo. 2020. Order-Preserving Key Compression for\nIn-Memory Search Trees. In Proceedings of the 2020 International Conference on\nManagement of Data, SIGMOD Conference 2020, online conference [Portland, OR,\nUSA], June 14-19, 2020 . 1601â€“1615.\n\nLITS: An Optimized Learned Index for Strings\n(An Extended Version)\nAlgorithm 2 Index search algorithm in LITS.\n1:procedure LITSSearch (string s)\n2: item = root\n3: while truedo\n4: ifitem.type == Trie then\n5: return trieSearch(item.trie(), s)\n6: ifitem.type == SingleEntry then\n7: return singleVerify(item.entry(), s)\n8: ifitem.type == CompactLeaf then\n9: return compactSearch(item.cnode(), s)\n10: ifitem.type == EmptySlot then\n11: return NULL\n12: item = locate(item.modelNode(), s, item.prefixLen())\n13: return NULL\n14:\n15:procedure singleVerify (singleEntry entry, string s)\n16: ifstrcmp(entry.key, s) == 0 then\n17: return entry.value()\n18: else\n19: return NULL\n20:\n21:procedure compactSearch (compactLeaf cnode, string s)\n22: hashVal = hash(s)\n23: forğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘¦ inğ‘ğ‘›ğ‘œğ‘‘ğ‘’ do\n24: ifentry.hashCode == hashVal then\n25: ifentry.key == s then\n26: return entry.value()\n27: return NULL\n28:\n29:procedure locate (modelNode node, string s, int prefixlen)\n30: len = node.getLen()\n31: ifstrncmp(s, node.prefix, prefixlen) < 0 then\n32: return node.itemArray[0]\n33: ifstrncmp(s, node.prefix, prefixlen) > 0 then\n34: return node.itemArray[len-1]\n35: cdf = hpt.getCDF(s + prefixLen)\n36: cdf = node.k * cdf + node.b\n37: pos = max(1, min(len-2, cdf * len))\n38: return node.itemArray[pos]\nA APPENDIX\nA.1 Proof of HPT Accuracy Theorem\nWe prove Theorem 3.1 in Section 3.2.\nProof. Supposeğ‘›Pğ‘is the number of occurrences of prefix P\nfollowed by character ğ‘. Then,ğ‘ğ‘Ÿğ‘œğ‘(ğ‘|P)=ğ‘›Pğ‘\nğ‘›P.\nSuppose for any other prefix hashed to the HPT[ â„ğ‘ğ‘ â„(P)] row,\nthere are in total ğ‘‘ğ‘occurrences of such a prefix followed by charac-\nterğ‘, where 0â‰¤ğ‘‘ğ‘â‰¤ğ‘‘. Then,ğ»ğ‘ƒğ‘‡[â„ğ‘ğ‘ â„(P)][ğ‘].ğ‘ğ‘Ÿğ‘œğ‘ =ğ‘›Pğ‘+ğ‘‘ğ‘\nğ‘›P+ğ‘‘.\nHence, we have the following:\nğ»ğ‘ƒğ‘‡[â„ğ‘ğ‘ â„(P)][ğ‘].ğ‘ğ‘Ÿğ‘œğ‘âˆ’ğ‘ğ‘Ÿğ‘œğ‘(ğ‘|P)=ğ‘›Pğ‘+ğ‘‘ğ‘\nğ‘›P+ğ‘‘âˆ’ğ‘›Pğ‘\nğ‘›P\nâ‰¤ğ‘›Pğ‘+ğ‘‘\nğ‘›P+ğ‘‘âˆ’ğ‘›Pğ‘\nğ‘›P=ğ‘‘(ğ‘›Pâˆ’ğ‘›Pğ‘)\nğ‘›P(ğ‘›P+ğ‘‘)=1âˆ’ğ‘›Pğ‘\nğ‘›P\nğ‘›P\nğ‘‘+1â‰¤1\nğ‘›P\nğ‘‘+1(7)ğ‘ğ‘Ÿğ‘œğ‘(ğ‘|P)âˆ’ğ»ğ‘ƒğ‘‡[â„ğ‘ğ‘ â„(P)][ğ‘].ğ‘ğ‘Ÿğ‘œğ‘ =ğ‘›Pğ‘\nğ‘›Pâˆ’ğ‘›Pğ‘+ğ‘‘ğ‘\nğ‘›P+ğ‘‘\nâ‰¤ğ‘›Pğ‘\nğ‘›Pâˆ’ğ‘›Pğ‘\nğ‘›P+ğ‘‘=ğ‘‘Â·ğ‘›Pğ‘\nğ‘›P(ğ‘›P+ğ‘‘)â‰¤ğ‘‘\nğ‘›P+ğ‘‘=1\nğ‘›P\nğ‘‘+1(8)\nFrom the above, it is clear that the absolute error is at most1ğ‘›P\nğ‘‘+1.\nâ–¡\nA.2 Index Search and Insert Algorithms\nSection 3.1 has described the search, insert, delete, update, and\nscan index operations. To make the description more concrete,\nwe present the pseudo-code of the index search and index insert\nalgorithms in this subsection.\nIndex Search Algorithm. Algorithm 2 lists the procedures for\nsearching a string key ğ‘ in LITS. The starting point is the root\nnode (Line 2). LITS goes into a while loop (Line 3). A loop iteration\nvisits a node at one level of the tree. At each level of the tree, LITS\nexamines the type of the item (Line 4â€“10). As depicted previously\nin Figure 3, the node type is a 3-bit field in a 64-bit item.\nFirst, if the item is a pointer pointing to a sub-trie (e.g. HOT),\nLITS calls the sub-trieâ€™s search function (Line 4â€“5).\nSecond, If the item is a pointer pointing to a single kv-entry,\nLITS invokes singleVerify to verify the key and return the value if\nthe key matches the search key (line 6â€“7).\nThird, if the item is a pointer pointing to a compact leaf node,\nLITS calls compactSearch (Line 8â€“9). compactSearch computes the\nhash value of the search key (line 22), then compares the hash\nvalue with the hash codes stored in the h-pointers (Line 23â€“24). It\ndereferences an h-pointer only if the hash value of the search key\nmatches the hash code in the h-pointer (Line 25â€“26).\nFourth, if the item is an empty slot, LITS returns NULL. The\nNULL return value indicates that the search does not find any\nmatching index entry (Line 10â€“11).\nFinally, if none of the above four conditions are met, the item\nmust be a pointer pointing to a model-based node. Under this cir-\ncumstance, LITS calls the locate procedure to locate the item in\nthe next level of the tree. It compares the prefix recorded in model-\nbased node with the search key (line 31â€“34). If the prefix does not\nmatch, LITS returns the first (the last) item in the item array for the\ncase where the search key prefix is less (greater) than the recorded\nprefix (line 32â€“34). If the prefix matches, LITS locates the item using\nHPT-based learned model (Line 35â€“37).\nIndex Insert Algorithm. Algorithm 3 lists the procedures for\ninserting a string key ğ‘ and its value ğ‘£ğ‘ğ‘™in LITS. Similar to the search\nalgorithm, LITS starts from the root node (Line 2), and searches the\ninsert key using a while loop (Line 5â€“20). A loop iteration visits\na node at one level of the tree. For a model-based node, like the\nsearch algorithm, LITS calls the locate procedure to locate the item\nin the next level of the tree (Line 20). If the item satisfies one of\nthe if-conditions (Line 6â€“18), then LITS processes the insertion by\ninvoking relevant procedures and exits from the while loop.\nFirst, if the item points to a sub-trie, LITS calls the sub-trieâ€™s\ninsert function (Line 6â€“8).\n\nYifan Yang Shimin Chen\nAlgorithm 3 Index insert algorithm in LITS.\n1:procedure LITSInsert (string s, int val)\n2: item = root\n3: result = false\n4: path = []\n5: while truedo\n6: ifitem.type == Trie then\n7: result = trieInsert(item.trie(), s, val)\n8: break\n9: ifitem.type == SingleEntry then\n10: result = singleInsert(item, s, val)\n11: break\n12: ifitem.type == CompactLeaf then\n13: result = compactInsert(item, s, val)\n14: break\n15: ifitem.type == EmptySlot then\n16: item = setEntry(s, val)\n17: result = true\n18: break\n19: recordPath(path, item) âŠ²record the path\n20: item = locate(item.modelNode(), s)\n21: ifresult == true then\n22: incCount(path)\n23: return result\n24:\n25:procedure singleInsert (Item& item, string s, int val)\n26: ifstrcmp(item.entry().key, s) == 0 then âŠ²key exists\n27: return false\n28: else\n29: item = createCnode(entry, s, val)\n30: return true\n31:\n32:procedure compactInsert (Item& item, string s, int val)\n33: cnode = item.cnode()\n34: pos = BinarySearch(cnode, s)\n35: ifpos == -1 then âŠ²key exists\n36: return false\n37: ifcnode.keyCount() < CnodeCapacity then\n38: item = cnode.insert(s, val, pos)\n39: else\n40: item = PMSSBuild(cnode, s, val)\n41: return true\n42:\n43:procedure incCount (Path path)\n44: forğ‘–ğ‘¡ğ‘’ğ‘š inğ‘ğ‘ğ‘¡â„ do\n45: node = item.modelNode()\n46: node.keyCount += 1\n47: ifnode.keyCount >= 2 * node.getLen() then\n48: item = PMSSBuild(node)\n49: return\n50: return\nSecond, If the item points to a single kv-entry, LITS invokes\nsingleInsert (Line 9â€“11). It checks the insert key against the stored\nkey (Line 26). If there is a match, then the key already exists. LITS\nreturns false. Otherwise, LITS creates a new compact node withtwo index entries, and updates the item (Line 29).\nThird, if the item points to a compact leaf node, LITS calls\ncompactInsert (Line 12â€“14). The keys in a compact node are sorted\nfor better scan performance. Hence, compactInsert performs a bi-\nnary search to locate the position to insert ğ‘ (Line 34). If the key\nexists in the compact node, then LITS returns false (Line 35â€“36).\nOtherwise, LITS inserts the new key into the cnode. The insertion\nactually creates a new cnode, copies the existing keys and the new\nkey to the new node, and updates the item (Line 38). If the compact\nnode is full, LITS invokes the PMSS-based decision and creates\neither a model-based node or a sub-trie (Line 40).\nFourth, if the item is an empty slot, LITS stores the pointer to\nthe kv-entry in the item (Line 15â€“18).\nLITS records the path of the model-based nodes from the root\nto the leaf (Line 19). After the while loop, LITS increases the key\ncount in each model-based node on the path (Line 22). The incCount\nprocedure also checks if the key count exceeds twice of the item\narray length in a model-based node (Line 47). If this is the case, it\ninvokes a resizing procedure (Line 48).\nA.3 Complexity Analysis of LITS\nThe time and space cost of tries (e.g., ART [ 19] and HOT [ 10]) have\nbeen extensively measured and studied. In the following, we mainly\nfocus on the time and space cost of LITS after removing the subtries.\nTree Height. In each inner node (i.e., model-based node), we con-\nsider the fraction of keys mapped to each index slot. LITS detects\nthe case where over 50% of the keys are mapped to an index slot. In\nsuch a case, LITS employs a subtrie for the child node correspond-\ning to the index slot. Consequently, it guarantees that the subtree\nrooted at a (non-subtrie) node contains less than 50% of the keys\nof the subtree rooted at its parent model-based node. As a result,\nthe height of the non-subtrie part of LITS is ğ‘‚(ğ‘™ğ‘œğ‘”ğ‘)in the worst\ncase, where ğ‘is the number of keys in the index.\nSearch Cost. A search in LITS moves from the root to a leaf node.\nThe cost depends mainly on the height of the tree. Therefore, the\nsearch cost for the non-subtrie part of LITS is ğ‘‚(ğ‘™ğ‘œğ‘”ğ‘).\nInsert/Delete/Update Cost. Compared to the search cost, the\ninsert cost is more complex. We need to consider the possibility\nof node adjustment. Similar to LIPP [ 28], we can show that the\namortized cost for insert operations is ğ‘‚(ğ‘™ğ‘œğ‘”2ğ‘).\nFirst, we consider the cost of adjusting a node ğ´withğ‘index\nentries. We need to read/copy all the keys, and compute the GPKL.\nThis costsğ‘‚(ğ‘). Ifğ´is a root of a subtree with ğ‘index entries, in\nthe worst case, each level in the subtree could cost ğ‘‚(ğ‘). Hence,\nthe total adjustment cost can be up to ğ‘‚(ğ‘ğ‘™ğ‘œğ‘”ğ‘).\nSecond, we perform amortized analysis. The idea is to save extra\nğ‘‚(ğ‘™ğ‘œğ‘”ğ‘)credits at each node along the traversal path of insert\noperations. Then adjustment operations consume the saved credits.\nIn this way, we can show that the amortized cost for an insert is\nğ‘‚(ğ‘™ğ‘œğ‘”2ğ‘).\nFor the delete cost, we can use a similar argument as the insert\ncost to show that the amortized cost for a delete is ğ‘‚(ğ‘™ğ‘œğ‘”2ğ‘).\nAn update of a value for an existing key performs a search for\nthe key, then an update for the value. The complexity is ğ‘‚(ğ‘™ğ‘œğ‘”ğ‘).\nSpace Cost. First, the HPT is a fixed sized data structure. Second,\nfor every model-based node or compact leaf node, the size of the\n\nLITS: An Optimized Learned Index for Strings\n(An Extended Version)\nYCSB-A (50% read 50% update) YCSB-B (95% read 5% update) YCSB-C (100% read) YCSB-F (50% read 50% rmw)Intel Core i7-4770\n/uni00000044/uni00000047/uni00000047/uni00000055 /uni00000047/uni00000045/uni0000004f/uni00000053/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000015/uni00000017/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000002b/uni00000032/uni00000037 /uni00000024/uni00000035/uni00000037 /uni0000002f/uni0000002c/uni00000037/uni00000036\n/uni00000044/uni00000047/uni00000047/uni00000055 /uni00000047/uni00000045/uni0000004f/uni00000053/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000015/uni00000017/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000002b/uni00000032/uni00000037 /uni00000024/uni00000035/uni00000037 /uni0000002f/uni0000002c/uni00000037/uni00000036\n/uni00000044/uni00000047/uni00000047/uni00000055 /uni00000047/uni00000045/uni0000004f/uni00000053/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000015/uni00000017/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000002b/uni00000032/uni00000037 /uni00000024/uni00000035/uni00000037 /uni0000002f/uni0000002c/uni00000037/uni00000036\n/uni00000044/uni00000047/uni00000047/uni00000055 /uni00000047/uni00000045/uni0000004f/uni00000053/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000015/uni00000017/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000002b/uni00000032/uni00000037 /uni00000024/uni00000035/uni00000037 /uni0000002f/uni0000002c/uni00000037/uni00000036 Intel Core i7-9700\n/uni00000044/uni00000047/uni00000047/uni00000055 /uni00000047/uni00000045/uni0000004f/uni00000053/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000015/uni00000017/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000002b/uni00000032/uni00000037 /uni00000024/uni00000035/uni00000037 /uni0000002f/uni0000002c/uni00000037/uni00000036\n/uni00000044/uni00000047/uni00000047/uni00000055 /uni00000047/uni00000045/uni0000004f/uni00000053/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000015/uni00000017/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000002b/uni00000032/uni00000037 /uni00000024/uni00000035/uni00000037 /uni0000002f/uni0000002c/uni00000037/uni00000036\n/uni00000044/uni00000047/uni00000047/uni00000055 /uni00000047/uni00000045/uni0000004f/uni00000053/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000015/uni00000017/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000002b/uni00000032/uni00000037 /uni00000024/uni00000035/uni00000037 /uni0000002f/uni0000002c/uni00000037/uni00000036\n/uni00000044/uni00000047/uni00000047/uni00000055 /uni00000047/uni00000045/uni0000004f/uni00000053/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000015/uni00000017/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000002b/uni00000032/uni00000037 /uni00000024/uni00000035/uni00000037 /uni0000002f/uni0000002c/uni00000037/uni00000036 Intel Xeon Platinum 8380\n/uni00000044/uni00000047/uni00000047/uni00000055 /uni00000047/uni00000045/uni0000004f/uni00000053/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000015/uni00000017/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000002b/uni00000032/uni00000037 /uni00000024/uni00000035/uni00000037 /uni0000002f/uni0000002c/uni00000037/uni00000036\n/uni00000044/uni00000047/uni00000047/uni00000055 /uni00000047/uni00000045/uni0000004f/uni00000053/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000015/uni00000017/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000002b/uni00000032/uni00000037 /uni00000024/uni00000035/uni00000037 /uni0000002f/uni0000002c/uni00000037/uni00000036\n/uni00000044/uni00000047/uni00000047/uni00000055 /uni00000047/uni00000045/uni0000004f/uni00000053/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000015/uni00000017/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000002b/uni00000032/uni00000037 /uni00000024/uni00000035/uni00000037 /uni0000002f/uni0000002c/uni00000037/uni00000036\n/uni00000044/uni00000047/uni00000047/uni00000055 /uni00000047/uni00000045/uni0000004f/uni00000053/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000015/uni00000017/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000002b/uni00000032/uni00000037 /uni00000024/uni00000035/uni00000037 /uni0000002f/uni0000002c/uni00000037/uni00000036Figure 17: Index performance on three different hardware platforms.\n/uni00000044/uni00000047/uni00000047/uni00000055/uni00000047/uni00000045/uni0000004f/uni00000053/uni00000048/uni00000050/uni00000044/uni0000004c/uni0000004f\n/uni0000004a/uni00000048/uni00000052/uni00000051/uni00000044/uni00000050/uni00000048/uni0000004c/uni00000047/uni00000046/uni00000044/uni00000055/uni00000047/uni0000004c/uni00000050/uni00000047/uni00000045/uni00000053/uni0000004b/uni00000052/uni00000051/uni00000048 /uni00000055/uni00000044/uni00000051/uni00000047/uni00000056/uni00000055/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000015/uni00000017/uni00000019/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000002b/uni00000032/uni00000037 /uni00000024/uni00000035/uni00000037 /uni0000002f/uni0000002c/uni00000037/uni00000036\n/uni00000044/uni00000047/uni00000047/uni00000055/uni00000047/uni00000045/uni0000004f/uni00000053/uni00000048/uni00000050/uni00000044/uni0000004c/uni0000004f\n/uni0000004a/uni00000048/uni00000052/uni00000051/uni00000044/uni00000050/uni00000048/uni0000004c/uni00000047/uni00000046/uni00000044/uni00000055/uni00000047/uni0000004c/uni00000050/uni00000047/uni00000045/uni00000053/uni0000004b/uni00000052/uni00000051/uni00000048 /uni00000055/uni00000044/uni00000051/uni00000047/uni00000056/uni00000055/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000015/uni00000017/uni00000019/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000002b/uni00000032/uni00000037 /uni00000024/uni00000035/uni00000037 /uni0000002f/uni0000002c/uni00000037/uni00000036\n(a) YCSB-A (b) YCSB-B\n/uni00000044/uni00000047/uni00000047/uni00000055/uni00000047/uni00000045/uni0000004f/uni00000053/uni00000048/uni00000050/uni00000044/uni0000004c/uni0000004f\n/uni0000004a/uni00000048/uni00000052/uni00000051/uni00000044/uni00000050/uni00000048/uni0000004c/uni00000047/uni00000046/uni00000044/uni00000055/uni00000047/uni0000004c/uni00000050/uni00000047/uni00000045/uni00000053/uni0000004b/uni00000052/uni00000051/uni00000048 /uni00000055/uni00000044/uni00000051/uni00000047/uni00000056/uni00000055/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000015/uni00000017/uni00000019/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000002b/uni00000032/uni00000037 /uni00000024/uni00000035/uni00000037 /uni0000002f/uni0000002c/uni00000037/uni00000036\n/uni00000044/uni00000047/uni00000047/uni00000055/uni00000047/uni00000045/uni0000004f/uni00000053/uni00000048/uni00000050/uni00000044/uni0000004c/uni0000004f\n/uni0000004a/uni00000048/uni00000052/uni00000051/uni00000044/uni00000050/uni00000048/uni0000004c/uni00000047/uni00000046/uni00000044/uni00000055/uni00000047/uni0000004c/uni00000050/uni00000047/uni00000045/uni00000053/uni0000004b/uni00000052/uni00000051/uni00000048 /uni00000055/uni00000044/uni00000051/uni00000047/uni00000056/uni00000055/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000015/uni00000017/uni00000019/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni0000002b/uni00000032/uni00000037 /uni00000024/uni00000035/uni00000037 /uni0000002f/uni0000002c/uni00000037/uni00000036\n(c) YCSB-C (d) YCSB-F\nFigure 18: Index performance for YCSB A, B, C and F workloads. (Zipf Distribution)\nnode is proportional to the number of index entries that the node\ncontains. Therefore, the space cost of a node with ğ‘index entriesisğ‘‚(ğ‘). Since the tree height is ğ‘‚(ğ‘™ğ‘œğ‘”ğ‘), the space cost of the tree\nisğ‘‚(ğ‘ğ‘™ğ‘œğ‘”ğ‘).\n\nYifan Yang Shimin Chen\nTable 4: Space cost and insert throughput of pre-allocation\nnormalized to that of no pre-allocation.\nData set addr idcards email geoname phones\nspace cost 1.49 1.93 1.01 1.33 1.58\ninsert throughput 1.006 1.000 1.009 1.000 1.019\nData set dblp imdb rands reddit url wiki\nspace cost 1.09 1.17 1.66 1.32 1.01 1.12\ninsert throughput 1.010 1.011 0.992 1.016 1.014 1.000\n/uni00000044/uni00000047/uni00000047/uni00000055 /uni00000047/uni00000045/uni0000004f/uni00000053/uni00000058/uni00000055/uni0000004f/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000050/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002a/uni0000004c/uni00000025/uni0000000c/uni00000016/uni00000017/uni00000003/uni0000002a/uni0000004c/uni00000025 /uni00000014/uni00000013/uni0000001b/uni00000003/uni0000002a/uni0000004c/uni00000025 /uni00000017/uni0000001c/uni00000003/uni0000002a/uni0000004c/uni00000025\n/uni00000027/uni00000044/uni00000057/uni00000044\n/uni00000036/uni00000053/uni00000044/uni00000046/uni00000048/uni00000003/uni00000058/uni00000056/uni00000048/uni00000047/uni00000003/uni00000049/uni00000052/uni00000055/uni00000003/uni0000004e/uni00000048/uni0000005c/uni00000010/uni00000053/uni00000044/uni00000047/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000b/uni00000036/uni0000002c/uni00000051/uni00000047/uni00000048/uni0000005b/uni0000000c\n/uni00000036/uni0000002c/uni00000051/uni00000047/uni00000048/uni0000005b\n/uni00000036/uni0000002f/uni0000002c/uni00000033/uni00000033\nFigure 19: Space used by SIndex and SLIPP.\nA.4 Performance Across Different Platforms\nWe compare LITS, HOT, and ART on three hardware platforms.\nAs described in Section 4.1, the default experimental machine is\nequipped with Intel Xeon Platinum 8380 CPUs. In addition to the\ndefault configuration, we also run experiments on two different\nmachines in this subsection:\nâ€¢Intel Core i7-4770 : The machine is equipped with a 3.9 GHz (turbo\nfrequency) Intel Core i7-4770 (32KB L1 cache, 256 KB L2 cache,\nand 8 MB L3 cache).\nâ€¢Intel Core i7-9700 : The machine is equipped with a 4.7 GHz (turbo\nfrequency) Intel Core i7-9700 (256KB L1 cache, 2 MB L2 cache,\nand a 12 MB L3 cache).\nFigure 17 compares four YCSB workloads on three different hard-\nware platforms. Each row of sub-figures shows the performance on\na hardware platform. Each column of sub-figures shows the perfor-\nmance of a workload. From the figure, we see that LITS achieves\nthe best performance for all the four workloads across the three\nhardware platforms.\nA.5 Performance for Zipf Distribution\nFigure 18 shows the performance of YCSB A, B, C and F workloads\nunder the zipf distribution with zipf factor = 1. Note that workloads\nwith insert operations would incur many redundant and thus invalid\ninsertions under the zipf distribution. Such inserts would be ignored.\nSince this situation is rare in practice, we do not show workloads\nwith insertions. Similarly, repeatedly deleting the popular keys does\nnot make much sense. Therefore, we do not show results for the\ndelete-only workload.\nA.6 In-depth Analysis of Space Consumption of\nLearned Indexes for Strings\nWe study the space consumption of SIndex, RSS, SLIPP, and LITS\nin this subsection.\nâ€¢SIndex : SIndex requires all strings to be padded to the maximal\nlength in a string data set. As shown in Figure 19, the key\npadding significantly increases the space overhead.â€¢RSS: Each RSS inner node has a Radix Spline. The Radix Spline\nis about 1MB large using 18 bits near the root (based on the\ndefault configuration of the Radix Spline code). We follow the\nRSS paper to use 6 bits in the Radix Spline near the leaves. Since\nRSS is read only, it can use an array index to access the key-\nvalue entries in the data array. This reduces the space cost of\nRSS. RSS has the lowest space cost among all compared index\nsolutions, which is consistent with the RSS paper [22].\nâ€¢SLIPP : LIPP (SLIPP) employs an aggressive node allocation strat-\negy. For a node with ğ‘šelements, where ğ‘š<100ğ¾, it allocates\nan item array with 6ğ‘šslots. This incurs significant memory\noverhead for small to medium sized nodes. In comparison, we\nset the maximum item array size to be up to 2x of the number\nof elements to save space in LITS. Moreover, SLIPP is much\nhigher than LITS because its learned models are less effective\nin distinguishing string key prefixes. This also contributes to\nthe large space overhead of SLIPP.\nâ€¢LITS: The following factors contribute to the modest space over-\nhead of LITS. First, the tree height of LITS is low. The HPT\nmodel can more effectively distinguish different string keys.\nThe compact leaf nodes compact subtrees that contain a small\nnumber of elements. Second, we reduce the space for each item\nby exploiting the upper 16 bits of the pointer to store meta\ninformation, as shown in Figure 3 in Section 3.1.\nA.7 Variants of Compact Leaf Node\nWe consider two factors in the implementation of compact leaf\nnodes: pre-allocation vs. no-preallocation (default), and SIMD vs.\nno SIMD (default).\nPre-allocation vs. No Pre-allocation. We compare the two meth-\nods for supporting inserts in compact nodes as described in Sec-\ntion 3.3. Table 4 compares both the space cost and the insert through-\nput of the two methods: pre-allocating 16 entries in compact nodes\nwith reserved empty slots vs. no pre-allocation. The latter is the\ndefault method in LITS. From the table, we see that preallocation\nincurs up to 93% additional space overhead due to the reserved\nempty slots. This impact is particularly pronounced on data sets\nwith lower GPKLs, as LITS tends to have fewer sub-tries and more\ncompact nodes. On the other hand, preallocation does not show sig-\nnificant improvement of insert throughput. As a result, we choose\nthe no-pre-allocation method for compact nodes in LITS to avoid\nthe significant space overhead.\nSIMD vs. No SIMD. We attempt to exploit SIMD instructions to\naccelerate the matching of hash codes in cnode. Specifically, we\nemploy AVX512 to simultaneously check eight 8-byte h-pointers.\nWe match the 16-bit hash codes of the h-pointers against the hash\ncode of the search key. However, our experiments show that the\nimprovement of search performance with SIMD is less than 1%\nacross various data sets. This is because cnode search is only a\nsmall part of the overall search procedure. LITS visits model-based\nnodes from the root node to the parent of the cnode before reaching\nthe cnode. After locating a matching hash code, LITS compares the\nsearch key and the stored string key to verify the match. Both the\ntree traversal and the string key comparison are often more time\nconsuming than the search within the cnode.",
  "textLength": 117008
}