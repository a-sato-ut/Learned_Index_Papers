{
  "paperId": "f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751",
  "title": "Deep Reinforcement Learning",
  "pdfPath": "f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751.pdf",
  "text": "DEEPREINFORCEMENT LEARNING\nYuxi Li (yuxili@gmail.com)\nABSTRACT\nWe discuss deep reinforcement learning in an overview style. We draw a big pic-\nture, ﬁlled with details. We discuss six core elements, six important mechanisms,\nand twelve applications, focusing on contemporary work, and in historical con-\ntexts. We start with background of artiﬁcial intelligence, machine learning, deep\nlearning, and reinforcement learning (RL), with resources. Next we discuss RL\ncore elements, including value function, policy, reward, model, exploration vs.\nexploitation, and representation. Then we discuss important mechanisms for RL,\nincluding attention and memory, unsupervised learning, hierarchical RL, multi-\nagent RL, relational RL, and learning to learn. After that, we discuss RL appli-\ncations, including games, robotics, natural language processing (NLP), computer\nvision, ﬁnance, business management, healthcare, education, energy, transporta-\ntion, computer systems, and, science, engineering, and art. Finally we summarize\nbrieﬂy, discuss challenges and opportunities, and close with an epilogue.1\nKeywords: deep reinforcement learning, deep RL, algorithm, architecture, ap-\nplication, artiﬁcial intelligence, machine learning, deep learning, reinforcement\nlearning, value function, policy, reward, model, exploration vs. exploitation,\nrepresentation, attention, memory, unsupervised learning, hierarchical RL, multi-\nagent RL, relational RL, learning to learn, games, robotics, computer vision, nat-\nural language processing, ﬁnance, business management, healthcare, education,\nenergy, transportation, computer systems, science, engineering, art\n1Work in progress. Under review for Morgan & Claypool: Synthesis Lectures in Artiﬁcial Intelligence and\nMachine Learning. This manuscript is based on our previous deep RL overview (Li, 2017). It beneﬁts from\ndiscussions with and comments from many people. Acknowledgements will appear in a later version.\n1arXiv:1810.06339v1  [cs.LG]  15 Oct 2018\n\nCONTENTS\n1 Introduction 5\n2 Background 8\n2.1 Artiﬁcial Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.2 Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.3 Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.4 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.4.1 Problem Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.2 Value Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.3 Exploration vs. Exploitation . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.4.4 Dynamic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.4.5 Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.4.6 Temporal Difference Learning . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.4.7 Multi-step Bootstrapping . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.4.8 Model-based RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.4.9 Function Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.4.10 Policy Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.4.11 Deep RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.4.12 Brief Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n2.5 Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nPart I: Core Elements 23\n3 Value Function 24\n3.1 Deep Q-Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n3.2 Distributional Value Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.3 General Value Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n4 Policy 30\n4.1 Policy Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n4.2 Actor-Critic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n4.3 Trust Region Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n4.4 Policy Gradient with Off-Policy Learning . . . . . . . . . . . . . . . . . . . . . . 34\n4.5 Benchmark Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n5 Reward 36\n6 Model 39\n2\n\n7 Exploration vs. Exploitation 42\n8 Representation 46\n8.1 Classics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n8.2 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n8.3 Knowledge and Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\nPart II: Important Mechanisms 53\n9 Attention and Memory 54\n10 Unsupervised Learning 56\n11 Hierarchical RL 59\n12 Multi-Agent RL 62\n13 Relational RL 66\n14 Learning to Learn 68\n14.1 Few/One/Zero-Shot Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n14.2 Transfer/Multi-task Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n14.3 Learning to Optimize . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n14.4 Learning Reinforcement Learn . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n14.5 Learning Combinatorial Optimization . . . . . . . . . . . . . . . . . . . . . . . . 72\n14.6 AutoML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\nPart III: Applications 75\n15 Games 76\n15.1 Board Games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\n15.2 Card Games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n15.3 Video Games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n16 Robotics 82\n16.1 Sim-to-Real . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n16.2 Imitation Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n16.3 Value-based Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n16.4 Policy-based Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n16.5 Model-based Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n16.6 Autonomous Driving Vehicles . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n17 Natural Language Processing 86\n3\n\n17.1 Sequence Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n17.2 Machine Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n17.3 Dialogue Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n18 Computer Vision 90\n18.1 Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n18.2 Motion Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\n18.3 Scene Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\n18.4 Integration with NLP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\n18.5 Visual Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n18.6 Interactive Perception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n19 Finance and Business Management 93\n19.1 Option Pricing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n19.2 Portfolio Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n19.3 Business Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n20 More Applications 96\n20.1 Healthcare . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n20.2 Education . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n20.3 Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n20.4 Transportation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n20.5 Computer Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n20.6 Science, Engineering and Art . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n20.6.1 Chemistry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n20.6.2 Mathematics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n20.6.3 Music . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n21 Discussions 103\n21.1 Brief Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n21.2 Challenges and Opportunities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n21.3 Epilogue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\n4\n\n1 I NTRODUCTION\nReinforcement learning (RL) is about an agent interacting with the environment, learning an optimal\npolicy, by trial and error, for sequential decision making problems, in a wide range of ﬁelds in natural\nsciences, social sciences, and engineering (Sutton and Barto, 1998; 2018; Bertsekas and Tsitsiklis,\n1996; Bertsekas, 2012; Szepesv ´ari, 2010; Powell, 2011).\nThe integration of reinforcement learning and neural networks has a long history (Sutton and Barto,\n2018; Bertsekas and Tsitsiklis, 1996; Schmidhuber, 2015). With recent exciting achievements of\ndeep learning (LeCun et al., 2015; Goodfellow et al., 2016), beneﬁting from big data, powerful\ncomputation, new algorithmic techniques, mature software packages and architectures, and strong\nﬁnancial support, we have been witnessing the renaissance of reinforcement learning (Krakovsky,\n2016), especially, the combination of deep neural networks and reinforcement learning, i.e., deep\nreinforcement learning (deep RL).2\nDeep learning, or deep neural networks, has been prevailing in reinforcement learning in the last\nseveral years, in games, robotics, natural language processing, etc. We have been witnessing break-\nthroughs, like deep Q-network (DQN) (Mnih et al., 2015), AlphaGo (Silver et al., 2016a; 2017),\nand DeepStack (Morav ˇc´ık et al., 2017); each of them represents a big family of problems and large\nnumber of applications. DQN (Mnih et al., 2015) is for single player games, and single agent con-\ntrol in general. DQN has implications for most algorithms and applications in RL. DQN ignites\nthis round of popularity of deep reinforcement learning. AlphaGo (Silver et al., 2016a; 2017) is for\ntwo player perfect information zero-sum games. AlphaGo makes a phenomenal achievement on a\nvery hard problem, and sets a landmark in AI. The success of AlphaGo inﬂuences similar games\ndirectly, and Alpha Zero (Silver et al., 2017) has already achieved signiﬁcant successes on chess\nand Shogi. The techniques underlying AlphaGo (Silver et al., 2016a) and AlphaGo Zero (Silver\net al., 2017), namely, deep learning, reinforcement learning, Monte Carlo tree search (MCTS), and\nself-play, will have wider and further implications and applications. As recommended by AlphaGo\nauthors in their papers (Silver et al., 2016a; 2017), the following applications are worth further inves-\ntigation: general game-playing (in particular, video games), classical planning, partially observed\nplanning, scheduling, constraint satisfaction, robotics, industrial control, online recommendation\nsystems, protein folding, reducing energy consumption, and searching for revolutionary new mate-\nrials. DeepStack (Morav ˇc´ık et al., 2017) is for two player imperfect information zero-sum games,\na family of problems with inherent hardness to solve. DeepStack, similar to AlphaGo, also makes\nan extraordinary achievement on a hard problem, and sets a milestone in AI. It will have rich impli-\ncations and wide applications, e.g., in defending strategic resources and robust decision making for\nmedical treatment recommendations (Morav ˇc´ık et al., 2017).\nWe also see novel algorithms, architectures, and applications, like asynchronous methods (Mnih\net al., 2016), trust region methods (Schulman et al., 2015; Schulman et al., 2017b; Nachum et al.,\n2018), deterministic policy gradient (Silver et al., 2014; Lillicrap et al., 2016), combining policy\ngradient with off-policy RL (Nachum et al., 2017; 2018; Haarnoja et al., 2018), interpolated policy\ngradient (Gu et al., 2017), unsupervised reinforcement and auxiliary learning (Jaderberg et al., 2017;\nMirowski et al., 2017), hindsight experience replay (Andrychowicz et al., 2017), differentiable neu-\nral computer (Graves et al., 2016), neural architecture design (Zoph and Le, 2017), guided policy\nsearch (Levine et al., 2016), generative adversarial imitation learning (Ho and Ermon, 2016), multi-\nagent games (Jaderberg et al., 2018), hard exploration Atari games (Aytar et al., 2018), StarCraft\nII Sun et al. (2018); Pang et al. (2018), chemical syntheses planning (Segler et al., 2018), character\nanimation (Peng et al., 2018a), dexterous robots (OpenAI, 2018), OpenAI Five for Dota 2, etc.\nCreativity would push the frontiers of deep RL further w.r.t. core elements, important mechanisms,\nand applications, seemingly without a boundary. RL probably helps, if a problem can be regarded\nas or transformed into a sequential decision making problem.\nWhy has deep learning been helping reinforcement learning make so many and so enormous achieve-\nments? Representation learning with deep learning enables automatic feature engineering and end-\nto-end learning through gradient descent, so that reliance on domain knowledge is signiﬁcantly\n2We choose to abbreviate deep reinforcement learning as ”deep RL”, since it is a branch of reinforcement\nlearning, in particular, using deep neural networks in reinforcement learning, and ”RL” is a well established\nabbreviation for reinforcement learning.\n5\n\nreduced or even removed for some problems. Feature engineering used to be done manually and\nis usually time-consuming, over-speciﬁed, and incomplete. Deep distributed representations exploit\nthe hierarchical composition of factors in data to combat the exponential challenges of the curse of\ndimensionality. Generality, expressiveness and ﬂexibility of deep neural networks make some tasks\neasier or possible, e.g., in the breakthroughs, and novel architectures, algorithms, and applications\ndiscussed above.\nDeep learning, as a speciﬁc class of machine learning, is not without limitations, e.g., as a black-box\nlacking interpretability, as an ”alchemy” without clear and sufﬁcient scientiﬁc principles to work\nwith, with difﬁculties in tuning hyperparameters, and without human intelligence so not being able\nto compete with a baby in some tasks. Deep reinforcement learning exacerbates these issues, and\neven reproducibility is a problem (Henderson et al., 2018). However, we see a bright future, since\nthere are lots of work to improve deep learning, machine learning, reinforcement learning, deep\nreinforcement learning, and AI in general.\nDeep learning and reinforcement learning, being selected as one of the MIT Technology Review 10\nBreakthrough Technologies in 2013 and 2017 respectively, will play their crucial roles in achieving\nartiﬁcial general intelligence. David Silver, the major contributor of AlphaGo (Silver et al., 2016a;\n2017), proposes a conjecture: artiﬁcial intelligence = reinforcement learning + deep learning (Silver,\n2016). We will further discuss this conjecture in Chapter 21.\nThere are several frequently asked questions about deep reinforcement learning as below. We brieﬂy\ndiscuss them in the above, and will further elucidate them in the coming chapters.\n\u000fWhy deep?\n\u000fWhat is the state of the art?\n\u000fWhat are the issues, and potential solutions?\nThe book outline follows. First we discuss background of artiﬁcial intelligence, machine learning,\ndeep learning, and reinforcement learning, as well as benchmarks and resources, in Chapter 2. Next\nwe discuss RL core elements, including value function in Chapter 3, policy in Chapter 4, reward\nin Chapter 5, model in Chapter 6, exploration vs exploitation in Chapter 7, and representation in\nChapter 8. Then we discuss important mechanisms for RL, including attention and memory in\nChapter 9, unsupervised learning in Chapter 10, hierarchical RL in Chapter 11, multi-agent RL in\nChapter 12, relational RL in Chapter 13, and, learning to learn in Chapter 14. After that, we discuss\nvarious RL applications, including games in Chapter 15, robotics in Chapter 16, natural language\nprocessing (NLP) in Chapter 17, computer vision in Chapter 18, ﬁnance and business management\nin Section 19, and more applications in Chapter 20, including, healthcare in Section 20.1, education\nin Section 20.2, energy in Section 20.3, transportation in Section 20.4, computer systems in Sec-\ntion 20.5, and, science, engineering and arts in Section 20.6. We close in Chapter 21, with a brief\nsummary, discussions about challenges and opportunities, and an epilogue.\nFigure 1 illustrates the manuscript outline. The agent-environment interaction sits in the center,\naround which are core elements, next important mechanisms, then various applications.\nMain readers of this manuscript are those who want to get more familiar with deep reinforcement\nlearning, in particular, novices to (deep) reinforcement learning. For reinforcement learning experts,\nas well as new comers, this book are helpful as a reference. This manuscript is helpful for deep\nreinforcement learning courses, with selected topics and papers.\nWe endeavour to discuss recent, representative work, and provide as much relevant information\nas possible, in an intuitive, high level, conceptual approach. We attempt to make this manuscript\ncomplementary to Sutton and Barto (2018), the RL classic focusing mostly on fundamentals in\nRL. However, deep RL is by nature an advanced topic; and most part of this manuscript is about\nintroducing papers, mostly without covering detailed background. (Otherwise, the manuscript length\nwould explode.) Consequently, most parts of this manuscript would be rather technical, somewhat\nrough, without full details. Original papers are usually the best resources for deep understanding.\nDeep reinforcement learning is growing very fast. We post blogs to complement this manuscript,\nand endeavour to track the development of this ﬁeld , at https://medium.com/@yuxili .\n6\n\nagent \nenvironment state \nreward action exploration representation model \npolicy value \nfunction reward multi-agent \nRL\nhierarchical \nRL\nlearning \nto learn attention \nand\nmemory \nunsupervised \nlearning \nrelational \nRLhealthcare business \nmanagement finance education energy transportation \ngames robotics NLPscience \nengineering \nartcomputer \nvision computer \nsystems Figure 1: Outline\nThis manuscript covers a wide spectrum of topics in deep reinforcement learning. Although we\nhave tried our best for excellence, there are inevitably shortcomings or even mistakes. Comments\nand criticisms are welcome.\n7\n\n2 B ACKGROUND\nIn this chapter, we brieﬂy introduce concepts and fundamentals in artiﬁcial intelligence (Russell\nand Norvig, 2009), machine learning, deep learning (Goodfellow et al., 2016), and, reinforcement\nlearning (Sutton and Barto, 2018).\nWe do not give detailed background introduction for artiﬁcial intelligence, machine learning and\ndeep learning; these are too broad to discuss in details here. Instead, we recommend the following\nrecent Nature/Science survey papers: Jordan and Mitchell (2015) for machine learning, and LeCun\net al. (2015) for deep learning. For reinforcement learning, we cover some basics as a mini tutorial,\nand recommend the textbook, Sutton and Barto (2018), two courses, RL course by David Silver at\nUCL (Silver, 2015) and Deep RL course by Sergey Levin at UC Berkeley (Levine, 2018), and a\nrecent Nature survey paper (Littman, 2015). We present some resources for deep RL in Section 2.5.\nIn Figure 2, we illustrate relationship among several concepts in AI and machine learning. Deep re-\ninforcement learning, as the name indicates, is at the intersection of deep learning and reinforcement\nlearning. We usually categorize machine learning as supervised learning, unsupervised learning, and\nreinforcement learning. Deep learning can work with/as supervised learning, unsupervised learning,\nreinforcement learning, and other machine learning approaches.3Deep learning is part of machine\nlearning, which is part of AI. Note that all these ﬁelds are evolving, e.g., deep learning and deep\nreinforcement learning are addressing classical AI problems, like logic, reasoning, and knowledge\nrepresentation. Reinforcement learning can be important for all AI problems, as quoted from Russell\nand Norvig (2009), ”reinforcement learning might be considered to encompass all of AI: an agent\nis placed in an environment and must learn to behave successfully therein”, and, ”reinforcement\nlearning can be viewed as a microcosm for the entire AI problem”.\n2.1 A RTIFICIAL INTELLIGENCE\nArtiﬁcial intelligence (AI) is a very broad area. Even an authoritative AI textbook like Russell and\nNorvig (2009) does not give a precise deﬁnition. Russell and Norvig (2009) discuss deﬁnitions of\nAI from four perspectives in two comparisons: 1) thought process and reasoning vs. behaviour, and,\n2) success in terms of ﬁdelity to human performance vs. rationality, an ideal performance measure.\nWe follow the discussions of Russell and Norvig (2009), list four ways to deﬁne AI, quoting directly\nfrom Russell and Norvig (2009).\nDeﬁnition 1, ”acting humanly”, follows the Turing Test approach. ”The art of creating machines\nthat perform functions that require intelligence when performed by people.” (Kurzweil, 1992) ”The\nstudy of how to make computers do things at which, at the moment, people are better.” (Rich and\nKnight, 1991)\nA computer passes a Turing Test, if a human interrogator can not tell if the written responses to\nsome written question are from a computer or a human. The computer needs the following capa-\nbilities: natural language processing (NLP) for successful communication in English, knowledge\nrepresentation for storage of what it knows or hears, automated reasoning for answering questions\nand drawing new conclusions from the stored information, and, machine learning for adaptation to\nnew scenarios and detection and extrapolation of patterns. In a total Turing Test, video signals are\ninvolved, so that a computer needs more capabilities, computer vision for object perception, and,\nrobotics for object manipulation and motion control. AI researchers have been devoting most efforts\nto the underlying principles of intelligence, mostly covered by the above six disciplines, and less on\npassing the Turing Test, since, duplicating an exemplar is usually not the goal, e.g., an airplane may\nnot simply imitate a bird.\nDeﬁnition 2, ”thinking humanly”, follows the cognitive modeling approach. ”The exciting new\neffort to make computers think ... machines with minds, in the full and literal sense.” (Haugeland,\n3Machine learning includes many approaches: decision tree learning, association rule learning, artiﬁcial\nneural networks, inductive logic programming, support vector machines, clustering, Bayesian networks, rein-\nforcement learning, representation learning, similarity and metric learning, sparse dictionary learning, genetic\nalgorithms, and, rule-based machine learning, according to Wikipedia, https://en.wikipedia.org/\nwiki/Machine_learning . Also check textbooks like Russell and Norvig (2009), Mitchell (1997), and\nZhou (2016).\n8\n\nreinforcement \nlearning \nsupervised \nlearning unsupervised \nlearning deep \nlearning \nartificial \nintelligence machine \nlearning deep \nreinforcement \nlearning Figure 2: Relationship among deep reinforcement learning, deep learning, reinforcement learn-\ning, supervised learning, unsupervised learning, machine learning, and, artiﬁcial intelligence. Deep\nlearning and deep reinforcement learning are addressing many classical AI problems.\n1989) ”[The automation of] activities that we associate with human thinking, activities such as\ndecision-making, problem solving, learning ...” (Bellman, 1978)\nDeﬁnition 3, ”thinking rationally”, follows the ”law of thought” approach. ”The study of mental\nfaculties through the use of computational models.” (Charniak and McDermott, 1985) ”The study of\nthe computation that make it possible to perceive, reason, and act.” (Winston, 1992)\nDeﬁnition 4, ”acting rationally”, follows the rational agent approach. ”Computational Intelligence is\nthe study of the design of intelligent agents.” (Poole et al., 1998) ”AI ... is concerned with intelligent\nbehavior in artifacts.” (Nilsson, 1998) The rational agent approach is more amenable to scientiﬁc\ndevelopment than those based on human behavior or human thought. Russell and Norvig (2009)\nthus focuse on general principles of rational agents and their building components.\nWe list the foundations and the questions they attempt to answer as in Russell and Norvig (2009):\nphilosophy , for questions like, ”Can formal rules be used to draw valid conclusions?”, ”How does\nthe mind arise from a physical brain?”, ”Where does knowledge come from?”, and, ”How does\nknowledge lead to action?”; mathematics , for questions like, ”What are the formal rules to draw valid\nconclusions?”, ”What can be computed?”, and, ”How do we reason with uncertain information?”;\neconomics , for questions like, ”How should we make decisions so as to maximize payoff?”, ”How\nshould we do this when others may not go along?”, and, ”How should we do this when the payoff\nmay be far in the future?”; neuroscience , for questions like, ”How do brains process information?”;\npsychology , for questions like, ”How do humans and animals think and act?”; computer engineering ,\nfor questions like, ”How can we build an efﬁcient computer?”; control theory and cybernetics , for\nquestions like, ”How can artifacts operate under their own control?”; and, linguistics , for questions\nlike, ”How does language relate to thought?”\nRussell and Norvig (2009) present the history of AI as, the gestation of artiﬁcial intelligence (1943-\n1955), the birth of artiﬁcial intelligence (1956), early enthusiasm, great expectations (1952 - 1969),\n9\n\na dose of reality (1966 - 1973), knowledge-based systems: the key to power? (1969 - 1979), AI\nbecomes an industry (1980 - present), the return of neural networks (1986 - present), AI adopts the\nscientiﬁc method (1987 - present), the emergence of intelligent agents (1995 - present), and, the\navailability of very large data sets (2001 -present).\n2.2 M ACHINE LEARNING\nMachine learning is about learning from data and making predictions and/or decisions. Quoting\nfrom Mitchell (1997), ”A computer program is said to learn from experience Ewith respect to some\nclass of tasks Tand performance measure Pif its performance at tasks in T, as measured by P,\nimproves with experience E.”\nUsually we categorize machine learning as supervised, unsupervised, and reinforcement learning.\nIn supervised learning, there are labeled data; in unsupervised learning, there are no labeled data.\nClassiﬁcation and regression are two types of supervised learning problems, with categorical and\nnumerical outputs respectively.\nUnsupervised learning attempts to extract information from data without labels, e.g., clustering and\ndensity estimation. Representation learning is a classical type of unsupervised learning. However,\ntraining deep neural networks with supervised learning is a kind of representation learning. Rep-\nresentation learning ﬁnds a representation to preserve as much information about the original data\nas possible, and, at the same time, to keep the representation simpler or more accessible than the\noriginal data, with low-dimensional, sparse, and independent representations.\nDeep learning, or deep neural networks, is a particular machine learning scheme, usually for super-\nvised or unsupervised learning, and can be integrated with reinforcement learning, for state repre-\nsentation and/or function approximator. Supervised and unsupervised learning are usually one-shot,\nmyopic, considering instant rewards; while reinforcement learning is sequential, far-sighted, con-\nsidering long-term accumulative rewards.\nReinforcement learning is usually about sequential decision making. In reinforcement learning, in\ncontrast to supervised learning and unsupervised learning, there are evaluative feedbacks, but no\nsupervised labels. Comparing with supervised learning, reinforcement learning has additional chal-\nlenges like credit assignment, stability, and, exploration. Reinforcement learning is kin to optimal\ncontrol (Bertsekas, 2012; Sutton et al., 1992), and operations research and management (Powell,\n2011), and is also related to psychology and neuroscience (Sutton and Barto, 2018).\nMachine learning is the basis for big data, data science (Blei and Smyth, 2017; Provost and Fawcett,\n2013), predictive modeling (Kuhn and Johnson, 2013), data mining (Han et al., 2011), information\nretrieval (Manning et al., 2008), etc, and becomes a critical ingredient for computer vision, natural\nlanguage processing, robotics, etc. Probability theory and statistics (Hastie et al., 2009; Murphy,\n2012; Vapnik, 1998) and optimization (Boyd and Vandenberghe, 2004; Bottou et al., 2018) are\nimportant for statistical machine learning. Machine learning is a subset of AI; however, it is evolving\nto be critical for all ﬁelds of AI.\nA machine learning algorithm is composed of a dataset, a cost/loss function, an optimization pro-\ncedure, and a model (Goodfellow et al., 2016). A dataset is divided into non-overlapping training,\nvalidation, and testing subsets. A cost/loss function measures the model performance, e.g., with\nrespect to accuracy, like mean square error in regression and classiﬁcation error rate.\nThe concept of entropy is important for the deﬁnition of loss functions. For a random variable X\nwith distribution p, the entropy is a measure of its uncertainty, denoted by H(X)orH(p),\nH(X),\u0000KX\nk=1p(X=k) log2p(X=k): (1)\nFor binary random variables, X2f0;1g, we havep(X= 1) =\u0012andp(X= 0) = 1\u0000\u0012,\nH(X) =\u0000[\u0012log2\u0012+ (1\u0000\u0012) log2(1\u0000\u0012)]: (2)\n10\n\nKullback-Leibler divergence (KL-divergence) or relative entropy, is one way to measure the dissim-\nilarity of two probability distributions, pandq,\nKL(pkq),KX\nk=1pklogpk\nqk,KX\nk=1pklogpk\u0000pklogqk=\u0000H(p) +H(p;q): (3)\nHereH(p;q)is the cross entropy. We have KL(p;q),\u0000PK\nk=1pklogqk. See Murphy (2012).\nTraining error measures the error on the training data, minimizing which is an optimization problem.\nGeneralization error, or test error, measures the error on new input data, which differentiates machine\nlearning from optimization. A machine learning algorithm tries to make the training error, and the\ngap between training error and testing error small. A model is under-ﬁtting if it can not achieve a\nlow training error; a model is over-ﬁtting if the gap between training error and test error is large.\nA model’s capacity measures the range of functions it can ﬁt. Ockham’s razor4states that, with\nthe same expressiveness, simple models are preferred. Training error and generalization error vs.\nmodel capacity usually form a U-shape relationship. We ﬁnd the optimal capacity to achieve low\ntraining error and small gap between training error and generalization error. Bias measures the\nexpected deviation of the estimator from the true value; while variance measures the deviation of the\nestimator from the expected value, or variance of the estimator. As model capacity increases, bias\ntends to decrease, while variance tends to increase, yielding another U-shape relationship between\ngeneralization error vs. model capacity. We try to ﬁnd the optimal capacity point, of which under-\nﬁtting occurs on the left and over-ﬁtting occurs on the right. Regularization adds a penalty term to\nthe cost function, to reduce the generalization error, but not training error. No free lunch theorem\nstates that there is no universally best model, or best regularizer. An implication is that deep learning\nmay not be the best model for some problems. There are model parameters, and hyperparameters\nfor model capacity and regularization. Cross-validation is used to tune hyperparameters, to strike a\nbalance between bias and variance, and to select the optimal model.\nMaximum likelihood estimation (MLE) is a common approach to derive good estimation of param-\neters. For issues like numerical underﬂow, the product in MLE is converted to summation to obtain\nnegative log-likelihood (NLL). MLE is equivalent to minimizing KL divergence, the dissimilarity\nbetween the empirical distribution deﬁned by the training data and the model distribution. Minimiz-\ning KL divergence between two distributions corresponds to minimizing the cross-entropy between\nthe distributions. In short, maximization of likelihood becomes minimization of the negative log-\nlikelihood (NLL), or equivalently, minimization of cross entropy.\nGradient descent is a common way to solve optimization problems. Stochastic gradient descent\nextends gradient descent by working with a single sample each time, and usually with minibatches.\nImportance sampling is a technique to estimate properties of a particular distribution, by samples\nfrom a different distribution, to lower the variance of the estimation, or when sampling from the\ndistribution of interest is difﬁcult.\nThere are mathematical analysis frameworks for machine learning algorithms. Kolmogorov com-\nplexity (Li and Vit ´anyi, 2008), or algorithmic complexity, studies the notion of simplicity in Ock-\nham’s razor. In probably approximately correct (PAC) learning (Valiant, 1984), a learning algorithm\naims to select a generalization function, to achieve low generalization error with high probability.\nVC dimension (Vapnik, 1998) measures the capacity of a binary classiﬁer.\n2.3 D EEPLEARNING\nDeep learning is in contrast to ”shallow” learning. For many machine learning algorithms, e.g.,\nlinear regression, logistic regression, support vector machines (SVMs), decision trees, and boost-\ning, we have input layer and output layer, and the inputs may be transformed with manual feature\nengineering before training. In deep learning, between input and output layers, we have one or\nmore hidden layers. At each layer except the input layer, we compute the input to each unit, as the\nweighted sum of units from the previous layer; then we usually use nonlinear transformation, or\nactivation function, such as logistic, tanh, or more popularly recently, rectiﬁed linear unit (ReLU),\nto apply to the input of a unit, to obtain a new representation of the input from the previous layer. We\n4”Occam’s razor” is a popular misspelling (Russell and Norvig, 2009).\n11\n\nhave weights on links between units from layer to layer. After computations ﬂow forward from input\nto output, at the output layer and each hidden layer, we can compute error derivatives backward, and\nbackpropagate gradients towards the input layer, so that weights can be updated to optimize some\nloss function.\nA feedforward deep neural network or multilayer perceptron (MLP) is to map a set of input values\nto output values with a mathematical function formed by composing many simpler functions at\neach layer. A convolutional neural network (CNN) is a feedforward deep neural network, with\nconvolutional layers, pooling layers, and, fully connected layers. CNNs are designed to process data\nwith multiple arrays, e.g., colour image, language, audio spectrogram, and video, beneﬁting from\nthe properties of such signals: local connections, shared weights, pooling, and, the use of many\nlayers, and are inspired by simple cells and complex cells in visual neuroscience (LeCun et al.,\n2015). ResNets (He et al., 2016d) are designed to ease the training of very deep neural networks\nby adding shortcut connections to learn residual functions with reference to the layer inputs. A\nrecurrent neural network (RNN) is often used to process sequential inputs like speech and language,\nelement by element, with hidden units to store history of past elements. A RNN can be seen as a\nmultilayer neural network with all layers sharing the same weights, when being unfolded in time of\nforward computation. It is hard for RNN to store information for very long time and the gradient\nmay vanish. Long short term memory networks (LSTM) (Hochreiter and Schmidhuber, 1997) and\ngated recurrent unit (GRU) (Chung et al., 2014) are proposed to address such issues, with gating\nmechanisms to manipulate information through recurrent cells. Gradient backpropagation or its\nvariants can be used for training all deep neural networks mentioned above.\nDropout (Srivastava et al., 2014) is a regularization strategy to train an ensemble of sub-networks\nby removing non-output units randomly from the original network. Batch normalization (Ioffe and\nSzegedy, 2015; Santurkar et al., 2018) and layer normalization (Ba et al., 2016) are designed to\nimprove training efﬁciency.\nDeep neural networks learn representations automatically from raw inputs to recover the compo-\nsitional hierarchies in many natural signals, i.e., higher-level features are composed of lower-level\nones, e.g., in images, the hierarch of objects, parts, motifs, and local combinations of edges. Dis-\ntributed representation is a central idea in deep learning, which implies that many features may\nrepresent each input, and each feature may represent many inputs. The exponential advantages of\ndeep, distributed representations combat the exponential challenges of the curse of dimensionality.\nThe notion of end-to-end training refers to that a learning model uses raw inputs, usually without\nmanual feature engineering, to generate outputs, e.g., AlexNet (Krizhevsky et al., 2012) with raw\npixels for image classiﬁcation, Graves et al. (2013) with a Fourier transformation of audio data for\nspeech recognition; Seq2Seq (Sutskever et al., 2014) with raw sentences for machine translation,\nand DQN (Mnih et al., 2015) with raw pixels and score to play games.\nThere are efforts to design new neural network architectures, like capsules (Sabour et al., 2017;\nHinton et al., 2018). There are also efforts to design deep machine learning architectures without\nneural networks, like DeepForest (Zhou and Feng, 2017; Feng and Zhou, 2017).\n2.4 R EINFORCEMENT LEARNING\nWe provide background of reinforcement learning brieﬂy in this section, as a mini tutorial. It is\nessential to have a good understanding of reinforcement learning, for a good understanding of deep\nreinforcement learning. Deep RL is a particular type of RL, with deep neural networks for state\nrepresentation and/or function approximation for value function, policy, transition model, or reward\nfunction. Silver (2015) is a clear introductory material for reinforcement learning.\nSutton and Barto (2018) present Bibliographical and Historical Remarks at the end of each chapter.\nRussell and Norvig (2009) present Bibliographical and Historical Notes in Chapter 21 Reinforce-\nment Learning. See Barto (2018) for a talk about a brief history of reinforcement learning.\nWe ﬁrst explain some terms in RL parlance. These terms would become clearer after reading the\nrest of this chapter. We put them collectively here to make it convenient for readers to check them.\nThe prediction problem, or policy evaluation, is to compute the state or action value function for a\npolicy. The control problem is to ﬁnd the optimal policy. Planning constructs a value function or a\npolicy with a model.\n12\n\nOn-policy methods evaluate or improve the behaviour policy, e.g., SARSA ﬁts the action-value\nfunction to the current policy, i.e., SARSA evaluates the policy based on samples from the same\npolicy, then reﬁnes the policy greedily with respect to action values. In off-policy methods, an\nagent learns an optimal value function/policy, maybe following an unrelated behaviour policy. For\ninstance, Q-learning attempts to ﬁnd action values for the optimal policy directly, not necessarily\nﬁtting to the policy generating the data, i.e., the policy Q-learning obtains is usually different from\nthe policy that generates the samples. The notion of on-policy and off-policy can be understood as\nsame-policy and different-policy.\nThe exploration vs exploitation dilemma is about the agent needs to exploit the currently best action\nto maximize rewards greedily, yet it has to explore the environment to ﬁnd better actions, when the\npolicy is not optimal yet, or the system is non-stationary.\nIn model-free methods, the agent learns with trail-and-error from experience directly; the model,\ni.e., for state transition and reward, is not known. RL methods that use models are model-based\nmethods; the model may be given, e.g. in the game of computer Go, or learned from experience.\nIn an online mode, training algorithms are executed on data acquired in sequence. In an ofﬂine\nmode, or a batch mode, models are trained on the entire data set.\nWith bootstrapping, an estimate of state or action value is updated from subsequent estimates.\n2.4.1 P ROBLEM SETUP\nAn RL agent interacts with an environment over time. At each time step t, the agent receives a state\nstin a state spaceS, and selects an action atfrom an action space A, following a policy \u0019(atjst),\nwhich is the agent’s behavior, i.e., a mapping from state stto actionsat. The agent receives a scalar\nrewardrt, and transitions to the next state st+1, according to the environment dynamics, or model,\nfor reward function R(s;a), and, state transition probability P(st+1jst;at), respectively. In an\nepisodic problem, this process continues until the agent reaches a terminal state and then it restarts.\nThe return is the discounted, accumulated reward with the discount factor \r2(0;1],\nRt=1X\nk=0\rkrt+k: (4)\nThe agent aims to maximize the expectation of such long term return from each state. The problem\nis set up in discrete state and action spaces. It is not hard to extend it to continuous spaces. In\npartially observable environments, an agent can not observe states fully, but has observations.\nWhen an RL problem satisﬁes the Markov property, i.e., the future depends only on the current state\nand action, but not on the past, it is formulated as a Markov decision process (MDP), deﬁned by\nthe 5-tuple (S;A;P;R;\r). When the system model is available, we use dynamic programming\nmethods: policy evaluation to calculate value/action value function for a policy, value iteration and\npolicy iteration for ﬁnding an optimal policy. When there is no model, we resort to RL methods. RL\nmethods also work when the model is available. An RL environment can be a multi-armed bandit,\nan MDP, a partially observable MDP (POMDP), a game, etc.\n2.4.2 V ALUE FUNCTION\nA value function is a prediction of the expected, accumulative, discounted, future reward, measuring\nhow good each state, or state-action pair, is. The state value,\nv\u0019(s) =E[Rtjst=s];where,Rt=1X\nk=0\rkrt+k; (5)\nis the expected return for following policy \u0019from states. The action value,\nq\u0019(s;a) =E[Rtjst=s;at=a]; (6)\nis the expected return for selecting action ain statesand then following policy \u0019. Value function\nv\u0019(s)decomposes into the Bellman equation:\nv\u0019(s) =X\na\u0019(ajs)X\ns0;rp(s0;rjs;a)[r+\rv\u0019(s0)]: (7)\n13\n\nAn optimal state value,\nv\u0003(s) = max\n\u0019v\u0019(s) = max\naq\u0019\u0003(s;a); (8)\nis the maximum state value achievable by any policy for state s, which decomposes into the Bellman\nequation:\nv\u0003(s) = max\naX\ns0;rp(s0;rjs;a)[r+\rv\u0003(s0)]: (9)\nAction value function q\u0019(s;a)decomposes into the Bellman equation:\nq\u0019(s;a) =X\ns0;rp(s0;rjs;a)[r+\rX\na0\u0019(a0js0)q\u0019(s0;a0)]: (10)\nAn optimal action value function,\nq\u0003(s;a) = max\n\u0019q\u0019(s;a); (11)\nis the maximum action value achievable by any policy for state sand actiona, which decomposes\ninto the Bellman equation:\nq\u0003(s;a) =X\ns0;rp(s0;rjs;a)[r+\rmax\na0q\u0003(s0;a0)]: (12)\nWe denote an optimal policy by \u0019\u0003.\nConsider the shortest path problem as an example. In graph theory, the single-source shortest path\nproblem is to ﬁnd the shortest path between a pair of nodes so that the sum of weights of edges\nin the path is minimized. In RL, the state is the current node. At each node, following the link to\neach neighbour is an action. The transition model indicates that, after choosing a link to follow,\nthe agent goes to a neighbour. The reward is then the negative of link weight/cost/distance. The\ndiscount factor can be \r= 1, since it is an episodic task. The goal is to ﬁnd a path to maximize\nthe negative of the total cost, i.e., to minimize the total distance. An optimal policy is to choose\nthe best neighbour to traverse to achieve the shortest path; and, for each state/node, an optimal\nvalue is the shortest distance from that node to the destination. Dijkstra’s algorithm is an efﬁcient\nalgorithm, with the information of the graph, including nodes, edges, and weights. RL can work\nin a model-free approach, by wandering in the graph according to some policy, without such global\ngraph information. RL algorithms are more general than Dijkstra’s algorithm, although with global\ngraph information, Dijkstra’s algorithm is very efﬁcient.\n2.4.3 E XPLORATION VS . EXPLOITATION\nAn RL agent needs to trade off between exploration of uncertain policies and exploitation of the\ncurrent best policy, a fundamental dilemma in RL. Here we introduce a simple approach, \u000f-greedy,\nwhere\u000f2(0;1), usually a small value close to 0. In \u000f-greedy, an agent selects a greedy action\na= arg maxa2AQ(s;a), for the current state s, with probability 1\u0000\u000f, and, selects a random action\nwith probability \u000f. That is, the agent exploits the current value function estimation with probability\n1\u0000\u000f, and explores with probability \u000f.\nWe will discuss more about the exploration vs exploitation dilemma in Chapter 7, including several\nprinciples: naive methods such as \u000f-greedy, optimistic initialisation, upper conﬁdence bounds, prob-\nability matching, and, information state search, which are developed in the settings of multi-armed\nbandit, but are applicable to RL problems (Silver, 2015).\n2.4.4 D YNAMIC PROGRAMMING\nDynamic programming (DP) is a general method for problems with optimal substructure and over-\nlapping subproblems. MDPs satisfy these properties, where, Bellman equation gives recursive de-\ncomposition, and, value function stores and reuses sub-solutions (Silver, 2015). DP assumes full\nknowledge of the transition and reward models of the MDP. The prediction problem is to evaluate\nthe value function for a given policy, and the control problem is to ﬁnd an optimal value function\nand/or an optimal policy.\n14\n\nIterative policy evaluation is an approach to evaluate a given policy \u0019. It iteratively applies Bellman\nexpectation backup,\nvk+1(s) =X\na2A\u0019(ajs)[R(s;a) +\rX\ns02SP(s0js;a)vk(s0)]; (13)\nso that at each iteration k+ 1, for all states s2S, updatevk+1(s)from value functions of its\nsuccessor states vk(s0). The value function will converge to v\u0019, the value function of the policy \u0019.\nPolicy iteration (PI) alternates between policy evaluation and policy improvement, to generate a\nsequence of improving policies. In policy evaluation, the value function of the current policy is\nestimated to obtain v\u0019. In policy improvement, the current value function is used to generate a\nbetter policy, e.g., by selecting actions greedily with respect to the value function v\u0019. This policy\niteration process of iterative policy evaluation and greedy policy improvement will converge to an\noptimal policy and value function.\nWe may modify the policy iteration step, stopping it before convergence. A generalized policy\niteration (GPI) is composed of any policy evaluation method and any policy improvement method.\nValue iteration (VI) ﬁnds an optimal policy. It iteratively applies Bellman optimality backup,\nv\u0003(s) = max\naR(s;a) +\rX\ns02SP(s0js;a)vk(s0): (14)\nAt each iteration k+ 1, it updates vk+1(s)fromvk(s0), for all states s2S. Such synchronous\nbackup will converge to the value function of an optimal policy. We may have asynchronous DP,\nand approximate DP.\nWe have Bellman equation for value function in Equation (7). Bellman operator is deﬁned as,\n(T\u0019v)(s):=X\na\u0019(ajs)X\ns0;rp(s0;rjs;a)[r+\rv\u0019(s0)]: (15)\nTD ﬁx point is then,\nv\u0019=T\u0019v\u0019: (16)\nWe can deﬁne the Bellman expectation backup operator, in matrix forms,\nT\u0019(v) =R+\rP\u0019v; (17)\nand the Bellman optimality backup operator,\nT\u0003(v) = max\na2ARa+\rPav; (18)\nWe can show that these operators are contractions with ﬁxed points, respectively, which help prove\nthe convergence of policy iteration, value iteration, and certain general policy iteration algorithms.\nSee Silver (2015), Sutton and Barto (2018), and Bertsekas and Tsitsiklis (1996) for more details.\n2.4.5 M ONTE CARLO\nMonte Carlo methods learn from complete episodes of experience, not assuming knowledge of\ntransition nor reward models, and use sample means for estimation. Monte Carlo methods are\napplicable only to episodic tasks.\nWith Monte Carlo methods for policy evaluation, we use empirical mean return rather than expected\nreturn for the evaluation. By law of large numbers, the estimated value function converges to the\nvalue function of the policy.\nOn-policy Monte Carlo control follows a generalized policy iteration scheme. For policy evaluation,\nit uses Monte Carlo policy evaluation for the action value. For policy improvement, it uses \u000f-greedy\npolicy improvement. It can be shown that Greedy in the limit with inﬁnite exploration (GLIE)\nMonte-Carlo control converges to the optimal action-value function (Singh et al., 2000).\nIn off-policy learning, we evaluate a target policy, following a behaviour policy. With off-policy,\nwe can learn with observations from humans or other agents, reuse experience from old policies,\n15\n\nlearn an optimal policy while following an exploratory policy, and, learn multiple policies based on\nexperience of one policy. (Silver, 2015)\nWe can use importance sampling for off-policy Monte Carlo methods, by multiply importance sam-\npling correction weights along the whole episode, to evaluate the target policy with experience\ngenerated by the behaviour policy. This may increase variance dramatically though.\n2.4.6 T EMPORAL DIFFERENCE LEARNING\nTemporal difference (TD) learning is central in RL. TD learning usually refers to the learning meth-\nods for value function evaluation in Sutton (1988). Q-learning (Watkins and Dayan, 1992) and\nSARSA (Rummery and Niranjan, 1994) are temporal difference control methods.\nTD learning (Sutton, 1988) learns value function V(s)directly from experience with TD error,\nwith bootstrapping, in a model-free, online, and fully incremental way. TD learning is a prediction\nproblem. The update rule is,\nV(s) V(s) +\u000b[r+\rV(s0)\u0000V(s)]; (19)\nwhere\u000bis a learning rate, and r+\rV(s0)\u0000V(s)is called the TD error. Algorithm 1 presents the\npseudo code for tabular TD learning. Precisely, it is tabular TD(0) learning, where ”0” indicates it\nis based on one-step returns.\nBootstrapping estimates state or action value function based on subsequent estimates as in the TD\nupdate rule, and is common in RL, like in TD learning, Q-learning, and SARSA. Bootstrapping\nmethods are usually faster to learn, and enable learning to be online and continual. Bootstrapping\nmethods are not instances of true gradient decent, since the target depends on the values to be\nestimated. The concept of semi-gradient descent is then introduced (Sutton and Barto, 2018).\nInput: the policy\u0019to be evaluated\nOutput: value function V\ninitializeVarbitrarily, e.g., to 0 for all states\nforeach episode do\ninitialize state s\nforeach step of episode, state sis not terminal do\na action given by \u0019fors\ntake actiona, observer,s0\nV(s) V(s) +\u000b[r+\rV(s0)\u0000V(s)]\ns s0\nend\nend\nAlgorithm 1: TD learning, adapted from Sutton and Barto (2018)\nOutput: action value function Q\ninitializeQarbitrarily, e.g., to 0 for all states, set action value for terminal states as 0\nforeach episode do\ninitialize state s\nforeach step of episode, state sis not terminal do\na action forsderived byQ, e.g.,\u000f-greedy\ntake actiona, observer,s0\nQ(s;a) Q(s;a) +\u000b[r+\rmaxa0Q(s0;a0)\u0000Q(s;a)]\ns s0\nend\nend\nAlgorithm 3: Q-learning, adapted from Sutton and Barto (2018)\n16\n\nOutput: action value function Q\ninitializeQarbitrarily, e.g., to 0 for all states, set action value for terminal states as 0\nforeach episode do\ninitialize state s\nforeach step of episode, state sis not terminal do\na action forsderived byQ, e.g.,\u000f-greedy\ntake actiona, observer,s0\na0 action fors0derived byQ, e.g.,\u000f-greedy\nQ(s;a) Q(s;a) +\u000b[r+\rQ(s0;a0)\u0000Q(s;a)]\ns s0,a a0\nend\nend\nAlgorithm 2: SARSA, adapted from Sutton and Barto (2018)\nSARSA, representing state, action, reward, (next) state, (next) action, is an on-policy control method\nto ﬁnd an optimal policy, with the update rule,\nQ(s;a) Q(s;a) +\u000b[r+\rQ(s0;a0)\u0000Q(s;a)]: (20)\nAlgorithm 2 presents the pseudo code for tabular SARSA, i.e., tabular SARSA(0).\nQ-learning is an off-policy control method to ﬁnd the optimal policy. Q-learning learns the action\nvalue function, with the update rule,\nQ(s;a) Q(s;a) +\u000b[r+\rmax\na0Q(s0;a0)\u0000Q(s;a)]: (21)\nQ-learning reﬁnes the policy greedily w.r.t. action values by the max operator. Algorithm 3 presents\nthe pseudo code for Q-learning, precisely, tabular Q(0) learning.\nTD-learning, Q-learning and SARSA converge under certain conditions. From an optimal action\nvalue function, we can derive an optimal policy.\n2.4.7 M ULTI -STEP BOOTSTRAPPING\nThe above algorithms are referred to as TD(0), Q(0) and SARSA(0), learning with one-step return.\nWe have variants with multi-step return for them in the forward view. In n-step update, V(st)is\nupdated toward the n-step return, deﬁned as,\nrt+\rrt+1+\u0001\u0001\u0001+\rn\u00001rt+n\u00001+\rnV(st+n): (22)\nThe eligibility trace from the backward view provides an online, incremental implementation, re-\nsulting in TD( \u0015), Q(\u0015) and SARSA( \u0015) algorithms, where \u00152[0;1]. TD(1) is the same as the Monte\nCarlo approach. Eligibility trace is a short-term memory, usually lasting within an episode, assists\nthe learning process, by affecting the weight vector. The weight vector is a long-term memory, last-\ning the whole duration of the system, determines the estimated value. Eligibility trace helps with the\nissues of long-delayed rewards and non-Markov tasks (Sutton and Barto, 2018).\nTD(\u0015) uniﬁes one-step TD prediction, TD(0), with Monte Carlo methods, TD(1), using eligibility\ntraces and the decay parameter \u0015, for prediction algorithms. De Asis et al. (2018) make uniﬁcation\nfor multi-step TD control algorithms.\nCOMPARISON OF DP, MC, AND TD\nIn the following, we compare dynamic programming (DP), Monte Carlo (MC), and temporal differ-\nence (TD) learning, based on Silver (2015) and Sutton and Barto (2018).\nDP requires the model; TD and MC are model-free. DP and TD bootstrap; MC does not. TD and\nMC work with sample backups, DP and exhaustive search work with full backups. DP works with\none step backups; TD works with one or multi-step backups; MC and exhaustive search work with\ndeep backups, until the episode terminates.\n17\n\nTD can learn online, from incomplete sequences, in continuous environments. MC learns from com-\nplete sequences, in episodic environments. TD has low variance, some bias, usually more efﬁcient\nthan MC, more sensitive to initialization. TD(0) converges to value function of a policy, may di-\nverge with function approximation. MC has high variance, zero bias, simple to understand and use,\ninsensitive to initilization. MC has good convergence properties, even with function approximation.\nTD exploits Markov property, so it is more efﬁcient in Markov environments. MC does not assume\nMarkov property, so it is usually more effective in non-Markov environments.\nTable 1 compare DP with TD (Silver, 2015; Sutton and Barto, 2018).\nFull Backup (DP) Sample Backup\niterative policy evaluation TD learning\nv(s) E[r+\rv(s0)js] v(s)\u000b r+\rv(s0)\nQ policy iteration SARSA\nQ(s;a) E[r+\rQ(s0;a0)js;a] Q(s;a)\u000b r+\rQ(s0;a0)\nQ value iteration Q-learning\nQ(s;a) E[r+\rmaxa02AQ(s0;a0)js;a]Q(s;a)\u000b r+\rmaxa02AQ(s0;a0)\nTable 1: Comparison between DP and TD Learning, where x\u000b y:=x \u000b(y\u0000x).\n2.4.8 M ODEL -BASED RL\nSutton (1990) proposes Dyna-Q to integrate learning, acting, and planning, by not only learning\nfrom real experience, but also planning with simulated trajectories from a learned model. Learning\nuses real experience from the environment; and planning uses experience simulated by a model.\nAlgorithm 4 presents the pseudo code for tabular Dyna-Q. We will discuss more about model-based\nRL in Chapter 6.\n//Model (s;a)denotes predicted reward and next state for state action pair (s;a)\ninitializeQ(s;a)andModel (s;a)for alls2S anda2A\nfortruedo\ns current, nonterminal, state\na action forsderived byQ, e.g.,\u000f-greedy\n//acting\ntake actiona; observe reward r, and next state s0\n// direct reinforcement learning\nQ(s;a) Q(s;a) +\u000b[r+\rmaxa0Q(s0;a0)\u0000Q(s;a)]\n// model learning\nModel (s;a) r;s0\n// planning\nforNiterations do\ns random state previously observed\na random action previously taken\nr;s0 Model (s;a)\nQ(s;a) Q(s;a) +\u000b[r+\rmaxa0Q(s0;a0)\u0000Q(s;a)]\nend\nend\nAlgorithm 4: Dyna-Q, adapted from Sutton and Barto (2018)\n2.4.9 F UNCTION APPROXIMATION\nWe discuss tabular cases above, where a value function or a policy is stored in a tabular form.\nFunction approximation is a way for generalization when the state and/or action spaces are large or\ncontinuous. Function approximation aims to generalize from examples of a function to construct\nan approximate of the entire function; it is usually a concept in supervised learning, studied in the\n18\n\nﬁelds of machine learning, patten recognition, and statistical curve ﬁtting; function approximation in\nreinforcement learning usually treats each backup as a training example, and encounters new issues\nlike nonstationarity, bootstrapping, and delayed targets (Sutton and Barto, 2018). Linear function\napproximation is a popular choice, partially due to its desirable theoretical properties, esp. before\nthe work of deep Q-network (Mnih et al., 2015). However, the integration of reinforcement learning\nand neural networks dates back a long time ago (Sutton and Barto, 2018; Bertsekas and Tsitsiklis,\n1996; Schmidhuber, 2015).\nAlgorithm 5 presents the pseudo code for TD(0) with function approximation. ^v(s;w)is the ap-\nproximate value function, wis the value function weight vector, r^v(s;w)is the gradient of the\napproximate value function w.r.t. the weight vector, which is updated following the update rule,\nw w+\u000b[r+\r^v(s0;w)\u0000^v(s;w)]r^v(s;w): (23)\nInput: the policy\u0019to be evaluated\nInput: a differentiable value function ^v(s;w),^v(terminal;\u0001) = 0\nOutput: value function ^v(s;w)\ninitialize value function weight warbitrarily, e.g., w= 0\nforeach episode do\ninitialize state s\nforeach step of episode, state sis not terminal do\na \u0019(\u0001js)\ntake actiona, observer,s0\nw w+\u000b[r+\r^v(s0;w)\u0000^v(s;w)]r^v(s;w)\ns s0\nend\nend\nAlgorithm 5: TD(0) with function approximation, adapted from Sutton and Barto (2018)\nWhen combining off-policy, function approximation, and bootstrapping, instability and divergence\nmay occur (Tsitsiklis and Van Roy, 1997), which is called the deadly triad issue (Sutton and Barto,\n2018). All these three elements are necessary: function approximation for scalability and gener-\nalization, bootstrapping for computational and data efﬁciency, and off-policy learning for freeing\nbehaviour policy from target policy. What is the root cause for the instability? Learning or sampling\nis not, since dynamic programming suffers from divergence with function approximation; explo-\nration, greediﬁcation, or control is not, since prediction alone can diverge; local minima or complex\nnon-linear function approximation is not, since linear function approximation can produce instabil-\nity (Sutton, 2016). It is unclear what is the root cause for instability – each single factor mentioned\nabove is not – there are still many open problems in off-policy learning (Sutton and Barto, 2018).\nTable 2 presents various algorithms that tackle various issues (Sutton, 2016). ADP algorithms re-\nfer to approximate dynamic programming algorithms like policy evaluation, policy iteration, and\nvalue iteration, with function approximation. Least square temporal difference (LSTD) (Bradtke\nand Barto, 1996) computes TD ﬁx-point directly in batch mode. LSTD is data efﬁcient, yet\nwith squared time complexity. LSPE (Nedi ´c and Bertsekas, 2003) extends LSTD. Fitted-Q algo-\nrithms (Ernst et al., 2005; Riedmiller, 2005) learn action values in batch mode. Residual gradient\nalgorithms (Baird, 1995) minimize Bellman error. Gradient-TD (Sutton et al., 2009a;b; Mahmood\net al., 2014) methods are true gradient algorithms, perform SGD in the projected Bellman error\n(PBE), converge robustly under off-policy training and non-linear function approximation. Expected\nSARSA (van Seijen et al., 2009) has the same convergence guarantee as SARSA, with lower vari-\nance. Emphatic-TD (Sutton et al., 2016) emphasizes some updates and de-emphasizes others by\nreweighting, improving computational efﬁciency, yet being a semi-gradient method. See Sutton and\nBarto (2018) for more details. Du et al. (2017) propose variance reduction techniques for policy\nevaluation to achieve fast convergence. Liu et al. (2018a) study proximal gradient TD learning.\nWhite and White (2016) perform empirical comparisons of linear TD methods, and make sugges-\ntions about their practical use. Jin et al. (2018) study the sample efﬁciency of Q-learning. Lu et al.\n(2018) study non-delusional Q-learning and value-iteration.\n19\n\nalgorithm\nTD(\u0015)\nSARSA(\u0015)ADPLSTD(\u0015)\nLSPE(\u0015)Fitted-QResidual\nGradientGTD(\u0015)\nGQ(\u0015)\nlinear\ncomputation X X X Xissuenonlinear\nconvergent X X X\noff-policy\nconvergent X X X\nmodel-free,\nonline X X X X\nconverges to\nPBE = 0 X X X X X\nTable 2: RL Issues vs. Algorithms, adapted from Sutton (2016)\n2.4.10 P OLICY OPTIMIZATION\nIn contrast to value-based methods like TD learning and Q-learning, policy-based methods opti-\nmize the policy \u0019(ajs;\u0012)(with function approximation) directly, and update the parameters \u0012by\ngradient ascent. Comparing with value-based methods, policy-based methods usually have better\nconvergence properties, are effective in high-dimensional or continuous action spaces, and can learn\nstochastic policies. However, policy-based methods usually converge to local optimum, are inef-\nﬁcient to evaluate, and encounter high variance (Silver, 2015). Stochastic policies are important\nsince some problems have only stochastic optimal policies, e.g., in the rock-paper-scissors game, an\noptimal policy for each player is to take each action (rock, paper, or scissors) with probability 1/3.\nFor a differentiable policy \u0019(ajs;\u0012), we can compute the policy gradient analytically, whenever it is\nnon-zero,\nr\u0012\u0019(ajs;\u0012) =\u0019(ajs;\u0012)r\u0012\u0019(ajs;\u0012)\n\u0019(ajs;\u0012)=\u0019(ajs;\u0012)r\u0012log\u0019(ajs;\u0012) (24)\nWe callr\u0012log\u0019(ajs;\u0012)score function, or likelihood ratio. Policy gradient theorem (Sutton et al.,\n2000) states that for a differentiable policy \u0019(ajs;\u0012), the policy gradient is,\nE\u0019\u0012[r\u0012log\u0019(ajs;\u0012)Q\u0019\u0012(s;a)]: (25)\nWe omit\u0019\u0012in value functions below for simplicity.\nREINFORCE (Williams, 1992) updates \u0012in the direction of\nr\u0012log\u0019(atjst;\u0012)Rt; (26)\nby using return Rtas an unbiased sample of Q(st;at). Usually a baseline bt(st)is subtracted from\nthe return to reduce the variance of gradient estimate, yet keeping its unbiasedness, to yield the\ngradient direction,\nr\u0012log\u0019(atjst;\u0012)(Q(at;st)\u0000bt(st)): (27)\nUsingV(st)as the baseline bt(st), we have the advantage function,\nA(at;st) =Q(at;st)\u0000V(st): (28)\nAlgorithm 6 presents the pseudo code for REINFORCE in the episodic case.\nIn actor-critic algorithms, the critic updates action-value function parameters, and the actor updates\npolicy parameters, in the direction suggested by the critic. Algorithm 7 presents the pseudo code for\none-step actor-critic algorithm in the episodic case.\nAs summarized in Silver (2015), policy gradient may take various forms: r\u0012log\u0019(ajs;\u0012)Rtfor\nREINFORCE,r\u0012log\u0019(ajs;\u0012)Q(s;a;w)for Q actor-critic,r\u0012log\u0019(ajs;\u0012)A(s;a;w)for advan-\ntage actor-critic,r\u0012log\u0019(ajs;\u0012)\u000efor TD actor-critic, r\u0012log\u0019(ajs;\u0012)\u000eefor TD(\u0015) actor-critic,\nandG\u0012wfor natural gradient decent, where G\u0012=E\u0019\u0012[r\u0012log\u0019(ajs;\u0012)r\u0012log\u0019(ajs;\u0012)T]is the\nFisher information matrix; and the critic may use Monte Carlo or TD learning for policy evaluation\nto estimate the value functions Q,AorV.\n20\n\nInput: policy\u0019(ajs;\u0012),^v(s;w)\nParameters: step sizes,\u000b>0,\f >0\nOutput: policy\u0019(ajs;\u0012)\ninitialize policy parameter \u0012and state-value weights w\nfortruedo\ngenerate an episode s0,a0,r1,\u0001\u0001\u0001,sT\u00001,aT\u00001,rT, following\u0019(\u0001j\u0001;\u0012)\nforeach steptof episode 0,\u0001\u0001\u0001,T\u00001do\nGt return from step t\n\u000e Gt\u0000^v(st;w)\nw w+\f\u000erw^v(st;w)\n\u0012 \u0012+\u000b\rt\u000er\u0012log\u0019(atjst;\u0012)\nend\nend\nAlgorithm 6: REINFORCE with baseline (episodic), adapted from Sutton and Barto (2018)\nInput: policy\u0019(ajs;\u0012),^v(s;w)\nParameters: step sizes,\u000b>0,\f >0\nOutput: policy\u0019(ajs;\u0012)\ninitialize policy parameter \u0012and state-value weights w\nfortruedo\ninitializes, the ﬁrst state of the episode\nI 1\nforsis not terminal do\na\u0018\u0019(\u0001js;\u0012)\ntake actiona, observes0,r\n\u000e r+\r^v(s0;w)\u0000^v(s;w)(ifs0is terminal, ^v(s0;w):= 0)\nw w+\f\u000erw^v(st;w)\n\u0012 \u0012+\u000bI\u000er\u0012log\u0019(atjst;\u0012)\nI \rI\ns s0\nend\nend\nAlgorithm 7: Actor-Critic (episodic), adapted from Sutton and Barto (2018)\n2.4.11 D EEPRL\nWe obtain deep reinforcement learning (deep RL) methods when we use deep neural networks to\nrepresent the state or observation, and/or to approximate any of the following components of rein-\nforcement learning: value function, ^v(s;\u0012)or^q(s;a;\u0012), policy\u0019(ajs;\u0012), and model (state transi-\ntion function and reward function). Here, the parameters \u0012are the weights in deep neural networks.\nWhen we use ”shallow” models, like linear function, decision trees, tile coding and so on as the\nfunction approximator, we obtain ”shallow” RL, and the parameters \u0012are the weight parameters in\nthese models. Note, a shallow model, e.g., decision trees, may be non-linear. The distinct difference\nbetween deep RL and ”shallow” RL is what function approximator is used. This is similar to the\ndifference between deep learning and ”shallow” machine learning. We usually utilize stochastic\ngradient descent to update weight parameters in deep RL. When off-policy, function approximation,\nin particular, non-linear function approximation, and bootstrapping are combined together, insta-\nbility and divergence may occur (Tsitsiklis and Van Roy, 1997). However, recent work like deep\nQ-network (Mnih et al., 2015) and AlphaGo (Silver et al., 2016a) stabilize the learning and achieve\noutstanding results. There are efforts for convergence proof of control with non-linear function\napproximation, e.g., Dai et al. (2018b); Nachum et al. (2018).\n21\n\n2.4.12 B RIEF SUMMARY\nAn RL problem is formulated as an MDP when the observation about the environment satisﬁes\nthe Markov property. An MDP is deﬁned by the 5-tuple (S;A;P;R;\r). A central concept in\nRL is value function. Bellman equations are cornerstone for developing RL algorithms. Temporal\ndifference learning algorithms are fundamental for evaluating/predicting value functions. Control\nalgorithms ﬁnd optimal policies. Policy-based methods become popular recently. Reinforcement\nlearning algorithms may be based on value function and/or policy, model-free or model-based, on-\npolicy or off-policy, with function approximation or not, with sample backups (TD and Monte Carlo)\nor full backups (dynamic programming and exhaustive search), and about the depth of backups,\neither one-step return (TD(0) and dynamic programming) or multi-step return (TD( \u0015), Monte Carlo,\nand exhaustive search). When combining off-policy, function approximation, and bootstrapping, we\nface instability and divergence (Tsitsiklis and Van Roy, 1997), the deadly triad issue (Sutton and\nBarto, 2018). Theoretical guarantee has been established for linear function approximation, e.g.,\nGradient-TD (Sutton et al., 2009a;b; Mahmood et al., 2014), Emphatic-TD (Sutton et al., 2016)\nand Du et al. (2017). When RL integrates with deep neural networks, either for representation or\nfunction approximation, we have deep RL. Deep RL algorithms like Deep Q-Network (Mnih et al.,\n2015) and AlphaGo (Silver et al., 2016a; 2017) stabilize the learning and achieve stunning results.\n2.5 R ESOURCES\nWe present some resources for deep RL in the following. We maintain a blog titled Resources for\nDeep Reinforcement Learning at https://medium.com/@yuxili/ .\nSutton and Barto’s RL book (Sutton and Barto, 2018) covers fundamentals and reﬂects new progress,\ne.g., in deep Q-network, AlphaGo, policy gradient methods, as well as in psychology and neuro-\nscience. David Silver’s RL course (Silver, 2015) and Sergey Levine’s Deep RL course (Levine,\n2018) are highly recommended.\nGoodfellow et al. (2016) is a recent deep learning book. Bishop (2011), Hastie et al. (2009), and\nMurphy (2012) are popular machine learning textbooks. James et al. (2013) is an introduction\nbook for machine learning. Domingos (2012), Zinkevich (2017), and Ng (2018) are about practical\nmachine learning advices. See Ng (2016b) and Schulman (2017) for practical advices for deep\nlearning and deep RL respectively.\nThere are excellent summer schools and bootcamps, e.g., Deep Learning and Reinforcement\nLearning Summer School: 2018 at https://dlrlsummerschool.ca , 2017 at https:\n//mila.umontreal.ca/en/cours/deep-learning-summer-school-2017/ ;\nDeep Learning Summer School: 2016 at https://sites.google.com/site/\ndeeplearningsummerschool2016/ , and, 2015 at https://sites.google.\ncom/site/deeplearningsummerschool/ ; and, Deep RL Bootcamp: at https:\n//sites.google.com/view/deep-rl-bootcamp/ .\nCommon benchmarks for general RL algorithms are Atari games in the Arcade Learning Environ-\nment (ALE) for discrete control, and simulated robots using the MuJoCo physics engine in OpenAI\nGym for continuous control.\nThe Arcade Learning Environment (ALE) (Bellemare et al., 2013; Machado et al., 2017) is a\nframework composed of Atari 2600 games to develop and evaluate AI agents. OpenAI Gym, at\nhttps://gym.openai.com , is a toolkit for the development of RL algorithms, consisting of\nenvironments, e.g., Atari games and simulated robots, and a site for the comparison and reproduc-\ntion of results. MuJoCo, Multi-Joint dynamics with Contact, at http://www.mujoco.org , is a\nphysics engine. DeepMind Lab (Beattie et al., 2016) is a ﬁrst-person 3D game platform, at https:\n//github.com/deepmind/lab . DeepMind Control Suite (Tassa et al., 2018) provides RL\nenvironments with the MuJoCo physics engine, at https://github.com/deepmind/dm_\ncontrol . Dopamine (Bellemare et al., 2018) is a Tensorﬂow-based RL framework from Google\nAI. ELF, at https://github.com/pytorch/ELF , is a platform for RL research (Tian et al.,\n2017). ELF OpenGo is a reimplementation of AlphaGo Zero/Alpha Zero using the ELF framework.\n22\n\nPART I: C ORE ELEMENTS\nA reinforcement learning (RL) agent observes states, executes actions, and receives rewards, with\nmajor components of value function, policy and model. A RL problem may be formulated as a\nprediction, control, or planning problem, and solution methods may be model-free or model-based,\nand value-based or policy-based. Exploration vs. exploitation is a fundamental tradeoff in RL.\nRepresentation is relevant to all elements in RL problems.\nFigure 3 illustrates value- and policy-based methods. TD methods, e.g., TD-learning, Q-learning,\nSARSA, and, Deep Q-network (DQN) (Mnih et al., 2015), are purely value-based. Direct policy\nsearch methods include policy gradient, e.g., REINFORCE (Williams, 1992), trust region methods,\ne.g., Trust Region Policy Optimization (TRPO) (Schulman et al., 2015), and, Proximal Policy Op-\ntimization (PPO) (Schulman et al., 2017b), and, evolution methods, e.g., Covariance Matrix Adap-\ntation Evolution Strategy (CMA-ES) (Hansen, 2016). Actor-critic methods combine value function\nand policy. Maximum entropy methods further bridge the gap between value- and policy-based\nmethods, e.g., soft Q-learning (Haarnoja et al., 2017), Path Consistency Learning (PCL) (Nachum\net al., 2017), and, trust-PCL (Nachum et al., 2018). Policy iteration, value iteration, and, generalized\npolicy iteration, e.g., AlphaGo (Silver et al., 2016a; 2017) and DeepStack (Morav ˇc´ık et al., 2017),\nare based on (approximate) dynamic programming.\nvalue TD methods: \nTD learning \nQ-learning \nSARSA \npolicy actor-critic \ndirect policy search: \npolicy gradient \ntrust region \nevolution \nmaximum \nentropy \nmethods \nmaximum \nentropy \nmethods \nmaximum \nentropy \nmethods \npolicy iteration \nvalue iteration \ngeneralized policy iteration \nFigure 3: Value- and Policy-based RL Methods\nIn this part, we discuss RL core elements: value function in Chapter 3, policy in Chapter 4, reward in\nChapter 5, model-based RL in Chapter 6, exploration vs exploitation in Chapter 7, and representation\nin Chapter 8.\n23\n\n3 V ALUE FUNCTION\nValue function is a fundamental concept in reinforcement learning. A value function is a prediction\nof the expected, accumulative, discounted, future reward, measuring the goodness of each state,\nor each state-action pair. Temporal difference (TD) learning (Sutton, 1988) and its extension, Q-\nlearning (Watkins and Dayan, 1992), are classical algorithms for learning state and action value\nfunctions respectively. Once we have an optimal value function, we may derive an optimal policy.\nIn the following, we ﬁrst introduce Deep Q-Network (DQN) (Mnih et al., 2015), a recent break-\nthrough, and its extensions. DQN ignited this wave of deep reinforcement learning, combating the\nstability and convergence issues with experience replay and target networks, which make Q-learning\ncloser to supervised learning. Next we introduce value distribution, rather than value expectation as\nin classical TD and Q learning. Then we discuss general value function, usually with the goal as a\nparameter of a value function, besides the state or the state action pair. General value functions hold\ngreat promise for further development of RL and AI.\nRecently, there are more and more work using policy-based methods to obtain an optimal policy\ndirectly. There are also work combing policy gradient with off-policy Q-learning, e.g., Gu et al.\n(2017b); O’Donoghue et al. (2017); Gu et al. (2017); Haarnoja et al. (2017; 2018); Nachum et al.\n(2017; 2018); Dai et al. (2018b). Dai et al. (2018b) propose to solve the Bellman equation using\nprimal-dual optimization; instead TD learning and Q-learning are based on ﬁxed point iteration. We\ndiscuss policy optimization in next Chapter.\n3.1 D EEPQ-L EARNING\nMnih et al. (2015) introduce Deep Q-Network (DQN) and ignite the ﬁeld of deep RL. There are\nearly work to integrate neural networks with RL, e.g. Tesauro (1994) and Riedmiller (2005). Be-\nfore DQN, it is well known that RL is unstable or even divergent when action value function is\napproximated with a nonlinear function like neural networks. That is, the deadly triad issue, when\ncombining off-policy, function approximation, and, bootstrapping. DQN makes several contribu-\ntions: 1) stabilizing the training of action value function approximation with deep neural networks,\nin particular, CNNs, using experience replay (Lin, 1992) and target network; 2) designing an end-\nto-end RL approach, with only the pixels and the game score as inputs, so that only minimal domain\nknowledge is required; 3) training a ﬂexible network with the same algorithm, network architecture\nand hyperparameters to perform well on many different tasks, i.e., 49 Atari games (Bellemare et al.,\n2013), outperforming previous algorithms, and performing comparably to a human professional\ntester. Note, different games are trained separately, so the network weights are different.\nDQN uses a CNN to approximate the optimal action value function,\nQ\u0003(s;a) = max\n\u0019E[1X\n0\rirt+1jst=s;at=a;\u0019]: (29)\nIn Section 2.4.9, we discuss deadly triad, i.e., instability and divergence may occur, when combin-\ning off-policy, function approximation, and bootstrapping. Several factors cause the instability: 1)\ncorrelations in sequential observations; 2) small updates to action value function Qmay change the\npolicy dramatically, and consequently change the data distribution; 3) correlations between action\nvaluesQandr+\rmaxa0Q(s0;a0), which is usually used as the target value in batch Q-learning.\nDQN uses experience replay and target networks to address the instability issues. In experience\nreplay, observation sequences (st;at;rt;st+1)are stored in the replay buffer, and sampled randomly,\nto remove correlations in the data, and to smooth data distribution changes. In DQN, experiences\nare sampled uniformly, and as we discuss later, prioritized experience replay (Schaul et al., 2016)\nsamples experiences according to their importance. A target network keeps its separate network\nparameters, and update them only periodically, to reduce the correlations between action values Q\nand the target r+\rmaxa0Q(s0;a0). The following is the loss function Q-learning uses to update\nnetwork parameters at iteration i,\n(r+\rmax\na0Q(s0;a0;\u0012\u0000\ni)\u0000Q(s;a;\u0012i))2(30)\n24\n\nwhere\u0012iare parameters of the Q-network at iteration i,\u0012\u0000\niare parameters of the target network at\niterationi. The target network parameters \u0012\u0000\niare updated periodically, and held ﬁxed in between.\nWe present DQN pseudo code in Algorithm 8.\nInput: the pixels and the game score\nOutput: Q action value function (from which we obtain a policy and select actions)\ninitialize replay memory D\ninitialize action-value function Qwith random weight \u0012\ninitialize target action-value function ^Qwith weights\u0012\u0000=\u0012\nforepisode = 1 to Mdo\ninitialize sequence s1=fx1gand preprocessed sequence \u001e1=\u001e(s1)\nfort = 1 toTdo\nfollowing\u000f-greedy policy, select at=\u001aa random action with probability \u000f\narg maxaQ(\u001e(st);a;\u0012)otherwise\nexecute action aiin emulator and observe reward rtand imagext+1\nsetst+1=st;at;xt+1and preprocess \u001et+1=\u001e(st+1)\nstore transition (\u001et;at;rt;\u001et+1)inD\n// experience replay\nsample random minibatch of transitions (\u001ej;aj;rj;\u001ej+1)fromD\nsetyj=\u001arj if episode terminates at step j+ 1\nrj+\rmaxa0^Q(\u001ej+1;a0;\u0012\u0000)otherwise\nperform a gradient descent step on (yj\u0000Q(\u001ej;aj;\u0012))2w.r.t. the network parameter \u0012\n// periodic update of target network\nin everyCsteps, reset ^Q=Q, i.e., set\u0012\u0000=\u0012\nend\nend\nAlgorithm 8: Deep Q-Nework (DQN), adapted from Mnih et al. (2015)\nDQN has a preprocessing step to reduce the input dimensionality. DQN also uses error clipping,\nclipping the update r+\rmaxa0Q(s0;a0;\u0012\u0000\ni)\u0000Q(s;a;\u0012i)in[\u00001;1], to help improve stability.\n(This is not reﬂected in the pseudocode.) Mnih et al. (2015) also present visualization results using\nt-SNE (van der Maaten and Hinton, 2008).\nDQN makes a breakthrough by showing that Q-learning with non-linear function approximation,\nin particular, deep convolutional neural networks, can achieve outstanding results over many Atari\ngames. Following DQN, many work improve DQN in various aspects, e.g., in the following, we\nwill discuss over-estimation in Q-learning, prioritized experience replay, and a dueling network to\nestimate state value function and associated advantage function, and then combine them to estimate\naction value function. We also discuss an integration method called Rainbow to combine several\ntechniques together.\nIn later chapters, we will discuss more extensions, like asynchronous advantage actor-critic\n(A3C) (Mnih et al., 2016) in Section 4.2, better exploration strategy to improve DQN (Osband et al.,\n2016) in Chapter 7, and hierarchical DQN in Chapter 11, etc. Experience replay uses more memory\nand computation for each interaction, and it requires off-policy RL algorithms. This motivates the\nasynchronous methods as we will discuss in Section 4.2.\nSee more work as the following. Anschel et al. (2017) propose to reduce variability and instabil-\nity by an average of previous Q-values estimates. Farebrother et al. (2018) study generalization\nand regularization in DQN. Guo et al. (2014) combine DQN with ofﬂine Monte-Carlo tree search\n(MCTS) planning. Hausknecht and Stone (2015) propose deep recurrent Q-network (DRQN) to\nadd recurrency to DQN for the partial observability issue in Atari games. He et al. (2017) propose\nto accelerate DQN by optimality tightening, a constrained optimization approach, to propagate re-\nward faster, and to improve accuracy over DQN. Kansky et al. (2017) propose schema networks\nand empirically study variants of Breakout in Atari games. Liang et al. (2016) propose to replicate\n25\n\nDQN with shallow features w.r.t. spatial invariance, non-Markovian features, and object detection,\ntogether with basic tile-coding features. Zahavy et al. (2018) study action elimination.\nSee a blog at https://deepmind.com/research/dqn/ . See Chapter 16 in Sutton and\nBarto (2018) for a detailed and intuitive description of Deep Q-Network. See Chapter 11 in Sutton\nand Barto (2018) for more details about the deadly triad.\nDOUBLE DQN\nvan Hasselt et al. (2016) propose Double DQN (D-DQN) to tackle the over-estimate problem in Q-\nlearning (van Hasselt, 2010). In standard Q-learning, as well as in DQN, the parameters are updated\nas follows:\n\u0012t+1=\u0012t+\u000b(yQ\nt\u0000Q(st;at;\u0012t))r\u0012tQ(st;at;\u0012t); (31)\nwhere\nyQ\nt=rt+1+\rmax\naQ(st+1;a;\u0012t); (32)\nso that the max operator uses the same values to both select and evaluate an action. As a conse-\nquence, it is more likely to select over-estimated values, and results in over-optimistic value es-\ntimates. van Hasselt et al. (2016) propose to evaluate the greedy policy according to the online\nnetwork, but to use the target network to estimate its value. This can be achieved with a minor\nchange to the DQN algorithm, replacing yQ\ntwith\nyD\u0000DQN\nt =rt+1+\rQ(st+1;arg max\naQ(st+1;a;\u0012t);\u0012\u0000\nt); (33)\nwhere\u0012tis the parameter for online network and \u0012\u0000\ntis the parameter for target network. For\nreference,yQ\ntcan be written as\nyQ\nt=rt+1+\rQ(st+1;arg max\naQ(st+1;a;\u0012t);\u0012t): (34)\nPRIORITIZED EXPERIENCE REPLAY\nIn DQN, experience transitions are uniformly sampled from the replay memory, regardless of the\nsigniﬁcance of experience. Schaul et al. (2016) propose to prioritize experience replay, so that\nimportant experience transitions can be replayed more frequently, to learn more efﬁciently. The\nimportance of experience transitions are measured by TD errors. The authors design a stochastic\nprioritization based on the TD errors, using importance sampling to avoid the bias in the update\ndistribution. The authors use prioritized experience replay in DQN and D-DQN, and improve their\nperformance on Atari games. In planning, prioritized sweeping (Moore and Atkeson, 1993) is used\nto set priorities for state action pairs according to TD errors to achieve more efﬁcient updates.\nDUELING ARCHITECTURE\nWang et al. (2016b) propose the dueling network architecture to estimate state value function V(s)\nand the associated advantage function A(s;a), and then combine them to estimate action value\nfunctionQ(s;a), to converge faster than Q-learning. In DQN, a CNN layer is followed by a fully\nconnected (FC) layer. In dueling architecture, a CNN layer is followed by two streams of FC layers,\nto estimate value function and advantage function separately; then the two streams are combined to\nestimate action value function. Usually we use the following to combine V(s)andA(s;a)to obtain\nQ(s;a),\nQ(s;a;\u0012;\u000b;\f ) =V(s;\u0012;\f) +\u0000\nA(s;a;\u0012;\u000b)\u0000max\na0A(s;a0;\u0012;\u000b)\u0001\n(35)\nwhere\u000band\fare parameters of the two streams of FC layers. Wang et al. (2016b) propose to\nreplace max operator with average as below for better stability,\nQ(s;a;\u0012;\u000b;\f ) =V(s;\u0012;\f) +\u0000\nA(s;a;\u0012;\u000b)\u0000a\njAjA(s;a0;\u0012;\u000b)\u0001\n(36)\nDueling architecture implemented with D-DQN and prioritized experience replay improves previous\nwork, DQN and D-DQN with prioritized experience replay, on Atari games.\n26\n\nRAINBOW\nHessel et al. (2018) propose Rainbow to combine DQN, double Q-learning, prioritized replay, duel-\ning networks, multi-step learning, value distribution (discussed in Section 3.2 below), and noisy nets\n(Fortunato et al. (2018), an exploration technique by adding parametric noises to network weights),\nand achieve better data efﬁciency and performance on Atari games. The ablation study show that\nremoving either prioritization or multi-step learning worsens performance for most games; however,\nthe contribution of each component vary signiﬁcantly for different games.\nRETRACE\nMunos et al. (2016) propose Retrace( \u0015) for a safe and efﬁcient return-based off-policy control RL\nalgorithm, for low variance, safe use of samples from any behaviour policy, and efﬁciency with\nusing samples from close behaviour policies. The authors analyze the property of convergence to\nthe optimal action value Q\u0003, without the assumption of Greedy in the Limit with Inﬁnite Exploration\n(GLIE) (Singh et al., 2000), and the convergence of Watkins’ Q(\u0015). The authors experiment with\nAtari games. Gruslys et al. (2017) extend Retrace (Munos et al., 2016) for actor-critic.\n3.2 D ISTRIBUTIONAL VALUE FUNCTION\nUsually, value-based RL methods use expected values, like TD learning and Q-learning. However,\nexpected values usually do not characterize a distribution, and more information about value distri-\nbution may be beneﬁcial. Consider a commuter example. For 4 out of 5 days, she takes 15 minutes\nto commute to work, and for the rest 1 out of 5 days, she takes 30 minutes. On average, she takes 18\nminutes to work. However, the commute takes either 15 or 30 minutes, but never 18 minutes.\nBellemare et al. (2017) propose a value distribution approach to RL problems. A value distribution\nis the distribution of the random return received by a RL agent. In contrast to, and analogous with,\nthe Bellman equation for action value function,\nQ(s;a) =ER(s;a) +\rEQ(S0;A0) (37)\nBellemare et al. (2017) establish the distributional Bellman equation,\nZ(s;a) =R(s;a) +\rZ(S0;A0) (38)\nHere,Zis the random return, and its expectation is the value Q. Three random variables charac-\nterize the distribution of Z: the reward R, the next state action pair (S0;A0), and the random return\nZ(S0;A0).\nBellemare et al. (2017) prove the contraction of the policy evaluation Bellman operator for the value\ndistribution, so that, for a ﬁxed policy, the Bellman operator contracts in a maximal form of the\nWasserstein metric. The Bellman operator for the value distribution, T\u0019:Z!Z , was deﬁned as,\nT\u0019Z(s;a) =R(s;a) +\rP\u0019Z(s;a) (39)\nwhereP\u0019Z(s;a) =Z(S0;A0);S0\u0018P(\u0001js;a);A0\u0018\u0019(\u0001jS0). The authors also show instability\nin the control setting, i.e., in the Bellman optimal equation for the value distribution. The authors\nargue that value distribution approximations have advantages over expectation approximations, for\npreserving multimodality in value distributions, and mitigating the effects of a nonstationary policy\non learning, and demonstrate the importance of value distribution for RL both theoretically and\nempirically with Atari games.\nBellemare et al. (2017) propose a categorical algorithm for approximate distributional reinforcement\nlearning. The value distribution is modelled using a discrete distribution, with a set of atoms as the\nsupport,fzi=VMIN +i4z: 0\u0014i < Ng,4z=VMAX\u0000VMIN\nN\u00001, whereN2N;VMIN2\nR;VMAX2Rare parameters. The atom probabilities are then given by\nZ\u0012(s;a) =zi, with probability, pi(s;a) =e\u0012i(s;a)\nP\nje\u0012j(s;a): (40)\nTo deal with the issue of disjoint supports caused by Bellman update TZ\u0012, and learning with sam-\nple transitions, Bellemare et al. (2017) project the sample update ^TZ\u0012onto the support of Z\u0012, by\n27\n\ncomputing the Bellman update ^Tzj=r+\rzjfor each atom zj, then distributing its probability to\nthe immediate neighbours of the update ^Tzj. Use \b^TZ\u0012(s;a)to denote the operations of a sample\nBellman operator following the projection of distributing probabilities. The sample loss function is\nthe cross-entropy term of the KL divergence, and can be optimized with gradient descent,\nLs;a(\u0012) =DKL(\b^TZ\u0012(s;a)jjZ\u0012(s;a)): (41)\nAlgorithm 9 presents pseudo-code for the categorial algorithm, which computes the sampling Bell-\nman operator followed by a projection of distributing probabilities for one transition sample.\nInput: a transitionst;at;rt;st+1;\rt\nOutput: cross-entropy loss\n//fzigare atom support, pi(s;a)are atom probabilities, i2f0;:::;N\u00001g\nQ(st+1;a) =P\nizipi(st+1;a)\na\u0003 arg maxaQ(st+1;a)\nmi= 0;i2f0;:::;N\u00001g\nforj2f0;:::;N\u00001gdo\n//compute the projection of ^Tzjonto the support of fzig\n//[\u0001]b\nabounds the argument in the range of [a;b]\n^Tzj [r+t+\rtzj]VMAX\nVMIN\n//bj2[0;N\u00001]\nbj (^T\u0000VMIN)=4z\nl bbjc;u dbje\n//distribute probability of ^Tzj\nml ml+pj(xt+1;a\u0003)(u\u0000bj)\nmu mu+pj(xt+1;a\u0003)(bj\u0000l)\nend\noutput\u0000P\nimilogpi(xt;at)\nAlgorithm 9: Categorical Algorithm, adapted from Bellemare et al. (2017)\nMorimura et al. (2010a;b) propose risk sensitive algorithms. Rowland et al. (2018) analyze the cate-\ngorical distributional reinforcement learning proposed in Bellemare et al. (2017). Doan et al. (2018)\npropose GAN Q-learning for distributional RL. Dabney et al. (2018) propose to utilize quantile\nregression for state-action return distribution.\nSee a talk by Marc Bellemare at https://vimeo.com/235922311 . See a blog at https://\ndeepmind.com/blog/going-beyond-average-reinforcement-learning/ , with\na video about Atari games experiments.\n3.3 G ENERAL VALUE FUNCTION\nHORDE\nSutton et al. (2011) discuss that value functions provide semantics for predictive knowledge and\ngoal-oriented (control) knowledge, and propose to represent knowledge with general value function,\nwhere policy, termination function, reward function, and terminal reward function are parameters.\nThe authors then propose Horde, a scalable real-time architecture for learning in parallel general\nvalue functions for independent sub-agents from unsupervised sensorimotor interaction, i.e., nonre-\nward signals and observations. Horde can learn to predict the values of many sensors, and policies to\nmaximize those sensor values, with general value functions, and answer predictive or goal-oriented\nquestions. Horde is off-policy, i.e., it learns in real-time while following some other behaviour pol-\nicy, and learns with gradient-based temporal difference learning methods, with constant time and\nmemory complexity per time step.\n28\n\nUNIVERSAL VALUE FUNCTION APPROXIMATORS\nSchaul et al. (2015) propose Universal Value Function Approximators (UVFAs) V(s;g;\u0012)to gen-\neralize over both states sand goalsg, to extend normal state value function approximators. The\naim of UVFAs is to exploit structure across both states and goals, by taking advantages of similari-\nties encoded in the goal representation, and the structure in the induced value function. Schaul et al.\n(2015) show that such UVFAs can be trained using direct bootstrapping with a variant of Q-learning,\nand the derived greedy policy can generalize to previously unseen action-goal pairs. UVFAs can be\nregarded as an inﬁnite Horde of demons, without scalability issue as in Horde.\nHINDSIGHT EXPERIENCE REPLAY\nAndrychowicz et al. (2017) propose Hindsight Experience Replay (HER) to combat with the sparse\nreward issue, inspired by UVFAs (Schaul et al., 2015). The idea is, after experiencing some episodes,\nstoring every transition in the replay buffer, with not only the original goal for this episode, but also\nsome other goals. HER can replay each trajectory with any goal with an off-policy RL algorithm,\nsince the original goal pursued does not inﬂuence the environment dynamics, albeit it inﬂuences the\nactions. Andrychowicz et al. (2017) combine HER with DQN and DDPG (as will be discussed in\nSection 4.1) on several robotics arm tasks, push, slide and pick-and-place, and perform well.\nAn OpenAI blog describes HER, with videos about HER experiments, introduces a baseline HER\nimplementation and simulated robot environment, and proposes potential research topics with HER,\nhttps://blog.openai.com/ingredients-for-robotics-research/ .\n29\n\n4 P OLICY\nA policy maps a state to an action, or, a distribution over actions, and policy optimization is to ﬁnd an\noptimal mapping. Value-based methods optimize value functions ﬁrst, then derive optimal policies.\nPolicy-based methods directly optimize an objective function, usually cumulative rewards.\nREINFORCE (Williams, 1992) is a classical policy gradient method, with a Monte Carlo approach\nto estimate policy gradients. Policy gradient methods can stay stable when combined with function\napproximation, under some conditions. However, sample inefﬁciency is a major issue; and Monte\nCarlo sampling with rollouts usually results in high variance in policy gradient estimates.\nIncorporating a baseline or critic can help reduce variance. In actor-critic algorithms (Barto et al.,\n1983; Konda and Tsitsiklis, 2003), the critic updates action value function parameters, the actor\nupdates policy parameters, in the direction suggested by the critic, and the value function can help\nreduce the variance of policy parameter estimates, by replacing rollout estimates, with the possi-\nbility of encountering a higher bias. We discuss policy gradient, actor critic, and REINFORCE in\nSection 2.4.10.\nOn-policy methods, like TD learning, are usually sample inefﬁcient by using data only once, with\nestimations based on trajectories from the current policy. Off-policy methods, like Q-learning, can\nlearn from any trajectories from any policies, e.g., expert demonstrations, from the same environ-\nment. Recently, experience replay regains popularity after DQN, as we discuss in Chapter 3. This\nusually makes off-policy methods more sample efﬁcient than on-policy methods. Importance sam-\npling is a variance reduction technique in Monte Carlo methods. Precup et al. (2001) study off-policy\nTD learning with importance sampling. Degris et al. (2012) propose off-policy actor-critic with im-\nportance sampling. Liu et al. (2018b) propose an off-policy estimation method with importance\nsampling to avoid the exploding variance issue.\nKakade (2002) introduce natural policy gradient to improve stability and convergence speed of\npolicy based methods. This leads to trust region methods, like Trust Region Policy Optimization\n(TRPO) (Schulman et al., 2015), and Proximal Policy Optimization (PPO) (Schulman et al., 2017b),\ntwo on-policy methods. Trust-PCL (Nachum et al., 2018) is an off-policy trust region method.\nIt is desirable to improve data efﬁciency of policy gradient, while keeping its stability and unbi-\nasedness. Asynchronous advantage actor-critic (A3C) (Mnih et al., 2016) integrates policy gradient\nwith on-line critic. There are recent work for policy gradient with off-policy critic, like deep de-\nterministic policy gradient (DDPG) (Lillicrap et al., 2016), and policy gradient with Q-learning\n(PGQL) (O’Donoghue et al., 2017). Interpolated policy gradient (Gu et al., 2017), and Q-Prop (Gu\net al., 2017b) are proposed to combine stability of trust region methods with off-policy sample\nefﬁciency. However, the deadly triad issue results from the combination off-policy, function ap-\nproximation, and bootstrapping, so that instability and divergence may occur (Sutton and Barto,\n2018).\nThere are efforts to establish theoretical guarantees, tackling the deadly triad, e.g., Retrace (Munos\net al., 2016), path consistency learning (PCL) (Nachum et al., 2017), Trust-PCL (Nachum et al.,\n2018), and SBEED (Dai et al., 2018b), etc.\nMaximum entropy methods integrate policy gradient with off-policy learning and attempt to bridge\nthe gap between value- and policy-based methods, e.g., Ziebart et al. (2008), Ziebart et al. (2010),\nG-learning (Fox et al., 2016), soft Q-learning (Haarnoja et al., 2017), and several mentioned above,\nincluding PGQL, PCL, and Trust-PCL.\nIn conventional RL, we are content with deterministic policies (Sutton and Barto, 2018). However,\nstochastic policies are desirable sometimes, for reasons like, partial observable environments, han-\ndling uncertainty (Ziebart et al., 2008; 2010), convergence and computation efﬁciency (Gu et al.,\n2017b), exploration and compositionality (Haarnoja et al., 2017), and optimal solutions for some\ngames like rock-paper-scissors, etc. Policy gradient methods usually obtain stochastic policies. So\nare maximum entropy regularized methods.\nHere we focus on model-free policy optimization algorithms. We will discuss model-based ones,\nlike stochastic value gradients (SVG) (Heess et al., 2015), and normalized advantage functions\n(NAF) (Gu et al., 2016), in Chapter 6. we discuss guided policy search (GPS) (Levine et al., 2016)\nin Chapter 16.\n30\n\nEvolution strategies achieve excellent results, e.g., Petroski Such et al. (2017), Salimans et al. (2017),\nLehman et al. (2017). See Hansen (2016) for a tutorial. Khadka and Tumer (2018) propose evolu-\ntionary reinforcement learning.\nPolicy search methods span a wide spectrum from direct policy search to value-based RL, in-\ncludes: evolutionary strategies, covariance matrix adaptation evolution strategy (CMA-ES) (Hansen\nand Ostermeier, 2001; Hansen, 2016), episodic relative entropy policy search (REPS) (Jan Peters,\n2010), policy gradients, probabilistic inference for learning control (PILCO) (Deisenroth and Ras-\nmussen, 2011), model-based REPS (Abdolmaleki et al., 2015), policy search by trajectory optimiza-\ntion (Levine and Koltun, 2014), actor critic, natural actor critic (Kakade, 2002), episodic natural\nactor critic (eNAC), advantage weighted regression (Peters and Schaal, 2007), conservative policy\niteration (Kakade and Langford, 2002), least square policy iteration (LSPI) (Lagoudakis and Parr,\n2003), Q-learning, and ﬁtted Q-learning (Riedmiller, 2005). See Peters and Neumann (2015) for\nmore details. AlphaGo (Silver et al., 2016a; 2017), as well as Sun et al. (2018), follows the scheme\nof generalized policy iteration. We will discuss AlphaGo in Section 15.1.\nSee Abbeel (2017b) for a tutorial on policy optimization. Levine (2018) discusses connections\nbetween RL and control, in particular, maximum entropy RL, and probabilistic inference. See NIPS\n2018 Workshop on Infer to Control: Probabilistic Reinforcement Learning and Structured Control,\nathttps://sites.google.com/view/infer2control-nips2018 .\nIn the following, we discuss policy gradient in Section 4.1, actor-critic in Section 4.2, trust region\nmethods in Section 4.3, policy gradient with off-policy learning in Section 4.4, and, benchmark\nresults in Section 4.5.\n4.1 P OLICY GRADIENT\nPolicy gradients are popular methods in RL, optimizing policies directly. Policies may be determin-\nistic or stochastic. Silver et al. (2014) propose Deterministic Policy Gradient (DPG) and Lillicrap\net al. (2016) extend it to deep DPG (DDPG) for efﬁcient estimation of policy gradients. Houthooft\net al. (2018) propose evolved policy gradients with meta-learning.\nAs discussed in Heess et al. (2015), most policy gradient methods, like REINFORCE, use likelihood\nratio method as discussed in Section 2.4.10, by sampling returns from interactions with the environ-\nment in a model-free manner; another approach, value gradient method, is to estimate the gradient\nvia backpropagation, and DPG and DDPG follow this approach.\nSilver et al. (2014) introduce the Deterministic Policy Gradient (DPG) algorithm for RL problems\nwith continuous action spaces. The deterministic policy gradient is the expected gradient of the\naction-value function, which integrates over the state space; whereas in the stochastic case, the pol-\nicy gradient integrates over both state and action spaces. Consequently, the deterministic policy\ngradient can be estimated more efﬁciently than the stochastic policy gradient. The authors introduce\nan off-policy actor-critic algorithm to learn a deterministic target policy from an exploratory be-\nhaviour policy, and to ensure unbiased policy gradient with the compatible function approximation\nfor deterministic policy gradients. Empirical results show its superior to stochastic policy gradients,\nin particular in high dimensional tasks, on several problems: a high-dimensional bandit; standard\nbenchmark RL tasks of mountain car, pendulum, and 2D puddle world with low dimensional action\nspaces; and controlling an octopus arm with a high-dimensional action space. The experiments are\nconducted with tile-coding and linear function approximators.\nLillicrap et al. (2016) propose an actor-critic, model-free, Deep Deterministic Policy Gradient\n(DDPG) algorithm in continuous action spaces, by extending DQN (Mnih et al., 2015) and DPG (Sil-\nver et al., 2014). With actor-critic as in DPG, DDPG avoids the optimization of action value func-\ntion at every time step to obtain a greedy policy as in Q-learning, which will make it infeasible in\ncomplex action spaces with large, unconstrained function approximators like deep neural networks.\nTo make the learning stable and robust, similar to DQN, DDPQ deploys experience replay and an\nidea similar to target network, a ”soft” target, which, rather than copying the weights directly as in\nDQN, updates the soft target network weights \u00120slowly to track the learned networks weights \u0012:\n\u00120 \u001c\u0012+(1\u0000\u001c)\u00120, with\u001c\u001c1. The authors adapt batch normalization to handle the issue that the\ndifferent components of the observation with different physical units. As an off-policy algorithm,\nDDPG learns an actor policy with experiences from an exploration policy by adding noises sampled\n31\n\nfrom a noise process to the actor policy. More than 20 simulated physics tasks of varying difﬁculty\nin the MuJoCo environment are solved with the same learning algorithm, network architecture and\nhyper-parameters, and obtain policies with performance competitive with those found by a planning\nalgorithm with full access to the underlying physical model and its derivatives. DDPG can solve\nproblems with 20 times fewer steps of experience than DQN, although it still needs a large number\nof training episodes to ﬁnd solutions, as in most model-free RL methods. It is end-to-end, with raw\npixels as input.\nHausknecht and Stone (2016) extend DDPG by considering parameterization of action spaces, and\nexperiment with the domain of simulated RoboCup soccer.\n4.2 A CTOR -CRITIC\nAn actor critic algorithm learns both a policy and a state value function, and the value function\nis used for bootstrapping, i.e., updating a state from subsequent estimates, to reduce variance and\naccelerate learning (Sutton and Barto, 2018).\nIn the following, we focus on asynchronous advantage actor-critic (A3C) (Mnih et al., 2016). Mnih\net al. (2016) also discuss asynchronous one-step SARSA, one-step Q-learning and n-step Q-learning.\nA3C achieves the best performance among these asynchronous methods, and it can work for both\ndiscrete and continuous cases.\nglobal shared parameter vectors \u0012and\u0012v, thread-speciﬁc parameter vectors \u00120and\u00120\nv\nglobal shared counter T= 0,Tmax\ninitialize step counter t 1\nforT\u0014Tmaxdo\nreset gradients, d\u0012 0andd\u0012v 0\nsynchronize thread-speciﬁc parameters \u00120=\u0012and\u00120\nv=\u0012v\nsettstart =t, get statest\nforstnot terminal and t\u0000tstart\u0014tmaxdo\ntakeataccording to policy \u0019(atjst;\u00120)\nreceive reward rtand new state st+1\nt t+ 1,T T+ 1\nend\nR=\u001a0 for terminal st\nV(st;\u00120\nv)otherwise\nfori2ft\u00001;:::;tstartgdo\nR ri+\rR\naccumulate gradients wrt \u00120:d\u0012 d\u0012+r\u00120log\u0019(aijsi;\u00120)(R\u0000V(si;\u00120\nv))\naccumulate gradients wrt \u00120\nv:d\u0012v d\u0012v+r\u00120v(R\u0000V(si;\u00120\nv))2\nend\nupdate asynchronously \u0012usingd\u0012, and\u0012vusingd\u0012v\nend\nAlgorithm 10: A3C, each actor-learner thread, based on Mnih et al. (2016)\nWe present pseudo code for A3C for each actor-learner thread in Algorithm 10. A3C maintains\na policy\u0019(atjst;\u0012)and an estimate of the value function V(st;\u0012v), being updated with n-step\nreturns in the forward view, after every tmax actions or reaching a terminal state, similar to using\nminibatches. In n-step update, V(s)is updated toward the n-step return, deﬁned as,\nrt+\rrt+1+\u0001\u0001\u0001+\rn\u00001rt+n\u00001+\rnV(st+n): (42)\nLines 15-19 show, each n-step update results in a one-step update for the last state, a two-step\nupdate for the second last state, and so on for a total of up to tmaxupdates, for both policy and value\nfunction parameters. The gradient update can be seen as\nr\u00120log\u0019(atjst;\u00120)A(st;at;\u0012;\u0012v); (43)\nwhere\nA(st;at;\u0012;\u0012v) =k\u00001X\ni=0\rirt+i+\rkV(st+k;\u0012v)\u0000V(st;\u0012v) (44)\n32\n\nis an estimate of the advantage function, with kupbound by tmax.\nIn A3C, parallel actors employ different exploration policies to stabilize training, so that experience\nreplay is not utilized, although experience replay could improve data efﬁciency. Experience replay\nuses more memory and computation for each interaction, and it requires off-policy RL algorithms.\nAsynchronous methods can use on-policy RL methods. Moreover, different from most deep learning\nalgorithms, asynchronous methods can run on a single multi-core CPU.\nFor Atari games, A3C runs much faster yet performs better than or comparably with DQN, Go-\nrila (Nair et al., 2015), D-DQN, Dueling D-DQN, and Prioritized D-DQN. A3C also succeeds on\ncontinuous motor control problems: TORCS car racing games and MujoCo physics manipulation\nand locomotion, and Labyrinth, a navigating task in random 3D mazes using visual inputs.\nWang et al. (2017c) propose ACER, a stable and sample efﬁcient actor-critic deep RL model us-\ning experience replay, with truncated importance sampling, stochastic dueling network (Wang et al.\n(2016b) as discussed in Section 3.1), and trust region policy optimization (Schulman et al. (2015)\nas will be discussed in Section 4.3). Babaeizadeh et al. (2017) propose a hybrid CPU/GPU imple-\nmentation of A3C. Gruslys et al. (2017) propose Reactor to extend Retrace (Munos et al., 2016)\nfor the actor-critic scheme. Horgan et al. (2018) propose Apex, a distributed version of actor-critic,\nwith prioritized experience replay, and improve the performance on Atari games substantially. One\nimportant factor is that Apex can learn on a large amount of data. Espeholt et al. (2018) propose\nIMPALA, a distributed actor-critic agent, and show good performance in multi-task settings. Dai\net al. (2018a) propose dual actor-critic, in which the critic is not learned by standard algorithms like\nTD but is optimized to help compute gradient of the actor.\n4.3 T RUST REGION METHODS\nTrust region methods are an approach to stabilize policy optimization by constraining gradi-\nent updates. In the following, we discuss Trust Region Policy Optimization (TRPO) (Schul-\nman et al., 2015), and Proximal Policy Optimization (PPO) (Schulman et al., 2017b). Nachum\net al. (2018) propose Trust-PCL, and extension of TRPO for off-policy learning, which we will\ndiscuss in Section 4.4. Heess et al. (2017) propose distributed proximal policy optimization.\nWu et al. (2017) propose scalable TRPO with Kronecker-factored approximation to the curva-\nture. Liu et al. (2018a) study proximal gradient TD learning. See a video about TRPO at,\nhttps://sites.google.com/site/trpopaper/ . See a blog about PPO with videos at,\nhttps://blog.openai.com/openai-baselines-ppo/ .\nTRUST REGION POLICY OPTIMIZATION (TRPO)\nSchulman et al. (2015) introduce an iterative procedure to monotonically improve policies theoret-\nically, guaranteed by optimizing a surrogate objective function, and then make several approxima-\ntions to develop a practical algorithm, Trust Region Policy Optimization (TRPO). In brief summary,\nTRPO iterates the following steps:\n1. collect state action pairs and Monte Carlo estimates of Q values\n2. average samples, construct the estimated objective and constraint in the previous optimiza-\ntion problem, where, \u0012olddenotes previous policy parameters, qdenotes the sampling dis-\ntribution, and \u000eis the trust region parameter\nmax\n\u0012^Eh\u0019\u0012(ajs)\nq(ajs)Q\u0012old(s;a)i\nsubject to ^E[KL(\u0019\u0012old(\u0001js)k\u0019\u0012(\u0001js))]\u0014\u000e (45)\n3. solve the above constrained optimization problem approximately, update the policy param-\neter\u0012\nSchulman et al. (2015) unify policy iteration and policy gradient with analysis, and show that policy\niteration, policy gradient, and natural policy gradient (Kakade, 2002) are special cases of TRPO. In\nthe experiments, TRPO methods perform well on simulated robotic tasks of swimming, hopping,\nand walking, as well as playing Atari games in an end-to-end manner directly from raw images.\n33\n\nPROXIMAL POLICY OPTIMIZATION (PPO)\nSchulman et al. (2017b) propose Proximal Policy Optimization (PPO), to alternate between data\nsampling and optimization, and to beneﬁt the stability and reliability from TRPO, with the goal of\nsimpler implementation, better generalization, and better empirical sample complexity. In PPO, pa-\nrameters for policy and value function can be shared in a neural network, and advantage function can\nbe estimated to reduce variance of policy parameters estimation. PPO utilizes a truncated version of\nGeneralized Advantage Estimator (GAE) (Schulman et al., 2016), reducing to multi-step TD update\nwhen\u0015= 1,^At=\u000et+(\r\u0015)\u000et+1+\u0001\u0001\u0001+(\r\u0015)T\u0000t+1\u000eT\u00001;where\u000et=rt+\rV(st+1)\u0000V(st). PPO\nachieves good performance on several continuous tasks in MuJoCo, on continuous 3D humanoid\nrunning and steering, and on discrete Atari games. As mentioned in an OpenAI blog about PPO,\nhttps://blog.openai.com/openai-baselines-ppo/ , ”PPO has become the default\nreinforcement learning algorithm at OpenAI because of its ease of use and good performance”.\n4.4 P OLICY GRADIENT WITH OFF-POLICY LEARNING\nIt is desirable to combine stability and unbiasedness of policy gradient, and sample efﬁciency of off-\npolicy learning. Levine (2018) connects RL and control with probabilistic inference, and discusses\nthat maximum entropy RL is equivalent to exact and variational probabilistic inference in determin-\nistic and stochastic dynamics respectively. We discuss several recent works following the approach\nof maximum entropy RL, including Haarnoja et al. (2017), Nachum et al. (2017), Nachum et al.\n(2018), Haarnoja et al. (2018), Gu et al. (2017b), etc. Maximum entropy RL can help exploration,\ncompositionality, and partial observability (Levine, 2018).\nSOFT Q-L EARNING\nHaarnoja et al. (2017) design a soft Q-learning algorithm, by applying a method for learning energy-\nbased policies to optimize maximum entropy policies. In soft Q-learning, an optimal policy is\nexpressed with a Boltzmann distribution, and a variational method is employed to learn a sampling\nnetwork to approximate samples from this distribution. Soft Q-learning can improve exploration,\nand help stochastic energy-based policies achieve compositionality for better transferability.\nHaarnoja et al. (2018) propose soft actor-critic, based on the maximum energy RL framework\nin (Haarnoja et al., 2017), so that the actor aims to maximize both expected reward and entropy.\nSchulman et al. (2017a) show equivalence between entropy-regularized Q-learning and policy gra-\ndient. Kavosh and Littman (2017) propose a new Q-value operator.\nPATH CONSISTENCY LEARNING (PCL)\nNachum et al. (2017) introduce the notion of softmax temporal consistency, to generalize the hard-\nmax Bellman consistency as in off-policy Q-learning, and in contrast to the average consistency\nas in on-policy SARSA and actor-critic. The authors establish the correspondence and a mutual\ncompatibility property between softmax consistent action values and the optimal policy maximizing\nentropy regularized expected discounted reward. The authors propose Path Consistency Learning\n(PCL), attempting to bridge the gap between value and policy based RL, by exploiting multi-step\npath-wise consistency on traces from both on and off policies. The authors experiment with several\nalgorithmic tasks. Soft Q-learning (Haarnoja et al., 2017) is a one-step special case of PCL.\nNachum et al. (2018) propose Trust-PCL, an off-policy trust region method, to address sample in-\nefﬁciency issue with the on-policy nature of trust region methods like TRPO and PPO. The authors\nobserve that an objective function maximizing rewards, regularized by relative entropy, led to an op-\ntimal policy and state value function which satisfy several multi-step pathwise consistencies along\nany path. Therefore, Trust-PCL achieves stability and off-policy sample efﬁciency by employing\nrelative entropy regularization. The authors design a method to determine the coefﬁcient for the\nrelative entropy regularization term, to simplify the task of hyperparameter tuning. The authors\nexperiment on standard continuous control tasks.\nDai et al. (2018b) reformulate the Bellman equation into a primal-dual optimization problem, and\npropose smoothed Bellman error embedding (SBEED) to solve it. The authors provide ”the ﬁrst\nconvergence guarantee for general non-linear function approximation, and analyze the algorithm’s\n34\n\nsample complexity”, and experiment with several control tasks. SBEED can be viewed as a de-\nbiased version of PCL and generalizes PCL.\nRECENT WORK\nGu et al. (2017b) propose Q-Prop to take advantage of the stability of policy gradient and the sample\nefﬁciency of off-policy learning. Q-Prop utilizes a Taylor expansion of the off-policy critic as a\ncontrol variate, which gives an analytical gradient term through critic, and a Monte Carlo policy\ngradient term.\nO’Donoghue et al. (2017) propose PGQL to combine policy gradient with off-policy Q-learning, to\nbeneﬁt from experience replay. The authors also show that action value ﬁtting techniques and actor-\ncritic methods are equivalent, and interprete regularized policy gradient techniques as advantage\nfunction learning algorithms.\nGu et al. (2017) show that the interpolation of off-policy updates with value function estimation and\non-policy policy gradient updates can satisfy performance guarantee. The authors employ control\nvariate methods for analysis, and design a family of policy gradient algorithms, with several recent\nones as special cases, including Q-Prop, PGQL, and ACER (Wang et al., 2017c). The author study\nthe correspondence between the empirical performance and the degree of mixture of off-policy gra-\ndient estimates with on-policy samples, on several continuous tasks.\n4.5 B ENCHMARK RESULTS\nDuan et al. (2016) present a benchmark study for continuous control tasks, including classic\ntasks like cart-pole, tasks with very large state and action spaces such as 3D humanoid locomo-\ntion and tasks with partial observations, and tasks with hierarchical structure. The authors im-\nplement and compare various algorithms, including batch algorithms: REINFORCE, Truncated\nNatural Policy Gradient (TNPG), Reward-Weighted Regression (RWR), Relative Entropy Policy\nSearch (REPS), Trust Region Policy Optimization (TRPO), Cross Entropy Method (CEM), Co-\nvariance Matrix Adaption Evolution Strategy (CMA-ES); and online algorithms: Deep Determin-\nistic Policy Gradient (DDPG); and recurrent variants of batch algorithms. See the open source at\nhttps://github.com/rllab/rllab .\nHenderson et al. (2018) investigate reproducibility, experimental techniques, and reporting proce-\ndures for deep RL. The authors show that hyperparameters, including network architecture and\nreward scale, random seeds and trials, environments (like Hopper or HalfCheetah etc. in OpenAI\nBaseline), and codebases inﬂuenced experimental results. This causes difﬁculties for reproducing\ndeep RL results.\nTassa et al. (2018) present the DeepMind Control Suite, a set of continuous tasks, implemented in\nPython, based on the MuJoCo physics engine. Tassa et al. (2018) include benchmarks for A3C,\nDDPG, and distributed distributional deterministic policy gradients (D4PG) (Barth-Maron et al.,\n2018). The open source is at, https://github.com/deepmind/dm_control , and a video\nshowing the tasks is at, https://youtu.be/rAai4QzcYbs .\n35\n\n5 R EWARD\nRewards provide evaluative feedbacks for a RL agent to make decisions. Reward function is a\nmathematical formulation for rewards.\nRewards may be sparse so that it is challenging for learning algorithms, e.g., in computer Go, a re-\nward occurs at the end of a game. Hindsight experience replay (Andrychowicz et al., 2017) is a way\nto handle sparse rewards, as we discuss in Chapter 3. Unsupervised auxiliary learning (Jaderberg\net al., 2017) is an unsupervised ways to harness environmental signals, as we discuss in Chapter 10.\nReward shaping is to modify reward function to facilitate learning while maintaining optimal policy.\nNg et al. (2000) show that potential-based reward shaping can maintain optimality of the policy.\nReward shaping is usually a manual endeavour. Jaderberg et al. (2018) employ a learning approach\nin an end-to-end training pipeline.\nReward functions may not be available for some RL problems. In imitation learning, an agent\nlearns to perform a task from expert demonstrations, with samples of trajectories from the expert,\nwithout reinforcement signal. Two main approaches for imitation learning are behavioral cloning\nand inverse reinforcement learning. Behavioral cloning, or learning from demonstration, maps state-\naction pairs from expert trajectories to a policy, maybe as supervised learning, without learning the\nreward function (Ho et al., 2016; Ho and Ermon, 2016).\nLevine (2018) discusses about imitation learning and RL. Pure imitation learning is supervised learn-\ning, stable and well-studied; however, it encounters the issue of distributional shift, and it can not\nperform better than the demonstrations. Pure RL is unbiased, and can improve until optimal, how-\never, with challenging issues of exploration and optimization. Initialization with imitation learning\nthen ﬁne-tuning with RL can take advantage of both approaches; however, it can forget initializa-\ntion from demonstration due to distributional shift. Pure RL with demonstrations as off-policy data\nis still RL, keeping advantages of RL; however, demonstrations may not always help. A hybrid\nobjective including both RL and imitation objectives, can take advantage of both, do not forget\ndemonstrations; however, it is not pure RL, may be biased, and may require considerable tuning.\nInverse reinforcement learning (IRL) is the problem of determining a reward function given observa-\ntions of optimal behaviour (Ng and Russell, 2000). Abbeel and Ng (2004) approach apprenticeship\nlearning via IRL. Finn et al. (2016b) study inverse optimal cost. Probabilistic approaches are devel-\noped for IRL with maximum entropy (Ziebart et al., 2008) and maximum causal entropy (Ziebart\net al., 2010) to deal with uncertainty in noisy and imperfect demonstrations. Ross et al. (2010) re-\nduce imitation learning and structured prediction (Daum ´e et al., 2009) to no-regret online learning,\nand propose DAGGER, which requires interaction with the expert. Syed and Schapire (2007), Syed\net al. (2008), and Syed and Schapire (2010) study apprenticeship learning with linear programming,\ngame theory, and reduction to classiﬁcation.\nA reward function may not represent the intention of the designer. A negative side effect of\na misspeciﬁed reward refers to potential poor behaviours resulting from missing important as-\npects (Amodei et al., 2016). Hadﬁeld-Menell et al. (2017) give an old example about the wish\nof King Midas, that everything he touched, turned into gold. Unfortunately, his intention did not\ninclude food, family members, and many more. Russell and Norvig (2009) give an example that a\nvacuum cleaner collects more dust to receive more rewards by ejecting collected dust.\nSingh et al. (2009) and Singh et al. (2010) discuss fundamental issues like, homeostatic vs. non-\nhomeostatic (heterostatic) theories, primary rewards vs. conditioned or secondary rewards, internal\nvs. external environments, intrinsic vs. extrinsic motivation, and, intrinsic vs. extrinsic reward. The\nauthors then formulate an optimal reward computational framework. Oudeyer and Kaplan (2007)\npresent a typology of computational approaches to these concepts.\nSee Yue and Le (2018) for a tutorial on imitation learning, Rhinehart et al. (2018) for a tutorial on\nIRL for computer vision, and, Argall et al. (2009) for a survey of robot learning from demonstration.\nSee NIPS 2018 Workshop on Imitation Learning and its Challenges in Robotics, at https://\nsites.google.com/view/nips18-ilr .\nIn the following, we ﬁrst discuss RL methods with and without reward learning respectively, when\nthere is no reward function given. We then discuss an approach to handle complex reward functions.\n36\n\nSee also Amin et al. (2017), Huang et al. (2018), Leike et al. (2018), Merel et al. (2017), Stadie et al.\n(2017), Su et al. (2016), Wang et al. (2017), and, Zheng et al. (2018b).\nWe discuss robotics with imitation learning in Section 16.2, including Duan et al. (2017); Finn et al.\n(2017c); Yu et al. (2018); Finn et al. (2016b). Hu et al. (2018d) leverage IRL techniques to learn\nknowledge constraints in deep generative models.\nREWARD LEARNING\nHadﬁeld-Menell et al. (2016) observe ﬂaws with IRL: a robot may take a human’s reward function\nas its own, e.g., a robot may learn to drink coffee, rather than learn to cook coffee; and, by assuming\noptimal demonstrations which achieve a task efﬁciently, a robot may miss chances to learn useful\nbehaviours, e.g., a robot learns to cook coffee by passive observing would miss chances to learn\nmany useful skills during the process of cooking coffee by active teaching and learning.\nThe authors then propose a cooperative inverse reinforcement learning (CIRL) game for the value\nalignment problem. CIRL is a two-player game of partial information, with a human, knowing the\nreward function, a robot, not knowing it, and the robot’s payoff is human’s reward. An optimal\nsolution to CIRL maximizes the human reward, and may involve active teaching by the human\nand active learning by the robot. The authors reduce ﬁnding an optimal policy pair for human and\nrobot to the solution of a single agent POMDP problem, prove that optimality in isolation, like\napprenticeship learning and inverse reinforcement learning, is suboptimal in CIRL, and present an\napproximate algorithm to solve CIRL.\nHadﬁeld-Menell et al. (2017) introduce inverse reward design (IRD), to infer the true reward func-\ntion, based on a designed reward function, an intended decision problem, e.g., an MDP, and a set\nof possible reward functions. The authors propose approximate algorithms to solve the IRD prob-\nlem, and experiment with the risk-averse behaviour derived from planning with the resulting reward\nfunction. Experiments show that IRD reduces chances of undesirable behaviours like misspeciﬁed\nreward functions and reward hacking.\nChristiano et al. (2017) propose to learn a reward function based on human preferences by compar-\ning pairs of trajectory segments. The proposed method maintains a policy and an estimated reward\nfunction, approximated by deep neural networks. The networks are updated asynchronously with\nthree processes iteratively: 1) produce trajectories with the current policy, and the policy is opti-\nmized with a traditional RL method, 2) select pairs of segments from trajectories, obtain human\npreferences, and, 3) optimize the reward function with supervised learning based on human prefer-\nences. Experiments show the proposed method can solve complex RL problems like Atari games\nand simulated robot locomotion.\nLEARNING FROM DEMONSTRATION\nHere we discuss several recent papers without reward learning.\nWe ﬁrst discuss deep Q-learning from demonstrations. Hester et al. (2018) propose deep Q-learning\nfrom demonstrations (DQfD) to attempt to accelerate learning by leveraging demonstration data, us-\ning a combination of temporal difference (TD), supervised, and regularized losses. In DQfQ, reward\nsignal is not available for demonstration data; however, it is available in Q-learning. The supervised\nlarge margin classiﬁcation loss enables the policy derived from the learned value function to imitate\nthe demonstrator; the TD loss enables the validity of value function according to the Bellman equa-\ntion and its further use for learning with RL; the regularization loss function on network weights and\nbiases prevents overﬁtting on small demonstration dataset. In the pre-training phase, DQfD trains\nonly on demonstration data, to obtain a policy imitating the demonstrator and a value function for\ncontinual RL learning. After that, DQfD self-generates samples, and mixes them with demonstra-\ntion data according to certain proportion to obtain training data. Experiments on Atari games show\nDQfD in general has better initial performance, more average rewards, and learns faster than DQN.\nIn AlphaGo (Silver et al., 2016a), as we discuss in Section 15.1, the supervised learning policy\nnetwork is learned from expert moves as learning from demonstration; the results initialize the RL\npolicy network. See also Kim et al. (2014); P ´erez-D’Arpino and Shah (2017); Ve ˇcer´ık et al. (2017).\n37\n\nNow we discuss generative adversarial imitation learning. With IRL, an agent learns a reward func-\ntion ﬁrst, then from which derives an optimal policy. Many IRL algorithms have high time complex-\nity, with a RL problem in the inner loop. Ho and Ermon (2016) propose the generative adversarial\nimitation learning (GAIL) algorithm to learn policies directly from data, bypassing the intermediate\nIRL step. Generative adversarial training is deployed to ﬁt the discriminator, about the distribution\nof states and actions that deﬁnes expert behavior, and the generator, representing the policy. Gener-\native adversarial networks (GANs) are a recent unsupervised learning framework, which we discuss\nin Section 10.\nGAIL ﬁnds a policy \u0019\u0012so that a discriminator DRcan not distinguish states following the expert\npolicy\u0019Eand states following the imitator policy \u0019\u0012, hence forcingDRto take 0.5 in all cases and\n\u0019\u0012not distinguishable from \u0019Ein the equillibrium. Such a game is formulated as:\nmax\n\u0019\u0012min\nDR\u0000E\u0019\u0012[logDR(s)]\u0000E\u0019E[log(1\u0000DR(s))]\nThe authors represent both \u0019\u0012andDRas deep neural networks, and ﬁnd an optimal solution by\nrepeatedly performing gradient updates on each of them. DRcan be trained with supervised learning\nwith a data set formed from traces from a current \u0019\u0012and expert traces. For a ﬁxed DR, an optimal\u0019\u0012\nis sought. Hence it is a policy optimization problem, with \u0000logDR(s)as the reward. The authors\ntrain\u0019\u0012by trust region policy optimization (Schulman et al., 2015), and experiment on various\nphysics-based control tasks with good performance.\nLi et al. (2017) extend GAIL to InfoGAIL for not only imitating, but also learning interpretable\nrepresentations of complex behaviours, by augmenting the objective with a mutual information term\nbetween latent variables and trajectories, i.e., the observed state-action pairs. InfoGAIL learns a\npolicy in which more abstract, high level latent variables would control low level actions. The au-\nthors experiment on autonomous highway driving using TORCS driving simulator (Wymann et al.,\n2014). See the open source at https://github.com/ermongroup/InfoGAIL . Song et al.\n(2018) extend GAIL to the multi-agent setting.\nREWARD MANIPULATING\nvan Seijen et al. (2017) propose hybrid reward architecture (HRA) to tackle the issue that optimal\nvalue function may not be embedded in low dimensional representation, by decomposing reward\nfunction into components, and learning value functions for them separately. Each component may\nbe embedded in a subset of all features, so its value function may be learned and represented in a\nlow dimensional space relatively easily. HRA agents learn with sample trajectories using off-policy\nlearning in parallel, similar to Horde (Sutton et al., 2011). Experiments on Atari game Ms. Pac-Man\nshow above-human performance. See open source at https://github.com/Maluuba/hra .\n38\n\n6 M ODEL\nA model is an agent’s representation of the environment, including the state transition model and the\nreward model. Usually we assume the reward model is known. We discuss how to handle unknown\nreward models in Chapter 5.\nModel-free RL approaches handle unknown dynamical systems, which usually requires large num-\nber of samples. This may work well for problems with good simulators to sample data, e.g., Atari\ngames and the game of Go. However, this may be costly or prohibitive for real physical systems.\nModel-based RL approaches learn value function and/or policy in a data-efﬁcient way, however, they\nmay suffer from issues of model identiﬁcation, so that the estimated models may not be accurate,\nand the performance is limited by the estimated model. Planning constructs a value function or a\npolicy usually with a model.\nCombining model-free RL with on-line planning can improve value function estimation. Sutton\n(1990) proposes Dyna to integrate learning and planning, by learning from both real experiences\nand simulated trajectories from a learned model.\nMonte Carlo tree search (MCTS) (Browne et al., 2012; Gelly and Silver, 2007; Gelly et al., 2012)\nand upper conﬁdence bounds (UCB) (Auer, 2002) applied to trees (UCT) (Kocsis and Szepesv ´ari,\n2006) are important techniques for planning. A typical MCTS builds a partial tree starting from\nthe current state, in the following stages: 1) select a promising node to explore further, 2) expand a\nleaf node and collect statistics, 3) evaluate a leaf node, e.g., by a rollout policy, or some evaluation\nfunction, 4) backup evaluations to update the action values. An action is then selected. A prominent\nexample is AlphaGo (Silver et al., 2016a; 2017) as we will discuss in Section 15.1. Sun et al. (2018)\nstudy dual policy iteration.\nSeveral papers design new deep neural networks architectures for RL problems, e.g., value iteration\nnetworks (VIN), predictron, value prediction network (VPN), TreeQN and ATreeC, imagination-\naugmented agents (IA2), temporal difference models (TDMs), MCTSnets, which we will discuss\nbelow, as well as dueling network as we discuss in Chapter 3. VIN, predictron, and, MCTSnets\nfollow the techniques of learning to learn, which we discuss in Chapter 14.\nR-MAX (Brafman and Tennenholtz, 2002) and E3(Kearns and Singh, 2002) achieve guaranteed ef-\nﬁciency for tabular cases. Li et al. (2011) present the ”know what it knows” framework. Deisenroth\nand Rasmussen (2011) present probabilistic inference for learning control (PILCO), and McAllis-\nter and Rasmussen (2017) extend PILCO to POMDPs. See papers about model predictive con-\ntrol (MPC) (Amos et al., 2018; Finn and Levine, 2015; Lenz et al., 2015). See more papers, e.g.,\nBerkenkamp et al. (2017), Buckman et al. (2018), Chebotar et al. (2017), Chua et al. (2018), de Avila\nBelbute-Peres et al. (2018), Farahmand (2018), Ha and Schmidhuber (2018), Haber et al. (2018),\nHenaff et al. (2017), Watter et al. (2015). We will discuss guided policy search (GPS) (Levine et al.,\n2016), and, Chebotar et al. (2017) in Chapter 16\nSutton (2018) discusses that ”planning with a learned model” means ”Intelligence is just knowing\na lot about the world, being able to use that knowledge ﬂexibly to achieve goals”, and, mentions\nthat ”planning\u0019reasoning\u0019thought”, and ”world model \u0019knowledge\u0019propositions\u0019facts”.\nHe quotes from Yann LeCun, ”obstacles to AI: learning models of the world, learning to reason and\nplan”, and ”predictive learning \u0019unsupervised learning \u0019model-based RL”. He also quotes from\nYoshua Bengio’s most important next step in AI, ”learning how the world ticks”, and ”predictive,\ncausal, explanatory models with latent variables ...”. He lists the following as some answers to\nthe problem of planning with a learned model: function approximation, off-policy learning, Dyna,\nlinear Dyna, non-linear Dyna, GVFs, Horde, options, option models, prioritized sweeping, intrinsic\nmotivation, curiosity, recognizers, predictive state representations, TD networks (Sutton and Tanner,\n2004), TD networks with options (Sutton et al., 2005), and, propagation with valuableness. LeCun\n(2018) also talks about world model, highlighting the role of self-supervised learning.\nGeffner (2018) discusses model-free learners and model-based solvers, and planners as particular\nsolvers. Learners are able to infer behaviour and functions from experience and data, solvers are able\nto address well-deﬁned but intractable models like classical planning and POMDPs, and, planners\nare particular solvers for models with goal-directed behaviours. Geffner (2018) makes connections\nbetween model-free learners vs. model-based solvers, and the two systems in current theories of\n39\n\nhuman mind (Kahneman, 2011): System 1 with a fast, opaque, and inﬂexible intuitive mind, vs.\nSystem 2 with a slow, transparent, and ﬂexible analytical mind.\nSee Finn (2017) for a tutorial, and Silver (2015) and Levine (2018) for lectures, about model-based\n(deep) RL. See NIPS 2018 Workshop on Modeling the Physical World: Perception, Learning, and\nControl at http://phys2018.csail.mit.edu . We discuss Lake et al. (2016) in Section 8.3,\nand, scene understanding and physics model learning in Section 18.3.\nMODEL -BASED RL\nWe discuss several recent papers about model-based RL. We may roughly have methods with RL\nﬂavour, e.g., Dyna, VPN, IA2, using TD methods; methods with optimal control ﬂavour, e.g., GPS,\nNAF, using local models like linear quadratic regulator (LQR) and MPC; and, methods with physics\nﬂavour, e.g., those with physics models as discussed in Lake et al. (2016).\nIn policy gradient methods, the gradient can be estimated either by likelihood ratio method as in RE-\nINFORCE, or by value gradient methods with backpropagation as in DPG/DDPG. Value gradients\nmethods are used for deterministic policies. Heess et al. (2015) propose stochastic value gradients\n(SVG) for stochastic policies, to combine advantages of model-free and model-based methods, and\nto avoid their shortcomings. SVG treats noise variables in Bellman equation as exogenous inputs,\nallowing direct differentiation w.r.t. states. This results in a spectrum of policy gradient algorithms,\nranging from model-free ones with value functions, to model-based ones without value functions.\nSVG learns model, value function, and policy jointly, with neural networks trained using experience\nfrom interactions with the environment. SVG mitigates compounded model errors by computing\nvalue gradients with real trajectories from the environment rather than those from an inaccurate,\nestimated model. SVG uses models for computing policy gradient, but not for prediction. Heess\net al. (2015) experiment with physics-based continuous control tasks in simulation.\nOh et al. (2017) propose value prediction network (VPN) to integrate model-free and model-based\nRL into a neural network. VPN learns a dynamic model to train abstract states to predict future\nvalues, rewards, and discount, rather than future observations as in typical model-based methods.\nThe author propose to train VPN by TD search (Silver et al., 2012) and multi-step Q learning. In\nVPNs, values are predicted with Q-learning, rewards are predicted with supervised learning, and\nlookahead planning are performed for choosing actions and computing target Q-values. VPN is\nevaluate on a 2D navigation collect task and Atari games.\nPong et al. (2018) propose temporal difference models (TDMs) to combine beneﬁts of model-free\nand model-based RL. TDMs are general value functions, as discussed in Section 3.3. TDMs can\nbe trained with model-free off-policy learning, and be used for model-based control. TDM learning\ninterpolates between model-free and model-based learning, seeking to achieve sample efﬁciency in\nmodel-based learning, and at the same time, avoiding model bias. Pong et al. (2018) evaluate TDMs\non both simulated and real-world robot tasks.\nFarquhar et al. (2018) propose TreeQN, using a differentiable, recursive tree structure neural network\narchitecture to map the encoded state to the predicted action value Q function, for discrete actions.\nTreeQN uses such recursive model to reﬁne the estimate of Q function, with the learned transition\nmodel, reward function, and value function, by tree transitioning, and value prediction & backup\nsteps in the recursive tree structure neural network. TreeQN takes advantage of the prior knowledge\nthat Q values are induced by MDPs. In contrast, DQN uses fully connected layers, not implementing\nsuch inductive bias. Farquhar et al. (2018) also propose ATreeC, an actor-critic variant. The authors\nevaluate TreeQN and ATreeC in a box-pushing environment and on Atari games.\nWeber et al. (2017) propose imagination-augmented agents (IA2), a neural network architecture, to\ncombine model-free and model-based RL. IA2 learns to augment model-free decisions by interpret-\ning environment models.\nGu et al. (2016) propose normalized advantage functions (NAF) to enable experience replay with\nQ-learning for continuous task, and propose to reﬁt local linear models iteratively. NAF extends\nDyna to continuous tasks. The authors evaluate NAF on several simulated MuJoCo robotic tasks.\nHester and Stone (2017) propose variance-and-novelty-intrinsic-rewards algorithm (TEXPLORE-\nV ANAIR), a model-based RL algorithm with intrinsic motivations. The authors study two intrinsic\n40\n\nmotivations, one for model uncertainty, and another for acquiring novel experiences new to the\nmodel. The authors conduct empirical study on a simulated domain and a real world robot and show\ngood results.\nLeonetti et al. (2016) propose domain approximation for reinforcement learning (DARLING), taking\nadvantage of both RL and planning, so that the agent can adapt to the environment, and the agent’s\nbehaviour is constrained to reasonable choices. The authors perform evaluation on a service robot\nfor tasks in an ofﬁce building.\nPLANNING\nWe discuss several recent papers about planning.\nGuez et al. (2018) propose to learn to search with MCTSnets, a neural network architecture that\nincorporates simulation-based search, using a vector embedding for expansion, evaluation, and\nbackup. The authors propose to jointly optimize in MCTSnets a simulation policy for where to\ntraverse in the simulation, an evaluation network for what to evaluate in the reached states, and a\nbackup network for how to backup evaluations, end-to-end with a gradient-based approach. The\nauthors experiment MCTSnets with a classical planning problem Sokoban.\nTamar et al. (2016) introduce value iteration networks (VIN), a fully differentiable CNN planning\nmodule to approximate the value iteration algorithm, to learn to plan, e.g, policies in RL. In contrast\nto conventional planning, VIN is model-free, where reward and transition probability are part of the\nneural network to be learned. VIN can be trained end-to-end with backpropagation. VIN can gen-\neralize in a diverse set of tasks: simple gridworlds, Mars Rover Navigation, continuous control and\nWebNav Challenge for Wikipedia links navigation (Nogueira and Cho, 2016). VIN plans via value\niteration over the full state space, and with local transition dynamics for states, e.g., 2D domains,\nwhich limits applicability of VIN. Lee et al. (2018) propose gated path planning networks to extend\nVIN.\nSilver et al. (2016b) propose the predictron to integrate learning and planning into one end-to-end\ntraining procedure with raw input in Markov reward process (MRP), which can be regarded as\nMarkov decision process without actions. Predictron rolls multiple planning steps of an abstract\nmodel represented by an MRP for value prediction; in each step, predictron applies the model to\nan internal state, and generates a next state, reward, discount, and value estimate. The predictron\nfocuses on evaluation tasks in uncontrolled environments; however, it can be used as Q-network,\ne.g., in DQN, for control tasks.\nSilver et al. (2012) propose temporal difference search (TD search) to combine TD learning with\nsimulation based search. TD search updates value function from simulated experience, and general-\nizes among states using value function approximation and bootstrapping. Xiao et al. (2018) propose\nmemory-augmented MCTS. Srinivas et al. (2018) propose universal planning networks.\n41\n\n7 E XPLORATION VS . EXPLOITATION\nA fundamental tradeoff in RL is between exploration of uncertain policies and exploitation of the\ncurrent best policy. Online decision making faces a central issue: either exploiting the information\ncollected so far to make the best decision, or exploring for more information. In sequential decision\nmaking, we may have to sacriﬁce short-term losses to achieve long-term gains. It is essential to\ncollect enough information to make the best overall decisions.\nThere are several principles in trading off between exploration and exploitation, namely, naive\nmethods, optimism in the face of uncertainty, including, optimistic initialization, upper conﬁdence\nbounds, and, probability matching, and, information state search (Silver, 2015). These are developed\nin the settings of multi-armed bandit, but are applicable to RL problems.\nThe multi-armed bandit problem is classical for studying exploration and exploitation. It is deﬁned\nby a tuple<A;R>, whereAis a given set of arms, or actions, and R(rja) =P(rja)is a\nprobability distribution over rewards, unknown to the agent. At each step t, the agent selects an\nactionat2A, receives a reward rt\u0018R(\u0001jat)from the environment, and the goal is to maximize\nthe cumulative rewardPt\n\u001c=1r\u001c.\nThe action-value function is the expected reward for action a,Q(a) =E[rja]. The optimal value is\nV\u0003=Q(a\u0003) = maxa2AQ(a). The regret is one step loss,\nlt=E[V\u0003\u0000Q(at)]: (46)\nThe total regret until time step tis then\nLt=E[tX\n\u001c=1V\u0003\u0000Q(a\u001c)]: (47)\nThe maximum cumulative reward is the minimum total regret.\nDenoteNt(a)as the expected number of selecting action auntil time step t. The greedy algorithm\nselects the action with the highest value, a\u0003\nt= arg maxa2A^Qt(a), where ^Qt(a)is an estimate of\nQ(a), e.g., by Monte Carlo evaluation,\n^Qt(a) =1\nNt(a)tX\n\u001c=1r\u001c1(a\u001c=a): (48)\nThe greedy algorithm may stick to a suboptimal action. However, \u000f-greedy, where \u000f2(0;1), can\nensure a minimum regret with a constant \u000f. In\u000f-greedy, an agent selects a greedy action a=\narg maxa2A^Q(a), with probability 1\u0000\u000f; and selects a random action with probability \u000f.\nA simple and practical idea for optimistic initialization is to initialize action value Q(a)to a high\nvalue, then update it with incremental Monte Carlo evaluation,\n^Qt(a) =^Qt\u00001(a) +1\nNt(a)(rt\u0000^Qt\u00001(a)): (49)\nThis encourages exploration in an early stage, but may stick to a suboptimal action.\nNext we discuss upper conﬁdence bounds (UCB) (Auer, 2002), an important result in bandit prob-\nlems. Its extension, UCT for search trees (Kocsis and Szepesv ´ari, 2006), in particular, in Monte\nCarlo tree search (MCTS) (Browne et al., 2012; Gelly and Silver, 2007; Gelly et al., 2012), plays\nimportant roles in many problems, including AlphaGo (Silver et al., 2016a; 2017).\nWe estimate an upper conﬁdence ^Ut(a)for each action, to have Q(a)\u0014^Qt(a) +^Ut(a)with a high\nprobability. When Nt(a)is small, ^Ut(a)is large, i.e., the estimated value is uncertain; when Nt(a)\nis large, ^Ut(a)is small, i.e., the estimated value is close to true value. We want to select an action to\nmaximize the upper conﬁdence bound,\nat= max\na2A^Qt(a) +^Ut(a): (50)\n42\n\nWe need to establish a theoretical guarantee with Hoeffding’s inequality theorem, which is: let\nX1;:::;Xtbe i.i.d. random variables in [0,1], and let \u0016Xt=1\n\u001cPt\n\u001c=1X\u001cbe the sample mean.\nThen,\nP[E[X]>\u0016Xt]\u0014e\u00002tu2: (51)\nWith Hoeffding’s inequality, conditioned on selecting action a, we have,\nP[Q(a)>^Qt(a) +^Ut(a)]<e\u00002Nt(a)Ut(a)2: (52)\nChoose a probability p=e\u00002Nt(a)Ut(a)2, so thatQ(a)>^Qt(a)+^Ut(a), i.e., the true value exceeds\nUCB, hence, Ut(a) =q\n\u0000logp\n2Nt(a). Choose a schedule for pas observing more rewards, e.g., p=t\u00004,\nhence,Ut(a) =q\n2 logp\nNt(a). This guarantees that, as t!1 , we select optimal actions. Thus, we\nobtain the UCB1 algorithm,\nat= arg max\na2AQ(a) +s\n2 logt\nNt(a): (53)\nThe UCB algorithm can achieve logarithmic asymptotic total regret, better than linear total regret\nachievable by \u000f-greedy and optimistic initialization (Auer, 2002).\nAs shown above, UCB employsp\n2 logt=Nt(a)as exploration bonus to encourage less discovered\nactions. Model-based interval estimation with exploration bonuses (MBIE-EB) (Strehl and Littman,\n2008) employsp\n1=Nt(a); and Bayesian exploration bonus (BEB) (Kolter and Ng, 2009) employs\n1=Nt(a).\nIn probability matching, we select an action aaccording to the probability that ais the optimal\naction with the largest value. It is optimistic under uncertainty, and uncertain actions tend to have\nhigher probabilities of having the largest value. Thompson sampling implements probability match-\ning, dealing with the difﬁculty of analytical computation with posterior distributions. Thompson\nsampling uses Bayes law to compute the posterior distribution, samples a reward distribution from\nthe posterior, evaluates action value function, and, selects the action that maximizes value estimates\non samples.\nGittins indices are a Bayesian model-based RL method for solving information state space bandits.\nIt is known as Bayes-adaptive RL, and ﬁnds Bayes-optimal exploration and exploitation trade off\nw.r.t. a prior distribution. The computation complexity may be prohibitive for large problems.\nThe above discussions are in the setting of multi-armed bandits. They do not pay attention to addi-\ntional information. However, such feature-based exploration vs exploitation problems are challeng-\ning (Auer et al., 2002; Langford and Zhang, 2007). Li et al. (2010) introduce contextual bandit and\ndesign algorithms based on UCB.\nThe techniques for multi-armed bandits are also applicable for full RL problems. A naive explo-\nration technique, \u000f-greedy is widely used. In model-free RL, we can initialize action value function\nQ(s;a) rmax\n1\u0000\r, wherermax is the maximal value of reward. Brafman and Tennenholtz (2002)\npresent R-MAX, a model-based RL, optimistically initializing all actions in all states to the max-\nimal reward. UCB can work with model-free and model-based RL methods. Lipton et al. (2018)\npropose to add variance information to DQN, which is then used to guide exploration following the\nprinciple of Thompson sampling. Guez et al. (2014) propose a simulation-based search approach\nfor Bayes-adaptive MDPs augmented with information states.\nA RL agent usually uses exploration to reduce its uncertainty about the reward function and tran-\nsition probabilities of the environment. In tabular cases, this uncertainty can be quantiﬁed as con-\nﬁdence intervals or posterior of environment parameters, which are related to the state-action visit\ncounts. An example is MBIE-EB (Strehl and Littman, 2008). Brafman and Tennenholtz (2002),\nJaksch et al. (2010), and Strehl and Littman (2008) provide theoretical guarantee for tabular cases.\nIntrinsic motivation (Barto, 2013; Schmidhuber, 2010; Oudeyer and Kaplan, 2007) suggests to ex-\nplore based on the concepts of curiosity and surprise, so that actions will transition to surprising\nstates that may cause large updates to the environment dynamics models. One particular exam-\nple of measuring surprise is by the change in prediction error in learning process (Schmidhuber,\n43\n\n1991), as studied recently in Bellemare et al. (2016). Intrinsic motivation methods do not require\nMarkov property and tabular representation as count-based methods. Watch a video (Barto, 2017).\nSee ICML 2018 workshop on Exploration in RL at, https://sites.google.com/view/\nerl-2018/home , andhttps://goo.gl/yxf16n for videos.\nLevine (2018) discusses three classes of exploration methods in deep RL: 1) optimistic exploration\nmethods, which estimate state visitation frequencies or novelty, typically with exploration bonuses,\ne.g., Bellemare et al. (2016), Fu et al. (2017), Schmidhuber (1991), and Tang et al. (2017); 2)\nThompson sampling methods, which learn distribution over Q-functions or policies, then act ac-\ncording to samples, e.g., Osband et al. (2016); 3) information gain methods, which reason about\ninformation gain from visiting new states, e.g., Houthooft et al. (2016). All these three methods\nfollow the principle of optimism in the face of uncertainty.\nIn the above, we discuss background of exploration and exploitation largely based on Lecture 9:\nExploration and Exploitation in Silver (2015), as well as, the lecture about exploration in Levine\n(2018), Chapter 2 in (Sutton and Barto, 2018) about multi-armed bandits, and, relevant papers. Lat-\ntimore and Szepesv ´ari (2018) is about bandit algorithms. Li (2012) surveys theoretical approaches\nto exploration efﬁciency in RL.\nIn the following we discuss several recent work about exploration in the setting of large scale RL\nproblems, in particular deep RL.\nCOUNT -BASED METHODS\nWith the count-based exploration, a RL agent uses visit counts to guide its behaviour to reduce un-\ncertainty. However, count-based methods are not directly useful in large domains. Bellemare et al.\n(2016) propose pseudo-count, a density model over the state space, for exploration with function\napproximation, to unify count-based exploration and intrinsic motivation, by introducing informa-\ntion gain, to relate to conﬁdence intervals in count-based exploration, and to relate learning progress\nin intrinsic motivation. The authors establish pseudo-count’s theoretical advantage over previous\nintrinsic motivation methods, implement it with a density model, use it as exploration bonuses in\nMBIE-EB (Strehl and Littman, 2008), in experience replay and actor-critic settings, and study its\nempirical performance with Atari games.\nOstrovski et al. (2017) further study the approach of pseudo-count (Bellemare et al., 2016) w.r.t.\nimportance of density model selection, modelling assumptions, and role of mixed Monte Carlo\nupdate, and propose to use a neural density model for images, PixelCNN (van den Oord et al., 2016),\nfor supplying pseudo-count, and combine it with various agent architectures, including DQN (Mnih\net al., 2015) and Reactor (Gruslys et al., 2017). The authors observe that mixed Monte Carlo update\nfacilitate exploration in settings with sparse rewards, like in the game of Montezuma’s Revenge.\nTang et al. (2017) propose to implement the count-based exploration method by mapping states to\nhash codes to count occurrences with a hash table, and the counts are used for reward bonus to guide\nexploration. The authors experiment with simple hash functions and a learned domain-dependent\nhash code on both Atari games and continuous control tasks.\nINTRINSIC MOTIVATION\nHouthooft et al. (2016) propose variational information maximizing exploration (VIME), a\ncuriosity-driven exploration approach to simultaneously optimizing both external reward and intrin-\nsic surprise, for continuous state and action spaces. The authors propose to measure the information\ngain with variance inference, and to approximate the posterior distribution of an agent’s internal\nbelief of environment dynamics, represented with Bayesian neural networks. The authors evaluate\nVIME with various continuous control tasks and algorithms.\nPathak et al. (2017) study curiosity-driven exploration with intrinsic reward signal for predicting the\nresult of its actions with self-supervised learning, and experiment with VizDoom and Super Mario\nBros.\n44\n\nMORE WORK\nOsband et al. (2016) propose bootstrapped DQN to combine deep exploration with deep neural\nnetworks to achieve efﬁcient learning. The authors use randomized value functions to implement\nThompson sampling, to enable exploration with non-linear function approximation, such as deep\nneural networks, and a policy is selected randomly according to its probability being optimal. The\nauthors implement bootstrapped DQN by building multiple estimates of the action value function in\nparallel, and each estimate is trained with its own target network. The authors evaluate the perfor-\nmance of bootstrapped DQN with Atari games.\nNachum et al. (2017) propose under-appreciated reward exploration (UREX) to avoid the ineffective,\nundirected exploration strategies of the reward landscape, as in \u000f-greedy and entropy regularization\npolicy gradient, and to promote directed exploration of the regions, in which the log-probability of an\naction sequence under the current policy under-estimates the resulting reward. UREX results from\nimportance sampling from the optimal policy, and combines a mode seeking and a mean seeking\nterms to tradeoff exploration and exploitation. The authors implement UREX with minor modiﬁca-\ntions to REINFORCE, and validate it, for the ﬁrst time with a RL method, on several algorithmic\ntasks. UREX is an effort for symbolic deep RL.\nAzar et al. (2017) study the problem of provably optimal exploration for ﬁnite horizon MDPs. Fu\net al. (2017) propose novelty detection with discriminative modeling for exploration. Fortunato\net al. (2018) propose NoisyNet for efﬁcient exploration by adding parametric noises to weights\nof deep neural networks. Jiang et al. (2017) study systematic exploration for contextual decision\nprocesses (CDPs). See also Dimakopoulou et al. (2018), Dong and Roy (2018), Gupta et al. (2018),\nKumaraswamy et al. (2018), Madhavan et al. (2018), and, Osband et al. (2018).\nAlso note that maximum entropy RL helps exploration, as discussed in Section 4.4.\n45\n\n8 R EPRESENTATION\nRepresentation is fundamental to reinforcement learning, machine learning, and AI in general. For\nRL, it is relevant not only to function approximation for state/observation, action, value function,\nreward function, transition probability, but also to agent (Albrechta and Stone, 2018; Rabinowitz\net al., 2018), environment, and any element in a RL problem. The ”representation” in ”represen-\ntation learning” basically refers to the ”feature” in ”feature engineering”. Representation learning\nis an approach to automatically ﬁnd good features. Here we discuss ”representation” in a broader\nperspective, i.e., about any element in a RL problem. Besides the ”feature” for function approxima-\ntion, we also refer ”representation” to problem representation, like Markov decision process (MDP),\npartially observable Markov decision process (POMDP), and, predictive state representation (PSR),\nand, moreover, for representing knowledge, reasoning, causality, and human intelligence, either in\nfunction approximation, or in discovering new neural network architectures. We attempt to use such\na notion of ”representation” to unify the roles deep learning has played, is playing, and would play\nin various aspects of deep reinforcement learning.\nWhen the problem is small, both state and action can be accommodated in a table, we can use a\ntabular representation. For large-scale problems, we need function approximation, to avoid curse of\ndimensionality. One approach is linear function approximation, using basis functions like polyno-\nmial bases, tile-coding, radial basis functions, Fourier basis, and proto-value functions (PVFs), etc.\nWe also discuss representations for state distributions, in particular, successor representation, which\nis related to value function.\nRecently, non-linear function approximations, in particular, deep neural networks, show exciting\nachievements. Common neural network structures include multiple layer perceptron (MLP), convo-\nlutional neural networks (CNNs), recurrent neural networks (RNNs), in particular long short time\nmemory (LSTM) and gated recurrent unit (GRU), (variational) autoencoder, and capsules, etc. There\nare new neural network architectures customized for RL problems, e.g., value iteration networks\n(VIN), predictron, and value prediction networks (VPN), etc.\nGeneral value function (GVF) is an approach to learn, represent, and use knowledge of the world.\nHierarchical representations, like options, feudal networks, and max-Q, etc. handle temporal ab-\nstraction. Relational RL integrates statistical relational learning and reasoning with RL to handle\nentities and relations.\nThere are renewed interests in deploying or designing networks for reasoning, including graph neural\nnetworks (GNN), graph networks (GN), relational networks (RNs), and compositional attention net-\nworks, etc. There are discussions about addressing issues of current machine learning with causality,\nand incorporating more human intelligence into artiﬁcial intelligence.\nAlthough there have been enormous efforts for representation, since reinforcement learning is fun-\ndamentally different from supervised learning and unsupervised learning, an optimal representation\nfor RL is probably different from generic CNN and RNN, thus it is desirable to search for an optimal\nrepresentation for RL. Our hypothesis is that this would follow a holistic approach, by considering\nperception and control together, rather than treating them separately, e.g., by deciding a CNN to\nhandle visual inputs, then ﬁxing the network, and designing some procedure to ﬁnd optimal weights\nfor value function and/or policy. Learning to learn techniques as we discuss in Chapter 14 may play\nan important role here.\n8.1 C LASSICS\nIn this section, we discuss classical methods for representation, as well as several papers for re-\ncent progress. We discuss (linear) function approximation, which is usually for value function and\npolicy approximation. We then discuss representations for an RL problem description, i.e., state,\ntransitions and reward, including, partially observable Markov decision process (POMDP), predic-\ntive state representation (PSR), and, contextual decision process (CDP). We also discuss successor\nrepresentation for state visitation, and work for state-action distribution.\n46\n\nFUNCTION APPROXIMATION\nHere we discuss linear function approximation. We discuss neural networks in Section 8.2. In linear\nfunction approximation, a value function is approximated by a linear combination of basis functions.\nBasis functions may take the form of polynomial bases, tile-coding, radial basis functions, Fourier\nbasis, and proto-value functions, etc.\nIn tile coding, tiles partition the input space exhaustively, and each tile is a binary feature. A tiling\nis such a partition. Each tiling represents a feature. There are various ways to tile a space, like grid,\nlog stripes, diagonal strips, and irregular, etc. (Sutton and Barto, 2018)\nWith radial basis functions (RBFs), typically, a feature ihas a Gaussian response \u001es(i) = exp\u0000\n\u0000\njjs\u0000cijj2\n2\u001b2\ni\u0001\n, wheresis the state,ciis the feature’s prototypical or center state, and \u001biis the feature’s\nwidth (Sutton and Barto, 2018). When using RBFs as features for a linear function approximator,\nwe have an RBF network.\nMahadevan and Maggioni (2007) propose proto-value functions (PVFs), using ”the eigenvectors of\nthe graph Laplacian on an undirected graph formed from state transitions induced by the MDP”.\nThe authors then propose to learn PVFs and optimal policies jointly.\nThere are also papers with Gaussian processes (Rasmussen and Williams, 2006) and kernel meth-\nods (Sch ¨olkopf and Smola, 2001), e.g., Ghavamzadeh et al. (2016) and Ormoneit and Sen (2002).\nRL P ROBLEM DESCRIPTION\nPartially observable Markov decision process (POMDP) (Kaelbling et al., 1998) generalizes MDP.\nIn POMDP, an MDP determines system dynamics, with partial observability of underlying states.\nA POMDP maintains a probability distribution over possible states, based on observations, their\nprobabilities, and the MDP. Hausknecht and Stone (2015) propose deep recurrent Q-learning for\nPOMDP.\nPredictive state representation (PSR) (Littman et al., 2001) utilizes vectors of predictions for action-\nobservation sequences to represent states of dynamical systems. The predictions relate directly\nto observable quantities, rather than hidden states. PSRs do not have strong dependence on prior\nmodels as POMDP, and, PSRs are more compact than POMDP, in dynamic systems which linear\nPSRs can represent. In fact, PSRs are more general than nth-order Markov models, hidden Markov\nmodels (HMM), and POMDP (Singh et al., 2004). Recently, Downey et al. (2017) present predictive\nstate RNNs, and Venkatraman et al. (2017) propose predictive state decoders, both of which combine\nPSRs with RNN to take their advantages.\nJiang et al. (2017) propose contextual decision processes (CDPs), RL problems with rich obser-\nvations and function approximation, for systematic exploration. The authors introduce the Bell-\nman rank, a complexity measure, and provide a uniﬁed framework for many problems in RL with\nlow Bellman rank, e.g., tabular MDP, low-rank MDP, a POMDP with reactive value-functions,\nlinear quadratic regulators (LQR), and reactive PSRs, and show that these problems are PAC-\nlearnable (Valiant, 1984; Strehl et al., 2009; Li, 2012).\nSTATE AND STATE -ACTION DISTRIBUTION\nDayan (1993) introduces successor representation (SR),\n =1X\nt=0[\rX\ns0P(s0js;\u0019)]t; (54)\nfor expected discounted future state visitations, w.r.t. a given policy and a discount factor, and being\nindependent of the reward. In the vector form,\n =1X\nt=0(\rP)t= (I\u0000\rP)\u00001; (55)\nwherePis the transition matrix, and Iis the identity matrix. SR captures the dynamics of the MDP,\ndescribing where the agent will traverse in the future, independent of the reward. SR can be learned\n47\n\nwith algorithms like TD learning. For value function, we have\nv\u0019(s) =E[Rtjst=s] =E[rt+1+\rRt+1jst=s] =E[rt+1jst=s]+\rX\ns0P(s0js;\u0019)v\u0019(s0):(56)\nIn vector form, we have\nv=\u0016r+\rPv;sov= (I\u0000\rP)\u00001\u0016r; (57)\nwhere \u0016ris the average one-step reward from each state. Thus, we have, v= \u0016r, decomposing the\nvalue function into environment dynamics (SR) and the reward signal. With SR, it is much easier\nto learn the value function. SR has wide applications in credit assignment, exploration, transfer\nlearning, planning, imitation learning, and continual learning, etc. There are some recent papers\nabout successor representation, e.g., Barreto et al. (2017), Kulkarni et al. (2016), Sherstan et al.\n(2018), and Zhang et al. (2017). See Gershman (2018) for a review.\nRecently, Chen et al. (2018b) develop a bilinear representation to capture state-action distributions.\n8.2 N EURAL NETWORKS\nIn this section, we discuss representation learning, neural network architectures, challenges to CNN\nand RNN, memory, and a recently proposed grid-like representation. We discuss generative query\nnetwork (GQN) for scene representation (Eslami et al., 2018) in Chapter 10. Deep learning, or deep\nneural networks, have been playing critical roles in many recent successes in AI.\nREPRESENTATION LEARNING\nRepresentation learning is central to the success of deep learning. An ideal representation captures\nunderlying disentangled, causal factors of data, and regularization strategies are necessary for gen-\neralization, following no free lunch theorem (Bengio et al., 2013; Goodfellow et al., 2016). We list\nthese regularization strategies below. With smoothness , training data generalize to close neighbour-\nhood in input space. Linearity assumes linear relationship, and may be orthogonal to smoothness.\nMultiple explanatory factors govern data generation, and motivate distributed representation, with\nseparate directions corresponding to separate factors of variation. Causal factors imply that learned\nfactors are causes of observed data, not vice versa. Depth, or a hierarchical organization of ex-\nplanatory factors deﬁnes high level, abstract concepts with simple, hierarchical concepts. Shared\nfactors across tasks enable sharing of statistical strength between tasks. Manifolds represent the\nconcentration of probability mass with lower dimensionality than original space of data. Natural\nclustering identiﬁes disconnected manifolds, each may contain a single class. Temporal and spa-\ntial coherence assumes that critical explanatory factors change more slowly than raw observations,\nthus easier to predict. Sparsity assumes that most inputs are described by only a few factors. And,\nsimplicity of factor dependencies assumes simple dependancies among factors, e.g., marginal inde-\npendence, linear dependency, or those in shallow autoencoders. Watch a talk Bengio (2018). See\nNIPS 2017 Workshop on Learning Disentangled Representations: from Perception to Control at\nhttps://sites.google.com/view/disentanglenips2017 .\nNEURAL NETWORK ARCHITECTURES\nA CNN is a feedforward deep neural network, with convolutional layers, pooling layers, and fully\nconnected layers. CNNs are designed to process data with multiple arrays, with locality and trans-\nlation invariance as inductive bias (LeCun et al., 2015).\nA RNN is built with a recurrent cell, and can be seen as a multilayer neural network with all layers\nsharing the same weights, with temporal invariance as inductive bias (LeCun et al., 2015). Long\nshort term memory networks (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent unit\n(GRU) (Chung et al., 2014) are two popular RNNs, to address issues with gradient computation with\nlong time steps.\nHinton and Salakhutdinov (2006) propose an autoencoder to reduce the dimensionality of data with\nneural networks. Sabour et al. (2017) and Hinton et al. (2018) propose capsules with dynamic rout-\ning, to parse the entire object into a parsing tree of capsules, each of which has a speciﬁc meaning.\n48\n\nThere are new neural network architectures customized for RL problems, e.g., value iteration\nnetworks (VIN) (Tamar et al., 2016), predictron (Silver et al., 2016b), value prediction network\n(VPN) (Oh et al., 2017), imagination-augmented agents (IA2) (Weber et al., 2017), TreeQN and\nATreeC (Farquhar et al., 2018), temporal difference models (TDMs) (Pong et al., 2018), MCT-\nSnetsGuez et al. (2018), and BBQ-Networks (Lipton et al., 2018). We discuss RL with models in\nChapter 6.\nCHALLENGES TO CNN AND RNN\nSome recent papers challenge if RNNs are a natural choice for sequence modelling. Bai et al. (2018)\nshow empirically that CNNs outperform RNNs over a wide range of tasks. See the open source\nathttps://github.com/locuslab/TCN . Miller and Hardt (2018) show that feed-forward\nnetworks approximate stable RNNs well, for both learning and inference with gradient descent,\nand validate theoretical results with experiments. Vaswani et al. (2017) propose a self-attention\nmechanism to replace recurrent and convolutional layers, for sequence transduction problems, like\nlanguage modelling and machine translation.\nMEMORY\nMemory provides long term data storage. LSTM is a classical approach for equipping a neural\nnetwork with memory, and its memory is for both storage and computation. Weston et al. (2015)\npropose memory networks to combine inference with a long-term memory. Graves et al. (2016)\npropose differentiable neural computer (DNC) to solve complex, structured problems. Wayne et al.\n(2018) propose memory, RL, and inference network (MERLIN) to deal with partial observabil-\nity. We discuss attention and memory including above papers in Chapter 9. Below we discuss\nbrieﬂy neural networks equipped with memory to facilitate reasoning, e.g., relational memory core\n(RMC) (Santoro et al., 2018), and, compositional attention networks (Hutson, 2018).\nGRID-LIKE REPRESENTATION\nBanino et al. (2018) study vector-based navigation with grid-like representations. In a process of\nvector-based navigation, i.e., planning direct trajectories to goals, animals travel to a remembered\ngoal, following direct routes by calculating goal-directed vectors with a Euclidean spatial metric pro-\nvided by grid cells. Grid cells are also important for integrating self-motion, i.e., path integration.\nThe authors study path integration with a recurrent network, and ﬁnd emergent grid-like represen-\ntations, which help improve performance of navigation with deep RL in challenging environments,\nand also help with mammal-like shortcut behaviors. Cueva and Wei (2018) is a concurrent work.\nCNNs are popular neural networks for image processing, and induce the representation to achieve\nexcellent results. CNNs were inspired from visual cortex. A popular representation in NLP is\nword2vec (Mikolov et al., 2013; Mikolov et al., 2017), which is inﬂuenced by linguistics, e.g.,\nquoting John Rupert Firth, ”You shall know a word by the company it keeps.” The grid cell repre-\nsentation, with origin from the brain, boosts performance for navigation tasks.\n8.3 K NOWLEDGE AND REASONING\nKnowledge and reasoning (Brachman and Levesque, 2004; Russell and Norvig, 2009) are funda-\nmental issues in AI. It is thus important to investigate issues about them, e.g., how to represent\nknowledge, like the predictive approach with general value function (GVF), or a symbolic approach\nwith entities, properties and relations, how to incorporate knowledge in the learning system, like an\ninductive bias, in particular, using a knowledge base to improve learning tasks (Chen et al., 2018a;\nYang and Mitchell, 2017), and how to design network architectures to help with reasoning, etc.\nBottou (2014) discuss machine learning and machine reasoning, and propose to deﬁne reasoning as\nthe manipulation of knowledge previously acquired to answer a new question, to cover ﬁrst-order\nlogical inference, probabilistic inference, and components in a machine learning pipeline. Evans\nand Grefenstette (2018) propose a differentiable inductive logic framework to deal with inductive\nlogic programming (ILP) problems with noisy data. Besold et al. (2017) discuss neural-symbolic\nlearning and reasoning. There are books about causality (Pearl, 2009; Pearl et al., 2016; Pearl and\n49\n\nMackenzie, 2018; Peters et al., 2017). Guo et al. (2018) present a survey of learning causality with\ndata.\nSee NIPS 2018 Workshop on Causal Learning. See NIPS 2018 Workshop on Relational Represen-\ntation Learning at https://r2learning.github.io . See NIPS 2017 Workshop on Causal\nInference and Machine Learning for Intelligent Decision Making at https://sites.google.\ncom/view/causalnips2017 . See 2015 AAAI Spring Symposium Knowledge Representation\nand Reasoning: Integrating Symbolic and Neural Approaches at https://sites.google.\ncom/site/krr2015/ .\nWe discuss general value function, hierarchical RL, and relational RL. We also discuss very brieﬂy\nseveral topics, including causality, reasoning facilitated by neural networks, and incorporating hu-\nman intelligence.\nThere are recent papers about neural approaches for algorithm induction, e.g., Balog et al. (2017);\nGraves et al. (2016); Liang et al. (2017a); Nachum et al. (2017); Reed and de Freitas (2016); Vinyals\net al. (2015); Zaremba and Sutskever (2015).\nGENERAL VALUE FUNCTION (GVF)\nA key problem in AI is to learn, represent, and use knowledge of the world. Sutton et al. (2011) dis-\ncuss that high-level representations based on ﬁrst-order logic and Bayesian networks are expressive,\nbut it is difﬁcult to learn the knowledge and it is expensive to use such knowledge; and low-level\nrepresentations like differential equations and state-transition matrices, can be learned from unsu-\npervised data, but such representations are less expressive. The authors further discuss that value\nfunctions provide semantics for predictive knowledge and goal-oriented (control) knowledge.\nSutton et al. (2011) propose to represent knowledge with General Value Function (GVF), where\npolicy, termination function, reward function, and terminal reward function are parameters. Schaul\net al. (2015) propose Universal Value Function Approximators (UVFAs) to generalize over both\nstates and goals. Andrychowicz et al. (2017) propose Hindsight Experience Replay (HER) to combat\nwith the issue of sparse reward, following the idea of GVF. We discuss GVF in Section 3.3.\nHIERARCHICAL RL\nHierarchical RL (Barto and Mahadevan, 2003) is a way to learn, plan, and represent knowledge\nwith temporal abstraction at multiple levels, with a long history, e.g., options (Sutton et al., 1999)\nand MAXQ (Dietterich, 2000). Hierarchical RL explores in the space of high-level goals to address\nissues of sparse rewards and/or long horizons. Hierarchical RL may be helpful for transfer and\nmulti-task learning, which we discuss in Section 14.2. Hierarchical planning is a classical topic in\nAI (Russell and Norvig, 2009). There are some recent papers, like, hierarchical-DQN (Kulkarni\net al., 2016), strategic attentive writer (Vezhnevets et al., 2016), feudal network (Vezhnevets et al.,\n2017), option-critic (Bacon et al., 2017), option discovery with a Laplacian framework (Machado\net al., 2017), and, stochastic neural networks (Florensa et al., 2017). We discuss hierarchical RL in\nChapter 11.\nRELATIONAL RL\nStatistical relational learning and reasoning studies uncertain relations, and manipulates structured\nrepresentations of entities and their relations, with rules about how to compose them (Battaglia\net al., 2018; Getoor and Taskar, 2007). Inductive logic programming (ILP) learns uncertain logic\nrules from positive and negative examples, entailing positive examples but not negative ones. Prob-\nabilistic ILP (Raedt et al., 2008; Manhaeve et al., 2018) is closely related to statistical relational\nlearning. Probabilistic ILP integrates rule-based learning with statistical learning, and tackles the\nhigh complexity of ILP. Graphical models (Koller and Friedman, 2009) are important approaches\nfor statistical relational learning.\nArtiﬁcial neural networks have alternative names, including connectionism, parallel distributed pro-\ncessing, and neural computation (Russell and Norvig, 2009). Symbolism is about a formal language\nwith symbols and rules, deﬁned by mathematics and logic. Relational learning and reasoning with\nneural networks is an approach integrating connectionism and symbolism.\n50\n\nRelational RL integrates RL with statistical relational learning, and connects RL with classical AI,\nfor knowledge representation and reasoning. Relational RL is not new. D ˇzeroski et al. (2001) pro-\npose relational RL. Tadepalli et al. (2004) survey relational RL. Guestrin et al. (2003) introduce rela-\ntional MDPs. Diuk et al. (2008) introduce objected-oriented MDPs (OO-MDPs). Recently, Battaglia\net al. (2018) propose graph network (GN) to incorporate relational inductive bias, Zambaldi et al.\n(2018) propose deep relational RL, Keramati et al. (2018) propose strategic object oriented RL, and\nthere are also deep learning approaches to deal with relations and/or reasoning, e.g., Battaglia et al.\n(2016), Chen et al. (2018a), Hutson (2018), Santoro et al. (2017), and Santoro et al. (2018). We\ndiscuss relational RL in Chapter 13.\nCAUSALITY\nPearl (2018) discusses that there are three fundamental obstacles for current machine learning sys-\ntems to exhibit human-level intelligence: adaptability or robustness, explainability, and understand-\ning of cause-effect connections. The author describes a three layer causal hierarchy: association,\nintervention, and counterfactual. Association invokes statistical relationships, with typical questions\nlike ”What is?” and ”How would seeing Xchange my belief in Y”. Intervention considers not only\nseeing what is, but also changing what we see, with typical questions like ”What if?” and ”What if I\ndoX?”. Counterfactual requires imagination and retrospection, with typical questions like ”Why?”\nand ”What if I had acted differently?”. Counterfactuals subsume interventional and associational\nquestions, and interventional questions subsume associational questions.\nPearl (2018) proposes structural causal model, which can accomplish seven pillar tasks in automated\nreasoning: 1) encoding causal assumptions - transparency and testability, 2) do-calculus and the\ncontrol of counfounding, 3) the algorithmization of counterfactuals, 4) mediation analysis and the\nassessment of direct and indirect effects, 5) adaptability, external validity and sample selection bias,\n6) missing data, and, 7) causal discovery.\nSee some recent papers using deep learning to treat causality, e.g., Johansson et al. (2016), Hartford\net al. (2017), and Lopez-Paz et al. (2017). Lattimore et al. (2016) discuss causal bandits. Tamar\net al. (2018) discuss learning plannable representations with causal InfoGAN. Liu et al. (2018d)\nstudy off-policy policy evaluation inspired by causal reasoning.\nREASONING\nBattaglia et al. (2018) propose graph network (GN) to incorporate relational inductive bias, to at-\ntempt to achieve combinatorial generalization. GN generalizes graph neural network (GNN), e.g.,\nScarselli et al. (2009). Santoro et al. (2017) propose relation networks (RNs) for relational reason-\ning. Santoro et al. (2018) propose a relational memory core (RMC) with self-attention to handle\ntasks with relational reasoning. Hudson and Manning (2018) propose memory, attention, and con-\ntrol (MAC) recurrent cell for reasoning. Yi et al. (2018) discuss disentangling reasoning from vision\nand language understanding. We discuss relational RL in Chapter 13.\nHUMAN INTELLIGENCE\nLake et al. (2016) discuss that we should build machines towards human-like learning and think-\ning. In particular, we should build causal world models, to support understanding and explanation,\nseeing entities rather than just raw inputs or features, rather than just pattern recognition; we should\nsupport and enrich the learned knowledge grounding in intuitive physics and intuitive psychology;\nwe should represent, acquire, and generalize knowledge, leveraging compositionality and learning\nto learn, rapidly adapt to new tasks and scenarios, recombining representations, without retraining\nfrom scratch.\nLake et al. (2016) discuss that the following are key ingredients to achieve human-like learning\nand thinking: a) developmental start-up software, or cognitive capabilities in early development,\nincluding, a.1) intuitive physics, and, a.2) intuitive psychology; b) learning as rapid model build-\ning, including, b.1) compositionality, b.2) causality, and, b.3) learning to learn; c) thinking fast,\nincluding, c.1) approximate inference in structured models, and, c.2) model-based and model-free\nreinforcement learning. Watch a video Tenenbaum (2018).\n51\n\nWe explain some of these key gradients by quoting directly from Lake et al. (2016). Intuitive physics\nrefers to that ”Infants have primitive object concepts that allow them to track objects over time and to\ndiscount physically implausible trajectories”. Intuitive psychology refers to that ”Infants understand\nthat other people have mental states like goals and beliefs, and this understanding strongly constrains\ntheir learning and predictions”. For causality: ”In concept learning and scene understanding, causal\nmodels represent hypothetical real-world processes that produce the perceptual observations. In\ncontrol and reinforcement learning, causal models represent the structure of the environment, such\nas modeling state-to-state transitions or action/state-to-state transitions.”\nBotvinick et al. (2017) discuss about one additional ingredient, autonomy, so that agents can build\nand exploit their own internal models, with minimal human manual engineering.\n52\n\nPART II: I MPORTANT MECHANISMS\nIn this part, we discuss important mechanisms for the development of (deep) reinforcement learning,\nincluding attention and memory in Chapter 9, unsupervised learning in Chapter 10, hierarchical RL\nin Chapter 11, relational RL in Chapter 13, multi-agent RL in Chapter 12, and, learning to learn in\nChapter 14.\nNote that we do not discuss some mechanisms, like Bayesian RL (Ghavamzadeh et al., 2015), and\nsemi-supervised RL (Audiffren et al., 2015; Cheng et al., 2016; Dai et al., 2017; Finn et al., 2017b;\nKingma et al., 2014; Papernot et al., 2017; Yang et al., 2017; Zhu and Goldberg, 2009).\n53\n\n9 A TTENTION AND MEMORY\nAttention is a mechanism to focus on the salient parts. Memory provides long term data storage.\nAttention can be an approach for memory addressing.\nA soft attention mechanism, e.g., Bahdanau et al. (2015), utilizes a weighted addressing scheme to\nall memory locations, or a distribution over memory locations, can be trained with backpropagation.\nA hard attention mechanism, e.g., Zaremba and Sutskever (2015), utilizes a pointer to address a\nmemory location, following the way conventional computers accessing memory, and can be trained\nwith reinforcement learning, in particular, policy gradient. Attention can help with visualization\nabout where a model is attending to, e.g., in machine translation and image captioning. Most papers\nfollow a soft attention mechanism. There are endeavours for hard attention (Liang et al., 2017a;\nMalinowski et al., 2018; Xu et al., 2015; Zaremba and Sutskever, 2015).\nSee Olah and Carter (2016) and Britz (2016) for discussions about attention and memory; the former\ndiscusses neural Turing machine (Graves et al., 2014) etc., and the latter discusses sequence-to-\nsequence model (Bahdanau et al., 2015), etc.\nIn the following, we discuss several papers about attention and/or memory.\nSee also Ba et al. (2014; 2016); Danihelka et al. (2016); Duan et al. (2017); Eslami et al. (2016);\nGregor et al. (2015); Jaderberg et al. (2015); Kaiser and Bengio (2016); Kadlec et al. (2016); Oh\net al. (2016); Oquab et al. (2015); Yang et al. (2016); Zagoruyko and Komodakis (2017); Zaremba\nand Sutskever (2015).\nATTENTION\nCho et al. (2014) and Sutskever et al. (2014) propose the sequence to sequence approach by using\ntwo RNNs to encode a sentence to a ﬁx-length vector and then decode the vector into a target\nsentence. To address the issues with encoding the whole sentence into a ﬁx-length vector in the\nbasic sequence to sequence approach, Bahdanau et al. (2015) introduce a soft-attention technique,\ni.e., weighted sum of annotations to which an encoder maps the source sentence, to learn to jointly\nalign and translate, by soft-searching for most relevant parts of the source sentence, and predicting\na target word with these parts and previously generated target words.\nMnih et al. (2014) introduce the recurrent attention model (RAM) to focus on selected sequence of\nregions or locations from an image or video for image classiﬁcation and object detection, to reduce\ncomputational cost for handling large video or images. The authors utilize REINFORCE to train\nthe model, to overcome the issue that the model is non-differentiable, and experiment on an image\nclassiﬁcation task and a dynamic visual control problem.\nXu et al. (2015) integrate attention to image captioning, inspired by the papers in neural machine\ntranslation (Bahdanau et al., 2015) and object recognition (Mnih et al., 2014; Ba et al., 2014). The\nauthors utilize a CNN to encode the image, and an LSTM with attention to generate a caption. The\nauthors propose a soft deterministic attention mechanism and a hard stochastic attention mechanism.\nThe authors show the effectiveness of attention with caption generation tasks on Flickr8k, Flickr30k,\nand MS COCO datasets.\nVaswani et al. (2017) propose Transformer, using self-attention to replace recurrent and convolu-\ntional layers, for sequence transduction problems, like language modelling and machine transla-\ntion. Transformer utilizes a scaled dot-product attention, to map a query and key-value pairs to\nan output, and computes a set of queries as matrices simultaneously to improve efﬁciency. Trans-\nformer further implements a multi-head attention by transforming queries, keys, and values with\ndifferent, learned linear projections respectively, performing the attention function in parallel, then\nconcatenating results and yielding ﬁnal values. Transformer follows the encoder-decoder archi-\ntecture. The encoder is composed of a stack of six identical layers, with two sub-layers, a multi-\nhead self-attention mechanism, then, a position-wise fully connected feed-forward network, with\nresidual connection around each sub-layer, followed by layer normalization. The decoder is the\nsame as the encoder, with an additional multi-head attention sub-layer between the two sub-layers,\nwhich takes inputs from the output of the encoder stack and the output from previous multi-head\nattention sub-layer. Transformer implements positional encoding to account for the order of the se-\nquence. The authors evaluate Transformer on two machine translation tasks, achieve good results\n54\n\nw.r.t. BLEU score, and show that an attention mechanism has better time efﬁciency and is more\nparallelizable than recurrent models. Dehghani et al. (2018) extend Transformer. See open source\nathttps://github.com/tensorflow/tensor2tensor , in particular, for Dehghani et al.\n(2018) at https://goo.gl/72gvdq . Tang et al. (2018b) study the hypothesis that self-attention\nand CNNs, rather than RNNs, can extract semantic features to improve long range dependences in\ntexts with NLP tasks.\nMEMORY\nWeston et al. (2015) propose memory networks to combine inference with a long-term memory,\nwhich could be read from and written to, and train these two components to use them jointly. The\nauthors present a speciﬁc implementation of the general framework on the task of question answer-\ning (QA), where the memory works as a dynamic knowledge base, and evaluate on a large-scale QA\ntask and a smaller yet complex one.\nSukhbaatar et al. (2015) extend Weston et al. (2015) with a recurrent attention model over a large\nexternal memory, train in an end-to-end way, and experiment with question answering and language\nmodelling tasks. See open source at https://github.com/facebook/MemNN .\nGraves et al. (2016) propose differentiable neural computer (DNC), in which, a neural network\ncan read from and write to an external memory, so that DNC can solve complex, structured prob-\nlems, which a neural network without read-write memory can not solve. DNC minimizes memory\nallocation interference and enables long-term storage. Similar to a conventional computer, in a\nDNC, the neural network is the controller and the external memory is the random-access memory,\nand a DNC represents and manipulates complex data structures with the memory. Differently, a\nDNC learns such representation and manipulation end-to-end with gradient descent from data in\na goal-directed manner. When trained with supervised learning, a DNC can solve synthetic ques-\ntion answering problems, for reasoning and inference in natural language. Moreover, it can solve\nthe shortest path ﬁnding problem between two stops in transportation networks and the relation-\nship inference problem in a family tree. When trained with reinforcement learning, a DNC can\nsolve a moving blocks puzzle with changing goals speciﬁed by symbol sequences. DNC outper-\nforms normal neural network like LSTM or DNC’s precursor neural Turing machine (Graves et al.,\n2014). With harder problems, an LSTM may simply fail. Although these experiments are rela-\ntively small-scale, we expect to see further improvements and applications of DNC. See a blog at\nhttps://deepmind.com/blog/differentiable-neural-computers/ .\nWayne et al. (2018) propose memory, RL, and inference network (MERLIN) to deal with partial\nobservability, by equipping with extensive memory, and more importantly, formatting memory in\nthe right way for storing right information trained with unsupervised predictive modelling. The\nauthor evaluate MERLIN on behavioural tasks in psychology and neurobiology, which may have\nhigh dimension sensory input and long duration of experiences.\n55\n\n10 U NSUPERVISED LEARNING\nUnsupervised learning takes advantage of the massive amount of data without labels, and would be\na critical mechanism to achieve artiﬁcial general intelligence.\nUnsupervised learning is categorized into non-probabilistic models, like sparse coding, autoen-\ncoders, k-means etc, and probabilistic (generative) models, where density functions are concerned,\neither explicitly or implicitly. Among probabilistic (generative) models with explicit density func-\ntions, some are with tractable models, like fully observable belief nets, neural autoregressive dis-\ntribution estimators, and PixelRNN, etc; some are with non-tractable models, like Botlzmann ma-\nchines, variational autoencoders, Helmhotz machines, etc. For probabilistic (generative) models\nwith implicit density functions, we have generative adversarial networks (GANs), moment match-\ning networks, etc. See Salakhutdinov (2016) for more details.\nLeCun (2018) summarizes the development of deep learning, and outlooks the future of AI, high-\nlighting the role of world models and self-supervised learning.5\nSelf-supervised learning is a special type of unsupervised learning, in which, no labels are given;\nhowever, labels are created from the data. Unsupervised auxiliary learning (Jaderberg et al., 2017;\nMirowski et al., 2017), GANs, and Aytar et al. (2018), as we discuss below, can be regarded as self-\nsupervised learning. Pathak et al. (2017) propose curiosity-driven exploration by self-supervised\nprediction. Watch two talks, Efros (2017) and Gupta (2017), about self-supervised learning.\nGoel et al. (2018) conduct unsupervised video object segmentation for deep RL. Mirowski et al.\n(2018) study learning to navigate in cities without a map. Hsu et al. (2018) study unsupervised\nlearning with meta-learning. See also Artetxe et al. (2018), Le et al. (2012), Liu et al. (2017b), Nair\net al. (2018), and van den Oord et al. (2018).\nIn the following, we discuss unsupervised auxiliary learning (Jaderberg et al., 2017; Mirowski et al.,\n2017), which, together with Horde (Sutton et al., 2011), are approaches to take advantages of\npossible non-reward training signals in environments. We also discuss generative adversarial net-\nworks (Goodfellow et al., 2014), generative query network (GQN) for scene representation (Eslami\net al., 2018), and, playing hard exploration games by watching YouTube (Aytar et al., 2018).\nUNSUPERVISED AUXILIARY LEARNING\nEnvironments may contain abundant possible training signals, which may help to expedite achieving\nthe main goal of maximizing the accumulative rewards, e.g., pixel changes may imply important\nevents, and auxiliary reward tasks may help to achieve a good representation of rewarding states.\nThis may be even helpful when the extrinsic rewards are rarely observed.\nJaderberg et al. (2017) propose unsupervised reinforcement and auxiliary learning (UNREAL) to\nimprove learning efﬁciency by maximizing pseudo-reward functions, besides the usual cumula-\ntive reward, while sharing a common representation. UNREAL is composed of four components:\nbase agent, pixel control, reward prediction, and value function replay. The base agent is a CNN-\nLSTM agent, and is trained on-policy with A3C (Mnih et al., 2016). Experiences of observations,\nrewards and actions are stored in a replay buffer, for being used by auxiliary tasks. The auxil-\niary policies use the base CNN and LSTM, together with a deconvolutional network, to maximize\nchanges in pixel intensity of different regions of the input images. The reward prediction mod-\nule predicts short-term extrinsic reward in next frame by observing the last three frames, to tackle\nthe issue of reward sparsity. Value function replay further trains the value function. UNREAL\n5LeCun (2018) uses the cake metaphor, as in his NIPS 2016 invited talk titled Predictive Learning. In\nthis metaphor, ”pure” reinforcement learning, as the single cherry on the cake, ”predicts a scalar reward given\nonce in a while”, with very low feedback information content; supervised learning, as the icing of the cake,\n”predicts a category or a few numbers for each input”, with medium feedback information content; and, self-\nsupervised learning, as cake genoise, ”predicts any part of its input for any observed part”, or ”predicts future\nframes in videos”, with high but stochastic feedback information content (”self-supervised learning” replacing\n”unsupervised/predictive learning” in his NIPS 2016 talk). As one response from the RL community, Pieter\nAbbeel presents a cake with many cherries in Abbeel (2017a), as a metaphor that RL methods can also have\nhigh information content, e.g., Hindsight Experience Replay (HER) (Andrychowicz et al., 2017) and Universal\nValue Function Approximators (UVFAs) (Schaul et al., 2015).\n56\n\nhas a shared representation among signals, while Horde trains each value function separately with\ndistinct weights. The authors show that UNREAL improves A3C’s performance on Atari games,\nand performs well on 3D Labyrinth game. See a blog at https://deepmind.com/blog/\nreinforcement-learning-unsupervised-auxiliary-tasks/ .\nMirowski et al. (2017) obtain navigation ability by solving a RL problem maximizing cumulative\nreward and jointly considering unsupervised tasks to improve data efﬁciency and task performance.\nThe authors address the sparse reward issues by augmenting the loss with two auxiliary tasks, 1)\nunsupervised reconstruction of a low-dimensional depth map for representation learning to aid ob-\nstacle avoidance and short-term trajectory planning; 2) self-supervised loop closure classiﬁcation\ntask within a local trajectory. The authors incorporate a stacked LSTM to use memory at differ-\nent time scales for dynamic elements in the environments. The proposed agent learns to navigate\nin complex 3D mazes end-to-end from raw sensory inputs, and performs similarly to human level,\neven when start/goal locations change frequently. In this approach, navigation is a by-product of the\ngoal-directed RL optimization problem, in contrast to conventional approaches such as simultane-\nous localization and mapping (SLAM), where explicit position inference and mapping are used for\nnavigation.\nGENERATIVE ADVERSARIAL NETS\nGoodfellow et al. (2014) propose generative adversarial nets (GANs) to estimate generative models\nvia an adversarial process by training two models simultaneously, a generative model Gto capture\nthe data distribution, and a discriminative model Dto estimate the probability that a sample comes\nfrom the training data but not the generative model G.\nGoodfellow et al. (2014) model GandDwith multilayer perceptrons: G(z:\u0012g)andD(x:\u0012d),\nwhere\u0012gand\u0012dare parameters, xare data points, and zare input noise variables. Deﬁne a prior on\ninput noise variable pz(z).Gis a differentiable function and D(x)outputs a scalar as the probability\nthatxcomes from the training data rather than pg, the generative distribution we want to learn.\nDwill be trained to maximize the probability of assigning labels correctly to samples from both\ntraining data and G. Simultaneously, Gwill be trained to minimize such classiﬁcation accuracy,\nlog(1\u0000D(G(z))). As a result, DandGform the two-player minimax game as follows:\nmin\nGmax\nDEx\u0018pdata(x)[logD(x)] +Ez\u0018pz(z)[log(1\u0000D(G(z)))]: (58)\nGoodfellow et al. (2014) show that as GandDare given enough capacity, generative adversarial nets\ncan recover the data generating distribution, and provide a training algorithm with backpropagation\nby minibatch stochastic gradient descent.\nGANs are notoriously hard to train. See Arjovsky et al. (2017) for Wasserstein GAN (WGAN) as\na stable GANs model. Gulrajani et al. (2017) propose to improve stability of WGAN by penaliz-\ning the norm of the gradient of the discriminator w.r.t. its input, instead of clipping weights as in\nArjovsky et al. (2017). Mao et al. (2016) propose Least Squares GANs (LSGANs), another stable\nmodel. Berthelot et al. (2017) propose BEGAN to improve WGAN by an equilibrium enforcing\nmodel, and set a new milestone in visual quality for image generation. Bellemare et al. (2017)\npropose Cram ´er GAN to satisfy three machine learning properties of probability divergences: sum\ninvariance, scale sensitivity, and unbiased sample gradients. Hu et al. (2017) uniﬁed GANs and\nVariational Autoencoders (V AEs).\nLucic et al. (2018) conduct a large-scale empirical study on GANs models and evaluation measures,\nand observe that, by ﬁne-tuning hyperparameters and random restarts, most models perform sim-\nilarly. The authors propose more data sets which enable computing of precision and recall. The\nauthors further observe that the evaluated models do not outperform the original GAN algorithm.\nKurach et al. (2018) discuss the state of the art of GANs in a practical perspective. The authors\nreproduce representative algorithms, discuss common issues, open-source their code on Github, and\nprovide pre-trained models on TensorFlow Hub. Brock et al. (2018) study image synthesis.\nWe discuss generative adversarial imitation learning (Ho and Ermon, 2016; Li et al., 2017) in Chap-\nter 5. Finn et al. (2016a) establish connections between GANs, inverse RL, and energy-based mod-\nels. Pfau and Vinyals (2016) establish connections between GANs and actor-critic algorithms.\n57\n\nSee Goodfellow (2017) for summary of his NIPS 2016 Tutorial on GANs. GANs have re-\nceived much attention and many work have been appearing after the publication of Goodfel-\nlow (2017). See CVPR 2018 tutorial on GANs at https://sites.google.com/view/\ncvpr2018tutorialongans/ , with several talks by several speakers.\nGENERATIVE QUERY NETWORK\nEslami et al. (2018) propose generative query network (GQN) for scene representation, to obtain a\nconcise description of a 3D scene from 2D visual sensory data, with a representation network and\na generator network trained jointly in an end-to-end fashion, without human semantic labels, e.g.,\nobject classes, object locations, scene types, or part labels, and domain knowledge.\nIn GQN, observations from different 2D viewpoints for 3D scenes are fed into the representation\nnetwork, to obtain a neural scene representation by summing observations’ representations element-\nwise. The neural scene representation is then fed into the generation network, which is a recurrent\nlatent variable model, to make prediction about the scene from a different viewpoint. GQN is trained\nwith many scenes, with various number of observations, and with back-propagation. GQN attempts\nto obtain a concise scene representation, to capture the scene contents, e.g., the identities, positions,\ncolors, and object counts, etc., and to make a generator successful for predictions, by maximizing\nthe likelihood to generate ground-truth images from query viewpoints. Variational approximations\nare used to deal with the intractability of latent variables.\nExperiments show that representations learned by GQN with the properties of viewpoint invariance,\ncompositionality, factorization, ”scene algebra”, similar to that of word embedding algebra, and\ndecreasing Bayesian surprise with more observations for both full and partial observability. Bayesian\nsurprise refers to the KL divergence between conditional prior and posterior. Robot arm reaching\nexperiments show that the GQN representation helps with data efﬁciency and robust control.\nPLAYING GAMES BY WATCHING YOUTUBE\nAytar et al. (2018) propose to play hard exploration Atari games, including Montezuma’s Revenge,\nPitfall! and Private Eye, by watching YouTube. YouTube videos are usually noisy and unaligned,\nwithout the frame-by-frame alignment between demonstrations, and the information of exact action\nand reward sequences in demonstrator’s observation trajectory, which are the properties of demon-\nstrations required by previous imitation learning, e.g., Hester et al. (2018) as we discuss in Chapter 5.\nAytar et al. (2018) overcome these limitations by a one-shot imitation method in two steps. First,\na common representation is learned from unaligned videos from multiple sources, with two self-\nsupervised objectives: temporal distance classiﬁcation (TDC) and cross-model temporal distance\nclassiﬁcation (CMC). In self-supervision, an auxiliary task is proposed to solve among all domains\nsimultaneously, for a network to attempt to learn a common representation. In TDC, temporal dis-\ntances between two frames in a single video sequence are predicted, to help learn a representation\nof the environment dynamics. In CMC, a representation is learned to correlate visual and audio ob-\nservations, and to highlight critical game events. Furthermore, a new measure of cycle-consistency\nis proposed to evaluate the quality of the learned representation. Second, a single YouTube video is\nembedded in such representation, and a reward function is built, so that an agent can learn to imitate\nhuman game play.\nExperiments using the distributed A3C RL algorithm IMPALA (Espeholt et al., 2018) show break-\nthrough results on these three hard Atari games.\n58\n\n11 H IERARCHICAL RL\nHierarchical RL (Barto and Mahadevan, 2003) is a way to learn, plan, and represent knowledge\nwith temporal abstraction at multiple levels, with a long history, e.g., options (Sutton et al., 1999),\nMAXQ (Dietterich, 2000), hierarchical abstract machines (Parr and Russell, 1998), and dynamic\nmovement primitives (Schaal, 2006). Hierarchical RL is an approach for issues of sparse rewards\nand/or long horizons, with exploration in the space of high-level goals. The modular structure\nof hierarchical RL approaches is usually conducive to transfer and multi-task learning, which we\ndiscussed in Section 14.2. The concepts of sub-goal, option, skill, and, macro-action are related.\nHierarchical planning is a classical topic in AI (Russell and Norvig, 2009).\nHere we introduce options brieﬂy. Markov decision process (MDP) is deﬁned by the 5-tuple\n(S;A;P;R;\r), for the state space, the action space, the state transition probability, the reward\nfunction, and the discount factor. An option oconsists of three components: 1) an initiation set\nof statesIo\u0012S, 2) a policy \u0019o:S\u0002A! [0;1], guiding the behaviour of an option, such that\n\u0019o(ajs)is the probability of taking action ain stateswhen following option o, and, 3) a termina-\ntion condition \fo:S! [0;1], roughly determining the length of an option, such that \fo(s)is the\nprobability of terminating the option oupon entering state s. A policy-over-options calls an option\no. During the execution of the option o, the agent selects an action until a termination condition is\nmet. An option may call another option. P(s0js;o)is the probability of next state s0conditioned\non that option oexecutes from state s.ro(s)is the expected return during the execution of option\no. Introducing options over an MDP constitutes a semi-Markov decision process (SMDP). It can be\nshown that learning and planning algorithms from MDPs can transfer to options. It can be shown\nthat the reward model for options is equivalent to a value function, and a Bellman equation can be\nwritten for it, so RL algorithms can be used to learn it. This also applies to the transition model for\noptions.\nUsually options are learned with provided sub-goals and pseudo-rewards, and good performance\nis shown for Atari games, e.g. with hierarchical-DQN (h-DQN) (Kulkarni et al., 2016), and for\nMineCraft, e.g., with Tessler et al. (2017). Automatic options discovery receives much atten-\ntion recently, e.g., strategic attentive writer (STRAW) (Vezhnevets et al., 2016), feudal network\n(FuN) (Vezhnevets et al., 2017), option-critic (Bacon et al., 2017), option discovery with a Lapla-\ncian framework (Machado et al., 2017), and, stochastic neural networks (Florensa et al., 2017).\nHierarchical RL follows the general algorithm design principle of divide and conquer, so that hard\ngoals, e.g. those with sparse long-term rewards are replaced with easy sub-goals, e.g. those with\ndense short-term rewards, and RL algorithms, e.g., policy-based or value-based, combined with\nrepresentations, are utilized to solve easy sub-goals, and ﬁnally to achieve hard goals.\nWatch recent talks on hierarchical RL, e.g., Silver (2017), Precup (2018). See NIPS 2017 workshop\non Hierarchical RL, at https://sites.google.com/view/hrlnips2017 , and videos at\nhttps://goo.gl/h9Mz1a .\nWe discuss several recent papers in the following. See also Kompella et al. (2017), Le et al. (2018),\nNachum et al. (2018), Peng et al. (2017a), Sharma et al. (2017), Tang et al. (2018a), Tessler et al.\n(2017), and Yao et al. (2014).\nHIERARCHICAL DQN\nKulkarni et al. (2016) propose hierarchical-DQN (h-DQN) by organizing goal-driven intrinsically\nmotivated deep RL modules hierarchically to work at different time-scales. h-DQN integrats a top\nlevel action value function and a lower level action value function. The former learns a policy over\nintrinsic sub-goals, or options (Sutton et al., 1999). And the latter learns policies over raw actions\nto satisfy the objective of each given sub-goal. In particular, h-DQN has a two-stage hierarchy with\na meta-controller and a controller. The meta-controller receives state sand select a goal g. The\ncontroller then selects an action aconditioned on state sand goalg, the goalgdoes not change\nuntil it is achieved or a termination condition is met. The internal critic evaluates if a goal has been\nachieved, and produces a reward r(g)to the controller, e.g., a binary internal reward 1 for achieving\nthe goal, and 0 otherwise. The objectives for the meta-controller and controller are to maximize\ncumulative extrinsic and intrinsic rewards, respectively. The authors evaluate h-DQN on a discrete\n59\n\nstochastic decision process, and a hard Atari game, Montezuma’s Revenge. See the open source at\nhttps://github.com/mrkulk/hierarchical-deep-RL\nFEUDAL NETWORKS\nVezhnevets et al. (2017) propose feudal networks (FuNs) for hierarchical RL, inspired by feudal\nRL (Dayan and Hinton, 1993). In FuNs, a Manager module discovers and sets abstract sub-goals\nand operates at a long time scale, and a Worker module selects atom actions at every time step of the\nenvironment to achieve the sub-goal set by the Manager. FuNs decouple end-to-end learning across\nmultiple levels at different time scales, by separating the Manager module from the Worker module,\nto facilitate long time scale credit assignment and emergence of sub-policies for sub-goals set by the\nManager. In FuNs, sub-goals are formulated as directions in the latent space, so that the Manager\nselects a subgoal direction to maximize reward, and the Worker selects actions to maximize cosine\nsimilarity to the direction of the subgoal set by the Manager. The authors utilize a dilated LSTM to\ndesign the Manager to allow backpropagation through long time steps, and experiment FuNs with a\nwater maze and Atari games.\nOPTION -CRITIC\nBacon et al. (2017) derive policy gradient theorems for options, and propose an option-critic archi-\ntecture to learn both intra-option policies and termination conditions gradually, at the same time\nwith the policy-over-options, combining options discovery with options learning. The option-\ncritic architecture works with linear and non-linear function approximations, with discrete or\ncontinuous state and action spaces, and without rewards or sub-goals. The authors experiment\nwith a four-room domain, a pinball domain, and Atari games. See the open source at https:\n//github.com/jeanharb/option_critic .\nHarutyunyan et al. (2018) study the dilemma between efﬁciency of long options and ﬂexibility of\nshort ones in the option-critic architecture. The authors decouple the behaviour and target termina-\ntion conditions, similar to off-policy learning for policies, and propose to cast options learning as\nmulti-step off-policy learning, The authors show beneﬁts of learning short options from longer ones,\nby analysis and with experiments.\nRiemer et al. (2018) further study the option-critic architecture.\nOPTION DISCOVERY WITH A L APLACIAN FRAMEWORK\nMachado et al. (2017) propose to discover options with proto-value functions (PVFs) (Mahadevan\nand Maggioni, 2007), which are well-known for representation in MDPs and deﬁne options implic-\nitly. The authors introduce the concepts of eigen-purpose and eigen-behavior. An eigen-purpose is\nan intrinsic reward function, to motivate an agent to traverse in principle directions of the learned\nrepresentation of state space. An eigen-behavior is the optimal policy for an intrinsic reward func-\ntion. The authors discover task-independent options, since the eigen-purposes are obtained without\nreward information. The authors observe that some options are not helpful for exploration, although\nthey improve the efﬁciency of planning. The authors further show that the options they discover\nimprove exploration, since these options operate at different time scales, and they can be sequenced\neasily. The authors experiment with tabular domains and Atari games.\nSTRATEGIC ATTENTIVE WRITER\nVezhnevets et al. (2016) propose strategic attentive writer (STRAW), a deep recurrent neural net-\nwork architecture, for learning high-level temporally abstract macro-actions in an end-to-end man-\nner based on observations from the environment. Macro-actions are sequences of actions commonly\noccurring. STRAW builds a multi-step action plan, updated periodically based on observing re-\nwards, and learns for how long to commit to the plan by following it without replanning. STRAW\nlearns to discover macro-actions automatically from data, in contrast to the manual approach in pre-\nvious work. Vezhnevets et al. (2016) validate STRAW on next character prediction in text, 2D maze\nnavigation, and Atari games.\n60\n\nSTOCHASTIC NEURAL NETWORKS\nFlorensa et al. (2017) propose to pre-train a large span of skills using stochastic neural networks with\nan information-theoretic regularizer, then on top of these skills, to train high-level policies for down-\nstream tasks. Pre-training is based on a proxy reward signal, which is a form of intrinsic motivation\nto explore agent’s own capabilities, requiring minimal domain knowledge about the downstream\ntasks. Their method combines hierarchical methods with intrinsic motivation, and the pre-training\nfollows an unsupervised way.\n61\n\n12 M ULTI -AGENT RL\nMulti-agent RL (MARL) is the integration of multi-agent systems (Horling and Lesser, 2004;\nShoham and Leyton-Brown, 2009; Stone and Veloso, 2000) with RL. It is at the intersection of\ngame theory (Leyton-Brown and Shoham, 2008) and RL/AI.\nBesides issues in RL like sparse rewards and sample efﬁciency, there are new issues like multi-\nple equilibria,6and even fundamental issues like what is the question for multi-agent learning, and\nwhether convergence to an equilibrium is an appropriate goal, etc. Consequently, multi-agent learn-\ning is challenging both technically and conceptually, and demands clear understanding of the prob-\nlem to be solved, the criteria for evaluation, and coherent research agendas (Shoham et al., 2007).\nIn a fully centralized approach, when global state and joint action information are available, estimat-\ning the joint Q action value function becomes possible. This can address the nonstationary issue.\nHowever, it encounters the issue of curse of dimensionality when the number of agents grows. An-\nother issue is that it may be hard to extract decentralized policies, for an agent to make decisions\nbased on its own observation.\nLittman (1994) employ stochastic games as a framework for MARL, propose minimax-Q learn-\ning for zero-sum games, and show convergence under certain conditions. Hu and Wellman (2003)\npropose Nash Q-learning for general-sum games and show its convergence with a unique Nash\nequilibrium. Bowling and Veloso (2002) propose the win or learn fast (WoLF) principle to vary the\nlearning rate to tackle the issues with learning a moving target, and show convergence with self-play\nin certain iterated matrix games. These papers, together with Foerster et al. (2018a), Lowe et al.\n(2017), and Usunier et al. (2017), follow centralized approaches.\nTan (1993) introduces independent Q-learning (IQL) for MARL, where each agent learns a Q action\nvalue function independently. For Q-learning to be stable and convergent, the environment would\nbe stationary. This is usually not the case for multi-agent systems, where an agent would change its\npolicy according to other agents, and the environment is usually nonstationary or even adversarial.\nIndependent approaches to MARL may not converge. Foerster et al. (2017) and Omidshaﬁei et al.\n(2017) propose to stabilize independent approaches.\nOliehoek et al. (2008) introduce the paradigm of centralized training for decentralized execution.\nWe discuss several papers following this scheme below.\nAlong with the success of RL in single agent problems, like, Mnih et al. (2015), Jaderberg et al.\n(2017), Schulman et al. (2015), Nachum et al. (2018), and two-player games, like Silver et al.\n(2016a; 2017); Morav ˇc´ık et al. (2017), recently, we see some progress in multi-agent RL problems,\nlike Jaderberg et al. (2018) for Quake III Arena Capture the Flag, Sun et al. (2018) and Pang et al.\n(2018) for StarCraft II, and OpenAI Five for Dota 2.\nZambaldi et al. (2018) investigate StarCraft II mini-games with relational deep RL, as discussed\nin Chapter 13. Bansal et al. (2018) investigate the emergent complex skills via multi-agent com-\npetition. Foerster et al. (2018b) propose learning with opponent-learning awareness, so that each\nagent considers the learning process of other agents in the environment. Hoshen (2017) present ver-\ntex attention interaction network (V AIN), for multi-agent predictive modelling, with an attentional\nneural network. Mhamdi et al. (2017) introduce dynamic safe interruptibility for MARL, in joint\naction learners and independent learners scenarios. Perolat et al. (2017) propose to use MARL for\nthe common pool resource appropriation problem. Hu et al. (2018b) propose opponent-guided tactic\nlearning for StarCraft micromanagement. Song et al. (2018) extend generative adversarial imitation\nlearning (GAIL) (Ho and Ermon, 2016) to multi-agent settings. Lanctot et al. (2018) study actor-\ncritic policy optimization in partially observable multi-agent settings. See also Hughes et al. (2018),\nWai et al. (2018), and Zhou et al. (2018)\nMulti-agent systems have many applications, e.g., as we will discuss, games in Chapter 15, robotics\nin Chapter 16, energy in Section 20.3, transportation in Section 20.4, and compute systems in Sec-\ntion 20.5.\n6Chen and Deng (2006) show that ﬁnding a Nash equilibrium in a two-player game is PPAD-complete, i.e.,\nunless every problem in PPAD is solvable in polynomial time, there is not a fully polynomial-time approxima-\ntion algorithm for ﬁnding a Nash equilibrium in a two-player game. PPAD is a complexity class for polynomial\nparity arguments on directed graphs.\n62\n\nBusoniu et al. (2008) and Ghavamzadeh et al. (2006) are surveys for multi-agent RL. Parkes and\nWellman (2015) is a survey about economic reasoning and AI.\nIn the following, we discuss centralized training for decentralized execution, several issues in game\ntheory, and games.\nCENTRALIZED TRAINING FOR DECENTRALIZED EXECUTION\nA centralized critic can learn from all available state information conditioned on joint actions, and\neach agent learns its policy from its own observation action history. The centralized critic is only\nused during learning, and only the decentralized actor is needed during execution. In the follow-\ning, several recently propose approaches use StarCraft as the experimental testbed, e.g., Peng et al.\n(2017b), Foerster et al. (2018a), and Rashid et al. (2018).\nFoerster et al. (2018a) propose the counterfactual multi-agent (COMA) actor-critic method. In\nCOMA, policies are optimized with decentralized actors, and Q-function is estimated with a cen-\ntralized critic, using a counterfactual baseline to marginalize out one agent’s action, and ﬁxing other\nagents’ actions, for the purpose of multi-agent credit assignment.\nSome papers propose communication mechanisms in MARL (Foerster et al., 2016; Sukhbaatar et al.,\n2016). Peng et al. (2017b) require communication.\nPeng et al. (2017b) propose a multiagent actor-critic framework, with a bidirectionally-coordinated\nnetwork to form coordination among multiple agents in a team, deploying the concept of dynamic\ngrouping and parameter sharing for better scalability. In the testbed of StarCraft, without human\ndemonstration or labelled data as supervision, the proposed approach learns strategies for coordi-\nnation similar to the level of experienced human players, like move without collision, hit and run,\ncover attack, and focus ﬁre without overkill.\nIt is desirable to design an algorithm between the two extremes of independent RL and fully central-\nized RL approaches. One way is to decompose Q function. Sunehag et al. (2017) and Rashid et al.\n(2018) fall into this category.\nSunehag et al. (2017) propose value-decomposition networks (VDN) to represent the centralized\naction value function Q as a sum of value functions of individual agents. In VDN, each agent trains\nits value function based on its observations and actions, and a decentralized policy is derived from\nits action value function.\nRashid et al. (2018) propose QMIX, so that each agent network represents an individual action\nvalue function, and a mixing network combines them into a centralized action value function, with\na non-linear approach, in contrast to the simple sum in VDN (Sunehag et al., 2017). Such factored\nrepresentation allows complex centralized action value function, extraction of decentralized policies\nwith linear time individual argmax operations, and scalability.\nISSUES IN GAME THEORY\nHeinrich and Silver (2016) propose neural ﬁctitious self-play (NFSP) to combine ﬁctitious self-play\nwith deep RL to learn approximate Nash equilibria for games of imperfect information in a scalable\nend-to-end approach without prior domain knowledge. NFSP is evaluated on two-player zero-sum\ngames. In Leduc poker, NFSP approaches a Nash equilibrium, while common RL methods diverges.\nIn Limit Texas Hold’em, a real-world scale imperfect-information game, NFSP performs similarly\nto state-of-the-art, superhuman algorithms which are based on domain expertise.\nLanctot et al. (2017) observe that independent RL, in which each agent learns by interacting with the\nenvironment, oblivious to other agents, can overﬁt the learned policies to other agents’ policies. The\nauthors propose policy-space response oracle (PSRO), and its approximation, deep cognitive hier-\narchies (DCH), to compute best responses to a mixture of policies using deep RL, and to compute\nnew meta-strategy distributions using empirical game-theoretic analysis. PSRO/DCH generalizes\nprevious algorithms, like independent RL, iterative best response, double oracle, and ﬁctitious play.\nThe authors present an implementation with centralized training for decentralized execution, as dis-\ncussed below. The authors experiment with grid world coordination, a partially observable game,\n63\n\nand Leduc Poker (with a six-card deck), a competitive imperfect information game, and show re-\nduced joint-policy correlation (JPC), a new metric to quantify the effect of overﬁtting.\nSocial dilemmas, e.g., prisoner’s dilemma, reveal the conﬂicts between collective and individual\nrationality. Cooperation is usually beneﬁcial for all. However, parasitic behaviours like free-riding\nmay result in the tragedy of the commons, which makes cooperation unstable. The formalism of\nmatrix game social dilemmas (MGSD) is a popular approach. However, as discussed in Leibo et al.\n(2017), MGSD does not consider several important aspects of real world social dilemmas: they\nare temporally extended, ”cooperation and defection are labels that apply to policies implementing\nstrategic decisions”, ”cooperative may be a graded quantity”, cooperation and defection may not\nhappen fully simultaneously, since information about the starting of a strategy by one player would\ninﬂuence the other player, and, decisions are mandatory although with only partial information about\nthe world and other players.\nLeibo et al. (2017) propose a sequential social dilemma (SSD) with MARL to tackle these issues.\nThe authors conduct empirical game theoretic analyses of two games, fruit gathering and wolfpack\nhunting, and show that, when treating cooperation and defection as one-shot decisions as in MGSD,\nthey have empirical payoff matrices as prisoner’s dilemma. However, gathering and wolfpack are\ntwo different games, with opposite behaviours in the emergence and stability of cooperation. SSDs\ncan capture the differences between these two games, using a factor to promote cooperation in gath-\nering and to discourage cooperation in wolfpack. The sequential structure of SSDs results in com-\nplex model to compute or to learn equilibria. The authors propose to apply DQN to ﬁnd equilibria\nfor SSDs.\nGAMES\nJaderberg et al. (2018) approach Capture the Flag, a 3D multi-player ﬁrst-person video game, in an\nend-to-end manner using raw inputs including pixels and game points, with techniques of population\nbased training, optimization of internal reward, and temporally hierarchical RL, and achieve human-\nlevel performance, for the ﬁrst time for multi-agent RL problems.\nJaderberg et al. (2018) propose to train a diverse population of agents, to form two teams against\neach other, and to train each agent independently in a decentralized way, only through interaction\nwith the environment, without knowledge of environment model, other agents, and human policy\nprior, and without communication with other agents. Each agent learn an internal reward signal, to\ngenerate internal goals, such as capturing a ﬂag, to complement the sparse game winning reward.\nThe authors propose a two-tier optimization process. The inner optimization maximizes agents’\nexpected discounted future internal rewards. The outer optimization solves a meta-game, to maxi-\nmize the meta-reward of winning the match, w.r.t. internal reward functions and hyperparameters,\nwith meta transition dynamics provided by the inner optimization. The inner optimization is solved\nwith RL. The outer optimization is solved with population based training (PBT) (Jaderberg et al.,\n2017), an online evolutionary method adapting internal rewards and hyperparameters and perform-\ning model selection by agent mutation, i.e., replacing under-performing agents with better ones.\nAuxiliary signals (Jaderberg et al., 2017) and differentiable neural computer memory (Graves et al.,\n2016) are also used to improve performance. RL agents are trained asynchronously from thousands\nof concurrent matches on randomly generated maps.\nThe authors design an agent to achieve strong capacity and avoiding several common RL issues,\nwith the integration of learning and evolution. Learning from a diverse population of agents on ran-\ndom maps helps achieve skills generalizable to variability of maps, number of players, and choice\nof teammates, and stability in partially observable multi-agent environments. Learning an inter-\nnal reward signal helps tackle the sparse reward problem and further the credit assignment issue.\nThe multi-timescale representation helps with memory and long term temporal reasoning for high\nlevel strategies. The authors choose PBT instead of self play, since self play may be unstable in\nmulti-agent RL environments, and needs more manipulation to support concurrent training for bet-\nter scalability.\nExperiments show that an agent can learn a disentangled representation to encode various knowledge\nof game situations, like conjunctions of ﬂag status, re-spawn state, and room type, associating with\nactivation of some individual neurones, and behaving like humans e.g., in navigating, following,\nand defending. Such knowledge is acquired through RL training, rather than from explicit models.\n64\n\nExperiments also show that the generalizable skills for tasks with random maps are supported by\nrich representation of spatial environments, induced by the temporal hierarchy and explicit memory\nmodule. Experiments show human level performance, and a survey shows that the agents are more\ncollaborative than human participants. It appears that the training was conducted with less than 2000\ncommodity computers.\nThe authors mention the limitations of the current work: ”the difﬁculty of maintaining diver-\nsity in agent populations, the greedy nature of the meta-optimisation performed by PBT, and the\nvariance from temporal credit assignment in the proposed RL updates”. See a blog at https:\n//deepmind.com/blog/capture-the-flag/ .\nSun et al. (2018) and Pang et al. (2018) have beaten full-game built-in AI in StarCraft II. OpenAI\nFive designs a Dota 2 agent for 5v5 plays, with a common RL algorithm, Proximal Policy Optimiza-\ntion (PPO) and self play, and beat human players. However, huge computation is involved, with 256\nGPUs and 128,000 CPU cores. See https://openai.com/five/ .\n65\n\n13 R ELATIONAL RL\nIntegrating reinforcement learning and relational learning (Getoor and Taskar, 2007) is a promising\napproach to problem solving in AI. Relational RL makes connections between RL and classical AI,\nfor knowledge representation and reasoning, which we discuss brieﬂy in Section 8.3.\nDˇzeroski et al. (2001) propose relational RL. Tadepalli et al. (2004) give an overview of relational\nRL, and identify several challenges: suitable function approximation to represent relational knowl-\nedge, generalization across objects, transferability across tasks, run-time planning and reasoning,\nand, incorporating prior knowledge. Tadepalli et al. (2004) further propose relational RL as a so-\nlution to these challenges. Guestrin et al. (2003) introduce relational MDPs. Diuk et al. (2008)\nintroduce objected-oriented MDPs (OO-MDPs), a close approach to relational RL. See more work,\ne.g., Mrowca et al. (2018), Palm et al. (2018), and Santoro et al. (2018).\nSee NIPS 2018 Workshop on Relational Representation Learning at https://r2learning.\ngithub.io .\nWe discuss some papers about relational learning and relational RL below.\nRELATIONAL LEARNING\nBattaglia et al. (2018) ﬁrst discuss ways to incorporate relational inductive bias with deep learning.\nIn fully connected networks (FC), entities are units, and their relations are all to all, thus have only\nweak relational inductive bias. In convolutional neural networks (CNNs), entities are still units, or\ngrid elements, e.g. pixels in images, and relations are local. CNNs impose locality and translation\ninvariance as relational inductive bias. In recurrent neural networks (RNN), entities include input\nand hidden state at each time step, and relations are the mapping from previous hidden state and\ncurrent input to the hidden state of this step. This mapping is reuse over time steps, thus temporal\ninvariance is the relational inductive bias for RNN.\nBattaglia et al. (2018) then propose graph network (GN) to incorporate relational inductive bias, to\nattempt to achieve combinatorial generalization, i.e., the capacity to use known elements to build\nnew inferences, predictions and behaviours. GN can operate on arbitrary relational structure, hav-\ning explicit representation of entities (nodes) and relationships (edges), grounding in data. Node\nand edge permutations are invariant in GN. GN generalizes previous graph neural networks, e.g.,\nScarselli et al. (2009).\nSantoro et al. (2017) propose relation networks (RNs) for relational reasoning in a plug-and-play\nway in neural networks. The authors experiment RN-augmented networks on visual question an-\nswering, text-based question answering, and reasoning about dynamic physical systems. Santoro\net al. (2018) propose a relational memory core (RMC), a memory module, to handle tasks with re-\nlational reasoning, using self attention (Vaswani et al., 2017) to deal with memory interaction. The\nauthors test RMC on RL tasks, program evaluation, and language modelling. Battaglia et al. (2016)\npropose interaction network to learning about objects, relations and physics, and evaluate it with\nn-body problems, rigid-body collision, and non-rigid dynamics.\nChen et al. (2018a) propose a framework for iterative visual reasoning, beyond current recognition\nsystems with CNNs. It is composed of a local module to store previous beliefs, a global model\nfor graph reasoning. It reﬁnes estimates by rolling out these models iteratively, and cross-feeding\npredictions to each other. The graph model consists of: 1) a knowledge graph, with nodes and edges\nto represent classes and their relationships respectively; 2) a region graph of the current image, with\nnodes and edges as regions in the images and their spatial relationships; 3) an assignment graph,\nassigning regions to classes. An attention mechanism is used to combine both local and global\nmodules for ﬁnal predictions.\nHudson and Manning (2018) propose memory, attention, and control (MAC) cell, with three units,\nnamely, control, read, and write units, to construct recurrent compositional attention networks for\nreasoning, by imposing structural constraints in the operation of and interaction between cells, for\nthe purpose of interpretability, generalization, computation efﬁciency, and data efﬁciency. Hud-\nson and Manning (2018) evaluate their proposed approach on a visual question answering (VQA)\ntask with the CLEVR data set. Watch a video at https://www.youtube.com/watch?v=\njpNLp9SnTF8 .\n66\n\nRELATIONAL RL\nZambaldi et al. (2018) propose to improve sample efﬁciency, generalization capacity, and inter-\npretability of deep RL with relational reinforcement learning (D ˇzeroski et al., 2001). The pro-\nposed network architecture consists of FC, CNNs, and a relational module, which uses self-\nattention (Vaswani et al., 2017) to reason about interactions between entities, and to facilitate model-\nfree policy optimization. Zambaldi et al. (2018) construct a navigation and planning task, BOX-\nWorld, to test the capacity of relational reasoning, and show good performance. Zambaldi et al.\n(2018) also experiment on StarCraft II mini-games in the the StarCraft II Learning Environment\n(SC2LE) (Vinyals et al., 2017), and achieve good performance on six mini-games. Note that Sun\net al. (2018) and Pang et al. (2018) have beaten full-game built-in AI in StarCraft II.\nWang et al. (2018b) propose NerveNet, resembling the neural nervous system to a graph, to\nlearn structured policy for continuous control. A policy is deﬁned using the graph neural net-\nworks (GNN) (Scarselli et al., 2009), in which, information is propagated over the agent struc-\nture, and actions are predicted for different parts of the agent. The authors evaluate NerveNet in\nOpenAI Gym environment, and on transfer learning and multi-task learning tasks. See http:\n//www.cs.toronto.edu/ ˜tingwuwang/nervenet.html for open source and demo.\nKeramati et al. (2018) propose strategic object oriented RL (SOORL) for model-learning with auto-\nmatic model selection, and planning with strategic exploration. The authors achieve positive reward\nin Pitfall!, a hard Atari game. However, we note that, concurrently, Aytar et al. (2018) achieve much\nbetter results in Pitfall! and two other hard Atari games with an approach of self-supervision+RL,\nalbeit relational and object oriented mechanisms are worth more efforts.\nYang et al. (2018) propose planning-execution-observation-RL (PEORL) to integrate hierarchical\nRL with symbolic planning for dynamic, uncertain environments.\nZambaldi et al. (2018) utilize pixel feature vectors resulting from CNNs as entities, a problem agnos-\ntic approach. Keramati et al. (2018) utilize bounding boxes to detect object. Yang et al. (2018) work\nwith manually crafted symbolic knowledge. The performance would be further improved with an\nadvanced reasoning technique about images to ﬁnd entities and relations, e.g., Chen et al. (2018a),\nSantoro et al. (2017), Santoro et al. (2018), or, Battaglia et al. (2016), etc.\n67\n\n14 L EARNING TO LEARN\nLearning to learn, a.k.a. meta-learning, is learning about some aspects of learning. It includes\nconcepts as broad as transfer learning, multi-task learning, one/few/zero-shot learning, learning\nto optimize, learning to reinforcement learn, learning combinatorial optimization, hyper-parameter\nlearning, neural architecture design, automated machine learning (AutoML), continual learning, etc.\nLearning to learn is a core ingredient to achieve strong AI (Lake et al., 2016), and has a long history,\ne.g., Ellis (1965), Schmidhuber (1987), Bengio et al. (1991), Sutton (1992), Thrun and Pratt (1998),\nHochreiter et al. (2001), and Brazdil et al. (2009).\nLi and Malik (2017) and Li and Malik (2017), along with the blog at http://bair.berkeley.\nedu/blog/2017/09/12/learning-to-optimize-with-rl/ , divide various learning\nto learn methods into three categories: learning what to learn, learning which model to learn, and,\nlearning how to learn. The authors mention that, ”roughly speaking, ’learning to learn’ simply\nmeans learning something about learning”. The authors discuss that the term of learning to learn has\nthe root in the idea of metacognition by Aristotle in 350 BC ( http://classics.mit.edu/\nAristotle/soul.html ), which describes ”the phenomenon that humans not only reason, but\nalso reason about their own process of reasoning”. In the category of learning what to learn, the\naim is to learn values for base-model parameters, gaining the meta-knowledge of commonalities\nacross the family of related tasks, and to make the base-learner useful for those tasks (Thrun and\nPratt, 1998). Examples in this category include methods for transfer learning, multi-task learning\nand few-shot learning. In the category of learning which model to learn, the aim is to learn which\nbase-model is most suitable for a task (Brazdil et al., 2009), gaining the meta-knowledge of cor-\nrelations of performance between various base-models, by investigating their expressiveness and\nsearchability. This learns the outcome of learning. In the category of learning how to learn, the\naim is to learn the process of learning, gaining the meta-knowledge of commonalities in learning\nalgorithms behaviours. There are three components for learning how to learn: the base-model, the\nbase-algorithm to train the base-model, and the meta-algorithm to learn the base-algorithm. The goal\nof learning how to learn is to design the meta-algorithm to learn the base-algorithm, which trains\nthe base-model. Bengio et al. (1991), Andrychowicz et al. (2016), Li and Malik (2017), and Li and\nMalik (2017) fall into this category. Fang et al. (2017) study learning how to active learn. Wang\net al. (2018c) study how to learn MCMC proposals. Zhang et al. (2018b) study learning to multitask.\nNegrinho et al. (2018) study learning beam search policies. Hsu et al. (2018) study unsupervised\nlearning with meta-learning.\nFinn et al. (2017a), along with the blog at https://github.com/cbfinn/maml , summarize\nthat there are three categories of methods for learning to learn, namely, recurrent models, metric\nlearning, and learning optimizers. In the approach of recurrent models, a recurrent model, e.g. an\nLSTM, is trained to take in data points, e.g., (image, label) pairs for an image classiﬁcation task,\nsequentially from the dataset, and then processes new data inputs from the task. The meta-learner\nusually uses gradient descent to train the learner, and the learner uses the recurrent network to\nprocess new data. Santoro et al. (2016), Mishra et al. (2018), Duan et al. (2016), and Wang et al.\n(2016) fall into this category. In the approach of metric learning, a metric space is learned to make\nlearning efﬁcient, mostly for few-shot classiﬁcation. Koch et al. (2015), Vinyals et al. (2016), and\nSnell et al. (2017) fall into this category. In the approach of learning optimizers, an optimizer is\nlearned, using a meta-learner network to learn to update the learner network to make the learner\nlearn a task effectively. Wichrowska et al. (2017), Andrychowicz et al. (2016), Li and Malik (2017),\nand Li and Malik (2017) fall into this category. Motivated by the success of using transfer learning\nfor initializing computer vision network weights with the pre-trained ImageNet weights (Donahue\net al., 2014), Finn et al. (2017a) propose model-agnostic meta-learning (MAML) to optimize an\ninitial representation for a learning algorithm, so that the parameters can be ﬁne-tuned effectively\nfrom a few examples.\nDuan (2017) gives a brief review of meta-learning. The author discusses meta-learning for super-\nvised learning, including metric-based models, optimization-based models, and fully generic mod-\nels, and other applications. The author also discusses meta-learning for control, and proposes to\nlearn reinforcement learning algorithms (Duan et al., 2016), and one-shot imitation learning (Duan\net al., 2017).\n68\n\nCombinatorial optimization is critical for many areas, like social networks, telecommunications,\nand transportation. Many combinatorial optimization problems are NP-hard, and algorithms follow\nthree approaches: exact algorithms, approximate algorithms, and heuristics, and all of them require\nspecialized knowledge and human efforts for trail-and-error. Dai et al. (2017) propose to automate\ncombinatorial optimization using deep RL with graph embedding (Dai et al., 2016).\nWe discuss learning to plan, including, value iteration networks (VIN) (Tamar et al., 2016), and\npredictron (Silver et al., 2016b), and learning to search, MCTSnets (Guez et al., 2018), in Chapter 6.\nSee NIPS 2018 Workshop on Meta-Learning at http://metalearning.ml/2018/ .\nContinual learning (Chen and Liu, 2016; Kirkpatrick et al., 2017; Lopez-Paz and Ranzato, 2017;\nXu and Zhu, 2018) is important for achieving general intelligence. See Singh (2017) for a tutorial\nabout continual learning. See NIPS 2018 Workshop on Continual Learning at https://sites.\ngoogle.com/view/continual2018 . See ICML 2018 Workshop on Lifelong Learning: A\nReinforcement Learning Approach at https://sites.google.com/view/llarla2018/\nhome .\nWe discuss few/one/zero-shot learning in Section 14.1, transfer and multi-task learning in Sec-\ntion 14.2, learning to optimize in Section 14.3, learning reinforcement learn in Section 14.4, learning\ncombinatorial optimization in Section 14.5, and AutoML in Section 14.6.\n14.1 F EW/ONE/ZERO-SHOT LEARNING\nAs discussed in (Finn et al., 2017a), the aim of few-shot meta-learning is to train a model adaptive\nto a new task quickly, using only a few data samples and training iterations. Finn et al. (2017a)\npropose model-agnostic meta-learning (MAML) to optimize an initial representation for a learning\nalgorithm, so that the parameters can be ﬁne-tuned effectively from a few examples. To illustrate\nMAML, consider a model f\u0012with parameter \u0012. The model parameter \u0012becomes\u00120\ni, when adapting\nto a new taskTi. We compute the new parameter \u00120\niwith one gradient descent update,\n\u00120\ni=\u0012\u0000\u000br\u0012LTi(f\u0012); (59)\nor more updates, on task Ti. Here\u000bis the step size. We train model parameters by optimizing the\nmeta-objective,\nmin\n\u0012X\nTi\u0018p(T)LTi(f\u00120\ni) = min\n\u0012X\nTi\u0018p(T)LTi(f\u0012\u0000\u000br\u0012LTi(f\u0012)); (60)\nthe performance of f\u00120\ni, w.r.t.\u0012across tasks sampled from p(T). Note, the meta-optimization is\nperformed over the old parameters \u0012, and the objective is computed using the new parameters \u00120.\nThis aims to optimize the model parameters so that one or a few gradient steps on a new task will\nproduce maximally effective behaviour on that task. We perform the meta-optimization across tasks\nvia SGD, and update \u0012as,\n\u0012 \u0012\u0000\fr\u0012X\nTi\u0018p(T)LTi(f\u00120\ni); (61)\nwhere\fis the meta step size. The intuition underlying MAML is that some internal representations\nare effective for adaptation, and the goal of MAML is to ﬁnd model parameters sensitive to changes\nin the task, so that after we draw a task from p(T), when the direction of the gradient of the loss\nfunction changes, small parameter changes will signiﬁcantly improve the loss function.\nMAML works for both supervised learning and reinforcement learning. Experiments show good\nresults on few-shot classiﬁcation, regression, and policy gradients RL with neural network poli-\ncies. See a blog about learning to learn and MAML at http://bair.berkeley.edu/blog/\n2017/07/18/learning-to-learn/\nFinn and Levine (2018) show that deep representation integrated with gradient descent has sufﬁcient\ncapacity to approximate any learning algorithms, i.e., the universality of meta-learning, and show\nempirically that gradient-based meta-learning found learning strategies with better generalization\ncapacity than recurrent models. Grant et al. (2018) treat gradient-based meta-learning as hierarchical\nBayes. Finn et al. (2018) study probabilistic MAML. Yoon et al. (2018) study Bayesian MAML.\n69\n\nAl-Shedivat et al. (2018) propose to use the framework of learning to learn for continuous adapta-\ntion in non-stationary and competitive environments, by treating a non-stationary environment as a\nsequence of stationary tasks. The authors develope a gradient-based meta-learning algorithm adap-\ntive in dynamically changing and adversarial scenarios based on MAML (Finn et al., 2017a), by\nanticipating changes in environment and updating policies accordingly. The proposed approach at-\ntempt to handle Markovian dynamics on two level of hierarchy: at the upper level for dynamics of\ntasks, and at the lower level for MDPs representing particular tasks. The authors evaluate the perfor-\nmance of the proposed approach 1) on a single-agent multi-leg robots locomotion task in MuJoCo\nphysics simulator, with handcrafted nonstationarity, by selecting a pair of legs to scale down the\ntorques applied to joints, until fully paralyzed, and, 2) on iterated adaptation games in RoboSumo,\nwhich is in a 3D environment, with simulated physics, having pairs of agents to compete, and not\nonly non-stationary but also adversarial. The authors assume that trajectories from the current task\ncontain some information about the next task, so that tasks become dependant sequentially. The\nproposed algorithm may not take advantage of more data, and it could diverge when there are large\ndistributional changes from iteration to iteration.\nLake et al. (2015) propose an one-shot concept learning model, for handwritten characters in par-\nticular, with probabilistic program induction. Koch et al. (2015) propose siamese neural networks\nwith metric learning for one-shot image recognition. Vinyals et al. (2016) design matching networks\nfor one-shot classiﬁcation. Duan et al. (2017) propose a model for one-shot imitation learning with\nattention for robotics. Johnson et al. (2017) present zero-shot translation for Google’s multilingual\nneural machine translation system. Kaiser et al. (2017b) design a large scale memory module for\nlife-long one-shot learning to remember rare events. Kansky et al. (2017) propose Schema Networks\nfor zero-shot transfer with a generative causal model of intuitive physics. Snell et al. (2017) pro-\npose prototypical networks for few/zero-shot classiﬁcation by learning a metric space to compute\ndistances to prototype representations of each class. George et al. (2017) propose a generative vision\nmodel to train with high data efﬁciency, breaking text-based CAPTCHA. Liu et al. (2018c) study\ngeneralization of zero-shot learning with deep calibration network.\n14.2 T RANSFER /MULTI -TASK LEARNING\nTransfer learning is about transferring knowledge learned from different domains, possibly with\ndifferent feature spaces and/or different data distributions (Taylor and Stone, 2009; Pan and Yang,\n2010; Weiss et al., 2016). As reviewed in Pan and Yang (2010), transfer learning can be induc-\ntive, transductive, or unsupervised. Inductive transfer learning includes self-taught learning and\nmulti-task learning. Transductive transfer learning includes domain adaptation and sample selec-\ntion bias/covariance shift. Taylor and Stone (2009) compare RL transfer learning algorithms w.r.t.\nthe following performance metrics: jumpstart, asymptotic performance, total reward, transfer ratio,\ntime to threshold, and against the following dimensions: task difference assumption, source task\nselection, task mapping, transferred knowledge, and allowed learners. Multitask learning (Caruana,\n1997; Zhang et al., 2018a; Ruder, 2017) learns related tasks with a shared representation in paral-\nlel, leveraging information in related tasks as an inductive bias, to improve generalization, and to\nhelp improve learning for all tasks. The modular structure of hierarchical RL approaches is usually\nconducive to transfer and multi-task learning, which we discussed in Chapter 11.\nWhye Teh et al. (2017) propose Distral, distill & transfer learning, for joint training of multiple\ntasks, by sharing a distilled policy, trained to be the centroid of policies for all tasks, to capture\ncommon behavioural structure across tasks, and training each task policy to be close to the shared\npolicy. The design of Distral is to overcome issues in transfer and multi-task learning, that in the\napproach of share neural network parameters, gradients from different tasks may interfere each\nother negatively, and that one task may dominate the learning of the shared model, due to different\nreward functions of different tasks. Whye Teh et al. (2017) design Distral following the techniques\nof distillation (Bucila et al., 2006; Hinton et al., 2014), and soft Q-learning (Haarnoja et al., 2017;\nNachum et al., 2017), a.k.a., G-learning (Fox et al., 2016). The authors observe that, the distillation\narises naturally, when optimize task models towards a distilled model, when using KL divergence as\na regularization. Moreover, the distilled model serves as a regularizer for task models training, and it\nmay help transferability by regularizing neural networks in a space more semantically meaningful,\ne.g., for policies, rather than for network parameters. Distral can be instantiated in several forms, and\nit outperforms empirically the baseline A3C algorithms in grid world and complex 3D environments.\n70\n\nBarreto et al. (2017) propose a transfer framework for tasks with different reward functions but\nthe same environment dynamics, based on two ideas, namely, successor features and generalized\npolicy improvement. Successor features are a value function representation decoupling environ-\nment dynamics from rewards, extending the successor representation (Dayan, 1993), as discussed\nin Section 8.1, for continuous tasks with function approximation. Generalized policy improvement\noperates on multiple policies, rather than one policy as in the policy improvement in dynamic pro-\ngramming. The authors integrate these two ideas for transfer learning among tasks, and establish\ntwo theorems, one for performance guarantee of a task before any learning, and another for perfor-\nmance guarantee of a task if it had seen similar tasks. The author design a method based on these\nideas and analysis, and show good performance on navigation tasks and a task to control a simulated\nrobotic arm.\nGupta et al. (2017a) formulate the multi-skill problem for two agents to learn multiple skills, deﬁne\nthe common representation using which to map states and to project the execution of skills, and\ndesign an algorithm for two agents to transfer the informative feature space maximally to transfer\nnew skills, with similarity loss metric, autoencoder, and reinforcement learning. The authors validate\ntheir proposed approach with two simulated robotic manipulation tasks.\nAs a practical example of transfer learning in computer vision, Kornblith et al. (2018) investigate the\ntransferability of ImageNet architectures and features, for 13 classiﬁcation models on 12 image clas-\nsiﬁcation tasks, in three settings: as ﬁxed feature extractors, ﬁne-tuning, and training from random\ninitialization. The authors observe that, ImageNet classiﬁcation network architectures generalize\nwell across datasets, but ﬁxed ImageNet features do not transfer well.\nSee recent work in transfer learning/multi-task learning e.g., Andreas et al. (2017), Dong et al.\n(2015), Kaiser et al. (2017a), Kansky et al. (2017), Killian et al. (2017) Long et al. (2015), Long\net al. (2016), Long et al. (2017), Mahajan et al. (2018), Maurer et al. (2016), McCann et al. (2017),\nMo et al. (2018), Parisotto et al. (2016), Papernot et al. (2017), P ´erez-D’Arpino and Shah (2017),\nRajendran et al. (2017), Sener et al. (2018), Smith et al. (2017), Sohn et al. (2018), Yosinski et al.\n(2014), and, Zhao et al. (2018a).\nSee NIPS 2015 workshop on Transfer and Multi-Task Learning: Trends and New Perspectives at\nhttps://sites.google.com/site/tlworkshop2015/ .\nWe will discuss sim-to-real transfer learning in robotics in Chapter 16.1.\n14.3 L EARNING TO OPTIMIZE\nLi and Malik (2017) and Li and Malik (2017) propose to automate unconstrained continuous op-\ntimization algorithms with RL, in particular, guided policy search (Levine et al., 2016). Algo-\nrithm 11 presents a general structure of optimization algorithms. Li and Malik (2017) and Li and\nMalik (2017) formulate a RL problem as follows: 1) the state is the current iterate, x(i), and may\nalso include some features along the historical optimization trajectory, like the history of gradi-\nents, iterates, and objective values; 2) the action is the step vector, \u0001x, which updates the it-\neratex(i); and, 3) the policy is the update formula \u001e(\u0001), which depends on the current iterate,\nand the history of gradients, iterates, and objective values. Thus, learning the policy is equiv-\nalent to learning the optimization algorithm. One possible cost function is the objective func-\ntion value at the current iterate. A RL algorithm in this problem does not have access to the\nstate transition model. See a blog at http://bair.berkeley.edu/blog/2017/09/12/\nlearning-to-optimize-with-rl/ . See also Andrychowicz et al. (2016) and Bello et al.\n(2017).\n14.4 L EARNING REINFORCEMENT LEARN\nXu et al. (2018) investigate a fundamental problem in RL to discover an optimal form of return,\nand propose a gradient-based meta-learning algorithm to learn the return function, by tuning meta-\nparameters of the return function, e.g., the discount factor, \r, the bootstrapping parameter, \u0015, etc.,\nin an online fashion, when interacting with the environment. This is in contrast to many recent work\nin learning to learn that are in a setting of transfer/multi-task learning. Experiments on Atari games\nshow good results. The technique would be general for other components of the return function, the\nlearning update rule, and hyperparameter tunning.\n71\n\nInput: objective function f\nx(0) random point in the domain of ffori= 1;2;3;:::do\n\u0001x \u001e(f;fx(0);:::;x(i\u00001)g)ifstopping condition is met then\nreturnx(i\u00001)\nend\nx(i) x(i\u00001)+ \u0001x\nend\nAlgorithm 11: General structure of optimization algorithms, adapted from Li and Malik (2017)\nWang et al. (2018a) investigate that prefrontal cortex works as a meta-reinforcement learning\nsystem. Learning to learn, or meta-learning, is related to the phenomenon that our brain can\ndo so much with so little. A hypothesis is that we learn on two timescales: learning on\nspeciﬁc examples in short term, and learning abstract skills or rules over long term. Kah-\nneman (2011) describes two modes of thought: one, fast, instinctive and emotional; another,\nslower, more deliberative, and more logical. See a blog at https://deepmind.com/blog/\nprefrontal-cortex-meta-reinforcement-learning-system/ .\nDuan et al. (2016) and Wang et al. (2016) propose to learn a ﬂexible RNN model to handle a family\nof RL tasks, to improve sample efﬁciency, learn new tasks in a few samples, and beneﬁt from prior\nknowledge. The agent is modelled with RNN, with inputs of observations, rewards, actions and\ntermination ﬂags. The weights of RNN are trained with RL, in particular, TRPO in Duan et al.\n(2016) and A3C in Wang et al. (2016). Duan et al. (2016) and Wang et al. (2016) achieve similar\nperformance as speciﬁc RL algorithms for various problems.\nHouthooft et al. (2018) propose evolved policy gradients with meta-learning. Gupta et al. (2018)\npropose meta-RL of structured exploration strategies. Stadie et al. (2018) study the importance of\nsampling in meta-RL.\n14.5 L EARNING COMBINATORIAL OPTIMIZATION\nDai et al. (2017) propose to automate algorithm design for combinatorial optimization problems on\ngraphs using deep RL with graph embedding (Dai et al., 2016), by learning heuristics on problem\ninstances from a distribution Dof a graph optimization problem, to generalize to unseen instances\nfromD.\nThe authors propose to construct a feasible solution following a greedy algorithm design pattern, by\nsuccessively adding nodes based on the graph structure and the current partial solution. A deep graph\nembedding, structure2vec (Dai et al., 2016), is used to represent nodes, considering their properties\nin the graph, and helps generalize the algorithm to different graph sizes.\nThe graph embedding is also used to represent state, action, value function, and policy. A state is a\nsequence of nodes on a graph. An action is selecting a node that is not part of the current state. The\nreward is the change in the cost function after taking an action, i.e., adding a node, and transition to\na new state. A deterministic greedy policy is used based on the action value function. For example,\nin the Minimum Vertex Cover (MVC) problem, we are to ﬁnd a subset of nodes in a graph, so that\nevery edge is covered and the number of nodes selected is minimized. In MVC, a state is the subset\nof nodes selected so far, an action is to add a node to the current subset, the reward is -1 for each\naction, and the termination condition is all edges are covered. The authors propose to learn a greedy\npolicy parameterized by the graph embedding network using n-step ﬁtted Q-learning.\nDai et al. (2017) evaluate the proposed approach on three graph combinatorial optimization prob-\nlems: Minimum Vertex Cover (MVC), Maximum Cut (MAXCUT), and Traveling Salesman Prob-\nlem (TSP), and compare with pointer networks with actor-critic (Bello et al., 2016), and strong\nbaseline approximate and heuristics algorithms for each problem respectively. Dai et al. (2017)\nachieve good performance on both synthetic and real graphs. See the open source at https:\n//github.com/Hanjun-Dai/graph_comb_opt .\n72\n\nVinyals et al. (2015) propose pointer networks to learn the conditional probability of an output se-\nquence of discrete tokens, corresponding to positions in an input sequence, by generating output\nsequence with attention as a pointer to select an element of the input sequence, and evaluate per-\nformance on three combinatorial optimization problems, ﬁnding planar convex hulls, computing\nDelaunay triangulations, and the planar Travelling Salesman Problem. Pointer networks are graph-\nagnostic, in contrast to the graph embedding in Dai et al. (2017).\n14.6 A UTOML\nAutoML is about automating the process of machine learning. See a website for AutoML at\nhttp://automl.chalearn.org . See NIPS 2018 AutoML for Lifelong Machine Learning\nCompetition at https://www.4paradigm.com/competition/nips2018 . See also a us-\nable machine learning project Bailis et al. (2017).\nNeural network architecture design is one particular task of AutoML. Neural architecture design is\na notorious, nontrivial engineering issue. Neural architecture search provides a promising avenue to\nexplore. See a survey (Elsken et al., 2018).\nZoph and Le (2017) propose neural architecture search to generate neural networks architectures\nwith an RNN trained by RL, in particular, REINFORCE, searching from scratch in variable-length\narchitecture space, to maximize the expected accuracy of the generated architectures on a valida-\ntion set. In the RL formulation, a controller generates hyperparameters as a sequence of tokens,\nwhich are actions chosen from hyperparameters spaces. Each gradient update to the policy param-\neters corresponds to training one generated network to convergence. An accuracy on a validation\nset is the reward signal. The neural architecture search can generate convolutional layers, with skip\nconnections or branching layers, and recurrent cell architectures. The authors design a parameter\nserver approach to speed up training. Comparing with state-of-the-art methods, the proposed ap-\nproach achieves competitive results for an image classiﬁcation task with CIFAR-10 dataset, and\nbetter results for a language modeling task with Penn Treebank.\nZoph et al. (2017) propose to transfer the architectural building block learned with the neural archi-\ntecture search (Zoph and Le, 2017) on small dataset to large dataset for scalable image recognition.\nBaker et al. (2017) propose a meta-learning approach, using Q-learning with \u000f-greedy exploration\nand experience replay, to generate CNN architectures automatically for a given learning task. Zhong\net al. (2017) propose to construct network blocks to reduce the search space of network design,\ntrained by Q-learning. See also Liu et al. (2017), Liu et al. (2017), Real et al. (2017), and Real\net al. (2018). Note that Real et al. (2018) show that evolutionary approaches can match or surpass\nhuman-crafted and RL-designed image neural network classiﬁers.\nJin et al. (2018) propose a Bayesian approach for efﬁcient search. See Cai et al. (2018a) for an\napproach with limited computation resources (200 GPU hours). See also Chen et al. (2018a), Kan-\ndasamy et al. (2018), Luo et al. (2018), and Wong et al. (2018).\nThere are recent works exploring new neural architectures (manually). Vaswani et al. (2017) pro-\npose a new archichitecture for translation that replaces CNN and RNN with attention and positional\nencoding. Kaiser et al. (2017a) propose to train a single model, MultiModel, which is composed of\nconvolutional layers, an attention mechanism, and sparsely-gated layers, to learn multiple tasks from\nvarious domains, including image classiﬁcation, image captioning and machine translation. Wang\net al. (2016b) propose the dueling network architecture to estimate state value function and associ-\nated advantage function, to combine them to estimate action value function for faster convergence.\nTamar et al. (2016) introduce value iteration networks (VIN), a fully differentiable CNN planning\nmodule to approximate the value iteration algorithm, to learn to plan. Silver et al. (2016b) propose\nthe predictron to integrate learning and planning into one end-to-end training procedure with raw\ninput in Markov reward process. These neural architectures were designed manually. It would be\ninteresting to see if learning to learn can help automate such neural architecture design.\nNeural architecture design has already had industrial impact. Google, among others, is working on\nAutoML, in particular, AutoML Vision, and extends AutoML to NLP and contact center, etc. See\nblogs about Google AutoML at http://goo.gl/ijBjUr ,http://goo.gl/irCvD6 , and,\nhttp://goo.gl/VUzCNt . See Auto-Keras at https://autokeras.com .\n73\n\nThere are recent interesting work in AutoML. He et al. (2018) propose AutoML for model compres-\nsion. Cubuk et al. (2018) propose AutoAugment to automate data augmentation for images. Chen\net al. (2018c) study learning to optimize tensor programs.\nSee 2018 International Workshop on Automatic Machine Learning (collocated with the Federated\nAI Meeting, ICML, IJCAI, AMAS, and ICCBR) at https://sites.google.com/site/\nautoml2018icml/ .\nOne limitation of neural architecture design is that the network components are manually designed,\nand it is not clear if AI has the creativity to discover new components, e.g., discovering residual\nconnections before ResNets were designed.\n74\n\nPART III: A PPLICATIONS\nReinforcement learning has a wide range of applications. We discuss games in Chapter 15 and\nrobotics in Chapter 16, two classical RL application areas. Games are important testbeds for RL/AI.\nRobotics will be critical in the era of AI. Natural language processing (NLP) follows in Chapter 17,\nwhich enjoys wide and deep applications of RL recently. Next we discuss computer vision in Chap-\nter 18, in which, there are efforts for integration with language. In Chapter 19, we discuss ﬁnance\nand business management, which have natural problems for RL. We discuss more applications in\nChapter 20, healthcare in Section 20.1, education in Section 20.2. energy in Section 20.3, trans-\nportation in Section 20.4, and computer systems in Section 20.5. We attempt to put reinforcement\nlearning in the wide context of science, engineering, and art in Section 20.6,\nReinforcement learning is widely utilized in operations research (Powell, 2011), e.g., supply chain,\ninventory management, resource management, etc; we do not list it as an application area — it is\nimplicitly a component in application areas like energy and transportation. We do not list smart\ncity, an important application area of AI, as it includes several application areas here: healthcare,\neducation, energy, transportation, etc.\nThese application areas build on RL techniques as discussed in previous chapters, and may overlap\nwith each other, e.g., a robot may need skills from application areas like computer vision and NLP.\nRL is usually for sequential decision making. However, some problems, seemingly non-sequential\non surface, like neural network architecture design (Zoph and Le, 2017), and, model compres-\nsion (He et al., 2018), have been approached by RL. Creativity would push the frontiers of deep\nRL further w.r.t. core elements, important mechanisms, and applications. RL is probably helpful, if\na problem can be regarded as or transformed to a sequential decision making problem, and states, ac-\ntions, maybe rewards, can be constructed. Many problems with manual design of strategies may be\nautomated, and RL is a potential solution method. We illustrate deep RL applications in Figure 4. We\nmaintain a blog, Reinforcement Learning Applications, at https://medium.com/@yuxili/ .\ngames robotics NLPhealthcare \nbusiness \nmanagement \nscience \nengineering \nartfinance \ncomputer \nvision education energy transportation \ncomputer \nsystems deep reinforcement learning \nGo, poker \nDota, bridge \nStarcraft sim-to-real \nco-robot \ncontrol recognition \ndetection \nperception seq. gen. \ntranslation \ndialog, QA,IE,IR resource mgmt \nperf. opt. \nsecurity maths, physics \nchemistry, music \ndrawing, animation proficiency est. \nrecommendation \neducation games DTRs \ndiagnosis \nEHR/EMR \npricing, trading \nportfolio opt. \nrisk mgmt recommendation \ne-commerce, OR \ncustomer mgmt \nadaptive \ndecision \n control \nadaptive \ntraffic signal \ncontrol \nFigure 4: Deep Reinforcement Learning Applications\n75\n\n15 G AMES\nGames provide excellent testbeds for AI algorithms, dated back to the ages of Alan Turing, Claude\nShannon, and John von Neumann. In games, we have good or even perfect simulators, and we\ncan generate unlimited data. We have seen great achievements of human-level or super-human\nperformance in computer games, e.g.,\n\u000fChinook (Schaeffer, 1997; Schaeffer et al., 2007) for Checkers,\n\u000fDeep Blue (Campbell et al., 2002) for chess,\n\u000fLogistello (Buro, 1999) for Othello,\n\u000fTD-Gammon (Tesauro, 1994) for Backgammon,\n\u000fGIB (Ginsberg, 2001) for contract bridge,\n\u000fMoHex (Huang et al., 2013; Gao et al., 2017) for Hex,\n\u000fDQN (Mnih et al., 2016) and Aytar et al. (2018) for Atari 2600 games,\n\u000fAlphaGo (Silver et al., 2016a) and AlphaGo Zero (Silver et al., 2017) for Go,\n\u000fAlpha Zero (Silver et al., 2017) for chess, shogi, and Go,\n\u000fCepheus (Bowling et al., 2015), DeepStack (Morav ˇc´ık et al., 2017), and Libratus (Brown\nand Sandholm, 2017a;b) for heads-up Texas Hold’em Poker,\n\u000fJaderberg et al. (2018) for Quake III Arena Capture the Flag,\n\u000fOpenAI Five, for Dota 2 at 5v5, https://openai.com/five/ ,\n\u000fZambaldi et al. (2018), Sun et al. (2018), and Pang et al. (2018) for StarCraft II.\nSchaeffer (1997), Buro (1999), Ginsberg (2001), and Campbell et al. (2002) employ heuristic search\ntechniques (Russell and Norvig, 2009), in particular, alpha-beta search. Tesauro (1994), Mnih et al.\n(2016), Silver et al. (2016a), Silver et al. (2017), Silver et al. (2017), Jaderberg et al. (2018), OpenAI\nFive, Zambaldi et al. (2018), Sun et al. (2018), and Pang et al. (2018) are powered with (deep)\nreinforcement learning; the ”Alpha series” are also equipped with Monte Carlo tree search (MCTS),\na heuristic search technique. Aytar et al. (2018) follows a self-supervised approach, together with\ndeep RL. Huang et al. (2013) employ MCTS, and Gao et al. (2017) extend it with deep learning.\nBowling et al. (2015) and Morav ˇc´ık et al. (2017), handle imperfect information with counterfactual\nregret minimization (CFR), and follow generalized policy iteration. Brown and Sandholm (2017a;b)\nutilize classical search techniques.\nAs deep RL has achieved human-level or superhuman performance for many two-play games, multi-\nplayer games are at the frontier for scientiﬁc discovery of AI, with outstanding achievements already\nachieved for games like Quake III Arena Capture the Flag and Dota 2 5v5.\nWe discuss three categories of games, namely, board games in Section 15.1, card games in Sec-\ntion 15.2, and video games in Section 15.3, loosely for two-player perfect information zero-sum\ngames, imperfect information zero-sum games, and games with video frames as inputs, mostly with\npartial observability or imperfect information, respectively.\nThe above classiﬁcation is not exhaustive, e.g., it does not include single agent, non-board, non-\ncard, non-video games, like Rubik’s Cube (McAleer et al., 2018). Computer games have much\nwider topics, e.g., storytelling (Thue et al., 2007).\nSee Yannakakis and Togelius (2018) for a book on AI and games. See Justesen et al. (2017)\nfor a survey about applying deep (reinforcement) learning to video games. See Onta ˜n´on\net al. (2013) for a survey about Starcraft. Check AIIDE and CIG Starcraft AI Competi-\ntions, and its history at https://www.cs.mun.ca/ ˜dchurchill/starcraftaicomp/\nhistory.shtml . See Lin et al. (2017) for a StarCraft dataset.\n15.1 B OARD GAMES\nBoard games like chess, Go and, Backgammon, are classical testbeds for RL/AI algorithms. In such\ngames, players have prefect information of two players. Tesauro (1994) approach Backgammon\nusing neural networks to approximate value function learned with TD learning, and achieve human\nlevel performance.\n76\n\nCOMPUTER GO\nThe challenge of solving computer Go comes from not only the gigantic search space of size 250150,\nan astronomical number, but also the hardness of position evaluation (M ¨uller, 2002), which was\nsuccessfully used in solving many other games, like chess and Backgammon.\nAlphaGo (Silver et al., 2016a), a computer Go program, won the human European Go cham-\npion, 5 games to 0, in October 2015, and became the ﬁrst computer Go program to won a hu-\nman professional Go player without handicaps on a full-sized 19 \u000219 board. Soon after that\nin March 2016, AlphaGo defeated Lee Sedol, an 18-time world champion Go player, 4 games\nto 1, making headline news worldwide. This set a landmark in AI. AlphaGo defeated Ke Jie\n3:0 in May 2017. AlphaGo Zero (Silver et al., 2017) further improved previous versions by\nlearning a superhuman computer Go program without human knowledge. Alpha Zero (Silver\net al., 2017) generalized the learning framework in AlphaGo Zero to more domains. See blogs\nfor AlphaGo at https://deepmind.com/research/alphago/ , and for AlphaGo Zero at\nhttps://deepmind.com/blog/alphago-zero-learning-scratch/ .\nTian and Zhu (2016) also investigate computer Go. See Facebook open source ELF OpenGo,\nhttps://github.com/pytorch/ELF/ , and a blog, https://research.fb.com/\nfacebook-open-sources-elf-opengo/ .\nALPHA GO: TRAINING PIPELINE AND MCTS\nWe discuss brieﬂy how AlphaGo works based on Silver et al. (2016a) and Sutton and Barto (2018).\nSee Sutton and Barto (2018) for an intuitive description of AlphaGo.\nAlphaGo is built with techniques of deep convolutional neural networks, supervised learning, rein-\nforcement learning, and Monte Carlo tree search (MCTS) (Browne et al., 2012; Gelly and Silver,\n2007; Gelly et al., 2012). AlphaGo is composed of two phases: neural network training pipeline\nand MCTS. The training pipeline phase includes training a supervised learning (SL) policy network\nfrom expert moves, a fast rollout policy, a RL policy network, and a RL value network.\nThe SL policy network has convolutional layers, ReLU nonlinearities, and an output softmax layer\nrepresenting probability distribution over legal moves. The inputs to the CNN are 19 \u000219\u000248\nimage stacks, where 19 is the dimension of a Go board and 48 is the number of features. State-\naction pairs are sampled from expert moves to train the network with stochastic gradient ascent to\nmaximize the likelihood of the move selected in a given state. The fast rollout policy uses a linear\nsoftmax with small pattern features.\nThe RL policy network improves SL policy network, with the same network architecture, and the\nweights of SL policy network as initial weights, and policy gradient for training. The reward function\nis +1 for winning and -1 for losing in the terminal states, and 0 otherwise. Games are played between\nthe current policy network and a random, previous iteration of the policy network, to stabilize the\nlearning and to avoid overﬁtting. Weights are updated by stochastic gradient ascent to maximize the\nexpected outcome.\nThe RL value network still has the same network architecture as SL policy network, except the out-\nput is a single scalar predicting the value of a position. The value network is learned in a Monte\nCarlo policy evaluation approach. To tackle the overﬁtting problem caused by strongly correlated\nsuccessive positions in games, data are generated by self-play between the RL policy network and\nitself until game termination. The weights are trained by regression on state-outcome pairs, us-\ning stochastic gradient descent to minimize the mean squared error between the prediction and the\ncorresponding outcome.\nIn MCTS phase, AlphaGo selects moves by a lookahead search. It builds a partial game tree starting\nfrom the current state, in the following stages: 1) select a promising node to explore further, 2)\nexpand a leaf node guided by the SL policy network and collected statistics, 3) evaluate a leaf node\nwith a mixture of the RL value network and the rollout policy, 4) backup evaluations to update the\naction values. A move is then selected.\n77\n\nALPHA GOZERO\nAlphaGo Zero can be understood as following a generalized policy iteration scheme, incorporating\nMCTS inside the training loop to perform both policy improvement and policy evaluation. MCTS\nmay be regarded as a policy improvement operator. It outputs move probabilities stronger than raw\nprobabilities of the neural network. Self-play with search may be regarded as a policy evaluation\noperator. It uses MCTS to select moves, and game winners as samples of value function. Then\nthe policy iteration procedure updates the neural network’s weights to match the move probabilities\nand value more closely with the improved search probabilities and self-play winner, and conduct\nself-play with updated neural network weights in the next iteration to make the search stronger.\nThe features of AlphaGo Zero (Silver et al., 2017), comparing with AlphaGo (Silver et al., 2016a),\nare: 1) it learns from random play, with self-play RL, without human data or supervision; 2) it uses\nblack and white stones from the board as input, without any manual feature engineering; 3) it uses\na single neural network to represent both policy and value, rather than separate policy network and\nvalue network; and 4) it utilizes the neural network for position evaluation and move sampling for\nMCTS, and it does not perform Monte Carlo rollouts. AlphaGo Zero deploys several recent achieve-\nments in neural networks: residual convolutional neural networks (ResNets), batch normalization,\nand rectiﬁer nonlinearities.\nAlphaGo Zero has three main components in its self-play training pipeline executed in parallel asyn-\nchronously: 1) optimize neural network weights from recent self-play data continually; 2) evaluate\nplayers continually; 3) use the strongest player to generate new self-play data.\nWhen AlphaGo Zero playing a game against an opponent, MCTS searches from the current state,\nwith the trained neural network weights, to generate move probabilities, and then selects a move.\nWe present a brief, conceptual pseudo code in Algorithm 12 for training in AlphaGo Zero, conducive\nfor easier understanding.\nALPHA ZERO\nAlpha Zero (Silver et al., 2017) generalizes the learning framework in AlphaGo Zero to more do-\nmains, in particular, perfect information two-player zero-sum games, with a general reinforcement\nlearning algorithm, learning with no human knowledge except the game rules, and achieves super-\nhuman performance for the games of chess, shogi, and Go.\nDISCUSSIONS\nAlphaGo Zero is a reinforcement learning algorithm. It is neither supervised learning nor unsu-\npervised learning. The game score is a reward signal, not a supervision label. Optimizing the loss\nfunctionlis supervised learning. However, it performs policy evaluation and policy improvement,\nas one iteration in generalized policy iteration.\nAlphaGo Zero is not only a heuristic search algorithm. AlphaGo Zero follows a generalized policy\niteration procedure, in which, heuristic search, in particular, MCTS, plays a critical role, but within\nthe scheme of reinforcement learning generalized policy iteration, as illustrated in the pseudo code\nin Algorithm 12. MCTS can be viewed as a policy improvement operator.\nAlphaGo Zero has attained a superhuman level perfromance. It may conﬁrm that human profes-\nsionals have developed effective strategies. However, it does not need to mimic human professional\nplays. Thus it does not need to predict their moves correctly.\nThe inputs to AlphaGo Zero include the raw board representation of the position, its history, and the\ncolour to play as 19 \u000219 images; game rules; a game scoring function; invariance of game rules\nunder rotation and reﬂection, and invariance to colour transposition except for komi.\nAlphaGo Zero utilizes 64 GPU workers and 19 CPU parameter servers for training, around 2000\nTPUs for data generation, and 4 TPUs for game playing. The computation cost is probably\ntoo formidable for researchers with average computation resources to replicate AlphaGo Zero.\nELF OpenGo is a reimplementation of AlphaGoZero/AlphaZero using ELF (Tian et al., 2017), at\nhttps://facebook.ai/developers/tools/elf .\n78\n\nInput: the raw board representation of the position, its history, and the colour to play as 19 \u000219\nimages; game rules; a game scoring function; invariance of game rules under rotation and\nreﬂection, and invariance to colour transposition except for komi\nOutput: policy (move probabilities) p, valuev\ninitialize neural network weights \u00120randomly\n//AlphaGo Zero follows a generalized policy iteration procedure\nforeach iteration ido\n// termination conditions:\n// 1. both players pass\n// 2. the search value drops below a resignation threshold\n// 3. the game exceeds a maximum length\ninitializes0\nforeach stept, until termination at step Tdo\n// MCTS can be viewed as a policy improvement operator\n// search algorithm: asynchronous policy and value MCTS algorithm (APV-MCTS)\n// execute an MCTS search \u0019t=\u000b\u0012i\u00001(st)with previous neural network f\u0012i\u00001\n// each edge (s;a)in the search tree stores a prior probability P(s;a), a visit count N(s;a),\n// and an action value Q(s;a)\nwhile computational resource remains do\nselect: each simulation traverses the tree by selecting the edge with maximum upper\nconﬁdence bound Q(s;a) +U(s;a), whereU(s;a)/P(s;a)=(1 +N(s;a))\nexpand and evaluate: the leaf node is expanded and the associated position sis\nevaluated by the neural network, (P(s;\u0001);V(s)) =f\u0012i(s); the vector of Pvalues are\nstored in the outgoing edges from s\nbackup: each edge (s;a)traversed in the simulation is updated to increment its visit\ncountN(s;a), and to update its action value to the mean evaluation over these\nsimulations, Q(s;a) = 1=N(s;a)P\ns0js;a!s0V(s0), wheres0js;a!s0indicates that\na simulation eventually reached s0after taking move afrom position s\nend\n// self-play with search can be viewed as a policy evaluation operator: select each move\nwith the improved MCTS-based policy, use the game winner as a sample of the value\nplay: once the search is complete, search probabilities \u0019/N1=\u001care returned, where Nis\nthe visit count of each move from root and \u001cis a parameter controlling temperature; play\na move by sampling the search probabilities \u0019t, transition to next state st+1\nend\nscore the game to give a ﬁnal reward rT2f\u0000 1;+1g\nforeach steptin the last game do\nzt \u0006rT, the game winner from the perspective of the current player\nstore data as (st;\u0019t;zt)\nend\nsample data (s;\u0019;z )uniformly among all time-steps of the last iteration(s) of self-play\n//train neural network weights \u0012i\n//optimizing loss function lperforms both policy evaluation, via (z\u0000v)2, and policy\nimprovement, via\u0000\u0019Tlogp, in a single step\nadjust the neural network (p;v) =f\u0012i(s):\nto minimize the error between the predicted value vand the self-play winner z, and\nto maximize similarity of neural network move probabilities pto search probabilities \u0019\nspeciﬁcally, adjust the parameters \u0012by gradient descent on a loss function\n(p;v) =f\u0012i(s)andl= (z\u0000v)2\u0000\u0019Tlogp+ck\u0012ik2\nlsums over the mean-squared error and cross-entropy losses, respectively\ncis a parameter controlling the level of L2weight regularization to prevent overﬁtting\nevaluate the checkpoint every 1000 training steps to decide if replacing the current best player\n(neural network weights) for generating next batch of self-play games\nend\nAlgorithm 12: AlphaGo Zero training pseudo code, based on Silver et al. (2017)\n79\n\nAlphaGo Zero requires huge amount of data for training, so it is still a big data issue. However, the\ndata can be generated by self play, with a perfect model or precise game rules.\nDue to the perfect model or precise game rules for computer Go, AlphaGo algorithms have their\nlimitations. For example, in healthcare, robotics and self driving problems, it is usually hard to\ncollect a large amount of data, and it is hard or impossible to have a close enough or even perfect\nmodel. As such, it is nontrivial to directly apply AlphaGo Zero algorithms to such applications.\nOn the other hand, AlphaGo algorithms, especially, the underlying techniques, namely, deep learn-\ning, reinforcement learning, Monte Carlo tree search, and self-play, have many applications. Silver\net al. (2016a) and Silver et al. (2017) recommend the following applications: general game-playing\n(in particular, video games), classical planning, partially observed planning, scheduling, constraint\nsatisfaction, robotics, industrial control, and online recommendation systems. AlphaGo Zero\nblog at https://deepmind.com/blog/alphago-zero-learning-scratch/ men-\ntions the following structured problems: protein folding, reducing energy consumption, and search-\ning for revolutionary new materials.7Several of these, like planning, scheduling, constraint satis-\nfaction, are constraint programming problems (Rossi et al., 2006).\nAlphaGo has made tremendous progress, and sets a landmark in AI. However, we are still far away\nfrom attaining artiﬁcial general intelligence (AGI).\nIt is interesting to see how strong a deep neural network in AlphaGo can become, i.e., to approximate\noptimal value function and policy, and how soon a very strong computer Go program would be\navailable on a mobile phone.\n15.2 C ARD GAMES\nVariants of card games, including Texas Hold’em Poker, majiang/mahjong, Skat, etc., are imperfect\ninformation games, which, as one type of game theory problems, have many applications, e.g.,\nsecurity and medical decision support (Chen and Bowling, 2012). It is interesting to see more\nprogress of deep RL in such applications, and the full version of Texas Hold’em.\nHeads-up Limit Hold’em Poker is essentially solved (Bowling et al., 2015) with counterfactual re-\ngret minimization (CFR), which is an iterative method to approximate a Nash equilibrium of an\nextensive-form game with repeated self-play between two regret-minimizing algorithms.\nDEEPSTACK\nRecently, signiﬁcant progress has been made for Heads-up No-Limit Hold’em Poker (Morav ˇc´ık\net al., 2017), the DeepStack computer program defeated professional poker players for the ﬁrst\ntime. DeepStack utilizes the recursive reasoning of CFR to handle information asymmetry, focusing\ncomputation on speciﬁc situations arising when making decisions and use of value functions trained\nautomatically, with little domain knowledge or human expert games, without abstraction and ofﬂine\ncomputation of complete strategies as before. DeepStack follows generalized policy iteration. Watch\na talk by Michael Bowling at https://vimeo.com/212288252 .\nIt is desirable to see extension of DeepStack to multi-player settings. The current study of Poker\nlimits to a single hand, while there are usually many hands in Poker. It is thus desirable to investigate\n7There is a blog titled ”AlphaGo, in context” in May 2017 by Andrej Karpathy, after AlphaGo defeated\nKe Jie, at https://medium.com/@karpathy/alphago-in-context-c47718cb95a5 . The au-\nthor characterizes properties of Computer Go as: fully deterministic, fully observable, discrete action space,\naccessible perfect simulator, relatively short episode/game, clear and fast evaluation conducive for many trail-\nand-errors, and huge datasets of human play games, to illustrate the narrowness of AlphaGo. (AlphaGo Zero\ninvalidates the last property, ”huge datasets of human play games”.) It is true that computer Go has limita-\ntions in the problem setting and thus for potential applications, and is far from artiﬁcial general intelligence.\nHowever, we see the success of AlphaGo, in particular, AlphaGo Zero, as the triumph of AI, in particular, the\nunderlying techniques, i.e., deep learning, reinforcement learning, Monte Carlo tree search, and self-play; these\ntechniques are present in many recent achievements in AI. AlphaGo techniques will shed light on classical AI\nareas, like planning, scheduling, and constraint satisfaction (Silver et al., 2016a), and new areas for AI, like ret-\nrosynthesis (Segler et al., 2018). Reportedly, the success of AlphaGo’s conquering titanic search space inspired\nquantum physicists to solve the quantum many-body problem (Carleo and Troyer, 2017).\n80\n\nsequential decision making in cases of multiple hands, and probably treating tournaments and cash\ngames differently.\n15.3 V IDEO GAMES\nVideo games are those with video frames as inputs to RL/AI agents. In video games, information\nmay be perfect or imperfect, and game theory may be deployed or not.\nWe discuss algorithms for Atari 2600 games, in particular, DQN and its extensions, mostly in Chap-\nter 3, when we talk about value-based methods. We discuss algorithms for StarCraft in Chapter 12,\nwhen we talk about multi-agent RL. We discuss Zambaldi et al. (2018), which investigated StarCraft\nII mini-games with relational deep RL, in Chapter 13. Sun et al. (2018) and Pang et al. (2018) have\nbeaten full-game built-in AI in StarCraft II. We discuss Jaderberg et al. (2018), a human-level agent\nfor Quake III Arena Capture the Flag in Chapter 12. We discuss Mnih et al. (2016) in Section 4.2,\nand Jaderberg et al. (2017) and Mirowski et al. (2017) in Section 10, which use Labyrinth as the\ntestbed. Oh et al. (2016) and Tessler et al. (2017) study Minecraft. Chen and Yi (2017) and Firoiu\net al. (2017) study Super Smash Bros.\nWu and Tian (2017) deploy A3C to train an agent in a partially observable 3D environment, Doom,\nfrom recent four raw frames and game variables, to predict next action and value function, following\nthe curriculum learning (Bengio et al., 2009) approach of starting with simple tasks and gradually\ntransition to harder ones. It is nontrivial to apply A3C to such 3D games directly, partly due to\nsparse and long term reward. The authors won the champion in Track 1 of ViZDoom Competition\nby a large margin. Dosovitskiy and Koltun (2017) approach the problem of sensorimotor control in\nimmersive environments with supervised learning. Lample and Chaplot (2017) also study Doom.\n81\n\n16 R OBOTICS\nRobotics is a classical application area for reinforcement learning. Robots have wide applications,\ne.g., manufacture, supply chain, healthcare, etc.\nRobotics pose challenges to reinforcement learning, including dimensionality, real world examples,\nunder-modelling (models not capturing all details of system dynamics) and model uncertainty, and,\nreward and goal speciﬁcation (Kober et al., 2013). RL provides tractable approaches to robotics,\nthrough representation, including state-action discretization, value function approximation, and pre-\nstructured policies; through prior knowledge, including demonstration, task structuring, and direct-\ning exploration; and through models, including mental rehearsal, which deals with simulation bias,\nreal world stochasticity, and optimization efﬁciency with simulation samples, and approaches for\nlearned forward models (Kober et al., 2013).\nWatch Abbeel (2017a) for a talk on deep learning for robotics. See Kober et al. (2013) for a sur-\nvey of RL in robotics, Deisenroth et al. (2013) for a survey on policy search for robotics, and\nArgall et al. (2009) for a survey of robot learning from demonstration. See the journal Science\nRobotics. It is interesting to note that from NIPS 2016 Invited Talk titled Dynamic Legged Robots\nby Marc Raibert, Boston Dynamics robots did not use machine learning. See NIPS 2018 Work-\nshop on Imitation Learning and its Challenges in Robotics at https://sites.google.com/\nview/nips18-ilr .\nNg et al. (2004) study autonomous helicopter. Reddy et al. (2018) study glider soaring. Mirowski\net al. (2017), Banino et al. (2018), and Wayne et al. (2018) propose methods for learning to navigate.\nLiu and Tomizuka (2016; 2017) study how to make robots and people to collaborate to achieve both\nﬂexibility and productivity in production lines. See a blog at http://bair.berkeley.edu/\nblog/2017/12/12/corobots/ .\nIn the following, we discuss sim-to-real, imitation learning, value-based learning, policy-based\nlearning, and model-based learning for robotics. Then we discuss autonomous driving vehicles,\na special type of robots.\n16.1 S IM-TO-REAL\nIt is easier to train a robot in simulation than in reality. Most RL algorithms are sample intensive.\nAnd exploration may cause risky policies to the robot and/or the environment. However, a simulator\nusually can not precisely reﬂect the reality. How to bridge the gap between simulation and reality,\ni.e., sim-to-real, is critical and challenging in robotics. Sim-to-real is a special type of transfer\nlearning, as discussed in Section 14.2.\nPeng et al. (2017c) propose to use dynamics randomization to train recurrent policies in simula-\ntion, and deploy the policies directly on a physical robot, achieving good performance on an object\npushing task, without calibration. This work does not consider visual observation.\nOpenAI (2018) propose to learn dexterity of in-hand manipulation to perform object reorientation\nfor a physical Shadow Dexterous Hand, using Proximal Policy Optimization (PPO), with dynamics\nrandomization in simulation, and transferring the learned policy directly to physical hand, sharing\ncode base with OpenAI Five for playing Dota 2. See a blog with video at https://blog.\nopenai.com/learning-dexterity/ . Chen et al. (2018b), Popov et al. (2017) and P ´erez-\nD’Arpino and Shah (2017) also study dexterity learning. See also https://bair.berkeley.\nedu/blog/2018/08/31/dexterous-manip/ .\nRusu et al. (2017) propose to use progressive networks (Rusu et al., 2016) to bridge the reality gap.\nIn progressive networks, lateral connections connect layers of network columns learned previously\nto each new column, to support compositionality, and to support transfer learning and domain adap-\ntation. In a progressive network, columns may not have the same capacity or structure, so that the\ncolumn for simulation can have sufﬁcient capacity, and the column for reality may have lower ca-\npacity, which can be initialized from the column trained with simulation, to encourage exploration\nand fast learning from scarce real data. Rusu et al. (2017) use MuJoCo physics simulator to train the\nﬁrst column, for a reaching task with the modelled Jaco; and use real Jaco to train the second column\nwith RGB images, to be deployed on a real robot. The authors also propose to handle dynamic tasks,\n82\n\ne.g., dynamic targets, by adding a third column and proprioceptive features, i.e., features for joint\nangles and velocities for arms and ﬁngers.\nSadeghi et al. (2018) propose a convolutional recurrent neural network to learn perception and con-\ntrol for servoing a robot arm to desired objects, invariant to viewpoints. The proposed method uses\nthe memory of past movements to select actions to reach the target, rather than assuming known dy-\nnamics or requiring calibration. The authors propose to learn the controller with simulated demon-\nstration trajectories and RL. The supervised demonstration learning usually learns a myopic policy,\nwith distance to goal as the objective; and RL helps learn a policy consider long term effect, by eval-\nuating the action value function to assess if the goal can be reached. The visual layers are adapted\nwith a small amount of realistic images for better transfer performance. The work assumes that the\nrobot can move to an object directly, without planning for a sophisticated policy, e.g., with obstacles.\nSee also Bousmalis et al. (2017), and a blog, https://research.googleblog.com/2017/\n10/closing-simulation-to-reality-gap-for.html\n16.2 I MITATION LEARNING\nFinn et al. (2016b) study inverse optimal cost, or inverse reinforcement learning in control. The\nauthors propose to utilize nonlinear cost functions, such as neural networks, to impose structures\non the cost for informative features and effective regularization, and approximate MaxEnt (Ziebart\net al., 2008) with samples for learning with unknown dynamics in high-dimensional continuous\nenvironments.\nDuan et al. (2017) propose one-shot imitation learning, in the setting with many tasks, as supervised\nlearning. The authors train a neural network with pairs of demonstrations for a subset of tasks, with\ninput as the ﬁrst demonstration and a state sampled from the second demonstration, and predicted\nthe action for the sampled state. The authors utilize the soft attention to process sequence of states\nand actions of a demonstration, and vector components for block locations, as in the block stacking\nexperiments, for better generalization to unseen conditions and tasks in training data. See videos at\nhttps://sites.google.com/view/nips2017-one-shot-imitation/ .\nFinn et al. (2017c) and Yu et al. (2018) propose one-shot meta-imitation learning methods to build\nvision-based policies ﬁne-tuned end-to-end from one demonstration, using model-agnostic meta-\nlearning (MAML) (Finn et al., 2017a) for pre-training on a diverse range of demonstrations from\nother environments. Usually learning from raw pixels requires a large amount of data. MAML\noptimizes an initial representation for a learning algorithm, to build a rich prior in the meta-learning\nphase, allowing the parameters to adapt to new tasks with a few examples. We discuss MAML in\nSection 14.1. In Finn et al. (2017c), demonstrations come from a teleoperated robot. In Yu et al.\n(2018), demonstrations come from a human, posing a challenging issue of domain shift. Yu et al.\n(2018) learn how to learn from both human and teleoperated demonstrations, and could adapt to a\nnew task with one human demonstration. Both Finn et al. (2017c) and Yu et al. (2018) experiment\nwith both simulation and physical robots. See a blog at http://bair.berkeley.edu/blog/\n2018/06/28/daml/ . The open source for Finn et al. (2017c) is at https://github.com/\ntianheyu927/mil .\n16.3 V ALUE -BASED LEARNING\nHere we discuss two papers using successor representation (SR). We introduce that a value function\ncan be decomposed into environment dynamics (SR) and reward signal in Chapter 8. We discuss\ngeneral value function (GVF) in Section 3.3, e.g., Sutton et al. (2011), universal function approx-\nimators (UVFAs) (Schaul et al., 2015), and, hindsight experience replay (HER) (Andrychowicz\net al., 2017). Sutton et al. (2011) and Andrychowicz et al. (2017) experiment with robots. Barreto\net al. (2017) extend successor representation to successor features for continuous tasks with function\napproximation, as discussed in Section 14.2.\nZhang et al. (2017) propose a deep RL approach with successor features for robot navigation tasks,\nfor transferring knowledge obtained from previous navigation tasks to new ones, without localiza-\ntion, mapping or planning. The authors experiment with both simulation and physical robots.\n83\n\nSherstan et al. (2018) propose to accelerate construction of knowledge represented by GVFs with\nsuccessor representation in a continual learning setting, so that learning new GVFs are sped up with\nprevious learned GVFs.\n16.4 P OLICY -BASED LEARNING\nLevine et al. (2016) propose to train perception and control systems jointly end-to-end, to map raw\nimage observations directly to torques at the robot’s motors. The authors introduce guided policy\nsearch (GPS) to train policies represented by CNNs, by transforming policy search into supervised\nlearning to achieve data efﬁciency, with training data provided by a trajectory-centric RL method\noperating under unknown dynamics. GPS alternates between trajectory-centric RL and supervised\nlearning, to obtain the training data coming from the policy’s own state distribution, to address\nthe issue that supervised learning usually does not achieve good, long-horizon performance. GPS\nutilizes pre-training to reduce the amount of experience data to train visuomotor policies. Good\nperformance is achieved on a range of real-world manipulation tasks requiring localization, visual\ntracking, and handling complex contact dynamics, and simulated comparisons with previous policy\nsearch methods. As the authors mention, ”this is the ﬁrst method that can train deep visuomotor\npolicies for complex, high-dimensional manipulation skills with direct torque control”.\nYahya et al. (2017) propose a distributed and asynchronous guided policy search for a vision-based\ndoor opening task with four robots.\nZhu et al. (2017b) propose target-driven visual navigation in indoor scenes with deep RL by treating\nthe policy as a function of both the goal and the current state in an actor-critic model, to gener-\nalize over targets, similar to general value function. The authors design the house of interactions\n(AI2-THOR), a simulation framework with high-quality 3D scenes and physics engine, which al-\nlows visual interactions for agents, to generate large amount of training data. The authors qualita-\ntively compare AI2-THOR with other simulators, like ALE, VizDoom, UETorch, Project Malmo,\nSceneNet, TORCS, SYTHNIA, and, Virtual KITTI.\n16.5 M ODEL -BASED LEARNING\nGu et al. (2016) propose normalized advantage functions (NAF) to enable experience replay with\nQ-learning for continuous task, and to reﬁt local linear models iteratively. Gu et al. (2017a) propose\nasynchronous NAF algorithm, with safety constraints, for 3D manipulation in simulation and door\nopening for real robots.\nFinn and Levine (2017) propose to combine model predictive control (MPC) with action-conditioned\nvideo prediction, in a self-supervised approach without labeled training data, for physical robotic\nmanipulation of previously unseen objects.\nChebotar et al. (2017) focus on time-varying linear-Gaussian policies, and integrated a model-based\nlinear quadratic regulator (LQR) algorithm with a model-free path integral policy improvement algo-\nrithm. To generalize the method for arbitrary parameterized policies such as deep neural networks,\nthe authors combined the proposed approach with guided policy search (GPS) (Levine et al., 2016).\nLee et al. (2017) study visual servoing by combining features learned from object classiﬁcation,\npredictive dynamic models, and ﬁtted Q-iteration algorithm.\n16.6 A UTONOMOUS DRIVING VEHICLES\nAutonomous driving is an important topic of intelligent transportation systems as we discuss\nin Chapter 20.4. O’Kelly et al. (2018) propose to test autonomous vehicles rare-event simu-\nlation. Fridman et al. (2018) propose DeepTrafﬁc, a micro-trafﬁc simulator, for deep RL. Yu\net al. (2018) release BDD100K, a large-scale diverse driving video database. The data is avail-\nable at, http://bdd-data.berkeley.edu . The blog is at, http://bair.berkeley.\nedu/blog/2018/05/30/bdd/ . See also Bojarski et al. (2016), Bojarski et al. (2017), Zhou\nand Tuzel (2018). See a website for Tesla Autopilot Miles at, https://hcai.mit.edu/\ntesla-autopilot-miles/ .\n84\n\nWe argue that the current science, engineering, and technology, including AI, are not ready\nfor road test of fully autonomous driving vehicles yet. One issue is adversarial exam-\nples, as we discuss in Section 20.5. From the Insurance Institute for Highway Safety\n(IIHS) website, https://www.iihs.org/iihs/topics/t/general-statistics/\nfatalityfacts/state-by-state-overview , we learn that human drivers in US en-\ncounter roughly one death per 100 million vehicle miles. Consider, for an autonomous system,\nhow many decisions to make, and what level of accuracy are required, for just one second. Simula-\ntors are probably far from reality; and road test data collected so far are far from for justiﬁcation of\nstatistically signiﬁcant level claims. Road tests are actually experiments, with humans involved. For\na ”self-driving vehicle” to run on roads, a feasible approach is to require a driver to sit on the driver’s\nseat, pay attention to the driving, and take control of the vehicle at any time if not always. For fully\nself driving vehicles, we propose to conduct ”road tests” in a closed environment, using robots as\npedestrians, etc., until AI has sufﬁcient intelligence, e.g., understanding scenes, with common sense,\netc.\n85\n\n17 N ATURAL LANGUAGE PROCESSING\nNatural language processing (NLP) learns, understands, and produces human language content using\ncomputational techniques (Hirschberg and Manning, 2015).\nThere are many interesting topics in NLP. we discuss sequence generation in Section 17.1, machine\ntranslation in Section 17.2, and, dialogue systems in Section 17.3. We list some NLP topics in the\nfollowing. We also list the integration of computer vision with NLP, like visual captioning, visual\ndialog, and visual relationship and attribute detection, which we discuss in Section 18.4.\n\u000flanguage tree-structure learning, e.g., Socher et al. (2011; 2013); Yogatama et al. (2017);\n\u000fsemantic parsing, e.g., Liang et al. (2017b);\n\u000fquestion answering, e.g., Shen et al. (2017), Trischler et al. (2016), Xiong et al. (2017a),\nWang et al. (2017b), Choi et al. (2017)\n\u000fsummarization, e.g., Chopra et al. (2016); Paulus et al. (2017); Zhang and Lapata (2017)\n\u000fsentiment analysis (Liu, 2012; Zhang et al., 2018)\n\u000finformation retrieval (Manning et al., 2008), e.g., and Mitra and Craswell (2017), Wang\net al. (2017a), Zhang et al. (2016a)\n\u000finformation extraction, e.g., Narasimhan et al. (2016);\n\u000fvisual captioning, e.g., Wang et al. (2018e); Xu et al. (2015); Liu et al. (2016); Lu et al.\n(2016); Ren et al. (2017); Pasunuru and Bansal (2017); Wang et al. (2018f);\n\u000fvisual dialog, e.g., Das et al. (2017); Strub et al. (2017);\n\u000fvisual relationship and attribute detection, e.g., Liang et al. (2017c);\n\u000fvisual question answering, e.g., Hudson and Manning (2018)\n\u000fpopular Reddit threads prediction, e.g., He et al. (2016c)\n\u000fautomatic query reformulation, e.g., Nogueira and Cho (2017);\n\u000flanguage to executable program, e.g., Guu et al. (2017);\n\u000fknowledge graph reasoning, e.g., Xiong et al. (2017c);\n\u000ftext games, e.g., Wang et al. (2016a), He et al. (2016b), Narasimhan et al. (2015);\n\u000fsemi-supervised learning, co-training, e.g., Wu et al. (2018).\nDeep learning has been permeating into many subareas in NLP, and helping make signiﬁcant\nprogress. It appears that NLP is still a ﬁeld more about synergy than competition, for deep learn-\ning vs. non-deep learning algorithms, and for approaches based on no domain knowledge vs. with\nlinguistics knowledge. Some non-deep learning algorithms are effective and perform well, e.g.,\nword2vec (Mikolov et al., 2013; Mikolov et al., 2017) and fastText (Joulin et al., 2017), and many\npapers study syntax and semantics of languages, with a recent example in semantic role labeling (He\net al., 2017). Some deep learning approaches to NLP problems incorporate explicitly or implicitly\nlinguistics knowledge, e.g., Socher et al. (2011; 2013); Yogatama et al. (2017). Manning (2017)\ndiscusses computational linguistics and deep learning. See ACL 2018 Workshop on Relevance of\nLinguistic Structure in Neural NLP, at https://sites.google.com/view/relsnnlp .\nMcCann et al. (2018) propose natural language decathlon (decaNLP), an NLP benchmark suitable\nfor multitask, transfer, and continual learning. See the website, http://decanlp.com . De-\nvlin et al. (2018) propose Bidirectional Encoder Representations from Transformers (BERT) for\nlanguage representation model pre-training.\nMelis et al. (2018) investigate the evaluation in neural language models, and observe that standard\nLSTM outperforms recent models. Bai et al. (2018) show empirically that CNNs outperforms RNNs\nover a wide range of tasks.\nSee Jurafsky and Martin (2017), Goldberg (2017), Deng and Liu (2018) for books on NLP;\nHirschberg and Manning (2015); Cho (2015); Young et al. (2017) for surveys on NLP; Deng and Li\n(2013); Gao et al. (2018a); Hinton et al. (2012); He and Deng (2013); Young et al. (2013) for surveys\n86\n\non dialogue systems; Neubig (2017) for a tutorial on neural machine translation and sequence-to-\nsequence models, Agarwal et al. (2018) for a tutorial on end to-end goal-oriented question answering\nsystems, and, Monroe (2017) for a gentle introduction to translation.\nSee three ACL 2018 tutorials: Wang et al. (2018d) for deep reinforcement learning for NLP, Gao\net al. (2018b) for neural approaches to conversational AI (see also Gao et al. (2018a)), and, Anderson\net al. (2018) for connecting language and vision to actions.\nSee several workshops: NIPS Workshop on Conversational AI: Today’s Practice and\nTomorrow’s Potential, in 2018 at http://alborz-geramifard.com/workshops/\nnips18-Conversational-AI/ , and, in 2017 at http://alborz-geramifard.com/\nworkshops/nips17-Conversational-AI/ ; NIPS 2018 Workshop on Wordplay: Re-\ninforcement and Language Learning in Text-based Games; NIPS 2016 Workshop on End-to-\nend Learning for Speech and Audio Processing, at https://sites.google.com/site/\nnips2016endtoendspeechaudio/ ; and, NIPS 2015 Workshop on Machine Learning for\nSpoken Language Understanding and Interactions, at http://slunips2015.wixsite.com/\nslunips2015 .\n17.1 S EQUENCE GENERATION\nA sequence may take the form of text, music, and molecule, etc. Sequence generation techniques\nmay be applicable to multiple domains, e.g., Jaques et al. (2017) experiment with musical melody\nand computational molecular generation. Here we focus on text generation, which is the basis\nfor many NLP problems, like conversational response generation, machine translation, abstractive\nsummarization, etc.\nText generation models are usually based on n-gram, feed-forward neural networks, or recurrent\nneural networks, trained to predict next word given the previous ground truth words as inputs; then\nin testing, the trained models are used to generate a sequence word by word, using the generated\nwords as inputs. The errors will accumulate on the way, causing the exposure bias issue. Moreover,\nthese models are trained with word level losses, e.g., cross entropy, to maximize the probability of\nnext word; however, the models are evaluated on different metrics like BLEU.\nRanzato et al. (2016) propose mixed incremental cross-entropy reinforce (MIXER) for sequence\nprediction, with incremental learning and a loss function combining both REINFORCE and cross-\nentropy. MIXER is a sequence level training algorithm, aligning training and testing objective, such\nas BLEU, rather than predicting the next word as in previous papers.\nBahdanau et al. (2017) propose an actor-critic algorithm for sequence prediction, to improve Ran-\nzato et al. (2016). The authors utilize a critic network to predict the value of a token, i.e., the\nexpected score following the sequence prediction policy, deﬁned by an actor network, trained by the\npredicted value of tokens. Some techniques are deployed to improve performance: SARSA rather\nthan Monter-Carlo method to lessen the variance in estimating value functions; target network for\nstability; sampling prediction from a delayed actor whose weights are updated more slowly than the\nactor to be trained, to avoid the feedback loop when actor and critic need to be trained based on the\noutput of each other; and, reward shaping to avoid the issue of sparse training signal.\nYu et al. (2017) propose SeqGAN, sequence generative adversarial nets with policy gradient, inte-\ngrating the adversarial scheme in Goodfellow et al. (2014).\n17.2 M ACHINE TRANSLATION\nNeural machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al.,\n2014; Bahdanau et al., 2015) utilizes end-to-end deep learning for machine translation, and becomes\ndominant, against the traditional statistical machine translation techniques. In sequence to sequence\nmodel (Cho et al., 2014; Sutskever et al., 2014), an input sequence of symbol representation is\nencoded to a ﬁx-length vector, which is then decoded to symbols one by one, in an auto-regressive\nway, using symbols generated previously as additional input. Bahdanau et al. (2015) introduce a\nsoft-attention technique to address the issues with encoding the whole sentence into a ﬁx-length\nvector.\n87\n\nHe et al. (2016a) propose dual learning mechanism to tackle the data hunger issue in machine trans-\nlation, inspired by the observation that the information feedback between the primal, translation\nfrom language A to language B, and the dual, translation from B to A, can help improve both trans-\nlation models, with a policy gradient method, using the language model likelihood as the reward\nsignal. Experiments show, with only 10% bilingual data for warm start and monolingual data, the\ndual learning approach perform comparably with previous neural machine translation methods with\nfull bilingual data in English to French tasks. The dual learning mechanism may be extended to\nother tasks, if a task has a dual form, e.g., speech recognition and text to speech, image caption and\nimage generation, question answering and question generation, search and keyword extraction, etc.\nXia et al. (2018) study model-level dual learning.\nSee Wu et al. (2016), Johnson et al. (2017) for Google’s Neural Machine Translation System,\nGehring et al. (2017) for convolutional sequence to sequence learning for fast neural machine trans-\nlation, Klein et al. (2017) for OpenNMT, an open source neural machine translation system, at\nhttp://opennmt.net , Cheng et al. (2016) for semi-supervised learning for neural machine\ntranslation, Wu et al. (2017c) for adversarial neural machine translation, Vaswani et al. (2017) for\na new approach for translation replacing ConvNets and RNN with self attention and positional en-\ncoding, open source at https://github.com/tensorflow/tensor2tensor , Dehghani\net al. (2018) for an extension of Vaswani et al. (2017) at https://goo.gl/72gvdq , Artetxe\net al. (2018) for an unsupervised approach to machine translation, and, Zhang et al. (2017) for an\nopen source toolkit for neural machine translation.\n17.3 D IALOGUE SYSTEMS\nIn dialogue systems, conversational agents, or chatbots, human and computer interacts with a nat-\nural language. We intentionally remove ”spoken” before ”dialogue systems” to accommodate both\nspoken and written language user interface (UI). Jurafsky and Martin (2017) categorize dialogue\nsystems as task-oriented dialog agents and chatbots; the former are set up to have short conversa-\ntions to help complete particular tasks; the latter are set up to mimic human-human interactions with\nextended conversations, sometimes with entertainment value. As in Deng (2017), there are four\ncategories: social chatbots, infobots (interactive question answering), task completion bots (task-\noriented or goal-oriented) and personal assistant bots. We have seen generation one dialogue sys-\ntems: symbolic rule/template based, and generation two: data driven with (shallow) learning. We are\nnow experiencing generation three: data driven with deep learning, in which reinforcement learning\nusually plays an important role. A dialogue system usually include the following modules: (spoken)\nlanguage understanding, dialogue manager (dialogue state tracker and dialogue policy learning),\nand a natural language generation (Young et al., 2013). In task-oriented systems, there is usually a\nknowledge base to query. A deep learning approach, as usual, attempts to make the learning of the\nsystem parameters end-to-end. See Deng (2017) for more details. See a survey paper on applying\nmachine learning to speech recognition (Deng and Li, 2013).\nDhingra et al. (2017) propose KB-InfoBot, a goal-oriented dialogue system for multi-turn infor-\nmation access. KB-InfoBot is trained end-to-end using RL from user feedback with differentiable\noperations, including those for accessing external knowledge database (KB). In previous work, e.g.,\nLi et al. (2017) and Wen et al. (2017), a dialogue system accesses real world knowledge from KB by\nsymbolic, SQL-like operations, which is non-differentiable and disables the dialogue system from\nfully end-to-end trainable. KB-InfoBot achieves the differentiability by inducing a soft posterior\ndistribution over the KB entries to indicate which ones the user is interested in. The authors design\na modiﬁed version of the episodic REINFORCE algorithm to explore and learn both the policy to\nselect dialogue acts and the posterior over the KB entries for correct retrievals.The authors deploy\nimitation learning from rule-based belief trackers and policy to warm up the system.\nSu et al. (2016) propose an on-line learning framework to train the dialogue policy jointly with\nthe reward model via active learning with a Gaussian process model, to tackle the issue that it is\nunreliable and costly to use explicit user feedback as the reward signal. The authors show empirically\nthat the proposed framework reduces manual data annotations signiﬁcantly and mitigates noisy user\nfeedback in dialogue policy learning.\nLi et al. (2016) propose to use deep RL to generate dialogues to model future reward for better\ninformativity, coherence, and ease of answering, to attempt to address the issues in the sequence\n88\n\nto sequence models based on Sutskever et al. (2014): the myopia and misalignment of maximizing\nthe probability of generating a response given the previous dialogue turn, and the inﬁnite loop of\nrepetitive responses. The authors design a reward function to reﬂect the above desirable properties,\nand deploy policy gradient to optimize the long term reward. It would be interesting to investigate\nthe reward model with the approach in Su et al. (2016) or with inverse RL and imitation learning as\ndiscussed in Chapter 5, although Su et al. (2016) mention that such methods are costly, and humans\nmay not act optimally.\nTang et al. (2018a) propose subtask discovery for hierarchical dialogue policy learning based on\na dynamic programming approach to segmentation, extending Peng et al. (2017a), which assumes\nsubtasks are deﬁned by experts. Williams et al. (2017) propose to combine an RNN with domain\nknowledge to improve data efﬁciency of dialog training. Lewis et al. (2017) study end-to-end learn-\ning for negotiation dialogues; open source at https://github.com/facebookresearch/\nend-to-end-negotiator . Zhang et al. (2016b) study end-to-end speech recognition with\nCNNs. Xiong et al. (2017) describe Microsoft’s conversational speech recognition system in 2017.\nZhou et al. (2018) propose an emotional chatting machine. Li et al. (2017) present an end-to-end\ntask-completion neural dialogue system with parameters learned by supervised and reinforcement\nlearning. See the open source at http://github.com/MiuLab/TC-Bot . See Serban et al.\n(2018) for a survey of corpora for building dialogue systems.\nSee more recent papers: Asri et al. (2016), Bordes et al. (2017), Chen et al. (2016b), Fatemi et al.\n(2016), Kandasamy et al. (2017), Li et al. (2017a), Li et al. (2017b), Lipton et al. (2018), Mesnil\net al. (2015), Mo et al. (2018), Saon et al. (2016), She and Chai (2017), Xiong et al. (2017b), Zhao\nand Eskenazi (2016).\n89\n\n18 C OMPUTER VISION\nComputer vision is about how computers gain understanding from digital images or videos. Com-\nputer vision has been making rapid progress recently, and deep learning plays an important role. We\ndiscuss brieﬂy recent progress of computer vision below.\nKrizhevsky et al. (2012) propose AlexNet, almost halving the error rate of an ImagetNet competition\ntask, and ignite this wave of deep learning/AI. He et al. (2016d) propose residual nets (ResNets) to\nease the training of very deep neural networks by adding shortcut connections to learn residual\nfunctions with reference to the layer inputs. Fast R-CNN (Girshick, 2015), Faster R-CNN (Ren\net al., 2015), and Mask R-CNN (He et al., 2017) are proposed for image segmentation. Facebook AI\nResearch (FAIR) open source Detectron for object detection algorithms, https://research.\nfb.com/downloads/detectron/ .\nGenerative adversarial networks (GANs) (Goodfellow et al., 2014; Goodfellow, 2017) attracts lots\nof attention recently. There are fundamental work to improve the stability of learning GANs, e.g.,\nWasserstein GAN (WGAN) (Arjovsky et al., 2017), Gulrajani et al. (2017), and Least Squares GANs\n(LSGANs) (Mao et al., 2016). Many proposals in GANs are using computer vision testbeds, e.g.,\nCycleGAN (Zhu et al., 2017a), DualGAN (Yi et al., 2017), and Shrivastava et al. (2017).\nFor disentangled factor learning, many papers use computer vision testbeds. Diederik P Kingma\n(2014) propose variational autoencoders (V AEs). Kulkarni et al. (2015) propose deep convolution\ninverse graphics network (DC-IGN), which follows a semi-supervised way. Chen et al. (2016a) pro-\npose InfoGAN, an information-theoretic extension to the GANs, following an unsupervised way.\nHiggins et al. (2017) propose \f-V AE to automatically discover interpretable, disentangled, fac-\ntorised, latent representations from raw images in an unsupervised way. When \f= 1,\f-V AE is\nthe same as V AEs. Eslami et al. (2016) propose the framework of Attend-Infer-Repeat for efﬁcient\ninference in structured image models to reason about objects explicitly. Zhou et al. (2015) show that\nobject detectors emerge from learning to recognize scenes, without supervised labels for objects.\nReinforcement learning is an effective tool for many computer vision problems, like classiﬁcation,\ne.g. Mnih et al. (2014), detection, e.g. Caicedo and Lazebnik (2015), captioning, e.g. Xu et al.\n(2015), etc. RL is an important ingredient for interactive perception (Bohg et al., 2017), where\nperception and interaction with the environment would be helpful to each other, in tasks like ob-\nject segmentation, articulation model estimation, object dynamics learning, haptic property estima-\ntion, object recognition or categorization, multimodal object model learning, object pose estimation,\ngrasp planning, and manipulation skill learning. See Rhinehart et al. (2018) for a tutorial on inverse\nreinforcement learning for computer vision.\nMalik (2018) discusses that there are great achievements in the ﬁelds of vision, motor control, and\nlanguage semantic reasoning, and it is time to investigate them together. Zhang and Zhu (2018) sur-\nvey visual interpretability for deep learning. Lucid, at https://github.com/tensorflow/\nlucid , is an open source for interpretability, implementing feature visualization techniques in Olah\net al. (2017). Olah et al. (2018) discuss building blocks of interpretability.\nIn the following, we discuss recognition in Section 18.1, motion analysis in Section 18.2, scene\nunderstanding in Section 18.3, integration with NLP in Section 18.4, visual control in Section 18.5,\nand interactive perception in Section 18.6.\nWe list more topics about applying deep RL to computer vision as follows: Liu et al. (2017) for\nsemantic parsing of large-scale 3D point clouds, Devrim Kaba et al. (2017) for view planning, which\nis a set cover problem, Cao et al. (2017) for face hallucination, i.e., generating a high-resolution face\nimage from a low-resolution input image, Brunner et al. (2018) for learning to read maps, Cubuk\net al. (2018) for data augmentation for images, Bhatti et al. (2016) for SLAM-augmented DQN.\n18.1 R ECOGNITION\nRL can improve efﬁciency for image classiﬁcation by focusing only on salient parts. For visual\nobject localization and detection, RL can improve efﬁciency over approaches with exhaustive spatial\nhypothesis search and sliding windows, and strike a balance between sampling more regions for\nbetter accuracy and stopping the search when sufﬁcient conﬁdence is obtained about the target’s\nlocation.\n90\n\nMnih et al. (2014) introduce the recurrent attention model (RAM), which we discuss in Chapter 9.\nCaicedo and Lazebnik (2015) propose an active detection model for object localization with DQN,\nby deforming a bounding box with transformation actions to determine the most speciﬁc location\nfor target objects. Jie et al. (2016) propose a tree-structure RL approach to search for objects se-\nquentially, considering both the current observation and previous search paths, by maximizing the\nlong-term reward associated with localization accuracy over all objects with DQN. Mathe et al.\n(2016) propose to use policy search for visual object detection. Kong et al. (2017) deploy collabo-\nrative multi-agent RL with inter-agent communication for joint object search. Welleck et al. (2017)\npropose a hierarchical visual architecture with an attention mechanism for multi-label image classi-\nﬁcation. Rao et al. (2017) propose an attention-aware deep RL method for video face recognition.\nKrull et al. (2017) study 6D object pose estimation.\n18.2 M OTION ANALYSIS\nIn tracking, an agent needs to follow a moving object. Supan ˇciˇc and Ramanan (2017) propose online\ndecision-making process for tracking, formulate it as a partially observable decision-making process\n(POMDP), and learn policies with deep RL algorithms, to decide where to look for the object, when\nto reinitialize, and when to update the appearance model for the object, where image frames may\nbe ambiguous and computational budget may be constrained. Yun et al. (2017) also study visual\ntracking with deep RL.\nRhinehart and Kitani (2017) propose to discover agent rewards for K-futures online (DARKO) to\nmodel and forecast ﬁrst-person camera wearer’s long-term goals, together with states, transitions,\nand rewards from streaming data, with inverse RL.\n18.3 S CENE UNDERSTANDING\nWu et al. (2017b) study the problem of scene understanding, and attempt to obtain a compact, ex-\npressive, and interpretable representation to encode scene information like objects, their categories,\nposes, positions, etc, in a semi-supervised way. In contrast to encoder-decoder based neural archi-\ntectures as in previous work, Wu et al. (2017b) propose to replace the decoder with a deterministic\nrendering function, to map a structured and disentangled scene description, scene XML, to an im-\nage; consequently, the encoder transforms an image to the scene XML by inverting the rendering\noperation, a.k.a., de-rendering. The authors deploy a variant of REINFORCE to overcome the non-\ndifferentiability issue of graphics rendering engines.\nWu et al. (2017a) propose a paradigm with three major components, a convolutional perception\nmodule, a physics engine, and a graphics engine, to understand physical scenes without human an-\nnotations. The perception module recovers a physical world representation by inverting the graphics\nengine, inferring the physical object state for each segment proposal in input and combining them.\nThe generative physics and graphics engines then run forward with the world representation to re-\nconstruct the visual data. The authors show results on both neural, differentiable and more mature\nbut non-differentiable physics engines.\nWe discuss generative query network (GQN) (Eslami et al., 2018) in Chapter 10. Chen et al. (2018a)\npropose a framework for iterative visual reasoning, which we discuss in Chapter 13. There are recent\npapers about physics learning, e.g., Agrawal et al. (2016); Battaglia et al. (2016); Denil et al. (2017);\nWatters et al. (2017); Wu et al. (2015).\n18.4 I NTEGRATION WITH NLP\nSome papers integrate computer vision with natural language processing (NLP). Xu et al. (2015)\nintegrate attention to image captioning. See also Liu et al. (2016), Lu et al. (2016), Rennie et al.\n(2017), and Ren et al. (2017) for image captioning. See Pasunuru and Bansal (2017); Wang et al.\n(2018f) for video captioning. Strub et al. (2017) propose end-to-end optimization with deep RL for\ngoal-driven and visually grounded dialogue systems for the GuessWhat?! game. Das et al. (2017)\npropose to learn cooperative visual dialog agents with deep RL. Wang et al. (2018e) propose to use\ninverse RL for visual storytelling. See also Kottur et al. (2017). See Liang et al. (2017c) for visual\nrelationship and attribute detection.\n91\n\n18.5 V ISUAL CONTROL\nVisual control is about deriving a policy from visual inputs, e.g., in games (Mnih et al., 2015; Silver\net al., 2016a; 2017; Oh et al., 2015; Wu and Tian, 2017; Dosovitskiy and Koltun, 2017; Lample\nand Chaplot, 2017; Jaderberg et al., 2017), robotics (Finn and Levine, 2017; Gupta et al., 2017b;\nLee et al., 2017; Levine et al., 2016; Mirowski et al., 2017; Zhu et al., 2017b), and self-driving\nvehicles (Bojarski et al., 2016; Bojarski et al., 2017; Zhou and Tuzel, 2018).\nFor a visual control problem in computer vision, there should be some ingredients of, by, for com-\nputer vision, but not just use a CNN or some deep neural network to take image or video as input,\nwithout further handling with computer vision techniques, e.g., DQN (Mnih et al., 2015) and Al-\nphaGo (Silver et al., 2016a; 2017).\n18.6 I NTERACTIVE PERCEPTION\nReinforcement learning is an important ingredient for interactive perception (Bohg et al., 2017).\nJayaraman and Grauman (2018) propose a deep RL approach with recurrent neural network for\nactive visual completion, to hallucinate unobserved parts based on a small number of observations.\nThe authors attempt to answer the question of how to make decisions about what to observe to\nacquire more information in visual perception, without labeled data, rather than making inference\ndecisions based on labeled observations. The look-around decisions are rewarded based on the\naccuracy of the predictions of unobserved views, in particular, the distance between view predictions\nand their ground truth for all viewpoints and all time steps. The authors propose a task agnostic\napproach for active visual completion, and, consider two tasks: panoramic natural scenes and 3D\nobject shapes, for illustration. The authors also discuss generalization and transferability of their\nproposed approach to new tasks and environments.\n92\n\n19 F INANCE AND BUSINESS MANAGEMENT\nMachine learning naturally has wide applications in ﬁnance, e.g., in fundamental analysis, be-\nhavioural ﬁnance, technical analysis, ﬁnancial engineering, ﬁnancial technology (FinTech), etc. Re-\ninforcement learning is a natural solution to some sequential decision making ﬁnance problems, like\noption pricing, trading, and multi-period portfolio optimization, etc. RL also has many applications\nin business management, like ads, recommendation, customer management, and marketing, etc.\nFinancial engineering is a discipline rooting in ﬁnance, computer science and mathematics (Hull,\n2014; Luenberger, 1997). Derivative pricing is an essential issue in ﬁnancial engineering. The values\nof ﬁnancial derivatives depend on the values of underlying assets. Options are the most fundamental\nderivatives. An option gives the holder the right, but not the obligation, to buy or sell an asset at a\ncertain price by a certain time. Portfolio optimization is about how to allocate assets so as to trade\noff between return and risk.\nThere are two schools in ﬁnance, Efﬁcient Markets Hypothesis (EMH) and Behavioral Finance (Lo,\n2004). According to EMH, “prices fully reﬂect all available information” and are determined by\nthe market equilibrium. However, psychologists and economists have found a number of behav-\nioral biases that are native in human decision-making under uncertainty. For example, Amos Tver-\nsky and Daniel Kahneman demonstrate the phenomenon of loss aversion, in which people tend to\nstrongly prefer avoiding losses to acquiring gains (Kahneman, 2011). Prashanth et al. (2016) inves-\ntigate prospect theory with reinforcement learning. Behavioral ﬁnance justiﬁes technical analysis\n(Murphy, 1999) to some extend. Lo et al. (2000) propose to use nonparametric kernel regression\nfor automatic technical pattern recognition. Lo (2004) proposes the Adaptive Market Hypothesis\nto reconcile EMH and behavioral ﬁnance, where the markets are in the evolutionary process of\ncompetition, mutation, reproduction and natural selection. RL may play an important role in this\nfundamental market paradigm.\nFinancial technology (FinTech) has been attracting lots of attention, especially after the notion of\nbig data and data science. FinTech employs machine learning techniques to deal with issues like\nfraud detection (Phua et al., 2010), and consumer credit risk (Khandani et al., 2010), etc.\nWe will discuss applications of deep learning and reinforcement learning to ﬁnance and business\nmanagement. We only pick a couple of papers for discussions, and do not include many relevant\npapers. Machine learning techniques, like support vector machines (SVM), decision trees, etc, have\nalso been applied to ﬁnance and business management. We can check them from the reference in\nthe papers we discuss.\nIt is nontrivial for ﬁnance and economics academia to accept machine learning methods like neural\nnetworks. One factor is that neural networks, among many machine learning methods, are black box;\nhowever, interpretability is desirable in ﬁnance. Doshi-Velez and Kim (2017) and Lipton (2018) dis-\ncuss issues of interpretability. Zhang and Zhu (2018) is a survey about interpretability in computer\nvision. National Bereau Economic Research (NBER) organizes a meeting on Economics of Ar-\ntiﬁcial Intelligence; see http://conference.nber.org/conferences/2018/AIf18/\nsummary.html . See Mullainathan (2017) for a lecture in American Finance Association (AFA)\n2017 annual meeting about machine learning and prediction in economics and ﬁnance. In 2018\nASSA Annual Meeting, at https://www.aeaweb.org/conference/2018 , AFA as part\nof it, we see many sessions with the keywords ”artiﬁcial intelligence” and/or ”machine learning”.\nWe see this as that AI and machine learning are starting to permeate to the mainstream of the ﬁeld of\nﬁnance and economics. It would be natural for economics and ﬁnance to marry reinforcement learn-\ning, machine learning, and AI, considering that quantitative approaches for economics and ﬁnance\nshare foundations of optimization, statistics, and probability with RL/ML/AI, and, behavioural ap-\nproaches for economics and ﬁnance share foundations of psychology, neuroscience, and cognitive\nscience with RL/ML/AI. We will see more and more applications of RL/ML/AI in ﬁnance, eco-\nnomics, and social sciences in general. See NIPS 2018 Workshop on Challenges and Opportunities\nfor AI in Financial Services: the Impact of Fairness, Explainability, Accuracy, and Privacy.\nBefore discussing applications of RL to ﬁnance and business management, we introduce ﬁnance\napplications with deep learning. Deep learning has a wide applications in ﬁnance, e.g., company\nfundamentals prediction (Alberg and Lipton, 2017), macroeconomic indicator forecasting (Cook\nand Hall, 2017), and limit order books (Sirignano, 2016), etc.\n93\n\nHeaton et al. (2016) introduce deep learning for ﬁnance, in particular, deep portfolios, and argue that\ndeep learning methods may be more powerful than the current standard methods in ﬁnance., e.g., the\nsimplistic traditional ﬁnancial economics linear factor models and statistical arbitrage asset manage-\nment techniques. The authors show the power of deep learning with a case study of smart indexing\nfor the biotechnology IBB index with a four step algorithm: 1) auto-encoding, ﬁnd the market-map\nto auto-encode the input variables with itself and to create a more efﬁcient representation of the\ninput variables; 2) calibrating, ﬁnd the portfolio-map to create a portfolio from the input variables\nto approximate an objective; 3) validating, balance the errors in the auto-encoding and calibrating\nsteps; and 4) verifying, choose market-map and portfolio-map to satisfy the validating step. The\nauthors make an interesting observation that the univariate activation functions such as ReLU, i.e.,\nmax(0;x), wherexis a variable, in deep learning can be interpreted as compositions of ﬁnancial\ncall and put options on linear combination of input assets.\nBao et al. (2017) investigate the problem of stock price forecasting by combining wavelet transforms\n(WT), stacked auto-encoders (SAEs) and long-short term memory (LSTM): WT for decomposing\nstock price time series to reduce noises, SAEs for generating high-level features for stock price pre-\ndiction, and LSTM for stock price forecasting by taking the denoised features. The authors evaluate\nthe performance of the proposed method with six market indices and their corresponding index fu-\ntures, together with a buy-and-sell trading strategy, using three performance metrics: Mean absolute\npercentage error (MAPE), correlation coefﬁcient (R) and Theil’s inequality coefﬁcient (Theil U),\nand show promising results in both predictive accuracy and proﬁtability performance.\n19.1 O PTION PRICING\nOptions are fundamental ﬁnancial instruments, dating back to the ancient time. A challenging prob-\nlem is option pricing, especially for American type options, which can be exercised before the\nmaturity date. For European options, which can only be exercised at the maturity date, prices can\nbe calculated by the Black-Scholes formula in certain cases. The key to American option pric-\ning (Longstaff and Schwartz, 2001; Tsitsiklis and Van Roy, 2001; Li et al., 2009) is to calculate\nthe conditional expected value of continuation. This is an optimal stopping problem. Hull (2014)\nprovides an introduction to options and other derivatives and their pricing methods; and Glasserman\n(2004) provides a book length treatment for Monte Carlo methods in ﬁnancial engineering. The\nleast squares Monte Carlo (LSM) method in Longstaff and Schwartz (2001), following approximate\ndynamic programming, is a standard approach in the ﬁnance literature for pricing American options.\n19.2 P ORTFOLIO OPTIMIZATION\nMean-variance analysis by Markowitz is a classical approach to portfolio optimization in one pe-\nriod (Luenberger, 1997). Dynamic portfolio optimization in multi-period renews its attraction re-\ncently (Campbell and Viceira, 2002; Brandt et al., 2005), following the recent empirical evidence\nof return predictability (Pastor and Stambaugh, 2009), and with the consideration of practical issues\nincluding parameter and model uncertainty, transaction cost and background risks (Brandt et al.,\n2005). Brandt et al. (2005) and Neuneier (1997) deploy the backward dynamic programming ap-\nproach in Longstaff and Schwartz (2001) for the dynamic portfolio problem. It is possible to apply\nreinforcement learning methods for it.\nMoody and Saffell (2001) learns to trade via direct reinforcement, without any forecasting. Deng\net al. (2016) extend it with deep neural networks. It may be beneﬁcial to take advantage of return\npredictability in RL methods.\nIt is critical to control the risk when forming portfolios. Value-at-Risk (VaR) is a popular risk\nmeasure; while conditional VaR (CVaR) has desirable mathematical properties (Hull, 2014). Yu\net al. (2009) provide formulations for VaR and CVaR with relaxed probability distributions by worst-\ncase analysis. Deep (reinforcement) learning would provide better solutions in some issues in risk\nmanagement. The generalization to continuous state and action spaces is an indispensable step for\nsuch methods to be applied to dynamic portfolio optimization.\n94\n\n19.3 B USINESS MANAGEMENT\nLi et al. (2010) formulate personalized news articles recommendation as a contextual bandit prob-\nlem, to learn an algorithm to select articles sequentially for users based on contextual information\nof users and articles, such as historical activities of users and descriptive information and categories\nof content, and to take user-click feedback to adapt selection policy to maximize total user clicks in\nthe long run.\nTheocharous et al. (2015) formulate a personalized ads recommendation systems as a RL problem to\nmaximize life-time value (LTV) with theoretical guarantees. This is in contrast to a myopic solution\nwith supervised learning or contextual bandit formulation, usually with the performance metric of\nclick through rate (CTR). As the models are hard to learn, the authors deploy a model-free approach\nto compute a lower-bound on the expected return of a policy to address the off-policy evaluation\nproblem, i.e., how to evaluate a RL policy without deployment.\nJiang and Li (2016) study off-policy value evaluation by extending the doubly robust estimator for\nbandits. The proposed method helps safety in policy improvements and applies to both shallow\nand deep RL. One experiment is about maximizing lifetime value of customers. Silver et al. (2013)\npropose concurrent reinforcement learning for the customer interaction problem. See Cai et al.\n(2018b) for mechanism design for fraudulent behaviour in e-commerce, Hu et al. (2018a) for ranking\nin e-commerce search engine, Hu et al. (2018c) for incentive mechanism design in crowdsourcing,\nLattimore et al. (2018) for ranking, Nazari et al. (2018) for vehicle routing in operations research,\nShi et al. (2018) for visualization of online retail environment for RL, and, Zhao et al. (2018b), Zhao\net al. (2018c) and Zheng et al. (2018a) for recommendation. See Zhang et al. (2017) for a survey on\nrecommendation.\nSee Section 16.7 Personalized Web Services in Sutton and Barto (2018) for a detailed and intuitive\ndescription of some topics discussed here.\n95\n\n20 M ORE APPLICATIONS\nIn this chapter, we discuss more reinforcement learning applications: healthcare in Section 20.1,\neducation in Section 20.2. energy in Section 20.3, transportation in Section 20.4, computer systems\nin Section 20.5, and, science, engineering and art in Section 20.6.\n20.1 H EALTHCARE\nThere are many opportunities and challenges in healthcare for machine learning (Miotto et al.,\n2017; Saria, 2014). Personalized medicine is getting popular in healthcare. It systematically\noptimizes patients’ health care, in particular, for chronic conditions and cancers using individ-\nual patient information, potentially from electronic health/medical record (EHR/EMR). Li et al.\n(2018b) propose a hybrid retrieval-generation reinforced agent for medical image report genera-\ntion. Rajkomar et al. (2018) investigate applying deep learning to EHR data. Rotmensch et al.\n(2017) learn a healthcare knowledge graph from EHR. Fauw et al. (2018) apply deep learn-\ning for diagnosis and referral in retinal disease; see a blog at https://deepmind.com/\nblog/moorfields-major-milestone/ . Gheiratmand et al. (2017) study network-based\npatterns of schizophrenia. Rajpurkar et al. (2018) introduce a large dataset of musculoskele-\ntal radiographs. See a tutorial on deep reinforcement learning for medical imaging at https:\n//www.hvnguyen.com/deepreinforcementlearning . See Liu and Sun (2017) for a\ntutorial on deep learning for health care applications. See a course on Machine Learning for Health-\ncare, at https://mlhc17mit.github.io .\nDynamic treatment regimes (DTRs) or adaptive treatment strategies are sequential decision making\nproblems. Some issues in DTRs are not in standard RL. Shortreed et al. (2011) tackle the missing\ndata problem, and design methods to quantify the evidence of the learned optimal policy. Goldberg\nand Kosorok (2012) propose methods for censored data (patients may drop out during the trial) and\nﬂexible number of stages. See Chakraborty and Murphy (2014) for a recent survey, and Kosorok\nand Moodie (2015) for an edited book about recent progress in DTRs. Currently Q-learning is the\nRL method in DTRs. Ling et al. (2017) apply deep RL to the problem of inferring patient pheno-\ntypes. Liu et al. (2018d) study off-policy policy evaluation and its application to sepsis treatment.\nKallus and Zhou (2018) study confounding-robust policy improvement and its application to acute\nischaemic stroke treatment. Peng et al. (2018c) study disease diagnosis.\nSome recent conferences and workshops at the intersection of machine learning and healthcare\nare: Machine Learning for Healthcare, https://www.mlforhc.org ; NIPS 2018 Work-\nshop on Machine Learning for Health (ML4H): Moving beyond supervised learning in health-\ncare; NIPS 2017 Workshop on Machine Learning for Health (ML4H), https://ml4health.\ngithub.io/2017/ ; NIPS 2016 Workshop on Machine Learning for Health (ML4H), http:\n//www.nipsml4hc.ws ; NIPS 2015 Workshop on Machine Learning in Healthcare, https:\n//sites.google.com/site/nipsmlhc15/ . See an issue of Nature Biomedical Engi-\nneering on machine learning at https://www.nature.com/natbiomedeng/volumes/\n2/issues/10 . See Hernandez and Greenwald (2018), a WSJ article about IBM Watson dilemma.\n20.2 E DUCATION\nNorthcutt (2017) presents tutorial-style slides for AI in online education with an emphasis on person-\nalization. The author presents a framework for AI in online education, including the active/passive\ncourse, content, and student, and give examples as below. Active AI refers to changes in course or\nexperience; passive AI refers to unseen estimation or modeling.\n\u000factive student: cognitive tutor\n\u000fpassive student: proﬁciency estimation (IRT)\n\u000factive content: content recommendation engine\n\u000fpassive content: estimating points of confusion in instructional videos\n\u000factive course: auto-generate new courses from pieces of other courses, students interact;\nmeasure outcomes, and iterate\n\u000fpassive course: estimate optimal course prerequisite structure for a new ﬁeld\n96\n\nNorthcutt (2017) present 18 problems, some with solutions, some with ideas.\nOne problem is representation, which is about how to represent courses as vectors, how to measure\nsimilarity of two courses, videos, and students, how to recommend content to students, and how to\nmatch students with other students, etc.\nOne approach to obtain representation is to use embeddings, on courses, content, or students. High-\ndimensional feature matrices can be used to capture the interactions between courses, content, and\nstudents. Dimension reduction can be done using PCA, SVD, etc. or hidden layers of a neural\nnetwork. With such embedded dense low-dimensional representations, we can generate new courses\nfrom existing courses, pair student, make inference, and structure content, etc.\nWe can employ a recommendation engine for MOOCs using embeddings, with a Siamese neural net-\nwork architecture, one network to represent students, and another network for content, and produce\nrepresentations so that cosine (student;content )measures the goodness of content forstudent .\nNorthcutt (2017) also discuss the following problems, detect struggling, rampant cheating, stats col-\nlaboration, how to personalize, independent treatment effect (ITE)/counterfactuals, trajectory pre-\ndiction, how to order content, adaptive learning, Google Scholar, majority bias in forums, feature\nextraction, cognitive state, content likability, points of confusion, cognitive modeling, human intel-\nligence vs. artiﬁcial intelligence, and the next edX. See (Northcutt, 2017) for more details.\nThere are some recent work in education using RL.\nMandel et al. (2014) propose an ofﬂine policy evaluation method, by combining importance sam-\npling with cross-validation, to investigate generalization of representations. The authors propose\na feature compaction algorithm for high-dimension problems, which beneﬁt from both PCA and\nneural networks. Furthermore, the authors apply the method to an educational game, optimizing\nengagement by learning concept selection.\nLiu et al. (2014) propose UCB-Explore, based on multi-armed bandit algorithm UCB1, to automat-\nicaly allocate experimental samples, to balance between learning the effectiveness of each experi-\nmental condition and users’ test performances, by explicitly specifying the tradeoff between these\ntwo objectives. The authors compare UCB-Explore with other multi-armed bandit algorithms like\nUCB1 and\u000f-greedy with simulation on an educational game.\nUpadhyay et al. (2018) apply reinforcement learning to marked temporal point processes with an\napplication in personalized education. See also Li et al. (2018a) for applying RL to temporal point\nprocesses. Oudeyer et al. (2016) discuss theory and applications of intrinsic motivation, curiosity,\nand learning in educational technologies.\nWatch a video titled Reinforcement Learning with People (Brunskill, 2017). See ACL 2018 Work-\nshop on Natural Language Processing Techniques for Educational Applications (NLPTEA 2018)\nwith a Shared Task for Chinese Grammatical Error Diagnosis (CGED), at https://sites.\ngoogle.com/view/nlptea2018/ .\n20.3 E NERGY\nA smart grid is a power grid utilizing modern information technologies to create an intelligent elec-\ntricity delivery network for electricity generation, transmission, distribution, consumption, and con-\ntrol (Fang et al., 2012). An important aspect is adaptive control (Anderson et al., 2011). Glavic et al.\n(2017) review application of RL for electric power system decision and control. See Platt (2017)\nfor a talk about energy. Here we brieﬂy discuss demand response (Wen et al., 2015; Ruelens et al.,\n2016).\nDemand response systems motivate users to dynamically adapt electrical demands in response to\nchanges in grid signals, like electricity price, temperature, and weather, etc. With suitable electricity\nprices, load of peak consumption may be rescheduled/lessened, to improve efﬁciency, reduce costs,\nand reduce risks. Wen et al. (2015) propose to design a fully automated energy management system\nwith model-free reinforcement learning, so that it doesn’t need to specify a disutility function to\nmodel users’ dissatisfaction with job rescheduling. The authors decompose the RL formulation\nover devices, so that the computational complexity grows linearly with the number of devices, and\nconduct simulations using Q-learning. Ruelens et al. (2016) tackle the demand response problem\n97\n\nwith batch RL. Wen et al. (2015) take the exogenous prices as states, and Ruelens et al. (2016) utilize\nthe average as feature extractor to construct states.\n20.4 T RANSPORTATION\nIntelligent transportation systems (Bazzan and Kl ¨ugl, 2014) apply advanced information technolo-\ngies for tackling issues in transport networks, like congestion, safety, efﬁciency, etc., to make trans-\nport networks, vehicles and users smart.\nAutonomous driving vehicles is an important topic of intelligent transportation systems. We discuss\nit in Section 16.6.\nSee NIPS Workshops on Machine Learning for Intelligent Transportation Systems, in 2018 at\nhttps://sites.google.com/site/nips2018mlits/ , in 2017 at https://sites.\ngoogle.com/site/nips2017mlits/ , and, in 2016 at https://sites.google.com/\nsite/nips2016intelligenttrans/ .\nADAPTIVE CONTROL\nAn important issue in intelligent transportation systems is adaptive control. El-Tantawy et al. (2013)\npropose to model the adaptive trafﬁc signal control problem as a multiple player stochastic game, and\nsolve it with the approach of multi-agent RL (Shoham et al., 2007; Busoniu et al., 2008). Multi-agent\nRL integrates single agent RL with game theory, facing challenges of stability, nonstationarity, and\ncurse of dimensionality. El-Tantawy et al. (2013) approach the issue of coordination by considering\nagents at neighbouring intersections. The authors validate their proposed approach with simulations,\nand real trafﬁc data from the City of Toronto. El-Tantawy et al. (2013) don’t explore function\napproximation. See van der Pol and Oliehoek (2017) for a recent work, and Mannion et al. (2016)\nfor an experimental review, about applying RL to adaptive trafﬁc signal control. Belletti et al. (2018)\nstudy expert level control of ramp metering based on multi-task deep reinforcement learning.\n20.5 C OMPUTER SYSTEMS\nComputer systems are indispensable in our daily life and work, e.g., mobile phones, computers, and\ncloud computing. Control and optimization problems abound in computer systems, e,g., Mestres\net al. (2017) propose knowledge-deﬁned networks, Gavrilovska et al. (2013) review learning and\nreasoning techniques in cognitive radio networks, and Haykin (2005) discuss issues in cognitive\nradio, like channel state prediction and resource allocation. We also note that Internet of Things\n(IoT)(Xu et al., 2014) and wireless sensor networks (Alsheikh et al., 2014) play important roles\nin robotics and autonomous driving as discussed in Chapter 16, in energy systems as discussed in\nSection 20.3, and in transportation as discussed in Section 20.4. See Zhang et al. (2018) for a recent\nsurvey about applying deep learning and reinforcement learning to issues in mobile and wireless\nnetworking. Mukwevho and Celik (2018) discuss fault tolerance in cloud computing.\nKraska et al. (2018) propose learned indexes, by treating B-Tree, Hash, BitMap, etc. as models, and\nuse neural networks to learn such models, by using the signal of learned model of structure or sort\norder of lookup keys to predict the existence or position of records. Experiments show promising\nresults. Wang and O’Boyle (2018) study compiler optimization. Reichstaller and Knapp (2017)\nstudy software testing. Krishnan et al. (2018) study SQL join queries optimization. Faust et al.\n(2018) study sorting. See recent papers about neural approaches for program synthesis, e.g., Balog\net al. (2017); Liang et al. (2017a; 2018); Nachum et al. (2017); Parisotto et al. (2017); Reed and\nde Freitas (2016); Vinyals et al. (2015); Zaremba and Sutskever (2015); Zhang et al. (2018).\nSee SysML conference, at the intersection of system and machine learning, at https://www.\nsysml.cc . See NIPS 2018 Workshop on Security in Machine Learning.\nPERFORMANCE OPTIMIZATION\nMirhoseini et al. (2017) propose to optimize device placement for Tensorﬂow computational graphs\nwith RL. The authors deploy a seuqence-to-sequence model to predict how to place subsets of\noperations in a Tensorﬂow graph on available devices, using the execution time of the predicted\n98\n\nplacement as reward signal for REINFORCE algorithm. The proposed method ﬁnds placements of\nTensorﬂow operations on devices for Inception-V3, recurrent neural language model and neural ma-\nchine translation, yielding shorter execution time than those placements designed by human experts.\nComputation burden is one concern for a RL approach to search directly in the solution space of a\ncombinatorial problem. We discuss learning combinatorial optimization in Section 14.5. Gao et al.\n(2018c) also study the problem of device placement.\nMao et al. (2016) study resource management in systems and networking with deep RL. The authors\npropose to tackle multi-resource cluster scheduling with policy gradient, in an online manner with\ndynamic job arrivals, optimizing various objectives like average job slowdown or completion time.\nThe authors validate their proposed approach with simulation.\nLiu et al. (2017a) propose a hierarchical framework to tackle resource allocation and power man-\nagement in cloud computing with deep RL. The authors decompose the problem as a global tier\nfor virtual machines resource allocation and a local tier for servers power management. The au-\nthors validate their proposed approach with actual Google cluster traces. Such hierarchical frame-\nwork/decomposition approach was to reduce state/action space, and to enable distributed operation\nof power management.\nGoogle deploy machine learning for data centre power management, reducing energy consumption\nby 40%. See blogs at http://goo.gl/4PHcos andhttp://goo.gl/N3Aoxm . Lazic et al.\n(2018) study data center cooling with model-predictive control (MPC).\nOptimizing memory control is discussed in Sutton and Barto (2018).\nSECURITY\nThere is a long history applying machine learning (ML) techniques to system security issues, e.g.,\nChandola et al. (2009) survey ML techniques for anomaly detection, and, Sommer and Paxson\n(2010) discuss issues in using ML techniques for network intrusion detection.\nAdversarial machine learning is about learning in the presence of adversaries. It is concerned with\nthe training stage, when facing data poisoning issues, and learning wrong models hard to detect. It\nis also concerned with the inference stage, when facing adversarial examples, and making wrong\ndecisions. Adversarial ML is a critical for some ML applications, like autonomous driving.\nAdversarial ML is an emerging ﬁeld, in this wave of deep learning, after researchers ﬁnd adversarial\nexamples to deep learning algorithms, e.g., Szegedy et al. (2013) show that various images, like a\ntruck, a building, or a dog, after being added small noises, are all classiﬁed by AlexNet as ”ostrich,\nStruthio camelus”. Goodfellow et al. (2015) also show a fast adversarial example generation method,\nso that an image of panda, after being added a small vector, is classiﬁed as a gibbon by GoogLeNet.\nEykholt et al. (2018) show that physical images, like stop signs, yield signs, left turn signs etc.,\nafter being perturbed by adding black or white stickers, are misclassiﬁed by the state of art deep\nneural networks as speed limit 45 signs. Evtimov et al. (2017) discuss attacks to physical images,\nhttp://bair.berkeley.edu/blog/2017/12/30/yolo-attack/ . RL algorithms are\nalso vulnerable to adversarial attacks, e.g., Huang et al. (2017) and Havens et al. (2018), as well as\nmulti-armed bandit algorithms, e.g., Jun et al. (2018). See a blog at http://rll.berkeley.\nedu/adversarial/ .\nAdversarial ML is an active area, with ﬁerce competition between the design of attack and de-\nfense algorithms. Athalye et al. (2018) show that seven of nine defense techniques, shortly after\ntheir papers being accepted to ICLR 2018, cause the issue of obfuscated gradients and are vul-\nnerable to their attacks. See their open source at https://github.com/anishathalye/\nobfuscated-gradients for implementations of their attack and the studied defense methods.\nAnderson et al. (2018) propose a black-box attack approach against static portable executable (PE)\nanti-malware engines with reinforcement learning, which produces functional evasive malware sam-\nples to help improve anti-malware engines. The performance still needs improvements. See the open\nsource at https://github.com/endgameinc/gym-malware .\nSee Song (2018) for a tutorial on AI and security. See Kantarcioglu and Xi (2016) for a tutorial\non adversarial data mining. See Yuan et al. (2017) for a survey on attacks and defenses for deep\n99\n\nlearning. Papernot et al. (2016) present CleverHans, a software library for reference implementations\nadversarial ML algorithms.\n20.6 S CIENCE , ENGINEERING AND ART\nReinforcement learning, deep learning, machine learning, and AI in general, have very wide inter-\nactions with science, engineering and art. We see that RL and areas in science, engineering and\nart inﬂuence each other, i.e., RL/AI has applications in these areas, with new observations or even\nnew algorithms and new principles; and intuitions and principles from these areas help further de-\nvelopment of RL/AI. For example, Sutton and Barto (2018) discuss the interplay between RL and\nneuroscience and psychology; and in Chapter 14, we discuss learning to learn new algorithms. Here\nwe focus on applications of RL/AI to these areas.\nSutton and Barto (2018) treat dynamic programming (DP) and Markov decision processes (MDPs)\nas foundations for RL, and also devote two chapters for neuroscience and psychology, respectively.\nThere are books discussing approximate dynamic programming, MDPs, operations research, op-\ntimal control, as well as the underlying optimization, statistics, and probability, e.g.,Bertsekas and\nTsitsiklis (1996), Bertsekas (2012), Szepesv ´ari (2010), and Powell (2011). There are strong relation-\nships between these areas with RL.8Powell (2010) discusses merging AI and operations research to\nsolve high-dimensional stochastic optimization problems with approximate dynamic programming.\nLake et al. (2016) discuss incorporating human intelligence into the current DL/RL/AI systems.\nHassabis et al. (2017) discuss the connection between neuroscience and RL/AI. Kriegeskorte and\nDouglas (2018) surveys cognitive computational neuroscience.\nRL/AI is relevant to many areas, e.g., mathematics, chemistry, physics (Cranmer, 2016), biol-\nogy (Mahmud et al., 2018), music, drawing (Xie et al., 2012; Ha and Eck, 2018), character an-\nimation (Peng et al., 2018a;b), dancing (Chan et al., 2018), storytelling (Thue et al., 2007), etc.\nDeVries et al. (2018) study earthquakes with deep learning. Some topics, e.g., music, drawing,\nstorytelling, are at the intersection of science and art.\nWe discuss games in Chapter 15, robotics in Chapter 16, computer vision in Chapter 18, natural\nlanguage processing (NLP) in Chapter 17, and, computer systems in Section 20.5, as areas in com-\nputer science. We put many topics in computer science like indexing (Kraska et al., 2018), compiler\noptimization (Wang and O’Boyle, 2018), software testing (Reichstaller and Knapp, 2017), SQL join\nqueries optimization (Krishnan et al., 2018), and sorting (Faust et al., 2018) etc. in computer systems\nin Section 20.5. See recent papers about neural approaches for program synthesis, e.g., Balog et al.\n(2017); Liang et al. (2017a; 2018); Nachum et al. (2017); Parisotto et al. (2017); Reed and de Freitas\n(2016); Vinyals et al. (2015); Zaremba and Sutskever (2015); Zhang et al. (2018).\nWe discuss ﬁnance and business management in Section 19, healthcare in Section 20.1, and, educa-\ntion in Section 20.2, as areas in social science. We discuss energy in Section 20.3, and transportation\nin Section 20.4, as areas in engineering.9\nImagination is critical for creative activities, like science, engineering and art. Mahadevan (2018b)\ndiscuss imagination machines as a new challenge for AI. See Mahadevan (2018a) for a tutorial on\nthis topic.\nQuantum machine learning is about designing machine learning algorithms on quantum computing\narchitectures, at the interaction of theoretical computer science, machine learning, and physics. Bi-\namonte et al. (2017) survey quantum machine learning, including quantum reinforcement learning.\nWe list some workshops in the following.\n\u000fNIPS 2015 Workshop on Quantum Machine Learning at https://www.microsoft.\ncom/en-us/research/event/quantum-machine-learning/\n\u000fMachine Learning for Science Workshop at LBNL at https://sites.google.com/\nlbl.gov/ml4sci/\n8Check for a special issue of IEEE Transactions on Neural Networks and Learning Systems on\nDeep Reinforcement Learning and Adaptive Dynamic Programming, published in June 2018, https://\nieeexplore.ieee.org/document/8353782/ .\n9Computer vision and computer systems are also in engineering.\n100\n\n\u000fMachine Learning for Physics and the Physics of Learning at\nhttp://www.ipam.ucla.edu/programs/long-programs/\nmachine-learning-for-physics-and-the-physics-of-learning/\n\u000fNIPS 2018 Workshop Machine Learning for Molecules and Materials at http://www.\nquantum-machine.org/workshops/nips2018draft/\n\u000fNIPS Workshop on Machine Learning for Creativity and Design\n\u000ein 2018 at https://nips2018creativity.github.io/\n\u000ein 2017 at https://nips2017creativity.github.io\nIn this section, we attempt to put reinforcement learning in the wide context of science, engineering,\nand art. We have already touched many aspects in previous chapters/sections. Here we only discuss\na small sample of the aspects we have not discussed before.\n20.6.1 C HEMISTRY\nRetrosynthesis is a chemistry technique to transform a target molecule into simpler precursors re-\ncursively. Segler et al. (2018) propose to combine Monte Carlo tree search (MCTS) with symbolic\nrules for automatic retrosynthesis. Three deep neural networks, namely, an expansion policy net-\nwork to guide the search with transformations extracted automatically, a ﬁlter network to feasibility\nof the proposed reactions, and a rollout policy network to sample transformations to estimate the\nvalue of a position, are trained with almost all reactions published in organic chemistry. The pro-\nposed approach improves previous computer-aided synthesis planning systems signiﬁcantly. Segler\net al. (2018) follow the approach of AlphaGo in Silver et al. (2016a). It is interesting to study if the\napproach of AlphaGo Zero (Silver et al., 2017), in particular, generalized policy iteration, self-play,\nand a single neural network, can be applied to retrosynthesis.\nPopova et al. (2018) apply deep RL for computational de novo drug design, discovering molecules\nwith desired properties. Jaques et al. (2017) as discussed below for music melody generation also\nstudy computational molecular generation.\n20.6.2 M ATHEMATICS\nDeep learning has many applications in maths, e.g., neural ordinary differential equations\n(ODEs) (Chen et al., 2018), proofs (Irving et al., 2016; Loos et al., 2017; Rockt ¨aschel and Riedel,\n2017; Urban et al., 2018). In the following, we discuss a case using deep RL for partial differential\nequations (PDEs).\nPDEs are mathematical tools for wide applications in science and engineering. It is desirable to\ndesign a PDE controller with minimal assumptions, without knowledge of the PDE, and being data-\ndriven. Pan et al. (2018) study how to control dynamical systems described by PDEs using RL\nmethods, with high-dimensional continuous action spaces, having spatial relationship among action\ndimensions. The authors propose action descriptors to encode such spatial regularities and to control\nsuch PDEs. The authors show sample efﬁciency of action descriptors theoretically, comparing with\nconventional RL methods not considering such regularities. The authors implement action descrip-\ntors with Deep Deterministic Policy Gradient (DDPG) (Lillicrap et al., 2016), and experiment with\ntwo high-dimensional PDE control problems.\n20.6.3 M USIC\nJaques et al. (2017) propose Sequence Tutor, combining maximum likelihood estimation (MLE)\nwith RL, to consider both data and task-related goals, to improve the structure and quality of gener-\nated sequences, and to maintain sample diversity and information learned from data, by pre-training\na Reward RNN using MLE, and training another RNN with off-policy RL, to generate sequences\nwith high quality, considering domain-speciﬁc rewards, and penalizing divergence from the prior\npolicy learned by Reward RNN. The authors investigate the connection between KL control and se-\nquence generation, and relationship among G-learning (Fox et al., 2016), \t-learning (Rawlik et al.,\n2012), and Q-learning. The authors evaluate Sequence Tutor on musical melody generation. It is\nnontrivial to design a reward function to capture the aesthetic beauty of generated melodies, and a\npure data-driven approach can not yield melodies with good structure. Sequence Tutor incorporates\n101\n\nrules from music theory into the model generating melodies, to produce more pleasant-sounding and\nsubjectively pleasing melodies than alternative methods. The authors also conduct experiments with\nSequence Tutor for computational molecular generation.\nvan den Oord et al. (2016) propose WaveNet for raw audio waveforms generation. See Briot et al.\n(2018) about deep learning techniques for music generation. See Zhu et al. (2018) for pop music\ngeneration. See also Dieleman et al. (2018). See the Magenta project, https://magenta.\ntensorflow.org , for investigation of deep learning and reinforcement learning for art and music\ncreation. See the 2018 ICML, IJCAI/ECAI, and AAMAS Joint Workshop on Machine Learning for\nMusic, https://sites.google.com/site/faimmusic2018/ .\n102\n\n21 D ISCUSSIONS\nWe present deep reinforcement learning in this manuscript in an overview style. We discuss the\nfollowing questions: Why deep? What is state of the art? What are the issues, and potential solu-\ntions? Brieﬂy, the powerful and ﬂexible representation by deep learning helps deep RL make many\nachievements. We discuss state of the art, issues and potential solutions for deep RL in chapters on\nsix core elements, six important mechanisms, and twelve applications. In the following, we present\na brief summary, discuss challenges and opportunities, and close with an epilogue.\n21.1 B RIEF SUMMARY\nThere are many concepts, algorithms, and issues in (deep) reinforcement learning (RL), as illustrated\nin Figure 5. We touch many of them in this manuscript.\nCredit assignment, sparse reward, and sample efﬁciency are common issues for RL problems. We\nlist several approaches proposed to address them in the following. In off-policy learning, both on-\npolicy and off-policy data can be used for learning. Auxiliary reward and self-supervised learning\nare for learning from non-reward signals in the environment. Reward shaping is for providing denser\nrewards. Hierarchical RL is for temporal abstraction. General value function, in particular, Horde,\nuniversal function approximator, and hindsight experience replay, is for learning shared representa-\ntion/knowledge among goals. Exploration techniques are for learning more from valuable actions.\nModel-based RL can generate more data to learn from. Learning to learn, e.g., one/zero/few-shot\nlearning, transfer learning, and multi-task learning, learns from related tasks to achieve efﬁcient\nlearning. Incorporating inductive bias, structure, and knowledge can help achieve more intelligent\nrepresentation and problem formulation. And so on and so forth.\nFinn (2017) and Levine (2018) discuss that sample efﬁciency improves roughly by ten times in each\nstep from one RL method to another in the following, from gradient-free methods, like CMA-ES,\nfully online methods like A3C, policy gradient methods, like TRPO, value estimation methods with\nreply buffer, like Q-learning, DQN, DDPG, and NAF, model-based deep RL methods, like guided\npolicy search (GPS), all the way to model-based ”shallow” RL methods, like PILCO.\nSilver (2018) summarizes principles of deep RL: evaluation drives progress, scalability determines\nsuccess, generality future-proofs algorithms, trust in the agent?s experience, state is subjective, con-\ntrol the stream, value functions model the world, planning: learn from imagined experience, em-\npower the function approximator, and, learn to learn.\nSome issues deserve more discussions, in particular, safety and interpretability. We discuss sev-\neral aspects of AI safety, e.g., reward in Chapter 5, multi-agent RL in Chapter 12, and, adver-\nsarial examples in Section 20.5. There are recent work on RL safety, e.g., Chow et al. (2018),\nHuang et al. (2018), and Wen and Topcu (2018). See surveys for AI safety (Amodei et al., 2016;\nGarc `ıa and Fern `andez, 2015). See a course on safety and control for artiﬁcial general intelligence,\nathttp://inst.eecs.berkeley.edu/ ˜cs294-149/ . See blogs about safety, e.g., at\nhttps://medium.com/@deepmindsafetyresearch . Doshi-Velez and Kim (2017), Lage\net al. (2018), Lipton (2018), and Melis and Jaakkola (2018) discuss issues of interpretability. Zhang\nand Zhu (2018) survey visual interpretability for deep learning.\n21.2 C HALLENGES AND OPPORTUNITIES\nIn the following, we discuss issues in deep RL, and propose research directions as both challenges\nand opportunities, which are challenging, or even widely open.\nRahimi and Recht (2017b) raise the concern that ”machine learning has become alchemy”. This\nalludes in particular to deep learning. See their blogs, Rahimi and Recht (2017c), Rahimi and\nRecht (2017a). In Sculley et al. (2018), Ali Rahimi and colleagues discuss productive changes\nfor empirical rigor, and recommend standards for empirical evaluation: tuning methodology, sliced\nanalysis, ablation studies, sanity checks and counterfactuals, and at least one negative result.\nLipton and Steinhardt (2018) discuss troubling trends in machine learning, including failure to dis-\ntinguish between explanation and speculation, failure to identify the sources of empirical gains,\nmathiness, which obfuscates or impresses but not clariﬁes with mathematics, and, misuse of lan-\n103\n\nstate drawing \naction discount factor \ntransition function \nmodel planning learning \nobservation \nrepresentation exploration \nexploitation \ndiscrete \ncontinuous \nenvironment Bellman \nequation dynamic \nprogramming (DP) Markov decision \nproblem (MDP) \nsingle agent \nmulti-agent \ncooperative \ncompetitive (non-)stationary deterministic \nstochastic episodic \ncontinuing \npolicy value function return \nreward goal\nagent state VF \n Vaction VF \nQSARSA actor \ncriticQ-learning TD\nlearning \nMonte Carlo \npolicy evaluation policy \nevaluation policy \nimprovement policy \niteration value \niteration \non-policy \noff-policy model-free \nmodel-based Dyna policy \ngradient \nMonte Carlo \ncontrol function \napproximation \nbootstrapping heuristic \nsearch attention \nmemory \ncredit \nassignment time/space/sample \nefficiency sparse \nreward \nstability \nconvergence credit \nassignment business \nmanagement \ngames \nmulti-armed \nbandit Bayesian \nRLsemi-supervised \nRL\nAutoML transfer \nlearning learning to learn \nmeta-learning learning \nreinforcement \nlearn learning \nto optimize few/one/zero \nshot learning multi-task \nlearning relational \nRLhierarchical \nRLmulti-agent \nRLunsupervised \nlearning finance NLP robotics computer \nvision \ntransportation healthcare education energy computer \nsystems \nscience maths physics chemistry music manufacturing \nmachine \nlearning \ncomputer \nscience maths economics operations \nresearch classical \nconditioning reward \nsystem \nengineering optimal \ncontrol \npsychology rationality \ngame theory \nneuroscience art engineering literature \nsafety interpretability Figure 5: Concepts, Algorithms, and Issues in Reinforcement Learning\n104\n\nguage, with suggestive deﬁnitions, overloading technical terminology, or suitcase words for a variety\nof meanings. See NIPS 2018 Workshop on Critiquing and Correcting Trends in Machine Learning.\nHenderson et al. (2018) investigate reproducibility, experimental techniques, and reporting proce-\ndures for deep RL. The authors show that experimental results are inﬂuenced by hyperparameters,\nincluding network architecture and reward scale, random seeds and trials, environments (like Hopper\nor HalfCheetah etc. in OpenAI Baseline), and codebases. This causes difﬁculties for reproducing\ndeep RL research results. The authors analyze the following reporting evaluation metrics: online\nview vs. policy optimization (ofﬂine), conﬁdence bounds (sample bootstrap), power analysis (about\nsample size), signiﬁcance (like t-test). Henderson et al. (2018) recommend to report implementation\ndetails, all hyperparameter settings, experimental setup, and evaluation methods for reproducibility.\nThe authors also recommend to ﬁnd the working set of hyperparameters, match baseline algorithm\nimplementation with the original codebase, run many trails with different random seeds then average\nresults, and perform proper signiﬁcance tests to validate better performance.\nKhetarpal et al. (2018) discuss evaluation differences in RL and in supervised learning, and propose\nan evaluation pipeline.\nLevine (2018) discusses challenges with deep RL, including stability, efﬁciency, scalability, hyper-\nparameters tuning, sample complexity, model-based learning, generalization, reward speciﬁcation,\nprior knowledge, etc.\nThese papers10discuss various issues with deep learning, machine learning, deep RL, and provide\nvaluable insights. There are also benchmark papers like Duan et al. (2016). However, we still\nlack papers conducting systematic, comparative study of deep RL algorithms, so that we pick one\nor more benchmark problems, do a thorough study, report both successes and failures, summarize\nadvices and lessons, and, give guidelines about how to use deep RL algorithms. Our deep RL\ncommunity need such papers. As well, most RL + NLP/computer vision papers use REINFORCE.\nA natural question is: how about other (deep) RL algorithms? We can evaluate performance of many\nalgorithms, like DQN, A3C, DDPG, TRPO, PPO, PCL, Trust-PCL, Retrace, Reactor, interpolated\npolicy gradient, soft Q-learning, etc. As such, we propose the following research direction.\nResearch Direction 1: systematic, comparative study of deep RL algorithms\nBellemare et al. (2018) open source Dopamine, aiming for a ﬂexible, stable, and reproducible\nTensorﬂow-based RL framework, as an achievement in this direction.\nWe have seen exciting results in two-player and multi-agent games recently. AlphaGo (Silver et al.,\n2016a; 2017) has achieved super-human performance. DeepStack (Morav ˇc´ık et al., 2017) defeated\nprofessional poker players. Jaderberg et al. (2018) achieve human level performance in the game of\nCapture the Flag (Chapter 12). OpenAI Five has beaten human players on 5v5 Dota 2, although with\nhuge computation ( https://blog.openai.com/openai-five/ ). Zambaldi et al. (2018)\nachieve decent results on StarCraft II mini-games (Chapter 13). Sun et al. (2018) and Pang et al.\n(2018) have beaten full-game built-in AI in StarCraft II.\nHowever, multi-agent problems are still very challenging, with issues like non-stationarity and even\ntheoretical infeasibility, as we discuss in Chapter 12. Even so, we can endeavour to achieve decent\nresults for multi-agent problems, like approximation solutions with high quality, and/or super-human\nperformance. Multi-agent systems are a great tool to model interactions among agents, with rich\napplications in human society; and their advancements can signiﬁcantly push the frontier of AI. We\nthus propose the second research direction as below.\nResearch Direction 2: ”solve” multi-agent problems\n10There is a blog titled Deep Reinforcement Learning Doesn’t Work Yet at https://www.alexirpan.\ncom/2018/02/14/rl-hard.html . It summarizes issues with deep RL, including sample inefﬁciency,\nbetter results with non-RL methods, issues with reward function, local optimal hard to escape, overﬁtting,\nand, hard to reproduce due to instability. The blog contains informative discussions; however, the title is\nwrong. There is another blog titled Lessons Learned Reproducing a Deep Reinforcement Learning Paper at\nhttp://amid.fish/reproducing-deep-rl .\n105\n\nStarCraft and Texas Hold’em Poker are great testbeds for studying multi-agent problems. It is\ndesirable to see extensions of DeepStack to multi-player settings, with many hands of playing, in\ntournament and cash game styles.\nStarCraft features many possible actions, complex interactions between players, short term tactics\nand long term strategies, etc. Learning strategies for Starcraft following videos with commentary\nwould be a feasible strategy. There are many videos about StarCraft with excellent commentaries. If\nwe may be able to extract valuable information, like strategies, from the multi-modality signals, and\napply these to the agent design, we may be able to achieve a human level AI StarCraft agent. Such a\nsystem would be an integration of RL, computer vision, and NLP. Aytar et al. (2018) achieve break-\nthrough results on three hard Atari games with self-supervision techniques by watching YouTube\n(Chapter 10). This may give us more motivation and encouragements. As a related work, Branavan\net al. (2012) propose to learn strategy games by reading manuals. With achievements in Sun et al.\n(2018) and Pang et al. (2018), it is interesting to watch if hierarchical RL approaches in these papers\ncan achieve super-human performance.\nWe now discuss end-to-end learning with raw inputs, a trendy paradigm recently, e.g.,\nAlexNet (Krizhevsky et al., 2012) with raw pixels for image classiﬁcation, Seq2Seq (Sutskever\net al., 2014) with raw sentences for machine translation, DQN (Mnih et al., 2015) with raw pixels\nand score to play Atari games, AlphaGo Zero (Silver et al., 2017) with piece information and score\nto play computer Go, and, Jaderberg et al. (2018) with raw pixels and score to play Quake III Arena\nCapture the Flag.\nOne question is, is such paradigm of end-to-end learning with raw input good? Sample efﬁciency is\nusually an issue. For example, as shown in Hessel et al. (2018), Rainbow needs 44 million frames\nto exceed the performance of distributional DQN (Bellemare et al., 2017), which needs much less\ndata than DQN (Mnih et al., 2015). Such huge amount of data require huge computation.\nAnother issue is adversarial examples, which may be more severe for critical applications. Szegedy\net al. (2013) show that various images, like a truck, a building, or a dog, after being added impercep-\ntible noises, are all classiﬁed by AlexNet as ”ostrich, Struthio camelus”. Eykholt et al. (2018) show\nthat physical images, e.g., stop signs, left turn signs etc., after being perturbed by adding black or\nwhite stickers, are misclassiﬁed by state of the art deep neural networks as speed limit 45 signs.\nSome papers propose to learn fully autonomously. AlphaGo Zero (Silver et al., 2017) and Jaderberg\net al. (2018) have achieved such a goal to some extend. However, we have to admit that both\ncomputer Go and Quake III Arena Capture the Flag have perfect simulation models, and unlimited\ndata can be generated relatively easily. Many practical applications, like robotics, healthcare, and,\neducation, do not have such luxury. We may or may not ultimately achieve such a goal of fully\nautonomous learning in a general sense; and it is not clear for problems with practical concerns.\nConsider education. Most of us follow some curricula designed by experts, and learn from experts,\nrather than learning tabula rasa. Even we will achieve such a goal, as in most scientiﬁc discoveries,\nwe may encounter spiral development, rather than going straightforwardly to the goal. We probably\nneed some hybrid solution in the interim.\nWe expect that manual engineering reconciles with end-to-end learning, and symbolism reconciles\nwith connectionism. We thus propose to add an ”intelligence” component in the end-to-end pro-\ncessing pipeline, rather than treating the system as an entire blackbox, as most current deep neural\nnetworks do, as shown in Figure 6.\ninput output update model \ninput output update model \nintelligence \nFigure 6: Add An Intelligence Component to End-to-End Pipeline\nIn the intelligence component, we may incorporate common knowledge like common sense, induc-\ntive bias, knowledge base, etc., common principles like Newton’s laws, Bellman equation, etc., and,\ncommon algorithms like gradient descent, TD learning, policy gradient, etc.\n106\n\nThe idea of adding an intelligent component is aligned with incorporating human intelligence as\ndiscussed in Lake et al. (2016). For example, when we study how to play billiard with a com-\nputer program, we probably want to incorporate Newton’s law, rather than using deep learning to\nrediscover such laws with many video clips. Graves et al. (2013) follows end-to-end training for\nspeech recognition, with a Fourier transformation of audio data. Self-supervised learning would be\na promising approach for adding this intelligence component, e.g., Jaderberg et al. (2017) and Aytar\net al. (2018).\nAdding an intelligence component is abstract. Now we discuss something more concrete, esp. for\ntasks with perception, like with visual inputs. We then propose the following research direction.\nResearch Direction 3: learn from entities, but not just raw inputs\nOur goal is to make the learning system more efﬁcient w.r.t. sample, time, and space, to achieve\ninterpretability and to avoid obvious mistakes like those in adversarial examples. At the same time,\nwe still strive for end-to-end processing, and being fully differentiable. Our thesis is that, if we could\nprocess the raw input with some principle or knowledge, the resulting representation would be more\nconvenient for the learning system to make further predictions or decisions.\nTake the hard Atari game Montezuma’s Revenge as an example. Suppose there were an intelligent\nsystem, which could identify entities in video frames, like agent, road, ladder, enemy, key, door,\netc., and their attributes and relationships. Then a RL agent working on such representation would\nbe much more efﬁcient than working on pixels. A question is, if RL, unsupervised learning, or\nsome machine learning/AI techniques can help identify entities, attributes, and their relationships.\nSuccesses in this direction would hinge on the maturity of computer vision, NLP, and AI.\nThere are recent progress in this direction. Goel et al. (2018) conduct unsupervised video object\nsegmentation for deep RL. Eslami et al. (2018) present GQN for discovering representations with\nunsupervised learning. Chen et al. (2018a) propose a framework for iterative visual reasoning. For\nNLP, word2vec (Mikolov et al., 2013; Mikolov et al., 2017) is probably the most popular represen-\ntation. van den Oord et al. (2018) propose to learn representations for multi-modality, including\nspeech, images, text, and reinforcement learning. There are some recent papers about reasoning,\ne.g. (Santoro et al., 2017; Hudson and Manning, 2018; Battaglia et al., 2018), as we discuss in\nChapter 13. We also discuss knowledge and reasoning in Section 8.3.\nMalik (2018) discusses that there are great achievements in the ﬁelds of vision, motor control, and\nlanguage semantic reasoning, and it is time to investigate them together. This supports our proposal.\nAnother fundamental, and related issue is about representation for RL problems. In deep learning,\nas well as in deep RL, neural network architecture is critical for the performance.\nThere are classical ways for function approximation, several popular neural network architectures,\nmechanisms for temporal abstraction, neural network architectures designed for deep RL and/or for\nreasoning, and discussions about causality and human intelligence. We discuss such representation\nissues in Chapter 8.\nWhen we talk about computer vision with deep learning, CNNs appear in many people’s minds.\nWhen we talk about RL algorithms, many people think about TD learning, Q-learning, and policy\ngradient. However, when we talk about representation or neural network architecture for (deep) RL,\ndifferent people may come up with different ideas. It would be great to discover something for RL\nlike CNNs for computer vision. We thus propose the following research direction.\nResearch Direction 4: design an optimal representation for RL\nRL problems have their own structures and characteristics, e.g., value functions satisfy Bellman\nequation, so that they are probably different from those in deep learning, like image recognition and\nmachine translation. Consequently, RL problems probably have their own optimal representation\nand neural network architecture. We conjecture that it is desirable to consider a holistic approach,\ni.e., considering perception and control together, rather than separately. Srinivas et al. (2018) and\nTamar et al. (2018) are efforts in this direction.\n107\n\nTo go one step further, we propose the next research direction to automate reinforcement learning,\nnamely, AutoRL. A successful AutoRL tool would help us choose optimal state representation,\nfunction approximator, learning algorithms, hyperparameters, etc. There are efforts currently for\nmachine learning tasks, namely, AutoML, as we discuss in Section 14.6.\nResearch Direction 5: AutoRL\nWe now talk about the last research direction. Reinforcement learning is very powerful and very\nimportant. Quoted from the authoritative AI textbook (Russell and Norvig, 2009), ”reinforcement\nLearning might be considered to encompass all of AI: an agent is placed in an environment and must\nlearn to behave successfully therein”, and, ”reinforcement learning can be viewed as a microcosm\nfor the entire AI problem”. Moreover, David Silver proposes a conjecture: artiﬁcial intelligence = re-\ninforcement learning + deep learning (Silver, 2016). We attempt to justify this conjecture as follows.\nHornik et al. (1989) establish that multilayer feedforward networks are universal approximators; that\nis, a feedback neural network with a single layer is sufﬁcient to approximate any continuous function\nto an arbitrary precision. Hutter (2005) proves that tasks with computable descriptions in computer\nscience can be formulated as RL problems. With deep learning providing mechanisms, and rein-\nforcement learning deﬁning the objective and achieving it, their integration can solve computable\ntasks, the aim of AI. Note the number of units in the hidden layer may be infeasibly large though,\nand, computability may be an issue, unless P = NP.\nHowever, we see that deep learning is used much more widely, in supervised learning, unsupervised\nlearning, and reinforcement learning. Furthermore, deep learning is also widely used in many appli-\ncations, and is the core technique for many commercial products, like those with face recognition,\nspeech recognition, etc. We enjoy the successes of deep RL, like those in Atari games and computer\nGo, which probably have limited commercial value. We see successes of a special RL family of tech-\nniques, namely, multi-armed bandits, for applications like news recommendation (Li et al., 2010).\nWe also see achievements like those for neural architecture design (Zoph and Le, 2017). However,\nreinforcement learning still needs more efforts to become more practical, and we are still lacking\nof wide and practical applications of reinforcement learning that generate considerable commercial\nvalue. We thus propose the following research direction.\nResearch Direction 6: develop killer applications for (deep) RL\nSuccesses of this research direction require the maturity of RL algorithms, for efﬁciency, stability,\nand robustness, etc. We see a positive feedback loop between algorithms and applications; they will\nhelp each other to make further improvements.\nWe now discuss a concrete recommendation for this direction: it is promising to invest more on\napplying AlphaGo techniques. AlphaGo techniques, in particular, deep learning, reinforcement\nlearning, MCTS, and self play, are successful techniques, and have many potential applications. In\nparticular, the elegant algorithm of AlphaGo Zero (Silver et al., 2017) would be straightforwardly\napplicable to a big family of problems.\nWe list potential applications of AlphaGo as suggested by the authors in their papers (Silver et al.,\n2016a; 2017), namely, general game-playing (in particular, video games) classical planning, partially\nobserved planning, scheduling, constraint satisfaction, robotics, industrial control, online recom-\nmendation systems, protein folding, reducing energy consumption, and searching for revolutionary\nnew materials. Although AlphaGo techniques have limitations, like requiring a good or even perfect\nsimulator, we expect to see more and more application of AlphaGo techniques.\nWe list six research directions, as both challenges and opportunities of deep RL. Research direc-\ntion 1, systematic, comparative study of deep RL algorithms, is about reproducibility, and under\nthe surface, about stability and convergence properties of deep RL algorithms. Research direction\n2, ”solve” multi-agent problems, is usually about sample efﬁciency, sparse reward, stability, non-\nstationarity, and convergence in a large-scale, complex setting, a frontier in AI research. Research\ndirection 3, learn from entities, but not just raw inputs, is about sample efﬁciency, sparse reward,\nand interpretability, by incorporating more knowledge, structure, and inductive bias. Research di-\nrection 4, design an optimal representation for RL, research direction 5, AutoRL, and, research\n108\n\ndirection 6, develop killer applications for (deep) RL, are about the whole RL problem, about all\nissues in RL, like credit assignment, sparse reward, time/space/sample efﬁciency, accuracy, stabil-\nity, convergence, interpretability, safety, scalability, robustness, simplicity, etc, from different angles\nof representation, automation, and application, respectively. We expect all these research directions\nto be open, except the ﬁrst one, which is also challenging, and progress in these directions would\ndeepen our understanding of (deep) RL, and push further frontiers of AI.\n21.3 E PILOGUE\nIt is both the best and the worst of times for the ﬁeld of deep RL, for the same reason: it has\nbeen growing so fast and so enormously. We have been witnessing breakthroughs, exciting new\nalgorithms, architectures, and applications, and we expect to see much more and much faster. As\na consequence, this manuscript is incomplete, in the sense of both depth and width. However, we\nattempt to summarize important achievements and discuss potential directions and applications in\nthis amazing ﬁeld.\nValue function is central to reinforcement learning, e.g., in Deep Q-Network and its many exten-\nsions. Policy optimization approaches have been gaining traction, with many new algorithms, and\nin many, diverse applications, e.g., robotics, neural architecture design, spoken dialogue systems,\nmachine translation, attention, and learning to learn, etc. This list is boundless. New learning mech-\nanisms have emerged, e.g., using learning to learn, unsupervised learning, self-supervised learning,\netc., to improve the quality and speed of learning, and more new mechanisms will be emerging.\nThis is the renaissance of reinforcement learning (Krakovsky, 2016). In fact, deep learning and\nreinforcement learning have been making steady progress even during the last AI winter.\nWe have seen breakthroughs about deep RL, including DQN (Mnih et al., 2015), AlphaGo (Silver\net al., 2016a; 2017), and DeepStack (Morav ˇc´ık et al., 2017).\nExciting achievements abound: differentiable neural computer (Graves et al., 2016), unsupervised\nreinforcement and auxiliary learning (Jaderberg et al., 2017), asynchronous methods (Mnih et al.,\n2016), guided policy search (Levine et al., 2016), generative adversarial imitation learning (Ho and\nErmon, 2016), and neural architecture design (Zoph and Le, 2017), etc. There are also many recent,\nnovel applications of (deep) RL in many, diverse areas as discussed in previous chapters. Creativity\nwould push the frontiers of deep RL further with respect to core elements, important mechanisms,\nand applications. In general, RL is probably helpful, if a problem can be regarded as or transformed\nto a sequential decision making problem, and states, actions, maybe rewards, can be constructed.\nRoughly speaking, if a task involves some manual designed ”strategy”, then there is a chance for\nreinforcement learning to help to automate and optimize the strategy.\nHaving a better understanding of how deep learning works is helpful for deep learning, machine\nlearning, and AI. Poggio et al. (2017) review why and when deep- but not shallow-networks can\navoid the curse of dimensionality. See Stanford STATS 385 course on Theories of Deep Learning at\nhttps://stats385.github.io . See Arora (2018) about theoretical understanding of deep\nlearning. There are also papers for interpretability of deep learning, e.g. Doshi-Velez and Kim\n(2017), Lipton (2018), and Zhang and Zhu (2018).\nIt is important to investigate comments/criticisms for further progress. A popular criticism about\ndeep learning is that it is a blackbox, or even an ”alchemy” during the NIPS 2017 Test of Time Award\nspeech (Rahimi and Recht, 2007). Lake et al. (2016) discuss incorporating machine intelligence\nwith human intelligence for stronger AI; one commentary, Botvinick et al. (2017), discusses the\nimportance of autonomy. Jordan (2018) discusses issues with AI. Darwiche (2018) discusses deep\nlearning in the context of AI. See Peter Norvig’s perspective (Press, 2016). Marcus (2018) criticizes\ndeep learning, and Dietterich (2018) responds. Watch two debates, LeCun and Marcus (2017),\nLeCun and Manning (2018). See Stoica et al. (2017) for systems challenges for AI.\nIt is worthwhile to envision deep RL considering perspectives from the society, academia and in-\ndustry on AI, e.g., Artiﬁcial Intelligence, Automation, and the Economy, Executive Ofﬁce of the\nPresident, USA; Artiﬁcial Intelligence and Life in 2030 - One Hundred Year Study on Artiﬁcial\nIntelligence: Report of the 2015-2016 Study Panel, Stanford University (Stone et al., 2016); and AI,\nMachine Learning and Data Fuel the Future of Productivity by The Goldman Sachs Group, Inc.,\netc. Brynjolfsson and Mitchell (2017) and Mitchell and Brynjolfsson (2017) discuss implications\n109\n\nof AI and machine learning for workforce. There are also many articles, e.g., in Harvard Business\nReview, like Agrawal et al. (2017), Ng (2016a), and Ng (2016c). See recent books about the science\nand technology of AI and machine learning, and their implications for business and society, e.g.,\nAgrawal et al. (2018), Domingos (2015), and Lee (2018).\nNature in May 2015 and Science in July 2015 featured survey papers on machine learning and AI.\nScience Robotics was launched in 2016. Science has a special issue on July 7, 2017 about AI on\nThe Cyberscientist. Nature Machine Intelligence was launched in January 2019. These illustrate the\napparent importance of AI. It is interesting to mention that NIPS 2018 main conference was sold out\nin less than 12 minutes after opening for registration; see (Li, 2018) .\nDeep learning was among MIT Technology Review 10 Breakthrough Technologies in 2013. We\nhave been witnessing the dramatic development of deep learning in both academia and industry in\nthe last few years. Reinforcement learning was among MIT Technology Review 10 Breakthrough\nTechnologies in 2017. Deep learning has made many achievements, has ”conquered” speech recog-\nnition, computer vision, and now NLP, is more mature and well-accepted, and has been validated by\nproducts and market. In contrast, RL has lots of (potential yet promising) applications, yet not many\nwide-spread products so far. RL may still need better algorithms, and may still need products and\nmarket validation. Prediction is very difﬁcult, especially about the future. However, it is probably\nthe right time to nurture, educate and lead the market for reinforcement learning. We will see both\ndeep learning and reinforcement learning prospering in the coming years and beyond.\nDeep learning, in this third wave of AI, will have deeper inﬂuences, as we have already seen from\nits many achievements. Reinforcement learning, as a more general learning and decision making\nparadigm, will deeply inﬂuence deep learning, machine learning, and artiﬁcial intelligence in gen-\neral. It is interesting to mention that when Professor Rich Sutton started working in the University\nof Alberta in 2003, he named his lab RLAI: Reinforcement Learning and Artiﬁcial Intelligence.\n110\n\nThese abbreviations are used for frequently cited conferences and journals.\nAAAI the AAAI Conference on Artiﬁcial Intelligence\nACL the Association for Computational Linguistics Annual Meeting\nAAMAS the International Conference on Autonomous Agents & Multiagent Systems\nArXiv ArXiv e-prints\nCCS the ACM Conference on Computer and Communications Security\nCVPR the IEEE Conference on Computer Vision and Pattern Recognition\nEMNLP the Conference on Empirical Methods in Natural Language Processing\nICCV the IEEE International Conference on Computer Vision\nICLR the International Conference on Learning Representations\nICML the International Conference on Machine Learning\nICRA IEEE International Conference on Robotics and Automation\nIJCAI the International Joint Conference on Artiﬁcial Intelligence\nIROS International Conference on Intelligent Robots\nJAIR Journal of Artiﬁcial Intelligence Research\nJMLR the Journal of Machine Learning Research\nKDD the ACM International Conference on Knowledge Discovery and Data Mining\nNAACL the Annual Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies\nNIPS the Annual Conference on Neural Information Processing Systems\nRSS Robotics: Science and Systems\nTNN IEEE Transactions on Neural Networks\nTPAMI IEEE Transactions on Pattern Analysis and Machine Intelligence\nUAI the Conference on Uncertainty in Artiﬁcial Intelligence\nWWW the International World Wide Web Conference\n111\n\nREFERENCES\nAbbeel, P. (2017a). Deep learning for robotics. http://goo.gl/oVonGS . NIPS 2017 Invited Talk.\nAbbeel, P. (2017b). Reinforcement learning - policy optimization. https://mila.quebec/en/cours/\ndeep-learning-summer-school-2017/ . Deep Learning and Reinforcement Learning Summer\nSchool 2017.\nAbbeel, P. and Ng, A. Y . (2004). Apprenticeship learning via inverse reinforcement learning. In ICML .\nAbdolmaleki, A., Lioutikov, R., Lau, N., Reis, L. P., Peters, J., and Neumann, G. (2015). Model-based relative\nentropy stochastic search. In NIPS .\nAgarwal, D., Chen, B.-C., He, Q., Obukhov?, M., Yang, J., and Zhang, L. (2018). End to-end goal-oriented\nquestion answering systems. https://sites.google.com/view/goal-oriented-qa/ . KDD\n2018 Tutorial.\nAgrawal, A., Gans, J., and Goldfarb, A. (2017). How AI will change the way we make decisions. https://\nhbr.org/2017/07/how-ai-will-change-the-way-we-make-decisions . Harvard Busi-\nness Review.\nAgrawal, A., Gans, J., and Goldfarb, A. (2018). Prediction Machines: The Simple Economics of Artiﬁcial\nIntelligence . Harvard Business Review Press.\nAgrawal, P., Nair, A., Abbeel, P., Malik, J., and Levine, S. (2016). Learning to poke by poking: Experiential\nlearning of intuitive physics. In NIPS .\nAl-Shedivat, M., Bansal, T., Burda, Y ., Sutskever, I., Mordatch, I., and Abbeel, P. (2018). Continuous adaptation\nvia meta-learning in nonstationary and competitive environments. In ICLR .\nAlberg, J. and Lipton, Z. C. (2017). Improving factor-based quantitative investing by forecasting company\nfundamentals. In NIPS 2017 Time Series Workshop .\nAlbrechta, S. V . and Stone, P. (2018). Autonomous agents modelling other agents: A comprehensive survey\nand open problems. Artiﬁcial Intelligence .\nAlsheikh, M. A., Lin, S., Niyato, D., and Tan, H.-P. (2014). Machine learning in wireless sensor networks:\nAlgorithms, strategies, and applications. IEEE Communications Surveys & Tutorials , 16(4):1996–2018.\nAmin, K., Jiang, N., and Singh, S. (2017). Repeated inverse reinforcement learning. In NIPS .\nAmodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and Man ´e, D. (2016). Concrete Problems in\nAI Safety. ArXiv .\nAmos, B., Jimenez, I., Sacks, J., Boots, B., and Kolter, J. Z. (2018). Differentiable MPC for end-to-end planning\nand control. In NIPS .\nAnderson, H. S., Kharkar, A., Filar, B., Evans, D., and Roth, P. (2018). Learning to Evade Static PE Machine\nLearning Malware Models via Reinforcement Learning. ArXiv .\nAnderson, P., Das, A., and Wu, Q. (2018). Connecting language and vision to actions. https://\nlvatutorial.github.io . ACL 2018 Tutorial.\nAnderson, R. N., Boulanger, A., Powell, W. B., and Scott, W. (2011). Adaptive stochastic control for the smart\ngrid. Proceedings of the IEEE , 99(6):1098–1115.\nAndreas, J., Klein, D., and Levine, S. (2017). Modular multitask reinforcement learning with policy sketches.\nInICML .\nAndrychowicz, M., Denil, M., Colmenarejo, S. G., Hoffman, M. W., Pfau, D., Schaul, T., Shillingford, B., and\nde Freitas, N. (2016). Learning to learn by gradient descent by gradient descent. In NIPS .\nAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Abbeel,\nP., and Zaremba, W. (2017). Hindsight experience replay. In NIPS .\nAnschel, O., Baram, N., and Shimkin, N. (2017). Averaged-DQN: Variance reduction and stabilization for deep\nreinforcement learning. In ICML .\nArgall, B. D., Chernova, S., Veloso, M., and Browning, B. (2009). A survey of robot learning from demonstra-\ntion. Robotics and Autonomous Systems , 57(5):469–483.\n112\n\nArjovsky, M., Chintala, S., and Bottou, L. (2017). Wasserstein GAN. ArXiv .\nArora, S. (2018). Toward theoretical understanding of deep learning. http://unsupervised.cs.\nprinceton.edu/deeplearningtutorial.html . ICML 2018 Tutorial.\nArtetxe, M., Labaka, G., Agirre, E., and Cho, K. (2018). Unsupervised neural machine translation. In ICLR .\nAsri, L. E., He, J., and Suleman, K. (2016). A sequence-to-sequence model for user simulation in spoken\ndialogue systems. In Annual Meeting of the International Speech Communication Association (INTER-\nSPEECH) .\nAthalye, A., Carlini, N., and Wagner, D. (2018). Obfuscated gradients give a false sense of security: Circum-\nventing defenses to adversarial examples. In ICML .\nAudiffren, J., Valko, M., Lazaric, A., and Ghavamzadeh, M. (2015). Maximum entropy semi-supervised inverse\nreinforcement learning. In IJCAI .\nAuer, P. (2002). Using conﬁdence bounds for exploitation-exploration trade-offs. JMLR , 3:397–422.\nAuer, P., Cesa-Bianchi, N., Freund, Y ., and Schapire, R. E. (2002). The nonstochastic multiarmed bandit\nproblem. SIAM Journal on Computing , 32(1):48–77.\nAytar, Y ., Pfaff, T., Budden, D., Paine, T., Wang, Z., and de Freitas, N. (2018). Playing hard exploration games\nby watching YouTube. In NIPS .\nAzar, M. G., Osband, I., and Munos, R. (2017). Minimax regret bounds for reinforcement learning. In ICML .\nBa, J., Hinton, G. E., Mnih, V ., Leibo, J. Z., and Ionescu, C. (2016). Using fast weights to attend to the recent\npast. In NIPS .\nBa, J., Kiros, J. R., and Hinton, G. E. (2016). Layer Normalization. ArXiv .\nBa, J., Mnih, V ., and Kavukcuoglu, K. (2014). Multiple object recognition with visual attention. In ICLR .\nBabaeizadeh, M., Frosio, I., Tyree, S., Clemons, J., and Kautz, J. (2017). Reinforcement learning through\nasynchronous advantage actor-critic on a gpu. In ICLR .\nBacon, P.-L., Harb, J., and Precup, D. (2017). The option-critic architecture. In AAAI .\nBahdanau, D., Brakel, P., Xu, K., Goyal, A., Lowe, R., Pineau, J., Courville, A., and Bengio, Y . (2017). An\nactor-critic algorithm for sequence prediction. In ICLR .\nBahdanau, D., Cho, K., and Bengio, Y . (2015). Neural machine translation by jointly learning to align and\ntranslate. In ICLR .\nBai, S., Zico Kolter, J., and Koltun, V . (2018). An Empirical Evaluation of Generic Convolutional and Recurrent\nNetworks for Sequence Modeling. ArXiv .\nBailis, P., Olukoton, K., Re, C., and Zaharia, M. (2017). Infrastructure for Usable Machine Learning: The\nStanford DAWN Project. ArXiv .\nBaird, L. (1995). Residual algorithms: Reinforcement learning with function approximation. In ICML .\nBaker, B., Gupta, O., Naik, N., and Raskar, R. (2017). Designing neural network architectures using reinforce-\nment learning. In ICLR .\nBalog, M., Gaunt, A. L., Brockschmidt, M., Nowozin, S., and Tarlow, D. (2017). Deepcoder: Learning to write\nprograms. In ICLR .\nBanino, A., Barry, C., Uria, B., Blundell, C., Lillicrap, T., Mirowski, P., Pritzel, A., Chadwick, M. J., Degris, T.,\nModayil, J., Wayne, G., Soyer, H., Viola, F., Zhang, B., Goroshin, R., Rabinowitz, N., Pascanu, R., Beattie,\nC., Petersen, S., Sadik, A., Gaffney, S., King, H., Kavukcuoglu, K., Hassabis, D., Hadsell, R., and Kumaran,\nD. (2018). Vector-based navigation using grid-like representations in artiﬁcial agents. Nature , 557:429–433.\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2018). Emergent complexity via multi-agent\ncompetition. In ICLR .\nBao, W., Yue, J., and Rao, Y . (2017). A deep learning framework for ﬁnancial time series using stacked\nautoencoders and long- short term memory. PLoS ONE , 12(7).\n113\n\nBarreto, A., Munos, R., Schaul, T., and Silver, D. (2017). Successor features for transfer in reinforcement\nlearning. In NIPS .\nBarth-Maron, G., Hoffman, M. W., Budden, D., Dabney, W., Horgan, D., TB, D., Muldal, A., Heess, N., and\nLillicrap, T. (2018). Distributed distributional deterministic policy gradients. In ICLR .\nBarto, A. (2013). Intrinsic motivation and reinforcement learning. In Baldassarre, G. and Mirolli, M., editors,\nIntrinsically Motivated Learning in Natural and Artiﬁcial Systems . Springer, Berlin, Heidelberg.\nBarto, A. (2017). Intrinsically motivated reinforcement learning. http://goo.gl/Q477g2 .\nBarto, A. (2018). A history of reinforcement learning. https://www.youtube.com/watch?v=\nul6B2oFPNDM . IJCAI-17 Award for Research Excellence.\nBarto, A. G. and Mahadevan, S. (2003). Recent advances in hierarchical reinforcement learning. Discrete\nEvent Dynamic Systems , 13(4):341–379.\nBarto, A. G., Sutton, R. S., and Anderson, C. W. (1983). Neuronlike adaptive elements that can solve difﬁcult\nlearning control problems. IEEE Transactions on Systems, Man, and Cybernetics , 13(5):835–846.\nBattaglia, P. W., Hamrick, J. B., Bapst, V ., Sanchez-Gonzalez, A., Zambaldi, V ., Malinowski, M., Tacchetti, A.,\nRaposo, D., Santoro, A., Faulkner, R., Gulcehre, C., Song, F., Ballard, A., Gilmer, J., Dahl, G., Vaswani, A.,\nAllen, K., Nash, C., Langston, V ., Dyer, C., Heess, N., Wierstra, D., Kohli, P., Botvinick, M., Vinyals, O.,\nLi, Y ., and Pascanu, R. (2018). Relational inductive biases, deep learning, and graph networks. ArXiv .\nBattaglia, P. W., Pascanu, R., Lai, M., Rezende, D., and Kavukcuoglu, K. (2016). Interaction networks for\nlearning about objects, relations and physics. In NIPS .\nBazzan, A. L. and Kl ¨ugl, F. (2014). Introduction to Intelligent Systems in Trafﬁc and Transportation . Morgan\n& Claypool.\nBeattie, C., Leibo, J. Z., Teplyashin, D., Ward, T., Wainwright, M., K ¨uttler, H., Lefrancq, A., Green, S., Vald ´es,\nV ., Sadik, A., Schrittwieser, J., Anderson, K., York, S., Cant, M., Cain, A., Bolton, A., Gaffney, S., King, H.,\nHassabis, D., Legg, S., and Petersen, S. (2016). DeepMind Lab. ArXiv .\nBellemare, M. G., Castro, P. S., Gelada, C., Kumar, S., and Moitra, S. (2018). Dopamine. https://\ngithub.com/google/dopamine . A blog at http://goo.gl/ZETkG6 .\nBellemare, M. G., Dabney, W., and Munos, R. (2017). A distributional perspective on reinforcement learning.\nInICML .\nBellemare, M. G., Danihelka, I., Dabney, W., Mohamed, S., Lakshminarayanan, B., Hoyer, S., and Munos, R.\n(2017). The Cramer Distance as a Solution to Biased Wasserstein Gradients. ArXiv .\nBellemare, M. G., Naddaf, Y ., Veness, J., and Bowling, M. (2013). The arcade learning environment: An\nevaluation platform for general agents. JAIR , 47:253–279.\nBellemare, M. G., Schaul, T., Srinivasan, S., Saxton, D., Ostrovski, G., and Munos, R. (2016). Unifying\ncount-based exploration and intrinsic motivation. In NIPS .\nBelletti, F., Haziza, D., Gomes, G., and Bayen, A. M. (2018). Expert level control of ramp metering based\non multi-task deep reinforcement learning. IEEE Transactions on Intelligent Transportation Systems ,\n19(4):1198–1207.\nBellman, R. (1978). An Introduction to Artiﬁcial Intelligence . Boyd & Fraser.\nBello, I., Pham, H., Le, Q. V ., Norouzi, M., and Bengio, S. (2016). Neural Combinatorial Optimization with\nReinforcement Learning. ArXiv .\nBello, I., Zoph, B., Vasudevan, V ., and Le, Q. V . (2017). Neural optimizer search with reinforcement learning.\nInICML .\nBengio, Y . (2018). From deep learning of disentangled representations to higher-level cognition. https:\n//www.youtube.com/watch?v=Yr1mOzC93xs .\nBengio, Y ., Bengio, S., and Cloutier, J. (1991). Learning a synaptic learning rule. In International Joint\nConference on Neural Networks (IJCNN) .\nBengio, Y ., Courville, A., and Vincent, P. (2013). Representation learning: A review and new perspectives.\nTPAMI , 35(8):1798–1828.\n114\n\nBengio, Y ., Louradour, J., Collobert, R., and Weston, J. (2009). Curriculum learning. In ICML .\nBerkenkamp, F., Turchetta, M., Schoellig, A. P., and Krause, A. (2017). Safe model-based reinforcement\nlearning with stability guarantees. In NIPS .\nBerthelot, D., Schumm, T., and Metz, L. (2017). BEGAN: Boundary Equilibrium Generative Adversarial\nNetworks. ArXiv .\nBertsekas, D. P. (2012). Dynamic programming and optimal control (Vol. II, 4th Edition: Approximate Dynamic\nProgramming) . Athena Scientiﬁc, Massachusetts, USA.\nBertsekas, D. P. and Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming . Athena Scientiﬁc.\nBesold, T. R., d’Avila Garcez, A., Bader, S., Bowman, H., Domingos, P., Hitzler, P., Kuehnberger, K.-U.,\nLamb, L. C., Lowd, D., Machado Vieira Lima, P., de Penning, L., Pinkas, G., Poon, H., and Zaverucha, G.\n(2017). Neural-Symbolic Learning and Reasoning: A Survey and Interpretation. ArXiv .\nBhatti, S., Desmaison, A., Miksik, O., Nardelli, N., Siddharth, N., and Torr, P. H. S. (2016). Playing Doom\nwith SLAM-Augmented Deep Reinforcement Learning. ArXiv .\nBiamonte, J., Wittek, P., Pancotti, N., Rebentrost, P., Wiebe, N., and Lloyd, S. (2017). Quantum machine\nlearning. Nature , 549:195–202.\nBishop, C. (2011). Pattern Recognition and Machine Learning . Springer.\nBlei, D. M. and Smyth, P. (2017). Science and data science. PNAS , 114(33):8689–8692.\nBohg, J., Hausman, K., Sankaran, B., Brock, O., Kragic, D., Schaal, S., and Sukhatme, G. S. (2017). Interactive\nperception: Leveraging action in perception and perception in action. IEEE Transactions on Robotics ,\n33(6):1273–1291.\nBojarski, M., Testa, D. D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., Jackel, L. D., Monfort, M.,\nMuller, U., Zhang, J., Zhang, X., Zhao, J., and Zieba, K. (2016). End to End Learning for Self-Driving Cars.\nArXiv .\nBojarski, M., Yeres, P., Choromanska, A., Choromanski, K., Firner, B., Jackel, L., and Muller, U. (2017).\nExplaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car. ArXiv .\nBordes, A., Boureau, Y .-L., and Weston, J. (2017). Learning end-to-end goal-oriented dialog. In ICLR .\nBottou, L. (2014). From machine learning to machine reasoning. Machine Learning , 94(2):133–149.\nBottou, L., Curtis, F. E., and Nocedal, J. (2018). Optimization methods for large-scale machine learning. SIAM\nReview , 60(2):223–311.\nBotvinick, M., Barrett, D. G. T., Battaglia, P., de Freitas, N., Kumaran, D., Leibo, J. Z., Lillicrap, T., Modayil,\nJ., Mohamed, S., Rabinowitz, N. C., Rezende, D. J., Santoro, A., Schaul, T., Summerﬁeld, C., Wayne, G.,\nWeber, T., Wierstra, D., Legg, S., and Hassabis, D. (2017). Building machines that learn and think for\nthemselves. Behavioral and Brain Sciences , 40.\nBousmalis, K., Irpan, A., Wohlhart, P., Bai, Y ., Kelcey, M., Kalakrishnan, M., Downs, L., Ibarz, J., Pastor, P.,\nKonolige, K., Levine, S., and Vanhoucke, V . (2017). Using Simulation and Domain Adaptation to Improve\nEfﬁciency of Deep Robotic Grasping. ArXiv .\nBowling, M., Burch, N., Johanson, M., and Tammelin, O. (2015). Heads-up limit hold’em poker is solved.\nScience , 347(6218):145–149.\nBowling, M. and Veloso, M. (2002). Multiagent learning using a variable learning rate. Artiﬁcial Intelligence ,\n136:215–250.\nBoyd, S. and Vandenberghe, L. (2004). Convex Optimization . Cambridge University Press.\nBrachman, R. and Levesque, H. (2004). Knowledge Representation and Reasoning . Morgan Kaufmann.\nBradtke, S. J. and Barto, A. G. (1996). Linear least-squares algorithms for temporal difference learning. Ma-\nchine Learning , 22(1-3):33–57.\nBrafman, R. I. and Tennenholtz, M. (2002). R-MAX - a general polynomial time algorithm for near-optimal\nreinforcement learning. JMLR , 3:213–231.\n115\n\nBranavan, S. R. K., Silver, D., and Barzilay, R. (2012). Learning to win by reading manuals in a monte-carlo\nframework. JAIR , 43.\nBrandt, M. W., Goyal, A., Santa-Clara, P., and Stroud, J. R. (2005). A simulation approach to dynamic port-\nfolio choice with an application to learning about return predictability. The Review of Financial Studies ,\n18(3):831–873.\nBrazdil, P., Carrier, C. G., Soares, C., and Vilalta, R. (2009). Metalearning: Applications to Data Mining .\nSpringer.\nBriot, J.-P., Hadjeres, G., and Pachet, F. (2018). Deep Learning Techniques for Music Generation . Springer\nInternational Publishing.\nBritz, D. (2016). Attention and memory in deep learning and nlp. http://www.wildml.com/2016/01/\nattention-and-memory-in-deep-learning-and-nlp/ .\nBrock, A., Donahue, J., and Simonyan, K. (2018). Large Scale GAN Training for High Fidelity Natural Image\nSynthesis. ArXiv e-prints .\nBrown, N. and Sandholm, T. (2017a). Safe and nested subgame solving for imperfect-information games. In\nNIPS .\nBrown, N. and Sandholm, T. (2017b). Superhuman ai for heads-up no-limit poker: Libratus beats top profes-\nsionals. Science .\nBrowne, C., Powley, E., Whitehouse, D., Lucas, S., Cowling, P. I., Rohlfshagen, P., Tavener, S., Perez, D.,\nSamothrakis, S., and Colton, S. (2012). A survey of Monte Carlo tree search methods. IEEE Transactions\non Computational Intelligence and AI in Games , 4(1):1–43.\nBrunner, G., Richter, O., Wang, Y ., and Wattenhofer, R. (2018). Teaching a machine to read maps with deep\nreinforcement learning. In AAAI .\nBrunskill, E. (2017). Reinforcement learning with people. https://www.facebook.com/\nnipsfoundation/videos/1555771847847382/ . NIPS 2017 Tutorial.\nBrynjolfsson, E. and Mitchell, T. (2017). What can machine learning do? workforce implications. Science ,\n358(6370):1530–1534.\nBucila, C., Caruana, R., and Niculescu-Mizil, A. (2006). Model compression. In KDD .\nBuckman, J., Hafner, D., Tucker, G., Brevdo, E., and Lee, H. (2018). Sample-efﬁcient reinforcement learning\nwith stochastic ensemble value expansion. In NIPS .\nBuro, M. (1999). How machines have learned to play othello. IEEE Intelligent Systems Journal , 14(6):12–14.\nBusoniu, L., Babuska, R., and Schutter, B. D. (2008). A comprehensive survey of multiagent reinforcement\nlearning. IEEE Transactions on Systems, Man, and Cybernetics - Part C: Applications and Reviews , 38(2).\nCai, H., Yang, J., Zhang, W., Han, S., and Yu, Y . (2018a). Path-level network transformation for efﬁcient\narchitecture search. In ICML .\nCai, Q., Filos-Ratsikas, A., Tang, P., and Zhang, Y . (2018b). Reinforcement mechanism design for fraudulent\nbehaviour in e-commerce. In AAAI .\nCaicedo, J. C. and Lazebnik, S. (2015). Active object localization with deep reinforcement learning. In ICCV .\nCampbell, J. Y . and Viceira, L. M. (2002). Strategic Asset Allocation Portfolio Choice for Long-Term Investors .\nOxford University Press.\nCampbell, M., Hoane, A. J., and Hsu, F. (2002). Deep blue. Artiﬁcial Intelligence , 134:57–83.\nCao, Q., Lin, L., Shi, Y ., Liang, X., and Li, G. (2017). Attention-aware face hallucination via deep reinforce-\nment learning. In CVPR .\nCarleo, G. and Troyer, M. (2017). Solving the quantum many-body problem with artiﬁcial neural networks.\nScience , 355(6325):602–606.\nCaruana, R. (1997). Multitask learning. Machine Learning , 28(1):41–75.\nChakraborty, B. and Murphy, S. A. (2014). Dynamic treatment regimes. Annual Review of Statistics and Its\nApplication , 1:447–464.\n116\n\nChan, C., Ginosar, S., Zhou, T., and Efros, A. A. (2018). Everybody Dance Now. ArXiv .\nChandola, V ., Banerjee, A., and Kumar, V . (2009). Anomaly detection : A survey. ACM Computing Surveys ,\n41(3):1–72.\nCharniak, E. and McDermott, D. (1985). Introduction to Artiﬁcial Intelligence Programming . Addison Wesley.\nChebotar, Y ., Hausman, K., Zhang, M., Sukhatme, G., Schaal, S., and Levine, S. (2017). Combining model-\nbased and model-free updates for trajectory-centric reinforcement learning. In ICML .\nChen, K. and Bowling, M. (2012). Tractable objectives for robust policy optimization. In NIPS .\nChen, L.-C., Collins, M., Zhu, Y ., Papandreou, G., Zoph, B., Schroff, F., Adam, H., and Shlens, J. (2018a).\nSearching for efﬁcient multi-scale architectures for dense image prediction. In NIPS .\nChen, T., Murali, A., and Gupta, A. (2018b). Hardware conditioned policies for multi-robot transfer learning.\nInNIPS .\nChen, T., Zheng, L., Yan, E., Jiang, Z., Moreau, T., Ceze, L., Guestrin, C., and Krishnamurthy, A. (2018c).\nLearning to optimize tensor programs. In NIPS .\nChen, T. Q., Rubanova, Y ., Bettencourt, J., and Duvenaud, D. (2018). Neural Ordinary Differential Equations.\nArXiv .\nChen, X. and Deng, X. (2006). Settling the complexity of two-player nash equilibrium. In IEEE Symposium\non Foundations of Computer Science (FOCS) .\nChen, X., Duan, Y ., Houthooft, R., Schulman, J., Sutskever, I., and Abbeel, P. (2016a). InfoGAN: Interpretable\nrepresentation learning by information maximizing generative adversarial nets. In NIPS .\nChen, X., Li, L.-J., Fei-Fei, L., and Gupta, A. (2018a). Iterative visual reasoning beyond convolutions. In\nCVPR .\nChen, Y ., Li, L., and Wang, M. (2018b). Scalable bilinear \u0019learning using state and action features. In ICML .\nChen, Y .-N. V ., Hakkani-T ¨ur, D., Tur, G., Gao, J., and Deng, L. (2016b). End-to-end memory networks with\nknowledge carryover for multi-turn spoken language understanding. In Annual Meeting of the International\nSpeech Communication Association (INTERSPEECH) .\nChen, Z. and Liu, B. (2016). Lifelong Machine Learning . Morgan & Claypool.\nChen, Z. and Yi, D. (2017). The Game Imitation: Deep Supervised Convolutional Networks for Quick Video\nGame AI. ArXiv .\nCheng, Y ., Xu, W., He, Z., He, W., Wu, H., Sun, M., and Liu, Y . (2016). Semi-supervised learning for neural\nmachine translation. In ACL.\nCho, K. (2015). Natural Language Understanding with Distributed Representation. ArXiv .\nCho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., and Bengio, Y . (2014). Learning phrase\nrepresentations using RNN encoder-decoder for statistical machine translation. In EMNLP .\nChoi, E., Hewlett, D., Polosukhin, I., Lacoste, A., Uszkoreit, J., and Berant, J. (2017). Coarse-to-ﬁne question\nanswering for long documents. In ACL.\nChopra, S., Auli, M., and Rush, A. M. (2016). Abstractive sentence summarization with attentive recurrent\nneural networks. In NAACL .\nChow, Y ., Nachum, O., Ghavamzadeh, M., and Duenez-Guzman, E. (2018). A Lyapunov-based approach to\nsafe reinforcement learning. In NIPS .\nChristiano, P., Leike, J., Brown, T. B., Martic, M., Legg, S., and Amodei, D. (2017). Deep reinforcement\nlearning from human preferences. In NIPS .\nChua, K., Calandra, R., McAllister, R., and Levine, S. (2018). Data-efﬁcient model-based reinforcement\nlearning with deep probabilistic dynamics models. In NIPS .\nChung, J., Gulcehre, C., Cho, K., and Bengio, Y . (2014). Empirical evaluation of gated recurrent neural\nnetworks on sequence modeling. In NIPS 2014 Deep Learning and Representation Learning Workshop .\n117\n\nCook, T. R. and Hall, A. S. (2017). Macroeconomic indicator forecasting with deep neural networks. Federal\nReserve Bank of Kansas City, Research Working Paper 17-11 .\nCranmer, K. (2016). Machine learning and likelihood-free inference in particle physics. https://bit.ly/\n2nJGSuf . NIPS 2016 Invited Talk.\nCubuk, E. D., Zoph, B., Mane, D., Vasudevan, V ., and Le, Q. V . (2018). AutoAugment: Learning Augmentation\nPolicies from Data. ArXiv e-prints .\nCueva, C. J. and Wei, X.-X. (2018). Emergence of grid-like representations by training recurrent neural net-\nworks to perform spatial localization. In ICLR .\nDabney, W., Ostrovski, G., Silver, D., and Munos, R. (2018). Implicit quantile networks for distributional\nreinforcement learning. In ICML .\nDai, B., Shaw, A., He, N., Li, L., and Song, L. (2018a). Boosting the actor with dual critic. In ICLR .\nDai, B., Shaw, A., Li, L., Xiao, L., He, N., Liu, Z., Chen, J., and Song, L. (2018b). SBEED: Convergent\nreinforcement learning with nonlinear function approximation. In ICML .\nDai, H., Dai, B., and Song, L. (2016). Discriminative embeddings of latent variable models for structured data.\nInICML .\nDai, H., Khalil, E. B., Zhang, Y ., Dilkina, B., and Song, L. (2017). Learning combinatorial optimization\nalgorithms over graphs. In NIPS .\nDai, Z., Yang, Z., Yang, F., Cohen, W. W., and Salakhutdinov, R. (2017). Good semi-supervised learning that\nrequires a bad gan. In NIPS .\nDanihelka, I., Wayne, G., Uria, B., Kalchbrenner, N., and Graves, A. (2016). Associative long short-term\nmemory. In ICML .\nDarwiche, A. (2018). Human-level intelligence or animal-like abilities? Communications of the ACM ,\n61(10):56–67.\nDas, A., Kottur, S., Moura, J. M. F., Lee, S., and Batra, D. (2017). Learning cooperative visual dialog agents\nwith deep reinforcement learning. In ICCV .\nDaum ´e, III, H., Langford, J., and Marcu, D. (2009). Search-based structured prediction. Machine Learning ,\n75(3):297–325.\nDayan, P. (1993). Improving generalization for temporal difference learning: The successor representation.\nNeural Computation , 5(4):613–624.\nDayan, P. and Hinton, G. E. (1993). Feudal reinforcement learning. In NIPS .\nDe Asis, K., Hernandez-Garcia, J. F., Zacharias Holland, G., and Sutton, R. S. (2018). Multi-step reinforcement\nlearning: A unifying algorithm. In AAAI .\nde Avila Belbute-Peres, F., Smith, K., Allen, K., Tenenbaum, J., and Kolter, J. Z. (2018). End-to-end differen-\ntiable physics for learning and control. In NIPS .\nDegris, T., White, M., and Sutton, R. S. (2012). Off-policy actor-critic. In ICML .\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, Ł. (2018). Universal Transformers. ArXiv .\nDeisenroth, M. P., Neumann, G., and Peters, J. (2013). A survey on policy search for robotics. Foundations\nand Trend in Robotics , 2:1–142.\nDeisenroth, M. P. and Rasmussen, C. E. (2011). PILCO: A model-based and data-efﬁcient approach to policy\nsearch. In ICML .\nDeng, L. (2017). Three generations of spoken dialogue systems (bots), talk at\nAI Frontiers Conference. https://www.slideshare.net/AIFrontiers/\nli-deng-three-generations-of-spoken-dialogue-systems-bots .\nDeng, L. and Li, X. (2013). Machine learning paradigms for speech recognition: An overview. IEEE Transac-\ntions on Audio, Speech, and Language Processing , 21(5):1060–1089.\nDeng, L. and Liu, Y ., editors (2018). Deep Learning in Natural Language Processing . Springer.\n118\n\nDeng, Y ., Bao, F., Kong, Y ., Ren, Z., and Dai, Q. (2016). Deep direct reinforcement learning for ﬁnancial signal\nrepresentation and trading. IEEE Transactions on Neural Networks and Learning Systems .\nDenil, M., Agrawal, P., Kulkarni, T. D., Erez, T., Battaglia, P., and de Freitas, N. (2017). Learning to perform\nphysics experiments via deep reinforcement learning. In ICLR .\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. ArXiv e-prints .\nDeVries, P. M. R., Vi ´egas, F., Wattenberg, M., and Meade, B. J. (2018). Deep learning of aftershock patterns\nfollowing large earthquakes. Nature , 560:632–634.\nDevrim Kaba, M., Gokhan Uzunbas, M., and Nam Lim, S. (2017). A reinforcement learning approach to the\nview planning problem. In CVPR .\nDhingra, B., Li, L., Li, X., Gao, J., Chen, Y .-N., Ahmed, F., and Deng, L. (2017). End-to-end reinforcement\nlearning of dialogue agents for information access. In ACL.\nDiederik P Kingma, M. W. (2014). Auto-encoding variational bayes. In ICLR .\nDieleman, S., van den Oord, A., and Simonyan, K. (2018). The challenge of realistic music generation: mod-\nelling raw audio at scale. In NIPS .\nDietterich, T. G. (2000). Hierarchical reinforcement learning with the MAXQ value function decomposition.\nJAIR , 13(1):227–303.\nDietterich, T. G. (2018). Reﬂections on innateness in machine learning. https://medium.com/\n@tdietterich/reflections-on-innateness-in-machine-learning-4eebefa3e1af .\nDimakopoulou, M., Osband, I., and Roy, B. V . (2018). Scalable coordinated exploration in concurrent rein-\nforcement learning. In NIPS .\nDiuk, C., Cohen, A., and Littman, M. L. (2008). An object-oriented representation for efﬁcient reinforcement\nlearning. In ICML .\nDoan, T., Mazoure, B., and Lyle, C. (2018). GAN Q-learning. ArXiv .\nDomingos, P. (2012). A few useful things to know about machine learning. Communications of the ACM ,\n55(10):78–87.\nDomingos, P. (2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake\nOur World . Basic Books.\nDonahue, J., Jia, Y ., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. (2014). Decaf: A deep\nconvolutional activation feature for generic visual recognition. In ICML .\nDong, D., Wu, H., He, W., Yu, D., and Wang, H. (2015). Multi-task learning for multiple language translation.\nInACL.\nDong, S. and Roy, B. V . (2018). An information-theoretic analysis of Thompson sampling for large action\nspaces. In NIPS .\nDoshi-Velez, F. and Kim, B. (2017). Towards A Rigorous Science of Interpretable Machine Learning. ArXiv .\nDosovitskiy, A. and Koltun, V . (2017). Learning to act by predicting the future. In ICLR .\nDowney, C., Hefny, A., Li, B., Boots, B., and Gordon, G. (2017). Predictive state recurrent neural networks. In\nNIPS .\nDu, S. S., Chen, J., Li, L., Xiao, L., and Zhou, D. (2017). Stochastic variance reduction methods for policy\nevaluation. In ICML .\nDuan, R. (2017). Meta Learning for Control . University of California at Berkeley PhD Thesis.\nDuan, Y ., Andrychowicz, M., Stadie, B. C., Ho, J., Schneider, J., Sutskever, I., Abbeel, P., and Zaremba, W.\n(2017). One-shot imitation learning. In NIPS .\nDuan, Y ., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P. (2016). Benchmarking deep reinforcement\nlearning for continuous control. In ICML .\n119\n\nDuan, Y ., Schulman, J., Chen, X., Bartlett, P. L., Sutskever, I., and Abbeel, P. (2016). RL2: Fast Reinforcement\nLearning via Slow Reinforcement Learning. ArXiv .\nDˇzeroski, S., Raedt, L. D., and Driessens, K. (2001). Relational reinforcement learning. Machine Learning ,\n43((1/2)):7–52.\nEfros, A. (2017). Self-supervised deep learning. https://www.youtube.com/watch?v=\nYhYsvD6IfKE .\nEl-Tantawy, S., Abdulhai, B., and Abdelgawad, H. (2013). Multiagent reinforcement learning for integrated net-\nwork of adaptive trafﬁc signal controllers (marlin-atsc): methodology and large-scale application on down-\ntown toronto. IEEE Transactions on Intelligent Transportation Systems , 14(3):1140–1150.\nEllis, H. C. (1965). The transfer of learning . Macmillan.\nElsken, T., Hendrik Metzen, J., and Hutter, F. (2018). Neural Architecture Search: A Survey. ArXiv .\nErnst, D., Geurts, P., and Wehenkel, L. (2005). Tree-based batch mode reinforcement learning. JMLR , 6:503–\n556.\nEslami, S. M. A., Heess, N., Weber, T., Tassa, Y ., Szepesv ´ari, D., Kavukcuoglu, K., and Hinton, G. E. (2016).\nAttend, infer, repeat: Fast scene understanding with generative models. In NIPS .\nEslami, S. M. A., Rezende, D. J., Besse, F., Viola, F., Morcos, A. S., Garnelo, M., Ruderman, A., Rusu, A. A.,\nDanihelka, I., Gregor, K., Reichert, D. P., Buesing, L., Weber, T., Vinyals, O., Rosenbaum, D., Rabinowitz,\nN., King, H., Hillier, C., Botvinick, M., Wierstra, D., Kavukcuoglu, K., and Hassabis, D. (2018). Neural\nscene representation and rendering. Science , 360:1204–1210.\nEspeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V ., Ward, T., Doron, Y ., Firoiu, V ., Harley, T., Dun-\nning, I., Legg, S., and Kavukcuoglu, K. (2018). IMPALA: Scalable Distributed Deep-RL with Importance\nWeighted Actor-Learner Architectures. ArXiv .\nEvans, R. and Grefenstette, E. (2018). Learning explanatory rules from noisy data. JAIR , 61(1):1–64.\nEvtimov, I., Eykholt, K., Fernandes, E., Kohno, T., Li, B., Prakash, A., Rahmati, A., and Song, D. (2017).\nRobust Physical-World Attacks on Deep Learning Models. ArXiv .\nEykholt, K., Evtimov, I., Fernandes, E., Li, B., Rahmati, A., Xiao, C., Prakash, A., Kohno, T., and Song, D.\n(2018). Robust physical-world attacks on deep learning models. In CVPR .\nFang, M., Li, Y ., and Cohn, T. (2017). Learning how to active learn: A deep reinforcement learning approach.\nInEMNLP .\nFang, X., Misra, S., Xue, G., and Yang, D. (2012). Smart grid - the new and improved power grid: A survey.\nIEEE Communications Surveys Tutorials , 14(4):944–980.\nFarahmand, A.-m. (2018). Iterative value-aware model learning. In NIPS .\nFarebrother, J., Machado, M. C., and Bowling, M. (2018). Generalization and Regularization in DQN. ArXiv\ne-prints .\nFarquhar, G., Rockt ¨aschel, T., Igl, M., and Whiteson, S. (2018). TreeQN and ATreeC: Differentiable tree-\nstructured models for deep reinforcement learning. In ICLR .\nFatemi, M., Asri, L. E., Schulz, H., He, J., and Suleman, K. (2016). Policy networks with two-stage training\nfor dialogue systems. In the Annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL) .\nFaust, A., Aimone, J. B., James, C. D., and Tapia, L. (2018). Resilient Computing with Reinforcement Learning\non a Dynamical System: Case Study in Sorting. ArXiv e-prints .\nFauw, J. D., Ledsam, J. R., Romera-Paredes, B., Nikolov, S., Tomasev, N., Blackwell, S., Askham, H., Glorot,\nX., O’Donoghue, B., Visentin, D., van den Driessche, G., Lakshminarayanan, B., Meyer, C., Mackinder,\nF., Bouton, S., Ayoub, K., Chopra, R., King, D., Karthikesalingam, A., Hughes, C. O., Raine, R., Hughes,\nJ., Sim, D. A., Egan, C., Tufail, A., Montgomery, H., Hassabis, D., Rees, G., Back, T., Khaw, P. T., Suley-\nman, M., Cornebise, J., Keane, P. A., and Ronneberger, O. (2018). Clinically applicable deep learning for\ndiagnosis and referral in retinal disease. Nature Medicine .\nFeng, J. and Zhou, Z.-H. (2017). AutoEncoder by Forest. ArXiv .\nFinn, C. (2017). Model-based RL. http://goo.gl/Fsd5n8 . Deep RL Bootcamp.\n120\n\nFinn, C., Abbeel, P., and Levine, S. (2017a). Model-agnostic meta-learning for fast adaptation of deep networks.\nInICML .\nFinn, C., Christiano, P., Abbeel, P., and Levine, S. (2016a). A connection between GANs, inverse reinforcement\nlearning, and energy-based models. In NIPS 2016 Workshop on Adversarial Training .\nFinn, C. and Levine, S. (2015). Deep visual foresight for planning robot motion. In ICRA .\nFinn, C. and Levine, S. (2017). Deep visual foresight for planning robot motion. In ICRA .\nFinn, C. and Levine, S. (2018). Meta-learning and universality: Deep representations and gradient descent can\napproximate any learning algorithm. In ICLR .\nFinn, C., Levine, S., and Abbeel, P. (2016b). Guided cost learning: Deep inverse optimal control via policy\noptimization. In ICML .\nFinn, C., Xu, K., and Levine, S. (2018). Probabilistic model-agnostic meta-learning. In NIPS .\nFinn, C., Yu, T., Fu, J., Abbeel, P., and Levine, S. (2017b). Generalizing skills with semi-supervised reinforce-\nment learning. In ICLR .\nFinn, C., Yu, T., Zhang, T., Abbeel, P., and Levine, S. (2017c). One-shot visual imitation learning via meta-\nlearning. In Conference on Robot Learning .\nFiroiu, V ., Whitney, W. F., and Tenenbaum, J. B. (2017). Beating the World’s Best at Super Smash Bros. with\nDeep Reinforcement Learning. ArXiv .\nFlorensa, C., Duan, Y ., and Abbeel, P. (2017). Stochastic neural networks for hierarchical reinforcement learn-\ning. In ICLR .\nFoerster, J., Assael, Y . M., de Freitas, N., and Whiteson, S. (2016). Learning to communicate with deep\nmulti-agent reinforcement learning. In NIPS .\nFoerster, J., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S. (2018a). Counterfactual multi-agent\npolicy gradients. In AAAI .\nFoerster, J., Nardelli, N., Farquhar, G., Torr, P. H. S., Kohli, P., and Whiteson, S. (2017). Stabilising experience\nreplay for deep multi-agent reinforcement learning. In ICML .\nFoerster, J. N., Chen, R. Y ., Al-Shedivat, M., Whiteson, S., Abbeel, P., and Mordatch, I. (2018b). Learning\nwith opponent-learning awareness. In AAMAS .\nFortunato, M., Gheshlaghi Azar, M., Piot, B., Menick, J., Osband, I., Graves, A., Mnih, V ., Munos, R., Hass-\nabis, D., Pietquin, O., Blundell, C., and Legg, S. (2018). Noisy networks for exploration. In ICLR .\nFox, R., Pakman, A., and Tishby, N. (2016). Taming the noise in reinforcement learning via soft updates. In\nthe Conference on Uncertainty in Artiﬁcial Intelligence (UAI) .\nFridman, L., Jenik, B., and Terwilliger, J. (2018). DeepTrafﬁc: Driving Fast through Dense Trafﬁc with Deep\nReinforcement Learning. ArXiv .\nFu, J., Co-Reyes, J. D., and Levine, S. (2017). Ex2: Exploration with exemplar models for deep reinforcement\nlearning. In NIPS .\nGao, C., Hayward, R. B., and M ¨uller, M. (2017). Move prediction using deep convolutional neural networks in\nhex. IEEE Transactions on Games .https://github.com/cgao3/benzene-vanilla-cmake .\nGao, J., Galley, M., and Li, L. (2018a). Neural approaches to Conversational AI. Foundations and Trends in\nInformation Retrieval . To appear.\nGao, J., Galley, M., and Li, L. (2018b). Neural approaches to Conversational\nAI. https://www.microsoft.com/en-us/research/publication/\nneural-approaches-to-conversational-ai/ . ACL 2018 Tutorial.\nGao, Y ., Chen, L., and Li, B. (2018c). Post: Device placement with cross-entropy minimization and proximal\npolicy optimization. In NIPS .\nGarc `ıa, J. and Fern `andez, F. (2015). A comprehensive survey on safe reinforcement learning. JMLR , 16:1437–\n1480.\n121\n\nGavrilovska, L., Atanasovski, V ., Macaluso, I., and DaSilva, L. A. (2013). Learning and reasoning in cognitive\nradio networks. IEEE Communications Surveys Tutorials , 15(4):1761–1777.\nGeffner, H. (2018). Model-free, model-based, and general intelligence. In IJCAI .\nGehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin, Y . N. (2017). Convolutional Sequence to Sequence\nLearning. ArXiv .\nGelly, S., Schoenauer, M., Sebag, M., Teytaud, O., Kocsis, L., Silver, D., and Szepesv ´ari, C. (2012). The\ngrand challenge of computer go: Monte carlo tree search and extensions. Communications of the ACM ,\n55(3):106–113.\nGelly, S. and Silver, D. (2007). Combining online and ofﬂine knowledge in UCT. In ICML .\nGeorge, D., Lehrach, W., Kansky, K., L ´azaro-Gredilla, M., Laan, C., Marthi, B., Lou, X., Meng, Z., Liu,\nY ., Wang, H., Lavin, A., and Phoenix, D. S. (2017). A generative vision model that trains with high data\nefﬁciency and breaks text-based CAPTCHAs. Science .\nGershman, S. J. (2018). The successor representation: Its computational logic and neural substrates. Journal\nof Neuroscience , 38(33):7193–7200.\nGetoor, L. and Taskar, B., editors (2007). Introduction to Statistical Relational Learning . MIT Press.\nGhavamzadeh, M., Engel, Y ., and Valko, M. (2016). Bayesian policy gradient and actor-critic algorithms.\nJMLR , 17(66):1–53.\nGhavamzadeh, M., Mahadevan, S., and Makar, R. (2006). Hierarchical multi-agent reinforcement learning.\nAutonomous Agents and Multi-Agent Systems , 13(2):197–229.\nGhavamzadeh, M., Mannor, S., Pineau, J., and Tamar, A. (2015). Bayesian reinforcement learning: a survey.\nFoundations and Trends in Machine Learning , 8(5-6):359–483.\nGheiratmand, M., Rish, I., Cecchi, G. A., Brown, M. R. G., Greiner, R., Polosecki, P. I., Bashivan, P., Green-\nshaw, A. J., Ramasubbu, R., and Dursun, S. M. (2017). Learning stable and predictive network-based\npatterns of schizophrenia and its clinical symptoms. Nature Schizophrenia , 3(22).\nGinsberg, M. L. (2001). GIB: Imperfect information in a computationally challenging game. JAIR , 14:303–358.\nGirshick, R. (2015). Fast R-CNN. In ICCV .\nGlasserman, P. (2004). Monte Carlo Methods in Financial Engineering . Springer-Verlag, New York.\nGlavic, M., Fonteneau, R., and Ernst, D. (2017). Reinforcement learning for electric power system decision and\ncontrol: Past considerations and perspectives. In The 20th World Congress of the International Federation\nof Automatic Control .\nGoel, V ., Weng, J., and Poupart, P. (2018). Unsupervised video object segmentation for deep reinforcement\nlearning. In NIPS .\nGoldberg, Y . (2017). Neural Network Methods for Natural Language Processing . Morgan & Claypool.\nGoldberg, Y . and Kosorok, M. R. (2012). Q-learning with censored data. Annals of Statistics , 40(1):529–560.\nGoodfellow, I. (2017). NIPS 2016 Tutorial: Generative Adversarial Networks. ArXiv .\nGoodfellow, I., Bengio, Y ., and Courville, A. (2016). Deep Learning . MIT Press.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., , and Bengio,\nY . (2014). Generative adversarial nets. In NIPS .\nGoodfellow, I. J., Shlens, J., and Szegedy, C. (2015). Explaining and harnessing adversarial examples. In ICLR .\nGrant, E., Finn, C., Levine, S., Darrell, T., and Grifﬁths, T. (2018). Recasting gradient-based meta-learning as\nhierarchical bayes. In ICLR .\nGraves, A., rahman Mohamed, A., and Hinton, G. (2013). Speech recognition with deep recurrent neural\nnetworks. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) .\nGraves, A., Wayne, G., and Danihelka, I. (2014). Neural Turing Machines. ArXiv .\n122\n\nGraves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwi ´nska, A., Colmenarejo, S. G.,\nGrefenstette, E., Ramalho, T., Agapiou, J., nech Badia, A. P., Hermann, K. M., Zwols, Y ., Ostrovski, G.,\nCain, A., King, H., Summerﬁeld, C., Blunsom, P., Kavukcuoglu, K., and Hassabis, D. (2016). Hybrid\ncomputing using a neural network with dynamic external memory. Nature , 538:471–476.\nGregor, K., Danihelka, I., Graves, A., Rezende, D., and Wierstra, D. (2015). Draw: A recurrent neural network\nfor image generation. In ICML .\nGruslys, A., Gheshlaghi Azar, M., Bellemare, M. G., and Munos, R. (2017). The Reactor: A Sample-Efﬁcient\nActor-Critic Architecture. ArXiv .\nGu, S., Holly, E., Lillicrap, T., and Levine, S. (2017a). Deep reinforcement learning for robotic manipulation\nwith asynchronous off-policy updates. In ICRA .\nGu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and Levine, S. (2017b). Q-Prop: Sample-efﬁcient policy\ngradient with an off-policy critic. In ICLR .\nGu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., Sch ¨olkopf, B., and Levine, S. (2017). Interpolated policy\ngradient: Merging on-policy and off-policy gradient estimation for deep reinforcement learning. In NIPS .\nGu, S., Lillicrap, T., Sutskever, I., and Levine, S. (2016). Continuous deep Q-learning with model-based\nacceleration. In ICML .\nGuestrin, C., Koller, D., Gearhart, C., and Kanodia, N. (2003). Generalizing plans to new environments in\nrelational MDPs. In IJCAI .\nGuez, A., Heess, N., Silver, D., and Dayan, P. (2014). Bayes-adaptive simulation-based search with value\nfunction approximation. In NIPS .\nGuez, A., Weber, T., Antonoglou, I., Simonyan, K., Vinyals, O., Wierstra, D., Munos, R., and Silver, D. (2018).\nLearning to search with MCTSnets. In ICML .\nGulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V ., and Courville, A. (2017). Improved training of wasser-\nstein gans. In NIPS .\nGuo, R., Cheng, L., Li, J., Hahn, P. R., and Liu, H. (2018). A Survey of Learning Causality with Data: Problems\nand Methods. ArXiv e-prints .\nGuo, X., Singh, S., Lee, H., Lewis, R. L., and Wang, X. (2014). Deep learning for real-time atari game play\nusing ofﬂine monte-carlo tree search planning. In NIPS .\nGupta, A. (2017). Supersizing self-supervision: Learning perception and action without human supervision.\nhttps://simons.berkeley.edu/talks/abhinav-gupta-2017-3-28 .\nGupta, A., Devin, C., Liu, Y ., Abbeel, P., and Levine, S. (2017a). Learning invariant feature spaces to transfer\nskills with reinforcement learning. In ICLR .\nGupta, A., Mendonca, R., Liu, Y ., Abbeel, P., and Levine, S. (2018). Meta-reinforcement learning of structured\nexploration strategies. In NIPS .\nGupta, S., Davidson, J., Levine, S., Sukthankar, R., and Malik, J. (2017b). Cognitive mapping and planning for\nvisual navigation. In CVPR .\nGuu, K., Pasupat, P., Liu, E. Z., and Liang, P. (2017). From language to programs: Bridging reinforcement\nlearning and maximum marginal likelihood. In ACL.\nHa, D. and Eck, D. (2018). A neural representation of sketch drawings. In ICLR .\nHa, D. and Schmidhuber, J. (2018). Recurrent world models facilitate policy evolution. In NIPS .\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement learning with deep energy-based\npolicies. In ICML .\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep\nreinforcement learning with a stochastic actor. In ICML .\nHaber, N., Mrowca, D., Wang, S., Fei-Fei, L., and Yamins, D. (2018). Learning to play with intrinsically-\nmotivated, self-aware agents. In NIPS .\n123\n\nHadﬁeld-Menell, D., Dragan, A., Abbeel, P., and Russell, S. (2016). Cooperative inverse reinforcement learn-\ning. In NIPS .\nHadﬁeld-Menell, D., Milli, S., Abbeel, P., Russell, S., and Dragan, A. (2017). Inverse reward design. In NIPS .\nHan, J., Kamber, M., and Pei, J. (2011). Data Mining: Concepts and Techniques (3rd edition) . Morgan\nKaufmann.\nHansen, N. (2016). The CMA Evolution Strategy: A Tutorial. ArXiv .\nHansen, N. and Ostermeier, A. (2001). Completely derandomized self-adaptation in evolution strategies. Evo-\nlutionary Computation , 9(2):159–195.\nHartford, J., Lewis, G., Leyton-Brown, K., and Taddy, M. (2017). Deep IV: A ﬂexible approach for counter-\nfactual prediction. In ICML .\nHarutyunyan, A., Vrancx, P., Bacon, P.-L., Precup, D., and Nowe, A. (2018). Learning with options that\nterminate off-policy. In AAAI .\nHassabis, D., Kumaran, D., Summerﬁeld, C., and Botvinick, M. (2017). Neuroscience-inspired artiﬁcial intel-\nligence. Neuron , 95:245–258.\nHastie, T., Tibshirani, R., and Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Infer-\nence, and Prediction . Springer.\nHaugeland, J. (1989). Artiﬁcial Intelligence: The Very Idea . The MIT Press.\nHausknecht, M. and Stone, P. (2015). Deep recurrent Q-learning for partially observable MDPs. In AAAI .\nHausknecht, M. and Stone, P. (2016). Deep reinforcement learning in parameterized action space. In ICLR .\nHavens, A., Jiang, Z., and Sarkar, S. (2018). Online robust policy learning in the presence of unknown adver-\nsaries. In NIPS .\nHaykin, S. (2005). Cognitive radio: brain-empowered wireless communications. IEEE Journal on Selected\nAreas in Communications , 23(2):201–220.\nHe, D., Xia, Y ., Qin, T., Wang, L., Yu, N., Liu, T.-Y ., and Ma, W.-Y . (2016a). Dual learning for machine\ntranslation. In NIPS .\nHe, F. S., Liu, Y ., Schwing, A. G., and Peng, J. (2017). Learning to play in a day: Faster deep reinforcement\nlearning by optimality tightening. In ICLR .\nHe, J., Chen, J., He, X., Gao, J., Li, L., Deng, L., and Ostendorf, M. (2016b). Deep reinforcement learning\nwith a natural language action space. In ACL.\nHe, J., Ostendorf, M., He, X., Chen, J., Gao, J., Li, L., and Deng, L. (2016c). Deep reinforcement learning with\na combinatorial action space for predicting popular reddit threads. In EMNLP .\nHe, K., Gkioxari, G., Doll ´ar, P., and Girshick, R. (2017). Mask R-CNN. In ICCV .\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016d). Deep residual learning for image recognition. In CVPR .\nHe, L., Lee, K., Lewis, M., and Zettlemoyer, L. (2017). Deep semantic role labeling: What works and what’s\nnext. In ACL.\nHe, X. and Deng, L. (2013). Speech-centric information processing: An optimization-oriented approach.\nProceedings of the IEEE — Vol. 101, No. 5, May 2013 , 101(5):1116–1135.\nHe, Y ., Lin, J., Liu, Z., Wang, H., Li, L.-J., and Han, S. (2018). Amc: Automl for model compression and\nacceleration on mobile devices. In ECCV .\nHeaton, J. B., Polson, N. G., and Witte, J. H. (2016). Deep learning for ﬁnance: deep portfolios. Applied\nStochastic Models in Business and Industry .\nHeess, N., TB, D., Sriram, S., Lemmon, J., Merel, J., Wayne, G., Tassa, Y ., Erez, T., Wang, Z., Eslami, A.,\nRiedmiller, M., and Silver, D. (2017). Emergence of Locomotion Behaviours in Rich Environments. ArXiv .\nHeess, N., Wayne, G., Silver, D., Lillicrap, T., Tassa, Y ., and Erez, T. (2015). Learning continuous control\npolicies by stochastic value gradients. In NIPS .\n124\n\nHeinrich, J. and Silver, D. (2016). Deep reinforcement learning from self-play in imperfect-information games.\nInNIPS 2016 Deep Reinforcement Learning Workshop .\nHenaff, M., Whitney, W. F., and LeCun, Y . (2017). Model-Based Planning in Discrete Action Spaces. ArXiv .\nHenderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D. (2018). Deep reinforcement\nlearning that matters. In AAAI .\nHernandez, D. and Greenwald, T. (2018). IBM has a Watson dilemma. https://on.wsj.com/2nrlp7g .\nHessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M.,\nand Silver, D. (2018). Rainbow: Combining Improvements in Deep Reinforcement Learning. In AAAI .\nHester, T. and Stone, P. (2017). Intrinsically motivated model learning for developing curious robots. Artiﬁcial\nIntelligence , 247:170–86.\nHester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul, T., Piot, B., Horgan, D., Quan, J., Sendonaris, A.,\nDulac-Arnold, G., Osband, I., Agapiou, J., Leibo, J. Z., and Gruslys, A. (2018). Deep Q-learning from\ndemonstrations. In AAAI .\nHiggins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., and Lerchner, A. (2017).\n\f-V AE: Learning basic visual concepts with a constrained variational framework. In ICLR .\nHinton, G., Deng, L., Yu, D., Dahl, G. E., rahman Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V ., Nguyen,\nP., Sainath, T. N., , and Kingsbury, B. (2012). Deep neural networks for acoustic modeling in speech\nrecognition. IEEE Signal Processing Magazine , 82.\nHinton, G., Vinyals, O., and Dean, J. (2014). Distilling the knowledge in a neural network. In NIPS 2014 Deep\nLearning Workshop .\nHinton, G. E., Sabour, S., and Frosst, N. (2018). Matrix capsules with EM routing. In ICLR .\nHinton, G. E. and Salakhutdinov, R. R. (2006). Reducing the dimensionality of data with neural networks.\nScience , 313(5786):504–507.\nHirschberg, J. and Manning, C. D. (2015). Advances in natural language processing. Science , 349(6245):261–\n266.\nHo, J. and Ermon, S. (2016). Generative adversarial imitation learning. In NIPS .\nHo, J., Gupta, J. K., and Ermon, S. (2016). Model-free imitation learning with policy optimization. In ICML .\nHochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation , 9:1735–1780.\nHochreiter, S., Younger, A. S., and Conwell, P. R. (2001). Learning to learn using gradient descent. In ICANN .\nHorgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., van Hasselt, H., and Silver, D. (2018). Dis-\ntributed prioritized experience replay. In ICLR .\nHorling, B. and Lesser, V . (2004). A survey of multi-agent organizational paradigms. Knowledge Engineering\nReview , 19(4):281–316.\nHornik, K., Stinchcombe, M., and White, H. (1989). Multilayer feedforward networks are universal approxi-\nmators. Neural Networks , 2(5):359–366.\nHoshen, Y . (2017). Vain: Attentional multi-agent predictive modeling. In NIPS .\nHouthooft, R., Chen, X., Duan, Y ., Schulman, J., Turck, F. D., and Abbeel, P. (2016). Vime: Variational\ninformation maximizing exploration. In NIPS .\nHouthooft, R., Chen, Y ., Isola, P., Stadie, B., Wolski, F., Ho, J., and Abbeel, P. (2018). Evolved policy gradients.\nInNIPS .\nHsu, K., Levine, S., and Finn, C. (2018). Unsupervised Learning via Meta-Learning. ArXiv e-prints .\nHu, J. and Wellman, M. P. (2003). Nash q-learning for general-sum stochastic games. JMLR , 4:1039–1069.\nHu, Y ., Da, Q., Zeng, A., Yu, Y ., and Xu, Y . (2018a). Reinforcement learning to rank in e-commerce search\nengine: Formalization, analysis, and application. In KDD .\n125\n\nHu, Y ., Li, J., Li, X., Pan, G., and Xu, M. (2018b). Knowledge-guided agent-tactic-aware learning for starcraft\nmicromanagement. In IJCAI .\nHu, Z., Liang, Y ., Liu, Y ., and Zhang, J. (2018c). Inference aided reinforcement learning for incentive mecha-\nnism design in crowdsourcing. In NIPS .\nHu, Z., Yang, Z., Salakhutdinov, R., Qin, L., Liang, X., Dong, H., and Xing, E. (2018d). Deep generative\nmodels with learnable knowledge constraints. In NIPS .\nHu, Z., Yang, Z., Salakhutdinov, R., and Xing, E. P. (2017). On Unifying Deep Generative Models. ArXiv .\nHuang, J., Wu, F., Precup, D., and Cai, Y . (2018). Learning safe policies with expert guidance. In NIPS .\nHuang, S., Papernot, N., Goodfellow, I., Duan, Y ., and Abbeel, P. (2017). Adversarial attacks on neural network\npolicies. In ICLR Workshop Track .\nHuang, S.-C., Arneson, B., Hayward, R. B., and M ¨uller, M. (2013). Mohex 2.0: A pattern-based mcts hex\nplayer. In International Conference on Computers and Games .\nHudson, D. A. and Manning, C. D. (2018). Compositional attention networks for machine reasoning. In ICLR .\nHughes, E., Leibo, J., Phillips, M., karl Tuyls, Due ˜nez-Guzman, E., Casta ˜neda, A. G., Dunning, I., Zhu,\nT., McKee, K., Koster, R., Roff, H., and Graepel, T. (2018). Inequity aversion improves cooperation in\nintertemporal social dilemmas. In NIPS .\nHull, J. C. (2014). Options, Futures and Other Derivatives (9th edition) . Prentice Hall.\nHutson, M. (2018). Basic instincts. Science , 360(6391):845–847.\nHutter, M. (2005). Universal Artiﬁcial Intelligence: Sequential Decisions Based on Algorithmic Probability .\nSpringer, Berlin.\nIoffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal\ncovariate shift. In ICML .\nIrving, G., Szegedy, C., Alemi, A. A., Een, N., Chollet, F., and Urban, J. (2016). Deepmath - deep sequence\nmodels for premise selection. In NIPS .\nJaderberg, M., Czarnecki, W. M., Dunning, I., Marris, L., Lever, G., Garcia Castaneda, A., Beattie, C., Rabi-\nnowitz, N. C., Morcos, A. S., Ruderman, A., Sonnerat, N., Green, T., Deason, L., Leibo, J. Z., Silver, D.,\nHassabis, D., Kavukcuoglu, K., and Graepel, T. (2018). Human-level performance in ﬁrst-person multiplayer\ngames with population-based deep reinforcement learning. ArXiv .\nJaderberg, M., Dalibard, V ., Osindero, S., Czarnecki, W. M., Donahue, J., Razavi, A., Vinyals, O., Green, T.,\nDunning, I., Simonyan, K., Fernando, C., and Kavukcuoglu, K. (2017). Population Based Training of Neural\nNetworks. ArXiv .\nJaderberg, M., Mnih, V ., Czarnecki, W., Schaul, T., Leibo, J. Z., Silver, D., and Kavukcuoglu, K. (2017).\nReinforcement learning with unsupervised auxiliary tasks. In ICLR .\nJaderberg, M., Simonyan, K., Zisserman, A., and Kavukcuoglu, K. (2015). Spatial transformer networks. In\nNIPS .\nJaksch, T., Ortner, R., and Auer, P. (2010). Near-optimal regret bounds for reinforcement learning. JMLR ,\n11:1563–1600.\nJames, G., Witten, D., Hastie, T., and Tibshirani, R. (2013). An Introduction to Statistical Learning with\nApplications in R . Springer.\nJan Peters, Katharina M ¨ulling, Y . A. (2010). Relative entropy policy search. In AAAI .\nJaques, N., Gu, S., Bahdanau, D., Hern ´andez-Lobato, J. M., Turner, R. E., and Eck, D. (2017). Sequence tutor:\nConservative ﬁne-tuning of sequence generation models with KL-control. In ICML .\nJayaraman, D. and Grauman, K. (2018). Learning to look around: Intelligently exploring unseen environments\nfor unknown tasks. In CVPR .\nJiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R. E. (2017). Contextual decision\nprocesses with low bellman rank are pac-learnable. In ICML .\n126\n\nJiang, N. and Li, L. (2016). Doubly robust off-policy value evaluation for reinforcement learning. In ICML .\nJie, Z., Liang, X., Feng, J., Jin, X., Lu, W. F., and Yan, S. (2016). Tree-structured reinforcement learning for\nsequential object localization. In NIPS .\nJin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M. (2018). Is Q-learning provably efﬁcient? In NIPS .\nJin, H., Song, Q., and Hu, X. (2018). Efﬁcient Neural Architecture Search with Network Morphism. ArXiv .\nJohansson, F. D., Shalit, U., and Sontag, D. (2016). Learning representations for counterfactual inference. In\nICML .\nJohnson, M., Schuster, M., Le, Q. V ., Krikun, M., Wu, Y ., Chen, Z., Thorat, N., Vi ´egas, F., Wattenberg, M.,\nCorrado, G., Hughes, M., and Dean, J. (2017). Google’s multilingual neural machine translation system:\nEnabling zero-shot translation. Transactions of the Association for Computational Linguistics , 5:339–351.\nJordan, M. (2018). Artiﬁcial intelligence???the revolution hasn?t happened yet. https://medium.com/\n@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7 .\nJordan, M. I. and Mitchell, T. (2015). Machine learning: Trends, perspectives, and prospects. Science ,\n349(6245):255–260.\nJoulin, A., Grave, E., Bojanowski, P., and Mikolov, T. (2017). Bag of tricks for efﬁcient text classiﬁcation. In\nProceedings of the 15th Conference of the European Chapter of the Association for Computational Linguis-\ntics (EACL) .\nJun, K.-S., Li, L., Ma, Y ., and Zhu, X. (2018). Adversarial attacks on stochastic bandits. In NIPS .\nJurafsky, D. and Martin, J. H. (2017). Speech and Language Processing (3rd ed. draft) . Prentice Hall.\nJustesen, N., Bontrager, P., Togelius, J., and Risi, S. (2017). Deep Learning for Video Game Playing. ArXiv .\nKadlec, R., Schmid, M., Bajgar, O., and Kleindienst, J. (2016). Text understanding with the attention sum\nreader network. In ACL.\nKaelbling, L. P., Littman, M. L., and Cassandra, A. R. (1998). Planning and acting in partially observable\nstochastic domains. Artiﬁcial Intelligence , 101:99–134.\nKahneman, D. (2011). Thinking, Fast and Slow . Farrar, Straus and Giroux.\nKaiser, L. and Bengio, S. (2016). Can active memory replace attention? In NIPS .\nKaiser, L., Gomez, A. N., Shazeer, N., Vaswani, A., Parmar, N., Jones, L., and Uszkoreit, J. (2017a). One\nModel To Learn Them All. ArXiv .\nKaiser, Ł., Nachum, O., Roy, A., and Bengio, S. (2017b). Learning to Remember Rare Events. In ICLR .\nKakade, S. (2002). A natural policy gradient. In NIPS .\nKakade, S. and Langford, J. (2002). Approximately optimal approximate reinforcement learning. In ICML .\nKalchbrenner, N. and Blunsom, P. (2013). Recurrent continuous translation models. In EMNLP .\nKallus, N. and Zhou, A. (2018). Confounding-robust policy improvement. In NIPS .\nKandasamy, K., Bachrach, Y ., Tomioka, R., Tarlow, D., and Carter, D. (2017). Batch policy gradient methods\nfor improving neural conversation models. In ICLR .\nKandasamy, K., Neiswanger, W., Schneider, J., Poczos, B., and Xing, E. (2018). Neural architecture search\nwith Bayesian optimisation and optimal transport. In NIPS .\nKansky, K., Silver, T., M ´ely, D. A., Eldawy, M., L ´azaro-Gredilla, M., Lou, X., Dorfman, N., Sidor, S., Phoenix,\nS., and George, D. (2017). Schema networks: Zero-shot transfer with a generative causal model of intuitive\nphysics. In ICML .\nKantarcioglu, M. and Xi, B. (2016). Adversarial data mining: Big data meets cyber security. https://\nwww.sigsac.org/ccs/CCS2016/tutorials/index.html . ACM Conference on Computer and\nCommunications Security (CCS 2016) Tutorial.\nKavosh and Littman, M. L. (2017). A new softmax operator for reinforcement learning. In ICML .\n127\n\nKearns, M. and Singh, S. (2002). Near-optimal reinforcement learning in polynomial time. Machine Learning ,\n49:209–232.\nKeramati, R., Whang, J., Cho, P., and Brunskill, E. (2018). Strategic Object Oriented Reinforcement Learning.\nArXiv .\nKhadka, S. and Tumer, K. (2018). Evolutionary reinforcement learning. In NIPS .\nKhandani, A. E., Kim, A. J., and Lo, A. W. (2010). Consumer credit-risk models via machine-learning algo-\nrithms. Journal of Banking & Finance , 34:2767–2787.\nKhetarpal, K., Ahmed, Z., Cianﬂone, A., Islam, R., and Pineau, J. (2018). Re-evaluate: Reproducibility in\nevaluating reinforcement learning algorithms. In Reproducibility in Machine Learning Workshop at ICML .\nKillian, T., Daulton, S., Konidaris, G., and Doshi-Velez, F. (2017). Robust and efﬁcient transfer learning with\nhidden-parameter markov decision processes. In NIPS .\nKim, B., Farahmand, A.-m., Pineau, J., and Precup, D. (2014). Learning from limited demonstrations. In NIPS .\nKingma, D. P., Rezende, D. J., Mohamed, S., and Welling, M. (2014). Semi-supervised learning with deep\ngenerative models. In NIPS .\nKirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J.,\nRamalho, T., Grabska-Barwinska, A., Hassabis, D., Clopath, C., Kumaran, D., and Hadsell, R. (2017).\nOvercoming catastrophic forgetting in neural networks. PNAS , 114(13):3521–3526.\nKlein, G., Kim, Y ., Deng, Y ., Senellart, J., and Rush, A. M. (2017). OpenNMT: Open-Source Toolkit for Neural\nMachine Translation. ArXiv .\nKober, J., Bagnell, J. A., and Peters, J. (2013). Reinforcement learning in robotics: A survey. International\nJournal of Robotics Research , 32(11):1238–1278.\nKoch, G., Zemel, R., and Salakhutdinov, R. (2015). Siamese neural networks for one-shot image recognition.\nInICML .\nKocsis, L. and Szepesv ´ari, C. (2006). Bandit based monte-carlo planning. In Proceedings of the European\nconference on Machine Learning (ECML) .\nKoller, D. and Friedman, N. (2009). Probabilistic Graphical Models: Principles and Techniques . MIT Press.\nKolter, J. Z. and Ng, A. Y . (2009). Near-bayesian exploration in polynomial time. In ICML .\nKompella, V . R., Stollenga, M., Luciw, M., and Schmidhuber, J. (2017). Continual curiosity-driven skill\nacquisition from high-dimensional video inputs for humanoid robots. Artiﬁcial Intelligence , 247:313–335.\nKonda, V . R. and Tsitsiklis, J. N. (2003). On actor-critic algorithms. SIAM Journal on Control and Optimiza-\ntion, 42(4):1143–1166.\nKong, X., Xin, B., Wang, Y ., and Hua, G. (2017). Collaborative deep reinforcement learning for joint object\nsearch. In CVPR .\nKornblith, S., Shlens, J., and Le, Q. V . (2018). Do Better ImageNet Models Transfer Better? ArXiv .\nKosorok, M. R. and Moodie, E. E. M. (2015). Adaptive Treatment Strategies in Practice: Planning Trials and\nAnalyzing Data for Personalized Medicine . ASA-SIAM Series on Statistics and Applied Probability.\nKottur, S., Moura, J. M., Lee, S., and Batra, D. (2017). Natural language does not emerge ’naturally’ in\nmulti-agent dialog. In EMNLP .\nKrakovsky, M. (2016). Reinforcement renaissance. Communications of the ACM , 59(8):12–14.\nKraska, T., Beutel, A., Chi, E. H., Dean, J., and Polyzotis, N. (2018). The case for learned index structures. In\nInternational Conference on Management of Data (ACM SIGMOD) .\nKriegeskorte, N. and Douglas, P. K. (2018). Cognitive computational neuroscience. Nature Neuroscience .\nKrishnan, S., Yang, Z., Goldberg, K., Hellerstein, J., and Stoica, I. (2018). Learning to Optimize Join Queries\nWith Deep Reinforcement Learning. ArXiv e-prints .\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classiﬁcation with deep convolutional neural\nnetworks. In NIPS .\n128\n\nKrull, A., Brachmann, E., Nowozin, S., Michel, F., Shotton, J., and Rother, C. (2017). Poseagent: Budget-\nconstrained 6d object pose estimation via reinforcement learning. In CVPR .\nKuhn, M. and Johnson, K. (2013). Applied Predictive Modeling . Springer.\nKulkarni, T. D., Narasimhan, K. R., Saeedi, A., and Tenenbaum, J. B. (2016). Hierarchical deep reinforcement\nlearning: Integrating temporal abstraction and intrinsic motivation. In NIPS .\nKulkarni, T. D., Saeedi, A., Gautam, S., and Gershman, S. J. (2016). Deep Successor Reinforcement Learning.\nArXiv .\nKulkarni, T. D., Whitney, W., Kohli, P., and Tenenbaum, J. B. (2015). Deep convolutional inverse graphics\nnetwork. In NIPS .\nKumaraswamy, R., Schlegel, M., White, A., and White, M. (2018). Context-dependent upper-conﬁdence\nbounds for directed exploration. In NIPS .\nKurach, K., Lucic, M., Zhai, X., Michalski, M., and Gelly, S. (2018). The GAN Landscape: Losses, Architec-\ntures, Regularization, and Normalization. ArXiv .\nKurzweil, R. (1992). The Age of Intelligent Machines . The MIT Press.\nLage, I., Ross, A., Gershman, S. J., Kim, B., and Doshi-Velez, F. (2018). Human-in-the-loop interpretability\nprior. In NIPS .\nLagoudakis, M. G. and Parr, R. (2003). Least-squares policy iteration. JMLR , 4:1107 – 1149.\nLake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning through proba-\nbilistic program induction. Science , 350(6266):1332–1338.\nLake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. (2016). Building machines that learn and\nthink like people. Behavioral and Brain Sciences , 24:1–101.\nLample, G. and Chaplot, D. S. (2017). Playing FPS games with deep reinforcement learning. In AAAI .\nLanctot, M., Srinivasan, S., Zambaldi, V ., Perolat, J., karl Tuyls, Munos, R., and Bowling, M. (2018). Actor-\ncritic policy optimization in partially observable multiagent environments. In NIPS .\nLanctot, M., Zambaldi, V ., Gruslys, A., Lazaridou, A., Tuyls, K., Perolat, J., Silver, D., and Graepel, T. (2017).\nA uniﬁed game-theoretic approach to multiagent reinforcement learning. In NIPS .\nLangford, J. and Zhang, T. (2007). The epoch-greedy algorithm for multi-armed bandits with side information.\nInNIPS .\nLattimore, F., Lattimore, T., and Reid, M. D. (2016). Causal bandits: Learning good interventions via causal\ninference. In NIPS .\nLattimore, T., Kveton, B., Li, S., and Szepesv ´ari, C. (2018). Toprank: A practical algorithm for online stochastic\nranking. In NIPS .\nLattimore, T. and Szepesv ´ari, C. (2018). Bandit Algorithms . Cambridge University Press.\nLazic, N., Boutilier, C., Lu, T., Wong, E., Roy, B., Ryu, M., and Imwalle, G. (2018). Data center cooling using\nmodel-predictive control. In NIPS .\nLe, H. M., Jiang, N., Agarwal, A., Dud ´ık, M., Yue, Y ., and Daum ´e, III, H. (2018). Hierarchical imitation and\nreinforcement learning. In ICML .\nLe, Q. V ., Ranzato, M., Monga, R., Devin, M., Chen, K., Corrado, G. S., Dean, J., and Ng, A. Y . (2012).\nBuilding high-level features using large scale unsupervised learning. In ICML .\nLeCun, Y . (2018). Learning world models: The next step towards AI. https://www.youtube.com/\nwatch?v=IK_svPHKA0U . IJCAI-ECAI 2018 Invited Talk.\nLeCun, Y ., Bengio, Y ., and Hinton, G. (2015). Deep learning. Nature , 521:436–444.\nLeCun, Y . and Manning, C. (2018). What innate priors should we build into the architecture of deep learning\nsystems? https://www.youtube.com/watch?v=fKk9KhGRBdI .\nLeCun, Y . and Marcus, G. (2017). Does AI need more innate machinery? https://www.youtube.com/\nwatch?v=aCCotxqxFsk .\n129\n\nLee, A. X., Levine, S., and Abbeel, P. (2017). Learning visual servoing with deep features and trust region\nﬁtted Q-iteration. In ICLR .\nLee, K.-F. (2018). AI Superpowers: China, Silicon Valley, and the New World Order . Houghton Mifﬂin\nHarcourt.\nLee, L., Parisotto, E., Chaplot, D. S., Xing, E., and Salakhutdinov, R. (2018). Gated path planning networks.\nInICML .\nLehman, J., Chen, J., Clune, J., and Stanley, K. O. (2017). Safe Mutations for Deep and Recurrent Neural\nNetworks through Output Gradients. ArXiv .\nLeibo, J. Z., Zambaldi, V ., Lanctot, M., Marecki, J., and Graepel, T. (2017). Multi-agent reinforcement learning\nin sequential social dilemmas. In AAMAS .\nLeike, J., Ibarz, B., Amodei, D., Irving, G., and Legg, S. (2018). Reward learning from human preferences and\ndemonstrations in Atari. In NIPS .\nLenz, I., Knepper, R., and Saxena, A. (2015). DeepMPC: Learning deep latent features for model predictive\ncontrol. In RSS.\nLeonetti, M., Iocchi, L., and Stone, P. (2016). A synthesis of automated planning and reinforcement learning\nfor efﬁcient, robust decision-making. Artiﬁcial Intelligence , 241:103–130.\nLevine, S. (2018). CS 294: Deep reinforcement learning. http://rail.eecs.berkeley.edu/\ndeeprlcourse/ .\nLevine, S. (2018). Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review. ArXiv .\nLevine, S., Finn, C., Darrell, T., and Abbeel, P. (2016). End-to-end training of deep visuomotor policies. JMLR ,\n17:1–40.\nLevine, S. and Koltun, V . (2014). Learning complex neural network policies with trajectory optimization. In\nICML .\nLewis, M., Yarats, D., Dauphin, Y . N., Parikh, D., and Batra, D. (2017). Deal or no deal? end-to-end learning\nfor negotiation dialogues. In EMNLP .\nLeyton-Brown, K. and Shoham, Y . (2008). Essentials of Game Theory: A Concise, Multidisciplinary Introduc-\ntion. Morgan & Claypool.\nLi, J., Miller, A. H., Chopra, S., Ranzato, M., and Weston, J. (2017a). Dialogue learning with human-in-the-\nloop. In ICLR .\nLi, J., Miller, A. H., Chopra, S., Ranzato, M., and Weston, J. (2017b). Learning through dialogue interactions\nby asking questions. In ICLR .\nLi, J., Monroe, W., Ritter, A., Galley, M., Gao, J., and Jurafsky, D. (2016). Deep reinforcement learning for\ndialogue generation. In EMNLP .\nLi, K. and Malik, J. (2017). Learning to optimize. In ICLR .\nLi, K. and Malik, J. (2017). Learning to Optimize Neural Nets. ArXiv .\nLi, L. (2012). Sample complexity bounds of exploration. In Wiering, M. and van Otterlo, M., editors, Rein-\nforcement Learning: State-of-the-Art , pages 175–204. Springer-Verlag Berlin Heidelberg.\nLi, L., Chu, W., Langford, J., and Schapire, R. E. (2010). A contextual-bandit approach to personalized news\narticle recommendation. In WWW .\nLi, L., Littman, M. L., Walsh, T. J., and Strehl, A. L. (2011). Knows what it knows: a framework for self-aware\nlearning. Machine Learning , 82(3):399–443.\nLi, M. and Vit ´anyi, P. (2008). An Introduction to Kolmogorov Complexity and Its Applications (3rd edition) .\nSpringer.\nLi, S., Xiao, S., Zhu, S., Du, N., Xie, Y ., and Song, L. (2018a). Learning temporal point processes via rein-\nforcement learning. In NIPS .\nLi, X., Chen, Y .-N., Li, L., and Gao, J. (2017). End-to-End Task-Completion Neural Dialogue Systems. ArXiv .\n130\n\nLi, Y . (2017). Deep Reinforcement Learning: An Overview. ArXiv .\nLi, Y . (2018). About AI conferences. https://medium.com/@yuxili/\nabout-ai-conferences-bbd8fbf84290 .\nLi, Y ., Liang, X., Hu, Z., and Xing, E. (2018b). Hybrid retrieval-generation reinforced agent for medical image\nreport generation. In NIPS .\nLi, Y ., Song, J., and Ermon, S. (2017). InfoGAIL: Interpretable imitation learning from visual demonstrations.\nInNIPS .\nLi, Y ., Szepesv ´ari, C., and Schuurmans, D. (2009). Learning exercise policies for American options. In\nProceedings of the International Conference on Artiﬁcial Intelligence and Statistics (AISTATS) .\nLiang, C., Berant, J., Le, Q., Forbus, K. D., and Lao, N. (2017a). Neural symbolic machines: Learning semantic\nparsers on freebase with weak supervision. In ACL.\nLiang, C., Berant, J., Le, Q., Forbus, K. D., and Lao, N. (2017b). Neural symbolic machines: Learning semantic\nparsers on freebase with weak supervision. In ACL.\nLiang, C., Norouzi, M., Berant, J., Le, Q. V ., and Lao, N. (2018). Memory augmented policy optimization for\nprogram synthesis with generalization. In NIPS .\nLiang, X., Lee, L., and Xing, E. P. (2017c). Deep variation-structured reinforcement learning for visual rela-\ntionship and attribute detection. In CVPR .\nLiang, Y ., Machado, M. C., Talvitie, E., and Bowling, M. (2016). State of the art control of atari games using\nshallow reinforcement learning. In AAMAS .\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y ., Silver, D., and Wierstra, D. (2016).\nContinuous control with deep reinforcement learning. In ICLR .\nLin, L.-J. (1992). Self-improving reactive agents based on reinforcement learning, planning and teaching.\nMachine learning , 8(3):293–321.\nLin, Z., Gehring, J., Khalidov, V ., and Synnaeve, G. (2017). Stardata: A starcraft ai research dataset. In AAAI\nConference on Artiﬁcial Intelligence and Interactive Digital Entertainment (AIIDE) .\nLing, Y ., Hasan, S. A., Datla, V ., Qadir, A., Lee, K., Liu, J., and Farri, O. (2017). Diagnostic inferencing via\nimproving clinical concept extraction with deep reinforcement learning: A preliminary study. In Machine\nLearning for Healthcare .\nLipton, Z., Li, X., Gao, J., Li, L., Ahmed, F., and Deng, L. (2018). BBQ-networks: Efﬁcient exploration in\ndeep reinforcement learning for task-oriented dialogue systems. In AAAI .\nLipton, Z. C. (2018). The mythos of model interpretability. ACM Queue , 16(3).\nLipton, Z. C. and Steinhardt, J. (2018). Troubling trends in machine learning scholarship. In ICML 2018\nMachine Learning Debates Workshop .\nLittman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning? In ICML .\nLittman, M. L. (2015). Reinforcement learning improves behaviour from evaluative feedback. Nature ,\n521:445–451.\nLittman, M. L., Sutton, R. S., and Singh, S. (2001). Predictive representations of state. In NIPS .\nLiu, B. (2012). Sentiment Analysis and Opinion Mining . Morgan & Claypool.\nLiu, B., Gemp, I., Ghamvamzadeh, M., Liu, J., Mahadevan, S., and Petrik, M. (2018a). Proximal gradient\ntemporal difference learning algorithms. JAIR .\nLiu, C. and Tomizuka, M. (2016). Algorithmic safety measures for intelligent industrial co-robots. In ICRA .\nLiu, C. and Tomizuka, M. (2017). Designing the robot behavior for safe human robot interactions. In Wang,\nY . and Zhang, F., editors, Trends in Control and Decision-Making for Human-Robot Collaboration Systems .\nSpringer.\nLiu, C., Zoph, B., Shlens, J., Hua, W., Li, L.-J., Fei-Fei, L., Yuille, A., Huang, J., and Murphy, K. (2017).\nProgressive Neural Architecture Search. ArXiv .\n131\n\nLiu, F., Li, S., Zhang, L., Zhou, C., Ye, R., Wang, Y ., and Lu, J. (2017). 3DCNN-DQN-RNN: A deep rein-\nforcement learning framework for semantic parsing of large-scale 3d point clouds. In ICCV .\nLiu, H., Simonyan, K., Vinyals, O., Fernando, C., and Kavukcuoglu, K. (2017). Hierarchical Representations\nfor Efﬁcient Architecture Search. ArXiv .\nLiu, N., Li, Z., Xu, Z., Xu, J., Lin, S., Qiu, Q., Tang, J., and Wang, Y . (2017a). A hierarchical framework of\ncloud resource allocation and power management using deep reinforcement learning. In 37th IEEE Interna-\ntional Conference on Distributed Computing (ICDCS 2017) .\nLiu, Q., Li, L., Tang, Z., and Zhou, D. (2018b). Breaking the curse of horizon: Inﬁnite-horizon off-policy\nestimation. In NIPS .\nLiu, S., Long, M., Wang, J., and Jordan, M. (2018c). Generalized zero-shot learning with deep calibration\nnetwork. In NIPS .\nLiu, S., Zhu, Z., Ye, N., Guadarrama, S., and Murphy, K. (2016). Improved Image Captioning via Policy\nGradient optimization of SPIDEr. ArXiv .\nLiu, Y ., Chen, J., and Deng, L. (2017b). Unsupervised sequence classiﬁcation using sequential output statistics.\nInNIPS .\nLiu, Y ., Gottesman, O., Raghu, A., Komorowski, M., Faisal, A. A., Doshi-Velez, F., and Brunskill, E. (2018d).\nRepresentation balancing MDPs for off-policy policy evaluation. In NIPS .\nLiu, Y . and Sun, J. (2017). Deep learning for health care applications: Challenges and solutions. https://\nsites.google.com/view/icml2017-deep-health-tutorial/home . ICML 2017 Tutorial.\nLiu, Y .-E., Mandel, T., Brunskill, E., and Popovi ´c, Z. (2014). Trading off scientiﬁc knowledge and user learning\nwith multi-armed bandits. In Educational Data Mining (EDM) .\nLo, A. W. (2004). The Adaptive Markets Hypothesis: Market efﬁciency from an evolutionary perspective.\nJournal of Portfolio Management , 30:15–29.\nLo, A. W., Mamaysky, H., and Wang, J. (2000). Foundations of technical analysis: Computational algorithms,\nstatistical inference, and empirical implementation. Journal of Finance , 55(4):1705–1765.\nLong, M., Cao, Y ., Wang, J., and Jordan, M. I. (2015). Learning transferable features with deep adaptation\nnetworks. In ICML .\nLong, M., Cao, Z., Wang, J., and Yu, P. S. (2017). Learning multiple tasks with multilinear relationship\nnetworks. In NIPS .\nLong, M., Zhu, H., Wang, J., and Jordan, M. I. (2016). Unsupervised domain adaptation with residual transfer\nnetworks. In NIPS .\nLongstaff, F. A. and Schwartz, E. S. (2001). Valuing American options by simulation: a simple least-squares\napproach. The Review of Financial Studies , 14(1):113–147.\nLoos, S., Irving, G., Szegedy, C., and Kaliszyk, C. (2017). Deep Network Guided Proof Search. ArXiv .\nLopez-Paz, D., Nishihara, R., Chintala, S., Sch ¨olkopf, B., and Bottou, L. (2017). Discovering causal signals in\nimages. In CVPR .\nLopez-Paz, D. and Ranzato, M. (2017). Gradient Episodic Memory for Continuum Learning. ArXiv .\nLowe, R., Wu, Y ., Tamar, A., Harb, J., Abbeel, P., and Mordatch, I. (2017). Multi-agent actor-critic for mixed\ncooperative-competitive environments. In NIPS .\nLu, J., Xiong, C., Parikh, D., and Socher, R. (2016). Knowing When to Look: Adaptive Attention via A Visual\nSentinel for Image Captioning. ArXiv .\nLu, T., Boutilier, C., and Schuurmans, D. (2018). Non-delusional Q-learning and value-iteration. In NIPS .\nLucic, M., Kurach, K., Michalski, M., Gelly, S., and Bousquet, O. (2018). Are GANs created equal? a large-\nscale study. In NIPS .\nLuenberger, D. G. (1997). Investment Science . Oxford University Press.\nLuo, R., Tian, F., Qin, T., Chen, E., and Liu, T. (2018). Neural architecture optimization. In NIPS .\n132\n\nMachado, M. C., Bellemare, M. G., and Bowling, M. (2017). A Laplacian framework for option discovery in\nreinforcement learning. In ICML .\nMachado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., and Bowling, M. (2017). Revisiting\nthe Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents. ArXiv .\nMadhavan, V ., Such, F. P., Clune, J., Stanley, K., and Lehman, J. (2018). Improving exploration in evolution\nstrategies for deep reinforcement learning via a population of novelty-seeking agents. In NIPS .\nMahadevan, S. (2018a). Imagination machines. https://people.cs.umass.edu/ ˜mahadeva/\nIJCAI_2018_Tutorial/ . IJCAI 2018 Tutorial.\nMahadevan, S. (2018b). Imagination machines: A new challenge for artiﬁcial intelligence. In AAAI .\nMahadevan, S. and Maggioni, M. (2007). Proto-value functions: A laplacian framework for learning represen-\ntation and control in markov decision processes. JMLR , 8:2169–2231.\nMahajan, D., Girshick, R., Ramanathan, V ., He, K., Paluri, M., Li, Y ., Bharambe, A., and van der Maaten, L.\n(2018). Exploring the Limits of Weakly Supervised Pretraining. ArXiv .\nMahmood, A. R., van Hasselt, H., and Sutton, R. S. (2014). Weighted importance sampling for off-policy\nlearning with linear function approximation. In NIPS .\nMahmud, M., Kaiser, M. S., Hussain, A., and Vassanelli, S. (2018). Applications of deep learning and re-\ninforcement learning to biological data. IEEE Transactions on Neural Networks and Learning Systems ,\n29(6):2063–2079.\nMalik, J. (2018). IJCAI Research Excellence Award talk. http://goo.gl/MqxYiU .\nMalinowski, M., Doersch, C., Santoro, A., and Battaglia, P. (2018). Learning Visual Question Answering by\nBootstrapping Hard Attention. ArXiv .\nMandel, T., Liu, Y . E., Levine, S., Brunskill, E., and Popovi ´c, Z. (2014). Ofﬂine policy evaluation across\nrepresentations with applications to educational games. In AAMAS .\nManhaeve, R., Dumancic, S., Kimmig, A., Demeester, T., and Raedt, L. D. (2018). DeepProbLog: Neural\nprobabilistic logic programming. In NIPS .\nManning, C. D. (2017). Last words: Computational linguistics and deep learning, a look at the importance of\nnatural language processing. https://bit.ly/2wtuemM .\nManning, C. D., Raghavan, P., and Sch ¨utze, H. (2008). Introduction to Information Retrieval . Cambridge\nUniversity Press.\nMannion, P., Duggan, J., and Howley, E. (2016). An experimental review of reinforcement learning algorithms\nfor adaptive trafﬁc signal control. In McCluskey, T., Kotsialos, A., M ¨uller, J., Kl ¨ugl, F., Rana, O., and R., S.,\neditors, Autonomic Road Transport Support Systems , pages 47–66. Springer.\nMao, H., Alizadeh, M., Menache, I., and Kandula, S. (2016). Resource management with deep reinforcement\nlearning. In ACM Workshop on Hot Topics in Networks (HotNets) .\nMao, X., Li, Q., Xie, H., Lau, R. Y . K., and Wang, Z. (2016). Least Squares Generative Adversarial Networks.\nArXiv .\nMarcus, G. (2018). Deep Learning: A Critical Appraisal. ArXiv .\nMathe, S., Pirinen, A., and Sminchisescu, C. (2016). Reinforcement learning for visual object detection. In\nCVPR .\nMaurer, A., Pontil, M., and Romera-Paredes, B. (2016). The beneﬁt of multitask representation learning. JMLR ,\n17(81):1–32.\nMcAleer, S., Agostinelli, F., Shmakov, A., and Baldi, P. (2018). Solving the Rubik’s Cube Without Human\nKnowledge. ArXiv .\nMcAllister, R. and Rasmussen, C. E. (2017). Data-efﬁcient reinforcement learning in continuous-state\nPOMDPs. In NIPS .\nMcCann, B., Bradbury, J., Xiong, C., and Socher, R. (2017). Learned in Translation: Contextualized Word\nVectors. ArXiv .\n133\n\nMcCann, B., Shirish Keskar, N., Xiong, C., and Socher, R. (2018). The Natural Language Decathlon: Multitask\nLearning as Question Answering. ArXiv .\nMelis, D. A. and Jaakkola, T. (2018). Towards robust interpretability with self-explaining neural networks. In\nNIPS .\nMelis, G., Dyer, C., and Blunsom, P. (2018). On the state of the art of evaluation in neural language models. In\nICLR .\nMerel, J., Tassa, Y ., TB, D., Srinivasan, S., Lemmon, J., Wang, Z., Wayne, G., and Heess, N. (2017). Learning\nhuman behaviors from motion capture by adversarial imitation. ArXiv .\nMesnil, G., Dauphin, Y ., Yao, K., Bengio, Y ., Deng, L., He, X., Heck, L., Tur, G., Hakkani-T ¨ur, D., Yu, D.,\nand Zweig, G. (2015). Using recurrent neural networks for slot ﬁlling in spoken language understanding.\nIEEE/ACM Transactions on Audio, Speech, and Language Processing , 23(3):530–539.\nMestres, A., Rodriguez-Natal, A., Carner, J., Barlet-Ros, P., Alarc ´on, E., Sol ´e, M., Munt ´es, V ., Meyer, D.,\nBarkai, S., Hibbett, M. J., Estrada, G., Ma `ruf, K., Coras, F., Ermagan, V ., Latapie, H., Cassar, C., Evans,\nJ., Maino, F., Walrand, J., and Cabellos, A. (2017). Knowledge-deﬁned networking. ACM SIGCOMM\nComputer Communication Review , 47(3):2–10.\nMhamdi, E. M. E., Guerraoui, R., Hendrikx, H., and Maurer, A. (2017). Dynamic safe interruptibility for\ndecentralized multi-agent reinforcement learning. In NIPS .\nMikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efﬁcient estimation of word representations in vector\nspace. In ICLR .\nMikolov, T., Grave, E., Bojanowski, P., Puhrsch, C., and Joulin, A. (2017). Advances in Pre-Training Dis-\ntributed Word Representations. ArXiv .\nMiller, J. and Hardt, M. (2018). When Recurrent Models Don’t Need To Be Recurrent. ArXiv .\nMiotto, R., Wang, F., Wang, S., Jiang, X., and Dudley, J. T. (2017). Deep learning for healthcare: review,\nopportunities and challenges. Brieﬁngs in Bioinformatics , pages 1–11.\nMirhoseini, A., Pham, H., Le, Q. V ., Steiner, B., Larsen, R., Zhou, Y ., Kumar, N., and Mohammad Norouzi,\nSamy Bengio, J. D. (2017). Device placement optimization with reinforcement learning. In ICML .\nMirowski, P., Grimes, M., Malinowski, M., Hermann, K. M., Anderson, K., Teplyashin, D., Simonyan, K.,\nkoray kavukcuoglu, Zisserman, A., and Hadsell, R. (2018). Learning to navigate in cities without a map. In\nNIPS .\nMirowski, P., Pascanu, R., Viola, F., Soyer, H., Ballard, A., Banino, A., Denil, M., Goroshin, R., Sifre, L.,\nKavukcuoglu, K., Kumaran, D., and Hadsell, R. (2017). Learning to navigate in complex environments. In\nICLR .\nMishra, N., Rohaninejad, M., Chen, X., and Abbeel, P. (2018). A simple neural attentive meta-learner. In ICLR .\nMitchell, T. (1997). Machine Learning . McGraw Hill.\nMitchell, T. and Brynjolfsson, E. (2017). Track how technology is transforming work. Nature , 544:290–292.\nMitra, B. and Craswell, N. (2017). Neural Models for Information Retrieval. ArXiv .\nMnih, V ., Badia, A. P., Mirza, M., Graves, A., Harley, T., Lillicrap, T. P., Silver, D., and Kavukcuoglu, K.\n(2016). Asynchronous methods for deep reinforcement learning. In ICML .\nMnih, V ., Heess, N., Graves, A., and Kavukcuoglu, K. (2014). Recurrent models of visual attention. In NIPS .\nMnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M.,\nFidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D.,\nWierstra, D., Legg, S., and Hassabis, D. (2015). Human-level control through deep reinforcement learning.\nNature , 518(7540):529–533.\nMo, K., Li, S., Zhang, Y ., Li, J., and Yang, Q. (2018). Personalizing a dialogue system with transfer learning.\nInAAAI .\nMonroe, D. (2017). Deep learning takes on translation. Communications of the ACM , 60(6):12–14.\nMoody, J. and Saffell, M. (2001). Learning to trade via direct reinforcement. TNN , 12(4):875–889.\n134\n\nMoore, A. and Atkeson, C. (1993). Prioritized sweeping: Reinforcement learning with less data and less time.\nMLJ, 13(1):103–130.\nMorav ˇc´ık, M., Schmid, M., Burch, N., Lis ´y, V ., Morrill, D., Bard, N., Davis, T., Waugh, K., Johanson, M.,\nand Bowling, M. (2017). Deepstack: Expert-level artiﬁcial intelligence in heads-up no-limit poker. Science ,\n356(6337):508–513.\nMorimura, T., Sugiyama, M., Kashima, H., Hachiya, H., and Tanaka, T. (2010a). Nonparametric return distri-\nbution approximation for reinforcement learning. In ICML .\nMorimura, T., Sugiyama, M., Kashima, H., Hachiya, H., and Tanaka, T. (2010b). Parametric return density\nestimation for reinforcement learning. In the Conference on Uncertainty in Artiﬁcial Intelligence (UAI) .\nMrowca, D., Zhuang, C., Wang, E., Haber, N., Fei-Fei, L., Tenenbaum, J., and Yamins, D. (2018). A ﬂexible\nneural representation for physics prediction. In NIPS .\nMukwevho, M. A. and Celik, T. (2018). Toward a smart cloud: A review of fault-tolerance methods in cloud\nsystems. IEEE Transactions on Services Computing .\nMullainathan, S. (2017). Machine learning and prediction in economics and ﬁnance. https://www.\nyoutube.com/watch?v=xl3yQBhI6vY . American Finance Association (AFA) Lecture.\nM¨uller, M. (2002). Computer go. Artiﬁcial Intelligence , 134(1-2):145–179.\nMunos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M. G. (2016). Safe and efﬁcient off-policy rein-\nforcement learning. In NIPS .\nMurphy, J. J. (1999). Technical Analysis of the Financial Markets: A Comprehensive Guide to Trading Methods\nand Applications . Prentice Hall Press.\nMurphy, K. P. (2012). Machine Learning: A Probabilistic Perspective . The MIT Press.\nNachum, O., Gu, S., Lee, H., and Levine, S. (2018). Data-efﬁcient hierarchical reinforcement learning. In\nNIPS .\nNachum, O., Norouzi, M., and Schuurmans, D. (2017). Improving policy gradient by exploring under-\nappreciated rewards. In ICLR .\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D. (2017). Bridging the gap between value and policy\nbased reinforcement learning. In NIPS .\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D. (2018). Bridging the gap between value and policy\nbased reinforcement learning. In ICLR .\nNair, A., Pong, V ., Dalal, M., Bahl, S., Lin, S., and Levine, S. (2018). Visual goal-conditioned reinforcement\nlearning by representation learning. In NIPS .\nNair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De Maria, A., Panneershelvam, V ., Suleyman,\nM., Beattie, C., Petersen, S., Legg, S., Mnih, V ., Kavukcuoglu, K., and Silver, D. (2015). Massively parallel\nmethods for deep reinforcement learning. In ICML 2015 Deep Learning Workshop .\nNarasimhan, K., Kulkarni, T., and Barzilay, R. (2015). Language understanding for text-based games using\ndeep reinforcement learning. In EMNLP .\nNarasimhan, K., Yala, A., and Barzilay, R. (2016). Improving information extraction by acquiring external\nevidence with reinforcement learning. In EMNLP .\nNazari, M., Oroojlooy, A., Snyder, L., and Tak ´aˇc, M. (2018). Reinforcement learning for solving the vehicle\nrouting problem. In NIPS .\nNedi ´c, A. and Bertsekas, D. P. (2003). Least squares policy evaluation algorithms with linear function approx-\nimation. Discrete Event Dynamic Systems: Theory and Applications , 13:79–110.\nNegrinho, R., Gormley, M., and Gordon, G. (2018). Learning beam search policies via imitation learning. In\nNIPS .\nNeubig, G. (2017). Neural Machine Translation and Sequence-to-sequence Models: A Tutorial. ArXiv .\nNeuneier, R. (1997). Enhancing Q-learning for optimal asset allocation. In NIPS .\n135\n\nNg, A. (2016a). Hiring your ﬁrst chief ai ofﬁcer. . Harvard Business Review.\nNg, A. (2016b). Nuts and bolts of building applications using deep learning. https://www.youtube.\ncom/watch?v=F1ka6a13S9I .\nNg, A. (2016c). What artiﬁcial intelligence can and can’t do right now. https://hbr.org/2016/11/\nwhat-artificial-intelligence-can-and-cant-do-right-now . Harvard Business Re-\nview.\nNg, A. (2018). Machine Learning Yearning (draft) . deeplearning.ai.\nNg, A. and Russell, S. (2000). Algorithms for inverse reinforcement learning. In ICML .\nNg, A. Y ., Harada, D., and Russell, S. J. (2000). Policy invariance under reward transformations: Theory and\napplication to reward shaping. In ICML .\nNg, A. Y ., Kim, H. J., Jordan, M. I., and Sastry, S. (2004). Autonomous helicopter ﬂight via reinforcement\nlearning. In NIPS .\nNilsson, N. J. (1998). Artiﬁcial Intelligence: A New Synthesis . Morgan Kaufmann.\nNogueira, R. and Cho, K. (2016). End-to-End Goal-Driven Web Navigation. ArXiv .\nNogueira, R. and Cho, K. (2017). Task-oriented query reformulation with reinforcement learning. In EMNLP .\nNorthcutt, C. G. (2017). Artiﬁcial intelligence in online education: With an emphasis on personal-\nization. http://curtisnorthcutt.com/resources/pdf/northcutt_mit_2017_ai_in_\nonline_education.pdf .\nO’Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V . (2017). Combining policy gradient and Q-\nlearning. In ICLR .\nOh, J., Chockalingam, V ., Singh, S., and Lee, H. (2016). Control of memory, active perception, and action in\nminecraft. In ICML .\nOh, J., Guo, X., Lee, H., Lewis, R., and Singh, S. (2015). Action-conditional video prediction using deep\nnetworks in atari games. In NIPS .\nOh, J., Singh, S., and Lee, H. (2017). Value prediction network. In NIPS .\nO’Kelly, M., Sinha, A., Namkoong, H., Tedrake, R., and Duchi, J. C. (2018). Scalable end-to-end autonomous\nvehicle testing via rare-event simulation. In NIPS .\nOlah, C. and Carter, S. (2016). Attention and augmented recurrent neural networks. Distill .http://\ndistill.pub/2016/augmented-rnns .\nOlah, C., Mordvintsev, A., and Schubert, L. (2017). Feature visualization. Distill .\nOlah, C., Satyanarayan, A., Johnson, I., Carter, S., Schubert, L., Ye, K., and Mordvintsev, A. (2018). The\nbuilding blocks of interpretability. Distill . https://distill.pub/2018/building-blocks.\nOliehoek, F. A., Spaan, M. T. J., and Vlassis, N. (2008). Optimal and approximate q-value functions for\ndecentralized pomdps. JAIR , 32(1):289–353.\nOmidshaﬁei, S., Pazis, J., Amato, C., How, J. P., and Vian, J. (2017). Deep decentralized multi-task multi-agent\nreinforcement learning under partial observability. In ICML .\nOnta ˜n´on, S., Synnaeve, G., Uriarte, A., Richoux, F., Churchill, D., and Preuss, M. (2013). A survey of real-time\nstrategy game ai research and competition in starcraft. IEEE Transactions on Computational Intelligence\nand AI in Games , 5(4):293–311.\nOpenAI (2018). Learning Dexterous In-Hand Manipulation. ArXiv .\nOquab, M., Bottou, L., Laptev, I., and Sivic, J. (2015). Is object localization for free? – weakly-supervised\nlearning with convolutional neural networks. In CVPR .\nOrmoneit, D. and Sen, ´S. (2002). Kernel-based reinforcement learning. Machine Learning , 49(2-3):161–178.\nOsband, I., Aslanides, J. S., and Cassirer, A. (2018). Randomized prior functions for deep reinforcement\nlearning. In NIPS .\n136\n\nOsband, I., Blundell, C., Pritzel, A., and Roy, B. V . (2016). Deep exploration via bootstrapped DQN. In NIPS .\nOstrovski, G., Bellemare, M. G., van den Oord, A., and Munos, R. (2017). Count-based exploration with neural\ndensity models. In ICML .\nOudeyer, P.-Y ., Gottlieb, J., and Lopes, M. (2016). Intrinsic motivation, curiosity and learning: theory and\napplications in educational technologies. Progress in brain research, Elsevier , 229:257–284.\nOudeyer, P.-Y . and Kaplan, F. (2007). What is intrinsic motivation? a typology of computational approaches.\nFrontiers in neurorobotics , 1(6).\nPalm, R., Paquet, U., and Winther, O. (2018). Recurrent relational networks. In NIPS .\nPan, S. J. and Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on Knowledge and Data\nEngineering , 22(10):1345 – 1359.\nPan, Y ., Farahmand, A.-m., White, M., Nabi, S., Grover, P., and Nikovski, D. (2018). Reinforcement learning\nwith function-valued action spaces for partial differential equation control. In ICML .\nPang, Z.-J., Liu, R.-Z., Meng, Z.-Y ., Zhang, Y ., Yu, Y ., and Lu, T. (2018). On Reinforcement Learning for\nFull-length Game of StarCraft. ArXiv e-prints .\nPapernot, N., Abadi, M., Erlingsson, ´U., Goodfellow, I., and Talwar, K. (2017). Semi-supervised knowledge\ntransfer for deep learning from private training data. In ICLR .\nPapernot, N., Faghri, F., Carlini, N., Goodfellow, I., Feinman, R., Kurakin, A., Xie, C., Sharma, Y ., Brown, T.,\nRoy, A., Matyasko, A., Behzadan, V ., Hambardzumyan, K., Zhang, Z., Juang, Y .-L., Li, Z., Sheatsley, R.,\nGarg, A., Uesato, J., Gierke, W., Dong, Y ., Berthelot, D., Hendricks, P., Rauber, J., Long, R., and McDaniel,\nP. (2016). Technical Report on the CleverHans v2.1.0 Adversarial Examples Library. ArXiv .\nParisotto, E., Ba, J. L., and Salakhutdinov, R. (2016). Actor-mimic: Deep multitask and transfer reinforcement\nlearning. In ICLR .\nParisotto, E., Mohamed, A.-r., Singh, R., Li, L., Zhou, D., and Kohli, P. (2017). Neuro-symbolic program\nsynthesis. In ICLR .\nParkes, D. C. and Wellman, M. P. (2015). Economic reasoning and artiﬁcial intelligence. Science ,\n349(6245):267–272.\nParr, R. and Russell, S. (1998). Reinforcement learning with hierarchies of machines. In NIPS .\nPastor, L. and Stambaugh, R. F. (2009). Predictive systems: Living with imperfect predictors. Journal of\nFinance , (to appear).\nPasunuru, R. and Bansal, M. (2017). Reinforced video captioning with entailment rewards. In EMNLP .\nPathak, D., Agrawal, P., Efros, A. A., and Darrell, T. (2017). Curiosity-driven exploration by self-supervised\nprediction. In ICML .\nPaulus, R., Xiong, C., and Socher, R. (2017). A Deep Reinforced Model for Abstractive Summarization. ArXiv .\nPearl, J. (2009). Causality . Cambridge University Press.\nPearl, J. (2018). The seven pillars of causal reasoning with reﬂections on machine learning. UCLA Technical\nReport R-481 .\nPearl, J., Glymour, M., and Jewell, N. P. (2016). Causal Inference in Statistics: A Primer . Wiley.\nPearl, J. and Mackenzie, D. (2018). The Book of Why: The New Science of Cause and Effect . Basic Books.\nPeng, B., Li, X., Li, L., Gao, J., Celikyilmaz, A., Lee, S., and Wong, K.-F. (2017a). Composite task-completion\ndialogue system via hierarchical deep reinforcement learning. In EMNLP .\nPeng, P., Wen, Y ., Yang, Y ., Yuan, Q., Tang, Z., Long, H., and Wang, J. (2017b). Multiagent Bidirectionally-\nCoordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games.\nArXiv .\nPeng, X. B., Abbeel, P., Levine, S., and van de Panne, M. (2018a). Deepmimic: Example-guided deep rein-\nforcement learning of physics-based character skills. In SIGGRAPH .\n137\n\nPeng, X. B., Andrychowicz, M., Zaremba, W., and Abbeel, P. (2017c). Sim-to-Real Transfer of Robotic Control\nwith Dynamics Randomization. ArXiv .\nPeng, X. B., Kanazawa, A., Malik, J., Abbeel, P., and Levine, S. (2018b). Sfv: Reinforcement learning of\nphysical skills from videos. ACM Transaction on Graphics , 37(6).\nPeng, Y .-S., Tang, K., Lin, H.-T., and Chang, E. (2018c). Exploring sparse features in deep reinforcement\nlearning towards fast disease diagnosis. In NIPS .\nP´erez-D’Arpino, C. and Shah, J. A. (2017). C-learn: Learning geometric constraints from demonstrations for\nmulti-step manipulation in shared autonomy. In ICRA .\nPerolat, J., Leibo, J. Z., Zambaldi, V ., Beattie, C., Tuyls, K., and Graepel, T. (2017). A multi-agent reinforce-\nment learning model of common-pool resource appropriation. In NIPS .\nPeters, J., Janzing, D., and Sch ¨olkopf, B. (2017). Elements of Causal Inference: Foundations and Learning\nAlgorithms . MIT Press.\nPeters, J. and Neumann, G. (2015). Policy search: Methods and applications. https://icml.cc/2015/\ntutorials/PolicySearch.pdf . ICML 2015 Tutorial.\nPeters, J. and Schaal, S. (2007). Reinforcement learning by reward-weighted regression for operational space\ncontrol. In ICML .\nPetroski Such, F., Madhavan, V ., Conti, E., Lehman, J., Stanley, K. O., and Clune, J. (2017). Deep Neu-\nroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for\nReinforcement Learning. ArXiv .\nPfau, D. and Vinyals, O. (2016). Connecting Generative Adversarial Networks and Actor-Critic Methods.\nArXiv .\nPhua, C., Lee, V ., Smith, K., and Gayler, R. (2010). A Comprehensive Survey of Data Mining-based Fraud\nDetection Research. ArXiv .\nPlatt, J. (2017). Powering the next 100 years. http://goo.gl/aa7svZ . NIPS 2017 Invited Talk.\nPoggio, T., Mhaskar, H., Rosasco, L., Miranda, B., and Liao, Q. (2017). Why and when can deep-but not\nshallow-networks avoid the curse of dimensionality: a review. International Journal of Automation and\nComputing , 14(5):503–519.\nPong, V ., Gu, S., Dalal, M., and Levine, S. (2018). Temporal difference models: Model-free deep rl for model-\nbased control. In ICLR .\nPoole, D., Mackworth, A., and Goebel, R. (1998). Computational Intelligence: A Logical Approach . Oxford\nUniversity Press.\nPopov, I., Heess, N., Lillicrap, T., Hafner, R., Barth-Maron, G., Vecerik, M., Lampe, T., Tassa, Y ., Erez, T., and\nRiedmiller, M. (2017). Data-efﬁcient Deep Reinforcement Learning for Dexterous Manipulation. ArXiv .\nPopova, M., Isayev, O., and Tropsha, A. (2018). Deep reinforcement learning for de novo drug design. Science\nAdvances , 4(7).\nPowell, W. B. (2010). Merging AI and OR to solve high-dimensional stochastic optimization problems using\napproximate dynamic programming. INFORMS Journal on Computing , 22(1):2–17.\nPowell, W. B. (2011). Approximate Dynamic Programming: Solving the curses of dimensionality (2nd Edition) .\nJohn Wiley and Sons.\nPrashanth, L., Jie, C., Fu, M., Marcus, S., and Szepes ´ari, C. (2016). Cumulative prospect theory meets rein-\nforcement learning: Prediction and control. In ICML .\nPrecup, D. (2018). Temporal abstraction. https://dlrlsummerschool.ca . Deep Learning and Rein-\nforcement Learning Summer School.\nPrecup, D., Sutton, R. S., and Dasgupta, S. (2001). Off-policy temporal difference learning with function\napproximation. In ICML .\nPress, G. (2016). Artiﬁcial intelligence pioneers: Peter Norvig, Google. http://goo.gl/Ly8gxQ . Forbes.\nProvost, F. and Fawcett, T. (2013). Data Science for Business . O’Reilly Media.\n138\n\nRabinowitz, N. C., Perbet, F., Song, H. F., Zhang, C., Eslami, S. A., and Botvinick, M. (2018). Machine theory\nof mind. In ICML .\nRaedt, L. D., Frasconi, P., Kersting, K., and Muggleton, S., editors (2008). Probabilistic inductive logic pro-\ngramming: theory and applications . Springer.\nRahimi, A. and Recht, B. (2007). Random features for large-scale kernel machines. In NIPS .\nRahimi, A. and Recht, B. (2017a). An addendum to alchemy. http://www.argmin.net/2017/12/\n11/alchemy-addendum/ .\nRahimi, A. and Recht, B. (2017b). Back when we were kids. https://www.youtube.com/watch?v=\nQi1Yry33TQE . NIPS 2017 Test-of-time Award Talk.\nRahimi, A. and Recht, B. (2017c). Reﬂections on random kitchen sinks. http://www.argmin.net/\n2017/12/05/kitchen-sinks/ .\nRajendran, J., Lakshminarayanan, A., Khapra, M. M., P, P., and Ravindran, B. (2017). Attend, adapt and\ntransfer: Attentive deep architecture for adaptive transfer from multiple sources in the same domain. ICLR .\nRajkomar, A., Oren, E., Chen, K., Dai, A. M., Hajaj, N., Liu, P. J., Liu, X., Sun, M., Sundberg, P., Yee, H.,\nZhang, K., Duggan, G. E., Flores, G., Hardt, M., Irvine, J., Le, Q., Litsch, K., Marcus, J., Mossin, A.,\nTansuwan, J., Wang, D., Wexler, J., Wilson, J., Ludwig, D., V olchenboum, S. L., Chou, K., Pearson, M.,\nMadabushi, S., Shah, N. H., Butte, A. J., Howell, M., Cui, C., Corrado, G., and Dean, J. (2018). Scalable\nand accurate deep learning for electronic health records. Nature Digital Medicine , 1(18).\nRajpurkar, P., Irvin, J., Bagul, A., Ding, D., Duan, T., Mehta, H., Yang, B., Zhu, K., Laird, D., Ball, R. L.,\nLanglotz, C., Shpanskaya, K., Lungren, M. P., and Ng, A. Y . (2018). Mura: Large dataset for abnormality\ndetection in musculoskeletal radiographs. In Conference on Medical Imaging with Deep Learning .\nRanzato, M., Chopra, S., Auli, M., and Zaremba, W. (2016). Sequence level training with recurrent neural\nnetworks. In ICLR .\nRao, Y ., Lu, J., and Zhou, J. (2017). Attention-aware deep reinforcement learning for video face recognition.\nInICCV .\nRashid, T., Samvelyan, M., de Witt, C. S., Farquhar, G., Foerster, J., and Whiteson, S. (2018). QMIX: Mono-\ntonic value function factorisation for deep multi-agent reinforcement learning. In ICML .\nRasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning . The MIT Press.\nRawlik, K., Toussaint, M., and Vijayakumar, S. (2012). On stochastic optimal control and reinforcement\nlearning by approximate inference. In RSS.\nReal, E., Aggarwal, A., Huang, Y ., and Le, Q. V . (2018). Regularized Evolution for Image Classiﬁer Architec-\nture Search. ArXiv .\nReal, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y . L., Tan, J., Le, Q., and Kurakin, A. (2017). Large-scale\nevolution of image classiﬁers. In ICML .\nReddy, G., Wong-Ng, J., Celani, A., Sejnowski, T. J., and Vergassola, M. (2018). Glider soaring via reinforce-\nment learning in the ﬁeld. Nature .\nReed, S. and de Freitas, N. (2016). Neural programmer-interpreters. In ICLR .\nReichstaller, A. and Knapp, A. (2017). Transferring context-dependent test inputs. In IEEE International\nConference on Software Quality, Reliability and Security (QRS) .\nRen, S., He, K., Girshick, R., and Sun, J. (2015). Faster R-CNN: Towards real-time object detection with region\nproposal networks. In NIPS .\nRen, Z., Wang, X., Zhang, N., Lv, X., and Li, L.-J. (2017). Deep reinforcement learning-based image captioning\nwith embedding reward. In CVPR .\nRennie, S. J., Marcheret, E., Mroueh, Y ., Ross, J., and Goel, V . (2017). Self-critical sequence training for image\ncaptioning. In CVPR .\nRhinehart, N., Kitani, K., and Vernaza, P. (2018). Inverse reinforcement learning for computer vision. https:\n//www.youtube.com/watch?v=JbNeLiNnvII . CVPR 2018 Tutorial.\n139\n\nRhinehart, N. and Kitani, K. M. (2017). First-person activity forecasting with online inverse reinforcement\nlearning. In ICCV .\nRich, E. and Knight, K. (1991). Artiﬁcial Intelligence . McGraw-Hill.\nRiedmiller, M. (2005). Neural ﬁtted Q iteration - ﬁrst experiences with a data efﬁcient neural reinforcement\nlearning method. In European Conference on Machine Learning (ECML) .\nRiemer, M., Liu, M., and Tesauro, G. (2018). Learning abstract options. In NIPS .\nRockt ¨aschel, T. and Riedel, S. (2017). End-to-end Differentiable Proving. ArXiv .\nRoss, S., Gordon, G. J., and Bagnell, J. A. (2010). A reduction of imitation learning and structured prediction\nto no-regret online learning. In Proceedings of the International Conference on Artiﬁcial Intelligence and\nStatistics (AISTATS) .\nRossi, F., Beek, P. V ., and Walsh, T., editors (2006). Handbook of constraint programming . Elsevier.\nRotmensch, M., Halpern, Y ., Tlimat, A., Horng, S., and Sontag, D. (2017). Learning a health knowledge graph\nfrom electronic medical records. Nature Scientiﬁc Reports , 7(5994).\nRowland, M., Bellemare, M. G., Dabney, W., Munos, R., and Teh, Y . W. (2018). An analysis of categorical dis-\ntributional reinforcement learning. In Proceedings of the International Conference on Artiﬁcial Intelligence\nand Statistics (AISTATS) .\nRuder, S. (2017). An Overview of Multi-Task Learning in Deep Neural Networks. ArXiv .\nRuelens, F., Claessens, B. J., Vandael, S., Schutter, B. D., Babu ˇska, R., and Belmans, R. (2016). Residential\ndemand response of thermostatically controlled loads using batch reinforcement learning. IEEE Transactions\non Smart Grid , PP(99):1–11.\nRummery, G. A. and Niranjan, M. (1994). On-line Q-learning using connectionist sytems . Technical Report\nCUED/F-INFENG-TR 166, Cambridge University, UK.\nRussell, S. and Norvig, P. (2009). Artiﬁcial Intelligence: A Modern Approach (3rd edition) . Pearson.\nRusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., and\nHadsell, R. (2016). Progressive neural networks. In NIPS Deep Learning Symposium .\nRusu, A. A., Vecerik, M., Roth ¨orl, T., Heess, N., Pascanu, R., and Hadsell, R. (2017). Sim-to-real robot learning\nfrom pixels with progressive nets. In Conference on Robot Learning (CoRL) .\nSabour, S., Frosst, N., and Hinton, G. E. (2017). Dynamic routing between capsules. In NIPS .\nSadeghi, F., Toshev, A., Jang, E., and Levine, S. (2018). Sim2real view invariant visual servoing by recurrent\ncontrol. In CVPR .\nSalakhutdinov, R. (2016). Foundations of unsupervised deep learning, Deep Learning School,\nhttps://www.bayareadlschool.org. https://www.youtube.com/watch?v=rK6bchqeaN8 .\nSalimans, T., Ho, J., Chen, X., and Sutskever, I. (2017). Evolution Strategies as a Scalable Alternative to\nReinforcement Learning. ArXiv .\nSantoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T. (2016). Meta-learning with memory-\naugmented neural networks. In ICML .\nSantoro, A., Faulkner, R., Raposo, D., Rae, J., Chrzanowski, M., Weber, T., Wierstra, D., Vinyals, O., Pascanu,\nR., and Lillicrap, T. (2018). Relational recurrent neural networks. ArXiv .\nSantoro, A., Raposo, D., Barrett, D. G. T., Malinowski, M., Pascanu, R., Battaglia, P., and Lillicrap, T. (2017).\nA simple neural network module for relational reasoning. In NIPS .\nSanturkar, S., Tsipras, D., Ilyas, A., and Madry, A. (2018). How does batch normalization help optimization?\n(no, it is not about internal covariate shift). In NIPS .\nSaon, G., Sercu, T., Rennie, S., and Kuo, H.-K. J. (2016). The IBM 2016 English Conversational Telephone\nSpeech Recognition System. In Annual Meeting of the International Speech Communication Association\n(INTERSPEECH) .\nSaria, S. (2014). A $3 trillion challenge to computational scientists: Transforming healthcare delivery. IEEE\nIntelligent Systems , 29(4):82–87.\n140\n\nScarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G. (2009). The graph neural network\nmodel. IEEE Transactions on Neural Networks , 20(1):61–80.\nSchaal, S. (2006). Dynamic movement primitives -a framework for motor control in humans and humanoid\nrobotics. In Kimura, H., Tsuchiya, K., Ishiguro, A., and Witte, H., editors, Adaptive Motion of Animals and\nMachines . Springer, Tokyo.\nSchaeffer, J. (1997). One Jump Ahead . Springer-Verlag, New York.\nSchaeffer, J., Burch, N., Bj ¨ornsson, Y ., Kishimoto, A., M ¨uller, M., Lake, R., Lu, P., and Sutphen, S. (2007).\nCheckers is solved. Science , 317:1518–1522.\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015). Universal value function approximators. In ICML .\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. (2016). Prioritized experience replay. In ICLR .\nSchmidhuber, J. (1987). Evolutionary principles in self-referential learning . Diploma thesis, Tech. Univ.\nMunich.\nSchmidhuber, J. (1991). A possibility for implementing curiosity and boredom in model-building neural con-\ntrollers. In International Conference on Simulation of Adaptive Behavior on From Animals to Animats , pages\n222–227.\nSchmidhuber, J. (2010). Formal theory of creativity, fun, and intrinsic motivation (1990-2010). IEEE Transac-\ntions on Autonomous Mental Development , 2(3):230–247.\nSchmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks , 61:85–117.\nSch¨olkopf, B. and Smola, A. J. (2001). Learning with Kernels: Support Vector Machines, Regularization,\nOptimization, and Beyond . MIT Press.\nSchulman, J. (2017). The nuts and bolts of deep reinforcement learning research. https://www.youtube.\ncom/watch?v=8EcdaCk9KaQ .\nSchulman, J., Abbeel, P., and Chen, X. (2017a). Equivalence Between Policy Gradients and Soft Q-Learning.\nArXiv .\nSchulman, J., Levine, S., Moritz, P., Jordan, M. I., and Abbeel, P. (2015). Trust region policy optimization. In\nICML .\nSchulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. (2016). High-dimensional continuous control\nusing generalized advantage estimation. In ICLR .\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017b). Proximal Policy Optimization\nAlgorithms. ArXiv .\nSculley, D., Snoek, J., Wiltschko, A., and Rahimi, A. (2018). Winner’s curse? on pace, progress, and empirical\nrigor. In ICLR Workshop Track .\nSegler, M. H. S., Preuss, M., and Waller, M. P. (2018). Planning chemical syntheses with deep neural networks\nand symbolic AI. Nature , 555:604–610.\nSener, O., Sener, O., and Koltun, V . (2018). Multi-task learning as multi-objective optimization. In NIPS .\nSerban, I. V ., Lowe, R., Charlin, L., and Pineau, J. (2018). A survey of available corpora for building data-driven\ndialogue systems: The journal version. Dialogue & Discourse , 9(1):1–49.\nSharma, S., Lakshminarayanan, A. S., and Ravindran, B. (2017). Learning to repeat: Fine grained action\nrepetition for deep reinforcement learning. In ICLR .\nShe, L. and Chai, J. (2017). Interactive learning for acquisition of grounded verb semantics towards human-\nrobot communication. In ACL.\nShen, Y ., Huang, P.-S., Gao, J., and Chen, W. (2017). Reasonet: Learning to stop reading in machine compre-\nhension. In KDD .\nSherstan, C., Machado, M. C., and Pilarski, P. M. (2018). Accelerating learning in constructive predictive\nframeworks with the successor representation. In IROS .\nShi, J.-C., Yu, Y ., Da, Q., Chen, S.-Y ., and Zeng, A.-X. (2018). Virtual-Taobao: Virtualizing Real-world Online\nRetail Environment for Reinforcement Learning. ArXiv .\n141\n\nShoham, Y . and Leyton-Brown, K. (2009). Multiagent Systems: Algorithmic, Game-Theoretic, and Logical\nFoundations . Cambridge University Press.\nShoham, Y ., Powers, R., and Grenager, T. (2007). If multi-agent learning is the answer, what is the question?\nArtiﬁcial Intelligence , 171:365–377.\nShortreed, S. M., Laber, E., Lizotte, D. J., Stroup, T. S., Pineau, J., and Murphy, S. A. (2011). Informing\nsequential clinical decision-making through reinforcement learning: an empirical study. MLJ, 84:109–136.\nShrivastava, A., Pﬁster, T., Tuzel, O., Susskind, J., Wang, W., and Webb, R. (2017). Learning from simulated\nand unsupervised images through adversarial training. In CVPR .\nSilver, D. (2015). UCL reinforcement learning course. http://www0.cs.ucl.ac.uk/staff/d.\nsilver/web/Teaching.html .\nSilver, D. (2016). Deep reinforcement learning, a tutorial at ICML 2016. http://icml.cc/2016/\ntutorials/deep_rl_tutorial.pdf .\nSilver, D. (2017). Deep reinforcement learning with subgoals. https://goo.gl/h9Mz1a . NIPS 2017\nHierarchical RL Workshop Invited Talk.\nSilver, D. (2018). Principles of deep rl. http://www.deeplearningindaba.com/uploads/1/0/\n2/6/102657286/principles_of_deep_rl.pdf . Deep Learning Indaba 2018.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J.,\nAntonoglou, I., Panneershelvam, V ., Lanctot, M., et al. (2016a). Mastering the game of go with deep neural\nnetworks and tree search. Nature , 529(7587):484–489.\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D.,\nGraepel, T., Lillicrap, T., Simonyan, K., and Hassabis, D. (2017). Mastering Chess and Shogi by Self-Play\nwith a General Reinforcement Learning Algorithm. ArXiv .\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. (2014). Deterministic policy\ngradient algorithms. In ICML .\nSilver, D., Newnham, L., Barker, D., Weller, S., and McFall, J. (2013). Concurrent reinforcement learning from\ncustomer interactions. In ICML .\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M.,\nBolton, A., Chen, Y ., Lillicrap, T., Hui, F., Sifre, L., van den Driessche, G., Graepel, T., and Hassabis, D.\n(2017). Mastering the game of go without human knowledge. Nature , 550:354–359.\nSilver, D., Sutton, R. S., and M ¨uller, M. (2012). Temporal-difference search in computer Go. Machine Learn-\ning, 87:183–219.\nSilver, D., van Hasselt, H., Hessel, M., Schaul, T., Guez, A., Harley, T., Dulac-Arnold, G., Reichert, D.,\nRabinowitz, N., Barreto, A., and Degris, T. (2016b). The predictron: End-to-end learning and planning. In\nNIPS 2016 Deep Reinforcement Learning Workshop .\nSingh, S. (2017). Steps towards continual learning. https://mila.quebec/en/cours/\ndeep-learning-summer-school-2017/ . Deep Learning and Reinforcement Learning Summer\nSchool 2017.\nSingh, S., Jaakkola, T., Littman, M. L., and Szepesv ´ari, C. (2000). Convergence results for single-step on-policy\nreinforcement-learning algorithms. Machine Learning , 38(3):287–308.\nSingh, S., James, M., and Rudary, M. (2004). Predictive state representations: A new theory for modeling\ndynamical systems. In the Conference on Uncertainty in Artiﬁcial Intelligence (UAI) .\nSingh, S., Lewis, R., Barto, A., and Sorg, J. (2010). Intrinsically motivated reinforcement learning: An evolu-\ntionary perspective. IEEE Transactions on Autonomous Mental Development , 2(2).\nSingh, S., Lewis, R. L., and Barto, A. G. (2009). Where do rewards come from? In the Annual Conference of\nthe Cognitive Science Society (CogSci) .\nSirignano, J. (2016). Deep Learning for Limit Order Books. ArXiv .\nSmith, V ., Chiang, C.-K., Sanjabi, M., and Talwalkar, A. (2017). Federated multi-task learning. In NIPS .\nSnell, J., Swersky, K., and Zemel, R. S. (2017). Prototypical Networks for Few-shot Learning. ArXiv .\n142\n\nSocher, R., Pennington, J., Huang, E. H., Ng, A. Y ., and Manning, C. D. (2011). Semi-supervised recursive\nautoencoders for predicting sentiment distributions. In EMNLP .\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C., Ng, A., and Potts, C. (2013). Recursive deep models\nfor semantic compositionality over a sentiment tree- bank. In EMNLP .\nSohn, S., Oh, J., and Lee, H. (2018). Multitask reinforcement learning for zero-shot generalization with subtask\ndependencies. In NIPS .\nSommer, R. and Paxson, V . (2010). Outside the closed world: On using machine learning for network intrusion\ndetection. In IEEE Symposium on Security and Privacy (SP) .\nSong, D. (2018). AI and security: Lessons, challenges and future directions. https://www.youtube.\ncom/watch?v=dcEo2r7_V44 . ICML 2018 Invited Talk.\nSong, J., Ren, H., Sadigh, D., and Ermon, S. (2018). Multi-agent generative adversarial imitation learning. In\nNIPS .\nSrinivas, A., Jabri, A., Abbeel, P., Levine, S., and Finn, C. (2018). Universal planning networks. In ICML .\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: A simple way\nto prevent neural networks from overﬁtting. JMLR , 15:1929–1958.\nStadie, B., Yang, G., Abbeel, P., Wu, Y ., Duan, Y ., Chen, X., Houthooft, R., and Sutskever, I. (2018). The\nimportance of sampling in meta-reinforcement learning. In NIPS .\nStadie, B. C., Abbeel, P., and Sutskever, I. (2017). Third person imitation learning. In ICLR .\nStoica, I., Song, D., Popa, R. A., Patterson, D. A., Mahoney, M. W., Katz, R. H., Joseph, A. D., Jordan, M.,\nHellerstein, J. M., Gonzalez, J., Goldberg, K., Ghodsi, A., Culler, D. E., and Abbeel, P. (2017). A berkeley\nview of systems challenges for AI. Technical Report No. UCB/EECS-2017-159 .\nStone, P., Brooks, R., Brynjolfsson, E., Calo, R., Etzioni, O., Hager, G., Hirschberg, J., Kalyanakrishnan,\nS., Kamar, E., Kraus, S., Leyton-Brown, K., Parkes, D., Press, W., Saxenian, A., Shah, J., Tambe, M., and\nTeller, A. (2016). Artiﬁcial Intelligence and Life in 2030 - One Hundred Year Study on Artiﬁcial Intelligence:\nReport of the 2015-2016 Study Panel . Stanford University.\nStone, P. and Veloso, M. (2000). Multiagent systems: A survey from a machine learning perspective. Au-\ntonomous Robots , 8(3):345–383.\nStrehl, A. L., Li, L., and Littman, M. L. (2009). Reinforcement learning in ﬁnite MDPs: PAC analysis. JMLR ,\n10:2413–2444.\nStrehl, A. L. and Littman, M. L. (2008). An analysis of model-based interval estimation for markov decision\nprocesses. Journal of Computer and System Sciences , 74:1309–1331.\nStrub, F., de Vries, H., Mary, J., Piot, B., Courville, A., and Pietquin, O. (2017). End-to-end optimization of\ngoal-driven and visually grounded dialogue systems. In IJCAI .\nSu, P.-H., Gas ˇi´c, M., Mrks ˇi´c, N., Rojas-Barahona, L., Ultes, S., Vandyke, D., Wen, T.-H., and Young, S. (2016).\nOn-line active reward learning for policy optimisation in spoken dialogue systems. In ACL.\nSukhbaatar, S., Szlam, A., and Fergus, R. (2016). Learning multiagent communication with backpropagation.\nInNIPS .\nSukhbaatar, S., Weston, J., and Fergus, R. (2015). End-to-end memory networks. In NIPS .\nSun, P., Sun, X., Han, L., Xiong, J., Wang, Q., Li, B., Zheng, Y ., Liu, J., Liu, Y ., Liu, H., and Zhang, T. (2018).\nTStarBots: Defeating the Cheating Level Builtin AI in StarCraft II in the Full Game. ArXiv e-prints .\nSun, W., Gordon, G., Boots, B., and Bagnell, J. (2018). Dual policy iteration. In NIPS .\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zambaldi, V ., Jaderberg, M., Lanctot, M., Sonnerat, N.,\nLeibo, J. Z., Tuyls, K., and Graepel, T. (2017). Value-decomposition networks for cooperative multi-agent\nlearning. In AAMAS .\nSupan ˇciˇc, III, J. and Ramanan, D. (2017). Tracking as online decision-making: Learning a policy from stream-\ning videos with reinforcement learning. In ICCV .\nSutskever, I., Vinyals, O., and Le, Q. V . (2014). Sequence to sequence learning with neural networks. In NIPS .\n143\n\nSutton, R. (2016). Reinforcement learning for artiﬁcial intelligence, course slides. http://www.\nincompleteideas.net/sutton/609%20dropbox/ .\nSutton, R. (2018). The next big step in AI: Planning with a learned model. https://www.youtube.com/\nwatch?v=6-Uiq8-wKrg . University of Alberta RLAI Tea Time Talk.\nSutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine Learning , 3(1):9–44.\nSutton, R. S. (1990). Integrated architectures for learning, planning, and reacting based on approximating\ndynamic programming. In ICML .\nSutton, R. S. (1992). Adapting bias by gradient descent: An incremental version of delta-bar-delta. In AAAI .\nSutton, R. S. and Barto, A. G. (1998). Reinforcement Learning: An Introduction . MIT Press.\nSutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: An Introduction (2nd Edition) . MIT Press.\nSutton, R. S., Barto, A. G., and Williams, R. J. (1992). Reinforcement learning is direct adaptive optimal\ncontrol. IEEE Control Systems , 12(2):19–22.\nSutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S., Silver, D., Szepesv ´ari, C., and Wiewiora, E. (2009a). Fast\ngradient-descent methods for temporal-difference learning with linear function approximation. In ICML .\nSutton, R. S., Mahmood, A. R., and White, M. (2016). An emphatic approach to the problem of off-policy\ntemporal-difference learning. JMLR , 17:1–29.\nSutton, R. S., McAllester, D., Singh, S., and Mansour, Y . (2000). Policy gradient methods for reinforcement\nlearning with function approximation. In NIPS .\nSutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski, P. M., White, A., and Precup, D. (2011). Horde: A\nscalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction, , proc.\nof 10th. In AAMAS .\nSutton, R. S., Precup, D., and Singh, S. (1999). Between MDPs and semi-MDPs: A framework for temporal\nabstraction in reinforcement learning. Artiﬁcial Intelligence , 112(1-2):181–211.\nSutton, R. S., Rafols, E. J., and Koop, A. (2005). Temporal abstraction in temporal-difference networks. In\nNIPS .\nSutton, R. S., Szepesv ´ari, C., and Maei, H. R. (2009b). A convergent O( n) algorithm for off-policy temporal-\ndifference learning with linear function approximation. In NIPS .\nSutton, R. S. and Tanner, B. (2004). Temporal-difference networks. In NIPS .\nSyed, U., Bowling, M., and Schapire, R. E. (2008). Apprenticeship learning using linear programming. In\nICML .\nSyed, U. and Schapire, R. E. (2007). A game-theoretic approach to apprenticeship learning. In NIPS .\nSyed, U. and Schapire, R. E. (2010). A reduction from apprenticeship learning to classiﬁcation. In NIPS .\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R. (2013). Intriguing\nproperties of neural networks. ArXiv .\nSzepesv ´ari, C. (2010). Algorithms for Reinforcement Learning . Morgan & Claypool.\nTadepalli, P., Givan, R., and Driessens, K. (2004). Relational reinforcement learning: An overview. In ICML\nWorkshop on Relational Reinforcement Learning .\nTamar, A., Abbeel, P., Yang, G., Kurutach, T., and Russell, S. (2018). Learning plannable representations with\ncausal InfoGAN. In NIPS .\nTamar, A., Wu, Y ., Thomas, G., Levine, S., and Abbeel, P. (2016). Value iteration networks. In NIPS .\nTan, M. (1993). Multi-agent reinforcement learning: Independent vs. cooperative agents. In ICML .\nTang, D., Li, X., Gao, J., Wang, C., Li, L., and Jebara, T. (2018a). Subgoal discovery for hierarchical dialogue\npolicy learning. In EMNLP .\nTang, G., Muller, M., Rios, A., and Sennrich, R. (2018b). Why self-attention? a targeted evaluation of neural\nmachine translation architectures. In EMNLP .\n144\n\nTang, H., Houthooft, R., Foote, D., Stooke, A., Chen, X., Duan, Y ., Schulman, J., Turck, F. D., and Abbeel, P.\n(2017). Exploration: A study of count-based exploration for deep reinforcement learning. In NIPS .\nTassa, Y ., Doron, Y ., Muldal, A., Erez, T., Li, Y ., de Las Casas, D., Budden, D., Abdolmaleki, A., Merel, J.,\nLefrancq, A., Lillicrap, T., and Riedmiller, M. (2018). DeepMind Control Suite. ArXiv .\nTaylor, M. E. and Stone, P. (2009). Transfer learning for reinforcement learning domains: A survey. JMLR ,\n10:1633–1685.\nTenenbaum, J. (2018). Building machines that learn & think like people. https://www.youtube.com/\nwatch?v=RB78vRUO6X8 . ICML 2018 Invited Talk.\nTesauro, G. (1994). TD-Gammon, a self-teaching backgammon program, achieves master-level play. Neural\nComputation , 6(2):215–219.\nTessler, C., Givony, S., Zahavy, T., Mankowitz, D. J., and Mannor, S. (2017). A deep hierarchical approach to\nlifelong learning in minecraft. In AAAI .\nTheocharous, G., Thomas, P. S., and Ghavamzadeh, M. (2015). Personalized ad recommendation systems for\nlife-time value optimization with guarantees. In IJCAI .\nThrun, S. and Pratt, L., editors (1998). Learning to Learn . Springer.\nThue, D., Bulitko, V ., Spetch, M., and Wasylishen, E. (2007). Interactive storytelling: A player modelling\napproach. In AAAI Conference on Artiﬁcial Intelligence and Interactive Digital Entertainment (AIIDE) .\nTian, Y ., Gong, Q., Shang, W., Wu, Y ., and Zitnick, L. (2017). ELF: An extensive, lightweight and ﬂexible\nresearch platform for real-time strategy games. In NIPS .\nTian, Y . and Zhu, Y . (2016). Better computer go player with neural network and long-term prediction. In ICLR .\nTrischler, A., Ye, Z., Yuan, X., and Suleman, K. (2016). Natural language comprehension with the epireader.\nInEMNLP .\nTsitsiklis, J. N. and Van Roy, B. (1997). An analysis of temporal-difference learning with function approxima-\ntion. IEEE Transactions on Automatic Control , 42(5):674–690.\nTsitsiklis, J. N. and Van Roy, B. (2001). Regression methods for pricing complex American-style options.\nIEEE Transactions on Neural Networks , 12(4):694–703.\nUpadhyay, U., De, A., and Rodriguez, M. G. (2018). Deep reinforcement learning of marked temporal point\nprocesses. In NIPS .\nUrban, J., Kaliszyk, C., Michalewski, H., and Ol ˇs´ak, M. (2018). Reinforcement learning of theorem proving.\nInNIPS .\nUsunier, N., Synnaeve, G., Lin, Z., and Chintala, S. (2017). Episodic exploration for deep deterministic policies:\nAn application to StarCraft micromanagement tasks. In ICLR .\nValiant, L. (1984). A theory of the learnable. Communications of the ACM , 27(11):1134–1142.\nvan den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A.,\nand Kavukcuoglu, K. (2016). WaveNet: A Generative Model for Raw Audio. ArXiv .\nvan den Oord, A., Kalchbrenner, N., Vinyals, O., Espeholt, L., Graves, A., and Kavukcuoglu, K. (2016).\nConditional image generation with PixelCNN decoders. In NIPS .\nvan den Oord, A., Li, Y ., and Vinyals, O. (2018). Representation Learning with Contrastive Predictive Coding.\nArXiv .\nvan der Maaten, L. and Hinton, G. (2008). Visualizing data using t-SNE. JMLR , pages 1–48.\nvan der Pol, E. and Oliehoek, F. A. (2017). Coordinated deep reinforcement learners for trafﬁc light control. In\nNIPS’16 Workshop on Learning, Inference and Control of Multi-Agent Systems .\nvan Hasselt, H. (2010). Double Q-learning. In NIPS .\nvan Hasselt, H., Guez, A., , and Silver, D. (2016). Deep reinforcement learning with double Q-learning. In\nAAAI .\n145\n\nvan Seijen, H., Fatemi, M., Romoff, J., Laroche, R., Barnes, T., and Tsang, J. (2017). Hybrid reward architecture\nfor reinforcement learning. In NIPS .\nvan Seijen, H., van Hasselt, H., Whiteson, S., and Wiering, M. (2009). A theoretical and empirical analysis of\nexpected sarsa. In ADPRL .\nVapnik, V . N. (1998). Statistical Learning Theory . Wiley.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I.\n(2017). Attention is all you need. In NIPS .\nVenkatraman, A., Rhinehart, N., Sun, W., Pinto, L., Hebert, M., Boots, B., Kitani, K. M., and Bagnell, J. A.\n(2017). Predictive-state decoders: Encoding the future into recurrent networks. In NIPS .\nVeˇcer´ık, M., Hester, T., Scholz, J., Wang, F., Pietquin, O., Piot, B., Heess, N., Roth ¨orl, T., Lampe, T., and\nRiedmiller, M. (2017). Leveraging demonstrations for deep reinforcement learning on robotics problems\nwith sparse rewards. In NIPS .\nVezhnevets, A. S., Mnih, V ., Agapiou, J., Osindero, S., Graves, A., Vinyals, O., and Kavukcuoglu, K. (2016).\nStrategic attentive writer for learning macro-actions. In NIPS .\nVezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver, D., and Kavukcuoglu, K. (2017).\nFeudal networks for hierarchical reinforcement learning. In ICML .\nVinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., and Wierstra, D. (2016). Matching networks for one\nshot learning. In NIPS .\nVinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Sasha Vezhnevets, A., Yeo, M., Makhzani, A., K ¨uttler, H.,\nAgapiou, J., Schrittwieser, J., Quan, J., Gaffney, S., Petersen, S., Simonyan, K., Schaul, T., van Hasselt,\nH., Silver, D., Lillicrap, T., Calderone, K., Keet, P., Brunasso, A., Lawrence, D., Ekermo, A., Repp, J., and\nTsing, R. (2017). StarCraft II: A New Challenge for Reinforcement Learning. ArXiv .\nVinyals, O., Fortunato, M., and Jaitly, N. (2015). Pointer networks. In NIPS .\nWai, H.-T., Wang, P. Z., Yang, Z., and Hong, M. (2018). Multi-agent reinforcement learning via double\naveraging primal-dual optimization. In NIPS .\nWang, J., Yu, L., Zhang, W., Gong, Y ., Xu, Y ., Wang, B., Zhang, P., and Zhang, D. (2017a). IRGAN: A\nminimax game for unifying generative and discriminative information retrieval models. In SIGIR .\nWang, J. X., Kurth-Nelson, Z., Kumaran, D., Tirumala, D., Soyer, H., Leibo, J. Z., Hassabis, D., and Botvinick,\nM. (2018a). Prefrontal cortex as a meta-reinforcement learning system. Nature Neuroscience , 21:860–868.\nWang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z., Munos, R., Blundell, C., Kumaran, D.,\nand Botvinick, M. (2016). Learning to reinforcement learn. ArXiv .\nWang, S. I., Liang, P., and Manning, C. D. (2016a). Learning language games through interaction. In ACL.\nWang, T., Liao, R., Ba, J., and Fidler, S. (2018b). Nervenet: Learning structured policy with graph neural\nnetworks. In ICLR .\nWang, T., WU, Y ., Moore, D., and Russell, S. (2018c). Meta-learning MCMC proposals. In NIPS .\nWang, W., Yang, N., Wei, F., Chang, B., and Zhou, M. (2017b). Gated self-matching networks for reading\ncomprehension and question answering. In ACL.\nWang, W. Y ., Li, J., and He, X. (2018d). Deep reinforcement learning for NLP. https://www.cs.ucsb.\nedu/ ˜william/papers/ACL2018DRL4NLP.pdf . ACL 2018 Tutorial.\nWang, X., Chen, W., Wang, Y .-F., and Wang, W. Y . (2018e). No metrics are perfect: Adversarial reward\nlearning for visual storytelling. In ACL.\nWang, X., Chen, W., Wu, J., Wang, Y .-F., and Wang, W. Y . (2018f). Video captioning via hierarchical rein-\nforcement learning. In CVPR .\nWang, Z., Bapst, V ., Heess, N., Mnih, V ., Munos, R., Kavukcuoglu, K., and de Freitas, N. (2017c). Sample\nefﬁcient actor-critic with experience replay. In ICLR .\nWang, Z., Merel, J., Reed, S., Wayne, G., de Freitas, N., and Heess, N. (2017). Robust Imitation of Diverse\nBehaviors. ArXiv .\n146\n\nWang, Z. and O’Boyle, M. (2018). Machine learning in compiler optimization. Proceedings of the IEEE ,\nPP(99):1–23.\nWang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and de Freitas, N. (2016b). Dueling network\narchitectures for deep reinforcement learning. In ICML .\nWatkins, C. J. C. H. and Dayan, P. (1992). Q-learning. Machine Learning , 8:279–292.\nWatter, M., Springenberg, J. T., Boedecker, J., and Riedmiller, M. (2015). Embed to control: A locally linear\nlatent dynamics model for control from raw images. In NIPS .\nWatters, N., Tacchetti, A., Weber, T., Pascanu, R., Battaglia, P., and Zoran, D. (2017). Visual interaction\nnetworks: Learning a physics simulator from video. In NIPS .\nWayne, G., Hung, C.-C., Amos, D., Mirza, M., Ahuja, A., Grabska-Barwinska, A., Rae, J., Mirowski, P., Leibo,\nJ. Z., Santoro, A., Gemici, M., Reynolds, M., Harley, T., Abramson, J., Mohamed, S., Rezende, D., Saxton,\nD., Cain, A., Hillier, C., Silver, D., Kavukcuoglu, K., Botvinick, M., Hassabis, D., and Lillicrap, T. (2018).\nUnsupervised Predictive Memory in a Goal-Directed Agent. ArXiv .\nWeber, T., Racani `ere, S., Reichert, D. P., Buesing, L., Guez, A., Jimenez Rezende, D., Puigdom `enech Badia,\nA., Vinyals, O., Heess, N., Li, Y ., Pascanu, R., Battaglia, P., Silver, D., and Wierstra, D. (2017). Imagination-\naugmented agents for deep reinforcement learning. In NIPS .\nWeiss, K., Khoshgoftaar, T. M., and Wang, D. (2016). A survey of transfer learning. Journal of Big Data , 3(9).\nWelleck, S., Mao, J., Cho, K., and Zhang, Z. (2017). Saliency-based sequential image attention with multiset\nprediction. In NIPS .\nWen, M. and Topcu, U. (2018). Constrained cross-entropy method for safe reinforcement learning. In NIPS .\nWen, T.-H., Vandyke, D., Mrksic, N., Gasic, M., Rojas-Barahona, L. M., Su, P.-H., Ultes, S., and Young, S.\n(2017). A network-based end-to-end trainable task-oriented dialogue system. In Proceedings of the 15th\nConference of the European Chapter of the Association for Computational Linguistics (EACL) .\nWen, Z., O’Neill, D., and Maei, H. (2015). Optimal demand response using device-based reinforcement learn-\ning. IEEE Transactions on Smart Grid , 6(5):2312–2324.\nWeston, J., Chopra, S., and Bordes, A. (2015). Memory networks. In ICLR .\nWhite, A. and White, M. (2016). Investigating practical linear temporal difference learning. In AAMAS .\nWhye Teh, Y ., Bapst, V ., Czarnecki, W. M., Quan, J., Kirkpatrick, J., Hadsell, R., Heess, N., and Pascanu, R.\n(2017). Distral: Robust multitask reinforcement learning. In NIPS .\nWichrowska, O., Maheswaranathan, N., Hoffman, M. W., Gomez Colmenarejo, S., Denil, M., de Freitas, N.,\nand Sohl-Dickstein, J. (2017). Learned optimizers that scale and generalize. In ICML .\nWilliams, J. D., Asadi, K., and Zweig, G. (2017). Hybrid code networks: practical and efﬁcient end-to-end\ndialog control with supervised and reinforcement learning. In ACL.\nWilliams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learn-\ning. Machine Learning , 8(3):229–256.\nWinston, P. H. (1992). Artiﬁcial Intelligence . Pearson.\nWong, C., Houlsby, N., Lu, Y ., and Gesmundo, A. (2018). Transfer learning with neural AutoML. In NIPS .\nWu, J., Li, L., and Wang, W. Y . (2018). Reinforced co-training. In NAACL .\nWu, J., Lu, E., Kohli, P., Freeman, B., and Tenenbaum, J. (2017a). Learning to see physics via visual de-\nanimation. In NIPS .\nWu, J., Tenenbaum, J. B., and Kohli, P. (2017b). Neural scene de-rendering. In CVPR .\nWu, J., Yildirim, I., Lim, J. J., Freeman, B., and Tenenbaum, J. (2015). Galileo: Perceiving physical object\nproperties by integrating a physics engine with deep learning. In NIPS .\nWu, L., Xia, Y ., Zhao, L., Tian, F., Qin, T., Lai, J., and Liu, T.-Y . (2017c). Adversarial Neural Machine\nTranslation. ArXiv .\n147\n\nWu, Y ., Mansimov, E., Liao, S., Grosse, R., and Ba, J. (2017). Scalable trust-region method for deep reinforce-\nment learning using kronecker-factored approximation. In NIPS .\nWu, Y ., Schuster, M., Chen, Z., Le, Q. V ., Norouzi, M., Macherey, W., Krikun, M., Cao, Y ., Gao, Q., Macherey,\nK., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser, L., Gouws, S., Kato, Y ., Kudo, T., Kazawa, H.,\nStevens, K., Kurian, G., Patil, N., Wang, W., Young, C., Smith, J., Riesa, J., Rudnick, A., Vinyals, O.,\nCorrado, G., Hughes, M., and Dean, J. (2016). Google’s neural machine translation system: Bridging the\ngap between human and machine translation. ArXiv .\nWu, Y . and Tian, Y . (2017). Training agent for ﬁrst-person shooter game with actor-critic curriculum learning.\nInICLR .\nWymann, B., Espi ´e, E., Guionneau, C., Dimitrakakis, C., and R ´emi Coulom, A. S. (2014). TORCS, The Open\nRacing Car Simulator. http://www.torcs.org .\nXia, Y ., Tan, X., Tian, F., Qin, T., Yu, N., and Liu, T.-Y . (2018). Model-level dual learning. In ICML .\nXiao, C., JinchengMei, and M ¨uller, M. (2018). Memory-augmented monte carlo tree search. In AAAI .\nXie, N., Hachiya, H., and Sugiyama, M. (2012). Artist agent: A reinforcement learning approach to automatic\nstroke generation in oriental ink painting. In ICML .\nXiong, C., Zhong, V ., and Socher, R. (2017a). Dynamic coattention networks for question answering. In ICLR .\nXiong, W., Droppo, J., Huang, X., Seide, F., Seltzer, M., Stolcke, A., Yu, D., and Zweig, G. (2017b). The mi-\ncrosoft 2016 conversational speech recognition system. In The IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) .\nXiong, W., Hoang, T., and Wang, W. Y . (2017c). Deeppath: A reinforcement learning method for knowledge\ngraph reasoning. In EMNLP .\nXiong, W., Wu, L., Alleva, F., Droppo, J., Huang, X., and Stolcke, A. (2017). The Microsoft 2017 Conversa-\ntional Speech Recognition System. ArXiv .\nXu, J. and Zhu, Z. (2018). Reinforced continual learning. In NIPS .\nXu, K., Ba, J. L., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R., Zemel, R. S., and Bengio, Y . (2015).\nShow, attend and tell: Neural image caption generation with visual attention. In ICML .\nXu, L. D., He, W., and Li, S. (2014). Internet of things in industries: A survey. IEEE Transactions on Industrial\nInformatics , 10(4):2233–2243.\nXu, Z., van Hasselt, H., and Silver, D. (2018). Meta-Gradient Reinforcement Learning. ArXiv .\nYahya, A., Li, A., Kalakrishnan, M., Chebotar, Y ., and Levine, S. (2017). Collective robot reinforcement\nlearning with distributed asynchronous guided policy search. In IROS .\nYang, B. and Mitchell, T. (2017). Leveraging knowledge bases in LSTMs for improving machine reading. In\nACL.\nYang, F., Lyu, D., Liu, B., and Gustafson, S. (2018). Peorl: Integrating symbolic planning and hierarchical\nreinforcement learning for robust decision-making. In IJCAI .\nYang, Z., He, X., Gao, J., Deng, L., and Smola, A. (2016). Stacked attention networks for image question\nanswering. In CVPR .\nYang, Z., Hu, J., Salakhutdinov, R., and Cohen, W. W. (2017). Semi-supervised qa with generative domain-\nadaptive nets. In ACL.\nYannakakis, G. N. and Togelius, J. (2018). Artiﬁcial Intelligence and Games . Springer.\nYao, H., Szepesvari, C., Sutton, R. S., Modayil, J., and Bhatnagar, S. (2014). Universal option models. In\nNIPS .\nYi, K., Wu, J., Gan, C., Torralba, A., Kohli, P., and Tenenbaum, J. (2018). Neural-Symbolic VQA: Disentan-\ngling reasoning from vision and language understanding. In NIPS .\nYi, Z., Zhang, H., Tan, P., and Gong, M. (2017). Dualgan: Unsupervised dual learning for image-to-image\ntranslation. In ICCV .\n148\n\nYogatama, D., Blunsom, P., Dyer, C., Grefenstette, E., and Ling, W. (2017). Learning to compose words into\nsentences with reinforcement learning. In ICLR .\nYoon, J., Kim, T., Dia, O., Kim, S., Bengio, Y ., and Ahn, S. (2018). Bayesian model-agnostic meta-learning.\nInNIPS .\nYosinski, J., Clune, J., Bengio, Y ., and Lipson, H. (2014). How transferable are features in deep neural net-\nworks? In NIPS .\nYoung, S., Ga ˇsi´c, M., Thomson, B., and Williams, J. D. (2013). POMDP-based statistical spoken dialogue\nsystems: a review. Proceedings of IEEE , 101(5):1160–1179.\nYoung, T., Hazarika, D., Poria, S., and Cambria, E. (2017). Recent Trends in Deep Learning Based Natural\nLanguage Processing. ArXiv .\nYu, F., Xian, W., Chen, Y ., Liu, F., Liao, M., Madhavan, V ., and Darrell, T. (2018). BDD100K: A Diverse\nDriving Video Database with Scalable Annotation Tooling. ArXiv .\nYu, L., Zhang, W., Wang, J., and Yu, Y . (2017). SeqGAN: Sequence generative adversarial nets with policy\ngradient. In AAAI .\nYu, T., Finn, C., Xie, A., Dasari, S., Zhang, T., Abbeel, P., and Levine, S. (2018). One-shot imitation from\nobserving humans via domain-adaptive meta-learning. In RSS.\nYu, Y .-L., Li, Y ., Szepesv ´ari, C., and Schuurmans, D. (2009). A general projection property for distribution\nfamilies. In NIPS .\nYuan, X., He, P., Zhu, Q., and Li, X. (2017). Adversarial Examples: Attacks and Defenses for Deep Learning.\nArXiv .\nYue, Y . and Le, H. M. (2018). Imitation learning. https://sites.google.com/view/\nicml2018-imitation-learning/ . ICML 2018 Tutorial.\nYun, S., Choi, J., Yoo, Y ., Yun, K., and Young Choi, J. (2017). Action-decision networks for visual tracking\nwith deep reinforcement learning. In CVPR .\nZagoruyko, S. and Komodakis, N. (2017). Paying more attention to attention: Improving the performance of\nconvolutional neural networks via attention transfer. In ICLR .\nZahavy, T., Harush, M., Merlis, N., Mankowitz, D. J., and Mannor, S. (2018). Learn what not to learn: Action\nelimination with deep reinforcement learning. In NIPS .\nZambaldi, V ., Raposo, D., Santoro, A., Bapst, V ., Li, Y ., Babuschkin, I., Tuyls, K., Reichert, D., Lillicrap, T.,\nLockhart, E., Shanahan, M., Langston, V ., Pascanu, R., Botvinick, M., Vinyals, O., and Battaglia, P. (2018).\nRelational Deep Reinforcement Learning. ArXiv .\nZaremba, W. and Sutskever, I. (2015). Reinforcement Learning Neural Turing Machines - Revised. ArXiv .\nZhang, C., Patras, P., and Haddadi, H. (2018). Deep Learning in Mobile and Wireless Networking: A Survey.\nArXiv .\nZhang, J., Ding, Y ., Shen, S., Cheng, Y ., Sun, M., Luan, H., and Liu, Y . (2017). THUMT: An Open Source\nToolkit for Neural Machine Translation. ArXiv .\nZhang, J., Springenberg, J. T., Boedecker, J., and Burgard, W. (2017). Deep reinforcement learning with\nsuccessor features for navigation across similar environments. In IROS .\nZhang, L., Rosenblatt, G., Fetaya, E., Liao, R., Byrd, W., Might, M., Urtasun, R., and Zemel, R. (2018). Neural\nguided constraint logic programming for program synthesis. In NIPS .\nZhang, L., Wang, S., and Liu, B. (2018). Deep Learning for Sentiment Analysis : A Survey. ArXiv .\nZhang, Q. and Zhu, S.-C. (2018). Visual interpretability for deep learning: a survey. Frontiers of Information\nTechnology & Electronic Engineering , 19(1):27–39.\nZhang, S., Yao, L., Sun, A., and Tay, Y . (2017). Deep Learning based Recommender System: A Survey and\nNew Perspectives. ArXiv e-prints .\nZhang, X. and Lapata, M. (2017). Sentence simpliﬁcation with deep reinforcement learning. In EMNLP .\n149\n\nZhang, Y ., , and Yang, Q. (2018a). An overview of multi-task learning. National Science Review , 5:30–43.\nZhang, Y ., Mustaﬁzur Rahman, M., Braylan, A., Dang, B., Chang, H.-L., Kim, H., McNamara, Q., Angert, A.,\nBanner, E., Khetan, V ., McDonnell, T., Thanh Nguyen, A., Xu, D., Wallace, B. C., and Lease, M. (2016a).\nNeural Information Retrieval: A Literature Review. ArXiv .\nZhang, Y ., Pezeshki, M., Brakel, P., Zhang, S., Yoshua Bengio, C. L., and Courville, A. (2016b). Towards\nend-to-end speech recognition with deep convolutional neural networks. In Interspeech .\nZhang, Y ., Wei, Y ., and Yang, Q. (2018b). Learning to multitask. In NIPS .\nZhao, H., Zhang, S., Wu, G., Moura, J. M. F., Costeira, J. P., and Gordon, G. (2018a). Adversarial multiple\nsource domain adaptation. In NIPS .\nZhao, T. and Eskenazi, M. (2016). Towards end-to-end learning for dialog state tracking and management using\ndeep reinforcement learning. In the Annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL) .\nZhao, X., Xia, L., Zhang, L., Ding, Z., Yin, D., and Tang, J. (2018b). Deep reinforcement learning for page-wise\nrecommendations. In the ACM Conference on Recommender Systems (ACM RecSys) .\nZhao, X., Zhang, L., Ding, Z., Xia, L., Tang, J., and Yin, D. (2018c). Recommendations with negative feedback\nvia pairwise deep reinforcement learning. In KDD .\nZheng, G., Zhang, F., Zheng, Z., Xiang, Y ., Yuan, N. J., Xie, X., and Li, Z. (2018a). DRN: A deep reinforcement\nlearning framework for news recommendation. In WWW .\nZheng, Z., Oh, J., and Singh, S. (2018b). On learning intrinsic rewards for policy gradient methods. In NIPS .\nZhong, Z., Yan, J., and Liu, C.-L. (2017). Practical Network Blocks Design with Q-Learning. ArXiv .\nZhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba, A. (2015). Object detectors emerge in deep scene\nCNNs. In ICLR .\nZhou, H., Huang, M., Zhang, T., Zhu, X., and Liu, B. (2018). Emotional chatting machine: Emotional conver-\nsation generation with internal and external memory. In AAAI .\nZhou, Y . and Tuzel, O. (2018). V oxelnet: End-to-end learning for point cloud based 3d object detection. In\nCVPR .\nZhou, Z., Mertikopoulos, P., Athey, S., Bambos, N., Glynn, P. W., and Ye, Y . (2018). Multi-agent online\nlearning with asynchronous feedback loss. In NIPS .\nZhou, Z.-H. (2016). Machine Learning (in Chinese) . Tsinghua University Press, Beijing, China.\nZhou, Z.-H. and Feng, J. (2017). Deep forest: Towards an alternative to deep neural networks. In IJCAI .\nZhu, H., Liu, Q., Yuan, N. J., Qin, C., Li, J., Zhang, K., Zhou, G., Wei, F., Xu, Y ., and Chen, E. (2018). XiaoIce\nBand: A melody and arrangement generation framework for pop music. In KDD .\nZhu, J.-Y ., Park, T., Isola, P., and Efros, A. A. (2017a). Unpaired image-to-image translation using cycle-\nconsistent adversarial networks. In ICCV .\nZhu, X. and Goldberg, A. B. (2009). Introduction to semi-supervised learning . Morgan & Claypool.\nZhu, Y ., Mottaghi, R., Kolve, E., Lim, J. J., Gupta, A., Li, F.-F., and Farhadi, A. (2017b). Target-driven visual\nnavigation in indoor scenes using deep reinforcement learning. In ICRA .\nZiebart, B. D., Bagnell, J. A., and Dey, A. K. (2010). Modeling interaction via the principle of maximum causal\nentropy. In ICML .\nZiebart, B. D., Maas, A., Bagnell, J., and Dey, A. K. (2008). Maximum entropy inverse reinforcement learning.\nInAAAI .\nZinkevich, M. (2017). Rules of Machine Learning: Best Practices for ML Engineering .http://martin.\nzinkevich.org/rules_of_ml/rules_of_ml.pdf .\nZoph, B. and Le, Q. V . (2017). Neural architecture search with reinforcement learning. In ICLR .\nZoph, B., Vasudevan, V ., Shlens, J., and Le, Q. V . (2017). Learning Transferable Architectures for Scalable\nImage Recognition. ArXiv .\n150",
  "textLength": 535188
}