{
  "paperId": "5b4dc6fc74ab9003c3230468fbc439c64cd548fa",
  "title": "Embracing Structure in Data for Billion-Scale Semantic Product Search",
  "pdfPath": "5b4dc6fc74ab9003c3230468fbc439c64cd548fa.pdf",
  "text": "Embracing Structure in Data for Billion-Scale\nSemantic Product Search\nVihan Lakshman\nAmazon\nvihan@amazon.comChoon Hui Teo\nAmazon\nchoonhui@amazon.comXiaowen Chu\nAmazon\nxiaowec@amazon.com\nPriyanka Nigam\nAmazon\nnigamp@amazon.comAbhinandan Patni\nAmazon\nabhpat@amazon.comPooja Maknikar\nAmazon\nmaknp@amazon.com\nS.V.N. Vishwanathan\nAmazon\nvishy@amazon.com\nABSTRACT\nWe present principled approaches to train and deploy dyadic neural\nembedding models at the billion scale, focusing our investigation\non the application of semantic product search. When training a\ndyadic model, one seeks to embed two different types of entities\n(e.g., queries and documents or users and movies) in a common\nvector space such that pairs with high relevance are positioned\nnearby. During inference, given an embedding of one type (e.g., a\nquery or a user), one seeks to retrieve the entities of the other type\n(e.g., documents or movies, respectively) that are highly relevant.\nIn this work, we show that exploiting the natural structure of real-\nworld datasets helps address both challenges efficiently. Specifically,\nwe model dyadic data as a bipartite graph with edges between\npairs with positive associations. We then propose to partition this\nnetwork into semantically coherent clusters and thus reduce our\nsearch space by focusing on a small subset of these partitions for a\ngiven input. During training, this technique enables us to efficiently\nmine hard negative examples while, at inference, we can quickly\nfind the nearest neighbors for a given embedding. We provide offline\nexperimental results that demonstrate the efficacy of our techniques\nfor both training and inference on a billion-scale Amazon.com\nproduct search dataset.\nCCS CONCEPTS\n‚Ä¢Information systems ‚ÜíRetrieval models and ranking ;Ap-\nplied computing ;Electronic commerce .\nKEYWORDS\nneural information retrieval, product search, graph partitioning\n1 INTRODUCTION\nMany real-world problems can be modeled with the following gen-\neral paradigm: there are two different types of entities, say Qand\nD, and one observes positive interactions between pairs, say\u0000ùëû,ùëë+\u0001\nwhereùëû‚ààQandùëë+‚ààD. In some cases, negative interactions or\ncontextual information about the interactions are also observed.\nGiven this, so-called, dyadic data, the goal is to generalize, that is,\nto either a) predict new positive interaction pairs for existing enti-\nties or b) predict the interactions for unseen entities. By carefullyselecting the setsQandDand determining which interactions\nare considered positive1, a variety of problems can be cast in this\nframework. For instance, let Qbe the set of queries that users type\ninto a search engine, and Dbe the set of all documents on the\nInternet. Furthermore, define an interaction as positive if the user\ntypes a query ùëûand clicks on a document ùëë. The generalization\nproblem is to find matching documents for a query ùëû. In product\nsearch,Qis the set of queries and Dis the set of products; if a\nqueryùëûwas used to purchase a product ùëë, then that interaction is\ndefined as positive. Similarly Qcan be a set of users and Da set\nof movies, with(ùëû,ùëë)being positive if the user watched the movie\nand rated it highly. The generalization problem in this case is to\nrecommend relevant movies to a user ùëû.\nOne popular approach to dealing with dyadic data is to use dyadic\nneural embedding models; one embeds entities ùëû‚ààQandùëë‚ààD\ninto a common vector space (say the ùëô-dimensional Euclidean space\nRùëô) such that‚ü®q,d‚ü©is high for pairs with positive interactions. Here,\nwe used the notation q(respectively d) to denote the ùëô-dimensional\nembedding of ùëû(respectively ùëë), and‚ü®¬∑,¬∑‚ü©to denote the usual Eu-\nclidean dot product. As the name implies, neural models use a deep\nneural network to represent the embedding function ùëì(¬∑)which\nmapsùëû‚Üíùëìùëû(ùëû):=qandùëë‚Üíùëìùëë(ùëë):=d.\nAs web-scale data is becoming ubiquitous, there is a growing\ndesire to train and deploy such models at massive scale involving\nhundreds of millions or even billions of entities and interactions.\nMany applications also require real-time inference with latencies on\nthe order of tens or hundreds of milliseconds. We find that the same\nunderlying technique happens to address both these problems by\nleveraging the structure inherent in real-world data. In this paper,\nwe study the problems of training and deploying neural embedding\nmodels operating on dyadic data with over a billion of entities,\nfocusing our investigation on product search where the underlying\ntask is to retrieve relevant items from a large catalog for a given\nsearch query.\nTo understand the challenge in training dyadic neural embed-\nding models, note that the models require supervision with both\npositive and negative examples. As noted above, positive pairs, such\nas clicked query-document pairs, are determined by the underlying\ntask and typically only form a miniscule fraction of all possible pairs.\n1In some cases, positive and/or negative interactions may be only observed implicitly.arXiv:2110.06125v1  [cs.IR]  12 Oct 2021\n\nNegative examples, on the other hand, constitute a much larger set\nsince most pairs in the universe of possibilities are dissimilar. On\nlarge datasets, selecting random pairs as negatives proves to be too\neasy as the probability of randomly chosen items exhibiting high\ndot-product values in the embedding space becomes minuscule.\nInstead, we desire tuples of related, but ultimately dissimilar enti-\nties. Such hard negative examples induce a larger loss and thereby\nproduce more effective parameter updates. These hard negative\nexamples can improve model generalization and accelerate conver-\ngence, both in terms of wall clock time and in sample complexity.\nHowever, efficiently identifying such informative negative exam-\nples emerges as a challenge for larger datasets since it becomes\ncomputationally infeasible to examine all possible pairs.\nMoreover, given a query embedding q, finding document em-\nbeddings dwith large dot product value ‚ü®q,d‚ü©remains the essence\nof deploying dyadic embedding models where the problem is to\nidentify the ùëò-nearest neighbors of qfrom the set of embedded\ndocuments.\nAs can be seen, both training and inference boil down to the\nproblem of finding nearby points in the embedding space. If one\nhad access to an efficient oracle to solve this problem, then both\ntraining and inference of dyadic neural models could be scaled up.\nIn this paper, we present a technique for approximating such an\noracle by leveraging the fact that real-world dyadic data is highly\nstructured, often exhibiting a fine-grained separability. For instance,\nin product search, the set of queries that lead to the purchase of\ndiapers do not overlap with queries used to buy shoes.\nIn a nutshell, we model dyadic data as a bipartite graph with\nedges between positively associated pairs. We partition the nodes\nof this graph into balanced clusters by approximately minimizing\nedge-cuts (the number of edges that cross cluster boundaries). Given\nthis partitioned graph, we can speed up training and inference as\nfollows:\nTraining: Given a positive example\u0000ùëû,ùëë+\u0001, we sample items\ninDfrom graph clusters adjacent to the one containing ùëû\nto find hard negative examples of the form (ùëû,ùëë‚àí)to add to\na mini-batch during training.\nInference: Given an input embedding q, we seek to find ùëò\npoints in the setDwhose embeddings are closest to qin\nthe embedding space. For large datasets, an exact search ex-\namining all pairs becomes infeasible under the strict latency\nconstraints of industrial production systems. Moreover, ap-\nproximate nearest neighbors algorithms, while dramatically\nreducing the latency at the expense of diminished recall,\ntypically require a time overhead in building an index to\nsearch over. This index build time presents a deployment\nchallenge in real-world search systems where indexes are\noften rebuilt with updated data on a daily basis. Instead,\nwe train a classifier that predicts the clusters most likely\nto contain the nearest neighbors to qand then perform a\nsearch only within those clusters using anypopular nearest\nneighbor search algorithm as a subroutine within the parti-\ntioning. For some approximate algorithms, we can reduce\nthe index build time considerably since the indexes for the\npartitions can be constructed in parallel. Furthermore, for\nother classes of approximate algorithms, we can also reducethe search latency. Moreover, we can use our classifier to\nassign new documents to clusters and thereby also avoid\nre-running our graph partitioning step from scratch. These\nimprovements enable us to deploy approximate well-known\nnearest neighbor algorithms in a production setting.\nOur contributions can be summarized as follows:\n‚Ä¢We propose a data-dependent algorithm that exploits the\nnatural structure inherent in real-world datasets to model\ndyadic data as a bipartite graph, which in turn is partitioned\nto approximately minimize edge-cut. When applied to the\nproduct search task, the partitioned graph is used to find\nhard negative examples on the fly during training. This con-\nsiderably speeds up training and improves the generalization\ncapability of the final model.\n‚Ä¢We propose to learn a classifier which learns to predict the\nclusters which are likely to contain the nearest neighbors\nof an embedded query. This allows us to limit the search\nfor nearest neighbors to a small subset of the documents,\nspeeding up inference and index build times by orders of\nmagnitude.\n‚Ä¢Our work benchmarks nearest neighbor algorithms at the\nbillion scale under constraints representative of a real pro-\nduction product search system. In particular, we search over\nqueries one-by-one as opposed to batching which adds un-\nnecessary delay in the response to queries in a real-time\nsystem. Secondly, we retrieve 100 items for each query as\nopposed to 1 since product search systems often involve\nretrieving a larger set of results to produce a more satisfying\nshopping experience. Finally, we report and analyze the in-\ndex build time of approximate nearest neighbor algorithms\nas another key metric to influence tradeoff decisions.\n‚Ä¢We demonstrate the scaling behavior of our algorithms on\na billion-scale product search dataset, providing offline ex-\nperiments demonstrating the feasibility of embedding-based\nretrieval for semantic product search at this scale. In par-\nticular we show that our methods lead to a faster time to\nconvergence during training and improved generalizability.\nFor inference, we provide a general algorithmic primitive to\nscale both exact and approximate ùëò-nearest neighbor (KNN)\nalgorithms such as HNSW, NGT, and inverse file index (IVF)\nmethods along the dimensions of latency and index build\ntime. Additionally, we perform our KNN search on a CPU\nmachine and avoid the need for GPUs or other types of\nspecialized hardware for inference.\nOur contributions also add to the growing body of work show-\ning that data-dependent algorithms, which take advantage of the\nspecific structure in a given dataset, can dramatically outperform\ndata-independent algorithms that guard against the worst case.\nThe rest of the paper is structured as follows: in Section 2 we pro-\nvide a brief background about factorized dyadic embedding models,\nwhich are used to illustrate our ideas on scaling up training and\ninference. Our algorithms are described in Section 3. We place our\ncontributions in the context of related work in Section 4. Experi-\nmental results can be found in Section 5, and we conclude with a\nbrief discussion of future work in Section 6.\n2\n\nFigure 1: High-level factorized model architecture; one can\nalso use separate embedding layers for ùëûandùëëinstead of\nsharing parameters.\n2 BACKGROUND\nAs in [ 13] we define dyadic data as a domain with two finite sets\nof entitiesQ={ùëû1,...,ùëûùëõ}andD={ùëë1,...,ùëëùëö}, where the set\nof positive observations ùëù‚ààPcomes from the Cartesian product\nofQandD, namelyP=\b\u0000ùëû,ùëë+\u0001|ùëû‚ààQandùëë+‚ààD\twhere\nP‚äÜQ√óD . We may also be given, implicitly or explicitly, a set\nof negative observations N, which also comes from the Cartesian\nproduct ofQandD. Typically|P|‚â™ |Q√óD| andP‚à©N =\n‚àÖ. We will use\u0000ùëû,ùëë+\u0001or\u0010\nùëûùëñ,ùëë+\nùëó\u0011\nto denote elements of P, and\ncorrespondingly represent elements of Nas(ùëû,ùëë‚àí)or\u0010\nùëûùëñ,ùëë‚àí\nùëó\u0011\n.\nA dyadic embedding model, in turn, is a function that maps ele-\nments fromQorDinto anùëô-dimensional Euclidean space endowed\nwith the usual Euclidean dot product ‚ü®¬∑,¬∑‚ü©. Broadly speaking, there\nare two types of dyadic embedding models:\nFactorized Models where the embeddings qanddofùëûandùëë\nrespectively are computed independently, via a function ùëì(¬∑).\nThe training objective is chosen to ensure that\nq,d+\u000b\n>\n‚ü®q,d‚àí‚ü©for any pair of positive and negative documents\nd+and d‚àí. As can be seen, these models remain agnostic\nto the interrelation between the inputs. Examples of such\nmodels include the influential Deep Structured Semantic\nModel (DSSM) of Huang et al . [15] as well as many others\n[28, 30, 31, 36].\nInteraction Models where we compute joint embeddings of\nthe formùëî(ùëû,ùëë)whereùëî:Q√óD‚Üí Ris an embedding func-\ntion. Clearly, such models take the relationship between ùëû\nandùëëinto account when determining vector representations\n[12, 14, 16‚Äì18, 29, 32, 39].\nIn this paper we focus exclusively on factorized models, simply\nbecause they can be deployed at scale; we precompute the embed-\ndings for all elements of D, and given a query ùëûwe simply need to\ncomputeùëì(ùëû):=q, and search for its nearest neighors in the set\n{d1,..., dùëö}in the Euclidean space Rùëô.\nIn Figure 1, we present a high-level schematic of a factorized\nmodel architecture, which takes the form of a Siamese network that\ncomputes embeddings for two inputs using a deep neural network,before calculating a dot product and loss. Although other variants\nare possible (e.g., by replacing the dot product with a different\nsimilarity function), we will work with this prototypical model in\nthis paper. Moreover, we will assume that the loss is computed in\na pointwise manner. Again, one can work with a variety of loss\nfunctions, but, for simplicity, we will only focus on the squared\nhinge loss:\nùêø(ÀÜùë¶,ùë¶):=ùë¶¬∑min(0,ÀÜùë¶‚àíùë°1)2+(1‚àíùë¶)¬∑max(0,ÀÜùë¶‚àíùë°2)2(1)\nwhereùë¶=1if a pair is positive and 0 otherwise and ùë°1andùë°2denote\nthe thresholds for positive and negative examples, respectively.\n3 SCALING UP TRAINING AND INFERENCE\n3.1 Preprocessing and Graph Clustering\nWe use the METIS library [ 24] to cluster the bipartite graph de-\nrived from the dyadic data. As shown in Figure 2, our real-world\nproduct search dataset is highly structured, with the partitioning\nidentifying a clear block-diagonal structure in the co-occurence\nmatrix of queries and items. Moreover, Figure 3 depicts that these\nclusters also contain a semantic coherence that is distinct from\nother partitions by plotting the frequent terms in the queries and\nproduct titles within two sample clusters. In the product search\ndataset used in our experiments, the graph edges represent pur-\nchased products in response to a query, weighted by the number of\npurchases. METIS also enforces a balance between clusters, stipulat-\ning that each cluster has roughly the same number of nodes (either\nqueries or products). The balance property is especially important\nfor our inference algorithm since we would like to build the indexes\nand perform the nearest neighbor search within a given partition\nquickly and therefore avoid degenerate clusters containing a large\nfraction of items. Due to the importance of balance, we favor algo-\nrithms like METIS over other types of clustering approaches such as\nùëò-means clustering over the embeddings directly. However, we note\nthat METIS, as applied to our bipartite graph, enforces a balance\nonly between the union of queries and documents; we may still\nobserve some variance in the number of documents per partition\nas shown in Figure 7.\nFigure 2: Co-occurence matrix of queries and items in a prod-\nuct search dataset. Left: Co-occurence before partitioning\nwhere dark points indicate a purchase. Right: Co-occurence\nmatrix after reordering queries and items by the partition-\ning\n3\n\nFigure 3: Word cloud of the frequent terms in two different\nclusters in the pets category of an e-commerce dataset. Clus-\nter 0 corresponds to dog flea treatments while Cluster 2 cen-\nters on dog and cat food\n3.2 Training\nIn this section, we discuss our proposed negative sampling tech-\nnique in further detail. Let G=(Q‚à™D,P)be a bipartite graph\nderived from our training set where an edge (ùëû,ùëë)‚ààP if and only\nifùëû‚ààQ,ùëë‚ààD and(ùëû,ùëë)has a positive association. Let Cdenote\na partition of the vertices of ùê∫intoùëüclustersùëê1,...,ùëêùëüsuch that\neach vertex in ùê∫belongs to one and only one partition2.\nGiven access to such a partitioning, we propose Algorithm 1 to\nsample negative examples for a minibatch during training.\nAlgorithm 1 Hard Negative Mining via Graph Partitioning\nInput: Partitions{ùëê1,...,ùëêùëü}, window size ùë§, sample size ùë°, and\nqueries{ùëû1,...,ùëûùëõ}\n1:ùëÜ‚Üê{}\n2:forùëûùëñin{ùëû1,...,ùëûùëõ}do\n3: Look up cluster ùëêùëñcontainingùëûùëñ.\n4: Get the top ùë§partitionsùëäby cluster affinity with ùëêùëñ.\n5: Select a high-affinity cluster ùëêùëóuniformly at random from\nùëä, excludingùëêùëñ.\n6: Sample‚åàùë°/ùëõ‚åâdocuments ùëë‚àí\nùëñ1,ùëë‚àí\nùëñ2,...,ùëë‚àí\nùëñùë†uniformly at ran-\ndom fromùëêùëó\n7:ùëÜ‚ÜêùëÜ‚à™{(ùëûùëñ,ùëë‚àí\nùëñ1),...,(ùëûùëñ,ùëë‚àí\nùëñùë†)}\n8:end for\n9:returnùëÜ\nIn Algorithm 1 we can utilize various definitions of cluster affin-\nity. In our work, we rely on the number of edges that cross between\ntwo clusters as a measure of their affinity. Intuitively, these edge\ncuts measure affinity as we expect to see more overlap between\nclusters pertaining to men‚Äôs and women‚Äôs shoes than, say, men‚Äôs\nshoes and dog food. In our experiments, we found that uniformly\nsampling from a fixed number of top clusters as opposed to select-\ning clusters with probability proportional to their affinity provided\n2We chose to uniquely assign each node to a single partition to reduce memory, but,\nin principle, one could replicate entities across clusters. We leave this exploration for\nfuture work.better model performance. We hypothesize that this phenomenon\nis due to the importance of diversity in our samples. In particular,\nuniform sampling allows us to include negative samples from a\nvariety of clusters whereas a probability distribution based on clus-\nter affinity tends to favor only the top clusters. We note that one\nmight also be able to extend this algorithm into a natural curricu-\nlum learning scheme where we progressively tighten the window\nparameterùë§over the course of training. We defer this investigation\nfor future work.\n3.3 Inference\nLetG=(Q‚à™D,P)be a bipartite graph constructed from a set\nof positively associated query-document pairs as defined in the\nprevious section and let Cdenote a partition of Ginto clusters\nùëù1,...,ùëùùëê. We propose to use this partitioned graph for a more\nscalable approximate ùëò-nearest neighbors algorithm as follows: we\nwill train a classifier that, given a query embedding q, predicts the\nclusters with the highest affinity to q. We then perform a nearest\nneighbor search inside these clusters to return our final result us-\ning a backend KNN algorithm Aof our choice. As an additional\noptimization, we introduce a cumulative probability cutoff where\nwe will stop probing for additional clusters if the cumulative prob-\nability of the clusters we have visited thus far, as predicted by our\nclassifier model, exceeds a provided threshold of ùë°.\nAlgorithm 2 Partitioned Nearest Neighbor Search (PNNS)\nInput: Partitions{ùëê1,...,ùëêùëü}, query embedding q, classifier‚Ñé,\nnumber of probes ùëë, number of neighbors ùëò, probability cutoff ùë°,\nand a backend KNN algorithm A\n1:Computeùë†ùëñ=‚Ñé(ùëû,ùëêùëñ)forùëñ‚àà{1,...,ùëë}\n2:Identify the top ùë§clustersùëê‚Ä≤\n1,...ùëê‚Ä≤ùë§where\nùë†‚Ä≤\nùëñ‚â•ùë†ùëñ+1‚àÄùëñ‚àà{1,...ùëë}, and√çùë§\nùëñ=1ùë†‚Ä≤\nùëñ‚â•ùë°\n3:returnA(ùëò,[ùëê‚Ä≤\n1,...,ùëê‚Ä≤ùë§]), theùëònearest neighbors computed\nby the backend algorithm across the top ùë§clusters.\nIn our experiments, we perform the nearest neighbor search\nover our candidate clusters serially. One could also perform the\nsearch over in each cluster in parallel and reduce the search latency\nfurther. We defer this optimization for future work.\n3.4 Cluster Prediction Models\nOur cluster prediction model takes a query embedding vector qas\ninput and outputs a probability distribution over all of partitions,\nrepresenting the likelihood of a given cluster containing relevant\ndocuments to the query. In our experiments, we use a two-layer\nfeed forward neural network followed by a softmax layer with 256\nhidden nodes in each hidden layer and a crossentropy loss. We train\nthe model on a set of query vectors computed by an embedding\nmodel and supervise over the labeled cluster containing the query.\nWe note that our partitioned nearest neighbors algorithm in-\ntroduces two distinct sources of error: 1) the cluster prediction\nmodel could make an incorrect prediction and lead us to search\nin the wrong partitions and 2) the graph partitioning itself might\nfail to group certain relevant documents together. In Figure 4 we\nplot the accuracy of our cluster prediction models in selecting the\ncorrect cluster for our test set of queries across different numbers\n4\n\nof clusters and different numbers of probes. We define the ‚Äúreduc-\ntion factor\" as the ratio of the number of clusters to the number\nof probes to examine the tradeoff between searching in fewer clus-\nters and the prediction accuracy. From these plots, we see that our\nprediction model suffers in performance with a larger reduction\nfactor, which introduces a tradeoff between search latency which\nnaturally decreases when we examine a smaller fraction of clusters.\nFigure 4: Evaluation of the tradeoff between number of par-\ntitions and number of probes in PNNS. The ‚Äúreduction\" fac-\ntor represents the ratio between the number of clusters to\nthe number of probes. We observe that the classifier accu-\nracy increases as we examine more clusters (since, for a fixed\nreduction factor, we add more probes). However, the accu-\nracy eventually plateaus, which suggests that the underly-\ning graph partitioning introduces some degree of noise in\nfailing to group all relevant products together.\n4 RELATED WORK\n4.1ùëò-Nearest Neighbors\nExact and approximate KNN remains a fundamental algorithmic\ntask that has seen an increased interest with the advent of neural\nembedding models. In the 1970s, Bentley introduced the KD-tree, a\ndata structure for dividing the Euclidean space to enable efficient\nexact searching [ 3]. However, KD-trees scale poorly with respect\nto the dimension, and are therefore not suitable for most modern\napplications.\nAs opposed to KD-trees, which divide the Euclidean space by\nusing a data-structure, Locality-sensitive hashing (LSH) is an alter-\nnate technique for approximate KNN, which uses randomization\nto quantize the space [ 1,9,19]. More recently, additional pow-\nerful approximate KNN algorithms have emerged including the\nHierarchical Navigable Small World (HNSW) [ 27], product quan-\ntization [ 21], cell probe methods such as the inverted file index\n[37], Navigating Spread-out Graph (NSG) [ 7], and Neighborhood\nGraph and Tree (NGT) [ 20] along with libraries implementing these\napproaches such as NMSLIB [5], FAISS [22], and Annoy [4].\nThe above techniques work in a data-independent manner. In\ncontrast, there is an exciting line of work recently that focuses onlearned indices [ 26]. Dong et al . [6] proposed a data-dependent\nalgorithm for approximate KNN, which they call Neural-LSH (also\nsee [ 34]). Applied to our context, the algorithm works as follows:\ngiven the document embeddings {d1,..., dùëö}construct a KNN\ngraph, that is, link dùëñwith dùëóifdùëóis aùëò-nearest neighbor of dùëñ.\nGiven the KNN graph, find a balanced partition of this graph by\nminimizing edge-cut. Finally, train a neural network classifier to\nmap document embeddings dto the corresponding graph partition.\nAt inference time, use the classifier to map the query embedding q\nto a partition and perform exact nearest neighbor search within that\npartition. While this approach also leverages learning and graph\npartitioning to improve upon classical techniques, the key difference\nbetween this algorithm and our proposed solution lies in the former\ntechnique having to build a KNN graph, which requires performing\na KNN search for each point in the space. This operation proves\nto be prohibitively expensive on large datasets with hundreds of\nmillions or billions of points. In contrast, our method relies upon a\ngraph that has already been constructed from our dyadic dataset,\nwhich eliminates the need to build the network ourselves, saving\nhours, if not days, of compute time.\n4.2 Partitioning\nWe note in the passing that the problem of graph partitioning is\nNP-Hard [ 2]. However, several approximation algorithms such as\nMETIS [ 24], KaHIP [ 35], SCOTCH [ 33], and PuLP [ 38] have been\ndeveloped. We evaluated these methods on our product search\ndataset and settled on METIS for our experiments since it offered\nthe best tradeoff between quality (recalling the relevant products\nfor a given query) and speed (partitioning our graph in roughly 6\nhours).\n4.3 Negative Sampling\nA number of papers have explored principled approaches for iden-\ntifying informative training examples to decrease time to conver-\ngence for training neural networks. A common theme in this body\nof work centers on importance sampling, constructing a distribu-\ntion over training examples with greater weight given to samples\nmore likely to produce large parameter updates [ 8,11,23]. These\napproaches, however, require maintaining a distribution over all\ntraining examples, which becomes infeasible at larger scales. In\ncontrast, by clustering the data, we can maintain a coarse-grained\ndistribution over the clusters instead of each training data point.\nA graph based approach for negative sampling, but very different\nfrom ours, was proposed by Ying et al . [40] in the context of the\nPinsage algorithm.\n5 EXPERIMENTS\nIn this section, we present experimental results demonstrating the\nscalability properties of our proposed partitioning scheme on a\nlarge-scale product search dataset. In particular, we focus on using\ngraph partitioning for improving the training of embedding models\nthrough hard negative sampling and for efficiently deploying popu-\nlar approximate KNN algorithms. We do not focus on presenting\nend-to-end retrieval results and instead focus on the improvements\nto training and deployment separately as the improvements to\nthese sub-components can be applied independent of each other\n5\n\nand can be extended to other dyadic data applications. In addition,\nwe measure recall in comparison to the baseline of an exact KNN\nsearch, which is a relative measure and thus independent of any\nimprovements to the underlying embedding model.\n5.1 Data & Algorithms\nWe evaluate our proposed negative sampling approach using a\nproduct search dataset sampled from Amazon.com search logs. Our\ntraining set consists of tens of millions of unique search queries\nand products and hundreds of millions of training examples. We\nuse the semantic product search model architecture proposed in\n[30] to learn query and product embeddings.\nTo evaluate our inference algorithm, we construct a dataset of\nproduct embeddings at the billion scale and use METIS to parti-\ntion our data into 64 clusters. We benchmark the performance of\nKNN algorithms on a CPU machine against a set of 1000 query\nembeddings. In our experiments, we measure 1) the algorithm‚Äôs\nability to recall the 100 closest vectors for each query, 2) the av-\nerage latency of a single query search, and 3) the time required\nto construct the approximate KNN index. We investigate scaling\n3 popular algorithms with PNNS: HNSW (HNSWLIB implemen-\ntation3), NGT4, and the Inverted File Index (IVF) method (Faiss\nimplementation5). For all algorithms, we use cosine similarity as\nour metric of choice to match with the similarity measure used to\ntrain these embeddings.\n5.2 Hardware\nWe trained our embedding models on a single AWS p3.16xlarge\nmachine with 8 NVIDIA Tesla V100 GPUs (16GB), Intel Xeon E5-\n2686v4 processors, and 488GB of RAM.\nWe performed the METIS graph clustering as well as all KNN\nbenchmarking experiments on an AWS x1e.32xlarge machine with\n128 vCPUS, 4TB of memory, and quad socket Intel Xeon E7 8880\nprocessors.\n5.3 Negative Sampling Experiments\nIn this section, we present experimental results with our proposed\nnegative sampling algorithm. As mentioned, we conduct all of our\nexperiments with an embedding model tuned for product search.\nWe construct a vocabulary consisting of 125,000 of the most fre-\nquent word unigrams, 25,000 word bigrams, and 50,000 character\ntrigrams along with 500,000 additional tokens reserved for out-of-\nvocabulary terms, which we randomly hash into these bins. The\ninputs to our model are query keywords and product title text,\nwhich we tokenize into 32 and 128-length arrays from our vocabu-\nlary, respectively. We set our embedding dimension to 256, batch\nsize to 8192, use Xavier weight initialization, and train using the\nAdam optimizer [ 25] withùõº=0.001,ùõΩ1=0.9,ùõΩ2=0.999,ùúñ=10‚àí8\nand the aforementioned squared hinge loss function (Equation 1)\nwith thresholds ùë°1=0.9andùë°2=0.2.\nSince we are focused on ad hoc retrieval, we evaluate model\nperformance on a hold-out validation test according to ‚ÄúMatching\"\nMean Average Precision (MAP) and ‚ÄúMatching\" Recall as defined in\n3https://github.com/nmslib/hnswlib\n4https://github.com/yahoojapan/NGT\n5https://github.com/facebookresearch/faiss[30] where we first sample a set of 20,000 queries and evaluate the\nmodel‚Äôs ability to retrieve purchased products from a sub-corpus\nof 1 million products for those queries.\nTables 1 and 2 show the result of our parameter sweep on our\nevaluation set for our proposed negative sampling algorithm where\neach row corresponds to the number of graph clusters used while\neach column represents the number of nearby clusters probed for\nsamples. We observe that we hit diminishing returns with too many\nclusters where we might split relevant items into different partitions\nand, consequently, sample related pairs as negatives. Similarly, we\nnotice that increasing the number of probes improves model perfor-\nmance, which we hypothesize is due to sampling a greater diversity\nof negatives. However, increasing the number of probes comes at\nthe cost of longer training times as we spend more computation\nwithin each training step sampling negatives.\n8 16 32 64 128 256 512 1024\n2048 0.317 0.318 0.319 0.314 0.312 0.304 0.295 0.285\n4096 0.323 0.326 0.327 0.321 0.320 0.312 0.306 0.293\n8192 0.328 0.330 0.339 0.331 0.332 0.321 0.312 0.302\n16384 0.329 0.333 0.338 0.336 0.338 0.332 0.319 0.309\n32768 0.323 0.332 0.338 0.337 0.334 0.339 0.328 0.310\n65536 0.306 0.322 0.332 0.334 0.335 0.341 0.331 0.307\n131072 0.286 0.302 0.327 0.331 0.338 0.337 0.329 0.298\nTable 1: Match MAP across various number of clusters (rows)\nand number of sampling probes (columns)\n8 16 32 64 128 256 512 1024\n2048 0.761 0.775 0.780 0.778 0.784 0.780 0.772 0.761\n4096 0.754 0.767 0.777 0.781 0.784 0.783 0.782 0.770\n8192 0.739 0.757 0.775 0.782 0.790 0.787 0.789 0.778\n16384 0.724 0.747 0.762 0.773 0.786 0.788 0.790 0.782\n32768 0.703 0.723 0.746 0.757 0.772 0.787 0.786 0.778\n65536 0.672 0.697 0.715 0.743 0.763 0.775 0.782 0.771\n131072 0.635 0.661 0.696 0.720 0.743 0.760 0.768 0.754\nTable 2: Match Recall across various number of clusters\n(rows) and number of sampling probes (columns)\nSecondly, we can compare our best performing graph-based\nnegative sampling models to our baseline with random negative\nsampling. Based on Tables 1 and 2, we select the model with the\nbest performing MAP (65,536 clusters and 256 probes), the model\nwith the best Recall (16,384 clusters and 512 probes) and a hybrid\nmodel that achieves strong performance on both metrics (16384\nclusters, 128 probes). In Figures 5 and 6, we compare these models\nto a baseline which sampled negatives uniformly at random while\nkeeping all other parameters fixed. In these plots, we compare the\nrelative training times for each model by measuring metrics across\nhours of training time. Since the baseline involved no computation\nbetween each minibatch aside from uniform random sampling, each\nstep of the baseline was approximately twice as fast as each step\nof the graph-based sampling models. However, these graph-based\n6\n\nsampling models compensated for their added computation per\nstep. They generalize better on the test set and achieve stronger\nperformance on our validation metrics for every fixed unit of time\npast the start of training, allowing us to train a better model in less\ntime than the baseline.\nFigure 5: Relative time plot of Matching MAP\nFigure 6: Relative time plot of Matching Recall\n5.4 KNN Experiments\nWe next turn our attention to benchmarking the performance of\nour Partitioned Nearest Neighbor Search (PNNS) algorithmic frame-\nwork for scaling KNN search on a billion-scale collection of vectors.\nWe conducted grid searches on a smaller collection size of 3 mil-\nlion vectors to identify performant hyperparameter settings that\nachieved over 95% recall for each algorithm. Ultimately, we settled\non the following parameter settings for our experiments:\n‚Ä¢NGT: ESC=30, ESS=70 (ESC=10, ESS=20 with no partition-\ning)\n‚Ä¢HNSW: EFC=700, EF=700, M=110\n‚Ä¢IVF: NLIST=256, NUM PROBES=16\nWe note that we selected weaker hyper-parameter settings for\nNGT without partitioning because we observed that the algorithm\nwould take an intractably long time (at least several months) to\nbuild the index otherwise. However, with PNNS partitioning, we\nwere able to use more aggressive hyperparameters to achieve com-\nparable latency and recall results to the other algorithms used in\nour experiment.5.4.1 Index Build Time. One challenge with deploying approximate\nKNN algorithms at the billion scale is the fact that these approaches\nalmost always involve a time-consuming step of converting input\nvectors into an index structure for searching. Many production\nsearch systems elect to rebuild their indexes at a regular cadence,\nsuch as every 24 hours. Although approximate approaches often\ndramatically reduce the search latency relative to a brute force\nsearch at marginal losses of recall, the index build time can possibly\ntake multiple days, making daily rebuilds of the index infeasible.\nThrough the PNNS graph partitioning approach, we can reduce\nthis build time by building the indexes for each partition in parallel\nacross multiple machines. Such a multi-machine index build is not\ncurrently supported by the libraries we experimented with.\nSince we partition a bipartite graph of queries and products, the\nMETIS algorithm enforces a balance between the sum of query\nand product nodes per cluster. Thus, we may still have a range\nin the number of products per cluster, as shown in Figure 7. As\na result, we find that the overall index build time does not scale\npurely linearly with the number of machines since some of the\npartitions take longer to build than others because they contain\nmore documents. This problem of efficiently building the indexes for\neach partition across some number of machines is an instance of the\nclassic algorithmic task of assigning jobs to machines. For simplicity,\nwe employ the well-known greedy algorithm of first sorting the jobs\nby their respective amounts of work and then iteratively assigning\nthe most intensive remaining job to the machine currently with\nthe lightest load. This approach guarantees an assignment of jobs\nwhere the maximum load across all machines is at most a constant\nfactor of 4/3more than the optimal solution, as first shown in the\nclassical paper of Graham [10].\nFigure 7: Size of the vector embeddings file across parti-\ntions. We observe that although METIS enforces a balance\nbetween the number of vertices across partitions in the bi-\npartite graph, we may still have some imbalance when re-\nstricting our focus just to the documents.\nIn Table 3, we report the index build time both for the standard\nKNN algorithms with no partitioning and with PNNS across a vari-\nous number of machines. To conserve computational resources, we\nsimulate running the PNNS index build over multiple machines by\n7\n\nonly running the jobs assigned to the machine with the maximum\nload, which will determine the overall index build time. From these\nresults, we see that PNNS can reduce the cost of the overall index\nbuild time and, in some cases, make daily index builds feasible when\nsuch a cadence would not be possible without partitioning. We note\nthat the times reported in Table 3 do not include the additional\ncomputation required for the graph partitioning. However, we can\navoid re-running the partitioning step on a daily basis by assigning\nnew documents to clusters via our classifier. Thus, in an amortized\nsense, the cost of graph partitioning becomes negligible compared\nto the cost of building the KNN indexes.\nWe also note that the savings in the build time from our par-\ntitioning scheme do come at the cost of increased computational\nresources as we construct the KNN indexes in parallel. However,\nsince none of the algorithms/libraries we benchmark currently en-\nable multi-machine index building, they cannot scale in the same\nmanner with more compute resources. Thus, our method provides\nan avenue for indexing billion-scale embedding data for popular\nKNN search approaches within the constraints of a product build\ncadence, such as daily updates.\nNGT HNSW IVF\nNo Partitioning >1000 87.2 9.28\nPNNS (2 machines) 77.03 33.37 2.12\nPNNS (4 machines) 38.35 18.15 1.03\nPNNS (8 machines) 23.5 12.13 0.533\nPNNS (16 machines) 21.07 11.42 0.483\nTable 3: Index Build Time (hours)\n5.4.2 Recall and Latency. In this section we focus on benchmarking\nthe recall and latency of PNNS on our billion-scale collection size. As\nmentioned, we define recall as|ùëÜùê∏‚à©ùëÜùê¥|\n|ùëÜùê∏|whereùëÜùê∏is the set of results\nreturned by an exact KNN search and ùëÜùê¥is the items retrieved\nby the approximate algorithm. In our experiments, we focus on\nrecall@100, namely the ability of the approximate algorithm to\nretrieve the top 100 results returned by an exact search. In Table\n4, we report the average recall@100 across the 1000 queries we\nevaluated against. For latency, we measure the average time for\neach algorithm to return results across our 1000 benchmark queries.\nIn all of our PNNS experiments, we evaluate the algorithm‚Äôs\nrecall and latency across varying number of probes. In addition,\nwe fix the cumulative cluster probability hyperparameter to 0.99,\nmeaning that we terminate our search early if the cumulative prob-\nability of the clusters we have searched within, as predicted by our\ncluster prediction classifier, exceeds this threshold.\nFrom Tables 4 and 5, we see that PNNS, for a larger fraction\nof probes, can achieve slightly reduced recall numbers compared\nto the standard HNSW algorithm without partitioning, but at the\ncost of increased latency. In practice, this tradeoff might still be\nfavorable given the potential for a considerably decreased index\nbuild time as shown in Table 3. In addition, we find that PNNS can\nenable us to use more performant hyperparameter settings for NGT\nsince the index build time becomes tractable. As a result, PNNS can\nNumber of ProbesNGT HNSW IVF\n1 0.737 0.744 0.735\n2 0.838 0.846 0.833\n4 0.898 0.907 0.892\n8 0.934 0.943 0.928\n16 0.957 0.967 0.950\nNo Partitioning 0.756 0.980 0.983\nTable 4: PNNS Recall@100Number of ProbesNGT HNSW IVF\n1 38.03 71.61 199.82\n2 53.90 113.09 277.93\n4 89.85 183.89 526.60\n8 151.09 300.39 812.89\n16 289.12 453.69 1265.90\nNo Partitioning 90.0 36.54 30313.07\nTable 5: PNNS Latency (ms)\nprovide a path for deploying NGT at the billion scale in practice\nand provides feasible latency and recall results across a variety of\nprobes. The IVF algorithm has a significantly smaller index build\ntime than the other methods we benchmarked, but comes at the\ncost of a much larger latency (though still orders-of-magnitude\nfaster than an exact search). For IVF, we found that PNNS produced\nrelatively marginal savings in index build time, but was able to\nreduce the search latency by an order of magnitude with a small\nreduction in recall.\nIn summary, we found that PNNS can serve as a general algorith-\nmic framework to run the popular HNSW and NGT approximate\nalgorithms at the billion scale in practical settings by making daily\nindex builds feasible. In the case of IVF, PNNS also reduced the\nsearch latency considerably. When assessing the tradeoffs between\nPNNS and the baselines techniques, we note that building the search\nindex within 24 hours to facilitate daily updates may be an essential\nrequirement for real production systems, in which case the ability\nof PNNS to parallelize the index build process may be essential at\nthis scale despite the loss of some recall and a potential increase in\nlatency. Furthermore, we note that each of the billion-scale KNN\nsearch indexes with no partitioning that we experimented with\nyielded a memory footprint of over 1 terabyte. While we were able\nto accommodate these large indexes in our experiments thanks to\nour use of an AWS x1 instance, we note that many practitioners\nmight be looking for ways to deploy cheaper instances in produc-\ntion settings. The efficient partitioning strategy behind PNNS also\nprovides a path for enabling distributed nearest neighbor search\nwhere we can store the indexes for each partition, which will have\nsmaller memory footprints than the full index, over multiple ma-\nchines and thereby sidestep this memory bottleneck. In this sense,\nPNNS, while possibly producing some regression in latency or re-\ncall when compared to its unpartitioned counterpart algorithm,\n8\n\nmay be an essential step in deploying nearest neighbor search at\nthe billion scale within the practical constraints of real systems.\n5.4.3 Deployment. We successfully deployed PNNS-based search\nfor several weeks in an online A/B test on a large e-commerce\nwebsite to augment the traditional inverted index-based keyword\nmatches with products retrieved by our neural embedding model.\nWe validated that PNNS was able to meet the constraints of the\nsearch system while improving several business metrics. These\nresults also validate that learned index structures can be integrated\ninto established production systems and meet strict engineering\nrequirements where data-independent algorithms fall short.\n6 CONCLUSION AND FUTURE WORK\nIn this paper, we address the problem of performing training and\ninference for dyadic embedding models at the billion scale, focus-\ning on the practical application of semantic product search. To our\nknowledge, our work is the first to present solutions for deploying\nembedding-based retrieval at this scale under the constraints of real-\nistic industrial systems. We demonstrated that the same underlying\nprinciple of leveraging the structure of real-world data can tackle\nboth of these problems. By modeling dyadic data as a bipartite graph\nand utilizing balanced graph partitioning algorithms, we showed\nboth improved model performance and reduced convergence time\nduring training through efficient hard negative sampling. In addi-\ntion, we presented a technique, based on graph clustering and a\nlearned classifier, for scaling popular KNN algorithms in terms of\neither search time (in the case of IVF) or in sharply reducing the\nindex build time (in the case of NGT and HNSW) with minimal\nimpact on recall. Unlike similar graph partitioning approaches for\nKNN search in the literature, our technique leverages a graph al-\nready constructed from the underlying dyadic data and thereby\neliminates the computationally prohibitive step of constructing a\nKNN graph at the billion scale. For future work on the negative\nsampling side, we can investigate curriculum learning strategies\nfor our graph-based negative sampling approach where we tighten\nthe window of adjacent cluster to sample from over the course of\ntraining. On the inference side, as mentioned in [ 6], we can con-\nsider investigating methods for learning the graph partitioning and\nthe cluster prediction model in an end-to-end fashion where one\noptimization problem informs the other. Finally, we note that our\nproposed techniques are general in nature and can be applied to\nnumerous problem domains that fit the dyadic data paradigm, and\nwe hope to extend our ideas to other applications beyond product\nsearch.\nREFERENCES\n[1]Alexandr Andoni and Piotr Indyk. 2006. Near-optimal hashing algorithms for\napproximate nearest neighbor in high dimensions. In 2006 47th annual IEEE\nsymposium on foundations of computer science (FOCS‚Äô06) . IEEE, 459‚Äì468.\n[2]Konstantin Andreev and Harald Racke. 2006. Balanced graph partitioning. Theory\nof Computing Systems 39, 6 (2006), 929‚Äì939.\n[3]Jon Louis Bentley. 1975. Multidimensional binary search trees used for associative\nsearching. Commun. ACM 18, 9 (1975), 509‚Äì517.\n[4]E Bernhardsson. 2017. ANNOY: Approximate nearest neighbors in C++/Python\noptimized for memory usage and loading/saving to disk. GitHub https://github.\ncom/spotify/annoy (2017).\n[5]Leonid Boytsov and Bilegsaikhan Naidan. 2013. Engineering Efficient and Ef-\nfective Non-metric Space Library. In Similarity Search and Applications - 6th\nInternational Conference, SISAP 2013, A Coru√±a, Spain, October 2-4, 2013, Proceed-\nings (Lecture Notes in Computer Science, Vol. 8199) , Nieves R. Brisaboa, OscarPedreira, and Pavel Zezula (Eds.). Springer, 280‚Äì293. https://doi.org/10.1007/\n978-3-642-41062-8_28\n[6]Yihe Dong, Piotr Indyk, Ilya Razenshteyn, and Tal Wagner. 2020. Learning Space\nPartitions for Nearest Neighbor Search. (2020).\n[7]Cong Fu, Chao Xiang, Changxu Wang, and Deng Cai. 2019. Fast Approximate\nNearest Neighbor Search With The Navigating Spreading-out Graphs. PVLDB\n12, 5 (2019), 461 ‚Äì 474. https://doi.org/10.14778/3303753.3303754\n[8]Jinyang Gao, HV Jagadish, and Beng Chin Ooi. 2015. Active sampler: Light-weight\naccelerator for complex data analytics at scale. arXiv preprint arXiv:1512.03880\n(2015).\n[9]Aristides Gionis, Piotr Indyk, Rajeev Motwani, et al .1999. Similarity search in\nhigh dimensions via hashing. In Vldb, Vol. 99. 518‚Äì529.\n[10] R. L. Graham. 1969. Bounds on Multiprocessing Timing Anomalies. SIAM\nJOURNAL ON APPLIED MATHEMATICS 17, 2 (1969), 416‚Äì429.\n[11] Guibing Guo, Songlin Zhai, Fajie Yuan, Yuan Liu, and Xingwei Wang. 2018.\nVse-ens: Visual-semantic embeddings with efficient negative sampling. In Thirty-\nSecond AAAI Conference on Artificial Intelligence .\n[12] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. 2016. A deep relevance\nmatching model for ad-hoc retrieval. In Proceedings of the 25th ACM International\non Conference on Information and Knowledge Management . ACM, 55‚Äì64.\n[13] Thomas Hofmann, Jan Puzicha, and Michael I Jordan. 1999. Learning from dyadic\ndata. In Advances in neural information processing systems . 466‚Äì472.\n[14] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional\nNeural Network Architectures for Matching Natural Language Sentences. In\nProceedings of the 27th International Conference on Neural Information Processing\nSystems - Volume 2 (Montreal, Canada) (NIPS‚Äô14) . MIT Press, Cambridge, MA,\nUSA, 2042‚Äì2050. http://dl.acm.org/citation.cfm?id=2969033.2969055\n[15] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry\nHeck. 2013. Learning deep structured semantic models for web search using\nclickthrough data. In Proceedings of the 22nd ACM international conference on\nConference on information & knowledge management . ACM, 2333‚Äì2338.\n[16] Kai Hui, Andrew Yates, Klaus Berberich, and Gerard de Melo. 2017. Pacrr:\nA position-aware neural ir model for relevance matching. arXiv preprint\narXiv:1704.03940 (2017).\n[17] Kai Hui, Andrew Yates, Klaus Berberich, and Gerard de Melo. 2017. Re-pacrr: A\ncontext and density-aware neural information retrieval model. arXiv preprint\narXiv:1706.10192 (2017).\n[18] Kai Hui, Andrew Yates, Klaus Berberich, and Gerard de Melo. 2018. Co-pacrr: A\ncontext-aware neural ir model for ad-hoc retrieval. In Proceedings of the Eleventh\nACM International Conference on Web Search and Data Mining . ACM, 279‚Äì287.\n[19] Piotr Indyk and Rajeev Motwani. 1998. Approximate nearest neighbors: towards\nremoving the curse of dimensionality. In Proceedings of the thirtieth annual ACM\nsymposium on Theory of computing . ACM, 604‚Äì613.\n[20] Masajiro Iwasaki. 2015. Ngt: Neighborhood graph and tree for indexing.\n[21] Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization\nfor nearest neighbor search. IEEE transactions on pattern analysis and machine\nintelligence 33, 1 (2010), 117‚Äì128.\n[22] Jeff Johnson, Matthijs Douze, and Herv√© J√©gou. 2017. Billion-scale similarity\nsearch with GPUs. arXiv preprint arXiv:1702.08734 (2017).\n[23] Tyler B Johnson and Carlos Guestrin. 2018. Training deep models faster with\nrobust, approximate importance sampling. In Advances in Neural Information\nProcessing Systems . 7265‚Äì7275.\n[24] George Karypis and Vipin Kumar. 1998. A fast and high quality multilevel scheme\nfor partitioning irregular graphs. SIAM Journal on scientific Computing 20, 1\n(1998), 359‚Äì392.\n[25] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\nmization. arXiv preprint arXiv:1412.6980 (2014).\n[26] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In Proceedings of the 2018 International\nConference on Management of Data (Houston, TX, USA) (SIGMOD ‚Äô18) . ACM,\nNew York, NY, USA, 489‚Äì504.\n[27] Yury A Malkov and Dmitry A Yashunin. 2018. Efficient and robust approximate\nnearest neighbor search using hierarchical navigable small world graphs. IEEE\ntransactions on pattern analysis and machine intelligence (2018).\n[28] Bhaskar Mitra and Nick Craswell. 2017. Neural models for information retrieval.\narXiv preprint arXiv:1705.01509 (2017).\n[29] Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to match using\nlocal and distributed representations of text for web search. In Proceedings of the\n26th International Conference on World Wide Web . International World Wide Web\nConferences Steering Committee, 1291‚Äì1299.\n[30] Priyanka Nigam, Yiwei Song, Vijai Mohan, Vihan Lakshman, Weitian Allen\nDing, Ankit Shingavi, Choon Hui Teo, Hao Gu, and Bing Yin. 2019. Semantic\nProduct Search. In Proceedings of the 25th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining . ACM, 2876‚Äì2885.\n[31] Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen,\nXinying Song, and Rabab Ward. 2016. Deep sentence embedding using long\nshort-term memory networks: Analysis and application to information retrieval.\nIEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP) 24, 4\n9\n\n(2016), 694‚Äì707.\n[32] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengxian Wan, and Xueqi Cheng.\n2016. Text Matching as Image Recognition.. In AAAI . 2793‚Äì2799.\n[33] Fran√ßois Pellegrini and Jean Roman. 1996. Scotch: A software package for static\nmapping by dual recursive bipartitioning of process and architecture graphs.\nInInternational Conference on High-Performance Computing and Networking .\nSpringer, 493‚Äì498.\n[34] Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Herv√© J√©gou. 2018.\nSpreading vectors for similarity search. arXiv preprint arXiv:1806.03198 (2018).\n[35] Peter Sanders and Christian Schulz. 2013. Think Locally, Act Globally: Highly\nBalanced Graph Partitioning. In Proceedings of the 12th International Symposium\non Experimental Algorithms (SEA‚Äô13) (LNCS, Vol. 7933) . Springer, 164‚Äì175.\n[36] Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gr√©goire Mesnil. 2014.\nA latent semantic model with convolutional-pooling structure for informationretrieval. In Proceedings of the 23rd ACM International Conference on Conference\non Information and Knowledge Management . ACM, 101‚Äì110.\n[37] Josef Sivic and Andrew Zisserman. 2003. Video Google: A text retrieval approach\nto object matching in videos. In null. IEEE, 1470.\n[38] George M Slota, Kamesh Madduri, and Sivasankaran Rajamanickam. 2014. PuLP:\nScalable multi-objective multi-constraint partitioning for small-world networks.\nIn2014 IEEE International Conference on Big Data (Big Data) . IEEE, 481‚Äì490.\n[39] Shengxian Wan, Yanyan Lan, Jun Xu, Jiafeng Guo, Liang Pang, and Xueqi Cheng.\n2016. Match-srnn: Modeling the recursive matching structure with spatial rnn.\narXiv preprint arXiv:1604.04378 (2016).\n[40] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton,\nand Jure Leskovec. 2018. Graph convolutional neural networks for web-scale\nrecommender systems. In Proceedings of the 24th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining . ACM, 974‚Äì983.\n10",
  "textLength": 52616
}