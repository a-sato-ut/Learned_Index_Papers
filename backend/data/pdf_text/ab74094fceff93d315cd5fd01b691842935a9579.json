{
  "paperId": "ab74094fceff93d315cd5fd01b691842935a9579",
  "title": "The role of classifiers and data complexity in learned Bloom filters: insights and recommendations",
  "pdfPath": "ab74094fceff93d315cd5fd01b691842935a9579.pdf",
  "text": "Open Access\n© The Author(s) 2024. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate -\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://  \ncreat iveco mmons. org/ licen ses/ by/4. 0/.RESEARCHMalchiodi  et al. Journal of Big Data           (2024) 11:45  \nhttps://doi.org/10.1186/s40537-024-00906-9Journal of Big Data\nThe role of classifiers and data \ncomplexity in learned Bloom filters: insights \nand recommendations\nDario Malchiodi1,2*†  , Davide Raimondi1†, Giacomo Fumagalli1†, Raffaele Giancarlo3† and Marco Frasca1† \nAbstract \nBloom filters, since their introduction over 50 years ago, have become a pillar to han-\ndle membership queries in small space, with relevant application in Big Data Min-\ning and Stream Processing. Further improvements have been recently proposed \nwith the use of Machine Learning techniques: learned Bloom filters. Those latter make \nconsiderably more complicated the proper parameter setting of this multi-criteria \ndata structure, in particular in regard to the choice of one of its key components (the \nclassifier) and accounting for the classification complexity of the input dataset. Given \nthis State of the Art, our contributions are as follows. (1) A novel methodology, sup -\nported by software, for designing, analyzing and implementing learned Bloom filters \nthat account for their own multi-criteria nature, in particular concerning classifier type \nchoice and data classification complexity. Extensive experiments show the validity \nof the proposed methodology and, being our software public, we offer a valid tool \nto the practitioners interested in using learned Bloom filters. (2) Further contributions \nto the advancement of the State of the Art that are of great practical relevance are \nthe following: (a) the classifier inference time should not be taken as a proxy for the fil-\nter reject time; (b) of the many classifiers we have considered, only two offer good \nperformance; this result is in agreement with and further strengthens early findings \nin the literature; (c) Sandwiched Bloom filter, which is already known as being one \nof the references of this area, is further shown here to have the remarkable property \nof robustness to data complexity and classifier performance variability.\nKeywords: Bloom filters, Learned Bloom filters, Approximate set membership, Dataset \ncomplexity\nIntroduction\nIn the past 25 years, Machine Learning (ML) radically impacted on the computing land -\nscape, unveiling a very significant part of the applications which brought Artificial Intel -\nligence in everyday life. Recent works show that ML can be used to design and analyze \nfrom a different perspective also some of the core components of “classical” computing \nfield, such as algorithms and data structures. In particular, Kraska et al. [1] initiated a \nnew research area focusing on learned data structures, currently under active develop -\nment and with documented impacts on several domains, such as databases, network †Dario Malchiodi, Davide \nRaimondi, Giacomo Fumagalli, \nRaffaele Giancarlo, and Marco \nFrasca have contributed equally \nto this work\n*Correspondence:   \ndario.malchiodi@unimi.it\n1 Department of Computer \nScience, Università degli \nStudi di Milano, Via Celoria 18, \n20133 Milan, Italy\n2 CINI National Laboratory \nof Artificial Intelligence \nand Intelligent Systems (AIIS), \nUniversity of Rome, 00185 Rome, \nItaly\n3 Department of Mathematics \nand CS, University of Palermo, Via \nArchirafi 34, 90123 Palermo, Italy\n\nPage 2 of 26 Malchiodi et al. Journal of Big Data           (2024) 11:45 \nmanagement [2] and Computational Biology [3]. An analogous trend concerning algo -\nrithms has been proposed by Mitzenmacher and Vassilvitskii [4]. Concerning data struc -\ntures, the common theme to this new approach is that of training a classifier [5] or a \nregression model [6] on the input data. Then such a learned model is used as an oracle \nthat a given “classical” data structure can use in order to answer queries with improved \nperformance (usually w.r.t. time). To date, learned indexes have been the most studied, \ne.g., [1, 7–17], although rank/select data structures have also received some attention \n[18]. In this work, we focus on Bloom filters (BFs) [19], which represent one of the cor -\nnerstones of Big Data processing and stream analysis [20]. In short, they are special data \nstructures expressly designed for storing a set which is large w.r.t. the available RAM \nresources. In a nutshell, rather than saving all the elements of the set, these structures \nwisely exploit hashing techniques so that it is possible to answer membership queries by \nusing a relatively small amount of main memory. This property, which however is bal -\nanced with a controllable amount of false positives when the data structure is queried, \nmakes Bloom filters heavily used in contexts characterized by the processing of a mas -\nsive amount of data, such as for instance in detecting redundancy within a stream [21], \nin sequencing DNA data [22, 23], or when it is necessary to limit the number of unnec -\nessary disk accesses in cloud-based databases [24]. As well as other data structures, also \nBloom filters received attention in the realm of learned data structures, within which \nlearned Bloom filters have been proposed to improve efficiency [25]. This is definitely \nnatural, accounting for their pervasive use. Indeed, several variants have been proposed \nalso for the classical version of Bloom filters, long before the appearance of their learned \ncounterparts.\nBloom filters (BF) solve the Approximate Set Membership problem, defined as follows: \nhaving fixed a universe U and a set of keys  S⊂U , for any given x∈U , find out whether \nor not x∈S . False negatives, that is negative answers when x∈S , are not allowed. \nOn the other hand, false positives (i.e., elements in U\\S wrongly decreed as keys) are \nallowed, albeit their fraction (termed henceforth false positive rate, FPR for short) should \nbe bounded by a given ǫ . The metrics that can be considered for evaluating any data \nstructure solving the approximate set membership problems are: (1) the FPR, (2) the \namount of RAM to be used to store the data structure, and (3) the reject time, that is the \nexpected time required to reject any element not belonging to S. It is to be noted that the \nBloom filter as described above assumes that U is one-dimensional, and we keep such \nan assumption throughout this paper. Bloom filters can be naturally extended to the \nmulti-dimensional case with the use of suitable hash functions; however, the problem \nof how to design a multi-dimensional Bloom filter has not been considered and prop -\nerly addressed in the scholarly literature, even in the learned setting which is considered \nhere, e.g. [26, 27].\nThe first proposal for the learned version of a Bloom filter was naturally called learned \nBloom filter (LBF) [1]; it is based on a binary classifier induced from data, with the aim \nof predicting set membership while requiring less space than a classical BF, being equal \nthe FPR of the two structures (or vice versa, fix space budget and reduce the FPR). Such \nclassifier is coupled with a standard BF storing the false negatives suffered from the for -\nmer. When queried, the learned filter computes the prediction via the classifier and pos -\nsibly uses the BF as a fallback ensuring that no false negatives on the training data are \n\nPage 3 of 26\n Malchiodi et al. Journal of Big Data           (2024) 11:45 \n \never produced. Mitzenmacher [28] has provided a model for those filters, together with \na very informative mathematical analysis of their pros/cons, resulting in new models \nfor LBFs, and additional models have been introduced recently [29, 30]. It is important \nto underline that all the mentioned learned Bloom filters are static, i.e., no updates are \nallowed. This is the realm we are studying here, aiming at a suitable joint optimization of \nthe aforementioned key resources. As for the dynamic case, some progress is reported in \n[31] in the Data Stream model of computation.\nIt is worth pointing out that, although they differ in architecture, each of these propos -\nals has a binary classifier at its core. Somehow, not much attention has been devoted to \nthe choice of the classifier to be used in practical settings, despite its centrality in this \nnew family of filters and its role in the related theoretical analysis [28]. Kraska et al. use \na Recurrent Neural Network, while Dai and Shrivastava [29] and Vaidya et al. [30] use \nRandom Forests. These choices are only informally motivated, giving no evidence of \nsuperiority with respect to other possible ones, e.g., via a comparative analysis. There -\nfore, apart from preliminary investigations presented in [32, 33], the important problem \nof suitably choosing the classifier to be used to build a specific LBF has not been fully \naddressed so far. In addition to that, although the entire area of learned data structures \nand Algorithms finds its methodological motivation as a conceptual tool to reconsider \nclassic approaches in a data-driven way, the role that the complexity of a dataset plays in \nguiding the practical choice of a learned data structure for that dataset has been consid -\nered to some extent for learned indexes only [15]. This aspect is even more relevant for \nLBFs. Indeed, as well discussed in [28], while the performance of classic BFs is “agnos -\ntic” w.r.t. the statistical properties of the input data, LBFs are quite dependent on them. \nIn addition, it is well-known that the performance of a learnt classifier (a central com -\nponent in this context) is very sensitive to the “classification complexity” of a dataset \n[34–37]. Nonetheless, the literature on LBFs usually describes experiments based on the \nprocessing of limited sets of benchmark data, e.g., in the realm of malicious URL detec -\ntion [38] or geospatial information retrieval [27], without analyzing the sensitivity of the \nproposed approaches to the properties of the processed data. Such a state of the art is \nproblematic, both methodologically and practically, for LBFs to be a reliable competitor \nof their classic counterparts.\nOur aim is to provide a methodology and the associated software to support the \ndesign, analysis and deployment of state-of-the-art learned Boom filters with respect to \nspecific constraints regarding their multi-criteria nature. Namely, space efficiency, false \npositive rate, and reject time. The present study is organized as follows. In “ Classical and \nlearned Bloom filters” section we briefly describe the original and learned variants of \nBloom filters, underlining the specific roles of the hyperparameters to be tuned during \nthe inductive phase. We illustrate in “Experimental methodology ” section the method -\nology which we propose to LBF designers and users in need of studying the relations \namong the metrics used to evaluate how a (either classic or learned) filter performs on \na fixed dataset, given the classification complexity of the latter, and the classifier the \nlearned filter is based upon. “Experiments” section has the twofold aim of introducing \nthe software platform within which we implemented the above-mentioned methodology \nand of describing the experiments which we carried out. In particular, having fixed the \ntotal space budget and the complexity of a dataset, we focus on the problem of choosing \n\nPage 4 of 26 Malchiodi et al. Journal of Big Data           (2024) 11:45 \nthe most suitable classifier backing a LBF, also considering reject time as a selection cri -\nterion. In [28], a related approach uses a fixed filter, thus providing partial answers to our \nresearch question and, moreover, somehow suggesting an experimental methodology \nwhich has not been validated. The results of our experiments are discussed in “Results \nand discussion ” section. In particular, we show that only two specific classifiers are worth \nto be considered when building a LBF, namely, those based on Support Vector Machines \nand feed-forward Neural Networks. Remarkably, they have been almost neglected in the \nexisting literature, with the exceptions of [32, 33, 39].\nTwo further contributions regard: (1) an analysis of the relation among the classi -\nfier inference time and the learned BF reject time, showing that the first should not be \ntaken as a proxy for second; (2) the study of how the complexity of a dataset impacts \non the performance of several state-of-the-art learned Bloom filters, identifying a vari -\nant exhibiting higher robustness against variations in the dataset complexity and in the \nclassifier performance. As a byproduct, “ Guidelines” section provides a set of guidelines \nand recommendations for the use of state-of-the-art learned Bloom filter variants. Some \nconcluding remarks end the paper.\nClassical and learned Bloom filters\nAs mentioned in the “Introduction ” section, a Bloom filter is a data structure designed in \norder to solve the Approximate Set Membership problem for a set S [19]. It is composed \nof an array v of m boolean entries and of k hash functions, h1,...,hk , mapping keys and \nnon-keys onto positions within v . Such functions are assumed to be k-wise independent \n[40, 41], although in practice less strict requirements are effective [19]. After the entries \nof v have been initialized to 0, all the keys x∈S are considered, setting vhj(x)←1 , for \neach j∈{1,...,k} . At this point, the filter can be used to predict membership in the \nset for a generic value x by evaluating vhj(x) , for each j: if at least once the result is 0, the \nvalue is rejected (that is, it is predicted to be a non-key). Otherwise, it is classified as a \nkey (an element of S). By construction, such predictions never incur in false negatives, \nalthough hash collisions might cause false positives. However, the rate ǫ of the latter is \ninversely bound by m as stated by equation (21) in [19], that formally relates reject time, \nspace requirements, and FPR. This fact is typically used to choose the filter configura -\ntion. For instance, FPR can be minimized by suitably selecting the reject time once the \nfilter space has been fixed. Similar relations can be exploited in order to fix m and k so \nas to build a BF exhibiting the most succinct configuration [25, 28]. Analogously, having \nfixed the FPR to ǫ and considering n=|S| keys, the BF with optimal reject time can be \nobtained via an array of size\nbits, as shown in [42].\nThe learned variants of Bloom filters [1] can be seen as systems exhibiting the same \nproperties of the latter, meanwhile reducing FPR or time/space resource demand. \nThe common point to these variants is that all rely on a classifier induced from data. \nTheir simplest implementation, which we will refer to as a LBF, is formally described \nas follows. Starting from a set of labeled instances (x ,yx) , where yx=1 if x∈S and 0 (1) m=n\nln 2log2(1/ǫ)\n\nPage 5 of 26\n Malchiodi et al. Journal of Big Data           (2024) 11:45 \n \notherwise, a classifier C:U→[ 0, 1] is trained through supervised ML techniques to \nclassify keys in S. Namely, the higher the classification score C(x) , the more likely x∈S . \nA crisp prediction is obtained through the application of a threshold τ∈[0, 1] , so that \nx∈U is, at this stage, considered as a key if and only if C(x)>τ  . This setting, however, \nmight give rise to a non-empty set FN={x∈S|C(x)≤τ}} of false negatives, which is \nnot accepted within the Approximate Set Membership problem. Therefore, the classifier \nis coupled with a (classical) Bloom filter F, called the backup filter, used to hold the ele -\nments in FN . All queries for which the classifier produces a negative output are searched \nin F, swapping the prediction whenever F answers x is a key. In sum, any x∈U is pre -\ndicted to be a key if the classification score C(x) is greater than τ , or if C(x)≤τ and F \ndoes not reject x . In all other cases x is rejected.\nAn important difference between classical and learned Bloom filters is that the former \nhave a FPR essentially independent of keys, whereas in a LBF the same quantity depends \non the distribution of the instances used to query the filter itself. To this end, in the fol -\nlowing we explicitly refer to the empirical FPR\nof a learned filter, where ǫτ= |{x∈S|C(x)>τ }|/|S| is the analogous empirical FPR of \nthe classifier C on a query set S⊂U\\S , and ǫF is the FPR of the backup filter. Given \nthe previous definition of empirical FPR (2), the overall LBF has FPR equal to ǫ when \nthe backup filter F is built ensuring ǫF=(ǫ−ǫτ)/(1−ǫτ) , under the constraint ǫτ<ǫ . \nFinally, we point out two important considerations. Firstly, choosing τ influences the val -\nues of the metrics used to evaluate a learned Bloom filter. Moreover, the dependency \non data makes estimating the FPR in a learned setting no longer immediate as it is for \nclassical filters, as highlighted in [28]. Similar considerations hold for the experimental \nmethodology to be used to assess this FPR, which is one of the novelties proposed in this \npaper.\nAlongside LBFs described so far, we consider the following variants. \n1. Sandwiched LBFs (SLBFs) [28] are based on the insight that filtering out non-\nkeys before querying the classifier can lead to an optimized space efficiency, and \nas a consequence to a smaller backup filter. More precisely, the provided keys are \ninitially used to build an initial  (still classical) BF I, and all keys in S not rejected \nby this filter are used to build a LBF. The overall system is queried as follows: if I \nrejects an element x∈U , the latter is predicted as a non-key, otherwise the predic -\ntion provided for this element by the LBF is returned. The empirical FPR of a SLBF \nis obtained as ǫ=ǫI/parenleftbig\nǫτ+(1−ǫτ)ǫF/parenrightbig\n , being ǫI the FPR of the initial filter. Also in \nthis case, the target FPR ǫ of the SLBF automatically identifies that of I. Precisely, \nǫI=(ǫ/ǫ τ)(1−|FN|/n) , where |FN | is the number of false negatives of C, under the \nconstraint ǫ(1−|FN|/n)≤ǫτ≤1−|FN|/n . Also in this case, FPR, space require -\nment and reject time are affected by the classifier accuracy.\n2. Adaptive LBFs (ADA-BF) [29] represent another variation of a LBF in which the \ntraining instances x are partitioned in g groups in function of their classification \nscore C(x) . Each group is assigned a different set of hash functions (although glob -\nally using the same number of functions a LBF would use), and when membership (2) ǫ=ǫτ+(1−ǫτ)ǫF\n\nPage 6 of 26 Malchiodi et al. Journal of Big Data           (2024) 11:45 \nis to be predicted for an instance x , this is done only using the hash functions of the \ncorresponding score group. As for LBFs and SLBFs, the expected FPR is estimated \nempirically, though with a rather complex formula (the interested reader can refer \nto [29] for the mathematical details). As there are several variants of ADA-BF, in our \nexperiments we consider the best performing one.\nWith the exception of another variant described in [30], for which the software is nei -\nther public nor available from the authors, our selection of learned BFs is state-of-the-art.\nThe induction of these learned filters requires to fine-tune some hyperparameters: \nLBF and SLBF have to fix the threshold τ , while ADA-BF needs the number g  of \ngroups to be set, as well as the value ¯c representing the proportion of non-keys scores \nfalling in two consecutive groups. “Selecting optimal classifiers and learned Bloom \nfilters” section details how such hyperparameters have been tuned.\nExperimental methodology\nThis section outlines the methodology which we use to design and analyse learned \nBloom filters, also accounting for the inherent complexity of the dataset to be classi -\nfied. In short, we generalize the approach described in [33] as follows. Starting from \na dataset (either real-world or synthetically generated in function of suitable clas -\nsification complexity metrics), we adopt the following pipeline: collect or generate \ndata; use them to induce a classifier, estimate the corresponding FPR; build a learned \nBloom filter based on this classifier; estimate the overall empirical FPR. Here below \nwe detail the classifier families that we consider, as well as the above-mentioned pro -\ncedure used to generate synthetic data.\nThe considered classifiers\nWe performed a preliminary experimentation phase involving an initial list of ML mod -\nels—compiled without exhaustiveness pretensions—with the aim of identifying which \namong them were worth of further consideration, on the basis of the performance/space \nrequirements trade-off (experiments and data about this initial step of our study are \navailable upon request). Confirming the results of another study [32], Logistic Regres -\nsion [43], Naive Bayes [44] and Recurrent Neural Networks [45] were removed from the \nlist of classifier families to be considered, due to their poor trade-off performance. Here \nbelow we succinctly illustrate the retained models, also detailing their inference process \nin a context characterized by unbalanced labels, as our evaluation both considers bal -\nanced and unbalanced classification problems. We distinguish between regular  and key  \nhyperparameters of the corresponding learning algorithms, where the second category \ncontains hyperparameters whose value affects the induced classifier space occupancy \n(which, when not differently specified, also acts as a proxy for classifier complexity). In \nlight of this distinction, we only consider regular hyperparameters in the model selection \nphase. On the other hand, dedicated experiments are carried out w.r.t. different configu -\nrations for the key hyperparameters, with the aim of analysing the interplay among FPR, \nspace requirements and reject time in learned Bloom filters.\n\nPage 7 of 26\n Malchiodi et al. Journal of Big Data           (2024) 11:45 \n \nIn order to fix notation, we assume U=Rq as universe, and we refer to D⊂U as a set \nof d labeled instances, denoting by x∈D a training instance and by yx∈{0, 1} its label \n(with a slightly different notation for SVMs, as explained here below).\nLinear SVM. Classification in Linear Support Vector Machines (SVMs) for a \ngiven instance x is usually based on the optimal hyperplane ( w,b ) and the sign of \nf(x)=w·x+b . To fall in the setting defined for a LBF, we need to transform the SVM \nprediction into a score in [0, 1]. To this end, we use f(x) as argument to a sigmoid func -\ntion. The optimal hyperplane ( w,b ) is learned via maximization of the margin between \nthe two classes, by solving\nwhere misclassification is allowed by the slack variables ξx and penalized using the \nhyperparameter c>0 (note that in this case yx∈ {−1, 1}  ). Nonlinear SVMs might have \nbeen used here, but the need of storing the kernel matrix (e.g., containing the values of \na Gaussian kernel) alongside the hyperplane parameters results in an unacceptable size \nfor the learned filters. For the linear case we have chosen, we have only one non-key \nhyperparameter, namely c. When dealing with unbalanced labels, we consider the cost-\nsensitive SVM version described in [46].\nFeed-forward NNs We also consider Feed-Forward neural networks [47, 48] accepting \ninstances as inputs and equipped with l hidden layers, respectively having h1,...,hl hid-\nden units (NN- h1,...,hl for short). One output unit completes the network topology. As \nusual, training involves the estimation of unit connections and biases from data. In this \ncase, the hi s are key hyperparameters, while the learning rate lr  acts as a regular hyper -\nparameter to be tuned. It is worth noting that we fix the activation functions for all net -\nwork units (although the former are tunable in principle), to limit the size of the already \nmassive set of experiments. Precisely, as typically done, we use ReLU and sigmoid activa -\ntions for hidden and output units, respectively. Where appropriate, label imbalance is \ndealt with by a cost-sensitive model variant [49].\nDecision trees Additionally, we consider the Decision Tree classifier (DT)  [50], \na non-linear classifier based on recursive splits of the input space so as to suitably \ndetect the regions to be assigned to the individual classes according to input data \nD. The set of recursive splits can be represented through a tree, where each node is \nassociated with a feature and a threshold, used to bi-partition input data according to \ntheir value for that feature. Data are recursively split till leaf nodes, where no further \nsplit is possible. DTs are typically used to perform binary or multiclass classification, \ntherefore leaves are associated with one of the classes. In our case instead, where a \nreal score in [0, 1] is needed as output, the usual approach is to take the fraction of \n1-labeled samples ending in a leaf node (w.r.t. the total number of samples ending in \nthe same node) as its prediction value. The setting of this classifier is jointly selected \nwith that of Random Forests (an extension of DTs explained in the next paragraph), to \navoid the model selection for both models to become too computationally intensive. \nAs a consequence, we fix the maximum depth and maximum number of leaves dur -\ning the DT growth, and consider only one non-key hyperparameter, the minimum minw,b1\n2�w�2+c/summationtext\nx∈Dξx,\nsuch that yxf(x)≥1−ξx∀x∈D,\nξx≥0∀x∈D,\n\nPage 8 of 26 Malchiodi et al. Journal of Big Data           (2024) 11:45 \nnumber δ of samples in a leaf. This hyperparameter prevents further splitting of nodes \nwhen the split would induce children containing less that δ samples, and accordingly \nallows to control the depth of the tree and its complexity. Although this can effec -\ntively affect the classifier size, the overall training setting of DT substantially limits \nthe impact of this hyperparameter on the DT size.\nIt is worth noting here that the ‘indirect’ way to derive a score of a classifier inher -\nently designed for discrete outputs might lead to ‘unusual’ distributions of out -\nput scores, which might produce unstable behaviors of the learned BF using it. For \ninstance, this is the case when the number of leaves is small and/or multiple leaves \ncontain the same fraction of 1-labeled instances (that is, they are associated with the \nsame output score), which can cause the classifier to have only a few distinct output \nscores.\nRandom Forests Finally, we consider also Random Forests [51], shortened as RF-t  to \nmake explicit that this model is an ensemble of t  classification trees. Each such tree \nis trained on a different bootstrap subset of D  randomly extracted with replacement. \nAnalogously, the splitting functions at the tree nodes are chosen from a random sub -\nset of the available attributes. The RF aggregates classifications uniformly across trees, \ncomputing for each instance the fraction of trees that output a positive classification. \nTo address the case of unbalanced labels, we adopt an imbalance-aware variant of \nRFs  [ 52, 53] in which, during the growth of each tree, the bootstrap sample is not \ndrawn uniformly over D , but by selecting an instance x with probability\nwhere D+={x∈D|yx=1} , and D−=D\\D+ . In this way, the probabilities of extract -\ning a positive or a negative example are both 1/2, and the trees are trained on balanced \nsubsets. The key hyperparameter of a RF is t, directly impacting on the classifier size. \nThe non-key hyperparameters are selected as for DTs (see previous paragraph).\nMeasures of classification complexity\nSeveral approaches can be considered when the classification complexity of a data -\nset is to be assessed (see [54] for a survey). In our experiments, we specifically focus \non binary classification tasks, using the notation “class i ”,  i=1, 2 , to refer to the two \nclasses involved in the problem (although when less precision is needed, we resort \nto the classical terminology involving a positive and a negative class). Some of the \nmeasures listed in the above mentioned survey (precisely, F1, T2, or T3) proved to \nbe insensitive across a variety of data synthetically generated, whereas the evaluation \nof other ones (mainly network- or neighborhood-based measures such as LSC  and \nN1) required an excessive amount of RAM. Therefore we selected the feature-based  \nmeasure F1v and the class-imbalance measure C2: both take values in [0, 1], and the \nhigher the value, the more complex is the dataset. The former quantity, also called \nthe Directional-vector Maximum Fisher’s Discriminant Ratio, is defined as follows: \ndenote, respectively, by pi , µi , and /Sigma1i the proportion of examples belonging to class px=/braceleftBigg1\n2|D+|ifyx=1,\n1\n2|D−|ifyx=0,\n\nPage 9 of 26\n Malchiodi et al. Journal of Big Data           (2024) 11:45 \n \ni and the corresponding centroid and scatter matrix, so that W=p1/Sigma11+p2/Sigma12 and \nB=(µ1−µ2)(µ 1−µ2)⊤ are the between- and within-class scatter matrices. In this \ncase, d=W−1(µ1−µ2) corresponds to the direction onto which there is maximal \nseparation of the two classes (being W−1 the pseudo-inverse of W ), and we can define \nthe F1v measure as\nThe second measure accounts for label balance in the dataset: letting ni be the number of \nexamples of class i, we have C2=(n1−n2)2/(n2\n1+n2\n2).\nData generation\nWe generate synthetic data considering three parameters, a , r and ρ , which allow to \ntune the complexity of generated data according to the aforementioned measures. \nIntuitively, a controls the linearity of the separation boundary, r  the label noise, and ρ \nthe label imbalance. More precisely, in order to generate a binary classification dataset \nwith a given level of complexity, n1 positive and n2=⌈ρn1⌉ negative instances (with \nN=n1+n2 ), we proceed as follows. Let {x1,...xN}⊂Rq be the set of samples, with \neach sample xi having q features xi1,...,xiq , and a binary label yi∈{0, 1} . The N sam -\nples are drawn from a multivariate normal distribution N(0,/Sigma1) , with /Sigma1=γIq (with \nγ> 0 and Iq denoting the q×q identity matrix). In our experiments we set γ=5 so \nas to have enough data spread, reminding that this value however does not affect the \ndata complexity. Without loss of generality, we consider the case q=2 . To determine \nthe classes of positive and negative samples, the parabola x2−ax2\n1=0 is considered, \nwith a>0 : a point xi=(xi1,xi2) is positive ( yi=1 ) if xi2−ax2\ni1>0 , negative other -\nwise ( yi=0 ). This choice allows us to control the linear separability of positive and \nnegative classes by varying the parameter a : the closer a  to 0, the more linear the sep -\naration boundary. As a consequence, a  controls the problem complexity for a linear \nclassifier. An example of generated data by varying a  is given in Fig.  1a–c. Further, to \nvary the data complexity even for non linear classifiers, labels are perturbed with dif -\nferent levels of noise: we flip the label of a fraction r  of positive samples, selected uni -\nformly at random, with an equal number on randomly selected negatives. The effect \nof three different levels of noise is depicted in Fig.  1d–f, where the higher the noise, \nthe less sharp the separation boundary. The third parameter ρ is the ratio between the \nnumber of negative and positive samples in the dataset, thus it controls the C 2 com -\nplexity measure. Higher values of ρ make the negative boundary more clear (Fig.  1g–\ni), while making harder training an effective classifier [55].\nExperiments\nThis section describes the datasets and the hardware we use in our experiments, as \nwell as the process we follow in order to select the optimal classifiers and the corre -\nsponding learned Bloom filters.(3) F1v=/parenleftBigg\n1+d⊤Wd\nd⊤Bd/parenrightBigg−1\n.\n\nPage 10 of 26 Malchiodi et al. Journal of Big Data           (2024) 11:45 \nDatasets\nWe process both domain-specific and synthetically generated datasets. Concerning \nthe first category, we use a URL dataset and a DNA dictionary. The first has been \nused by [29], who also kindly provided us with the dataset, as part of their experi -\nmentation on learned Bloom filters. The second dataset comes from experiments in \nBioinformatics regarding the storage and retrieval of k -mers (i.e., strings of length k \nappearing in a given genome, whose spectrum is the dictionary of k-mers) [56] and \nwas directly generated by us. We point out that no sensible information is contained \nin these datasets. With reference to Table  1, they represent two extreme cases of clas -\nsification complexity: the URL dataset is easy , as it is simple in terms of linear sepa -\nrability (F 1v), albeit exhibiting a relevant C 2 complexity due to the label imbalance; \nthe DNA data is hard, in that it has almost the maximum F 1v possible value, meaning \nthat positive and negative classes are indistinguishable by a linear classifier.\nFig. 1 Graphical representation of synthetic data: first row, parameter configuration is np=500 , r=0 , ρ=1 \nand a=0.01 (a), a=0.1 (b), and a=1 (c); second row np=500 , a=0.1 , ρ=1 and r=0 (d), r=0.1 (e), and \nr=0.25 (f); third row, np=100 , a=0.1 , r=0 , ρ=1 (g), ρ=3 (h), and ρ=5 (i). “pos” and “neg” entries in the \nlegend stand for positive and negative class, respectively\nTable 1 Complexity of the real data\nData F1v C2\nURL 0.08172 0.62040\nDNA 0.99972 0\n\nPage 11 of 26\n Malchiodi et al. Journal of Big Data           (2024) 11:45 \n \nThe URL dataset contains 485,730 unique URLs, 80,002 malicious and the remaining \nbenign . Seventeen lexical features are associated with each URL, which are used as the \nclassification features. It is worth pointing out that all of the previous works on learned \nBloom filters have used URL data. In this context, a Bloom filter can be used as a time- \nand space-efficient data structure to quickly reject benign URLs, never erroneously \ntrusting a malicious URL although occasionally misclassifying a benign one. We adhere \nto this standard here.\nThe DNA dataset refers to the human chromosome 14, containing n=49,906,253  \n14-mers [56] constituting the set of our keys. As non-keys, we uniformly generate other \nn 14-mers from the 414 possible strings on the alphabet {A,T ,C ,G } . Each 14-mer is \nassociated with a 14-dimensional feature vector, whose components are the integers \n0, 1, 2, 3, each associated with one of the four DNA nucleobases A, T, C, G, respectively \n(for instance a 14-mer TAATTACGAATGGT  is coded as (1, 0, 0, 1, 1, 0, 2, 3, 0, 0, 1, 3, \n3, 1)). A fundamental problem in Bionformatics, both technological [57] and in terms \nof evolutionary studies [58], is to quickly establish whether a given k-mer belongs to the \nspectrum of a genome. In this case, the Bloom filter stores the dictionary. It is worth \nmentioning that the use of Bloom filters in Bioinformatics is one of their latest fields of \napplication, with expected high impact [59]. Such a domain has not been considered for \nthe evaluation of learned Bloom filters, as we do here.\nWe also generate two categories of synthetic data, each attempting to reproduce the \ncomplexity of one of the domain-specific data. The first category has nearly the same \nC2 complexity of the URL dataset, i.e., it is unbalanced, with n1=105 and ρ=5 . The \nsecond one has the same C2 complexity of the DNA dataset, i.e., it is balanced , with \nn1=105 and ρ=1 . The choice of n1 allows to have a number of keys similar to that in \nthe URL data, and at the same time to reduce the computational burden of the mas -\nsive set of experiments planned. Indeed, both balanced and unbalanced categories \ncontain nine datasets, exhibiting increasing levels of F1v complexity. Specifically, all \npossible combinations of parameters a∈{0.01, 0.1, 1}  and r∈{0, 0.1, 0.25}  are used. The \ncorresponding complexity estimation is shown in Table  2. Consistently, F1v complex -\nity increases with a and r values, in both balanced and unbalanced settings. Notewor -\nthy, the label imbalance slightly affects also the F1v measure: in absence of label noise \n( r=0 ), F1v augments, likely due to the fact that F1v is an imbalance-unaware measure; \nTable 2 Complexity of the synthetic data\na r Balanced Unbalanced\nF1v C2 F1v C2\n0.01 0 0.127 0.0 0.129 0.615\n0.1 0 0.181 0.0 0.202 0.615\n1 0 0.306 0.0 0.360 0.615\n0.01 0.1 0.268 0.0 0.187 0.615\n0.1 0.1 0.327 0.0 0.269 0.615\n1 0.1 0.459 0.0 0.433 0.615\n0.01 0.25 0.571 0.0 0.308 0.615\n0.1 0.25 0.619 0.0 0.399 0.615\n1 0.25 0.718 0.0 0.563 0.615\n\nPage 12 of 26 Malchiodi et al. Journal of Big Data           (2024) 11:45 \non the contrary, in presence of noise the F1v complexity is barely reduced w.r.t. the bal -\nanced case. Although not immediate, the sense of this behavior might reside in what \nwe also observe in Fig.  1g–i. That is, the boundary of negative class tends to be more \ncrisp when ρ increases, thus mitigating the opposite effect the noise has on the boundary \n(Fig. 1d–f).\nHardware and software\nWe use two Ubuntu machines: an Intel Core i7-10510U CPU at 1.80 GHz × 8 with 16 GB \nRAM, and an Intel Xeon Bronze 3106 CPU at 1.70 GHz × 16 with 192 GB RAM. This \nlatter is used for experiments that require a large amount of main memory, i.e., on the \nDNA dataset. The supporting software [60] is written in Python 3.8, leveraging the \nADA-BF public implementation provided in [61], which we extend as follows: (1) the \nconstruction of learned Bloom filters can be done in terms of the classifiers listed in “ The \nconsidered classifiers” section and of the datasets illustrated in “Datasets ” section; (2) \nSLBF is added to the already included BF models; (3) the choice of the classifier thresh -\nold τ is performed considering any number of evenly spaced percentiles of the obtained \nclassification scores, instead than checking fixed values; (4) ranges for the hyperparam -\neters of the learned versions of BFs can be specified by the user; (5) a main script allows \nto perform all experiments, rather than invoking several scripts, each dedicated to a LBF \nvariant.\nThe provided implementation is built on top of standard libraries, listed in a dedicated \nenvironment file in order to foster replicability. In particular, the space required by a \ngiven classifier is computed, as typically done in these cases, using the Pickle module and \naccounting for both structure information and parameters [62], in order to obtain a fair \ncomparison among all tested configurations. Moreover, the software is open to exten -\nsions concerning the inclusion of new datasets and/or new LBF models, thus it can be \nused as a starting point for further independent researches.\nSelecting optimal classifiers and learned Bloom filters\nClassifiers Being the classifier performance/size trade-off crucial for the learned vari -\nants of BF, we first assess the classifier generalization ability independently of the filter \nemploying it. We adopt a threefold cross validation (CV) procedure (outer), measur -\ning the classifier performance in terms of (a) the area under the ROC curve (AUC), \nand of (b) the area under the precision-recall curve (AUPRC), averaged across folds. \nWe tune non-key hyperparameters of each model via a nested threefold CV (inner), \nwhere in each round of the outer CV, we select the optimal non-key hyperparameters \nthrough a grid search, retaining the configuration yielding the best AUPRC value. The \nfollowing grids are considered: c∈{10−1, 1, 10, 102, 103} (SVM); δ∈{1, 3, 5}  (DT and \nRF); lr∈{10−4, 10−3} (NN). The different size of the grid across classifiers is due to \nthe different training time of the classifier (SVM is the fastest one). The configuration \nof a classifier is strictly dependent on the space budget assigned to the LBF leveraging \nthat classifier (see Table  5 discussed later on); consequently, the key hyperparameters \nfor a given classifier, i.e., hyperparameters influencing the space occupancy, are set \nas follows. Recalling that no key hyperparameters exist for SVMs, we consider RFs \nrelated to two values of t , leading to a simpler and a more complex model. The choice \n\nPage 13 of 26\n Malchiodi et al. Journal of Big Data           (2024) 11:45 \n \nt=10 , a reference already used in the literature [29], leads to a simple model, which \nwe use on all datasets. As for the complex model, we have two subcases, depending \non space budget. For the low budget case, we set t=20 , and the corresponding model \nis used for the synthetic/URL dataset. As for the high budget case, we set t=100 , and \nthe corresponding model is used for the DNA dataset. Those choices, although empir -\nical, indeed provide models with the required complexity. For completeness, we men -\ntion that other values of t  could have been considered, but we have verified that those \nchoices would not add any further insight to the perspective of our analysis. The key \nhyperparameters for NNs are selected so as to yield three models nearly having the \nsame occupancy of the SVM and of the two RF models, for a fair comparison. Indeed, \nwe concentrate on those two classifiers since they represent the two end-points in \nregard to the space occupancy spectrum: DTs use more space than the SVM and less \nthan the RFs. The above-mentioned preliminary experiments have suggested, where \nenough space budget was available, that a two-layered topology is to be preferred to \nthe one-layered one. Precisely, we consider the following models: NN-25, NN-150, 50 \nand NN-200, 75 (synthetic dataset); NN-7, NN-150, 35 and NN-175, 70 (URL data -\nset); NN-7, NN-125, 50, NN-500, 150 (DNA dataset). The final classifier configuration \nfor all experiments and their space requirements are detailed in Table  3. In Table  4 \nwe also include the average prediction time of the tested classifiers, that, jointly with Table 3 Space occupancy in Kbits of selected classifiers on the considered datasets\nThe acronyms for all classifiers refer to the notation introduced in “ The considered classifiers ” sectionSVM DT RF-10 RF-20 NN-25 NN-150,50 NN-200,75\nSynthetic Data\n 5 30.9 259.3 508.6 5.1 260.2 506.6\n SVM DT RF-10 RF-20 NN-7 NN-150,35 NN-175,70\nURL Data\n 5.9 31 259.3 508.7 6.2 259.2 499.9\n SVM DT RF-10 RF-100 NN-7 NN-125,50 NN-500,150\nDNA Data\n 5.8 30.9 259.5 2504 5.6 265.8 2652.3\nTable 4 Average classifier inference time (across samples) in seconds. Same notations as in Table 3\nSVM DT RF-10 RF-20 NN-25 NN-150,50 NN-200,75\nSynthetic Data\n 1.278 ·10−82.651 ·10−84.425 ·10−78.968 ·10−78.494 ·10−69.257 ·10−61.008 ·10−5\n SVM DT RF-10 RF-20 NN-7 NN-150,35 NN-175,70\nURL Data\n 3.730 ·10−86.515 ·10−85.815 ·10−79.930 ·10−76.825 ·10−67.018 ·10−67.198 ·10−6\n SVM DT RF-10 RF-100 NN-7 NN-125,50 NN-500,150\nDNA Data\n 2.87·10−81.236 ·10−75.572 ·10−75.364 ·10−66.572 ·10−68.138 ·10−61.044 ·10−5\n\nPage 14 of 26 Malchiodi et al. Journal of Big Data           (2024) 11:45 \nresults in Table  3, will be discussed in “Results and discussion ” and  “ Guidelines” \nsections.\nLearned Bloom filters We leverage the evaluation setting for the Bloom filter var -\niants proposed by [29], and composed of the following steps: (1) train the classifi -\ners using all keys and 30% of non-keys; (2) query the filter using remaining 70% of \nnon-keys to compute the empirical FPR; (3) fix an overall memory budget of m  bits \nthe filters must use, and compare them in terms of their empirical FPR ǫ . Further -\nmore, we also measure the average reject time of filters, motivated by the fact that it \ncan unveil interesting trends about the synergy of the filter variants and the classifier \nthey employ. Indeed, we train any filter variant using in turn each of the considered \nclassifiers.\nThe budget m  is selected in relation to the desired (expected) ǫ of a classical Bloom \nfilter, according to (1 ). We choose budgets differently on each dataset, since m  directly \ndepends on the key set size n . For synthetic data, having generated several datasets, \nwe only test two different choices for the space budget m  for each of them. Namely, \nthose yielding ǫ∈{0.05, 0.01}  for the classical Bloom filter using a bit vector of m \nbits. Conversely, more budget choices are tested on real datasets, since they are less \nnumerous, namely those leading to ǫ∈{0.01, 0.005, 0.001, 0.0005, 0.0001}  . The differ -\nence between this setting and that of synthetic data is due to the following consid -\nerations. First, the dimensionality of synthetic data is 2, whereas that of real data it is \n17 and 14, respectively, for URL and DNA. This makes the classifiers using real data \nlarger than their counterparts on synthetic data. For this reason, on real data we omit \nthe case ǫ=0.05 , which yielded a too small budget. Indeed, some classifiers alone \nexceed the budget in this case (cfr. Table  3 for details about the space occupancy of \nclassifiers). Moreover, having only two datasets, we can test more choices of ǫ , and \naccordingly better evaluate the behavior of learned Bloom filters when a smaller \n(expected) false positive rate is required.\nTable  5 shows the obtained budget configurations for synthetic and real datasets. \nEven to train a learned Bloom filter we have operated a grid search to choose the opti -\nmal hyperparameters on the training data, optimizing with respect to the FPR. The \nfollowing grids are utilized: (a) LBF and SLBF, 15 evenly spaced values for threshold τ \nin the classifier score range [0, 1]; (b) ADA-BF, the integers within [3, 15] for g , and 10 \nevenly spaced values in [1, 5] for ¯c (cfr. “ Classical and learned Bloom filters’ section’). \nImportantly, the latter choice includes and extends the configurations adopted in [61] \n(namely, [8, 12] for g  and [1.6, 2.5] for ¯c).\nTable 5 Space budget in Kbits adopted on the various datasets. ǫ is the false positive rate, n is the \nnumber of keys in the dataset\nData ǫ Budget (Kbits) n\nSynthetic 0.05, 0.01 622, 956 105\nURL 0.01, 0.005, 0.001, 0.0005, 0.0001 765, 880, 1148, 1263, 1530 8·104\nDNA 0.01, 0.005, 0.001, 0.0005, 0.0001 477460, 549325, 716191, 788056, \n9549214.99 ·107\n\nPage 15 of 26\n Malchiodi et al. Journal of Big Data           (2024) 11:45 \n \nResults and discussion\nIn this section we present the results obtained from the classifier screening on both syn -\nthetic and real data, the experimental evaluation of all variants of LBFs based on those \nclassifiers, and the relative discussion. In particular, we focus on the following questions: \n(1) It is possible to detect in advance, before constructing the learned BF, which classifier \nis best of use starting only from the classifier performance? (2) Is there a direct impact \nof the data complexity on the performance of a learned BF? (3) The noise in data plays \na role of the performance of a learned BF? (4) Is there a relation linking the choice of a \nlearned BF to reject time? We discuss the first question in “Performance of classifiers” \nsection, the two subsequent ones in “Data classification complexity versus learned filters \nperformance” section, and the last one in “Reject time” section.\nPerformance of classifiers\nAs clear from previous sections, the classifier can be seen as an oracle for a learned BF, \nwhere the better the oracle, the better the related filter, i.e., its FPR for a given fixed the \nspace budget. Accordingly, we first discuss the performance of classifiers, tested on the \ndatasets described in “Datasets ” section, and according to the hyperparameter configu -\nrations described in “Selecting optimal classifiers and learned Bloom filters” section. Fig -\nure 2 depicts the performance of classifiers on balanced  and unbalanced synthetic data, \nwhereas results obtained on real data are shown in Fig.  3. However, it is central here to \nFig. 2 Performance averaged across folds of compared classifiers on synthetic data. First row for balanced \ndata, second row for unbalanced data. Bars are grouped by dataset, in turn denoted by a couple (a, r) \nexpressing, respectively, the separation boundary linearity and the amount of label noise\n\nPage 16 of 26 Malchiodi et al. Journal of Big Data           (2024) 11:45 \nemphasize that the interpretation of such results is somewhat different than what one \nwould do in a standard machine learning setting. Indeed, we have a space budget for the \nentire filter, and the classifier must discriminate well between keys and non-keys, while \nbeing substantially succinct with regard to the space budget of the data structure. Such \na scenario implicitly imposes a performance/space trade-off: hypothetically, we might \nhave a perfect classifier using less space than the budget, and on the other extreme, a \npoor classifier exceeding the space budget.\nOverall results analysis\nFirst, the behavior of classifiers in terms of AUC and AUPRC is coherent with what \nexpected according to our methodology to generate synthetic data. Indeed, the SVM \nperformance decays when the parameter a increases, being in line with the fact that it \nmeans increasing the non-linearity of the class separation boundary. Analogously, all \nclassifiers worsen as noise r increases, which is clearly what to expect in this case. More -\nover and most importantly, two main cases arise with respect to classification complex -\nity: roughly F1v≤0.35 and F1v>0.35 . Being this threshold experimentally derived, the \ndivision between the two cases is not crisp. We refer to the first case as datasets ‘easier \nand easier’ to classify, for brevity ‘easy’ , and to the second as datasets ‘harder and harder’ \nto classify, for brevity ‘hard’ .\nEasy datasets. All classifiers perform very well on synthetic datasets with the stated \ncomplexity (except for SVMs when a>0.01 ). Clearly, with such excellent oracles, the \nremaining part of a learned Bloom filter (e.g., with reference to the description of LBF, \nthe backup filter) is intuitively expected to be very succinct.\nHard datasets. In this case, both AUC and AUPRC sensibly drop, being in some cases \n(SVM) not so far from the behavior of a random classifier. While in the previous case the \nperformance of classifiers clearly yields the choice of the most succinct and faster model, \nhere there is a trade-off to consider. Indeed, within the given space budget, at one end of \nthe spectrum, we have the choice of a small-space and inaccurate classifier, at the other \nend of the spectrum we have larger and more accurate ones. As an example, for the LBF \nFig. 3 Performance averaged across folds of compared classifiers on real data: a URL, b DNA\n\nPage 17 of 26\n Malchiodi et al. Journal of Big Data           (2024) 11:45 \n \nin the first case a large backup filter is required, whereas in the second one the classifier \nwould use most of the space budget.\nPreliminary observations on the classifiers to be retained\nHere we address the question of how to choose a classifier to build the filter upon, based \nonly on the knowledge of space budget and data classification complexity/classifier per -\nformance. On synthetic and URL data (Figs.  2, 3a), more complex classifiers perform just \nslightly better than the simpler ones, likely due to the low data complexity in these cases. \nAt the same time, they require a sensibly higher fraction of the space budget (Table  5), \nand it is thereby natural to retain in these cases only the smallest/simplest variants, \nnamely: DT, RF-10 and NN-25 (synthetic) and NN-7 (URL), in addition to SVM (we \nrefer here, and in the rest of the paper, to the acronyms introduced in “ The considered \nclassifiers” section). Conversely, in our DNA experiments more complex classifiers sub -\nstantially outperform the simpler counterparts, coherently with the fact that this classifi -\ncation problem is much harder (Tables  1, 2). Since the available space budget is larger in \nthis case, all classifiers have been retained in the subsequent filter evaluation.\nPerformance of learned Bloom filters\nIn this section we: (1) explore the behavior of learned filters with respect to the data \nclassification complexity, an aspect so far neglected in the literature (see Introduction); \n(2) discuss the reject time of filters with regard to their empirical FPR; (3) gain further \ninsights about the interplay between the classifiers and the learned Bloom filter variants.\nData classification complexity versus learned filters performance\nThe empirical FPR of learned Bloom filters on balanced and unbalanced synthetic data \n(generated according to the procedure described in “Datasets ” section) are respectively \nshown in Figs.  4 and  5, whereas Figs.  6 and  7 depict the results on URL and DNA data. \nIn all cases, also the baseline Bloom filter is present.\nEasy datasets. According to the rough definition introduced in “ Overall results analy -\nsis” section (F1v around 0.35 or smaller), the three/four leftmost configurations on the \nx-axis in Figs.  4 and  5 of synthetic data, and URL data can be considered as ‘easy’ data. \nIn such cases, our results are coherent with those obtained in the literature, where ADA-\nBF slightly outperforms the other competitors [29], and RF-10 induces lower FPR values \nwith regard to the baseline BF. However, such a classifier is not the best choice, since \nother ones induce filters with lower FPR, e.g., NN-25 for synthetic data and NN-7 for \nURL data. This again warns from the use of classifiers without a justification, as it has \nbeen done in previous studies.\nHard datasets. A novel scenario emerges with the increase of data complexity, i.e., \nwhen moving towards right on the horizontal axis in Figs.  4 and  5, or when consider -\ning DNA data. The performance of the filters drops more and more, in compliance \nwith the performance decay of the subordinate classifiers (“Performance of classifi -\ners” section), and unexpectedly the drop is faster in ADA-BF (and LBF) w.r.t. SLBF. \nThis trend can be ascertained for instance on all synthetic data having r>0 (noise \ninjection). Unexpectedly because it represents an inversion with reference to the \ntrend reported in the literature, where usually ADA-BF outperforms SLBF (which in \n\nPage 18 of 26 Malchiodi et al. Journal of Big Data           (2024) 11:45 \nturn improves LBF). Our results indeed manifest that SLBFs is more robust to noise, \nwhich is likely due to a reduced dependency for SLBF on the classifier performance, \nfavoured by the usage of the initial Bloom filter. Such a filter indeed allows the classi -\nfier to be queried only on a subset of the data.\nWhen adopting RFs in this setting, the empirical FPR of learned filters strongly \nincreases, and potential explanations reside in the excessive score discretization: hav -\ning 10 trees, only 11 distinct scores are possible. In addition, RF space occupancy \nis larger (limiting in turn the space to be assigned to initial/backup filters). These \nresults have a relevant confirmation on the very hard and large DNA dataset (Fig.  7), \nwhere LBF cannot attain any improvement with regard to the baseline BF, unlike \nSLBF and ADA-BF. A potential cause is in the worse performance of classifiers on \nFig. 4 False positive rates of LBF (first row), SLBF (second row), and ADA-BF (third row) attained on balanced \nsynthetic datasets (cfr. “Datasets ” section). On the horizontal axis, labels X_Y denote the dataset obtained \nwhen using a=X and r=Y . The blue dotted line corresponds to the empirical false positive rate of the \nclassical BF in that setting. Two space budgets m are tested, ensuring that ǫ=0.05 (left) and ǫ=0.01 (right) \nfor the classical BF. Legends shared across rows\n\nPage 19 of 26\n Malchiodi et al. Journal of Big Data           (2024) 11:45 \n \nFig. 5 False positive rates of LBF (first row), SLBF (second row), and ADA-BF (third row) attained on \nunbalanced synthetic datasets (cfr. “Datasets ” section). On the horizontal axis, the labels X_Y denote the \ndataset obtained when using a=X and r=Y . The blue dotted line corresponds to the measured false \npositive rate of the classical Bloom filter in that setting. Two space budgets are tested: that ensuring ǫ=0.05 \nfor a classical Bloom filter (left), and that ensuring ǫ=0.01 (right). Legends shared across rows\nFig. 6 Empirical false positive rate of LBF (left), SLBF (central), and ADA-BF (right) filters on URL data. On the \nhorizontal axis the different budgets configurations. Dotted blue line represents the baseline classical Bloom \nfilter. The legend is shared across rows\n\nPage 20 of 26 Malchiodi et al. Journal of Big Data           (2024) 11:45 \nthis hard dataset, differently from those obtained on synthetic and URL data, and in \na too marked dependency of LBF on the classifier performance. Such a dependency is \nlikely to be mitigated instead in the other two filter variants by the usage of the ini -\ntial BF (SLBF) and by the fine-grained classifier score partition (ADA-BF). DTs even \namplify this behavior: for the reasons explained in “ The considered classifiers” sec -\ntion, its scores are strongly quantized and on this hard task we found they do not \nspan the whole range [0, 1] (most leaf scores are concentrated around 0.5), fostering a \nnearly flat FPR for both LBF and ADA-BF. As a general tendency, SLBF outperforms \nboth LBF and baseline of one order of magnitude in FPR with the same space amount, \nand ADA-BF when using weaker classifiers, or when a higher budget is available. We \nsuspect the latter case is due to overfitting in the partitioning of classifier codomain \nADA-BF operates, which is more deleterious when the classifier performance is not \nexcellent, as it happens for DNA data. As a consequence, here the classifiers leading \nto the best empirical FPR values are the most complex, differently from hard synthetic \ndata, where the key set was smaller (and consequently also the space budget was \nsmaller). In particular, the best choice for these data are NN-500,150 and NN-125,50, \nwhich, as an additional motivation to employ them, can be also further compressed \nusing specific techniques recently emerged [63]. In other words, we can conclude that \ntoo simple classifiers are useless or even deleterious on hard datasets, see for instance \nSVM-based filters, that never improve the baseline.\nReject time\nTable  6 provides the average per-element reject time of all learned filters, taken across \nall the query sequences and space budgets that we have used in our experiments. \nThey are expressed as percentage increment (or decrement) of the time required by \nthe baseline. A first novel and interesting feature which emerges is that learned BF are \nsometimes faster than the baseline, which in principle is not expected, since learned \nvariants have to query a classifier in addition to a classical BF. Our interpretation is \nthat this can happen for two main reasons: (1) the adopted classifier is very fast and \nalso effective, hence allowing in most cases to skip querying the backup filter; (2) the \nkey set is very large, thus requiring a large baseline BF, whereas a good classifier can \nallow to sensibly drop the dimension of backup filters, thus making their invocation \nFig. 7 Empirical false positive rate of LBF (left), SLBF (central), and ADA-BF (right) filters on DNA data. On the \nhorizontal axis the different budgets configurations. Dotted blue line represents the baseline classical Bloom \nfilter. Same legends as in Fig. 6\n\nPage 21 of 26\n Malchiodi et al. Journal of Big Data           (2024) 11:45 \n \nmuch faster. See for instance the case of DNA data, where most learned filters are \nfaster that the baseline, with most classifiers.\nAnother intriguing behavior concerns the reject time of ADA-BF, often the worst \narchitecture in terms of this metric. We believe it depends on the more complex proce -\ndure used in order to establish whether or not to access the backup filter—an exception \nis represented by DT-based filters, where, as apposite to other classifiers, the ‘anoma -\nlous’ score distribution moves the optimal number of groups learned towards the lowest \nvalues, and the resulting filters are faster. Indeed, such a procedure is subject to tuning, \nwhich in turn can yield less or more complex instances of the filter. As evident from our \nexperiments, such a strategy does not always payoff.\nFinally, we discuss the relationship between the inference time of the classifiers and \nthe reject time of the learned Bloom filters, an aspect completely overlooked in the lit -\nerature. In particular, from our experiments it emerges that the classifier inference time \ncannot be considered as a proxy for the reject time of the induced learned Bloom filter, \nand accordingly it is to be considered in the choice of the classifier only when classifiers \nexhibit very similar classification performance.\nIndeed, we can observe that the order of the inference time of classifiers (Table  4) is \noften inverted with regard to the reject time of the corresponding filters. For instance, \nSVM is always the fastest classifier, but in some cases RF-based filters are faster (e.g., \nLBF and SLBF on synthetic data). This behavior is not so immediate to explain, and \nmight be related to the following discussion. The accuracy of the classifier can impact on \nthe false negative rate of the filter, and consequently on the number of false negatives to Table 6 Learned Bloom filters average reject time, expressed as ratio between learned filter and \nbaseline BF reject times (whose time in seconds is in parentheses) \nPositive (resp. negative) values indicate that the learned filter is slower (faster) than the baseline. The best configurations \nare highlighted in bold. Results are averaged across test queries and the filter space budgets considered. We remark that for \nDNA experiments another machine has been used w.r.t. synthetic and URL data (see “Hardware and software ” section)Classifier LBF SLBF ADA-BF\nSynthetic Data  ( 1.364 ·10−5)\n SVM 18.4 6.1 151.2\n DT −1.1 − 1.2 1.3\n RF −11.1 − 17.5 112.8\n NN 106.9 54.1 159.3\nURL Data  ( 3.259 ·10−5)\n SVM 22.6 3.7 3.9\n DT − 1.4 − 1.4 −1.1\n RF 6.6 7.1 9.7\n NN 43.9 49.6 35.3\nDNA Data (4.817 ·10−5)\n SVM − 12.5 −11.7 35.9\n DT −1.1 − 1.3 1.3\n RF-10 1.4 − 20.6 32.0\n RF-100 19.8 − 7.4 40.6\n NN-7 −5.0 − 12.0 25.8\n NN-125,50 −3.6 − 7.5 25.2\n NN-500,150 −1.9 − 11.6 39.2\n\nPage 22 of 26 Malchiodi et al. Journal of Big Data           (2024) 11:45 \nbe stored in the backup filter/s (as well as on the reject time of the backup filter). There -\nfore, to optimize the reject time of a learned BF one can reduce the inference time of the \nclassifier and/or reduce its false negative rate (that is, lowering the size of the backup \nfilter). As an example, on synthetic data RFs outperform SVMs (Fig.  2), their inference \ntime is one order of magnitude higher than SVMs (on average), but all the learned filters \nit induces have a lower reject time than the SVM-based counterparts (cfr. Table 6).\nGuidelines\nWe summarize in this section our findings about the configuration of learned variants \nof Bloom filters, given a prior knowledge of data complexity and available space budget.\nData Complexity and Classifier Choice. With reference to Fig.  8, and recalling from \n“Reject time” section that the classifier inference time is not a proxy for the filter reject time, \nalthough it is natural to choose a fast classifier among several ones having comparable classi -\nfication power, our recommendations are as follows. First, it must be evaluated how complex \nis the dataset, e.g. in our experiments easy  ( F1v≤0.35 ) and hard (otherwise). Choosing the \nclassifier is relatively straightforward on easy datasets: independently of the space budget, it \nis always more convenient to select simple classifiers. We found linear SVMs to be the best \nchoice on datasets having almost linear separation boundary ( a<0.1 ), and the smallest/sim -\nplest NNs are advisable to be used in the remaining cases of this category.\nConversely, the space budget becomes more discriminant for the classifier choice on \nhard datasets. Within the budget given by (1), we can distinguish two extreme cases for \nhard datasets: those having a relatively small set of keys, and accordingly a small budget, \nand those having instead a large key set and a high budget. When the budget is small, for \ninstance like it happens in more complex synthetic data, the following considerations \nhold: (1) the choice is almost forced towards small (and potentially inaccurate) classi -\nfiers, being the larger ones too demanding for the available budget; (2) a (linear) SVM is \nto be excluded, due to the increased difficulty of the task; Therefore, among the remain -\ning classifiers, we recommend the use of small NNs, which in our experiments were the \nFig. 8 Recommended guidelines for choosing the classifier in a learned BF\n\nPage 23 of 26\n Malchiodi et al. Journal of Big Data           (2024) 11:45 \n \nmost succinct model among the best performing ones (cfr. Fig.  2). On the other hand, \nwhen the budget is high (as with DNA data) our findings suggest to learn more accurate \nclassifiers, even if this requires the usage of a considerable budget fraction. Therefore, \nwe suggest to invest a significant part of the budget in a complex model (in our experi -\nments, the most complex NNs) that will better fit the available data. This is motivated \nby the following facts: (1) the gain related to higher classification abilities allows to save \nspace when constructing the auxiliary BFs of the learnt filters, and to consequently have \na smaller reject time (cfr. “Data classification complexity versus learned filters perfor -\nmance” section); (2) too poor classifiers induce the learned BFs to perform even worse \nthan their classical counterpart (see Fig.  7); (3) the availability of a big amount of data \nallows to train complex classifiers more effectively [64].\nLearned Bloom Filters Choice. With regard to the choice of the learned BF to be \napplied, we recommend the suggestions depicted in Fig.  9. In presence of complex or \nnoisy data, use a SLBF, in view of its ability to exploit even classifiers having a poor \nperformance. In the remaining situations, the required reject time becomes discri -\nminant. Since ADA-BF often exhibits the highest reject times, if a fast learned BF \nis required, SLBF should be preferred over ADA-BF, otherwise ADA-BF is a better \nchoice (confirming previous literature results).\nConclusions and future developments\nThe present study proposes an experimental evaluation that can guide in the design \nand validation of learned Bloom filters. The key point is to base the choice of the \nclassifier to be used in a learned Bloom filter on the space budget of the entire data \nFig. 9 Recommended guidelines for choosing the learned BF type\n\nPage 24 of 26 Malchiodi et al. Journal of Big Data           (2024) 11:45 \nstructure, as well as on the classification complexity of the dataset in input. Our \nexperimental approach has unveiled behaviors of learned Bloom filters neglected by \nprevious studies, including: (1) how robust are the current learned filters for the pro -\ncessing of datasets of increasing complexity: indeed, only the “easy to classify” sce -\nnario has been considered in the Literature so far; (2) how robust are the learned \nfilters with regard to noise injection in input data: our methodology revealed that \nthe efficiency ranking of learned Bloom filters emerged in previous studies must be \nrevised in presence of noisy data; (3) how crucial and discriminant is the selection of \nthe classifier in terms of false positive rate, size and reject time of the learned Bloom \nfilter. We have summarized such novel insights within a “ Guidelines” section to help \npractitioners in suitably designing learned Bloom filters for their applications.\nA potential limitation of such results is that they might be dependent on the consid -\nered data; nonetheless, this is somehow inevitable due to the nature of learned data \nstructures. Finally, we point out that the societal impacts of our contributions are in \nline with general-purpose Machine Learning technology. Natural extensions of this \nresearch are the following. As already remarked, we have complied with an experi -\nmental setting coherent with the state of the art. We can also consider the scenario \nin which the desired false positive rate is fixed (instead of the overall filter size) and \none asks for the most succinct pair classifier-filter (instead of the lowest false posi -\ntive rate). Moreover, in his seminal paper [28], Mitzenmacher has shown that learned \nBloom filters can be quite sensitive to the input query distribution. Yet, no study is \navailable to quantify this aspect. Our methodology can be extended also to those \ntypes of analysis and work in this direction is in progress. Finally, as outlined in the \nIntroduction, the multi-dimensional case has received very little attention, and it \ndeserves further investigations.\nAcknowledgements\nWe would like to thank Z. Dai and A. Shrivastava for having provided us the dataset used in [29].\nAuthors’ contributions\nDM, RG and MF conceived the study, designed the experiments and discussed the results. They also were major con-\ntributors in writing the manuscript. DR, GF and MF performed the experiments. All authors read and approved the final \nmanuscript.\nFunding\nThis work has been supported by the Italian MUR PRIN project “Multicriteria data structures and algorithms: from \ncompressed to learned indexes, and beyond” (Prot. 2017WR7SHH). Additional support to RG has been granted by Project \nINdAM—GNCS “Analysis and Processing of Big Data based on Graph Models” . DM, DR, GF and MF acknowledge the sup -\nport of the APC central fund of the University of Milan.\nAvailability of data and materials\nThe source code, data, and/or other artifacts have been made available at https:// github. com/ Raimo ndiD/ LBF_ ADABF_ \nexper  iment.\nDeclarations\nEthics approval and consent to participate\nNot applicable\nConsent for publication\nNot applicable\nCompeting interest\nThe authors declare that they have no competing interest.\nReceived: 11 May 2023   Accepted: 14 March 2024\n\n\nPage 25 of 26\n Malchiodi et al. Journal of Big Data           (2024) 11:45 \n \nReferences\n 1. Kraska T, Beutel A, Chi EH, Dean J, Polyzotis N. The case for learned index structures. In: Proceedings of the 2018 \ninternational conference on management of data. SIGMOD ’18. New York: Association for Computing Machin-\nery, 2018. p. 489–504. https:// doi. org/ 10. 1145/ 31837 13. 31969 09.\n 2. Wu Q, Wang Q, Zhang M, Zheng R, Zhu J, Hu J. Learned bloom-filter for the efficient name lookup in informa-\ntion-centric networking. J Netw Comput Appl. 2021;186:103077. https:// doi. org/ 10. 1016/j. jnca. 2021. 103077.\n 3. Kirsche M, Das A, Schatz MC. Sapling: accelerating suffix array queries with learned data models. Bioinformatics. \n2020;37(6):744–9. https:// doi. org/ 10. 1093/ bioin forma tics/ btaa9 11.\n 4. Mitzenmacher M, Vassilvitskii S. Algorithms with predictions. In: Roughgarden T, editor. Beyond the worst-case \nanalysis of algorithms. Cambridge: Cambridge University Press; 2021. p. 646–62. https:// doi. org/ 10. 1017/ 97811  \n08637 435. 037\n 5. Duda RO, Hart PE, Stork DG. Pattern classification. 2nd ed. New York: Wiley; 2000.\n 6. Freedman D. Statistical models?: Theory and practice. Cambridge: Cambridge University Press; 2005.\n 7. Amato D, Lo Bosco G, Giancarlo R. Standard versus uniform binary search and their variants in learned \nstatic indexing: the case of the searching on sorted data benchmarking software platform. Softw Pract Exp. \n2023;53(2):318–46. https:// doi. org/ 10. 1002/ spe. 3150.\n 8. Amato D, Giancarlo R, Lo Bosco G. Learned sorted table search and static indexes in small-space data models. \nData. 2023;8(3):56. https:// doi. org/ 10. 3390/ data8 030056.\n 9. Amato D, Lo Bosco G, Giancarlo R. Neural networks as building blocks for the design of efficient learned indexes. \nNeural Comput Appl. 2023;35(29):21399–414. https:// doi. org/ 10. 1007/ s00521- 023- 08841-1.\n 10. Ferragina P , Frasca M, Marinò GC, Vinciguerra G. On nonlinear learned string indexing. IEEE Access. \n2023;11:74021–34. https:// doi. org/ 10. 1109/ ACCESS. 2023. 32954 34.\n 11. Ferragina P , Vinciguerra G. The PGM-index: a fully-dynamic compressed learned index with provable worst-case \nbounds. PVLDB. 2020;13(8):1162–75. https:// doi. org/ 10. 14778/ 33891 33. 33891 35.\n 12. Ferragina P , Lillo F, Vinciguerra G. On the performance of learned data structures. Theor Comput Sci. \n2021;871:107–20.\n 13. Kipf A, Marcus R, van Renen A, Stoian M, Kemper A, Kraska T, Neumann T. Radixspline: a single-pass learned \nindex. In: Proceedings of the of the third international workshop on exploiting artificial intelligence techniques \nfor data management. aiDM ’20. New York: Association for Computing Machinery; 2020. p. 1–5.\n 14. Kirsche M, Das A, Schatz MC. Sapling: accelerating suffix array queries with learned data models. Bioinformatics. \n2020;37(6):744–9.\n 15. Maltry M, Dittrich J. A critical analysis of recursive model indexes. Proc VLDB Endow. 2022;15(5):1079–91. https://  \ndoi. org/ 10. 14778/ 35103 97. 35104 05.\n 16. Marcus R, Kipf A, van Renen A, Stoian M, Misra S, Kemper A, Neumann T, Kraska T. Benchmarking learned \nindexes, vol. 14; 2020. p. 1–13. arXiv preprint arXiv: 2006. 12804\n 17. Marcus R, Zhang E, Kraska T. CDFShop: Exploring and optimizing learned index structures. In: Proceedings of the \n2020 ACM SIGMOD international conference on management of data. SIGMOD ’20; 2020; p. 2789–2792.\n 18. Boffa A, Ferragina P , Vinciguerra G. A “learned” approach to quicken and compress rank/select dictionaries. In: \nProceedings of the SIAM symposium on algorithm engineering and experiments (ALENEX); 2021.\n 19. Bloom BH. Space/time trade-offs in hash coding with allowable errors. Commun ACM. 1970;13(7):422–6. https://  \ndoi. org/ 10. 1145/ 362686. 362692.\n 20. Leskovec J, Rajaraman A, Ullman JD. Mining of massive data sets. 2nd ed. Cambridge: Cambridge University \nPress; 2014. https:// doi. org/ 10. 1017/ CBO97 81139 924801.\n 21. Almeida PS, Baquero C, Preguiça N, Hutchison D. Scalable Bloom filters. Inf Process Lett. 2007;101(6):255–61.\n 22. Melsted P , Pritchard JK. Efficient counting of k-mers in DNA sequences using a Bloom filter. BMC Bioinf. \n2011;12(1):1–7.\n 23. Zhang Z, Wang W. RNA-Skim: a rapid method for RNA-Seq quantification at transcript level. Bioinformatics. \n2014;30(12):283–92. https:// doi. org/ 10. 1093/ bioin forma tics/ btu288.\n 24. Chang F, Dean J, Ghemawat S, Hsieh WC, Wallach DA, Burrows M, Chandra T, Fikes A, Gruber RE. Bigtable: a \ndistributed storage system for structured data. ACM Trans Compute Syst. 2008;26(2):1–26.\n 25. Broder A, Mitzenmacher M. Network applications of Bloom filters: a survey. In: Internet mathematics, vol. 1, 2002. \np. 636–646. http:// cites eerx. ist. psu. edu/ viewd oc/ summa ry? doi=  10.1. 1. 20. 98\n 26. Crainiceanu A, Lemire D. Bloofi: multidimensional Bloom filters. Inf Syst. 2015;54:311–24. https:// doi. org/ 10.  \n1016/j. is. 2015. 01. 002.\n 27. Zeng M, Zou B, Kui X, Zhu C, Xiao L, Chen Z, Du J, et al. Pa-lbf: prefix-based and adaptive learned bloom filter for \nspatial data. Int J Intell Syst. 2023;2023.\n 28. Mitzenmacher M. A model for learned bloom filters and optimizing by sandwiching. In: Bengio S, Wallach H, \nLarochelle H, Grauman K, Cesa-Bianchi N, Garnett R, editors. Advances in Neural Information Processing Systems, \nvol. 31. Red Hook: Curran Associates; 2018. p. 1.\n 29. Dai Z, Shrivastava A. Adaptive Learned Bloom Filter (Ada-BF): Efficient utilization of the classifier with application \nto real-time information filtering on the web. In: Advances in neural information processing systems, vol. 33, \nRed Hook: Curran Associates, Inc.; 2020. p. 11700–11710. https:// proce edings. neuri ps. cc/ paper/ 2020/ file/ 86b94  \ndae7c 6517e c1ac7 67fd2 c1365 80- Paper. pdf\n 30. Vaidya K, Knorr E, Kraska T, Mitzenmacher M. Partitioned learned Bloom filters. In: International conference on \nlearning representations; 2021. https:// openr  eview. net/ forum? id=  6BRLO  frMhW\n 31. Liu Q, Zheng L, Shen Y, Chen L. Stable learned Bloom filters for data streams. Proc VLDB Endow. \n2020;13(12):2355–67. https:// doi. org/ 10. 14778/ 34077 90. 34078 30.\n 32. Fumagalli G, Raimondi D, Giancarlo R, Malchiodi D, Frasca M. On the choice of general purpose classifiers in \nlearned Bloom filters: an initial analysis within basic filters. In: Proceedings of the 11th international conference \non pattern recognition applications and methods (ICPRAM); 2022. p. 675–682.\n\nPage 26 of 26 Malchiodi et al. Journal of Big Data           (2024) 11:45 \n 33. Dai Z, Shrivastava A, Reviriego P , Hernández JA. Optimizing learned bloom filters: How much should be learned? \nIEEE Embedded Syst Lett. 2022;14(3):123–6.\n 34. Ali S, Smith KA. On learning algorithm selection for classification. Appl Soft Comput. 2006;6(2):119–38.\n 35. Cano J-R. Analysis of data complexity measures for classification. Expert Syst Appl. 2013;40(12):4820–31. https://  \ndoi. org/ 10. 1016/j. eswa. 2013. 02. 025.\n 36. Flores MJ, Gámez JA, Martínez AM. Domains of competence of the semi-naive Bayesian network classifiers. Inf \nSci. 2014;260:120–48.\n 37. Luengo J, Herrera F. An automatic extraction method of the domains of competence for learning classifiers \nusing data complexity measures. Knowl Inf Syst. 2015;42(1):147–80.\n 38. Patgiri R, Biswas A, Nayak S. deepbf: Malicious url detection using learned bloom filter and evolutionary deep \nlearning. Comput Commun. 2023;200:30–41.\n 39. Malchiodi D, Raimondi D, Fumagalli G, Giancarlo R, Frasca M. A critical analysis of classifier selection in learned \nbloom filters: the essentials. In: Iliadis, L., Maglogiannis, I., Castro, S., Jayne, C., Pimenidis, E. (eds.) Engineering \napplication of neural networks—24th international Conference—EAAAI/EANN 2023—León, Spain, June 14-17, \n2023—Proceedings. Communications in Computer and Information Science, vol. 1826; 2023, p. 47–61. Springer.\n 40. Wegman MN, Carter JL. New hash functions and their use in authentication and set equality. J Comput Syst Sci. \n1981;22(3):265–79. https:// doi. org/ 10. 1016/ 0022- 0000(81) 90033-7.\n 41. Carter JL, Wegman MN. Universal classes of hash functions. J Comput Syst Sci. 1979;18(2):143–54. https:// doi.  \norg/ 10. 1016/ 0022- 0000(79) 90044-8.\n 42. Broder A, Mitzenmacher M. Network applications of Bloom filters: a survey. Internet Math. 2004;1(4):485–509.\n 43. Cox DR. The regression analysis of binary sequences. J R Stat Soc Ser B (Methodol). 1958;20(2):215–32.\n 44. Duda RO, Hart PE. Pattern classification and scene analysis. New York: Willey; 1973.\n 45. Cho K, van Merriënboer B, Bahdanau D, Bengio Y. On the properties of neural machine translation: Encoder–\ndecoder approaches. In: Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statisti-\ncal Translation. p. 103–111. Association for Computational Linguistics, Doha, Qatar; 2014. https:// doi. org/ 10.  \n3115/ v1/ W14- 4012. https:// aclan tholo gy. org/ W14- 4012\n 46. Morik K, Brockhausen P , Joachims T. Combining statistical learning with a knowledge-based approach: a case \nstudy in intensive care monitoring. Technical Report; 1999.\n 47. Zell A. Simulation neuronaler netze. habilitation, Uni Stuttgart, 1994.\n 48. Haykin S. Neural networks: a comprehensive foundation. Upper Saddle River: Prentice Hall PTR; 1994.\n 49. Bruzzone L, Serpico SB. Classification of imbalanced remote-sensing data by neural networks. Pattern Recogn \nLett. 1997;18(11):1323–8. https:// doi. org/ 10. 1016/ S0167- 8655(97) 00109-8.\n 50. Breiman L, Friedman JH, Olshen RA, Stone CJ. Classification and regression trees. Boca Raton: Chapman & Hall/\nCRC; 1984.\n 51. Breiman L. Random forests. Mach Learn. 2001;45(1):5–32.\n 52. Van Hulse J, Khoshgoftaar TM, Napolitano A. Experimental perspectives on learning from imbalanced data. \nIn: Proceedings of the 24th International Conference on Machine Learning. ICML ’07. New York: ACM; 2007. p. \n935–942. https:// doi. org/ 10. 1145/ 12734 96. 12736 14\n 53. Khalilia M, Chakraborty S, Popescu M. Predicting disease risks from highly imbalanced data using random forest. \nBMC Med Inf Decis Mak. 2011;11(1):51.\n 54. Lorena AC, Garcia LPF, Lehmann J, Souto MCP , Ho TK. How complex is your classification problem? A survey on \nmeasuring classification complexity. ACM Comput Surv. 2019;52(5):1–34. https:// doi. org/ 10. 1145/ 33477 11.\n 55. He H, Garcia EA. Learning from imbalanced data. IEEE Trans Knowl Data Eng. 2009;21(9):1263–84. https:// doi.  \norg/ 10. 1109/ TKDE. 2008. 239.\n 56. Rahman A, Medevedev P . Representation of k-mer sets using spectrum-preserving string sets. J Comput Biol. \n2021;28(4):381–94. https:// doi. org/ 10. 1089/ cmb. 2020. 0431.\n 57. Solomon B, Kingsford C. Fast search of thousands of short-read sequencing experiments. Nat Biotechnol. \n2016;34(3):300–2. https:// doi. org/ 10. 1038/ nbt. 3442.\n 58. Chor B, Horn D, Goldman N, Levy Y, Massingham T, et al. Genomic DNA k-mer spectra: models and modalities. \nGenome Biol. 2009;10(10):108.\n 59. Elworth RAL, Wang Q, Kota PK, Barberan CJ, Coleman B, Balaji A, Gupta G, Baraniuk RG, Shrivastava A, Treangen \nTJ. To petabytes and beyond: recent advances in probabilistic and signal processing algorithms and their appli-\ncation to metagenomics. Nucleic Acids Res. 2020;48(10):5217–34. https:// doi. org/ 10. 1093/ nar/ gkaa2 65.\n 60. Raimondi D, Fumagalli G. A Critical Analysis of Classifier Selection in Learned Bloom Filters—Supporting Soft -\nware. https:// github. com/ Raimo ndiD/ LBF_ ADABF_ exper  iment. Last checked on May, 2023; 2023.\n 61. Dai Z. Adaptive Learned Bloom Filter (ADA-BF): Efficient Utilization of the Classifier. https:// github. com/ DAIZH  \nENWEI/ Ada- BF . Last checked on November 8, 2022; 2022.\n 62. Python Software Foundation: pickle—Python object serialization. https:// docs. python. org/3/ libra ry/ pickle. html . \nLast checked on May 17, 2022 (2022)\n 63. Marinò GC, Petrini A, Malchiodi D, Frasca M. Deep neural networks compression: a comparative survey and \nchoice recommendations. Neurocomputing. 2022. https:// doi. org/ 10. 1016/j. neucom. 2022. 11. 072.\n 64. Raudys S. On the problems of sample size in pattern recognition. In: Detection, pattern recognition and experiment \ndesign: Vol. 2. Proceedings of the 2nd All-union conference statistical methods in control theory (1970). Publ. House \n“Nauka” .\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "textLength": 83227
}