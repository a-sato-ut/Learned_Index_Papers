{
  "paperId": "c33108867e8a6b26b73b7c6f14a73ec6d0c5fe3f",
  "title": "Learning Based Distributed Tracking",
  "pdfPath": "c33108867e8a6b26b73b7c6f14a73ec6d0c5fe3f.pdf",
  "text": "Learning Based Distributed Tracking\nHao WU\nwhw4@student.unimelb.edu.au\nThe University of Melbourne\nMelbourne, AustraliaJunhao Gan\njunhao.gan@unimelb.edu.au\nThe University of Melbourne\nMelbourne, AustraliaRui Zhang\nrui.zhang@unimelb.edu.au\nThe University of Melbourne\nMelbourne, Australia\nABSTRACT\nInspired by the great success of machine learning in the past decade,\npeople have been thinking about the possibility of improving the\ntheoretical results by exploring data distribution. In this paper,\nwe revisit a fundamental problem called Distributed Tracking (DT)\nunder an assumption that the data follows a certain (known or\nunknown) distribution, and propose a number data-dependent al-\ngorithms with improved theoretical bounds. Informally, in the DT\nproblem, there is a coordinator and kplayers, where the coordinator\nholds a threshold Nand each player has a counter. At each time\nstamp, at most one counter can be increased by one. The job of the\ncoordinator is to capture the exact moment when the sum of all\nthese kcounters reaches N. The goal is to minimise the communi-\ncation cost. While our first type of algorithms assume the concrete\ndata distribution is known in advance , our second type of algo-\nrithms can learn the distribution on the fly. Both of the algorithms\nachieve a communication cost bounded by O(klog log N)with high\nprobability, improving the state-of-the-art data-independent bound\nO(klogN\nk). We further propose a number of implementation opti-\nmisation heuristics to improve both efficiency and robustness of\nthe algorithms. Finally, we conduct extensive experiments on three\nreal datasets and four synthetic datasets. The experimental results\nshow that the communication cost of our algorithms is as least as\n20%of that of the state-of-the-art algorithms.\nCCS CONCEPTS\n•Mathematics of computing →Probabilistic algorithms .\nKEYWORDS\nAlgorithms, Sampling, Distributed Tracking, Machine Learning\nACM Reference Format:\nHao WU, Junhao Gan, and Rui Zhang. 2020. Learning Based Distributed\nTracking. In Proceedings of the 26th ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining (KDD ’20), August 23–27, 2020, Virtual Event, CA,\nUSA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3394486.\n3403255\n1 INTRODUCTION\nThe great success of machine learning in the past decade has proven\nthe assumption that data in practice follows certain patterns (e.g.,\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nKDD ’20, August 23–27, 2020, Virtual Event, CA, USA\n©2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-7998-4/20/08. . . $15.00\nhttps://doi.org/10.1145/3394486.3403255either known or unknown distributions). Such an assumption in\nturn becomes the base of those (machine learning) techniques. In-\nspired by this, people have been thinking about the possibility to\nimprove theoretical results on traditional problems with machine\nlearning techniques. Successful progresses have been made on a\nwide range of problems, such as frequency estimation [ 1,9], approxi-\nmate membership [ 15], combinatorial optimization [ 3,12,17], index\nstructures [ 14], and etc. The rationale behind these progresses is to\nexplore and exploit the underlying distribution of the input data\nand to design data-dependent algorithms customized for the data\ndistribution. In this paper, we design data-dependent algorithms,\nimproving the state-of-the-art theoretical bounds, for solving the\nDistribution Tracking (DT) problem [7].\nThe DT problem setting. In the DT problem, there is a coor-\ndinator and kplayers (a.k.a. sites); between each player and the\ncoordinator, there is a two-way communication channel. The coor-\ndinator holds a threshold N, while the i-th player has a counter ni\ninitialized as 0for all i∈{1,2, . . . , k}. At each time stamp, there is\nat most one (that means it can be none) player having its counter\nincreased by one, conceptually representing an item arrives at the\nplayer. The job of the coordinator is to raise an alarm at the exact\nmoment that the N-th item arrives (at some player), equivalently,\nthe moment that the sum of the counters of all the players reaches\nN, i.e.,Ík\ni=1ni=N. The efficiency of an algorithm for solving\nthe DT problem is measured by the communication cost, i.e., the\ntotal number of messages that received and sent by the coordinator,\nwhere each message can only carry at most O(1)words .\nTo solve the DT problem, a straightforward algorithm is to in-\nstruct each player to send a message to notify the coordinator for\nevery increment on its counter. The communication cost of this\nalgorithm is clearly N(messages). However, such a communication\ncost is considered expensive, as Nis large in practice. Existing\nwork [ 7] showed that the DT problem actually admits an algorithm\n(the CMY algorithm named after its authors) with a communica-\ntion cost of O(klogN\nk). When Nis far larger than k, this algorithm\nconsumes significantly less communication cost than the straight-\nforward algorithm.\nThe CMY algorithm. For the ease of explanation, we introduce a\nsimplified version of the state-of-the-art CMY algorithm achieving\nthe same communication bound. The simplified algorithm runs in\nrounds . In each round, if N<4k, run the straightforward algo-\nrithm with O(N)=O(k)messages. Otherwise (i.e., N≥4k), the\ncoordinator sends a slack s=⌊N\n2k⌋to each player. Each player\nsends a message to notify the coordinator, whenever its counter is\nincreased by ssince its last communication with the coordinator. If\nthe coordinator receives the k-th message, then it collects all the k\ncounter values nifrom the players and calculate N′=N−Ík\ni=1ni.\nIfN′=0, the coordinator raises the alarm. Otherwise, start a newarXiv:2006.12943v1  [cs.DS]  23 Jun 2020\n\n0\n6\n12\n18\n24\n30Round 1\nRound 2\nRound 3Player 1 Player 2 Player 3\nPlayer 4Coordinator  Figure 1: A running example of the simplified CMY algo-\nrithm with N=50andk=4, where the counter increments\nin different rounds are highlighted with different colors and\ntextures. The slack values in the first two rounds are s=6\nands=2, respectively; and in the third round, the algorithm\nswitches to the straightforward algorithm.\nround to solve a new DT problem instance with N=N′from\nscratch. As it is easy to verified that in each round, the coordinator\nsends and receives O(k)messages; and after each round, Nis de-\ncreased by a constant factor. Hence, there can be at most O(logN\nk)\n(with respect to the original N) rounds; the total communication\ncost is bounded by O(klogN\nk)messages.\nA running example. Figure 1 shows a running example of the\nDT algorithm on an instance with N=50andk=4, In the\nfirst round, since N>4k=16, the coordinator sends a slack\ns=⌊N\n2k⌋=⌊50\n2∗4⌋=6to each player. At the end of this round,\nPlayer 1 has received 18 counter increments and thus in total 3\nmessages have been sent from Player 1 to the coordinator, which\nwere sent for every s=6increments. Likewise, Player 2 has sent\na message as its counter is increased by s=6, while both of the\ncounters of Players 3 and 4 are just increased by 3: no messages\nwere sent from them. The coordinator collects all the counters as\nsoon as it receives the k-th (i.e., the fourth) messages and calculate\nN′=50−(6∗3+6+3+3)=20. Next, the coordinator starts a new\nround with a new instance with N=20andk=4from scratch,\nwhere s=⌊20\n2∗4⌋=2. As shown in the figure, at the end of this\nround, the counters of the four players have been increased by 6,\n2,1and1, respectively; and N′=10. A new round with N=10\nandk=4is thus started, in which the algorithm switches to the\nstraightforward algorithm (as N<4k): a message is sent for each\ncounter increment. As for the communication cost, in each of the\nfirst two rounds, the coordinator sends 4messages for sending the\nslack sto the four players, receives 4messages, sends 4messages\nfor requesting the counter values, and finally receives 4messages\nfor the counter values. Therefore, the communication cost in each\nof these two rounds is 16. Plus the 4+10messages in the last round,\nthe total communication cost is thus 16∗2+14=46.\nExploiting the counter increment distribution. In order to pro-\nvide a worst-case communication bound, the CMY algorithm has\nto be pessimistic and conservative: it makes no assumption on the\ndata distribution; and only identical slacks scan be sent to all the\nplayers in a round. Such pessimism and conservation, unfortunately,\nprevents the CMY algorithm from reducing the communication by\nexploiting the data distribution. In particular, the data distributionNotation Description\nN the threshold to monitor\nk the number of players\n[k] the set of integers from 1tok\nni the counter of the i-th player\nsi the slack of the i-th player\nµi the probability that a new item arrives at player i\n¯µi the estimation of µi\nT(N) the communication cost with threshold N\nTable 1: Frequently used notations\nwe mean here is the Multinomial Distribution of the counter incre-\nments , more specifically, the probability distribution of a counter\nincrement happening in the players. For the example shown in\nFigure 1, the probability distribution of the counter increments is\n(0.6,0.2,0.1,0.1): for each counter increment, it has probability of\n0.6happening in Player 1, 0.2in Player 2, 0.1in Player 3, and 0.1\nin Player 4.\nThe knowledge on the probability distribution (i.e., the Multino-\nmial Distribution) of the counter increments indeed can be lever-\naged to significantly reduce the communication cost. As an extreme\ncase in our earlier example in Figure 1, suppose that one knows the\nfinal counter value ciof the i-th player for all i∈{1,2, . . . , k}at\nthe moment when their sum reaches N, namely, c1=30,c2=10,\nc3=c4=5. A better solution is to instruct the coordinator to send\nacustomized slack si=cito the i-th player, and to raise the alarm\nwhen it receives the fourth messages from the players. Clearly,\nthe communication cost of this solution is only 8messages, five\ntimes less than the cost of the CMY algorithm. While knowing\nall the final counter values cia prior is, of course, too good to be\ntrue, this observation sheds a light on the possibility of designing\nimproved data-dependent algorithms with the knowledge of the\ncounter increment distribution.\nMotivated by the above, in this paper, we consider the DT prob-\nlem under the assumption below:\nAssumption 1. The counter increments follow a certain Multi-\nnomial Distribution which can be unknown. More specifically, each\ncounter increment occurs in the i-th player with probability µifor all\ni∈{1,2, . . . , k}, whereÍk\ni=1µi=1.\nOur contributions. We make the following contributions:\n•First, for the case that the concrete Multinomial Distribution (i.e.,\nthe concrete values of all µi’s) is known a prior, we show a data-\ndependent algorithm, called StcSlk -KwnDst , for solving the DT\nproblem. The communication cost of the StcSlk -KwnDst algo-\nrithm is bounded by O(klog log N)with high probability, improv-\ning the state-of-the-art O(klogN\nk)bound. We further propose\ntheDynSlk -KwnDst algorithm which improves the practical per-\nformance of StcSlk -KwnDst , while retaining exactly the same\ncommunication bound.\n•Second, for the case that the distribution is unknown , we pro-\npose two learning based algorithms, called StcSlk -LrnDst and\nDynSlk -LrnDst , corresponding to the two algorithms in the first\ncase. Both of these two algorithms can learn the data distribution\non the fly, meanwhile achieving exactly the same theoretical\nbounds as their counterpart algorithms.\n\n•Moreover, we design an effective heuristics to optimize our algo-\nrithm implementations.\n•Finally, we conduct extensive experiments on both three real\ndatasets and four synthetic datasets of different data distributions.\nThe experimental results show that our proposed algorithms\noutperform the state-of-the-art algorithms by consuming up to\nfive times (i.e., 5x) less communication cost.\n2 RELATED WORK\nThe distributed tracking (DT) problem has been well studied in\nterms of both upper bounds and lower bounds, since it was first\nproposed.\nPrior to the CMY algorithm, an uniform slack (called UniSlk )\nalgorithm was proposed by Cormode et al. [ 5]. The communication\ncost of UniSlk is bounded by O(k2logN\nk). The design of UniSlk\nis based on the observation that for Nitems arriving at kplay-\ners, there must be at least one of player received N/kitems. The\nUniSlk algorithm runs in rounds. At the at the beginning of the\nfirst round, the coordinator broadcasts a slack N/kto each players.\nThe player notifies the coordinator when its counter exceeds N/k.\nUpon receiving one notification, the coordinator informs the rest\nof players to report the values of their counters. This ends the first\nround. The number of items arrived, namely,Ík\ni=1niis at least\nN/kand at most⌈N/k⌉+(k−1)⌊N/k⌋ ≤ N. The coordinator\nupdates the threshold N←N−Ík\ni=1ni. IfN=0, it raises an\nalarm, otherwise it starts a new round. Each round incurs O(k)\ncommunications and decreases the threshold Nby a factor of at\nleast(1−1/k)≤exp(−1/k). Therefore, the number of rounds is at\nmost O(klogN\nk)and the communication cost is thus O(k2logN\nk).\nLater on, the CMY algorithm was proposed in [ 7]. As introduced\nearlier, the CMY algorithm consumes O(klogN\nk)communication\ncost. Although we are focusing on the number of messages in this\npaper, in [ 7], Cormode et al. showed a Ω(klogN\nk)-bit communica-\ntion lower bound for the DT problem. Moreover, the CMY algorithm\nindeed admits a bit-version implementation with communication\ncost of O(klogN)bits.\nMore works have been done for the variants of the DT prob-\nlem. Randomized algorithm was also proposed for approximate\ncount tracking [7]. Instead of reporting exactly the N-th item, the\ncoordinator is allowed to raise an alarm on the arrival of any item\nbetween[(1−ϵ)N,N], whereϵ∈(0,1)is a specified parameter. The\nproblem is easier than the exact count tracking problem due to the\nrelaxation and the proposed algorithm has cost O(1\nϵ2log1\nδ)bits [ 7],\nwhereδis the failure probability of the algorithm. [ 10,11] studied\nthe continuous count tracking problem, in which the coordinator\nis required to report an estimation ˆnofn=Í\ni∈[k]niat any time\nstamp, such that ˆn∈(1±ϵ)n. Their algorithms have communi-\ncation costs of O(k\nϵlogϵN\nk)[11] and O(√\nk\nϵlogN)messages [ 10],\nrespectively. The work of [ 7] also considered threshold tracking of\nFpmoment, where DT can be considered as a special case of p=1.\nData pattern has been studies and exploited for different dis-\ntributed monitoring queries ([ 6,8]), but none of them targets the\nfundamental threshold count tracking problem. [ 6] introduced the\nUpdate-Rate Model for distributed tracking of approximate quan-\ntiles, which assumes that items arrives at the i-th player at a localAlgorithm 2.1 Theℓ-Notifications-to-End Framework\nInput: a threshold N, the number of players k, and\npossibly a failure probability δif applicable\n1:Setni←0for all ∀i∈[k]\n2:IfN≤4k, run the straightforward algorithm until it terminates.\n3:IfN≤β·klnk\nδ, run the CMY algorithm until it terminates,\nwhereβis an algorithm-specified constant.\n4:The coordinator send a slack sito player ifor∀i∈[k].\n5:Each Player inotifies the coordinator, when nimeets certain\ncondition with respect to si.\n6:The coordinator collects all the ni’s from the players, when it\nreceives the ℓ-th notification .\n7:SetN←N−Ík\ni=1ni\n8:IfN=0, the coordinator raises an alarm and terminate.\n9:Otherwise, go to Step 1 and start a new round.\nrate specified to i. Note that this is captured by the multinomial dis-\ntribution model if we normalize the rates by the summation of the\nplayers’ rates (and with proper scaling of time). Later [ 8] extends\nthe idea for geometric monitoring to reduce the communication\ncost.\nBesides, interestingly, the techniques for solving the DT problem\nhas also been applied to solve some seemly “remote” problem in\nthesingle machine setting [16].\n3 A UNIFIED ALGORITHM FRAMEWORK\nBefore we get into the details of our algorithms, in this section, we\nfirst propose a unified algorithm framework , called ℓ-Notifications-\nto-End , where ℓis acharacteristic parameter of an algorithm and not\nan input parameter. As we will see shortly, all our algorithms, the\nCMY algorithm as well as the UniSlk algorithm are all under this\nframework. Algorithm 2.1 shows the pseudo code of the framework.\nAlgorithm characteristics. Essentially, algorithms under the ℓ-\nNotifications-to-End framework only differ in the following three\ncharacteristics :\n•Characteristic 1: the value of slack sifor∀i∈[k](Line 4);\n•Characteristic 2: the condition for a player to notify the coor-\ndinator (Line 5);\n•Characteristic 3: the number ℓof notifications received to end\na round (Line 6).\nTo see this, consider the CMY algorithm, where: (i) si=⌊N\n2k⌋; (ii)\na player notifies the coordinator when niis increased by si; and\n(iii)ℓ=k. Thus, the CMY algorithm is an k-Notifications-to-End\nalgorithm. On the other hand, the UniSlk algorithm is, in fact, a\n1-Notification-to-End algorithm with si=N/k, where a player\nnotifies the coordinator when ni≥si, and with Line 3 being never\nexecuted. As we will see in the next two sections, all our algorithms\nwill be focusing on designing the above three characteristics.\nCommunication cost expression. Observe that the communi-\ncation cost for executing Line 2 and Line 3 in Algorithm 2.1 are\nbounded by O(k)andO(klog logk\nδ), respectively. As these two\ncases are easy to check and solve, it suffices to focus on the case\nN=ω(klnk\nδ). In this case, it can be verified that an ℓ-Notification-\nto-End algorithm consumes O(k+ℓ)communication cost per round.\n\nTherefore, the overall communication cost in this case is bounded\nbyO((k+ℓ)·R+klog logk\nδ), where Ris the total number of rounds\nthat have been executed before entering into Line 2.\n4 TRACKING WITH KNOWN DISTRIBUTION\nIn this section, we consider the case that the concrete Multinomial\nDistribution of the counter increments (a.k.a. the item arrivals) is\nknown. That is, the concrete values of µifori∈[k]are given. This\ncase allows us to just focus on algorithmic design without worrying\ntoo much about the learning of the distribution.\n4.1 Tracking with Static Slacks\nThe challenges. We note that even the concrete values of µi’s are\nknown in advance, the problem is still challenging.\nAs the concrete values of all µi’s are known, it is natural to think\nabout modifying the CMY algorithm such that the slack siis set to\nthe expected number of items that Player iwill receive when the\nN-th item arrives, namely, si=µiNfor∀i∈[k]. We denote this\nnew k-Notifications-to-End algorithm by A.\nUnfortunately, in general, it is unlikely that every player will\nreceive exactlyµiNitems, i.e., ni=µiN, when the N-th item\narrives; the actual nicould be more or less than µiN. As a result,\nAmay fail to capture the moment of the N-th item’s arrival, when\nthe coordinator receives the k-th notification.\nTo remedy this, one possible way is to further modify Ainto\na1-Notifications-to-End algorithm by setting ℓto 1, where the\ncoordinator inAcollects all the ni’s as soon as it receives a notifi-\ncation from the player. This guarantees thatÍk\ni=1ni<Nand the\ncoordinator would not miss the N-th item. However, Acould be\nless efficient than CMY . This is because any small deviation from\nµiNwould easily make Aend the round too early, resulting in an\nincrease on the number of rounds R. As an illustration, suppose that\nthere are 10players and the first three players have very low prob-\nabilities of receiving items. In particular, µ1=µ2=µ3=1\nN. In this\ncase, if any item arrives at any of the first three players, a notifica-\ntion is sent to the coordinator and the round ends. With probability\nat least 1−1/e, this round captures only N/3items: for a given\nitem, the probability that it arrives at a player other than the first\nthree is(1−3/N); therefore, with probability (1−3/N)N/3≤1/e,\nnone of the first N/3items arrive at any of the first three player. In\ncomparison, CMY captures at least N/2in one round.\nTo capture more items in one round, the slack should contain a\nleeway for the player’s counter to bear deviations from its expec-\ntation, i.e., sishould be greater than µiNfori∈[k]. The player\nshould receive a few more items than its expectation to defer the\nnotification. However, this conflicts with our goal of capturing the\nN-th item – if we set si>µiNfori∈[k], thenÍ\ni∈[k]si>N. The\ncoordinator could miss the arrival of the N-th item. The following\nconstraint for ensuring the correctness\nkÕ\ni=1si≤N (1)\nimplies that tracking Nitems in just one round seems too ambitious.\nWe relax our goal and, instead, aim to track titems for some t<N\nand set si>µit, while ensuring the correctness, i.e.,Ík\ni=1si≤N.\nWe prove that there exists si’s, such that when the first titemsarrives, with high probability that none of the player has received\nmore than siitems. It implies that no notification would have been\nsent to the coordinator. In other words, when the first notification\nis sent, more than titems have arrived in this round. Clearly, the\nlarger tis, the less number of rounds we need.\nThe StcSlk -KwnDst Algorithm. Motivated by the above ob-\nservation, we propose our first data-dependent algorithm,\nStcSlk -KwnDst , which is an 1-Notifications-to-End algorithm. char-\nacterized by the followings:\n•Characteristic 1 :\nsi=µit+r\n2tµi(1−µi)lnk\nδ+2\n3lnk\nδ,∀i∈[k]; (2)\n•Characteristic 2 : Player inotifies the coordinator when ni=si;\n•Characteristic 3 :ℓ=1, that is, the coordinator ends a round\nwhen it receives the first notification from the players.\nSubstituting the above implementations of the three characteristics\nto Lines 4, 5 and 6, respectively, in Algorithm 2.1 gives the pseudo\ncode of the StcSlk -KwnDst algorithm. Furthermore, we have the\nfollowing key theorem:\nTheorem 4.1. With probability 1−δ′, the StcSlk -KwnDst algo-\nrithm:\n•runs at most O(log logN\nklogk\nδ+log logk\nδ)rounds;\n•has total communication cost O(klog logN\nklogk\nδ+klog logk\nδ),\nwhereδ′=δ·O(log log N).\nAnswering the following three questions is the key to proving\nTheorem 4.1:\n•Why does Expression (2)ensure that StcSlk -KwnDst can capture\nat least titems at one round with probability 1−δ?\n•How large can tbe?\n•How many rounds does StcSlk -KwnDst need?\nIn what’s follows, we address these questions one by one.\nSetting the Slack. Expression (2)is actually derived from the fol-\nlowing concentration inequality:\nFact 1. (Bernstein inequality [ 4]) Let Y1, . . . , Ytbe independent,\nrandom variables. Let Y=Ít\nj=1Yj, and M>0be such that Yj≤\nE[Yj]+Mfor all j∈[t]. For anyλ≥0,\nPr[Y≥E[Y]+λ]≤exp\u0012\n−λ2\n2(Var[Y]+Mλ/3)\u0013\n. (3)\nConsider a fixed Player iand the first titems arrived in the\ncurrent round. Denote by Xjthe Bernoulli random variable that\nXj=1if the j-th item arrives at player i, and Xj=0otherwise.\nThen E[Xj]=µi,Var[Xj]=µi(1−µi)andXj≤E[Xj]+1for\nj∈[t]. By dentition, the counter ni=Í\nj∈[t]Xj. Fact 1 gives the\nfollowing results:\nLemma 4.2. Given t>0and a failure probability δ>0, define\nU Bi=µit+r\n2tµi(1−µi)lnk\nδ+2\n3lnk\nδ. (4)\nThen the probability Pr[ni≥U Bi]≤δ\nk. Likewise, for\nLBi=µit−r\n2tµi(1−µi)lnk\nδ−2\n3lnk\nδ, (5)\n\nwe have Pr[ni≤LBi]≤δ\nk.\nThe proof of Lemma 4.2 can be found in the Appendix A. By\nLemma 4.2, for a fixed i, by setting the slack si=U Bi, when the\nfirsttitems arrive, the event ni≥sihappens with probability at\nmostδ/k. By union bound, the probability that ni≥sifor any\ni∈[k]is at mostδ. Therefore, the following corollary holds.\nCorollary 4.3. With probability≥1−δ, the coordinator receives\nno notification from the players for the first titems in a round.\nSetting t.Define function f(t)\u0011Í\ni∈[k]si, with si=U Bi(Expres-\nsion (2)). It is easy to verify that f(t)is monotonically increasing\nwith t. Clearly, the larger tthe more items can be tracked in a\nround, while tshould also satisfy: f(t) ≤ Nto ensure the cor-\nrectness of the algorithm. As a result, it is desired to maximise t\nsubject to f(t)≤N. This will possibly reduce the total number\nof rounds in the StcSlk -KwnDst algorithm and hence, reduce the\ntotal communication cost. However, computing the optimal tpre-\ncisely may not be an easy task. Nonetheless, as we show shortly,\nt=N−(√\n2+2/3)q\nkNlnk\nδis already good enough for our pur-\npose. In particular, we have the following lemma whose proof is in\ngiven in Appendix A.\nLemma 4.4. f(t)≤Nfort=N−(√\n2+2/3)q\nkNlnk\nδ.\nBounding the communication cost. Consider an implementa-\ntion of the StcSlk -KwnDst algorithm with its three characteristics\nplug in to the unified framework (Algorithm 2.1), where we further\nexplicitly set β=2·(√\n2+2/3)2. According to this implementation,\nwe know that when N≥β·klnk\nδ, at the end of each round, the\nvalue of Ncan be decreased to at most (√\n2+2/3)q\nkNlnk\nδwith\nprobability 1−δ(by Corollary 4.3 and Lemma 4.4). Furthermore,\nwhen Nis found smaller than β·klnk\nδat the start of a round,\nthe algorithm switches to the CMY algorithm (according to Line\n3 in Algorithm 2.1). Therefore, this gives the following recursion,\nwhere T(N)denotes the communication cost of the algorithm with\nrespect to N.\nT(N)= \nT\u0012\n(√\n2+2/3)q\nkNlnk\nδ\u0013\n+O(k),N≥2(√\n2+2/3)2klnk\nδ\nT(N/2)+O(k), 4k≤N<2(√\n2+2/3)2klnk\nδ\nO(k), N<4k\nSolving the recursion gives the last lemma we need for Theorem 4.1.\nLemma 4.5. T(N)=O(klog logN\nklnk\nδ+klog logk\nδ).\nProving Theorem 4.1. Putting Corollary 4.3, Lemma 4.4 and 4.5\ntogether, it thus completes the proof for Theorem 4.1.\n4.2 Tracking with Dynamic Slacks\nIn the previous subsection, we know that the StcSlk -KwnDst al-\ngorithm assigns to Player ia slack si=U Bi>µit. In expectation,\nµititems arrives at the i-th player. Intuitively, U Bi−µitis the\ntolerance that how much the counter niis allowed to deviate from\nits expectation µit, when titems arrives. As soon as nireaches U Bi,\nPlayer iis not allowed to further receive any new items (to ensurethe correctness). Hence, at this moment, the coordinator collects\nthe precise counters and ends the current round.\nWhile the above strategy has been shown to be effective in the\nprevious subsection, setting si=U Biis astatic slack assignment\nstrategy. In the sense that, the tolerance for deviations, i.e., U Bi−µit,\nispre-determined and fixed for each Player i. When the coordinator\nends a round, except for the player sending the notification, all\nother players actually have not fully used up their deviation toler-\nance. An immediate question comes up: Can we further improve\nthe utilization of those non-fully-used deviation tolerances, before\nending a round?\nMotivated by the question, we design a new slack assignment\nstrategy to dynamically adjust the deviation tolerance for the play-\ners. The basic idea is as follows. First, observe that the sum of all\nthe deviation tolerance of the players is computed as\nkÕ\ni=1(U Bi−µit)≤N−t.\nInstead of pre-assigning a static tolerance to each player, we adopt\nthe strategy of the CMY algorithm. More specifically, the coor-\ndinator sends a base value bi=µit, and a deviation tolerance\nsi=⌊N−t\n2k⌋to Player i, for∀i∈[k]. A player sends a notifica-\ntion to the coordinator for every counter increment sionly when\nni≥bi. The coordinator collects the counters and ends the round\nwhen it receives the k-th notification. The resulted algorithm is\ncalled DynSlk -KwnDst ; since a round is ended when the coordi-\nnator receives knotifications, the DynSlk -KwnDst algorithm is a\nk-Notifications-to-End algorithm, according to our unified frame-\nwork.\nIn particular, DynSlk -KwnDst implements the three characteris-\ntics as follows:\n•Characteristic 1: the slack is a pair(bi,si)for∀i∈[k], where\nbi=µitandsi=⌊N−t\n2k⌋;\n•Characteristic 2: Player isends a notification to the coordinator\nfor every counter increment sionly when ni≥bi;\n•Characteristic 3: ℓ=k.\nSubstituting the above implementations to the algorithm frame-\nwork (Algorithm 2.1), gives the pseudo code of the DynSlk -KwnDst\nalgorithm.\nAs strategy of CMY can guarantee that the coordinator will\nnot miss the arrival of the (N−t)-th item, it thus guarantees that\nno more than Nitems can be received in a round. Therefore, the\ncorrectness of the DynSlk -KwnDst algorithm follows. Furthermore,\nthe theorem below shows that with probability at least 1−δ, at the\nend of a round, at least titems arrive to the players.\nTheorem 4.6. With probability at least 1−δ, less than knotifica-\ntions will be sent from the players for the first titems in a round.\nBy Theorem 4.6, Lemmas 4.4 and 4.5, we have:\nTheorem 4.7. TheDynSlk -KwnDst algorithm achieves exactly\nthe same bounds of StcSlk -KwnDst as stated in Theorem 4.1.\n5 LEARNING BASED TRACKING\nIn this section, we consider the case that the underlying counter\nincrement distribution is unknown . The basic idea is to learn the\n\ndistribution on the fly. To learn the unknown distribution, we run\ntheCMY algorithm for the first round . This allows us to receive\nat least N/2items with only O(k)communication. At the end of\nthis first round, we estimate µiby¯µi=niÍ\ni∈[k]nifor∀i∈ [k].\nThese ¯µi’s are used in the subsequent rounds to determine the\nslacks. As ¯µi’s are just estimations, they may introduce additional\nerrors. Furthermore, since ¯µicould be an underestimation of µi,\nthe upper bound U Bicomputed by simply replacing µiwith ¯µiin\nExpression (4)may be no longer a proper upper bound for niwhen\nthe first titems arrive. Therefore, modifications to the previous\nalgorithms are required.\nThe modifications consist of three steps. First we construct some\nˆµibased on ¯µisuch that it is guaranteed that ˆµi≥µi. Next, we\nshow how to construct U Biwith ˆµi. Last, tneeds to be change to\nensureÍ\ni∈[k]U Bi≤N.\nUpper bound on µi.The concentration inequality below is needed.\nFact 2. (Empirical Bernstein Bound) [ 2] Let Y1, ...,Ywbe inde-\npendent, random variables with mean µ. Let ¯Y=1\nwÍ\nj∈[w]Yj, and\nM>0be such that|Yj|≤Mfor all j∈[w]. With probability at most\nδ, it holds that\n|¯Y−µ|≥s\n2¯σ2ln3\nδ\nw+3Mln3\nδ\nw\nwhere ¯σ2is the empirical variance of Yj’s:¯σ2=1/wÍ\nj∈[w](Yj−¯Y)2.\nConsider a fixed i∈[k]and the number of items nithat arrive\nat Player i, for the witems tracked in the first round. Denote Yj\nthe Bernoulli random variable such that Yj=1if the j-th item\narrives at Player i, and Yj=0otherwise. Then Yj≤1forj∈[w].\nDenote ¯µi=¯Y=1/wÍ\nj∈[w]Yjas the empirical mean. As Yj’s are\nBernoulli random variables, the empirical variance is ¯σ2=¯µi(1−¯µi).\nAccording to Fact 2, a upper bound ˆµionµican be obtained:\nˆµi\u0011¯µi+s\n2(¯µi−(¯µi)2)ln3\nδ\nw+3 ln3\nδ\nw(6)\nModification on U Bi.The slack si=U Biis modified as below,\nU Bi=ˆµit+r\n2tˆµilnk\nδ+2\n3lnk\nδ(7)\nModification on t.We need to change the value of t. Define ˆΣ=Í\ni∈[k]ˆµi. Then we set\nt=N/ˆΣ−(√\n2+2\n3)r\nk(N/ˆΣ)lnk\nδ(8)\nSubstituting the modified ˆui,U Biandtto the known-distribution\ncounterparts, we can have the learning based versions for tracking\nwith static slack (called StcSlk -LrnDst ) and with dynamic slacks\n(called DynSlk -LrnDst ) respectively. Some extra care is required to\nset the constant β=2·(2√\n2+2/3+3)2in the condition of when\nto switch to the CMY algorithm (at Line 3 in Algorithm 2.1) in the\nframework. Moreover, we show our final theorem whose proof can\nbe found in Appendix A.\nTheorem 5.1. With probability at least 1−δ, the communication\ncost of the StcSlk -LrnDst algorithm (respectively, the DynSlk -LrnDst\nalgorithm) is bounded by O(klog logN\nklnk\nδ+klog logk\nδ)).Name Threshold (N) #Players (k)\nWorldCup Day 30/60/90 3.4 M/48 M/1.8 M 8/29/2\nDartmouth 1st Oct/Nov/Dec 184 K/254 K/297 K 336/348/319\nUber Feb/Apr/June 2.2 M/2.2 M/2.8 M 262/262/262\nUniform 210−2242−256\nGaussian 210−2242−256\nZipfian 210−2242−256\nExponential 210−2242−256\nTable 2: Dataset Characteristics ( M=106and K=103)\n6 EXPERIMENTAL EVALUATION\nThis section evaluates the proposed algorithms against the state-\nof-art competitors on a machine running on Ubuntu 18.04 with\nIntel(R) Core(TM) i7-8665U CPU @1.90 GHz and 16GB memory.\nWe compare our four algorithms: StcSlk -KwnDst ,StcSlk -LrnDst ,\nDynSlk -KwnDst ,DynSlk -LrnDst with CMY andUniSlk . All the al-\ngorithms are implemented by C++ and compiled with gcc 7.4.0.\nA backup heuristic is implemented such that, when the empirical\ndistribution is not stable, our algorithms can detect this case and\nswitch to CMY . The details can be found in Appendix B. We con-\nduct experiments on three real datasets and four synthetic datasets;\nthe meta data are summarised in Table 2.\nReal Datasets. Below are the three real datasets we used. In each\nof the real datasets, we assign a unique id to each player (randomly\nand uniquely) in[0,k), where kis the number of players in the\ncorresponding dataset.\nWorld Cup HTTP request data1. The dataset consists of 92 days’\nrequests to the 1998 World Cup website servers between April 30,\n1998 and July 26, 1998. We use the requests of three representative\ndays, namely the 30-th, the 60-th and the 90-th day, as the datasets\nin our experiment. On each selected day, the players are the servers\nthat have received at least one request and the threshold to track\nis the number of requests on that day. Each item is a request that\narrives at some server, in ascending order according to its time\nstamp.\nDartmouth Campus Snmp Traceset2[13]. The dataset contains\npolling records of access points (AP) at Dartmouth College by Sim-\nple Network Management Protocol (SNMP) in Fall 2001. We use\nthe records in three days, i.e., 1st Oct, 1st Nov, and 1st Dec. Each\nAP is reviewed as a player and the number of polling records on\nthe selected day as the threshold. Each polling record is an item\narriving in ascending order by its time stamp.\nUber Pickups3. This dataset contains data on over the Uber pickups\nin New York City from January to June, 2015. Each record has a\npickup time, a pickup location id and some other information. We\ntake the pickups in three months, i.e., February, April and June as\ndatasets. We consider the locations as players and the threshold to\nreport is the number of pickups within the corresponding month.\nEach pickup record is treated as an item arriving in ascending order\nby the pickup time.\n1ftp://ita.ee.lbl.gov/html/contrib/WorldCup.html\n2https://crawdad.org/dartmouth/campus/20090909/snmp\n3https://www.kaggle.com/fivethirtyeight/uber-pickups-in-new-york-city$#$uber-\nraw-data-janjune-15.csv\n\nCMY StcSlk-KwnDst StcSlk-LrnDst UniSlk DynSlk-KwnDst DynSlk-LrnDst\n0 7 14 21 28 35106\n104\n102\n100\n0 30 60 90 120 150108\n106\n104\n102\n100\n0 2 4 6 8 10106\n104\n102\n100\n(a)WorldCup Day 30 (b)WorldCup Day 60 (c)WorldCup Day 90\nFigure 2: The Untracked Item Percentage v.s. Communication Cost ( 10x) on WorldCup datasets\n0 2 4 6 8 10105\n104\n103\n102\n101\n100\n0 2 4 6 8 10105\n104\n103\n102\n101\n100\n0 2 4 6 8 10105\n104\n103\n102\n101\n100\n(a)Dartmouth 1st Oct (b)Dartmouth 1st Nov (c)Dartmouth 1st Dec\nFigure 3: The Untracked Item Percentage v.s. Communication Cost ( 103x) on Dartmouth datasets\n0 2 4 6 8 10106\n104\n102\n100\n0 2 4 6 8 10106\n104\n102\n100\n0 2 4 6 8 10106\n104\n102\n100\n(a)Uber Feb (b)Uber Apr (c)Uber June\nFigure 4: The Untracked Item Percentage v.s. Communication Cost ( 103x) on Uber datasets\nSynthetic Datasets. The data in the synthetic datasets are gener-\nated with various distributions. Specifically, the distributions are:\nUniform, Gaussian, Zipfian and Exponential. The number kof the\nplayers varies from 2to256(with multiplicative factor 2) and the\nthreshold Nranges from 210to224. Moreover, for each generated\nvalue x, ifxis not[0,k), then we just simply discard it; on the other\nhand, if xis not an integer, we take its floor, i.e., ⌊x⌋to round it\ninto a player id. In particular, the parameters of each of distribution\nare as follows:\n•Uniform : the id of the player for each item is generated uni-\nformly at random in [0,k).\n•Gaussian : we set the mean to k/2and standard variance to k/6;\n•Zipfian : each item arrives at player iwith probability propor-\ntional4to1√\ni+1;\n4By proportional we mean here: the probability is normalized subject to the condition\nthat the sum of the probabilities corresponding to the players is 1.•Exponential : each item has probability proportional to exp(−i)\narriving at the i-th player.\nParameter Settings. As the combination of all the parameters and\nthe distributions is considerably large, we set the default values\nofNto220and of kto16. When varying a parameter, the other\nis set to its default value. Furthermore, the StcSlk -KwnDst and\nDynSlk -KwnDst require to know the concrete distribution of the\ndatasets as an input, which may be unavailable for real datasets.\nThus, we use frequencies of each players as its multinormial distri-\nbution. Finally, we set the failure probability to 1%, which suffices\nfor most of the applications in practice.\n6.1 Results on the Real Datasets\nFigure 2-4 illustrate the results of the algorithms on three real\ndatasets. All plots in the figures refer to the percentage of untracked\nitems as a function of the number of used communications. Each\n\nplot represents a round. Different algorithms require different num-\nber of communications for a round. Some plots are truncated for\nUniSlk because it takes much more communications that others.\nThe figures show several results. First all algorithms,\nStcSlk -KwnDst and DynSlk -KwnDst perform the best, with\nDynSlk -KwnDst slightly better in most cases. This is as expected\nbecause they know the frequency information and have more\nknowledge than the other algorithms. It confirms the effectiveness\nof our strategy. Compared to CMY , the number of communications\nreduces by 25%(Figure 3 (b)) to 75%(Figure 2 (c)).\nSecond, the performances of StcSlk -LrnDst and DynSlk -LrnDst\nare inferior to StcSlk -KwnDst /DynSlk -KwnDst but better than\nCMY in general. Compared to the StcSlk -KwnDst /StcSlk -KwnDst ,\nthey don’t know the item arrival frequency at each player in the\ndatasets and therefore have less information. They run CMY for\nthe first round to learn an approximate distribution of the dataset.\nTherefore, their performance in the first round is exactly the same\nasCMY in the first round. The learned distribution helps in tracking\nthe incoming items in most cases. Compared to CMY , we observe\nmuch sharper decreases in the curves from the second round.\nThird, both DynSlk -KwnDst and DynSlk -LrnDst exhibit better\nperformance than StcSlk -KwnDst and StcSlk -LrnDst . As the real\ndatasets do not necessarily have perfect distribution, incorporating\nCMY to handle counters’ deviation from their expectation values\nin an aggregate and dynamic manner is more stable than using\nmerely predetermined and static slacks. The only exception is the\ndataset WorldCup Day 90 (Figure 2 (c)), in which there are only two\nplayers and the distribution is rather skew and stable. Therefore,\nStcSlk -KwnDst and StcSlk -LrnDst win with static slacks.\nThe figures also show the effectiveness of our backup mechanism\n(as described in Appendix B) when the distribution of real dataset is\nnot stable. In Dartmouth datasets, the distribution is rather unstable\n–StcSlk -KwnDst andStcSlk -LrnDst fail in the second round and\ntrack much fewer percentage of items in the second round than\nthe first one. Detecting the degeneracy in efficiency, they switch\ntoCMY in the third round. StcSlk -LrnDst lose only by a marginal\namount to CMY even in this case.\nFinally, the UniSlk algorithm gives the worst performance as its\ntime complexity grows quadratically with respect to k, the number\nof players. Further, it is sensitive to skew distortions. In WorldCup\nDay 60 (Figure 2 (b)), UniSlk exhibits frequent termination of rounds\nas the tailing items comes in a very unbalanced manner. All other\nalgorithms have switched to CMY and handle the tailing items\nsmoothly.\n6.2 Results on the Synthetic Datasets\nSensitivity to Distribution. Figure 5 shows the efficiency of the\nalgorithms under various distributions. It plots the percentage of un-\ntracked items as a function of the number of communications. Each\nplot represents one round. When the datasets are generated from\nsome distribution, our algorithms perform consistently better than\ntheCMY algorithms, regardless of the distribution. The communi-\ncation is reduced by 33%to80%. Furthermore, our algorithms with\nstatic slacks perform better than their CMY counterparts. When\nthe distribution is stable, the static slacks capture more accurately\nthe number of items a player will receive.Sensitivity to Threshold. Figure 6 plots the number of commu-\nnications as a function of the N(the threshold) under the various\ndistributions, with the number of players fixed to default value 16.\nWe truncate the plots with cost more than 103.UniSlk performs\nreally good on Uniform distribution as each player receives roughly\nthe same number of items. But its communication cost blows up\non other datasets. It does not show up in the plot for Exponential\ndistribution because it uses more than 103communication even\nforN=1k. Moreover, the figures shows an increasing efficiency\ngain of our algorithms compared to CMY , asNincreases. This\nis consistent with our analysis of their communication complex-\nity. While StcSlk -KwnDst offers the best performance over all the\ndatasets, DynSlk -KwnDst ,StcSlk -LrnDst andDynSlk -LrnDst yield\ncomparable performance.\nSensitivity to Number of Players. Figure 7 plots the number\nof communications as a function of the number of players under\nthe various distributions, with the threshold fixed to default value\n1m. We truncate the result for UniSlk when its communication\nexceeds 104. The figures illustrate a shrinking gap in the number\nof communications between CMY and the our algorithms as the\nnumber of players increases. This complies with our theoretical\nanalysis as the ratio of the communication complexity between\nthe two is given by O((klogN\nk)/(klog logN\nklnk\nδ+klog logk\nδ))=\nO((logN\nk)/(log logN\nklnk\nδ+log logk\nδ)). When Nandδare fixed,\nthe ratio decreases as kincreases. The only exception is the dataset\nwith Exponential distribution and with 2 players, in which the\nCMY perform very well. In such case the number of items the first\nplayer receives is roughly exp(1)times that of the second player. A\nround terminates after player one sending two notifications to the\ncoordinator (the slack size is s=n1/2). On the other hand, player\ntwo is assigned the same slack s=n1/2but receives n1/exp(1)\nitems. Only a small fraction of the slack is wasted in this case.\n7 CONCLUSION\nThe paper exploits the counter increment distribution and presents\nfour data-dependent algorithms that utilize knowledge on the\ndata distribution. All our algorithms have communication cost\nO(klog logN\nklnk\nδ+klog logk\nδ), whereδis a parameter controls the\nfailure probability, improving the state-of-the-art O(klogN\nk)bound.\nIn addition, our algorithms are equipped with backup mechanism\nthat guarantees comparable performance as the data-independent\nCMY algorithm when the distribution fluctuates. We experimentally\nevaluate our algorithms against the state-of-the-art competitors,\nusing both real and synthetic datasets. Our experimental results\nshow the efficiency and robustness of our algorithms.\nACKNOWLEDGMENTS\nJunhao Gan is supported by Australian Research Council (ARC)\nDECRA DE190101118.\nREFERENCES\n[1]Anders Aamand, Piotr Indyk, and Ali Vakilian. 2019. (Learned) Frequency Es-\ntimation Algorithms under Zipfian Distribution. CoRR abs/1908.05198 (2019).\narXiv:1908.05198\n\nCMY StcSlk-KwnDst StcSlk-LrnDst UniSlk DynSlk-KwnDst DynSlk-LrnDst\n0 2 4 6 8 10106\n104\n102\n100\n0 2 4 6 8 10106\n104\n102\n100\n0 2 4 6 8 10106\n104\n102\n100\n0 2 4 6 8 10106\n104\n102\n100\n(a)Uniform (b)Gaussian (c)Zipfian (d)Exponential\nFigure 5: The Untracked Item Percentage v.s. Communication Cost ( 100x) on synthetic datasets\n10 12 14 16 18 20 22 24102103\n10 12 14 16 18 20 22 24102103\n10 12 14 16 18 20 22 24102103\n10 12 14 16 18 20 22 24102103\n(a)Uniform (b)Gaussian (c)Zipfian (d)Exponential\nFigure 6: Communication Cost v.s. N(2x) on synthetic datasets\n2 4 816 32 64128 256101102103104\n2 4 816 32 64128 256101102103104\n2 4 816 32 64128 256101102103104105\n2 4 816 32 64128 256101102103104105106\n(a)Uniform (b)Gaussian (c)Zipfian (d)Exponential\nFigure 7: Communication Cost v.s. kon synthetic datasets\n[2]Jean-Yves Audibert, Rémi Munos, and Csaba Szepesvári. 2009. Exploration-\nexploitation tradeoff using variance estimates in multi-armed bandits. Theor.\nComput. Sci. 410, 19 (2009), 1876–1902.\n[3]Irwan Bello, Hieu Pham, Quoc V. Le, Mohammad Norouzi, and Samy Bengio.\n2017. Neural Combinatorial Optimization with Reinforcement Learning. In 5th\nInternational Conference on Learning Representations, ICLR 2017, Toulon, France,\nApril 24-26, 2017, Workshop Track Proceedings .\n[4]Fan R. K. Chung and Lincoln Lu. 2006. Survey: Concentration Inequalities and\nMartingale Inequalities: A Survey. Internet Mathematics 3, 1 (2006), 79–127.\n[5]Graham Cormode. 2013. The continuous distributed monitoring model. SIGMOD\nRecord 42, 1 (2013), 5–14.\n[6]Graham Cormode, Minos N. Garofalakis, S. Muthukrishnan, and Rajeev Rastogi.\n2005. Holistic Aggregates in a Networked World: Distributed Tracking of Ap-\nproximate Quantiles. In Proceedings of the ACM SIGMOD International Conference\non Management of Data, Baltimore, Maryland, USA, June 14-16, 2005 . 25–36.\n[7]Graham Cormode, S. Muthukrishnan, and Ke Yi. 2011. Algorithms for distributed\nfunctional monitoring. ACM Trans. Algorithms 7, 2 (2011), 21:1–21:20.\n[8]Nikos Giatrakos, Antonios Deligiannakis, Minos N. Garofalakis, Izchak Sharfman,\nand Assaf Schuster. 2012. Prediction-based geometric monitoring over distributed\ndata streams. In Proceedings of the International Conference on Management of\nData, SIGMOD, Scottsdale, AZ, USA, May 20-24, 2012 . 265–276.\n[9]Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. 2019. Learning-Based\nFrequency Estimation Algorithms. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .\n[10] Zengfeng Huang, Ke Yi, and Qin Zhang. 2019. Randomized Algorithms for\nTracking Distributed Count, Frequencies, and Ranks. Algorithmica 81, 6 (2019),2222–2243.\n[11] Ram Keralapura, Graham Cormode, and Jeyashankher Ramamirtham. 2006.\nCommunication-efficient distributed monitoring of thresholded counts. In Pro-\nceedings of the ACM SIGMOD International Conference on Management of Data,\nChicago, Illinois, USA, June 27-29, 2006 . 289–300.\n[12] Elias B. Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. 2017. Learn-\ning Combinatorial Optimization Algorithms over Graphs. In Advances in Neural\nInformation Processing Systems 30: Annual Conference on Neural Information Pro-\ncessing Systems 2017, 4-9 December 2017, Long Beach, CA, USA . 6348–6358.\n[13] David Kotz, Tristan Henderson, Ilya Abyzov, and Jihwang Yeo. 2009. CRAWDAD\ndataset dartmouth/campus (v. 2009-09-09).\n[14] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In Proceedings of the 2018 International\nConference on Management of Data, SIGMOD 2018 . 489–504.\n[15] Michael Mitzenmacher. 2018. A Model for Learned Bloom Filters and Optimizing\nby Sandwiching. In Advances in Neural Information Processing Systems 31: Annual\nConference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8\nDecember 2018, Montréal, Canada . 462–471.\n[16] Miao Qiao, Junhao Gan, and Yufei Tao. 2016. Range Thresholding on Streams. In\nProceedings of the 2016 International Conference on Management of Data, SIGMOD\nConference 2016, San Francisco, CA, USA, June 26 - July 01, 2016 . 571–582.\n[17] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer Networks. In\nAnnual Conference on Neural Information Processing Systems (NeuIPS), December\n7-12, 2015, Montreal, Quebec, Canada . 2692–2700.\n\nA PROOFS OF LEMMAS AND THEOREMS\nProof of Lemma 4.2 . Denote X=ni=Í\nj∈[t]Xj. AsE[Xj]=µi,\nby linearity of expectation and independence, we have E[X]=tµi.\nFurther, as the Xj’s are independent and Var[Xj]=σ2=µi(1−µi),\nVar[X]=tσ2. Applying Fact 1 with Yj=Xj,M=1and setting\nthe failure probability toδ\nk, we have\nPr[X−tµi≥λ]≤exp\u0012\n−λ2\n2(tσ2+λ/3)\u0013\n=δ\nk\nIt follows that\nλ2−\u00122\n3lnk\nδ\u0013\nλ−2tσ2lnk\nδ=0\nSolving the quadratic equation gives\nλ=1\n2 \n2\n3lnk\nδ+r\n4\n9ln2k\nδ+8tσ2lnk\nδ!\n≤2\n3lnk\nδ+r\n2tσ2lnk\nδ\nTherefore U Bi\u0011tµi+2\n3lnk\nδ+q\n2tσ2lnk\nδ≥tµi+λ, and we have\nPr[X≥U Bi]≤Pr[X≥tµi+λ]≤δ\nk\nSimilarly, if we take Zj=−Xj, then E[Zj]=−µiandVar[Zj]=\nVar[−Xj]=σ2. Let Z=Í\nj∈[t]Zj, then we have\nPr\u0002\nZ≥−tµi+r\n2tσ2lnk\nδ+2\n3lnk\nδ\u0003\n≤δ\nk\nwhich is equivalent to\nPr\u0002\nX≤tµi−r\n2tσ2lnk\nδ−2\n3lnk\nδ\u0003\n≤δ\nk\nThis finishes the proof.\n□\nProof of Lemma 4.4 . Recall that si=µit+q\n2tµi(1−µi)lnk\nδ+\n2\n3lnk\nδ. Summing over all players i∈[k], we get\nf(t)=kÕ\ni=1µit+kÕ\ni=1r\n2tµi(1−µi)lnk\nδ+kÕ\ni=12\n3lnk\nδ\nThe first and third terms sum up to tand2k\n3lnk\nδrespectively. It\nis left to bound the second term. We utilize the concavity of the\nsquare root function√·and obtain\nkÕ\ni=11\nkr\n2tµi(1−µi)lnk\nδ≤vutkÕ\ni=11\nk\u0012\n2tµi(1−µi)lnk\nδ\u0013\n(9)\n≤r\n2t\nklnk\nδ(10)\nThe second inequality follows from (1−µi)≤1for∀i∈[k]andÍk\ni=1µi=1. Therefore,\nf(t)≤t+r\n2ktlnk\nδ+2k\n3lnk\nδ(11)Further, when the StcSlk -KwnDst does not run the CMY algorithm,\nit holds that klnk\nδ≤N. Hence2k\n3lnk\nδ≤2\n3q\nkNlnk\nδ. Combining\nthatt≤N, we have\nf(t)≤t+r\n2kNlnk\nδ+2\n3r\nkNlnk\nδ(12)\nIt suffices to take t=N−(√\n2+2/3)q\nkNlnk\nδto ensure that\nf(t)≤N□.\nProof of Lemma 4.5. The claim is trivial true when N∈(0,4k).\nIf4k≤N≤2(√\n2+2/3)2klnk\nδ, then\nT(N)=T(N/2)+O(k)=T(4k)+O(klogN\n4k)=O(klogN\nk)\nSince N≤2(√\n2+2/3)2klnk\nδ, we have T(N)=O(klog logk\nδ).\nFinally, if N>2(√\n2+2/3)2klnk\nδ, rewrite the following numbers\nas a power of two:\n(√\n2+2/3)2klnk\nδ=2d1,N=2d2\nfor positive numbers d1=log\u0010\n(√\n2+2/3)2klnk\nδ\u0011\nandd2=logN.\nDefine S(d2)=T(2d2)=T(N)=T\u0012\n(√\n2+2/3)q\nkNlnk\nδ\u0013\n+O(k).\nThen\nS(d2)=S((d1+d2)/2)+O(k)\n=S(d1+(d2−d1)/2)+O(k)\n=S(d1+(d2−d1)/4)+2·O(k)\n=...\n=S(d1+(d2−d1)/2log(d2−d1))+log(d2−d1)·O(k)\nBy the definition of S(·), we have S(d1+(d2−d1)/2log(d2−d1))=\nS(d1+1)=T(2d1·2)=T\u0010\n2(√\n2+2/3)2klnk\nδ\u0011\n. Moreover, log(d2−\nd1)=log log 2d2−d1=log logN\n(√\n2+2/3)2klnk\nδ. Therefore,\nT(N)=T\u0012\n2(√\n2+2/3)2klnk\nδ\u0013\n+O(klog logN\nklnk\nδ)\nThe former term equals to O(klog logk\nδ).\n□\nProof of Theorem 4.6 . Denote by nithe number of items received\nby player iand by Ithe set of players with ni>µit(i.e., the set of\nplayers that may send notifications to the coordinator). Our goal is\nto prove that players in Isend less than knotifications5:\nÕ\ni∈Ini−µit\n(N−t)/2k<k\nFirst notice that by Lemma 4.2, with probability at least 1−δ, we\nhave for all i∈[k]\n|ni−µit|≤r\n2tµi(1−µi)lnk\nδ+2\n3lnk\nδ\n5Without loss of generality, we assume thatN−t\n2kis always an integer and thus, we\ncan get rid of the floor operation. This is because otherwise, one can always use at\nmost O(k)straightforward communications to reduce N−tto a multiple of 2k. The\ncommunication bound will not be affected.\n\nby similar argument as Inequality (9-10), we have\nÕ\ni∈[k]|ni−µit|≤r\n2ktlnk\nδ+2k\n3lnk\nδ(13)\nOn the other hand, we haveÍk\ni=1ni=t, hence\nÕ\ni∈I(ni−µit)=Õ\ni∈[k]\\I(µit−ni)\nIt follows that\nÕ\ni∈[k]|ni−µit|=2Õ\ni∈I(ni−µit) (14)\nCombining Inequality (13) and Equality (14), we have\nÕ\ni∈Ini−µit\n(N−t)/2k≤1\n2q\n2ktlnk\nδ+2k\n3lnk\nδ\n(N−t)/2k<k\nThe last inequality can be simplified to t+q\n2ktlnk\nδ+2k\n3lnk\nδ<\nN, which holds for t=N−(√\n2+2/3)q\nkNlnk\nδ, as proven in\nLemma 4.4.\n□\nProof of Theorem 5.1. Denote N0the threshold to track when the\nalgorithm begins and let wbe the items tracked by CMY in the first\nround. As CMY tracks at least half the threshold in one round, it\nholds that w≥N0/2. After the first round, the threshold to track is\nN←N0−w. Therefore, w≥Nholds in all the subsequent rounds.\nFor any subsequent round that runs our customized tracking\nalgorithm, we are going to prove that: (i) it captures at least titems,\nwhere tis defined by Expression (8); (ii)Í\ni∈[k]U Bi≤N; (iii)\nt=N−O(q\nkNlogk\nδ), in order to construct a similar recursion as\nthe one that Lemma 4.5 solves. The theorem is proven by the same\ntechniques used by Lemma 4.5.\nFirst, when the first titems arrives, by Equation (4), with proba-\nbility 1−δ, we have\nni≤µit+r\n2tµi(1−µi)lnk\nδ+2\n3lnk\nδ\nfor all i∈[k]. Observing that(1−µi)≤1andµi<ˆµi, we get\nni≤ˆµit+r\n2tˆµilnk\nδ+2\n3lnk\nδ\nAs a result, U Bi=ˆµit+q\n2tˆµilnk\nδ+2\n3lnk\nδdefined by Expres-\nsion (7) is indeed an upper bound of ni.\nSecond, it remains to verify that the these U Bisatisfy that correct-\nness constraint (Inequality (1)). Recall thatÍ\ni∈[k]ˆµi=ˆΣ. Summing\nover i∈[k], we have\nkÕ\ni=1U Bi=tˆΣ+kÕ\ni=1r\n2tˆµilnk\nδ+kÕ\ni=12\n3lnk\nδ\nThe third term sums up to2\n3klnk\nδ. By concavity of the square root\nfunction, the second term is upper bounded byq\n2ktˆΣlnk\nδ. Now, bythe definition of t=N/ˆΣ−(√\n2+2\n3)q\nk(N/ˆΣ)lnk\nδin Expression (8),\nwe have\nkÕ\ni=1U Bi≤N−(√\n2+2\n3)r\nkNˆΣlnk\nδ+r\n2kNlnk\nδ+2\n3klnk\nδ\nwhich concludes thatÍk\ni=1U Bi≤NasN>klnk\nδwhen this\nround runs our customized algorithm.\nFinally, we need to show that t=N−O(q\nkNlogk\nδ). It suffices\nto show that N/ˆΣ=N−O(q\nkNlogk\nδ). By substituting ˆµiwith\nExpression (6), and by a similar argument as in Inequality (9-10),\nwe get\nˆΣ≤(1+s\n2kln3\nδ\nw+3kln3\nδ\nw)\nTherefore,\nN/ˆΣ≥N/(1+s\n2kln3\nδ\nw+3kln3\nδ\nw)\nDefineд(x)=r\n2kln3\nδ\nx+3kln3\nδ\nx. Byw≥N, it holdsд(w)≤д(N).\nHence N(1+д(w))(1−д(N))≤ N(1−д(N)2)≤NandN/ˆΣ≥\nN/(1+д(w))≥ N(1−д(N)). Thus, the proof is completed.\n□\nB IMPLEMENTATION OPTIMISATIONS\nDetecting non-stable distribution. In general, the performance\nof a data-dependent algorithm may degenerate, when the empirical\ndata does not follow the underlying distribution well. This is also the\ncase for our algorithms. To remedy this issue, we propose a simple\nheuristic to detect whether the current empirical data still follows\na distribution well. If it does not, we switch to the CMY algorithm\nright away to minimize the impact of performance degeneration.\nThe heuristic works as follows.\nDenote by Nthe threshold to track at the start of the current\nround and N′the one at the start of the next round. In other words,\nN−N′items have been tracked in the current round. Intuitively,\nthe ratio ofN−N′\nNserves as an indicator of the effectiveness of\nthe algorithm. If the ratio drops below a pre-specified threshold\n(say 0.75), we switch the algorithm to the CMY algorithm. Such a\nmechanism guarantees that as soon as the empirical distribution is\ndetected to be unstable, our algorithm will lose its effectiveness in\nat most one round, compared to the CMY algorithm. And therefore,\nat most O(k)communication can be wasted and performance is still\nupper bounded by the communication bound of the CMY algorithm.",
  "textLength": 54946
}