{
  "paperId": "638766b215dc7b2bd946a0e69e8914a9bbb04ae7",
  "title": "The Price of Tailoring the Index to Your Data: Poisoning Attacks on Learned Index Structures",
  "pdfPath": "638766b215dc7b2bd946a0e69e8914a9bbb04ae7.pdf",
  "text": "ThePrice ofTailoringthe Index to Your Data:\nPoisoning A/t_tacksonLearned Index Structures\nEvgenios M.Kornaropoulos\nGeorgeMason University,USA\nevgenios@gmu.eduSileiRen\nCornellUniversity,USA\nsr2262@cornell.eduRobertoTamassia\nBrownUniversity,USA\nroberto@tamassia.net\nABSTRACT\nTheconceptof learnedindexstructures reliesontheideathatthe\ninput-outputfunctionalityofadatabaseindexcanbeviewedasa\npredictiontaskand,thus,implementedusingamachinelearning\nmodel instead of traditional algorithmic techniques. This novel\nangle for a decades-old problem has inspired exciting results at\nthe intersection of machine learning and data structures. However,\nthe advantage of learned index structures, i.e., the ability to adjust\nto the data at hand via the underlying ML-model, can become a\ndisadvantagefrom asecurity perspective as itcould be exploited.\nInthiswork,wepresentthe/f_irststudyofdatapoisoningattacks\non learned index structures. Our poisoning approach is diﬀerent\nfromallpreviousworkssincethemodelunderattackistrainedona\ncumulativedistributionfunction(CDF)and,thus,everyinjectionon\nthetrainingsethasacascadingimpactonmultipledatavalues.We\nformulatethe/f_irstpoisoningattacksonlinearregressionmodels\ntrainedonaCDF,whichisabasicbuildingblockoftheproposed\nlearnedindex structures. We generalize our poisoningtechniques\nto attack the advanced two-stage design of learned index struc-\nturescalledrecursivemodelindex(RMI),whichhasbeenshownto\noutperform traditional B-Trees. We evaluate our attacks under a\nvarietyofparameterizationsofthe modelandshowthat the error\noftheRMIincreasesupto300 ×andtheerrorofitssecond-stage\nmodels increasesupto3000 ×.\nCCS CONCEPTS\n•Information systems →Data structures ; •Security and\nprivacy→Cryptanalysis and other attacks ; •Computing\nmethodologies→Machine learning approaches .\nKEYWORDS\nLearnedSystems, Data Poisoning,Attacks,Indexing\n1 INTRODUCTION\nDatabase systems rely on index structures to access stored data\neﬃciently.Itisknowntothedatabasecommunitythatthemotto\n“onesize/f_itsall”doesnotapplytotraditionalindexingschemes[ 24]\nsince each index provides diﬀerent performance guarantees that\ndependontheaccesspattern,thenatureoftheworkload,andthe\nunderlying hardware. Even after choosing an appropriate index\nstructure for a speci/f_ic application, it is usually the case that a\ndatabaseadministratorhastomanually/f_ine-tunetheparametersof\nthe system, either through experience or with help from tools. The\nworkbyKraska,Beutel,Chi,Dean,andPolyzotis[ 30]challenged\nthe state of aﬀairs by re-framing index structures as a machine\nlearningproblem where the index directs a query to a memory\nlocation(s) basedonatrainedmodeltailoredonthe data at hand.Learned Index Structures. The core idea of a learned index struc-\nture(LIS)istomodeladatastructureasa predictiontask ,i.e.,getan\ninputkeyandpredictitslocationinasortedsequenceofkey-record\npairs.Thisapproachallowstheuseof( /u1D456)continuousfunctionsto\nencode the data, and ( /u1D456/u1D456)learning algorithms to approximate the\nfunction.The speci/f_icLISapproach proposed byKraska et al.[30]\nistobuildthe cumulativedistributionfunction (CDF)forthekeys.\nGivenakey, /u1D458,theCDFreturnstheprobabilitythatakeychosen\naccording to this distribution takes value less than or equal to /u1D458.\nSince the above probability is built from the set of keys at hand,\nit is expressed as the ratio of the number of keys less than /u1D458to\nthetotalnumberofkeys.Giventhisinsight,onecanusetheCDF\nto (i) compute the number of keys less than the (queried) key /u1D458,\nand (ii)infer the key’s memory location assumingthe keyswere\nsortedduringtheinitialization.Therefore,asimplelinearregres-\nsion onthe CDFgivesan approximate locationof the queriedkey.\nIndeed a linear regression on the CDF is one of the building blocks\nthat hasbeenshowntoworkwell [ 30] andcanbe combinedwith\nhierarchicalmodels ,alsocalledrecursivemodelindex(RMI)struc-\ntures,soastobalancethe/f_inalmodelforlatency,memoryusage,\nand computational cost. The hierarchy can be seen as building a\nmixtureof“experts”[ 39]responsibleforsubsetsofthedata.The\nnotion of a LIS has spurred a surge of works that blend ideas from\nmachinelearning,datastructures,andsystems(e.g.,[ 5,7,10,12–\n14,17–21,23,24,29,31,35,36,41,43,45,47–49,52,55,56,58,59]).\nFirstVulnerabilityAssessmentofLearnedIndex. Aspromising\nas it may sound to combine ideas from machine learning and data\nstructures, no analysis has been performed to understand potential\nvulnerabilitiesof theLIS paradigm.Intuitively,the advantageof a\nLIS is that the model adapts to the data at hand. However this eﬃ-\nciencymightbeproblematiciftheadversaryiscapableofinjecting\nmaliciously crafted data before the training of the model, i.e., at the\ninitialization stage of the index structure, so as to cause inaccurate\npredictions ofthe location of legitimate data.\nThe technique of data poisoning has been known to be an eﬀec-\ntive attack vector for over a decade, e.g., see the references in [ 26].\nInthecontextofstaticindexstructures,wefocusonthecasewhere\nthedatastoredintheindexcomesfrommultiplesourcesasdiﬀerent\nentities directly or indirectly contribute data, e.g., by generating\ndata with their actions or behavior. A malicious actor can tailor its\ncontributed data to deteriorate the index performance. Indeed,the\nreal-worlddatasetsusedintheoriginalLISwork[ 30]comefrom\nmultiplecontributorsand,thus,aresusceptibletopoisoningattacks.\nOtherexamplesofindexeddata generatedbymultiplesourcesin-\nclude data from personalized medicine,where patients voluntarily\ncontribute their own data, as well as cybersecurity analytics where\nany user can submit itsown indicators of compromise. Our threat\nmodel,muchlikeallpoisoningworks[ 3,4,26,60,61],assumesthat\n\nthe adversary has the ability to contribute maliciously crafted data\nandhas access to the legitimate (training) data.\nWe analyze this blind spot by addressing the following question:\nIsitpossibleto degradetheperformance oflearned\nindex structures bypoisoningitslearned components?\nA /f_irst attempt to answer this question is to directly deploy pre-\nvious poisoning attacks in this new CDF setting. Unfortunately,\nnone of the proposed poisoning attacks applies to models based\non a CDF. This is because all previous attacks operate under the\nassumptionthatanypoint (/u1D44B,/u1D44C)chosenbytheadversaryisavalid\npoisoning point, i.e., /u1D44B-,/u1D44C-value can take any arbitrary number.\nOn the contrary, when dealing with a point (/u1D44B,/u1D44C)from a CDF, the\ndimension/u1D44Cdependsonthe /u1D44B-valuesofthe entiredataset since\n/u1D44Cdescribestherankofitsassociated key.Thus, /u1D44Ccanonlytakea\nsinglevaluegivenan /u1D44B-value.ThisnewandrestrictiveCDFsetting\nforbidstheuseofanypreviousattacksandsignalsthatweneeda\nnewmethodology for addressing data poisoningattacksonCDFs.\nOur Contributions. The purpose of this work is to ascertain\nwhethertheparadigmoflearnedsystemsisvulnerabletopoisoning\nattacks.Towardsthis end we make the following contributions:\n•Weformulatetheproblemof single-pointpoisoning alinearre-\ngression model on the cumulative distribution function (CDF).\nWe propose a poisoning attack algorithm on CDFs that /f_inds\nanear-optimalpoisoningkeyandhascomplexitylineartothe\nnumber oflegitimate keys.\n•Weproposea multiple-pointpoisoning attackonlinearregression\non CDF. We follow a greedy approach based on our eﬃcient\nsingle-pointattack.Weconductextensiveexperimentsundera\nvarietyofscenariostostudytheimpactofdiﬀerentparameteri-\nzations. Our experiments show that the eﬀect of our multi-point\npoisoning attack can increase the mean squared error (typical\nlossfunctionofregression)upto100 ×.To/f_ixthisaccuracyerror,\nthesystemmayhavetoaccessupto10 ×morememorylocations.\n•Weproposeapoisoningattackforthetwo-stagerecursivemodel\nindex (RMI) architecture that was shown to outperform [ 30] the\nhighly-optimized traditional B-Tree data structure. Our attack\ntargets the linear regression models of the second stage of the\nRMI.Weevaluatetheattackinuniformlyandhighlyskewedkey\ndistributionsunderavarietyofRMImodels.Wealsoapplythe\nattack on real data from (i) the salaries of employees in Dade\nCounty in Miami [ 38], and (ii) publicly available geolocation\ndataset [44]. Our experiments show that the poisoning attack\ncanincreasetheerroroftheRMImodelupto300 ×andtheerror\nofan individualsecond-stage modelupto3000 ×.\nWe hope our /f_indings will motivate a systematic exploration of the\nrobustness of learned systems by researchers in data management,\nmachine learning,security,andsystems.\n2 RELATED WORK\nSincethe/f_irstworkonlearnedindexstructures[ 30]thecommu-\nnity has explored several interesting research directions [ 19,58].\nIn terms of dynamizing the LIS framework , the work by Hadian\nand Heinis [ 18] proposes a method to minimize the error caused\nby updating the index. The /f_irst family of updatable LIS is called\nALEX [7]. The work by Tang et al.[52] studies the case where\nthedistribution of the workload evolves as queries are issued andproposes to re-train the model to account for such a dynamic envi-\nronment. Another recent interesting line of work proposes learned\nmulti-dimensional structures[ 8,35,43].P/a.sc/v.sc/o.sc[59]takesanunsu-\npervised neural network approach, for locating the position of the\nkey value in the index. The combination of ML and data structures\nfrom [30] has inspired new ML-based approaches for traditionally\nalgorithmicproblemssuchaslow-rankdecomposition[25],space\npartitioning fornearest neighbor search[ 9],frequencyestimation\nin data streams [ 21]. The work by Setiawan et al.[47] observes\nthat the underlying process of LIS can be seen as a /f_itting problem,\nasopposed to a learning problem.The authorsdeploy polynomial\ninterpolation techniques to approximate the location of a key. The\nPGM /i.sc/n.sc/d.sc/e.sc/x.sc [15] proposes a model that adopts an RMI design that\nauto-tunesoverspaceandlatencyrequirementsandsupportsup-\ndates.BasedthePGMindex, R/a.sc/d.sc/i.sc/x.scS/p.sc/l.sc/i.sc/n.sc/e.sc [28]proposesaRMIthat\nfeaturesanalternatelinearinterpolationbasedindexing.RadixS-\npline is eﬃcient to build and tune, but falls back to tree-structured\nradixtable onheavily skeweddataset.\nSystemdesignsarealsoinspiredbytheworkin[ 30].S/a.sc/g.sc/e.scDB[29]\nis a database that adapts toan applicationthrough codesynthesis\nand ML.D/e.sc/c.sc/i.sc/m.sc/a.sc[36] deploys reinforcement learning and ML to\nlearn workload-speci/f_icscheduling algorithms.A lineof works in\nbenchmarking[ 27,37]isalsoproposedtoshowtheeﬀectiveness\nofLearnedIndex Structures over traditional structures.\nFor completeness we note that other works considered a similar\napproach [2, 23, 33, 53, 54].\nKnown Poisoning Attacks. The line of work for poisoning at-\ntacks[4,60,61] considers adversaries that deliberately augment\nthe training data to manipulate the results of the predictive model.\nThe work by Biggio et al.[3] inserts maliciously crafted training\ndatatochangethedecisionfunctionoftheSVMand,thus,increase\nthe test error.Yang et al.[61] propose gradient-basedmethodsfor\ngenerating poisoning points for Neural Networks. Suciu et al.[50]\nproposeaframeworkforevaluating realisticadversaries thatcon-\nduct poisoning attacks on machine learning algorithms. Finally,\nthe work that is closest to ours is the results by Jagielski et al.[26].\nTheauthors proposeanoptimizationframeworkfor poisoningat-\ntacks on linear regression as well as a defense mechanism called\nT/r.sc/i.sc/m.sc. However, we emphasize that our attacks diﬀer signi/f_icantly\nfrom[26]sinceweaddressthecaseofpoisoning CDFsforwhich\nevery insertionaﬀectsthe valuesof all points ofthe dataset .\n3 PRELIMINARIES\nTerminology. We denote a key by /u1D458and its key universe as K,\nwhere|K|=/u1D45A.Thesetofallkeysofanindexisdenotedas /u1D43E⊆K.\nThe set of keys /u1D43Ehas size/u1D45Band contains no multiplicities. The\ndensityof a keyset /u1D43Eis the ratio|/u1D43E|/|K|=/u1D45B//u1D45A. For simplicity,\nweassumethatkeysarenon-negativeintegers,therefore,wecan\nalwaysderivethetotalorderofthekeyset,muchlike[ 30].Eachkey\nis associated with a record; the records are stored at an in-memory\ndensearray that issortedwithrespectto the key values.\n3.1 Background on LearnedIndexStructures\nTheworkbyKraska etal.[30]proposesalternativeML-basedimple-\nmentation for index structures such as range indexes , traditionally\nimplementedbyB-trees, pointindexes ,traditionallyimplemented\n2\n\nbyHashMap,and existenceindexes ,traditionallyimplementedby\nBloom /f_ilters. Learned Index Structures (LIS) are based on a sim-\npleyetpowerfulobservationthatlocatingakey /u1D458withinasetof\nlinearly ordered keys can be reduced to approximating the prob-\nability that a random key would take value less or equal than /u1D458,\ni.e.,Pr(/u1D44B≤/u1D458)=R/a.sc/n.sc/k.sc(/u1D458)//u1D45B, where/u1D44Bis the random variable that\nfollows the empirical distribution of the /u1D45Bkeys and R/a.sc/n.sc/k.sc(·)is a\nfunctionthattakesakeyasaninputandoutputsitsrelativeposition\namongthe/u1D45Bkeys.Thisprobabilityiscapturedbythecumulative\ndistributionfunction(CDF),thus,thetaskoflocatingakeyboils\ndown to learning the CDF of the sorted key set . We consider the\nnon-normalizedCDF,therefore,inaCDFplotthe /u1D44B-axisrepresents\nthe key values and the /u1D44C-axis the rank of the key, see Figure 1. For\ngeneralizingtocomplexdistributionstheauthorsproposethe Re-\ncursiveModelIndex (RMI),amulti-stagearchitecturewheremodels\non a higher stage direct the query to models on a lower stage to\n/f_ine-tunethe precision ofthe predictedmemory location.\nFigure 1: An illustration of the Recursive Model Index (RMI)\nwithatwo-stagearchitecture.The/f_irststageisasingleneural\nnetwork model while the second stage is series of linear\nregressionmodelson1-out-of- /u1D441keypartitionsofequalsize.\nTheevaluationin[ 30]showsthatthefollowingRMIarchitecture\noutperforms B-Treesinspeed andmemory utilization: a two-stage\nRMIarchitecturewith treestructure betweenmodelsandapartition\nof non-overlappingkeyset of equal sizeassignedtomodels onthe\nleaves. For the root model, or /f_irst-stage model, the authors deploy\naneuralnetworkmodel thatcancapturethecoarse-grainedshapeof\ncomplex functions. The authors propose the use of a large number\nof leaves, i.e., fan-out in the order of thousands, each of which\npredicts the /f_inal memory location of the queried key. Each leaf\nrepresents a second-stage model that performs linear regression on\ntheCDFofasubsetofkeys,i.e., afast and storage-eﬃcientmodel.\nForsimplicity,weassume/f_ixed-lengthrecordsandlogicalpaging\nover acontinuous memory region.\nPrediction Error. If the prediction is not accurate then a local\nsearchis called which discovers the record around the (falsely)\npredictedlocation.Weemphasizeherethatifthepredictionerroris\nlarge, then the design needs to correct the error by performing an\nextended local search which translates into accessing more memory\nlocationwhichslowsdownthe performance ofthe overalldesign.3.2 Background on Poisoning Attacks\nLet/u1D437={/u1D465/u1D456,/u1D466.alt/u1D456}/u1D45B\n/u1D456=1denotethedatausedbyalearningmodel,where\nthe feature vector is /u1D465∈R/u1D451and the response variable is /u1D466.alt∈R.\nIn the linear regression model, the output is computed via a lin-\near function /u1D453(/u1D465,/u1D464,/u1D44F)=/u1D464/u1D447/u1D465+/u1D44Fwith parameters /u1D464∈R/u1D451and\n/u1D44F∈R.Theparameters /u1D464,/u1D44Fare chosensoasto minimizethe loss\nfunctionL(/u1D437,/u1D464,/u1D44F)=1\n/u1D45B/summationtext.1/u1D45B\n/u1D456=1(/u1D453(/u1D465/u1D456,/u1D464,/u1D44F)−/u1D466.alt/u1D456)2which is the mean\nsquarederror(MSE).Wenotethatregressiondiﬀersfromclassi/f_i-\ncationsince theoutputisanumericalvalue,asopposedtoaclass\nlabel.Poisoningattackisdescribedasabileveloptimizationprob-\nlem:giventhetargetmodelasthe/f_irst-levelminimizationofthe\nerror function, the attacker aims to /f_ind new data that maximize\nthe outcome ofthe /f_irst-level minimization. Previous worksfocus\nongradient-based poisoningattacks. Someworksproposealterna-\ntivesto analyticallysolvingthebileveloptimization problem,e.g.,\nsampling-based approach [26] andgenerative method [61].\nOursingle-pointpoisoningattackinSection4.3takesadiﬀerent\napproachby exploitingthestructureofCDFs andcomputestheloca-\ntion of the poisoning point that maximizes the minimum error in a\nsingle pass over the legitimate keys. The multiple-point poisoning\nattack and the attack on RMI are extensions of our optimal and\neﬃcient poisoningapproach.\n3.3 ThreatModel\nAttacker’sGoal. Oneofthemainbene/f_itsofLISisthattheyare\nsigni/f_icantly faster [ 30] than traditional index structures. In this\nwork we focus on adversaries that insert maliciously crafted in-\nputs to the set of keys of the LIS so as to corrupt the model and\ndegrade its overall performance. The reason for such adversarial\nbehavior depends on the adversarial gains and application context,\ne.g.,competitorthatwantstodeteriorateperformanceoradenialof\nserviceattack.Allrecentpointersshowthatdeployingtechnologies\nthat are susceptible to adversarial manipulations is a dangerous\napproach with hardto predict consequences, e.g.,extracting faces\nfrom models [ 16], corrupting autopilot models in cars [ 1]. In our\ncase,theattacker’sobjectiveistogeneratemaliciouslychosentrain-\ningkeys,called poisoningkeys ,thattogetherwiththe legitimatekeys\nwilltrainanLISthathaspredictionaccuracy lowercomparedtoan\nLIS trained only on legitimate keys. Poisoning attacks can be cate-\ngorizedinto poisoningintegrityattacks andpoisoningavailability\nattacks. Integrity attacks form a loss function over speci/f_ic data-\npoints of interest and, therefore,aim for a targeted mis-prediction\nonthisdata.Availabilityattacks,whichisthefocusofthiswork,\naim to indiscriminatelydeterioratethe performance.\nAttacker’s Knowledge. We focus on white-box attacks where\nthe attacker has access to the training data, i.e., keyset /u1D43E, and\nthe parameters of the model, i.e, the /u1D464and/u1D44Fvalues for linear\nregression models. White-box attacks are a standard setting in\nrobustness/privacyanalysisofmachinelearningmodels[ 3,4,26,\n32,50,51,60,61].Speci/f_ically,themajorityofacademicworkinLIS\nusepubliclyavailabledatasets,e.g.,[ 15,17,28,30,43],whichmakes\nsuch a white-box attack plausible. In practice, given the training\ndata, an attacker can derive the parameters of the linear regression\nmodels by building the index locally before the computation of\nthe poisoning keys. The scenario in which the attacker does not\nhave accessto thedata ortheparametersofthe model, iscalled a\n3\n\nblack-box attackanditisoutsidethescopeofthiswork.Wenote\nhere that even if the exact training data is unknown, an attacker\ntypicallyhasinformationaboutthedistributionwhichallowsthe\nconstructionofa substitutedataset .Aftergeneratingasubstitute\ndataset, an attacker can take advantage of the transferability of\npoisoning,whichisshowntoworkwell[ 60],andmountablack-\nbox attack.\nAttackerCapabilities. Weassumetheattackercaninsert /u1D45Dma-\nliciouslycraftedpoisoningkeysbeforethetrainingofthemodel.\nPoisoningattacks have been showntocorrupt the modelin diﬀer-\nentscenariosoutsideLIS,e.g.,recommendationsystems[ 22]and\ncrowdsourcingsystems[ 11].Wedenotethesetof /u1D45Dpoisoningkeys\nas/u1D443andtheoverall poisoned keysetas/u1D43E∪/u1D443,whichcontainsthe\nsum of legitimate and poisoning keys, i.e., /u1D45B+/u1D45D. We callpoisoning\npercentage theterm100·(/u1D45D//u1D45B).Wefollowthefootstepsofprevious\nworksinpoisoning[ 3,26]andinvestigatetheeﬀectofpoisoning\nfor up to 20%poisoning percentage. Jumping ahead, in our experi-\nmentswedemonstratetheperformanceforarangeofpoisoning\npercentages and we observe that our attack is eﬀective on small\npoisoningpercentagesas well.\nAttackEvaluationMetric. TheevaluationoftheLISperformance\nintheoriginalworkbyKraska etal.[30]isperformedbymeasuring\nthelookuptime(innanoseconds).The/f_inalbenchmarknumbersin\ntheirworkaretheresultofcustomcode,designedforsmallmodels,\nwhich removes all unnecessary overhead and instrumentation that\nTensor/f_lowintroducesinlargermodels.Unfortunately,theresult\nof their engineering eﬀorts are not publicly available and, there-\nfore, we can not directly measure the eﬀect of poisoning in LIS\nwith respect to the time performance. In our work we evaluate the\nperformance of our attacks with two implementation-independent\nmetrics: (1) we computethe RatioLoss ,which istheratio between\nthe mean square error (MSE) function of the poisoned dataset and\ntheMSEofthenon-poisoneddataset;(2)wecomputethe Average\nMemoryOﬀset ,whichistheaveragememorydiﬀerencebetween\nthe model prediction and true position of the associated record (in\nthe unitofthe memory size ofakey-valuepair).\n4 POISONINGREGRESSIONMODELSON CDF\nIn theparadigm oflearned index structures(LIS) [ 30]thelocation\nofakey-recordpairiscomputedby approximatingtherelativeorder ,\ni.e., the rank, of the queried key. An accurate approximation of the\nrank allows the algorithm to jump directly to the desired memory\nlocation of the linearly ordered key-record pairs without touching\nthe rest of the data in the index. In this section we propose attacks\nforpoisoningthelinearregressionmodelonCDF ,abuildingblock\nforRMI.Theproposedattackinsertspointsintheindexwiththe\ngoal of increasing the approximation error of the regression and as\naresultdegrade thetimeperformance oftheoverall design .\nANewFlavorofPoisoning. Foranindexwithkeyset /u1D43Eofsize/u1D45B,\neachkey/u1D458∈/u1D43Ehasarank/u1D45Fintheinterval[1,/u1D45B],whichcorresponds\ntoitspositioninthesortedsequenceof /u1D43E.LISapproximatestherank\nofakeybyalinearregressionmodelonthepair (/u1D458,/u1D45F),wherethe/u1D44B-\nvalueisthekeyandthe /u1D44C-valueisitsrank.Themodelapproximates\nthe non-normalizedcumulative distributionfunction.\nInthetraditionalpoisoningattacksonregressionmodels[ 26],\ni.e.,notonCDFfunctions,theinsertionofapoisoningpointcausesonly a “local” change since it does not aﬀect the/u1D44B- and/u1D44C-values\nof any of the legitimate points. On the contrary, for the case of\nLIS, the insertion of a single key /u1D458/u1D45Dchanges the rank of all the\nlegitimate keys larger than /u1D458/u1D45D. Consequently, this “global” change\non the rank of the legitimate keys triggers a change on the CDF\nitself. This compound eﬀect of an adversarial insertion has not been\nanalyzed before. Another diﬀerence is that the rank of a chosen\npoisoning key /u1D458/u1D45Dis automatically implied by the other keys of\nthedataset.Thus,theattackercannotpickarbitrary /u1D44B-,/u1D44C-value\nassignments which is a major departure from the setting of all\npreviouspoisoningattacks.Inthiswork,weintroduceanew/f_lavor\nofpoisoningattacksonlearnedindex structures.\nBefore addressing the general LIS design, which is comprised\nofatwo-stagearchitecture,we/f_irstposeafundamentalquestion\nthatfocusesonthe new /f_lavorofpoisoning:What is the optimal\npoisoningstrategythatmaximizestheerroroflinearregressionap-\npliedonaCDF?Weanswerthisquestionbydevelopinganeﬃcient\nattackthat maximizesthe errorof alinearregressiononaCDF.\nTherestofthissectionisorganizedasfollows.Section4.1formal-\nizes the problem statement. In Section 4.2, we provide an intuitive\nexplanation of our attack strategy. A detailed description of our\nattack method and analysis of its performance and complexity are\ngiven in Sections 4.3–4.4. Finally, Section 4.5 contains an experi-\nmental evaluation ofthe practical eﬀectiveness of our attack.\n4.1 ProblemStatement\nWe /f_irst de/f_ine the linear regression framework on cumulative\ndensityfunctions.Thefollowingde/f_initionbridgesthenotionsof\nranks andtheircorresponding CDF.\nD/e.sc/f.sc/i.sc/n.sc/i.sc/t.sc/i.sc/o.sc/n.sc 1 ( L/i.sc/n.sc/e.sc/a.sc/r.sc R/e.sc/g.sc/r.sc/e.sc/s.sc/s.sc/i.sc/o.sc/n.sc /o.sc/n.sc CDF/s.sc ).Let/u1D43E=\n{/u1D4581,···,/u1D458/u1D45B}⊆Kbethesetofintegersthatcorrespondtothe\nkeys of the index. Every key /u1D458/u1D456∈/u1D43Ehas its associated rank /u1D45F/u1D456∈\n[1,/u1D45B]. Thelinear regression model on a CDF computes a pair\nof regression parameters (/u1D464,/u1D44F)that minimizes the following\nmean squared error(MSE) function:\nmin\n/u1D464,/u1D44FL/parenleftbig{/u1D458/u1D456,/u1D45F/u1D456}/u1D45B\n/u1D456=1,/u1D464,/u1D44F/parenrightbig=min\n/u1D464,/u1D44F/parenleftBig/summationdisplay.1/u1D45B\n/u1D456=1(/u1D464/u1D458/u1D456+/u1D44F−/u1D45F/u1D456)2/parenrightBig\n.\nIn this work we focus on non-regularized linear regression , much\nlike the original work on LIS by Kraska et al.[30]. The goal of\nregularizationistogeneralizethemodelonunseen(testing)data;\nin LIS the majority of queries are expected to be data stored in\nthe index structure, i.e., training data. Therefore, the impact of\nregularizationisunclear inthe contextof LIS.\nWe can derive a closed-form solution to the minimizationprob-\nlem of De/f_inition 1. Notice that the set /u1D43Ecan be interpreted as a\nsamplefromthesetofkeys K.Giventhisprobabilisticpointofview,\nwe de/f_ine the sample mean of the key set as /u1D440/u1D43Eand the sample\nmeanoftheranksetas /u1D440/u1D445.Wede/f_inethe samplevariance asVar/u1D43E\nandVar/u1D445, and the sample covariance between/u1D43Eand/u1D445asCov/u1D43E/u1D445.\nLastly,thesamplemeanofthesquaresofthekeys,resp.ranks,is\nde/f_inedas/u1D440/u1D43E2,resp./u1D440/u1D4452.Recallthattheformulasofvarianceand\ncovariance are, Cov /u1D44B/u1D44C=/u1D440/u1D44B/u1D44C−/u1D440/u1D44B/u1D440/u1D44C,Var/u1D44B=/u1D440/u1D44B2−/u1D4402\n/u1D44B.\n4\n\nT/h.sc/e.sc/o.sc/r.sc/e.sc/m.sc 1. [ 42]The Linear Regression from De/f_inition 1 admits\nthefollowingclosed-formsolution:\n/u1D464∗=Cov/u1D43E/u1D445\nVar/u1D43E, /u1D44F∗=/u1D440/u1D445−/u1D464∗/u1D440/u1D43E,L(/u1D43E,/u1D445,/u1D464∗,/u1D44F∗)=−Cov2\n/u1D43E/u1D445\nVar/u1D445+Var/u1D43E.\nTheadversarialgoalofthenewlyintroducedpoisoningforlinear\nregressiononCDFisdescribedinthe following.\nD/e.sc/f.sc/i.sc/n.sc/i.sc/t.sc/i.sc/o.sc/n.sc2( P/o.sc/i.sc/s.sc/o.sc/n.sc/i.sc/n.sc/g.scL/i.sc/n.sc/e.sc/a.sc/r.scR/e.sc/g.sc/r.sc/e.sc/s.sc/s.sc/i.sc/o.sc/n.sc/o.sc/n.scCDF ).\nLet/u1D43Ebe the set of /u1D45Bintegers that correspond to the keys and\nlet/u1D443be the set of /u1D45Dintegers that comprise the poisoning points.\nThe augmented set on which the linear regression model is\ntrained is{(/u1D458′\n1,/u1D45F′\n1),(/u1D458′\n2,/u1D45F′\n2),···,(/u1D458′\n/u1D45B′,/u1D45F′\n/u1D45B′)}, where/u1D458′\n/u1D456∈/u1D43E∪/u1D443\nand/u1D45F′\n/u1D456∈[1,/u1D45B+/u1D45D]. The goal of the adversary is to choose a set\n/u1D443of size at most /u1D706so as to maximize the loss function of the\naugmented set /u1D43E∪/u1D443which is equivalent to solving the bilevel\noptimizationproblem:\nargmax/u1D443s.t.|/u1D443|≤/u1D706/parenleftbigg\nmin\n/u1D464,/u1D44FL/parenleftBig\n{/u1D458′\n/u1D456,/u1D45F′\n/u1D456}/u1D45B+/u1D45D\n/u1D456=1,/u1D464,/u1D44F/parenrightBig/parenrightbigg\nThe upper bound /u1D706in the size of /u1D443is chosen to be proportional\nto the size of the keyset, e.g., /u1D706=0.2/u1D45B. We de/f_ine the notions of\nnumbersequence anddiscretederivative [40]as:anumbersequence\n/u1D434is an ordered list of numbers and we denote with /u1D434(/u1D456)the/u1D456-th\nnumber inthe sequence.\nD/e.sc/f.sc/i.sc/n.sc/i.sc/t.sc/i.sc/o.sc/n.sc3. [DiscreteDerivative ][40]Adiscretederivativeof\nasequence/u1D434isde/f_inedasthediﬀerencebetweenconsecutivenumbers\nin the sequence /u1D434. We denote the sequenceof discrete derivatives of /u1D434\nasΔ/u1D434.Formally:\nΔ/u1D434(/u1D456)=/u1D434(/u1D456+1)−/u1D434(/u1D456)\n4.2The Compound Eﬀect ofPoisoning CDF\nFor thesake of illustration we use anaive brute forceapproach to\npoisonakeysetwith /u1D45B=10keys.Figure2-(A)showstheoriginal\nindex key set on the /u1D44B-axis and the corresponding ranks on the /u1D44C-\naxis,whileFigure2-(B)showstheregressionlineafterthepoisoning.\nThe blue vertical lines indicate the distance of the point from the\nregressionline ,i.e.,theerrorincurredbythiskey.Therefore,onecan\nillustratetheMSEbysummingthesquareoftheabovedistances.\nThekey/u1D458/u1D45D,coloredinred,istheoptimalpoisoning location. Due\ntothecompoundeﬀectofaninsertionontheCDF,theranks,i.e.\nthe/u1D44C-value,ofthe pointsafter /u1D458/u1D45Dincreasebyone.\nIn a typical setting of poisoning regression models, the addition\nof a single point has a limited overall impact since all the other\npoints stay in their original /u1D44B-,/u1D44C-coordinates. On the contrary,\nfor the case of CDFs the addition of a single point can aﬀect the\nrank, i.e.,/u1D44C-coordinates, of many original points of the CDF. In\nother words, a single point might force the regression line to incur\nacompounderror fromalargeportionoftheoriginalpoints.The\nexample of Figure 2 demonstrates this phenomenon, notice that\ntheerror-contributionbythemajorityofthekeys,depictedwith\nvertical blue segments issigni/f_icantly larger insub/f_igure (B).\nWe now illustrate why previous poisoning attacks fail in the\nCDFsetting.Ifweweretofeedthis10-keydatasettoapoisoning\nalgorithm that neglects the fact that /u1D44C-dimension describes a CDF,\nthentheoptimalpoisoningpointwouldbe (40,1).Noticethough0 4 8 12 16 20 24 28 32 36 40\nKeys12345678910RankRegression Before Poisoning\nOriginal Regression\n0 4 8 12 16 20 24 28 32 36 40\nKeys12345678910 RankRegression After Poisoning\nOriginal Regression\nPoisoned Regression\n(A) (B)\nFigure 2: Illustration of the compound eﬀect of poisoning\nusingasinglekey /u1D458/u1D45Dcoloredinred.Alloriginalkeysthatare\nlargerthan /u1D458/u1D45Dincreasetheirrankbyone.Thenewregression\nline,dottedredline,accumulateslargererrorfrommostof\ntheoriginalpoints due to the adjustmentofranks.\nthatthepoisoningkey /u1D458=40wouldbeassignedtherank11,and\nnot1,sinceitisthegreatestkey.Therefore,theoutputsofprevious\npoisoningattackscan not dealwiththe characteristics of aCDF.\n4.3 Single-Point Poisoning on CDF\nThis subsection proposes an eﬃcientattack for a single poisoning\npoint that has strong evidence of being optimal. To avoid insert-\ningout-of-rangekeysorintroducingoutliers(bothpoisoningap-\nproachesaredetectedandeliminatedbyknownmitigations),we\ndeem as potential poisoning keys the ones that liein-between the\nsmallest andthe largestlegitimate key.\nA First Attempt. Since the key space is /f_inite, we are guaranteed\nto/f_indtheoptimalpoisoningkeyifwecomputetheupdatedloss\nfunctionforeverypotentialpoisoningkey.Foreachpoisoning,it\ntakes/u1D442(/u1D45B)time to compute the loss from scratch, where /u1D45Bis the\nnumberofexistingkeys.Sincethereare /u1D45A−/u1D45Bpossiblelocationsthe\ntime complexity of this approach is /u1D442(/u1D45A/u1D45B). For large indexes with\nmillions of keys and a large keyspace this approach is not practical.\nOurApproach. Theeﬃciencyandoptimalityofourpoisoningattack\nisbasedonthe following observations aboutthe problem:\n1) The loss function Lcan be seen as a sequence . For a\n/f_ixed keyset /u1D43E, the loss function after poisoning boils down to a\nfunctionthatonlydependsonthelocationofthepoisoningkey /u1D458/u1D45D.\nTherefore,onecanseethelossfunction Lasasequencedenotedas\n/u1D43Fwhereitsindex representsthelocationofthepoisoningkey /u1D458/u1D45D,\nand its output, denoted as /u1D43F(/u1D458/u1D45D), represents the MSE if we were to\nchoose/u1D458/u1D45Dasthepoisoningkey.Sincemultiplicitiesarenotallowed\nwe getthe following expressionfor the sequence:\n/u1D43F(/u1D458/u1D45D)= \nmin/u1D464,/u1D44F/parenleftBigg\n/summationtext.1\n/u1D458′∈/u1D43E∪/u1D458/u1D45D(/u1D464/u1D458′+/u1D44F−/u1D45F′)2/parenrightBigg\n,if/u1D458/u1D45D∉/u1D43E\n⊥ ,if/u1D458/u1D45D∈/u1D43E(1)\nLikewise,themeanofkeys /u1D440/u1D43E(/u1D458/u1D45D)isasequence whereeachvalue\nis the mean of the poisoned keyset with respect to the chosen\npoisoning key. The mean of the new ranks /u1D440/u1D445(/u1D458/u1D45D), the variance\nof the poisoned keys Var/u1D43E(/u1D458/u1D45D), and the covariance between the\npoisonedkeysandthenewranks Cov/u1D43E/u1D445(/u1D458/u1D45D)areallsequenceswith\nthe poisoningkey as their index.\n5\n\n2)Thevalueof /u1D43F(/u1D458/u1D45D)canbere-usedfor /u1D43F(/u1D458/u1D45D+1).Inthe“/f_irst\nattempt”, one computes the new loss function on each potential\npoisoningkeybyprocessingtheentiredataset,i.e.,timecomplexity\n/u1D442(/u1D45B)foreveryevaluation.Ourinsightisthatwecanpaythelinear\ncostonce, i.e., /u1D442(/u1D45B)forthe/f_irstpotentialpoisoningkey, andthen\ncompute the new loss function for the next poisoning key in constant\ntime.Tocomputethelossforconsecutivekeysweusethenotionof\ndiscretederivative ofthesequence /u1D43F(/u1D458/u1D45D),denotedas Δ/u1D43F(/u1D458/u1D45D).Using\ntheexpressionsfortheoptimalparametersfromTheorem1wecan\ndirectly compute the new loss function with optimal parameters\nfortheupdatedregressionmodel .Thediscretegradientsofthese-\nquencesarecomputedinconstanttime.Therefore,wecancompute\nthevalueoftheentiresequenceforthelossfunctionin /u1D442(/u1D45A+/u1D45B)\ntime,as opposedto /u1D442(/u1D45A/u1D45B)ofthe “/f_irst attempt”.\n3)Lossfunctionisthecompositionofconvexsubsequences.\nAsidentical keysarenot allowedintheindex, thedomainofthe\nentirelosssequenceiscomprisedof subsequences .Thedomainof\neachsubsequenceconsistsofconsecutivepoisoningkeys.Theexist-\ning keys/u1D43Edivide the key space into at most (/u1D45B−1)subsequences.\nThe rank of a potential poisoning key is the same within each\nsubsequence. We conjecture that each subsequence is convex with\nrespectto the evaluation ofthe lossfunction.\nC/o.sc/n.sc/j.sc/e.sc/c.sc/t.sc/u.sc/r.sc/e.sc 1. Let/u1D43Ebe the set of original keys. Let /u1D43F(/u1D458/u1D45D)be the\nsequencewhere forinput /u1D458/u1D45Ditoutputs thevalueof theloss function\non a linear regression model trained on /u1D43E∪/u1D458/u1D45D. The loss sequence\n/u1D43F(/u1D458/u1D45D)isconvexonthedomainde/f_inedbetweeneachconsecutivepair\nofkeyvalues /u1D458/u1D456and/u1D458/u1D456+1from/u1D43E.\nOurconjectureisbasedonthefollowingobservation:theconvex-\nityonlossfunction /u1D43F(/u1D458/u1D45D)isaresultoftheconvexityof /u1D43F′(/u1D458/u1D45D,/u1D464,/u1D44F)=\n/u1D43F(/u1D458/u1D45D), in which model parameter /u1D464and/u1D44Fare viewed asvariables.\nThelosstermisasummationofaseriesof /u1D45Bquadraticfunctions\non/u1D464and/u1D44Fwith the exception of one term (/u1D464/u1D458/u1D45D+/u1D44F−/u1D45F/u1D45D)2, where\n/u1D464/u1D458/u1D45Disanonlinearvariableterm.However,thevalueofthisterm\nis dominated by the summation of the other /u1D45B−1quadratic terms.\nBesidesthetheoreticalintuition,ineveryexperimentweperformed,\nthelossfunctionwasconvex.Acorollaryoftheaboveconjectureis\nthat,themaximumlossforeachconvexsubsequenceisgiveneither\nby the /f_irst or the last poisoning key, i.e., the endpoints. Therefore,\nwecancomputetheglobalmaximumoftheentirelosssequenceby\naconstant-timecomputationforeachsubsequence whichreducesthe\ntimecomplexityofthecomputationfrom /u1D442(/u1D45A+/u1D45B)to/u1D442(/u1D45B).Figure3\npresentsthelossfunctionacrossthekeyspace;theconvexitycan\nbe seen bythe /f_irstderivative plot ofthis example.\n0 1 02 03 04 0Keys101520Loss FunctionLoss Function\nLoss After Poisoning\nLoss Before Poisoning\n0 1 02 03 04 0\nKeys-1012First Derivative\n of Loss FunctionFirst Derivative\nFigure 3: An illustration of the loss function and its /f_irst\nderivativefromthe keyset ofFigure2.Each subsequenceof\nconsecutive poisoning keysisconvexw.r.t.theloss function.Single-point Poisoning. We put the above observations together\nto de/f_ine the closed-form formulas that compute the evaluation of\nthelossfunctionforthepoisonedsetofkeys.Let (/u1D4581,/u1D45F1),...,(/u1D458/u1D45B,/u1D45F/u1D45B)\nbe the sequence of pairs of keys and ranks for a dataset. The set\nof keys/u1D4581,...,/u1D458/u1D45Bimply a collection of subsequences in the key\ndomain such that each subsequence is comprised of consecutive\nnon-occupied keys .For example,considerthe key-rank pairs:\n(/u1D4581,/u1D45F1),(/u1D4582,/u1D45F2),(/u1D4583,/u1D45F3),(/u1D4584,/u1D45F4)←(2,1),(6,2),(7,3),(12,4).\nThesubsequencesofnon-occupiedkeysforthekeydomain K=[1,13]\nare:{1},{3,4,5},{8,9,10,11},{13}. Due to observation 3) we only\nneedtoconsiderthe endpointsofeachsubsequence ,i.e.,forourexam-\npletheendpointsare {1},{3,5},{8,11},{13}.Wede/f_inesequence /u1D446\nwheretheelement /u1D446(/u1D456)correspondstothe /u1D456-thsmallestkeyamong\nalltheendpointsfromallsubsequences .Forourrunningexamplewe\nhave/u1D446(1)=1,/u1D446(2)=3,/u1D446(3)=5,/u1D446(4)=8,/u1D446(5)=11,/u1D446(6)=13.We\nalso de/f_ine the sequence /u1D447where the element /u1D447(/u1D456)correspondsto\ntherankthatthepoisoningkey /u1D446(/u1D456)wouldtakeifitisinserted.For\nourexamplewehave /u1D447(1)=1,/u1D447(2)=2,/u1D447(4)=2,/u1D447(5)=4,/u1D447(6)=\n4,/u1D447(7)=5. To simplify the notation we index all the subsequences\nwithrespecttotheindex ofsequence /u1D446,i.e.,thenotation /u1D440/u1D43E(/u1D456)is\nequivalentto /u1D440/u1D43E(/u1D458)where/u1D458←/u1D446(/u1D456).\nThe attacker /f_irst calculates the eﬀect of inserting the /f_irst po-\ntential poisoning key /u1D446(1), which implies the calculation of the\nvalues/u1D440/u1D43E(1),/u1D440/u1D43E2(1),/u1D440/u1D43E/u1D445(1),and/u1D43F(1). The eﬀect of inserting\npoisoning key /u1D446(/u1D456+1)on the loss function, i.e., /u1D43F(/u1D456+1), can be\ncomputedinconstant time as:\n/u1D440/u1D43E(/u1D456+1)=/u1D440/u1D43E(/u1D456)+Δ/u1D446(/u1D456)\n/u1D45B+1,/u1D440/u1D43E2(/u1D456+1)=/u1D440/u1D43E2(/u1D456)+(2/u1D446(/u1D456)+Δ/u1D446(/u1D456))Δ/u1D446(/u1D456)\n/u1D45B+1\n/u1D440/u1D445(/u1D456)=/u1D45B+2\n2,/u1D440/u1D4452(/u1D456)=(/u1D45B+2)(2/u1D45B+3)\n6,/u1D440/u1D43E/u1D445(/u1D456+1)=/u1D440/u1D43E/u1D445(/u1D456)+/u1D447(/u1D456)Δ/u1D446(/u1D456)\n(/u1D45B+1)\n/u1D43F(/u1D456+1)=−(/u1D440/u1D43E/u1D445(/u1D456+1)−/u1D440/u1D43E(/u1D456+1)/u1D440/u1D445(/u1D456+1))2\n/u1D440/u1D43E2(/u1D456+1)−(/u1D440/u1D43E(/u1D456+1))2+/u1D440/u1D4452(/u1D456+1)−(/u1D440/u1D445(/u1D456+1))2\nAlgorithm complexity. The algorithm for the single-point poi-\nsoning runs as a subroutine in the Algorithm 1, see Lines 3-10. The\nabove approach maximizes the error of the poisoning for a single\ninsertionin/u1D442(/u1D45B)time complexity. Space complexity is constant for\neach single-point poisoning call. In each iteration, the algorithm\naccesses an item from the endpoint sequence /u1D446and updates the\nmaximalloss.Sinceweonlystoretheendpointkeyvalueofeach\niteration, the memory usage is constant throughout the execution.\nComparison with Previous Poisoning Attacks. There are a\nfew similarities but, more importantly, several diﬀerences between\nour approach and the state of the art result in poisoning for linear\nregression[26]. We elaborate inthe following:\n•Similarities: Both [26] and our work formulate the poisoning\nattack as a bilevel optimization problem. Additionally, both works\ndeploy an iterative methodto derive the poisoningpoints.\n•Diﬀerences: The /f_irst diﬀerence concerns the dimension of the\npoisoningpoints.Speci/f_ically,whenitcomestopoisoningaCDF,\ntherankofeachkeydependsonthevaluesofallthekeysofthein-\ndex.Thus,topoisonaCDF,theattackerchoosesaone-dimensional\npoint. Whereas in [ 26], the attacker chooses a two-dimensional\npoint. We cannot apply the attack from [ 26] to our CDF setting\nbecause their algorithm (falsely) assumes that it can pick arbitrary\n6\n\nranking which creates inconsistent and unusable poisoning points,\ni.e., it chooses both /u1D44B- and/u1D44C-coordinates. The above simple obser-\nvationintroducesaanothertechnicaldiﬃcultythatconcernsthe\ngradient formula . That is, the second diﬀerence is that the gradient\noftheoptimizationproblemin[ 26]simplyrequirestopluginthe\npoisoning(/u1D465,/u1D466.alt)coordinates both of which are chosen by the ad-\nversary.Onthecontrary,intheCDFsetting,inordertocompute\nthe gradient for an arbitrary point the adversary has to access a\n(potentially) large number of keys to calculate the /u1D44C-coordinate\nsinceitcorrespondstotherankofthenewpoisoningpoint.Thus,a\nnaive poisoning for CDFs has to pay linear time for every gradient\ncomputation which is why we devised a tailored amortized anal-\nysisforpoisoningintheCDFsettinginSection4.3.Weachievea\n“constant-time”computationbecausewe iteratethroughpotential\npoisoning points with +/-1 change in ranking, our eﬃcient method\ncannot be applied to arbitrarystepsofaniterative algorithmsuch\nas[26].Finally,asdiscussedinConjecture1,theoptimizationprob-\nlem of poisoning a CDF has a globalmaximum, whereas the attack\nof[26] maygettrappedinalocal optimum.\nAlgorithm1: G/r.sc/e.sc/e.sc/d.sc/y.scP/o.sc/i.sc/s.sc/o.sc/n.sc/i.sc/n.sc/g.scR/e.sc/g.sc/r.sc/e.sc/s.sc/s.sc/i.sc/o.sc/n.scCDF\nData:The number of allowed poisoning keys /u1D45D, the originaldatasetfor the\nregression{(/u1D4581,/u1D45F1),...,(/u1D458/u1D45B,/u1D45F/u1D45B)}where/u1D458/u1D456∈/u1D43Eand/u1D45F/u1D456∈[1,/u1D45B].\nResult:Set of poisoning keys /u1D443such that/u1D443∩/u1D43E=∅and|/u1D443|=/u1D45D.\n1Initialize the set of poisoning keys /u1D443←∅;\n// Follow a greedy approach, choose locally optimal poisoning\n2forevery/u1D457from1 to/u1D45Ddo\n3Partitionthe non-occupied keys, i.e., keysnot in /u1D43E∪/u1D443, into\nsubsequences such that eachsubsequence consistsof consecutive\nnon-occupied keys;\n// Due to convexity, the loss function is maximized at an\nendpoint\n4Extractthe endpoints of eachsubsequence and sort them to construct\nthe newsequence of endpoints /u1D446(/u1D456), where/u1D456≤2(/u1D45B+/u1D457);\n5Computethe rankthat key /u1D446(/u1D456)would haveif it was inserted in /u1D43E∪/u1D443\nand assignthisrankas the /u1D456-th element of the newsequence /u1D447(/u1D456),\nwhere/u1D456≤2(/u1D45B+/u1D457);\n// Evaluate each sequence for the smallest endpoint\n6Compute the eﬀect of choosing /u1D446(1)as a poisoning key and inserting it\nto/u1D43E∪/u1D443with the appropriate rank adjustments. Speci/f_ically, evaluate\nthesequenceseachofwhichisthemean /u1D440foradiﬀerentvariable,e.g.,\n/u1D43E,/u1D445,/u1D43E/u1D445. Compute/u1D440/u1D43E(1),/u1D440/u1D43E2(1),/u1D440/u1D43E/u1D445(1),and/u1D43F(1);\n7forevery/u1D456from2 tothe lengthofsequence /u1D446do\n8 Computethe eﬀect of choosing /u1D446(/u1D456+1)as apoisoning keyby\ncalculating the loss function /u1D43F(/u1D456+1)from the equationsin (4.3);\n9end\n10De/f_ine as/u1D458OPT←/u1D446(argmax/u1D456/u1D43F(/u1D456))the chosenpoisoning keywhich\nmaximizesthe loss;\n11Augment/u1D443as/u1D443←/u1D443∪/u1D458OPT;\n12end\n13returnthe setofpoisoningkeys /u1D443;\n4.4 GreedyMultiple-Point Poisoning on CDF\nWe generalize the single-point approach so as to insert multiple\npoisoning keys. Speci/f_ically, we propose a greedy approach where\nat each iteration the attacker makes a locally optimal decision and\ninsertsthepoisoningkeythatmaximizestheerroroftheaugmented\nkeyset so far, see Algorithm 1. Even though we do not provide a\nproof of optimality for the multiple-point poisoning, we experi-\nmentally observed that our approach matched the performance\nof the brute-force attack in every tested dataset. Intuitively, ourapproach places poisoning keys in a dense area so as to exacerbate\nthenon-linearity oftheCDF andconsequently increasethe error.\n4.5 Evaluation\nIn this subsection we evaluate the eﬀect of greedy multi-point\npoisoning. We observe that the ratio loss increases up to 100 ×\ndepending on the size of the dataset and the domain of the keys.\nBesidesthelossfunction,wealsoobservedthatthepoisonedindex\nneedstoaccesssigni/f_icantlymorememorylocations,i.e.,viathe\ncorrectness process of the so-called local search of LIS. Speci/f_ically,\nwe observed that after poisoning, every query has to access (on\naverage)5%oftheentiredatasettocorrecttheaccuracyerror.That\nis 10×larger number ofaccessesthanthe non-poisoneddataset.\nSetup.Weproducesyntheticdatasetsofkeysthatareuniformly\ndistributed.Wenoteherethattheuniformdistributionhassmall\nMSElossbecauseofthenear-linearCDFduetouniformity.This\nimpliesthatthesedatasetsaretheonesthatthelinearregression\non CDF can capture well and, thus, one of the cases where learned\nindexstructuresoutperformtraditionalmethods.Wechosediﬀerent\nparametersforourexperiments.The/f_irstisthe numberoflegitimate\nkeys(denoted as “Keys”), the second is the densityof the legitimate\nkeys over the key domain. We evaluate on the following /f_ixed\ndensities: 5%, 10%, 50% and 80%. The third, which can be computed\nfrom the /f_irst two, is the size of the key domain , and the fourth\nis thepercentage of poisoning keys with respect to the number\noflegitimatekeys.Followingthefootstepsofpreviouspoisoning\nattacks [26] we only consider poisoning percentage up to 15%. We\nnote that in our experiments we /f_ix the number of keys and the\ndensityand adjustthekeydomain accordingly.Thereason behind\nthis design choice has to do with the architecture of the original\nwork on LIS [ 30]. Speci/f_ically, in the two-level architecture of RMI\nthe model partitions the entire keyspace so that each partition has\na /f_ixed number of keys and as a result the size of the key domain\nvaries between partitions, as opposed to a /f_ixed key domain size\nwith varying numberof keys. Thechosen key domain sizes in our\nexperimentswerepickedsoastofollowtypicaldomainsizeswhere\nthe regression on LIS was performed in the original work [ 30].\nTomeasuretheeﬀectivenessofourmulti-pointpoisoningattack,\nwe record Ratio Loss , which is the ratio of the MSE loss on the\nunionofpoisoningandlegitimatekeys,overtheMSElossonjust\nlegitimate keys. We also record Average Memory Oﬀset , which is\nthe average oﬀset between the predicted location and real location\nof the key. E.g., memory oﬀset 100 means that the local search has\nto gothrough100key-valuepairs to correctthe error.\nResults. The results of our evaluation are presented in Figure 4.\nFora/f_ixednumberoflegitimatekeysanda/f_ixedkeydensity,i.e.,\nfocusing on a single plot of Figure 4 (a), we see that the higher\nthepoisoningpercentagethelargertheratiolossandtheaverage\nmemory oﬀset. For instance,inthe large key domains we see that\nthe ratio increases up to 100 ×as the poisoning percentage gets\nlarger.Ontheotherhand,wealsoobservethatwhenthedensityis\ntoohigh,itmayresultinamuchsmallererrorincreaseinbothmea-\nsurements. Another interesting observation from our experiments\nis that lower density for the same /f_ixed number of legitimate keys\nimplies a larger set of potential poisoning keys and, thus, allows\nfor a greater increase of error. This can be seen from the drop of\n7\n\n 3  6  9 12 15\nPoisoning Percentage2468101214Ratio LossKeys: 100 Density:5%\n 3  6  9 12 15\nPoisoning Percentage2468101214Ratio LossKeys: 100 Density:10%\n 3  6  9 12 15\nPoisoning Percentage2468101214Ratio LossKeys: 100 Density:50%\n 3  6  9 12 15\nPoisoning Percentage2468101214Ratio LossKeys: 100 Density:80%\n 3  6  9 12 15\nPoisoning Percentage612182430364248Ratio LossKeys: 500 Density:5%\n 3  6  9 12 15\nPoisoning Percentage612182430364248Ratio LossKeys: 500 Density:10%\n 3  6  9 12 15\nPoisoning Percentage612182430364248Ratio LossKeys: 500 Density:50%\n 3  6  9 12 15\nPoisoning Percentage612182430364248Ratio LossKeys: 500 Density:80%\n 3  6  9 12 15\nPoisoning Percentage102030405060708090100Ratio LossKeys: 1000 Density:5%\n 3  6  9 12 15\nPoisoning Percentage102030405060708090100Ratio LossKeys: 1000 Density:10%\n 3  6  9 12 15\nPoisoning Percentage102030405060708090100Ratio LossKeys: 1000 Density:50%\n 3  6  9 12 15\nPoisoning Percentage102030405060708090100Ratio LossKeys: 1000 Density:80%\n 3  6  9 12 15\nPoisoning Percentage50100150200250300350400450500Ratio LossKeys: 5000 Density:5%\n 3  6  9 12 15\nPoisoning Percentage50100150200250300350400450500Ratio LossKeys: 5000 Density:10%\n 3  6  9 12 15\nPoisoning Percentage50100150200250300350400450500Ratio LossKeys: 5000 Density:50%\n 3  6  9 12 15\nPoisoning Percentage50100150200250300350400450500Ratio LossKeys: 5000 Density:80% 3  6  9 12 15\nPoisoning Percentage246810Average Memory OffsetKeys: 100 Density:5%\n 3  6  9 12 15\nPoisoning Percentage246810Average Memory OffsetKeys: 100 Density:10%\n 3  6  9 12 15\nPoisoning Percentage246810Average Memory OffsetKeys: 100 Density:50%\n 3  6  9 12 15\nPoisoning Percentage246810Average Memory OffsetKeys: 100 Density:80%\n 3  6  9 12 15\nPoisoning Percentage612182430Average Memory OffsetKeys: 500 Density:5%\n 3  6  9 12 15\nPoisoning Percentage612182430Average Memory OffsetKeys: 500 Density:10%\n 3  6  9 12 15\nPoisoning Percentage612182430Average Memory OffsetKeys: 500 Density:50%\n 3  6  9 12 15\nPoisoning Percentage612182430Average Memory OffsetKeys: 500 Density:80%\n 3  6  9 12 15\nPoisoning Percentage1020304050Average Memory OffsetKeys: 1000 Density:5%\n 3  6  9 12 15\nPoisoning Percentage1020304050Average Memory OffsetKeys: 1000 Density:10%\n 3  6  9 12 15\nPoisoning Percentage1020304050Average Memory OffsetKeys: 1000 Density:50%\n 3  6  9 12 15\nPoisoning Percentage1020304050Average Memory OffsetKeys: 1000 Density:80%\n 3  6  9 12 15\nPoisoning Percentage50100150200Average Memory OffsetKeys: 5000 Density:5%\n 3  6  9 12 15\nPoisoning Percentage50100150200Average Memory OffsetKeys: 5000 Density:10%\n 3  6  9 12 15\nPoisoning Percentage50100150200Average Memory OffsetKeys: 5000 Density:50%\n 3  6  9 12 15\nPoisoning Percentage50100150200Average Memory OffsetKeys: 5000 Density:80%\nFigure 4: Evaluation of the multi-point poisoning for linear regression on CDF. The /f_irst set evaluates the Ratio Loss , and the\nsecondsetevaluatesthe AverageMemoryOﬀset .Eachboxplotshowstherespectivelossonthelegitimatekeysetover20distinct\nkeysets. The legitimatekeys are distributed uniformly. The number of legitimatekeys, denoted as “Keys” and the size of the\nkey domain, denoted as “Key Domain” are presented on the title of each plot. The number of poisoning keys varies on the\n/u1D44B-axisandisrepresented as apoisoning percentagewith respectto thenumberoflegitimate keys.\ntheratiolossona/f_ixedrowoftheFigure4(a).Theaboveintuition\niscon/f_irmedevenwhenwecompareacolumnofFigure4(a),i.e.,\n/f_ix the key density and increase the domain. In that case we ob-\nserve that larger domains with the same /f_ixed density also imply a\nlarger set of potential poisoning keys and thus allows for a greater\nincrease oferror. Wealso observe thattheaverage memory oﬀset,\nFigure 4(b), grows linearly with the size ofdataset. Forexample,\nin thelast boxplot of thebottom leftplot, thelinear regression on\naverage predicts a memory address that is 200×the size of a single\nkey-valuepair afarfrom the true address.\nWhyUniformlyDistributedData? Thesuperiorperformance\nof RMI relies (in part) on the fact that it partitions the keyset in\nsmall consecutive keysets that are roughly similarly distributed.\nTherefore, even if the overall CDF has structure that is hard to\ncapture,each“local”structureissimilarenoughtobecapturedwith\nasimplemodel.Basedontheaboverationalewerunexperiments\nonauniformlydistributedkeysetinordertocaptureatypicallocal\nmodelofRMI.Forcompleteness,wealsoperformedexperiments\nonadatasetgeneratedwithanormaldistribution;wereportour\n/f_indings in the Appendix of this work. For a key domain /u1D448=\n[/u1D6FC,/u1D6FD]we form the normal distribution with mean /u1D707=/u1D6FD+/u1D6FC\n2andstandarddeviation /u1D70E=/u1D6FD−/u1D6FC\n3.Because the normal distributionsare\nnot captured well by linear models, the MSE loss of the original\ndataset is high. Despite that, our attack achieves up to 8×increase\noferror.\nOnLocalSearchStrategies. Kraskaetal.proposetwolocalsearch\nstrategies: linear search and binary search. In case of linear search,\ntheaverage memory oﬀsetcorresponds to theamountofmemory\nthat needs to be scanned to correct the accuracy error.In case of\na binary search, LIS take the maximum memory oﬀset from the\nentire dataset in order to “bound” the search area they perform the\nbinarysearchalgorithmon.Forcompletenessweprovideadditional\nexperiments that show the maximum memory oﬀset to address the\ncaseofabinarysearchbasedlocalsearch.Insteadofmeasuringthe\naverage memory oﬀset for each dataset, we measure the maximum\nmemory oﬀset over all queries on the keys that present in the\ndataset.Similartoaveragememoryoﬀset,maximummemoryoﬀset\ngrows linearly with the size of the dataset. The maximum memory\noﬀset is roughly two times the average memory oﬀset given the\nsame setup, which implies that the skew of the distribution of\nmemory oﬀsets isclose to center.\n8\n\n5 POISONINGTWO-STAGERMI MODELS\nArmedwiththepoisoningtechniqueforregressiononCDFfrom\nSection 4, in this section, we develop a poisoning attack for the\ntwo-stage hierarchical model of recursive model index (RMI). Our\napproach istailoredtotheindex architecture proposedby Kraska\net al.[30]. We evaluate our attack on RMI on synthetic and real\ndata undervariousscenarios.\nStructureofSecond-StageModels. AccordingtoKraska etal.[30],\nan index architecture that outperforms traditional B-Trees is com-\nprised of a neural network for the /f_irst-stage model and a linear\nregressiononaCDFforthesecond-stagemodel.Asdescribedin\nSection 3.1, each second-stage model is the “expert” in /f_ine-tuning\nthe prediction on a /f_ixed subset of keys . During the initialization\nof the RMI, the designer partitions the set of all keys, /u1D43E, into/u1D441\nnon-overlappingsubsets /u1D43E1,...,/u1D43E/u1D441ofequalsize.With /u1D445/u1D456wede-\nnote the set of ranks for the corresponding subset of keys /u1D43E/u1D456of the\n/u1D456-thsecond-stagemodel.Asanextstepthedesignerindependently\ntrainsalinearregressiononeach {/u1D43E/u1D456,/u1D445/u1D456}andstorestheregression\nparameters/u1D464/u1D456,/u1D44F/u1D456.Jumpingahead,ourattackexploitsthefactthata\nkey-partitionsteptakesplacetodecidewhichsecond-stagemodels\nto poison andhowmanykeys to inject.\nLossFunctionforRMI. Inthiswork,wedonotconsiderthecase\nof poisoning the neural network (NN) model of the /f_irst-stage. The\nreason behind this design choice is that according to the experi-\nments in [ 30], a query key that is used in the training of the NN\nmodelwillalwaysdirecttothecorrectregressionmodel.Therefore,\nwefocusourpoisoningattackonmanipulatingthemodelsofthe\nsecond-stageandassumethattheNNmodelwillalwayspointtothe\ncorrect(albeitpoisoned)second-stagemodel.Let L({/u1D43E/u1D456,/u1D445/u1D456},/u1D464/u1D456,/u1D44F/u1D456)\ndenotethelossfunctionofthe /u1D456-thlinearregressionmodelofthe\nsecond-stage that is trained on the CDF of {/u1D43E/u1D456,/u1D445/u1D456}and has param-\neters/u1D464/u1D456,/u1D44F/u1D456.We de/f_inethe lossfunctionofthe RMIas:\nLRMI(/u1D43E)=1\n/u1D441/u1D441/summationdisplay.1\n/u1D456=1L({/u1D43E/u1D456,/u1D445/u1D456},/u1D464/u1D456,/u1D44F/u1D456).\nPoisoning Threshold per Regression Model. An important\nobservation is that the adversary controls (1) which regression\nmodels to poison among the second-stage models, and (2) how\nmany poisoning keys to inject in each regression model. We argue\nthatinjectingtoomanypoisoningkeysinasingleregressionmodel\nmight allow a defense mechanism to detect such a behavior. In our\nattack, we handle this issue by imposing a poisoning threshold for\neachindividualregressionmodel,denotedby /u1D461.Recallthattheterm\npoisoningpercentage,seeSection3.3,controlsthe total,asopposed\nto per model,number ofpoisoningkeys.\nA/f_irstattemptonthisattackscenarioistopicka/f_ixedpoisoning\nthresholdacrossall linearregressionmodelsto followthe overall\npoisoningpercentage, /u1D719,i.e.,/u1D461=/u1D719/u1D45B\n/u1D441.Suchanapproachallowsonly\na single way of allocating poisoning keys to each model, e.g., if\nthe poisoning percentage /u1D719is10%on a keyset of size 106with key\npartitions of size 103, then the above approach can only assign 100\npoisoningkeysoneachregressionmodel.Thus,itisnotpossible\nto skew the assignmentof poisoning keys. To increase theimpact\nof the attack, we allow the poisoning threshold to vary across\nmodelsprovideit doesnotexceedacertainupperbound .Inparticular,\ndenotingwith /u1D461/u1D456thepoisoningpercentageofthe /u1D456-themodel,werequire/u1D461/u1D456≤/u1D461=/u1D6FC·/u1D719/u1D45B\n/u1D441,where/u1D6FCisa small constant. Forexample,\nin our experiments we pick /u1D6FC∈{2,3}which means that for 10%\npoisoningpercentageinourpreviousexampleweallow upto/u1D461=\n200(resp./u1D461=300) poisoning keys per regression. This approach\nallows multiple ways of assigning poisoning keys to regression\nmodels and, thus, allows our attack to achieve larger LRMIerror\nwithoutoverpopulating withpoisoningkeys the regression.\nFormulationfor PoisoningRMI. Let/u1D443/u1D456be the set of poisoning\nkeys injected to the /u1D456-th model of the second-stage. Let /u1D719be the\nallowedoverallpoisoningpercentagefortheRMImodelandlet /u1D461\nbe the poisoning threshold per second-stage model. Then, the goal\nofthe poisoningattackontwostageRMIcan be expressedas:\nargmax/u1D4431,...,/u1D443/u1D441/u1D441/summationdisplay.1\n/u1D456=1min\n/u1D464/u1D456,/u1D44F/u1D456L({/u1D43E/u1D456∪/u1D443/u1D456,[1,/u1D45B+|/u1D443/u1D456|]},/u1D464/u1D456,/u1D44F/u1D456)\nsuch that,|/u1D443/u1D456|≤/u1D461,∀/u1D456∈[1,/u1D441],and/summationdisplay.1/u1D441\n/u1D456=1|/u1D443/u1D456|≤/u1D719/u1D45B\nWecanre-frametheoptimizationintotwosubproblems.The/f_irst\nistochoosehowmanypoisoningkeysareinjectedperpartition,\nwe call this volume allocation problem , and the second is to choose\nwhichpoisoningkeystoinjectwithinaspeci/f_icpartition,wecall\nthis thekey allocation problem . For the latter problem we use Algo-\nrithm1forgreedypoisoningtheregression.Thevolumeallocation\nproblemisan /u1D441-dimensionalintegerprogrammingproblemwith\ninput vector(|/u1D4431|,...,|/u1D443/u1D441|). The search space is comprised of the\nvolumeassignmentssuchthat |/u1D4431|+···+|/u1D443/u1D441|=/u1D719/u1D45Bandthevolume\non each dimension is bounded by 0 and /u1D461. For realistic datasets\nit is infeasible to explore this search space. We propose a greedy\napproach for the volume allocation that performs well inpractice.\n5.1 Poisoning Algorithm\nGiven the discussed formulation for poisoning RMI we propose an\nattackthat(A)followsagreedyapproachforthevolumeallocation\nproblem,and(B)givena/f_ixedvolumepersecond-stagemodel,it\napplies Algorithm 1for the key allocation problem.\nOurSecond-StageVolumeAllocationApproach. InAlgorithm2,\nwefollowagreedyapproachthattakeslocallyoptimalstepsforde-\ncidinghowmanypoisoningkeystoallocate,i.e.,volumeallocation,\nat each second-stage regression model. Note that our attacker has\na total of/u1D719/u1D45Bpoisoning keys to allocate. As a /f_irst step the attacker\ndistributes poisoning keys uniformly among models, i.e., /u1D719/u1D45B//u1D441\npoisoningkeys for eachof the /u1D441second-stage models.\nThe intuition for our greedy approach is that we exchange a\npoisoning key for a legitimate key between one regression model\nanditsneighbor,i.e.,eitherthenextmodelorthepreviousmodel,\nifthisre-allocationcausesthemaximumincreaseinlossfunction\nLRMIamong allthe key-exchanges of this type.\nWeaccompanythere-allocationofapoisoningkeyfromthe /u1D456-th\nmodel to the /u1D457-th model with the (reverse) move of a legitimate\nkey from the /u1D457-th model to the /u1D456-th. The above step guarantees\nthatthenumberofkeys,i.e.,thesumofpoisonedandlegitimate,\nstays /f_ixed throughout the re-allocation moves. More formally we\nusethenotation /u1D456→/u1D456+1toindicatetheexchangeofkeyswhere\nthe direction of the arrow indicates the move of a poisoning key.\nSpeci/f_ically,the/f_irstmoveisthatapoisoningkeythatwasavailable\nforplacementinthe /u1D456-thmodelnowisallowedtobeplacedintothe\n(/u1D456+1)-th modelinstead,and the second move is that the minimum\n9\n\nlegitimate key of the(/u1D456+1)-th model is assigned to the /u1D456-th model.\nSimilarly,thenotation /u1D456←/u1D456+1indicatesthatapoisoningkeythat\nwasavailabletothe (/u1D456+1)-thmodelnowmovestothe /u1D456-th,andthat\nthemaximum legitimate key of the/u1D456-th model is assigned to the\n(/u1D456+1)-thmodel.ForthepurposeofAlgorithm2weneedtokeep\ntrackofwhichreallocationofapoisoningkeycausesthemaximum\nincreaseinLRMI.Wede/f_ineasimpletwodimensionalarraydenoted\nasC/h.sc/a.sc/n.sc/g.sc/e.scL/o.sc/s.sc/s.sc where entry C/h.sc/a.sc/n.sc/g.sc/e.scL/o.sc/s.sc/s.sc (/u1D456,/u1D456+1) contains the\nchangeinlossLRMIiftheattackerexecutesthemovesimpliedby\n/u1D456→/u1D456+1giventhecurrentstateofallocation.Similarly,thenotation\nC/h.sc/a.sc/n.sc/g.sc/e.scL/o.sc/s.sc/s.sc (/u1D456+1,/u1D456)capturesthereallocation /u1D456←/u1D456+1.Inevery\niteration,Algorithm2/f_indsthemaximumentryof C/h.sc/a.sc/n.sc/g.sc/e.scL/o.sc/s.sc/s.sc and\nappliesthe exchange,seeLine6, withthecaveatthattheaddition\nof a poisoning key to /u1D457does not violate the upper bound threshold\n/u1D461ofpoisoningkeys.Algorithm2terminateswhentheincreasein\nLRMIlossisless than /u1D716,see Line5.\nPerforming the set of moves for exchange /u1D456→/u1D456+1(or/u1D456←\n/u1D456+1) in Line 7, implies that a constant number of entries from\nC/h.sc/a.sc/n.sc/g.sc/e.scL/o.sc/s.sc/s.sc are rendered inconsistent since they rely on an old\nstate of the volume allocation. Interestingly, because the exchange\nin keys takes place between models /u1D456and/u1D456+1it only aﬀects the\nC/h.sc/a.sc/n.sc/g.sc/e.scL/o.sc/s.sc/s.sc entriesoftheirdirectneighbors,allothermodelsstay\nunaﬀected with respect to their C/h.sc/a.sc/n.sc/g.sc/e.scL/o.sc/s.sc/s.sc entries. Without loss\nofgenerality,weassumethat /u1D456→/u1D456+1waschosen.Then,sincea\nnewpoisoningkeyisinjectedinthe (/u1D456+1)-thmodelandalegitimate\nisremovedwehavetoupdatealltheentriesthatreferto (/u1D456+1).Thus,\nwehavetorecomputethefollowingentries: C/h.sc/a.sc/n.sc/g.sc/e.scL/o.sc/s.sc/s.sc (/u1D456,/u1D456+1),\nC/h.sc/a.sc/n.sc/g.sc/e.scL/o.sc/s.sc/s.sc (/u1D456+1,/u1D456),C/h.sc/a.sc/n.sc/g.sc/e.scL/o.sc/s.sc/s.sc (/u1D456+1,/u1D456+2),C/h.sc/a.sc/n.sc/g.sc/e.scL/o.sc/s.sc/s.sc (/u1D456+2,/u1D456+1).\nAdditionally,wehavetorecomputethefollowingentriesofthe /u1D456-th\nmodelthatnowhasonelesspoisoningkeyandonemorelegitimate\nkey:C/h.sc/a.sc/n.sc/g.sc/e.scL/o.sc/s.sc/s.sc (/u1D456,/u1D456−1),C/h.sc/a.sc/n.sc/g.sc/e.scL/o.sc/s.sc/s.sc (/u1D456−1,/u1D456). To compute the\nupdatedentryfor C/h.sc/a.sc/n.sc/g.sc/e.scL/o.sc/s.sc/s.sc werunAlgorithm 1whichtakes\ntime linear to the number of keys of the second-stageregression\nmodel,i.e.,/u1D442(/u1D45B//u1D441).Sincethere are six updates thattake placefor\nthechosenexchange /u1D456→/u1D456+1,thecomplexityofLine8is /u1D442(/u1D45B//u1D441).\nThespacecomplexityisdominatedbythemaintenanceofthearray\nC/h.sc/a.sc/n.sc/g.sc/e.scL/o.sc/s.sc/s.sc that contains 2/u1D441elements.\n5.2Evaluationon Synthetic Data\nOurresultsshowthattheRMIerrorafterpoisoningincreasesupto\n150×whilethe individualregressionerrorincreasesupto 1000×.\nSetup.IntheexperimentsshowninFigure5,weproducesynthetic\ndata sets of keys that are uniformly distributed in key domains\n|K|=106and|K|=108. We implemented three diﬀerent architec-\nture for assigning legitimate keys to second-stage models, i.e., RMI\narchitectures, so as to validate our approach across diﬀerent eﬃ-\nciencyandaccuracytrade-oﬀsforRMIs.The/f_irstscenarioanalyzes\nthe case of a large number of second-stage models, speci/f_ically 105\nindicated with “#Models” on the title of the boxplot, where each is\nresponsibleforasmallnumberofkeys,speci/f_ically 102indicated\nwith “Model Size”. The above RMI architecture corresponds to the\nplots on the /f_irst column of Figure 5. In the second and third ar-\nchitectures we decrease the number of second-stage models which\nimplies anincreasein thenumber ofkeys permodel. Thesearchi-\ntecturescorrespondtotheboxplotsonthesecondandthirdcolumn\nof Figure 5. Overall, as we iterate through columns of Figure 5Algorithm2: G/r.sc/e.sc/e.sc/d.sc/y.scP/o.sc/i.sc/s.sc/o.sc/n.sc/i.sc/n.sc/g.scRMI\nData:Poisoningpercentage /u1D719, number of second-stage models /u1D441, keyset\n/u1D43E={/u1D4581,...,/u1D458/u1D45B}where/u1D458/u1D456∈K, terminationbound /u1D716, poisoning\nthreshold per regressionmodel /u1D461\nResult:Set of poisoning keys /u1D4431,...,/u1D443/u1D441such that/u1D4431∩...∩/u1D443/u1D441∩/u1D43E=∅,/summationtext.1/u1D441\n/u1D456=1|/u1D443/u1D456|=/u1D719/u1D45B, and|/u1D443/u1D456|</u1D461.\n// Initial Volume Allocation\n1Iterate through all the regression models of the second-stage and for the /u1D456-th\nmodel, initialize /u1D443/u1D456by injecting/u1D719/u1D45B//u1D441poisoning keysusing Algorithm1;\n2ComputeLRMIby averaging the loss of second-stage models;\n// Store effect of exchange /u1D456→/u1D456+1in ChangeLoss\n3Iteratethrough all second-stage models; for eachmodel /u1D456∈[1,/u1D441], compute\nthe change inLRMIif we were to (A) add a poisoning key to /u1D443/u1D456+1, (B) move\nthe smallestlegitimatekeyfrom (/u1D456+1)-th modelto the /u1D456-th, and (C)\nremoveapoisoning keyfrom /u1D443/u1D456. Storethe diﬀerencebetween the new\nLRMIafterthe abovemovesand the currentloss LRMIin the entry\nC/h.sc/a.sc/n.sc/g.sc/e.scL/o.sc/s.sc/s.sc (/u1D456,/u1D456+1);\n// Store effect of exchange /u1D456←/u1D456+1in ChangeLoss\n4Iteratethrough all second-stage models; for eachmodel /u1D456∈[1,/u1D441], compute\nthe change inLRMIif wewereto (A)adda poisoning keyto /u1D443/u1D456, (B)move\nthe largest legitimate key from /u1D456-th model to the(/u1D456+1)-th, and (C) remove\na poisoning key from /u1D443/u1D456+1. Store the diﬀerence between the new LRMIafter\nthe abovemovesand the currentloss LRMIin the entry\nC/h.sc/a.sc/n.sc/g.sc/e.scL/o.sc/s.sc/s.sc (/u1D456+1,/u1D456);\n// Greedy iteration that increase the loss of the RMI\n5whilethe change inLRMIis largerthan /u1D716do\n// Perform a Greedy exchange of a poisoning key with a\nlegitimate key between consecutive models\n6Find the indices /u1D456,/u1D457that (A)correspond to the largestentry\nC/h.sc/a.sc/n.sc/g.sc/e.scL/o.sc/s.sc/s.sc (/u1D456,/u1D457)and (B)do not violate the poisoning threshold /u1D461for\nthe/u1D457-th model;\n7Perform the exchange /u1D456→/u1D457between models /u1D456and/u1D457and use\nAlgorithm1 for addinga poisoning keyto /u1D443/u1D457;\n// Fix the consistency of ChangeLoss as a constant number\nof entries from ChangeLoss were modified with respect\nto the previous volume allocation\n8Recomputethe inconsistent entries of C/h.sc/a.sc/n.sc/g.sc/e.scL/o.sc/s.sc/s.sc , i.e., entries that\naddress referto /u1D456, or/u1D457, orboth;\n9end\n10returnthe setofpoisoningkeys /u1D4431∪...∪/u1D443/u1D441;\nfrom left-to-right, the RMI model decreases its storage overhead as\nwellasitspredictionaccuracyofthesecond-stagemodel.Finally,\nwe tested two diﬀerent multiplicative constants /u1D6FCof poisoning\nthresholds for eachindividualsecond-stage model.\nResults. The results of our evaluation are presented in Figure 5.\nEach boxplot shows the ratio loss across all second-stage linear\nregression models. We present the ratio loss for each regression\nmodel individually so as to provide a more /f_ine-grained analysis of\nhow the attack performs. We also present the ratio between the\nloss of the poisoned RMI model and the non-poisoned RMI with a\nblackhorizontalline.Asisexpectedwhenthepoisoningpercent-\nage increases the eﬀectiveness of the attack increases as well. For a\n/f_ixedrowofboxplotsinFigure5,weseethatthelargerthesecond-\nstage model the better our attack performs. This phenomenon is\nexplained by the fact that the linear regression is responsible for\nmore data; therefore,there are moreopportunitiesfor the poison-\ning to increase its eﬀectiveness. Given the same key distribution\nand setup we see that the size of the key domain did not aﬀect\nsigni/f_icantlytheperformanceofourpoisoningattackonRMI,i.e.,\nthe loss for RMI is slightly largerfor keydomain 109compared to\n5·107.Additionally,thediﬀerencebetweentheerrorforpoisoning\nthreshold per regression /u1D6FC=2and/u1D6FC=3isnot signi/f_icant.\n10\n\nUniform Key Distribution1% 5% 10%Poisoning Percentage048121620Ratio LossKeys:107  Model Size:102  #Models:105   Key Domain:109\n=3\n=2\n1% 5% 10%\nPoisoning Percentage020406080100120Ratio LossKeys:107  Model Size:103  #Models:104  Key Domain:109\n=3\n=2\n1% 5% 10%Poisoning Percentage0150300450600750900Ratio LossKeys:107  Model Size:104  #Models:103  Key Domain:109\n=3\n=2\n1% 5% 10%\nPoisoning Percentage048121620Ratio LossKeys:107  Model Size:102  #Models:105  Key Domain: 5 107\n=3\n=2\n1% 5% 10%\nPoisoning Percentage020406080100120Ratio LossKeys:107  Model Size:103  #Models:104  Key Domain:5 107\n=3\n=2\n1% 5% 10%\nPoisoning Percentage0150300450600750900Ratio LossKeys:107  Model Size:104  #Models:103  Key Domain: 5 107\n=3\n=2\n1% 5% 10%\nPoisoning Percentage048121620Average Memory OffsetTotal Keys: 107 Model Size: 102 #Models: 105 Key Domain: 109\nAfter Poisoning\nBefore Poisoning\n1% 5% 10%\nPoisoning Percentage01632486480Average Memory OffsetTotal Keys: 107 Model Size: 103 #Models: 104 Key Domain: 109\nAfter Poisoning\nBefore Poisoning\n1% 5% 10%\nPoisoning Percentage0100200300400500Average Memory OffsetTotal Keys: 107 Model Size: 104 #Models: 103 Key Domain: 109\nAfter Poisoning\nBefore Poisoning\nFigure5:Evaluationofthemulti-pointpoisoningforRMIonuniformdistributedkeyset.The/f_irsttworowsshowtheratioloss\nandthethirdrowshowstheaveragememoryoﬀset.Inthe/f_irsttworows,thecoloroftheboxplotdenotesdiﬀerent /u1D6FCvaluesfor\nthe poisoning threshold per model. In the last row, the blue boxes show the non-poisoned datasets and the red boxes show the\ncorresponding datasetsafter poisoning. Inallboxplots,theblackline showstherespective erroroverthe entire dataset.\nLog-NormalKey Distribution1% 5% 10%\nPoisoning Percentage369152127Ratio LossKeys:107  Model Size:102  #Models:105  Key Domain:109\n=3\n=2\n1% 5% 10%\nPoisoning Percentage306090150210270Ratio LossKeys:107  Model Size:103  #Models:104  Key Domain:109\n=3\n=2\n1% 5% 10%\nPoisoning Percentage300600900150021002700Ratio LossKeys:107  Model Size:104  #Models:103  Key Domain:109\n=3\n=2\n1% 5% 10%\nPoisoning Percentage369152127Ratio LossKeys:107  Model Size:102  #Models:105 Key Domain:5 107\n=3\n=2\n1% 5% 10%\nPoisoning Percentage306090150210270Ratio LossKeys:107  Model Size:103  #Models:104  Key Domain:5 107\n=3\n=2\n1% 5% 10%\nPoisoning Percentage300600900150021002700Ratio LossKeys:107 Model Size:104  #Models:103  Key Domain:5 107\n=3\n=2\nFigure 6: Evaluation of the multi-point poisoning for RMI on log-normal distributed key set. Each boxplot shows the ratio loss\nacrossallsecond-stagemodels.TheratiobetweenthelossofthepoisonedRMImodelandthenon-poisonedisrepresentedwith\na black horizontal line. The /u1D44B-axis represents diﬀerent poisoning percentages and the color of the boxplot denotes diﬀerent /u1D6FC\nvaluesforthepoisoning threshold perregressionmodel.\nAdditional Experiments. Additionally, we tested our method on\nsyntheticdatasetsofkeysthataredistributedwithalog-normal\ndistribution with /u1D707=0and/u1D70E=2(the same parameterization\nas the experiments in [ 30]). We illustrate the result in Figure 6.\nWe observe that the performance of the attack is superior in the\nlog-normal distribution compared to the uniform. In fact, the ratioloss is up to 2×larger for the same RMI setup. Interestingly, the\nwhiskersoftheboxplotarecloseto 3×largerforthecaseofthelog-\nnormaldistribution,whichimpliesamuchlargerspreadofratioloss\nvaluesamongthesecond-stagemodels.Thisisexplainedbythefact\nthat in the log-normal case, we have some regressions that handle\nconcentrated keys and by poisoning these models, we amplify the\n11\n\nMiami FloridaSalaries5% 10% 20%\nPoisoning Percentage05101520Ratio LossKeys: 5.3  103  Model Size:50  #Models:106 \n5% 10% 20%\nPoisoning Percentage01020406080100Ratio LossKeys: 5.3  103  Model Size:100  #Models:53 \n5% 10% 20%\nPoisoning Percentage01020406080100Ratio LossKeys: 5.3  103  Model Size:200  #Models:26 \n1% 5% 10%\nPoisoning Percentage012345678910Average Memory OffsetTotal Keys: 5.3k Model Size: 50 #Models: 106\nAfter Poisoning\nBefore Poisoning\n1% 5% 10%\nPoisoning Percentage02468101214161820Average Memory OffsetTotal Keys: 5.3k Model Size: 100 #Models: 53\nAfter Poisoning\nBefore Poisoning\n1% 5% 10%\nPoisoning Percentage036912151821242730Average Memory OffsetTotal Keys: 5.3k Model Size: 200 #Models: 26\nAfter Poisoning\nBefore Poisoning\n0.2 40,000 60,000 80,000 100,000 120,000 140,000 160,000 180,000\nKeys - Annual Salary (in US Dollars)50015002500350045005500RankCumulative Density Function (CDF) of the Salaries from Miami Florida  Keys:5,300  Key Domain:167,301  Density:3.71%   \nFigure 7: Evaluation of the multi-point poisoning for RMI applied on the CDF of the unique salaries of employees from Dada\nCounty in Miami.The /u1D44B-axisrepresents diﬀerent overall poisoning percentage wherethe second-stagepoisoning threshold /u1D6FC\ntakesvalue /u1D6FC=3.The third row presentstheCDF.\nnon-linearityofthelegitimatekeyswhichresultsinlargererrors.\nIn general, for the log-normal distribution, the RMI error presents\nup to300×increase; whereas, in the individual second-stage level,\nwe observedupto 3000×errorincrease.\n5.3 Evaluationon Real-WorldData\nWe evaluate the proposed greedy poisoning attack on RMI models\nonareal-worlddatasetofsalariesoftheemployeesofDadeCounty\ninFlorida[ 38].OurexperimentshowthattheRMIerrorofapoi-\nsonedkey setincreases up to 24×and the errorofthe individual\nregressionincreasesupto 70×.\nSetup.Weusethepubliclyavailabledatasetofsalariesofemployees\nofMiamiDadeCountyinFlorida[ 38].Weonlytakeuniquesalaries\nbetween $22,733 and $190,034, i.e., we pre-process to /f_ilter the\noutliers. The /f_inal dataset has /u1D45B=5,300keys in a key universe\nof size/u1D45A=167,301which gives a 3.71%key density. The CDF\nisdepictedinFigure7.Inthisexperiment,wetestthreediﬀerent\nRMIsetups.Inthe/f_irstsetup,weinitializethesecond-stagemodels\nwith 50 keys each, which results in 106 second-stage models. In\nthesecondsetup,weinitializethesecond-stagewith100keyseach,\nwhich results in 53 models. In the third setup, we initialize the\nsecond-stagewith200keyseach,whichresultsin26models.Forall\nthesetups,weconsiderthepoisoningthresholdofthesecondstage\nmodel to have parameter /u1D6FC=3. For each experiment, we recordboth the ratio loss and the average memory oﬀset. We consider\npoisoningpercentages 5%,10%,and20%.\nResults. The results of our evaluation are presented in Figure 7.\nFor a /f_ixed setup, we observe that as the poisoning percentage\nincreases, the ratio loss increases as well. Across all setups, the\nratio loss increases between 4×to24×. The average memory oﬀset\nof both the original dataset and the poisoned dataset scales with\nthesizeofsecond stagemodel. On allthree setups, 10%poisoning\ncauses the average memory oﬀset to increase by 3×. Since the\nnumber of poisoning keys per second-stage model is a percentage\nover its number of keys, we see that larger models allow more\npoisoning and, consequently, larger RMI error. The discrepancy\nbetweentheeﬀectivenessoftheattackonsyntheticdataandthe\nrealdataispartlyexplainedbythefactthatthesyntheticdatasetis\nfourorders ofmagnitude larger thanthe real dataset.\nAdditional Experiments. In theAppendixwe present indetail\nexperiments on the geolocation dataset [ 44] which is index by the\nlatitudesoflocations.Wepickthedatapointsthatarelabelledas\nschoolsovertheworldwithlatitudesbetween-30and+50;andscale\nupthelatitudesby15,000beforeroundingtoachieveuniqueness\nof keys. The /f_inal dataset has /u1D45B=302,973keys in a key universe of\n/u1D45A=1,200,000,whichyieldsa 25%density.\nIn this experiment, we test three diﬀerent RMI model setups.\nIn the /f_irst setup, we initialize the second-stage models with 50\nkeys each, which translates to 6,059 second-stage models. In the\n12\n\nsecond setup, we initialize the second-stage with 100 keys each,\nwhich translates to 3,029 models. In the third setup, we initialize\nthe second-stage with 200 keys each, which translates to 1,514\nmodels. For all the setups, we consider the poisoning threshold\nof the second stage model to have parameter /u1D6FC=3. Finally, we\nconsideredpoisoningpercentages 5%,10%,and20%.\nFora/f_ixedsetup,weobservethatasthepoisoningpercentage\nincreases,theratiolossincreasesaswell.Amongallexperimentsin\nboth datasets, the RMI loss increases between 4×to24×. Since the\nnumber of poisoning keys per second-stage model is a percentage\nover its number of keys, we see that larger models allow more\npoisoningand,consequently,larger RMIerror.\n6 DISCUSSION\nOnGeneralizingtheAttack. Wepresentourattackonthedesign\nthatinitiatedthe study oflearnedsystems [ 30].Although ourat-\ntack does not always transfers as is to all proposed LIS designs, we\nbelieve poisoning attacks mounted in a similar fashion are possible\nin most cases until we formally analyze the robustness of these\nmodels. The underlying theme of all learned index proposed so\nfar is that a model adapts to the underlying data , e.g., via linear\nregression [ 7,30] or via piece-wise linear approximation [ 15] or\nvialinearinterpolation [ 17]orviapolynomialinterpolation [ 47].\nGiven that a single maliciously chosen datapoint changes (i) the\nparameterizationofthemodel,i.e.,anMLmodelneedstoretrainto\ntakeintoaccounttheerrorintroducedbytheaddition,aswellas(ii)\nthe coordinates of up to a linear number of legitimate CDF entries\n(see Section 4.2), we expect that poisoning would be a reality until\nwedevelopdefensemechanismsorrobustmodelsonCDFs.Ona\nmore technical note, following the blueprint of thiswork, one can\ndevise attacks that target the (typically simple and space-eﬃcient)\nmodel and then apply the attack iteratively to aﬀect the overall\nperformance, e.g., via a greedy approach. For example, works such\nas thePGM I/n.sc/d.sc/e.sc/x.sc [15], use linear regression as last-stage indexing\nmodel, but employs intricate model for /f_irst-stage partitioning. We\nbelieve that by applying directly our technique from Section 4 for\nthelaststageof PGM, onecan developtailored attacks toa signif-\nicant number of these variations. Other works such as Setiawan\net al.[47] propose the use of function interpolation as a second-\nstagemodel.ApossibleattackmayinvolvemaximizingtheMSEon\nfunctioninterpolation ,as opposedto the MSE oflinearregression.\nOnDefendingAgainstPoisoninginLIS. Defendingagainstthe\nproposedpoisoningattacksofthisworkischallenging.Weseetwo\nways forward, one is to swap every linear regression model to a\nmorerobustmodelandtheotheristodevelopdefensemechanisms\nthat can identifypoisoningpoints.\nRegarding the model substitution approach, LIS is a more favor-\nableoptioncomparedtotraditionalindexdatastructuresbecauseof\n(i) the storage and (ii) time eﬃciency of the model used, i.e., linear\nregression [ 7,30]. Speci/f_ically, an LIS based on linear regression\nrequires onlytwoorfour8-bytedoublevalues permodel and be-\ncause RMI has only 2-3 levels, it requires storing a large number of\nsuch models. Thanks to the small storage space of each model, LIS\nachieves a magnitude smaller memoryfootprintinmain memory\nstoragecomparedtoB+Trees(havetostorekeysandpointersininternalnodes).MigratingaLIStoamorecomplexmodeltomiti-\ngate the risk of poisoning, would introduce signi/f_icant overhead in\ntimeand/or storage,thus,substantiallyreduce(oreveneliminate)\nthe performance advantage of LIS over traditional index structures.\nRegarding the poisoning detection approach, most poisoning\ndefenses focus on neural networks [ 34,46,57] and classi/f_ication\ntasks[6];there are very fewworks onmitigationfor poisoningof\nlinear regression. Contrary to traditional poisoning attacks that\ntendtointroduceoutliers,poisoningCDFfunctionstendstopop-\nulate relatively dense areas of the key space and as a result we\nexpect that the poisoning points (in this new CDF context) are\nhardtoidentify.Jagielski etal.[26]proposedapoisoningdetection\nalgorithm, TRIM,onlinearregressions. TRIMsearchesforthekeys\nthatcausethelargestlossandlabelsthemaspoisoningkeys.There\naretwomajorlimitationsinapplying TRIMtooursetting.Firstly,\nin our setting, the rank of each key depends on the value of all\notherkeysinthedataset;thisimpliesthat TRIMhastoiteratively\nre-calibrate its parameters and as a result become extremely in-\neﬃcient.Secondly,ourpoisoningkeysaretypicallyconcentrated\naround legitimate keys which makes it hard to detect. Thus, we\nbelievethat TRIMcannotremovepoisoningkeyswithoutremoving\nasigni/f_icant number oflegitimate keys.\nOverall, the intricacies of the CDF itself, i.e., each update aﬀects\nmultiple entries, as well as the behavior of poisoning algorithms\nin this context, i.e., populate dense areas instead of sparse, suggest\nthat we needto develop newandtailoredmitigations for CDFs.\n7 CONCLUSION\nLearned index structures [ 30] aim at achieving the functionality of\ndatastructuresusingmachinelearningmodels.Whatdiﬀerentiates\ntheLISparadigmfrompreviousapproachesisthatML-modelsadapt\ntothedataathand.Inthiswork,weproposedatapoisoningattacks\nthat exploit the above advantage for adversarial purposes. Our\nattacks poison ML-models on CDFs, which is a family of functions\nthathasnotbeenstudiedunderanadversariallens.Wedemonstrate\nour attacks on synthetic and real-world datasets under various\ndistributionsandLISparameterizationsandshowthattheyachieve\nsigni/f_icant slow-downinevery testedscenario.\nREFERENCES\n[1]Evan Ackerman. 2019. Three Small Stickers in Intersection Can\nCause Tesla Autopilot to Swerve Into Wrong Lane. In IEEE Spectrum .\nhttps://spectrum.ieee.org/three-small-stickers-on-road-can-steer-tesla-\nautopilot-into-oncoming-lane [Accessed:11-Oct-2021].\n[2]IoannisAlagiannis,StratosIdreos,andAnastasiaAilamaki.2014. H2O:AHands-\nFreeAdaptiveStore. In Proc. ofACMSIGMOD . 1103–1114.\n[3]BattistaBiggio,BlaineNelson,andPavelLaskov.2012. PoisoningAttacksAgainst\nSupport Vector Machines. arXiv:1206.6389\n[4]BattistaBiggio,IgnazioPillai,SamuelRotaBulò,DavideAriu,MarcelloPelillo,\nand Fabio Roli.2013. IsData Clusteringin Adversarial Settings Secure? Proc. of\nAISec.\n[5]Antonio Boﬀa, Paolo Ferragina, and Giorgio Vinciguerra. 2021. A \"Learned\"\nApproach to Quickenand Compress Rank/Select Dictionaries. In ALENEX.\n[6]DanielCullina,ArjunNitinBhagoji,andPrateek Mittal.2018. PAC-learningin\nthe presence of adversaries.In Proc. ofNeurIPS . 230–241.\n[7]Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned\nIndex. In Proc. ofthe ACMSIGMOD . 969–984.\n[8]Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020.\nTsunami:ALearnedMulti-DimensionalIndexforCorrelatedDataandSkewed\nWorkloads. Proc. VLDBEndow. 14,2 (Oct.2020),74–86.\n13\n\n[9]Yihe Dong, Piotr Indyk, Ilya P. Razenshteyn, and Tal Wagner. 2019. Learning\nSublinear-Time Indexingfor Nearest NeighborSearch. arXiv:1901.08544\n[10]Martin Eppert,PhilippFent,andThomasNeumann. 2021. ATailoredRegression\nfor Learned Indexes: Logarithmic Error Regression. Proc. of the International\nWorkshop onExploitingArti/f_icial Intelligence Techniques forDataManagement\n(aiDM), 9–15.\n[11]Minghong Fang, Minghao Sun, Qi Li, Neil Zhenqiang Gong, Jin Tian, and Jia\nLiu.2021. DataPoisoningAttacksandDefensestoCrowdsourcingSystems.In\nProceedingsofthe Web Conference (WWW) . 969–980.\n[12]Paolo Ferragina, Fabrizio Lillo, andGiorgio Vinciguerra. 2020. Why Are Learned\nIndexesSo Eﬀective?. In Proc. ofthe 37thICML , Vol. 119.\n[13]Paolo Ferragina, Fabrizio Lillo, and Giorgio Vinciguerra. 2021. On the Perfor-\nmanceofLearnedDataStructures. TheoreticalComputerScience (2021),107–120.\n[14]Paolo Ferragina and Giorgio Vinciguerra. 2020. Learned Data Structures. In\nRecent TrendsinLearning FromData . 5–41.\n[15]PaoloFerraginaandGiorgioVinciguerra.2020.ThePGM-Index:AFully-Dynamic\nCompressedLearnedIndexwithProvableWorst-CaseBounds. Proc.VLDBEndow.\n13,8 (April2020),1162–1175.\n[16]Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model Inversion\nAttacks That Exploit Con/f_idence Information and Basic Countermeasures. In\nProc. ofthe 22nd ACMCCS . 1322–1333.\n[17]Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim\nKraska. 2019. FITing-Tree: A Data-Aware Index Structure. In Proc. of ACM\nSIGMOD. 1189–1206.\n[18]Ali Hadian and Thomas Heinis. 2019. Considerations for Handling Updates\nin Learned Index Structures. Proc. of the International Workshop on Exploiting\nArti/f_icial IntelligenceTechniquesfor Data Management(aiDM) .\n[19]AliHadianandThomasHeinis.2019. Interpolation-friendlyB-trees:Bridging\nthe Gap Between Algorithmicand Learned Indexes. In Proc. ofEDBT . 710–713.\n[20]Ali Hadian and Thomas Heinis. 2021. Shift-Table: A Low-latency Learned Index\nfor Range Queriesusing ModelCorrection. arXiv:2101.10457\n[21]Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. 2019. Learning-Based\nFrequency EstimationAlgorithms.In Proc. ofICLR .\n[22]Hai Huang, Jiaming Mu, Neil Zhenqiang Gong, Qi Li, Bin Liu, and Mingwei Xu.\n2021. DataPoisoningAttackstoDeepLearningBasedRecommenderSystems.\nProc. ofNDSS .\n[23]Stratos Idreos, Niv Dayan, Wilson Qin, Mali Akmanalp, Sophie Hilgard, Andrew\nRoss,JamesLennon,VarunJain,HarshitaGupta,DavidLi,andZichenZhu.2019.\nDesignContinuumsandthePathTowardSelf-DesigningKey-ValueStoresthat\nKnowand Learn. In Proc. ofCIDR .\n[24]Stratos Idreos and Tim Kraska. 2019. From Auto-Tuning One Size Fits All to\nSelf-Designed and Learned Data-Intensive Systems. In Proc. of ACM SIGMOD .\n2054–2059.\n[25]Piotr Indyk, Ali Vakilian, and Yang Yuan. 2019. Learning-Based Low-Rank\nApproximations. In Proc. ofNeurIPS . 7400–7410.\n[26]MatthewJagielski,AlinaOprea,BattistaBiggio,ChangLiu,CristinaNita-Rotaru,\nandBo Li.2018. ManipulatingMachine Learning:PoisoningAttacksandCoun-\ntermeasuresfor RegressionLearning. Proc. ofIEEE S&P .\n[27]AndreasKipf,RyanMarcus,AlexandervanRenen,MihailStoian,AlfonsKemper,\nTim Kraska, and Thomas Neumann. 2019. SOSD: A Benchmark for Learned\nIndexes. arXiv:1911.13014\n[28]AndreasKipf,RyanMarcus,AlexandervanRenen,MihailStoian,AlfonsKemper,\nTimKraska,andThomasNeumann.2020. RadixSpline:ASingle-PassLearned\nIndex.Proc. of the International Workshop on Exploiting Arti/f_icial Intelligence\nTechniquesfor Data Management(aiDM) .\n[29]TimKraska,MohammadAlizadeh,AlexBeutel,EdH.Chi,AniKristo,Guillaume\nLeclerc, Samuel Madden, Hongzi Mao, and Vikram Nathan. 2019. SageDB: A\nLearned DatabaseSystem.In Proc. ofCIDR .\n[30]Tim Kraska, AlexBeutel,Ed H. Chi,JeﬀreyDean, and NeoklisPolyzotis.2018.\nThe Case for Learned IndexStructures. In Proc. ofACMSIGMOD . 489–504.\n[31]YaliangLi,DaoyuanChen,BolinDing,KaiZeng,andJingrenZhou.2021. APlug-\ngable Learned Index Method via Sampling and Gap Insertion. arXiv:2101.00808\n[32]Xuanqing Liu, Si Si, Xiaojin Zhu, Yang Li, and Cho-Jui Hsieh. 2019. A Uni/f_ied\nFrameworkforDataPoisoningAttacktoGraph-basedSemi-supervisedLearning.\narXiv:1910.14147\n[33]Lin Ma, Dana Van Aken, Ahmed Hefny, Gustavo Mezerhane, Andrew Pavlo, and\nGeoﬀreyJ.Gordon.2018. Query-basedWorkloadForecastingforSelf-Driving\nDatabaseManagementSystems. In Proc. ofACMSIGMOD . 631–645.\n[34]Shiqing Ma and Yingqi Liu. 2019. NIC: Detecting Adversarial Samples with\nNeural NetworkInvariant Checking. In Proc. ofthe NDSS .\n[35]Stephen Macke, Alex Beutel, Tim Kraska, Maheswaran Sathiamoorthy,\nDerek Zhiyuan Cheng, and Ed H. Chi. 2018. Lifting the Curse of Multidimen-\nsionalDatawithLearnedExistenceIndexes.In WorkshoponMLforSystemsat\nNeurIPS.\n[36]HongziMao,MalteSchwarzkopf,ShaileshhBojjaVenkatakrishnan,ZiliMeng,\nandMohammadAlizadeh.2019. LearningSchedulingAlgorithmsforDataPro-\ncessing Clusters.In Proc. ofACMSIGCOMM . 270–288.[37]Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra,\nAlfonsKemper,ThomasNeumann,andTimKraska.2020.BenchmarkingLearned\nIndexes. arXiv:2006.12804\n[38]Florida Miami-Dade County. 2019. Employee Pay Information . https://gis-\nmdc.opendata.arcgis.com/datasets/employee-pay-information/data.\n[39]David J. Miller and Hasan S. Uyar. 1996. A Mixture of Experts Classi/f_ier with\nLearning Based on Both Labelled and Unlabelled Data. In Proc. of NeurIPS .\n571–577.\n[40]KennethS.Miller.1966. AnIntroductiontotheCalculusofFiniteDiﬀerencesand\nDiﬀerence Equations, by KennethS.Miller . Dover Publications,NewYork.\n[41]MichaelMitzenmacher.2018. AModelforLearnedBloomFiltersandOptimizing\nby Sandwiching. In Proc. ofNeurIPS . 464–473.\n[42]Douglas C. Montgomery.2006. Design and AnalysisofExperiments . JohnWiley\n& Sons,Inc.,Hoboken, NJ, USA.\n[43]Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ningMulti-DimensionalIndexes. In Proc. ofACMSIGMOD . 985–1000.\n[44]OpenStreetMap contributors. 2017. Planet dump retrieved from\nhttps://planet.osm.org . https://www.openstreetmap.org.\n[45]Jack W. Rae, Sergey Bartunov, and Timothy P. Lillicrap. 2019. Meta-Learning\nNeural Bloom Filters. In Proc. ofICML , Vol. 97.5271–5280.\n[46]AhmedSalem, Yang Zhang, Mathias Humbert,PascalBerrang, Mario Fritz,and\nMichael Backes. 2019. ML-Leaks: Model and Data Independent Membership\nInference Attacks and Defenses on Machine Learning Models. In Proc. of NDSS .\n[47]Naufal Fikri Setiawan, Benjamin I. P. Rubinstein, and Renata Borovica-Gajic.\n2020. FunctionInterpolationforLearnedIndexStructures.In ADC-Databases\nTheoryand Applications . Springer, 68–80.\n[48]Benjamin Spector, Andreas Kipf, Kapil Vaidya, Chi Wang, Umar Farooq Minhas,\nandTimKraska.2021. BoundingtheLastMile:EﬃcientLearnedStringIndexing.\narXiv:2111.14905\n[49]Mihail Stoian, Andreas Kipf, Ryan Marcus, and Tim Kraska. 2021. Towards\nPracticalLearned Indexing. arXiv:2108.05117\n[50]Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daume III, and Tudor\nDumitras.2018. WhenDoesMachineLearningFAIL?GeneralizedTransferability\nfor Evasion and PoisoningAttacks. In Proc. ofUSENIXSecurity . 1299–1316.\n[51]MingjieSun,JianTang,HuichenLi,BoLi,ChaoweiXiao,YaoChen,andDawn\nSong. 2018. Data Poisoning Attack against Unsupervised Node Embedding\nMethods. arXiv:1810.12881\n[52]Chuzhe Tang, Zhiyuan Dong, Minjie Wang, Zhaoguo Wang, and Haibo Chen.\n2019. Learned Indexesfor Dynamic Workloads. arXiv:1902.00655\n[53]KapilVaidya,EricKnorr,TimKraska,andMichaelMitzenmacher.2020. Parti-\ntioned Learned Bloom Filter. arXiv:2006.03176\n[54]Dana Van Aken, Andrew Pavlo, Geoﬀrey J. Gordon, and Bohan Zhang. 2017.\nAutomatic Database Management System Tuning Through Large-Scale Machine\nLearning. In Proc. ofACMSIGMOD . 1009–1024.\n[55]PeterVanSandt,YannisChronis,andJigneshM.Patel.2019. EﬃcientlySearching\nIn-MemorySorted Arrays:Revengeof theInterpolationSearch?.In Proc.ofACM\nSIGMOD. 36–53.\n[56]Giorgio Vinciguerra, Paolo Ferragina, and Michele Miccinesi. 2019. Superseding\nTraditional Indexes by Orchestrating Learning and Geometry. arXiv:1903.00507\n[57]B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y. Zhao. 2019.\nNeuralCleanse:IdentifyingandMitigatingBackdoorAttacksinNeuralNetworks.\nInProc. ofthe 40thIEEE S&P . 707–723.\n[58]Yingjun Wu, Jia Yu, Yuanyuan Tian, Richard Sidle, and Ronald Barber. 2019.\nDesigning Succinct Secondary Indexing Mechanism by Exploiting Column Cor-\nrelations. In Proc. ofACMSIGMOD . 1223–1240.\n[59]WenkunXiang,HaoZhang,RuiCui,XingChu,KeqinLi,andWeiZhou.2019.\nPavo: A RNN-Based Learned Inverted Index, Supervised or Unsupervised? IEEE\nAccess7 (2019), 293–303.\n[60]Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, and\nFabioRoli.2015. IsFeatureSelectionSecureagainstTrainingDataPoisoning?.\nInProc. ofthe 32nd ICML , Vol. 37.1689–1698.\n[61]Chaofei Yang, Qing Wu, Hai Li, and Yiran Chen. 2017. Generative Poisoning\nAttack Method AgainstNeural Networks. arXiv:1703.01340\n14\n\nA APPENDIX\nA.1 AdditionalIllustrationoftheProposed Poisoning Attackon CDFs\nWe show in Figure 8 an applicationof the poisoning attackon CDFusing 10 poisoning keyson adata setof 90 uniformly distributed keys.\nOurattack increases theerror by 7.4×.Each pointcontributesto theoverall error by its distance to theregression line,indicated with blue\nvertical lines. Poisoning keys, colored red, are clustered on dense areas so as to exacerbate the non-linearity of the poisoned CDF. Intuitively,\nthe attack creates non-linearity by “overpopulating” the dense area of the key values in dataset. As a result, it is hard to distinguish the\npoisoningkeys from existing keys.\n0 40 80 120 160 200 240 280 320 360 400 440 480\nKeys04812162024283236404448525660646872768084889296100 RankRegression Before Poisoning\nOriginal Regression\n0 40 80 120 160 200 240 280 320 360 400 440 480\nKeys04812162024283236404448525660646872768084889296100 RankRegression After Poisoning\nOriginal Regression\nPoisoned Regression\nFigure 8:Example ofpoisoning attack using10poisoning keyson adata set of90uniformlydistributed keys.\nA.2 Evidence ofOptimality forConjecture1\nRecallthat the lossfunction /u1D43F(/u1D458/u1D45D)is:\n/u1D43F(/u1D458/u1D45D)=min\n/u1D464,/u1D44F(/u1D45B+1/summationdisplay.1\n/u1D456=1(/u1D464/u1D458/u1D456+/u1D44F−/u1D45F/u1D456)2)=min\n/u1D464,/u1D44F(/summationdisplay.1\n/u1D456≠/u1D45D((/u1D464/u1D458/u1D456+/u1D44F−/u1D45F/u1D456)2)+(/u1D464/u1D458/u1D45D+/u1D44F−/u1D45F/u1D45D)2)\nTheobjectiveis/f_indingthe /u1D458/u1D45Dthatmaximizestheloss.Although /u1D464and/u1D44Fareresultsofminimizationproblemthatdependson /u1D458/u1D45D,we\ndenotethemas variable inthe following de/f_initionofLoss:\n/u1D43F′(/u1D458/u1D45D,/u1D464,/u1D44F)=/u1D45B/summationdisplay.1\n/u1D456=1(/u1D464/u1D458/u1D456+/u1D44F−/u1D45F/u1D456)2=/summationdisplay.1\n/u1D456≠/u1D45D(/u1D464/u1D458/u1D456+/u1D44F−/u1D45F/u1D456)2\n15\n\nTo prove/u1D43Fconvex over /u1D458/u1D45D, it is suﬃcient to prove that /u1D43F′is convex because /u1D43F′is a minimization problem of /u1D43F. For all/u1D456≠/u1D45D,(/u1D464/u1D458/u1D456+/u1D44F−/u1D45F/u1D456)2\nisaquadraticfunction andisthusconvex.Therefore,only (/u1D464/u1D458/u1D45D+/u1D44F−/u1D45F/u1D45D)2ispotentiallynon-convex( /u1D464and/u1D458/u1D45Darebothvariables).While\nthis term is not strictly convex for any {/u1D464,/u1D458/u1D45D,/u1D44F}, we observe from experiment that the non-convexity of this term does not overwhelm the\nrest/u1D45B−1convex terms. Consider:\n/u1D43F′(/u1D458/u1D45D,/u1D464,/u1D44F)+/u1D43F′(/u1D458/u1D45D+2,/u1D464+2/u1D716/u1D464,/u1D44F+2/u1D716/u1D44F)−2/u1D43F′(/u1D458/u1D45D+1,/u1D464+/u1D716/u1D464,/u1D44F+/u1D716/u1D44F),\nwhere/u1D716/u1D464and/u1D716/u1D44Fare small gradient values.After simplifying we have:\n(/summationdisplay.1\n/u1D456≠/u1D45D2(/u1D716/u1D464/u1D458/u1D456+/u1D716/u1D44F)2)+4/u1D716/u1D464((/u1D464+/u1D716/u1D464)(/u1D458/u1D45D+1)+(/u1D44F+/u1D716/u1D44F)−/u1D45F/u1D45D)+6/u1D7162\n/u1D464\nSince/u1D716/u1D464isasmallvalueand (/u1D464+/u1D716/u1D464)(/u1D458/u1D45D+1)+(/u1D44F+/u1D716/u1D44F)−/u1D45F/u1D45Dthelossofpoisoningkeyboundedby /u1D45D/u1D45C/u1D459/u1D466.alt(/u1D43F),thesecondtermisoverpowered\nbythe squaredpositive terms. Therefore,the lossisvery likely convex.\nA.3EvaluationofMax MemoryOﬀsetofUniformly Distributed Data (Regression)\nSetup.The setup is identical to the experiment in Section 4. Insteadof measuring the average memory oﬀset for each dataset, we measure\nthemaximum memory oﬀsetover allqueriesonthe keys that present inthe dataset.\nResults. Similartoaveragememoryoﬀset,maximummemoryoﬀsetgrowslinearlywiththesizeofthedatasetasshowninFigure9.In\ncomparison,themaximummemoryoﬀsetisroughlytwotimestheaveragememoryoﬀsetgiventhesamesetup,whichimpliesthatthe\nskewofthe distributionofmemory oﬀsets isclose to center.\n 3  6  9 12 15\nPoisoning Percentage48121620Max Memory OffsetKeys: 100 Density:5%\n 3  6  9 12 15\nPoisoning Percentage48121620Max Memory OffsetKeys: 100 Density:10%\n 3  6  9 12 15\nPoisoning Percentage48121620Max Memory OffsetKeys: 100 Density:50%\n 3  6  9 12 15\nPoisoning Percentage48121620Max Memory OffsetKeys: 100 Density:80%\n 3  6  9 12 15\nPoisoning Percentage12243648Max Memory OffsetKeys: 500 Density:5%\n 3  6  9 12 15\nPoisoning Percentage12243648Max Memory OffsetKeys: 500 Density:10%\n 3  6  9 12 15\nPoisoning Percentage12243648Max Memory OffsetKeys: 500 Density:50%\n 3  6  9 12 15\nPoisoning Percentage12243648Max Memory OffsetKeys: 500 Density:80%\n 3  6  9 12 15\nPoisoning Percentage20406080100Max Memory OffsetKeys: 1000 Density:5%\n 3  6  9 12 15\nPoisoning Percentage20406080100Max Memory OffsetKeys: 1000 Density:10%\n 3  6  9 12 15\nPoisoning Percentage20406080100Max Memory OffsetKeys: 1000 Density:50%\n 3  6  9 12 15\nPoisoning Percentage20406080100Max Memory OffsetKeys: 1000 Density:80%\n 3  6  9 12 15\nPoisoning Percentage100200300400500Max Memory OffsetKeys: 5000 Density:5%\n 3  6  9 12 15\nPoisoning Percentage100200300400500Max Memory OffsetKeys: 5000 Density:10%\n 3  6  9 12 15\nPoisoning Percentage100200300400500Max Memory OffsetKeys: 5000 Density:50%\n 3  6  9 12 15\nPoisoning Percentage100200300400500Max Memory OffsetKeys: 5000 Density:80%\nFigure 9: Evaluation of the multi-point poisoning for linear regression on CDF. Each boxplot shows the maximum model\nprediction memory oﬀset of legitimate key queries over 20 distinct keysets. The legitimate keys have normal distribution.\nThenumberoflegitimatekeys,denotedas“Keys”andratiobetweenthenumberoflegitimatekeysandthesizeofthekey\ndomain, denoted as “Density” are presented on the title of each plot. The number of poisoning keys varies on the /u1D44B-axis and is\nrepresented as apoisoning percentagewith respectto thenumberoflegitimatekeys.\n16\n\nFigure10:Evaluationofthemulti-pointpoisoningforlinearregressiononCDF.Eachboxplotshowstheratiooftheevaluation\noftheMSElossonthepoisonedkeysetovertheevaluationoftheMSElossonthelegitimatekeysetover20runs.Thelegitimate\nkeysareinnormaldistribution.Thenumberofpoisoningkeysvariesonthe /u1D44B-axisandisrepresentedasapoisoningpercentage\nwith respectto thenumberoflegitimatekeys.\nA.4 Experiments on Synthetic Data from a Normal Distribution(Regression)\nSetup.ThesetupisidenticaltotheexperimentinSection4butonanormaldistributeddataset.Speci/f_ically,forakeydomain /u1D448=[/u1D6FC,/u1D6FD]we\nform the normal distributionwithmean /u1D707=/u1D6FD+/u1D6FC\n2andstandarddeviation /u1D70E=/u1D6FD−/u1D6FC\n3.\nResults. The resultsare presentedinFigure 10. Because thenormal distributions are not capturedwell by linearmodels, theMSE lossof\nthe originaldataset ishigh.Despitethat, our attackachieves upto 8×increaseoferror.\n17\n\nOpenStreetMap Locations5% 10% 20%\nPoisoning Percentage05101520Ratio LossKeys: 302k  Model Size: 50  #Models: 6,059\n5% 10% 20%\nPoisoning Percentage01020406080100Ratio LossKeys: 302k  Model Size: 100  #Models: 3,029\n5% 10% 20%\nPoisoning Percentage01020406080100Ratio LossKeys: 302k  Model Size: 200  #Models: 1,514\n1% 5% 10%\nPoisoning Percentage012345678910Average Memory OffsetTotal Keys: 302k; Model Size: 50 #Models: 6059\nBefore Poisoning\nAfter Poisoning\n1% 5% 10%\nPoisoning Percentage02468101214161820Average Memory OffsetTotal Keys: 302k; Model Size: 100 #Models: 3029\nBefore Poisoning\nAfter Poisoning\n1% 5% 10%\nPoisoning Percentage036912151821242730Average Memory OffsetTotal Keys: 302k; Model Size: 200 #Models: 1514\nBefore Poisoning\nAfter Poisoning\n024681 0 1 2\nKeys - Latitudes (Scaled and rounded) 1050123Rank105Cumulative Density Function (CDF) of Latitudes of School Locations (Scaled) Keys: 302k  Key Domain: 1.2M  Density: 25.25%\nFigure 11: Evaluation of the multi-point poisoning for RMI applied on the CDF of latitudes of schools in OpenStreetMap,\n/f_iltered by latitudes between -30 and +50. The /u1D44B-axis represents diﬀerent overall poisoning percentage where the second-stage\npoisoning threshold /u1D6FCtakesvalue /u1D6FC=3.The third row presentstheCDF.\nA.5 Evaluationon Real-WorldData:OpenStreetMap Geolocations\nSetup.Weusethepubliclyavailablegeolocationdataset[ 44]andindexbythelatitudesoflocations.Wepickthedatapointsthatarelabelled\nas schools over the world with latitudes between -30 and +50; and scale up the latitudes by 15,000 before rounding to achieve uniqueness of\nkeys. The /f_inal dataset has /u1D45B=302,973keys in a key universe of /u1D45A=1,200,000, which yields a 25%density. The cumulative distribution is\ndepictedinFigure 11.\nInthis experiment,wetestthree diﬀerentRMImodel setups.Inthe /f_irstsetup,weinitialize thesecond-stagemodels with50keyseach,\nwhichtranslatesto6,059second-stagemodels.Inthesecondsetup,weinitializethesecond-stagewith100keyseach,whichtranslatesto\n3,029models.In thethirdsetup,weinitializethesecond-stagewith 200keyseach,which translatesto1,514models. Forallthesetups, we\nconsider the poisoning threshold of the second stage model to have parameter /u1D6FC=3. Finally, we considered poisoning percentages 5%,10%,\nand20%.\nResults. TheresultsofourevaluationarepresentedinFigure11.Fora/f_ixedsetup,weobservethatasthepoisoningpercentageincreases,the\nratiolossincreasesaswell.Amongallexperimentsinbothdatasets,theRMIlossincreasesbetween 4×to24×.Sincethenumberofpoisoning\nkeyspersecond-stagemodelisapercentageoveritsnumberofkeys,weseethatlargermodelsallowmorepoisoningand,consequently,\nlarger RMI error. The discrepancy between the eﬀectiveness of the attack on synthetic data and the real data is partly explained by the fact\nthat the syntheticdataset isfourorders ofmagnitude larger thanthe real dataset.\n18",
  "textLength": 100163
}