{
  "paperId": "53b25819e28e2ea286ab21946f31383243635151",
  "title": "Are We Ready For Learned Cardinality Estimation?",
  "pdfPath": "53b25819e28e2ea286ab21946f31383243635151.pdf",
  "text": "Are We Ready For Learned Cardinality Estimation?\nXiaoying Wang†∗, Changbo Qu†∗, Weiyuan Wu†∗, Jiannan Wang†, Qingqing Zhou♦\nSimon Fraser University†Tencent Inc.♦\n{xiaoying_wang, changboq, youngw, jnwang}@sfu.ca hewanzhou@tencent.com\nABSTRACT\nCardinality estimation is a fundamental but long unresolved prob-\nlem in query optimization. Recently, multiple papers from diﬀerent\nresearch groups consistently report that learned models have the\npotential to replace existing cardinality estimators. In this paper,\nwe ask a forward-thinking question: Are we ready to deploy these\nlearned cardinality models in production? Our study consists of three\nmain parts. Firstly, we focus on the static environment (i.e., no data\nupdates) and compare ﬁve new learned methods with nine tradi-\ntional methods on four real-world datasets under a uniﬁed workload\nsetting. The results show that learned models are indeed more ac-\ncurate than traditional methods, but they often suﬀer from high\ntraining and inference costs. Secondly, we explore whether these\nlearned models are ready for dynamic environments (i.e., frequent\ndata updates). We ﬁnd that they cannot catch up with fast data up-\ndates and return large errors for diﬀerent reasons. For less frequent\nupdates, they can perform better but there is no clear winner among\nthemselves. Thirdly, we take a deeper look into learned models and\nexplore when they may go wrong. Our results show that the perfor-\nmance of learned methods can be greatly aﬀected by the changes\nin correlation, skewness, or domain size. More importantly, their\nbehaviors are much harder to interpret and often unpredictable.\nBased on these ﬁndings, we identify two promising research direc-\ntions (control the cost of learned models and make learned models\ntrustworthy) and suggest a number of research opportunities. We\nhope that our study can guide researchers and practitioners to worktogether to eventually push learned cardinality estimators into real\ndatabase systems.\nPVLDB Reference Format:\nXiaoying Wang, Changbo Qu, Weiyuan Wu, Jiannan Wang, Qingqing Zhou.\nAre We Ready For Learned Cardinality Estimation?. PVLDB, 14(9): 1640 -\n1654, 2021.\ndoi:10.14778/3461535.3461552\nPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nhttps://github.com/sfu-db/AreCELearnedYet.\n1 INTRODUCTION\nThe rise of “ML for DB” has sparked a large body of exciting research\nstudies exploring how to replace existing database components with\nlearned models [ 32,37,39,68,84,98]. Impressive results have been\nrepeatedly reported from these papers, which suggest that “ML for\nDB” is a promising research area for the database community to\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 14, No. 9 ISSN 2150-8097.\ndoi:10.14778/3461535.3461552\n* The ﬁrst three authors contributed equally to this research.explore. To maximize the impact of this research area, one natural\nquestion that we should keep asking ourselves is: Are we ready to\ndeploy these learned models in production?\nIn this paper, we seek to answer this question for cardinality\nestimation. In particular, we focus on single-table cardinality esti-\nmation, a fundamental and long standing problem in query opti-\nmization [ 18,95]. It is the task of estimating the number of tuples\nof a table that satisfy the query predicates. Database systems use\na query optimizer to choose an execution plan with the estimated\nminimum cost. The performance of a query optimizer largely de-\npends on the quality of cardinality estimation. A query plan based\non a wrongly estimated cardinality can be orders of magnitude\nslower than the best plan [42].\nMultiple recent papers [ 18,28,30,34,95] have shown that learned\nmodels can greatly improve the cardinality estimation accuracy\ncompared with traditional methods. However, their experimentshave a number of limitations (see Section 2.5 for more detailed\ndiscussion). Firstly, they do not include all the learned methods in\ntheir evaluation. Secondly, they do not use the same datasets and\nworkload. Thirdly, they do not extensively test how well learned\nmethods perform in dynamic environments (e.g., by varying update\nrate). Lastly, they mainly focus on when learned methods will go\nright rather than when they may go wrong.\nWe overcome these limitations and conduct comprehensive ex-\nperiments and analyses. The paper makes four contributions:\nAre Learned Methods Ready For Static Environments? We\npropose a uniﬁed workload generator and collect four real-world\nbenchmark datasets. We compare ﬁve new learned methods with\nnine traditional methods using the same datasets and workload in\nstatic environments (i.e., no data updates). The results on accuracy\nare quite promising. In terms of training/inference time, there isonly one method [\n18] that can achieve similar performance with\nexisting DBMSs. The other learned methods typically require 10 −\n1000×more time in training and inference. Moreover, all learned\nmethods have an extra cost for hyper-parameter tuning.\nAre Learned Methods Ready For Dynamic Environments?\nWe explore how each learned method performs by varying up-\ndate rate on four real-world datasets. The results show that learned\nmethods fail to catch up with fast data updates and tend to re-\nturn large error for various reasons (e.g., the stale model processes\ntoo many queries, the update period is not long enough to get a\ngood updated model). When data updates are less frequent, learned\nmethods can perform better but there is no clear winner among\nthemselves. We further explore the update time vs. accuracy trade-\noﬀ, and investigate how much GPU can help learned methods in\ndynamic environments.\nWhen Do Learned Methods Go Wrong? We vary correlation,\nskewness, and domain size, respectively, on a synthetic dataset, and\ntry to understand when learned methods may go wrong. We ﬁnd\nthat all learned methods tend to output larger error on more cor-\nrelated data, but they react diﬀerently w.r.t. skewness and domain\nsize. Due to the use of black-box models, their wrong behaviors are\n1640\n\n\nTable 1: Taxonomy of New Learned Cardinality Estimators.\nMethodology Input Model\nMSCN [34] Regression Query+Data Neural Network\nLW-XGB [18] Regression Query+Data Gradient Boosted Tree\nLW-NN [18] Regression Query+Data Neural Network\nDQM-Q [28] Regression Query Neural Network\nNaru [95] Joint Distribution Data Autoregressive Model\nDeepDB [30] Joint Distribution Data Sum Product Network\nDQM-D [28] Joint Distribution Data Autoregressive Model\nvery hard to interpret. We further investigate whether their behav-\niors follow some simple and intuitive logical rules. Unfortunately,\nmost of them violate these rules. We discuss four issues related to\ndeploying (black-box and illogical) learned models in production.\nResearch Opportunities. We identify two future research direc-\ntions: i) control the cost of learned methods and ii) make learned\nmethods trustworthy, and suggest a number of promising research\nopportunities. We publish our code and datasets on GitHub1to\nfacilitate future research studies. We hope our work can attract\nmore research eﬀorts in these directions and eventually overcome\nthe barriers of deploying learned estimators in production.\nThe rest of the paper is organized as follows: We present a sur-\nvey on learned cardinality estimation in Section 2 and describe\nthe general experimental setup in Section 3. We explore whether\nlearned methods are ready for static environments in Section 4\nand for dynamic environments in Section 5, and examine when\nlearned methods go wrong in Section 6. Future research opportuni-\nties are discussed in Section 7. Multi-table scenario are discussed\nin Section 8 and related works are reviewed in Section 9. Finally,\nwe present our conclusions in Section 10.\n2 LEARNED CARDINALITY ESTIMATION\nIn this section, we ﬁrst formulate the cardinality estimation (CE)\nproblem, then put new learned methods into a taxonomy and\npresent how each method works, and ﬁnally discuss the limita-\ntions of existing evaluation on learned methods.\n2.1 Problem Statement\nConsider a relation \u0001푅with \u0001푛attributes {\u0001퐴1,..., \u0001퐴\u0001푛}and a query\nover \u0001푅with a conjunctive of \u0001푑predicates:\nSELECT COUNT(*) FROM R\nWHERE \u0001휃1AND ···and \u0001휃\u0001푑,\nwhere \u0001휃\u0001푖(\u0001푖∈[1,\u0001푑]) can be an equality predicate like \u0001퐴=\u0001푎, an\nopen range predicate like \u0001퐴≤\u0001푎, or a close range predicate like\n\u0001푎≤\u0001퐴≤\u0001푏. The goal of CE is to estimate the answer to this query,\ni.e., the number of tuples in \u0001푅that satisfy the query predicates. An\nequivalent problem is called selectivity estimation, which computes\nthepercentage of tuples that satisfy the query predicates.\n2.2 Taxonomy\nThe idea of using ML for CE is not new (see Section 9 for more\nrelated work). The novelty of recent learned methods is to adopt\nmore advanced ML models, such as deep neural networks [ 18,28,\n34], gradient boosted trees [ 18], sum-product networks [ 30], and\ndeep autoregressive models [ 28,95]. We call these methods “new\nlearned methods” or “learned methods” if the context is clear. In\ncontrast, we refer to “traditional methods” as the methods based on\nhistogram or classic ML models like KDE and Bayesian Network.\n1https://github.com/sfu-db/AreCELearnedYetData ProcessingQuery FeaturizationRegression Model\nQuery Pool \n+ LabelsQueryGreen: TrainStag e Blue : Inference Stage\nConstructTrainCE\nResult\nStatistics\nDataQuery ProcessingCE\nResult\nTrainQuery\n(a)Regression Methods (b)Joint Distribution MethodsLook Up\nStatistics: Optional\nDataJoint Distribution Model\nFigure 1: Workﬂow of Learned Methods.\nTable 1 shows a taxonomy of new learned methods2. Based on\nthe methodology, we split them into two groups - Regression and\nJoint Distribution methods. Regression methods (a.k.a query-driven\nmethods) model CE as a regression problem and aim to build a map-\nping between queries and the CE results via feature vectors, i.e.,\n\u0001푞\u0001푢\u0001푒\u0001푟\u0001푡u\u0001푐D\u0001퐸\u0001푙\u0001푙\u0001푓alt →\u0001푓 \u0001푒\u0001푎\u0001푡\u0001푢\u0001푟\u0001푒 _\u0001푣\u0001푒\u0001푐\u0001푡\u0001표\u0001푟 →\u0001퐶\u0001퐸_\u0001푟\u0001푒\u0001푠\u0001푢\u0001푙\u0001푡 .Joint Distribution methods\n(a.k.a data-driven methods) model CE as a joint probability distribu-\ntion estimation problem and aim to construct the joint distribution\nfrom the table, i.e., \u0001푃(\u0001퐴1,\u0001퐴2,··· ,\u0001퐴\u0001푛), then estimate the cardinal-\nity. The Input column represents what is the input to construct\neach model. Regression methods all require queries as input while\njoint distribution methods only depend on data. The Model column\nindicates which type of model is used correspondingly. We will\nintroduce these methods in the following.\n2.3 Methodology 1: Regression\nWorkﬂow. Figure 1(a) depicts the workﬂow of regression methods.\nIn the training stage, it ﬁrst constructs a query pool and gets the\nlabel (CE result) of each query. Then, it goes through the query\nfeaturization module, which converts each query to a feature vector.\nThe feature vector does not only contain query information but\nalso optionally include some statistics (like a small sample) from\nthe data. Finally, a regression model is trained on a set of /angbracketleftfeature\nvector, label /angbracketrightpairs. In the inference stage, given a query, it converts\nthe query to a feature vector using the same process as the training\nstage, and applies the regression model to the feature vector to get\nthe CE result. To handle data updates, regression methods need to\nupdate the query pool and labels, generate new feature vectors, and\nupdate the regression model.\nThere are four regression methods: MSCN, LW-XGB, LW-NN,\nandDQM-Q . One common design choice in them is the usage\nof log-transformation on the selectivity label since the selectivity\noften follows a skewed distribution and log-transformation is com-\nmonly used to handle this issue [ 19]. These works vary from many\nperspectives, such as their input information, query featurization,\nand model architecture.\nMSCN [34] introduces a specialized deep neural network model\ntermed multi-set convolutional network (MSCN). MSCN can sup-\nport join cardinality estimation. It represents a query as a feature\nvector which contains three modules (i.e., table, join, and predicate\nmodules). Each module is a two-layer neural network and diﬀerent\nmodule outputs are concatenated and fed into a ﬁnal output net-\nwork, which is also a two-layer neural network. MSCN enriches the\ntraining data with a materialized sample. A predicate will be evalu-\nated on a sample, and a bitmap, where each bit indicates whether\na tuple in the sample satisﬁes the predicate or not, will be added\n2Naru, DeepDB andMSCN are named by their authors. For convenience of discussion,\nwe give others the following short names. Lightweight Gradient Boosting Tree (LW-\nXGB) and Lightweight Neural Network (LW-NN) are two models from [ 18]. From [ 28],\ntwo complementary methods are proposed, Data&Query Model - Data (DQM-D) and\nData&Query Model - Query (DQM-Q ).\n1641\n\n\nMSCNNaru\nLW-XGB/NN\nDeepDB DQM -D/Q\nFigure 2: Comparison results available in existing studies.\nto the feature vector. This enrichment has been proved to make\nobvious positive impact on the model performance [34, 95].\nLW-XGB/NN [18] introduces a lightweight selectivity estimation\nmethod. Its feature vector consists of two parts: range features +\nCE features. The range features represent a set of range predicates:\n/angbracketleft\u0001푎1,\u0001푏1,\u0001푎2,\u0001푏2,··· ,\u0001푎\u0001푛,\u0001푏\u0001푛/angbracketright. The CE features represent heuristic es-\ntimators (e.g., the one that assumes all columns are independent).\nNote that the CE features can be cheaply derived from the statistics\navailable in the database system. LW-NN (LW-XGB) train a neural\nnetwork (gradient boost tree) model using the generated features.\nUnlike MSCN which minimizes the mean q-error, they minimize\nthe mean square error (MSE) of the log-transformed label, which\nequals to minimizing the geometric mean of q-error with more\nweights on larger errors and also can be computed eﬃciently.\nDQM-Q [28] proposes a diﬀerent featurization approach. It uses\none-hot encoding to encode categorical columns and treats nu-\nmerical attributes as categorical attributes by automatic discretiza-\ntion [ 15].DQM-Q trains a neural network model. When a real-\nworld query workload is available, DQM-Q is able to augment the\ntraining set and train the model with the augmented set.\n2.4 Methodology 2: Joint Distribution\nWorkﬂow . Figure 1(b) depicts the workﬂow of joint distribution\nmethods. In the training stage, it transforms the data into a format\nready for training a joint distribution model. In the inference stage,\ngiven a query, it generates one or multiple requests to the model\nand combine the model inference results into the ﬁnal CE result.\nTo handle data updates, joint distribution methods need to update\nor retrain the joint distribution model.\nThere are three joint distribution methods: Naru, DeepDB, and\nDQM-D. Compared to traditional methods like histogram and sam-\npling, these new methods adopt more complex models to further\ncapture additional information in the data, such as ﬁne-grained\ncorrelation or conditional probability between columns.\nAutoregressive Model. Naru [95] and DQM-D [28] propose simi-\nlar ideas. They factorize the joint distribution into conditional distri-butions using the product rule:\n\u0001푃(\u0001퐴1,\u0001퐴2,. . . ,\u0001퐴\u0001푛)=\u0001푃(\u0001퐴1)\u0001푃(\u0001퐴2|\u0001퐴1)·\n··\u0001푃(\u0001퐴\u0001푛|\u0001퐴1,. . . ,\u0001퐴\u0001푛−1). They adopt the state-of-the-art deep autore-\ngressive models such as MADE [ 23] and Transformer [ 89] to ap-\nproximate the joint distribution.\nThe joint distribution can directly return results to point queries.\nTo support range queries, they adopt a sampling based method,\nwhich runs importance sampling in an adaptive fashion. Speciﬁcally,\nNaru uses a novel approximation technique named progressive\nsampling, which samples values column by column according to\neach internal output of conditional probability distribution. DQM-\nDadopts an algorithm [ 44] originally designed for Monte-Carlo\nmulti-dimensional integration, which conducts multiple stages of\nsampling. At each stage, it selects sample points in proportion to\nthe contribution they make to the query cardinality according to\nthe result from the previous stage.\nSum-Product Network. DeepDB [30] builds Sum-Product Net-\nworks (SPNs) [ 72] to capture the joint distribution. The key idea is\nto recursively split the table into diﬀerent clusters of rows (creating\na sum node to combine them) or clusters of columns (assuming dif-\nferent column clusters are independent and creating a product nodeTable 2: Workload used in existing experimental studies.\nPredicate Operator Consider\nNumber Equal Range OOD\nMSCN 0∼|\u0001퐷| /check/check ×\nLW-XGB/NN 2∼|\u0001퐷| × close range /check\nNaru 5∼11 /check open range /check\nDeepDB 1∼5 /check/check ×\nDQM-D/Q 1∼|\u0001퐷| /check × /check\nOur Workload 1∼|\u0001퐷| /check/check /check\nto combine them). KMeans is used to cluster rows and Random-\nized Dependency Coeﬃcients [ 50] is used to identify independent\ncolumns. Leaf nodes in an SPN represent a single attribute dis-\ntribution, which can be approximated by histograms for discrete\nattributes or piecewise linear functions for continuous attributes.\n2.5 Limitations of Existing Experiments\nAs pointed in the Introduction, existing experimental studies have\na number of limitations. We provide more detail in this section.\nFirstly, many new learned methods have not been compared with\neach other directly. Figure 2 visualizes the available comparison\nresults using a directed graph. Each node represents a method, and\nif method A has compared with method B in A’s paper, we draw\na directed edge from A to B. Since many methods were proposed\nin the same year or very close period, the graph is quite sparse\nand misses over half of the edges. For example, LW-XGB/NN is\none of the best regression methods, but it has no edge with any\nother method. DeepDB andNaru are two state-of-the-art joint\ndistribution methods, but there is no edge between them.\nSecondly, there is no standard about which datasets to use and\nhow to generate workloads. Other than the IMDB dataset (adopted\nbyMSCN andDeepDB), none of the datasets adopted in one work\nappear in another work. As for workloads, these works generate\nsynthetic queries diﬀerently. Table 2 compares their generated\nworkloads. For join queries in the JOB-light benchmark (used in\nMSCN andDeepDB), we report their properties related to single\ntable. |\u0001퐷|denotes the number of columns in the dataset and OOD\n(out-of-domain) means that the predicates of a query are generated\nindependently. Such queries often lead to zero cardinality.\nThirdly, existing works are mostly focused on the static environ-\nment (i.e., no data update setting). However, dynamic environments\nare also common in practice. Some papers have explored how their\nmethod performs when the data updates, but the way that they\nupdate the data varies. As a result, the performance numbers cannotbe used to compare between methods. Furthermore, existing studies\nhave not extensively explored the trade-oﬀ between accuracy and\nupdating time. For example, Naru is a more accurate method but\nrequires longer time to update the model. It is unclear whether it\ncan still give good accuracy for high update rates.\n3 EXPERIMENTAL SETUP\nOur study evaluates learned cardinality estimators under diﬀerent\nsettings. We describe the general setup used in all of our experi-\nments in this section.\nEvaluation Metric. We use q-error as our accuracy metric to mea-\nsure the quality of the estimation result. Q-error is a symmetric\nmetric which computes the factor by which an estimate diﬀers from\nthe actual cardinality: \u0001푒\u0001푟\u0001푟\u0001표\u0001푟 =\u0001푚\u0001푎\u0001푥 (\u0001푒\u0001푠\u0001푡 (\u0001푞),\u0001푎\u0001푐\u0001푡 (\u0001푞))\n\u0001푚\u0001푖\u0001푛 (\u0001푒\u0001푠\u0001푡(\u0001푞),\u0001푎\u0001푐\u0001푡 (\u0001푞)).For example, if\na query’s actual cardinality is 10 and estimated cardinality is 100,\nthen \u0001푒\u0001푟\u0001푟\u0001표\u0001푟 =\u0001푚\u0001푎\u0001푥 (100 ,10)\n\u0001푚\u0001푖\u0001푛 (100 ,10)=10.\n1642\n\n\nTable 3: Dataset characteristics. “Cols/Cat\" means the num-\nber of columns and categorical columns; “Domain\" is the\nproduct of the number of distinct values for each column.\nDataset Size(MB) Rows Cols/Cat Domain\nCensus [16] 4.8 49K 13/8 1016\nForest [16] 44.3 581K 10/0 1027\nPower [16] 110.8 2.1M 7/0 1017\nDMV [62] 972.8 11.6M 11/10 1015\nQ-error is the metric adopted by all learned methods [ 18,28,30,\n34,95]. It measures the relative error, which can penalize large and\nsmall results to the same extent. Furthermore, it has been proved\nto be directly related to the plan quality in query optimization [ 59].\nLearned Methods & Implementation. As shown in Table 1, there\nareﬁve recently published papers on learned methods: Naru [95],\nMSCN [34],LW-XGB/NN [18],DeepDB [30], and DQM [28]. We\nexclude DQM from our study since its data driven model has a\nsimilar performance with Naru and its query driven model does\nnot support our workload (conﬁrmed with DQM’s authors).\nForNaru3andDeepDB4, we adopt the implementation released\nby the authors with minor modiﬁcations in order to support our\nexperiments. We choose ResMADE as basic autoregressive build-\ning block for Naru because it is both eﬃcient and accurate. For\nMSCN, since the original model supports join query, it needs extra\ninput features to indicate diﬀerent joins and predicates on diﬀerent\ntables. To ensure a fair comparison on single table cardinality es-\ntimation, we modify the original code5by only keeping features\nrepresent predicates and qualifying samples. We implement both\nneural network (LW-NN, on PyTorch [ 67]) and gradient boosted\ntree (LW-XGB , on XGBoost [ 10]) approach for LW-XGB/NN accord-\ning to the description in its original paper [ 18], and use Postgres’s\nestimation result on single column to compute the CE features.\nAll the code including dataset manipulation, workload genera-\ntion and estimator evaluation are released6.\nHardware and Platform. We perform our experiments on a server\nwith 16 Intel Xeon E7-4830 v4 CPUs (2.00GHz). For the neural net-\nwork models (Naru, MSCN, LW-NN), we run them not only on\nCPU but also on a NVIDIA Tesla P100 GPU to gain more insights\nunder diﬀerent settings.\n4 ARE LEARNED METHODS READY FOR\nSTATIC ENVIRONMENTS?\nAre learned estimators more accurate than traditional methods in\nstatic environment? What is the cost for the high accuracy? In this\nsection, we ﬁrst compare the accuracy of learned methods with\ntraditional methods, and then measure their training and inference\ntime in order to see whether they are ready for production.\n4.1 Setup\nDataset. We use four real-world datasets with various character-\nistics (Table 3). We choose these datasets because ﬁrst, the size of\nthese datasets are in diﬀerent magnitudes and the ratio between\ncategorical and numerical columns varies; second, each dataset has\nbeen used in the evaluation of at least one prior work in this ﬁeld.\nWorkload. We propose a uniﬁed workload generator. The goal\nof our workload generator is to be able to cover all the workload\n3https://github.com/naru-project/naru\n4https://github.com/DataManagementLab/deepdb-public\n5https://github.com/andreaskipf/learnedcardinalities\n6https://github.com/sfu-db/AreCELearnedYetFigure 3: Distribution of workload selectivity.\nsettings used in existing learned methods (see Table 2). We apply\nthe same generator setting on all datasets in the same experiment.\nIntuitively, a query with \u0001푑predicates can be thought of as a hyper-\nrectangle in a \u0001푑-dimensional space. A hyper-rectangle is controlled\nby its center and width. Correspondingly, a query is controlled by\nitsquery center and range width. For example, consider a query\nwith \u0001푑=2 predicates:\nSELECT COUNT(*) FROM R\nWHERE 0≤\u0001퐴1≤20AND 20≤\u0001퐴2≤100\nIts query center is (20−0\n2,100−20\n2)=(10 ,40)and its range width is\n(20−0, 100 −20) = (20 ,80).\nThere are two ways to generate query centers. For ease of illustra-\ntion, suppose that we want to generate a query center for columns\n\u0001퐴1,\u0001퐴2. The ﬁrst way ( 1) is to randomly select a tuple \u0001푡from the\ntable. Let \u0001푡[\u0001퐴1],\u0001푡[\u0001퐴2]denote the attribute values of the tuple on \u0001퐴1\nand\u0001퐴2. Then, we set the query center to ( \u0001푡[\u0001퐴1],\u0001푡[\u0001퐴2]). The second\nway ( 2) is to independently draw a random value \u0001푐1and\u0001푐2from\nthe domain of \u0001퐴1and\u0001퐴2, respectively, and set the query center to\n(\u0001푐1,\u0001푐2).2is called out-of-domain (OOD in Table 2), which aims to\ntest the robustness of learned estimators more comprehensively\nfrom the entire joint domain.\nThere are two ways to generate range widths. Let the domain for\n\u0001퐴\u0001푖be[min\u0001푖,max \u0001푖]and the domain size be size\u0001푖=max \u0001푖−min\u0001푖. The\nﬁrst way ( 1) is to uniformly select a value \u0001푤\u0001푖from [0,size\u0001푖]. The\nsecond way ( 2) is to select a value from an exponential distribution\nwith a parameter \u0001휆\u0001푖(we set \u0001휆=10/size \u0001푖by default). Note that if \u0001퐴\u0001푖\nis a categorical column, we will only generate an equality predicate\nfor it, thus the width is set to zero in this case. If a range on one\nside is larger than max \u0001푖or smaller than min\u0001푖, then it becomes an\nopen range query. Thus, our workload contains both open and close\nrange queries.\nOur workload generator covers all the above settings ( 1,2,\n1,2). To generate a query, we ﬁrst uniformly select a number \u0001푑\nfrom 1 to |\u0001퐷|and randomly sample \u0001푑distinct columns to place the\npredicates. The query center is generated from 1and 2with a\nprobability of 90% and 10%, respectively, and the range width is\ngenerated from 1and 2in equal proportions. The reason that we\ndo not use an equal probability for the query center is that OOD\nis typically less common than the other way in real workloads.\nFigure 3 shows the selectivity distribution of generated workloads\non diﬀerent datasets, which results in a broad spectrum.\nHyper-parameter Tuning. We describe hyper-parameter tuning\nfor each model. More details can be found in our Github repository.\nFor neural network methods (Naru, MSCN, LW-NN), we control\nthe model size within 1.5% of the data size for each dataset. For\neach method, we select four model architectures with diﬀerent\nnumbers of layers, hidden units, embedding size, etc. and train each\nmodel in diﬀerent batch size and learning rate in accordance with\nthe original papers. Since MSCN andLW-NN are query-driven\nmethods, we select 10K queries as a validation set to determine\nwhich hyper-parameters are better. Since Naru is a data-driven\nmethod (i.e., no query as input), we use training loss to ﬁnd optimal\nhyper-parameters.\nForLW-XGB, we vary the number of trees (16, 32, 64...) as in [ 18].\nSince LW-XGB is a query-driven method, similar to MSCN and\nLW-NN, we select 10K validation queries for it.\n1643\n\n\nForDeepDB, we do a grid search on RDC threshold andmini-\nmum instance slice and only keep the models within the size budget\n(i.e., 1.5% of the data size). An interesting ﬁnding is that DeepDB\ndoes not output the training loss like Naru during construction, thus\nqueries are needed for hyper-parameter tuning. However, DeepDB\nis designed to be a data-driven method, which is not supposed to\nuse queries. To ensure a fair comparison with other methods, we\nselect a very small number of validation queries (i.e., 100 queries)\nforDeepDB to do hyper-parameter tuning.\nTo ensure a fair comparison, we use 100K queries to train all the\nquery-driven methods (MSCN, LW-XGB/NN).\nTraditional Techniques. We compare with a variety of traditional\ntechniques, which are either used by real database systems or re-\nported to achieve the state-of-the-art performance recently. The\nmethods we chose can represent a wide range of solutions.\n•Postgres, MySQL andDBMS-A are used to represent the perfor-\nmance of real database systems. We use PostgreSQL 11.5 and\n8.0.21 MySQL Community Server, and DBMS-A is a leading com-\nmercial database system. They estimate cardinality rapidly with\nsimple statistics and assumptions. In order to let them achieve\ntheir best accuracy level, we set the number of histogram buckets\nto the upper limit (10,000 for Postgres, 1024 for MySQL). For\nDBMS-A, we create several multi-column statistics in order to\ncover all columns with histograms. Note that even with the max-\nimum number of buckets, size of these statistics is much smaller\nthan our size budget, and result in less memory consumption\nthan other traditional and learned methods in our experiment.\n•Sample-A, Sample-B exhibit estimators adopt sampling. Sample-A\nuses a uniform random sample, which is well known that it would\nresult in large error when no tuple in the sample satisﬁes all the\npredicates. Therefore we also include Sample-B, which assumes\nindependence between each predicate in zero-tuple cases. We\nsample 1.5% tuples from each dataset for both methods.\n•MHIST [73] builds a multi-dimensional histogram on the entire\ndataset. We choose Maxdiﬀ as the partition constraint with Value\nandArea being the sort and source parameter since it is the most\naccurate choice according to [ 74]. We run the MHIST-2 algorithm\niteratively until it reaches to 1.5% of the data size.\n•/Q_uickSel [66] represents query-driven multi-dimensional synop-\nsis approaches’ performance. It models the data distribution with\nuniform mixture model by leveraging query feedback. We choose\n/Q_uickSel because it shows better accuracy than query-driven\nhistograms including STHoles [ 6] and ISOMER [ 81] in [ 66]. We\nuse 10K queries to train the model.\n•Bayes [13] shows the estimation results of probabilistic graphical\nmodel approaches [ 14,24,88]. We adopt the same implementa-\ntion in [ 95], which uses progressive sampling to estimate range\nqueries and shows a very promising accuracy.\n•KDE-FB [29] represents the performance of modeling data dis-\ntribution with kernel density models. It improves naive KDE by\noptimizing the bandwidth with query feedback. We sample 1.5%\ntuples from each dataset (max to 150K) and use 1K queries to\ntrain the model.\n4.2 Are Learned Methods More Accurate?\nWeﬁrst want to understand the learned methods’ eﬀort on accu-\nracy improvement, comparing with traditional methods. We test all\nthe methods using 10K queries on each dataset. Table 4 shows the\nq-error comparison result. Bold values in the “Traditional Methods”\nsection denotes the minimum q-error that traditional methods canreach, while in the “Learned Methods” section it highlights the\nlearned methods that can achieve a smaller (or equal) q-error than\nthe best traditional method. The last row summaries the compari-\nson by using “win” to denote learned methods beating traditional\nmethods, and “lose” means the opposite.\nOverall, learned methods are more accurate than traditional\nmethods in almost all the scenarios. The best learned method can\nbeat the best traditional method up to 14 ×on max q-error. The\nimprovement over the three real database systems is particularly\nimpressive. For example, they achieve 28 ×, 51×, 938×, and 1758 ×\nbetter max q-error on Census, Forest, Power and DMV, respectively.\nEven in the only exception that learned methods lose (50th on\nForest), they can still achieve very similar performance to the best\ntraditional result.\nTo see how the predicates aﬀect the accuracy, we group the test\nqueries based on the number of predicates and plot the q-error\ndistribution of each group on Census dataset. Figure 4 shows the re-sult. “Best Learned” (or “Best Traditional”) represents the minimum\nq-error that learned (or traditional) methods can achieve on each\npercentile (max, 75th, median, 25th, min) value in the boxplot. We\ncan see that the performance degrades when the number of predi-\ncates increases for both methods. It is because queries with more\npredicates tend to result in lower selectivity and the correlation be-\ntween attributes tend to be more complex. In addition, within each\ngroup, Best Learned always outperforms Best Traditional, which\nfurther demonstrates the superiority of learned methods over tra-\nditional methods. We also divide the queries based on the operator\ntype (equality or range) and have the same observation that Best\nLearned outperforms Best Traditional in both groups.\nAmong all learned methods, Naru is the most robust and accurate\none. It basically has the best q-error across all scenarios and keeps\nits max q-error within 200. As for query-driven methods, LW-XGB\ncan achieve the smallest q-error in most situations except for max\nq-error, in which it cannot beat MSCN. We ﬁnd that the queries\nwhich have large errors on LW-XGB andLW-NN usually follow\nthe same pattern: the selectivity on each single predicate is large\nwhile the conjunctive of multiple such predicates is very small. This\npattern cannot be well captured by the CE features (AVI, MinSel,\nEBO) adopted LW-XGB/NN. In comparison, MSCN can handle this\nsituation better which may be due to the sample used in its input.\nWe observe that the same algorithm performs quite diﬀerently\non diﬀerent datasets in terms of max q-error. But for the other error\nmetrics like median, the performance is consistent across datasets.\nThis is because max q-error can be easily aﬀected by a few queries.\nMost methods (e.g., DeepDB, LW-XGB/NN) tend to have bigger\nmax error on larger dataset due to the increasing range of possible\ncardinality values (number of tuples in total). On the other hand,\nNaru shows very impressive max q-error on the largest DMV than\nother datasets. It is because Naru models all columns as discrete\nvalues and learns the embedding representation of each value. Since\nDMV has the smallest domain size (smaller number of discrete\nvalues) and also the biggest model budget (larger embedding size),\nNaru can learn a better representation. MSCN maintains its max\nerror in the same magnitude on all datasets using a random sample,\nwhich also leads to the same observation in Sample-A.\n4.3 What Is the Cost For High Accuracy?\nSince learned methods can beat the cardinality estimators used in\nreal database systems by a large margin, can we just directly deploy\nthem? In this section, we examine the cost of these highly accurate\n1644\n\n\nTable 4: Estimation errors on four real-world datasets.\nEstimator Census Forest Power DMV\n50th 95th 99th Max 50th 95th 99th Max 50th 95th 99th Max 50th 95th 99th Max\nTraditional Methods\nPostgres 1.40 18.6 58.0 1635 1.21 17.0 71.0 9374 1.06 15.0 235 2 ·1051.19 78.0 3255 1 ·105\nMySQL 1.40 19.2 63.0 1617 1.20 48.0 262 7786 1.09 26.0 2481 2 ·1051.40 1494 3 ·1044·105\nDBMS-A 4.16 122 307 2246 3.44 363 1179 4 ·1041.06 8.08 69.2 2 ·1051.46 23.0 185 3 ·104\nSample-A 1.16 31.0 90.0 389 1.04 17.0 67.0 416 1.01 1.22 8.00 280 1.01 1.42 19.0 231\nSample-B 1.16 11.0 34.0 1889 1.04 9.83 38.0 9136 1.01 1.25 8.00 2·1051.01 1.43 10.0 3·104\nMHIST 4.25 138 384 1673 3.83 66.5 288 2 ·1044.46 184 771 1 ·1051.58 13.8 90.8 3 ·104\n/Q_uickSel 3.02 209 955 6523 1.38 15.0 142 7814 3.13 248 1 ·1044·105126 1 ·1054·1054·106\nBayes 1.12 3.50 8.00 303 1.13 7.00 29.0 1218 1.03 2.40 15.0 3 ·1041.03 1.85 12.9 1 ·105\nKDE-FB 1.18 23.0 75.0 293 1.04 5.00 17.0 165 1.01 1.25 9.00 254 1.01 1.50 36.0 283\nLearned Methods\nMSCN 1.38 7.22 15.5 88.0 1.14 7.62 20.6 377 1.01 2.00 9.91 199 1.02 5.30 25.0 351\nLW-XGB 1.16 3.00 6.00 594 1.10 3.00 7.00 220 1.02 1.72 5.04 5850 1.00 1.68 6.22 3·104\nLW-NN 1.17 3.00 6.00 829 1.13 3.10 7.00 1370 1.06 1.88 4.89 4·1041.16 3.29 22.1 3 ·104\nNaru 1.09 2.50 4.00 57.0 1.06 3.30 9.00 153 1.01 1.14 1.96 161 1.01 1.09 1.35 16.0\nDeepDB 1.11 4.00 8.50 59.0 1.06 5.00 14.0 1293 1.00 1.30 2.40 1568 1.02 1.86 5.88 5086\nL v.s. T win win win win lose win win win win win win win win win win win\nFigure 4: Q-Error comparison between the best learned and\ntraditional method by varying # of predicates on Census.\nlearned methods. We compare learned methods with database sys-\ntems in terms of training time and inference time to see whether\nthey can reach the level of DBMS products. Figure 5 shows the\ncomparison result.\nTraining Time. For learned methods, we record the time used\nto train the models reported in Table 4. For database systems, we\nrecord the time to run the statistics collection commands.\nDatabase systems can ﬁnish collecting statistics in seconds on\nall datasets, while learned methods generally need minutes or even\nhours, which depends on the underlying machine learning model.\nLW-XGB, which builds on gradient boosted tree, is the fastest\nlearned method. It can be as fast as some DBMS when using fewer\ntrees like in Census and Power dataset. DeepDB is the second\nfastest, which needs a few minutes to build the SPN model, which\nalso aﬀected by the input sample size and stop conditions. Meth-\nods adopt neural networks in general need longer time. Since we\nuse the same epochs on all datasets, Naru’s training time highly\ndepends on the data size and platform. With GPU, it only needs 1\nminute on Census but takes more than 4 hours on DMV, and this\ntime would be 5 ×to 15×slower on CPU. GPU acceleration also\nimpacts LW-NN, which takes around 30 minutes to ﬁnish training\non all datasets but the time can be up to 20 ×longer on CPU. On\nthe other hand, MSCN exhibits similar training time on the two\ndevices, and GPU is even 3 .5×slower than CPU on small datasets.\nIt is because MSCN needs to handle the conditional workﬂow for\nminimizing its loss (mean q-error), which becomes slower on GPU\nand the impact becomes more obvious when the model is smaller.\nThere is a tradeoﬀ between training time and model accuracy.\nNeural network methods (Naru, MSCN andLW-NN) trained in an\niterative fashion would produce larger error with fewer training\niterations. For all these models, we adopt the same epochs reportedin the original paper on all datasets, although some models can\nachieve similar performance with much fewer iterations. For exam-ple, using 80% less time, we can train a Naru model on DMV dataset\nwith only slightly performance degrade. However, even if we only\nrun 1 epoch on GPU, it will still be much slower than database\nsystems. We will further explore this trade-oﬀ in Section 5.3.\nInference Time. We compute the average inference time of the\n10K test queries by issuing the queries one by one. Figure 5 shows\nthe result. For database systems, we approximate the time by the\nlatency they return execution plan (without executing it), whichshould be longer than the real cardinality estimation time due toother overheads such as parsing and binding. Despite of that, all\nthree DBMSs can ﬁnish the whole process in 1 or 2 milliseconds. In-\nference time of learned methods depends on the underlying model.\nQuery-driven methods (MSCN andLW-XGB/NN) are very com-\npetitive and can achieve similar or better latency than DBMS (but\nnotice that DBMS’s result includes other overheads). It is because\nthey adopt general regression models that directly model the queryspace and also has been well optimized in terms of implementation.\nOn the other hand, the remaining methods adopt more specialized\nmodels and are much slower. SPN model in DeepDB needs around\n25ms on three larger datasets and takes an average of 5ms on Cen-\nsus.Naru’s inference procedure includes a progressive sampling\nmechanism, which needs to run the model thousand of times inorder to get the accurate result. Its total time is sensitive to therunning device, which needs 5ms to 15ms on GPU, and CPU can\nbe up to 20× slower.\nThe cardinality estimator could be invoked many times dur-\ning query optimization. Long inference latency can be a blocking\nissue of bring these accurate learned estimators like Naru and\nDeepDB into production, especially for OLTP applications with\nshort-running queries. In addition, shortening the inference time\nof these methods is not a trivial task. Despite the featurization,\nthe bottlenecks of learned methods mostly come from the under-\nlining models, i.e. NN, SPN, XGB, MSCN. To speed up a model’sinference time may require techniques, such as model compres-\nsion/distillation. One exception is Naru, whose bottleneck is, instead\nof Auto-regressive Model, the dependency of the selectivity com-\nputation for each attribute in the progressive sampling procedure,\nwhich needs to be done sequentially.\n1645\n\n\nFigure 5: Training and inference time comparison between\nlearned methods and real database system (MSCN’s CPU and\nGPU results on DMV are overlapped).\nHyper-parameter Tuning. Hyper-parameter tuning is another\ncost for learned methods. The learned models shown in Table 4\nrepresent the models with the best hyper-parameters. However,\nwithout hyper-parameter tuning, learned models may perform very\nbadly. In our experiment, we found the ratio between the largest\nand the smallest max q-error among models with diﬀerent hyper-\nparameters for the same method can be up to 105.\nWhile essential for high accuracy, hyper-parameter tuning is a\nhighly expensive process since it needs to train multiple models in\norder to ﬁnd the best hyper-parameters. For example, as shown in\nFigure 5, Naru spends more than 4 hours in training a single model\non DMV with GPU. If ﬁve models are trained, then Naru needs to\nspend 20+ hours (almost a day) on hyper-parameter tuning.\n4.4 Main Findings\nOur main ﬁndings of this section are summarized as follows:\n•In our experiment, new learned estimators can deliver more ac-\ncurate prediction than traditional methods in general and among\nlearned methods, Naru shows the most robust performance.\n•In terms of training time, new learned methods can be slower\nthan DBMS products in magnitudes except for LW-XGB.\n•New learned estimators that based on regression models (MSCN\nandLW-XGB/NN) can be competitive to database systems in\ninference time, while methods that model the joint distribution\ndirectly (Naru andDeepDB) requires much longer time.\n•GPU can speed up the training and inference time of some of the\nnew learned estimators, however it cannot make them as quick\nas DBMS products and sometimes introduce overhead.\n•Hyper-parameter tuning is an extra cost which cannot be ignored\nfor adopting neural network based estimators.\n5 ARE LEARNED METHODS READY FOR\nDYNAMIC ENVIRONMENTS?\nData updates in databases occur frequently, leading to a “dynamic”\nenvironment for cardinality estimators. In this section, we aim to\nanswer a new question: Are learned methods ready for dynamic\nenvironments? We want to understand the gap of adopting recent\nlearned methods in real systems. We ﬁrst discuss how learned\nmethods perform against DBMSs in dynamic environments, then\nexplore the trade-oﬀ between the number of updating epochs and\naccuracy, and ﬁnally investigate how much GPU can help.\n5.1 Setup\nDynamic Environment. In a dynamic environment, both model\naccuracy and updating time matter. Consider a time range [0,\u0001푇].\nSuppose that there are \u0001푛queries uniformly distributed in this time0 T= 100 mins\nStale Model Updated Model Naru\n25% queries 75% querie sFinish updatingtu= 75 mins\nStart updating\nFigure 6: An illustration of a dynamic environment.\nrange. Suppose that given a trained initial model, the model update\nstarts at timestamp 0 and ﬁnishes at timestamp \u0001푡\u0001푢(\u0001푡\u0001푢≤\u0001푇). For the\nﬁrst \u0001푛·\u0001푡\u0001푢\n\u0001푇queries, their cardinalities will be estimated using the\nstale model. For the remaining \u0001푛·(1−\u0001푡\u0001푢\n\u0001푇)queries, the updated\nmodel will be used.\nFigure 6 shows an example. Suppose \u0001푇=100 mins and Naru\nspends \u0001푡\u0001푢=75 mins updating its model. Then, Naru needs to esti-\nmate the cardinalities for 75% (25%) of the queries using the stale\n(updated) model. Since many queries will be handled by the (inac-\ncurate) stale model, although Naru performs the best in the static\nenvironment, this may not be the case in this dynamic environment.\nDataset & Workload & Metric We use the same four real-world\ndatasets as Section 4. We append 20% new data to the original\ndataset and apply our workload generation method to the updated\ndata to general 10K test queries. That is, the testing workload con-\ntains 10K queries. And these queries will be uniformly distributed\nin[0,\u0001푇]. Here, \u0001푇is a parameter in our dynamic environment. Intu-\nitively, it represents how “frequent” the data is being updated. For\nexample, if the data is periodically updated every 100 mins, then\nwe can set \u0001푇= 100 mins. We report the 99th percentile q-error of\nthe 10K queries. It is worth noting that we have shown a variety\nof error metrics (50%, 95%, 99%, and max errors) in Table 4. Based\non the results of Table 4, we found that learned methods improve\nmore on the larger errors (99% and max), compared to traditional\nmethods. Since max error is sensitive to outliers, we chose 99%\nerror. To further mitigate the impact of outliers, in our experiment\nsetting, we were using a large number of queries (10,000 queries)\nfor testing. It means that 99% error is the 100th largest error, thus\nit was not dominated by a few outlier queries.\nData Update. We ensure that the appended 20% new data has\ndiﬀerent correlation characteristics from the original dataset. Oth-\nerwise, the stale model may still perform well and there is no need\nto update the model. To achieve this, we create a copy of the origi-\nnal dataset and sort each column individually in ascending order,\nwhich leads to the maximum Spearman’s rank correlation between\nevery pair of columns. We randomly pick up 20% of the tuples from\nthis copied dataset and append them to the original dataset.Model Update.\nThe initial models we use are the same as Section 4,\nwhich are tuned towards a better accuracy. We follow the original\npapers of the learned methods to update their models unless stated\notherwise. Naru andDeepDB are trained on data. As described\nin their papers, Naru is updated by one epoch, while DeepDB is\nupdated by inserting a small sample (1%) of the appended data\nto its tree model. MSCN andLW-XGB/NN use query results as\ntraining data. Since the updating procedure is not discussed in the\noriginal MSCN paper, we adopt LW-XGB/NN’s updating procedure\nforMSCN. After generating a training workload, we use a sample\n(5% of the original datasets) to update the query label. LW-XGB and\nLW-NN originally use 2K and 16K queries for updating accordingly.\nWe assign 10K queries for MSCN as a fair size of training data.\nNote that the updating time is diﬀerent from the training time\npresented in Figure 5. To update a model quickly, the updating\ntime involves fewer epochs. Also, for query driven methods, they\n1646\n\n\nFigure 7: DBMSs vslearned methods under diﬀerent dynamic environments on four datasets.\nneed to add the query results’ updating time because this is a major\ndiﬀerence between data-driven and query-driven learned methods.\n5.2 Which Method Performs the Best in\nDynamic Environments?\nIn this experiment, we test 5 learned methods against 3 DBMSs\non CPU. We vary \u0001푇for each dataset to represent diﬀerent update\nfrequencies: high, medium, low. Note that our four datasets are\ndiﬀerent in size, so \u0001푇is set diﬀerently for each dataset. The results\nare shown in Figure 7. If a model cannot ﬁnish updating within \u0001푇,\nwe will put “×” in the ﬁgure.\nWeﬁrst compare DBMSs with learned methods. We can see that\nDBMSs have more stable performance than learned methods by\nvarying \u0001푇. The reason is that DBMSs have very short updating time\nand almost all the queries are run on their updated statistics. We also\nobserve that many learned methods cannot catch up with fast data\nupdates. Even if they can, they do not always outperform DBMSs.\nFor example, when \u0001푇=50 mins on DMV, DBMS-A outperforms\nDeepDB by about 100 ×since the updated DeepDB model cannot\ncapture correlation change well.\nWe then compare diﬀerent learned methods. Overall, LW-XGB\ncan perform better or at least comparable with others in most cases.\nMSCN andLW-NN do not perform well since they need longer up-\ndating time and the stale models process too many queries. DeepDB\nusually has a very short updating time. However, its updated modelcannot capture the correlation change well, thus it does not outper-\nform LW-XGB/NN in most cases. Recall that Naru has a very good\naccuracy when there is no update. In dynamic environments, how-\never, Naru does not outperform LW-XGB when update frequencies\nare high or medium. Naru has a similar performance with DBMSs\non Census and Forest. This is because Naru uses 1 epoch to update\nits model, which is not enough to have good accuracy for Census\nand Forest. For DMV, we have the same observation as [ 18].Naru\nperforms well on DMV within 1 epoch. We will discuss this trade-oﬀ\nbetween updating epochs and accuracy in the next subsection.\nIn terms of updating time, there is no all-time winner on diﬀer-\nent datasets. For example, on Census, DeepDB (data driven) is the\nfastest method, whereas on DMV, LW-XGB (query driven) is the\nfastest one, although these two methods are the top-2 fastest meth-\nods in this experiment. The reason behind this is that the updating\ntime of data driven methods is usually proportional to the size of\nthe data. Intuitively, data driven methods compress the information\nof the data to the models to represent the joint distribution. When\nthe size of the data gets larger, the complexity of the model should\nbe higher and harder to train. In contrast, query driven methodshave the training overhead of generating query labels. However,\ngiven a larger dataset and a ﬁxed number of training queries, the\ncomplexity of their models do not necessarily become higher. In\npractice, the choice of using data or query driven methods is really\nsubjective to the applications.\nWe can observe that each method performs diﬀerently on diﬀer-\nent datasets. One major reason is that in the dynamic environment,\nthere is a trade-oﬀ between updating time and estimation accu-\nracy. If a method needs a longer updating time, more queries inFigure 8: Trade-oﬀ (Naru): epochs vs accuracy.\nthe workload will be estimated by the stale model, thus the overall\nestimation accuracy will degrade.\nIn addition, there are some other reasons that could cause the\ndegradation of accuracy for each individual method. For Naru, one\nepoch of updating might be insuﬃcient to learn a good updated\nmodel. For MSCN, LW-XGB andLW-NN, the ground truth labels\nare generated from a sample which might introduce errors. For\nDeepDB, without restructuring the tree, the underlying correlation\nis assumed unchanged. This assumption might hurt the perfor-\nmance when correlation change happens.\n5.3 What Is the Trade-oﬀ Between Updating\nTime and Accuracy?\nWe explore the trade-oﬀ between the number of updating epochs\nand accuracy for learned methods. Due to the space limit, we only\nshow Naru’s results on Census and Forest to illustrate this point.\nWe set \u0001푇= 10 mins on Census and \u0001푇=100 mins on Forest to\nensure Naru with diﬀerent epochs can ﬁnish updating within \u0001푇.\nFigure 8 shows our results. “Stale” represents the stale model’s per-\nformance on 10K queries. “Updated” represents the updated model’s\nperformance. “Dynamic” represents the Naru’s performance (the\nstale model ﬁrst and then the updated model) on 10K queries. We\ncan see a clear trade-oﬀ ofNaru on Forest. That is, “Dynamic” ﬁrst\ngoes down and then goes up. The reason is that long training time\n(epochs) makes the model update slow. It leaves more queries exe-\ncuted using the stale mode. Even though more epochs improve the\nupdated model’s performance, it hurts the overall performance.\nIn this Naru experiment, we show the trade-oﬀ between updating\ntime and accuracy by varying the number of epochs. There are\nother ways to achieve this trade-oﬀ. For example, for query-driven\nmethods, they need to update the answers to a collection of queries.\nUsing sampling is a nice way to reduce the updating, but it will\nlead to approximate answers, thus hurting the accuracy. It is an\ninteresting research direction to study how to balance the trade-oﬀ\nfor learned methods.\n5.4 How Much Does GPU Help?\nWe explore how much GPU can help Naru andLW-NN. We set \u0001푇=\n100 mins on Forest and \u0001푇= 500 mins on DMV to ensure they can\nﬁnish updating within \u0001푇. The results are shown in Figure 9.\nWe can see that with the help of GPU, LW-NN is improved by\naround 10 ×and 2×on Forest and DMV, respectively. There are two\nreasons for these improvements: (1) LW-NN’s training time can be\nimproved by up to 20 ×with GPU; (2) A well-trained LW-NN (500\nepochs) has a good accuracy. For Naru, it is improved by 2 ×on\nDMV. However, it does not get improved on Forest. This is because\n1647\n\n\nFigure 9: GPU aﬀects the performance.\nthat 1 epoch is not enough for Naru to get a good updated model\non Forest, although shorter updating time leaves more queries for\nthe updated model.\n5.5 Main Findings\nOur main ﬁndings of this section are summarized as follows:\n•Learned methods cannot catch up with fast date updates. MSCN,\nLW-NN, Naru, and DeepDB return large error in dynamic envi-\nronments for diﬀerent reasons.\n•Within learned methods, there is no clear winner. Naru performs\nthe best when date updates are not frequent, while LW-XGB\nperforms the best in more dynamic environments.\n•In terms of updating time, DeepDB is the fastest data-driven\nmethod and LW-XGB is the fastest query-driven method.\n•There is a trade-oﬀ between updating time and accuracy for\nlearned methods. It is not easy to balance the trade-oﬀ in practice\nand requires more research eﬀorts on this topic.\n•GPU is able to, but not necessarily, improve the performance. It\nis important to design a good strategy to handle model updates\nin order to beneﬁt from GPU.\n6 WHEN DO LEARNED METHODS GO\nWRONG?\nOne advantage of simple traditional methods like histogram and\nsampling is their transparency. We know that when the assumptions\n(e.g., attribute-value-independence (AVI), uniform spread) made by\nthese estimators are violated, they tend to produce large q-errors. In\ncomparison, learned estimators are opaque and lack understanding.\nIn this section, we seek to explore scenarios when learned methods\ndo not work well. We run a micro-benchmark to observe how their\nlarge error changes when we alter the underlying dataset. We also\nidentify some logical rules that are simple and intuitive but are\nfrequently violated by these learning models.\n6.1 Setup\nDataset. We introduce our synthetic dataset generation procedure.\nWe generate datasets with two columns by varying three key fac-\ntors: distribution (of the ﬁrst column), correlation (between the two\ncolumns) and domain size (of the two columns). Each dataset con-\ntains 1 million rows.\nTheﬁrst column is generated from the genpareto function in\nscipy [ 90], which can generate random numbers from evenly dis-\ntributed to very skewed. We vary the distribution parameter \u0001푠from\n0 to 2, where \u0001푠=0 represents uniform distribution and the data\nbecomes more skewed as \u0001푠increases.\nThe second column is generated based on the ﬁrst column in\norder to control the correlation between the two columns. We use\n\u0001푐∈[0,1]to represent how correlated the two columns are. For\neach row (\u0001푣1,\u0001푣2), we set \u0001푣2to\u0001푣1with a probability of \u0001푐and set \u0001푣2\nto a random value drawn from the domain of the ﬁrst column with\na probability of 1 −\u0001푐. Obviously, the two columns are independent\nwhen \u0001푐=0. They are more correlated as \u0001푐increases and become\nfunctional dependent when \u0001푐=1.\nWe also consider domain size \u0001푑(the number of distinct values),\nwhich is related to the amount of information contained in a dataset.(a)\u0001푠=1.0,\u0001푑=1000 (b)\u0001푐=1.0,\u0001푑=1000\nFigure 10: Top 1% q-error distribution under diﬀerent corre-\nlations (a) and distributions (b).\nIt can aﬀect the size needed to encode the space for models like Naru.\nTo control the domain size, we convert the generated continuous\nvalues into bins. In our experiment, we generate datasets with\ndomain size 10, 100, 1K and 10K.\nWorkload. Since the goal of this experiment is to study the cases\nwhen learned methods go wrong, we generate center values from\neach column’s domain independently (OOD) for all the queries\nin order to explore the whole query space and ﬁnd as many hard\nqueries as possible. Other workload generation settings are the\nsame as Section 4.\nHyper-parameter Tuning. We adopt default hyper-parameters\nrecommended in [ 30](RDC threshold =0.3 and minimum instance\nslice=0.01) for DeepDB andﬁx the tree size of LW-XGB to 128.\nAs for neural network models, we randomly pick up three hyper-\nparameter settings with 1% size budget using the same way as\nSection 4 and select one that consistently reports good results. The\ndetailed hyper-parameters used in this experiment can be found in\nour released code.\n6.2 When Do Learned Estimators Produce\nLarge Error?\nWe examine how the accuracy of learned models will be aﬀected by\ndiﬀerent factors. We train the exact same model on datasets with\nonly one factor varied and the other two ﬁxed, and use the same 10K\nqueries to test the models. Instead of comparing diﬀerent models,\nhere we aim to observe the performance change for the same model\non diﬀerent datasets. We only exhibit the distribution of the top 1%\nq-errors to make the trend on large errors more clear. Similar with\nSection 5, in this experiment, we care more about when learned\nmethods produce large errors.\n1648\n\n\nFigure 11: Top 1% error distribution under diﬀerent domain size ( \u0001푠=1.0,\u0001푐=1.0).\nCorrelation. A common thing we found when we vary the corre-\nlation parameter \u0001푐is that all methods tend to produce larger q-error\non more correlated data. Figure 10a shows the top 1% q-error distri-\nbution trend on diﬀerent correlation degrees with the ﬁrst column\ndistribution \u0001푠=1.0 (exponential distribution) and domain size\n\u0001푑=1000. It is clear that boxplots in all the ﬁgures have a trend to\ngo up when \u0001푐increases.\nAnother observation is that the q-error of all estimators rises\ndramatically (10 ∼100×) when two columns become functional\ndependent ( \u0001푐=1.0). This pattern commonly exists on diﬀerent\npairs of \u0001푠and\u0001푑values we tested, which indicates that there is space\nto improve theses learned estimators on highly correlated datasets\nespecially when functional dependency exists.\nIn our experiment, we found these two observations hold in\ngeneral on diﬀerent values for \u0001푠and\u0001푑.\nDistribution. Each learned method reacts diﬀerently when we\nchange the distribution of the ﬁrst column, and this reaction also\nchanges when the underlying correlation setting varies. Figure 10b\nshows the top 1% q-error distribution trend when \u0001푠goes from 0 .0\nto 2 .0 while ﬁxing the correlation \u0001푐=1.0 and domain size \u0001푑=1000.\nWe choose to show the case when two columns are functional\ndependent because it tends to produce larger errors.\nIn general, Naru outputs larger max q-errors when data is more\nskewed ( \u0001푠>1.0), while MSCN, LW-XGB/NN andDeepDB show\nan opposite pattern. We suspect this diﬀerence might be caused\nby the diﬀerent basic building blocks used in each method. The\ncommon thing shared within the latter approaches is that they all\nincorporate basic synopsis like sampling or 1D histogram in their\nmodels. These statistics might directly record a relatively accurate\ncardinality for the query involving a frequent value in the dataset,\nand thus reduce the max error when data is very skewed. If this is\ntrue, we can study how to incorporate a similar idea into Naru and\nmake it more robust on skewed data.\nAnother interesting thing is that unlike max q-error, the 99th\npercentile q-error (the lower extreme of the boxplot since we only\nreport top 1% q-errors) shows an opposite pattern on MSCN and\nDeepDB.Here we guess that for both methods, it might be because\nof the number of queries with very small selectivity increases when\n\u0001푠increases. In such cases, the sample feature in MSCN would remain\nin all zero on many queries, which is not very useful. As for DeepDB,\nsince its leaf node has the AVI assumption, it would produce very\nlarge result when the selectivity of each predicate is large but the\ncombined result is very small, which is common when \u0001푠is large.\nDomain Size. Figure 11 shows the top 1% q-error distribution on\ndatasets generated under diﬀerent domain size ( \u0001푠=1.0 and \u0001푐=1.0).\nNotice that Naru may use a diﬀerent model architecture on each\ndomain size to meet the same 1% size budget.\nExcept for LW-NN, all methods output larger error on larger do-\nmain size. Naru exhibits a 100 ×performance degrade when domain\nsize goes from 1K to 10K. This may be because that the embedding\nmatrix on 10K domain occupies a big portion of the size budget and\nthus the rest of the model does not have enough capacity to learn\nthe data distribution. Having a more eﬃcient encoding method\ncould mitigate this issue for Naru. LW-XGB shows a very strongresult when domain size is 10 and the error becomes 100 ×bigger\non larger domains. MSCN andDeepDB are relatively more robust\nthan other methods but still experience around 10 ×degrade when\ndomain size increases from 10 to 10K.\nIt is interesting to see that LW-NN andLW-XGB show opposite\ntrend even though they share the same input feature and optimiza-\ntion goal. It is very likely that this phenomenon is caused by the\nunderlying model they adopt. We suspect that the input query space\nbecomes more “discrete” when the domain size is as small as 10.\nTherefore a small change in the query predicate can dramatically\nchange the cardinality result or might not aﬀect it at all. It can be\nhard for the neural network used in LW-NN to learn since compared\nwith the tree-based model in LW-XGB, neural network intuitively\nﬁts the data in a more smooth and continuous way.\n6.3 Do Learned Estimators Behave Predictably?\nDuring our experimental study, we identify some illogical behaviors\nfrom some of the learned models. For example, when we changed\none of the query predicates from [320 ,800 ]to a smaller range\n[340 ,740 ], the real cardinality decreased, but the estimated cardi-\nnality by LW-XGB unexpectedly increased by 60.8%.\nThis kind of unreasonable behavior caught our attention. The vi-\nolation of simple logical rules like this could cause troubles for both\nDBMS developers and users (see Section 6.4 for more discussion).\nInspired by the work [ 83] in the deep learning explanation ﬁeld, we\npropose ﬁve basic rules for CE. These rules are simple and intuitive\nwhich the users may expect cardinality estimators to satisfy:\n(1)Monotonicity: With a stricter (or looser) predicate, the esti-\nmation result should not increase (or decrease).\n(2)Consistency: The prediction of a query should be equal to the\nsum of the predictions of queries split from it (e.g., a query\nwith predicate [100 ,500]on\u0001퐴\u0001푖can be split to two queries with\n[100 ,200)and [200 ,500]on\u0001퐴\u0001푖respectively and other predicates\nremain the same).\n(3)Stability: For any query, the prediction result from the same\nmodel should always be the same.\n(4)Fidelity-A: Result should be 1 for querying on the entire do-\nmain (e.g. SELECT * FROM R WHERE \u0001푚\u0001푖\u0001푛 \u0001푖≤\u0001퐴\u0001푖≤\u0001푚\u0001푎\u0001푥 \u0001푖).\n(5)Fidelity-B: Result should be 0 for a query with an invalid pred-\nicate (e.g. SELECT * FROM R WHERE 100 ≤\u0001퐴\u0001푖≤10).\nAccording to these proposed rules, we check each learned esti-\nmator and summarize whether it satisﬁes or violates each rule in\nTable 5. Some of the rules like Fidelity-B can be ﬁxed with some\nsimple checking mechanisms, however here we only consider the\noriginal output of the underlying model used in each estimator in\norder to see whether these models behave in a logical way natively.\nNaru’s progressive sampling technique introduces uncertainty\nto the inference process, which causes the violation of stability.\nFigure 12 shows an example of the estimation results using Naru\nto run a query (the actual cardinality is 1036) for 2000 times under\nthis setting. The results are spread over the range of [0, 5992]. This\ninstability also causes Naru to violate monotonicity and consistency\nrules. The regression-based methods (MSCN, LW-NN, LW-XGB)\nviolate all the rules except for stability. It is not a very surprising\n1649\n\n\nTable 5: Satisfaction and violation of rules by learned esti-\nmators. ( /check: satisﬁed, ×: violated)\nRule Naru MSCN LW-XGB LW-NN DeepDB\nMonotonicity × ×× × /check\nConsistency ×× × × /check\nStability × /check/check /check/check\nFidelity-A /check ×× × /check\nFidelity-B /check ×× × /check\nresult since there is no constraint enforced to the model during both\ntraining and inference stages. DeepDB does not violate any rules\nsince it is built on basic histograms and the computation between\nnodes is restricted to addition and multiplication.\n6.4 What Will Go Wrong in Production?\nWe discuss four issues that may appear when deploying (black-box\nand illogical) learned models in production.\nDebuggability. It is challenging to debug black-box models like\nNaru, MSCN andLW-XGB/NN. Firstly, black-box models may fail\nsilently, thus there is a high risk to miss a bug. For example, if there\nis a bug in the hyper-parameter tuning stage, the model can still\nbe trained and may pass all test cases. Secondly, black-box models\nmake it hard to trace an exception back to the actual bug. If the\nlearned model produces a large error for a given query, it is diﬃcult\nto tell whether it is a normal bad case or caused by a bug in the\ncode or training data.\nExplainability. Another issue is that black-box models lack ex-\nplainability. It brings some challenges for query optimizer version\nupdate. We might ﬁnd a new model architecture improve the accu-\nracy and want to adopt it to the new version. However, it is hard to\nexplain to the database users about which type of query and what\nkind of scenario will be aﬀected by this upgrade.Predicability.\nSince learned methods do not follow some basic\nlogic rules, the database system may behave illogically, thus con-\nfusing database users. For example, a user would expect a query to\nrun faster by adding more ﬁlter conditions. Due to the violation of\nthe monotonicity rule, this may not be the case when the database\nsystem adopts a learned model like Naru, MSCN, or LW-XGB/NN.\nReproducibility. It is common that a database developer wants to\nreproduce customers’ issues. In order to reproduce the issues, the\ndeveloper needs information, such as the input query, optimizer\nconﬁgurations, and metadata [ 80]. However, if the system adopts\nNaru which violates the stability rule, it would be hard to reproduce\nthe result due to the stochastic inference process.\n6.5 Main Findings\nOur main ﬁndings of this section are summarized as follows:\n•All new learned estimators tend to output larger error on more\ncorrelated data, and the max q-error jumps quite dramatically\nwhen two columns are functional dependent.\n•Diﬀerent methods react diﬀerently for more skewed data or for\ndata with larger domain size. This might be due to the diﬀerences\nin the choice of models, input features, and loss functions.\n•We propose ﬁve rules for cardinality estimators and ﬁnd that all\nnew learned models except for DeepDB violate these rules.\n•The non-transparency of the models used in new learned estima-\ntors can be troublesome in terms of debuggability, explainability,\npredicabiltiy, and reproducibility when deployed in production.\n7 RESEARCH OPPORTUNITY\nWe have discussed that the high cost (Section 4 and Section 5)\nand the non-transparency (Section 6) are the two main challengesFigure 12: Prediction result of running Naru on the same\nquery 2000 times ( \u0001푠=0.0,\u0001푐=1.0,\u0001푑=1000).\nof applying learned carnality estimators in DBMS. What can we\ndo in order to close these gaps? In this section, we discuss some\nopportunities in the two research directions.\n7.1 Control the Cost of Learned Estimators\nBalance the Eﬃciency-Accuracy Tradeoﬀ. Balancing the trade-\noﬀbetween accuracy and training (updating) time as well as infer-\nence latency can be an interesting aspect to start with. To retrain a\nmodel, simple approximate methods like using a sample instead of\nfull data to calculate the queries’ ground-truth or incrementally up-\ndating the model, can be leveraged to make neural network models\nmore eﬃcient. Similar ideas in machine learning techniques such\nas early stop [ 8] and model compression [ 11] can also be used to\nreduce the cost.\nHyper-parameter Tuning for Learned Estimators. Hyper-\nparameter tuning is crucial for new learned models to achieve\nhigh accuracy. Algorithms like random search [ 5], bayesian opti-\nmization [ 78], and bandit-based approaches [ 46] can be adopted to\nreduce the cost of obtaining a good hyper-parameter conﬁguration.\nAnother aspect for hyper-parameter tuning is the goal of tuning.\nUsually, the goal is to ﬁnd the conﬁguration with the best accu-\nracy/loss. In the cardinality estimation setting, it is worth doing\nmore exploration to take training/updating time into consideration,\nbecause of the trade-oﬀ above.\n7.2 Make Learned Estimators Trustworthy\nInterpret Learned Estimators. There have been extensive works\nin machine learning explanation trying to understand why a model\nmakes a speciﬁc prediction for a speciﬁc input, such as surrogate\nmodels [ 75], saliency maps [ 79], inﬂuence function [ 35], decision\nsets [ 40], rule summaries [ 76], and general feature attribution meth-\nods [ 52,83]. These techniques could be leveraged to interpret black\nbox cardinality estimators to some extend. For example, when we\nget a large error for a query during the test phase, we can use inﬂu-\nence function [ 35] toﬁnd the most inﬂuential training examples,\nor we can use shapely value [ 52] to check the importance of each\ninput feature. However, how eﬀective these methods are in the\ncardinality estimation setting is still an open problem.\nHandle Illogical Behaviours. Our study shows that many learned\nmethods do not behave logically. One way to handle this is to de-\nﬁne a complete set of logical rules and identify which rules are\nviolated for a certain method. This will add more transparency to\neach learned method and enable the database developers to know\nwhat kind of behavior can be expected from each method. The\nlogical rules we propose in Section 6.3 can be seen as an eﬀort\nfrom this perspective. Another way is to enforce logical rules as\nconstraints for model design. There are some existing works in the\nmachine learning community [ 12,20,36]. Similar ideas could be\napplied to the design of cardinality estimation models.\n8 MULTI-TABLE SCENARIO\nOur paper focuses on understanding single table learned cardinality\nestimation. In this section, we discuss how to expand current tech-\nniques for multi-table cardinality estimation scenario and several\nchallenges involved.\n1650\n\n\nExtend to Multi-table Scenarios. MSCN natively supports joins\nby featurizing the set of base tables, and joins’ indicator and pred-\nicates. DeepDB supports joins by learning a model on the outer\njoin table if the tables pass the tuple-wise correlation check. LW-\nXGB/NN andNaru do not support multi-table natively. However,\nnew methods are proposed to support joins based on these two\nmethods. In [ 94], NeuroCard is proposed as an extension of Naru.\nThey train the auto-regressive model on samples from the full outer\njoin of tables. In [ 17], they extend LW-XGB/NN to multi-table by\ntraining the underlining regression model on queries against a\nmaterialized view of the join table.\nAdditional Challenges. In the training phase, multi-table meth-\nods require to be exposed to the information of the joined tables.\nThis is an additional challenge which will require the model to cap-\nture table correlation on more complex data. Moreover, if updates\nhappen in one or more tables in joins, another challenge would\nbe how to update the model accordingly and how to balance the\ntrade-oﬀ between update time and model accuracy.\nIn the inference phase, when joins are considered, the number of\ncandidate query plans is exponential to the number of joins. That\nis, cardinality estimation will be invoked by many times. One way\nis to adopt the brute force solution, which does inference on every\noperator in every candidate query plan. But the total inference\ntime could be unacceptably long. Another way is to make inference\nonly on the base tables and calculate the following cardinalities\nusing some formulas. However, this might propagate the errors and\nhurt the accuracy. Therefore, a big challenge is how to intelligently\nallocate the inference budget to better balance the accuracy and\neﬃciency trade-oﬀ.\n9 RELATED WORK\nSingle Table Cardinality Estimation. Histogram is the most\ncommon cardinality estimation approach and has been studied\nextensively [ 1,6,21,25,26,31,47,57,60,61,71,73,74,81,86] and\nadopted in database products. Sampling based methods [ 22,48,77,\n93,97] have the advantage to support more complex predicates than\nrange predicates. Prior work mainly adopts traditional machine\nlearning techniques to estimate cardinality, such as curve-ﬁtting [ 9],\nwavelet [ 58], KDE [ 29], uniform mixture model [ 66], and graphical\nmodels [ 14,24,88]. Early works [ 3,41,49,51] also use neural net-\nwork models to approximate the data distribution in a regression\nfashion. In comparison, new learned methods have shown more\npromising results [18, 34].\nJoin Cardinality Estimation. Traditional database systems esti-\nmate the cardinality of joins following simple assumptions suchas uniformity and independence [\n42]. Some works [ 30,34] can\nsupport joins directly, while others [ 17,33,91,94] study how to\nextend single table cardinality estimation methods to support join\nqueries. Empirical study [ 64] evaluates diﬀerent deep learning ar-\nchitectures and machine learning models on select-project-join\nworkloads. Leis et. al [ 43] propose an index-based sampling tech-\nnique which is cheap but eﬀective. Focusing on a small amount of\n“diﬃcult” queries, some works [ 70,92] introduce a re-optimization\nprocedure during inference to “catch” and correct the large errors,\nwhile another line of research tries to avoid poor plans by inferring\nthe upper bound of the intermediate join cardinality [7].\nEnd-to-End Query Optimization. Recently, more and more works\ntry to tackle the query optimization problem in an end-to-end\nfashion. Sun et. al [ 82] propose a learning-based cost estimation\nframework based on a tree-structured model, which estimate bothcost and cardinality simultaneously. Pioneer work [ 63] shows the\npossibility of learning state representation of query optimization\nfor the join tree with reinforcement learning, and many follow-up works [\n38,55,87,96] reveal the eﬀectiveness of using deep\nreinforcement learning for join order selection. Marcus et. al pro-\npose Neo [ 56], which uses deep learning to generate query plans\ndirectly. There are also several end-to-end query optimization sys-\ntems [4, 80, 99] available in the open-source community.\nBenchmark and Empirical Study in Cardinality Estimation.\nLeis et. al [ 42] propose the Join Order Benchmark (JOB), which is\nbased on the real-world IMDB dataset with synthetic queries having\n3 to 16 joins [ 42]. Unlike JOB, we focus on single table cardinality\nestimation. Ortiz et. al [ 64] provide an empirical analysis on the\naccuracy, space and time trade-oﬀ across several deep learning\nand machine learning model architectures. Our study is diﬀerent\nfrom their work in many aspects. We include both data-driven\nand query-driven learned methods (whereas they focus on query-\ndriven models) and both static and dynamic settings. Also we try\nto explore when learned models would go wrong with controlled\nsynthetic datasets and propose simple logical rules to evaluate them.\nHarmouch et. al [ 27] conduct an experimental survey on cardinality\nestimation, but their target is on estimating the number of distinct\nvalues, which is diﬀerent from our paper.\nMachine Learning for Database Systems. Zhou et. al [ 100] pro-\nvide a thorough survey on how ML and DB can beneﬁt each other.\nIn addition to cardinality estimation, ML has the potential to re-\nplace and enhance other components in database systems suchas indexes [\n37] and sorting algorithms [ 39]. Another aspect is to\nleverage ML to automate database conﬁ gurations like knob tun-\ning [ 84,98], index selection [ 68], and view materialization [ 32].\nBeyond that, Approximate Query Processing (AQP) [ 2,45,65,69]\nengines which support COUNT queries can be potentially adopted\nfor cardinality estimation. Learned AQP [ 53,54,85] is recently\nin a rising trend. It is interesting to study their eﬀectiveness on\nsupporting cardinality estimation in DBMS.\n10 CONCLUSION\nIn our paper, we raised an important but unexplored question: “Are\nwe ready for learned cardinality estimation?”. We surveyed seven\nnew learned methods and found that existing experimental studies\nare inadequate to answer this question. In response, we proposed a\nuniﬁed workload generator and explored whether learned methods\nare ready for both static environments and dynamic environments,\nand dived into when learned methods may go wrong. In the end,\nwe identiﬁed a number of promising research opportunities.\nWe concluded that new learned methods are more accurate\nthan traditional methods. However, in order to put them in a well-\ndeveloped system, there are many missing parts to be resolved, such\nas low speed in training and inference, hyper-parameter tuning,\nblack-box property, illogical behaviors, and dealing with frequent\ndata updates. As a result, the current learned methods are still not\nready to be deployed in a real DBMS. Overall, this is an important\nand promising direction to be further explored by our community.\nACKNOWLEDGMENTS\nThis work was supported in part by Mitacs through an Acceler-\nate Grant, NSERC through a discovery grant and a CRD grant,and WestGrid (www.westgrid.ca) and Compute Canada (www.\ncomputecanada.ca). All opinions, ﬁndings, conclusions and rec-\nommendations in this paper are those of the authors and do not\nnecessarily reﬂect the views of the funding agencies.\n1651\n\n\nREFERENCES\n[1]A. Aboulnaga and S. Chaudhuri. Self-tuning histograms: Building histograms\nwithout looking at data. In SIGMOD 1999, Proceedings ACM SIGMOD Interna-\ntional Conference on Management of Data, June 1-3, 1999, Philadelphia, Pennsyl-\nvania, USA, pages 181–192. ACM Press, 1999.\n[2]S. Agarwal, B. Mozafari, A. Panda, H. Milner, S. Madden, and I. Stoica. Blinkdb:\nqueries with bounded errors and bounded response times on very large data. In\nEighth Eurosys Conference 2013, EuroSys ’13, Prague, Czech Republic, April 14-17,\n2013, pages 29–42. ACM, 2013.\n[3]C. Anagnostopoulos and P. Triantaﬁllou. Learning to accurately COUNT with\nquery-driven predictive analytics. In 2015 IEEE International Conference on Big\nData, Big Data 2015, Santa Clara, CA, USA, October 29 - November 1, 2015, pages\n14–23. IEEE Computer Society, 2015.\n[4]E. Begoli, J. Camacho-Rodríguez, J. Hyde, M. J. Mior, and D. Lemire. Apache\ncalcite: A foundational framework for optimized query processing over hetero-\ngeneous data sources. In Proceedings of the 2018 International Conference on\nManagement of Data, SIGMOD Conference 2018, Houston, TX, USA, June 10-15,\n2018, pages 221–230. ACM, 2018.\n[5]J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. J.\nMach. Learn. Res., 13:281–305, 2012.\n[6]N. Bruno, S. Chaudhuri, and L. Gravano. Stholes: A multidimensional workload-\naware histogram. In Proceedings of the 2001 ACM SIGMOD international confer-\nence on Management of data, Santa Barbara, CA, USA, May 21-24, 2001, pages\n211–222. ACM, 2001.\n[7]W. Cai, M. Balazinska, and D. Suciu. Pessimistic cardinality estimation: Tighter\nupper bounds for intermediate join cardinalities. In Proceedings of the 2019\nInternational Conference on Management of Data, SIGMOD Conference 2019,\nAmsterdam, The Netherlands, June 30 - July 5, 2019, pages 18–35. ACM, 2019.\n[8]R. Caruana, S. Lawrence, and C. L. Giles. Overﬁtting in neural nets: Back-propagation, conjugate gradient, and early stopping. In Advances in Neural\nInformation Processing Systems 13, Papers from Neural Information Processing\nSystems (NIPS) 2000, Denver, CO, USA, pages 402–408. MIT Press, 2000.\n[9]C. Chen and N. Roussopoulos. Adaptive selectivity estimation using queryfeedback. In Proceedings of the 1994 ACM SIGMOD International Conference\non Management of Data, Minneapolis, Minnesota, USA, May 24-27, 1994, pages\n161–172. ACM Press, 1994.\n[10] T. Chen and C. Guestrin. XGBoost: A scalable tree boosting system. In Proceed-\nings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery\nand Data Mining, KDD ’16, pages 785–794, New York, NY, USA, 2016. ACM.\n[11] Y. Cheng, D. Wang, P. Zhou, and T. Zhang. A survey of model compression and\nacceleration for deep neural networks. CoRR, abs/1710.09282, 2017.\n[12] J. Chorowski and J. M. Zurada. Learning understandable neural networks\nwith nonnegative weight constraints. IEEE Trans. Neural Networks Learn. Syst.,\n26(1):62–69, 2015.\n[13] C. K. Chow and C. N. Liu. Approximating discrete probability distributions\nwith dependence trees. IEEE Trans. Inf. Theory, 14(3):462–467, 1968.\n[14] A. Deshpande, M. N. Garofalakis, and R. Rastogi. Independence is good:\nDependency-based histogram synopses for high-dimensional data. In Pro-\nceedings of the 2001 ACM SIGMOD international conference on Management of\ndata, Santa Barbara, CA, USA, May 21-24, 2001, pages 199–210. ACM, 2001.\n[15] J. Dougherty, R. Kohavi, and M. Sahami. Supervised and unsupervised discretiza-\ntion of continuous features. In Machine Learning, Proceedings of the Twelfth\nInternational Conference on Machine Learning, Tahoe City, California, USA, July\n9-12, 1995, pages 194–202. Morgan Kaufmann, 1995.\n[16] D. Dua and C. Graﬀ. UCI machine learning repository. http://archive.ics.uci.\nedu/ml, 2017. Accessed: 2020-01-01.\n[17] A. Dutt, C. Wang, V. R. Narasayya, and S. Chaudhuri. Eﬃciently approximating\nselectivity functions using low overhead regression models. Proc. VLDB Endow.,\n13(11):2215–2228, 2020.\n[18] A. Dutt, C. Wang, A. Nazi, S. Kandula, V. R. Narasayya, and S. Chaudhuri.\nSelectivity estimation for range predicates using lightweight models. Proc.\nVLDB Endow., 12(9):1044–1057, 2019.\n[19] C. FENG, H. WANG, N. LU, T. CHEN, H. HE, Y. LU, and X. M. TU. Log-\ntransformation and its implications for data analysis. Shanghai Archives of\nPsychiatry, 26(2):105, 2014.\n[20] W. Fleshman, E. Raﬀ, J. Sylvester, S. Forsyth, and M. McLean. Non-negative\nnetworks against adversarial attacks. CoRR, abs/1806.06108, 2018.\n[21] A. V. Gelder. Multiple join size estimation by virtual domains. In Proceedings of\nthe Twelfth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database\nSystems, May 25-28, 1993, Washington, DC, USA, pages 180–189. ACM Press,\n1993.\n[22] R. Gemulla. Sampling algorithms for evolving datasets. PhD thesis, Dresden\nUniversity of Technology, Germany, 2008.\n[23] M. Germain, K. Gregor, I. Murray, and H. Larochelle. MADE: masked autoen-\ncoder for distribution estimation. In Proceedings of the 32nd International Con-\nference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37\nofJMLR Workshop and Conference Proceedings, pages 881–889. JMLR.org, 2015.\n[24] L. Getoor, B. Taskar, and D. Koller. Selectivity estimation using probabilisticmodels. In Proceedings of the 2001 ACM SIGMOD international conference on\nManagement of data, Santa Barbara, CA, USA, May 21-24, 2001, pages 461–472.\nACM, 2001.[25] D. Gunopulos, G. Kollios, V. J. Tsotras, and C. Domeniconi. Approximating\nmulti-dimensional aggregate range queries over real attributes. In Proceedings\nof the 2000 ACM SIGMOD International Conference on Management of Data, May\n16-18, 2000, Dallas, Texas, USA, pages 463–474. ACM, 2000.\n[26] D. Gunopulos, G. Kollios, V. J. Tsotras, and C. Domeniconi. Selectivity estimators\nfor multidimensional range queries over real attributes. VLDB J., 14(2):137–154,\n2005.\n[27] H. Harmouch and F. Naumann. Cardinality estimation: An experimental survey.\nProc. VLDB Endow., 11(4):499–512, 2017.\n[28] S. Hasan, S. Thirumuruganathan, J. Augustine, N. Koudas, and G. Das. Deep\nlearning models for selectivity estimation of multi-attribute queries. In Pro-\nceedings of the 2020 International Conference on Management of Data, SIGMOD\nConference 2020, online conference [Portland, OR, USA], June 14-19, 2020, pages\n1035–1050. ACM, 2020.\n[29] M. Heimel, M. Kiefer, and V. Markl. Self-tuning, gpu-accelerated kernel density\nmodels for multidimensional selectivity estimation. In Proceedings of the 2015\nACM SIGMOD International Conference on Management of Data, Melbourne,\nVictoria, Australia, May 31 - June 4, 2015, pages 1477–1492. ACM, 2015.\n[30] B. Hilprecht, A. Schmidt, M. Kulessa, A. Molina, K. Kersting, and C. Binnig.\nDeepdb: Learn from data, not from queries! Proc. VLDB Endow., 13(7):992–1005,\n2020.\n[31] H. V. Jagadish, H. Jin, B. C. Ooi, and K. Tan. Global optimization of histograms.\nInProceedings of the 2001 ACM SIGMOD international conference on Management\nof data, Santa Barbara, CA, USA, May 21-24, 2001, pages 223–234. ACM, 2001.\n[32] A. Jindal, S. Qiao, H. Patel, Z. Yin, J. Di, M. Bag, M. Friedman, Y. Lin, K. Karanasos,\nand S. Rao. Computation reuse in analytics job service at microsoft. In Pro-\nceedings of the 2018 International Conference on Management of Data, SIGMOD\nConference 2018, Houston, TX, USA, June 10-15, 2018, pages 191–203. ACM, 2018.\n[33] M. Kiefer, M. Heimel, S. Breß, and V. Markl. Estimating join selectivities using\nbandwidth-optimized kernel density models. Proc. VLDB Endow., 10(13):2085–\n2096, 2017.\n[34] A. Kipf, T. Kipf, B. Radke, V. Leis, P. A. Boncz, and A. Kemper. Learned cardinal-\nities: Estimating correlated joins with deep learning. In CIDR 2019, 9th Biennial\nConference on Innovative Data Systems Research, Asilomar, CA, USA, January\n13-16, 2019, Online Proceedings. www.cidrdb.org, 2019.\n[35] P. W. Koh and P. Liang. Understanding black-box predictions via inﬂuence func-\ntions. In Proceedings of the 34th International Conference on Machine Learning,\nICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings\nof Machine Learning Research, pages 1885–1894. PMLR, 2017.\n[36] A. Kołcz and C. H. Teo. Feature weighting for improved classiﬁer robustness.\nInCEAS’09: sixth conference on email and anti-spam, 2009.\n[37] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 International Conference on Man-\nagement of Data, SIGMOD Conference 2018, Houston, TX, USA, June 10-15, 2018,\npages 489–504. ACM, 2018.\n[38] S. Krishnan, Z. Yang, K. Goldberg, J. M. Hellerstein, and I. Stoica. Learning to\noptimize join queries with deep reinforcement learning. CoRR, abs/1808.03196,\n2018.\n[39] A. Kristo, K. Vaidya, U. Çetintemel, S. Misra, and T. Kraska. The case for a\nlearned sorting algorithm. In Proceedings of the 2020 International Conference on\nManagement of Data, SIGMOD Conference 2020, online conference [Portland, OR,\nUSA], June 14-19, 2020, pages 1001–1016. ACM, 2020.\n[40] H. Lakkaraju, S. H. Bach, and J. Leskovec. Interpretable decision sets: A joint\nframework for description and prediction. In Proceedings of the 22nd ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining, San\nFrancisco, CA, USA, August 13-17, 2016, pages 1675–1684. ACM, 2016.\n[41] M. S. Lakshmi and S. Zhou. Selectivity estimation in extensible databases\n- A neural network approach. In VLDB’98, Proceedings of 24rd International\nConference on Very Large Data Bases, August 24-27, 1998, New York City, New\nYork, USA, pages 623–627. Morgan Kaufmann, 1998.\n[42] V. Leis, A. Gubichev, A. Mirchev, P. A. Boncz, A. Kemper, and T. Neumann. How\ngood are query optimizers, really? Proc. VLDB Endow., 9(3):204–215, 2015.\n[43] V. Leis, B. Radke, A. Gubichev, A. Kemper, and T. Neumann. Cardinality es-\ntimation done right: Index-based join sampling. In CIDR 2017, 8th Biennial\nConference on Innovative Data Systems Research, Chaminade, CA, USA, January\n8-11, 2017, Online Proceedings. www.cidrdb.org, 2017.\n[44] G. P. Lepage. A new algorithm for adaptive multidimensional integration.\nJournal of Computational Physics, 27(2):192–203, 1978.\n[45] F. Li, B. Wu, K. Yi, and Z. Zhao. Wander join: Online aggregation via random\nwalks. In Proceedings of the 2016 International Conference on Management of\nData, SIGMOD Conference 2016, San Francisco, CA, USA, June 26 - July 01, 2016,\npages 615–629. ACM, 2016.\n[46] L. Li, K. G. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar. Hyperband:\nA novel bandit-based approach to hyperparameter optimization. J. Mach. Learn.\nRes., 18:185:1–185:52, 2017.\n[47] L. Lim, M. Wang, and J. S. Vitter. SASH: A self-adaptive histogram set for\ndynamically changing workloads. In Proceedings of 29th International Conference\non Very Large Data Bases, VLDB 2003, Berlin, Germany, September 9-12, 2003,\npages 369–380. Morgan Kaufmann, 2003.\n[48] R. J. Lipton, J. F. Naughton, and D. A. Schneider. Practical selectivity estimation\nthrough adaptive sampling. In Proceedings of the 1990 ACM SIGMOD Interna-\ntional Conference on Management of Data, Atlantic City, NJ, USA, May 23-25,\n1990, pages 1–11. ACM Press, 1990.\n1652\n\n\n[49] H. Liu, M. Xu, Z. Yu, V. Corvinelli, and C. Zuzarte. Cardinality estimation using\nneural networks. In Proceedings of 25th Annual International Conference on\nComputer Science and Software Engineering, CASCON 2015, Markham, Ontario,\nCanada, 2-4 November, 2015, pages 53–59. IBM / ACM, 2015.\n[50] D. López-Paz, P. Hennig, and B. Schölkopf. The randomized dependence coef-\nﬁcient. In Advances in Neural Information Processing Systems 26: 27th Annual\nConference on Neural Information Processing Systems 2013. Proceedings of a meet-\ning held December 5-8, 2013, Lake Tahoe, Nevada, United States, pages 1–9, 2013.\n[51] H. Lu and R. Setiono. Eﬀective query size estimation using neural networks.\nAppl. Intell., 16(3):173–183, 2002.\n[52] S. M. Lundberg and S. Lee. A uniﬁed approach to interpreting model predictions.\nInAdvances in Neural Information Processing Systems 30: Annual Conference on\nNeural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA,\nUSA, pages 4765–4774, 2017.\n[53] Q. Ma, A. M. Shanghooshabad, M. Almasi, M. Kurmanji, and P. Triantaﬁllou.\nLearned approximate query processing: Make it light, accurate and fast. In\n11th Conference on Innovative Data Systems Research, CIDR 2021, Virtual Event,\nJanuary 11-15, 2021, Online Proceedings. www.cidrdb.org, 2021.\n[54] Q. Ma and P. Triantaﬁllou. Dbest: Revisiting approximate query processing\nengines with machine learning models. In Proceedings of the 2019 International\nConference on Management of Data, SIGMOD Conference 2019, Amsterdam, The\nNetherlands, June 30 - July 5, 2019, pages 1553–1570. ACM, 2019.\n[55] R. Marcus and O. Papaemmanouil. Deep reinforcement learning for join order\nenumeration. In Proceedings of the First International Workshop on Exploiting\nArtiﬁcial Intelligence Techniques for Data Management, aiDM@SIGMOD 2018,\nHouston, TX, USA, June 10, 2018, pages 3:1–3:4. ACM, 2018.\n[56] R. C. Marcus, P. Negi, H. Mao, C. Zhang, M. Alizadeh, T. Kraska, O. Papaem-\nmanouil, and N. Tatbul. Neo: A learned query optimizer. Proc. VLDB Endow.,\n12(11):1705–1718, 2019.\n[57] V. Markl, N. Megiddo, M. Kutsch, T. M. Tran, P. J. Haas, and U. Srivastava.\nConsistently estimating the selectivity of conjuncts of predicates. In Proceedings\nof the 31st International Conference on Very Large Data Bases, Trondheim, Norway,\nAugust 30 - September 2, 2005, pages 373–384. ACM, 2005.\n[58] Y. Matias, J. S. Vitter, and M. Wang. Wavelet-based histograms for selectivity\nestimation. In SIGMOD 1998, Proceedings ACM SIGMOD International Conference\non Management of Data, June 2-4, 1998, Seattle, Washington, USA, pages 448–459.\nACM Press, 1998.\n[59] G. Moerkotte, T. Neumann, and G. Steidl. Preventing bad plans by bounding the\nimpact of cardinality estimation errors. Proc. VLDB Endow., 2(1):982–993, 2009.\n[60] M. Müller, G. Moerkotte, and O. Kolb. Improved selectivity estimation by combin-\ning knowledge from sampling and synopses. Proc. VLDB Endow., 11(9):1016–1028,\n2018.\n[61] M. Muralikrishna and D. J. DeWitt. Equi-depth histograms for estimating\nselectivity factors for multi-dimensional queries. In Proceedings of the 1988 ACM\nSIGMOD International Conference on Management of Data, Chicago, Illinois, USA,\nJune 1-3, 1988, pages 28–36. ACM Press, 1988.\n[62] S. of New York. Vehicle, snowmobile, and boat registrations. catalog.data.gov/\ndataset/vehicle-snowmobile-and-boat-registration, 2019. Accessed: 2019-03-01.\n[63] J. Ortiz, M. Balazinska, J. Gehrke, and S. S. Keerthi. Learning state representations\nfor query optimization with deep reinforcement learning. In Proceedings of\nthe Second Workshop on Data Management for End-To-End Machine Learning,\nDEEM@SIGMOD 2018, Houston, TX, USA, June 15, 2018, pages 4:1–4:4. ACM,\n2018.\n[64] J. Ortiz, M. Balazinska, J. Gehrke, and S. S. Keerthi. An empirical analysis of\ndeep learning for cardinality estimation. CoRR, abs/1905.06425, 2019.\n[65] Y. Park, B. Mozafari, J. Sorenson, and J. Wang. Verdictdb: Universalizing approx-\nimate query processing. In Proceedings of the 2018 International Conference on\nManagement of Data, SIGMOD Conference 2018, Houston, TX, USA, June 10-15,\n2018, pages 1461–1476. ACM, 2018.\n[66] Y. Park, S. Zhong, and B. Mozafari. Quicksel: Quick selectivity learning with\nmixture models. In Proceedings of the 2020 International Conference on Manage-\nment of Data, SIGMOD Conference 2020, online conference [Portland, OR, USA],\nJune 14-19, 2020, pages 1017–1033. ACM, 2020.\n[67] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison,\nA. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch:\nAn imperative style, high-performance deep learning library. In Advances in\nNeural Information Processing Systems 32, pages 8024–8035. Curran Associates,\nInc., 2019.\n[68] W. G. Pedrozo, J. C. Nievola, and D. C. Ribeiro. An adaptive approach for index\ntuning with learning classiﬁer systems on hybrid storage environments. In\nHybrid Artiﬁcial Intelligent Systems - 13th International Conference, HAIS 2018,\nOviedo, Spain, June 20-22, 2018, Proceedings, volume 10870 of Lecture Notes in\nComputer Science, pages 716–729. Springer, 2018.\n[69] J. Peng, D. Zhang, J. Wang, and J. Pei. AQP++: connecting approximate query\nprocessing with aggregate precomputation for interactive analytics. In Pro-\nceedings of the 2018 International Conference on Management of Data, SIGMOD\nConference 2018, Houston, TX, USA, June 10-15, 2018, pages 1477–1492. ACM,\n2018.\n[70] M. Perron, Z. Shang, T. Kraska, and M. Stonebraker. How I learned to stop\nworrying and love re-optimization. In 35th IEEE International Conference on\nData Engineering, ICDE 2019, Macao, China, April 8-11, 2019, pages 1758–1761.\nIEEE, 2019.[71] G. Piatetsky-Shapiro and C. Connell. Accurate estimation of the number of\ntuples satisfying a condition. In SIGMOD’84, Proceedings of Annual Meeting,\nBoston, Massachusetts, USA, June 18-21, 1984, pages 256–276. ACM Press, 1984.\n[72] H. Poon and P. M. Domingos. Sum-product networks: A new deep architecture.\nInIEEE International Conference on Computer Vision Workshops, ICCV 2011 Work-\nshops, Barcelona, Spain, November 6-13, 2011, pages 689–690. IEEE Computer\nSociety, 2011.\n[73] V. Poosala and Y. E. Ioannidis. Selectivity estimation without the attribute\nvalue independence assumption. In VLDB’97, Proceedings of 23rd International\nConference on Very Large Data Bases, August 25-29, 1997, Athens, Greece, pages\n486–495. Morgan Kaufmann, 1997.\n[74] V. Poosala, Y. E. Ioannidis, P. J. Haas, and E. J. Shekita. Improved histograms\nfor selectivity estimation of range predicates. In Proceedings of the 1996 ACM\nSIGMOD International Conference on Management of Data, Montreal, Quebec,\nCanada, June 4-6, 1996, pages 294–305. ACM Press, 1996.\n[75] M. T. Ribeiro, S. Singh, and C. Guestrin. \"why should I trust you?\": Explaining\nthe predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining, San Francisco,\nCA, USA, August 13-17, 2016, pages 1135–1144. ACM, 2016.\n[76] M. T. Ribeiro, S. Singh, and C. Guestrin. Anchors: High-precision model-agnostic\nexplanations. In Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence, (AAAI-18), the 30th innovative Applications of Artiﬁcial Intelligence\n(IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artiﬁcial\nIntelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages\n1527–1535. AAAI Press, 2018.\n[77] M. Riondato, M. Akdere, U. Çetintemel, S. B. Zdonik, and E. Upfal. The vc-\ndimension of SQL queries and selectivity estimation through sampling. In\nMachine Learning and Knowledge Discovery in Databases - European Confer-\nence, ECML PKDD 2011, Athens, Greece, September 5-9, 2011, Proceedings, Part\nII, volume 6912 of Lecture Notes in Computer Science, pages 661–676. Springer,\n2011.\n[78] B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. de Freitas. Taking\nthe human out of the loop: A review of bayesian optimization. Proc. IEEE,\n104(1):148–175, 2016.\n[79] A. Shrikumar, P. Greenside, and A. Kundaje. Learning important features\nthrough propagating activation diﬀerences. In Proceedings of the 34th Inter-\nnational Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia,\n6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages\n3145–3153. PMLR, 2017.\n[80] M. A. Soliman, L. Antova, V. Raghavan, A. El-Helw, Z. Gu, E. Shen, G. C. Caragea,\nC. Garcia-Alvarado, F. Rahman, M. Petropoulos, F. Waas, S. Narayanan, K. Krikel-\nlas, and R. Baldwin. Orca: a modular query optimizer architecture for big data.\nInInternational Conference on Management of Data, SIGMOD 2014, Snowbird,\nUT, USA, June 22-27, 2014, pages 337–348. ACM, 2014.\n[81] U. Srivastava, P. J. Haas, V. Markl, M. Kutsch, and T. M. Tran. ISOMER: consis-\ntent histogram construction using query feedback. In Proceedings of the 22nd\nInternational Conference on Data Engineering, ICDE 2006, 3-8 April 2006, Atlanta,\nGA, USA, page 39. IEEE Computer Society, 2006.\n[82] J. Sun and G. Li. An end-to-end learning-based cost estimator. Proc. VLDB\nEndow., 13(3):307–319, 2019.\n[83] M. Sundararajan, A. Taly, and Q. Yan. Axiomatic attribution for deep networks.\nInProceedings of the 34th International Conference on Machine Learning, ICML\n2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of\nMachine Learning Research, pages 3319–3328. PMLR, 2017.\n[84] J. Tan, T. Zhang, F. Li, J. Chen, Q. Zheng, P. Zhang, H. Qiao, Y. Shi, W. Cao, and\nR. Zhang. ibtune: Individualized buﬀer tuning for large-scale cloud databases.\nProc. VLDB Endow., 12(10):1221–1234, 2019.\n[85] S. Thirumuruganathan, S. Hasan, N. Koudas, and G. Das. Approximate query\nprocessing for data exploration using deep generative models. In 2020 IEEE 36th\nInternational Conference on Data Engineering (ICDE), pages 1309–1320, 2020.\n[86] H. To, K. Chiang, and C. Shahabi. Entropy-based histograms for selectivity\nestimation. In 22nd ACM International Conference on Information and Knowledge\nManagement, CIKM’13, San Francisco, CA, USA, October 27 - November 1, 2013,\npages 1939–1948. ACM, 2013.\n[87] I. Trummer, J. Wang, D. Maram, S. Moseley, S. Jo, and J. Antonakakis. Skinnerdb:\nRegret-bounded query evaluation via reinforcement learning. In Proceedings of\nthe 2019 International Conference on Management of Data, SIGMOD Conference\n2019, Amsterdam, The Netherlands, June 30 - July 5, 2019 , pages 1153–1170. ACM,\n2019.\n[88] K. Tzoumas, A. Deshpande, and C. S. Jensen. Lightweight graphical models for\nselectivity estimation without independence assumptions. Proc. VLDB Endow.,\n4(11):852–863, 2011.\n[89] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser,\nand I. Polosukhin. Attention is all you need. In Advances in Neural Informa-\ntion Processing Systems 30: Annual Conference on Neural Information Processing\nSystems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 5998–6008, 2017.\n[90] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau,\nE. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett,\nJ. Wilson, K. J. Millman, N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson,\nC. J. Carey, İ. Polat, Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold,\nR. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H.\nRibeiro, F. Pedregosa, P. van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0:\nFundamental Algorithms for Scientiﬁc Computing in Python. Nature Methods,\n1653\n\n\n17:261–272, 2020.\n[91] L. Woltmann, C. Hartmann, M. Thiele, D. Habich, and W. Lehner. Cardinal-\nity estimation with local deep learning models. In Proceedings of the Second\nInternational Workshop on Exploiting Artiﬁcial Intelligence Techniques for Data\nManagement, aiDM@SIGMOD 2019, Amsterdam, The Netherlands, July 5, 2019,\npages 5:1–5:8. ACM, 2019.\n[92] W. Wu, J. F. Naughton, and H. Singh. Sampling-based query re-optimization. In\nProceedings of the 2016 International Conference on Management of Data, SIGMOD\nConference 2016, San Francisco, CA, USA, June 26 - July 01, 2016, pages 1721–1736.\nACM, 2016.\n[93] Y. Wu, D. Agrawal, and A. E. Abbadi. Using the golden rule of sampling for query\nestimation. In Proceedings of the 2001 ACM SIGMOD international conference on\nManagement of data, Santa Barbara, CA, USA, May 21-24, 2001, pages 449–460.\nACM, 2001.\n[94] Z. Yang, A. Kamsetty, S. Luan, E. Liang, Y. Duan, P. Chen, and I. Stoica. Neurocard:\nOne cardinality estimator for all tables. Proc. VLDB Endow., 14(1):61–73, 2020.\n[95] Z. Yang, E. Liang, A. Kamsetty, C. Wu, Y. Duan, P. Chen, P. Abbeel, J. M. Heller-\nstein, S. Krishnan, and I. Stoica. Deep unsupervised cardinality estimation. Proc.VLDB Endow., 13(3):279–292, 2019.\n[96] X. Yu, G. Li, C. Chai, and N. Tang. Reinforcement learning with tree-lstm for\njoin order selection. In 36th IEEE International Conference on Data Engineering,\nICDE 2020, Dallas, TX, USA, April 20-24, 2020, pages 1297–1308. IEEE, 2020.\n[97] M. Zaït, S. Chakkappen, S. Budalakoti, S. R. Valluri, R. Krishnamachari, and\nA. Wood. Adaptive statistics in oracle 12c. Proc. VLDB Endow., 10(12):1813–1824,\n2017.\n[98] J. Zhang, Y. Liu, K. Zhou, G. Li, Z. Xiao, B. Cheng, J. Xing, Y. Wang, T. Cheng,\nL. Liu, M. Ran, and Z. Li. An end-to-end automatic cloud database tuning system\nusing deep reinforcement learning. In Proceedings of the 2019 International\nConference on Management of Data, SIGMOD Conference 2019, Amsterdam, The\nNetherlands, June 30 - July 5, 2019, pages 415–432. ACM, 2019.\n[99] Q. Zhou. An experimental relational optimizer and executor. https://github.\ncom/zhouqingqing/qpmodel. Accessed: 2020-11-30.\n[100] X. Zhou, C. Chai, G. Li, and J. Sun. Database meets artiﬁcial intelligence: A\nsurvey. IEEE Transactions on Knowledge and Data Engineering, 2020.\n1654\n",
  "textLength": 102651
}