{
  "paperId": "e2d55453c901bc5b26ae24b80d14a9a88c35f71f",
  "title": "Ways of Applying Artificial Intelligence in Software Engineering",
  "pdfPath": "e2d55453c901bc5b26ae24b80d14a9a88c35f71f.pdf",
  "text": "Ways of Applying Artificial Intelligence in Software Engineering\nRobert Feldt, Francisco G. de Oliveira Neto, and Richard Torkar\nChalmers and the University of Gothenburg\nGothenburg, Sweden\n[robert.feldt|gomesf|torkarr]@chalmers.se\nABSTRACT\nAs Artificial Intelligence (AI) techniques have become more power-\nful and easier to use they are increasingly deployed as key compo-\nnents of modern software systems. While this enables new function-\nality and often allows better adaptation to user needs it also creates\nadditional problems for software engineers and exposes compa-\nnies to new risks. Some work has been done to better understand\nthe interaction between Software Engineering and AI but we lack\nmethods to classify ways of applying AI in software systems and\nto analyse and understand the risks this poses. Only by doing so\ncan we devise tools and solutions to help mitigate them. This paper\npresents the AI in SE Application Levels (AI-SEAL) taxonomy that\ncategorises applications according to their point of AI application,\nthe type of AI technology used and the automation level allowed.\nWe show the usefulness of this taxonomy by classifying 15 papers\nfrom previous editions of the RAISE workshop. Results show that\nthe taxonomy allows classification of distinct AI applications and\nprovides insights concerning the risks associated with them. We\nargue that this will be important for companies in deciding how to\napply AI in their software applications and to create strategies for\nits use.\nCCS CONCEPTS\n•Computing methodologies →Artificial intelligence ;•Soft-\nware and its engineering ;\nKEYWORDS\nTaxonomy, Software Engineering, Artificial Intelligence\nACM Reference Format:\nRobert Feldt, Francisco G. de Oliveira Neto, and Richard Torkar. 2018. Ways\nof Applying Artificial Intelligence in Software Engineering. In Proceedings\nof 6th International Workshop on Realizing Artificial Intelligence Synergies in\nSoftware Engineering (RAISE’18). ACM, New York, NY, USA, 7 pages.\n1 INTRODUCTION\nArtificial Intelligence (AI) has shown a lot of promise in the last\ndecades but with the recent resurgence of interest and improved\nresults on real-world tasks the field is undergoing explosive growth.\nMany of the improved results have come from larger and more com-\nplex neural networks, stacked many layers deep (for so called Deep\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nRAISE’18, May 2018, Gothenburg, Sweden\n©2018 Association for Computing Machinery.Learning), but much of progress can also be attributed to larger\ndata sets and large-scale learning/training on GPUs [ 40]. But the\nrenewed interest and increasing amount of resources has also lead\nto breakthroughs in related AI technologies, e.g. Bayesian statis-\ntics [ 7,21], generative models [ 22], and probabilistic programming/\ninduction [29].\nHowever, there has been recent criticism that many of these\napproaches to building more intelligent software are too far from\nhuman-level intelligence and, thus, are not likely to be enough [ 30,\n33]. Instead the critics argue that we actually need algorithms that\nbuild and extend causal models, can learn from very few examples\n(one- or few-shot learning), and can reason symbolically with the\npatterns and knowledge they extract from sensors [30, 33].\nRegardless if the current set of AI technologies will be enough\nto reach human-level intelligence or not it is clear that software\nsystems will increasingly incorporate them as components and\nsub-systems. The form of the solutions produced from these AI/ML\ntechnologies often look inherently different from the software that\nis normally developed and deployed. Thus, not only does the AI\ntechnology itself change quickly and at an increasing pace, the\nsolutions it provides typically look very different from what soft-\nware organisations and engineers are used to. This poses a new\nand unique set of risks and opportunities for software organisa-\ntions and they need to understand and analyse these risks to select\nappropriate strategies.\nThis hybridisation of AI/ML and software engineering is in-\nevitable also in another sense. There will be ample opportunity to\napply AI and ML models to improve software development itself.\nSoftware engineers are close to these technologies and are likely to\nbe early adopters in applying them on their own problems, methods\nand tools. This is also helped by the trend that AI/ML technologies\nis increasingly componentised and can be more easily used and\nreused, even by non-experts. Advances in software engineering\nallow AI technologies to be packaged and easily reusable through\nRESTful APIs1as automated cloud solutions, which can use mul-\ntiple technologies before selecting and automatically tuning one/\nseveral2. As AI becomes more accessible its use can be expected to\nincrease even more.\nGiven the current expansion of the field and the large number of\ndifferent ways that AI/ML can be applied during the engineering of\nsoftware and in the software systems themselves, there is a risk of\nconfusion and miscommunication. If we do not have shared terms\nto describe the terrain and an overview of possibilities it becomes\nharder for engineers and software organisations to properly assess\nrisks and discuss mitigation strategies. In our experience, of work-\ning with and applying AI/ML technologies to software engineering\nproblems in industry, we have seen this first-hand [ 1,6,11,16–19].\n1https://bigml.com/api\n2https://cloud.google.com/automlarXiv:1802.02033v2  [cs.SE]  7 Feb 2018\n\nRAISE’18, May 2018, Gothenburg, Sweden Robert Feldt, Francisco G. de Oliveira Neto, and Richard Torkar\nClear definition of terms and a way to classify and understand\nopportunities and risks can be critical in improving communication\nand enabling more detailed weighing of alternatives. This forms the\nbasis of creating organisational strategies. Here, we propose an ini-\ntial taxonomy with which to analyse and understand different ways\nof applying artificial intelligence and machine learning in software\nengineering. To show the utility of the approach we apply it to a\nsample of papers that have been published at the RAISE workshop\non realizing synergies between AI and software engineering.\n2 BACKGROUND AND RELATED WORK\nSince the early days when Barr and Feigenbaum [ 4] discussed the\npossibilities of combining AI with software engineering, there have\nbeen some attempts to classify the field to systematically be able\nto attack key challenges. In 1987, Barstow [ 5] presented a review\nof how one should apply AI techniques to software engineering\nproblems. He distinguished between programming-in-the-small\n(by individuals or very small groups) and programming-in-the-\nlarge (by very large groups of people). He divided AI usage into\nfive broad categories: Software engineering methodologies, pro-\ngramming techniques, the architecture of the target machine, the\napplication domain, and the history of the target software.\nAt RAISE’12, Clifton et al. [ 9] provided an overview of machine\nlearning and software engineering in health informatics by pre-\nsenting ongoing work from several projects. Many of the presented\ncases show that the scale of clinical practice requires new engineer-\ning approaches from both disciplines. This is but one example where\none sees that both AI and SE need new engineering approaches for\nspecific domains. However, we also see a need to address challenges\non a higher level of abstraction as Harman [ 24] pointed out in his\nRAISE keynote that same year. From Harman’s perspective, the\nabstraction would serve us in developing “strategies for finding\nsolutions rather than the solutions themselves.” In this particular\ncase, we are not looking specifically at finding strategies by the use\nof abstraction, but rather to provide researchers and practitioners\nwith a view of AI in SE, and how ‘embedded’ we would wish for AI\ntechniques to be, i.e., answering the why and what, while taking\nrisk into consideration.\nOn that note, Davis et al. [ 10], propose a taxonomy of AI ap-\nproaches for adaptive distributed real-time embedded (DRE) sys-\ntems. The taxonomy classifies AI approaches according to five prop-\nerties needed for adaptive DRE systems: i) supporting a distributed\nenvironment, ii) supporting real-time requirements, iii) supporting\nan embedded environment, iv) robustly handling new data, and v)\nincorporating new data into the approach as it becomes available\nwhile the system is running. Their classification is fine-grained and\nis limited to the context of DRE system, since the goal is identifying\nsuitable AI applications.\nAnother example of a taxonomy tailored to specific AI appli-\ncations in SE is proposed by Charte et al. [ 8]. Their taxonomy\nprovides a broad view of autoencoders (AE) which are Artificial\nNeural Networks (ANNs) that produce codifications for input data\nand are trained so that their decodifications resemble the inputs\nas closely as possible. The AEs are used in different applications\nrelated to SE such as data compression, hashing and visualisation.\nThe taxonomy is based on the different features of an AE, such aslower dimensionality, or noise tolerance. However, Charte et al. do\nnot include AI technologies beyond AEs.\nWe could not find other taxonomies that covers classification\nwithin both areas of AI and SE; existing proposals focus on one\nor the other and are rather fine-grained. Unlike the existing tax-\nonomies, the one we present in this paper, AI-SEAL (Artificial\nIntelligence in Software Engineering Application Levels), aims to\nbe more general and allow classification beyond a specific subject\nmatter, such as for a specific type of software system, e.g., DRE\nsystems, or a specific type of AI technology, e.g., AEs. It has three\nmain facets (dimensions) to explore different perspectives of AI\nin SE on a more general level, thus allowing us to cover more AI\napproaches and be more inclusive of other application domains\nwithin software engineering. By being more coarse-grained it can\nalso be complemented by more detailed taxonomies, such as the\nones described above, or additional, lower-level facets, as outlined\nat the end of the next section.\nAI-SEAL also differentiates itself by targeting a different use\ncase. While the taxonomies and categorisations we have described\nabove focus on providing an overview of the field or outlining\npossibilities when applying AI technologies in SE, our overall aim\nis to support companies and organisations developing strategies for\nsuch applications. The main aim is thus to be able to estimate and\nanalyse the risks and costs involved, not only the possibilities. When\na certain type of technology is new there is a tendency to focus\non possibilities and the upside of its application while real-world\nadoption requires a deeper understanding also of the downsides.\nOur focus is on helping organisations better understand the risks.\n2.1 Methodology\nIn order to propose AI-SEAL we investigated the creation and usage\nof taxonomies in SE. There are several taxonomies proposed for\nSE in all of its different knowledge areas (e.g., requirements and\ntesting [ 43]), but very few are created systematically [ 44]. Usman\net al. performed a systematic mapping on the use of taxonomies\nin software engineering, and proposed a method to develop such\ntaxonomies [44], which we used when defining AI-SEAL.\nThere are four phases in the Usman et al. method: i) planning,\nii) identification and extraction, iii) design and construction, and\niv) validation. During planning we decided to be inclusive of all\nknowledge areas within software engineering (e.g., testing, require-\nments, processes) and rather seek a more fundamental aspect of\nhow AI is applied. We argue that this is natural since it would not\nmatter much to the risks involved whether one applies AI to for\nexample requirements or design; we argue that the main risks arise\nif the actual software itself changes shape and to what degree the\nengineers or, even later, the users, can change the proposals and/or\ndecisions put forward or implemented by the AI or software created\nby it.\nThe goal with the identification and extraction phase is to iden-\ntify the main categories and associated terms used in the taxonomy.\nThe challenge was to identify terms and categories pertaining to\nthe different SE knowledge areas and the variety of available AI\ntechnologies, not to mention that both fields are constantly chang-\ning and evolving. Based on our own experience from applying AI\nin SE, and from reading relevant papers published in the last years,\n\nWays of Applying Artificial Intelligence in Software Engineering RAISE’18, May 2018, Gothenburg, Sweden\nwe identified many key facets/dimensions but then filtered them\ndown.\nWe extracted three key facets for our taxonomy: point of ap-\nplication, automation level and AI technology. We decided to use\na faceted analysis because this type of classification structure is\nsuitable for new and evolving fields, since complete knowledge\nrelated to the subject area is not required or available [44].\nThe design and construction phase presents the different lev-\nels within those facets, how they were chosen and to what extent\nthey are connected to the taxonomy’s purpose and usage (we ex-\nplain the details in Section 3). Lastly, the validation phase aims at\ndemonstrating how the subject matter can be classified using the\nproposed taxonomy. Literature provides three distinct validation\nmethods: Orthogonality demonstration, benchmarking and utility\ndemonstration [ 44]. We chose the latter and classified a set of AI\napplications reported in previous editions of the Workshop on Re-\nalizing Artificial Intelligence Synergies in Software Engineering\n(RAISE). Additionally, utility demonstration is the most reported\nmethod to validate taxonomies in SE, and it allows us to showcase\nthe classification extent of the taxonomy [44].\n3 TAXONOMY FOR AI-IN-SE APPLICATION\nLEVELS\n3.1 Overview and Description\nThe purpose with the Artificial Intelligence in Software Engineering\nApplication Levels (AI-SEAL) taxonomy is to support researchers\nand practitioners to communicate, understand and discuss the pros\nand cons of applying AI approaches when developing and in run-\nning software systems. We argue that since the end goal of any\nsoftware engineering process is to deliver a running software sys-\ntem we cannot exclude the actual use of AI during system execution.\nAn explicit goal was to keep the number of facets to a minimum;\nin the end we propose only three. Even though they can be fur-\nther sub-divided and additional facets can help detail them more\nwe argue that this can be left for future work. A simpler taxon-\nomy is more likely to be useful, in particular for practitioners and\ncompanies. The three facets we propose as critical are Point of Ap-\nplication (PA), Type of AI (TAI) applied, and Level of Automation\n(LA) offered.\nThe point of application (PA) includes both the ‘when’ (in time)\nand the ‘on what’ (location) the AI technology is being applied\n(Figure 1). There are three major levels of this facet, two that are\nrelevant before deployment of the software system ( process and\nproduct ) while the third is post-deployment representing the run-\ntime application of AI in a software system.\nThe process level indicates that the AI is applied in the software\ndevelopment process and does not necessarily affect, directly, the\nsource code that will be deployed. An example would be test analyt-\nics, which could be used to optimise testing, but it does not by itself\ndirectly alter the code, e.g., [ 2,18]. In contrast, the product level\nindicates that the AI directly affects the source code. A concrete\nexample would be automated program repair, which manipulates\nthe code directly to automatically fix defects [35].\nThe runtime level represents AI applications that affect the de-\nployed software system during runtime. The canonical example\nwould be autonomous and self-adaptive software systems in which\nProcess\nProduct\nRuntime\nIndirectDirectPre deployDuring execution\nEffect on Source Code\nWhenFigure 1: Overview of the different points of application (PA)\nin the AI-SEAL taxonomy.\nsome AI technology is learning and changing the system itself in a\nfeedback loop [ 42]. A more mundane, but recent example, would\nbe the online learning of more optimal data structures and database\nindices based on the actual data stored during operation, in line\nwith recent results from Google [28].\nSome applications can span these main levels of PA or can be\nviewed as borderline between levels. An example would be au-\ntonomous driving software that includes an ANN. In a situation\nwhere the AI develops a part of the code, which is then compiled\ninto the binary that goes into the final product, it would be clas-\nsified under the product application level. However, if the ANN is\ndynamically updating itself using runtime information from the\nexecuting software, then its PA classification would be set to the\nruntime application of AI. For risk analysis we argue it makes more\nsense to then select the latter (higher) level, i.e., runtime over prod-\nuct and product over process. The reason is that the higher the level\nthe less time there is, in general, for humans to intervene or even\nto analyse the result of the applied AI technology.\nThe next facet of AI-SEAL covers the Type of AI (TAI) that is\napplied. Since there is not even a consensus around what AI is it\nbecomes hard to propose a particular and stable set of levels for this\nfacet. This facet is, thus, most likely to need to change as progress\nis made and new types of AI approaches are proposed. As a starting\npoint we propose that ‘the five tribes of AI’ classification introduced\nby Domingos [15] can be useful:\n•Symbolist, e.g., inverse deduction.\n•Connectionist, e.g., backpropagation.\n•Evolutionaries, e.g., genetic programming.\n•Bayesians, e.g., probabilistic inference.\n•Analogizers, e.g., kernel machines.\nEven though these five tribes capture general types of AI tech-\nnology it is clear that the TAI facet can be made more detailed and\ndivided into further sub-dimensions, depending on the represen-\ntations, algorithms, and artefacts used in a particular application\nof AI. However, we argue that specifying sub-dimensions hinders\nthe practical use of the taxonomy since it can be confusing to pre-\ncisely distinguish among the different existing algorithms, mainly\nif more than one AI technique is involved in, e.g., the product. In\nother words, AI-SEAL users can choose to go deeper into that facet\nwithin their domain-specific constraints, but we do not incorporate\nthose sub-dimensions into the taxonomy itself. The main purpose of\n\nRAISE’18, May 2018, Gothenburg, Sweden Robert Feldt, Francisco G. de Oliveira Neto, and Richard Torkar\nTable 1: Levels of automation (LA) of decision and action se-\nlection (from [20] and [41])\n10.Computer makes and implements decision if it feels it should,\nand informs human only if it feels this is warranted.\n9. Computer makes and implements decision, and informs human\nonly if it feels this is warranted.\n8. Computer makes and implements decision, and informs human\nonly if asked to.\n7.Computer makes and implements decision, but must inform\nhuman after the fact.\n6.Computer makes decision but gives human option to veto\nbefore implementation.\n5.Computer offers a restricted set of alternatives and suggests\none, which it will implement if human approve.\n4.Computer offers a restricted set of alternatives and suggests\none, but human still makes and implements final decision.\n3.Computer offers a restricted set of alternatives, and human\ndecides which to implement.\n2. Computer offers a set of alternatives which human may ignore\nin making decision.\n1. Human considers alternatives, makes and implements decision.\nthe TAI facet is to consider the particular properties of the applied\nAI technology and how they interact with the PA and LA facets in a\nparticular application. For example, in a product application the risk\nmight be much higher with using a Connectionist AI technology,\nwhich produces an opaque neural net that is hard to analyse and\ntest, than if using an evolutionary search process to find decision\nrules that are short and can be analysed before they are deployed.\nThe last facet is the level of automation , i.e., LA, which the AI\napplication aims at or achieves. We base the levels of this facet on the\nSheridan-Verplanck 10 levels of automation, an existing taxonomy\nfrom Automation/HCI research that focuses on human-computer\ndecision making [20, 41] (Table 1).\nThe Sheridan-Verplanck taxonomy conveys how different hu-\nman operators (e.g., a developer, tester, user, or any stakeholder\nin the software system) and the technical system (e.g., AI tech-\nnology) should cooperate by sharing the control of determining\nand selecting options to implement tasks. At lower levels of au-\ntomation, the AI technology simply provides data through, for\ninstance, dashboards with descriptive and visual information, while\nthe stakeholder responsible for understanding the information and\ndetermining the next course of action of the software.\nAs we climb up these levels, we allow the AI technology to be\nmore autonomous by either allowing it to suggest alternatives to the\nhuman operator (Level 2) or even, at the top level, implement the\ndecisions itself and only inform the human if it so decides (Level 10).\nHence, the higher the level of automation, the more autonomous the\nAI technology becomes in making decisions related to the element,\ne.g., product, process, or runtime, where it is being applied. And,\nwe argue, the higher the risk involved3.\n3We admit that there is also a case to be made that ultimately, and especially for some\ntasks, solutions based on AI might be less risky than having humans have the final say\nbefore taking action. However, we argue that the decision to trust AI solutions will\nhave to be based on other risk-reduction strategies such as taking a general decision to\ntrust certain types of systems based on empirical evidence of their safety. We thus see\nPoint of Application\nLevels of AutomationManualAutonomousProcessProductRuntimeHigher RiskLower RiskFigure 2: Levels of risk on the AI-SEAL taxonomy with re-\nspect to Points of Application (PA) and Levels of Automation\n(LA).\nNote that one of the main benefits of the three facets of the\ntaxonomy is to allow practitioners and researchers to classify and\nunderstand the risks involved in the AI application. If we consider\nthe PA facet, we argue that the risk of applying AI increases as\nwe move from process to the runtime level. At each step, there is\nmore at stake since negative consequences due to the application of\ninappropriate AI technologies are worse/costlier to reverse closer to\n(or after) deploying the product. As such, PA is closely related to the\nsize of the impact as well as level of control that developers have on\nthe AI application. Similarly, the LA facet is riskier at higher levels,\nsince stakeholders will have less time to reverse decisions when\nthe AI has a higher degree of autonomy (see the risk boundaries in\nFigure 2).\nThus, if an AI technology is new to a company, practitioners\nshould start at low levels of automation (LA) to allow more hu-\nman intervention, as well as at a ‘lower’ point of application (PA),\nwhere potential issues with introducing AI will not be directly in-\ntroduced to the source code. Then, by building more experience\none can expand to explore higher levels of automation and points\nof application of the AI technology in SE and software systems.\n4 EVALUATION\nWe evaluate the AI-SEAL taxonomy through a utility demonstra-\ntion [ 44] by classifying papers from previous editions of the RAISE\nworkshop. The workshop focuses on papers showcasing applica-\ntions of AI in SE, in a broad sense, and is, thus, a suitable venue for\nidentifying relevant papers.\nThe first instance of RAISE was held in 2012 and, except for\n2017, it has been running annually (2012–2016). A total of 44 papers\nhave been presented at RAISE over the years. Based on the title we\nexcluded ten papers that we assessed as not presenting a specific\napplication/solution of AI in SE, e.g., surveys, or papers presenting\nchallenges or open issues. For the remaining set of papers we then\nincluded the most recent ones, published at RAISE 2015 (six out of\nthis as a future extension possibility rather than a threat to the taxonomy presented\nhere.\n\nWays of Applying Artificial Intelligence in Software Engineering RAISE’18, May 2018, Gothenburg, Sweden\nTable 2: Classification of 15 RAISE papers according to the\nAI-SEAL taxonomy. Papers #6 and #15 both discuss Runtime\nand higher levels of automation of the AI so we mark them\nas borderline FW (Future Work) below and list them twice.\nID ReferenceAppl. Point Type of AI Level of Auto\n(PA) (TAI) (LA)\n#1 [26] Process Analogizer 2\n#2 [27] Process Analogizer 4\n#31 [32] Process Connectionist 7\n#34 [38] Process Symbolist 9\n#36 [39] Process Analogizer 2–3\n#37 [37] Process Symbolist 2–3\n#38 [23] Process Symbolist 2\n#40 [13] Process Evolutionary 2–3\n#41 [14] Process Analogizer 2–3\n#42 [36] Process Analogizer 2–3\n#43 [3] Process Analogizer 2–3\n#44 [34] Process Analogizer 2–3\n#6 [12] Product Symbolist 4\n#15 [25] Product Analogizer 7\n#35 [31] Product Symbolist 9\n#6 [12] (Runtime) FW Symbolist (8) FW\n#15 [25] (Runtime) FW Analogizer 7\nseven papers; one not found in the IEEE database) and 2016 (four\npapers), as well as a random sample of another five for a total of\n15 papers4. We then applied the AI-SEAL taxonomy to these 15\npapers. In Table 2 we show an overview of the results and below\nwe describe four papers in more detail, to show the value of our\napproach.\nPaper #1, written by Iliev et al. [ 26] uses an ontology, based\non design information provided by stakeholders, to automatically\npredict the severity level of a defect. The AI suggests the sever-\nity levels to stakeholders who, in turn, can accept or ignore the\nsuggestions. Therefore, the AI-SEAL classification of Paper #1, for\nPA, TAI and LA is, respectively, Process ,Level 2 of Automation and\nincluded in the Analogizer tribe since the design information and\nthe classification rules used by the ontology are defined in advance\nby stakeholders.\nPaper #6, written by de Souza Alcantara et al. [ 12] presents an\napproach where a tool learns a set of gestures that UI designers can\nuse to design gesture-based applications for multi-touch devices.\nWe classify this as a Symbolist AI technology since it analyses\nthe relations between and reasons about the different steps of a\nspecific gesture, which are inferred based on a set of primitives.\nHence, the designer first trains the tool to learn a set of gestures\nthat can be used when designing the UI of the application. Then,\nduring the actual design of the UI, IGT can detect gestures outside\nthe standards and prompt the designer to ask whether the drawn\ngesture was a mistake. Authors state that the gestures definitions are\nnot updated; therefore, the levels for PA and LA are, respectively,\nProduct (the final gesture identification output from the tool in\nincluded in the developed application) and Level 4 . However, authors\n4https://goo.gl/ERPoUk\nPA\nLA110ProcessProductRuntime\n512SymbolistConnectionistEvolutionaryBayesianAnalogizer\n3436383141424344403515637Figure 3: AI-SEAL classification of papers from different edi-\ntions of RAISE. The papers used in this classification were\n[3, 12–14, 23, 25–27, 31, 32, 34, 36–39].\ndiscuss future work where the interaction between designer and\nthe tool is higher, and the gestures definitions are updated; this\ncould move the technology to higher levels of LA and, possibly, PA.\nPaper #15, written by Heitmeyer et al. [ 25], is titled High as-\nsurance human-centric decision systems and proposes an approach\nwhere AI techniques are used to detect and assist operators of a\ndecision system that start to feel overloaded given the complexity\nof tasks in the decision system. They use two AI techniques, both\nfrom the Analogizers tribe, to predict human overload based on the\npast interactions of the operator with the system. Ultimately, the AI\napplication should take over the system operation while alerting the\noperator ( LA = 7 ). Therefore, the application is at the product level,\nsince the overload model does not update during runtime, even\nthough authors plan to address this issue in future work. This is an\nexample how the application can actually begin at lower risk levels,\nand then evolve to higher risk areas of the AI-SEAL taxonomy.\nPaper #31, written by Langer and Oswald [ 32], is titled A self-\nlearning approach for validation of communication in embedded\nsystems . The authors use neural networks ( Connectionist ) to learn\nwhich communication traces are valid in a distributed embedded\nsystem. The proposed approach is used for automated integra-\ntion testing focusing the communication between the distributed\ncomponents, thus being applied at the process level. Then, the AI\nautomatically analyses the communication traces, and notify users\nonly if an invalid communication trace is found ( LA = 7 ).\nWe summarise the classification of the papers in our taxonomy\nin Figure 3 and Table 2. Most of the papers ( 53%) use analogizers,\nand, regarding the PA facet, most of them are applied at the process\nlevel ( 80%), and at lower levels of automation. In contrast, none\nof the evaluated papers proposed applications of AI technologies\nfrom the Bayesian tribe, and only two of them discussed Runtime\napplication of the AI as future work. We argue that this more might\nshow a bias of the RAISE workshop than anything fundamental; for\nexample we gave two papers as examples of Runtime applications\nwhen introducing the facet. An explanation for the little use of\nBayesian solutions might be that they are not as clearly seen as\npart of AI technology as part of advanced statistics; however, as\nDomingos makes clear in his book their view of AI is also valid and\ngeneral [15].\n\nRAISE’18, May 2018, Gothenburg, Sweden Robert Feldt, Francisco G. de Oliveira Neto, and Richard Torkar\nWe note that this is an initial proposal on how the taxonomy\ncan be used to raise awareness of current state of art regarding AI\napplications in SE. Certainly, some applications will be more chal-\nlenging than others to classify, particularly when the application\nor technology span across different levels of one or more facets or\nas new AI technologies and solutions are introduced.\n5 DISCUSSION\nWe have introduced a taxonomy (AI-SEAL) having three facets\nthat can be used to classify and analyse ways of applying AI in\nsoftware engineering and thus help understand the associated risks\nand opportunities. Our main argument is that the risk one takes\nin applying AI relates to the level of control and the time given to\nexert control that the developers and users have (over) the decisions\nproposed or taken by the AI component. Together this explains two\nof the three facets; the third is used for basic characterisation of\nthe AI technology being applied.\nOur evaluation shows that AI-SEAL can classify applications\nregardless of the SE knowledge area (e.g., design, development,\nmaintenance) and even domain-specific information (e.g., UI design,\nsafety-critical systems, or object-oriented programming) involved.\nThis is by design; we argue that a taxonomy needs to have this\nproperty to be generally useful. It can of course be complemented\nby additional facets or existing taxonomies to further detail the two\nmain facets.\nSimilarly, the PA facet is inclusive of different application do-\nmains and types in SE, since it depends on elements present in any\nSE application, i.e., the source code of the system, that is eventually\ndeployed, and a process to develop it. There are plenty of ways to\nexpand on the PA facet, for example by using SWEBOK’s knowl-\nedge areas to further describe what the AI is applied to and when. In\nfact, this possibility to update and expand the facets independently\nis one of the main benefits of a faceted taxonomy [ 44], thus being\na suitable design choice for a taxonomy in dynamic fields such as\nAI and SE.\nThe demonstration (Figure 3) also reflects the risks of the classi-\nfied applications. For instance, Paper #15 is an example of a software\nsystem controlled by an AI and a human operator that cooperate to\ncontrol unmanned air vehicles (UAV), an example of autonomous\nvehicle, which are widely used by the military for surveillance and\ntargeting [ 25]. Therefore, the risk related to that AI in SE applica-\ntion is high since mistakes caused by the AI could have adverse\noutcomes.\nOn the other hand, Paper #1 [ 26] has lower risk, where the AI\nautomatically classifies the level of severity of found defects. Note\nthat at LA=2the AI provides input, but stakeholders are the ones\nresponsible to decide, so the development process can recover from\neventual adversities with poor classification. Certainly, at higher\nlevels of automation (e.g., LA ≥6), the risks would be higher since\nthe AI becomes more autonomous and can disrupt the development\nprocess or lead to decisions that ultimately decrease quality.\nOn that note, the risks in Papers #2 and #6 differ mainly due to\nthe PA levels. Paper #2 detects clones on source code during main-\ntenance, thus having an impact on maintenance costs and internal\nquality (e.g., refactoring) as opposed to paper #6 that is used to as-\nsist its end-user (UI designers). Any mistakes in the AI applicationin Paper #6 affects the product, thus its impact on external quality\nis higher.\nCertainly, the risk analysis is sensitive to other factors involving\nboth the AI and SE parts. For example, the familiarity that the\ncompany and the engineers have with the particular AI technology\nbeing used will be important. However, the value of AI-SEAL is to\ngive an overview of the risk and the possible impacts5. Therefore,\nwe recommend practitioners interested in exploring possible AI\napplication in their SE projects to begin in the lower risk areas, and\nthen move towards riskier areas as they gather experience on the\nAI application in SE. For researchers, AI-SEAL is useful to map the\nfield and identify areas to employ more research effort.\nIn it’s current form AI-SEAL does not help in specific classifica-\ntion of AI technology, for example helping practitioners to under-\nstand the distinction between different ANN models, or exposing\nthe trade-offs when applying machine learning algorithms on differ-\nent knowledge areas in SE. Instead, our taxonomy aims to support\npractitioners and researchers in understanding the high-level as-\npects and the impact of their AI applications. That decision is more\non the strategic level rather than a solution instance (e.g., which\nANN to choose from).\nIn general we agree with Harman that compartmentalising and\ndeconstructing AI for SE into sub-domains is tempting but would\nbe a mistake [ 24]. The SE community can benefit significantly by\ndiscussing the strategies rather than the solutions themselves, and\nwe believe that AI-SEAL can help in this endeavour.\n6 CONCLUDING REMARKS\nIn this paper we propose the AI-SEAL taxonomy to help researchers\nand practitioners to classify different AI applications in software\nengineering. The taxonomy has three facets allowing its users to\nclassify, the point of application (process, product and runtime),\nthe type of AI technology (based, initially, on the five tribes pro-\nposed by Domingos [ 15]) and the level of automation of the applied\ntechnology (inspired by Sheridan-Verplanck’s 10 levels of automa-\ntion [41]).\nBesides helping its user to understand the field of AI applica-\ntions in SE, AI-SEAL provides a basis for software engineers to\nconsider the risks of applying AI. This advantage allows, for in-\nstance, practitioners to reason about the trade-offs of introducing\nthe AI technology in their processes and products. In addition, the\ntaxonomy is not constrained by domain-specific applications, thus\ncovering all knowledge areas of software engineering.\nWe demonstrate the use of the taxonomy by classifying 15 pa-\npers from past RAISE workshops. Most of the papers focused on\nsupporting stakeholders during the development process but did\nnot directly affect the source code or the runtime behaviour of the\nsystems. There was also an uneven use of the many different AI\napproaches that exist; in particular a lack of Bayesian and, sur-\nprisingly, Connectionist (neural net) ones. Future work includes\nclassification of more papers as well as proposing more detailed\nand explicit support for risk analysis.\n5We also see a potential to analyse the overall ‘reach’ of the decisions/proposals of AI\ncomponent in the wider system in a more detailed risk analysis. Since the ultimate risk\nmight not be high, despite a high LA value, if the impact of the decisions are limited\nby the rest of the system.\n\nWays of Applying Artificial Intelligence in Software Engineering RAISE’18, May 2018, Gothenburg, Sweden\nREFERENCES\n[1]W. Afzal and R. Torkar. 2008. A comparative evaluation of using genetic program-\nming for predicting fault count data. In 3rd International Conference on Software\nEngineering Advances . IEEE, NJ, USA, 407–414. https://doi.org/10.1109/ICSEA.\n2008.9\n[2]W. Afzal and R. Torkar. 2008. A comparative evaluation of using genetic program-\nming for predicting fault count data. In 3rd International Conference on Software\nEngineering Advances . IEEE Press, NJ, USA, 407–414. https://doi.org/10.1109/\nICSEA.2008.9\n[3]S. Akbarinasaji, A. B. Bener, and A. Erdem. 2016. Measuring the principal of defect\ndebt. In 5th International Workshop on Realizing Artificial Intelligence Synergies in\nSoftware Engineering (RAISE ’16) . ACM, NY, USA, 1–7. https://doi.org/10.1145/\n2896995.2896999\n[4]A. Barr and E. A. Feigenbaum. 1981. The handbook of artificial intelligence .\nHeirisTech Press, CA, USA.\n[5]D. Barstow. 1987. Artificial intelligence and software engineering. In 9th Inter-\nnational Conference on Software Engineering (ICSE ’87) . IEEE Computer Society\nPress, CA, USA, 200–211.\n[6]M. Bäumer, P. Seidler, R. Torkar, R. Feldt, P. Tomaszewski, and L.-O. Damm. 2008.\nPredicting fault inflow in highly iterative software development processes: An\nindustrial evaluation. In 19th International Symposium on Software Reliability\nEngineering: Industry Track . IEEE, NJ, USA.\n[7]B. Carpenter, A. Gelman, M. Hoffman, D. Lee, B. Goodrich, M. Betancourt, M. A.\nBrubaker, J. Guo, P. Li, A. Riddell, et al .2016. Stan: A probabilistic programming\nlanguage. Journal of Statistical Software 20, 2 (2016), 1–37.\n[8]D. Charte, F. Charte, S. García, M. J. del Jesus, and F. Herrera. 2018. A prac-\ntical tutorial on autoencoders for nonlinear feature fusion: Taxonomy, mod-\nels, software and guidelines. Information Fusion 44 (2018), 78–96. https:\n//doi.org/10.1016/j.inffus.2017.12.007\n[9]D. A. Clifton, J. Gibbons, J. Davies, and L. Tarassenko. 2012. Machine learning\nand software engineering in health informatics. In 1st International Workshop\non Realizing Artificial Intelligence Synergies in Software Engineering (RAISE ’12) .\nIEEE Press, NJ, USA, 37–41. https://doi.org/10.1109/RAISE.2012.6227968\n[10] J. Davis, J. Hoffert, and E. Vanlandingham. 2016. A taxonomy of artificial intelli-\ngence approaches for adaptive distributed real-time embedded systems. In 2016\nIEEE International Conference on Electro Information Technology (EIT) . IEEE, NJ,\nUSA, 233–238. https://doi.org/10.1109/EIT.2016.7535246\n[11] F. G. de Oliveira Neto, R. Feldt, R. Torkar, and P. Machado. 2013. Searching\nfor models to evaluate software technology. In 1st International Workshop on\nCombining Modelling and Search-Based Software Engineering . IEEE, NJ, USA,\n12–15.\n[12] T. de Souza Alcantara, J. Denzinger, J. Ferreira, and F. Maurer. 2012. Learning\ngestures for interacting with low-fidelity prototypes. In 1st International Workshop\non Realizing Artificial Intelligence Synergies in Software Engineering (RAISE ’12) .\nIEEE Press, NJ, USA, 32–36. https://doi.org/10.1109/RAISE.2012.6227967\n[13] T. Diamantopoulos and A. Symeonidis. 2015. Towards interpretable defect-prone\ncomponent analysis using genetic fuzzy systems. In 4th International Workshop\non Realizing Artificial Intelligence Synergies in Software Engineering (RAISE ’15) .\nIEEE Press, NJ, USA, 32–38. http://dl.acm.org/citation.cfm?id=2820668.2820677\n[14] S. M. Didar Al Alam, M. R. Karim, D. Pfahl, and G. Ruhe. 2016. Comparative analy-\nsis of predictive techniques for release readiness classification. In 5th International\nWorkshop on Realizing Artificial Intelligence Synergies in Software Engineering\n(RAISE ’16) . ACM, NY, USA, 15–21. https://doi.org/10.1145/2896995.2896997\n[15] P. Domingos. 2015. The master algorithm: How the quest for the ultimate learning\nmachine will remake our world . Basic Books, NY, USA. https://books.google.se/\nbooks?id=glUtrgEACAAJ\n[16] R. Feldt. 1998. Generating multiple diverse software versions with genetic pro-\ngramming. In 24th Euromicro Conference , Vol. 1. IEEE, NJ, USA, 387–394.\n[17] R. Feldt. 1999. Genetic programming as an explorative tool in early software\ndevelopment phases. In 1st International Workshop on Soft Computing Applied to\nSoftware Engineering . IEEE, NJ, USA, 11–19.\n[18] R. Feldt. 2014. Do system test cases grow old?. In Software Testing, Verification\nand Validation (ICST), 2014 IEEE Seventh International Conference on . IEEE, NJ,\nUSA, 343–352.\n[19] R. Feldt and S. Poulding. 2013. Finding test data with specific properties via\nmetaheuristic search. In 24th International Symposium on Software Reliability\nEngineering (ISSRE) . IEEE, NJ, USA, 350–359.\n[20] J. Frohm. 2008. Levels of automation in production systems . Ph.D. Dissertation.\nChalmers University of Technology, Gothenburg, Sweden.\n[21] A. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin.\n2014. Bayesian data analysis . Vol. 2. CRC press, FL, USA.\n[22] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A.\nCourville, and Y. Bengio. 2014. Generative adversarial nets. In Advances in Neural\nInformation Processing Systems , Z. Ghahramani, M. Welling, C. Cortes, N. D.\nLawrence, and K. Q. Weinberger (Eds.). Vol. 27. Curran Associates, Inc., NY, USA,\n2672–2680. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf[23] M. Hamza and R. J. Walker. 2015. Recommending features and feature rela-\ntionships from requirements documents for software product lines. In 4th In-\nternational Workshop on Realizing Artificial Intelligence Synergies in Software\nEngineering (RAISE ’15) . IEEE Press, NJ, USA, 25–31. http://dl.acm.org/citation.\ncfm?id=2820668.2820675\n[24] M. Harman. 2012. The role of artificial intelligence in software engineering. In\n1st International Workshop on Realizing Artificial Intelligence Synergies in Software\nEngineering (RAISE ’12) . IEEE Press, NJ, USA, 1–6. https://doi.org/10.1109/RAISE.\n2012.6227961\n[25] C. Heitmeyer, M. Pickett, L. Breslow, D. Aha, J. G. Trafton, and E. Leonard. 2013.\nHigh assurance human-centric decision systems. In 2nd International Workshop\non Realizing Artificial Intelligence Synergies in Software Engineering (RAISE ’13) .\nIEEE Press, NJ, USA, 35–41. https://doi.org/10.1109/RAISE.2013.6615202\n[26] M. Iliev, B. Karasneh, M. R. V. Chaudron, and E. Essenius. 2012. Automated\nprediction of defect severity based on codifying design knowledge using on-\ntologies. In 1st International Workshop on Realizing Artificial Intelligence Syn-\nergies in Software Engineering (RAISE ’12) . IEEE Press, NJ, USA, 7–11. https:\n//doi.org/10.1109/RAISE.2012.6227962\n[27] I. Keivanloo and J. Rilling. 2012. Clone detection meets semantic web-based tran-\nsitive closure computation. In 1st International Workshop on Realizing Artificial\nIntelligence Synergies in Software Engineering (RAISE ’12) . IEEE Press, NJ, USA,\n12–16. https://doi.org/10.1109/RAISE.2012.6227963\n[28] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. 2017. The case for\nlearned index structures. ArXiv e-prints (Dec. 2017). arXiv:cs.DB/1712.01208\n[29] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. 2015. Human-\nlevel concept learning through probabilistic program induction. Sci-\nence 350, 6266 (2015), 1332–1338. https://doi.org/10.1126/science.aab3050\narXiv:http://science.sciencemag.org/content/350/6266/1332.full.pdf\n[30] B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman. 2017. Building\nmachines that learn and think like people. Behavioral and Brain Sciences 40 (2017),\ne253.\n[31] M. Landhaeusser and R. Hug. 2015. Text understanding for programming in\nnatural language: Control structures. In 4th International Workshop on Realizing\nArtificial Intelligence Synergies in Software Engineering . IEEE, NJ, USA, 7–12.\nhttps://doi.org/10.1109/RAISE.2015.9\n[32] F. Langer and E. Oswald. 2014. A self-learning approach for validation of com-\nmunication in embedded systems. In 3rd International Workshop on Realizing\nArtificial Intelligence Synergies in Software Engineering (RAISE 2014) . ACM, NY,\nUSA, 38–44. https://doi.org/10.1145/2593801.2593808\n[33] G. Marcus. 2018. Deep learning: A critical appraisal. ArXiv e-prints (Jan. 2018).\narXiv:cs.AI/1801.00631\n[34] J. Misra, S. Sengupta, and S. Podder. 2016. Topic cohesion preserving require-\nments clustering. In 5th International Workshop on Realizing Artificial Intelli-\ngence Synergies in Software Engineering (RAISE ’16) . ACM, NY, USA, 22–28.\nhttps://doi.org/10.1145/2896995.2896998\n[35] M. Monperrus. 2018. Automatic software repair: A bibliography. ACM Comput.\nSurv. 51, 1, Article 17 (Jan. 2018), 24 pages. https://doi.org/10.1145/3105906\n[36] V. Musco, A. Carette, M. Monperrus, and P. Preux. 2016. A learning algorithm for\nchange impact prediction. In 5th International Workshop on Realizing Artificial\nIntelligence Synergies in Software Engineering (RAISE ’16) . ACM, NY, USA, 8–14.\nhttps://doi.org/10.1145/2896995.2896996\n[37] P. Papadopoulos and N. Walkinshaw. 2015. Black-box test generation from\ninferred models. In 4th International Workshop on Realizing Artificial Intelligence\nSynergies in Software Engineering (RAISE ’15) . IEEE Press, NJ, USA, 19–24. http:\n//dl.acm.org/citation.cfm?id=2820668.2820674\n[38] S. Roychoudhury, V. Kulkarni, and N. Bellarykar. 2015. Mining enterprise models\nfor knowledgeable decision making. In 4th International Workshop on Realizing\nArtificial Intelligence Synergies in Software Engineering (RAISE ’15) . IEEE Press,\nNJ, USA, 1–6. http://dl.acm.org/citation.cfm?id=2820668.2820670\n[39] M. Schindler, O. Fox, and A. Rausch. 2015. Clustering source code elements by\nsemantic similarity using Wikipedia. In 4th International Workshop on Realizing\nArtificial Intelligence Synergies in Software Engineering (RAISE ’15) . IEEE Press,\nNJ, USA, 13–18. http://dl.acm.org/citation.cfm?id=2820668.2820672\n[40] J. Schmidhuber. 2015. Deep learning in neural networks: An overview. Neural\nnetworks 61 (2015), 85–117.\n[41] T. B. Sheridan. 1980. Computer control and human alienation. Technology review\n83, 1 (1980), 60–73.\n[42] W. Truszkowski, M. Hinchey, J. Rash, and C. Rouff. 2004. NASA’s swarm missions:\nThe challenge of building autonomous software. IT professional 6, 5 (2004), 47–52.\n[43] M. Unterkalmsteiner, R. Feldt, and T. Gorschek. 2014. A taxonomy for re-\nquirements engineering and software test alignment. Transactions on Soft-\nware Engineering Methodology 23, 2, Article 16 (April 2014), 38 pages. https:\n//doi.org/10.1145/2523088\n[44] M. Usman, R. Britto, J. Börstler, and E. Mendes. 2017. Taxonomies in software\nengineering: A systematic mapping study and a revised taxonomy development\nmethod. Information and Software Technology 85 (2017), 43–59. https://doi.org/\n10.1016/j.infsof.2017.01.006",
  "textLength": 48299
}