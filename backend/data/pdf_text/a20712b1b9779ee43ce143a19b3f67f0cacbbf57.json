{
  "paperId": "a20712b1b9779ee43ce143a19b3f67f0cacbbf57",
  "title": "Neural Databases",
  "pdfPath": "a20712b1b9779ee43ce143a19b3f67f0cacbbf57.pdf",
  "text": "Neural Databases\nJames Thorne\nUniversity of Cambridge\nFacebook AI\njt719@cam.ac.ukMajid Yazdani\nFacebook AI\nmyazdani@fb.comMarzieh Saeidi\nFacebook AI\nmarzieh@fb.com\nFabrizio Silvestri\nFacebook AI\nfsilvestri@fb.comSebastian Riedel\nFacebook AI\nsriedel@fb.comAlon Halevy\nFacebook AI\nayh@fb.com\nABSTRACT\nIn recent years, neural networks have shown impressive perfor-\nmance gains on long-standing AI problems, and in particular, an-\nswering queries from natural language text. These advances raise\nthe question of whether they can be extended to a point where\nwe can relax the fundamental assumption of database manage-\nment, namely, that our data is represented as fields of a pre-defined\nschema.\nThis paper presents a first step in answering that question. We\ndescribe NeuralDB , a database system with no pre-defined schema,\nin which updates and queries are given in natural language. We\ndevelop query processing techniques that build on the primitives\noffered by the state of the art Natural Language Processing methods.\nWe begin by demonstrating that at the core, recent NLP trans-\nformers, powered by pre-trained language models, can answer\nselect-project-join queries if they are given the exact set of rele-\nvant facts. However, they cannot scale to non-trivial databases and\ncannot perform aggregation queries. Based on these findings, we\ndescribe a NeuralDB architecture that runs multiple Neural SPJ\noperators in parallel, each with a set of database sentences that\ncan produce one of the answers to the query. The result of these\noperators is fed to an aggregation operator if needed. We describe\nan algorithm that learns how to create the appropriate sets of facts\nto be fed into each of the Neural SPJ operators. Importantly, this\nalgorithm can be trained by the Neural SPJ operator itself. We exper-\nimentally validate the accuracy of NeuralDB and its components,\nshowing that we can answer queries over thousands of sentences\nwith very high accuracy.\nPVLDB Reference Format:\nJames Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian\nRiedel, and Alon Halevy. Neural Databases. PVLDB, 14(1): XXX-XXX, 2020.\ndoi:XX.XX/XXX.XX\nPVLDB Availability Tag:\nThe source code of this research paper has been made publicly available at\nhttp://vldb.org/pvldb/format_vol14.html.\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 14, No. 1 ISSN 2150-8097.\ndoi:XX.XX/XXX.XX1 INTRODUCTION\nIn recent years, neural networks have shown impressive perfor-\nmance gains on long-standing AI problems, such as natural lan-\nguage understanding, speech recognition, and computer vision.\nBased on these successes, researchers have considered the applica-\ntion of neural nets to data management problems, including learn-\ning indices [ 21], query optimization and entity matching [ 25,29].\nIn applying neural nets to data management, research has so far\nassumed that the data was modeled by a database schema.\nThe success of neural networks in processing unstructured data\nsuch as natural language and images raises the question of whether\ntheir use can be extended to a point where we can relax the funda-\nmental assumption of database management, which is that the data\nwe process is represented as fields of a pre-defined schema. What\nif, instead, data and queries can be represented as short natural\nlanguage sentences, and queries can be answered from these sen-\ntences? This paper presents a first step in answering that question.\nWe describe NeuralDB, a database system in which updates and\nqueries are given in natural language. The query processor of a\nNeuralDB builds on the primitives that are offered by the state\nof the art Natural Language Processing (NLP) techniques. Figure 1\nshows example facts and queries that NeuralDB can answer.\nRealizing the vision of NeuralDB will offer several benefits that\ndatabase systems have struggled to support for decades. The first,\nand most important benefit is that a NeuralDB , by definition,\nhas no pre-defined schema. Therefore, the scope of the database\ndoes not need to be defined in advance and any data that becomes\nrelevant as the application is used can be stored and queried. The\nsecond benefit is that updates and queries can be posed in a variety\nof natural language forms, as is convenient to any user. In contrast,\na traditional database query needs to be based on the database\nschema. A third benefit comes from the fact that the NeuralDB\nis based on a pre-trained language model that already contains a\nlot of knowledge. For example, the fact that London is in the UK is\nalready encoded in the language model. Hence, a query asking who\nlives in the UK can retrieve people who are known to live in London\nwithout having to explicitly specify an additional join. Furthermore,\nusing the same paradigm, we can endow the NeuralDB with more\ndomain knowledge by extending the pre-training corpus to that\ndomain.\nBy nature, a NeuralDB is not meant to provide the same cor-\nrectness guarantees of a traditional database system, i.e., that the\nanswers returned for a query satisfy the precise binary semantics of\nthe query language. Hence, NeuralDB s should not be consideredarXiv:2010.06973v1  [cs.CL]  14 Oct 2020\n\nJames Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, and Alon Halevy\nas an alternative to traditional databases in applications where such\nguarantees are required.\nGiven its benefits, Neural Databases are well suited for emerging\napplications where the schema of the data cannot be determined in\nadvance and data can be stated in a wide range of linguistic patterns.\nA family of such applications arise in the area of storing knowledge\nfor personal assistants that currently available for home use and\nin the future will accompany Augmented Reality glasses. In these\napplications, users store data about their habits and experiences,\ntheir friends and their preferences, and designing a schema for such\nan application is impractical. Another class of applications is the\nmodeling and querying of political claims [ 46] (with the goal of\nverifying their correctness). Here too, claims can be about a huge\nvariety of topics and expressed in many ways.\nOur first contribution is to show that state of the art transformer\nmodels [ 47] can be adapted to answer simple natural language\nqueries. Specifically, the models can process facts that are relevant\nto a query independent of their specific linguistic form, and combine\nmultiple facts to yield correct answers, effectively performing a\njoin. However, we identify two major limitations of these models:\n(1) they do not perform well on aggregation queries (e.g., counting,\nmax/min), and (2) since the input size to the transformer is bounded\nand the complexity of the transformer is quadratic in the size of its\ninput, they only work on a relatively small collection of facts.\nOur second contribution is to propose an architecture for neural\ndatabases that uses the power of transformers at its core, but puts\nin place several other components in order to address the scalability\nand aggregation issues. Our architecture runs multiple instances\nof a Neural SPJ operator in parallel. The results of the operator\nare either the answer to the query or the input to an aggregation\noperator, which is done in a traditional fashion. Underlying this\narchitecture is a novel algorithm for generating the small sets of\ndatabase sentences that are fed to each Neural SPJ operator.\nFinally, we describe an experimental study that validates the\ndifferent components of NeuralDBs, namely the ability of the Neural\nSPJ to answer queries or create results for a subsequent aggregation\noperator even with minimal supervision, and our ability to produce\nsupport sets that are fed into each of the Neural SPJ operators.\nPutting all the components together, our final result shows that we\ncan accurately answer queries over thousands of sentences with\nvery high accuracy. To run the experiments we had to create an\nexperimental dataset with training data for NeuralDB s, which we\nmake available for future research.\n2 PROBLEM DEFINITION\nThe main goal of NeuralDB is to support data management appli-\ncations where users do not need to pre-define a schema. Instead,\nthey can express the facts in the database in any linguistic form they\nwant, and queries can be posed in natural language. To that end,\ndata and queries in a NeuralDB are represented as short sentences\nin natural language and the neural machinery of the NeuralDB is\napplied to these sentences.\nData: Users input data into an NeuralDB using simple natural\nlanguage sentences. Intuitively, a sentence corresponds to a single\nfact, such as Sue is Maryâ€™s mom , orGustavo likes espresso . But\nin many situations, especially when updates to the database areFacts : (4 of 50 shown)\nNicholas lives in Washington D.C. with Sheryl.\nSheryl is Nicholasâ€™s spouse.\nTeuvo was born in 1912 in Ruskala.\nIn 1978, Sherylâ€™s mother gave birth to her in Huntsville.\nQueries:\nDoes Nicholasâ€™s spouse live in Washington D.C.?\n(Boolean Join)âˆ’â†’TRUE\nWho is Sherylâ€™s husband?\n(Lookup)âˆ’â†’Nicholas\nWho is the oldest person in the database?\n(Max)âˆ’â†’Teuvo\nWho is Sherylâ€™s mother?\n(Lookup)âˆ’â†’NULL\nFigure 1: In NeuralDB, facts and queries are posed with short\nnatural language sentences. The queries above are answered\nby our first prototype described in Section 3.\nspoken by users, it is more convenient for sentences to express\nmultiple facts, such as Kiara is married to Kyrone and they have 3\nkids. We refer to the latter sentences as composite and the former\nas atomic. To focus on the novel issues of NeuralDB s, we mostly\nconsider atomic sentences in this paper, but we do demonstrate in\nSection 3 both atomic and composite sentences.\nFormally, the data in an NeuralDB is a set of sentences, each\nwith a time stamp, ğ·={(ğ‘¢1,ğ‘¡1),...,(ğ‘¢ğ‘˜,ğ‘¡ğ‘˜)}. ANeuralDB allows\nupdates, deletions and queries. The user does not need to distin-\nguish between an update and a query because NLP technology\ncan reliably make that distinction. We assume that deletions are\nexplicitly marked and also refer to individual sentences.\nQueries: Formally, a query ğ‘„over a database, ğ·, produces a set\nof answers: ğ‘„(ğ·)={ğ‘1,...,ğ‘ğ‘™}. While queries are formulated in\nnatural language, we only consider queries that, if translated to\nSQL, would involve a select-project-join (SPJ) component followed\nby an aggregation. However, for our analysis we need to make some\nfurther distinctions between several classes of queries (outlined in\nFigure 1).\nAlookup query is a query where each answer comes from a\nsingle fact in the database (e.g., Who is Susanâ€™s husband? ), whether\nthere is a single answer or several. If the query returns True/False,\nwe refer to it as a Boolean query. Note that in our context, lookup\nqueries are non-trivial because facts in the database are expressed\nin a variety of linguistic forms.\nAjoin query is one that requires combining two (or more) facts\nin the database in order to produce each answer. For example, the\nquery Who works in a company in France? could combine facts\npertaining to a personâ€™s country of residence, and others pertaining\nto peopleâ€™s jobs. In some cases, the query may require a join even if\none is not explicitly specified in the query. For example, the query\n\nNeural Databases\nWho is Johnâ€™s uncle? could involve combining a fact about Johnâ€™s\nparents and about their brothers.\nWe also consider queries that require performing an aggregation\n(e.g., how many kids does Pat have? ). In this paper, we focus on the\ncount ,minandmaxaggregation operators.\nWe note that while there is an intuitive mapping between natural\nlanguage queries and SQL constructs, the mapping is not always\nprecise in the context of NeuralDB s. For example, even though a\nquery is a mere lookup, it may involve an implicit join. Consider\nthe query does Susan live in the UK? , and a database that contains\nSusan lives in London . In this case, the query answering engine of\ntheNeuralDB will benefit from a rich underlying language model\nin order to deduce that London is in the UK, and therefore answer\nthat Susan lives in the UK. Finally, the selections in our queries\nare equality predicates on strings. Comparison predicates will be\nhandled in future work.\nUnique names assumption: In order to focus on the new issues\nthat are raised by NeuralDB s, in this paper we assume that a given\nname refers to exactly one entity in the domain and each entity\nhas a single name. We also assume that pronouns (e.g., she, they)\nare not used or have been resolved in advance. The application of\nthe rich body of work on entity resolution to NeuralDB s will be\nreserved for future work.\n2.1 NLP background\nThe field of natural language processing has made impressive\nprogress in recent years by building on transformer architectures\nand pre-trained language models. Such models have led to major\nimprovements on tasks such as question answering from text, text\nsummarization, and machine translation. In NeuralDB , our goal\nis to represent the facts in a database as a set of simple natural\nlanguage sentences. Hence, as a first step to realizing NeuralDB s,\nwe consider whether techniques for question answering on text\ncan be adapted to our context.\nIn this section we describe the relevant aspects of pre-trained\nlanguage models and transformers that operate over them, and the\nchallenges in adapting these techniques to our context.\nPre-trained language models and transformers. Pre-trained language\nmodels such as BERT [ 10], GPT-3 [ 8], RoBERTa [ 26], T5 [ 35] are\nneural models trained on large corpora of text. The models are\ntrained by randomly removing certain tokens from the corpus and\ntraining the neural net to predict the missing token, given its context\n(i.e., the words preceding and following the missing token). At an\nintuitive level, these models obtain two kinds of knowledge (1)\nthe ability to make predictions independent of the exact linguistic\nform in which knowledge is stated [ 31,45], and (2) some world\nknowledge that is mentioned frequently in text (e.g., London is the\ncapital of the UK) [ 8,33].1Pre-trained language models are usually\nfine-tuned to a given task. For example, for question answering from\ntext, the system would be further trained with a pair of (question,\nanswer) pairs in order to produce the final model. Importantly, since\nthe pre-trained language models capture world knowledge [ 33] and\n1Note that the second type of knowledge can also lead to implicit biases. We address\nthis point in Section 8act as an implicit regularizer [ 16,34], fewer examples are needed\nfor the fine-tuning compared to training a model from scratch.\nThe Transformer model [ 47] is the most common neural archi-\ntecture to operate on pre-trained language models based on the\nhigh accuracy it produces on downstream tasks including ques-\ntion answering on text. In our prototype experiments, detailed in\nSection 3, we demonstrate that these reasoning abilities enable the\ntransformer architecture to generate correct answers to a number\nof queries that we might pose to a NeuralDB .\nTraining models. We will train the parameters of our neural network\nof the NeuralDB with examples that includes (query, answer) pairs.\nIn Section 3.1 we describe how we obtain training data by leveraging\nfacts from Wikidata. The training data sets we use contain in the\norder of 10,000-100,000 examples. In a sense, one can view the need\nfor training data as the cost we pay to support a database with no\npre-defined schema. Indeed, we will show that without training\ndata, the performance of NeuralDB degrades quite a bit. However,\ntraining is important not only for understanding new relations, but\nalso to be able to handle the different linguistic variations in which\nfacts are expressed. In contrast, a schema will always be brittle and\nallow only one way of expressing a given fact. Furthermore, training\ndata is more easily transferred between different applications.\nAlong these lines, there is a question of whether the training data\nneeds to cover all the relationships that are seen in the queries. For\nexample, can a NeuralDB answer a query what are Ruthâ€™s hobbies?\nif it has not been trained on examples that include mentions of\nhobbies. In Section 6 we show that while accuracy of answers drops\nfor such out of domain queries, the number of additional training\nexamples that need to be added to achieve reasonable performance\nis relatively small.\nIn more detail, training neural networks is an optimization prob-\nlem: minimizing the expected loss over the instances in the training\ndata through changes to the model parameters, weighted by their\ncontribution to the error term over all instances in the training\ndataset. Formally, the loss is the cross-entropy between the model\nprediction and the reference: ğ¿=âˆ’Ã\nğ‘âˆˆğ¶ğ‘(ğ‘¦ğ‘)logË†ğ‘(ğ‘¦ğ‘)which\nis non-negative and real-valued. For both sentence classification\n(assigning a single discrete label ğ‘âˆˆğ¶for a sequence of tokens)\nand language generation tasks (decoding a sequence of tokens\n(ğ‘1,...,ğ‘ğ‘›)from the vocabulary ğ¶as output from the model) the\nmodel output is a probability distribution over labels Ë†ğ‘(ğ‘¦ğ‘)âˆˆğ¶.\nEvaluating accuracy of answers. We measure the correctness of the\nanswers generated by a NeuralDB by comparing them against ref-\nerence data that contain the correct answers. The neural networks\nare trained with subset of the available data, leaving a portion of it\nheld-out for evaluation, referred to as the test set.\nFor most queries, we measure correctness using Exact Match\n(EM), which is 1 if a binary the answer string generated by the\nNeuralDB is exactly equal to the reference answer and 0 otherwise.\nThis metric is used to score outputs where either a Boolean, null\nanswer, string or numeric answer is expected.\nWhen a set of results is returned, we also consider the ğ¹1score\nthat weighs the precision and recall of the answer generated by the\nNeuralDB as compared to the reference data. Precision and recall\npenalize false positives (denoted fp) and false negatives (denoted\n\nJames Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, and Alon Halevy\nfn) compared to true positives (denoted tp) respectively: ğ‘=ğ‘¡ğ‘\nğ‘¡ğ‘+ğ‘“ğ‘,\nğ‘Ÿ=ğ‘¡ğ‘\nğ‘¡ğ‘+ğ‘“ğ‘›. Theğ¹1score, ranging from 1.0 for perfect match to 0.0\nfor no match is the harmonic mean of precision and recall: ğ¹1=2ğ‘ğ‘Ÿ\nğ‘+ğ‘Ÿ\nwhich has an affinity for the lower of two scores. When comparing\nmodels and reporting results, we average the instance-level scores\n(either EM and ğ¹1) over all examples in the test set.\nThe transformer architecture. Transformers [ 47] take as input an\nsequence of symbols x=(ğ‘¥1,...,ğ‘¥ğ‘›). They are typically trained\nin one of two configurations: encoder only or encoder-decoder. In\nthe former, each token is encoded to a vector representation that is\nused to predict a label. In the latter, used in sequence-to-sequence\napplications (e.g., question answering or machine translation), the\ndecoder produces the output sequence.\nIn both configurations, the transformer works in two phases. In\nthe first phase, the transformer encodes the input into an interme-\ndiate representation z=(ğ‘§1,...,ğ‘§ğ‘›)where the dimension of the\nvector is fixed, typically where ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ =768. In the second phase,\nthe transformer decodes zto produce the output. For example, in\nsequence-to-sequence generation the output would be a sequence\nof tokens y=(ğ‘¦1,...,ğ‘¦ğ‘™), ending with a special token.\nThe model contains two stacks of repeating layers. The stacks\ndiffer slightly in composition: one stack is for encoding an input\nsequence, x, tozand the other stack is for incrementally decoding\nthe output yfrom z, using the partially generated yas context when\ndecoding. In each stack, the repeating layer contains a multi-head\nattention mechanism, that weighs the importance of tokens given\nthe context it appears in, as well as fully connected network that\nindependently performs a transformation on the attended tokens.\nA transformer model is composed of a fixed number of between\nğ‘=6andğ‘=12layers.\nIt is important to note the complexity of transformers. During\nencoding, self-attention considers the similarity between all tokens\nin the input sequence, which has quadratic complexity with respect\nto input sequence length. Similarly, during decoding, at step ğ‘¡, the\nattention mechanism scores the generated tokens y[1:ğ‘¡âˆ’1]against\nall of the encoded context z, which is also quadratic. This complexity\nis clearly of concern when inputs are large.\nScaling NLP to DB scale. The NLP problem of question answering\nwith external knowledge such as Wikipedia, (a.k.a. open-book QA )\nforms a good starting point to explore the application of trans-\nformers to NeuralDB . In the context of NeuralDB s, we use the\ntransformer in an encoder-decoder configuration, and the input\ncontains the query to the NeuralDB and all the relevant facts in\nthe database separated by a special delimiter symbol. The output is\na sequence of tokens that answers the query.\nTo scale neural reasoning to databases of non-trivial size, it\nwould not be feasible to encode the entire database as input to the\ntransformer for a given query, because transformers cannot accept\nsuch large inputs, and even if they could, the latency would be\nprohibitive. It is common to use a maximum input size of 512or\n1024 tokens. The typical approach in open-book QA is to comple-\nment the transformer reasoning with an information retriever that\nextracts a small subset of the facts from the corpus. The informa-\ntion retrieval component can either be a simple one (e.g., BM25)\nor trained jointly with the transformer to learn to extract relevantparts of the corpus [ 14,20,22,32,46,57]. However, the following\nadditional challenges arise in the context of NeuralDB s:\nâ€¢Unlike open-book QA, which typically requires extracting\na span from a single document or predicting a token as an\nanswer, answering queries in a NeuralDB may require pro-\ncessing a large number of facts and in some cases performing\naggregations over large sets.\nâ€¢NeuralDB s do not enjoy the locality properties that usu-\nally hold in open-book QA. In NeuralDB s, a query may be\ndependent on multiple facts that can be anywhere in the\ndatabase. In fact, by definition, the current facts in a data-\nbase can be reordered and the query answers should not\nchange. In contrast, in open-book QA, the fact needed to\nanswer a given question is typically located in a paragraph\nor document with multiple sentences about the same subject\nwhere this additional context may help information recall.\nâ€¢When determining which facts to input to the transformer,\nNeuralDB s may require conditional retrieval from the data-\nbase. For example, to answer the query Whose spouse is\na doctor? weâ€™d first need to fetch spouses and then their\nprofessions. In the NLP community this is known as multi-\nhop query answering [ 6], which has recently become an\nactive area of research, but restricted to the case where weâ€™re\nlooking for a single answer. In NeuralDB s, we may need to\nperform multi-hops for sets of facts.\n3 NEURAL QUERY PROCESSING\nIn this section we describe an initial experiment whose goal is to\nbetter understand and quantify the applicability of transformers\nto query processing in NeuralDB s. In particular, the goal of the\nexperiment is to answer the following question. Given a query\nand a small number of facts from the database, can the transformer\naccurately answer queries that are posed in natural language, whose\nanswer may require projection (i.e., extracting part of a sentence),\njoin and aggregation. Note that for the purpose of this experiment,\nwe are momentarily putting aside the issue that transformers can\nonly take a small number of facts as input. Weâ€™ll address that issue\nwith our full architecture in Section 4.\n3.1 Data\nSince NeuralDB s are a new kind of database, there are no existing\ndata sets that are directly applicable to evaluating them. Hence we\nnow describe a data set we developed to test NeuralDB s, and to\nshare with the community. While this dataset does not include data\nin the wild , it has enough variety that it provides a good signal\nabout the validity of our techniques.\nTraining a NeuralDB requires supervision in the form of (ğ·,ğ‘„,ğ´)\ntriples, where ğ·is a set of facts, ğ‘„is a query and ğ´is the correct\nanswer toğ‘„overğ·. We generate training data in a controlled\nfashion using data from Wikidata [ 48] to express facts in natural\nlanguage. Because of the scale of Wikidata, it is possible to gen-\nerate large numbers of training instances about a wide range of\nrelationships requiring very few templates. The data set we create\nenables us to drill down our analysis by query type and relation\ntype to understand the performance limitations of NeuralDB s.\n\nNeural Databases\nJohn works at Shell\nSarah is a doctor\nMarcello lives in USASarah married JohnFacts\nNeural Query\nProcessor\n(Transformer)\nSarah is from FranceResult set\nSarah\nQuery: Who is\nFrench?\nFigure 2: Prototype neural query processor using a T5 Trans-\nformer. The facts and the query are concatenated and given\nas input to the transformer. Our results show that transform-\ners can answer lookup and join queries when given the sup-\nport set of facts needed to generate an answer, but that the\narchitecture does not scale to many facts.\nD1: Query and Answer dataset. Wikidata stores triples of the form\n(S,R,O), where ğ‘…is a relationship between the subject ğ‘†and the\nobjectğ‘‚, e.g., (Bezos, employedBy, Amazon) . For every relation-\nship we consider, we construct multiple natural language templates,\nthereby providing different linguistic forms of stating the same fact.\nEach template has a placeholder for the subject and the object, e.g.,\n$O is the place of birth of $S . We then generate different data in-\nstances by substituting entities from Wikidata for the placeholders\nin the templates.\nWe create databases with 7different relationships and for each\nrelationship we create between 5and 14templates which vary\npronouns, temporal expressions and phrasing of the fact and query.\nWe generate a training, validation and held-out test set containing\n535,50and50databases respectively, each containing 50facts. Each\ndatabase has between 100-200question and answer pairs yielding\n60000 training, 5500 validation and 6000 test instances in total.\nThe facts that are generated from Wikipedia are consistent with\nreal-world facts. Hence, there is a risk that the NeuralDB is get-\nting its answers from the pre-trained language model (trained on\nWikipedia) and not by processing facts in the database itself. We\nmitigate this issue in two ways. First, we create some facts about\nfictional characters with simple traits (e.g. likes coffee orgood at\nmusic ). For these facts, we use relationships and entities that are not\nin Wikidata (the entities are referenced by first name only). Second,\nwe attach a timestamp with each query, and only facts prior to\nthis timestamp are used for inference. By purposefully setting the\ntimestamp lower than the timestamp of the relevant facts, we verify\nthat the model is returning the NULL answer when itâ€™s supposed to\nand not relying on facts in the pre-trained language model.\nThe dataset contains both atomic and composite facts: composite\nfacts combine more than one relation over the same subject, e.g.,\nJohn [subject] is employed as a driver [object(employedAs)]\nin London [object(employedWhere)] . To generate queries that re-\nquire joins we apply the same technique to combine two connectedrelations for the query rather than the fact. For example, we use\nâ€œfatherOfâ€ and â€œemployedAsâ€ to create the template for the query\nDoes $Sâ€™s father work as a $O . To generate queries that require\nimplicit language reasoning and test the knowledge captured by\nthe language modelling task, we replace place names with a more\ngeneral location. For example, for the fact: Maheshâ€™s mum gave\nbirth to him in Mumbai , the questions generated would ask: Was\nMahesh born in Europe? (with the answer No) and Was Mahesh\nborn in India? (with the answer Yes).\n3.2 Results\nIn our experiment, we use a T5 transformer model [ 35], a trans-\nformer variant that is designed for conditional language generation,\nas a neural query processor. To provide input to the transformer, we\njointly encode relevant facts from the database by concatenating\nthem with the query (separated by a special delimiter token), as\nillustrated in Figure 2. In what follows, we consider several variants\non which input facts are encoded by the transformer which both\nprovides an upper bound on performance (if all necessary facts\nwere encoded), as well as evaluates the transformerâ€™s resilience\nto noisy (low precision) or incomplete (low recall) information\nretrieval when performing neural query processing.\n(Avg) Atomic\nBoolQAAtomic\nExtQAJoin\nBoolQAJoin\nExtQASet Count Min/Max020406080100Exact Match %T5 Transformer answer accuracy by query type\nPerfect IR\nWhole DB\nTF-IDF IR (k=50)\nTF-IDF IR (k=5)\nDPR (k=5)\nFigure 3: Exact match accuracy for different classes of\nqueries. The results show that the transformer obtains\nhigh accuracy for lookup and join queries, but falls short\nfor queries with aggregation or yielding set answers. Fur-\nthermore, adding information retrieval to scale to larger\ndatabases harms EM for queries outputting a set of results\nor requiring aggregations.\nVarying the inputs to the transformer. We first investigate how re-\nsilient the transformer is to the number and relevance of its input\nfacts. Figure 3 shows the exact match scores for several ways of\nretrieving/filtering facts before they enter the transformer, with\nrespect to different query types. Examples of correctly answered\nqueries for the Perfect IR model are shown in Figure 1.\nOur first observation is that for queries which required either\nextracting information from the facts, or performing Boolean infer-\nence, the model attained near perfect scores, regardless of whether\n\nJames Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, and Alon Halevy\nthe queries need to be answered from a single fact or by joining\nmultiple facts. The fact that the model had high scores for queries\nthat require the combination of multiple facts indicates that the\ntransformer model is able to combine information from multiple\nsources when generating an answer to the user query. The different\nvariants of our experiment provide further insights.\nPerfect IR: This approach assumes that the information retrieval\ncomponent of model can perfectly retrieve the set of facts needed\nto answer a query. This version assesses the modelâ€™s capability\nof performing the right computations when only the appropriate\nfacts are given as input to the model. To implement it we select\nthe appropriate facts using meta-data from construction of the\ncontrolled reference dataset.\nOur results suggest that given the right facts, the model can be\nrobust to multiple linguistic variations and generates the correct\noutput. However, the model performs poorly for query types where\nan aggregation step is necessary or when the query result is a large\nset. This observation corroborates work by Hupkes et al. [17] that\nshowed that neural models canâ€™t generate long sequences well for\ncertain sequence transduction tasks.\nWhole DB: in this the approach we encode the whole database\nas input to the model. This would only work for small databases\n(it is prohibitive even for databases with 50 facts) because the self-\nattention mechanism in the transformer model has quadratic com-\nplexity2with respect to input size. This method allows us to evalu-\nate the modelâ€™s sensitivity to noise and whether having too much\ninformation (i.e. low precision with high recall) has an impact.\nOne positive takeaway is that the model is able to generate the\ncorrect answers despite the fact that it was exposed to irrelevant\nfacts. Interestingly, this model also has a high ğ¹1score for queries\nthat generated sets as results (as opposed to a single fact) as well as\nfor min/max queries. For count queries its performance is low.\nTFÂ·IDF IR: In this version we evaluate a simple baseline for the\ninformation retrieval component using TF Â·IDF which considers\noverlapping tokens between the query and fact. This evaluation\nwould highlight whether the NeuralDB model adequately handles\nnoise from the IR component as well as the difficulty of retriev-\ning the necessary facts. We may fail to retrieve the relevant facts\neither due to the need for multi-hop reasoning or because facts\nare expressed with different linguistic expressions. Our experiment\nconsiders using both the top-5 and top-503returned results from\nthe TFÂ·IDF component, and evaluating whether a small number of\ntop results is sufficient for some query types.\nUsing TFÂ·IDF for IR, the model still attains near-perfect accu-\nracy for atomic queries. For queries where a join is required, the\naccuracy drops due to low recall. While the EM from Boolean QA\nis reduced from 99.2% (using the whole DB) to 91.6%, for queries\nthat require extracting information the accuracy drop is more stark:\nfrom near perfect to 62.0%. From a modeling perspective, Boolean\nquery answering is quite a simple simple task with low entropy\nthat the model may be able to guess using other cues in the query\n2While optimizations and linearizations have recently been introduced [ 42,49] a single\nencoder would present a bottleneck. Furthermore aggregating large number of facts\n(such as with count ) would still suffer from low accuracy.\n3The DB size is 50 facts. This evaluates whether the limitation in TF Â·IDF is the require-\nment for token overlap or whether the limitation is in ranking\n(Avg) Atomic\nBoolQAAtomic\nExtQAJoin\nBoolQAJoin\nExtQASet Count Min/Max020406080100Exact Match %T5 Transformer answer accuracy by query type\nPerfect IR: Fusion in Encoder \n(Concatenation)\nPerfect IR: Fusion in DecoderFigure 4: Fusing in the decoder, as opposed to concatenating\nthe inputs (hence fusing in the encoder), reduces the trans-\nformerâ€™s computational complexity, but accuracy drops sig-\nnificantly. This experiment suggests that NeuralDBs should\nonly process small numbers of facts in every transformer\nwhile fusing facts in the encoder.\nor facts (i.e., it may be correct for the wrong reasons). The accuracy\nfor queries that output sets, or require aggregation is much lower.\nFor queries with min/max aggregation, the EM for TF Â·IDF was near\nzero, this is due to there being no token overlap between the query\n(such as Who is the oldest? ) and the facts (such as John was born\nin 1962 ), yielding no results from the TF Â·IDF search engine.\nDPR: To alleviate some of the limitations of TF Â·IDF, we experi-\nmented with dense passage retriever (DPR) [ 20], which scores the\nsimilarity of vector encodings of the query and the facts through\ncomputing the inner product assigned a non-zero score for all fact\n(meaning that all relevant facts could potentially be retrieved - even\nif there are no overlapping tokens). Our experiments, however,\nhighlight a difficult precision-recall trade off. To retrieve facts for\njoins, many false positive facts must also be input into the model\nas we find that facts participating in joins are often ranked outside\nof the top 5items returned from the information retrieval.\nIndependent encoding of facts with Fusion in Decoder. The following\nexperiment provides an additional insight that is useful for devel-\noping the architecture for NeuralDB s. In the experiments so far,\nwe concatenated all the facts before we fed it to the encoder of\nthe transformer. All facts were encoded jointly with self-attention\nwhich both considers intra- and inter-fact relations. Here we adapt\nthe approach known as Fusion in Decoder (FiD) [ 18] to our context.\nSpecifically, each fact is fed separately to the encoder, but the fu-\nsion of the facts happens only in the decoder. Rather than one large\nself-attention operation over all facts, self-attention is computed\nindependently for each fact, considering the relation between the\nfact and the query, but there is no attention between facts. This\nreduces the complexity of joint encoding of multiple concatenated\nfacts from a single quadratic complexity operation (w.r.t. total input\nlength) to a linear (w.r.t. number of facts) allowing this method to\nscale to larger numbers of facts.\n\nNeural Databases\nWe compare FiD with the aforementioned perfect-IR version,\nwhere the only input to the models are the facts necessary to an-\nswer the query. The results in Figure 4 show that while the two\nmodels perform comparably for lookup queries, the FiD approach\nfails for queries that require join or aggregation. The implication\nof this experiment is that the self-attention in encoding is impor-\ntant to capture the inter-sentence dependencies between the facts.\nTherefore, trying to feed growing numbers of facts to a transformer\nis unlikely to scale.\nSummary. We believe that the initial experiment suggests the fol-\nlowing: (1) if there were a way to feed the transformer the relevant\nfacts from the database, it can produce results with reasonable ac-\ncuracy, (2) aggregation queries need to be performed outside of the\nneural machinery, and (3) in order to handle queries that result in\nsets of answers and in order to prepare sets for subsequent aggre-\ngation operators, we need to develop a neural operator that can\nprocess individual (or small sets of) facts in isolation and whose\nresults outputted as the answer or fed into a traditional (i.e. non-\nneural) aggregation operator. The next section describes our first\nsteps towards such an architecture.\n4 THE ARCHITECTURE OF A NEURAL\nDATABASE\nWe now describe the architecture of a NeuralDB that addresses\nthe challenges exposed in Section 3. The architecture is shown in\nFigure 5 and has the following key ideas:\nRunning multiple transformers in parallel: In Section 3, we\ndemonstrated that if we provide our transformer model with the\nright facts needed to derive the query, and even in the presence of\nirrelevant facts, the transformer can produce correct answers for\nSPJ queries. The problem is that we can only feed a small number\nof facts to the transformer.\nIn our architecture we address this challenge by running mul-\ntiple copies of a Neural SPJ operators in parallel. Each copy is a\ntransformer similar to the one we used in Section 3. When queries\ndonâ€™t involve aggregation, the union of the outputs of the Neural\nSPJ operators are the answer to the query. When the query does\ninvolve aggregation, these machine-readable outputs are fed into\nthe aggregation operator.\nAggregation with a traditional operator: Since the Neural SPJ\noperators were designed to output structured results, our archi-\ntecture can use a separate traditional aggregation operator. Using\na separate aggregation overcomes the limitation on transformers\ndemonstrated in Section 3, and enables us to extend the system\nto new aggregation operators without retraining the basic models\nused in the system. The aggregation operator is selected through a\nclassifier we train separately that maps the query to one of 6 labels:\n{no_aggregation, count, min, max, argmin, argmax}.\nSupport set generation: Intuitively, the answer to every SPJ query\nhas a derivation tree, where the leaves are the facts from the data-\nbase. The Neural SPJ operator needs to be given a preferably small\nsuperset4of these leaves. More formally, each copy of the Neural\n4In a perfect model, the SPJ operator should be given exactly the leaves of the derivation\ntree, but this is challenging. We optimize the SSG generator for recall and the SPJ for\nprecision over noisier support setsSPJ operator is given a support set : a restricted subset Ë†ğ·âˆˆP(ğ·)\nof the facts in the database that contains the minimal support to\ngenerate an answer to the query.\nTraining the Neural SPJ operator. The Neural SPJ module is a trans-\nformer model. Unlike the prototype model described in Section 3,\nwhich was trained to generate the final answers to the query, the\nNeural SPJ is trained to generate an intermediate result of the query.\nFigure 6 shows examples of the output of the Neural SPJ operator.\n5 SUPPORT SET GENERATION\nThe Support set generator (SSG) is a module that given a query ğ‘„\nand a database ğ·, produces a set of support sets ğ‘†ğ‘†ğºğ‘„(ğ·), each\nof which is used to generate an answer with the SPJ module in\nparallel. Note that sets in ğ‘†ğ‘†ğºğ‘„(ğ·)may not be pairwise disjoint\nbecause some facts may be required for multiple answers (consider,\nfor example, a one-to-many relation).\nThe outputs generated by SSG depend on the information need\nof the downstream SPJ, and whether the query requires joins or\naggregation: (1) for queries that are answered by a single sentence,\ne.g.,Who is Sherylâ€™s husband? , the support set containing a single\nfact should be generated, e.g., Sheryl is Nicholasâ€™s spouse . (2) When\nthe SPJ operator requires multiple facts to be joined, the support\nset would contain all participating facts. (3) For queries that require\naggregation or whose answer is a set, multiple support sets must\nbe generated, each containing enough information to generate the\nintermediate results that are aggregated. For example, for the query\nWho is the oldest person? , each of the support sets would contain\na single fact that includes a person and their birth date. If joins are\nrequired to generate the intermediate result, the support sets would\ncontain multiple facts.\nUsing information retrieval, such as TF Â·IDF in Section 3, could\nbe considered a primitive SSG generating a single support set con-\ntaining all relevant facts. As our experiments indicated, this method\nworked well for queries whose answer is generated from a single\nfact, but not for joins, aggregation queries or for queries outputting\na set of answers. In what follows we describe a more robust algo-\nrithm for generating support sets.\nIncremental support set generation. The output of the SSG is the\nset of all relevant support sets: ğ‘†ğ‘†ğºğ‘„(ğ·) âŠ‚ P(ğ·). It would be\nintractable to consider all possible support sets, as this would be\nakin to enumerating the powerset. We instead efficiently and in-\ncrementally construct support sets, starting from the empty set\nby modeling the task as multi-label structured prediction. At each\nstep, a classifier, ğ¶, considers the partially generated support set\nË†ğ·ğ‘˜and the query and predicts which candidate facts ğ‘¢ğ‘–âˆˆğ·from\nthe database should be added or whether to stop the iteration.\nIncremental SSG is described in Algorithm 1 and illustrated in\nFigure 7. The action classifier predicts which facts ğ‘¢ğ‘–âˆˆğ·should\nbe explored or whether to stop. If STOP is predicted, Ë†ğ·ğ‘˜is closed\n(i.e., it forms part of the output); otherwise, for each fact added,\na new intermediate (i.e., open) support set is generated which is\nexplored in the next iteration. For efficiency, to build the multi-\nlabel SSG action classifier, we use a bi-encoder architecture that\nindependently encodes the facts in the database and the state (query\nand a partial support set) and computes the inner product between\n\nJames Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, and Alon Halevy\nJohn works at Shell\nSarah is a doctor\nSarah married JohnJohn works at Shell\nSarah is a doctor\nSarah married JohnSarah married JohnFacts Support sets\nNULL\nJohnQuery-based\nderivation\nNeural SPJ\nSupport Set\nGenerator\nQuery : \nHow many peoples'\nspouses are doctors?Neural SPJResult set\nAggregation 1\nFigure 5: Overview of NeuralDB architecture. The support set generator creates small sets of facts that are each fed into a\nseparate Neural SPJ operator that runs a single transformer. The results of the individual Neural SPJ operators are either\nunioned to produce the result or passed on to a traditional aggregation operator.\n(1) Does Nicholasâ€™s spouse live in Washington D.C.?\n{Nicholas lives in Washington D.C. with Sheryl.,\nSheryl is Nicholasâ€™s spouse.} âˆ’â†’TRUE\n(2) Who is the oldest person in the database?\n{Teuvo was born in 1912.} âˆ’â†’(Teuvo, 1912)\n(3) Does Nicholasâ€™s spouse live in Washington D.C.?\n{Teuvo was born in 1912.} âˆ’â†’NULL\nFigure 6: Examples of the intermediate results that are pro-\nduced by the Neural SPJ operator.\n{}Support Set\nSSGÂ \nClassiï¬erSarah is a doctor\nSSGÂ \nClassiï¬erSarah married John Sarah is a doctor\nSTOPSSGÂ \nClassiï¬er\nSarah married JohnSarah is a doctorSelected Facts\nStep 0\nStep 1\nStep 2\nQuery :Â \nHow many peoples'\nspouses are doctors?\nFigure 7: ISSG incrementally creates support sets. At each\nstep, the classifier either decides to add another fact to the\nsupport set or to stop and output a completed support set.\nthe encoded representations to generate a score. The encoders are\npre-trained transformers fine-tuned to yield a high inner product\nbetween the stateâ€™s encodings and relevant facts to be added to\nanswer the query. The vectors encoding of the facts are static andAlgorithm 1: Support Set Generator (SSG) modeled as\nmulti-label classification: using maximum inner product\nsearch (MIPS) over vector encodings of facts ğ‘ˆand stateğ‘‰\nInput: Bi-encoders ğ¶:ğ¶ğ‘ˆ(for actions), ğ¶ğ‘‰(for state),\nDatabaseğ·, Queryğ‘„, Threshold ğœ\nOutput: Set of support sets ( Ë†ğ·1,..., Ë†ğ·ğ‘)âŠ‚P(ğ·)\nopen := {{}};\nclosed := {};\nU :=[ğ¶ğ‘ˆ(ğ‘¢1);...;ğ¶ğ‘ˆ(ğ‘¢ğ‘›);ğ¶ğ‘ˆ(STOP)]forğ‘¢ğ‘–âˆˆğ·;\nwhile openâ‰ {}do\nnext := {};\nfor Ë†ğ·ğ‘˜inopen do\nV :=[ğ¶ğ‘‰(ğ‘„,ğ‘¢ 1...ğ‘¢ğ‘š)], forğ‘¢ğ‘–âˆˆË†ğ·ğ‘˜;\nA := MIPS(ğ‘ˆ,ğ‘‰,ğœ);\nforğ‘ğ‘—inğ´do\nifğ‘ğ‘—==STOP then\nğ‘ğ‘™ğ‘œğ‘ ğ‘’ğ‘‘ :=ğ‘ğ‘™ğ‘œğ‘ ğ‘’ğ‘‘âˆª{Ë†ğ·ğ‘˜};\nelse\nğ‘›ğ‘’ğ‘¥ğ‘¡ :=ğ‘›ğ‘’ğ‘¥ğ‘¡âˆª{{ğ‘ğ‘—âˆªË†ğ·ğ‘˜}};\nopen := next;\nreturn closed;\nare pre-computed offline. At each step, ğ‘¡, we encode the state using\na transformer by concatenating the query and all facts already\nincluded in ğ·ğ‘˜âˆˆğ‘œğ‘ğ‘’ğ‘› .\nComplexity: The inner loop of Algorithm 1 involves a Maximum\nInner Product Search (MIPS) between the encoded state and the\nencodings of the facts, which linear in the number of facts. However,\nwe use FAISS [ 19] to accelerate retrieval to ğ‘‚(log2ğ‘›). If we assume\na query needs a maximum of ğ‘support sets, and the average size\nof a support set is ğ‘š, then the complexity of the SSG algorithm is\nğ‘‚(ğ‘ğ‘šlog2ğ‘›). Bothğ‘andğ‘šare bounded by the number of facts\nin the database ğ‘›, but in practice weâ€™d expect only one of ğ‘orğ‘š\nfactors to be large. However, there is fertile ground for developing\n\nNeural Databases\nAlgorithm 2: Generating training signal for distant super-\nvision of support sets by querying a NDB with perturbed\nfacts which the triggers the generated answer to change\nInput:ğ‘„the NeuralDB for question, ğ·all facts\nInput:ğ‘šğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿğ‘  optional control for early stopping\nResult: Support set Ë†ğ·âŠ†ğ·such thatğ‘„(ğ·)=ğ‘„(Ë†ğ·)\nGenerator: Predict( D, reference, history )is\nifğ·â€“ history = {} then\nreturn {};\nsearch = random.shuffle( ğ·â€“ history);\nforfactinsearch do\npredicted = Q(Dâ€“{history âˆª{fact}} );\nifpredicted â‰ reference then\nyield fact\nyield from Predict(D, reference, history)\nhistory := historyâˆª{fact};\ni := 0; Ë†ğ·:= {}; ref :=ğ‘„(ğ·);\nwhile fact := Predict(D, ref, {}) âˆ§ğ‘–<ğ‘šğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿğ‘  do\nË†ğ·:=Ë†ğ·âˆª{ğ‘“ğ‘ğ‘ğ‘¡};\ni += 1;\nreturn Ë†ğ·;\nmethods for indexing (and/or clustering) the facts in the database\nso that only few facts need to be considered in each iteration of the\ninner loop of the algorithm, leading to significant speedups.\nTraining the action classifier. To train the action classifier, we need\na supervision signal as to which facts the classifier must select at a\ngiven the current state for a query. We generate such training data\nfrom the data set ğ·1, where we have a set of queries, answers, and\ntheir corresponding support sets.\nTo alleviate the need for training data we describe a novel dis-\ntant supervision approach for training the action classifier (see\nAlgorithm 2). Instead of having perfect training data, we generate\npossibly noisy training data by taking advantage of the neural SPJ\noperator itself. Specifically, with a known (query, answer) pair and\na pre-trained model, we can incrementally remove facts from the\ndatabase until the answer predicted by the model changes from the\ncorrect answer to one which is incorrect. We know that for small\ndatabases, it is possible to encode the entire database as input to\nthe SPJ operator, so we use the whole DB model we pre-trained\nfrom Section 3 for this purpose. The combination of facts removed\nfrom the DB that change the answer from the correct answer to\nan incorrect answer would be labeled as forming part of a support\nset. For example, removing the either the fact Sarah is a doctor\norSarah married John from the input to the model may change\nthe output prediction for the How many peopleâ€™s spouses are\ndoctors? query from 1to0, and would be added to the support set.\nThe training data may be noisy because the neural SPJ operator is\nnot perfect: robustness to stochasticity could be introduced through\nreinforcement learning techniques such as Q-learning [ 50], but in-\nvestigating that option is reserved for future work. Experimental\nresults in Section 6.3 indicate that the models can be robust to this\nnoise without explicitly mitigating the noise. Our method is a formTable 1: Exact match scores showing our proposed NeuralDB\narchitecture of SSG, SPJ and Aggregation accurately answers\naggregation and join queries whereas IR(k=5) with a single\nT5 transformer does not.\nMethodExact Match (%)\nCount Min/Max Sets Atomic Joins\nNeuralDB 79.31 100.00 94.56 98.72 96.95\nNeuralDB (DS) 79.45 100.00 91.91 97.90 79.29\nTFÂ·IDF+T5 31.06 0.00 44.25 98.05 68.02\nDPR+T5 38.07 21.19 54.55 97.38 58.64\nof erasure-based black-box model explanation technique [ 24,38].\nHowever, we treat facts as features rather than individual tokens.\n6 EXPERIMENTS\nWe begin in Section 6.1 by demonstrating the accuracy of the end-\nto-end NeuralDB , showing that queries can be answered with high\naccuracy over thousands of facts. We then validate the components\nof our architecture in isolation. In Section 6.2 we consider the\nbasic architecture, consisting of Neural SPJ followed by aggregation\n(without the SSG). In section 6.3 we evaluate our SSG module. In\nSection 6.4 we discuss how the results depend on the amount of\ntraining data and how learning transfers to relations unseen in the\ntraining set.\nImplementation. We use the HuggingFace [ 56] transformers library\nand its implementation of the t5-base transformer module [ 35]\nfor SPJ. With the exception of one experiment, the parameters of\nthet5-base model have been pre-trained on the colossal Common\nCrawl (C4) dataset [35] by predicting tokens masked in sentences.\nFor SSG, we use BERT which has a comparable architecture to T5.\nThe learning-rate for fine-tuning and number of epochs were se-\nlected through maximizing the Exact-Match (EM) accuracy on a\nheld-out validation set for the tasks. Fine-tuning is a stochastic\nprocess: for each experiment, we independently train 5 separate\nmodels with different random seeds and report mean accuracy.\n6.1 End-to-end NeuralDB\nBefore validating the SSG and SPJ components independently, we\nfirst evaluate the full NeuralDB architecture end to end, reporting\nresults in Table 1. At run time, the support sets generated by the SSG\nare input in to SPJ operators in parallel and then aggregated. Com-\npared to state of the art transformer models (bottom 2 rows) with\ninformation retrieval, the NeuralDB achieves the highest EM accu-\nracy on all types of queries. While these numbers cannot be directly\ncompared (as the NeuralDB requires supervision to the intermedi-\nate results), the NeuralDB makes a substantial improvements over\nseveral query types, overcoming fundamental limitations in how\nthe IR+transformer models perform aggregations. It is expected that\ndistantly supervising the SSG would decrease accuracy somewhat.\nHowever, given that the supervision signal is generated for free,\nthis is encouraging, especially as the impact was negligible in some\ncases. The decrease in accuracy is most evident for joins because\n\nJames Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, and Alon Halevy\nTable 2: Accuracy of Neural SPJ + Aggregation portion of ar-\nchitecture. We use 200 parallel SPJ operators (4xV100 GPUs,\nbatch size = 50) over a database with 8400 facts, averaged over\n14000 queries.\nInput at Training Projection EM (%) Answer EM (%)\nPerfect IR 97.87Â±0.1 98 .60Â±0.1\n+ (noise added at test ) 76.25Â±2 70 .35Â±3\nNoisy IR 98.21Â±0.1 98 .52Â±0.1\nof the over sensitivity of the model used to distantly supervise the\nSSG, reflected in the low SSG recall in Table 3.\n6.2 Neural SPJ + Aggregation\nThe experiments in Section 3 showed that the transformer model\nwas capable of generating the correct answer to queries requiring\nlookups and joins with near perfect accuracy on our test set when\nprovided with the right facts or small supersets thereof. We evaluate\nthe Neural SPJ on two settings. In Perfect IR , we provide the Neural\nSPJ only the needed facts. Hence, Neural SPJ does not need to decide\nwhich facts to use to answer the query. In Noisy IR , we sample\nrandom facts and add them to the support set used to train the\nmodel. Hence, weâ€™re also training the Neural SPJ to select the right\nset of facts to use for inference, thereby validating the resilience of\nthe SPJ operator to extraneous facts that might be returned by a low-\nprecision high-recall SSG. The noise was generated by sampling\n1âˆ’3additional facts with uniform probability for2\n3of instances.\nDataset D2. Training and evaluating the Neural SPJ operator re-\nquires data that differs slightly from the data in D1. In D1, the\ntraining data for a given query ğ‘„involved the answer, ğ‘„(ğ·). How-\never, for the Neural SPJ training we need to provide the result of\nthe SPJ component before the aggregation. In D1, the output to the\nquestion How many countries use Euro? would just be the number\nof countries, after aggregation. However, in D2, the training data\nconsists of a pairs of facts and intermediate results. For the fact,\nBelgiumâ€™s currency is the Euro , the intermediate result is Belgium\nbecause the count operator needs the names of the countries that\nuse the Euro. For the query What is the most widely-used currency? ,\nthe intermediate result for the same fact would include the pair\n((Euro, Belgium) ) because the aggregation operator first needs\nto group by currency before computing the max.\nD2 includes queries for training the model to answer set and\naggregation queries. We create a total of 632 templates: 115 for facts\nand 517 for the different query types over 27 relations, covering a a\nvariety of linguistic expressions. Using a sample of popular Wiki-\ndata entities, we generate a single database of 8400 facts and 14000\nqueries over it. For training, we generate approximately 10,000\ntraining instances per aggregation operation, preventing class im-\nbalance by preserving the proportions of D1.\nFindings. Table 2 shows the EM of the intermediate results produced\nby the Neural SPJ (projection EM), and the EM of the final answer\n(answer EM). In row 2 of Table 2 we observe a substantial drop of\nmore than 20% in EM, both Projection, and Answer, when we add\nnoise to the retrieved support set at test time. This suggests that\n104105\nNumber of training data020406080100Exact match %Intermediate result generation learning curve\nAll\nBoolean\nSet\nMax\nMin\nCount\nNULL AnswerFigure 8: Training the Neural SPJ model with a varying num-\nber data indicates that 40k training instances are required\nto train a model and that learning when not to generate an\nanswer (by predicting NULL) can be dominated by other rela-\ntions if there are insufficient examples for it.\nadding noise from retrieval at training time (third row), makes the\nneural SPJ module more resilient to errors introduced by the SSG.\nThe Neural SPJ operator was also trained to predict the down-\nstream aggregation function, which it did correctly with 99.97%\naccuracy. In 0.03%of cases, the wrong function name was selected.\nAnswer EM reflects the accuracy of the end-to-end performance of\nthe system. Furthermore, we inspected the error rate for each query\ntype for the results reported in Table 2 and noted that there was no\nlarge deviation between queries with aggregation and those with-\nout. This validates our choice to postpone aggregation functions to\nhappen after the Neural SPJ.\nError Analysis. For the Noisy IR model reported in Table 2 were no\nfalse positive or false negative errors (the model outputs a result\nwhen it should output NULL or vice versa). All errors were errors\nin the the content of the fact returned. Of the 253 errors, 166 were\nfor symmetric relations (X borders Y) which introduced ambiguity\nin training. 80 errors were due to the model incorrectly applying\nimplicit world knowledge when provided with insufficient facts to\nresolve geographical meronomy relations (e.g. incorrectly predict-\ning Zeeland [sic] is in Asia). The remaining errors were differing\npunctuation (e.g. missing a period).\nTraining data requirements. To evaluate training data requirements\nfor the Neural SPJ, we plot the learning curve for generating inter-\nmediate results using dataset D2 in Figure 8. For each point on this\ngraph, 5 models are trained with different random initializations\nand we report the mean EM score. The model attains near perfect\naccuracy with approximately 80k training instances. It is interesting\nto note that the Neural SPJ struggled the most with learning what\nitdoesnâ€™t know. With little training data, the model often outputted\na result even when it should have outputted NULL. This is a well\nknown weakness of sequence-to-sequence models [36].\n6.3 Support Set Generation\nIn this section we evaluate how well the SSG component (Section 5)\nretrieves facts to be fed to the Neural SPJ operators. It is tricky\n\nNeural Databases\nTable 3: Precision and recall of distantly supervised SSG w.r.t.\nthe reference set. Note that errors in training do not neces-\nsarily translate to wrong query answers because the Neural\nSPJ operator is somewhat robust to extra information.\nQuery\nTypeExact Match (%) Soft Match (%)\nPrecision Recall Precision Recall\nAtomic (bool) 81.45 98.92 95.02 99.90\nAtomic (extractive) 80.44 88.47 94.00 99.61\nJoin (bool) 62.28 83.13 62.28 83.13\nJoin (extractive) 72.22 85.95 72.22 85.95\nSet 29.97 89.14 33.13 89.14\nCount 56.42 92.80 59.68 92.80\nmin/max 68.63 100.00 68.63 100.00\nTotal 68.28 93.57 77.25 95.68\nto evaluate the SSG because errors in the SSG do not necessarily\ntranslate into errors in query answers. For example, the SSG may\nreturn a superset of a support set, but the Neural SPJ may still\ngenerate the correct answer.\nTable 3 shows the performance of the SSG. In the table, an output\nof the SSG is considered an exact match (correspondingly, a soft\nmatch) if it is exactly the same as (correspondingly, a super set of) a\nsupport set in the reference data. The table shows the precision and\nrecall of the distantly supervised SSG. The results for the supervised\nSSG are between 10-20% higher and up to 30% higher for queries\nwith aggregation. We tuned the training to increase recall so more\nsupport sets are generated.\nAs can be seen, while the results for lookups and joins are high,\nthey drop significantly for aggregation and set answers. However,\nthe point to note is that even in these cases, the SSG still learns what\nit needs. For example, consider training the SSG for the query who\nis the oldest person? When we train it by running a single Neural SPJ,\nit will rarely get the correct support set because that support set\n(any fact pertaining to peopleâ€™s age) is large and may not even be\nconsidered. However, the SSG still learns that it needs to find facts\nabout peopleâ€™s ages. So when the SSG action classifier is applied to\na single decision in Algorithm 2 (namely, should I add a fact that is\nabout a personâ€™s age to a support set? ), it will return True, thereby\nobtaining the desired result.\n6.4 Supervision and transfer learning\nAn important question for NeuralDB s is whether the system can\nanswer queries about relationships it has not seen in the training\ndata. We perform two experiments. In the first experiment we test\nthe accuracy of answering queries on a single new relation that\nwas not seen in training across a sample of relations. The result\nis shown in Figure 9 where the bars indicate the error rate at test\ntime for a model with that relation omitted during training.\nIn the second experiment, we double the number of relations\nin the test set compared to the training set. This corresponds to a\nscenario in which there is a massive sudden shift in the queries that\nhighlights the importance of the underlying language model. When\nPlace of Birth\nPlace of Death\nGender\nFather of\nSpouse\nCitizenship\nHead of state\nCurrency\nShares border\nAuthor\nMember of Sports Team\nDirector\nHead of government\nRelation omitted at training05101520253035EM error rate (%) of relation at testError rate of relations omitted during trainingFigure 9: Even with relations omitted during training, Neu-\nral SPJ often generates correct intermediate results.\nwe trained on 13 out of 27 relations and evaluated on all 27, EM\nfalls from 99% to 86%. However, if we started with a randomly ini-\ntialized language model and trained on the same 13 relations, then\nour accuracy drops to only 55% EM, showing that the pre-trained\nlanguage model is critical to the performance of the NeuralDB .\n7 RELATED WORK\nNLP and data management. Bridging the gap between unstructured\nnatural language data and database-style querying has been a long-\nstanding theme in database research [ 15]. The work on information\nextraction has developed techniques for translating segments of\nnatural language text into triples that can be further processed\nby a database system. Wikidata [ 48] itself is a social experiment\nwhere additions to the knowledge graph are encouraged to use\nalready existing relation names if possible, thereby alleviating the\nneed for information extraction. There has been significant work\non translating queries posed in natural language into SQL queries\non a database whose schema is known [ 4,23,58], with extensions\nto semi-structured data and knowledge bases [ 7,30]. More recently,\nsystems such as BREAK [ 57] and ShARC [ 41] have trained models\nto translate a natural language query into a sequence of relational\noperators (or variants thereof).\nNeuralDB s do not try to map data or queries into a pre-defined\nschema. At the core, we use neural techniques to process the facts\nin the database with the query given as context in natural language.\nHowever, NeuralDB s do some rudimentary analysis of the query\nwhen they decide whether it requires an aggregation operator, and\none can imagine that NeuralDB s will need more sophisticated\nunderstanding of the structure of a query as they tackle more com-\nplex queries. Similarly, the processing performed by the Neural\nSPJ operator is reminiscent of information extraction in the sense\nthat it produces a structured representation of facts that can be\nused by subsequent operators. However, a key difference is that the\n\nJames Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, and Alon Halevy\nextraction performed by the Neural SPJ is query dependent and is\nindependent of any schema.\nWith similar goals in mind, the Information Retrieval commu-\nnity has developed search engines to answer SQL queries [ 1,12].\nThe work most close to ours [ 51], explores the problem of answer-\ning queries from a collection of non-schematic XML documents\nthat exhibit heterogeneous structures, and hence are cumbersome\nin languages such as XPath or XQuery. Another similar research\nline is that of Whang et al. [54,55]. Similarly to what we propose\nthey also support natural language queries but they still exploit\nsemi-structured data. Whereas in our case, the system needs to\nâ€œunderstandâ€ what are the relations and attributes that need to be\nused and the relative operators\nQuestion answering from text. The NLP community has made great\nstrides recently on the problem of answering queries from text,\nwhich includes tasks such as open-book question answering [ 37,39]\nand fact verification [ 46]. To efficiently scale machine comprehen-\nsion to very large databases, the NLP community adopt either\na pipelined [ 22,46] or jointly trained [ 14] architecture of infor-\nmation retrieval with neural reasoning. Like NeuralDB s, many\nof these works answer questions using an explicit memory of\nknowledge (free-form text) in addition to the pre-trained language\nmodel [ 14,22,32]. However, these works typically require extract-\ning a span from a single document or predicting a token or label as\nan answer, whereas NeuralDB s require combining multiple facts,\nperforming selections and aggregation. While in-roads have been\nmade to perform discrete reasoning over passages [ 11], with ex-\nplicit computation [ 2], these use only a single passage rather than\nrequiring aggregation over large numbers of facts.\nMulti-hop question answering is a recent setting where answer-\ning a query requires finding supporting evidence in multiple doc-\numents (see [ 44,52,57] for data sets facilitating this research). In\nsolving multi-hop questions, the works either decompose the ques-\ntion into simpler sub questions [ 27,57], or condition each hop\non the previously retrieved documents [ 6]. Some of these ideas\ninspired the design of the SSG in Section 5. Transformers have\nbeen shown to perform soft-reasoning when provided with simple\npropositional rules [ 9]. In that work, transformers were able to join\na small number of facts and rules of the form ğ´â†’ğµ.\nWhile other works modeling the web as a knowledge bases have\nfocused on combining multiple snippets of text together [ 44], their\nassumption is that the query is decomposed into a SPARQL program\nthat is executed on pre-extracted information. Our innovation is\nthat no latent program or structure is needed and that information\nextraction is dynamic and dependent on the query.\nExtending neural architectures to reasoning tasks. In the same spirit\nto Neural Turing Machines[ 13] and Memory Networks[ 43] archi-\ntectures, an alternative way of building NeuralDB is to encode all\nthe facts in the database to a neural memory and build machinery\nto read, write, and reason on top of this neural memory. However,\nsuch an approach would not have control and transparency: It is\nchallenging to remove facts from the database or check whether\na particular fact exists. Also, it would not be possible to explain\nquery results. Furthermore, these architectures perform well on\nbAbI [ 53] tasks where the number of facts is limited, and mainly\nlookup or simple reasoning is needed. But, In our experiments inNeuralDB they couldnâ€™t perform well; we hypothesize that encod-\ning the query and facts together by a stack of self-attention in the\nencoder is necessary to answer database queries. There also have\nbeen considerable efforts in mixing traditional symbolic reasoning\nor data management algorithms with neural network architectures.\nFor example, RocktÃ¤schel et al. [40] have developed a differentiable\nversion of the backward chaining algorithm that drives prolog.\nMost closely to our work, Minervini et al. [28] has showed how\ndifferentiable prolog interpreters can be used to support reasoning\nwith facts in natural language. Instead of â€œneuralizingâ€ existing\nsymbolic reasoners, in our work we start off with a scalable neural\narchitecture, and support it with symbolic computation only where\nnecessary. This enables us to directly leverage the rapid progress\nmade in retrieval augmented QA models and ensures scalability.\n8 CONCLUSIONS AND FUTURE WORK\nWe described NeuralDB , a new kind of database system that uses\nneural reasoning, and is therefore able to answer queries from data\nexpressed as natural language sentences that do not conform to a\npre-defined schema. The design of the NeuralDB architecture was\nbased on a careful examination of the strengths and weaknesses of\ncurrent NLP transformer models. Our experimental results suggest\nthat it is possible to attain very high accuracy for a class of queries\nthat involve select, project, join possibly followed by an aggregation.\nTo fully realize the promise of NeuralDB s, more research is\nneeded on scaling up NeuralDB s to larger databases, supporting\nmore complex queries and increasing the accuracy of the answers.\nIn particular, an interesting area of research noted in Section 5 is de-\nveloping novel indexing techniques that enable efficient support set\ngeneration. Another exciting area to investigate is to consider other\nmedia in the database. For example, a database can also contain a\nset of images and some queries can involve combining information\nfrom language and from images. Such an extension would benefit\nfrom recent progress on visual query answering systems [3, 5].\nA possible downside of using neural techniques in a database\nsystem is the potential for bias that might be encoded in the un-\nderlying language model. As we discussed, the fact that London\nin the UK is encoded in the language model and is useful. How-\never, suppose our database included facts saying that John and Jane\nwork at a hospital, but when we asked what their profession is,\nthe system would answer doctor for John and nurse for Jane. Cur-\nrently, there is no good way of distinguishing biased from unbiased\nknowledge in a language model. A possible approach to this impor-\ntant issue is to design a separate module that attacks the database\nwith queries in order to discover hidden biases. Then, we could\ndevise safeguards within the database that ensure that we donâ€™t\nuse such biased knowledge in answering queries. Developing these\ncomponents is an area for future research.\nFinally, another interesting challenge concerns developing se-\nmantic knowledge that helps in identifying which updates should\nreplace previous facts and which should not. For example, if the\nfact, Mariah is unemployed , was in the database and later the fact,\nMariah works for Apple , was added, then the first fact should be\nremoved (or at least, apply only to queries about the past). However,\nthe same does not hold for the facts Kasper likes tea followed by\nthe fact Kasper likes coffee .\n\nNeural Databases\nREFERENCES\n[1]Sihem Amer-Yahia, Pat Case, Thomas RÃ¶lleke, Jayavel Shanmugasundaram, and\nGerhard Weikum. Report on the db/ir panel at sigmod 2005. SIGMOD Rec. ,\n34(4):71â€“74, December 2005.\n[2]Daniel Andor, Luheng He, Kenton Lee, and Emily Pitler. Giving Bert a calculator:\nFinding operations and arguments with reading comprehension. EMNLP-IJCNLP\n2019 - 2019 Conference on Empirical Methods in Natural Language Processing and\n9th International Joint Conference on Natural Language Processing, Proceedings of\nthe Conference , 2:5947â€“5952, 2019.\n[3]Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module\nnetworks. In 2016 IEEE Conference on Computer Vision and Pattern Recognition,\nCVPR 2016, Las Vegas, NV, USA, June 27-30, 2016 , pages 39â€“48. IEEE Computer\nSociety, 2016.\n[4]I Androutsopoulos, G D Ritchie, and P Thanisch. Natural Language Interfaces to\nDatabases - an Introduction. Natural Language Engineering , 1(1):29â€“81, 1995.\n[5]Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,\nC. Lawrence Zitnick, and Devi Parikh. VQA: visual question answering. In 2015\nIEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile,\nDecember 7-13, 2015 , pages 2425â€“2433. IEEE Computer Society, 2015.\n[6]Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caim-\ning Xiong. Learning to retrieve reasoning paths over wikipedia graph for question\nanswering. arXiv preprint arXiv:1911.10470 , 2019.\n[7]Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing\non freebase from question-answer pairs. EMNLP 2013 - 2013 Conference on\nEmpirical Methods in Natural Language Processing, Proceedings of the Conference ,\n(October):1533â€“1544, 2013.\n[8]Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. Language Models are Few-Shot Learners. 2020.\n[9]Peter Clark, Oyvind Tafjord, and Kyle Richardson. Transformers as Soft Reason-\ners over Language. IJCAI , pages 3882â€“3890, 2020.\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers) , Minneapolis, Minnesota, 2019.\n[11] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh,\nand Matt Gardner. DROP: A Reading Comprehension Benchmark Requiring\nDiscrete Reasoning Over Paragraphs. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers) , pages 2368â€“2378,\nMinneapolis, Minnesota, jun 2019. Association for Computational Linguistics.\n[12] Norbert Fuhr. Models for integrated information retrieval and database systems.\nIEEE Data Eng. Bull. , 19(1):3â€“13, 1996.\n[13] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR ,\nabs/1410.5401, 2014.\n[14] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-wei Chang.\nREALM : Retrieval-Augmented Language Model Pre-Training. 2020.\n[15] Alon Y. Halevy, Oren Etzioni, AnHai Doan, Zachary G. Ives, Jayant Madhavan,\nLuke K. McDowell, and Igor Tatarinov. Crossing the structure chasm. In CIDR\n2003, First Biennial Conference on Innovative Data Systems Research, Asilomar, CA,\nUSA, January 5-8, 2003, Online Proceedings . www.cidrdb.org, 2003.\n[16] Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Visualizing and understanding the\neffectiveness of BERT. EMNLP-IJCNLP 2019 - 2019 Conference on Empirical\nMethods in Natural Language Processing and 9th International Joint Conference\non Natural Language Processing, Proceedings of the Conference , pages 4143â€“4152,\n2019.\n[17] Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. Composition-\nality Decomposed: How do Neural Networks Generalise? Journal of Artificial\nIntelligence Research , 67:757â€“795, 2020.\n[18] Gautier Izacard and Edouard Grave. Leveraging Passage Retrieval with Genera-\ntive Models for Open Domain Question Answering. 2020.\n[19] Jeff Johnson, Matthijs Douze, and Herve Jegou. Billion-scale similarity search\nwith GPUs. IEEE Transactions on Big Data , pages 1â€“1, 2019.\n[20] Vladimir Karpukhin, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey\nEdunov, Danqi Chen, and Wen-tau Yih. Dense Passage Retrieval for Open-Domain\nQuestion Answering. 2020.\n[21] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. The\ncase for learned index structures. In Gautam Das, Christopher M. Jermaine, and\nPhilip A. Bernstein, editors, Proceedings of the 2018 International Conference on\nManagement of Data, SIGMOD Conference 2018, Houston, TX, USA, June 10-15,\n2018, pages 489â€“504. ACM, 2018.[22] Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntÃ¤schel, Sebastian Riedel, and Douwe Kiela. Retrieval-Augmented Generation for\nKnowledge-Intensive NLP Tasks. 2020.\n[23] Fei Li and H V Jagadish. Constructing an Interactive Natural Language Interface\nfor Relational Databases. Proceedings of the VLDB Endowment2 , 8(1):73â€“84, 2014.\n[24] Jiwei Li, Will Monroe, and Dan Jurafsky. Understanding Neural Networks through\nRepresentation Erasure. 2016.\n[25] Yuliang Li, Jinfeng Li, Yoshihiko Suhara, AnHai Doan, and Wang-Chiew Tan.\nDeep entity matching with pre-trained language models. CoRR , abs/2004.00584,\n2020.\n[26] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly\nOptimized BERT Pretraining Approach. 2019.\n[27] Sewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi. Multi-\nhop reading comprehension through question decomposition and rescoring.\narXiv preprint arXiv:1906.02916 , 2019.\n[28] Pasquale Minervini, Matko Bosnjak, Tim RocktÃ¤schel, Sebastian Riedel, and\nEdward Grefenstette. Differentiable reasoning on large knowledge bases and\nnatural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence,\nAAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence\nConference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in\nArtificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020 , pages\n5182â€“5190. AAAI Press, 2020.\n[29] Sidharth Mudgal, Han Li, Theodoros Rekatsinas, AnHai Doan, Youngchoon\nPark, Ganesh Krishnan, Rohit Deep, Esteban Arcaute, and Vijay Raghavendra.\nDeep learning for entity matching: A design space exploration. In Gautam Das,\nChristopher M. Jermaine, and Philip A. Bernstein, editors, Proceedings of the\n2018 International Conference on Management of Data, SIGMOD Conference 2018,\nHouston, TX, USA, June 10-15, 2018 , pages 19â€“34. ACM, 2018.\n[30] Panupong Pasupat and Percy Liang. Compositional Semantic Parsing on Semi-\nStructured Tables. Proceedings of the 53rd Annual Meeting of the Association for\nComputational Linguistics and the 7th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers) , pages 1470â€“1480, 2015.\n[31] Matthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. Dissecting\nContextual Word Embeddings: Architecture and Representation. In Proceedings\nof the 2018 Conference on Empirical Methods in Natural Language Processing , pages\n1499â€“1509, Brussels, Belgium, 2018. Association for Computational Linguistics.\n[32] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani,\nNicola De Cao, James Thorne, Yacine Jernite, Vassilis Plachouras, Tim RocktÃ¤schel,\net al. Kilt: a benchmark for knowledge intensive language tasks. arXiv preprint\narXiv:2009.02252 , 2020.\n[33] Fabio Petroni, Tim RocktÃ¤schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\nAlexander H. Miller, and Sebastian Riedel. Language Models as Knowledge\nBases? In Proceedings of EMNLP-IJCNLP , Hong Kong, China, 2019. Association\nfor Computational Linguistics.\n[34] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving\nLanguage Understanding by Generative Pre-Training. arXiv , pages 1â€“12, 2018.\n[35] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the Limits\nof Transfer Learning with a Unified Text-to-Text Transformer. Journal of Ma-\nchine Learning Research , 21:1â€“67, 2020.\n[36] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you donâ€™t know:\nUnanswerable questions for SQuAD. ACL 2018 - 56th Annual Meeting of the\nAssociation for Computational Linguistics, Proceedings of the Conference (Long\nPapers) , 2:784â€“789, 2018.\n[37] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD:\n100,000+ Questions for Machine Comprehension of Text. pages 2383â€“2392, 2016.\n[38] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \"Why Should I Trust\nYou?\": Explaining the Predictions of Any Classifier. 39(2011):117831, 2016.\n[39] Matthew Richardson, Christopher J C Burges, and Erin Renshaw. {MCT}est: A\nChallenge Dataset for the Open-Domain Machine Comprehension of Text. In\nProceedings of the 2013 Conference on Empirical Methods in Natural Language\nProcessing , pages 193â€“203, Seattle, Washington, USA, oct 2013. Association for\nComputational Linguistics.\n[40] Tim RocktÃ¤schel and Sebastian Riedel. End-to-end Differentiable Proving. In\nI Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, and\nR Garnett, editors, Advances in Neural Information Processing Systems 30 , pages\n3788â€“3800. Curran Associates, Inc., 2017.\n[41] Marzieh Saeidi, Max Bartolo, Patrick Lewis, Sameer Singh, Tim RocktÃ¤schel, Mike\nSheldon, Guillaume Bouchard, and Sebastian Riedel. Interpretation of natural\nlanguage rules in conversational machine reading. arXiv preprint arXiv:1809.01494 ,\n2018.\n[42] Asa Cooper Stickland and Iain Murray. BERT and PALs: Projected attention\nlayers for efficient adaptation in multi-task learning. 36th International Conference\non Machine Learning, ICML 2019 , 2019-June:10477â€“10488, 2019.\n[43] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory\nnetworks. In Advances in neural information processing systems , pages 2440â€“2448,\n\nJames Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, and Alon Halevy\n2015.\n[44] Alon Talmor and Jonathan Berant. The web as a knowledge-base for answering\ncomplex questions. NAACL HLT 2018 - 2018 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguistics: Human Language\nTechnologies - Proceedings of the Conference , 1:641â€“651, 2018.\n[45] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy,\nNajoung Kim, Benjamin Van Durme, Samuel R Bowman, Dipanjan Das, and Ellie\nPavlick. What do you learn from context? Probing for sentence structure in\ncontextualized word representations. ICLR , pages 1â€“17, 2019.\n[46] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal.\nFEVER: a large-scale dataset for Fact Extraction and VERification. In Proceedings\nof the 2018 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Volume 1 (Long Papers) ,\npages 809â€“819, New Orleans, Louisiana, 2018. Association for Computational\nLinguistics.\n[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Lilon Jones, Aidan\nGomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In 31st\nConference on Neural Information Processing Systems (NIPS 2017) , Long Beach,\nCA, USA, 2017.\n[48] Denny VrandeÄiÄ‡ and Markus KrÃ¶tzsch. Wikidata: a free collaborative knowl-\nedgebase. Communications of the ACM , 57(10):78â€“85, 2014.\n[49] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer:\nSelf-Attention with Linear Complexity. 2048(2019), 2020.\n[50] Christopher J C H Watkins and Peter Dayan. Q-learning. Machine Learning ,\n8(3):279â€“292, 1992.\n[51] Gerhard Weikum. Db&ir: both sides now. In Proceedings of the 2007 ACM SIGMOD\ninternational conference on Management of data , pages 25â€“30, 2007.\n[52] Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. Constructing datasets\nfor multi-hop reading comprehension across documents. Transactions of the\nAssociation for Computational Linguistics , 6:287â€“302, 2018.\n[53] Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van\nMerriÃ«nboer, Armand Joulin, and Tomas Mikolov. Towards ai-complete question\nanswering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698 , 2015.\n[54] Kyu-Young Whang, Jae-Gil Lee, Min-Jae Lee, Wook-Shin Han, Min-Soo Kim,\nand Jun-Sung Kim. Db-ir integration using tight-coupling in the odysseus dbms.\nWorld Wide Web , 18(3):491â€“520, 2015.\n[55] Kyu-Young Whang, Tae-Seob Yun, Yeon-Mi Yeo, Il-Yeol Song, Hyuk-Yoon Kwon,\nand In-Joong Kim. Odys: an approach to building a massively-parallel search\nengine using a db-ir tightly-integrated parallel dbms for higher-level functionality.\nInProceedings of the 2013 ACM SIGMOD International Conference on Management\nof Data , pages 313â€“324, 2013.\n[56] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\nAnthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe\nDavison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,\nCanwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,\nand Alexander M. Rush. Huggingfaceâ€™s transformers: State-of-the-art natural\nlanguage processing, 2020.\n[57] Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel\nDeutch, and Jonathan Berant. Break It Down: A Question Understanding Bench-\nmark. Transactions of the Association for Computational Linguistics , 8:183â€“198,\n2020.\n[58] Jichuan Zeng, Xi Victoria Lin, Caiming Xiong, Richard Socher, Michael R. Lyu,\nIrwin King, and Steven C. H. Hoi. Photon: A robust cross-domain text-to-sql\nsystem. CoRR , abs/2007.15280, 2020.",
  "textLength": 84684
}