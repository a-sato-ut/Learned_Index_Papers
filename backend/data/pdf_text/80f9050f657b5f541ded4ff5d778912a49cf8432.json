{
  "paperId": "80f9050f657b5f541ded4ff5d778912a49cf8432",
  "title": "Airphant: Cloud-oriented Document Indexing",
  "pdfPath": "80f9050f657b5f541ded4ff5d778912a49cf8432.pdf",
  "text": "AIRPHANT : Cloud-oriented Document Indexing\nSupawit Chockchowwat, Chaitanya Sood and Yongjoo Park\nUniversity of Illinois at Urbana-Champaign\nfsupawit2,csood2,yongjoo g@illinois.edu\nAbstract —Modern data warehouses can scale compute nodes\nindependently of storage. These systems persist their data on\ncloud storage, which is always available and cost-efﬁcient. Ad-\nhoc compute nodes then fetch necessary data on-demand from\ncloud storage. This ability to quickly scale or shrink data systems\nis highly beneﬁcial if query workloads may change over time.\nWe apply this new architecture to search engines with a focus\non optimizing their latencies in cloud environments. However,\nsimply placing existing search engines (e.g., Apache Lucene)\non top of cloud storage signiﬁcantly increases their end-to-end\nquery latencies (i.e., more than 6 seconds on average in one of\nour studies). This is because their indexes can incur multiple\nnetwork round-trips due to their hierarchical structure (e.g.,\nskip lists, B-trees, learned indexes). To address this issue, we\ndevelop a new statistical index (called IoU Sketch). For lookup,\nIoU Sketch makes multiple asynchronous network requests in\nparallel. While IoU Sketch may fetch more bytes than existing\nindexes, it signiﬁcantly reduces the index lookup time because\nparallel requests do not block each other. Based on IoU Sketch,\nwe build an end-to-end search engine, called A IRPHANT ; we\ndescribe how A IRPHANT builds, optimizes, and manages IoU\nSketch; and ultimately, supports keyword-based querying. In our\nexperiments with four real datasets, A IRPHANT ’s average end-to-\nend latencies are between 13 milliseconds and 300 milliseconds,\nbeing up to 8.97\u0002faster than Apache Lucence and 113.39 \u0002faster\nthan Elasticsearch.\nI. I NTRODUCTION\nIn public clouds, compute and storage can be scaled inde-\npendently. Based on this, modern data warehouses including\nSnowﬂake [1], Google BigQuery [2], Amazon Redshift [3],\nand Presto [4], [5] allow users to quickly start new clusters,\nanalyze a large volume of data retrieved on demand from cloud\nstorage (e.g., AWS S3 [6], Azure Blob Storage [7], GCP Cloud\nStorage [8]), and ﬁnally terminate those clusters if they are no\nlonger needed.1Users can easily tune many properties of those\nclusters such as number of nodes, number of cores, memory\nsize. To maximize the performance, users can upgrade their\nclusters. To reduce the cost, users can downsize their clusters.\nUsers can thus ﬁne-tune the overall cluster bill and expected\nperformance; regardless, their data is kept independently in\ncloud storage, offering cost-efﬁciency and robustness.2This\ncapability to quickly scale up and down clusters has been well\n1Google BigQuery effectively takes this series of operations. However, they\nare hidden from users’ perspective; the users are charged based on the cost\nof each query.\n2Note that compute resources (e.g., cores, memory) typically takes a larger\nportion in cloud bills than storage. Thus, scaling down compute nodes can\nlead to bigger savings.received.3We refer to this new architectural shift as separation\nof compute and storage .\na) New Direction: We investigate applying this new ar-\nchitectural shift to search engines . Search engines are systems\nfor retrieving relevant documents that contain user-provided\nsearch keywords. Commonly used search engines all rely on\nsome form of indexes (e.g., skip lists [11], B+ tree [12]–[14],\nlearned indexes [15], [16]) to quickly ﬁnd relevant documents.\nFor fast index traversals, they need their compute and storage\nclosely located on the same machine. This architecture has\na disadvantage that we need to keep a large cluster running\nas more documents are indexed, even if query workloads are\nlight or even if some documents are queried infrequently.\nHowever, if we can build a special index stored entirely\nin cloud storage, we can scale compute nodes independently\nof indexing. This offers ﬂexibility when query workloads\nchange over time or inquire about particular documents more\nfrequently than others. For instance, rarely queried documents\ncan simply be dormant in low-cost cloud storage along with\ntheir index. This new approach, if possible, can enable highly\ncost-efﬁcient searching.\nb) Challenges: Existing search engines do not perform\nwell under the separation of compute and storage. Because ex-\nisting indexes have hierarchical structures [11]–[17], they incur\nsigniﬁcant traversal overhead when the data is moved further\naway from compute. To identify postings (i.e. references to\ndocuments) of documents that contain the search keywords, or\nspeciﬁcally, to locate the data block containing those postings\n(called a postings list ), an existing index requires traversing\nfrom its root, to a child node at the next level, and so on, until\nwe reach a leaf node. In this process, to go one level deeper, we\nneed to fetch the next node from the storage device, incurring\nan additional communication. This means that to reach a leaf\nnode at level N, we need to make Nsequential back-to-\nback communications. If those individual communications are\nfast enough (e.g., local SSDs), the end-to-end latency for N\ncommunications may be tolerable. However, if those nodes are\nstored in cloud storage, each communication needs a network\nround-trip, which can be orders of magnitude slower than local\nSSDs. If such communications must be made sequentially mul-\ntiple times, the total end-to-end latency quickly adds up, signif-\nicantly affecting end-user experiences. Although node caching\n3For example, as of early 2021, Snowﬂake is one of the fastest growing\nprivate companies in the data space, with its market capitalization reaching\nabout $70 billion [9]. Also, Amazon Redshift introduced in December 2019 a\nnew type of node called ra3, which offers the ﬂexibility in adjusting cluster\nsizes [10].arXiv:2112.13323v1  [cs.DB]  26 Dec 2021\n\nmay reduce communications, allocating a large enough cache\nto store the entire index is prohibitively expensive when the\ncorpus size is large. For this reason, it is generally hard to\navoid multiple sequential communications in existing indexes.\nOn a separate note, Elasticsearch has a S3 plugin, only for\nsnapshots [18], but not for interactive querying.\nc) Our System: We introduce A IRPHANT ,4a new search\nengine speciﬁcally designed for low-latency querying under\nthe separation of compute and storage . While A IRPHANT\nrequires several nontraditional design decisions, its core idea\nis straightforward: to minimize end-to-end query latencies, we\nshould completely avoid sequential back-to-back communica-\ntions with cloud storage. Instead, A IRPHANT issues multiple\nasynchronous requests in parallel (thus, no blocking in be-\ntween) to obtain a postings list . This asynchronous approach is\nenabled by our novel statistical index. Once we have a postings\nlist, the rest of the process is almost identical; A IRPHANT\nretrieves individual documents and presents them to users.\nAs noted above, the crux of A IRPHANT is our statistical in-\ndex called Intersection of Unions Sketch (IoU Sketch) (§VI-B).\nUnlike existing index structures, IoU Sketch initially produces\nmultiple super postings lists (superpost) , where each superpost\ncontains both: 1) all relevant postings of documents containing\nsearch keywords, and 2) some irrelevant postings of the\ndocuments not containing search keywords. These superposts\nhave two important properties. First, they are independent from\neach other in retrieval process; thus, we can retrieve them in\nparallel. Second, by intersecting them, the number of irrelevant\ndocuments reduces exponentially due to their incoherence\nenforced by randomization. Thus, the ﬁnal postings list—the\nintersection of all superposts—is almost identical to the one\nfrom existing indexes.\nYet the ﬁnal postings list may contain a few false positives\nin expectation. Nonetheless, A IRPHANT removes them as it\nretrieves actual content of documents at a later step, recovering\na perfect precision as a result. Also importantly, superposts and\nso the ﬁnal postings list contain no false negative; A IRPHANT\nrecalls all documents that indeed contain search keywords.\nMoreover, IoU Sketch is carefully optimized according to a\nword frequency distribution in the corpus and queries (§VI-B)\ngiven memory and accuracy constraints. Although these false\npositives lead to an additional cost in document retrieval, IoU\nSketch’s performance beneﬁt overcomes such cost, leading to\nan improvement over existing indexing techniques. As a result,\nAIRPHANT signiﬁcantly outperforms existing search engines\ndeployed similarly, keeping its query latencies always under a\nsecond even for the largest dataset we have tested. IoU Sketch\nis built per corpus; thus, it is sufﬁcient to download it only\nonce before querying. In addition, its size is conﬁgurable (less\nthan 2MB in our experiments), making IoU Sketch lightweight\nto keep it in memory.\nd) Summary of Contributions: The contributions of this\nwork are organized as follows:\n4A portmanteau from “air” and “elephant”, signifying the lightweight\nsystem capable of serving from a large corpus1. We introduce A IRPHANT , a new search engine optimized\nfor cloud environments. Following the separation of com-\npute and storage, A IRPHANT persists all data (indexes and\ndocuments) in cloud storage. (§II and §III)\n2. We describe a novel statistical index called IoU Sketch as a\ncore component of A IRPHANT . IoU Sketch can accurately\nidentify documents containing search keywords with a\nsingle batch of concurrent communications with cloud stor-\nage; consequently, its end-to-end latency is signiﬁcantly\nshorter than existing indexes. (§IV)\n3. We show how we can optimize IoU Sketch for a given cor-\npus and requirement. Speciﬁcally, we express IoU Sketch’s\naccuracy in terms of expected number of false positives\nparameterized by IoU Sketch structure. Then, we analyze\nand present an efﬁcient optimization algorithm. (§IV)\n4. We empirically compare and analyze the performance of\nAIRPHANT to other existing systems such as Lucene [19],\nElasticsearch [20], SQLite [21], and na ¨ıve hash table. (§V)\nII. A IRPHANT : CORE IDEAS\nThis section introduces the core ideas of A IRPHANT . For\nthis, we ﬁrst brieﬂy recap core concepts in search engines\n(§II-A). Next, we discuss system challenges of running search\nengines on top of cloud storage (§II-B). Finally, we present\nthe core ideas of A IRPHANT for addressing those challenges\n(§II-C).\nA. Search Engines: Background\nThis section overviews the core concepts in document\nindexing and searching [22]. First, we describe high-level\nuser interface of search engines. Next, we describe how a\nsearch engine quickly ﬁnds the documents containing a search\nkeyword.\na) User Interface: We brieﬂy describe a typical program-\nming interface of search engines from end users’ perspectives.\nFigure 1 shows an example code snippet for creating/index-\ning documents and performing a keyword-based search. The\nstrings passed to document objects (i.e., \"hello world\"\nand\"hello airphant\" ) are parsed by the search engine\ninto multiple words (i.e., \"hello\" and\"world\" for doc1;\n\"hello\" and\"airphant\" for doc2). These parsing rules\nare conﬁgurable by users [23]. Then, the user can use any\nof those parsed keywords (i.e., hello, world, airphant) for\nsearching.\nb) Internal Workﬂow: A typical search engine internally\nimplements an inverted index to retrieve relevant documents.\nAn inverted index is a data structure to quickly identify the\ndocuments containing a search keyword. It normally consists\nof two sub-components: 1) postings lists , and 2) term index .\nA postings list is a list of document IDs (e.g., doc1 and doc2)\ncontaining an associated keyword (e.g., “hello”). There are as\nmany postings lists as the number of indexed keywords. The\nterm index is a map from each keyword (e.g., “hello”) to the\nlocation of its associated postings list. For fast searching, the\nterm index should be able to quickly return the location of\nthe postings list. B-trees and skip lists and commonly used\n\n// index two documents\nIndex index = new Index();\nDocument doc1 = new Document(\"hello world\");\nindex.addDocument(doc1);\nDocument doc2 = new Document(\"hello airphant\");\nindex.addDocument(doc2);\n// search for documents containing \"airphant\"\nDocument[] docs = index.search(\"airphant\");\nFig. 1: Example user interface of a search engine\ndata structures for their asymptotically fast lookup operations.\nInverted indexes (both postings lists and term index) are\nconstructed when documents are inserted into a search engine.\nGiven a query keyword, a search engine performs the\nfollowing operations. First, using the term index, it identiﬁes\nthe location of the postings list associated with the search\nkeyword. Second, it fetches the postings list for the search\nkeyword. Third, it fetches the actual contents of the documents\nand return them to the user.\nFor the term index, hierarchical indexes are commonly used.\nA skip list [11], [24] is used by Apache Lucene [19], an\nunderlying engine for distributed search engines such as Elas-\nticSearch [20] and Apache Solr [25]. While it is probabilistic,\na skip list needs O(logn)steps on average for each lookup.\nA B-tree can also be used for the term index, which has the\nsame lookup cost O(logn).\nB. Search Engines on Cloud: Challenges\nPlacing search engines directly on top of cloud storage\nincreases their end-to-end search latencies signiﬁcantly. One\ncritical performance bottleneck comes from term indexes.\nWe ﬁrst explain why a term index lookup operations slow\ndown substantially. Then, we describe why simple alternative\napproaches are not likely to solve the issue. These challenges\nmotivate A IRPHANT (§II-C).\na) Cloud Storage: Major public cloud computing ven-\ndors (e.g., AWS, GCP, Azure) all offer cloud storage services.\nSimply speaking, these services are object storage where each\nobject or blob is identiﬁed by a name. To upload and download\ndata to and from cloud storage, vendor-provided programming\nAPIs are used. These APIs make requests over the network\nto fetch data. These services are robust against data loss\n(e.g., 99.999999999% durability of AWS S3) and cost-efﬁcient\n($20/TB/month at the time of writing).\nb) Performance Challenge: While the network band-\nwidth is increasing, we observe that network latency —the\ntime to get the ﬁrst byte—acts as a critical bottleneck for\nsearch engines. That is, whether we are retrieving a smaller or\nlarger volume of data, there is some overhead we must pay for\nevery network request. This network latency causes an afﬁne\nrelationship between 1) the size of data to fetch and 2) the total\nelapsed time (referred to as retrieval latency ). Figure 2 depicts\nsuch a relationship using a small virtual machine (2 cores, 2GB\nmemory) with multithreaded download on Google Cloud Note\nthat both X-axis and Y-axis are in log-scale. The ﬁgure shows\nthat the retrieval latency remains around 50 milliseconds until1KB2KB4KB8KB16KB32KB64KB128KB256KB512KB1MB2MB4MB8MB16MB32MB64MB128MB256MB512MB3264128256512102420484096\nSize of data to fetch at a timeLatency (ms)Average latency with standard deviation (10 runs)\nFig. 2: End-to-end latency between Google Cloud virtual\nmachines and Google Cloud Storage.\nwe increase the data size beyond 2MB. After that point, the\nretrieval latency increases linearly.\nThis afﬁne characteristic of network performance causes\nexisting search engines to perform poorly. To ﬁnd the postings\nlist for a keyword, a search engine performs a lookup using\na term index. This lookup may involve multiple round-trips\nto storage devices for fetching intermediate nodes.5If the\nstorage device is cloud storage (not a local SSD), each round-\ntrip incurs a network latency. When multiple network requests\nare made, lookup latency quickly increases even if each trip\nretrieves a small amount of data. Parallelizing these network\nrequests is non-trivial because in order to make the next\nnetwork request, we need the information obtained from the\nprevious network request. Of course, if we can cache the entire\nterm indexes in memory for every corpus, we can completely\navoid any communications over the network. However, such\nscenario requires expensive equipment and so the approach is\ncostly. As described in §I, our goal is the opposite: we want\nto minimize our compute resources without much sacriﬁcing\nsearch performance.\nC. Statistical Approach to Indexing\nIn this section, we present the core ideas of our statistical\nindexing. Its primary goal is to avoid multiple sequential\ncommunications in obtaining a postings list. To achieve this,\nwe propose non-trivial changes to inverted indexes. In essence,\nit makes the following systems tradeoff: the payloads of\nindividual requests become larger, but those requests are made\nin parallel.\na) Bins: To keep the pointers to allpostings lists in mem-\nory, we form bins by merging multiple keywords. Accordingly,\nassociated postings lists are also merged into one. For exam-\nple, consider three regular keywords and their postings lists:\n(“hello”!(doc1, doc2)), (“world” !(doc1)), and (“airphant”\n!(doc2, doc3)). Suppose we merge “hello” and “world” into\none binb1, which produces: ( b1!(doc1, doc2)), (“airphant”\n!(doc2, doc3)). After merge, the number of keywords is\nreduced from three to two, but we no longer have exact\npostings list for merged keywords. In other words, the postings\nlist forb1contain both the postings list for “hello” and the list\nfor “world”. Thus, some of the documents in (doc1, doc2)\nmay not contain “hello” (false positives); however, no other\ndocuments contain “hello” ( no false negatives ). Systematically,\n5See our description on term index operations in §II-A.\n\nwe rely on hash functions to determine how to group original\nkeywords into bins.\nThis merge operation has one drawback: we will have to\nfetch more documents. This drawback is more signiﬁcant if\nthere are much fewer bins than original keywords (e.g., 1000\nkeywords to 1 bin on average). Note that although the false\npositive documents can be removed by examining the actual\ncontent, in order to do so, we must ﬁrst fetch those documents.\nFetching, say, 1000\u0002more documents can slow down end-to-\nend search latencies greatly. To address this, we exploit the\nprobabilistic nature of random merge, as described below.\nb) Multi-layer Structure: Retrieving a large number of\nactual documents is slow. To avoid this, we reduce false\npositives by extending the above approach. Note that merging\nkeywords produces a map from a bin to a (merged) postings\nlist. By repeating this merge operation, we construct a multi-\nlayer (sayLlayers) structure where each layer contains a dif-\nferent hash function. Different hash functions produce different\ngroupings; thus, each layer is associated with different sets of\nbins.\nGiven a keyword, we use this L-layer structure to obtain\nan accurate postings list, as follows. First, we apply Lhash\nfunctions to a keyword, and look up those hash values in our\nmulti-layer maps; then, we obtain the pointers toLpostings\nlists. Second, we fetch Lpostings lists in parallel from cloud\nstorage (here, we make a single batch of concurrent network\nrequests). Finally, we intersect those Lpostings lists to obtain\na ﬁnal posting list.\nThe ﬁnal postings list is accurate (i.e., small false positives)\nbecause most false positives are eliminated as part of inter-\nsection operations. This is due to the multiplication rule for\nindependent events in probability. That is, if we consider a\nsingle merge operation, it introduces many false positives into\na postings list (from the perspective of one of the merged\nkeywords). However, if we consider multiple independent\nmerge operations and take only the common postings among\nthem, the number of false positives decreases exponentially.\nWe formally discuss its mathematical properties in §IV. The\nmulti-layer structure is still memory-efﬁcient because it only\nstores hash functions and the pointers to (merged) postings\nlists; it does not store original keywords.\nIII. S YSTEM DESIGN\nWe describe A IRPHANT ’s core building blocks and con-\ncepts, with a focus on their high-level operations. Speciﬁcally,\nwe present the current scope of A IRPHANT (§III-A), its\ninternal components, and ofﬂine operations (e.g., indexing\nbuilding).\nA. Current Scope\nAIRPHANT supports important use cases, but its capability\nis still limited compared to full-ﬂedged search engines such\nas ElasticSearch [20] and Apache Solr [25]. This section\ndescribes what A IRPHANT currently supports: (a) the query\nworkloads it can handle and (b) a few requirements it needs\nfrom cloud storage.TABLE I: Component-wise correspondence between Apache\nLucene [19] and A IRPHANT (this work). Skip list and postings\nlist are sub-components of Lucene index. Likewise, MHT and\nsuperpost are sub-components of IoU Sketch.\nApache Lucene A IRPHANT (Ours)\nLucene index (inverted index) IoU Sketch\n\u0001Skip Lists (term index) \u0001Multilayer Hash Table (MHT)\n\u0001Postings List \u0001Super Postings List (superpost)\na) Documents and Queries: AIRPHANT offers document\nsearches with exact keyword matching; that is, it ﬁnds the\ndocuments containing a given set of keywords. In comparison,\nexisting full-ﬂedged search engines also support others such\nas range queries, fuzzy queries, etc. A IRPHANT aims towards\nread-oriented workloads where the corpus doesn’t change\nfrequently. This focus allows many design decisions to fully\nharness strength of cloud storage. As of now, extensions to\nsupport a wider class of queries and handle frequent corpus\nupdates are deferred to future works.\nb) Target Environments: AIRPHANT targets modern pub-\nlic cloud, where compute nodes can quickly turn on and\noff while most data can be stored safely in cloud storage.\nAIRPHANT heavily relies on cloud storage for data man-\nagement. It indexes the documents stored in cloud storage\nand subsequently persists index structures in cloud storage\nas well. It assumes that cloud storage offers random read\noperations; that is, fetching bytes from an arbitrary offset\ndoesn’t require full read. Random reads are useful in packing\nmultiple postings lists in a single blob; A IRPHANT can read\narbitrary postings lists without performance penalty. Also,\nkeeping all postings lists in a few blob makes the overall\ndata management easier because otherwise, we will need as\nmany blobs as the number of bins. Random reads are already\nsupported by major cloud vendors [26]–[28]. Note that original\ndocuments may be stored in a single blob (e.g., delimited by\nline breaks) or in different blobs. In each posting, A IRPHANT\nrecords (blob name, offset, length) as part of a document\nidentiﬁer.\nAfter constructing necessary structures, A IRPHANT\nSearcher (described later shortly) can reside anywhere with\nan access to the cloud storage to serve search queries.\nBecause of its conﬁgurable memory usage, it can be\nsufﬁciently portable to deploy on IoT and mobile devices,\nor it can inﬂate to support a humongous corpus on powerful\nmachines. Perhaps one of interesting settings is the serverless\ndeployment serving query requests, for example, function-\nas-a-service (FaaS) e.g. Google Cloud Functions [29], AWS\nLambda [30], and Microsoft Azure Functions [31]. Because\nof the minimal initialization, the deployment manager can\nquickly scale up or down based on the current demand across\ndifferent corpuses.\nB. Statistical Inverted Index\nThis section describes systems aspect of our statistical\n(inverted) index. We will mention core concepts introduced\n\nin §II-C, with a focus on deployment and management per-\nspectives.\nOur statistical index is called IoU Sketch ; we named it\nIoU or intersection of unions because our index construction\ninvolves unioning (or merging) multiple postings lists into one,\nand our search involves intersecting multiple postings lists.\nIoU Sketch consists of two sub-components: a multi-layer hash\ntable (MHT) and super postings lists (superposts). MHT is the\nmulti-layer map we described in §II-C. Superposts are merged\npostings lists (also described in §II-C). MHT is downloaded\nand kept in memory when a certain corpus is searched for the\nﬁrst time. Superposts are always kept on cloud storage. To\nclarify their roles in existing contexts, Table I makes one-to-\none comparisons between Apache Lucene and ours. Of course,\ntheir internal operations are different.\nC. Components and Workﬂow\nAIRPHANT consists of two components: Builder and\nSearcher. Builder creates and persists an index; Searchers uses\nthe index for querying.\na) Builder Workﬂow: AIRPHANT Builder (or simply,\nBuilder) is a component that creates IoU Sketchs and persists\nthem on cloud storage. Builder creates a single IoU sketch\nper corpus. Each IoU Sketch consists of superposts and MHT,\nboth of which Builder creates and persists. Figure 3 lays out\nthe steps to build IoU Sketch from proﬁling to optimization.\nBuilder’s index creation starts when the user passes a corpus\nand conﬁgurations. Builder uses a corpus-document parser\nto unwrap a blob into documents and generate postings that\nrefer to their documents’ byte ranges, allowing direct retrieval\nafterwards. Builder then uses a document-word parser to\nextract words. The user can select both a corpus-document and\na document-word parsers for each corpus. By default, a single\nblob may contain multiple documents; however, developers\ncan override this with custom parsers.\nThese parsed documents are then proﬁled to collect statistics\nnecessary for index building. A IRPHANT Builder makes a\nsingle pass over all documents during proﬁling. The collected\nstatistics include the total numbers of documents and words,\ndocument lengths, and document frequencies (i.e., the number\nof documents that contains a speciﬁc word). The statistics are\nused for IoU’s structural optimization (§§ IV-A and IV-E).\nBased on the proﬁle, Builder optimizes its IoU Sketch\nstructure using the algorithm described in §IV-A. The accuracy\nand memory requirements may also be speciﬁed. Alternative\nto auto-tuning, users can also manually select the IoU Sketch\nstructure, skipping both proﬁling and optimization steps.\nBuilder ﬁrst creates superposts. A IRPHANT Builder gener-\nates superposts from all documents. That is, a superpost is\nconstructed for each bin, and a collection of all superposts\nare persisted. Recall that each superpost is a list of merged\npostings, where each posting comprises (a blob name, an off-\nset, and a length), which are used to locate actual documents.\nThe collection of superposts are concatenated into a single\nblob using a compaction encoding (§IV-C). This makes data\nmanagement easier.Builder\nParsed Doc Corpus Proﬁle\nIoU Sketch\nSearcherMetadata\nClientCorpus\nPostings Lists\n1. Initialize\n2.1 Submit query2.2 Fetch postings2.3. Retrieve and\nﬁlter documents\n2.4 OutputMHTCompaction\nFig. 3: A IRPHANT Overview\nNext, Builder creates a MHT. MHT stores the pointers to\nthose superposts, which are compacted in a single blob. To\nlocate each superpoint, MHT’s pointer has a blob name, byte\noffset, and byte size. In addition, Builder stores seeds of hash\nfunctions (used for creating super posts) and other metadata\nin the same ﬁle. This ﬁle is persisted as another blob.\nb) Conﬁguring Builder: The user can conﬁgure A IR-\nPHANT in different ways. A storage driver speciﬁes how to\nread a corpus. Parsers speciﬁes how to separate documents in\na corpus and how to extract keywords from a document. The\naccuracy of IoU sketch can be set in terms of average number\nof irrelevant documents (i.e, false positive rate). The memory\nlimit on MHT can also be placed.\nc) Searcher: AIRPHANT Searcher (or simply, Searcher)\nis a light-weight component that retrieves the documents\ncontaining search keywords. Searcher relies on IoU Sketches.\nSearcher performs two types of operations: initialization and\nquerying. Initialization happens only once per corpus. Query-\ning happens for each query. The same diagram (Figure 3)\nillustrates the two procedures to fulﬁll queries: initialization\nand querying.\nWhen a user opens a corpus, Searcher initializes itself from\ncloud-stored index structure; it retrieves hash seeds and post-\nings list pointers (for MHT), both of whose memory footprint\nis predictable and controllable via IoU Sketch conﬁguration.\nIt then reconstructs hash functions, and hence, MHT.\nWhen a query arrives, Searcher hashes each word in the\nquery to collect a set of pointers to postings list. Using these\npointers, Searcher concurrently fetches the corresponding post-\nings lists from cloud storage and computes the intersection of\npostings lists (i.e., the ﬁnal postings list). This is the only\nbatch of concurrent requests for lookup. Then, the documents\nidentiﬁed by this ﬁnal postings list is retrieved from cloud\nstorage. Finally, Searcher ﬁlters out irrelevant documents after\nfetching the documents. This ﬁltering process is much fast\ncompared to document-fetching. The user can also fetch only\ntopKrelevant documents, which is useful for lowing latencies\n(§IV-D).\n\nIV. S TATISTICAL INVERTED INDEX\nIn this section, we detail our statistical inverted index\n(IoU Sketch). First, we deﬁne IoU Sketch (§IV-A). Then,\nwe detail corpus proﬁling (§IV-B) and superposts encoding\n(§IV-C). Next, we introduce two techniques to accelerate\nAIRPHANT query: top-Kqueries (§IV-D) and special handling\nof extremely common words (§IV-E).\nA. IoU Sketch\nIoU Sketch is the core index data structure that maps a\nkeyword to a postings list. We ﬁrst explain its data structure\nand interface as well as raw parameters. Second, given a\ncorpus, we analyze the expected accuracy based on speciﬁc\nsettings of raw parameters. Because one of raw parameters\nis not intuitive from user’s perspective, IoU Sketch offers\nan alternative conﬁguration based on memory constraint and\ndesired accuracy. Using earlier analysis, it then automatically\ntunes its raw parameters to meet those constraints and optimize\nfor search performance. Lastly, we further examine tightness\nof IoU Sketch’s accuracy.\na) Data Structure: Internally, IoU Sketch is a L-layer\nhash table with Ldifferent hash functions. Each bin in a\ntable contains a superposts that is aggregated via insertion\noperations. Under IoU Sketch’s hash functions, any given word\nis mapped to one bin per layer ( Lbins in total) where the\npostings list of the word is guaranteed to be a subset of the\nsuperpost in each associated bin. Such guarantee eliminates\nfalse negative but entails false positive. IoU Sketch supports\ntwo main functionalities.\n1.insert(word, document list) : For each layer, it\nhashes the word to ﬁnd its bin and update bin’s superpost\nto its union with word’s postings list.\n2.query(word) : It retrieves superposts from all layers;\nthen, outputs the intersection of all superposts.\nFor illustration, Figure 4 shows IoU Sketch hash tables after\ninserting four words w1,w2,w3, and w4with different\npostings lists. Speciﬁcally, word w2is mapped to (layer1,\nbin2) ,(layer2, bin2) , and (layer3, bin1) using\nhash functions from the three layers. It shares the same bin\nasw3in the ﬁrst layer, w4in the second layer, and both w1\nandw3in the third layer. Each bin then stores the aggre-\ngated superpost of the words; for example, the superpost of\n(layer1, bin2) is the union of postings lists of words w2\nandw3:fd2;d3g[fd2;d3;d4g=fd2;d3;d4g. Querying\nthe word w2therefore results in a postings list fd2;d3;d4g\\\nfd2;d3;d4;d5g\\fd1;d2;d3;d4g=fd2;d3;d4gwhich\ncontains a false positive postings, d4. On the other hand,\nquerying the word w1fortunately produces in the exact\npostings listfd1gdespite the word sharing bins in second\nand third layers with other three words.\nRecall there are ndocuments and let Bbe the total\nnumber of bins across all layers. To persist, IoU Sketch only\nrequires superposts and hash seeds to reconstruct itself, making\nits worst case storage complexity O(minfnmL;Bn +Lg)if\ndocuments have mwords on average. To achieve single-cycle\nretrievals, A IRPHANT Builder split IoU Sketch into superpostsw1: d1\nw2: d2, d3\nw3: d2, d3, d4\nw4: d2, d3, d4, d5w1 w2, w3 w4\nw2, w4 w1, w3\nw1, w2,\nw3w4d1 d2, d3,\nd4d2, d3,\nd4, d5\nd2, d3,\nd4, d5d1, d2,\nd3, d4\nd1, d2,\nd3, d4d2, d3,\nd4, d5\nPostings Lists Word-bin Mapping Super Postings Lists\nFig. 4: Example of IoU Sketch for a corpus with 5 documents\nand 4 distinct words. Word-bin mapping shows sets of words\nin corresponding IoU Sketch’s bins. Super postings lists are\nthe merged postings lists for those sets of words.\nand MHT. It then stores superposts on cloud storage, so that\nMHT isLlayers of hash tables where each entry has a\npointer to the corresponding superpost. Therefore, A IRPHANT\nSearcher needs to hold O(B)of memory for MHT ( O(L)hash\nseeds andO(B)bin pointers where L\u001cB).\nFigure 5 empirically demonstrates that IoU Sketch achieves\nsigniﬁcantly fewer false positives compared to a hash table\n(L= 1) whenBis ﬁxed. As Lincreases from 1, the number\nof false positives decreases rapidly. After a certain value,\nthe allocated bins are divided into too many layers, resulting\nin a smaller number of bins per layer, and consequently, a\nhigher selectivity and more false positives. Not only do these\nobservations justify the use of multiple hash tables over a\nsingle one, but it also suggests that there is an optimal choice\nofLdepending on both the corpus and the choice of B.\nb) Expected False Positives: For a family of pairwise\nindependent hash functions, IoU Sketch selects (hl)L\nl=1where\neachhlis the hash function for l-th layer. Let Wibe the set of\ndistinct words in i-th document with its size being jWij, and\nWbe the set of all words. Suppose Bis given and assume\nBis divisible by Lfor simplicity, the probability that i-th\ndocument is a false positive in a query of any irrelevant word\nwisqi(L) =qi(L;B;fWign\ni=1). Independence between wand\nqiis a result of pairwise independent hash family. Equation (1)\nalso shows the approximation ^qi(L)whose properties leads to\nan efﬁcient optimization.\nqi(L) =\"\n1\u0000\u0012\n1\u00001\nB=L\u0013jWij#L\n\u0019\"\n1\u0000e\u0000jWijL\nB#L\n= ^qi(L)\n(1)\nAssume a query word distribution w\u0018Cat(W;~ p)wherepw\nis the prior probability of the word win a query, Equation (2)\ndescribes the expected number of false positives over all words\nwhose unit is count per query. This is the primary objective\nfunction to tune IoU Sketch. For brevity, we write F(L) =\nF(L;B;fWign\ni=1)and deﬁne its approximation ^F(L)similarly\nusing ^qi(L)in place of qi(L). Hereci=P\nw2WnWipwis the\nprobability of words not contained in i-th document and acts\nas a linear combination coefﬁcient in F. Figure 5 asserts the\nresemblance between this formula and empirical observations.\nIn fact, later, we conﬁrms that the observed number of false\npositives is highly concentrated around this expectation.\nF(L) =nX\ni=1X\nw2WnWipwqi(L) =nX\ni=1ciqi(L) (2)\n\n500 1000 1500 2000 2500\n3000 3500 4000 4500 5000\n12468101214160.1110102103\nNumber of layers ( L)False Positives\n(a)Average False Positives12468101214160.1110102103\nNumber of layers ( L)False Positives\n(b)Expected False Positives\nFig. 5: Average and expected numbers of false positives (count\nper query) when varying the numbers of layers Land binsB\nonCranfield . Each line corresponds each Bvalue.\nEven though Lis a discrete variable, we extend its domain to\na continuous one L2R;1\u0014L\u0014Bto study richer character-\nistics ofF(L). In particular, the extension endows us with the\nderivative ^f(L) =d\ndL^F(L) =Pn\ni=1ci^qi(L). We condense the\nformula by substituting in zi(L) = 1\u0000exp (\u0000jWijL=B). The\nshift of focus towards the approximation eases the analysis\nand leads to an efﬁcient algorithm to optimize IoU Sketch\nstructure.\n^q0\ni(L) =zi(L)L\u00001[zi(L) lnzi(L)\u0000(1\u0000zi(L)) ln(1\u0000zi(L))]\n(3)\nc) Optimization on Number of Layers: The overall per-\nformance improves when the number of layers is smaller\nsince a query would fetch and intersect a fewer superposts.\nIn addition, a larger number of bins would replicate more\npostings across layers, further increasing the index storage\nsize. Consequently, we are interested in minimizing the num-\nber of layers given constraints on the number of bins Band\nfalse positives F0. In other words, the optimization problem\n(Equation (4)) ﬁnds the smallest number of layers that the\n(B;L)IoU Sketch has a fewer expected number of false\npositives than F0.\nL\u0003= minLs.t.1\u0014L\u0014B;F(L;B;fWign\ni=1))\u0014F0(4)\nUnfortunately, F(L)is non-convex and can contain multiple\nminimizers. Nevertheless, an analysis on its approximation\n^F(L)reveals three important characteristics as the building\nblocks for Algorithm 1. First, there is a region of fast opti-\nmization covering practical F0values (Lemma 2). Secondly,\nalthoughLcan be as large as B, we only need to search\nin a much smaller interval (Lemma 3). Finally, there is a\nrelatively cheap lower bound which allows us to quickly check\nthe feasibility (Lemma 1).\nWith these lemmas in mind, Algorithm 1 ﬁrst validates the\nproposed constraints BandF0with the lower bound. It then\ndetermines whether Lfalls in the fast or slow region and\npicks the optimization routine accordingly. For the fast region\nwhere ^F(L)is decreasing, it performs a binary search to ﬁnd\nthe smallest Lin the range [1;Lmin]. On the other hand, the\nslow region in the range [Lmin;Lmax]does not guarantee such\nmonotonicity, so the algorithm iteratively attempts increasingAlgorithm 1: Number of Layers Minimization\nInput: Number of bins B, expected false positive F0, sets of\nall distinct wordsfWign\ni=1, query word distribution\nCat(W;~ p)\nOutput: Minimum number of layer L\u0003or rejection\n1ifPn\ni=1ci2\u0000L\u0003\ni\u0014F0then\n2 ifF(Lmin)\u0014F0then\n3L\u0003 binary search L2[1;Lmin]\n4 else if iterative search L2[Lmin;Lmax]succeeds then\n5L\u0003 the result from iterative search\n6returnL\u0003if assigned, otherwise reject\nvalues ofLuntil the constraint is met. If either the lower bound\nchecking or iterative search fails, it is impossible to ﬁnd a L\nthat satisﬁes the constrains and so the algorithm rejects.\nLemma 1. L\u0003\ni= arg minL^qi(L) =B\njWijln 2. It immediately\nfollows that ^qi(L\u0003\ni) = 2\u0000L\u0003\niand so ^F(L)\u0015Pn\ni=1ci2\u0000L\u0003\ni.\nProof. From Equation (3), the minimizer L\u0003\nisatisﬁes ^q0\ni(L\u0003\ni) =\n0. Note that the left factor is always positive, so it must be\nthe case that the right factor is zero or equivalently zi(L\u0003\ni) =\n1=2. Therefore, exp\u0000\n\u0000jWijL\u0003\ni=B\u0001\n=L\u0003\ni; in other words,\nL\u0003\ni=B\njWijln 2. Substitute this minimizer to Equation (1) and\nEquation (2) to produce the two results later.\nRemark. BecauseF(L)>^F(L)for1\u0014L\u0014B, we have also\nderived a lower bound F(L)>Pn\ni=1ci2\u0000L\u0003\ni, validating the\nfeasibility check in alg. 1.\nLemma 2. ForL < miniL\u0003\ni=Lmin, expected false positive\nis strictly and exponentially decreasing ^f(L)<0and ^F(L) =\nO\u0010n\n2L\u0011\n.\nProof. Notice that if L < L\u0003\ni,zi(L)<1=2. Using Equa-\ntion (3), we prove the strictly decreasing property: ^q0\ni(L)<\n2\u0000L\u00001lnzi(L)\n1\u0000zi(L)<0. Similarly from zi(L)<1=2, we\nalso have that ^qi(L)<2\u0000Lby Equation (1); therefore, if\nL < miniL\u0003\ni,^qi(L)<2\u0000Lfor alli2[n]. Subsequently,\nthe expected false positive is also exponentially decreasing\n^F(L) =Pn\ni=1ci^qi(L)<Pn\ni=1ci2\u0000L\u0014n2\u0000L.\nRemark. The region [1;Lmin]covers a wide range of F0.\nEven in the worst case where ci= 1, the region covers the\nexpected number of false positives down as low as n2\u0000Lmin=\nn2\u0000B\nmaxijWijln 2, that is,B\u00151\nln 2\u0002maxijWij\u0002log2n\nF0surely\nenables fast optimization via binary search on the strictly\ndecreasing function. Nonetheless, alg. 1 measures a tighter\nexpected false positive lower bound of the region F(Lmin)to\ndecide whether to use fast optimization.\nLemma 3. ForL>maxiL\u0003\ni=Lmax, expected false positive\nis strictly increasing ^f(L)>0.\nProof. IfL>L\u0003\ni,zi(L)>1=2. Together with Equation (3), it\nimplies that ^q0\ni(L)>2\u0000L\u00001lnzi(L)\n1\u0000zi(L)>0.\n\nd) False Positive Guarantee: For ﬁxed numbers of bins\nBand layers L, each false positive from i-th document on\nirrelevant query word wis a multiple of Bernoulli random\nvariable, i.e. xi;w=pwbiwherebi\u0018Bern (qi(L)). Since\nxi;w2[0;pw]andE[xi;w] =pwqi(L), Hoeffding’s inequal-\nity guarantees that the observed number of false positives\nX=Pn\ni=1P\nw2WnWixi;wdoes not deviate more than the\nexpectation E[X] =F(L)by\"with probability at least 1\u0000\u000e.\n\u000e= Pr[X\u0015F(L) +\"]\u0014exp\u0010\n\u00002\"2=\u001b2\nX\u0011\n(5)\nwhere\u001b2\nX=Pn\ni=1P\nw2WnWip2w. Thus, the deviation is\nbounded with \"\u0014q\n1\n2\u001b2\nXln1\n\u000e. In the worst case where very\nfew query words are irrelevant to all documents and dominate\nthe distribution ( pw!1) making\u001b2\nX!n, the deviation can\npossibly be as large as \"\u0014q\nn\n2ln1\n\u000e=O(pn); however,\na memoization technique such as query caching sufﬁces to\nsolve this case. It is worth noting that, in a typical case when\nthere are many irrelevant query words with similar probability,\nthe deviation would instead shrink as the number of words\nincreases:\"\u0014qn\n2jWjln1\n\u000e=O\u0012qn\njWj\u0013\n.\nAs a point of reference, Table II summarizes corpus-\ndependent coefﬁcients \u001bXfor each corpus in the experiment\nassuming query words distribute uniformly among relevant\nwords found in corpus. To reiterate, this analysis only ensures\nthe concentration of the observed number of false positives\naroundF(L). The number of false positives itself can be\nreduced by adjusting IoU Sketch’s structure (B;L).\nB. Corpus Proﬁling\nAlthough the optimization formulation factors in any cat-\negorical query word distribution w, A IRPHANT assumes a\nuniform distribution by default; in other words, a query equally\nlikely contains words in the corpus or pw= 1=jWj. While no\nfurther evidence support nor deny such choice, it is potentially\nsimplistic. Other possible apparent choices with their proﬁling\nare: (a)pw=occurrences (w)by proﬁling word occurrences\nand total number of words, and (b) user-provided or statistical\npriorpwwith no proﬁling. One might also consider assigning\nnon-zeropw0wherew0=2W. We defer seeking more suitable\ndistributions to future studies and let this implementation be a\ncase study. Subsequently, A IRPHANT Builder’s proﬁling then\ncounts the number of distinct words in the corpus as well as\nthe numbers of distinct words within each document.\nC. Superpost Compaction\nAIRPHANT Builder implements a simple superpost com-\npaction to avoid creating too many tiny or a few huge ﬁles,\nand to allow single-cycle retrievals in A IRPHANT Searcher.\nPreviously hinted in earlier sections, the compaction comprises\nof two components: a header block and superpost blocks.\nEach of the superpost blocks stores serialized multiple\nsuperposts consecutively. A IRPHANT internally uses Protocol\nBuffers to serialize superposts to byte arrays. While indexing,\nAIRPHANT Builder keeps track of each superpost location\nand builds the dictionary of bin pointers. Given the superpostblock structure, each bin pointer need to represent block ID,\noffset, and byte length to retrieve the superpost’s bytes in a\nsingle round-trip. In addition, A IRPHANT compresses repeated\nstrings within postings into integer keys. The compression\nreduces the number of bytes per superpost to be downloaded\nwhich speeds up query overall.\nAIRPHANT Builder persists these bin pointers and string\ncompression table along side with hash seeds and other\nmetadata in the header block. The hash seeds are collected\nfrom the hash function in IoU Sketch; in other words, they\nconcisely represents IoU Sketch mapping. It is this header\nblock that is loaded on A IRPHANT Searcher initialization.\nD. Top-K Query\nInstead of retrieving all relevant documents in a query,\nAIRPHANT Searcher supports fetching at least Krelevant\ndocuments. Top- Kquery enables pagination for providing a\nquick view or batch processing. Thanks to IoU Sketch’s false\npositive guarantee, that is, the approximated superpost contains\nF0irrelevant documents on average, A IRPHANT Searcher can\nsample a subset of the superpost to fetch from. Suppose the\nsuperpost contains Rpostings, if K\u0015R\u0000F0, then A IRPHANT\nSearcher fetches all Rdocuments. Otherwise, each posting\ncorresponds to a relevant document with Bernoulli distribution\nBern (p= 1\u0000F0=R). With probability at least 1\u0000\u000e, solving\na quadratic inequality after applying Hoeffding’s inequality\nguarantees that sampled postings of size RK(Equation (6))\ncomprise of at least Krelevant documents.\nRK=2\n6662pK+1\n2ln1\n\u000e+q\n(2pK+1\n2ln1\n\u000e)2\u00004p2K2\n2p23\n777(6)\nE. Common Words\nCommon words are keywords that are contained in many\ndocuments in the corpus. Some information retrieval systems\nassign them as stop words and ﬁlter them out during all\nretrieval steps. In contrary, A IRPHANT supports searching for\ncommon words. The challenge is that, merging their large post-\nings lists into IoU Sketch’s bins would deteriorate performance\non other keyword query as well. As a workaround, A IRPHANT\nsets aside 1%of the bins to store the exact postings lists of\nmost common words. For example, if B= 105, AIRPHANT\nwould use 99;000bins for IoU Sketch and 1;000bins to carry\n1;000most common words’ postings lists. We use the same\nsuperpost compaction for these postings lists.\nF . Applicable Queries\nAlthough IoU Sketch only natively supports queries on a\nsingle term, we can adapt it to accelerate other classes of\nqueries. For one, like an inverted index, IoU Sketch naturally\ngeneralizes to Boolean queries [32]. Let Q(w)be the superpost\nfrom querying IoU Sketch with word w. IoU Sketch executes\nany Boolean query by distributing its query function to each\nterm predicate Q(W\niV\njwij) =S\niT\njQ(wij). In particular,\nintersection and union operators apply to superposts where the\nformer reduces false positives and the latter adds. Furthermore,\n\nregular expression (RegEx) can beneﬁt from IoU Sketch as\ninverted index by considering indexing N-grams as shown\nin RegEx engines [33] [34]. These engines use an inverted\nindex as a ﬁlter to avoid a full corpus scan, and later match\nthe remaining documents with the RegEx to remove false\npositives. Hence, superpost’s false positives do not affect the\nﬁnal correctness.\nG. Built-in Replication for Reliability\nDuring its query execution, IoU Sketch retrieves superposts\nfrom allLlayers simultaneously in parallel I/O requests. As a\nresult, the slowest retrieval among Lrequests deﬁnes the total\nquery latency, exposing IoU Sketch to the Long Tail Problem if\nthere is any extreme variation during the I/Os such as dormant\nstorage or network congestion [35]. Nonetheless, IoU Sketch’s\nmulti-layer structure comes to the rescue as a built-in replica-\ntion mechanism. The core idea is that IoU Sketch can simply\ndiscard the pending slow request and return the available-but-\nsuboptimal superpost. The simplest mitigation is then to set\na timeout before aborting the trailing request. Alternatively,\nIoU Sketch can enhance its reliability by overestimating the\nnumber layers, say L+, adding magnitudes of accuracy as\na consequence. The more layers IoU Sketch overestimates,\nthe stronger its reliability against long-tail requests becomes.\nDuring a query, IoU Sketch would then submit L+I/O requests\nbut only wait for any Lsuccessful retrievals. We refer to [36]\nfor more sophisticated techniques and analysis frameworks.\nV. E XPERIMENTS\nWe have conducted empirical studies to evaluate A IR-\nPHANT ’s performance. We observe that:\n1. A IRPHANT outperforms Lucene, Elasticsearch, SQLite,\nand HashTable with upto 8:97\u0002,113:39\u0002,3:15\u0002, and\n378:59\u0002faster response respectively (§V-B0a).\n2. A IRPHANT competitively controls the effect of physical\ndistance between compute and storage compared to base-\nlines (§V-B0b).\n3. Our latency analysis shows that baselines are slower due\nto either time spent being blocked waiting for network\nresponse or downloading large amount of data. A IRPHANT\nreduces both portions of time at the same time (§V-B0c).\n4. With the separation of compute and storage, A IRPHANT\nis more cost-efﬁcient than a Elasticsearch deployed with\nlocal persistence when the workload is skewed (high or\nrare peak throughput) and/or the size of indexed data is\nlarge (§V-C).\n5. Our last experiments justify IoU Sketch analysis, showing\nthat IoU Sketch provides a trade-off between memory\nusage and latency and that its structure optimization is\ncrucial. (§V-D)\nA. Environment Setup\nOur experiments used Google Cloud Platform. It uses GCP\nCloud Storage [8] as the cloud storage for index structure\npersistence. To provide ﬁle-system interface for all bench-\nmarks, we connect all necessary storage buckets to a directoryTABLE II: Corpus Statistics. #documents: number of docu-\nments. #terms: number of distinct words. #words: total number\nof words across all documents. \u001bX: corpus-dependent coefﬁ-\ncient discussed in §IV-A.\nCorpus #documents #terms #words \u001bX\ndiag(8,8,0) 1081081081:00\nunif(8,8,1) 1081:0\u00021081:0\u00021091:00\nzipf(8,8,1) 1085:0\u00021079:5\u00021081:41\nCranfield 1:4\u00021035:3\u00021031:2\u00021050:51\nHDFS 1:1\u00021073:6\u00021061:4\u00021081:77\nWindows 1:1\u00021088:3\u00021051:7\u000210911:73\nSpark 3:3\u00021075:2\u00021063:5\u00021082:53\nusing Cloud Storage FUSE [37] ( gcsfuse ) adaptor with no\nlimit on rate of operations. We allocate two VM instances\nn2-highmem-32 (32 vCPUs, 256 GB memory, 1 TB SSD\npersistent disk) and e2-small (2 vCPUs, 2 GB memory, 10\nGB default boot disk) for indexing and query benchmarking\nrespectively. All experiments run from September to Novem-\nber 2021.\na) Datasets: There are 4 corpuses to benchmark these\nindex systems. Cranfield (Cranﬁeld 1400) [38] is a small\ncorpus of 1398 documents, each contains abstracts from aero-\ndynamics research papers. HDFS [39],Windows , and Spark\nare system logs in their corresponding systems collected by\nLoghub [40].\nIn addition, we use 3 types of synthetic datasets whose size\nis conﬁgurable. We denote the size of each synthetic dataset\nusing a tuple (log10nd;log10nw;log10nl)for its numbers of\ndocuments nd, wordsnw, and words per documents nl.diag\nis a dataset where each document icontains only one word\nwi. As a result, diag always has nl= 1.unif is a dataset\nwhere each word in a document is uniformly sampled from\nthenw-word dictionary. zipf is similar to unif but uses\na Zipﬁan distribution with the exponent equal to 1:07. In\nother word, each word in a document is equal to wjwith\nprobability proportional to 1=j1:07. Note that unif andzipf\ncan under-generate the actual set of distinct words form nw\ndue to Coupon collector’s problem [41]. Table II summarizes\ncorpuses’ statistics that are used in §V-B0a.\nb) Baselines: We compare A IRPHANT to Lucene\n7.4.0 [19], Elasticsearch 7.15.1 [20], SQLite 3.34.0 [21], and\nna¨ıve hash table. Lucene is an information retrieval library.\nElasticsearch is a search and analytics engine. We benchmark\ntheir efﬁciency in matching exact keywords. We ﬁrst parse\nkeywords out similarly to other baselines and feed parsed\ndocuments to a text ﬁeld using Elasticsearch’s whitespace\nanalyzer and Lucene’s WhitespaceAnalyzer along with\nthe document’s posting. To benchmark Elasticsearch, we\nmount a Searchable Snapshot [42] onto an Elasticsearch empty\ninstance to speed up its initial latency.\nSQLite is a light database we choose as a practical B-tree\nimplementation. We ﬁrst create a two-column table consisting\nof keyword column and postings column to mimic the inverted\nindex dictionary. We then build SQLite’s B-tree index [43] on\n\ndiag(8,8,0) uniform(8,8,1) zipf(8,8,1) Cranfield HDFS Windows Spark02004006008001;000\n2696\n1306\n3885\n22071703\n34204\n3178\n1655\n16972864\n113871\n36396\n6781\n3555Search Latency (ms)Lucene\nElasticsearch\nSQLite\nHashTable\nAIRPHANT\nFig. 6: End-to-end search latencies of indexes on different datasets. Solid bars show average latencies; the upper error bars\nshow 99th percentiles. Latencies over 1.1 seconds are truncated and labeled. 99 percentiles whose means are over the limit\nare omitted.\nthe keyword column as its term index and store its database\nﬁle on the cloud-mounted directory. In each query, after\nretrieving the postings, SQLite reuses the same document\nretrieval routine from A IRPHANT .\nLastly, HashTable refers to an inverted index that stores\npostings lists according to their corresponding terms’ hashes.\nIt is equivalent to IoU Sketch with the only exception that\nit has a single layer L= 1. Other relevant conﬁgurations\nsuch as the total number of bins and common word bins are\nidentical to IoU Sketch. All postings inserted in all baselines\nare compressed in the same way as in A IRPHANT (§IV-C).\nc) Parameters: If otherwise speciﬁed, we set the number\nof binsB= 105, accuracy constraint F0= 1, and probability\nof top-Kquery failure \u000e= 10\u00006forK= 10 . Remind\nthat we allocate 1% of the bins to store postings lists of\nmost common words. We similarly set top- Kquery to other\nbaselines accordingly. The top- Kquery failure never occurs\nduring the experiment due to the conservative setting which\nselects about 23samples to answer top- 10query. The accuracy\nchoice leads to optimal number of layers L\u0003in the at most 3\nlayers depending on the corpus. The number of bins results\nin A IRPHANT Search’s runtime size about 2MB. We use 32\nthreads to concurrently read any data from the cloud storage.\nB. End-to-end Search Performance\na) Within Region: To demonstrate A IRPHANT ’s perfor-\nmance, we measure end-to-end search latency for each query.\nFigure 6 shows mean and 99th-percentile latencies. A IRPHANT\nquery executions are 1:45\u0002to8:97\u0002faster than Lucene’s on\naverage (except Cranfield dataset where Lucene is 8:00\u0002\nfaster), 1:09\u0002to113:39\u0002faster than Elasticsearch’s, 1:12\u0002to\n3:15\u0002faster than SQLite’s, and 1:15\u0002to378:59\u0002faster than\nHashTable’s. HashTable is slow because it spends the majority\nof its latency to ﬁlter out false-positive documents, which\nhighlights the strength of IoU Sketch’s multi-layer structure.\nFrom these benchmarks, we see that A IRPHANT operates\nat less than 300 ms on average. According to [44], [45], web\nsearch users start to notice the latency when it is on the order\nof seconds, and so, they are tolerable with A IRPHANT latency.\nTo put its latency into a perspective, A IRPHANT would only\nadd a factor to the 250 ms “speed-of-light web search latency”.\nb) Cross Region: Separation of compute and storage\nallows A IRPHANT to host each component in different phys-\nical locations, even across continents. To construct such aus-central1-c europe-west2-casia-southeast1-b01;0002;0003;0004;000\n14117\n3743116474\n16817\n146886781\n7817\n10545Search Latency (ms)Lucene\nElasticsearch\nSQLite\nHashTable\nAIRPHANT\nFig. 7: End-to-end search latencies across regions of indexes.\nTheWindows dataset was used. Solid bars show average\nlatencies; the upper error bars show 99th percentiles. of\nmeasured latencies.\n0 500 1;000 1;500 2;000Lucene\nElasticsearch\nSQLite\nHashTable\nAIRPHANT\nLatency (ms)Wait Time\nDownload Time\nFig. 8: Search latency breakdown in terms of network com-\nmunication on the Spark dataset. The total search latency is\nequal to the summation of wait time and download time.\nscenario, we allocate a cloud storage in the default US(multi-\nregion US), while hosting VMs at us-central1-c (Iowa),\neurope-west2-c (London), and asia-southeast1-b\n(Singapore). Because we observe similar performance patterns\nacross all datasets, Figure 7 only presents Windows as a\nrepresentative. It comes as no surprise that each method\ntakes longer as its VM moves further from the data. Among\nthose baselines with competitive latencies, Lucene is 3:3\u0002and\n8:2\u0002slower and SQLite is 3:2\u0002and8:0\u0002slower in London\nand Singapore. In contrast, A IRPHANT achieves a milder\nslowdown: 2:4\u0002and6:5\u0002(3:3\u0002and6:8\u0002across all datasets,\nnot shown). Elasticsearch and HashTable, on the other hand,\nare consistently slower than others, across different regions:\nElasticsearch spends much time in mounting its searchable\nsnapshots; HashTable reads too many false positives.\nc) Latency Breakdown: To dig deeper, we study data\nmovement patterns from the perspective of network commu-\nnication via TCP packets captured by tcpdump6. We sample\n6https://www.tcpdump.org\n\n32 queries from each method, capture packets on appropriate\nports. We then compute two metrics per query: waiting time\nand download time.7We calculate bandwidth usage based\non moving averages of the size of packets with window\nsize of 10ms. Figure 8 depicts the result of this procedure.\nNote that the latency is slower than those in Figure 6 partly\ndue to tcpdump and a smaller number of query samples.\nThere are two opposite patterns among baselines. First, as\nwe have mentioned, HashTable’s latency dominantly consists\nof time to download false-positive documents. Secondly and\nconversely, Lucene and SQLite tend to wait for their dependent\nreads, i.e. B-tree or skip list traversal. A IRPHANT minimizes\nsuch dependency thanks to the independence among IoU\nSketch’s layers, reducing wait time as a result. A IRPHANT\nalso improves download time because of its parallel I/O reads\nwhich utilizes more network bandwidth. A IRPHANT spends\n220ms waiting and 117ms downloading on average where its\nbreakdown distribution concentrates around.\nC. Cost Comparison\nThis experiment compares costs between the two architec-\nture paradigms: coupled (compute and storage adjacency) and\ndecoupled (compute and storage separation). To this end, we\nconsider the peak-trough workload that elucidates the trade-\noff between the two paradigms. The peak-trough workload is\nmotivated by the periodically varying utilization commonly\nfound in web services. As the name suggests, it consists of\ntwo parts: peak and trough. Peak refers to the time where\nthe workload is higher, whereas trough’s workload is lower.\nBecause our following cost formulae are linear on the amount\nof time, we can identify a peak-trough instance with (A;a;\u001c )\nwhere peak and trough cover \u001cand1\u0000\u001cfraction of time with\nworkloadAandaops/s respectively.\nFurthermore, we also consider the total number of corpora.\nFor simplicity, we consider a collection of corpora whose\ndocument-word pairs are distributed similarly to Windows .\nWe denote Sas the total size of original data in bytes. Esti-\nmated from their storage usage in Windows , AIRPHANT uses\n1:008\u0002Sbytes, while Elasticsearch has a better compression\nrate and only uses 0:3316\u0002Sbytes of storage.\nSince A IRPHANT can scale up/down easily as per work-\nloads, A IRPHANT costs proportionally to the workload over\ntime:O(apt+at(1\u0000t)). From previous performance results,\nAIRPHANT operates at 175ms/op or 5:71ops/s. Deploying\nAIRPHANT one2-small VM costs $13.23/month, and stor-\ning its index on GCP Cloud Storage costs $0.02/GB/month.\nIn contrast, Elasticsearch cannot automatically scale down\nwithout rebalancing its index over remaining servers; as a re-\nsult, it requires the resource to handle the peak workload at all\ntimes, amounting to O(ap)cost. Nonetheless, we optimistically\nassume that Elasticsearch’s sharding and load balancing are\nperfect such that its throughput scales linearly with the number\nof servers without shard replication. We deploy Elasticsearch\n7The former includes the time when the bandwidth usage is less than 10\nkB/s while the latter includes the rest where the trafﬁc is high.0% 20% 40% 60% 80% 100%1/41/2124\nFraction of Peak Time \u001cRelative Cost CE=CASizeN\n16TB\n8TB\n4TB\n2TB\n1TB\nFig. 9: Relative cost between local Elasticsearch and cloud-\nstored A IRPHANT in terms of fraction of peak time \u001cand size\nof indexed data N(increasing from bottom to top lines). Peak\nand trough workloads are ﬁxed to A= 154:08op/s (throughput\nof a single Elasticsearch server) and a=A=20 = 7:704op/s.\none2-medium VM ($26.46/month) and store its index on\nlocal disk ($0.2/GB/month). This achieves 6.49 ms/op and is\nthe most cost-efﬁcient among the options we explored.\nFigure 9 shows a slice of the relative cost CE=CAat ﬁxedA\nanda. Overall, A IRPHANT costs less if the total corpus size N\nis larger and/or the fraction of peak time \u001cis smaller. In fact,\nwe would asymptotically save limN!1CE=CA\u00193:29times\nof the cost of coupled Elasticsearch. Also, focusing on the\nVM cost, A IRPHANT ’s cost would be A=(13:48a)times over\nElasticsearch’s. A relatively higher peak workload A> 13:48a\nimplies that a relatively lower cost to deploy A IRPHANT , and\nvice versa. These trends align with the intuition: the decouple\nparadigm is more cost-efﬁcient than coupled one when 1) the\ndata is large, 2) the workload is concentrated over peak times,\nand/or 3) the peak’s workload is high compared to trough’s.\nD. IoU Sketch Structure\nWe use HDFS to exploreB2f50k;100k;200k;400kgaround\nthe conﬁguration used in §V-B0a and a wide range of L2\nf1;2;4;6;8;:::; 16g. Recall that L= 1corresponds to the na ¨ıve\nhash table index structure and note that our optimizer selects\nL\u0003= 2. Figure 10a conﬁrms our analysis which predicts a\nrapid error reduction over Lgiven a sufﬁcient B; the observed\naverage numbers of false positives are enormous at L= 1,\nbecome less than 1atL= 2, and are exactly zero after L=\n4. Such high numbers of false positives in L= 1 result in\na higher search latency (Figure 10b) due to heavy ﬁltering\ndemands. The rest of the search latency trend correlates closely\nwith the term lookup latency (Figure 10c); when Lincreases,\nso does the lookup latency. Even though A IRPHANT fetches\nsuperposts in layers separately, a bandwidth contention drives\nup the lookup latency and search latency as a result.\nVI. R ELATED WORK\nA. Data Systems in the Cloud\nCloud services provide a convenient interface to share\ncomputing resources. As one of its most important impact,\nthe cost of scaling up or down is now low enough that\nelasticity, or ability to scale to meet the demand, is a desirable\nfeature of a system. Some work achieves elasticity through\nseparation of compute and storage. Particularly, applications\nthat are functional or can be partitioned into smaller functional\n\n1 4 8 12 160200400600\nNumber of Layers LFalse Positives\n(a)Expected False Positives1 4 8 12 160100200300400\nNumber of Layers LSearch Latency\n(ms)\n(b)Average Search Latency1 4 8 12 16050100150200\nNumber of Layers LLookup Latency\n(ms)\n(c)Average Term Lookup Latency50k\n100k\n200k\n400k\nFig. 10: Effects of numbers of bins Band layers Lon expected number of false positives (in logarithmic scale),\naverage search latency, and average term lookup latency, from left to right. Each line corresponds to a number of bins\nB2f50k;100k;200k;400kg, while x-axis corresponds to the number of layers L. Average search latency (not shown) for\n(B= 50k;L= 1) is3;556ms. The underlying corpus is HDFS .\nmodules enjoy elasticity by FaaS [46]. Apart from web serv-\ning and ﬁle processing, ggframework [47] empowers FaaS\nexecution for commonly local applications, for example, video\nencoding, object recognition, unit testing. Towards FaaS-based\nquery engine, Starling [48] partitions a query into ﬁne-grained\ntasks on operation level such as table scanning, joining, and\nshufﬂing. A IRPHANT partitions a functional searching into\nAIRPHANT Searcher, and persists index data structure stored\non cloud storage. In this way, A IRPHANT can also beneﬁt from\nFaaS elasticity.\nB. Index Data Structures\na) Sketches: [49] lays out numerous sketch techniques.\nMost notably similar sketches to IoU Sketch sketches are\ncount-min sketch [50] and Bloom ﬁlter [51]. IoU Sketch and\ncount-min sketch both maintain a multi-layer hash table and\nanswer queries using aggregations. Bloom ﬁlter’s accuracy\nformulation is similar to IoU Sketch’s counterpart; however,\nIoU Sketch focuses on maximizing its retrieval performance\nusing its accuracy term as one of the constraints while Bloom\nﬁlter directly maximizes its accuracy. Moreover, Bloom ﬁlter\nhas a closed-form optimal conﬁguration, but IoU Sketch’s can\ncontain multiple local optima.\nb) Hash Tables: Open addressing probes for available\nentries to insert colliding values accordingly to a mechanism,\nfor example, Hopscotch [52], SmartCuckoo [53], and Min-\nCounter [54]. Open addressing performs well when load factor\nis low [55], which is infeasible with a memory constraint.\nUnlike B-trees, hash table is popular in distributed peer-\nto-peer systems, especially for point-query indexing. Each\nsolution differs from others based on its query routing [56],\nsuch as tree-like (Pastry [57], Tapestry [58], Kademlia [59]) or\nskip-list-like (Chord [60], Koorde [61]). A keyword searching\napplication can rely on these systems to store and retrieve\npostings lists; however, it would require long-running servers\nequipped with storage, as opposed to our serverless system.\nc) Hierarchical Indexes: B-tree [12] is widely used due\nto its efﬁciency, self-balancing, and cheap reorganization [62].\nMany B-tree variants and techniques optimize B-tree across\nmemory levels and storage mediums, e.g. CPU cache [63],\nSRAM cache [14], disk [13], SSD [64], NVMe [65], and\ndistributed systems [66]–[68].Skip list is another index structure based on linked list\nwith skip connections. Like B-trees, many skip lists are\noptimized to various settings, such as multi-core [69], cache-\nsensitive for range queries [70], non-uniform access [71], and\ndistributed nodes [72]. Skip list can be combined with other\ndata structures as well: hash table [60], [61], search tree [73],\nmulti-dimensional octree [74], and compressed perfect skip\nlist for inverted index [75].\nHowever, these hierarchical index structure can be inefﬁ-\ncient in our cloud setting when corpus size is large. Although\n[76] suggests increasing the effective fan-out, a server with\nlimited memory still requires multiple sequential round-trips.\nA workaround is to increase the server’s memory capacity,\nthereby incurring extra costs. A IRPHANT achieves single\nround-trip per query even on a memory-constrained node.\nC. Approximation Techniques for Data Systems\nBesides sketching (§VI-B), approximation and learning-\nbased techniques have been proposed to speed up large-\nscale data analytics [77]–[83], selectivity estimation [84]–\n[87], etc. and to automate database tuning [88]–[90], etc. In\ncontrast, A IRPHANT develops a novel statistical inverted index\nfor document stores.\nVII. C ONCLUSION\nThis work presents A IRPHANT , a search engine developed\nto achieve low end-to-end query latencies under the separation\nof compute and storage. While A IRPHANT keeps almost the\nentire data on cloud storage (both inverted index and original\ndocuments), it delivers search results within 300 milliseconds.\nThis is a signiﬁcant improvement compared to placing existing\nsearch engines directly on top of cloud storage. At its core,\nAIRPHANT relies on our new statistical inverted indexing\ntechnique called IoU Sketch. Unlike many other existing\nindexes, IoU Sketch does not require sequential back-to-\nback communications with storage devices. With IoU Sketch,\nwe can make a single batch of concurrent communications,\nwhich signiﬁcantly lowers the end-to-end wall clock time in\nobtaining relevant documents (i.e., the documents containing\nsearch keywords). We plan to open-source this technology\n(and A IRPHANT itself) after enhancing its index building\ncomponent (for higher scalability) and also making it more\nextensible for diverse public cloud environments.\n\nVIII. A CKNOWLEDGEMENT\nThis work is supported in part by Microsoft Azure and\nGoogle Cloud Platform.\nREFERENCES\n[1] B. Dageville, T. Cruanes, M. Zukowski, V . Antonov, A. Avanes,\nJ. Bock, J. Claybaugh, D. Engovatov, M. Hentschel, J. Huang et al. ,\n“The snowﬂake elastic data warehouse,” in Proceedings of the 2016\nInternational Conference on Management of Data , 2016, pp. 215–226.\n[2] G. Cloud, “BigQuery: Cloud data warehouse,” https://cloud.google.com/\nbigquery, [Online; accessed April-14-2021].\n[3] A. W. Services, “Amazon Redshift: Cloud data warehouse,”\nhttps://aws.amazon.com/redshift/?whats-new-cards.sort-by=item.\nadditionalFields.postDateTime&whats-new-cards.sort-order=desc,\n[Online; accessed April-14-2021].\n[4] PrestoDB, “Distributed sql query engine for big data,” https://prestodb.\nio/, [Online; accessed April-14-2021].\n[5] Trino, “Distributed sql query engine for big data,” https://trino.io/,\n[Online; accessed April-14-2021].\n[6] A. W. Services, “Cloud object storage,” https://aws.amazon.com/s3/,\n[Online; accessed April-14-2021].\n[7] M. Azure, “Azure blob storage,” https://azure.microsoft.com/en-us/\nservices/storage/blobs/, [Online; accessed April-14-2021].\n[8] G. Cloud, “Cloud storage,” https://cloud.google.com/storage, [Online;\naccessed April-14-2021].\n[9] C. B. Paul R. La Monica, “Snowﬂake shares more than double. it’s the\nbiggest software ipo ever,” https://www.cnn.com/2020/09/16/investing/\nsnowﬂake-ipo/index.html, [Online; accessed May-01-2021].\n[10] A. AWS, “Amazon redshift introduces ra3 nodes,”\nhttps://aws.amazon.com/about-aws/whats-new/2019/12/\namazon-redshift-announces-ra3-nodes-managed-storage/, [Online;\naccessed April-14-2021].\n[11] W. Pugh, “Skip lists: a probabilistic alternative to balanced trees,”\nCommunications of the ACM , vol. 33, no. 6, pp. 668–676, 1990.\n[12] R. Bayer and E. M. McCreight, “Organization and maintenance of large\nordered indexes,” Acta Informatica , vol. 1, pp. 173–189, 1972.\n[13] S. Chen, P. B. Gibbons, T. C. Mowry, and G. Valentin, “Fractal\nprefetching b+-trees: Optimizing both cache and disk performance,”\n2002.\n[14] S. Chen, P. B. Gibbons, and T. C. Mowry, “Improving index performance\nthrough prefetching,” SIGMOD Record (ACM Special Interest Group on\nManagement of Data) , vol. 30, 2001.\n[15] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” 2018.\n[16] J. Ding, U. F. Minhas, J. Yu, C. Wang, J. Do, Y . Li, H. Zhang,\nB. Chandramouli, J. Gehrke, D. Kossmann, D. Lomet, and T. Kraska,\n“Alex: An updatable adaptive learned index,” 2020.\n[17] A. Białecki, R. Muir, G. Ingersoll, and L. Imagination, “Apache lucene\n4,” in SIGIR 2012 workshop on open source information retrieval , 2012,\np. 17.\n[18] ElasticSearch, “S3 repository plugin,” https://www.elastic.co/guide/\nen/elasticsearch/plugins/current/repository-s3.html, [Online; accessed\nApril-14-2021].\n[19] A. Lucene, “Apache lucene,” https://lucene.apache.org, [Online; ac-\ncessed April-24-2021].\n[20] ElasticSearch, “Free and open search: The creators of elasticsearch, elk\n& kibana,” https://www.elastic.co/, [Online; accessed April-14-2021].\n[21] SQLite, “SQLite,” https://www.sqlite.org, [Online; accessed April-24-\n2021].\n[22] H. Sch ¨utze, C. D. Manning, and P. Raghavan, Introduction to informa-\ntion retrieval . Cambridge University Press Cambridge, 2008, vol. 39.\n[23] A. Lucene, “Package org.apache.lucene.codecs,” https://lucene.apache.\norg/core/8 10/core/org/apache/lucene/codecs/package-summary.html,\n[Online; accessed May-01-2021].\n[24] P. Kirschenhofer, C. Mart ´ınez, and H. Prodinger, “Analysis of an opti-\nmized search algorithm for skip lists,” Theoretical Computer Science ,\nvol. 144, no. 1-2, pp. 199–220, 1995.\n[25] D. Smiley, E. Pugh, K. Parisa, and M. Mitchell, Apache Solr enterprise\nsearch server . Packt Publishing Ltd, 2015.\n[26] G. Cloud, “Http headers and query string parameters for xml api,” https:\n//cloud.google.com/storage/docs/xml-api/reference-headers#range, [On-\nline; accessed April-20-2021].[27] A. W. Service, “Use byte-range fetches,” https://docs.aws.amazon.\ncom/whitepapers/latest/s3-optimizing-performance-best-practices/\nuse-byte-range-fetches.html, [Online; accessed April-20-2021].\n[28] M. Azure, “Specifying the range header for blob service\noperations,” https://docs.microsoft.com/en-us/rest/api/storageservices/\nspecifying-the-range-header-for-blob-service-operations, [Online;\naccessed April-20-2021].\n[29] G. Cloud, “Cloud functions,” https://cloud.google.com/functions, [On-\nline; accessed April-20-2021].\n[30] A. W. Service, “Aws lambda,” https://aws.amazon.com/lambda/, [Online;\naccessed April-20-2021].\n[31] M. Azure, “Azure functions,” https://azure.microsoft.com/en-us/services/\nfunctions/, [Online; accessed April-24-2021].\n[32] C. D. Manning, P. Raghavan, and H. Sch ¨utze, Introduction to Informa-\ntion Retrieval . USA: Cambridge University Press, 2008.\n[33] T. Qiu, X. Yang, B. Wang, and W. Wang, “Efﬁcient regular expression\nmatching based on positional inverted index,” IEEE Transactions on\nKnowledge and Data Engineering , pp. 1–1, 2020.\n[34] J. Cho and S. Rajagopalan, “A fast regular expression indexing engine,”\ninProceedings 18th International Conference on Data Engineering ,\n2002, pp. 419–430.\n[35] S. S. Gill, X. Ouyang, and P. Garraghan, “Tails in the cloud: a survey\nand taxonomy of straggler management within large-scale cloud data\ncentres,” The Journal of Supercomputing , pp. 1–40, 2020.\n[36] D. Wang, G. Joshi, and G. W. Wornell, “Efﬁcient straggler replication\nin large-scale parallel computing,” ACM Transactions on Modeling and\nPerformance Evaluation of Computing Systems (TOMPECS) , vol. 4, pp.\n1 – 23, 2019.\n[37] G. Cloud, “Cloud storage fuse,” https://cloud.google.com/storage/docs/\ngcs-fuse, [Online; accessed April-14-2021].\n[38] C. Cleverdon, “The cranﬁeld tests on index language devices,” 1967.\n[39] W. Xu, L. Huang, A. Fox, D. Patterson, and M. I. Jordan, “Detecting\nlarge-scale system problems by mining console logs,” 2009.\n[40] S. He, J. Zhu, P. He, and M. R. Lyu, “Loghub: A large collection of\nsystem log datasets towards automated log analytics,” 2020.\n[41] P. Flajolet, D. Gardy, and L. Thimonier, “Birthday paradox,\ncoupon collectors, caching algorithms and self-organizing search,”\nDiscrete Applied Mathematics , vol. 39, no. 3, pp. 207–229, 1992.\n[Online]. Available: https://www.sciencedirect.com/science/article/pii/\n0166218X9290177C\n[42] ElasticSearch, “Elasticsearch searchable snapshots,” https://www.elastic.\nco/elasticsearch/elasticsearch-searchable-snapshots/, [Online; accessed\nOctober-30-2021].\n[43] SQLite, “Database ﬁle format,” https://www.sqlite.org/ﬁleformat.html,\n[Online; accessed April-20-2021].\n[44] J. D. Brutlag, H. B. Hutchinson, and M. Stone, “User preference and\nsearch engine latency,” 2008.\n[45] I. Arapakis, S. Park, and M. Pielot, “Impact of response latency on user\nbehaviour in mobile web search,” Proceedings of the 2021 Conference\non Human Information Interaction and Retrieval , 2021.\n[46] J. Schleier-Smith, “Serverless foundations for elastic database systems,”\n2019.\n[47] S. Fouladi, F. Romero, D. Iter, Q. Li, S. Chatterjee, C. Kozyrakis,\nM. Zaharia, and K. Winstein, “From laptop to lambda: Outsourcing\neveryday jobs to thousands of transient functional containers,” 2019.\n[48] M. Perron, R. C. Fernandez, D. Dewitt, and S. Madden, “Starling: A\nscalable query engine on cloud functions,” 2020.\n[49] G. Cormode, “Sketch techniques for approximate query processing,”\ninSynposes for Approximate Query Processing: Samples, Histograms,\nWavelets and Sketches, Foundations and Trends in Databases. NOW\npublishers , 2011.\n[50] G. Cormode and S. Muthukrishnan, “An improved data stream summary:\nThe count-min sketch and its applications,” Journal of Algorithms ,\nvol. 55, 2005.\n[51] B. H. Bloom, “Space/time trade-offs in hash coding with allowable\nerrors,” Communications of the ACM , vol. 13, 1970.\n[52] M. Herlihy, N. Shavit, and M. Tzafrir, “Hopscotch hashing,” vol. 5218\nLNCS, 2008.\n[53] Y . Sun, Y . Hua, S. Jiang, Q. Li, S. Cao, and P. Zuo, “Smartcuckoo: A\nfast and cost-efﬁcient hashing index scheme for cloud storage systems,”\n2019.\n[54] Y . Sun, Y . Hua, D. Feng, L. Yang, P. Zuo, and S. Cao, “Mincounter:\nAn efﬁcient cuckoo hashing scheme for cloud storage systems,” vol.\n2015-August, 2015.\n\n[55] S. Richter, V . Alvarez, and J. Dittrich, “A seven-dimensional analysis of\nhashing methods and its implications on query processing,” 2016.\n[56] H. Balakrishnan, M. F. Kaashoek, D. Karger, R. Morris, and I. Stoica,\n“Looking up data in p2p systems,” 2003.\n[57] A. Rowstron and P. Druschel, “Storage management and caching in past,\na large-scale, persistent peer-to-peer storage utility,” Operating Systems\nReview (ACM) , vol. 35, 2001.\n[58] K. Hildrum, J. D. Kubiatowicz, S. Rao, and B. Y . Zhao, “Distributed\nobject location in a dynamic network,” 2002.\n[59] P. Maymounkov and D. Mazi `eres, “Kademlia: A peer-to-peer informa-\ntion system based on the xor metric,” Lecture Notes in Computer Science\n(including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture\nNotes in Bioinformatics) , vol. 2429, 2002.\n[60] I. Stoica, R. Morris, D. Karger, M. F. Kaashoek, and H. Balakrishnan,\n“Chord: A scalable peer-to-peer lookup service for internet applications,”\nvol. 31, 2001.\n[61] M. F. Kaashoek and D. R. Karger, “Koorde: A simple degree-optimal\ndistributed hash table,” Lecture Notes in Computer Science (including\nsubseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in\nBioinformatics) , vol. 2735, 2003.\n[62] D. Comer, “Ubiquitous b-tree.” Comput Surv , vol. 11, 1979.\n[63] G. Graefe and P. ˚Ake Larson, “B-tree indexes and cpu caches,” Pro-\nceedings - International Conference on Data Engineering , 2001.\n[64] R. Jin, S. J. Kwon, and T. S. Chung, “Flashb-tree: A novel b-tree index\nscheme for solid state drives,” 2011.\n[65] L. Wang, Z. Zhang, B. He, and Z. Zhang, “Pa-tree: Polled-mode\nasynchronous b+ tree for nvme,” vol. 2020-April, 2020.\n[66] S. Wu, D. Jiang, B. C. Ooi, and K. Wu, “Efﬁcient btree based indexing\nfor cloud data processing,” Proceedings of the VLDB Endowment , vol. 3,\n2010.\n[67] W. Zhou, J. Lu, Z. Luan, S. Wang, G. Xue, and S. Yao, “Snb-index: A\nskipnet and b+ tree based auxiliary cloud index,” Cluster Computing ,\nvol. 17, 2014.\n[68] H. Bin and P. Yuxing, “An efﬁcient distributed b-tree index method\nin cloud computing,” Open Cybernetics and Systemics Journal , vol. 8,\n2014.\n[69] I. Dick, A. Fekete, and V . Gramoli, “A skip list for multicore,” Concur-\nrency Computation , vol. 29, 2017.\n[70] S. Sprenger, S. Zeuch, and U. Leser, “Cache-sensitive skip list: Efﬁcient\nrange queries on modern cpus,” vol. 10195 LNCS, 2017.\n[71] H. Daly, A. Hassan, M. F. Spear, and R. Palmieri, “Numask: High\nperformance scalable skip list for numa,” vol. 121, 2018.\n[72] J. He, S. wen Yao, L. Cai, and W. Zhou, “Slc-index: A scalable skip\nlist-based index for cloud data processing,” Journal of Central South\nUniversity , vol. 25, 2018.\n[73] J. Zhang, S. Wu, Z. Tan, G. Chen, Z. Cheng, W. Cao, Y . Gao, and\nX. Feng, “S3: A scalable in-memory skip-list index for key-value store,”\nvol. 12, 2018.\n[74] “Dynamic multidimensional index for large-scale cloud data,” Journal\nof Cloud Computing , vol. 5, 2016.\n[75] P. Boldi and S. Vigna, “Compressed perfect embedded skip lists for\nquick inverted-index lookups,” vol. 3772 LNCS, 2005.\n[76] G. Graefe, “Modern b-tree techniques,” Foundations and Trends in\nDatabases , vol. 3, 2010.\n[77] Y . Park, A. S. Tajik, M. Cafarella, and B. Mozafari, “Database learning:\nToward a database that becomes smarter every time,” in Proceedings of\nthe 2017 ACM International Conference on Management of Data , 2017,\npp. 587–602.\n[78] T. Kraska, M. Alizadeh, A. Beutel, H. Chi, A. Kristo, G. Leclerc,\nS. Madden, H. Mao, and V . Nathan, “Sagedb: A learned database\nsystem,” in CIDR , 2019.\n[79] Y . Park, B. Mozafari, J. Sorenson, and J. Wang, “Verdictdb: Univer-\nsalizing approximate query processing,” in Proceedings of the 2018\nInternational Conference on Management of Data , 2018, pp. 1461–1476.\n[80] W. He, Y . Park, I. Hanaﬁ, J. Yatvitskiy, and B. Mozafari, “Demonstration\nof verdictdb, the platform-independent aqp system,” in Proceedings of\nthe 2018 International Conference on Management of Data , 2018, pp.\n1665–1668.\n[81] J. Bater, Y . Park, X. He, X. Wang, and J. Rogers, “Saqe: practical\nprivacy-preserving approximate query processing for data federations,”\nProceedings of the VLDB Endowment , vol. 13, no. 12, pp. 2691–2705,\n2020.[82] Y . Park, J. Qing, X. Shen, and B. Mozafari, “Blinkml: Efﬁcient max-\nimum likelihood estimation with probabilistic guarantees,” in Proceed-\nings of the 2019 International Conference on Management of Data ,\n2019, pp. 1135–1152.\n[83] Y . Park, M. Cafarella, and B. Mozafari, “Visualization-aware sampling\nfor very large databases,” in 2016 IEEE 32nd International Conference\non Data Engineering (ICDE) . IEEE, 2016, pp. 755–766.\n[84] Z. Yang, E. Liang, A. Kamsetty, C. Wu, Y . Duan, X. Chen, P. Abbeel,\nJ. M. Hellerstein, S. Krishnan, and I. Stoica, “Deep unsupervised\ncardinality estimation,” Proceedings of the VLDB Endowment , vol. 13,\nno. 3.\n[85] R. Marcus, P. Negi, H. Mao, C. Zhang, M. Alizadeh, T. Kraska,\nO. Papaemmanouil, and N. Tatbul23, “Neo: A learned query optimizer,”\nProceedings of the VLDB Endowment , vol. 12, no. 11.\n[86] Y . Park, S. Zhong, and B. Mozafari, “Quicksel: Quick selectivity learn-\ning with mixture models,” in Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data , 2020, pp. 1017–1033.\n[87] A. Kipf, T. Kipf, B. Radke, V . Leis, P. Boncz, and A. Kemper, “Learned\ncardinalities: Estimating correlated joins with deep learning,” arXiv\npreprint arXiv:1809.00677 , 2018.\n[88] J. Zhang, Y . Liu, K. Zhou, G. Li, Z. Xiao, B. Cheng, J. Xing, Y . Wang,\nT. Cheng, L. Liu et al. , “An end-to-end automatic cloud database tuning\nsystem using deep reinforcement learning,” in Proceedings of the 2019\nInternational Conference on Management of Data , 2019, pp. 415–432.\n[89] D. Van Aken, A. Pavlo, G. J. Gordon, and B. Zhang, “Automatic\ndatabase management system tuning through large-scale machine learn-\ning,” in Proceedings of the 2017 ACM International Conference on\nManagement of Data , 2017, pp. 1009–1024.\n[90] Y . Zhu, J. Liu, M. Guo, Y . Bao, W. Ma, Z. Liu, K. Song, and Y . Yang,\n“Bestconﬁg: tapping the performance potential of systems via automatic\nconﬁguration tuning,” in Proceedings of the 2017 Symposium on Cloud\nComputing , 2017, pp. 338–350.\n\n0 0:5 1 1:5 2 2:5 3 3:5 4 4:5 500:511:522:53\nWait Time (s)Download Time (s)Lucene\nElasticsearch\nSQLite\nHashTable\nAirphant\nFig. 11: Individual search latency breakdown in terms of net-\nwork communication on the Spark dataset. Each scatter mark\nrepresents how long each individual search spends waiting\nand downloading. The total search latency is equal to the\nsummation of wait time and download time. Faster searches\nreside in the lower left corner.\nAPPENDIX A\nADDITIONAL VISUALIZATIONS\nHere we include additional visualizations that we trim in\nthe main sections due to space limitation.\nA. Individual Latency Breakdown\n§V-B0c summarizes search latency breakdown averages.\nFigure 11 instead visualizes latency breakdowns for individual\nsearch queries across systems. Most visibly perhaps is the fact\nthat search latencies vary across queries due to many factors\nsuch as the result size, network variability, or buffer pool\nstate. Nevertheless, the underlying search engine signiﬁcantly\ndictates the breakdown. Overall, A IRPHANT outperforms other\nsystems by minimizing both wait time and download time\nsimultaneously.\nOn a closer look, this scatter plot reveals two extreme\naccess patterns. First, wait-heavy systems (e.g. Lucene) spend\nmost of the search waiting for network responses. The most\nprobable culprit is their dependent sequential reads, that is,\nreads whose locations depend on decisions in preceding reads.\nFor example, skip list traversal requires the current node to ﬁnd\nthe next node to skip to; therefore, to know which block to\nread next, the skip list needs to complete reading the current\nnode ﬁrst. Second, download-heavy systems (e.g. HashTable)\nspends most of the search downloading their data. These\nsystems minimize their number of roundtrips but consume\nmore data. In particular, HashTable (i.e. IoU Sketch where\nL= 1) contains a large number of false positives due to its\nlimited ﬁltering power. As a result, it unnecessarily reads a lot\nmore false-positive documents but avoids roundtrips thanks to\nprefetching.\nB. Full Cross-Region Results\n§V-B0b shows search latency trends as we move further\naway from the cloud storage; however, it only selects thosemeasurements based on Windows dataset as the representa-\ntive. Figure 12 and Figure 13 fully display the measurements\nacross all 7 datasets.\nAPPENDIX B\nADDITIONAL RESULTS\nIn contrary to the main results, we collect the following\nresults from March to April 2021. Some measurements might\ndisagree with those in earlier sections.\nA. Term Index Lookup Performance\nHow does A IRPHANT achieve faster latencies? Recall that\nAIRPHANT and SQLite share the same document retrieval\nroutine, so the difference in their latencies comes from the\ndifferences in both term index lookup operations and amounts\nof retrieved documents. Although A IRPHANT would need\nto retrieve more documents to take into account the false\npositives, its lookup speed gain justiﬁes additional costs.\nFigure 14 reiterates the importance of A IRPHANT ’s single-\nround-trip lookup operation. It evidently outperforms SQLite’s\ncached B-tree traversal both on average and at tail latency.\nIn the best case, A IRPHANT is upto 2:79\u0002faster on average\nand2:81\u0002faster at 99th percentile of term index lookup than\nSQLite.\nB. Scalability with Corpus Size\nWe conduct a scalability experiment where we vary the size\nof synthetic datasets. In particular, we generate 17datasets\nfrom diag ,unif , and zipf by varying both numbers of\ndocuments ndand distinct words nwranging from 103to\n108. Figure 15 summarizes the measurement results of search\nlatency and the index storage usage on zipf datasets. Results\nfrom diag andunif datasets possess the same pattern.\nIt conﬁrms that when the corpus is small, the baselines\nare faster, suggesting a room for A IRPHANT ’s improvement\nin more aggressive caching policy. As the size of corpus\nincreases, A IRPHANT relatively outperforms more and more\nacross all synthetic datasets. The performance at the high end\nis summarized in previous section §V-B0a. In terms of storage\nsize, A IRPHANT generally allocates more space comparing to\nboth SQLite and Lucene but all follow the same trend in a\nlogarithmic scale. In the worst setting, A IRPHANT uses 2:85\u0002\nmore storage than Lucene.\nC. Tiny IoU Sketch Structure\nApart from §V-D, we also use Cranfield to explore\na restrictive range of B2 f1000;1500;:::; 3000gand a ex-\ncessively wide one for L2f1;2;4;6;8;:::; 16g. Recall that\nL= 1 corresponds to the na ¨ıve hash table index structure.\nFigure 16 shows four aspects of measurement. The false\npositive averages (Figure 16a) conﬁrm our formulation: for\na ﬁxedB, there exists some L\u0003that minimizes the error. As\nBincreases, we see the false positive averages decrease across\nallL. Such high numbers of false positives result in a higher\nsearch latency (Figure 16b) observed near the two ends of\ndotted line. Although the search latencies across different B\n\ndiag(8,8,0) uniform(8,8,1) zipf(8,8,1) Cranfield HDFS Windows Spark100101102103104105106107Search Latency (ms)Lucene\nElasticsearch\nSQLite\nHashTable\nAIRPHANT\nFig. 12: End-to-end search latencies of indexes on different datasets from Europe (London) . Solid bars show average latencies\nwhile the upper error bars show 99th percentiles of measured latencies. Notice the logarithmic scale on the y-axis.\ndiag(8,8,0) uniform(8,8,1) zipf(8,8,1) Cranfield HDFS Windows Spark101102103104105106107Search Latency (ms)Lucene\nElasticsearch\nSQLite\nHashTable\nAIRPHANT\nFig. 13: End-to-end search latencies of indexes on different datasets from Asia (Singapore) . Solid bars show average latencies\nwhile the upper error bars show 99th percentiles of measured latencies. Notice the logarithmic scale on the y-axis.\ndiag(8,8,0)uniform(8,8,1)zipf(8,8,1)CranfieldHDFSWindowsSpark01002003004005006007008009001;000Lookup Latency (ms)SQLite AIRPHANT\nFig. 14: Term index lookup latencies of indexes on different\ndatasets. Solid bars show average latencies while the upper\nerror bars show 99th percentiles of measured latencies.\n(B\u00151500) are similar due to stochasticity, higher settings of\nBgenerally results in a faster search. It should come as no\nsurprise that spending more resource (memory in this case)\nresults in a better performance.\nThe number of layers Lhas the most impact on the\nstorage usage because it roughly determines how many bins\neach posting would belong to. In the worst case where no\ncommon postings fall into the same bin, the storage usage is\na linear function of L; however, Figure 16d reveals a sublinear\nrelationship, especially in small B. This is due to a high\nchance of word’s hash collision, inducing more intersection\nof postings. Developing a hash function that promotes suchintersection to save space while controlling false positives\nis a promising future direction. Moreover, Lalso approxi-\nmately linearly affects term lookup latencies as presented in\nFigure 16c. Thanks to concurrent network communication,\nlookup latency is substantially smaller than a multiple of L;\nfor example, lookup latency for L= 16 is much less than 16\u0002\nof that for L= 1.\nD. Tighter Accuracy Requirement\nWe also study the relationship between expected false\npositive parameter F0on optimal number of layers, and\nultimately, search and term index lookup latencies. We select\nF02 f1:0;0:01;0:0001gusingB= 105bins. Then, we\nobserve the optimal numbers of layers L\u0003, search latency, and\nterm index lookup latency. Despite differences in magnitude\nfrom these setting, the resulting optimal number of layers\nL\u0003increases only slightly (Figure 17a). This is consistent\nwith but even stronger (in terms of exponentiation base) than\nthe earlier upper bound analysis that the expected number of\nfalse positives is exponentially decreasing at O(2\u0000L). As we\nhave seen earlier, a small difference in number of layers with\nﬁxed number of bins reﬂects in a slight difference in both\nsearch and term index lookup latencies. Overall, Figure 17b\ndisplays slight increases in latencies when accuracy parameter\nincreases. Those observations that do not follow the trend do\nso because of variability in cloud storage.\n\nSQLite Lucene AIRPHANT\n1041051061071080200400600\ndataset: diag\nCorpus Size ( N)Latency (ms)\n10310410510610710802004006008001;000\ndataset: unif\nCorpus Size ( N)Latency (ms)\n(a)Search Latency1031041051061071080200400600800\ndataset: zipf\nCorpus Size ( N)Latency (ms)\n10410510610710810510610710810910101011\ndataset: diag\nCorpus Size ( N)Size (bytes)\n10410510610710810510610710810910101011\ndataset: unif\nCorpus Size ( N)Size (bytes)\n(b)Index Storage Usage10410510610710810510610710810910101011\ndataset: zipf\nCorpus Size ( N)Size (bytes)\nFig. 15: Effects of corpus size on avg search latency (top) and index size (bottom). Given a corpus size X, synthetic\ndatasets are generated with the numbers of documents and words nd=nw=N= 10x:diag(x,x,0),unif(x,x,1),\nandzipf(x,x,1).\n1000 1500 2000 2500 3000\n14 8 12 160.020.2220120Count\n(a)Average False Positives14 8 12 16200300400500Latency (ms)\n(b)Average Search Latency14 8 12 16100150200250300Latency (ms)\n(c)Average Lookup Latency14 8 12 1605101520Size (MB)\n(d)Storage Usage\nFig. 16: Effects of numbers of bins Band layersLon average number of false positives, average search latency, average term\nlookup latency, and index storage size, from left to right. Each line corresponds to a number of bins B2f1000;1500;:::; 3000g,\nwhile x-axis corresponds to the number of layers L. We use Cranfield as the corpus in these experiments.\n01234Number of LayersF0= 1:0F0= 0:01F0= 0:0001\n(a)Optimal Number of Layers L\u000304008001;2001;600Search Latency (ms)\n(b)Search Latency\nFig. 17: Latencies of A IRPHANT with different accuracy constraint conﬁgurations. Solid bars show average latencies while the\nupper error bars show 99th percentiles of measured latencies.",
  "textLength": 91066
}