{
  "paperId": "cc37ec52e4d47cdadeb5c51f845a29e26e84f6a2",
  "title": "One stone, two birds: A lightweight multidimensional learned index with cardinality support",
  "pdfPath": "cc37ec52e4d47cdadeb5c51f845a29e26e84f6a2.pdf",
  "text": "One stone, two birds: A lightweight multidimensional learned\nindex with cardinality support\nYingze Li\nHarbin Institude of Technology\nHarbin, China\n1190202126@stu.hit.edu.cnHongzhi Wang\nHarbin Institude of Technology\nHarbin, China\nwangzh@hit.edu.cnXianglong Liu\nHarbin Institude of Technology\nHarbin, China\n1190200525@stu.hit.edu.cn\nABSTRACT\nInnovative learning-based structures have recently been proposed\nto tackle index and cardinality estimation(CE) tasks, specifically\nlearned indexes and data-driven cardinality estimators. These struc-\ntures exhibit excellent performance in capturing data distribution,\nmaking them promising for integration into AI-driven database\nkernels. However, accurate estimation for corner case queries re-\nquires a large number of network parameters, resulting in higher\ncomputing resources on expensive GPUs and more storage over-\nhead. Additionally, the separate implementation for CE and learned\nindex result in a redundancy waste by storage of single table dis-\ntribution twice. These present challenges for designing AI-driven\ndatabase kernels. As in real database scenarios, a compact kernel\nis necessary to process queries within a limited storage and time\nbudget. Directly integrating these two AI approaches would result\nin a heavy and complex kernel due to a large number of network\nparameters and repeated storage of data distribution parameters.\nOur proposed CardIndex structure effectively killed two birds\nwith one stone. It is a fast multi-dim learned index that also serves\nas a lightweight cardinality estimator with parameters scaled at the\nKB level. Due to its special structure and small parameter size, it\ncan obtain both cumulative density function (CDF) and probability\ndensity function (PDF) information for tuples with an incredibly\nlow latency of 1âˆ’10ğœ‡ğ‘ . For tasks with low selectivity estimation,\nwe did not increase the modelâ€™s parameters to obtain fine-grained\npoint density. Instead, we fully utilized our structureâ€™s characteris-\ntics and proposed a hybrid estimation algorithm in providing fast\nand exact results. Extensive experiments show that our CardIndex\noutperforms the state-of-the-art CE methods by 1.3-114 Ã—on low\nselectivity queries, while its inference latency on CPU and storage\nconsumption is also 1-2 orders of magnitude smaller than these\ndeep models on GPU. On index tasks, our CardIndexâ€™s index perfor-\nmance is 30%-40%faster in point queries and 4-10 Ã—faster in range\nqueries compared to traditional multi-dim indexes.\nPVLDB Reference Format:\nYingze Li, Hongzhi Wang, and Xianglong Liu . One stone, two birds: A\nlightweight multidimensional learned index with cardinality support.\nPVLDB, 14(1): XXX-XXX, 2020.\ndoi:XX.XX/XXX.XX\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 14, No. 1 ISSN 2150-8097.\ndoi:XX.XX/XXX.XXPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nhttps://github.com/LiYingZe/CardIndex.\n1 INTRODUCTION\nLearning-based architectures are offering novel prospects to tradi-\ntional database problems [ 14]. The learned index methodology [ 13]\nendeavors to acquire a function that associates the key of an in-\ndexed tuple with its cumulative density function (CDF), while the\ndata-driven cardinality estimators [ 24] aim to sample from the prob-\nability density function (PDF) approximated by deep autoregressive\nmodels. Due to their excellent performance in estimation quality\nand rapid index searching, they are expected to be integrated into\nthe AI-driven database kernel [ 11], creating a new generation of\ndatabase products.\nEven though these mechanisms give promising chances, em-\nbedding both of them into a single database brings challenges. In\npractical databases, a compact and elegant database kernel is nec-\nessary for the efficient operation of the database system [ 17]. That\nis, query processing is required to be accomplished within a lim-\nited storage space and time budget. Although these learning-based\nstrategies are excellent in learning data distribution, the existing\nmethods are often too cumbersome to be applicable in the scenario\nof multidimensional data. For example, given a table with 80 million\ntuples. The autoregressive network proposed by Naru [ 24] requires\n10 hours of training, more than 100 MB of space to store parameters,\nand at least 20ğ‘šğ‘ of inference latency on an expensive GPU. These\nheavy structures demand many training resources for a converged\nmodel, many computing resources for inference, and large memory\noverhead to store the distribution of tables.\nThis dilemma is difficult to solve. The reason is that, for learning-\nbased tasks, if we want to obtain an accurate estimation of some\ncorner cases, e.g. some queries with small cardinalities, then we\nshould pay the corresponding price. The model parameter scale\nis increased to achieve higher prediction accuracy. Larger model\nparameters require more computing resources to train, higher com-\nputing latency to inference, and more space to store. Meanwhile,\nconsidering that multi-dimensional learned indexes and CE tasks\nlearn from the distribution of the same table, the information of a\nsingle table is stored twice via two different networks, which leads\nto redundant waste of space.\nFortunately, this gives us chance to solve both indexing and\nCE problems for multi-dimensional scenarios together with one\nlightweight structure. That is, a small model is used to learn the\ndistribution of a single table and to act as both a learned index\nand a cardinality estimator. The parameters of the model are con-\ntrolled at the KB level to achieve the ğœ‡ğ‘ level of inference latencyarXiv:2305.17674v1  [cs.DB]  28 May 2023\n\n\n  \n(, )0 2 8 10 32\n1 3 9 11 33\n4 6 12 14 30\n5 7 13 15 37\n16 18 24 26 480 1 2 3 4\n0\n1\n2\n3\n4\n âˆ’  âˆ’ Figure 1: In a single dimension, a simple subtraction is suffi-\ncient for transforming an index into a cardinality estimator.\nHowever, multi-dim cases are more difficult. Directly using\nsubtraction in Z-Order will result in a larger estimation error.\non a cheap CPU than an expensive GPU. Given its tiny parameter\nsize, it cannot achieve very accurate point density estimation. This\ncauses poor performance in directly estimating some small cardi-\nnality queries, and it needs multiple auxiliary refining structures\nto accurately pinpoint the index address. However, for these low\nselectivity queries, it is much faster to use index execution directly\nto obtain accurate cardinality values compared to those sampling\nmethods. For example, for queries with a result size of 100-1000, it\nonly takes about 50ğœ‡ğ‘ to search the precise cardinality directly via\nthe index on CPU, and 20-200ğ‘šğ‘ to obtain an approximate result\nfor a progressive sampling using autoregressive models on GPU.\nAbove analysis inspires us to construct a general lightweight\nstructure for these two tasks of index learning and CE, which not\nonly plays as an efficient multidimensional index but also stands\nas a lightweight cardinality estimator. It can efficiently identify\nthe query size and select the appropriate estimation strategy. For\nestimation over high selectivity, it uses the progressive sampling\nstrategy for estimation. For small selectivity estimation tasks, it\ndirectly uses the index structure to obtain the exact cardinality\nvalue in less time.\nThe discussed structure is intuitive for one-dimensional data,\nbut direct implementations are not scalable to multi-dimensional\nsituations. Ji et al [12]. modified the learned index to achieve CDF\nestimation on both sides of the one-dim query through twice point\nqueries. By subtracting, the estimation of the one-dimensional car-\ndinality is obtained. However, these efforts are not suitable for\nmulti-dimensional cases, where the problem is much more com-\nplex. As demonstrated in Fig 1, using the Z-Order curve directly\nas a dimensionality reduction bridge leads to greater estimation\nerrors, as not all records between ğ‘ğ‘šğ‘–ğ‘›andğ‘ğ‘šğ‘ğ‘¥ will be contained\nin the query box. Therefore, the barrier exists in utilizing multi-\ndimensional CDF in CE tasks.\nTo achieve the goal on multi-dimensional data, we crossed the\nbarrier. Our solution derives a unified structure that can obtain\nCDF and PDF information simultaneously under only a single in-\nference step. CDF information can be used for index queries, while\nPDF information can be used for progressive sampling estimation.\nTherefore, our proposed CardIndex structure effectively killed two\nbirds with one stone. In the case of multidimensional index tasks, it\ncan be utilized as a learned index to swiftly conduct point queries\nwith an average latency of 1ğœ‡ğ‘ and the performance of range query\nis not inferior to the traditional structure. In the case of cardinalityestimation, only a few computations on the CPU are required to\nachieve close estimations with a similar latency compared to main-\nstream deep estimators on GPU. Moreover, our proposed hybrid\nestimation method brought us a chance to get the precise result\nof low selectivity queries. Specifically, our proposed CardIndex\nstructure faces the following challenges:\nC1: Data organization: How can multidimensional data be orga-\nnized so that multidimensional CDFs can be associated with sorted\ndata addresses to efficiently support multidimensional queries and\nestimation tasks?\nC2: Model Implementation: How can a learning-based model\nbe implemented to replace traditional database index structures,\nsuch as KDB tree and R tree, while maintaining low latency?\nC3: Algorithm design: What methodology can be utilized in\ninferring this unified structure to support both index processing and\ncardinality estimation? Furthermore, how can the precision quality\nof the model be preserved in problem resolution, particularly when\ndealing with limited model parameters?\nWe have addressed these challenges, and given the following\ncontributions as follows:\n1. We propose the CardIndex structure, a lightweight multidi-\nmensional learned index that learns the Z-Order bit encoding dis-\ntribution of data through the autoregressive model. The learned\nparameters of our structure are in KB scale. Thus, our CardIndex\ncan perform one fast inference under 1âˆ’10ğœ‡ğ‘ to output the CDF\nand PDF information required by Index and CE. We designed algo-\nrithms for point query, range query, and CE tasks, making this the\nfirst attempt to solve both CE and multidimensional indexing tasks\nin a single learned structure.\n2. For the indexing task, we developed two approaches to improve\nthe performance, Pre- ğ‘˜neuron linkage and Linear Refinement. The\nformer effectively shortens the neuronal dependency complexity,\nmaking the structure more scalable on Z-Ordered bit inference\nlength. The latter refines the approximated CDF estimation of deep\nstructure through cheaply linear functions. Thus we could obtain\na more accurate address through the index, which makes query\nprocessing more efficient.\n3. For the cardinality estimation task, we propose the hybrid esti-\nmation method. We first use several point index queries to probe the\nupper bound of the cardinality selectivity, namely, our FastCDFEst\nalgorithm. This helps us distinguish whether the cardinality of the\nquery is large or small. We only use progressive sampling of our\ndeep model to estimate large cardinality, while for the estimation of\nsmall cardinality, we directly use the multidimensional index part\nto achieve precise estimation in much less time.\n4. We compared our cardinality estimation algorithm with the\nstate-of-the-art single table estimators and multidimensional in-\ndexes on multiple workloads of real and synthetic data. Experi-\nment results show that our hybrid estimation algorithm improves\nestimation performance by 1.3-114 Ã—in scenarios with extremely\nsmall cardinality queries. The model size and inference time of\nour method on CPU is also 1-2 orders of magnitude smaller than\ncurrent deep estimators on GPU. Experiments also show that our\nindex is 30-40% faster on point query tasks and 4-10 Ã—faster than\ntraditional architectures under certain workloads un range queries.\n2\n\nThe remaining sections of this paper are structured as follows.\nSection 2 reviews related works. Section 3 introduces the funda-\nmental concepts of our approach. Section 4 outlines the underlying\nstructure of our index, and Section 5 provides a detailed description\nof the algorithm used for estimation and index queries utilizing our\nCardIndex. In Section 6, we demonstrate the effectiveness of our\nmethodology through extensive experimentation. We conclude our\nwork in Section 7.\n2 RELATED WORK\nZ-Order Curve : A space-filling curve that maps multidimensional\ndata to one-dimensional [ 22]. It is also good in locality [ 6], meaning\nthat the adjacent Z-Order value tuples tend to be similar in many\nattribute values. As a result, it is widely used in the implementation\nof the multidimensional index, such as UB-Tree[ 20]. The easiest\napproach to using the Z-Order curve in a range query is to calculate\nthe Z-values of the top left ğ‘ğ‘šğ‘–ğ‘›and bottom right ğ‘ğ‘šğ‘ğ‘¥ endpoints of\nthe query box and scan for all the data between them [ 23]. However,\nmany elements between ğ‘ğ‘šğ‘–ğ‘›andğ‘ğ‘šğ‘ğ‘¥ are not in the query box.\nTherefore, discontinuous gaps arise between ğ‘ğ‘šğ‘–ğ‘›andğ‘ğ‘šğ‘ğ‘¥. To\nskip these gaps, tropf et al [ 22] proposed the getBIGMIN method.\nGiven a tuple ğ‘¡outside the query box, getBIGMIN shall â€œjump\" to the\nminimum Z-value greater than ğ‘¡in the query box within ğ‘‚(ğ‘›)time\nwhereğ‘›is the bit encoding length. The counterpart of the above\nmethod is the getLITMAX method, which can get the maximum\nZ-value in the query box less than the tuple ğ‘¡inğ‘‚(ğ‘›)time.\nLearned Index : By using a learned function that maps a search\nkey to its address, the learned index achieves point query in con-\nstant time. Compared with traditional index structures, this ap-\nproach avoids searching in a log-scale depth tree index. Under one-\ndimensional cases, RMI [ 13] first utilized this idea by using MLPs\nto map the search key to its CDF. After RMI, many newly learned\nindexes sprang up. For example, the Fitting-Tree [ 8] structure fits\ndata with piecewise linear functions and gives bound guarantees\nfor data coordinates. Meanwhile, a lot of work has been done on\nlearned index in multidimensional scenarios. Tsunami [ 7] takes the\norganization structure of grid files and uses the tuned grid parti-\ntioning to organize the index. RSMI [ 19] and ZM [ 23] are similar to\nour approach of indexing multidimensional data under the Z-Order\nspace-filling curve, but it does not take advantage of Z-Orderâ€™s\noptimization to skip the discontinuous regions.\nData-driven CE : Through the data distribution learned from\nthe model, we can estimate the cardinality of the query. There are\ntwo kinds of representative genres in data-driven methods: (1) SPN\n(Sum Product Network) [ 10]: SPN recursively performs horizontal\nand vertical decomposition of the table through a KD tree-like space\npartition strategy. It uses sum nodes to combine different rows as\nclusters. The multiplication node is used to decompose the attribute\ngroup with weak column correlation. (2) Autoregressive model [ 24]:\nThrough the strong fitting ability of deep learning networks, deep\nautoregressive models learns the joint distribution between data\nattributes, thus achieving good point density estimation. The rep-\nresentative method is Naru, which achieves a good estimation of\nquery distribution through a progressive sampling algorithm. This\nalgorithm belongs to a special branch of Monte Carlo integratedsampling, which makes the sampled model concentrate on some\nattribute columns with higher density and saves the sampling times.\n3 PRELIMINARIES\nIn this section, we shall commence by presenting fundamental\nnotions outlined in Â§ 3.1. Subsequently, we shall derive the key\nconcepts of indexing and estimation by utilizing the aforementioned\nfundamental notions in conjunction with the bit joint distribution\ndiscussed in Â§ 3.2. Lastly, we will deliberate on the insights about\nthe information our model has learned from both the cumulative\ndistribution function (CDF) and probability density function (PDF)\nperspectives, as called upon in Â§ 3.3.\n3.1 Basic Concepts\nFor a relational table ğ‘‡onğ‘tuples, andğ‘šattributes{ğ´1,ğ´2...ğ´ğ‘š},\neach of which is encoded by ğ‘binary bits, i.e. ğ´ğ‘–=ğ·ğ‘–1ğ·ğ‘–2...ğ·ğ‘–ğ‘.\nFor a query predicate: ğœƒ:ğ´1Ã—ğ´2Â·Â·Â·Ã—ğ´ğ‘šâ†’{0,1}, tuples satisfy-\ning predicate in table ğ‘‡forms a result set ğ‘…0={ğ‘¡âˆˆğ‘‡:ğœƒ(ğ‘¡)=1}.\nThe index demands an auxiliary structure in accelerating the pro-\ncess of getting the result set ğ‘…0. And CE requires the cardinality\nestimation of the result set, i.e., ğ‘ğ‘ğ‘Ÿğ‘‘(ğœƒ)=|ğ‘…0|.\nThe joint distribution ğ‘ƒ(ğ‘¡)=ğ‘œ(ğ‘¡)/ğ‘of the tuple ğ‘¡is related\nto the query selectivity ğ‘ ğ‘’ğ‘™(ğœƒ), whereğ‘œ(ğ‘¡)is the number of occur-\nrences of tuple ğ‘¡in tableğ‘‡. Therefore,\nğ‘ ğ‘’ğ‘™(ğœƒ)=âˆ‘ï¸\nğ‘¡âˆˆğ´1Ã—ğ´2Ã—...ğ´ğ‘šğœƒ(ğ‘¡)Â·ğ‘ƒ(ğ‘¡) (1)\nMeanwhile, if data ordering satisfies the bit ordering property\nğ¿={ğ·ğ‘¥1,ğ·ğ‘¥2...ğ·ğ‘¥ğ‘}, ordering relationship ğ‘¡2<ğ‘¡1is satisfied, if\nand only if:âˆƒğ‘˜:ğ‘ .ğ‘¡.ğ‘¡ 2.ğ·ğ‘¥ğ‘˜<ğ‘¡1.ğ·ğ‘¥ğ‘˜andâˆ€ğ‘—<ğ‘˜:ğ‘¡2.ğ·ğ‘¥ğ‘—=ğ‘¡1.ğ·ğ‘¥ğ‘—.\nIn this particular ordering, the tuple address distribution ğ¶ğ·ğ¹(ğ‘¡)\nis also closely related to the distribution of tuples ğ‘ƒ(ğ‘¡):\nğ¶ğ·ğ¹(ğ‘¡)=âˆ‘ï¸\nğ‘¡ğ‘–<ğ‘¡ğ‘œ(ğ‘¡ğ‘–)/ğ‘=âˆ‘ï¸\nğ‘¡ğ‘–<ğ‘¡ğ‘ƒ(ğ‘¡ğ‘–) (2)\nğ¿ğ¿ğ‘’ğ‘¥is lexicographical ordering, when ğ¿ğ¿ğ‘’ğ‘¥={ğ·11,ğ·12,...ğ· 1ğ‘,\nğ·21,ğ·22,...ğ· 2ğ‘...ğ·ğ‘š1,ğ·ğ‘š2,...ğ·ğ‘šğ‘}.\nğ¿ğ‘is Bitwise entanglement ordering(Z-Order), when ğ¿ğ‘={ğ·11,\nğ·21,...ğ·ğ‘š1,ğ·12,ğ·22,...ğ·ğ‘š2...ğ· 1ğ‘,ğ·2ğ‘,...ğ·ğ‘šğ‘}.\nTo process multidimensional range index queries more efficiently,\nwe convention the bit ordering as Z-Order ğ¿ğ‘and adopt the follow-\ning notations: We use ğ‘›to replaceğ‘šğ‘in denoting the total coding\nlength, set the ordering notation as ğ¿ğ‘={ğ‘1,ğ‘2,...ğ‘ğ‘›}, tupleğ‘¡\nunder Z-Ordering is denoted as ğ‘¡=(ğ‘§1,ğ‘§2...ğ‘§ğ‘›).\n3.2 Bitwise Joint Distribution\nWe note that for a given Z-Ordering ğ¿ğ‘, the tuple distribution\nğ‘ƒ(ğ‘1,ğ‘2...ğ‘ğ‘›), can naturally take the following decomposition:\nğ‘ƒ(ğ‘1,...ğ‘ğ‘›)=ğ‘ƒ(ğ‘1)Â·ğ‘ƒ(ğ‘2|ğ‘1)...ğ‘ƒ(ğ‘ğ‘›|ğ‘1,ğ‘2,...ğ‘ğ‘›âˆ’1).\nTherefore, it is possible for us to use the deep autoregressive\nmodel to estimate the above decomposed probability function. We\nwill set the chain of autoregressive reasoning as the Z-Order ğ¿ğ‘,\nin predicting:{Ë†ğ‘ƒ(ğ‘ğ‘–|ğ‘1,ğ‘2,...ğ‘ğ‘–âˆ’1)}. That is, the model takes the\ngiven Z-encoded tuple: ğ‘¡=(ğ‘§1,ğ‘§2,...ğ‘§ğ‘›)as the input, and gets\nğ‘›conditional probability estimations: {Ë†ğ‘ƒ(ğ‘§ğ‘–|ğ‘§1,ğ‘§2,...ğ‘§ğ‘–âˆ’1)}as the\noutput.\n3\n\n= \n( = ) ( = )\n( = | = )\n( = | = )\n(â‰¤  )( = | = ) âˆ’  âˆ’ \n(=  )\n00 01 10 1100\n01\n10\n11\n âˆ’   âˆ’ Figure 2: What Our structure learns\nBased on above input and output of the model, we use the\ncross entropy loss in calculating the distance between the pre-\ndicted and ground truth Z-Ordered binary sequences: ğ»(ğ‘ƒ,Ë†ğ‘ƒ)=\nâˆ’Ã\nğ‘¡âˆˆğ‘‡ğ‘ƒ(ğ‘¡)ğ‘™ğ‘œğ‘”(Ë†ğ‘ƒ(ğ‘¡)). Gradient descent is used to train the model.\nThrough the learned Z-Order distribution, we implement the\ngeneral idea for point queries. Since the point index query of tuple\nğ‘¡=(ğ‘§1,ğ‘§2...ğ‘§ğ‘›)needs to estimate all the tuples that come ahead\nof it:Ã\nğ‘¡ğ‘–<ğ‘¡ğ‘œ(ğ‘¡ğ‘–). Directly traversal of each element in table ğ‘‡to\ncalculate the above sum is unwise, which has a complexity is ğ‘‚(ğ‘),\nwhereğ‘is the cardinality of the entire table.\nFor any table ğ‘‡sorted by Z-Ordering, any tuple ğ‘¡ğ‘¦smaller than\nğ‘¡ğ‘¥must satisfy thatâˆƒğ‘–,ğ‘¡ğ‘¦.ğ‘§ğ‘–=0,ğ‘¡ğ‘¥.ğ‘§ğ‘–=1andâˆ€ğ‘—<ğ‘–:ğ‘¡ğ‘¦.ğ‘§ğ‘—=ğ‘¡ğ‘¥.ğ‘§ğ‘—.\nTherefore, to index tuple ğ‘¡ğ‘¥, all records in table ğ‘‡smaller than\nğ‘¡ğ‘¥can be virtually regrouped based on the longest common bit\nstring length starting from bit position 0. Specifically, let ğº(ğ‘¡ğ‘¥,ğ‘–)=\n{ğ‘¡ğ‘¦âˆˆğ‘‡,âˆ€ğ‘—<ğ‘–,ğ‘¡ğ‘¦.ğ‘§ğ‘—=ğ‘¡ğ‘¥.ğ‘§ğ‘—,ğ‘¡ğ‘¦.ğ‘§ğ‘–<ğ‘¡ğ‘¥.ğ‘§ğ‘–}. As a result, the\nCDF ofğ‘¡ğ‘¥can be calculated according to the formula: ğ¶ğ·ğ¹(ğ‘¡ğ‘¥)=Ã\nğ‘–â‰¤ğ‘›|ğº(ğ‘¡ğ‘¥,ğ‘–)|/ğ‘. Note that the conditional probabilities learned\nby our model correspond exactly to the cardinals of these groups,\nwhereğ‘ƒğ‘Ÿ(ğ‘ğ‘–=0|ğ‘1,...,ğ‘ğ‘–âˆ’1=ğ‘§1,...,ğ‘§ğ‘–âˆ’1)Ã—ğ‘§ğ‘–corresponds to\n|ğº(ğ‘¡ğ‘¥,ğ‘–)|/ğ‘. Thus, we can use the following conditional probability\nsum expression in obtaining the index address distribution.\nğ¶ğ·ğ¹(ğ‘¡)=âˆ‘ï¸\nğ‘–â‰¤ğ‘›ğ‘ƒğ‘Ÿ(ğ‘ğ‘–=0|ğ‘1...ğ‘ğ‘–âˆ’1=ğ‘§1...ğ‘§ğ‘–âˆ’1)Â·ğ‘§ğ‘– (3)\nConsidering the accuracy of the model and the computational\nefficiency, we truncate the above summation and use the following\napproximation to calculate only the first â„“terms of the sum formula:\nğ¶ğ·ğ¹(ğ‘¡)â‰ƒâˆ‘ï¸\nğ‘–â‰¤â„“ğ‘ƒğ‘Ÿ(ğ‘ğ‘–=0|ğ‘1...ğ‘ğ‘–âˆ’1=ğ‘§1...ğ‘§ğ‘–âˆ’1)Â·ğ‘§ğ‘– (4)\nThis sum can be computed in ğ‘‚(â„“)time, where â„“is a fixed con-\nstant that represents the first â„“representative bits. For example, we\ntake the first 32 bits of Z-Order encoding in our experiments.\n3.3 Insights From Index And CE Perspective\nThe distribution acquired by our model through Z-Order can be\nmodeled as a probabilistic transition tree, which can be visualized\nin Figure 2. Under the Z-Order bit sequence, the output of our\nautoregressive model is equivalent to a probabilistic transition path\nfrom the root node to the leaf node in the tree. These paths can be\ninvoked in two ways, depending on the usage requirements:\nMADE\n( âˆ’ )\nÂ  Â   Â  Z-Key\nâ€¦\nÂ  Â   Â   â€¦\n1 0 1 0 0 1\nCDF\nZ-ValueRefine-CDF \n \n Params\n â‹…  Figure 3: The layout of CardIndex\n1. CDF Call : This method returns the summation of probabilities\nfor all child nodesâ€™ sibling nodes with a bit value of 1.\n2. PDF Call : This method directly outputs the probability density\nvalue that corresponds to the end of the Z-Order path.\nFrom the perspective of the multidimensional index, the above-\nmentioned CDF call can be perceived as a traversal of the Z-Ordered\nspace partitioning tree. Each ğ‘¤bit output by the model is equivalent\nto a visit to an internal node of the KDB tree. By summing up the\nprobability value corresponding to these ğ‘¤bits, the model can\ndirectly predict the corresponding position of the leaf node.\nFrom the perspective of the cardinality estimator, the above-\nmentioned PDF call achieves the acquisition of point density esti-\nmation. By employing some existing sampling techniques, such as\nprogressive sampling, we can effectively utilize this point density\nestimation information for the cardinality estimation of the query.\n4 CARDINDEX STRUCTURE\nIn this section, we propose the basic hierarchical structure of our\nmodel, as illustrated in Fig 3. At the highest level, a nimble Z-\nOrdered autoregressive network is utilized to perform two sub-\ntasks, namely indexing and estimation, achieved by invoking CDF\nand PDF calls. The remaining nodes refine the CDF value produced\nby the root using linear functions to achieve precise address infor-\nmation. We discuss the layout and optimization techniques of our\nmodel in Â§ 4.1 and introduce our implementation details in Â§ 4.2.\n4.1 Layout\nIn principle, any deep autoregressive model can serve as the CDF\nestimate structure for the root. However, for the sake of efficient in-\nference, we employ a three-layered MADE [ 9] as our autoregressive\nstructure. Given that both index and CE tasks require low inference\nlatency, and the index task requires a precise tuple address of a\ngiven query, we employ two optimization approaches from the\naspect of shortening the dependency complexity of the deep model\nand finding cheaper functions to decrease the estimated CDF error.\nPre-ğ‘˜neuron linkage: To achieve scalability of binary coding\nlength, we use limited mask connection of neurons to establish the\nconditional probability information fitting between bits. In detail,\nwe adopt the following conventions for mask settings. The ğ‘–-th neu-\nron of the current layer only depends on the output of the (ğ‘–âˆ’ğ‘˜)-th\nto(ğ‘–âˆ’1)-th neurons of the previous layer. The upper-right corner of\n4\n\nFig 3 illustrates the case when ğ‘˜=2. This method yields better scal-\nability on input coding length. In contrast, for the original MADE\nnetwork, the ğ‘–-th neuron of the current layer depends, in the worst\ncase, on the output of all the neurons of the first (ğ‘–âˆ’1)-th output of\nthe previous layer. This results in a complexity of scale ğ‘‚(ğ‘›2)for the\ncalculation of the network between layers, where ğ‘›is the length of\nthe encoded bits. Our approach results in a computational complex-\nity of each layer being ğ‘‚(ğ‘›Ã—ğ‘˜). This feature enables us to calculate\ndensity functions quicker and index data faster. Although this set-\nting may reduce the performance of the model in point density es-\ntimation to some extent, it will not affect the cardinality estimation\nperformance of our structure too much. This is because estimations\nthat engage with large cardinalities do not require particularly fine-\ngrained point density estimations. For small cardinality estimations,\nas we will see in Â§ 5.3, can be delegated to the index rather than\nthe inaccurate sampler to get precise cardinality values efficiently.\nLinear Refinement: After using the deep network to nonlin-\nearly fit the data distribution of first â„“bits, we use a more cheaply\ncalculated linear function to refine the CDF of the data. We take\nthe fully Z-Order coded key as the input and the CDF address of\nthe index as the refined output. Our linear interpolation process\nis similar to the construction of the works of FITing tree [ 8]. By\nmaintaining the maximum error limit ğ‘ğ‘’ğ‘Ÿğ‘Ÿfor linear interpolation\nwhen building the tree, two sets of linear parameters are obtained\nto ensure the refined CDF of each tuple will be sandwiched between\na region with the maximum estimated error within ğ‘ğ‘’ğ‘Ÿğ‘Ÿ. In deploy-\nment, given the stored starting point and its two linear boundary\nslopes, we can bound the CDF value of the required key in an inter-\nval predicted from these two functions satisfying that the interval\nlength does not exceed 2Ã—ğ‘ğ‘’ğ‘Ÿğ‘Ÿ. Since we want to scan as few tuples\nas possible during point queries execution, when constructing the\ntree, we can assign the ğ‘ğ‘’ğ‘Ÿğ‘Ÿless than half of the CDF capacity of\na single data page. In this way, if a miss occurs on performing an\nindex point query on a certain page, we only need to expand the\nsearch of its adjacent page.\n4.2 Implementations\nDirectly implementing our model with the existing deep learning\nframework is inefficient, considering these frameworks tend to\nbe designed for optimizing large models, resulting in significant\ninvocation overhead which makes them infeasible for tasks like\nindexing [ 13]. At the same time, the framework used in RMI is\ndesigned for a fully connected network under single-dimensional\nindexing task, which is not compatible with our autoregressive net-\nwork structure. Therefore, in order to accelerate the overall training\nand reasoning process of our structure, we adopted the following\nimplementation ideas.\nWe first train our model using the PyTorch framework [ 16] on\nGPU, then set up the tree structure in the Python environment [ 2]\nand output it to disk. The inference component implemented via\nC++ reads the trained structure into memory and uses the CPU for\nnetwork reasoning to achieve the two tasks of CE and indexing.\nOur structure is established with the following steps. First, we\ntrain the autoregressive model and use it as our root to calculate the\nZ-Order CDF values for all tuples. Then, we sort the data with the\nCDF value as the primary key. Given the number of second-levelchild nodes ğ‘›, the tuples with CDF values at ((ğ‘–âˆ’1)/ğ‘›,ğ‘–/ğ‘›)are\ngrouped into the ğ‘–-th (ğ‘–<ğ‘›) linear sub-model. Finally, we maintain\nthe maximum error bound ğ‘ğ‘’ğ‘Ÿğ‘Ÿin linear refinement and calculate\nthe slopes and intercepts of linear functions within each sub-model.\n5 CARDINDEX INFERENCE\nIn this section, we attempt to use the trained CardIndex to solve\nthe task of index query and cardinality estimation. We propose the\nprocessing algorithm of point and range query in Â§ 5.1 and Â§ 5.2,\nrespectively. The Naive Hybrid CE algorithm is proposed in Â§ 5.3,\nwhile its improvements and properties are discussed in Â§ 5.4.\n5.1 Point Query Processing\nIn Â§ 3.2 and Â§ 3.3, we derived that our CardIndex structure can\nsimulate complex inner node traversal in the space partition tree\nby summing up the following conditional probability sequence:\nğ¶ğ·ğ¹(ğ‘¡)â‰ƒâˆ‘ï¸\nğ‘–â‰¤â„“ğ‘ƒğ‘Ÿ(ğ‘ğ‘–=0|ğ‘1...ğ‘ğ‘–âˆ’1=ğ‘§1...ğ‘§ğ‘–âˆ’1)Â·ğ‘§ğ‘– (5)\nConsidering that the parameter scale of our model is too tiny\nand we truncate the above sum from ğ‘›toâ„“, there are errors in the\nabove approximation. In Â§ 4.1, we introduce the Linear Refinement\nstrategy of using two sets of linear functions to refine the point\nquery error. So that the CDF estimation of each record can be\nbounded between the interval sandwiched by two linear equations.\nTherefore, the flow of our algorithm is as follows. We first perform\na CDF call to sum up the conditional probability sequence of the\nmodel. This replaces the complex node traversal process in the\nspace partitioning tree of the traditional index structure. Then the\nobtained CDF value is refined through two linear functions to obtain\nthe exact data page storing the particular record.\nAlgorithm 1 Point Index Query.\nRequire: CardIndex Root, ğ‘€:ğ¾ğ‘’ğ‘¦â†’ğ¶ğ·ğ¹ ; Linear submodel\nğ‘†:ğ¶ğ·ğ¹â†’{ğ‘™0,ğ‘™1,ğ‘¥,ğ‘¦}; Z-Order encoding of the queried tuple,\nğ‘=ğ‘§1ğ‘§2...ğ‘§ğ‘›;\nEnsure: The Address Pointer of the tuple: ğ´ğ‘‘ğ‘‘ğ‘Ÿğ‘¡;\n1:ğ´ğ‘ğ‘=1;\n2:ğ¶ğ·ğ¹ =0;\n3:ğ‘1,ğ‘2...ğ‘â„“=ğ‘€(ğ‘§1,ğ‘§2,...ğ‘§â„“)\n4:forğ‘–=1,...â„“ do\n5:ğ¶ğ·ğ¹ =ğ¶ğ·ğ¹+ğ‘§ğ‘–Â·(1âˆ’ğ‘ğ‘–);\n6:ğ´ğ‘ğ‘=ğ´ğ‘ğ‘Â·((ğ‘§ğ‘–Â·ğ‘ğ‘–)+(1âˆ’ğ‘§ğ‘–)Â·(1âˆ’ğ‘ğ‘–));\n7:ğ‘™0,ğ‘™1,ğ‘¥,ğ‘¦=ğ‘†[ğ¶ğ·ğ¹]\n8:ğ‘ğ‘‘ğ‘“â„“=ğ‘™0Â·(ğ‘âˆ’ğ‘¥)+ğ‘¦\n9:ğ‘ğ‘‘ğ‘“ğ‘¢=ğ‘™1Â·(ğ‘âˆ’ğ‘¥)+ğ‘¦\n10:ğ‘ƒğ‘ğ‘”ğ‘’ =ğ¹ğ‘–ğ‘›ğ‘‘ğ‘ƒğ‘ğ‘”ğ‘’(ğ‘ğ‘‘ğ‘“â„“,ğ‘ğ‘‘ğ‘“ğ‘¢)\n11:ğ´ğ‘‘ğ‘‘ğ‘Ÿ(ğ‘¡)=ğ‘†ğ‘ğ‘ğ‘›ğ‘ƒğ‘ğ‘”ğ‘’(ğ‘ƒğ‘ğ‘”ğ‘’)\n12:ifğ´ğ‘‘ğ‘‘ğ‘Ÿ(ğ‘¡)=ğ‘ğ‘ˆğ¿ğ¿ then\n13:ğ´ğ‘‘ğ‘‘ğ‘Ÿ(ğ‘¡)=ğ´ğ‘‘ğ‘—ğ‘†ğ‘ğ‘ğ‘›ğ‘ƒğ‘ğ‘”ğ‘’(ğ‘ƒğ‘ğ‘”ğ‘’)\n14:returnğ´ğ‘‘ğ‘‘ğ‘Ÿ(ğ‘¡);\nTo begin, the algorithm initially calculates the approximated CDF\nvalue in lines 1-6. It acquires the parameters of a linear function\nby utilizing the approximated CDF in line 7 and then uses them to\n5\n\nlocate the page where the tuple is stored(lines 8-10). Finally, the\nalgorithm scans the data on the page in lines 11-14 to obtain the\nexact tuple address. If the tuple is not found on this page, the search\nis extended to adjacent pages. The complexity of the algorithm is\nğ‘‚(ğ‘šğ‘ğ‘¥(â„“,ğµ)), whereâ„“denotes the inference length of our structure\nğ‘€, andğµis the size of the leaf page. Given that the page size and\ninference length are fixed, we can conclude that the above algorithm\noperates in constant time.\n5.2 Range Query Processing\nThe processing flow of our CardIndex structure for range queries\nresembles the range queries processing algorithm in Z-Ordered In-\ndex structures, such as UB-Tree[ 20]. We obtain the two boundaries\nğ‘ğ‘šğ‘–ğ‘›,ğ‘ğ‘šğ‘ğ‘¥ of the query hyperrectangle ğ‘„under Z-Order encod-\ning. Utilizing the monotone ordering property in Z-Ordering, any\ntupleğ‘¡ğ‘–within the query box satisfies: ğ´ğ‘‘ğ‘‘ğ‘Ÿ(ğ‘ğ‘šğ‘–ğ‘›)â‰¤ğ´ğ‘‘ğ‘‘ğ‘Ÿ(ğ‘¡ğ‘–)â‰¤\nğ´ğ‘‘ğ‘‘ğ‘Ÿ(ğ‘ğ‘šğ‘ğ‘¥). Thus, we can obtain the page pointer ptrStart, ptrEnd\nfrom the top-left and bottom-right corners by performing point\nqueries twice, and then search within the sandwiched pages be-\ntween pointers ptrStart andptrEnd .\nHowever, many records between ğ‘ğ‘šğ‘–ğ‘›andğ‘ğ‘šğ‘ğ‘¥ may not exist\nin the final result, as illustrated in Fig 4. Therefore, blindly travers-\ning all the pages between them will result in excessive scanning\noverhead. To address this issue, we utilize the getBIGMIN method\nproposed by Tropf [ 22]. If all the elements in the current scanning\npage are not within the query box, the getBIGMIN method can ob-\ntain the next smallest Z-Order value contained in the actual query\nresult page in ğ‘‚(ğ‘›)time, where ğ‘›is the Z-Ordered bit length. This\nfeature allows us to skip incoherent pages in a Z-Order interval. The\ncounterpart to the getBIGMIN method is the getLITMAX method,\nwhich obtains the previous largest Z-Order address containing the\nactual query results in ğ‘‚(ğ‘›)time. We will employ the getLITMAX\nmethod in Â§ 5.4. To conclude, the pseudocode description of our\nmethod is as follows:\nAlgorithm 2 Range Index Query.\nRequire: CardIndex Root, ğ‘€:ğ¾ğ‘’ğ‘¦â†’ğ¶ğ·ğ¹ ; Linear submodel\nğ‘†:ğ¶ğ·ğ¹â†’{ğ‘™0,ğ‘™1,ğ‘¥,ğ‘¦}; QueryBox, ğ‘„\nEnsure: The Result set: ğ‘…;\n1:ğ‘ğ‘šğ‘–ğ‘›,ğ‘ğ‘šğ‘ğ‘¥=ğ‘ğ¸ğ‘›ğ‘ğ‘œğ‘‘ğ‘–ğ‘›ğ‘”(ğ‘„);\n2:ğ‘ğ‘¡ğ‘Ÿğ‘†ğ‘¡ğ‘ğ‘Ÿğ‘¡ =ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ğ‘„ğ‘¢ğ‘’ğ‘Ÿğ‘¦(ğ‘€,ğ‘†,ğ‘ğ‘šğ‘–ğ‘›);\n3:ğ‘ğ‘¡ğ‘Ÿğ¸ğ‘›ğ‘‘ =ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ğ‘„ğ‘¢ğ‘’ğ‘Ÿğ‘¦(ğ‘€,ğ‘†,ğ‘ğ‘šğ‘ğ‘¥);\n4:ğ‘…=ğœ™\n5:ğ‘ğ‘¡ğ‘Ÿğ‘†ğ‘’ğ‘ğ‘Ÿğ‘â„ =ğ‘ğ‘¡ğ‘Ÿğ‘†ğ‘¡ğ‘ğ‘Ÿğ‘¡\n6:whileğ‘ğ‘¡ğ‘Ÿğ‘†ğ‘’ğ‘ğ‘Ÿğ‘â„.ğ‘§ğ‘šğ‘ğ‘¥â‰¤ğ‘ğ‘¡ğ‘Ÿğ¸ğ‘›ğ‘‘.ğ‘§ğ‘šğ‘ğ‘¥ do\n7:ğ‘…ğ‘–=ğ‘™ğ‘’ğ‘ğ‘“ğ‘†ğ‘’ğ‘ğ‘Ÿğ‘â„(ğ‘ğ‘¡ğ‘Ÿğ‘†ğ‘’ğ‘ğ‘Ÿğ‘â„,ğ‘„)\n8: ifğ‘…ğ‘–=ğœ™then\n9:ğ‘ğ‘’ğ‘¥ğ‘¡ğ‘ =ğ‘”ğ‘’ğ‘¡ğµğ¼ğºğ‘€ğ¼ğ‘(ğ‘„,ğ‘ğ‘¡ğ‘Ÿğ‘†ğ‘’ğ‘ğ‘Ÿğ‘â„.ğ‘§ğ‘šğ‘ğ‘¥ )\n10:ğ‘ğ‘¡ğ‘Ÿğ‘†ğ‘’ğ‘ğ‘Ÿğ‘â„ =ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ğ‘„ğ‘¢ğ‘’ğ‘Ÿğ‘¦(ğ‘€,ğ‘†,ğ‘ğ‘’ğ‘¥ğ‘¡ğ‘)\n11: else\n12:ğ‘…=ğ‘…âˆªğ‘…ğ‘–\n13:ğ‘ğ‘¡ğ‘Ÿğ‘†ğ‘’ğ‘ğ‘Ÿğ‘â„ =ğ‘ğ‘¡ğ‘Ÿğ‘†ğ‘’ğ‘ğ‘Ÿğ‘â„.ğ‘ ğ‘¢ğ‘ğ‘\n14:returnğ‘…;\nIn the first step of the algorithm, point queries are executed\ntwice in lines 1-3 to obtain the pointers ptrStart andptrEnd , whichcorrespond to the data pages containing the upper left and lower\nright endpoints of the query hyperrectangle. Then, in lines 6-13,\nthe scanning pointer ptrSearch traverses through the data space\nsandwiched between ptrStart andptrEnd , scanning the data pages\none by one. If there is no intersection between the data page and\nthe query box(line 8), the getBIGMIN method is used to combine\nanother point query in skipping to the next relevant page(lines\n9-10). If an intersection exists between the page and the query box,\nthe page is scanned to find all the elements in the query box, and\nptrSearch is updated to ptrSearch.succ , which points to the next\nadjacent page(lines 11-13).\nWe claim that the worst-case complexity of the range query\nalgorithm above is ğ‘‚((ğµ+ğ‘›)Ã—(ğ‘›ğ¼)), whereğµis the size of the\npage, andğ‘›ğ¼is the number of data pages intersecting the query box.\nThis is because the length of the tuple after Z-Order encoding is\nğ‘›bits, getBIGMIN method with time complexity ğ‘‚(ğ‘›)is calledğ‘›ğ¼\ntimes, and we scanned at most ğ‘‚(ğµÃ—ğ‘›ğ¼)tuples.\n5.3 Cardinality Estimation\nFor cardinality estimation tasks, we can naturally use the proposed\nCardIndex as a cardinality estimator, as conditional probability\ncorrelation information between Z-Order bits of data is learned\nfrom our autoregressive model.\nNoting that the parameters of our model are so small. It can not\nachieve very accurate estimations of point densities. Direct engage-\nment in estimating small cardinality will bring huge estimation\nerrors. Therefore, we designed a hybrid estimation algorithm in\nachieving precise estimation results within a much shorter time. In\nthe case of large cardinality estimation, our estimation algorithm\nis similar to the progressive sampling in Naru[ 24]. Single inference\nthrough the PDF call of our model provides enough information for\npoint density estimation. Based on the results of multiple progres-\nsive samples, we can estimate the cardinality of the queried region.\nIf the query selectivity is low enough, we directly use our index\nstructure to execute the query to obtain the exact cardinality. In this\nway, we can obtain precise cardinality values in a much shorter time.\nOur idea is outlined in Algorithm 3. In line 2, we use the CDF\nestimation ability of the structure to probe a rough upper bound\nğ¶0of the query cardinality(The correctness of the upper bound is\nproved in Â§ 5.4). And ğ¶0is compared with a fixed threshold ğ‘ğ¸ğ‘†ğ‘‡\nin line 3. In line 4, with the assistance of the index, we execute\nidentified small cardinality queries to obtain the exact cardinality\nvalue. In lines 5-7, we use the progressive sampling algorithm for\nlarge cardinalities.\nThe implementation of rough estimation is straightforward. As\nillustrated in lines 9-13, we roughly suppose all the tuples whose\nZ-value is sandwiched between two endpoints ğ‘ğ‘šğ‘–ğ‘›,ğ‘ğ‘šğ‘ğ‘¥ are in\nthe query box. We take ğ‘ğ‘‘ğ‘“ğ¿âˆ’ğ‘ğ‘‘ğ‘“ğ‘ˆas our rough estimations.\nAs the attributes in Z-Order are bitwise entangled, the original\nprogressive sampling algorithm cannot be applied directly. Thus,\nwe modify the original progressive sampling strategy so that it can\nbe efficiently adapted to CardIndex. Meanwhile, our pre- ğ‘˜neuron\nlinkage provides us with the ability to ignore unnecessary compu-\ntation by only focusing on the ğ‘˜dependent neurons. Therefore, we\nmodify the original progressive sampling method to enable quick\nexecution under Z-Order (lines 15-25). In line 19 of our algorithm,\n6\n\nwe exploit the property of our structureâ€™s pre- ğ‘˜connection by using\nonly the adjacent ğ‘˜bits of the input to the current sampled bit string\nand reusing some intermediate results generated in the previous\nstep to calculate the current probability information. This avoids\niterating from the head of the sampling sequence each time, saving\nthe overhead of repeated computations. Line 21 of our algorithm\nuses Z-encoded query box information to prune the obtained bit\ntransfer path.\nAlgorithm 3 Hybrid Estimation.\nRequire: CardIndex Structrue ğ‘€; QueryBox,ğ‘„; Sample Num: ğ‘ğ‘ ;\nEnsure: Cardinality Estimation of Query Box Q: ğ‘ğ‘ğ‘Ÿğ‘‘ ;\n1:procedure CardEst (ğ‘„,ğ‘€,ğ‘ğ‘ )\n2:ğ¶0=ğ¹ğ‘ğ‘ ğ‘¡ğ¶ğ·ğ¹ğ¸ğ‘ ğ‘¡(ğ‘€,ğ‘„) âŠ²Rough but fast est\n3: ifğ¶0â‰¤ğ‘ğ¸ğ‘†ğ‘‡then âŠ²Using index to scan small card\n4:ğ‘ğ‘ğ‘Ÿğ‘‘=ğ‘…ğ‘ğ‘›ğ‘”ğ‘’ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥(ğ‘€,ğ‘„)\n5: else\n6: âŠ²Using Progressive Sample to estimate big card\n7:ğ‘ğ‘ğ‘Ÿğ‘‘=ğ‘ğ‘ƒğ‘Ÿğ‘œğ‘”ğ‘Ÿğ‘’ğ‘ ğ‘ ğ‘–ğ‘£ğ‘’ğ‘†ğ‘ğ‘šğ‘ğ‘™ğ‘’ (ğ‘€,ğ‘„,ğ‘ğ‘ )\n8:returnğ‘ğ‘ğ‘Ÿğ‘‘\n9:procedure FastCDFEst (ğ‘„,ğ‘€ ) âŠ²Naive implement\n10:ğ‘ğ‘šğ‘–ğ‘›,ğ‘ğ‘šğ‘ğ‘¥=ğ‘ğ¸ğ‘›ğ‘ğ‘œğ‘‘ğ‘–ğ‘›ğ‘”(ğ‘„)\n11:ğ‘ğ‘‘ğ‘“ğ¿=ğ‘€(ğ‘ğ‘šğ‘–ğ‘›)\n12:ğ‘ğ‘‘ğ‘“ğ‘ˆ=ğ‘€(ğ‘ğ‘šğ‘ğ‘¥)\n13:returnğ‘ğ‘‘ğ‘“ğ‘ˆâˆ’ğ‘ğ‘‘ğ‘“ğ¿\n14:procedure ZProgressiveSample (ğ‘„,ğ‘€,ğ‘ğ‘ ) âŠ²\nProgressiveSample under Z-Curve\n15: Ë†ğ‘ƒ=0\n16: forğ‘–=1...ğ‘ğ‘ do\n17:ğ‘ğ‘–=1,s=0ğ‘›\n18:ğ‘€.ğ‘ğ‘™ğ‘’ğ‘ğ‘Ÿ() âŠ²Clear the intermediate result\n19: forğ‘—=1...ğ‘›do\n20: Ë†ğ‘ƒ(ğ‘ğ‘–=1|s<ğ‘–)=ğ‘€(ğ‘ ğ‘–âˆ’ğ‘˜,...ğ‘ ğ‘–âˆ’1)\n21: //Zero-Out the non-intercected bit branch\n22: Ë†ğ‘ƒ(ğ‘ğ‘–=1|s<ğ‘–)=ğ‘ğ‘Ÿğ‘¢ğ‘›ğµğ‘Ÿğ‘ğ‘›ğ‘â„(ğ‘„,s,ğ‘—)\n23: ğ‘ğ‘–=ğ‘ğ‘–Â·Ë†ğ‘ƒ(ğ‘ğ‘–=1|s<ğ‘–)\n24: ğ‘ ğ‘–=ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’\u0010\nË†ğ‘ƒ(ğ‘ğ‘–=1|s<ğ‘–)\u0011\n25: Ë†ğ‘ƒ=Ë†ğ‘ƒ+ğ‘ğ‘–\n26:return(Ë†ğ‘ƒ/ğ‘ğ‘ )Â·ğ‘\nAlthough the naive FastCDFEst algorithm may looks simple, it\nonly requires two inferences. It is already effective for the detec-\ntion of some small cardinality queries since the locality of Z-Order\nmakes the address of these low selectivity queries tend to be really\nclose under bitwise entangled encoding. However, there are also\nsome corner cases, whose region is discontinuous under Z-Order\ncoding(see Fig 4). This discontinuity, results in a very large esti-\nmation error of the naive FastCDFEst algorithm. Therefore, we\nimproved the FastCDFEst algorithm, which alleviates the above\nproblems. We discuss the improvement in Â§ 5.4.\n5.4 Rough CDF Estimation Improvement\nIn this section, we solve the problem left in the previous section.\nThat is, for some corner case queries, their interval under Z-Orderencoding is discontinuous. This large discontinuity leads to a large\nestimation error if we use naive FastCDFEst. Therefore, we develop\nan improved algorithm.\nThe intuition of our method is to conduct a binary search by\nprobing the query space under Z-Order. The pseudo-code of our\nalgorithm is shown in Algorithm 4. We recursively divide the search\nspace by half and accumulate the interval values on both sides(lines\n8-11). If the separation point is not in the query box, we use the\nmeans of calculating its ğ‘ğ‘–ğ‘”ğ‘šğ‘–ğ‘› andğ‘™ğ‘–ğ‘¡ğ‘šğ‘ğ‘¥ values to correct the\nafterward search space, such the discontinuous region in the middle\nis skipped (lines 12-18).\nAlgorithm 4 FastCDFEst-Refine.\nRequire: Lower bound Z-value ğ‘ğ‘šğ‘–ğ‘›; Upper bound Z-value ğ‘ğ‘šğ‘ğ‘¥;\nQueryBox,ğ‘„; CardIndex Structrue ğ‘€; Current Recursion depth\nğ‘‘;\nEnsure: Refined Upper Bound of Qâ€™s Selectivity: ğ‘ ğ‘’ğ‘™;\n1:procedure FastCDFEst (ğ‘ğ‘šğ‘–ğ‘›,ğ‘ğ‘šğ‘ğ‘¥,ğ‘„,ğ‘€,ğ‘‘ )\n2:ğ‘ğ‘‘ğ‘“ğ¿=ğ‘€(ğ‘ğ‘šğ‘–ğ‘›)\n3:ğ‘ğ‘‘ğ‘“ğ‘ˆ=ğ‘€(ğ‘ğ‘šğ‘ğ‘¥)\n4:ğ›¿=ğ‘ğ‘‘ğ‘“ğ‘ˆâˆ’ğ‘ğ‘‘ğ‘“ğ¿\n5: ifğ‘‘â‰¤0orğ›¿â‰¤ğ›¿0then âŠ²Search space pruning\n6:ğ‘ ğ‘’ğ‘™=ğ›¿\n7: else âŠ²Binary CDF Search\n8:ğ‘€ğ‘–ğ‘‘=(ğ¿+ğ‘ˆ)/2\n9: ifğ‘€âˆˆğ‘„then\n10: ğ‘0=ğ¹ğ‘ğ‘ ğ‘¡ğ¶ğ·ğ¹ğ¸ğ‘ ğ‘¡(ğ‘ğ‘šğ‘–ğ‘›,ğ‘€ğ‘–ğ‘‘,ğ‘„,ğ‘€,ğ‘‘âˆ’1)\n11: ğ‘1=ğ¹ğ‘ğ‘ ğ‘¡ğ¶ğ·ğ¹ğ¸ğ‘ ğ‘¡(ğ‘€ğ‘–ğ‘‘,ğ‘ğ‘šğ‘ğ‘¥,ğ‘„,ğ‘€,ğ‘‘âˆ’1)\n12: else\n13: ğ‘ğ‘–ğ‘”ğ‘šğ‘–ğ‘› =ğ‘”ğ‘’ğ‘¡ğµğ¼ğºğ‘€ğ¼ğ‘(ğ‘„,ğ‘€ğ‘–ğ‘‘)\n14: ğ‘™ğ‘–ğ‘¡ğ‘šğ‘ğ‘¥ =ğ‘”ğ‘’ğ‘¡ğ¿ğ¼ğ‘‡ğ‘€ğ´ğ‘‹(ğ‘„,ğ‘€ğ‘–ğ‘‘)\n15: ğ‘0=ğ¹ğ‘ğ‘ ğ‘¡ğ¶ğ·ğ¹ğ¸ğ‘ ğ‘¡(ğ‘ğ‘šğ‘–ğ‘›,ğ‘™ğ‘–ğ‘¡ğ‘šğ‘ğ‘¥,ğ‘„,ğ‘€,ğ‘‘âˆ’1)\n16: ğ‘1=ğ¹ğ‘ğ‘ ğ‘¡ğ¶ğ·ğ¹ğ¸ğ‘ ğ‘¡(ğ‘ğ‘–ğ‘”ğ‘šğ‘–ğ‘›,ğ‘ ğ‘šğ‘ğ‘¥,ğ‘„,ğ‘€,ğ‘‘âˆ’1)\n17:ğ‘ ğ‘’ğ‘™=ğ‘0+ğ‘1\n18:returnğ‘ ğ‘’ğ‘™\nIn order to realize efficient computation in a limited time, we\nprune the search space using the following strategies. If the search\ndepth exceeds budget (ğ‘‘â‰¤0)or the search spaceâ€™s selectivity is too\nlow(ğ›¿â‰¤ğ›¿0), we directly return the CDF distance between ğ‘ğ‘šğ‘–ğ‘›\nandğ‘ğ‘šğ‘ğ‘¥(lines 1-7). In the worst case, the time complexity of the\nabove algorithm is ğ‘‚(ğ‘›Ã—2ğ‘‘0), whereğ‘‘0is the maximum search\ndepth, andğ‘›is the Z-Ordered binary bit encoding length.\nWe use Fig 4 as an example to illustrate the above process. For\nthe query(ğ‘‹=0,4â‰¤ğ‘Œâ‰¤8)in Fig 4, direct use of all tuples\nbetween query endpoints 16-64 as an approximation will bring\ngreat errors. On the contrary, we take the middle point 40 and\ncalculate its ğ‘ğ‘–ğ‘”ğ‘šğ‘–ğ‘› 21 andğ‘™ğ‘–ğ‘¡ğ‘šğ‘ğ‘¥ 64. We then use them to narrow\nthe search space. We can observe that, through a single recursive\nsearch step, we can reduce the rough cardinality estimation of the\nquery region from 48 to 7, which is reduced to its original 1/7.\nA natural question is, does our proposed hybrid algorithm get\ncaught up in a trap searching too many tuples, resulting in per-\nformance degradation? We claim that this will never happen. The\nnumber of the tuples we scanned ğ‘ğ‘ ğ‘ğ‘ğ‘› is limited under our preset\nselectivity threshold ğ‘ğ¸ğ‘†ğ‘‡. The correctness is proved through the\nfollowing theorem.\n7\n\n0 2 8 10 32 34 40 42\n1 3 9 11 33 35 41 43\n4 6 12 14 36 38 44 46\n5 7 13 15 37 39 45 47\n16 18 24 26 48 50 56 58\n17 19 25 27 49 51 57 59\n20 22 28 30 52 54 60 62\n21 23 29 31 53 55 61 63\n64 66 72 74 96 98 104 10601234567\n0\n1\n2\n3\n4\n5\n6\n7\n8\n âˆ’ \n âˆ’ \n\nÂ Figure 4: Refining FastCDFEst\nTheorem 5.1. ğ‘ğ‘ ğ‘ğ‘ğ‘›â‰¤ğ‘ğ¸ğ‘†ğ‘‡Ã—ğ‘, whereğ‘ğ‘ ğ‘ğ‘ğ‘› is the number of\nthe tuples scanned, ğ‘is the cardinality of the whole table, and ğ‘ğ¸ğ‘†ğ‘‡\nis our preset selectivity threshold in Alg 3 .\nProof. (Sketch): Since we only perform index execution under\nthe condition that the estimated cost ğ¶0â‰¤ğ‘ğ¸ğ‘†ğ‘‡, the direction of\nour proof is to prove that ğ‘ğ‘ ğ‘ğ‘ğ‘›â‰¤ğ¶0Ã—ğ‘. In Â§ 5.2, we analyzed\nthat the number of tuples scanned ğ‘ğ‘ ğ‘ğ‘ğ‘› is smaller than ğµÃ—ğ‘›ğ¼,\nwhereğ‘›ğ¼is the number of pages intersecting the query box, and ğµ\nis the size of the page. According to this conclusion, we give the\nintuition that the results of both ğ¹ğ‘ğ‘ ğ‘¡ğ¶ğ·ğ¹ğ¸ğ‘ ğ‘¡ algorithms designed\nwill be larger than this value.\nFor naiveğ¹ğ‘ğ‘ ğ‘¡ğ¶ğ·ğ¹ğ¸ğ‘ ğ‘¡ , many Z-Ordered pages between ğ‘ğ‘šğ‘–ğ‘›\nandğ‘ğ‘šğ‘ğ‘¥ are not intersected by the query box. Therefore, ğµÃ—ğ‘›ğ¼â‰¤\n(ğ‘ğ‘‘ğ‘“ğ‘ˆâˆ’ğ‘ğ‘‘ğ‘“ğ¿)Ã—ğ‘. Given thatğ‘ğ‘ ğ‘ğ‘ğ‘›â‰¤ğµÃ—ğ‘›ğ¼andğ¶0=(ğ‘ğ‘‘ğ‘“ğ‘ˆâˆ’ğ‘ğ‘‘ğ‘“ğ¿)\nin naiveğ¹ğ‘ğ‘ ğ‘¡ğ¶ğ·ğ¹ğ¸ğ‘ ğ‘¡ , we conclude that ğ‘ğ‘ ğ‘ğ‘ğ‘›â‰¤ğ¶0Ã—ğ‘.\nFor improved ğ¹ğ‘ğ‘ ğ‘¡ğ¶ğ·ğ¹ğ¸ğ‘ ğ‘¡ , its recursive search, in fact, can be\nthought of as a pre-execution of limited getBIGMIN calls in a range\nquery. Due to its limited recursively search depth, it cannot remove\ntoo many discontinuous intervals, and thus the â€œjumped\" distance\nshould be less than the discontinuous Z-value interval of the â€œskip\"\nin real execution, which means that ((ğ‘ğ‘‘ğ‘“ğ‘ˆâˆ’ğ‘ğ‘‘ğ‘“ğ¿)Ã—ğ‘âˆ’ğ¶0Ã—ğ‘)â‰¤\n((ğ‘ğ‘‘ğ‘“ğ‘ˆâˆ’ğ‘ğ‘‘ğ‘“ğ¿)Ã—ğ‘âˆ’ğµÃ—ğ‘›ğ¼). Therefore,ğµÃ—ğ‘›ğ¼â‰¤ğ¶0Ã—ğ‘is obtained,\nwhich derives that ğ‘ğ‘ ğ‘ğ‘ğ‘›â‰¤ğ¶0Ã—ğ‘.\nIn summary, both of our algorithms will maintain ğ‘ğ‘ ğ‘ğ‘ğ‘›â‰¤ğ¶0Ã—ğ‘.\nSince we only execute ğ¶0â‰¤ğ‘ğ¸ğ‘†ğ‘‡queries, we derived that ğ‘ğ‘ ğ‘ğ‘ğ‘›â‰¤\nğ‘ğ¸ğ‘†ğ‘‡Ã—ğ‘. â–¡\nThe above upper bound ensures that the maximum number of\ntuples we scanned is limited to be smaller than ğ‘ğ¸ğ‘†ğ‘‡Ã—ğ‘when\nwe perform Alg 3â€™s hybrid method in obtaining the exact value\nof cardinality. Index scans are performed only when the scanning\ncost is lower than ğ‘ğ¸ğ‘†ğ‘‡Ã—ğ‘. Therefore, we just set the maximum\nindex scanning selectivity ğ‘ğ¸ğ‘†ğ‘‡beforehand. Our hybrid estimation\nstrategy will never fall into the trap of performing index scans on\nlarger cardinalities than ğ‘ğ¸ğ‘†ğ‘‡Ã—ğ‘.6 EXPERIMENTS\nIn this section, we answer the following questions experimentally.\n1. Compared with the state-of-the-art selectivity estimators, how\ndoes CardIndex perform in terms of the estimated quality of dif-\nferent workloads, model size, training time, and inference time?\n(Â§ 6.2)\n2. Compared with the state-of-the-art traditional multidimen-\nsional indexes, how does CardIndex perform in terms of range\nquery, point query, model size, and construction time? (Â§ 6.3)\n3. How do the techniques adopted in the approach, i.e. Pre- ğ‘˜\nneuron linkage, FastCDFEst, affect the quality and efficiency of the\nCardIndex structure? (Â§ 6.4)\n6.1 Experimental Setup\nEnvironment. The experiments are done on a computer with an\nAMD Ryzen 7 5800H CPU, NVIDIA RTX3060 GPU, 64 GB RAM,\nand a 1 TB hard disk. We use PyTorch in Python to train our model\non GPU and use C++ on CPU to infer our model on two tasks:\nIndex and Estimation. For the baselines of cardinality estimation,\nwe implemented them in Python, and for the baselines of indexing\ntasks, we implemented them in a C++ environment.\nCompetitors. We choose the following baselines in Index and\nEstimation.\n(1)Naru [24]: The state-of-the-art estimator on single tableâ€™s\ncardinality estimation. Using the autoregressive model and progres-\nsive sampling to estimate the query regionâ€™s cardinality. We used\nthe default net parameters in its methods\n(2)Sample : Sample several records in memory for CE. The sam-\npled Ratio is set as 1%.\n(3)DeepDB [10]: Use Sum-Product Network to estimate cardinal-\nity.\n(4)MHIST [18]: The multidimensional histogram is used to store\nthe PDF approximation and to predict the cardinality value based\non it.\n(5)KDB-Tree [21]: This index uses a KD-tree with a B-tree struc-\nture to support block storage. We adopt the state-of-the-art imple-\nmentation from Moinâ€™s baseline code in the Waffle project [15].\n(6)R-Tree :[5] The state-of-the-art traditional multidimensional\nindex. We adopt implementation from boost.org.\nDatasets. We use three real-world datasets and one synthetic\ndataset for experimental study on index and estimation tasks.\n(1)Power [4], an electric power consumption data, which owns\na large domain size in all attributes (each â‰ˆ2M). Our snapshot\ncontains 2,049,280 tuples with 6 columns.\n(2)DMV-INT [3], Real-world dataset consists of vehicle registra-\ntion information in New York. We use the following 11 columns\nwith widely differing data types and domain sizes (the numbers\nin parentheses): record type (4), reg class(75), state (89), county\n(63), body type (59), fuel type (9), valid date (2101), color (225),\nsco ind (2), sus ind (2), rev ind(2). Given that there are many non-\nnumeric data, we sort each column and convert each string of it to\nits corresponding integer coordinate value. Our snapshot contains\n12,300,116 tuples with 11 columns.\n(3)OSM [1], Real-world geographical data. We downloaded the\ndata set of Central America from OpenSteetMarket and selected\n8\n\ntwo columns of latitude and longitude. This data set has 80M tu-\nples(1.8GB in size). Our snapshot contains 80,000,000 tuples with\n2 columns.\n(4)Synthetic , Synthetic geographical data. We made a slight change\nto the OSM data and scaled it to 128M by copying its first 48M rows.\nOur snapshot contains 128,000,000 tuples with 2 columns.\nEvaluation Metrics. To evaluate the performance on index\nand estimation tasks, we adopt the following perspectives: Q-error\nmetric : defined as ğ‘„(ğ¸,ğ‘‡)=ğ‘šğ‘ğ‘¥{ğ¸\nğ‘‡,ğ‘‡\nğ¸}whereğ¸is the estimated\ncardinality value and ğ‘‡is the real cardinality. We reported the entire\nQ-error distribution as (50%, 95%, 99%, and Q-Max quantile) of each\nworkload. Latency metric : for index tasks, we report their average\nresponse time for each query execution. For estimation tasks, we\nreport the average inference time for each estimation. Size metric :\nWe report the total size of the non-leaf nodes for the index structure\nand the size of the overall structure for an estimator model.\nParameter Settings. We adopt the original settings of all the\nbaseline methods. For the CE tasks of our CardIndex(CI) in Â§ 6.2, we\nfixed the Minimum estimation bound ğ‘ğ¸ğ‘†ğ‘‡in our method as 1ğ‘’âˆ’2\nand Linked Neuronâ€™s number as 32. For our CIâ€™s index performance\nstudy in Â§ 6.3, we allocate the number of linear sub-models through\nthe following principle: For each 1M tuple, we assign 128 sub-\nmodels. Therefore, we allocate 256 sub-models on the Power data\nset, 1536 on the DMV data set, and 10272 on the OSM data set.\nThe linear error bound for the CardIndexâ€™s Linear Refinement is\nset to 1/10of the page CDF capacity by default, and the default\nminimum pruning precision of ğ›¿0of ourğ¹ğ‘ğ‘ ğ‘¡ğ¶ğ·ğ¹ğ¸ğ‘ ğ‘¡ is set to 0.001.\nThe fanout number is set to 100 for all index experiments including\nCIâ€™s submodels and other baseline methods.\nWorkloads. We generate multidimensional queries through\nthe following procedures. Firstly, we randomly select several rows\nfrom the data as the center of the query, select an integer from\nthe range of[1,ğ·]as the number of predicates, and then sample a\nnumber from(0,ğ‘ ğ‘–Â·ğ‘¤ğ‘–)among the selected columns as the width\nof the query, where ğ‘ ğ‘–is the scaling coefficient to generate some\nsmall cardinality queries, and ğ‘¤ğ‘–is the distance between the maxi-\nmum and minimum values of the column. We first generate several\nqueries under the three ğ‘ ğ‘–(1,0.01,0.001)and then mix them. Then,\naccording to their selection degree, they are divided into three\ngroups with each group having 1k queries, i.e. High( ğ‘ ğ‘’ğ‘™â‰¥1ğ‘’âˆ’2),\nHigh(ğ‘ ğ‘’ğ‘™â‰¥1ğ‘’âˆ’4), Extreame-Low ( ğ‘ ğ‘’ğ‘™â‰¤1ğ‘’âˆ’4). (See Fig.5)\n6.2 Estimation Evaluation\nIn general, in terms of estimation quality on large cardinalities\nworkloads, our method is slightly worse (2.5 Ã—) than the current\nSOTA method due to the parameter scale of our model being 2-\n3 orders of magnitude smaller than these models. However, the\nestimation quality on low selectivity workloads is 1.3-114 Ã—better\nthan the current SOTA data-driven cardinality estimation strategy.\nIn terms of the estimated latency, although there is a slight distance\nbetween our method with the direct sampling strategy, the average\nestimation latency of our method on CPU is up to 12 times faster\nthan the inference of Naruâ€™s method on GPU. In terms of training\ntime, under the same epoch iteration, our network training is 2\norders of magnitude faster than Naruâ€™s.\n/s80/s111/s119 /s101/s114 /s68/s77/s86 /s79/s83/s77/s49/s48/s45/s56/s49/s48/s45/s54/s49/s48/s45/s52/s49/s48/s45/s50/s49/s48/s48/s83/s101/s108/s101/s99/s116/s105/s118/s105/s116/s121/s32/s58/s72/s105/s103/s104\n/s32/s58/s76/s111/s119\n/s32/s58/s69/s120/s45/s76/s111/s119Figure 5: Distribution of workload selectivity\n(Sampled 10% from the query workload for demo)\nThe details are as follows. We tested the existing baseline esti-\nmators on three different workloads in three real-world datasets.\nThe number of the progressive samples in our Card Index (CI for\nshort) is fixed as 2,000 as Naruâ€™s.\nAs can be seen from Table 1, for large cardinality estimation,\nexisting cardinality estimators all perform well. Compared to the\nstate-of-the-art methods, our method performs slightly worse on\nthis type of load, where the maximum Q-error is twice as high as\nNaruâ€™s. This is because, under these high selectivity workloads, our\nhybrid estimation algorithm cannot use the index to enhance the\nperformance on the low selectivity query. Therefore, the method de-\ngenerates into a progressive sampling algorithm under tiny models.\nConsidering that our network has the smallest number of param-\neters, only 10-100KB of parameter size (1/100 - 1/1000 of Naruâ€™s),\nthe small parameters of the model make it impossible for us to\nuse a single structure to learn detailed fine-grained distribution\ninformation like Naruâ€™s, thus bringing errors to the estimation.\nWhen the selectivity of the query decreases, the advantage of our\nmethod is demonstrated via hybrid estimation. At Low selectivity\nworkloads(ğ‘ ğ‘’ğ‘™â‰¥1ğ‘’âˆ’4), the maximum Q-error is outperformed by\n2-114Ã—compared with Naruâ€™s and 125-2000Ã—compared with sam-\npling. On the Ex-Low workload, the performance distance is also\npronounced, i.e. the 1.3-38Ã—smaller Q-error against Naruâ€™s and\n3-222Ã—smaller than DeepDBâ€™s. We analyzed the reasons for our\nbaselineâ€™s poor performance as follows.\nUnder such low selectivity workload, it is often hard for Sample\nto extract elements from the queried region, so it tends to obtain the\nprediction result of 0 cards, resulting in Q-error being the size of the\nreal cardinality number. For DeepDB, it is difficult to accurately de-\nlineate correlations between columns, therefore for OSM and DMV\ndata, which may have a strong correlation between attributes, it\nproduces a large error. Although Naru can use autoregressive mod-\nels to learn correlations between attributes, it is still very difficult to\nobtain accurate estimations with such extremely-small cardinality.\nBecause such a small cardinality actually imposes a great demand\non the point density prediction accuracy and the number of progres-\nsive samples of the model. The limited progressive-sampling points\nin Naruâ€™s setting can hardly cover such a complex space intercepted\nby multiple range predicates. Therefore, insufficient sampling and\n9\n\nTable 1: Q-errors,Avg latency(ms) on 3 real-world Datasetsâ€™s 3 workloads\nDataSet EstimatorHigh(ğ‘ ğ‘’ğ‘™â‰¥1ğ‘’âˆ’2) LOW(ğ‘ ğ‘’ğ‘™â‰¥1ğ‘’âˆ’4) Extreame Low( ğ‘ ğ‘’ğ‘™â‰¤1ğ‘’âˆ’4)Time(ms)50th 95th 99th MAX 50th 95th 99th MAX 50th 95th 99th MAX\nPowerNaru 1.00 1.04 1.12 1.43 1.10 1.58 2.20 5.96 1.18 3.00 5.01 8.0 17.4\nSample 1.00 1.02 1.03 1.03 1.47 671 1ğ‘’31ğ‘’314.0 157 194 201 2.09\nDeepDB 1.00 1.01 1.02 1.18 1.08 2.44 5.23 18.8 1.30 4.67 10 37.0 12.98\nMHist 1.00 3.13 4.90 10.1 3.35 289 1ğ‘’39ğ‘’32.91 67.1 130 204 1086\nCI(d=0) 1.00 1.06 1.27 1.89 1.24 2.20 2.93 34.7 1.00 1.41 5.00 70.0 22.6\nCI(d=16) 1.00 1.06 1.27 1.89 1.00 1.06 1.31 2.12 1.00 1.00 1.00 11.0 17.08\nOSMNaru 1.00 1.08 1.13 1.28 1.18 2.91 6.79 432 2.81 42.2 141 311 84.1\nSample 1.00 1.01 1.02 1.02 1.22 2.24 293 2ğ‘’4238 4ğ‘’37ğ‘’37ğ‘’31.09\nDeepDB 1.00 1.01 1.01 1.03 1.24 1.93 3.57 11.7 2.89 56.9 254 2ğ‘’37.08\nMHist 1.08 1.57 2.37 4.15 2.67 26.1 51.7 159 15.3 1ğ‘’32ğ‘’35ğ‘’35235\nCI(d=0) 1.00 1.22 1.79 2.69 1.00 2.22 3.05 30.6 1.00 1.36 2.03 21.0 5.65\nCI(d=16) 1.00 1.22 1.79 2.69 1.00 1.24 1.70 10.9 1.00 1.00 1.35 8.94 7.06\nDMVNaru 1.00 1.04 1.06 1.09 1.90 3.56 47.3 943 2.98 4.3 55.9 250 30.9\nSample 1.00 1.02 1.06 2.59 1.14 1.79 17.5 1ğ‘’346.5 708 874 969 4.14\nDeepDB 1.92 2.11 2.66 33.7 1.98 8.18 131 6ğ‘’32.18 29.8 220 908 6.78\nMHist 1.93 2.62 3.20 35.6 3.57 19.4 50.1 685 13.0 174 399 686 1055\nCI(d=0) 1.00 1.08 1.32 2.75 1.42 3.00 3.97 36.6 2.43 8.22 17.0 147 11.5\nCI(d=16) 1.00 1.08 1.32 2.75 1.00 1.71 2.40 8.21 1.00 1.00 1.00 10.0 7.6\nlow precision point estimation result in large errors in prediction on\nsome corner cases with small cardinalities. To conclude, for these\nabove data-driven methods, it is impossible to learn completely\nprecise distribution information under a limited parameter storage\nbudget, and thus for these small cardinality estimations, there are\noften very large errors.\nIn contrast, with the help of an auxiliary index structure, we can\nefficiently search through the original table and retrieve the precise\nresults of these small cardinality queries. Thus, the advantage of our\nhybrid estimation is that the estimations of small cardinalities are\nperformed efficiently directly using the index to obtain a true and\nerror-free distribution. Although the method is efficient enough\nto be effective for 50% of Ex-low workload queries at ğ‘‘=0(the\nnaive implementation mentioned in Â§ 5.3), deeper search steps of\nFastCDFEst refine the search space, making our CardIndexâ€™s classifi-\ncation more precise and easier to identify small cardinality queries\nexecuted on the index. When the search depth increases from 0 to\n16, the maximum Q-error of the mixed estimation is reduced by\n10 times. The FastCDFEst method is extremely effective, as under\nPower and DMV dataset, 99% of the extremely Low workloads have\nbeen accurately identified and precisely estimated, with a Q-error\nof 1.0 by index execution.\nAt the same time, in terms of latency, it is worth acknowledging\nthat the time efficiency of sampling is the best. Given that we do not\nhave to perform that much computation on such methods, going\nthrough the sampled data is the only cost. Meanwhile, it should\nbe noted that although CardIndex uses progressive sampling as\nNaru does, the inference latency of progressive sampling of ours\nperformed on CPU is even 1.02-12 times faster than that of Naru\nmethod implemented on GPU. This is for two reasons. On the one\nhand, our networkâ€™s parameters are 1/100 of the size of Naruâ€™s, so\ninference does not require as much computation. On the other hand,\nfor small cardinality queries, direct execution on the fly will be muchfaster than entirely sampling estimation algorithms. Meanwhile, as\nour experimental results show, a larger depth does not necessarily\nlead to more time consumption. Meanwhile, since the direct index\nexecution time of a small cardinality is usually 2-3 orders of magni-\ntude smaller than these estimation algorithms, the larger the depth\nis, and the faster the estimated time on Power and DMV data sets is.\nWe also report the training time and the structure size of the\nestimation model in Fig 6(a) and Fig 6(b). We fixed the epoch of\ntraining to 20 in both neuro-based methods. It is noted that the\ntraining time of our network is 1-2 orders of magnitude faster than\nthat of Naru due to the network parameters of our models(CI-Core)\nbeing 1/100 of Naruâ€™s (6(a)).\n6.3 Index Evaluation\nStructure Construction. We report the size and construction\ntime of our structure and baseline approaches taken on Fig 6. It\nis not difficult to point out that, in terms of space, our structure\noccupies a space similar to the memory space of a traditional index\ncompared to the current mainstream traditional methods. Also\nin terms of building time, compared to other deep autoregressive\napproaches, our approach is much faster. (CIâ€™s construction is 2\norders of magnitude faster than Naruâ€™s). However, there is still\nroom for improvement in our approach compared to traditional\nindex building. (The construction time of CI is 10 times slower than\nKDB and RTree).\nPoint Query. We sample 1k points from each dataset, use them\nas query points and report the average response time. As Fig 7(a)\nshows, our CI has the best point performance on all datasets. It is\n30%âˆ’40%faster than the SOTA traditional structures on three real\ndata sets. The results in the synthetic data set are consistent with\nthose in the real data set. For example, the point query on the OSM\ndataset takes 4.4ğœ‡ğ‘ for KDB, 2.9ğœ‡ğ‘ for the R tree, and 2.2ğœ‡ğ‘ for our\nCardIndex. Given the fact that our structure is also a learned index,\n10\n\n/s80/s111/s119/s101/s114 /s68/s77/s86 /s79/s83/s77\n/s49/s48 /s49/s48/s48 /s49/s48/s48/s48 /s49/s48/s48/s48/s48\n/s67/s111/s110/s115/s116/s114/s117/s99/s116/s32/s84/s105/s109/s101/s40/s115/s41/s68/s97/s116/s97/s115/s101/s116/s32/s68/s105/s115/s116/s32/s68/s101/s101/s112/s68/s66\n/s32/s77/s72/s105/s115/s116\n/s32/s78/s97/s114/s117/s45/s84/s114/s97/s105/s110\n/s32/s67/s73/s45/s77/s97/s107/s101\n/s32/s67/s73/s45/s84/s114/s97/s105/s110\n/s32/s75/s68/s66\n/s32/s82/s84/s114/s101/s101(a) Construction Time\n/s80/s111/s119/s101/s114 /s68/s77/s86 /s79/s83/s77\n/s48/s46/s48/s48/s49 /s48/s46/s48/s49 /s48/s46/s49 /s49 /s49/s48 /s49/s48/s48\n/s83/s116/s114/s117/s99/s116/s117/s114/s101/s32/s83/s105/s122/s101/s40/s77/s66/s41/s68/s97/s116/s97/s115/s101/s116/s32/s68/s105/s115/s116/s32/s68/s101/s101/s112/s68/s66\n/s32/s77/s72/s105/s115/s116\n/s32/s78/s97/s114/s117\n/s32/s67/s73/s45/s67/s111/s114/s101\n/s32/s67/s73/s45/s76/s105/s110/s101/s97/s114\n/s32/s75/s68/s66\n/s32/s82/s84/s114/s101/s101 (b) Structure Size\n/s49 /s50 /s52 /s56 /s49/s54 /s51/s50 /s54/s52 /s49/s50/s56/s49/s49/s48/s49/s48/s48/s49/s48/s48/s48/s49/s48/s48/s48/s48\n/s67/s111/s110/s115/s116/s114/s117/s99/s116/s105/s111/s110/s32/s116/s105/s109/s101/s40/s115/s41\n/s68/s97/s116/s97/s115/s101/s116/s32/s83/s105/s122/s101/s32/s82/s84/s114/s101/s101\n/s32/s75/s68/s66\n/s32/s67/s73/s45/s84/s114/s97/s105/s110\n/s32/s67/s73/s45/s77/s97/s107/s101\n(c) Construction Scalability\n/s49 /s50 /s52 /s56 /s49/s54 /s51/s50 /s54/s52 /s49/s50/s56/s49/s49/s48/s49/s48/s48/s73/s110/s100/s101/s120/s83/s105/s122/s101/s40/s77/s66/s41\n/s68/s97/s116/s97/s32/s115/s101/s116/s32/s83/s105/s122/s101/s40/s77/s105/s108/s108/s105/s111/s110/s41/s32/s82/s84/s114/s101/s101\n/s32/s75/s68/s66\n/s32/s67/s73 (d) Size Scalability\nFigure 6: Structure Construction Info\nthis experimental fact is not surprising. Because our structure can\ncalculate the value of a given tuple coordinate in constant time, it\navoids the log-scale scanning traversal of blocks in the traditional\nstructure.\n/s80/s111/s119/s101/s114 /s68/s77/s86 /s79/s83/s77\n/s48/s46/s49 /s49 /s49/s48\n/s82/s101/s115/s112/s111/s110/s115/s101/s32/s84/s105/s109/s101/s40 /s109 /s115 /s41/s68/s97/s116/s97/s32/s68/s105/s115/s116\n/s32/s67/s73\n/s32/s75/s68/s66\n/s32/s82/s84/s114/s101/s101\n(a) Real-World Dist\n/s49 /s50 /s52 /s56 /s49/s54 /s51/s50 /s54/s52 /s49/s50/s56/s48/s46/s49/s49/s49/s48/s82/s101/s115/s112/s111/s110/s115/s101/s32/s84/s105/s109/s101/s40 /s109 /s115 /s41\n/s68/s97/s116/s97/s32/s115/s101/s116/s32/s83/s105/s122/s101/s40/s77/s105/s108/s108/s105/s111/s110/s41/s32/s82/s84/s114/s101/s101\n/s32/s75/s68/s66\n/s32/s67/s73 (b) Query Scalability\nFigure 7: Point Query Details\nRange Query . We directly evaluate the index task using two\nquery workloads generated when evaluating the cardinality es-\ntimator. We chose High( W1) and Low workloads( W2). As under\nExtremeLow workload, the result set is too small, and the result\nis quite similar to the point query. For the interests of space, we\ndid not show the corresponding experimental results. We find that\nthe average time of several structures is similar under High work-\nloads( W1). While on Low workloads( W2), our CardIndex is 4-10\ntimes faster than KDB and RTree. Indeed, RTree is twice as fast as\nour approach on the Power data set. Our debugging observations\nshow that this problem is related to the ordering of the untuned\nZ-Order curve may not be optimal for some certain range queries.\nThe average selection among the accessed blocks is so low that\nonly a small fraction of each fetched block actually falls into the\nquery area. Tuning Z-Order and a more compact block could solve\nthis problem. We will solve this problem in future work.\nTo sum up, our CardIndex is 30%-40% faster on point query\nprocessing task than SOTA on three real data sets. For range queries,\nthe performance is similar to that of R trees in general cases. And\n/s80/s45/s87/s49/s68/s45/s87/s49/s79/s45/s87/s49/s80/s45/s87/s50/s68/s45/s87/s50/s79/s45/s87/s50\n/s48/s46/s48/s49 /s48/s46/s49 /s49 /s49/s48 /s49/s48/s48\n/s82/s101/s115/s112/s111/s110/s115/s101/s32/s84/s105/s109/s101 /s40/s109/s115 /s41/s68/s97/s116/s97/s115/s101/s116/s32/s68/s105/s115/s116/s32/s67/s73\n/s32/s75/s68/s66\n/s32/s82/s84/s114/s101/s101(a) Real-World Dist\n/s49 /s50 /s52 /s56 /s49/s54 /s51/s50 /s54/s52 /s49/s50/s56/s48/s46/s49/s49/s49/s48/s49/s48/s48/s49/s48/s48/s48/s82/s101/s115/s112/s111/s110/s115/s101/s32/s84/s105/s109/s101 /s40/s109/s115 /s41\n/s68/s97/s116/s97/s32/s115/s101/s116/s32/s83/s105/s122/s101/s40/s77/s105/s108/s108/s105/s111/s110/s41/s32/s82/s84/s114/s101/s101\n/s32/s75/s68/s66\n/s32/s67/s73 (b) Query Scalability\nFigure 8: Range Query Details\nunder specific workloads, it can be 4 to 10 times faster than the\ntraditional structure. For range queries, the performance is similar\nto that of R trees. And under specific workloads, it can be 4 to 10\ntimes faster than the traditional structure\n6.4 Variance Evaluation\nIn this section, we evaluate the effects of techniques employed in\nour approach: pre- ğ‘˜neuron Linkage and FastCDFEst with a certain\ndepth. We use Ex-Low workload on the Power dataset.\n/s48 /s53 /s49/s48 /s49/s53 /s50/s48 /s50/s53 /s51/s48 /s51/s53/s49/s53/s49/s54/s49/s55/s49/s56/s49/s57/s50/s48/s50/s49/s50/s50/s50/s51\n/s32/s80/s83/s97/s109/s112/s108/s101/s45/s50/s48/s48/s48\n/s32/s80/s111/s105/s110/s116/s81/s117/s101/s114/s121\n/s76/s105/s110/s107/s45/s78/s117/s109/s83/s97/s109/s112/s108/s101/s84/s105/s109/s101/s40/s109/s115/s41\n/s49/s46/s48/s49/s46/s50/s49/s46/s52/s49/s46/s54/s49/s46/s56/s50/s46/s48/s50/s46/s50\n/s80/s111/s105/s110/s116/s81/s117/s101/s114/s121/s40 /s109 /s115/s41\n(a) Inference Time\n/s100/s61/s48 /s100/s61/s49 /s100/s61/s50 /s100/s61/s52 /s100/s61/s56/s49/s48/s49/s48/s48/s81/s45/s69/s114/s114/s111/s114\n/s32/s58/s76/s105/s110/s107/s49\n/s32/s58/s76/s105/s110/s107/s52\n/s32/s58/s76/s105/s110/s107/s49/s54 (b) top-10% Q-error\nFigure 9: Link\nEffect On Linkage Number. In Fig 9(a), we vary the Link-Num\nand report the average estimation time and point Query time of our\nmethod. It can be observed that as the number of links increases,\nmore computation is required, so the progressive sampling time\nand point query time get longer. The benefits brought by increasing\nthe number of connections are intuitive from Fig 9(b). When the\nnumber of connections is increased, the overall Q-error is reduced\nin all search depths. This is because, more connections of neurons\nshall bring a more powerful learned network, which makes the\nresult of progressive sampling more accurate.\nEffect On FastCDFEst Search Depth. We also report the effect\nof the search depth ğ‘‘on the quality of the estimation problem in\nFig 9(b). It is clear that deeper search depth leads to better estimation\nquality. For example, as search depth ğ‘‘increases from 0 to 8, the\nmaximum Q-error drops by almost an order of magnitude, even in\nthe case of Link-Number equals 1. The reason is that a deeper CDF\nsearch depth will locate small cardinality queries easier. Therefore,\nmore estimation queries are delegated to the index than inaccurate\nprogressive sampling, making Q-error lower.\n7 CONCLUSIONS\nIn this paper, we propose CardIndex, a lightweight multidimen-\nsional learned index with cardinality support, which killed two\n11\n\nbirds with one stone. These birds that our stone killed, not only\nin the literal sense, i.e. use a structure to solve both indexing and\nCE tasks. It is more in the obstacles we solve. (1) Our CardIndex\nfixed the duplication distribution storage waste. (2) Our CardIndex\nsolved the dilemma between the lightweight model deployment\nand corner-case accuracy. Our experiment validated that not only\ndoes our methodâ€™s index performance exceed traditional structures\n30%âˆ’40%in point query task, 4-10 Ã—in range query task, but also\noutperforms the state-of-the-art CE methods by 1.3-114 Ã—with 1-2\norders of magnitude smaller storage and computational overhead.\nREFERENCES\n[1] Openstreetmap data set., 2017. https://www.openstreetmap.org.\n[2] Python., 2017. https://www.python.org.\n[3]Vehicle, snowmobile, and boat registrations., 2019. catalog.data.gov/dataset/\nvehiclesnowmobile-and-boat-registrations.\n[4] Individual household electric power consumption data set., 2021. https://github.\ncom/gpapamak/maf.\n[5] N. Beckmann, H.-P. Kriegel, R. Schneider, and B. Seeger. The r*-tree: An efficient\nand robust access method for points and rectangles. In Proceedings of the 1990\nACM SIGMOD international conference on Management of data , pages 322â€“331,\n1990.\n[6]H. Dai and H.-C. Su. On the locality properties of space-filling curves. In\nAlgorithms and Computation: 14th International Symposium, ISAAC 2003, Kyoto,\nJapan, December 15-17, 2003. Proceedings 14 , pages 385â€“394. Springer, 2003.\n[7]J. Ding, V. Nathan, M. Alizadeh, and T. Kraska. Tsunami: A learned multi-\ndimensional index for correlated data and skewed workloads. arXiv preprint\narXiv:2006.13282 , 2020.\n[8] A. Galakatos, M. Markovitch, C. Binnig, R. Fonseca, and T. Kraska. Fiting-tree:\nA data-aware index structure. In Proceedings of the 2019 International Conference\non Management of Data , pages 1189â€“1206, 2019.\n[9] M. Germain, K. Gregor, I. Murray, and H. Larochelle. Made: Masked autoencoder\nfor distribution estimation. In International conference on machine learning , pages\n881â€“889. PMLR, 2015.\n[10] B. Hilprecht, A. Schmidt, M. Kulessa, A. Molina, K. Kersting, and C. Binnig.\nDeepdb: Learn from data, not from queries! arXiv preprint arXiv:1909.00607 ,\n2019.\n[11] M. Jasny, T. Ziegler, T. Kraska, U. Roehm, and C. Binnig. Db4ml-an in-memory\ndatabase kernel with machine learning support. In Proceedings of the 2020 ACM\nSIGMOD International Conference on Management of Data , pages 159â€“173, 2020.\n[12] Y. Ji, D. Amagata, Y. Sasaki, and T. Hara. A performance study of one-dimensional\nlearned cardinality estimation. In DOLAP , pages 86â€“90, 2022.\n[13] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned index\nstructures. In Proceedings of the 2018 international conference on management of\ndata, pages 489â€“504, 2018.\n[14] G. Li, X. Zhou, and L. Cao. Ai meets database: Ai4db and db4ai. In Proceedings\nof the 2021 International Conference on Management of Data , pages 2859â€“2866,\n2021.\n[15] M. H. Moti, P. Simatis, and D. Papadias. Waffle: A workload-aware and query-\nsensitive framework for disk-based spatial indexing. Proceedings of the VLDB\nEndowment , 16(4):670â€“683, 2022.\n[16] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Z. Yang, Z. DeVito, M. Raison,\nA. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch:\nAn imperative style, high-performance deep learning library. Neural Information\nProcessing Systems , 2019.\n[17] H.-B. Paul, H.-J. Schek, and M. H. Scholl. Architecture and implementation of\nthe darmstadt database kernel system. In Proceedings of the 1987 ACM SIGMOD\ninternational conference on Management of data , pages 196â€“207, 1987.\n[18] V. Poosala, P. J. Haas, Y. Ioannidis, and E. J. Shekita. Improved histograms for se-\nlectivity estimation of range predicates. International Conference on Management\nof Data , 1996.\n[19] J. Qi, G. Liu, C. S. Jensen, and L. Kulik. Effectively learning spatial indices.\nProceedings of the VLDB Endowment , 13(12):2341â€“2354, 2020.\n[20] F. Ramsak, V. Markl, R. Fenk, M. Zirkel, K. Elhardt, and R. Bayer. Integrating the\nub-tree into a database system kernel. In VLDB , volume 2000, pages 263â€“272,\n2000.\n[21] J. T. Robinson. The kdb-tree: a search structure for large multidimensional\ndynamic indexes. In Proceedings of the 1981 ACM SIGMOD international conference\non Management of data , pages 10â€“18, 1981.\n[22] H. Tropf and H. Herzog. Multidimensional range search in dynamically balanced\ntrees. ANGEWANDTE INFO. , (2):71â€“77, 1981.[23] H. Wang, X. Fu, J. Xu, and H. Lu. Learned index for spatial queries. In 2019\n20th IEEE International Conference on Mobile Data Management (MDM) , pages\n569â€“574, 2019.\n[24] Z. Yang, E. Liang, A. Kamsetty, C. Wu, Y. Duan, X. Chen, P. Abbeel, J. M. Heller-\nstein, S. Krishnan, and I. Stoica. Deep unsupervised cardinality estimation. arXiv\npreprint arXiv:1905.04278 , 2019.\n12",
  "textLength": 69435
}