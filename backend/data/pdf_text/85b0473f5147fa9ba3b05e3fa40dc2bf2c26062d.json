{
  "paperId": "85b0473f5147fa9ba3b05e3fa40dc2bf2c26062d",
  "title": "Learning-Based Low-Rank Approximations",
  "pdfPath": "85b0473f5147fa9ba3b05e3fa40dc2bf2c26062d.pdf",
  "text": "Learning-Based Low-Rank Approximations\nPiotr Indyk\u0003Ali VakilianyYang Yuanz\nAbstract\nWe introduce a \\learning-based\" algorithm for the low-rank decomposition problem: given\nann\u0002dmatrixA, and a parameter k, compute a rank- kmatrixA0that minimizes the ap-\nproximation loss kA\u0000A0kF. The algorithm uses a training set of input matrices in order to\noptimize its performance. Speci\fcally, some of the most e\u000ecient approximate algorithms for\ncomputing low-rank approximations proceed by computing a projection SA, whereSis a sparse\nrandomm\u0002n\\sketching matrix\", and then performing the singular value decomposition of SA.\nWe show how to replace the random matrix Swith a \\learned\" matrix of the same sparsity to\nreduce the error.\nOur experiments show that, for multiple types of data sets, a learned sketch matrix can\nsubstantially reduce the approximation loss compared to a random matrix S, sometimes by one\norder of magnitude. We also study mixed matrices where only some of the rows are trained and\nthe remaining ones are random, and show that matrices still o\u000ber improved performance while\nretaining worst-case guarantees.\nFinally, to understand the theoretical aspects of our approach, we study the special case of\nm= 1. In particular, we give an approximation algorithm for minimizing the empirical loss,\nwith approximation factor depending on the stable rank of matrices in the training set. We also\nshow generalization bounds for the sketch matrix learning problem.\n1. Introduction\nThe success of modern machine learning made it applicable to problems that lie outside of the\nscope of \\classic AI\". In particular, there has been a growing interest in using machine learning\nto improve the performance of \\standard\" algorithms, by \fne-tuning their behavior to adapt to\nthe properties of the input distribution, see e.g., [Wang et al., 2016, Khalil et al., 2017, Kraska\net al., 2018, Balcan et al., 2018, Lykouris and Vassilvitskii, 2018, Purohit et al., 2018, Gollapudi\nand Panigrahi, 2019, Mitzenmacher, 2018, Mousavi et al., 2015, Baldassarre et al., 2016, Bora et al.,\n2017, Metzler et al., 2017, Hand and Voroninski, 2018, Khani et al., 2019, Hsu et al., 2019]. This\n\\learning-based\" approach to algorithm design has attracted a considerable attention over the last\nfew years, due to its potential to signi\fcantly improve the e\u000eciency of some of the most widely\nused algorithmic tasks. Many applications involve processing streams of data (video, data logs,\ncustomer activity etc) by executing the same algorithm on an hourly, daily or weekly basis. These\ndata sets are typically not \\random\" or \\worst-case\"; instead, they come from some distribution\nwhich does not change rapidly from execution to execution. This makes it possible to design better\nalgorithms tailored to the speci\fc data distribution, trained on past instances of the problem.\nThe method has been particularly successful in the context of compressed sensing . In the latter\nframework, the goal is to recover an approximation to an n-dimensional vector x, given its \\linear\n\u0003CSAIL, MIT; indyk@mit.edu.\nyDept. of Computer Science, University of Wisconsin - Madison; vakilian@wisc.edu.\nzIIIS, Tsinghua University; yuanyang@tsinghua.edu.cn. This work was mostly done when the second and third\nauthors were at MIT.\n1arXiv:1910.13984v1  [cs.LG]  30 Oct 2019\n\nmeasurement\" of the form Sx, whereSis anm\u0002nmatrix. Theoretical results [Donoho, 2006,\nCand\u0012 es et al., 2006] show that, if the matrix Sis selected at random , it is possible to recover the\nklargest coe\u000ecients of xwith high probability using a matrix Swithm=O(klogn) rows. This\nguarantee is general and applies to arbitrary vectors x. However, if vectors xare selected from some\nnatural distribution (e.g., they represent images), recent works [Mousavi et al., 2015, Baldassarre\net al., 2016, Metzler et al., 2017] show that one can use samples from that distribution to compute\nmatricesSthat improve over a completely random matrix in terms of the recovery error.\nCompressed sensing is an example of a broader class of problems which can be solved using\nrandom projections. Another well-studied problem of this type is low-rank decomposition : given\nann\u0002dmatrixA, and a parameter k, compute a rank- kmatrix\n[A]k= argminA0:rank (A0)\u0014kkA\u0000A0kF:\nLow-rank approximation is one of the most widely used tools in massive data analysis, machine\nlearning and statistics, and has been a subject of many algorithmic studies. In particular, multiple\nalgorithms developed over the last decade use the \\sketching\" approach, see e.g., [Sarlos, 2006,\nWoolfe et al., 2008, Halko et al., 2011, Clarkson and Woodru\u000b, 2009, 2017, Nelson and Nguy^ en,\n2013, Meng and Mahoney, 2013, Boutsidis and Gittens, 2013, Cohen et al., 2015]. Its idea is to use\ne\u000eciently computable random projections (a.k.a., \\sketches\") to reduce the problem size before\nperforming low-rank decomposition, which makes the computation more space and time e\u000ecient.\nFor example, [Sarlos, 2006, Clarkson and Woodru\u000b, 2009] show that if Sis a random matrix of\nsizem\u0002nchosen from an appropriate distribution1, formdepending on \", then one can recover\na rank-kmatrixA0such that\nkA\u0000A0kF\u0014(1 +\u000f)kA\u0000[A]kkF\nby performing an SVD on SA2Rm\u0002dfollowed by some post-processing. Typically the sketch\nlengthmis small, so the matrix SAcan be stored using little space (in the context of streaming\nalgorithms) or e\u000eciently communicated (in the context of distributed algorithms). Furthermore,\nthe SVD of SAcan be computed e\u000eciently, especially after another round of sketching, reducing the\noverall computation time. See the survey [Woodru\u000b, 2014] for an overview of these developments.\nIn light of the aforementioned work on learning-based compressive sensing, it is natural to ask\nwhether similar improvements in performance could be obtained for other sketch-based algorithms,\nnotably for low-rank decompositions. In particular, reducing the sketch length mwhile preserving\nits accuracy would make sketch-based algorithms more e\u000ecient. Alternatively, one could make\nsketches more accurate for the same values of m. This is the problem we address in this paper.\nOur Results. Our main \fnding is that learned sketch matrices can indeed yield (much) more\naccurate low-rank decompositions than purely random matrices. We focus our study on a stream-\ning algorithm for low-rank decomposition due to [Sarlos, 2006, Clarkson and Woodru\u000b, 2009],\ndescribed in more detail in Section 2. Speci\fcally, suppose we have a training set of matrices\nTr=fA1;:::;ANgsampled from some distribution D. Based on this training set, we compute a\nmatrixS\u0003that (locally) minimizes the empirical loss\nX\nikAi\u0000SCW(S\u0003;Ai)kF (1.1)\n1Initial algorithms used matrices with independent sub-gaussian entries or randomized Fourier/Hadamard matri-\nces [Sarlos, 2006, Woolfe et al., 2008, Halko et al., 2011]. Starting from the seminal work of [Clarkson and Woodru\u000b,\n2017], researchers began to explore sparse binary matrices, see e.g., [Nelson and Nguy^ en, 2013, Meng and Mahoney,\n2013]. In this paper we mostly focus on the latter distribution.\n2\n\nwhere SCW( S\u0003;Ai) denotes the output of the aforementioned Sarlos-Clarkson-Woodru\u000b streaming\nlow-rank decomposition algorithm on matrix Aiusing the sketch matrix S\u0003. Once the sketch matrix\nS\u0003is computed, it can be used instead of a random sketch matrix in all future executions of the\nSCW algorithm.\nWe demonstrate empirically that, for multiple types of data sets, an optimized sketch matrix S\u0003\ncan substantially reduce the approximation loss compared to a random matrix S, sometimes by one\norder of magnitude (see Figure 5.1 or 5.2). Equivalently, the optimized sketch matrix can achieve\nthe same approximation loss for lower values of mwhich results in sketching matrices with lower\nspace usage. Note that since we augment a streaming algorithm, our main focus is on improving its\nspace usage (which in the distributed setting translates into the amount of communication). The\nlatter isO(md), the size of SA.\nA possible disadvantage of learned sketch matrices is that an algorithm that uses them no longer\no\u000bers worst-case guarantees. As a result, if such an algorithm is applied to an input matrix that\ndoes not conform to the training distribution, the results might be worse than if random matrices\nwere used. To alleviate this issue, we also study mixed sketch matrices, where (say) half of the\nrows are trained and the other half are random. We observe that if such matrices are used in\nconjunction with the SCW algorithm, its results are no worse than if only the random part of the\nmatrix was used (Theorem 4.1 in Section 4)2. Thus, the resulting algorithm inherits the worst-case\nperformance guarantees of the random part of the sketching matrix. At the same time, we show\nthat mixed matrices still substantially reduce the approximation loss compared to random ones,\nin some cases nearly matching the performance of \\pure\" learned matrices with the same number\nof rows. Thus, mixed random matrices o\u000ber \\the best of both worlds\": improved performance for\nmatrices from the training distribution, and worst-case guarantees otherwise.\nFinally, in order to understand the theoretical aspects of our approach further, we study the\nspecial case of m= 1. This corresponds to the case where the sketch matrix Sis just a single\nvector. Our results are two-fold:\n\u000fWe give an approximation algorithm for minimizing the empirical loss as in Equation 1.1,\nwith an approximation factor depending on the stable rank of matrices in the training set.\nSee Appendix B.\n\u000fUnder certain assumptions about the robustness of the loss minimizer, we show generalization\nbounds for the solution computed over the training set. See Appendix C.\nThe theoretical results on the case of m= 1 are deferred to the full version of this paper.\n1.1. Related work\nAs outlined in the introduction, over the last few years there has been multiple papers exploring\nthe use of machine learning methods to improve the performance of \\standard\" algorithms. Among\nthose, the closest to the topic of our paper are the works on learning-based compressive sensing,\nsuch as [Mousavi et al., 2015, Baldassarre et al., 2016, Bora et al., 2017, Metzler et al., 2017], and on\nlearning-based streaming algorithms [Hsu et al., 2019]. Since neither of these two lines of research\naddresses computing matrix spectra, the technical development therein was quite di\u000berent from\nours.\nIn this paper we focus on learning-based optimization of low-rank approximation algorithms\nthat use linear sketches , i.e., map the input matrix AintoSAand perform computation on the\n2We note that this property is non-trivial, in the sense that it does not automatically hold for allsketching\nalgorithms. See Section 4 for further discussion.\n3\n\nlatter. There are other sketching algorithms for low-rank approximation that involve non-linear\nsketches [Liberty, 2013, Ghashami and Phillips, 2014, Ghashami et al., 2016]. The bene\ft of linear\nsketches is that they are easy to update under linear changes to the matrix A, and (in the context\nof our work) that they are easy to di\u000berentiate, making it possible to compute the gradient of the\nloss function as in Equation 1.1. We do not know whether it is possible to use our learning-based\napproach for non-linear sketches, but we believe this is an interesting direction for future research.\n2. Preliminaries\nNotation. Consider a distribution Don matrices A2Rn\u0002d. We de\fne the training set as\nfA1;\u0001\u0001\u0001;ANgsampled fromD. For matrix A, its singular value decomposition (SVD) can be\nwritten asA=U\u0006V>such that both U2Rn\u0002nandV2Rd\u0002nhave orthonormal columns and\n\u0006 = diagf\u00151;\u0001\u0001\u0001;\u0015dgis a diagonal matrix with nonnegative entries. Moreover, if rank( A) =r,\nthen the \frst rcolumns of Uare an orthonormal basis for the column space ofA(we denote it as\ncolsp(A)), the \frst rcolumns of Vare an orthonormal basis for the row space ofA(we denote it\nas rowsp(A))3and\u0015i= 0 fori > r . In many applications it is quicker and more economical to\ncompute the compact SVD which only contains the rows and columns corresponding to the non-zero\nsingular values of \u0006: A=Uc\u0006c(Vc)>whereUc2Rn\u0002r;\u0006c2Rr\u0002randVc2Rd\u0002r.\nHow sketching works. We start by describing the SCW algorithm for low-rank matrix approx-\nimation, see Algorithm 1. The algorithm computes the singular value decomposition of SA=\nU\u0006V>, and compute the best rank- kapproximation of AV. Finally it outputs [ AV]kV>as a rank-\nkapproximation of A. We emphasize that Sarlos and Clarkson-Woodru\u000b proposed Algorithm 1\nwith random sketching matrices S. In this paper, we follow the same framework but use learned\n(or partially learned) matrices.\nAlgorithm 1 Rank-kapproximation of a matrix Ausing a sketch matrix S(refer to Section 4.1.1\nof [Clarkson and Woodru\u000b, 2009])\n1:Input:A2Rn\u0002d;S2Rm\u0002n\n2:U;\u0006;V> CompactSVD (SA)Bfr= rank(SA);U2Rm\u0002r;V2Rd\u0002rg\n3:Return: [AV]kV>\nNote that if mis much smaller than dandn, the space bound of this algorithm is signi\fcantly\nbetter than when computing a rank- kapproximation for Ain the na \u0010ve way. Thus, minimizing m\nautomatically reduces the space usage of the algorithm.\nSketching matrix. We use matrix Sthat is sparse4Speci\fcally, each column of Shas exactly\none non-zero entry, which is either +1 or \u00001. This means that the fraction of non-zero entries in\nSis 1=m. Therefore, one can use a vector to represent S, which is very memory e\u000ecient. It is\nworth noting, however, after multiplying the sketching matrix Swith other matrices, the resulting\nmatrix (e.g., SA) is in general not sparse.\n3. Training Algorithm\nIn this section, we describe our learning-based algorithm for computing a data dependent sketch\nS. The main idea is to use backpropagation algorithm to compute the stochastic gradient of S\n3The remaining columns of UandVrespectively are orthonormal bases for the nullspace of AandA>.\n4The original papers [Sarlos, 2006, Clarkson and Woodru\u000b, 2009] used dense matrices, but the work of [Clarkson\nand Woodru\u000b, 2017] showed that sparse matrices work as well. We use sparse matrices since they are more e\u000ecient\nto train and to operate on.\n4\n\nwith respect to the rank- kapproximation loss in Equation 1.1, where the initial value of Sis\nthe same random sparse matrix used in SCW. Once we have the stochastic gradient, we can run\nstochastic gradient descent (SGD) algorithm to optimize S, in order to improve the loss. Our\nalgorithm maintains the sparse structure of S, and only optimizes the values of the nnon-zero\nentries (initially +1 or \u00001).\nAlgorithm 2 Di\u000berentiable SVD implementation\n1:Input:A12Rm\u0002dwherem<d\n2:U;\u0006;V fg;fg;fg\n3:fori 1:::m do\n4:v1 random initialization in Rd\n5:fort 1:::Tdo\n6:vt+1 A>\niAivt\nkA>\niAivtk2Bfpower methodg\n7:end for\n8:V[i] vT+1\n9: \u0006[i] kAiV[i]k2\n10:U[i] AiV[i]\n\u0006[i]\n11:Ai+1 Ai\u0000\u0006[i]U[i]V[i]>\n12:end for\n13:Return:U;\u0006;Vv1\nvt+1 A>\niAivt\nkA>\niAivtk2\u0002Ttimes\nU\n\u0006\nVU[i]\n\u0006[i]\nV[i]\nFigure 3.1: i-th iteration of power method\nHowever, the standard SVD implementation (step 2 in Algorithm 1 ) is not di\u000berentiable,\nwhich means we cannot get the gradient in the straightforward way. To make SVD implementation\ndi\u000berentiable, we use the fact that the SVD procedure can be represented as mindividual top\nsingular value decompositions (see e.g. [Allen-Zhu and Li, 2016]), and that every top singular value\ndecomposition can be computed using the power method. See Figure 3.1 and Algorithm 2. We\nstore the results of the i-th iteration into the i-th entry of the list U;\u0006;V, and \fnally concatenate\nall entries together to get the matrix (or matrix diagonal) format of U;\u0006;V. This allows gradients\nto \row easily.\nDue to the extremely long computational chain, it is infeasible to write down the explicit form\nof loss function or the gradients. However, just like how modern deep neural networks compute\ntheir gradients, we used the autograd feature in PyTorch to numerically compute the gradient with\nrespect to the sketching matrix S.\nWe emphasize again that our method is only optimizing Sfor the training phase. After Sis\nfully trained, we still call Algorithm 1 for low rank approximation, which has exactly the same\nrunning time as the SCW algorithm, but with better performance (i.e., the quality of the returned\nrank-kmatrix). We remark that the time complexity of SCW algorithm is O(nmd) assuming\nk\u0014m\u0014min(n;d).\n4. Worst Case Bound\nIn this section, we show that concatenating two sketching matrices S1andS2(of size respectively\nm1\u0002nandm2\u0002n) into a single matrix S\u0003(of size (m1+m2)\u0002n) will not increase the approximation\nloss of the \fnal rank- ksolution computed by Algorithm 1 compared to the case in which only one\nofS1orS2are used as the sketching matrix. In the rest of this section, the sketching matrix S\u0003\n5\n\ndenotes the concatenation of S1andS2as follows:\nS\u0003((m1+m2)\u0002n)=2\n4S1(m1\u0002n)\nS2(m2\u0002n)3\n5\nFormally, we prove the following theorem on the worst case performance of mixed matrices .\nTheorem 4.1. LetU\u0003\u0006\u0003V>\n\u0003andU1\u00061V>\n1respectively denote the SVD of S\u0003AandS1A. Then,\njj[AV\u0003]kV>\n\u0003\u0000AjjF\u0014jj[AV1]kV>\n1\u0000AjjF:\nIn particular, the above theorem implies that the output of Algorithm 1 with the sketching\nmatrixS\u0003is a better rank- kapproximation to Acompared to the output of the algorithm with S1.\nIn the rest of this section we prove Theorem 4.1.\nBefore proving the main theorem, we state the following helpful lemma.\nLemma 4.2 (Lemma 4.3 in [Clarkson and Woodru\u000b, 2009]). Suppose that Vis a matrix\nwith orthonormal columns. Then, a best rank-k approximation to Ain the colsp(V)is given by\n[AV]kV>.\nSince the above statement is a transposed version of the lemma from [Clarkson and Woodru\u000b,\n2009], we include the proof in the appendix for completeness.\nProof of Theorem 4.1. First, we show that colsp( V1)\u0012colsp(V\u0003). By the properties of the (com-\npact) SVD, colsp( V1) = rowsp( S1A) and colsp( V\u0003) = rowsp( S\u0003A). Since,S\u0003has all rows of S1,\nthen\ncolsp(V1)\u0012colsp(V\u0003): (4.1)\nBy Lemma 4.2,\njjA\u0000[AV\u0003]kV>\n\u0003jjF= min\nrowsp(X)\u0012colsp(V\u0003);\nrank(X)\u0014kjjX\u0000AjjF\njjA\u0000[AV1]kV>\n1jjF= min\nrowsp(X)\u0012colsp(V1);\nrank(X)\u0014kjjX\u0000AjjF\nFinally, together with (4.1),\njjA\u0000[AV\u0003]kV>\n\u0003jjF= min\nrowsp(X)\u0012colsp(V\u0003);\nrank(X)\u0014kjjX\u0000AjjF\n\u0014 min\nrowsp(X)\u0012colsp(V1);\nrank(X)\u0014kjjX\u0000AjjF=jjA\u0000[AV1]kV>\n1jjF:\nwhich completes the proof. \u0003\nFinally, we note that the property of Theorem 4.1 is not universal, i.e., it does not hold for all\nsketching algorithms for low-rank decomposition. For example, an alternative algorithm proposed\nin [Cohen et al., 2015] proceeds by letting Zto be the top ksingular vectors of SA(i.e.,Z=V\nwhere [SA]k=U\u0006VT) and then reports AZZ>. It is not di\u000ecult to see that, by adding extra\nrows to the sketching matrix S(which may change all top ksingular vectors compared to the ones\nofSA), one can skew the output of the algorithm so that it is far from the optimal.\n6\n\n5. Experimental Results\nThe main question considered in this paper is whether, for natural matrix datasets, optimizing\nthe sketch matrix Scan improve the performance of the sketching algorithm for the low-rank\ndecomposition problem. To answer this question, we implemented and compared the following\nmethods for computing S2Rm\u0002n.\n\u000fSparse Random . Sketching matrices are generated at random as in [Clarkson and Woodru\u000b,\n2017]. Speci\fcally, we select a random hash function h: [n]![m], and for all i= 1:::n,\nSh[i];iis selected to be either +1 or \u00001 with equal probability. All other entries in Sare set\nto 0. Therefore, Shas exactly nnon-zero entries.\n\u000fDense Random . All thenmentries in the sketching matrices are sampled from Gaussian\ndistribution (we include this method for comparison).\n\u000fLearned . Using the sparse random matrix as the initialization, we run Algorithm 2 to\noptimize the sketching matrix using the training set, and return the optimized matrix.\n\u000fMixed (J) . We \frst generate two sparse random matrices S1;S22Rm\n2\u0002n(assumingmis\neven), and de\fne Sto be their combination. We then run Algorithm 2 to optimize Susing\nthe training set, but only S1will be updated, while S2is \fxed. Therefore, Sis a mixture\nof learned matrix and random matrix, and the \frst matrix is trained jointly with the second\none.\n\u000fMixed (S) . We \frst compute a learned matrix S12Rm\n2\u0002nusing the training set, and then\nappend another sparse random matrix S2to getS2Rm\u0002n. Therefore, Sis a mixture of\nlearned matrix and random matrix, but the learned matrix is trained separately .\nLogo Eagle Friends Hyper Tech02468Test Error\n0.1 0.2 0.20.52.8\n1.94.0 4.1\n3.17.9\n2.04.7\n4.03.57.8 Learned\nSparse Random\nDense Random\nFigure 5.1: Test error by datasets and sketching matrices for k= 10;m= 20\n20 40 60 80\nm101\n100Test ErrorLearned\nSparse Random\nDense Random\n20 40 60 80\nm100101Test ErrorLearned\nSparse Random\nDense Random\n20 40 60 80\nm100101Test ErrorLearned\nSparse Random\nDense Random\nFigure 5.2: Test error for Logo (left), Hyper (middle) and Tech (right) when k= 10.\nDatasets. We used a variety of datasets to test the performance of our methods:\n7\n\n\u000fVideos5: Logo, Friends, Eagle. We downloaded three high resolution videos from Youtube,\nincluding logo video, Friends TV show, and eagle nest cam. From each video, we collect 500\nframes of size 1920 \u00021080\u00023 pixels, and use 400 (100) matrices as the training (test) set.\nFor each frame, we resize it as a 5760 \u00021080 matrix.\n\u000fHyper. We use matrices from HS-SOD, a dataset for hyperspectral images from natural\nscenes [Imamoglu et al., 2018]. Each matrix has 1024 \u0002768 pixels, and we use 400 (100)\nmatrices as the training (test) set.\n\u000fTech. We use matrices from TechTC-300, a dataset for text categorization [Davidov et al.,\n2004]. Each matrix has 835 ;422 rows, but on average only 25 ;389 of the rows contain non-zero\nentries. On average each matrix has 195 columns. We use 200 (95) matrices as the training\n(test) set.\nEvaluation metric. To evaluate the quality of a sketching matrix S, it su\u000eces to evaluate the\noutput of Algorithm 1 using the sketching matrix Son di\u000berent input matrices A. We \frst de\fne\nthe optimal approximation loss for test set Teas follows: App\u0003\nTe,EA\u0018TekA\u0000[A]kkF:\nNote that App\u0003\nTedoes not depend on S, and in general it is not achievable by any sketch Swith\nm<d , because of information loss. Based on the de\fnition of the optimal approximation loss, we\nde\fne the error of the sketch SforTeas Err( Te;S),EA\u0018TekA\u0000SCW (S;A)kF\u0000App\u0003\nTe:\nIn our datasets, some of the matrices have much larger singular values than the others. To\navoid imbalance in the dataset, we normalize the matrices so that their top singular values are all\nequal.\nFigure 5.3: Low rank approximation results for Logo video frame: the best rank-10 approximation\n(left), and rank-10 approximations reported by Algorithm 1 using a sparse learned sketching matrix\n(middle) and a sparse random sketching matrix (right).\n5.1. Average test error\nWe \frst test all methods on di\u000berent datasets, with various combination of k;m. See Figure 5.1 for\nthe results when k= 10;m= 20. As we can see, for video datasets, learned sketching matrices can\nget 20\u0002better test error than the sparse random or dense random sketching matrices. For other\ndatasets, learned sketching matrices are still more than 2 \u0002better. In this experiment, we have\nrun each con\fguration 5 times, and computed the standard error of each test error6. For Logo,\nEagle, Friends, Hyper and Tech, the standard errors of learned ,sparse random and dense random\nsketching matrices are respectively, (1 :5;8:4;35:3;124;41)\u000210\u00006, (3:1;5:3;7:0;2:9;4:5)\u000210\u00002and\n(3:5;18:1;4:6;10:7;3:3)\u000210\u00002. It is clear that the standard error of the learned sketching matrix\nis a few order of magnitudes smaller than the random sketching matrices, which shows another\nbene\ft of learning sketching matrices.\n5They can be downloaded from http://youtu.be/L5HQoFIaT4I ,http://youtu.be/xmLZsEfXEgE and http://\nyoutu.be/ufnf_q_3Ofg\n6They were very small, so we did not plot in the \fgures\n8\n\nTable 5.1: Test error in various settings\nk;m; Sketch Logo Eagle Friends Hyper Tech\n10;10;Learned 0.39 0.31 1.03 1.25 6.70\n10;10;Random 5.22 6.33 11.56 7.90 17.08\n10;20;Learned 0.10 0.18 0.22 0.52 2.95\n10;20;Random 2.09 4.31 4.11 2.92 7.99\n20;20;Learned 0.61 0.66 1.41 1.68 7.79\n20;20;Random 4.18 5.79 9.10 5.71 14.55\n20;40;Learned 0.18 0.41 0.42 0.72 3.09\n20;40;Random 1.19 3.50 2.44 2.23 6.20\n30;30;Learned 0.72 1.06 1.78 1.90 7.14\n30;30;Random 3.11 6.03 6.27 5.23 12.82\n30;60;Learned 0.21 0.61 0.42 0.84 2.78\n30;60;Random 0.82 3.28 1.79 1.88 4.84Table 5.2: Comparison with mixed sketches\nk;m; Sketch Logo Hyper Tech\n10;10;Learned 0.39 1.25 6.70\n10;10;Random 5.22 7.90 17.08\n10;20;Learned 0.10 0.52 2.95\n10;20;Mixed (J) 0.20 0.78 3.73\n10;20;Mixed (S) 0.24 0.87 3.69\n10;20;Random 2.09 2.92 7.99\n10;40;Learned 0.04 0.28 1.16\n10;40;Mixed (J) 0.05 0.34 1.31\n10;40;Mixed (S) 0.05 0.34 1.20\n10;40;Random 0.45 1.12 3.28\n10;80;Learned 0.02 0.16 0.31\n10;80;Random 0.09 0.32 0.80\nSimilar improvement of the learned sketching matrices over the random sketching matrices can\nbe observed when k= 10;m= 10;20;30;40;\u0001\u0001\u0001;80, see Figure 5.2. We also include the test error\nresults in Table 5.1 for the case when k= 20;30. Finally, in Figure 5.3, we visualize an example\noutput of the algorithm for the case k= 10;m= 20 for the Logo dataset.\n5.2. Comparing Random, Learned and Mixed\nIn Table 5.2, we investigate the performance of the mixed sketching matrices by comparing them\nwith random and learned sketching matrices. In all scenarios, the mixed sketching matrices yield\nmuch better results than the random sketching matrices, and sometimes the results are comparable\nto those of learned sketching matrices. This means, in most cases it su\u000eces to train half of the\nsketching matrix to obtain good empirical results, and at the same time, by our Theorem 4.1, we\ncan use the remaining random half of the sketching matrix to obtain worst-case guarantees.\nMoreover, if we do not \fx the number of learned rows to be half, the test error increases as\nthe number of learned rows decreases. In Figure 5.4, we plot the test error for the setting with\nm= 20;k= 10 using 100 Logo matrices, running for 3000 iterations.\n0 10 20\n#Learned Rows0.00.51.01.52.0Test Error\nFigure 5.4: Test errors of mixed sketching matrices with di\u000berent number of \\learned\" rows.\n5.3. Mixing Training Sets\nIn our previous experiments, we constructed a di\u000berent learned sketching matrix Sfor each data\nset. However, one can use a single random sketching matrix for all three data sets simultaneously.\nNext, we study the performance of a single learned sketching matrix for all three data sets. In\nTable 5.3, we constructed a single learned sketching matrix Swithm=k= 10 on a training set\n9\n\ncontaining 300 matrices from Logo, Eagle and Friends (each has 100 matrices). Then, we tested S\non Logo matrices and compared its performance to the performance of a learned sketching matrix\nSLtrained on Logo dataset (i.e., using 100 Logo matrices only), as well as to the performance of a\nrandom sketching SR. The performance of the sketching matrix Swith a mixed training set from\nall three datasets is close to the performance of the sketching matrix SLwith training set only from\nLogo dataset, and is much better than the performance of the random sketching matrix SR.\nTable 5.3: Evaluation of the sketching matrix trained on di\u000berent sets\nLogo+Eagle+Friends Logo only Random\nTest Error 0.67 0.27 5.19\n5.4. Running Time\nThe runtimes of the algorithm with a random sketching matrix and our learned sketching matrix\nare the same, and are much less than the runtime of the \\standard\" SVD method (implemented in\nPytorch). In Table 5.4, we present the runtimes of the algorithm with di\u000berent types of sketching\nmatrices (i.e., learned andrandom ) on Logo matrices with m=k= 10, as well as the training time\nof the learned case. Notice that training only needs to be done once, and can be done o\u000fine.\nTable 5.4: Runtimes of the algorithm with di\u000berent sketching matrices\nSVD Random Learned-Inference Learned-Training\n2.2s 0.03s 0.03s 9481.25s\n6. Conclusions\nIn this paper we introduced a learning-based approach to sketching algorithms for computing low-\nrank decompositions. Such algorithms proceed by computing a projection SA, whereAis the input\nmatrix and Sis a random \\sketching\" matrix. We showed how to train Susing example matrices\nAin order to improve the performance of the overall algorithm. Our experiments show that for\nseveral di\u000berent types of datasets, a learned sketch can signi\fcantly reduce the approximation loss\ncompared to a random matrix. Further, we showed that if we mix a random matrix and a learned\nmatrix (by concatenation), the result still o\u000bers an improved performance while inheriting worst\ncase guarantees of the random sketch component.\nAcknowledgment\nThis research was supported by NSF TRIPODS award #1740751 and Simons Investigator Award.\nThe authors would like to thank the anonymous reviewers for their insightful comments and sug-\ngestions.\nReferences\nZ. Allen-Zhu and Y. Li. Lazysvd: even faster svd decomposition yet without agonizing pain. In\nAdvances in Neural Information Processing Systems , pages 974{982, 2016.\nM.-F. Balcan, T. Dick, T. Sandholm, and E. Vitercik. Learning to branch. In International\nConference on Machine Learning , pages 353{362, 2018.\nL. Baldassarre, Y.-H. Li, J. Scarlett, B. G ozc u, I. Bogunovic, and V. Cevher. Learning-based\ncompressive subsampling. IEEE Journal of Selected Topics in Signal Processing , 10(4):809{822,\n2016.\n10\n\nA. Bora, A. Jalal, E. Price, and A. G. Dimakis. Compressed sensing using generative models. In\nInternational Conference on Machine Learning , pages 537{546, 2017.\nC. Boutsidis and A. Gittens. Improved matrix algorithms via the subsampled randomized hadamard\ntransform. SIAM Journal on Matrix Analysis and Applications , 34(3):1301{1340, 2013.\nE. J. Cand\u0012 es, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction\nfrom highly incomplete frequency information. IEEE Transactions on information theory , 52(2):\n489{509, 2006.\nK. L. Clarkson and D. P. Woodru\u000b. Numerical linear algebra in the streaming model. In Proceedings\nof the forty-\frst annual symposium on Theory of computing (STOC) , pages 205{214, 2009.\nK. L. Clarkson and D. P. Woodru\u000b. Low-rank approximation and regression in input sparsity time.\nJournal of the ACM (JACM) , 63(6):54, 2017.\nM. B. Cohen, S. Elder, C. Musco, C. Musco, and M. Persu. Dimensionality reduction for k-\nmeans clustering and low rank approximation. In Proceedings of the forty-seventh annual ACM\nsymposium on Theory of computing , pages 163{172, 2015.\nD. Davidov, E. Gabrilovich, and S. Markovitch. Parameterized generation of labeled datasets\nfor text categorization based on a hierarchical directory. In Proceedings of the 27th Annual\nInternational ACM SIGIR Conference on Research and Development in Information Retrieval ,\nSIGIR '04, pages 250{257, 2004.\nD. L. Donoho. Compressed sensing. IEEE Transactions on information theory , 52(4):1289{1306,\n2006.\nM. Ghashami and J. M. Phillips. Relative errors for deterministic low-rank matrix approximations.\nInProceedings of the twenty-\ffth annual ACM-SIAM symposium on Discrete algorithms (SODA) ,\npages 707{717, 2014.\nM. Ghashami, E. Liberty, J. M. Phillips, and D. P. Woodru\u000b. Frequent directions: Simple and\ndeterministic matrix sketching. SIAM Journal on Computing , 45(5):1762{1792, 2016.\nS. Gollapudi and D. Panigrahi. Online algorithms for rent-or-buy with expert advice. In Interna-\ntional Conference on Machine Learning , pages 2319{2327, 2019.\nN. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic\nalgorithms for constructing approximate matrix decompositions. SIAM review , 53(2):217{288,\n2011.\nP. Hand and V. Voroninski. Global guarantees for enforcing deep generative priors by empirical\nrisk. In Conference On Learning Theory , 2018.\nS. Har-Peled, P. Indyk, and R. Motwani. Approximate nearest neighbor: Towards removing the\ncurse of dimensionality. Theory of computing , 8(1):321{350, 2012.\nC.-Y. Hsu, P. Indyk, D. Katabi, and A. Vakilian. Learning-based frequency estimation algorithms.\nInternational Conference on Learning Representations , 2019.\nN. Imamoglu, Y. Oishi, X. Zhang, G. Ding, Y. Fang, T. Kouyama, and R. Nakamura. Hyperspectral\nimage dataset for benchmarking on salient object detection. In Tenth International Conference\non Quality of Multimedia Experience, (QoMEX) , pages 1{3, 2018.\n11\n\nE. Khalil, H. Dai, Y. Zhang, B. Dilkina, and L. Song. Learning combinatorial optimization algo-\nrithms over graphs. In Advances in Neural Information Processing Systems , pages 6348{6358,\n2017.\nM. Khani, M. Alizadeh, J. Hoydis, and P. Fleming. Adaptive neural signal detection for massive\nMIMO. CoRR , abs/1906.04610, 2019.\nT. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned index structures.\nInProceedings of the 2018 International Conference on Management of Data , pages 489{504,\n2018.\nE. Liberty. Simple and deterministic matrix sketching. In Proceedings of the 19th ACM SIGKDD\ninternational conference on Knowledge discovery and data mining , pages 581{588, 2013.\nT. Lykouris and S. Vassilvitskii. Competitive caching with machine learned advice. In International\nConference on Machine Learning , pages 3302{3311, 2018.\nX. Meng and M. W. Mahoney. Low-distortion subspace embeddings in input-sparsity time and\napplications to robust linear regression. In Proceedings of the forty-\ffth annual ACM symposium\non Theory of computing , pages 91{100, 2013.\nC. Metzler, A. Mousavi, and R. Baraniuk. Learned d-amp: Principled neural network based\ncompressive image recovery. In Advances in Neural Information Processing Systems , pages 1772{\n1783, 2017.\nM. Mitzenmacher. A model for learned bloom \flters and optimizing by sandwiching. In Advances\nin Neural Information Processing Systems , pages 464{473, 2018.\nA. Mousavi, A. B. Patel, and R. G. Baraniuk. A deep learning approach to structured signal\nrecovery. In Communication, Control, and Computing (Allerton), 2015 53rd Annual Allerton\nConference on , pages 1336{1343. IEEE, 2015.\nJ. Nelson and H. L. Nguy^ en. Osnap: Faster numerical linear algebra algorithms via sparser subspace\nembeddings. In Foundations of Computer Science (FOCS), 2013 IEEE 54th Annual Symposium\non, pages 117{126, 2013.\nM. Purohit, Z. Svitkina, and R. Kumar. Improving online algorithms via ml predictions. In\nAdvances in Neural Information Processing Systems , pages 9661{9670, 2018.\nT. Sarlos. Improved approximation algorithms for large matrices via random projections. In 47th\nAnnual IEEE Symposium on Foundations of Computer Science (FOCS) , pages 143{152, 2006.\nS. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algo-\nrithms . Cambridge University Press, 2014.\nJ. Wang, W. Liu, S. Kumar, and S.-F. Chang. Learning to hash for indexing big data - a survey.\nProceedings of the IEEE , 104(1):34{57, 2016.\nD. P. Woodru\u000b. Sketching as a tool for numerical linear algebra. Foundations and Trends R\rin\nTheoretical Computer Science , 10(1{2):1{157, 2014.\nF. Woolfe, E. Liberty, V. Rokhlin, and M. Tygert. A fast randomized algorithm for the approxi-\nmation of matrices. Applied and Computational Harmonic Analysis , 25(3):335{366, 2008.\n12\n\nA. The case of m= 1\nIn this section, we denote the SVD of AasUA\u0006A(VA)>such that both UAandVAhave or-\nthonormal columns and \u0006A= diagf\u0015A\n1;\u0001\u0001\u0001;\u0015A\ndgis a diagonal matrix with nonnegative entries. For\nsimplicity, we assume that for all A\u0018D, 1 =\u00151\u0015\u0001\u0001\u0001\u0015\u0015d. We useUA\nito denote the i-th column\nofUA, and similarly for VA\ni. Denote \u0006A= diagf\u0015A\n1;\u0001\u0001\u0001;\u0015A\ndg.\nWe want to \fnd [ A]k, the rank-kapproximation of A. In general, it is hard to obtain a closed\nform expression of the output of Algorithm 1. However, for m= 1, such expressions can be\ncalculated. Indeed, if m= 1, the sketching matrix becomes a vector s2R1\u0002n. Therefore [ AV]k\nhas rank at most one, so it su\u000eces to set k= 1. Consider a matrix A\u0018D as the input to Algorithm\n1. By calculation, SA=P\ni\u0015A\nihs;UA\nii(VA\ni)>, which is a vector. For example, if S=UA\n1, we obtain\n\u0015A\n1(VA\n1)>. Note that in this section to emphasize that m= 1 (i.e.,Sis a vector), we denote Sas\ns. SinceSAis a vector, applying SVD on it is equivalent to performing normalization. Therefore,\nV=Pd\ni=1\u0015A\nihs;UA\nii(VA\ni)>\nqPd\ni=1(\u0015A\ni)2hs;UA\nii2\nIdeally, we hope that Vis as close to VA\n1as possible, because that means [ AV]1V>is close to\n\u0015A\n1UA\n1(VA\n1)>, which captures the top singular component of A, i.e., the optimal solution. More\nformally,\nAV=Pd\ni=1(\u0015A\ni)2hs;UA\niiUA\niqPd\ni=1(\u0015A\ni)2hs;UA\nii2\nWe want to maximize its norm, which is:\nPd\ni=1(\u0015A\ni)4hs;UA\nii2\nPd\ni=1(\u0015A\ni)2hs;UA\nii2(A.1)\nWe note that one can simplify (A.1) by considering only the contribution from the top left singular\nvectorUA\n1, which corresponds to the maximization of the following expression:\nhs;UA\n1i2\nPd\ni=1(\u0015A\ni)2hs;UA\nii2(A.2)\nB. Optimization Bounds\nMotivated by the empirical success of sketch optimization, we investigate the complexity of op-\ntimizing the loss function. We focus on the simple case where m= 1 and therefore Sis just a\n(dense) vector. Our main observation is that a vector spicked uniformly at random from thed-\ndimensional unit sphere achieves an approximately optimal solution, with the approximation factor\ndepending on the maximum stable rank of matrices A1;\u0001\u0001\u0001;AN. This algorithm is not particularly\nuseful for our purpose, as our goal is to improve over the random choice of the sketching matrix S.\nNevertheless, it demonstrates that an algorithm with a non-trivial approximation factor exists.\nDe\fnition B.1 (stable rank ( r0)).For a matrix A, the stable rank of Ais de\fned as the squared\nratio of Frobenius and operator norm of A. I.e.,\nr0(A) =jjAjj2\nF\njjAjj2\n2=P\ni(\u0015A\ni)2\nmaxi(\u0015A\ni)2:\n13\n\nNote that since we assume for all matrices A\u0018D, 1 =\u0015A\n1\u0015\u0001\u0001\u0001\u0015\u0015A\nd>0, for all these matrices\nr0(A) =P\ni(\u0015A\ni)2.\nFirst, we consider the simpli\fed objective function as in (A.2).\nLemma B.2. A random vector swhich is picked uniformly at random from the d-dimensional\nunit sphere, is an O(r0)-approximation to the optimum value of the simpli\fed objective function in\nEquation (A.2) , wherer0is the maximum stable rank of matrices A1;\u0001\u0001\u0001;AN.\nProof: We will show that\nE\"\nhs;UA\n1i2\nPd\ni=1(\u0015A\ni)2hs;UA\nii2#\n= \n(1=r0(A))\nfor allA\u0018D wheresis a vector picked uniformly at random from Sd\u00001. Since for all A\u0018D we\nhavehs;UA\n1i2\nPd\ni=1(\u0015A\ni)2hs;UA\nii2\u00141, by the linearity of expectation we have that the vector sachieves an\nO(r0)-approximation to the maximum value of the objective function,\nNX\nj=1hs;UAj\n1i2\nPd\ni=1(\u0015Aj\ni)2hs;UAj\nii2:\nFirst, recall that to sample suniformly at random from Sd\u00001we can generate sasPd\ni=1\u000biUA\niqPd\ni=1\u000b2\ni\nwhere for all i\u0014d,\u000bi\u0018N (0;1). This helps us evaluate E\u0014\nhs;UA\n1i2\nPd\ni=1(\u0015A\ni)2hs;UA\nii2\u0015\nfor an arbitrary\nmatrixA\u0018D:\nE=E\"\nhs;UA\n1i2\nPd\ni=1(\u0015A\ni)2hs;UA\nii2#\n=E\"\n(\u000b1)2\nPd\ni=1(\u0015A\ni\u0001\u000bi)2#\n\u0015E\"\n(\u000b1)2\nPd\ni=1(\u0015A\ni\u0001\u000bi)2j\t1\\\t2#\n\u0001Pr(\t 1\\\t2)\nwhere the events \t 1;\t2are de\fned as:\n\t1,1\u0014\nj\u000b1j\u00151\n2\u0015\n;and \t 2,1\"dX\ni=2(\u0015A\ni)2(\u000bi)2\u00142\u0001r0(A)#\nSince\u000bis are independent, we have\nE\u0015E\u0014(\u000b1)2\n(\u000b1)2+ 2\u0001r0(A)j\t1\\\t2\u0015\n\u0001Pr(\t 1)\u0001Pr(\t 2)\u00151\n8\u0001r0(A) + 1\u0001Pr(\t 1)\u0001Pr(\t 2)\nwhere we used that(\u000b1)2\n(\u000b1)2+2\u0001r0(A)is increasing for ( \u000b1)2\u00151\n4. It remains to prove that Pr(\t 1);Pr(\t 2) =\n\u0002(1). We observe that, since \u000bi\u0018N(0;1), we have\nPr(\t 1) = Pr\u0012\nj\u000b1j\u00151\n2\u0013\n= \u0002(1)\nSimilarly, by Markov inequality, we have\nPr(\t 2) = Pr dX\ni=1(\u0015A\ni)2(\u000bi)2\u00142r0(A)!\n\u00151\u0000Pr dX\ni=1(\u0015A\ni)2(\u000bi)2>2r0(A)!\n\u00151\n2\u0003\n14\n\nNext, we prove that a random vector s2Sd\u00001achieves an O(r0(A))-approximation to the\noptimum of the main objective function as in Equation (A.1).\nLemma B.3. A random vector swhich is picked uniformly at random from the d-dimensional unit\nsphere, is an O(r0)-approximation to the optimum value of the objective function in Equation (A.1) ,\nwherer0is the maximum stable rank of matrices A1;\u0001\u0001\u0001;AN.\nProof: We assume that the vector sis generated via the same process as in the proof of Lemma B.2.\nIt follows that\nE\"Pd\ni=1(\u0015A\ni)4hs;UA\nii2\nPd\ni=1(\u0015A\ni)2hs;UA\nii2#\n\u0015E\"\n(\u000b1)2\nPd\ni=1(\u0015A\ni)2\u0001(\u000bi)2#\n= \n(1=r0(A))\u0003\nC. Generalization Bounds\nDe\fne the loss function as\nL(s),\u0000EA\u0018D\"Pd\ni=1\u0000\n\u0015A\ni\u00014hs;UA\nii2\nPd\ni=1\u0000\n\u0015A\ni\u00012hs;UA\nii2#\nWe want to \fnd a vector s2Sd\u00001to minimize L(s), where Sd\u00001is thed-dimensional unit\nsphere. SinceDis unknown, we are optimizing the following empirical loss:\n^LTr(s),\u00001\nNNX\nj=12\n64Pd\ni=1\u0010\n\u0015Aj\ni\u00114\nhs;UAj\nii2\nPd\ni=1\u0010\n\u0015Aj\ni\u00112\nhs;UAj\nii23\n75\nThe importance of robust solutions We start by observing that if sminimizes the training\nloss^L, it is not necessarily true that sis the optimal solution for the population loss L. For example,\nit could be the case that fAjgj=1;\u0001\u0001\u0001;Nare diagonal matrices with only 1 non-zeros on the top row,\nwhiles= (\u000f;p\n1\u0000\u000f2;0;\u0001\u0001\u0001;0) for\"close to 0. In this case, we know that ^LTr(s) =\u00001, which is at\nits minimum value.\nHowever, such a solution is not robust. In the population distribution, if there exists a matrix\nAsuch thatA= diag(p\n1\u0000100\"2;10\";0;0;\u0001\u0001\u0001;0), insertsinto (A.1),\nPd\ni=1\u0000\n\u0015A\ni\u00014hs;UA\nii2\nPd\ni=1\u0000\n\u0015A\ni\u00012hs;UA\nii2=(1\u0000100\"2)2\"2+ 104\"4(1\u0000\"2)\n(1\u0000100\"2)\"2+ 100\"2(1\u0000\"2)<\"2+ 104\"4\n101\"2\u0000100\"4=1 + 104\"2\n101\u0000100\"2\nThe upper bound is very close to 0 if \"is small enough. This is because when the denominator is\nextremely small, the whole expression is susceptible to minor perturbations on A. This is a typical\nexample showing the importance of \fnding a robust solution. Because of this issue, we will show a\ngeneralization guarantee for a robust solutions.\nDe\fnition of robust solution First, de\fne event \u0010A;\u000e;s,1hPd\ni=1\u0000\n\u0015A\ni\u00012hs;UA\nii2<\u000ei\n, which is\nthe denominator in the loss function. Ideally, we want this event to happen with a small probability,\nwhich indicates that for most matrices, the denominator is large, therefore sis robust in general.\nWe have the following de\fnition of robustness.\nDe\fnition C.1 (( \u001a;\u000e)-robustness). sis (\u001a;\u000e)-robust with respect to DifEA\u0018D[\u0010A;\u000e;s]\u0014\u001a.s\nis (\u001a;\u000e)-robust with respect to TrifEA\u0018Tr[\u0010A;\u000e;s]\u0014\u001a.\n15\n\nFor a givenD, we can de\fne robust solution set that includes all robust vectors.\nDe\fnition C.2 (( \u001a;\u000e)-robust set). M D;\u001a;\u000eis de\fned to be the set of all vectors s2Sd\u00001s.t.s\nis (\u001a;\u000e)-robust with respect to D.\nEstimating M D;\u001a;\u000e The drawback of the above de\fnition is that MD;\u001a;\u000eis de\fned by the unknown\ndistributionD, so for \fxed \u000eand\u001a, we cannot tell whether sis inMD;\u001a;\u000eor not. However, we can\nestimate the robustness of susing the training set. Speci\fcally, we have the following lemma:\nLemma C.3 (Estimating robustness). For a training set Trof sizeNsampled uniformly at\nrandom fromD, and a given s2Rd, a constant 1>\u0011 > 0, ifsis(\u001a;\u000e)-robust with respect to Tr,\nthen with probability at least 1\u0000e\u0000\u00112pN\n2,sis\u0010\n\u001a\n1\u0000\u0011;\u000e\u0011\n-robust with respect to D.\nProof: Suppose that Pr A\u0018D[\u0010A;\u000e;s] =\u001a1, which means E\u0002P\nAi2Tr\u0010Ai;\u000e;s\u0003\n=\u001a1N. Since events\n\u0010Ai;\u000e;s's are 0-1 random variables, by Cherno\u000b bound,\nPr0\n@X\nAi2Tr\u0010Ai;\u000e;s\u0014(1\u0000\u0011)\u001a1N1\nA\u0014e\u0000\u00112\u001a1N\n2\nIf\u001a1< \u001a < \u001a= (1\u0000\u0011), our claim is immediately true. Otherwise, we know e\u0000\u00112\u001a1N\n2\u0014e\u0000\u00112\u001aN\n2.\nHence, with probability at least 1 \u0000e\u0000\u00112\u001aN\n2,N\u001a=P\nAi\u0018Tr\u0010Ai;\u000e;s>(1\u0000\u0011)\u001a1N. This implies that\nwith probability at least 1 \u0000e\u0000\u00112\u001aN\n2,\u001a1\u0014\u001a\n1\u0000\u0011. \u0003\nLemma C.3 implies that for a \fxed solution s, if it is (\u001a;\u000e)-robust in Tr, it is also ( O(\u001a);\u000e)-\nrobust inDwith high probability. However, Lemma C.3 only works for a single solution s, but\nthere are in\fnitely many potential son thed-dimensional unit sphere.\nTo remedy this problem, we discretize the unit sphere to bound the number of potential solu-\ntions. Classical results tell us that discretizing the unit sphere into a grid of edge length\"p\ndgives\nC\n\"dpoints on the grid for some constant C(e.g., see Section 3.3 in [Har-Peled et al., 2012] for more\ndetails). We will only consider these points as potential solutions, denoted as ^Bd. Thus, we can\n\fnd a \\robust\" solution s2^Bdwith decent probability, using Lemma C.3 and union bound.\nLemma C.4 (Picking robust s).For a \fxed constant \u001a>0;1>\u0011> 0, with probability at least\n1\u0000C\n\"de\u0000\u00112\u001aN\n2, any (\u001a;\u000e)-robusts2^Bdwith respect to Tris\u0010\n\u001a\n1\u0000\u0011;\u000e\u0011\n-robust with respect to D.\nSince we are working on the discretized solution, we need a new de\fnition of robust set.\nDe\fnition C.5 (Discretized ( \u001a;\u000e)-robust set). ^MD;\u001a;\u000eis de\fned to be the set of all vector\ns2^Bds.t.sis (\u001a;\u000e)-robust with respect to D.\nUsing similar arguments as Lemma C.4, we know all solutions from ^MD;\u001a;\u000eare robust with\nrespect to Tras well.\nLemma C.6. With probability at least 1\u0000C\n\"de\u0000\u00112\u001aN\n3, for a constant \u0011>0, all solutions in ^MD;\u001a;\u000e,\nare((1 +\u0011)\u001a;\u000e)-robust with respect to Tr.\nProof: Consider a \fxed solution s2^MD;\u001a;\u000e. Note that E\u0002P\nAi2Tr\u0010Ai;\u000e;s\u0003\n=\u001aNand\u0010Ai;\u000e;sare 0-1\nrandom variables. Therefore by Cherno\u000b bound,\nPr0\n@X\nAi2Tr\u0010Ai;\u000e;s\u0015(1 +\u0011)\u001aN1\nA\u0014e\u0000\u00112\u001aN\n3:\n16\n\nHence, with probability at least 1 \u0000e\u0000\u00112\u001aN\n3,sis ((1 +\u0011)\u001a;\u000e)-robust with respect to Tr.\nBy union bound on all points in ^MD;\u001a;\u000e\u0012^Bd, the proof is complete. \u0003\nC.1. Generalization bound\nFinally, we show the generalization bounds for robust solutions,. To this can we use Rademacher\ncomplexity to prove generalization bound. De\fne Rademacher complexity R(^MD;\u001a;\u000e\u000eTr) as\nR(^MD;\u001a;\u000e\u000eTr),1\nNE\n\u001b\u0018f\u00061gNsup\ns2^MD;\u001a;\u000eNX\nj=12\n64\u001bjPd\ni=1\u0010\n\u0015Aj\ni\u00114\nhs;UAj\nii2\nPd\ni=1\u0010\n\u0015Aj\ni\u00112\nhs;UAj\nii23\n75:\nR(^MD;\u001a;\u000e\u000eTr) is handy, because we have the following theorem (notice that the loss function\ntakes value in [\u00001;0]):\nTheorem C.7 (Theorem 26.5 in [Shalev-Shwartz and Ben-David, 2014]). Given constant\n\u000e>0, with probability of at least 1\u0000\u000e, for alls2^MD;\u001a;\u000e,\nL(s)\u0000^LTr(s)\u00142R(^MD;\u001a;\u000e\u000eTr) + 4r\n2 log(4=\u000e)\nN\nThat means, it su\u000eces to bound R(^MD;\u001a;\u000e\u000eTr) to get the generalization bound. We have the\nfollowing Lemma.\nLemma C.8 (Bound on R(^MD;\u001a;\u000e\u000eTr)).For a constant \u0011 > 0, with probability at least 1\u0000\nC\n\"de\u0000\u00112pN\n3,R(^MD;\u001a;\u000e\u000eTr)\u0014(1 +\u0011)\u001a+1\u0000\u000e\n2\u000e+dp\nN.\nProof: De\fne\u001a0= (1 +\u0011)\u001a. By Lemma C.6, we know that with probability 1 \u0000C\n\"de\u0000\u00112pN\n3, any\ns2^MD;\u001a;\u000eis (\u001a0;\u000e)-robust with respect to Tr, henceP\nA2Tr\u0010A;\u000e;s\u0014\u001a0N. The analysis below is\nconditioned on this event.\nDe\fnehA;\u000e;s,maxf\u000e;Pd\ni=1(\u0015A\ni)2hs;UA\nii2g. We know that with probability 1 \u0000C\n\"de\u0000\u00112pN\n3,\nN\u0001R(^MD;\u001a;\u000e\u000eTr) =E\u001b\u0018f\u00061gNsup\ns2^MD;\u001a;\u000eNX\nj=1\u001bjPd\ni=1\u0010\n\u0015Aj\ni\u00114\nhs;UAj\nii2\nPd\ni=1\u0010\n\u0015Aj\ni\u00112\nhs;UAj\nii2\n\u0014\u001a0N+E\u001bsup\ns2^MD;\u001a;\u000eNX\nj=1\u001bjPd\ni=1\u0010\n\u0015Aj\ni\u00114\nhs;UAj\nii2\nhA;\u000e;s(C.1)\nwhere (C.1) holds because by de\fnition, hA;\u000e;s\u0015Pd\ni=1(\u0015A\ni)2hs;UA\nii2if and only if \u0010A;\u000e;s= 1, which\nhappens for at most \u001a0Nmatrices. Note that for any matrix Aj,\u001bjPd\ni=1\u0010\n\u0015Aj\ni\u00114\nhs;UAj\nii2\nPd\ni=1\u0010\n\u0015Aj\ni\u00112\nhs;UAj\nii2\u00141.\nNow,\nE\u001bsup\ns2^MD;\u001a;\u000eNX\nj=1\u001bjPd\ni=1\u0010\n\u0015Aj\ni\u00114\nhs;UAj\nii2\nhA;\u000e;s\n\u0014E\u001bsup\ns2^MD;\u001a;\u000eNX\nj=10\nB@1\u001bj=1Pd\ni=1\u0010\n\u0015Aj\ni\u00114\nhs;UAj\nii2\n\u000e\u00001\u001bj=\u00001dX\ni=1\u0010\n\u0015Aj\ni\u00114\nhs;UAj\nii21\nCA (C.2)\n17\n\n=E\u001bsup\ns2^MD;\u001a;\u000eNX\nj=1 \n\u001bjdX\ni=1\u0010\n\u0015Aj\ni\u00114\nhs;UAj\nii2+1\u001bj=1dX\ni=1\u0010\n\u0015Aj\ni\u00114\nhs;UAj\nii2\u00121\n\u000e\u00001\u0013!\n\u0014N\n2\u000e\u0000N\n2+E\u001bsup\ns2^MD;\u001a;\u000eNX\nj=1\u001bjdX\ni=1\u0010\n\u0015Aj\ni\u00114\nhs;UAj\nii2(C.3)\nThe \frst inequality, (C.2), holds asPd\ni=1\u0010\n\u0015Aj\ni\u00114\nhs;UAj\nii2\nhA;\u000e;s2[\u000e;1]. It remains to bound the last\nterm (C.3).\nE\u001bsup\ns2^MD;\u001a;\u000eNX\nj=1\u001bjdX\ni=1\u0010\n\u0015Aj\ni\u00114\nhs;UAj\nii2\u0014dX\ni=1E\u001bsup\ns2^MD;\u001a;\u000eNX\nj=1\u001bj\u001c\ns;\u0010\n\u0015Aj\ni\u00112\nUAj\ni\u001d2\n(C.4)\nBy contraction lemma of Rademacher complexity, we have\nE\u001bsup\ns2^MD;\u001a;\u000eNX\nj=1\u001bj\u001c\ns;\u0010\n\u0015Aj\ni\u00112\nUAj\ni\u001d2\n\u0014E\u001bsup\ns2^MD;\u001a;\u000eNX\nj=1\u001bj\u001c\ns;\u0010\n\u0015Aj\ni\u00112\nUAj\ni\u001d\n=E\u001bsup\ns2^MD;\u001a;\u000e*\ns;NX\nj=1\u001bj\u0010\n\u0015Aj\ni\u00112\nUAj\ni+\n\u0014E\u001b\r\r\r\r\r\rNX\nj=1\u001bj\u0010\n\u0015Aj\ni\u00112\nUAj\ni\r\r\r\r\r\r\n2\nWhere the last inequality is by Cauchy-Schwartz inequality. Now, using Jensen's inequality, we\nhave\nE\u001b\r\r\r\r\r\rNX\nj=1\u001bj\u0010\n\u0015Aj\ni\u00112\nUAj\ni\r\r\r\r\r\r\n2\u00140\n@E\u001b\r\r\r\r\r\rNX\nj=1\u001bj\u0010\n\u0015Aj\ni\u00112\nUAj\ni\r\r\r\r\r\r2\n21\nA1=2\n=0\n@NX\nj=1\u0010\n\u0015Aj\ni\u001141\nA1=2\n\u0014p\nN (C.5)\nCombining (C.1), (C.3), (C.4) and (C.5), we have R(^MD;\u001a;\u000e\u000eTr)\u0014\u001a0+1\u0000\u000e\n2\u000e+dp\nN.\u0003\nCombining with Theorem C.7, we get our main theorem:\nTheorem C.9 (Main Theorem). Given a training set Tr=fAjgN\nj=1sampled uniformly from\nD, and \fxed constants 1> \u001a\u00150;\u000e > 0;1> \u0011 > 0, if there exists a (\u001a;\u000e)-robust solution s2^Bd\nwith respect to Tr, then with probability at least 1\u0000C\n\"de\u0000\u00112pN\n2\u0000C\n\"de\u0000\u00112pN\n3(1\u0000\u0011), fors2^Bdthat is a\n(\u001a;\u000e)-robust solution with respect to Tr,\nL(s)\u0014^LTr(s) +2(1 +\u0011)\u001a\n1\u0000\u0011+1\u0000\u000e\n\u000e+2dp\nN+ 4r\n2 log(4=\u000e)\nN\nProof: Since we can \fnd s2^Bds.t.sis (\u001a;\u000e)-robust with respect to Tr, by Lemma C.4, with\nprobability 1\u0000C\n\"de\u0000\u00112pN\n2,sis (\u001a\n1\u0000\u0011;\u000e)-robust with respect to D. Therefore, s2^MD;\u001a\n1\u0000\u0011;\u000e. Apply\nLemma C.8, we have With probability at least 1 \u0000C\n\"de\u0000\u00112\u001aN\n3(1\u0000\u0011),R(^MD;\u001a;\u000e\u000eTr)\u0014\u001a(1+\u0011)\n1\u0000\u0011+1\u0000\u000e\n2\u000e+dp\nN.\nCombined with Theorem C.7, the proof is complete. \u0003\nIn summary, Theorem C.9 states that if we can \fnd a solution swhich \\\fts\" the training set,\nand is very robust, then it generalizes to the test set.\n18\n\nD. Missing Proofs of Section 4\nFact D.1 (Pythagorean Theorem). IfAandBare matrices with the same number of rows and\ncolumns, then AB>= 0impliesjjA+Bjj2\nF=jjAjj2\nF+jjBjj2\nF.\nProof: (Proof of Lemma 4.2) Note thatAVV>is a row projection of Aon the colsp( V). Then, for\nany conforming Y,\n(A\u0000AVV>)(AVV>\u0000YV>)>=A(I\u0000VV>)V(AV\u0000Y)>=A(V\u0000VV>V)(AV\u0000Y)>= 0:\nwhere the last equality follows from the fact if Vhas orthonormal columns then VV>V=V(e.g.,\nsee Lemma 3.5 in [Clarkson and Woodru\u000b, 2009]). Then, by the Pythagorean Theorem (Fact D.1),\nwe have\njjA\u0000YV>jj2\nF=jjA\u0000AVV>jj2\nF+jjAVV>\u0000YV>jj2\nF (D.1)\nSinceVhas orthonormal columns, for any conforming x,jjx>V>jj=jjxjj. Thus, for any Zof rank\nat mostk,\njjAVV>\u0000[AV]kV>jjF=jj(AV\u0000[AV]k)V>jjF=jjAV\u0000[AV]kjjF\n\u0014jjAV\u0000ZjjF=jjAVV>\u0000ZV>jjF (D.2)\nHence,\njjA\u0000[AV]kV>jj2\nF=jjA\u0000AVV>jj2\nF+jjAVV>\u0000[AV]kV>jj2\nFBBy (D.1)\n\u0014jjA\u0000AVV>jj2\nF+jjAVV>\u0000ZV>jj2\nFBBy (D.2)\nThis implies that [ AV]kV>is a best rank- kapproximation of Ain the colsp( V). \u0003\n19",
  "textLength": 49189
}