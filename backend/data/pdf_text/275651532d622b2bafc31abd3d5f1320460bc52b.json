{
  "paperId": "275651532d622b2bafc31abd3d5f1320460bc52b",
  "title": "Micro-architectural analysis of in-memory OLTP: Revisited",
  "pdfPath": "275651532d622b2bafc31abd3d5f1320460bc52b.pdf",
  "text": "The VLDB Journal (2021) 30:641–665\nhttps://doi.org/10.1007/s00778-021-00663-8\nREGULAR PAPER\nMicro-architectural analysis of in-memory OLTP: Revisited\nUtku Sirin1·Pınar Tözün2·Danica Porobic3·Ahmad Yasin4·Anastasia Ailamaki1\nReceived: 15 May 2020 / Revised: 11 January 2021 / Accepted: 17 March 2021 / Published online: 31 March 2021\n© The Author(s) 2021\nAbstract\nMicro-architectural behavior of traditional disk-based online transaction processing (OLTP) systems has been investigatedextensively over the past couple of decades. Results show that traditional OLTP systems mostly under-utilize the available\nmicro-architectural resources. In-memory OLTP systems, on the other hand, process all the data in main-memory and,\ntherefore, can omit the buffer pool. Furthermore, they usually adopt more lightweight concurrency control mechanisms,cache-conscious data structures, and cleaner codebases since they are usually designed from scratch. Hence, we expect\nsigniﬁcant differences in micro-architectural behavior when running OLTP on platforms optimized for in-memory processing\nas opposed to disk-based database systems. In particular, we expect that in-memory systems exploit micro-architecturalfeatures such as instruction and data caches signiﬁcantly better than disk-based systems. This paper sheds light on themicro-architectural behavior of in-memory database systems by analyzing and contrasting it to the behavior of disk-based\nsystems when running OLTP workloads. The results show that, despite all the design changes, in-memory OLTP exhibits\nvery similar micro-architectural behavior to disk-based OLTP: more than half of the execution time goes to memory stallswhere instruction cache misses or the long-latency data misses from the last-level cache (LLC) are the dominant factors in\nthe overall execution time. Even though ground-up designed in-memory systems can eliminate the instruction cache misses,\nthe reduction in instruction stalls ampliﬁes the impact of LLC data misses. As a result, only 30% of the CPU cycles areused to retire instructions, and 70% of the CPU cycles are wasted to stalls for both traditional disk-based and new generation\nin-memory OLTP .\nKeywords OLTP ·Workload characterization ·In-memory OLTP systems ·Micro-architectural analysis\nDanica Porobic: The work was done while the author was at EPFL.\nB Utku Sirin\nutku.sirin@epﬂ.ch\nPınar Tözün\npito@itu.dk\nDanica Porobic\ndanica.porobic@oracle.com\nAhmad Yasin\nahmad.yasin@intel.com\nAnastasia Ailamaki\nanastasia.ailamaki@epﬂ.ch\n1EPFL, Lausanne, Switzerland\n2IT University of Copenhagen, Copenhagen, Denmark\n3Oracle, Redwood City, USA\n4Intel Corporation, Mountain View, USA1 Introduction\nRecent years have witnessed the rise of in-memory or main-\nmemory optimized OLTP systems [ 10,33,56]. Traditional\nOLTP engines are disk-based since they are designed in an\nera where the main-memory size was in megabytes. Today,however, a server hardware with 1TB main-memory is a\ncommodity. Therefore, the database management systems\n(DBMS) are able to process the data working set of mostOLTP applications in-memory. This has led various ven-\ndors and researchers to design brand-new OLTP engines\noptimized for the case where the dataset resides in memory[23,26,27,52].\nIn-memory OLTP systems have signiﬁcant differences\ncompared to disk-based systems. First, since the data work-\ning set resides mostly in memory, in-memory OLTP systemsomit the buffer pool component, which acts as the virtual\nmemory of a DBMS and is, therefore, essential for the disk-\nbased systems. Then, they tend to adopt more lightweight\n123\n\n642 U. Sirin et al.\nconcurrency control mechanisms to avoid the scalability bot-\ntlenecks that arise due to traditional centralized locking. Theyalso opt for cache-conscious indexes instead of the disk-\noptimized B-trees. Finally, since their codebases are written\nfrom scratch, they tend to have lighter storage engines interms of the instruction footprint.\nOLTP benchmarks are famous for their suboptimal micro-\narchitectural behavior. There is a large body of work that\ncharacterizes OLTP benchmarks at the micro-architecturallevel [ 5,12,22,41,50,53,54]. They all conclude that OLTP\nexhibits high stall time ( >50% of the execution cycles)\nand a low instructions-per-cycle (IPC) value ( <1 IPC on\nmachines that can retire up to 4 instructions in a cycle) [ 12].\nThe instruction cache misses that mainly stem from the large\ninstruction footprint of transactions are the main source ofthe stall time, while the next contributing factor is the long-latency data misses from the last-level cache (LLC) [ 53].\nAll the previous workload characterization studies, how-\never, run the OLTP benchmarks on a disk-based OLTPengine. Considering the lighter components, cache-friendly\ndata structures, and cleaner codebase of in-memory systems,\none expects them to exhibit better cache locality (especiallyfor the instruction cache) and less memory stall time. Due\nto the distinctive design features of the in-memory systems\nfrom the disk-based ones, however, it is not straightforwardto extrapolate how OLTP benchmarks behave at the micro-architectural level when run on an in-memory engine solely\nby looking at the results of previous studies.\nIn this paper, we perform a detailed analysis of the micro-\narchitectural behavior of the in-memory OLTP systems.\nMore speciﬁcally, we compare three in-memory OLTP sys-\ntems (an in-memory OLTP engine of a popular commercialvendor, a ground-up designed in-memory OLTP system, and\nan open-source OLTP engine, Silo [ 55]) to two disk-based\nOLTP systems (a popular commercial DBMS and an open-source OLTP engine, Shore-MT [43]). We examine CPU\ncycles breakdowns while running simple micro-benchmarks\nas well as the more complex TPC benchmarks (TPC-B and\nTPC-C) [ 1]. Our analysis demonstrates the following:\n– Despite all the design differences, in-memory OLTP\nspends more than half of the execution cycles in mem-ory stalls, either due to instruction or data cache misses,similar to disk-based OLTP .\n– Popular commercial OLTP systems that rely on legacy\ncodebases mainly suffer from instruction cache missesdue to their large and complex instruction footprint,\nregardless the system is disk-based or in-memory. Even\nthough the in-memory optimized DBMS componentsreduce the total instruction footprint at the storage man-\nager side, the instruction footprint and code complexity\nof the rest of the components overshadow the beneﬁts ofthese optimizations at the micro-architectural level.– Ground-up designed in-memory OLTP systems do not\nsuffer from instruction cache misses thanks to their rela-tively shorter and simpler instruction footprint compared\nto the popular commercial systems relying on legacy\ncodebases. However, ground-up designed in-memoryOLTP systems spend more than half of their executiontime in data cache miss stalls due to the random data\naccesses the OLTP workloads do. As a result, their CPU\nutilization characteristics remain close to the popularcommercial OLTP systems relying on legacy codebases.\nThis paper revisits [ 48] by using the methodology pro-\nposed by [ 49] on a later generation Intel processor. The newly\nused methodology allows end-to-end execution time proﬁl-\ning rather than merely relying on cache miss counts. Thenew generation of Intel processor reveals a signiﬁcant changein the micro-architectural behavior of the OLTP systems. In\nparticular, the main performance bottleneck of the disk-based\nand ground-up designed in-memory OLTP system shifts frominstruction cache miss stalls to data cache miss stalls thanks to\nthe improvements in the core micro-architecture. We update\nthe contributions and the text according to the newly seenmicro-architectural behavior. Furthermore, we compare and\ncontrast Intel’s two successive processor generations and rea-\nson about the signiﬁcant change in the micro-architecturalbehavior. We use Silo kernel OLTP engine instead of HyPer ,\nasHyPer is not publicly available anymore. Using Silo allows\nassessing the complexity of the core of an in-memory OLTP\nsystem and hence making the difference between an end-to-end OLTP system and an OLTP kernel engine. Finally, we\nanalyze memory bandwidth and commonly available acceler-\nation features—hyper-threading, turbo-boost, and hardwareprefetchers—completing the hardware–software interaction\nanalysis of OLTP on modern processors.\nThe rest of the paper is organized as follows. Sec-\ntion 2gives an overview of the in-memory OLTP systems-\nand surveys-related work on workload characterization and\nmicro-architectural analysis studies. Section 3describes the\nexperimental methodology. Sections 4and5present the anal-\nysis results with a micro-benchmark and TPC benchmarks,\nrespectively. Section 6analyzes the effects of transaction\ncompilation, index structures, and data types, whereas Sect. 7\ninvestigates the impact of multi-threading on the micro-\narchitectural behavior. Section 8presents the analysis of the\nmemory bandwidth consumptions. Section 9analyzes the\nacceleration features such as hyper-threading, turbo-boost,and hardware prefetchers. Section 10compares two latest\ngenerations of Intel’s successive micro-architectures. Finally,\nSect. 11discusses the results and Sect. 12concludes.\n123\n\nMicro-architectural analysis of in-memory OLTP: Revisited 643\n2 Background and related work\nIn-memory DBMS gained a lot of popularity in the last\ndecade. In Sect. 2.1, we detail the underlying factors for\nthis trend and the main design characteristics of in-memoryOLTP systems. Then, in Sect. 2.2, we go over the recent work-\nload characterization studies that focus on OLTP applications\nand highlight why they are not representative for in-memory\nOLTP systems.\n2.1 In-memory OLTP\nCommodity servers of the last decade follow two funda-mental trends: (1) main-memory becoming cheaper and (2)\nnumber of cores increasing exponentially. Simply increasing\nbuffer pool size and number of worker threads to exploit thelarge main-memory and all the available cores, respectively,\nlead to marginal gains. Therefore, these two hardware trends\nhave triggered alternative architectures for new-generationDBMS.\nAs DRAM prices become cheap enough to buy 1TB\nmain-memory for ∼$30 K, today it is possible for most\nOLTP applications to keep all of their data working set inmain-memory. This has led to the development of various in-\nmemory or main-memory optimized OLTP systems. These\nsystems either manage all the data in main-memory or makesure that the hot data resides in main-memory. Since they\nmanage to eliminate/minimize the disk I/O for the data page\naccesses, the overheads associated with managing the bufferpool overweigh its beneﬁts [ 16]. Therefore, the in-memory\nOLTP systems omit the buffer pool component even though\nit is essential for the traditional disk-based DBMS.\nOn the other hand, in step with Moore’s law, the hardware\nvendors keep providing more and more opportunities for par-\nallelism. Modern servers tend to have multiple multi-core\nprocessors in the same machine and allow OLTP systemsto handle increasing number of transactional requests in\nparallel. However, the traditional concurrency control mech-\nanisms using a centralized lock manager and two-phaselocking are designed at an era where the server hardware\nwere uniprocessors. Therefore, they do not scale on multi-\ncores preventing OLTP systems from exploiting the sheernumber of cores available to them [ 37,61].\nIn order to achieve better scalability, in-memory OLTP\nsystems adopt alternative concurrency control mechanisms.\nThese mechanisms can be broadly grouped into two cate-gories based on whether they partition the data or not. The\nones that partition the data use one data partition for each\ncore and a single worker thread for each partition. Systemslike V oltDB [ 52] (or its ancestor H-Store [ 51]) and the ini-\ntial version of HyPer [ 23] deploy this approach. As a result,\nthey avoid any form of locking within a partition and need tocoordinate worker threads only when a transaction is multi-partition. The systems that prefer avoiding any kind of data\npartitioning, like Hekaton [ 26], SAP HANA [ 27], or the latest\nversion of HyPer [ 36], rely on optimistic and multi-version\nconcurrency control [ 7].\nIn addition to alternative concurrency control mecha-\nnisms, in-memory database systems also deploy cache-conscious index structures. They align the index page sizes\nto the size of a cache line as opposed to the size of a\ndisk page and/or adopt lock-free index page access mech-anisms rather than using traditional page latches [ 28,55].\nMoreover, the in-memory OLTP systems tend to depend on\npre-determined stored procedures instead of ad hoc queries[23,26,52] and apply efﬁcient compilation optimization tech-\nniques that optimize the instruction stream for a particular\ntransaction [ 26,35]. Finally, the new-age in-memory OLTP\nsystems have codebases that are implemented from scratch.Therefore, they are expected to have a cleaner codebase\ncompared to the traditional disk-based systems where the\ncodebase consists of many branch statements and obsoletecode paths due to different release versions spanning several\ndecades of development.\nOverall, in-memory OLTP engines deploy lighter storage\nmanager components compared to the traditional disk-based\nsystems aiming to utilize the resources of the modern server\nhardware in a more effective way.\n2.2 OLTP at the micro-architectural Level\nThere is a large body of related work analyzing the micro-architectural behavior of OLTP workloads. Barrosso et al. [ 5]\ninvestigate the memory system behavior of OLTP and DSS\nstyle workloads both on a real machine and with a full-systemsimulation. They argue that these two types of workloadswould beneﬁt from different architectural designs in terms\nof the memory system. Ranganathan et al. [ 41] perform a\nsimilar analysis. However, they only focus on the effective-ness of out-of-order execution on SMPs while running these\nworkloads in a simulation environment. On the other hand,\nKeeton et al. [ 22] and Stets et al. [ 50] experiment only with\nOLTP benchmarks (TPC-B and TPC-C) on real hardware.\nAll of these studies agree that OLTP workloads utilize the\nunderlying micro-architectural resources very poorly.\nAilamaki et al. [ 2] examine where the time goes on\nfour commercial DBMS using a micro-benchmark to have\na ﬁne-grain understanding of the memory system behavior\non multi-processors, whereas Hardavellas et al. [ 15] ana-\nlyze TPC-C and TPC-H on both in-order and out-of-order\nmachines in a simulation environment. These studies focus\non the implications for the DBMS rather than the hardwareto achieve better hardware utilization.\nMore recent workload characterization studies [ 12,54]\nadditionally analyze the TPC-E benchmark and show thatmicro-architecturally TPC-E behaves very similarly to the\n123\n\n644 U. Sirin et al.\nTPC-B and TPC-C benchmarks. These studies also corrob-\norate the ﬁndings of the previous studies in terms of theinefﬁcient use of the memory hierarchy when running OLTP .\nThey highlight that the L1-I stalls are the dominant factor\nin the overall stall time followed by the long-latency datamisses. Sirin et al. [ 46] have analyzed Online Analytics Pro-\ncessing (OLAP) workloads. The study has shown that OLAP\nworkloads spend most of the CPU cycles to data cache miss\nstalls, either due to saturation of the memory bandwidth orlong-latency data cache misses. Our experimental methodol-\nogy while measuring various hardware events using counters\non real hardware is very similar to the methodologies of thesestudies.\nYasin et al. [ 59] examine Naive-Bayes algorithm and its\nhardware behavior in an Hadoop execution environment. Thestudy shows that software stack, such as the used JVM, andapplication code efﬁciency has a signiﬁcant impact on the\noverall performance. Kanev et al. [ 20] examine collective\nof machines in a Google datacenter running collective ofGoogle datacenter applications. The study shows that the\ndatacenter workload collection spends most of the time on\nwaiting for dependent data cache accesses due to the data-intensive nature of the datacenter workloads. Beamer et al.\n[6] presents a graph workload analysis and highlights that\ngraph workloads severely under-utilize the memory band-width. These studies are complementary to our work as theyfocus on workloads with different data and instruction access\npatterns.\nHarizopoulos et al. [ 16] demonstrate that traditional OLTP\nsystems spend more than half of their execution time within\nthe buffer pool, latching, locking, and logging components.\nOn the other hand, Wenisch et al. [ 57] and Tozun et. al [ 53]\ntie the micro-architectural behavior of the disk-based OLTP\ninto speciﬁc code modules by presenting the breakdown of\nthe cache misses into speciﬁc code parts of the traditionalOLTP software stack at different code granularities.\nAs Sect. 2.1explains, the in-memory OLTP systems either\nremove or simplify most of the traditional disk-based OLTP\ncomponents. Therefore, the micro-architectural behavior ofOLTP workloads when run on disk-based systems cannot be\nrepresentative for the in-memory systems. Even worse, the\nprevious ﬁndings might mislead researchers and develop-ers that aim to improve utilization at the micro-architectural\nlevel when running OLTP workloads using in-memory OLTP\nsystems. Therefore, the focus of this paper is to performa workload characterization study for OLTP benchmarksrunning on in-memory OLTP systems to understand the low-\nlevel differences between in-memory and disk-based OLTP ,\nand based on these ﬁndings, provide valuable insights forOLTP systems’ design.3 Setup and methodology\nThe experiments presented in this paper are executed on\nreal hardware. The rest of this section details the setup and\nmethodology for our study.Hardware We run experiments on a modern commodity\nserver with Intel’s Broadwell processors. Table 1shows the\narchitectural details of this server. To collect numbers about\nvarious hardware events and break down the time spent inspeciﬁc code modules, we use Intel VTune Ampliﬁer XE\n2018 [ 17], which provides an API for lightweight hardware\ncounter sampling. We disable hyper-threading and turbo-boost to obtain more precise hardware sampling values and\nincrease predictability in measurements.\nOS & Compiler We use Ubuntu 16.04.6 LTS and gcc 5.4.0\non the Broadwell server.Benchmarks We run two types of benchmarks: micro-\nbenchmarks and TPC benchmarks [ 1]. Our goal is to perform\nsensitivity analysis and have a more detailed understandingof the systems using the micro-benchmark, while the experi-\nments using the TPC benchmarks serve to give an idea about\nthe behavior of the systems when running well-known real-world applications.\nThe micro-benchmark uses a randomly generated table\nwith two columns ( key andvalue ) of the type Long. It\nhas two versions: read-only and read-write. The read-onlyversion reads Nrandom rows from the table, whereas the\nread-write version updates Nrandom rows. Both versions\nuse an index lookup operation on the randomly picked key\nvalue to reach the row to be read or updated. We also use\na modiﬁed version of the micro-benchmark where we use\nstrings of 50 bytes for both columns to quantify the impactof data type on micro-architectural utilization in Sect. 6.2.\nAs for the TPC benchmarks, we use TPC-B and TPC-\nC. We omit the TPC-E benchmark since recent workloadcharacterization studies demonstrate that TPC-E exhibitssimilar micro-architectural behavior to the TPC-B and TPC-\nC benchmarks [ 12,54].\nAnalyzed systems We analyze three in-memory OLTP sys-\ntems: the in-memory OLTP engine of a closed-source\ncommercial vendor ( DBMS M ), an open-source commercial\nOLTP system ( DBMS N ), and an open-source OLTP engine\n(Silo [55]).\nWe pick these three systems as they are well known in\nthe community and their design characteristics representa good variety among today’s in-memory OLTP systems.While DBMS M adopts multi-versioned concurrency con-\ntrol, DBMS N uses physical data partitioning, and Silo uses\noptimistic concurrency control. DBMS M implements both\nhash index and a variant of cache-conscious B-tree index\nsimilar to [ 28,29].DBMS N uses a variant of a self-balancing\nbinary search tree, red-black tree. Silo implements Masstree,\na highly parallel in-memory index structure [ 32]. For DBMS\n123\n\nMicro-architectural analysis of in-memory OLTP: Revisited 645\nTable 1 Server ParametersProcessor Intel(R) Xeon(R) CPU E5-2680 v4 (Broadwell)\n#Sockets 2\n#Cores per socket 14Hyper-threading OffTurbo-boost OffClock Speed 2.40 GHzBandwidth (per socket) 66 GB/sL1I / L1D (per core) 32 KB / 32 KB 16-cycle miss latencyL2 (per core) 256 KB 26-cycle miss latencyLLC (shared) 35 MB 160-cycle miss latencyMemory 256 GB\nM, we use the B-tree index. Moreover, DBMS M use trans-\naction compilation techniques for the stored procedures,whereas DBMS N and Silo do not. We evaluate DBMS M\nalways having its transaction compilation feature turned on\nin our analyses, except in Sect. 6.1, where we evaluate the\ntransaction compilation feature.\nIn order to gain better insights about the differences\nbetween the in-memory and disk-based OLTP systems, we\nalso include two disk-based systems: a popular, commercialsystem ( DBMS D ) and the open-source Shore-MT [43]s t o r -\nage manager.\nTo implement benchmarks, we use the SQL frontend of\nthe commercial systems, DBMS D ,DBMS M , and DBMS N ,\nand Silo’s benchmarks in C++ and Shore-MT ’s Shore-Kits\nsuite that provides an environment to implement benchmarks\nfor Shore-MT in C++.\nFor all the systems, we use asynchronous logging. There-\nfore, there is no delay due to I/O in the critical path of the\ntransaction execution.Measurements We populate the databases from scratch\nbefore each experiment and the data remains memory-\nresident throughout the experiment. In the following sections,we indicate the database sizes used in each experiment beforediscussing the results. In our experiments, both the database\nserver process executing the transactions and the client pro-\ncesses generating the transactions run on the same machine.We ﬁrst start the server process, populate the database, and\nthen start the experiment by simultaneously launching all\nclients that generate and submit transactional requests to thedatabase server.\nWe proﬁle the database server process by attaching VTune\nto it during a 120-second benchmark run following a 60-second warm-up period. We repeat every experiment threetimes and report the average result.\nIn terms of micro-architectural efﬁciency, our goal is to\nobserve how well each system exploits the resources of asingle core regardless of the parallelism in the system. There-fore, all the experiments except for the ones in Sects. 7and 8\nuse a single worker thread executing the transactions.\nThe choice of a single worker thread also eliminates con-\ntention due to several threads trying to access the shared data\nin the case of non-partitioned systems and distributed transac-\ntions in the case of partitioning-based systems. This way weavoid possible misleading micro-architectural conclusions.\nFor example, high contention for a shared data page could\nlead to multiple threads spinning on a latch for that data page,thus artiﬁcially increasing the cache hit ratio.\nWe use one client to generate request in the single-\nthreaded experiments. Shore-MT ,DBMS D ,Silo, and DBMS\nMassign one worker thread per client. DBMS N , on the other\nhand, generates one worker thread per data partition, so we\nconﬁgure it to have only one partition. From VTune, we ﬁl-\nter the hardware counter results particularly for the identiﬁedworker thread excluding the other threads that are responsi-\nble for background tasks, e.g., communication between the\nserver and client, parsing transactions, etc.\nIn multi-threaded experiments (Sects. 7and 8), we use\nmultiple clients to generate requests for all systems. For\nDBMS N , we also use multiple data partitions and ensure\nthat all transactions access only a single partition. For eachsystem, we gradually increase the number of clients, and\nproﬁle the execution with the number of clients that give the\nhighest aggregate throughput. From VTune, we ﬁlter hard-ware counter results for each worker thread separately and\nreport their average.\nVTune We use Intel VTune 2018. We use VTune’s built-\nin general-exploration analysis for the breakdown of the\nCPU cycles per the Top-down Micro-architecture Analysis\nMethod. We use VTune’s built-in memory-access analy-\nsis to measure the consumed memory bandwidth. As wenuma-localize our experiments on a single socket, we report\naverage bandwidth per-socket values. We use VTune’s built-\nin advanced-hotspots analysis to perform function call tracebreakdown.\n123\n\n646 U. Sirin et al.\nFig. 1 Breakdowns of the CPU cycles as we increase the database size when running the read-only micro-benchmark\nVTune’s general-exploration provides the breakdown of\nthe CPU cycles [ 49,58]. We use a simpliﬁed version of\nVTune’s original CPU cycles categorization to be able to\ninterpret the results more easily. We follow the same simpli-ﬁed categorization used in our previous work [ 46]. In Table\n18, Section C of the “Appendix”, we provide how each indi-\nvidual CPU cycles component that VTune reports maps tothe CPU cycles category that we use.\nEach CPU pipeline slot is categorized into one of two\ncomponents: retiring and stalling. A retiring cycle is a cyclewhere the processor ﬁnishes the execution of an instruction,i.e., retires an instruction. A stalling cycle is a cycle where\nthe processor has to wait, i.e., stall, (e.g., to perform a read\nfrom the caches). In an ideal scenario, all CPU cycles wouldbe retiring. Stalling cycles can be further decomposed into\nﬁve components: (i) branch misprediction, (ii) Icache, (iii)\ndecoding, (iv) Dcache, and (v) resource/dependency. Today’sprocessors use a hardware unit called branch predictor; it\npredicts the outcome of a branch instruction (i.e., an if()\nstatement) and speculatively executes instructions per thepredicted branch direction and/or target. If the processor thenrealizes the prediction is not correct, it undoes whatever it\nhas been doing and starts executing the correct set of instruc-\ntions. This cost is deﬁned as the branch misprediction andcan be very costly, as it requires canceling a large amount of\nwork. Icache deﬁnes the cost of instruction cache and instruc-\ntion translation lookaside buffer misses. Decoding deﬁnesthe cost of sub-optimal micro-architectural implementation\nof the instruction decoding unit. Dcache deﬁnes the cost of\ndata-cache misses. Resource/dependency deﬁnes the cost ofexecuting instruction that has resource and/or data dependen-cies. For example, if two instructions require using the same\narithmetic-logic unit, one has to wait for the other. This time is\nidentiﬁed as the resource dependency time. Or, if an instruc-tion’s operand depends on the result of another instruction,\nthe instruction with the dependent operand has to wait for the\nother instruction to ﬁnish. This time is identiﬁed as the datadependency time.4 Micro-benchmark\nBefore performing an analysis using the community standardTPC benchmarks, we devise a sensitivity study using themicro-benchmark. The goal of this study is to answer the\nfollowing questions:\n– Where do CPU cycles go when running in-memory\nOLTP? Are they wasted on memory stalls or used to retire\ninstructions?\n– Where do memory stalls come from? Are they mainly\ndue to instructions or data for in-memory OLTP?\n– What is the impact of the database size on the above\nmetrics?\n– Does the amount of work done per transaction affect the\nresults and, if yes, how?\nTo answer these questions, we break the analysis into two\nparts. The ﬁrst part (Sect. 4.1) varies the database size by\nvarying the number of rows in the table while keeping the\namount of work done per transaction constant. On the other\nhand, the second part (Sect. 4.2) varies the amount of work\ndone per transaction by increasing the number of rows readin a transaction while keeping the database size constant.\n4.1 Sensitivity to data size\nTo investigate the impact of database size on the micro-architectural behavior, we populate databases of size 1MB,10MB, 10GB, and 100 GB. DBMS M has a 32 GB of data size\nlimitation. Hence, we use maximum of 10 GB of database\nfor DBMS M . The micro-architectural behavior of DBMS M\ndoes not signiﬁcantly change as the data size varies (see Fig.\n1and Table 2). Hence, we rely on 10 GB of data to inter-\npret DBMS M ’s micro-architectural behavior for a large data\nsize. Then, we collect hardware events as the systems run\nthe micro-benchmark with a single transaction type that just\nreads/updates one random row after an index probe operation.While the results for the read-only version of the micro-\n123\n\nMicro-architectural analysis of in-memory OLTP: Revisited 647\nTable 2 Normalized throughput as we increase the database size\n1M B 1 0M B 1 0G B 1 0 0G B\nDBMS D 1 1 1 1\nShore-MT 3.0 2.9 2.1 1.6DBMS M 2.8 3.0 3.0 –DBMS N 7.8 7.8 6.4 3.9Silo 75.4 62.9 27.5 19.3\nbenchmark are in the following subsections, the results for\nthe read-write version of the micro-benchmark are in Section\nA.1 of the “Appendix”.\n4.1.1 CPU cycles breakdown\nFigure 1shows the CPU cycles breakdowns as the database\nsize is increased. The retiring cycles ratios are similar fordatabases of sizes 1 MB and 10 MB since the data working\nset mainly ﬁts in the last-level cache (LLC). As we increase\nthe database size to 10 GB and 100 GB, the retiring cyclesratios are decreased since the data working set no longerﬁts in caches and the long-latency data misses become more\nsigniﬁcant. All the retiring cycles ratios are less than 30%\nfor a database of size 10 GB and 100 GB for all the systems.Hence, in-memory OLTP systems spend most of their CPU\ncycles to stalls similar to disk-based systems.\nShore-MT is bound by Dcache and resource/dependency\nstalls. Shore-MT is a disk-based system known to have a\nlarge and complex instruction footprint. Existing work on\nShore-MT has long shown that Shore-MT is Icache-stalls-\nbound [ 48,49,53,54]. All machines used in the existing work\nis Intel’s, version 2, Ivy Bridge micro-architecture. We,\non the other hand, use a later-generation version 4 micro-\narchitecture, Broadwell, which is the slightly improvedversion of the version 3 Haswell micro-architecture. Intel has\nannounced an important micro-architectural improvement on\nthe instruction fetch unit of the Haswell micro-architecture[14]. As Hammarlund et al. [ 14] speciﬁes, “State-of-the-art\nadvances in branch prediction algorithms enable accurate\nfetch requests to run ahead of micro-operation supply to hideinstruction TLB and cache misses.”. Hence, instruction fetchunit keeps supplying instructions even though there is an\ninstruction cache miss, which allows overlapping the instruc-\ntion cache miss latency with useful work. As a result, Shore-\nMT, being a long-standing-Icache-stalls-bound system, has\nbecome Dcache- and resource/dependency-stalls-bound. We\ncompare Ivy Bridge and Broadwell in Sect. 10.\nThe instruction footprint of disk-based systems is large\nand complex due to the complex relationships among vari-\nous individual components such as buffer manager and lockmanager. Shore-MT not suffering from Icache stalls showsthat today’s processors are powerful enough to mitigate the\nIcache stalls caused by the large and complex the instructionfootprint of disk-based systems. We summarize our ﬁndings\non Icache stalls in terms of database software and hardware\ndevelopment in Sect. 11.\nShore-MT spends 40% of its time on Dcache and\nresource/dependency stalls even when the data size is small.\nWe examine Shore-MT ’s call stack and observe that most of\nthe Dcache and resource/dependency stalls are due to buffermanager and centralized locking & latching operations such\nas looking into the hash table that keeps track of the buffer\npool pages and acquiring a lock. As the data size exceeds theLLC size, Shore-MT becomes more and more Dcache-bound\ndue to its working set not ﬁtting into the LLC and suffering\nfrom expensive LLC misses.\nDBMS D and DBMS M suffer from high Icache stalls.\nDBMS D , despite being a popular commercial, disk-based\nOLTP system, relies on a legacy codebase whose instruc-\ntion footprint is large and complex. DBMS M , being the\nin-memory OLTP engine of DBMS D , borrows legacy code\nmodules from DBMS D . As a result, its instruction footprint is\nalso large and complex, and it mainly suffers from the Icachestalls. Nevertheless, DBMS M ’s throughput is three times of\nDBMS D (see Table 2). Hence, the optimizations DBMS M\nadopts help in reducing the instruction and data footprints of\nDBMS D . Nevertheless, both DBMS D and Mmainly suffer\nfrom Icache stalls showing the severe effect of using legacy\ncode modules both for disk-based and in-memory OLTP sys-\ntems.\nUnlike DBMS M ,DBMS N does not suffer from Icache\nstalls. This is because DBMS N is a ground-up designed in-\nmemory system. It does not rely on legacy code modules asDBMS M does. As a result, its instruction footprint is smaller\nand less complex than DBMS M , and hence, it does not suffer\nfrom Icache stalls.\nOur previous work [ 48,49] shows that V oltDB, a similar\nground-up designed system to DBMS N , is instruction-bound\nrather than data-bound as we present here. The reason is,\nonce again, the improved micro-architecture of the machine(Broadwell) we use compared to the machine used by\n[48,49] (Ivy Bridge). The improved instruction fetch unit of\nBroadwell eliminates the Icache stalls and makes DBMS N\ndata-bound. Section 10discusses this issue in more detail. We\nsummarize our ﬁndings on Icache stalls in terms of database\nsoftware and hardware development in Sect. 11.\nDBMS N is an in-memory system, which eliminates the\nheavy disk-based system components such as buffer pool and\nlock manager. Nevertheless, it suffers signiﬁcant amount of\nDcache and resource/dependency stalls for small data sizes,similar to Shore-MT . We examine DBMS N ’s call stack and\nobserve that the Dcache and resource/dependency stalls are\nlargely due to the cost of setting up and instantiating the trans-actions. As the data size is increased, DBMS N becomes more\n123\n\n648 U. Sirin et al.\n(a) (b)\nFig. 2 Execution time and Dcache stalls breakdown for Shore-MT and DBMS N for 1 MB of database size\nand more Dcache-bound due to its working set not ﬁtting into\nthe LLC and suffering from expensive LLC misses.\nSilo has high retiring cycles for small data sizes. Silo is an\nin-memory system eliminating the costly buffer manager andcentralized locking & latching components that disk-basedsystems use. Moreover, Silo is a kernel OLTP engine that has\ntransactions hard-coded in C++. Hence, it does not suffer\nfrom the cost of instantiating and scheduling user request asDBMS N does. As a result, it does not suffer from Dcache\nstalls when the data size is small, and it has high retiring\ncycles ratio for small data sizes. As the data size exceedsthe LLC size, Silo becomes Dcache-bound such that Silo’s\nretiring cycles are the lowest among all the systems.\nIn addition to the Dcache stalls, Silo also suffers from sig-\nniﬁcant amount of branch misprediction. This is due to thein-node searches that Silo performs during the index traver-\nsal\n1. While all the systems perform an index traversal during\ntheir transaction processing, it only surfaces up on Silo thanks\nto its lean codebase and efﬁcient data structure it uses. Other\nsystems suffer from other overhead such as complex instruc-\ntion footprint as in DBMS D and M, large data footprint as\ninShore-MT , or inefﬁcient index structure as in DBMS N .\nWe observe that all the systems suffer 10–15% decoding\nstalls. Decoding stalls are the stalls due to the inefﬁcienciesin the micro-architectural implementation of the instruc-tion decoding unit of the processor. As Intel relies on a\nComplex Instruction Set Computer (CISC) type of micropro-\ncessor design, it requires decoding instructions into micro-operations. It is known that Intel’s instruction decoding unit\nis a legacy micro-architecture and has several penalties [ 18].\nTo avoid these penalties, Intel has introduced a DecodedStream Buffer (DSB), which is a micro-operation cache\ninside the pipeline that allows side-stepping the decoding\nunit and providing already decoded instructions to the pro-cessor. However, DSB is small (1.5 KB). If the workload’s\n1Silo uses linear search as its in-node search algorithm.instruction working set does not ﬁt into the DSB, it has topass through the legacy decoding unit and suffer from the\npenalties of the legacy decoding unit. As OLTP workloads’\ninstruction footprint is large and complex, they pass throughthe legacy decoding unit and suffer the decoding stalls. Nev-ertheless, the caused penalties are relatively small, 10-15%,\nand hence do not constitute a signiﬁcant problem.\n4.1.2 Throughput\nTable 2shows normalized throughputs for each system as\nthe database size is increased. The throughput values are\nnormalized to DBMS D for each database size individually.\nThe relative performance between DBMS D and Mremains\nstable. This is because both DBMS D and Mare instruction-\nbound. Hence, the increased data size does not signiﬁcantlyaffect their performance. Shore-MT ,DBMS N , and Silo’s rel-\native performance, on the other hand, is decreased as the data\nsize is increased. This is because Shore-MT ,DBMS N and\nSilo are data-bound, i.e., Dcache and resource/dependency-\nstalls-bound. The increased data size results in a more\nsubstantial drop in their throughput than DBMS D and M.\nAs a result, their throughputs get close to DBMS D and Mas\nthe data size increases.\nAll the in-memory systems are faster than the disk-based\nsystems for all the data sizes. This shows that the optimiza-tions that in-memory systems implement signiﬁcantly helpimproving the throughput. DBMS M , despite its large and\ncomplex instruction footprint, is faster than Shore-MT for\n10 GB of database. As Shore-MT mainly suffers from Dcache\nstalls, this shows the severe negative effects of disk-based\nsystems’ data overhead, such as large index and buffer pool\npages.\nDBMS N is faster than DBMS M thanks to its smaller and\nsimpler codebase. DBMS N is a ground-up designed system.\nHence, it does not borrow any legacy codebase as DBMS M\ndoes. As a result, it does not suffer from Icache stalls, and it\n123\n\nMicro-architectural analysis of in-memory OLTP: Revisited 649\nFig. 3 Breakdowns of the CPU cycles for Transaction setup and Index\nlookup for DBMS N and Silo\nis signiﬁcantly faster than DBMS M .Silo is the fastest system\nwe have proﬁled. One reason for that is Silo is a kernel OLTP\nengine, which does not suffer from the cost of setting up and\ninstantiating the transactions. DBMS N , on the other hand,\nis an end-to-end SQL-based OLTP system. Another reasonis that Silo uses an efﬁciently implemented index structure,\nMasstree [ 32]. Hence, its data footprint is also succinct. As a\nresult, it is 4.9x faster than DBMS N for 100 GB of database.\nWe examine the inefﬁcient index structure issue of DBMS N\nin Sects. 4.1.3 and 4.2.1 in more detail.\n4.1.3 DBMS N versus Silo\nSilo is ∼5×faster than DBMS N as shown by Table 2for\n100 GB of database. We examine the call stack of DBMS N\nand Silo.DBMS N spends ∼33% of its time while setting up\nand instantiating the transactions, from which Silo minimally\nsuffers due to being a kernel OLTP engine. Hence, 33% ofthe 5×difference is due to the transaction setup and instanti-\nation work that DBMS N performs. These are functions like\nprocessInitiateTask() ,xfer() (dequeues user\nrequests) and coreExecutePlan Fragments() .\nDBMS N spends most of the remaining 67% of its time in\nindex lookup. In particular, DBMS N spends ∼44% of its time\nin index lookup and ∼19% of its time in various small-sized\nfunctions each taking less than 1% of the execution time. Silo\nspends ∼75% of its time in index lookup. Therefore, most of\nthe remaining 67% of the 5 ×throughput difference between\nDBMS N and Silo is due to Silo using a more efﬁcient index\nstructure than DBMS N .\nWe examine the micro-architectural behavior of the trans-\naction setup and index lookup components of DBMS N\nseparately in Fig. 3for 100 GB of database. The ﬁgure\nshows that transaction setup component only modestly suf-\nfers from Dcache stalls, whereas the lookup componentis exclusively Dcache-stalls-dominated. The overall micro-\narchitectural behavior of DBMS N is the composition of\nthese two micro-architectural behavior. We also examine the\nmicro-architectural behavior of the index lookup componentofSilo. Similar to DBMS N , the micro-architectural behavior\nof the index lookup operation of Silo is exclusively dominated\nby Dcache stalls. As Silo’s execution time dominated by the\nindex lookup operation, 67% of the 5 ×throughput difference\nbetween DBMS N and Silo is due to the used index structure.\nTherefore, DBMS N can signiﬁcantly improve its perfor-\nmance by using a more efﬁcient index structure. We describe\nDBMS N and Silo’s used index structures in Sect. 4.2.1 in\nmore detail.\n4.1.4 Shore-MT versus DBMS N\nShore-MT and DBMS N spend a signiﬁcant portion of their\ntime in Dcache stalls for small data sizes (as shown by Fig. 1).\nWe examine the execution time and Dcache stalls breakdownfor Shore-MT and DBMS N for small and large data sizes. We\nidentify ﬁve components for Shore-MT at the software level:\n(i) B-tree, (ii) buffer manager, (iii) locking and latching, (iv)transaction setup, and (v) rest. We identify four components\nfor DBMS N at the software level: (i) Transaction setup, (ii)\nIndex lookup, (iii) Post-lookup, and (iv) Rest. Figure 2shows\nthe results for 1 MB of data.\nShore-MT spends most of the execution time process-\ning disk-based system components such as buffer manager\nand centralized locking & latching components. Simi-larly, most of Shore-MT ’s Dcache stalls are coming from\nthe locking & latching and buffer manager components.\nDBMS N is an in-memory system. It does not use a\nbuffer manager component, and it uses a partitioning-based,\nlightweight concurrency control mechanism. However, as\nit is a real-life OLTP system, it performs the necessarywork to setup and instantiate transactions using functionslikeprocessInitiateTask() ,xfer() (dequeues\nuser requests) and coreExecutePlanFragments() .\nDBMS N spends most of the execution in transaction setup,\nand most of DBMS N ’s Dcache stalls are coming from the\ntransaction setup component.\nWe also examine Shore-MT and DBMS N ’s execution time\nand Dcache stalls breakdowns for 100 GB of database. Fig-\nure4shows the results. Shore-MT , being a disk-based system,\nspends most of the execution time in buffer manager and lock-ing & latching components. However, unlike it is for 1 MBof database, buffer manager component consumes a signiﬁ-\ncantly larger portion of the execution time than the locking &\nlatching component. As the data size is increased, there arelarger and larger number of buffer pool pages. Hence, the data\nfootprint is increased, and the amount of time spent inside\nbuffer manager is increased. Similarly, Shore-MT\n’s Dcache\nstalls are mostly due to the buffer manager components for\n100 GB of database as shown by Fig. 4b. Our ﬁndings for\nShore-MT corroborates with the ﬁndings of Harizopoulos et\nal. [16].\n123\n\n650 U. Sirin et al.\n(a) (b)\nFig. 4 Execution time and Dcache stalls breakdown for Shore-MT and DBMS N for 100 GB of database size\nDBMS N spends signiﬁcantly less time on setting up the\ntransactions for 100 GB of database compared to 1 MB of\ndatabase. As the data size is increased, the amount of workthat the transactions perform is increased. As a result, trans-\naction setup time is reduced from ∼60% to ∼33%, and the\nindex lookup time is increased from ∼10% to ∼44%.\nShore-MT ’s disk-based system overhead is persistent\nacross different data sizes due to the fundamental architec-\ntural reasons. Most of the execution time is spent inside thedisk-based system components such as locking & latchingand buffer manager, although the system component that con-\nsumes the largest portion of the execution time shifts from\nthe locking & latching to the buffer manager component asthe data size is increased from 1 MB to 100 GB.\nDBMS N ’s transaction setup overhead depends on the data\nsize. As the data size is increased, the amount of work thattransactions perform is increased. As a result, the transaction\nsetup consumes smaller and smaller portion of the execution\ntime as the amount of work per transaction is increased.\nTherefore, Shore-MT requires a major architectural re-\ndesign to reduce the data footprint and deliver as high\nthroughput as in-memory OLTP systems deliver. DBMS N\nspends the large portion of its execution time inside the OLTPengine for large data sizes. Hence, it can highly beneﬁt from\noptimizing internals of the OLTP engine such as using a more\nefﬁcient index structure as shown in Sect. 4.1.3 .\n4.1.5 Summary\nRelative throughput of OLTP systems widely vary among\ndifferent categories of OLTP systems. However, CPU cycles\nutilization of all the OLTP systems are low. DBMS D and\nM, relying on legacy codebases, suffers from Icache stalls.\nShore-MT ,DBMS N and Silo, being either OLTP kernels\nhaving transactions hard-coded in C++, and/or a ground-up\ndesigned in-memory system, eliminate the Icache stalls. Thereduced Icache stalls cause Dcache stalls to surface, whichShore-MT ,DBMS N and Silo suffer from. The Dcache stalls\nare largely due to the random-data-access nature of the work-\nload, in addition to the cost of buffer manager and locking &latching overhead for Shore-MT and the small cost of trans-\naction setup for DBMS N . As a result, Shore-MT ,DBMS\nNand Silo spend only 30% of the CPU cycles for retiring\ninstructions similar to DBMS D and M.\n4.2 Sensitivity to work per transaction\nTo investigate the impact of the amount of work per trans-\naction on the micro-architectural behavior, we increase the\nnumber of rows that a transaction accesses from 1 to 10and then to 100. We perform these experiments with 100 GB\ndataset for all the systems except DBMS M . We use 10 GB of\ndatabase for DBMS M due to its 32 GB of maximum database\nsize limitation. In the following sub-sections, we present theresults for the read-only version of the micro-benchmark. The\nresults for the read-write version of the micro-benchmark can\nbe found in Section A.2 of the “Appendix”.\nFigure 5shows the CPU cycles breakdowns as the amount\nof work per transaction is increased. DBMS D and Msuffer\nless and less from Icache stalls as we increase the amountof work per transaction. The repetitive behavior within a\ntransaction leads to a better instruction cache locality. As\na result, the code for the other layers of the system that sur-round a transaction’s execution (e.g., the code outside thestorage manager) is executed less frequently since the trans-\nactions get longer as we increase the amount of work done\nper transaction. For example, where probing 100 rows pertransaction stresses purely the storage manager component,\nprobing 1 row also stresses the other layers such as query\nparsing, work done while starting/ending a transaction, etc.As a result, Icache stalls are decreased as the amount of work\nper transaction is increased.\nDBMS D and M’s Dcache stalls are increased as we\nincrease the work done per transaction. As we read more\n123\n\nMicro-architectural analysis of in-memory OLTP: Revisited 651\nFig. 5 Breakdowns of the CPU cycles as we increase the amount of work done per transaction with a database of size 100 GB (10 GB for DBMS\nM)\nrandom rows per transaction, we make more frequent ran-\ndom data accesses, which leads to a higher data miss rateand hence higher Dcache stalls.\nShore-MT ,DBMS N and Silo have slightly increased\nDcache stalls as the amount of work per transaction isincreased. Shore-MT ,DBMS N and Silo’s instruction foot-\nprint is small and simple enough when the number of rows\nread per transaction is 1. Hence, the increased instruction\nlocality does not make a signiﬁcant difference in terms oftheir micro-architectural behavior.\nTable 3shows the normalized throughputs. DBMS D and\nDBMS M are instruction-stalls-bound as shown in the previ-\nous section. Therefore, their delivered throughput does not\ndrop signiﬁcantly as the data size is increased. DBMS D ’s nor-\nmalized throughput for 1MB, 10MB, 10GB, and 100 GB is: 1,0.93, 0.84, and 0.81. The throughput of DBMS D drops only\nby 3% as the data size is increased from 1 MB to 100 GB. Sim-\nilarly, DBMS M ’s normalized throughput for 1MB, 10MB,\nand 10 GB is: 1, 1.01, and 0.89. DBMS M ’s throughput drops\nby only 11% as the data size is increased from 1 MB to 10 GB.\nTherefore, we assume that DBMS M ’s throughput for 100 GB\nis similar to its throughput for 10GB, and we use DBMS M ’s\nthroughput for 10 GB in Table 3.\nDBMS M ’s relative throughput is increased as the number\nof rows per transaction is increased. DBMS M becomes even\nfaster than DBMS N as the number of rows per transaction\nis increased to 10 and 100. This because of the increased\ninstruction locality that DBMS M beneﬁts from. As the num-\nber of rows read per transaction is increased, DBMS M\nexecutes the in-memory OLTP engine more and more. As a\nresult, it suffers less and less from the legacy codebase that it\nborrows from DBMS D . This shows that the core in-memory\nOLTP engine of DBMS M is efﬁciently implemented and\nbeneﬁts from the in-memory systems optimizations.\nShore-MT ,DBMS N , and Silo’s relative throughput to\nDBMS D is decreased as the number of rows per transac-\ntion is increased. This is because DBMS D beneﬁts fromTable 3 Normalized throughput as we increase the amount of work\ndone per transaction with a database of size 100 GB (10 GB for DBMS\nM)\n1 row 10 rows 100 rows\nDBMS D 1 1 1\nShore-MT 1.6 0.7 0.7DBMS M 3.0 3.7 3.9DBMS N 3.9 2.1 1.7Silo 19.3 8.0 6.0\nthe increased instruction locality as the number of rows is\nincreased.\nDBMS N ’s relative throughput with respect to Silo is\ndecreased as the number of rows is increased. DBMS N ’s\nrelative throughput with respect to Silo is: 4.9, 3.8, and 3.5\nfor 1, 10, and 100 rows, respectively. This is because DBMS\nNexecutes less and less the code to setup and instantiate\nthe transactions as the amount of work per transaction is\nincreased. As a result, its throughput gets closer and closer toSilo, which minimally suffers from the work required to setup\nand instantiate the transactions. Nevertheless, Silo is 3.5×\nfaster than DBMS N even when DBMS N largely eliminates\nthe transaction setup overhead. We examine this throughputdifference in more detail in the following section.\n4.2.1 Code modules breakdown\nTo better understand the impact of legacy code, as well as\ncomponents outside the storage manager, we quantify thepercentage of the execution time spent in the OLTP engine\nas the amount of work per transaction is increased for the\ndisk-based system DBMS D , and in-memory systems DBMS\nMand DBMS N . While performing this breakdown, we have\ndone a best-effort categorization based on the code modules\nreported by VTune as part of the worker thread execution ofeach system. Figure 6shows the results.\n123\n\n652 U. Sirin et al.\nFig. 6 The percentage of the time spent inside the OLTP engine as we\nincrease the amount of work done per transaction with a database ofsize 100 GB for DBMS D and DBMS N , and 10 GB for DBMS M\nWe observe that DBMS D and Msystems spend only 35\nto 45% of their time inside OLTP engine showing the domi-\nnance of the legacy code overhead both systems suffer. Theamount of time spent inside the OLTP engine increases as\nthe number of rows read increases. This increase is less pro-\nnounced for the disk-based system DBMS D showing the\nhigher overhead of the code outside the OLTP engine. ForDBMS M , when we increase the number of rows from 1 to\n10, the percentage inside the OLTP engine is signiﬁcantly\nincreased showing the reduced effect the legacy code over-head that DBMS M borrows from the traditional disk-based\nOLTP system it belongs to. These results explain better why\nDBMS D and M’s relative throughput (shown in Table 3) gets\nhigher and higher as the number of rows read per transaction\nis increased.\nDBMS N is a ground-up designed in-memory system. As\na result, it does not suffer from a legacy codebase as DBMS D\nand Mdo. However, as being an end-to-end system, it spends\ncertain amount of time to setup, instantiate and ﬁnalize the\ntransactions. In Fig. 6, the amount of time that DBMS N does\nnot spend inside the OLTP engine corresponds to this instan-\ntiation and ﬁnalization. This time is ∼33% for reading 1 row\nper transaction since the transaction is short. As the numberof rows per transaction increases to 10 and 100, the amount\nof time spent for setting up and instantiating the transactions\nis reduced to ∼10% and 5%, respectively. This shows that,\ndepending on the amount of work per transaction, the workrequired for setting up and ﬁnalizing the transaction can be\nsigniﬁcant.\nOn the other hand, despite the large amount of work per\ntransaction and reduced overhead of transaction setup and\nﬁnalization, DBMS N is still 3.5 ×slower than Silo for 100\nrows per transaction (see Table 3). We break the execu-\ntion time of DBMS N and Silo down to their function call\nstack to see this difference better. We use the same execution\ntime breakdown at the software level used for DBMS N in\nSect. 4.1.3 . Figure 7shows the results for 100 rows.\nFig. 7 Function call trace breakdown for DBMS N and Silo when run-\nning the micro-benchmark that reads 100 rows per transaction\nBoth systems spend most of their time performing the\nindex lookup. Therefore, the main reason for the performance\ndifference between DBMS N and Silo is the inefﬁcient index\nstructure of DBMS N . We examined DBMS N ’s index data\nstructure. We saw that DBMS N uses red-black tree as its\nindex structure. Red-black trees are self-balancing binary\nsearch trees, where the number of elements per node is 1.\nHence, at every level of the tree, red-black tree performs asingle comparison. As every node of the tree is in a random\nmemory location, red-black tree is subject to a data cache\nmiss at every level during the index traversal.\nSilo, on the other hand, uses Masstree, which is a variant of\nB-tree. As all the B-trees, Masstree’s nodes have a particular\nnode size and fanout. It is 15 for Silo. Hence, instead of 1, it\nkeeps 15 elements per node. As the tree depth drops exponen-tially with the node size, Silo’s index is much more shallow\nthan DBMS N ’s red-black tree. Therefore, Masstree is sub-\njected to a signiﬁcantly less number of data cache misses.Furthermore, Masstree software prefetches the node’s data\nblocks by injecting a software prefetch instruction during\nthe index traversal. As a result, Silo is subject to a single data\ncache miss for the entire node it accesses at every level of\nthe tree, making Silo signiﬁcantly faster than DBMS N .T h i s\nshows that, despite the overhead of a real-life system, theefﬁciency of the used index structure is still the most cru-cial factor in deﬁning the performance characteristics of an\nin-memory OLTP system.\nFinally, DBMS N ’s rest component is signiﬁcantly higher\nthan that of Silo. This is because DBMS N , being a real-life\nsystem, executes more functions to provide the end-to-end\nresponse.\n5 TPC benchmarks\nSection 4performs a sensitivity analysis using a simple\nmicro-benchmark to gain a ﬁne-grained understanding of thethe micro-architectural behavior of the OLTP systems. This\n123\n\nMicro-architectural analysis of in-memory OLTP: Revisited 653\nsection investigates the behavior of the same systems while\nrunning the more complex and community standard TPC-B(Sect. 5.1) and TPC-C (Sect. 5.2) benchmarks. All the exper-\niments in this section use a database of size 100 GB, except\nDBMS M for which we use the maximum allowed database\nsize of 32 GB. Similar to Sect. 4, we analyze the CPU cycles\nbreakdowns and normalized throughput values.\nDBMS D and DBMS M are instruction-stalls-bound.\nTherefore, their delivered throughput does not drop signiﬁ-cantly as the data size is increased as described in Sect. 4.2,\nand we assume that DBMS M ’s throughput for 100 GB is sim-\nilar to its throughput for 32 GB both for TPC-B and TPC-C.\n5.1 TPC-B\nTPC-B is an update-heavy benchmark that simulates abanking system. AccountUpdate is its only transac-\ntion type, which updates one row each in three tables,\nBranch ,Teller , and Account , and appends a row to\ntheHistory table.\nFigure 8shows the CPU cycles breakdowns and Table 4\nshows normalized throughputs for TPC-B. DBMS D ,Shore-\nMT,DBMS N , and Silo suffer less from Dcache stalls\ncompared to the micro-benchmark that reads 1 row per trans-\naction (see Fig. 1). This is mainly because TPC-B has a better\ndata locality compared to the micro-benchmark. When run-ning the micro-benchmark, we randomly probe rows from\na 100 GB table, which includes more than one billion rows.\nOn the other hand, TPC-B ﬁrst probes one of the ∼20K\nBranch es randomly. Then, it probes one of the ∼200K\nTeller s and one of the ∼2 billion Account s. Finally, it\ninserts on row into the History table. Hence, the proba-\nbility of re-accessing the same branch or teller as well as thesame History table page is quite high compared to that of\nthe micro-benchmark’s single large table.\nDBMS M suffers less from Icache stalls and more from\nDcache stalls for TPC-B compared to the micro-benchmark.\nDBMS M relies on a multi-version concurrency control\nmechanism, where the updates are kept in deltas. Each newversion of the data is written to a new memory location called\ndelta, and the pointer to the record is updated to point to the\nlocation of the delta. Multiple versions are kept in multi-ple deltas that are chained one-after-the-other. Occasionally,the deltas are consolidated. As TPC-B is an update-heavy\nbenchmark, DBMS M requires creating a new delta for every\ntransactions and hence perform more random data accessesduring the traversal and/or consolidation of the delta chain.\nThis results in higher degree of Dcache stalls for TPC-B com-\npared to the read-only micro-benchmark. We conﬁrm thishypothesis by the read-write version of the read-only micro-\nbenchmark discussed in Section A.1 in the “Appendix”.\nThe normalized throughput values follow a similar trend to\nthe micro-benchmark. All the in-memory systems are faster\nFig. 8 Breakdowns of the CPU cycles while running TPC-B\nthan the disk-based systems. Among the in-memory systems,\nDBMS N is faster than DBMS M thanks to not suffering from\nthe legacy code modules, and Silo is faster than DBMS N\nthanks to its efﬁcient engine components and not suffering\nfrom the cost of setting up and instantiating the transactions.\n5.2 TPC-C\nAfter investigating the TPC-B benchmark, this sectionfocuses on the more complex TPC-C benchmark. TPC-C\nmodels a wholesale supplier with nine tables and ﬁve trans-action types (2 of which are read-only and form 8% of\nthe benchmark mix). In terms of the database operations,\nthe TPC-C transactions contain probes, inserts, updates, andjoins covering a richer set of operations than TPC-B. There-\nfore, we expect a different behavior for TPC-C than TPC-B.\nFigure 9shows the CPU cycles breakdowns and Table 4\nshows normalized throughputs. The micro-architectural behav-ior follows a similar trend to the micro-benchmark and TPC-\nB. While DBMS D and Mare Icache-stalls-bound, Shore-MT ,\nDBMS N , and Silo are Dcache- and resource/dependency-\nstalls-bound. All the systems suffer less from the Dcache\nstalls compared to the micro-benchmark thanks to the work-\nload locality that TPC-C has. As a result, they have slightlyhigher retiring cycles ratio than the micro-benchmark.\nThe normalized throughput values follow a similar trend to\nthe micro-benchmark. All the in-memory systems are fasterthan the disk-based systems. Among the in-memory systems,Silo is faster than DBMS M and DBMS N .\nDBMS N ’s relative throughput is less for TPC-C com-\npared to the micro-benchmark and TPC-B. We examinedDBMS N ’s call stack. DBMS N ’s time spent in serializ-\ning/deserializing tuples is signiﬁcantly increased for TPC-C\ncompared to the micro-benchmark. DBMS N keeps every\ntuple in its own format (byte array) and deserializes/serializes\nthe data during transaction processing. As TPC-C requires in-\ntransaction processing of the tuples, such as incrementing theorder ID for the new order transaction, DBMS N ’s relative\n123\n\n654 U. Sirin et al.\nFig. 9 Breakdowns of the CPU cycles while running TPC-C\nTable 4 Normalized throughput\nfor TPC-B and TPC-C with adatabase of size 100 GB (32 GBfor DBMS M )TPC-B TPC-C\nDBMS D 1 1\nShore-MT 1.7 1.2DBMS M 2.2 3.1DBMS N 5.4 2.7Silo 18.5 20.0\nthroughput is decreased when running complex benchmark\ncompared to when running a simple micro-benchmark.\nShore-MT ’s relative throughput is less for TPC-C com-\npared to the micro-benchmark and TPC-B. We examined\nShore-MT ’s call stack. Shore-MT ’s time spent on B-tree\nsearch and lock manager are signiﬁcantly increased for TPC-\nC than for the micro-benchmark. This highlights Shore-MT ’s\nindex structure and locking & latching processing overheadbecoming more prominent for a complex benchmark. This is\nalso visible in Table 3, where Shore-MT ’s throughput is lower\nthan DBMS D for probing 10 and 100 rows per transaction.\n6 Index and compilation optimizations, and\ndata types\nThis section analyzes the impact of index and compilation\noptimizations the in-memory systems adopt, as well as the\nimpact of the data types, at the micro-architectural level.\nAmong the systems used in this study, DBMS M is the only\none that allows enabling/disabling the compilation optimiza-\ntions and using two different index structures: hash index and\na variant of cache-conscious B-tree index similar to [ 28,29].\nTherefore, while we use DBMS M for analyzing the impact\nof index and compilation optimizations, we experiment with\nall the three in-memory systems ( DBMS M ,DBMS N , and\nSilo) to quantify the effect of different data types.\nFig. 10 Breakdowns of the CPU cycles for different index structures\nwith and without compilation optimizations while running the micro-benchmark for DBMS M\nTable 5 Normalized throughput for different index structures with and\nwithout compilation for DBMS M\nMicro-bench. TPC-C\nB-tree w/o comp. 1 1\nB-tree w/ comp. 5 5Hash w/o comp. 1.5 1Hash w/ comp. 9 5\n6.1 Impact of index type and compilation\nTo quantify the impact of the type of index and compila-\ntion on the micro-architectural utilization, we start with the\nread-only variant of the micro-benchmark where we access\n10 rows per transaction from the 10 GB dataset. Figure 10\npresents the CPU cycles breakdowns, and Table 5presents the\nnormalized throughput values. The results for the read-write\nversion of the micro-benchmark can be found in Section B\nof the “Appendix”. Transaction compilation has a signiﬁcanteffect on the instruction stalls, resulting in ∼50% reduction\nin the Icache stalls regardless of the index type. Transaction\ncompilation enables many optimizations in the instructionstream. It can eliminate the virtual function calls. It can inline\ntemplated function calls. It can eliminate type checkings and\ncertain branches. Lastly, it allows the compiler to employ itsown optimizations more aggressively as it reduces the wholetask into a generated code ﬁle. As a result, transaction com-\npilation reduces the length and complexity of the instruction\nfootprint.\nTable 5shows the transaction compilation improves the\nthroughput by 5–6 ×. We observe that B-tree has more\nDcache stalls than the Hash index when transactions are notcompiled. This is expected as B-tree requires multiple levels\nof random lookups, whereas Hash index usually requires one\nor two random lookups. When the transactions are compiled,B-tree and Hash index have similar ratios of Dcache stalls.\n123\n\nMicro-architectural analysis of in-memory OLTP: Revisited 655\nFig. 11 Breakdowns of the CPU cycles for different index structures\nwith and without compilation while running TPC-C for DBMS M\nHowever, as Table 5shows, throughput with Hash index is\n80% higher than throughput with B-tree.\nWe repeat the experiment above using the TPC-C bench-\nmark. Figure 11shows the CPU cycles breakdown, and\nTable 5shows the normalized throughput values. Once\nagain, compilation optimizations reduce instruction stallssigniﬁcantly for both index types. Moreover, transaction\ncompilation improves DBMS M ’s throughput by 5 ×for\nboth B-tree and Hash index types. For Dcache stalls, sincethe TPC-C benchmark requires fewer random data reads\ncompared to the micro-benchmark, we do not observe a sig-\nniﬁcant difference in Dcache stalls for B-tree and Hash index.\n6.2 Impact of data type\nTo quantify the impact of different data types on micro-architectural utilization, we use the read-only version ofthe micro-benchmark where we probe 1 row per transactionover a 100 GB database (10 GB for DBMS M ). The results\nfor the read-write version of the micro-benchmark can be\nfound in Section B of the “Appendix”. We modify the micro-benchmark to use two 50 bytes string columns instead of two\nlong columns in the table and compare the two versions.\nFigure 12presents the CPU cycles breakdown. DBMS M ’s\nDcache stalls are higher for string than they are for long. We\nexamined the modules breakdown of DBMS M for string and\nlong. We observed that the increased Dcache stalls are due tothe legacy string processing code that DBMS M borrows from\nits legacy codebase. As string processing operations usually\nhave high spatial locality, the increased Dcache stalls high-\nlight an inefﬁcient string processing implementation such ascarrying high memory overheads for the string objects.\nDBMS N suffers less from Dcache stalls for string com-\npared to long. This is expected as string processing operationsusually have high spatial locality. We examined DBMS N ’s\nfunction call stack and observed that string comparison code\nconstitutes a larger fraction of the execution time with lessDcache stalls.\nFig. 12 Breakdowns of the CPU cycles for string and long data types\nwhile running the micro-benchmark\nTable 6 Normalized throughput\nfor string and long data typeswhile running themicro-benchmarkLong String\nDBMS M 1 0.7\nDBMS N 1 1Silo 1 0.7\nSilo has a similar micro-architectural behavior for long\nand string data types. This is because Silo’s index structure,\nMasstree, combines B-tree and trie index structures, where\nevery node of the trie structure is a separate B-tree. Masstree\nslices the keys into pieces of eight bytes and does a separateB-tree search for every eight bytes of the key, while traversing\nthe overall trie structure. As a result, using a long, 50-byte of\nstring key does not make a signiﬁcant difference in terms ofthe data access pattern during the key comparisons.\nKeeping B-tree within a trie structure allows skipping the\nupper levels of the trie structure for keys with long common\npreﬁxes (such as http URLs). The keys we use do not havesuch a feature. Hence, Masstree search boils down to multiple\nlevels of B-tree searches.\nTable 6presents the throughput values for the string data\ntype that is normalized to the long data type. The results show\nthat DBMS M and Silo deliver lower throughput for string\nthan they deliver for long. This is expected as the amount ofwork required to process the longer-sized string data type islarger than it is for the long data type. DBMS N ’s through-\nput remains the same for the long and string data type. This\nis because, unlike DBMS M and Silo,DBMS N is able to\nexploit the spatial locality of string processing. As a result,\nthe increased work due to using strings is balanced out with\nthe higher spatial locality of string search.\n7 Impact of multi-threading\nThis section analyzes the effect of running multiple serverside threads on the micro-architectural behavior. The single-threaded experiments aim to present an idealized case since it\n123\n\n656 U. Sirin et al.\nFig. 13 Breakdowns of the CPU cycles for the multi-threaded experi-\nments while running the micro-benchmark\navoids cache invalidations due to data sharing across different\nworker threads or misleading artiﬁcially high IPC values due\nto threads spinning under possible contention. On the other\nhand, multi-threaded experiments aim to investigate a morerealistic scenario where systems are loaded with multiple\nthreads executing transactions from multiple clients.\nFigures 13and 14show the CPU cycles breakdowns while\nrunning the read-only version of the micro-benchmark whenreading 1 row and TPC-C benchmark, respectively. We use a\ndatabase of size 100 GB in both of the experiments for all\nthe systems except DBMS M . We use 10 GB of database\nfor the micro-benchmark, and 32 GB of database for the\nTPC-C benchmark for DBMS M . We observe that the micro-\narchitectural behavior of the individual systems is similarto the single-threaded executions when running the micro-\nbenchmark. DBMS D mainly suffers from the Icache stalls,\nwhereas Shore-MT ,DBMS N and Silo majorly suffer from\nthe Dcache stalls. DBMS M suffers less from the Icache\nstalls compared to the single-threaded execution. We exam-\nined the code modules breakdown of DBMS M and observed\nthat the amount of time it spends inside the OLTP engineremains the same across the single- and multi-threaded exe-\ncutions. However, the amount of time DBMS M spends in\ndifferent modules that are outside the OLTP engine changessigniﬁcantly across the single- and multi-threaded execu-\ntions. Hence, the reduced Icache stalls in the multi-threaded\nexecution is likely due to the higher instruction locality ofthe code modules that are more active during multi-threadedexecution.\nMicro-architectural behavior follows similar trends for\nTPC-C for the multi-threaded execution compared to thesingle-threaded execution. DBMS D largely suffers from\nIcache stalls, and Shore-MT ,DBMS N , and Silo mainly suffer\nfrom Dcache stalls. DBMS M , once again, suffers less from\nthe Icache stalls than it does for the single-threaded execu-\ntion. This is, once again, likely due to the instruction locality\nof the modules that are more active during the multi-threadedexecution.\nFig. 14 Breakdowns of the CPU cycles for the multi-threaded experi-\nments while running TPC-C\nTable 7 Consumed bandwidth in GB/s as we increase the database size\nfor multi-threaded execution when reading 1 row per transaction\n1M B 1 0M B 1 0G B 1 0 0G B\nDBMS D 0 0 0 0\nShore-MT 0 0 2 2DBMS M 0 0 1 -DBMS N 5.2 5.1 6.2 6.2Silo 0 0 8.2 8.3\n8 Memory bandwidth consumption\nThis section presents the consumed memory bandwidth for\nthe sensitivity to data size and to work per transaction micro-\nbenchmarks and TPC-C benchmark. We measured both\nsingle- and multi-threaded consumed memory bandwidth.We observed that the consumed single-threaded bandwidth\nis always less than 1 GB/s for all the systems. Hence, we\nomit the single-threaded bandwidth results, and focus on themulti-threaded ones.\n8.1 Data size micro-benchmark\nTable 7presents the consumed bandwidth for increasing data\nsize. All the systems consume signiﬁcantly lower memory\nbandwidth than the maximum available bandwidth. Whilethe maximum available bandwidth is 66 GB/s, the maximumconsumed bandwidth is 8.3 GB/s by Silo for 100 GB of data\nsize.\nDBMS D ,Shore-MT , and DBMS M consume signiﬁcantly\nlower bandwidth than DBMS N and Silo.DBMS D and Msuf-\nfers from Icache stalls, which prevents them from stressing\nthe memory bandwidth. Shore-MT suffers from Dcache stalls\nfor 10 GB and 100 GB. As being a disk-based system, it nev-\nertheless has a signiﬁcantly larger instruction footprint and\nis signiﬁcantly slower (see Table 2) than DBMS N and Silo.\nAs a result, it stresses the memory bandwidth only modestly.\n123\n\nMicro-architectural analysis of in-memory OLTP: Revisited 657\nTable 8 Consumed bandwidth in GB/s as we increase the amount of\nwork per transaction for multi-threaded execution for a database of size100 GB (10 GB for DBMS N)\n1 row 10 rows 100 rows\nDBMS D 0 2 3\nShore-MT 2 2 2.5DBMS M 1 7 11DBMS N 6.2 8.5 8.4Silo 8.3 8.3 8.4\nDBMS N has relatively high bandwidth consumption for\n1 and 10 MB of data. This is due to DBMS N ’s transaction\nsetup processing. As the data size is increased the consumedbandwidth is also increased. As DBMS N spends more time\non index lookup when the data size is increased, it consumes\nmore memory bandwidth.\nSilo consumes no memory bandwidth for 1 and 10 MB of\ndata as the data is mostly cache-resident. Silo consumes the\nhighest bandwidth among the systems we analyze for 10 and\n100 GB of data. Silo eliminates the overheads of disk-based\nsystems and also does not suffer from the cost of transaction\nsetup and instantiation. As a result, it delivers the highest\nrelative throughput (see Table 2) and stresses the memory\nbandwidth the highest. Nevertheless, Silo’s maximum con-\nsumed bandwidth is signiﬁcantly less than the maximum\navailable bandwidth of 66 GB/s. This shows that OLTP sys-tems generate only modest amount of memory trafﬁc andhence severely under-utilize the memory bandwidth.\n8.2 Work per transaction micro-benchmark\nTable 8shows the consumed bandwidth as we increase the\namount of work per transaction. We use 100 GB of database\nfor all the systems, except DBMS M and 10 GB of database\nfor DBMS M . The consumed bandwidth is increased as the\namount of work per transaction is increased for all the sys-\ntems. This increase is more pronounced for DBMS M .A s\nthe amount of work per transaction is increased DBMS M\nsuffers less and less from the legacy code modules that it\nborrows from DBMS D . Hence, it suffers less and less from\nthe Icache stalls. As a result, it’s relative throughput and con-sumed bandwidth is signiﬁcantly increased.\nThe increase is also observable for DBMS D and DBMS N .\nBoth DBMS D and DBMS N suffer from the code outside the\nstorage manager. As the number of rows read per transaction\nis increased, the effects of the code outside the storage man-\nager is reduced. Hence, the stress on the memory bandwidthis increased.\nThe increase in the consumed bandwidth is signiﬁcantly\nless for DBMS D than it is for DBMS M .DBMS D ’s legacy\ncodebase overheads are heavier than they are for DBMS M .Table 9 Consumed bandwidth\nin GB/s for TPC-C benchmarkfor multi-threaded executionTPC-C\nDBMS D 0\nShore-MT 2.6DBMS M 0DBMS N 2.6Silo 5.3\nAs a result, the increased amount of work inside the transac-\ntion mitigates the overheads of the code outside the storage\nmanager only partially. This effect is more clean in Fig. 6.A s\nthe ﬁgure shows, DBMS D spends ∼35% of its time outside\nthe storage manager when reading 100 rows per transaction.\nShore-MT and Silo’s consumed bandwidths are only mod-\nestly increased. This is because Shore-MT and Silo hard code\ntransactions in C++. Hence, the increased amount of work\nper transaction stresses the memory bandwidth at a similarlevel per unit of a time. As a result, the consumed bandwidthremains mostly stable as the amount of work per transaction\nis increased.\nOverall, despite the increased amount of work per trans-\naction, all the OLTP systems we examine consume only a\nmodest fraction of the maximum available bandwidth. While\nthe maximum available bandwidth is 66 GB/s, the maximumconsumed bandwidth is 11 GB/s by DBMS M when reading\n100 rows per transaction.\n8.3 TPC-C\nIn this section, we examine the amount of consumed band-\nwidth for TPC-C benchmark for a database of size 100 GB,except for DBMS M . We use 32 GB of database for DBMS M .\nTable 9shows the results. The consumed bandwidth values\nare less for all the systems compared to the micro-benchmark.This is expected as TPC-C transactions are more complex\nwith more workload locality, and hence require more on-chip\ncomputation rather than stressing memory.\nDBMS D and M, being Icache-stalls-bound systems, con-\nsume very low memory bandwidth. Shore-MT ,DBMS N , and\nSilo, being Dcache- and resource/dependency-stalls-bound,\nconsume certain amount of memory bandwidth. Silo, being\nthe fastest OLTP system we analyze, consumes the highest\namount of memory bandwidth. Unlike the micro-benchmark,\nDBMS N consumes a similar amount of bandwidth to Shore-\nMT. This is due to DBMS N ’s increased time spent on tuple\nserializing/deserializing, preventing DBMS N from creat-\ning higher memory trafﬁc. Nevertheless, all the systems weanalyze consume bandwidth that is signiﬁcantly below the\nmaximum available bandwidth. While maximum available\nbandwidth is 66 GB/s, the highest consumed bandwidth is5.3 GB/s by Silo.\n123\n\n658 U. Sirin et al.\nTable 10 Normalized throughput values for hyper-threading evaluation\nMicro-bench. TPC-C\nST MT ST MT\nDBMS M 1.2 1.3 1.2 1.3\nSilo 1.4 1.6 1.4 1.7\n9 Acceleration features\nIn this section, we examine three popular acceleration\nfeatures that today’s processors provide: hyper-threading,turbo-boost, and hardware prefetchers. We present normal-ized throughput values.\nWe use DBMS M and Silo when running the read-only\nmicro-benchmark while probing 1 row per transaction, andwhen running the TPC-C benchmark for single- and multi-\nthreaded executions. We use 10 GB of database for the micro-\nbenchmark and 32 GB of database for TPC-C when proﬁlingDBMS M . We use 100 GB of database for Silo.\n9.1 Hyper-threading\nTable 10shows normalized throughput values for hyper-\nthreading evaluation. For single-threaded execution, thenormalized throughput shows the throughput improvementwhen running two threads on the same physical core com-\npared to running a single thread on a single physical core. For\nmulti-threaded execution, the normalized throughput showsthe throughput improvement when running 28 threads on 14\nphysical cores compared to running 14 threads on 14 physical\ncores (assuming that 14 threads deliver the highest through-put). We use the DBMS’s and/or OS’s relevant conﬁguration\ninterface to bind the threads to a single socket and allocate\nmemory locally.\nWe observe that hyper-threading is modestly useful for\nDBMS M , whereas it is signiﬁcantly useful for Silo. Hyper-\nthreading is the most useful when there are long-latency data\nstalls that can easily be overlapped. As Silo highly suffers\nfrom Dcache stalls, hyper-threading provides a more signif-\nicant speedup for Silo. We also observe that the improved\nthroughput is higher for multi-threaded execution than it isfor single-threaded both for DBMS M and Silo. This is likely\ndue to the increased sharing of the data structures at the last-\nlevel cache when running concurrently on the multiple cores.\n9.2 Turbo-boost\nTable 11shows normalized throughput values for turbo-boost\nevaluation. We present the throughput values with turbo-\nboost turned on normalized to the ones with turbo-boostturned off.Table 11 Normalized throughput values for turbo-boost evaluation\nMicro-bench. TPC-C\nST MT ST MT\nDBMS M 1.3 1.1 1.2 1.2\nSilo 1.2 1.1 1.2 1.1\nTable 12 Normalized throughput values for prefetcher evaluation\nMicro-bench. TPC-C\nST MT ST MT\nDBMS M 1.0 1.0 1.0 1.0\nSilo 1.0 1.0 1.0 1.0\nWe observe that both DBMS M and Silo modestly beneﬁt\nfrom turbo-boost. Turbo-boost provides the highest speedupswhen the computation is arithmetic-operation-heavy ratherthan memory-access-bound as it is the case for OLTP . As a\nresult, both systems only modestly beneﬁt from turbo-boost\nfeature.\n9.3 Hardware prefetchers\nTable 12shows normalized throughput values for prefetcher\nevaluation. We present the throughput values with prefetch-\ners disabled normalized to the ones with prefetchers enabled.There are four hardware prefetchers that today’s server pro-\ncessors provide: L1 next line, L1 streamer, L2 next line, and\nL2 streamer prefetchers [ 18]. We disable them all and enable\nthem all.\nWe observe that prefetchers have no visible effect on the\nOLTP system performance. OLTP workloads are random-data-access-bound and have low spatial locality. As a result,the streamer prefetchers might not be providing a visible\nperformance gain when enabled. DBMS M mainly suffers\nfrom Icache stalls. Hence, the improvement that the nextline prefetchers bring is likely to be negligible. Silo uses\nsoftware prefetching to prefetch consecutive cache lines that\nbelong to the same index node, during its index traversal.Hence, the disabled next line prefetcher is likely not creating\nan observable effect on Silo’s throughput.\n10 Ivy Bridge versus Broadwell\nIn this section, we compare two Intel generations in terms of\ntheir micro-architectural behavior when running the micro-\nbenchmark that randomly reads 1 row and the TPC-Cbenchmark. We use 100 GB of database. We use 10 GB of\na database for the micro-benchmark and 32 GB of a database\nfor TPC-C for DBMS M . We examine the Intel Xeon v2 line\nIvy Bridge micro-architecture and Intel Xeon v4 line Broad-\n123\n\nMicro-architectural analysis of in-memory OLTP: Revisited 659\nFig. 15 Breakdowns of the CPU cycles for successive Intel micro-architectures when running micro-benchmark\nFig. 16 Breakdowns of the CPU cycles for successive Intel micro-architectures when running TPC-C\nwell micro-architecture. We choose these two generations\nas there is major micro-architectural change from the Ivy\nBridge to the Haswell micro-architecture, especially in theinstruction fetch unit of the processors [ 14] (see Sect. 4.1.1 ,\nparagraph 2). Broadwell is slightly improved version of the\nHaswell micro-architecture. As OLTP systems are known toseverely suffer from Icache stalls, we examine how effective\nthe instruction fetch unit improvement for OLTP systems.\nFigures 15and 16show the results. We observe that there\nis a signiﬁcant change in the micro-architectural behaviorof the OLTP systems across the two processor generations.\nOn the Ivy Bridge micro-architecture, all the systems except\nfor Silo mainly suffer from Icache stalls. On the Broadwell\nmicro-architecture, Shore-MT and DBMS N ’s main micro-\narchitectural bottlenecks shift from Icache stalls to Dcache\nstalls. Similarly, DBMS D and Msuffer signiﬁcantly less\nfrom the Icache stalls on the Broadwell micro-architecture\ncompared to the Ivy Bridge micro-architecture. This shows\nthat the advances in the instruction fetch unit of the processorsigniﬁcantly help for reducing the Icache stalls.\nOur ﬁnding on Ivy Bridge vs. Broadwell corroborates the\nrecent work of Yasin et al. [ 60] where SPEC benchmarks\nare evaluated across Ivy Bridge and Skylake (the generationafter Broadwell) micro-architectures. Yasin et al. also show\nthat the improvement on the Skylake micro-architecture,Table 13 Throughput on Broadwell normalized to throughput on Ivy\nBridge\nMicro-bench. TPC-C\nDBMS D 2.0 1.6\nShore-MT 1.6 1.6DBMS M 2.5 1.4DBMS N 1.6 1.5Silo 1.1 1.3\nwhich inherits the improvements from the Broadwell micro-\narchitecture, signiﬁcantly reduces the Icache stalls.\nTable 13shows the throughput of the Broadwell machine\nnormalized to the throughput of the Ivy Bridge machine.\nAs expected, the Broadwell machine delivers signiﬁcantly\nhigher throughput than the Ivy Bridge machine thanks to itsmicro-architectural improvements.\nExisting work on micro-architectural analysis of OLTP\nsystems has mostly used an Ivy Bridge or an earlier micro-architecture generation. As a result, their conclusions were\nmostly referring to the instruction overheads of disk-based\nand in-memory OLTP systems [ 48,49,53,54]. In this paper,\nwe take the existing work one step ahead and provide\n123\n\n660 U. Sirin et al.\nconclusions on one of the latest generations of Intel micro-\narchitectures.\n11 Lessons learned\nThis section summarizes the highlights of our work. In-\nmemory OLTP systems implement a series of optimizations\nto reduce the instruction footprint and improve cache uti-lization. Despite all of the optimizations, they severely\nunder-utilize the micro-architectural features similarly to the\ntraditional disk-based systems.\nInstruction stalls DBMS D and Mincur the highest num-\nber of instruction stalls due to the large amount of legacy\ncode they use. Hence, OLTP systems relying on legacy code\nmodules should ﬁrst optimize their instruction footprint.Transaction compilation is a promising technique that can\nbe used to reduce the instruction footprint size and complex-\nity of an OLTP system as shown by [ 34,48] (see Sect. 6.1).\nShore-MT , despite being a disk-based system, does not\nsuffer from instruction cache miss stalls. Hence, today’s pro-\ncessors are powerful enough to fetch and execute instructionstream of complex disk-based systems without stalling theinstruction fetch unit. DBMS N and Silo do not suffer from the\ninstruction stalls either. This shows that ground-up designed\nOLTP systems’ instruction footprint is simple enough fortoday’s processors to fetch instructions without stalling in\nthe instruction cache (see Sect. 4.1.1 ).\nData stalls DBMS N and Silo mainly suffer from data\ncache stalls. DBMS M suffers from data stalls only when\nthe instruction cache misses are mitigated by an increased\ninstruction locality in their instruction stream (see Sect. 4.2).\nThe data cache misses for DBMS N and Silo are mostly due\nto the random data accesses made during the index traver-\nsal. Hence, ground-up designed in-memory systems should\nﬁrstly optimize their index structures. We have seen thatMasstree [ 32]u s e db y Silo signiﬁcantly outperforms the red-\nblack tree used by DBMS N (see Sect. 4.1.3 ,4.2.1 ).\nWhile using an efﬁcient index structure improves the per-\nformance, its execution time is still dominated by data cache\nmisses caused by the random data accesses during the index\ntraversal. Hence, ground-up designed in-memory OLTP sys-tems should adopt techniques that can mitigate the negativeperformance effect of random data accesses (see Sect. 4.2.1 ).\nMitigating data stalls One promising way to mitigate the\nrandom data accesses is using co-routines. Co-routines is acheap thread interleaving mechanism that allows interleaving\nlong-latency data stalls with computation. Psaropoulos et al.\n[38–40] and Jonathan et al. [ 19] have shown that co-routines\ncan successfully be used to improve index join and index\nlookup.\nAnother promising technique to mitigate the random data\naccesses is using machine learning to learn the distribution ofthe keys and jump to the index location that the key belongs\nto without actually performing the index traversal. Kraska etal. [25], Sirin et al. [ 30], and Ding et al. [ 11] have shown that\nmachine-learned indexes can successfully replace/accelerate\nthe index search operation.\nRow- versus column-oriented storage All the systems\nwe proﬁle use the row-oriented storage format. However,\nrecently, several systems adopted the column-oriented stor-\nage, mostly to use a single format for both transactional andanalytical processing [ 36]. Using column-oriented storage\nas opposed to row-oriented storage requires making multiple\nrandom data accesses per row access, as different attributesof the same row will be spread around the main-memory.\nToday’s processors are capable of overlapping random\ndata accesses if their locations in the instruction stream areclose to each other. Hence, making multiple random dataaccesses might not hurt the performance, if the data access\nprimitives are well-implemented.\nAlternatively, today’s processors have the software prefetch-\ning capability. Programs can prefetch those memory blocks\nahead of the access such that the memory access times are\noverlapped with useful work. Developers can use softwareprefetching to mitigate the negative effect of making multiple\nrandom data accesses.\nOn the other hand, bringing multiple cache lines to the\nprocessor cache might result in inefﬁciently using the pro-cessor caches. If an attribute is 8 bytes and we access four\nattributes of the same row, we bring 64 ×4=256 bytes (as\neach cache line is 64 bytes), instead of 8 ×4=32 bytes. The\n256−32=224 bytes of the data that is brought to the cache\npollutes the cache, which can hurt the overall performance\nof the system.\nHardware In this study, we conclude that software-level\noptimizations do not directly translate into more efﬁcient\nutilization of micro-architectural resources, and might evenhinder it, on modern processors. One needs to optimizethe hardware and software together as the next step putting\nmicro-architectural utilization as a high priority goal.\nExhibiting low instruction- and memory-level parallelism,\nOLTP workloads are unable to utilize the wide-issue, com-\nplex out-of-order cores. Most of the time goes to the memory\nstalls for bringing either instructions or data items from thememory. Instruction cache sizes have been unchanged for the\nlast decade due to the strict latency limitations, and we can-\nnot expect them to increase. On the other hand, improvedinstruction fetch units can make a signiﬁcant change inthe micro-architectural behavior of the OLTP systems (see\nSect. 10). Further advancements at the micro-architectural\nlevel, especially at the instruction fetch unit, can still have fur-ther potential impact to improve OLTP system performance,\nas popular OLTP systems such as DBMS D and Mthat we\nproﬁled still highly suffer from Icache stalls. Proﬁle-guided(i.e., feedback-directed) optimization via the compiler and\n123\n\nMicro-architectural analysis of in-memory OLTP: Revisited 661\nhardware support for software code prefetching are shown to\nbe effective for reducing Icache stalls for server workloads,which also signiﬁcantly suffer from Icache stalls [ 4,8,59].\nAs ground-up designed OLTP systems mainly suffer from\nlong-latency data cache stalls, hardware designers can investmore on hardware mechanisms that can overlap long-latencydata stalls. Hyper-threading improves performance up to 70%\nin a carefully designed and implemented in-memory OLTP\nsystem (see Sect. 9.1). Turbo-boost and hardware prefetchers\nare modestly useful for OLTP workloads, as OLTP work-\nloads are memory-latency-bound (see Sects. 9.2,9.3). As\nthe processor’s power budget is limited, hardware designerscan invest more power budget on the features that would\noverlap long-latency data stalls such as larger number of\nhyper-threads per physical core.\nOn the other hand, whatever the size of the last-level cache\n(LLC) is, megabytes of LLC will not be enough to keep\nthe gigabytes of the data footprint of most standard OLTP\nbenchmarks. Hence, instead of using beefy and complexout-of-order cores consuming large amount of energy, using\nsimpler cores with intelligent hyper-threading mechanisms\ncan improve the throughput of OLTP applications with asmaller power budget [ 12,13,31]. Sirin et al. [ 47]h a v es h o w n\nthat low-power ARM processor can provide 1.7 to 3 times\nlower throughput with 3 to 15 times less power consumptionthan a state-of-the-art Intel Xeon processor, achieving up to9 times higher energy efﬁciency.\nWith that said, Kanev et al. [ 20] have shown that server\nworkloads partially beneﬁt from the wide-issue out-of-orderexecution. Hence, the use of wimpy cores with narrow-issue\nexecution engines might produce a suboptimal performance\nand may not satisfy some application requirements. Simi-larly, Sirin et al. [ 47] have shown that ARM processors’\nquantiﬁed latency can be up to 11 times higher than Intel\nXeon towards the tail of the latency distribution, which makesIntel Xeon more suitable for tail-latency-critical applications.\nGPU/FPGA-based acceleration of database systems is\nanother line of research that allows improving database per-\nformance by using alternative computing devices to power-hungry processors. [ 9,42,45] present techniques on using\nGPUs for accelerating analytical processing queries such as\nhash join. Kim et al. [ 24] have proposed a transaction process-\ning engine architecture that exploits the wide-parallelism.\nAlonso et al. [ 3] present an open-source hardware-software\nco-design platform for database systems, which uses CPUand FPGA as the main building blocks. [ 21,44] present\ntechniques on integrating FPGAs into common database\noperations such as data partitioning and regular expression.\nThese studies highlight the opportunities to enrich the tra-ditional computing space of database systems by alternative\ncomputing devices such as FPGAs and GPUs. As these com-\nputing devices provide massive parallelism and/or low-powerconsumption, they allow investigating the energy-efﬁciencyspace and potentially serve as the processors of the future\ndatabase systems.\n12 Conclusion\nIn this paper, we perform a detailed micro-architecturalanalysis of the in-memory OLTP systems contrasting themto the disk-based OLTP systems. Our study demonstrates\nthat in-memory OLTP systems spend most of their time in\nstalls similarly to the disk-based OLTP systems despite allthe design differences and lighter storage manager compo-nents of the memory-optimized systems. The lighter storage\nmanager components reduce the instruction footprint at the\nstorage manager layer, but the overall instruction footprintof an in-memory OLTP system is still large if the code base\nrelies on legacy code modules. This leads to a poor instruc-\ntion locality and high number of instruction cache misses.Ground-up designed in-memory OLTP systems can elim-\ninate the instruction cache misses. In the absence of the\ninstruction cache misses, the impact of long-latency datamisses surfaces up, resulting in spending ∼70% of the exe-\ncution time in stalls.\nAcknowledgements We would like to thank the anonymous reviewers\nand the member of DIAS laboratory for their feedback. This project hasreceived funding from the European Union Seventh Framework Pro-gramme (ERC-2013-CoG), under grant agreement no 617508 (ViDa),and Swiss National Science Foundation, Project No.: 200021 146407/1(Workload- and hardware-aware transaction processing).\nFunding Open Access funding provided by EPFL Lausanne.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-tation, distribution and reproduction in any medium or format, aslong as you give appropriate credit to the original author(s) and thesource, provide a link to the Creative Commons licence, and indi-cate if changes were made. The images or other third party materialin this article are included in the article’s Creative Commons licence,unless indicated otherwise in a credit line to the material. If materialis not included in the article’s Creative Commons licence and yourintended use is not permitted by statutory regulation or exceeds thepermitted use, you will need to obtain permission directly from the copy-right holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/ .\nAppendix A: Read-write micro-benchmark\nIn this appendix, we present the evaluation of the read-write\nversion of the micro-benchmark.\nA.1 Sensitivity to data size\nIn this section, we perform the sensitivity analysis for thedatabase size. Figure 17shows the CPU cycles breakdowns.\n123\n\n662 U. Sirin et al.\nFig. 17 Breakdowns of the CPU cycles as we increase the database size when running the read-write micro-benchmark\nAll the systems follow similar trends to what we observed for\nthe read-only micro-benchmark, except Shore-MT .Shore-\nMT suffers signiﬁcantly more from Dcache stalls for the read-\nwrite micro-benchmark than it is for the read-only micro-\nbenchmark. We examined the function call trace of Shore-\nMT. The increased Dcache stalls are due to the logging costs\nthat Shore-MT does for update queries, but not read-only\nqueries. Being a disk-based system, Shore-MT uses a heavy\ndata structure to keep a large amount of log information.\nDBMS N , on the other hand, uses command logging where\nonly the invoked transaction and its parameters are logged.\nHence, DBMS N suffers less from the logging operations.\nDBMS M has higher Dcache stalls compared to the read-\nonly micro-benchmark. This is likely due to that DBMS M\nuses a multi-version concurrency control mechanism, where\nupdates are kept in deltas. Each new version of the datais written to a new memory location called delta, and thepointer to the record is updated to point to the location of\nthe delta. Multiple versions are kept in multiple deltas that\nare chained one-after-the-other. Occasionally, the deltas areconsolidated. Chain traversal/consolidation requires more\nrandom data accesses and hence more Dcache stalls.\nDBMS N and Silo has less Dcache stalls compare to the\nread-only micro-benchmark. This is due to that update opera-\ntion has a higher data locality than the read, as update requires\nreading from and writing to the same data block. As a result,it results in less Dcache stalls.\nTable 14shows the normalized throughputs for the\nread-write micro-benchmark. We normalize the values with\nrespect to the read-only micro-benchmark, as the nor-malization across the systems provides similar results to\nTable 2. Read-write micro-benchmark is always slower. This\nis because the update operation requires more work for con-currency control and logging than the read-only operation.\nShore-MT ’s relative throughput is the lowest among the\nsystem. This highlights Shore-MT ’s inefﬁcient locking and\nlogging mechanisms.\nAs the data size increases, read-write micro-benchmark’s\nthroughput gets closer to the read-only micro-benchmark.Table 14 Normalized throughput for the read-write micro-benchmark.\nThroughput is normalized to the throughput values of the read-onlymicro-benchmark\n1M B 1 0M B 1 0G B 1 0 0G B\nDBMS D 0.7 0.7 0.8 0.8\nShore-MT 0.5 0.5 0.6 0.6DBMS M 0.7 0.7 0.7 –DBMS N 0.7 0.7 0.7 0.8Silo 0.8 0.8 0.8 0.9\nThis is because of the higher data locality of the read-write\nmicro-benchmark. As the data size is increased, data locality\nmatters more and more for the micro-benchmark’s through-put. Nevertheless, read-only micro-benchmark’s throughput\nis 20% to 50% higher than the read-write micro-benchmark.\nA.2 Sensitivity to work per transaction\nIn this section, we perform the sensitivity analysis for\nthe amount of work per transaction. Figure 18shows the\nCPU cycles breakdowns. We observe similar trends to\nthe read-only micro-benchmark As the number of rows\nupdated per transaction is increased DBMS D and M’s\nIcache stalls are decreased, and they become more and more\nDcache-stalls-bound. Shore-MT ,DBMS N and Silo, being\nalready Dcache-stalls-bound systems, have similar micro-architectural behavior across the varied amount of work pertransaction.\nTable 15shows the normalized throughputs. We normalize\nthe values with respect to the read-only micro-benchmark, asthe normalization across the systems provides similar results\nto Table 3. We observe that read-write transaction through-\nput is decreased as the amount of work per transaction isincreased for DBMS D and M. This is because the read-only\nmicro-benchmark beneﬁts more from the increased amount\nof work per transaction compared to the read-write micro-benchmark. The read-write micro-benchmark requires more\n123\n\nMicro-architectural analysis of in-memory OLTP: Revisited 663\nFig. 18 Breakdowns of the CPU cycles as we increase the amount of work per transaction when running the read-write micro-benchmark\nTable 15 Normalized throughput for the read-write micro-benchmark.\nThroughput is normalized to the throughput values of the read-onlymicro-benchmark\n1 row 10 rows 100 rows\nDBMS D 0.8 0.8 0.7\nShore-MT 0.6 0.8 0.8DBMS M 0.7 0.5 0.5DBMS N 0.8 0.8 0.8Silo 0.9 0.9 0.9\nwork and hence more instructions than the read-only micro-\nbenchmark. As a result, the increased instruction locality is\nless useful to the read-write micro-benchmark than the read-only micro-benchmark.\nAppendix B: Index, compilation, and data\ntype\nIn this section, we evaluate the index, compilation and data\ntype for the read-write micro-benchmark. Figure 19presents\nCPU cycles breakdowns for different index types having\ncompilation turned on and off. Compilation reduces theIcache stalls signiﬁcantly similar to the read-only micro-\nbenchmark. Table 16presents the normalized throughput\nvalues. Compilation improves the throughput by 5–7.7 ×sim-\nilar to the read-only micro-benchmark, showing how usefulit is to reduce the instruction and the data footprint of DBMS\nM.\nFigure 20shows the CPU cycles breakdowns for long and\nstring data types. Unlike the read-only micro-benchmark,\nDBMS M has similar amounts of Dcache stalls for long\nand string data types. The reason is the read-write micro-benchmark’s increased Dcache stalls for the long data type.\nAs a result, the overall CPU cycles breakdowns remains the\nsame for the long and string data types. DBMS N and Silo\npresent similar results to the read-only micro-benchmark.\nFig. 19 Breakdowns of the CPU cycles for different index structures\nwith and without compilation optimizations while running the read-write micro-benchmark\nTable 16 Normalized throughput for different index structures with and\nwithout compilation when running the read-write micro-benchmark\nMicro-bench.\nB-tree w/o comp. 1\nB-tree w/ comp. 5Hash w/o comp. 1.5Hash w/ comp. 11.5\nFig. 20 Breakdowns of the CPU cycles for string and long data types\nwhile running the read-write micro-benchmark\n123\n\n664 U. Sirin et al.\nTable 17 Normalized\nthroughput for string and longdata types while running theread-write micro-benchmarkLong String\nDBMS M 1 0.6\nDBMS N 1 0.9Silo 1 0.6\nDBMS N has less Dcache stalls for the string data type,\nwhereas Silo has similar amount of Dcache stalls for the\nstring and long data types.\nTable 17shows the normalized throughput values for\nstring and long data types. The results are similar to the read-\nonly micro-benchmark. While DBMS M and Silo have lower\nthroughput for string due to the increased amount of work forstring, DBMS N has similar throughput thanks to utilization\nof the workload locality for the string data type.\nAppendix C: CPU cycles categorization\nIn this section, we present how we map each CPU cycles\ncategory that VTune provides to the individual categoriesthat we use. Table 18presents the mapping.\nTable 18 The mapping between VTune’s original and our simpliﬁed\nCPU cycles categorization\nVTune’s original category Mapped category\nBack-End, Memory Dcache\nBack-End, Core Resource/dependencyFront-End, Front-End Latency,\nICache MissesIcache\nFront-End, Front-End Latency,\nITLB OverheadIcache\nFront-End, Front-End Latency,\nBranch ResteerBranch misprediction\nFront-End, Front-End Latency,\nDSB SwitchesDecoding\nFront-End, Front-End Latency,\nLength Changing PreﬁxesDecoding\nFront-End, Front-End Latency, MS\nSwitchesDecoding\nFront-End, Front-End Bandwidth Decoding\nBad Speculation Branch mispredictionRetiring Retiring\nReferences\n1. TPC Transcation Processing Performance Council. http://www.\ntpc.org/\n2. Ailamaki, A., DeWitt, D.J., Hill, M.D., Wood, D.A.: DBMSs on a\nModern Processor: Where Does Time Go? In: VLDB, pp. 266–277(1999)3. Alonso, G., Roscoe, T., Cock, D., Ewaida, M., Kara, K., Korolija,\nD., Sidler, D., Wang, Z.: Tackling Hardware/Software co-designfrom a database perspective. In: CIDR (2020)\n4. Ayers, G., Nagendra, N.P ., August, D.I., Cho, H.K., Kanev, S.,\nKozyrakis, C., Krishnamurthy, T., Litz, H., Moseley, T., Ran-ganathan, P .: AsmDB: Understanding and Mitigating Front-EndStalls in Warehouse-Scale Computers. In: ISCA, pp. 462–473(2019)\n5. Barroso, L.A., Gharachorloo, K., Bugnion, E.: Memory system\ncharacterization of commercial workloads. In: ISCA, pp. 3–14(1998)\n6. Beamer, S., Asanovic, K., Patterson, D.: Locality Exists in graph\nprocessing: workload characterization on an Ivy bridge server. In:IISWC, pp. 56–65 (2015)\n7. Bernstein, P .A., Goodman, N.: Multiversion concurrency control-\ntheory and algorithms. ACM TODS 8(4), 465–483 (1983)\n8. Chen, D., Li, D.X., Moseley, T.: AutoFDO: automatic feedback-\ndirected optimization for warehouse-scale applications. In: CGO,pp. 12–23 (2016)\n9. Chrysogelos, P ., Karpathiotakis, M., Appuswamy, R., Ailamaki,\nA.: HetExchange: Encapsulating heterogeneous CPU-GPU par-allelism in JIT compiled engines. Proc. VLDB Endow. 12(5),\n544–556 (2019)\n10. Diaconu, C., Freedman, C., Ismert, E., Larson, P .A., Mittal, P .,\nStonecipher, R., V erma, N., Zwilling, M.: Hekaton: SQL Server’sMemory-optimized OLTP Engine. In: SIGMOD, pp. 1243–1254(2013)\n11. Ding, J., Minhas, U.F., Y u, J., Wang, C., Do, J., Li, Y ., Zhang, H.,\nChandramouli, B., Gehrke, J., Kossmann, D., Lomet, D., Kraska,T.: ALEX: An Updatable Adaptive Learned Index. Technical report(2020)\n12. Ferdman, M., Adileh, A., Kocberber, O., V olos, S., Alisafaee, M.,\nJevdjic, D., Kaynak, C., Popescu, A.D., Ailamaki, A., Falsaﬁ, B.:Clearing the clouds: a study of emerging scale-out workloads onmodern hardware. In: ASPLOS, pp. 37–48 (2012)\n13. Haj-Yihia, J., Yasin, A., Asher, Y .B., Mendelson, A.: Fine-grain\npower breakdown of modern out-of-order cores and its implicationson skylake-based systems. ACM Trans. Archit. Code Optim. 13,1\n(2016)\n14. Hammarlund, P ., Martinez, A.J., Bajwa, A.A., Hill, D.L., Hallnor,\nE.G., Jiang, H., Dixon, M.G., Derr, M., Hunsaker, M., Kumar,R., Osborne, R.B., Rajwar, R., Singhal, R., D’Sa, R., Chappell,R., Kaushik, S., Chennupaty, S., Jourdan, S., Gunther, S., Piazza,T., Burton, T.: Haswell: the fourth-generation intel core processor.IEEE Micro 34(2), 6–20 (2014)\n15. Hardavellas, N., Pandis, I., Johnson, R., Mancheril, N., Ailamaki,\nA., Falsaﬁ, B.: Database Servers on Chip Multiprocessors: Limi-\ntations and Opportunities. In: CIDR, pp. 79–87 (2007)\n16. Harizopoulos, S., Abadi, D.J., Madden, S., Stonebraker, M.: OLTP\nthrough the looking glass, and what we found there. In: SIGMOD,pp. 981–992 (2008)\n17. Intel: Intel VTune Ampliﬁer XE Performance Proﬁler. http://\nsoftware.intel.com/en-us/articles/intel-vtune-ampliﬁer-xe/\n18. Intel: Intel(R) 64 and IA-32 Architectures Optimization Reference\nManual (2016)\n19. Jonathan, C., Minhas, U.F., Hunter, J., Levandoski, J.J., Nishanov,\nG.V .: Exploiting coroutines to attack the killer nanoseconds. Proc.VLDB Endow. 11(11), 1702–1714 (2018)\n20. Kanev, S., Darago, J.P ., Hazelwood, K., Ranganathan, P ., Moseley,\nT., Wei, G.Y ., Brooks, D.: Proﬁling a warehouse-scale computer.In: ISCA, pp. 158–169 (2015)\n21. Kara, K., Giceva, J., Alonso, G.: FPGA-based data partitioning. In:\nSIGMOD, pp. 433–445 (2017)\n22. Keeton, K., Patterson, D.A., He, Y .Q., Raphael, R.C., Baker, W.E.:\nperformance characterization of a quad pentium pro SMP usingOLTP workloads. In: ISCA, pp. 15–26 (1998)\n123\n\nMicro-architectural analysis of in-memory OLTP: Revisited 665\n23. Kemper, A., Neumann, T., Finis, J., Funke, F., Leis, V ., Mühe,\nH., Mühlbauer, T., Rödiger, W.: Processing in the hybrid OLTP &OLAP main-memory database system hyper. IEEE DEBull 36(2),\n41–47 (2013)\n24. Kim, K., Johnson, R., Pandis, I.: BionicDB: Fast and power-\nefﬁcient OLTP on FPGA. In: EDBT, pp. 301–312 (2019)\n25. Kraska, T., Beutel, A., Chi, E.H., Dean, J., Polyzotis, N.: The case\nfor learned index structures. In: SIGMOD, pp. 489–504 (2018)\n26. Larson, P ., Zwilling, M., Farlee, K.: The Hekaton memory-\noptimized OLTP engine. IEEE DEBull 36(2), 34–40 (2013)\n27. Lee, J., Muehle, M., May, N., Faerber, F., Sikka, V ., Plattner, H.,\nKrüger, J., Grund, M.: High-performance transaction processing inSAP HANA. IEEE DEBull 36(2), 28–33 (2013)\n28. Levandoski, J., Lomet, D., Sengupta, S.: The Bw-Tree: A B-tree\nfor new hardware platforms. In: ICDE, pp. 302–313 (2013)\n29. Lindstrom, J., Raatikka, V ., Ruuth, J., Soini, P ., V akkila, K.: IBM\nsolidDB: In-memory database optimized for extreme speed andavailability. IEEE DEBull 36(2), 14–20 (2013)\n30. Llaveshi, A., Sirin, U., Ailamaki, A., West, R.: Accelerating B+tree\nsearch by using simple machine learning techniques. In: AIDB, pp.1–10 (2019)\n31. Lotﬁ-Kamran, P ., Grot, B., Ferdman, M., V olos, S., Kocberber, O.,\nPicorel, J., Adileh, A., Jevdjic, D., Idgunji, S., Ozer, E., Falsaﬁ, B.:\nscale-out processors. In: ISCA, p. 500–511 (2012)\n32. Mao, Y ., Kohler, E., Morris, R.T.: Cache craftiness for fast multi-\ncore key-value storage. In: EuroSys, pp. 183–196 (2012)\n33. MemSQL. http://www.memsql.com/\n34. Neumann, T.: Efﬁciently Compiling Efﬁcient Query Plans for Mod-\nern Hardware. Proc. VLDB Endow. 4(9), 539–550 (2011)\n35. Neumann, T., Leis, V .: Compiling database queries into machine\ncode. IEEE DEBull 37(1), 3–11 (2014)\n36. Neumann, T., Mühlbauer, T., Kemper, A.: Fast serializable multi-\nversion concurrency control for main-memory database systems.In: SIGMOD, p. 677–689 (2015)\n37. Pandis, I., Johnson, R., Hardavellas, N., Ailamaki, A.: Data-\noriented transaction execution. PVLDB 3(1), 928–939 (2010)\n38. Psaropoulos, G., Legler, T., May, N., Ailamaki, A.: Interleaving\nwith coroutines: a practical approach for robust index joins. Proc.VLDB Endow. 11(2), 230–242 (2017)\n39. Psaropoulos, G., Legler, T., May, N., Ailamaki, A.: Interleaving\nwith Coroutines: thisatic and practical approach to hide memorylatency in index joins. VLDB J. 28(4), 451–471 (2019)\n40. Psaropoulos, G., Oukid, I., Legler, T., May, N., Ailamaki, A.: Bridg-\ning the latency gap between NVM and DRAM for latency-boundoperations. In: DaMoN, pp. 13:1–13:8 (2019)\n41. Ranganathan, P ., Gharachorloo, K., Adve, S.V ., Barroso, L.A.: Per-\nformance of database workloads on shared-memory systems with\nout-of-order processors. In: ASPLOS, pp. 307–318 (1998)\n42. Raza, A., Chrysogelos, P ., Sioulas, P ., Indjic, V ., Anadiotis, A.G.,\nAilamaki, A.: GPU-accelerated data management under the test oftime. In: CIDR (2020)\n43. Shore-MT: Shore-MT Ofﬁcial Website. http://diaswww.epﬂ.ch/\nshore-mt/44. Sidler, D., István, Z., Owaida, M., Kara, K., Alonso, G.: doppioDB:\na hardware accelerated database. In: SIGMOD, pp. 1659–1662(2017)\n45. Sioulas, P ., Chrysogelos, P ., Karpathiotakis, M., Appuswamy, R.,\nAilamaki, A.: Hardware-conscious hash-joins on GPUs. In: ICDE,pp. 698–709 (2019)\n46. Sirin, U., Ailamaki, A.: Micro-architectural analysis of OLAP: lim-\nitations and opportunities. Proc. VLDB Endow. 13(6), 840–853\n(2020)\n47. Sirin, U., Appuswamy, R., Ailamaki, A.: OLTP on a server-grade\nARM: power, throughput and latency comparison. In: DaMoN, pp.10:1–10:7. ACM (2016)\n48. Sirin, U., Tözün, P ., Porobic, D., Ailamaki, A.: Micro-architectural\nanalysis of in-memory OLTP . In: SIGMOD, pp. 387–402 (2016)\n49. Sirin, U., Yasin, A., Ailamaki, A.: A Methodology for OLTP micro-\narchitectural analysis. In: DaMoN, pp. 1:1–1:10 (2017)\n50. Stets, R., Gharachorloo, K., Barroso, L.: A Detailed comparison of\ntwo transaction processing workloads. In: WWC, pp. 37–48 (2002)\n51. Stonebraker, M., Madden, S., Abadi, D.J., Harizopoulos, S.,\nHachem, N., Helland, P .: The end of an architectural era: (It’s Timefor a Complete Rewrite). In: VLDB, pp. 1150–1160 (2007)\n52. Stonebraker, M., Weisberg, A.: The V oltDB Main Memory DBMS.\nIEEE DEBull 36(2), 21–27 (2013)\n53. Tözün, P ., Gold, B., Ailamaki, A.: OLTP in Wonderland—Where\ndo cache misses come from in major OLTP components? In:DaMoN, pp. 8:1–8:6 (2013)\n54. Tözün, P ., Pandis, I., Kaynak, C., Jevdjic, D., Ailamaki, A.: From\nA to E: Analyzing TPC’s OLTP Benchmarks—The Obsolete, TheUbiquitous, The Unexplored. In: EDBT, pp. 17–28 (2013)\n55. Tu, S., Zheng, W., Kohler, E., Liskov, B., Madden, S.: Speedy\ntransactions in multicore in-memory databases. In: SOSP , pp. 18–32 (2013)\n56. V oltDB. http://www.voltdb.com\n57. Wenisch, T.F., Ferdman, M., Ailamaki, A., Falsaﬁ, B., Moshovos,\nA.: Temporal streams in commercial server applications. In:IISWC, pp. 99–108 (2008)\n58. Yasin, A.: A top-down method for performance analysis and coun-\nters architecture. In: ISPASS, pp. 35–44 (2014)\n59. Yasin, A., Ben-Asher, Y ., Mendelson, A.: Deep-dive Analysis of\nthe Data Analytics Workload in CloudSuite. In: IISWC, pp. 202–211 (2014)\n60. Yasin, A., Haj-Yahya, J., Ben-Asher, Y ., Mendelson, A.: A metric-\nguided method for discovering impactful features and architecturalinsights for skylake-based processors. TACO 16(4), 46:1–46:25\n(2020)\n61. Y u, X., Bezerra, G., Pavlo, A., Devadas, S., Stonebraker, M.: Star-\ning into the Abyss: an evaluation of concurrency control with one\nthousand cores. PVLDB 8(3), 209–220 (2014)\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\n123",
  "textLength": 112107
}