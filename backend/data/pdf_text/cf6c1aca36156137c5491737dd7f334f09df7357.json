{
  "paperId": "cf6c1aca36156137c5491737dd7f334f09df7357",
  "title": "The three pillars of machine programming",
  "pdfPath": "cf6c1aca36156137c5491737dd7f334f09df7357.pdf",
  "text": "MIT Open Access Articles\nThe three pillars of machine programming\nThe MIT Faculty has made this article openly available. Please share\nhow this access benefits you. Your story matters.\nCitation: Gottschlich, Justin et al. \"The three pillars of machine programming.\" MAPL 2018: \nProceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and \nProgramming Languages, June 2018, Philadelphia, Pennsylvania, Association for Computing \nMachinery, June 2018 © 2018 ACM\nAs Published: http://dx.doi.org/10.1145/3211346.3211355\nPublisher: Association for Computing Machinery (ACM)\nPersistent URL: https://hdl.handle.net/1721.1/129780\nVersion: Original manuscript: author's manuscript prior to formal peer review\nTerms of use: Creative Commons Attribution-Noncommercial-Share Alike\n\n\nThe Three Pillars of Machine Programming\nJustin Gottschlich\nIntel Labs, USA\njustin.gottschlich@intel.comArmando Solar-Lezama\nMIT, USA\nasolar@csail.mit.eduNesime Tatbul\nIntel Labs and MIT, USA\ntatbul@csail.mit.edu\nMichael Carbin\nMIT, USA\nmcarbin@csail.mit.eduMartin Rinard\nMIT, USA\nrinard@csail.mit.eduRegina Barzilay\nMIT, USA\nregina@csail.mit.edu\nSaman Amarasinghe\nMIT, USA\nsaman@csail.mit.eduJoshua B. Tenenbaum\nMIT, USA\njbt@mit.eduTim Mattson\nIntel Labs, USA\ntimothy.g.mattson@intel.com\nAbstract\nIn this position paper, we describe our vision of the future of\nmachine programming through a categorical examination of\nthree pillars of research. Those pillars are: (i)intention, (ii)in-\nvention, and (iii)adaptation. Intention emphasizes advance-\nments in the human-to-computer and computer-to-machine-\nlearning interfaces. Invention emphasizes the creation or\nrefinement of algorithms or core hardware and software\nbuilding blocks through machine learning (ML). Adaptation\nemphasizes advances in the use of ML-based constructs to\nautonomously evolve software.\nKeywords program synthesis, machine programming, soft-\nware development, software maintenance, intention, inven-\ntion, adaptation\n1 Introduction\nProgramming is a cognitively demanding task that requires\nextensive knowledge, experience and a large degree of cre-\nativity, and is notoriously difficult to automate. Machine\nlearning (ML) has the capacity to reshape the way soft-\nware is developed. At some level, this has already begun,\nas machine-learned components progressively replace com-\nplex hand-crafted algorithms in domains such as natural-\nlanguage understanding and vision. Yet, we believe that it is\npossible to move much further. We envision machine learn-\ning and automated reasoning techniques that will enable new\nprogramming systems; systems that will deliver a significant\ndegree of automation to reduce the cost of producing secure,\ncorrect, and efficient software. These systems will also en-\nable non-programmers to harness the full power of modern\ncomputing platforms to solve complex problems correctly\nand efficiently. We call such efforts machine programming .\n1.1 Why Now?\nProgramming is the process of turning a problem defini-\ntion (the intent ) into a sequence of instructions that when\nexecuted on a computer, produces a solution to the origi-\nnal problem. Over time, a program must be maintained as\nFigure 1. The Three Pillars of Machine Programming: Inten-\ntion, Invention, and Adaptation. Each pillar in the diagram\nincludes a few example sub-domains generally related to\nthem.\nit adapts to changes in the program’s goals, errors in the\nprogram, and the features in new computer platforms. A\nmachine programming system is any system that automates\nsome or all of the steps of turning the user’s intent into an\nexecutable program and maintaining that program over time.\nThe automation of programming has been a goal of the\nprogramming systems community since the birth of Fortran\nin the 1950s. The first paper on “The FORTRAN Automatic\nCoding System” made it clear that its goal was “for the 704\n[IBM’s next large computer] to code problems for itself and\nproduce as good programs as human coders (but without\nthe errors)” [ 8]. The broader AI community has also been\ninterested in automatic programming dating back to the Pro-\ngrammer’s Apprentice Project back in the late 1970s [ 64].\nA number of technological developments over the past few\nyears, however, are creating both the need and the opportu-\nnity for transformative advances in our ability to use ma-\nchines to help users write software programs.\nOpportunity Humans interact through speech, images,\nand gestures; so-called “natural inputs”. Advances in deeparXiv:1803.07244v2  [cs.AI]  8 May 2018\n\nIntel Labs, MIT, 2018 Gottschlich et al.\nlearning and related machine learning technologies have dra-\nmatically improved a computer’s ability to associate meaning\nwith natural inputs. Deep learning also makes it possible to\nefficiently represent complex distributions over classes of\nstructured objects; a crucial capability if one wants to auto-\nmatically synthesize a program using probabilistic or direct\ntransformation techniques. In parallel to advances in ma-\nchine learning, the programming systems community has\nbeen making notable advances in its ability to reason about\nprograms and manipulate them. Analyzing thousands of\nlines of code to derive inputs that expose a bug has moved\nfrom an intractable problem into one that is routinely solved\ndue to advances in automated reasoning tools such as SAT\nand SMT solvers. Data, a key enabler for learning-based\nstrategies, is also more available now than at any time in\nthe past. This is the byproduct of at least two factors: (i)the\nemergence of code repositories, such as GitHub, and (ii)the\ngrowing magnitude of the web itself, where it is possible\nto observe and analyze the code (e.g., JavaScript) powering\nmany web applications. Finally, the advent of cloud comput-\ning makes it possible to harness large-scale computational\nresources to solve complex analysis and inference problems\nthat were out of reach only a few years ago.\nNeed The end of Dennard scaling means that performance\nimprovements now come through increases in the complex-\nity of the hardware, with resulting increases in the com-\nplexity of compilation targets [ 27]. Traditional compilation\ntechniques rely on an accurate model of relatively simple\nhardware. These techniques are inadequate for exploiting\nthe full potential of heterogeneous hardware platforms. The\ntime is ripe for techniques, based on modern machine learn-\ning, that learn to map computations onto multiple platforms\nfor a single application. Such techniques hold the promise of\neffectively working in the presence of the multiple sources\nof uncertainty that complicate the use of traditional compiler\napproaches. Moreover, there is a growing need for people\nwith core expertise outside of computer science to program,\nwhether for the purpose of data collection and analysis, or\njust to gain some control over the growing set of digital\ndevices permeating daily life.\n1.2 The Three Pillars\nGiven the opportunity and the need, there are already a\nnumber of research efforts in the direction of machine pro-\ngramming in both industry and academia. The general goal\nof machine programming is to remove the burden of writing\ncorrect and efficient code from a human programmer and to\ninstead place it on a machine. The goal of this paper is to pro-\nvide a conceptual framework for us to reason about machine\nprogramming. We describe this framework in the context of\nthree technical pillars: (i)intention, (ii)invention, and (iii)\nadaptation. Each of these pillars corresponds to a class ofcapabilities that we believe are necessary to transform the\nprogramming landscape.\nIntention is the ability of the machine to understand the\nprogrammer’s goals through more natural forms of inter-\naction, which is critical to reducing the complexity of writ-\ning software. Invention is the ability of the machine to dis-\ncover how to accomplish such goals; whether by devising\nnew algorithms, or even by devising new abstractions from\nwhich such algorithms can be built. Adaptation is the ability\nto autonomously evolve software, whether to make it exe-\ncute efficiently on new or existing platforms, or to fix errors\nand address vulnerabilities. As suggested in Figure 1, inten-\ntion, invention, and adaptation intersect in interesting ways.\nWhen advances are made in one pillar, another pillar may\nbe directly or indirectly influenced. In Section 5 we show\nthat sometimes such influence can be negative. This further\nemphasizes the importance for the machine programming\nresearch community to be cognizant of these pillars moving\nforward and to understand how their research interacts with\nthem.\nThe remainder of this report is organized as follows. In\nSections 2, 3, 4, we provide a detailed examination of inten-\ntion, invention, and adaptation, respectively. We also discuss\nthe interactions between them, throughout. In Section 5, we\nprovide a concrete analysis of verified lifting [ 36] and how\nit interacts with each of the three pillars (in some cases, dis-\nruptively). We close with a discussion on the impact of data,\nas it is the cornerstone for many ML-based advances.\n2 Intention\nIntention corresponds to the class of challenges involved in\ncapturing the user’s intent in a way that does not require\nsubstantial programming expertise. One of the major chal-\nlenges in automating programming is capturing the user’s\nintent; describing any complex functionality to the level of\ndetail required by a machine quickly becomes just program-\nming by another name. Table 1 provides a brief overview of\nexisting research in the space of intention. It consists of three\ncolumns: Research Area ,System , and Influence . The Research\nArea column includes subdomains of research for the given\npillar. The System column includes a non-exhaustive list of\nexamples of systems for that subdomain. The Influence col-\numn lists the different pillars, other than intention, that are\ninfluenced by the system listed in the corresponding System\ncolumn.1This table structure is also used for the invention\nand adaptation pillar sections.\nIt is useful to contrast programming with human-human\ninteractions, where we are often able to convey precise in-\ntent by relying on a large body of shared context. If we ask\n1This table is not meant as an exhaustive survey of all the research in the\nintention pillar. Rather, it is meant as an example of work in subdomains of\nintention.\n\nThe Three Pillars of Machine Programming Intel Labs, MIT, 2018\na human to perform a complex, nuanced task such as gener-\nating a list of researchers in the ML domain, we can expect\nthe hidden details not provided in our original description\nto be implicitly understood (e.g., searching the internet for\nindividuals both in academia and industry with projects,\npublications, etc., in the space of ML). By contrast, writing a\ncomputer program to do this would be a more significant un-\ndertaking because we would have to explicitly detail how to\naccomplish each step of the process. Libraries provide some\nassistance, but libraries themselves are difficult to write, can\nbe difficult to use, and, in many cases, even difficult to find.\nTable 1. Examples of Research in the Intention Pillar.\nResearch Area System Influence\nExamples Domain-Specific\nInput-Output ML [18] –\nFlashFill [32] Invention\nGeneralizability Recursion [15] Adaptation\nNatural Language Babble Labble [63] –\nSQLNet [81] –\nNL2P [39] –\nPartial Sketch [72] Adaptation\nImplementations AI Programmer [13] Adaptation\nFor the last decade, the program synthesis community has\nstruggled with this problem, both in the broad context of syn-\nthesizing programs from specifications, as well as in the nar-\nrower context of inductive programming or programming by\nexample [ 20,49]. There are at least two major observations\nthat have emerged from prior work. The first observation is\nthat by tightly controlling the set of primitives from which\nprograms can be built and imposing strong biases on how\nthese primitives should be connected, it is possible to cope\nwith significant ambiguities in the specification. For example,\nthe work on FlashFill [ 32] demonstrated that it is possible\nto synthesize a desired string manipulation from a small\nnumber of examples, often only one, by restricting the space\nof programs to a carefully crafted domain specific language\n(DSL) with carefully tuned biases. Moreover, subsequent\nwork showed that such biases could be learned, rather than\nhaving to be tailored by hand [ 25,69]. Similar observations\nhave been made in other contexts, from the synthesis of SQL\nqueries, to the synthesis of Java APIs [ 50,83]. The Bayou\nproject [ 50], for example, shows that it is possible to use deep\nneural networks to learn complex conditional distributions\nover programs, allowing a user to generate complex Java\ncode from concise traces of evidence. Models not based on\nneural networks can also be used to learn distributions over\nprograms for this purpose [14].The second major observation is that multi-modal specifi-\ncation can help in unambiguously describing complex func-\ntionality. One of the earliest examples of multi-modal syn-\nthesis was the Storyboard Programming Tool (SPT), which\nallowed a user to specify complex data-structure manipula-\ntions through a combination of abstract diagrams, concrete\nexamples, and code skeletons [ 70,71]. The observation was\nthat fully specifying a transformation via any of these modal-\nities on its own, code, examples or abstract diagrams, was\ndifficult, but in combination each of these formalisms could\ncover for the shortcomings of the other. A similar observa-\ntion was made by the Transit project, which showed that\nit was possible to synthesize complex cache coherence pro-\ntocols from a combination of temporal properties, concrete\nand symbolic examples [75].\nAddressing the intention challenges more fully, however,\nwill require additional breakthroughs at the intersection of\nmachine learning and programming systems. One of the ma-\njor opportunities is exploiting the ability modern learning\ntechniques to extract meaning from high-dimensional un-\nstructured inputs, such as images or speech. Recent work, for\nexample on converting natural-language to programs, has\nshown the potential for exploiting this in the context of nar-\nrow domains [ 35,39,83]. Similarly, recent work on extracting\nprogrammatic representations from hand-drawn images has\ndemonstrated the possibilities of using visual data as a basis\nfor conveying intent [ 26]. Many of these systems, however,\nare one-off efforts targeted at narrow domains; one of the\nmajor questions is how to support this kind of functionality\nwhile maintaining the versatility of modern programming\nsystems, and how to scale such high-level interactions to\nricher more complex tasks, including tasks that may require\ninput from more than one person to fully describe.\n3 Invention\nInvention emphasizes the creation or refinement of algo-\nrithms or core hardware and software building blocks. For\nprogram construction, invention usually involves generating\nthe series of steps that a machine would have to execute to\nfulfill a user’s intent; in essence, it is the process of generat-\ning algorithms. This may require discovering new algorithms\nthat are unique and different from prior contributions within\nthe same space. In many instances, however, invention will\nbe accomplished by identifying how to combine and adapt\nknown data structures and algorithmic primitives to solve\na particular problem. Both the program synthesis and the\nmachine learning communities have made notable progress\nin this space in recent years, but there remain many open\nproblems to be solved. See Table 2 for highlights of existing\nresearch in the space of invention.2\n2This table is not meant as an exhaustive survey of all the research in the\ninvention pillar. Rather, it is meant as an example of work in subdomains\nof invention.\n\nIntel Labs, MIT, 2018 Gottschlich et al.\nTable 2. Examples of Research in the Invention Pillar.\nResearch Area System Influence\nExplicit Search λ2[28] Intention\nSynQuid [57] Intention\nConstraint-Based Sketch [72] Intention\nPTS [74] Intention\nSymbolic Version\nSpace FlashFill [32] Intention\nDeductive Paraglide [77] Adaptation\nFiat [22] Adaptation\nSpiral [60] Adaptation\nLearning Directed DeepCoder [9] Intention\nBayou [50] Intention\nLearning to Learn Learning to\nOptimize [40] Adaptation\n3.1 Program Synthesis\nFor program synthesis, the modern approach has been to\nframe invention as a search problem where, given a space\nof candidate programs, the goal is to search for one that\nsatisfies a set of constraints on the desired behavior [ 4]. This\ntype of research has focused on questions of (i)how to rep-\nresent the search space, (ii)how to explore it efficiently by\nexploiting knowledge of the semantics of the underlying\nbuilding blocks, as well as (iii)understanding and advancing\nthe structure of the semantic constraints themselves.\nAt a high-level, researchers have explored at least two\nmajor classes of approaches to this problem. The first class\ninvolves search techniques that explicitly try to build a syn-\ntactic representation of each program in the search space—\nabstract syntax trees (ASTs) are common as a representation.\nThese techniques achieve efficiency by ruling out large sets\nof possible programs without exploring them one-by-one,\nusually by discovering that particular sub-structures can\nnever be part of a correct solution [ 3,28,52,57,75]. The\nsecond class involves symbolic search techniques, where the\nentire program space is represented symbolically, either us-\ning a special purpose representation [ 32,58], or, in the case\nofconstraint-based synthesis , by reducing it to a set of con-\nstraints whose solution can be mapped to a concrete program,\nwhich can be solved using a SAT or SMT solver [ 33,72,74],\nor in some cases a numerical optimization procedure [17].\nMany of these techniques, especially those designed to\nsupport rich specifications instead of simply input-output\nexamples, have been enabled by the ability to automatically\nreason about the correctness of candidate programs, and in\nsome cases, the ability to use static analysis to rule out large\nsets of candidate programs all at once [52, 57].These techniques have made tremendous progress in re-\ncent years; for example, in the domain of bit-level manip-\nulation, the most recent winner of the Syntax Guided Syn-\nthesis competition (SyGuS Comp) was able to automatically\ndiscover complex bit-level manipulation routines that were\nconsidered intractable only a few years earlier [ 5]. In the case\nof string manipulations, program synthesis is now robust\nenough to ship as part of commercial products (e.g. Flashfill\nin Excel [ 32]). In the context of data-structure manipulations,\nroutines such as red-black tree insertion and complex manip-\nulations of linked lists can now be synthesized and verified in\nthe context of both imperative and functional languages [ 57],\nand in the functional programming realm, routines that were\nonce considered functional pearls can now be synthesized\nfrom a few examples [28].\nThat said, there are fundamental limitations to the recent\nprogram synthesis approach to invention. Even with a restric-\ntive set of primitives, the search-space grows exponentially\nwith the size of the code-fragments that one aims to discover,\nmaking it difficult to scale beyond a dozen or so lines of code.\nThere are some instances of systems that have been able to\ndiscover more complex algorithms, either by building them\nincrementally [ 54], or by breaking down the problem into\nsmaller pieces—either by providing the synthesizer with the\ninterfaces of sub-components to use [ 57], or by leveraging\nsome domain-specific structure of the problem to decompose\nit into a large number of independent sub-problems [34].\nDeductive synthesis techniques are another class of ap-\nproaches to the Invention problem, where the idea is to start\nwith a high-level specification and refine it to a low-level\nimplementation by applying deductive rules or semantics-\npreserving transformations. This class of techniques has\nproven to be successful, for example, in automating the de-\nvelopment of concurrent data-structures [ 77] or signal pro-\ncessing pipelines [ 60]. The growing power of interactive\ntheorem provers such as Coq have also made it possible\nto get strong correctness guarantees from code developed\nthrough this approach [ 22]. The main drawback of this class\nof techniques is that while they tend not to suffer from the\nsame scalability problems as the search-based techniques—\nbecause they break the problem into a number of small local\nreasoning steps—they tend to be domain specific, because\nthey rely on carefully engineered deductive rules for the\nparticular problem domain to operate effectively.\n3.2 Machine Learning\nParallel to these efforts, the ML community has been explor-\ning similar ideas in a different context. At one level, machine\nlearning itself can be considered a form of invention. Many\nML algorithms, including support vector machines (SVMs)\nand deep learning, can be seen as a form of constraint-based\nsynthesis, where the space of programs is restricted to the\nset of parameters for a specific class of parameterized func-\ntions, and where numerical optimization is used to solve\n\nThe Three Pillars of Machine Programming Intel Labs, MIT, 2018\nthe constraints, which in this case involve minimizing an\nerror term. By focusing on a narrow class of parameterized\nfunctions, machine-learning techniques are able to support\nsearch spaces that are larger than what the aforementioned\nsynthesis techniques can support. Neural networks with a\nmillion real-valued parameters are becoming standard for\nmany applications, whereas the largest problems solved by\nthe SMT-based techniques have on the order of a thousand\nboolean parameters. This allows neural-networks to capture\nsignificantly more complexity.\nMore recently, there have been significant efforts to cap-\nture more general program structure with neural networks,\neither by encoding differentiable Turing Machines [ 31], or by\nincorporating the notion of recursion directly into the neu-\nral network [ 15]. However, when it comes to synthesizing\nprograms with significant control structure and that require\nmore discrete reasoning, the techniques from the synthesis\ncommunity tend to outperform the neural network based\ntechniques [30].\n3.3 New Directions\nA major opportunity for breakthroughs in the invention\nproblem lies at the intersection of the two lines of research.\nOne important idea that is beginning to emerge is the use\nof learning-based techniques to learn distributions over the\nspace of programs that are conditioned on features from\nthe stated goals of the desired program. These distributions\ncan be used to narrow the space of programs to something\ntractable. For example, DeepCoder [ 9] uses a neural network\nto map from the intention (given as a set of examples) to a\nrestricted set of components that it has learned to recognize\nas useful when satisfying similar intentions. This allows it\nto then use an off-the-shelf synthesizer to solve the synthe-\nsis problem on this restricted program space. The Bayou\nproject uses a more sophisticated network architecture to\nlearn much more complex conditional distributions, allow-\ning it to automatically determine, for example, how to use\ncomplex Java and Android APIs [50].\nOne of the open challenges in this space is to develop sys-\ntems that can solve large-scale invention challenges, moving\nbeyond simple algorithms and data-structure manipulations,\nby solving problems at the scale of an ACM programming\ncompetition or a collegiate programming course. This re-\nquires systems that can better mimic the way programmers\napproach these problems today. That is, using knowledge\naccumulated through practice and directed study to iden-\ntify the core algorithmic building blocks needed to solve\na problem. This also includes reasoning at a high-level of\nabstraction about how those building blocks fit together, and\nonly then reasoning at the code level in a targeted fashion.\nAn important set of challenges in solving this problem is\nthat while there are extensive resources to help humans learn\nto program, from tutorials to textbooks to stackoverflow.com,\nmost of those resources are not suitable for data-hungryML methods, such as deep learning. Applying ML in this\ndomain may require a combination of new ML methods that\ncan learn from data-sources aimed at humans, with novel\nsolutions to exploit large-scale data sources, such as code\nrepositories like GitHub, or synthetic data-sources such as\nrandomly generated programs and datasets.\nAs architectures continue to evolve and become more com-\nplex and reconfigurable, some of the responsibility for coping\nwith this complexity will fall on the invention layer, either\nbecause it will have to discover algorithms that map well to\nthe constraints imposed by the hardware, or in the case of ar-\nchitectures that include FPGAs, the invention layer may need\nto derive the hardware abstractions themselves that align\nfor a given algorithm. There is already some precedent in\nusing constraint-based synthesis to handle non-standard ar-\nchitectures, ranging from exploiting vector instructions [ 11],\nto synthesizing for complex low-power architectures [ 56],\nbut significantly more research is needed for this problem to\nbe fully addressed.\n4 Adaptation\nDetermining the algorithmic steps to solve a problem is\nonly one part of the software development process. The\nresulting algorithms must be made to run efficiently on one\nor more target platforms, and after the code is deployed in the\nfield, the code must be maintained as users expose bugs or\ncorner cases where the system does not behave as expected.\nMoreover, as workloads on the system evolve, it may be\nnecessary to re-evaluate optimization decisions to keep the\nsoftware running at its peak performance. Together, these\ncapabilities make up the Adaptation pillar. See Table 3 for\nhighlights of existing research in the space of adaptation.3\n4.1 Pre-deployment Optimization\nIn recent years, there have been significant efforts in automat-\ning the work to adapt an algorithm to perform optimally on\na particular platform. To some extent, the entire field of com-\npiler optimization is dedicated to this goal, but recently there\nhas been a strong push to move beyond the traditional ap-\nplication of pre-defined optimization steps according to a\ndeterministic schedule and to embrace learning-based tech-\nniques and search in order to explore the space of possible\nimplementations to find a truly optimal one.\nA turning point for the field came with the advent of auto-\ntuning, first popularized by the ATLAS [ 79] and FFTW [ 29]\nprojects in the late 90s. The high-level idea of auto-tuning\nis to explicitly explore a space of possible implementation\nchoices to discover the one that works most efficiently on a\nparticular architecture. The PetaBricks language pushed this\n3This table is not meant as an exhaustive survey of all the research in the\nadaptation pillar. Rather, it is meant as an example of work in subdomains\nof adaptation.\n\nIntel Labs, MIT, 2018 Gottschlich et al.\nTable 3. Examples of Research in the Adaptation Pillar.\nResearch Area System Influence\nAutotuning OpenTuner [7] –\nPetaBricks [6] –\nCode-to-Code Verified Lifting [36] Intention\nTree-to-Tree\nTranslation [19] Intention\nCorrectness ACT [2] Intention\nCodePhage [66] Intention\nData Structures Learned Index\nStructures [37] Invention\nMathematics SPIRAL [60] –\nSelf-Adapting Linear –\nAlgebra Algorithms [23] –\nidea all the way to the language level, allowing the program-\nmer to explicitly provide implementation choices throughout\nthe code, replacing all compiler heuristics with reinforcement\nlearning to discover close to optimal implementations [6].\nStarting in the mid 2000s, there was also a realization that\ndomain specific languages (DSLs) offered an important op-\nportunity for automation. By eliminating a lot of the complex-\nity of full-featured programming languages and exploiting\ndomain specific representations, DSLs enabled aggressive\nsymbolic manipulation of the computation, allowing the\nsystem to explore a much wider range of implementation\nstrategies than what a human could possibly consider. Spi-\nral [60] and the Tensor Contraction Engine (TCE) [ 12] were\nearly examples of this approach. More recently, Halide has\ndemonstrated the potential of this approach to bridge the\n”ninja-gap” by generating code that significantly outperforms\nexpert-tuned codes with only a small amount of high-level\nguidance from the developer [61].\nDespite this successes, there is significant scope for ad-\nvances in this direction. For example, how do we enable\ntransfer learning, so that the N-th program can be optimized\nfaster by leveraging learned optimizations from the previous\n(N−1) programs? Could a system learn new optimization\nstrategies by analyzing a corpus of existing hand-optimized\nprograms? Could learning help reduce the cost of developing\nhigh-performance DSLs, for example by reducing the need\nfor custom heuristics or optimizations?\n4.2 Post-deployment Maintenance\nOne of the most important maintenance tasks today is the\nrepair of bugs and vulnerabilities. Fixing software bugs is\ncurrently an entirely manual process. A programmer must\ndiagnose, isolate, and correct the bug. While the bug remains\nin place, it can impair program behavior or even open up se-\ncurity vulnerabilities. Recent research has demonstrated thefeasibility of automating many aspects of repair. Two early\nsystems include ClearView [ 55] and GenProg [ 78]. ClearView\nuses learned invariants that characterize correct execution\nto generate patches (which can be applied to a running pro-\ngram) that repair a range of execution integrity defects. Gen-\nProg uses genetic programming to search for patches for\ndefects exposed by input/output pairs. More recently, Fan\nLong and Martin Rinard have pioneered machine learning\nmechanisms for automatically generating correct patches\nfor large software systems [41, 66].\nThis recent research has highlighted the importance of\nlearning and statistical techniques for program repair. At a\nfundamental level, program repair is an underdetermined\nproblem, so a repair system must be able to select among all\nthe possible patches that eliminate the symptoms of the bug\nto select the one that actually eliminates the bug without\nintroducing other undesired behaviors. This can be done by\nautomatically learning invariants that characterize correct\nbehavior so that the generated patch can be required to\nmaintain these invariants [ 55], by using machine learning\nover a large corpus of real bug fixes to build a model of\nsuccessful patches [ 43], using change statistics from past\nhuman patches [ 38], or even leveraging a large corpus of\nbug fixes to learn how to generate successful patches [41].\nOther successful program repair techniques focus on spe-\ncific classes of defects such as incorrect conditionals [ 24,80].\nHere constraint solving can play an important role [ 16,21,\n42,45,51]. Automating repetitive source code edits can also\neliminate or correct errors introduced by developer mistakes\nwhen working with similar code patterns [ 46,47,65,73].\nCode transfer, potentially augmented with machine learning\nto find appropriate code to transfer, can automatically work\nwith code across multiple applications to eliminate defects\nand security vulnerabilities [10, 66, 68].\nThese demonstrated techniques lay out the initial case\nfor the feasibility of automated bug detection and correc-\ntion, autonomously without programmer involvement. Many\nexisting successful techniques focus largely on surviving exe-\ncution integrity bugs (bugs that can cause crashes or open up\nsecurity vulnerabilities). Future directions include the addi-\ntional incorporation of machine learning to enhance current\nlatent bug detection techniques and to generate more sophis-\nticated corrections for larger classes of bugs.\nIn addition to bug fixing, there are a number of other\npost-deployment maintenance tasks that could benefit from\nlearning. In general these fall into the category of bit-rot pre-\nvention, and include, for example, upgrading to new versions\nof APIs and web-services, porting to new platforms, such\nas new cloud or mobile environments, or specializing code\nfor particular uses. We envision the eventual development\nof systems that continuously monitor program execution,\nincorporate user feedback, and learn from large code repos-\nitories to deliver a system of autonomous and continuous\nprogram correction and improvement [66, 67].\n\nThe Three Pillars of Machine Programming Intel Labs, MIT, 2018\n5 The Interplay Between Pillars\nSystems for machine programming will most likely be com-\nposed of a set of tools each of which focuses on a particular\npillar. We anticipate that, in most cases, an individual tool\ncannot be fully understood in terms of a single pillar. The\nmachine programming problem is multifaceted and issues\nconcerning one pillar will inevitably impact the other pil-\nlars. Hence, we need to understand machine programming\nsystems in terms of the interplay between the three pillars.\nWe expect this interplay to expose a tension between\nfeatures of a tool that are supportive of the needs of any given\npillar and those that are disruptive to the needs of the other\npillars. The challenge in designing a machine programming\nsystem is to understand this interplay and reach an effective\nresolution of those tensions. As an example to explore this\ninterplay, consider verified lifting [36].\nVerified lifting tools input code written in one language,\ntranslate the code into a new language, and then formally\nverify that the new code produces results that are consis-\ntent with the original code. The prototypical example [ 36]\ntakes stencil codes written in an imperative language, trans-\nlates them into a modern DSL such as Halide, and then uses\ntheorem proving technology to verify that the original and\ngenerated DSL codes are functionally the same. The newer\ncode defines an abstract representation of the problem that\ncan adapt onto a wide range of computer systems. Therefore,\nwe see that the verified lifting problem is primarily used to\nsupport the adaptation pillar.\nVerified lifting, however, goes well beyond the adaptation\npillar. Consider the early steps in the verified lifting problem.\nA verified lifting tool must first understand the problem as\nrepresented in the input code. It discovers the intent of the\nprogram and produces an internal high-level representation\nof the problem often in mathematical or functional terms.\nThis phase of the verified lifting process is firmly grounded\nin the intention pillar. From the high-level representation of\nthe original problem, the verified lifting system can explore\na range of algorithms appropriate to the target language;\ntherefore working within the invention pillar. It then synthe-\nsizes the new code (the adaption pillar) and verifies that it is\nconsistent with the high level representation of the problem.\nHence, a verified lifting tool, while nominally focused on\nadaption, touches, in a supportive way, all three pillars.\nIt is important, however, to consider ways that a tool dis-\nrupts analysis within the different pillars. For example, when\nverified lifting translates low-level code into a compact rep-\nresentation in a DSL it is making the intent behind the code\nmore apparent. Yet, the transformation can also interfere\nwith other tools at the intention layer. For example, if the lift-\ning transformation is not careful to preserve variable names,\nit may hamper the performance of intention layer tools that\nfocus on names in the code to estimate whether a piece\nof code is relevant for a particular task. In general, whenadaptation layer tools modify code, it is important to think\nabout how the change may impact the ability of intention\nand invention layer tools to use that code.\nWhile we have discussed the interplay between pillars in\nterms of just one machine based programming technique\n(verified lifting) we expect this complex interplay to be a\ncommon feature of machine programming systems. As re-\nsearchers in machine program apply the three pillars in their\nown research, it is essential to consider the interplay be-\ntween the three pillars and how this interplay is supportive\nor disruptive to the overall programming process.\n6 Data\nNearly all machine programming systems require data to\ndrive their algorithms. More specifically, every Research Area\nlisted in Tables 1, 2, and 3 requires data (in some form) to\nfunction properly. The data required by these subdomains\ncomes in a variety of forms (e.g., code, input/output examples,\nDSLs, etc.), but is ever-present. This dependency on data\nmakes it essential that we consider the open problems and\nemerging uses around data when reasoning about machine\nprogramming and the systems that implement it.\nThe various approaches to address the three machine pro-\ngramming pillars have different needs in terms of the type\nand size of data they require. Moreover, there is a wide spec-\ntrum in terms of the quality of the data that a project might\nuse. We discuss some of these emerging data uses and issues\nfor the remainder of this section.\nCode Repositories Large version control repositories, such\nas GitHub, offer the promise of access to full revision histo-\nries for all the code necessary to build and run a project, as\nwell as its accompanying documentation. The code available\nin these public repositories has grown exponentially over\nthe last several years and show no indication of stopping.\nMany projects in these repositories have long commit his-\ntories with detailed commit logs which could be of notable\nvalue to machine programming systems [ 76]. However, re-\ncent analysis of public repositories has shown that a large\nfraction of the projects are duplicates, making a significant\nportion of the data less useful [44].\nOne use of code repositories is to use their version control\nhistories to identify code changes that correspond to the\nintroduction of performance or correctness bugs. This type\nof data utilization has been explored to train models for pro-\ngram repair [ 1,43]. Additionally, the presence of complete\ncodebases makes it possible to run whole program analyses\non the code. In some contexts, it has been shown that aug-\nmenting the code with features discovered from program\nanalysis can help train more effective models [ 62]. How-\never, complete codebases may not always be available, and\nrunning whole program analysis may not always be feasible.\n\nIntel Labs, MIT, 2018 Gottschlich et al.\nIncomplete and Synthetic Code + Natural Language\nSources such as stackoverflow provide a wealth of infor-\nmation beyond code, which can be used to correlate code\nand natural language. There has been some work in the com-\nmunity in extracting information from code that comes in\nthe form of code snippets like those usually found in stack-\noverflow. This requires assembling information gathered\nfrom multiple different snippets into a coherent model of\nthe behavior of a code component [ 48,53]. There have even\nbeen some efforts aimed at extracting code from video tu-\ntorials, which offers the possibility of correlating the code\nwith the accompanying narration [ 82]. In some contexts, the\ndata needed to train a model does not even have to come\nfrom real code; synthetic data generated from random com-\nbinations of components can be useful, as demonstrated by\nDeepCoder [9].\nData Privacy One of the open issues of machine program-\nming data is that of privacy. In the context of code, machine\nprogramming systems will eventually have to work with and\nprotect intellectual property as well as software licensing\nagreements. As we move toward a future where data will\nbe more openly shared, used, and traded, new models and\ntools for secure and privacy-preserving exchange will be-\ncome increasingly important [59, 84]. In the case of models\nlearned from code, there are important open cyclic questions\nsurrounding the copyright status of code generated from\nmodels trained from copyrighted code.\nLifecycle Management Machine programming systems\nwill require lifecycle management practices, similar in scope\nto those used in traditional software engineering. Much of\nthis is due to the need to fulfill the goals of the adaptation pil-\nlar. These lifecycle management efforts will be long-lasting\nand will require support for continued monitoring and im-\nprovement around changing software needs and advances\nin an increasingly complex and heterogeneous hardware\necosystem. A significant portion of this lifecycle manage-\nment will be centered on managing the data that are required\nfor such an adaptive machine programming system, as these\ndata will help ensure the stability and maturity of the system.\nAs machine programming systems evolve so will the data\nthey ingest to baseline and advance the system. Because of\nthis, proper data management is likely to be a key enabler\nto calibrate any machine programming system’s lifecycle.\n7 Conclusion\nIn the post Dennard scaling world, where performance comes\nfrom architectural innovation rather than increased transis-\ntor count with constant power density, hardware complexity\nwill only increase. Heterogeneous computing will become\nmore widely used and more diverse than it is today. Over\nthe next several years, specialized accelerators will play an\nincreasingly important role in the hardware platforms wedepend on. At the same time, the nature of programming\nis changing. Instead of computer scientists trained in the\nlow level details of how to map algorithms onto hardware,\nprogrammers are more likely to come from a broad range of\nacademic and business backgrounds. Moreover, rather than\nprogramming in low level languages that interface almost\ndirectly to hardware, programmers are more likely to use\nhigher level abstractions and scripting languages. This will\nfundamentally change how we write software. We believe\nthis change is already well underway.\nWe envision a future where computers will participate\ndirectly in the creation of software, which we call machine\nprogramming . This paper presents a framework to organize\nwork on this problem. We call this framework the three pillars\nof machine programming . The three pillars are intention,\ninvention, and adaptation.\nIntention focuses on the interface between the human and\nthe machine programming system; i.e., techniques to dis-\ncover what a program needs to do from input that is natural\nto express by the human. A system grounded in the intention\npillar meets human programmers on their terms rather than\nforcing them to express code in computer/hardware nota-\ntions. Invention emphasizes machine systems that create and\nrefine algorithms or the core hardware and software building\nblocks from which systems are built. Adaptation focuses on\nML-based tools that help software adapt to changing condi-\ntions; whether they are bugs or vulnerabilities found in an\napplication or new hardware systems.\nData is at the foundation of the modern renaissance in\nartificial intelligence (AI). Without vast amounts of data, it is\nunlikely that AI would have had significant impact outside\nspecialized academic circles. In this paper, we explored the\nimpact of data as it pertains to machine programming. Soft-\nware repositories in systems such as GitHub and the vast\namount of software embedded in countless webpages is the\nraw material that will likely support a large majority of the\nemergence of machine programming.\nFinally, there are numerous open problems that must be\nsolved to make machine programming a practical reality.\nWe outlined some of these open problems in this paper. It\nwill take a large community of researchers years of hard\nwork to solve this problem. If we can agree on a conceptual\nframework to organize this research, it will help us advance\nthe field and more quickly bring us to a world where every-\none programs computers; on human-terms with machine\nsystems handling the low level details of finding the right\nalgorithm for the right hardware to solve the right problem.\nReferences\n[1]Mohammad Mejbah Ul Alam, Justin Gottschlich, and Abdullah Muza-\nhid. 2017. AutoPerf: A Generalized Zero-Positive Learning System to\nDetect Software Performance Anomalies . Technical Report. http:\n//arxiv.org/abs/1709.07536/\n\nThe Three Pillars of Machine Programming Intel Labs, MIT, 2018\n[2]Mohammad Mejbah ul Alam and Abdullah Muzahid. 2016. Production-\nrun Software Failure Diagnosis via Adaptive Communication Track-\ning. In Proceedings of the 43rd International Symposium on Computer\nArchitecture (ISCA ’16) . IEEE Press, Piscataway, NJ, USA, 354–366.\nhttps://doi.org/10.1109/ISCA.2016.39\n[3]Aws Albarghouthi, Sumit Gulwani, and Zachary Kincaid. 2013. Recur-\nsive Program Synthesis. In Computer Aided Verification - 25th Interna-\ntional Conference, CAV 2013, Saint Petersburg, Russia, July 13-19, 2013.\nProceedings . 934–950. https://doi.org/10.1007/978-3-642-39799-8_67\n[4]Rajeev Alur, Rastislav Bodík, Eric Dallal, Dana Fisman, Pranav Garg,\nGarvit Juniwal, Hadas Kress-Gazit, P. Madhusudan, Milo M. K. Martin,\nMukund Raghothaman, Shambwaditya Saha, Sanjit A. Seshia, Rishabh\nSingh, Armando Solar-Lezama, Emina Torlak, and Abhishek Udupa.\n2015. Syntax-Guided Synthesis. In Dependable Software Systems Engi-\nneering . 1–25. https://doi.org/10.3233/978-1-61499-495-4-1\n[5]Rajeev Alur, Pavol Cerný, and Arjun Radhakrishna. 2015. Synthesis\nThrough Unification. In Computer Aided Verification - 27th International\nConference, CAV 2015, San Francisco, CA, USA, July 18-24, 2015, Proceed-\nings, Part II . 163–179. https://doi.org/10.1007/978-3-319-21668-3_10\n[6]Jason Ansel, Cy Chan, Yee Lok Wong, Marek Olszewski, Qin Zhao,\nAlan Edelman, and Saman Amarasinghe. 2009. PetaBricks: A Lan-\nguage and Compiler for Algorithmic Choice. In Proceedings of the\n30th ACM SIGPLAN Conference on Programming Language Design\nand Implementation (PLDI ’09) . ACM, New York, NY, USA, 38–49.\nhttps://doi.org/10.1145/1542476.1542481\n[7]Jason Ansel, Shoaib Kamil, Kalyan Veeramachaneni, Jonathan Ragan-\nKelley, Jeffrey Bosboom, Una-May O’Reilly, and Saman Amarasinghe.\n2014. OpenTuner: An Extensible Framework for Program Autotuning.\nInProceedings of the 23rd International Conference on Parallel Architec-\ntures and Compilation (PACT ’14) . ACM, New York, NY, USA, 303–316.\nhttps://doi.org/10.1145/2628071.2628092\n[8]J. W. Backus, R. J. Beeber, S. Best, R. Goldberg, L. M. Haibt, H. L.\nHerrick, R. A. Nelson, D. Sayre, P. B. Sheridan, H. Stern, I. Ziller, R. A.\nHughes, and R. Nutt. 1957. The FORTRAN Automatic Coding System.\nInPapers Presented at the February 26-28, 1957, Western Joint Computer\nConference: Techniques for Reliability (IRE-AIEE-ACM ’57 (Western)) .\nACM, New York, NY, USA, 188–198. https://doi.org/10.1145/1455567.\n1455599\n[9]Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian\nNowozin, and Daniel Tarlow. 2017. DeepCoder: Learning to Write\nPrograms. ICLR (2017). https://arxiv.org/abs/1611.01989\n[10] Earl T Barr, Mark Harman, Yue Jia, Alexandru Marginean, and Justyna\nPetke. 2015. Automated software transplantation. In Proceedings of the\n2015 International Symposium on Software Testing and Analysis . ACM,\n257–269.\n[11] Gilles Barthe, Juan Manuel Crespo, Sumit Gulwani, César Kunz, and\nMark Marron. 2013. From relational verification to SIMD loop synthe-\nsis. In ACM SIGPLAN Symposium on Principles and Practice of Parallel\nProgramming, PPoPP ’13, Shenzhen, China, February 23-27, 2013 . 123–\n134. https://doi.org/10.1145/2442516.2442529\n[12] Gerald Baumgartner, David E. Bernholdt, Daniel Cociorva, Robert J.\nHarrison, So Hirata, Chi-Chung Lam, Marcel Nooijen, Russell M. Pitzer,\nJ. Ramanujam, and P. Sadayappan. 2002. A high-level approach to\nsynthesis of high-performance codes for quantum chemistry. In Pro-\nceedings of the 2002 ACM/IEEE conference on Supercomputing, Bal-\ntimore, Maryland, USA, November 16-22, 2002, CD-ROM . 33:1–33:10.\nhttps://doi.org/10.1109/SC.2002.10056\n[13] Kory Becker and Justin Gottschlich. 2017. AI Programmer: Au-\ntonomously Creating Software Programs Using Genetic Algorithms.\nCoRR abs/1709.05703 (2017). arXiv:1709.05703 http://arxiv.org/abs/\n1709.05703\n[14] Pavol Bielik, Veselin Raychev, and Martin T. Vechev. 2016. PHOG:\nProbabilistic Model for Code. In Proceedings of the 33nd International\nConference on Machine Learning, ICML 2016, New York City, NY, USA,June 19-24, 2016 . 2933–2942. http://jmlr.org/proceedings/papers/v48/\nbielik16.html\n[15] Jonathon Cai, Richard Shin, and Dawn Song. 2017. Making Neu-\nral Programming Architectures Generalize via Recursion. CoRR\nabs/1704.06611 (2017). arXiv:1704.06611 http://arxiv.org/abs/1704.\n06611\n[16] Satish Chandra, Emina Torlak, Shaon Barman, and Rastislav Bodik.\n2011. Angelic Debugging. In Proceedings of the 33rd International\nConference on Software Engineering (ICSE ’11’) .\n[17] Swarat Chaudhuri and Armando Solar-Lezama. 2010. Smooth inter-\npretation. In Proceedings of the 2010 ACM SIGPLAN Conference on\nProgramming Language Design and Implementation, PLDI 2010, Toronto,\nOntario, Canada, June 5-10, 2010 . 279–291. https://doi.org/10.1145/\n1806596.1806629\n[18] Xinyun Chen, Chang Liu, and Dawn Song. 2017. Towards Synthesizing\nComplex Programs from Input-Output Examples. CoRR abs/1706.01284\n(2017). arXiv:1706.01284 http://arxiv.org/abs/1706.01284\n[19] Xinyun Chen, Chang Liu, and Dawn Song. 2017. Tree-to-tree Neural\nNetworks for Program Translation. CoRR abs/1712.01208 (2017). https:\n//arxiv.org/abs/1712.01208\n[20] Allen Cypher, Daniel C. Halbert, David Kurlander, Henry Lieberman,\nDavid Maulsby, Brad A. Myers, and Alan Turransky (Eds.). 1993. Watch\nWhat I Do: Programming by Demonstration . MIT Press, Cambridge,\nMA, USA.\n[21] Loris D’Antoni, Roopsha Samanta, and Rishabh Singh. 2016. Qlose:\nProgram Repair with Quantitative Objectives. In Computer-Aided Veri-\nfication (CAV) .\n[22] Benjamin Delaware, Clément Pit-Claudel, Jason Gross, and Adam\nChlipala. 2015. Fiat: Deductive Synthesis of Abstract Data Types in\na Proof Assistant. In Proceedings of the 42nd Annual ACM SIGPLAN-\nSIGACT Symposium on Principles of Programming Languages, POPL\n2015, Mumbai, India, January 15-17, 2015 . 689–700. https://doi.org/10.\n1145/2676726.2677006\n[23] J. Demmel, J. Dongarra, V. Eijkhout, E. Fuentes, A. Petitet, R. Vuduc,\nR. C. Whaley, and K. Yelick. 2005. Self-Adapting Linear Algebra\nAlgorithms and Software. Proc. IEEE 93, 2 (Feb 2005), 293–312.\nhttps://doi.org/10.1109/JPROC.2004.840848\n[24] Thomas Durieux, Matias Martinez, Martin Monperrus, Romain Som-\nmerard, and Jifeng Xuan. 2015. Automatic Repair of Real Bugs: An\nExperience Report on the Defects4J Dataset. CoRR abs/1505.07002\n(2015). http://arxiv.org/abs/1505.07002\n[25] Kevin Ellis and Sumit Gulwani. 2017. Learning to Learn Programs\nfrom Examples: Going Beyond Program Structure. In Proceedings of\nthe Twenty-Sixth International Joint Conference on Artificial Intelligence,\nIJCAI 2017, Melbourne, Australia, August 19-25, 2017 . 1638–1645. https:\n//doi.org/10.24963/ijcai.2017/227\n[26] Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, and Joshua B.\nTenenbaum. 2017. Learning to Infer Graphics Programs from Hand-\nDrawn Images. CoRR abs/1707.09627 (2017). arXiv:1707.09627 http:\n//arxiv.org/abs/1707.09627\n[27] H. Esmaeilzadeh, E. Blem, R. S. Amant, K. Sankaralingam, and D.\nBurger. 2011. Dark silicon and the end of multicore scaling. In 2011\n38th Annual International Symposium on Computer Architecture (ISCA) .\n365–376.\n[28] John K. Feser, Swarat Chaudhuri, and Isil Dillig. 2015. Synthesizing data\nstructure transformations from input-output examples. In Proceedings\nof the 36th ACM SIGPLAN Conference on Programming Language Design\nand Implementation, Portland, OR, USA, June 15-17, 2015 . 229–239.\nhttps://doi.org/10.1145/2737924.2737977\n[29] Matteo Frigo and Steven G. Johnson. 1998. FFTW: an adaptive\nsoftware architecture for the FFT. In Proceedings of the 1998 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing,\nICASSP ’98, Seattle, Washington, USA, May 12-15, 1998 . 1381–1384.\nhttps://doi.org/10.1109/ICASSP.1998.681704\n\nIntel Labs, MIT, 2018 Gottschlich et al.\n[30] Alexander L. Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kush-\nman, Pushmeet Kohli, Jonathan Taylor, and Daniel Tarlow. 2016. Ter-\npreT: A Probabilistic Programming Language for Program Induction.\nCoRR abs/1608.04428 (2016). arXiv:1608.04428 http://arxiv.org/abs/\n1608.04428\n[31] Alex Graves, Greg Wayne, and Ivo Danihelka. 2014. Neural Turing\nMachines. CoRR abs/1410.5401 (2014). arXiv:1410.5401 http://arxiv.\norg/abs/1410.5401\n[32] Sumit Gulwani. 2011. Automating String Processing in Spreadsheets\nUsing Input-output Examples. In Proceedings of the 38th Annual ACM\nSIGPLAN-SIGACT Symposium on Principles of Programming Languages\n(POPL ’11) . ACM, New York, NY, USA, 317–330. https://doi.org/10.\n1145/1926385.1926423\n[33] Sumit Gulwani, Susmit Jha, Ashish Tiwari, and Ramarathnam Venkate-\nsan. 2011. Synthesis of loop-free programs. In Proceedings of the 32nd\nACM SIGPLAN Conference on Programming Language Design and Im-\nplementation, PLDI 2011, San Jose, CA, USA, June 4-8, 2011 . 62–73.\nhttps://doi.org/10.1145/1993498.1993506\n[34] Jeevana Priya Inala, Rohit Singh, and Armando Solar-Lezama. 2016.\nSynthesis of Domain Specific CNF Encoders for Bit-Vector Solvers.\nInTheory and Applications of Satisfiability Testing - SAT 2016 - 19th\nInternational Conference, Bordeaux, France, July 5-8, 2016, Proceedings .\n302–320. https://doi.org/10.1007/978-3-319-40970-2_19\n[35] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy,\nand Luke Zettlemoyer. 2017. Learning a Neural Semantic Parser\nfrom User Feedback. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics, ACL 2017, Vancouver,\nCanada, July 30 - August 4, Volume 1: Long Papers . 963–973. https:\n//doi.org/10.18653/v1/P17-1089\n[36] Shoaib Kamil, Alvin Cheung, Shachar Itzhaky, and Armando Solar-\nLezama. 2016. Verified Lifting of Stencil Computations. In Proceedings\nof the 37th ACM SIGPLAN Conference on Programming Language Design\nand Implementation (PLDI ’16) . ACM, New York, NY, USA, 711–726.\nhttps://doi.org/10.1145/2908080.2908117\n[37] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis.\n2017. The Case for Learned Index Structures. CoRR abs/1712.01208\n(2017). https://arxiv.org/abs/1712.01208\n[38] Xuan-Bach D. Le, David Lo, and Claire Le Goues. 2016. History Driven\nProgram Repair. In IEEE 23rd International Conference on Software Anal-\nysis, Evolution, and Reengineering, SANER 2016, Suita, Osaka, Japan,\nMarch 14-18, 2016 . 213–224.\n[39] Tao Lei, Fan Long, Regina Barzilay, and Martin C. Rinard. 2013. From\nNatural Language Specifications to Program Input Parsers. In Proceed-\nings of the 51st Annual Meeting of the Association for Computational\nLinguistics, ACL 2013, 4-9 August 2013, Sofia, Bulgaria, Volume 1: Long\nPapers . 1294–1303.\n[40] Ke Li Li and Jitendra Malik. 2017. Learning to Optimize. ICLR (2017).\nhttps://arxiv.org/abs/1606.01885\n[41] Fan Long, Peter Amidon, and Martin Rinard. 2017. Automatic inference\nof code transforms for patch generation. In Proceedings of the 2017 11th\nJoint Meeting on Foundations of Software Engineering, ESEC/FSE 2017,\nPaderborn, Germany, September 4-8, 2017 . 727–739.\n[42] Fan Long and Martin Rinard. 2015. Staged Program Repair with Con-\ndition Synthesis. In Proceedings of the 2015 10th Joint Meeting on Foun-\ndations of Software Engineering (ESEC/FSE 2015) .\n[43] Fan Long and Martin Rinard. 2016. Automatic patch generation by\nlearning correct code. In Proceedings of the 43rd Annual ACM SIGPLAN-\nSIGACT Symposium on Principles of Programming Languages, POPL\n2016, St. Petersburg, FL, USA, January 20 - 22, 2016 . 298–312.\n[44] Cristina V. Lopes, Petr Maj, Pedro Martins, Vaibhav Saini, Di Yang,\nJakub Zitny, Hitesh Sajnani, and Jan Vitek. 2017. DéjàVu: a map of\ncode duplicates on GitHub. PACMPL 1, OOPSLA (2017), 84:1–84:28.\nhttps://doi.org/10.1145/3133908\n[45] Sergey Mechtaev, Jooyong Yi, and Abhik Roychoudhury. 2016. Angelix:\nscalable multiline program patch synthesis via symbolic analysis. InProceedings of the 38th International Conference on Software Engineering,\nICSE 2016, Austin, TX, USA, May 14-22, 2016 . 691–701.\n[46] Na Meng, Miryung Kim, and Kathryn S. McKinley. 2011. Systematic\nEditing: Generating Program Transformations from an Example. In\nProceedings of the 32Nd ACM SIGPLAN Conference on Programming\nLanguage Design and Implementation (PLDI ’11’) . 329–342.\n[47] Na Meng, Miryung Kim, and Kathryn S. McKinley. 2013. LASE: lo-\ncating and applying systematic edits by learning from examples. In\n35th International Conference on Software Engineering, ICSE ’13, San\nFrancisco, CA, USA, May 18-26, 2013 . 502–511.\n[48] Alon Mishne, Sharon Shoham, and Eran Yahav. 2012. Typestate-based\nSemantic Code Search over Partial Programs. In Proceedings of the\nACM International Conference on Object Oriented Programming Systems\nLanguages and Applications (OOPSLA ’12) . ACM, New York, NY, USA,\n997–1016. https://doi.org/10.1145/2384616.2384689\n[49] Stephen Muggleton. 1991. Inductive logic programming . Vol. 8. 295–318\npages. https://doi.org/10.1007/BF03037089\n[50] Vijayaraghavan Murali, Swarat Chaudhuri, and Chris Jermaine. 2017.\nBayesian Sketch Learning for Program Synthesis. CoRR abs/1703.05698\n(2017). arXiv:1703.05698 http://arxiv.org/abs/1703.05698\n[51] Hoang Duong Thien Nguyen, Dawei Qi, Abhik Roychoudhury, and\nSatish Chandra. 2013. SemFix: Program Repair via Semantic Anal-\nysis. In Proceedings of the 2013 International Conference on Software\nEngineering (ICSE ’13’) . IEEE Press, Piscataway, NJ, USA, 772–781.\nhttp://dl.acm.org/citation.cfm?id=2486788.2486890\n[52] Peter-Michael Osera and Steve Zdancewic. 2015. Type-and-example-\ndirected program synthesis. In Proceedings of the 36th ACM SIGPLAN\nConference on Programming Language Design and Implementation, Port-\nland, OR, USA, June 15-17, 2015 . 619–630. https://doi.org/10.1145/\n2737924.2738007\n[53] Hila Peleg, Sharon Shoham, Eran Yahav, and Hongseok Yang. 2013.\nSymbolic Automata for Static Specification Mining. In Static Analysis -\n20th International Symposium, SAS 2013, Seattle, WA, USA, June 20-22,\n2013. Proceedings . 63–83. https://doi.org/10.1007/978-3-642-38856-9_6\n[54] Daniel Perelman, Sumit Gulwani, Dan Grossman, and Peter Provost.\n2014. Test-driven synthesis. In ACM SIGPLAN Conference on Program-\nming Language Design and Implementation, PLDI ’14, Edinburgh, United\nKingdom - June 09 - 11, 2014 . 408–418. https://doi.org/10.1145/2594291.\n2594297\n[55] Jeff H. Perkins, Sunghun Kim, Samuel Larsen, Saman P. Amaras-\ninghe, Jonathan Bachrach, Michael Carbin, Carlos Pacheco, Frank\nSherwood, Stelios Sidiroglou, Greg Sullivan, Weng-Fai Wong, Yoav\nZibin, Michael D. Ernst, and Martin C. Rinard. 2009. Automatically\npatching errors in deployed software. In Proceedings of the 22nd ACM\nSymposium on Operating Systems Principles 2009, SOSP 2009, Big Sky,\nMontana, USA, October 11-14, 2009 . 87–102.\n[56] Phitchaya Mangpo Phothilimthana, Tikhon Jelvis, Rohin Shah, Nishant\nTotla, Sarah Chasins, and Rastislav Bodík. 2014. Chlorophyll: synthesis-\naided compiler for low-power spatial architectures. In ACM SIGPLAN\nConference on Programming Language Design and Implementation, PLDI\n’14, Edinburgh, United Kingdom - June 09 - 11, 2014 . 396–407. https:\n//doi.org/10.1145/2594291.2594339\n[57] Nadia Polikarpova, Ivan Kuraj, and Armando Solar-Lezama. 2016. Pro-\ngram synthesis from polymorphic refinement types. In Proceedings of\nthe 37th ACM SIGPLAN Conference on Programming Language Design\nand Implementation, PLDI 2016, Santa Barbara, CA, USA, June 13-17,\n2016. 522–538. https://doi.org/10.1145/2908080.2908093\n[58] Oleksandr Polozov and Sumit Gulwani. 2015. FlashMeta: a frame-\nwork for inductive program synthesis. In Proceedings of the 2015\nACM SIGPLAN International Conference on Object-Oriented Program-\nming, Systems, Languages, and Applications, OOPSLA 2015, part of\nSPLASH 2015, Pittsburgh, PA, USA, October 25-30, 2015 . 107–126. https:\n//doi.org/10.1145/2814270.2814310\n\nThe Three Pillars of Machine Programming Intel Labs, MIT, 2018\n[59] Raluca Ada Popa, Catherine M. S. Redfield, Nickolai Zeldovich, and\nHari Balakrishnan. 2011. CryptDB: Protecting Confidentiality with\nEncrypted Query Processing. In Proceedings of the Twenty-Third ACM\nSymposium on Operating Systems Principles (SOSP ’11) . ACM, New\nYork, NY, USA, 85–100. https://doi.org/10.1145/2043556.2043566\n[60] Markus Püschel, José M. F. Moura, Bryan Singer, Jianxin Xiong,\nJeremy R. Johnson, David A. Padua, Manuela M. Veloso, and Robert W.\nJohnson. 2004. Spiral: A Generator for Platform-Adapted Libraries of\nSignal Processing Alogorithms. IJHPCA 18, 1 (2004), 21–45. https:\n//doi.org/10.1177/1094342004041291\n[61] Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain\nParis, Frédo Durand, and Saman P. Amarasinghe. 2013. Halide: a\nlanguage and compiler for optimizing parallelism, locality, and recom-\nputation in image processing pipelines. In ACM SIGPLAN Conference\non Programming Language Design and Implementation, PLDI ’13, Seattle,\nWA, USA, June 16-19, 2013 . 519–530. https://doi.org/10.1145/2462156.\n2462176\n[62] Veselin Raychev, Martin Vechev, and Andreas Krause. 2015. Pre-\ndicting Program Properties from \"Big Code\". In Proceedings of the\n42Nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Pro-\ngramming Languages (POPL ’15) . ACM, New York, NY, USA, 111–124.\nhttps://doi.org/10.1145/2676726.2677009\n[63] Chris Re. 2017. Babble Labble . Technical Report. Department of Com-\nputer Science, Stanford, Stanford, CA. https://hazyresearch.github.io/\nsnorkel/blog/babble_labble.html\n[64] Charles Rich and Richard C. Waters. 1988. The Programmer’s Ap-\nprentice: A Research Overview. Computer 21, 11 (Nov. 1988), 10–25.\nhttps://doi.org/10.1109/2.86782\n[65] Reudismam Rolim, Gustavo Soares, Loris D’Antoni, Oleksandr Polozov,\nSumit Gulwani, Rohit Gheyi, Ryo Suzuki, and Björn Hartmann. 2017.\nLearning syntactic program transformations from examples. In Pro-\nceedings of the 39th International Conference on Software Engineering,\nICSE 2017, Buenos Aires, Argentina, May 20-28, 2017 .\n[66] Stelios Sidiroglou-Douskos, Eric Lahtinen, Fan Long, and Martin Ri-\nnard. 2015. Automatic error elimination by horizontal code transfer\nacross multiple applications. In Proceedings of the 36th ACM SIGPLAN\nConference on Programming Language Design and Implementation, Port-\nland, OR, USA, June 15-17, 2015 . 43–54.\n[67] Stelios Sidiroglou-Douskos, Eric Lahtinen, Nathan Rittenhouse, Paolo\nPiselli, Fan Long, Deokhwan Kim, and Martin C. Rinard. 2015. Targeted\nAutomatic Integer Overflow Discovery Using Goal-Directed Condi-\ntional Branch Enforcement. In Proceedings of the Twentieth Interna-\ntional Conference on Architectural Support for Programming Languages\nand Operating Systems, ASPLOS ’15, Istanbul, Turkey, March 14-18, 2015 .\n473–486.\n[68] Stelios Sidiroglou-Douskos, Eric Lantinen, Anthony Eden, Fan Long,\nand Martin Rinard. 2017. CodeCarbonCopy. In Proceedings of the 2017\n11th Joint Meeting on Foundations of Software Engineering (ESEC/FSE\n2017) .\n[69] Rishabh Singh and Sumit Gulwani. 2015. Predicting a Correct Pro-\ngram in Programming by Example. In Computer Aided Verification\n- 27th International Conference, CAV 2015, San Francisco, CA, USA,\nJuly 18-24, 2015, Proceedings, Part I . 398–414. https://doi.org/10.1007/\n978-3-319-21690-4_23\n[70] Rishabh Singh and Armando Solar-Lezama. 2011. Synthesizing data\nstructure manipulations from storyboards. In SIGSOFT/FSE’11 19th\nACM SIGSOFT Symposium on the Foundations of Software Engineering\n(FSE-19) and ESEC’11: 13th European Software Engineering Conference\n(ESEC-13), Szeged, Hungary, September 5-9, 2011 . 289–299. https://doi.\norg/10.1145/2025113.2025153\n[71] Rishabh Singh and Armando Solar-Lezama. 2012. SPT: Storyboard\nProgramming Tool. In Computer Aided Verification - 24th InternationalConference, CAV 2012, Berkeley, CA, USA, July 7-13, 2012 Proceedings .\n738–743. https://doi.org/10.1007/978-3-642-31424-7_58\n[72] Armando Solar-Lezama, Liviu Tancau, Rastislav Bodík, Sanjit A. Se-\nshia, and Vijay A. Saraswat. 2006. Combinatorial sketching for fi-\nnite programs. In Proceedings of the 12th International Conference on\nArchitectural Support for Programming Languages and Operating Sys-\ntems, ASPLOS 2006, San Jose, CA, USA, October 21-25, 2006 . 404–415.\nhttps://doi.org/10.1145/1168857.1168907\n[73] Sooel Son, Kathryn S McKinley, and Vitaly Shmatikov. 2013. Fix Me\nUp: Repairing Access-Control Bugs in Web Applications.. In NDSS .\n[74] Saurabh Srivastava, Sumit Gulwani, and Jeffrey S. Foster. 2010. From\nprogram verification to program synthesis. In Proceedings of the 37th\nACM SIGPLAN-SIGACT Symposium on Principles of Programming\nLanguages, POPL 2010, Madrid, Spain, January 17-23, 2010 . 313–326.\nhttps://doi.org/10.1145/1706299.1706337\n[75] Abhishek Udupa, Arun Raghavan, Jyotirmoy V. Deshmukh, Sela\nMador-Haim, Milo M. K. Martin, and Rajeev Alur. 2013. TRANSIT:\nspecifying protocols with concolic snippets. In ACM SIGPLAN Confer-\nence on Programming Language Design and Implementation, PLDI ’13,\nSeattle, WA, USA, June 16-19, 2013 . 287–296. https://doi.org/10.1145/\n2462156.2462174\n[76] Martin Vechev and Eran Yahav. 2016. Programming with \"Big Code\".\nFound. Trends Program. Lang. 3, 4 (Dec. 2016), 231–284. https://doi.\norg/10.1561/2500000028\n[77] Martin T. Vechev and Eran Yahav. 2008. Deriving linearizable fine-\ngrained concurrent objects. In Proceedings of the ACM SIGPLAN 2008\nConference on Programming Language Design and Implementation, Tuc-\nson, AZ, USA, June 7-13, 2008 . 125–135. https://doi.org/10.1145/1375581.\n1375598\n[78] Westley Weimer, ThanhVu Nguyen, Claire Le Goues, and Stephanie\nForrest. 2009. Automatically finding patches using genetic pro-\ngramming. In 31st International Conference on Software Engineering,\nICSE 2009, May 16-24, 2009, Vancouver, Canada, Proceedings . 364–374.\nhttps://doi.org/10.1109/ICSE.2009.5070536\n[79] R. Clinton Whaley and Jack J. Dongarra. 1998. Automatically Tuned\nLinear Algebra Software. In Proceedings of the ACM/IEEE Conference\non Supercomputing, SC 1998, November 7-13, 1998, Orlando, FL, USA . 38.\nhttps://doi.org/10.1109/SC.1998.10004\n[80] Yingfei Xiong, Jie Wang, Runfa Yan, Jiachen Zhang, Shi Han, Gang\nHuang, and Lu Zhang. 2017. Precise Condition Synthesis for Program\nRepair. In Proceedings of the 39th International Conference on Software\nEngineering (ICSE ’17) .\n[81] Xiaojun Xu, Chang Liu, and Dawn Song. 2017. SQLNet: Generating\nStructured Queries From Natural Language Without Reinforcement\nLearning. CoRR abs/1711.04436 (2017). arXiv:1711.04436 http://arxiv.\norg/abs/1711.04436\n[82] Shir Yadid and Eran Yahav. 2016. Extracting Code from Program-\nming Tutorial Videos. In Proceedings of the 2016 ACM International\nSymposium on New Ideas, New Paradigms, and Reflections on Program-\nming and Software (Onward! 2016) . ACM, New York, NY, USA, 98–111.\nhttps://doi.org/10.1145/2986012.2986021\n[83] Navid Yaghmazadeh, Yuepeng Wang, Isil Dillig, and Thomas Dillig.\n2017. SQLizer: query synthesis from natural language. PACMPL 1,\nOOPSLA (2017), 63:1–63:26. https://doi.org/10.1145/3133887\n[84] Wenting Zheng, Ankur Dave, Jethro G. Beekman, Raluca Ada Popa,\nJoseph E. Gonzalez, and Ion Stoica. 2017. Opaque: An Oblivious and\nEncrypted Distributed Analytics Platform. In 14th USENIX Sympo-\nsium on Networked Systems Design and Implementation, NSDI 2017,\nBoston, MA, USA, March 27-29, 2017 . 283–298. https://www.usenix.\norg/conference/nsdi17/technical-sessions/presentation/zheng",
  "textLength": 67824
}