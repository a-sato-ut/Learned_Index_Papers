{
  "paperId": "90e9ed8c8fb99ab6122edad9e97169ab9ea2fc6e",
  "title": "SPIDER: Improved Succinct Rank and Select Performance",
  "pdfPath": "90e9ed8c8fb99ab6122edad9e97169ab9ea2fc6e.pdf",
  "text": "SPIDER: Improved Succinct Rank and Select\nPerformance\nMatthew D. Laws /envel⌢pe/orcid\nWilliams College Computer Science, Williamstown, MA USA\nJocelyn Bliven /envel⌢pe/orcid\nWilliams College Computer Science, Williamstown, MA USA\nKit Conklin /envel⌢pe/orcid\nWilliams College Computer Science, Williamstown, MA USA\nElyes Laalai /envel⌢pe/orcid\nWilliams College Computer Science, Williamstown, MA USA\nSamuel McCauley /envel⌢pe/orcid\nWilliams College Computer Science, Williamstown, MA USA\nZach S. Sturdevant /envel⌢pe/orcid\nWilliams College Computer Science, Williamstown, MA USA\nAbstract\nRank and select data structures seek to preprocess a bit vector to quickly answer two kinds of\nqueries: Rank (i)gives the number of 1bits in slots 0throughi, and Select (j)gives the first slot\nswith Rank (s)=j. A succinct data structure can answer these queries while using space much\nsmaller than the size of the original bit vector.\nState of the art succinct rank and select data structures use as little as 4% extra space (over the\nunderlying bit vector) while answering rank and select queries very quickly. Rank queries can be\nanswered using only a handful of array accesses. Select queries can be answered by starting with\nsimilar array accesses, followed by a linear scan through the bit vector.\nNonetheless, a tradeoff remains: data structures that use under 4% space are significantly slower\nat answering rank and select queries than less-space-efficient data structures (using, say, over 20%\nextra space).\nIn this paper we make significantly progress towards closing this gap. We give a new data\nstructure, SPIDER, which uses 3.82% extra space. SPIDER gives the best known rank query time\nfor data sets of 8 billion or more bits, even compared to much less space-efficient data structures. For\nselect queries, SPIDER outperforms all data structures that use less than 4% space, and significantly\ncloses the gap in select performance between data structures with less than 4% space, and those\nthat use more (over 20% for both rank and select) space.\nSPIDER makes two main technical contributions. For rank queries, it improves performance\nby interleaving the metadata with the bit vector to improve cache efficiency. For select queries, it\nuses predictions to almost eliminate the cost of the linear scan. These predictions are inspired by\nrecent results on data structures with machine-learned predictions, adapted to the succinct data\nstructure setting. Our results hold on both real and synthetic data, showing that these predictions\nare effective in practice.\n2012 ACM Subject Classification Theory of computation →Sorting and searching\nKeywords and phrases Rank and Select, Succinct Data Structures, Data Structres, Cache Perform-\nance, Predictions\nSupplementary Material Software :https://github.com/williams-cs/spider\nFunding This research was supported in part by NSF grant CCF 2103813.arXiv:2405.05214v1  [cs.DS]  8 May 2024\n\nM. D. Laws, J. Bliven, K. Conklin, E. Laalai, S. McCauley, Z. S. Sturdevant 1\n1 Introduction\nRank and select are classic bit vector queries in computer science. Given a bit vector V,\nRank (i)gives the number of 1bits among V[0],...,V [i].Select (j)gives the location of\nthejth1bit; in other words, Select (j)gives the smallest isuch that Rank (i)=j. Thus, if\nnis the length of V, andn1is the number of 1bits it contains, rank queries are well-defined\nfori= 0,...,n−1and select queries are well-defined for i= 1,...,n 1.\nRank and select data structures are most interesting when they are restricted to small\nspace. After all, with unlimited space one could just store the answer to all n+n1rank and\nselect queries.\nThus, we focus on succinct data structures: those that use much less than nbits of\nspace in addition to V.\nRank and select queries can be used to implement many other succinct data structures,\nincluding, for example, trees and graphs [ 17,25], suffix trees [ 14], and filters [ 22,13]. Wavelet\ntrees are a particularly motivating example, as they use a sequence of rank and select data\nstructures as a subroutine [ 18,8], and have applications in areas such as text compression,\nDNA alignment, and computational geometry [ 4]. We note that all of these applications\nrequire a data structure that can handle bothrank and select queries on V.\nThese applications have motivated a long line of work on engineering rank and select data\nstructures that can answer queries quickly with very small space overhead [ 28,27,9,16,7,20].\nWhen space is at a premium, the state of the art result is pasta-flat by Kurpicz [ 11]. His\ndata structure uses only 3.58% extra space ( 0.0358nbits in addition to |V|), and is very\nperformant, achieving query time up to 16.5% faster than the previous state of the art.\nHowever, rank and select query performance still incurs a tradeoff between query time\nand space. For example, Vigna [ 27] gives a data structure for rank queries using 25% space\nthat is roughly 19% faster than pasta-flat , and a data structure for select queries using\n12.2% space, which is roughly 65% faster than pasta-flat .\nIn this paper, we make significant progress on overcoming this tradeoff.\n1.1 How Rank and Select Data Structures Work\nWe give a high-level description of the basic ideas behind rank and select data structures.\nThis is a useful introduction to our data structure, and gives context to related work. We\nbegin with the naive data structure: one can store the solution to all rank and select queries,\nrequiring 64(n+n1)bits of space. Throughout the paper we give data structures that allow\nfornup to 264, and thus each bit vector index requires 64bits.\n1.1.1 Searching in a Cache Line\nRank and select data structures often operate on bit vectors with billions of bits. Therefore,\ncache efficiency is by far the most important consideration for performance. In this paper we\nassume a cache line size of 512 bits and a word size of 64 bits. (Our results likely generalize\nby adjusting the parameters in Section 2.)\nWith cache efficiency in mind, we can improve the naive rank and select data structure.\nTo begin, we observe that there is no need to store metadata to help with search within a\ncache line. This is because cache efficiency is our primary goal: a single cache miss allows us\nto bring in 512bits, and look for 1s manually. First we describe how this can be used to\nsave space, and second we describe how to decrease the computation cost within a cache line.\nThis is the strawman data structure, also described in [28].\n\n2 SPIDER: Improved Succinct Rank and Select Performance\nSaving Space. The strawman data structure has a rank array of length⌊n/512⌋that\nstores the number of 1s before each cache line: its ith entry stores Rank (512i−1). To\nanswer Rank (i), first look up the ⌊i/512⌋th entry in the rank array and store its value in r1\n(thus,r1is the number of 1bits in slots 0through 512⌊i/512⌋−1ofV). Then, access Vto\nfind the number of 1s in slots 512⌊i/512⌋throughi; adding this to r1gives Rank (i).\nFor select, the strawman data structure uses a heuristic common to most succinct data\nstructures, called rank-based select [11]. First, set a parameter σ(oftenσ= 8192).\nSimilar to rank, the strawman data structure stores the select array , whoseith slot stores\nSelect (σ·i+ 1), fori= 0,...⌊(n1−1)/σ⌋.\nThe idea of rank-based select is to use the select array to get an approximate solution,\nand then refine it with a linear search—but we note that using the rank array saves\nconsiderable time for this search. To find Select (i), use the select array to look up\ns=Select (⌊(i−1)/σ⌋+ 1). Then, begin a linear scan in the rank array from position\n⌊s/512⌋; the goal is to find the largest entry in the rank array that is smaller than i. Lete\nbe this entry and pbe its position in the rank array; once eandpare found, search for the\n(i−e)th1bit within the cache line starting at 512pinV.\nWe note that rank-based select is Ω(n)time in the worst-case due to the linear scan, but\npast work has shown that it is efficient in practice [11, 28].\nThe strawman data structure uses, for σ= 8192,64·n/512 + 64·n1/8192≤.133nspace\n(13.3% overhead), and is very cache-efficient. However, the cost for a naive scan within a\ncache line is considerable. Fortunately, as we now explain, many modern machines support\nlow-level operations that make this cost almost negligible.\nRank Search Within a Cache Line. Many modern processors have a popcount\noperation that gives the number of 1s in a 64-bit word. Thus, we can find the number of 1s\nin a cache line at or before position iwith at most 8popcount operations. Before the final\npopcount operation, we must right shift to remove all bits in the word after i. This results\nin an extremely computationally efficient way to compute the rank within a cache line.\nSelect Search Within a Cache Line. Pandey, Bender, and Johnson [ 23] give a method\nto search within a bit vector using a clever application of specialized processor instructions.\nIn particular, many modern x86 machines support the pdepandtzcntinstructions. A single\napplication of these two instructions allows us to find the location of the ith1bit in a 64-bit\nword; see [ 23] for details. Thus, we can find the jth1bit within a cache line as follows. We\nperform up to 8popcount instructions, keeping a running count cof the number of 1s seen.\nIf adding the popcount of the new word would cause c>j, we instead use pdepandtzcnt\nto find the (j−c)th1bit within the word.\nThis methodology is often called fast select , and it noticeably speeds up select perform-\nance on the machine we used in our experiments (see, for example, the difference between\npasta-flat andpasta-flat-fs in Figure 3). We note that, except for pasta-flat which\nwe keep as a baseline, all data structures we compare to in our select experiments use fast\nselect within a cache line.\n1.1.2 Multi-Level Rank Data Structures\nTo reduce space past 13%, many data structures use a multi-level rank data structure .\nConsider splitting the bit vector into blocks of size 2b(we will retain a linear search in\nthe cache line, so assume 2b≫512). Then, we can store two rank arrays. The ith entry in\nthe high-level rank array stores the number of ones before bit 2b·iinV. The low-level rank\narray recurses within a 2b-size block: the jth entry of the low-level rank array stores the\n\nM. D. Laws, J. Bliven, K. Conklin, E. Laalai, S. McCauley, Z. S. Sturdevant 3\nnumber of ones between bit 2b·⌊512·j/2b⌋and bit 512jofV.\nTo find Rank (i)we sum the⌊i/2b⌋th entry in the high-level array, the ⌊i/512⌋th entry\nin the low-level array, and finally a scan within the cache line beginning at slot 512⌊i/512⌋of\nV. Select searches can be handled similarly: first a linear scan is performed in the high-level\nrank array, and then in the low-level rank array, finally searching within a cache line.\nThe multi-level approach incurs an extra array lookup in exchange for less space. The\ntop-level array requires space 64·n/2b. Each entry in the second-level array is <2band can\ntherefore be stored in bbits, giving space b·n/512. The most performant and space-efficient\ndata structures recurse another time, using three rank arrays [28, 11].\nImproving Select Queries with Multi-Level Rank. The multi-level rank data\nstructure improves select queries as well. For example, consider a select query where\nSelect (σ⌊i/σ⌋)is10·2bslots away from Select (i). With a single rank array, the algorithm\nwould have to scan through roughly 10·2b/512array slots. Meanwhile, with the two-level\ndata structure, the algorithm would scan through roughly 10entries in the top-level array,\nfollowed by 2b/512entries in the low-level array; for large bnearly a factor 10speedup.\nThis speedup leads to a tension between rank and select queries. Optimizations to the\nrank data structure are limited by their impact on select performance. This tradeoff can\nbe seen in the pasta-wide data structure of Kurpicz [ 11].pasta-wide is a simple two-level\nrank data structure with extremely strong performance; unfortunately, the simplified rank\narray makes select queries extremely slow. This holds even if the select queries use binary\nsearch in an attempt to speed up performance.\nOne of the main contributions of SPIDER is to resolve this tension: we show how\npredictions can reduce the cost of a select query even for highly efficient rank data structures,\nachieving the best of both worlds.\nInterleaving Rank Arrays. For sufficiently large bit vectors, accessing each rank array\nmay incur a separate cache miss, significantly impacting performance. For this reason, Zhou\net al. [28] introduced the idea of interleaving the rank arrays. Specifically, in the above\ndescription, we access entry kof the high-level rank array only when querying Rank (i)\nwithk2b≤i <(k+ 1)2b; thus the entry jaccessed in the low-level rank array satisfies\nj∈{k2b/512,..., (k+ 1)2b/512−1}. Thus, Zhou et al. store a single array storing (for each\nk) thekth entry of the high-level array, followed by all possible lower-array entries given\nabove. If these entries all fit in 512bits, both arrays can be accessed with a single cache miss.\n1.2 Our Contributions\nWe give a data structure, SPIDER (a Succinct Predictive InDEx forRank and select) which\nimproves rank and select performance.\nFor rank queries, SPIDER uses a simple two-level data structure—but interleaves the\nlower-level rank array with the bit vector itself . Thus, if the high-level rank array fits in cache,\nSPIDER can answer rank queries with a single cache miss. This noticeably improves rank\nquery time, even compared to data structures that use far more space.\nFor select queries, SPIDER avoids a costly scan using predictions . In particular, we\ndraw inspiration from recent work on learned data structures, e.g. [ 10,5]. Learned data\nstructures use best-fit lines to predict the answer to queries; past work has shown that\nreal world data is roughly piecewise linear, leading to good results. SPIDER applies this\nprinciple to select queries. We assume that successive select array entries are separated by\napproximately-evenly-spaced 1s, and use this assumption to warm-start our linear scan.\nSPIDER also uses the multi-level strategy on the selectarray to allow us to sample more\n\n4 SPIDER: Improved Succinct Rank and Select Performance\nStructure Cite Space Rank(ns) Select(ns)\nvigna-rank [27] 25.00 40.69 179.15\nsdsl-v [6] 25.00 40.31 -\npasta-wide [11] 3.23 38.81 -\nvigna-select [27] 12.20 - 95.04\nvigna-select-H [27] 15.63 - 71.56\npasta-flat-fs [11] 3.58 50.20 207.47\nni-spider §2.3 3.62 36.98 138.61\nspider §2 3.83 33.56 126.96\nTable 1Comparing the performance of succinct rank and select data structures. See Section 3\nfor details of the experiments. We list the space used as a percentage of the size of the original\nbit vector, and the average time for rank and select on a 8 billion bit segment of the Wikipedia\ndataset. The best performance numbers are in bold, and the best among data structures with under\n5% space are italicized.\nbits, further improving select performance; to our knowledge this is the first data structure\nto use a multi-level select array.\nWe also give a variant, Non-Interleaved SPIDER, which does not interleave the rank array\nwith the bit vector. Non-Interleaved SPIDER’s rank performance is worse than SPIDER but\ncompetitive with the state of the art; its select performance is similar to SPIDER.\nOverall, SPIDER gives improved bounds over any known succinct data structure for\nrank queries. For select queries, SPIDER improves on the state of the art data structure\nwith less than 5% extra space usage, and nearly matches the performance of the best known\nsuccinct data structure that uses 12.5% space and cannot answer rank queries. In particular,\ncompared to the best known data structure with under 5% space, SPIDER gives up to a\n22% speedup for rank, and 41% speedup for select.\n1.3 State of the Art Rank and Select\nWe use the following rank and select data structures as a baseline to compare our data\nstructure. See Table 1 for a summary of the advantages of each.\npoppy: Uses a three-level strategy, interleaving the last two levels, with carefully imple-\nmented bit tricks to improve rank and select performance [28].\nThe publicly-available implementation contains a bug that results in incorrect queries for\nbit arrays beyond 4 billion bits.\npasta-flat : Introduced by Kurpicz [ 11],pasta-flat achieves the state of the art\nperformance for rank and select data structures with less than 5% space overhead. The basic\nidea of pasta-flat is a three level data structure with interleaved second and third levels,\nmuch like poppy. However, pasta-flat uses a different strategy for interleaving the bottom\ntwo arrays using 128-bit entries. These entries are handled efficiently using SIMD operations.\nOverall, Kurpicz showed that pasta-flat gave a significant speedup over poppy(therefore,\nwe only compare to pasta-flat in our tests).\nOne advantage of pasta-flat is that it supports both select 1queries (as defined in this\npaper), as well as select 0queries, which give the location of the jth0. We do not consider\nselect 0queries; however, SPIDER can be easily extended to handle select 0queries by using a\nsecond select data structure (that stores the location of sampled 0s rather than sampled 1s),\nslightly increasing the space.\npasta-flat-fs : Identical to pasta-flat , but uses fast select [ 23] within each cache line.\n\nM. D. Laws, J. Bliven, K. Conklin, E. Laalai, S. McCauley, Z. S. Sturdevant 5\npasta-wide : The second pastadata structure [ 11] uses a simple two-level structure for\nvery fast rank queries. Adding the select structure from pasta-flat results in large space\nand slow select performance; therefore we only consider rank queries on pasta-wide .\nvigna-rank : Vigna’s rank structure, rank9, follows a two level approach [ 27]. Entries\nin the two levels are grouped into 128-bit “words” that are unpacked during select queries\nusing broadword programming techniques. Rank9 has a select data structure, select9, but\nwe do not include it in Section 3 because it is much slower than vigna-select .\nvigna-select : Vigna also proposes a select-only structure (called “simple select” in [ 27])\nwhich we refer to as vigna-select .vigna-select is a position based select structure (it is\nnot rank based); see [ 27] for more details. This data structure takes a tuning parameter; we\nuse2as it gave the best performance (see also the comparison in [11]).\nvigna-select-H : uses similar techniques to vigna-select but is specifically optimized\nfor bit vectors where each bit is a 1with probability .5.\nsdsl-v: The Succinct Data Structure Library [ 6] contains an implementation of the\nvigna-rank data structure described above.\n1.4 Other Related Work\nThere are tight upper and lower theoretical bounds for rank and select data structures [ 24,12].\nCompressed Bit Vectors. An orthogonal line of work has looked at how to save space\nby compressing the underlying bit vector V. In particular, many practical bit vectors can\nbe stored using much less than nbits of space. The goal of this line of work is a trade-off\nbetween how much space is used on practical bit vectors, and how much time rank and select\nqueries take. See for example [25, 21, 19, 2].\nThere are two downsides to compressing V. First, this compression is situational: some bit\nvectors have high entropy and cannot be compressed. Second, queries require uncompressing\nthe underlying data, which results in a cost overhead; see e.g. the experiments in [2].\nIn this paper we assume that Vis stored uncompressed. It is plausible that our ideas\ncould be combined with past work to speed up compressed queries as well.\nLearned Data Structures. A recent, exciting line of work has looked at learned data\nstructures [10,15,5]. Rather than keeping a worst case index, learned data structures use\nmachine learning techniques to store high-level information about the data.\nMore recently, Boffa, Ferragina, and Vinciguerra [ 2] used a learned index to achieve an\nimproved data structure for rank and select. Their focus was distinct from ours: their goal\nwas to compress the bit vector itself (as mentioned above), whereas our goal is to store\nsuccinct metadata to speed up queries.\nHowever, SPIDER’s predictive method for select uses a structure that is reminiscent\nof these learned indices. In particular, the basic strategy of learned indices is to a store a\nsequence of best-fit lines on the underlying data. Past results indicate that on real world\ndata, best fit lines can often store much more accurate and much more concise information\nthan classic worst-case data structures [10, 15, 3, 5].\nOne can view our select methodology as a very lightweight application of these learned\nindices. We do not have space to explicitly store best-fit lines; instead, we look up the\nnumber of ones in a subarray to roughly estimate what a best fit line could look like for a\ngiven subsection of the bit vector. This allows us to estimate the best place to start when\nscanning the bit vector during a select query.\n\n6 SPIDER: Improved Succinct Rank and Select Performance\nVThe bit vector we are querying.\nnLength of the bit vector.\nn1Number of 1 bits in the bit vector.\niA given query (Rank or Select).\nsThe predicted superblock index.\nr1Number of 1 bits in all slots before the superblock containing the query.\nb2Index of the basic block containing the query.\nr216 bit count prepended to the original block; represents the number of 1 bits\nfrom a the start of the superblock until the start of the basic block.\nσhHigh-level sampling threshold: the frequency with which locations are stored\nin the high-level select array.\nσℓLow-level sampling threshold: the frequency with which locations are stored\nin the low-level select array.\nℓIndex in the low-level select array; gives a lower bound on the query solution.\npPrediction for a select query.\np′Altered prediction when the sampled bits cross a superblock boundary.\nBFirst basic block we search in given porp′.\nσSampling threshold for Non-Interleaved SPIDER.\nTable 2Table of notation used in this paper.\n2 The SPIDER Data Structure\nWe now describe our data structures: first SPIDER, and then Non-Interleaved SPIDER.\nThen we compare the techniques of SPIDER and Non-Interleaved SPIDER and how these\ndifferences impact performance.\n2.1 SPIDER Rank\nSPIDER Rank Data Structure.\nSPIDER stores two arrays to help answer rank queries: a rank array, and a modified bit\nvector.\nFirst, consider partitioning the bit vector Vinton/63488contiguous superblocks of\n63488bits1(we explain this constant below). Thus, we say that superblock icontains a\nslotsif63488i≤s<63488(i+ 1). Therank array stores the number of 1sbeforethe first\nbit in each superblock.2Thus, the rank array consists of n/63488 64 -bit entries and the ith\nentry in the rank array stores Rank (63488i−1).\nThen, we store a modified bit vector . We partition Vinton/496contiguous original\nblocksof496bits. Thus, each superblock consists of 128 original blocks. The modified bit\nvector consists of n/496basic blocks , each of 512bits. For original block istarting at slot\nso, letsbbe the first slot in the superblock containing so; we define the local rank to be the\nnumber of 1s in slotssbthroughso−1(inclusive).\nThus, thelocalrankoforiginalblock iisRank (496i−1)−Rank (63488⌊496i/63488⌋−1).\n1We assume for simplicity that the bit vector is padded to have length a multiple of 63488.\n2The first entry in the rank array is always 0, but we store it anyway for simplicity. Recall that Rank (i)\nis defined to be inclusive of i—thus, the number of bits before slot iisRank (i−1).\n\nM. D. Laws, J. Bliven, K. Conklin, E. Laalai, S. McCauley, Z. S. Sturdevant 7\nTheith basic block has two parts: first, a 16-bit number storing the local rank of the ith\noriginal block, followed by the 496bits of the ith original block. The modified bit vector\nconsists of all basic blocks.\nAfter the modified bit vector has been constructed, we no longer need to store V. (Its\ncontents are present in the modified bit vector anyway.) Thus, our data structure consists\nonly of the rank array and the modified bit vector.\nPreprocessing. Therankarrayandthemodifiedbitvectorcanbecreatedsimultaneously\nduring a linear scan over the original bit vector.\nQueries. On a query Rank (i), we begin by using the rank array to find r1=\nRank (63488⌊i/63488⌋−1)(this can be done by looking up the ⌊i/63488⌋th position in\nthe rank array).\nThen, we access the modified bit vector. Bit ican be found in the b2=⌊i/496⌋th basic\nblock. Let r2be the 16-bit value stored at the beginning of block b2in the modified bit\nvector. Then r2+r1gives the rank of the last slot before b2.\nFinally, we must count the number of 1s before position iwithinb2. This can be found\nusing the popcount instruction on the first ⌊i/64⌋+ 1words of the 8words in the basic block.\nDuring this process, we must mask out the first 16bits of the first word, and then remove\nall bits after iin the last word using a shift operation. Summing the result of all of these\npopcount operations with r2+r1gives the rank of i.\nHow we Chose the Superblock Size.\nThe key parameter we chose is the size of the local rank; we chose a moderate value of\n16bits. If the local rank fits in 16bits, each superblock must have at most 216slots. The\nbasic block is the size of a cache line ( 512bits), and 16bits are used to store the local rank,\nleaving 496bits to store the original basic block.\nThe size of the superblock is chosen to be a power of 2 times the size of a basic block.\nWhen calculating the rank, we divide iby first the size of a superblock, and later divide iby\nthe size of a basic block. This requires two divisions, a potentially-expensive CPU operation.\nWe reduce this to one division and one shift by setting the size of a superblock to 128times\nthe size of a basic block: thus, after we calculate ⌊i/63488⌋, we can right shift by 7to get\n⌊i/496⌋. This choice slightly increases our space, but significantly reduces query time.\n2.2 SPIDER Select\nNow we describe how SPIDER works on Select ()queries. We use a rank-based approach,\nso\nselect queries also access the rank array and modified bit vector to help guide the query.\nStoring Metadata. We store two arrays in addition to the rank array and modified\nbit vector: a high-level select array, and a low-level select array. In short, we augment the\nrank-based select strategy described in Section 1.1.1 with the multi-level strategy.\nWe define two sampling thresholds, one for each select array: a high-level sampling\nthreshold σh, and alow-level sampling threshold σℓ.\nFirst, we store a high-level select array of length at most n/63488 + 2 .\nWe define the high-level sampling threshold to be σh= 2⌈log2(63488·n1/n)⌉. We will see\nthat the high-level select array has 2 +⌊n1/σh⌋entries; thus, σhis the smallest power of 2\nthat ensures that the high-level select array has at most n/63488 + 2 entries.3\n3Choosingσhto be a power of 2allows us to replace a division with a shift, improving query time.\n\n8 SPIDER: Improved Succinct Rank and Select Performance\nThe idea of the high-level select array is to store, for every σhth1bit, the nearest\nsuperblock entry of that 1bit.\nThe nearest superblock boundary to slot s′is given by⌊s′/63488 + 1/2⌋. Thus, entry 0in\nthe high-level select array stores ⌊Select (1)/63488 + 1/2⌋. Then, entry iin the high-level\nselect array (for ifrom 1to⌊(n1−1)/σh⌋) stores⌊Select (i·σh+ 1)/63488 + 1/2⌋.\nFinally, we store n/63488−1in the last element of the high-level select array. Thus, the\nhigh-level select array has ⌊(n1−1)/σh⌋+ 2entries, each of 64bits.\nThen, we store a low-level select array . Our goal is to have each entry in the low-\nlevel select array take only 16bits—thus, drawing inspiration from the local rank used in\nSection 2.1, we define the superblock offset of any slots′to be the number of slots between\ns′and the first slot of the superblock containing s′. The first slot of the superblock containing\ns′is63488⌊s′/63488⌋, so the superblock offset of s′iss′−63488⌊s′/63488⌋.\nWe define the low-level sampling threshold σℓ= 2⌈log2(4096·.99n1/n)⌉. The low-level select\narray has 2 +⌊n1/σℓ⌋entries, each storing a superblock offset. Thus, σℓis the smallest power\nof2such that the low-level select array stores at most 2 +n/(.99·4096)entries.4\nTheith entry in the low-level select array stores the superblock offset of Select (σℓ·i).\nIn other words, the first slot of the superblock containing Select (σℓ·i)is slotsi=\n63488⌊Select (σℓ·i)/63488⌋; thus theith entry stores Select (σℓ·i)−si. The last entry\nof the low level array stores Select (n1)−63488⌊Select (n1)/63488⌋. Each entry has value\nat most 63488, so each low-level select array entry can be stored in 16bits.\nPreprocessing.\nFirst we build the rank array and modified bit vector in one scan. Then we can calculate\nn1=Rank (n−1), from which we obtain σhandσℓ. Then, in a second scan through the\ndata we build the high level select array and low level select array.\nQueries and Predictions. On a query Select (i), we use the high-level select array to\nfind the superblock containing the ith set bit as follows. We first set sto be entry⌊(i−1)/σh⌋\nin the high-level select array; sis a guess for the superblock containing Select (i).\nWe then do a linear scan starting at sto find the superblock containing Select (i). We\nbegin with the values stored in the sth and (s+ 1)st entries in the rank array; if iis between\nthese values then Select (i)is in thesth superblock. Otherwise, we begin a linear scan for\nthe correct superblock: if iis less than the sth rank array entry we decrement sand recurse;\nifiis greater than the (s+ 1)st rank array entry we increment sand recurse. At this point,\nwe know that Select (i)is in thesth superblock.\nNow we look in the low-level select array. Let abe the value stored in the ℓth entry of\nthe low-level select array with ℓ=⌊(i−1)/σℓ⌋; letbbe the value stored in the (ℓ+ 1)st entry\nof the low-level select array.\nWe now create our prediction for Select (i). First, some motivation. We say that i\nwell-specified ifaandbboth represent slots in the sth superblock—that is to say, iis\nwell-specified if the sth superblock contains Select (σℓ·ℓ)andSelect (σℓ·(ℓ+ 1)). See\nFigure 1a. If iis well-specified, Select (i)must be between 63488·s+aand63488·s+b.\nLet’s assume (momentarily) that iis well-specified, and, further, that all 1bits are spread\nevenly between slots 63488·s+aand63488·s+b. There are σℓ1s between these slots\nby definition, so if they are spread evenly, there is a 1every (b−a)/σℓslots. By definition,\n4The.99term is to help performance for bit vectors where very slightly over half of the bits are 1s:\nwithout this term, if (say) n1/n=.50001, we would store roughly n/8192entries (whereas we would\nstoren/4096forn1/n=.5), hurting select query performance. Thus, the .99term slightly increases our\nworst-case space bound, but helps performance on datasets with n1/n>. 5andn1/n≤.505.\n\nM. D. Laws, J. Bliven, K. Conklin, E. Laalai, S. McCauley, Z. S. Sturdevant 9\ns\nℓ·σℓ1sσℓ1saslots\nbslotsi\n(a)s\nℓ·σℓ1sσℓ1saslots bslotsi\np′\n(b)\nFigure 1 Two examples of the values used to create a prediction for Select (i). The example on\nthe left is well-specified, and imust be between aandbin superblock s. The example on the right\nis not well-specified, since aandbare local ranks from different superblocks.\nthere arei−σℓ·ℓ1s between Select (σℓ·ℓ)andSelect (i). If these 1sare evenly spread,\nsubstituting Select (σℓ·ℓ) = 63488·s+a, then Select (i)is located at position\np= 63488·s+a+b−a\nσℓ·(i−σℓ·ℓ). (1)\nThus, for any i, we define pusing Equation 1; we search for Select (i)using a linear\nscan beginning at p. Specifically, since Select (i)is contained in the sth superblock, we\nbegin at basic block B=⌊p/496⌋. We proceed with a linear scan. First, we ensure that the\nnumber of 1bits before basic block Bis at mosti.\nThe number of 1bits before block Bcan be obtained by summing the local rank of B\nwith the⌊B/128⌋th entry in the rank array. While this sum is at most i, we decrement B\nand check again. Then, we use the same strategy to ensure that the number of 1bits before\nblockB+ 1is at leasti(while it is not, we increment B). After both loops complete, we\nknow that Select (i)is in blockB.\nNow, let us revisit the assumptions we made when defining predictions: what happens\nwheniis not well defined? Even on very well-behaved data, our low-level select array entries\nwill cross superblock boundaries relatively frequently (see Figure 1b); we must ensure that in\nthis case our queries are correct, and that our predictions are as accurate as possible.\nAt first glance, correctness appears to be guaranteed since we do a linear scan for the\ncorrect basic block—however, our algorithm as presented is not correct if b<a. In particular,\nifb<aandiis in the first or last superblock, we may have p<0orp>n−1. On top of\nthe correctness issue, b<aalso causes particularly bad predictions. We found that this case\nis common enough to noticeably slow down our queries.\nThus, ifb < a, we make the following adjustment to ensure correctness and improve\nthe solution quality. In short, we can test which sampled 1is in the same superblock as i,\nand adjust aorbaccordingly. Specifically, we test if entry sin the rank array is at least\nσℓ·ℓ. If so, Select (σℓ·ℓ)is in a superblock before i, soais not in the same superblock as\ni; we calculate a′=a−63488. Otherwise we set b′=b+ 63488. In each case we calculate\np′using Equation 1 with a′orb′, and begin the search at B=⌊p′/496⌋. For example, in\nFigure 1b,a′would be a negative number representing the slots between aand the beginning\nof superblock s; thus,p′is in the range shown in the figure.\nThis fix improves results in a good prediction if the sampled bits are in successive blocks.\nFurthermore, a case-by-case analysis shows that pis now always between 0andn−1. Of\ncourse, this simple fix does not always result in a good prediction—if aandbare separated by\nmultiple blocks, or if iis not well-specified but a<b, thenpis unlikely to be accurate—but\nthese cases are rare enough on real-world data as to not significantly impact performance.\n\n10 SPIDER: Improved Succinct Rank and Select Performance\nAfter the above, we have found the basic block Bcontaining Select (i). We perform a\nselect within Bby first masking out the first 16 bits, and then using fast select (Section 1.1.1).\nSpace. The rank array requires 64·n/63488bits. The modified bit vector requires\n512·n/496bits. Summing, we obtain 3.33%extra space. The high-level select array requires\nat most 64·n/63488bits; the low-level select array requires at most 16·n/4096/.99bits.\nThus, select requires .495%space. Summing, SPIDER requires a 3.83%space overhead.\n2.3 Non-Interleaved SPIDER\nFor some use cases, it may not be possible to modify V. With these cases in mind, we\ndefine a version of spider that does not interleave metadata: it leaves the original bit vector\nuntouched, and uses 3.62% space to help answer rank and select queries. We call this data\nstructure Non-Interleaved SPIDER, or ni-spider .\nNon-Interleaved SPIDER has further advantages, even for use cases where the bit vector\nmay be modified.\nIt is simpler: ni-spider uses a simpler one-level select method that is easier to implement,\nand it avoids several corner cases. Furthermore, we discuss in Section 2.4 that the non-\ninterleaved rank metadata\nimproves select performance on some datasets.\nNon-Interleaved Rank Data Structure. The rank data structure and queries for\nnon-interleaved SPIDER is essentially the same as pasta-wide as given by by Kurpicz [ 11]\n(however, the two data structures differ for select queries). Nonetheless, we describe the rank\ndata structure for completeness, as well as for reference when describing select queries.\nWe partition the array into n/65536contiguous superblocks of 65536bits. The rank\narrayconsists ofn/65536 64 -bit entries, where the ith entry stores Rank (65536i−1).\nThen, we store a second level rank array . We partition the bit vector into n/512basic\nblocks of 512bits.\nWe define the local rank of blockito be\nRank (512i−1)−Rank (65536⌊512i/65536⌋−1). We store this value in slot iof the\nsecond level rank array. We retain Vas well to help with rank and select queries.\nRank Queries. To find Rank (i), we begin by using the rank array to find r1=\nRank (⌊i/65536⌋−1)(this can be done by looking up the ⌊i/65536⌋th position in the rank\narray). Then, we use the second level rank array to find r2=Rank (⌊i/512⌋−1).\nFinally, we use popcount to count the number of 1s before position iwithin basic block\nb2ofV(see Section 1.1.1).\nNon-interleaved Select Data Structure For non-interleaved SPIDER we use a\nsingle-level select array. We discuss why a single level is sufficient in Section 2.4.\nWe calculate a single sampling threshold σ= 2⌈log2(16384·n1/n)⌉. Theselect array has\nat mostn/16384entries, each of 64bits.5Entryiof the select array stores Select (σ·i).\nNon-interleaved Select Queries To answer Select (i), letabe the value stored in\nthe⌊i/σ⌋th entry of the select array, and let bbe the value stored in the (⌊i/σ⌋+ 1)th entry.\nThus we predict the value of Select (i)to bep=a+b−a\nσ(i−σ⌊i/σ⌋).\nSince we store a single select array, there is no issue when aandbspan a superblock\nboundary.\n5The constant 16384was chosen to remain under 4% space usage.\n\nM. D. Laws, J. Bliven, K. Conklin, E. Laalai, S. McCauley, Z. S. Sturdevant 11\nWe begin at basic block B=⌊p/512⌋. We use the rank array and the second-level rank\narray to scan for the correct basic block. The number of 1s beforeBcan be obtained by\nadding the⌊B/128⌋th entry in the rank array to the Bth entry in the second-level rank\narray; while this number is greater than iwe decrement B. We then increment Bwhile the\n(⌊B/128⌋−1)st rank array entry plus the (B+ 1)st second-level rank array entry are smaller\nthani. OnceBis found, we select in a block using fast select (see Section 1.1.1).\nSpace Usage. Summing 64n/65536 +16n/512+64n/16384we obtain 3.62%extra space.\n2.4 Comparing SPIDER and Non-Interleaved SPIDER Select Queries\nHere we discuss some performance differences between SPIDER and Non-Interleaved SPIDER.\nFor rank queries, the interleaved strategy has strictly better cache performance. SPIDER\nincurs≤1cache miss when accessing the rank array, and a cache miss when accessing the\nmodified bit vector. Non-Interleaved SPIDER incurs ≤1cache miss for the rank array, a\ncache miss to access the second-level rank array, and another to access the bit vector.\nInterleaving the ranks has an immediate impact on select queries. Non-Interleaved\nSPIDER does a linear search for the block containing Select (i)in the second-level rank\narray, whereas SPIDER searches the modified bit vector.\nThis leads to an interesting tradeoff in cache performance. SPIDER incurs a cache miss\neach time it considers a new basic block. In contrast, Non-Interleaved SPIDER almost never\nincurs more than one cache miss in the second-level rank array, since 32consecutive basic\nblocks fit in a cache line. However, Non-Interleaved SPIDER incurs an extra cache miss to\naccess the bit vector itself. Thus, assuming that all array accesses result in a cache miss, if p\nisgbasic blocks away from Select (i), SPIDER incurs 1 +gcache misses. Non-Interleaved\nSPIDER incurs on average 2 +g/16cache misses.\nThus, the cache advantages of SPIDER’s rank queries also apply to select queries if the\npredictions give exactly the correct basic block. However, inaccurate predictions immediately\nlead to cache misses for SPIDER, whereas Non-Interleaved SPIDER’s cache performance is\nminimally affected by prediction quality. The break-even point is when each prediction is 1\nbasic block away from the correct basic block on average.\nThis tradeoff motivates our two-level select array for SPIDER. The two-level design leads\nto more computation and a more complicated data structure. In exchange, the two-level\narray samples far more 1positions in the same space, increasing the prediction quality. This\nis particularly important on sparse data (see Figure 5).\nIn addition to the difference in cost for bad predictions, SPIDER has a secondary\nadvantage from avoid a second-level rank array: the extra array uses up the cache, potentially\ncausing other metadata to be evicted much earlier. This may cause Non-Interleaved SPIDER\nto incur more cache misses than SPIDER when nis small.\n2.5 SPIDER vs Non-Interleaved SPIDER Predictions\nWe ran experiments to compare SPIDER and Non-Interleaved SPIDER in more detail. In\nTable 3, we give results for the accuracy of the prediction of each method: the table gives\nthe average number of incorrect basic blocks looked at by SPIDER and Non-Interleaved\nSPIDER on three datasets. We see that the predictions are generally very accurate; even on\nthe sparse Protein dataset, the predictions are off by approximately one basic block.\nWe also see that, as one might expect, real data is more difficult to predict than random\nsynthetic data. Nonetheless, the results in Figure 3 show that even with this modest decrease\nin prediction quality, the algorithms retain good results.\n\n12 SPIDER: Improved Succinct Rank and Select Performance\n3 Experiments\nWe ran all experiments on a x86-64 11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz with\nL1d, L1i, L2, and L3 cache sizes 192 KiB, 128 KiB, 5 MiB and 12 MiB respectively. The\nmachine had 32GB RAM and was running Ubuntu 20.04 LTS. The code was written in C\nand compiled using gcc version 9.4.0 with compiler flags: -O9 -march=native -mpopcnt\n-mlzcnt.\nFor all experiments, we first ran 108warmup queries to ensure a warm cache. Then, we\nran108queries, recording the total time and dividing to get the average query time. We ran\neach experiment 5times; the results are the average of these 5experiments.\nWe give the code for all spidervariants at https://github.com/williams-cs/spider .\nDatasets. Our experiments use both synthetic and real data sets. For synthetic data,\nwe provide three different densities: 10%, 50%, and 90%. The data is generated by setting\neach bit to 1 independently with probabilities 0.1, 0.5, or 0.9 respectively. We refer to these\ndatasets as 10% Random, 50% Random, and 90% Random.\nWe give two bit vectors based on protein data, which we call Protein and Protein-Even.\nThese bit vectors are based on the Uniref90 protein data set [ 26].6To generate Protein, we\nset each occurrence of a leucine amino acid (character L) to a 1 bit and all others to zero.\nProtein is sparse: 9.72% of the bits are 1s, and the rest are 0s. We also give experiments on a\nsecond dataset, which we call Protein-Even; this dataset was generated by setting characters\nAthrough Lto0and all others to 1; this gives a density of 44.5% 1s.\nTheseconddatasetconsistsofadumpofallWikipediaarticles.7Wemappedallcharacters\nathrough nandAthrough Nto 1 and all others to 0; this resulted in a dataset with 47% 1s.\nThe Wikipedia data has less than 32billion characters, so results only go up to 16billion.\nWe note that wavelet trees in particular motivate our Wikipedia and Protein-Even\ndatasets: the top level in a wavelet tree is a bit vector that maps (roughly) half the original\ncharacters to 1and the other half to 0[18]. Thus, our Wikipedia and Protein-Even bit\nvectors are exactly the top level of a wavelet tree on the original wikipedia and Uniref 90\ndatasets.\nRank Query Performance. We compare spiderandni-spider to four other datasets.\nFirst, we compare to pasta-flat : this is the most performant data structure that can handle\nboth rank and select queries with under 5% space. We also compare to pasta-wide ,sdsl-v,\nandvigna-rank , all of which are tuned specifically for rank queries and cannot handle select\nqueries efficiently (see Section 1.3). We note that ni-spider andpasta-wide are effectively\nthe same method for rank (though not for select) queries, as are sdsl-vand vigna-rank .\nUnsurprisingly, their performance values are very similar; any discrepancies are likely due to\nsmall implementation differences. Nonetheless, we include all results for completeness.\nWe give our rank results in Figure 2; results for the remaining datasets are essentially\nidentical and are omitted. Overall, SPIDER gives the best rank results for large n, even\ncompared to data structures that have worse space efficiency and are tailored specifically to\nrank queries. For smaller n, SPIDER is still competitive, though is not the best, likely due\nto the extra computation required to extract the interleaved metadata.\nSelect Query Performance. We compare spiderandni-spider topasta-flat and\n6Obtained from https://www.uniprot.org/help/uniref .\n7We used the Wikipedia dump from 01-01-2024: enwiki-20240101-pages-articles-multistream.xml .\nThis was downloaded from https://mirror.accum.se/mirror/wikimedia.org/dumps/enwiki/ , and\nextracted from xml to text using WikiExtractor [1].\n\nM. D. Laws, J. Bliven, K. Conklin, E. Laalai, S. McCauley, Z. S. Sturdevant 13\n1 2 4 8 16 32\nBit V ector Size / 10/uni2079203040506070A verage Query Time (n )50 % Random\n1 2 4 8 16 32\nBit V ector Size / 10/uni2079203040⁹06070Protein\nspider\nni-spiderpasta-flat\npasta-widesdsl-v\nvigna-rank\nFigure 2 Average rank query time in nanoseconds, for bit vectors from 1 billion to 32 billion bits.\npasta-flat-fs , the most performant data structures under 10% space, and vigna-select-H\nandvigna-select , which use over 10% space and cannot answer rank queries.\nWe give our select results in Figure 3. SPIDER achieves better performance than\npasta-flat , the state of the art data structure using less than 5% space. Even compared to\nthe space-inefficient data structures, SPIDER performs quite well, achieving roughly similar\nperformance to vigna-select on 50% random and 90% random data, and performing only\nslightly worse than vigna-select on all other datasets. SPIDER does noticeably worse\nthan vigna-select-H on Wikipedia, Protein Even, and Random 50% data, but better on\nthe sparse datasets—this is unsurprising since vigna-select-H is specifically tailored to\nperform well for data sets with n1/n≈.5.\nSPIDER and Non-Interleaved SPIDER achieve similar select performance on all datasets.\nBuild Time. Weranexperimentscomparingbuildtime. Preprocessingisverylightweight\nfor SPIDER and Non-Interleaved SPIDER: the data structures only consist of a few arrays\nthat can each be built with a linear scan through the data. We used popcount and fast\nselect during our build to minimize the computation time.\nWe give the results for build on 50% Random data in Figure 4; SPIDER and Non-\nInterleaved SPIDER are significantly faster to build than any other data structure. We note\nthat the pasta-flat andpasta-flat-fs lines overlap in this figure.\nComparison of SPIDER Variants. SPIDER and Non-Interleaved SPIDER differ in\ntwo ways: Non-Interleaved SPIDER does not interleave data, and uses a single-level select\narray. Here, we compare to the other two natural variants of SPIDER: spider-1L-select\ninterleaves the ranks and keeps a 1-level select array, and ni-spider-2L-select does not\ninterleave ranks, but has a 2-level select array. The results are given in Figure 5.\nOverall, SPIDER and Non-Interleaved SPIDER have the best results. However, the other\ntwo variants often achieve similar or even non-negligibly better performance, particularly for\nsmall values of n. One interesting point is that spider-1L-select has a single-level select\narray, but interleaves local ranks (and thus has a high cost for bad guesses)—for the Random\n50% data the single-level select array is sufficient for high-quality predictions, whereas for\nthe sparse, relatively-difficult-to-predict Protein bit vector it lags noticeably behind the other\nvariants.\nWe note that the distinction between the 1-level and 2-level select arrays has no impact\non rank performance. Therefore, the performance of spider-1L-select is identical to that\n\n14 SPIDER: Improved Succinct Rank and Select Performance\n1 2 4 8 16 32\nBit V ector Size / 10/uni207975100125150175200225250A verage Query Time (n )10 % Random\n1 2 4 8 16 32\nBit V ector Size / 10/uni20797⁹10012⁹1⁹017⁹20022⁹2⁹0Protein\n1 2 4 8 16 32\nBit  V ec or Size / 10/uni207950100150200250A verage Query Time (ns)50⁹ Random\n1 2 4 8 16 32\nBit V ector Size / 10/uni2079⁹07⁹10012⁹1⁹017⁹20022⁹2⁹0Wikipedia\n1 2 4 8 16 32\nBit V ector Size / 10/uni207950100150200250Average Q ery Time ⁹ns)Protein Even\n1 2 4 8 16 32\nBit V ector Size / 10/uni207950100150200250A verage Query Time (n )90 % Random\nspider\nni-spiderpasta-flat\npasta-flat-fsvigna-select-H\nvigna-select\nFigure 3 Average select query time in nanoseconds, for datasets from 1 billion to 32 billion bits.\n\nM. D. Laws, J. Bliven, K. Conklin, E. Laalai, S. McCauley, Z. S. Sturdevant 15\n1 2 4 8 16 32\nBit V ecto  Size / 10/uni2079103104105Build Time (ms)50 % Random Select Build Times\nspider\nni-spider\npasta-flat\npasta-flat-fs\nvigna-select-H\nvigna-select\nFigure 4 Total build time in milliseconds.\nTheyaxis uses a logarithmic scale.Dataset spider ni-spider\n50% Random 0.040526 0.078617\nProtein 0.413667 1.387546\nWikipedia 0.059781 0.156082\nTable 3Prediction accuracy comparison\nof SPIDER and Non-Interleaved SPIDER. All\ndatasets have 1 billion bits. The values given\nare the average number of incorrect basic\nblocks searched during a select query before\nthe correct basic block is found.\nofspider(and ni-spider-2L-select is identical to ni-spider ) in Figure 2, so we do not\ngive additional rank experiments here.\n4 Conclusion\nIn this work we give SPIDER, a succinct data structure for rank and select. SPIDER\ninterleaves metadata with the underlying bit vector to improve rank performance, and uses\npredictions and a two-level select array to improve select performance, while using only 3.83%\nadditional space. For rank queries, SPIDER gives an up to 22%\nimprovement in query speed over the state of the art, even comparing to data structures\nusing much more space. For select queries, SPIDER is up to 41%\nfaster than the state of the art when comparing only to data structures that use under\n5% space. SPIDER is as little as 3.1%\nslower than the state of the art data structure (which uses 12.2% extra space and cannot\nanswer rank queries). While these speedups represent our best results, SPIDER consistently\noutperforms data structures using similar space, and is competitive with state of the art\ndata structures using more space. Thus, SPIDER makes significant progress on eliminating\nthe tradeoff between space usage and query time for rank and select queries. We show that\nSPIDER is effective on both real and synthetic datasets.\nOur second data structure, Non-Interleaved SPIDER, leaves the underlying bit vector\nunchanged. Non-Interleaved SPIDER roughly matches the performance of the best known\nrank data structure other than SPIDER, and matches SPIDER select performance.\nReferences\n1Giusepppe Attardi. Wikiextractor. https://github.com/attardi/wikiextractor , 2015.\n2Antonio Boffa, Paolo Ferragina, and Giorgio Vinciguerra. A learned approach to design\ncompressed rank/select data structures. Transactions on Algorithms (TALG) , 18(3):1–28,\n2022.\n3Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li, Hantian Zhang,\nBadrish Chandramouli, Johannes Gehrke, Donald Kossmann, et al. ALEX: an updatable\nadaptive learned index. In Proceedings of the ACM International Conference on Management\nof Data (SIGMOD) , pages 969–984, 2020.\n4Patrick Dinklage, Jonas Ellert, Johannes Fischer, Florian Kurpicz, and Marvin Löbel. Practical\nwavelet tree construction. Journal of Experimental Algorithmics (JEA) , 26:1–67, 2021.\n\n16 SPIDER: Improved Succinct Rank and Select Performance\n1 2 4 8 16 32\nBit V ector Size / 10/uni207980100120140160180200Average Q ery Time (ns)10⁹ Random\n1 2 4 8 16 32\nBit V ector Size / 10/uni207980100120140160180200⁹rotein\n1 2 4 8 16 32\nBit  V ec or Size / 10/uni207980100120140160180200A verage Query Time (ns)50⁹ Random\n1 2 4 8 16 32\nBit V ector Size / 10/uni207980100120140160180⁹ikipedia\nspider ni-spider spider-1L-select ni-spider-2L-select\nFigure 5 Average time for a select query in nanoseconds for the four different variants of SPIDER.\n5Paolo Ferragina and Giorgio Vinciguerra. Learned data structures. In Recent Trends in\nLearning From Data: Tutorials from the INNS Big Data and Deep Learning Conference\n(INNSBDDL2019) , pages 5–41. Springer, 2020.\n6Simon Gog, Timo Beller, Alistair Moffat, and Matthias Petri. From theory to practice: Plug\nand play with succinct data structures. In 13th International Symposium on Experimental\nAlgorithms (SEA) , pages 326–337. Springer, 2014.\n7Rodrigo González, Szymon Grabowski, Veli Mäkinen, and Gonzalo Navarro. Practical imple-\nmentation of rank and select queries. In Poster Proc. Volume of 4th Workshop on Efficient\nand Experimental Algorithms (WEA) , pages 27–38. CTI Press and Ellinika Grammata Greece,\n2005.\n8Roberto Grossi, A Gupta, JS Vitter, et al. High-order entropy-compressed text indexes. In\nProceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA) ,\npages 841–850, 2003.\n9Guy Jacobson. Space-efficient static trees and graphs. In 30th Annual Symposium on\nFoundations of Computer Science (FOCS) , pages 549–554. IEEE Computer Society, 1989.\n10Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the ACM International Conference on Management of\nData (SIGMOD) , pages 489–504, 2018.\n11Florian Kurpicz. Engineering compact data structures for rank and select queries on bit\nvectors. In International Symposium on String Processing and Information Retrieval (SPIRE) ,\npages 257–272. Springer, 2022.\n\nM. D. Laws, J. Bliven, K. Conklin, E. Laalai, S. McCauley, Z. S. Sturdevant 17\n12Tianxiao Li, Jingxun Liang, Huacheng Yu, and Renfei Zhou. Dynamic “succincter”. In 64th\nAnnual Symposium on Foundations of Computer Science (FOCS) , pages 1715–1733. IEEE,\n2023.\n13Mingmou Liu, Yitong Yin, and Huacheng Yu. Succinct filters for sets of unknown sizes. In\n47th International Colloquium on Automata, Languages, and Programming (ICALP) . Schloss\nDagstuhl-Leibniz-Zentrum für Informatik, 2020.\n14Veli Mäkinen and Gonzalo Navarro. Succinct suffix arrays based on run-length encoding. In\n16th Annual Sympsoim on Combinatorial Pattern Matching (CPM) , pages 45–56. Springer,\n2005.\n15Michael Mitzenmacher. A model for learned bloom filters and optimizing by sandwiching.\nAdvances in Neural Information Processing Systems (NeurIPS) , 31, 2018.\n16J Ian Munro. Tables. In International Conference on Foundations of Software Technology and\nTheoretical Computer Science , pages 37–42. Springer, 1996.\n17J Ian Munro and Venkatesh Raman. Succinct representation of balanced parentheses, static\ntrees and planar graphs. In Proceedings of the 38th Annual Symposium on Foundations of\nComputer Science (FOCS) , pages 118–126. IEEE, 1997.\n18Gonzalo Navarro. Wavelet trees for all. Journal of Discrete Algorithms , 25:2–20, 2014.\n19Gonzalo Navarro. Compact data structures: A practical approach . Cambridge University Press,\n2016.\n20Gonzalo Navarro and Eliana Providel. Fast, small, simple rank/select on bitmaps. In\nInternational Symposium on Experimental Algorithms (SEA) , pages 295–306. Springer, 2012.\n21Giuseppe Ottaviano and Rossano Venturini. Partitioned elias-fano indexes. In Proceedings\nof the 37th international ACM SIGIR conference on Research & development in information\nretrieval, pages 273–282, 2014.\n22Prashant Pandey, Michael A Bender, and Rob Johnson. A fast x86 implementation of select.\narXiv preprint arXiv:1706.00990 , 2017.\n23Prashant Pandey, Michael A Bender, Rob Johnson, and Rob Patro. A general-purpose counting\nfilter: Making every bit count. In International conference on Management of Data (SIGMOD) ,\npages 775–787, 2017.\n24Mihai Patrascu. Succincter. In 49th Annual Symposium on Foundations of Computer Science\n(FOCS), pages 305–313. IEEE, 2008.\n25Rajeev Raman, Venkatesh Raman, and Srinivasa Rao Satti. Succinct indexable dictionaries\nwith applications to encoding k-ary trees, prefix sums and multisets. ACM Transactions on\nAlgorithms (TALG) , 3(4):43–es, 2007.\n26Baris E Suzek, Hongzhan Huang, Peter McGarvey, Raja Mazumder, and Cathy H Wu. Uniref:\ncomprehensiveandnon-redundantuniprotreferenceclusters. Bioinformatics , 23(10):1282–1288,\n2007.\n27SebastianoVigna. Broadwordimplementationofrank/selectqueries. In International Workshop\non Experimental and Efficient Algorithms (SEA) , pages 154–168. Springer, 2008.\n28Dong Zhou, David G Andersen, and Michael Kaminsky. Space-efficient, high-performance\nrank and select structures on uncompressed bit sequences. In Symposium on Experimental\nAlgorithms (SEA) , pages 151–163. Springer, 2013.",
  "textLength": 56665
}