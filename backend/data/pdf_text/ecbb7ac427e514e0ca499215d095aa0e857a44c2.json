{
  "paperId": "ecbb7ac427e514e0ca499215d095aa0e857a44c2",
  "title": "Blueprinting the Cloud: Unifying and Automatically Optimizing Cloud Data Infrastructures with BRAD",
  "pdfPath": "ecbb7ac427e514e0ca499215d095aa0e857a44c2.pdf",
  "text": "Blueprinting the Cloud: Unifying and Automatically Optimizing\nCloud Data Infrastructures with BRAD â€“ Extended Version\nGeoffrey X. Yu\nMIT CSAIL\nCambridge, MA\ngeoffxy@mit.eduZiniu Wu\nMIT CSAIL\nCambridge, MA\nziniuw@mit.eduFerdi Kossmann\nMIT CSAIL\nCambridge, MA\nkossmann@mit.eduTianyu Li\nMIT CSAIL\nCambridge, MA\nlitianyu@mit.edu\nMarkos Markakis\nMIT CSAIL\nCambridge, MA\nmarkakis@mit.eduAmadou Ngom\nMIT CSAIL\nCambridge, MA\nngom@mit.eduSamuel Madden\nMIT CSAIL\nCambridge, MA\nmadden@csail.mit.eduTim Kraska\nMIT CSAIL, AWS\nCambridge, MA\nkraska@mit.edu\nABSTRACT\nModern organizations manage their data with a wide variety of\nspecialized cloud database engines (e.g., Aurora, BigQuery, etc.).\nHowever, designing and managing such infrastructures is hard.\nDevelopers must consider many possible designs with non-obvious\nperformance consequences; moreover, current software abstrac-\ntions tightly couple applications to specific systems (e.g., with\nengine-specific clients), making it difficult to change after initial\ndeployment. A better solution would virtualize cloud data manage-\nment, allowing developers to declaratively specify their workload\nrequirements and rely on automated solutions to design and man-\nage the physical realization. In this paper, we present a technique\ncalled blueprint planning that achieves this vision. The key idea is\nto project data infrastructure design decisions into a unified design\nspace (blueprints). We then systematically search over candidate\nblueprints using cost-based optimization, leveraging learned mod-\nels to predict the utility of a blueprint on the workload. We use\nthis technique to build BRAD, the first cloud data virtualization\nsystem. BRAD users issue queries to a single SQL interface that can\nbe backed by multiple cloud database services. BRAD automatically\nselects the most suitable engine for each query, provisions and\nmanages resources to minimize costs, and evolves the infrastruc-\nture to adapt to workload shifts. Our evaluation shows that BRAD\nmeet user-defined performance targets and improve cost-savings\nby 1.6â€“13Ã—compared to serverless auto-scaling or HTAP systems.\n1 INTRODUCTION\nOver the past decade, the cloud has transformed how organiza-\ntions manage their data through two key forces: (i) offering a\nplethora of specialized database engines optimized for diverse work-\nloads [ 10,12,15], and (ii) enabling â€œone-clickâ€ on-demand access to\nconceptually â€œinfiniteâ€ resources [ 14,17,45,74]. To reap these bene-\nfits, cloud users must curate a collection of such specialized database\nengines, each offering a competitive edge on different parts of their\nworkload. For example, an organization might use Aurora [ 12] to\nmanage client accounts with transactions, Snowflake [ 34] to analyze\nhistorical sales data, and BigQuery Omni [ 43] for exploratory anal-\nysis. Benefits aside, these multi-system infrastructures introduce\nnew management challenges. Data engineers need to (i) choose a\nsuitable set of engines (out of dozens [ 20,21,44]) for their work-\nload, (ii) partition and/or replicate their data across the engines,\n(iii) decide which engines to use for each aspect of their workload(i.e., which queries go to each engine), (iv) provision the engines\nappropriately, and (v) repeat these steps each time their workload\nor business needs change. Navigating these decisions is hard; prior\nwork showed that an optimal infrastructure depends on many inter-\nconnected factors such as query selectivity, service level objectives\n(SLOs), and dynamic load of the system [ 60]. Designs based on con-\nventional wisdom can miss out on significant performance and cost\nsavings (Section 2.1). As a result, organizations struggle to design\ntheir infrastructure while also keeping costs under control [46].\nTo address this challenge, we recently presented our vision for\nBRAD [ 60]. BRAD is fundamentally a virtualization layer for cloud\ndata infrastructure. BRAD users do not specify the mapping of data\nto specific engines or explicitly provision resources. Instead, BRAD\nuses a proxy-like indirection layer [ 19,24,29] to abstract away\nmultiple database engines, appearing to end-users as a single SQL\nendpoint. Under the covers, BRAD allocates data and operates the\ninfrastructure by picking the â€œbestâ€ set of engines for the workload,\nchoosing the appropriate data distribution and provisioning for\neach engine, and routing queries optimally. This is a fundamentally\nchallenging because BRAD must explore a huge space of possible\nsolutions, while meeting performance expectations.\nWe solve this problem using a novel technique we call blueprint\nplanning , which is a holistic cost-based optimization over the in-\nfrastructure design space. Specifically, blueprints are system plans\nthat define a BRAD deployment. They contain (i) the set of engines\nto include in the infrastructure, (ii) their provisioning configura-\ntions (e.g., instance type and number of nodes), (iii) the engine(s)\non which each table in the dataset is placed, and (iv) a policy for\nrouting queries to the engines. Blueprints allow us to systematically\nand quantitatively consider all aspects of the infrastructure design\nproblem in a unified search space, analogous to traditional query\nplanning [ 96]. However, accurately assigning scores to blueprints\nis significantly harder than query planning. First, the utility of a\nblueprint is not captured by performance alone, as a good blueprint\nfor a given workload minimizes dollar-based operating costs under\na latency-based performance constraint (or vice-versa, depending\non user-specified goals). Second, accurately predicting a workloadâ€™s\nperformance (e.g., a queryâ€™s run time) on a blueprint is difficult due\nto (i) engines having opaque system implementations and (ii) new\nconstraints in our setting. Specifically, we must make these pre-\ndictions when a physical query plan is unavailable, preventing us\nfrom reusing existing learned models [ 50,70,71,73,100,114]. ForarXiv:2407.15363v1  [cs.DB]  22 Jul 2024\n\nexample, a candidate blueprint may add an engine into the infras-\ntructure that is not yet running (e.g., starting up a data warehouse)\nor replicate a table onto a new engine to support a query.\nIn this paper, we show that these challenges are tractable. In the\ncloud setting, infrastructure operators can collect performance data\nover a wealth of workloads and deployments to build learned perfor-\nmance models. Moreover, most query optimizers are deterministic.\nThus, we can train a model to predict a queryâ€™s run time using just\nits logical properties (e.g., filter selectivities, join templates) since\nthe optimizer will pick similar query plans with comparable run\ntimes for similar queries. We leverage these observations to build a\ngraph neural network with a novel query featurization that relies\nonly on such logical query features (Section 3.2). Together with\nother analytical models, we use this model to predict the perfor-\nmance and cost of candidate blueprints on a given workload. We\nthen use these predictions to drive a greedy beam-based search\nover the blueprint search space to find an optimized infrastructure\ndesign. We have implemented our blueprint planner in BRAD, en-\nabling it to automatically design infrastructures consisting of three\nengines that cover a large part of enterprise needs: (i) a transac-\ntional store (Aurora [ 12]), (ii) a data warehouse (Redshift [ 15]), and\n(iii) a data lake query engine (Athena [10]).\nWhile there is a wealth of prior work in automatically optimiz-\ning single systems [ 68,78,82â€“84,86â€“88,95,106], and in managing\nexisting multi-engine deployments [ 5,25,27,28,37,41,52,53,92,\n97,110,117], BRAD holistically automates and optimizes the design\nand operation of multi-engine infrastructures. Doing so involves\nreasoning about cost and performance across engines and hypothet-\nical deployments, which, to our knowledge, have not been studied.\nWe evaluate BRAD by having it automatically optimize a data\ninfrastructure for cost under a performance constraint, We use a\nworkload with both transactions and diverse analytics running on\nan adapted version of the IMDB dataset [ 63]. Overall, we show that\nBRAD is able to react to changing workloads and select designs\nthat achieve performance targets in diverse deployment scenarios.\nWhen compared to a baseline that naÃ¯vely auto-scales transactional\nand analytical systems, BRAD achieves 1.6â€“13 Ã—cost savings due\nto its ability to route queries between engines and precisely scale\nto the resource needs of a workload, instead of reacting passively\nto increased system load.\nContributions. In summary, we make the following contributions:\nâ€¢We introduce blueprint planning : a new framework for virtual-\nized, automated cloud data infrastructure design and manage-\nment that applies cost-based optimization.\nâ€¢We present a practical blueprint planning solution. We leverage\na graph neural network with a novel logical query featurization\nthat generalizes to common gradual workload changes.\nâ€¢We present the design, implementation, and evaluation of BRAD:\na virtualized cloud database management system that uses blue-\nprint planning to automate infrastructure design.\n2 CONQUERING THE COMPLEX CLOUD\nWe first illustrate the subtle challenges in cloud infrastructure de-\nsign and contrast this experience with using BRAD.\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nQ7\nQ8\nQ9\nQ100510Query Run Time (s)Design 1\nDesign 2\nQ11\nQ12\nQ1302040\nR14\nR15\nR16\nR1702040\n0 100 200 300 400 500\nMonthly Cost ($)Des. 2Des. 1A B C\nD\nFigure 1: Query performance and operating costs of the same\nworkload on two data infrastructure designs.\n2.1 When Conventional Wisdom Falls Short\nConsider the data processing needs of a movie theater chain. Un-\nder conventional wisdom, they should run an OLTP engine (e.g.,\nAurora) for their transactions and a data warehouse (e.g., Red-\nshift) for their analytics. To show the downsides of this approach,\nwe run a synthetic workload based on a 160 GB version of the\nIMDB dataset [ 63] comprising transactions, repeating dashboard-\ning queries AB, and periodic reporting queries C. We run Aurora\nwith one db.t4g.medium instance and two Redshift dc2.large nodes\n(Design 1). Figure 1 shows the analytical query latencies. We aim\nto keep some queries under 3 s Aand others under 30 s BC.\nAt first glance, Design 1 appears reasonable. However, consider\nan alternative design with just two Aurora db.t4g.medium instances:\na primary and replica (Design 2). We can run a subset of the queries\nABon the Aurora replica and offload the reporting queries C\nonto Athena (a serverless data lake engine). As shown in Figure 1,\nDesign 2 saves 2Ã—on cost D, meets the performance targets, and\neven improves query latency on some queries (up to 48 Ã—Band\n2.1Ã—C). Transaction latency is unaffected on both designs because\nthey run on the unchanged Aurora db.t4g.medium primary instance.\nDesign 2 performs better because some queries Bhave pred-\nicates on indexed columns, which Aurora can leverage. Redshift\ndoes not support indexes and must use table scans. The reporting\nqueries Crun infrequently, once every four hours, so they can be\noffloaded to Athena (a serverless engine) instead of incurring a high\ncost on a provisioned but underutilized Redshift cluster. Athenaâ€™s\nserverless burst capability enables the up to 2 Ã—decrease in query\nlatency C. These queries cannot meet the performance targets on\nAurora; they would run for over 150 seconds each.\nThis example shows that an effective design strongly depends\non the specifics of the workload and engines, rather than high-level\nguiding principles (e.g., run transactions on an OLTP engine and\nanalytics on a data warehouse). Here, the conventional wisdom\ndesign is about twice as expensive and an order of magnitude slower\non some queries than Design 2. An engineer would need an intimate\nunderstanding of the engines and workloads to find such a design,\npossibly spending a lot of time doing so. Moreover, because the\nbest design is workload-dependent, it can change in response to\nworkload shifts, forcing the engineer to redo their work. These\nrepeated design endeavors are unscalable and difficult to get right,\nunderscoring the need for a principled and automated alternative.\n2\n\nRegular Operation\nBlueprint nTransition to \nNew BlueprintIf Decide to \nRe-OptimizeSelect New Blueprint\nSearchBlueprint Optimizer\nBlueprint n + 1Workload Comp. C(B 1, B 2)\nScoring Modelsq1\nq2\nâ‹®Candidate\nScoreFigure 2: BRADâ€™s blueprint planning life cycle.\n2.2 BRAD to the Rescue\nWith BRAD, users no longer manually design and operate their\ninfrastructures. Instead, each virtualized database engine managed\nby BRAD has a user-specified design goal (e.g., minimize cost and\nkeep query latency under 30 seconds), and users simply submit\ntheir queries and transactions directly to BRAD as if it were a single\nengine. BRAD uses this design goal to optimize the infrastructure\nto best run the userâ€™s workloadâ€”what we call blueprint planning .\nIn this paper, we focus on the models, algorithms, and mechanisms\nused in BRADâ€™s blueprint planner (Section 3). That said, virtualiza-\ntion comes with many more challenges. In the remainder of this\nsection, we briefly outline how BRAD tackles them. We leave a\nmore detailed exploration of this topic for future work.\nData consistency and freshness. Since BRAD can choose to back\na (virtual) table with multiple replicas across engines, consistency\nand freshness are natural concerns. In BRAD, (i) a table ğ‘‡will have\nexactly one â€œwriter engineâ€ ğ¸(i.e., all DML statements affecting\nğ‘‡run on the same ğ¸), and (ii) transactions run on a single engine\n(Aurora). BRAD syncs its table replicas (if any) at a user-defined\nfrequency. Analytical queries (i.e., read-only queries not part of a\ntransaction) run against a snapshot, but the snapshot can be stale up\nto the last sync. All transactions always run on the latest snapshot.\nThis approach provides similar freshness to existing solutions [ 55].\nTransformations. Modern data infrastructure designs typically\nuse ELTs [ 7,99], meaning that tables are first replicated into a\ndata warehouse and then transformed inside the warehouse using\nDML statements. BRAD supports this model, as it already syncs\ntable replicas across engines; these transformations would thus\nrun as regular DML statements that modify the logical tables in\nBRAD. Running these transformations on a schedule is orthogonal\nto BRAD and can be handled using an external tool.\nSQL dialects and semantics. Different database engines can have\ndifferent SQL dialects and semantics, meaning the same SQL state-\nment may not be executable on every engine. Currently, we assume\nthat BRAD can detect the subset of its engines that can correctly\nrun a given SQL query; BRAD will ensure it only routes the query\nto those eligible engines (see Section 4.3). SQL dialect translation\ntechniques [ 23] may enable BRAD to expand a queryâ€™s set of eligible\nengines; we leave this to future work.\n3 BRADâ€™S BLUEPRINT PLANNER: KEY IDEAS\nWe now describe the key ideas behind BRADâ€™s blueprint planner.\nWe continue with further implementation details in Section 4.3.1 The Blueprint Planning Life Cycle\nBRAD automatically designs and operates data infrastructures us-\ning a blueprint planning life cycle , which we depict in Figure 2. The\ncore idea is to select the â€œbestâ€ blueprint for the userâ€™s workload, op-\nerate the infrastructure according to the blueprint, and then trigger\nre-optimization if the workload changes (Section 4.6). Concretely\nin BRAD, blueprints are infrastructure plans that contain\nâ€¢The engines to include in the underlying data infrastructure.\nâ€¢The provisioning configuration to use for each engine when\napplicable (e.g., instance type, the number of nodes to use).\nâ€¢The placement of data tables and replicas on the engines.\nâ€¢A policy for routing queries to the engines in the infrastructure.\nBelow,ğµis an example blueprint describing an infrastructure com-\nprising Aurora (provisioned with one db.r6g.xlarge instance) and\nAthena. Table ğ‘‡1is placed on Aurora, and ğ‘‡2is replicated on Aurora\nand Athena. The routing policy consists of concrete query assign-\nments chosen during blueprint optimization (query ğ‘1to Aurora\nandğ‘2to Athena) (see Section 3.3) and an online policy ğ‘ƒ(ğ‘)that\nselects an engine for a given query ğ‘(see Section 3.4).\nğµ=ï£±ï£´ï£´ï£´ï£´ ï£²\nï£´ï£´ï£´ï£´ï£³{Aurora,Athena} Engines\n{(Aurora,db.r6g.xlarge,1)} Provisioning\n{ğ‘‡1â†’Aurora,ğ‘‡2â†’Aurora,ğ‘‡2â†’Athena}Placement\n{ğ‘1â†’Aurora,ğ‘2â†’Athena,ğ‘ƒ(ğ‘)} Routing\nTo automatically find an optimized blueprint, BRAD needs a\nmechanism to (i) quantify the utility of (i.e., assign a â€œscoreâ€ to)\ncandidate blueprints on the userâ€™s workload (Section 3.2), and (ii) to\nsystematically search over the blueprint design space (Section 3.3).\nHere, a workload is a representative (but not necessarily exhaustive)\nlist of expected queries and DML statements along with dataset\nstatistics (e.g., its size). Concretely, BRAD obtains a userâ€™s workload\nby logging their transactions and queries (see Section 4.1).\nScoring a blueprint is challenging because multiple factors in-\nfluence a blueprintâ€™s utility (e.g., performance, cost), and different\nusers may have different design goals (e.g., maximizing perfor-\nmance vs. minimizing cost). Consequently, BRAD assigns vector\nscores to its candidates, which comprise three components:\n(1)Workload Performance. BRAD predicts the run time of the\nqueries and transactions in the workload on the blueprint.\n(2)Operating Cost. The monetary cost of operating the data\ninfrastructure and routing policy specified by the blueprint.\n(3)Transitions. The time and monetary cost of transitioning the\nunderlying infrastructure to the candidate blueprint.\nFor example, a vector score would look like [ğ‘1ğ‘2... ğ‘ğ‘¡ğ‘‡ğ‘ğ‘‡]âŠº\nwhere theğ‘ğ‘–s are predicted query and transaction latencies, ğ‘is\nthe operating cost, and ğ‘¡ğ‘‡andğ‘ğ‘‡are the transition time and cost.\nBRAD uses a set of learned models to assign values to all three\ncomponents, which we discuss next in Section 3.2.\nUsers express their â€œdesign goalsâ€ to BRAD by providing a com-\nparator function that ranks the vector scores (Section 4.5), analo-\ngous to the comparators used in sort routines [ 33]. For example, one\nsuch goal could be to design an infrastructure that minimizes cost\nwhile maintaining a performance constraint (e.g., a latency SLO,\nsee Section 4.5). The comparator would therefore rank blueprints\nby their operating cost while treating blueprints that are predicted\nto not meet the latency constraint as having an infinite cost.\n3\n\n3.2 Blueprint Scoring\nIn the blueprintâ€™s score vector, the main challenge is predicting\nthe performance of the workload on the blueprint. For BRAD, this\nmeans estimating the latencies of the queries in the workload on\neach of BRADâ€™s engines while taking into account their provision-\ning and load. We discuss scoring in more depth in Section 4.2.\n3.2.1 Query Run Times. Prior work has proposed run time predic-\ntion methods for use in query optimization [ 100], workload sched-\nuling [ 109], resource management [ 95], and maintaining SLOs [ 32].\nThese methods require the queryâ€™s physical execution plan as in-\nput. For example, DBMS cost models and traditional predictors use\nhand-derived heuristics to understand the cost of each physical\noperator [ 4,38,65,111]. Advanced methods featurize the physi-\ncal query plans and train deep learning models to predict their\nrun time [50, 70, 71, 73, 100, 114]. BRAD cannot directly use these\nmethods because it cannot always get a physical plan. For example,\nBRAD may need to predict the run time of a query on an engine\nthat is not running or does not have the relevant data loaded (e.g., to\ndecide whether to start Redshift and/or move a table there). BRAD\nmust also account for the effects of provisioning and system load.\nBRAD addresses these challenges using a graph neural network\n(GNN) and two analytical models. We design a new GNN that pre-\ndicts a queryâ€™s run time using the queryâ€™s SQL as input (i.e., relies\nonly on logical features) for an unloaded engine on a fixed provi-\nsioning. Our GNNâ€™s novelty is that it featurizes a query based on its\nSQL text and data properties. In contrast, existing models featurize\nqueries using their physical query plans. We then use two analytical\nmodels, based on Amdahlâ€™s law [ 22] and queuing theory [ 48] to\nadjust this modelâ€™s estimates for different provisionings and system\nloads, respectively. We take this approach because making such\npredictions with a single model is expensive and hard to realize due\nto the need for diverse run time observations across various query\ntypes, provisionings, and system loads. We describe the details of\nour analytical models in Section 4.2.2; they provide an acceptable\naccuracy and enable BRAD to find effective blueprints (Section 5.2).\nGNN model and query featurization. We use a GNN model\nwith a novel query featurization that depends only on logical query\nproperties (e.g., the join template, join/filter selectivity) and dataset\nstatistics (e.g., estimated join selectivity). Our design is based on\nthe key observation that most query optimizers are deterministic:\nthey will choose similar query plans with similar run times for\nqueries with similar features and statistics. Thus, we identify these\nfeatures and then model them with a novel graph structure. As a\nresult, even without physical plans, our model learns the optimizerâ€™s\nbehavior and makes accurate predictions for queries similar to the\ntraining queries in our featurization space (Section 5.3). We use\nthe same approach to predict the amount of data a query scans (to\nestimate Athenaâ€™s query cost). We describe the featurization and\ngraph structure in more detail in Section 4.2.1.\n3.2.2 Model Bootstrapping. BRAD is designed to be gradually de-\nployed onto an existing infrastructure running one or more of our\ncomponent engines. When first deployed, BRAD observes the run-\nning workload and gathers performance data (e.g., query run times)\nfor each engine in a brief â€œbootstrapping phase.â€ BRAD then uses\nthis data to train these aforementioned models. Once complete,BRAD then begins to actively optimize the infrastructure using\nits blueprint planner. Avoiding a bootstrapping phase would re-\nquire having performance models that are fully transferable across\nworkloads and datasets, which we leave to future work.\n3.3 Blueprint Search\nExhaustively searching the entire design space is intractable for\nmost workloads because it is exponentially large with respect to\nthe number of tables and queries. Given this challenge, BRAD must\nuse an efficient search algorithm that finds optimized blueprints\nwithout visiting the entire search space.\nBRAD uses a greedy beam-based search [ 69] over the blueprintâ€™s\nrouting policy (i.e., query-to-engine mapping), which directly im-\npacts the workloadâ€™s performance. Blueprints are optimized for a\nworkload which contains a representative list of expected queries.\nThe idea is to incrementally expand a set of top- ğ‘˜blueprints (the\nâ€œbeamâ€) by examining queries in the workload one-by-one. For each\nblueprint in the current top- ğ‘˜, the planner takes the next query and\nassigns it to each of the three engines, creating three new candidate\nblueprints. It then places tables according to these routing decisions\n(e.g., if a query accessing table ğ‘‡is routed to ğ¸, then a copy of ğ‘‡\nis placed on ğ¸). After each step, the planner only keeps the top- ğ‘˜\nblueprints, and it repeats until all the queries have been assigned.\nBRAD runs this beam search for each provisioning â€œnearâ€ the cur-\nrent one (in computational resources) and returns the best-scoring\nblueprint. BRAD uses a beam of size 100 (i.e., ğ‘˜=100). We analyze\nand discuss our search algorithm in more detail in Section 4.4.\n3.4 Operating Blueprints: Query Routing\nOnce BRAD has chosen a blueprint, the final step is to use it to serve\nthe workload. To route queries, BRAD first consults the query-to-\nengine assignment in its blueprint. If the query is in the assignment,\nBRAD uses this pre-planned routing decision. Otherwise, it uses an\nonline routing policy. The key challenge in designing this policy is\nthat it must make intelligent routing decisions without imposing\nan undue overhead (i.e., doing so within tens of milliseconds). This\nefficiency constraint precludes the use of computationally expen-\nsive models, such as the query run time model we use for blueprint\nscoring. To address this challenge, we make two key observations.\nFirst, in tasks like run time prediction, prior work showed that\nclassification is easier than regression in terms of model efficiency\nand accuracy [ 39,119]. Thus, we can cast this routing problem\nas a classification problem and leverage a lightweight classifier.\nSecond, this online routing policy can be trained during blueprint\nplanning, which runs off of the critical path. This workflow gives\nus the opportunity to bootstrap the online routing policy using a\nmore sophisticated but computationally expensive model, e.g., our\nquery run time model.\nWe leverage these observations to design BRADâ€™s online routing\npolicy. BRADâ€™s online policy is a decision forest that, for a given\nquery, produces a ranking of the engines (most preferred routing\nto least). BRAD routes the query to the highest-ranked engine that\nhas all the tables the query accesses. As input, the forest takes the\nestimated scan cardinality of each table that the query accesses;\nthese cardinalities can be computed using an off-the-shelf cardi-\nnality estimator. The forest is trained as the final step in blueprint\n4\n\noptimization using run times that our query run time model pre-\ndicts (the engine with the lowest predicted run time is the most\npreferred). Inference over the decision forest is fast and does not im-\npose an undue overhead on the queries. We empirically evaluate the\neffectiveness and overhead of our routing policy in Section 5.4. We\ndiscuss additional practical details for query routing in Section 4.3.\n4 BRADâ€™S BLUEPRINT PLANNER: DETAILS\nIn this section we outline the implementation details behind BRAD\nand provide additional details about BRADâ€™s blueprint planner.\n4.1 Realizing the Blueprint Planning Life Cycle\nFigure 3 depicts BRADâ€™s system architecture, which implements\nthe blueprint planning life cycle using two components: (i) a fron-\nt-end server responsible for interfacing with clients and operating\nthe blueprint, and (ii) a blueprint planner that monitors the work-\nload and selects new blueprints when appropriate. We describe the\narchitecture by walking through the blueprint planning life cycle.\nThe life cycle begins at BRADâ€™s front-end server (Figure 3 A).\nUsers submit SQL queries to the server, which get routed to a suit-\nable engine for execution (Section 3.4). Crucially, to keep track of\nthe executing workload, the front end (i) logs the queries it receives\nB, and (ii) collects metrics about the workload (e.g., transaction\nlatency, query latency). The blueprint planner monitors these met-\nrics C, alongside others it retrieves from the underlying engines\n(e.g., CPU utilization) and triggers blueprint optimization when\nthey exceed or fall below specified thresholds D(Section 4.6)\nWhen starting blueprint optimization, BRAD first extracts the\nqueries that ran during its planning window (a sliding window of a\nconfigurable length E) from its workload log. These queries, along\nwith dataset statistics (e.g., the sizes of the tables), are passed to\nthe optimizer and represent the workload that BRAD uses when\nscoring candidate blueprints F. BRADâ€™s optimizer then searches\nover valid blueprints (Section 3.3), scores them (Section 4.2), and\nreturns the best-scoring blueprint G(Section 4.5). The planner then\ntransitions the infrastructure to the chosen blueprint and passes it\nto the front end H. The front end uses this blueprint until the next\none is chosen, completing the blueprint planning life cycle.\n4.2 Additional Blueprint Scoring Details\n4.2.1 Query Run Time and Data Scanned. As discussed in Sec-\ntion 3.2, BRAD uses a graph neural network with a novel query\nfeaturization to predict a queryâ€™s run time and the amount of data it\nscans. We now describe the featurization and model in more detail.\nQuery featurization. As discussed, existing run time predictors\nrequire the queryâ€™s physical execution plan as input, which is not\nalways available in BRADâ€™s setting (Section 3.2). Thus, we design a\nnew graph featurization approach to encode information, such as\nfilters, joins, and group-bys, purely from a queryâ€™s SQL. This logical\nfeaturization differentiates our GNN from existing models [ 50,70,\n71,73,100,114]. Figure 4 shows an example procedure to extract\nsuch a query feature graph. It has five types of nodes, each with\ndistinct node features (shown in Table Din Figure 4) and edges\nrepresenting the dependencies between these nodes. We parse thequeryâ€™s SQL to extract the tables, columns, predicates, and logical\noperations it involves and construct the feature graph in four steps.\nFirst, we extract the tables and columns involved in the query.\nWe show an example table ğ´and its relevant columns ğ´1,ğ´2,ğ´3in\nblue in Figure 4 A. We create a table node for each table and a col-\numn node connecting to a table node for each column (Figure 4 B).\nSecond, we extract the operations on a single table, such as â€œscanâ€,\nâ€œaggregateâ€, and â€œgroup-byâ€, as highlighted in green in Figures 4 A\nand 4 B. Specifically, we create a predicate node for each filter\npredicate and connect the column nodes involved in this predicate\nto it. We extract the features of the predicate nodes using an ap-\nproach similar to recent work [ 50,100]. We connect these predicate\nnodes to their parent operation node. For operations without a\npredicate, such as â€œaggregateâ€, we just connect them to the relevant\ncolumn nodes. Third, we parse the operations involving multiple\ntables, e.g., â€œjoinâ€, as highlighted in red. The children of each join\noperation node are the relevant scan operation nodes and the join\npredicate node extracted the same way as a filter predicate. We list\nthe features for all node types in Figure 4 D. Finally, as highlighted\nin yellow in Figure 4 B, all the operation nodes are connected to\nthe embedding node, which aggregates the overall information.\nThis generic query graph representation does not encode any\ninformation about a physical plan (e.g., join order or physical opera-\ntors). The features we use can all be independently derived without\nrelying on BRADâ€™s underlying engines. We use a value of âˆ’1for\nthe aforementioned node features if the feature is unavailable.\nSelectivity estimates. Our model also uses an estimate of the\nselectivity of each operation node. This is because the selectivity\ninfluences the chosen physical plan, and thus the queryâ€™s run time\nas well. BRAD collects simple statistics about each table (e.g., his-\ntograms) using analysis pass over the underlying data. BRAD then\nuses the Selinger method [ 96] to make these estimates because of\nits simplicity and negligible overhead. The choice of estimator is or-\nthogonal to our model; other methods are also applicable [ 81,113].\nGraph neural network. Inspired by the zero-shot run time predic-\ntor [50], we use a graph neural network to model interactions be-\ntween nodes and propagate information through the feature graph.\nWe construct one multi-layer perception (MLP) node encoder for\neach level that embeds the node features into a fixed-length vec-\ntor. Then, we create another MLP for message passing through\nedges [ 42]. At each internal node, we sum its childrenâ€™s embedded\nvectors, concatenate them with its own vector, and feed the result-\ning vector to the MLP to get a new vector. The message passing\nstops at the last level when the â€œembedding nodeâ€ has aggregated\nall of the queryâ€™s information into a single vector. This vector is\nfed to two MLPs (Figure 4 C) to predict the queryâ€™s run time and\namount of data scanned. The entire model is trained end-to-end.\nDiscussion. By using logical features and selectivity estimates, our\nmodel implicitly learns a query optimizerâ€™s behavior and makes\naccurate predictions for queries similar to our training queries (see\nSection 5.3). Our model may not work well in some cases where the\ntesting queries are significantly different from our training queries.\nFor example, suppose the model was only trained on short running\nqueries joining small tables and the user then submits a long run-\nning query joining large tables. Then, the engine could possibly\nchoose a join operator/order that it has never chosen before, and\n5\n\nğŸ‘¨ğŸ’»\nğŸ‘©ğŸ’»\nğŸ§‘ğŸ’»\nPlanning WindowBlueprintFront End Server\nEngine Eligibility\nQuery Routing \nPolicyTable \nPlacement\nWorkloadPlanning TriggersBlueprint Scoring\nProvisioning and \nWorkload Cost\nProvisioning \nTransition TimeTable Movement \nTime ModelProvisioning and \nSystem Load ModelTransaction \nLatency Model\nPerformance Transition CostQuery Run \nTime and Data \nScanned Modelq1\nq2\nâ‹®\nqn\nc\ntc\ntt\nScoreBlueprint \nCandidateBlueprint Search Algorithm Blueprint Comparator C(B 1, B 2)Blueprint Optimizer\nNewly Chosen \nBlueprintSELECT â€¦ \nFROM â€¦\nUPDATE â€¦ \nSET â€¦SELECT â€¦ \nFROM â€¦\nINSERT \nINTO â€¦\nSELECT â€¦ \nFROM â€¦\nâ€¦SELECT â€¦ \nFROM â€¦\nSELECT â€¦ \nFROM â€¦BRAD\nTransition Orchestrator\nWorkload LogTriggers\nAurora\n Redshift\n AthenaProvisioningBlueprint Planner\nNew BlueprintsMetrics MonitorA\nBCD\nE\nFGH\nJ\nFigure 3: A detailed view of BRADâ€™s architecture and its end-to-end blueprint planning life cycle.\nTable ATable CTable BA2A3C1C5B3=B1=<LIKEEmbeddingAggA11Scan AScan BScan CSELECT   SUM(A1)       FROM A, B, C WHERE   A3 = B1       AND   B3 = B1       AND   A2 < 5       AND   C5 LIKE                    â€˜%SAM%â€™;234Join ABJoin BC\n12.31 s346 MBTable featuresnum_rows, num_columns, storage_size, storage_formatAttribute featureswidth, data_Type, NDV , null_Frac, is_PK, is_FK,is_partition_key, is_sorted, index_typePredicate featuresoperator_type (e.g. AND, OR, <, =), literal_complexityOperation featurestype (e.g. scan, join), width, estimated_selectivity, join_type (e.g. inner, left), agg_type (e.g. SUM, AVG)Embedding featuresnum_tables, num_columns, num_predicates, num_operationsABCDMessage passing\nFinal predictionEmbedding  1 x 128128 x 3232 x 2MLP1 x 2\nFigure 4: The query featurization used by our model, which\npredicts a queryâ€™s run time and the amount of data it scans.\nour model (and possibly all existing run time models) would be\nunlikely to correctly predict that queryâ€™s run time. However, recent\nwork shows that in practical workload traces, a large portion of\nqueries are repeating and â€œsimilarâ€ to prior queries [ 112]. Thus,\nencountering a query that greatly impacts the selected plan in a\nway that was not captured in the training data should be rare.\n4.2.2 Provisioning and System Load. Next, we describe the two\nanalytical models we use to adjust our GNNâ€™s run time estimates\nfor different provisionings and system load.\nA queryâ€™s run time consists of two components: the time spent\nrunning and the time it queues due to system load. Thus, BRAD\nmodels a queryâ€™s complete run time ğ‘…asğ‘…(ğº,ğœŒ)=ğ‘ƒ(ğº)+ğ‘Š(ğœŒ),\nwhereğºis the run time given by our GNN, ğ‘ƒ(Â·)is a model that\naccounts for the engineâ€™s provisioning, and ğ‘Š(Â·)is the time spent\nqueuing depending on the systemâ€™s utilization (load) ğœŒ.\nCompute resources. BRAD uses ğ‘ƒ(ğº)=(ğ‘1(ğ‘/ğ‘‘)+ğ‘2)ğº, which\nwe derive from Amdahlâ€™s law [ 22]. We model the queryâ€™s run time\nas having two parts: (i) one that will decrease (or increase) if the\nengineâ€™s provisioning is changed to have more (or less) compute\nresources, and (ii) a fixed part that will not change even with more\nresources. Here ğ‘1ğºrepresents part (i) and is multiplied by ğ‘/ğ‘‘,\nwhich is the ratio between the resources available on the â€œbaseâ€\nand â€œdestinationâ€ provisionings. The base is the provisioning onwhich the graph neural network was trained. The destination is the\nengineâ€™s provisioning in the candidate blueprint. The ğ‘2ğºterm is\npart (ii). BRAD uses the total number of vCPUs in the provisioning\nto represent the available compute. We learn one set of constants\nğ‘1andğ‘2for each engine (Aurora and Redshift) using least squares\nlinear regression [ 26] on the query workload. This approach as-\nsumes that provisioning changes do not cause significant query\nplan changes that would affect the run time. Empirically, we find\nthis model to be sufficient for blueprint planning (Section 5.2).\nSystem load. BRAD models ğ‘Š(ğœŒğ‘Ÿ)using queuing theory, approx-\nimating an engine as an M/M/1 system [ 48]. We useğ‘Š(ğœŒğ‘Ÿ)=\nâˆ’ğ¾(1âˆ’ğœŒğ‘Ÿ)âˆ’1log\u0000ğœŒâˆ’1ğ‘Ÿ(1âˆ’ğ‘)\u0001which models the ğ‘-th percentile\nqueuing time on a system with utilization ğœŒğ‘Ÿ[2].ğ¾represents the\nmean processing time, which we estimate as the average ğ‘ƒ(ğº)for\nall queries assigned to the engine. We approximate an engineâ€™s\nutilization using its measured CPU utilization. In our experiments,\nBRAD optimizes for a p90 latency constraint, so we use ğ‘=0.9.\nWe use this model as it provides a simple closed-form expression\nfor wait time. Not all engines are M/M/1 systems; for example, the\nquery arrival distribution may not be exponential and/or the engine\nmay process queries in parallel. However, we empirically find that\nthis simple model is sufficient for blueprint planning (Section 5.2).\nAdjustingğœŒğ‘Ÿ.ğ‘Š(ğœŒğ‘Ÿ)relies on a representative ğœŒğ‘Ÿ. BRAD cannot\ndirectly use CPU utilization because the candidate blueprint may\nuse a different query routing as the current blueprint, thereby im-\nposing different loads. Instead, we assume that a queryâ€™s â€œloadâ€ is\nproportional to its run time. BRAD thus scales its observed CPU uti-\nlization by a factor: the sum of the run times of the queries routed to\nthe engine in the candidate blueprint divided by the sum of the run\ntimes that actually ran on the engine in the last planning window.\nIf no queries ran on the engine in the previous planning window\n(e.g., the engine was paused), BRAD scales the queryâ€™s predicted\nrun times to a load value using a learned proportionality constant.\n4.2.3 Transaction Latency. BRAD estimates transactional latency\non a candidate blueprint to ensure it provisions Aurora appropri-\nately for the transactional load it experiences. In general, estimating\na transactionâ€™s run time is a hard problem due to the many factors\n6\n\nthat can influence its latency (e.g., lock contention, buffer pool state,\netc.) [ 79]. We make a simplifying assumption that the transactional\nworkload is uncontended and consists of TPC-C-like [ 101] indexed\npoint reads and writes made interactively over the network. For this\nsetting, we use an analytical function that models a transactionâ€™s\nlatency as a function of system utilization: ğ‘…(ğœŒğ‘¡)=ğ‘/(ğ‘€âˆ’ğœŒğ‘¡)+ğ‘.\nHere,ğ‘…(ğœŒğ‘¡)is the overall transactional latency. ğ‘,ğ‘, andğ‘€are\nworkload-specific learned constants. ğœŒğ‘¡âˆˆ[0,1]is the system uti-\nlization; we require that ğ‘€>ğœŒğ‘¡. We use CPU utilization as a simple\nproxy metric for ğœŒğ‘¡. This model captures that transaction latency in-\ncreases rapidly as ğœŒğ‘¡approachesğ‘€, like it would on an overloaded\nsystem [ 48]. We derive this model empirically; it is loosely based\non the queuing theory equations for an M/M/1 system [48].\nAdjustingğœŒt.The candidate blueprintâ€™s ğœŒğ‘¡may not be the same as\nthe measured ğœŒğ‘¡on the current blueprint (e.g., due to a changed pro-\nvisioning and/or query routing). BRAD applies two scaling factors\nto compensate. First, it multiplies ğœŒğ‘¡by the ratio of vCPUs between\nthe candidate blueprintâ€™s provisioning and the current blueprintâ€™s\nprovisioning. Second, it applies the same query load scaling factor\nmentioned in Section 4.2.2 to account for query movement.\n4.2.4 Operating Cost and Transitions. A blueprintâ€™s cost comprises\nprovisioning costs, Athena query costs, and storage costs. For\nAurora and Redshift, BRAD uses AWSâ€™ on-demand instance pric-\ning [ 13,16]. Currently BRAD only considers Aurora I/O optimized\ninstances, which do not bill I/O usage [ 8]. Since Athena bills by the\namount of data scanned, BRAD uses its data scanned predictions\n(Section 4.2.1) along with Athenaâ€™s scan pricing [ 11]. For storage\ncosts, BRAD models a tableâ€™s size as ğ‘˜|ğ‘‡|where|ğ‘‡|is the number\nof rows in the table and ğ‘˜is an engine and table-specific constant.\nTo compare blueprints, BRAD normalizes costs to be in $/hour.\nTable movement. BRAD currently moves tables via S3 (i.e., export\nto S3 and import from S3). It estimates the movement time as ğ‘†/ğ‘˜ğ‘’+\nğ‘†/ğ‘˜ğ‘–, whereğ‘†is the physical size of the table and ğ‘˜ğ‘’andğ‘˜ğ‘–are\nempirically measured export and import rates. These rates are\nengine-specific but independent of the engine provisioning, which\nwe confirmed empirically. AWS does not charge for S3 transfers\nbetween AWS services, so BRAD does not incur movement costs.\nAurora provisioning time. BRAD estimates Auroraâ€™s provision-\ning time as the number of instance changes multiplied by a fixed\namount of time (we empirically measured 5 minutes). Removing\nreplicas is considered to take no time since BRAD does not need to\nwait for the completion of removal to start using the next blueprint.\nRedshift provisioning time. The time it takes to complete a\nRedshift provisioning change depends on whether it can be done\nusing an elastic resize or not [ 9]. For elastic resizes, BRAD uses\nAWSâ€™ published estimate of 15 minutes [ 9]. BRAD estimates classic\nresizes to take|ğ‘…|/ğ‘˜where|ğ‘…|is the physical size of the data in the\nRedshift cluster. We empirically observed ğ‘˜to be approximately 18\nmegabytes per second. Pausing Redshift is also considered to take\nno time for the same reason as Aurora replica removals.\n4.3 Additional Query Routing Considerations\nUpon receiving a query, BRAD selects a suitable engine in two\nsteps: (i) determine the set of engines that are able to execute the\nquery, and then (ii) select the most suitable (e.g., best performing)engine from this set. In this section, we describe step (i). For (ii),\nBRAD uses the online routing policy described in Section 3.4.\nIn BRAD, an engineâ€™s eligibility to run a query depends on the\ntable placement and its functionality support . Table placement is the\nset of engines that hold a copy of a table and is governed by the\nblueprint; BRAD currently only routes a query to an engine if it\nhas a copy of every table the query accesses. BRAD parses the SQL\nquery to extract the tables it references and compares them against\nthe blueprintâ€™s table placement. During blueprint planning, BRAD\nensures that all tables are placed together on at least one engine to\nso that it can always run a query that joins any subset of tables.\nA query may also use specialized functionality only available\non a subset of the engines (e.g., vector similarity search [ 64,90]).\nBy taking functionality into account, BRAD ensures that it only\nselects engines that can run the query. Automatically determin-\ning the â€œspecialized functionalityâ€ that a query uses is something\nwe leave to future work. BRAD currently uses keyword matching\nagainst pre-specified keyword lists to determine if a query uses\nsuch functionality (e.g., the presence of the <=>operator would\nimply that the query uses vector similarity search).\n4.4 Additional Blueprint Search Details\nAlgorithm 1 contains the pseudocode for BRADâ€™s greedy beam\nblueprint search algorithm, which we outline in Section 3.3. The\nintuition for using a top- ğ‘˜beam search instead of a naÃ¯ve greedy\nsearch lies in the nature of the search space. At the beginning of the\nsearch, assigning queries to some engines may be better (e.g., prefer\nAthena, which is pay-per-query, instead of Redshift where you pay\nfor provisioning). But after assigning more queries, this trade-off\nchanges (e.g., there are enough queries to justify running Redshift).\nKeeping a set of promising candidates helps BRAD balance these\nchanging trade-offs. We search over nearby provisionings because\nBRAD handles gradual workload changes; the next best provision-\ning is likely to be near the current provisioning.\nDiscussion. Beam search works well empirically in our setting\nfor two reasons. First, BRAD uses a large beam ( ğ‘˜=100), which\nhelps prevent some promising candidates from being eliminated\ntoo early. Second, the queries in our workload have a skewed arrival\nfrequency (i.e., some queries arrive more frequently than others).\nThis property was also observed by our industrial partners in their\nreal-world workloads [ 112]. As a result, BRAD processes queries\nin decreasing order of arrival frequency. Along with using a large\nbeam, this processing strategy makes it more likely for â€œimportantâ€\n(i.e., frequently occurring) queries to be assigned to the best engine.\nAnalysis. Letğ‘šbe the number of engines considered, ğ‘be the\nnumber of queries in the workload, and ğ‘be the number of distinct\nprovisionings considered. This algorithm considers ğ‘‚(ğ‘˜ğ‘šğ‘ğ‘)can-\ndidate blueprints. Currently, BRAD has ğ‘š=3and usesğ‘˜=100. We\nevaluate our algorithm empirically in Section 5.5.\n4.5 Blueprint Comparator: Minimizing Cost\nAs discussed in Section 3.1, end-users need to define a comparator\nfunction, which imposes an ordering on blueprint vector scores.\nThis comparator is how users convey their infrastructure design\ngoals to BRAD. A common goal is to minimize an infrastructureâ€™s\noperating costs while maintaining a service level objective (SLO)\n7\n\n(e.g., p90 query latency should be under 30 seconds). We use this\ndesign goal when evaluating BRAD in Section 5. We now describe\nhow this goal is implemented as a comparator.\nGiven two blueprints ( ğµ1,ğµ2), the general idea is to map their\nvector scores to scalar costs ( ğ‘Š1,ğ‘Š2); the candidate with the lower\ncost is considered better. A simple mapping would be to use the\nblueprintâ€™s operating cost when the predicted query latency falls\nunder the desired latency constraint and an infinite cost otherwise\n(to indicate infeasibility). However, this mapping does not consider\nthe time and cost of transitioning to the candidate. Instead, our\napproach is to weigh the cost of operating each blueprint using the\ntransition time ğ‘‡ğ‘‡and a user-defined â€œbenefit periodâ€ ğ‘‡ğµ. This pe-\nriod represents how long the user expects the workload to â€œbenefitâ€\nfrom the new blueprint. Concretely, we use\nğ‘Š=ğ‘ƒğ›¾ğ¶0ğ‘‡ğ‘‡+ğ¶ğ‘‡+ğ¶ğ‘‡ğµğ‘ƒ=1+max(ğ‘¡/ğ‘¡SLO,ğ‘/ğ‘SLO)\nğ¶0represents the current blueprintâ€™s operating cost, ğ¶is the candi-\ndate blueprintâ€™s predicted operating cost and ğ¶ğ‘‡is the transition\ncost.ğ‘ƒâˆˆ[1,âˆ)is a penalty multiplier that grows as the current\nblueprint approaches and exceeds the performance constraints (e.g.,\nbecause the workload changes). ğ›¾is a user-chosen weight (we use\nğ›¾=2).ğ‘¡andğ‘represent the transaction and analytical latency\nmeasured on the current blueprint; ğ‘¡SLOandğ‘SLOrepresent the\nuser-specified performance constraints for these values. Users can\ndeclare multiple such constraints (e.g., for different queries).\nWhen the current blueprint exceeds the performance constraints,\nthe first term in the equation on the left will dominate. Thus BRAD\nwill prefer candidate blueprints that are faster to transition to. Oth-\nerwise, BRAD weighs the operating costs by the time spent transi-\ntioning versus running the new blueprint. This means BRAD will\nstill consider blueprints requiring very expensive transitions (high\nğ‘‡ğ‘‡) but will only select them if their benefit is large enough (low ğ¶\nduringğ‘‡ğµ). If a candidate blueprint has a predicted transactional\nor analytical latency greater than ğ‘¡SLOorğ‘SLO, we just assign an\ninfinite cost. If all of the candidates are infeasible, BRAD will ask\nthe user to change their constraints.\n4.6 Triggering Blueprint Optimization\nBRAD periodically checks a set of triggers to decide when to initiate\nblueprint optimization. BRAD initiates blueprint optimization if\none of them fires. Concretely, BRAD uses the following triggers:\nâ€¢Aurora / Redshift CPU utilization. BRAD triggers blueprint\noptimization if they consistently violate preset thresholds (e.g.,\nexceeding 85% or falling below 15% for 10 minutes or more).\nâ€¢Transaction and query latency. When optimizing for cost\nunder a performance SLO, BRAD will trigger re-optimization if\nthese latencies consistently exceed the userâ€™s SLOs.\nâ€¢Recent provisioning change. If BRAD selects a new blueprint\nwith a provisioning change, it will trigger re-optimization after\na fixed period of time to ensure performance is as expected. This\nis because BRAD makes conservative provisioning decisions to\navoid selecting blueprints that will violate the userâ€™s SLOs. Re-\noptimizing after the new blueprint takes effect gives BRAD an\nopportunity to revisit its choice after observing the workload on\nthe new blueprint.Algorithm 1 BRADâ€™s greedy beam blueprint search algorithm.\nInput:ğµ0: Current blueprint, ğ‘Š: Workload,\nScore(Â·,Â·,Â·): Blueprint scoring function\nOutput:ğµâˆ—: Best found blueprint\nfunction DoSearch (ğ‘ƒ: Provisioning)\nSort queries in ğ‘Šin decreasing order of arrival probability\nand then largest predicted speedup across engines\nâŠ²Initial blueprint with provisioning ğ‘ƒand no routed queries âŠ³\nğ‘‡â†[(ğµ(ğ‘ƒ,âˆ…),Score(ğµ(ğ‘ƒ,âˆ…),ğµ0,ğ‘Š))]\nfor all queriesğ‘inğ‘Šdo\nğ‘‡â€²â†[ ]\nfor all blueprintsğµinTdo\nfor all enginesğ‘’in{Aurora,Athena,Redshift}do\nğµâ€²â†(ğµâˆª{ğ‘â†’ğ‘’}) âŠ²Route query ğ‘toğ‘’inğµâ€²\nifğµâ€²is valid then\nğ‘‡â€²â†ğ‘‡â€²add(ğµâ€²,Score(ğµâ€²,ğµ0,ğ‘Š))\nâŠ²Note thatğ‘‡â€²is implemented as a top- ğ‘˜heap. âŠ³\nğ‘‡â†ğ‘‡â€²truncated to the top- ğ‘˜candidates\nreturn Best candidate in ğ‘‡\nğµâˆ—â†None\nfor all provisionings ğ‘ƒnear the provisioning in ğµ0do\nğµâ†DoSearch(ğ‘ƒ)\nifğµis better than ğµâˆ—then\nğµâˆ—â†ğµ\noutputğµâˆ—\n4.7 Discussion\nBlueprint planning practicality. Our blueprint planning frame-\nwork has three practical benefits. First, blueprints and their scores\narehuman-interpretable , making it easy for data engineers to in-\nspect BRAD-chosen designs. Second, blueprints provide a useful\nabstraction for realizing cloud infrastructure designs. Conceptually,\nthey can be â€œcompiled downâ€ into infrastructure-as-code manifests\n(e.g., CloudFormation [ 18]) for deployment. Finally, blueprints are\ngeneralizable to other cloud infrastructure design problems that\ninvolve cost/performance-based resource provisioning, task sched-\nuling, and adaptation under changing conditions. Some example\nuse cases include resource configuration selection for Ray [ 77] pro-\ngrams, designing long-lived infrastructures used by Sky intercloud\nbrokers [30], or assembling a model serving service [93].\nAdding engines to BRAD. BRAD can support additional engines\nbeyond Aurora, Redshift, and Athena. For an engine to be included\nin BRAD, it must (i) support relational data, (ii) have a SQL-based\nquery interface, (iii) expose system metrics (e.g., CPU utilization),\n(iv) use a deterministic query planner, and (v) have deterministic\noperational costs. Practically, the engine should also have manage-\nment APIs that allow BRAD to programmatically alter its allocated\nresources (i.e., provisioning) to deploy blueprints. By (iv), we mean\nthat the query planner must pick the same physical plan for the\nsame query if it has the same dataset statistics (e.g., estimated scan\nselectivity) (see Section 4.2.1). Finally by (v), we mean that the cost\nof running the engine in the cloud must be a deterministic function\nof the engineâ€™s physical configuration (e.g., its provisioning and the\nsize of its data) and the userâ€™s workload. For example, the engineâ€™s\n8\n\noperating cost cannot depend on external factors that BRAD cannot\ndirectly observe (e.g., resource demands from other cloud users).\n5 EVALUATION\nIn our evaluation, we seek to answer the following questions:\nâ€¢How effective is BRAD at optimizing a data infrastructure for cost\nwhen compared to serverless autoscaling systems? (Section 5.2)\nâ€¢How accurate are the models that BRAD uses to score its candi-\ndate blueprints and how well do they generalize? (Section 5.3)\nâ€¢How effective is BRADâ€™s query routing and how much overhead\ndoes BRAD add to query execution? (Section 5.4)\nâ€¢How effective is BRADâ€™s blueprint search algorithm? (Section 5.5)\nâ€¢How sensitive is BRADâ€™s blueprint planner to model errors?\n(Section 5.6)\nAcross five workload scenarios, we find that BRAD selects designs\nthat meet performance targets while outperforming a serverless\nAurora and Redshift infrastructure and a serverless HTAP system\n(where comparable) on cost by up to 13 Ã—and 4.6Ã—respectively.\nOverall implementation. We implemented BRAD in Python us-\ning approximately 30k lines of code. Although BRAD currently uses\nAWS services, the concepts underlying blueprint planning and our\nscoring models are general and applicable to other cloud providers.\n5.1 Workload and Experimental Setup\nWe evaluate BRAD on a new workload that models the data process-\ning needs of a fictitious movie theater company called QuickFlix .\nWhy create a new workload? BRAD automates the design of\nmulti-engine cloud data infrastructures serving diverse transac-\ntional and analytical workloads. Thus, we need a realistic and di-\nverse workload that warrants multiple specialized engines. To our\nknowledge, no such public workloads exist. The TPC [ 102,103] and\nHATtrick benchmarks [ 76] are entirely synthetic. IMDB JOB [ 63]\nand STATS CEB [ 47] use real-world datasets and queries, but only\ncontain OLAP queries as they are for evaluating query optimizers.\nSnowset [ 108] has statistics about real OLAP workloads, but no data\nor queries. Our workload addresses these limitations: it (i) contains\ntransactions and diverse analytical queries, (ii) adapts a real-world\ndataset, and (iii) mimics Snowset statistics where possible.\nDataset. We use an adapted version of the IMDB dataset [ 63],\nwhich is based on real-world data. As the original IMDB dataset is\nsmall (3 GB), we create a larger dataset by replicating each tuple\nin the datasetâ€™s major tables 30 times. Then, we assign new values\nfor each replicated primary key and re-assign these values to their\ncorresponding foreign keys. This approach preserves the datasetâ€™s\nattribute correlations, skew, and join-key distributions. We addition-\nally add three synthetic tables representing movie theaters, movie\nshowings, and ticket orders to capture the companyâ€™s transactional\nneeds. The final uncompressed dataset is 160 GB.\nAnalytical queries. Our workload consists of two classes of an-\nalytical queries: (i) recurring queries (e.g., used for QuickFlixâ€™s\ndashboards and interactive internal tools), and (ii) complex ad-hoc\nanalytical queries (e.g., representing exploratory data analysis). The\nrecurring queries consist of single table scans and two table inner\njoins, both with predicates. The complex ad-hoc queries are ran-\ndomly generated to resemble the IMDB JOB queries. They spanhundreds of distinct templates that join 4 to 15 tables with complex\nfilter predicates. Of the unique queries in our workload, 80% are\nrecurring and 20% are ad-hoc; we chose this split to match what\nour industry partners have observed in their production workloads.\nTransactions. We use 3 transaction types: (i) Purchase , (ii)Add-\nShowing , and (iii) UpdateMovie .Purchase looks up a theater,\nselects a showing, inserts a ticket order, and updates the showingâ€™s\nseat count. AddShowing looks up a theater and movie and inserts a\nnew showing record. UpdateMovie selects a movie from the â€œtitleâ€\ntable and edits the corresponding note column in the â€œmovie_infoâ€\nand â€œaka_titleâ€ tables. We run these transactions with a breakdown\nof 70% Purchase , 20% AddShowing , and 10% UpdateMovie .\nBaselines. We compare BRAD against (i) an infrastructure using\nserverless Aurora for all transactions and serverless Redshift for\nanalytics, and (ii) System H, a popular open-source serverless HTAP\ndatabase. Note that these comparisons are not perfectly fair as these\nsystems provide different guarantees. We select them because they\nrepresent existing industry-standard infrastructure solutions that\nprovide the same hands-off autoscaling experience.\nMetrics. We record three metrics: (i) transaction latency, (ii) analyt-\nical query latency, and (iii) monthly operating cost. In our experi-\nments, BRAD optimizes a data infrastructure to minimize operating\ncost while ensuring that p90 transaction latency remains under\n30 milliseconds and p90 query latency remains under 30 seconds.\nOperating cost calculations. We compute BRADâ€™s operating cost\nusing the on-demand instance hourly cost scaled to 30 days. For\nqueries running on Athena, we compute their cost using the re-\nported bytes scanned. We project this value into a monthly cost by\nassuming that the query repeats at the same observed rate. We in-\nclude storage costs for the tables placed on Aurora and Athena (S3).\nFor serverless Aurora and Redshift, we use the ACU and RPU values\nreported by AWS during the workload and scale them to monthly\ncosts. For System H, we use the cost reported by the vendor.\nConsidered instance types. For Aurora, BRAD currently only\nconsiders Graviton-based instances [ 6] and I/O optimized cluster\nconfigurations (i.e., I/O costs are included in the hourly provisioning\ncost) [ 8]. We leave the consideration of different instance hardware\ntypes (e.g., r6g vs. r6i instances) to future work. For Redshift, BRAD\nconsiders the dc2 and ra3 family of instance types.\n5.2 Optimizing a Data Infrastructure\nWe have BRAD optimize a data infrastructure for cost under a\nperformance constraint in five scenarios faced by QuickFlix:\n(1) Scaling down an over-provisioned infrastructure.\n(2) Scaling engines due to increased load.\n(3) Maintaining support for specialized functionality.\n(4) Adjusting to user-changed constraints.\n(5) Workload intensity variations during a day.\n5.2.1 Scaling Down an Over-Provisioned Infrastructure. QuickFlix\nhas been struggling with their data infrastructure. After learning\nabout BRAD, they adopt it to free up their data engineers. QuickFlix\ndeploys BRAD on their infrastructure, consisting of two Aurora\ndb.r6g.xlarge instances (primary and read replica) and two dc2.large\nRedshift nodes. Following conventional wisdom, they use Redshift\n9\n\n2050\n0 20 40 60 80030Transaction\nLatency (ms)BRAD Servl. (Aurora & Redshift) System H\n0 20 40 60 803050Analytics\nLatency (s)\n0 20 40 60 80\nTime Elapsed (minutes)01000Monthly\nCost ($)2879.28A\nBC\nDEF\nFigure 5: BRAD reduces cost while maintaining p90 latency\nconstraints (shaded region). The dotted (solid) vertical lines\nindicate when a new blueprint is chosen (takes effect).\nfor analytical queries and Aurora for transactions. Figure 5 shows\nworkload performance and the monthly operating cost over time.\nThe shaded area indicates QuickFlixâ€™s performance constraints:\n30 ms p90 transaction latency and 30 s p90 analytics latency.\nSoon after starting, BRAD triggers blueprint optimization A\nbecause it detects a low Redshift CPU utilization. BRAD removes\nthe Aurora read replica, shifts the entire analytical workload onto\nAurora, and pauses Redshift. BRAD makes these changes to reduce\ncost B, as it correctly predicts that Aurora is sufficient to handle\nthe workload. After observing the workload on this new blueprint,\nBRAD then correctly predicts that Aurora can support the workload\nwith a smaller (cheaper) instance type and thus downscales Aurora\nto two db.t4g.medium instances to further reduce cost D. The\nmomentary spike in p90 transactional latency is due to the Aurora\nprimary failover that occurs when changing instance types C. The\nchosen blueprint meets QuickFlixâ€™s performance constraints EF.\nFrom these results, we draw the following two conclusions.\nBRAD reduces operating cost by 6.0 Ã—over its starting pro-\nvisioning and by 4.6 Ã—over System H, the next best baseline.\nServerless Aurora and Redshift is 13 Ã—more expensive than BRADâ€™s\nbecause serverless Redshift has a large minimum size, making it\ncost-ineffective on this workload. Although System H is only 4.6 Ã—\nmore expensive than BRAD, its p90 transaction latency is nearly\n100Ã—higher than the other baseline. We hypothesize that this is\ndue to System Hâ€™s internal replication on writes.\nBRAD shifts workloads across engines to reduce cost, dif-\nferentiating it from static multi-engine autoscaling infras-\ntructures. BRAD correctly predicts that Aurora can support the\nanalytical workload, enabling it to pause Redshift to reduce cost.\nThis decision would never be considered in static autoscaling in-\nfrastructures, such as our serverless Aurora and Redshift baseline,\nsince they only scale to respond to system load while keeping the\nworkload assignment fixed (i.e., analytics always run on Redshift).\n5.2.2 Scaling Engines Due to Increased Load. As QuickFlix grows,\ntheir transactional load increases. Figure 6 shows how BRAD han-\ndles this change; we plot the same metrics as before and include the\n2050\n0 10 20 30 40 5003090Transaction\nLatency (ms)BRAD Servl. (Aurora & Redshift) System H\n0 10 20 30 40 50050Analytics\nLatency (s)\n0 10 20 30 40 5002500Monthly\nCost ($)\n0 10 20 30 40 50\nTime Elapsed (minutes)010Txn.\nClientsAB C\nDE\nF\nFigure 6: As transactional load increases, BRAD switches to\na larger Aurora instance and also removes the read replica.\nnumber of transactional clients over time A. We begin with the\nsame blueprint as the end of the previous scenario. After a few min-\nutes, BRAD notices that the transaction p90 latency is exceeding\nQuickFlixâ€™s 30 ms ceiling Band triggers blueprint optimization.\nBRAD chooses to scale up Aurora to a single db.r6g.xlarge instance\nas it correctly predicts that a single Aurora instance can support\nboth the increased transactional load and the running analytical\nqueries. The spike in p90 transactional latency Cis when the\nAurora primary failover occurs. On the new blueprint, the transac-\ntional p90 latency falls under the latency ceiling E; the analytical\nqueries also continue to complete under the 30 s ceiling F. Again,\nthe increased System H analytics latency might be due to a combi-\nnation of its internal physical autoscaling and storage writes.\nThis shows that BRAD reacts to transactional load to main-\ntain latency constraints. The blueprint that BRAD selects is 2.6 Ã—\ncheaper than the serverless Aurora and Redshift baseline Dbut\nup to 1.1Ã—more expensive than System H. Serverless Aurora and\nRedshift is more expensive due to Redshiftâ€™s large minimum size.\n5.2.3 Maintaining Support for Specialized Functionality. To increase\nengagement, QuickFlix decides to deploy a new feature that recom-\nmends movies similar to the ones shown in their theaters. To make\nrecommendations, they use vector similarity search [64,90] queries\nthat find movies with title embeddings that are closest to a given\nmovieâ€™s title embedding. QuickFlix deploys this feature on their\nexisting infrastructure; since similarity search is only supported on\nAurora, they place the relevant tables on Aurora and run all other\nqueries that access these tables on Aurora as well. They use two\nAurora db.r6g.2xlarge instances and two dc2.large Redshift nodes.\nFigure 7 shows performance and infrastructure cost over time.\nThe dashed lines are from before BRAD deploys its first optimized\nblueprint. Crucially, System H cannot run this workload because\nit does not support vector similarity search A. BRAD notices that\nthe analytical p90 latency exceeds QuickFlixâ€™s constraint of 30 s\nBand initiates blueprint optimization. BRAD selects a blueprint\nthat shifts the non-vector similarity search queries onto Redshift\n10\n\n0 20 40 60 80 100 12003060Transaction\nLatency (ms)BRAD Servl. (Aurora & Redshift) System H\n0 20 40 60 80 100 1200200Analytics\nLatency (s)\n0 20 40 60 80 100 120\nTime Elapsed (minutes)05000Monthly\nCost ($)A\nBC\nDE\nF\nFigure 7: BRAD runs vector similarity search on Aurora and\nshifts other queries to Redshift for performance. System H\ndoes not support vector similarity search.\n(replicating the tables they access onto Redshift) while keeping the\nvector similarity search queries on Aurora. It also correctly predicts\nthat it can downscale Aurora (to a db.r6g.xlarge instance) to save\ncost, as much of Auroraâ€™s former query load was moved onto Red-\nshift. After making this change, the workloadâ€™s performance falls\nwithin the userâ€™s performance constraints DE. The momentary\nspike in analytics latency at the 40 minute mark Cis due to a cold\nRedshift cache when BRAD first moves queries onto Redshift. The\nserverless Aurora and Redshift design is 2.4 Ã—more expensive F\nbecause of the large Redshift minimum size and because Aurora\nhas scaled up to support the new similarity search queries.\nThis scenario shows how BRAD is fundamentally different\nfrom single-system (e.g., HTAP) solutions like System H.\nWhen using a single system to run a diverse data workload, you\ncan always run into situations where you want to use a feature that\ndoes not exist on your system of choice. In contrast, in the BRAD\narchitecture, one can (in concept) always incorporate a system with\nthe required functionality into the underlying infrastructure.\n5.2.4 Adjusting to Changed Constraints. As QuickFlixâ€™s business\nchanges, they revise their performance constraints; Figure 8 shows\nhow BRAD adapts to their new needs. QuickFlix initially uses a\np90 transaction and query SLO of 40 ms and 40 s respectively A\nB. BRAD starts with one db.r6g.xlarge Aurora instance and two\nRedshift dc2.large nodes. Later, QuickFlix lowers their transaction\nand query SLOs to 20 ms and 20 s respectively (the change happens\nat the dashed line in Figure 8 C). This SLO change causes the trans-\naction latency to exceed QuickFlixâ€™s constraints. Thus, BRAD scales\nup Aurora to one db.r6g.2xlarge instance Dand leaves Redshift\nas-is. This change increases the operating cost Eas BRAD switches\nto a larger Aurora instance. After BRAD finishes transitioning the\ninfrastructure, the transaction latency falls under the new 20 ms\np90 constraint F. The query latency constraint also remains under\nthe new 20 s p90 constraint G. BRADâ€™s operating costs are 4.2 Ã—\nlower than the serverless Aurora and Redshift baseline. This is be-\ncause serverless Redshift has a large minimum size. While System H\nends with a 4.4Ã—lower monthly operating cost than BRAD, it does\n2050\n0 10 20 30 40 50 60 7003060Transaction\nLatency (ms)BRAD Servl. (Aurora & Redshift) System H\n0 10 20 30 40 50 60 70050Analytics\nLatency (s)\n0 10 20 30 40 50 60 70\nTime Elapsed (minutes)05000Monthly\nCost ($)AB\nCD\nEF\nG\nFigure 8: After the user changes their SLO constraints, BRAD\nselects a new blueprint to meet the new constraints.\nnot meet the 20 ms transaction latency constraint (its transactions\nhave a latency around 2 seconds). We again think that this elevated\nlatency is due to System Hâ€™s internal replication on writes. This\nscenario shows that BRAD adapts to changes to a userâ€™s constraints.\n5.2.5 Workload Intensity Variations During a Day. Finally, we run\nBRAD on a workload representing a full day at QuickFlix. For\npractical reasons, we scale the actual workload to 12 hours. Figure 9\nshows performance and cost over the day. We use the workload and\ndataset from Section 5.1 adapted to mimic the Snowset trace [ 108].\nConcretely, we run queries with a run time distribution similar to\nthe Snowset trace and vary the number of clients issuing queries\nand transactions to mimic the diurnal pattern observed in Snowset\n(a peak near the middle of the day, Figure 9 F).\nInitially, the workload is light. BRAD uses a blueprint with four\ndc2.large Redshift nodes and two Aurora db.t4g.medium instances,\nwhich is 2.5Ã—cheaper than the serverless Aurora and Redshift\nbaseline A. As the workload intensity increases, BRAD detects\nthe increases in latency B Cand triggers blueprint optimiza-\ntion, ultimately scaling Redshift up to 16 nodes and Aurora to one\ndb.r6g.xlarge instance at the peak. The serverless Aurora and Red-\nshift baseline also scales up, but it does not consistently meet the\nanalytics performance target of 30 s D, despite being 2.1Ã—more\nexpensive than BRADâ€™s design Eat the workload peak. System H\nmaintains the p90 analytics latency SLO throughout the workload,\nbut its transactional latency is again almost two orders of magni-\ntude higher than the other systems (we hypothesize for the same\nreasons as discussed earlier). The brief analytics latency spikes are\ndue to Redshift resizes, which force clients to reconnect.\nThis result shows that BRAD effectively responds to load\nvariations during the day . Over the day, BRAD maintains perfor-\nmance targets while reducing cumulative cost by 1.7 Ã—compared to\nthe serverless Aurora and Redshift baseline.\n5.3 Scoring: Model Accuracy and Generalization\nWe next examine the test accuracy of our predictive models and\ntheir generalizability across common workload shifts.\n11\n\n2050\n0 100 200 300 400 500 600 700030Transaction\nLatency (ms)BRAD Servl. (Aurora & Redshift) System H\n0 100 200 300 400 500 600 70030100Analytics\nLatency (s)\n0 100 200 300 400 500 600 700010000Monthly\nCost ($)\n0 100 200 300 400 500 600 700\nTime Elapsed (minutes)025Num.\nClientsABC\nD\nE\nF\nFigure 9: BRAD optimizes for load variations during the day.\nTable 1: Median test Q error of our blueprint scoring models.\nPrediction Target Aurora Redshift Athena\nQuery Run Time 1.5769 1.6539 1.3427\nData Accessed â€“ â€“ 1.2614\nRun Time on Different Provisioning 1.6718 1.6824 â€“\nTransaction Latency 1.2030 â€“ â€“\n5.3.1 Model Accuracy. Table 1 shows the median test Q error of our\nmodels for each engine. Q error is ğ‘„(ğ‘,ğ‘)=max(ğ‘/ğ‘,ğ‘/ğ‘), where\nğ‘refers to the predicted value and ğ‘to the actual value. Lower is\nbetter; 1 is the best possible score. We train each run time and data\naccessed model using approximately 8000 queries, validate on 2000\nqueries, and test on 125 unseen queries. Our query dataset consists\nof over 1000 unique join templates. The models for Athena perform\nbetter than Aurora and Redshift because the run time distribution\nof Athena queries has a lower variance. We test our provisioning\nand transaction latency models on an unseen provisioning that is\nlarger (i.e., has more resources) than all the training provisionings.\nOverall, we find that our modelsâ€™ prediction accuracy is sufficient\nfor BRAD to design effective infrastructures (Section 5.2).\n5.3.2 Generalizability. We evaluate our query run time modelâ€™s\ngeneralizability on three workload shifts: (i) unseen join templates,\n(ii) adding a new table to the dataset, and (iii) a larger dataset size.\nUnseen join templates. We train our run time models on queries\nwith less than 5 joins. We then test the modelâ€™s predictions on\nqueries with 5, 6, and â‰¥7 joins. Figure 10 shows each modelâ€™s\nmedian test Q error compared with (i) a model trained on all the\njoin templates (â€œfullâ€), and (ii) a naÃ¯ve linear model that scales the\ncost returned by the engineâ€™s query optimizer to a run time. We\nlabel the percentage difference from the model trained on the full\ndataset. Our model generalizes across unseen join templates with a\nQ error of at most 20% above the model trained on the full dataset.\nOur model still performs much better than the naÃ¯ve linear model,\nwhich has a Q error of at least 4.6. Since Athenaâ€™s optimizer does\nnot provide a query cost, we do not include a linear model result.\n< 5 56â‰¥ 7\nNumber of Joins148Median Q Error\n-0.37%\n+9.23%\n+14.57%\n+20.18%\n(a) Aurora< 5 56â‰¥ 7\nNumber of Joins\n-0.94%\n+8.06%\n+20.53%\n+21.66%\n(b) Redshift< 5 56â‰¥ 7\nNumber of Joins\n+2.86%\n+2.77%\n+3.63%\n+2.16%\n(c) AthenaLinear Model Trained < 5 Joins Trained FullFigure 10: BRAD generalizes to unseen join templates.\nAurora Redshift Athena15Median Q Error+22.17% +8.65%\n-0.40%Linear Model Trained w/o Table Trained Full\nFigure 11: BRAD generalizes to an added table.\nAurora Redshift Athena148Median Q Error+56.96% +8.31%+1.11%Linear Model Trained on â‰¤ 60 GB Trained on 160 GB\nFigure 12: BRAD generalizes to an increased dataset size.\nAdded table. We train our run time models on queries that do\nnot access the â€œperson_infoâ€ table and test them only on queries\nthat access the â€œperson_infoâ€ table. This table is around 10 GB (the\nsecond largest table in the dataset); the overall dataset size is 160 GB.\nFigure 11 shows our results using the same baselines and notation\nas Figure 10. Our model generalizes to an added table with a Q error\nat most 22% above the model trained on the full dataset. The linear\nmodel still does poorly, with a Q error of 4.6.\nIncreased dataset size. Finally, we evaluate our run time modelâ€™s\ngeneralizability to larger datasets. We train our models using queries\nexecuted on a 3 GB, 20 GB, 40 GB, and 60 GB version of our work-\nload dataset. Then we test the models on the original 160 GB dataset.\nFigure 12 shows that our model generalizes to the larger dataset\nwith a Q error of 1.1%, 8.3%, and 57% above a model specifically\ntrained on the 160 GB dataset on Athena, Redshift, and Aurora\nrespectively. The Aurora model has a higher error due to a change\nin caching behavior that occurs beyond 60 GB. The linear model\nhas a Q error of 6.3 and 7.7 on Aurora and Redshift respectively.\nOverall, these results are sufficient for BRAD since an increase from\n60 GB to 160 GB would likely happen over a longer period of time,\nallowing for BRAD to update its models given newly observed data.\n12\n\n0 1 2 3 4\nPer Query Geomean Slowdown\nRelative to Optimal RoutingBRADBRAD Query Run Time ModelBRAD Query Run Time Model (w/o Overh.)Redshift OnlyRandom RoutingFigure 13: BRADâ€™s routing quality is on par with its query\nrun time model but without the inference overhead.\n5.4 Query Routing Quality and Overhead\nNext, we evaluate BRADâ€™s query routing (Section 3.4) against four\nbaselines: (i) selecting an engine randomly, (ii) routing all queries to\nRedshift, (iii) routing using the BRAD run time model (Section 4.2.1)\nbut excluding its inference overhead, and (iv) routing using the\nBRAD run time model. We use 125 queries from our workload; 80%\nof the queries represent recurring queries and 20% are complex\nad-hoc queries. We report the geomean slowdown over optimal per\nrouting decision (i.e., lower is better, 1.0 Ã—is optimal). That is, for\neach query, we divide its run time on the chosen engine over its run\ntime on the fastest engine, and take the geomean across queries.\nFigure 13 shows our results. BRAD performs the best with a ge-\nomean slowdown of 1.31 Ã—, comparable to using the run time model\nand excluding its inference overhead (1.34 Ã—). Routing by actually\nusing the run time model (i.e. including its inference overhead) has\na geomean slowdown of 1.54 Ã—. These results highlight why we do\nnot directly use our run time model for routing; it imposes a high\noverhead on the queryâ€™s critical path (up to 115 ms), negatively\naffecting the routing performance. They also show that routing\nneeds an intelligent strategy; the Random (3.78 Ã—) and Redshift Only\n(1.85Ã—) strategies both perform worse than BRAD.\nOverall query processing overhead. Similar to common database\nproxies [ 19,24,29], BRAD imposes some overhead. We measure\na median overhead of 2.46 ms per complete transaction (these are\ninteractive multi-statement transactions) and around 10 ms per\nanalytical query. These could be further reduced, as BRAD is now\nimplemented in Python, but we believe they are reasonable given\nthat BRAD operates in the cloud, serving remote clients.\n5.5 Blueprint Search Effectiveness\nWe next evaluate the effectiveness of BRADâ€™s blueprint search algo-\nrithm. To compare the search algorithms, we report the final scalar\nscore computed by BRADâ€™s optimizer (Section 4.5). All baselines\nuse the same scoring models and optimize for the same workload;\nthey only differ in how they search for candidate blueprints.\nWe compare against three baselines: (i) uniform random sam-\npling, (ii) naÃ¯ve greedy, and (iii) exhaustive search. In uniform ran-\ndom sampling, each query is mapped to an engine that is cho-\nsen uniformly at random. We repeat this process to sample 10,000\nblueprints and select the best one. In naÃ¯ve greedy, each query is\nmapped to the engine with the lowest predicted run time (Sec-\ntion 4.2.1) without consideration of any other queries. Finally, ex-\nhaustive search looks through all possible mappings and therefore\nhas a run time that is exponential in the number of queries. To be\ntractable, we use only 12 randomly chosen queries from the IMDB\nworkload, comprising a search space of 530,000 candidates.\n0 5 10\nBlueprint Score (Monetary Cost)Beam Search (BRAD)Exhaustive SearchNaÃ¯ve GreedyUniform Sampling (10k)Figure 14: BRADâ€™s blueprint planner compared to baselines.\nâˆ’50 050\nPercentage Change (%)89Blueprint Score\n(Monetary Cost)\n(a) Run Timeâˆ’50 050\nPercentage Change (%)91011\n(b) Data Scannedâˆ’50 050\nPercentage Change (%)1015\n(c) Txn. Latency100% Data 80% Data 40% Data 20% Data 10% Data\nFigure 15: BRADâ€™s planner is robust to prediction errors.\nWe compare the algorithms on the scale down scenario from\nSection 5.2.1. Figure 14 shows that BRADâ€™s beam search finds a\nblueprint as good as exhaustive search. The score represents a\nmonetary cost, and so lower is better. BRADâ€™s blueprintâ€™s score is\nsignificantly lower than the blueprints selected through uniform\nsampling and the naÃ¯ve greedy approach. This result indicates that\nBRADâ€™s beam strategy is effective at finding optimized blueprints.\n5.6 Blueprint Planner Sensitivity\nFinally, we examine our blueprint plannerâ€™s sensitivity to prediction\nerrors. We inject errors into BRADâ€™s predictions during blueprint\nplanning and record the selected blueprintâ€™s score. Concretely, we\nselect a random subset of the predicted values (query run time, data\nscanned, and transaction latency) and increase (or decrease) their\npredicted values by a percentage. We use subsets that include 10%,\n20%, 40%, and 80% of the predictions. We run the blueprint planner\non the scale down scenario from Section 5.2.1. Figure 15 shows\nour results. The ğ‘¥-axis is the amount of injected error, which we\nvary fromâˆ’80%to+80%. Theğ‘¦-axis is the blueprintâ€™s scalar score.\nWe study the effects of prediction error on query run time, data\nscanned, and transaction latency.\nFigure 15(a) shows our results for query run time. There is no\nchange in the selected blueprint even when up to 40% of the pre-\ndicted query run times have injected errors of Â±80%. When 80%\nof the predictions have injected errors of more than âˆ’40%, BRAD\nselects a different (cheaper) blueprint as it predicts that the cheaper\nblueprint can meet the performance constraints.\nFigure 15(b) shows results for a queryâ€™s predicted data scanned.\nSimilar to run times, there is no change to the chosen blueprint\nwhen up to 40% of the predictions have injected errors. At 80%,\nBRAD chooses to route queries onto Athena. Thus the blueprintâ€™s\nmonetary cost varies linearly with respect to the injected error.\nFigure 15(c) shows results for transaction latency. Note that we\ninject errors into 100% of the data since BRAD makes just one\nlatency prediction. Here, an injected error of +50%causes BRAD\n13\n\nto select a larger Aurora instance (to meet the latency constraint),\nwhich increases the blueprint score (monetary cost).\nOverall, our results indicate that BRADâ€™s planner is robust to\nprediction errors; more than 40% of the predictions need to have an\nerror of more than Â±50%for BRAD to choose a different blueprint.\nThe intuition is that blueprints represent coarse-grained design\ndecisions, and thus are more tolerant to prediction errors.\n6 RELATED WORK\nInstance-optimized, self-driving, and auto-tuning systems.\nRecent work has proposed techniques to automatically (i) adapt data\nsystems to the workload [ 1,35,36,57â€“59,61,70â€“72,80,89,115,116],\n(ii) manage complex systems [ 68,78,86â€“88,95] (iii) adapt cloud\ndatabase instance sizing [ 82â€“84,106], and (iv) tune their knobs [ 54,\n85,105]. In contrast, BRAD optimizes an entire multi-engine data\ninfrastructure instead of tuning individual services. BRAD can be\nseen as applying instance-optimization at the granularity of cloud\ndatabase services instead of within a database engine [58].\nSimplifying and optimizing the cloud. Like BRAD, recent re-\nsearch has explored ways to simplify and optimize the design and\noperation of cloud infrastructures. These thrusts include (i) high-\n-level cloud programming abstractions [ 31,67,77,94], (ii) infras-\ntructure as code [ 18,49], (iii) enhancing cross-cloud compatibil-\nity [30,104], and (iv) improving resilience across services [ 66].\nBRADâ€™s key difference is that it focuses on simplifying cloud in-\nfrastructures containing multiple relational database systems while\noptimizing their use for cost under a performance constraint.\nSingle-system solutions. Another way to handle diverse data\nworkloads is to use a single specialized (e.g., HTAP) database sys-\ntem designed for high performance across many workloads [ 40,\n51,56,62,98]. For some workloads (e.g., real-time analytics), such\nsystems can be more efficient than BRAD because they are not in-\nternally constrained by engine boundaries. But these single-system\nsolutions can be difficult to migrate to and they limit users to their\nspecific feature set. In contrast, BRAD is designed to optimize exist-\ning multi-engine data infrastructures and (in concept) can include\nnew systems to support specialized functionality (Section 5.2.3).\nPolystores and federated databases. Prior work on polystores [ 3,\n5,37,91,107,110,118] and federated databases [ 25,27,28,41,52,\n53,75,92,97,117] also aim to distribute query workloads across\nheterogeneous engines. Unlike BRAD, these systems focus on (i) op-\ntimizing queries within a given set of engines and hardware con-\nfiguration, and (ii) bridging different data models [ 37]. In contrast,\nBRAD tackles the problem of selecting the best set of engines to\ninclude in the underlying infrastructure for the userâ€™s workload\n(among Aurora, Redshift, and Athena), while also jointly optimizing\nthe workload assignment, engine provisioning, and data placement.\n7 CONCLUSION\nThis paper presents blueprints ,blueprint planning , and BRAD: a\nsystem that virtualizes a cloud data infrastructure and leverages\nblueprint planning to automatically manage its physical realization.\nThe key takeaway is to cast infrastructure design as a cost-based\noptimization problem , which we refer to as blueprint planning . Thisapproach allows us to systematically search for an optimized de-\nsign for a given workload by leveraging learned models to predict\nthe utility of candidate blueprints. We show that BRAD automat-\nically achieves performance targets while saving 1.6â€“13 Ã—in cost\ncompared to existing serverless autoscaling systems.\nACKNOWLEDGMENTS\nThis research was supported by Amazon, Google, and Intel as part\nof the MIT Data Systems and AI Lab (DSAIL) at MIT and NSF\nIIS 1900933. Geoffrey X. Yu was partially supported by an NSERC\nPGS D. This research was also sponsored by the United States Air\nForce Research Laboratory and the Department of the Air Force\nArtificial Intelligence Accelerator and was accomplished under\nCooperative Agreement Number FA8750-19-2-1000. The views and\nconclusions contained in this document are those of the authors and\nshould not be interpreted as representing the official policies, either\nexpressed or implied, of the Department of the Air Force or the U.S.\nGovernment. The U.S. Government is authorized to reproduce and\ndistribute reprints for Government purposes notwithstanding any\ncopyright notation herein.\nREFERENCES\n[1]Michael Abebe, Horatiu Lazu, and Khuzaima Daudjee. 2022. Proteus: Au-\ntonomous Adaptive Storage for Mixed Workloads. In Proceedings of the 2022\nInternational Conference on Management of Data (SIGMOD â€™22) . 700â€“714. https:\n//doi.org/10.1145/3514221.3517834\n[2] Ivo Adan and Jacques Resing. 2015. Queueing Systems . https://www.win.tue.nl/\n~iadan/queueing.pdf.\n[3]Divy Agrawal, Sanjay Chawla, Bertty Contreras-Rojas, Ahmed Elmagarmid,\nYasser Idris, Zoi Kaoudi, Sebastian Kruse, Ji Lucas, Essam Mansour, Mourad\nOuzzani, Paolo Papotti, Jorge-Arnulfo QuianÃ©-Ruiz, Nan Tang, Saravanan Thiru-\nmuruganathan, and Anis Troudi. 2018. RHEEM: Enabling Cross-Platform Data\nProcessing: May the Big Data Be with You! Proceedings of the VLDB Endowment\n11, 11 (2018), 1414â€“1427. https://doi.org/10.14778/3236187.3236195\n[4]Mert Akdere, Ugur Ã‡etintemel, Matteo Riondato, Eli Upfal, and Stanley B.\nZdonik. 2012. Learning-based Query Performance Modeling and Prediction.\nInProceedings of the IEEE 28th International Conference on Data Engineering\n(ICDE â€™12) . 390â€“401. https://doi.org/10.1109/ICDE.2012.64\n[5] Rana Alotaibi, Damian Bursztyn, Alin Deutsch, Ioana Manolescu, and Stamatis\nZampetakis. 2019. Towards Scalable Hybrid Stores: Constraint-Based Rewriting\nto the Rescue. In Proceedings of the 2019 International Conference on Management\nof Data (SIGMOD â€™19) . 1660â€“1677.\n[6]Amazon Web Services. 2021. Achieve up to 35% better price/performance with\nAmazon Aurora using new Graviton2 instances . https://aws.amazon.com/about-\naws/whats-new/2021/03/achieve-up-to-35-percent-better-price-\nperformance-with-amazon-aurora-using-new-graviton2-instances/.\n[7]Amazon Web Services. 2022. AWS announces Amazon Aurora zero-ETL inte-\ngration with Amazon Redshift . https://aws.amazon.com/about-aws/whats-\nnew/2022/11/amazon-aurora-zero-etl-integration-redshift/. Retrieved July 20,\n2024.\n[8]Amazon Web Services. 2023. AWS announces Amazon Aurora I/O-\nOptimized . https://aws.amazon.com/about-aws/whats-new/2023/05/amazon-\naurora-i-o-optimized/. Retrieved July 20, 2024.\n[9]Amazon Web Services. 2023. How do I resize an Amazon Redshift cluster?\nhttps://repost.aws/knowledge-center/resize-redshift-cluster. Retrieved July 20,\n2024.\n[10] Amazon Web Services. 2024. Amazon Athena. https://aws.amazon.com/athena/.\nRetrieved July 20, 2024.\n[11] Amazon Web Services. 2024. Amazon Athena Pricing . https://aws.amazon.com/\nathena/pricing/. Retrieved July 20, 2024.\n[12] Amazon Web Services. 2024. Amazon Aurora. https://aws.amazon.com/rds/\naurora/. Retrieved July 20, 2024.\n[13] Amazon Web Services. 2024. Amazon Aurora Pricing . https://aws.amazon.com/\nrds/aurora/pricing/. Retrieved July 20, 2024.\n[14] Amazon Web Services. 2024. Amazon EC2. https://aws.amazon.com/ec2/.\nRetrieved July 20, 2024.\n[15] Amazon Web Services. 2024. Amazon Redshift. https://aws.amazon.com/\nredshift/. Retrieved July 20, 2024.\n14\n\n[16] Amazon Web Services. 2024. Amazon Redshift Pricing . https://aws.amazon.\ncom/redshift/pricing/. Retrieved July 20, 2024.\n[17] Amazon Web Services. 2024. Amazon S3. https://aws.amazon.com/s3/. Re-\ntrieved July 20, 2024.\n[18] Amazon Web Services. 2024. AWS CloudFormation. https://aws.amazon.com/\npm/cloudformation/. Retrieved July 20, 2024.\n[19] Amazon Web Services. 2024. AWS RDS Proxy. https://aws.amazon.com/rds/\nproxy/. Retrieved July 20, 2024.\n[20] Amazon Web Services. 2024. Data Lakes and Analytics on AWS. https:\n//aws.amazon.com/big-data/datalakes-and-analytics/. Retrieved July 20, 2024.\n[21] Amazon Web Services. 2024. Purpose-Built Databases on AWS. https://aws.\namazon.com/products/databases/. Retrieved July 20, 2024.\n[22] Gene M. Amdahl. 1967. Validity of the single processor approach to achieving\nlarge scale computing capabilities. In Proceedings of the April 18-20, 1967, Spring\nJoint Computer Conference (AFIPS â€™67 (Spring)) . 483â€“485. https://doi.org/10.\n1145/1465482.1465560\n[23] Lyublena Antova, Derrick Bryant, Tuan Cao, Michael Duller, Mohamed A.\nSoliman, and Florian M. Waas. 2018. Rapid Adoption of Cloud Data Warehouse\nTechnology Using Datometry Hyper-Q. In Proceedings of the 2018 International\nConference on Management of Data (SIGMOD â€™18) . 825â€“839. https://doi.org/10.\n1145/3183713.3190652\n[24] Pgbouncer Authors. 2024. Pgbouncer - Lightweight connection pooler for\nPostgreSQL. https://www.pgbouncer.org/. Retrieved July 20, 2024.\n[25] Graham Bent, Patrick Dantressangle, David Vyvyan, Abbe Mowshowitz, and\nValia Mitsou. 2008. A Dynamic Distributed Federated Database. In Proceedings\nof the 2nd Annual Conference on International Technology Alliance (ACITA â€™08) .\n[26] Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning .\nSpringer.\n[27] Yuri Breitbart, Hector Garcia-Molina, and Abraham Silberschatz. 1992.\nOverview of Multidatabase Transaction Management. VLDB Journal 1 (10\n1992), 181â€“239. https://doi.org/10.1145/1925805.1925811\n[28] Yuri Breitbart and Avi Silberschatz. 1988. Multidatabase Update Issues. In\nProceedings of the 1988 ACM SIGMOD International Conference on Management\nof Data (SIGMOD â€™88) . 135â€“142. https://doi.org/10.1145/50202.50217\n[29] Matthew Butrovich, Karthik Ramanathan, John Rollinson, Wan Shen Lim,\nWilliam Zhang, Justine Sherry, and Andrew Pavlo. 2023. Tigger: A Database\nProxy That Bounces with User-Bypass. Proceedings of the VLDB Endowment 16,\n11 (2023), 3335â€“3348. https://doi.org/10.14778/3611479.3611530\n[30] Sarah Chasins, Alvin Cheung, Natacha Crooks, Ali Ghodsi, Ken Goldberg,\nJoseph E. Gonzalez, Joseph M. Hellerstein, Michael I. Jordan, Anthony D. Joseph,\nMichael W. Mahoney, Aditya Parameswaran, David Patterson, Raluca Ada Popa,\nKoushik Sen, Scott Shenker, Dawn Song, and Ion Stoica. 2022. The Sky Above\nThe Clouds. arXiv:2205.07147 [cs.DC] https://arxiv.org/abs/2205.07147\n[31] Alvin Cheung, Natacha Crooks, Joseph M. Hellerstein, and Mae Milano. 2021.\nNew Directions in Cloud Programming. arXiv:2101.01159 [cs.DC] https:\n//arxiv.org/abs/2101.01159\n[32] Yun Chi, Hyun Jin Moon, Hakan HacigÃ¼mÃ¼s, and Junâ€™ichi Tatemura. 2011. SLA-\ntree: A Framework for Efficiently Supporting SLA-based Decisions in Cloud\nComputing. In Proceedings of the 14th International Conference on Extending\nDatabase Technology (EDBT â€™11) . 129â€“140. https://doi.org/10.1145/1951365.\n1951383\n[33] cppreference.com. 2024. C++ named requirements: Compare. https://en.\ncppreference.com/w/cpp/named_req/Compare. Retrieved July 20, 2024.\n[34] Benoit Dageville, Thierry Cruanes, Marcin Zukowski, Vadim Antonov, Artin\nAvanes, Jon Bock, Jonathan Claybaugh, Daniel Engovatov, Martin Hentschel,\nJiansheng Huang, Allison W. Lee, Ashish Motivala, Abdul Q. Munir, Steven\nPelley, Peter Povinec, Greg Rahn, Spyridon Triantafyllis, and Philipp Unter-\nbrunner. 2016. The Snowflake Elastic Data Warehouse. In Proceedings of the\n2016 International Conference on Management of Data (SIGMOD â€™16) . 215â€“226.\nhttps://doi.org/10.1145/2882903.2903741\n[35] Jialin Ding, Umar Farooq Minhas, Badrish Chandramouli, Chi Wang, Yinan Li,\nYing Li, Donald Kossmann, Johannes Gehrke, and Tim Kraska. 2021. Instance-\nOptimized Data Layouts for Cloud Analytics Workloads. In Proceedings of the\n2021 International Conference on Management of Data (SIGMOD â€™21) . 418â€“431.\nhttps://doi.org/10.1145/3448016.3457270\n[36] Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020.\nTsunami: A Learned Multi-Dimensional Index for Correlated Data and Skewed\nWorkloads. Proceedings of the VLDB Endowment 14, 2 (2020), 74â€“86. https:\n//doi.org/10.14778/3425879.3425880\n[37] Jennie Duggan, Aaron J. Elmore, Michael Stonebraker, Magda Balazinska, Bill\nHowe, Jeremy Kepner, Sam Madden, David Maier, Tim Mattson, and Stan\nZdonik. 2015. The BigDAWG Polystore System. SIGMOD Rec. 44, 2 (August\n2015), 11â€“16. https://doi.org/10.1145/2814710.2814713\n[38] Jennie Duggan, Olga Papaemmanouil, Ugur Ã‡etintemel, and Eli Upfal. 2014.\nContender: A Resource Modeling Approach for Concurrent Query Performance\nPrediction. In Proceedings of the 17th International Conference on Extending\nDatabase Technology (EDBT â€™14) . 109â€“120. https://doi.org/10.5441/002/EDBT.\n2014.11[39] Johannes Frnkranz and Eyke Hllermeier. 2010. Preference Learning . Springer-\nVerlag, Berlin, Heidelberg.\n[40] Franz FÃ¤rber, Norman May, Wolfgang Lehner, Philipp GroÃŸe, Ingo MÃ¼ller,\nHannes Rauhe, and Jonathan Dees. 2012. The SAP HANA Database â€“ An\nArchitecture Overview. IEEE Data Engineering Bulletin 35 (03 2012), 28â€“33.\n[41] Dimitrios Georgakopoulos, Marek Rusinkiewicz, and Amit P. Sheth. 1991. On\nSerializability of Multidatabase Transactions Through Forced Local Conflicts.\nInProceedings of the Seventh International Conference on Data Engineering\n(ICDE â€™91) . 314â€“323.\n[42] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and\nGeorge E. Dahl. 2017. Neural Message Passing for Quantum Chemistry. In\nProceedings of the 34th International Conference on Machine Learning (Pro-\nceedings of Machine Learning Research) , Vol. 70. PMLR, 1263â€“1272. https:\n//proceedings.mlr.press/v70/gilmer17a.html\n[43] Google, Inc. 2024. BigQuery Omni. https://cloud.google.com/bigquery/docs/\nomni-introduction. Retrieved July 20, 2024.\n[44] Google, Inc. 2024. Google Cloud Databases. https://cloud.google.com/products/\ndatabases. Retrieved July 20, 2024.\n[45] Google, Inc. 2024. Google Compute Engine. https://cloud.google.com/compute.\nRetrieved July 20, 2024.\n[46] Xingyu Gu, Adam Ronthal, Robert Thanaraj, and Julian Sun. 2024. Data and An-\nalytics Cloud Adoption Survey Reveals Data Governance and Cost Challenges.\nGartner Report. https://www.gartner.com/document/5106731.\n[47] Yuxing Han, Ziniu Wu, Peizhi Wu, Rong Zhu, Jingyi Yang, Liang Wei Tan,\nKai Zeng, Gao Cong, Yanzhao Qin, Andreas Pfadler, Zhengping Qian, Jingren\nZhou, Jiangneng Li, and Bin Cui. 2021. Cardinality Estimation in DBMS: A\nComprehensive Benchmark Evaluation. Proceedings of the VLDB Endowment\n15, 4 (2021), 752â€“765. https://doi.org/10.14778/3503585.3503586\n[48] Mor Harchol-Balter. 2013. Performance Modeling and Design of Computer Sys-\ntems: Queueing Theory in Action . Cambridge University Press.\n[49] HashiCorp. 2024. Terraform. https://www.terraform.io. Retrieved July 20, 2024.\n[50] Benjamin Hilprecht and Carsten Binnig. 2022. Zero-Shot Cost Models for Out-\nof-the-box Learned Cost Prediction. Proceedings of the VLDB Endowment 15, 11\n(2022), 2361â€“2374. https://www.vldb.org/pvldb/vol15/p2361-hilprecht.pdf\n[51] Dongxu Huang, Qi Liu, Qiu Cui, Zhuhe Fang, Xiaoyu Ma, Fei Xu, Li Shen,\nLiu Tang, Yuxing Zhou, Menglong Huang, Wan Wei, Cong Liu, Jian Zhang,\nJianjun Li, Xuelian Wu, Lingyu Song, Ruoxi Sun, Shuaipeng Yu, Lei Zhao,\nNicholas Cameron, Liquan Pei, and Xin Tang. 2020. TiDB: A Raft-Based HTAP\nDatabase. Proceedings of the VLDB Endowment 13, 12 (2020), 3072â€“3084. https:\n//doi.org/10.14778/3415478.3415535\n[52] S.-Y. Hwang, E.-P. Lim, H.-R. Yang, S. Musukula, K. Mediratta, M. Ganesh, D.\nClements, J. Stenoien, and J. Srivastava. 1994. The MYRIAD Federated Database\nPrototype. In Proceedings of the 1994 ACM SIGMOD International Conference on\nManagement of Data (SIGMOD â€™94) . https://doi.org/10.1145/191839.191986\n[53] Vanja Josifovski, Peter Schwarz, Laura Haas, and Eileen Lin. 2002. Garlic: A\nNew Flavor of Federated Query Processing for DB2. In Proceedings of the 2002\nACM SIGMOD International Conference on Management of Data (SIGMOD â€™02) .\n524â€“532.\n[54] Konstantinos Kanellis, Cong Ding, Brian Kroth, Andreas MÃ¼ller, Carlo Curino,\nand Shivaram Venkataraman. 2022. LlamaTune: Sample-Efficient DBMS Config-\nuration Tuning. Proceedings of the VLDB Endowment 15, 11 (2022), 2953â€“2965.\nhttps://doi.org/10.14778/3551793.3551844\n[55] Anastasios Karagiannis, Panos Vassiliadis, and Alkis Simitsis. 2013. Scheduling\nStrategies for Efficient ETL Execution. Information Systems 38, 6 (2013), 927â€“\n945.\n[56] Alfons Kemper and Thomas Neumann. 2011. HyPer: A Hybrid OLTP & OLAP\nMain Memory Database System Based on Virtual Memory Snapshots. In Pro-\nceedings of the 2011 IEEE 27th International Conference on Data Engineering\n(ICDE â€™11) . 195â€“206. https://doi.org/10.1109/ICDE.2011.5767867\n[57] Ferdi Kossmann, Ziniu Wu, Eugenie Lai, Nesime Tatbul, Lei Cao, Tim Kraska,\nand Sam Madden. 2023. Extract-Transform-Load for Video Streams. Proceedings\nof the VLDB Endowment 16, 9 (2023), 2302â€“2315. https://doi.org/10.14778/\n3598581.3598600\n[58] Tim Kraska, Mohammad Alizadeh, Alex Beutel, Ed H. Chi, Ani Kristo, Guillaume\nLeclerc, Samuel Madden, Hongzi Mao, and Vikram Nathan. 2019. SageDB: A\nLearned Database System. In Proceedings of the 9th Biennial Conference on\nInnovative Data Systems Research (CIDR â€™19) . http://cidrdb.org/cidr2019/papers/\np117-kraska-cidr19.pdf\n[59] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In Proceedings of the 2018 International\nConference on Management of Data (SIGMOD â€™18) . 489â€“504. https://doi.org/10.\n1145/3183713.3196909\n[60] Tim Kraska, Tianyu Li, Samuel Madden, Markos Markakis, Amadou Ngom,\nZiniu Wu, and Geoffrey X. Yu. 2023. Check Out the Big Brain on BRAD:\nSimplifying Cloud Data Processing with Learned Automated Data Meshes.\nProceedings of the VLDB Endowment 16, 11 (8 2023), 3293â€“3301. https://doi.org/\n10.14778/3611479.3611526\n15\n\n[61] Sanjay Krishnan, Zongheng Yang, Ken Goldberg, Joseph Hellerstein, and Ion\nStoica. 2019. Learning to Optimize Join Queries With Deep Reinforcement\nLearning. arXiv:1808.03196 [cs.DB] https://arxiv.org/abs/1808.03196\n[62] Tirthankar Lahiri, Shasank Chavan, Maria Colgan, Dinesh Das, Amit Ganesh,\nMike Gleeson, Sanket Hase, Allison Holloway, Jesse Kamp, Teck-Hua Lee, Juan\nLoaiza, Neil Macnaughton, Vineet Marwah, Niloy Mukherjee, Atrayee Mullick,\nSujatha Muthulingam, Vivekanandhan Raja, Marty Roth, Ekrem Soylemez, and\nMohamed Zait. 2015. Oracle Database In-Memory: A Dual Format In-Memory\nDatabase. In Proceedings of the 2015 IEEE 31st International Conference on Data\nEngineering (ICDE â€™15) . 1253â€“1258. https://doi.org/10.1109/ICDE.2015.7113373\n[63] Viktor Leis, Andrey Gubichev, Atanas Mirchev, Peter Boncz, Alfons Kemper, and\nThomas Neumann. 2015. How good are query optimizers, really? Proceedings\nof the VLDB Endowment 9, 3 (2015), 204â€“215.\n[64] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen tau Yih, Tim\nRocktÃ¤schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented\nGeneration for Knowledge-Intensive NLP Tasks. In Advances in Neural Infor-\nmation Processing Systems (NeurIPS â€™20) .\n[65] Jiexing Li, Arnd Christian KÃ¶nig, Vivek R. Narasayya, and Surajit Chaudhuri.\n2012. Robust Estimation of Resource Consumption for SQL Queries using\nStatistical Techniques. Proceedings of the VLDB Endowment 5, 11 (2012), 1555â€“\n1566. https://doi.org/10.14778/2350229.2350269\n[66] Tianyu Li, Badrish Chandramouli, Sebastian Burckhardt, and Samuel Madden.\n2023. DARQ Matter Binds Everything: Performant and Composable Cloud\nProgramming via Resilient Steps. Proceedings of the ACM on Management of\nData 1, 2, Article 117 (2023), 27 pages. https://doi.org/10.1145/3589262\n[67] Tianyu Li, Badrish Chandramouli, Sebastian Burckhardt, and Samuel Madden.\n2024. Serverless State Management Systems. In Proceedings of the Conference on\nInnovative Data Research (CIDR â€™24) . https://www.cidrdb.org/cidr2024/papers/\np16-li.pdf\n[68] Wan Shen Lim, Matthew Butrovich, William Zhang, Andrew Crotty, Lin Ma,\nPeijing Xu, Johannes Gehrke, and Andrew Pavlo. 2023. Database Gyms. In\nProceedings of the Conference on Innovative Data Systems Research (CIDR â€™23) .\n[69] B. T. Lowerre. 1976. The HARPY Speech Recognition System . Ph.D. Dissertation.\nCarnegie Mellon University.\n[70] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Nesime Tatbul, Mohammad Al-\nizadeh, and Tim Kraska. 2022. Bao: Making Learned Query Optimization\nPractical. In Proceedings of the International Conference on Management of Data\n(SIGMOD â€™22) .\n[71] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh,\nTim Kraska, Olga Papaemmanouil, and Nesime Tatbul. 2019. Neo: A Learned\nQuery Optimizer. Proceedings of the VLDB Endowment 12, 11 (2019).\n[72] Ryan Marcus and Olga Papaemmanouil. 2018. Deep Reinforcement Learning\nfor Join Order Enumeration. In Proceedings of the First International Workshop\non Exploiting Artificial Intelligence Techniques for Data Management (aiDM â€™18) .\n[73] Ryan Marcus and Olga Papaemmanouil. 2019. Plan-Structured Deep Neural\nNetwork Models for Query Performance Prediction. Proceedings of the VLDB\nEndowment 12, 11 (2019), 1733â€“1746. https://doi.org/10.14778/3342263.3342646\n[74] Microsoft Corporation. 2024. Azure Compute. https://azure.microsoft.com/en-\nus/products/category/compute. Retrieved July 20, 2024.\n[75] Microsoft Corporation. 2024. Microsoft Fabric Documentation. https://learn.\nmicrosoft.com/en-us/fabric/. Retrieved July 20, 2024.\n[76] Elena Milkai, Yannis Chronis, Kevin P Gaffney, Zhihan Guo, Jignesh M Patel,\nand Xiangyao Yu. 2022. How Good is My HTAP System?. In Proceedings of the\n2022 International Conference on Management of Data (SIGMOD â€™22) . 1810â€“1824.\n[77] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard\nLiaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I. Jordan,\nand Ion Stoica. 2018. Ray: A Distributed Framework for Emerging AI Applica-\ntions. In Proceedings of the 13th USENIX Conference on Operating Systems Design\nand Implementation (OSDI â€™18) . 561â€“577.\n[78] Barzan Mozafari, Radu Alexandru Burcuta, Alan Cabrera, Andrei Constantin,\nDerek Francis, David GrÃ¶mling, Alekh Jindal, Maciej Konkolowicz, Valentin\nMarian Spac, Yongjoo Park, Russell Razo Carranzo, Nicholas Richardson, Ab-\nhishek Roy, Aayushi Srivastava, Isha Tarte, Brian Westphal, and Chi Zhang. 2023.\nMaking Data Clouds Smarter at Keebo: Automated Warehouse Optimization Us-\ning Data Learning. In Companion of the 2023 International Conference on Manage-\nment of Data (Seattle, WA, USA) (SIGMOD â€™23) . Association for Computing Ma-\nchinery, New York, NY, USA, 239â€“251. https://doi.org/10.1145/3555041.3589681\n[79] Barzan Mozafari, Carlo Curino, Alekh Jindal, and Samuel Madden. 2013. Per-\nformance and Resource Modeling in Highly-Concurrent OLTP Workloads. In\nProceedings of the 2013 ACM SIGMOD International Conference on Management\nof Data (SIGMOD â€™13) .\n[80] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020.\nLearning Multi-Dimensional Indexes. In Proceedings of the 2020 ACM SIG-\nMOD International Conference on Management of Data (Portland, OR, USA)\n(SIGMOD â€™20) . Association for Computing Machinery, New York, NY, USA,\n985â€“1000. https://doi.org/10.1145/3318464.3380579[81] Parimarjan Negi, Ziniu Wu, Andreas Kipf, Nesime Tatbul, Ryan Marcus, Sam\nMadden, Tim Kraska, and Mohammad Alizadeh. 2023. Robust Query Driven\nCardinality Estimation under Changing Workloads. Proceedings of the VLDB\nEndowment 16, 6 (2023), 1520â€“1533. https://doi.org/10.14778/3583140.3583164\n[82] Jennifer Ortiz. 2019. Performance-Based Service Level Agreements for Data\nAnalytics in the Cloud . Ph.D. Dissertation. University of Washington.\n[83] Jennifer Ortiz, Brendan Lee, Magdalena Balazinska, Johannes Gehrke, and\nJoseph L Hellerstein. 2018. SLAOrchestrator: Reducing the Cost of Performance\nSLAs for Cloud Data Analytics. In Proceedings of the 2018 USENIX Annual\nTechnical Conference ((USENIX ATC â€™18)) . 547â€“560.\n[84] Jennifer Ortiz, Brendan Lee, Magdalena Balazinska, and Joseph L. Hellerstein.\n2016. PerfEnforce: A Dynamic Scaling Engine for Analytics with Performance\nGuarantees. arXiv:1605.09753 [cs.DB]\n[85] OtterTune, Inc. 2024. OtterTune | AI Powered Automatic PostgreSQL & MySQL\nTuning. https://web.archive.org/web/20240605143522/https://ottertune.com/.\nRetrieved July 20, 2024.\n[86] Andrew Pavlo, Gustavo Angulo, Joy Arulraj, Haibin Lin, Jiexi Lin, Lin Ma,\nPrashanth Menon, Todd Mowry, Matthew Perron, Ian Quah, Siddharth San-\nturkar, Anthony Tomasic, Skye Toor, Dana Van Aken, Ziqi Wang, Yingjun\nWu, Ran Xian, and Tieying Zhang. 2017. Self-Driving Database Management\nSystems. In Proceedings of the Conference on Innovative Data Systems Research\n(CIDR â€™17) . https://db.cs.cmu.edu/papers/2017/p42-pavlo-cidr17.pdf\n[87] Andrew Pavlo, Matthew Butrovich, Ananya Joshi, Lin Ma, Prashanth Menon,\nDana Van Aken, Lisa Lee, and Ruslan Salakhutdinov. 2019. External vs. In-\nternal: An Essay on Machine Learning Agents for Autonomous Database\nManagement Systems. IEEE Data Engineering Bulletin (June 2019), 32â€“46.\nhttp://sites.computer.org/debull/A19june/p32.pdf\n[88] Andrew Pavlo, Matthew Butrovich, Lin Ma, Wan Shen Lim, Prashanth Menon,\nDana Van Aken, and William Zhang. 2021. Make Your Database System Dream\nof Electric Sheep: Towards Self-Driving Operation. Proceedings of the VLDB\nEndowment 14, 12 (2021), 3211â€“3221. https://doi.org/10.14778/3476311.3476411\n[89] Matthew Perron, Raul Castro Fernandez, David Dewitt, Michael Cafarella, and\nSamuel Madden. 2023. Cackle: Analytical Workload Cost and Performance\nStability With Elastic Pools. In Proceedings of the ACM on Management of Data ,\nVol. 1. Issue 4. https://doi.org/10.1145/3626720\n[90] pgvector Authors. 2023. Open source vector similarity search for Postgres.\nhttps://github.com/pgvector/pgvector.\n[91] Maksim Podkorytov and Michael Gubanov. 2019. Hybrid.Poly: A Consolidated\nInteractive Analytical Polystore System. In Proceedings of the 2019 IEEE 35th\nInternational Conference on Data Engineering (ICDE â€™19) . 1996â€“1999. https:\n//doi.org/10.1109/ICDE.2019.00223\n[92] Calton Pu. 1988. Superdatabases for Composition of Heterogeneous Databases.\nInProceedings of the Fourth International Conference on Data Engineering\n(ICDE â€™88) . 548â€“555.\n[93] Francisco Romero, Qian Li, Neeraja J. Yadwadkar, and Christos Kozyrakis.\n2021. INFaaS: Automated Model-less Inference Serving. In Proceedings of the\n2021 USENIX Annual Technical Conference (USENIX ATC â€™21) . 397â€“411. https:\n//www.usenix.org/conference/atc21/presentation/romero\n[94] Mingwei Samuel. 2021. Hydroflow: A Model and Runtime for Distributed Systems\nProgramming . Masterâ€™s thesis. University of California, Berkeley. http://www2.\neecs.berkeley.edu/Pubs/TechRpts/2021/EECS-2021-201.html\n[95] Gaurav Saxena, Mohammad Rahman, Naresh Chainani, Chunbin Lin, George\nCaragea, Fahim Chowdhury, Ryan Marcus, Tim Kraska, Ippokratis Pandis, and\nBalakrishnan (Murali) Narayanaswamy. 2023. Auto-WLM: Machine Learning\nEnhanced Workload Management in Amazon Redshift. In Companion of the\n2023 International Conference on Management of Data (SIGMOD â€™23) . 225â€“237.\nhttps://doi.org/10.1145/3555041.3589677\n[96] P. Griffiths Selinger, M. M. Astrahan, D. D. Chamberlin, R. A. Lorie, and T. G.\nPrice. 1979. Access Path Selection in a Relational Database Management System.\nInProceedings of the 1979 ACM SIGMOD International Conference on Management\nof Data (SIGMOD â€™79) . 23â€“34. https://doi.org/10.1145/582095.582099\n[97] Amit P Sheth and James A Larson. 1990. Federated Database Systems for\nManaging Distributed, Heterogeneous, and Autonomous Databases. ACM\nComputing Surveys (CSUR) 22, 3 (1990), 183â€“236.\n[98] Vishal Sikka, Franz FÃ¤rber, Wolfgang Lehner, Sang Kyun Cha, Thomas Peh,\nand Christof BornhÃ¶vd. 2012. Efficient Transaction Processing in SAP HANA\nDatabase: The End of a Column Store Myth. In Proceedings of the 2012 ACM SIG-\nMOD International Conference on Management of Data (SIGMOD â€™12) . 731â€“742.\nhttps://doi.org/10.1145/2213836.2213946\n[99] Snowflake, Inc. 2024. ETL vs. ELT: Differences and Similarities . https://www.\nsnowflake.com/guides/etl-vs-elt. Retrieved July 20, 2024.\n[100] Ji Sun and Guoliang Li. 2019. An End-to-End Learning-based Cost Estimator.\nProceedings of the VLDB Endowment 13, 3 (2019), 307â€“319. https://doi.org/10.\n14778/3368289.3368296\n[101] Transaction Processing Performance Council (TPC). 2024. TPC-C . https:\n//www.tpc.org/tpcc/. Retrieved July 20, 2024.\n[102] Transaction Processing Performance Council (TPC). 2024. TPC-DS . https:\n//www.tpc.org/tpcds/default5.asp. Retrieved July 20, 2024.\n16\n\n[103] Transaction Processing Performance Council (TPC). 2024. TPC-H . https:\n//www.tpc.org/tpch/default5.asp. Retrieved July 20, 2024.\n[104] University of California, Berkeley. 2024. Sky Computing. https://sky.cs.berkeley.\nedu/. Retrieved July 20, 2024.\n[105] Dana Van Aken, Andrew Pavlo, Geoffrey J. Gordon, and Bohan Zhang. 2017. Au-\ntomatic Database Management System Tuning Through Large-Scale Machine\nLearning. In Proceedings of the 2017 ACM International Conference on Man-\nagement of Data (SIGMOD â€™17) . 1009â€“1024. https://doi.org/10.1145/3035918.\n3064029\n[106] Shivaram Venkataraman, Zongheng Yang, Michael Franklin, Benjamin Recht,\nand Ion Stoica. 2016. Ernest: Efficient Performance Prediction for Large-Scale\nAdvanced Analytics. In Proceedings of the 13th USENIX Symposium on Networked\nSystems Design and Implementation (NSDI â€™16) . 363â€“378.\n[107] Marco Vogt, Alexander Stiemer, and Heiko Schuldt. 2018. Polypheny-DB:\nTowards a Distributed and Self-Adaptive Polystore. In Proceedings of the 2018\nIEEE International Conference on Big Data (IEEE Big Data â€™18) . 3364â€“3373.\n[108] Midhul Vuppalapati, Justin Miron, Rachit Agarwal, Dan Truong, Ashish Mo-\ntivala, and Thierry Cruanes. 2020. Building An Elastic Query Engine on\nDisaggregated Storage. In Proceedings of the 17th USENIX Symposium on\nNetworked Systems Design and Implementation (NSDI â€™20) . 449â€“462. https:\n//www.usenix.org/conference/nsdi20/presentation/vuppalapati\n[109] Benjamin Wagner, AndrÃ© Kohn, and Thomas Neumann. 2021. Self-Tuning\nQuery Scheduling for Analytical Workloads. In Proceedings of the International\nConference on Management of Data (SIGMOD â€™21) . 1879â€“1891. https://doi.org/\n10.1145/3448016.3457260\n[110] Jingjing Wang, Tobin Baker, Magdalena Balazinska, Daniel Halperin, Brandon\nHaynes, Bill Howe, Dylan Hutchison, Shrainik Jain, Ryan Maas, Parmita Mehta,\nDominik Moritz, Brandon Myers, Jennifer Ortiz, Dan Suciu, Andrew Whitaker,\nand Shengliang Xu. 2017. The Myria Big Data Management and Analytics\nSystem and Cloud Services. In Proceedings of the Conference on Innovative Data\nSystems Research (CIDR â€™17) .\n[111] Wentao Wu, Yun Chi, Shenghuo Zhu, Junâ€™ichi Tatemura, Hakan HacigÃ¼mÃ¼s,\nand Jeffrey F. Naughton. 2013. Predicting Query Execution Time: Are OptimizerCost Models Really Unusable?. In Proceedings of the 29th IEEE International\nConference on Data Engineering (ICDE â€™13) . 1081â€“1092. https://doi.org/10.1109/\nICDE.2013.6544899\n[112] Ziniu Wu, Ryan Marcus, Zhengchun Liu, Parimarjan Negi, Vikram Nathan, Pas-\ncal Pfeil, Gaurav Saxena, Mohammad Rahman, Balakrishnan Narayanaswamy,\nand Tim Kraska. 2024. Stage: Query Execution Time Prediction in Amazon\nRedshift. In Companion of the 2024 International Conference on Management of\nData (SIGMOD â€™24) . 280â€“294. https://doi.org/10.1145/3626246.3653391\n[113] Ziniu Wu, Parimarjan Negi, Mohammad Alizadeh, Tim Kraska, and Samuel\nMadden. 2023. FactorJoin: A New Cardinality Estimation Framework for Join\nQueries. Proceedings of the ACM on Management of Data 1, 1, Article 41 (2023),\n27 pages. https://doi.org/10.1145/3588721\n[114] Ziniu Wu, Pei Yu, Peilun Yang, Rong Zhu, Yuxing Han, Yaliang Li, Defu Lian, Kai\nZeng, and Jingren Zhou. 2022. A Unified Transferable Model for ML-Enhanced\nDBMS. In Proceedings of the 12th Conference on Innovative Data Systems Research\n(CIDR â€™22) . https://www.cidrdb.org/cidr2022/papers/p6-wu.pdf\n[115] Geoffrey X. Yu, Markos Markakis, Andreas Kipf, Per-Ã…ke Larson, Umar Farooq\nMinhas, and Tim Kraska. 2022. TreeLine: An Update-In-Place Key-Value Store\nfor Modern Storage. Proceedings of the VLDB Endowment 16, 1 (2022), 99â€“112.\n[116] Xiang Yu, Guoliang Li, Chengliang Chai, and Nan Tang. 2020. Reinforcement\nLearning with Tree-LSTM for Join Order Selection. In Proceedings of the 2020\nIEEE 36th International Conference on Data Engineering (ICDE â€™20) . 1297â€“1308.\n[117] Jianqiu Zhang, Kaisong Huang, Tianzheng Wang, and King Lv. 2022. Skeena:\nEfficient and Consistent Cross-Engine Transactions. In Proceedings of the 2022\nInternational Conference on Management of Data (SIGMOD â€™22) . 34â€“48. https:\n//doi.org/10.1145/3514221.3526171\n[118] Xiuwen Zheng, Subhasis Dasgupta, Arun Kumar, and Amarnath Gupta. 2022.\nAWESOME: Empowering Scalable Data Science on Social Media Data with an\nOptimized Tri-Store Data System. arXiv:2112.00833 [cs.DB]\n[119] Rong Zhu, Wei Chen, Bolin Ding, Xingguang Chen, Andreas Pfadler, Ziniu\nWu, and Jingren Zhou. 2023. Lero: A Learning-to-Rank Query Optimizer.\nProceedings of the VLDB Endowment 16, 6 (2023), 1466â€“1479. https://doi.org/10.\n14778/3583140.3583160\n17",
  "textLength": 111413
}