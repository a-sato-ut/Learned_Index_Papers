{
  "paperId": "0ee873806dd6a1711692f23054394dcbc9b607e3",
  "title": "Learned Compression of Nonlinear Time Series with Random Access",
  "pdfPath": "0ee873806dd6a1711692f23054394dcbc9b607e3.pdf",
  "text": "Learned Compression of Nonlinear Time Series\nWith Random Access\nAndrea Guerra*\nUniversity of Pisa\nPisa, ItalyGiorgio Vinciguerra*\nUniversity of Pisa\nPisa, ItalyAntonio Boffa‚Ä†\nEPFL\nLausanne, SwitzerlandPaolo Ferragina‚Ä†\nSant‚ÄôAnna School for Advanced Studies\nPisa, Italy\nAbstract ‚ÄîTime series play a crucial role in many fields, includ-\ning finance, healthcare, industry, and environmental monitoring.\nThe storage and retrieval of time series can be challenging due\nto their unstoppable growth. In fact, these applications often\nsacrifice precious historical data to make room for new data.\nGeneral-purpose compressors like Xz and Zstd can mitigate\nthis problem with their good compression ratios, but they lack\nefficient random access on compressed data, thus preventing real-\ntime analyses. Ad-hoc streaming solutions, instead, typically opti-\nmise only for compression and decompression speed, while giving\nup compression effectiveness and random access functionality.\nFurthermore, all these methods lack awareness of certain special\nregularities of time series, whose trends over time can often be\ndescribed by some linear and nonlinear functions.\nTo address these issues, we introduce NeaTS, a randomly-\naccessible compression scheme that approximates the time series\nwith a sequence of nonlinear functions of different kinds and\nshapes, carefully selected and placed by a partitioning algorithm\nto minimise the space. The approximation residuals are bounded,\nwhich allows storing them in little space and thus recovering the\noriginal data losslessly, or simply discarding them to obtain a\nlossy time series representation with maximum error guarantees.\nOur experiments show that NeaTS improves the compression\nratio of the state-of-the-art lossy compressors that use linear\nor nonlinear functions (or both) by up to 14%. Compared to\nlossless compressors, NeaTS emerges as the only approach to date\nproviding, simultaneously, compression ratios close to or better\nthan the best existing compressors, a much faster decompression\nspeed, and orders of magnitude more efficient random access,\nthus enabling the storage and real-time analysis of massive and\never-growing amounts of (historical) time series data.\nI. I NTRODUCTION\nTime series are pervasive across a multitude of fields, in-\ncluding finance, healthcare, industry, and environmental mon-\nitoring. These sorted sequences of time-stamped data points\nrepresent a wide variety of dynamic phenomena, from market\nprices to patient vitals and sensor readings, and they have\nbecome invaluable for decision-making, trend analysis, and\nforecasting.\nUnsurprisingly, the efficient storage, transmission, and anal-\nysis of time series have become more and more challenging\nas their volume has grown exponentially [1], [2], leading to\nthe development of numerous time series databases [3]‚Äì[7].\nThis work has been accepted for publication in Proceedings of the 41st\nIEEE International Conference on Data Engineering (ICDE 2025)\n*Equal contribution. Correspondence to: andrea.guerra@phd.unipi.it, gior-\ngio.vinciguerra@unipi.it\n‚Ä†Work done while the author was at the University of Pisa.Data compression is the key strategy to lower the cost of\ntime series storage and transmission [8]. The easiest way to\napproach it is to use one of the off-the-shelf general-purpose\ncompressors (such as Brotli [9], Zstd [10], Xz [11], Lz4 [12],\nSnappy [13], etc.). These tools are capable of achieving com-\nmendable compression ratios, but they require a substantial\ncomputational overhead, both in terms of CPU and memory\nusage, which often makes them unsuitable on hardware-\nand energy-constrained devices such as smartphones, smart\nwearable, IoT or edge devices.\nMotivated by this shortcoming, several new special-purpose\ncompressors have been developed for time series, often re-\nducing the computational overhead at the expense of lower\ncompression ratios. Most notably, several works [14]‚Äì[17]\nhave shown how to compress and decompress floating-point\ntime series data much faster than general-purpose compressors,\nenabling both high ingestion rates and efficient scans. How-\never, not much attention has been given to the design and the\nevaluation of the random access operation to single values of\nthe time series [18], [19], even in benchmarking studies [20].\nThis is quite surprising given that the most fundamental\nqueries in time series databases ultimately rely on accessing\ndata within a specific time interval [21], [22], which from\na compressed storage perspective boils down to combining a\nrandom access operation (to retrieve the first data point) with a\nscan (to retrieve the subsequent data points within the interval).\nHowever, providing efficient random access is challenging, and\nit often conflicts with achieving good compression ratios.\nFurthermore, none of the above compressors can harness\na key peculiarity of time series data: its trends over time\ncan often be described by some linear and nonlinear func-\ntions [23], [24]. Indeed, although there is a rich literature on\napproximating and indexing a time series via linear [25]‚Äì[27],\npolynomial and other functions [28]‚Äì[31], or via Fourier and\nwavelet transforms [32], all these methods are lossy [8] and\nthus inapplicable in cases where we need to reconstruct the\noriginal data for accurate analyses. A step in this direction\nhas been made by some learned compressors that are not\nspecifically designed for time series [33]‚Äì[35]. But these\napproaches either exploit linear functions only [33], [34] or\nuse sub-optimal partitioning algorithms and non-error-bounded\napproximations [33], [35], so they fall short of reaching the\nbest possible compression efficacy.arXiv:2412.16266v1  [cs.LG]  20 Dec 2024\n\nOur contribution. We contribute to the long line of research\non time series compression as follows:\n‚Ä¢We show how to compute piecewise approximations using\nseveral kinds of nonlinear functions (such as quadratic,\nradical, exponential, logarithmic, and Gaussian) under a\ngiven error bound Œµoptimally, i.e. in linear time and with\nthe guarantee that the number of pieces is minimised.\nThis generalises the classic algorithm to compute piecewise\nlinear approximations [36].\n‚Ä¢We introduce an algorithm to partition a time series into\nvariable-sized fragments, each associated with a different\nnonlinear approximation, so that the space of the output is\nminimised. This generalises a previous result for increasing\nlinear functions only [34].\n‚Ä¢By combining the above two results with proper suc-\ncinct data structures, we design NeaTS, a new randomly-\naccessible compression scheme that approximates the time\nseries with a sequence of nonlinear functions of different\nkinds and shapes. The residuals of the approximation are\nbounded, which allows storing them in little space and thus\nrecovering the original data losslessly, or discarding them\nto obtain a lossy time series representation with maximum\nerror guarantees. Figure 1 shows an example of NeaTS.\n‚Ä¢We conduct a thorough experimental evaluation on 16 real-\nworld time series, whose size ranges from thousands to\nhundreds of millions of data points, comparing our NeaTS\nagainst 2 lossy compressors, as well as 5 general-purpose\nand 7 special-purpose lossless compressors, including the\nrecent ALP [17] and LeCo [35]. Our results show that the\nlossy version of NeaTS improves uniformly the compression\nratio of previous error-bounded approximations based on\nlinear or nonlinear functions (or both), with an improvement\nof up to 14%. Compared to lossless compressors, NeaTS\nemerges as the only approach to date delivering, simultane-\nously, compression ratios close to or better than the existing\ncompressors (i.e. the best compression ratio among the\nspecial-purpose compressor on 14/16 datasets, and the best\noverall on 4/16 dataset), a much faster decompression speed,\nand up to 3 orders of magnitude more efficient random\naccess. No other compressor to date can achieve such a good\nperformance in one of these factors without significantly\nsacrificing others. We finally show that NeaTS delivers\nsuperior performance across range queries of different sizes,\nthus benefiting the wide variety of queries in time series\ndatabases that access data within specific time intervals.\nOutline. Section II gives the background and definitions.\nSection III introduces our NeaTS. Section IV presents our\nexperimental results. Section V discusses related work. Sec-\ntion VI concludes the paper and suggests some open problems.\nII. B ACKGROUND\nWe now provide some background information, starting\nwith a definition of the data we compress.\nDefinition 1 (Time series) .Atime series Tis a sequence\nofndata points of the form (xk, yk), where xk‚ààNis the\n123456789020406080100\n‚àíùúÄ‚â§ùë¶2‚àí‚åäùëöùë•2+ùëû‚åã‚â§ùúÄ\n‚àíùúÄ‚Ä≤‚â§ùë¶6‚àí‚åäùëéùë•2\n6+ùëèùë•6+ùëê‚åã‚â§ùúÄ‚Ä≤\nTime ( ùë•)Value ( ùë¶)-1-246\n1 2 3 4log(2ùúÄ+1)bits\n083-6-11\n5 6 7 8 9log(2ùúÄ‚Ä≤+1)bits\n1Fig. 1: NeaTS represents fragments of the time series via linear\nor nonlinear functions learned from the data. The residuals of the\napproximation are bounded by a value Œµso, if lossless compression\nis needed, we store them in packed arrays (shown on top).\ntimestamp, and yk‚ààZis the value associated with it, ordered\nincreasingly by time, i.e. T= [(x1, y1),(x2, y2), . . . , (xn, yn)]\nwhere x1< x2<¬∑¬∑¬∑< xn. Afragment ofTis a subsequence\nT[i, j] = [( xi, yi), . . . , (xj, yj)]for any two indexes i, jsuch\nthat1‚â§i‚â§j‚â§n.\nWe require the values to be integers, which is common in\npractice. In fact, single/double-precision floating-point values\ncan be interpreted as 32/64-bit integers, or better, since the\nvalues in real-world time series typically have a fixed number\nxof significant digits after the decimal point, we can multiply\nthem by the constant 10xand turn them into integers [19].\nWe focus on functional approximations for time series but,\nunlike known approaches [8], we use them to design not only\na lossy compressor but also a lossless one.\nTo illustrate, consider a time series Ton a Cartesian plane\nwhere the horizontal axis represents time and the vertical\naxis represents values. Any function fpassing through all\nthe data points (x1, y1), . . . , (xn, yn)is a lossless encoding\nofTbecause, for a given timestamp xk, we can recover the\ncorresponding value as yk=f(xk), thus requiring us to store\nonly the (parameters of the) function f. However, the number\nof parameters of a function passing exactly through all the data\npoints could be so large to result in no compression (consider\ne.g. a polynomial interpolation of the ndata points, which\ngenerally requires storing ncoefficients). Therefore, we allow\nfto make some ‚Äúerrors‚Äù but in a controlled way, namely, we\nbound the infinity norm of the errors.\nDefinition 2 (Œµ-approximations) .LetŒµ‚â•0be an integer. A\nfunction fis said to be an Œµ-approximation of a time series\nT(or fragment T[i, j]) if we have |f(xk)‚àíyk| ‚â§Œµfor every\ndata point (xk, yk)inT(resp. T[i, j]).\nA function fdefined in this way is a lossy representation\nofT. To make it lossless, as observed in [34], it is sufficient to\nalso store the ‚Äúcorrections‚Äù (i.e. residuals) ck=yk‚àí‚åäf(xk)‚åã\nin‚åàlog(2Œµ+1)‚åâbits each, which thus allows recovering ykas\n‚åäf(xk)‚åã+ck. Intuitively, the smaller the value of Œµ, the less\n\nspace is needed to store the corrections; however, at the same\ntime, more space may be required to store the parameters of f,\ndue to it being more complex to better fit the data points.\nIn this scenario, there are two issues to solve. The first is\nhow to compute a function that Œµ-approximates (a fragment\nof)T. The second is how to choose Œµso that the storage of\nboth the function fand the corrections takes minimal space,\npossibly using a different Œµ-value for different fragments of T.\nIn the case fis a linear function, these two issues have already\nbeen solved [34], [36].\nWe now recall how to compute a linear Œµ-approximation,\nwhich will be the starting point of our extension to nonlinear\nfunctions. Given an integer Œµ‚â•0and an index iinto a time\nseries T, O‚ÄôRourke‚Äôs algorithm [36] finds the longest fragment\nT[i, j]that admits a linear Œµ-approximation f(x) =mx+bin\noptimal O(j‚àíi)time.1The algorithm works by maintaining\na setPof feasible coefficients in the 2D space with mon the\nhorizontal axis and bon the vertical axis, and by processing the\ndata points in T[i, j]left-to-right by shrinking |P|at each data\npoint. Regarding P, notice that, according to Definition 2, the\nlinear function fmust satisfy the inequality |f(xk)‚àíyk|=\n|mxk+b‚àíyk| ‚â§Œµfor every k=i, . . . , j , which can be\nrewritten as b‚â§(‚àíxk)m+yk+Œµandb‚â•(‚àíxk)m+yk‚àíŒµ.\nTherefore, Pis a convex polygon in the 2D space defined by\nthe intersection of the 2(j‚àíi+ 1) half-planes arising from\nthese two inequalities for k=i, . . . , j .\nWhen adding the (j+ 1)th data point would cause Pto be\nempty (i.e. the fragment T[i, j]cannot be made any longer),\nthe algorithm stops and picks a pair (m, b)‚ààPas the\ncoefficients of the linear function fthatŒµ-approximates the\nfragment T[i, j].\nLet us call Pkthe polygon at a generic step k. Crucial for\nthe time-efficiency of the algorithm is the fact that each edge\nofPkhas a slope that lies in the range [‚àíxk,0), which is easy\nto see since each half-plane defining Pkhas slope of the form\n‚àíxl(due to the above inequalities) and that 0< x l< x k\nforl < k (due to Definition 1). This fact allows updating Pk\nwith the inequalities arising from the (k+ 1) th data point in\nconstant amortised time.\nWe refer the reader to the seminal paper [36] for more\ndetails and anticipate that we will generalise this algorithm\nto work with several kinds of nonlinear functions.\nIII. T HENEATS COMPRESSOR\nWe now introduce our compression scheme for time series.\nWe call it NeaTS (N onlinear e rror-bounded a pproximation\nfor T ime S eries) since it exploits the potential of nonlinear\nfunctions to compress time series. We design it in three steps.\n1) We describe how to compute an Œµ-approximation for a\nfragment of the time series using several kinds of nonlinear\nfunctions, such as quadratic, radical, exponential, logarith-\nmic, and Gaussian functions (Section III-A).\n1More recent papers address this same problem [25]‚Äì[27], [37], [38],\nsometimes proposing algorithms that are equivalent, or sub-optimal in terms\nof time complexity, or that find shorter fragments compared to the earlier\nalgorithm by O‚ÄôRourke [36] we consider here.2) We introduce an algorithm to divide the input time series\ninto variable-sized fragments, each with an associated\nnonlinear approximation with a different Œµ-value, so that\nthe space of the compressed output is minimised. While\nour focus is on lossless compression, we also show how\nto adapt this algorithm to produce a (lossy) piecewise\nnonlinear Œµ-approximation of a time series in linear time\n(Section III-B).\n3) We show how to support random access to individual\nvalues of the time series by combining the compressed\noutput with proper succinct data structures (Section III-C).\nA. Computing a nonlinear Œµ-approximation\nLinear functions have surely been the most widely used\nrepresentations for time series due to their simplicity [25]‚Äì\n[27], [37], [38]. Nonetheless, they may not always be the best\nchoice to approximate a time series because of the possible\npresence of nonlinear patterns in real-world data [23], [24].\nLet us be given an integer Œµ‚â•0and an index iinto a\ntime series T. We now show how to find the longest fragment\nT[i, j]that admits an Œµ-approximation for some kinds of\nnonlinear functions with two parameters Œ∏1andŒ∏2. We do\nso by generalising the classic algorithm by O‚ÄôRourke [36] for\nlinear functions (recalled in Section II).\nWe start with exponential functions of the form f(x) =\nŒ∏2eŒ∏1x. According to Definition 2, we must ensure that f\nsatisfies the inequality |f(xk)‚àíyk|=|Œ∏2eŒ∏1xk‚àíyk| ‚â§Œµ\nfor every data point k=i, . . . , j , which can be rewritten as\nlnŒ∏2‚â§(‚àíxk)Œ∏1+ ln( yk+Œµ)\nlnŒ∏2‚â•(‚àíxk)Œ∏1+ ln( yk‚àíŒµ),\nunder the assumption yk‚àíŒµ >0.2\nThe above inequalities define a pair of half-planes in the\nCartesian plane with Œ∏1on the horizontal axis and lnŒ∏2on\nthe vertical axis. Therefore, their intersection for k=i, . . . , j\noriginates a convex polygon of feasible parameters for the\nexponential function f. Since the slope of each polygon edge\nis‚àíxk, a direct reduction to the algorithm by O‚ÄôRourke gives\nan optimal O(j‚àíi)-time algorithm to compute an exponential\nŒµ-approximation of T[i, j].\nIn general, we can show the following.\nTheorem 1. LetT= [(x1, y1), . . . , (xn, yn)]be a time series,\nletŒµ‚â•0be an integer, and let fbe a function with two\nparameters Œ∏1andŒ∏2. If, for any k, the inequalities ‚àíŒµ‚â§\nf(xk)‚àíyk‚â§Œµcan be transformed into inequalities of the\nform Œ±k‚â§tkm+b‚â§œâk, where:\n1)Œ±k,tk, and œâkcan be computed in constant time from Œµ\nandT[k];\n2)mandbare derived from Œ∏1andŒ∏2, respectively, via a\nchange of variable, i.e. m=œï(Œ∏1)andb=œà(Œ∏2)for\nsome invertible functions œïandœà;\n3)tkis a positive increasing function of xk.\nThen, for any i, we can compute the longest fragment T[i, j]\nthat admits an Œµ-approximation fin optimal O(j‚àíi)time.\n2If not satisfied, just add Œµ+ 1‚àíminkykto all yks in the time series.\n\nTABLE I: Some examples of two-parameter functions fthat we\ncan use in Theorem 1, together with the terms m, b, t k, Œ±kandœâk\ndefining the transformed inequalities.\nf(x) m b t k Œ±k œâk\nŒ∏2eŒ∏1xŒ∏1lnŒ∏2 xk ln(yk‚àíŒµ) ln( yk+Œµ)\nŒ∏2xŒ∏1 Œ∏1lnŒ∏2lnxk ln(yk‚àíŒµ) ln( yk+Œµ)\nln(Œ∏2xŒ∏1) Œ∏1lnŒ∏2lnxk yk‚àíŒµ y k+Œµ\nŒ∏1x+Œ∏2 Œ∏1 Œ∏2 xk yk‚àíŒµ y k+Œµ\nŒ∏1‚àöx+Œ∏2 Œ∏1 Œ∏2‚àöxk yk‚àíŒµ y k+Œµ\nŒ∏1x2+Œ∏2 Œ∏1 Œ∏2 x2\nkyk‚àíŒµ y k+Œµ\nŒ∏1x2+Œ∏2x Œ∏ 1 Œ∏2 xk (yk‚àíŒµ)/xk(yk+Œµ)/xk\nŒ∏1x3+Œ∏2x Œ∏ 1 Œ∏2 x2\nk(yk‚àíŒµ)/xk(yk+Œµ)/xk\nŒ∏1x3+Œ∏2x2Œ∏1 Œ∏2 xk (yk‚àíŒµ)/x2\nk(yk+Œµ)/x2\nk\nProof. We reduce the computation of the feasible parameters\nŒ∏1andŒ∏2of the (possibly nonlinear) function fto the\nintersection of half-planes in the 2D space with mon the\nhorizontal axis and bon the vertical axis, for which we\ncan use the algorithm of O‚ÄôRourke [36]. Using the same\nnotation as in [36], let us rewrite the transformed inequalities\nasb‚â•(‚àítk)m+Œ±kandb‚â§(‚àítk)m+œâk. These inequalities\nclearly represent half-planes in that 2D space.\nNow, let Pkbe the convex polygon resulting from the\nintersection of these inequalities where the subscript ranges\nasi, i+ 1, . . . , k . Analogously to [36, Lemma 1], we need\nto establish the property that the slope of each edge of Pk\nbelongs to [‚àítk,0). The polygon is specified by edges of the\nform b= (‚àítl)m+Œ±landb= (‚àítl)m+œâlforl=i, . . . , k .\nBy assumption (3), the value tlis the result of applying a\npositive increasing function to xl. This, combined with the\nfact that 0< x l< x k(by Definition 1), implies that each\nedge of Pkhas slope ‚àítl‚â• ‚àítkand that ‚àítl<0, thus\neach slope belongs to [‚àítk,0). This property is enough to\nguarantee the correctness and time complexity of the algorithm\nthat maintains Pk(cf. proofs of [36, Theorems 1 and 2]).\nWe thus conclude by observing that once the next data point\nT[j+ 1] causes the polygon to be empty, we choose a pair\n(m, b)‚ààPjand return Œ∏1=œï‚àí1(m)andŒ∏2=œà‚àí1(b)as the\nparameters of f.\nTaking again as an example exponential functions of the\nform f(x) =Œ∏2eŒ∏1x, we first transform the inequalities ‚àíŒµ‚â§\nf(xk)‚àíyk‚â§Œµvia simple algebraic manipulations to\nln(yk‚àíŒµ)|{z}\nŒ±k‚â§xk|{z}\ntkŒ∏1|{z}\nm+ lnŒ∏2|{z}\nb‚â§ln(yk+Œµ)|{z}\nœâk,\nand then we apply Theorem 1, which gives the desired\nexponential Œµ-approximation for a fragment T[i, j]in optimal\nO(j‚àíi)time.3Table I shows other examples with linear,\nexponential, power, logarithmic, and radical functions.4\n3The logarithm and other operations can be computed in constant time with\nmild assumptions on the model of computation [39].\n4It is straightforward (and sometimes useful to better approximate the\ndata) to compute a function whose graph is horizontally shifted to the first\ntimestamp xiofT[i, j]: we simply store xi, subtract it from the timestamps\ninT[i, j], then we apply Theorem 1 to compute a function gwith domain\n[0, xj‚àíxi]and output f(x) =g(x‚àíxi).In some cases, we can use Theorem 1 even for functions f\nwith three parameters, provided that we add some constraints\nto reduce the number of free parameters to two, since other-\nwise the set of feasible parameters for fbecomes a polyhedron\nPkin a 3D space (i.e. one dimension for each parameter),\nwhich we cannot handle in linear time [30], [31], [40], [41].\nTake, for example, quadratic functions of the form f(x) =\nŒ∏1x2+Œ∏2x+Œ∏3. By forcing the function to pass through\nthe first data point T[i], i.e. by setting f(xi) =yiand thus\nfixing Œ∏3=yi‚àíŒ∏1x2\ni‚àíŒ∏2xi(which we store explicitly),\nwe can transform the inequalities ‚àíŒµ‚â§f(xk)‚àíyk‚â§Œµvia\nsimple algebraic manipulations to\nyk‚àíyi‚àíŒµ\nxk‚àíxi|{z}\nŒ±k‚â§(xk+xi)\n|{z}\ntkŒ∏1\n|{z}\nm+Œ∏2\n|{z}\nb‚â§yk‚àíyi+Œµ\nxk‚àíxi|{z}\nœâk,\nand then we apply Theorem 1. A similar derivation can\nbe done for Gaussian-like functions of the form f(x) =\neŒ∏1x2+Œ∏2x+Œ∏3.\nWe conclude this section by observing that a repeated appli-\ncation of Theorem 1 from T[1]toT[n]allows partitioning T\ninto the longest fragments associated with an Œµ-approximation,\nthus giving the following result.\nCorollary 1. Given a time series T= [(x1, y1), . . . , (xn, yn)],\na value Œµ‚â•0, and a function fsatisfying the assumptions\nof Theorem 1, we can compute a piecewise Œµ-approximation\nofTwith the smallest number of functions of the f-kind in\nO(n)time.\nCorollary 1 directly yields a lossy error-bounded (in terms\nof infinity norm) representation of T. As discussed in Sec-\ntion II, this can be made lossless by storing the corrections\nyk‚àí‚åäf(xk)‚åãin‚åàlog(2Œµ+1)‚åâbits each. In the next section, we\ndescribe a more powerful partitioning algorithm to orchestrate\ndifferent types of nonlinear functions and error bounds.\nB. Partitioning a time series with nonlinear Œµ-approximations\nLet us be given a set Fof functions that satisfy the\nassumptions of Theorem 1, and a set Eof error bounds. We\nnow turn our attention to the problem of partitioning a time\nseries Tinto fragments, each Œµ-approximated (with Œµ‚àà E)\nby a function from F, with the goal of minimising the overall\nspace of the lossless representation of T, which is given by the\nstorage of the corrections and the parameters of the functions.\nAt a high level, our approach computes, for each f‚àà F and\neachŒµ‚àà E, the piecewise Œµ-approximation of Tcomposed of\nfunctions of the f-kind, and then produces the desired partition\nofTby stitching together properly-chosen fragments (possibly\nadjusting their start and end points) taken from the |F| ¬∑ |E|\ndifferent piecewise approximations of T. This generalises a\nprevious result for increasing linear functions only [34]\nMore in detail, we define a graph Gwith one node for each\ndata point in T, plus one sink node denoting the end of the\ntime series. Each fragment T[i, j‚àí1]that is Œµ-approximated\nby a function f‚àà F produces an edge (i, j)ofGwhose\nweight wf,Œµ(i, j)is defined as the bit-size of the compression\n\nofT[i, j‚àí1]viafand the j‚àíicorrections stored in\n‚åàlog(2Œµ+ 1)‚åâ-bits each, i.e. wf,Œµ(i, j) = (j‚àíi)‚åàlog(2Œµ+1)‚åâ+\nŒ∫f, where Œ∫fis the space in bits taken by the parameters of f\n(plus some small metadata, such as the function kind, encoded\nas an index from {1, . . . ,|F|}). Moreover, since fis also an\nŒµ-approximation of any prefix and suffix of T[i, j‚àí1], other\nthan the edge (i, j)we add to Galso the prefix edge (i, k)\nand the suffix edge (k, j), for all k=i, . . . , j ‚àí1[34]. It is\nnot difficult to conclude that the shortest path from node 1to\nnode n+ 1gives the desired partition of T.\nIt is well-known that, in the case of a directed acyclic graph\n(likeG), the shortest path can be computed by taking the\nnodes in order 1, . . . , n and relaxing their outgoing edges, i.e.\nchecking whether these edges can improve the shortest path\nfound so far [42]. Furthermore, generalising what has been\ndone in [34], instead of precomputing all the |F|¬∑|E| different\npiecewise Œµ-approximations, we only keep track of the |F|¬∑|E|\nedges of the form (i, j)that overlap the currently visited node\nk, i.e. i‚â§k < j , and split them on-the-fly into prefix and\nsuffix edges of the forms (i, k)and(k, j), respectively.\nAlgorithm 1 formalises this description. We use distance [k]\nto store an upper bound on the cost of the shortest path from\nnode 1 to k, and previous [k]to store the previous node and\ncorresponding fragment in the shortest path. We use Jf,Œµto\nkeep track of the start/end positions of the fragment overlap-\npingkand the parameters of the corresponding function of the\nf-kind that Œµ-approximates it. We initialise and update Jf,Œµ\nin Line 10 with a call to M AKEAPPROXIMATION (T, k, f, Œµ ),\nwhich runs the algorithm of Theorem 1 starting from the data\npoint T[k]. Lines 12‚Äì15 and 17‚Äì20 relax prefix and suffix\nedges, respectively, and Lines 21‚Äì26 conclude the algorithm\nby reading and returning the shortest path.\nComplexity analysis: We now discuss the time complex-\nity of Algorithm 1. For a fixed fandŒµ, the overall contribution\nof Line 10 to the time complexity is O(n), since it eventually\ncomputes via Theorem 1 the piecewise Œµ-approximation of T\ncomposed of a function of the f-kind. Since there are |F|¬∑|E|\npossible pairs of fandŒµ, the overall computation of piecewise\napproximations takes O(|F||E| n)time. It is easy to see that\nthe relaxation of all the prefix and suffix edges runs within that\nsame asymptotic time bound, thus the overall time complexity\nof Algorithm 1 is O(|F||E| n).\nConcerning |F|, we can assume that a real-world time series\ncan be approximated well by a fixed number of function kinds,\nsuch as those in Table I, and thus it holds |F|=O(1). Con-\ncerning |E|, instead, we can pessimistically bound it as follows.\nLet‚àÜbe one plus the difference between the maximum value\nand the minimum value ÀÜyinT. Then, each value ykofT\ncan be stored in ‚åàlog ‚àÜ‚åâbits by just encoding the binary\nrepresentation of yk‚àíÀÜy. This, in turn, entails that we can\nrestrict our attention to the set E={0,21, . . . , 2‚åàlog ‚àÜ‚åâ}, since\nhigher values of Œµwould not pay off, i.e. even the most trivial\nconstant function can Œµ-approximate the whole time series.\nGiven that such a set Ehas size O(log ‚àÜ) , the time complexity\nof Algorithm 1 under these conditions is O(nlog ‚àÜ) .\nThe average value of log ‚àÜ for the diverse dataset we useAlgorithm 1 Partitioning a time series with NeaTS.\nIn:Time series T[1, n], setFof functions, set Eof error\nbounds\nOut: A partitioning of Tinto fragments, each associated with\nanŒµ-approximation f(with Œµ‚àà E,f‚àà F), that minimises\nthe size of the NeaTS encoding of T\n1:distance [1, n+ 1]‚Üê[‚àû, . . . ,‚àû]\n2:previous [1, n+ 1]‚Üê[NULL, . . . , NULL]\n3:for all (f, Œµ)‚àà F √ó E do‚ñ∑Initialise edges and functions\n4:Jf,Œµ.start‚Üê ‚àí‚àû\n5:Jf,Œµ.end‚Üê ‚àí‚àû\n6:Jf,Œµ.params ‚ÜêNULL\n7:fork‚Üê1tondo\n8:for all (f, Œµ)‚àà F √ó E do\n9: ifJf,Œµ.end‚â§kthen ‚ñ∑A new edge overlaps k\n10: Jf,Œµ‚ÜêMAKEAPPROXIMATION (T, k, f, Œµ )\n11: else\n12: i‚ÜêJf,Œµ.start ‚ñ∑Relax prefix edge (i, k)\n13: ifdistance [k]>distance [i] +wf,Œµ(i, k)then\n14: distance [k]‚Üêdistance [i] +wf,Œµ(i, k)\n15: previous [k]‚Üê(i, Jf,Œµ)\n16: for all (f, Œµ)‚àà F √ó E do\n17: j‚ÜêJf,Œµ.end ‚ñ∑Relax suffix edge (k, j)\n18: ifdistance [j]>distance [k] +wf,Œµ(k, j)then\n19: distance [j]‚Üêdistance [k] +wf,Œµ(k, j)\n20: previous [j]‚Üê(k, Jf,Œµ)\n21:result ‚Üêan empty dynamic array\n22:k‚Üên+ 1\n23:while kÃ∏= 1 do ‚ñ∑Read the shortest path backwards\n24: result .PUSHFRONT (previous [k])\n25: k‚Üêthe first element of previous [k]\n26:return result\nin Section IV is 28.8, which is a small constant. Moreover,\nwe do not actually need to use all F √óE pairs in Algorithm 1\nbut rather those surviving a model-selection procedure. For\ninstance, we can initially run Algorithm 1 on a small sample\nofT(chosen e.g. according to the seasonality of T) and\nselect just the pairs that are used in the result, as these are\nlikely to be effective and enough for the whole time series\ntoo. Our experiments show that this model-selection procedure\nimproves the compression speed by an order of magnitude,\nwith little impact on the compression ratio (see Section IV-C1).\nPartitioning for lossy compression: We can easily mod-\nify Algorithm 1 to obtain a lossy representation of the time\nseries Twith a given Œµ-bound on the error, still using functions\nfrom a given set Fand minimising the space, which is given\nthis time by just the storage of the functions‚Äô parameters (since\nwe drop the corrections). It is enough to set E={Œµ}and\ndefine the edge weight wf(i, j)to be equal to the space in\nbits taken by the parameters of f. The resulting algorithm\nruns in O(|F|n), so in linear time if |F|=O(1).\nOur experiments will show that, for a fixed Œµ-bound, this\nalgorithm produces more succinct lossy representations of time\n\nseries than known algorithms based on linear or nonlinear\nfunctions (namely, the algorithm by O‚ÄôRourke [36] and the\nAdaptive Approximation algorithm [30], [31], respectively).\nC. Designing the NeaTS compressor\nWe now describe the layout of the compressed time series\nand how to support the random access operation. As common\nin the literature [8], [14]‚Äì[17], we focus on the storage of the\nvalues y1, . . . , y nand assume the timestamps are 1, . . . , n .5\nLet us assume that the output of Algorithm 1 is a sequence\nofmtuples having the form ‚ü®fi,paramsi, Œµi,start i,endi‚ü©,\nwhere each tuple indicates a fragment T[start i,endi]of\nT[1, n]that is Œµi-approximated by a function of kind fi‚àà F\nwith parameters paramsi. We encode these mtuples and the\nvalues in their corresponding time series fragments via:\n‚Ä¢An integer array S[1, m]storing in S[i]the starting position\nof the ith fragment, i.e. S[i] =start i. To obtain the index of\nthe fragment that covers a certain data point T[k], we use the\nS.rank(k)operation, which returns the number of elements\ninSthat are smaller than or equal to k. Since Sis an in-\ncreasing integer sequence, we compress it via the Elias-Fano\nencoding [45], [46], which supports accessing an element in\nO(1)time and S.rank inO(min(log m,logn\nm))time [47].\n‚Ä¢An integer array B[1, m]storing in B[i]the bit size of the\ncorrections of the ith fragment, i.e. B[i] =‚åàlog(2Œµi+ 1)‚åâ.\n‚Ä¢An integer array O[1, m+ 1] storing in O[i]the cumulative\nbit size of the corrections in the fragments preceding the\nith one, i.e. O[i] =Pi‚àí1\nj=1B[j] (endj‚àístart j+ 1) . Notice\nthatO[m+1] denotes the overall bit size of the corrections.\nSimilarly to S, we compress Ovia the Elias-Fano encoding.\n‚Ä¢A bit string C[1, O[m+1]] storing in C[O[i], O[i+1]‚àí1]the\ncorrection values yj‚àí ‚åäfi(xj)‚åãof the ith fragment, where\nj‚àà[start i,endi].\n‚Ä¢An integer array K[1, m]storing in K[i]the function kind\nfor the ith fragment, i.e. K[i] = fi. We regard Kas a\nstring over the alphabet {1, . . . ,|F|} and represent it as\na wavelet tree data structure [47], [48]. This allows us\nto compute the K.rank f(i)operation, which returns the\nnumber of occurrences of the function kind finK[1, i]\ninO(log|F|) =O(1)time.\n‚Ä¢For each f‚àà F, an array Pfconcatenating the parameters\nparamsiof the functions of the same kind f. This way, the\nparameters paramsiof the ith fragment can be found in\nPfi[K.rank fi(i)].\nAll the above arrays use cells whose bit size is just enough\nto contain the largest value stored in them. If Fcontains\nfunctions with the same number of parameters (recall from\nSection III-A that we can use functions with more than two\nparameters), we can simplify the above encoding by avoiding\nthe use of a wavelet tree for Kand by concatenating all the\n5The timestamps x1, . . . , x nform an increasing sequence of integers\nthat can be easily mapped to 1, . . . , n via monotone minimal perfect hash\nfunctions [43] or compressed rank data structures [34], [44]: the former are\nvery succinct (about 3 bits per integer), the latter take more space but enable\nrange queries over timestamps.Algorithm 2 Full decompression in NeaTS.\nIn:The NeaTS encoding ‚ü®S, B, O, C, K, P ‚ü©ofT\nOut: The uncompressed values of T\n1:o‚Üê1 ‚ñ∑Bit-offset to the correction\n2:fori‚Üê1tomdo ‚ñ∑For each fragment\n3: start‚ÜêS[i] ‚ñ∑First data point index\n4: end‚ÜêS[i+ 1]‚àí1 ‚ñ∑Last data point index\n5:f‚ÜêK[i] ‚ñ∑Function kind\n6: params ‚ÜêPf[K.rank f(i)] ‚ñ∑Function parameters\n7:b‚ÜêB[i] ‚ñ∑Correction bit size\n8:fork‚Üêstart toenddo\n9: Àúy‚Üêcompute ‚åäf(k)‚åãusing params\n10: output Àúy+int(C[o, o+b‚àí1])\n11: o‚Üêo+b\nAlgorithm 3 Random access in NeaTS.\nIn:An index k, the NeaTS encoding ‚ü®S, B, O, C, K, P ‚ü©ofT\nOut: The value of T[k]\n1:i‚ÜêS.rank(k) ‚ñ∑Index of the fragment\n2:start‚ÜêS[i] ‚ñ∑First data point index\n3:f‚ÜêK[i] ‚ñ∑Function kind\n4:params ‚ÜêPf[K.rank f(i)] ‚ñ∑Function parameters\n5:b‚ÜêB[i] ‚ñ∑Correction bit size\n6:Àúy‚Üêcompute ‚åäf(k)‚åãusing params\n7:o‚ÜêO[i] + (k‚àístart)b ‚ñ∑ Bit-offset to the correction\n8:return Àúy+int(C[o, o+b‚àí1])\nfunctions‚Äô parameters Pfinto a single array, which is accessed\nsimply through the index of the queried fragment.\nHaving defined how we represent Tin compressed form via\na tuple ‚ü®S, B, O, C, K, P ‚ü©of data structures, we are now ready\nto discuss the decompression and random access operations.\nAlgorithm 2 shows how to decompress the whole time\nseries. For each fragment, we first decode the associated\nboundaries and kind of approximation function (Lines 3‚Äì7),\nand then we output all the values ykwithin the fragment‚Äôs\nboundaries by applying the function to index kand adding the\ncorresponding correction value (Lines 8‚Äì11). It is easy to see\nthat the time complexity of Algorithm 2 is O(n)given that\nall the involved operations take constant time. Furthermore,\nsince each data point is decompressed independently from\nthe others, the algorithm could be parallelised trivially by\ndecompressing different fragments with different workers, and\nthe computation of the function within a fragment could be\nimplemented via SIMD instructions.\nAlgorithm 3 shows how to perform the random access oper-\nation to the value of T[k], for a given index k‚àà {1,2, . . . , n }.\nWe start by identifying the index of the fragment where T[k]\nfalls into (Line 1), then we decode the index of the first\ndata point in that fragment and the (kind and parameters\nof) function associated with that fragment (Lines 2‚Äì5), and\nfinally we apply the function to position kand add the corre-\nsponding correction value (Lines 6‚Äì8). The time complexity\nof Algorithm 3 is dominated by the operation S.rank at\n\nLine 1, which takes O(min(log m,logn\nm))time. We can easily\nachieve O(1)time by representing Sas a bitvector of length n\nwith a 1in each position start i, and then using the well-known\nconstant-time rank /select operations [49], [50].\nIV. E XPERIMENTS\nA. Experimental setting\nWe run our experiments on a machine with 1.17 TiB of\nRAM and an Intel Xeon Gold 6140M CPU, running CentOS 7.\nOur code is in C ++23, compiled with GCC 13.2.1, and pub-\nlicly available at https://github.com/and-gue/NeaTS. We refer\nto our lossy and lossless approaches as NeaTS-L and NeaTS,\nrespectively. We use four types of functions ‚Äî namely, linear,\nexponential, quadratic, and radical ‚Äî which turned out to be\nsufficient to capture the trends in our real-world datasets well.\nWe use vector instructions in our decompression procedures\nvia the std::experimental::simd library, and succinct\ndata structures from the sdsl [51] and sux [52] libraries.\n1) Datasets: We use 16 real-world time series datasets out\nof which 13 were sourced by Chimp [15], and the remaining\n3 were obtained from the Geolife project [53] and a study on\narrhythmia [54], [55]. Consistent with previous studies [14]‚Äì\n[17], we ignore timestamps as they are either consecutive\nincreasing integers or can be transformed into such with other\nad hoc data structures (see Footnote 5). All the datasets report\nvalues in textual fixed-precision format, therefore, unless the\ncompressor is designed for doubles, we transform them into\n64-bit integers by multiplying each value by a factor 10x,\nwhere xis the number of fractional digits.\n-IR-bio-temp (IT) [56] contains about 477M biological\ntemperature observations from an infrared sensor, with 2\nfractional digits.\n-Stocks-USA (US) ,Stocks-UK (UK) , and Stocks-DE\n(GE) [57] contain about 282M, 59M, and 43M stock\nexchange prices of USA, UK, and Germany, with 2, 1, and\n3 fractional digits, respectively.\n-Electrocardiogram (ECG) [54], [55] contains about 226M\nelectrocardiogram signals of over 45K patients, with 3\nfractional digits.\n-Wind-direction (WD) [58] contains about 199M wind\ndirection observations, with 2 fractional digits.\n-Air-pressure (AP) [59] contains about 138M timestamped\nvalues of barometric pressure corrected to sea level and\nsurface level, with 5 fractional digits.\n-Geolife-longitude (LON) , and Geolife-latitude (LAT) [53]\ncontain about 25M timestamped longitude and latitude val-\nues of 182 users‚Äô GPS trajectories, with 4 fractional digits.\n-Dewpoint-temp (DP) [60] contains about 5M relative dew\npoint temperature observations, with 3 fractional digits.\n-City-temp (CT) [61] contains about 3M temperature obser-\nvations of cities around the world, with 1 fractional digit.\n-PM10-dust (DU) [62] contains about 334K measurements\nof PM10 in the atmosphere, with 3 fractional digits.\n-Basel-wind (BW) , and Basel-temp (BT) [63] contain about\n130K records of wind speed and temperature data of Basel\n(Switzerland), with 7 and 9 fractional digits, respectively.-Bird-migration (BM) [64] contains about 18K positions of\nbirds, with 5 fractional digits.\n-Bitcoin-price (BP) [64] contains about 7K prices of Bitcoin\nin the dollar exchange rate, with 4 fractional digits.\n2) Competitors: Regarding the lossy compressors, we com-\npare our NeaTS-L against 2 functional approximation algo-\nrithms: the optimal Piecewise Linear Approximation algo-\nrithm (PLA) [36], and the Adaptive Approximation algorithm\n(AA) [30], [31] that combines linear, exponential and quadratic\nfunctions. We implemented the PLA and the AA algorithms\nin C++since their code is not publicly available.\nRegarding the lossless compressors, we compare our NeaTS\nagainst 5 widely-used general-purpose compressors ‚Äî namely,\nXz [11], Brotli [9], Zstd [10], Lz4 [12], and Snappy [13] ‚Äî\nand 7 state-of-the-art special-purpose compressors ‚Äî namely,\nChimp and Chimp128 [15], TSXor [65], DAC [66], Go-\nrilla [14], LeCo [35], and ALP [17]. We use the Squash\nlibrary [67] for all the general-purpose compressors. We use\nthe public implementations of TSXor and Gorilla available in\nthe repository of [65], the implementation of DAC available\ninsdsl [51], and the original implementations of LeCo and\nALP. We ported Chimp and Chimp128 to C ++since their\noriginal implementations are in Java.\nFollowing [15], [16], we apply compressors that do not\nnatively support random access (thus excluding DAC, LeCo,\nand NeaTS) to blocks of 1000 consecutive values. We then\nmaintain an array that maps each block index to a pointer refer-\nencing the starting byte of the block in the compressed output.\nB. On the lossy compressors\nTo evaluate the lossy compressors with a meaningful error-\nbound parameter Œµ, we determined the smallest Œµsuch that\nNeaTS-L achieves better compression than our lossless com-\npressor NeaTS. We express the resulting Œµas a % of the range\nof values (i.e. largest minus smallest value) in a dataset, and\nthe compression ratio as the size of the compressed output\ndivided by the size of the original data.\nThe results in Table II show that NeaTS-L outperforms in\ncompression ratio both the PLA and the AA algorithms on\nall datasets. On average, NeaTS-L improves the compression\nratio of PLA by 7.02% and the one of AA by 11.77%.\nThis demonstrates that, under the same Œµ-bound, the use\nof nonlinear functions allows achieving better compression\ncompared to linear functions alone (as in the widely-used\nPLA). In turn, despite employing nonlinear approximations,\nAA is worse than PLA for nearly all datasets due to the\nuse of a heuristic technique to partition the time series into\nfragments and of a sub-optimal algorithm for (non)linear Œµ-\napproximations, two issues that our NeaTS-L solve.\nFor the approximation accuracy, we report that the Mean\nAbsolute Percentage Error (MAPE) ‚Äî i.e. the mean of the\nabsolute relative errors between the approximated and the\nactual values, expressed as a percentage ‚Äî is 2.47% for AA,\n2.85% for NeaTS-L, and 4.37% for PLA (on average over all\ndatasets). Therefore, NeaTS-L has a much better accuracy than\nPLA and a slightly worse accuracy than AA. This is because\n\nTABLE II: Compression ratios of the 3 experimented lossy ap-\nproaches ‚Äî i.e. AA, PLA, and NeaTS-L ‚Äî on the 16 datasets.\nDataset Œµ(%)Compression ratio (%) NeaTS-L improv. (%)\nAA PLA NeaTS-L wrt AA wrt PLA\nIT 1.15E-1 12.11 12.07 11.07 8.57 8.29\nUS 2.40E-3 7.96 7.41 6.99 12.09 5.65\nECG 5.43E-2 15.03 13.46 12.97 13.71 3.64\nWD 6.36E-0 28.09 26.94 24.76 11.88 8.11\nAP 3.08E-3 21.90 20.00 19.17 12.49 4.16\nUK 9.53E-3 9.82 9.21 8.69 11.50 5.63\nGE 9.12E-3 13.95 12.79 12.08 13.35 5.52\nLAT 7.00E-6 25.40 23.59 22.09 13.03 6.35\nLON 1.40E-5 19.92 18.32 17.26 13.37 5.78\nDP 6.32E-2 17.51 16.89 15.87 9.35 6.07\nCT 3.88E0 16.19 14.45 13.92 14.03 3.69\nDU 6.00E-3 10.04 10.32 9.15 8.93 11.39\nBT 4.85E-1 59.62 61.29 53.77 9.81 12.26\nBW 3.16E-3 52.19 48.28 45.01 13.75 6.77\nBM 1.42E-2 27.13 25.32 23.29 14.15 8.00\nBP 3.61E-1 43.05 41.76 38.52 10.54 7.76\nAA creates more time series fragments than NeaTS-L, and its\nfunctions pass through the first data point of each fragment:\ntwo factors that together yield zero errors on many data points.\nIn terms of compression speed, PLA is the fastest at\n123.36 MB/s, followed by AA at 63.11 MB/s, and NeaTS-L\nat 18.23 MB/s. These results reflect the higher computational\neffort of NeaTS-L in achieving better compression ratios.\nIn terms of decompression speed, PLA is the fastest at\n2997.00 MB/s, followed by NeaTS at 2561.31 MB/s, and AA\nat 2420.20 MB/s. This can be attributed to the fact that the\nlinear models in PLA are faster to evaluate, and that NeaTS\nuses fewer fragments than AA, thus reducing the overhead\nassociated with switching between fragments.\nC. On the lossless compressors\nWe now compare our NeaTS against the 5 lossless general-\npurpose compressors (i.e. Xz, Brotli, Zstd, Lz4, and Snappy)\nand the 7 lossless special-purpose compressors (i.e. Chimp,\nChimp128, TSXor, DAC, Gorilla, LeCo, and ALP).\nTable III reports the compression ratio, decompression\nspeed, and random access speed of all compressors on each\ndataset, where the best result in each family of compressors\nis in bold, and the best result overall is underlined. Moreover,\nwe plot the trade-offs compression ratio vs compression speed,\ncompression ratio vs decompression speed, and compression\nratio vs random access speed in Figures 2 to 3 and dig into\nthem in Sections IV-C1 to IV-C3, respectively. Then, in\nSection IV-C4, we explore the benefits of our approach from\na data management perspective by focusing on the important\ncase of range queries. Finally, we provide a summary of our\nexperiments in Section IV-C5.\n1) Compression ratio vs compression speed: Figure 2\nshows the trade-off between compression speed and compres-\nsion ratio of the lossless compressors.\nFirst, we notice that Xz, followed by Brotli, achieve on\naverage (but not always, see below) the best compression ratio\nat the cost of a slow compression speed, indeed, they are at the\nbottom-left of Figure 2. The opposite extreme in this trade-off\nBe/t_ter\n20 30 40 50 60 70100101102103\nCompression ratio (%)Compression speed (MB/s)\nXz\nChimp\nBrotli\nChimp128Zstd\nTSXor\nLz4\nGorillaSnappy\nLeCo\nDAC\nALPNeaTS\nLeaTS\nSNeaTSFig. 2: The trade-off between compression ratio and speed of the\nlossless compressors, averaged on the 16 datasets.\nis occupied by the special-purpose compressor Gorilla (at the\ntop-right of Figure 2), which is 3 orders of magnitude faster\nin compression speed than Brotli but achieves a compression\nratio above 70% on average. In between these two extremes,\nwe notice that ALP is on the Pareto front of this trade-\noff, dominating (in order of increasingly higher compression\nspeeds and worse compression ratios) LeCo, TSXor, DAC,\nZstd, Chimp, Chimp128, Lz4 and Snappy, but not our NeaTS.\nIndeed, we observe from Table III that NeaTS achieves the\nbest compression ratio among the special-purpose compressors\non 14/16 datasets, and the best compression ratio overall on\n4/16 datasets. Its compression speed is low but still 164.14%\nfaster than Brotli with just a 4.09% worse compression ratio\non average. Moreover, NeaTS always achieves better compres-\nsion ratios than Lz4 and Snappy (the fastest general-purpose\ncompressors in terms of compression speed) by 52.77% and\n50.20% on average, respectively. NeaTS also achieves better\ncompression ratios than Zstd for almost all the datasets (except\nfor AP), with an average improvement of 28.49%.\nCompared to the special-purpose compressors, the only\n2/16 datasets in which NeaTS does not achieve the best\ncompression ratio are BP and BT, where ALP is slightly better.\nHowever, these two datasets are also among the smallest ones,\nand NeaTS achieves a better compression ratio than ALP by\n16.36% on average.\nWe conclude this section by experimenting with two vari-\nants of NeaTS that improve the compression speed at the cost\nof worse compression. The first, named LeaTS, reduces the\nset of functions considered by Algorithm 1 to linear functions\nonly. The second, named SNeaTS, reduces the set of functions\nand error bounds considered by Algorithm 1 to those surviving\na model-selection procedure (included in the construction\ntime) that picks the top-5 most-used pairs in the first 10% of\nthe dataset. The results, depicted in Figure 2, show that LeaTS\nand SNeaTS achieve a compression speed that is 5.22 √óand\n12.86√óthat of NeaTS, and a compression ratio that is 0.89%\n\nBe/t_ter\n20 30 40 50 60 70\nCompression ratio (%)050010001500200025003000Decompression speed (MB/s)Be/t_ter\n20 30 40 50 60 70\nCompression ratio (%)10‚àí1100101102Random access speed (MB/s)\nXz\nChimpBrotli\nChimp128Zstd\nTSXorLz4\nGorillaSnappy\nLeCoDAC\nNeaTSALPFig. 3: The trade-off between compression ratio and decompression speed (left plot), and between compression ratio and random access\nspeed (right plot) of the lossless compressors, averaged on the 16 datasets. Note that the vertical axis of the right plot is logarithmic.\nand 8.18% worse than NeaTS, respectively.6The latter variant,\nin particular, is both faster in compression speed than LeCo by\n36.26% and better in compression ratio by 12.80% on average.\nDespite their better compression ratios, these variants are\nstill not as fast as ALP or Gorilla in compression speed.\nHowever, we anticipate from the next subsections that NeaTS\nalso excels in decompression and random access speed, thus\nmaking it the most competitive compressor in a query-\nintensive and space-constrained scenario. Moreover, if com-\npression speed is key for the underlying application, we could\nimagine using a lightweight compressor like ALP or Gorilla\nwhen the time series is first ingested, and running NeaTS later\non (or in the background) to provide much more effective\ncompression and efficient query operations in the long run.\n2) Compression ratio vs decompression speed: In an ana-\nlytical scenario, the decompression speed is a crucial perfor-\nmance metric. The middle of Table III shows the decompres-\nsion speed of all the compressors on all the datasets, while\nFigure 3 shows the trade-off between compression ratio and\ndecompression speed averaged on all the datasets.\nFirst, we notice from Table III that NeaTS achieves the\nfastest decompression speed on 10/16 datasets thanks to\nits cache-friendly and vectorised decompression procedure.\nCompared to ALP, which obtains better performance on the\nremaining 6/16 datasets, NeaTS is both 16.36% better in\ncompression ratio and 27.08% faster in decompression speed\non average. If we consider instead Xz and Brotli (i.e. the\nclosest competitors to NeaTS in terms of compression ratio,\nas commented above), their decompression speeds are 44.92 √ó\nand 12.27 √ólower on average than that of NeaTS, respectively.\nFinally, NeaTS dominates all the other special-purpose\ncompressors in this compression ratio vs decompression\n6In the lossless scenario, the space is clearly dominated by the storage of the\ncorrections rather than the function parameters, which is why the improvement\nin compression ratio of nonlinear functions over linear ones is not as high as\n12%, as experienced in the lossy scenario (Table II).speed trade-off, as Figure 3 clearly shows by placing them\nat the bottom-right of NeaTS in the decompression plot.\nFor instance, compared to LeCo, NeaTS is 18.23% better in\ncompression ratio and 201.11% faster in decompression speed.\n3) Compression ratio vs random access speed: Another key\nperformance metric for the efficient analysis of time series\nis the random access speed. The bottom of Table III shows\nthe average random access speed (for 10M queries) of all\nthe compressors on all the datasets, while Figure 3 shows the\ntrade-off between compression ratio and random access speed\naveraged on all the datasets.\nFirst, we notice from Table III that DAC, followed by\nNeaTS, achieves the best random access speed. However, we\nremark that NeaTS is much more effective than DAC in terms\nof compression ratio, i.e. 37.25% better on average and up to\n67.86% better overall. This is why both NeaTS and DAC oc-\ncupy a prominent position in the compression ratio vs random\naccess speed trade-off, as Figure 3 clearly shows by placing\nthem close to the top-left edge of the random access plot.\nLeCo is the only other compressor supporting random ac-\ncess natively (i.e. without the block-wise approach described in\nSection IV-A2). NeaTS is both 118.55% faster in random ac-\ncess speed and 18.23% better in compression ratio than LeCo.\nThe remaining compressors are from 2 to 3 orders of\nmagnitude slower in random access speed than NeaTS. In\nparticular, NeaTS dominates all other special-purpose com-\npressors and the vast majority of general-purpose compressors\nin the compression ratio vs random access speed trade-off. The\nonly exceptions are Xz and Brotli that, compared to NeaTS,\ncan provide slightly better compression ratios (except for the\n4/16 dataset where NeaTS is better) but much slower random\naccess speeds (always), which is why they are at the bottom-\nleft of every plot.\n4) Range queries: The most fundamental queries in time\nseries databases ‚Äî such as trend analysis, anomaly detection,\ncorrelation analysis, and data aggregation ‚Äî ultimately rely on\n\n101102103104105106103104105106\nRange size/T_hroughput (queries/s)\nALP DAC Lz4 NeaTSFig. 4: Range queries throughput across different range sizes.\naccessing data within a specific time interval (i.e. a range\nquery) [21], [22], which boils down to a random access\noperation (to retrieve the first data point) followed by a scan\n(to retrieve the subsequent data points within the interval). We\nthus now focus on the best compressors in terms of random\naccess or decompression speed (i.e. ALP, DAC, Lz4, and\nNeaTS) and evaluate their range query performance at different\nrange sizes, from 10¬∑20to10¬∑216data points. Figure 4 shows\nthe throughput in queries per second measured on 10K random\nrange queries and averaged over the 11 largest datasets. For\nrange sizes smaller than 40, DAC is the fastest solution,\nfollowed by NeaTS, which remains an order of magnitude\nmore efficient than other compressors. For larger range sizes,\nNeaTS clearly outperforms all competitors. This demonstrates\nthe ability of NeaTS to provide a full spectrum of efficient\ndata access, from small to large ranges, thus benefiting a wide\nvariety of queries in time series databases.\n5) Summary: Our results on 16 real-world time series show\nthat NeaTS emerges as the only approach to date providing,\nsimultaneously, compression ratios close to or better than the\nexisting compressors (i.e. the best compression ratio among\nthe special-purpose compressor on 14/16 datasets, and the best\noverall on 4/16 dataset), a much faster decompression speed,\nand up to 3 orders of magnitude more efficient random access.\nNo other compressor can strike such a good trade-off among\nall these factors together. For example, Xz achieves the best\ncompression ratio on 9/16 datasets, but on average its decom-\npression and random access speeds are 44.92 √óand 1657.10 √ó\nslower than that of NeaTS, respectively. ALP achieves the\nfastest compression speed (ignoring Gorilla, whose compres-\nsion ratio is not very competitive), but on average NeaTS\nachieves 16.36% better compression ratio, 27.08% faster de-\ncompression speed, and at least one order of magnitude faster\nrandom access speed than ALP. DAC achieves the fastest ran-\ndom access speed, but on average NeaTS is 37.25% better incompression ratio and 234.89% faster in decompression speed\nthan DAC. Furthermore, NeaTS outperforms the other com-\npressors for range queries involving 40 or more data points.\nThis is evidence that NeaTS has the potential to be the\ncompressor of choice for the storage and real-time analysis of\nmassive and ever-growing amounts of time series data.\nV. R ELATED WORK\nWe now review the literature of general- and special-purpose\nlossless compressors for time series, and of lossy compressors.\nFor the latter, we focus on approaches based on error-bounded\nfunctional approximations, which are relevant to our work.\na) General-purpose lossless compressors: These com-\npressors are not specifically designed for time series but can\nbe applied to any byte sequence. We discuss below the ones\nbased on the LZ77-parsing [68], which currently offer the best\ncombination of compression ratio and (de)compression speed.\nBrotli [9] relies on a modern variant of the LZ77-parsing\nof the input file that uses a pseudo-optimal entropy encoder\nbased on second-order context modelling. Xz [11] achieves\neffective compression by using Markov chain modelling and\nrange coding of the LZ77-parsing. Zstd [10] achieves very\nfast (de)compression speed and good compression ratios\nvia a tabled asymmetric numeral systems encoding. Finally,\nLz4 [12] and Snappy [13] trade compression effectiveness with\nspeed by adopting a faster byte-oriented encoding format for\nthe LZ77-parsing. Given this plethora of approaches offering\na variety of trade-offs, we tested them all in our experiments.\nb) Special-purpose lossless compressors: Most recent\ncompressors for time series are often based on encoding the\nresult of bitwise XOR operations between close or adjacent\nfloating-point values. Their compression ratio is strongly in-\nfluenced by data fluctuations: the more severe the fluctuations,\nthe less effective the compression. On the other hand, these\nalgorithms offer very fast (de)compression speeds.\nFor instance, Gorilla [14] improves earlier floating-point\ncompressors [69]‚Äì[71] by simply computing the XOR between\nconsecutive values of the time series and properly encoding\nthe number of leading zeros and significant bits of the result.\nChimp [15] improves both the compression ratio and speed\nof Gorilla by using different encoding modes based on the\nnumber of trailing and leading zeros of the XOR result.\nChimp128 [15] and TSXor [65] use a window of 128 values\nto choose the best reference value for the XOR computation:\nChimp128 uses the value that produces the most trailing zeros,\nwhile TSXor selects the value with the most bits in common.\nElf [16] performs an erasing operation on the floats before\nXORing them, which makes the resulting value more com-\npressible. We do not experiment with Elf because ALP [72]\n(described next and included in our experiments) was shown\nto achieve better compression ratios on average, and always\nfaster compression and decompression speeds than Elf.\nALP [72], unlike the above approaches, does not use XOR\noperations but rather builds on the idea of encoding a double\nxvia the storage of the significant digits dand an exponent\ne, i.e. d=round (x¬∑10e), also known as Pseudodecimal\n\nTABLE III: Compression ratio (top), decompression speed (middle), and random access speed (bottom) achieved by the 5 general-purpose\nand the 8 special-purpose lossless compressors (including our NeaTS) on 16 datasets, sorted by decreasing size. We highlight in bold the\nbest result in each family, and in underline the best result overall.\nDatasetGeneral-purpose compressors Special-purpose compressors\nXz Brotli Zstd Lz4 Snappy Chimp128 Chimp TSXor DAC Gorilla LeCo ALP NeaTSCompression ratio (%)IT 12.86 14.25 23.46 41.31 36.96 29.43 72.30 30.76 23.83 78.60 13.62 16.86 11.88\nUS 9.18 8.70 12.82 27.09 21.51 18.94 54.55 18.89 24.95 57.54 9.16 10.50 8.02\nECG 12.12 12.12 17.04 26.14 33.75 54.11 43.18 20.03 25.39 45.26 15.58 16.23 12.96\nWD 23.60 27.60 33.78 52.70 54.19 43.38 84.09 46.42 25.75 91.02 24.71 24.90 24.37\nAP 12.35 12.69 17.87 26.50 24.82 30.00 35.76 34.78 41.13 37.67 23.52 25.74 19.27\nUK 9.42 9.06 12.99 26.94 21.41 23.13 46.95 15.85 25.79 53.92 10.83 11.64 9.09\nGE 11.07 11.04 15.27 30.25 23.94 21.08 66.90 21.44 29.01 71.49 13.43 13.88 12.11\nLON 17.03 18.63 32.72 49.71 49.28 58.64 61.70 71.64 47.27 63.09 20.74 26.87 17.53\nLAT 21.51 23.67 40.77 52.12 51.44 58.09 61.44 71.93 47.27 65.02 25.56 26.70 22.22\nDP 16.37 17.02 29.35 48.61 47.54 49.53 77.17 60.91 26.95 83.53 17.83 22.04 16.10\nCT 15.72 16.37 25.33 42.92 37.31 36.09 73.25 30.96 19.14 87.11 17.91 15.27 14.20\nDU 8.21 7.78 11.37 23.00 18.62 21.68 39.74 18.31 11.14 44.49 28.54 13.34 9.46\nBT 45.66 45.69 58.12 67.20 68.64 46.90 84.01 53.88 57.07 92.88 58.15 46.25 54.01\nBW 36.17 41.49 50.24 58.74 58.79 71.27 87.16 82.32 45.91 99.72 56.99 50.01 45.21\nBM 19.67 20.70 29.52 43.58 39.39 40.96 61.96 48.18 37.42 74.67 50.72 30.80 23.44\nBP 36.97 39.85 66.43 69.03 71.22 72.09 67.84 87.86 42.79 82.72 39.03 38.37 39.89Decompression speed (MB/s)IT 90.83 304.65 459.91 1405.36 1207.61 725.74 598.36 743.69 999.45 795.22 1082.45 2249.26 2549.04\nUS 133.37 396.11 643.89 1609.06 1928.74 1109.06 692.86 1084.44 896.80 839.86 1097.74 2295.05 2982.09\nECG 94.39 253.72 512.45 1325.59 1473.75 559.59 645.71 773.99 1082.36 790.96 1306.75 2344.27 2897.08\nWD 56.94 227.86 490.46 1443.34 1191.21 732.56 606.28 754.89 864.86 854.76 1035.08 2253.39 1936.42\nAP 106.07 332.69 683.65 1740.56 1599.33 885.36 893.27 885.33 705.69 978.22 1013.16 2116.29 2944.05\nUK 131.95 392.03 634.25 1645.54 1815.86 853.25 670.05 1102.54 829.75 863.34 1087.50 2312.32 3015.90\nGE 115.76 350.47 594.39 1611.32 1761.48 1008.27 656.04 986.84 962.52 829.82 1062.78 2307.16 3243.73\nLAT 53.10 171.93 376.31 1289.07 1033.77 758.29 785.61 492.87 1019.50 622.10 960.91 2144.64 2935.03\nLON 61.55 212.49 375.82 1198.19 963.81 776.44 801.64 493.81 1025.93 625.60 925.42 2113.61 2870.15\nDP 75.14 293.10 429.39 1456.28 1098.91 581.50 626.82 618.88 1056.31 782.95 963.67 2057.49 1953.35\nCT 79.93 298.52 458.10 1467.29 1225.00 760.50 556.01 768.86 1070.76 841.18 969.50 3290.46 3269.65\nDU 147.58 436.39 696.34 1701.39 2031.31 923.52 805.54 1093.90 658.38 948.68 746.33 5410.27 4007.91\nBT 33.43 173.70 414.01 1418.67 1039.45 662.70 590.54 672.81 783.96 805.45 846.57 4241.33 2570.33\nBW 41.74 153.77 378.60 1527.78 1020.14 607.71 611.78 655.01 1189.51 849.33 1501.59 4006.59 3675.40\nBM 69.31 235.46 494.08 1450.65 1283.83 758.16 626.94 700.60 802.93 790.78 1441.36 1677.20 4508.69\nBP 36.85 187.29 366.83 1400.66 1163.14 624.77 682.54 612.99 1231.05 803.20 768.30 1752.34 3649.14Random access speed (MB/s)IT 0.09 0.36 0.39 1.21 1.04 1.05 0.92 1.28 137.93 1.09 19.94 5.43 48.19\nUS 0.11 0.45 0.52 1.24 1.57 1.55 1.10 1.76 108.11 1.22 22.15 6.36 57.14\nECG 0.11 0.36 0.49 1.18 1.46 1.16 1.32 1.29 153.85 1.50 30.08 3.78 50.00\nWD 0.05 0.26 0.42 1.23 1.01 1.12 1.02 1.21 135.59 1.28 20.05 5.19 47.06\nAP 0.10 0.39 0.57 1.46 1.30 1.29 1.60 1.50 78.43 1.50 19.02 5.31 53.69\nUK 0.12 0.50 0.56 1.46 1.63 1.20 1.05 2.01 131.15 1.26 25.94 6.26 76.92\nGE 0.12 0.42 0.51 1.38 1.46 1.42 1.02 1.65 186.05 1.18 30.75 4.23 80.00\nLAT 0.06 0.25 0.37 1.22 1.01 1.14 1.24 0.98 210.53 0.98 22.15 5.19 76.19\nLON 0.07 0.32 0.38 1.22 0.99 1.16 1.26 0.97 210.53 0.99 21.07 5.12 80.01\nDP 0.07 0.33 0.37 1.31 0.97 0.82 0.93 0.94 571.43 1.05 53.48 5.56 123.08\nCI 0.07 0.34 0.41 1.28 1.10 1.15 0.90 1.19 666.67 1.16 57.22 6.04 140.35\nDU 0.15 0.58 0.66 1.62 1.95 1.38 1.38 1.92 363.64 1.50 96.39 7.87 142.46\nBT 0.03 0.18 0.39 1.31 0.97 1.05 1.06 1.15 666.67 1.23 145.45 4.49 140.35\nBW 0.04 0.17 0.37 1.46 0.99 0.93 1.08 1.05 800.00 1.23 131.15 4.38 148.15\nBM 0.07 0.27 0.47 1.40 1.25 1.05 1.06 1.18 533.33 1.13 145.45 4.61 160.00\nBP 0.04 0.22 0.38 1.37 1.17 1.06 1.17 1.18 888.89 1.35 181.82 4.55 163.26\nEncoding [73]. It finds a single best exponent for a block\nof 1024 values and bit-packs the resulting significant digits\nvia the frame-of-reference integer code. Values failing to be\nlosslessly encoded as pseudodecimals are stored uncompressed\nseparately. Further optimisations (such as cutting trailing zeros\nand using vector instructions) are applied to improve the\ncompression ratio and speed.\nBUFF [74] compresses a float by eliminating the less\nsignificant bits based on a given precision, splitting it into the\ninteger and fractional parts, and then compressing the two parts\nseparately with a fixed-length encoding. We do not experiment\nwith BUFF because its average compression ratio on time\nseries was shown to be worse than that of Chimp (which,in turn, is always worse than NeaTS in our experiments) and\nits compression and decompression speeds were shown to be\nno more than 6 √óthat of Chimp [20] (which, in turn, are\noutmatched by those of ALP by one order of magnitude [17]).\nSprintz [75] encodes time series using four components:\nforecasting, bit packing, run-length encoding, and entropy\ncoding. Sprintz focuses on 8- or 16-bit integers, which is a\nlimitation for our datasets with 64-bit data. Also, it was shown\nto be worse than BUFF in compression ratio and speed [74].\nThere are also floating-point compressors targeted to scien-\ntific simulation and observational data, such as fpzip [76] and\nndzip [77], but we do not experiment with them since their\ncompression ratios were shown to be poor on time series [20].\n\nConcerning the random access operation to time-series\nvalues, this is not directly offered by most compressor im-\nplementations. Therefore, the typical approach is to compress\nblocks of the time series separately, and then access a single\nvalue by decompressing just the corresponding block. This is\noften insufficient to guarantee a reasonable speed and use of\ncomputational resources (as our experiments confirm), which\nis why Brisaboa et al. [66] introduced the Directly Addressable\nCodes (DAC) scheme that enables fast access to individual\nvalues. Given this feature, although designed for generic\ninteger sequences, we included DAC in our experiments.\nDACTS [19] uses Re-Pair [78] on top of DAC to better\ncapture repeating patterns. This additional compression step\nis effective when the time series is highly repetitive but slows\ndown the access time, so DACTS proved to be useful on the\nso-called industrial time series originating from sensors pro-\nducing long sequences of constant values. This is a restrictive\nsituation, so we did not experiment with DACTS.\nTitchy [18] focuses on random access to IoT data and relies\non a dictionary-based approach combined with a partitioning\nof the time series into chunks. Each chunk is represented with\na pair‚ü®base,deviation ‚ü©, where base indicates the item of the\ndictionary to copy, and deviation encodes what makes the\nchunk slightly different from the other. We could not experi-\nment with Titchy because its source code is not available.\nFinally, LeCo [35] is a recent proposal (not specific for time\nseries) that, similarly to our NeaTS and earlier work [33], [34],\nis based on the idea of lossless compression via functional ap-\nproximations and the storage of residuals. LeCo uses a learned\nmodel to choose a kind of function suitable for the values\nat hand, and then it uses a heuristic partitioning algorithm\nthat greedily splits fragments (each associated with a function\nlearned through regression methods) and merges neighbouring\nones if this improves an estimate of the compression ratio. Our\nNeaTS, instead, compresses data with different function types,\nlearns each function optimally under a given error bound,\nand employs a rigorous partitioning algorithm to minimise\nthe actual compression ratio. These features, together with\nthe more query-efficient compressed layout, make NeaTS\nbetter than LeCo in compression ratio, random access, and\ndecompression speed, as our experiments show.\nc) Functional approximation for lossy compression: The\nidea behind lossy functional approximation is to represent a\ntime series as a sequence of functions over time, often under a\nchosen error metric [8], [79]. In the case of L2-norm, the goal\nis to minimise the sum of the squared residuals (i.e. the vertical\ndistances between the true and the approximated values). In\nthe case of the L1-norm, the goal is to minimise the sum of\nthe absolute values of the residuals. The focus of our paper is\ninstead to bound the L‚àû-norm of the residuals, thus bounding\nthemaximum absolute residual.\nAs discussed in Section II, the optimal linear-time algorithm\nto solve this problem using a Piecewise Linear Approxi-\nmation (PLA) with the minimum number of segments was\nfirst proposed by O‚ÄôRourke [36] (see also [25]‚Äì[27], [37],\n[38]). Interestingly, ModelarDB [80] has shown how PLAscan be used in a fully-fledged distributed time series database.\nModelarDB could benefit from our results since it computes\nPLAs via an algorithm [38] that was shown to use more\nsegments than the optimal one that we use as a baseline [27],\n[36]. Other works [81], [82] proposed to further compress\nsimilar segments, which is a post-processing step that we can\napply to our techniques too.\nA few works [29]‚Äì[31] address the problem of partitioning\na time series using nonlinear functions. We compared our\napproach to the most recent one that employs nonlinear Œµ-ap-\nproximations [30], [31], called Adaptive Approximation (AA).\nAA heuristically partitions a time series using quadratic, linear,\nand exponential functions. However, as our experiments show,\ndespite the use of nonlinear functions, AA is almost always\nworse than PLA, and in turn worse than our approach. In\nfact, NeaTS-L finds a better partition of the time series into\nfragments and selects the best approximating function within\na larger set of nonlinear ones.\nFinally, we mention HIRE [83], which focuses on the\ndistinct problem of constructing a single encoding of a time\nseries that can be decompressed at different L‚àûerror bounds.\nSince HIRE relies on piecewise constant approximations as\nbuilding blocks, we believe it could benefit from our more\ngeneral nonlinear approximations.\nVI. C ONCLUSIONS AND FUTURE WORK\nWe introduced new lossy and lossless compressors that\nharness the trends and patterns in time series data via a se-\nquence of error-bounded linear and nonlinear approximations\nof different kinds and shapes. Our approaches experimentally\nproved to offer new trade-offs in terms of random access\nspeed, decompression speed, and compression ratio compared\nto existing compressors on 16 diverse time series datasets.\nFor future work, we suggest further compressing the nonlin-\near approximation models by exploiting similarities between\nfunctions as introduced in [81], [84]. Another interesting\nresearch direction is to exploit the information encoded by\nthe functions to efficiently answer aggregate queries on the\ntime series data. Finally, it would be interesting to investigate\nthe impact of our new techniques for computing error-bounded\nnonlinear approximations in the design of learned data struc-\ntures [43], [44], [81], [85]‚Äì[95].\nAcknowledgments. This work was supported by the European Union\n‚Äì Horizon 2020 Program under the scheme ‚ÄúINFRAIA-01-2018-2019\n‚Äì Integrating Activities for Advanced Communities‚Äù, Grant Agree-\nment n. 871042, ‚ÄúSoBigData++: European Integrated Infrastructure\nfor Social Mining and Big Data Analytics‚Äù (http://www.sobigdata.eu),\nby the NextGenerationEU ‚Äì National Recovery and Resilience Plan\n(Piano Nazionale di Ripresa e Resilienza, PNRR) ‚Äì Project: ‚ÄúSoBig-\nData.it - Strengthening the Italian RI for Social Mining and Big Data\nAnalytics‚Äù ‚Äì Prot. IR0000013 ‚Äì Avviso n. 3264 del 28/12/2021, by\nthe spoke ‚ÄúFutureHPC & BigData‚Äù of the ICSC ‚Äì Centro Nazionale\ndi Ricerca in High-Performance Computing, Big Data and Quan-\ntum Computing funded by European Union ‚Äì NextGenerationEU ‚Äì\nPNRR, by Regione Toscana under POR FSE 2021/27.\n\nREFERENCES\n[1] S. K. Jensen, T. B. Pedersen, and C. Thomsen, ‚ÄúTime series manage-\nment systems: A survey,‚Äù IEEE Transactions on Knowledge and Data\nEngineering , vol. 29, no. 11, pp. 2581‚Äì2600, 2017.\n[2] C. C. Aggarwal, Ed., Managing and Mining Sensor Data . Springer,\n2013.\n[3] C. Wang, J. Qiao, X. Huang, S. Song, H. Hou, T. Jiang, L. Rui,\nJ. Wang, and J. Sun, ‚ÄúApache IoTDB: A time series database for IoT\napplications,‚Äù Proc. ACM Manag. Data , vol. 1, no. 2, jun 2023.\n[4] InfluxData Inc., ‚ÄúInfluxDB,‚Äù 2023. [Online]. Available: https://www.\ninfluxdata.com/\n[5] The OpenTSDB Authors, ‚ÄúOpenTSDB,‚Äù 2023. [Online]. Available:\nhttp://opentsdb.net/\n[6] The Prometheus Authors, ‚ÄúPrometheus,‚Äù 2023. [Online]. Available:\nhttps://prometheus.io/\n[7] Timescale Inc., ‚ÄúTimescale,‚Äù 2023. [Online]. Available: https://www.\ntimescale.com/\n[8] G. Chiarot and C. Silvestri, ‚ÄúTime series compression survey,‚Äù ACM\nComput. Surv. , aug 2022.\n[9] J. Alakuijala, A. Farruggia, P. Ferragina, E. Kliuchnikov, R. Obryk,\nZ. Szabadka, and L. Vandevenne, ‚ÄúBrotli: A general-purpose data\ncompressor,‚Äù ACM Transactions on Information Systems , vol. 37, no. 1,\npp. 1‚Äì30, 2018.\n[10] Meta Platforms, Inc., ‚ÄúZstandard - fast real-time compression algorithm,‚Äù\n2016, retrieved March 19, 2023 from https://github.com/facebook/zstd.\n[11] The Tukaani Project, ‚ÄúThe .xz file format,‚Äù 2023, retrieved March 19,\n2023 from https://tukaani.org/xz/xz-file-format.txt.\n[12] Y . Collet et al. , ‚ÄúLz4: Extremely fast compression algorithm,‚Äù 2013,\nretrieved March 19, 2023 from https://github.com/lz4/lz4.\n[13] Google, ‚ÄúSnappy: A fast compressor/decompressor,‚Äù 2023, retrieved\nMarch 19, 2023 from https://github.com/google/snappy.\n[14] T. Pelkonen, S. Franklin, J. Teller, P. Cavallaro, Q. Huang, J. Meza,\nand K. Veeraraghavan, ‚ÄúGorilla: A fast, scalable, in-memory time series\ndatabase,‚Äù Proc. VLDB Endowment , vol. 8, no. 12, pp. 1816‚Äì1827, 2015.\n[15] P. Liakos, K. Papakonstantinopoulou, and Y . Kotidis, ‚ÄúChimp: Efficient\nlossless floating point compression for time series databases,‚Äù PVLDB ,\nvol. 15, no. 11, pp. 3058‚Äì3070, sep 2022.\n[16] R. Li, Z. Li, Y . Wu, C. Chen, and Y . Zheng, ‚ÄúElf: Erasing-based lossless\nfloating-point compression,‚Äù PVLDB , vol. 16, no. 7, pp. 1763‚Äì1776, may\n2023.\n[17] A. Afroozeh, L. X. Kuffo, and P. Boncz, ‚ÄúALP: Adaptive lossless\nfloating-point compression,‚Äù Proc. ACM Manag. Data , vol. 1, no. 4,\ndec 2023.\n[18] R. Vestergaard, Q. Zhang, M. ¬¥A. Sipos, and D. E. Lucani, ‚ÄúTitchy:\nOnline time-series compression with random access for the internet of\nthings,‚Äù IEEE Internet Things J. , vol. 8, no. 24, pp. 17 568‚Äì17 583, 2021.\n[19] A. G ¬¥omez-Brand ¬¥on, J. R. Param ¬¥a, K. Villalobos, A. Illarramendi, and\nN. R. Brisaboa, ‚ÄúLossless compression of industrial time series with\ndirect access,‚Äù Computers in Industry , vol. 132, p. 103503, 2021.\n[20] X. Chen, J. Tian, I. Beaver, C. Freeman, Y . Yan, J. Wang, and D. Tao,\n‚ÄúFCBench: Cross-domain benchmarking of lossless compression for\nfloating-point data,‚Äù PVLDB , vol. 17, no. 6, pp. 1418‚Äì1431, may 2024.\n[21] A. Khelifati, M. Khayati, A. Dign ¬®os, D. E. Difallah, and P. Cudr ¬¥e-\nMauroux, ‚ÄúTSM-Bench: Benchmarking time series database systems for\nmonitoring applications,‚Äù PVLDB , vol. 16, no. 11, pp. 3363‚Äì3376, 2023.\n[22] Y . Hao, X. Qin, Y . Chen, Y . Li, X. Sun, Y . Tao, X. Zhang, and X. Du,\n‚ÄúTS-Benchmark: A benchmark for time series databases,‚Äù in Proc. 37th\nInternational Conference on Data Engineering (ICDE) , 2021, pp. 588‚Äì\n599.\n[23] K. F. Turkman, M. G. Scotto, and P. de Zea Bermudez, Non-Linear Time\nSeries . Springer Cham, 2014.\n[24] J. Fan and Q. Yao, Nonlinear Time Series . Springer New York, NY ,\n2008.\n[25] E. J. Keogh, S. Chu, D. M. Hart, and M. J. Pazzani, ‚ÄúAn online\nalgorithm for segmenting time series,‚Äù in Proc. 2001 IEEE International\nConference on Data Mining (ICDM) , 2001, pp. 289‚Äì296.\n[26] X. Liu, Z. Lin, and H. Wang, ‚ÄúNovel online methods for time series\nsegmentation,‚Äù IEEE Transactions on Knowledge and Data Engineering ,\nvol. 20, no. 12, pp. 1616‚Äì1626, 2008.\n[27] Q. Xie, C. Pang, X. Zhou, X. Zhang, and K. Deng, ‚ÄúMaximum error-\nbounded piecewise linear representation for online stream approxima-\ntion,‚Äù The VLDB Journal , vol. 23, no. 6, pp. 915‚Äì937, 2014.[28] E. Fuchs, T. Gruber, J. Nitschke, and B. Sick, ‚ÄúOnline segmentation of\ntime series based on polynomial least-squares approximations,‚Äù IEEE\nTransactions on Pattern Analysis and Machine Intelligence , vol. 32,\nno. 12, pp. 2232‚Äì2245, 2010.\n[29] F. Eichinger, P. Efros, S. Karnouskos, and K. B ¬®ohm, ‚ÄúA time-series\ncompression technique and its application to the smart grid,‚Äù The VLDB\nJournal , vol. 24, no. 2, pp. 193‚Äì218, 2015.\n[30] Z. Xu, R. Zhang, K. Ramamohanarao, and U. Parampalli, ‚ÄúAn adaptive\nalgorithm for online time series segmentation with error bound guar-\nantee,‚Äù in Proc. 15th International Conference on Extending Database\nTechnology (EDBT) , 2012, pp. 192‚Äì203.\n[31] J. Qi, R. Zhang, K. Ramamohanarao, H. Wang, Z. Wen, and D. Wu,\n‚ÄúIndexable online time series segmentation with error bound guarantee,‚Äù\nWorld Wide Web , vol. 18, no. 2, pp. 359‚Äì401, 2015.\n[32] E. J. Keogh, K. Chakrabarti, M. J. Pazzani, and S. Mehrotra, ‚ÄúDi-\nmensionality reduction for fast similarity search in large time series\ndatabases,‚Äù Knowledge and Information Systems , vol. 3, no. 3, pp. 263‚Äì\n286, 2001.\n[33] N. Ao, F. Zhang, D. Wu, D. S. Stones, G. Wang, X. Liu, J. Liu,\nand S. Lin, ‚ÄúEfficient parallel lists intersection and index compression\nalgorithms using graphics processing units,‚Äù PVLDB , vol. 4, no. 8, pp.\n470‚Äì481, 2011.\n[34] A. Boffa, P. Ferragina, and G. Vinciguerra, ‚ÄúA learned approach to\ndesign compressed rank/select data structures,‚Äù ACM Transactions on\nAlgorithms , 2022.\n[35] Y . Liu, X. Zeng, and H. Zhang, ‚ÄúLeCo: lightweight compression via\nlearning serial correlations,‚Äù Proc. ACM Manag. Data , vol. 2, no. 1,\nmar 2024.\n[36] J. O‚ÄôRourke, ‚ÄúAn on-line algorithm for fitting straight lines between\ndata ranges,‚Äù Communications of the ACM , vol. 24, no. 9, pp. 574‚Äì578,\n1981.\n[37] M. Dalai and R. Leonardi, ‚ÄúApproximations of one-dimensional digital\nsignals under the l‚àûnorm,‚Äù IEEE Transactions on Signal Processing ,\nvol. 54, no. 8, pp. 3111‚Äì3124, 2006.\n[38] H. Elmeleegy, A. K. Elmagarmid, E. Cecchet, W. G. Aref, and\nW. Zwaenepoel, ‚ÄúOnline piece-wise linear approximation of numerical\nstreams with precision guarantees,‚Äù PVLDB , vol. 2, no. 1, pp. 145‚Äì156,\naug 2009.\n[39] ¬¥E. Grandjean and L. Jachiet, ‚ÄúWhich arithmetic operations can be\nperformed in constant time in the RAM model with addition?‚Äù 2023.\n[40] F. P. Preparata and D. E. Muller, ‚ÄúFinding the intersection of nhalf-\nspaces in time o(nlogn),‚ÄùTheoretical Computer Science , vol. 8, pp.\n45‚Äì55, 1979.\n[41] M. H. Overmars, The Design of Dynamic Data Structures , ser. Lecture\nNotes in Computer Science. Springer, 1983, vol. 156.\n[42] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction\nto Algorithms , 3rd ed. The MIT Press, 2009.\n[43] P. Ferragina, H.-P. Lehmann, P. Sanders, and G. Vinciguerra, ‚ÄúLearned\nmonotone minimal perfect hashing,‚Äù in Proc. 31st Annual European\nSymposium on Algorithms (ESA) , 2023, pp. 46:1‚Äì46:17.\n[44] P. Ferragina, G. Manzini, and G. Vinciguerra, ‚ÄúCompressing and query-\ning integer dictionaries under linearities and repetitions,‚Äù IEEE Access ,\nvol. 10, pp. 118 831‚Äì118 848, 2022.\n[45] P. Elias, ‚ÄúEfficient storage and retrieval by content and address of static\nfiles,‚Äù Journal of the ACM , vol. 21, no. 2, pp. 246‚Äì260, 1974.\n[46] R. M. Fano, On the number of bits required to implement an associative\nmemory . Massachusetts Institute of Technology, Project MAC, 1971.\n[47] G. Navarro, Compact data structures: a practical approach . Cambridge\nUniversity Press, 2016.\n[48] R. Grossi, A. Gupta, and J. S. Vitter, ‚ÄúHigh-order entropy-compressed\ntext indexes,‚Äù in Proc. 14th Annual ACM-SIAM Symposium on Discrete\nAlgorithms (SODA) , 2003, pp. 841‚Äì850.\n[49] G. Jacobson, ‚ÄúSpace-efficient static trees and graphs,‚Äù in Proc. 30th\nIEEE Symposium on Foundations of Computer Science (FOCS) , 1989,\npp. 549‚Äì554.\n[50] D. R. Clark, ‚ÄúCompact pat trees,‚Äù Ph.D. dissertation, University of\nWaterloo, Canada, 1996.\n[51] S. Gog, T. Beller, A. Moffat, and M. Petri, ‚ÄúFrom theory to practice:\nPlug and play with succinct data structures,‚Äù in Proc. 13th International\nSymposium on Experimental Algorithms (SEA) , 2014, pp. 326‚Äì337, code\navailable at https://github.com/simongog/sdsl-lite.\n[52] S. Vigna, ‚ÄúBroadword implementation of rank/select queries,‚Äù in Proc.\n7th International Workshop on Experimental Algorithms (WEA) , 2008,\npp. 154‚Äì168, code available at https://github.com/vigna/sux.\n\n[53] Y . Zheng, H. Fu, X. Xie, W.-Y . Ma, and Q. Li, ‚ÄúGeolife\nGPS trajectory dataset ‚Äì user guide,‚Äù July 2011. [On-\nline]. Available: https://www.microsoft.com/en-us/research/publication/\ngeolife-gps-trajectory-dataset-user-guide/\n[54] J. Zheng, H. Guo, and H. Chu, ‚ÄúA large scale 12-lead electrocardiogram\ndatabase for arrhythmia study (version 1.0.0),‚Äù 2022.\n[55] J. Zheng, H. Chu, D. Struppa, J. Zhang, S. M. Yacoub, H. El-Askary,\nA. Chang, L. Ehwerhemuepha, I. Abudayyeh, A. Barrett, G. Fu, H. Yao,\nD. Li, H. Guo, and C. Rakovski, ‚ÄúOptimal multi-stage arrhythmia\nclassification approach,‚Äù Scientific Reports , vol. 10, no. 1, p. 2898, 2020.\n[56] National Ecological Observatory Network (NEON), ‚ÄúIr biological tem-\nperature (dp1.00005.001),‚Äù 2022, retrieved March 19, 2023 from https:\n//data.neonscience.org/data-products/DP1.00005.001/RELEASE-2022.\n[57] INFORE project, ‚ÄúFinancial data set used in infore project,‚Äù 2023,\nretrieved March 19, 2023 from https://zenodo.org/record/3886895.\n[58] National Ecological Observatory Network (NEON), ‚Äú2d wind speed and\ndirection (dp1.00001.001),‚Äù 2022, retrieved March 19, 2023 from https:\n//data.neonscience.org/data-products/DP1.00001.001/RELEASE-2022.\n[59] ‚Äî‚Äî, ‚ÄúBarometric pressure (dp1.00004.001),‚Äù 2022, retrieved March\n19, 2023 from https://data.neonscience.org/data-products/DP1.00004.\n001/RELEASE-2022.\n[60] ‚Äî‚Äî, ‚ÄúRelative humidity above water on-buoy (dp1.20271.001),‚Äù\n2022, retrieved March 19, 2023 from https://data.neonscience.org/\ndata-products/DP1.20271.001/RELEASE-2022.\n[61] University of Dayto, ‚ÄúDaily temperature of major cities,‚Äù 2023, re-\ntrieved March 19, 2023 from https://www.kaggle.com/sudalairajkumar/\ndaily-temperature-of-major-cities.\n[62] National Ecological Observatory Network (NEON), ‚ÄúDust and par-\nticulate size distribution (dp1.00017.001),‚Äù 2022, retrieved March 19,\n2023 from https://data.neonscience.org/data-products/DP1.00017.001/\nRELEASE-2022.\n[63] Meteoblue, ‚ÄúHistorical weather data download,‚Äù 2023, retrieved March\n19, 2023 from https://www.meteoblue.com/en/weather/archive/export/\nbasel switzerland.\n[64] InfluxData Inc., ‚ÄúInfluxDB 2.0 sample data,‚Äù 2023, retrieved March 19,\n2023 from https://github.com/influxdata/influxdb2-sample-data.\n[65] A. Bruno, F. M. Nardini, G. E. Pibiri, R. Trani, and R. Venturini,\n‚ÄúTSXor: A simple time series compression algorithm,‚Äù in Proc. 28th\nInternational Symposium on String Processing and Information Re-\ntrieval (SPIRE) , 2021, pp. 217‚Äì223, code available at https://github.com/\nandybbruno/TSXor/.\n[66] N. R. Brisaboa, S. Ladra, and G. Navarro, ‚ÄúDACs: bringing direct ac-\ncess to variable-length codes,‚Äù Information Processing & Management ,\nvol. 49, no. 1, pp. 392‚Äì404, 2013.\n[67] E. Nemerson, ‚ÄúSquash - compression abstraction library,‚Äù 2015, https:\n//quixdb.github.io/squash.\n[68] J. Ziv and A. Lempel, ‚ÄúA universal algorithm for sequential data\ncompression,‚Äù IEEE Trans. Inf. Theory , vol. 23, no. 3, pp. 337‚Äì343,\n1977.\n[69] P. Ratanaworabhan, J. Ke, and M. Burtscher, ‚ÄúFast lossless compression\nof scientific floating-point data,‚Äù in Proc. 16th Data Compression\nConference (DCC) , 2006, pp. 133‚Äì142.\n[70] P. Lindstrom and M. Isenburg, ‚ÄúFast and efficient compression of\nfloating-point data,‚Äù IEEE Transactions on Visualization and Computer\nGraphics , vol. 12, no. 5, pp. 1245‚Äì1250, 2006.\n[71] M. Burtscher and P. Ratanaworabhan, ‚ÄúHigh throughput compression\nof double-precision floating-point data,‚Äù in 2007 Data Compression\nConference (DCC) , 2007, pp. 293‚Äì302.\n[72] A. Afroozeh and P. Boncz, ‚ÄúThe FastLanes compression layout: Decod-\ning>100 billion integers per second with scalar code,‚Äù PVLDB , vol. 16,\nno. 9, pp. 2132‚Äì2144, may 2023.\n[73] M. Kuschewski, D. Sauerwein, A. Alhomssi, and V . Leis, ‚ÄúBtrBlocks:\nefficient columnar compression for data lakes,‚Äù Proc. ACM Manag. Data ,\nvol. 1, no. 2, jun 2023.\n[74] C. Liu, H. Jiang, J. Paparrizos, and A. J. Elmore, ‚ÄúDecomposed bounded\nfloats for fast compression and queries,‚Äù PVLDB , vol. 14, no. 11, pp.\n2586‚Äì2598, jul 2021.[75] D. W. Blalock, S. Madden, and J. V . Guttag, ‚ÄúSprintz: Time series\ncompression for the internet of things,‚Äù Proc. ACM Interact. Mob.\nWearable Ubiquitous Technol. , vol. 2, no. 3, pp. 93:1‚Äì93:23, 2018.\n[76] P. Lindstrom and M. Isenburg, ‚ÄúFast and efficient compression of\nfloating-point data,‚Äù IEEE Transactions on Visualization and Computer\nGraphics , vol. 12, no. 5, pp. 1245‚Äì1250, 2006.\n[77] F. Knorr, P. Thoman, and T. Fahringer, ‚Äúndzip: a high-throughput parallel\nlossless compressor for scientific data,‚Äù in Proc. 31st Data Compression\nConference (DCC) , 2021, pp. 103‚Äì112.\n[78] N. J. Larsson and A. Moffat, ‚ÄúOff-line dictionary-based compression,‚Äù\nProceedings of the IEEE , vol. 88, no. 11, pp. 1722‚Äì1732, 2000.\n[79] Y . Liu, J. Li, H. Gao, and X. Fang, ‚ÄúEnabling Œµ-approximate querying\nin sensor networks,‚Äù Proc. VLDB Endow. , vol. 2, no. 1, pp. 169‚Äì180,\n2009.\n[80] S. K. Jensen, T. B. Pedersen, and C. Thomsen, ‚ÄúModelarDB: modu-\nlar model-based time series management with spark and cassandra,‚Äù\nPVLDB , vol. 11, no. 11, pp. 1688‚Äì1701, jul 2018.\n[81] P. Ferragina and G. Vinciguerra, ‚ÄúThe PGM-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds,‚Äù PVLDB ,\nvol. 13, no. 8, pp. 1162‚Äì1175, 2020.\n[82] X. Kitsios, P. Liakos, K. Papakonstantinopoulou, and Y . Kotidis, ‚ÄúSim-\nPiece: Highly accurate piecewise linear approximation through similar\nsegment merging,‚Äù PVLDB , vol. 16, no. 8, pp. 1910‚Äì1922, 2023.\n[83] B. Barbarioli, G. Mersy, S. Sintos, and S. Krishnan, ‚ÄúHierarchical\nresidual encoding for multiresolution time series compression,‚Äù Proc.\nACM Manag. Data , vol. 1, no. 1, may 2023.\n[84] X. Kitsios, P. Liakos, K. Papakonstantinopoulou, and Y . Kotidis, ‚ÄúSim-\npiece: Highly accurate piecewise linear approximation through similar\nsegment merging,‚Äù PVLDB , vol. 16, no. 8, pp. 1910‚Äì1922, jun 2023.\n[85] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, ‚ÄúThe case\nfor learned index structures,‚Äù in Proc. 44th International Conference on\nManagement of Data (SIGMOD) , 2018, pp. 489‚Äì504.\n[86] R. Marcus, E. Zhang, and T. Kraska, ‚ÄúCDFShop: Exploring and opti-\nmizing learned index structures,‚Äù in Proc. International Conference on\nManagement of Data (SIGMOD) , 2020, pp. 2789‚Äì2792.\n[87] A. Kipf, R. Marcus, A. van Renen, M. Stoian, A. Kemper, T. Kraska, and\nT. Neumann, ‚ÄúRadixSpline: a single-pass learned index,‚Äù in Proc. 3rd\nInternational Workshop on Exploiting Artificial Intelligence Techniques\nfor Data Management (aiDM) , 2020.\n[88] P. Ferragina and G. Vinciguerra, ‚ÄúLearned data structures,‚Äù in Recent\nTrends in Learning From Data , L. Oneto, N. Navarin, A. Sperduti, and\nD. Anguita, Eds. Springer International Publishing, 2020, pp. 5‚Äì41.\n[89] C. Wongkham, B. Lu, C. Liu, Z. Zhong, E. Lo, and T. Wang, ‚ÄúAre\nupdatable learned indexes ready?‚Äù PVLDB , vol. 15, no. 11, pp. 3004‚Äì\n3017, 2022.\n[90] Z. Sun, X. Zhou, and G. Li, ‚ÄúLearned index: A comprehensive experi-\nmental evaluation,‚Äù PVLDB , vol. 16, no. 8, pp. 1992‚Äì2004, 2023.\n[91] P. Ferragina, M. Frasca, G. C. Marin `o, and G. Vinciguerra, ‚ÄúOn\nnonlinear learned string indexing,‚Äù IEEE Access , vol. 11, pp. 74 021‚Äì\n74 034, 2023.\n[92] D. Amato, G. Lo Bosco, and R. Giancarlo, ‚ÄúOn the suitability of neural\nnetworks as building blocks for the design of efficient learned indexes,‚Äù\ninProc. 23rd International Conference on Engineering Applications of\nNeural Networks (EANN) , 2022, pp. 115‚Äì127.\n[93] D. Amato, G. L. Bosco, and R. Giancarlo, ‚ÄúStandard versus uniform\nbinary search and their variants in learned static indexing: The case of\nthe searching on sorted data benchmarking software platform,‚Äù Softw.\nPract. Exp. , vol. 53, no. 2, pp. 318‚Äì346, 2023.\n[94] D. Amato, R. Giancarlo, and G. Lo Bosco, ‚ÄúLearned sorted table search\nand static indexes in small-space data models,‚Äù Data , vol. 8, no. 3, 2023.\n[95] M. H. Abrar and P. Medvedev, ‚ÄúPLA-index: A k-mer index exploiting\nrank curve linearity,‚Äù in Proc. 24th International Workshop on Algo-\nrithms in Bioinformatics (WABI) , 2024, pp. 13:1‚Äì13:18.",
  "textLength": 84736
}