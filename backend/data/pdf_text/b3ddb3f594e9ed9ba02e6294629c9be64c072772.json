{
  "paperId": "b3ddb3f594e9ed9ba02e6294629c9be64c072772",
  "title": "Effectively learning spatial indices",
  "pdfPath": "b3ddb3f594e9ed9ba02e6294629c9be64c072772.pdf",
  "text": "Effectively Learning Spatial Indices\nJianzhong Qi1, Guanli Liu1, Christian S. Jensen2, Lars Kulik1\n1School of Computing and Information Systems, The University of Melbourne, Australia\n2Department of Computer Science, Aalborg University, Denmark\n1fjianzhong.qi, guanli.liu1, lkulik g@unimelb.edu.au2csj@cs.aau.dk\nABSTRACT\nMachine learning, especially deep learning, is used increas-\ningly to enable better solutions for data management tasks\npreviously solved by other means, including database index-\ning. A recent study shows that a neural network can not only\nlearn to predict the disk address of the data value associ-\nated with a one-dimensional search key but also outperform\nB-tree-based indexing, thus promises to speed up a broad\nrange of database queries that rely on B-trees for e\u000ecient\ndata access. We consider the problem of learning an index\nfor two-dimensional spatial data. A direct application of a\nneural network is unattractive because there is no obvious\nordering of spatial point data. Instead, we introduce a rank\nspace based ordering technique to establish an ordering of\npoint data and group the points into blocks for index learn-\ning. To enable scalability, we propose a recursive strategy\nthat partitions a large point set and learns indices for each\npartition. Experiments on real and synthetic data sets with\nmore than 100 million points show that our learned indices\nare highly e\u000bective and e\u000ecient. Query processing using our\nindices is more than an order of magnitude faster than the\nuse of R-trees or a recently proposed learned index.\nPVLDB Reference Format:\nJianzhong Qi, Guanli Liu, Christian S. Jensen, and Lars Kulik.\nE\u000bectively Learning Spatial Indices. PVLDB , 13(11): 2341-2354,\n2020.\nDOI: https://doi.org/10.14778/3407790.3407829\n1. INTRODUCTION\nSpatial data and query processing are becoming ubiqui-\ntous. This is due in part to the proliferation of location-based\nservices such as digital mapping, location-based social net-\nworking, and geo-targeted advertising. For example, Google\nMaps includes a large number of spatial objects such as\npoints of interest (POIs). The query \\Search this area (mo-\nbile window view)\" in Fig. 1a is a typical example of spatial\nwindow query . As another example, Foursquare1, a popular\n1https://www.foursquare.com/\nThis work is licensed under the Creative Commons Attribution-\nNonCommercial-NoDerivatives 4.0 International License. To view a copy\nof this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. For\nany use beyond those covered by this license, obtain permission by emailing\ninfo@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 13, No. 11\nISSN 2150-8097.\nDOI: https://doi.org/10.14778/3407790.3407829\n(a)Window query\n (b)kNN query\nFigure 1: Spatial queries\nsocial networking app, has a \\Dinner near me\" (Fig. 1b)\nfunction that returns restaurants sorted by their distances\nto the app user. This is a knearest neighbor (kNN) query,\nwhere the spatial objects are restaurants.\nTo process spatial queries, indices such as R-trees [16],\nkd-trees [5], and quadtrees [12] are used. A tree traversal\nis often required during query processing, which may access\nmany tree nodes and yield decreased performance, especially\nwhen the index is stored in external memory.\nA recent study [26] advances the notion of a learned in-\ndex. A database index is formulated as a function fthat is\nlearned to map a search key to the storage address where the\ncorresponding data object is stored. Given a learned func-\ntion, an object can be located by a function invocation.\nMotivated by the performance bene\fts of learned indices\nfor one-dimensional data [26], we learn an index for spatial\ndata, with a focus on point data. We target applications\nsuch as those mentioned above, where queries are much more\nfrequent than data updates. Our index is highly e\u000ecient for\nqueries, while it also supports dynamic updates.\nTo learn an index, a basic step is to order the data points.\nGiven a set of ordered data points, the learned function f\nmaps a search key p:key to the rank (a percentage value,\ndenoted by p:rank ) of the corresponding data point p. This\ne\u000bectively learns a cumulative distribution function (CDF)\n 2341\n\nof the data set indexed. The address of pis computed as\np:addr =f(p:key )\u0001n, wherenis the data set cardinality.\nTo order spatial points in order to learn an index, an ex-\nisting solution, the Z-order model [46], uses a space-\flling\ncurve (SFC), i.e., the Z-curve [35]. To apply an SFC, the\nunderlying space is partitioned by a grid. The SFC then\nenumerates the cells in the grid. The visiting order gives ev-\nery cell (and the points inside) a curve value , i.e., a Z-value\nfor Z-curves. The Z-order model uses a Z-value as the search\nkeyp:key and learns a function fto predict the rank of the\ncorresponding point pamong the data points sorted by their\nZ-values. For example, in Fig. 2a, there are eight data points\np1;p2;:::;p 8. Their Z-values are shown in parentheses, e.g.,\np3has Z-value 6, meaning that p3:key = 6. Among the eight\npoints, points p1andp2rank ahead of p3by their Z-values,\nthus,p3:rank = 3=8, which is the intended output of f. We\nplot the CDF to be learned in Fig. 2b.\n02/83/84/86/8\n5/87/88/8\n(b) (a)CDFrank\nkey240 105 56554265415\n15 01/8(105)(240)\n(6)\n(5)(55)(42)\n(4)(56)p7p88p\n7p\np p4p\n6p\n5p\np1p3\n1 22p3p4p5p6p\nFigure 2: Index prediction with the Z-order model\nTo use an SFC, a grid must be imposed, but it is di\u000ecult\nto choose an optimal grid resolution. If the grid cells are\ntoo large (i.e., a low resolution), many points may share the\nsame cell, which may yield false positives at query time. If we\nimpose a limit of one point per cell (i.e., a high resolution),\nvery small cells may be needed for dense data sets. When the\ndata distribution is skewed, this leads to a large number of\nempty cells and to uneven gaps between the curve values of\nthe data points. Meanwhile, the ranks are always continuous.\nThis makes it di\u000ecult to learn function fthat maps the\ncurve values to ranks. Fig. 2b has large and uneven gaps\nbetween the Z-values of p3andp4(42\u00006 = 36),p6and\np7(105\u000056 = 49), and p7andp8(240\u0000105 = 135). The\nresult is a CDF with many segments, which is di\u000ecult to\napproximate using a single function f. This di\u000eculty tends\nto increase for more points. For the existing learned spatial\nindex, experiments are run on up to 130,000 points, which\nis below the requirement of some practical applications.\nTo overcome this limitation, we take advantage of the\nstate-of-the-art R-tree bulk-loading technique [37] to order\nthe data points. This technique maps the data points into a\nrank space , orders them in the rank space using an SFC, and\npacks every Bordered points into a disk block ( Bdenotes\nthe block size) to form an R-tree. The rank space has the\nsame dimensionality as the original (Euclidean) data space,\nand the coordinate of a point pin each dimension is the rank\nofpin the corresponding dimension of the original space.\nWhile we do not aim to bulk-load an R-tree, the rank space\nhas the key property to guarantee that there is one point in\nevery row/column of the grid of the SFC. This enables cre-\nating more even gaps between the curve values of the data\npoints, which simpli\fes the function fto be learned.A challenge in applying the above ordering technique is\nthat it creates an n\u0002ngrid givenndata points. When nis\nlarge, this yields a very large grid, and there might be many\ndi\u000berent gaps between the curve values, making it di\u000ecult\nto learn the function f. To address this challenge, we re-\ncursively partition the data set (in the original data space)\nuntil each partition allows a simple feedforward neural net-\nwork (FFN) to learn an accurate function f. The novelty of\nour partitioning strategy is that it is learned by a hierarchy\nof partitioning functions based on the distribution of the\nunderlying data (in contrast to based on the index order of\nthe data [46], detailed in Section 3). A partitioning function\ntakes the coordinates of a data point as input and outputs\na partition ID for the point. Data points with the same par-\ntition ID form a partition. Thus, the partitioning function\ndetermines the partition in which a data point is indexed.\nThis allows us to reuse the partitioning function as an in-\ndexing function to predict data locations, which translates\ninto a highly scalable and accurate learned spatial index.\nWe further design query algorithms for point, window, and\nkNN queries that exploit this index. Our learned index and\nquery algorithms can handle over 100 million points with\nhigh e\u000eciency and accuracy. We also propose algorithms to\nhandle data updates without impinging our query accuracy.\nIn summary, the paper makes the following contributions:\n1. We propose to learn a spatial index based on order-\ning the data points by a rank space-based transforma-\ntion [37]. To scale to large data sets, we design a multi-\ndimensional data partitioning technique and learn an\nindex for each partition in a recursive manner.\n2. We propose algorithms for point, window, and kNN\nqueries that exploit the learned index. In particular,\nourkNN algorithm is the \frst of its kind for a learned\nspatial index. We also propose update algorithms for\nthe learned spatial index.\n3. Using real and synthetic data, our extensive experi-\nments on the proposed index and algorithms show that\nthey can handle data sets of over 100 million points\nwith signi\fcant query performance gains over both tra-\nditional spatial indices such as R-trees and an existing\nspatial adaption of the learned index technique [46].\n2. RELATED WORK\nWe review studies on spatial indices and learned indices.\nSpatial indices. Spatial indices [13] organize spatial data\nto enable e\u000ecient query processing. They can be classi\fed\ninto data partitioning based, space partitioning based, and\nmapping-based indices.\nData partitioning based indices partition the data based\non clusters formed by the data. The R-trees [3, 4, 16, 42] are\ntypical examples. An R-tree is often maintained by means\nof dynamic updates. In that case, the node in which a data\npoint is placed is determined by not only the point's loca-\ntion (i.e., its coordinates), but also by the order in which the\npoints are inserted. Learning an indexing function that maps\nthe coordinates of data points to nodes is thus more di\u000ecult,\nand the resulting function may be less e\u000bective. An alterna-\ntive approach is to pack data points into leaf nodes, thereby\nbuilding, or bulk-loading, an R-tree bottom up. Most pack-\ning procedures [1, 8, 15, 23, 27, 41] rely on some ordering of\nthe points based on their coordinates, e.g., sorting by their\n 2342\n\nx-coordinates [41]. Every Bpoints are packed into a node\naccording to the ordering, where Bis the node capacity.\nThis establishes a relationship between data point locations\nand their leaf nodes, which may be learned.\nSpace partitioning based indices recursively partition the\nspace in which the data is embedded until the spatial ob-\njects in a partition \ft into an index node. Kd-trees [5] and\nquadtrees [12] are typical examples. To learn a meaningful\nmapping between the spatial objects and the nodes in which\nthey are stored, some order on the nodes must be estab-\nlished. Since each node corresponds to a space partition, this\nbecomes a problem of establishing an order on the partitions,\nwhich resembles the idea of an SFC. We discuss this with\nthe mapping-based indices. Attempts also have been made\nto obtain balanced indices using space partitioning [19, 39].\nA typical example is the K-D-B-tree [39], which implements\na kd-tree with a B-tree structure to support block-based\nstorage. We compare with it empirically.\nMapping-based indices map multi-dimensional objects to\none-dimensional values. The mapped values are indexed us-\ning a one-dimensional index, e.g., the B+-tree. SFCs are of-\nten used for mapping spatial objects with low dimensional-\nity. For two popular SFCs, the Z-curve [35] and the Hilbert-\ncurve [10], their curve values are also called Z-values and\nHilbert-values , respectively. The curve value of a cell is used\nas the one-dimensional value (i.e., the index key) to which\na point in this cell is mapped. The curve values of the data\npoints establish a monotonic order over the points. It is nat-\nural to store the points in this order and learn a function\nthat maps a curve value to the rank of the corresponding\npoint, i.e., to the address of the node that holds the point.\nThis is done in a recent learned spatial index [46] using the\nZ-curve (detailed next). There are other mapping schemes,\ne.g., [2, 6, 21], which are designed for non-point or higher\ndimensional objects, and are less relevant here.\nLearned indices. The intuition behind a learned index is\nthat a database index can be thought of as a function fthat\nmaps a search key to the storage address of a data object.\nIf such a function can be learned, a (point) query can be\nprocessed by a function invocation in constant time. This\navoids a logarithmic-time search over a hierarchical index.\nThe recursive model index (RMI) [26] adopts this ap-\nproach and learns a function ffor one-dimensional data us-\ning a feedforward neural network (FFN). The data points\nare \frst sorted. The function fthen maps a search key\np:key to the rank (a percentage value, denoted by p:rank )\nof the corresponding data point p. Function fessentially\nlearns a cumulative distribution function (CDF) on the data\nset. The address of pis computed as f(p:key )\u0001n. To han-\ndle larger data sets, RMI creates a hierarchical structure.\nAt the root level, just one function (termed \\model\" [26])\nf0(p:key ) is learned to predict p:rank2(0;1]. This may only\nyield a rough prediction of p:rank . At the next level (i.e.,\nlevel 1),m1functions are learned, where the i-th function\nf(i)\n1(p:key ) is trained to predict the rank for a data point\npwheref0(p:key )2(i=m 1;(i+ 1)=m1];i2[0;m1). This\nmeans that function f(i)\n1can focus on a subset of the data\npoints whose ranks fall into ( i=m 1;(i+ 1)=m1]. The same\nprocess is repeated recursively to de\fne subsequent levels of\nthe RMI. The function learning (i.e., model training) is done\niteratively starting from f0. TheL2loss is used to minimize\nthe di\u000berence between the predicted ranks and the ground\ntruth ranks of the data points allocated to each function. Anumber of studies (e.g., [11, 14, 18, 24, 43]) follow this idea\nand optimize learned indices on one-dimensional data.\nThe Z-order model [46] extends RMI to spatial data by\nusing a Z-curve to order the data points. Function fnow\nlearns to predict the rank of point pgiven its Z-value as the\nsearch key. At query time, a query point is \frst mapped to\nits Z-value by interleaving the bits of its coordinates. Then,\nfunctionfpredicts the rank (address) given the Z-value.\nSageDB [25] is a learned database system which includes\na learned multi-dimensional index. To learn this index, the\ndata points are successively sorted and partitioned along\na sequence of dimensions into equal-sized cells. Then, the\npoints are ordered by the cells in which they lie, and func-\ntionfis learned to predict the rank of a point given its\ncoordinates. The dimensions used for sorting and the parti-\ntion granularity are both learned. No details are available on\nhow this learning is done and on how the cells are ordered.\nThus, it is not possible to implement and compare empiri-\ncally with this technique. Another study [31, 32] partitions\nad-dimensional space using a ( d\u00001)-dimensional grid and\nlearns an RMI on the d-th dimension for the data points\nin each grid cell. The dimension used for RMI learning, the\nordering of the other d\u00001 dimensions, and the numbers of\ncolumns in those d\u00001 dimensions are learned from a sample\nquery workload to obtain better data selectivity. Once these\nare learned, a cell table records the coordinate ranges of the\ngrid cells, which serve to identify the cells intersected by a\ngiven window query. The RMI for each intersected cell is\nused to re\fne the data points in the cell based on their co-\nordinates in the d-th dimension. This study is not discussed\nfurther as we do not assume a known query workload.\nFour other learned multi-dimensional indices are proposed\nin parallel to ours. The ML-Index [7] adopts the iDistance\ntechnique [20] to map points to one-dimensional values and\nindexes such values with an RMI. The LISA [28] structure\npartitions the data space with a grid, numbers the grid cells\nwith a partially monotonic function, and learns a data par-\ntitioning based on this numbering. The Qd-tree [47] applies\nreinforcement learning to optimize the data partitioning for\na kd-tree-like structure according to a given query workload.\nPolyFit [29] learns polynomial approximations in order to\nprovide e\u000ecient support for approximate multi-dimensional\nrange aggregate queries with guaranteed error bounds.\nUpdate handling can impact the query performance of a\nlearned index. This is because function fmay have predic-\ntion errors. A prediction error range , [err`;erra], needs to\nbe derived from the initial data set from which fis learned,\nsuch that the search for a point pcan be constrained to\n[f(p:key )\u0001n\u0000err`;f(p:key )\u0001n+erra]. Deletions do not\nimpact the error range if the data points are simply \ragged\nas \\deleted.\" Insertions may impact the error range, and a\ntrivial way to handle insertions is to update the error range\nto [err`+i;erra+i] afteriupdates. A tighter estimation of\nthe updated error range [17] is achieved by tracking the error\nrange drifts of a number of reference points . At query time,\nthe closest reference points on both sides of the query point\nare fetched, and their error range drifts are used to estimate\nthe updated error range of the query point with a linear in-\nterpolation. The FITing-Tree [14] uses an additional \fxed-\nsized bu\u000ber for each data segment (an index block) to handle\ninsertions. Data in the bu\u000ber is kept sorted for fast search\nand merge operations. When a bu\u000ber is full, it is merged\nwith its corresponding segment (where new segments may\n 2343\n\n6/8\n4/8\n3/8\n2/8\n1/8\n3/87/8\n5/8\nCDF(c) Rank space(b) Original space(a)8/8\n7/8\n6/8\n5/8\n4/8\n3/8\n2/8\n1/8\nrankyrank rank\n1 12 277 40 48 54 63 x 0 xy\np.cv 1/8 4/8 5/8 6/8 7/8 8/82/88/8\n(54)(63)\n(48)\n(7)(40)\n(27)\n(12)\n(1)p88p8p\n7p\np7p7\n4\n5p6p\np1p2p3p4 p6\np5\n1p3\np5p4p\n2p6p\n1p2p3ppFigure 3: Point ordering and our indexing model\nTable 1: Frequently Used Symbols\nSymbol Description\nP A set of data points\nd The dimensionality of P\nn The cardinality of P\np A data point\nq A query\nM The index structure (mapping function)\nB A data block\nB The capacity of a block\nLM The loss function of M\nN The partitioning threshold\nh The height of RSMI\nO(M)The prediction cost of a sub-model in RSMI\nI The number of new blocks created due to insertions\nbe created). ALEX [9] enhances the RMI by leaving gaps in\ndata nodes and splits nodes to support data insertions.\n3. LEARNING A SPATIAL INDEX\nGiven a set of npointsP=fp1;p2;:::;pngind-dimensional\nEuclidean space, we aim to index Pin a structureMfor e\u000e-\ncient query processing. We consider point, window (range),\nandknearest neighbor ( kNN) queries. For scalability, we\nconsider points storing in external storage (e.g., a hard drive)\nin blocks of capacity B, i.e., a block holds at most Bpoints.\nWe present the structure in Section 3.1 and scale it to large\ndata sets in Section 3.2. For ease of presentation, we use\nd= 2 in the discussion, although our techniques also apply\nto anyd2N+. Table 1 lists frequently used symbols.\n3.1 Index Structure\nThe indexMis a mapping from the coordinates of a point\np2P,p:cord , to the location in the storage that holds data\nrelated top. This location is given by a block ID: p:blk.\nOrdering points. We establish a relationship between\np:blk andp:cord by packing the points into blocks based on\ncoordinates. We exploit the state-of-the-art R-tree packing\nstrategy [37, 38] which takes three steps (cf. Fig. 3).\n(1) The points are \frst mapped to a rank space , which\nis ann\u0002ngrid, where each row and each column has ex-\nactly one point. To perform this mapping, the points are\nsorted by their x-coordinates ( y-coordinates), and the rank\np:rankx(p:ranky) of a point pis used as its x-coordinate\n(y-coordinate) in the rank space, where ties are broken by\nthey-coordinates ( x-coordinates) of the points in the orig-\ninal space. We assume that no two points have the same\ncoordinates in both dimensions. In Figs. 3a and 3b, pointp1is mapped to the second column in the rank space. Point\np3has the same x-coordinate as p1, but itsy-coordinate is\nlarger. Thus, p3is mapped to the third column.\n(2) An SFC (e.g., a Z-curve) goes through the points in\nthe rank space and maps every point pto a curve value p:cv.\nIn Fig. 3b, the curve values are shown in parentheses next\nto the points, e.g., p3:cv= 12.\n(3) The points are sorted in ascending order of their curve\nvalues, and every Bpoints are packed into a block in the\nsorted order. Let p:rank be the rank of point pin the sorted\norder. The block ID for pis computed as:\np:blk =bp:rank\u0001n=Bc (1)\nThis packing strategy was used previously [37, 38] to con-\nstruct R-trees with worst-case optimal window query perfor-\nmance. Here, we use this strategy to obtain a more uniform\ndata distribution and more even gaps between the curve val-\nues of the data points. In Fig. 3c, our minimum and maxi-\nmum gaps between the curve values of two adjacently ranked\npoints are 12\u00007 = 5 and 27\u000012 = 15, while those given by\nthe Z-ordering [46] are 5 \u00004 = 1 and 240\u0000105 = 135, re-\nspectively (cf. Fig. 2). Our variance in the curve value gaps\nis much smaller, leading to a simpler CDF to be learned.\nModel learning. IndexMis learned so that it approxi-\nmately maps p:cord top:blk for each point p:\np:blk\u0019M (p:cord ) (2)\nAny regression model may be used to learn M. We follow\nrecent learned indices [26, 46] and use a multilayer percep-\ntron (MLP, a type of feedforward neural networks), to ex-\nploit its ability to learn a non-linear function. We minimize\ntheL2loss on the model predictions using standard learn-\ning procedures ( stochastic gradient descent , SGD). The loss\nfunctionLMis de\fned as:\nLM=X\np2P(M(p:cord )\u0000p:blk)2(3)\nAfterMis trained, we use it to predict p:blk for every\npointpand record the maximum errors for the cases where\nM(p:cord ) is smaller than p:blk andM(p:cord ) exceedsp:blk,\ndenoted byM:err`andM:erra, respectively.\nM:err`= max\n8p2P(M(p:cord )<p:blk )fjM(p:cord )\u0000p:blkjg(4)\nM:erra= max\n8p2P(M(p:cord )>p:blk )fM(p:cord )\u0000p:blkg(5)\nDiscussion. The design of function Mand its learning\nprocess resemble those of previous learned indices [26, 46].\n 2344\n\nB\nM\nM1,01,1MM1,21,3M\n2,0MM2,12,2MM2,3\n7BB910B\n0,01213B\nB4 6 BB15\n1B B 2 B3B58B11B14B\nB0Figure 4: RSMI index structure ( N= 8 andB= 2)\nThe advantage of Mderives from the rank space based or-\ndering. As discussed above, this ordering creates more even\ngaps between the curve values of the data points. Meanwhile,\nthe gap between of the ranks of two adjacently ranked points\nis a constant (1 =n). This makes it easier to learn a mapping\nfrom the coordinates (which determine the curve values) to\nblock IDs (which are determined by the ranks, cf. Fig. 3c).\n3.2 Scaling to Large Data sets\nLarger data sets make it more di\u000ecult for a single func-\ntion to map all data points to their ranks. We propose a re-\ncursive spatial model index (RSMI) for such cases. Suppose\nthat functionMcan predict block IDs of up to Npoints\nwith su\u000eciently high accuracy. Such an Nvalue depends on\nthe data distribution and the model learning capacity, and\nit may be determined empirically. Since Npoints occupy\ndN=Beblocks, this means that we have a function that can\npredictdN=Bedi\u000berent block IDs with a high accuracy.\nGiven a data set Pwith cardinality n > N , our RSMI\nmodel recursively partitions Puntil each partition has at\nmostNpoints. Then, for each partition, we learn an index-\ning model following the procedure in Section 3.1.\nThe partitioning strategy works as follows. We start by\npartitioning the full data set PintodN=Bepartitions (such\nthat we may reuse Mto predictdN=Bedi\u000berent parti-\ntion IDs later). This partitioning is done via a 2blog4N=Bc\u0002\n2blog4N=Bcgrid with 4blog4N=Bc\u0014N=B cells that form at\nmostdN=Bepartitions. In Fig. 4, when N= 8 andB= 2,\n2blog4N=Bc\u00022blog4N=Bc= 2\u00022 cells partition the space.\nThe grid is created by \frst cutting the data space into\n2blog4N=Bccolumns where each column has dn=2blog4N=Bce\npoints (i.e., partitioning by x-coordinates). We further cut\neach column individually into 2blog4N=Bccells such that each\ncell has at mostdn=4blog4N=Bcepoints (i.e., partitioning by\ny-coordinates). The grid follows the data distribution and is\nnon-regular, but is still a 2blog4N=Bc\u00022blog4N=Bcgrid. We\ncan apply an SFC of order blog4N=Bcto the grid to obtain a\ncurve value for each cell. We learn a mapping function M0;0\n(i.e., the 0-th model at level 0 of RSMI) to map a point\np2Pto the curve value corresponding to its cell. To learn\nM0;0, we reuse the model structure used for M(since we\nknow thatMpredictsdN=Bedi\u000berent values with a high\naccuracy). The loss function for the learning also resembles\nthat ofM(Equation 3), except that now the ground truth\nis notp:blk but the curve value of pin the grid forM0;0.\nOnceM0;0is trained, we use it to predict a curve value\nfor each point p2P. Since a learned model may not befully accurate,M0;0(p:cord ) may not yield the curve value\nofp. We group the points in Pby the predicted curve values.\nThis results in a learned point grouping, as illustrated by the\ndi\u000berently colored points in Fig. 4. For the j-th group, if it\nstill has more than Npoints, we repeat the above partition-\ning procedure to learn a function M1;j(e.g.,M1;2) that\npredicts the partition membership of each point pin this\ngroup. Otherwise, we follow the procedure in Section 3.1 to\nlearn a function M1;jto predictp:blk for each point pin\nthis group. We call such a function a leaf model .\nWhen the partitioning is complete and the data points\nare all packed into blocks, in each block, we further store\npointers (block IDs) to its preceding and subsequent blocks.\nIn Fig. 4,Birepresents a block, and arrows between blocks\nrepresent the pointers. These pointers allow data scans for\nqueries. The order of blocks under the same leaf model fol-\nlows that of the data points in the blocks. The order of blocks\nunder the di\u000berent leaf models follows the order of the par-\ntition IDs (recursively) corresponding to the leaf models.\nDiscussion. Our RSMI model looks similar to the RMI\nmodel [26] at a \frst glance. However, the design strategies\nof RSMI and RMI are fundamentally di\u000berent. In RMI, each\nsub-modelMi;jhandles data points whose predicted ranks\n(i.e., model output ) fall in a range ( j=mi;(j+ 1)=mi];j2\n[0;mi), assuming misub-models at level i). Due to the na-\nture of SFCs, points ranked adjacently can have vastly dif-\nferent curve values and may be quite far apart (e.g., p7and\np8in Fig. 2). Thus, each sub-model may still need to han-\ndle points with very di\u000berent curve values, and the mapping\nto ranks is still di\u000ecult to learn. In contrast, in RSMI, each\nsub-modelMi;jhandles data points whose coordinates (i.e.,\nmodel input ) fall into a region. This allows each sub-model\nto focus on a subset of points nearby, which may facilitate\nbetter prediction accuracy, to be veri\fed in experiments.\nAlso note that we have not used rank space for the higher-\nlevel sub-models in RSMI. The impact of skewed data dis-\ntribution is mitigated by the use of non-regular grids and\nthe partitioning of data points by model predictions.\nFor query processing, RSMI only requires a function invo-\ncation to determine the (single) sub-model to be accessed at\neach level. As shown in the experiments, this is more e\u000ecient\nthan traditional hierarchical indices that require scanning\nand comparing entries (e.g., MBRs) in the inner nodes.\n4. QUERY PROCESSING\nWe present algorithms to process point (Section 4.1), win-\ndow (Section 4.2), and kNN (Section 4.3) queries using RSMI.\n 2345\n\n4.1 Point Queries\nAlgorithm 1: Point Query\nInput:q: a query point.\nOutput: A pointer to the point indexed in our RSMI\nstructure that has the same coordinates as q.\n1i 0;j 0;\n2whileMi;jis not a leaf model do\n3j Mi;j(q:cord );i i+ 1;\n4j Mi;j(q:cord );\n5forj02[j\u0000M:err`;j+M:erra]do\n6 ifq2Bj0then\n7 return a pointer to the point in Bj0that has the\nsame coordinates as q;\n8return NULL;\nAs summarized in Algorithm 1, point queries are done via\na recursive call of the sub-models in the RSMI using the\ncoordinates of a query point qas the input (Lines 1 to 3).\nThe process starts from the root sub-model M0;0. At each\nlevel, only one sub-model needs to be visited. Let Mi;jbe\nthe sub-model visited at level i. Then, at level i+ 1, the\nsub-model to be visited is Mi+1;j0wherej0=Mi;j(q:cord ).\nThis process continues until a leaf model is reached. The leaf\nmodel output is the predicted block ID of q,M(q:cord ). We\nexamine the corresponding block and its neighboring blocks\nas bounded by [M(q:cord )\u0000M:err`;M(q:cord )+M:erra].\nIfqis found, we return a pointer to the found point (Lines\n4 to 7). Otherwise, qis not in our structure (Line 8).\nCorrectness. The algorithm correctness is guaranteed by\nthe RSMI structure. As Fig. 4 shows, if j0=Mi;j(p:cord ),\npointpis only indexed by Mi+1;j0at the next level. Thus,\nifj0=Mi;j(q:cord ) forq, we only need to access Mi+1;j0at\nthe next level. At the leaf level, the model prediction may\nhave an error, which is bounded by M:err`andM:erra.\nScanning the blocks in [ M(q:cord )\u0000M:err`;M(q:cord ) +\nM:erra] guarantees no false negatives.\nQuery cost. There are two cost components. The \frst is\nfor model prediction, which consists of numerical computa-\ntions. This cost depends on the prediction model. We use\nO(M) to denote the prediction cost of a sub-model. For ex-\nample, consider a simple fully connected feedforward neural\nnetwork with two neurons in the input layer, mneurons in\nthe (only) hidden layer, and a single neuron in the output\nlayer. Then, O(M) =O(2m). Given that RSMI has height\nh, we needO(hM) time to predict a block ID for q. The\nsecond cost component is for locating the query point on\ndisk. In the best case, we need to access the disk block with\nIDM(q:cord ), which takes O(B) time (scanning Bpoints\nin the block). In the worst case, we need to access M:err`+\nM:erra+1 blocks with O((M:err`+M:erra)B) time. Over-\nall, a point query takes O(hM+(M:err`+M:erra)B) time.\n4.2 Window Queries\nFig. 5a illustrates the window query algorithm. The solid\nrectangle in the middle denotes a window query q. The grid\ncells inqare covered by several red segments of the SFC.\nProcessing query qmeans \fnding the points corresponding\nto the curve segments in q. For each such segment, if we\ncan locate the storage location of the points corresponding\nto the minimum and maximum curve values of the segment\nthen any other points on the curve segment must be stored\n3/88/8\n7/8\n6/8\n5/8\n4/8\n2/8\n1/8\n8/87/86/85/84/83/82/81/87p8p\n16p\nqh\nlp\nq4\np5\np2p3\np(a)Window query\n6/88/8\n7/8\n5/8\n4/8\n3/8\n2/8\n1/8\n8/87/86/85/84/83/82/81/8p8p\n7\n3\n14\npq\n6p\np\n5p\n2p\np (b)kNN query ( k= 3)\nFigure 5: Query examples (The queries are plotted in the\nrank space for ease of illustration. They are processed in\nthe original space since RSMI takes point coordinates in the\noriginal space as input and outputs the predicted block ID.)\nbetween these two points, which form the query answer. This\nis because the data points are stored in curve value order.\nIdentifying the points for the minimum and maximum\ncurve values of the curve segments is feasible (e.g., via query\nwindow mapping [48]). However, locating such points im-\nplies issuing point queries, which may cause repetitive scan-\nning over the same blocks (due to prediction errors).\nTo avoid excessive data scans, the window query algo-\nrithm only locates the points corresponding to the minimum\nand maximum curve values among allcurve segments in q.\nWe thus only need two point queries for a window query:\n(1) Compute two points qlandqhinqwith the minimum\nand the maximum curve values, respectively.\n(2) Compute point queries for qlandqhand retrieve all\npoints between them. This forms a superset of the query\nanswer, since some of these points may not be in q(p4in\nFig. 5a is between p3andp6but not inq).\n(3) Filter the points retrieved in Step 2 with qand return\nthe remaining points.\nWe focus on locating qlandqhin Step 1. Steps 2 and 3\nare done by Algorithm 1 and a simple block scan.\nThe locations of qlandqhdepend on the SFC. We discuss\nthe cases for two commonly used SFCs, the Z-curve and the\nHilbert-curve. For the Z-curve, qlandqhare the bottom\nleft and the top right corners of the query window, respec-\ntively [30]. This is because a Z-curve goes through the space\nfrom the bottom left to the top right recursively. For exam-\nple, in Fig. 5a, the bottom left and top right cells in qare\nthe two with the minimum and the maximum curve values.\nFor the Hilbert-curve, qlandqhmust be on the boundary of\nq[48]. Computing qlandqhfrom the boundary in the rank\nspace requires B-tree searches to map the point coordinates\nto their ranks [37, 38]. To avoid constructing and search-\ning B-tree indices, we heuristically use the four corners of q,\ndenoted by qxl,qxh,qyl, andqyh, for query processing.\nAlgorithm. Based onqlandqh, Algorithm 2 summa-\nrizes the window query algorithm. We \frst obtain qlandqh\n(Line 1). We query these two points using our point query\nalgorithm (Line 2). If ql(qh) is found, we use its block ID\nql:blk(qh:blk) as the lower (upper) bound for data scan-\nning. Otherwise, we use M(ql:cord )\u0000M:err`(M(qh:cord )+\nM:erra) to approximate the bound (Lines 3 to 10). We\nthen scan the data blocks between the lower and upper\nbounds and return the points in q(Lines 11 to 15). For\nHilbert-curves, we need point queries for qxl,qxh,qyl, and\nqyh. The lower and upper bounds for data scanning be-\n 2346\n\ncome minfM(qi:cord )\u0000M:err`gand maxfM(qi:cord ) +\nM:errag, respectively, where qi2fqxl;qxh;qyl;qyhg.\nAlgorithm 2: Window Query\nInput:q: a window query.\nOutput:S: the set of data points in q.\n1Obtain points qlandqhfor query window q;\n2Run point queries for qlandqh;\n3ifqlis found then\n4begin ql:blk;\n5else\n6begin M (ql:cord )\u0000M:err`;\n7ifqhis found then\n8end qh:blk;\n9else\n10end M (qh:cord ) +M:erra;\n11fori2[begin;end ]do\n12 forp2Bido\n13 ifpinqthen\n14 S S[fpg;\n15returnS;\nQuery cost. Algorithm 2 runs two (four for Hilbert-\ncurves) point queries and a data scan. The point queries take\nO(hM+ (M:err`+M:erra)B) time, as discussed earlier.\nThe maximum number of blocks scanned is ( M(qh:cord ) +\nM:erra)\u0000(M(ql:cord )\u0000M:err`). The overall query time is\nO(hM+(M:err`+M:erra)B+(M(qh:cord )+M:erra)B\u0000\n(M(ql:cord )\u0000M:err`)B) =O(hM+(2M:err`+2M:erra+\nM(qh:cord )\u0000M (ql:cord ))B). For Hilbert-curves, the query\ntime isO(hM+(2M:err`+2M:erra+maxfM(qi:cord )g\u0000\nminfM(qi:cord ))B), whereqi2fqxl;qxh;qyl;qyhg.\nDiscussion. Algorithm 2 o\u000bers approximate answers. The\nerrors occur for two reasons. First, when Hilbert-curves are\nused, the four corners of qmay not correspond to the exact\nminimum and maximum curve values bounded by q. Second,\nwhen querying qlandqh(orqxl;qxh;qyl;andqyh), if these\npoints are not indexed in the RSMI, we may not return the\npoint just next to them. Thus, the point queries may not\nbound the accurate blocks to be scanned for q.\nEmpirically, the query answer errors are small. Experi-\nments under various settings show that the query answer\naccuracy (i.e., recall) is consistently above 87%. Our query\nanswer does not have false positives (i.e., we may miss points\ninqbut will not return any points outside q). This suits ap-\nplications that need to quickly identify regions with POIs\nbut may not require all POIs in the region, e.g., to recom-\nmend a region with many restaurants to a user.\nThe RSMI can also o\u000ber exact query answers with a simple\nmodi\fcation to store a minimum bounding rectangle (MBR)\nfor each sub-model in its parent model. This modi\fcation en-\nables an R-tree-style tree traversal of the RSMI to compute\nexact query answers with reasonable e\u000eciency.\n4.3 K Nearest Neighbor Queries\nWe can use R-tree kNN algorithms (e.g., the best-\frst al-\ngorithm [40]) to compute kNNs accurately using the RSMI\nwith MBRs. Below, we design a prediction-based approxi-\nmatekNN algorithm to achieve high query e\u000eciency.\nOur approximate kNN algorithm follows a classical search\nregion expansion paradigm. It starts with a small search\nregion around the query point qand keeps expanding until\nkpoints are found. Each time the search region is expanded,a window query is run using Algorithm 2 to \fnd points in\nthe region. See Fig. 5b for a 3NN example. The query point\nqis represented by a red dot, and the window queries run\nare represented by the two solid rectangles enclosing q.\nAlgorithm. Algorithm 3 summarizes the approximate al-\ngorithm, where Qis a queue to store the found NNs prior-\nitized by their distances to q(Line 1). The algorithm \frst\nestimates an initial search region based on the data distri-\nbution. Intuitively, if the points are uniformly distributed\nin a unit space, a search region of size k=naroundqis ex-\npected to contain kpoints. We thus use a rectangular-shaped\nsearch region centered at qwith width \u000bxp\nk=nand height\n\u000byp\nk=nas the initial search region (Line 2). Here, \u000bxand\n\u000byareskew parameters to adjust the search region size in\nboth dimensions according to data skew { \u000bx=\u000by= 1 for\nuniform data. We estimate \u000bxand\u000bylater.\nUsing the initial search region, we run a window query to\n\fnd the data blocks containing points in the region (Lines 4\nand 5). We go through these blocks. If a block Biis closer\ntoq(measured by the MINDIST metric [40]) than the cur-\nrently found k-th NN,Q[k], we go through every point p2Bi\nand addptoQifpis closer to qthanQ[k] (Lines 6 to 9).\nNext, if there are less than kpoints inQ, we double the\nwidth and the height of the search region (Lines 10 and\n11). If the distance between qandQ[k] (dist(q;Q[k])) ex-\nceedsp\nwidth2+height2=2 (i.e.,Q[k] is outside the current\nsearch region), we also enlarge the width and height to be\n2\u0001dist(q;Q[k]) (Lines 12 and 13). Otherwise, we terminate\nand return the \frst kpoints inQ(Lines 14 to 16).\nAlgorithm 3: kNN Query\nInput:q: a query point; k: the number of targeted NNs\nOutput:S: the set of (approximate) kNNs ofq.\n1Initialize an empty priority queue Q;\n2width \u000bxp\n(k=n);height \u000byp\n(k=n);\n3while TRUE do\n4 Construct window query wqwithq,width , andheight ;\n5 Querywqto obtain data block ID range [ begin;end ];\n6 fori2[begin;end ]do\n7 ifBiis unvisited AND\n(mindist (q;Bi)<dist (q;Q[k]) ORQ:size ()<k)\nthen\n8 forp2BiAND (dist(q;p)<dist (q;Q[k]) OR\nQ:size ()<k)do\n9 InsertpintoQ;\n10 ifQ:size ()<kthen\n11width 2\u0001width ;height 2\u0001height ;\n12 else ifdist(q;Q[k])>p\nwidth2+height2=2then\n13width 2\u0001dist(q;Q[k]);height 2\u0001dist(q;Q[k]);\n14 else\n15break ;\n16returnS \frstkelements in Q;\nEstimating \u000bxand\u000by.When the data distribution is\nnon-uniform, we estimate \u000bxand\u000byby learning the CDF\nin thex- andy-dimensions. Let CDFx(X) be a function to\npredict the percentage of points with an x-coordinate less\nthan or equal to X. Then,\u000bxis estimated by the slope of\nCDFx(X) at thex-coordinate q:cordxof the query q:\n\u000bx= \u0001=(CDFx(q:cordx+ \u0001)\u0000CDFx(q:cordx)) (6)\nHere, \u0001 is a system parameter (0 :01 in our experiments).\nWe estimate \u000bywith CDFy(Y) for they-dimension.\n 2347\n\nObtaining CDF is expensive. We compute a piecewise\nmapping function [48]PMFx(X) to approximate CDFx(X)\n(and PMFy(Y) for CDFy(Y)). We partition the data set\ninto\rpartitions by their x-coordinates and compute a cu-\nmulative count xi:cfor the boundary point xiof each par-\ntition. We let PMFx(xi:cord ) =xi\u00001:c=n, wherexi:cord is\nthex-coordinate of xi. We compute piecewise linear func-\ntions to connect every pair of points ( xi:cord;xi\u00001:c=n) to\nform PMFx(X). We use\r= 100 in the experiments.\nQuery cost. The key factor that determines the time\ncost of akNN query is the search region size. A small initial\nsearch region may incur more searches later, while a large\ninitial search region may incur many unnecessary data block\naccesses. In the worst case, the number of window queries\nneeded is log21=(minf\u000bx;\u000bygp\nk=n), which is the number\nof times that the search region width/height needs to be dou-\nbled to cover a unit space. Let Wbe the time cost of a win-\ndow query derived in Section 4.2. Then, the worst-case time\ncost of akNN query is O(Wlog21=(minf\u000bx;\u000bygp\nk=n)).\n5. UPDATE HANDLING\nOur index allows both insertions and deletions of data.\nGiven a new point pto be inserted, we \frst run a point query\nforp. We insert pinto the block predicted by the query.\nThere are two cases: (1) If the predicted block has space for\np(e.g., space left by a deleted point), we simply place pin\nthe block. (2) If the predicted block is full, we create a new\nblockB, placepinB, and insertBas the next block of the\npredicted block in the list of data blocks. We mark Bas an\ninserted block such that it does not count towards the error\nbounds (M:err`andM:erra) during query processing. In\neither case, we need to recursively update the MBRs of the\nancestor models indexing pto complete the insertion. An\ninsertion takes O(hM+ (M:erra+M:err`)B+IB+h)\ntime, where O(hM+ (M:erra+M:err`)B) is the point\nquery time to locate p,O(IB) is the additional search time\non the blocks created after the predicted block by previous\ninsertions, and O(h) is the MBR update time.\nGiven a point pto be deleted, we run a point query for\np. When the data block containing pis found, we swap p\nwith the last point in this block and mark pas \\deleted.\"\nWe then recursively update the MBRs of the ancestor mod-\nels. We do not delete a disk block when it under\rows (e.g.,\nwhen becoming half empty), to ensure the validity of the er-\nror bounds. The deletion also takes an O(hM+ (M:erra+\nM:err`)B+IB+h) time, as it also needs a point query to\nlocatepand entails recursive MBR updates. We omit the\npseudo-code of these update algorithms for succinctness.\nInsertions and deletions impact the layout of the learned\nindex and hence impact the query performance (i.e., adding\nanO(IB) cost for each point query). A periodic rebuild may\nbe run (e.g., overnight) to retain a high query e\u000eciency.\n6. EXPERIMENTS\nWe report on experimental results on the RSMI.\n6.1 Experimental Setup\nThe experiments are done on a computer running 64-bit\nUbuntu 20.04 with a 3.60 GHz Intel i9 CPU, 64 GB RAM,\nand a 1 TB hard disk. We use PyTorch 1.4 [36] and its C++\nAPIs to implement the learned indices based on CPU. TheTable 2: Parameters and Their Values\nParameter Setting\nDistribution Uniform, Normal, Skewed\nn(million) 1, 2, 4, 8, 16, 32, 64, 128\nQuery window size (%) 0.0006, 0.0025, 0.01, 0.04, 0.16\nQuery window aspect ratio 0.25, 0.5, 1, 2, 4\nk 1, 5,25, 125, 625\nInserted points (%) 10, 20, 30, 40, 50\nDeleted points (%) 10, 20, 30, 40, 50\ntraditional indices are implemented using C++ (except for\nthe RR\u0003, which was implemented in C [4]) based on CPU.\nCompetitors. We compare with the following indices:\n(1) Z-order model [46] ( ZM): This model predicts the\nstorage address of a data point by its Z-value (cf. Section 2).\nWe implement a recursive version of the model with three\nlevels with 1,p\nn=B2, andn=B2sub-models each.\n(2) Grid File [33] ( Grid ): This index partitions the data\nspace with a regular grid, assigns data points to cells based\non their coordinates, and stores the data points by their\ncells. We use ap\nn=B\u0002p\nn=B grid, i.e.,Bpoints (one\nblock) per cell under a uniform distribution.\n(3) K-D-B-tree [39] ( KDB ): This index implements a kd-\ntree [5] with a B-tree structure to support block storage.\n(4) Rank space based R-tree [37, 38] ( HRR ): This is an\nR-tree bulk-loaded using the rank space technique (cf. Sec-\ntion 3.1) and a Hilbert-curve for the ordering. This R-tree\no\u000bers the state-of-the-art window query performance.\n(5) Revised R\u0003-tree [4] ( RR\u0003): This is an improvement of\nthe R\u0003-tree, which has shown strong query performance.\nKDB, HRR, and RR\u0003have up to \fve levels in the exper-\niments (on data sets with up to 128 million points).\nImplementation. We use the original implementation of\nHRR and RR\u0003. For Grid, ZM, and KDB, no source code is\navailable. We use the static component of a Grid File for\nmoving objects [22] for Grid. We implement ZM and KDB\nfollowing their papers. We run all indices and algorithms in\nmain memory for ease of comparison (it is straightforward\nto place the data blocks in external memory).\nFor all structures, we use data blocks with a capacity of\n100, i.e.,B= 100. The R-tree and K-D-B-tree leaf nodes\n(blocks) and the Grid File blocks can store up to 100 points\neach, while the internal nodes of the tree structures store up\nto 100 MBRs. No bu\u000bering is assumed.\nFor both ZM and RSMI, each sub-model (e.g., Mi;jin\nRSMI) is a multilayer perceptron (MLP) with an input layer,\na hidden layer, and an output layer. The number of nodes\nin the hidden layer equals the sum of the number of input\nattributes and the number of output classes divided by two,\ne.g., 51 in RSMI, where the input is two coordinates and\nthe output has 100 di\u000berent block ID values. We use the\nsigmoid activation function for the hidden layer. The MLPs\nare trained level by level, starting from the root sub-model\n(cf. Section 3.2), with a learning rate of 0.01 and 500 epochs\nper MLP. RSMI uses a partitioning threshold N= 10;000,\ni.e., a leaf model handles at most 10,000 data points. It learns\nthe number of levels and the number of sub-models at each\nlevel adaptively for each data set. For ease of model training,\nthe point coordinates and block IDs are normalized into the\nunit range. RSMI uses Hilbert-curves for ordering as these\nyield better query performance than Z-curves.\nData sets. We use two real data sets: Tiger andOSM .\nTiger contains over 17 million rectangles (950 MB in size)\n 2348\n\nrepresenting geographical features in 18 Eastern states of\nthe USA [45]. We use the centers of the rectangles as our\ndata points. OSM contains over 100 million points (2.2 GB\nin size) in the USA extracted from OpenStreetMap [34].\nWe generate three groups of synthetic data sets, Uni-\nform ,Normal , and Skewed , with up to 128 million points\n(2.5 GB in size). The synthetic data falls into the unit square.\nUniform and Normal data sets follow uniform and normal\ndistributions, respectively. Skewed data sets are generated\nfrom uniform data by raising the y-coordinates to their pow-\nersy\u000b(\u000b= 4 by default), following HRR [37, 38].\nAs summarized in Table 2, we vary the data set size n,\nthe query window size and aspect ratio, the query param-\neterk, and the percentages of points inserted or deleted,\nrespectively. Default settings are shown in boldface.\nWe generate queries that follow the data distribution for\neach set of query experiments and report the average re-\nsponse time andnumber of block accesses per query.\nThe block accesses serve as a performance indicator for an\nexternal memory based implementation of the algorithms.\n6.2 Results\nWe report results on the impact of N, point, window, and\nkNN query processing, and update handling.\n6.2.1 Impact of RSMI Partition Threshold N\nTable 3: Impact ofN\nN 2,500 5,000 10,000 20,000 40,000\nConstruction time (s) 10,997 8,215 7,553 7,602 7,161\nHeight 9 5 4 4 3\nIndex size (MB) 488.5 425.5 405.5 398.9 391.3\nQuery # block accesses 1.28 1.35 1.44 1.47 1.52\nQuery time ( \u0016s) 1.79 1.59 1.44 1.47 1.49\nWe \frst study the impact of Nto optimize RSMI. As Ta-\nble 3 shows, when Nincreases (from 2,500 to 40,000), the\nindex construction time, height, and index size all decrease\noverall. This is because a larger Nmeans more points in\neach partition and fewer partitions, leading to fewer levels\nand sub-models, and hence shorter training times. Mean-\nwhile, the average number of block accesses per point query\nincreases, because the leaf models become less accurate. The\npoint query time, however, \frst decreases and then increases\nagain. This is resulted from a combined e\u000bect of fewer sub-\nmodels to compute while more data blocks to examine as N\nincreases. The query time is the shortest when N= 10;000.\nWe use this Nvalue in the rest of the experiments.\n6.2.2 Point Queries\nWe use all data points in each data set as the query points\nand report the average performance per point query.\nVarying the data distribution. As Fig. 6 shows, RSMI of-\nfers the best query performance on both real (Tiger and\nOSM) and synthetic (Normal and Skewed) data. It improves\nthe query time by at least 1.3 times and up to 5.5 times com-\npared with the competing techniques, i.e., 3.0 \u0016s vs. 3.8\u0016s\n(KDB) and 16.5 \u0016s (Grid) on OSM. It also improves the\nnumber of block accesses by at least a factor of 5.3 (1.4 vs.\n7.4 for RSMI vs. Grid on Skewed) and up to 77.5 times (1.3\nvs. 100.8 for RSMI vs. Grid on OSM). Grid works better\non Uniform data, as such data can be partitioned relatively\nevenly across the cells and take full advantage of Grid. Note\n10-1100101102\nUni. Nor. Ske. Tig.OSMResponse time ( s)\nData distribution(a)Query time\n100101102103\nUni. Nor. Ske. Tig. OSM# block accesses\nData distributionGrid\nHRRKDB\nRR*RSMI\nZM (b)# block accesses\nFigure 6: Point query vs. data distribution\nthat Grid has much higher numbers of block accesses than\nthe other techniques while its running time may not seem\nas high, because it features a simple checking procedure per\nblock, and the blocks are stored in memory.\nTable 4: Prediction Error Bounds ( M:err`,M:erra)\nUni. Nor. Ske. Tig. OSM\nZM (\u0002104)(1.9, 1.9 ) (1.2, 5.5) (0.9, 3.7) (0.7, 0.7) (7.4, 11)\nRSMI (43, 82) (37, 91) (55, 78) (70, 69) (89, 74)\nThe strength of RSMI comes from its fast and accurate\npredictions. Table 4 summarizes the maximum prediction\nerrors (M:err`,M:erra) of ZM and RSMI. For example,\nM:err`andM:erraof RSMI on Skewed are 55 and 78\nblocks, respectively. The average number of block accesses is\nmuch lower than these bounds, e.g., 1.4 for RSMI on Skewed.\nZM o\u000bers less accurate predictions due to its design. On\nSkewed, its prediction error can be as large as 3 :7\u0002104\nblocks, and its average number of block accesses is 8.1 (bi-\nnary search on the Z-values is used to reduce the number\nof block accesses). KDB, HRR, and RR\u0003incur fewer block\naccesses than ZM does. However, they still need to access in-\nner nodes. Also, due to overlapping node MBRs, the R-trees\nmay need to access multiple nodes at each tree level.\nWe further report the average depth of RSMI, i.e., the av-\nerage number of sub-models invoked to reach a data block,\nwhich are 3.11, 3.26, 3.30, 3.04, and 4.01 on Uniform, Nor-\nmal, Skewed, Tiger, and OSM, respectively. RSMI only needs\n3 or 4 function invocations to locate a data block. KDB,\nHRR, and RR\u0003have a depth of 3 on the \frst four data sets\nand 4 on OSM (excluding the data block level). They need\nto scan 3 or 4 nodes (maybe more for R-trees due to over-\nlapping MBRs) to locate a data block, which is slower. Grid\nand ZM have \fxed depths of 1 and 3. They su\u000ber from the\nnumber of block accesses as discussed above.\nAnother observation concerning ZM vs. RSMI is that ZM\nsu\u000bers more in terms of block accesses and less in terms\nof response time. This is because ZM can quickly skip a\ndata block accessed by testing whether the Z-value of the\nquery point belongs to the Z-value range of the block. Its\nprocessing time per block is smaller than that of RSMI.\nFig. 7a shows the index size. The learned indices are the\nsmallest because they only store the data blocks and the\n(small) sub-models. In contrast, Grid stores the data blocks\nand a cell table that maps grid cells to the corresponding\ndata blocks; KDB, HRR, and RR\u0003store the data blocks\n(leaf nodes) and the internal nodes. RSMI has a slightly\nlarger index size than ZM due to its slightly larger number of\nsub-models. This is because RSMI is constructed adaptively\nbased on the data distribution, while the number of sub-\nmodels in ZM is determined by the number of data points\n 2349\n\n102103104\nUni. Nor. Ske. Tig. OSMSize (MB)\nData distribution(a)Index size\n100101102103104105106\nUni. Nor. Ske. Tig. OSMTime (s)\nData distributionGrid\nHRRKDB\nRR*RSMI\nZM (b)Construction time\nFigure 7: Index size and build time vs. data distribution\n(cf. Section 6.1). For example, on OSM, RSMI has 10,445\nsub-models, while ZM has 10,101 (2,303 MB vs. 2,244 MB,\ni.e., 2.6% larger). RR\u0003is the largest because its nodes are\nless compact. HRR is also larger than RSMI because it uses\ntwo extra B-trees for its rank space mapping [37, 38].\nThe advantages of RSMI in query performance and index\nsize come with a higher construction time (Fig. 7b), which\nis a common characteristic of learned indices. RSMI can be\ntrained within 16 hours on OSM (over 100 million points),\nwhich is justi\fed given its query performance. This training\ntime assumes CPUs. If GPUs are used, we can reduce the\ntraining time by over 74%. For example, training RSMI on a\nSkewed data set with 128 million points takes 60,514 seconds\non a CPU. This can be reduced to 15,698 seconds on an RTX\n2080 Ti GPU. ZM is faster than RSMI at training, because\nRSMI needs to \frst sort and partition the data points each\ntime it learns a sub-model. ZM only sorts the data points\nonce for learning allsub-models. Among the traditional in-\ndices, HRR is bulk-loaded, which only takes a few rounds\nof sorting and data scans. This is faster than RR\u0003, which is\ncreated by means of top-down insertions. Grid and KDB are\nthe fastest due to their simple sorting-based construction.\n10-1100101102\n1248163264128Response time ( s)\nData set size (million)\n(a)Query time\n100101102\n12 481632 64128# block accesses\nData set size (million)Grid\nHRRKDB\nRR*RSMI\nZM (b)# block accesses\nFigure 8: Point query vs. data set size\n101102103104\n12 481632 64128Size (MB)\nData set size (million)\n(a)Index size\n10-1100101102103104105106\n1248163264128Time (s)\nData set size (million)Grid\nHRRKDB\nRR*RSMI\nZM (b)Construction time\nFigure 9: Index size and construction time vs. data set size\nVarying the data set size. In Fig. 8, we vary the data set\nsize using Skewed data. Results on the other data sets show\nsimilar patterns and are omitted due to the space limitation.\nThe same applies in the remaining experiments.\nAs expected, the query costs increase with the data set\nsize. RSMI o\u000bers the lowest query costs, and the advantageis up to 5.8 times (1.3 \u0016s vs. 7.6\u0016s for RSMI vs. ZM with 8\nmillion points). We observe a slight drop in the number of\nblock accesses for RSMI. When there are more points, RSMI\nmay learn a structure with more levels, where the leaf models\nare more compact, yielding more accurate predictions and\nfewer block accesses. The query time still increases as there\nare more levels { the average depth increases from 2.49 to\n4.46 for 1 to 128 million points, and the maximum depth is\n10. These \fndings indicate that RSMI is scalable.\nThe index sizes and construction times also increase with\nthe data set size, as shown in Fig. 9. RSMI is consistently\nsmall in size, while its construction time does not grow dras-\ntically (which can be further reduced using GPUs, as men-\ntioned above). This allows it to scale to very large data sets.\n6.2.3 Window Queries\nWe generate 1,000 window queries under each setting and\nreport the average cost per query. Since the number of block\naccesses aligns with the query response time, we omit cov-\nerage of the number of block accesses. Also, index size and\nconstruction time are independent of the query type and\nare omitted hereafter. As learned indices may o\u000ber approxi-\nmate window query answers (without false positives, cf. Sec-\ntion 4.2), we report their recall | the number of points\nreturned over the cardinality of the ground truth answer.\nWe add one more technique called RSMIa to the com-\nparison. It o\u000bers accurate query answers by performing an\nR-tree-like traversal by utilizing MBRs associated with the\nsub-models in RSMI, as described at the end of Section 4.2.\n10-210-1100101102\nUni. Nor. Ske. Tig.OSMResponse time (ms)\nData distributionGrid\nHRR\nKDBRR*\nRSMI\nRSMIaZM\n(a)Query time\n 0 0.2 0.4 0.6 0.8 1\nUni. Nor. Ske. Tig.OSMRecall\nData distribution (b)Recall\nFigure 10: Window query vs. data distribution\nVarying the data distribution. Fig. 10 reports the window\nquery costs across di\u000berent data sets. As for point queries,\nRSMI is the fastest for window queries except for on Uniform\ndata where Grid is slightly faster (0.025 ms vs. 0.024 ms).\nThis is because Grid can easily locate the cells and hence the\ndata blocks that overlap with the query window. On non-\nuniform data, Grid may have cells that only partially overlap\nwith the query window while containing many blocks (and\nfalse positives) to be \fltered for the query. On such data\nsets, RSMI outperforms the traditional indices by at least a\nfactor of 1.33 (6.4 ms vs. 8.0 ms for RSMI vs. KDB on OSM)\nand up to 17 times (0.05 ms vs. 0.85 ms for RSMI vs. Grid\non Tiger). RSMI also outperforms ZM by 4.4 times on OSM\n(6.4 ms vs. 28.5 ms) and by over an order of magnitude on\nthe other four data sets. Meanwhile, RSMIa outperforms ZM\non Uniform, Normal, and OSM, and its query performance\nis comparable to those of the R-tree indices on the real data\nsets Tiger and OSM. These \fndings show the applicability\nof RSMIa when accurate query answers are needed.\nNext, we consider the recall of the learned indices. RSMIa\nhas 100% recall as it uses MBRs for query processing. ZM is\n 2350\n\nmore accurate than RSMI. It uses Z-curves, where the bot-\ntom left and top right corners of a query window bound the\nsearch region better than all four corners of the query win-\ndow in RSMI, which uses Hibert-curves. However, RSMI has\nmuch lower query costs. It also has a consistently high recall\nof over 91.4% and up to 95.4% on Normal data (Fig. 10b).\n10-210-1100101102103\n1248163264128Response time (ms)\nData set size (million)Grid\nHRR\nKDBRR*\nRSMI\nRSMIaZM\n(a)Query time\n 0 0.2 0.4 0.6 0.8 1\n1248163264128Recall\nData set size (million) (b)Recall\nFigure 11: Window query vs. data set size\nVarying the data set size. When the data set size varies\n(Fig. 11a), RSMI is the fastest except for on data sets with\nfewer than 4 million points, where KDB is slightly faster.\nKDB creates non-overlapping data partitions, which bene-\n\fts query performance. However, as the data set grows, the\npartitions become long and thin due to the block size limit\n(especially on Skewed). This leads to many tree nodes over-\nlapping with queries and high query costs. RSMIa is faster\nthan ZM when the data set size is below 8 million or exceeds\n128 million. It queries 128 million points in just 6.1 ms.\nFig. 11b shows the recall of RSMI, which is consistently\nhigh and is over 89.8% for 128 million points. The recall\ndrops slightly as the data set size increases, since it is more\ndi\u000ecult to train an accurate prediction model on more points.\n10-210-1100101102\n0.0006 0.0025 0.01 0.04 0.16Response time (ms)\nQuery window size (%)Grid\nHRR\nKDBRR*\nRSMI\nRSMIaZM\n(a)Query time\n 0 0.2 0.4 0.6 0.8 1\n0.0006 0.0025 0.01 0.04 0.16Recall\nQuery window size (%) (b)Recall\nFigure 12: Window query vs. query window size\nVarying the query window size. We vary the query window\nsize from 0.0006% to 0.16% of the data space size. As Fig. 12\nshows, the query times grow with the query window size\nsince more points are queried. The relative performance in\nboth the query time and the recall among the indices is\nsimilar to that observed above. RSMI o\u000bers highly accurate\nanswers (over 90.3%) and lowest query times, thus showing\nrobustness for window queries in varying settings.\nVarying the query window aspect ratio. We further vary\nthe query window aspect ratio from 0.25 to 4.0. Fig. 13 shows\nthat the aspect ratio is less impactful than the query window\nsize. We conjecture that this is because the query costs are\naveraged over 1,000 queries that are positioned following the\ndata distribution. Every set of 1,000 queries of a given aspect\nratio may cover a similar set of data points overall, and hence\nhas a similar average query cost. RSMI again outperforms\nall competitors and is at least 1.4 times faster (0.058 ms vs.\n0.083 ms for RSMI vs. KDB when the aspect ratio is 4) than\nthe other structures, and its recall exceeds 89.4%.\n10-210-1100101102\n0.25 0.5 1.0 2.0 4.0Response time (ms)\nQuery window aspect ratioGrid\nHRR\nKDBRR*\nRSMI\nRSMIaZM(a)Query time\n 0 0.2 0.4 0.6 0.8 1\n0.25 0.5 1.0 2.0 4.0Recall\nQuery window aspect ratio (b)Recall\nFigure 13: Window query vs. query window aspect ratio\n6.2.4 KNN Queries\nWe generate 1,000 kNN queries under each setting and\nreport the average query time and recall. Here, the recall\nrefers to the number of true kNN points returned divided\nbyk. This is the same as the precision. ZM does not come\nwith akNN algorithm, so we use our kNN algorithm for it.\nFor the other indices, we use the best-\frst algorithm [40].\n10-210-1100101102103104\nUni. Nor. Ske. Tig.OSMResponse time (ms)\nData distributionGrid\nHRR\nKDBRR*\nRSMI\nRSMIaZM\n(a)Query time\n 0 0.2 0.4 0.6 0.8 1\nUni. Nor. Ske. Tig.OSMRecall\nData distribution (b)Recall\nFigure 14: KNN query vs. data distribution\nVarying the data distribution. Fig. 14 shows that RSMI is\nalso the fastest for kNN queries. It outperforms ZM by up to\n46 times (0.03 ms vs. 1.38 ms on Tiger). This is because both\ntechniques use window queries for kNN queries, where RSMI\nis much faster. RSMI also outperforms the other indices. For\nGrid, thekNNs may spread in multiple cells which makes\nit uncompetitive. The other traditional indices require tree\ntraversals and accesses to possibly many tree nodes. In terms\nof recall, RSMI is again very close to ZM and is over 91.8%.\nThis shows the applicability of RSMI to kNN queries.\n10-1100\n1248163264128Response time (ms)\nData set size (million)Grid\nHRR\nKDBRR*\nRSMI\nRSMIaZM\n(a)Query time\n 0 0.2 0.4 0.6 0.8 1\n1248163264128Recall\nData set size (million) (b)Recall\nFigure 15: KNN query vs. data set size\nVarying the data set size. In Fig. 15, we vary the data\nset size. RSMI again yields high recall and the fastest query\ntime, while RSMIa produces accurate answers and is faster\nthan ZM (except when n= 64 million). The query times\ngrow with the data set size. When n= 128 million, RSMI\nis over an order of magnitude faster than ZM. The recall of\nRSMI also decreases slightly with the data set size but it\nstays above 88.3%. This is in line with the observations for\nwindow queries where the data set size is varied (cf. Fig. 11).\n 2351\n\n10-1100101\n1 5 25 125 625Response time (ms)\nkGrid\nHRR\nKDBRR*\nRSMI\nRSMIaZM(a)Query time\n 0 0.2 0.4 0.6 0.8 1\n1 5 25 125 625Recall\nk (b)Recall\nFigure 16: KNN query vs. k\nVaryingk.In Fig. 16, we vary the query parameter kfrom\n1 to 625. We see that the query costs increase with k, which\nis expected as more blocks and data points are examined.\nThe high e\u000eciency and recall (89.1% to 96.5%) of RSMI\nindicate that it scales to large kvalues.\n6.2.5 Update Handling\nWe further examine the impact of data updates. ZM does\nnot come with update algorithms, so we adapt ours for it.\nWe initialize the indices with the default data set, insert (or\ndelete) 10% nto 50%ndata points, and query the updated\nindices with 1,000 queries. We report the average response\ntime per insertion and the average response time per query.\nWe also studied the impact of deletions but omit those due\nto space constraints. We note however, that they replicate\nthe performance \fgures of insertions.\n10-1100101102\n10 20 30 40 50Response time ( s)\nInserted points (%)Grid\nHRR\nKDBRR*\nRSMI\nRSMIrZM\n(a)Insertion time\n100101102\n10 20 30 40 50Response time ( s)\nInserted points (%) (b)Point query time\nFigure 17: Insertion and point queries after insertions\nInsertion. Fig. 17a shows slowly increasing insertion times\nwith more insertions, as the tree index height has not in-\ncreased till 50% ninsertions, while the learned indices keep\nappending new points to the end of the predicted blocks.\nGrid adds a new point pto the last block in the cell enclos-\ningp, which is the fastest. RSMI insertions cost more than\nthose of ZM, since it has more sub-models than ZM, and it\ntakes more time to reach a data block for the insertion.\nFig. 17b shows that insertions cause the point query times\nto increase. There are more points to query, and the indices\nbecome less optimal. The learned indices ZM and RSMI are\nimpacted the most as they have more blocks to scan with in-\nsertions. RSMI still yields the best query performance after\n50%ninsertions, i.e., 2.7 \u0016s (RSMI) vs. 3.9 \u0016s (Grid).\nWe further compare with an RSMI variant named RSMIr ,\nwhich rebuilds the sub-models that exceeded the partition\nthreshold after every 10% ninsertions. RSMIr has an amor-\ntized insertion time of less than 130 \u0016s, which con\frms the\nfeasibility of periodic rebuilds. It also improves the point\nquery performance, especially when there are more inser-\ntions. A similar pattern is observed on window and kNN\nqueries. We omit its curve in those \fgures for succinctness.\n10-210-1100101102\n10 20 30 40 50Response time (ms)\nInserted points (%)Grid\nHRR\nKDBRR*\nRSMI\nRSMIaZM(a)Query time\n 0 0.2 0.4 0.6 0.8 1\n10 20 30 40 50Recall\nInserted points (%) (b)Recall\nFigure 18: Window queries after insertions\n10-1100101102103\n10 20 30 40 50Response time (ms)\nInserted points (%)Grid\nHRR\nKDBRR*\nRSMI\nRSMIaZM\n(a)Query time\n 0 0.2 0.4 0.6 0.8 1\n10 20 30 40 50Recall\nInserted points (%) (b)Recall\nFigure 19: KNN queries after insertions\nFor window queries (Fig. 18a), RR\u0003keeps adjusting MBRs\nto reduce overlaps, while HRR checks the newly created\nblocks with tree traversals. They now perform similarly to\nRSMI. For RSMI, the created blocks are linked after blocks\nwith the same predicted location. When the \frst block does\nnot contain a query window corner point, more blocks need\nto be checked, which increase the query time. For kNN\nqueries (Fig. 19a), as there are more points, the size of the\ninitial search region of RSMI decreases. This helps RSMI re-\ntain the fastest query time. The recall of RSMI for both win-\ndow andkNN queries consistently exceeds 87.5% (Fig. 18b\nand Fig. 19b). These results indicate the e\u000bectiveness of the\nupdate algorithm at maintaining the performance of RSMI.\n7. CONCLUSIONS\nExploiting recent advances in machine learning, we pro-\npose algorithms to learn indices for spatial data. We order\nthe data points using a rank space based technique. This\nordering simpli\fes the indexing functions to be learned that\nmap spatial coordinates (search keys) to disk block IDs (lo-\ncation). To scale to large data sets, we propose a recursive\nstrategy that partitions a large data set and learns indices\nfor each partition. We also provide algorithms based on the\nlearned indices to compute point, window, and kNN queries,\nas well as update algorithms. Experimental results with both\nreal and synthetic data show that the proposed learned in-\ndices and query algorithms are highly e\u000bective and e\u000ecient.\nQuery processing using our indices is more than an order of\nmagnitude faster than R-trees and a recent baseline learned\nindex, while our window and kNN query results are highly\naccurate, i.e., over 87% across a variety of settings.\nOur learned indices may be applied to spatial objects with\nnon-zero extent using query expansion [44, 48], although this\nimpacts query accuracy and e\u000eciency. We leave in-depth\nstudies of learned indices for spatial objects with non-zero\nextent for future work. Further, it is important to attempt\nto establish theoretical performance bounds for the learned\nspatial indices including the height, query accuracy, and\ncurve value gaps, and to improve the e\u000eciency for highly\ndynamic data updates, e.g., in moving object databases.\n 2352\n\n8. REFERENCES\n[1] L. Arge, M. D. Berg, H. Haverkort, and K. Yi. The\nPriority R-tree: A practically e\u000ecient and worst-case\noptimal R-tree. ACM Transactions on Algorithms ,\n4(1):9:1{9:30, 2008.\n[2] R. Bayer. The universal B-tree for multidimensional\nindexing: General concepts. In International\nConference on Worldwide Computing and Its\nApplications , pages 198{209, 1997.\n[3] N. Beckmann, H.-P. Kriegel, R. Schneider, and\nB. Seeger. The R\u0003-tree: An e\u000ecient and robust access\nmethod for points and rectangles. In SIGMOD , pages\n322{331, 1990.\n[4] N. Beckmann and B. Seeger. A revised R\u0003-tree in\ncomparison with related index structures. In\nSIGMOD , pages 799{812, 2009.\n[5] J. L. Bentley. Multidimensional binary search trees\nused for associative searching. Communications of the\nACM , 18(9):509{517, 1975.\n[6] S. Berchtold, C. B ohm, and H.-P. Kriegel. The\nPyramid-technique: Towards breaking the curse of\ndimensionality. In SIGMOD , pages 142{153, 1998.\n[7] A. Davitkova, E. Milchevski, and S. Michel. The\nML-Index: A multidimensional, learned index for\npoint, range, and nearest-neighbor queries. In EDBT ,\npages 407{410, 2020.\n[8] D. J. DeWitt, N. Kabra, J. Luo, J. M. Patel, and J.-B.\nYu. Client-server paradise. In VLDB , pages 558{569,\n1994.\n[9] J. Ding, U. F. Minhas, J. Yu, C. Wang, J. Do, Y. Li,\nH. Zhang, B. Chandramouli, J. Gehrke, D. Kossmann,\nD. Lomet, and T. Kraska. ALEX: An updatable\nadaptive learned index. In SIGMOD , pages 969{984,\n2020.\n[10] C. Faloutsos and S. Roseman. Fractals for secondary\nkey retrieval. In PODS , pages 247{252, 1989.\n[11] P. Ferragina and G. Vinciguerra. The PGM-index: A\nfully-dynamic compressed learned index with provable\nworst-case bounds. PVLDB , 13(8):1162{1175, 2020.\n[12] R. A. Finkel and J. L. Bentley. Quad trees: A data\nstructure for retrieval on composite keys. Acta\nInformatica , 4(1):1{9, 1974.\n[13] V. Gaede and O. G unther. Multidimensional access\nmethods. ACM Computing Surveys , 30(2):170{231,\n1998.\n[14] A. Galakatos, M. Markovitch, C. Binnig, R. Fonseca,\nand T. Kraska. FITing-Tree: A data-aware index\nstructure. In SIGMOD , pages 1189{1206, 2019.\n[15] Y. J. Garc\u0013 \u0010a R, M. A. L\u0013 opez, and S. T. Leutenegger.\nA greedy algorithm for bulk loading R-trees. In ACM\nGIS, pages 163{164, 1998.\n[16] A. Guttman. R-trees: A dynamic index structure for\nspatial searching. In SIGMOD , pages 47{57, 1984.\n[17] A. Hadian and T. Heinis. Considerations for handling\nupdates in learned index structures. In International\nWorkshop on Exploiting Arti\fcial Intelligence\nTechniques for Data Management , pages 3:1{3:4, 2019.\n[18] A. Hadian and T. Heinis. Interpolation-friendly\nB-trees: Bridging the gap between algorithmic and\nlearned indexes. In EDBT , pages 710{713, 2019.\n[19] A. Henrich, H.-W. Six, and P. Widmayer. The LSDtree: Spatial access to multidimensional point and\nnon-point objects. In VLDB , pages 45{54, 1989.\n[20] H. Jagadish, B. C. Ooi, K.-L. Tan, C. Yu, and\nR. Zhang. iDistance: An adaptive B+-tree based\nindexing method for nearest neighbor search. ACM\nTransactions on Database Systems , 30(2):364{397,\n2005.\n[21] C. S. Jensen, D. Lin, and B. C. Ooi. Query and\nupdate e\u000ecient B+-tree based indexing of moving\nobjects. In VLDB , pages 768{779, 2004.\n[22] D. V. Kalashnikov, S. Prabhakar, and S. Hambrusch.\nMain memory evaluation of monitoring queries over\nmoving objects. Distributed and Parallel Databases ,\n15(2):117{135, 2004.\n[23] I. Kamel and C. Faloutsos. Hilbert R-tree: An\nimproved R-tree using fractals. In VLDB , pages\n500{509, 1994.\n[24] A. Kipf, R. Marcus, A. van Renen, M. Stoian,\nA. Kemper, T. Kraska, and T. Neumann. RadixSpline:\nA single-pass learned index. In International\nWorkshop on Exploiting Arti\fcial Intelligence\nTechniques for Data Management , pages 5:1{5:5, 2020.\n[25] T. Kraska, M. Alizadeh, A. Beutel, E. H. Chi,\nA. Kristo, G. Leclerc, S. Madden, H. Mao, and\nV. Nathan. SageDB: A learned database system. In\nCIDR , 2019.\n[26] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and\nN. Polyzotis. The case for learned index structures. In\nSIGMOD , pages 489{504, 2018.\n[27] S. T. Leutenegger, J. M. Edgington, and M. A. L\u0013 opez.\nSTR: A simple and e\u000ecient algorithm for R-tree\npacking. In ICDE , pages 497{506, 1997.\n[28] P. Li, H. Lu, Q. Zheng, L. Yang, and G. Pan. LISA: A\nlearned index structure for spatial data. In SIGMOD ,\npages 2119{2133, 2020.\n[29] Z. Li, T. N. Chan, M. L. Yiu, and C. S. Jensen.\nPolyFit: Polynomial-based indexing approach for fast\napproximate range aggregate queries. CoRR ,\nabs/2003.08031, 2020.\n[30] V. Markl. Mistral: Processing relational queries using\na multidimensional access technique. In Ph.D. Thesis,\nTechnische Universit at M unchen . 1999.\n[31] V. Nathan, J. Ding, M. Alizadeh, and T. Kraska.\nLearning multi-dimensional indexes. In NeurIPS 2019\nWorkshop on Machine Learning for Systems , 2019.\n[32] V. Nathan, J. Ding, M. Alizadeh, and T. Kraska.\nLearning multi-dimensional indexes. In SIGMOD ,\npages 985{1000, 2020.\n[33] J. Nievergelt, H. Hinterberger, and K. C. Sevcik. The\ngrid \fle: An adaptable, symmetric multikey \fle\nstructure. ACM Transactions on Database Systems ,\n9(1):38{71, 1984.\n[34] OpenStreetMap US Northeast data dump.\nhttps://download.geofabrik.de/ , 2018. Accessed:\n2020-06-10.\n[35] J. A. Orenstein and T. H. Merrett. A class of data\nstructures for associative searching. In PODS , pages\n181{190, 1984.\n[36] PyTorch. https://pytorch.org , 2016. Accessed:\n2020-06-10.\n[37] J. Qi, Y. Tao, Y. Chang, and R. Zhang. Theoretically\noptimal and empirically e\u000ecient R-trees with strong\n 2353\n\nparallelizability. PVLDB , 11(5):621{634, 2018.\n[38] J. Qi, Y. Tao, Y. Chang, and R. Zhang. Packing\nR-trees with space-\flling curves: Theoretical\noptimality, empirical e\u000eciency, and bulk-loading\nparallelizability. ACM Transactions on Database\nSystems , accepted to appear in 2020.\n[39] J. T. Robinson. The K-D-B-tree: A search structure\nfor large multidimensional dynamic indexes. In\nSIGMOD , pages 10{18, 1981.\n[40] N. Roussopoulos, S. Kelley, and F. Vincent. Nearest\nneighbor queries. In SIGMOD , pages 71{79, 1995.\n[41] N. Roussopoulos and D. Leifker. Direct spatial search\non pictorial databases using packed R-trees. In\nSIGMOD , pages 17{31, 1985.\n[42] T. K. Sellis, N. Roussopoulos, and C. Faloutsos. The\nR+-tree: A dynamic index for multi-dimensional\nobjects. In VLDB , pages 507{518, 1987.\n[43] N. F. Setiawan, B. I. P. Rubinstein, and\nR. Borovica-Gajic. Function interpolation for learnedindex structures. In Australasian Database\nConference , pages 68{80, 2020.\n[44] E. Stefanakis, T. Theodoridis, T. K. Sellis, and Y.-C.\nLee. Point representation of spatial objects and query\nwindow extension: A new technique for spatial access\nmethods. International Journal of Geographical\nInformation Science , 11(6):529{554, 1997.\n[45] TIGER/Line Shape\fles. https://www.census.gov/\ngeo/maps-data/data/tiger-line.html , 2006.\nAccessed: 2020-06-10.\n[46] H. Wang, X. Fu, J. Xu, and H. Lu. Learned index for\nspatial queries. In MDM , pages 569{574, 2019.\n[47] Z. Yang, B. Chandramouli, C. Wang, J. Gehrke, Y. Li,\nU. F. Minhas, P.- \u0017A. Larson, D. Kossmann, and\nR. Acharya. Qd-tree: Learning data layouts for big\ndata analytics. In SIGMOD , pages 193{208, 2020.\n[48] R. Zhang, J. Qi, M. Stradling, and J. Huang. Towards\na painless index for spatial objects. ACM Transactions\non Database Systems , 39(3):19:1{19:42, 2014.\n 2354",
  "textLength": 75040
}