{
  "paperId": "8f5fb3de5a95781e7db6771961d26ea37e24274d",
  "title": "FITing-Tree: A Data-aware Index Structure",
  "pdfPath": "8f5fb3de5a95781e7db6771961d26ea37e24274d.pdf",
  "text": "FITing-Tree: A Data-aware Index Structure\nAlex Galakatos1∗Michael Markovitch1∗Carsten Binnig2\nRodrigo Fonseca1Tim Kraska3\n1Brown University {first_last}@brown.edu2TU Darmstadt {first.last@cs.tu-darmstadt.de}3MIT CSAIL {last@mit.edu}\nABSTRACT\nIndex structures are one of the most important tools that\nDBAs leverage to improve the performance of analytics\nand transactional workloads. However, building several in-\ndexes over large datasets can often become prohibitive and\nconsume valuable system resources. In fact, a recent study\nshowed that indexes created as part of the TPC-C benchmark\ncan account for 55% of the total memory available in a mod-\nern DBMS. This overhead consumes valuable and expensive\nmain memory, and limits the amount of space available to\nstore new data or process existing data.\nIn this paper, we present FITing-Tree , a novel form of a\nlearned index which uses piece-wise linear functions with\na bounded error specified at construction time. This error\nknob provides a tunable parameter that allows a DBA to FIT\nan index to a dataset and workload by being able to balance\nlookup performance and space consumption. To navigate\nthis tradeoff, we provide a cost model that helps determine\nan appropriate error parameter given either (1) a lookup\nlatency requirement (e.g., 500ns) or (2) a storage budget (e.g.,\n100MB). Using a variety of real-world datasets, we show that\nour index is able to provide performance that is comparable\nto full index structures while reducing the storage footprint\nby orders of magnitude.\nACM Reference Format:\nAlex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fon-\nseca, Tim Kraska. 2019. FITing-Tree: A Data-aware Index Structure.\nIn2019 International Conference on Management of Data (SIGMOD\n’19), June 30-July 5, 2019, Amsterdam, Netherlands. ACM, New York,\nNY, USA, 18 pages. https://doi.org/10.1145/3299869.3319860\n*Authors contributed equally.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear\nthis notice and the full citation on the first page. Copyrights for components\nof this work owned by others than ACM must be honored. Abstracting with\ncredit is permitted. To copy otherwise, or republish, to post on servers or to\nredistribute to lists, requires prior specific permission and/or a fee. Request\npermissions from permissions@acm.org.\nSIGMOD ’19, June 30-July 5, 2019, Amsterdam, Netherlands\n©2019 Association for Computing Machinery.\nACM ISBN 978-1-4503-5643-5/19/06. . . $15.00\nhttps://doi.org/10.1145/3299869.33198601 INTRODUCTION\nTree-based index structures (e.g., B+ trees) are one of the\nmost important tools that DBAs leverage to improve the per-\nformance of analytics and transactional workloads. However,\nfor main-memory databases, tree-based indexes can often\nconsume a significant amount of memory. In fact, a recent\nstudy [ 48] shows that the indexes created for typical OLTP\nworkloads can consume up to 55% of the total memory avail-\nable in a state-of-the-art in-memory DBMS. This overhead\nnot only limits the amount of space available to store new\ndata but also reduces space for intermediates that can be\nhelpful when processing existing data.\nTo reduce the storage overhead of B+ trees, various com-\npression schemes have been developed [ 7,9,23,49]. The\nmain idea behind these techniques is to remove the redun-\ndancy that exists among keys and/or to reduce the size of\neach key inside a node of the index. For example, prefix and\nsuffix truncation can be used to store common parts of keys\nonly once per index node, reducing the total size of the tree.\nAdditionally, more expensive compression techniques like\nHuffmann coding can be applied within each node but come\nat a higher runtime cost since pages must be decompressed\nto search for an item.\nAlthough each of the previously mentioned compression\nschemes reduce the size of an index node, the memory foot-\nprint of these indexes still grows linearly with the number\nof distinct keys to be indexed, resulting in indexes that can\nconsume a significant amount of memory. This observation\nis especially true for data such as timestamps or sensor read-\nings that are generated in a wide variety of applications (e.g.,\nautonomous vehicles, IoT devices). Even worse, the number\nof unique keys to be indexed for such data types typically\ngrow over time, resulting in indexes that are constantly grow-\ning. Consequently, a DBA has no way to restrict memory\nconsumption other than dropping an index completely.\nTo tackle this issue, we present FITing-Tree , a novel index\nstructure that compactly captures trends in data using piece-\nwise linear functions. Unlike typical indexes which use fixed-\nsize pages on the leaf level that point to the data, FITing-Tree\nuses piece-wise linear functions to quickly approximate the\nposition of an element. By leveraging the trends within the\ndata, FITing-Tree can reduce the memory consumption ofarXiv:1801.10207v2  [cs.DB]  25 Mar 2020\n\nan index by orders of magnitude compared to a traditional\nB+ tree. At the core of our index structure is a parameter\nthat specifies the amount of acceptable error (i.e., a constant\nthat is the maximum distance between the predicted and\nactual position of any key). Unlike existing index structures,\nour error parameter allows a DBA to FIT an index to a given\nscenario and balance the lookup performance and space\nconsumption of an index. To navigate this tradeoff, we also\npresent a cost model that helps a DBA choose an appropriate\nerror term given either (1) a lookup latency requirement (e.g.,\n500ns) or (2) a storage budget (e.g., 100MB).\nIn the basic version of FITing-Tree , we assume that the\ntable data to be indexed is sorted by the index key (i.e., clus-\ntered index) but we also show how our techniques extend\nto secondary (i.e., non-clustered) indexes. Using a variety of\nreal-world datasets, we show that our index structure pro-\nvides performance that is comparable to full and fixed-page\nindexes (even for a worst-case dataset) while reducing the\nstorage footprint by orders of magnitude.\nUsing linear functions to approximate the distribution\nmakes FITing-Tree a form of a learned index [ 30]. However,\nin contrast to the initially proposed techniques, our approach\nallow us to (1) bound for the worst-case lookup performance,\n(2) efficiently support insert operations, and (3) enable paging\n(i.e., the entire data does not have to reside in a contiguous\nmemory region). Furthermore, although the problem of ap-\nproximating distributions using piece-wise functions is also\nnot new [ 10,12,13,16,18,27,33,35,41,46], none of these\ntechniques have been applied to indexing and therefore do\nnot consider operations that indexes must support.\nAnother interesting observation is that our compression\nscheme in FITing-Tree is orthogonal to node-level compres-\nsion techniques such as the previously mentioned prefix/suf-\nfix truncation. In other words, since FITing-Tree internally\nuses a tree structure for inner nodes, we can still apply these\ntechniques to further reduce an index’s size.\nIn summary, we make the following contributions:\n•We propose FITing-Tree , a novel index structure that\nleverages properties about the underlying data distri-\nbution to reduce the size of an index.\n•We present and analyze an efficient segmentation al-\ngorithm that incorporates a tunable error parameter\nthat allows DBAs to balance the lookup performance\nand space footprint of our index.\n•We propose a cost model that helps a DBA determine\nan appropriate error threshold given either a latency\nor storage requirement.\n•Using several real-world datasets, we show that our\nindex provides similar (or in some cases even better)\nperformance compared to existing index structures\nwhile consuming orders of magnitude less space.\n0 5000 10000 15000 20000\nTimestamp0123456789Position1e8\n{ErrorWeekendDay\nNightActual\nApproxFigure 1: Key to position mapping for IoT data.\nThe remainder of the paper is organized as follows. In\nSection 2, we first present an overview of our new index\nstructure called FITing-Tree . Afterwards, we discuss the\nmain index operations: bulk loading (Section 3), lookups\n(Section 4) and inserts (Section 5). Section 6 then introduces\nour cost model that allows a DBA to balance the lookup\nperformance and the space consumption of a FITing-Tree .\nFinally, in Section 7 we discuss the results of our evaluation\non real and synthetic datasets, summarize related work in\nSection 8, and finally conclude in Section 9.\n2 OVERVIEW\nAt a high level, indexes (and B+ trees over sorted attributes\nin particular) can be represented by a function that maps\na key (e.g., a timestamp) to a storage location. Using this\nrepresentation, FITing-Tree partitions the key space into\na series of disjoint linear segments that approximate the\ntrue function, since it is (generally) not possible to fully\nmodel the underlying data distribution. At the core of this\nprocess is a tunable error threshold which represents the\nmaximum distance that the predicted location of any key\ninside a segment is from its actual location. Instead of storing\nall values in the key space, FITing-Tree stores only (1) the\nstarting key of each linear segment and (2) the slope of the\nlinear function in order to compute a key’s approximate\nposition using linear interpolation.\nIn the following, we first discuss how we can use functions\nto map key values to storage locations. Then, we discuss how\nwe leverage this function representation to efficiently imple-\nment our index structure on top of a B+ tree for clustered\nindexes. Finally, we show how our ideas can also be applied\nto compress secondary indexes.\n2.1 Function Representation\nOne key insight to our approach is that we can abstractly\nmodel an index as a monotonically increasing function that\nmaps keys (i.e., values of the indexed attribute) to storage\nlocations (i.e., its page and the offset within that page). To\nexplain this intuition, assume that all keys to be indexed\n\nare stored in a sorted array, allowing us to use an element’s\nposition in the array as its storage location.\nAs an example, consider the IoT dataset [ 1], which con-\ntains events from various devices (e.g., door sensors, motion\nsensors, power monitors) installed throughout a university\nbuilding. In this dataset, the data is sorted by the timestamp\nof an event, allowing us to construct a function that maps\neach timestamp (i.e., key) to its position in the dataset (i.e.,\nposition in a sorted array), as shown in Figure 1. Unsurpris-\ningly, since the installed IoT devices monitor human activity,\nthe timestamps of the recorded actions follow a pattern (e.g.,\nthere is little activity during the weekend and at night).\nSince a function that represents an index can be arbitrar-\nily complex and data-dependent, the precise function that\nmaps keys to positions may not be possible to learn and\nis expensive to build and update. Therefore, our goal is to\napproximate the function that represents the mapping of a\nkey to a position.\nTo compactly capture trends that exist in the data while\nbeing able to efficiently build a new index and handle updates,\nwe use a series of piece-wise linear functions to approximate\nan arbitrary function. As shown in Figure 1, for example,\nour segmentation algorithm (described further in Section 3)\npartitions the timestamp values into several linear segments\nthat are able to accurately reflect the various trends that exist\nin the data (e.g., less activity during the weekend). Since the\napproximation captures trends in the data, it is agnostic to\nkey density (a trend with sparse keys can be captured as well\nas a trend with dense keys).\nAlthough more complex functions (e.g., higher order poly-\nnomials) can be used to approximate the true function, piece-\nwise linear approximation is significantly less expensive to\ncompute. This dramatically reduces (1) the initial index con-\nstruction cost, and (2) improves insert latency for new items\n(see Section 5).\nThe resulting piece-wise linear approximation, however, is\nnot precise (i.e., a key’s predicted location is not necessarily\nits true position). We therefore define the error associated\nwith our approximation as the maximum distance between\nthe actual and predicted location of any key, as shown below,\nwhere pred _pos(k)andtrue_pos(k)return the predicated\nand actual position of an element krespectively.\nerror =max(|pred _pos(k)−true_pos(k)|)∀k∈keys(1)\nThis formulation allows us to define the core building\nblock of FITing-Tree , asegment . A segment is a contiguous\nregion of a sorted array for which any key is no more than\na specified error threshold from its interpolated position.\nDepending on the data distribution and the error threshold,\nthe segmentation process will yield a different number of\nsegments that approximate the underlying data. Therefore,\nSegment 1Segment 2Segment 3Leaf Nodes…separators………key + slopekey + slopekey + slopeseparatorsseparatorskeys + slopesInner Nodes\nTable Pages(Sorted)Figure 2: A clustered FITing-Tree index.\nimportantly, the error threshold enables us to balance mem-\nory consumption and performance. After the segmentation\nprocess, FITing-Tree stores the boundaries and slope of\neach segment (instead of each individual key) in a B+ tree,\nreducing the overall memory footprint of the index.\n2.2 FITing-Tree Design\nAs previously mentioned, our segmentation process parti-\ntions the key space of an attribute into disjoint linear seg-\nments such that the predicted position of any key inside a\nsegment is no more than a bounded distance away from the\nkey’s true position. FITing-Tree organizes these segments\nin a tree to efficiently support insert and lookup operations.\nIn the following, we first discuss clustered indexes, where\nrecords are already sorted by the key that is being indexed.\nAfterwards, we show how our technique can be extended to\nprovide similar benefits for secondary indexes.\n2.2.1 Clustered Indexes. In a traditional clustered B+ tree,\nthe table data is stored in fixed-size pages and the leaf level\nof the index contains only the first key of each of these table\npages. Unlike a clustered B+ tree, in a FITing-Tree , the table\ndata is partitioned into variable-sized segments (pages) that\nsatisfy the given error threshold. Each segment is essentially\na fixed-size array, but successive segments can be allocated\nindependently (i.e., non contiguously).\nFigure 2 shows the structure of a clustered FITing-Tree\nindex. As shown, the underlying data is partitioned into\na series of variable-sized segments that approximate the\ndistribution of keys to be indexed. Depending on the error\nparameter and the data distribution, several consecutive keys\ncan thus be summarized into a single segment. Details of\nthe segmentation algorithm that divides the table data into\nvariable-sized segments are discussed in Section 3.\nUnlike a traditional B+ tree, each leaf node in a FITing-\nTree stores the segment’s slope, starting key, and a pointer to\na segment. This allows us to use interpolation search in each\nsegment since the data within this segment is approximated\nby a linear function given by the slope.\n\nSegment 1Segment 2Segment 3Leaf Nodes…separators………key + slopekey + slopekey + slopeseparatorsseparatorskeys + slopesInner Nodes\nKey Pages(Sorted)Table Pages(Unsorted)Figure 3: A non-clustered FITing-Tree index.\nThe inner nodes of a FITing-Tree are the same as a B+\ntree (i.e., lookup and insert operations are identical to a nor-\nmal B+ tree). However, once a lookup or an insert reaches\nthe leaf level, FITing-Tree needs to perform additional work.\nFor lookups, we need to use the slope and the distance to\nthe starting key to calculate the key’s approximate position\n(offset in the segment). Since the resulting position is approx-\nimate, FITing-Tree must then perform a local search (e.g.,\nbinary, linear) to find the item, discussed further in Section 4.\nInsert operations also require additional work upon reach-\ning a leaf level page, since we must ensure that the error\nthreshold is always satisfied. Therefore, we present two dif-\nferent insertion strategies (described in detail in Section 5).\nThe first strategy performs in-place updates to the segment\n(as a baseline) while the second strategy uses a more ad-\nvanced buffer-based strategy to hold inserted data items.\nFinally, instead of internally using a B+ tree to locate the\nsegment for a key, FITing-Tree could instead use any other\nindex structure. For example, if the workload is read-only,\nother high performance index structures (e.g., FAST [ 28]) can\nbe used. In Section 7.4 we show how FITing-Tree performs\nwhen using different internal data structures, including FAST.\n2.2.2 Non-clustered Indexes. Secondary indexes can dramat-\nically improve the performance of queries involving selec-\ntions over a non-primary key attribute. Without secondary\nindexes, these queries must examine all tuples, which is of-\nten prohibitive. Unlike a clustered index, a non-primary key\nattribute is not sorted and may contain duplicates.\nThe primary difference between a clustered and an non-\nclustered FITing-Tree is that a non-clustered FITing-Tree\nrequires an additional “indirection layer” (called “Key Pages”\nin Figure 3). This layer is essentially an array of pointers\nthat is the same size as the data but is sorted by the key\nthat is being indexed. For example, the first position in this\nindirection layer will contain a pointer to the smallest valuekeyloc\n(x1,y1)(x2,y2)(x3,y3)\n>error\nFigure 4: A segment from (x1,y1)to(x3,y3)is not valid\nif(x2,y2)is further than error from the interpolated\nline.\nof the key being indexed. Note that a secondary B+ tree that\nuses fixed-size paging also requires this indirection layer.\nThe first step in creating a non-clustered FITing-Tree is to\nbuild the indirection layer by sorting the data by the indexed\nkey (e.g., temperature, age) and materializing the array of\npointers to each data item in the sorted order. Next, like in\nthe clustered index case, the segmentation algorithm scans\nthe indirection layer and produces a valid set of segments\nthat are then inserted into the upper level tree.\nAll operations on a non-clustered FITing-Tree internally\noperate on the indirection layer. For example, for a lookup,\nthe returned position of the data item is its position in the\nindirection layer. Then, to access the value, FITing-Tree\nfollows the pointer in the indirection layer at the predicted\nposition.\nAlthough the sorted level of key pages in a non-clustered\nFITing-Tree introduces additional overhead compared to\na clustered FITing-Tree index, this overhead occurs in any\nnon-clustered (secondary) index. However, as we show in our\nexperiments, a non-clustered FITing-Tree is significantly\nsmaller than a non-clustered B+ tree with fixed-size pages\nsince it has fewer leaf and internal nodes.\n3 SEGMENTATION\nIn the following, we describe how FITing-Tree partitions the\nkey space of an attribute into variable-sized segments that\nsatisfy the specified error. After this process, each segment is\ninserted into a B+ tree to enable efficient insert and lookup\noperations, described further in Section 4 and Section 5.\n3.1 Design Choices\nA common objective when fitting a function is to minimize\nthe least square error (minimizing the second error norm E2).\nUnfortunately, such an objective does not provide a guaran-\ntee for the maximal error and therefore does not provide a\nbound on the number of locations which must be scanned\nafter interpolating a key’s position. Therefore, our objective\nis to satisfy a maximal error ( E∞), demonstrated in Figure 4.\n\nAlgorithm 1 ShrinkingCone Segmentation\n1slhiдh←∞\n2sllow←0\n3 the first key is the segment origin\n4forevery k∈keys (in increasing order)\n5 do if kis inside the cone:\n6 then update slhiдh\n7 update sllow\n8 else keykis the origin of a new segment\n9 slhiдh←∞\n10 sllow←0\nWhile several optimal (in the number of produced seg-\nments) piece-wise linear approximation algorithms exist,\nthese techniques are prohibitively expensive (e.g., a dynamic\nprogramming algorithm [ 31] has a runtime of O(n3)using\nO(n)memory). Second, most existing online piece-wise lin-\near approximation algorithms [ 17,27] have a high storage\ncomplexity and/or do not guarantee a maximal error. Lastly,\neven linear time algorithms may not be efficient enough\nsince multiplicative constants have a significant effect.\nTherefore, to be able to efficiently (1) construct the index,\nand (2) support inserts, we need a highly efficient one-pass\nlinear algorithm. This focus on efficiency led us to choose lin-\near piece-wise functions, since higher order approximations\noften incur additional costs as previously discussed.\nIn the following, we describe a proposed segmentation al-\ngorithm, similar to FSW [ 33,46], which is linear in runtime,\nhas low constant memory usage, and guarantees a maxi-\nmal error in each segment. Importantly, though, we address\n(1) how to extend these techniques to indexing, including\nlooking up and inserting data items, (2) prove that, in the\nworst case, segments are bounded in size, and (3) analyze\nthe algorithm and compare it to an optimal algorithm.\n3.2 Segment Definition\nAs previously described, a segment is a region of the key\nspace that can be represented by a linear function whereby\nall keys are within a bounded distance from their linearly\ninterpolated position. More specifically, a segment is repre-\nsented by the first point (first key) and by the last point (the\nlast key) in the segment. Using this definition, we can fit a\nlinear function to the locations of keys in the segment (using\nthe start key, end key, and the number of positions).\nRecall that every segment must satisfy the maximal error\n(i.e., a key’s predicated position is at most error number\nof elements away from its true position). This leads to an\nimportant property (proof in Appendix A.1) of a maximal\nsegment (a segment is maximal when the addition of a key\nwill violate the specified error):\nTheorem 3.1. The minimal number of locations covered by\na maximal linear segment is error +1.keykeyloc\n14\n23\nFigure 5: ShrinkingCone - Point 1 is the origin of the\ncone. Point 2 is then added, resulting in the dashed\ncone. Point 3 is added next, yielding in the dotted cone.\nPoint 4 is outside the dotted cone and therefore starts\na new segment.\nThis allows us to quantify how bad a \"worst case\" (i.e.,\ndataset and error threshold that produce maximal number of\nsegments) can be. Since the minimum number of locations\ncovered by a maximal segment is bound by the error, the\ntotal size of a FITing-Tree is also bounded. Therefore, in\nthe worst case (every maximal segment covers error +1\nlocations), a FITing-Tree will not be larger than an index\nthat uses fixed-size pages of size error (e.g., B+ tree).\n3.3 Segmentation Algorithm\nAs previously mentioned, we needed a fast and efficient algo-\nrithm rather than an optimal one. We therefore chose to use\na greedy streaming algorithm ShrinkingCone (Algorithm 1)\nwhich, given a starting point (key) of a segment, attempts to\nmaximize the length of a segment while satisfying a given\nerror threshold. ShrinkingCone is similar to FSW [ 33] but\nconsiders only monotonically increasing functions and can\nproduce disjoint segments. The main idea behind Shrink-\ningCone is that a new key can be added to a segment if and\nonly if it does not violate the error constraint of any previous\nkey in the segment.\nMore specifically, we define a cone by the triple: origin\npoint (the key and its location), high slope ( slhiдh), and low\nslope ( sllow). The combination of the starting point and the\nlow slope gives the lower bound of the cone, and the com-\nbination of the starting point and the high slope gives the\nupper bound of the cone. Intuitively, the cone represents the\nfamily of feasible linear functions for a segment starting at\nthe origin of the cone (the high and low slopes represent\nthe range of valid slopes). When a new key is added to a\nsegment, the high and low slopes are calculated using the\nkey and the key’s position plus error and minus error (re-\nspectively). In the update step (lines 6-7 of Algorithm 1), the\nlowest high slope and the highest low slope values are then\nselected (between the newly calculated and previous slopes).\nTherefore, the cone either narrows (the high slope decreases\nand/or the low slope increases), or stays the same. If a new\nkey to be added to the segment is outside of the cone, there\n\nDataset error ShrinkingCone Optimal Ratio\nTaxi drop lat 10 5358 4996 1.07\nTaxi drop lat 100 351 271 1.29\nTaxi drop lat 1000 51 48 1.06\nTaxi drop lon 10 1198 1138 1.05\nTaxi drop lon 100 371 325 1.14\nTaxi drop lon 1000 40 37 1.08\nTaxi pick time 10 6238 4359 1.43\nTaxi pick time 100 165 137 1.2\nOSM lon 10 7727 6027 1.28\nOSM lon 100 101 63 1.6\nWeblogs 10 16961 14179 1.2\nWeblogs 100 909 642 1.42\nIoT 10 8605 6945 1.24\nIoT 100 723 572 1.26\nTable 1: ShrinkingCone compared to optimal.\nmust exist at least one previous key in the segment for which\nthe error constraint will be violated. Therefore, a new key\nthat is not inside the cone cannot be included in the segment,\nand becomes the origin point of the new segment.\nFigure 5 illustrates how the cone is updated: point 1 is the\norigin of the cone. Point 2 updates both the high and low\nslopes. Point 3 is inside the cone, however it only updates\nthe upper bound of the cone (point 3 is less than error above\nthe lower bound). Point 4 is outside of the updated cone, and\ntherefore will be the first point of a new segment.\n3.4 Algorithm Analysis\nWhile the ShrinkingCone algorithm has a runtime of O(n)\nand only uses a small constant amount memory (to keep\ntrack of the cone), it is not optimal. Moreover, for a given\nmaximal error and an adversarial dataset the number of\nsegments that it produces can be arbitrarily worse than an\noptimal algorithm, as we prove in Appendix A.2.\nAlthough ShrinkingCone can be arbitrarily worse com-\npared to optimal segmentation for a given maximal error,\nthere is a limit for how bad it can be in practice since we\ndo have a guarantee that a maximal segment covers at least\nerror +1locations.\nThe maximum number of segments ShrinkingCone pro-\nduces is at most min\u0010\n|keys|\n2,|D|\nerror +1\u0011\n, where|D|is the size\nof the dataset. This guarantee stems from Theorem 3.1: no\ninput with less than 3 keys spanning at least error +2posi-\ntions will cause ShrinkingCone to create a new segment.\nThus, compared to traditional B+ trees, in the worst case,\nFITing-Tree will produce no more segments (pages) than a\nB+ tree that uses fixed-size pages (of size error ).\nTo evaluate ShrinkingCone , we implemented the optimal\nalgorithm (runtime of O(n2)and memory consumption of\nO(n2)) using 106elements from real-world datasets: NYC Taxi\nDataset [ 2], OpenStreetMap [ 3], Weblogs [ 1], and IoT [ 1]. Ta-\nble 1 shows the number of segments generated by the optimal\nalgorithm and by ShrinkingCone . As shown, the number\nof segments that our algorithm produces is comparable to\nthe number of segments in the optimal case.4 INDEX LOOKUPS\nOne of the most important operations of an index is to lookup\na single key or a range of keys. However, since each entry in\nthe leaf level of FITing-Tree points to a segment, perform-\ning a lookup requires first locating the segment that a key\nbelongs to and then performing a local search inside the seg-\nment. In the following, we first describe how FITing-Tree\nperforms lookup operations for a single key and then show\nhow we can extend this technique to range predicates.\n4.1 Point Queries\nThe process of searching a FITing-Tree for a single element\ninvolves two steps: (1) searching the tree to find the segment\nthat the element belongs to, and (2) finding the element\nwithin a segment. These steps are outlined in Algorithm 2.\n4.1.1 Tree Search. Since, as previously described, each seg-\nment is stored in a B+ tree (with its first key as the key and\nthe segment’s slope and a pointer to the table page as its\nvalue), we must first search the tree to find the segment that\na given key belongs to. To do this, we begin traversing the B+\ntree from the root to the leaf, using standard tree traversal\nalgorithms. These steps, outlined in the SearchTree func-\ntion of Algorithm 2, terminate when reaching a leaf node\nwhich points to the segment that contains the key.\nSince the B+ tree is used to index segments rather than\nindividual points, the runtime for searching for the segment\nthat a key belongs in is O(loдb(p)), where bis a constant\nrepresenting the fanout of the tree (i.e., number of separators\ninside a node) and pis the number of segments created during\nthe segmentation process.\n4.1.2 Segment Search. Once FITing-Tree finds the segment\nfor a key, it then must find the key’s position inside the\nsegment. Recall that segments are created such that an ele-\nment is no more than a constant distance ( error ) from the\nelement’s position determined through linear interpolation.\nOther techniques for interpolation search inside a fixed-size\nindex page are discussed in [22].\nTo compute the approximate location of a key kwithin\na given segment s, we subtract the key from the first key\nthat appears in the segment s.start . Then, we multiply the\ndifference by the segment’s slope s.slope , as shown below in\nthe following equation.\npred _pos=(k−s.start)×s.slope (2)\nAfter interpolating an element’s position, the true position\nof an element is guaranteed to be within the error threshold.\nTherefore, FITing-Tree locally searches the following region\nusing binary search (as shown in Algorithm 2).\ntrue_pos∈[pred _pos−error ,pred _pos+error](3)\n\nAlgorithm 2 Lookup Algorithm\nLookup(tree,key)\n1seд←SearchTree(tree.root ,key)\n2val←SearchSegment(seg,key)\n3return val\nSearchTree(node ,key)\n1i←0\n2while key<node .keys[i]\n3 doi←i+1\n4ifnode .value[i].isLeaf()\n5 then j←0\n6 while key<node .values[j]\n7 doj←j+1\n8 return node .values[j]\n9return SearchTree(node .values[i],key)\nSearchSegment(seд,key)\n1pos←(key−seg.start)×seg.slope\n2return BinarySearch(seg.data ,pos−error ,pos+error ,key)\nHowever, it is also important to note that any search algo-\nrithm, including linear search, binary search, or exponential\nsearch can also be used depending on the specific scenario\n(e.g., hardware properties, error threshold).\nSince segments satisfy the specified error condition, the\ncost of searching for an element inside a segment is bounded.\nMore specifically, the runtime for locating an element inside\na segment is O(loд2(error))where error is constant.\n4.2 Range Queries\nRange queries, unlike point queries, have the additional re-\nquirement that they examine every item in the specified\nrange. Therefore, for range queries, the selectivity of the\nquery (i.e., number of tuples that satisfy the predicate) has a\nlarge influence on the total runtime of the query.\nHowever, like point queries, range queries must also find a\nsingle tuple: either the start or the end of the range. Therefore,\nFITing-Tree uses the previously described point lookup\ntechniques to find the beginning of the specified range. Then,\nsince segments either store keys contiguously (clustered\nindex) or have an indirection layer with pointers that is\nsorted by the key (non-clustered index), FITing-Tree can\nsimply scan from the starting location until it finds a key\nthat is outside of the specified range. For a clustered index,\nscanning the relevant range performs only very efficient\nsequential access, while for a non-clustered index, range\nqueries require random memory accesses (which is true for\nany non-clustered index).\n5 INDEX INSERTS\nAlong with locating an element, an index needs to be able\nto handle insert operations. In some applications, maintain-\ning a strict ordering guarantee is necessary, in which case\nFITing-Tree should ensure that new items are inserted in-\nplace. However, in situations where this is not the case, we’vedeveloped a more efficient insert strategy that improves in-\nsert throughput. In the following, we discuss each of these\nstrategies for inserting new items into a FITing-Tree . Then,\nin Section 7.1.3, we show how these strategies compare for\nvarious workloads and parameters.\n5.1 In-place Insert Strategy\nIn a typical B+ tree that uses paging, pages are left partially\nfilled and new values are inserted into an empty slot using\nan in-place strategy. When a given page is full, the node is\nsplit into two nodes, and the changes are propagated up the\ntree (i.e., the inner nodes in the tree are updated).\nAlthough similar, insert operations in FITing-Tree require\nadditional consideration since any key in the segment must\nbe no more than specified error amount ( error ) away from its\ninterpolated position. Importantly, in-place inserts require\nmoving keys in the page to preserve the order of the keys.\nWithout any a priori knowledge about the error of a given\nkey, any attempt to move the key requires checking to see\nif the error condition is satisfied. To make matters worse, a\nsingle insert may require moving many keys (in the worst\ncase, all keys in the page) to maintain the sorted order. Thus,\nwe must have a priori knowledge about any given key to de-\ntermine if it can be moved in any direction while preserving\nthe error guarantee.\nSimilar to the fill factor of a page, we divide the specified\nerror in 2 parts: the segmentation error e(error used to seg-\nment the data), and an insert budget ε(number of locations a\nkey can be moved in any direction). To preserve the specified\nerror, we require that error =e+ε. By keeping an insert\nbudget for each page, FITing-Tree can ensure that inserting\na new element will not violate the error for the page.\nMore specifically, given a segment s, the page has a total\nsize of|s|+2ε(|s|is the number of locations in the segment).\nData is placed in the middle of the new page, yielding εempty\nlocations at the beginning and end of the page.\nWith this strategy, it is possible to move any key in a\ndirection which has free space without violating the error\ncondition. Therefore, to insert a new item using an in-place\ninsert strategy, FITing-Tree first locates the position in the\npage where the new item belongs. Then, depending on which\nend of the page (left or right) is closer, all elements are shifted\n(either left or right) into the empty region of the page. Once\nall of the empty spaces are filled, the segment needs to be re-\napproximated (using the segmentation algorithm described\nin Section 3). If the segmentation algorithm produces more\nthan one segment, we create nnew segments (where nis the\nnumber of segments produced after running Algorithm 1 on\nthe single segment that is now full). Finally, each new seg-\nment is inserted into the upper level tree, and any references\nto the old segment are deleted.\n\nAlgorithm 3 Delta Insert Algorithm\nInsertKey(tree,key)\n1seg←SearchTree(tree.root ,key)\n2seg.buffer .insert(key)\n3ifseg.buffer .isFull()\n4 then\n5 segs=segmentation(seg.data ,seg.buffer)\n6 fors∈segs\n7 dotree.insert(s)\n8 tree.remove(seg)\n9return\n5.2 Delta Insert Strategy\nSince the segments in FITing-Tree are of variable size, the\ncost of an insert operation using the previously described\nin-place insertion strategy can be high, particularly for large\nerror thresholds or uniform data that produces large seg-\nments with many data items. Specifically, on average,|s|\n2\nkeys may need to be moved for a single insert operation,\nwhere|s|is the number of locations in the segment. There-\nfore, a better approach for inserting new data items into a\nFITing-Tree should amortize the cost of moving keys in the\nsegment.\nTo reduce the overhead of moving data items inside a page\nwhen inserting a new item, each segment in a FITing-Tree\ncontains an additional fixed-size buffer instead of extra space\nat each end. More specifically, as shown in Algorithm 3, new\nkeys are added to the buffer portion of the segment for which\nthe key belongs to (line 2). This buffer is kept sorted to enable\nefficient search and merge operations.\nOnce the buffer reaches its predetermined size ( buff),\nit is combined with the data in the segment and then re-\nsegmented using the previously described segmentation al-\ngorithm (Algorithm 1) to create a series of valid segments\nthat satisfy the error threshold (line 5). Note that depending\non the data, the number of segments after this process can be\none (i.e., the data inserted into the buffer does not violate the\nerror threshold) or several. Finally, each of the new segments\ngenerated from the segmentation process are inserted into\nthe tree (line 6-7) and the old page is removed (line 8).\nStoring additional data inside a segment impacts how to\nlocate a given item, as well as how the error is defined. Since\nadding a buffer for each segment can violate the error guar-\nantees that FITing-Tree provides, we transparently incor-\nporate the buffer’s size into the error parameter for the seg-\nmentation process. More formally, given a specified error\noferror , we transparently set the error threshold for the\nsegmentation process to ( error−buff). This ensures that a\nlookup operation will satisfy the specified error even if the\nelement is located in the buffer.\nThe overall runtime for inserting a new element into a\nFITing-Tree is the time required to locate the segment and\nadd the element to the segment’s buffer. With ppages storedin aFITing-Tree , and a fanout of b(i.e., number of keys in\neach internal separator node), inserting a new key into a\nFITing-Tree has the following runtime:\ninsert runtime :O(logbp)+O(buff) (4)\nNote that when the buffer is full and the segment needs to\nbe re-segmented, the runtime has an additional cost of O(d),\nwhere dis the sum of a segment’s data size and buffer size.\nAdditionally, if the write-rate is very high, we could also\nsupport merging algorithms that use a second buffer similar\nto how column stores merge a write-optimized delta to the\nmain compressed column. However, this is an orthogonal\nconsideration that heavily depends on the read/write ratio\nof a workload and is outside the scope of this paper.\n6 COST MODEL\nSince the specified error threshold affects both the perfor-\nmance of lookups and inserts as well as the index’s size, the\nnatural question follows: how should a DBA pick the error\nthreshold for a given workload? To navigate this tradeoff, we\nprovide a cost model that helps a DBA pick a “good” error\nthreshold when creating a FITing-Tree . At a high level, there\nare two main objectives that a DBA can optimize: perfor-\nmance (i.e., lookup latency) and space consumption [ 8,15].\nTherefore, we present two ways to apply our cost model that\nhelp a DBA choose an error threshold.\n6.1 Latency Guarantee\nFor a given workload, it is valuable to be able to provide\nlatency guarantees to an application. For example, an appli-\ncation may require that lookups take no more than a speci-\nfied time threshold (e.g., 1000ns) due to SLAs or application-\nspecific requirements (e.g., for interactive applications). Since\nFITing-Tree incorporates an error term that in turn affects\nperformance, we can model the index’s latency in order to\npick an error threshold that satisfies the specified latency\nrequirement.\nAs discussed, lookups require finding the relevant seg-\nment and then searching the segment (data and buffer) for\nthe element. Since the error threshold influences the number\nof segments that are created (i.e., a smaller error threshold\nyields more segments), we use a function that returns the\nnumber of segments created for a given dataset and error\nthreshold. This function can either be learned for a specific\ndataset (i.e., segment the data using different error thresh-\nolds) or a general function can be used (e.g., make the sim-\nplifying assumption that the number of segments decreases\nlinearly as the error increases). We use Seto represent the\nnumber of resulting segments for given dataset using an\nerror threshold of e.\nTherefore, the total estimated lookup latency for an error\nthreshold of ecan be modeled by the following expression,\n\nwhere bis the tree’s fanout, buff is a segment’s maximum\nbuffer size, and cis a constant representing the latency (in ns)\nof a cache miss on the given hardware (e.g., 50ns). Moreover,\nthe cost function assumes binary search for the area that\nneeds to be searched within a segment bounded by eas well\nas searching the complete buffer.\nlatency(e)=c\u0002\nloдb(Se)\n|    {z    }\nTree Search+loд2(e)\n| {z }\nSegment Search+log2(buff)\u0003\n|       {z       }\nBuffer Search(5)\nSetting cto a constant value implies that all random mem-\nory accesses have a constant penalty but caching can often\nchange the penalty for a random access. In theory, instead\nof being a constant, ccould be a function that returns the\npenalty of a random access but we make the simplifying that\ncis a constant.\nUsing this cost estimate, the index with the smallest stor-\nage footprint that satisfies the given latency requirement\nLreq(in nanoseconds) is given by the following expression,\nwhere Erepresents a set of possible error values (e.g., E=\n{10,100,1000}) and SIZE is a function that returns the esti-\nmated size of an index (defined in the next section).\ne= arg min\n{e∈E|LATENCY(e)≤Lreq}\u0000SIZE(e)\u0001(6)\nIn addition to modeling the latency for lookup operations,\nwe can similarly model the latency for insert operations.\nHowever, there are a few important differences. First, inserts\ndo not have to probe the segment. Also, instead of searching\na segment’s buffer, inserts require adding the item to the\nbuffer in sorted order. Finally, we must also consider the cost\nassociated with splitting a full segment.\n6.2 Space Budget\nInstead of specifying a lookup latency bound, a DBA can\nalso give FITing-Tree a storage budget to use. In this case,\nthe goal becomes to provide the highest performance (i.e.,\nlowest latency for lookups and inserts) while not exceeding\nthe specified storage budget.\nMore formally, we can estimate the size of a read-only\nclustered index (in bytes) for a given error threshold of e\nusing the following function, where again Seis the number\nof segments that are created for an error threshold of e, and\nbis the fanout of the tree.\nSIZE(e)=(Se·loдb(Se)·16B)\n|                  {z                  }\nTree+(Se·24B)|     {z     }\nSegment(7)\nThe first term is a pessimistic bound on the storage cost\nof the tree (leaf + internal nodes using 8 byte keys/pointers),\nwhile the second term represents the added metadata about\neach segment (i.e., each segment has a starting key, slope,\nand pointer to the underlying data, each 8 bytes).Therefore, the smallest error threshold that satisfies a\ngiven storage budget Sreq(given in bytes) is given by the\nfollowing expression where again Erepresents a set of all\npossible error values (e.g., E={10,100,1000}).\ne= arg min\n{e∈E|SIZE(e)≤Sreq}\u0000Latency(e)\u0001(8)\nAs we show in Section 7.7, our cost model can accurately\nestimate the size of a FITing-Tree over real-world datasets,\nproviding DBAs with a valuable way to balance performance\n(i.e., latency) with the storage footprint of a FITing-Tree .\n7 EVALUATION\nThis section evaluates FITing-Tree and the presented tech-\nniques. Overall, we see that FITing-Tree achieves compara-\nble performance to a full index as well as indexes that use\nfixed-size paging while using orders of magnitude less space.\nFirst, in Section 7.1, we compare the overall performance of\nFITing-Tree , measuring its lookup and insert performance\nfor both clustered and non-clustered indexes using a variety\nof real-world datasets. Next, we compare the two proposed\ninsert strategies in Section 7.2. Then, in Section 7.3, we mea-\nsure the construction cost of FITing-Tree and in Section 7.4\nwe show how FITing-Tree can leverage other internal index\nstructures. Section 7.5 shows the scalability of our index for\ndifferent dataset sizes. Finally, Section 7.6 shows how FITing-\nTree performs for an adversarial synthetically generated\ndataset (i.e., worst-case data distribution) and Section 7.7\nevaluates our cost model.\nAppendix B includes additional experiments that compare\nFITing-Tree to Correlation Maps [ 29], show results for range\nqueries, measure the throughput for various buffer sizes, and\nbreakdown the lookup performance of FITing-Tree .\nWe conducted all experiments on a single server with an\nIntel E5-2660 CPU (2.2GHz, 10 cores, 25MB L3 cache) and\n256GB RAM and all index and table data was held in memory.\n7.1 Exp. 1: Overall Performance\nIn the following, we evaluate the overall lookup and insert\nperformance of FITing-Tree . For these comparisons, we\nbenchmark FITing-Tree against both a full index (i.e., a\ndense index) as well as an index that uses fixed-size pages (i.e.,\na sparse index). A full index can be seen as best case baseline\nfor lookup performance and thus gives us an interesting\nreference point.\nFor the two baselines (full and fixed-size paging), we use\na popular B+ tree implementation (STX-tree [ 4] v0.9) since\nourFITing-Tree prototype also uses this tree to index the\nvariable sized segments. Importantly, as we show in Sec-\ntion 7.4, any other tree implementation can also serve as the\norganization layer.\n\n10-3\n10-2\n10-1\n100\n101\n102\n103\n104\n105\nIndex Size (MB)6008001000120014001600Time (ns) per LookupFIT\nFixed\nFull\nBinary(a) Weblogs\n10-3\n10-2\n10-1\n100\n101\n102\n103\n104\n105\nIndex Size (MB)6008001000120014001600Time (ns) per LookupFIT\nFixed\nFull\nBinary (b) IoT\n10-3\n10-2\n10-1\n100\n101\n102\n103\n104\n105\nIndex Size (MB)6008001000120014001600Time (ns) per LookupFIT\nFixed\nFull\nBinary (c) Maps\nFigure 6: Latency for Lookups (per thread)\n101\n102\n103\n104\nError0.00.20.40.60.81.0Insert Throughput (Million/s)FIT\nFixed\nFull\n(a) Weblogs\n101\n102\n103\n104\nError0.00.20.40.60.81.0Insert Throughput (Million/s)FIT\nFixed\nFull (b) IoT\n101\n102\n103\n104\nError0.00.20.40.60.81.0Insert Throughput (Million/s)FIT\nFixed\nFull (c) Maps\nFigure 7: Throughput for Inserts (per thread)\n101\n102\n103\n104\nError0.00.20.40.60.81.0Insert Throughput (Million/s)Delta\nIn-Place (low)\nIn-Place (high)\n(a) Weblogs\n101\n102\n103\n104\nError0.00.20.40.60.81.0Insert Throughput (Million/s)Delta\nIn-Place (low)\nIn-Place (high) (b) IoT\n101\n102\n103\n104\nError0.00.20.40.60.81.0Insert Throughput (Million/s)Delta\nIn-Place (low)\nIn-Place (high) (c) Maps\nFigure 8: Insertion Strategy Microbenchmark\n7.1.1 Datasets. Since performance of our index depends on\nthe distribution of elements in a given dataset, we evaluate\nFITing-Tree on real-world datasets with different distribu-\ntions. Later, in Section 7.6, we show that our techniques\nare still valuable using a synthetically generated worst-case\ndataset. For our evaluation, we use three different real-world\ndatasets, each with very different underlying data distribu-\ntions: (1) Weblogs [1], (2) IoT [1], and (3) Maps [3].\nThe Weblogs dataset contains ≈715Mlog entries for every\nweb request to the CS department at a university over the\npast 14years. This dataset contains subtle trends, such as\nthe fact that more requests occur during certain times (e.g.,\nschool year vs. summer, day vs. night). On the other hand,\nthe IoT dataset contains ≈5Mreadings from around 100\ndifferent IoT sensors (e.g., door, motion, power) installed\nthroughout an academic building at a university. Since these\nsensors generally reflect human activity, this dataset has\ninteresting patterns, such as the fact there is more activity\nduring certain hours because classes are in session. For each\nof these datasets, we create a clustered FITing-Tree using the\ntimestamp attribute (e.g., the time at which a resource was\nrequested). Finally, the Maps dataset contains the longitudeof≈2Bfeatures (e.g., museums, coffee shops) across the\nworld. Unsurprisingly, the longitude of locations is relatively\nlinear and does not contain many periodic trends. Unlike the\nprevious two datasets, we create a non-clustered FITing-Tree\nover the longitude attribute of this dataset.\nFor our approach, the most important aspect of a dataset\nthat impacts FITing-Tree ’s performance is the data’s pe-\nriodicity. For now, think of the periodicity as the distance\nbetween two “bumps” in a stepwise function that maps keys\nto storage locations as shown in Figure 13a (blue line). If the\nspecified error of FITing-Tree is larger than the periodicity\n(green line), the segmentation results in a single segment.\nHowever, if the error is smaller than the periodicity (red\nline), we need multiple segments to approximate the data\ndistribution.\nTherefore, we define a non-linearity ratio to show the peri-\nodicity of a dataset. To compute this ratio, we first calculate\nthe number of segments required to cover the dataset for a\ngiven error threshold. We then normalize this result by the\nnumber of segments required for a dataset of the same size\nwith periodicity equal to the error (which is the worst case,\nor the most “non-linear” in that scale).\n\nTo show that all datasets contain a distinct periodicity pat-\ntern, Figure 9 plots the non-linearity ratio of each of dataset.\nThe IoT dataset has a very significant bump, signifying that\nthere is very strong periodicity the scale of 104, likely due to\npatterns that follow human behavior (e.g., day/night hours).\nWeblogs has multiple bumps which are likely correlated to\ndifferent periodic patterns (e.g., daily, weekly, and yearly\npatterns). The Maps dataset, unlike the others, is linear at\nsmall scales (but has stronger periodicity at larger scales).\n7.1.2 Lookups. The first series of benchmarks show how\nFITing-Tree compares to (1) a full index (i.e., dense), (2) an\nindex that uses fixed-size paging (i.e., sparse), and (3) binary\nsearch over the entire dataset. We include binary search since\nit represents the most extreme case where the error is equal\nto the data size (i.e., our segmentation creates one segment).\nFor the Weblog and IoT dataset, we created a clustered index\nusing the timestamp attribute which is the primary key of\nthese datasets. We created a non-clustered (i.e., secondary)\nindex over the longitude attribute of the Maps dataset, which\nis not unique.\nThe results (Figure 6) show the lookup latency for various\nsizes of the index for the Weblogs (scaled to 1.5B records), IoT\n(scaled to 1.5B records), and Maps (not scaled, 2B records)\ndatasets. More specifically, since the size of both FITing-\nTree and the fixed-size paging baseline can be varied (i.e.,\nFITing-Tree ’s error term and the page size influence the\nnumber of leaf-level entries), we show the performance of\neach of these approaches for various index sizes. Note that\nthe size of a full index cannot be varied and is therefore a\nsingle point in the plot. Additionally, since binary search\ndoes not have any additional storage requirement, its size is\nzero but is visualized as a dotted line.\nIn general, the results show that FITing-Tree always per-\nforms better than an index that uses fixed-size paging. Most\nimportantly, however, FITing-Tree offers significant space\nsavings compared to both fixed-size paging and a full index.\nFor example, in the Maps dataset, FITing-Tree is able to\nmatch the performance of a full index using only 609MBof\nmemory, while a full index consumes over 30GBof space.\nMoreover, compared to a tree that uses fixed-size paging, a\nFITing-Tree which consumes only 1MBof memory is able\nto match the performance of a fixed-size index which con-\nsumes over 10GBof memory, offering a space savings of four\norders of magnitude.\nAs expected, for very small index sizes (i.e., very large\npages or a high error threshold), both FITing-Tree and fixed-\nsize paging mimic the performance of binary search since\nthere are only a few pages that are searched using binary\nsearch. On the other hand, as the index grows, the perfor-\nmance of both fixed-size paging as well as FITing-Tree con-\nverge to that of a full index due to the fact that pages contain\n101\n102\n103\n104\n105\n106\nError0.000.050.100.150.200.25Non-linearity ratioIoT\nWeblogs\nMapsFigure 9: Non-linearity\n101\n102\n103\n104\n105\n106\n107\n108\nPage Size (Error)0.01.02.03.04.05.06.07.08.0Time (s)Fixed\nFIT\nFull Figure 10: Build Time\nvery few elements. Note that the spike in the graph for the\nfixed-size index is due to the fact that the index begins to\nfall out of the CPU’s L2 cache.\nFinally, as expected, the data distribution impacts the per-\nformance of FITing-Tree . More specifically, we can see that\nFITing-Tree is able to more quickly match the performance\nof a full tree with the Maps dataset, compared to the Weblogs\nand IoT datasets. This is due to the fact that the Maps dataset\nis relatively linear, when compared to the Weblogs and IoT\ndatasets (shown in Figure 9).\n7.1.3 Inserts. Next, we compare the insert performance of\nFITing-Tree to both a full index, as well as an index that\nuses fixed-size paging as previously described. To insert new\nitems, FITing-Tree uses the previously described delta insert\ntechnique since it provides the best overall performance,\nwhich we show in next section where we perform an in-depth\ncomparison of the delta and the in-place insert strategies.\nMore specifically, to ensure a fair comparison and that\nFITing-Tree is not unfairly write-optimized, we set the size\nof the delta buffer to half of the specified error (i.e, for an er-\nror threshold of 100, the underlying data is segmented using\nan error of 50and each segment’s buffer has a maximum size\nof50elements). Similarly, for the index with fixed-size pages,\nthe page size is given by the half of the error threshold we\nused for FITing-Tree and the same amount (i.e., half of the\nerror used in FITing-Tree ) is used as the buffer size. As usual,\nonce the buffer is full, the page is split into two pages. The\nresults, shown in Figure 7, compare the throughput of each\nindex after building the index over the first half of the data\nand inserting the second half for various error thresholds\nusing shuffled versions of the previously described datasets.\nAs shown, FITing-Tree is able to achieve insert performance\nthat is, in general, comparable to an index that uses fixed-\nsize paging. Unsurprisingly, a full B+ tree is able to handle a\nhigher write load than either a FITing-Tree or an index that\nuses fixed-size paging since both need to periodically split\npages that become full. Additionally, FITing-Tree needs to\nexecute the segmentation algorithm, explaining the perfor-\nmance gap between FITing-Tree and fixed-size paging.\nInterestingly, FITing-Tree is faster than fixed-size paging\nin some cases since the error determines the number of seg-\nments in the tree. For a large error, there typically are fewer\n\n10-3\n10-2\n10-1\n100\n101\n102\n103\nIndex Size (MB)600800100012001400\n1200Time (ns) per LookupLookup\nFAST\nSTX-treeFigure 11: Other Indexes\n1 2 4 816 32\nScale Factor50070090011001300Latency (ns)Binary\nFIT\nFull\nFixed Figure 12: Scalability\nsegments generated, which reduces the number of times that\nFITing-Tree needs to merge the buffer with the segment’s\ndata and execute the segmentation algorithm.\n7.2 Exp. 2: In-place vs. Delta Inserts\nIn the following, we compare the two insert strategies de-\nscribed in Section 5 and show how they perform for various\ndatasets, fill factors, and error thresholds.\nFigure 8 shows the insert throughput for both the delta and\nin-place insert strategies for each of the previously described\ndatasets. As mentioned, for in-place inserts, the amount of\nfree space reserved at the beginning and end of page ( ε)\ncan be tuned based on the read/write characteristics of the\nworkload. Therefore, we now show results for both a low\n(i.e., εis 25% of the error) and high (i.e., εis 75% of the error)\nfill factor. For the delta insert results, we use the same setup\nas the previously described insert experiment (i.e., we set the\nsize of the delta buffer to half of the specified error).\nAs shown, the delta insert strategy generally offers the\nhighest insert throughput for error thresholds higher than\n100. This is due to the fact that the in-place insert strategy\nmust move many data items when inserting a new item\nsince segments created with a higher error threshold contain\nmore keys. However, for low error thresholds, the in-place\ninsert strategy outperforms the delta strategy, since there\nare significantly fewer data items that need to be shifted.\nThe fill factor impacts (1) how many data items are in a\ngiven segment and must be copied when inserting a new\nelement, and (2) how often a segment fills up and needs to\nbe re-segmented. As shown, in general, the in-place strategy\nwith a low fill factor offers the highest insert performance.\n7.3 Exp 3: Index Construction\nIn the following, we quantify the cost to construct a FITing-\nTree . More specifically, we measure the amount of time\nrequired to bulk load a FITing-Tree and compare it to the\ntime required to construct a B+ tree that uses fixed-size pages\nas well as a full index. The results, shown in Figure 10, plot\nthe runtime for each approach for various page sizes (Fixed\nB+ tree) or error thresholds ( FITing-Tree ) using the Weblogs\ndataset.Since a full index must insert every element into the tree, it\ntakes a constant amount of time. However, the time required\nto bulk load a B+ tree that uses fixed-size pages decreases as\nthe page size increases since only the page boundaries (e.g.,\nevery 100th element) must be accessed from the underly-\ning data. As previously mentioned, building a FITing-Tree\nrequires first segmenting the data, and then inserting each\nsegment into the underlying tree structure. Unsurprisingly,\nsince the segmentation algorithm must examine every ele-\nment in the data, a FITing-Tree incurs a constant amount\nof extra overhead ( ≈4.2 seconds in the shown experiment)\ncompared to a B+ tree that uses fixed-size pages.\nInterestingly, however, this constant amount extra over-\nhead can be avoided in some cases. For example, when ini-\ntially loading the data into the system, it is possible to execute\nthe segmentation algorithm in a streaming fashion. In this\ncase, it is possible that building a FITing-Tree is actually\nless expensive than building a B+ tree with fixed-size pages\nsince the resulting segmentation algorithm leverages trends\nin the data to produce fewer lead-level entries.\n7.4 Exp 4: Other Indexes\nAs mentioned, FITing-Tree internally uses STX-tree [ 4] to\norganize the variable sized pages generated through our\nsegmentation process (Section 3). However, depending on\nthe workload characteristics (e.g., read/write ratio), other\ninternal data structures could also be used to index segments\nto provide additional performance improvements.\nTherefore, to show that our techniques are generalizable,\nwe compare using an STX-tree to (1) FAST [ 28], a highly\noptimized index structure for read-only workloads and (2)\na simple lookup table that simply stores the variable sized\npages in sorted order. Figure 11 shows the results for vari-\nous index sizes (i.e., error thresholds) over a subset of the\nWeblogs dataset (since FAST requires a power of two ele-\nments). As shown, a lookup table provides superior space\nsavings (since there are no internal separator nodes) but with\na higher lookup latency. On the other hand, a FITing-Tree\nthat internally uses the highly optimized FAST index can\nprovide faster lookups but often consumes more space. There-\nfore, a FITing-Tree is able to leverage alternative indexes\nthat may be more performant depending on the workload\ncharacteristics (e.g., read-only).\n7.5 Exp. 5: Data Size Scalability\nTo evaluate how FITing-Tree performs for various dataset\nsizes, we measure the lookup latency for the Weblogs dataset\nfor various scale factors where both the error threshold and\nfixed page size are set to 100, which is optimal for this dataset.\n\n02004006008001000\nTiPestaPp02004006008001000PositionActualError 100Error 1000(a) Worst Case Data\n101102103104105106107108109(rror10-310-210-1100101102103104105Index Size (MB)Approx\nFixed\nFull (b) Worst Case Index Size\nFigure 13: Worst Case Analysis\nSince the performance of our index depends on the under-\nlying data distribution, we scale the dataset while maintain-\ning the underlying trends. We omit the result for all other\ndatasets here (IoT and Maps) since they follow similar trends.\nFigure 12 shows that the indexes (i.e., FITing-Tree , a\nfull index, fixed-size paging) scale better than binary search\ndue to the better theoretical asymptotic runtime ( loдb(n)vs.\nloд2(n)) and cache performance. Additionally, FITing-Tree ’s\nperformance over various dataset sizes closely follows that of\na full index which offers the best performance, demonstrating\nthat our techniques offer valuable space savings without\nsacrificing performance. Most importantly, neither a full\nindex nor an index that uses fixed-size paging could scale to\na scale factor of 32since the size of the index exceeded the\namount of available memory. This again shows that FITing-\nTree is able to offer valuable space savings.\n7.6 Exp 6: Worst Case Data\nSince the data distribution influences the performance of\nFITing-Tree , we synthetically generated data to illustrate\nhow our index performs with data that represents a worst-\ncase. We define the worst case as as a dataset which max-\nimizes the number of segments given a specific error, de-\nscribed further in Section 3.2. To do this, we generate data\nusing a step function with a fixed step size of 100, as shown\nin Figure 13a. Since the step size is fixed, an error threshold\nless than the step size yield a single segment per step. How-\never, given an error threshold larger than the step size, our\nsegmentation algorithm will be able to use a single segment\nto represent the entire dataset.\nFigure 13b shows the performance for various sizes of\neach index built over this worst case dataset. As shown, for\nerror thresholds of less than 100, the size of a FITing-Tree\nis the same as a fixed-size index but still smaller than a full\nindex. This is due to the fact that for the error thresholds less\nthan the step size, FITing-Tree creates segments of size 100\n(step size), resulting in a large number of nodes in the tree.\nOn the other hand, for an error threshold that is larger than\n100,FITing-Tree is able to represent the step dataset with\nonly a single segment, dramatically reducing the index’s size.\nAs shown, importantly, a FITing-Tree will not contain more\n100101102103104105(rror75080085090095010001050110011501200Lookup tLme (ns)Actual\n0odel(a) Latency\n101102103104105(rror10-210-1100101102103Size(MB)Actual\n0odel (b) Size\nFigure 14: Cost Model Accuracy\nleaf-level entries than an index that uses fixed-size paging,\nas discussed in Section 3.4.\n7.7 Exp. 7: Accuracy of Cost Model\nSince, as previously described, the error threshold influences\nboth the latency as well as space consumption of our in-\ndex, the cost model presented in Section 6 aims to guide a\nDBA when determining what error threshold to use for a\nFITing-Tree . More specifically, given a latency requirement\n(e.g., 1000ns) or a space budget (e.g., 2GB), our cost model\nautomatically determines an appropriate error threshold that\nsatisfies the given constraint.\nFigure 14a shows the estimated and actual lookup latency\nfor various error thresholds on the Weblogs dataset using\na value of 50nsforc(the cost of a random memory access)\ndetermined through a memory benchmarking tool on the\ngiven hardware. As shown, our latency model predicts an\naccurate upper bound for the actual latency of a lookup\noperation. Our model slightly overestimates the latency due\nto the fact that it does not incorporate CPU caching effects.\nSince we overestimate the cost, we ensure that a specified\nlatency threshold will always be observed.\nTo evaluate our size cost model, we show the predicted\nand actual size of a FITing-Tree for various error thresholds\nin Figure 14b. As shown, our model is able to accurately\npredict the size of an index for a given error threshold while\nensuring that our estimates are pessimistic (i.e., the estimated\ncost is higher than the true cost).\n8 RELATED WORK\nThe presented techniques in this paper overlap with work in\ndifferent areas including (1) index compression,\n(2) partial/adaptive indexes, and (3) function approximation.\nIndex Compression: Since B+ trees can often consume sig-\nnificant space, several index compression techniques have\nbeen proposed. These approaches reduce the size of keys\nin internal nodes by applying techniques such as prefix/suf-\nfix truncation, dictionary compression, and key normaliza-\ntion [ 21,23,36]. Importantly, these techniques can also be\napplied within FITing-Tree to further reduce the size of the\nunderlying tree structure.\n\nSimilar to B+ tree compression, several methods have\nbeen proposed in order to more compactly represent bitmap\nindexes [ 6,11,26,39,42,45]. Many of these techniques are\nspecific to bitmap indexes, which are primarily only useful\nfor attributes with few distinct values and not the general\nworkloads that FITing-Tree targets.\nCorrelation Maps [ 29] try to leverage correlations between\nan unclustered attribute and a clustered attribute when an\nexisting primary key index exists. Our approach, on the other\nhand, does not assume an existing index already exists and\nuses variable sized paging (instead of fixed-sized buckets)\nthat better model the underlying data.\nFAST [ 28] is another more recent tree structure that or-\nganizes tree elements in a more compact and efficient repre-\nsentation in order to exploit modern hardware features (e.g.,\nSIMD, cache line size) for read-heavy workloads. Similarly,\nan Adaptive Radix Tree (ART) [ 32] leverages CPU caches\nfor in-memory indexing. Another idea discussed in [ 48]\nare hybrid indexes which separate the index into hot and\ncold regions where cold data is stored in a compressed for-\nmat. Lastly, Log-structured Merge-trees [ 37] are designed for\nmostly write intensive workloads and extensions including\nMonkey [ 15] balance performance and memory consump-\ntion. Each of these techniques can be seen as orthogonal and\nthus also could be used by FITing-Tree to more efficiently\nstore the underlying tree structure as well as optimize for\nread-heavy workloads or hot/cold data.\nOther indexing techniques have been proposed that store\ninformation about a region of the dataset, instead of the\nindexing individual keys. For example, leaf nodes in a BF-\nTree [ 7] are bloom filters. Unlike FITing-Tree , BF-Tree does\nnot exploit properties about the data’s distribution when seg-\nmenting a dataset. Another example are learned indexes [ 30],\nwhich aim to learn the underlying data distribution to index\ndata items. Unlike learned indexes, FITing-Tree has strict\nerror guarantees, supports insert operations, and provides a\ncost model to ensure predictable performance and size.\nSparse indexes like Hippo [ 47], Block Range Indexes [ 44],\nand Small Materialized Aggregates [ 34] all store informa-\ntion about value ranges similar to the idea of segments in\nFITing-Tree . However, these techniques do not consider the\nunderlying data distribution or bound lookup/insert latency.\nFinally, several approximation techniques have been pro-\nposed in order to improve the performance of similarity\nsearch [ 19,24,40] (for string or multimedia data), unlike\nFITing-Tree which uses approximation for compressing\nindexes optimized for traditional point and range queries.\nPartial and Adaptive Indexes: Partial indexes [ 43] aim to\nreduce the storage footprint of an index since they index only\na subset of the data that is of interest to the user. For example,\nTail Indexes [ 14,20] store only rare data items in order to re-\nduce the storage footprint of the overall index. FITing-Tree ,on the other hand, supports queries over all attribute values\nbut could be extended to index only “important” data ranges\nas well. Furthermore, database cracking [ 25] is a technique\nthat physically reorders values in a column in order to more\nefficiently support selection queries without needing to store\nsecondary indexes. Since database cracking reorganizes val-\nues based on past queries, it does not efficiently support\nad-hoc queries, like FITing-Tree can.\nFunction Approximation: The main idea of a FITing-Tree is\nto approximate the data distribution using piece-wise linear\nfunctions and approximating curves using piece-wise func-\ntions is not new [ 10,16,18,33]. The error metrics E2(integral\nsquare error) and E∞(maximal error) for these approxima-\ntions have been discussed as well as different segmentation\nalgorithms [ 17,35,38]. Unlike prior work, we consider only\nmonotonic increasing functions, E∞, and potentially disjoint\nlinear segments. Moreover, none of these techniques have\nbeen applied to indexing and therefore do not consider look-\ning up or inserting data items.\nMore recent work [ 12,13,27,41,46] specific to time se-\nries data also leverages piece-wise linear approximations to\nstore patterns for similarity search. While these approaches\nalso trade-off the number of segments with the accuracy of\nthe approximate representation, they do not aim to provide\nthe lookup and space consumption guarantees that FITing-\nTree does, and do not have the analysis related to these\nguarantees.\nFinally, other work [ 5] leverages piece-wise linear func-\ntions to compress inverted lists by storing functions and\nthe distances of elements from the extrapolated functions.\nHowever, these approximations use linear regression (which\nminimizes E2), and there are no bounds on the error.\n9 CONCLUSION\nIn this paper, we introduced FITing-Tree , a new index struc-\nture that incorporates a tunable error parameter to allow a\nDBA to balance lookup performance and space consumption\nof an index. To navigate this tradeoff, we presented a cost\nmodel that determines an appropriate error parameter given\neither (1) a lookup latency requirement (e.g., 500ns) or (2)\na storage budget (e.g., 100MB). We evaluated FITing-Tree\nusing several real-world datasets and showed that our index\ncan achieve comparable performance to a full index structure\nwhile reducing the storage footprint by orders of magnitude.\n10 ACKNOWLEDGEMENTS\nThis research is funded in part by the NSF CAREER Awards\nIIS-1453171 and CNS-1452712, NSF Award IIS-1514491, Air\nForce YIP AWARD FA9550-15-1-0144, and the Data Systems\nand AI Lab (DSAIL) at MIT, as well as gifts from Intel, Mi-\ncrosoft, and Google.\n\nREFERENCES\n[1]2019. A Benchmark for Machine-generated Data Management. https:\n//github.com/BrownBigData/MgBench.\n[2]2019. NYC Taxi & Limousine Commission Trip Record Data. http:\n//www.nyc.gov/html/tlc/html/about/trip_record_data.shtml.\n[3]2019. OpenStreetMap database. https://aws.amazon.com/\npublic-datasets/osm.\n[4] 2019. STX B+ Tree. https://panthema.net/2007/stx-btree/.\n[5]Naiyong Ao et al .2011. Efficient Parallel Lists Intersection and Index\nCompression Algorithms Using Graphics Processing Units. VLDB\n(2011), 470–481.\n[6]Manos Athanassoulis et al .2016. UpBit: Scalable In-Memory Updatable\nBitmap Indexing. In SIGMOD . 1319–1332.\n[7]Manos Athanassoulis and Anastasia Ailamaki. 2014. BF-tree: Approxi-\nmate Tree Indexing. In VLDB . 1881–1892.\n[8]Manos Athanassoulis and Stratos Idreos. 2016. Design Tradeoffs of\nData Access Methods. In SIGMOD . 2195–2200.\n[9]Rudolf Bayer and Karl Unterauer. 1977. Prefix B-trees. ACM Trans.\nDatabase Syst. (1977), 11–26.\n[10] Dietrich Braess. 1971. Chebyshev Approximation by Spline Functions\nwith Free Knots. Numerische Mathematik (1971), 357–366.\n[11] Chee-Yong Chan and Yannis E. Ioannidis. 1998. Bitmap Index Design\nand Evaluation. In SIGMOD . 355–366.\n[12] Lu Chen et al .2017. Efficient Metric Indexing for Similarity Search\nand Similarity Joins. In KDE. 556–571.\n[13] Tak chung Fu. 2011. A Review on Time Series Data Mining. Engineering\nApplications of Artificial Intelligence (2011), 164 – 181.\n[14] Andrew Crotty et al .2016. The Case for Interactive Data Exploration\nAccelerators (IDEAs). In HILDA . 11:1–11:6.\n[15] Niv Dayan, Manos Athanassoulis, and Stratos Idreos. 2017. Monkey:\nOptimal Navigable Key-Value Store. In SIGMOD . 79–94.\n[16] Frank Eichinger et al .2015. A Time-series Compression Technique and\nIts Application to the Smart Grid. The VLDB Journal (2015), 193–218.\n[17] Hazem Elmeleegy et al .2009. Online Piece-wise Linear Approximation\nof Numerical Streams with Precision Guarantees. VLDB (2009), 145–\n156.\n[18] R.E Esch and W.L Eastman. 1969. Computational Methods for Best\nSpline Function Approximation. Journal of Approximation Theory\n(1969), 85 – 96.\n[19] Andrea Esuli. 2012. Use of Permutation Prefixes for Efficient and\nScalable Approximate Similarity Search. Inf. Process. Manage. (2012),\n889–902.\n[20] Alex Galakatos et al .2017. Revisiting Reuse for Approximate Query\nProcessing. In VLDB . 1142–1153.\n[21] Jonathan Goldstein et al .1998. Compressing Relations and Indexes. In\nICDE . 370–379.\n[22] Goetz Graefe. 2006. B-tree Indexes, Interpolation Search, and Skew. In\nDaMon .\n[23] Goetz Graefe and Per-Åke Larson. 2001. B-Tree Indexes and CPU\nCaches. In ICDE . 349–358.\n[24] Michael E. Houle and Jun Sakuma. 2005. Fast Approximate Similarity\nSearch in Extremely High-Dimensional Data Sets. In ICDE . 619–630.\n[25] Stratos Idreos et al. 2007. Database Cracking. In CIDR . 68–78.\n[26] Theodore Johnson. 1999. Performance Measurements of Compressed\nBitmap Indices. In VLDB . 278–289.\n[27] Eamonn Keogh et al .2001. An Online Algorithm for Segmenting Time\nSeries. In ICDM . IEEE, 289–296.\n[28] Changkyu Kim et al .2010. FAST: Fast Architecture Sensitive Tree\nSearch on Modern CPUs and GPUs. In SIGMOD . 339–350.\n[29] Hideaki Kimura et al. 2009. Correlation Maps: A Compressed Access\nMethod for Exploiting Soft Functional Dependencies. VLDB , 1222–\n1233.[30] Tim Kraska et al .2018. The Case for Learned Index Structures. In\nSIGMOD . 489–504.\n[31] Domine M. W. Leenaerts and Wim M. Van Bokhoven. 1998. Piecewise\nLinear Modeling and Analysis .\n[32] Viktor Leis et al .2013. The Adaptive Radix Tree: ARTful Indexing for\nMain-memory Databases. In ICDE . 38–49.\n[33] Xiaoyan Liu, Zhenjiang Lin, and Huaiqing Wang. 2008. Novel Online\nMethods for Time Series Segmentation. IEEE Trans. on Knowl. and\nData Eng. (2008), 1616–1626.\n[34] Guido Moerkotte. 1998. Small Materialized Aggregates: A Light Weight\nIndex Structure for Data Warehousing. In VLDB . 476–487.\n[35] Thomas Neumann and Michel Sebastian. 2008. Smooth Interpolating\nHistograms with Error Guarantees. In BNCOD .\n[36] Thomas Neumann and Gerhard Weikum. 2008. RDF-3X: A RISC-style\nEngine for RDF. In VLDB . 647–659.\n[37] Patrick O’Neil et al .1996. The Log-structured Merge-tree (LSM-tree).\nActa Inf. (1996), 351–385.\n[38] T. Pavlidis and S. L. Horowitz. 1974. Segmentation of Plane Curves.\nIEEE Trans. Comput. (1974), 860–870.\n[39] Ali Pinar et al .2005. Compressing Bitmap Indices by Data Reorganiza-\ntion. In ICDE . 310–321.\n[40] K. V. Ravi Kanth et al .1998. Dimensionality Reduction for Similarity\nSearching in Dynamic Databases. In SIGMOD . 166–176.\n[41] Hagit Shatkay and Stanley B Zdonik. 1996. Approximate Queries and\nRepresentations for Large Data Sequences. In ICDE . IEEE, 536–545.\n[42] Michał Stabno and Robert Wrembel. 2009. RLH: Bitmap Compression\nTechnique Based on Run-length and Huffman Encoding. Inf. Syst.\n(2009), 400–414.\n[43] Michael Stonebraker. 1989. The Case for Partial Indexes. SIGMOD\nRecord (1989), 4–11.\n[44] Michael Stonebraker and Lawrence A. Rowe. 1986. The Design of\nPOSTGRES. In SIGMOD . 340–355.\n[45] Kesheng Wu et al .2006. Optimizing Bitmap Indices with Efficient\nCompression. ACM Trans. Database Syst. (2006), 1–38.\n[46] Zhenghua Xu et al .2012. An Adaptive Algorithm for Online Time\nSeries Segmentation with Error Bound Guarantee. In EDBT .\n[47] Jia Yu and Mohamed Sarwat. 2016. Two Birds, One Stone: A Fast, Yet\nLightweight, Indexing Scheme for Modern Database Systems. In VLDB .\n385–396.\n[48] Huanchen Zhang et al .2016. Reducing the Storage Overhead of Main-\nMemory OLTP Databases with Hybrid Indexes. In SIGMOD . 1567–\n1581.\n[49] Marcin Zukowski et al .2006. Super-Scalar RAM-CPU Cache Compres-\nsion. In ICDE . 59–.\nA SEGMENTATION ANALYSIS\nIn the following we provide additional information about\nour segmentation algorithm, ShrinkingCone , described in\nSection 3. First, we prove the minimum size of a segment\nproduced by our algorithm. Then, although efficient in prac-\ntice, we show that our algorithm can be arbitrarily worse\nthan an optimal algorithm when considering the number of\nsegments it produces.\nA.1 ShrinkingCone Segment Size\nWe prove the claim from Theorem 3.1 regarding the size of\na maximal linear segment.\n\nProof. Consider 3 arbitrary points (x1,y1),(x2,y2),(x3,y3),\nwhere x1<x2<x3andy1<y2<y3. By definition, the lin-\near function starts at the first point in a segment, and ends at\nthe last point in the segment. The linear segment is not valid\nif the distance on the yaxis ( loc) is larger than the specified\nerror. Therefore, given the 3 points, a linear segment starting\nat(x1,y1)and ending at(x3,y3)is not feasible if:\ny2−err>y3−y1\nx3−x1(x2−x1)+y1 (9)\nBy rearranging the inequality we get:\nerr<y2−y1−y3−y1\nx3−x1(x2−x1) (10)\n=(y3−y1)·\u0012\n1−x2−x1\nx3−x1\u0013\n−(y3−y2) (11)\n≤(y3−y1)·\u0012\n1−x2−x1\nx3−x1\u0013\n−1 (12)\nIn(11)y3was added and subtracted, and in (12)we use the\nfact that y2andy3are integers (thus y3−y2≥1). This pro-\nvides a lower bound for the distance between the first point\nin a segment and the first point in the following segment:\ny3−y1>err+1\n1−x2−x1\nx3−x1=(err+1)·x3−x1\nx3−x2>err+1(13)\n⇒y3−y1>err+1 (14)\nSince(x3,y3)is the first point outside of the segment, the\nnumber of locations in the segment is y3−1−y1≥err+1.□\nA.2 ShrinkingCone Competitive\nAnalysis\nIn the following, we prove that ShrinkingCone can be ar-\nbitrarily worse than the optimal solution when considering\nthe number of segments produced (i.e., ShrinkingCone is\nnot competitive).\nProof. Given the error threshold E=100, consider the\nfollowing input to ShrinkingCone :\n(1)3 keys(x1,y1),(x2,y2),(x3,y3)where y1=1,y2=2,y3=\n3andx3−x2=x2−x1=E\n2(this is step 1 in Figure 15).\n(2)The key x4=x3+1\nErepeated E+1times (using E+1\nconsecutive locations), and the key x5=x4+1\nEwithout\nrepetitions (using 1 location).\nAfter that repeat for i∈[1,N]the following pattern:\nthe key x2(i+2)=x2(i+2)−1+Erepeated E+1times, and\na single appearance of the key x2(i+2)+1=x2(i+2)+1\nE\n(this is step 2 in Figure 15).\n(3)The key x2(N+1+2)=x2(N+1+2)−1+E\n2(step 3 in Fig-\nure 15).\nkeyloc\nStep 1 Step 2Step 3Figure 15: Competitive analysis sketch: the dots are\nthe input, the dashed line is the optimal segmentation\n(the first dot is a segment), and the solid lines are the\nsegments created by ShrinkingCone .\nThe algorithm will then create the following segments (an\nillustration is shown in Figure 15):\n•[x1,x4](with slope3\nE+1\nE): adding the key x5will result\nin the slope3+E+1\nE+2\nEwhich will not satisfy the error\nrequirement for x4,y1+3+E+1\nE+2\nE·(x4−x1)−y4=1+\n3+E+1\nE+2\nE·(E+1\nE)−4=100.98>E.\n•Each of the next segments will contain exactly two\nkeys (where the first key appears once, and the second\nkey appears E+1times), since otherwise the error\nfor the second key will be1+E+1\nE+1\nE·E−1=100.98>E.\nJust like before, the E+1repetitions of a single key\nwill cause a violation of the error (due to the spacing\nbetween subsequent keys).\nTherefore, the algorithm will create N+2segments given\nthis input.\nOn the other hand, the optimal algorithm will need only\n2segments: the first segment is the first key, and the second\nsegment covers the rest of the input since the line starting\nat the second key and ending at the last key is never further\naway than Efrom any key, due to the construction of the\ninput. The slope of the second segment will be3+(N+1)·(E+2)\nE+(N+1)·(E+1\nE),\nand the first key in the segment is aboutE\n2away on the x\naxis from the first repeated key. Since the repeated keys are\nspaced evenly (distance on the xaxis of E+1\nE), the linear\nfunction will not violate the error threshold for any key.\nSince Ncan be arbitrarily large, the algorithm is not com-\npetitive. □\nB FURTHER EVALUATION\nIn this section, we present additional experimental results\nthat provide a more in-depth study of FITing-Tree and other\napproaches. More specifically, we compare FITing-Tree to\nCorrelation Maps [ 29], show how FITing-Tree performs for\n\n10-2\n10-1\n100\n101\n102\nIndex Size (MB)101102103Lookup Latency (ns)Fixed\nFIT\nCM(a) Linear Distribution\n10-1\n100\n101\n102\nIndex Size (MB)4005006007008009001000Lookup Latency (ns)FIT\nCM (b) Weblogs\nFigure 16: Correlation Maps\nqueries that involve range predicates, show how the buffer\nsize impacts insert throughput, and finally breakdown the\nlookup latency.\nB.1 Correlation Maps\nIn the following, we compare FITing-Tree to Correlation\nMaps (CMs) which are designed to exploit correlations that\nexist between attributes. To ensure a fair comparison and to\nadapt the described techniques to build a clustered primary\nindex, we assume that each tuple has an implicit position\n(e.g., ROWID). In addition to the design in the original paper\n[29] (e.g., bucketing both along the clustered and unclustered\ndimension), we implemented additional optimizations not\ndescribed in the paper since our use case has the additional\nknowledge that the unclustered attribute (i.e., timestamp) is\nsorted with respect to the clustered attribute (i.e., ROWID).\nFor example, instead of storing several buckets for a given\nunclustered range (e.g., {100-200} →[b0,b1,b2,b3]) our imple-\nmentation stores only the first and last bucket (e.g., {100-200}\n→[b0,b3]). Additionally, when looking up a key, our imple-\nmentation uses binary search within the entire region instead\nof searching each bucket individually. We found that these\ntwo optimizations improved lookup performance/reduced\nthe size of a CM.\nFirst, to show that CMs are no more efficient for primary\nindexes than an index that uses fixed-size pages, we con-\nsider the simple case of indexing all integer values from 1 to\n100M. Although simple, this is not unusual since users often\nindex monotonically increasing identifiers (e.g., customer\nID). Figure 16a shows the lookup latency for various index\nsizes (x-axis) for CMs, FITing-Tree , and fixed-size paging.\nAs previously described, we vary the size of FITing-Tree\nby selecting various error thresholds and use different page\nsizes/bucket sizes for CMs and B+ trees that use fixed-size\npaging. As shown, CMs perform similar to B+ trees with\nfixed-size pages, since they use fixed-size buckets to parti-\ntion the attribute domain. FITing-Tree , on the other hand,\ncan use a single segment to represent this data and can locate\nto the exact position of any element using almost no space.\n10-3\n10-2\n10-1\n100\n101\nSelectivity (percent)103104105106107108Lookup Latency (ns)Sum\nCountFigure 17: Range Queries\nNext, we also used CMs also to build a primary key in-\ndex on the Weblogs data set and compare it to our FITing-\nTree . Figure 16b shows index size (x-axis) vs. the lookup\nlatency (y-axis) for both CMs and FITing-Tree using 400M\ntimestamps from the Weblogs dataset. Since FITing-Tree\ncreates variable-sized segments that better model the under-\nlying data distribution (instead of the fixed-size bucketing\napproach that CMs use), FITing-Tree is able to provide faster\nlookup performance using a smaller memory footprint.\nB.2 Range Queries\nIn addition to point queries, FITing-Tree also supports range\nqueries whereby an arbitrary number of tuples must be exam-\nined to compute the result. Figure 17 shows the performance\nof aFITing-Tree for range queries for both a sum and count\naggregate for various selectivities using the Weblogs dataset.\nInterestingly, to compute the result for a count query, a\nFITing-Tree can subtract the start position from the end\nposition of the range (i.e., a count aggregate over a range\nis essentially two point lookups), resulting in a constant\nlookup latency. On the other hand, computing the sum of an\nattribute over a range requires examining every tuple in the\nrange, resulting in significantly more work for larger ranges.\nB.3 Varying Fill Factor\nAs previously mentioned, the buffer size of a segment deter-\nmines the amount of space that a segment reserves to hold\nnew data items. Once the segment’s insert buffer reaches\nthis threshold, the data from the segment and the segment’s\nbuffer are merged, and FITing-Tree executes the previously\ndescribed segmentation algorithm to generate new segments\nthat satisfy the specified error threshold.\nTherefore, in Figure 18, we vary the buffer size and mea-\nsure the total throughput using the Weblogs dataset with\nan error threshold of e=20,000. As shown, the size of the\nbuffer can dramatically impact the write throughput of a\nFITing-Tree . More specifically, larger buffers result in fewer\nsplitting operations, improving performance. However, a\nbuffer that is too large will result in longer lookup latencies\n(modeled in the cost model in Section 6).\n\n101\n102\n103\n104\nBuffer Size0.00.20.40.60.81.0Insert Throughput (Million/s)Figure 18: Insert Throughput / Varying Buffer Size\nTherefore, the fill factor of an FITing-Tree can be effec-\ntively used by a DBA to tune a FITing-Tree to be more read\nor write optimized, depending on the workload.\nB.4 Lookup Breakdown\nAs described in Section 4, a lookup involves two steps (i.e., lo-\ncating the segment where a key belongs and then searching\nthe segment’s data in order to find the item within the seg-\nment). Therefore, we examine the amount of time spent in\neach of these two steps for FITing-Tree as well as an index\nthat uses fixed-size paging for various error thresholds.\nThe results in Figure 19 show that in both cases the major-\nity of time is spent searching the tree to find the page where\nthe data item belongs for smaller error thresholds (and page\nsizes). Since FITing-Tree is able to leverage properties of\nthe underlying data distribution in order to create variable\nsized segments, the resulting tree is significantly smaller.\nTherefore, FITing-Tree spends less time searching the tree\nto find the corresponding segment for a given key.\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% \n10^110^210^310^410^510^610^710^810^9TreePage(a)FITing-Tree\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% \n10^110^210^310^410^510^610^710^810^9TreePage\n(b) Fixed-size Index\nFigure 19: Lookup Breakdown",
  "textLength": 86207
}