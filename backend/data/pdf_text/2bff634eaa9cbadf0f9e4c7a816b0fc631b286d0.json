{
  "paperId": "2bff634eaa9cbadf0f9e4c7a816b0fc631b286d0",
  "title": "Parallel computation of PDFs on big spatial data using Spark",
  "pdfPath": "2bff634eaa9cbadf0f9e4c7a816b0fc631b286d0.pdf",
  "text": "Parallel Computation of PDFs on Big Spatial Data Using\nSpark\nJi Liu\u00031, Noel Moreno Lemusy2, Esther Pacittiz1, Fabio Portox2, and Patrick\nValduriez{1\n1Inria and LIRMM, Univ. of Montpelier, France\n2LNCC Petr\u0013 opolis, Brazil\nAbstract\nWe consider big spatial data, which is typically produced in scienti\fc areas such as\ngeological or seismic interpretation. The spatial data can be produced by observation ( e.g.\nusing sensors or soil instrument) or numerical simulation programs and correspond to points\nthat represent a 3D soil cube area. However, errors in signal processing and modeling\ncreate some uncertainty, and thus a lack of accuracy in identifying geological or seismic\nphenomenons. Such uncertainty must be carefully analyzed. To analyze uncertainty, the\nmain solution is to compute a Probability Density Function (PDF) of each point in the\nspatial cube area. However, computing PDFs on big spatial data can be very time consuming\n(from several hours to even months on a parallel computer). In this paper, we propose a new\nsolution to e\u000eciently compute such PDFs in parallel using Spark, with three methods: data\ngrouping, machine learning prediction and sampling. We evaluate our solution by extensive\nexperiments on di\u000berent computer clusters using big data ranging from hundreds of GB to\nseveral TB. The experimental results show that our solution scales up very well and can\nreduce the execution time by a factor of 33 (in the order of seconds or minutes) compared\nwith a baseline method.\nKeywords: Spatial data , big data, parallel processing, Spark\n1 Introduction\nBig spatial data is now routinely produced and used in scienti\fc areas such as geological or seismic\ninterpretation [4]. The spatial data are produced by observation, using sensors [25], [5] or soil\ninstruments [12], or numerical simulation, using mathematical models [6]. These spatial data\nallow identifying some phenomenon over a spatial reference [9]. For instance, the spatial reference\nmay be a three dimensional soil cube area and the phenomenon a seismic fault, represented as\nquantities of interest (QOIs) of sampled points (or points for short) in the cube space. The\ncube area is composed of multiple horizontal slices, each slice having multiple lines and each line\nhaving multiple points. A single simulation produces a spatial data set whose points represent a\n3D soil cube area.\n\u0003ji.liu@inria.fr\nynmlemus@gmail.com\nzesther.pacitti@lirmm.fr\nxfporto@lncc.br\n{patrick.valduriez@inria.fr\n1arXiv:1805.03141v1  [cs.DC]  8 May 2018\n\nFigure 1: The distribution of a point.\nHowever, errors in signal processing and modeling create some uncertainty, and thus a lack of\naccuracy when identifying phenomenons. Such uncertainty must be carefully quanti\fed. In order\nto understand uncertainty, several simulation runs with di\u000berent input parameters are usually\nconducted, thus generating multiple spatial data sets that can be very big, e.g.hundreds of GB\nor TB. Within multiple spatial data sets, each point in the cube area is associated to a set of\ndi\u000berent observation values in the spatial data sets. The observation values are those observed\nby sensors, or generated from simulation, at a speci\fc point of the spatial area.\nUncertainty quanti\fcation of spatial data is of much importance for geological or seismic\nscientists [19], [24]. It is the process of quantifying the uncertainty error of each point in the\nspatial cube space, which requires computing a Probability Density Function (PDF) of each point\n[13]. The PDF is composed of the distribution type ( e.g. normal, exponential) and necessary\nstatistical parameters ( e.g. the mean and standard deviation values for normal and rate for\nexponential).\nFigure 1 shows that the set of observation values at a point may have four distribution\ntypes, i.e.uniform (a), normal (b), exponential (c), and log-normal (d). The horizontal axis\nrepresents the values (V) and the vertical axis represents the frequency (F). The green bars\nrepresent the frequency of the observation values in value intervals and the red outline represents\nthe calculated PDF. During the calculation of the PDF at a point, there may be some error\nbetween the distribution of the observation values and the calculated PDF. We denote this error\nby PDF error (or error for short in the rest of the paper). In order to precisely \ft a PDF based\non observation values, we need to reduce this error. For instance, the set of observation values\ncorresponding to the QOI at a point obeys a normal distribution shown in Figure 1 (b). The\nmean value of the set of observation values (see Equation 1) may be used as a representative of\nQOI since it has the highest chance to be the QOI. However, the distribution can be di\u000berent\nfrom normal. And analyzing uncertainty just using the mean or standard deviation values is\ndi\u000ecult and imprecise. For instance, if the distribution type of simulated values is exponential\n(see Figure 1 (c)), we should take the value zero (di\u000berent from the mean value of the simulated\nvalues) as the QOI value since it has the highest possibility. Once we have the PDF of a point,\nwe can calculate the QOI value that has the highest possibility, with which we can compute the\nimprecision, i.e.quantify uncertainty, of each spatial data set.\nCalculating the PDF that best \fts the observation values at each point can be time consuming.\nFor instance, one simulation of an area of 10km (distance) * 10km (depth) * 5km (width)\ncorresponds to 2.4 TB data with 10000 measurements at each point [1]. This area contains 6.25\n* 108points. The time to calculate the PDF with consideration of 4 distribution types (normal,\nuniform, exponential and log-normal) can be up to several days or months using a computer\ncluster.\nIn this paper, we propose a new solution to e\u000eciently compute PDFs in parallel by taking\nadvantage of Spark [27], a popular in-memory big data processing framework for computer\nclusters (see [15] for a survey on big data systems). To validate our solution, we use the spatial\n2\n\ndata generated from simulations based on the models from the seismic benchmark of the HPC4e\nproject between Europe and Brazil for oil and gas exploration [1]. This benchmark includes\nmodels for seismic wave propagation. In oil and gas exploration, seismic waves are sent deep into\nthe Earth and allowed to bounce back. Geophysicists record the waves to learn about oil and\ngas reservoirs located beneath Earth's surface.\nThe problem we address is how to e\u000eciently compute PDFs under bounded error constraints.\nIn addition to deploying PDF computation over Spark, we propose three methods to e\u000eciently\ncompute PDFs: data grouping, machine learning (ML) prediction and sampling. Data grouping\nconsists in grouping similar points to compute the PDF. In the original input data, the data\ncorresponding to some points may be the same or very similar to the data corresponding to a\ncommon point. ML prediction uses ML classi\fcation methods to predict the distribution type\nof each point. Sampling method enables to e\u000eciently compute statistical parameters of a region\nby sampling a fraction of the total number of points to reduce the computation space.\nThis paper makes the following contributions:\n\u000fAn architecture to compute PDFs of QOIs in large spatial data sets using Spark.\n\u000fThree new methods to reduce the time of computing PDFs, i.e.data grouping, ML pre-\ndiction and sampling.\n\u000fAn extensive experimental evaluation based on the implementation of the methods in a\nSpark/HDFS cluster and big data sets ranging from 235 GB to 2.4 TB. The experimental\nresults show that our methods scale up very well and reduce the execution time by a factor\nof 33 (in the order of seconds or minutes) compared with a baseline method.\nThe paper is organized as follows. Section 2 introduces some background on Spark. Section\n3 gives the problem de\fnition. Section 4 presents our architecture for computing PDFs with\nSpark, with its main functions. Section 5 presents our solution to compute PDFs in parallel,\nwith three methods, i.e.data grouping, ML prediction and sampling. Section 6 presents our\nexperimental evaluation on di\u000berent computer clusters with di\u000berent data sizes, ranging from\nhundreds of GB to several TB. Section 7 concludes.\n2 Background on Spark\nSpark [27] is an Apache open-source data processing framework. It extends the MapReduce model\n[7] for two important classes of analytics applications: iterative processing (machine learning,\ngraph processing) and interactive data mining (with R, Excel or Python). Compared with\nMapReduce, it improves the ease of use with the Scala language (a functional extension of Java)\nand a rich set of operators (Map, Reduce, Filter, Join, Aggregate, Count, etc.). In Spark, there\nare two types of operations: transformations, which create a new dataset from an existing one (\ne.g.Map and Filter), and actions, which return a value to the user after running a computation\non the dataset ( e.g.Reduce and Count). Spark can be deployed on shared-nothing clusters, i.e.\nclusters of commodity computers with no sharing of either disk or memory among computers.\nIn a Spark cluster, a master node is used to coordinate job execution while worker nodes are in\ncharge of executing the parallel operations.\nSpark provides an important abstraction, called resilient distributed dataset (RDD), which\nis a read-only and fault-tolerant collection of data elements (represented as key-value pairs)\npartitioned across the nodes of a shared-nothing cluster. RDDs can be created from disk-based\nresident data in \fles or intermediate data produced by transformations. They can also be made\nmemory resident for e\u000ecient reuse across parallel operations.\n3\n\nSpark data can be stored in the Hadoop Distributed File System (HDFS) [22], a popular open\nsource \fle system inspired by Google File System [10]. Like GFS, HDFS is a highly scalable,\nfault-tolerant \fle system for shared-nothing clusters. HDFS partitions \fles into large blocks,\nwhich are distributed and replicated on multiple nodes. An HDFS \fle can be represented as a\nSpark RDD and processed in parallel.\nSpark provides a functional-style programming interface with various operations to execute\na user-provided function in parallel on an RDD. We can distinguish between operations without\nshu\u000fing, e.g. Map and Filter, and with shu\u000fing, e.g. Reduce, Aggregate and Join. Shu\u000fing\nis the process of redistributing the data produced by an operation, e.g. Map, so that it gets\npartitioned for the next operation to be done in parallel, e.g.Reduce. This process is complex\nand expensive and requires moving data across cluster nodes.\nIn this paper, we exploit Spark MLlib [2], a scalable machine learning (ML) library that can\nhandle big data [14] in memory by exploiting Spark RDDs and Spark operations. In one method\nwhich we propose, we use ML techniques to classify the distribution types of the observation\nvalues at di\u000berent points in order to reduce useless calculation.\n3 Problem De\fnition\nA spatial data set contains the information to generate an observation value matrix that corre-\nsponds to a three dimensional cube area. As a result of running multiple simulations, multiple\nspatial data sets are produced with a set of observation values at each point. The set of observa-\ntion values at each point enables the computation of their mean value, standard deviation values\nand PDF.\nLet us illustrate the problem with the spatial data generated from simulations based on\nthe models from the seismic benchmark of the HPC4e project [1]. An important parameter\nof the models is wave phase velocity, noted Vpin electromagnetic theory, which is the rate at\nwhich the phase of the wave propagates in space. The models contain 16 layers and each layer is\nassociated to a value of Vp. The top layer delineates the topography and contains the description\ninformation of the other 15 layers. Each of the 15 layers is used to generate the observation values\nof points in a horizontal space of the cube area.\nSince our purpose is to study the uncertainty in the output as a result of the wave propagation\nof the input uncertainty through the model, we assume that the input value of each layer is\nuncertain and obeys a PDF. The distribution type for every four layers are: Normal, Log-\nnormal, Exponential and Uniform. We use a Monte Carlo method to generate di\u000berent sets of\nthe 16 input parameters. For each set of input parameters, we generate a spatial data set using\nthe models as shown in Figure 2.\nThe main problem we address is to e\u000eciently compute PDFs on big spatial data sets, such\nas the seismic simulation data discussed above. Since it takes much time to compute the PDFs\nof the points in the whole cube area, our approach is to divide it into multiple spatial regions\nand compute the PDFs of all the points in a chosen region within a reasonable time. The spatial\nregion is chosen based on some statistical parameters of the region. Thus, the main problem is\nhow to e\u000eciently calculate the PDF of each point in a spatial region, e.g.a horizontal slice in the\ncube area, with a small average error between the PDF and the distribution of the observation\nvalues. In order to choose a region, the mean and standard deviation values and the distribution\ntype of a part of the points in the spatial region should be computed. In addition, the average\nmean value, the average standard deviation value and the percentage of points corresponding to\neach distribution type in all the points of the region can also be computed. We denote these\nstatistical parameters of a spatial region to compute by the features of a spatial region. A related\n4\n\nFigure 2: Data generation from simulation. This process is repeated multiple times in\norder to generate multiple data sets. The values of Vpare di\u000berent at di\u000berent iterations. Type\nrepresents the distribution type.\nsubproblem is how to e\u000eciently calculate the features of a spatial region. In this paper, we take\na horizontal slice as a spatial region.\nLetDSbe a set of spatial data sets, dkbe a spatial data set in DSandNbe the number of\npoints in a region. Each point px;y, wherexandyare spatial dimensions in the slice, has a set\nof valuesV=fv1;v2;:::;v ngwhilevkis the value corresponding to the point px;yindk2DS.\nBased on these notations, we de\fne Equations 1 - 4 and 6, which are based on the formulas in\n[8]. The mean ( \u0016x;y) and standard deviation ( \u001bx;y) values of a point can be calculated according\nto Equations 1 and 2, respectively. And the average mean ( \u0016i) and standard deviation ( \u001bi)\nvalues of Slice ican be calculated according to Equations 3 and 4, respectively. The error ex;y;i\nbetween the PDF Fand the set of observation values Vcan be calculated according to Equation\n5, which compares the probability of the values in di\u000berent intervals in Vand the probability\ncomputed according to the PDF. The intervals are obtained by evenly splitting the space between\nthe maximum value and the minimum value in V.min is the minimum value in V,max is the\nmaximum value in VandLrepresents the number of all considered intervals, which can be\ncon\fgured. Freq krepresents the number of values in Vthat are in the kth interval. The integral\nofPDF (x) computes the probability according to the PDF in the kth interval. Equation 5 is\ninspired by the Kolmogorov-Smirnov Test [16], which tests whether a PDF is adequate for a\ndata set. In addition, we assume that the probability of the values outside the space between\nthe maximum value and the minimum value is negligible for this equation. Then, the average\nerrorEof Sliceican be calculated according to Equation 6.\n\u0016x;y=Pn\ni=1vi\nn(1)\n\u001bx;y=sPn\ni=1(vi\u0000\u0016)2\nn\u00001(2)\n\u0016i=P\npx;y2slice i\u0016x;y\nN(3)\n\u001bi=P\npx;y2slice i\u001bx;y\nN(4)\n5\n\nFigure 3: Architecture for Computing PDFs.\nex;y;i=NX\nk=1jFreq k\nN\u0000Zmin +(max\u0000min )\u0003k\nL\nmin +(max\u0000min )\u0003k\u00001\nLPDF (x)dxj (5)\nE=P\npx;y2slice iex;y;i\nN(6)\nWe can now express the main problem as follows: given a set of spatial data sets DS=\nfd1;d2;:::;d ngcorresponding to the same spatial cube area C=fslice 1;slice 2;:::;slice jg, how\nto e\u000eciently calculate the mean, standard deviation values and the PDF Fat each point in\nslice i2Cwith a small average error Enot higher than a prede\fned average error \". In\naddition, we also need to compute the statistical parameters of slices in order to choose Slice i\nmentioned in the \frst subproblem. Thus, the related subproblem can be expressed as: how to\ne\u000eciently calculate the features of a slice when given the same data sets DS. The features are:\n\u000f\u0016,\u001band distribution type of some points in the slice\n\u000f\u0016i,\u001biand the percentage of points for each distribution type in all the points of the slice\n4 Architecture for Computing PDFs\nIn this section, we describe the architecture for PDF computation (see Figure 3). This architec-\nture has four layers, i.e.infrastructure, basic process to compute PDFs, memory management\nand methods to compute PDFs. The higher layers take advantage of the lower layers' services\nto implement their functionality. The infrastructure layer provides the basic execution environ-\nment, including Spark, HDFS and Network File System (NFS) in a computer cluster. The basic\nprocessing layer provides guiding principles to load the big spatial data and to compute PDFs.\nThe memory management layer allows optimizing the execution of the basic process by caching\ndata and managing sliding windows on big data. The methods to compute PDFs are presented\nin Section 5.\n6\n\nFigure 4: Infrastructure. direpresents the ith spatial data set.\n4.1 Infrastructure with Spark\nFigure 4 illustrates the infrastructure we deploy to process big spatial data. The big spatial data\nis produced by simulation application programs and stored in NFS [21], a shared-disk \fle system\nthat is popular in scienti\fc applications. Spark and HDFS are deployed over the nodes of the\ncomputer cluster. The intermediate data produced during PDF computation and the output\ndata are stored in HDFS, which provides persistence and fault-tolerance.\nKeeping the input spatial data in NFS allows us to maximize the use of the cluster resources\n(disk, memory and CPU), which can be fully dedicated for PDF computation. With the input\ndata stored in NFS, the NFS server is outside the Spark/HDFS cluster and takes care of \fle\nmanagement services, including transferring the data that is read to the cluster nodes. An\nalternative solution would have been to store the input data in HDFS, which would lead to have\nHDFS tasks competing with Spark tasks for resource usage on the same cluster nodes. We did try\nthis solution and it is much less e\u000ecient in terms of data transfer between cluste nodes. This is\nbecause HDFS is more complex and does more work due to fault-tolerance and data replication.\nWe use Spark as our execution environment for computing PDFs. We developed a program\nwritten in Scala, which realizes the functionality of di\u000berent methods to compute PDFs. Once\nthe method to compute PDFs is chosen, the Scala program is executed as a job within Spark.\n4.2 Principles for the Basic Processing of PDFs\nThe basic processing of PDFs consists of data loading, from NFS to Spark RDDs, followed by\nPDF computation using Spark. The data loading process treats the data corresponding to a slice\nand pre-processes it, i.e.calculates statistical parameters of observation values of each point and\nrelates the observation values of each point to an identi\fcation of the point. The identi\fcation\nof each point is an integer value which represents the location of the point in the cube area.\nThen, the PDF computation process groups the data and calculates the PDFs and errors of all\nthe points in a slice based on the pre-processed data. To make the basic processing of PDFs\ne\u000ecient, we use the following guiding principles.\n1.Parallel data loading. To perform data loading in parallel, we store the identi\fcations\nof points in an RDD, which is evenly distributed on multiple cluster nodes. For each point\nin a node, all the corresponding values in di\u000berent spatial data sets are retrieved from\nNFS. At the same time, the mean and standard deviation values are calculated. Then, the\nidenti\fcation of the point is stored as a key of the point. The mean and standard deviation\nvalues and the observations values are stored as the value of the point. The key and value\n7\n\nof the point are stored as a key-value pair in the RDD. This loading process is realized by\na Map operation in Spark, which is fully parallel.\n2.Data grouping. In order to avoid repeating the PDF computation for the same or\nsimilar sets of observation values, we group the data of di\u000berent points that shares similar\nstatistical features, e.g. the mean and standard values. Then, a representative point of\nthe group is chosen. The PDF of the representative point is taken to represent all the\npoints in the group. Thus, the PDF computation of all the points in the group is reduced\nto the computation of the representative point. The grouping can be realized using an\nAggregation operation in Spark. However, the shu\u000fing process in this operation may take\nmuch time. When it takes too much time to group data, we can ignore this principle.\nAfter data grouping, the set of the identi\fcations of the points in the group is stored as a\npart of the value in the key-value pair. For each representative point, the key represents\nthe identi\fcation while the value contains the mean and standard deviation values and the\nobservation values.\n3.Parallel processing of PDFs. The PDF of each point (or representative point) is\ncomputed based on its observation values. This process is also realized in a Map operation,\nwhich distributes the key-value pairs in RDD of di\u000berent points to di\u000berent nodes. Once the\nPDF of a point is calculated, the error between the observation values and the calculated\nPDF is computed in the same node. If multiple PDFs of di\u000berent distribution types are\ncomputed, the PDF with the minimum error will be chosen as the PDF of the point.\nAfter the parallel processing of PDFs, the key remains the identi\fcation of each point\n(representative point) while the PDF is stored as a part of the value in the key-value pair\nof the RDD. In addition, since the mean and standard deviation values and the observation\nvalues are no longer useful, the corresponding data is removed from the value in the key-\nvalue pair. Finally, the PDF of each point is persisted in a \fle or database system for\nfuture use. In addition, an average error of the PDF of the points in the slice is calculated\nand shown as the result of executing the Scala program.\n4.Sliding window. Since aslice can have many points that won't \ft in memory, we use a\nsliding window over the slice during data loading, data grouping and parallel processing\nof PDFs. A window represents a set of points to process, which correspond to several\ncontinuous lines in the slice to process. Any two windows have no intersection. After the\nprocessing of one window, the process of the next window begins until the end of the slice.\nOnce the size of the window is con\fgured, it stays the same during execution. The size of\nthe window has strong impact on execution and must be chosen carefully (see details in\nSection 4.3.2).\n5.Use of external programs. In the parallel data loading and parallel processing of\nPDFs, we use external programs, which do the speci\fc loading of the points. Since some\nJava functions, e.g.skipBytes (Skips and discards a speci\fed number of bytes in the\ncurrent \fle input stream), may not work correctly during the parallel execution of a Map\noperation in Spark [11], we call an external Java program in the Map operation to retrieve\nobservation values of a point from di\u000berent spatial data sets and pre-process values. Since\nthe PDF computation is implemented by an external program (in R), we call it within the\nMap operation for the parallel processing of PDFs. Finally, the output data of the external\nprogram is transformed to key-value pairs and stored in RDDs by the same Map operation,\nwhich executes the external program.\n8\n\n4.3 Memory Management\nIn order to e\u000eciently compute PDFs, we use two memory management techniques to optimize\nthe calculation of PDFs over big data: data caching and window size adjustment.\n4.3.1 Data Caching\nWe use data caching, i.e. keeping data in main memory, to reduce disk accesses.To identify\nwhich data to cache, we distinguish between four kinds of data: input data, instruction data,\nintermediate data and output data. The input data is the original data to be processed, i.e.the\nbig spatial data sets. The instruction data is the data corresponding to the external programs,\ne.g.Java or R programs used in data loading and PDF computation. The intermediate data is\nthe data generated by the data loading process or the execution of external programs, and used\nby subsequent execution. The output data is the \fnal data generated by the PDF computation.\nWe use a simple caching strategy. We do not cache input data because it can be very big\nand read only once. We only cache instruction data and intermediate data, which are accessed\nmuch during execution. However, intermediate data that is not used in subsequent operations is\nremoved from main memory. The output data is written to memory \frst and then persisted.\nDuring execution, some intermediate data that is stored in Spark RDDs can be cached using\nthe SparkCache operation, which stores RDD data in main memory. However, the instruction\ndata of external programs and the intermediate data directly generated by executing these ex-\nternal programs are outside of RDDs. We cache this data in temporary \fles in memory, using a\nmemory-based \fle system [23]. Then, the information in the cached \fles is retrieved and stored\nas intermediate data in RDDs, which can be again cached using the Cache operation of Spark.\n4.3.2 Window Size Adjustment\nThe size of the sliding window is critical for e\u000ecient PDF computation. When it is too small,\nthe degree of parallel execution among multiple cluster nodes is low. Increasing the window size\nincreases the degree of parallelism, but may introduce some overhead in terms of data transfers\namong nodes or management of concurrent tasks. Thus, it is important to \fnd an optimal\nwindow size in order to make a good trade-o\u000b between the degree of parallelism and overhead.\nTo \fnd the optimal window size, we distinguish between the data loading process and the PDF\ncomputation process, which have di\u000berent data access patterns. In data loading, the processing\nof each point can be done independently by a Map operation in parallel. Thus, the overhead\nof data transfers and concurrent task management with a big window size is small. Thus, we\ncan choose a window size that ensures that there is enough work to do for each node and that\nmultiple nodes can be used. For instance, consider a computer cluster with nnodes, each node\nwithcCPU cores. Assuming that the data loading for each point can occupy a CPU core, we\ncan choose a maximum window size corresponding to n*cpoints. If the number of points is less\nthann*c, we choose the maximum number of points as the window size.\nDuring PDF computation, the overhead of having a big window can be high since the process-\ning of di\u000berent points are not independent especially when we use data grouping. For instance,\nwhen the size of window is bigger, the data of more points is present at node. In order to group\ndata, the data of each point need to be compared with the data of more points, which takes\nmore time. In addition, there is much more data transferred among di\u000berent nodes in the data\nshu\u000fing process of data grouping. Furthermore, since the PDF computation takes much time,\nthe management of concurrent tasks within each node also increases execution time for a big\nwindow. As a result, the overhead of having a big window becomes high for PDF computation.\n9\n\nTo \fnd an optimal window size, we test the Scala program on a small workload (with a\nsmall number of points) with di\u000berent window sizes, and then use the optimal size for the PDF\ncomputation of all the points in the slice.\n5 Methods to Compute PDFs\nIn this section, we present our methods to compute PDFs e\u000eciently. First, we introduce a\nbaseline method, which will be useful for comparison. Then, we propose two methods, i.e.data\ngrouping and ML prediction, to compute PDFs, which addresses the main problem de\fned in\nSection 3. Finally, we propose a sampling method to calculate the features of a slice, which\naddresses the related problem.\n5.1 Baseline Method\nThe baseline method computes the PDF of each point in a slice as follows (see Algorithm 1).\nLine 2 loads the spatial data sets and calculates the mean and standard deviation values of each\npoint by using Algorithm 2. Lines 3 - 13 compute the PDF for all the points in the ith slice.\nLine 5 gets a window in the slice. Line 6 selects all the points in the window to process. For each\npoint in the window, Line 8 computes the PDF based on the observation values and the error\nbetween the PDF and the observation values. This can be achieved by executing an R program.\nThe loop of Lines 7-10 can be executed in parallel using the Map operation in Spark. The\nComputePDF &Error function is realized by Algorithm 3. The data is persisted in the storage\nresources (Line 11) and the average error Eis calculated (Line 14). Lines 3-14 correspond to the\nPDF computation process.\nAlgorithm 1 PDF computation\nInput:DS: a set of spatial data sets corresponding to a spatial cube area; i: theith slice to\nanalyze\nOutput:PDF : the PDF of all the points in the ith slice of the cube area; E: the average error\nbetween the PDF and the observation values of all the points in the ith slice\n1:PDF ;\n2:RawData loadData (DS;i )\n3:while not all points in slice iare processed do\n4:PDF ;\n5:window GetNextWindow (slice i)\n6:Points Select (window;RawData )\n7: for eachp2Points do\n8: (pdf;error ) ComputePDF &Error (p;RawData )\n9:pdfs pdf[wsf\n10: end for\n11: persist(pdf)\n12:PDF PDF[pdfs\n13:end while\n14:E Average (error )\nend\nAlgorithm 2 loads and preprocesses the spatial data. Line 4 chooses a window to load the\ndata. Then, for each point in the window, the data in each data set of DSis loaded (Lines\n10\n\n6-10). This process is realized in a Map function in Spark. A Java program is called in the\nMap function to read the data at a speci\fc position instead of loading all the data. Then, the\nmean and standard deviation values are calculated (Lines 11 and 12). Finally, the loaded data\nis cached in memory in a Spark RDD (Line 16).\nAlgorithm 2 Data loading\nInput:DS: a set of data sets corresponding to a spatial cube area; i: theith slice to analyze\nOutput:RawData : mean, standard deviation and the original data set of each point in the\ncube area\n1:RawData ;\n2:whileRawData does not contain all the points in slice ido\n3:windowData ;\n4:window GetNextWindow (slice i)\n5: for eachp2window do\n6:rd ;\n7: for eachds2DSdo\n8: data GetData (ds;p;i )\n9: rd data[rd\n10: end for\n11:\u0016 ComputeMean (rd)\n12:\u001b ComputeStd (rd)\n13:rd \u0016[\u001b[rd\n14:windowData rd[windowData\n15: end for\n16:Cache (windowData )\n17:RawData windowData[RawData\n18:end while\nend\nAlgorithm 3 computes the PDF of a point with the smallest error in a set of candidate distri-\nbution types. Line 3 calculates the statistical parameters of PDFs based on di\u000berent distribution\ntypes in a set of distribution candidates Types . For instance, the parameters for normal are mean\nand standard deviation values while the parameter for exponential is rate. The more types are\nconsidered in Types , the longer the execution time of Algorithm 3 is. Then, the corresponding\nerror of the PDF based on type andparameters are calculated using Equation 5. Finally, the\nPDF that incurs the smallest error is chosen (Line 7).\nWe exploit Algorithms 1 and 2 in both the data grouping and ML prediction methods.\nHowever, the Select andComputePDF &Error functions are di\u000berent in di\u000berent methods.\n5.2 Data Grouping\nSome points may have the same distribution of observation values. Thus, using the data grouping\nmethod, we can execute the PDF computation process only once and use the result to represent\nall the corresponding points. In this method, the Select function in Algorithm 1 has two steps.\nFirst, the points with exactly the same mean and standard deviation values are aggregated into\na group. The grouping can be realized by the aggregation operation in Spark. Then, one point in\neach group is selected to represent the group of points. Then, the data corresponding to selected\npoints are processed to compute the PDFs.\n11\n\nAlgorithm 3 PDF computing\nInput:d: a set of observation values for a point; Types : a set of distribution types\nOutput:PDF : the PDF of the point; error : the error between the PDF de\fned by the distri-\nbution type and the statistical parameters and the observation values of the point\n1:results ;\n2:for eachtype2Types do\n3:parameters fitDistribution (d;type )\n4:pError CalculateError (type;parameters;d ) .According to Equation 5\n5:results f(type;parameter;pError )g[results\n6:end for\n7:(PDF;error ) GetSmallestError (results )\nend\nIn some cases, although some points may share the same PDF, the observation values of the\npoints are slightly di\u000berent. And the mean and standard values may di\u000berent by a very small\n\ructuation. In this case, we can cluster the points that have similar mean and standard deviation\nvalues with an acceptable error.\nWhen the number of nodes in the cluster is small and the window size is small, there is few\ndata to be transferred for the grouping. And the grouping method can avoid repeated execution\non the same set of observation values. However, when the number of nodes is high or the window\nsize is big, there is much more data to be transferred among di\u000berent nodes, which may take\nmuch time. In some situations, the time to transfer the data may be longer than the time to\nrun the repeated execution. In addition, if a point corresponds to big amounts of data (many\nobservation values), even though the window size is small, the shu\u000fing process of data grouping\nmay also take much time. In this case, the data grouping method is not e\u000ecient.\n5.2.1 Reuse Optimization\nWhen there are points of the same mean and standard deviation values in di\u000berent windows, we\ncan reuse the existing calculated results in order to avoid new executions. Thus, we propose the\nreuse optimization method, which not only aggregates the data to groups but also checks if there\nare already existing results, i.e.the PDFs for the point of the same mean and standard deviation\nvalues, generated from the previous execution of processed windows. We expect this method to\nbe better than data grouping only. However, it may take time to store all the calculated results\nand to search existing PDFs from a large list of previously generated results. This extra time\nmay be longer than the reduced time, i.e.the time to compute the PDF on the data set (the\nexecution time of Algorithm 3).\n5.3 ML Prediction\nIn this section, we propose an ML prediction method based on a decision tree to compute PDFs.\nDecision tree classi\fer [20] is a typical ML technique to classify an object into di\u000berent categories.\nThe basic idea is to break up a complex decision into a union of several simpler decisions, hoping\nthe \fnal solution obtained this way would resemble the intended desired solution. The decision\ntree can be described as a tree selected from a lattice [3]. A tree G= (V;E) consists of a \fnite,\nnonempty set of nodes Vand a set of edges E. A path in a tree is a sequence of edges. If there\nis a path from vtow(vw), thenvis a proper ancestor of w,wis a proper descendant of vand\nthe path contains an entry edge for wand an out edge for v.\n12\n\nFigure 5: A decision tree to choose the distribution type for a set of data.\nA tree has only one root node that does not have any entry edge and all the other nodes\nhave only one entry edge. Each node has only one path from the root. The depth of the graph\nin a tree is the length of the largest path from the root to a leaf. A leaf is the node that has no\nproper descendant. The data to be used to generate a decision tree is training data. In order to\ngenerate a decision tree, the training data can be split into di\u000berent data sets (bins) according\nto di\u000berent features and each data set is a unit to be processed. The maximum number of bins\nis de\fned in order to reduce the time to generate a decision tree. Figure 5 shows a decision tree\nof depth 3 to choose a distribution type for a data set. However, in this paper, we take some\nstatistical features as parameters to choose a distribution as explained below.\nIn Algorithm 3, the execution of Lines 3-5 is repeated several times (the number of distribution\ntype candidates), which is very ine\u000ecient. We assume that we can learn the correlation among\nstatistical features, e.g.mean and standard deviation values, and the distribution type. Then,\nwe can directly predict the distribution type based on the relationship and use the predicted\ndistribution type to execute Lines 3-5 in Algorithm 3 once for each point.\nWe assume that we have some previously generated output data, which contains the results\nof several points, i.e. the mean and standard deviation values and the type of distribution.\nBefore execution of our method, we can generate a ML model (that correlates between statistical\nfeatures and distribution types), i.e.decision tree, based on the previous existing data. Then,\nwe use Algorithm 4 to replace Algorithm 3.\nWhen the points in the current slice have the same correlation between the statistical features\nand the distribution types as that in the previously generated output data, we can use ML\nprediction. Generally, the points in di\u000berent slices but corresponding to the same spatial data\nset have the same correlation. Thus, we can use the output data generated based on some points\nin one slice, e.g.Slice 0, to generate decision tree model and use the model to calculate PDFs of\nthe points in other slices, e.g.Slice 201.\nIn addition, since the ML approach optimizes the ComputePDF &Error function, it can be\ncombined with other methods. Thus, we call the pure adoption of ML: ML or baseline + ML; the\ncombination of data grouping and ML: data grouping + ML or grouping + ML; the combination\nof reuse and ML: reuse + ML. Grouping + ML \frst uses the data grouping methods to group\npoints and then use ML prediction to calculate PDF and error for each representative point.\n13\n\nReuse + ML \frst groups the points using the data grouping method and then searches the PDF\ncorresponding to the set of the mean and standard deviation values of each representative point\nin the results generated by previous execution. If there is no results for the set of mean and\nstandard deviation values, it uses the ML method to calculate the PDF and the error.\nAlgorithm 4 PDF computing based on ML prediction\nInput:d: a vector of observation values for a point; model : the decision tree; \u0016: the mean value\nofd;\u001b: the standard deviation value of d\nOutput:T: the distribution type of the point; P: the parameters of the distribution; error : the\nerror between the PDF de\fned by the distribution type and the parameters of the distribution\nand the real distribution of data in d\n1:T predict (model;\u0016;\u001b )\n2:P fitDistribution (d;type )\n3:error CalculateError (type;parameter;d ) .According to Equation 5\nend\n5.3.1 ML Model Generation\nIn order to generate the decision tree model, there are some hyper-meters to determine, e.g.\nthe depth of the tree ( depth ) and the maximum number of bins ( maxBins ). In order to tune\nthe hyper-meters, we randomly split the previously generated output data into two data sets,\ni.e.training set and validation set. We use the training set to train the model with di\u000berent\ncombinations of hyper-meters. Then, we test the generated models on the validation set in order\nto choose an optimal combination of hyper-meters. We take the wrong prediction rate in the\nvalidation set as the model error while tuning the hyper-meters. The model error represents the\npreciseness of the decision tree model.\nThe model error decreases at the beginning and then increases, because of over\ftting [26], as\nthe values of depth andmaxBins increase. While the time to train the model becomes longer\nwhen the values of depth andmaxBins increase. We choose the minimum values of depth and\nmaxBins , from which the error does not decrease when they ( depth ormaxBin ) increase. The\nprocess to tune the hyper-meters can take much time. We assume that the points in di\u000berent\nslices have the same correlation between the statistical features and distribution types. We also\nassume that the hyper-meters can be shared to train the models based on di\u000berent previously\ngenerated output data in order to avoid tuning the hyper-meters and to reduce the time to\ntrain the decision tree model. Thus, the chosen values of depth andmaxBins are used as \fxed\nhyper-meters to generate the decision tree model for di\u000berent previously generated output data.\nWithin the previously generated output data, each point has mean and standard deviation\nvalues and the distribution type. Since we have \fxed hyper-meters, we do not need to use a\nvalidation data set to tune the hyper-meters. Thus, in order to generate the model, we randomly\npartition the previously generated output data set into two parts, i.e.training set and test set.\nIn the training processing, we train the model using the training set. The input of the trained\nmodel is the mean and standard values and the output is a predicted distribution type. After\ngenerating the model, we take the wrong prediction rate in the test set as the model error while\ntraining models. During the PDF computation process, the generated model is broadcast to all\nthe nodes, which reduces communication cost. .\nThere are scenarios in which the mean and standard deviation share the same values, respec-\ntively, but the distribution type is di\u000berent. In this situation, we can take into consideration\n14\n\nother normalized moments1,e.g.3th, 4th etc. However, it may take additional time to calculate\nother normalized moments. Thus, we only consider the mean and standard deviations. We will\nexperiment with data sets where the points of the same set of mean and standard deviation\nvalues have the same distribution types.\n5.4 Sampling\nIn order to choose a slice to compute PDFs, we need to compute the features of a slice very\nquickly. We propose a sampling method (see Algorithm 5) that samples the points and uses\nML prediction to generate the distribution type of each point. This method does not run the\nstatistical calculation of the each point in the ComputePDF &Error function in Algorithm 1 in\norder to reduce execution time.\nIn Algorithm 5, Line 2 samples the points in slice ibased on a prede\fned sampling rate. A\nsampling rate represents the ratio between the selected points from the sampling process and all\nthe original points. Lines 4-14 implement the process that loads the data from the data sets and\ncalculates the mean and standard deviation values for each double sampled point. Line 15 groups\nthe data as explained in Section 5.2. When the number of nodes in the cluster is high, we can\nremove Line 15 in order to reduce the time of shu\u000fing. Lines 17 -20 calculate the distribution\ntype of each double sampled point based on a decision tree (see Section 5.3.1 for details). Lines\n22-26 calculate the \fnal results, i.e.the average mean value, the average standard deviation and\nthe percentage of di\u000berent distribution types. Lines 17 - 26 correspond to the PDF computation\nprocess.\nWe can randomly sample the points. The random sampling method takes very time while the\nselected points may contain little repeated information. We could also use a k-means clustering\nalgorithm [18] and choose the point that is the closest to the center of each cluster as double\nsampled points. The double sampled data generated by k-means contains diverse information,\nwhich does not help much to choose a slice (see details in Section 6.2.3). Furthermore, k-means\nmay take much time to converge. Therefore, instead of k-means, we use random sampling in\nAlgorithm 5.\n6 Experimental Evaluation\nIn this section, we evaluate and compare the di\u000berent methods presented in Section 5 for di\u000berent\ndata sets and cluster sizes. To ease reading, we call each method by a name as: Baseline,\nGrouping, Reuse, ML and Sampling. First, we introduce the experimental setup, with two\ndi\u000berent computer clusters (a small one and a big one). Then, we perform experiments with a\n235 GB data set. Finally, we perform experiments with big data sets (1.9 TB and 2.4 TB).\n6.1 Experimental Setup\nIn this section, we present the di\u000berent spatial data sets, the candidate distribution types (see\nAlgorithms 3 and 5) and the cluster testbeds, which we use in our various experiments.\nTo generate the spatial data, we use the UQlab framework [17] to produce 16 values as the\ninput parameters, i.e.Vp, of the models from the seismic benchmark of the HPC4e project\n1Thenth moment is de\fned as:\n\u0016n=mX\ni=1(xi\u0000\u0016)n(7)\nwheremis the number of values in the data set, xiis theith value in the data set and \u0016is the mean value of the\ndata set.\n15\n\nAlgorithm 5 Sampling process\nInput:DS: a set of data sets produced by simulations; i: theith slice to analyze; model : the\ndecision tree corresponding to the relationship between the mean and standard value and\nthe distribution type; Types : a set of distribution types; rate: the sampling rate\nOutput:\u0016: the average mean value of the slice; \u001b: the average standard deviation of the slice;\nTypesPercentage : the percentage of distribution types of the points in the slice\n1:SF ;\n2:Points Sample (slice i;rate )\n3:RawData ;\n4:for eachp2points do\n5:rd ;\n6: for eachds2DSdo\n7:data GetData (ds;p;i )\n8:rd data[rd\n9: end for\n10:\u0016 ComputeMean (rd)\n11:\u001b ComputeStd (rd)\n12:rd \u0016[\u001b[rd\n13:RawData rd[RawData\n14:end for\n15:Points selectByGrouping (RawData )\n16:allTypes ;\n17:for eachp2Points do\n18:type predict (model;RawData )\n19:allTypes type[allTypes\n20:end for\n21:TypesPercentage  ;\n22:for eachtype2Types do\n23:pct calculatePercentage (type;allTypes )\n24:TypesPercentage  pct[TypesPercentage\n25:end for\n26:(\u0016;\u001b) averageCalculation (RawData )\nend\n16\n\n[1]. The input parameters obey four distribution types, i.e.normal, exponential, uniform and\nlog-normal. In each simulation, we generate a set of the input parameters according to the\nPDF of each layer and generate a spatial data set by using the set of the input parameters and\nthe models. We run the simulation multiple times and generate three sets of spatial data sets,\ndenoted by Set1,Set2 andSet3. The simulation is repeated 1000 times to generate Set1.Set1\ncontains 1000 \fles, each of which is a spatial data set and has 235 GB. In Set1, the dimension of\nthe cube area is 251 * 501 * 501, i.e.501 slices, each slice has 501 lines and each line is composed\nof 251 points. To generate Set2, we run the simulation 1000 times while the dimension is 501\n* 1001 * 1001. Set2 has 1.9 TB. A point is associated to 1000 observation values in both Set1\nandSet2. Finally, we run the simulation 10000 times with the dimension of 251 * 501 * 501\nin order to generate Set3, which has 2.4 TB. In this data set, a point is associated to 10000\nobservation values. We use the same slice (Slice 201 because it has interesting information) in\nall the experiments. We consider two sets of candidate distribution types Types , introduced\nin Algorithms 3 and 5. The \frst set is the set of distribution types of the input parameters,\ni.e.normal, exponential, uniform and log-normal, since we assume that the distribution types\nof di\u000berent points are within those of the input parameters of the simulation. In addition, we\nassume that the distribution types may belong to other types beyond the scope of the distribution\ntypes of the input parameters because of non-linear relationship between the input parameters\nand the values of each point of the spatial cube area. With this assumption, we have a second\ntype of candidate distribution types, i.e. normal, exponential, uniform, log-normal, Cauchy,\ngamma, geometric, logistic, Student's t and Weibull. We call the \frst set 4 \u0000types and the\nsecond 10\u0000types .\nWe use two cluster testbeds, each with NFS, HDFS and Spark deployed. The \frst one, which\nwe call LNCC cluster, is a cluster located at LNCC with 6 nodes, each having 32 CPU cores\nand 94 GB memory. The second one is a cluster of Grid5000, which we call G5k cluster, with 64\nnodes, each having 16 CPU cores and 131 GB of RAM storage.\n6.2 Experiments on a 235 GB Data Set\nIn this section, we compare the di\u000berent methods based on the spatial data set Set1 of 235\nGB. First, we compare the performance of di\u000berent methods using the LNCC cluster: Baseline,\nGrouping, Reuse and the combination of these methods with ML. Then, we study the scalability\nof di\u000berent methods using the G5k cluster. Finally, we study the performance of Sampling.\nWe take the relationship between the set of mean and standard deviation values and the\ndistribution types of 25000 points in Slice 0 in the previously generated output data of Set1 to\ntune the hyper-meters of the decision tree. The hyper-meters are tuned at the very beginning,\nwhich takes 18 minutes for 4 \u0000types and 22 minutes for 10 \u0000types using Spark on a workstation\nwith 8 CPU cores and 32 GB of RAM.\nWe take advantage of the previously generated output data of Set1 to generate the decision\ntree model for the experiments of this section. The model error is 0.03 for 4 \u0000types , and 0.08 for\n10\u0000types . Although the model error of 10 \u0000types is bigger than that of 4 \u0000types , the average\nerrorEin Algorithm 1 is smaller. The time to train the model ranges from 1 to 20 seconds,\nwhich is negligible compared with the execution time of the whole PDF computation process.\n6.2.1 Performance Comparisons\nIn this section, we compare the performance of di\u000berent methods, i.e.Baseline, Grouping, Reuse,\nand the combination with ML, using the LNCC cluster. First, we execute the Scala program\n(for computing PDFs with di\u000berent methods) on a small workload (6 lines and 3006 points).\n17\n\n 0 100 200 300 400 500 600\nBaseline Grouping Reuse4-types\n10-types\n4-types + ML\n10-types + MLFigure 6: Execution time for PDF computation on a small workload (6 lines and 3006\npoints) with 235 GB input data. The time unit is second.\n 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18\n4-types 10-typesNoML\nWithML\nFigure 7: Error in PDF computation. NoML represents the error of di\u000berent methods\nwithout adoption of ML, i.e.Baseline, Grouping and Reuse with 4 \u0000types or 10\u0000types (see\nFigure 6). WithML represents the combinations of methods with ML, i.e. Baseline + ML,\nGrouping + ML, Reuse + ML with 4 \u0000types or 10\u0000types (see Figure 6).\nSecond, we compare the performance of di\u000berent methods for di\u000berent window sizes. Finally, we\ncompare the performance of di\u000berent methods with the tuned window size for the whole slice.\nExperiments with Small Workload In this section, we use a small workload of 6 lines, i.e.\n3006 points, to compare the performance of di\u000berent methods. The execution time for PDF\ncomputation is shown in Figure 6 and the error is shown in Figure 7.\nWe execute the program for the points of the \frst 6 lines in Slice 201 using Baseline, Grouping,\nReuse, ML and the combination of these methods with ML. We take 3 lines as a window for\nPDF computation and we process the data of the points in two windows. The execution time\nfor data loading (see Algorithm 2) is 67s, which is the same for all the methods since we use the\nsame algorithm.\nFigure 6 shows the good performance of our methods: Grouping (without Reuse), Reuse and\nML. The execution time of Baseline with 10 \u0000types is much longer than that with 4 \u0000types .\nThis is expected since the complexity of Algorithm 3 is O(n), withndistribution types, and\n18\n\n 4 6 8 10 12 14 16 18 20 22\n12345678910202325273050Execution time (s) per line\nNumber of lines in a windowFigure 8: Average execution time per line for PDF computation with two windows.\nthe execution time increases with n. However, with ML, the execution time is signi\fcantly\nreduced (46% for 4 \u0000types and 78% for 10\u0000types ). If we use Grouping without ML or Reuse,\nthe execution time is also reduced a lot (69% for 4 \u0000types and 72% for 10\u0000types ). This\nshows that there are many points that obey the same distribution and have the same mean and\nstandard deviation values. In addition, since the data size corresponding to a point is small,\ni.e.1000 observation values, the shu\u000fing time for computing the aggregation function is short.\nThus, Grouping outperforms Baseline much. When we couple Grouping and ML, the advantage\nbecomes more obvious. The combined method is 88% and 95% better compared with the Baseline\nfor 4\u0000types and 10\u0000types ,i.e.the combined method is up to more than 17 times better than\nBaseline. Reuse is slightly worse than Grouping, but it is still much better than Baseline. This\nis expected since it takes more time to search for the existing results than to compute PDFs.\nThe di\u000berence between Grouping and Reuse is small when the workload is small but it can be\nsigni\fcant for bigger workloads.\nFigure 7 shows the average error ( Ein Equation 6) corresponding to di\u000berent methods. In\nFigure 7, we distinguish between the methods that do not use ML, i.e.Baseline, Grouping and\nReuse, which we call NoML, and those that do exploit ML, i.e.Baseline with ML, Grouping\nwith ML and Reuse with ML, which we call WithML. The methods NoML and WithML have\nthe same error for the same distribution type set.\nThe \fgure shows that 10 \u0000types does not reduce much the average error for Baseline (up to\n0.0013). However, PDF computation may take more time when we consider 10 \u0000types as shown\nin Figure 6. The average error is higher for WithML than NoML. The di\u000berence is small (up to\n0.017) but WithML can reduce much the execution time of the PDF computation. In addition,\nFigure 7 shows that 10 \u0000types leads to a smaller average error, even though the model error\nis higher for WithML. This is reasonable since there are some types that are very di\u000ecult to\ndistinguish in 10\u0000types but the wrong classi\fcation of the distribution types does not increase\nmuch the average error. In addition, with more distribution candidate types, the decision tree\nproduces a better classi\fcation.\nWindow Size Adjustment The window size is critical for the PDF computation with Group-\ning. To adjust the window size, we conduct the following experiment. We execute the Scala\nprogram with Grouping (with 4 \u0000types and without ML prediction) for two windows while each\nwindow is composed of di\u000berent numbers of lines. Then, we choose a window that corresponds\nto the shortest average execution time of each line. Figure 8 shows the average execution time of\n19\n\nFigure 9: Average execution time per line for PDF computation with two windows.\nWith 4\u0000types (4) and 10\u0000types (10).\nPDF computation for di\u000berent window sizes using Grouping. As shown in Figure 8, the average\nexecution time of each line decreases for larger window sizes. This is reasonable because when\nthe number of lines increases, more points are aggregated to each group so that more redundant\ncalculations are avoided. When the window size is 25 lines, the average execution time of each\nline is minimal. From that point on, when the window size increases, the execution time also\nincreases. This is expected since when the number of lines in a window increases, the time to\nshu\u000fe data among di\u000berent nodes increases more than the reduced time by avoiding redundant\ncalculations. As a result, the average execution time of each line becomes more signi\fcant. How-\never, the average execution time of data loading stays the same for di\u000berent window sizes, i.e.\nabout 12s per line.\nWe measure the execution time of PDF computation for di\u000berent window sizes using other\nmethods. In Figure 9, we can see that the window size of 25 is the optimal size for the other\nmethods. In addition, the execution time of PDF computation is almost the same for di\u000berent\ncombinations of methods: Grouping plus ML and Reuse plus ML both with 4 \u0000types or 10\u0000types .\nBaseline always corresponds to longer execution times of PDF computation. Compared with\nBaseline, Grouping, Reuse and ML can reduce the execution time up to 91% (more than 10\ntimes faster) and 84% (more than 6 times faster). In addition, the combination of Grouping\nand ML can be up to 97% (more than 33 times) faster. This shows the obvious performance\nadvantage of our methods in computing PDFs.\nExecution of One Slice In this section, we compare the performance of di\u000berent methods\nto execute the program for the points of the whole slice, i.e.Slice 201. We take 25 lines as the\nwindow size for PDF computation and execute the program with di\u000berent methods the whole\nslice, i.e.11 windows of points in Slice 201. The execution time of data loading is the same for\nthe di\u000berent methods, i.e.4098s. The average execution time of each line is longer than that\nof the small workload since we cache all the data in memory during the execution of di\u000berent\nmethods and the time to store data increases as the amount of cached data grows. The execution\ntime of PDF computation is shown in Figure 10 and the error is shown in 11.\n20\n\n 0 100 200 300 400 500 600 700 800 900\nBaseline Grouping Reuse4-types\n10-types\n4-types + ML\n10-types + MLFigure 10: Execution time of PDF computation of Slice 201 with di\u000berent methods\n(235 GB input data). The time unit is minute. The window size is 25 lines.\n 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14\n4-types 10-typesNoML\nWithML\nFigure 11: Error in PDF computation. NoML stands for Baseline, Grouping and Reuse plus\n4\u0000type and 10\u0000types . WithML stands for Baseline + ML, Grouping + ML, Reuse + ML\n4\u0000type and 10\u0000types .\nFigure 10 shows that our proposed methods, i.e.Grouping, Reuse and ML, always outperform\nBaseline for both 4 \u0000types and 10\u0000types . Grouping can reduce the execution time up to 92%\n(more than 10 times) and ML up to 78% (more than 3 times faster). The performance of Reuse\nis better than that of Baseline when we do not combine ML and the advantage can be up to\n70% (more than 2 times faster). However, the performance of the combination of Reuse and ML\ncan be worse than the combination of Baseline and ML. This is possible when it takes too much\ntime to search for the existing results. The combination of Grouping and ML can reduce the\nexecution time up to 97% (more than 27 times faster) compared with Baseline.\nFigure 11 shows the error of PDF computation. The error is smaller than that of the small\nworkload. The error of NoML is still smaller than that of WithML. The error for 4 \u0000types is\nalmost the same as that for 10 \u0000types . But the error for 10 \u0000types using ML is smaller that\nfor 4\u0000types . This is similar to what was observed when executing the small workload. In\naddition, the error for 10 \u0000types with ML is even smaller than the error for 4 \u0000types without\nML. Although there is very small di\u000berence (up to 0.0016) of error between Baseline and ML,\nthe execution time of the PDF computation process is largely reduce by ML.\n21\n\n 300 400 500 600 700 800 900\n10 20 30 40 50Execution time (s)\nNumber of nodesFigure 12: Execution time for data loading with di\u000berent numbers of nodes.\n 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000\n10 20 30 40 50 60Execution time (s)\nNumber of nodesBaseline(4)\nBaseline(10)\nGrouping(4)\nGrouping(10)\nBaseline+ML(10)\nGrouping+ML(10)\nFigure 13: Execution time of PDF computation with di\u000berent numbers of nodes.\n6.2.2 Scalability Comparisons\nIn this section, we study the scalability of Baseline and the methods that have good performance,\ni.e.Grouping, ML and Grouping + ML. We use the G5k cluster with di\u000berent numbers of nodes\nfrom 10 to 60.\nThe execution time of data loading with di\u000berent methods remains the same for the same\nnumber of nodes. Figure 12 shows the execution time of data loading, which decreases rapidly as\nthe number of nodes increases. This indicates the good scalability of our data loading process.\nThe execution time of PDF computation for di\u000berent methods and di\u000berent numbers of nodes\nis shown in Figure 13. We do not consider Reuse since it is less e\u000ecient than Grouping. We\nfocus on ML for 10 \u0000types because it has small error and an execution time similar to that\nfor 4\u0000types . The \fgure shows that the execution time of Grouping and ML is always better\nthan that of Baseline. The advantage of Grouping can be up to 87% (more than 6 times faster).\nML outperforms Baseline up to 89% (more than 8 times faster). Grouping + ML can be better\nthan Baseline by up to 90% (more than 9 times faster). In addition, the execution time of each\nmethod decreases as the number of nodes increases, which indicates that all methods have good\nscalability. However, the advantage becomes less obvious from 50 nodes on.\nFigure 14 gives a focus on our methods. We do not compare the error of PDF computation\nsince it is similar to that given in Section 6.2.1. The \fgure shows that Grouping + ML is better\n22\n\n 200 400 600 800 1000 1200 1400\n10 20 30 40 50 60Execution time (s)\nNumber of nodesGrouping(4)\nGrouping(10)\nBaseline+ML(10)\nGrouping+ML(10)Figure 14: Execution time of PDF computation with di\u000berent numbers of nodes and\na focus on Grouping, ML, Baseline + ML and Grouping + ML.\n 1 10 100 1000 10000\n10.5 0.2 0.1 0.05 0.02 0.01 0.005 0.002 0.001 0 2 4 6 8 10Execution time (data loading) (s)\nExecution time (PDF computation) (s)\nSampling ratePDF computation\nData loading\nFigure 15: Execution time with di\u000berent sampling rates using random sampling.\nthan either Grouping or ML when the number of nodes is 10. However, ML starts outperforming\nGrouping + ML when the number of nodes exceeds 10. This is because the shu\u000fing of data\namong di\u000berent nodes takes much time. As the number of nodes increases, the more time to\ntransfer data among the nodes also increases. Thus, the performance of the aggregation function\nbecomes a bottleneck for Grouping + ML when the number of nodes is high.\n6.2.3 Performance of Sampling\nWe now study the e\u000eciency of Sampling. We carried out the experiments in the LNCC cluster.\nWe use two sampling methods, i.e.random sampling and k-means clustering (see Section 5.4).\nWe compare the performance of the two sampling method with di\u000berent sampling rates.\nFigure 15 shows the execution time of random sampling with di\u000berent sampling rates. The\nexecution time for data loading decreases almost linearly as the sampling rate decreases (both\nthe X and Y axis use a base-10 log scale). This is reasonable since when the sampling rate gets\nsmall, the data loading needs to process less points and thus loads less data. The execution time\nfor PDF computation is very short (about 2 seconds). This is expected since we avoid calling the\nR program to compute PDFs and use a decision tree model to predict the distribution type of\neach point. The execution time is almost the same for di\u000berent sampling rates since it is already\n23\n\n 10 100 1000 10000\n0.2 0.1 0.05 0.02 0.01 0.005 0.002 0.001 0 2 4 6 8 10Execution time (data loading) (s)\nExecution time (PDF computation) (s)\nSampling ratePDF computation\nData loadingFigure 16: Execution time with di\u000berent sampling rates using k-means sampling.\n 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45\n0.0001 0.0002 0.0005 0.001 0.002 0.005 0.01Distance\nSampling ratek-means\nRandom\nFigure 17: Distance of the distribution type percentage between the double sampled\npoints and all points.\nvery short and data shu\u000fing also takes some time. In addition, we only need to transfer the\nmean and standard deviation values instead of the whole data set for the prediction, which also\nreduces time. This execution time is about 2 seconds, which is very short. However, this method\ncannot calculate the error of the PDF computation process. It can help to have a general view\nof the whole slice. Then a slice is chosen to compute the PDFs of the corresponding points.\nFigure 16 shows the performance of k-means clustering for sampling the points. The results\nare almost the same as that of random sampling, i.e. the execution time for data loading\ndecreases linearly and the execution time for PDF computation is very short and almost the\nsame for di\u000berent sampling rates. The execution time of k-means is longer than that of random\nfor the same sampling rate. We measure the sampling rate from 0.2 since the corresponding\nexecution time of k-means for data loading is already longer than that of random sampling with\nthe sampling rate of 1.\nFigure 17 shows the Euclidean distance of the distribution type percentage between the\ndouble sampled points and all points in one slice using k-means and random sampling. When\nthe sampling rate is small, the result of k-means is close to the percentage of overall points. This\nis because the double sampled points of k-means method contain diverse information. However,\nwhen the sampling rate is high, the results of random sampling are similar or better since enough\n24\n\n 0 2000 4000 6000 8000 10000 12000 14000 16000\n30 604-types\n10-types\n4-types + ML + Grouping\n10-types + ML + Grouping\n4-types + Grouping\n10-types + Grouping\n4-types + ML\n10-types + MLFigure 18: Execution time for all the points in Slice 201 (1.9 TB input data). The\ntime unit is second.\npoints are selected with a high sampling rate. Since random sampling is faster than k-means, we\nchoose it in the following experiments.\n6.3 Experiments on Big Data Sets\nIn this section, we compare the performance of di\u000berent methods based on big spatial data sets\nof several TB, i.e.Set2 andSet3 (see details in Section 6.1). We use the G5k cluster with 30\nand 60 nodes.\n6.3.1 Experiments with 1000 Simulations\nIn this section, we perform experiments using Set2 of 1.9 TB generated by 1000 simulations. We\ntake the relationship between the combination of mean and standard deviation values and the\ndistribution types of 25000 points in Slice 0 as previously generated data to build the decision\ntree model. The model error of 4 \u0000types is 0.02 and the model error of 10 \u0000types is 0.09. The\ntime to load the model ranges between 1 and 20 seconds, which is negligible compared with the\nexecution time of PDF computation. The time for data loading is 2671 seconds with 30 nodes\nand 1619 seconds with 60 nodes, which shows the good scalability of the data loading process.\nFigure 18 shows the good performance of our methods, i.e.Grouping and ML. Grouping is\nbetter than Baseline (up to 79%) but not as good as ML, because of data shu\u000fing with many\nnodes. And the scalability of Grouping is not good. ML largely outperforms Baseline (up to\n89%) and has good scalability. Because of the shu\u000fing bottleneck, the combination of Grouping\nand ML is worse than ML but still better (up to 77%) than Baseline. The error of ML with\n10\u0000types is always smaller than with 4 \u0000types .\nFinally, we experimented with random sampling to process the data in two clusters of 30\nand 60 nodes. The execution time of PDF computation ranges between 260s and 280s while the\nsampling rate ranges between 0.001 and 1. The average execution time (272s for 30 nodes and\n266 for 60 nodes) is 82% and 71% shorter than the minimum execution time (ML with 4 \u0000types\nfor 30 and 60 nodes) shown in Figure 18. Note that doubling the number of nodes does not yield\nmuch improvement. This is because using more nodes increases data transfers to send the mean\nand standard deviation values and the distribution type from each node to the Spark master\nnode in Spark cluster to compute the distribution percentage.\n25\n\n 0 50 100 150 200 250 300 350 400 450 500\nBaseline Grouping4-types\n10-types\n4-types + ML\n10-types + MLFigure 19: Execution time for PDF computation on small workload (2 lines and 1002\npoints) with 2.4 TB input data. The time unit is second.\n6.3.2 Experiments with 10000 Simulations\nIn this section, we perform experiments using Set3 of 2.4 TB generated by 10000 simulations.\nAgain, we take the relationship between the combination of mean and standard deviation values\nand the distribution types of 25000 points in Slice 0 as the previously generated data for the\ndecision tree model. The model error of 4 \u0000types is 0.006 and the model error of 10 \u0000types is\n0.012. The time to load the model ranges between 1 and 20 seconds, which is negligible compared\nwith the execution time of PDF computation. First, we execute the program for the points of\nthe \frst 2 lines in Slice 201 in a cluster of 30 nodes. We take 1 line as a window, and we process\nthe data of two windows. The execution time for data loading (Algorithm 2) is 28s, which is the\nsame for all the methods since we use the same algorithm (Algorithm 2). The execution time\nfor PDF computation is shown in Figure 19.\nFigure 19 shows the superior performance of ML. The execution time of Baseline with 10 \u0000\ntypes is much longer than that of 4 \u0000types as expected. Compared with Baseline, ML reduces\nexecution time much (57% with 4 \u0000types and 72% with 10\u0000types ). However, the execution time\nof Grouping is much longer. This is because the data size of each point is 9 times bigger since\na point corresponds to 10000 observation values instead of 1000. During Grouping, much more\ndata is transferred among di\u000berent nodes, which takes much time. Thus, in the next experiment,\nwe do not use Grouping.\nWe also measure the error ( Ede\fned in Equation 6) during execution. 10 \u0000types does not\nreduce much the average error for Baseline (up to 0.008). However, it may take much more time\nfor PDF computation when we consider 10 \u0000types as shown in Figure 19. Using WithML, the\naverage error is slightly bigger than that of NoML. The di\u000berence is very small (up to 0.007) but\nML can reduce much the execution time of PDF computation. Furthermore, 10 \u0000types leads to\nsmaller average error even though the model error is bigger when using ML.\nNow, we execute the program for all points in Slice 201. As we only compare the execution\nwith Baseline and ML, we take 126 lines as a window in order to parallelize the execution of\ndi\u000berent points in di\u000berent nodes. The time for data loading is 4592 seconds with 30 nodes.\nFigure 20 shows the superior performance of ML (up to 88%, i.e. more than 7 times faster\nthan Baseline ). Furthermore, the PDF computation process scales very well. We measured\nthe average error and found that ML incurs small error while reducing execution time much.\nFinally, we use the random sampling to process the data in two clusters of 30 and 60 nodes.\nThe execution time of PDF computation ranges between 128s and 200s while the sampling rate\n26\n\n 0 1000 2000 3000 4000 5000 6000\n30 604-types\n10-types\n4-types + ML\n10-types + MLFigure 20: Execution time for all the points in Slice 201 (2.4 TB input data). The\ntime unit is second.\nranges between 0.001 and 1. The average execution time (172s for 30 nodes and 155s for 60\nnodes) is 64% and 41% smaller than the minimum execution time (ML with 4 \u0000types for 30 and\n60 nodes) shown in Figure 20. Again (as for the experiment with 1000 simulations), doubling\nthe number of nodes does not yield much improvement.\n7 Conclusion\nUncertainty quanti\fcation of spatial data requires computing a Probability Density Function\n(PDF) of each point in a spatial cube area. However, computing PDFs on big spatial data, as\nproduced by applications in scienti\fc areas such as geological or seismic interpretation, can be\nvery time consuming.\nIn this paper, we addressed the problem of e\u000eciently computing PDFs under bounded error\nconstraints. We proposed a parallel solution using a Spark cluster with three new methods:\nGrouping, ML and Sampling. Grouping aggregates the points of the same statistical features\ntogether in order to reduce redundant calculation. This method is very e\u000ecient when the data to\nbe transferred is not too big and the number of cluster nodes is small. ML generates a decision\ntree model based on previously generated data and predicts the distribution type of a point\nin order to avoid useless calculation based on wrong distribution types. Sampling enables to\ne\u000eciently compute statistical parameters of a region by sampling a fraction of the total number\nof points to reduce the computation space.\nTo validate our solution, we implemented these methods in a Spark cluster and performed\nextensive experiments on two di\u000berent computer clusters (with 6 and 64 nodes) using big spatial\ndata ranging from hundreds of GB to several TB. This data was generated from simulations based\non the models from a seismic benchmark for oil and gas exploration, which includes models for\nseismic wave propagation.\nThe experimental results show that our solution is e\u000ecient and scales up very well compared\nwith Baseline. Grouping outperforms Baseline by up to 92% (more than 10 times) without\nintroducing extra error. ML can be up to 91% (more than 9 times) better than Baseline with\nvery slight acceptable error (up to 0.017). The combination of Grouping and ML can be up to\n97% (more than 33 times) better than Baseline. As the number of nodes exceeds 10 nodes, ML\noutperforms the combination. Thus, in order to compute PDFs, the combination of Grouping\nand ML is the optimal method when each point corresponds to a small number of observation\n27\n\nvalues, e.g. 1000, and when there is small number of nodes (less than 20). Otherwise, ML is\nthe best option. We also showed that Sampling is very e\u000ecient to calculate general statistics\ninformation in order to choose a slice for calculating PDFs. Finally, Sampling should be used\nwith the aforementioned best option in order to e\u000eciently compute PDFs.\nAcknowledgment\nThis work was partially funded by EU H2020 Project HPC4e with MCTI/RNP-Brazil, CNPq,\nFAPERJ, and Inria Associated Team SciDISC. The work of J. Liu, E. Pacitti and P. Val-\nduriez were performed in the context of the Computational Biology Institute ( http://www.\nibc-montpellier.fr ). The experiments were carried out using a cluster at LNCC in Brazil and\nthe Grid5000 testbed in France ( https://www.grid5000.fr ).\nReferences\n[1] Hpc geophysical simulation test suite. https://hpc4e.eu/downloads/\nhpc-geophysical-simulation-test-suite .\n[2] Spark MLib. https://spark.apache.org/mllib/ .\n[3] R. Belohl\u0013 avek, B. D. Baets, J. Outrata, and V. Vychodil. Inducing decision trees via concept\nlattices. Int. Journal of General Systems , 38(4):455{467, 2009.\n[4] R. Campisano, F. Porto, E. Pacitti, F. Masseglia, and E. S. Ogasawara. Spatial sequential\npattern mining for seismic data. In Simp\u0013 osio Brasileiro de Banco de Dados (SBBD) , pages\n241{246, 2016.\n[5] M. Chen, S. Mao, and Y. Liu. Big data: A survey. Mobile Networks and Applications ,\n19(2):171{209, 2014.\n[6] N. Cressie. Statistics for spatial data . John Wiley & Sons, 2015.\n[7] J. Dean and S. Ghemawat. Mapreduce: Simpli\fed data processing on large clusters. In\nSymp. on Operating System Design and Implementation (OSDI) , pages 137{150, 2004.\n[8] W. J. Dixon and F. J. Massey. Introduction to statistical analysis . 1968.\n[9] S. Fotheringham, C. Brunsdon, and M. Charlton. Quantitative Geography: Perspectives on\nSpatial Data Analysis . 2000.\n[10] S. Ghemawat, H. Gobio\u000b, and S. Leung. The google \fle system. In ACM Symp. on Operating\nSystems Principles (SOSP) , pages 29{43, 2003.\n[11] E. R. Harold. Java I/O: Tips and Techniques for Putting I/O to Work , pages 131{132.\n2006.\n[12] T. J. Jackson, D. M. L. Vine, A. Y. Hsu, A. Oldak, P. J. Starks, C. T. Swift, J. D. Isham,\nand M. Haken. Soil moisture mapping at regional scales using microwave radiometry: the\nsouthern great plains hydrology experiment. IEEE Transactions Geoscience and Remote\nSensing , 37(5):2136{2151, 1999.\n28\n\n[13] F. Kathryn, J. T. Oden, and D. Faghihi. A bayesian framework for adaptive selection,\ncalibration, and validation of coarse-grained models of atomistic systems. Journal of Com-\nputational Physics , 295:189 { 208, 2015.\n[14] S. Landset, T. M. Khoshgoftaar, A. N. Richter, and T. Hasanin. A survey of open source\ntools for machine learning with big data in the hadoop ecosystem. Journal of Big Data ,\n2(1):24, 2015.\n[15] J. Liu, E. Pacitti, and P. Valduriez. A survey of scheduling frameworks in big data systems.\nInternational Journal of Cloud Computing , page 27, 2018.\n[16] R. H. C. Lopes. Kolmogorov-smirnov test. In Int. Encyclopedia of Statistical Science , pages\n718{720, 2011.\n[17] S. Marelli and B. Sudret. Uqlab: A framework for uncertainty quanti\fcation in matlab. In\nInt. Conf. on Vulnerability, Risk Analysis and Management (ICVRAM) , pages 2554{2563,\n2014.\n[18] X. Meng, J. Bradley, B. Yavuz, E. Sparks, S. Venkataraman, D. Liu, J. Freeman, D. Tsai,\nM. Amde, S. Owen, D. Xin, R. Xin, M. J. Franklin, R. Zadeh, M. Zaharia, and A. Tal-\nwalkar. MLlib: Machine Learning in Apache Spark. Journal of Machine Learning Research ,\n17(34):1{7, 2016.\n[19] C. Michele, T. Stefano, and S. Andrea. Sensitivity and uncertainty analysis in spatial\nmodelling based on gis. Agriculture, Ecosystems & Environment , 81(1):71 { 79, 2000.\n[20] S. R. Safavian and D. Landgrebe. A survey of decision tree classi\fer methodology. IEEE\nTransactions on Systems, Man, and Cybernetics , 21(3):660{674, 1991.\n[21] R. Sandberg, D. Goldberg, S. Kleiman, D. Walsh, and B. Lyon. Design and implementation\nof the sun network \fle system. In the Summer USENIX conf. , pages 119{130, 1985.\n[22] K. Shvachko, H. Kuang, S. Radia, and R. Chansler. The hadoop distributed \fle system. In\nIEEE Symp. on Mass Storage Systems and Technologies (MSST) , pages 1{10, 2010.\n[23] P. Snyder. tmpfs: A virtual memory \fle system. In European UNIX Users Group Conf. ,\npages 241{248, 1990.\n[24] G. Trajcevski. Uncertainty in spatial trajectories. In Computing with Spatial Trajectories ,\npages 63{107, 2011.\n[25] F. Wang and J. Liu. Networked wireless sensor data collection: Issues, challenges, and\napproaches. IEEE Communications Surveys and Tutorials , 13(4):673{687, 2011.\n[26] B. Zadrozny and C. Elkan. Obtaining calibrated probability estimates from decision trees\nand naive bayesian classi\fers. In Int. Conf. on Machine Learning (ICML) , pages 609{616,\n2001.\n[27] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and I. Stoica. Spark: Cluster\ncomputing with working sets. In USENIX Workshop on Hot Topics in Cloud Computing\n(HotCloud) , 2010.\n29",
  "textLength": 78340
}