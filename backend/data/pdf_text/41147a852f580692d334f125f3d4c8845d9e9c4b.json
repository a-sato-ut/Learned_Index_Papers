{
  "paperId": "41147a852f580692d334f125f3d4c8845d9e9c4b",
  "title": "Predictive Flows for Faster Ford-Fulkerson",
  "pdfPath": "41147a852f580692d334f125f3d4c8845d9e9c4b.pdf",
  "text": "Predictive Flows for Faster Ford-Fulkerson\nSami Davies∗, Benjamin Moseley†, Sergei Vassilvitskii‡, Yuyan Wang‡\nMarch 3, 2023\nAbstract\nRecent work has shown that leveraging learned predictions can improve the running time of algorithms\nfor bipartite matching and similar combinatorial problems. In this work, we build on this idea to improve\nthe performance of the widely used Ford-Fulkerson algorithm for computing maximum \rows by seeding\nFord-Fulkerson with predicted \rows. Our proposed method o\u000bers strong theoretical performance in terms\nof the quality of the prediction. We then consider image segmentation, a common use-case of \rows in\ncomputer vision, and complement our theoretical analysis with strong empirical results.\n1 Introduction\nThe Ford-Fulkerson method is one of the most ubiquitous in combinatorial optimization, both in theory\nand in practice. While it was \frst developed for solving the maximum \row problem, many problems in\nscheduling (Ahuja et al., 1993), computer vision (Vineet and Narayanan, 2008), resource allocation, matroid\nintersection (Im et al., 2021b), and other areas are solvable by \fnding a reduction to a \row problem.\nTheoretically, max \row algorithms exist that are asymptotically much faster than the original Ford-\nFulkerson formulation, most recently the near-linear time algorithm of Chen et al. (2022b). However|as\noften happens|algorithms with great theoretical guarantees might be di\u000ecult to implement in practice.\nIndeed, algorithms used in practice still leave room for improvement. In fact, for computing max \rows in\nnetworks, practitioners often stick to older algorithms such as Dinic's algorithm (Bhadra et al., 2020), push-\nrelabel (Cherkassky and Goldberg, 1997), pseudo\row (Chandran and Hochbaum, 2009), or these algorithms\nlayered with heuristics to \ft particular settings.\nWhen \row algorithms are deployed in practice, they are often used to solve several problem instances\narising naturally over time. However, the theoretical analysis, as well as many implementations, considers\nsolving each new problem from scratch to derive worst-case guarantees. This approach needlessly discards\ninformation that may exist between instances. We are interested in discovering whether \row problems can be\nsolved more e\u000eciently by leveraging information from past examples. Seeding an algorithm from a non-trivial\nstarting point is referred to as a warm-start .\nWe are motivated by the question: can one warm-start Ford-Fulkerson to improve theoretical and em-\npirical performance? Towards this goal, we leverage the recently developed algorithms with predictions\nframework (a.k.a learning-augmented algorithms). Research over the past several years has showcased the\npower of augmenting an algorithm with a learned prediction, leading to improvements in caching (Im et al.,\n2022; Lindermayr and Megow, 2022; Lykouris and Vassilvitskii, 2021), scheduling (Im et al., 2021a; Lattanzi\n∗Northwestern University. sami@northwestern.edu . Supported by an NSF Computing Innovation Fellowship.\n†Carnegie Mellon University. moseleyb@andrew.cmu.edu . Supported in part by a Google Research Award, an Inform\nResearch Award, a Carnegie Bosch Junior Faculty Chair, and NSF grants CCF-2121744 and CCF-1845146.\n‡Google Research { New York. sergeiv@google.com ,wangyy@google.com .\n1arXiv:2303.00837v1  [cs.DS]  1 Mar 2023\n\net al., 2020), clustering (Lattanzi et al., 2021), matching (Chen et al., 2022a; Dinitz et al., 2021), and more\n(see the survey by Mitzenmacher and Vassilvitskii (2022)). An algorithm is learning-augmented if it can\nuse a prediction that relays information about the problem instance. Most prior work uses predictions to\novercome uncertainty in the online setting. However, recent work by Dinitz et al. (2021)|and the follow-up\nwork by Chen et al. (2022a)|instead focuses on improving the run-time of bipartite matching algorithms\nby predicting the dual variables and using these to warm-start the primal-dual algorithm.\nMotivated by the idea of warm-starting combinatorial optimization algorithms, we seek to provide faster\nrun-time guarantees for \row problems via warm-start. The paper will focus on \row problems generally, but\nwill additionally showcase a common, practical use-case in computer vision: image segmentation. In the\nimage segmentation problem, the input is an image containing an object/ foreground, and the goal is to\nlocate the foreground in the image.\n1.1 Our contributions\nFor a graph G= (V;E) equipped with a capacity vector c2ZjEj\n\u00150, letfbe a \row on G, wherefeis the \row\nvalue on each edge e2E. LetF\u0003be the collection of all feasible, maximum \rows on G. Given a potentially\ninfeasible \row bf, let\u0011(bf) = minf\u00032F\u0003jjbf\u0000f\u0003jj1. This term denotes how close bfis to being optimal.\nWarm-starting Ford-Fulkerson on general networks Our main contribution is Algorithm 1, which\ncan be used to warm-start any implementation of Ford-Fulkerson, i.e., Ford-Fulkerson with any speci\fed\nsubroutine for \fnding augmenting paths. Algorithm 1 takes as input a predicted \row bf. Notebfmay be\ninfeasible for G, as predictions can be erroneous. Algorithm 1 \frst projects bfto a feasible \row for G, and\nthen runs the Ford-Fulkerson procedure from the feasible state to \fnd a maximum \row. While our warm-\nstarted Ford-Fulkerson has its performance tied to the quality of the prediction, it also enjoys the same\nworst-case run-time bounds as the vanilla Ford-Fulkerson procedure.\nTheorem 1. Letbfbe a potentially infeasible \row on network G= (V;E). LetTbe the worst-case run-\ntime for Ford-Fulkerson with a chosen augmenting path subroutine. Using the same subroutine, Algorithm 1\nseeded withbf\fnds an optimal \row f\u0003onGwithin time O(minfjEj\u0001\u0011(bf);Tg).\nAt various points, we specify two Ford-Fulkerson implementations: Edmonds-Karp and Dinic's algorithm,\nfor which the run-time TisO(jEj2jVj) andO(jVj2jEj), respectively.\nOne may wonder how to obtain such a bf. We prove that when the networks come from a \fxed but\nunknown distribution, one can PAC-learn the best approximation (Theorem 10) for optimal \rows, which\ncan be used to warm-start.\nFaster warm-start on locally-changed networks Next, we improve the analysis of Algorithm 1 for\nnetwork instances with gradual, local transitions from one to another. We prove the local transitions among\nnetworks, informally characterized in Theorem 2, give rise to many short paths along which we can send\n\row, thus improving the run-time.\nTheorem 2 (Informal, formally Theorem 14) .Fix separable networks G1andG2, where the transition\nbetween them is d-local. Forbfan optimal \row on G1, the run-time of Algorithm 1 seeded with bfonG2to\n\fnd optimal f\u0003onG2isO(d\u0001jEj+d2\u0001\u0011(bf)).\nEmpirical results Motivated by our theoretical results, we use our warm-started Ford-Fulkerson proce-\ndure on networks derived from instances of image segmentation on sequences of photos taken of a moving\nobject or from changing angles. We show that warm-start is faster than standard Ford-Fulkerson procedures\n(2-5\u0002running time improvements), thus demonstrating that our theory is predictive of practical perfor-\nmance. A key piece of the speed gain of warm-start comes not from sending the \row along fewer paths, but\nrather from using shorter paths to project bfto a feasible \row, as predicted by Theorem 2. We note that the\n2\n\ngoal of our experiments is not necessarily to provide state-of-the-art algorithms for image segmentation, but\ninstead to show that warm-starting Ford-Fulkerson leads to substantial run-time improvements on practical\nnetworks as compared to running Ford-Fulkerson from scratch.\n1.2 Related work\nFlow problems have been well studied. See the survey by Ahuja et al. (1993). The Ford-Fulkerson method\ngreedily computes a maximum \row by iteratively using an augmenting-path \fnding subroutine (Ford and\nFulkerson, 1956). Di\u000berent subroutines give rise to di\u000berent implementations such as Edmonds-Karp (using\nBFS) (Edmonds and Karp, 1972) and the even faster Dinic's algorithm (Dinitz, 2006). Sherman (2013) and\nKelner et al. (2014) give fast algorithms that compute approximate maximum \rows. Chen et al. (2022b)\ngave a nearly-linear time max \row algorithm.\nSimilar in spirit to our work, Altner and Ergun (2008) demonstrate empirically that one can warm-start\nthe push-relabel algorithm on similar networks. Additionally, we are aware of concurrent work on a warm-\nstarted max \row algorithm by Polak and Zub (2022). Importantly, they require an additional assumption\nthat the predicted \row satisfy \row conservation constraints, a limitation that the authors highlight. In\ncontrast, we have an explicit feasibility restoration step, allowing us to get rid of this assumption.\nLearning-augmented algorithms have become popular recently. The area was jump started by Kraska\net al. (2018), who showed results on learned data structures. The area has since become popular for the\ndesign of online algorithms where the algorithm uses predictions to cope with uncertainty (Purohit et al.,\n2018; Lattanzi et al., 2020; Antoniadis et al., 2020). For the reader less familiar with this literature, we\nrecommend reading the paragraph Learning-augmented algorithms in Section 1.3.\nWhile Kraska et al. (2018) showed that running times can be improved using predictions, this is still\nyet to be well-understood theoretically. The work of Dinitz et al. (2021) showed how to improve the run-\ntime of the Hungarian algorithm for weighted bipartite matching. Chen et al. (2022a) has extended this\nto other graph problems. Both of these works warm-start primal dual algorithms with a predicted dual\nsolution. Other run-time improvements have been made using predicitions too, as Sakaue and Oki (2022)\ngave algorithms for faster discrete convex analysis, and Lu et al. (2021) showed predictions can be used to\nimprove the run-time of generalized sorting. A closely related area is that of data-driven algorithm design\n(Gupta and Roughgarden, 2017; Balcan et al., 2021).\n1.3 Organization and preliminaries\nSection 2 presents the warm-start algorithm, proves its correctness, and provides run-time guarantees. In\nSection 3, we give additional theoretical guarantees if the networks are from a speci\fc subclass. In Section\n4, we show our empirical results on networks arising from image segmentation, which are closely related\nto the networks in Section 3. Additional empirical evidence can be found in Appendix 6 and more at\nhttps://github.com/wang-yuyan/warmstart-graphcut-algorithms-pulic .\nLearning-augmented algorithms In this model, an algorithm is given access to a predicted parameter.\nThe prediction can be erroneous and must come equipped with an error metric. In our setting, for a given\nnetworkG, we will predict an integral \row bf. This predicted \row may be infeasible for G. Recall that for\nF\u0003the set of optimal \rows on G, we de\fne the error of bfonGto be\u0011(bf) = minf\u00032F\u0003jjbf\u0000f\u0003jj1.\nIt is well-established in the literature on learning-augmented algorithms that the desired properties of\nthe prediction and algorithm are learnability, consistency, and robustness . We show that given an additional\nassumption on the uniqueness of optimal \rows (see Assumption 8), predicted \rows are PAC-learnable in The-\norem 10. Additionally, we use the instance-robustness (Lavastida et al., 2021) of \rows to justify learnability\nfor special networks in Theorem 2 and observe this empirically, as well. If we are given a predicted \row bffor\n3\n\na networkGthat is actually an optimal \row, then the run-time is 0, so Algorithm 1 is consistent. We are also\nguaranteed robustness and a worst-case guarantee, as the run-time of Algorithm 1 is O(minfjEj\u0011(bf);Tg),\nwhich degrades smoothly as a function of how far bfis far from an optimal \row f\u0003on a network, but the\nworst-case is still bounded by O(T)1.\nNetwork \row LetG= (V;E) be a \fxed network with jVj=nandjEj=m. The source sand sinkt\nare part of the vertex set V. The network is equipped with a capacity vector c2Zm\n\u00150. A \rowf2Zm\n\u00150on\nGis feasible if it satis\fes \row conservation, i.e. for all vertices u2Vnfs;tgthe incoming \row for uequals\nthe outgoing \rowP\ne=(v;u)fe=P\ne=(u;w)fe, and capacity constraints, i.e. fe\u0014cefor all edges e2E.\nThroughout the paper we refer to a \row satisfying these constraints as feasible .\nGiven a \row fthat satis\fes capacity constraints but not necessarily \row conservation, the residual\ngraphGfis the network on the same set of vertices, and for any edge e= (u;v)2E, add edge etoGf\nbut with capacity c0\ne=ce\u0000feand a reversed edge ~e= (v;u) with capacity fe. Let\u0017(f) be the amount of\n\row thatfsends from the source, \u0017(f) =P\ne=(s;u)fe. Anaugmenting path inGfis a pathpfromstot\nwhere every edge e2phasc0\ne>0.\nWhenfdoes not satisfy \row conservation, the total incoming and outgoing \row values on a node are\ndi\u000berent. Call this di\u000berence the excess /de\fcit (exf=deff) of the node if the incoming is more/less than\noutgoing \row, respectively. For shorthand, we let the total excess and de\fcit in Gaccording to \row f\nbeexf=P\nu62fs;tgexf(u) and deff=P\nu62fs;tgdeff(u); note that this excludes the source and sink. Let\nAf;Bf\u0012Vbe the nodes with positive excess/de\fcit with respect to f(sot2Af,s2Bf), respectively. It\nwill be convenient to further refer to these sets excluding s;twithA0\nf=AfnftgandB0\nf=Bfnfsg.\n2 Warm-start Ford-Fulkerson\nHere, we give our algorithm for using predicted \rows. The next proposition holds from the following ob-\nservations. Any Ford-Fulkerson method (e.g., Edmonds-Karp or Dinic's) can be seeded with any feasible\n\row, since running Ford-Fulkerson seeded with a feasible \row is the same as running it from scratch on the\nresidual graph. Each iteration of Ford-Fulkerson increases the value of the \row and takes O(jEj) time to\n\fnd an augmenting path and send \row.\nProposition 3. Letfbe a feasible \row on G, where\u0017(f)< \u0017(f\u0003)forf\u0003an optimal \row on G. Ford-\nFulkerson seeded with fterminates in at most \u0017(f\u0003)\u0000\u0017(f)many iterations, so its run-time is O(jEj\u0001(\u0017(f\u0003)\u0000\n\u0017(f))).\nLetbfbe a predicted \row for network G. It may be infeasible, that is, it can violate capacity or \row\nconservation constraints. Algorithm 1 has two primary steps: step (1) projects bfto a feasible \row|we call\nthis the feasibility projection |and step (2) runs a Ford-Fulkerson method seeded with a feasible \row\nand \fnds an optimal \row. During feasibility projection, we \frst round down the \row wherever capacity\nconstraints are violated. Then we send \row along projection paths , that is, a path from excess nodes to\nde\fcit nodes where all capacities are positive in the residual graph, to get \row conservation.\nLetfbe an infeasible \row that satis\fes capacity constraints and is integral. In the main WHILE loop\nof Algorithm 1, projection paths are found in three rounds: A0\nf\u0000B0\nf,A0\nf\u0000s,B0\nf\u0000t. Within each rounds,\nwe \fnd the projection paths by constructing an auxiliary graph G0and applying on this graph the chosen\naugmenting path subroutine (e.g., BFS) in Ford-Fulkerson. To build G0for \fnding all possible A0\nf\u0000B0\nf\nprojection paths, take the residual graph Gfw.r.tfand treat it as a new network. Add a super source s\u0003\nand super sink t\u0003. Add arcs ( s\u0003;u) to everyu2A0\nf(non-sink excess nodes) with capacity exf(u), and (u;t\u0003)\nfrom every u2B0\nf(non-source de\fcit nodes) to t\u0003. Initialize all \rows to be 0 on this network. An s\u0003\u0000t\u0003\n1Recall, Tdepends on the Ford-Fulkerson implementation.\n4\n\nAlgorithm 1 Warm-starting Ford-Fulkerson with bf\nInput : predicted \row bf\nwhile9edgeeinGwithbfe>cedo\nUpdatebftobfe ce\nSetf bf\nBuild the residual graph Gf;as well asA0\nfandB0\nf, the sets of nodes with excess/de\fcit to round\n//Main while loop, feasibility projection\nwhilejA0\nf[B0\nfj>0do\nifjA0\nfj>0then\nif9projection path from u2A0\nftov2B0\nfthen\nLetpbe the path from utov\nelse\nLetpbe a path from u2A0\nftos\nelse\nLetv2B0\nf, letpbe a path from ttov\n//See the text for more details on path-\fnding\nLetw;w0be the beginning and ending nodes of p, respectively\nSend\u0016p= minfexf(w);deff(w0);mine2pc0\negunits of \row down path p\nUpdatef,Gf; A0\nf, andB0\nf\n//Feasibility projection ends, optimization starts\nRun Ford-Fulkerson on Gseeded with funtil optimality\nOutput :f\u0003\naugmenting path in G0corresponds to a A0\nf\u0000B0\nfprojection path in Gf. This is because for any projection\npath fromutou0one can let there be \row on s\u0003touandu0tot\u0003, thus making it an augmenting path in\nG0, and the reverse procedure holds. The G0graphs for the other two rounds are built similarly; for \fnding\nA0\nf\u0000sprojection paths, add arcs ( s\u0003;u) tou2A0\nfand (s;t\u0003); fort\u0000B0\nf, add arcs ( s\u0003;t) and (v;t\u0003) to every\nv2B0\nf.\nTo see that in the main WHILE loop of Algorithm 1, the projection paths can be found from A0\nf\u0000B0\nf,\nthen fromA0\nf\u0000s, and lastly from B0\nf\u0000t, we use the following lemma.\nLemma 4. Fix an infeasible \row f. If there is no path from A0\nftoB0\nfwith positive capacity in Gf, then\nsending \row from A0\nftos(or fromttoB0\nf) to form the \row hwill not result in a path from A0\nhtoB0\nhwith\npositive capacity in Gh.\nProof. Assume for sake of deriving a contradiction that (without loss of generality) sending \row from some\nu2A0\nftosformed \row h, for which there exists a path from A0\nftoB0\nfwith positive capacity in Gh. Let\np1= (u1;:::;s ) be the path along which \row was sent in Gf, and letp2= (u2;:::;v ) be the path with\npositive capacity in Gh. Note that u1;u22A0\nfandv2B0\nfby the assumptions. Since p2went from having\n0 capacity in Gfto positive capacity in Gh, at least one edge e= (a;b) ofp2has~e= (b;a) inp1. If there\nare multiple such edges take eto be the last such edge in p1:Letp0\n1= (u1;:::;a ) be the truncation of p1at\naand letp0\n2= (a;:::;v ) be the su\u000ex of p2that begins at a. Then letp0be the concatenation of p0\n1andp0\n2.\nNote thatp0\n1andp0\n2both have positive capacity in Gf. Thusp0is a path in Gfwith positive capacity from\nutov, which is a contradiction.\n5\n\n2.1 Warm-start algorithm analysis\nIn this section we analyze how Algorithm 1 works.\nValidity of algorithm We prove that a projection path must exist in the main WHILE loop.\nLemma 5. Given infeasible \row f,8u2A0\nf,9v2Bfsuch that there is a projection path from utov;\n8v2B0\nf,9u2Afsuch that there is a projection path from utov.\nThis lemma results from the following observation that links the summation of excess/de\fcit to the\ndi\u000berence between in-\rows and out-\rows for any \fxed set of nodes.\nProposition 6. Letfbe a \row satisfying the capacity constraints of a network G. Then for any S\u0012V,\nthe di\u000berence between the total de\fcits in Sand the total excesses in Sis exactly the di\u000berence between the\ntotal \row out of Sand intoS. Formally,\nX\nu2Sdeff(u)\u0000X\nu2Sexf(u) =X\nu2SX\ne=(u;v):\nv62Sfe\u0000X\nu2SX\ne=(v;u):\nv62Sfe:\nProof. InP\nu2Sdeff(u)\u0000P\nu2Sexf(u), the edges with \row within Sare counted with a positive and negative\nsign, but the edges carrying \row into Sor out ofSare counted once by the excess and once by the de\fcit,\nrespectively.\nProof of Lemma 5. We prove one direction by contradiction. The proof for the other direction is similar.\nFor anyu2A0\nf, assume no such path exists. In Proposition 6 take Sto be the set of all vertices reachable\nfromuinGf. None of the nodes in Scan have positive de\fcit, so the LHS of Proposition 6 must be negative.\nOn the other hand, Smust have 0 \row incoming to it, otherwise there is an edge pointing from StoVnS\ninGf, producing a vertex in VnSreachable from uand contradicting the maximality of S. Therefore, the\nRHS in Proposition 6 is non-negative, contradicting the equation.\nNote each iteration decreases the total amount of excess and de\fcit in the system, exf+deff, by at least\none, so the WHILE loop terminates after restoring \row conservation, giving rise to a feasible \row. Then the\nvanilla Ford-Fulkerson takes over until an optimal solution is found.\nRunning-time analysis Here we work towards proving Theorem 1. We show the running time is tied to\nthe quality of prediction, \u0011(bf) = minf\u00032F\u0003jjbf\u0000f\u0003jj1. We \frst bound the times the path-\fnding subroutine\nis called. For projection paths, the total excess and de\fcit could increase by at mostP\nemaxfbfe\u0000ce;0gwhen\nAlgorithm 1 rounds down the \row where it exceeds capacity. Thus it takes at most ( exbf+defbf) projection\npaths to restore feasibility. For augmenting paths, the di\u000berence in \row value, \u0017(bf)\u0000\u0017(f\u0003), could decrease\nby at mostP\nemaxfbfe\u0000ce;0gduring the round-down and another exbf+defbfduring feasibility projection,\nthus the total number is at most the summation of the three. Each path-\fnding takes O(jEj) time. This\ncombined with the next lemma proves Theorem 1.\nLemma 7. \u0017(f\u0003)\u0000\u0017(f);exbf+defbf, andP\nemaxfbfe\u0000ce;0gare upper bounded by \u0011(bf).\nProof. First, we see that j\u0017(f)\u0000\u0017(f\u0003)jandj\u0017(bf)\u0000\u0017(f\u0003)jcan only di\u000ber by however much value was lost\nand however much excess and de\fcit was gained in projecting bfto the feasible \row f:Therefore, we can\nupper boundj\u0017(f)\u0000\u0017(f\u0003)jby\nj\u0017(f)\u0000\u0017(f\u0003)j\u0014j\u0017(bf)\u0000\u0017(f\u0003)j+X\nemaxfbfe\u0000ce;0g+exbf+defbf:\n6\n\nThen, we can further upper bound j\u0017(bf)\u0000\u0017(f\u0003)jby rewriting the di\u000berence between the values of the \rows\nj\u0017(f\u0003)\u0000\u0017(bf)j=\f\f\f\fX\ne=(s;v)f\u0003\ne\u0000X\ne=(s;v)bfe\f\f\f\f\u0014\u0011(bf):\nThe next term to bound in terms of the `1error isP\nemaxfbfe\u0000ce;0g, though it is straight forward to see\nX\nemaxfbfe\u0000ce;0g\u0014X\nemaxfbfe\u0000f\u0003\ne;0g\u0014\u0011(bf):\nLastly, we see that the excess/ de\fcit of any node v2Vnfs;tgcan be charged to the di\u000berence between bfe\nandf\u0003\neforeadjacent to v, as anyf\u00032F\u0003has excess/ de\fcit 0 on all non-source and sink nodes. Therefore,\nexbf+defbf\u0014\u0011(bf):\nProof of Theorem 1. Given a \row bfthat does not satisfy capacity constraints, Algorithm 1 simply updates\nthe edgesE0\u0012Ethat violate capacity bfe> cetobfe cefore2E0. This can be done in time O(jE0j).\nFurther, rounding down the \row on these edges changes the value of the \row and the sum of the excess and\nde\fcit by at mostP\nemaxfbfe\u0000ce;0g.\nIn Lemma 5, we showed that given bfthat satis\fes capacity constraints, Algorithm 1 will \fnd an optimal\nf\u0003. Next, we analyze the run-time of Algorithm 1. Each iteration of the main while loop in Algorithm\n1 costs time O(jEj). Further, the number of iterations in the main while loop in Algorithm 1 is at most\nexbf+defbf. Letfbe the feasible \row obtained by Algorithm 1 at the end of the main while loop. The\nrun-time to produce \row fisO(jEj(exbf+defbf)).\nAt mostj\u0017(f\u0003)\u0000\u0017(f)jiterations of any Ford-Fulkerson procedure are needed to arrive at the optimal\n\row value\u0017(f\u0003) fromfby Proposition 3. Each iteration of Ford-Fulkerson also costs O(jEj).\nTherefore, the run-time of Algorithm 1 given a prediction which satis\fes capacity constraints is at most\nO(jEj\u0001(j\u0017(f)\u0000\u0017(f\u0003)j+exbf+defbf)):\nCombining this with the loss from projecting to satisfy capacity constraints, the full run-time of Algorithm\n1 is\nO\u0000\njEj\u0001\u0000X\nemaxfbfe\u0000ce;0g+j\u0017(f)\u0000\u0017(f\u0003)j+exbf+defbf\u0001\u0001\n:\nWe showed that all of the terms multiplying jEjin the above can be bounded by O(\u0011(bf)) (Lemma 7).\nIt is straight-forward to justify the worst-case run-time of Algorithm 1 is bounded by O(T). During the\nfeasibility projection step an auxiliary graph is constructed only three times (recall that from Lemma 4, we\ncan send \row on A0\nf\u0000B0\nfpaths, then A0\nf\u0000spaths, then t\u0000B0\nfpaths), each time with jVj+ 2 vertices and\nO(jEj) edges. Thus running the chosen Ford-Fulkerson implementation on these graphs takes time O(T).\nRecall that the optimization step also takes time O(T), since running Ford-Fulkerson starting with a feasible\n\row is equivalent to running it from scratch on the residual graph as a new input. Thus the total running\ntime isO(T).\n2.2 PAC-learning Flows\nHere, we show theoretically that high quality \rows are learnable, thus giving evidence that \rows can be\nlearned for input to Algorithm 1. We show that given a distribution over capacity vectors for a network, one\ncan learn a predicted \row from samples that is the best approximation.\n7\n\nConsider a \fxed network Gwith integral edge capacities c. An instance is a network Gion the same\nvertex and edge set as G, but the capacity vector is ci, where every edge einGimust satisfy ci\ne2[0;ce]\\Z.\nLetDbe an unknown distribution over such instances. Since an instance is exactly characterized by its new\ncapacity vector, we notationally write this as sampling a capacity vector ci\u0018D.\nSuppose we sample instances c1;:::;csfromD. LetFbe the set of all integral \rows on Gthat satisfy\nthe capacities in c, noting that \rows in Fdo not have to satisfy \row conservation. Technically, a network\nGmight have several optimal solutions. Here we make the following assumption.\nAssumption 8. For a network G, there is a uniquely associated, computable optimal \row.2\nGiven samples c1;:::;cs, we can compute the uniquely associated optimal \rows on samples f\u0003(c1),:::,\nf\u0003(cs). Letbfdenote a predicted \row. When our goal is to warm-start Ford-Fulkerson, we choose the\npredicted \row to be that in Fwhich minimizes the empirical risk bf= argminf2F1\nsPs\nj=1jjbf\u0000f\u0003(cj)jj1:,\nwhich given Assumption 8 can be e\u000eciently computed, as in Lemma 9.\nLemma 9. One can \fnd a \row bf2 F minimizing1\nsPs\nj=1jjbf\u0000f\u0003(cj)jj1from independent samples\nc1;:::;cs\u0018D in polynomial time by taking bfe=median (f\u0003(c1)e;:::;f\u0003(cs)e)for alle2E.\nProof. We would like to \fnd bf2F that minimizes1\nsPs\nj=1jjbf\u0000f\u0003(cj)jj1:Since we do not require \row\nconservation, the minimization can occur over each edge independently, where bfewill be in [0 ;ce]\\Z, i.e. it\nsu\u000eces to minimize1\nsPs\nj=1jbfe\u0000f\u0003(cj)ejfor eache2E. The function1\nsPs\nj=1jbfe\u0000f\u0003(cj)ejis continuous\nand piece-wise linear in bfe, where the slope changes at the points ff\u0003(cj)egj. It is well-known that the\nminimum of this function in [0 ;ce] ismedian (f\u0003(c1)e;:::;f\u0003(cs)e).\nWe will now state our PAC-learning result. The proof of this theorem follows that of Dinitz et al. (2021).\nTheorem 10. Letc1;:::;csbe sampled i.i.d. from Dand letbf=argminf2F01\nsPs\nj=1jjf\u0000f\u0003(cj)jj1:For\ns= \n((max\nec2\ne\u0001m2)(logm+ log(1=p)))\nandef=argminf2F0Eci\u0018Djjf\u0000f\u0003(ci)jj1, then with probability at least 1\u0000p,bfsatis\fes\nEci\u0018Djjbf\u0000f\u0003(ci)jj1\u0014Eci\u0018Djjef\u0000f\u0003(ci)jj1+O(1):\nIn the proof of Theorem 10, we will use some well-known results regarding the pseudo-dimension of a\nclass of functions.\nThe VC dimension is a quantity that captures the complexity of a family of binary functions, and the\npseudo-dimension is the analog of this for real-valued functions Speci\fcally, the pseudo-dimension of a\nfamily of real-valued functions His the largest sized subset shattered by H. A subsetS=fx1;:::;xsgofX\nisshattered byHif there exists real-valued witnesses r1;\u0001\u0001\u0001;rssuch that for each of the 2ssubsetsTof\nS, there exists a function h2H withh(xi)\u0014riif and only if i2T.\nThe following theorem relates the convergence of the sample mean of some h2H to its expectation, and\nthis relation depends on the pseudo-dimension.\nTheorem 11 (Uniform convergence) .LetHbe a class of functions with domain Xand range in [0;H]. Let\ndHbe the pseudo-dimension of H. For every distribution DoverX, every\u000f>0, and every \u000e2(0;1], if\ns\u0015c(H=\u000f)2(dH+ ln(1=\u000e))\n2Such assumptions are standard for the PAC-learning results in learning-augmented based run-time improvements, even if\nnot explicitly stated. See, for instance Sakaue and Oki (2022).\n8\n\nfor some constant c, then with prob at least 1\u0000\u000eoverssamplesx1:::;xs2D,\n\f\f\f\f\f \n1\nssX\ni=1h(xi)!\n\u0000Ex\u0018D[h(x)]\f\f\f\f\f<\u000f:\nEquipped with Theorem 11, we are ready to prove our PAC-learning result.\nProof of Theorem 10. We will construct a class of functions that contains the loss functions of the \row f\ngiven capacity constraints ci. Then, we will apply Theorem 11 to this class of functions.\nFor every integral \row f2RjEjthat satis\fes the capacity vector ci, let the function gf(ci) =jjf\u0003(ci)\u0000fjj1\nbe the loss function of fonci. Then letH=fgfjf2Rmgbe the family of all of these loss functions.\nWe saw in Lemma 9 how to e\u000eciently compute the empirical risk minimizer. Also, the upper bound of\nthe range of the loss functions, i.e. Hin the statement of Theorem 11, is at most m\u0001maxece. To prove our\nlemma, it remains to bound the pseudo-dimension of H.\nWe will upper bound the pseudo-dimension of Hby showing it is no more than the pseudo-dimension\nof another class of functions, Hm, whose pseudo-dimension is already known. Let Hm=fhyjy2Rmgfor\nhy(x) =jjy\u0000xjj1. The following result appears as Theorem 19 in Dinitz et al. (2021), and the reader may\nrefer to that paper for its proof.\nLemma 12. The pseudo-dimension of Hmis at mostO(mlogm):\nNow all that remains is to prove the following lemma, relating the pseudo-dimensions of the two classes.\nLemma 13. If the pseudo-dimension of Hmis at mostd, then the pseudo-dimension of His at mostd.\nProof. LetS=fc1;:::;cdgbe a set that is shattered by H. Letr1;:::;rd2Rbe the witnesses such that\nfor allS0\u0012[d], there exists some gfS02H withgfS0(cj) =jjf\u0003(cj)\u0000fS0jj1\u0014rjif and only if j2S0.\nWe will construct a set eSof sizedfromSthat is shattered by Hm. LeteS=ff\u0003(c1);:::;f\u0003(cd)gand \fx\nsomeS0\u0012[d]. ThenhfS0(f\u0003(cj)) =jjfS0\u0000f\u0003(cj)jj1\u0014rjif and only if j2S0.\nPluggingH\u0014m\u0001maxeceanddH\u0014O(mlogm) into Theorem 11, we see that it su\u000eces to take\ns\u0015\n((max\nece\u0001m=\u000f)2(mlogm+ ln(1=\u000e))):\n3 Faster Flows via Shorter Projection Paths\nHere, we show that Algorithm 1 can be even faster for a certain class of networks. Intuitively, the additional\nspeed-up is obtained due to \fnding shorter projection paths. We will then explain in Section 4 how simple\nimage segmentation networks \ft into this class, so this theory explains the speed-up we see on the image\nsegmentation instances.\nLetG= (V;E) be a directed graph, with s;t2V. Suppose Vnfs;tgforms a two-dimensional grid.\nFurther for u;v2Vnfs;tg, ife= (u;v)2Ethen the reverse direction edge ~e= (v;u)2E. We consider\na pair of networks G1;G2onG. The only di\u000berence in these networks is their capacity vectors, though we\nassume they have capacity vectors c1;c22f1;Mgmfor some large integer M, and we assume that all edges\nincident to sorthave capacity M.\nFor`2[2], letE`=fe2Ejc`\ne= 1g. We callG`separable if the vertices in Vnfs;tgcan be\npartitioned into subsets V`andW`=Vn(fs;tg[V`), such that there is some x2V1\\V2with (x;t)2Eand\ny2W1\\W2with (s;y)2E, and for all e= (u;v)2E`,ehas one endpoint in V`and the other in W`. We\n9\n\nsay the transition between G1andG2isd-local if for all pairs of distinct nodes u;v2(V2nV1)[(W2nW1),\ntheir distance, i.e., the length of the shortest path between them, is at most d. Here,dcontrols the length\nof the projection paths.\nWhile we require additional assumptions for our theoretical results, our empirical results in Section 4,\nshow that these assumptions are su\u000ecient but not necessary for Algorithm 1 to take advantage of short\nprojection paths.\nFor the proof of Theorem 2, the feasibility projection of bfhas several steps. First, Algorithm 1 can\nchoose paths along which to route \row so that either the excess and de\fcit are \fxed, or V2only contains\nnodes with positive excess and W2only contains nodes with positive de\fcit or vice versa. We argue that\nthis \fxes the total excess and de\fcit by at most \u0011(bf) with paths of length O(d). Then, it remains to \fx the\nexcess/ de\fcit in V2andW2usingsandt. These paths have unbounded length, i.e. length O(jEj), and we\ncan argue the change in \row value is at most jjE2j\u0000jE1jj\u0014O(d2), where the upper bound comes from he\nde\fnition of d-local.\nTheorem 14 (Restates Theorem 2) .Fix separable networks G1andG2, where the transition between them\nisd-local. Forbfan optimal \row on G1, the run-time of Algorithm 1 seeded with bfonG2to \fnd an optimal\nf\u0003isO(d2\u0001jEj+d\u0001\u0011(bf)).\nProof. For the network G2with capacity constraints c2, projectbfto satisfyc2as in Algorithm 1, and let\nthe resulting \row be f. Note that a node can only be in A0\nf[B0\nfif it is incident to an edge whose \row was\nrounded down in the projection from bftof. An edge ( u;v) has \row rounded down in the projection only\nif it went from being an edge with both uandveither inV1or outside of V1to being a boundary edge, i.e.\none ofuorvis inV2and one is in W2.\nTo project fto a \row that satis\fes \row conservation, Algorithm 1 can choose paths to \frst route \row\nfrom nodes with positive excess in V2to nodes with positive de\fcit in V2, and route \row from nodes with\npositive excess in W2to nodes with positive de\fcit in W2. By our assumption that the networks are d\u0000local,\nexcess and de\fcit within V2or inW2have distance at most d. Since the capacity of the non-boundary edges\nisM, Algorithm 1 can route \row until there is only excess or only de\fcit contained within V2and within\nW2with these projection paths. Additionally, if V2contains a node with positive de\fcit and W2contains a\nnode with positive excess and if there are any edges with positive \row going from V2toW2, one can send\n\row on the reverse edge and remove that excess/ de\fcit with a projection path of length at most 2 d.\nFurther, Algorithm 1 can choose paths to perform this \row routing so excess and de\fcit are symmetric\nacross the boundary. Speci\fcally, for a node u2V2, one can route \row so exf(u) =P\nv:(u;v)2E2deff(v), and\nthis holds for the de\fcits, and analogously for the nodes of W2. Note that this re-routing is possible since\nMis su\u000eciently large. From the proof of Theorem 1, we know that exf+deff\u0014\u0011(bf), so the run-time of\nre-routing this excess and de\fcit is at most O(d\u0001\u0011(bf)).\nWe will show any remaining de\fcit/ excess can be handled by paths using sandt. First, the max \row\nonG1has valuejE1j, since the edges crossing from W1intoV1form a cut. On the other hand, for large M\nand by the existence of x2V1\\V2andy2W1\\W2, there exists feasible \rows with every edge ( u;v) with\nu2W1andv2V1having \row 1 incoming to V1. Second, our re-routing procedure made the excess within\nV2symmetric across the boundary to the de\fcit in W2, so there is either positive excess in V2and positive\nde\fcit inW2, or vice versa.\nWe use these two observations. Suppose that after re-routing, there is de\fcit inside of V2and excess\noutside of it. As a consequence of our routing, we can assume all edges with positive \row in E2are directed\nfromW2toV2. For every node uwith positive excess in W2, take the excess of uand send it to s, which is\npossible from the conditions for being separable. Similarly, for every node uwith positive de\fcit inside of V2,\n\fnd paths from ttouand send deff(u) fromttou, and this is again possible by the separable condition. The\n10\n\nresulting \row is feasible. Further, there are no s\u0000tpaths since all edges in E2going intoV2are saturated\nand form a cut, so the \row is optimal. So re-routing this \row using sandttakes timeO(jEj(jE1j\u0000jE2j)).\nWhen after re-routing there is excess inside of V2and de\fcit inside of W2, the proof is similar. The edges\nwith no excess/ de\fcit incident to them have \row 1 going into V2. Since the de\fcit is outside of V2, the\nboundary edges crossing from V2intoW2have \row 1 going out of V2. Send \row from sto the nodes with\npositive de\fcit inside of W2, and send the excess inside of V2tot, which is possible by the conditions for\nbeing separable. The resulting \row is feasible, though perhaps not optimal, as one may need to saturate\nnew boundary edges. The run-time is still O(jEj(jE2j\u0000jE1j)).\nBy the de\fnition of d\u0000local,jjE2j\u0000jE1jj\u0014O(d2). Therefore, the run-time of Algorithm 1 on these\nlocally-changed instances is at most O(d2\u0001jEj+d\u0001\u0011(bf))).\n4 Empirical Results\nIn this section, we validate the theoretical results in Sections 2 and 33.We consider image segmentation, a\ncore problem in computer vision that aims at separating an object from the background in a given image.\nThe problem is re-formulated as a max-\row/ min-cut optimization problem in a line of work (Boykov and\nJolly, 2001; Boykov and Kolmogorov, 2004; Boykov and Funka-Lea, 2006) and solved with combinatorial\ngraph-cut algorithms, including Ford-Fulkerson.\nWe do not attempt to provide state-of-the-art run-time results on image segmentation. Our goal is to show\nthat on real-world networks, warm-starting Ford-Fulkerson leads to big run-time improvements compared to\ncold-start Ford-Fulkerson. We highlight the following:\n•For both Edmonds-Karp and Dinic's implementation of Ford-Fulkerson, warm-start o\u000bers improved\nrunning time compared to starting the algorithm from scratch (referred to as a cold-start ).\n•As we increase the number of image pixels (i.e., its resolution), the size of the constructed graph\nincreases and the savings in time becomes more signi\fcant.\n•The feasibility projection step in Algorithm 1 has high performance. It returns a feasible \row that is\nonly slightly sub-optimal, and it \fnds short paths to \fx the excess/de\fcits in doing so. Both factors\ncontribute to warm-start being way more e\u000ecient than cold-start.\nDatasets and data prepossessing We use four di\u000berent image groups from the Pattern Recognition\nand Image Processing dataset from the University of Freiburg4, named Birdhouse ,Head ,Shoe , and Dog\nrespectively. The \frst three groups are from the dataset Image Sequences5, in the format of .jpg images,\nwhereas Dog , from Stereo Ego-Motion Dataset6, is a video which we converted to .jpg.\nEach image group contains a sequence of photos featuring the same object and background. The sequence\nmay feature the object's motion relative to the background or changes in the camera's shooting angle. Any\nimage is only slightly di\u000berent from the previous one in the sequence, and this could potentially lead to\nminor di\u000berences in segmentation solutions. This justi\fes warm-starting with the optimal \row for the max\n\row problem found on the previous image.\nWe take 10 images from each group, cropped them to be 600 \u0002600 pixels with the object included, and\ngray-scaled them. Then we resize the images to generate image sequences of di\u000berent sizes. See Table 1 for\ndetailed information about the image groups, the featured object/backround, and the original and cropped\nsizes of each image. See Figures 1 and 2 for an image instance from each group.\nGraph construction Following the practice in Boykov and Funka-Lea (2006), we brie\ry describe how\nto formulate image segmentation as a max-\row/min-cut problem and how to write the boundary-based\n3The code is published at https://github.com/wang-yuyan/warmstart-graphcut-algorithms-pulic .\n4https://lmb.informatik.uni-freiburg.de/resources/datasets/\n5https://lmb.informatik.uni-freiburg.de/resources/datasets/sequences.en.html\n6https://lmb.informatik.uni-freiburg.de/resources/datasets/StereoEgomotion.en.html\n11\n\nTable 1: Image groups' descriptions\nImage Group Object, background Original size Cropped size\nBirdhouse wood birdhouse, backyard 1280, 720 600, 600\nHead a person's head, buildings 1280, 720 600, 600\nShoe a shoe, \roor and other toys 1280, 720 600, 600\nDog Bernese Mountain dog, lawn 1920, 1080 500, 500\n(a) Birdhouse\n (b) Head\n(c) Shoe\n (d) Dog\nFigure 1: Examples of original images in each group.\nobjective function. Our input is an image with pixel set V, along with two sets of seedsO;B, which are\npixels predetermined to be part of the object or background, respectively (often selected by human experts),\nto make the segmentation task easier. Let Ivdenote the intensity (or gray scale) of pixel v. For any two\npixelsp;q, separating them in the object/background segmentation solution induces a penalty of\fp;q. If\np;qare neighboring pixels, i.e. pandqare either in the same column and in adjacent rows or same row and\nadjacent columns, then \fp;q=Cexp(\u0000(Ip\u0000Iq)2\n2\u001b2), whereCis a relatively big constant scalar, otherwise it is\n0. Thus\fp;qgets bigger with stronger contrast between neighboring pandq.\nFor a given solution let Jdenote the object pixels. The boundary-based objective function is the\nsummation of the penalties over all pairs of pixels: max JP\np2J;q=2J\fp;q;forJsatisfyingO\u0012J;B\u0012VnJ.\nPenalties are only imposed on the object boundary. The best segmentation minimizes the total penalty,\n(a) Birdhouse\n (b) Head\n (c) Shoe\n (d) Dog\nFigure 2: Cropped, gray-scaled images in each group.\n12\n\n(a) Image 1\n (b) Image 5\n (c) Image 10\nFigure 3: Seeds on the \frst, \ffth, and last images from the 120 \u0002120 pixels Birdhouse sequence. Red for object,\ngreen for background.\nthus maximizing the contrast between the object and background across the boundary, while satisfying the\nconstraints imposed by seeds.\nThis is equivalent to solving the max-\row/min-cut problem on the following graph. Let the node set be\nall the pixels plus two terminal nodes: the object terminal s(source) and the background terminal t(sink).\nWe add the following arcs: (1) from sto every node inO, with a huge capacity M; (2) from every node in B\ntot, again with capacity M; (3) from every pair of node p;q2V(including the seeds), both arcs ( p;q) and\n(q;p) with capacity \fp;q. The value Mshould ensure that these arcs never appear in the optimal cut. The\n\row goes from stot. For ann\u0002npixels image, the graph is sparse with O(n2) nodes and also O(n2) arcs.\nLink to theory For an image sequence, the constructed graphs are a generalization of the setting in Section\n3. The graphs form 2-dimensional grids and share the same network structure, the only di\u000berences being\nthe capacity vectors. In addition, Section 3 makes other assumptions which also translate into properties\nof the images. The 1 or Medge capacities assumption implies an extreme contrast between the gray scales\nof object and background pixels. The d-local assumption says that from one image to the next, the new\nobject and background pixels are geographically close, implying only minor changes in the object's shape and\nlocation. Our image sequences do not strictly satisfy these properties. However, in all of our experiments\nthe conclusions remain robust against moderate violations of the theoretical assumptions, showing that\nwarm-starts can be bene\fcial in practice beyond current theoretical limits.\nDetailed experiment settings Each image sequence has 10 images and they share the same set of seeds,\nso the constructed graphs have the same structure. See Figure 3 for seeds for Birdhouse . Starting with\nthe second image, we reuse the old max-\row solution on the previous one and pass the \row to Algorithm\n1. During the feasibility projection, we pick a node and keep diminishing its excess/de\fcit by \fnding a\nprojection path and sending \row down that path, until excess/de\fcit is 0. As in Section 3, we prioritize\nprojection paths excluding sandt, since these modi\fcations preserve the overall \row value, and we only\nsend \row back to sand fromtwhen no other paths exist.\nWe compare cold- and warm-start for both Edmonds-Karp and Dinic's algorithms. We use breadth-\fst-\nsearch (BFS) to \fnd such projection paths in our warm-starts for both Edmonds-Karp and Dinic's. We use\nthe BFS procedure for our warm-started Dinic's instead of the expected subroutine from Dinic's algorithm\nbecause the overhead of building the level graph is more time consuming than running BFS. This is due to\nthe projection paths being short.\nWe usen\u0002npixel images for n2f30;60;120g. Numerically, the \u001bin the de\fnition of \f\u0001;\u0001is 50, and\nCis 100. To make the capacities integral, all \fp;q's are rounded down to the nearest integer. Notice that\n\fp;q\u0014Cby de\fnition. We let M=CjVj2to make the term su\u000eciently large.\nAll experiments are run on a device with Intel(R) Core(TM) i7-7600U CPU @ 2.80GHz, with 24G\nmemory. We record the wall-clock running time for both algorithms. Many of the image process tools and\n13\n\n(a) Image 1\n (b) Image 5\n (c) Image 10\nFigure 4: Cuts (red) on the \frst, \ffth, and last images from the 120 \u0002120 pixels Birdhouse sequence.\nfunctions are based on the Image Segmentation Github project (Jiang, 2017).\nTable 2: Average running times of cold-/warm-start Ford Fulkerson and the percentage of time saved by\nwarm-start, Edmonds-Karp\nImage Group 30\u000230 60 \u000260 120 \u0002120\nBirdhouse 0.83/0.55, 34.07% 8.48/3.48, 58.98% 109.06/37.31, 65.78%\nHead 0.65/0.45 31.06% 9.52/4.28, 55.07% 112.66/31.77, 71.80%\nShoe 0.72/0.46, 36.01% 8.81/3.04, 65.47% 111.05/30.44, 72.59%\nDog 0.73/0.41, 42.96% 22.38/6.89, 69.22% 202.99 / 42.04, 79.29%\nTable 3: Average running times of cold-/warm-start Ford Fulkerson and the percentage of time saved by\nwarm-start, Dinic\nImage Group 30\u000230 60\u000260 120 \u0002120\nBirdhouse 0.38/0.37, 2.49% 5.81/3.17, 45.43% 82.52/35.37, 57.14%\nHead 0.36/0.36 0.58% 7.7/4.44, 42.35% 149.12/49.44, 62.88%\nShoe 0.39/0.37, 5.07% 7.01/3.35, 52.24% 140.52/49.33, 64.9%\nDog 0.5/0.41, 10.16% 12.38/4.99, 59.66% 206.85 / 58.98, 71.48%\nResults We \frst show that the boundary-based image segmentation approach generates reasonable cuts.\nFor example, Figure 4 illustrates cuts from the 120 \u0002120Birdhouse sequence. See Appendix 6 for other\nexamples. We then compare the running time of cold- and warm-start Ford-Fulkerson. As all algorithms\nare returning optimal \rows, there are no qualitative aspects of the solutions to measure. Tables 2 and 3\nshow results in all experiments settings for Edmonds-Karp and Dinic, rows being image groups and columns\nimage sizes. Each entry is formatted as \\cold-start time (s) / warm-start time(s), warm-start time savings\n(%)\".\nThese results show warm-starting Ford-Fulkerson greatly improves the e\u000eciency in all settings. Further,\nboth cold- and warm- start's running time increases polynomially with the image width n, but warm-start\ngrows slower, making it a potentially desirable approach on large scale networks. This is most obvious on\nimage group Dog using Edmonds-Karp, where warm-start time is 60% of cold-start time on 30 \u000230 pixels\nversus 20% on 120 \u0002120 pixels. These conclusions hold for both Edmonds-Karp and Dinic, with Dinic being\nslightly more e\u000ecient on smaller datasets.\nNext we examine the execution of cold-/warm-start in more detail, taking the 120 \u0002120Birdhouse\nsequence for Edmonds-Karp for example (Table 4). The table gives the average length of the augmenting\n14\n\npaths (`avg length') and the average number of paths found (`avg #') over the sequence of images. See\nAppendix 6 for complete data.\nTable 4: Comparison of projection and augmenting paths in cold- and warm-start Ford-Fulkerson, the \frst\n5 images from the 120 \u0002120Birdhouse image sequence\nImage #cold-start\naug path #cold-start\naug path\navg lengthwarm-start\nproj path #warm-start\nproj path\navg lengthwarm-start\naug path #warm-start\naug path\navg length\n12453 67.93 2105 9.39 628 81.48\n22093 65.22 3393 19.28 0 0\n32536 74.88 2038 9.71 896 101.731\n42089 69.09 3335 28.55 0 0\n51908 68.53 3226 22.97 0 0\nResults in Table 4 suggest that the projected feasible \row is in general only slightly sub-optimal, which is\nkey for warm-start's e\u000eciency. Max-\row on the previous image is a good starting point for warm-start with\nthe feasibility projection algorithm. On average, after rounding down the previous max-\row to satisfy the\nnew edge capacities, the total excess/de\fcit is (1 :75\u00060:44) % of the real maximum \row value. Moreover,\n\fxing the excess/de\fcit results in a near optimal \row. Indeed, the projection quickly gives a feasible \row\nthat recovers (96 \u00066)% of the maximum \row.\nAnother factor contributing to the e\u000eciency of warm-start is the projection path-\fnding subroutine.\nRecall that both cold- and warm-start use the same BFS subroutine to \fnd either an s;taugmenting path\nor a projection path. The theory in Section 3 suggests that paths in the projection step will take less time\nto \fnd. To show this empirically, we collected data on the number of augmenting/feasibility projection\npaths found and their average lengths for both cold- and warm-start. Overall, compared with cold-start,\nwarm-start has shorter projection paths on average, suggesting massive savings in the BFS running time per\npath. While we show this for the Birdhouse images in Table 4, this is true on other datasets too, available\nin Appendix 6. This explains the e\u000eciency even if the excess/de\fcit is large. This shows that the theoretical\nexpectations raised in Section 3 are predictive of empirical performance.\n5 Conclusion\nWe show how to warm-start the Ford-Fulkerson algorithm for computing \rows, as well as prove strong\ntheoretical results and give empirical evidence of good performance of our algorithm. We further re\fne our\nanalysis to capture the gains due to using short projection paths to route excess \row and show that these\nscenarios are prevalent in image segmentation applications.\nMany interesting challenges remain. For one, there are many known algorithms for computing \rows,\nand it would be interesting to see if those methods can also be sped up in a similar fashion. A technical\nroadblock lies in handling both under- and over- predictions, particularly when predictions lead to infeasible\n\rows. More generally, a network \row problem can be written as a linear program. Another direction is\n\fnding algorithms for solving general LPs that can be helped by judiciously chosen predictions.\n15\n\nReferences\nAhuja, R. K., Magnanti, T. L., and Orlin, J. B. (1993). Network Flows: Theory, Algorithms, and Applica-\ntions . Prentice hall.\nAltner, D. S. and Ergun, O. (2008). Rapidly solving an online sequence of maximum \row problems with\nextensions to computing robust minimum cuts. In Perron, L. and Trick, M. A., editors, Integration\nof AI and OR Techniques in Constraint Programming for Combinatorial Optimization Problems, 5th\nInternational Conference, CPAIOR 2008, Paris, France, May 20-23, 2008, Proceedings , volume 5015 of\nLecture Notes in Computer Science , pages 283{287. Springer.\nAntoniadis, A., Gouleakis, T., Kleer, P., and Kolev, P. (2020). Secretary and online matching problems\nwith machine learned advice. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.,\neditors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual .\nBalcan, M., DeBlasio, D. F., Dick, T., Kingsford, C., Sandholm, T., and Vitercik, E. (2021). How much\ndata is su\u000ecient to learn high-performing algorithms? generalization guarantees for data-driven algorithm\ndesign. In Khuller, S. and Williams, V. V., editors, STOC '21: 53rd Annual ACM SIGACT Symposium\non Theory of Computing, Virtual Event, Italy, June 21-25, 2021 , pages 919{932. ACM.\nBhadra, S., Kundu, A., and Khatua, S. (2020). Optimization of road tra\u000ec congestion in urban tra\u000ec\nnetwork using dinic's algorithm. In Abraham, A., Sasaki, H., Rios, R., Gandhi, N., Singh, U., and Ma,\nK., editors, Innovations in Bio-Inspired Computing and Applications - Proceedings of the 11th Interna-\ntional Conference on Innovations in Bio-Inspired Computing and Applications (IBICA 2020) held during\nDecember 16-18, 2020 , volume 1372 of Advances in Intelligent Systems and Computing , pages 372{379.\nSpringer.\nBoykov, Y. and Funka-Lea, G. (2006). Graph cuts and e\u000ecient nd image segmentation. International journal\nof computer vision , 70(2):109{131.\nBoykov, Y. and Kolmogorov, V. (2004). An experimental comparison of min-cut/max-\row algorithms for\nenergy minimization in vision. IEEE transactions on pattern analysis and machine intelligence , 26(9):1124{\n1137.\nBoykov, Y. Y. and Jolly, M.-P. (2001). Interactive graph cuts for optimal boundary & region segmentation\nof objects in nd images. In Proceedings eighth IEEE international conference on computer vision. ICCV\n2001, volume 1, pages 105{112. IEEE.\nChandran, B. G. and Hochbaum, D. S. (2009). A computational study of the pseudo\row and push-relabel\nalgorithms for the maximum \row problem. Operations research , 57(2):358{376.\nChen, J. Y., Silwal, S., Vakilian, A., and Zhang, F. (2022a). Faster fundamental graph algorithms via learned\npredictions. In Chaudhuri, K., Jegelka, S., Song, L., Szepesv\u0013 ari, C., Niu, G., and Sabato, S., editors,\nInternational Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA ,\nvolume 162 of Proceedings of Machine Learning Research , pages 3583{3602. PMLR.\nChen, L., Kyng, R., Liu, Y. P., Peng, R., Gutenberg, M. P., and Sachdeva, S. (2022b). Maximum \row and\nminimum-cost \row in almost-linear time. CoRR , abs/2203.00671.\nCherkassky, B. V. and Goldberg, A. V. (1997). On implementing the push|relabel method for the maximum\n\row problem. Algorithmica , 19(4):390{410.\n16\n\nDinitz, M., Im, S., Lavastida, T., Moseley, B., and Vassilvitskii, S. (2021). Faster matchings via learned duals.\nIn Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W., editors, Advances in\nNeural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems\n2021, NeurIPS 2021, December 6-14, 2021, virtual , pages 10393{10406.\nDinitz, Y. (2006). Dinitz'algorithm: The original version and even's version. Theoretical Computer Science:\nEssays in Memory of Shimon Even , pages 218{240.\nEdmonds, J. and Karp, R. M. (1972). Theoretical improvements in algorithmic e\u000eciency for network \row\nproblems. J. ACM , 19(2):248{264.\nFord, L. R. and Fulkerson, D. R. (1956). Maximal \row through a network. Canadian Journal of Mathematics ,\n8:399{404.\nGupta, R. and Roughgarden, T. (2017). A PAC approach to application-speci\fc algorithm selection. SIAM\nJ. Comput. , 46(3):992{1017.\nIm, S., Kumar, R., Petety, A., and Purohit, M. (2022). Parsimonious learning-augmented caching. In\nChaudhuri, K., Jegelka, S., Song, L., Szepesv\u0013 ari, C., Niu, G., and Sabato, S., editors, International\nConference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA , volume 162\nofProceedings of Machine Learning Research , pages 9588{9601. PMLR.\nIm, S., Kumar, R., Qaem, M. M., and Purohit, M. (2021a). Non-clairvoyant scheduling with predictions. In\nAgrawal, K. and Azar, Y., editors, SPAA '21: 33rd ACM Symposium on Parallelism in Algorithms and\nArchitectures, Virtual Event, USA, 6-8 July, 2021 , pages 285{294. ACM.\nIm, S., Moseley, B., and Pruhs, K. (2021b). The matroid intersection cover problem. Oper. Res. Lett. ,\n49(1):17{22.\nJiang, J. (2017). Image segmentation. https://github.com/julie-jiang/image-segmentation/.\nKelner, J. A., Lee, Y. T., Orecchia, L., and Sidford, A. (2014). An almost-linear-time algorithm for ap-\nproximate max \row in undirected graphs, and its multicommodity generalizations. In Chekuri, C., editor,\nProceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2014,\nPortland, Oregon, USA, January 5-7, 2014 , pages 217{226. SIAM.\nKraska, T., Beutel, A., Chi, E. H., Dean, J., and Polyzotis, N. (2018). The case for learned index structures.\nIn Das, G., Jermaine, C. M., and Bernstein, P. A., editors, Proceedings of the 2018 International Conference\non Management of Data, SIGMOD Conference 2018, Houston, TX, USA, June 10-15, 2018 , pages 489{\n504. ACM.\nLattanzi, S., Lavastida, T., Moseley, B., and Vassilvitskii, S. (2020). Online scheduling via learned weights.\nIn Chawla, S., editor, Proceedings of the 2020 ACM-SIAM Symposium on Discrete Algorithms, SODA\n2020, Salt Lake City, UT, USA, January 5-8, 2020 , pages 1859{1877. SIAM.\nLattanzi, S., Moseley, B., Vassilvitskii, S., Wang, Y., and Zhou, R. (2021). Robust online correlation cluster-\ning. In Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W., editors, Advances in\nNeural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems\n2021, NeurIPS 2021, December 6-14, 2021, virtual , pages 4688{4698.\nLavastida, T., Moseley, B., Ravi, R., and Xu, C. (2021). Learnable and instance-robust predictions for\nonline matching, \rows and load balancing. In Mutzel, P., Pagh, R., and Herman, G., editors, 29th\n17\n\nAnnual European Symposium on Algorithms, ESA 2021, September 6-8, 2021, Lisbon, Portugal (Virtual\nConference) , volume 204 of LIPIcs , pages 59:1{59:17. Schloss Dagstuhl - Leibniz-Zentrum f ur Informatik.\nLindermayr, A. and Megow, N. (2022). Permutation predictions for non-clairvoyant scheduling. In Agrawal,\nK. and Lee, I. A., editors, SPAA '22: 34th ACM Symposium on Parallelism in Algorithms and Architec-\ntures, Philadelphia, PA, USA, July 11 - 14, 2022 , pages 357{368. ACM.\nLu, P., Ren, X., Sun, E., and Zhang, Y. (2021). Generalized sorting with predictions. In Le, H. V. and King,\nV., editors, 4th Symposium on Simplicity in Algorithms, SOSA 2021, Virtual Conference, January 11-12,\n2021, pages 111{117. SIAM.\nLykouris, T. and Vassilvitskii, S. (2021). Competitive caching with machine learned advice. J. ACM ,\n68(4):24:1{24:25.\nMitzenmacher, M. and Vassilvitskii, S. (2022). Algorithms with predictions. Commun. ACM , 65(7):33{35.\nPolak, A. and Zub, M. (2022). Learning-augmented maximum \row. CoRR , abs/2207.12911.\nPurohit, M., Svitkina, Z., and Kumar, R. (2018). Improving online algorithms via ML predictions. In Bengio,\nS., Wallach, H. M., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors, Advances in\nNeural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems\n2018, NeurIPS 2018, December 3-8, 2018, Montr\u0013 eal, Canada , pages 9684{9693.\nSakaue, S. and Oki, T. (2022). Discrete-convex-analysis-based framework for warm-starting algorithms with\npredictions. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural\nInformation Processing Systems 2022, NeurIPS 2022, .\nSherman, J. (2013). Nearly maximum \rows in nearly linear time. In 54th Annual IEEE Symposium on\nFoundations of Computer Science, FOCS 2013, 26-29 October, 2013, Berkeley, CA, USA , pages 263{269.\nIEEE Computer Society.\nVineet, V. and Narayanan, P. (2008). Cuda cuts: Fast graph cuts on the gpu. In 2008 IEEE Computer\nSociety Conference on Computer Vision and Pattern Recognition Workshops , pages 1{8. IEEE.\n18\n\n6 Appendix\nWe give a description of the experiment settings and provide more collected data and results. More can be\nfound at https://github.com/wang-yuyan/warmstart-graphcut-algorithms-pulic .\nMore on choice of seeds and cuts Recall that on the 10 images from the same sequence, the seed\npixels are always \fxed. We note here the choice of seeds (number of seeds and their locations) a\u000bects which\nmin-cut solution is found. However, as long as the seeds give a reasonable solution that is close to the\nreal object/background boundary, the conclusions in the comparison between cold- and warm-start remain\nrobust against a change of seeds.\nIn Section 4, we showed seeds and optimal cuts on images 1, 5, and 10 of the 120 \u0002120 pixel Birdhouse\nsequence in Figures 3 and 4. Here we show, in addition, our seeds and resulting cuts on images 1, 5, and 10\nof the other 120\u0002120 sequences. Those of Head are in Figure 5, those of Shoe in Figure 6, and those of\nDog in Figure 7.\n(a)Image 1, seeds\n (b)Image 5, seeds\n (c)Image 10, seeds\n (d)Image 1, cut\n (e)Image 5, cut\n (f)Image 10, cut\nFigure 5: Seeds and resulting cuts on the \frst, \ffth, and last images from the 120 \u0002120 pixels head sequence. Red\nseeds for object, green seeds for background, red line for cut.\n(a)Image 1, seeds\n (b)Image 5, seeds\n (c)Image 10, seeds\n (d)Image 1, cut\n (e)Image 5, cut\n (f)Image 10, cut\nFigure 6: Seeds and resulting cuts on the \frst, \ffth, and last images from the 120 \u0002120 pixels shoe sequence. Red\nseeds for object, green seeds for background, red line for cut.\n(a)Image 1, seeds\n (b)Image 5, seeds\n (c)Image 10, seeds\n (d)Image 1, cut\n (e)Image 5, cut\n (f)Image 10, cut\nFigure 7: Seeds and resulting cuts on the \frst, \ffth, and last images from the 120 \u0002120 pixels dog sequence. Red\nseeds for object, green seeds for background, red line for cut.\nWhen we select a seed, we draw a two-dimensional ball around the target seed and let every pixel in\nthis ball be a seed as well. We found this practice to work better than simply choosing individual pixels as\n19\n\nseeds. When we switch from low-resolution (30 \u000230) to high-resolution (120 \u0002120) images, we rescale the\nradius of this ball proportional to the number of pixels on each side. On the 30 \u000230, 60\u000260, 120\u0002120 pixel\nimages, the ball's radius is 1, 2 and 4 pixels, respectively. In other words, if we stretch/compress the images\nof di\u000berent resolution to be the same size, the ball will roughly have the same area geometrically. We also\nfound this to be more e\u000bective than \fxing the pixel radius, despite the change in resolution.\n(a) 30\u000230\n (b) 60\u000260\n (c) 120 \u0002120\nFigure 8: Area that each seed covers on the same image with di\u000berent resolutions.\nNote that although the location of the seeds remains unchanged throughout an image sequence, we may\nstill need to provide more seeds when we switch from low- to high-resolution images. Intuitively, blurring\nthe image lessens the minor contrast of pixels within the object and makes the geometric shape easier to\ncapture. The seeds and min-cut results on the 30 \u000230 and 60\u000260 sequences can be found in the code\ndirectory uploaded in the github linked at the beginning of this section.\nMore on the warm-start magic In the main body we gave evidence|both theoretically and empirically|\nthat the savings in the run-time of warm-start is mostly due to:\n•The algorithm's ability to use short projection paths to re-route excess \row to nodes with de\fcit \row,\nthus projecting the predicted \row to a feasible one quickly.\n•An only slightly sub-optimal \row after the feasibility projection, so that warm-start takes fewer aug-\nmenting paths to reach an optimal \row.\nHere we provide more results in support of these two claims. To show the level of total excess/de\fcit\n(whichever one is larger) and the \row value after the feasibility projection step, we show two ratios: total\nexcess/dei\fcit over max-\row (Table 5), and feasible \row value over max-\row (Table 6). One can see that\ntypically the total excess/de\fcit is not negligible. In fact they are quite high and if the algorithm does not\nresolve excesses/de\fcits in the right way (such as sending all excess to the source) it could cause the \row\nvalue to diminish a lot. Our feasibility projection makes good decisions about using projection paths to\nmake up for excess/de\fcit, so that it outputs a feasible \row with almost optimal \row value.\nTable 5: Average ratio of total excess/de\fcit over max-\row value in warm-start\nImage Group 30\u000230 60 \u000260 90 \u000290\nBirdhouse 1.06\u00060.22 1.60 \u00060.21 1.75 \u00060.44\nHead 0.49\u00060.12 0.6 \u00060.12 0.74 \u00060.1\nShoe 0.49\u00060.13 0.66 \u00060.08 0.95 \u00060.14\nDog 0.55\u00060.07 0.8 \u00060.07 1.08 \u00060.19\nTo show that the conclusion of projection paths being short broadly holds for all image groups, we give\nthe average length of the augmenting and projection paths (`avg length') and the number of paths found\n(`aug path #' and `proj path #') over the \frst 5 images in the sequence for the 120 \u0002120Head sequence\nin Table 7, the 120 \u0002120Shoe sequence in Table 8, and the 120 \u0002120Dog sequence in Table 9. Note the\nanalogous table for the 120 \u0002120Birdhouse sequence (Table 4) is in Section 4.\n20\n\nTable 6: Average ratio of \row value after feasibility projection over max-\row value in warm-start\nImage Group 30\u000230 60 \u000260 90 \u000290\nBirdhouse 0.94\u00060.09 0.98 \u00060.03 0.96 \u00060.06\nHead 0.98\u00060.03 0.98 \u00060.03 0.99 \u00060.01\nShoe 0.98\u00060.02 0.98 \u00060.03 0.98 \u00060.02\nDog 0.97\u00060.04 0.97 \u00060.03 0.98 \u00060.03\nTable 7: Comparison of projection and augmenting paths in cold- and warm-start Ford-Fulkerson, the \frst\n5 images from the 120 \u0002120Head image sequence\nImage #cold-start\naug path #cold-start\naug path\navg lengthwarm-start\nproj path #warm-start\nproj path\navg lengthwarm-start\naug path #warm-start\naug path\navg length\n12714 82.65 2573 15.93 221 80.42\n22687 82.74 2512 20.40 217 135.68\n32475 76.63 2667 19.78 0 0\n42379 76.44 2140 17.00 0 0\n52349 75.66 2260 19.97 112 138.14\nTable 8: Comparison of projection and augmenting paths in cold- and warm-start Ford-Fulkerson, the \frst\n5 images from the 120 \u0002120Shoe image sequence\nImage #cold-start\naug path #cold-start\naug path\navg lengthwarm-start\nproj path #warm-start\nproj path\navg lengthwarm-start\naug path #warm-start\naug path\navg length\n11948 89.23 2252 22.70 0 0\n22081 91.67 1992 16.54 112 148.41\n32039 93.88 1936 14.91 177 142.51\n42110 101.97 2525 35.04 0 0\n52016 93.68 2375 18.60 0 0\nTable 9: Comparison of projection and augmenting paths in cold- and warm-start Ford-Fulkerson, the \frst\n5 images from the 120 \u0002120Dog image sequence\nImage #cold-start\naug path #cold-start\naug path\navg lengthwarm-start\nproj path #warm-start\nproj path\navg lengthwarm-start\naug path #warm-start\naug path\navg length\n13314 63.04 3684 12.51 0 0\n23200 65.56 4611 21.69 0 0\n33138 63.53 3515 12.30 0 0\n43259 66.61 3270 10.74 444 87.08\n53120 64.43 3932 12.63 0 0\nFurther, we show the equivalence of these tables for the other two image sizes/resolutions, 30 \u000230 and\n60\u000260, for image groups head (Table 10 and 11) and shoe (Table 12 and 13). For these two groups,\nsequences of all three sizes share the same location of seeds. One can see that, the average length of the\naugmenting paths in cold-start Ford-Fulkerson grows roughly proportional to the width of the image. The\naverage length of the projection paths during the warm-start feasibility projection also grows as the width\nof the image grows, but slightly slower than the former. This could potentially cause warm-start to be more\nadvantageous on high-resolution images.\nThe omitted data tables and other experiment results can be found in the uploaded program directory\n(see the README.md \fle in the linked github repository for instructions.\n21\n\nTable 10: Comparison of projection and augmenting paths in cold- and warm-start Ford-Fulkerson, the \frst\n5 images from the 30 \u000230Head image sequence\nImage #cold-start\naug path #cold-start\naug path\navg lengthwarm-start\nproj path #warm-start\nproj path\navg lengthwarm-start\naug path #warm-start\naug path\navg length\n1267 25.16 226 8.61 61 40.72\n2244 23.26 254 11.63 3 44.33\n3253 22.11 236 12.05 0 0\n4248 21.45 238 11.17 0 0\n5250 22.98 252 12.24 10 43.30\nTable 11: Comparison of projection and augmenting paths in cold- and warm-start Ford-Fulkerson, the \frst\n5 images from the 60 \u000260Head image sequence\nImage #cold-start\naug path #cold-start\naug path\navg lengthwarm-start\nproj path #warm-start\nproj path\navg lengthwarm-start\naug path #warm-start\naug path\navg length\n1789 46.52 674 10.36 164 56.57\n2852 44.17 763 9.69 99 62.59\n3752 41.09 866 14.71 0 0\n4782 40.19 567 7.52 169 48.0\n5777 42.62 931 16.67 0 0\nTable 12: Comparison of projection and augmenting paths in cold- and warm-start Ford-Fulkerson, the \frst\n5 images from the 30 \u000230Shoe image sequence\nImage #cold-start\naug path #cold-start\naug path\navg lengthwarm-start\nproj path #warm-start\nproj path\navg lengthwarm-start\naug path #warm-start\naug path\navg length\n1165 20.85 192 9.59 0 0\n2172 20.72 164 7.53 24 27.21\n3175 21.91 195 12.42 2 34.5\n4201 22.51 164 12.34 15 29.93\n5162 21.21 215 9.22 0 0\nTable 13: Comparison of projection and augmenting paths in cold- and warm-start Ford-Fulkerson, the \frst\n5 images from the 60 \u000260Shoe image sequence\nImage #cold-start\naug path #cold-start\naug path\navg lengthwarm-start\nproj path #warm-start\nproj path\navg lengthwarm-start\naug path #warm-start\naug path\navg length\n1585 41.22 580 13.40 31 58.65\n2508 40.21 562 14.06 0 0\n3609 42.29 469 7.13 147 50.65\n4646 43.86 675 14.13 17 58.24\n5595 44.64 683 14.82 0 0\n22",
  "textLength": 66712
}