{
  "paperId": "182f4d6445575d68c28f7fb76ad48839ae5df1fa",
  "title": "Spatial Interpolation-based Learned Index for Range and kNN Queries",
  "pdfPath": "182f4d6445575d68c28f7fb76ad48839ae5df1fa.pdf",
  "text": "Spatial Interpolation-based Learned Index for Range\nandkNN Queries\nSongnian Zhang\nFaculty of Computer Science\nUniversity of New Brunswick\nFredericton, NB, Canada\nszhang17@unb.caSuprio Ray\nFaculty of Computer Science\nUniversity of New Brunswick\nFredericton, NB, Canada\nsray@unb.caRongxing Lu\nFaculty of Computer Science\nUniversity of New Brunswick\nFredericton, NB, Canada\nrlu1@unb.caYandong Zheng\nFaculty of Computer Science\nUniversity of New Brunswick\nFredericton, NB, Canada\nyzheng8@unb.ca\nAbstract ‚ÄîA corpus of recent work has revealed that the\nlearned index can improve query performance while reducing the\nstorage overhead. It potentially offers an opportunity to address\nthe spatial query processing challenges caused by the surge in\nlocation-based services. Although several learned indexes have\nbeen proposed to process spatial data, the main idea behind\nthese approaches is to utilize the existing one-dimensional learned\nmodels, which requires either converting the spatial data into one-\ndimensional data or applying the learned model on individual\ndimensions separately. As a result, these approaches cannot fully\nutilize or take advantage of the information regarding the spatial\ndistribution of the original spatial data. To this end, in this paper,\nwe exploit it by using the spatial (multi-dimensional) interpolation\nfunction as the learned model, which can be directly employed\non the spatial data. SpeciÔ¨Åcally, we design an efÔ¨Åcient SP atial\ninteR polation functI on based G rid index (SPRIG) to process the\nrange and kNN queries. Detailed experiments are conducted on\nreal-world datasets, and the results indicate that our proposed\nlearned index can signiÔ¨Åcantly improve the performance in\ncomparison with the traditional spatial indexes and a state-of-\nthe-art multi-dimensional learned index.\nIndex Terms ‚ÄîLearned index, Spatial interpolation function,\nRange query, kNN query\nI. I NTRODUCTION\nAs location-based services (LBS) have been widely de-\nployed and have become highly popular, spatial query pro-\ncessing has attracted considerable interests in the research\ncommunity. Although several spatial indexes, such as R-tree\nand k-d tree, have been proposed to facilitate spatial query\nperformance, it is still challenging to process the spatial\nqueries efÔ¨Åciently due to the rapidly growing volume of spatial\ndata. Recently, Kraska et al. [1] suggested substituting the\ntraditional indexes with machine learned based indexes (also\ncalled learned index ). Since then, several follow-up research\nprojects [2], [3], [4], [5], [6] have shown that the learned\nindex can indeed improve query performance by learned data\ndistribution and query workload patterns.\nTypically, there are two main aspects involving a learned\nindex, namely, a learned model and a local search. The former\nis trained and used to quickly locate the approximate position\nof a search key, while the latter is responsible for reÔ¨Åning the\naccurate position. Since the latter can be achieved by perform-\ning a local binary or exponential search, it is a fundamental\nbut challenging topic to Ô¨Ånd a reasonable learned model andfurther employ it as the learned index. Existing learned indexes\nare constructed based on mainly one of two categories of\nlearned models: machine learning [1] and piecewise linear\nfunctions [3]. However, to the best of our knowledge, both of\nthese learned models can be only applied in single dimensional\ndata. As a result, the current spatial learned indexes either\ntransform multi-dimensional data into one-dimensional data\nbefore introducing the learned model as a foundation [7], [8]\nor apply a learned model on every single dimension [6]. For\nthis reason, the question then arises, ‚Äú Is there a learned model\nthat can be directly applied to spatial (multi-dimensional) data\nand achieve better performance? ‚Äù\nAiming to address the above-mentioned question, in this\npaper, we explore how to utilize spatial (two-dimensional)\ninterpolation functions as the learned models to directly predict\nthe position of a spatial search key. Based on this idea, we\npropose a SP atial inteR polation functI on based G rid index\n(SPRIG) to support range and kNN queries over spatial\ndata. In particular, we sample the spatial data to construct\nan adaptive grid and use the sample data as inputs to Ô¨Åt\na spatial interpolation function. Given a spatial search key,\nÔ¨Årst, we can use the Ô¨Åtted spatial interpolation function to\npredict the approximate position of the key. Then, around the\nestimated position, we can conduct a local binary search to\nÔ¨Ånd the target key. However, it entails a new challenge: how\nto guarantee that the target key is in the local search range.\nTo address this issue, we introduce the maximum estimation\nerror based error guarantee, which is derived based on the\nquery workload. Furthermore, we propose efÔ¨Åcient range and\nkNN query execution strategies using our proposed index. In\nthese strategies, we take full advantage of the properties of the\nadaptive grid to facilitate the query executions, and a pivot\nbased Ô¨Åltering technique is introduced to improve the kNN\nquery performance.\nWe conduct extensive experiments to evaluate our learned\nindex, SPRIG. First, we evaluate Ô¨Åve spatial interpolation\nfunctions and choose the bilinear interpolation function, which\nhas the best performance and estimation accuracy, as our\nlearned model. Then, we compare SPRIG against the state-of-\nthe-art multi-dimensional learned index Flood [6], along with\na few spatial indexes. The experimental results involving real-\nworld datasets show that: 1) SPRIG outperforms the alternativearXiv:2102.06789v1  [cs.DB]  12 Feb 2021\n\nspatial indexes on range queries and is competitive on kNN\nqueries in terms of execution time. In the best case, SPRIG is\n3\u0002faster than Flood with range queries and 9 \u0002faster than\nFlood withkNN queries; 2) SPRIG consumes less storage to\nachieve a favorable execution performance compared with the\ntraditional indexes. Our evaluations demonstrate that SPRIG\ncan reduce the storage footprint of traditional spatial indexes\nby orders of magnitude.\nThe remainder of this paper is organized as follows. In\nSection II, we discuss the related work. Then, we introduce the\nspatial interpolation function, error guarantee, and pivot based\nÔ¨Åltering in Section III. After that, we present our SPRIG in\nSection IV, followed by performance evaluation in Section V.\nFinally, we draw our conclusion in Section VI.\nII. R ELATED WORK\nKraska et al. [1] presented the idea of the learned index,\nwhich is based on learning the relationship between keys and\ntheir positions in a sorted array. They adopted a machine\nlearning based technique as the learned model and built a\nrecursive model index (RMI), which predicts the position of a\nsearch key within a known error bound. Since then, a variety\nof learned indexes was proposed to handle one-dimensional\ndata. Recently, Tang et al. [2] proposed a scalable learned\nindex XIndex based on RMI, which focuses on handling\nconcurrent writes without affecting the query performance.\nVery differently, Galakatos et al. [3] exploited the piecewise\nlinear function as the learned model to build a data-aware\nindex FITing-tree that replaces leaf nodes of B+-tree with\nthe learned piecewise linear functions. Unlike FITing-tree ,\nFerragina et al. [4] introduced a pure learned index PGM-index\nthat does not mix the traditional data structure and learned\nmodel. However, their work still focuses on one-dimensional\ndata and uses the existing linear learned model.\nNaturally, the idea of the learned index has been extended to\nspatial and multi-dimensional data. Wang et al. [7] proposed\na learned index ZM-index for spatial queries. In that work, the\nauthors utilized the Z-order curve to convert two-dimensional\ndata into one-dimensional values, and then applied a machine\nlearning model to predict a key‚Äôs position on one-dimensional\ndata. Qi et al. [5] reÔ¨Åned the idea of ZM-index and built a\nrecursive spatial model index (RSMI). Before applying Z-order\ncurve, their work adopts a rank space-based transformation\ntechnique to mitigate the uneven-gap problem. LISA [9] is\na disk-based spatial learned index that achieves low storage\nconsumption and I/O cost. In this work, the authors used a\nmapping function to map spatial keys into one-dimensional\nvalues and a monotone shard prediction function, which is sim-\nilar to the piecewise linear functions, to predict the shard id for\na given mapped value. Extending to multi-dimensional data,\nML-index [8] is an RMI based learned index. It Ô¨Årst converts\nthe multi-dimensional data into one dimension by employing\nthe i-Distance technique. Based on the one-dimensional data,\nML-index uses the RMI to estimate the approximate position\nof a search key. Recently proposed index Flood [6] can also\nsupport multi-dimensional data and is very relevant to our\nQuery \nWorkload ùì¶Dataset ùìì\nSpatial Interpolation \nFunction ùìïùíäùíèCost Model\nID Info\nTableùì£\nPredict \nCell IDQueriesùíè= 5ùíé= 6\nGrid Layout ùëÆùíè√óùíé\nLocate CellLocal Binary \nSearch\nQuery StrategiesRange Query\nùíåNN QueryResultIndex Building\nQuery ProcessingPreprocess\nMapSelect \nPivotSortFig. 1. System Architecture of SPRIG\nwork. It adopts the FITing-tree as the building block to predict\nthe key‚Äôs position on a single dimension. By integrating d\u00001\ndimensions‚Äô positions, where dis the number of dimensions,\nFlood can locate the cell that covers the search key.\nIII. B ACKGROUND\nBefore delving into the details of SPRIG, in this section,\nwe introduce three basic concepts: 1) Spatial Interpolation\nFunction; 2) Error Guarantee; and 3) Pivot Based Filtering,\nwhich serve as the building blocks of the proposed index.\nA. Spatial Interpolation Function\nGiven a set of 2-dimensional sample points f(xi;yi)j1\u0014\ni\u0014ngand their corresponding values fvi=f(xi;yi)j1\u0014i\u0014\nng, one can construct a spatial (two-dimensional) interpolation\nfunction Fin=f(x;y)that passes through all these sample\npoints [10]. Afterward, given any point (x;y), it is easy to\nestimate the value of f(x;y)with the interpolation function.\nBorrowing the idea from the learned index [1], if we treat vi\nas the position of the point (xi;yi), we can use Finto quickly\nestimate the position of any given point. Moreover, if the\nsample points are not random but can represent the distribution\nof the original spatial dataset, we can use fewer sample points\nto Ô¨Åt the spatial interpolation function for estimating positions.\nIt indicates that the spatial interpolation function can learn\nthe spatial position distribution with a lower storage overhead,\nwhich Ô¨Åts well with the goal of the learned index. Therefore,\nit is feasible and promising to exploit the spatial interpolation\nfunction Finas the learned model.\nB. Error Guarantee\nFor a learned index, it is essential to conduct a local search\nafter predicting the position of a given point. Generally, the\nlocal search range is [pos\u0000eg; pos +eg], whereposis the\npredicted position and egis the estimation error, also called\nerror guarantee. Consequently, egis an essential concept in\na learned index. Different from FITing-tree [3] (used in\nFlood [6]), we adopt the maximum estimation error as the\nerror guarantee in our scheme, which can be determined by a\nquery workloadW.\neg=max(Fin(qx;qy)\u0000f(qx;qy)); (1)\nwhere (qx;qy)2W . Regarding a spatial point, its spatial po-\nsition can be determined by its xandycoordinates. Therefore,\nin our scheme, we project the estimated spatial position to x\n\ndimension and ydimension and obtain two error guarantees,\ni.e,\negx=max(Px(Fin(qx;qy))\u0000P x(f(qx;qy)));\negy=max(Py(Fin(qx;qy))\u0000P y(f(qx;qy)));\nwherePx() andPy()project a spatial position to xdimension\nandydimension, respectively.\nC. Pivot Based Filtering\nAssume that a set Dcontainsn2-dimensional points, i.e,\nD=fpi= (xi;yi)j1\u0014i\u0014ng. Also, we deÔ¨Åne a distance\nbased range query Qc=fqc;rg, whereqcis a point, and ris\na radius. Launching a query QcoverDmeans one would\nlike to Ô¨Ånd points in Dthat satisfy d(pi;qc)\u0014r, where\nd(\u0001)calculates the Euclidean distance between two spatial\npoints. The intuitive solution is to scan and check points in\nDone by one. However, it is inefÔ¨Åcient. Although we can\nbuild tree based index structures, such as k-d tree and M-\ntree [11], to speed up the query processing, it will incur much\nextra storage. In order to improve the performance of Qc\nwithout introducing too much storage overhead, we adopt a\npivot based Ô¨Åltering technique. The key idea is to select a\nvirtual pivot pvforDand calculate distances d(pi;pv)for\neach pointpi. Then, sortDaccording to the corresponding\nd(pi;pv). When performing a query QcoverD, we can only\ncalculate the distance d(qc;pv)and check the points whose\ndistancesd(pi;pv)lie in [d(qc;pv)\u0000r;d(qc;pv) +r], instead\nof all points inD. This optimization technique is based on\ntriangle inequality, and its correctness is as follows:\njd(pi;pv)\u0000d(qc;pv)j\u0014d(pi;qc)\u0014r\n)\u0000r\u0014d(pi;pv)\u0000d(qc;pv)\u0014r\n)d(qc;pv)\u0000r\u0014d(pi;pv)\u0014d(qc;pv) +r:\nThe above inequality indicates that if pifalls within Qc, it\nmust satisfy d(pi;pv)2[d(qc;pv)\u0000r;d(qc;pv) +r]. Thus,\nwe can narrow down the scan range from the whole dataset\nDto the points that have distances d(pi;pv)in[d(qc;pv)\u0000\nr;d(qc;pv) +r].\nIV. O UR PROPOSED INDEX -SPRIG\nIn this section, we present the details of our proposed index,\nSPRIG. Fig. 1 depicts the system architecture of our index,\nwhich is comprised of two parts: index building and query\nprocessing. In the following, we Ô¨Årst discuss how to build the\nlearned index. Then, we describe the detailed query processing\non our index.\nA. Index Building\nOur SPRIG mainly consists of three components: 1) An\nn\u0002mgrid layout Gn\u0002m, wherenis the number of columns\nalongxdimension and mis forydimension; 2) A table T; and\n3) A spatial interpolation function Finbased learned model,\nas shown in Fig. 1. Here, we may use some parameters for\nour index, which will be further discussed in Section IV-C. To\nbuild then\u0002mgrid layout, we Ô¨Årst Ô¨Ånd n\u00001boundaries on\nxdimension to generate nnon-equal size columns, and addthese boundaries into a set Bx. Our goal is to make data records\nevenly distributed across columns, i.e., each column has a\nroughly equal number of records (in this paper, we use ‚Äúpoint‚Äù\nand ‚Äúrecord‚Äù interchangeably). For ydimension, there will be\nm\u00001boundaries in set By. We denote the maximum and\nminimum values in xdimension asfxmin;xmaxg, while those\narefymin;ymaxgforydimension. After adding fxmin;xmaxg\nintoBxandfymin;ymaxgintoBy, we separately sort BxandBy\nin an increasing order, where jBxj=n+ 1andjByj=m+ 1.\nTotally, there are n\u0002mcells for the grid, and we have\nGn\u0002m= (Bx;By). Algorithm 2 shows the process of building\nthe grid.\nNext, we allocate integers in the range [0, n\u0002m\u00001] as\ncell ids along xdimension and deÔ¨Åne a 2-dimensional array\nCidto index these cell ids: Cid[i][j] =j\u0001n+i;0\u0014i<n and\n0\u0014j <m . Afterward, we can build a table Tto map the cell\nid to the covered records, in which the key is the cell id and\nthe value is a pair (Ô¨ÅrstAddress;size)indicating the pointer to\nthe Ô¨Årst record and the number of records in the cell. Based on\nGn\u0002mandCid, we can Ô¨Åt a spatial interpolation function Fin.\nIn particular, we treat fBx;Bygas inputs and Cidas the desired\nestimation values to determine Fin, i.e., Cid Fin(Bx;By).\nBesides, to speed up the distance related search, for example,\nkNN queries, we employ the pivot based Ô¨Åltering technique,\nwhich drives us to select a pivot pvin a cell and sort the\nrecords (in a cell) according to the distances d(pi;pv). In this\npaper, we adopt the k-means clustering to select pivots for\neach cell. It is worth noting this technique does not incur much\nextra storage except for pivot points.\nB. Query Processing\nIn this paper, we focus on the range and kNN queries, in\nwhich the point query is implicitly included. Next, we describe\nour approaches.\nRange Query . For spatial data, the range query identiÔ¨Åes\nthe recordspithat fall within Qr=(qb;qt), whereqb= [bx;by]\nandqt= [tx;ty]are the bottom left and top right point of\na query window, respectively. The main idea of processing a\nrange query is to convert it to locate the cell ids of qband\nqt. Here, we take locating qb‚Äôs cell id as an example, and the\nsteps are as follows.\n\u000fStep-1 . Givenqb, we can use Finto obtain a predicted\ncell id pidb. It is simple to calculate the locations of pidb\nin set BxandBy. That is,lpx=pidbmodnandlpy=\npidb= n.\n\u000fStep-2 . Given a pair of error guarantee (egx;egy), by\napplying local binary searches on BxandBy, we can\nobtain the real xandylocations of the search point,\ndenoted as lrxandlry. The search range on Bxset is\n[lpx\u0000egx; lpx+egx], while it is [lpy\u0000egy; lpy+egy]\nonByset.\n\u000fStep-3 . Finally, we can obtain the real cell id of qb,\nnamely, ridb=lry\u0001n+lrx.\nAlgorithm 1 formally outlines the above steps to obtain the\nreal cell id of a search point. Similarly, we can get the real\n\n050100150200250300350400450500\n0102030405060708090100\nùíôdimensionùíödimensionùíëùíäùíÖùíï\nùíëùíäùíÖùíÉùíìùíäùíÖùíÉùíìùíäùíÖùíï\nùííùíÉùííùíï\n0 5 20 70 86 100\n0 52 120 355 430 500ùêÅùê±ùíçùíëùíÉùíô‚àíùíÜùíàùíô ùíçùíëùíÉùíô+ùíÜùíàùíô\nùêÅùíö maxùíçùíëùíÉùíö‚àíùíÜùíàùíö,ùüéùíçùíëùíÉùíö+ùíÜùíàùíö\nùíçùíëùíÉùíô=ùíëùíäùíÖùíÉmod 5\nùíçùíëùíÉùíö=ùíëùíäùíÖùíÉ/   5ùíìùíäùíÖùíÉ=ùüè√óùüì+ùüè=ùüî(a) Range Query\n050100150200250300350400450500\n0102030405060708090100\nùíôdimensionùíödimensionùêúùê≠ùêúùê´\nùêúùêõùêúùê•ùííùíå ùìê\nùìëùùàID Info\n13 (firstAddress ,size=t ),  ùíëùíó\nt\nùíÖùííùíå,ùíëùíó‚àíùùàùíÖùííùíå,ùíëùíó+ùùà\nputQueuePivot Based Filtering (b)kNN Query\nFig. 2. Processing queries on SPRIG. A 5 \u00025 grid layout. (a) Range Query. The red dashed rectangle indicates the range query window, and the yellow\ncells are estimated with Fin. Assume that ( egx=2,egy=1), we take Ô¨Ånding ridbas an example. (b) kNN Query. Spread the search range to the outer-layer\nadjacent cells. The hollow circles represent the closest point of cell Aand cellBtoqk. Assume that cellBhastrecords, we use the pivot based Ô¨Åltering to\nprocess the records in the cell.\nAlgorithm 1 GetRealCellId\nInput: A given point (x; y); The grid layout, Gn\u0002m= (Bx;By); A\npredicted cell id of the given point, pid; Trained error guarantee\neg= (egx;egy);\nOutput: The real cell id ridof the given point (x; y).\n1:n jBxj\u00001\n2:lpx pidmod n; lpy pid = n\n3:lrx BinarySearch( Bx,x,lpx\u0000egx,lpx+egx)\n4:lry BinarySearch( By,y,lpy\u0000egy,lpy+egy)\n5:return lry\u0001n+lrx\ncell id ridtofqt. After that, the range query result can be\ncollected with the following strategy:\n1) For cells intersected by query window Qr, we scan the\nrecords in these cells and put the records that fall within Qr\ninto the range query result.\n2) For cells contained inside query window Qr, we directly\nadd all the records covered in these cells into the result.\nAs shown in Fig. 2(a), the intersected cells must be on\nthe vertical and horizontal lines of ridbandridt. Therefore,\nit is easy to determine the intersected cells (gray area) and\ncontained cells (blue area).\nkNN Query . In this paper, we denote the kNN query as\nQkNN= (qk;k), whereqkis a point, and kis the number of\nnearest neighbors. The formal deÔ¨Ånition is: 8pi2S;8pj2\nDnS;d(qk;pj)\u0015d(qk;pi), whereSis the result set of kNN\nquery. ForkNN queries, our solution is to locate the real cell\nid of the query point qk. Then, starting from the cell, we\nrecursively spread the search range and incrementally check\nthe records in the outer-layer adjacent cells until the result\nis complete. To facilitate improved performance, we employ\ntwo pruning techniques. One is the closest point pruning\ntechnique that is used to determine whether a cell should\nbe checked. A cell may have different closest points to the\ndifferent query points. However, in our scheme, since it is\nsimple to obtain the locations (lrx;lry)of the located real\ncell, we can get the closest point of the cell, denoted as pc,\nwith the help of BxandByeasily. In Fig. 2(b), cell Ahas\nthe closest point (Bx[lrx];By[lry+1]) , and the cellB‚Äôs closest\npoint is (Bx[lrx+ 1];qk:x). The other pruning technique is\nthe pivot based Ô¨Åltering that can Ô¨Ålter out records in a cell\nby the principle of the triangle inequality. By employing theAlgorithm 2 Build Grid\nInput: A spatial dataset, D; The number of columns along x\ndimension, n; The number of columns along y dimension, m.\nOutput: A grid layout Gn\u0002m= (Bx;By).\n1:mapX ?; mapY ?\n2:foreach entry inDdo\n3: cntX (mapX :get(entry:x)is?) ? 1: mapX :get(entry:x)+1\n4: mapX :put(entry:x;cntX)\n5: cntY (mapY:get(entry:y)is?) ? 1: mapY:get(entry:y) + 1\n6: mapY:put(entry:y;cntY)\n7:(xmin; ymin) Ô¨ÅndMin (D);(xmax; ymax) Ô¨ÅndMax (D)\n8:mapX .sortByKey(); mapY .sortByKey() #Ascending\n9:avgX D:size=n;avgY D:size=m\n10:Bx getBoundary (mapX ;avgX; xmin; xmax)\n11:By getBoundary (mapY;avgY; ymin; ymax)\n12:return (Bx;By)\n13:\n14:function getBoundary (map;avg;min;max)\n15: B ?;cnt 0;pre 0\n16: B:add(min)\n17: foreach entry in map do\n18: singleCnt entry.value\n19: ifsingleCnt >avgthen\n20: B:add(entry.key +pre\n2)\n21: pre entry.key\n22: cnt 0\n23: continue\n24: cnt cnt+singleCnt\n25: ifcnt>avgthen\n26: B:add(entry.key +pre\n2)\n27: cnt 0\n28: else\n29: pre entry.key\n30: B:add(max)\n31: return B\nspreading outwards strategy, closest-point pruning, and pivot\nbased Ô¨Åltering, our kNN solution works as follows and is\nformally depicted in Algorithm 3:\n\u000fStep-1 . Locate the real cell of qk. With Finand local\nbinary search, we obtain a real cell id ridof the query\npointqk, which is shown in line 1 andline 2 .\n\u000fStep-2 . Calculate the distances from qkto the borders of\nthe located cell. We denote them as ct,cb,cl, andcr, as\nshown in Fig. 2(b). Then, we obtain a radius r=min(ct,\ncb,cl,cr). See line 6 -line 8 for this step.\n\nAlgorithm 3 kNN Query\nInput: AkNN query QkNN= (qk; k); Our index, Gn\u0002m= (Bx;By),\nT, and Fin; Trained error guarantee eg= (egx;egy).\nOutput: A priority queue queue contains the kclosest points.\n1:pid Fin(qk:x; q k:y);\n2:rid getRealCellId (qk:x; q k:y;pid;Bx;By;eg)\n3:lrx ridmod n;lry rid=n\n4:ec 0;kcnt 0;queue ?;\n5:while kcnt< k do\n6: cb qk:y\u0000By[lry\u0000ec];ct By[lry+ 1 + ec]\u0000qk:y\n7: cl qk:x\u0000Bx[lrx\u0000ec];cr Bx[lrx+ 1 + ec]\u0000qk:x\n8: r min(cb; ct; cl; cr)\n9: ifec>0then\n10: cells collectAdjacent (qk;Gn\u0002m;T; ec)\n11: foreach cell in cells do\n12: ifd(qk; cell:p c)<queue.peek.dist then\n13: records PivotFilter (cell; qk;queue )\n14: foreach entry in records do\n15: putQueue (entry;queue ; qk; kcnt; r; k)\n16: else\n17: foreach entry inT:get(rid)do\n18: putQueue (entry;queue ; qk; kcnt; r; k)\n19: ec ec+ 1\n20:\n21:function putQueue (entry;queue ; qk; kcnt; r; k )\n22: dist getDisance (qk;entry)\n23: ifqueue:size < k then\n24: queue.offer(entry)\n25: ifdist\u0014rthen\n26: kcnt kcnt+ 1\n27: else if queue.peek().dist > dist then\n28: queue.poll()\n29: queue.offer(entry)\n30: ifdist\u0014rthen\n31: kcnt kcnt+ 1\n\u000fStep-3 . Scan records in the cell. After Ô¨Åltering the records\nby the pivot based Ô¨Åltering technique, we put a record\npiinto a priority queue if the queue‚Äôs size is less than\nkord(pi;qk)< \u001b , where\u001bis the distance between\nqueue.peek andqk. Besides, we use a counter kcntto\nrecord the result size and increase it if d(pi;qk)\u0014r.\nIn Algorithm 3, we use a separate function putQueue()\n(line 21\u0000line 31 ) to show the details of this step.\n\u000fStep-4 . Ifkcnt< k , we expand the search range and\ncalculate a new radius r=min(ct,cb,cl,cr), wherect,\ncb,cl,crare the distances from qkto the borders of\nouter layer adjacent cells. For each adjacent cell, we Ô¨Årst\ncheck whether d(pc;qk)> \u001b. If yes, we skip the cell.\nOtherwise, perform Step-3 . As shown in Fig. 2(b), we\nwould skip cellAand further process cell B.\nRepeat Step-4 and Step-3 untilkcnt\u0015k. Eventually, the\npriority queue holds the result of kNN query. Note that,\nsince the collectAdjacent() and PivotFilter() functions are\nstraightforward, we do not present them in Algorithm 3 due\nto the limited space.\nC. Cost Model\nIn this section, we build a cost model, which can be used\nto determine the number of columns in xdimension and y\ndimension, i.e., the values of nandm. Motivated by [6],we model the execution time of performing a query over\nour index. Here, we take the range query workload as an\nexample. From Section IV-B, we know that the query time\nof range queries consists of four parts: 1) Prediction; 2)\nLocal binary search; 3) Retrieve cells; 4) Scan and check data\npoints in the intersected cells. Clearly, different nandmwill\ngenerate different Bx;By. Consequently, the execution time of\nFinand the local binary search will be affected. For ease\nof description, we denote the execution time of the spatial\ninterpolation function as T(Fn\u0002m\nin)and the execution time\nof local binary search as T(Bn\u0002m). Assume there are Ni\nintersected cells and Nccontained cells fall within the range\nquery Qr. Meanwhile, we deÔ¨Åne Npas the data points in\nthe intersected cells. Through simulations, we can obtain the\naverage time of retrieving a cell and scanning a data point,\nwhich are denoted as TrandTs, respectively. Putting these\nfour parts together, the time cost of performing a rang query\nis modeled as:\nTime =T(Fn\u0002m\nin) +T(Bn\u0002m) +Tr\u0001(Ni+Nc) +Ts\u0001Np:\nGiven a datasetDand a query workload W, where Qr2W ,\nwe expect to obtain the best layout parameter: n\u0002mthat\nmakes Time have minimal average value.\nV. E VALUATION\nIn this section, we experimentally evaluate the performance\nof our index and compare it with alternative schemes in pro-\ncessing range and kNN queries. SpeciÔ¨Åcally, we Ô¨Årst explore\nthe efÔ¨Åciency and accuracy of the typical spatial interpolation\nfunctions. Then, we compare SPRIG with traditional indices\nand Flood [6], which is a recently proposed multi-dimensional\nlearned index, in terms of range and kNN query time and\nstorage overheads. We implemented all indexes in Java and\nevaluated them with in-memory versions. To analyze the\nimpact of dataset size on index performance, we adopt three\nTwitter datasets [12] consisting of tweets with their locations:\nTweet200k ,Tweet2M , and Tweet20M that have 200k, 2M,\nand 20M spatial points, respectively. All experiments are\nconducted on a machine with 16 GB memory and 3.4 GHz\nIntel(R) Core(TM) i7-3770 processors and running Ubuntu\n16.04 OS.\nEfÔ¨Åciency and Accuracy of Spatial Interpolation Func-\ntions . We evaluate Ô¨Åve spatial (two-dimensional) interpola-\ntion functions on adaptive grids, i.e., bilinear interpolation ,\nbicubic interpolation ,piecewise bicubic interpolation ,Shepard\ninterpolation , and radial based function (RBF) interpolation\n[13]. Given a query workload, we expect to evaluate the\naverage execution time and maximum estimation errors of\nthese Ô¨Åve functions varying grid layout from 10\u000210to\n200\u0002200. It is noted that, for ease of comparison, we adopt\neg(Eq. (1)) instead of egxandegyto evaluate the accuracy\nofFin. All of these interpolation functions are evaluated on\nTweet200k . Fig. 5(a) shows the average execution time for\nthree interpolation functions: bilinear ,bicubic , and piecewise\nbicubic , while Fig. 5(b) presents their accuracy. We exclude the\nShepard andRBF interpolation functions, for which they have\n\n0.1% 0.5% 1.0% 1.5% 2.0%\nSelectivity0255075100125150175200Average query time (ms)R-tree \nk-d tree\nFlood\nSPRIG(a) Tweet200K\n0.1% 0.5% 1.0% 1.5% 2.0%\nSelectivity025050075010001250150017502000Average query time (ms)R-tree \nk-d tree\nFlood\nSPRIG (b) Tweet2M\n0.1% 0.5% 1.0% 1.5% 2.0%\nSelectivity051015202530Average query time (s)R-tree \nk-d tree\nFlood\nSPRIG (c) Tweet20M\nFig. 3. Average query time of range query over different datasets\n4 8 16 32 64\nk0100200300400500Average query time (ms)M-tree \nk-d treeFlood\nSPRIG\n(a) Tweet200K\n4 8 16 32 64\nk025050075010001250150017502000Average query time (ms)M-tree \nk-d treeFlood\nSPRIG (b) Tweet2M\n4 8 16 32 64\nk025050075010001250150017502000Average query time (ms)M-tree \nk-d tree\nSPRIG (c) Tweet20M\nFig. 4. Average query time of kNN query over different datasets. Note that, the average query time of Flood is individually shown in Table II due to its\nsigniÔ¨Åcant execution time.\nTABLE I\nSHEPARD AND RBF I NTERPOLATION FUNCTIONS\nMetrics Functions 10\u000210 20\u000220 50\u000250 100\u0002100 200\u0002200\nExecution time (ms) Shepard 6.7 25.8 161.2 646.3 2595.7\nExecution time (ms) RBF 6.1 10.3 60.9 243.3 Out of memory\nEstimation Error Shepard 60 223 1322 5195 20587\nEstimation Error RBF 1815 6497 30813 116772 Out of memory\n10x10 20x20 50x50 100x100 200x200\nGrid Layout024681012Average execution time (ms)\nPiecewise bicubic\nBicubic\nBilinear\n(a)\n10x10 20x20 50x50 100x100 200x200\nGrid Layout02004006008001000Maximum Estimation Error\nPiecewise bicubic\nBicubic\nBilinear (b)\nFig. 5. Spatial Interpolation Functions. (a) Average Execution Time. (b)\nMaximum Estimation Error\nmuch larger execution time and estimation errors. However,\nwe still list their average execution time and estimation errors\nin Table I. From Fig. 5(a), 5(b), and Table I, we can see\nthat the bilinear interpolation function has the best efÔ¨Åciency\nand accuracy in all grid layouts. Therefore, in the following\ncomparisons, we apply bilinear interpolation function in our\nindex. It is interesting that the maximum estimation error of\nbilinear interpolation function is always n+ 1, which makes it\nquite suitable to reduce the execution time of the local search.\nComparison on Range Query. For range queries, we\ncompare our proposed index with other multi-dimensional\nindexes that support range queries over the aforementioned\nthree datasets. We choose R-tree, k-d tree, and Flood as the\ncompeting indexes, in which the Ô¨Årst two are representativetraditional indexes, and the third one is a state-of-the-art\nlearned index and similar to our work. In addition, we consider\nÔ¨Åve sets of queries with different selectivities f0.1%, 0.5%,\n1%, 1.5%, 2%g. Aggregating these query sets as a big query\nworkload, we tune our index by the cost model (See details\nin Section IV-C) to obtain the best grid layout. To be fair,\nwe also train Flood by the approach proposed in [6], which\ncan obtain the best layout and error guarantee. Similarly, the\ntraditional indices, R-tree and k-d tree, are tuned to obtain their\nbest parameter, i.e., the maximum number of children for a\nnode. Therefore, all indexes participating in the comparison are\nevaluated with their best parameters. From Fig. 3, we can see\nthat SPRIG is always signiÔ¨Åcantly faster than the traditional\nindexes in all selectivities, which is more advantageous on big\ndata set. For example, on the Tweet20M dataset (Fig. 3(c)), our\nindex achieves up to an order of magnitude better performance\nthan k-d tree for range queries. Besides, our index outperforms\nFlood in most cases. This beneÔ¨Åt comes from the fact that\nFlood only learns the distribution of one dimension, while\nSPRIG learns the spatial distribution, which allows us to do\nmore Ô¨Åne-grained Ô¨Åltering.\nComparison on kNN Query. ForkNN queries, we replace\nR-tree with M-tree [11]. It is because R-tree is much slower\nthan other indexes in kNN queries, and M-tree is a typical\nkNN-support index. Since Flood does not provide any detail\n\nabout how to deal with kNN queries, we implement it with a\nsimilarkNN search strategy as our index. In the kNN query\ncomparison, we consider k=f4;8;16;32;64g. Similar to the\nrange query, we tune all the comparison indexes to obtain their\nbest parameters. From Fig. 4, we can see that M-tree is more\nexpensive than our index in executing kNN queries. For Flood,\nour index outperforms this learned index on all datasets. Since\nFlood is much slower than other indexes on the Tweet20M\ndataset, we exclude it from Fig. 4(c) and list its performance\nas follows. As shown in Fig. 4 and Table II, our index is\nTABLE II\nAVERAGE QUERY TIME OF FLOOD ON Tweet20M\nk 4 8 16 32 64\nQuery Time (ms) 4590 4796 4818 4778 5073\naround 2\u0002, 5\u0002, and 9\u0002faster than Flood on the Tweet200k ,\nTweet2M , and Tweet20M dataset, respectively. For k-d tree,\nour index is at least 2 \u0002faster on the Tweet200k dataset and\nis slightly faster on the Tweet2M dataset. For the Tweet20M\ndataset, although our index still has a huge advantage over M-\ntree and Flood in the query performance, it is not as good as\nk-d tree. However, k-d tree has a signiÔ¨Åcantly higher storage\nfootprint to achieve such query performance. We demonstrate\nthe storage overheads of these indexes in the next section.\nStorage Overhead. Due to the limited space, we only\ncompare the storage overheads of different indexes on the\nTweet20M dataset, which actually has a similar relationship\nbetween indexes on the other two datasets. In the range and\nkNN query comparisons, we use the corresponding query\nworkload to tune these indexes for their best parameters. For\nexample, our index has the best grid layout 710\u0002690on rang\nquery workload, while it is 3000\u000210forkNN query workload.\nSince different parameters of one index may lead to different\nstorage overheads, we compare the storage overheads of these\nindexes on the range and kNN query workloads, separately, as\nshown in Table III and Table IV, respectively. To obtain the\nstorage footprint of a tree index, we evaluate the necessary\nstorage of one node, e.g., the minimum bounding rectangle\n(MBR) of R-tree, and count the total number of internal nodes\nby traversing the tree. For Flood, it has two components:\naFITing-tree on one dimension and a table to map a cell\nto the covered data records. For our index, there are three\ncomponents: a grid layout Gn\u0002m, a tableTfor managing\nthe cells, and a spatial interpolation function Fin. The storage\nconsumption of these two learned indexes is related to their\ngrid layout. Table III and Table IV show that both the\nlearned indexes have less storage overheads than traditional\nindexes on the range and kNN query workloads. Although our\nindex consumes more storage compared to Flood, we achieve\nbetter query performance with an acceptable storage overhead.\nRecall that in the kNN query time comparison, our index is\nslower than k-d tree on Tweet20M . However, as shown in\nTable IV, the storage overhead of k-d tree is three orders\nof magnitude larger than that of our index, which renders\nit challenging to use in practice. Thus, our index is moreTABLE III\nSTORAGE OF INDEXES ON RANGE QUERY WORKLOAD\nIndex R-tree k-d tree Flood SPRIG\nStorage Overhead (MB) 7.95 305.17 0.11 6.30\nTABLE IV\nSTORAGE OF INDEXES ON kNN QUERY WORKLOAD\nIndex M-tree k-d tree Flood SPRIG\nStorage Overhead (MB) 65.89 305.17 0.05 0.60\npractical than k-d tree with big datasets.\nVI. C ONCLUSION\nIn this paper, we have proposed a new learned model that\ncan learn the spatial distribution of the spatial data directly.\nBased on the learned model, we have built a novel learned\nspatial index SPRIG and designed the range and kNN query\nexecution strategies over the index. Our experimental results\nsuggest that 1) the bilinear interpolation function is the best\noption as the spatial learned model compared with the other\nspatial interpolation functions; 2) our index SPRIG is efÔ¨Åcient\nwith the relatively small storage footprint. In our future work,\nwe expect to further reduce the average query time of kNN\nqueries on big datasets and make our index more Ô¨Çexible.\nREFERENCES\n[1] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, ‚ÄúThe case\nfor learned index structures,‚Äù in SIGMOD , 2018, pp. 489‚Äì504.\n[2] C. Tang, Y . Wang, Z. Dong, G. Hu, Z. Wang, M. Wang, and H. Chen,\n‚ÄúXindex: a scalable learned index for multicore data storage,‚Äù in SIG-\nPLAN , 2020, pp. 308‚Äì320.\n[3] A. Galakatos, M. Markovitch, C. Binnig, R. Fonseca, and T. Kraska,\n‚ÄúFiting-tree: A data-aware index structure,‚Äù in SIGMOD , 2019, pp.\n1189‚Äì1206.\n[4] P. Ferragina and G. Vinciguerra, ‚ÄúThe pgm-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds,‚Äù VLDB ,\nvol. 13, no. 8, pp. 1162‚Äì1175, 2020.\n[5] J. Qi, G. Liu, C. S. Jensen, and L. Kulik, ‚ÄúEffectively learning spatial\nindices,‚Äù VLDB , vol. 13, no. 12, pp. 2341‚Äì2354, 2020.\n[6] V . Nathan, J. Ding, M. Alizadeh, and T. Kraska, ‚ÄúLearning multi-\ndimensional indexes,‚Äù in SIGMOD , 2020, pp. 985‚Äì1000.\n[7] H. Wang, X. Fu, J. Xu, and H. Lu, ‚ÄúLearned index for spatial queries,‚Äù\nin2019 20th IEEE MDM . IEEE, 2019, pp. 569‚Äì574.\n[8] A. Davitkova, E. Milchevski, and S. Michel, ‚ÄúThe ml-index: A multidi-\nmensional, learned index for point, range, and nearest-neighbor queries.‚Äù\ninEDBT , 2020, pp. 407‚Äì410.\n[9] P. Li, H. Lu, Q. Zheng, L. Yang, and G. Pan, ‚ÄúLisa: A learned index\nstructure for spatial data,‚Äù in SIGMOD , 2020, pp. 2119‚Äì2133.\n[10] L. Mitas and H. Mitasova, ‚ÄúSpatial interpolation,‚Äù Geographical infor-\nmation systems: principles, techniques, management and applications ,\n1999.\n[11] P. Ciaccia, M. Patella, and P. Zezula, ‚ÄúM-tree: An e cient access method\nfor similarity search in metric spaces,‚Äù in VLDB . Citeseer, 1997, pp.\n426‚Äì435.\n[12] https://developer.twitter.com/en, 2018.\n[13] D. E. Myers, ‚ÄúSpatial interpolation: an overview,‚Äù Geoderma , 1994.",
  "textLength": 36429
}