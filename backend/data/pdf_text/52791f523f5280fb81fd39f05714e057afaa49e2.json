{
  "paperId": "52791f523f5280fb81fd39f05714e057afaa49e2",
  "title": "Running Neural Networks on the NIC",
  "pdfPath": "52791f523f5280fb81fd39f05714e057afaa49e2.pdf",
  "text": "Running Neural Network Inference on the NIC\nGiuseppe Siracusano\nNEC Laboratories EuropeSalvator Galea\nUniversity of CambridgeDavide Sanvito\nNEC Laboratories Europe\nMohammad Malekzadeh\nImperial College LondonHamed Haddadi\nImperial College London\nGianni Antichi\nQueen Mary University of LondonRoberto Bifulco\nNEC Laboratories Europe\nAbstract\nIn this paper we show that the data plane of commodity\nprogrammable Network Interface Cards (NICs) can run\nneural network inference tasks required by packet moni-\ntoring applications, with low overhead. This is particularly\nimportant as the data transfer costs to the host system and\ndedicated machine learning accelerators, e.g., GPUs, can\nbe more expensive than the processing task itself. We\ndesign and implement our system - N3IC - on two dif-\nferent NICs and we show that it can greatly beneﬁt three\ndifferent network monitoring use cases that require ma-\nchine learning inference as ﬁrst-class-primitive. N3IC\ncan perform inference for millions of network ﬂows per\nsecond, while forwarding trafﬁc at 40Gb/s. Compared to\nan equivalent solution implemented on a general purpose\nCPU, N3IC can provide 100x lower processing latency,\nwith 1.5x increase in throughput.\n1 Introduction\nWith the slowdown of Moore’s law [ 22,13,23], the archi-\ntecture of modern servers is evolving to include an increas-\ning number of domain-speciﬁc co-processors dedicated to\ntasks such as cryptography, video processing and machine\nlearning [ 26]. Network Interface Cards (NICs) are also\nfollowing the same trend and now integrate programmble\ncomponents, i.e., Systems-on-Chip (SoC), FPGA, within\ntheir hardware data path [ 16,54,44,20] to lower the pres-\nsure on the host CPU, which is usually the bottleneck [ 57].\nin-network end-host\non-switch\n(pForest, IISY )network -attached \ndedicated accelerator\n(Brainwave, Taurus )commodity \nprogrammable\nSmartNIC\n(N3IC )host -attached \naccelerator\n(TPU, GPU)Figure 1: Design space for ofﬂoading machine learning\ninference tasks from the end-host’s CPU.\nSeveral efforts have already took advantage of end-host\nhardware programmability to improve data center scalabil-\nity [15,16], applications performance [ 12] or speciﬁc func-\ntions such as load balancing [ 2], consensus protocols [ 35]\nand key-value store [ 39]. Lately, an increasing number of\nnetworking-related applications require machine learning\ninference as ﬁrst-class-primitive [ 14,42]. However, the\ncomplex operations required by machine learning algo-\nrithms, specially deep neural networks, often do not allow\nsuch applications to effectively leverage programmable\nNICs [18].\nIn response to this, we design, implement, and eval-\nuateN3IC , a solution for adding a lightweight neural\nnetwork (NN) inference support to the list of tasks that\ncan be efﬁciently run on commodity NICs. This idea\naligns with the current trend of research that moves ma-\nchine learning capabilities into programmable hardware,\nbut takes a novel direction (See Figure 1). Recent ef-\nforts focused either on implementing basic machine learn-\ning algorithms, e.g., decision trees, using programmable\nswitches [ 8,79], or deploying expensive and dedicated\nfully-ﬂedged network-attached accelerators (e.g., Brain-\nWave [ 49] and Taurus [ 70]). In N3IC , we design and\n1arXiv:2009.02353v1  [cs.DC]  4 Sep 2020\n\nimplement a NN executor that mainly uses already avail-\nable hardware in the data plane of off-the-shelf pro-\ngrammable NICs, instead. NNs are general-purpose ma-\nchine learning algorithms that suite a growing set of ap-\nplications [ 80,37,36], but they are widely considered too\nresource-hungry and computationally complex to run on\ncommodity NICs [ 8,79]. We demonstrate that modern pro-\ngrammable NICs can run NN inference, thereby ofﬂoading\na signiﬁcant amount of computation from the CPUs, and\nenabling applications with strict timing constraints. In\nfact, we show that PCI Express (PCIe) data transfer over-\nheads [55] to the host system can be more expensive than\nthe actual NN inference process ( x2).\nWe share the challenges of implementing NN executors\ntaking into account both resource and computational con-\nstraints of today’s commodity programmable NICs ( x3).\nWe then report the design of N3IC (x4) on two different\nhardware targets: a System-on-Chip (SoC) based NIC from\nNetronome [ 53] , and an FPGA-accelerated NIC (NetF-\nPGA [ 83]). Our solution relies on a quantization technique\nknown as binarization [ 10], which reduces the number of\nbits used to represent NN’s inputs and parameters. The\nresulting NN, i.e., Binary Neural Network (BNN), has\nlow memory requirements in the order of KBytes, and its\ncomputation can be performed using much lighter math-\nematical operations with small impact on the considered\nuse cases’ inference accuracy ( x5). We provide three im-\nplementations of N3IC . First, leveraging existing NIC\nprogramming languages’ primitives, using microC for the\nNetronome NIC and P4 for the NetFPGA. In this last case,\nwe developed a compiler that derives fully functional P4\ncode from a given NN architecture, which can be then\nsynthetized in hardware with the P4-NetFPGA framework.\nFinally, we provide a third implementation that realizes\nNN inference with a dedicated hardware circuitry, which\ncan be exposed as a new primitive to the high-level lan-\nguages.\nTo evaluate the versatility of N3IC , we implemented\nthree different use cases related to network monitoring:\n(i) trafﬁc classiﬁcation, (ii) anomaly detection, and (iii)\nnetwork tomography. In the ﬁrst two, we seek to identify\nspeciﬁc patterns into the trafﬁc data. The latter infers occu-\npation levels of a datacenter’s switch queues using probe\npackets, by implementing a slightly modiﬁed version of SI-\nMON [ 18]. Using the presented implementations, we show\n(x6) that N3IC can greatly improve the performance of\nGPU  \nMotherboard  NIC \nPCIe  PKT \nIN \nPCIe  \n1 \nMemory             CPU  2 3 \n4 Figure 2: A system with a GPU working as NN accelerator.\nEnabling the NIC at performing NN inference could save\nup to 4 data transfers over the PCIe bus.\ntypical networking applications that need relatively small\nNNs. Our evaluations show that N3IC can perform trafﬁc\nanalysis for millions of network ﬂows per second, while\nforwarding trafﬁc at 40Gb/s. Compared to a similar system\nimplemented entirely on a general purpose CPU, N3IC\ncan provide 100x lower processing latency, 1.5x higher\nthroughput, and save precious CPU’s cores. Furthermore,\nN3IC allows applications to run NN inference even when\ntheir time budget is of few µ s.\nIn summary, the main contributions of this paper are:\n\u000fWe present N3IC , an approach to accelerate NN infer-\nence for network trafﬁc on commodity programmable\nNICs, and implement it on two different hardware\ntargets, showing the ﬂexibility of our approach and\nits applicability on off-the-shelf NIC hardware.\n\u000fWe share the three implementation designs: a microC-\nbased implementation for the NFP4000 network\nprocessor; a P4-based implementation for the P4-\nNetFPGA framework; and a Verilog hardware design\nfor the NetFPGA;\n\u000fWe provide a compiler that generates fully functional\nP4 code from NN abstractions. The compiler targets\nboth a software bmv2 switch and a P4 NIC, using the\nP4-NetFPGA framework.\n\u000fWe present three different use cases releated to net-\nwork monitoring: (i) anomaly detection, (ii) trafﬁc\nclassiﬁcation and (iii) network tomography and we\nstudy the trade-offs and beneﬁts in ofﬂoading neural\nnetwork inference on modern NICs.\n2 Motivation\nGiven the wide availability of PCIe-attached off-the-shelf\nNN accelerators, i.e. GPUs, one may reasonably consider\n2\n\n41K2K4K8K16K32K64K128K256K491K2K4K8K\nTransfer size [bytes]                     NN size [neurons]   010203040506070Time [us]Traffic\nanalysis\nuse casesCPU-to-GPU data transfer time\nCPU Inference timeFigure 3: The PCIe RTT compared to the time needed by a\nsingle CPU core to run NN inference. A single PCIe RTT\nis larger than the NN inference time for small NNs.\nthe use of such systems to ofﬂoad a CPU from the process-\ning of NN workloads, rather than a NIC. However: (i) NIC\nhardware is placed in a strategic position to reduce PCIe\ndata transfer overheads; (ii) widely deployed NN architec-\ntures require only a small number arithmetic operations\nper byte of loaded data, thus imposing little computational\nburden on the NIC; (iii) commodity programmable NICs\nhave a fair amount of computational power, which is po-\ntentially available depending on the input trafﬁc patterns.\nWe detail these three observations in the rest of the section\nand then present a motivating use case.\n2.1 Observations\nI/O operations are expensive . Host-attached accelera-\ntors, i.e., GPUs, may introduce considerable data transfer\noverheads, as they are generally connected to a host system\nvia the PCIe bus (Figure 2). This rules them out as ofﬂoad-\ning engines for time-sensitive network applications. For\nexample, ofﬂoading the computation of NN inference to a\nGPU requires the input data to enter the host system (ﬁrst\nPCIe transfer), then to reach the GPU (second PCIe trans-\nfer), and ﬁnally to transfer the result back to the CPU (third\nPCIe transfer). If the NN inference result is needed for\npacket forwarding decisions, this may involve a potential\nadditional transfer to the NIC (fourth PCIe transfer)1.\nTo better understand the overheads in place, we mea-\nsured the time needed to transfer an increasingly bigger\ninput vector to a GPU through the PCIe and then retrieve\n1It is worth noting that although the newly designed GPUDirect\nRDMA (Remote Direct Memory Access) technology enables network\ndevices to directly access GPU memory thus bypassing CPU host mem-\nory [56], it cannot prevent data from crossing the PCIe.back just the result from the inference.2We compared\nthis with the amount of time needed to directly run the\ninference on the CPU itself for a variable-sized NN (Fig-\nure 3).3The results show that transferring just few bytes of\ninput vector and retrieving back the result from the GPU\nmight already require 8-10 µs. To put this in perspective,\nexecuting relatively small NNs directly on the CPU takes\nas much as a single transfer. For instance, an efﬁcient\nbinary NN [ 43] with about 2k neurons takes about 8 µs\nto be executed. Furthermore, depending on the speciﬁc\nuse case, and as demonstrated in x5 in the context of traf-\nﬁc analysis, the NN might need less than 50 neurons. In\nthis scenario, the CPU only needs about 400 nsto run the\ninference, making inefﬁcient the use of ML accelerators\nthat need to be reached through PCIe. While batching the\nexecution of multiple NNs together could be an effective\nway to reduce the data transfer overhead, it would also sig-\nniﬁcantly impact the processing latency. For time sensitive\nworkloads, NN inference is typically performed directly\non the CPU, as it is a more effective solution than using a\nPCIe connected accelerators [24].\nObservation 1: Running NN inference on a NIC\ncould completely avoid crossing the PCIe bus and\nhelp in the execution of time sensitive workloads.\nNN processing is memory-bounded in many cases . A\nNN is generally composed of several layers. Depending\non the neurons’ interconnections and operations, different\ntypes of layers can be deﬁned, and, accordingly, differ-\nent types of NN. N3IC implements fully connected layers\n(FCs), which are the building block of Multi-layer Percep-\ntrons (MLPs). Other well known and widely used NNs are\nRecurrent NNs (RNNs) and Convolutional NNs (CNNs).\nHowever, in classiﬁcation and prediction tasks for for-\nmatted and tabular data, with low dimensions and lower\nnumber of target categories, MLPs are usually used, being\nfairly efﬁcient. Indeed, despite the popularity of RNNs\nand CNNs, large industries such as Google and Facebook\nreport that the large majority (e.g., above 60%) of their\nNNs workloads requires the execution of MLPs [30, 25].\n2We set this to 1B to minimize as much as possible the cost of the\nI/O.\n3This test used an Intel Xeon E5-1630 v3 CPU (4 cores@3.7GHz)\nconnected through a PCIe x16 v3.0 to an NVIDIA GTX 1080 TI 11GB.\n3\n\n0 50 100 150 200 250 300 350\nNN execution latency [ms]01234IPCconv fc\n01234\nL3 cache misses [105]\nIPC\nL3 cache missesFigure 4: IPC and cache misses during the execution of the\ndifferent layers of a VGG16 CNN. Unlike convolutional\nlayers, fully-connected layers have a relatively low number\nof operations per byte of data loaded.\nMLPs, compared to other NNs, are a better ﬁt to the con-\nstraints of networking devices, mainly due to the computa-\ntion proﬁle of FCs, which require much lower computation\nthan other types of layers as demonstrated in the follow-\ning test. We used Intel’s optimized Caffe [ 32] to perform\ntests on a machine equipped with an Intel Xeon E5-1630\nv3 CPU (4 cores@3.7GHz), and run VGG16 [ 67] as test\nworkload. VGG16 is a well-known CNN with a sequen-\ntial structure, with convolutional (conv) layers followed\nby FCs, which allows us to clearly observe the different\ncomputation proﬁles of such layers. We use a single core\nto perform the computations, and pre-load the model in the\nlocal memory.4With perf , we monitor the CPU’s per-\nformance counters for number of instructions, cycles and\nL3 cache misses. We use the Instructions per Cycle (IPC)\nrate as a proxy for the arithmetic intensity of the workload,\ni.e., the number of operations performed per byte of data\nloaded. Intuitively, high IPC and low L3 cache misses tell\nthat the workload keeps most of the CPU’s pipeline busy.\nLow IPCs and high L3 cache misses are a good indica-\ntor of memory-bound workloads, i.e., a small number of\noperations is executed for byte of loaded memory.\nFigure 4 shows that the IPC is high when executing\nconvolutional layers, while it is low during the execution\nof FC layers. The increase in L3 cache misses shows that\nthis happens due to the need to wait for data to be loaded\nfrom memory, conﬁrming the relatively low arithmetic\nintensity of MLPs.\n4The core has 64KB of L1 cache memory, 256KB of L2 cache and\n10MB of L3 cache, which are essentially dedicated to the processing of\nsuch core, being the other 3 cores completely idle.Observation 2: MLPs have relatively low arith-\nmetic intensity. This makes hardware architectures\noptimized for fast memory accesses a good ﬁt for\ntheir execution.\nNICs processing load depends on trafﬁc patterns . A\nNIC handling a bandwidth of 40Gb/s needs to process\nabout 3 million packets per second (Mpps) when packets\nare 1500B in size. In contrast, for the same bandwidth, a\nNIC would need to process 15Mpps of size 256B, which\nis a 5x bigger processing load. Given that the hardware\nof NICs is often designed to handle worst case trafﬁc sce-\nnarios, i.e., processing many small network packets, this\ncreates opportunities to run additional operations without\npaying operational penalties. To illustrate this, we show\nin Figure 5 the throughput of a Netronome NIC when\nperforming an increasingly higher number of per-packet\noperations, i.e, integer arithmetic over the received packet,\nbeside the common parsing and forwarding tasks, when\nloaded with 25Gb/s of constant bit-rate trafﬁc with packet\nsizes of 512B, 1024B and 1500B. The ﬁgure shows that\nas the packet size increases, the NIC can perform more\nper-packet operations in addition to its regular forwarding\ntasks, without affecting the forwarding throughput. Specif-\nically, considering an average case of 512B input packets,\ntypical for a data center scenario [ 61], the available budget\nis of 10K operations per-packet before trading with opera-\ntional performance. Notably, the processing budget grows\nby ten times when the packets size doubles.\nObservation 3: The relatively small arithmetic\nintensity of MLPs aligns well with the potentially\navailable processing power of a NIC.\n2.2 A motivating use case\nNNs are a relevant tool for trafﬁc classiﬁcation [ 50,42]\nand anomaly detection [ 33]. Usually, the end-to-end pro-\ncess requires the reception of packets, then the calculation\nof trafﬁc statistics that will ultimately feed an NN in charge\nof producing the ﬁnal result. In this scenario, a NIC usu-\nally ofﬂoads the CPU only for the network ﬂows statistic\ncollection. The NN inference has to be entirely run on\n4\n\n102103104105106\nPer-packet operations0510152025Gbit/s\n1512\n1024\n512\n256Figure 5: Forwarding\nthroughput of a Netronome\nNFP when performing\nan increasing number of\narithmetic operations per-\npacket. With larger packet\nsizes many operations can\nbe performed, without\naffecting the forwarding\nthroughput.\n0.00.20.50.81.01.2\nThroughput [Million flow/s]1101001K10KLatency [us]\nCPU (1 core)\nN3ICFigure 6: Comparison of\nthe processing latency for\nA CPU-based executor and\nN3IC , when an increasing\nnumber of network ﬂows/s\nhas to be analyzed. By\nrunning on the NIC, for\nhigher throughput values\nN3IC can provide a 100x\nlower latency.\nthe host’s CPU, which therefore has to continuously fetch\nthe trafﬁc statistics from the NIC’s hardware to perform\nthe NN inference. Although the CPU no longer has to run\nexpensive operations such as packet reception and statis-\ntics collection, the NN inference can already occupy an\nentire CPU core, even when performing optimized batched\ncomputation of the per-ﬂow statistics.\nIn Figure 6, we show the performance achieved by us-\ning a single CPU core for NN inference. Here, the trafﬁc\nanalysis can scale to process up to 1.2M network ﬂows per\nsecond, but only when performing input batching. Unfortu-\nnately, batching increases the processing latency from 10s\nofµs, for a batch of size 1, to 10s of ms, for a batch of size\n10k. This hinders the implementation of time-sensitive ap-\nplications relying on quick results. N3IC , instead, without\nusing expensive hardware, allows the system to completely\nofﬂoad most of the computation using available NIC’s re-\nsources, thereby avoiding data movements, freeing CPU\ncores, providing timely results and reducing processing\nlatency by up to 100x.\n3 Neural Networks on the NIC\nIn this section we describe the challenges of supporting NN\ninference in the data plane of a commodity programmable\nNIC, and how we overcame them with N3IC .Challenge 1: The need for small NN models: A typical\nNIC has at most few 10s of MBs of fast on-chip SRAM\nmemory [ 53,73,72], which is commonly used to store the\ndata required for packet processing. Small NNs, which\nare common in networking use cases ( x5), could ﬁt their\nmodel’s parameters in such space. However, the on-chip\nmemory needs to also host forwarding and policy tables.\nSuch tables can easily take several MBs of memory, leav-\ning little space available for the NNs. An important obser-\nvation is that NNs, being inherently a tool of approximate\ncomputing [ 27], can effectively take advantage of com-\npression and quantization techniques. These techniques\nreduce the number of bits used to represent the NN’s pa-\nrameters, e.g., 8b instead of 32b, and are already widely\napplied to improve the efﬁciency of NN accelerators. Re-\ncent approaches have pushed quantization even further,\nusing just few bits per parameter [ 38,60,81]. Effectively,\nthis provides opportunities to reduce memory requirements\nby 2-8x even when compared to already efﬁcient 8b repre-\nsentations. Although there is a trade-off between memory\nrequirements and NN accuracy, for many use cases the\ndrop in accuracy is usually tolerable [38].\nChallenge 2: The need for a low complexity algebra:\nA second major constraint is related to the arithmetic ca-\npabilities of networking devices. Networking hardware\noften only has the ability to perform bitwise logic, shifts\nand simple integer arithmetic operations [ 69]. Implement-\ning the operations required by a NN, such as multiplica-\ntions, may not always be possible. Certain quantization\ntechniques help in addressing this issue, since they can\nsimplify the arithmetic operations required to compute a\nNN. Indeed, the already mentioned 8b quantization is used\nby the TPU [ 30] to enable the use of integer multiplica-\ntions in place of ﬂoating point. More interestingly, some\nquantization techniques can even completely change the\nnature of the operations required to compute a NN. For\nexample, log domain quantization [ 38] uses log base-2 val-\nues to represent NN parameters, which enables replacing\nmultiplications with simple shift operations.\nChallenge 3: The need for hardware compatibility:\nThe number of currently available quantization techniques\nis fairly large, e.g, [ 28,38,60,81,21,9,82,40]. However,\nour solution space is reduced by a third ﬁnal constraint:\nnetworking devices have quite heterogeneous hardware ar-\n5\n\nchitectures, ranging from SoCs, FPGAs, embedded CPUs\nto switching ASICs. Thus, the selected quantization tech-\nnique should guarantee a wide hardware compatibility.\nEffectively, this translates into selecting a quantization ap-\nproach that uses arithmetic operations widely supported in\nthe above architectures.\nAlgorithm 1: FC processing function. Weights and\ninputs are in groups of block size .\nInput : xinput vector, wweights matrix, nnum. of\noutput neurons;\nOutput : youtput vector\n1block size 32;\n2assert (n%block size == 0) ;\n3sign thr= (len(x)\u0003block size)=2;\n4y[n=block size] f0g;\n5forneur 0tonby1do\n6 tmp 0;\n7 fori 0tolen(x)by1do\n8 tmp+=popcnt (w[neur ][i]\fx[i]);\n9 end\n10 iftmp > =sign thrthen\n11 tmpoutj= (1<<(neur %block size));\n12 end\n13 if(neur + 1) % block size == 0 then\n14 y[neur ] tmpout;\n15 tmpout 0;\n16 end\n17end\n3.1 The solution adopted by N3IC\nWith the previous challenges in mind, we ﬁnally select\nbinary neural networks (BNN) as the N3IC ’s quantiza-\ntion technique to implement NN in the network data plane\nof a NIC. A BNN uses only 1b to represent inputs and\nweights [ 60,10], and replaces multiplications with a com-\nbination of simpler operations, i.e., bitwise XNOR, and\npopulation count ( popcnt ). Compared to other solu-\ntions, such as log domain quantization, which requires shift\noperations, BNNs provide better opportunities for paral-\nlelizing the computation. Finally, despite the signiﬁcantly\nlower computation and memory complexity, BNNs can\nstill achieve remarkable results. In many tasks, BNNs pro-\nvide prediction accuracy only 3-10% points lower than the\nPacket \nParser  Forwarding Module  \n(Packet modification, statistics \ngathering, forwarding decision)  \nNN Executor  Memory  Trigger  \nTrigger  Input  \nOutput  \nInput  selector  Output  selector  Figure 7: N3IC logical architecture.\naccuracy of the much heavier non-quantized models [ 43].\nWhile this may be a signiﬁcant drop for some use cases, it\nstill enables such models to be used in a large set of applica-\ntions, as we will discuss more thoroughly in x5. Precisely,\nstarting from the original NN, we ﬁrst applied the binariza-\ntion technique from Courbariaux and Bengio [ 10], which\nis based on a canonical back-propagation algorithm. This\nsolution ensures that the NN’s weights converge to val-\nues included in the [-1, 1] range, and that are normally\ndistributed around the 0. This helps in losing only little\ninformation when the real weight values are mapped to just\ntwo values, i.e., 0 and 1 [ 10]. In fact, the MLP’s weights\nobtained after training are still real numbers, and an addi-\ntional ﬁnal step is required to obtain the binarized weights.\nThis is done by setting the NN weigths to 0 if their value\nis negative or 1 otherwise.\nWe implemented the processing of a BNN with Algo-\nrithm 1, which parametrizes the BNN operations accord-\ning to the variable block size . This corresponds to the\nlargest contiguous set of bits on which an operation can be\nperformed in the target hardware executor. For instance,\na modern CPU could have a block size of 64, corre-\nsponding to the CPU’s registers size. Speciﬁcally, for each\nof the NN’s neurons, the algorithm performs (1) a XNOR\nof the input with the weights vector, (2) a population count\noperation and (3) a comparison. The comparison checks\nif the result of the population count is smaller than half\nthe number of neuron’s inputs, in which case the neuron’s\noutput is 1.\n3.2N3IC architecture\nThe above described BNN algorithm is integrated in the\nNIC data plane as depicted in Figure 7. The NN Executor\nis separated from the forwarding module, although some\nimplementations may allow both instances to be integrated.\n6\n\nThe NN Executor is triggered either by the reception of\na network packet, or directly by the forwarding module\nin charge of the regular packet processing. In fact, the\nforwarding module may collect network ﬂows statistics\nand only trigger the NN Executor when enough packets\nhave been received.\nThe input selector allows to choose the input of the\nNN executor between a packet ﬁeld or a speciﬁc memory\narea, e.g., containing collected ﬂow statistics. The output\nselector is used instead to write the inference output either\nto a packet ﬁeld, or to a speciﬁed memory location. When\nthe input and output selectors are conﬁgured to read or to\nwrite to a packet ﬁeld, the NN Executor works as an inline\nmodule.\n4 Implementations\nIn this section we describe the implementation of N3IC on\ntwo different programmable hardware targets: a SoC-based\n(NFP4000 Netronome [ 53]) and an FPGA-based (NetF-\nPGA [ 83]) NICs. We ﬁrst show the implementation of\nN3IC using only the currently available NICs’ primitives\naccessible through high level programming, i.e., microC\nand P4. We then describe the design of a new hardware\nmodule that implements a dedicated BNN executor using\nHardware Description Language (HDL). This provides a\npathway to enable the use of our solution as a primitive for\nhigh-level languages.\n4.1 SoC NIC: Netronome NFP4000\nThe NFP4000 architecture comprises several different\nhardware blocks. Some of them are dedicated to network-\nspeciﬁc operations, e.g., load balancing, encryption. Oth-\ners are binded to programmable components that are used\nto implement custom packet operations. The architecture\nis shown in Figure 8, and comprises tens of independent\nprocessing cores, which in Netronome terminology are\nnamed micro-engines (MEs). MEs are programmed with a\nhigh level language named micro-C , a C dialect. Each ME\nhas 8 threads, which allow the system to efﬁciently hide\nmemory access times, e.g., context switching between\nthreads as they process different packets. MEs are fur-\nther organized in islands, and each island has two shared\nSRAM memory areas of 64KB and 256KB, called CLSand CTM, respectively. Generally, these memory areas\nare used to host data required for the processing of each\nnetwork packet. Finally, the chip provides a memory area\nshared by all islands, the IMEM, of 4MB SRAM, and\na memory subsystem that combines two 3MB SRAMs,\nused as cache, with larger DRAMs, called EMEMs. These\nlarger memories generally host forwarding tables, access\ncontrol lists and ﬂow counters. MEs can communicate\nand synchronize with any other ME, irrespective of the\nlocation. Communications across islands take longer and\nmay impact the performance of a program.\nWhen developing N3IC on this hardware platform we\nhave to share the MEs and memory resources between\ntasks, thus, we had to strike the right balance between the\nneeds of quickly forwarding network packets and running\nNN inference. For both processing tasks the main bot-\ntleneck is the memory access time. Thus, selecting the\nmemory area to store NN’s weights has played a major\nrole in our design.\nIf the NN is small, as for the use cases considered in\nthis paper (x5), it is worth considering the fastest available\non chip memories, i.e., the CTM and CTS, with an access\ntime of less than 100ns [ 53]. However, the CTM mem-\nory is usually dedicated to packet processing tasks, being\nthe memory used by the NFP to store incoming packets\nand making them available to the MEs. Thus, using the\nCTM may impact packet processing and should be avoided.\nBecause of this, we decided to load the NN’s weights at\nconﬁguration time in the CLS memory. At boot time,\neach of the MEs’ threads registers itself to be notiﬁed of\npackets reception, with the NFP taking care of distributing\npackets to threads on a per-ﬂow basis. This is a standard\napproach when programming the NFP. Thus, whenever\na new packet is received, the NFP copies its content in\nan island’s CTM, and notiﬁes one of the island’s threads\nto start packet processing. The notiﬁed thread performs\nregular packet processing tasks, such as parsing, counters\nupdate, forwarding table lookups. If a trigger condition is\nveriﬁed, the thread starts the processing of the conﬁgured\nNN. Typical conditions could be the arrival of a new ﬂow,\nthe reception of a predeﬁned number of packets for a given\nﬂow, the parsing of a given value in a packet header. To\nrun the NN, a thread performs Algorithm 1, with input\nand weights packed in 32b integers, i.e., block size is\n32. As a consequence, multiple threads can perform NN\ninference in parallel (Figure 8).\n7\n\nIMEM\n4MBEMEM\n3MB (Cache)DRAM\nDRAMIsland\nME\nCLS\n64KBCTM\n256KBME\nThread 1\n Thread 2\n Thread n\nNN INPUT\n NN INPUT\n NN INPUT\nEMEM\n3MB (Cache)DRAM\nDRAMIsland\nME\nCLS\n64KBCTM\n256KBME\nT1\n T2\nT3\n T4\nT5\n T6\nT7\n T8\nFigure 8: The architecture of a Netronome NFP4000’s\nprogrammable blocks and the NN processing with N3IC -\nNFP.\nWhen NNs are larger, running them in a single thread\nwould take long, because the NN has to be stored in slower\noff-chip memory. In this case, using multiple threads to\nrun a single inference is more effective, even if synchro-\nnization among them incurs in some overhead. We leave\nthe discussion of this trade-off to the Appendix.\n4.2 From Neural Networks to P4 and to\nNetFPGA\nP4 [5] is a domain-speciﬁc, platform agnostic language\nfor the programming of packet processing functions. We\ndesigned a compiler that transforms an NN description\ninto a N3IC implementation described with P4. In prin-\nciple, the P4-based implementation allows us to separate\ntheN3IC solutions from the underlying hardware-speciﬁc\ndetails, thus generalizing our approach. However, as we\nwill discuss at the end of the section, the target hardware\narchitecture has still an important impact on the ﬁnal im-\nplementation.\nCompiling NN to P4. Our compiler, NNtoP4 , takes as\ninput a NN description, i.e., number of layers with corre-\nsponding number of neurons, and generates P416code for\na generic P4 target based on the PISA architecture. PISA is\na spacial forwarding pipeline architecture, with a number\nof match-action units (MAUs) in series. A packet header\nvector (PHV), containing both the input packet and meta-\ndata information, is passed through the MAUs to perform\nthe programmed processing tasks. Each MAU combines a\ntable memory structure, for quick lookups using the PHV\nﬁelds, with arrays of ALUs that perform operations on\nsuch ﬁelds. The code generated by NNtoP4 implements a\nfunction, on top of the PISA architecture, which reads the\ninput value from the PHV , performs the NN execution andwrites back to a PHV’s ﬁeld the result of the computation.\nThe NN weights are stored in the MAUs’ fast memories\nto enable runtime reconﬁguration. The generated P4 code\nalso includes headers deﬁnition, parser, de-parser and con-\ntrol blocks. The code can therefore be easily extended to\nintegrate with any other required packet processing func-\ntion.\nAlgorithm 2: popcount implementation. B is the num-\nber of bits required to represent n.Xjyindicates the y-\ntimes concatenation of the binary number XandZjjW\nis the concatenation of the binary numbers ZandW.\nInput : ninput number;\nOutput : coutput counter\n1L log2B;\n2bits[L] f1;2;4; :::; B= 2g;\n3masks [L] \nf01jB=2;0011 jB=4;00001111 jB=8; :::;0jB=2jj1jB=2g;\n4c n;\n5fori 0toL\u00001by1do\n6 c (c&masks [i]) + (( c >> bits [i]) &\nmasks [i]);\n7end\nThe basic operations needed to implement Algorithm 1\nare (1) XNOR, (2) popcount and (3) SIGN function. Exe-\ncuting a XNOR and a comparison (SIGN) is readily sup-\nported by the P4 language. Unfortunately, the popcount\noperation is not. The main issue is that its execution time\ndepends on the input size, which makes popcount difﬁcult\nto implement in networking hardware, and therefore not\nsupported in the PISA architecture. To overcome this issue\nusing only current P4 primitives, we adopted the solution\nproposed in [ 4] and reported in Algorithm 2. The idea is to\nimplement the popcount by combining basic integer arith-\nmetic and logic operations in a tree structure whose depth\nis dependent on the input size. A tree structure is easily\nparallelizable and allows the distribution of computations\nin the pipeline, with the processing of different tree’s levels\nassigned to different pipeline’s stages.\nOverall, the processing includes ﬁve steps, each one\nmapped to a logical pipeline stage, except for the popcount\nwhich requires multiple stages, depending on the input\nsize (cf. Figure 9). In the ﬁrst step, the NN input is repli-\n8\n\nXNOR \nW1 \nPHV  \nReplication  XNOR  \nand Duplication  PHV  PHV  PHV  \nSIGN \nPHV  \n PHV  \nPopulation count  \n(multiple elements)  Sign  Folding  PHV  \nX X \nX \nX X1 \nX1 \nX2 \nX2 \nX3 \nX3 X1’ \nX1” \nX2’ \nX2” \nX3’ \nX3” X1 \nX2 \nX3 Y Y1 \nY2 \nY3 SIGN \nSIGN \n+ \n+ \n+ \n>> & \n>> & \n>> & \n>> & \n>> & \n>> & \n~ = \nXNOR \nW2 \nXNOR \nW3 Figure 9: The logical steps required to implement a BNN\nusing a PISA architecture.\ncated in as many PHV ﬁelds as the number of neurons to\nexploit the parallel processing on multiple packet header\nﬁelds. Speciﬁcally, this corresponds to an unrolling (or\npartial unrolling) of the ﬁrst for cycle of Algorithm 1. In\nthe second step, each ﬁeld, containing a copy of the NN\ninput, is XNORed with the corresponding weight. The\nresulting value is further duplicated to additional ﬁelds to\nimplement the shift, AND and sum as described in Algo-\nrithm 2. The outcome of each popcount is then compared\nwith a threshold to implement the SIGN function, whose\nresult is the output of each neuron. Finally, the resulting\nbits, stored in one PHV ﬁeld for each neuron, are folded\ntogether in a single ﬁeld. Depending on the NN depth,\nNNtoP4 replicates and concatenates the described oper-\nations as many times as the number of layers to obtain\nthe complete MLP execution. We functionally tested the\ngenerated P4 implementations using the software target\nbmv2.\nFor hardware targets, it is worth noticing that the PHV\nsize limits the number of neurons the pipeline can execute\nin parallel. This is due to the need to replicate the input\nin the PHV to enable parallelism at the MAU level. Fur-\nthermore, the number of neurons executed in parallel by\na single MAU is also limited by the maximum amount of\nmemory a MAU can read in a single operation.\nPorting P4 code to NetFPGA. The NetFPGA is a pro-\ngrammable hardware platform with 4x10GbE Ethernet\ninterfaces incorporating a Xilinx Virtex-7 FPGA. We im-\nplemented N3IC on top of the reference NIC project pro-\nvided with the main NetFPGA-SUME code base.5We\nused the P4-NetFPGA workﬂow [ 29] to port the gener-\nated target-independent P4 code to the NetFPGA platform.\nThe P4-NetFPGA workﬂow is built upon the Xilinx P4-\n5https://github :com/NetFPGA/NetFPGA-SUME-\npublic/wiki/NetFPGA-SUME-Reference-NIC\nCAM  \n(Memory)  \nOutput  XOR  \nRegister  popcnt  \n(8b LT ) \npopcnt  \n(8b LT ) \npopcnt  \n(8b LT ) ADD  SIGN  \nm \nStage 1  Stage 2  Stage 3  n Block 1  Block K  Block 2  \nTrigger  \nWeight buffer  Input  selector  Output  selector  Figure 10: Hardware design of the NN Executor module.\nSDNet [ 78] compiler and the NetFPGA-SUME code base.\nIt translates P4 code to HDL (Verilog), and integrates it\nwithin the NetFPGA pipeline.\nThe P4-NetFPGA workﬂow required several adapta-\ntions to NNtoP4 , in order to meet the FPGA resources\nand timing constraints. First, the P4-SDNet compiler does\nnot support if statements among the operations of\na MAU. Thus, we replaced all the if statements required\nby the SIGN function using a combination of bitwise logic\noperations and masks. Second, MAUs use the CAM IP\ncore from Xilinx to implement lookup tables, which re-\nstricts the maximum width size that can be used for each\nentry. Consequently, a maximum of 32B can be fetched\nfrom memory every time a table is called, limiting the\nnumber of neuron weights that could be loaded in parallel\nby each table. To overcome this issue we had to write\nthe weights as constant values in the MAU’s operations\ncode, effectively trading the possibility to perform runtime\nreconﬁguration with the ability to compute more neurons\nin parallel. Finally, P4-SDNet is capable of performing a\nlarge number of operations on a ﬁeld in a single MAU. This\nis in contrast with ASIC targets, which are instead usually\nconstrained to execute a single operation per MAU [ 69].\nThis allowed us to describe several steps of a BNN com-\nputation in a single MAU, thus reducing the number of\nMAUs required to implement the BNN computation.\n4.3 BNN inference primitive\nFinally, we designed a dedicated NN executor for the NetF-\nPGA in HDL, which allows the NIC to expose BNN in-\nference as an ofﬂoading primitive. Figure 10 shows the\n9\n\narchitecture of our NN executor as per Algorithm 1. The\nmodule is composed by multiple blocks. Each of them\nperforms the computation of a single NN layer, and can be\nparametrized providing the sizes nandmfor the input and\noutput vectors, respectively. Together, the blocks build an\nNN Executor for speciﬁc NN architectures. For instance,\nthree of these blocks are required to build a 3 layers MLP.\nThe NN layer weights are stored in the FPGA on-chip\nmemories, i.e., Block RAM (BRAM). The BRAMs are\norganized as tables with a number of rows strictly depen-\ndent to the amount of neurons and with a width of 256b.\nEach row can be read in 2 clock cycles and, depending on\nthe size nof the input vector, can store one or multiple\nweights, e.g., 1x256b or 16x23b. The BRAMs are shared\nby all the blocks of a NN Executor module.\nA single block is a pipeline of three stages. The ﬁrst\nreads the weights from the BRAM and performs the XNOR\nwith the input. The second performs a ﬁrst step of the\npopcount. Here, we create Lookup-Tables (LTs) of 256\nentries each, in order to associate one 8b integer (address)\nto the corresponding population count value. Each block\nhasn=8of these LTs. As a consequence, for a 256b input\nwe create 32 LTs that operate in parallel. In the last stage,\nthe LTs outputs are summed together, the sign function is\napplied on the ﬁnal sum and its result is stored in one of\nthembits of the output register. If multiple weights are\nplaced in a single BRAM’s row, the module performs the\nexecution of several neurons in parallel.\n5 Use cases\nIn this section, we show how N3IC can beneﬁt applica-\ntions from both backbone and datacenter networks. Table 1\nsummarizes the three implemented use cases. More details\nare provided in the Appendix.\n#1: Trafﬁc classiﬁcation. Backbone network operators\nuse trafﬁc classiﬁers to make a best-guess about the type of\ntrafﬁc carried by a ﬂow, in order to assign priority classes\nand improve user experience [ 31]. This operation is often\ncarried at the network edge [ 76] using software middle-\nboxes for enhanced ﬂexibility [ 45,46,66] at the cost of\nreduced performance [ 65,3,34]. In this context, N3IC can\nbe used to reduce the amount of trafﬁc processed by the\nsoftware classiﬁer by performing a pre-classiﬁcation task\nCPU\nCPU\nCLASSIFICATION TASK\nPRE-CLASSIFICATION TASKFINE -GRAINED CLASSIFICATION TASKW/O N3IC W/ N3ICFigure 11: For trafﬁc anal-\nysis use cases, N3IC can\nsplit the classiﬁcation task\nand work as a ﬂow shunting\nsystem, thereby increasing\napplication’s scalability.\nPPP\nDELAY 1\n…\nDELAY n\nFigure 12: N3IC enables\nthe real-time implementa-\ntion of SIMON [ 18], using\nNNs in the NIC to iden-\ntify congested queue from\nprobes’ one-way delays.\nto reduce the pressure on the host CPU (See Figure 11).\nTo illustrate this use case, we used the UPC-AAU\ndataset [ 7]. The dataset has more than 750K ﬂows, includ-\ning 76 commonly used applications and services, which\naccount for 55GB of packet-level data. Here, N3IC can be\nused to classify the trafﬁc class that contributes the most.\nIn the UPC-AAU dataset, this corresponds to the identiﬁca-\ntion of P2P trafﬁc. With this approach, N3IC would be in\ncharge of directly classifying P2P trafﬁc, while passing the\nrest to the software middlebox running on the host system\nfor more ﬁne-grained classiﬁcation. That is, N3IC can be\nused as an effective ﬂow-shunting system. We trained a\nbinarized MLP to classify trafﬁc in two classes (i.e., P2P\nand other), and deploy it with N3IC . On this classiﬁcation\ntask, a binarized MLP with 32, 16, 2 neurons is capable of\nachieving 88.6% accuracy, while requiring only 1KB of\nmemory.\n#2: Anomaly detection. Anomaly detection is the prac-\ntice of analyzing trafﬁc to seek anomalies and reveal suspi-\ncious behavior [ 59]. Unfortunately, to cope with the ever-\nincreasing large trafﬁc volumes, operators only perform\nanalysis on randomly chosen samples of the ﬂows [ 75].\nWithN3IC , instead, we can allow software middleboxes\ndeployed at the edge network to perform continuous ﬂow-\nlevel analysis for all the ﬂows, without impacting the re-\nsource usage of the host CPU.\nFor this use case we used the the UNSW-NB15\ndataset [ 52], which provides over 170K network ﬂows\nlabeled in two main categories, i.e., good, bad, and their\nﬂow-level statistics. A regular MLP with 3 FCs of 32, 16, 1\nneurons achieves 90.3% accuracy on this dataset, requiring\n10\n\nUse CaseInput size NN size Memory Accuracy\n(bits) (neurons) (KBytes) (%)\nTrafﬁc256 32, 16, 2 1.1 88.6Classiﬁcation\nAnomaly256 32, 16, 2 1.1 85.3Detection\nNetwork152 128, 64, 2 3.4 92.0Tomography\nTable 1: Use cases implemented with N3IC .\n35KB of memory. Using a similar binarized MLP (32,\n16, 2 neurons and 1KB in total of memory) with N3IC\nachieves an 85.3% accuracy, instead. While the N3IC\nNN gives a 5% lower accuracy, it is applicable to large\nvolumes of trafﬁc, enabling a type of security monitoring\nthat would not be economically viable otherwise.\n#3: Network Tomography. In datacenter networks, end-\nhost based network monitoring approaches are being ex-\nplored for their ease of deployment and lightweight opera-\ntions [ 51,19,18]. For example, SIMON [ 18] periodically\nsends probe packets to measure datacenter’s network paths\ndelays. This allows to accurately infer congestion points\nand the size of the related queues. SIMON uses MLPs to\nspeed-up the inference tasks. However, it relies on GPUs\nto run the MLPs, which makes the tool better applicable for\ndebugging than for creating a feedback loop between mea-\nsurement and control, i.e., for path selection. Here, notice\nthat according to [ 18] the probe periodicity depends on the\nfastest link speed. For instance, in the presence of 10Gb/s\nlinks, probes have to be sent every 1ms, while for 40Gb/s\nlinks this lowers to 0.25ms. As a consequence, for this\nuse case to work at higher link speeds and in real time, the\nexecution latency has to be lower than the probe sending\nperiodicity. This can be challenging considering I/O over-\nheads and MLP processing time as previously discussed\ninx2. With N3IC , we implemented a modiﬁed version of\nSIMON that can quickly identify congestion points, and\ntimely notify them to the network control plane.\nWe tested the use case simulating a CLOS-like Fat\nTree datacenter network with ns3 [ 74], using different link\nspeeds (from 100Mb/s to 10Gb/s) and trafﬁc workloads.\nFollowing the methodology suggested by [ 18], we split the\nproblem of inferring queue sizes in multiple sub-problems,\neach targeting a subset of the queues. This allows us to run\nsmaller MLPs on each of the NICs.6Unlike SIMON, our\n6This also requires a careful selection of the probe packets destina-approach does not infer the actual size of a queue, but it\nonly infers which queues are bigger than given thresholds\nlevels. This information is usually sufﬁcient for the control\nplane to take a ﬂow-steering decision, while more accurate\ninferences, e.g., for debugging, can be still run ofﬂine (See\nFigure 12).\nEach binarized MLP running with N3IC has 19 probes’\none-way delays as input, and 128, 64, 2 neurons. A NIC\ncan run multiple of these NNs, since each of them infers\nthe congestion status of a speciﬁc queue. Across all the\nqueues of the simulated network, we achieve a median\naccuracy in predicting a congested queue above 92%.\n6 Evaluation\nIn this section we present the experimental evaluations of\nN3IC . We report and discuss the end-to-end performance\nof the use cases presented in x5, and evaluate the scala-\nbility of the three implementations with targeted micro-\nbenchmarks. More details alongside the performance of\nalternative design choices are discussed in appendices.\nTestbed . Unless stated otherwise, we run all the tests on\na machine equipped with an Intel Haswell E5-1630 v3\nCPU and either a single Netronome Agilio CX SmartNIC,\nwith an NFP4000 processor, or a NetFPGA-SUME.7The\nHaswell is clocked at 3.7GHz, the NFP at 800MHz, and the\nNetFPGA at 200MHz for both the N3IC -FPGA and N3IC -\nP4 implementations. The NIC under test is connected back-\nto-back to a 40Gb/s capable DPDK packet generator8. The\nhost system runs Linux, kernel v.4.18.15.\nComparison term . We compared our prototypes\nwith a software implementation that runs binary layers\n(bnn-exec ), available at [ 58].bnn-exec has been\nwritten in C, and optimized for the Haswell CPU, with\nsome parts in assembler to take fully advantage of the\nCPU’s architecture features, such as A VX instructions.\nWe setup bnn-exec to read ﬂows statistics/data from\nthe Netronome NIC and ran bnn-exec only with the\ntions, to guarantee that the probes sent to a given NIC are traversing the\nqueues whose size is inferred by such NIC.\n7We chose a Haswell CPU since it was produced with a 22nm fac-\ntory process, i.e., a technology fairly comparable to the NFP4000 and\nNetFPGA Virtex7, which were produced with 22nm and 28nm factory\nprocesses, respectively.\n8https://git :dpdk :org/apps/pktgen-dpdk/\n11\n\n111 101001K10K\nBatch size0.00.250.50.751.01.251.51.752.02.25Throughput [Million NN exec/s]N3IC-NFP\nN3IC-P4\nN3IC-FPGA\nCPUFigure 13: For trafﬁc analy-\nsis,N3IC implementations\nmatch the offered load of\n1.8M inferences per second,\nwhile forwarding packets at\n40Gb/s. This is 1.5x the\nmaximum throughput pro-\nvided by bnn-exec .\n111 101001K10K\nBatch size0.11101001K10KLatency [us]N3IC-NFP\nN3IC-P4\nN3IC-FPGA\nCPUFigure 14: For trafﬁc anal-\nysis use cases, N3IC imple-\nmentations can provide at\nleast 10-100x lower latency\nthanbnn-exec , avoiding\nthe need of performing\nbatching to amortize data\ntransfer costs.\nNetronome NIC since its driver is more mature than the\nNetFPGA’s: it can better handle fast communication be-\ntween the NIC and the host system. When performing\ninference with bnn-exec we took into account (1) the\ntime to read one or more ﬂow statistics; (2) the time to run\nthe BNN itself; and (3) the time to write back the result\non the NIC. This allows us to perform a fair comparison\nagainst N3IC .\n6.1 Trafﬁc analysis use cases\nIn both the trafﬁc classiﬁcation and anomaly detection use\ncases, we conﬁgured the NICs to collect ﬂow statistics. We\nassumed that the provided trafﬁc contains 1.8M ﬂows per\nsecond, which need to be analyzed by running NN infer-\nence9. This is a challenging load for a single server, being\nmore common in ToR switches handling trafﬁc for high\nthroughput user-facing services [ 48]. Here, we observe\nthat if N3IC can meet this performance goal, it is likely to\nbe capable of handling a large range of ordinary use cases.\nWe ﬁrst measured the NIC performance when only\ncollecting ﬂow statistics, that requires for each received\npacket: packet parsing; a lookup in a hash-table for retriev-\ning the ﬂow counters; and updating several counters. The\nNetronome provides its 40Gb/s line rate only with packets\nof size 256B (18.1Mpps) or bigger. This is achieved using\n90 out of the 480 available threads, and it is inline with\n9That is, an average of 10 packets per ﬂow at 40Gb/s@256B.\n32x16x2 64x32x2 128x64x2\nNN size0.11101001K10KLatency [us]N3IC-NFP\nN3IC-P4\nN3IC-FPGA\nCPUFigure 15: N3IC -FPGA\ncan support the network to-\nmography use case even\nin fast 400Gb/s networks\nwith probes sent every 25 µs.\nN3IC -P4 can only run this\nuse case with smaller and\nless accurate NNs.\n32x16x2 64x32x2 128x64x2\nNN size7580859095Accuracy [%]Figure 16: Box plot of the\naccuracies for the predicted\nqueues in the network to-\nmography use case. Larger\nNNs have longer execution\nlatency but better accuracy.\nthe device’s expected performance for such class of appli-\ncations. The NetFPGA, instead, is capable of forwarding\n40Gb/s with minimum size (64B) packets while collecting\nﬂow statistics.\nWe summarized the throughput results in Figure 13. The\nN3IC implementations can all achieve the offered through-\nput of 1.81M ﬂow analysis/s, while forwarding packets at\n40Gb/s (40Gb/s@256B in the case of the Netronome).\nThat is, N3IC does not reduce the packet forwarding\nperformance of the NICs. In comparison, even if using\nlarger batch sizes, bnn-exec is unable to cope with such\nload, when running on a single CPU core. bnn-exec\nmaximum throughput is 1.18M analyzed ﬂows/s, when\nusing very large batches of 10K ﬂows. More interest-\ningly, Figure 14 shows that N3IC implementations pro-\nvide also a low processing latency, with a 95-th percentile\nof 42 µsforN3IC -NFP, and only 2 µsand 0.5 µsforN3IC -\nP4 and N3IC -FPGA, respectively. In comparison, for\nbnn-exec to achieve a throughput above the 1M ﬂows/s,\nthe processing latency is 1ms and 8ms with batch sizes 1K\nand 10K, respectively.\nResult 1 : when performing trafﬁc analysis, N3IC\nsaves at least an entire CPU core, while providing\na 1.5x higher throughput and 10-100x lower pro-\ncessing latency than a system running on the host’s\nCPU.\n12\n\n6.2 Network Tomography\nWhen testing the network tomography use case, the NIC\nstores the one-way-delay value for the received network\nprobes, before passing them to the analysis engine, i.e.,\neither N3IC orbnn-exec . Here, processing latency is\nthe critical performance indicator, since in networks with\n40, 100, 400Gb/s links, SIMON requires a new set of\nprobe packets to be generated every 250, 100, 25 µs, re-\nspectively [18].\nFigure 15 shows that bnn-exec provides a process-\ning latency of about 40 µs, which is within the budget of\n100µs.10However, upcoming network links of 400Gb/s\ncould not be supported, since they would lower the peri-\nodicity of the probes to 25 µs.N3IC processing latency\nfor SIMON’s NN with 128, 64, 2 neurons is 170 µsfor\nN3IC -NFP and below 2 µsforN3IC -FPGA. As we fur-\nther clarify next, N3IC -P4 cannot scale to run such NN,\nand can only run the smaller 32, 16, 2 neurons networks\nwith about 2 µsof delay, at the cost of reduced accuracy. In\nFigure 16, we show the accuracies for the predicted queues,\nusing different network sizes. Larger networks improve\naccuracy by up to 10 percentage points. Thus, for future\nhigh-performance networks, unless a lower accuracy with\na smaller NN is tolerable, only N3IC -FPGA can meet the\nSIMON’s timing constraints.\nResult 2 : compared to a host-based system, for\nlow throughput but latency-sensitive use cases\nN3IC -FPGA can reduce processing latency by\n20x. This enables applications like SIMON to\nrun in real time in networks running at 400Gb/s\nand beyond.\n6.3 Neural Network size\nWe now evaluate the processing throughput and latency\nwhen varying the size of the binary neural network. We\nperformed this evaluation fully loading N3IC , and by exe-\ncuting a single FC layer with 256 binary inputs. We varied\nthe number of neurons to be 32, 64 and 128.11\n10We can use a batch size of 1 in this use case, since high-throughput\nis not required.\n11Notice that the FC size is number of input times number of neurons:\nthe FC layer with 128 neurons has 4KB of weights, i.e., 4x the size of\n32 64 128\nLayer size0.1110100Throughput [Million NN exec/s]N3IC-NFP\nN3IC-P4\nN3IC-FPGAFigure 17: NNs execu-\ntion throughput decreases\nlinearly with the NN size.\nN3IC -P4 cannot scale to\nlarger networks\n32 64 128\nLayer size0.11101001K10KLatency [us]N3IC-NFP\nN3IC-P4\nN3IC-FPGAFigure 18: NNs execution\nlatency increases linearly\nwith the NN size. N3IC -\nP4 cannot scale to larger net-\nworks\nFigure 17 and Figure 18 show that both metrics have\na linear decrease/increase with the NN size for N3IC -\nNFP and N3IC -FPGA, which is expected. In comparison,\nN3IC -P4 throughput results are much higher for an FC\nwith 32 and 64 neurons. Unfortunately, results for 128\nneurons are missing. As anticipated, N3IC -P4 could not\nscale to handle such layers. Both results can be explained\nwith the constraints imposed by the PISA architecture as\nimplemented by the P4-NetFPGA toolchain. The computa-\ntions of the NN described by P4 are expanded and unrolled\nin the FPGA, to serialize the execution in the pipelined\nmodel implemented by P4-NetFPGA. This consumes a\nlarge amount of the FPGA resources, thereby providing\nvery high throughput at the cost of limited scalability.\nResult 3 :N3IC -NFP’s and N3IC -FPGA’s pro-\ncessing throughput decreases linearly, while la-\ntency increases linearly, with the NN size. N3IC -\nP4 does not scale to larger NNs.\n6.4 NIC resources usage\nFinally, we quantify the NIC resources needed by N3IC\nprototypes to run the NNs used in the trafﬁc analysis use\ncases.\nIn the NFP case, N3IC has to store the NN’s weights\nin the NFP4000’s memory system. The NNs used with\nthe trafﬁc analysis use cases require 1.5% of the CLS\nmemory, and 480 threads to face the offered load, instead\nthe NN used for the trafﬁc analysis use cases.\n13\n\nof the 90 required to achieve line rate throughput when\nthe NIC is only collecting ﬂow statistics. Here, it should\nbe noted that it is possible to use less threads, as well\nas the larger and slower EMEM memories to store NN’s\nweights, if a performance drop in NN inference throughput\nis acceptable. For instance, using only 120 threads, i.e.,\n30 additional thread compared to the baseline, reduces the\nthroughput of ﬂows analyzed per second by 10x (more\ndetails in the Appendix). This still provides the ability to\nanalyze over 100k ﬂows per second, which is sufﬁcient for\nmany workloads.\nIn the NetFPGA cases, we measured the hardware re-\nsources required to synthesize N3IC implementations on\nthe Virtex7 FPGA, and compare them to the standard NetF-\nPGA reference NIC design’s resources. Table 2 summa-\nrizes the results. N3IC -FPGA requires only an additional\n0.6% and 1.2% of the FPGA’s LUTs and BRAMs, re-\nspectively. The N3IC -P4 implementation, as anticipated,\nrequires a more signiﬁcant amount of resources, with an ad-\nditional 22% for both LUTs and BRAMs, when compared\nto the reference NIC.\nIt is worth noticing that N3IC -FPGA is designed to\nmatch the required performance for the use cases, using\nminimum resources. The design allows for computing\nneuron results serially in a loop structure. E.g., we are\nable to match the throughput of N3IC -P4 for a FC layer\nof 32 neurons by using 16 NN Executor modules in par-\nallel within N3IC -FPGA. The N3IC -FPGA’s throughout\nscales linearly with the number of modules, but so does\nthe resources usage, which grows to additional 10% of the\nLUTs and 19% of the BRAMs.12\nResult 4 :N3IC -FPGA consumes a very small\nfraction of the NetFPGA resources, and both\nN3IC -FPGA and N3IC -NFP can be conﬁgured\nto trade-off used NICs’ resources with provided\nperformance.\n12We did not optimize the NN Executor modules to share the use of\nCAMs across modules. It is thus likely possible to considerably reduce\nthis BRAM utilization if needed.Table 2: NetFPGA resources usage. N3IC -FPGA requires\nlittle additional resources. N3IC -P4 uses a large amount\nof NIC resources due to the PISA computation model\nconstraints.\nDESIGN LUT (#, % TOT)BRAM (#, % TOT)\nREFERENCE NIC 49.4 K, 11.4% 194, 13.2%\nN3IC -FGPA 52.0 K, 12.0% 211, 14.4%\nN3IC -P4 144.5 K, 33.4% 518, 35.2%\n7 Discussion\nOur evaluation results show that N3IC can beneﬁt a num-\nber of networking use cases. However, like in similar\nsolutions that leverage in-network computation [ 48,35,\n39,11,41,64,8], the beneﬁts of bringing the inference\ncloser to the wire come at a cost: (1) there is a need to\nconvert the machine learning models in binary NNs; and\n(2) applications have to be designed taking into account\nthe network-host split.\nA second interesting observation is that it is possible to\nreadily implement N3IC on commercial programmable\nNICs, using existing programming models such as microC\nand P4, even though these implementations come with\nsome limitations. N3IC -NFP is unable to meet latency\nrequirements in the order of the µs, while N3IC -P4 is\ninefﬁcient in using the hardware resources, which signif-\nicantly limits the NNs maximum size. Both limitations\ncan be overcame if the NIC supports natively binary NN\nexecution as a primitive with a dedicated hardware mod-\nule, as shown by N3IC -FPGA. In fact, our tests show that\nsupporting such primitive comes at very little overhead in\nterms of required hardware resources.\nFurthermore, N3IC -FPGA can be considerably more\nefﬁcient in running the NNs, thereby enabling larger NNs,\nor even supporting multiple different NNs at the same\ntime. For instance, in the network tomography use case,\nour modiﬁed version of SIMON runs several small NNs,\neach predicting the congestion status of a given queue.\nInN3IC -NFP, running them requires a good share of the\nnetwork processor’s threads to compute the NNs in parallel,\nleaving little space for other computations. Instead, N3IC -\nFPGA uses a single NN executor module, which serially\nprocesses NNs one after the other, while still respecting\n14\n\nthe strict timing constraints of the use case. Each NN\nexecution takes in fact few µs, and given the small resource\noverhead of the NN executor module, it would be possible\nto include multiple of them if the need arises.\nFinally, it should be noted that N3IC is not a generic\ntool to run any NN on the NIC. For large NNs, e.g., with\nthousands of neurons, its execution time may grow signiﬁ-\ncantly, making the overhead of the PCIe transfer relatively\nlower. In such cases, a better choice is the use of a differ-\nent NN executor, i.e., BrainWave and Taurus, or keeping\nthe computation on the host system. More details in the\nAppendix.\n8 Related Work\nN3IC relates with the works that move machine learn-\ning capabilities into programmable hardware. This has\nbeen explored in the context of programmable switches [ 8,\n79], or by using fully-ﬂedged network-attached acceler-\nators [ 49,70]. In this paper, instead, we show that it is\npossible to design and implement a signiﬁcantly smaller\nscale NN executor in the data plane of commodity off-the-\nshelf programmable NICs.\nIn this direction, we extend some ideas presented\nin [68,62]. However, those works focused either on a\nconceptual design for RMT [ 6] switches [ 68], or on end-\nhost ML applications, in which the NIC works as a co-\nprocessor for CNNs running on the host. Instead, in this\npaper, we present a complete evaluation of BNN executors\non two NICs, propose a dedicated hardware-native imple-\nmentation, and include an end-to-end evaluation of three\nnetworking use cases, with related trade-offs in terms of\nmodel size and speciﬁc hardware limitations. N3IC can be\nalso positioned in the larger trend of in-network computing\nresearch. In fact, researchers have been exploring ways to\napply programmable switches to domains not necessarily\nrelated to networking, such as key-value store [ 41], dis-\ntributed consensus [ 11], and the acceleration of parameter\nservers for ML model training [ 63,64]. Another related\narea is the design of hardware for NN accelerators. A valu-\nable survey on the topic is provided by [ 71]. Particularly\nrelevant to our work are architectures such as YodaNN [ 1]\nand FINN [ 77]. Finally, while not directly related to N3IC ,\nrecent work on the security of network applications that\nuse machine learning [ 47] is likely to inﬂuence develop-ments in this area.\n9 Conclusion\nIn this paper, we presented N3IC , a technique to run\nneural networks in the data plane of commodity pro-\ngrammable NICs. We experimentally evaluated N3IC\non the Netronome NFP4000 and on the NetFPGA. We\nshowed that N3IC can greatly beneﬁt network applications\ndiscussing three use cases: trafﬁc classiﬁcation, anomaly\ndetection and network tomography. Our results show that\nN3IC can be readily implemented in existing commercial\nprogrammable NICs. By doing so, in the trafﬁc analy-\nsis use cases, we demonstrated that N3IC provides 1.5x\nhigher throughput and 100x lower latency than a state-of-\nthe-art software implementation, while saving precious\nCPU resources. At the same time, supporting N3IC as\na primitive with dedicated hardware requires only little\nadditional hardware resources: the N3IC NetFPGA im-\nplementation increases the logic and memory resources of\na standard NIC design by less than 2%, and it enables a\nreal time network tomography use case, which would be\notherwise unfeasible.\nReferences\n[1]Renzo Andri, Lukas Cavigelli, Davide Rossi, and\nLuca Benini. YodaNN: an ultra-low power convo-\nlutional neural network accelerator based on binary\nweights. In Annual Symposium on VLSI (ISVLSI) .\nIEEE, 2016.\n[2]Hitesh Ballani, Paolo Costa, Christos Gkantsidis,\nMatthew P. Grosvenor, Thomas Karagiannis, Lazaros\nKoromilas, and Greg O’Shea. Enabling End-Host\nNetwork Functions. In Special Interest Group on\nData Communication (SIGCOMM) . ACM, 2015.\n[3]Tom Barbette, Cyril Soldani, and Laurent Mathy.\nFast Userspace Packet Processing. In Architec-\ntures for Networking and Communications Systems\n(ANCS) . IEEE/ACM, 2015.\n[4]M. Beeler, R.W. Gosper, and R. Schroeppel. Hak-\nmem AI Memo No. 239. In MIT Artiﬁcial Intelli-\ngence Laboratory, Cambridge, US , 1972.\n15\n\n[5]Pat Bosshart, Dan Daly, Glen Gibb, Martin Izzard,\nNick McKeown, Jennifer Rexford, Cole Schlesinger,\nDan Talayco, Amin Vahdat, George Varghese,\nand David Walker. P4: Programming Protocol-\nindependent Packet Processors. In Computer Com-\nmunication Review, Volume: 44, Issue: 3 . ACM,\n2014.\n[6]Pat Bosshart, Glen Gibb, Hun-Seok Kim, George\nVarghese, Nick McKeown, Martin Izzard, Fernando\nMujica, and Mark Horowitz. Forwarding Metamor-\nphosis: Fast Programmable Match-action Processing\nin Hardware for SDN. In Special Interest Group on\nData Communication (SIGCOMM) . ACM, 2013.\n[7]Tomasz Bujlow, Valent ´ın Carela-Espa ˜nol, and Pere\nBarlet-Ros. Independent comparison of popular DPI\ntools for trafﬁc classiﬁcation. In Computer Networks,\nVolume: 76, Issue: C . Elsevier, 2015.\n[8]Coralie Busse-Grawitz, Roland Meier, Alexan-\nder Dietmuller, Tobiad Buhler, and Laurent Van-\nbever. pForest: In-Network Inference with Random\nForests. In Computing Research Repository, Volume:\nabs/1909.05680 , 2019.\n[9]Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vas-\nconcelos. Deep Learning with Low Precision by\nHalf-wave Gaussian Quantization. In Computing Re-\nsearch Repository, Volume: abs/1702.00953 , 2017.\n[10] Matthieu Courbariaux, Itay Hubara, Daniel Soudry,\nRan El-Yaniv, and Yoshuao Bengio. Binarized\nneural networks: Training deep neural networks\nwith weights and activations constrained to+ 1 or-\n1. In Computing Research Repository, Volume:\nabs/1602.02830 , 2016.\n[11] Huynh Tu Dang, Marco Canini, Fernando Pedone,\nand Robert Soul ´e. Paxos made switch-y. In Computer\nCommunication Review, Volume: 46, Issue: 2 . ACM,\n2016.\n[12] Haggai Eran, Lior Zeno, Maroun Tork, Gabi Malka,\nand Mark Silberstein. NICA: An Infrastructure for\nInline Acceleration of Network Applications. In An-\nnual Technical Conference (ATC) . USENIX Associa-\ntion, 2019.[13] Hadi Esmaeilzadeh, Emily Blem, Renee St. Amant,\nKarthikeyan Sankaralingam, and Doug Burger. Dark\nsilicon and the end of multicore scaling. In Interna-\ntional Symposium on Computer Architecture (ISCA) .\nACM, 2011.\n[14] Nick Feamster and Jennifer Rexford. Why (and How)\nNetworks Should Run Themselves. In Applied Net-\nworking Research Workshop (ANRW) . ACM, 2018.\n[15] Daniel Firestone. Building hardware-accelerated\nnetworks at scale in the c/guloud, Jan 2019.\nhttps://conferences :sigcomm :org/\nsigcomm/2017/files/program-kbnets/\nkeynote-2 :pdf.\n[16] Daniel Firestone, Andrew Putnam, Sambhrama\nMundkur, Derek Chiou, Alireza Dabagh, Mark An-\ndrewartha, Hari Angepat, Vivek Bhanu, Adrian\nCaulﬁeld, Eric Chung, Harish K. Chandrappa,\nSomesh Chaturmohta, Matt Humphrey, Jack Lavier,\nNorman Lam, Fengfen Liu, Kalin Ovtcharov, Jitu\nPadhye, Gautham Popuri, Shachar Raindel, Tejas\nSapre, Mark Shaw, Gabriel Silva, Madhan Sivaku-\nmar, Nisheeth Srivastava, Anshuman Verma, Qasim\nZuhair, Deepak Bansal, Doug Burger, Kushagra Vaid,\nDavid A. Maltz, and Albert Greenberg. Azure Accel-\nerated Networking: SmartNICs in the Public Cloud.\nInNetworked Systems Design and Implementation\n(NSDI) . USENIX, 2018.\n[17] George Forman. An extensive empirical study of\nfeature selection metrics for text classiﬁcation. In\nJournal of machine learning research, Volume: 3 .\nJMLR.org, 2003.\n[18] Yilong Geng, Shiyu Liu, Zi Yin, Ashish Naik, Balaji\nPrabhakar, Mendel Rosenblum, and Amin Vahdat.\nSIMON: A Simple and Scalable Method for Sensing,\nInference and Measurement in Data Center Networks.\nInNetworked Systems Design and Implementation\n(NSDI) . USENIX, 2019.\n[19] Mojgan Ghasemi, Theophilus Benson, and Jennifer\nRexford. Dapper: Data Plane Performance Diagnosis\nof TCP. In Symposium on SDN Research (SOSR) .\nACM, 2017.\n16\n\n[20] Prabhat K. Gupta. Xeon+FPGA Platform\nfor the Data Center, 2019. https://\nwww:ece:cmu:edu/ ˜calcm/carl/lib/exe/\nfetch :php?media=carl15-gupta :pdf.\n[21] Song Han, Huizi Mao, and William J. Dally. Deep\nCompression: Compressing Deep Neural Network\nwith Pruning, Trained Quantization and Huffman\nCoding. In Computing Research Repository, Volume:\nabs/1510.00149 , 2015.\n[22] Nikos Hardavellas. The rise and fall of dark silicon.\nIn;login:, Volume: 37 . USENIX, 2012.\n[23] Nikos Hardavellas, Michael Ferdman, Babak Fal-\nsaﬁ, and Anastasia Ailamaki. Toward dark silicon in\nservers. In Micro, Volume: 31, Issue: 4 . IEEE, 2011.\n[24] Kim Hazelwood, Sarah Bird, David Books, Soumith\nChintala, Utku Diril, Dmytro Dzhulgakov, Mohamed\nFawzy, Bill Jia, Yangqing Jia, Aditya Kalro, James\nLaw, Kevin Lee, Jason Lu, Pieter Noordhuis, Misha\nSmelyanskiy, Liang Xiong, and Xiaodong Wang. Ap-\nplied Machine Learning at Facebook: A Datacen-\nter Infrastructure Perspective. In High Performance\nComputer Architecture (HPCA) . IEEE, 2018.\n[25] Kim Hazelwood, Sarah Bird, David Brooks, Soumith\nChintala, Utku Diril, Dmytro Dzhulgakov, Mohamed\nFawzy, Bill Jia, Yangqing Jia, Aditya Kalro, James\nLaw, Kevin Lee, Jason Lu, Pieter Noordhuis, Misha\nSmelyanskiy, Liang Xiong, and Xiaodong Wang. Ap-\nplied machine learning at Facebook: a datacenter\ninfrastructure perspective. In High Performance Com-\nputer Architecture (HPCA) . IEEE, 2018.\n[26] John L. Hennessy and David A. Patterson. A New\nGolden Age for Computer Architecture. In Commu-\nnications of the ACM, Volume: 62, Issue: 2 . ACM,\n2019.\n[27] Kur Hornik, Maxwell Stinchcombe, and Halber\nWhite. Multilayer feedforward networks are uni-\nversal approximators. In Neural Networks, Volume:\n2, Issue: 5 . Elsevier Science Ltd., 1989.\n[28] Itay Hubara, Matthieu Courbariaux, Daniel Soudry,\nRan El-Yaniv, and Yoshua Bengio. Binarized neuralnetworks. In Neural Information Processing Systems\n(NIPS) . Curran Associates Inc., 2016.\n[29] Stephen Ibanez, Gordon Brebner, Nick McKeown,\nand Noa Zilberman. The p4-netfpga workﬂow for\nline-rate packet processing. In Field-Programmable\nGate Arrays (FPGA) . ACM, 2019.\n[30] Norman P. Jouppi, Cliff Young, Nishant Patil, David\nPatterson, Gaurav Agrawal, Raminder Bajwa, Sarah\nBates, Suresh Bhatia, Nan Boden, Al Borchers,\nRick Boyle, Pierre-luc Cantin, Clifford Chao, Chris\nClark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey\nDean, Ben Gelb, Tara Vazir Ghaemmaghami, Ra-\njendra Gottipati, William Gulland, Robert Hagmann,\nC. Richard Ho, Doug Hogberg, John Hu, Robert\nHundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek\nJaworski, Alexander Kaplan, Harshit Khaitan, Daniel\nKillebrew, Andy Koch, Naveen Kumar, Steve Lacy,\nJames Laudon, James Law, Diemthu Le, Chris Leary,\nZhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon\nMacKean, Adriana Maggiore, Maire Mahony, Kieran\nMiller, Rahul Nagarajan, Ravi Narayanaswami, Ray\nNi, Kathy Nix, Thomas Norrie, Mark Omernick,\nNarayana Penukonda, Andy Phelps, Jonathan Ross,\nMatt Ross, Amir Salek, Emad Samadiani, Chris Sev-\nern, Gregory Sizikov, Matthew Snelham, Jed Souter,\nDan Steinberg, Andy Swing, Mercedes Tan, Gre-\ngory Thorson, Bo Tian, Horia Toma, Erick Tuttle,\nVijay Vasudevan, Richard Walter, Walter Wang, Eric\nWilcox, and Doe Hyun Yoon. In-datacenter perfor-\nmance analysis of a Tensor Processing Unit. In In-\nternational Symposium on Computer Architecture\n(ISCA) . ACM, 2017.\n[31] Thomas Karagiannis, Konstantina Papagiannaki, and\nMichalis Faloutsos. BLINC: Multilevel Trafﬁc Clas-\nsiﬁcation in the Dark. In Special Interest Group on\nData Communication (SIGCOMM) . ACM, 2005.\n[32] Vadim Karpusenko, Andres Rodriguez, Jacek Czaja,\nand Mariusz Moczala. Caffe* optimized for Intel\narchitecture: applying modern code techniques. In\nTechnical Report . Intel, 2016.\n[33] Georgios Kathareios, Andreea Anghel, Akos Mate,\nRolf Clauberg, and Mitch Gusat. Catch It If You Can:\nReal-Time Network Anomaly Detection with Low\n17\n\nFalse Alarm Rates. In International Conference on\nMachine Learning and Applications (ICMLA) . IEEE,\n2017.\n[34] Georgios P. Katsikas, Tom Barbette, Dejan Kostiun-\ndeﬁned, Rebecca Steinert, and Gerald Q. Maguire.\nMetron: NFV Service Chains at the True Speed of the\nUnderlying Hardware. In Networked Systems Design\nand Implementation (NSDI) . USENIX Association,\n2018.\n[35] Daehyeok Kim, Amirsaman Memaripour, Anirudh\nBadam, Yibo Zhu, Hongqiang Harry Liu, Jitu Padhye,\nShachar Raindel, Steven Swanson, Vyas Sekar, and\nSrinivasan Seshan. Hyperloop: Group-based NIC-\nofﬂoading to Accelerate Replicated Transactions in\nMulti-tenant Storage Systems. In Special Interest\nGroup on Data Communication (SIGCOMM) . ACM,\n2018.\n[36] Tim Kraska, Mohammad Alizadeh, Alex Beutel,\nEd H. Chi, Jialin Ding, Ani Kristo, Guillaume\nLeclerc, Samuel Madden, Hongzi Mao, and Vikram\nNathan. SageDB: a learned database system. In\nConference on Innovative Data Systems Research\n(CIDR) , 2019.\n[37] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean,\nand Neoklis Polyzotis. The case for learned index\nstructures. In Conference on Management of Data\n(SIGMOD) . ACM, 2018.\n[38] Edward H. Lee, Daisuke Miyashita, Elaina Chai,\nBoris Murmann, and Simon S. Wong. LogNet:\nEnergy-efﬁcient neural networks using logarithmic\ncomputation. In International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) . IEEE,\n2017.\n[39] Bojie Li, Zhenyuan Ruan, Wencong Xiao, Yuan-\nwei Lu, Yongqiang Xiong, Andrew Putnam, En-\nhong Chen, and Lintao Zhang. KV-Direct: High-\nPerformance In-Memory Key-Value Store with Pro-\ngrammable NIC. In Symposium on Operating Sys-\ntems Principles (SOSP) . ACM, 2017.\n[40] Fengfu Li and Bin Liu. Ternary Weight Net-\nworks. In Computing Research Repository, Volume:\nabs/1605.04711 , 2016.[41] Xiaozhou Li, Raghav Sethi, Michael Kaminsky,\nDavid G. Andersen, and Michael J. Freedman. Be\nfast, cheap and in control with switchkv. In Net-\nworked Systems Design and Implementation (NSDI) .\nUSENIX, 2016.\n[42] Eric Liang, Hang Zhu, Xin Jin, and Ion Stoica. Neu-\nral Packet Classiﬁcation. In Special Interest Group\non Data Communication (SIGCOMM) . ACM, 2019.\n[43] Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accu-\nrate binary convolutional neural network. In Neural\nInformation Processing Systems (NIPS) . Curran As-\nsociates, Inc., 2017.\n[44] Layong Luo. Towards converged smartNIC ar-\nchitecture for bare metal & public clouds, 2018.\nhttps://conferences :sigcomm :org/\nevents/apnet2018/slides/larry :pdf.\n[45] Joao Martins, Mohamed Ahmed, Costin Raiciu, and\nFelipe Huici. Enabling Fast, Dynamic Network Pro-\ncessing with ClickOS. In Hot Topics in Software\nDeﬁned Networking (HotSDN) . ACM, 2013.\n[46] Joao Martins, Mohamed Ahmed, Costin Raiciu,\nVladimir Olteanu, Michio Honda, Roberto Bifulco,\nand Felipe Huici. ClickOS and the Art of Network\nFunction Virtualization. In Networked Systems De-\nsign and Implementation (NSDI) . USENIX Associa-\ntion, 2014.\n[47] Roland Meier, Thomas Holterbach, Stephan Keck,\nMatthias St ¨ahli, Vincent Lenders, Ankit Singla, and\nLaurent Vanbever. (Self) Driving Under the Inﬂuence:\nIntoxicating Adversarial Network Inputs. In Hot\nTopics in Networks (HotNets) . ACM, 2019.\n[48] Rui Miao, Hongyi Zeng, Changhoon Kim,\nJeongkeun Lee, and Minlan Yu. SilkRoad: making\nstateful layer-4 load balancing fast and cheap using\nswitching ASICs. In Special Interest Group on Data\nCommunication (SIGCOMM) . ACM, 2017.\n[49] Microsoft. Microsoft unveils project brain-\nwave for real-time ai, 2017. https:\n//www :microsoft :com/en-us/research/\nblog/microsoft-unveils-project-\nbrainwave/ .\n18\n\n[50] Andrew W. Moore and Denis Zuev. Internet Trafﬁc\nClassiﬁcation Using Bayesian Analysis Techniques.\nInConference on Measurement and Modeling of\nComputer Systems (SIGMETRICS) . ACM, 2005.\n[51] Masoud Moshref, Minlan Yu, Ramesh Govindan, and\nAmin Vahdat. Trumpet: Timely and Precise Triggers\nin Data Centers. In Special Interest Group on Data\nCommunication (SIGCOMM) . ACM, 2016.\n[52] Nour Moustafa and Jill Slay. UNSW-NB15: a com-\nprehensive data set for network intrusion detection\nsystems (UNSW-NB15 network data set). In Military\nCommunications and Information Systems Confer-\nence (MilCIS) . IEEE, 2015.\n[53] Netronome. Netronome AgilioTM\nCX 2x40GbE intelligent server adapter,\n2018. https://www :netronome :com/\nmedia/redactor files/\nPBAgilio CX2x40GbE :pdf.\n[54] Netronome. Packet and Netronome inno-\nvate on smart networking-focused edge com-\npute hardware, December 2018. https:\n//www :netronome :com/press-releases/\npacket-and-netronome-innovate-\nsmart-networking-focused-edge-\ncompute-hardware/ .\n[55] Rolf Neugebauer, Gianni Antichi, Jos ´e Fernando\nZazo, Yury Audzevich, Sergio L ´opez-Buedo, and An-\ndrew W. Moore. Understanding PCIe Performance\nfor End Host Networking. In Special Interest Group\non Data Communication (SIGCOMM) . ACM, 2018.\n[56] Nvidia. GPUDirect Technology, [On-\nline; accessed 10-Jan-2020]. https:\n//developer :nvidia :com/gpudirect .\n[57] Kay Ousterhout, Ryan Rasti, Sylvia Ratnasamy,\nScott Shenker, and Byung-Gon Chun. Making\nSense of Performance in Data Analytics Frameworks.\nInNetworked Systems Design and Implementation\n(NSDI) . USENIX, 2015.\n[58] Paper anonymized authors. Paper anonymized\nsource code repository, 2019. https://paper-\nanonymized-link .[59] Vern Paxon. The Zeek Network Security Moni-\ntor, [Online; accessed 04-Feb-2020]. https://\nwww:zeek :org/ .\n[60] Mohammad Rastegari, Vicente OrdonezJ, Joseph\nRedmon, and Ali Farhadi. XNOR-Net: imageNet\nclassiﬁcation using binary convolutional neural net-\nworks. In Lecture Notes in Computer Science, Vol-\nume: 9908 . Springer, Cham, 2016.\n[61] Arjun Roy, Hongyi Zeng, Jasmeet Bagga, George\nPorter, and Alex C. Snoeren. Inside the Social Net-\nwork’s (Datacenter) Network. In Special Interest\nGroup on Data Communication (SIGCOMM) . ACM,\n2015.\n[62] Davide Sanvito, Giuseppe Siracusano, and Roberto\nBifulco. Can the network be the AI accelerator? In\nIn-Network Computing (NetCompute) . ACM, 2018.\n[63] Amedeo Sapio, Ibrahim Abdelaziz, Abdulla Aldilai-\njan, Marco Canini, and Panos Kalnis. In-network\ncomputation is a dumb idea whose time has come. In\nHot Topics in Networks (HotNets) . ACM, 2017.\n[64] Amedeo Sapio, Marco Canini, Chen-Yu Ho, Jacob\nNelson, Panos Kalnis, Changhoon Kim, Arvind Kr-\nishnamurthy, Masoud Moshref, Dan R.K. Ports, and\nPeter Richtarik. Scaling Distributed Machine Learn-\ning with In-Network Aggregation. In Computing Re-\nsearch Repository, Volume: abs/1903.06701 , 2019.\n[65] Vyas Sekar, Norbert Egi, Sylvia Ratnasamy,\nMichael K. Reiter, and Guangyu Shi. Design and\nImplementation of a Consolidated Middlebox Archi-\ntecture. In Networked Systems Design and Implemen-\ntation (NSDI) . USENIX Association, 2012.\n[66] Justine Sherry, Peter Xiang Gao, Soumya Basu, Au-\nrojit Panda, Arvind Krishnamurthy, Christian Ma-\nciocco, Maziar Manesh, Jo ˜ao Martins, Sylvia Rat-\nnasamy, Luigi Rizzo, and et al. Rollback-Recovery\nfor Middleboxes. In Special Interest Group on Data\nCommunication (SIGCOMM) . ACM, 2015.\n[67] Karen Simonyan and Andrew Zisserman. Very deep\nconvolutional networks for large-scale image recog-\nnition. In Computing Research Repository, Volume:\nabs/1409.1556 , 2014.\n19\n\n[68] Giuseppe Siracusano and Roberto Bifulco. In-\nnetwork neural networks. In Computing Research\nRepository, Volume: abs/1801.05731 , 2018.\n[69] Anirudh Sivaraman, Alvin Cheung, Mihai Budiu,\nChanghoon Kim, Mohammad Alizadeh, Hari Bal-\nakrishnan, George Varghese, Nick McKeown, and\nSteve Licking. Packet transactions: high-level pro-\ngramming for line-rate switches. In Special Interest\nGroup on Data Communication (SIGCOMM) . ACM,\n2016.\n[70] Tushar Swamy, Alexander Rucker, Muhammad Shah-\nbaz, Neeraja Yadwadkar, Yaqi Zhang, and Kunle\nOlukotun. Taurus: An Intelligent Data Plane. In P4\nWorkshop , 2019.\n[71] Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel\nEmer. Efﬁcient processing of deep neural networks:\na tutorial and survey. In Proceedings of the IEEE,\nVolume: 105, Issue: 12 . IEEE, 2017.\n[72] Mellanox Technologies. BlueField Smart-\nNIC, 2019. http://www :mellanox :com/\nrelated-docs/prod adapter cards/\nPBBlueField Smart NIC:pdf.\n[73] Mellanox Technologies. TILEncore-Gx72,\n2019. https://www :mellanox :com/\npage/products dyn?product family=\n231&mtag=tilencore gx72 adapter mtag .\n[74] The University of Washington NS-3 Consortium.\nNS3 ofﬁcial website, [Online; accessed 10-Jan-2020].\nhttps://www :nsnam :org/ .\n[75] Olivier Tilmans, Tobias B ¨uhler, Ingmar Poese, Ste-\nfano Vissicchio, and Laurent Vanbever. Stroboscope:\nDeclarative Network Monitoring on a Budget. In Net-\nworked Systems Design and Implementation (NSDI) .\nUSENIX Association, 2018.\n[76] Gareth Tyson, Shan Huang, Felix Cuadrado, Igna-\ncio Castro, Vasile C Perta, Arjuna Sathiaseelan, and\nSteve Uhlig. Exploring HTTP header manipulation\nin-the-wild. In World Wide Web (WWW) . Interna-\ntional World Wide Web Conferences Steering Com-\nmittee, 2017.[77] Yaman Umuroglu, Nicholas J. Fraser, Giulio Gam-\nbardella, Michaela Blott, Philip Leong, Magnus\nJahre, and Kees Vissers. FINN: A Framework for\nFast, Scalable Binarized Neural Network Inference.\nInField-Programmable Gate Arrays (FPGA) . ACM,\n2017.\n[78] Xilinx. SDNet compiler, 2019. https://\nwww:xilinx :com/sdnet .\n[79] Zhaoqi Xiong and Noa Zilberman. Do Switches\nDream of Machine Learning? Toward In-Network\nClassiﬁcation. In Hot Topics in Networks (HotNets) .\nACM, 2019.\n[80] Chaoyun Zhang, Paul Patras, and Hamed Haddadi.\nDeep learning in mobile and wireless networking: A\nsurvey. In Computing Research Repository, Volume:\nabs/1803.04311 , 2018.\n[81] Shuchang Zhou, Zekun Ni, Xinyu Zhou, He Wen,\nYuxin Wu, and Yuheng Zou. DoReFa-Net: Training\nLow Bitwidth Convolutional Neural Networks with\nLow Bitwidth Gradients. In Computing Research\nRepository, Volume: abs/1606.06160 , 2016.\n[82] Chenzhuo Zhu, Song Han, Huizi Mao, and William J.\nDally. Trained Ternary Quantization. In Comput-\ning Research Repository, Volume: abs/1612.01064 ,\n2016.\n[83] Noa Zilberman, Yury Audzevich, Adam G. Coving-\nton, and Andrew W. Moore. NetFPGA SUME: to-\nward 100 Gbps as research commodity. In Micro,\nVolume: 34, Issue: 5 . IEEE, 2014.\n20\n\nData Parallel  Thread 1  \nINPUT  \nThread 2  \nThread 3  Model Parallel  NN model  \nINPUT  \n INPUT  \nINPUT  \nThread 1  \nThread 2  \nThread 3  NN model  Figure 19: N3IC -NFP data parallel and model parallel\nmodes to process NNs.\nA Netronome: implementation de-\ntails\nThis section reports additional detail regarding the N3IC\nimplementation on the Netronome NFP4000 when run-\nning large NNs. Netronome NFP4000 NICs have several\ndifferent memory areas, with different sizes and access\nperformance. Table 3 summarizes the memory properties.\nTable 3: Access times and size for the different memory\nareas of an NFP4000.\nMEMORY ACCESS TIME (NS) MEMORY\nTYPE MIN MAX SIZE\nCLS 25 62.5 64KB\nCTM 62.5 125 256KB\nIMEM 187.5 312.5 4MB\nEMEM 312.5 625 3MB\nWhen NNs are large, running them in a single ME’s\nthread would take a long time, making the use of multi-\nple threads more effective, even if synchronization among\nthreads incurs some overhead. Furthermore, the use of\nmultiple threads allows the NFP to context switch them\nwhen waiting for memory, hiding memory access time\nand enabling a more effective use of the MEs’ process-\ning power. For this reason depending on the NN size, we\nprovided two different operation modes for N3IC over\nNetronome: (1) data parallel and (2) model parallel (cf.\nT \n T \n T \n T \n T \n T \nDispatcher  Inference execution chain  \nNew  \nPacket  \nStart notification  \nSend  \nPacket  \nLocal CTM  \n \n \n Packet Data  EMEM  \n \n \n Layers Weights  \nEnd notification  \nStart notification  \nEnd notification  \nFigure 20: MEs’ threads dedicated to inference are or-\nganized in a notiﬁcation chain that is executed mostly in\nparallel. All threads read neuron’s weights from a contigu-\nous memory area located in the EMEM. The network input\nis encapsulated in the packet data, located in the CTM of\nthe dispatching thread’s island.\nFig 19). The former, described in x4, is preferable when\nNN size is small. The latter, instead, when NN is big. In\nModel Parallel mode MEs’ threads are conﬁgured with\ntwo different types of processing tasks. A ﬁrst subset\nof threads registers for packet reception notiﬁcation. We\ncall such threads dispatchers , and distribute them evenly\nacross the available MEs and islands. In our implementa-\ntion, we empirically found that two of such threads per ME\nis enough to achieve our performance goals. The second\nsubset of threads, named executors , is instead dedicated\nto the processing of NNs, and organized in an execution\nchain . At boot time, dispatchers registers themselves for\npacket reception notiﬁcations. When a new network packet\nis received, the notiﬁed dispatcher parses the packet and\nperforms the regular forwarding function. If a NN process-\ning is triggered, a NN processing function is executed by\nthe MEs’ threads in the execution chain. The dispatcher\nworks as a coordinator for the NN processing, by starting\nthe processing of one NN layer, and waiting for the result\nbefore starting the processing of the next layer. Figure 20\ndepicts this process.\nExecution Chain The execution chain is statically deﬁned,\nwith each thread knowing its predecessor and successor\n21\n\nthreads at boot time. To start processing a layer, the dis-\npatcher notiﬁes the ﬁrst thread in the chain with a start\nnotiﬁcation , which is then propagated throughout the chain,\nwith each thread notifying its successor, until the last\nthread. After receiving the start notiﬁcation, and send-\ning it to the next thread in the chain, a thread performs the\ncomputation of a subset of the current layer’s neurons, with\nthe actual number depending on the number of neurons in\nthe layer and the level of conﬁgured execution parallelism.\nEach of the threads is an executor from the perspective\nofN3IC , so the conﬁgured number of executors (threads)\ndeﬁnes the level of parallelism. For instance, for a layer\nwith 4096 neurons, and using 128 executors, each executor\nwould be in charge of computing 32 neurons. This effec-\ntively means, e.g., that N3IC would use 32 MEs, and 4\nthreads per ME, with each of the threads processing 32\nneurons: 32\u00024\u000232 = 4096 . The execution of the neu-\nrons happens like in the data parallel case, with the main\ndifference being that the model’s weights are stored in\nthe DRAM-backed EMEM. Here, the weights are placed\nin a contiguous memory space, which allows an executor\nto directly point to the weights of a set of neurons given\nits position in the execution chain. At the end of a layer\ncomputation, each executor writes its ﬁnal result to the\nglobal IMEM memory, from which the dispatching thread\ncan read it. The last executor in the chain sends an end no-\ntiﬁcation to its predecessor after writing its portion of the\nresult to IMEM. The notiﬁcation is propagated backward\nthrough the chain as all executors conclude their compu-\ntations and write their results, until it is received by the\ndispatcher. Here, either the computation for a new layer is\nstarted or the ﬁnal result is collected.\nIt is worth noticing that the spatial organization of the\nMEs, which are distributed across islands and at different\ndistances from each other, makes more efﬁcient the notiﬁ-\ncation chain, when compared to other mechanisms, such\nas broadcast messages. That is, the notiﬁcation system is\nfaster even if some MEs remain idle while waiting to prop-\nagate the end notiﬁcation back to the dispatcher. Unfortu-\nnately, explicit notiﬁcations are required since each ME’s\nexecution time depends on the memory reads from the\nEMEM, which may take a variable amount of time. Sec-\nond, the notiﬁcation propagation time is relatively short,\nmaking the use of an asymmetric number of threads (or\nneurons to compute) per-ME inefﬁcient. For instance one\ncould assign more work to MEs early in the chain, but thisin fact rises a problem of stragglers that harms the overall\nperformance.\nB Evaluation details\nB.1 NFP4000\nDuring the development and evaluation of N3IC -NFP we\nrun a number of benchmarks to understand the nuances\nof the NFP4000 architecture, and to tune our implemen-\ntation. In all the tests, the system is always loaded with\n40Gb/s@256B. Next we report a subset of those results.\nB.1.1 Benchmarks for small NNs (data parallel)\nTo better characterize the data parallel performance, we\nmeasured the packet forwarding capabilities and analyzed\nﬂow/s the system achieves when using different ﬂow ar-\nrival rates, in the orders of 10k, 100k and 1M new ﬂows\nper second, and executing a NN inference for each new\nreceived ﬂow. This should cover a wide spectrum of possi-\nble workloads [ 48]. Furthermore, we changed the number\nof NFP’s threads used by N3IC , and measured the NN ex-\necution time for different conﬁgurations. Figure 21 shows\nthatN3IC matches the baseline performance by using 120\nthreads, i.e., 30 more threads than baseline, when handling\n200k new ﬂows per second and performing as many NN\nexecutions per second. This conﬁrms that the computa-\ntion of the per-ﬂow statistics is a memory-bound operation\nfor the NFP4000, which therefore has idle computing re-\nsources that could be leveraged for NN execution. When\nfurther increasing the threads to 240 and 480, N3IC can\ncome close to, or match, the baseline performance even\nwhile processing about 2M NN executions per second,\nas mentioned inx6. To check the maximum achievable\nthroughput under heavy load, we conﬁgured the NFP to\nprocess the trafﬁc classiﬁcation’s NN for each received\npacket, i.e., a stress test. In this case, Figure 21 shows\nN3IC can forward 7.1Mpps, i.e., line rate of 40 Gb/s for\npacket sizes bigger than 512B, while running a NN for\neach packet using the 480 threads conﬁguration.\nNN Size (Figure 22) . In data parallel mode we placed a\ncopy of the the BNN’s weights in each of the available CLS\nmemories, since each island is provided with a dedicated\none. The weights are accessed in read-only mode and\n22\n\n0.01 0.1 1 10\nThroughput [Million NN exec/s]02468101214161820Packet rate [Mpps]\n40 Gb/s\n@256B\n@512B\n@1024B\n@1514BBaseline\nN3IC 480\nN3IC 240\nN3IC 120Figure 21: N3IC -NFP data paral-\nlel forwarding performance (y axis),\nwhen processing 40Gb/s@256B, as\na function of the number of ﬂows to\nanalyze (x axis). X axis in log scale .\n32 64 128\nLayer size02468Throughput [Million NN exec/s]N3IC 120\nN3IC 240\nN3IC 480Figure 22: N3IC -NFP data parallel\nmaximum BNN execution through-\nput (y axis) as a function of the BNN\nsize (x axis). The throughput scales\nlinearly with the BNN size.\n83264120240360480\nNumber of threads0.010.1110Packet rate [Mpps]CLS\nIMEM\nEMEMFigure 23: N3IC -NFP data parallel\nmaximum BNN execution through-\nput (y axis) as a function of the num-\nber of used threads (x axis), and for\ndifferent memories. Y axis in log\nscale .\n120 240 480\nNumber of threads050100150200250300350NN execution time [us]CLS\nIMEM\nEMEM\nFigure 24: Micro-benchmark of the\nN3IC -NFP data parallel BNN execu-\ntion latency (y axis) as a function of\nthe number of threads (x axis), and\nfor different memories.\n2048 4096 8192 16384\nLayer size01234567Latency [ms]\n3.03.54.04.5\nN3IC (256) vs bnn-exec ratio\nN3IC 64\nbnn-execN3IC 128\nratioN3IC 256Figure 25: bnn-exec andN3IC -\nNFP model parallel processing la-\ntency (left y axis) for an FC layer,\nwhen varying FC’s size (x axis)\nand number of N3IC -NFP threads.\nRight y axis shows the ratio between\nN3IC -NFP with 256 threads and\nbnn-exec .\n2048 4096 8192 16384\nLayer size1101001k10k100k1M10MThroughput [FC/s]\n0.000.010.020.030.040.050.06\nN3IC (256) vs bnn-exec ratio\nN3IC 64\nbnn-execN3IC 128\nratioN3IC 256Figure 26: bnn-exec andN3IC -\nNFP model parallel throughput (left\ny axis, log scale ) for an FC layer,\nwhen varying FC’s size (x axis)\nand number of N3IC -NFP threads.\nRight y axis shows the ratio between\nN3IC -NFP with 256 threads and\nbnn-exec .\nshared among all the island’s threads. We can ﬁt, at most,\nabout 32k weights in CLS13. Figure 22 shows how varying\nthe size of an FC scales linearly the N3IC -NFP maximum\nthroughput. The tested layer has 256 inputs, and we run it\nwith a different number of neurons: 32 (8.1k weights), 64\n(16.3k weights), 128 (32.7k weights).\n13This number is also affected by the number of layers and number\nof neuron per layers, since each thread allocates in CLS the variables\nrequired to hold intermediate computation results.Memory benchmarks . To understand the impact of the\nmemory selected to store NNs’ weights, we re-run the\nstress test using the IMEM and EMEM in place of the\nCLS, and measure both throughput and NN execution la-\ntency. Figure 23 shows that throughput lowers to 1.4Mpps\nin both cases. Likewise, Figure 24 shows that the NN\nexecution latency is signiﬁcantly worse. In particular, the\n95-th percentile of the latency when using the CLS is 42 µs,\ninstead, the use of IMEM and EMEM incurs a much larger\n23\n\nvariation, with a 95-th percentile inference time of 352 µs\nand 230 µs, respectively. This latency variability is due to\nthe need to share the NFP4000’s data buses among multi-\nple threads. Interestingly, although generally faster, there\nare cases in which using the IMEM is slower than using\nthe EMEM. We believe this is an artefact of the NFP’s\nmemory access arbiter.\nB.1.2 Benchmarks for big NNs (model parallel)\nWe measured the impact of using a different number of\nthreads and different FC sizes when running N3IC -NFP\nin model parallel mode. We compared the results to those\nachieved by bnn-exec when running on a single core for\nlatency measurements, and on 4 cores for throughput ones.\nWe deﬁned the maximum batch size bnn-exec can use\nsetting a maximum of 7ms for the processing latency [ 30].\nThe 7ms execution latency constraint allows bnn-exec\nto run with a batch size of 64, 32, 16 and 8 for the 2k,\n4k, 8k and 16k neurons layers, respectively. The layer has\n4096 inputs.\nLatency (Figure 25) . For layers between 2k and 16k\nneurons (8M to 67M weights), N3IC -NFP achieves a pro-\ncessing latency which is 4 times higher than bnn-exec ’s\none, varying between 400 µsand 2700 µs. Considering that\nthe Haswell CPU has a clock frequency more than 4 times\nhigher than NFP’s 800MHz, i.e., each operation is effec-\ntively executed in less than a fourth of the time, this shows\nthat the NFP is slightly more efﬁcient than the Haswell in\nperforming the FC layer operations.\nThroughput (Figure 26) .N3IC -NFP, though unable to\nperform batching, and using only a subset of the NFP re-\nsources, can still provide 4-5% of the bnn-exec through-\nput running on a much more powerful Intel CPU. Here,\ntake into account that the NFP provides only 3MB of\nSRAM that have to be shared with with packet processing\nfunction, while the CPU’s over 10MB of L3 cache are ded-\nicated to bnn-exec processing. Furthermore, unlike the\nCPU (4 cores) that is dedicated to the processing of NNs,\nN3IC -NFP performs such processing while forwarding\n18.1Mpps.\n32 64 128\nLayer size02468101214Throughput [Million NN exec/s]1 NN exec\n2 NN exec\n4 NN execFigure 27: N3IC -FPGA throughput when processing dif-\nferent NN sizes and using multiple NN Executor modules.\nThe NN has a single layer with 256b of input.\n32 64 128\nLayer size0.00.10.20.30.40.50.60.7Latency [ns]1 NN exec\n2 NN exec\n4 NN exec\nFigure 28: N3IC -FPGA latency when processing different\nNN sizes and using multiple NN Executor modules. The\nNN has a single layer with 256b of input.\n1612182430\nNN executors0.00.20.40.60.8Throughput [Million NN exec/s]1e8\nFigure 29: N3IC -\nFPGA TPUT scal-\ning\n1612182430\nNN executors0K20K40K60K80KLUTFigure 30: N3IC -\nFPGA LUT scal-\ning\n1612182430\nNN executors0100200300400500BRAMFigure 31: N3IC -\nFPGA BRAM\nscaling\n24\n\nB.2 NetFPGA\nWe designed the NN Executor module in HDL. This allows\nus to provide predictable performance, i.e., throughput and\nlatency, when performing NN inference.\nNN Size (Figure 27 and Figure 28) . Figure 27 shows\nhow varying the size of an FC scales linearly the N3IC -\nFPGA maximum throughput. The tested layer has 256\ninputs, and we run it with a different number of neurons:\n32 (8.1k weights), 64 (16.3k weights), 128 (32.7k weights).\nIncreasing the number of NN Executor modules linearly\nscales the throughput. Since each module is dedicated\nto the execution of a NN, adding more modules does not\nimpact the latency of execution, which is only affected by\nthe size of the processed network (cf. Figure 28).\nResources scaling (Figure 29, Figure 30 and Figure 31) .\nN3IC -FPGA performance can be increased by deploying\nmultiple NN Executor in parallel. Since the logic to man-\nage multiple modules is negligible, also the required FPGA\nresources scale linearly. Figure 29 shows the maximum\nthroughput performance when running the anomaly detec-\ntion NN (cf.x5), during the stress test explained in x6.\nEach NN Executor module increases by about 1.8M infer-\nences per second the obtained performance. Figure 30 and\nFigure 31 show that the LUTs and BRAMs resources also\nscale linearly with the number of NN Executors. Here, it is\nworth noticing that the use of BRAMs can be considerably\noptimized. In the current setting, each NN Executor has a\ndedicated CAM element to store the NN weights. However,\nweights are read-only, thus, sharing a CAM module across\nmultiple NN Executors can be achieved with relatively\nlittle effort. We did not provide such optimizations, since a\nsingle NN Executor could already achieve the performance\ngoals we set for N3IC -FPGA.\nC Use cases details\nWe present three use cases to demonstrate the versatility\nofN3IC . The results are summarized in Table 5.\nC.1 Classiﬁcation and Anomaly Detection\nWe consider two typical networking applications: trafﬁc\nclassiﬁcation and anomaly detection. The former aims attraining a NN for application identiﬁcation and requires the\nsystem to be able to extract per packet features. The latter\nfocuses on discovering suspicious data ﬂows by analyzing\nﬂow level features in the network trafﬁc.\nDatasets . For the trafﬁc classiﬁcation use case, we used\nUPC-AAU dataset [ 7] of more than 750K ﬂows, including\nper packet trafﬁc traces from 76 commonly used applica-\ntions and services, which account for 55GB of packet-level\ndata14. For the anomaly detection use case we used the\nthe UNSW-NB15 dataset [ 52], which provides over 170K\nnetwork ﬂows labeled in two main categories, i.e., good,\nbad, and their ﬂow-level statistics15. In our experiments,\nwe used only the 16 most important features by computing\nthe chi-squared value between each feature and the class\nlabel [ 17]. Furthermore, to train a binary classiﬁer, we\nonly use features that can be computed in the NIC hard-\nware. Hence, we ignored features related to the packets’\ncontent, which we assumed encrypted. For the UPC-AAU\ndataset, we trained a binary classiﬁer to detect encrypted\nBitTorrent ﬂows.\nMLP Classiﬁer . For the trafﬁc classiﬁcation use case,\nwe ﬁrst trained a regular MLP to classify the trafﬁc in\n10 classes provided by the UPC-AAU dataset16(see Ta-\nble 4), achieving 69.4% accuracy. When training a similar\nbinarized MLP model, we achieve 59.1% accuracy. Fig-\nure 32 shows the classiﬁcation confusion matrix for the\nbinarized MLP. As we can see, there are some classes, such\nas code 1,2,6, and 8 in Table 4, which cannot be easily\ndistinguished from other classes. Furthermore, we could\nachieve this classiﬁcation accuracy only using a larger bi-\nnarized MLP with 256 neurons in each of the two hidden\nlayers. Using the binarized MLP presented in x5 only\nachieves about 14% accuracy. To address the issue, we\ntransformed the classiﬁcation problem in a binary clas-\nsiﬁcation. That is, we trained the MLP to classify only\nbetween BitTorrent trafﬁc and non-BitTorrent trafﬁc. In\nsuch a setting we achieved 96.2% accuracy for the regular\nMLP and 88.6% for a binarized MLP with only 32, 16, 2\n14https://cba.upc.edu/monitoring/trafﬁc-classiﬁcation\n15https://www.unsw.adfa.edu.au/unsw-canberra-\ncyber/cybersecurity/ADFA-NB15-Datasets/\n16Although in UPC-AAU dataset [ 7] there 76 different class, there\nare not enough data samples for each class to train a reliable multiclass\nclassiﬁer on it. For this reason, we choose the 10 biggest classes in terms\nof number of available samples.\n25\n\nCode Name Size\n0 bittorrent-all-encrypted 25000\n1 bittorrent-outgoing-non-encrypted 25000\n2 emule-outgoing-non-obfuscated 10960\n3 pandomediabooster 12070\n4 rdp 25000\n5 web-browser 24914\n6 dns 10761\n7 new-samba-session-service 25000\n8 ntp 17834\n9 ssh 25000\nTable 4: Trafﬁc classes of UPC-AAU with at least 10000\nsamples.\nneurons. This experience shows an interesting application\nofN3IC , which can be used as a pre-ﬁlter to classify trafﬁc\nat a coarse grain. By doing so, it is often possible to shred\na signiﬁcant amount of network trafﬁc that does not need\nfurther analysis, and dedicate a reduced amount of host\nsystem’s resources to the analysis of network trafﬁc. That\nis, the host system would need to analyze only the subset\nof trafﬁc that could not be classiﬁed by the NIC.\nFigure 32: Confusion matrix for multiclass classiﬁcation\non UPC-AAU dataset. Numbers show the accuracy (%).\nRows and columns shows the class code in Table 4.\nWe conﬁgured an MLP with 3 FCs of 32, 16, 1 neurons\nfor the anomaly detection use case as well. For both use\ncases, since each selected feature’s numeric value falls inTable 5: Memory requirements and accuracies of the NNs\nused by the presented use cases.\nDATA NN SIZEMEMORY ACCURACY\nMLP BIN MLP BIN\nUNSW 32,16,2 35KB 1.1KB 90.3% 85.3%\nUPC 32,16,2 35KB 1.1KB 96.2% 88.6%\nNS3 32,16,2 21.6KB 676B 92.0% 90.0%\nNS3 64,32,2 47.2KB 1.5KB 94.0% 90.0%\nNS3 128,64,2 110.8KB 3.4KB 94.0% 92.0%\nthe range [0, 65k], we represented them using 16b for each,\nand provide each bit as separated input to the MLP. With\nthis conﬁguration, the MLP has a total of 8.7k weights,\nand a memory requirement of 35KB, assuming 4B to rep-\nresent each weight. We then performed training using a\ntypical state-of-the-art approach, which employs a back-\npropagation algorithm with Adam optimizer, Dropout on\nthe output of each hidden layer with probability 0.25, and\nbinary cross-entropy as loss function. The trained classiﬁer\nachieves 90.3% accuracy.\nBinarized MLP . We then designed a binarized MLP to\nprocess the same set of features. The binarized MLP has\n32, 16, 2 neurons and a total of 8.7k weights. Being binary\nweights, the overall required memory is only 1KB. Bina-\nrization mainly consists in applying a training technique\nthat ensures that the weights converge to values included\nin the range [-1, 1] and normally distributed around the\n0. In particular, we apply the binarization technique from\nCourbariaux and Bengio [ 10], and again train the network\nusing back-propagation, Dropout with probability 0.25,\nAdam optimizer and a squared hinge loss function. The\ntrained MLP’s weights obtained after training are still real\nnumbers, thus, an additional ﬁnal step is required to obtain\nthe binarized weights. This step is a simple application of\na step function to the weights, which are set to 0 if their\nvalue is lower than 0, or to 1 otherwise. After this, the\nbinarized MLP achieves a 85.3% and 88.6% accuracy on\nthe test set for UNSW-NB15 and UPC-AAU, respectively.\nWe observe that the BNN provides only 5 and 8 percentage\npoints lower accuracy than the non-binarized version.\n26\n\nFigure 33: ns3 topology for\nthe network tomography use\ncase.\n32x16x2 64x32x2 128x64x2\nNN size7580859095100Accuracy [%]\nNN\nBNNFigure 34: Box plot of the\naccuracies for the predicted\nqueues in the network to-\nmography use case.\nC.2 Network Tomography\nIn this use case, we run SIMON [ 18], a network tomogra-\nphy application which infers congestion points and queue\nsizes in a datacenter network starting from host-to-host\nnetwork paths’ delays. All the end-hosts periodically ex-\nchange probe packets among them and send the measured\ndelays to a centralized node which is then responsible for\nrunning the reconstruction engine and whose algorithm\nhas been approximated using a Neural Network. We im-\nplemented a modiﬁed version of SIMON where we are\ninterested in quickly detecting the congestion point without\nestimating the exact size of the queues. Rather than run-\nning a single NN in the centralized node, a set of NICs is\nin charge of computing the congestion status of the queues.\nDataset . We simulated a small CLOS-like Fat Tree dat-\nacenter network in ns3 [ 74]. The network, reported in\nFig. 33, includes 10 switches and 32 hosts organized in\ntwo pods (4 ToR switches, 4 aggregation switches and 2\ncore switches). The datacenter operates under an incast\ntrafﬁc load as described in [ 18]. In addition to the trafﬁc,\nall the servers (except the ﬁrst one) periodically send a\nprobe packet (once every 10ms) towards the ﬁrst server\nto measure the network paths delays.17From any server\nthere are up to 8 distinct paths towards the ﬁrst server,\ntraversing in total 17 distinct output queues (reported as\ngreen dots in Fig. 33). Our task is to train multiple NNs,\n17For the sake of simplicity we focus on a scenario where SIMON\nis just run on a single NIC, speciﬁcally in the ﬁrst server, but in the\ncomplete scenario every server will probe all the other servers so that\nmultiple NICs, connected under different racks, cover the whole set of\nqueues in the network.one for each queue, each one in charge of detecting the\nqueue congestion status.\nWe selected a subset of 19 out of 31 probes in order\nto keep 1 probe per distinct path. Our dataset consists of\n30k samples, one per each 10ms interval, with 19 features\n(path delays, in ms) and 17 corresponding outputs (queue\nsizes, in packets). We considered 17 independent binary\nclassiﬁcation problems where the output class is 1 if in a\ngiven 10ms interval the corresponding queue is above a\nconﬁgurable threshold, 0 otherwise.\nMLP Classiﬁer . We ﬁrst trained a regular MLP, with\nthe same hyperparameters used in the previous use cases,\nwith three different architectures (32x16x1, 64x32x1 and\n128x64x1). The 19 inputs of the MLP are represented\nusing 8 bits and, as in the previous use cases, we provide\neach bit as separated input to the MLP. Table 5 reports the\nmemory requirements for the different NN architectures.\nThe resulting median accuracies range from 92% to 94%\nfor an increasing NN size.\nBinarized MLP . We then designed a binarized MLP to\nprocess the same set of features with three different ar-\nchitectures (32x16x2, 64x32x2 and 128x64x2), all with\n19 inputs and two output neurons. The drop in the me-\ndian accuracy when moving from a full precision NN to\na binarized NN ranges from 2% to 4%. Fig. 34 reports\nmore in detail the distribution of the median accuracies for\ndifferent NN sizes.\n27",
  "textLength": 103918
}