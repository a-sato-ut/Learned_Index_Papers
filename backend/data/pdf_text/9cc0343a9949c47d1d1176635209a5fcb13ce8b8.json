{
  "paperId": "9cc0343a9949c47d1d1176635209a5fcb13ce8b8",
  "title": "Progressive Neural Index Search for Database System",
  "pdfPath": "9cc0343a9949c47d1d1176635209a5fcb13ce8b8.pdf",
  "text": "Progressive Neural Index Search for Database System\nSai Wu\nwusai@zju.edu.cn\nZhejiang UniversityXinyi Yu\nyuxinyi@zju.edu.cn\nZhejiang UniversityXiaojie Feng\nxiaojie.fxj@alibaba-inc.com\nAlibaba Cloud\nFeifei Li\nlifeifei@alibaba-inc.com\nAlibaba CloudWei Cao\nmingsong.cw@alibaba-inc.com\nAlibaba CloudGang Chen\ncg@zju.edu.cn\nZhejiang University\nABSTRACT\nAs a key ingredient of the DBMS, index plays an important\nrole in the query optimization and processing. However, it\nis a non-trivial task to apply existing indexes or design new\nindexes for new applications, where both data distribution\nand query distribution are unknown. To address the issue,\nwe propose a new indexing approach, NIS (Neural Index\nSearch), which searches for the optimal index parameters\nand structures using a neural network. In particular, NIS\nis capable for building a tree-like index automatically for\nan arbitrary column that can be sorted/partitioned using a\ncustomized function. The contributions of NIS are twofold.\nFirst, NIS constructs a tree-like index in a layer-by-layer way\nvia formalizing the index structure as abstract ordered and\nunordered blocks. Ordered blocks are implemented usingB\n+-tree nodes or skip lists, while unordered blocks adopt\nhash functions with diï¿¿erent conï¿¿gurations. Second, all pa-\nrameters of the building blocks (e.g., fanout of B+-tree node,\nbucket number of hash function and etc.) are tuned by NIS\nautomatically. We achieve the two goals for a given workload\nand dataset with one RNN-powered reinforcement learning\nmodel. Experiments show that the auto-tuned index built by\nNIS can achieve a better performance than the state-of-the-\nart index.\n1 INTRODUCTION\nMillions of users deploy various applications on the Alibaba\nCloud and employ our database PolarDB[ 2] as their data\nmanagement system. One of our crucial tasks is to optimize\nusersâ€™ data access with a limited cloud resource, where we\nï¿¿nd that indexes play an important role. However, it is very\nchallenging to ï¿¿ne-tune the index performance, currently\nrequiring several weeks of eï¿¿orts from our experienced DBA.\nThe main causes are data diversity and variety of user ac-\ncess patterns. E.g., Taobao, an online shopping platform,\nshows very diï¿¿erent data distributions and access patterns\nfrom DingTalk, a mobile collaboration tool. The same index\nconï¿¿guration from Taobao does not necessarily provide asatisï¿¿ed performance for DingTalk. In the ideal case, ourDBA should provide a customized index conï¿¿guration forevery application, which is not possible for a Cloud service\nprovider.\nFortunately, we observe that an application normally has\naï¿¿xed access pattern (most queries follow some pre-deï¿¿ned\ntemplates and only a few ad-hoc queries) and its data also\nshow a stable distribution. In practice, our DBAs start with anindex conï¿¿guration based on their experiences and gradually\nimprove it based on the access pattern and data distribution.\nBut they still face two challenges. First, given so many exist-\ning index structures, it is unknown which one performs best.\nA safe guess is B+-Tree for columns requiring range search\nand hash index for columns requiring fast lookup. But in\nmost cases, they are not the optimal solution. Sometimes, no\nexisting index structure is capable of handling the unique\naccess pattern eï¿¿ciently for a speciï¿¿c application. We may\nhave to design a new one. Second, even we limit our scope\nto popular indexes like B+-Tree, skip list and hash. There\nare many tunable parameters, such as node size and fanout\nof B+-Tree, the growing-up probability of skip list and the\nbucket number of hash function. To ï¿¿nd a proper conï¿¿gu-\nration for those parameters, we need to run a series of A/B\ntests, lasting for a few days.\nIn this paper, we propose NIS, a Neural Index Search\napproach, to automatically assemble an index for a given\ndataset and query workload, which can free our DBAs from\nthe heavy index building and tuning work. The only assump-\ntion of NIS is that users can provide a function to sort or\npartition the data, which is valid for most applications. NIS\nformalizes various index structures into two abstract index\nbuilding blocks, ordered block and unordered block. Ordered\nblock, where keys are sorted in ascending order, can be im-\nplemented as B+-Tree node or skip list. Unordered block,\nwhere keys are partitioned using customized functions, can\nbe implemented as hash bucket. Both abstract blocks follow\nthe format of [:4~ ,E0;D4 ]+, where:4~denotes the indexed\nkey, andE0;D4 refers to the pointer to the next index block\nor the memory/disk address for the data values.\nTo address the two challenges (index selection/construction\nand index tuning), we apply the policy gradient[ 14] strategy\nto train a reinforcement learning model using RNN (Recur-\nrent Neural Network) as backbones, which can\n\nâ€¢Construct a tree-like index in a layer-by-layer way,\nwhere each layer is a sequence of abstract index blocks\npartitioning the search space with a pre-deï¿¿ned func-\ntion.\nâ€¢Search the optimal conï¿¿guration for each index block,\nincluding block type, block size, minimal and maximal\nnumber of keys in a block and etc.\nTheï¿¿rst step predicts the general structure, while the second\nstep materializes the index. In our implementations, the two\nsteps are interleaved by stacking RNN together. Predictions\nof previous layers are used as context input to the RNN for\nthe next layer, which decides whether to create a new layer\nor not and if a new layer is being constructed, predicts all\nthe tunable parameters and types for each index block (or-\ndered/unordered). In theory, NIS can produce many diï¿¿erent\ntree-based, list-based and hash-based indexes via diï¿¿erent\nconï¿¿gurations (shown in Section 2).\nIn this paper, we focus on the in-memory version of NIS.\nCompared to the disk version, in-memory NIS is more chal-\nlenging because 1) the in-memory index can have multiple\nlayers, while the disk-based one is limited to 2-3 layers; 2)\nsince fragmentation does not apply to memory-optimized\nindexes, each index block can have a customized size; and\n3) in-memory index is more sensitive to the access patterns.\nIn our experiments, NIS outperforms many existing state-\nof-the-art in-memory indexes on various workloads. More-\nover, we also provide an incremental learning mechanism for\nNIS. So it can handle the case where query/data distribution\nchanges gradually over time. Our experiments show that if\nnewly inserted data follow the same distribution, indexes\ngenerated by NIS can provide a good performance without\nany adjustment. If only a few portion (e.g., 10%) of new data\nshow a diï¿¿erent distribution, NIS employs an economic in-\ncremental learning model to adjust the index conï¿¿gurations.\nExperiments show that it still provides a better performance\nthan others.\nThe idea of NIS is analogy to the NAS(Neural Architecture\nSearch)[ 27]. In NAS, the hyper parameters of a neural model\nare tuned by another neural network. Therefore, we do not\nneed to design a new neural model for a particular imageprocessing task (image classiï¿¿cation, image segmentation\nand object recognition) on new datasets. The parameters of\nCNN (convolutational neural network), e.g., kernel size, the\nstride size and channel number, and how the CNNs, pooling\nlayers and normalization layers are stacked together are\nsearched and learned automatically[ 12][23][11]. The auto-\ngenerated models show comparable performance to the ï¿¿ne-\ntuned models by human experts.\nThe closest work to ours is the Learned Index Structures\nproposed by Google[ 7]. It tries to learn an ordered neural\nmapping function for each key and stack those functions as atree index. One challenge is to improve the prediction perfor-\nmance of neural models (from milliseconds to nanoseconds),\ninvoking many engineering eï¿¿orts. NIS adopts a diï¿¿erentstrategy by searching for the solution of how to combine\nexisting index blocks and tune their parameters for speciï¿¿c\napplications. NIS does not suï¿¿er from the slow prediction of\nneural models, since once the index has been materialized,\nit can work independently.\nIn summary, we make the following contributions in NIS.\nâ€¢We propose a neural index search framework, which\napplies a reinforcement learning model to search and\ntune new indexes for a given dataset and query work-\nload automatically.\nâ€¢We propose a conditional RNN model to generatemulti-layer tree-like indexes, where each layer is an\nordered list of index blocks and the construction of a\nlayer depends on all existing layers.\nâ€¢An incremental learning approach is adopted to sup-\nport gradually updated query patterns.\nâ€¢Experiments with state-of-the-art indexes show that\nthe NIS generated index can achieve better perfor-\nmances.\nThe remaining of the paper is organized as follows. In Sec-\ntion 2, we formulate our problem and give an overview of\nour framework. In Section 3, we introduce our conditional\nRNN model and training process. We discuss our index con-\nstruction and implementation details in Section 4. The NIS\nis evaluated in Section 5 and we brieï¿¿y review some related\nwork in Section 6. The paper is concluded in Section 7.\n2 BACKGROUND AND OVERVIEW\n2.1 Problem Formulations\nMillions of applications are deployed on the Alibaba Cloud\nand many of them adopt the PolarDB as their data man-\nagement systems. Once we make an agreement with our\ncustomers for the performance of their applications (e.g., av-\nerage response time less than 10ms and throughput greater\nthan 100,000/s), it is our job to tune and optimize their data-\nbase instances on PolarDB.\nIndex tuning is one of the most important and challenging\ntasks that we ever met, which including two steps: identi-fying candidate columns for indexing and constructing a\nproper index for each candidate. Given a workload ,con-\nsisting of a set of frequently used queries, the ï¿¿rst step picks\nthe columns that are used as predicates for indexing. We\nhave another work discussing the technical details. In this\npaper, we focus on the second step, formalized as:\nProblem Deï¿¿nition : Given a column â‡ and workload\n,, assume that â‡ can be sorted by a function 5. How can we\ngenerate a proper tree-like hierarchical index structure Ifor\nâ‡ , which is tuned to minimize the total processing latency\n2\n\nThe Controller\n(Stacked RNN )Build index with architecture A \nunder a certain workload W to \nget Reward such as latency & \nspace utilizationSample architecture A \nwith probability p\nCompute Gradient of p and scale it \nby Reward to update ControllerFigure 1: General Idea of NIS\nof,with a storage budget âŒ«, computed as:\n2C=â€™\n8@82,5(@8)â‡¥F(@8)\n5(@8)returns the latency of processing @8with the existence\nofI.F(@8)represents the weight of @8, which is set as the\nfrequency of @8in,currently. The total storage cost of the\nindex should be less than âŒ«.\nWe do not intent to invent new index structures. Instead,\nour plan is to reuse existing index structures and ï¿¿ne-tune\nthem for our target workloads. Therefore, we introduce two\nabstract index blocks that can be materialized as popular\nexisting index structures.\nDeï¿¿nition 2.1. The ordered index block is described as\n\u0000>={(,\u0000,[!,*),?C}, where(denotes a sorted list of keys,\n\u0000is the mapping function, [!,*)denotes the key range of\nthe block and ?Cpoints to the next sibling block. For \u0000>,w e\nhave the properties:\nâ€¢8([8]2(,!ï£¿([8]<*.\nâ€¢([8]ï£¿([9], if8<9.\nâ€¢\u0000(([8])ï£¿\u0000(([9])if8<9\nIn the ordered index block, the mapping function \u0000is main-\ntained as a list of sorted key-value pairs [(:8,E8)â‡¤], where\n:8<:9for8<9andE8refers to the position of the index\nblock in the next layer or the real data values.\nDeï¿¿nition 2.2. The unordered index block is denoted as\n\u0000D={(,\u0000,[!,*),?C}, where(is a set of keys, \u0000is a hash\nfunction(currently, we use the standard SHA-1 hash func-\ntion), [!,*)denotes the key range of the block and ?Cpoints\nto the next sibling block. For \u0000D, we have 8([8]2(,!ï£¿\n([8]<*.\nEach index block can hold up to <keys.<can be cal-\nculated by the key size and the cache line size(for memory\nindex) or block size(for disk index). However, the initial num-\nber of keys inside each block(denoted as G) is a tunable pa-\nrameter, which is learned by the NIS through training. Table\n1 lists the hyper-parameters learned by the NIS. In unorderedindex block, the whole block is maintained as a hash table,\nwhere we have maximal <buckets and the buckets maintain\npointers to index blocks in next layer.\nTable 1: Tunable Hyper-Parameters\nBlock Type ordered/unordered\nG the initial number of keys in an index block\n~ the number of blocks in a group\nU the block will split when more than U<keys\nV two blocks will merge when both less than V<keys\nW a probability vector for creating skip links\n2.2 Overview of NIS\nThe design of NIS follows the same philosophy of the NAS.\nFigure 1 shows the general architecture. Given a database\nâ‡¡and query workload ,, NIS employs a controller to tune\nthe hyper-parameters listed in Table 1 and decides how the\nabstract index blocks can be assembled as an index. In this pa-\nper, the controller is a reinforcement-learning model, which\napplies the policy gradient[ 14] to update status and is imple-\nmented as stacked RNN. After the controller makes a predic-\ntion, an index builder materializes the corresponding index\nand deploys it on the database. We test the query workload\n,using the index to get the latency and space utilization as\nour rewards, which are used as feedbacks for the controller\nto update its predictions. The process continues, until the\nlatency and space utilization converge.\nOne challenging of applying NIS to predict the index struc-\nture is the scalability. Suppose we have 10 million keys and\neach block can hold up to 1000 keys. We need at least 10,000\nblocks to maintain those keys. In other words, the NIS needsto generate hyper-parameters for a large number of blocks se-\nquentially. However, existing neural models are not capable\nof predicting such a long sequence. To reduce the prediction\ncost of NIS, we classify the blocks into groups.\nDeï¿¿nition 2.3. An index block group âŒ§is a set of index\nblocks responsible for consecutive key ranges and sharing\nthe same hyper-parameters.\nGiven a set of keys in [<8= ,<0G ), to generate a new index,\nthe controller ï¿¿rst creates a layer with one index block. The\ntype of the block and the initial size of its keys are all decidedby the controller. Suppose the block partitions the key ranges\ninto%0=[:0,:1),%1=[:1.:2),...,%=\u00001=[:=\u00002,:=\u00001)(namely,\nGis set as=by the controller). By default, we partition the key\nrange evenly. The controller starts building the second layer\nof index for each range by adaptively generating an index\nblock group. The hyper-parameters of a group is learned by\nthe controller. In other words, ~index blocks are created for\neach group and the corresponding key range is partitioned\n3\n\nData BlocksUnordered Index\nOrdered IndexzFigure 2: An Example Index Generated by NIS\ninto~non-overlapped consecutive ranges, one for each block.\nInside each group, to facilitate the query, each index block\ncan create skip links up to log~blocks inside the same group.\nIn particular, suppose current group âŒ§has~index blocks:\n{\u00000,. . . ,\u0000~\u00001}. For block\u00000, it will create a skip link to \u00001with\na probability ?8, if1=0+28(0<8ï£¿log~)and1<~. The\nprobability ?Bis estimated by the controller, and we have\nW={?1,?2,. . . ,?log~}.\nThe above index construction process continues for each\nindex block, until no key range has more than V<keys. In\nthis way, we may generate an imbalanced index search tree\nwith skip links inside each block group.\nThe controller outputs its ï¿¿nal decision as a sequence\nof operations, which are read by the index builder for con-\nstruction. The index builder adopts a streaming approach\nto materialize the index. In particular, it ï¿¿rst creates an ab-\nstract index by stacking the index blocks predicted by thecontroller. Then, it reads in the data and applies the data\nstream to materialize the index. If block 18is split into10and\n11, both10and11will share the same hyper-parameters of\n18. However, if the controller generates a good prediction,\nwe do not need to split data blocks frequently. To speed up\nthe index construction, the index builder employs multiple\nthreads to assemble the index. Finally, the index is deployed\non the database and tested against the given workload.\nAs an example, Figure 2 shows an index generated by\nNIS, which performs better than an ordinary B+-tree for\nqueries following Zipï¿¿an distribution. The blue and yellow\nnodes represent the ordered and unordered index blocks,respectively. The dashed line denotes the skip links insidea group. To process a query, we start from the root blockas searching a B+-tree. When reaching a group, we pick the\nskip links to simulate the search process as the skip-list.\nBy learning diï¿¿erent hyper-parameters, NIS can simulate\ndiï¿¿erent types of conventional indexes, such as:\nB+-tree All the index blocks are ordered blocks with the\nsame conï¿¿guration of <andUandVare set to 1 and\n0.5 respectively. ~is set as 1 for all groups.\nHash All the index blocks are unordered blocks and the\nindex only has one layer.\nSkip-list Each layer only has one index group and the\nupper layer group has fewer blocks (a smaller ~).\nIt can be seen that the large search space of NIS allows us to\nexplore more new index structures by combining diï¿¿erent\nexisting index structures for speciï¿¿c workloads and datasets.\n3 IMPLEMENTATION OF CONTROLLER\nIn this section, we show how the controller learns to predict\nthe hyper-parameters for the index. We ï¿¿rst discuss the\narchitecture of our neural model and then elaborate on how\nthe training process works.\n3.1 The Architecture of Controller\nWe consider the prediction of hyper-parameters as a task\nof sequence prediction. Therefore, the backbone of our con-\ntroller is an RNN network powered by the LSTM[ 1]. Fig-\nure 3 illustrates the basic architecture of the controller. The\ncontroller consists of multiple layers of LSTMs to predict a\ntree-like index. The new layer will use the hidden states of\nprevious layers as the context during its prediction.\nOur model consists of a basic building block as shown in\nFigure 3. The neural block predicts the six hyper-parameters\n(Block Type, G,~,U,VandW) for an index block group as a\nsequence via the RNN model. The whole block consists of\nthree neural layers, an embedding layer, a LSTM layer and a\nsoftmax layer.\nThe bottom layer is an embedding layer, formalizing the\ninput as a binary vector representation. For the ï¿¿rst state\nof the RNN, the min/max values of the keys, the number of\nunique keys and a coarse histogram are transformed into\nbinary vectors and concatenated together as the input. For\nthe following states, the generated vector from the softmax\nlayer for previous state is used as the input.\nThe middle layer applies the LSTM to learn the correla-\ntions between diï¿¿erent states. Selections of previous hyper-\nparameters aï¿¿ect the choices for the following ones. In fact,\nwe also tested the biLSTM(bi-directional LSTM), but did not\nï¿¿nd a signiï¿¿cant improvement. So we stick to the basic LSTM\nmodel.\nThe top layer is a softmax layer for prediction. We trans-\nform our task into a classiï¿¿cation problem by creating a set\nof pre-deï¿¿ned values for each hyper-parameter and only\n4\n\nType ð‘¥ ð‘¦ ð›¼ ð›½ Type\nOPParent Node\nHidden Output1st\nChild2nd\nChild\nType\nTypeOPParent Node\nHidden Output1st\nChild\nð›¾ \n ð‘¥ ð‘¦ ð›¼ ð›½ ð›¾ \n \nð‘¥ ð‘¦ ð›¼ ð›½ ð›¾ \n ð‘¥ ð‘¦ ð›¼ ð›½ ð›¾ \n \nSelect \nIndex Block \nTypeSelect initial \nnumber of \nkeys [    ]Softmax \nlayer\nController \nhidden layerSelect number \nof inside \nblocks [    ]Select ratio \n[    ]\nto split    Select ratio \n[    ]\nto merge    Select skip \nlinks [    ]\nEmbedding\nlayerð‘¥ ð‘¦ ð›¼ ð›½ ð›¾ \n Figure 3: The Neural Model of NIS\nTable 2: Pre-deï¿¿ned Values\nHyper-Parameters Values\nG<\n4,<\n2,3<\n4,<\n~ 32, 64, 128, 256\nU 0.5, 0.6, 0,7, 0.8, 0.9, 1\nV 0.1, 0.2, 0.3, 0.4\nallowing the neural model to pick one of the pre-deï¿¿ned\nvalues. Table 2 lists our current pre-deï¿¿ned values. The pa-\nrameterWhas no default values, since it depends on the\nvalue of~. Speciï¿¿cally, we will generate a probability vector\n{?1,?2,. . . ,?log~}indicating whether to create skip links to\nthe neighboring log~blocks in the same group. The 8th skip\nlink will be established with a probability ?8.\nThe basic neural blocks are chained together to predict\nthe hyper-parameters for index block groups. After success-\nfully generating the prediction for one layer of index, the\ncontroller can start up a new layer, if some index blocks need\nto be further partitioned. Then, the hidden states of current\nlayer are used as the context during the prediction for the\nnext layer.\nFigure 3 illustrates the idea. The outputs from the softmax\nlayer of a neural block are concatenated together and used\nas input to the neural blocks in the next layer. The estimated\nnumber of children of a neural block is G, and hence, its hid-\nden states will be used as contexts for Gconsecutive neuralblocks in the following layer. In this way, we can progres-\nsively generate the hyper-parameters for a multi-level index,\nwhich, in fact, can simulate most existing index structures.\n3.2 Training the Controller\nThe hyper-parameters generated by the controller can be\nconsidered as a series of operators 01:C(operators from the\nstart to time C) which are used to construct a new index\nfor a given workload ,and database â‡¡. At convergence,\nthe new index is integrated into â‡¡and we test it against\nthe workload ,. The total processing time 2Cand the index\nspace utilization cost 2B(the percentage of the index that has\nbeen used for maintaining keys) are our main concerns. So\nthe reward is deï¿¿ned as:\n'=d21\u00002C\n21+(1\u0000d)2B\n21is the baseline processing time without any index and d\nis a tunable parameter to balance the importance of the two\nterms. We have conducted experiments to show the eï¿¿ect of\nd.\nToï¿¿nd the optimal index, we ask the controller to maxi-\nmize its expected reward, represented as \u0000(\\2).W eu s e\\2to\ndenote all parameters of the controller model. We have:\n\u0000(\\2)=â‡¢1:);\\2[']\nThe reward 'is achieved by monitoring the performance\nof database â‡¡and hence, is not diï¿¿erentiable. We apply the\n5\n\npolicy gradient strategy as in [14]:\nr\u0000(\\2)=â‡¢%(01:);\\2))â€™\nC=1('\u00001)rlog%(0C|0C\u00001) (1)\n)is the total number of hyper-parameters for predicting\nand1is a baseline function to reduce the variance of the re-\nward. Currently, let `be the aging factor. 1(=)in=th training\nsample is recursively deï¿¿ned as\n1(=)=â‡¢0if==0\n`1(=\u00001)+(1\u0000`)'(=)=\u00001\nAs a result,1(=)can be estimated as:\n1(=)=(1\u0000`)('(=)+`'(=\u00001)+`2'(=\u00002)+...+`C\u00001'(1))\nEquation 1 shows how the parameters \\2of the controller\nnetwork are updated based on the reward 'gradually, which\nis represented as\n\\2:=\\2+fr\u0000(\\2)\nwherefis the learning rate.\nIn practice, we use the exhaustive weighted summation\nform to replace the expected value in Equation 1. If we have\nenough training samples, we can estimate the r\u0000(\\2)as (#\nis the batch size of the controller):\nr\u0000(\\2)=1\n##â€™\n==1)â€™\nC=1('(=)\u00001(=))rlog%(0(=)\nC|0(=)\nC\u00001)(2)\nIn our experiments, we ï¿¿nd that a small #is good enough\nfor the model to converge to a satisï¿¿ed result.\nThe intuition of policy gradient is to increase the probabil-\nity of%(0C|0C\u00001), if'\u00001is positive. Otherwise, we decrease\nthe probability. However, during the training process, we ï¿¿nd\nthat if%(0C|0C\u00001)is large enough, '\u00001will be always positive\n(because the model gives up on exploring new results and\nsticks to current sub-optimal one), causing %(0C|0C\u00001)to con-\nverge to 1. On the contrary, if %(0C|0C\u00001)is very small, '\u00001\nwill be negative in most estimations, and hence, %(0C|0C\u00001)\nwill converge to 0. In both cases, we obtain a local optimal\nresults. To avoid such problems, we clip the sample data\nand only update the probabilities within [n,1\u0000n], where\n0<n<<1.\nSimilar to other policy gradient approaches, the training\nprocess lasts for days, since we need to build each predicted\nindex and performs benchmarking to gather corresponding\nrewards. To speed up the training process, we apply two\noptimization techniques.\nWe generate a set of probabilities after the softmax layer of\nthe controller. They are used to select the hyper-parameters.\nFor example, we obtain the probabilities [?1,?2,?3,?4]for pa-\nrameter~, indicating that we may set ~as 32, 64, 128, 256 with\nprobabilities ?1,?2,?3and?4, respectively. In value-based\nlearning approach, it was shown that random explorationcan speed up the convergence. We adopt this approach for\nour policy-based approach. In particular, we ask the con-troller to ignore the generated probabilities and randomly\npick a value for a hyper-parameter with a pre-deï¿¿ned prob-\nability_. Initially,_=1to allow a fast random exploration\nand gradually, we decrease _to 0.\nThe training of vanilla policy gradient approach is ex-\ntremely slow due to a large exploration space. A new ap-\nproach, PPO(Proximal Policy Optimization) [ 17], can be used\nto facilitate the parameter updates. In policy gradient, we\nupdate model parameters, only when we obtain new training\nsamples. This strategy is called â€œon-policy\" strategy. Instead,\nin PPO, we create a new controller model \\0\n2, which is em-\nployed with the environment(in our case, the index builder\nand database) to get training samples. The training samples\nobtained from \\0\n2are repeatedly used by the real model \\2,\nso that parameters of \\2get multiple updates for one sample.\nThis is called â€œoï¿¿-policy\" strategy.\nUsing PPO, Equation 1 is rewritten as:\nâ‡¢%(01:);\\02))â€™\nC=1ï£¿?\\2(0C|0C\u00001)\n?\\02(0C|0C\u00001)('\\0\n2\u00001)rlog?\\2(0C|0C\u00001)\u0000\n(3)\nHowever, if the distribution of \\0\n2and\\2diï¿¿ers a lot, the ap-\nproach may not work. So, PPO introduces the KL-divergence\nto balance the diï¿¿erence between two distributions. The\nequation is further revised as:\n\u0000\\0\n2\n%%$(\\2)=\u0000\\0\n2(\\2)\u0000q ! (\\2,\\0\n2) (4)\n=â‡¢%(01:);\\02))â€™\nC=1ï£¿?\\2(0C|0C\u00001)\n?\\02(0C|0C\u00001)('\\0\n2\u00001)\u0000\n\u0000q ! (\\2,\\0\n2)\n3.3 Incremental Updating\nAs mentioned before, most applications on PolarDB have\na stable data and query distribution. So we can learn anindex to achieve a good performance. However, data and\nquery will slowly evolve. Hence, we design an incremental\nupdating model, which is a by-product of the controller. In\nother words, we reuse training samples from controller to\nbuild the incremental updating model.\nThe intuition of incremental updating model is to learn a\nperformance prediction function âŒ§. Given a data distribution\nD, query distribution Qand speciï¿¿c index I,âŒ§(D ,Q,I)\nreturns the estimated processing latency of Q. We use equi-\nwidth histograms to maintain data distribution and query\ndistribution. We generate a vector representation for each\nindex block by encoding its conï¿¿guration parameters. Finally,\nâŒ§is learned through a tree structured LSTM model [ 18].\nFigure 4 shows the architecture of our model.\n6\n\nNeural Block\nData DistributionNeural Block\nNeural Block Neural Block\nNeural Block\nNeural Block Neural BlockQuery Distribution\nIndex RepresentationModel GReal Performance \nDistribution\nExpected Performance \nDistribution\nHidden StatesHidden StatesIndex BlockFigure 4: A Tree-LSTM for Incremental Updating\nWe train a neural block for each index block and con-\nnect them as a tree structure, where parent node accepts\nhidden states from its child nodes as context for prediction.\nAll neural blocks actually share the same hyperparameters\ntrained with samples from controller. During the training of\na controller, we obtain an index structure under a speciï¿¿c\ndata distribution Dand query distribution Q. We collect the\nstatistics of the processing latency for each index block. If a\nblock is a leaf block, its latency is its total cost of processing\nqueries. If the block is an internal block, its latency includes\nboth its own cost and the accumulative costs of all its de-\nscendants. Then, we train a performance prediction network\nto predict the cost of each index block. The network accepts\nD,Q, vector representation of the block and hidden states\nfrom child nodes as input. It outputs a hidden state which\nis further applied to generate predictions for internal nodes.\nSince we do not need a very precise estimation of latency,\nwe transform the prediction into a classiï¿¿cation problem,\nwhere we create 100 diï¿¿erent performance classes and pick\nthe class with maximal probability.\nWe consider queries in Qas a training batch and also\npropose a batch loss function. The loss function is based on\nthe KL-divergence:\n;>BB=â€™\n8%(8)log%(8)\n&(8)\nwhere8is a performance class, %(8)and&(8)indicate how\nmany queries are assigned to class 8in the prediction and\nreal statistics.\nThe tree-LSTM is trained together with the controller.\nThen, it is applied to help us identify performance outliers.\nDuring the query processing, we collect the statistics of our\nindex and use it to make a prediction for the performance\nperiodically. Let G0be the initial performance of an index\nblock after the last update. We use GCandG0\nCto denote itspredicted performance and real performance at the Cth epoch.\nAn index block is marked as an outlier if either GC<G0\nCfor\nmore thangepochs, orGC\u0000G0>lG0.gandlcan be tuned\nto balance the index tuning cost and processing cost.\nTo reduce the tuning cost, we identify the outliers in a\nbottom-up way. If an index block is outlier, we continue to\ncheck its parent. If all child blocks are not outliers, we stop thecheck for this block. For those marked as outliers, we invokethe controller to ï¿¿nd a new index structure, while for the rest\nindex blocks, we keep their existing conï¿¿gurations. To speed\nup the learning, instead of processing queries against the\nnew index to collect latencies, the controller asks function\nâŒ§to obtain an estimated performance. During the updating\nprocess, we reserve the old outlier blocks to support queries.\nOnly when the new ones have been learned, will we replace\nthe old ones.\n4 PROCESSING OF THE INDEX\nIn this section, we introduce how the predicted index can be\nmaterialized as a physical index, and how the index can be\napplied to process queries and updates.\n4.1 Index Materialization\nThe index construction is performed in two steps. In the ï¿¿rst\nstep, the index builder loads the hyper-parameter predictionsfrom the controller, which are indexed in a key-value store, to\nbuild a logical index. The logic index establishes the general\nstructure of the index, but cannot support queries. In thesecond step, index builder scans data and feeds them into\nthe logical index in a streaming way. The logical index ï¿¿lls\nin detailed key ranges and builds necessary pointers, which\nareï¿¿nally materialized as a physical index.\n4.1.1 Logical Index. To create the logical index, we ï¿¿rst set\nup the parent-child relationships between index groups. Theleft index in Figure 5 shows a logical index (to simplify the di-gram, the skip links are not shown). During the construction\nof the logical index, we create the parent-child pointers, the\nkey ranges of each index blocks and the skip links inside each\nindex block group. Algorithm 1 shows the workï¿¿ow of how\nthe logical index is established. If parent node is an ordered\nblock, we split its key range evenly and assign to each child\nblock group (line 3-5). Otherwise, child block groups will\nshare the same key range with their parent block, because\nthe hash function will project keys into random blocks (line\n7-9).\nAlgorithm 2 illustrates how an index block group is set\nup. First, it partitions the key range evenly and generates a\nï¿¿xed number of index blocks based on the predicted hyper-parameters (line 3-6). Then, it creates multiple skip connec-\ntions with the probabilities speciï¿¿ed in the hyper-parameters\n(line 7-16).\n7\n\n0         199        200    399        400     599        600      799\n        0         33     34       66   67        99         100     124          125    149           150     174           175      199\n        100  108   109  116    117  124        100   112   113  124 150   162  163  174...0          198       212     352        429    579        654     760\n     0            33    35       60             105     124         125       144         151       174        176       198\n       106   108  111  111   118   120        105   109   113  124\nData Blocks                    164   172     151  155   158   158   163   167    174   174  Unordered Index\nOrdered Index\n...Key Bucket Id\n155\n163\n164\n158\n172\n174\n167\n166\n...2\n2\n1\n2\n1\n2\n2\n1\n...\n150   156   157   162   163   168    169    174...\n...\n...Group 1\nGroup 2 Group 3\nGroup 4 Group 5 Group 6 Group 7Group 1\nGroup 2 Group 3\nGroup 4 Group 5 Group 6 Group 7Figure 5: Materialization of the Index\nAlgorithm 1 LogicalIndex(ParentBlock ?, ParameterIndex\n8=)\n1:for8= 1 to ?.xdo\n2: if?is ordered block then\n3: A=[?.!+(?.*\u0000?.!)(8\u00001)\n?.G,? .!+(?.*\u0000?.!)8\n?.G)\n4: 6= CreateIndexBlockGroup( A,8,8=[8])\n5: ?.2âŒ˜8;3 [8]=6\n6: else\n7: A=[?.!,?.*)\n8: 6= CreateIndexBlockGroup( A,8,8=[8])\n9: ?.2âŒ˜8;3 [8]=6\nAlgorithm 2 CreateIndexBlockGroup(Range A, Index8, Pa-\nrameterIndex 8=)\n1:?0A0<B =8=.getHyperParameters( 8)\n2:List6A>D? =;\n3:for9=1 to ?0A0<B .~do\n4: A0=[A.!+(A.*\u0000A.!)(9\u00001)\n?0A0<B .~,A.!+(A.*\u0000A.!)9\n?0A0<B .~)\n5: IndexBlock âŒ«= new IndexBlock( ?0A0<B ,A0)\n6: 6A>D? .add(âŒ«)\n7:for9=0 to ?0A0<B .~\u00001do\n8: IndexBlock âŒ«=6A>D? .get(9)\n9: for:=1 to log~do\n10: C0A64C =9+2:\n11: ifC0A64C >?0A0<B .~then\n12: break\n13: Â¯?= roll a dice\n14: ifÂ¯?\u0000?0A0<B .W[:]then\n15: IndexBlock âŒ«0=6A>D? .get(C0A64C )\n16: create a skip link between âŒ«andâŒ«0\nIf all index blocks are ordered blocks, the generated index\nstructure is similar to the B+-tree. However, if unordered\nindex blocks are adopted, the index structure becomes a\nhybrid one. For the left index in Figure 5, suppose Gis 2 for\nthe root index block group. The ï¿¿rst index block group and\nsecond index block group at level 2 are responsible for the\nkey range [0, 99] and [100, 199], respectively. The ï¿¿rst groupis ordered blocks and suppose its ~is set as 3. We create three\nordered index blocks inside the group and partition the key\nrange evenly as [0, 33], [34, 66] and [67, 99]. The second index\nblock group in level 2 is unordered index group. If ~=4,\nwe will create four unordered blocks by partitioning the key\nrange [100, 199] uniformly. Because unordered block applies\nhash functions to map keys to its child block groups, the child\nblock groups share the same key range with their parent. For\nexample, group 4 and group 5 all have the key range [100,\n124]. However, group 4 creates 3 blocks and group 5 creates\n2 blocks. Note that in our index, two index block groups may\nhave overlapped key ranges, but for the index blocks in the\nsame group, they always maintain sorted non-intersected\nranges.\n4.1.2 Physical Index. In the second phase, the index builder\nloads data from the disk and feeds them to the logical index\nto materialize the index. The right index in Figure 5 shows\nthe corresponding physical index for the left logic index.\nThe materialization process mainly handles three tasks:\n(1)Update key ranges of index blocks. The key range of\neach block in logical index is just a rough estimation.\nDuring the materialization process, we maintain a set\nof [Min, Max] values for each block, indicating the\nactual minimal and maximal keys in each key range.\nAfter all data have been processed, we shrink the key\nranges of an index block by the values. This helps us\nreduce the search cost by ï¿¿ltering the blocks as early\nas possible.\n(2)Set up hash tables for unordered blocks. As shown in\nFigure 5, when data are streamed over an unordered\nblock, we will set up the corresponding hash table.\nSuppose there are Gchild index block groups and the\nnext key is:.:will be routed to the 8th group, where\n8=âŒ˜0BâŒ˜ (:)%G. To help the search, we also create a\n8\n\nAlgorithm 3 Materialize(BlockGroup âŒ§, Tuple), Oï¿¿set$)\n1:ifâŒ§is ordered block group then\n2: âŒ«=G.ï¿¿ndOverlappedBlock( ).:)\n3: ifâŒ«is at the bottom level then\n4: âŒ«.insert( ).:,$)\n5: else\n6: 8=âŒ«.ï¿¿nd( ).:)\n7: âŒ«[8].updateMinMax( ).:)\n8: Materialize( âŒ«[8],),$)\n9:else\n10: âŒ«=ï¿¿ndOverlappedBlock( ).:)\n11: ifâŒ«is at the bottom level then\n12: âŒ«.hash[ ).:]=$\n13: else\n14: 8=âŒ«.hash( ).:)%âŒ«.G\n15: âŒ«[8].updateMinMax( ).:)\n16: âŒ«.updateBloomFilter( ).:)\n17: Materialize( âŒ«[8],),$)\nbloom ï¿¿lter for each unordered block to check whether\na key exists or not.\n(3)Create pointers to disk data. When a key is routed to\nthe bottom level of the index, we will create a pointer\nfrom the key to the disk oï¿¿set of the corresponding\nrecord. For secondary index, one key may refer to\nmultiple records. Then, we will merge them as a sorted\nlist for disk oï¿¿sets.\nAlgorithm 3 summarizes the whole materialization pro-\ncess. For a new tuple )and its disk oï¿¿set $, weï¿¿rst retrieve\nthe block whose key range overlaps with ).:. If current\ngroup is at the bottom level, we just insert the key and its\noï¿¿set. Otherwise, we forward the tuple to the corresponding\nchild group. The same process repeats for the unordered\nblock group. The only diï¿¿erence is that we apply the hash\nfunction to map the tuple to a speciï¿¿c child group.\nThe right index in Figure 5 is the materialized index for\nthe left one. We can ï¿¿nd that the key ranges of index blocks\nare shrunk. E.g., the ranges of group 1 change from [0, 199],\n[200, 399],...,[600, 799] to [0,198], [212,352],...,[654,760]. The\nhash tables have been set up (we show the hash table of the\nthird block in group 3). Note that we do not need to maintainthe hash tables explicitly. We only need to know which hash\nfunction is being applied. Finally, we create the links from\nthe keys to their disk oï¿¿sets in the bottom level. Note that in\nFigure 5, the gray nodes indicate that the nodes are empty,\nsince their key ranges do not contain any keys.\nDuring the materialization, an index block at bottom layer\nmay be overloaded during the materialization process, trig-\ngering the block splitting operation. We will discuss this\nissue in our index update section.\n0         198      212    352      429    579       654     760\n     0          33   35       60             105     124        125     144        151      174       176      198\n       106   108  111  111   118   120        105  109  113  124\nData Blocks                    164   172     151  155   158   158    163   167   174   174  ...\n...\n... Group 1\nGroup 2 Group 3\nGroup 4 Group 5 Group 6 Group 7LOOKUP  167Figure 6: Processing of Lookup Queries\n4.2 Search via Index\nAfter the index has been materialized, we can apply it to\nprocess queries. In this paper, we focus on the lookup and\nrange queries. As a hybrid index, our search process is a com-\nbination of B+-tree, Hash and skip-list. We use two examples\nin Figure 6 and Figure 7 to demonstrate the lookup and range\nqueries, respectively.\nSuppose a lookup query retrieves the key â€œ167\". The search\nprocess works as follows. We ï¿¿rst check the root block\ngroup(Group 1). Because the second key range of the ï¿¿rst\nblock contains 167, we route the query to the second child\nindex group of the ï¿¿rst block(Group 3). In group 3, the\nkey ranges of the ï¿¿rst block do not contain the key. So we\nroute the queries based on the skip link to the third block.\nThen, we apply the hash function to retrieve the next index\ngroup(Group 7). Before forwarding the query to group 7, wealso test it against the bloomï¿¿lter of the block. If bloomï¿¿lter\nreturns a positive result, we continue the query in group 7.\nSince no skip link is set up in group 7, we scan the blocksone by one until reaching the third one, where the key is\nlocated.\nAlgorithm 4 gives the pseudo code for the lookup. The\nfunction SkipListSearch simulates the search of skip list, where\nwe follow the skip link which points to the block satisfying:\neither its key range contains the key or its maximal key is\nthe largest maximal key smaller than the search key.\nFor the range query [118, 124] in Figure 7, we start the\nsame process as the lookup query. The main diï¿¿erence is how\nthe query is processed when reaching an unordered block\ngroup(Group 3). The query overlaps with the ï¿¿rst block. But\na hash function may distribute the keys to all the child block\ngroups (Group 4 and Group 5). So the range query should\nbe forwarded to both groups. Inside each group, we follow\n9\n\n0          198      212     352      429    579      654     760\n       0        33   35       60           105     124        125    144         151      174       176      198\n106    108   111   111   118    120       105   109   113  124\nData Blocks                    164   172     151  155   158   158   163   167    174   174  \n  ...\n...\n...Group 1\nGroup 2 Group 3\nGroup 4 Group 5 Group 6 Group 7RANGE  118  124Figure 7: Processing of Range Queries\nAlgorithm 4 Lookup(Key :, GroupâŒ§)\n1:âŒ«=âŒ§.SkipListSearch( :)\n2:ifâŒ«is the ordered block then\n3: ifâŒ«is at the bottom level then\n4: return âŒ«.ï¿¿nd( :)\n5: else\n6: 8=âŒ«.ï¿¿nd( :)\n7: Lookup( :,âŒ«[8])\n8:else\n9: ifnotâŒ«.bloomï¿¿lter( :)then\n10: return NULL\n11: ifâŒ«is at the bottom level then\n12: return âŒ«.hash[ :]\n13: else\n14: 8=âŒ«.hash[ :]%âŒ«.G\n15: Lookup( :,âŒ«[8])\nthe skip link to locate the smallest key and then scan the\nremaining blocks until reaching the largest key. We discard\nthe details of range search algorithm.\n4.3 Update of the Index\nDuring the index materialization, blocks at the bottom level\nmay be overï¿¿owed, if more than U<keys are inserted. This\nwill trigger a node splitting operation. If the controller gener-\nates a good estimation for the data distribution, this problem\ncan be partially avoided by generating an index tailored for\nthe data distribution.\nOne key design philosophy of the NIS is that the index\ndoes not need to be balanced. It may create more levels of\nblocks for high-density data partition to facilitate the search.\nThe unbalanced assumption reduces the complexity of node\nsplitting and merging, since we can limit node splitting andmerging to the groups at bottom level and do not propagate\nto the upper level.\nWhen a bottom index block has more than U<keys, we\nsplit the block evenly into two new blocks. The new blocks\nshare the same hyper-parameters as they reside in the same\ngroup. The splitting does not aï¿¿ect the parent block, since the\nkey range of the index group does not change. However, we\nneed to update the skip links, since new blocks are inserted\ninto the group.\nThe merge process follows the same strategy as the split\none. Two consecutive blocks are merged together when both\nblocks have less than V<keys. And we update the skip links\nof the new block.\nDuring the insertion and deletion, we also need to handle\nthe changes of key ranges. As shown in Figure 5, the ini-\ntial key ranges are setup during the materialization process.\nWhen a new key â€œ150\" is inserted, no existing index blocks\ncan hold the key. So we need to ï¿¿nd the nearest block to\nexpand it key range. We called the process, expanding. The\nnearest block is deï¿¿ned as block âŒ«in the group with the\nminimal min (|âŒ«.!\u0000:|,|âŒ«.*\u0000:|), where [âŒ«.!,âŒ«.*]isâŒ«â€™s\nkey range.\nIn Figure 5, the insertion process invokes the expanding\nfunction for group 3, who ï¿¿nds the closest block to key â€œ150\"\nis the third one and expands its range from [151, 174] to [150,\n174]. Since this is an unordered block, it applies the hash\nfunction to decide which child index group should handlethe insertion. Suppose it is group 6, who has one empty\nblock and one block for [164, 172]. The empty block has the\nhighest priority during the expanding process. So key â€œ150\"\nwill be stored at the empty block, which updates its range as\n[150, 150]. The deletion process follows the same strategyby introducing a range shrinking processing. We will not\nelaborate the details.\n5 EXPERIMENTS\nWe train the NIS using two servers sharing the same hard-\nware conï¿¿gurations (Xeon CPU 32 cores, 64GB DDR3 RAM,\n2MB L2 and 20MB L3 and NVIDIA GTX TITAN X). One\nserver is dedicated to the training process of controller and\nthe other one is used for index materialization and evalua-\ntions. We employ four datasets for evaluations: a synthesis\nuniform dataset (uniform64) and three real datasets ( amzn,\nfacebook, osmc). All datasets have 200 million keys. Detailed\ndescriptions of the datasets can be found in [5].\nFor comparison purpose, we use the open-sourced im-\nplementations of B+-Tree, SkipList, ART[ 9], FAST[ 3], Bw-\nTree[ 21] and Learned Index[ 7] (denoted as RMI) as our base-\nlines1. All indexes are in-memory indexes and no disk I/Os\nare involved.\n1https://github.com/learnedsystems/rmi\n10\n\n0 100 200 300\nEpoch0.990.9920.9940.9960.9981Space Utilization(%)\n600 700 800 900\nLatency per query(ns)0.990.9920.9940.9960.9981Space Utilization(%)\n( b ) ( a )\nFigure 8: Training of the Controller0 50 100 150\nEpoch10-210-1100101102103Materialization Time (s)100M\n10M\n1M\nFigure 9: Cost of Index Materialization\nosmc64\nBtree ART RMI NIS\nMasstreeBwtree020040060080010001200Latency per query(ns)uniform64\nBtree ART RMI NIS\nMasstreeBwtree020040060080010001200Latency per query(ns)amzn32\nBtreeART RMI NIS\nMasstreeBwtreeFAST020040060080010001200Latency per query(ns)face64\nBtree ART RMI NIS\nMasstreeBwtree0200400600800100012001400Latency per query(ns)\nFigure 10: Performance of Lookup-Only Workload\n5.1 Training the NIS\nWeï¿¿rst show the training performance of the controller\nfor the uniform64 dataset with read-only workloads. During\nour training, the batch size of the controller is set as 64.\nNamely, it will generate 64 diï¿¿erent index structures. Toobtain proper rewards, those indexes are materialized by\nthe index builder and deployed to be evaluated against the\npre-deï¿¿ned workloads. The training process for one batch\nis considered as an epoch for the learning model, typically\nlasting from a few minutes to less than one second (mainly for\nindex materialization and evaluation). This is because when\nthe index structure is close to convergence, its processingcost is signiï¿¿cantly reduced. As shown in Figure 8(a), the\ncontroller requires about 100 epochs to converge.\nThe reward function used in the controller is a combina-\ntion of the processing latency and index space utilizationcost. We use parameter\ndto tune the weights of the two\nterms. Figure 8(b) shows the eï¿¿ect of diï¿¿erent reward func-\ntions. When d=0, we only consider the space utilization and\nhence the generated index is almost full. On the other hand,\nifd=1, the latency is the only concern. We observe that by\ntuning parameter d, we can achieve a trade-oï¿¿ between the\nlatency and space utilization. In the following experiments,\ndis set as 1 to minimize the search latency.In Figure 9, we show the cost of index materialization,\nnamely, the latency of transforming a logic index into phys-\nical index. We test 1 million to 100 million data. The mate-\nrialization cost reduces during the training, since the index\nstructure has been cached and is only partially updated in\nthe following epochs.\n5.2 Performance of Lookup-Only\nWorkload\nIn this experiment, we compare the NIS with other base-\nline approaches. We generate 10 million single key lookup\nqueries following the same distribution with the correspond-\ning dataset. Namely, we have more queries for high-density\ndata ranges. Figure 10 shows the performances of diï¿¿erent\napproaches on diï¿¿erent datasets. The y-axis denotes the av-\nerage processing cost of queries in nano-seconds. osmc is the\nmost complex dataset and hence, NIS performs much better\nthan the other approaches, indicating that it can tune theindex structure based on data and query patterns. For the\nother two real datasets, amzn andfacebook, NIS still performs\nbetter than the RMI. However, for the uniform dataset, RMI\nachieves a better performance. This is because of the com-\nplex structure of NIS and the neural network may not fully\nconverge. We also ï¿¿nd that some state-of-the-art indexes,\nsuch as ART and FAST, are quite good at processing lookup\n11\n\nosmc w1\nBtreeART\nSKIPLISTRMI NIS\nMasstreeBwtree0200040006000Latency per query(ns)osmc w2\nBtree ART\nSKIPLISTNIS\nMasstreeBwtree01234Latency per query(ns)104 osmc w3\nBtree ART\nSKIPLISTNIS\nMasstreeBwtree02000400060008000Latency per query(ns)osmc w4\nBtree ART\nSKIPLISTNIS\nMasstreeBwtree0300060009000Latency per query(ns)\nuniform w1\nBtreeART\nSKIPLISTRMI NIS\nMasstreeBwtree0200040006000Latency per query(ns)uniform w2\nBtree ART\nSKIPLISTNIS\nMasstreeBwtree0123Latency per query(ns)104 uniform w3\nBtree ART\nSKIPLISTNIS\nMasstreeBwtree0200040006000Latency per query(ns)uniform w4\nBtree ART\nSKIPLISTNIS\nMasstreeBwtree0200040006000Latency per query(ns)\nFigure 11: Performance of Mixed Workload\n5 10 15 20 25 30\nIncreased datasize(M)740760780800820840860880Latency per query(ns)default.\ninc\ntrained\n5 10 15 20 25 30\nIncreased datasize(M)640660680700720740760780Latency per query(ns)default.\ninc\ntrained\n5 10 15 20 25 30\nIncreased datasize(M)600650700750800850Latency per query(ns)default.\ninc\ntrained\n5 10 15 20 25 30\nIncreased datasize(M)850900950100010501100Latency per query(ns)default.\ninc\ntrained\n( a )(  b )(  c )(  d )\nFigure 12: Incremental Learning\nrequests (the open-sourced FAST does not support 64 bits\nkeys, so we only show its performance on 32 bits dataset).\n5.3 Performance of Mixed Workload\nIn this experiment, we focus on two datasets, osmc anduni-\nform, and generate four diï¿¿erent workloads. W1 denotes the\nworkload that we used in previous experiment (10 million\nlookup queries). W2 contains 1 million range queries with a\nselectivity 1%. W3 mixes 5 millions lookups and 5 millions\ninsertions operations. W4 mixes 2 millions lookups, 2 mil-\nlions insertions and 1 million range queries (selectivity=1%).\nWe show the average query processing cost in nano-seconds.\nFigure 11 shows the results. RMI is only shown in W1, be-\ncause current RMI implementation does not support range\nqueries and updates. For the mixed workload, NIS shows asuperior performance than the other indexes even on the\nuniform dataset, indicating that it can be used to support\nvarious application scenarios.\n0 10 20 30 40 50 60 70 80 90 100\nTime(s)5006007008009001000110012001300Latency per query(ns)\nFigure 13: Performance of Incremental Learning\n12\n\n5.4 Performance of Incremental Learning\nNIS assumes that data and query distributions of an appli-\ncation only change slowly and we propose an incremental\nlearning technique to periodically update our index struc-\nture. In this experiment, we evaluate the performance ofincremental learning on the uniform dataset with 200 mil-\nlion keys initially. Each experiment is run for 6 episodes, anddiï¿¿erent workloads are submitted for processing during each\nepisode. The ï¿¿rst workload is used to train the NIS model,\nwhile the rest 5 are employed to test the performance of\nincremental learning. A workload contains 5 million lookupsand 5 million insertions. So the size of dataset increases from\n200 millions to 230 millions with 30 millions newly inserted\ndata. Consequently, the average processing cost of queries\nwill gradually increase. We compare three approaches. The\ndefault approach does not apply incremental learning. The\nincapproach shows the performance of NIS using incremen-\ntal learning, while the trained line shows the ideal case that\nwe train a new model from scratch at each episode for all\nexisting data and queries.\nIn Figure 12(a), we train NIS with a uniform workload,\nwhere both reads and insertions are randomly generated\nbased on the data density. All the following 5 workloads\nare also uniform workloads. This represents the case where\nquery patterns do not change over time, and hence, even\ndefault approach shows a performance similar to the ideal\ncase. In Figure 12(b), both reads and writes follow the log-\nnormal distribution (mean=0, stdvar=0.7). We simulate the\ncase where we change data from uniform distribution to\nlog-normal one, while keeping the query distribution as log-\nnormal distribution. We can see that the incremental learning\nstill can provide a similar performance as the ideal one. In\nFigure 12(c), we simulate the case where both data and query\ndistributions change over time. All writes follow the log-normal distribution, and all reads are randomly sampled\nfrom keys based on the data density. During the insertion,\nthe density changes over time, causing the query distribution\nto evolve as well. We observe that the incremental learning\ncan still tune the index structure, but a bit slower than the\nideal case, because it realizes the changes only when a new\nepisode starts. In the last case (Figure 12(d)), we test the\ncase where the query distribution dramatically changes in\na short time. The reads follow a normal distribution with a\nmoving center for each episode. Writes follow the log-normal\ndistribution. After a few episodes, the performance of NIS\nhas a large gap from the ideal case. It indicates that a full\nlearning process is required, if data and query distribution\nchange frequently over time.\nFinally, we show the progress of incremental learning with\na dramatic changing workload in Figure 13. We have an orig-\ninal dataset with 50 million keys and 9 diï¿¿erent workloads,each of which contains 10 million lookups and 10 million\ninsertions, and follows a log-normal distribution with a dif-ferent center. Every 10 seconds, the incremental learning is\ninvoked to update the index. And we can see that the new\nindex structure can provide a better performance until the\nnext workload starts.\n6 RELATED WORK\nThe modern database management system (DBMS) becomes\nso complex for optimization and maintenance, that even\ndatabase experts may not be able to ï¿¿gure out the optimal\ndesign and conï¿¿guration of the database for speciï¿¿c applica-tions. Recently, the database community starts applying deep\nlearning techniques to reduce the complexity of database\nmanagement. In [ 20], some possible research areas for deep\nlearning techniques on database, such as database tuning,\nquery optimization and index search, are discussed.\nIn the database tuning area, the CMU group designs the\nOtterTune2, an autonomous database system [ 15][25][16].\nThe OtterTune collects the data for the running status of the\ndatabase and builds a series of machine learning models (in-\ncluding deep learning models and classic machine learning\nmodels) to ï¿¿nd the optimal conï¿¿guration knobs. The idea\nis to allow anyone to deploy a DBMS without any exper-tise in database administration. Following their approach,\nCBDTune[\n26] proposes to use the reinforcement learning\nmodel to perform the conï¿¿guration tuning for the cloud data-\nbase. They adopt the deterministic policy gradient model\nwhich is similar to the one used in the NIS. The performance\nchange (latency and throughput) is used as the reward during\nthe training process.\nDiï¿¿erent from OtterTune and CBDTune, Li et. al. propose a\nquery-aware automatic tuning approach, QTune[ 10]. QTune\nvectorizes a set of SQL queries by extracting the semantic\nfeatures of the SQL queries. The vector representations of\nSQL queries are fed to the deep reinforcement learning model,\nwhich is trained to identify the proper conï¿¿gurations of the\nDBMS optimized for those queries.\nQueries involving multiple join operators incur high pro-\ncessing costs and are hard to optimize. Krishnan et. al. pro-\npose applying the deep learning techniques for join query\noptimization[ 8]. Their reinforcement learning model is in-\ntegrated with Postgres and SparkSQL and is shown to beable to generate plans with optimization costs and query\nexecution times competitive with the native query opti-\nmizer. SkinnerDB[ 19] further improves the prediction of join\nqueries by splitting queries into many small time slices. The\nlearning model tries diï¿¿erent join orders during diï¿¿erent\ntime slices and promising plan is selected. Neo[ 13], on the\n2https://github.com/cmu-db/ottertune\n13\n\nother hand, tries to rewrite the database optimizer in a learn-\ning language. Neo bootstraps its query optimization model\nfrom the conventional database optimizers and updates its\nstrategy based on the predicted query plans and their real\nprocessing costs. Experiments show that Neo outperforms\nthe original optimizer in Postgres.\nInstead of replacing the whole database optimizer, some\nwork try to improve the cost estimation using the deep learn-\ning model. In [ 22], [24] and [ 4], deep learning models are\napplied to estimate the query selectivities or data cardinali-\nties. If columns are highly correlated, the histogram-based\nestimation may be far from the real result, due to the indepen-\ndent assumption. On the contrary, deep learning model can\ncatch the correlations among columns and rows. Therefore,\nit can potentially generate a more precise estimation.\nThe deep learning approach can be also adopted to search\nfor new data structures for the DBMS[ 6]. The closest work\nto ours is the learned index from Google[ 7]. They formalize\nthe index as a key mapping function(given a key, return its\nposition at the disk) and apply the neural models to learn the\nindex in an ad hoc way. We adopt a totally diï¿¿erent approachby sticking to the conventional index structures (ordered and\nunordered blocks in this paper) and ask the neural model to\nlearn how those basic structures can be assembled together\nas a full-ï¿¿edged index. Once the index has been predicted,\nwe do not need the neural model. Hence, the performance\nof index is comparable to state-of-the-art indexes, because\nwe avoid the expensive cost incurred by the neural model\nprediction in [7].\n7 CONCLUSION\nIn this paper, we propose a Neural Index Search(NIS) ap-\nproach to automatically tune indexes for a given dataset and\nworkload. The NIS applies the reinforcement learning ap-\nproach to assemble abstract index blocks into a full-ï¿¿edged\nindex and tests it against the target workload. The index\nperformance is used as the reward for the learning model to\nupdate its strategy. Gradually, the predicted index converges\nto aï¿¿ne-tuned structure. In theory, our NIS can simulate\nmany existing index structures, such as B+-tree index, Hash\nindex and Skip List index. It can also explore the index struc-\ntures that have never been examined. We also propose anincremental learning approach to support progressive up-\ndates of NIS. In our experiments, the index generated by NIS\nachieves a comparable performance to existing state-of-the-\nart index structures.\nREFERENCES\n[1]S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural\nComputation, 9(8):1735â€“1780, 1997.\n[2]G. Huang, X. Cheng, J. Wang, Y. Wang, D. He, T. Zhang, F. Li, S. Wang,\nW. Cao, and Q. Li. X-engine: An optimized storage engine for large-\nscale e-commerce transaction processing. In Proceedings of the 2019International Conference on Management of Data, SIGMOD Conference\n2019, Amsterdam, The Netherlands, June 30 - July 5, 2019, pages 651â€“665,\n2019.\n[3]C. Kim, J. Chhugani, N. Satish, E. Sedlar, A. D. Nguyen, T. Kaldewey,\nV. W. Lee, S. A. Brandt, and P. Dubey. FAST: fast architecture sensitive\ntree search on modern cpus and gpus. In Proceedings of the ACM\nSIGMOD International Conference on Management of Data, SIGMOD\n2010, Indianapolis, Indiana, USA, June 6-10, 2010, pages 339â€“350, 2010.\n[4]A. Kipf, T. Kipf, B. Radke, V. Leis, P. A. Boncz, and A. Kemper. Learned\ncardinalities: Estimating correlated joins with deep learning. In CIDR\n2019, 9th Biennial Conference on Innovative Data Systems Research,\nAsilomar, CA, USA, January 13-16, 2019, Online Proceedings, 2019.\n[5]A. Kipf, R. Marcus, A. van Renen, M. Stoian, A. Kemper, T. Kraska,\nand T. Neumann. Sosd: A benchmark for learned indexes. NeurIPS\nWorkshop on Machine Learning for Systems, 2019.\n[6]T. Kraska, M. Alizadeh, A. Beutel, E. H. Chi, A. Kristo, G. Leclerc,\nS. Madden, H. Mao, and V. Nathan. Sagedb: A learned database sys-\ntem. In CIDR 2019, 9th Biennial Conference on Innovative Data Systems\nResearch, Asilomar, CA, USA, January 13-16, 2019, Online Proceedings,\n2019.\n[7]T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case\nfor learned index structures. In Proceedings of the 2018 International\nConference on Management of Data, SIGMOD Conference 2018, Houston,\nTX, USA, June 10-15, 2018, pages 489â€“504, 2018.\n[8]S. Krishnan, Z. Yang, K. Goldberg, J. M. Hellerstein, and I. Stoica.\nLearning to optimize join queries with deep reinforcement learning.\nCoRR, abs/1808.03196, 2018.\n[9]V. Leis, A. Kemper, and T. Neumann. The adaptive radix tree: Artful\nindexing for main-memory databases. In 29th IEEE International Con-\nference on Data Engineering, ICDE 2013, Brisbane, Australia, April 8-12,\n2013, pages 38â€“49, 2013.\n[10] G. Li, X. Zhou, S. Li, and B. Gao. Qtune: A query-aware database tuning\nsystem with deep reinforcement learning. PVLDB, 12(12):2118â€“2130,\n2019.\n[11] C. Liu, L. Chen, F. Schroï¿¿, H. Adam, W. Hua, A. L. Yuille, and L. Fei-Fei.\nAuto-deeplab: Hierarchical neural architecture search for semantic\nimage segmentation. CoRR, abs/1901.02985, 2019.\n[12] H. Liu, K. Simonyan, and Y. Yang. DARTS: diï¿¿erentiable architecture\nsearch. In 7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019.\n[13] R. C. Marcus, P. Negi, H. Mao, C. Zhang, M. Alizadeh, T. Kraska,O. Papaemmanouil, and N. Tatbul. Neo: A learned query optimizer.\nPVLDB, 12(11):1705â€“1718, 2019.\n[14] B. Oâ€™Donoghue, R. Munos, K. Kavukcuoglu, and V. Mnih. Combining\npolicy gradient and q-learning. In 5th International Conference on\nLearning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings, 2017.\n[15] A. Pavlo, G. Angulo, J. Arulraj, H. Lin, J. Lin, L. Ma, P. Menon, T. Mowry,\nM. Perron, I. Quah, S. Santurkar, A. Tomasic, S. Toor, D. V. Aken,\nZ. Wang, Y. Wu, R. Xian, and T. Zhang. Self-driving database manage-\nment systems. In CIDR 2017, Conference on Innovative Data Systems\nResearch, 2017.\n[16] A. Pavlo, M. Butrovich, A. Joshi, L. Ma, P. Menon, D. V. Aken, L. Lee,\nand R. Salakhutdinov. External vs. internal: An essay on machine\nlearning agents for autonomous database management systems. IEEE\nData Engineering Bulletin, pages 32â€“46, June 2019.\n[17] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proxi-\nmal policy optimization algorithms. CoRR, abs/1707.06347, 2017.\n[18] K. S. Tai, R. Socher, and C. D. Manning. Improved semantic repre-sentations from tree-structured long short-term memory networks.\n14\n\nInProceedings of the 53rd Annual Meeting of the Association for Com-\nputational Linguistics and the 7th International Joint Conference on\nNatural Language Processing of the Asian Federation of Natural Lan-\nguage Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1:\nLong Papers, pages 1556â€“1566, 2015.\n[19] I. Trummer, J. Wang, D. Maram, S. Moseley, S. Jo, and J. Antonakakis.\nSkinnerdb: Regret-bounded query evaluation via reinforcement learn-\ning. In Proceedings of the 2019 International Conference on Management\nof Data, SIGMOD Conference 2019, Amsterdam, The Netherlands, June\n30 - July 5, 2019., pages 1153â€“1170, 2019.\n[20] W. Wang, M. Zhang, G. Chen, H. V. Jagadish, B. C. Ooi, and K. Tan.\nDatabase meets deep learning: Challenges and opportunities. SIGMOD\nRecord, 45(2):17â€“22, 2016.\n[21] Z. Wang, A. Pavlo, H. Lim, V. Leis, H. Zhang, M. Kaminsky, and D. G.\nAndersen. Building a bw-tree takes more than just buzz words. In\nProceedings of the 2018 International Conference on Management ofData, SIGMOD Conference 2018, Houston, TX, USA, June 10-15, 2018,\npages 473â€“488, 2018.\n[22] L. Woltmann, C. Hartmann, M. Thiele, D. Habich, and W. Lehner.\nCardinality estimation with local deep learning models. In Proceedings\nof the Second International Workshop on Exploiting Artiï¿¿cial IntelligenceTechniques for Data Management, aiDM@SIGMOD 2019, Amsterdam,\nThe Netherlands, July 5, 2019, pages 5:1â€“5:8, 2019.\n[23] S. Xie, H. Zheng, C. Liu, and L. Lin. SNAS: stochastic neural architec-\nture search. In 7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019.\n[24] Z. Yang, E. Liang, A. Kamsetty, C. Wu, Y. Duan, X. Chen, P. Abbeel,\nJ. M. Hellerstein, S. Krishnan, and I. Stoica. Selectivity estimation with\ndeep likelihood models. CoRR, abs/1905.04278, 2019.\n[25] B. Zhang, D. V. Aken, J. Wang, T. Dai, S. Jiang, J. Lao, S. Sheng, A. Pavlo,and G. J. Gordon. A demonstration of the ottertune automatic database\nmanagement system tuning service. PVLDB, 11(12):1910â€“1913, 2018.\n[26] J. Zhang, Y. Liu, K. Zhou, G. Li, Z. Xiao, B. Cheng, J. Xing, Y. Wang,\nT. Cheng, L. Liu, M. Ran, and Z. Li. An end-to-end automatic cloud\ndatabase tuning system using deep reinforcement learning. In Pro-\nceedings of the 2019 International Conference on Management of Data,\nSIGMOD Conference 2019, Amsterdam, The Netherlands, June 30 - July\n5, 2019., pages 415â€“432, 2019.\n[27] B. Zoph and Q. V. Le. Neural architecture search with reinforcement\nlearning. In 5th International Conference on Learning Representations,\nICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceed-\nings, 2017.\n15",
  "textLength": 64629
}