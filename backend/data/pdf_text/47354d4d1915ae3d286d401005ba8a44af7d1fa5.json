{
  "paperId": "47354d4d1915ae3d286d401005ba8a44af7d1fa5",
  "title": "A Deep Look into Neural Ranking Models for Information Retrieval",
  "pdfPath": "47354d4d1915ae3d286d401005ba8a44af7d1fa5.pdf",
  "text": "A Deep Look into Neural Ranking Models for\nInformation Retrieval\nJiafeng Guoa,b, Yixing Fana,b, Liang Panga,b, Liu Yangc, Qingyao Aic, Hamed\nZamanic, Chen Wua,b, W. Bruce Croftc, Xueqi Chenga,b\naUniversity of Chinese Academy of Sciences, Beijing, China\nbCAS Key Lab of Network Data Science and Technology, Institute of Computing\nTechnology, Chinese Academy of Sciences, Beijing, China\ncCenter for Intelligent Information Retrieval, University of Massachusetts Amherst,\nAmherst, MA, USA\nAbstract\nRanking models lie at the heart of research on information retrieval (IR). Dur-\ning the past decades, di\u000berent techniques have been proposed for constructing\nranking models, from traditional heuristic methods, probabilistic methods, to\nmodern machine learning methods. Recently, with the advance of deep learn-\ning technology, we have witnessed a growing body of work in applying shallow\nor deep neural networks to the ranking problem in IR, referred to as neural\nranking models in this paper. The power of neural ranking models lies in the\nability to learn from the raw text inputs for the ranking problem to avoid many\nlimitations of hand-crafted features. Neural networks have su\u000ecient capacity\nto model complicated tasks, which is needed to handle the complexity of rel-\nevance estimation in ranking. Since there have been a large variety of neural\nranking models proposed, we believe it is the right time to summarize the cur-\nrent status, learn from existing methodologies, and gain some insights for future\ndevelopment. In contrast to existing reviews, in this survey, we will take a deep\nlook into the neural ranking models from di\u000berent dimensions to analyze their\nunderlying assumptions, major design principles, and learning strategies. We\ncompare these models through benchmark tasks to obtain a comprehensive em-\npirical understanding of the existing techniques. We will also discuss what is\nmissing in the current literature and what are the promising and desired future\ndirections.\nPreprint submitted to Journal of Information Processing and Management June 28, 2019arXiv:1903.06902v3  [cs.IR]  27 Jun 2019\n\nKeywords: neural ranking model, information retrieval, survey\n2010 MSC: 00-01, 99-00\n1. Introduction\nInformation retrieval is a core task in many real-world applications, such as\ndigital libraries, expert \fnding, Web search, and so on. Essentially, IR is the\nactivity of obtaining some information resources relevant to an information need\nfrom within large collections. As there might be a variety of relevant resources,\nthe returned results are typically ranked with respect to some relevance notion.\nThis ranking of results is a key di\u000berence of IR from other problems. Therefore,\nresearch on ranking models has always been at the heart of IR.\nMany di\u000berent ranking models have been proposed over the past decades,\nincluding vector space models [1], probabilistic models [2], and learning to rank\n(LTR) models [3, 4]. Existing techniques, especially the LTR models, have al-\nready achieved great success in many IR applications, e.g., modern Web search\nengines like Google1or Bing2. There is still, however, much room for improve-\nment in the e\u000bectiveness of these techniques for more complex retrieval tasks.\nIn recent years, deep neural networks have led to exciting breakthroughs in\nspeech recognition [5], computer vision [6, 7], and natural language processing\n(NLP) [8, 9]. These models have been shown to be e\u000bective at learning abstract\nrepresentations from the raw input, and have su\u000ecient model capacity to tackle\ndi\u000ecult learning problems. Both of these are desirable properties for ranking\nmodels in IR. On one hand, most existing LTR models rely on hand-crafted\nfeatures, which are usually time-consuming to design and often over-speci\fc\nin de\fnition. It would be of great value if ranking models could learn the\nuseful ranking features automatically. On the other hand, relevance, as a key\nnotion in IR, is often vague in de\fnition and di\u000ecult to estimate since relevance\njudgments are based on a complicated human cognitive process. Neural models\n1http://google.com\n2http://bing.com\n2\n\nwith su\u000ecient model capacity have more potential for learning such complicated\ntasks than traditional shallow models. Due to these potential bene\fts and along\nwith the expectation that similar successes with deep learning could be achieved\nin IR [10], we have witnessed substantial growth of work in applying neural\nnetworks for constructing ranking models in both academia and industry in\nrecent years. Note that in this survey, we focus on neural ranking models for\ntextual retrieval, which is central to IR, but not the only mode that neural\nmodels can be used for [11, 12].\nPerhaps the \frst successful model of this type is the Deep Structured Se-\nmantic Model (DSSM) [13] introduced in 2013, which is a neural ranking model\nthat directly tackles the ad-hoc retrieval task. In the same year, Lu and Li\n[14] proposed DeepMatch, which is a deep matching method applied to the\nCommunity-based Question Answering (CQA) and micro-blog matching tasks.\nNote that at the same time or even before this work, there were a number of\nstudies focused on learning low-dimensional representations of texts with neu-\nral models [15, 16] and using them either within traditional IR models or with\nsome new similarity metrics for ranking tasks. However, we would like to refer\nto those methods as representation learning models rather than neural ranking\nmodels, since they did not directly construct the ranking function with neural\nnetworks. Later, between 2014 and 2015, work on neural ranking models began\nto grow, such as new variants of DSSM [13], ARC I and ARC II [17], MatchPyra-\nmid [18], and so on. Most of this research focused on short text ranking tasks,\nsuch as TREC QA tracks and Microblog tracks [19]. Since 2016, the study of\nneural ranking models has bloomed, with signi\fcant work volume, deeper and\nmore rigorous discussions, and much wider applications [20]. For example, re-\nsearchers began to discuss the practical e\u000bectiveness of neural ranking models\non di\u000berent ranking tasks [21, 22]. Neural ranking models have been applied to\nad-hoc retrieval [23, 24], community-based QA [25], conversational search [26],\nand so on. Researchers began to go beyond the architecture of neural ranking\nmodels, paying attention to new training paradigms of neural ranking models\n[27], alternate indexing schemes for neural representations [28], integration of\n3\n\nexternal knowledge [29, 30], and other novel uses of neural approaches for IR\ntasks [31, 32].\nUp to now, we have seen exciting progress on neural ranking models. In\nacademia, several neural ranking models learned from scratch can already out-\nperform state-of-the-art LTR models with tens of hand-crafted features [33, 34].\nWorkshops and tutorials on this topic have attracted extensive interest in the\nIR community [10, 35]. Standard benchmark datasets [36, 37], evaluation tasks\n[38], and open-source toolkits [39] have been created to facilitate research and\nrigorous comparison. Meanwhile, in industry, we have also seen models such as\nDSSM put into a wide range of practical usage in the enterprise [40]. Neural\nranking models already generate the most important features for modern search\nengines. However, beyond these exciting results, there is still a long way to go\nfor neural ranking models: 1) Neural ranking models have not had the level of\nbreakthroughs achieved by neural methods in speech recognition or computer\nvision; 2) There is little understanding and few guidelines on the design princi-\nples of neural ranking models; 3) We have not identi\fed the special capabilities\nof neural ranking models that go beyond traditional IR models. Therefore, it is\nthe right moment to take a look back, summarize the current status, and gain\nsome insights for future development.\nThere have been some related surveys on neural approaches to IR (neural\nIR for short). For example, Onal et al.[20] reviewed the current landscape of\nneural IR research, paying attention to the application of neural methods to\ndi\u000berent IR tasks. Mitra and Craswell [41] gave an introduction to neural in-\nformation retrieval. In their booklet, they talked about fundamentals of text\nretrieval, and brie\ry reviewed IR methods employing pre-trained embeddings\nand neural networks. In contrast to this work, this survey does not try to cover\nevery aspect of neural IR, but will focus on and take a deep look into ranking\nmodels with deep neural networks. Speci\fcally, we formulate the existing neural\nranking models under a uni\fed framework, and review them from di\u000berent di-\nmensions to understand their underlying assumptions, major design principles,\nand learning strategies. We also compare representative neural ranking models\n4\n\nthrough benchmark tasks to obtain a comprehensive empirical understanding.\nWe hope these discussions will help researchers in neural IR learn from previous\nsuccesses and failures, so that they can develop better neural ranking models in\nthe future. In addition to the model discussion, we also introduce some trending\ntopics in neural IR, including indexing schema, knowledge integration, visual-\nized learning, contextual learning and model explanation. Some of these topics\nare important but have not been well addressed in this \feld, while others are\nvery promising directions for future research.\nIn the following, we will \frst introduce some typical textual IR tasks ad-\ndressed by neural ranking models in Section 2. We then provide a uni\fed\nformulation of neural ranking models in Section 3. From section 4 to 6, we re-\nview the existing models with regard to di\u000berent dimensions as well as making\nempirical comparisons between them. We discuss trending topics in Section 7\nand conclude the paper in Section 8.\n2. Major Applications of Neural Ranking Models\nIn this section, we describe several major textual IR applications where neu-\nral ranking models have been adopted and studied in the literature, including\nad-hoc retrieval, question answering, community question answering, and auto-\nmatic conversation. There are other applications where neural ranking models\nhave been or could be applied, e.g., product search [12], sponsored search [42],\nand so on. However, due to page limitations, we will not include these tasks in\nthis survey.\n2.1. Ad-hoc Retrieval\nAd-hoc retrieval is a classic retrieval task in which the user speci\fes his/her\ninformation need through a query which initiates a search (executed by the\ninformation system) for documents that are likely to be relevant to the user.\nThe term ad-hoc refers to the scenario where documents in the collection remain\nrelatively static while new queries are submitted to the system continually [43].\n5\n\nThe retrieved documents are typically returned as a ranking list through a\nranking model where those at the top of the ranking are more likely to be\nrelevant.\nThere has been a long research history on ad-hoc retrieval, with several well\nrecognized characteristics and challenges associated with the task. A major\ncharacteristic of ad-hoc retrieval is the heterogeneity of the query and the doc-\numents. The query comes from a search user with potentially unclear intent\nand is usually very short, ranging from a few words to a few sentences [41].\nThe documents are typically from a di\u000berent set of authors and have longer\ntext length, ranging from multiple sentences to many paragraphs. Such hetero-\ngeneity leads to the critical vocabulary mismatch problem [44, 45]. Semantic\nmatching, meaning matching words and phrases with similar meanings, could\nalleviate the problem, but exact matching is indispensable especially with rare\nterms [21]. Such heterogeneity also leads to diverse relevance patterns. Di\u000berent\nhypotheses, e.g. verbosity hypothesis and scope hypothesis [46], have been pro-\nposed considering the matching of a short query against a long document. The\nrelevance notion in ad-hoc retrieval is inherently vague in de\fnition and highly\nuser dependent, making relevance assessment a very challenging problem.\nFor the evaluation of di\u000berent neural ranking models on the ad-hoc retrieval\ntask, a large variety of TREC collections have been used. Speci\fcally, retrieval\nexperiments have been conducted over neural ranking models based on TREC\ncollections such as Robust [21, 18], ClueWeb [21], GOV2 [33, 34] and Microblog\n[33], as well as logs such as the AOL log [27] and the Bing Search log [13, 47, 48,\n23]. Recently, a new large scale dataset has been released, called the NTCIR\nWWW Task [49], which is suitable for experiments on neural ranking models.\n2.2. Question Answering\nQuestion-answering (QA) attempts to automatically answer questions posed\nby users in natural languages based on some information resources. The ques-\ntions could be from a closed or open domain [50], while the information re-\nsources could vary from structured data (e.g., knowledge base) to unstructured\n6\n\ndata (e.g., documents or Web pages) [51]. There have been a variety of task for-\nmats for QA, including multiple-choice selection [52], answer passage/sentence\nretrieval [53, 37], answer span locating [54], and answer synthesizing from mul-\ntiple sources [55]. However, some of the task formats are usually not treated as\nan IR problem. For example, multiple-choice selection is typically formulated\nas a classi\fcation problem while answer span locating is usually studied under\nthe machine reading comprehension topic. In this survey, therefore, we focus\non answer passage/sentence retrieval as it can be formulated as a typical IR\nproblem and addressed by neural ranking models. Hereafter, we will refer to\nthis speci\fc task as QA for simplicity.\nCompared with ad-hoc retrieval, QA shows reduced heterogeneity between\nthe question and the answer passage/sentence. On one hand, the question is\nusually in natural language, which is longer than keyword queries and clearer\nin intent description. On the other hand, the answer passages/sentences are\nusually much shorter text spans than documents (e.g., the answer passage length\nof WikiPassageQA data is about 133 words [56]), leading to more concentrated\ntopics/semantics. However, vocabulary mismatch is still a basic problem in\nQA. The notion of relevance is relatively clear in QA, i.e., whether the target\npassage/sentence answers the question, but assessment is challenging. Ranking\nmodels need to capture the patterns expected in the answer passage/sentence\nbased on the intent of the question, such as the matching of the context words,\nthe existence of the expected answer type, and so on.\nFor the evaluation of QA tasks, several benchmark data sets have been\ndeveloped, including TREC QA [53], WikiQA [37], WebAP [57, 58], Insur-\nanceQA [59], WikiPassageQA [56] and MS MARCO [36]. A variety of neural\nranking models [60, 19, 61, 25, 14] have been tested on these data sets.\n7\n\n2.3. Community Question Answering\nCommunity question answering (CQA) aims to \fnd answers to users' ques-\ntions based on existing QA resources in CQA websites, such as Quora3, Yahoo!\nAnswers4, Stack Over\row5, and Zhihu6. As a retrieval task, CQA can be fur-\nther divided into two categories. The \frst is to directly retrieval answers from\nthe answer pool, which is similar to the above QA task with some additional\nuser behavioral data (e.g., upvotes/downvotes) [62]. So we will not discuss this\nformat here again. The second is to retrieve similar questions from the question\npool, based on the assumption that answers to similar question could answer\nnew questions. Unless otherwise noted, we will refer to the second task format\nas CQA.\nSince it involves the retrieval of similar questions, CQA is signi\fcantly dif-\nferent from the previous two tasks due to the homogeneity between the input\nquestion and target question. Speci\fcally, both input and target questions are\nshort natural language sentences (e.g. the question length in Yahoo! Answers\nis between 9 and 10 words on average [63]), describing users' information needs.\nRelevance in CQA refers to semantic equivalence/similarity, which is clear and\nsymmetric in the sense that the two questions are exchangeable in the relevance\nde\fnition. However, vocabulary mismatch is still a challenging problem as both\nquestions are short and there exist di\u000berent expressions for the same intent.\nFor evaluation of the CQA task, a large variety of data sets have been re-\nleased for research. The well-known data sets include the Quora Dataset7,\nYahoo! Answers Dataset [25] and SemEval-2017 Task3 [64]. The recent pro-\nposed datasets include CQADupStack8[65], ComQA9[66] and LinkSO [67]. A\nvariety of neural ranking models [68, 18, 69, 70, 25] have been tested on these\n3https://www.quora.com/\n4https://answers.yahoo.com\n5https://www.stackover\row.com\n6https://zhihu.com\n7https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs\n8https://github.com/D1Doris/CQADupStack\n9http://qa.mpi-inf.mpg.de/comqa\n8\n\ndata sets.\n2.4. Automatic Conversation\nAutomatic conversation (AC) aims to create an automatic human-computer\ndialog process for the purpose of question answering, task completion, and social\nchat (i.e., chit-chat) [71]. In general, AC could be formulated either as an IR\nproblem that aims to rank/select a proper response from a dialog repository [72]\nor a generation problem that aims to generate an appropriate response with re-\nspect to the input utterance [73]. In this paper, we restrict AC to the social chat\ntask with the IR formulation, since question answering has already been covered\nin the above QA task and task completion is usually not taken as an IR prob-\nlem. From the perspective of conversation context, the IR-based AC could be\nfurther divided into single-turn conversation[74] or multi-turn conversation [75].\nWhen focusing on social chat, AC also shows homogeneity similar to CQA.\nThat is, both the input utterance and the response are short natural language\nsentences (e.g., the utterance length of Ubuntu Dialog Corpus is between 10\nto 11 words on average and the median conversation length of it is 6 words\n[76]). Relevance in AC refers to certain semantic correspondence (or coherent\nstructure) which is broad in de\fnition, e.g., given an input utterance \\OMG I\ngot myopia at such an `old' age\", the response could range from general (e.g.,\n\\Really?\") to speci\fc (e.g., \\Yeah. Wish a pair of glasses as a gift\") [26]. There-\nfore, vocabulary mismatch is no longer the central challenge in AC, as we can\nsee from the example that a good response does not require semantic matching\nbetween the words. Instead, it is critical to model correspondence/coherence\nand avoid general trivial responses.\nFor the evaluation of di\u000berent neural ranking models on the AC task, several\nconversation collections have been collected from social media such as forums,\nTwitter and Weibo. Speci\fcally, experiments have been conducted over neural\nranking models based on collections such as Ubuntu Dialog Corpus (UDC) [75,\n77, 78], Sina Weibo dataset [74, 26, 79, 80], MSDialog [81, 30, 82] and the\n\"campaign\" NTCIR STC [83].\n9\n\n3. A Uni\fed Model Formulation\nNeural ranking models are mostly studied within the LTR framework. In\nthis section, we give a uni\fed formulation of neural ranking models from a\ngeneralized view of LTR problems.\nSuppose thatSis the generalized query set, which could be the set of search\nqueries, natural language questions or input utterances, and Tis the generalized\ndocument set, which could be the set of documents, answers or responses. Sup-\npose thatY=f1;2;\u0001\u0001\u0001;lgis the label set where labels represent grades. There\nexists a total order between the grades l\u001fl\u00001\u001f\u0001\u0001\u0001\u001f 1, where\u001fdenotes the\norder relation. Let si2Sbe thei-th query,Ti=fti;1;ti;2;\u0001\u0001\u0001;ti;nig2T be the\nset of documents associated with the query si, and yi=fyi;1;yi;2;\u0001\u0001\u0001;yi;nig\nbe the set of labels associated with query si, wherenidenotes the size of Ti\nandyiandyi;jdenotes the relevance degree of ti;jwith respect to si. LetF\nbe the function class and f(si;ti;j)2F be a ranking function which associates\na relevance score with a query-document pair. Let L(f;si;ti;j;yi;j) be the loss\nfunction de\fned on prediction of fover the query-document pair and their cor-\nresponding label. So a generalized LTR problem is to \fnd the optimal ranking\nfunctionf\u0003by minimizing the loss function over some labeled dataset\nf\u0003= arg minX\niX\njL(f;si;ti;j;yi;j) (1)\nWithout loss of generality, the ranking function fcould be further abstracted\nby the following uni\fed formulation\nf(s;t) =g( (s);\u001e(t);\u0011(s;t)) (2)\nwheresandtare two input texts,  ,\u001eare representation functions which extract\nfeatures from sandtrespectively, \u0011is the interaction function which extracts\nfeatures from ( s;t) pair, and gis the evaluation function which computes the\nrelevance score based on the feature representations.\nNote that for traditional LTR approaches [3], functions  ,\u001eand\u0011are usu-\nally set to be \fxed functions (i.e., manually de\fned feature functions). The\n10\n\nevaluation function gcan be any machine learning model, such as logistic re-\ngression or gradient boosting decision tree , which could be learned from the\ntraining data. For neural ranking models, in most cases, all the functions  ,\u001e,\n\u0011andgare encoded in the network structures so that all of them can be learned\nfrom training data.\nIn traditional LTR approaches, the inputs sandtare usually raw texts. In\nneural ranking models, we consider that the inputs could be either raw texts or\nword embeddings. In other words, embedding mapping is considered as a basic\ninput layer, not included in  ,\u001eand\u0011.\n4. Model Architecture\nBased on the above uni\fed formulation, here we review existing neural rank-\ning model architectures to better understand their basic assumptions and design\nprinciples.\n4.1. Symmetric vs. Asymmetric Architectures\nStarting from di\u000berent underlying assumptions over the input texts sand\nt, two major architectures emerge in neural ranking models, namely symmetric\narchitecture and asymmetric architecture.\nSymmetric Architecture : The inputs sandtare assumed to be homo-\ngeneous, so that symmetric network structure could be applied over the inputs.\nNote here symmetric structure means that the inputs sandtcan exchange\ntheir positions in the input layer without a\u000becting the \fnal output. Speci\fcally,\nthere are two representative symmetric structures, namely siamese networks and\nsymmetric interaction networks.\nSiamese networks literally imply symmetric structure in the network ar-\nchitecture. Representative models include DSSM [13], CLSM [47] and LSTM-\nRNN [48]. For example, DSSM represents two input texts with a uni\fed pro-\ncess including the letter-trigram mapping followed by the multi-layer perceptron\n(MLP) transformation, i.e., function \u001eis the same as function  . After that a\n11\n\ncosine similarity function is applied to evaluate the similarity between the two\nrepresentations, i.e., function gis symmetric. Similarly, CLSM [47] replaces the\nrepresentation functions  and\u001eby two identical convolutional neural networks\n(CNNs) in order to capture the local word order information. LSTM-RNN [48]\nreplaces and\u001eby two identical long short-term memory (LSTM) networks in\norder to capture the long-term dependence between words.\nSymmetric interaction networks , as shown by the name, employ a symmetric\ninteraction function to represent the inputs. Representative models include\nDeepMatch [14], Arc-II [17], MatchPyramid [18] and Match-SRNN [69]. For\nexample, Arc-II de\fnes an interaction function \u0011oversandtby computing\nsimilarity (i.e., weighted sum) between every n-gram pair from sandt, which is\nsymmetric in nature. After that, several convolutional and max-pooling layers\nare leveraged to obtain the \fnal relevance score, which is also symmetric over\nsandt. MatchPyramid de\fnes a symmetric interaction function \u0011between\nevery word pair from sandtto capture \fne-grained interaction signals. It\nthen leverages a symmetric evaluation function g, i.e., several 2D CNNs and a\ndynamic pooling layer, to produce the relevance score. A similar process can be\nfound in DeepMatch and Match-SRNN.\nSymmetric architectures, with the underlying homogeneous assumption, can\n\ft well with the CQA and AC tasks, where sandtusually have similar lengths\nand similar forms (i.e., both are natural language sentences). They may some-\ntimes work for the ad-hoc retrieval or QA tasks if one only uses document\ntitles/snippets [13] or short answer sentences [61] to reduce the heterogeneity\nbetween the two inputs.\nAsymmetric Architecture: The inputs sandtare assumed to be het-\nerogeneous, so that asymmetric network structures should be applied over the\ninputs. Note here asymmetric structure means if we change the position of the\ninputssandtin the input layer, we will obtain totally di\u000berent output. Asym-\nmetric architectures have been introduced mainly in the ad-hoc retrieval task\n[13, 33], due to the inherent heterogeneity between the query and the document\nas discussed in Section 2.1. Such structures may also work for the QA task\n12\n\nSTùëîs1s2s3ùúÇùúÇùúÇ(a) Query Split\nST1ùëîùúÇT2T3ùúÇùúÇ (b) Document Split\nSTùúôùúìùëîAttention (c) One-way Attention\nFigure 1: Three types of Asymmetric Architecture.\nwhere answer passages are ranked against natural language questions [84].\nHere we take the ad-hoc retrieval scenario as an example to analyze the\nasymmetric architecture. We \fnd there are three major strategies used in the\nasymmetric architecture to handle the heterogeneity between the query and the\ndocument, namely query split, document split, and joint split.\n\u000fQuery split is based on the assumption that most queries in ad-hoc re-\ntrieval are keyword based, so that we can split the query into terms to\nmatch against the document, as illustrated in Figure 1(a). A typical model\nbased on this strategy is DRMM [21]. DRMM splits the query into terms\nand de\fnes the interaction function \u0011as the matching histogram mapping\nbetween each query term and the document. The evaluation function g\nconsists of two parts, i.e., a feed-forward network for term-level relevance\ncomputation and a gating network for score aggregation. Obviously such\na process is asymmetric with respect to the query and the document. K-\nNRM [85] also belongs to this type of approach. It introduces a kernel\npooling function to approximate matching histogram mapping to enable\nend-to-end learning.\n\u000fDocument split is based on the assumption that a long document could\nbe partially relevant to a query under the scope hypothesis [2], so that we\n13\n\nsplit the document to capture \fne-grained interaction signals rather than\ntreat it as a whole, as depicted in Figure 1(b). A representative model\nbased on this strategy is HiNT [34]. In HiNT, the document is \frst split\ninto passages using a sliding window. The interaction function \u0011is de\fned\nas the cosine similarity and exact matching between the query and each\npassage. The evaluation function gincludes the local matching layers and\nglobal decision layers.\n\u000fJoint split , by its name, uses both assumptions of query split and doc-\nument split. A typical model based on this strategy is DeepRank [33].\nSpeci\fcally, DeepRank splits the document into term-centric contexts with\nrespect to each query term. It then de\fnes the interaction function \u0011be-\ntween the query and term-centric contexts in several ways. The evaluation\nfunctiongincludes three parts, i.e., term-level computation, term-level ag-\ngregation, and global aggregation. Similarly, PACRR [24] takes the query\nas a set of terms and splits the document using the sliding window as well\nas the \frst-k term window.\nIn addition, in neural ranking models applied for QA, there is another popu-\nlar strategy leading the asymmetric architecture. We name it one-way attention\nmechanism which typically leverages the question representation to obtain the\nattention over candidate answer words in order the enhance the answer repre-\nsentation, as illustrated in Figure 1(c). For example, IARNN [86] and Com-\npAgg [87] get the attentive answer representation sequence that weighted by\nthe question sentence representation.\n4.2. Representation-focused vs. Interaction-focused Architectures\nBased on di\u000berent assumptions over the features (extracted by the represen-\ntation function \u001e; or the interaction function \u0011) for relevance evaluation, we\ncan divide the existing neural ranking models into another two categories of ar-\nchitectures, namely representation-focused architecture and interaction-focused\narchitecture, as illustrated in Figure 2. Besides these two basic categories, some\n14\n\nSTùúôùúìùëî(a) Representation-focused\nSTùëîùúÇ (b) Interaction-focused\nFigure 2: Representation-focused and Interaction-focused Architectures.\nneural ranking models adopt a hybrid way to enjoy the merits of both architec-\ntures in learning relevance features.\nRepresentation-focused Architecture : The underlying assumption of\nthis type of architecture is that relevance depends on compositional meaning\nof the input texts. Therefore, models in this category usually de\fne complex\nrepresentation functions \u001eand (i.e., deep neural networks), but no interaction\nfunction\u0011, to obtain high-level representations of the inputs sandt, and uses\nsome simple evaluation function g(e.g. cosine function or MLP) to produce\nthe \fnal relevance score. Di\u000berent deep network structures have been applied\nfor\u001eand , including fully-connected networks, convolutional networks and\nrecurrent networks.\n\u000fTo our best knowledge, DSSM [13] is the only one that uses the fully-\nconnected network for the functions \u001eand , which has been described in\nSection 4.1.\n\u000fConvolutional networks have been used for \u001eand in Arc-I [17], CNTN [25]\nand CLSM [47]. Take Arc-I as an example, stacked 1D convolutional layers\nand max pooling layers are applied on the input texts sandtto produce\ntheir high-level representations respectively. Arc-I then concatenates the\ntwo representations and applies an MLP as the evaluation function g.\n15\n\nThe main di\u000berence between CNTN and Arc-I is the function g, where\nthe neural tensor layer is used instead of the MLP. The description on\nCLSM could be found in Section 4.1.\n\u000fRecurrent networks have been used for \u001eand in LSTM-RNN [48] and\nMV-LSTM [88]. LSTM-RNN uses a one-directional LSTM as \u001eand \nto encode the input texts, which has been described in Section 4.1. MV-\nLSTM employs a bi-directional LSTM instead to encode the input texts.\nThen, the top-k strong matching signals between the two high-level rep-\nresentations are fed to an MLP to generate the relevance score.\nBy evaluating relevance based on high-level representations of each input\ntext, representation-focused architecture better \fts tasks with the global match-\ning requirement [21]. This architecture is also more suitable for tasks with short\ninput texts (since it is often di\u000ecult to obtain good high-level representations\nof long texts). Tasks with these characteristics include CQA and AC as shown\nin Section 2. Moreover, models in this category are e\u000ecient for online computa-\ntion, since one can pre-calculate representations of the texts o\u000fine once \u001eand\n have been learned.\nInteraction-focused Architecture : The underlying assumption of this\ntype of architecture is that relevance is in essence about the relation between\nthe input texts, so it would be more e\u000bective to directly learn from interactions\nrather than from individual representations. Models in this category thus de-\n\fne the interaction function \u0011rather than the representation functions \u001eand\n , and use some complex evaluation function g(i.e., deep neural networks) to\nabstract the interaction and produce the relevance score. Di\u000berent interaction\nfunctions have been proposed in literature, which could be divided into two cate-\ngories, namely non-parametric interaction functions and parametric interaction\nfunctions.\n\u000fNon-parametric interaction functions are functions that re\rect the close-\nness or distance between inputs without learnable parameters. In this\n16\n\ncategory, some are de\fned over each pair of input word vectors, such as\nbinary indicator function [18, 33], cosine similarity function [18, 61, 33],\ndot-product function [18, 33, 34] and radial-basis function [18]. The oth-\ners are de\fned between a word vector and a set of word vectors, e.g. the\nmatching histogram mapping in DRMM [21] and the kernel pooling layer\nin K-NRM [85].\n\u000fParametric interaction functions are adopted to learn the similarity/distance\nfunction from data. For example, Arc-II [17] uses 1D convolutional layer\nfor the interaction bwteen two phrases. Match-SRNN [69] introduces the\nneural tensor layer to model complex interactions between input words.\nSome BERT-based model [89] takes attention as the interaction function\nto learn the interaction vector (i.e., [CLS] vector) between inputs. In gen-\neral, parametric interaction functions are adopted when there is su\u000ecient\ntraining data since they bring the model \rexibility at the expense of larger\nmodel complexity.\nBy evaluating relevance directly based on interactions, the interaction-focused\narchitecture can \ft most IR tasks in general. Moreover, by using detailed in-\nteraction signals rather than high-level representations of individual texts, this\narchitecture could better \ft tasks that call for speci\fc matching patterns (e.g.,\nexact word matching) and diverse matching requirement [21], e.g., ad-hoc re-\ntrieval. This architecture also better \ft tasks with heterogeneous inputs, e.g.,\nad-hoc retrieval and QA, since it circumvents the di\u000eculty of encoding long\ntexts. Unfortunately, models in this category are not e\u000ecient for online compu-\ntation as previous representation-focused models, since the interaction function\n\u0011cannot be pre-calculated until we see the input pair ( s;t). Therefore, a better\nway for practical usage is to apply these two types of models in a \\telescope\"\nsetting, where representation-focused models could be applied in an early search\nstage while interaction-focused models could be applied later on.\nIt is worth noting that parts of the interaction-focused architectures have\nsome connections to those in the computer vision (CV) area. For example, the\n17\n\ndesigns of MatchPyramid [18] and PACRR [24] are inspired by the neural models\nfor the image recognition task. By viewing the matching matrix as a 2-D image,\na CNN network is naturally applied to extract hierarchical matching patterns for\nrelevance estimation. These connections indicate that although neural ranking\nmodels are mostly applied over textual data, one may still borrow many useful\nideas in neural architecture design from other domains.\nHybrid Architecture : In order to take advantage of both representation-\nfocused and interaction-focused architectures, a natural way is to adopt a hybrid\narchitecture for feature learning. We \fnd that there are two major hybrid\nstrategies to integrate the two architectures, namely combined strategy and\ncoupled strategy.\n\u000fCombined strategy is a loose hybrid strategy, which simply adopts both\nrepresentation-focused and interaction-focused architectures as sub-models\nand combines their outputs for \fnal relevance estimation. A representa-\ntive model using this strategy is DUET [23]. DUET employs a CLSM-like\narchitecture (i.e., a distributed network) and a MatchPyramid-like archi-\ntecture (i.e., a local network) as two sub-models, and uses a sum operation\nto combine the scores from the two networks to produce the \fnal relevance\nscore.\n\u000fCoupled strategy, on the other hand, is a compact hybrid strategy. A typ-\nical way is to learn representations with attention across the two inputs.\nTherefore, the representation functions \u001eand and the interaction func-\ntion\u0011are compactly integrated. Representative models using this strategy\ninclude IARNN [86] and CompAgg [87], which have been discussed in the\nSection 4.1. Both models learn the question and answer representations\nvia some one-way attention mechanism.\n4.3. Single-granularity vs. Multi-granularity Architecture\nThe \fnal relevance score is produced by the evaluation function g, which\ntakes the features from \u001e, , and\u0011as input for estimation. Based on di\u000berent\n18\n\nùëîSTùúôùúìSTùëîùúÇ(a) Vertical Multi-granularity\nSTùëîùúôwordùúìwordùúôphraseùúìphraseùúôsentenceùúìsentence (b) Horizontal Multi-granularity\nFigure 3: Multi-granularity Architectures.\nassumptions on the estimation process for relevance, we can divide existing\nneural ranking models into two categories, namely single-granularity models\nand multi-granularity models.\nSingle-granularity Architecture : The underlying assumption of the single-\ngranularity architecture is that relevance can be evaluated based on the high-\nlevel features extracted by \u001e, and\u0011from the single-form text inputs. Under\nthis assumption, the representation functions \u001e, and the interaction function\n\u0011are actually viewed as black-boxes to the evaluation function g. Therefore, g\nonly takes their \fnal outputs for relevance computation. Meanwhile, the inputs\nsandtare simply viewed a set/sequence of words or word embeddings without\nany additional language structures.\nObviously, the assumption underlying the single-granularity architecture is\nvery simple and basic. Many neural ranking models fall in this category, with\neither symmetric (e.g., DSSM and MatchPyramid) or asymmetric (e.g., DRMM\nand HiNT) architectures, either representation-focused (e.g., ARC-I and MV-\nLSTM) or interaction-focused (e.g., K-NRM and Match-SRNN).\nMulti-granularity Architecture : The underlying assumption of the multi-\ngranularity architecture is that relevance estimation requires multiple granulari-\nties of features, either from di\u000berent-level feature abstraction or based on di\u000ber-\nent types of language units of the inputs. Under this assumption, the represen-\n19\n\ntation functions \u001e, and the interaction function \u0011are no longer black-boxes to\ng, and we consider the language structures in sandt. We can identify two ba-\nsic types of multi-granularity, namely vertical multi-granularity and horizontal\nmulti-granularity, as illustrated in Figure 3.\n\u000fVertical multi-granularity takes advantage of the hierarchical nature of\ndeep networks so that the evaluation function gcould leverage di\u000berent-\nlevel abstraction of features for relevance estimation. For example, In\nMultigranCNN [90], the representation functions  and\u001eare de\fned as\ntwo CNN networks to encode the input texts respectively, and the eval-\nuation function gtakes the output of each layer for relevance estimation.\nMACM [91] builds a CNN over the interaction matrix from \u0011, uses MLP\nto generate a layer-wise score for each abstraction level of the CNN, and\naggregates all the layers' scores for the \fnal relevance estimation. Similar\nideas can also be found in MP-HCNN [92] and MultiMatch [93].\n\u000fHorizontal multi-granularity is based on the assumption that language\nhas intrinsic structures (e.g., phrases or sentences), and we shall consider\ndi\u000berent types of language units, rather than simple words, as inputs for\nbetter relevance estimation. Models in this category typically enhance the\ninputs by extending it from words to phrases/n-grams or sentences, apply\ncertain single-granularity architectures over each input form, and aggre-\ngate all the granularity for \fnal relevance output. For example, in [94], a\nCNN and an LSTM are applied to obtain the character-level, word-level,\nand sentence-level representations of the inputs, and each level represen-\ntations are then interacted and aggregated by the evaluation function g\nto produce the \fnal relevance score. Similar ideas can be found in Conv-\nKNRM [84] and MIX [95].\nAs we can see, the multi-granularity architecture is a natural extension of the\nsingle-granularity architecture, which takes into account the inherent language\nstructures and network structures for enhanced relevance estimation. With\n20\n\nmulti-granularity features extracted, models in this category are expected to\nbetter \ft tasks that require \fne-grained matching signals for relevance compu-\ntation, e.g., ad-hoc retrieval [84] and QA [95]. However, the enhanced model\ncapability is often reached at the expense of larger model complexity.\n5. Model Learning\nBeyond the architecture, in this section, we review the major learning ob-\njectives and training strategies adopted by neural ranking models for compre-\nhensive understadning.\n5.1. Learning objective\nSimilar to other LTR algorithms, the learning objective of neural ranking\nmodels can be broadly categorized into three groups: pointwise ,pairwise , and\nlistwise . In this section, we introduce a couple of popular ranking loss functions\nin each group, and discuss their unique advantages and disadvantages for the\napplications of neural ranking models in di\u000berent IR tasks.\n5.1.1. Pointwise Ranking Objective\nThe idea of pointwise ranking objectives is to simplify a ranking problem to\na set of classi\fcation or regression problems. Speci\fcally, given a set of query-\ndocument pairs ( si;ti;j) and their corresponding relevance annotation yi;j, a\npointwise learning objective tries to optimize a ranking model by requiring it to\ndirectly predict yi;jfor (si;ti;j). In other words, the loss functions of pointwise\nlearning objectives are computed based on each ( s;t) pair independently. This\ncan be formulated as\nL(f;S;T;Y) =X\niX\njL(yi;j;f(si;ti;j)) (3)\nFor example, one of the most popular pointwise loss functions used in neural\nranking models is Cross Entropy :\nL(f;S;T;Y) =\u0000X\niX\njyi;jlog(f(si;ti;j)) + (1\u0000yi;j) log(1\u0000f(si;ti;j)) (4)\n21\n\nwhereyi;jis a binary label or annotation with probabilistic meanings (e.g.,\nclickthrough rate), and f(si;ti;j) needs to be rescaled into the range of 0 to 1\n(e.g., with a sigmoid function \u001b(x) =1\n1+exp(\u0000x)). Example applications include\nthe Convolutional Neural Network for question answering [19]. There are other\npointwise loss functions such as Mean Squared Error for numerical labels, but\nthey are more commonly used in recommendation tasks.\nThe advantages of pointwise ranking objectives are two-fold. First, pointwise\nranking objectives are computed based on each query-document pair ( si;ti;j)\nseparately, which makes it simple and easy to scale. Second, the outputs of\nneural models learned with pointwise loss functions often have real meanings\nand value in practice. For instance, in sponsored search, a model learned with\ncross entropy loss and clickthrough rates can directly predict the probability of\nuser clicks on search ads, which is more important than creating a good result\nlist in some application scenarios.\nIn general, however, pointwise ranking objectives are considered to be less\ne\u000bective in ranking tasks. Because pointwise loss functions consider no docu-\nment preference or order information, they do not guarantee to produce the best\nranking list when the model loss reaches the global minimum. Therefore, better\nranking paradigms that directly optimize document ranking based on pairwise\nloss functions and listwise loss functions have been proposed for LTR problems.\n5.1.2. Pairwise Ranking Objective\nPairwise ranking objectives focus on optimizing the relative preferences be-\ntween documents rather than their labels. In contrast to pointwise methods\nwhere the \fnal ranking loss is the sum of loss on each document, pairwise loss\nfunctions are computed based on the permutations of all possible document\npairs [96]. It usually can be formalized as\nL(f;S;T;Y) =X\niX\n(j;k);yi;j\u001fyi;kL(f(si;ti;j)\u0000f(si;ti;k)) (5)\nwhereti;jandti;kare two documents for query siandti;jis preferable comparing\ntoti;k(i.e.,yi;j\u001fyi;k). For instance, a well-known pairwise loss function is\n22\n\nHinge loss :\nL(f;S;T;Y) =X\niX\n(j;k);yi;j\u001fyi;kmax(0;1\u0000f(si;ti;j) +f(si;ti;k)) (6)\nHinge loss has been widely used in the training of neural ranking models such\nas DRMM [21] and K-NRM [85]. Another popular pairwise loss function is the\npairwise cross entropy de\fned as\nL(f;S;T;Y) =\u0000X\niX\n(j;k);yi;j\u001fyi;klog\u001b(f(si;ti;j)\u0000f(si;ti;k)) (7)\nwhere\u001b(x) =1\n1+exp(\u0000x). Pairwise cross entropy is \frst proposed in RankNet by\nBurges et al. [97], which is considered to be one of the initial studies on applying\nneural network techniques to ranking problems.\nIdeally, when pairwise ranking loss is minimized, all preference relationships\nbetween documents should be satis\fed and the model will produce the optimal\nresult list for each query. This makes pairwise ranking objectives e\u000bective in\nmany tasks where performance is evaluated based on the ranking of relevant\ndocuments. In practice, however, optimizing document preferences in pairwise\nmethods does not always lead to the improvement of \fnal ranking metrics due\nto two reasons: (1) it is impossible to develop a ranking model that can correctly\npredict document preferences in all cases; and (2) in the computation of most\nexisting ranking metrics, not all document pairs are equally important. This\nmeans that the performance of pairwise preference prediction is not equal to the\nperformance of the \fnal retrieval results as a list. Given this problem, previous\nstudies [98, 99, 100, 101] further proposed listwise ranking objectives for learning\nto rank.\n5.1.3. Listwise Ranking Objective\nThe idea of listwise ranking objectives is to construct loss functions that\ndirectly re\rect the model's \fnal performance in ranking. Instead of comparing\ntwo documents each time, listwise loss functions compute ranking loss with\neach query and their candidate document list together. Formally, most existing\n23\n\nlistwise loss functions can be formulated as\nL(f;S;T;Y) =X\niL(fyi;j;f(si;ti;j)jti;j2Tig) (8)\nwhereTiis the set of candidate documents for query si. Usually,Lis de\fned as\na function over the list of documents sorted by yi;j, which we refer to as \u0019i, and\nthe list of documents sorted by f(si;ti;j). For example, Xia et al. [98] proposed\nListMLE for listwise ranking as\nL(f;S;T;Y) =X\nij\u0019ijX\nj=1logP(yi;jjT(j)\ni;f) (9)\nwhereP(yi;jjT(j)\ni;f) is the probability of selecting the jth document in the\noptimal ranked list \u0019iwithf:\nP(yi;jjT(j)\ni;f) =exp(f(si;ti;j))\nPj\u0019ij\nk=jexp(f(si;ti;k))(10)\nIntuitively, ListMLE is the log likelihood of the optimal ranked list given the\ncurrent ranking function f, but computing log likelihood on all the result posi-\ntions is computationally prohibitive in practice. Thus, many alternative func-\ntions have been proposed for listwise ranking objectives in the past ten years.\nOne example is the Attention Rank function used in the Deep Listwise Context\nModel proposed by Ai et al. [101]:\nL(f;S;T;Y) =\u0000X\niX\njP(ti;jjYi;Ti) logP(ti;jjf;Ti)\nwhereP(ti;jjYi;Ti) =exp(yi;j)\nPjTij\nk=1exp(yi;k);\nP(ti;jjfi;Ti) =exp(f(si;ti;j))\nPjTij\nk=1exp(f(si;ti;k))(11)\nWhen the labels of documents (i.e., yi;j) are binary, we can further simplify the\nAttention Rank function with a softmax cross entropy function as\nL(f;S;T;Y) =\u0000X\niX\njyi;jlogexp(f(si;ti;j))\nPjTij\nk=1exp(f(si;ti;k))(12)\nThe softmax-based listwise ranking loss is one of the most popular learning\nobjectives for neural ranking models such as GSF [102]. It is particularly useful\n24\n\nwhen we train neural ranking models with user behavior data (e.g., clicks) under\nthe unbiased learning framework [103]. There are other types of listwise loss\nfunctions proposed under di\u000berent ranking frameworks in the literature [100, 99].\nWe ignore them in this paper since they are not popular in the studies of neural\nIR.\nWhile listwise ranking objectives are generally more e\u000bective than pairwise\nranking objectives, their high computational cost often limits their applications.\nThey are suitable for the re-ranking phase over a small set of candidate docu-\nments. Since many practical search systems now use neural models for docu-\nment re-ranking, listwise ranking objectives have become increasingly popular\nin neural ranking frameworks [13, 47, 23, 101, 102, 103].\n5.1.4. Multi-task Learning Objective\nIn some cases, the optimization of neural ranking models may include the\nlearning of multiple ranking or non-ranking objectives at the same time. The\nmotivation behind this approach is to use the information from one domain\nto help the understanding of information from other domains. For example,\nLiu et al. [104] proposed to unify the representation learning process for query\nclassi\fcation and Web search by training a deep neural network in which the\n\fnal layer of hidden variables are used to optimize both a classi\fcation loss\nand a ranking loss. Chapelle et al. [105] proposed a multi-boost algorithm to\nsimultaneously learn ranking functions based on search data collected from 15\ncountries.\nIn general, the most common methodology used by existing multi-task learn-\ning algorithms is to construct shared representations that are universally e\u000bec-\ntive for ranking in multiple tasks or domains. To do so, previous studies mostly\nfocus on constructing regularizations or restrictions on model optimizations so\nthat the \fnal model is not speci\fcally designed for a single ranking objec-\ntive [104, 105]. Inspired by recent advances on generative adversarial networks\n(GAN) [106], Cohen et al. [107] introduced an adversarial learning framework\nthat jointly learns a ranking function with a discriminator which can distin-\n25\n\nguish data from di\u000berent domains. By training the ranking function to produce\nrepresentations that cannot be discriminated by the discriminator, they teach\nthe ranking system to capture domain-independent patterns that are usable in\ncross-domain applications. This is important as it can signi\fcantly alleviate the\nproblem of data sparsity in speci\fc tasks and domains.\n5.2. Training Strategies\nGiven the data available for training a neural ranking model, an appropriate\ntraining strategy should be chosen. In this section, we brie\ry review a set\nof e\u000bective training strategies for neural ranking models, including supervised,\nsemi-supervised, and weakly supervised learning.\nSupervised learning refers to the most common learning strategy in which\nquery-document pairs are labeled. The data can be labeled by expert asses-\nsors, crowdsourcing, or can be collected from the user interactions with a search\nengine as implicit feedback. In this training strategy, it is assumed that a su\u000e-\ncient amount of labeled training data is available. Given this training strategy,\none can train the model using any of the aforementioned learning objectives,\ne.g., pointwise and pairwise. However, since neural ranking models are usually\ndata \\hungry\", academic researchers can only learn models with constrained\nparameter spaces under this training paradigm due to the limited annotated\ndata. This has motivated researchers to study learning from limited data for\ninformation retrieval [108].\nWeakly supervised learning refers to a learning strategy in which the query-\ndocument labels are automatically generated using an existing retrieval model,\nsuch as BM25. The use of pseudo-labels for training ranking models has been\nproposed by Asadi et al. [109]. More recently, Dehghani et al. [27] proposed\nto train neural ranking models using weak supervision and observed up to 35%\nimprovement compared to BM25 which plays the role of weak labeler. This\nlearning strategy does not require labeled training data. In addition to ranking,\nweak supervision has shown successful results in other information retrieval\ntasks, including query performance prediction [110], learning relevance-based\n26\n\nword embedding [111], and e\u000ecient learning to rank [112].\nSemi-supervised learning refers to a learning strategy that leverages a small\nset of labeled query-document pairs plus a large set of unlabeled data. Semi-\nsupervised learning has been extensively studied in the context of learning to\nrank. Preference regularization [113], feature extraction using KernelPCA [114],\nand pseudo-label generation using labeled data [115] are examples of such ap-\nproaches. In the realm of neural models, \fne-tuning weak supervision models\nusing a small set of labeled data [27] and controlling the learning rate in learning\nfrom weakly supervised data using a small set of labeled data [116] are another\nexample of semi-supervised approaches to ranking. Recently, Li et al. [117]\nproposed a neural model with a joint supervised and unsupervised loss func-\ntions. The supervised loss accounts for the error in query-document matching,\nwhile the unsupervised loss computes the document reconstruction error (i.e.,\nauto-encoders).\n6. Model Comparison\nIn this section, we compare the empirical evaluation results of the previously\nreviewed neural ranking models on several popular benchmark data sets. We\nmainly survey and analyze the published results of neural ranking models for\nthe ad-hoc retrieval and QA tasks. Note that sometimes it is di\u000ecult to com-\npare published results across di\u000berent papers - small changes such as di\u000berent\ntokenization, stemming, etc. can lead to signi\fcant di\u000berences. Therefore, we\nattempt to collect results from papers that contain comparisons across some of\nthese models performed at a single site for fairness .\n6.1. Empirical Comparison on Ad-hoc Retrieval\nTo better understand the performances of di\u000berent neural ranking models\non ad-hoc retrieval, we show the published experimental results on benchmark\ndatasets. Here, we choose three representative datasets for ad-hoc retrieval:\n(1) Robust04 dataset is a standard ad-hoc retrieval dataset where the queries\n27\n\nare from TREC Robust Track 2004. (2) Gov2 MQ2007 is an Web Track ad-\nhoc retrieval dataset where the collection is the Gov2 corpus. The queries are\nfrom the Million Query Track of TREC 2007. (3) Sougou-Log dataset [85]\nis built on query logs sampled from search logs of Sougou.com. (4) WT09-\n14 is the 2009-2014 TREC Web Track, which are based on the ClueWeb09\nand ClueWeb12 datasets. The detailed data statistics can be found in related\nliterature [21, 33, 34, 85, 118].\nFor meaningful comparison, we have tried our best to restrict the reported\nresults to be under the same experimental settings. Speci\fcally, experiments\non Robust04 take the title as the query, and all the documents are processed\nwith the Galago Search Engine10[21, 28]. For experiments on the Gov2 MQ2007\ndataset, all the queries and documents are processed using the Galago Search\nEngine under the same setting as described in [33, 34]. Besides, the results on\nthe WT09-14 dataset and the Sougou-Log dataset are all from a same paper\n[118, 84] respectively.\nTable 1 shows an overview of previous published results on ad-hoc retrieval\ndatasets. We have included some well-known probabilistic retrieval models,\npseudo-relevance feedback (PRF) models and LTR models as baselines. Based\non the results, we have the following observations:\n1. The probabilistic models (i.e., QL and BM25), although simple, can al-\nready achieve reasonably good performance. The traditional PRF model\n(i.e., RM3) and LTR models (i.e., RankSVM and LambdaMart) with hu-\nman designed features are strong baselines whose performance is hard to\nbeat for most neural ranking models based on raw texts. However, the\nPRF technique can also be leveraged to enhance neural ranking models\n(e.g., SNRM+PRF [28] and NPRF+DRMM [119] in Table 1), while hu-\nman designed LTR features can be integrated into neural ranking models\n[33, 31] to improve the ranking performance.\n10http://www.lemurproject.org/galago.php\n28\n\nTable 1: Overview of previously published results on ad hoc retrieval datasets. The citation in\neach row denotes the original paper where the method is proposed. The superscripts 1-6 denote\nthat the results are cited from [21],[33],[34],[118], [28], [119], [84] respectively. The subscripts\ndenote the model architecture belongs to (S)ymmetric or (A)symmetric/(R)epresentation-\nfocused or (I)nteraction-focused or (H)ybrid/Singe-(G)ranularity or (M)ulti-granularity. The\nback slash symbols denote that there are no published results for the speci\fc model on the\nspeci\fc data set in the related literature.\nModelData Set Robust04 GOV2 MQ2007 WT09-14 Sougo-Log\nMAP P@20 MAP P@10 ERR@20 NDCG@1\nBM25[46] (1994)1;20.255 0.370 0.450 0.366 n 0.142\nQL[120] (1998)1;40.253 0.369 n n 0.113 0.126\nRM3[121](2001)50.287 0.377 n n n n\nRankSVM[122] (2002)2n n 0.464 0.381 n 0.146\nLambdaMart[100] (2010)2n n 0.468 0.384 n n\nDSSM[13] (2013)1;2\nS=R=G0.095 0.171 0.409 0.352 n n\nCDSSM[47] (2014)1;2\nS=R=G0.067 0.125 0.364 0.291 n 0.144\nARC-I[17] (2014)1;2\nS=R=G0.041 0.065 0.417 0.364 n n\nARC-II[17] (2014)1;2\nS=I=G0.067 0.128 0.421 0.366 n n\nMP[18] (2016)1;2;4\nS=I=G0.189 0.290 0.434 0.371 0.148 0.218\nMatch-SRNN[69] (2016)2\nS=H=Gn n 0.456 0.384 n n\nDRMM[21] (2016)1;2;4\nA=I=G0.279 0.382 0.467 0.388 0.171 0.137\nDuet[23] (2017)3;4\nA=H=Gn n 0.474 0.398 0.134 n\nDeepRank[33] (2017)2\nA=I=Gn n 0.497 0.412 n n\nK-NRM[85] (2017)4\nA=I=Gn n n n 0.154 0.264\nPACRR[123] (2017)6;4\nA=I=M0.254 0.363 n n 0.191 n\nCo-PACRR[118] (2018)4\nA=I=Mn n n n 0.201 n\nSNRM[28] (2018)5\nS=R=G0.286 0.377 n n n n\nSNRM+PRF[28] (2018)5\nS=R=G0.297 0.395 n n n n\nCONV-KNRM[84] (2018)4\nA=I=Mn n n n n 0.336\nNPRF-KNRM[119] (2018)6\nA=I=G0.285 0.393 n n n n\nNPRF-DRMM[119] (2018)6\nA=I=G0.290 0.406 n n n n\nHiNT[34] (2018)3\nA=I=Gn n 0.502 0.418 n n\n29\n\n2. There seems to be a paradigm shift of the neural ranking model architec-\ntures from symmetric to asymmetric and from representation-focused to\ninteraction-focused over time. This is consistent with our previous anal-\nysis where asymmetric and interaction-focused structures may \ft better\nwith the ad-hoc retrieval task which shows heterogeneity inherently.\n3. With bigger data size in terms of distinct number of queries and labels\n(i.e., Sogou-Log\u001fGOV2MQ2007\u001fWT09-14\u001fRobust04), neural models\nare more likely to achieve larger performance improvement against non-\nneural models. As we can see, the best neural models based on raw texts\ncan signi\fcantly outperform LTR models with human designed features\non Sogou-Log dataset.\n4. Based on the reported results, in general, we observe that the asymmetric,\ninteraction-focused, multi-granularity architecture can work better than\nthe symmetric, representation-focused, single-granularity architecture on\nthe ad-hoc retrieval tasks. There is one exception, i.e., SNRM on Ro-\nbust04. However, this model was trained with a large amount of data\nusing the weak supervision strategy, and may not be appropriate to di-\nrectly compare with those models trained on Robust04 alone.\n6.2. Empirical Comparison on QA\nIn order to understand the performance of di\u000berent neural ranking models re-\nviewed in this paper for the QA task, we survey the previously published results\non three QA data sets, including TREC QA [124], WikiQA [37] and Yahoo! An-\nswers [88]. TREC QA and WikiQA are answer sentence selection/retrieval data\nsets and they mainly contain factoid questions, while Yahoo! Answers is an an-\nswer passage retrieval data set sampled from the CQA website Yahoo! Answers.\nThe detailed data statistics can be found in related literature [125, 37, 88].\nWe have tried our best to report results under the same experimental set-\ntings for fair comparison between di\u000berent methods. Speci\fcally, the results on\n30\n\nTREC QA are over the raw version of the data [126]11. WikiQA only has a\nsingle version with the same train/ valid/ test data partitions [37]. Yahoo An-\nswers data is the processed version from the same related work [88]. Therefore,\nquestions and answer candidates in all the train/valid/test sets used in di\u000berent\nsurveyed papers are the same, and the results are comparable with each other.\nTable 2 shows the overview of the published results on the QA benchmark\ndata sets. We include several traditional non-neural methods as baselines. We\nsummarize our observations as follows:\n1. Unlike ad-hoc retrieval, symmetric architectures have been more widely\nadopted in the QA tasks possibly due to the increased homogeneity be-\ntween the question and the answer, especially for answer sentence retrieval\ndata sets like TREC QA and WikiQA.\n2. Representation-focused architectures have been more adopted on short\nanswer sentence retrieval data sets, i.e., TREC QA and WikiQA, while\ninteraction-focused architectures have been more adopted on longer an-\nswer passage retrieval data sets, e.g., Yahoo! Answer. However, unlike ad-\nhoc retrieval, there seems to be no clear winner between the representation-\nfocused architecture and the interaction-focused architecture on QA tasks.\n3. Similar to ad-hoc retrieval, neural models are more likely to achieve larger\nperformance improvement against non-neural models on bigger data sets.\nFor example, on small data set like TREC QA, feature engineering based\nmethods such as LCLR can achieve very strong performance. However,\non large data set like WikiQA and Yahoo! Answers, we can see a clear\ngap between neural models and non-neural models.\n4. The performance in general increases over time, which might be due to\nthe increased model capacity as well as the adoption of some advanced\napproaches, e.g., the attention mechanism. For example, IARNN utilizes\nattention-based RNN models with GRU to get an attentive sentence rep-\nresentation. MIX extracts grammar information and integrates attention\n11https://aclweb.org/aclwiki/Question_Answering_(State_of_the_art)\n31\n\nTable 2: Overview of previously published results on QA benchmark data sets. The\ncitation in each row denotes the original paper where the method is proposed. The su-\nperscripts 1-10 denote that the results are cited from [37], [69], [61], [86], [88], [127],\n[87], [128], [125], [95] respectively. The subscripts denote the model architecture be-\nlongs to (S)ymmetric or (A)symmetric/(R)epresentation-focused or (I)nteraction-focused or\n(H)ybrid/Single-(G)ranularity or (M)ulti-granularity. The back slash symbols denote that\nthere are no published results for the speci\fc model on the speci\fc data set in the related\nliterature.\nData Set TREC QA WikiQA Yahoo! Answers\nModel MAP MRR MAP MRR P@1 MRR\nBM25[46] (1994)2n n n n 0.579 0.726\nLCLR[129] (2013)1;90.709 0.770 0.599 0.609 n n\nWord Cnt[125] (2014)1;90.571 0.627 0.489 0.492 n n\nWgt Word Cnt[125] (2014)1;90.596 0.652 0.510 0.513 n n\nDeepMatch[14] (2013)5\nS=I=Gn n n n 0.452 0.679\nCNN[125] (2014)1;9\nS=R=G0.569 0.661 0.619 0.628 n n\nCNN-Cnt[125] (2014)1;9\nS=R=G0.711 0.785 0.652 0.665 n n\nARC-I[17] (2014)2\nS=R=Gn n n n 0.581 0.756\nARC-II[17] (2014)2\nS=I=Gn n n n 0.591 0.765\nCDNN[19] (2015)3\nS=R=G0.746 0.808 n n n n\nBLSTM[60] (2015)3\nS=R=G0.713 0.791 n n n n\nCNTN[25] (2015)2;6\nS=R=G0.728 0.783 n n 0.626 0.781\nMultiGranCNN[90] (2015)2\nS=I=Mn n n n 0.725 0.840\nLSTM-RNN[48] (2016)2\nS=R=Gn n n n 0.690 0.822\nMV-LSTM[88] (2016)2;6\nS=R=G0.708 0.782 n n 0.766 0.869\nMatchPyramid[18] (2016)2\nS=I=Gn n n n 0.764 0.867\naNMM[61] (2016)3\nA=I=G0.750 0.811 n n n n\nMatch-SRNN[69] (2016)2\nS=I=Gn n n n 0.790 0.882\nIARNN[86] (2016)4\nA=H=Gn n 0.734 0.742 n n\nHD-LSTM[127] (2017)6\nS=R=G0.750 0.815 n n n n\nCompAgg[87] (2017)7\nA=I=Gn n 0.743 0.755 n n\nHyperQA[128] (2018)8\nS=R=G0.770 0.825 0.712 0.727 n n\nMIX[95] (2018)10\nS=I=Mn n 0.713 n n n\n32\n\nmatrices in the attention channels to encapsulate rich structural patterns.\naNMM adopts attention mechanism to encode question term importance\nfor aggregating interaction matching features.\n7. Trending Topics\nIn this section, we discuss several trending topics related to neural ranking\nmodels. Some of these topics are important but have not been well addressed\nin this \feld, while some are very promising directions for future research.\n7.1. Indexing: from Re-ranking to Ranking\nModern search engines take advantage of a multi-stage cascaded architecture\nin order to e\u000eciently provide accurate result lists to users. In more detail, there\ncan be a stack of rankers, starting from an e\u000ecient high-recall model. Learning\nto rank models are often employed to model the last stage ranker whose goal is\nto re-rank a small set of documents retrieved by the early stage rankers. The\nmain objective of these learning to rank models is to provide high-precision\nresults.\nSuch a multi-stage cascaded architecture su\u000bers from an error propagation\nproblem. In other words, the errors initiated by the early stage rankers are\npropagated to the last stage. This clearly shows that multi-stage systems are\nnot optimal. However, for e\u000eciency reasons, learning to rank models cannot be\nused as the sole ranker to retrieve from large collections, which is a disadvantage\nfor such models.\nTo address this issue, Zamani et al. [28] recently argued that the sparse\nnature of natural languages enables e\u000ecient term-matching retrieval models to\ntake advantage of an inverted index data structure for e\u000ecient retrieval. There-\nfore, they proposed a standalone neural ranking model (SNRM) that learns\nhigh-dimensional sparse representations for queries and documents. In more\ndetail, this type of model should optimize two objectives: ( i) a relevance ob-\njective that maximizes the e\u000bectiveness of the model in terms of the retrieval\n33\n\nperformance, and ( ii) a sparsity objective that is equivalent to minimizing L0\nof the query and document representations. SNRM has shown superior perfor-\nmance compared to competitive baselines and has performed as e\u000eciently as\nterm-matching models, such as TF-IDF and BM25.\nLearning inverted indexes has been also started to be explored in the database\ncommunity. Kraska et al. [130] recently proposed to look at indexes as models.\nFor example, a B-Tree-Index can be seen as a function that maps each key to a\nposition of record in a sorted list. They proposed to replace traditional indexes\nused in databases with the indexes learned using deep learning technologies.\nTheir models demonstrate a signi\fcant con\rict reduction and memory footprint\nimprovement.\nGraph-based hashing and indexing algorithms have also attracted a consid-\nerable attention, which could be leveraged to index neural representations for\nthe initial retrieval. For instance, Boytsov et al. [131] proposed to replace term-\nmatching retrieval models with approximate nearest neighbor algorithms. Van\nGysel et al. [132] used a similar idea to design an unsupervised neural retrieval\nmodel, however, their model architecture is not scalable to large document col-\nlections.\nMoving from re-ranking a small set of documents to retrieving documents\nfrom a large collection is a recent research direction with a number of unan-\nswered questions that require further investigation. For example, understand-\ning and interpreting the learned neural representations has yet to be addressed.\nFurthermore, there is a known trade-o\u000b between e\u000eciency and e\u000bectiveness in\ninformation retrieval systems, however, understanding this trade-o\u000b in learning\ninverted indexes requires further research. In addition, although index com-\npression is a common technique in the search engine industry to reduce the size\nof the posting lists and improve e\u000eciency, compression of the learned latent\nindexes is an unexplored area of research.\nIn summary, learning to index and developing e\u000bective and at the same time\ne\u000ecient retrieval models is a promising direction in neural IR research, however,\nwe still face several open questions in this area.\n34\n\n7.2. Learning with External Knowledge\nMost existing neural ranking models focus on learning the matching pat-\nterns between the two input texts. In recent years, some researchers have gone\nbeyond matching textual objects by leveraging external knowledge to enhance\nthe ranking performance. These research works can be grouped into two cate-\ngories: 1) learning with external structured knowledge such as knowledge bases\n[133, 29, 134, 135, 136, 137]; 2) learning with external unstructured knowledge\nsuch as retrieved top results, topics or tags [30, 138, 139]. We now brie\ry review\nthis work.\nThe \frst category of research explored improving neural ranking models\nwith semantic information from knowledge bases. Liu et al. [133] proposed\nEDRM that incorporates entities in interaction-focused neural ranking mod-\nels. EDRM \frst learns the distributed representations of entities using their\nsemantics from knowledge bases in descriptions and types. Then the model\nmatches documents to queries with both bag-of-words and bag-of-entities. Sim-\nilar approaches were proposed by Xiong et al. [29], which also models queries\nand documents with word-based representations and entity-based representa-\ntions. Nguyen et al. [134] proposed combining distributional semantics learned\nthrough neural networks and symbolic semantics held by extracted concepts\nor entities from text knowledge bases to enhance the learning algorithm of la-\ntent representations of queries and documents. Shen et al. [135] proposed the\nKABLSTM model, which leverages external knowledge from knowledge graphs\nto enrich the representational learning of QA sentences. Xu et al. [137] designed\na Recall gate, where domain knowledge can be transformed into the extra global\nmemory of LSTM, with the aim of enhancing LSTM by cooperating with its lo-\ncal memory to capture the implicit semantic relevance between sentences within\nconversations.\nBeyond structured knowledge in knowledge bases, other research has ex-\nplored how to integrate external knowledge from unstructured texts, which are\nmore common for information on the Web. Yang et al. [30] studied response\nranking in information-seeking conversations and proposed two e\u000bective meth-\n35\n\nods to incorporate external knowledge into neural ranking models with pseudo-\nrelevance feedback (PRF) and QA correspondence knowledge distillation. They\nproposed to extract the \\correspondence\" regularities between question and\nanswer terms from retrieved external QA pairs as external knowledge to help\nresponse selection. Another representative work on integrating unstructured\nknowledge into neural ranking models is the KEHNN model proposed by Wu et\nal. [139], which de\fned prior knowledge as topics, tags, and entities related to\nthe text pair. KEHNN represents global context obtained from external textual\ncollection, and then exploits a knowledge gate to fuse the semantic informa-\ntion carried by the prior knowledge into the representation of words. Finally, it\ngenerates a knowledge enhanced representation for each word to construct the\ninteraction matrix between text pairs.\nIn summary, learning with external knowledge is an active research area\nrelated to neural ranking models. More research e\u000borts are needed to improve\nthe e\u000bectiveness of neural ranking models with distilled external knowledge and\nto understand the role of external knowledge in ranking tasks.\n7.3. Learning with Visualized Technology\nWe have discussed many neural ranking models in this survey under the\ntextual IR scenario. There have also been a few studies showing that the textual\nIR problem could be solved visually. The key idea is that we can construct the\nmatching between two inputs as an image so that we can leverage deep neural\nmodels to estimate the relevance based on visual features. The advantage of\nthe matching image, compared with traditional matching matrix, is that it can\nkeep the layout information of the original inputs so that many useful features\nsuch as spatial proximity, font size and colors could be modeled for relevance\nestimation. This is especially useful when we consider ad-hoc retrieval tasks\non the Web where pages are often well designed documents with rich layout\ninformation.\nSpeci\fcally, Fan et al. [31] proposed a visual perception model (ViP) to per-\nceive visual features for relevance estimation. They \frst rendered the Web pages\n36\n\ninto query-independent snapshots and query-dependent snapshots. Then, the\nvisual features are learned through a combination of CNN and LSTM, inspired\nby users' reading behaviour. The results have demonstrated the e\u000bectiveness of\nlearning the visual features of document for ranking problems. Zhang et al. [140]\nproposed a joint relevance estimation model which learns visual patterns, textual\nsemantics and presentation structures jointly from screenshots, titles, snippets\nand HTML source codes of search results. Their results have demonstrated\nthe viability of the visual features in search result page relevance estimation.\nRecently, Akker et al. [141] built a dataset for the LTR task with visual fea-\ntures, named Visual learning TO Rank (ViTOR). The ViTOR dataset consists\nof visual snapshots, non-visual features and relevance judgments for ClueWeb12\nwebpages and TREC Web Track queries. Their results have demonstrated that\nvisual features can signi\fcantly improve the LTR performance.\nIn summary, solving the textual ranking problem through visualized technol-\nogy is a novel and interesting direction. In some sense, this approach simulates\nhuman behavior as we also judge relevance through visual perception. The ex-\nisting work has only demonstrated the e\u000bectiveness of visual features in some\nrelevance assessment tasks. However, more research is needed to understand\nwhat can be learned by such visualized technology beyond those text-based\nmethods, and what IR applications could bene\ft from such models.\n7.4. Learning with Context\nSearch queries are often short and cannot precisely express the underlying\ninformation needs. To address this issue, a common strategy is to exploit query\ncontext to improve the retrieval performance. Di\u000berent types of query context\nhave been explored in the literature:\n\u000fShort-term history: the user past interactions with the system in the\ncurrent search session [142, 143, 144].\n\u000fLong-term history: the historical information of the user's queries that is\noften used for web search personalization [145, 146].\n37\n\n\u000fSituational context: the properties of the current search request, indepen-\ndent from the query content, such as location and time [147, 148].\n\u000f(Pseudo-) relevance feedback: explicit, implicit, or pseudo relevance sig-\nnals for a given query can be used as the query context to improve the\nretrieval performance.\nAlthough query context has been widely explored in the literature, incor-\nporating query context into neural ranking models is relatively less studied.\nZamani et al. [148] proposed a deep and wide network architecture in which\nthe deep part of the model learns abstract representations for contextual fea-\ntures, while the wide part of the model uses raw contextual features in binary\nformat in order to avoid information loss as a result of high-level abstraction.\nAhmad et al. [149] incorporated short-term history information into a neural\nranking model by multi-task training of document ranking and query sugges-\ntion. Short- and long-term history have been also used by Chen et al. [150] for\nquery suggestion.\nIn addition, learning high-dimensional representation for pseudo-relevance\nfeedback has been also studied in the literature. In this area, embedding-based\nrelevance models [151] extend the original relevance models [121] by consider-\ning word embedding vectors. The word embedding vectors can be obtained\nfrom self-supervised algorithms, such as word2vec [152], or weakly supervised\nalgorithms, such as relevance-based word embedding [111]. Zamani et al. [153]\nproposed RFMF, the \frst pseudo-relevance feedback model that learns latent\nfactors from the top retrieved document. RFMF uses non-negative matrix\nfactorization for learning latent representations for words, queries, and docu-\nments. Later on, Li et al. [119] extended existing neural ranking models, e.g.,\nDRMM [21] and KNRM [85], by a neural pseudo-relevance feedback approach,\ncalled NPRF. The authors showed that in many cases extending a neural rank-\ning model with NPRF leads to signi\fcant improvements. Zamani et al. [28] also\nmade a similar conclusion by extending SNRM with pseudo-relevance feedback.\nIn summary, with the emergence of interactive or conversational search sys-\n38\n\ntem, context-aware ranking would be an indispensable technology in these sce-\nnarios. These exist several open research questions on how to incorporate query\ncontext information in neural ranking models. More research work is expected\nin this direction in the short future.\n7.5. Neural Ranking Model Understanding\nDeep learning techniques have been widely criticized as a \\black box\" which\nproduces good results but no problem insights and explanations. Thus, how\nto understand and explain neural models has been an important topic in both\nMachine Learning and IR communities. To the best of our knowledge, the\nexplainability of neural ranking models has not been fully studied. Instead,\nthere have been a few papers on analyzing and understanding the empirical\ne\u000bect of di\u000berent model components in IR tasks.\nFor example, Pang et al. [154] conducted an extensive analysis on the Match-\nPyramid model in ad-hoc retrieval and compared di\u000berent kernals, pooling sizes,\nand similarity functions in terms of retrieval performance. Cohen et al. [155]\nextracted the internal representations of neural ranking models and evaluated\ntheir e\u000bectiveness in four natural language processing tasks. They \fnd that top-\nical relevance information is usually captured in the high-level layers of a neural\nmodel. Nie et al. [156] conducted empirical studies on the interaction-based neu-\nral ranking model to understand what have been learned in each neural network\nlayer. They also notice that low-level network layers tend to capture detailed\ntext information while high-level layers tend to have higher topical information\nabstraction.\nWhile the paradigms of analyzing neural ranking models often rely on a deep\nunderstanding of speci\fc model structure, Cohen et al. [22] argue that there are\nsome general patterns of which types of neural models are more suitable for each\nIR task. For example, retrieval tasks with \fne granularity (e.g., factoid QA)\nusually need higher levels of information abstraction and semantic matching,\nwhile retrieval tasks with coarse granularity (e.g., document retrieval) often\nrely on the exact matching or interaction between query words and document\n39\n\nwords.\nOverall, the research area on the explainability of neural ranking models is\nlargely unexplored up till now. Some skepticism about neural ranking models\nis also related to this, e.g., what new things can be learned by neural ranking\nmodels? It is a very challenging and promising direction for researchers in neural\nIR.\n8. Conclusion\nThe purpose of this survey is to summarize the current research status on\nneural ranking models, analyze the existing methodologies, and gain some in-\nsights for future development. We introduced a uni\fed formulation over the\nneural ranking models, and reviewed existing models based on this formulation\nfrom di\u000berent dimensions under model architecture and model learning. For\nmodel architecture analysis, we reviewed existing models to understand their\nunderlying assumptions and major design principles, including how to treat the\ninputs, how to consider the relevance features, and how to make evaluation. For\nmodel learning analysis, we reviewed popular learning objectives and training\nstrategies adopted for neural ranking models. To better understand the current\nstatus of neural ranking models on major applications, we surveyed published\nempirical results on the ad-hoc retrieval and QA tasks to conduct a compre-\nhensive comparison. In addition, we discussed several trending topics that are\nimportant or might be promising in the future.\nJust as there has been an explosion in the development of many deep learning\nbased methods, research on neural ranking models has increased rapidly and\nbroadened in terms of applications. We hope this survey can help researchers\nwho are interested in this direction, and will motivate new ideas by looking\nat past successes and failures. Neural ranking models are part of the broader\nresearch \feld of neural IR, which is a joint domain of deep learning and IR\ntechnologies with many opportunities for new research and applications. We are\nexpecting that, through the e\u000borts of the community, signi\fcant breakthroughs\n40\n\nwill be achieved in this domain in the near future, similar to those happened in\ncomputer vision or NLP.\n9. Acknowledgments\nThis work was funded by the National Natural Science Foundation of China\n(NSFC) under Grants No. 61425016 and 61722211, and the Youth Innova-\ntion Promotion Association CAS under Grants No. 20144310. This work was\nsupported in part by the UMass Amherst Center for Intelligent Information Re-\ntrieval and in part by NSF IIS-1715095. Any opinions, \fndings and conclusions\nor recommendations expressed in this material are those of the authors and do\nnot necessarily re\rect those of the sponsor.\nReferences\n[1] G. Salton, A. Wong, C. S. Yang, A vector space model for automatic\nindexing, Commun. ACM 18 (11) (1975) 613{620.\n[2] S. E. Robertson, K. S. Jones, Relevance weighting of search terms, Journal\nof the American Society for Information science 27 (3) (1976) 129{146.\n[3] T.-Y. Liu, Learning to rank for information retrieval, Found. Trends Inf.\nRetr. 3 (3) (2009) 225{331.\n[4] H. Li, Learning to Rank for Information Retrieval and Natural Language\nProcessing, Morgan & Claypool Publishers, 2011.\n[5] G. Hinton, L. Deng, D. Yu, G. Dahl, A.-r. Mohamed, N. Jaitly, A. Se-\nnior, V. Vanhoucke, P. Nguyen, B. Kingsbury, T. Sainath, Deep neural\nnetworks for acoustic modeling in speech recognition, IEEE Signal Pro-\ncessing Magazine 29 (2012) 82{97.\n[6] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classi\fcation with\ndeep convolutional neural networks, in: Advances in Neural Information\nProcessing Systems 25, Curran Associates, Inc., 2012, pp. 1097{1105.\n41\n\n[7] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (2015) 436\nEP {.\n[8] Y. Goldberg, Neural network methods for natural language processing,\nSynthesis Lectures on Human Language Technologies 10 (1) (2017) 1{309.\n[9] D. Bahdanau, K. Cho, Y. Bengio, Neural machine translation by jointly\nlearning to align and translate, arXiv preprint arXiv:1409.0473.\n[10] N. Craswell, W. B. Croft, J. Guo, B. Mitra, M. de Rijke, Report on the\nsigir 2016 workshop on neural information retrieval (neu-ir), SIGIR Forum\n50 (2) (2017) 96{103.\n[11] J. Wan, D. Wang, S. C. H. Hoi, P. Wu, J. Zhu, Y. Zhang, J. Li, Deep\nlearning for content-based image retrieval: A comprehensive study, in:\nProceedings of the 22Nd ACM International Conference on Multimedia,\nMM '14, ACM, New York, NY, USA, 2014, pp. 157{166.\n[12] E. Brenner, J. Zhao, A. Kutiyanawala, Z. Yan, End-to-end neural ranking\nfor ecommerce product search: an application of task models and textual\nembeddings, arXiv preprint arXiv:1806.07296.\n[13] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, L. Heck, Learning deep\nstructured semantic models for web search using clickthrough data, in:\nProceedings of the 22Nd ACM International Conference on Information\n& Knowledge Management, CIKM '13, ACM, New York, NY, USA, 2013,\npp. 2333{2338.\n[14] Z. Lu, H. Li, A deep architecture for matching short texts, in: Advances in\nNeural Information Processing Systems 26, Curran Associates, Inc., 2013,\npp. 1367{1375.\n[15] R. Salakhutdinov, G. Hinton, Semantic hashing, International Journal of\nApproximate Reasoning 50 (7) (2009) 969{978.\n42\n\n[16] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, J. Dean, Distributed\nrepresentations of words and phrases and their compositionality, in: Ad-\nvances in Neural Information Processing Systems 26, Curran Associates,\nInc., 2013, pp. 3111{3119.\n[17] B. Hu, Z. Lu, H. Li, Q. Chen, Convolutional neural network architectures\nfor matching natural language sentences, in: Advances in Neural Informa-\ntion Processing Systems 27, Curran Associates, Inc., 2014, pp. 2042{2050.\n[18] L. Pang, Y. Lan, J. Guo, J. Xu, S. Wan, X. Cheng, Text matching as im-\nage recognition, in: Thirtieth AAAI Conference on Arti\fcial Intelligence,\n2016.\n[19] A. Severyn, A. Moschitti, Learning to rank short text pairs with convo-\nlutional deep neural networks, in: Proceedings of the 38th International\nACM SIGIR Conference on Research and Development in Information\nRetrieval, SIGIR '15, ACM, New York, NY, USA, 2015, pp. 373{382.\n[20] K. D. Onal, Y. Zhang, I. S. Altingovde, M. M. Rahman, P. Karagoz,\nA. Braylan, B. Dang, H.-L. Chang, H. Kim, Q. Mcnamara, A. Angert,\nE. Banner, V. Khetan, T. Mcdonnell, A. T. Nguyen, D. Xu, B. C. Wallace,\nM. Rijke, M. Lease, Neural information retrieval: At the end of the early\nyears, Inf. Retr. 21 (2-3) (2018) 111{182.\n[21] J. Guo, Y. Fan, Q. Ai, W. B. Croft, A deep relevance matching model for\nad-hoc retrieval, in: Proceedings of the 25th ACM International on Con-\nference on Information and Knowledge Management, CIKM '16, ACM,\nNew York, NY, USA, 2016, pp. 55{64.\n[22] D. Cohen, Q. Ai, W. B. Croft, Adaptability of neural networks on varying\ngranularity ir tasks, arXiv preprint arXiv:1606.07565.\n[23] B. Mitra, F. Diaz, N. Craswell, Learning to match using local and dis-\ntributed representations of text for web search, in: Proceedings of the\n43\n\n26th International Conference on World Wide Web, WWW '17, Interna-\ntional World Wide Web Conferences Steering Committee, Republic and\nCanton of Geneva, Switzerland, 2017, pp. 1291{1299.\n[24] K. Hui, A. Yates, K. Berberich, G. de Melo, Pacrr: A position-aware\nneural ir model for relevance matching, arXiv preprint arXiv:1704.03940.\n[25] X. Qiu, X. Huang, Convolutional neural tensor network architecture for\ncommunity-based question answering, in: Proceedings of the 24th Interna-\ntional Conference on Arti\fcial Intelligence, IJCAI'15, AAAI Press, 2015,\npp. 1305{1311.\n[26] R. Yan, Y. Song, H. Wu, Learning to respond with deep neural networks\nfor retrieval-based human-computer conversation system, in: SIGIR, 2016.\n[27] M. Dehghani, H. Zamani, A. Severyn, J. Kamps, W. B. Croft, Neural\nranking models with weak supervision, in: Proceedings of the 40th In-\nternational ACM SIGIR Conference on Research and Development in In-\nformation Retrieval, SIGIR '17, ACM, New York, NY, USA, 2017, pp.\n65{74.\n[28] H. Zamani, M. Dehghani, W. B. Croft, E. Learned-Miller, J. Kamps, From\nneural re-ranking to neural ranking: Learning a sparse representation for\ninverted indexing, in: Proceedings of the 27th ACM International Confer-\nence on Information and Knowledge Management, CIKM '18, ACM, New\nYork, NY, USA, 2018, pp. 497{506.\n[29] C. Xiong, J. Callan, T.-Y. Liu, Word-entity duet representations for doc-\nument ranking, in: Proceedings of the 40th International ACM SIGIR\nConference on Research and Development in Information Retrieval, SI-\nGIR '17, ACM, New York, NY, USA, 2017, pp. 763{772.\n[30] L. Yang, M. Qiu, C. Qu, J. Guo, Y. Zhang, W. B. Croft, J. Huang,\nH. Chen, Response ranking with deep matching networks and external\n44\n\nknowledge in information-seeking conversation systems, in: The 41st In-\nternational ACM SIGIR Conference on Research & Development in In-\nformation Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018,\n2018, pp. 245{254.\n[31] Y. Fan, J. Guo, Y. Lan, J. Xu, L. Pang, X. Cheng, Learning visual fea-\ntures from snapshots for web search, in: Proceedings of the 2017 ACM on\nConference on Information and Knowledge Management, ACM, 2017, pp.\n247{256.\n[32] Z. Tang, G. H. Yang, Deeptilebars: Visualizing term distribution for neu-\nral information retrieval, arXiv preprint arXiv:1811.00606.\n[33] L. Pang, Y. Lan, J. Guo, J. Xu, J. Xu, X. Cheng, Deeprank: A new\ndeep architecture for relevance ranking in information retrieval, in: Pro-\nceedings of the 2017 ACM on Conference on Information and Knowledge\nManagement, CIKM '17, ACM, New York, NY, USA, 2017, pp. 257{266.\n[34] Y. Fan, J. Guo, Y. Lan, J. Xu, C. Zhai, X. Cheng, Modeling diverse\nrelevance patterns in ad-hoc retrieval, in: The 41st International ACM\nSIGIR Conference on Research & Development in Information Retrieval,\nSIGIR '18, ACM, New York, NY, USA, 2018, pp. 375{384.\n[35] N. Craswell, W. B. Croft, M. de Rijke, J. Guo, B. Mitra, Sigir 2017\nworkshop on neural information retrieval (neu-ir'17), in: Proceedings of\nthe 40th International ACM SIGIR Conference on Research and Devel-\nopment in Information Retrieval, SIGIR '17, ACM, New York, NY, USA,\n2017, pp. 1431{1432.\n[36] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder,\nL. Deng, MS MARCO: A human generated machine reading comprehen-\nsion dataset, CoRR abs/1611.09268. arXiv:1611.09268 .\n[37] Y. Yang, S. W.-t. Yih, C. Meek, Wikiqa: A challenge dataset for open-\ndomain question answering, proceedings of the 2015 conference on empir-\n45\n\nical methods in natural language processing Edition, ACL - Association\nfor Computational Linguistics, 2015.\n[38] L. Dietz, M. Verma, F. Radlinski, N. Craswell, TREC complex answer\nretrieval overview, in: Proceedings of The Twenty-Sixth Text REtrieval\nConference, TREC 2017, Gaithersburg, Maryland, USA, November 15-17,\n2017, 2017.\n[39] Y. Fan, L. Pang, J. Hou, J. Guo, Y. Lan, X. Cheng, Matchzoo: A toolkit\nfor deep text matching, arXiv preprint arXiv:1707.07270.\n[40] X. He, J. Gao, L. Deng, Deep learning for natural language processing:\nTheory and practice (tutorial), 2014.\n[41] B. Mitra, N. Craswell, Neural models for information retrieval, arXiv\npreprint arXiv:1705.01509.\n[42] M. Grbovic, N. Djuric, V. Radosavljevic, F. Silvestri, N. Bhamidipati,\nContext- and content-aware embeddings for query rewriting in sponsored\nsearch, in: Proceedings of the 38th International ACM SIGIR Conference\non Research and Development in Information Retrieval, SIGIR '15, ACM,\nNew York, NY, USA, 2015, pp. 383{392.\n[43] R. Baeza-Yates, B. d. A. N. Ribeiro, et al., Modern information retrieval,\nNew York: ACM Press; Harlow, England: Addison-Wesley,, 2011.\n[44] G. W. Furnas, T. K. Landauer, L. M. Gomez, S. T. Dumais, The vocab-\nulary problem in human-system communication, Commun. ACM 30 (11)\n(1987) 964{971.\n[45] L. Zhao, J. Callan, Term necessity prediction, in: Proceedings of the 19th\nACM International Conference on Information and Knowledge Manage-\nment, CIKM '10, ACM, New York, NY, USA, 2010, pp. 259{268.\n[46] S. Robertson, S. Walker, Some simple e\u000bective approximations to the 2-\npoisson model for probabilistic weighted retrieval, in: SIGIR '94, 1994.\n46\n\n[47] Y. Shen, X. He, J. Gao, L. Deng, G. Mesnil, A latent semantic model with\nconvolutional-pooling structure for information retrieval, in: Proceedings\nof the 23rd ACM International Conference on Conference on Information\nand Knowledge Management, CIKM '14, ACM, New York, NY, USA,\n2014, pp. 101{110.\n[48] H. Palangi, L. Deng, Y. Shen, J. Gao, X. He, J. Chen, X. Song, R. Ward,\nDeep sentence embedding using long short-term memory networks: Anal-\nysis and application to information retrieval, IEEE/ACM Trans. Audio,\nSpeech and Lang. Proc. 24 (4) (2016) 694{707.\n[49] Y. Zheng, Z. Fan, Y. Liu, C. Luo, M. Zhang, S. Ma, Sogou-qcl: A new\ndataset with click relevance label, in: The 41st International ACM SIGIR\nConference on Research & Development in Information Retrieval, ACM,\n2018, pp. 1117{1120.\n[50] D. Moll\u0013 a, J. L. Vicedo, Question answering in restricted domains: An\noverview, Computational Linguistics 33 (1) (2007) 41{61.\n[51] A. Moschitti, L. M\u0013 arquez, P. Nakov, E. Agichtein, C. Clarke, I. Szpektor,\nSigir 2016 workshop webqa ii: Web question answering beyond factoids, in:\nProceedings of the 39th International ACM SIGIR Conference on Research\nand Development in Information Retrieval, SIGIR '16, ACM, New York,\nNY, USA, 2016, pp. 1251{1252.\n[52] M. Richardson, Mctest: A challenge dataset for the open-domain machine\ncomprehension of text, proceedings of the 2013 conference on empirical\nmethods in natural language processing (emnlp 2013) Edition, 2013.\n[53] E. M. Voorhees, D. M. Tice, Building a question answering test collection,\nin: Proceedings of the 23rd Annual International ACM SIGIR Conference\non Research and Development in Information Retrieval, SIGIR '00, ACM,\nNew York, NY, USA, 2000, pp. 200{207.\n47\n\n[54] P. Rajpurkar, J. Zhang, K. Lopyrev, P. Liang, Squad: 100, 000+ questions\nfor machine comprehension of text, CoRR abs/1606.05250. arXiv:1606.\n05250 .\n[55] B. Mitra, G. Simon, J. Gao, N. Craswell, L. Deng, A proposal for evalu-\nating answer distillation from web data.\n[56] D. Cohen, L. Yang, W. B. Croft, Wikipassageqa: A benchmark collec-\ntion for research on non-factoid answer passage retrieval, in: The 41st\nInternational ACM SIGIR Conference on Research & Development in In-\nformation Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018,\n2018, pp. 1165{1168.\n[57] M. Keikha, J. H. Park, W. B. Croft, Evaluating answer passages using\nsummarization measures, in: Proceedings of the 37th International ACM\nSIGIR Conference on Research &#38; Development in Information Re-\ntrieval, SIGIR '14, ACM, New York, NY, USA, 2014, pp. 963{966.\n[58] L. Yang, Q. Ai, D. Spina, R. Chen, L. Pang, W. B. Croft, J. Guo, F. Sc-\nholer, Beyond factoid QA: e\u000bective methods for non-factoid answer sen-\ntence retrieval, in: Advances in Information Retrieval - 38th European\nConference on IR Research, ECIR 2016, Padua, Italy, March 20-23, 2016.\nProceedings, 2016, pp. 115{128.\n[59] M. Feng, B. Xiang, M. R. Glass, L. Wang, B. Zhou, Applying deep learning\nto answer selection: A study and an open task, CoRR abs/1508.01585.\narXiv:1508.01585 .\n[60] D. Wang, E. Nyberg, A long short-term memory model for answer sen-\ntence selection in question answering, in: Proceedings of the 53rd Annual\nMeeting of the Association for Computational Linguistics and the 7th In-\nternational Joint Conference on Natural Language Processing, Association\nfor Computational Linguistics, 2015, pp. 707{712.\n48\n\n[61] L. Yang, Q. Ai, J. Guo, W. B. Croft, anmm: Ranking short answer texts\nwith attention-based neural matching model, in: Proceedings of the 25th\nACM International Conference on Information and Knowledge Manage-\nment, CIKM 2016, Indianapolis, IN, USA, October 24-28, 2016, 2016, pp.\n287{296.\n[62] L. Yang, M. Qiu, S. Gottipati, F. Zhu, J. Jiang, H. Sun, Z. Chen, Cqarank:\nJointly model topics and expertise in community question answering, in:\nProceedings of the 22Nd ACM International Conference on Information\n& Knowledge Management, CIKM '13, ACM, New York, NY, USA, 2013,\npp. 99{108.\n[63] A. Shtok, G. Dror, Y. Maarek, I. Szpektor, Learning from the past: An-\nswering new questions with past answers, in: Proceedings of the 21st\nInternational Conference on World Wide Web, WWW '12, ACM, New\nYork, NY, USA, 2012, pp. 759{768.\n[64] P. Nakov, D. Hoogeveen, L. M\u0012 arquez, A. Moschitti, H. Mubarak, T. Bald-\nwin, K. Verspoor, SemEval-2017 task 3: Community question answering,\nin: Proceedings of the 11th International Workshop on Semantic Evalua-\ntion, SemEval '17, Association for Computational Linguistics, Vancouver,\nCanada, 2017.\n[65] D. Hoogeveen, K. M. Verspoor, T. Baldwin, Cqadupstack: A benchmark\ndata set for community question-answering research, in: Proceedings of\nthe 20th Australasian Document Computing Symposium, ACM, 2015,\np. 3.\n[66] A. Abujabal, R. S. Roy, M. Yahya, G. Weikum, Comqa: A community-\nsourced dataset for complex factoid question answering with paraphrase\nclusters, arXiv preprint arXiv:1809.09528.\n[67] X. Liu, C. Wang, Y. Leng, C. Zhai, Linkso: a dataset for learning to\nretrieve similar question answer pairs on software development forums,\n49\n\nin: Proceedings of the 4th ACM SIGSOFT International Workshop on\nNLP for Software Engineering, ACM, 2018, pp. 2{5.\n[68] Z. Wang, W. Hamza, R. Florian, Bilateral multi-perspective matching\nfor natural language sentences, in: Proceedings of the 26th International\nJoint Conference on Arti\fcial Intelligence, IJCAI'17, AAAI Press, 2017,\npp. 4144{4150.\n[69] S. Wan, Y. Lan, J. Xu, J. Guo, L. Pang, X. Cheng, Match-srnn: Model-\ning the recursive matching structure with spatial rnn, in: Proceedings of\nthe Twenty-Fifth International Joint Conference on Arti\fcial Intelligence,\nIJCAI'16, AAAI Press, 2016, pp. 2922{2928.\n[70] L. Chen, Y. Lan, L. Pang, J. Guo, J. Xu, X. Cheng, Ri-match: Integrating\nboth representations and interactions for deep semantic matching, in: In-\nformation Retrieval Technology, Springer International Publishing, Cham,\n2018, pp. 90{102.\n[71] J. Gao, M. Galley, L. Li, Neural approaches to conversational AI, CoRR\nabs/1809.08267. arXiv:1809.08267 .\n[72] Z. Ji, Z. Lu, H. Li, An information retrieval approach to short text con-\nversation, CoRR abs/1408.6988.\n[73] A. Ritter, C. Cherry, W. B. Dolan, Data-driven response generation in\nsocial media, in: Proceedings of the Conference on Empirical Methods\nin Natural Language Processing, EMNLP '11, Association for Computa-\ntional Linguistics, Stroudsburg, PA, USA, 2011, pp. 583{593.\n[74] H. Wang, Z. Lu, H. Li, E. Chen, A dataset for research on short-text con-\nversations, in: Proceedings of the 2013 Conference on Empirical Methods\nin Natural Language Processing, 2013, pp. 935{945.\n[75] Y. Wu, W. Wu, C. Xing, M. Zhou, Z. Li, Sequential matching network:\nA new architecture for multi-turn response selection in retrieval-based\nchatbots, in: ACL '17, 2017.\n50\n\n[76] R. Lowe, N. Pow, I. Serban, J. Pineau, The ubuntu dialogue corpus: A\nlarge dataset for research in unstructured multi-turn dialogue systems,\nCoRR abs/1506.08909.\n[77] X. Zhou, D. Dong, H. Wu, S. Zhao, D. Yu, H. Tian, X. Liu, R. Yan, Multi-\nview response selection for human-computer conversation, in: EMNLP,\n2016.\n[78] L. Yang, H. Zamani, Y. Zhang, J. Guo, W. B. Croft, Neural matching\nmodels for question retrieval and next question prediction in conversation,\nCoRR.\n[79] R. Yan, Y. Song, X. Zhou, H. Wu, \"shall I be your chat companion?\":\nTowards an online human-computer conversation system, in: CIKM '16,\n2016.\n[80] R. Yan, D. Zhao, W. E., Joint learning of response ranking and next\nutterance suggestion in human-computer conversation system, in: SIGIR\n'17, 2017.\n[81] C. Qu, L. Yang, W. B. Croft, J. Trippas, Y. Zhang, M. Qiu, Analyzing\nand characterizing user intent in information-seeking conversations., in:\nSIGIR '18, 2018.\n[82] C. Qu, L. Yang, W. B. Croft, Y. Zhang, J. Trippas, M. Qiu, User intent\nprediction in information-seeking conversations, in: CHIIR '19, 2019.\n[83] L. Shang, T. Sakai, Overview of the ntcir-12 short text conversation task.\n[84] Z. Dai, C. Xiong, J. Callan, Z. Liu, Convolutional neural networks for\nsoft-matching n-grams in ad-hoc search, in: Proceedings of the Eleventh\nACM International Conference on Web Search and Data Mining, WSDM\n'18, ACM, New York, NY, USA, 2018, pp. 126{134.\n[85] C. Xiong, Z. Dai, J. Callan, Z. Liu, R. Power, End-to-end neural ad-hoc\nranking with kernel pooling, in: Proceedings of the 40th International\n51\n\nACM SIGIR Conference on Research and Development in Information\nRetrieval, SIGIR '17, ACM, New York, NY, USA, 2017, pp. 55{64.\n[86] B. Wang, K. Liu, J. Zhao, Inner attention based recurrent neural networks\nfor answer selection, in: Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics, Association for Computational\nLinguistics, 2016, pp. 1288{1297.\n[87] S. Wang, J. Jiang, A compare-aggregate model for matching text se-\nquences, in: Proceedings of the 5th International Conference on Learning\nRepresentations, ICLR'17, 2017.\n[88] S. Wan, Y. Lan, J. Guo, J. Xu, L. Pang, X. Cheng, A deep architecture for\nsemantic matching with multiple positional sentence representations, in:\nProceedings of the Thirtieth AAAI Conference on Arti\fcial Intelligence,\nAAAI'16, AAAI Press, 2016, pp. 2835{2841.\n[89] W. Yang, H. Zhang, J. Lin, Simple applications of bert for ad hoc docu-\nment retrieval, arXiv preprint arXiv:1903.10972.\n[90] W. Yin, H. Sch utze, Multigrancnn: An architecture for general matching\nof text chunks on multiple levels of granularity, in: Proceedings of the\n53rd Annual Meeting of the Association for Computational Linguistics and\nthe 7th International Joint Conference on Natural Language Processing,\nAssociation for Computational Linguistics, 2015, pp. 63{73.\n[91] Y. Nie, A. Sordoni, J.-Y. Nie, Multi-level abstraction convolutional model\nwith weak supervision for information retrieval, in: The 41st International\nACM SIGIR Conference on Research & Development in Information Re-\ntrieval, SIGIR '18, ACM, New York, NY, USA, 2018, pp. 985{988.\n[92] J. Rao, W. Yang, Y. Zhang, F. Ture, J. J. Lin, Multi-perspective relevance\nmatching with hierarchical convnets for social media search, national con-\nference on arti\fcial intelligence.\n52\n\n[93] Y. Nie, Y. Li, J. Nie, Empirical study of multi-level convolution models\nfor ir based on representations and interactions (2018) 59{66.\n[94] J. Huang, S. Yao, C. Lyu, D. Ji, Multi-granularity neural sentence model\nfor measuring short text similarity, in: Database Systems for Advanced\nApplications, Springer International Publishing, Cham, 2017, pp. 439{\n455.\n[95] H. Chen, F. X. Han, D. Niu, D. Liu, K. Lai, C. Wu, Y. Xu, Mix: Multi-\nchannel information crossing for text matching, in: Proceedings of the\n24th ACM SIGKDD International Conference on Knowledge Discovery &\nData Mining, KDD '18, ACM, New York, NY, USA, 2018, pp. 110{119.\n[96] W. Chen, T.-Y. Liu, Y. Lan, Z.-M. Ma, H. Li, Ranking measures and\nloss functions in learning to rank, in: Advances in Neural Information\nProcessing Systems, 2009, pp. 315{323.\n[97] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton,\nG. N. Hullender, Learning to rank using gradient descent, in: Proceedings\nof the 22nd International Conference on Machine learning (ICML-05),\n2005, pp. 89{96.\n[98] F. Xia, T.-Y. Liu, J. Wang, W. Zhang, H. Li, Listwise approach to learning\nto rank: theory and algorithm, in: Proceedings of the 25th international\nconference on Machine learning, ACM, 2008, pp. 1192{1199.\n[99] M. Taylor, J. Guiver, S. Robertson, T. Minka, Softrank: optimizing non-\nsmooth rank metrics, in: Proceedings of WSDM'08, ACM, 2008, pp. 77{\n86.\n[100] C. J. Burges, From ranknet to lambdarank to lambdamart: An overview,\nLearning 11 (2010) 23{581.\n[101] Q. Ai, K. Bi, J. Guo, W. B. Croft, Learning a deep listwise context model\nfor ranking re\fnement, in: The 41st International ACM SIGIR Conference\n53\n\non Research & Development in Information Retrieval, ACM, 2018, pp.\n135{144.\n[102] Q. Ai, X. Wang, N. Golbandi, M. Bendersky, M. Najork, Learning\ngroupwise scoring functions using deep neural networks, arXiv preprint\narXiv:1811.04415.\n[103] Q. Ai, J. Mao, Y. Liu, W. B. Croft, Unbiased learning to rank: Theory\nand practice, in: Proceedings of the 27th ACM International Conference\non Information and Knowledge Management, ACM, 2018, pp. 2305{2306.\n[104] X. Liu, J. Gao, X. He, L. Deng, K. Duh, Y.-Y. Wang, Representation\nlearning using multi-task deep neural networks for semantic classi\fcation\nand information retrieval.\n[105] O. Chapelle, P. Shivaswamy, S. Vadrevu, K. Weinberger, Y. Zhang,\nB. Tseng, Multi-task learning for boosting with application to web search\nranking, in: Proceedings of the 16th ACM SIGKDD international confer-\nence on Knowledge discovery and data mining, ACM, 2010, pp. 1189{1198.\n[106] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, Y. Bengio, Generative adversarial nets, in: Ad-\nvances in neural information processing systems, 2014, pp. 2672{2680.\n[107] D. Cohen, B. Mitra, K. Hofmann, W. B. Croft, Cross domain regulariza-\ntion for neural ranking models using adversarial learning, in: The 41st\nInternational ACM SIGIR Conference on Research & Development in In-\nformation Retrieval, ACM, 2018, pp. 1025{1028.\n[108] H. Zamani, M. Dehghani, F. Diaz, H. Li, N. Craswell, Sigir 2018 workshop\non learning from limited or noisy data for information retrieval, in: The\n41st International ACM SIGIR Conference on Research & Development\nin Information Retrieval, SIGIR '18, ACM, New York, NY, USA, 2018,\npp. 1439{1440.\n54\n\n[109] N. Asadi, D. Metzler, T. Elsayed, J. Lin, Pseudo test collections for learn-\ning web search ranking functions, in: Proceedings of the 34th International\nACM SIGIR Conference on Research and Development in Information Re-\ntrieval, SIGIR '11, ACM, New York, NY, USA, 2011, pp. 1073{1082.\n[110] H. Zamani, W. B. Croft, J. S. Culpepper, Neural query performance pre-\ndiction using weak supervision from multiple signals, in: The 41st Interna-\ntional ACM SIGIR Conference on Research & Development in Information\nRetrieval, SIGIR '18, ACM, New York, NY, USA, 2018, pp. 105{114.\n[111] H. Zamani, W. B. Croft, Relevance-based word embedding, in: Proceed-\nings of the 40th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, SIGIR '17, ACM, New York, NY,\nUSA, 2017, pp. 505{514.\n[112] D. Cohen, J. Foley, H. Zamani, J. Allan, W. B. Croft, Universal approx-\nimation functions for fast learning to rank: Replacing expensive regres-\nsion forests with simple feed-forward networks, in: The 41st International\nACM SIGIR Conference on Research & Development in Information Re-\ntrieval, SIGIR '18, ACM, New York, NY, USA, 2018, pp. 1017{1020.\n[113] M. Szummer, E. Yilmaz, Semi-supervised learning to rank with preference\nregularization, in: Proceedings of the 20th ACM International Conference\non Information and Knowledge Management, CIKM '11, ACM, New York,\nNY, USA, 2011, pp. 269{278.\n[114] K. Duh, K. Kirchho\u000b, Learning to rank with partially-labeled data, in:\nProceedings of the 31st Annual International ACM SIGIR Conference on\nResearch and Development in Information Retrieval, SIGIR '08, ACM,\nNew York, NY, USA, 2008, pp. 251{258.\n[115] X. Zhang, B. He, T. Luo, Training query \fltering for semi-supervised\nlearning to rank with pseudo labels, World Wide Web 19 (5) (2016) 833{\n864.\n55\n\n[116] M. Dehghani, A. Severyn, S. Rothe, J. Kamps, Avoiding your teacher's\nmistakes: Training neural networks with controlled weak supervision,\nCoRR abs/1711.00313. arXiv:1711.00313 .\n[117] B. Li, P. Cheng, L. Jia, Joint learning from labeled and unlabeled data\nfor information retrieval, in: Proceedings of the 27th International Con-\nference on Computational Linguistics, COLING '18, Association for Com-\nputational Linguistics, Santa Fe, New Mexico, USA, 2018, pp. 293{302.\n[118] K. Hui, A. Yates, K. Berberich, G. de Melo, Co-pacrr: A context-aware\nneural ir model for ad-hoc retrieval, in: Proceedings of the eleventh ACM\ninternational conference on web search and data mining, ACM, 2018, pp.\n279{287.\n[119] C. Li, Y. Sun, B. He, L. Wang, K. Hui, A. Yates, L. Sun, J. Xu, NPRF:\nA neural pseudo relevance feedback framework for ad-hoc information\nretrieval, in: Proceedings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, EMNLP '18, 2018.\n[120] J. M. Ponte, W. B. Croft, A language modeling approach to information\nretrieval, Ph.D. thesis, University of Massachusetts at Amherst (1998).\n[121] V. Lavrenko, W. B. Croft, Relevance based language models, in: Proceed-\nings of the 24th annual international ACM SIGIR conference on Research\nand development in information retrieval, ACM, 2001, pp. 120{127.\n[122] T. Joachims, Optimizing search engines using clickthrough data, in: Pro-\nceedings of the eighth ACM SIGKDD international conference on Knowl-\nedge discovery and data mining, ACM, 2002, pp. 133{142.\n[123] K. Hui, A. Yates, K. Berberich, G. de Melo, A position-aware deep\nmodel for relevance matching in information retrieval, arXiv preprint\narXiv:1704.03940.\n[124] M. Wang, N. A. Smith, T. Mitamura, What is the jeopardy model? a\nquasi-synchronous grammar for qa, in: Proceedings of the 2007 Joint\n56\n\nConference on Empirical Methods in Natural Language Processing and\nComputational Natural Language Learning (EMNLP-CoNLL), 2007.\n[125] L. Yu, K. M. Hermann, P. Blunsom, S. Pulman, Deep learning for answer\nsentence selection, CoRR abs/1412.1632. arXiv:1412.1632 .\n[126] J. Rao, H. He, J. Lin, Noise-contrastive estimation for answer selection\nwith deep neural networks, in: Proceedings of the 25th ACM International\non Conference on Information and Knowledge Management, CIKM '16,\n2016.\n[127] Y. Tay, M. C. Phan, A. T. Luu, S. C. Hui, Learning to rank question\nanswer pairs with holographic dual LSTM architecture, in: Proceedings\nof the 40th International ACM SIGIR Conference on Research and Devel-\nopment in Information Retrieval, Shinjuku, Tokyo, Japan, August 7-11,\n2017, 2017, pp. 695{704.\n[128] Y. Tay, L. A. Tuan, S. C. Hui, Hyperbolic representation learning for fast\nand e\u000ecient neural question answering, in: Proceedings of the Eleventh\nACM International Conference on Web Search and Data Mining, WSDM\n2018, Marina Del Rey, CA, USA, February 5-9, 2018, 2018, pp. 583{591.\n[129] W. Yih, M. Chang, C. Meek, A. Pastusiak, Question answering using\nenhanced lexical semantic models, in: Proceedings of the 51st Annual\nMeeting of the Association for Computational Linguistics, ACL 2013, 4-9\nAugust 2013, So\fa, Bulgaria,, The Association for Computer Linguistics,\n2013, pp. 1744{1753.\n[130] T. Kraska, A. Beutel, E. H. Chi, J. Dean, N. Polyzotis, The case for\nlearned index structures, in: Proceedings of the 2018 International Con-\nference on Management of Data, SIGMOD '18, ACM, New York, NY,\nUSA, 2018, pp. 489{504.\n[131] L. Boytsov, D. Novak, Y. Malkov, E. Nyberg, O\u000b the beaten path: Let's\nreplace term-based retrieval with k-nn search, in: Proceedings of the 25th\n57\n\nACM International on Conference on Information and Knowledge Man-\nagement, CIKM '16, ACM, New York, NY, USA, 2016, pp. 1099{1108.\n[132] C. V. Gysel, M. de Rijke, E. Kanoulas, Neural vector spaces for unsu-\npervised information retrieval, ACM Trans. Inf. Syst. 36 (4) (2018) 38:1{\n38:25.\n[133] Z. Liu, C. Xiong, M. Sun, Z. Liu, Entity-duet neural ranking: Understand-\ning the role of knowledge graph semantics in neural information retrieval,\nin: Proceedings of the 56th Annual Meeting of the Association for Com-\nputational Linguistics, Association for Computational Linguistics, 2018,\npp. 2395{2405.\n[134] G. Nguyen, L. Tamine, L. Soulier, N. Bricon-Souf, Toward a deep neural\napproach for knowledge-based IR, CoRR abs/1606.07211. arXiv:1606.\n07211 .\n[135] Y. Shen, Y. Deng, M. Yang, Y. Li, N. Du, W. Fan, K. Lei, Knowledge-\naware attentive neural network for ranking question answer pairs, in: The\n41st International ACM SIGIR Conference on Research & Development\nin Information Retrieval, SIGIR '18, ACM, New York, NY, USA, 2018,\npp. 901{904.\n[136] X. Song, F. Feng, X. Han, X. Yang, W. Liu, L. Nie, Neural compatibility\nmodeling with attentive knowledge distillation, in: The 41st International\nACM SIGIR Conference on Research & Development in Information Re-\ntrieval, SIGIR '18, ACM, New York, NY, USA, 2018, pp. 5{14.\n[137] Z. Xu, B. Liu, B. Wang, C. Sun, X. Wang, Incorporating loose-structured\nknowledge into LSTM with recall gate for conversation modeling, CoRR\nabs/1605.05110. arXiv:1605.05110 .\n[138] M. Ghazvininejad, C. Brockett, M. Chang, B. Dolan, J. Gao, W. Yih,\nM. Galley, A knowledge-grounded neural conversation model, in: Pro-\n58\n\nceedings of the Thirty-Second AAAI Conference on Arti\fcial Intelligence,\n(AAAI-18), 2018, pp. 5110{5117.\n[139] Y. Wu, W. Wu, C. Xu, Z. Li, Knowledge enhanced hybrid neural network\nfor text matching, in: Proceedings of the Thirty-Second AAAI Conference\non Arti\fcial Intelligence, (AAAI-18), 2018, pp. 5586{5593.\n[140] J. Zhang, Y. Liu, S. Ma, Q. Tian, Relevance estimation with multiple\ninformation sources on search engine result pages, in: Proceedings of the\n27th ACM International Conference on Information and Knowledge Man-\nagement, ACM, 2018, pp. 627{636.\n[141] B. v. d. Akker, I. Markov, M. de Rijke, Vitor: Learning to rank webpages\nbased on visual features, arXiv preprint arXiv:1903.02939.\n[142] X. Shen, B. Tan, C. Zhai, Context-sensitive information retrieval using\nimplicit feedback, SIGIR '05, ACM, New York, NY, USA, 2005, pp. 43{\n50.\n[143] Y. Ustinovskiy, P. Serdyukov, Personalization of web-search using short-\nterm browsing context, CIKM '13, ACM, New York, NY, USA, 2013, pp.\n1979{1988.\n[144] B. Xiang, D. Jiang, J. Pei, X. Sun, E. Chen, H. Li, Context-aware ranking\nin web search, in: Proceedings of the 33rd International ACM SIGIR\nConference on Research and Development in Information Retrieval, SIGIR\n'10, ACM, New York, NY, USA, 2010, pp. 451{458.\n[145] P. N. Bennett, R. W. White, W. Chu, S. T. Dumais, P. Bailey, F. Borisyuk,\nX. Cui, Modeling the impact of short- and long-term behavior on search\npersonalization, in: Proceedings of the 35th International ACM SIGIR\nConference on Research and Development in Information Retrieval, SIGIR\n'12, ACM, New York, NY, USA, 2012, pp. 185{194.\n[146] N. Matthijs, F. Radlinski, Personalizing web search using long term brows-\ning history, in: Proceedings of the Fourth ACM International Conference\n59\n\non Web Search and Data Mining, WSDM '11, ACM, New York, NY, USA,\n2011, pp. 25{34.\n[147] P. N. Bennett, F. Radlinski, R. W. White, E. Yilmaz, Inferring and using\nlocation metadata to personalize web search, in: Proceedings of the 34th\nInternational ACM SIGIR Conference on Research and Development in\nInformation Retrieval, SIGIR '11, ACM, New York, NY, USA, 2011, pp.\n135{154.\n[148] H. Zamani, M. Bendersky, X. Wang, M. Zhang, Situational context for\nranking in personal search, in: Proceedings of the 26th International\nConference on World Wide Web, WWW '17, International World Wide\nWeb Conferences Steering Committee, Republic and Canton of Geneva,\nSwitzerland, 2017, pp. 1531{1540.\n[149] W. U. Ahmad, K.-W. Chang, H. Wang, Multi-task learning for document\nranking and query suggestion, in: Proceedings of the Sixth International\nConference on Learning Representations, ICLR '18, 2018.\n[150] W. Chen, F. Cai, H. Chen, M. de Rijke, Attention-based hierarchical\nneural query suggestion, in: Proceedings of the 41st International ACM\nSIGIR Conference on Research & Development in Information Retrieval,\nSIGIR '18, ACM, New York, NY, USA, 2018, pp. 1093{1096.\n[151] H. Zamani, W. B. Croft, Embedding-based query language models, in:\nProceedings of the 2016 ACM International Conference on the Theory of\nInformation Retrieval, ICTIR '16, 2016, pp. 147{156.\n[152] T. Mikolov, K. Chen, G. Corrado, J. Dean, E\u000ecient estimation of word\nrepresentations in vector space, arXiv preprint arXiv:1301.3781.\n[153] H. Zamani, J. Dadashkarimi, A. Shakery, W. B. Croft, Pseudo-relevance\nfeedback based on matrix factorization, in: Proceedings of the 25th ACM\nInternational on Conference on Information and Knowledge Management,\nCIKM '16, 2016, pp. 1483{1492.\n60\n\n[154] L. Pang, Y. Lan, J. Guo, J. Xu, X. Cheng, A study of matchpyramid\nmodels on ad-hoc retrieval, arXiv preprint arXiv:1606.04648.\n[155] D. Cohen, B. O'Connor, W. B. Croft, Understanding the representational\npower of neural retrieval models using nlp tasks, in: Proceedings of the\n2018 ACM SIGIR International Conference on Theory of Information Re-\ntrieval, ACM, 2018, pp. 67{74.\n[156] Y. Nie, Y. Li, J.-Y. Nie, Empirical study of multi-level convolution mod-\nels for ir based on representations and interactions, in: Proceedings of\nthe 2018 ACM SIGIR International Conference on Theory of Information\nRetrieval, ACM, 2018, pp. 59{66.\n61",
  "textLength": 114717
}