{
  "paperId": "a7a21b1be7dd3ddf31d48e83fd84412c480d58ab",
  "title": "Adversary Resilient Learned Bloom Filters",
  "pdfPath": "a7a21b1be7dd3ddf31d48e83fd84412c480d58ab.pdf",
  "text": "Adversary Resilient Learned Bloom Filters\nGhada Almashaqbeh1, Allison Bishop2,3and Hayder Tirmazi3\n1University of Connecticut, ghada@uconn.edu\n2Proof Trading, abishop@ccny.cuny.edu\n3City College of New York, hayder.research@gmail.com\nAbstract. A learned Bloom filter (LBF) combines a classical Bloom\nfilter (CBF) with a learning model to reduce the amount of memory\nneeded to represent a given set while achieving a target false positive rate\n(FPR). Provable security against adaptive adversaries that advertently\nattempt to increase FPR has been studied for CBFs, but not for LBFs.\nIn this paper, we close this gap and show how to achieve adaptive\nsecurity for LBFs. In particular, we define several adaptive security\nnotions capturing varying degrees of adversarial control, including full and\npartial adaptivity, in addition to LBF extensions of existing adversarial\nmodels for CBFs, including the Always-Bet and Bet-or-Pass notions. We\npropose two secure LBF constructions, PRP-LBF and Cuckoo-LBF, and\nformally prove their security under these models assuming the existence\nof one-way functions. Based on our analysis and use case evaluations,\nour constructions achieve strong security guarantees while maintaining\ncompetitive FPR and memory overhead.\n1 Introduction\nBloom filters are probabilistic data structures that allow building a succinct\nrepresentation of a data set while offering approximate membership queries, i.e.,\nwhether an element xis in a set S. Bloom filters have one-sided error guarantees:\nifx∈Sthe filter always returns yes (i.e., no false negatives), but if x /∈S, the\nfilter may return yes, instead of no, with some probability resulting in a false\npositive. Bloom filters greatly improve space efficiency; instead of storing the\nfull set, this set is encoded in a much shorter bit string using a family of hash\nfunctions to map each item into a few bits (set to 1) in this string [3].\nLearned Bloom filters (LBFs) have been introduced to improve upon con-\nventional, or classical Bloom filters (CBFs) [14]. In particular, and as shown in\nFigure 1, an LBF combines a learning model with a CBF to obtain a lower false\npositive rate (FPR) than CBFs under the same memory budget. The learning\nmodel acts as an initial filter, providing a probabilistic estimate on whether x∈S,\nwhile a smaller CBF serves as a backup to prevent any false negatives.\nBloom filters are used in various practical applications, including Linux\nnetwork drivers, network protocols, deep packet scanners, peer-to-peer networking,\nand caching [3,28]. Many well-known systems, such as Apache Hadoop, Apache\nHBase, Google BigTable, Google LevelDB, and Meta RocksDB, use Bloom filtersarXiv:2409.06556v6  [cs.CR]  1 Sep 2025\n\nLearning Model Backup CBFInput Negatives Negatives\nPositives Positives\nFig.1: Conventional LBF architecture—a backup CBF only checks values that\nare identified as (highly probably) negative by the learning model.\nas part of their implementations. As LBFs provide the same one-sided error\nguarantees as CBFs, the applications of LBFs and CBFs are identical. Recently,\nRoblox used LBFs to achieve 25% cost savings in their production workloads for\nSpark join queries [16].\nIn such critical infrastructures, adversaries may attempt to craft false positives,\ncausing the Bloom filter to deviate from its expected behavior [10], thereby\ndisrupting system operation. For example, as discussed in [20], in a Bloom filter-\nbased spam email whitelist (that stores addresses of known senders), crafting\nfalse positives allows spam emails to pass as benign emails. Similarly, in web\ncaches, a Bloom filter can be used to represent the web pages in the cache; a\nfalse positive causes an unsuccessful cache access, which degrades performance\nand may eventually cause a denial of service attack. Another case is related to\ndatabases [29], including Meta’s RocksDB and Google’s LevelDB; crafting false\npositives in the Bloom filters of RocksDB (respectively LevelDB) can degrade\nthe performance of query lookups by up to 8x (respectively 2x).\nThese cases attest to the importance of studying provable security for Bloom\nfilters against adaptive adversaries. This has been done for CBFs, including study-\ning practical attacks [10], and formalizing notions for adaptive security alongside\nshowing new CBF constructions that realize these notions [20,21]. However, to\nthe best of our knowledge, no such notions/provably-secure constructions exist for\nLBFs. Indeed, as an LBF contains a backup CBF, it inherits all the adversarial\nvulnerabilities of CBFs. However, securing the backup CBF does not imply that\nthe LBF employing this CBF is secure, as LBFs have different designs. That is,\nhaving a learning model allows adversaries to craft false positives in new ways [23].\nExamples include mutating an existing false positive or modifying certain features\n(relevant to the learning model) of a true negative input to convert it into a false\npositive. Nonetheless, the work [23] only demonstrated such attacks while leaving\nprovable adaptive security of LBFs as an open problem.\n1.1 Contributions\nIn this paper, we close this gap and initiate a formal study on the security of LBFs\nagainst adaptive adversaries. In particular, we make the following contributions.\nNew notions for adaptive security. We define several new security notions\nfor LBFs that capture a spectrum of adversarial capabilities. These include\nfully-adaptive adversaries who choose all queries, partially-adaptive adversaries\nwho choose a fraction of queries, bet-or-pass adversaries who can selectively\n2\n\nchoose to output a guess in the security game, and always-bet adversaries who\nalways output a guess (the latter two are inspired by similar models defined for\nCBFs [21]). These models provide fine-grained understanding of the attack surface\nagainst LBFs and several security-efficiency tradeoffs. Moreover, we explore the\nrelationships between these various notions, proving which security notions imply\nwhich. We believe that our notions contribute to establishing a foundation for\nanalyzing learning-augmented data structures in adversarial settings.\nConstructions. We propose two constructions for LBFs: PRP-LBF and Cuckoo-\nLBF.OurconstructionsextendapriorLBFvariantcalledPartitionedLBFs[30]by\nintroducing cryptographic hardness through pseudorandom permutations (PRPs)\nand pseudorandom functions (PRFs). Assuming one-way functions exist, we show\nthat PRP-LBF achieves security under the fully-adaptive and partially-adaptive\nadversarial models. We also show (assuming one-way functions) that Cuckoo-LBF\nsatisfies the bet-or-pass security notion (the extended notion for LBFs). Bet-or-\npass is one of the strongest adversaries considered by prior work [15,21] for CBFs.\nOur constructions require only O\u0000\nnlog1\nε+λ\u0001\nadditional bits of memory, where\nnis the cardinality of the set represented by the filter, εis the desired FPR, and\nλis the security parameter.\nPerformance evaluation . We analyze FPR of our constructions in both adver-\nsarial and non-adversarial settings. Our results show that LBFs can provide lower\nFPR than CBFs in realistic workloads using the same memory budget. They also\nshow that our LBF constructions achieve better FPR-memory tradeoffs than\nprior CBF constructions while maintaining strong security guarantees. These\nresults demonstrate that adversarial resilience can be supported for LBFs with\nminimal performance overhead.\n1.2 Related Work\nSecurity of Classical Bloom Filters. Gerbet et al. [10] demonstrate practical\nattacks on CBFs in the context of web crawlers and spam email filtering, and\npropose combining universal hash functions with message authentication codes\nto mitigate a subset of these attacks. Naor et al. [20] define an adversarial model\nfor CBFs and show that for computationally bounded adversaries, non-trivial\nadversary resilient CBFs exist if and only if one-way functions exist, and that for\ncomputationally unbounded adversaries there exists a CBF that is secure when\nthe adversary makes tqueries while using only O(nlog1\nε+t)bits of memory.\nClayton et al. [4] and Filic et al. [8] present stronger adversarial models\nthan [20], giving the adversary the capability of performing insertions and access-\ning the internal CBF state, and they show secure CBF constructions realizing\nthese models. Naor and Oved [21] introduce a comprehensive study of CBF\nsecurity and define several robustness notions in a generalized adversarial model.\nConcretely, [21] calls the notion of [20] as Always-Bet (AB) test since the adver-\nsary must output a guess. Then, it introduces a new, strictly stronger, security\nnotion called the Bet-Or-Pass (BP) test, giving the adversary the option to pass\n3\n\nwithout outputting a guess. Lotan and Naor [15] provide further results related\nto relationships between the security notions of [21].\nSecurity of Learned Bloom Filters. LBFs were first introduced by Kraska\net al. [14] who showed that LBFs can offer better FPR vs. memory tradeoffs\nthan CBFs. Mitzenmacher [18] provides the first rigorous mathematical model for\nLBFs, focusing on analyzing performance in terms of memory size and FPR, and\nintroduces an LBF variant called Sandwiched LBF. Vaidia et al. [30] introduce\nanother LBF variant, called partitioned LBFs (PLBFs). PLBFs use a learning\nmodel to partition the set Sintop∈Npartitions and use a separate backup\nCBF for each of the ppartitions. PLBFs reduce the FPR for specific partitions\ncompared to original LBFs that employ only one backup CBF.\nAll these works only examine performance, but not security. We are only\naware of one prior work that studied LBFs in adversarial settings: Reviriego et\nal. [23] demonstrate practical attacks on LBFs, which, as mentioned earlier, rely\non exploiting the learning model to craft false positives in new ways. In response,\nthey propose two potential mitigations: the first relies on switching back to a\nCBF once an attack is detected, while the second adds a second backup CBF.\nHowever, Reviriego et al. [23] do not provide provable security guarantees for the\nsuggested mitigations, or even empirical evaluations of their effectiveness, and\nleave provable security of LBFs as an open problem.\n1.3 Future Directions\nOur work offers important foundational steps towards understanding provable\nsecurity of LBFs. Here, we list some natural directions for future work.\nUnsteady setting. We provide secure constructions for LBFs in which the query\nalgorithm does not modify the internal representation of the LBF. Naor and\nYogev [20] call this the steadysetting, and show secure CBF constructions for\ntheunsteady setting where the query algorithm changes the internal filter repre-\nsentation after each query. An interesting direction is to build LBF constructions\nand prove their adaptive security in the unsteady setting.\nDynamic Bloom filters. As opposed to static Bloom filters, which do not\nmodify the input set Safter construction, dynamic Bloom filters allow insertions\nof new elements S′={x} ∪Safter construction. Note that to maintain their\none-sided error guarantees, even dynamic Bloom filters do not allow deletions.\nSimilar to prior work [15,20,21] that consider static CBFs, our work focuses on\nstatic LBFs. Clayton et al. [4] and Filic et al. [8] show provably-secure security\nconstructions for dynamic CBFs. As such, another interesting direction to explore\nis formulating adaptive security and building secure constructions for dynamic\nLBFs under an adversarial model that admits insertions.\nComputationally unbounded adversaries. Our constructions are secure\nagainst polynomial-time adversaries. Another interesting avenue for future re-\nsearch is showing LBF constructions that realize our security notions while\nassuming computationally unbounded adversaries.\n4\n\n2 Preliminaries\nIn this section, we review CBFs and their adversarial models from [20,21], which\nform the basis for the adversarial models we define for LBFs.\nNotation. For a set S,|S|denotes the size of S, and x←$Sdenotes that xis\nsampled uniformly at random from S. For n∈N,[n]denotes {1,···, n}. Bloom\nfilters can store elements from a finite domain D, so we have S⊆D. Lastly, λ\ndenotes the security parameter, negl(λ)denotes a function negligible in λ, and\nPPTdenotes probabilistic polynomial time.\nCBF Modeling. We adopt the CBF model from [20] while considering the steady\nsetting in which the query algorithm does not change the filter representation.\nDefinition 1. A Bloom filter B= (B1,B2)is a pair of polynomial time algo-\nrithms. B1is a randomized construction algorithm that takes as input a set\nS⊆Dof size n, and outputs a representation M.B2is a deterministic query\nalgorithm that takes as input an element x∈Dand a representation M, and out-\nputs 1 indicating that x∈S, and 0 otherwise. We say that Bis an (n, ε)-Bloom\nfilter if for all sets S⊆Dof cardinality n, the following hold:\n1. Completeness: ∀x∈S: Pr[B2(B1(S), x) = 1] = 1\n2. Soundness: ∀x /∈S: Pr[B2(B1(S), x) = 1] ≤ε\nwhere the probabilities are taken over the random coins of B1.\nA standard CBF is implemented as a string strof length nb∈Nbits indexed\nover [nb], along with nh∈Nhash functions hi, fori∈[nh], from a universal hash\nfamily Hused for element mapping. That is, each himaps an element from D\nto an index value within [nb], sohi:D7→[nb]. Then, for each element x∈S,\nfori∈[nh], the bit at index hi(x)ofstris set to 1(if it is already set to 1, it\nstays 1). For a queried element x, a CBF returns 1 if all bits within strat indices\ncorresponding to hi(x)are set to 1, otherwise, the CBF returns 0.\nAlways-Bet Security. Naor et al. [20,21] introduced the Always-Bet security\ngame for CBFs, denoted as ABGame, in which an adversary who has oracle access\nto the CBF aims to find a false positive. It is denoted as always-bet since the\nadversary is required to output a guess x∗at the end of the game that will be\ntested whether it is a false positive (i.e., always bets that x∗is a false positive).\nIn detail, for an adversary A= (A1,A2),A1chooses a set S⊆Dfor which B1\nwill compute a representation M, andA2getsSas input and attempts to find a\nfalse positive x∗that was not queried before, given only oracle access B2(M,·).\nABGame( A, t, λ):\n1.S← A 1(1λ)such that S⊆Dand|S|=n.\n2.M←B1(S).\n3.x∗← AB2(M,·)\n2 (1λ, S)where A2can make at most tqueries to B2(M,·)—in\nthis and other games, we denote the adversarial queries as {x1, . . . , x t}.\n5\n\n4. If x∗/∈S∪ {x1,···, xt}andB2(M, x∗) = 1, return 1. Otherwise, return 0.\nWe use the following security notion for all security games discussed in this\npaper (except for BPGame, which we discuss next).\nDefinition 2. A Bloom filter Bis(n, t, ε )-secure under a security game Game\nif for for all large enough λ∈N, all PPTadversaries A, and all sets S⊆Dof\ncardinality n, it holds that Pr[Game (A, t, λ) = 1] ≤ε, where the probability is\ntaken over the random coins of BandA.\nBet-or-Pass Security. Naor and Oved [21] introduce a security game stronger\nthan ABGamecalled Bet-or-Pass, or BPGame. Here, Acan either output x∗or\npass, so it is not always betting on the output of the game to be a false positive.\nBPGamealso defines A’sprofit CA, rewarding Aifx∗is a false positive, penalizing\nAifx∗is not a false positive, and leaving CAunchanged if Achooses to pass.\nBPGame( A, t, λ):\n1.S← A 1(1λ)such that S⊆Dand|S|=n.\n2.M←B1(S).\n3.(b, x∗)← AB2(M,·)\n2 (1λ, S)where A2can make at most tqueries, {x1, . . . , x t},\ntoB2(M,·).\n4. Return A’s profit, CA, which is defined as\nCA=\n\n0, ifb= 0\nε−1, ifb= 1andx∗/∈S∪ {x1, . . . , x t}\nandB2(M, x∗) = 1\n−(1−ε)−1,otherwise\nThe profit formulation is set in this way to ensure that an adversary that makes a\nrandom guess (which will be a false positive with probability ε) has an expected\nprofit of zero [21]. Abreaks the security of the CBF if its profit is noticeably\nlarger than zero. Thus, the security guarantee for BPGameis defined as an upper\nbound on the expectation of the adversary’s profit.\nDefinition 3. A Bloom filter Bis(n, t, ε )-secure under BPGameif for all large\nenough λ∈N, all PPTadversaries A, and all sets S⊆Dof cardinality n,\nthere exists a negligible function negl(·)such that E[CA]≤negl(λ), where the\nexpectation is taken over the random coins of BandA.\n3 Definitions and Adversarial Models for LBFs\nIn this section, we first define a model for LBFs, followed by three LBF adaptive\nsecurity notions: full adaptivity (which when slightly modified captures always-bet\nsecurity), partial adaptivity, and learned bet-or-pass.\n6\n\n3.1 LBF Definition\nWe present our definitions for LBFs, which are based on the model of [18] with\nadditional formalism and adaptations to make it convenient to compare with the\nmodels of CBFs introduced earlier. An LBF uses a learning model trained over\nthe dataset the LBF represents, such that the model determines a function L\nthat models this set. In particular, on input x,Loutputs the probability that\nx∈S. In what follows, we define the notion of a training dataset and a learning\nmodel in the context of LBFs (where yiis a label stating whether xi∈S).\nDefinition 4. LetS⊆Dbe any set represented by a Bloom filter. For any two\nsetsP⊆SandN⊆D\\S, the training dataset is the set T={(xi, yi= 1)|\nxi∈P} ∪ { (xi, yi= 0)|xi∈N}.\nDefinition 5. For an L:D7→[0,1]and threshold τ, we say Lis an (S, τ, ε p, εn)-\nlearning model if for any set S⊆Dthe following hold:\n1. P-Soundness: ∀x /∈S: Pr[L(x)≥τ]≤εp\n2. N-Soundness: ∀x∈S: Pr[L(x)< τ]≤εn\nwhere the probability is taken over the random coins of L.\nNow, we define an LBF capturing both the classic and learning components.\nWe consider the steadysetting in which the query algorithm B2does not change\nthe learned representation of the set S(including both the set representation\nheld by the backup CBF, i.e., M, and (L, τ)).\nDefinition 6. An LBF B= (B1,B2,B3,B4)is a tuple of four polynomial-time\nalgorithms: B1is as before, B2is a query algorithm, B3is a randomized algorithm\nthat takes a set S⊆Das input and outputs a training dataset T, andB4is a\nrandomized algorithm that takes the training dataset Tas input and returns a\nlearning model Land a threshold τ∈[0,1]. The internal representation of an LBF\ncontains two components: the classical component Mand the learned component\n(L, τ).B2takes as inputs an element x∈D,M, and ( L, τ), and outputs 1,\nindicating that x∈S, and 0 otherwise. We say that Bis an (n, τ, ε, ε p, εn)-LBF\nif for all sets S⊆Dof cardinality n, it holds that:\n1. Completeness: ∀x∈S: Pr[B2(B1(S),B4(B3(S)), x) = 1] = 1 .\n2. Filter soundness: ∀x /∈S: Pr[B2(B1(S),B4(B3(S)), x) = 1] ≤ε.\n3. Learning model soundness: B4(B3(S))is an (S, τ, ε p, εn)-learning model.\nwhere the probabilities are over the random coins of B1,B3, andB4.\nStandard LBFs (Figure 1) use a learning model as a pre-filter before a CBF.\nThe CBF is called a backup CBF as it is only queried on inputs xfor which the\nlearning model decides that xis not an element of the stored set S, i.e.,L(x)< τ.\n7\n\n3.2 Full Adaptive Security\nFull adaptivity means that Achooses all the queries submitted to the query\nalgorithm B2, i.e., the adversary controls the entire workload. For CBFs, the full\nadaptivity game, besides the oracle access to B2(M,·), gives oracle access to B1\nenabling Ato obtain Mfor any set Sof its choosing. Based on that, we define\nthe following game capturing full adaptive security for CBFs.\nFAGame( A, t, λ):\n1.S← A 1(1λ)such that S⊆Dand|S|=n.\n2.M←B1(S).\n3.x∗← AO\n2(1λ, S), where O={B1(·),B2(M,·)}.A2can make at most tqueries\ntoB2, and any polynomial number of queries to B1.\n4. If x∗/∈S∪ {x1,···, xt}andB2(M, x∗) = 1, return 1, else, return 0.\nFor LBFs, we define a similar security game, denoted LFAGame. The difference\nis that Anow has oracle access to the additional algorithms in the LBF structure.\nWe note that our results hold even if we let Achoose Tdirectly (rather than\nhaving the challenger invoke B3over the set Schosen by A), as long as the\nchallenger validates that this Tsatisfies Definition 4.\nLFAGame(A, t, λ):\n1.S← A 1(1λ)such that S⊆Dand|S|=n.\n2.M←B1(S),T←B3(S), and (L, τ)←B4(T).\n3.x∗← AO\n2(1λ, S), where O={B1(·),B2(M,L, τ,·),B3(·),B4(·)}.A2can\nmake at most tqueries to B2, and any polynomial number of queries to each\nofB1,B3, andB4.\n4. If x∗/∈S∪ {x1,···, xt}andB2(M,L, τ, x∗) = 1, return 1, else, return 0.\nIf we remove A2’s oracle access to B1(·),B3(·), andB4(·), we obtain a notion\nfor thealways-bet security game, which we refer to as LABGame( A, t, λ).\n3.3 Partial Adaptive Security\nFor partial adaptivity, among the tqueries to B2,Acan choose αtof them,\nwhere α∈[0,1]. These tqueries may be part of a batch workload or a streaming\nworkload under any streaming models described by [19]. Systems incorporating\nBloom filters can operate under such a partial-adaptivity model in many real-\nworld scenarios, including content caching (e.g., as in content delivery networks)\nand database systems. For example, LSM (log-structured merge) Tree stores,\nincluding Google’s LevelDB [11] and Facebook’s RocksDB [17], use Bloom filters\nto reduce read times [6]. These stores can receive queries from both malicious\nand non-malicious users, which captures the fact that Acan observe the output\nof queries made by others while it can choose the rest.\nIn the partial adaptivity game PAGame, as in FAGame,A’s goal is to produce\na previously unseen false positive. However, this time, Acannot choose all the\nqueries; it can only choose a fraction αof them, while the remaining (1−α)t\n8\n\nqueries are uniformly sampled at random from D.A, however, still observes\nthe output of all queries. Aalso has the freedom to choose the order in which\nadversarial queries are interleaved between non-adversarial queries. We show this\nnotion first for CBFs, where cindicates A’s choice of whether to evaluate the\nadversarial query if c= 1, or a non-adversarial one if c= 0.\nPAGame( A, α, t, λ ):\n1.S← A 1(1λ)such that S⊆Dand|S|=n.\n2.M←B1(S), and set i=β= 0.\n3.(c, xi)← AB1(·)\n2(1λ, S, i, β )—A2can make any polynomial number of queries\ntoB1.\n4. If c= 1andβ < αt, give A2the output of B2(M, x i), and set β=β+ 1.\n5. Otherwise, xi←$Dand give A2the output of B2(M, x i).\n6. Set i=i+ 1. Ifi < t, go back to Step 3.\n7.A2outputs x∗. Ifx∗/∈S∪ {x1, . . . , x t}andB2(M, x∗) = 1, return 1. Other-\nwise, return 0.\nThis security game can be modified to work for LBFs as follows.\nLPAGame( A, α, t, λ ):\n1.S← A 1(1λ)such that S⊆Dand|S|=n.\n2.M←B1(S),T←B3(S),(L, τ)←B4(T), and set i=β= 0.\n3.(c, xi)← AO\n2(1λ, S, i, β ), where O={B1(·),B3(·),B4(·)}andA2can make\nany polynomial number of queries to O.\n4.Ifc= 1andβ < αt, giveA2the output of B2(M,L, τ, x i), and set β=β+ 1.\n5. Otherwise, xi←$Dand give A2the output of B2(M,L, τ, x i).\n6. Set i=i+ 1. Ifi < t, go back to Step 3.\n7.A2outputs x∗. Ifx∗/∈S∪ {x1, . . . , x t}andB2(M,L, τ, x∗) = 1, return 1.\nOtherwise, return 0.\nDefinition 2 still applies to these games with one change; now we say a Bloom\nfilter is (n, α, t, ε )-secure to account for the additional parameter α.\n3.4 Learned Bet-or-Pass Security\nThis section extends the CBF BPGamefrom Naor and Oved [21] to LBFs, denoted\nasLBPGame (the same expectation notion from Definition 3 applies here as well).\nLBPGame( A, t, λ):\n1.S← A 1(1λ)such that S⊆Dand|S|=n.\n2.M←B1(S),T←B3(S),(L, τ)←B4(T).\n3.(b, x∗)← AO\n2(1λ, S)where O={B1(·),B2(M,L, τ,·),B3(·),B4(·)}.A2can\nmake at most tqueries to B2, and any polynomial number of queries to each\nofB1,B3, andB4.\n4. Return A’s profit, CA, which is defined as\nCA=\n\n0, ifb= 0\nε−1, ifb= 1andx∗/∈S∪ {x1, . . . , x t}\nandB2(M,L, τ, x∗) = 1\n−(1−ε)−1,otherwise\n9\n\n3.5 Relationships between Security Notions\nWe investigate relationships between the security games we defined so far. For\nclarity, we focus on LBF versions of all security games. Naor et al. [15,21] inves-\ntigate similar relationships for the CBF security notions they defined, including\nABGameandBPGame.\nWe first explore connections between LFAGame andLPAGame. Notice that when\nα= 1,LPAGame is equivalent to LFAGame.LFAGame, therefore, is the special case\nofLPAGame where all queries are adversarial. The converse relationship is false,\nmeaning that LFAGame is stronger than LPAGame.\nTheorem 1. Forε∈(0,1), α∈[0,1]andn, t∈N, we have:\n(n, t, ε )-security in LFAGame =⇒(n, α, t, ε )-security in LPAGame\n(n, α, t, ε )-security in LPAGame ̸=⇒(n, t, ε )-security in LFAGame\nProof.For the first relationship, fix a PPTadversary A= (A1,A2)andt, λ∈N.\nPick any α, α′∈[0,1]such that α′≥α. Construct another adversary A′=\n(A′\n1,A′\n2)as follows. Let A′\n1(1λ, α′, t) =A1(1λ, α, t)andA′\n2(1λ, S, α′, t, β, i ) =\nA2(1λ, S, α, t, β, i ). This does not break the rules of LPAGame as still α′∈[0,1].\nSince α′≥α, the winning probability of A′is at least as high as A’s winning\nprobability. When α′= 1, we have LPAGame (A′,1, t, λ) =LFAGame (A′, t, λ), thus\nthe result follows.\nFor the second relationship, let Bbe an (n, α, t, ε )-secure LBF construction in\nLPAGame. Consider LFAGame with the same parameters (excluding α) asLPAGame.\nAlso, let B′be an alternative LBF construction such that the construction\nalgorithm B′is identical to B, while the query algorithm B′\n2differs from B2as\nfollows. Initially, B′\n2(M,L, τ,·) =B2(M,L, τ,·). However, B′\n2tracks the number\nof identical queries it receives. On receiving tidentical queries, B′\n2switches\nto always output 1for all future queries. An adversary can win LFAGame with\nnon-negligible probability by having all its tqueries over the same value x. At this\npoint,B′\n2will output 1 for any guess x∗̸=xthat the adversary outputs and wins\nthe game. Therefore, B′is not (n, α, t, ε )-secure in LFAGame. Now, consider an\nadversary AinLPAGame. To win with non-negligible probability, Amust trigger\nB′\n2to always output 1.Acan have at most t−1adversarial queries, and can\nchoose all of them to be over the same value x. However, to trigger B′\n2to always\noutput 1, the final query, which is chosen uniformly randomly by the challenger,\nmust also be x. Assuming the domain Dis large, the probability of this happens\nis negligible. Therefore, B′remains (n, α, t, ε )-secure in LPAGame. ⊓ ⊔\nWe now compare LFAGame andLPAGame toLABGame andLBPGame.\nTheorem 2. Forε∈(0,1), α∈[0,1]andn, t∈N, we have:\n(n, t, ε )-security in LFAGame =⇒(n, t, ε )-security in LABGame\n(n, α, t, ε )-security in LPAGame =⇒(n, αt, ε )-security in LABGame\n10\n\nProof.Fix a PPTadversary A, and t, λ∈N.LABGame (A, t, λ)is identical to\nLFAGame (A, t, λ)aside from the fact that Adoes not have oracle access to B1(·)\ninLABGame. Hence, A’s winning probability in LABGame (A, t, λ)cannot be greater\nthanA’s winning probability in LFAGame (A, t, λ). Following a similar argument,\nwe get an implication between LPAGame andLABGame. ⊓ ⊔\nTo disprove the converse, we introduce an (n, t, ε )-secure construction under\nLABGame and then modify it such that it remains secure under LABGame but not\nunder LFAGame. We first recall the following theorem for CBFs by Naor and\nYogev [20].\nTheorem 3 (Naor-Yogev Theorem). LetBbe an (n, ε)-Bloom filter using\nmmemory bits. If pseudorandom permutations (PRPs) exist, then for security\nparameter λthere exists a negligible function negl(·)and an (n, ε+negl(λ))-\nstrongly resilient Bloom filter in ABGamethat uses m′=m+λbits of memory.4\nThis theorem is proved for the following construction, which we denote as\nNaor-Yogev CBF, or NY-CBF. Run the initialization algorithm of a CBF with the\nsetS′={PRP sk(x) :x∈S}instead of S, where PRP skis a keyed PRP. Similarly,\nfor an element x∈D, query the filter over PRP sk(x)instead of x. This new CBF\nconstruction uses m+λbits of memory and is (n, ε+negl(λ))-secure for any\nt∈ O(poly(n, λ))under ABGame. We modify this construction in the proof for the\ntheorem below.\nTheorem 4. Forε∈(0,1), α∈[0,1],n, t∈N, and δ∈(0,1), we have:\n(n, t, ε )-security in LABGame ̸=⇒(n, t, δ )-security in LFAGame\n(n, αt, ε )-security in LABGame ̸=⇒(n, α, t, δ )-security in LPAGame\nProof.LetBbe an NY-CBF that is (n, t, ε )-secure under ABGame. Consider\na construction B′that is (n, t, ε )-secure under LABGame.B′replaces Bwith\nstandard (or conventional) LBF that uses Bas its backup CBF and has a trivial\nlearning model that responds negative (i.e., x /∈S) to all queries x∈D. Although\ncontrived, B′is a correct LBF by Definition 6. Consider a PPTadversary A.\nSince the learned representation of B′contains no information on the input set\nSand routes all queries to its backup CBF, which is a NY-CBF, AinLABGame\nhas no advantage over AinABGame. Hence, B′is also (n, t, ε )-secure in LABGame.\nNow consider a second construction B′′that is identical to B′but with\none modification: instead of storing the internal representation MlikeB′,B′′\nstores the internal representation M′= (M,sk)where skis the secret key of\nPRP.B′′is still (n, t, δ =ε+negl(λ))-secure under LABGame asAdoes not have\naccess to the internal representation of the Bloom filter. However, B′′is not\n(n, t, δ =ε+negl(λ))-secure under LFAGame orLPAGame where Ahas oracle access\ntoM′and can obtain the secret key sk. ⊓ ⊔\n4Strongly resilient means being (n, t, ε )-secure under BPGame for any t∈\nO(poly(n, λ)). For more details, see Definition 2.4 in [21].\n11\n\nBefore we show our next result, we recall a theorem by Naor and Oved [21],\nin which they proved that security in ABGamedoes not imply security in BPGame.\nTheir proof uses a counterexample construction that usually behaves like an\n(n, t, ε )-secure CBF in ABGamebut has a small probability of reaching an always-\none state, i.e., the query algorithm always outputs 1 for any query.\nTheorem 5 (Naor-Oved Theorem). Let0< ε < 1andn∈N, then for\nany0< δ < 1, assuming the existence of one-way functions, there exists a\nnon-trivial Bloom filter Bthat is (n, ε)-strongly resilient under ABGameand is\nnot(n, δ)-strongly resilient under BPGame.\nTheorem 6. Forε∈(0,1),n, t∈N, and δ∈(0,1), we have:\n(n, t, ε )-security in LABGame ̸=⇒(n, t, δ )-security in LBPGame\n(n, t, ε )-security in LBPGame =⇒(n, t, ε )-security in LFAGame andLABGame\nProof.For the first relationship, let Bbe the Naor-Oved CBF construction\nthat is (n, t, ε )-secure under ABGamebut not under BPGame. We demonstrated in\nTheorem 4 how to create a construction B′that is (n, t, ε )-secure under LABGame\nby having a trivial learning model that routes all queries to the backup CBF.\nThis backup CBF is B, which is secure under the ABGame, making the overall\nconstruction secure under the LABGame. By Theorem 5, we know that Bis not\n(n, t, δ )-secure under BPGamefor any δ∈(0,1). Since the learning model in B′is\ntrivial and adds no adversarial resilience, it follows that B′is not (n, t, δ )-secure\nunder LBPGame for any δ∈(0,1).\nFor the second relationship, Naor and Oved [21] also prove that for CBFs\n(n, t, ε )-security under BPGameimplies (n, t, ε )-security under ABGame. We use a\nproof technique similar to theirs. Let Bbe a construction that is (n, t, ε )-secure\nunder LBPGame. Fix an LBPGame adversary Athat outputs a guess x∗. LetFP\ndenote the event that x∗is a false positive. For construction B, the expected\nprofit of Ais:\nE[CA] =1\nεPr[FP]−1\n1−εPr[¬FP] =1\nεPr[FP]−1\n1−ε(1−Pr[FP])\n=\u00121\nε+1\n1−ε\u0013\nPr[FP]−1\n1−ε=1\nε(1−ε)Pr[FP]−1\n1−ε\nUsing E[CA]≤negl(λ), we obtain\n1\nε(1−ε)Pr[FP]−1\n1−ε≤negl(λ)\nSince ε∈(0,1), we have ε(1−ε)∈(0,1), and thus\nPr[FP]≤ε+ε(1−ε)negl(λ)≤ε+negl(λ)\n.\nWe have shown that the probability of x∗being a false positive is at most\nnegligibly greater than ε. This is the condition needed for Bto be (n, t, ε )-secure\nunder LFAGame andLABGame. ⊓ ⊔\n12\n\nFinally, we show that security in LFAGame does not imply security in LBPGame.\nAs mentioned before, Naor and Oved proved Theorem 5 using a CBF counterex-\nample that has a small probability of reaching an always-one state. We extend\nthis idea to LBFs and prove that security in LFAGame does not imply security in\nLBPGame. Our proof is simpler than Naor and Oved’s proof for CBFs because we\ngive the adversary an oracle access to the Bloom filter’s internal representation.\nTheorem 7. Forε∈(0,1),n, t∈N, and δ∈(0,1), we have:\n(n, t, ε )-security in LFAGame ̸=⇒(n, t, δ )-security in LBPGame\nProof.Fix an ε′∈(0, ε)and let Bbe an LBF construction that is (n, t, ε′)-secure\ninLFAGame (we show the existence of such a construction in Theorem 10). Let B′\nbe a modified construction that behaves as follows. The construction algorithm\nB′\n1flips a bit bwith probability pof being 1. Ifb= 1,B′always answers 1\nto any query. Otherwise, B′behaves identically to B. Let Mbe the internal\nclassical representation of B.B′stores M′= (M, b)as its internal classical\nrepresentation. We first show that B′is(n, t, ε )-secure in LFAGame. An adversary\nAinLFAGame is always required to output a guess x∗. Let FP denote the event\nthat x∗is a false positive. Note that Pr[FP|b= 1] = 1 while Pr[FP|b= 0]\nis same as the probability of Awinning LFAGame with construction B, i.e.,\nPr[FP|b= 0]≤ε′+negl(λ). Based on that, and by choosing p=ε−ε′\n1−ε′, we have:\nPr[FP] = Pr[FP|b= 1] Pr[ b= 1] + Pr[ FP|b= 0] Pr[ b= 0]\n≤p+ (ε′+negl(λ))(1−p)\n≤ε′+p(1−ε′) +negl(λ)\n≤ε′+ε−ε′\n1−ε′(1−ε′) +negl(λ)\n≤ε+negl(λ)\nThus,B′is(n, t, ε )-secure in LFAGame. All that is left to show is that B′\nis not (n, t, ε )-secure in LBPGame. Consider an adversary A′inLBPGame. Recall\nthatA′has oracle access to B′’s internal classical representation M′. Once B′is\nconstructed, A′can read binM′to check whether B′is in the always-one state.\nA′chooses to bet only if b= 1. Thus, we have:\nE[CA′] =E[CA′|b= 1] Pr[ b= 1] + E[CA′|b= 0] Pr[ b= 0]| {z }\n0sinceA′won’t bet\n=E[CA′|b= 1] Pr[ b= 1] = ε−1p≥p\nTherefore, E[CA′]is not negligible, violating the (n, t, ε )-security in LBPGame.⊓ ⊔\nFigure 2 summarizes the relationships that we proved in this section.\n13\n\nLFAGame LBPGame\nLPAGame LABGame× ×\n×××\n×\nFig.2: Security notion implications.\n4 Our Constructions\nWe propose two adaptively-secure LBF constructions; PRP-LBF and Cuckoo-\nLBF. In both of these constructions, we employ a Partitioned LBF [30] with a\npartition of cardinality 2. PRP-LBF combines partitioning with PRPs, while\nCuckoo-LBF combines partitioning with Cuckoo hashing and pseudorandom\nfunctions (PRFs).\nConstruction 1: PRP-LBF. As shown in Figure 3, in PRP-LBF B, the\nlearning model Land threshold τare used to partition S⊆Dinto two sets:\nS1={x∈S|L(x)≥τ}andS2=S\\S1={x∈S|L(x)< τ}. We then use\nPRP skAand PRP skBas bijections on sets S1andS2, respectively, to form sets\nSA={PRP skA(x) :x∈S1}andSB={PRP skB(x) :x∈S2}. PRP-LBF has two\nbackup CBFs, BAthat stores SAandBBthat stores SB. To query PRP-LBF\nover an element x∈D, we first evaluate the learning model over x. IfL(x)≥τ,\nthen we compute y=PRP skA(x)and pass yto the query algorithm of BAto\nobtain an answer for the membership query on whether x∈S. On the other\nhand, if L(x)< τ, then we repeat the same steps but while using PRP skBand\nthe query algorithm of BB.\nMore formally, PRP-LBF is a data structure with six components P=\n(BA,BB,B3,B4,PRP skA,PRP skB).BA= (BA1,BA2)andBB= (BB1,BB2)\nare backup CBFs (act like NY-CBF). B3is a randomized dataset construction\nalgorithmthatoninput Sconstructsatrainingdataset TforS.B4isarandomized\nlearning model construction algorithm that on input Toutputs a learning model\nLand a threshold τ∈[0,1].PRP skAandPRP skBare pseudorandom permutations\nwith secret keys skAandskB. The internal representation of Sconsists of:\n1.MA, the representation of SA={PRP skA(x) :x∈S|L(x)≥τ}stored by\nbackup CBF BA.\n2.MB, the representation of SB={PRP skB(x) :x∈S|L(x)< τ}stored by\nbackup CBF BB.\n3.(L, τ), the learning model and the threshold.\nThe query algorithm for PRP-LBF is B2(MA, MB,L, τ, x) = (L(x)≥τ∧\nBA2(MA,PRP skA(x)) = 1) ∨(L(x)< τ∧BB2(MB,PRP skB(x)) = 1). Similar to\nprior work [8,20,21], we assume that the internal state available to the adversary\ndoesnotinclude the PRP secret keys, which are held securely.\n14\n\nLearning Model PRP CBFBB\nPRP\nCBFBAInput Negatives\nPositivesNegatives\nPositives\nNegatives Positives\nFig.3: The PRP-LBF construction.\nConstruction 2: Cuckoo-LBF. This construction, shown in Figure 4, combines\npartitioning with Cuckoo hashing and PRFs. We first review prior constructions\nthat also used Cuckoo hashing, which form the basis for ours. Naor et al. [15,21]\npresent a CBF construction, that is provably secure under BPGame, based on a\nprior Cuckoo hashing-based construction by Naor and Yogev [20]. We denote\nthis construction Naor-Oved-Yogev Cuckoo filter or simply NOY-Cuckoo Filter.\nNOY-Cuckoo Filter is similar to a CBF variant called a Cuckoo filter [7]. For a\nsetS⊆Dof size n, NOY-Cuckoo Filter B= (B1,B2)stores Susing two tables\nZ1, Z2, each with nc=O(n)cells. Each table has a corresponding hash function,\ndenoted as h1, h2:D→[nc], respectively. There is a fingerprint function, namely,\na keyed pseudorandom function PRF sk:D×{1,2} → { 0,1}r, where r∈ O\u0000\nlog1\nε\u0001\nfor target FPR ε. The NOY-Cuckoo Filter works as follows:\n–Construction algorithm B1: Stores Sin a Cuckoo hashing dictionary [22],\nwhere an element xis stored in either Z1[h1(x)]orZ2[h2(x)]. To save space,\nthe PRF output is stored instead of x. In particular, if xis to be stored in\nZ1, we store y1=PRF sk(x,1). Otherwise, we store y2=PRF sk(x,2)inZ2.\n–Query algorithm B2: To query x∈D, we compare PRF sk(x,1)with Z1[h1(x)]\nandPRF sk(x,1)with Z2[h2(x)]. If either match occurs, we return 1.\nNOY-Cuckoo Filter achieves the completeness and soundness properties from\nDefinition 1. However, correctness is guaranteed only if the filter parameters are\ntuned carefully. Boskov et al. [2] show empirically that false negatives occur even\nin state-of-the-art Cuckoo filter implementations.\nWe combine NOY-Cuckoo Filter with the partitioning strategy we used for\nPRP-LBF to obtain Cuckoo-LBF. Formally, Cuckoo-LBF is a data structure with\nseven components C= (Z,H,B2,B3,B4,PRF skA,PRF skB).Z={Z1,···, Z4}is\na set of 4 tables, and H={h1,···, h4}is a set of 4 hash functions. B2,B3,B4are\npolynomial time algorithms. B2is a query algorithm, B3is a randomized dataset\nconstruction algorithm, and B4is a randomized learning model construction\nalgorithm (with the same description as before). PRF skAandPRF skBare PRFs\nwith secret keys skAand skB, respectively. The internal representation of a\nCuckoo-LBF storing a set Sconsists of the following:\n1.MA, the representation of SA={x∈S|L(x)≥τ}stored by NOY-Cuckoo\nFilterBA, including tables Z1, Z2, hash functions h1, h2, and PRF skA.\n15\n\nLearning ModelInputZ3\nPRFsk(x)\nZ4\nPRFsk(x)\nZ1\nPRFsk(x)\nZ2\nPRFsk(x)Negatives\nPositivesh3(x)\nh4(x)\nh1(x)\nh2(x)Negatives PositivesPositives\nNegatives\nFig.4: The Cuckoo-LBF construction.\n2.MB, the representation of SB={x∈S|L(x)< τ}stored by NOY-Cuckoo\nFilterBB, including tables Z3, Z4, hash functions h3, h4, and PRF skB.\n3.(L, τ), the learning model and the threshold.\nThe query algorithm B2for a Cuckoo-LBF is B2(MA, MB,L, τ, x) = (L(x)≥\nτ∧BA2(MA, x) = 1) ∨(L(x)< τ∧BB2(MB, x) = 1)whereBA2andBB2are\nquery algorithms for the NOY-Cuckoo Filters BAandBB, respectively. As before,\nwe assume that the internal state available to the adversary does notinclude\nthe PRF secret keys.\nFalse Positive Rate Analysis. We analyze FPR of both constructions in\nnon-adversarial settings. We show that for PRP-LBF P; the same analysis and\nresults hold for Cuckoo-LBF Cas both constructions use the same partitioning\nstrategy. We also note that in our analysis, FPR refers to the probability of\nthe event that some input x∈Dis a FP. Thus, in this and other sections that\nanalyze FPR, we compute the probability of this event. A false positive (FP) for\na query xhappens in PRP-LBF if any of the following holds (see Figure 5):\n1.xgenerates a FP in Land a FP in BA.\n2.xgenerates a TN (true negative) in Land a FP in BB.\nForx←$D,5letFP(x, S, m )denote the event that xis a false positive in a\nCBF that encodes the set Swith memory budget m,FPL(x, S,T, m)denote the\nevent that xis a false positive in a learning model Lgenerated using the training\ndataset T(where this dataset is generated based on S) with memory budget\nm, andTNL(x, S,T, m)denote the event that xis a true negative in a learning\nmodel Lgenerated using the training dataset T(also based on the set S) with\nmemory budget m. We assume that the correctness probability of the learning\n5The quantification of FPR here, or simply the probability a given input is FP, is\nfor non-adversarial queries, hence, we have x←$D.\n16\n\nLearning Model PRP CBFBB\nPRP\nCBFBAInputAdversarial input\nNegativesTrue negatives\nPositivesFalse positivesNegatives\nPositivesFalse positives\nNegatives Positives\nFig.5: To generate a false positive in a PRP-LBF, an adversary Amust either\ngenerate a false positive in the learning model and direct its query through\nbackup CBF BA, or generate a true negative in the learning model and direct\nits query through backup CBF BB\nmodel is independent of that of the backup CBFs. In particular, we assume that\nfor any m, m′, we have:\nPr[FPL(x, S,T, m)∩FP(x, S, m′)] = Pr[FPL(x, S,T, m)] Pr[FP(x, S, m′)]\nPr[TNL(x, S,T, m)∩FP(x, S, m′)] = Pr[TNL(x, S,T, m)] Pr[FP(x, S, m′)]\nConsider a total memory budget of mT. The memory allocation of a PRP-LBF\nfrom mTis assigned as follows: mLbits are for the learning model L,mAbits\nare for backup CBF BA,mBbits are for backup CBF BB,2λbits are for the\nPRP keys. Thus, we have mT≥mL+mA+mB+ 2λ. Based on that, we have\nthe following theorem.\nTheorem 8. Letx←$D, for any memory budget mT∈N, set S⊆D, let\nFPLBFdenote the event that xis a false positive in PRP-LBF (respectively,\nCuckoo-LBF) encoding S. The probability of this event (in non-adversarial\nsettings) is:\nPr[FPLBF(x, S,T, mT)] = Pr[FPL(x, S,T, mL)] Pr[FP(x, SA, mA)]\n+ Pr[TNL(x, S,T, mL)] Pr[FP(x, SB, mB)]\nwhere TandLare the training dataset and learning model, respectively, corre-\nsponding to S,SA={x∈S|L(x)≥τ},SB={x∈S|L(x)< τ}, and the\nprobability is taken over the random coins of Land the backup CBFs used in\nPRP-LBF (respectively, Cuckoo-LBF).\nProof.Based on the design of PRP-LBF (and Cuckoo-LBF), it follows that a\nresulting FP must either be a FP in the backup CBF encoding SAor in the\nbackup CBF encoding SB. A FP in the former must also be a FP in L, while a\nFP in the latter must be a TN in L. ⊓ ⊔\n17\n\n5 Security and Performance Analysis\nIn this section, we formally prove security of our LBF constructions. We show that\nPRP-LBF is secure under LABGame,LFAGame, and LPAGame, and that Cuckoo-\nLBF is secure under the LBPGame and LABGame. We leave studying whether\nCuckoo-LBF is secure under LFAGame andLPAGame or introducing a construction\nthat is secure against all LBF games as a future work direction.\nWe also analyze FPR (achieved for a given memory budget) of our con-\nstructions under these adversarial settings. Our results show that our LBF\nconstructions achieve strong security guarantees while maintaining competitive\nperformance (in terms of FPR achieved for a given memory budget).\n5.1 Fully Adaptive Adversary\nThis section establishes the security of PRP-LBF against fully adaptive adver-\nsaries. We first prove that NY-CBF maintains its security guarantees even when\nfacing a fully adaptive adversary who can access the filter’s internal represen-\ntation. Building on this result, we then prove that PRP-LBF preserves these\nsecurity properties in the LBF context.\nTheorem 9. LetBbe an (n, ε)NY-CBF. Assuming PRPs exist, then for a\nsecurity parameter λthere exists a negligible function negl(·)such that Bis\n(n, t, ε +negl(λ))-secure under FAGamefor any t∈ O(poly(n, λ)).\nProof.By Theorem 3, we know that NY-CBF is a correct and sound CBF that is\n(n, t, ε +negl(λ))-secure for any t∈ O(poly(n, λ))under ABGame. All that is left to\nshow is that the additional oracle access to O={B1(·),B2(M,·)}inFAGamedoes\nnot allow a PPTadversary Ato win in FAGamewith non-negligible probability.\nWe can prove this using a hybrid argument. Let Bbe a NY-CBF.\nHybrid Game H0Game: This is the original game with B.\nHybrid Game H1Game: LetB′be the same construction as Bbut with PRP sk\nreplaced with a truly random permutation π.\nSince PRPis a secure PRP, i.e., indistinguishable from a truly random per-\nmutation, thus Acannot distinguish between H0Gameand H1Game. In H1Game,\ntoA, the representation B1(π(S)) = Mπis indistinguishable from a repre-\nsentation constructed from a random set. So Acannot gain any information\nabout set SfromB1(π(S))andA’s view is identical to that in ABGame. Hence,\nPr[Awins H1Game ]≤Pr[Awins ABGame ] =ε+negl(λ).\nNow, in the original construction Bthat uses PRP sk, assuming PRPs exist, we\nhave Pr[Awins FAGame ] =Pr[Awins H1Game ] +negl(λ)=ε+negl(λ). Therefore,\nNY-CBF is (n, t, ε +negl(λ))-secure under FAGame. ⊓ ⊔\nThe following theorem shows that a PRP-LBF is (n, t, ε +negl(λ))-secure\nunder LFAGame. To prove this result, we first prove that the differences between\na PRP-LBF and a standard LBF construction still allow a PRP-LBF to be\na correct (n, τ, ε, ε p, εn)-LBF. Put differently, if there exists a standard LBF\nconstruction for any set S⊆Dof cardinality nthat satisfies the properties of an\n18\n\n(n, τ, ε, ε p, εn)-LBF, then there also exists a PRP-LBF construction that satisfies\nthose properties. We prove security by doing a case analysis that reduces the\nsecurity of PRP-LBF under LFAGame to the security of NY-CBF under FAGame.\nTheorem 10. LetBbe a standard construction for an (n, ε, ε p, εn)-LBF that\nuses mbits of memory out of which mCbits are used for the backup CBF.\nAssuming PRPs exist, then for security parameter λthere exists a negligible\nfunction negl(·)and a PRP-LBF, BskA,skB, that is (n, t, ε +negl(λ))-secure for any\nt∈ O(poly(n, λ))under LFAGame, and uses m′=m+mC+ 2λbits of memory.\nProof.Standard LBF Bcan be transformed into a PRP-LBF Pas follows.\nChoose random secret keys skA,skB∈ {0,1}λand use 2λbits of extra memory\nto store them. Use the memory budget of B’s backup CBF to construct backup\nCBFBA. Use mCextra bits to construct backup CBF BB. Keep the learning\nmodel Las is.P’s completeness follows from BB’s completeness and the fact\nthat for any xsuch that L(x)< τ,Pwill return x /∈Sif and only if the query\nalgorithm of BBoutputs 0. The soundness of BskA,skBfollows from the soundness\nofBAandBB, andP’s learning model soundness follows from L’s soundness.\nConsider a false positive (FP), i.e., an x /∈Sfor which BskA,skBreturns 1.\nThis occurs in two cases:\n–Case 1: L(x)≥τandBAreturns 1, i.e.,BA2(MA,PRP skA(x)) = 1.\n–Case 2: L(x)< τandBBreturns 1, i.e.,BB2(MB,PRP skB(x)) = 1.\nIn both cases, for adversary Ato induce a FP in P, it must induce a FP\neither in backup CBF BAor in backup CBF BB.BAandBBare NY-CBFs\nand, by Theorem 9, are (n, t, ϵ +negl(λ))-secure under FAGame. Therefore, Pis\n(n, t, ϵ +negl(λ))-secure under LFAGame. ⊓ ⊔\nWe now discuss how PRP-LBF mitigate concrete attacks discussed in the\nliterature, namely, the two attacks on LBFs by Reviriego et al. [23] (which we\nrefer to as opaque-box and clear-box attacks), as well as a general poisoning\nattack on learned index structures introduced by Kornaropoulos et al. [13].\nOpaque-boxattack. Theopaque-boxadversarialmodelissimilarto LABGame\nas both allow Ato query the LBF. However, in LABGame Achooses the LBF’s\ninput set S, whereas in Reviriego et al.’s model, Adoes not choose that. The\nopaque-box attack first tests elements until a false positive or a true positive is\nfound. They it mutates the positive by changing a small fraction of the bits in\nthe input to generate more false positives. The attack targets the learning model\nin standard LBFs by making it generate false positives without having the input\nreach the backup CBF. Unlike a standard LBF, PRP-LBF ensures both positive\nand negative queries are routed to a backup CBF that is (n, t, ε )-secure under\nABGame. This ensures that the opaque-box attack will not induce a false positive\nin PRP-LBF with probability non-negligibly greater than ε.\nClear-box attack. The clear-box adversarial model is similar to LFAGame.\nWith knowledge of the internal state of the LBF’s learning model, Agenerates\nmutations in a more sophisticated way. Reviriego et al. provide the example of a\n19\n\nmalicious URL dataset where Amay begin with a non-malicious URL and make\nchanges such as removing the \"s\"in\"https\". Since PRP-LBF is (n, t, ϵ )-secure\neven when Ahas access to oracle O={B1(·),B3(·),B4(·)}, which reveals learning\nmodel state, PRP-LBF remains secure.\nPoisoning attacks. Kornaropoulos et al. [13] discuss an attack where A\npoisons the learning model’s training dataset by having maliciously-chosen inputs\nin this set. This poisoning attack modifies the training dataset, but not the\nqueries sent to the LBF. The results of our LFAGame hold even if we let Achoose\nTas long as the challenger validates that Tsatisfies Definition 4. To accommodate\npoisoning attacks, we can let Achoose a Tthat is not validated by the challenger.\nEven in this relaxed version of LFAGame, PRP-LBF will prevent Afrom inducing\nfalse positives with probability non-negligibly larger than ε. This is because our\nsecurity guarantees do not rely on the learning model, but on the (n, t, ε )-secure\nbackup CBFs, which do not use the training dataset T.\nTheorem 11. InLFAGame, for a PPTadversary Athat outputs a guess x∗∈D,\nthe probability that x∗is a false positive in a PRP-LBF Pstoring set S⊆D\nwith training dataset Tand learning model Lis:\nPr[FPLBF(x∗, S,T, mT)]≤max(Pr[FP(x∗, SA, mA)],Pr[FP(x∗, SB, mB))]\nwhere SA={x∈S|L(x)≥τ},SB=S\\SA={x∈S|L(x)< τ},FPLBFis\nthe event denoting a false positive in P,FPis the event denoting a false positive\nin NY-CBF, mT,mA, and mBare the total memory of P, memory used by\nbackup CBF BA, and memory used by backup CBF BB, respectively, and the\nprobability is taken over the random coins of AandP.\nProof.As established in Theorem 8, x∗can only induce a FP in Pifx∗also\ninduces a FP or a TN in the learning model L. Therefore, the probability of x∗\ninducingaFPin Pwillbetheprobabilityof x∗inducingaFPinoneofthebackup\nCBFs, i.e., the probability will be Pr[FP(x∗, SA, mA)]orPr[FP(x∗, SB, mB)].\nThus, the upper bound above follows.6⊓ ⊔\n5.2 Partially Adaptive Adversary\nRecall that Theorem 1 proves that any LBF that is (n, t, ε )-secure under LFAGame\nis(n, t, ε )-secureunder LPAGame.SincePRP-LBFis (n, t, ε )-secureunder LFAGame,\nas we proved in the previous section, it is also (n, t, ε )-secure under LPAGame.\nNote that in both LFAGame and LPAGame, aPPTadversary Aoutputs a guess\nx∗∈D. The difference between LFAGame and LPAGame is the fraction αofA’s\ninitial exploratory query budget t. Therefore, for a guess x∗∈Doutput by\nadversary in LPAGame, the probability of x∗inducing a FP in PRP-LBF is the\nsame as the expression we derived for LFAGame in Theorem 11.\n6Notethatif Achooses x∗atrandomfrom D,thenthisreducestothenon-adversarial\ncase analyzed in Theorem 8. The bound in that theorem also respects the bound stated\nin Theorem 11 above.\n20\n\nAs discussed in Section 3.3, LPAGame is actually designed to capture is a\nmixed workload where a percentage of the queries are adversarial and the rest\nare non-adversarial. This is a more relevant scenario when it comes to analyzing\nreal-world performance in terms of FPR for a given memory budget under a\ngiven workload. Thus, we analyze FPR over the tqueries x1, . . . , x tinLPAGame,\ncovering the αtadversarial queries and the (1−α)tnon-adversarial queries. For\nclarity, we refer to these tqueries as workload queries to distinguish them from\nthe guess x∗.\nWithout loss of generality, let αPof the adversarial queries generate FPs in\nthe learning model that go through backup CBF BA. Similarly, let αNof the\nadversarial queries generate TNs in the learning model that go through backup\nCBFBB. Note that α=αP+αN. The adversary makes at most αPtqueries\nthat generate FPs in the learning model and at most αNtqueries that generate\nTNs in the learning model.\nTheorem 12. InLPAGame, for a workload query xi∈D, the probability that\nxiis a false positive in a PRP-LBF Pstoring set S⊆Dwith training dataset T\nand learning model Lis\nαPPr[FP(xi, SA, mA)] +αNPr[FP(xi, SB, mB)]\n+ (1−αP−αN) Pr[FPLBF(xi, S,T, mT)]\nwhere SA={x∈S|L(x)≥τ},SB=S\\SA={x∈S|L(x)< τ},FP(·)is\nthe event denoting a false positive in a CBF, FPLBF(·)is the event denoting\na false positive in P,αPis the fraction of tqueries chosen by Athat induce\nfalse positives in L, and αNis the fraction of tqueries chosen by Athat induce\nTNs in L. The probability is taken over the random coins used by A,P, and the\ngeneration of the (1−αP−αN)tnon-adversarial queries.\nProof.One of the following cases holds for any query xiamong the tworkload\nqueries in LPAGame.\n–Case 1:xiis not adversary-generated. Therefore, as established by Theorem 8,\nthese have Pr[FPLBF(xi, S,T, mT)]to be FP. There are (1−αP−αN)tsuch\nqueries.\n–Case 2:xiis adversary-generated and generates a FP in the learning model\nL(xi). Since L(xi)generating a FP and L(xi)generating a TN are mutually\nexclusiveevents,theprobabilityof xiinducingaFPin Pisjusttheprobability\nofxiinducing a FP in backup CBF BA, i.e, Pr[FP(xi, SA, mA). There are\nαPtsuch queries.\n–Case 3:xiis adversary-generated and generates a TN in L(xi). Similar to case\n2, we can derive the probability of xiinducing a FP to be Pr[FP(xi, SB, mB)].\nThere are αNtsuch queries.\nThe statement of the theorem follows by applying the law of total probability. ⊓ ⊔\n21\n\n5.3 Bet-or-Pass Adversary\nNY-CBF has only been shown to be secure under ABGame. Naor and Oved [21]\nprovide compelling reasons for why NY-CBF may not be secure under BPGame.\nWhether it is possible to modify NY-CBF in a way that makes it secure under\nBPGameis an open problem. On the other hand, NOY-Cuckoo Filter issecure\nunder BPGame, proved by Naor and Oved [21], and we recall this result below.\nTheorem 13 (NOY Theorem). Assuming one-way functions exist, for any\nn∈N, universe of size n < u, and 0< ε < 1/2, there exists a Bloom filter that is\n(n, ε)-strongly resilient in BPGameand uses O\u0000\nnlog1\nε+λ\u0001\nbits of memory. There\nexists a CBF construction B′(which is NOY-Cuckoo Filter mentioned above)\nwhere for any constant 0< ε < 1/2,B′is an (n, ε)-strongly resilient in BPGame\nand uses O\u0000\nnlog1\nε+λ\u0001\nbits of memory.\nWe now show that our Cuckoo-LBF construction is secure under LBPGame. We\nprove this using a case analysis of all the decisions available to an adversary Ain\nLBPGame. Our case analysis shows that all decision paths reduce the security of\nCuckoo-LBF under LBPGame to the security of NOY-Cuckoo Filter under BPGame.\nTheorem 14. LetBbe a standard (n, ε, ε p, εn)-LBF that uses mbits of memory\nout of which mCbits are used for the backup CBF and mLbits are used for\nthe learning model, such that mC+mL=m. Assuming one-way functions\nexist, for a security parameter λ, any n∈N, domain Dsuch that n <|D|, and\n0< ε < 0.5there exists an LBF that is (n, t,negl(λ))-secure under LBPGame for\nanyt∈ O(poly(n, λ))and uses m′=mL+O\u0000\nnlog1\nε+λ\u0001\nbits of memory.\nProof.By Theorem 13, we know that NOY-Cuckoo Filter is (n, t,negl(λ))-secure\nCBF under BPGamefor any t∈ O(poly(n, λ)), and uses O\u0000\nnlog1\nε+λ\u0001\nbits of\nmemory. Recall that in LBPGame, unlike BPGame,Ahas oracle access to the filter\nconstruction algorithm B1(·)which returns the internal representation M. Let\nBPGamePlus be a modified version of BPGamewhere Ahas oracle access to B1(·).\nWe first show that NOY-Cuckoo Filter B(that uses PRF sk) is(n, t,negl(λ))-\nsecure under BPGamePlus . We define a hybrid game H1Gamein which PRF skis\nreplaced by a true random function f. By the security of PRFs, this hybrid is\nindistinguishable from the original game that uses PRFs. We denote H1Game’s\ninternal representation of NOY-Cuckoo Filter as M′. ToA, the representation\nM′is indistinguishable from a representation constructed from a random set. So\nAcannot gain any information about the input set Sfrom M′andA’s view is\nidentical to that in BPGame. Hence Pr[Awins H1Game ]≤Pr[Awins BPGame ] =ε+\nnegl(λ). Now, in the original construction Bthat uses PRF sk, assuming PRFs exist,\nwe have Pr[Awins BPGamePlus ]≤Pr[Awins H1Game ] +negl(λ)=ε+negl(λ).\nTherefore, NOY-Cuckoo Filter remains (n, t,negl(λ))-secure under BPGamePlus\nfor any t∈ O(poly(n, λ)).\nLetCdenote the Cuckoo-LBF construction with two backup NOY-Cuckoo\nFilters BAandBB.C’s completeness follows from BB’s completeness and the\nfact that any xsuch that L(x)< τis declared to be not in SbyB2(which is\n22\n\nthe query algorithm of the Cuckoo-LBF) if and only if the query algorithm of\nBBalso outputs 0.C’s soundness follows directly from the soundness of BAand\nBB.C’s learning model soundness follows from L’s soundness. Hence, Cis a\ncorrect (n, τ, ε, ε p, εn)-LBF.\nNow, we show that the security of Cuckoo-LBF under LBPGame is reducible\nto the security of NOY-Cuckoo Filter under BPGamePlus . Let EAandEBbe the\nevents that the query goes through backup CBF BAandBB, respectively. Based\non the construction of Cuckoo-LBF, EAandEBare mutually exclusive events\nand that Pr[EA∪EB] = 1. Let CTbe the overall adversary profit. We denote by\nCAandCBthe expected adversary profit from queries that go to backup CBFs\nBAandBB, respectively. Based on that, we have:\nE[CT] =E[CA|EA] Pr[EA] +E[CB|EB] Pr[EB]≤E[CA|EA] +E[CB|EB]\nIf the total profit CTis non-negligible, it must be true that either E[CA|EA]\norE[CB|EB]is non-negligible. However, since BAandBBare NOY-Cuckoo\nFilters, by Theorem 13 and our result regarding BPGamePlus above, we know that\nE[CA|EA]andE[CB|EB]are negligible. Therefore, E[CT]is negligible meaning\nthat Cuckoo-LBF is (n, t,negl(λ))-secure under LBPGame. ⊓ ⊔\nThe probability that the guess x∗that an adversary Aoutputs be a false\npositive in Cuckoo-LBF is upper-bounded by the decision path where Aalways\nchooses to bet, i.e, b= 1. In this decision path, the false positive probability of\nLBPGame can be analyzed in a simialr way as done for LFAGame.\nTheorem 15. InLBPGame, for a PPTadversary Athat outputs a guess x∗, the\nprobability that x∗is a false positive in a Cuckoo-LBF Cstoring set Swith\nmodel Lis≤max(Pr[FP(x∗, SA, mA)],Pr[FP(x∗, SB, mB)])where SA={x∈\nS|L(x)≥τ},SB=S\\SA={x∈S|L(x)< τ},FPis the event denoting a\nfalse positive in NY-CBF, mT,mA, and mBare the total memory of C, memory\nused by backup CBF BA, and memory used by backup CBF BB, respectively,\nand the probability is taken over the random coins of AandC.\nProof.Since we assume Aalways bets and never passes, it always outputs\nx∗to be tested whether it is a FP. A case analysis identical to Theorem 11\nshows that the probability of x∗inducing a FP is either Pr[FP(x∗, SA, mA)]\norPr[FP(x∗, SB, mB)]. The upper bound follows from the fact that x∗going\nthrough backup CBF BAand backup CBF BBare mutually exclusive events. ⊓ ⊔\n6 Evaluation\nWe focus on evaluating FPR vs. memory tradeoffs of our LBF constructions in\ncomparison with known secure CBF constructions. In Section 6.1 we conduct a\nnumerical analysis for that in LPAGame and LFAGame based on our FPR model\ncovering a large number of parameters. Note that only PRP-LBF is included\nin this analysis, as proving the security of Cuckoo-LBF under LPAGame and\n23\n\nLFAGame is left to future work. To emphasize the practicality of our constructions,\nSection 6.2 evaluates PRP-LBF and Cuckoo-LBF, and compares them with Naor\net al.’s CBF constructions NY-CBF and NOY-Cuckoo Filter, for a common\nuse-case in a non-adversarial setting.\n6.1 Numerical Analysis\nWe show performance results for real-world scenarios in both the fully ( α= 1)\nand partially ( α≤1) adaptive adversarial models. The goal is to demonstrate\nscenarios where using a PRP-LBF instead of a NY-CBF yields better FPR, under\na given memory budget, while maintaining security guarantees. Broder et al. [3]\nshow that the probability of x∈Dbeing a FP in a CBF with nbbits storing\na set Susing nhhash functions, is Pr[FP(x, S, n b)] = (1 −e−nh|S|/nb)nh. We\nchoose the number of hash functions nhto be optimal nh=ln2·(nb/|S|), as\nderived in [3].\nSimilar to [14,18], we let the false positive probability of a learning model,\nPr[FPL], can be modeled as a fraction of that of a CBF storing set Sfor the\nsame memory budget (i.e., the learning model has a better FPR than a CBF):\nPr[FPL(x, S,T, m)] =c(1−e−nh|S|/nb)nhwhere c≤1. Learning models have\nboth FPs and TNs, and we note that the probability of an entry being a TN in\nthe original set is constant as we assume the input set Sdoes not change after\nconstruction. Let QNbe the fraction of TN non-adversarial queries (where the\nnumber of adversarial queries is αNand so the number of non-adversarial queries\nis|D| −αN). Thus, we have\nPr[TNL(S,T, nb)] = (1 −Pr[FPL(S,T, nb)])QN= (1−c(1−e−nh|S|/nb)nh)QN\nWe choose realistic values for our example from prior work on evaluating\nLBFs [14] on Google’s Transparency Report. We pick 2Megabytes as our memory\nbudget, m, chosen from the range of values in Figure 10 of [14]. We choose the\ncardinality of the stored set, |S|, as1.7million based on the number of unique\nURLs in Google’s Transparency Report. Kraska et al. [14] demonstrates that an\nLBF with a memory budget of 2Megabytes has 0.25of the false positive rate of\na CBF. Hence, we also use 0.25as the value for cin our calculations. Following\nprior work [20], we use 128bits as the size of our security parameter, λ. For the\ncase of a PRP-LBF, we let the learning model take 1Megabytes while dividing\nthe remaining 1Megabytes equally between backup CBFs BAandBB. The\nbackup CBFs store SAandSB, respectively, in PRP-LBF. Our chosen values are\nsummarized in Table 1. The full numerical analysis for our model uses 494lines\nofCcode and it is available at [12].\nVarying the fraction of adversarial queries. We take the fraction of ad-\nversarial queries αto be a variable ranging from 0to1. We assume a constant\nadversarial strategy , i.e., the fraction of adversarial queries that are FPs (so they\ngo through backup CBF BA) vs. the fraction of adversarial queries that are TNs\n(so they go through backup CBF BB) is constant. In particular, we assume that\nadversary Aequally divides its queries between FPs and TNs, soαP\nα= 0.5and\n24\n\nTable 1: Model parameters for PRP-LBF and NY-CBF.\nParameter Value\nmT Total memory budget 2 MB\nmL Memory budget for the learning model 1 MB\nmA Memory budget for backup CBF BA 0.5MB\nmB Memory budget for backup CBF BB 0.5MB\nn Cardinality of stored set S⊆D 1.7 Million\ncPr[FPL]of learning model\nPr[FP]of CBFfor same memory budget 0.25\nλ Security parameter 128 bits\nQN Fraction of true negative non-adversarial queries 0.5\nQNis0.5. As shown in Figure 6a, we observe that a PRP-LBF outperforms a\nNY-CBF for the same memory budget when the fraction of adversarial queries\nis less than a certain cutoff of 0.5. So, as long as adversarial traffic is at most\nhalf of the total workload of an application, PRP-LBF will offer a lower FPR\nthan NY-CBF. Note that for all our figures, the result for LFAGame is simply the\nα= 1point in the figure, whereas the entire spectrum of αvalues shows how\nFPR varies in the weaker LPAGame.\nVarying the adversarial strategy. In Figure 6b, we relax the assumption\nthat the adversary divides their queries equally between FPs and TNs. We\nshow results for all partitions of αbetween αPandαN. To see how the FPR\nof PRP-LBF is impacted, we vary the fraction of αassigned to αPfrom 0to\n1. Here, 0means all αNadversarial queries are TNs, and 1means that all αN\nadversarial queries are FPs. In this framework, recall that our first calculation\n(Figure 6a) sets this fraction to 0.5. The key observation here is that as Auses\nmore of its query budget directing traffic to the backup CBF that has the higher\nFPR, the performance of PRP-LBF degrades. Recall that the upper bound for\nthe FPR of an adversarial query is the FPR of the “weaker” backup CBF. For\nbrevity, Figure 6b only shows results when α= 0.2, i.e., 20%of the workload is\nadversarial. We note that we observed the same trend for other values of α, and\nhence, we do not include detailed results for that.\nVarying the dataset. In addition to Google’s Transparency Report, we also\nshow results in Figures 6c and 6d) for two other datasets evaluated in prior work\non LBFs [5,24,30]. These two datasets are: Malicious URLs Dataset [25] that\ncontains 223,088malicious and 428,118benign URLs, and EMBER Dataset [1]\nthat contains 300,000malicious and 400,000benign files. We change the set’s\ncardinality values, n, in Table 1. We use the same values listed in the table for all\nother model parameters. The figures show that, similar to Google’s Transparency\nReport, also for these datasets FPR increases as the fraction of adversarial queries\nαincreases.\nVarying the fraction of true negatives. Recall that in Figure 6a, we set the\nfraction of non-adversarial queries that are TNs, QN, to be 0.5. Now we relax\n25\n\n0.0 0.2 0.4 0.6 0.8 1.0\nValue of α0.000.050.100.150.20False Positive Rate\nPRP-LBF\nNY CBF(a) Results for the Google Transparency\nReport as αvaries with adversarial queries\ndivided equally between FPs and TNs:\nαP=αN.\n0.0 0.2 0.4 0.6 0.8 1.0\nValue of αP\nα0.0000.0250.0500.0750.1000.1250.150False Positive Rate\nPRP-LBF\nNY CBF(b) Results for the Google Transparency\nReport with α= 0.2while varying the\ndivision of adversarial queries between FPs\nand TNs (i.e., αP/α).\n0.0 0.2 0.4 0.6 0.8 1.0\nValue of α0.000.010.020.030.040.05False Positive Rate\nPRP-LBF\nNY CBF\n(c) Results for the Malicious URLs Dataset\n(same setting as Figure 6a).\n0.0 0.2 0.4 0.6 0.8 1.0\nValue of α0.000.010.020.030.040.05False Positive Rate\nPRP-LBF\nNY CBF(d) Results for the EMBER Dataset (same\nsetting as Figure 6b).\nFig.6: FPR comparison between PRP-LBF and NY-CBF while varying the\nfraction of adversarial queries αand the adversarial strategy αP/αfor various\ndatasets. Results for the fully adaptive model are the α= 1points in the figures,\nwhereastheentirespectrumof αvaluesshowstheresultsforthepartiallyadaptive\nmodel.\nthis assumption and show results for the entire range of values of QN∈[0,1]\nwith αtaking 4values: 0.2,0.3,0.5,1.0, such that each value partitioned equally\nbetween αPandαN. The results are shown in Figure 7.\nWeobservethatFPRincreaseswithincreasing QNbuttherateofthisincrease,\ni.e.,∂FPR\n∂QN, decreases as αincreases. This is due to the following reason. In Theo-\nrem12,when α=αP+αN→1,wehave (1−αP−αN)Pr[FPLBF(xi, S,T, mT)]→\n0, so the overall FPR of the mixed workload is dominated by the FPR of the\nbackup CBFs which does not depend on QN. On the other hand, when α→0,\nthe overall FPR of the mixed workload is dominated by FPLBFwhich increases\nwith increasing QN(see Theorem 8).\n26\n\n0.0 0.2 0.4 0.6 0.8 1.0\nValue of QN0.000.050.100.150.20False Positive Rate\nPRP-LBF\nNY CBF(a)α= 0.2\n0.0 0.2 0.4 0.6 0.8 1.0\nValue of QN0.000.050.100.150.20False Positive Rate\nPRP-LBF\nNY CBF (b)α= 0.3\n0.0 0.2 0.4 0.6 0.8 1.0\nValue of QN0.000.050.100.150.20False Positive Rate\nPRP-LBF\nNY CBF\n(c)α= 0.5\n0.0 0.2 0.4 0.6 0.8 1.0\nValue of QN0.000.050.100.150.20False Positive Rate\n PRP-LBF\nNY CBF (d)α= 1.0\nFig.7: FPR comparison between PRP-LBF and NY-CBF in the partially-adaptive\nadversarial model for the Google Transparency Report with QN∈[0,1].\n6.2 Use-case Evaluation\nWe evaluate the performance of our constructions within the context of a use case\nto get a sense of how they would perform in practice. Historically, web browsers,\nincluding Google Chrome, used a Bloom filter [9] to store a set of Malicious\nURLs. In this design, whenever a user attempts to access a URL on the web\nbrowser, the browser queries the Bloom filter for the URL. If the Bloom filter\nsays that the URL is in the Malicious URLs set, the web browser warns the user\nthat they are accessing a potentially unsafe website. The malicious URLs use\ncase has been studied by prior works on LBFs [24,30]. Thus, we evaluate this\nuse case using the same public Malicious URLs dataset [25] as prior work. This\ndataset contains around 223K malicious and around 428K benign URLs.\nImplementation and experimental setup. We implemented PRP-LBF,\nCuckoo-LBF, NY-CBF, and NOY-Cuckoo Filter in 903lines of Python 3 code,\nwhich can be found in our open-source repository [12]. The implementation allows\nany PRP/PRF implementation to be plugged in for internal use. Similar to [20],\nwe use AES to instantiate PRPs and PRFs in our implementation. In particular,\n27\n\nwe use AES in the ECB mode where the input size of the PRP/PRF is 128 bits (so\none block for AES encryption). Our implementation uses the PyCryptoDome [26]\nlibrary for these cryptographic primitives. Our implementation is also modular in\na way that allows any machine learning model to be easily plugged in. We tested\nthe correctness of our implementation on a broad range of common classification\nmodels, including the Random Forest model, Gaussian Naive Bayes, the Gradient\nBoosting Classifier, Support Vector Machine-based Classifiers, and Adaptive\nBoosting, using implementations provided by scikit-learn .\nFor our experiments, we use a set of 20features to train the learning models,\nincluding URL length, whether the URL contains an IP address, whether the\nURL uses a shortening service, whether the URL is “abnormal”, digit count and\nletter count of the URL, and whether the URL contains special symbols. This set\nof features for the Malicious URLs Dataset is common in open-source learning\nmodels, and similar features have been used by prior work [30]. In the case of\nCuckoo filter-based constructions, we use fingerprints of size 4bits, 2as the table\nsize constant factor, and 5000maximum eviction attempts in the Cuckoo hashing\ntables. For Cuckoo filter-based constructions, we also skip elements that cannot\nbe inserted after the maximum eviction attempts have been reached.\nWe calculate FPR of all constructions by uniformly randomly sampling 10%\nof URLs that are notmalicious from the Malicious URLs Dataset and counting\nhow many of them are returned as FPs by these constructions. The amount of\nmemory the learning model uses is measured as the serialized size in bytes of\nthe trained classifier, using joblib [27]. After subtracting the memory used by\nthe learning model from the memory budget, we divide the remaining memory\nbudget equally between backup CBFs BAandBBin our constructions.\nResults. Figure 8a shows how FPR varies for NY-CBF and PRP-LBF as we\nmodify the memory budget. This figure uses the Gaussian Naive-Bayes Classifier\nas the learning model. Figure 8b shows the same results using a Linear Support\nVector Classifier as the learning model. Similarly, Fig. 8c and Figure 8d show how\nFPR varies for NOY-Cuckoo Filter and Cuckoo-LBF as we modify the memory\nbudget available for the Gaussian Naive-Bayes Classifier and the Linear Support\nVector Classifier as the learning model, respectively.\nWhileourimplementationsarenotfocusedonoptimization,weseeaconsistent\ntrend of our LBF constructions having lower FPRs than CBF constructions for\nthe same memory budget. This is consistent with prior work [14] that shows a\nsimilar trend between non-adversarial LBF constructions and non-adversarial\nCBF constructions. An interesting outlier is the FPR of Cuckoo-LBF being\nslightly larger than the FPR of NOY-Cuckoo Filter in one of the data points\nof Figure 8c. We conjecture that this is due to our naive method of equally\ndistributing the memory leftover, i.e., after taking out the memory needed for\nthe learning model, between the backup CBFs. We leave investigating better\nmemory allocation strategies and other optimizations to future work.\nLarge memory budgets. We explore how the trend of Figure 8 continues as\nwe keep increasing the memory budget to the point where it no longer becomes\na bottleneck for FPR. To better understand this, we conduct a second set of\n28\n\n0 1 2 3 4\nMemory Budget (Mbits)10−1\n10−2\n10−3\n10−4False Positive Rate\nNY CBF\nPRP-LBF(a) Gaussian Naive-Bayes Classifier\n0 1 2 3 4\nMemory Budget (Mbits)10−1\n10−2\n10−3\n10−4False Positive Rate\nNY CBF\nPRP-LBF (b) Linear Support Vector Classifier\n0 1 2 3 4\nMemory Budget (Mbits)10−310−2False Positive Rate\nNOY-Cuckoo Filter\nCuckoo-LBF\n(c) Gaussian Naive-Bayes Classifier\n0 1 2 3 4\nMemory Budget (Mbits)10−3False Positive Rate\nNOY-Cuckoo Filter\nCuckoo-LBF (d) Linear Support Vector Classifier\nFig.8: FPR of our LBF constructions compared to NY-CBF and NOY-Cuckoo\nFilter, with varying memory budget and learning models for the Malicious URLs\nDataset.\nexperiments over a much larger range of memory budgets with the results shown\nin Figure 9. We observe that the trend from Figure 8 continues. FPR for both\nour LBF constructions and the CBF constructions eventually approaches the\nsame value as the memory budget increases.\nAcknowledgments\nThe work of G.A. is supported by NSF Grant No. CNS-2226932.\nReferences\n1.Anderson, H.S., Roth, P.: Ember: an open dataset for training static pe malware\nmachine learning models. arXiv preprint arXiv:1804.04637 (2018)\n2.Boskov, N., Trachtenberg, A., Starobinski, D.: Birdwatching: False negatives in\ncuckoo filters. In: Proceedings of the Student Workshop. pp. 13–14 (2020)\n29\n\n0.4 0.8 2.0 4.0 8.0 16.0 32.0 64.0\nMemory Budget (Mbits)0.00.10.20.30.4False Positive Rate\nNY CBF\nPRP-LBF(a) Gaussian Naive-Bayes Classifier\n0.4 0.8 2.0 4.0 8.0 16.0 32.0 64.0\nMemory Budget (Mbits)0.00.10.20.30.4False Positive Rate\nNY CBF\nPRP-LBF (b) Linear Support Vector Classifier\n0.4 0.8 2.0 4.0 8.0 16.0 32.0 64.0\nMemory Budget (Mbits)0.0000.0010.0020.0030.0040.0050.006False Positive Rate\nNOY-Cuckoo Filter\nCuckoo-LBF\n(c) Gaussian Naive-Bayes Classifier\n0.4 0.8 2.0 4.0 8.0 16.0 32.0 64.0\nMemory Budget (Mbits)0.0000.0010.0020.0030.0040.0050.006False Positive Rate\nNOY-Cuckoo Filter\nCuckoo-LBF (d) Linear Support Vector Classifier\nFig.9: FPR of our LBF constructions compared to NY-CBF and NOY-Cuckoo\nFilter with large memory budgets and different learning models (for the Malicious\nURLs Dataset).\n3.Broder, A., Mitzenmacher, M.: Network applications of bloom filters: A survey.\nInternet mathematics 1(4), 485–509 (2004)\n4.Clayton, D., Patton, C., Shrimpton, T.: Probabilistic data structures in adversarial\nenvironments. In: ACM SIGSAC Conference on Computer and Communications\nSecurity (CCS) (2019)\n5.Dai, Z., Shrivastava, A.: Adaptive learned bloom filter (ada-bf): Efficient utilization\nof the classifier with application to real-time information filtering on the web. In:\nAdvances in Neural Information Processing Systems (NeurIPS) (2020)\n6.Dayan, N., Athanassoulis, M., Idreos, S.: Monkey: Optimal navigable key-value\nstore. In: Proceedings of the 2017 ACM International Conference on Management\nof Data. pp. 79–94 (2017)\n7.Fan, B., Andersen, D.G., Kaminsky, M., Mitzenmacher, M.D.: Cuckoo filter: Prac-\ntically better than bloom. In: Proceedings of the 10th ACM International on\nConference on emerging Networking Experiments and Technologies. pp. 75–88\n(2014)\n8.Filic, M., Paterson, K.G., Unnikrishnan, A., Virdia, F.: Adversarial correctness\nand privacy for probabilistic data structures. In: ACM SIGSAC Conference on\nComputer and Communications Security (CCS) (2022)\n30\n\n9.Gerbet, T., Kumar, A., Lauradoux, C.: On the (in) security of google safe browsing.\nINRIA ePrint (2014)\n10.Gerbet, T., Kumar, A., Lauradoux, C.: The power of evil choices in bloom filters. In:\nIEEE/IFIP International Conference on dependable systems and networks (2015)\n11.Google: LevelDB Bloom Filter. https://github.com/google/leveldb/blob/mai\nn/util/bloom.cc , accessed: 2023-05-04\n12.Open Source Implementation and Experiments. https://github.com/jadidbour\nbaki/permuted-partitioned-lbf (2025)\n13.Kornaropoulos, E.M., Ren, S., Tamassia, R.: The price of tailoring the index to\nyour data: Poisoning attacks on learned index structures. In: Proceedings of the\n2022 International Conference on Management of Data. pp. 1331–1344 (2022)\n14.Kraska, T., Beutel, A., Chi, E.H., Dean, J., Polyzotis, N.: The case for learned\nindex structures. In: International Conference on Management of Data (2018)\n15.Lotan, C., Naor, M.: Adversarially robust bloom filters: Monotonicity and betting.\nIACR Communications in Cryptology 2(1) (2025)\n16.Mangal, A., Roy, S., Akinapelli, S., Singh, A., Boulon, J.: How roblox reduces\nspark join query costs with machine learning optimized bloom filters. https:\n//corp.roblox.com/newsroom/2023/11/roblox-reduces-spark-join-query-c\nosts-machine-learning-optimized-bloom-filters\n17.Meta: RocksDB Bloom Filter. https://github.com/facebook/rocksdb/blob/mai\nn/util/dynamic_bloom.h , accessed: 2023-05-04\n18.Mitzenmacher, M.: A model for learned bloom filters and optimizing by sandwiching.\nAdvances in neural information processing systems 31(2018)\n19.Muthukrishnan, S.: Data streams: algorithms and applications. Found. Trends\nTheor. Comput. Sci. 1(2), 117–236 (2005)\n20.Naor, M., Eylon, Y.: Bloom filters in adversarial environments. ACM Transactions\non Algorithms (TALG) 15(3), 1–30 (2019)\n21.Naor, M., Oved, N.: Bet-or-pass: Adversarially robust bloom filters. In: Theory of\nCryptography Conference (2022)\n22.Pagh, R., Rodler, F.F.: Cuckoo hashing. In: European Symposium on Algorithms.\npp. 121–133. Springer (2001)\n23.Reviriego, P., Hernández, J.A., Dai, Z., Shrivastava, A.: Learned bloom filters in\nadversarial environments: A malicious url detection use-case. In: IEEE International\nConference on High Performance Switching and Routing (HPSR). IEEE (2021)\n24.Sato, A., Matsui, Y.: Fast partitioned learned bloom filter. In: International Con-\nference on Neural Information Processing Systems (NeurIPS) (2024)\n25.Siddharta, M.: Malicious URLs Dataset. https://www.kaggle.com/datasets/si\nd321axn/malicious-urls-dataset\n26. Source, O.: PyCryptoDome library. https://www.pycryptodome.org/\n27. Source, O.: Joblib. https://joblib.readthedocs.io (2025)\n28.Tarkoma, S., Rothenberg, C.E., Lagerspetz, E.: Theory and practice of bloom filters\nfor distributed systems. IEEE Communications Surveys & Tutorials 14(1), 131–155\n(2012)\n29.Tirmazi, H.: Lsm trees in adversarial environments. arXiv preprint arXiv:2502.08832\n(2025)\n30.Vaidya, K., Knorr, E., Mitzenmacher, M., Kraska, T.: Partitioned learned bloom\nfilters. In: International Conference on Learning Representations (2021)\n31\n\nGenerative AI Disclosure\nThe paper was written directly by the authors. The student author used ChatGPT\nby OpenAI and Claude by Anthropic for feedback on technical writing and to\nedit and debug the Ti kZ and matplotlib figure code in the L ATEX manuscript.\nThe artifacts of our work were primarily written and verified by the authors. The\nstudent author used Cursor IDE, which includes a generative AI code assistant,\nfor the following tertiary coding tasks: unit-test generation, code refactoring,\nfeedback on bug-fixing, feedback on anonymizing the artifact, and code reviews.\n32",
  "textLength": 78357
}