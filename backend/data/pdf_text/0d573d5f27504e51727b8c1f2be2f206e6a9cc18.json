{
  "paperId": "0d573d5f27504e51727b8c1f2be2f206e6a9cc18",
  "title": "In-network Neural Networks",
  "pdfPath": "0d573d5f27504e51727b8c1f2be2f206e6a9cc18.pdf",
  "text": "In-network Neural Networks\nGiuseppe Siracusano, Roberto Bifulco\nNEC Laboratories Europe\n1 INTRODUCTION\nNetwork devices, such as switches and routers, process data at rates\nof terabits per second, forwarding billions of network packets per\nsecond. Recently, such devices’ switching chips have been enhanced\nto support new levels of programmability [ 3]. Leveraging these new\ncapabilities, a switching chip’s packets classiﬁcation and modiﬁca-\ntion tasks can now be adapted to implement custom functions. For\nexample, researchers have proposed approaches that rethink load\nbalancers [ 11], key-value stores [ 7], and consensus protocols [ 5] op-\nerations. In general, there is a trend to ofﬂoad to the switching chips\n(parts of) functions typically implemented in commodity servers,\nthereby achieving new levels of performance and scalability.\nThese solutions often ofﬂoad some data classiﬁcation tasks, en-\ncoding relevant information, e.g., the keyof a key-value store en-\ntry [10], in network packets’ headers. Unlike packets’ payload, the\nheader values can be parsed and processed by the switching chips,\nwhich perform classiﬁcation using lookup tables. While providing\nvery high throughput, lookup tables need to be ﬁlled with entries that\nenumerate the set of values used to classify packets, and therefore\nthe table’s size directly correlates to the ability to classify a large\nnumber of patterns. Unfortunately, the amount of memory used for\nthe tables is hard to increase, since it is the main cost factor in a\nnetwork device’s switching chip [ 3], accounting for more than half\nof the chip’s silicon resources.\nIn this paper, we explore the feasibility of using an artiﬁcial\nneural network (NN) model as classiﬁer in a switching chip, as a\ncomplement to existing lookup tables. A NN can better ﬁt the data\nat hand, potentially reducing the memory requirements at the cost\nof extra computation [ 9]. Here, our work builds on the observation\nthat, while adding memory is expensive, adding circuitry to perform\ncomputation is much cheaper. For reference, in a programmable\nswitching chip the entire set of computations is implemented using\nless then a tenth of the overall chip’s area.\nTo this end, we implement N2Net, a system to run NNs on a\nswitching chip. We provide the following contributions: ﬁrst, we\nshow that a modern switching chip is already provided with the prim-\nitives required to implement the forward pass of quantized models\nsuch as binary neural networks , and that performing such computa-\ntion is feasible at packets processing speeds; second, we provide an\napproach to efﬁciently leverage the device parallelism to implement\nsuch models; third, we provide a compiler that, given a NN model,\nautomatically generates the switching chip’s conﬁguration that im-\nplements it. Our experience shows that current switching chips can\nrun simple NN models, and that with little additions a chip design\ncould easily support more complex models, thereby addressing a\npotentially larger set of applications.\nUse cases At the time of writing we are still in the process of im-\nplementing full-ﬂedged applications, thus, we just mention our two\ninitial use cases, postponing to a later publication a throughout tech-\nnical description. First, similar to [ 9], we envision the use of a\nneural network classiﬁer to implement packet classiﬁcation inside\nFigure 1: A schematic view of a switching chip’s pipeline.\nthe chip, e.g., to create large white/blacklist indexes for Denial of\nService protection. Second, the outcome of the NN classiﬁcation\ncan be encoded in the packet header and used in an end-to-end sys-\ntem, to provide ”hints” to a more complex processor located in a\nserver, e.g., on how to handle the packet’s payload to optimize data\nlocality/cache coherency or to support load balancing [15].\nTo encourage the community in exploring more use cases, we are in\nthe process of making N2Net code publicly available.\n2 N2NET\nSwitching chip primer Switching chips process network packets\nto take a forwarding decision, e.g., forward to a given port or drop.\nThey also perform transformations to the packet header values, e.g.,\nto decrement a time-to-live ﬁeld. We use RMT [ 3] as representative\nmodel of state-of-the-art switching chips (Cf. Fig. 1). When a packet\nis received, an RMT chip parses several 100s bytes of its header to\nextract protocol ﬁelds’ values, e.g., IP addresses. These values are\nwritten to a packet header vector (PHV) that is then processed by\na pipeline of elements that implement match-action tables. Each\nelement has a limited amount of memory to implement lookup tables\n(the match part), and hundreds of RISC processors that can read\nand modify the PHV in parallel (the action part). The values in the\nPHV are used to perform table lookups and retrieve the instruction\nthe processors should apply. To provide very high performance,\nthese processors implement only simple operations, such as bitwise\nlogic, shifts and simple arithmetic (e.g., increment, sum). Using a\nlanguage such as P4 [ 2], the chip can be programmed to deﬁne the\nparser logic and the actions performed on the PHV . In particular,\nthe actions are deﬁned as a combination of the simpler primitives\nmentioned earlier.\nDesign The limited set of arithmetic functions supported by a switch-\ning chip does not enable the implementation of the multiplications\nand activation functions usually required by a NN. However, simpli-\nﬁed models designed for application in resource-limited embedded\ndevices, such as binary neural networks (BNNs), do not require such\ncomplex arithmetic, especially during the forward pass [ 4]. In our\ncase, we select models that only use bitwise logic functions, such as\nXNOR, the Hamming weight computation (POPCNT), and the SIGNarXiv:1801.05731v1  [cs.DC]  17 Jan 2018\n\nFigure 2: Implementation of a 3 neurons BNN processing.\nfunction as activation function. While research in these models is at\nits early stages, it shows already promising results [6, 8, 13].\nActivations (bits) 16 32 64 128 256 512 1024 2048\nParallel neur. (max) 128 64 32 16 8 4 2 1\nElements number 12 14 16 18 20 22 24 25\nTable 1: Maximum number of parallel neurons and required\nnumber of elements for different activations vector sizes.\nN2Net implements the forward pass of a BNN, and assumes that\nthe BNN activations are encoded in a portion of the packet header.\nThe header is parsed as soon as a packet is received, and the parsed\nactivations vector is placed in a PHV’s ﬁeld. Fig. 2 summarizes the\noperations performed by N2Net to implement a 3 neurons BNN. The\nentire process comprises ﬁve steps:\n\u000fReplication : in the ﬁrst step the activations are replicated in the\ndestination PHV as many times as the number of neurons that\nshould be processed in parallel;\n\u000fXNOR and Duplication : in the second step such ﬁelds are pro-\ncessed by the element’s RISC processors. The applied actions\nperform XNOR operations on the activations taking as parameters\nthe neurons’ weights. The results are stored twice in the destina-\ntion PHV . This duplication is required by the implementation of\nthe POPCNT as explained next;\n\u000fPOPCNT : RMT does not support a POPCNT primitive operation.\nA naive implementation using an unrolled forcycle that counts\nover the vector bits may require a potentially big number of\nelements. Instead, we adapted a well-known algorithm that counts\nthe number of 1 bits performing additions of partial counts in\na tree pattern [ 1]. The advantage of this algorithm is that it\ncan perform some computations in parallel, while using only\nshifts, bitwise logic and arithmetic sums. N2Net implements such\nalgorithm combining two pipeline’s elements. The ﬁrst element\nperforms shift/bitwise AND in parallel on the two copies of the\ninput vector. Each copy contains the mutually independent leaves\nof the algorithm’s tree structure. The second element performs\nthe SUM on the outcome of the previous operations. Depending\non the length of the activation vector, there may be one or more\ngroups of these two elements, in which case the sum’s result is\nagain duplicated in two destination PHV’s ﬁelds.\n\u000fSIGN : the fourth step implements the sign operation verifying\nthat the POPCNT result is bigger or equal to half the length of\nthe activations vector. The result is a single bit stored in the least\nsigniﬁcant bit of the destination PHV’s ﬁeld.\n\u000fFolding : the last step folds together the bits resulting from the\nSIGN operations to build the ﬁnal Y vector, which can be used as\ninput for a next sequence of 5 steps.N2Net currently implements fully-connected BNNs taking a model\ndescription (number of layers, neurons per layer) and creating a P4\ndescription that modiﬁes/replicates the above ﬁve steps as needed.\nBNN are relatively small models whose weights ﬁt in the pipeline\nelement’s SRAMs, however, we are required to pre-conﬁgure the\nweights. This is similar to the BrainWave’s approach [12].\nEvaluation Our implementation is subject to two main constraints.\nFirst, the PHV is 512B long. Since we use the PHV to store the\nBNN input, the maximum activation vector length is 2048 (i.e.,\nhalf the PHV’s size, 256B, since we perform the duplication step).\nSmaller activation vectors enable the parallel execution of multiple\nneurons, using the replication step of Fig. 2. For example, with a\n32b activation vector, up to 64 neurons can be processed in paral-\nlel. Second, the RMT pipeline has 32 elements, and each element\ncan only perform one operation on each of the PHV’s ﬁelds (for a\nmaximum of 224 parallel operations on independent ﬁelds in each\nelement). While we implement the POPCNT leveraging parallelism\nin order to minimize the number of required elements, we still need\n3+2lo/afii10069.ital2¹Nºelements to implement a single neuron, where N is\nthe size of the activations vector (cf. Table1). Using the previous\nexamples, the execution of a neuron with 2048 activations would\nrequire 25 elements, while with a 32b activations vector we would\ntake just 13 elements. Furthermore, in this last case the addition of\nthe replication step (i.e., an additional element) would correspond to\nthe parallel execution of up to 64 neurons using only 14 out of the\n32 pipeline’s elements.\nIn terms of throughput, an RMT pipeline can process 960 million\npackets per second. Since we encode in one packet our activations,\nN2Net enables the processing of 960 million neurons per second,\nwhen using 2048b activations. Processing smaller activations en-\nables higher throughput because of parallel processing.\nTo put this in perspective, considering the above constraints, we\ncould run about a billion small BNNs per second, i.e., one per each\nreceived packet. For instance, we could run 960 million two-layers-\nBNNs per second, using 32b activations (e.g., the destination IP\naddress of the packet), and two layers of 64 and 32 neurons.\n3 CHALLENGES AND OUTLOOK\nN2Net shows that BNN models can be implemented with already\navailable switching chip technology. However, models complexity\nis limited by relevant constraints. In turn, this limits the possible\napplications. We argue these constraints are the outcome of an archi-\ntecture that was not designed to run NN models, and that supporting\nthem would require relatively cheap design changes.\nFor example, implementing a simple POPCNT primitive on 32b\noperands requires few additional logic gates [ 16] but could cut the\nnumber of required pipeline’s elements. I.e., this would change the\n12-25 elements range of Table1 to a 5-10 range. Also, this removes\nthe need for the duplication step, immediately doubling the available\nspace in the PHV , hence doubling the neurons executed in parallel.\nFurthermore, the circuitry dedicated to computation (including\nparsers) accounts for less than 10% of the switching chip’s area.\nUsing 5-10 pipeline’s elements to implement BNN computations\ntakes less than a third of that circuitry. Thus, adding a dedicated\ncircuitry for the execution of BNN computations is likely to account\nfor less than a 3-5% increase in the overall chip area costs.\n2\n\nOverall, we believe N2Net has the potential to open a new inter-\nesting ﬁeld of applications, contributing a novel building block for\nfuture networked systems [14].\nREFERENCES\n[1]M. Beeler, R. W. Gosper, and R. Schroeppel. Hakmem. Technical report, Cam-\nbridge, MA, USA, 1972.\n[2] P. Bosshart, D. Daly, G. Gibb, M. Izzard, N. McKeown, J. Rexford, C. Schlesinger,\nD. Talayco, A. Vahdat, G. Varghese, et al. P4: Programming protocol-independent\npacket processors. ACM SIGCOMM CCR , 44(3):87–95, 2014.\n[3] P. Bosshart, G. Gibb, H.-S. Kim, G. Varghese, N. McKeown, M. Izzard, F. Mujica,\nand M. Horowitz. Forwarding metamorphosis: Fast programmable match-action\nprocessing in hardware for sdn. In Proceedings of the ACM SIGCOMM 2013\nConference on SIGCOMM , SIGCOMM ’13, pages 99–110, New York, NY , USA,\n2013. ACM.\n[4]M. Courbariaux and Y . Bengio. Binarynet: Training deep neural networks with\nweights and activations constrained to +1 or -1. CoRR , abs/1602.02830, 2016.\n[5] H. T. Dang, M. Canini, F. Pedone, and R. Soul ´e. Paxos made switch-y. SIGCOMM\nComput. Commun. Rev. , 46(2):18–24, May 2016.\n[6]I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y . Bengio. Binarized\nneural networks. In D. D. Lee, M. Sugiyama, U. V . Luxburg, I. Guyon, and\nR. Garnett, editors, Advances in Neural Information Processing Systems 29 , pages\n4107–4115. Curran Associates, Inc., 2016.\n[7]X. Jin, X. Li, H. Zhang, R. Soul ´e, J. Lee, N. Foster, C. Kim, and I. Stoica. Net-\ncache: Balancing key-value stores with fast in-network caching. In Proceedings of\nthe 26th Symposium on Operating Systems Principles , SOSP ’17, pages 121–136,\nNew York, NY , USA, 2017. ACM.\n[8]M. Kim and P. Smaragdis. Bitwise neural networks. CoRR , abs/1601.06071,\n2016.\n[9]T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned\nindex structures. 2017.\n[10] X. Li, R. Sethi, M. Kaminsky, D. G. Andersen, and M. J. Freedman. Be fast,\ncheap and in control with switchkv. In 13th USENIX Symposium on Networked\nSystems Design and Implementation (NSDI 16) , pages 31–44, Santa Clara, CA,\n2016. USENIX Association.\n[11] R. Miao, H. Zeng, C. Kim, J. Lee, and M. Yu. Silkroad: Making stateful\nlayer-4 load balancing fast and cheap using switching asics. In Proceedings\nof the Conference of the ACM Special Interest Group on Data Communication ,\nSIGCOMM ’17, pages 15–28, New York, NY , USA, 2017. ACM.\n[12] Microsoft. Microsoft unveils project brainwave for real-time ai. https://www.\nmicrosoft.com/en-us/research/blog/microsoft-unveils-project-brainwave/.\n[13] M. Rastegari, V . Ordonez, J. Redmon, and A. Farhadi. Xnor-net: Imagenet\nclassiﬁcation using binary convolutional neural networks. CoRR , abs/1603.05279,\n2016.\n[14] A. Sapio, I. Abdelaziz, A. Aldilaijan, M. Canini, and P. Kalnis. In-network\ncomputation is a dumb idea whose time has come. In Proceedings of the 16th\nACM Workshop on Hot Topics in Networks, Palo Alto, CA, USA, HotNets 2017,\nNovember 30 - December 01, 2017 , pages 150–156, 2017.\n[15] N. K. Sharma, A. Kaufmann, T. Anderson, A. Krishnamurthy, J. Nelson, and\nS. Peter. Evaluating the power of ﬂexible packet processing for network resource\nallocation. In 14th USENIX Symposium on Networked Systems Design and Imple-\nmentation (NSDI 17) , pages 67–82, Boston, MA, 2017. USENIX Association.\n[16] A. Sivaraman, A. Cheung, M. Budiu, C. Kim, M. Alizadeh, H. Balakrishnan,\nG. Varghese, N. McKeown, and S. Licking. Packet transactions: High-level\nprogramming for line-rate switches. In ACM SIGCOMM ’16 , ACM SIGCOMM\n’16, pages 15–28. ACM, 2016.\n3",
  "textLength": 15603
}