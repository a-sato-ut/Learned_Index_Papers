{
  "paperId": "adf5fedf79029d589228de834f701eb1190b0be5",
  "title": "Benchmarking learned indexes",
  "pdfPath": "adf5fedf79029d589228de834f701eb1190b0be5.pdf",
  "text": "Benchmarking Learned Indexes\nRyan Marcus13, Andreas Kipf1, Alexander van Renen2, Mihail Stoian2,\nSanchit Misra3, Alfons Kemper2, Thomas Neumann2, Tim Kraska1\n1MIT CSAIL2TUM3Intel Labs\nfryanmarcus, kipf, kraska g@mit.edu frenen, stoian, kemper, neumann g@in.tum.de sanchit.misra@intel.com\nABSTRACT\nRecent advancements in learned index structures propose\nreplacing existing index structures, like B-Trees, with ap-\nproximate learned models. In this work, we present a uni-\n\fed benchmark that compares well-tuned implementations\nof three learned index structures against several state-of-the-\nart \\traditional\" baselines. Using four real-world datasets,\nwe demonstrate that learned index structures can indeed\noutperform non-learned indexes in read-only in-memory work-\nloads over a dense array. We also investigate the impact of\ncaching, pipelining, dataset size, and key size. We study the\nperformance pro\fle of learned index structures, and build\nan explanation for why learned models achieve such good\nperformance. Finally, we investigate other important prop-\nerties of learned index structures, such as their performance\nin multi-threaded systems and their build times.\n1. INTRODUCTION\nWhile index structures are one of the most well-\nstudied components of database management systems, re-\ncent work [12, 19] provided a new perspective on this\ndecades-old topic, showing how machine learning techniques\ncan be used to develop so-called learned index structures.\nUnlike their traditional counterparts (e.g., [10,15,16,20,31,\n33]), learned index structures build an explicit model of the\nunderlying data to provide e\u000bective indexing.\nSince learned index structures have been proposed, they\nhave been criticized [26,27]. The main reasons for these crit-\nicisms were the lack of an e\u000ecient open-source implemen-\ntation of the learned index structure, inadequate data-sets,\nand the lack of a standardized benchmark suite to ensure a\nfair comparison between the di\u000berent approaches.\nEven worse, the lack of an open-source implementation\nforced researchers to re-implement the techniques of [19],\nor only use back-of-the-envelop calculations, to compare\nagainst learned index structures. While not a bad thing\nper se , it is easy to leave the baseline unoptimized, or make\nother unrealistic assumptions, even with the best of inten-\ntions, potentially rendering the main takeaways void.\nFor example, recently Ferragina and Vinciguerra proposed\nthe PGM index [13], a learned index structure with interest-\ning theoretical properties, which is recursively built bottom-\nup. Their experimental evaluation showed that the PGM-\nindex was strictly superior to traditional indexes as well as\ntheir own implementation of the original learned index [19].\nThis strong result surprised the authors of [19], who had ex-\nperimented with bottom-up approaches and usually found\nthem to be slower to execute (see Section 3.4 for a discussionwhy this may be case). This motivated us to investigate if\nthe results of [13] would hold against tuned implementations\nof the original learned index [19] and other structures.\nFurther complicating matters, learned structures have\nan \\unfair\" advantage on synthetic datasets, as synthetic\ndatasets are often surprisingly easy to learn. Hence, it is\noften easy to show that a learned structure outperforms the\nmore traditional approaches just by using the right kind of\ndata. While this is also true for almost any benchmark, it\nis much more pronounced for learned algorithms and data\nstructures as their entire goal is to automatically adjust to\nthe data distribution and even the workload.\nIn this paper, we try to address these problems on three\nfronts: (1) we provide a \frst open-source implementation of\nRMIs for researchers to compare against and improve upon,\n(2) we created a repository of several real-world datasets and\nworkloads for testing, and (3) we created a benchmarking\nsuite, which makes it easy to compare against learned and\ntraditional index structures. To avoid comparing against\nweak baselines, our open-source benchmarking suite [5] con-\ntains implementations of index structures that are either\nwidespread, tuned by their original authors, or both.\nUnderstanding learned indexes. In addition to provid-\ning an open source benchmark for use in future research,\nwe also tried to achieve a deeper understanding of learned\nindex structures, extending the work of [17].\nFirst, we present a Pareto analysis of three recent learned\nindex structures (RMIs [19], PGM indexes [13], and RS in-\ndexes [18]) and several traditional index structures, includ-\ning trees, tries, and hash tables. We show that, in a warm-\ncache, tight-loop setting, all three variants of learned in-\ndex structures can provide better performance/size tradeo\u000bs\nthan several state-of-the-art traditional index structures.\nWe extend this analysis to multiple dataset sizes, 32 and\n64-bit integers, and di\u000berent search techniques (i.e., binary\nsearch, linear search, interpolation search).\nSecond, we analyze why learned index structures achieve\nsuch good performance. While we were unable to \fnd a\nsingle metric that fully explains the performance of an in-\ndex structure (it seems intuitive that such a metric does not\nexist), we o\u000ber a statistical analysis of performance coun-\nters and other properties. The single most important ex-\nplanatory variable was cache misses, although cache misses\nalone are not enough for a statistically signi\fcant expla-\nnation. Surprisingly, we found that branch misses do not\nexplain why learned index structures perform better than\ntraditional structures, as originally claimed in [19]. In fact,\nwe found that both learned index structures and traditional\n1arXiv:2006.12804v2  [cs.DB]  29 Jun 2020\n\nLookup Key\nIndex\nStructure1\n3\n9\n12\n56\n57\n58\n95\n98\n99Data\n (sorted)\n(e.g., 72)\nSearch bound1A query for a particular key \nis made.\n2An index structure maps the lookup \nkey to a search bound, which must \ncontain the correct index.3Given a valid search \nbound, a search \nfunction (e.g., binary \nsearch) is used to \nlocate the correct \nindex within the search \nbound.Figure 1: Index structures map each lookup key to a search\nbound . This search bound must contain the \\lower bound\"\nof the key (i.e., the smallest key greater than or equal to\nthe lookup key). The depicted search bound is valid for the\nlookup key 72 because the key 95 is in the bound. A search\nfunction, such as binary search, is used to locate the correct\nindex within the search bound.\nindex structures use branching e\u000eciently.\nThird, we analyze the performance of a wide range of in-\ndex structures in the presence of memory fences, cold caches,\nand multi-threaded environments, to test their behavior un-\nder more realistic settings. In all scenarios, we found that\nlearned approaches perform surprisingly well.\nHowever, our study is not without its limitations. We fo-\ncused only on read-only workloads, and we tested each index\nstructure in isolation (e.g., a lookup loop, not with integra-\ntion into any broader application). While this certainly does\nnot cover all potential use cases, in-memory performance is\nincreasingly important, and many write-heavy DBMS archi-\ntectures are also moving towards immutable read-only data-\nstructures (for example, see LSM-trees in RocksDB [4,21]).\nHence, we believe our benchmark can still guide the design\nof many systems to come and, more importantly, serve as\na foundation to develop benchmarks for mixed read/write\nworkloads and the next generation of learned index struc-\ntures which supports writes [11,13,14].\n2. FORMULATION & DEFINITIONS\nAs depicted in Figure 1, we de\fne an index structure I\nover a zero-indexed sorted array Das a mapping between\nan integer lookup key x2Zand a search bound (lo;hi )2\n(Z+\u0002Z+), where Z+is the positive integers and zero:\nI:Z!(Z+\u0002Z+)\nWe do not consider indexes over unsorted data, nor do we\nconsider non-integer keys. We assume that data is stored in\na way supporting fast random access (e.g., an array).\nSearch bounds are indexes into D. A valid index structure\nmaps any possible lookup key xto a bound that contains\nthe \\lower bound\" of x: the smallest key in Dthat is greater\nthan or equal to x. Formally, we de\fne the lower bound of\na keyx,LB(x), as:\nLB(x) =i$[Di\u0015x^:9j(j <i^Dj\u0015x)]\nAs a special case, we de\fne the lower bound of any key\ngreater than or equal to the largest key in Das one more\nthan the size of D:LB(maxD) =jDj. Our de\fnition of\n\\lower bound\" corresponds to the C++ standard [2].\n1\n3\n9\n12\n56\n57\n58\n95\n98\n990.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0 20 40 60 80 100\nCDF Input (key)0.00.20.40.60.81.0CDF Output (relative position)Approximation\nCDF\nCDF FunctionDataRelative positionFigure 2: The cumulative distribution function (CDF) view\nof a sorted array.\nWe say that an index structure is valid if and only if it\nproduces search bounds that contain the lower bound for\nevery possible lookup key.\n8x2Z[I(x) = (lo;hi )!Dlo\u0014LB(x)\u0014Dhi]\nIntuitively, this view of index structures corresponds to\nanapproximate index , an index that returns a search range\ninstead of the exact position of a key. We are not the \frst\nto note that both traditional structures like B-Trees and\nlearned index structures can be viewed in this way [8,19].\nGiven a valid index, the actual index of the lower bound\nfor a lookup key is located via a \\last mile\" search (e.g.,\nbinary search). This last mile search only needs to examine\nthe keys within the provided search bound (e.g., Figure 1).\n2.1 Approximating the CDF\nLearned index structures use machine learning techniques\nranging from deep neural networks to simple regression in\norder to model the cumulative distribution function , or CDF,\nof a sorted array [19]. Here, we use the term CDF to mean\nthe function mapping keys to their relative position in an\narray. This is strongly connected to the traditional interpre-\ntation of the CDF from statistics: the CDF of a particular\nkeyxis the proportion of keys less than x. Figure 2 shows\nthe CDF for some example data.\nGiven the CDF of a dataset, \fnding the lower bound of a\nlookup key xin a dataset Dwith a CDF CDF Dis trivial:\none simply computes CDF D(x)\u0002jDj. Learned index struc-\ntures function by approximating the CDF of the dataset us-\ning learned models (e.g., linear regressions). Of course, such\nlearned models are never entirely accurate. For example,\nthe blue line in Figure 2 represents one possible imperfect\napproximation of the CDF. While imperfect, this approx-\nimation has a bounded error: the largest deviation from\nthe blue line to the actual CDF occurs at key 12, which\nhas a true CDF value of 0.4 but an approximated value\nof 0.24. The maximum error of this approximation is thus\n0:4\u00000:24 = 0:16 (some adjustments may be required for\nlookups of absent keys). Given this approximation function\nAand the maximum error of A, we can de\fne an index\nstructureIAas such:\nIA(x) = (A(x)\u0000jDj\u00020:16;A(x) +jDj\u00020:16)\nIn other words, we can use the approximation of the CDF\nas an index structure by estimating the position of a given\nkey and then computing the search bound of that estimate\n2\n\nlinear\ncubic1cubic2cubicn...Stage 1\nStage 2Figure 3: A recursive model index (RMI). The linear model\n(stage 1) makes a coarse-grained prediction. Based on this,\none of the cubic models (stage 2) makes a re\fned prediction.\nusing the maximum error of the approximation. Note that\nthis technique, while utilizing approximate machine learning\ntechniques, never produces an incorrect search bound .\nOne can view a B-Tree as a way of memorizing the CDF\nfunction for a given dataset: a B-Tree in which every nth\nkey is inserted can be viewed as an approximate index with\nan error bound of n\u00001. At one extreme, if every key is\ninserted into the B-Tree, the B-Tree perfectly maps any pos-\nsible lookup key to its position in the underlying data (an\nerror bound of zero). Instead, one can insert every other key\ninto a B-Tree in order to reduce the size of the index. This\nresults in a B-Tree with an error bound of one: any location\ngiven by the B-Tree can be o\u000b by at most one position.\n3. LEARNED INDEX STRUCTURES\nIn this work, we evaluate the performance of three dif-\nferent learned index structures: recursive model indexes\n(RMI), radix spline indexes (RS), and piecewise geometric\nmodel indexes (PGM). We do not compare with a number\nof other learned index structures [11, 14, 24] because tuned\nimplementations could not be made publicly available.\nWhile all three of these techniques approximate the CDF\nof the underlying data, the way these approximations are\nconstructed vary. We next give a high-level overview of each\ntechnique, followed by a discussion of their di\u000berences.\n3.1 Recursive model indexes (RMI)\nOriginally presented by Kraska et al. [19], RMIs use\na multi-stage model, combining simpler machine learning\nmodels together. For example, as depicted in Figure 3, an\nRMI with two stages, a linear stage and a cubic stage, would\n\frst use a linear model to make an initial prediction of the\nCDF for a speci\fc key (stage 1). Then, based on that pre-\ndiction, the RMI would select one of several cubic models to\nre\fne this initial prediction (stage 2).\nStructure. When all keys can \ft in memory, RMIs with\nmore than two stages are almost never required [22]. Thus,\nhere we explain only two-stage RMIs for simplicity. See [19]\nfor a generalization to nstages. A two-stage RMI is a CDF\napproximator Atrained onjDjdata points (key / index\npairs). The RMI approximator Ais composed of a single\n\frst stage model f1, andBsecond-stage models fi\n2. The\nvalueBis referred to as the \\branching factor\" of the RMI.\nFormally, the RMI is de\fned as:\nA(x) =fbB\u0002f1(x)=jDjc\n2 (x) (1)\nIntuitively, the RMI \frst uses the stage-one model f1(x)\nto compute a rough approximation of the CDF of the input\nkeyx. This coarse-grained approximation is then scaled be-\ntween 0 and the branching factor B, and this scaled value\nis used to select a model from the second stage, fi\n2(x). The\n01234567\nKeyIndex4718310     \n1011 1000 0100 11112 Lookup key:\nRadix table\nSpline point\nCDFPointerFigure 4: A radix spline index. A linear spline is used to ap-\nproximate the CDF of the data. Pre\fxes of resulting spline\npoints are indexed in a radix table to accelerate the search\non the spline. Figure from [18].\nselected second-stage model is used to produce the \fnal ap-\nproximation. The stage-one model f1(x) can be thought of\nas partitioning the data into Bbuckets, and each second-\nstage model fi\n2(x) is responsible for approximating the CDF\nof only the keys that fall into the ith bucket.\nChoosing the correct models for both stages ( f1andf2)\nand selecting the best branching factor for a particular\ndataset depends on the desired memory footprint of the RMI\nas well as the underlying data. In this work, we use the CDF-\nShop [22] auto-tuner to determine these hyperparameters.\nTraining. Let (x;y)2Dbe the set of key / index pairs in\nthe underlying data. Then, an RMI is trained by adjusting\nthe parameters contained in f1(x) andfi\n2(x) to minimize:\nX\n(x;y)2D(F(x)\u0000y)2(2)\nIntuitively, minimizing Equation 2 is done by training\n\\top down\": \frst, the stage one model is trained, and then\neach stage 2 model is trained to \fne-tune the prediction.\nDetails can be found in [19] and our implementation at [1].\n3.2 Radix spline indexes (RS)\nAn RS index [18] consists of a linear spline [25] that ap-\nproximates the CDF of the data and a radix table that in-\ndexes resulting spline points (cf., Figure 4). In contrast to\nRMI [19], and similar to FITing-Tree [14] and PGM [13], RS\nis built in a bottom-up fashion. Uniquely, RS can be built\nin a single pass with a constant worst-case cost per element\n(PGM provides a constant amortized cost per element).\nStructure. As depicted in Figure 4, RS consists of a radix\ntable and a set of spline points that de\fne a linear spline over\nthe CDF of the data. The radix table indexes r-bit pre\fxes\nof the spline points and serves as an approximate index over\nthe spline points. Its purpose is to accelerate binary searches\nover the spline points. The radix table is represented as an\narray containing 2ro\u000bsets into the sorted array of spline\npoints. The spline points themselves are represented as key\n/ index pairs. To locate a key in a spline segment, linear\ninterpolation between the two spline points is used.\nUsing the example in Figure 4, a lookup in RS works as\nfollows: First, the rmost signi\fcant bits bof the lookup key\nare extracted ( r= 3 andb= 101). Then, the extracted bits\nbare used as an o\u000bset into the radix table to retrieve the\n3\n\n1\n3\n9\n12\n56\n57\n58\n95\n98\n99Data \nKey: 1 Model: f1\nKey: 56 Model: f2\nKey: 95 Model: f31\n56\n95Key: 56 Model: f5Key: 1 Model: f4PGM Level 1 PGM Level 2Figure 5: A piecewise geometric model (PGM) index.\no\u000bsets stored at the bth and theb+1th position (e.g., the 5th\nand the 6th position). Next, RS performs a binary search\nbetween the two o\u000bsets on the sorted array of spline points\nto locate the two spline points that encompass the lookup\nkey. Once the relevant spline segment has been identi\fed,\nit uses linear interpolation between the two spline points to\nestimate position of the lookup key in the underlying data.\nTraining. To build the spline layer, RS uses a one-pass\nspline \ftting algorithm [25] that is similar to the shrink-\ning cone algorithm of FITing-Tree [14]. The spline algo-\nrithm guarantees a user-de\fned error bound. At a high\nlevel, whenever the current error corridor exceeds the user-\nsupplied bound, a new spline point is created. Whenever\nthe spline algorithm encounters a new r-bit pre\fx, a new\nentry is inserted into the pre-allocated radix table.\nRS has only two hyperparameters (spline error and num-\nber of radix bits), which makes it straightforward to tune.\nIn practice, few con\fgurations need to be tested to reach a\ndesired performance / size tradeo\u000b on a given dataset [18].\n3.3 Piecewise geometric model indexes (PGM)\nThe PGM index is a multi-level structure, where each level\nrepresents an error-bounded piecewise linear regression [13].\nAn example PGM index is depicted in Figure 5. In the\n\frst level, the data is partitioned into three segments, each\nrepresented by a simple linear model ( f1;f2;f3). By con-\nstruction, each of these linear models predicts the CDF of\nkeys in their corresponding segments to within a preset er-\nror bound. The partition boundaries of this \frst level are\nthen treated as their own sorted dataset, and another error-\nbounded piecewise linear regression is computed. This is\nrepeated until the top level of the PGM is su\u000eciently small.\nStructure. A piecewise linear regression partitions the data\ninton+ 1 segments with a set of points p0;p1;:::;p n. The\nentire piecewise linear regression is expressed as a piecewise\nfunction:\nF(x) =8\n>>>>><\n>>>>>:a0\u0002x+b0ifx<p 0\na1\u0002x+b1ifx\u0015p0andx<p 1\na2\u0002x+b2ifx\u0015p1andx<p 2\n:::\nan\u0002x+bnifx\u0015pnandx<p n\nEach regression in the PGM index is constructed with\na \fxed error bound \u000f. Such a regression can trivially be\nused as an approximate index. PGM indexes apply this\ntrick recursively, \frst building an error-bounded piecewiseregression model over the underlying data, then building\nanother error-bounded piecewise regression model over the\npartitioning points of the \frst regression. Key lookups are\nperformed by searching each index layer until the regression\nover the underlying data is reached.\nTraining. Each regression is constructed optimally, in the\nsense that the fewest pieces are used to achieve a preset max-\nimum error. This can be done quickly using the approach\nof [32]. The \frst regression is performed on the underly-\ning data, resulting in a set of split points (the boundaries\nof each piece of the regression) and regression coe\u000ecients.\nThese split points are then treated as if they were a new\ndataset, and the process is repeated, resulting in fewer and\nfewer pieces at each level. Since each piecewise linear regres-\nsion contains the fewest possible segments, the PGM index\nis optimal in the sense of piecewise linear models [13].\nIntuitively, PGM indexes are constructed \\bottom-up\":\n\frst, an error bound is chosen, and then a minimal piece-\nwise linear model is found that achieves that error bound.\nThis process is repeated until the piecewise models become\nsmaller than some threshold. The PGM index can also han-\ndle inserts, and can be adapted to a particular query work-\nload. We do not evaluate either capability here.\n3.4 Discussion\nRMIs, RS indexes, and PGM indexes all provide an ap-\nproximation of the CDF of some underlying data using ma-\nchine learning techniques. However, the speci\fcs vary.\nModel types. While RS indexes and PGM indexes use\nonly a single type of model (spline regression and piecewise\nlinear regression, respectively), RMIs can use a wide variety\nof model types. This gives the RMI a greater degree of\n\rexibility, but also increases the complexity of tuning the\nRMI. While both the PGM index and RS index can be tuned\nby adjusting just two knobs, automatically optimizing an\nRMI requires a more involved approach, such as [22]. Both\nthe PGM index authors and the RS index authors mention\nintegrating other model types as future work [13,18].\nTop-down vs. bottom-up. RMIs are trained \\top down\",\n\frst \ftting the topmost model and training subsequent lay-\ners to correct errors. PGM and RS indexes are trained \\bot-\ntom up\", \frst \ftting the bottommost layer to a \fxed accu-\nracy and then building subsequent layers to quickly search\nthe bottommost layer for the appropriate model. Because\nboth PGM and RS indexes require searching this bottom-\nmost layer (PGM may require searching several intermediate\nlayers), they may require more branches or cache misses than\nan RMI. While an RMI uses its topmost model to directly\nindex into the next layer, avoiding a search entirely, the bot-\ntommost layer of the RMI does not have a \fxed error bound;\nany bottom-layer model could have a large maximum error.\nRS indexes and PGM indexes also di\u000ber in how the bot-\ntommost layer is searched. PGM indexes decompose the\nproblem recursively, essentially building a second PGM in-\ndex on top of the bottommost layer. Thus, a PGM in-\ndex may have many layers, each of which must be searched\n(within a \fxed range) during inference. On the other hand,\nan RS index uses a radix table to narrow the search range,\nbut there is no guarantee on the search range's size. If the\nradix table provides a comparable search range as the up-\nper level of a PGM index, then an RS index locates the\nproper \fnal model with a comparatively cheaper operation\n4\n\nMethod Updates Ordered Type\nPGM [13] Yes Yes Learned\nRS [18] No Yes Learned\nRMI [19] No Yes Learned\nBTree [7] Yes Yes Tree\nIBTree [15] Yes Yes Tree\nFAST [16] No Yes Tree\nART [20] Yes Yes Trie\nFST [33] Yes Yes Trie\nWormhole [31] Yes Yes Hybrid hash/trie\nCuckooMap [6] Yes No Hash\nRobinHash [3] Yes No Hash\nRBS No Yes Lookup table\nBS No Yes Binary search\nTable 1: Search techniques evaluated\n(a bitshift and an array lookup). If the radix table does\nnot provide a narrow search range, signi\fcant time may be\nspent searching for the appropriate bottom-layer model.\n4. EXPERIMENTS\nOur experimental analysis is divided into six sections.\n1. Setup (Section 4.1): we describe the index structures,\nbaselines, and datasets used.\n2. Pareto analysis (Section 4.2): we analyze the size and\nperformance tradeo\u000bs o\u000bered by each index structure,\nincluding variations in dataset and key size. We \fnd that\nlearned index structures o\u000ber competitive performance.\n3. Explanatory analysis (Section 4.3): we analyze indexes\nvia performance counters (e.g., cache misses) and other\ndescriptive statistics. We \fnd that no single metric can\nfully account for the performance of learned structures.\n4. CPU interactions (Section 4.4): we analyze how CPU\ncache and operator reordering impacts the performance\nof index structures. We \fnd that learned index struc-\ntures bene\ft disproportionately from these e\u000bects.\n5. Multithreading (Section 4.5): we analyze the through-\nput of each index in a multithreaded environment. We\n\fnd that learned structures have comparatively high\nthroughput, possibly attributable to the fact that they\nincur fewer cache misses per lookup.\n6. Build times (Section 4.6): we analyze the time to build\neach index structure. We \fnd that RMIs are slow to\nbuild compared to PGM and RS indexes, but that (un-\nsurprisingly) no learned structure yet provides builds as\nfast as insert-optimized traditional index structures.\n4.1 Setup\nExperiments are conducted on a machine with 256 GB of\nRAM and an Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz.\n4.1.1 Indexes\nIn this section, we describe the index structures we evalu-\nate, and how we tune their size/performance tradeo\u000bs. Ta-\nble 1 lists each technique and its capabilities.\nLearned indexes. We compare with RMIs, PGM indexes,\nand RadixSpline indexes (RS), each of which are described\n0.00.51.01.52.0Offset1e8 amzn face\n0.0 0.2 0.4 0.6 0.8 1.0\nNormalized Key0.00.51.01.52.0Offset1e8 osm\n0.0 0.2 0.4 0.6 0.8 1.0\nNormalized KeywikiFigure 6: CDF plots of each testing dataset. The face\ndataset contains\u0019100 large outlier keys, not plotted.\nin Section 3. We use implementations tuned by each struc-\nture's original authors. RMIs are tuned using CDFShop [22],\nan automatic RMI optimizer. RS and PGM are tuned by\nvarying the error tolerance of the underlying models.\nTree structures. We compare with several tree-structured\nindexes: the STX B-Tree (BTree) [7], an interpolating BTree\n(IBTree) [15], the Adaptive Radix Trie (ART) [20], the Fast\nArchitectural-Sensitive Tree (FAST) [16], Fast Succinct Trie\n(FST) [33], and Wormhole [31].\nFor each tree structure, we tune the size/performance\ntradeo\u000b by inserting a subset of the data as described in\nSection 2.1. To build a tree of maximum size with per-\nfect accuracy, we insert every key. To build a tree with a\nsmaller size and decreased accuracy, we insert every other\nkey. We note that this technique, while simple, may not\nbe the ideal way to trade space for accuracy in each tree\nstructure. Speci\fcally, ART may admit a smarter method\nin which keys are retained or discarded based on the \fll level\nof a node. We only evaluate the simple and universal tech-\nnique of inserting fewer keys into each structure, and leave\nstructure-speci\fc optimizations to future work.\nHashing. While most hash tables do not support range\nqueries, hash tables are still an interesting point of compari-\nson due to their unmatched lookup performance. Unordered\nhash tables cannot be shrunk using the same technique as\nwe use for trees.1Therefore, we only evaluate hash tables\nthat contain every key. We evaluate a standard implemen-\ntation of a Robinhood hash table (RobinHash) [3] and a\nSIMD-optimized Cuckoo map (CuckooMap) [6].\nBaselines. We also include two naive baselines: binary\nsearch (BS), and a radix binary search (RBS). Radix binary\nsearch [17] stores only the radix table used by the learned\nRS approach. We vary the size of the radix table to achieve\ndi\u000berent size/performance tradeo\u000bs.\n4.1.2 Datasets\nWe use four real-world datasets for our evaluation. Each\ndataset consists of 200 million unsigned 64-bit integer keys.\nWe test larger datasets in Section 4.2.1, and we test 32-\nbit datasets in Section 4.2.2. We generate 8-byte (random)\n1Wormhole [31], which we evaluate, represents a state-of-\nthe-art ordered hashing approach.\n5\n\npayloads for each key. For each lookup, we compute the sum\nof these values to ensure the results are accurate.\n\u000famzn : book popularity data from Amazon. Each key\nrepresents the popularity of a particular book.\n\u000fface: randomly sampled Facebook user IDs [30]. Each\nkey uniquely identi\fes a user.\n\u000fosm: cell IDs from Open Street Map. Each key repre-\nsents an embedded location.\n\u000fwiki: timestamps of edits from Wikipedia. Each key\nrepresents the time an edit was committed.\nThe CDFs of each of these datasets are plotted in Figure 6.\nThe zoom window on each plot shows 100 keys. While the\n\\zoomed out\" plots appear smooth, each CDF function is\nmuch more complex, containing both structure and noise.\nFor each dataset, we generate 10M random lookup keys.\nIndexes are required to return search bounds that contain\nthe lower bound of each lookup key (see Section 2).\nWhy not test synthetic datasets? Synthetic datasets\nare often used to benchmark index structures, learned or\notherwise [13,19,20]. However, synthetic datasets are prob-\nlematic for evaluating learned index structures. Synthetic\ndatasets are either (1) entirely random, in which case there\nis no possibility of learning an e\u000bective model of the under-\nlying data (although a model may be able to over\ft to the\nnoise), or (2) drawn from a known distribution, in which\ncase learning the distribution is trivial. Here, we focus only\non datasets drawn from real world distributions, which we\nbelieve are the most important. For readers speci\fcally in-\nterested in synthetic datasets, we refer to [17].\n4.2 Pareto analysis\nA primary concern of index structures is lookup perfor-\nmance: given a query, how quickly can the correct record\nbe fetched? However, size is also important: with no lim-\nits, one could simply store a lookup table and retrieve the\ncorrect record with only a single cache miss. Such a lookup\ntable would be prohibitively large in many cases, such as 64-\nbit keys. Thus, we consider the performance / size tradeo\u000b\nprovided by each index structure, plotted in Figure 7.\nFor each index structure, we selected ten con\fgurations\nranging from minimum to maximum size. While di\u000berent\napplications may weigh performance and size di\u000berently, all\napplications almost surely desire a Pareto optimal index:\nan index for which no alternative has both a smaller size\nand improved performance. For the amzn andwiki datasets,\nlearned structures are Pareto optimal up to a size of 100MB,\nat which point the RBS lookup table becomes e\u000bective. For\nface, learned structures are Pareto optimal throughout.\nPoor performance on osm.Both traditional and learned\nindex structures fail to outperform RBS on the osmdataset\nfor nearly any size. The poor performance of learned in-\ndex structures can be attributed to the osm's dataset lack\nof local structure: even small pieces of the CDF exhibit\ndi\u000ecult-to-model erratic behavior. This is an artifact of the\ntechnique used to project the Earth into one-dimensional\nspace (a Hilbert curve). In Section 4.3, we con\frm this in-\ntuition by analyzing the errors of the learned models; all\nthree learned structures required signi\fcantly more storage\nto achieve errors comparable to those observed on the other\ndatasets. Simply put, learned structures perform poorly onosmbecause osmis di\u000ecult to learn. Because osmis a one-\ndimensional projection of multi-dimensional data, a multi-\ndimensional learned index [24] may yield improvements.\nPerformance of PGM. In [13], the authors showed that\n\\the PGM-index dominates RMI,\" contradicting our previ-\nous experience that the time spent on searches between the\nlayers of the index outweighed the bene\fts of having a lower\nerror. Indeed, in our experimental evaluation we found that\nthe PGM index performs signi\fcantly worse than RMI on 3\nout of the 4 datasets and slightly worse on osm. After con-\ntacting the authors of [13], we found that their RMI imple-\nmentation was missing several key optimizations: their RMI\nonly used linear models rather than tuning di\u000berent type of\nmodels as proposed in [19,22], and omitted some optimiza-\ntions for RMIs with only linear models.2This highlights\nhow implementation details can a\u000bect experimental results,\nand the importance of having a common benchmark with\nstrong implementations. We stress that our results are the\n\frst to compare RMI and PGM implementations tuned by\ntheir respective authors.\nPerformance of RBS. RBS exhibits substantially de-\ngraded performance on face compared to other datasets.\nThis is due to a small number ( \u0019100) of outliers in the\nface dataset: most keys fall within (0 ;250), but the outliers\nfall in (259;264\u00001). These outliers cause the \frst 16 pre\fx\nbits of the RBS lookup table to be nearly useless. One could\nadjust RBS to handle this simple case (when all outliers are\nat one end of the dataset), but in general such large jumps\nin values represents a severe weakness of RBS. ART [20] can\nbe viewed as a generalization of RBS to handle this type of\nskew regardless of where it occurs in the dataset.\nOn other datasets, RBS is surprisingly competitive, often\noutperforming other indexes. This is partially explained by\nthe low inference time required by RBS: getting a search\nbound requires only a bit shift and an array lookup. When\nthe pre\fxes of keys are distributed uniformly across a range,\nan RBS with a radix table of size 2bprovides equally accu-\nrate bounds as a binary search tree with blevels, but requires\nonly a single cache miss. When the keys are heavily skewed\n(as is the case with face), the radix table is nearly useless.\nTree structures are non-monotonic. All tree structures\ntested (ART, BTree, IBTree, and FAST) become less ef-\nfective after a certain size. For example, the largest ART\nindex for the amzn data occupies nearly 1GB of space, but\nhas worse lookup performance than an ART index occupy-\ning only 100MB of space. This is because, at a certain point,\nperforming a binary search on a small densely-packed array\nbecomes more e\u000ecient than traversing a tree. As a result,\ntree structures show non-monotonic behavior in Figure 7.\nIndexes slower than binary search? At extremely small\nor large sizes, some index structures perform worse than bi-\nnary search. In both cases, this is because some index struc-\ntures are unable to provide su\u000eciently small search bounds\nto make up for the inference time required. For example, on\ntheosmdataset, very small RMIs barely narrow down the\nsearch range at all. Because this small RMIs \ft is so poor\n2We shared our RMI implementation with Ferragina and Vin-\nciguerra before the publication of [13], but since [13] was already\nundergoing revision, they elected to continue with their own RMI\nimplementation instead, without note. All PGM results in this\npaper are based on Ferragina and Vinciguerra's tuned PGM code\nas of May 18th, 2020.\n6\n\n102\n100\nSize (MB)0200400600800Lookup time (ns)amzn\n102\n100\nSize (MB)face\n102\n100\nSize (MB)osm\n102\n100\nSize (MB)wiki\nRMI\nPGM\nRS\nRBS\nART\nBTree\nIBTree\nFASTFigure 7: Performance and size tradeo\u000bs provided by several index structures for four di\u000berent datasets. The black horizontal\nline represents the performance of binary search (which has a size of zero). Extended plots with all techniques are available\nhere: https://rm.cab/lis1\n101\n101\nSize (MB)50010001500Lookup time (ns)amzn\n101\n101\nSize (MB)face\nRMI\nBTree\nFST\nWormhole\nFigure 8: Performance of index structures built for strings\n(stars) on our integer datasets.\nMethod Time Size\nPGM 326.48 ns 14.0 MB\nRS 266.58 ns 4.0 MB\nRMI 180.90 ns 48.0 MB\nBTree 482.11 ns 166.0 MB\nIBTree 446.55 ns 9.0 MB\nFAST 435.33 ns 102.0 MB\nBS 741.69 ns 0.0 MB\nCuckooMap 114.50 ns 1541.0 MB\nRobinHash 93.69 ns 6144.0 MB\nTable 2: The fastest variant of each index structure com-\npared against two hashing techniques on the amzn dataset.\n(analyzed later, Figure 12), the time required to execute the\nRMI model and produce the search bound is comparatively\nworse than executing a binary search on the entire dataset.\nStructures for strings. Many recent works on index struc-\ntures have focused on indexing keys of arbitrary length (e.g.,\nstrings) [31, 33]. For completeness, we evaluated two struc-\ntures designed for string keys { FST and Wormhole { in\nFigure 8. Unsurprisingly, neither performed as well as bi-\nnary search. These string indexes contain optimizations that\nassume that comparing two keys is expensive. These opti-\nmizations translate to overhead when considering only in-\nteger keys, which can be compared in a single instruction.\nART, an index designed for both string and integer data,\ndoes so by indexing one key-byte per radix tree level.\nHashing. Hashing provides O(1) time point lookups. How-\never, hashing di\u000bers from both traditional and learned in-dexes in a number of ways: \frst, hashing generally does not\nsupport lower bound lookups.3Second, hash tables gener-\nally have a large footprint, as they store every key. We eval-\nuate two hashing techniques { a Cuckoo hash table [6] and\na Robinhood hash table [3]. We found that a load factor of\n0.99 and 0.25 (respectively) maximized lookup performance.\nTable 2 lists the size and lookup performance of the best-\nperforming (and thus often largest) variant of each index\nstructure and both hashing techniques for a 32-bit version4\nof the amzn dataset (results similar for others). Unsurpris-\ningly, both hashing techniques o\u000ber superior point-lookup\nlatency compared to traditional and learned index struc-\ntures. This decreased latency comes at the cost of a larger\nin-memory footprint. For example, CuckooMap provides a\n114ns lookup time compared to the 180ns provided by the\nRMI, but CuckooMap uses over 1GB of memory, whereas\nthe RMI uses only 48MB. When range lookups and memory\nfootprint are not concerns, hashing is a clear choice.\n4.2.1 Larger datasets\nFigure 9 shows the performance / size tradeo\u000b for each\nlearned structure and a BTree for four di\u000berent data sizes\nof the amzn dataset, ranging from 200M to 800M. All three\nlearned structures are capable of scaling to larger dataset\nsizes, with only a logarithmic slowdown (as is expected from\nthe \fnal binary search step). For example, consider an\nRMI that produces an average search bound that spans 128\nkeys. Such a bound requires 7 steps of binary search. If the\ndataset size doubles, an RMI of equal size is likely to return\nbounds that are twice as large: one could expect an RMI\nof equal size to produce search bounds that span 256 keys.\nSuch a bound requires only 8 total (1 additional) binary\nsearch steps. Thus, learned index structures scale to larger\ndatasets in much the same way as BTrees. If larger datasets\nhave more pronounced and modelable patterns, learned in-\ndex structures may provide better scaling.\n4.2.2 32-bit datasets\nOther sections evaluate 64-bit datasets. Here, we scale\ndown the amzn dataset from 64 to 32 bits, and compare the\n3Wormhole, evaluated in Figure 8, is a hash-based technique that\nprovides ordering, but is primarily optimized for strings.\n4The SIMD Cuckoo implementation only supports 32-bit keys.\n7\n\n101\n102\nSize (MB)50010001500Lookup time (ns)RMI, amzn\n101\n102\nSize (MB)PGM, amzn\n101\n102\nSize (MB)RS, amzn\n101\n102\nSize (MB)BTree, amzn\n200M\n400M\n600M\n800MFigure 9: Performance / size tradeo\u000bs for datasets of various sizes (200M, 400M, 600M, and 800M keys) for the amzn dataset.\nThe face and wiki datasets were not su\u000eciently large to compare. Extended plots with all techniques and the osmdataset\nare available here: https://rm.cab/lis2\n102\n100102\nSize (MB)50010001500Lookup time (ns)amzn\nRMI (32-bit)\nRMI (64-bit)\n102\n100102\nSize (MB)amzn\nRS (32-bit)\nRS (64-bit)\n102\n100102\nSize (MB)amzn\nPGM (32-bit)\nPGM (64-bit)\n102\n100102\nSize (MB)amzn\nBTree (32-bit)\nBTree (64-bit)\n102\n100102\nSize (MB)amzn\nFAST (32-bit)\nFAST (64-bit)\nFigure 10: Performance / size tradeo\u000b for 32 and 64 bit keys. While decreasing the key size to 32-bits has a minimal impact\non learned structures, the ability to pack more values into a single cache line improve the performance of tree structures.\nperformance of the three learned index structures, BTrees,\nand FAST. The results are plotted in Figure 10.\nFor learned structures, the performance on 32-bit data\nis nearly identical to performance on 64-bit data. Our im-\nplementations of RS and RMI both transform query keys\nto 64-bit \roats, so this is not surprising. We attempted\nto perform computations on 32-bit keys using 32-bit \roats,\nbut found that the decreased precision caused \roating point\nerrors. The PGM implementation uses 32-bit computations\nfor 32-bit inputs, achieving some modest performance gains.\nFor both tree structures, the switch from 64-bit to 32-\nbit keys allows twice as many keys to \ft into a single cache\nline, improving performance. For FAST, which makes heavy\nuse of AVX-512 streaming operations, doubling the num-\nber of keys per cache line essentially doubles computational\nthroughput as well, as each operator can work on 16 32-bit\nvalues simultaneously (as opposed to 8 64-bit values).\n4.2.3 Search function\nNormally, we use binary search to locate the correct key\nwithin the search bound provided by the index structure.\nHowever, other search techniques can be used. Figure 11\nevaluates binary, linear, and interpolation search for each\nlearned structure and the RBS baseline on osmand amzn.\nWe observed that binary search (\frst column) was always\nfaster than linear search (second column). This aligns with\nprior work that showed binary search being e\u000bective until\nthe data size dropped below a very small threshold [29].\nInterpolation search (third column) behaves similarly to\nbinary search on the amzn dataset, even o\u000bering improved\nperformance on average ( \u00192%). This was surprising, be-\ncause interpolation search works by assuming that keys are\nuniformly distributed between two end points. If this were\n103104Lookup time (ns)amzn (bin. search)\n amzn (lin. search)\n amzn (int. search)\n102\n100102\nSize (MB)103104105Lookup time (ns)osm (bin. search)\n102\n100102\nSize (MB)osm (lin. search)\n102\n100102\nSize (MB)osm (int. search)\nRMI (binary)\nPGM (binary)\nRS (binary)RMI (linear)\nPGM (linear)\nRS (linear)RMI (interpolation)\nPGM (interpolation)\nRS (interpolation)Figure 11: A comparison of \\last mile\" (Section 2) search\ntechniques for the osmand amzn datasets.\nthe case, one would expect a learned index to learn this dis-\ntribution, subsuming any gains from interpolation search.\nHowever, because the learned structures have a limited size,\nthere can be many segments of the underlying data that ex-\nhibit linear behavior that the learned structure does have the\ncapacity to learn. For the osmdataset, which is relatively\ncomplex, interpolation search does not provide a bene\ft,\nand is often slower than binary search. This is unsurprising,\nsince interpolation search works best on smooth datasets.\nOne could also integrate more complex interpolation\nsearch techniques, such as SIP [30]. One di\u000eculty with in-\ncorporating SIP is the precomputation steps, which vary de-\npending on the search bound used. Integrating an exponen-\ntial search [9] technique could also be of interest, although\n8\n\nit is not immediately clear how to integrate a search bound.\nWe leave such investigations to future work.\n4.3 Explaining the performance\nIn this section, we investigate why learned index struc-\ntures have such strong performance and size properties.\nWhile prior work [19] attributed this to decreased branching\nand instruction count, we discovered that the whole story\nwas more complex. None of model accuracy, model size (or\n\\precision gain\", the combination of the two in [19]), cache\nmisses, instruction count, or branch misses can fully account\nfor learned index structures' performance.\nFigure 12 shows the correlation between lookup time and\nvarious performance characteristics of learned index struc-\ntures, BTrees, and ART for the amzn andosmdatasets. The\n\frst column shows the total in-memory size of each model,\nthe second column shows average log2search bound size\n(i.e., the expected number of binary search steps required),\nthe third column shows last-level cache misses, the fourth\ncolumn shows branch mispredictions, and the \ffth column\nshows instruction counts. One can visually dismiss any sin-\ngle metric as explanatory: any vertical line corresponds to\nstructures that are equal on the given metric, but exhibit dif-\nferent lookup times. For example, at a size of 1MB, RMIs\nachieve a latency of 220ns on amzn, but a BTree with the\nsame size achieves a latency of 650ns (blue vertical line).\nThe second column (\\log2error\"), is especially interest-\ning. Learned indexes must balance inference time with\nmodel error [22]. For example, with a log2error of 7, an\nRMI achieves a lookup time of 250ns on the amzn dataset,\nbut the PGM index with the same log2error achieves a\nlatency of 480ns (red vertical line). In other words, even\nthough the average size of the search bound generated by\nboth structures was the same, the RMI still achieved faster\nlookup times. This is attributable to the higher inference\ntime of the PGM index. Of course, other factors, such as\noverall model size, must be taken into account as well.\nAnalysis. In order to statistically test each potential ex-\nplanatory factor, we performed a linear regression analysis\nusing every index structure on all four datasets at 200 mil-\nlion 64-bit keys. The results indicated that cache misses,\nbranch misses, and instruction count had a statistically sig-\nni\fcant e\u000bect on lookup time ( p<0:001), whereas size and\nlog2error did not ( p > 0:15). To be clear, this means\nthat given the branch misses, cache misses, and instruction\ncounts , the size and log2error do not signi\fcantly a\u000bect per-\nformance. This does not mean that the log2error and size\ndo not have an impact on cache misses; just that the rele-\nvant variation in lookup time explained by model size and\nlog2error is accounted for fully in the other measures.\nOverall, a regression on cache misses, branch misses, and\ninstruction count explained 95% of the variance ( R2=\n0:955). This means that 95% of the variation we observed\nin our experiments can be explained by a linear relation-\nship between cache misses, branch misses, instructions, and\nlookup latency. The standardized regression coe\u000ecients for\ncache misses, branch misses, and instruction misses were\n0:85,\u00000:28, and 0:50, respectively. Standardized regression\ncoe\u000ecients can be interpreted as the number of standard\ndeviations that a particular measure needs to increase by,\nassuming the other measures stay \fxed, in order to increase\nthe output by one standard deviation; in other words, these\ncoe\u000ecients are descriptive of the variations within our mea-surements, not of the actual hardware impact of the metrics\n(although these are obviously related).\nInterpretation: branch misses. While the magnitude of\nstandardized regression coe\u000ecients are not useful on their\nown, their sign can provide interesting insights. Surpris-\ningly, the coe\u000ecient on branch misses is negative. This does\nnot mean that an increased number of branch misses leads\nto increased model performance. Instead, the negative co-\ne\u000ecient means that for a \fxed number of cache misses and\ninstructions , the tested indexes that incurred more branch\nmisses performed better. In other words, indexes are getting\nsigni\fcant value from branch misses; when an index incurs\na branch miss, it does so in such a way that reduces lookup\ntime more than an hypothetical alternative index that uses\nthe same number of instructions and cache misses.\nWe o\u000ber two possible explanations for this surprising ob-\nservation. First, structures may be over-optimized to avoid\nbranching, trading additional cache misses or instructions\nto reduce branching. Second, indexes that experience more\nbranch misses may bene\ft from speculative loads on modern\nhardware. We leave further investigation to future work.\nInterpretation: what metrics matter? If there is a\nsingle metric that explains the performance of learned index\nstructures, we were unable to \fnd it. Any of model size, log2\nerror, cache misses, branch misses, and instruction count\nalone are not enough to determine if one index structure will\nbe faster than another. Linear regression analysis suggests\nthat cache misses, branch misses, and instruction counts are\nall signi\fcant, and account for model size and log2error.\nOf the signi\fcant measures, cache misses had the largest\nexplanatory power. This is consistent with indexes being\nlatency-bound (i.e., limited by the round-trip time to RAM).\nThe vast majority of cache misses for RMIs happen during\nthe last-mile search. Two-layer RMIs require at most two\ncache misses for inference (potentially only one if the RMI's\ntop layer is small enough). On the other hand, for a full\nBTree, no cache misses happen during the \fnal search at\nall, but BTrees generally require at least one cache miss per\nlevel of the tree. Cache misses also help explain performance\ndi\u000berences between RMI and PGM: since each additional\nPGM layer likely requires a cache miss at inference time, a\nlarge RMI with low log2error will incur fewer cache misses\nthan a large PGM index with a similar log2error (e.g., amzn\nin Figure 12). When an RMI is not able to achieve a low\nlog2error, this advantage vanishes, as more cache misses are\nrequired during the last-mile search (e.g., osmin Figure 12).\nCurrent implementations of learned index structures seem\nto prioritize fast inference time over log2error. This makes\nsense, since a linear increase in log2error only leads to a\nlogarithmic increase in lookup time (due to binary search).\nHowever, our analysis suggests that a learned index struc-\nture could use signi\fcantly more cache misses if it could ac-\ncurately pinpoint the cache line containing the lookup key.\nWe experimented with multi-stage RMIs ( >10 levels), but\nwere unable to achieve such an accuracy. This could be an\ninteresting direction for future work.\nWe encourage future development of index structures to\ntake into account cache misses, branch misses, and instruc-\ntion counts. Since all three of these metrics have a statisti-\ncally signi\fcant impact on performance, ignoring one or two\nof them in favor of the other may lead to poor results. While\nwe cannot suggest a single metric for evaluating index struc-\n9\n\n02004006008001000Lookup time (ns)amzn\nLookup time (ns)amzn\nLookup time (ns)amzn\nLookup time (ns)amzn\nLookup time (ns)amzn\nPGM\nRS\nRMI\nBTree\nART\n101\n102\nSize (MB)02004006008001000Lookup time (ns)osm\n0 10 20\nLog2 ErrorLookup time (ns)osm\n50 100\nCache missesLookup time (ns)osm\n5 10\nBranch missesLookup time (ns)osm\n200 400\nInstructionsLookup time (ns)osmFigure 12: Various metrics compared with lookup times across index structures and datasets. No single metric can fully\nexplain the performance of di\u000berent index structures, suggesting a multi-metric analysis is required. Extended plots for all\ntechniques and datasets are available here: https://rm.cab/lis5\n101\n102\nSize01020Log2amzn\n101\n102\nSizeosm\nRS\nRMI\nPGM\nBTree\nFigure 13: Size and log2error bound of various index struc-\ntures. When evaluated as a compression technique, learned\nindex structures can be evaluated purely based on their\nsize and log2error. Extended plots are available here:\nhttps://rm.cab/lis7\ntures, if one must select a single metric, our analysis suggests\nthat cache misses are the most signi\fcant.\nLearned indexes as compression. A common view of\nlearned index structures is to think of learned indexes as a\nlossy compression of the CDF function [13,19]. In this view,\nthe goal of a learned index is similar to lossy image com-\npression (like JPG): come up with a representation that is\nsmaller than the CDF with minimal information loss. The\nquality of a learned index can thus be judged by just two\nmetrics: the size of the structure, and the log2error (infor-\nmation loss). Figure 13 plots these two metrics for the three\nlearned index structures and BTrees. These plots indicate\nthat the information theoretic view, while useful, is not fully\npredictive of index performance. For example, for face, all\nthree structures have very similar size and log2errors after\n1MB. However, some structures are substantially faster than\nothers at a \fxed size (Figure 7).\nWe encourage researchers and practitioners to familiarize\nthemselves with the information theoretic view of learned\nindex structures, but we caution against ending analysis at\nthis stage. For example, an index structure that achieves\noptimal compression (i.e., an optimal size to log2error ra-\ntio) is not necessarily going to outperform an index withsuboptimal compression. The simplest way this could oc-\ncur is because of inference time: if the index structure with\nsuperior compression takes a long time to produce a search\nbound, an index structure that quickly generates less ac-\ncurate search bounds may be superior. However, if one as-\nsumes that storage mediums are arbitrarily slow (i.e., search\ntime is strictly dominated by the size of search bound), then\nthere is merit in viewing learned index structures as a pure\ncompression problem, and investigating more advanced com-\npression techniques for these structures [13] could be fruitful.\n4.4 CPU interactions\nMany prior works on both learned and non-learned index\nstructures (including those by authors of this work) have\nevaluated their index structures by repeatedly performing\nlookups in a tight loop. While convenient and applicable to\nmany applications, this experimental setup may exaggerate\nthe performance of some index structures due, in part, to\ncaching and operator reordering .\n4.4.1 Caching\nExecuting index lookups in a tight loop, as it is often done\nto evaluate an index structure, will cause nearly all of the\nCPU cache to be \flled with the index structure and un-\nderlying data. Since accessing a cached value is signi\fcantly\nfaster (10s of nanoseconds) than accessing an uncached value\n(\u0019100 nanoseconds), this may cause such tight-loop exper-\niments to exaggerate the performance of an index structure.\nThe amount of data that will remain cached from one\nindex lookup to another is clearly application dependent.\nIn Figure 14, we investigate the e\u000bects of caching by eval-\nuating the two possible extremes: the datapoints labeled\n\\warm\" correspond to a tight loop in which large portions\nof the index structure and underlying data can be cached\nbetween lookups. The datapoints labeled \\cold\" correspond\nto the same workload, but with additionally fully \rushing\nthe cache after each lookup. The gain from a warm cache\nfor all \fve index structures ranges from 2x to 2.5x. With\nsmall index sizes ( <1MB), the cold-cache variant of several\n10\n\n102\n100102\nSize (MB)0100020003000Lookup time (ns)amzn\nRMI (cold)\nRMI (warm)\n102\n100102\nSize (MB)amzn\nRS (cold)\nRS (warm)\n102\n100102\nSize (MB)amzn\nPGM (cold)\nPGM (warm)\n102\n100102\nSize (MB)amzn\nBTree (cold)\nBTree (warm)\n102\n100102\nSize (MB)amzn\nFAST (cold)\nFAST (warm)Figure 14: The performance impact of having a cold cache for various index structures. Extended plots with all techniques\nare available here: https://rm.cab/lis3\nlearned index structures outperform the warm-cache BTree.\nWith larger (and arguably more realistic) index structure\nsizes, obviously whether or not the cache is warm or cold is\nmore important than the choice of index structure. Regard-\nless of if the cache is warm or cold, we found that learned\napproaches exhibited dominant performance / size tradeo\u000bs.\n4.4.2 Memory fences\nModern CPUs and compilers may reorder instructions to\noverlap computation and memory access or otherwise im-\nprove pipelining. For example, consider a simple program\nthat loads x, does a computation f(x), loadsy, and then\ndoes a computation g(y). Assuming the load of ydoes not\ndepend on x, a load ofymay be reordered to occur before\nthe computation of f(x), so that the latency from loading y\ncan be hidden within the computation of f(x). When con-\nsidering index structures, lookups placed in a tight loop may\ncause the CPU or compiler to overlap the \fnal computation\nof one query with the initial memory read of the next query.\nIn some applications, this may be realistic and desirable {\nin other applications, expensive computations between index\nlookups may prevent such overlapping. Thus, some indexes\nmay disproportionately bene\ft from this reordering.\nTo test the impact of reordering on lookup time, we in-\nserted a memory fence instruction into our experimental\nloop. This prevents the CPU or compiler from reordering\noperations across the fence. Figure 15 shows that RMI and\nRS { two of the most competitive index structures { have the\nlargest drop in performance when a memory fence is intro-\nduced (approximately a 50% slowdown). The BTree, FAST\nand PGM are almost entirely una\u000bected. While the inclu-\nsion of a memory fence harms the performance of RMI and\nRS, learned structures still provide a better performance /\nsize tradeo\u000b for the amzn dataset (results for other datasets\nare similar, but omitted due to space constraints).\nThe impact of a memory fence was highly correlated with\nthe number of instructions used by an index structure (Fig-\nure 12): indexes using fewer instructions, like RMI and\nRS, were impacted to a greater extent than structures us-\ning more instructions, like BTrees. Since reordering opti-\nmizations often examine only a small window of instruc-\ntions (i.e., \\peephole optimizations\" [23]), reordering opti-\nmizations may be more e\u000bective when instruction counts\nare lower. This may explain why RMI and RS are impacted\nmore by a memory fence.\nWe recommend that future researchers test their index\nstructures with memory fences to determine how much ben-\ne\ft their structure gets from reordering. Getting a lot ofbene\ft from reordering is not necessarily a bad thing; plenty\nof applications require performing index lookups in a tight\nloop, with only minimal computation being performed on\neach result. Ideally, researchers should evaluate their in-\ndex structures within a speci\fc application, although this is\nmuch more di\u000ecult.\n4.5 Multithreading\nHere, we evaluate how various index structures scale when\nqueried by concurrent threads. Our test CPU had 20 physi-\ncal cores, capable of executing 40 simultaneous threads with\nhyperthreading. Since multithreading strictly increases la-\ntency, here we evaluate throughput (lookups per second).\nVarying thread count. We \frst vary the number of\nthreads, \fxing the model size at 50MB except for Robin-\nHash, which is still the full size. The results are plotted\nin Figure 16a, with and without a memory fence. Over-\nall, all three learned index variants scale with an increasing\nnumber of threads, although only the RMI achieved higher\nthroughput than the RBS lookup table in this experiment.\nRobinHash, the technique with the lowest latency with\na single thread, does not achieve the highest throughput\nin a concurrent environment.5Even the RBS lookup ta-\nble achieves higher throughput than RobinHash, regard-\nless of whether or not a memory fence was used. We do\nnot consider hash tables optimized for concurrent environ-\nments [28]; here we only demonstrate that an o\u000b-the-shelf\nhash table with a load factor optimized for single-threaded\nlookups does not scale seamlessly.\nTo help explain why certain indexes scaled better than\nothers, we measured the number of cache misses incurred\nper second by each structure, plotted in Figure 16c. If a in-\ndex structure incurs more cache misses per second, then the\nbene\fts of multithreading will be diminished, since threads\nwill be latency bound waiting for access to RAM. Indeed,\nRobinHash incurs a much larger number of cache misses per\nsecond than any other technique. The larger size of the hash\ntable may contribute to this, as fewer cache lines may be\nshared in between lookups compared with a smaller index.\nPGM and FAST have the fewest cache misses per second\nat 40 threads, suggesting that PGM and FAST may bene\ft\nthe most from multithreading. To investigate this, we tab-\nulated the relative speedup factor of each technique. Due\nto space constraints, the plot is available online: https:\n//rm.cab/lis8 . FAST has the highest relative speedup,\nachieving 32x throughput with 40 threads. In addition to\n5The SIMD Cuckoo implementation [6] only supports 32-bit keys,\nand was not included in this experiment.\n11\n\n101\n102\nSize (MB)50010001500Lookup time (ns)amzn\nRMI (fence)\nRMI (no fence)\n101\n102\nSize (MB)amzn\nRS (fence)\nRS (no fence)\n101\n102\nSize (MB)amzn\nPGM (fence)\nPGM (no fence)\n101\n102\nSize (MB)amzn\nBTree (fence)\nBTree (no fence)\n102\n100102\nSize (MB)amzn\nFAST (fence)\nFAST (no fence)Figure 15: Performance of various index structures with and without a memory fence. Without the fence, the CPU may\nreorder instructions and overlap computation between lookups. With the fence, each lookup must be completed before the\nnext lookup begins. Extended plots with all techniques and datasets are available here: https://rm.cab/lis4\n0 10 20 30 40\nThreads050100150M lookups per sno fence (amzn)\n0 10 20 30 40\nThreadsfence (amzn)\n(a) Multithreaded throughput for the amzn\ndataset, models have a \fxed size of 50MB.\nNo memory fence (left) and with memory\nfence (right).\n103\n102\n101\n100101102103104\nSize (MB)050100150M lookups per sSize vs. Throughput, 40 threads (amzn)(b) Model size vs. 40-thread throughput for\ntheamzn dataset. An extended plot with all\nindex techniques is available here: https:\n//rm.cab/lis6\n0 10 20 30 40\nThreads020406080100Cache misses / lookup / secamzn (fence)\nRMI\nPGM\nRS\nRBS\nART\nBTree\nIBTree\nFAST\nRobinHash(c) Cache misses per lookup per second for\nvarious data structures. More cache misses\nper second indicates that speedup from mul-\ntithreading may be negatively impacted.\nFigure 16: Multithreading results\nhaving few cache misses per second, FAST also takes advan-\ntage of streaming AVX-512 instructions, which allows for ef-\nfective overlap of computation with memory reads. PGM,\ndespite having the least cache misses per second, achieved\nonly a 27x speedup at 40 threads. On the other hand, Robin-\nHash had by far the most cache misses per second and the\nlowest relative speedup at 40 threads (20x). Thus, cache\nmisses per second correlate with, but do not always deter-\nmine, the speedup factor of an index structure.\nVarying index size. Next, we \fx the number of threads\nat 40, and vary the size of the index. Results are plotted\nin Figure 16b. One might expect smaller structures to have\nbetter throughput because of caching e\u000bects; we did not \fnd\nthis to be the case. In general, larger indexes had higher\nthroughput than smaller ones. One possible explanation of\nthis behavior is that smaller models, while more likely to\nremain cached, produce larger search bounds, which cause\nmore cache misses during the last mile search.\nPGM, BTree, RS, and ART indexes su\u000bered decreased\nthroughput at large model sizes. This suggests that the\ncache misses incurred from the larger model sizes are not\nenough to make up for the re\fnement in the search bound.\nThe RMI did not su\u000ber such a regression, possibly because\neach RMI inference requires at most two cache misses (one\nfor each model level), whereas for other indexes the number\nof cache misses per inference could be higher.\n4.6 Build times\nFigure 17 shows the single-threaded build time required\nfor the fastest (in terms of lookup time) variants of each in-\ndex structure on amzn at di\u000berent dataset sizes. We do not\ninclude the time required to tune each structure (automati-\nPGM\nRS\nRMI\nRBS\nART\nBTree\nIBTree\nFAST\nFST\nWormhole\nRobinHash100101102Build time (s)200M\n400M\n600M\n800MFigure 17: Build times for the fastest (in terms of query\ntime) variant of each index type for the amzn dataset at four\ndi\u000berent data sizes. Note the log scale.\ncally via CDFShop [22] for RMIs, manually for other struc-\ntures). We note that automatically tuning an RMI may take\nseveral minutes. Unsurprisingly, BTrees, FST, and Worm-\nhole provide the fastest build times, as these structures were\ndesigned to support fast updates.6Of the non-learned in-\ndexes, FAST and RobinHash have the longest build times.\nMaximizing the performance of Robinhood hashing requires\nusing a high load factor (to keep the structure compact),\nwhich induces a high number of swaps. We note that many\nvariants of Robinhood hashing support parallel operations,\nand thus lower build times.\nFor the largest dataset, the build times for the fastest\nvariants of RMI, PGM, and RS were 80 seconds, 38 sec-\n6In particular, Wormhole and PGM can handle parallel inserts\nand builds respectively, which we do not evaluate here.\n12\n\nonds, and 20 seconds respectively. Of the learned index\nstructures, RS consistently provides the fastest build times\nregardless of dataset size. This is explained by the fact that\nan RS index can be built in a single pass over the data with\nconstant time per element [18]. In contrast, while a PGM\nindex could theoretically be built in a single pass, the tested\nimplementation of the PGM index builds the initial layer of\nthe index in a single pass, and builds subsequent layers in\na single pass over the previous layer (each logarithmically\nsmaller). RMIs require one full pass over the underlying\ndata per layer. In our experiments, no learned index takes\nadvantage of parallelism during construction, which could\nprovide a speedup.\n5. CONCLUSION AND FUTURE WORK\nIn this work, we present an open source benchmark that\nincludes several state-of-the-art tuned implementations of\nlearned and traditional index structures, as well as sev-\neral real-world datasets. Our experiments on read-only in-\nmemory workloads searching over dense arrays showed that\nlearned structures provided Pareto dominant performance /\nsize behavior. This dominance, while sometimes diminished,\npersists even when varying dataset sizes, key sizes, mem-\nory fences, cold caches, and multi-threading. We demon-\nstrate that the performance of learned index structures\nis not attributable to any speci\fc metric, although cache\nmisses played the largest explanatory role. In our experi-\nments, learned structures generally had higher build times\nthan insert-optimized traditional structures like BTrees.\nAmongst learned structures, we found that RMIs provided\nthe strongest performance / size but the longest build times,\nwhereas both RS and PGM indexes could be constructed\nfaster but had slightly slower lookup times.\nIn the future, we plan to examine the end-to-end impact of\nlearned index structures on real applications. Opportunities\nto combine a simple radix table with an RMI structure (or\nvice versa) are also worth investigating. As more learned in-\ndex structures begin to support updates [11,13,14], a bench-\nmark against traditional indexes (which are often optimized\nfor updates) could be fruitful.\nAcknowledgments\nThis research is supported by Google, Intel, and Microsoft\nas part of the MIT Data Systems and AI Lab (DSAIL) at\nMIT, NSF IIS 1900933, DARPA Award 16-43-D3M-FP040,\nand the MIT Air Force Arti\fcial Intelligence Innovation Ac-\ncelerator (AIIA).\n6. REFERENCES\n[1] MIT RMI, https://learned.systems/rmi.\n[2] C++ lower bound,\nhttp://www.cplusplus.com/reference/algorithm/lower bound/.\n[3] RobinMap, https://github.com/Tessil/robin-map.\n[4] RocksDB, https://rocksdb.org/.\n[5] Searching on sorted data benchmark,\nhttps://learned.systems/sosd.\n[6] SIMD Cuckoo Hash, https://github.com/stanford-\nfuturedata/index-baselines.\n[7] STX B+ Tree, https://panthema.net/2007/stx-btree/.\n[8] N. Ao, F. Zhang, D. Wu, D. S. Stones, G. Wang,\nX. Liu, J. Liu, and S. Lin. E\u000ecient parallel listsintersection and index compression algorithms using\ngraphics processing units. Proceedings of the VLDB\nEndowment , 4(8):470{481, May 2011.\n[9] J. L. Bentley and A. C.-C. Yao. An almost optimal\nalgorithm for unbounded searching. Information\nProcessing Letters , 5(3):82{87, Aug. 1976.\n[10] R. Binna, E. Zangerle, M. Pichl, G. Specht, and\nV. Leis. HOT: A height optimized trie index for\nmain-memory database systems. In Proceedings of the\n2018 International Conference on Management of\nData , SIGMOD '18, pages 521{534, New York, NY,\nUSA, 2018. Association for Computing Machinery.\n[11] J. Ding, U. F. Minhas, H. Zhang, Y. Li, C. Wang,\nB. Chandramouli, J. Gehrke, D. Kossmann, and\nD. Lomet. ALEX: An Updatable Adaptive Learned\nIndex. arXiv:1905.08898 [cs] , May 2019.\n[12] P. Ferragina and G. Vinciguerra. Learned data\nstructures. In Recent Trends in Learning From Data ,\nvolume 896 of Studies in Computational Intelligence .\nSpringer, 2020.\n[13] P. Ferragina and G. Vinciguerra. The PGM-index: A\nfully-dynamic compressed learned index with provable\nworst-case bounds. Proceedings of the VLDB\nEndowment , 13(8):1162{1175, Apr. 2020.\n[14] A. Galakatos, M. Markovitch, C. Binnig, R. Fonseca,\nand T. Kraska. FITing-Tree: A Data-aware Index\nStructure. In Proceedings of the 2019 International\nConference on Management of Data , SIGMOD '19,\npages 1189{1206, New York, NY, USA, 2019. ACM.\n[15] G. Graefe. B-tree indexes, interpolation search, and\nskew. In Proceedings of the 2nd International\nWorkshop on Data Management on New Hardware ,\nDaMoN '06, Chicago, Illinois, June 2006. Association\nfor Computing Machinery.\n[16] C. Kim, J. Chhugani, N. Satish, E. Sedlar, A. D.\nNguyen, T. Kaldewey, V. W. Lee, S. A. Brandt, and\nP. Dubey. FAST: Fast architecture sensitive tree\nsearch on modern CPUs and GPUs. In Proceedings of\nthe 2010 International Conference on Management of\nData , SIGMOD '10, 2010.\n[17] A. Kipf, R. Marcus, A. van Renen, M. Stoian,\nA. Kemper, T. Kraska, and T. Neumann. SOSD: A\nBenchmark for Learned Indexes. In ML for Systems at\nNeurIPS , MLForSystems @ NeurIPS '19, Dec. 2019.\n[18] A. Kipf, R. Marcus, A. van Renen, M. Stoian,\nA. Kemper, T. Kraska, and T. Neumann. RadixSpline:\nA single-pass learned index. In Proceedings of the\nThird International Workshop on Exploiting Arti\fcial\nIntelligence Techniques for Data Management , aiDM\n@ SIGMOD '20, pages 1{5, Portland, Oregon, June\n2020. Association for Computing Machinery.\n[19] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and\nN. Polyzotis. The Case for Learned Index Structures.\nInProceedings of the 2018 International Conference\non Management of Data , SIGMOD '18, pages\n489{504, New York, NY, USA, 2018. ACM.\n[20] V. Leis, A. Kemper, and T. Neumann. The adaptive\nradix tree: ARTful indexing for main-memory\ndatabases. In Proceedings of the 2013 IEEE\nInternational Conference on Data Engineering , ICDE\n'13, pages 38{49, USA, 2013. IEEE Computer Society.\n[21] C. Luo and M. J. Carey. LSM-based storage\n13\n\ntechniques: A survey. PVLDB , 29(1):393{418, Jan.\n2020.\n[22] R. Marcus, E. Zhang, and T. Kraska. CDFShop:\nExploring and Optimizing Learned Index Structures.\nInProceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data ,\nSIGMOD '20, Portland, OR, June 2020.\n[23] W. M. McKeeman. Peephole optimization.\nCommunications of the ACM , 8(7):443{444, July 1965.\n[24] V. Nathan, J. Ding, M. Alizadeh, and T. Kraska.\nLearning Multi-dimensional Indexing. In ML for\nSystems at NeurIPS , MLForSystems @ NeurIPS '19,\nDec. 2019.\n[25] T. Neumann and S. Michel. Smooth interpolating\nhistograms with error guarantees. In Sharing Data,\nInformation and Knowledge, 25th British National\nConference on Databases , BNCOD '08, pages 126{138,\n2008.\n[26] Peter Bailis, Kai Sheng Tai, Pratiksha Thaker, and\nMatei Zaharia. Don't Throw Out Your Algorithms\nBook Just Yet: Classical Data Structures That Can\nOutperform Learned Indexes (blog post),\nhttps://dawn.cs.stanford.edu/2018/01/11/index-\nbaselines/,\n2018.\n[27] Peter Boncz and Thomas Neumann. The Case for\nB-Tree Index Structures (blog post),\nhttp://databasearchitects.blogspot.com/2017/12/the-\ncase-for-b-tree-index-structures.html,\n2017.\n[28] S. Richter, V. Alvarez, and J. Dittrich. A\nseven-dimensional analysis of hashing methods and its\nimplications on query processing. Proceedings of the\nVLDB Endowment , 9(3):96{107, Nov. 2015.\n[29] L.-C. Schulz, D. Broneske, and G. Saake. An\neight-dimensional systematic evaluation of optimized\nsearch algorithms on modern processors. Proceedings\nof the VLDB Endowment , 11(11):1550{1562, July\n2018.\n[30] P. Van Sandt, Y. Chronis, and J. M. Patel. E\u000eciently\nSearching In-Memory Sorted Arrays: Revenge of the\nInterpolation Search? In Proceedings of the 2019\nInternational Conference on Management of Data ,\nSIGMOD '19, pages 36{53, New York, NY, USA,\n2019. ACM.\n[31] X. Wu, F. Ni, and S. Jiang. Wormhole: A Fast\nOrdered Index for In-memory Data Management. In\nProceedings of the Fourteenth EuroSys Conference\n2019, EuroSys '19, pages 1{16, Dresden, Germany,\nMar. 2019. Association for Computing Machinery.\n[32] Q. Xie, C. Pang, X. Zhou, X. Zhang, and K. Deng.\nMaximum error-bounded Piecewise Linear\nRepresentation for online stream approximation. The\nVLDB Journal , 23(6):915{937, Dec. 2014.\n[33] H. Zhang, H. Lim, V. Leis, D. G. Andersen,\nM. Kaminsky, K. Keeton, and A. Pavlo. SuRF:\nPractical Range Query Filtering with Fast Succinct\nTries. In Proceedings of the 2018 International\nConference on Management of Data , SIGMOD '18,\npages 323{336, Houston, TX, USA, May 2018.\nAssociation for Computing Machinery.\n14",
  "textLength": 72398
}