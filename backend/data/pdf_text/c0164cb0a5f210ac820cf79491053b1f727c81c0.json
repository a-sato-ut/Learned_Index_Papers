{
  "paperId": "c0164cb0a5f210ac820cf79491053b1f727c81c0",
  "title": "Learned k-NN distance estimation",
  "pdfPath": "c0164cb0a5f210ac820cf79491053b1f727c81c0.pdf",
  "text": "Learned k-NN Distance Estimation\nDaichi Amagataâˆ—\nOsaka University\nJapan\namagata.daichi@ist.osaka-u.ac.jpYusuke Araiâˆ—\nOsaka University\nJapan\narai.yusuke@ist.osaka-u.ac.jp\nSumio Fujita\nYahoo Japan Corporation\nJapan\nsufujita@yahoo-corp.jpTakahiro Hara\nOsaka University\nJapan\nhara@ist.osaka-u.ac.jp\nABSTRACT\nBig data mining is well known to be an important task for data\nscience, because it can provide useful observations and new knowl-\nedge hidden in given large datasets. Proximity-based data analysis\nis particularly utilized in many real-life applications. In such analy-\nsis, the distances to ğ‘˜nearest neighbors are usually employed, thus\nits main bottleneck is derived from data retrieval. Much efforts have\nbeen made to improve the efficiency of these analyses. However,\nthey still incur large costs, because they essentially need many\ndata accesses. To avoid this issue, we propose a machine-learning\ntechnique that quickly and accurately estimates the ğ‘˜-NN distances\n(i.e., distances to the ğ‘˜nearest neighbors) of a given query. We\ntrain a fully connected neural network model and utilize pivots to\nachieve accurate estimation. Our model is designed to have useful\nadvantages: it infers distances to the ğ‘˜-NNs at a time, its inference\ntime isğ‘‚(1)(no data accesses are incurred), but it keeps high ac-\ncuracy. Our experimental results and case studies on real datasets\ndemonstrate the efficiency and effectiveness of our solution.\n1 INTRODUCTION\nDue to the proliferation of IoT devices, it is becoming easier to\nmaintain big data. Big data mining is well known to be an important\ntask for data science, because it can provide useful observations and\nnew knowledge hidden in given large datasets. Proximity-based\ndata analysis is particularly utilized in many real-life applications\n[11, 34].\nMotivation. Many proximity-based analysis tools employ distances\ntoğ‘˜nearest neighbors ( ğ‘˜-NNs), because they are an intuitive and\neffective way to measure the density (or sparsity) of a given point.\nFor example, geometric inference [ 10], outlier/anomaly detection\n[3,4], clustering [ 13], and cell analysis [ 58] employ this approach.\nBelow, by using some representative analysis tools that are used by\nindustrial applications, we introduce how to use the distances to\nğ‘˜-NNs more concretely.\nExample 1 (ğ‘˜-NN density estimation & visualization ).Density\nestimation [ 32] is one of the data visualization tools used most fre-\nquently, e.g., for detecting hot spots w.r.t. traffic and crime. Given a\nquery pointğ‘, its density based on ğ‘˜-NNs is obtained via the following\nâˆ—Both authors contributed equally to this research.estimator [10]:\nË†ğ‘’(ğ‘)=1\nğ‘›ğ‘‰ğ‘‘Â©Â­\nÂ«Ãğ‘˜\nğ‘—=1ğ‘—ğ‘‘/2\nÃğ‘˜\nğ‘—=1ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘—\nğ‘)ğ‘‘ÂªÂ®\nÂ¬ğ‘‘/2\n, (1)\nwhereğ‘›is the dataset size, ğ‘‰ğ‘‘is the volume of the unit hypersphere in\nağ‘‘-dimensional Euclidean space, ğ‘¥ğ‘—\nğ‘is theğ‘—-th NN ofğ‘, andğ‘‘ğ‘–ğ‘ ğ‘¡(Â·,Â·)\nis the Euclidean distance between two points. Given a space for visual-\nization and its resolution, each pixel can be a query ğ‘and obtains its\nğ‘˜-NN density via the above equation, which is then displayed in the\ncorresponding (sub-)space. (The visualized result of a crime position\ndataset may be used by security service companies/organizations to\nhighlight danger areas.)\nExample 2 (Distance-based outlier detection ).The importance\nof outlier detection and its wide range of applications are well known,\ne.g., see survey papers [ 11,17,48]. Let us consider machine learning\n(ML) models, and they are now used to automate operations in many\ncompanies. However, ML models suffer from data errors [ 43], and\nsuch errors (outliers) should be detected and removed to train high\nperformance models. Distance-based outlier detection is often utilized\nto achieve this, because it is intuitive and effective [ 3]. Literature\n[27] proposed a definition that a point ğ‘¥is an outlier iff the distance\nbetweenğ‘¥and itsğ‘˜-NN is larger than a threshold. Similarly, literature\n[40] proposed a definition that a point ğ‘¥is an outlier iff the distance\nbetweenğ‘¥and itsğ‘˜-NN is in the top- ğ‘among a given dataset.\nExample 3 (Reverse engineering for clustering ).Let us consider\ndensity-peaks clustering (DPC) [ 42], one of the density-based cluster-\ning algorithms. DPC has been employed in many domains [ 2]. For\nexample, [ 55] used DPC to analyze industrial IoT datasets. As a crite-\nrion for clustering, DPC computes the local density ğœŒğ‘–for each point ğ‘¥ğ‘–,\nand it is the number of points ğ‘¥ğ‘—such thatğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘¥ğ‘–,ğ‘¥ğ‘—)â‰¤ğ‘‘ğ‘ğ‘¢ğ‘¡, where\nğ‘‘ğ‘ğ‘¢ğ‘¡is a threshold. If ğœŒğ‘–<ğœŒğ‘šğ‘–ğ‘›, DPC regards ğ‘¥ğ‘–as noise, similarly\nto the distance-based outlier.\nAssume that a result of DPC is given but the value of ğ‘‘ğ‘ğ‘¢ğ‘¡is\nunknown. Assume furthermore that we know the number of noises\n(from clustering labels) and ğœŒğ‘šğ‘–ğ‘›. To reproduce the clustering result,\nwe need to know ğ‘‘ğ‘ğ‘¢ğ‘¡, but, because ğ‘‘ğ‘ğ‘¢ğ‘¡is a real value, testing all\npossible values is not practical. Then, it is important to notice that,\nby measuring the distance to ğœŒğ‘šğ‘–ğ‘›-NN, we can see the distance that\ndistinguishes noises. (Details will be given in Section 5.3.) From this\nobservation, we can estimate ğ‘‘ğ‘ğ‘¢ğ‘¡.\nThe above examples clarify the importance of obtaining the dis-\ntances toğ‘˜-NNs .arXiv:2208.14210v2  [cs.DB]  27 Nov 2022\n\nA straightforward approach to computing the ğ‘˜-NN distances\nis to retrieve the ğ‘˜-NN points. Because the above applications face\nlarge datasets, there are many approaches that efficiently retrieve\ntheğ‘˜-NN points, such as tree indices [ 8,9,39]. However, when the\nobjective is to know the distances toğ‘˜-NNs , data retrieval approaches\nstill incur large computational cost, because they need to access\nmany points.\nContribution. In this paper, we consider a machine-learning (ML)\ntechnique to alleviate this cost. Recently, ML techniques have been\nintroduced in many database management systems [ 21,34,44,54]\nbecause of their adaptability to the distributions of real datasets and\naccurate inference ability. It is important to note that, to employ\nan ML technique for the problem of ğ‘˜-NN distance estimation,\nthe ML model has to be simple (to enable fast inference) yet\naccurate . We tackle this challenge and propose a regression-based\nsolution.\nAssume that we have a set ğ‘‹ofğ‘‘-dimensional points, there\nare a lot of pivot points in the data space, and the distances to\ntheğ‘˜-NNs of the pivots are pre-computed. Given a query point ğ‘,\nthere would exist a pivot that is very near ğ‘. Therefore, by using\ntheğ‘˜-NN distances of the pivot, we can infer the distances to the\nğ‘˜-NNs ofğ‘and its error would be very small. It is however not\nfeasible, in practice, to prepare pivots so that there exist sufficiently\nclose pivots for arbitrary queries, as this approach is prohibitive\nw.r.t. space cost. Hence, we train a regression model based on a\nfully connected neural network and exploit a reasonable number\nof pivots to learn an accurate model. Our model, namely PivNet , is\ndesigned to deal with multiple values of ğ‘˜inğ‘‚(1)time, which is\nrequired in some applications such as Example 1. In addition to the\nabove application examples, our ğ‘˜-NN distance estimation enables\nto obtain a threshold for ğ‘˜-NN search. This speeds-up the ğ‘˜-NN\nquery processing because we do not need to set âˆas the initial\nthreshold any more (although the result may be approximate). To\nsummarize, our main contributions are as follows:\nâ€¢We propose PivNet, a learned model that infers the ğ‘˜-NN dis-\ntances of a given query in ğ‘‚(1)time. Its source code is open-\nsource at https://github.com/arailly/pivnet.\nâ€¢We conduct experiments to demonstrate that PivNet provides a\nfast inference time while keeping small errors.\nâ€¢We conduct case studies w.r.t. ğ‘˜-NN density visualization, distance-\nbased outlier detection, reverse engineering for density-based\nclustering, and approximate ğ‘˜-NN search. The results show the\npractical efficiency and effectiveness of PivNet.\nThis is a full version of [1].\nOrganization. The remainder of this paper is organized as follows.\nSection 2 formally defines our problem. Section 3 presents our\nsolution. Sections 4 and 5 respectively report our experimental and\ncase studies. Finally, Section 7 concludes this paper.\n2 PRELIMINARY\nLetğ‘‹be a set ofğ‘‘-dimensional points in the Euclidean space. We\nuseğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘¥,ğ‘¥â€²)to denote the Euclidean distance between two points\nğ‘¥andğ‘¥â€². Below, we define the ğ‘˜nearest neighbors ( ğ‘˜-NNs).Definition 1 (ğ‘˜nearest neighbors ).Given a set ğ‘‹of points, a\nquery point ğ‘, andğ‘˜,ğ´is a set of the ğ‘˜nearest neighbors of ğ‘that\nsatisfies|ğ´|=ğ‘˜andâˆ€ğ‘¥âˆˆğ´,âˆ€ğ‘¥â€²âˆˆğ‘‹\\ğ´,ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘¥,ğ‘)â‰¤ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘¥â€²,ğ‘).\nThen, our problem is defined as follows:\nDefinition 2 (ğ‘˜-NN distance estimation problem) .Given a setğ‘‹of\npoints, a query ğ‘, andğ‘˜, letğ´be a set of the ğ‘˜nearest neighbors of ğ‘\ndefined in Definition 1. This problem estimates the distance between\nğ‘andğ‘¥for eachğ‘¥âˆˆğ´.\nWe assume that ğ‘‹is static (or is not frequently updated), which\nis a common assumption, e.g., for applications in Examples 1â€“3.\nFurthermore, we assume that there is a maximum ğ‘˜, denoted by\nğ‘˜ğ‘šğ‘ğ‘¥ (e.g.,ğ‘˜ğ‘šğ‘ğ‘¥ is 50 or 100), which is specified by applications (so\nthat it satisfies their requirements). Our solution is then designed\nto estimate the distances to ğ‘˜ğ‘šğ‘ğ‘¥-NNs.\nConsider a query ğ‘, and let Ë†ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜ğ‘)be an estimated distance\nto itsğ‘˜-NN. For distance estimation, minimizing absolute error\n|ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜ğ‘)âˆ’ Ë†ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜ğ‘)|is clearly important, as can be seen from\nExamples 2 and 3. Therefore, as an error metric, we mainly use\nMAE (Mean Absolute Error), and we compute the MAE of ğ‘,ğ‘€ğ´ğ¸ ğ‘,\nas follows:\nğ‘€ğ´ğ¸ ğ‘=Ãğ‘˜ğ‘šğ‘ğ‘¥\nğ‘˜=1|ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜ğ‘)âˆ’ Ë†ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜ğ‘)|\nğ‘˜ğ‘šğ‘ğ‘¥. (2)\n(In our empirical evaluation, Mean Absolute Percentage Error, MAPE,\nis also measured.)\nBecause the application examples in Section 1 typically assume\nlow-dimensional datasets [ 2,14â€“16,50], this paper considers that ğ‘‘\nis low. The case of high-dimensional data is a future work, because\nsolutions for this case can be totally different1.\n3 OUR SOLUTION\nMain idea. The main requirements of the ğ‘˜-NN distance estimation\nproblem are high efficiency and accuracy. To satisfy these require-\nments, our first idea uses pivot points. Consider that a lot of pivots\nare distributed in the data space âˆˆRğ‘‘. Specifically, the data space is\ndivided equally by fine-grained grid cells and each cell has a pivot\nğ‘centered at the cell. Given a query ğ‘, its corresponding cell can be\nfound inğ‘‚(1)time. Letğ‘¥ğ‘–ğ‘be theğ‘–-th NN ofğ‘inğ‘‹. From triangle\ninequality, if ğ‘¥ğ‘˜ğ‘âˆ‰{ğ‘¥ğ‘–ğ‘|ğ‘–âˆˆ[1,ğ‘˜âˆ’1]}, we have\nğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜\nğ‘)â‰¤ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘)+ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜\nğ‘). (3)\nBesides, in a very fine-grained grid, we can have ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘)â‰ˆ0.\nThis suggests that the distances to the ğ‘˜NNs ofğ‘can be obtained\nfrom those of ğ‘. Note thatğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘–ğ‘), whereğ‘–âˆˆ[1,ğ‘˜ğ‘šğ‘ğ‘¥], can be\nobtained efficiently in a pre-processing phase (by using a tree index,\ne.g.,ğ‘˜d-tree). That is, for ğ‘˜ğ‘šğ‘ğ‘¥=ğ‘‚(1), this approach infers the\nğ‘˜NN distances of ğ‘inğ‘‚(1)time and its accuracy would be high.\nAlthough the above idea functions well in theory, preparing\nsuch an unlimited number of pivots is space-consuming and is\nnot feasible in practice. Now the challenge is how to achieve the\nabove high accuracy while keeping ğ‘‚(1)inference time and the\nusefulness of pivots. We overcome this challenge by using a neural\nnetwork model, because it can effectively learn the distributions of\n1This is trivial. For example, space partitioning methods, such as tree indices, are\ngenerally used for similarity search on low-dimensional data, whereas they are not\nused for high-dimensional data [30].\n\nâ€¦\nğ‘\nğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘2)â€¦ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘2,ğ‘¥ğ‘21)\nğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘2,ğ‘¥ğ‘2ğ‘˜ğ‘šğ‘ğ‘¥)à·¢ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘1)\nà·¢ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘ğ‘˜ğ‘šğ‘ğ‘¥)ğ‘1 ğ‘2ğ‘3\nğ‘\nğ‘4 ğ‘5ğ‘6\nğ‘7ğ‘8ğ‘9â€¦â€¦\nâ€¦â€¦Figure 1: The left part shows the data space and a grid. Blue stars centered at cells represent pivots and the red star represents\na queryğ‘. The right part shows our five-layer fully connected neural network model. The input layer has the coordinates of\nğ‘,ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘2), whereğ‘2is the nearest pivot of ğ‘, andğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘2,ğ‘¥ğ‘˜ğ‘2)(ğ‘˜âˆˆ[1,ğ‘˜ğ‘šğ‘ğ‘¥]). It has three hidden layers, and its output layer is\nregarded as a vector âŸ¨Ë†ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥1ğ‘),..., Ë†ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜ğ‘šğ‘ğ‘¥ğ‘)âŸ©.\nreal datasets. We incorporate the idea that the ğ‘˜NN distances of a\npivot nearğ‘can be similar to those of ğ‘into the neural network, to\ntrain an accurate estimation model. This is our second idea.\nModeling by a neural network. First, we divide each coordinate\nof the data space equally to build a grid offline. The granularity is\napplication-dependent and can be specified based on memory size.\nEach cell of the grid has a pivot ğ‘at its centroid. Note that, given a\nqueryğ‘, its nearest pivot is obtained in ğ‘‚(1)time because pivots\nare centroids of their cells.\nThen, to implement our main ideas, we consider the ğ‘˜NN dis-\ntance estimation problem as a regression problem that outputs\nË†ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜ğ‘). By preparing a set ğ‘„ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› of training queries, we can\nobtainğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜ğ‘)offline. Therefore, we can run supervised learn-\ning to train the regression model. We employ a neural network to\nachieve this, motivated by the fact that it provides high inference\naccuracy in many problems, domains, and fields. It is important to\nrecall that the inference time has to be fast while minimizing estima-\ntion errors, as mentioned in Section 1. To achieve this, a simple yet\neffective model is required. (Neural networks with complex and/or\nmany layers cannot achieve this, because they increase the number\nof running costly matrix multiplication.) We specifically utilize a\nfive-layer (three hidden layers) fully connected neural network\nand use ReLU [ 35] as an activation function2. As the input of this\nneural network, we use ğ‘,ğ‘˜,ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘), whereğ‘is the nearest pivot\nofğ‘, andğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜ğ‘). Via stochastic gradient descent, this neural\nnetwork is trained to minimize the following loss function:\nL=1\nğ‘˜ğ‘šğ‘ğ‘¥|ğ‘„ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›|âˆ‘ï¸\nâŸ¨ğ‘,ğ‘˜âŸ©âˆˆğ‘ƒ|ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜\nğ‘)âˆ’ Ë†ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜\nğ‘)|,\nwhereğ‘ƒis a set of all pairs of ğ‘âˆˆğ‘„ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› andğ‘˜âˆˆ[1,ğ‘˜ğ‘šğ‘ğ‘¥], and\nË†ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜ğ‘)is obtained by the neural network ğ‘“, i.e.,\nË†ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜\nğ‘)=ğ‘“(ğ‘,ğ‘˜,ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘),ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜\nğ‘)).\n2Our experimental result shows that a deeper neural network does not yield a clear\nadvantage.By addingğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘)andğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜ğ‘)as feature values, its inference\naccuracy becomes higher than that of its variant without them\nwhile keeping simpleness, see Section 4.\nOptimization. The above neural network has several drawbacks.\nFirst, it needs to deal with each ğ‘˜âˆˆ [1,ğ‘˜ğ‘šğ‘ğ‘¥]iteratively, as it\noutputs only Ë†ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜ğ‘). As described in Definition 2, when we need\ndistances to ğ‘˜-NNs, this approach incurs ğ‘‚(ğ‘˜)time. For example, in\nExample 1,ğ‘˜is not small, so this cost is not negligible. Moreover, this\napproach incurs a long training time because we need to consider\neach pair of ğ‘âˆˆğ‘„ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› andğ‘˜âˆˆ[1,ğ‘˜ğ‘šğ‘ğ‘¥].\nTo remove these drawbacks, we optimize our design of the neural\nnetwork and propose PivNet, which is illustrated in Figure 1. The\nlast layer of this neural network has ğ‘˜ğ‘šğ‘ğ‘¥ neurons, and this model is\ntrained so that it can infer the distances to ğ‘˜ğ‘šğ‘ğ‘¥-NNs at a time . This\nmodel,ğ‘“ğ‘ƒğ‘–ğ‘£ğ‘ğ‘’ğ‘¡ , requiresğ‘,ğ‘˜,ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘), whereğ‘is the nearest pivot\nofğ‘, andâŸ¨ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥1ğ‘),...,ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜ğ‘šğ‘ğ‘¥ğ‘)âŸ©as input, and it outputs\nestimatedğ‘˜ğ‘šğ‘ğ‘¥-NN distances. That is,\nË†vğ‘=ğ‘“ğ‘ƒğ‘–ğ‘£ğ‘ğ‘’ğ‘¡(ğ‘,ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘),vğ‘),where\nvğ‘=âŸ¨ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥1\nğ‘),...,ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜ğ‘šğ‘ğ‘¥ğ‘)âŸ©,and\nvğ‘=âŸ¨Ë†ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥1\nğ‘),..., Ë†ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜ğ‘šğ‘ğ‘¥ğ‘)âŸ©.\nThe loss function of PivNet is also optimized to deal with the dis-\ntances toğ‘˜ğ‘šğ‘ğ‘¥-NNs at the same time while achieving accurate\nregression. Let||Â·|| 1be theğ¿1norm of a given vector. To obtain a\nsimilar accuracy to that of ğ‘“, PivNet specifically utilizes the follow-\ning loss function:\nLğ‘œğ‘ğ‘¡=1\n|ğ‘„ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›|âˆ‘ï¸\nğ‘âˆˆğ‘„ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›||vğ‘âˆ’Ë†vğ‘||1\nğ‘˜ğ‘šğ‘ğ‘¥.\n(Notice that vğ‘is the ground truth.) We train PivNet so that Lğ‘œğ‘ğ‘¡is\nminimized. Then, as long as ğ‘˜â‰¤ğ‘˜ğ‘šğ‘ğ‘¥, PivNet infers the distances\nto theğ‘˜-NNs ofğ‘inğ‘‚(1)time (for fixed hyper-parameters) without\nlosing accurate estimation ability.\nPreparing query training set. A straightforward approach to\npreparing queries for training is to randomly sample points from ğ‘‹.\nIn this case, however, the model overfits the distribution of ğ‘‹. When\n\nqueries are specified uniformly at random in Rğ‘‘as in Example 1,\nthe performance of this model would degrade, particularly when\nqueries are specified at sparse spaces. To avoid this drawback, we\naugment training queries by generating synthetic ones that follow\na uniform distribution in Rğ‘‘(like pivots), and they are added into\nğ‘„ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› . After that, for each ğ‘âˆˆğ‘„ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› , we compute its ğ‘˜-NNs\namongğ‘‹\\ğ‘„ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› and train PivNet.\n4 EXPERIMENT\nThis section presents our experimental results. All experiments\nwere conducted on a Ubuntu 18.04 LTS machine with 3.0GHz Core\ni9-10980XE CPU and 128GB RAM.\n4.1 Setting\nMethods. We compared PivNet with the following approaches:\nâ€¢ğ‘˜d-tree [ 8]: This returns the exact answer by retrieving the\nğ‘˜-NNs via a branch-and-bound algorithm.\nâ€¢LightGBM [26]: This is a state-of-the-art gradient boosting de-\ncision tree-based regression model. For its input, we used the\nsame feature values as those of PivNet, to be fair.\nâ€¢Pivot: This algorithm uses the right side of Equation (3) as\ninference of the ğ‘˜-NN distances.\nâ€¢QueryNet: This is a variant of PivNet and does not utilize pivots.\nThis model also estimates the distances to ğ‘˜ğ‘šğ‘ğ‘¥-NNs at a time.\nâ€¢PivNet-itr: This is a variant of PivNet, and it infers the distance\ntoğ‘˜-NN for each ğ‘˜âˆˆ[1,ğ‘˜ğ‘šğ‘ğ‘¥]iteratively.\nThe above methods were implemented by Python, were single\nthreaded, and ran in-memory. (We have no other competitors, be-\ncause our problem has no prior works.) QueryNet, PivNet-itr, and\nPivNet were trained in PyTorch. For ğ‘˜d-tree, we used SciPy3.\nDatasets. We used the following publicly available real datasets\nfor evaluation. In them, we removed points with missing values.\nâ€¢Crime [15]: A set of 259,809 geo-location (i.e., 2D) points where\ncrimes occurred in Atlanta, U.S.A.\nâ€¢HEPMASS [6]: A set of 10,049,359 5D mass feature datasets.\nâ€¢Household4: A set of 1,771,612 power consumption data mea-\nsured in a house. We used 4 attributes (active power, reactive\npower, voltage, and intensity).\nâ€¢PAMAP2 [41]: A set of 2,643,873 5D-sensor data.\nâ€¢Places5: A set of 9,033,486 geo-locations of public places in the\nU.S.A.\nâ€¢Wisdom [53]: A set of 4,476,481 3D gyroscopes sensors obtained\nfrom smartphones and smartwatches.\nFor each dataset, we randomly choose 100,000 points in the\ndataset and added them into ğ‘„ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› . Moreover, we added 100,000\npoints augmented by the approach presented in Section 3 into\nğ‘„ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› . We used random 80% points in ğ‘„ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› for training and\nthe remaining points for validation. We randomly choose another\n10,000 points in the dataset, augmented 10,000 points in the above\nmanner, and used them as test queries. A set of the remaining points\nin the dataset was used as ğ‘‹.\n3https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html\n4https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+\nconsumption\n5https://archive.org/details/2011-08-SimpleGeo-CC0-Public-SpacesHyper-parameters. We setğ‘˜ğ‘šğ‘ğ‘¥=50. For the grid introduced in\nSection 3, we defined a constant ğ‘to equally divide the coordinate\nof each domain. For 2D and 3D datasets, ğ‘=2048 andğ‘=256,\nrespectively. For 4D and 5D datasets, ğ‘=32.\nThe numbers of neurons in the second and third layers were 64\non Crime and Places and 128 on the others. The number of neurons\nin the fourth layer was 32 on all datasets. The batch size was 500.\nThe learning rate of QueryNet was 0.1, 0.1, 0.1, 0.2, 0.2, and 0.1\non Crime, HEPMASS, Household, PAMAP2, Places, and Wisdom,\nrespectively. Also, that of PivNet(-itr) was 0.03 (0.01), 0.02 (0.003),\n0.01 (0.005), 0.04 (0.003), 0.01 (0.003), and 0.02 (0.005) on Crime,\nHEPMASS, Household, PAMAP2, Places, and Wisdom, respectively.\nEvaluation criteria. To measure the accuracy of each inference\nmodel, we computed the average and median of ğ‘€ğ´ğ¸ ğ‘, which is\ndefined in Equation (2), and those of ğ‘€ğ´ğ‘ƒğ¸ ğ‘among all queries in\nthe test query set. Note that ğ‘€ğ´ğ‘ƒğ¸ ğ‘is obtained as follows:\nğ‘€ğ´ğ‘ƒğ¸ ğ‘=1\nğ‘˜ğ‘šğ‘ğ‘¥ğ‘˜ğ‘šğ‘ğ‘¥âˆ‘ï¸\nğ‘˜=1|ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜ğ‘)âˆ’ Ë†ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜ğ‘)|\nğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘¥ğ‘˜ğ‘).\nAs for running time, we measured the average time to obtain the\nğ‘˜ğ‘šğ‘ğ‘¥-NN distances of a given query. In addition, we measured the\ntraining time of each ML model.\n4.2 Result\nTables 1 and 2 respectively show the results of MAE and MAPE\n(smaller is better), while Table 3 shows the result of time.\nEffectiveness of using neural network. First, let us compare\nPivNet (or PivNet-itr) with Pivot to study the effectiveness of uti-\nlizing neural network. Tables 1 and 2 show that the accuracy of\nPivNet(-itr) is better than that of Pivot. This result suggests that our\nneural network does not simply use the ğ‘˜-NN distances of pivots as\ninference but learns an accurate regression model. Notice that Pivot\nprovides upper-bounds of ğ‘˜-NN distances, see Equation (3). These\nbounds become loose as ğ‘‘increases. For example, Pivot yields a\nsmall MAE on Crime and Places ( ğ‘‘=2) but its accuracy degrades\non PAMAP2 ( ğ‘‘=5).\nMoreover, we see that PivNet yields better accuracy than Light-\nGBM in most tests. This result also justifies our choice of utilizing\na neural network.\nEffectiveness of using pivots. We next investigate the usefulness\nof pivots by comparing PivNet with QueryNet. From Table 1, we see\nthat PivNet is always better than QueryNet. Also, Table 2 shows that\nPivNet is better than QueryNet with only a single exception of the\naverage case of PAMAP2. (Recall that, as mentioned in Section 2, we\nweight MAE.) For example, PivNet provides about 3.9x (3.7x), 1.1x\n(1.4x), 1.9x (1.8x), 1,5x (1.3x), 6.9x (7.3x), and 4.1x (3.3x) less MAE\nthan QueryNet in the average (median) case on Crime, HEPMASS,\nHousehold, PAMAP2, Places, and Wisdom, respectively. Clearly,\npivots and their ğ‘˜-NN distances support accurate model training.\nComparison with PivNet-itr. For PivNet and PivNet-itr, we study\nhow the difference in neural network structures impacts their per-\nformances. Tables 1 and 2 show that their accuracy is generally\ncompetitive. This actually suggests that, although PivNet learns\neveryğ‘˜âˆˆ[1,ğ‘˜ğ‘šğ‘ğ‘¥]at the same time, it still effectively learns an\naccurate regression model. When PivNet-itr yields a less error (e.g.,\n\nTable 1: Average and median of MAE\nCrime HEPMASS Household PAMAP2 Places Wisdom\nLightGBMAverage 0.00057 0.05989 0.09785 0.09005 0.09216 0.03367\nMedian 0.00014 0.01714 0.02031 0.04586 0.00284 0.01168\nPivotAverage 0.00049 0.12295 0.28569 0.38003 0.00811 0.05177\nMedian 0.00046 0.12486 0.28337 0.37429 0.00742 0.05077\nQueryNetAverage 0.00075 0.02225 0.02949 0.07335 0.07702 0.04213\nMedian 0.00048 0.01597 0.01691 0.04512 0.02336 0.02483\nPivNet-itrAverage 0.00018 0.00977 0.01024 0.03828 0.00711 0.01184\nMedian 0.00012 0.00578 0.00536 0.02499 0.00313 0.00806\nPivNetAverage 0.00019 0.01965 0.01852 0.04768 0.01113 0.01038\nMedian 0.00013 0.01117 0.00916 0.03528 0.00320 0.00748\nTable 2: Average and median of MAPE\nCrime HEPMASS Household PAMAP2 Places Wisdom\nLightGBMAverage 0.14 0.14 0.33 0.38 0.78 0.37\nMedian 0.07 0.06 0.14 0.06 0.11 0.05\nPivotAverage 0.35 0.95 7.32 2.16 4.19 0.93\nMedian 0.15 0.40 1.46 0.45 0.26 0.19\nQueryNetAverage 0.38 0.10 0.19 0.12 6.45 0.34\nMedian 0.27 0.06 0.08 0.07 0.74 0.12\nPivNet-itrAverage 0.14 0.04 0.11 0.07 1.09 0.12\nMedian 0.06 0.02 0.03 0.04 0.13 0.04\nPivNetAverage 0.14 0.07 0.14 0.17 0.89 0.11\nMedian 0.06 0.04 0.04 0.05 0.14 0.03\nTable 3: Average time to obtain ğ‘˜ğ‘šğ‘ğ‘¥-NN distances and training time\nCrime HEPMASS Household PAMAP2 Places Wisdom\nğ‘˜d-tree Retrieval time [microsec] 56.04 148.02 104.68 154.18 54.46 65.77\nLightGBMInference time [microsec] 13.20 11.35 11.44 11.36 13.67 13.47\nTraining time [min] 0.32 0.34 0.32 0.34 0.32 0.31\nPivot Inference time [microsec] 1.36 1.60 1.15 1.48 1.62 1.67\nQueryNetInference time [microsec] 1.50 2.02 2.23 2.63 1.05 2.14\nTraining time [min] 6 2 12 5 7 7\nPivNet-itrInference time [microsec] 191.49 193.84 192.37 193.37 192.38 196.04\nTraining time [min] 119 575 217 746 47 232\nPivNetInference time [microsec] 3.24 3.94 5.65 5.13 3.58 4.81\nTraining time [min] 15 6 9 3 10 38\nTable 4: MAE of PivNet with different ğ‘˜\nCrime HEMPASS Household PAMAP2 Places Wisdom\nğ‘˜âˆˆ[1,10] 0.00023 0.02214 0.02001 0.05660 0.02167 0.01315\nğ‘˜âˆˆ[11,20]0.00019 0.01930 0.01790 0.04582 0.00995 0.01029\nğ‘˜âˆˆ[21,30]0.00019 0.01888 0.01775 0.04497 0.00648 0.00961\nğ‘˜âˆˆ[31,40]0.00017 0.01888 0.01827 0.04525 0.00733 0.00958\nğ‘˜âˆˆ[41,50]0.00019 0.01926 0.01890 0.04665 0.01097 0.00951\nHEPMASS case) and applications need a single ğ‘˜, PivNet-itr can be\nan option.\nInference time. We turn our attention to time to obtain ğ‘˜-NN\ndistances. From Table 3, we first observe that Pivot is the fastest.\nThis is reasonable, because it simply computes Equation (3), i.e.,just â€œ+â€ operation is required after computing ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘). Recall that\nthe accuracy of Pivot is the worst in most cases.\nSecond, QueryNet is faster than PivNet(-itr). This is also a natu-\nral observation. QueryNet does not use pivots, so the number of\nneurons in its input layer is smaller and it does not need to compute\n\nthe cell of a given query and the distance between the query and\nits nearest pivot. These provide the time difference. We next see\nthat PivNet-itr is the slowest, i.e., it is outperformed by the exact\napproachğ‘˜d-tree. Since PivNet-itr needs ğ‘˜ğ‘šğ‘ğ‘¥ times inferences,\nthis is also a reasonable result. (However, if we need a single ğ‘˜, its\ninference time becomes ğ‘˜ğ‘šğ‘ğ‘¥ times faster.)\nPivNet is slower than Pivot and QueryNet, but its accuracy is\nmuch better than those of them and its inference is absolutely fast\nand stable (as it needs a constant time for inference). In addition,\nits inference times are about 17.3x (4.1x), 37.6x (2.9x), 18.5x (2.0x),\n39.8x (2.9x), 15.2x (3.8x), and 13.7x (2.8x) faster than those of ğ‘˜d-tree\n(LightGBM) on Crime, HEPMASS, Household, PAMAP2, Places, and\nWisdom, respectively. The above results demonstrate that PivNet\nclearly has the best trade-off relationship between inference time\nand accuracy among the evaluated methods.\nTraining times of ML methods are also shown in Table 3. Light-\nGBM has less training time than the neural network models, but\nPivNet also spent reasonable time for one-time training. For neural\nnetwork models, their tendencies of training time clearly follow\nthat of inference time, because the training time also depends on\nthe structure of a given neural network. Although we assume static\ndatasets, PivNet can deal with dataset updates if the update fre-\nquency is moderate (e.g., updates are reflected in a batch manner\nonce a few hours), since its training time is not significant.\nImpact ofğ‘˜.We next study the error of PivNet by decomposing\n[1,50]into five disjoint subsets as shown in Table 4. (We omit\nMAPE, since its tendency simply follows that of MAE.) We observe\nthat PivNet has a higher MAE in the case of ğ‘˜âˆˆ[1,10]compared\nwith the other cases. When ğ‘˜is small, a density difference in ğ‘‹\nclearly appears. As the number of points existing in sparse spaces\nis much smaller than that of points in dense spaces, perhaps neural\nnetworks still face difficulty of estimating accurate ğ‘˜-NN distances\nfor such sparse points. However, the errors are still sufficiently\nsmall. (Recall that the inference time of PivNet is not affected by ğ‘˜\niffğ‘˜â‰¤ğ‘˜ğ‘šğ‘ğ‘¥.)\nIs a deeper neural network better? Last, we address this ques-\ntion by using PivNet-5, which has five hidden layers (recall that\nPivNet originally has three hidden layers). The MAE (average case)\nof PivNet-5 was 0.00019, 0.02399, 0.02051, 0.04859, 0.01163, and\n0.01166 on Crime, HEPMASS, Household, PAMAP2, Places, and\nWisdom, respectively. Also, its inference time [microsec] was 3.62,\n5.19, 5.00, 5.13, 5.74, and 5.37 on Crime, HEPMASS, Household,\nPAMAP2, Places, and Wisdom, respectively. Comparing with the\nresults in Tables 1 and 3, PivNet-5 does not improve accuracy and its\ninference time is slower. We therefore conclude that deeper neu-\nral networks are not necessary for PivNet and it can learn\nan accurate model with a small number of hidden layers .\n5 CASE STUDY\nWe tested our ğ‘˜-NN distance estimation model in the application\nexamples in Section 1. In this section, we set ğ‘˜ğ‘šğ‘ğ‘¥=100, and\nthe hyper-parameters were the same as those in Section 4. We\ncomputed the exact results by using a ğ‘˜d-tree.5.1ğ‘˜-NN Density Visualization\nSetting. We used Crime and Places in this case study. For each\ndataset, we made 1,000Ã—1,000pixels by equally dividing each\ndomain. We used the centroid of each pixel as a query and computed\nitsğ‘˜-NN density from Equation (1), where ğ‘˜=100. The result is\nvisualized by a contour diagram, and its interval is determined by\nusing 20%, 60%, and 90% percentiles.\nResult. Figure 2 illustrates the visualization results on Crime and\nPlaces. Note that green, blue, and red show high, middle, and low\ndensities, respectively. For example, the number of crimes is higher\nin the green areas of Crime. Although there are minor differences,\nPivNet displays almost the same result as the exact solution. There-\nfore, PivNet would not hinder the detection of information from\nthe visualization results.\nPivNetâ€™s (ğ‘˜d-treeâ€™s) time to compute (estimate) the ğ‘˜-NN density\nof all pixels was 3.1 (16.2) and 3.1 (35.8) seconds on Crime and Places,\nrespectively. PivNet is much faster than ğ‘˜d-tree and completes the\ncomputation within a few seconds for a million pixels.\n5.2 Distance-based Outlier Detection\nWe next consider the distance-based outlier detection problems\ndefined below.\nDefinition 3 ((ğ‘Ÿ,ğ‘˜)-distance-based outlier detection problem\n[27]).Given a set ğ‘‹of points,ğ‘˜, and a distance-threshold ğ‘Ÿ, this\nproblem is to detect all points ğ‘¥âˆˆğ‘‹such that the number of points\nğ‘¥â€²satisfyingğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘¥,ğ‘¥â€²)â‰¤ğ‘Ÿis less thanğ‘˜.\nDefinition 4 ((ğ‘,ğ‘˜)-distance-based outlier detection prob-\nlem [40]).Given a set ğ‘‹of points,ğ‘˜, andğ‘, this problem finds ğ‘\npointsğ‘¥âˆˆğ‘‹whose distances to their ğ‘˜-th nearest neighbor are the\nlargest among the points in ğ‘‹.\nBoth problems can be solved by evaluating the distance to the ğ‘˜-th\nnearest neighbor for each ğ‘¥âˆˆğ‘‹.\nSetting. In this study, we set ğ‘˜=50. For the(ğ‘Ÿ,ğ‘˜)-distance-based\noutlier detection ((ğ‘Ÿ,ğ‘˜)-DOD) problem, we set ğ‘Ÿso that the number\nof outliers is 1,000. Similarly, we set ğ‘=1,000for the(ğ‘,ğ‘˜)-DOD\nproblem. To measure the accuracy, we used precision andrecall . Let\nğ‘‡ğ‘ƒ,ğ¹ğ‘ƒ, andğ¹ğ‘be the numbers of true positives, false positives,\nand false negatives, respectively. Precision and recall are defined as\nfollows:\nğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› =ğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ, ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ =ğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ+ğ¹ğ‘.\nFor evaluation, we used PivNet-itr (PivNet) on HEPMASS, House-\nhold, PAMAP2, and Wisdom.\nResult. Table 5 shows the accuracy of our estimation-based detec-\ntion. We see that its detection accuracy is high (i.e., both ğ¹ğ‘ƒand\nğ¹ğ‘are small). In particular, we achieve almost perfect recall on\nHEPMASS, Household, and Wisdom. This result also confirms its\nhigh inference accuracy.\nThe detection time is shown in Table 6. Our approach is again\nmuch faster than ğ‘˜d-tree. The time complexity of ğ‘˜d-tree for retriev-\ningğ‘˜-NNs isğ‘‚(ğ‘˜ğ‘›1âˆ’1/ğ‘‘), whereğ‘›=|ğ‘‹|. It hence incurs ğ‘‚(ğ‘˜ğ‘›2âˆ’1/ğ‘‘)\ntime for the distance-based outlier detection problems6, and it takes\na long time for DOD on large datasets, e.g., HEPMASS. On the\n6This is faster than other works [5, 7, 37, 46], because they incur ğ‘‚(ğ‘›2)time.\n\n(a) Exact on Crime\n (b) Estimation by PivNet on Crime\n (c) Exact on Places\n (d) Estimation by PivNet on Places\nFigure 2:ğ‘˜-NN density visualization\nTable 5: Precision and recall in the problems of distance-based outlier detection\nHEPMASS Household PAMAP2 Wisdom\nPrecision Recall Precision Recall Precision Recall Precision Recall\n(ğ‘Ÿ,ğ‘˜)-DOD 0.92 0.94 0.85 0.94 0.86 0.86 0.94 0.95\n(ğ‘,ğ‘˜)-DOD - 0.93 - 0.89 - 0.86 - 0.95\nTable 6: Running time [sec] for distance-based outlier detec-\ntion\nHEPMASS Household PAMAP2 Wisdom\nğ‘˜d-tree 964 93 248 102\nOurs 21 3 7 13\nother hand, for each point in ğ‘‹, we need only ğ‘‚(1)time to evaluate\n(estimate) the ğ‘˜NN distance, thus our approach needs ğ‘‚(ğ‘›)time\nfor the problems. Its scalability to large datasets is hence obvious.\nPivNet(-itr) can be a good option to quickly deal with such datasets\nof applications that allow approximate results. This is often the\ncase, because spatial data analysis requires interaction [ 20,49], e.g.,\noutliers are evaluated while varying ğ‘˜,ğ‘Ÿ, orğ‘.\n5.3 Reverse Engineering for Density-Peaks\nClustering\nWe next consider the estimation of a missing parameter of density-\nbased clustering by utilizing PivNet. As density-based clustering,\nwe focus on density-peaks clustering (DPC) [ 42]. This is a com-\nparatively new algorithm but already has many applications [ 2].\n(For another famous density-based clustering DBSCAN, a reverse\nengineering-like tool has been already provided in [13].)\nFor eachğ‘¥ğ‘–âˆˆğ‘‹, DPC computes the local density ğœŒğ‘–, which is\nthe number of points ğ‘¥ğ‘—such thatğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘¥ğ‘–,ğ‘¥ğ‘—)â‰¤ğ‘‘ğ‘ğ‘¢ğ‘¡, whereğ‘‘ğ‘ğ‘¢ğ‘¡\nis a user-specified distance threshold. In addition, for each ğ‘¥ğ‘–âˆˆğ‘‹,\nDPC computes its dependent point, which is the nearest point of\nğ‘¥ğ‘–with higher local density than ğœŒğ‘–, and the dependent distance\nğ›¿ğ‘–, which is the distance between ğ‘¥ğ‘–and its dependent point. As\nwith Definition 3, DPC removes noises that have less local density\nthanğœŒğ‘šğ‘–ğ‘›. It is intuitively seen that, if a non-noise point ğ‘¥has a\nlarge dependent distance, it is a density-peak in the space centered\natğ‘¥. DPC employs this concept and regards it as a cluster center.\nConsider the dataset illustrated in Figure 3(a). After we compute ğœŒğ‘–\nandğ›¿ğ‘–for eachğ‘¥ğ‘–âˆˆğ‘‹,âŸ¨ğœŒğ‘–,ğ›¿ğ‘–âŸ©can be mapped into a 2-dimensionalspace called a decision graph, see Figure 3. It visualizes that there\nare some points with large dependent distances, i.e., cluster centers.\nBy setting a threshold ğ›¿ğ‘šğ‘–ğ‘›of dependent distance (e.g., ğ›¿ğ‘šğ‘–ğ‘›=800\nin Figure 3(d)), DPC determines cluster centers, and the other non-\nnoise points belong to the same clusters as their dependent points.\nIt is often the case that analysis (e.g., clustering) results are\nknown but (some of) their parameters are missing [ 25]. In the con-\ntext of DPC, it is possible that we have cluster labels and a decision\ngraph butğ‘‘ğ‘ğ‘¢ğ‘¡is missing. To reproduce the results, a straightfor-\nward approach is to run DPC while testing some values of ğ‘‘ğ‘ğ‘¢ğ‘¡.\nHowever, DPC needs ğ‘œ(ğ‘›2)time [ 2] (on low-dimensional datasets),\nso running DPC multiple times is not promising. We alleviate this\nconcern by estimating ğ‘‘ğ‘ğ‘¢ğ‘¡and consider how to achieve this.\nFor simplicity, let us assume that ğœŒğ‘šğ‘–ğ‘›and the number of noises\n(denoted by ğ‘š) are known. (They are obtained from cluster labels\nand the decision graph.) From Section 5.2, we can consider that\neach noise point ğ‘¥has the top-ğ‘šlargest distance between ğ‘¥and its\nğœŒğ‘šğ‘–ğ‘›-NN. Therefore, by using the top- ğ‘šlargestğœŒğ‘šğ‘–ğ‘›-NN distance\nasğ‘‘ğ‘ğ‘¢ğ‘¡, we can (approximately) distinguish non-noise points from\nnoises and then reproduce the clustering result.\nTo test this idea, we used the synthetic dataset Syn in Figure 3(a),\nwhich was generated based on a random walk model [ 22]. Syn has\n200,000 points and eight density-peaks. The exact clustering result\nwithğ‘‘ğ‘ğ‘¢ğ‘¡=200and its decision graph are respectively illustrated\nin Figures 3(b) and 3(d) (best viewed in color). We set ğœŒğ‘šğ‘–ğ‘›=50,\nand then Syn has 186 noises. Next, we used PivNet to estimate\ntheğœŒğ‘šğ‘–ğ‘›-NN distance for each point. As a result, the 186-th largest\n50-NN distance was 179.23. We used this value as the estimated ğ‘‘ğ‘ğ‘¢ğ‘¡\nand ran DPC. The clustering result and decision graph are depicted\nin Figures 3(c) and 3(e), respectively. It can be seen that our idea\nreproduces the clustering result almost perfectly. This reproduction\nneeded only one-time DPC and ğ‘‚(ğ‘›logğ‘›)time to estimate ğ‘‘ğ‘ğ‘¢ğ‘¡\n(ğ‘‚(ğ‘›)time forğœŒğ‘šğ‘–ğ‘›-NN distance estimation and ğ‘‚(ğ‘›logğ‘›)time\nto sort the distances), demonstrating that PivNet efficiently helps\nreverse engineering.\n\n(a) Visualization of Syn\n (b) Clustering result\n (c) Clustering result via estimated ğ‘‘ğ‘ğ‘¢ğ‘¡\n0 2000 4000 6000 8000 10000 12000 14000\nÏ025050075010001250150017502000Î´\n(d) Decision graph\n0 2000 4000 6000 8000 10000 12000\nÏ025050075010001250150017502000Î´\n (e) Decision graph via estimated ğ‘‘ğ‘ğ‘¢ğ‘¡\nFigure 3: Comparison of clustering results and decision graphs via reverse engineering\nTable 7: Running time [microsec] for (approximate) ğ‘˜-NN search\nğ‘˜ Crime HEPMASS Household PAMAPA2 Places Wisdom\nExact25 47 122 87 116 47 55\n50 56 148 105 154 54 66\n75 54 176 120 183 61 74\n100 59 203 133 210 66 83\nApproximate25 26 60 66 85 22 25\n50 29 76 83 114 22 32\n75 29 89 86 129 23 32\n100 30 104 94 149 23 33\nTable 8: Recall of A ğ‘˜NN search\nğ‘˜ Crime HEPMASS Household PAMAP2 Places Wisdom\n25Average 0.86 0.91 0.84 0.80 0.91 0.85\nMedian 1.00 1.00 0.96 1.00 1.00 1.00\n50Average 0.8 0.93 0.93 0.80 0.88 0.90\nMedian 1.00 1.00 1.00 1.00 1.00 1.00\n75Average 0.89 0.94 0.86 0.81 0.88 0.88\nMedian 1.00 1.00 0.93 1.00 1.00 1.00\n100Average 0.91 0.95 0.88 0.81 0.88 0.88\nMedian 1.00 1.00 0.94 1.00 1.00 1.00\n5.4 Approximate ğ‘˜-NN Search\nLast, we apply our distance estimation to approximate ğ‘˜-NN search.\nIn traditional ğ‘˜-NN search algorithms, the threshold is initialized at\nâˆ, and it is updated during the search. If we can use a threshold thatis (very) close to the exact ğ‘˜-NN distance at initialization, the search\nefficiency is accelerated (although it can produce an approximate\nresult when it is smaller than the exact value). We tested this idea\nby using the datasets in Section 4. For each dataset, we randomly\n\nsampled 10,000 points and used them as queries. We initialized the\nthreshold by our estimation and ran ğ‘˜-NN search on a ğ‘˜d-tree.\nTable 7 shows the running time (inference time is also included)\nwithğ‘˜=25,50,75, and 100. We achieved 1.2xâ€“2.9x speed-up,\ncompared with the exact retrieval. This result confirms that (i) the\nabove idea is effective and (ii) our estimation does not yield a loose\nthreshold.\nTable 8 shows the average and median recall of the approximate\nğ‘˜-NN search results. The average recall is generally high but is less\nthan 1. This means that, for some queries, the estimated thresholds\nwere less than the exact values, yielding answer sets with less than ğ‘˜\npoints. However, this is easily alleviated. For example, if applications\nrequire 25 nearest points, they can specify ğ‘˜that is larger than 25\n(e.g.,ğ‘˜=30), since the time of the estimation-based approximate\nğ‘˜-NN search does not change much even if ğ‘˜increases a bit. It is\nalso important to note that the estimation-based approximate ğ‘˜-NN\nsearch (i) is robust w.r.t. ğ‘˜(which is consistent with the result in\nSection 4.2) and (ii) provides the exact result (with less running\ntime) for many queries, see the median result.\n6 RELATED WORK\nSimilarity search is a primitive operator for many applications,\nand efficient processing of range and ğ‘˜-NN queries has been signif-\nicantly studied. Representative approaches are space partitioning\nones, such as ğ‘˜d-tree and R-tree. Given a range or ğ‘˜-NN query,\na branch-and-bound or best-first algorithm is conducted on such\na tree to prune unnecessary nodes. However, when we want to\nknow the distances to the ğ‘˜-NNs of the query, these approaches are\nnot efficient, since they involve many data accesses. For example,\nğ‘˜d-trees incur ğ‘‚(ğ‘˜ğ‘›1âˆ’1/ğ‘‘)data accesses to retrieve ğ‘˜-NNs [47].\nRecently, a concept â€œindex as a modelâ€ [ 28] has been intro-\nduced to build indices of datasets, and learned indices have been\ndevised. The first learned index RMI [ 28] was considered for one-\ndimensional data to alleviate the traversal costs of B+-trees. An\nRMI has some neural network models that estimate the position\nof a given query, and this approach can skip many data accesses.\nSome works introduced learned indices to multi-dimensional data\n[19,29,36,38,56]. However, their objectives are different from ours\nor their adaptability is limited. For example, literature [ 56] consid-\nered only 2-dimensional data. Literatures [ 19,36] assume multi-\nattribute data and focus on relational range queries that search ğ‘¥\nsatisfying(ğ‘â‰¤ğ‘¥[1]â‰¤ğ‘)âˆ§Â·Â·Â·âˆ§(ğ‘â€²â‰¤ğ‘¥[ğ‘‘]â‰¤ğ‘â€²), whereğ‘¥[ğ‘–]\nshows theğ‘–-th attribute of ğ‘¥. Although [ 29,38] support (approx-\nimate)ğ‘˜-NN queries, their objective is to minimize disk I/O cost\nand the indices are specific to disk blocks. Their approaches are\ntrivially not suitable for in-memory distance estimation, and Pivot\nin Section 4 is a better competitor as it needs only ğ‘‚(1)time for\nour problem.\nCardinality/Selectivity estimation is also related to our work.\nIn a nutshell, this problem estimates the output size of a range\nquery. Query cost estimation and making a batch query processing\nschedule receive benefits from this problem. A classic approach that\ntries to accurately estimate the cardinality of a given query is multi-\ndimensional histogram [ 12,23]. Given a query and a threshold, this\napproach finds the bin that corresponds to the query. Assuming that\npoints in this bin follows a specific (e.g., uniform) distribution, thisapproach estimates the cardinality. However, real datasets generally\ndo not follow a specific distribution, so the accuracy of this approach\ntends to be low.\nTo effectively adapt to real data distributions, ML-based cardinal-\nity estimation techniques were devised, e.g., in [ 24,31,45,57]. Some\nworks, e.g., [ 21], employ decision-tree-based models [ 18]. (Section\n4 shows that PivNet beats the state-of-the-art decision-tree-based\nmodel.) Neural network models were also considered for cardinality\nestimation [ 45,51,52,57], but they cannot provide multiple values\nas the output at a time. Moreover, it is not trivial to employ these\ncardinality estimation models in our problem.\n7 CONCLUSION\nMotivated by the importance of scaling proximity-based data anal-\nysis, we addressed the problem of ğ‘˜-NN distance estimation. As a\nsolution for this problem, we proposed a machine-learning tech-\nnique, PivNet. This is a neural network-based regression model\nthat estimates the distances to the ğ‘˜-NNs of a given query at a time\ninğ‘‚(1)time. This model is trained not only from training queries\nbut also from their nearest pivots to make it more accurate. We con-\nducted experiments on real datasets. The results demonstrate that\nPivNet yields a small estimation error while keeping fast inference\ntime. Moreover, we did case studies to investigate the usefulness of\nour model for existing proximity-based data analysis tools. Thanks\nto its accuracy and efficiency, we observed that it provides faster\nanalysis with high accuracy.\nThis paper focused on low-dimensional data. It is natural to use\ndimensionality reduction for high-dimensional data cases, such as\nprincipal component analysis and t-SNE [ 33], in order to fit them\ninto our assumption. However, dimensionality reduction may break\nthe original global distribution, so there would exist applications\nthat need to deal with high-dimensional data as it is. Estimation of\nğ‘˜-NN distances for high-dimensional data is an open problem.\nACKNOWLEDGMENTS\nThis research is partially supported by JSPS Grant-in-Aid for Sci-\nentific Research (A) Grant Number 18H04095, JST PRESTO Grant\nNumber JPMJPR1931, and JST CREST Grant Number JPMJCR21F2.\nREFERENCES\n[1]Daichi Amagata, Yusuke Arai, Sumio Fujita, and Takahiro Hara. 2022. Learned\nk-NN Distance Estimation. In SIGSPATIAL . 1:1â€“1:4.\n[2]Daichi Amagata and Takahiro Hara. 2021. Fast Density-Peaks Clustering:\nMulticore-based Parallelization Approach. In SIGMOD . 49â€“61.\n[3]Daichi Amagata, Makoto Onizuka, and Takahiro Hara. 2021. Fast and Exact\nOutlier Detection in Metric Spaces: A Proximity Graph-based Approach. In\nSIGMOD . 36â€“48.\n[4]Daichi Amagata, Makoto Onizuka, and Takahiro Hara. 2022. Fast, Exact, and\nParallel-friendly Outlier Detection Algorithms with Proximity Graph in Metric\nSpaces. The VLDB Journal (2022).\n[5]Fabrizio Angiulli and Fabio Fassetti. 2009. DOLPHIN: An Efficient Algorithm for\nMining Distance-based Outliers in Very Large Datasets. ACM Transactions on\nKnowledge Discovery from Data (TKDD) 3, 1 (2009), 1â€“57.\n[6]Pierre Baldi, Kyle Cranmer, Taylor Faucett, Peter Sadowski, and Daniel Whiteson.\n2016. Parameterized machine learning for high-energy physics. arXiv preprint\narXiv:1601.07913 (2016).\n[7]Stephen D Bay and Mark Schwabacher. 2003. Mining Distance-based Outliers in\nNear Linear Time with Randomization and a Simple Pruning Rule. In SIGKDD .\n29â€“38.\n[8]Jon Louis Bentley. 1975. Multidimensional Binary Search Trees Used for Associa-\ntive Searching. Commun. ACM 18, 9 (1975), 509â€“517.\n\n[9]Alina Beygelzimer, Sham Kakade, and John Langford. 2006. Cover trees for\nNearest Neighbor. In ICML . 97â€“104.\n[10] GÃ©rard Biau, FrÃ©dÃ©ric Chazal, David Cohen-Steiner, Luc Devroye, and Carlos\nRodriguez. 2011. A Weighted k-Nearest Neighbor Density Estimate for Geometric\nInference. Electronic Journal of Statistics 5 (2011), 204â€“237.\n[11] Azzedine Boukerche, Lining Zheng, and Omar Alfandi. 2020. Outlier Detection:\nMethods, Models, and Classification. Comput. Surveys 53, 3 (2020), 1â€“37.\n[12] Nicolas Bruno, Surajit Chaudhuri, and Luis Gravano. 2001. STHoles: A Multidi-\nmensional Workload-aware Histogram. In SIGMOD . 211â€“222.\n[13] Ricardo JGB Campello, Davoud Moulavi, Arthur Zimek, and JÃ¶rg Sander. 2015.\nHierarchical Density Estimates for Data Clustering, Visualization, and Outlier\nDetection. ACM Transactions on Knowledge Discovery from Data 10, 1 (2015),\n1â€“51.\n[14] Lei Cao, Yizhou Yan, Caitlin Kuhlman, Qingyang Wang, Elke A Rundensteiner,\nand Mohamed Eltabakh. 2017. Multi-tactic Distance-based Outlier Detection. In\nICDE . 959â€“970.\n[15] Tsz Nam Chan, Reynold Cheng, and Man Lung Yiu. 2020. QUAD: Quadratic-\nbound-based Kernel Density Visualization. In SIGMOD . 35â€“50.\n[16] Tsz Nam Chan, Zhe Li, Jianliang Xu, and Reynold Cheng. 2021. Fast Augmentation\nAlgorithms for Network Kernel Density Visualization. PVLDB 14, 9 (2021), 1503â€“\n1516.\n[17] Varun Chandola, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly Detection:\nA Survey. ACM computing surveys 41, 3 (2009), 1â€“58.\n[18] Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A Scalable Tree Boosting\nSystem. In SIGKDD . 785â€“794.\n[19] Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020.\nTsunami: A Learned Multi-dimensional Index for Correlated Data and Skewed\nWorkloads. PVLDB 14, 2 (2020), 74â€“86.\n[20] Liming Dong, Qiushi Bai, Taewoo Kim, Taiji Chen, Weidong Liu, and Chen Li.\n2020. Marviq: Quality-Aware Geospatial Visualization of Range-Selection Queries\nUsing Materialization. In SIGMOD . 67â€“82.\n[21] Anshuman Dutt, Chi Wang, Azade Nazi, Srikanth Kandula, Vivek Narasayya,\nand Surajit Chaudhuri. 2019. Selectivity Estimation for Range Predicates using\nLightweight Models. PVLDB 12, 9 (2019), 1044â€“1057.\n[22] Junhao Gan and Yufei Tao. 2015. DBSCAN revisited: Mis-claim, Un-fixability,\nand Approximation. In SIGMOD . 519â€“530.\n[23] Yannis Ioannidis. 2003. The History of Histograms (Abridged). In VLDB . 19â€“30.\n[24] Yuchen Ji, Daichi Amagata, Yuya Sasaki, and Takahiro Hara. 2022. A Performance\nStudy of One-dimensional Learned Cardinality Estimation. In DOLAP . 86â€“90.\n[25] Dmitri V Kalashnikov, Laks VS Lakshmanan, and Divesh Srivastava. 2018.\nFastQRE: Fast Query Reverse Engineering. In SIGMOD . 337â€“350.\n[26] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,\nQiwei Ye, and Tie-Yan Liu. 2017. LightGBM: a highly efficient gradient boosting\ndecision tree. In NIPS . 3149â€“3157.\n[27] Edwin M Knox and Raymond T Ng. 1998. Algorithms for Mining Distance-based\nOutliers in Large Datasets. In VLDB . 392â€“403.\n[28] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In SIGMOD . 489â€“504.\n[29] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. 2020. LISA: A Learned\nIndex Structure for Spatial Data. In SIGMOD . 2119â€“2133.\n[30] Wen Li, Ying Zhang, Yifang Sun, Wei Wang, Mingjie Li, Wenjie Zhang, and\nXuemin Lin. 2020. Approximate Nearest Neighbor Search on High Dimensional\nDataâ€”Experiments, Analyses, and Improvement. IEEE Transactions on Knowledge\nand Data Engineering 32, 8 (2020), 1475â€“1488.\n[31] Qiyu Liu, Yanyan Shen, and Lei Chen. 2021. LHist: Towards Learning Multi-\ndimensional Histogram for Massive Spatial Data. In ICDE . 1188â€“1199.\n[32] Don O Loftsgaarden and Charles P Quesenberry. 1965. A Nonparametric Estimate\nof a Multivariate Density Function. The Annals of Mathematical Statistics 36, 3\n(1965), 1049â€“1051.\n[33] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing Data Using\nt-SNE. Journal of Machine Learning Research 9, Nov (2008), 2579â€“2605.\n[34] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra,\nAlfons Kemper, Thomas Neumann, and Tim Kraska. 2020. Benchmarking Learned\nIndexes. PVLDB 14, 1 (2020), 1â€“13.\n[35] Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve restricted\nboltzmann machines. In ICML . 807â€“814.\n[36] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning Multi-dimensional Indexes. In SIGMOD . 985â€“1000.\n[37] Gustavo H Orair, Carlos HC Teixeira, Wagner Meira Jr, Ye Wang, and Srini-\nvasan Parthasarathy. 2010. Distance-based Outlier Detection: Consolidation and\nRenewed Bearing. PVLDB 3, 1-2 (2010), 1469â€“1480.\n[38] Jianzhong Qi, Guanli Liu, Christian S Jensen, and Lars Kulik. 2020. Effectively\nLearning Spatial Indices. PVLDB 13, 12 (2020), 2341â€“2354.\n[39] Jianzhong Qi, Yufei Tao, Yanchuan Chang, and Rui Zhang. 2018. Theoretically\nOptimal and Empirically Efficient R-trees with Strong Parallelizability. PVLDB\n11, 5 (2018), 621â€“634.\n[40] Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. 2000. Efficient Algo-\nrithms for Mining Outliers from Large Data Sets. In SIGMOD . 427â€“438.[41] Attila Reiss and Didier Stricker. 2012. Introducing a new benchmarked dataset\nfor activity monitoring. In ISWC . 108â€“109.\n[42] Alex Rodriguez and Alessandro Laio. 2014. Clustering by fast search and find of\ndensity peaks. Science 344, 6191 (2014), 1492â€“1496.\n[43] Sebastian Schelter, Tammo Rukat, and Felix Biessmann. 2021. JENGA-A Frame-\nwork to Study the Impact of Data Errors on the Predictions of Machine Learning\nModels.. In EDBT . 529â€“534.\n[44] Ji Sun and Guoliang Li. 2019. An End-to-End Learning-based Cost Estimator.\nPVLDB 13, 3 (2019), 307â€“319.\n[45] Ji Sun, Guoliang Li, and Nan Tang. 2021. Learned Cardinality Estimation for\nSimilarity Queries. In SIGMOD . 1745â€“1757.\n[46] Yufei Tao, Xiaokui Xiao, and Shuigeng Zhou. 2006. Mining Distance-based\nOutliers from Large Databases in any Metric Space. In SIGKDD . 394â€“403.\n[47] Csaba D Toth, Joseph Oâ€™Rourke, and Jacob E Goodman. 2017. Handbook of\nDiscrete and Computational Geometry .\n[48] Hongzhi Wang, Mohamed Jaward Bah, and Mohamed Hammad. 2019. Progress\nin Outlier Detection Techniques: A Survey. Ieee Access 7 (2019), 107964â€“108000.\n[49] Lu Wang, Robert Christensen, Feifei Li, and Ke Yi. 2015. Spatial Online Sampling\nand Aggregation. PVLDB 9, 3 (2015), 84â€“95.\n[50] Yiqiu Wang, Yan Gu, and Julian Shun. 2020. Theoretically-efficient and Practical\nParallel DBSCAN. In SIGMOD . 2555â€“2571.\n[51] Yaoshu Wang, Chuan Xiao, Jianbin Qin, Xin Cao, Yifang Sun, Wei Wang, and\nMakoto Onizuka. 2020. Monotonic Cardinality Estimation of Similarity Selection:\nA Deep Learning Approach. In SIGMOD . 1197â€“1212.\n[52] Yaoshu Wang, Chuan Xiao, Jianbin Qin, Rui Mao, Makoto Onizuka, Wei Wang,\nRui Zhang, and Yoshiharu Ishikawa. 2021. Consistent and Flexible Selectivity\nEstimation for High-dimensional Data. In SIGMOD . 2319â€“2327.\n[53] Gary M Weiss, Kenichi Yoneda, and Thaier Hayajneh. 2019. Smartphone and\nSmartwatch-based Biometrics using Activities of Daily Living. IEEE Access 7\n(2019), 133190â€“133202.\n[54] Zongheng Yang, Eric Liang, Amog Kamsetty, Chenggang Wu, Yan Duan, Xi Chen,\nPieter Abbeel, Joseph M Hellerstein, Sanjay Krishnan, and Ion Stoica. 2019. Deep\nUnsupervised Cardinality Estimation. PVLDB 13, 3 (2019), 279â€“292.\n[55] Qingchen Zhang, Chunsheng Zhu, Laurence T Yang, Zhikui Chen, Liang Zhao,\nand Peng Li. 2017. An incremental CFS algorithm for clustering large data in\nindustrial internet of things. IEEE Transactions on Industrial Informatics 13, 3\n(2017), 1193â€“1201.\n[56] Songnian Zhang, Suprio Ray, Rongxing Lu, and Yandong Zheng. 2021. SPRIG: A\nLearned Spatial Index for Range and kNN Queries. In SSTD . 96â€“105.\n[57] Rong Zhu, Ziniu Wu, Yuxing Han, Kai Zeng, Andreas Pfadler, Zhengping Qian,\nJingren Zhou, and Bin Cui. 2021. FLAT: Fast, Lightweight and Accurate Method\nfor Cardinality Estimation. PVLDB 14, 9 (2021), 1489â€“1502.\n[58] Carly GK Ziegler, Samuel J Allon, Sarah K Nyquist, Ian M Mbano, Vincent N\nMiao, Constantine N Tzouanas, Yuming Cao, Ashraf S Yousif, Julia Bals, Blake M\nHauser, et al .2020. SARS-CoV-2 receptor ACE2 is an interferon-stimulated gene\nin human airway epithelial cells and is detected in specific cell subsets across\ntissues. Cell181, 5 (2020), 1016â€“1035.",
  "textLength": 52206
}