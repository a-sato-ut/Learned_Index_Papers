{
  "paperId": "c29a7919c85a606b6da82f24cb3818ae5f6b4fd6",
  "title": "(Learned) Frequency Estimation Algorithms under Zipfian Distribution",
  "pdfPath": "c29a7919c85a606b6da82f24cb3818ae5f6b4fd6.pdf",
  "text": "(Learned) Frequency Estimation Algorithms under Zip\fan\nDistribution\nAnders Aamand\u0003Piotr IndykyAli Vakilianz\nAbstract\nThe frequencies of the elements in a data stream are an important statistical measure and the\ntask of estimating them arises in many applications within data analysis and machine learning.\nTwo of the most popular algorithms for this problem, Count-Min and Count-Sketch, are widely\nused in practice.\nIn a recent work [Hsu et al., ICLR'19], it was shown empirically that augmenting Count-\nMin and Count-Sketch with a machine learning algorithm leads to a signi\fcant reduction of\nthe estimation error. The experiments were complemented with an analysis of the expected\nerror incurred by Count-Min (both the standard and the augmented version) when the input\nfrequencies follow a Zip\fan distribution. Although the authors established that the learned\nversion of Count-Min has lower estimation error than its standard counterpart, their analysis of\nthe standard Count-Min algorithm was not tight. Moreover, they provided no similar analysis\nfor Count-Sketch.\nIn this paper we resolve these problems. First, we provide a simple tight analysis of the\nexpected error incurred by Count-Min. Second, we provide the \frst error bounds for both the\nstandard and the augmented version of Count-Sketch. These bounds are nearly tight and again\ndemonstrate an improved performance of the learned version of Count-Sketch.\nIn addition to demonstrating tight gaps between the aforementioned algorithms, we believe\nthat our bounds for the standard versions of Count-Min and Count-Sketch are of independent\ninterest. In particular, it is a typical practice to set the number of hash functions in those\nalgorithms to \u0002(log n). In contrast, our results show that to minimize the expected error, the\nnumber of hash functions should be a constant, strictly greater than 1.\n\u0003BARC, University of Copenhagen, aa@di.ku.dk\nyCSAIL, MIT, indyk@mit.edu\nzUniversity of Wisconsin-Madison, vakilian@wisc.eduarXiv:1908.05198v2  [cs.DS]  11 Aug 2020\n\n1 Introduction\nThe last few years have witnessed a rapid growth in using machine learning methods to solve\n\\classical\" algorithmic problems. For example, they have been used to improve the performance\nof data structures [KBC+18, Mit18], online algorithms [LV18, PSK18, GP19, Kod19, CGT+19,\nADJ+20, LLMV20, Roh20, ACE+20], combinatorial optimization [KDZ+17, BDSV18, Mit20], sim-\nilarity search [WLKC16, DIRW19], compressive sensing [MPB15, BJPD17] and streaming algo-\nrithms [HIKV19, IVY19, JLL+20, CGP20]. Multiple frameworks for designing and analyzing such\nalgorithms have been proposed [ACC+11, GR17, BDV18, AKL+19]. The rationale behind this line\nof research is that machine learning makes it possible to adapt the behavior of the algorithms to\ninputs from a speci\fc data distribution, making them more e\u000ecient or more accurate in speci\fc\napplications.\nIn this paper we focus on learning-augmented streaming algorithms for frequency estimation.\nThe latter problem is formalized as follows: given a sequence Sof elements from some universe U,\nconstruct a data structure that for any element i2Ucomputes an estimation ~fioffi, the number\nof timesioccurs inS. Since counting data elements is a very common subroutine, frequency\nestimation algorithms have found applications in many areas, such as machine learning, network\nmeasurements and computer security. Many of the most popular algorithms for this problem, such\nas Count-Min (CM) [CM05a] or Count-Sketch (CS) [CCFC02] are based on hashing. Speci\fcally,\nthese algorithms hash stream elements into Bbuckets, count the number of items hashed into each\nbucket, and use the bucket value as an estimate of item frequency. To improve the accuracy, the\nalgorithms use k>1 such hash functions and aggregate the answers. These algorithms have several\nuseful properties: they can handle item deletions (implemented by decrementing the respective\ncounters), and some of them (Count-Min) never underestimate the true frequencies, i.e., ~fi\u0015fi.\nIn a recent work [HIKV19], the authors showed that the aforementioned algorithm can be\nimproved by augmenting them with machine learning. Their approach is as follows. During the\ntraining phase, they construct a classi\fer (neural network) to detect whether an element is \\heavy\"\n(e.g., whether fiis among top kfrequent items). After such a classi\fer is trained, they scan the\ninput stream, and apply the classi\fer to each element i. If the element is predicted to be heavy, it\nis allocated a unique bucket, so that an exact value of fiis computed. Otherwise, the element is\nforwarded to a \\standard\" hashing data structure C, e.g., CM or CS. To estimate ~fi, the algorithm\neither returns the exact count fi(ifiis allocated a unique bucket) or an estimate provided by the\ndata structureC.1An empirical evaluation, on networking and query log data sets, shows that this\napproach can reduce the overall estimation error.\nThe paper also presents a preliminary analysis of the algorithm. Under the common assumption\nthat the frequencies follow the Zip\fan law, i.e.,2fi/1=i, fori= 1;:::;n for somen, and further\nthat itemiis queried with probability proportional to its frequency, the expected error incurred\nby the learning-augmented version of CM is shown to be asymptotically lower than that of the\n\\standard\" CM.3However, the exact magnitude of the gap between the error incurred by the\nlearned and standard CM algorithms was left as an open problem. Speci\fcally, [HIKV19] only\nshows that the expected error of standard CM with khash functions and a total of Bbuckets is\n1See Figure 1 for a generic implementation of the learning-based algorithms of [HIKV19].\n2In fact we will assume that fi= 1=i. This is just a matter of scaling and is convenient as it removes the\ndependence of the length of the stream in our bounds\n3This assumes that the error rate for the \\heaviness\" predictor is su\u000eciently low.\n1\n\nbetweenk\nBlog(k)andklog(k+2)=(k\u00001)(kn=B )\nB. Furthermore, no such analysis was presented for CS.\n1.1 Our results\nIn this paper we resolve the aforementioned questions left open in [HIKV19]. Assuming that the\nfrequencies follow a Zip\fan law, we show:\n\u000fAn asymptotically tight bound of \u0002(klog(kn=B )\nB) for the expected error incurred by the CM\nalgorithm with khash functions and a total of Bbuckets. Together with a prior bound for\nLearned CM (Table 1), this shows that learning-augmentation improves the error of CM by\na factor of \u0002(log( n)=log(n=B)) if the heavy hitter oracle is perfect.\n\u000fThe \frst error bounds for CS and Learned CS (see Table 1). In particular, we show that for\nLearned CS, a single hash function as in [HIKV19] leads to an asymptotically optimal error\nbound, improving over standard CS by a factor of \u0002(log( n)=log(n=B)) (same as CM).\nWe highlight that our results are presented assuming that we use a total ofBbuckets. With k\nhash functions, the range of each hash functions is therefore [ B=k]. We make this assumption since\nwe wish to compare the expected error incurred by the di\u000berent sketches when the total sketch size\nis \fxed.\nk= 1 k>1\nCount-Min (CM) \u0002\u0010\nlogn\nB\u0011\n[HIKV19] \u0002\u0012\nk\u0001log(kn\nB)\nB\u0013\nLearned Count-Min (L-CM) \u0002\u0010log2(n\nB)\nBlogn\u0011\n[HIKV19] \n\u0010log2(n\nB)\nBlogn\u0011\n[HIKV19]\nCount-Sketch (CS) \u0002\u0010\nlogB\nB\u0011\n\n\u0010\nk1=2\nBlogk\u0011\nandO\u0010\nk1=2\nB\u0011\nLearned Count-Sketch (L-CS) \u0002\u0010logn\nB\nBlogn\u0011\n\n\u0010logn\nB\nBlogn\u0011\nTable 1: This table summarizes our and previously known results on the expected frequency esti-\nmation error of Count-Min (CM), Count-Sketch (CS) and their learned variants (i.e., L-CM and\nL-CS) that use kfunctions and overall space k\u0002B\nkunder Zip\fan distribution. For CS, we assume\nthatkis odd (so that the median of kvalues is well de\fned).\nFor our results on L-CS in Table 1 we initially assume that the heavy hitter oracle is perfect ,\ni.e., that it makes no mistakes when classifying the heavy items. This is unlikely to be the case in\npractice, so we complement the results with an analysis of L-CS when the heavy hitter oracle may err\nwith probability at most \u000eon each item. As \u000evaries in [0;1], we obtain a smooth trade-o\u000b between\nthe performance of L-CS and its classic counterpart. Speci\fcally, as long as \u000e=O(1=logB), the\nbounds are as good as with a perfect heavy hitter oracle.\nIn addition to clarifying the gap between the learned and standard variants of popular frequency\nestimation algorithms, our results provide interesting insights about the algorithms themselves. For\nexample, for both CM and CS, the number of hash functions kis often selected to be \u0002(log n),\nin order to guarantee that every frequency is estimated up to a certain error bound. In contrast,\nwe show that if instead the goal is to bound the expected error, then setting kto a constant\n(strictly greater than 1) leads to the asymptotic optimal performance. We remark that the same\n2\n\nphenomenon holds not only for a Zip\fan query distribution but in fact for an arbitrary distribution\non the queries (see Remark 2.2).\nLet us make the above comparison with previous known bounds for CM and CS a bit more\nprecise. With frequency vector fand for an element xin the stream, we denote by f(B)\n\u0000x, the vector\nobtained by setting the entry corresponding to xas well as the Blargest entries of fto 0. The classic\ntechnique for analysing CM and CS (see, e.g., [CCFC02]) shows that using a single hash function\nandBbuckets, with probability \n(1), the error when querying the frequency of an element xis\nO(kf(B)\n\u0000xk1=B) for CM and O(kf(B)\n\u0000xk2=p\nB) for CS. By creating O(log(1=\u000e)) sketches and using the\nmedian trick, the error probability can then be reduced to \u000e. For the Zip\fan distribution, these two\nbounds become O(log(n=B)=B) andO(1=B) respectively, and to obtain them with high probability\nfor all elements we require a sketch of size \n( Blogn). Our results imply that to obtain similar\nbounds on the expected error, we only require a sketch of size O(B) and a constant number of hash\nfunctions. The classic approach described above does not yield tight bounds on the expected errors\nof CM and CS when k>1 and to obtain our bounds we have to introduce new and quite di\u000berent\ntechniques as to be described in Section 1.3.\nOur techniques are quite \rexible. To illustrate this, we study the performance of the classic\nCount-Min algorithm with one and more hash functions, as well as its learned counterparts, in the\ncase where the input follows the following more general Zip\fan distribution with exponent \u000b>0.\nThis distribution is de\fned by fi/1=i\u000bfori2[n]. We present the precise results in Table 3\nin Appendix A.\nIn Section 6, we complement our theoretical bounds with empirical evaluation of standard and\nlearned variants of Count-Min and Count-Sketch on a synthetic dataset, thus providing a sense of\nthe constant factors of our asymptotic bounds.\n1.2 Related work\nThe frequency estimation problem and the closely related heavy hitters problem are two of the\nmost fundamental problems in the \feld of streaming algorithms [CM05a, CM05b, CCFC02, M+05,\nCH08, CH10, BICS10, MP14, BCIW16, LNNT16, ABL+17, BCI+17, BDW18]. In addition to\nthe aforementioned hashing-based algorithms (e.g., [CM05a, CCFC02]), multiple non-hashing algo-\nrithms were also proposed, e.g., [MG82, MM02, MAEA05]. These algorithms often exhibit better\naccuracy/space tradeo\u000bs, but do not posses many of the properties of hashing-based methods, such\nas the ability to handle deletions as well as insertions.\nZipf law is a common modeling tool used to evaluate the performance of frequency estimation\nalgorithms, and has been used in many papers in this area, including [MM02, MAEA05, CCFC02].\nIn its general form it postulates that fiis proportional to 1 =i\u000bfor some exponent parameter \u000b>0.\nIn this paper we focus mostly on the \\original\" Zipf law where \u000b= 1. We do, however, study\nCount-Min for more general values of \u000band the techniques introduced in this paper can be applied\nto other values of the exponent \u000bfor Count-Sketch as well.\n1.3 Our techniques\nOur main contribution is our analysis of the standard Count-Min and Count-Sketch algorithms\nfor Zip\fans with k >1 hash functions. Showing the improvement for the learned counterparts is\nrelatively simple (for Count-Min it was already done in [HIKV19]). In both of these analyses we\nconsider a \fxed item iand bound E[jfi\u0000~fij] whereupon linearity of expectation leads to the desired\n3\n\nresults. In the following we assume that fj= 1=jfor eachj2[n] and describe our techniques for\nbounding E[jfi\u0000~fij] for each of the two algorithms.\nCount-Min. With a single hash function and Bbuckets it is easy to see that the head of\nthe Zip\fan distribution, namely the items of frequencies ( fj)j2[B], contribute with log B=B to\nthe expected error E[jfi\u0000~fij], whereas the light items contribute with log( n=B)=B. Our main\nobservation is that with more hash functions the expected contribution from the heavy items\ndrops to 1=Band so, the main contribution comes from the light items. To bound the expected\ncontribution of the heavy items to the error jfi\u0000~fijwe bound the probability that the contribution\nfrom these items is at least t, then integrate over t. The main observation is that if the error is at\nleasttthen for each of the hash functions, either there exist t=sitems in [B] hashing to the same\nbucket asior there is an item j6=iin [B] of weight at most shashing to the same bucket as i. By\na union bound, optimization over s, and some calculations, this gives the desired bound. The lower\nbound follows from simple concentration inequalities on the contribution of the tail. In contrast\nto the analysis from [HIKV19] which is technical and leads to suboptimal bounds, our analysis is\nshort, simple, and yields completely tight bounds in terms of all of the parameters k;nandB.\nCount-Sketch. Simply put, our main contribution is an improved understanding of the\ndistribution of random variables of the form S=Pn\ni=1fi\u0011i\u001bi. Here the \u0011i2 f0;1gare\ni.i.d Bernouilli random variables and the \u001bi2 f\u0000 1;1gare independent Rademachers, that is,\nPr[\u0011i= 1] = Pr[\u0011i=\u00001] = 1=2. Note that the counters used in CS are random variables having\nprecisely this form. Usually such random variables are studied for the purpose of obtaining large de-\nviation results. In contrast, in order to analyze CS, we are interested in a \fne-grained picture of the\ndistribution within a \\small\" interval Iaround zero, say with Pr[ S2I] = 1=2. For example, when\nproving a lower bound on E[jfi\u0000~fij], we must establish a certain anti-concentration ofSaround 0.\nMore precisely we \fnd an interval J\u001aIcentered at zero such that Pr[ S2J] =O(1=p\nk). Com-\nbined with the fact that we use kindependent hash functions as well as properties of the median\nand the binomial distribution, this gives that E[jfi\u0000~fij] = \n(jJj). Anti-concentration inequali-\nties of this type are in general notoriously hard to obtain but it turns out that we can leverage\nthe properties of the Zip\fan distribution, speci\fcally its heavy head. For our upper bounds on\nE[jfi\u0000~fij] we need strong lower bounds on Pr[ S2J] for intervals J\u001aIcentered at zero. Then\nusing concentration inequalities we can bound the probability that half of the krelevant counters\nare smaller (larger) than the lower (highter) endpoint of J, i.e., that the median does not lie in J.\nAgain this requires a precise understanding of the distribution of SwithinI.\n1.4 Structure of the paper\nIn Section 2 we describe the algorithms Count-Min and Count-Sketch. We also formally de\fne the\nestimation error that we will study as well as the Zip\fan distribution. In Sections 3 and 4 we provide\nour analyses of the expected error of Count-Min and Count-Sketch. In Section 5 we analyze the\nperformance of learned Count-Sketch both when the heavy hitter oracle is perfect and when it may\nmisclassify each item with probability at most \u000e. In Section 6 we present our experiments. Finally,\nin Appendix A, we analyse Count-Min for the generalized Zip\fan distribution with exponent \u000b>0\nboth in the classic and learned case and prove matching lower bounds for the learned algorithms.\n4\n\n2 Preliminaries\nWe start out by describing the sketching algorithms Count-Min and Count-Sketch. Common to\nboth of these algorithms is that we sketch a stream Sof elements coming from some universe Uof\nsizen. For notational convenience we will assume that U= [n] :=f1;:::;ng. If itemioccursfi\ntimes then either algorithm outputs an estimate ~fioffi.\nCount-Min. We usekindependent and uniformly random hash functions h1;:::;hk: [n]!\n[B]. LettingCbe an array of size [ k]\u0002[B] we letC[`;b] =P\nj2[n][h`(j) =b]fj. When querying\ni2[n] the algorithm returns ~fi= min`2[k]C[`;h`(i)]. Note that we always have that ~fi\u0015fi.\nCount-Sketch. We pick independent and uniformly random hash functions h1;:::;hk: [n]!\n[B] ands1;:::;sk: [n]!f\u0000 1;1g. Again we initialize an array Cof size [k]\u0002[B] but now we let\nC[`;b] =P\nj2[n][h`(j) =b]s`(j)fj. When querying i2[n] the algorithm returns the estimate\n~fi=median`2[k]s`(i)\u0001C[`;h`(i)].\nRemark 2.1. The bounds presented in Table 1 assumes that the hash functions have codomain\n[B=k] and not [B], i.e., that the total number of buckets is B. In the proofs to follows we assume for\nnotational ease that the hash functions take value in [ B] and the claimed bounds follows immediately\nby replacing BbyB=k.\nEstimation Error. To measure and compare the overall accuracy of di\u000berent frequency es-\ntimation algorithms, we will use the expected estimation error which is de\fned as follows: let\nF=ff1;\u0001\u0001\u0001;fngand ~FA=f~f1;\u0001\u0001\u0001;~fngrespectively denote the actual frequencies and the esti-\nmated frequencies obtained from algorithm Aof items in the input stream. We remark that when\nAis clear from the context we denote ~FAas~F. Then we de\fne\nErr(F;~FA) :=Ei\u0018Djfi\u0000~fij; (1)\nwhereDdenotes the query distribution of the items. Here, similar to previous work (e.g., [RKA16,\nHIKV19]), we assume that the query distribution Dis the same as the frequency distribution of\nitems in the stream, i.e., for any i\u00032[n], Pri\u0018D[i=i\u0003]/fi\u0003(more precisely, for any i\u00032[n],\nPri\u0018D[i=i\u0003] =fi\u0003=NwhereN=P\ni2[n]fidenotes the total sum of all frequencies in the stream).\nRemark 2.2. As all upper/lower bounds in this paper are proved by bounding the expected error\nwhen estimating the frequency of a single item, E[j~fi\u0000fij], then using linearity of expectation, in\nfact we obtain bounds for anyquery distribution ( pi)i2[n].\nZip\fan Distribution. In our analysis we assume that the frequency distribution of items\nfollows Zipf's law. That is, if we sort the items according to their frequencies with no loss of\ngenerality assuming that f1\u0015f2\u0015\u0001\u0001\u0001\u0015fn, then for any i2[n],fi/1=i. In fact, we shall assume\nthatfi= 1=i, which is just a matter of scaling, and which conveniently removes the dependence on\nthe length of the stream in our bounds. Assuming that the query distribution is the same as the\ndistribution of the frequencies of items in the input stream (i.e., Pr i\u0018D[i\u0003] =fi\u0003=N= 1=(i\u0003\u0001Hn)\nwhereHndenotes the n-th harmonic number), we can write the expected error in eq. (1) as follows:\nErr(F;~FA) =Ei\u0018D[jfi\u0000~fij] =1\nN\u0001X\ni2[n]j~fi\u0000fij\u0001fi=1\nHn\u0001X\ni2[n]j~fi\u0000fij\u00011\ni(2)\n5\n\nThroughout this paper, we present our results with respect to the objective function at the right\nhand side of eq. (2), i.e., (1 =Hn)\u0001Pn\ni=1j~fi\u0000fij\u0001fi. However, it is easy to use our results to obtain\nbounds for any query distribution as stated in Remark 2.2.\nLater we shall study the generalized Zip\fan distribution with exponent \u000b > 0. Sorting the\nitems according to their frequencies, f1;\u0015f2\u0015\u0001\u0001\u0001\u0015fn, it holds for any i2[n] thatfi/1=i\u000b.\nAgain we present our result with respect to the objective functionPn\ni=1j~fi\u0000fij\u0001fi.\nAlgorithm 1 Learning-Based Frequency Estimation\n1:procedure LearnedSketch (B,Bh,HH-Oracle ,SketchAlg )\n2: foreach stream element ido\n3: ifHH-Oracle (i) = 1 then .predicts whether iis heavy (in top Bh- frequent items)\n4: ifa unique bucket is already assigned to item ithen\n5: counter i counter i+ 1\n6: else\n7: allocate a new unique bucket to item iand counter i 1\n8: end if\n9: else\n10: feeditoSketchAlg (B\u0000Bh) .an instance of SketchAlg withB\u0000Bhbuckets\n11: end if\n12: end for\n13:end procedure\nFigure 1: A generic learning augmented algorithm for the frequency estimation problem.\nHH-Oracle denotes a given learned oracle for detecting whether the item is among the top Bh\nfrequent items of the stream and SketchAlg is a given (sketching) algorithm (e.g., CM or CS)\nfor the frequency estimation problem.\nLearning Augmented Sketching Algorithms for Frequency Estimation. In this paper,\nfollowing the approach of [HIKV19], the learned variants of CM and CS are algorithms augmented\nwith a machine learning based heavy hitters oracle. More precisely, we assume that the algorithm\nhas access to an oracle HH-Oracle that predicts whether an item is \\heavy\" (i.e., is one of the Bh\nmost frequent items) or not. Then, the algorithm treats heavy and non-heavy items di\u000berently: (a)\na unique bucket is allocated to each heavy item and their frequencies are computed with no error,\n(b) the rest of items are fed to the given (sketching) algorithm SketchAlg using the remaining\nB\u0000Bhbuckets and their frequency estimates are computed via SketchAlg (see Figure 1). We\nshall assume that Bh= \u0002(B\u0000Bh) = \u0002(B), that is, we use asymptotically the same number\nof buckets for the heavy items as for the sketching of the light items. One justi\fcation for this\nassumption is that in any case we can increase both the number of buckets for heavy and light\nitems toBwithout a\u000becting the overall asymptotic space usage.\nNote that, in general the oracle HH-Oracle can make errors. In our analysis we \frst obtain\na theoretical understanding, by assuming that the oracle is perfect, i.e., the error rate is zero. We\nlater complement this analysis, by studying the incurred error when the oracle misclassi\fes each\nitem with probability at most \u000e.\n6\n\n3 Tight Bounds for Count-Min with Zip\fans\nFor both Count-Min and Count-Sketch we aim at analyzing the expected value of the variableP\ni2[n]fi\u0001j~fi\u0000fijwherefi= 1=iand ~fiis the estimate of fioutput by the relevant sketching\nalgorithm. Throughout this paper we use the following notation: For an event Ewe denote by\n[E] the random variable in f0;1gwhich is 1 if and only if Eoccurs. We begin by presenting our\nimproved analysis of Count-Min with Zip\fans. The main theorem is the following.\nTheorem 3.1. Letn;B;k2Nwithk\u00152andB\u0014n=k. Let further h1;:::;hk: [n]![B]\nbe independent and truly random hash functions. For i2[n]de\fne the random variable ~fi=\nmin`2[k]\u0010P\nj2[n][h`(j) =h`(i)]fj\u0011\n. For anyi2[n]it holds that E[j~fi\u0000fij] = \u0002\u0012\nlog(n\nB)\nB\u0013\n.\nReplacingBbyB=k in Theorem 3.1 and using linearity of expectation we obtain the desired\nbound for Count-Min in the upper right hand side of Table 1. The natural assumption that B\u0014n=k\nsimply says that the total number of buckets is upper bounded by the number of items.\nTo prove Theorem 3.1 we start with the following lemma which is a special case of the theorem.\nLemma 3.2. Suppose that we are in the setting of Theorem 3.1 and further that4n=B. Then\nE[j~fi\u0000fij] =O\u00121\nn\u0013\n:\nProof. It su\u000eces to show the result when k= 2 since adding more hash functions and corresponding\ntables only decreases the value of j~fi\u0000fij. De\fneZ`=P\nj2[n]nfig[h`(j) =h`(i)]fjfor`2[2] and\nnote that these variables are independent. For a given t\u00153=nwe wish to upper bound Pr[ Z`\u0015t].\nLets < t be such that t=sis an integer, and note that if Z`\u0015tthen either of the following two\nevents must hold:\nE1: There exists a j2[n]nfigwithfj>sandh`(j) =h`(i).\nE2: The setfj2[n]nfig:h`(j) =h`(i)gcontains at least t=selements.\nTo see this, suppose that Z`\u0015tand thatE1does not hold. Then\nt\u0014Z`=X\nj2[n]nfig[h`(j) =h`(i)]fj\u0014sjfj2[n]nfig:h`(j) =h`(i)gj;\nso it follows that E2holds. By a union bound,\nPr[Z`\u0015t]\u0014Pr[E1] + Pr[E2]\u00141\nns+\u0012n\nt=s\u0013\nn\u0000t=s\u00141\nns+\u0010es\nt\u0011t=s\n:\nChoosings= \u0002(t\nlog(tn)) such that t=sis an integer, and using t\u00153\nn, a simple calculation yields\nthat Pr[Z`\u0015t] =O\u0010\nlog(tn)\ntn\u0011\n. Note thatj~fi\u0000fij= min(Z1;Z2). AsZ1andZ2are independent,\nPr[j~fi\u0000fij\u0015t] =O\u0012\u0010\nlog(tn)\ntn\u00112\u0013\n, so\nE[j~fi\u0000fij] =Z1\n0Pr[Z\u0015t]dt\u00143\nn+O Z1\n3=n\u0012log(tn)\ntn\u00132\ndt!\n=O\u00121\nn\u0013\n:\n4In particular we dispose with the assumption that B\u0014n=k.\n7\n\nWe can now prove the full statement of Theorem 3.1.\nProof of Theorem 3.1. We start out by proving the upper bound. Let N1= [B]nfigandN2=\n[n]n([B][fig). Letb2[k] be such thatP\nj2N1fj\u0001[hb(j) =hb(i)] is minimal. Note that bis itself\na random variable. We also de\fne\nY1=X\nj2N1fj\u0001[hb(j) =hb(i)];andY2=X\nj2N2fj\u0001[hb(j) =hb(i)]:\nThen,j~fi\u0000fij\u0014Y1+Y2. Using Lemma 3.2, we obtain that E[Y1] =O(1\nB). ForY2we observe that\nE[Y2jb] =X\nj2N2fj\nB=O \nlog\u0000n\nB\u0001\nB!\n:\nWe conclude that\nE[j~fi\u0000fij]\u0014E[Y1] +E[Y2] =E[Y1] +E[E[Y2jb]] =O \nlog\u0000n\nB\u0001\nB!\n:\nNext we prove the lower bound. We have already seen that the main contribution to the error comes\nfrom the tail of the distribution. As the tail of the distribution is relatively \\\rat\" we can simply\napply a concentration inequality to argue that with probability \n(1), we have this asymptotic\ncontribution for each of the khash functions. To be precise, for j2[n] and`2[k] we de\fne\nX(j)\n`=fj\u0001\u0000\n[h`(j) =h`(i)]\u00001\nB\u0001\n. Note that the variables ( X(j)\n`)j2[n]are independent. We also\nde\fneS`=P\nj2N2X(j)\n`for`2[k]. Observe thatjX(j)\n`j\u0014fj\u00141\nBforj\u0015B,E[X(j)\n`] = 0, and that\nVar[S`] =X\nj2N2f2\nj\u00121\nB\u00001\nB2\u0013\n\u00141\nB2:\nApplying Bennett's inequality(Theorem B.1 of Appendix B), with \u001b2=1\nB2andM= 1=Bthus\ngives that\nPr[S`\u0014\u0000t]\u0014exp (\u0000h(tB)):\nDe\fningW`=P\nj2N2fj\u0001[h`(j) =h`(i)] it holds that E[W`] = \u0002\u0012\nlog(n\nB)\nB\u0013\nandS`=W`\u0000E[W`],\nso puttingt=E[W`]=2 in the inequality above we obtain that\nPr[W`\u0014E[W`]=2] = Pr[S`\u0014\u0000E[W`]=2]\u0014exp\u0010\n\u0000h\u0010\n\n\u0010\nlogn\nB\u0011\u0011\u0011\n:\nAppealing to Remark B.2 and using that B\u0014n=kthe above bound becomes\nPr[W`\u0014E[W`]=2]\u0014exp\u0010\n\u0000\n\u0010\nlogn\nB\u0001log\u0010\nlogn\nB+ 1\u0011\u0011\u0011\n= exp(\u0000\n(logk\u0001log(logk+ 1))) =k\u0000\n(log(logk+1)): (3)\nBy the independence of the events ( W`>E[W`]=2)`2[k], we have that\nPr\u0014\nj~fi\u0000fij\u0015E[W`]\n2\u0015\n\u0015(1\u0000k\u0000\n(log(logk+1)))k= \n(1);\nand so E[j~fi\u0000fij] = \n( E[W`]) = \n\u0012\nlog(n\nB)\nB\u0013\n, as desired.\n8\n\nRemark 3.3. We have stated Theorem 3.1 for truly random hash functions but it su\u000eces with\nO(logB)-independent hashing to prove the upper bound. Indeed, the only step in which we require\nhigh independence is in the union bound in Lemma 3.2 over the\u0000n\nt=s\u0001\nsubsets of [n] of sizet=s. To\noptimize the bound we had to choose s=t=log(tn), so thatt=s= log(tn). As we only need to\nconsider values of twitht\u0014Pn\ni=1fi=O(logn), in factt=s=O(logn) in our estimates. Finally,\nwe applied Lemma 3.2 with n=Bso it follows that O(logB)-independence is enough to obtain\nour upper bound.\n4 (Nearly) Tight Bounds for Count-Sketch with Zip\fans\nIn this section we proceed to analyze Count-Sketch for Zip\fans either using a single or more hash\nfunctions. We start with two simple lemmas which for certain frequencies ( fi)i2[n]of the items\nin the stream can be used to obtain respectively good upper and lower bounds on E[j~fi\u0000fij] in\nCount-Sketch with a single hash function. We will use these two lemmas both in our analysis of\nstandard and learned Count-Sketch for Zip\fans.\nLemma 4.1. Letw= (w1;:::;wn)2Rn,\u00111;:::;\u0011nBernoulli variables taking value 1with proba-\nbilityp, and\u001b1;:::;\u001bn2f\u0000 1;1gindependent Rademachers, i.e., Pr[\u001bi= 1] = Pr[\u001bi=\u00001] = 1=2.\nLetS=Pn\ni=1wi\u0011i\u001bi. Then, E[jSj] =O\u0000ppkwk2\u0001\n.\nProof. Using that E[\u001bi\u001bj] = 0 fori6=jand Jensen's inequality E[jSj]2\u0014E[S2] =E\u0002Pn\ni=1w2\ni\u0011i\u0003\n=\npkwk2\n2, from which the result follows.\nLemma 4.2. Suppose that we are in the setting of Lemma 4.1. Let I\u001a[n]and letwI2Rnbe\nde\fned by (wI)i= [i2I]\u0001wi. Then\nE[jSj]\u00151\n2p(1\u0000p)jIj\u00001kwIk1:\nProof. LetJ= [n]nI,S1=P\ni2Iwi\u0011i\u001bi, andS2=P\ni2Jwi\u0011i\u001bi. LetEdenote the event that S1\nandS2have the same sign or S2= 0. Then Pr[ E]\u00151=2 by symmetry. For i2Iwe denote by Ai\nthe event thatfj2I:\u0011j6= 0g=fig. Then Pr[Ai] =p(1\u0000p)jIj\u00001and furthermore AiandEare\nindependent. If Ai\\Eoccurs, thenjSj\u0015jwijand as the events ( Ai\\E)i2Iare disjoint it thus\nfollows that E[jSj]\u0015P\ni2IPr[Ai\\E]\u0001jwij\u00151\n2p(1\u0000p)jIj\u00001kwIk1.\nWith these tools in hand, we proceed to analyse Count-Sketch for Zip\fans with one and more\nhash functions in the next two sections.\n4.1 One hash function\nBy the same argument as in the discussion succeeding Theorem 3.1, the following theorem yields\nthe desired result for a single hash function as presented in Table 1.\nTheorem 4.3. Suppose that B\u0014nand leth: [n]![B]ands: [n]!f\u0000 1;1gbe truly random\nhash functions. De\fne the random variable ~fi=P\nj2[n][h(j) =h(i)]s(j)fjfori2[n]. Then\nE[j~fi\u0000s(i)fij] = \u0002\u0012logB\nB\u0013\n:\n9\n\nProof. Leti2[n] be \fxed. We start by de\fning N1= [B]nfigandN2= [n]n([B][fig) and note\nthat\nj~fi\u0000s(i)fij\u0014\f\f\f\f\f\fX\nj2N1[h(j) =h(i)]s(j)fj\f\f\f\f\f\f+\f\f\f\f\f\fX\nj2N2[h(j) =h(i)]s(j)fj\f\f\f\f\f\f:=X1+X2:\nUsing the triangle inequality E[X1]\u00141\nBP\nj2N1fj=O(logB\nB). Also, by Lemma 4.1, E[X2] =O\u00001\nB\u0001\nand combining the two bounds we obtain the desired upper bound. For the lower bound we\napply Lemma 4.2 with I=N1concluding that\nE[j~fi\u0000s(i)fij]\u00151\n2B\u0012\n1\u00001\nB\u0013jN1j\u00001X\ni2N1fi= \n\u0012logB\nB\u0013\n:\n4.2 Multiple hash functions\nLetk2Nbe odd. For a tuple x= (x1;:::;xk)2Rkwe denote by medianxthe median of the\nentries ofx. The following theorem immediately leads to the result on CS with k\u00153 hash functions\nclaimed in Table 1.\nTheorem 4.4. Letk\u00153be odd,n\u0015kB, andh1;:::;hk: [n]![B]ands1;:::;sk: [n]!f\u0000 1;1g\nbe truly random hash functions. De\fne ~fi=median`2[k]\u0010P\nj2[n][h`(j) =h`(i)]s`(j)fj\u0011\nfori2[n].\nAssume that5k\u0014B. Then\nE[j~fi\u0000s(i)fij] = \n\u00121\nBp\nklogk\u0013\n;and E[j~fi\u0000s(i)fij] =O\u00121\nBp\nk\u0013\nThe assumption n\u0015kBsimply says that the total number of buckets is upper bounded by\nthe number of items. Again using linearity of expectation for the summation over i2[n] and\nreplacingBbyB=k we obtain the claimed upper and lower bounds ofp\nk\nBlogkandp\nk\nBrespectively.\nWe note that even if the bounds above are only tight up to a factor of log kthey still imply that it\nis asymptotically optimal to choose k=O(1), e.g.k= 3. To settle the correct asymptotic growth\nis thus of merely theoretical interest.\nIn proving the upper bound in Theorem 4.4, we will use the following result by Minton and\nPrice (Corollary 3.2 of [MP14]) proved via an elegant application of the Fourier transform.\nLemma 4.5 (Minton and Price [MP14]) .LetfXi:i2[n]gbe independent symmetric random\nvariables such that Pr[Xi= 0]\u00151=2for eachi. LetX=Pn\ni=1Xiand\u001b2=E[X2] = Var[X]. For\n\"<1it holds that Pr[jXj<\"\u001b ] = \n(\")\nProof of Theorem 4.4. IfB(and hence k) is a constant, then the results follow easily\nfrom Lemma 4.1, so in what follows we may assume that Bis larger than a su\u000eciently large\nconstant. We subdivide the exposition into the proofs of the upper and lower bounds.\n5This very mild assumption can probably be removed at the cost of a more technical proof. In our proof it can\neven be replaced by k\u0014B2\u0000\"for any \"= \n(1).\n10\n\nUpper bound De\fneN1= [B]nfigandN2= [n]n([B][fig). Let for `2[k],X(`)\n1=P\nj2N1[h`(j) =h`(i)]s`(j)fjandX(`)\n2=P\nj2N2[h`(j) =h`(i)]s`(j)fjand letX(`)=X(`)\n1+X(`)\n2.\nAs the absolute error in Count-Sketch with one pair of hash functions ( h;s) is always upper\nbounded by the corresponding error in Count-Min with the single hash function h, we can use the\nbound in the proof of Lemma 3.2 to conclude that Pr[ jX(`)\n1j\u0015t] =O(log(tB)\ntB), whent\u00153=B. Also\nVar[X(`)\n2] = (1\nB\u00001\nB2)P\nj2N2f2\nj\u00141\nB2, so by Bennett's inequality (Theorem B.1) with M= 1=B\nand\u001b2= 1=B2and Remark B.2,\nPr[jX(`)\n2j\u0015t]\u00142 exp (\u0000h(tB))\u00142 exp\u0012\n\u00001\n2tBlog (tB+ 1)\u0013\n=O\u0012log(tB)\ntB\u0013\n;\nfort\u00153\nB. It follows that for t\u00153=B,\nPr[jX(`)j\u00152t]\u0014Pr[(jX(`)\n1j\u0015t)] + Pr(jX(`)\n2j\u0015t)] =O\u0012log(tB)\ntB\u0013\n:\nLetCbe the implicit constant in the O-notation above. If j~fi\u0000s(i)fij\u00152t, at least half of the\nvalues (jX(`)j)`2[k]are at least 2 t. Fort\u00153=Bit thus follows by a union bound that\nPr[j~fi\u0000s(i)fij\u00152t]\u00142\u0012k\ndk=2e\u0013\u0012\nClog(tB)\ntB\u0013dk=2e\n\u00142\u0012\n4Clog(tB)\ntB\u0013dk=2e\n: (4)\nIf\u000b=O(1) is chosen su\u000eciently large it thus holds that\nZ1\n\u000b=BPr[j~fi\u0000s(i)fij\u0015t]dt= 2Z1\n\u000b=(2B)Pr[j~fi\u0000s(i)fij\u00152t]dt\n\u00144\nBZ1\n\u000b=2\u0012\n4Clog(t)\nt\u0013dk=2e\ndt\n\u00141\nB2k\u00141\nBp\nk:\nHere the \frst inequality uses eq. (4) and a change of variable. The second inequality uses that\u0010\n4Clogt\nt\u0011dk=2e\n\u0014(C0=t)2k=5for some constant C0followed by a calculation of the integral. Now,\nE[j~fi\u0000s(i)fij] =Z1\n0Pr[j~fi\u0000s(i)fij\u0015t]dt;\nso for our upper bound it therefore su\u000eces to show thatR\u000b=B\n0Pr[j~fi\u0000s(i)fij\u0015t]dt=O\u0010\n1\nBp\nk\u0011\n.\nFor this we need the following claim:\nClaim 4.6. LetI\u001aRbe the closed interval centered at the origin of length 2t, i.e.,I= [\u0000t;t].\nSuppose that 0<t\u00141\n2B. For`2[k],Pr[X(`)2I] = \n(tB).\nProof. Note that Pr[ X(`)\n1= 0]\u0015Pr[V\nj2N1(h`(j)6=h`(i))] = (1\u00001\nB)N1= \n(1). Secondly\nVar[X(`)\n2] = (1\nB\u00001\nB2)P\nj2N2f2\nj\u00141\nB2. Using that X(`)\n1andX(`)\n2are independent and Lemma 4.5\nwith\u001b2= Var[X(`)\n2], it follows that Pr[ X(`)2I] = \n\u0010\nPr[X(`)\n22I]\u0011\n= \n(tB).\n11\n\nLet us now show how to use the claim to establish the desired upper bound. For this let\n0< t\u00141\n2Bbe \fxed. Ifj~fi\u0000s(i)fij\u0015t, at least half of the values ( X(`))`2[k]are at least tor at\nmost\u0000t. Let us focus on bounding the probability that at least half are at least t, the other bound\nbeing symmetric giving an extra factor of 2 in the probability bound. By symmetry and Claim 4.6,\nPr[X(`)\u0015t] =1\n2\u0000\n(tB). For`2[k] we de\fne Y`= [X(`)\u0015t], and we put S=P\n`2[k]Y`. Then\nE[S] =k\u00001\n2\u0000\n(tB)\u0001\n. If at least half of the values ( X(`))`2[k]are at least tthenS\u0015k=2. By\nHoe\u000bding's inequality (Theorem B.4) we can bound the probability of this event by\nPr[S\u0015k=2] = Pr[S\u0000E[S] = \n(ktB)] = exp(\u0000\n(kt2B2)):\nIt follows that Pr[ j~fi\u0000s(i)fij\u0015t]\u00142 exp(\u0000\n(kt2B2)). Thus\nZ\u000b=B\n0Pr[j~fi\u0000s(i)fij\u0015t]dt\u0014Z 1\n2B\n02 exp(\u0000\n(kt2B2))dt+Z\u000b=B\n1\n2B2 exp(\u0000\n(k))dt\n\u00141\nBp\nkZp\nk=2\n0exp(\u0000t2)dt+2\u000bexp(\u0000\n(k))\nB=O\u00121\nBp\nk\u0013\n:\nHere the second inequality used a change of variable. The proof of the upper bound is complete.\nLower Bound Fix`2[k] and letM1= [Blogk]nfigandM2= [n]n([Blogk][fig). Write\nS:=X\nj2M1[h`(j) =h`(i)]s`(j)fj+X\nj2M2[h`(j) =h`(i)]s`(j)fj:=S1+S2:\nWe also de\fne J:=fj2M1:h`(j) =h`(i)g. LetI\u0012Rbe the closed interval around s`(i)fiof\nlength1\nBp\nklogk. We now upper bound the probability that S2Iconditioned on the value of S2.\nTo ease the notation, the conditioning on S2has been left out in the notation to follow. Note \frst\nthat\nPr[S2I] =jM1jX\nr=0Pr[S2IjjJj=r]\u0001Pr[jJj=r]: (5)\nFor a given r\u00151 we now proceed to bound Pr[ S2IjjJj=r]. This probability is the same as\nthe probability that S2+P\nj2R\u001bjfj2I, whereR\u0012M1is a uniformly random r-subset and the\n\u001bj's are independent Rademachers. Suppose that we sample the elements from Ras well as the\ncorresponding signs ( \u001bi)i2Rsequentially, and let us condition on the values and signs of the \frst\nr\u00001 sampled elements. At this point at mostBlogkp\nk+ 1 possible samples for the last element in R\ncan cause that S2I. Indeed, the minimum distance between distinct elements of ffj:j2M1gis\nat least 1=(Blogk)2and furthermore Ihas length1\nBp\nklogk. Thus, at most\n1\nBp\nklogk\u0001(Blogk)2+ 1 =Blogkp\nk+ 1\nchoices for the last element of Rensure that S2I. For 1\u0014r\u0014(Blogk)=2 we can thus upper\nbound\nPr[S2IjjJj=r]\u0014Blogkp\nk+ 1\njM1j\u0000r+ 1\u00142p\nk+2\nBlogk\u00143p\nk:\n12\n\nNote that\u0016:=E[jJj]\u0014logkso forB\u00156, it holds that\nPr[jJj\u0015(Blogk)=2]\u0014Pr\u0014\njJj\u0015\u0016B\n2\u0015\n\u0014Pr\u0014\njJj\u0015\u0016\u0012\n1 +B\n3\u0013\u0015\n\u0014exp (\u0000\u0016h(B=3)) =k\u0000\n(h(B=3));\nwhere the last inequality follows from the Cherno\u000b bound of Theorem B.3. Thus, if we assume\nthatBis larger than a su\u000eciently large constant, then Pr[ jJj\u0015Blogk=2]\u0014k\u00001. Finally, Pr[jJj=\n0] = (1\u00001=B)Blogk\u0014k\u00001. Combining the above, we can continue the bound in (5) as follows.\nPr[S2I]\u0014Pr[jJj= 0] +(Blogk)=2X\nr=1Pr[S2IjjJj=r]\u0001Pr[jJj=r]\n+jM1jX\nr=(Blogk)=2+1Pr[jJj=r] =O\u00121p\nk\u0013\n; (6)\nwhich holds even after removing the conditioning on S2. We now show that with probability\n\n(1) at least half the values ( X(`))`2[k]are at least1\n2Bp\nklogk. Letp0be the probability that\nX(`)\u00151\n2Bp\nklogk. This probability does not depend on `2[k] and by symmetry and (6), p0=\n1=2\u0000O(1=p\nk). De\fne the function f:f0;:::;kg!Rby\nf(t) =\u0012k\nt\u0013\npt\n0(1\u0000p0)k\u0000t:\nThenf(t) is the probability that exactly tof the values ( X(`))`2[k]are at least1\nBp\nklogk. Using\nthatp0= 1=2\u0000O(1=p\nk), a simple application of Stirling's formula gives that f(t) = \u0002\u0010\n1p\nk\u0011\nfor\nt=dk=2e;:::;dk=2 +p\nkewhenkis larger than some constant C. It follows that with probability\n\n(1) at least half of the ( X(`))`2[k]are at least1\nBp\nklogkand in particular\nE[j~fi\u0000fij] = \n\u00121\nBp\nklogk\u0013\n:\nFinally we handle the case where k\u0014C. It follows from simple calculations (e.g., using Lemma 4.2)\nthatX(`)= \n(1=B) with probability \n(1). Thus this happens for all `2[k] with probability \n(1)\nand in particular E[j~fi\u0000fij] = \n(1=B), which is the desired for constant k.\n5 Learned Count-Sketch for Zip\fans\nWe now proceed to analyze the learned Count-Sketch algorithm. In Section 5.1 we estimate the\nexpected error when using a single hash function and in Section 5.2 we show that the expected\nerror only increases when using more hash functions. Recall that we assume that the number of\nbucketsBhused to store the heavy hitters that Bh= \u0002(B\u0000Bh) = \u0002(B).\n5.1 One hash function\nBy takingB1=Bh= \u0002(B) andB2=B\u0000Bh= \u0002(B) in the theorem below, the result on L-CS\nfork= 1 claimed in Table 1 follows immediately.\n13\n\nTheorem 5.1. Leth: [n]n[B1]![B2]ands: [n]!f\u0000 1;1gbe truly random hash functions where\nn;B 1;B22Nand6n\u0000B1\u0015B2\u0015B1. De\fne the random variable ~fi=Pn\nj=B1+1[h(j) =h(i)]s(j)fj\nfori2[n]n[B1]. Then\nE[j~fi\u0000s(i)fij] = \u0002 \nlogB2+B1\nB1\nB2!\nProof. LetN1= [B1+B2]n([B1][fig) andN2= [n]n([B1+B2][fig). LetX1=P\nj2N1[h(j) =\nh(i)]s(j)fjandX2=P\nj2N2[h(j) =h(i)]s(j)fj. By the triangle inequality and linearity of expec-\ntation,\nE[jX1j] =O \nlogB2+B1\nB1\nB2!\n:\nMoreover, it follows directly from Lemma 4.1 that E[jX2j] =O\u0010\n1\nB2\u0011\n. Thus\nE[j~fi\u0000s(i)fij]\u0014E[jX1j] +E[jX2j] =O \nlogB2+B1\nB1\nB2!\n;\nas desired. For the lower bound on Eh\f\f\f~fi\u0000s(i)fi\f\f\fi\nwe apply Lemma 4.2 with I=N1to obtain\nthat,\nEh\f\f\f~fi\u0000s(i)fi\f\f\fi\n\u00151\n2B2\u0012\n1\u00001\nB2\u0013jN1j\u00001X\ni2N1fi= \n \nlogB2+B1\nB1\nB2!\n:\nCorollary 5.2. Leth: [n]n[Bh]![B\u0000Bh]ands: [n]!f\u0000 1;1gbe truly random hash functions\nwheren;B;Bh2NandBh= \u0002(B)\u0014B=2. De\fne the random variable ~fi=Pn\nj=Bh+1[h(j) =\nh(i)]s(j)fjfori2[n]n[Bh]. Then E[j~fi\u0000s(i)fij] = \u0002(1=B).\nRemark 5.3. The upper bounds of Theorem 5.1 and Corollary 5.2 hold even without the as-\nsumption of fully random hashing. In fact, we only require that handsare 2-independent. In-\ndeed Lemma 4.1 holds even when the Rademachers are 2-independent (the proof is the same).\nMoreover, we need hto be 2-independent as we condition on h(i) in our application of Lemma 4.1.\nWith 2-independence the variables [ h(j) =h(i)] forj6=iare then Bernoulli variables taking value\n1 with probability 1 =B2.\n5.2 More hash functions\nWe now show that, like for Count-Sketch, using more hash functions does not decrease the expected\nerror. We \frst state the Littlewood-O\u000bord lemma as strengthened by Erd} os.\n6The \frst inequality is the standard assumption that we have at least as many items as buckets. The second\ninequality says that we use at least as many buckets for non-heavy items as for heavy items (which doesn't change\nthe asymptotic space usage).\n14\n\nTheorem 5.4 (Littlewood-O\u000bord [LO39], Erd} os [Erd45]) .Leta1;:::;an2Rwithjaij\u00151for\ni2[n]. Let further \u001b1;:::;\u001bn2f\u0000 1;1gbe random variables with Pr[\u001bi= 1] = Pr[\u001bi=\u00001] = 1=2\nand de\fneS=Pn\ni=1\u001biai. For anyv2Rit holds that Pr[jS\u0000vj\u00141] =O(1=pn).\nSettingB1=Bh= \u0002(B) andB2=B\u0000B2= \u0002(B) in the theorem below gives the \fnal bound\nfrom Table 1 on L-CS with k\u00153.\nTheorem 5.5. Letn\u0015B1+B2\u00152B1,k\u00153odd, andh1;:::;hk: [n]n[B1]![B2=k]and\ns1;:::;sk: [n]n[B1]!f\u0000 1;1gbe independent and truly random. De\fne the random variable\n~fi=median`2[k]\u0010P\nj2[n]n[B1][h`(j) =h`(i)]s`(j)fj\u0011\nfori2[n]n[B1]. Then\nE[j~fi\u0000s(i)fij] = \n\u00121\nB2\u0013\n:\nProof. Like in the proof of the lower bound of Theorem 4.4 it su\u000eces to show that for each\nithe probability that the sum S`:=P\nj2[n]n([B1][fig)[h`(j) =h`(i)]s`(j)fjlies in the interval\nI= [\u00001=(2B2);1=(2B2)] isO(1=p\nk). Then at least half the ( S`)`2[k]are at least 1 =(2B2) with\nprobability \n(1) by an application of Stirling's formula, and it follows that E[j~fi\u0000s(i)fij] = \n(1=B2).\nLet`2[k] be \fxed,N1= [2B2]n([B2][fig), andN2= [n]n(N1[fig), and write\nS`=X\nj2N1[h`(j) =h`(i)]s`(j)fj+X\nj2N2[h`(j) =h`(i)]s`(j)fj:=X1+X2:\nNow condition on the value of X2. LettingJ=fj2N1:h`(j) =h`(i)git follows by Theorem 5.4\nthat\nPr[S`2IjX2] =O0\n@X\nJ0\u0012N1Pr[J=J0]p\njJ0j+ 11\nA=O\u0010\nPr[jJj<k= 2] + 1=p\nk\u0011\n:\nAn application of Chebyshev's inequality gives that Pr[ jJj< k= 2] =O(1=k), so Pr[S`2I] =\nO(1=p\nk). Since this bound holds for any possible value of X2we may remove the conditioning and\nthe desired result follows.\nRemark 5.6. The bound above is probably only tight for B1= \u0002(B2). Indeed, we know that it\ncannot be tight for all B1\u0014B2since when B1becomes very small, the bound from the standard\nCount-Sketch with k\u00153 takes over | and this is certainly worse than the bound in the theorem.\nIt is an interesting open problem (that requires a better anti-concentration inequality than the\nLittlewood-O\u000bord lemma) to settle the correct bound when B1\u001cB2.\n5.3 Learned Count-Sketch using a noisy heavy hitter oracle\nIn [HIKV19] it was demonstrated that if the heavy hitter oracle is noisy, misclassifying an item\nwith probability \u000e, then the expected error incurred by Count-Min for Zip\fans is\nO\u00121\nlogn\u000e2ln2Bh+ ln2(n=Bh)\nB\u0000Bh\u0013\n:\n15\n\nHereBhis the number of buckets used to store the heavy hitters and Bis the total number of\nbuckets. Taking Bh= \u0002(B) = \u0002(B\u0000Bh), this bound becomes O\u0010\n\u000e2ln2B+ln2(n=B)\nBlogn\u0011\n. As\u000evaries\nin [0;1], this interpolates between the expected error incurred in respectively the learned case\nwith a perfect heavy hitter oracle and the classic case. In particular it is enough to assume that\n\u000e=O(ln(n=B)=ln(B)) in order to obtain the results in the idealized case with a perfect oracle.\nWe now provide a similar analysis for the learned Count-Sketch. More precisely we assume that\nwe allocate Bhbuckets to the heavy hitters and B\u0000Bhto the lighter items. We moreover assume\naccess to a heavy hitter oracle HH\u000esuch that for each i2[n], Pr[HH\u000e(i)6=HH 0(i)]\u0014\u000e, where\nHH 0is a perfect heavy hitter oracle that correctly classi\fes the Bhheaviest items.\nTheorem 5.7. Learned Count-Sketch with a single hash functions, a heavy hitter oracle HH\u000e,\nBh= \u0002(B)bins allocated to store the Bhitems classi\fed as heavy and B\u0000Bh= \u0002(B)bins\nallocated to a Count-Sketch of the remaining items, incurs an expected error of\nO\u0012(\u000elogB+ log(n=B))(1 +\u000elogB)\nBlogn\u0013\n:\nProof. Leth: [n]![B\u0000Bh] ands: [n]!f\u0000 1;1gbe the hash functions used for the Count-Sketch.\nIn the analysis to follow, it is enough to assume that they are 2-independent. Suppose item iis\nclassi\fed as non-heavy. For j2[n], let\u0011j= [h(j) =h(i)], and let\u000bjbe the indicator for item j\nbeing classi\fed as non-heavy. Then\nj~fi\u0000fij=\f\f\f\f\f\fX\nj2[n]nfig\u000bj\u0011js(j)fj\f\f\f\f\f\f\u0014X\nj2[Bh]nfig\u000bj\u0011jfj+\f\f\f\f\f\fX\nj2[n]n(Bh[fig)\u000bj\u0011js(j)fj\f\f\f\f\f\f:=S1+S2\nNote that E[S1] =O\u0010\n\u000elogBh\nB\u0000Bh\u0011\n=O\u0010\n\u000elogB\nB\u0011\n. ForS2, we letpj= Pr[\u000bj\u0011j= 1]\u00141\nB\u0000Bh=O(1\nB).\nThen\nE[S2]\u0014(E[S2\n2])1=2=0\n@X\nj2[n]n(Bh[fig)pjf2\nj1\nA1=2\n=O\u00121\nB\u0013\n;\nusing that E[s(i)s(j)] = 0 fori6=jassis 2-independent. It follows that E[j~fi\u0000fij] =O\u0010\n1+\u000elogB\nB\u0011\n,\ngiven that item iis classi\fed as non-heavy. Let N=P\ni2[n]fi= \u0002(logn). As the probability of\nitemi2[Bh] being classi\fed as non-heavy is at most \u000e, the the expected error is upper bounded\nby\n1\nN0\n@\u000eX\nj2[Bh]nfigfi+X\nj2[n]n(Bh[fig)fi1\nA\u0001O\u00121 +\u000elogB\nB\u0013\n=O\u0012(\u000elogB+ log(n=B))(1 +\u000elogB)\nBlogn\u0013\n;\nas desired.\nWe see that with \u000e= 1, we recover the bound oflogB\nBpresented in Table 1 for the classic\nCount-Sketch. On the other hand, it is enough to assume that \u000e=O(1=logB) in order to obtain\nthe bound of O\u0010\nlog(n=B)\nBlogn\u0011\n, which is what we obtain with a perfect heavy hitter oracle.\n16\n\n6 Experiments\nIn this section, we provide the empirical evaluation of CountMin, CountSketch and their learned\ncounterparts under Zip\fan distribution. Our empirical results complement the theoretical analysis\nprovided earlier in this paper.\nExperiment setup. We consider a synthetic stream ofn= 10Kitems where the frequencies\nof the items follow the standard Zip\fan distribution (i.e., with \u000b= 1). To be consistent with our\nassumption in our theoretical analysis, we scale the frequencies so that the frequency of item iis\n1=i. In our experiments, we vary the values of the number of buckets ( B) and the number of rows\nin the sketch ( k) as well as the number of predicted heavy items in the learned sketches. We remark\nthat in this section we assume that the heavy hitter oracle predicts without errors .\nWe run each experiment 20 times and take the average of the estimation error de\fned in eq. (2).\n00.020.040.060.080.10.120.140.160.180.2100012001400160018002000220024002600280030003200340036003800400042004400460048005000Estimation Error\nNumber of Buckets (B)Count -Min\nk=1 k=2 k=3 k=5\n00.010.020.030.040.050.060.07100012001400160018002000220024002600280030003200340036003800400042004400460048005000Estimation Error\nNumber of Buckets (B)Learned Count -Min (HH = 10%)\nk=1 k=2 k=3\nFigure 2: The performance of (Learned) Count-Min with di\u000berent number of rows.\nSketches with the same number of buckets but di\u000berent shapes. Here, we compare\nthe empirical performances of both standard and learned variants of Count-Min and Count-Sketch\nwith varying choices for the parameter. More precisely, we \fx the sketch size and vary the number\nof rows (i.e., number of hash functions) in the sketch.\nAs predicted in our theoretical analysis, Figures 2 and 3 show that setting the number of rows\nto some constant larger than 1 for standard CM and CS, leads to a smaller estimation error as\nwe increase the size of the sketch. In contrast, in the learned variant, the average estimation error\nincreases in kbeing smallest for k= 1, as was also predicted by our analysis.\nLearned vs. Standard Sketches. In Figure 4, we compare the performance of learned\nvariants of Count-Min and Count-Sketch with the standard Count-Min and Count-Sketch. To be\nfair, we assume that each bucket that is assigned a heavy hitter consumes two bucket of memory:\none for counting the number of times the heavy item appears in the stream and one for indexing\nthe heavy item in the data structure.\n17\n\n00.010.020.030.040.050.060.07100012001400160018002000220024002600280030003200340036003800400042004400460048005000Estimation Error\nNumber of Buckets (B)Count -Sketch\nk=1 k=3 k=5 k=7\n00.00010.00020.00030.00040.00050.00060.0007100012001400160018002000220024002600280030003200340036003800400042004400460048005000Estimation Error\nNumber of Buckets (B)Learned Count -Sketch (HH=10%)\nk=1 k=3 k=5Figure 3: The performance of (Learned) Count-Sketch with di\u000berent number of rows.\nB CM ( k= 1) CM ( k= 2) L-CM CS (k= 1) CS ( k= 3) L-CS\n1000 0.085934 0.080569 0.026391 0.058545 0.054315 0.000577138\n1200 0.077913 0.06266 0.020361 0.054322 0.047214 0.000460688\n1400 0.074504 0.052464 0.016036 0.03972 0.033348 0.00036492\n1600 0.071528 0.043798 0.01338 0.056626 0.032925 0.000312238\n1800 0.059898 0.038554 0.011142 0.036881 0.025003 0.000275648\n2000 0.046389 0.033746 0.009556 0.035172 0.022403 0.000237371\n2200 0.036082 0.029059 0.008302 0.029388 0.02148 0.000209376\n2400 0.032987 0.025135 0.007237 0.02919 0.020913 0.00018811\n2600 0.041896 0.023157 0.006399 0.032195 0.018271 0.00016743\n2800 0.026351 0.021402 0.005694 0.036197 0.017431 0.000152933\n3000 0.032624 0.020155 0.005101 0.023175 0.016068 0.000138081\n3200 0.023614 0.018832 0.004599 0.051132 0.01455 0.000127445\n3400 0.021151 0.016769 0.004196 0.022333 0.013503 0.000122947\n3600 0.021314 0.015429 0.003823 0.022012 0.014316 0.000109171\n3800 0.027798 0.014677 0.003496 0.025378 0.013082 0.000102035\n4000 0.021407 0.013279 0.00322 0.017303 0.012312 0.0000931\n4200 0.020883 0.012419 0.002985 0.017719 0.011748 0.0000878\n4400 0.022383 0.011608 0.002769 0.016037 0.011097 0.0000817\n4600 0.020378 0.011151 0.002561 0.015941 0.010202 0.0000757\n4800 0.015114 0.010612 0.002406 0.011642 0.010757 0.0000725\n5000 0.01603 0.009767 0.002233 0.014829 0.009451 0.0000698\nTable 2: The estimation error of di\u000berent sketching methods under Zip\fan distribution. In this\nexample, the number of unique items nis equal to 10 K. In the learned variants, number of rows,\nk, is equal to 1 and the perfect heavy hitter oracles detect top c-frequent items where c=B=10.\nWe observe that the learned variants of Count-Min and Count-Sketch signi\fcantly improve\nupon the estimation error of their standard \\non-learned\" variants. We note that the estimation\nerrors for the learned Count-Sketches in Figure 4 are not zero but very close to zero; see Table 2\nfor the actual values.\n18\n\n00.010.020.030.040.050.060.070.080.090.1100012001400160018002000220024002600280030003200340036003800400042004400460048005000Estimation Error\nNumber of Buckets (B)Count -Min vs Learned Count -Min\nCM (k=1) CM (k=2) LCM (k=1, HH=10%) LCM (k=1, HH=20%)\n00.010.020.030.040.050.060.07100012001400160018002000220024002600280030003200340036003800400042004400460048005000Estimation Error\nNumber of Buckets (B)Count -Sketch vs Learned Count -Sketch\nCS (k=1) CS (k=3) LCS (k=1, HH=10%) LCS (k=1, HH=20%)Figure 4: The comparison of the performance of learned and standard variants of Count-Min and\nCount-Sketch.\nReferences\n[ABL+17] Daniel Anderson, Pryce Bevan, Kevin Lang, Edo Liberty, Lee Rhodes, and Justin\nThaler. A high-performance algorithm for identifying frequent items in data streams.\nInProceedings of the 2017 Internet Measurement Conference , pages 268{282, 2017.\n[ACC+11] Nir Ailon, Bernard Chazelle, Kenneth L Clarkson, Ding Liu, Wolfgang Mulzer, and\nC Seshadhri. Self-improving algorithms. SIAM Journal on Computing , 40(2):350{375,\n2011.\n[ACE+20] Antonios Antoniadis, Christian Coester, Marek Elias, Adam Polak, and Bertrand\nSimon. Online metric algorithms with untrusted predictions. arXiv preprint\narXiv:2003.02144 , 2020.\n[ADJ+20] Spyros Angelopoulos, Christoph D urr, Shendan Jin, Shahin Kamali, and Marc Re-\nnault. Online computation with untrusted advice. In 11th Innovations in Theoretical\nComputer Science Conference (ITCS 2020) . Schloss Dagstuhl-Leibniz-Zentrum f ur In-\nformatik, 2020.\n[AKL+19] Daniel Alabi, Adam Tauman Kalai, Katrina Ligett, Cameron Musco, Christos Tzamos,\nand Ellen Vitercik. Learning to prune: Speeding up repeated computations. In Con-\nference on Learning Theory , 2019.\n[BCI+17] Vladimir Braverman, Stephen R Chestnut, Nikita Ivkin, Jelani Nelson, Zhengyu Wang,\nand David P Woodru\u000b. Bptree: an `2heavy hitters algorithm using constant memory.\nInProceedings of the 36th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles\nof Database Systems , pages 361{376, 2017.\n[BCIW16] Vladimir Braverman, Stephen R Chestnut, Nikita Ivkin, and David P Woodru\u000b. Beat-\ning countsketch for heavy hitters in insertion streams. In Proceedings of the forty-eighth\nannual ACM symposium on Theory of Computing , pages 740{753, 2016.\n19\n\n[BDSV18] Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning to\nbranch. In International Conference on Machine Learning , pages 353{362, 2018.\n[BDV18] Maria-Florina Balcan, Travis Dick, and Ellen Vitercik. Dispersion for data-driven\nalgorithm design, online learning, and private optimization. In 2018 IEEE 59th Annual\nSymposium on Foundations of Computer Science (FOCS) , pages 603{614. IEEE, 2018.\n[BDW18] Arnab Bhattacharyya, Palash Dey, and David P Woodru\u000b. An optimal algorithm\nfor`1-heavy hitters in insertion streams and related problems. ACM Transactions on\nAlgorithms (TALG) , 15(1):1{27, 2018.\n[Ben62] George Bennett. Probability inequalities for the sum of independent random variables.\nJournal of the American Statistical Association , 57(297):33{45, 1962.\n[BICS10] Radu Berinde, Piotr Indyk, Graham Cormode, and Martin J Strauss. Space-optimal\nheavy hitters with strong error bounds. ACM Transactions on Database Systems\n(TODS) , 35(4):1{28, 2010.\n[BJPD17] Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing\nusing generative models. In International Conference on Machine Learning , pages 537{\n546, 2017.\n[CCFC02] Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in\ndata streams. In International Colloquium on Automata, Languages, and Programming ,\npages 693{703. Springer, 2002.\n[CGP20] Edith Cohen, O\fr Geri, and Rasmus Pagh. Composable sketches for functions of\nfrequencies: Beyond the worst case. arXiv preprint arXiv:2004.04772 , 2020.\n[CGT+19] Shuchi Chawla, Evangelia Gergatsouli, Yifeng Teng, Christos Tzamos, and\nRuimin Zhang. Learning optimal search algorithms from data. arXiv preprint\narXiv:1911.01632 , 2019.\n[CH08] Graham Cormode and Marios Hadjieleftheriou. Finding frequent items in data streams.\nProceedings of the VLDB Endowment , 1(2):1530{1541, 2008.\n[CH10] Graham Cormode and Marios Hadjieleftheriou. Methods for \fnding frequent items in\ndata streams. The VLDB Journal , 19(1):3{20, 2010.\n[Che52] Herman Cherno\u000b. A measure of asymptotic e\u000eciency for tests of a hypothesis based\non the sum of observations. Annals of Mathematical Statistics , 23(4):493{507, 1952.\n[CM05a] Graham Cormode and Shan Muthukrishnan. An improved data stream summary: the\ncount-min sketch and its applications. Journal of Algorithms , 55(1):58{75, 2005.\n[CM05b] Graham Cormode and Shan Muthukrishnan. Summarizing and mining skewed data\nstreams. In Proceedings of the 2005 SIAM International Conference on Data Mining ,\npages 44{55. SIAM, 2005.\n[DIRW19] Yihe Dong, Piotr Indyk, Ilya Razenshteyn, and Tal Wagner. Learning sublinear-time\nindexing for nearest neighbor search. arXiv preprint arXiv:1901.08544 , 2019.\n20\n\n[Erd45] Paul Erd os. On a lemma of littlewood and o\u000bord. Bulletin of the American Mathemat-\nical Society , 51(12):898{902, 1945.\n[GP19] Sreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with\nexpert advice. In Proceedings of the 36th International Conference on Machine Learn-\ning, pages 2319{2327, 2019.\n[GR17] Rishi Gupta and Tim Roughgarden. A pac approach to application-speci\fc algorithm\nselection. SIAM Journal on Computing , 46(3):992{1017, 2017.\n[HIKV19] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency\nestimation algorithms. In International Conference on Learning Representations , 2019.\n[Hoe63] Wassily Hoe\u000bding. Probability inequalities for sums of bounded random variables.\nJournal of the American Statistical Association , 58(301):13{30, 1963.\n[IVY19] Piotr Indyk, Ali Vakilian, and Yang Yuan. Learning-based low-rank approximations.\nInAdvances in Neural Information Processing Systems , pages 7400{7410, 2019.\n[JLL+20] Tanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, and David P. Woodru\u000b. Learning-\naugmented data stream algorithms. In International Conference on Learning Repre-\nsentations , 2020.\n[KBC+18] Tim Kraska, Alex Beutel, Ed H Chi, Je\u000brey Dean, and Neoklis Polyzotis. The case\nfor learned index structures. In Proceedings of the 2018 International Conference on\nManagement of Data , pages 489{504, 2018.\n[KDZ+17] Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning com-\nbinatorial optimization algorithms over graphs. In Advances in Neural Information\nProcessing Systems , pages 6348{6358, 2017.\n[Kod19] Rohan Kodialam. Optimal algorithms for ski rental with soft machine-learned predic-\ntions. arXiv preprint arXiv:1903.00092 , 2019.\n[LLMV20] Silvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online\nscheduling via learned weights. In Proceedings of the Fourteenth Annual ACM-SIAM\nSymposium on Discrete Algorithms , pages 1859{1877. SIAM, 2020.\n[LNNT16] Kasper Green Larsen, Jelani Nelson, Huy L Nguy^ en, and Mikkel Thorup. Heavy hitters\nvia cluster-preserving clustering. In 2016 IEEE 57th Annual Symposium on Foundations\nof Computer Science (FOCS) , pages 61{70. IEEE, 2016.\n[LO39] John Edensor Littlewood and Albert C O\u000bord. On the number of real roots of a random\nalgebraic equation. ii. In Mathematical Proceedings of the Cambridge Philosophical\nSociety , volume 35, pages 133{148. Cambridge University Press, 1939.\n[LV18] Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned\nadvice. In International Conference on Machine Learning , pages 3302{3311, 2018.\n[M+05] Shanmugavelayutham Muthukrishnan et al. Data streams: Algorithms and applica-\ntions. Foundations and Trends R\rin Theoretical Computer Science , 1(2):117{236, 2005.\n21\n\n[MAEA05] Ahmed Metwally, Divyakant Agrawal, and Amr El Abbadi. E\u000ecient computation of\nfrequent and top-k elements in data streams. In International Conference on Database\nTheory , pages 398{412. Springer, 2005.\n[MG82] Jayadev Misra and David Gries. Finding repeated elements. Science of computer\nprogramming , 2(2):143{152, 1982.\n[Mit18] Michael Mitzenmacher. A model for learned bloom \flters and optimizing by sandwich-\ning. In Advances in Neural Information Processing Systems , pages 464{473, 2018.\n[Mit20] Michael Mitzenmacher. Scheduling with predictions and the price of misprediction. In\n11th Innovations in Theoretical Computer Science Conference (ITCS 2020) . Schloss\nDagstuhl-Leibniz-Zentrum f ur Informatik, 2020.\n[MM02] Gurmeet Singh Manku and Rajeev Motwani. Approximate frequency counts over data\nstreams. In VLDB'02: Proceedings of the 28th International Conference on Very Large\nDatabases , pages 346{357. Elsevier, 2002.\n[MP14] Gregory T Minton and Eric Price. Improved concentration bounds for count-sketch. In\nProceedings of the twenty-\ffth annual ACM-SIAM symposium on Discrete algorithms ,\npages 669{686. Society for Industrial and Applied Mathematics, 2014.\n[MPB15] Ali Mousavi, Ankit B Patel, and Richard G Baraniuk. A deep learning approach to\nstructured signal recovery. In Communication, Control, and Computing (Allerton),\n2015 53rd Annual Allerton Conference on , pages 1336{1343. IEEE, 2015.\n[PSK18] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml\npredictions. In Advances in Neural Information Processing Systems , pages 9661{9670,\n2018.\n[RKA16] Pratanu Roy, Arijit Khan, and Gustavo Alonso. Augmented sketch: Faster and more\naccurate stream processing. In Proceedings of the 2016 International Conference on\nManagement of Data , pages 1449{1463, 2016.\n[Roh20] Dhruv Rohatgi. Near-optimal bounds for online caching with machine learned advice. In\nProceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms ,\npages 1834{1845. SIAM, 2020.\n[WLKC16] Jun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang. Learning to hash for indexing\nbig data - a survey. Proceedings of the IEEE , 104(1):34{57, 2016.\nA Count-Min for General Zip\fan (with \u000b6= 1)\nIn this appendix we provide an analysis of the expected error with Count-Min in the case with\ninput coming from a general Zip\fan distribution, i.e., fi/1\ni\u000b, for some \fxed \u000b>0. By scaling we\ncan assume that fi=1\ni\u000bwith no loss of generality. Our results on the expected error is presented\nin Table 3 below. We start by analyzing the standard Count-Min sketch that does not have access\nto a machine learning oracle.\n22\n\nk= 1k>1\nCM,\u000b<1 \u0002\u0010\nn2\u00002\u000b\nB\u0011\n\u0002\u0010\nkn2\u00002\u000b\nB\u0011\nCM,\u000b>1O\u00001\nB\u0001\n(O(1))k(log(B))k=\u000b+1\u0001\u0010\nkk\nBk+k\u000b\nB\u000b\u0011\nand \n\u0010\nkk\nBk+k\u000b\n(Blogk)\u000b\u0011\nL-CM,\u000b<1\u0002\u0010\nn2\u00002\u000b\nB\u0011\n\n\u0010\nn2\u00002\u000b\nB\u0011\nL-CM,\u000b>1\u0002\u0000\nB1\u00002\u000b\u0001\n\n\u0000\nB1\u00002\u000b\u0001\nTable 3: The (scaled) expected errors Err( F;~FA) =P\ni2[n]fijfi\u0000~fijof classic and learned Count-\nMin withkhash functions when the input has a Zip\fan distribution with exponent \u000b6= 1. The\nexpected errors can be found by normalizing withP\ni2[n]fiwhich is \u0002( n1\u0000\u000b) for\u000b<1 and \u0002(1)\nfor\u000b > 1. We note that when k > 1 is a constant, the upper and lower bounds for CM for\n\u000b > 1 are within logarithmic factors of each other. In particular we obtain the combined bound\nof~\u0002\u00001\nBk+1\nBk\u0001\nin this case, demonstrating that the bounds, even if they appear complicated, are\nalmost tight.\nA.1 Standard Count-Min\nWe begin by considering the case \u000b<1, in which case we have the following result.\nTheorem A.1. Let0<\u000b< 1be \fxed and fi= 1=i\u000bfori2[n]. Letn;B;k2Nwithk\u00151and\nB\u0014n=k. Let further h1;:::;hk: [n]![B]be independent and truly random hash functions. For\ni2[n]de\fne the random variable ~fi= min`2[k]\u0010P\nj2[n][h`(j) =h`(i)]fj\u0011\n. For anyi2[n]it holds\nthatE[j~fi\u0000fij] = \u0002\u0010\nn1\u0000\u000b\nB\u0011\n.\nWe again note the phenomenon that with a total ofBbuckets, i.e., replacing BbyB=k in the\ntheorem, the expected error is \u0002\u0010\nkn1\u0000\u000b\nB\u0011\n, which only increases as we use more hash functions.\nProof. For a \fxed `2[k] we have that\nE2\n4X\nj2[n]nfig[h`(j) =h`(i)]fj3\n5=1\nBX\nj2[n]nfig1\nj\u000b=O\u0012n1\u0000\u000b\nB\u0013\n;\nand so E[j~fi\u0000fij] =O\u0010\nn1\u0000\u000b\nB\u0011\n.\nFor the lower bound, we de\fne N= [n]n([B][fig) and for`2[k],X`=P\nj2N[h`(j) =h`(i)]fj.\nSimple calculations yield that E[X`] = \u0002\u0010\nn1\u0000\u000b\nB\u0011\nand\nVar[X`] =8\n>>><\n>>>:\u0002\u0012\nlog(n\nB)\nB\u0013\n; \u000b = 1=2;\n\u0002\u0010\nn1\u00002\u000b\nB\u0011\n; \u000b< 1=2;\n\u0002\u0000\nB\u00002\u000b\u0001\n; \u000b> 1=2:\n23\n\nUsing Bennett's inequality (Theorem B.1), with M=B\u0000\u000bwe obtain that\nPr[X`\u0014E[X`]=2]\u00148\n>>><\n>>>:exp\u0010\n\u0000\n(log(n=B)h(\u0000n\nB\u00011=2 1\nlog(n=B)))\u0011\n; \u000b = 1=2;\nexp\u0010\n\u0000\n\u0010\u0000n\nB\u00011\u00002\u000bh\u0000\u0000n\nB\u0001\u000b\u0001\u0011\u0011\n; \u000b< 1=2;\nexp\u0010\n\u0000\n\u0010\nh\u0010\u0000n\nB\u00011\u0000\u000b\u0011\u0011\u0011\n; \u000b> 1=2:\nUsing that n\u0015kBand Remark B.2 we in either case obtain that\nPr[X`\u0014E[X`]=2] = exp\u0000\n\u0000\n(k1\u0000\u000blogk)\u0001\n=k\u0000\n(k1\u0000\u000b):\nAs the events ( X`>E[X`]=2)`2[k]are independent, they happen simultaneously with probability\n(1\u0000k\u0000\n(k1\u0000\u000b))k= \n(1). If they all occur, then j~fi\u0000fij= \n\u0010\nn1\u0000\u000b\nB\u0011\n, so it follows that E[j~fi\u0000fij] =\n\n\u0010\nn1\u0000\u000b\nB\u0011\n, as desired.\nNext, we consider the case \u000b>1. In this case we have the following theorem where we obtain\nthe result presented in Table 3 by replacing BwithB=k.\nTheorem A.2. Let\u000b > 1be \fxed and fi= 1=i\u000bfori2[n]. Letn;B;k2Nwithk\u00152and\nB\u0014n=k. Let further h1;:::;hk: [n]![B]be independent and truly random hash functions. For\ni2[n]de\fne the random variable ~fi= min`2[k]\u0010P\nj2[n][h`(j) =h`(i)]fj\u0011\n. For anyi2[n]it holds\nthat\nE[j~fi\u0000fij]\u0014Ck(log(B))k=\u000b+1\u0001\u00121\nBk+1\nB\u000b\u0013\n;\nfor some constant Cdepending only on \u000b. Furthermore, E[j~fi\u0000fij] = \n\u0010\n1\nBk+1\n(Blogk)\u000b\u0011\n.\nProof. Let us start by proving the lower bound. Let N= [bBlogkc]. With probability\n\u0010\n1\u0000(1\u00001=B)jNnfigj\u0011k\n\u0015\u0010\n1\u0000ejNnfigj\nB\u0011k\n= \n(1)\nit holds that for each `2[k] there exists j2Nnfigsuch thath`(j) =h`(i). In this case\nj~fi\u0000fij\u00151\n(Blogk)\u000b, so it follows that also E[j~fi\u0000fij]\u00151\n(Blogk)\u000b. Note next that with probability\n1=Bk,h`(1) =h`(i) for each`2[k]. If this happens, j~fi\u0000fij\u00151, so it follows that E[j~fi\u0000fij]\u00151=Bk\nwhich is the second part of the lower bound.\nNext we prove the upper bound. The technique is very similar to the proof of Theorem 3.1. We\nde\fneN1= [B]nfigandN2= [n]n([B][fig). We further de\fne X(`)\n1=P\nj2N1[h`(j) =h`(i)]fj\nandX(`)\n2=P\nj2N2[h`(j) =h`(i)]fjfor`2[k]. Note that for any `2[k],E[X(`)\n2] =O\u00001\nB\u000b\u0001\n, so it\nsu\u000eces to bound E[min`2[k](X(`)\n1)]. Lett\u00153=B\u000bbe given. A similar union bound to that given\nin the proof of Theorem 3.1 gives that for any s\u0014t,\nPr[X(`)\n1\u0015t]\u0014\u0012B\nt=s\u00131\nBt=s+1\nBs1=\u000b\u0014\u0010es\nt\u0011t=s\n+(t=s)1=\u000b\nBt1=\u000b:\n24\n\nChoosingssuch thatt=s= \u0002(log(Bt1=\u000b)) is an integer, we obtain the bound\nPr[X(`)\n1\u0015t]\u0014C1(log(Bt1=\u000b))1=\u000b\nBt1=\u000b=C1(log(Bt\r))\r\nBt\r;\nwhere we have put \r= 1=\u000bandC1is a universal constant. Let Z= min`2[k](X(`)\n1). Note that\nZ\u0014P1\nj=11=j\u000b\u0014C2, whereC2is a constant only depending on \u000b. Thus\nE[Z]\u00143\nB\u000b+ZC2\n3=B\u000bPr[Z\u0015t]dt\u00143\nB\u000b+ZC2\n3=B\u000b\u0012\nC1(log(Bt\r))\r\nBt\r\u0013k\ndt\n\u00143\nB\u000b+Ck\n3log(B)k=\u000b\nBkZC2\n3=B\u000b1\ntk=\u000bdt\nfor some constant C3(depending on \u000b). Ifk\u0014\u000b, the integral is O(logB) and this bound su\u000eces.\nIfk>\u000b , the integral is O(Bk\u0000\u000b), which again su\u000eces to give the desired bound.\nRemark A.3. As discussed in Remark 3.3 we only require the hash functions to be O(logB)-\nindependent in the proof of the upper bound of Theorem A.2. In the upper bound of Theorem A.1\nwe only require the hash functions to be 2-independent.\nA.2 Learned Count-Min\nWe now proceed to analyse the learned Count-Min algorithm which has access to an oracle which,\ngiven an item, predicts whether it is among the Bheaviest items. The algorithm stores the\nfrequencies of the Bheaviest items in Bindividual buckets, always outputting the exact frequency\nwhen queried one of these items. On the remaining items it performs a regular Count-Min sketch\nwith a single hash function hashing to Bbuckets.\nTheorem A.4. Let\u000b > 0be \fxed and fi= 1=i\u000bfori2[n]. Letn;B2Nwith 2B\u0014n\nandh: [n]![B]be a 2-independent hash functions. For i2[n]de\fne the random variable\n~fi=P\nj2[n]n[B][h(j) =h(i)]fj. Then\nE[j~fi\u0000fij] =(\n\u0002\u0010\nn1\u0000\u000b\nB\u0011\n; \u000b< 1\n\u0002 (B\u0000\u000b); \u000b> 1:\nProof. Both results follows using linearity of expectation.\nE[j~fi\u0000fij] =1\nBX\nj2[n]n([B][fig)1\nj\u000b=(\n\u0002\u0010\nn1\u0000\u000b\nB\u0011\n; \u000b< 1;\n\u0002 (B\u0000\u000b); \u000b> 1:\nCorollary A.5. Using the learned Count-Min on input coming from a Zip\fan distribution with\nexponent\u000b, it holds that\nE2\n4X\ni2[n]fi\u0001j~fi\u0000fij3\n5=(\n\u0002\u0010\nn2\u00002\u000b\nB\u0011\n; \u000b< 1;\n\u0002\u0000\nB1\u00002\u000b\u0001\n; \u000b> 1:\n25\n\nWhy are we only analysing learned Count-Min with a single hash function? After all, might it\nnot be conceivable that more hash functions can reduce the expected error? It turns out that if\nour aim is to minimize the expected error Err( F;~FA) we cannot do better than in Corollary A.5.\nIndeed, we can employ similar techniques to those used in [HIKV19] to prove the following lower\nbound extending their result to general exponents \u000b6= 1.\nTheorem A.6. Let\u000b>0be \fxed and fi= 1=i\u000bfori2[n]. Letn;B2Nwithn\u0015cBfor some\nsu\u000eciently large constant cand leth: [n]![B]beany function. For i2[n]de\fne the random\nvariable ~fi=P\nj2[n][h(j) =h(i)]fj. Then\nX\ni2[n]fi\u0001j~fi\u0000fij] =(\n\n\u0010\nn2\u00002\u000b\nB\u0011\n; \u000b< 1\n\n\u0000\nB1\u00002\u000b\u0001\n; \u000b> 1:\nA simple reduction shows that Count-Min with a total of Bbuckets and any number of hash\nfunctions cannot provide and expected error that is lower than the lower bound in Theorem A.6\n(see [HIKV19]).\nProof. We subdivide the exposition into the cases 0 <\u000b< 1 and\u000b>1.\nCase 1: 0<\u000b< 1.In this case\nX\ni2[n]fi\u0001j~fi\u0000fij\u0015X\ni2[n]n[B]fi\u0001j~fi\u0000fij=X\nb2[B]0\n@X\nj2[n]n[B]:h(j)=bfj1\nA2\n\u0000X\ni2[n]n[B]f2\ni\n=X\nb2[B]S2\nb\u0000X\ni2[n]n[B]f2\ni; (7)\nwhere we have put Sb=P\nj2[n]n[B]:h(j)=bfj, the total weight of items hashing to bucket b. Now by\nJensen's inequality\nX\nb2[B]S2\nb\u00151\nB0\n@X\ni2[n]n[B]fi1\nA2\nFurthermore, we have the estimates\nX\ni2[n]n[B]fi=nX\ni=B1\ni\u000b\u00001\nB\u000b\u0015Zn\nBx\u0000\u000bdx\u00001\nB\u000b=1\n1\u0000\u000b(n1\u0000\u000b\u0000B1\u0000\u000b)\u00001\nB\u000b;\nand\nX\ni2[n]n[B]f2\ni\u0014Zn\nBx\u00002\u000b=(\n1\n1\u00002\u000b(n1\u00002\u000b\u0000B1\u00002\u000b); \u000b6= 1=2\nlog(n=B); \u000b = 1=2:\nHere we have used the standard technique of comparing a sum to an integral. Assuming that n\u0015cB\nfor some su\u000eciently large constant c(depending on \u000b), it follows thatP\nb2[B]S2\nb= \n\u0010\nn2\u00002\u000b\nB\u0011\n. It\n26\n\nmoreover follows (again for nsu\u000eciently large) that,\nX\ni2[n]n[B]f2\ni=8\n><\n>:O(log(n=B)); \u000b = 1=2;\nO(n1\u00002\u000b); \u000b< 1=2;\nO(B1\u00002\u000b); \u000b> 1=2:\nPlugging into (7), we see that in each of the three cases \u000b < 1=2,\u000b= 1=2 and\u000b > 1=2 it holds\nthatP\nb2[B]S2\nb\u0000P\ni2[n]f2\ni= \n\u0010\nn2\u00002\u000b\nB\u0011\n.\nCase 2: 0\u000b > 1.For this case we simply assume that n\u00153B. LetI\u0012[3B]n[B] consist of\nthoseisatisfying that h(i) =h(j) for somej2[3B]n[B],j6=i. ThenjIj\u0015Band ifi2I, then\nfi\u0015(3B)\u0000\u000bandj~fi\u0000fij\u0015(3B)\u0000\u000b. Thus\nX\ni2[n]fi\u0001j~fi\u0000fij\u0015X\ni2Ifi\u0001j~fi\u0000fij\u0015B(3B)\u00002\u000b= \n(B1\u00002\u000b):\nA.3 Learned Count-Min using a noisy oracle\nAs we did in the case \u000b= 1 (Theorem 5.7), we now present an analogue to Theorem A.4 when\nthe heavy hitter oracle is noisy. Note that the results in Table 3 demonstrates that we obtain\nno asymptotic improvement using the heavy hitter oracle when 0 < \u000b < 1 and therefore we only\nconsider the case \u000b>1. We show the following trade-o\u000b between the classic and learned case, as\nthe error probability, \u000e, that the heavy hitter oracle misclassi\fes an item, varies in [0 ;1].\nTheorem A.7. Suppose that the input follows a generalized Zip\fan distribution with n\u0015Bdif-\nferent items and exponent \u000bfor some constant \u000b > 1. Learned Count-Sketch with a single hash\nfunctions, a heavy hitter oracle HH\u000e,Bh= \u0002(B)bins allocated to the Bhitems classi\fed as heavy\nandB\u0000Bh= \u0002(B)bins allocated to a Count-Sketch of the remaining items, incurs an expected\nerror of\nO\u00121\nB\u0000\n\u000e+B1\u0000\u000b\u00012\u0013\nProof. The proof is very similar to that of Theorem 5.7 Let h: [n]![B\u0000Bh] be the hash function\nused for the Count-Min. In the analysis to follow, it is enough to assume that it is 2-independent.\nSuppose item iis classi\fed as non-heavy. The expected error incurred by item iis then\n1\nB\u0000Bh0\n@\u000eX\nj2[Bh]nfigfj+X\nj2[n]n([Bh][fig)fj1\nA=O\u0012\u000e+B1\u0000\u000b\nB\u0013\n:\nLettingN=P\ni2[n]fi=O(1), the expected error (as de\fned in (2)) is at most\n1\nN0\n@\u000eX\nj2[Bh]nfigfj+X\nj2[n]n([Bh][fig)fj1\nA\u0001O\u0012\u000e+B1\u0000\u000b\nB\u0013\n=O\u00121\nB\u0000\n\u000e+B1\u0000\u000b\u00012\u0013\n;\nas desired.\nFor\u000e= 1, we recover the bound for the classic Count-Min. We also see that it su\u000eces that\n\u000e=O(B1\u0000\u000b) in order to obtain the same bound as with a perfect heavy hitter oracle.\n27\n\nB Concentration bounds\nIn this appendix we collect some concentration inequalities for reference in the main body of the\npaper. The inequality we will use the most is Bennett's inequality. However, we remark that for\nour applications, several other variance based concentration result would su\u000ece, e.g., Bernstein's\ninequality.\nTheorem B.1 (Bennett's inequality [Ben62]) .LetX1;:::;Xnbe independent, mean zero random\nvariables. Let S=Pn\ni=1Xi, and\u001b2;M > 0be such that Var[S]\u0014\u001b2andjXij\u0014Mfor alli2[n].\nFor anyt\u00150,\nPr[S\u0015t]\u0014exp\u0012\n\u0000\u001b2\nM2h\u0012tM\n\u001b2\u0013\u0013\n;\nwhereh:R\u00150!R\u00150is de\fned by h(x) = (x+ 1) log(x+ 1)\u0000x. The same tail bound holds on the\nprobability Pr[S\u0014\u0000t].\nRemark B.2. Forx\u00150,1\n2xlog(x+1)\u0014h(x)\u0014xlog(x+1). We will use these asymptotic bounds\nrepeatedly in this paper.\nA corollary of Bennett's inequality is the classic Cherno\u000b bounds.\nTheorem B.3 (Cherno\u000b [Che52]) .LetX1;:::;Xn2[0;1]be independent random variables and\nS=Pn\ni=1Xi. Let\u0016=E[S]. Then\nPr[S\u0015(1 +\u000e)\u0016]\u0014exp(\u0000\u0016h(\u000e)):\nEven weaker than Cherno\u000b's inequality is Hoe\u000bding's inequality.\nTheorem B.4 (Hoe\u000bding [Hoe63]) .LetX1;:::;Xn2[0;1]be independent random variables. Let\nS=Pn\ni=1Xi. Then\nPr[S\u0000E[S]\u0015t]\u0014e\u00002t2\nn:\n28",
  "textLength": 69144
}