{
  "paperId": "52da09511534cb2ab24e4dd72a8b45a82c53cc18",
  "title": "Deep Hashing using Entropy Regularised Product Quantisation Network",
  "pdfPath": "52da09511534cb2ab24e4dd72a8b45a82c53cc18.pdf",
  "text": "DEEP HASHING USING ENTROPY REGULARISED\nPRODUCT QUANTISATION NETWORK\nJo Schlemper, Jose Caballero, Andy Aitken, Joost van Amersfoort\nCortex Applied Machine Learning, Twitter\njo.schlemper11@imperial.ac.uk, fjcaballero,aaitken g@twitter.com,\njoost.van.amersfoort@cs.ox.ac.uk\nABSTRACT\nIn large scale systems, approximate nearest neighbour search is a crucial algo-\nrithm to enable efﬁcient data retrievals. Recently, deep learning-based hashing\nalgorithms have been proposed as a promising paradigm to enable data dependent\nschemes. Often their efﬁcacy is only demonstrated on data sets with ﬁxed, limited\nnumbers of classes. In practical scenarios, those labels are not always available or\none requires a method that can handle a higher input variability, as well as a higher\ngranularity. To fulﬁl those requirements, we look at more ﬂexible similarity mea-\nsures. In this work, we present a novel, ﬂexible, end-to-end trainable network for\nlarge-scale data hashing. Our method works by transforming the data distribution\nto behave as a uniform distribution on a product of spheres. The transformed data\nis subsequently hashed to a binary form in a way that maximises entropy of the\noutput, (i.e. to fully utilise the available bit-rate capacity) while maintaining the\ncorrectness (i.e. close items hash to the same key in the map). We show that the\nmethod outperforms baseline approaches such as locality-sensitive hashing and\nproduct quantisation in the limited capacity regime.\n1 I NTRODUCTION\nIn the modern era where we have an increasingly large amount of high-dimensional data to handle, it\ncan be useful to have a system that can efﬁciently retrieve information that we care about. Examples\nof such systems are content-based image retieval (CBIR) (Datta et al., 2008; Babenko et al., 2014)\nand document/information retrieval (Mitra & Craswell, 2018). In large scale systems, linear search\nthrough the dataset is prohibitive. Therefore, one often resorts to approximate methods, which\nallow trading off accuracy for speed. These methods are commonly called approximate nearest\nneighbour (ANN) methods. Another important aspect of modern systems is data locality – ideally\nthe data is stored a local fast disk, which restricts our representation to be quantised due to memory\nrestrictions. Notable examples commonly used are locality-sensitive hashing (LSH) (Datar et al.,\n2004) and product quantisation (PQ) (Jegou et al., 2011; Ge et al., 2013).\nRecently, deep learning has become an increasingly powerful tool for learning embeddings, due to\nthe success of deep embedding learning (Oh Song et al., 2016; Hermans et al., 2017; Wu et al., 2017).\nThese advances have motivated an approach called deep hashing (Wang et al., 2016; Erin Liong\net al., 2015; Zhu et al., 2016), where one attempts to directly obtain a hash code from an image\nthat can be used for content-based image retrieval tasks. These methods have been shown to greatly\noutperform traditional approaches. However, most methods rely on explicitly incorporating the\nclass label prediction (as opposed to constructing an afﬁnity matrix) to improve performance, which\nleads to the following issues. Firstly, while exploiting the class labels can improve the discriminative\ncapability, it makes incorporating new labels a non-trivial task. Secondly, the methods do not directly\naccount for semantic similarities at a granular level, making it unsuitable for certain tasks such as\na duplication detection. Lastly, it is common to only demonstrate the efﬁcacy of the methods for\ndataset with a small number of classes ( n\u0014100), and the generalisation for the large scale dataset\nseems yet to be proven.\nIn this work, we propose a novel network architecture for end-to-end semantic hashing, which can\nbe used for both deep hashing and learning an index structure (Kraska et al., 2018). Our network is\n1arXiv:1902.03876v1  [cs.LG]  11 Feb 2019\n\ninspired by a catalyser network (Sablayrolles et al., 2018) and a supervised structured binary code\n(SUBIC) (Jain et al., 2017): it explores the idea of transforming an input distribution to a uniform\ndistribution, but directly learns to generate the hash code. Our method is also ﬂexible such that\nit relies on a similarity distance, which can be neighbour ranking or class labels. We show the\napplicability of our model for retrieval task using publicly available data set and we experimentally\nshow our approach outperforms baseline methods such as LSH and PQ, in particular when the\navailable bit-rate is limited.\n2 R ELATED WORK\n... \n \n... \n \n... \n Conv+Pooling\nBinary-like\nOutputConv+Pooling DenseDense\nT raining\nT esting... ... ... ... Similarity Loss: \n  - classiﬁcation loss \n  - triplet loss\n  - etc.\n \nBit Rate Loss\n  - Entropy\n  - etc..\n \nBinarisation\nT raining\nQuery... \n \nFigure 1: Schematics of Supervised Deep Hashing.\nIn the literature of hashing, common methods include: LSH, Iterative Quantisation (ITQ) (Gong\net al., 2013) and PQ. While the ﬁrst two aim to generate code in hamming space, PQ aims to rep-\nresent data using a code book for a set of sub vectors. Recently deep learning-based hashing has\nbecome an active area of research (Lin et al., 2015; Liu et al., 2016). In particular, supervised-\nhashing is a research area that is concerned with hashing and retrieving objects (e.g. text or images)\nbelonging to speciﬁc categories. In deep-learning based hashing, often three aspects are considered\nfor the objective function, which are:\n1. How to preserve semantic similarities of the inputs in their generated hash codes?\n2. How to devise a continuous representation that can be trained using a neural network, which\nsimultaneously minimises the discrepancy from test-time discretisation/binarisation?\n3. How can we optimise the available bit-rate of a database (the output space), which can help\nminimise the collision probability?\nThe ﬁrst challenge is often addressed by using a metric learning approach, such as contrastive loss\n(Hadsell et al., 2006), triplet loss (Schroff et al., 2015; Hermans et al., 2017) and their n-way exten-\nsions (Chen et al., 2017). The main drawback of these losses is that it is difﬁcult to optimise them\nin a high dimensional space due to the curse of dimensionality (Friedman et al., 2001). Therefore,\nwhenclass labels are available, it can be more effective to utilise those (Jain et al., 2017). Doing\nso induces a dependence on quality and availability of the class labels. In fact, as pointed out in\nSablayrolles et al. (2017), retrieving objects based on classiﬁcation puts an upper bound on the per-\nformance for the recall value. It can be more desirable to optimise the model on a more ﬂexible\nobjective which allows granular hashing based on semantic similarity, rather than solely the label\ninformation. The second challenge is a product of the non-differentiability of a naive discretisation\nstep. Therefore at train time, one can resort to non-linearities such as sigmoid andtanh , or contin-\nuous functions with better properties (Cao et al., 2017; 2018). The last point is usually handled by\nvariants of entropy-based regularisation (Jain et al., 2017).\nWe also provide a more in-depth account of SUBIC (Jain et al., 2017) and catalyser network (Sablay-\nrolles et al., 2018) as these networks form a foundation of our network architecture.\nSUBIC Letx2RNbe an input data (e.g. an image) and y2Y =f1;:::;Cgis the the class\nlabel. Given an image z, SUBIC outputs a structured binary code b, which is expressed as M K -\nblocks:b= [b(1);:::;b(M)]withb(i)2KK=fd2f0;1gKs.t.kdk1= 1g. This is achieved by\nthe following network:\n2\n\nb=fSUBIC (x) =\u001b\u000efenc\u000effeat(x) (1)\nwhereffeat:RN!RN0is a feature extractor, fenc:RN0!RMKis a hash encoder, \u001bis a\nnonlinearity which applies K-softmax function to each of the Mblock and “\u000e“ is a composition\noperator. During training, the K-blocks are relaxed into K-simplicies:eb= [eb(1);:::;eb(M)]with\neb(i)2\u0001K=fd2[0;1]Ks.t.kdk1= 1g. The novelty of SUBIC is to ﬁt a classiﬁcation layer\nfclf:RMK!RCto learn a discriminative binary code. Given minibatch B=f(xi;yi)gB\ni=1, the\nnetwork is trained by minimising the following loss function:\nL(B) =1\nBBX\ni=1logs[yi]\n|{z}\nCross Entropy+\r1\nBBX\ni=1E(ebi)\n|{z}\nEntropy\u0000\u0016E \n1\nBBX\ni=1ebi!\n|{z}\nBatch Entropy(2)\nwheres=fclf(eb),eb=fSUBIC (x)andEis mean entropy of Kblocks:\nE(b) =\u00001\nMMX\nm=1KX\nk=1b(m)[k] log2b(m)[k] (3)\nThe idea of the Entropy term is to encourage the network output to become one-hot like. On the other\nhand, the Negative Batch Entropy term encourages the uniform block support so that the available\nbit rate is fully exploited. At test time, \u001bis replaced by a block-wise arg max operation, where the\nbinary code is obtained by setting the maximum activated entry in each block to 1, the rest to 0.\nCatalyser network Letxbe the input, z=fcat(x)be the network embedding before quantisation\nis applied. Let b=fquant(z)be the quantised representation. The idea of the catalyser network is to\nembed data uniformly on an `2-sphere, i.e. z2SN, which is subsequently encoded by an efﬁcient\nlattice quantiser. The network is trained by minimising teh triplet rank loss (Hermans et al., 2017)\nand maximising the entropy loss. Given a triplet of input (xa;xp;xn)(anchor point, positive sample\nand negative sample respectively), the loss is deﬁned as:\nLtri= [kfcat(xa)\u0000fcat(xp)k2\u0000kfcat(xa)\u0000fcat(xn)k2+\u000b]+(4)\nfor margin\u000b. For entropy regularisation, Kozachenko and Leonenko (KoLeo) entropy estimator is\nused as a surrogate function:\nHn=\u000b\nnnX\nilog(\u001an;i) +\f; \u001an;i= min\ni6=jkfcat(xi)\u0000fcat(xj)k (5)\nThe equation is simpliﬁed to:\nLKoLeo =nX\nilog(\u001an;i) (6)\nThe geometric idea is to ensure any two points are sufﬁciently far from each other, where the penalty\ndecays logarithmically. The network then quantises the output using a Gosset Code, we refer the\ninterested reader to Sablayrolles et al. (2018) for more details.\n3\n\n... ... W... ... ... ... \nBinarisation\n... Conv+PoolingConv+PoolingDense T raining\nQuery... ... \n... Dense\n... ... Dense\nfcat fquant fencℒKoLeo-W ℒKoLeo-y ℒtri-zℒtri-y ℒquant\nz y\nbFigure 2: Schematic of the proposed architecture.\n3 P ROPOSED APPROACH\nWhile SUBIC is effective, its application is limited to cases where classiﬁcation labels are available.\nOn the other hand, catalyser networks are more ﬂexible, but not end-to-end trainable. In this work,\nwe propose a ﬂexible approach which incorporates the beneﬁt of both and mitigates their limitations.\nThe proposed network is composed of three components: a feature extractor fenc, a catalyser fcat,\nand a quantiser fquant, see Fig. 2. Given an input image x, the network directly generates a hash,\nwhich is a structured binary code as in SUBIC: b2[0;1]M\nK. Lety=fcat\u000efenc(x),b=fquant(y). The\ndifference between catalyser networks and our work is that we learn the quantisation network fquant,\nmaking the architecture end-to-end trainable. Our quantiser fquant(y)is given byz(m)=\u001bk(Wy),\nwhere\u001bkis a block-wise K-softmax.\nFor simplicity, consider each K-block separately. Our key insight is the following: a fully con-\nnected layer is simply a dot product between yand the row vectors of W. We have z(m)=\n\u001bk(hw1;yi;:::;hwK;yi). Since at test-time, binarisation is done by selecting the maximally ac-\ntivated entry (within each K-block), this is equivalent to selecting the row vector with the smallest\nangular difference. This can be visualised using row vectors, w1;:::;wkwhich linearly partition\nthe output space, and the decision boundary is extending from the origin (Fig. 3).\nFigure 3: The geometry of the linear layer. The dashed line deﬁnes the assignment boundary for\neachwi;s\nLetp(y)denote the probability distribution of the catalyser output taking a speciﬁc value in y2\nSK\u00001. To achieve the maximal entropy of fquant(y)(i.e. to maximise the used bit-rate), one can\nassignyto each row vector wi’s with an equal probability. Geometrically, this can be seen as equally\npartitioning the support of p(y)bywi’s, where supp (p(y)) =fy2SK\u00001jp(y=fcat\u000efenc(x))>\n0g. Ifyisuniformly distributed on SK\u00001, then it is sufﬁcient to uniformly distribute wi’s to partition\ntheSK\u00001equally. This gives us the following strategy: we (1) encourage the distribution of p(y)\n4\n\nto be uniform on a sphere and (2) uniformly distribute wi’s on the sphere. We can achieve both by\nusing KoLeo entropy estimators in Eq. 6:\nLKoLeo-y =nX\nilog(\u001an;i); \u001an;i= min\nj6=ikyi\u0000yjk (7)\nLKoLeo-W =nX\nilog(\u001cn;i); \u001cn;i= min\nj6=ikwi\u0000wjk (8)\nHowever, it is likely that a perfect uniform distribution cannot be achieved by the training, especially\nonce combined with other deep embedding losses. To circumvent this, we add the following:\nLquant=X\nikyi\u0000w\u0013(yi)k (9)\nwherej=\u0013(yi) = arg min j0kyi\u0000wj0k(i.e. the index of the closest wjtoy). By minimising Eq.\n9, the row vectors wi’s will be gravitated towards the probability mass of p(y).\nThe remaining aspect is similar to the previous approaches: we minimise triplet loss to ensure\nsimilar points are embedded closely. For this, we can minimise triplet loss either in the output\nspace of catalyser SK\u00001\u0002\u0001\u0001\u0001\u0002SK\u00001\n|{z}\nMor the relaxed output space \u0001M\nK. Indeed, it is beneﬁcial to\ndirectly optimise in the ﬁnal target space. However, interestingly, it turns out that minimising triplet\nrank loss in simplex is difﬁcult due to the fact that most points are very close to each other in high\ndimensional simplicies (see Appendix), yielding training instability. We mitigate this issue by using\nasymmetrical triplet loss:\nLtri-z= [kza\u0000bpk\u0000kza\u0000bnk+\u000b]+(10)\nwhereza,zp,znare the anchor, positive and negative points in the embedded space respectively, \u000b\nis a margin, b\u0003’s are the discretised point (i.e. by replacing softmax by argmax). Note that sampling\nb\u0003’s is non-differentiable due to argmax, but we can nevertheless backpropagate the information\nusing straight-through estimator (STE) proposed by Bengio et al. (2013), which has resemblance to\nstochastic graph computation approaches (Maddison et al., 2016). Secondly, the loss becomes zero\nifbpandbnshare the same binary representation. We empirically found incorporating the triplet\nloss inSK\u00001\u0002\u0001\u0001\u0001\u0002SK\u00001\n|{z}\nMwas a useful additional loss to overcome this issue. The ﬁnal objective\nis thus:\nL=Ltri-z+\u00151Ltri-y+\u00152LKoLeo-y +\u00153LKoLeo-W +\u00154Lquant (11)\nwhere\u0015i’s are hyper-parameter to be optimised. Note that for triplet loss, `2-normalising yand the\nrows ofWis important as otherwise arbitrary scaling can make the training unstable. Secondly, to\nreduce the parameters, we partition Wto only take each K-blocks and learn W1;:::;Wk, where\nWi2RK\u0002K.1Finally, note that feature extractor and catalyser are only optimised with respect to\nLtri-z;Ltri-y, andLKoLeo-y , whereas the quantiser weight Wis optimised only with respect to Ltri-z,\nLKoLeo-w andLquant. In particular, Eq. 9 is only minimised by W.\n3.1 E NCODING AND DISTANCE COMPUTATION\nGiven a set of data points, we encode via b=fquant\u000efcat\u000effeat(x). The resulting vector can\nbe compressed by storing indices of one-of- Kvectors, which only requires Mlog(K)bits. The\n1Ideally, M K -blocks are decorrelated to remove the redundancy. This is left as a future work.\n5\n\ndistance between two compressed points can be given by Euclidean distance: kbi\u0000bjk2, which can\nbe efﬁciently computed by Mlook up :PM\nib(i)\nquery[i(b(i))], wherei(:)is the index of b(i)having one.\nOne can also perform asymmetric distance comparison (ADC), which in case bquery is replaced by\nyquery=fcat\u000effeat(x), the data representation prior to quantisation.\n3.2 N ETWORK IMPLEMENTATION\nFor the feature encoder, a pre-trained network can be used, such as VGG or Resnet architectures.\nThe catalyser was implemented using a fully connected network with 2 hidden layers, each having\n256 features, and a ﬁnal layer which maps the dimension to d=MK . We used batch-normalisation\nand Rectiﬁed Linear Unit (ReLU) for non-linearity, on all layers except the ﬁnal one. The quantiser\nareMseparate fully connected layers with Kfeatures. The overall network was trained using Adam\nwith\u000b= 10\u00003;\f1= 0:9;\f2= 0:999. The convergence speed of the network depends on the size\nofK, but usually sufﬁcient performance can be obtained within 3 hours of training.\nFigure 4: 1-Recall@10 for BigANN1M dataset.\n4 E XPERIMENT\n4.1 B IGANN1M\nWe evaluate our proposed approach using BigANN1M dataset2: the dataset contains a collection\nof 128 dimensional SIFT feature vectors. As the input is already feature vectors, we set fenc=id.\nTraining data contains 30,000 points, test data contains 10,000 query points and 1 million database\npoints. For each point, we labelled the top k= 10 nearest points in terms of Euclidean distance to\nbe the neighbours for triplet loss.\nFor evaluation, we used the metric 1-Recall@K=10, which measures the probability of retrieving\nthe true ﬁrst neighbour within the ﬁrst 10 candidates. We compare to LSH, ITQ and PQ for the\nbaseline methods. For PQ, we chose K= 256 for each sub-vector and varied the values of Mto\nachieve the desired bit-length B2f16;32;64;128g.\nThe result is summarised in Fig. 4. For the proposed method, we varied the number of d,Mand\nKto get different number of bits. One can see that the performance of the proposed approach is\n2Publicly available at http://corpus-texmex.irisa.fr/\n6\n\ncomparable to PQ, but better for lower number of bits. Note that ITQ and LSH uses symmetric\ndistance comparison so it is an unfair comparison. We also compared the proposed model with\nand withoutLquant and we see a noticeable improvement. We speculate that this is because, while\neven without the loss, since the points are uniformly distributed it can achieve sufﬁcient level of\nreconstruction, by minimising the quantisation loss, we remove the “gaps“ in supp (p(y)).\n4.1.1 V ISUALISATION\nWe visualise the learnt weight vectors of the quantiser fquant. For eachMsub-block, we randomly\nselect 500 row vectors. Then we visualise 2 axes of these vectors (i.e. a projection onto 2 di-\nmensional plane rather than using dimensionality reduction techniques). Without using Lquant, the\nweights are uniformly distributed on k-sphere (Fig 5). However, when the loss is introduced, we see\nthe mass of the rows concentrates on a more local area.\nFigure 5: The distribution of the weights wiof the quantiser without Lquant. Each sphere corresponds\nto one sub-block. For each sub-block, we randomly select 2 axes for visualization.\nFigure 6: The distribution of weights wiof the quantiser with Lquant. Each sphere corresponds to\none sub-block. For each sub-block, we randomly select 2 axes for visualization.\n5 C ONCLUSION\nIn this work, we proposed a deep neural network which can perform end-to-end hashing of input,\nwhich only requires the knowledge of similarity graph, which is a slightly more relaxed constraint\nthan class labels. The network operates by transforming the input space into a uniform distribution\nby penalising the cost given by KoLeo differential estimator, which was quantised by weight vectors\nuniformly distributed in its support. The network performs comparatively to the baseline methods,\nhowever, there is plenty of room for improvement. In the future, it will be interesting to impose a\ndifferent prior on the distribution of the simplex, e.g. via Dirichlet distribution, to help control the\noutput distribution, rather than relying on a uniform distribution.\nACKNOWLEDGEMENTS\nWe thank Lucas Theis, Ferenc Husz ´ar, Hanchen Xiong and Twitter London CAML team for their\nvaluable insights and comments for this work.\n7\n\nREFERENCES\nArtem Babenko, Anton Slesarev, Alexandr Chigorin, and Victor Lempitsky. Neural codes for image\nretrieval. In European conference on computer vision , pp. 584–599. Springer, 2014.\nYoshua Bengio, Nicholas L ´eonard, and Aaron C. Courville. Estimating or propagating gradients\nthrough stochastic neurons for conditional computation. CoRR , abs/1308.3432, 2013. URL\nhttp://arxiv.org/abs/1308.3432 .\nYue Cao, Mingsheng Long, Bin Liu, Jianmin Wang, and MOE KLiss. Deep cauchy hashing for\nhamming space retrieval. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition , pp. 1229–1237, 2018.\nZhangjie Cao, Mingsheng Long, Jianmin Wang, and S Yu Philip. Hashnet: Deep learning to hash\nby continuation. In ICCV , pp. 5609–5618, 2017.\nWeihua Chen, Xiaotang Chen, Jianguo Zhang, and Kaiqi Huang. Beyond triplet loss: a deep quadru-\nplet network for person re-identiﬁcation. In The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , volume 2, 2017.\nMayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S Mirrokni. Locality-sensitive hashing\nscheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on\nComputational geometry , pp. 253–262. ACM, 2004.\nRitendra Datta, Dhiraj Joshi, Jia Li, and James Z Wang. Image retrieval: Ideas, inﬂuences, and\ntrends of the new age. ACM Computing Surveys (Csur) , 40(2):5, 2008.\nVenice Erin Liong, Jiwen Lu, Gang Wang, Pierre Moulin, and Jie Zhou. Deep hashing for compact\nbinary codes learning. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pp. 2475–2483, 2015.\nJerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning , vol-\nume 1. Springer series in statistics New York, NY , USA:, 2001.\nTiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. Optimized product quantization for approximate\nnearest neighbor search. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition , pp. 2946–2953, 2013.\nYunchao Gong, Svetlana Lazebnik, Albert Gordo, and Florent Perronnin. Iterative quantization: A\nprocrustean approach to learning binary codes for large-scale image retrieval. IEEE Transactions\non Pattern Analysis and Machine Intelligence , 35(12):2916–2929, 2013.\nRaia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant\nmapping. In null, pp. 1735–1742. IEEE, 2006.\nAlexander Hermans, Lucas Beyer, and Bastian Leibe. In defense of the triplet loss for person re-\nidentiﬁcation. arXiv preprint arXiv:1703.07737 , 2017.\nHimalaya Jain, Joaquin Zepeda, Patrick P ´erez, and R ´emi Gribonval. Subic: A supervised, structured\nbinary code for image search. In Proc. Int. Conf. Computer Vision , volume 1, pp. 3, 2017.\nHerve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor\nsearch. IEEE transactions on pattern analysis and machine intelligence , 33(1):117–128, 2011.\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 International Conference on Management of Data ,\npp. 489–504. ACM, 2018.\nKevin Lin, Huei-Fang Yang, Jen-Hao Hsiao, and Chu-Song Chen. Deep learning of binary hash\ncodes for fast image retrieval. In Proceedings of the IEEE conference on computer vision and\npattern recognition workshops , pp. 27–35, 2015.\nHaomiao Liu, Ruiping Wang, Shiguang Shan, and Xilin Chen. Deep supervised hashing for fast im-\nage retrieval. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npp. 2064–2072, 2016.\n8\n\nChris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous\nrelaxation of discrete random variables. arXiv preprint arXiv:1611.00712 , 2016.\nBhaskar Mitra and Nick Craswell. An introduction to neural information retrieval. Foundations and\nTrends R\rin Information Retrieval (to appear) , 2018.\nHyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted\nstructured feature embedding. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition , pp. 4004–4012, 2016.\nAlexandre Sablayrolles, Matthijs Douze, Nicolas Usunier, and Herv ´e J´egou. How should we eval-\nuate supervised hashing? In Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE\nInternational Conference on , pp. 1732–1736. IEEE, 2017.\nAlexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Herv ´e J´egou. A neural network\ncatalyzer for multi-dimensional similarity search. arXiv preprint arXiv:1806.03198 , 2018.\nFlorian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A uniﬁed embedding for face\nrecognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pp. 815–823, 2015.\nJun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang. Learning to hash for indexing big dataa\nsurvey. Proceedings of the IEEE , 104(1):34–57, 2016.\nChao-Yuan Wu, R Manmatha, Alexander J Smola, and Philipp Kr ¨ahenb ¨uhl. Sampling matters in\ndeep embedding learning. In Proc. IEEE International Conference on Computer Vision (ICCV) ,\n2017.\nHan Zhu, Mingsheng Long, Jianmin Wang, and Yue Cao. Deep hashing network for efﬁcient simi-\nlarity retrieval. In AAAI , pp. 2415–2421, 2016.\n9\n\n6 A PPENDIX\n6.1 D ISTRIBUTION OF PAIRWISE DISTANCES ON SURFACES IN N DIMENSION\nIn the main manuscript, we argued that it is difﬁcult to directly train triplet rank loss on high-\ndimensional simplex. Here we show how points on n-dimensional objects are distributed in high\ndimension as a part of the argument.\nFigure 7: The distribution of `2distance between two random points on n-dimensional objects\n6.1.1 I NTERIOR OF SIMPLEX\nWe use Dirichlet distribution with concentration parameter a1=\u0001\u0001\u0001=an= 1 to sample points\nuniformly in the interior of n-dimensional simplex. As one can see from Fig 8, as the dimension in-\ncreases, the the points become more concentrated around the center of simplex \u0016= (1=n;:::; 1=n).\nThe distribution of `2distance between two uniformly sampled points on n-simplex also sharply\nconcentrates around small value, as it can be seen in 7. However, in the case of triplet rank loss, we\nwould like to guarantee sufﬁciently high margin to ensure the separation between different classes.\nFor example, the distance from any of the vertices to the centre of simplex ispn\u00001pnand the distance\nbetween two vertices isp\n2). We empirically saw that often the network collapses to predicting just\n\u0016and it is difﬁcult to satisfy meaningful margin \u000bas well as pushing the points approach towards\none of the vertices.\nFigure 8: 3D projection of high-dimensional simplex\n6.1.2 S URFACE OF N -SPHERE\nWe sample points uniformly on n-sphere by ﬁrst sampling nfromN(0;1), followed by `2-\nnormalisation. In this case, the distribution of distances between two uniformly sampled points\nonn-sphere is given by: p(d)/dn\u00002[1\u00001\n4d2]n\u00003\n2(Wu et al., 2017). As n!1 , the probability\ndistribution converges to N(p\n2;1\n2n). In this case, there is sufﬁcient space left between majority of\npoints, which is why we speculate that it is easier to train with triplet rank loss. Note that this how-\never also means that since all points are alreadyp\n2far, careful negative example mining becomes\nvery important to yield useful gradient.\n6.1.3 I NTERIOR OF N -CUBE\nWe also study the distribution of distances between two random points in the interior of a hypercube.\nHere, the distances gets increasingly large as n!1 . Therefore, hypercube would have been an\n10\n\nFigure 9: 3D projection of high-dimensional n-sphere\nalternative shape we could use as a domain for hashing, which could be interesting for future work.\nHowever, In this case, we could use sigmoid function to set the range, but this could result in gradient\nsaturation.\nFigure 10: 3D projection of high-dimensional n-cube\n11",
  "textLength": 27288
}