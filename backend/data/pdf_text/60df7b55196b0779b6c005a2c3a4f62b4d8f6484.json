{
  "paperId": "60df7b55196b0779b6c005a2c3a4f62b4d8f6484",
  "title": "Evaluating Learned Indexes in LSM-tree Systems: Benchmarks,Insights and Design Choices",
  "pdfPath": "60df7b55196b0779b6c005a2c3a4f62b4d8f6484.pdf",
  "text": "arXiv:2506.08671v1  [cs.DB]  10 Jun 2025[Experiments & Analysis]\nEvaluating Learned Indexes in LSM-tree Systems: Benchmarks,\nInsights and Design Choices\nJunfeng Liu\nNanyang Technological\nUniversity\nSingapore\njunfeng001@e.ntu.edu.sgJiarui Ye\nNanyang Technological\nUniversity\nSingapore\njiarui005@e.ntu.edu.sgMengshi Chen\nNanyang Technological\nUniversity\nSingapore\nmengshi002@e.ntu.edu.sgMeng Li\nNanjing University\nChina\nmeng@nju.edu.cn\nSiqiang Luo‚àó\nNanyang Technological\nUniversity\nSingapore\nsiqiang.luo@ntu.edu.sg\nAbstract\nLSM-tree-based data stores are widely used in industry due to\ntheir exceptional performance. However, as data volumes grow,\nefficiently querying large-scale databases becomes increasingly\nchallenging. To address this, recent studies attempted to integrate\nlearned indexes into LSM-trees to enhance lookup performance,\nwhich has demonstrated promising improvements. Despite this,\nonly a limited range of learned index types has been considered,\nand the strengths and weaknesses of different learned indexes re-\nmain unclear, making them difficult for practical use. To fill this gap,\nwe provide a comprehensive and systematic benchmark to pursue\nan in-depth understanding of learned indexes in LSM-tree systems.\nIn this work, we summarize the workflow of 8 existing learned\nindexes and analyze the associated theoretical cost. We also iden-\ntify several key factors that significantly influence the performance\nof learned indexes and conclude them with a novel configuration\nspace, including various index types, boundary positions, and gran-\nularity. Moreover, we implement different learned index designs\non a unified platform to evaluate across various configurations.\nSurprisingly, our experiments reveal several unexpected insights,\nsuch as the marginal lookup enhancement when allocating a large\nmemory budget to learned indexes and modest retraining overhead\nof learned indexes. Besides, we also offer practical guidelines to\nhelp developers intelligently select and tune learned indexes for\ncustom use cases.\n‚àóCorresponding Author\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nConference acronym ‚ÄôXX, Woodstock, NY\n¬©2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-XXXX-X/18/06\nhttps://doi.org/XXXXXXX.XXXXXXXACM Reference Format:\nJunfeng Liu, Jiarui Ye, Mengshi Chen, Meng Li, and Siqiang Luo. 2025.\n[Experiments & Analysis] Evaluating Learned Indexes in LSM-tree Sys-\ntems: Benchmarks, Insights and Design Choices. In Proceedings of Make\nsure to enter the correct conference title from your rights confirmation emai\n(Conference acronym ‚ÄôXX). ACM, New York, NY, USA, 14 pages. https:\n//doi.org/XXXXXXX.XXXXXXX\n1 Introduction\nLog-structured Merge Trees (LSM-trees), have already been widely\nused as the fundamental storage structure underpinning many key-\nvalue stores like Google Spanner [ 6], Apache Cassandra [ 29], and\nMongoDB WiredTiger [ 5]. These key-value stores play pivotal roles\nin various applications in social media, streaming processing, and\nfile systems.\nIndex in LSM-tree stores. As illustrated in Figure 1(A), a typi-\ncal LSM-tree consists of both memory and disk components. The\nmemory components include a write buffer, bloom filters, and fence\npointers, while the disk components are sorted key-value arrays\norganized across multiple levels. To prevent the need for sequen-\ntial scanning of these arrays on disk, the fence pointers in memory\nhelp quickly locate the specific data block where the target key is\nstored. This allows only the relevant data block to be read, thereby\nsignificantly reducing the number of I/Os required.\nWhile fence pointers have successfully reduced the number of\nI/Os, the question still arises: can we push the performance further\nwithin the same memory constraints? Recent advances in machine\nlearning have led to the development of several learned indexes,\nwhich have demonstrated superior memory efficiency through ex-\ntensive evaluations [ 41]. These learned indexes achieve impressive\nresults using much less memory space, presenting an exciting op-\nportunity to further optimize indexing in LSM-trees. However, two\nmain questions remain unanswered when applying them to LSM-\ntree systems:\nAre all learned indexes suitable for LSM-tree systems? Recent\nyears have seen a surge of interest in integrating learned indexes\ninto LSM-tree systems. Dai et al. [7] are among the first to ex-\nplore this integration by applying a Piece-wise Linear Regression\n\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY Junfeng Liu, Jiarui Ye, Mengshi Chen, Meng Li, and Siqiang Luo\n(PLR) model to LevelDB [ 18], achieving significant improvements\nover traditional fence pointers. Similarly, Lu et al. [37] demonstrate\npromising results using the Recursive Model Index (RMI) within\nan LSM-tree framework. While these studies show the potential of\nlearned indexes in LSM-tree systems, they also prompt a natural\nquestion: are all recently proposed learned indexes well-suited for\nuse in LSM-tree architectures? Evaluating this is nontrivial. Given\nthe large and growing number of proposed learned index designs,\nimplementing all of them within a single system for empirical com-\nparison is infeasible. A more practical approach is to first identify\nthe distinctive storage characteristics of LSM-tree systems, such\nas the hierarchical level and immutable files, then classify learned\nindexes based on their compatibility with these characteristics.\nThis allows us to systematically determine which types of learned\nindexes are most appropriate for LSM-tree environments.\nIs there a universal guideline and configuration space for tun-\ning learned indexes in LSM-tree systems? While prior bench-\nmarks [ 27,41,54] have outlined practical principles for deploying\nlearned indexes in both in-memory and on-disk systems, tuning\nthem within LSM-tree systems remains significantly more complex.\nThis complexity arises from the presence of multiple interdepen-\ndent components, such as Bloom filters, caches, and write buffers,\nwhich all compete for the same limited memory budget and jointly\ninfluence both update and lookup performance. How to effectively\nallocate memory between learned indexes and these LSM-specific\ncomponents is still an open question. Furthermore, the configu-\nration space of learned indexes is highly diverse, with different\nmodels requiring different parameters and optimization strategies.\nWhile developing a universal tuning guideline would be highly\nvaluable, it is also inherently challenging due to the variability in\nindex structures and the interplay with system-level tradeoffs.\nMotivated by these questions, in this paper, we conduct a com-\nprehensive study on applying learned indexes in LSM-tree systems.\nOur contributions are concluded as the following:\nWe revisit several prevalent learned indexes and identify the\nkey factors that impact the compatibility between learned in-\ndexes and LSM-tree. We start with revisiting the existing learned\nindexes, summarizing their structures and underlying algorithms.\nNext, we classify these indexes based on their data layout to de-\ntermine which ones are most compatible with LSM-tree architec-\ntures. Particularly, some representative learned indexes, such as\nALEX [ 12] and LIPP [ 55], are primarily designed for in-memory\nlookups and involve less efficient pointer jumping, making their\ndata layout incompatible with the continuous and sorted struc-\nture of LSM-tree levels. A detailed categorization and compatibility\nstudy of learned indexes tailored for LSM-trees would offer valuable\ninsights for designing more suitable models for LSM-tree systems.\nWe identify three unified key parameters that affect perfor-\nmance across all compatible indexes and propose a compre-\nhensive configuration space for LSM-trees. Our theoretical\nanalysis reveals three core tuning parameters that significantly\ninfluence the performance of learned indexes in LSM-tree systems:\nindex type, position boundary, and index granularity. Index type\nrefers to the specific learned index model employed, each charac-\nterized by unique segment partitioning strategies and inner indexstructures, leading to different memory-performance tradeoffs. Po-\nsition boundary denotes the final search range that the LSM-tree\nretrieves from disk. This parameter directly affects I/O cost and\nis a crucial tuning knob for many learned index designs. Index\ngranularity determines the number of entries over which a learned\nindex is constructed, influencing both lookup accuracy and index\noverhead. These three parameters define a unified and inclusive\nconfiguration space that enables systematic experimentation and\nperformance evaluation. This framework provides a solid founda-\ntion for understanding and optimizing the integration of learned\nindexes in LSM-tree systems.\nWe develop a unified testbed LSM-tree system with a learned-\nindex-compatible interface that enables seamless integration\nand fair comparison of six representative indexes. In Section 4,\nwe detail the implementation of a universal interface that allows di-\nverse learned indexes to be easily integrated into LevelDB. We also\nillustrate how the three key parameters‚Äîindex type, position bound-\nary, and index granularity‚Äîaffect LSM-tree performance. Using this\nplatform, we conduct a comprehensive evaluation of six represen-\ntative learned indexes compatible with LSM-trees, testing them\nunder various configurations and datasets to assess their impact on\ncore system operations such as point lookups, range lookups, and\ncompaction. Our findings yield several important insights. First, all\nevaluated learned indexes exhibit a superior memory-latency trade-\noff compared to traditional fence pointers, offering significantly\nlower lookup latency for the same memory usage. This underscores\nthe potential of learned indexes in LSM-tree systems. Among the\ntested models, RMI, and PGM consistently demonstrate strong per-\nformance and dominate other approaches in most scenarios. We\nalso observe that the overhead introduced by model training and\nwrite-time indexing during compaction is modest relative to the\noverall compaction cost. Additionally, increasing index granularity\nreduces the number of learned indexes required, thereby lowering\noverall memory consumption. Beyond point queries, we further\nevaluate the effects of learned indexes on range lookups and mixed\nworkloads involving both reads and updates. Based on these ex-\nperiments, we identify three key design principles for integrating\nlearned indexes into LSM-trees:\n‚óèPosition boundary is the most critical factor for lookup perfor-\nmance. While the choice of index type mainly affects the memory-\nlatency tradeoff, prioritizing models with smaller error bounds\nunder a fixed memory budget provides greater benefits than\noptimizing internal index structures.\n‚óèIncreasing SSTable size improves lookup performance under a\nfixed memory budget by reducing index memory overhead and\nenabling the use of smaller position boundaries.\n‚óèMemory allocation exhibits diminishing returns: the performance\nimprovement from increasing the memory budget plateaus once\nthe segment size becomes smaller than or equal to the I/O block\nsize.\n‚óèThe construction and training of learned indexes introduce min-\nimal overhead compared to traditional fence pointers during\ncompaction.\n\n[Experiments & Analysis]\nEvaluating Learned Indexes in LSM-tree Systems: Benchmarks, Insights and Design Choices Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY\n(A) LSM -Tree StructureT times lagerT times lagerL1\nL2\nL33D\n1E\n2D4F\n2B 6A\n5D 7B 8EBuffer 6E9C\nBloom \nFilterFence \nPointer\n3\n1\n2\nDisk\n0134569AC0 4 9ùëÆùíÜùíï (ùüê)\nFence \nPointer\nBinary search \non {0,1,3}Binary search \non {0,4,9}\n0134569ACMMM\nPredict and search \nin error boundsùëÆùíÜùíï (ùüê)\nLearned \nIndexDetermine which \nmodels to use\n(B) Fence Pointer in LSM -tree (C) Learned Index in LSM -treeMemory\nFlush and \nMerge\nFigure 1: (A) presents the general structure of an LSM-tree; (B) and (C) illustrate how original fence pointer and learned indexes\nwork in an LSM-tree, respectively\n2 Background\nThis section discusses the background knowledge about the LSM-\ntree systems and the indexing schemes over it.\n2.1 Log-Structured Merge Trees\nAn LSM-tree efficiently manages data across multiple disk com-\nponents, organizing data into sorted arrays at different levels. It\nalso maintains an in-memory component, where recent updates\nare stored in a write buffer. To enhance lookup performance, each\nsorted array is associated with Bloom filters and indexes. The ca-\npacity of each level increases exponentially by a size ratio of ùëá,\nmeaning the total number of levels required to store ùëÅentries is\napproximately ùêø=[Ô∏ÇlogùëáùëÅ‚ãÖùëí\nùêπ‚åâÔ∏Ç, whereùêπis the size of the write\nbuffer, andùëíis the size of individual entries. Each level consists of\nkey-value pairs stored in a sorted array, referred to as a sorted run\nin some works. LSM-trees primarily support three operations:\nUpdates. Updates in an LSM-tree are initially written to the in-\nmemory write buffer. Once the buffer is full, the key-value entries\nare flushed to disk and merged into the sorted array at level-1,\nas shown in Figure 1 (A). If a level exceeds its capacity, a com-\npaction operation is triggered, merging entries into the next level\nto maintain efficient storage and query performance. Alternatively,\nto mitigate resource consumption spikes, some databases [ 14,18]\nperform partial compactions by merging only a subset of entries\ninto the next level, rather than compacting the entire level at once.\nIn these systems, a sorted run is divided into sorted files, known as\nSSTables , and only a subset of these SSTables is selected for merging\nduring each compaction.\nPoint Lookups. A point lookup searches for the value of a specific\nkey by checking levels sequentially until the key is found. To expe-\ndite this process, Bloom filters and indexes are employed. When a\nBloom filter indicates a potential match, the LSM-tree locates the\napproximate range within that level and performs a binary search\nto verify the key‚Äôs presence.\nRange Lookups. Range lookups collect entries from each LSM-tree\nlevel and use a sort-merge process to remove duplicates, returning\nonly the most recent values.\n2.2 Indexes in LSM-tree\nTraditional index structures in LSM-trees often rely on fence point-\ners, as shown in Figure 1(B). These pointers store the smallest (orlargest) key of a fixed range of key-value pairs, allowing the LSM-\ntree to quickly narrow down the search to a small data range when\nlocating a specific key. During compaction, the smallest or largest\nkey of each newly created data range is stored in memory. By\nquerying these index structures, the LSM-tree can skip unneces-\nsary searches through large amounts of data on disk, significantly\nreducing I/O operations and improving lookup efficiency.\nLearned Indexes in LSM-tree. Although fence pointers are widely\nused in most LSM-tree systems [ 14,18,29], there remains an oppor-\ntunity to improve memory efficiency by replacing them with more\nadvanced learned index structures. This potential arises for two key\nreasons: (1) The sorted arrays on disk are immutable, meaning they\nare only created and deleted during compactions, making them\nwell-suited for even non-updatable learned indexes, and (2) since\nthe entries are already stored in a naturally sorted order on disk,\nlearned indexes can efficiently map the data, potentially reducing\nthe overhead of sorting. As shown in Figure 1 (C), by training the\nlearned model during compactions, we can easily replace the fence\npointers with learned indexes.\nAbu-Libdeh et al. [2] were the first to evaluate the feasibility\nof using a linear regression model in LSM-tree systems, finding\na positive impact when replacing traditional fence pointers with\nlearned index structures. However, their study did not systemati-\ncally explore the performance variations across different types of\nlearned indexes or how various configurations might affect results.\nBuilding on this idea, Dai et al. integrated learned indexes into a\nkey-value separated LSM-tree system, Wisckey [ 38], developing\nBourbon [ 7], an LSM-tree system equipped with a piecewise linear\nlearned index. While Bourbon demonstrated notable performance\nimprovements, it still did not thoroughly investigate all possible\ndesign choices for learned indexes, such as experimenting with\ndifferent index types.\n3 Learned Indexes Revisited\nWe revisit eight learned indexes in detail and assess their compat-\nibility with LSM-tree systems. Broadly, we classify these indexes\ninto two categories based on their data layout: data-clustered in-\ndexes , such as FITing-Tree, PGM, and RMI, and data-unclustered\nindexes , including LIPP and ALEX. As illustrated in Figure 2, data-\nclustered indexes store key-value pairs in physically continuous\nblocks, whereas, as shown in Figure 3, data-unclustered indexes\n\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY Junfeng Liu, Jiarui Ye, Mengshi Chen, Meng Li, and Siqiang Luo\nKey Slope Intercept Key Slope Intercept ‚ãØ{ùëòùëíùë¶1,ùëòùëíùë¶2,‚ãØ,ùëòùëíùë¶ùëõ}Query Key\nBinary Search\nSorted keys \narray\nSpline Points01Radix Table\n(prefix bits=1)Query Key\nSpline PointsHist TreeQuery KeySegment Segment Segment Segment\n(A) Piece -wise Linear Model (B) FITING -tree\n(D) RadixSpline (E) PLEXKey Slope Intercept Key Slope Intercept ‚ãØB+-treeQuery Key\nSegment Segment Segment Segment\nSegment Segment Segment Segment Segment Segment Segment Segment(C) PGM -Index\n(F) Recursive Model Index (RMI)K   Sl   Ic\nK   Sl   Ic K   Sl   Ic K   Sl   Ic\nK   Sl   Ic K   Sl   Ic K   Sl   Ic ‚ãØQuery Key\nSegment Segment Segment Segment\nModel 1.1\nModel 2.1 Model 2.2‚ãØ Model 2.xQuery Key\nSegment Segment Segment SegmentSl   IcSl   Ic Sl   IcSl   Ic Sl   IcSl   Ic Sl   IcSl   Ic\nFigure 2: (A) to (F) present the data structures of data-clustered learned indexes and the general lookup procedure. Ic represents\nthe intercept of the linear model, Sl is the slope, and K is short for the key.\ndo not. Instead, data-unclustered indexes require additional steps,\nsuch as traversing via pointers, to retrieve continuous key-value\npairs. In the following sections, we first review the structures of the\nmost representative data-clustered and data-unclustered indexes.\nWe then analyze which indexes are most compatible with LSM-tree\nsystems, followed by a theoretical analysis of the cost associated\nwith each index.\n3.1 Data Clustered Indexes\nIn this subsection, we review six well-known data-clustered learned\nindexes and their respective lookup procedures.\nPiece-wise Linear Regression (PLR) [ 7], shown in Figure 2 (A),\nuses a greedy algorithm to divide a sorted array into segments\nbased on a specified error bound, which represents the maximum\nallowable difference between the estimated and actual key posi-\ntions. For each segment, PLR builds a linear model to predict the\napproximate position of a key. During a lookup, PLR first locates the\nsegment containing the key by performing a binary search over the\nsegments. It then uses the corresponding linear model to estimate\nthe key‚Äôs position, denoted as appx_pos . Since the linear model\nensures that the true key position lies within a range of [ appx_pos\n- error ,appx_pos + error ]‚Äìwhere the error reflects the prediction\ntolerance‚Äìa binary search is performed within this range to find\nthe exact key location.\nFITing-Tree [ 16], shown in Figure 2 (B), uses a greedy algorithm\nto divide a sorted array into segments similarly based on a specified\nerror bound. A linear model is built for each segment to predict\nthe position of keys. To efficiently search through these segments,\nFITing-Tree uses a B+-tree to index the segments. When looking up\na key, FITING-tree first traverses the B+-tree to locate the segment\ncontaining the key. It then uses the corresponding linear model to\npredict the approximate position. Since the linear model ensures\nthat the true key lies within the range [ appx_pos - error ,appx_pos +error ], a binary search is performed within this range to find the\nexact key location.\nPiecewise Geometric Model index (PGM) [ 15], shown in Fig-\nure 2 (C), takes a different approach by using a streaming algorithm,\nrather than a greedy one, to divide the array into segments and\nbuild linear models with a given error bound. PGM further applies\nthis streaming algorithm recursively to construct parent nodes and\nbuild linear models for these higher-level nodes. To look up a key,\nPGM predicts an approximate position appx_pos using its model,\nthen recursively performs a binary search within [ appx_pos - error ,\nappx_pos + error ] until the exact key is found in the leaf node.\nRadixSpline (RS) [ 26], shown in Figure 2 (D), selects a subset\nof keys from the sorted array as spline points and uses linear in-\nterpolation models to estimate the positions of keys between any\ntwo spline points. RadixSpline ensures the accuracy of its spline\nlayer by imposing error bounds on the approximations. If the error\nexceeds a predefined threshold, additional spline points are added\nto improve the approximation. To index these spline points, RadixS-\npline constructs a radix table. When looking up a key, RadixSpline\nfirst uses the radix table to locate the correct spline segment, then\napplies the linear interpolation model to predict the key‚Äôs approxi-\nmate position. A binary search is then performed within the range\n[appx_pos - error ,appx_pos + error ] to locate the exact key.\nPractical Learned Index (PLEX) [ 51], shown in Figure 2 (E), is an\nimproved version of RadixSpline that employs a hierarchical Hist\nTree (or Radix Tree) to index spline points and reduce search space.\nLike RadixSpline, PLEX uses spline points and linear interpolation\nmodels to estimate key positions but improves lookup efficiency\nby leveraging this hierarchical structure. A key feature of PLEX is\nits self-tuning capability, which dynamically adjusts the number\nof spline points based on data distribution and workload. This\noptimization ensures a balance between prediction accuracy and\nmemory usage, improving overall performance. During lookup,\nPLEX first uses the Hist Tree to locate the segment containing the\n\n[Experiments & Analysis]\nEvaluating Learned Indexes in LSM-tree Systems: Benchmarks, Insights and Design Choices Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY\nMQuery Key\n(A) ALEXData NodeInner NodeData\nNode\nNull\n(B) LIPPM MMM\nMQuery Key\nM\nFigure 3: (A) and (B) present the structure of ALEX and LIPP.\nThe data (key-value pairs) are not stored continuously to\naccommodate incoming new entries.\nkey, then applies the corresponding linear model to predict the\napproximate position. A binary search is then performed within\n[appx_pos - error ,appx_pos + error ] to find the exact key. With its\nhierarchical structure and self-tuning, PLEX efficiently adapts to\nlarge datasets and varying workloads, offering better scalability\nthan RadixSpline.\nRecursive Model Index (RMI) [ 28], shown in Figure 2 (F), is a\nlearned index that recursively applies machine learning models to\napproximate the position of keys in a sorted array. It organizes mod-\nels in a hierarchical structure, where upper-level models predict\nthe position of keys for the next layer, progressively refining the\nprediction until the lowest layer estimates the key‚Äôs position. RMI\nis built in a top-down manner, where the top-level model is trained\nfirst to give a coarse estimate of key positions. Based on this, the\ndataset is divided, and lower-level models are trained on smaller\nsubsets, improving accuracy as you move down the hierarchy. This\napproach allows RMI to tailor the complexity of each model to the\nportion of the data it handles, optimizing both performance and\nmemory usage. During lookup, RMI first uses the top-level model\nto make a rough prediction, then refines this through subsequent\nlayers. The final model predicts the approximate key position, fol-\nlowed by a binary search within a small range to find the exact\nkey. RMI‚Äôs error is not predefined by the user but rather recorded\nduring the training process, adapting to the data‚Äôs characteristics.\n3.2 Data Unclustered Indexes\nALEX [ 12], shown in Figure 3 (A), uses two types of nodes: inner\nnodes and data nodes, both combining arrays with linear models\nto predict key positions. Inner nodes contain an array of pointers\nto other nodes, while data nodes use gapped arrays that store key-\nvalue pairs, interleaving empty slots to support efficient insertions.\nDuring a lookup, ALEX traverses from the root through inner nodes,\nusing the model to predict the appropriate child node. Once at a\ndata node, the model predicts the position of the key, followed by\nan exponential search to locate the exact key. ALEX dynamically\nadjusts its structure by splitting or merging nodes as data evolves,\nensuring efficient handling of both lookups and updates while\nmaintaining optimal performance for dynamic workloads.\nLIPP [ 55], shown in Figure 3 (B), uses a linear model in each\nnode to precisely predict key positions. Each node contains a data\narray and a bitmap to distinguish between three slot types: DATA,\nNULL, and NODE. The linear model predicts which slot shouldbe accessed during a lookup. LIPP employs the Fastest Minimum\nConflict Degree (FMCD) algorithm to minimize conflicts (multiple\nkeys mapped to the same slot). When a key is inserted into a NULL\nslot, it is marked as DATA and stores the key-value pair. If inserted\ninto an occupied DATA slot, the slot becomes a NODE, pointing\nto a new child node created with the conflicting keys. The child\nnode is built using the same algorithm. During lookup, LIPP uses\nthe root node‚Äôs linear model to predict the key‚Äôs position. If the\npredicted slot is NODE, it follows the pointer to the child node and\nrepeats the process. If the slot is DATA, it checks for a match and\nreturns the key-value pair if found.\nDILI [ 34]uses a two-phase approach to build the index: a bottom-\nup tree-building process using linear regression models based on\nglobal and local key distributions, followed by a top-down refine-\nment where the fanout of internal nodes is customized according\nto local key distributions. This design strikes a balance between\nthe number of leaf nodes and the tree height, both crucial factors\nin minimizing key search time. Additionally, DILI includes flexible\nalgorithms for efficient key insertion and deletion, allowing the\nindex to dynamically adjust its structure when necessary.\nNFL [ 56]introduces a new approach to addressing the challenges of\nlearned indexes by transforming complex key distributions before\nconstructing the index. NFL uses a two-stage framework: first, it\napplies Numerical Normalizing Flow (Numerical NF) to transform\nthe key distribution into a near-uniform one. Then, it builds a\nlearned index using a specialized After-Flow Learned Index (AFLI),\noptimized for the normalized data.\n3.3 LSM-Compatible Indexes\nThough data-unclustered indexes offer excellent read performance\nand memory efficiency, we argue that data-clustered indexes are\na more suitable choice for LSM-trees. This is because LSM-trees\nrely on the sorted and contiguous storage of data across levels (i.e.,\nSSTables and sorted runs) to optimize read and write performance.\nData-clustered indexes naturally maintain this physical continuity,\nallowing them to replace existing fence pointers with minimal en-\ngineering effort. By contrast, data-unclustered indexes like ALEX\nand LIPP disrupt the current data layout (i.e., SSTable), making in-\ntegration with LSM-trees more complex. Moreover, range lookups,\na critical operation in LSM-trees, benefit from the sequential access\nprovided by data-clustered indexes. These indexes ensure fast ac-\ncess to key-value pairs stored contiguously, while data-unclustered\nindexes, which scatter data, require additional memory and disk\njumps, significantly increasing the overhead of such queries.\nWhile integrating data-unclustered indexes into LSM-trees is a\npromising and intriguing topic, this integration is not feasible with-\nout significantly altering the widely recognized LSM-tree storage\narchitecture for the following reasons:\n(1)Need for non-compact and uncontinous storage layout re-\nplacement: Successful integration would necessitate replac-\ning the current compact data layout ( i.e., SSTables) with a\ndiscontinous data-unclustered structures, which represents\na considerable undertaking.\n(2)Re-Implementation of Basic Functions: Basic functions, such\nas range lookup and compaction iterators, would need to be\nre-implemented when using data-unclustered indexes.\n\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY Junfeng Liu, Jiarui Ye, Mengshi Chen, Meng Li, and Siqiang Luo\nLearnedIndexTable  Interface\nMeta & LogsData Files\nùë≥ùíä ‚ãØSeg 1Inner Index\nExecuted Workload\nIndex Type\nPosition Boundary\nIndex GranularityConfiguration  Space\nDetermine size of fileDivide segs.Query Interface (e.g., Get/Put/ NewIter‚Ä¶)\nBuffer\nGet NewIter BuildTableùíäùüè\nùíäùüê\nùíäùüëNewLeveIterNewDBIter\nBloom FiltersGet Key\nSort MergeCompact\nFlushPut Get NewIter\nSeg 2 Seg 3 Seg n ‚Ä¶‚Ä¶Build Index \nfor segmentsInput\nFigure 4: The figure demonstrates the architecture of our testbed system. The left-hand side is the detailed conduction of\ndifferent operations to test while the right-hand side is how the three configuration impact the system.\nGiven these considerations, this paper primarily focuses on the\nperformance of data-clustered indexes within existing LSM-tree\nsystems, while leaving the exploration of data-unclustered indexes\nfor future research.\n4 Exploring the Configuration Space of\nIndexing in LSM-trees\nIn this section, we are going to analyze the cost of LSM-compatible\nindexes to identify the tuning options that affect the read perfor-\nmance and memory consumption when applied to LSM-tree sys-\ntems.\n4.1 Configuration Space\nTo begin with, data-clustered indexes locate a key by first identi-\nfying the segment likely to contain it, then reading that segment\nfrom disk, and finally searching within it under a bounded error\nrange, as illustrated in Figure 4. The cost of the first step depends\non the type of index used, since different index structures organize\nsegment metadata differently. The second cost, related to I/O, is\nprimarily determined by the error bound. Data-clustered indexes\nensure that segment lengths do not exceed 2√óthe error bound ( ùúñ),\nand thus, the I/O cost would not exceed ùëÇ{Ô∏É2ùúñ\nùêµ}Ô∏É, whereùêµis the size\nof an I/O block. The final cost stems from the in-segment search,\ntypically performed using binary search, and is also governed by\nthe error bound. Based on this process, the two most critical factors\nto tune are:\nIndex Type. The choice of index structure has a direct impact\non lookup speed and memory overhead. For example, FITing-tree\nuses a B+-tree to index segments, which offers faster lookups but\nincurs higher memory consumption than simpler alternatives like\nthe sorted arrays used in PLR. Thus, selecting the appropriate index\ntype involves balancing performance and memory usage.\nPosition Boundary. Once the approximate segment is identified,\nthe LSM-tree must read it from disk. We define the ‚Äúposition bound-\nary‚Äù as the range length of this segment, which is usually 2√ótheerror bound in the learned indexes. A smaller position boundary re-\nduces I/O cost, which is often the dominant overhead in LSM-trees,\nbut increases the number of segments, which raises memory usage.\nTuning this boundary is therefore crucial for achieving an optimal\ntradeoff between I/O efficiency and memory consumption.\nBeyond these two main factors, Index Granularity also plays an\nimportant role in determining the performance of learned indexes\nwithin LSM-tree systems. Modern LSM-tree implementations (e.g.,\nLevelDB, RocksDB, PebblesDB [ 14,18,46]) often apply partial com-\npaction strategies, where sorted runs are divided into multiple files\n(SSTables), and only some are compacted into the next level. In such\ncases, learned indexes are typically built at the SSTable level, mean-\ning their position boundary is bounded by the SSTable size. Dai et\nal.[7] suggest that coarser-grained indexing‚Äîsuch as level-grained\nmodels like LevelModel‚Äîcan yield performance improvements of\naround 10% under read-heavy workloads. To examine this claim,\nwe evaluate the performance of learned indexes built at varying\nSSTable sizes, as well as those constructed across entire levels in-\nstead of individual files.\nConfiguration Tradeoff. Selecting an index type entails a tradeoff\nbetween memory consumption and lookup latency. To fairly evalu-\nate index types, it is important to compare them under consistent\nconfigurations. Position boundary is likely the most influential pa-\nrameter for read performance due to its impact on I/O cost. While\nreducing position boundaries can improve I/O efficiency, this often\nleads to increased memory usage by creating more, smaller seg-\nments. The memory overhead introduced by this tradeoff remains\nan open question and warrants empirical evaluation. Moreover,\nalthough increasing SSTable size (i.e., index granularity) reduces\nthe number of indexes, it may also lead to less efficient compactions\nby increasing the volume of data processed per compaction round.\nTherefore, careful tuning of index granularity is also essential for\noptimizing read and compaction performance.\n\n[Experiments & Analysis]\nEvaluating Learned Indexes in LSM-tree Systems: Benchmarks, Insights and Design Choices Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY\n0.00.20.40.60.81.0ProportionRandom\n0.00.20.40.60.81.0ProportionSegment\n0.00.20.40.60.81.0ProportionLongitude\n0.00.20.40.60.81.0ProportionLonglat\n0.00.20.40.60.81.0ProportionBooks\n0.00.20.40.60.81.0Proportion\n0.00.20.40.60.81.0ProportionWiki FB\nFigure 5: The CDF of different datasets\n4.2 Implementation\nTo systematically evaluate the effectiveness of the three aforemen-\ntioned configurations, we build a benchmark system based on Lev-\nelDB, a well-known and streamlined LSM-tree implementation. To\nintegrate learned indexes without disrupting the system‚Äôs core func-\ntionality, we implement a new class, LearnedIndexTable , which\ninherits from and replaces the original Table class. We override\nthree key functions: InternalGet (Get), NewIterator (NewIter),\nandTableBuilder (BuildTable). Below, we describe each imple-\nmentation in detail and explain how these functions interact with\nthe rest of the system.\nGet. TheInternalGet function handles point lookups by locating\na specific key in a data file. In our implementation, key-value pairs\nare sorted and stored in segments, each indexed by a learned model.\nTo perform a lookup, the learned index first consults its internal\nmodel to identify the corresponding segment. The segment is then\nfetched from disk using the Linux pread interface, and a binary\nsearch is conducted within the segment to retrieve the target key.\nNewIter. The iterator interface is essential in LSM-tree systems,\nsupporting both range queries and compaction. Our learned index\niterator begins by seeking to a target key using the same procedure\nasInternalGet , and then proceeds to iterate over subsequent key-\nvalue pairs within the segment. Once all entries in the current\nsegment are exhausted, the iterator advances to the next segment.\nBuildTable. The TableBuilder interface is responsible for con-\nstructing learned-index-based tables. During flushes or compactions,\nthe builder receives sorted key-value pairs and constructs a learned\nindex over them, which is similar to how traditional fence pointers\nare built in baseline LevelDB. Additionally, the original on-disk\nSSTable format is replaced by the LearnedIndexTable format, in\nwhich the inner index and data segments are serialized separately,\nwith their offsets recorded in the file header.\nSpecifically, by using the interface, some commonly used LSM-\ntree processes and operations are implemented as the following:\nCompaction. The process of building learned indexes is similar\nto that of fence pointers: when an new table is created through a\nflush or compaction, the sorted key-value pairs are used to train the\ncorresponding learned index. The index is then serialized, written\nto disk, and marked as ‚Äúlearned‚Äù. Once the table is complete, the\nlearned index is linked to it.\nPoint Lookup. When a file is marked as ‚Äúlearned‚Äù, LevelDB first\nlocates the table containing the key and accesses the learned index\nto get a prediction for the target key. An I/O operation is thenperformed to fetch the approximate segment, followed by a binary\nsearch to retrieve the exact key.\nRange Lookup. A range lookup involves two phases: (1) identify-\ning the starting point of the range at each level, and (2) retrieving\nthe key-value pairs at each level and merging them until the entire\nrange is fetched. The first phase is similar to a point lookup, involv-\ning access to the learned index and a binary search. The second\nphase differs slightly from LevelDB‚Äôs default behavior. We retrieve\none I/O block at a time for the starting segment (typically 4096B) for\nall indexes until all key-value pairs in the target range are fetched\nfrom disk.\nWe integrate the following baselines by implementing the above\ninterface into our system. The implementations of PGM1, RadixS-\npline2, PLEX3, and PLR4[7] are based on versions released by the\nrespective authors. For RMI and FITing-Tree, as no suitable C++\nversions are available, we used the RMI implementation from a\nbenchmark paper5[40] and the FITing-Tree6implementation from\nthe SOSD benchmark [42].\n5 Evaluation\nExperiment Design. In this section, we evaluate the performance\nof learned indexes across various scenarios. First, we assess the\nmemory-latency tradeoff in lookups to determine the effectiveness\nof replacing fence pointers with learned indexes and to identify\nthe key factors affecting lookup performance. Next, we address a\ncommon concern about the additional training time introduced dur-\ning compaction by running a write-only workload and measuring\ncompaction time. Following this, we evaluate unique features of\nLSM-tree systems, such as the asymmetric read overhead across\ndifferent levels and the impact on range lookups. Finally, we test the\nperformance under mixed workloads using the YCSB benchmark\nto assess their effectiveness in real-world applications.\nDatasets and Workload Setup We evaluate all the indexes on\nseven different datasets generated by the SOSD benchmark [ 42]:\nRandom, Segment, Longitude, Longlat, Books, FB, and Wiki with\ntheir cumulative distributions shown in Figure 5 (a). Due to space\nconstraints, we present results for the Random dataset in this paper.\nFor a complete set of results on all datasets, please refer to our\ntechnical report [ 1]. Each dataset consists of 6.4 million key-value\npairs, with 24-byte keys and 1000-byte values. In each experiment,\nwe perform 1,000,000 operations.\nRunning Environment. We conduct our experiments on a ma-\nchine running Ubuntu 22.04, equipped with an Intel Core i9-13900K\nCPU (36 MB L3 cache), 128 GB of memory, and a 2 TB NVMe SSD.\nAll learned indexes are integrated into LevelDB, with the LSM-tree\nconfigured to use a leveling compaction policy, a size ratio of 10,\nand a 10-bit-per-key Bloom filter.\nSettings of Learned Indexes. To evaluate the performance of\nfence pointers under different position boundaries, we adjust the\ndata block size in LevelDB to generate varying numbers of fence\npointers (abbr. FP) . For PLR, FITing-Tree (abbr. FT) , and PLEX, we\n1https://github.com/gvinciguerra/PGM-index\n2https://github.com/learnedsystems/RadixSpline\n3https://github.com/stoianmihail/PLEX\n4We contacted the author to obtain the code\n5https://github.com/BigDataAnalyticsGroup/analysis-rmi\n6https://github.com/RKolla99/FITing-Tree\n\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY Junfeng Liu, Jiarui Ye, Mengshi Chen, Meng Li, and Siqiang Luo\n/uni00000015/uni00000018/uni00000019 /uni00000014/uni00000015/uni0000001b /uni00000019/uni00000017 /uni00000016/uni00000015 /uni00000014/uni00000019 /uni0000001b\n/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000013/uni00000015/uni00000013/uni00000055/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050\n/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000058/uni00000056/uni0000000c/uni00000029/uni00000048/uni00000051/uni00000046/uni00000048/uni00000033/uni00000052/uni0000004c/uni00000051/uni00000057/uni00000048/uni00000055 /uni00000029/uni0000002c/uni00000037/uni0000004c/uni00000051/uni0000004a/uni00000010/uni00000037/uni00000055/uni00000048/uni00000048 /uni00000033/uni0000002f/uni00000035 /uni00000033/uni0000002f/uni00000028/uni0000003b /uni00000035/uni00000036 /uni00000035/uni00000030/uni0000002c /uni00000033/uni0000002a/uni00000030\n/uni00000015/uni00000018/uni00000019 /uni00000014/uni00000015/uni0000001b /uni00000019/uni00000017 /uni00000016/uni00000015 /uni00000014/uni00000019 /uni0000001b\n/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000014/uni00000013/uni00000018/uni00000014/uni00000013/uni0000001a/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni00000025/uni0000000c\n/uni00000015/uni00000018/uni00000019 /uni00000014/uni00000015/uni0000001b /uni00000019/uni00000017 /uni00000016/uni00000015 /uni00000014/uni00000019 /uni0000001b\n/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000013/uni00000015/uni00000013/uni00000056/uni00000048/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057\n/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000058/uni00000056/uni0000000c\n/uni00000015/uni00000018/uni00000019 /uni00000014/uni00000015/uni0000001b /uni00000019/uni00000017 /uni00000016/uni00000015 /uni00000014/uni00000019 /uni0000001b\n/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000014/uni00000013/uni00000018/uni00000014/uni00000013/uni0000001a/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni00000025/uni0000000c\n/uni00000015/uni00000018/uni00000019 /uni00000014/uni00000015/uni0000001b /uni00000019/uni00000017 /uni00000016/uni00000015 /uni00000014/uni00000019 /uni0000001b\n/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000013/uni00000015/uni00000013/uni0000004f/uni00000052/uni00000051/uni0000004a/uni0000004c/uni00000057/uni00000058/uni00000047/uni00000048\n/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000058/uni00000056/uni0000000c\n/uni00000015/uni00000018/uni00000019 /uni00000014/uni00000015/uni0000001b /uni00000019/uni00000017 /uni00000016/uni00000015 /uni00000014/uni00000019 /uni0000001b\n/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000014/uni00000013/uni00000018/uni00000014/uni00000013/uni0000001a/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni00000025/uni0000000c\n/uni00000015/uni00000018/uni00000019 /uni00000014/uni00000015/uni0000001b /uni00000019/uni00000017 /uni00000016/uni00000015 /uni00000014/uni00000019 /uni0000001b\n/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000013/uni00000015/uni00000013/uni0000004f/uni00000052/uni00000051/uni0000004a/uni0000004f/uni00000044/uni00000057\n/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000058/uni00000056/uni0000000c\n/uni00000015/uni00000018/uni00000019 /uni00000014/uni00000015/uni0000001b /uni00000019/uni00000017 /uni00000016/uni00000015 /uni00000014/uni00000019 /uni0000001b\n/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000014/uni00000013/uni00000018/uni00000014/uni00000013/uni0000001a/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni00000025/uni0000000c\n/uni00000015/uni00000018/uni00000019 /uni00000014/uni00000015/uni0000001b /uni00000019/uni00000017 /uni00000016/uni00000015 /uni00000014/uni00000019 /uni0000001b\n/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000013/uni00000015/uni00000013/uni00000045/uni00000052/uni00000052/uni0000004e/uni00000056\n/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000058/uni00000056/uni0000000c\n/uni00000015/uni00000018/uni00000019 /uni00000014/uni00000015/uni0000001b /uni00000019/uni00000017 /uni00000016/uni00000015 /uni00000014/uni00000019 /uni0000001b\n/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000014/uni00000013/uni00000018/uni00000014/uni00000013/uni0000001a/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni00000025/uni0000000c\n/uni00000015/uni00000018/uni00000019 /uni00000014/uni00000015/uni0000001b /uni00000019/uni00000017 /uni00000016/uni00000015 /uni00000014/uni00000019 /uni0000001b\n/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000013/uni00000015/uni00000013/uni00000049/uni00000045\n/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000058/uni00000056/uni0000000c\n/uni00000015/uni00000018/uni00000019 /uni00000014/uni00000015/uni0000001b /uni00000019/uni00000017 /uni00000016/uni00000015 /uni00000014/uni00000019 /uni0000001b\n/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000014/uni00000013/uni00000018/uni00000014/uni00000013/uni0000001a/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni00000025/uni0000000c\n/uni00000015/uni00000018/uni00000019 /uni00000014/uni00000015/uni0000001b /uni00000019/uni00000017 /uni00000016/uni00000015 /uni00000014/uni00000019 /uni0000001b\n/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000013/uni00000015/uni00000013/uni0000005a/uni0000004c/uni0000004e/uni0000004c\n/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000058/uni00000056/uni0000000c\n/uni00000015/uni00000018/uni00000019 /uni00000014/uni00000015/uni0000001b /uni00000019/uni00000017 /uni00000016/uni00000015 /uni00000014/uni00000019 /uni0000001b\n/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000014/uni00000013/uni00000018/uni00000014/uni00000013/uni0000001a/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni00000025/uni0000000c\nFigure 6: Latency and memory usage of different indexes under different position boundary under different datasets.\ndirectly vary the error bounds to control the position boundaries.\nFor RMI, we follow the guidelines in [ 40] and use their RMILabs\nimplementation, as recommended in the paper. This setup uses a\ntwo-level model tree. To vary the position boundary, we adjust the\nsize of the second level, which in turn affects the position boundary.\nFor both RadixSpline (abbr. RS) and PGM, the error bounds can\nbe adjusted to control the position boundaries. However, since\nboth have additional parameters that affect their internal structure,\nthese must also be fine-tuned for optimal performance. In PGM,\ntheEpsilonRecursive parameter defines the error bound for internalnodes. We test various values, and find that EpsilonRecursive has\nlittle impact on PGM‚Äôs performance in LSM-tree systems. Therefore,\nwe retain the default setting of ùê∏ùëùùë†ùëñùëôùëúùëõùëÖùëíùëêùë¢ùëüùë†ùëñùë£ùëí =4. For RS, the\nRadixBits parameter controls the size of its radix table. After varying\nthis value, we determine that ùëÖùëéùëëùëñùë•ùêµùëñùë°ùë† =1offers the best tradeoff\nin LSM-tree systems, reducing memory usage while maintaining\nsatisfactory performance.\n\n[Experiments & Analysis]\nEvaluating Learned Indexes in LSM-tree Systems: Benchmarks, Insights and Design Choices Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY\n/uni00000029/uni00000033 /uni00000029/uni00000037/uni00000033/uni0000002f/uni00000035/uni00000033/uni0000002f/uni00000028/uni0000003b /uni00000035/uni00000036/uni00000035/uni00000030/uni0000002c /uni00000033/uni0000002a/uni00000030/uni00000015/uni00000016\n/uni00000015/uni00000014\n/uni00000015/uni00000014/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000058/uni00000056/uni0000000c\n/uni0000000b/uni00000024/uni0000000c/uni00000003/uni0000002c/uni00000051/uni00000047/uni00000048/uni0000005b/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048/uni0000002c/uni00000032 /uni00000033/uni00000055/uni00000048/uni00000047 /uni00000025/uni00000036\n/uni0000001b /uni00000016/uni00000015 /uni00000014/uni00000015/uni0000001b/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000033/uni00000055/uni00000048/uni00000047/uni0000004c/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000058/uni00000056/uni0000000c\n/uni0000000b/uni00000025/uni0000000c/uni00000003/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000029/uni00000033\n/uni00000029/uni00000037/uni00000033/uni0000002f/uni00000035\n/uni00000033/uni0000002f/uni00000028/uni0000003b/uni00000035/uni00000036\n/uni00000035/uni00000030/uni0000002c/uni00000033/uni0000002a/uni00000030\nFigure 7: Query time breakdown\n5.1 Analysis on Position Boundary\nObservation 1 : Smaller position boundary positively reduces\nthe latency for all indexes at the cost of increasing the memory\nusage. The efficiency of improving performance by adding mem-\nory budget varies from indexes, indicating the memory-latency\ntradeoff is very different.\nWe vary the position boundary from 256 to 8 for each index and\nevaluate their performance under a point lookup workload consist-\ning of 1,000,000 queries. We record both the lookup latency and\nmemory usage, as shown in Figure 6 (A) and (B). Overall, decreas-\ning the position boundary reduces lookup latency but increases\nmemory consumption.\nWhen the position boundary is held constant, all indexes exhibit\nnearly identical lookup latencies. This suggests that I/O dominates\nthe cost of point lookups, while the time spent accessing the in-\nner index and searching within a segment is relatively minor. As\nillustrated in Figure 7(A) and (B), the I/O time required to fetch the\nsegment from disk is approximately 10 times greater than the com-\nbined time for model prediction (including inner index access) and\nbinary search within the segment. Although model prediction time\nincreases slightly as the position boundary decreases (Figure 7(B)),\nthis increase is outweighed by the I/O reduction, resulting in an\noverall latency improvement for all indexes.\nDespite similar lookup performance, memory usage varies sig-\nnificantly across index types. Traditional fence pointers exhibit the\nworst memory-latency tradeoff: as the position boundary decreases,\ntheir memory usage grows rapidly compared to learned indexes.\nThis aligns with one of the core design goals of learned indexes‚Äîto\nreduce memory overhead. However, the specific design of a learned\nindex also impacts its memory-latency tradeoff. For instance, while\nFITing-Tree outperforms fence pointers, its memory consumption\nstill increases quickly. This is due to its use of a B+-tree structure,\nwhich improves prediction accuracy at the cost of additional mem-\nory. RadixSpline (RS) and PLEX demonstrate comparable tradeoffs;\nboth divide sorted data using spline points and use a radix table (RS)\nor radix tree (PLEX) to index those points. Although PLEX incorpo-\nrates a self-tuning mechanism to optimize radix tree performance,\nthis benefit is less pronounced in LSM-tree systems. This is likely\nbecause PLEX‚Äôs optimizations are most effective under skewed\nkey distributions, which are uncommon in LSM-trees‚Äîespecially\nat levels with fewer keys. PLR shows a memory-latency tradeoff\nsimilar to RS and PLEX, largely due to its lightweight inner index\nstructure for leaf segments, which helps conserve memory. Amongall the evaluated learned indexes, PGM and RMI achieve the best\nmemory-latency tradeoffs. PGM employs an optimized segment-\ning algorithm that reduces the number of segments needed for a\ngiven error bound, thus lowering memory usage. RMI stands out\nfor its flexible configuration: by setting a large second-level index,\nit can achieve extremely low error bounds (as small as 1), enabling\nhigh precision with minimal memory. In contrast, other indexes\nare constrained by their minimal achievable error bounds, which\nlimits their precision under fixed memory budgets.\nObservation 2 : The improvement in read performance becomes\nincreasingly marginal as the memory budget grows, suggesting\nthat allocating additional memory to the index may not always\nyield proportional benefits to the system.\nWhile increasing the memory budget for indexes generally en-\nhances performance, we observe diminishing returns as index size\ncontinues to grow. As shown in Figure 6(A), lookup latency de-\ncreases substantially when the position boundary is reduced from\n256 to 128 and then to 64. However, beyond this point, further\nreductions in the position boundary yield little to no performance\nimprovement for most learned indexes, even though their memory\nusage grows exponentially, as shown in Figure 6(B). This phenome-\nnon occurs because, initially, additional memory allows the index\nto model the key space with higher precision, effectively narrowing\nthe search range and reducing I/O cost. But once the precision\nreaches the granularity of one or two I/O blocks, the dominant\ncost‚Äîdisk I/O‚Äîcan no longer be significantly reduced. At this stage,\nfurther memory investment offers minimal performance gain.\nAs a result, the growth in memory consumption does not trans-\nlate into a proportional improvement in lookup latency. This trend\nmirrors findings from prior benchmarks on learned indexes in in-\nmemory systems [ 42], underscoring a fundamental characteristic\nof the memory-latency tradeoff inherent to learned index designs.\n5.2 Impact of Index Granularity\nObservation 3 : Learned indexes consume less memory as the\ngranularity grows while it does not significantly affect the query\nperformance.\nTo evaluate the impact of index granularity, we vary the SSTable size\nfrom 8MB to 128MB and also include the level-granularity model\nproposed by Dai et al. [7]. We run a point lookup-only workload\nwith 1,000,000 queries and record the latency and memory usage of\neach index configuration. As shown in Figure 8 (rightmost), lookup\nlatency remains largely unaffected by granularity, varying by only a\nfew microseconds across all configurations. In contrast, granularity\nhas a significant impact on memory usage: increasing SSTable size\n(i.e., coarser granularity) leads to substantial memory savings, with\nover a 10 √óreduction in memory usage when moving from 8MB\nSSTables to the level model.\nSpecifically, when the position boundary is greater than 64, mem-\nory usage decreases exponentially as SSTable size increases. This is\nbecause larger SSTables reduce the total number of SSTables in the\nsystem (for a fixed dataset size), thereby lowering the number of\n\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY Junfeng Liu, Jiarui Ye, Mengshi Chen, Meng Li, and Siqiang Luo\n/uni0000001b/uni00000014/uni00000019 /uni00000016/uni00000015 /uni00000019/uni00000017/uni00000014/uni00000015/uni0000001b /uni0000002f\n/uni00000036/uni00000036/uni00000037/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000025/uni0000000c/uni00000014/uni00000013/uni00000017/uni00000014/uni00000013/uni00000018/uni0000002c/uni00000051/uni00000047/uni00000048/uni0000005b/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048/uni00000003/uni0000000b/uni00000025/uni0000000c\n/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000003/uni00000014/uni00000015/uni0000001b\n/uni0000001b/uni00000014/uni00000019 /uni00000016/uni00000015 /uni00000019/uni00000017/uni00000014/uni00000015/uni0000001b /uni0000002f\n/uni00000036/uni00000036/uni00000037/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000025/uni0000000c/uni00000014/uni00000013/uni00000017/uni00000014/uni00000013/uni00000018/uni00000014/uni00000013/uni00000019\n/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000003/uni00000019/uni00000017\n/uni0000001b/uni00000014/uni00000019 /uni00000016/uni00000015 /uni00000019/uni00000017/uni00000014/uni00000015/uni0000001b /uni0000002f\n/uni00000036/uni00000036/uni00000037/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000025/uni0000000c/uni00000014/uni00000013/uni00000018/uni00000014/uni00000013/uni00000019\n/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000003/uni00000016/uni00000015/uni00000029/uni00000037 /uni00000033/uni0000002f/uni00000035 /uni00000033/uni0000002f/uni00000028/uni0000003b /uni00000035/uni00000036 /uni00000035/uni00000030/uni0000002c /uni00000033/uni0000002a/uni00000030\n/uni0000001b/uni00000014/uni00000019 /uni00000016/uni00000015 /uni00000019/uni00000017/uni00000014/uni00000015/uni0000001b /uni0000002f\n/uni00000036/uni00000036/uni00000037/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000025/uni0000000c/uni00000014/uni00000013/uni00000018/uni00000014/uni00000013/uni00000019/uni00000014/uni00000013/uni0000001a\n/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000003/uni00000014/uni00000019\n/uni00000029/uni00000037 /uni00000033/uni0000002f/uni00000035 /uni00000033/uni0000002f/uni00000028/uni0000003b /uni00000035/uni00000036 /uni00000035/uni00000030/uni0000002c /uni00000033/uni0000002a/uni00000030/uni00000013/uni00000014/uni00000013\n/uni0000002f/uni00000052/uni00000052/uni0000004e/uni00000058/uni00000053/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000058/uni00000056/uni0000000c/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000003/uni00000019/uni00000017/uni00000019/uni00000017/uni00000030/uni00000025 /uni00000014/uni00000015/uni0000001b/uni00000030/uni00000025 /uni0000002f/uni00000048/uni00000059/uni00000048/uni0000004f/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f\nFigure 8: Impact of index granularity on point lookup.\n/uni00000015/uni00000018/uni00000019 /uni00000014/uni00000015/uni0000001b /uni00000019/uni00000017 /uni00000016/uni00000015\n/uni0000000b/uni00000024/uni0000000c/uni00000003/uni00000026/uni00000052/uni00000050/uni00000053/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003/uni0000002c/uni00000051/uni00000047/uni00000048/uni0000005b/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048/uni00000014/uni00000015/uni00000016/uni00000017/uni00000026/uni00000052/uni00000050/uni00000053/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000058/uni00000056/uni0000000c/uni00000014/uni00000048/uni0000001a/uni00000029/uni00000033\n/uni00000029/uni00000037/uni00000033/uni0000002f/uni00000035\n/uni00000033/uni0000002f/uni00000028/uni0000003b/uni00000035/uni00000036\n/uni00000035/uni00000030/uni0000002c/uni00000033/uni0000002a/uni00000030\n/uni0000002e/uni00000039/uni00000003/uni0000002c/uni00000032 /uni0000002f/uni00000048/uni00000044/uni00000055/uni00000051 /uni0000003a/uni00000055/uni0000004c/uni00000057/uni00000048/uni00000003/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f\n/uni0000000b/uni00000025/uni0000000c/uni00000003/uni00000026/uni00000052/uni00000050/uni00000053/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000055/uni00000048/uni00000044/uni0000004e/uni00000047/uni00000052/uni0000005a/uni00000051/uni00000014/uni00000013/uni00000018/uni00000014/uni00000013/uni00000019/uni00000014/uni00000013/uni0000001a\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000058/uni00000056/uni0000000c\n/uni00000014/uni00000013/uni00000018/uni00000014/uni00000013/uni00000019/uni00000014/uni00000013/uni0000001a\n/uni0000002c/uni00000051/uni00000047/uni00000048/uni0000005b/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048/uni00000003/uni0000000b/uni00000025/uni0000000c\n/uni00000029/uni00000033\n/uni00000029/uni00000037/uni00000033/uni0000002f/uni00000035\n/uni00000033/uni0000002f/uni00000028/uni0000003b/uni00000035/uni00000036\n/uni00000035/uni00000030/uni0000002c/uni00000033/uni0000002a/uni00000030\nFigure 9: Compaction time and breakdown.\ninner indexes and associated overhead. However, when the posi-\ntion boundary is smaller than 32, memory usage becomes relatively\nstable for most learned indexes‚Äîexcept for RMI. In this regime,\nalthough fewer inner indexes are needed, the number of segments\ncreated by the models remains high, and segment-level overhead\nbecomes the dominant contributor to memory consumption. RMI\nbehaves differently: its memory usage consistently decreases re-\ngardless of the position boundary. This is because RMI‚Äôs memory\ncost is primarily driven by its inner index (i.e., the first-stage model),\nwhich remains the dominant component of memory usage across\nconfigurations, regardless of how many segments are created.\n5.3 Compaction Overhead of Learned Indexes\nObservation 4 : Unexpectedly, the learning overhead introduced\nby learned indexes can be considered modest compared to the\noverall compaction overhead. Index learning and writing time\naccount for less than 5% of the total compaction time in most of\nthe cases.\nAs discussed in Section 2, compaction is triggered when a level\nreaches its maximum capacity, merging the data from that level\ninto the next. In some systems[ 14,18], only a portion of the data\nis merged to the next level to reduce compaction overhead and\nprevent write stalls. The compaction process involves reading the\nsorted run into memory, merging the data, writing it back to disk,\nand building an index for the new sorted run. When using a learned\nindex, as described in the implementation section, this process also\nincludes segmenting the sorted run and training a model for each\nsegment. Previous research [ 7] highlights that the training time\nfor learned indexes introduces significant overhead, particularly\nin write-heavy workloads. Therefore, in this section, we evaluatethe overhead of learned indexes in a write-only workload with\n1,000,000 operations with setting the write buffer to 64MB.\nAs shown in Figure 9, the compaction time remains almost un-\nchanged as the index size grows for all the indexes. This is because\nthe primary cost of compaction comes from reading and writing\nkey-value pairs to and from disk. Additionally, the compaction time\nfor learned indexes does not significantly exceed that of fence point-\ners. Specifically, most learned indexes show less than a 5% increase\nin compaction time, while PLEX exhibits around a 10% increase.\nTo break this down, we examine the time spent on learning and\nwriting the model. For most learned indexes, model training takes\nless than 5% of the compaction time, which explains the minor\nincrease in total compaction time. However, PLEX uses around\n10-15% of the compaction time for training due to its self-tuning\nalgorithm. As for model writing, most learned indexes consume less\nthan 5% of compaction time. This demonstrates that the overhead\nintroduced by learned indexes during compaction is modest.\nThe lower training overhead compared to Bourbon [ 7] result\nfrom advancements in hardware. Since the training process occurs\nin memory, CPU performance is a key factor. Using a high-end CPU,\nsuch as the i9-13900K, helps reduce this overhead. Additionally,\nBourbon does not pipeline the training process with compaction.\nInstead, it reads data from the disk first and then performs the train-\ning using a background thread, likely leading to higher resource\nconsumption.\n5.4 Read Overhead at Different Levels\nObservation 5 : Evenly assigned position boundary across dif-\nferent LSM-tree levels may lead to suboptimal memory-latency\ntradeoff especially for skewed workload.\nThe LSM-tree has a layered structure, with each level‚Äôs capacity\ngrowing exponentially. This leads to varying read overhead at each\nlevel, raising the question: is setting the same position boundary\nfor learned indexes across all levels optimal?\nAs shown in Figure 10, we evaluate both uniformly and non-\nuniformly distributed workloads to measure the read overhead (i.e.,\nlookup time) at each level when using the same position bound-\nary. In the case of uniformly distributed workloads, the proportion\nof read overhead closely follows the level size, and the index size\nscales similarly with the level capacity due to the consistent posi-\ntion boundary across levels. However, for skewed workloads (i.e.,\nnon-uniform distribution), the read overhead is no longer directly\nproportional to the level capacity, resulting in an imbalance between\nindex memory allocation and read overhead.\n\n[Experiments & Analysis]\nEvaluating Learned Indexes in LSM-tree Systems: Benchmarks, Insights and Design Choices Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY\n/uni0000002f/uni00000013 /uni0000002f/uni00000014 /uni0000002f/uni00000015 /uni0000002f/uni00000016 /uni0000002f/uni00000017\n/uni0000000b/uni00000024/uni0000000c/uni00000003/uni00000038/uni00000051/uni0000004c/uni00000049/uni00000052/uni00000055/uni00000050/uni00000003/uni00000034/uni00000058/uni00000048/uni00000055/uni0000005c/uni00000003/uni00000027/uni0000004c/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000045/uni00000058/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000035/uni00000048/uni00000044/uni00000047/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004b/uni00000048/uni00000044/uni00000047 /uni0000002c/uni00000051/uni00000047/uni00000048/uni0000005b/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048 /uni0000002f/uni00000048/uni00000059/uni00000048/uni0000004f/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048\n/uni0000002f/uni00000013 /uni0000002f/uni00000014 /uni0000002f/uni00000015 /uni0000002f/uni00000016 /uni0000002f/uni00000017\n/uni0000000b/uni00000025/uni0000000c/uni00000003/uni00000035/uni00000048/uni00000044/uni00000047/uni00000010/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000056/uni00000057/uni00000003/uni00000027/uni0000004c/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000045/uni00000058/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018\nTable 1: The table presents the processing time for each stage\nof point lookup in PLR, with the position boundary set to 10.\nStatistics of Point Lookup In Detail\nProcess SST=4MB SST=32MB SST=128MB\nTable Lookup 0.19 us / op 0.11 op / us 0.07 us / op\nPrediction 0.17 us / op 0.15 us / op 0.15 us / op\nDisk I/O 2.12 us / op 2.10 us / op 2.16 us / op\nBinary Search 0.16 us / op 0.15 us / op 0.16 us / op\nFigure 10: The figure (left) shows the read overhead, the index size, and entries at different levels; the table (right) presents the\ndetail of point lookup.\n/uni00000014/uni00000015/uni0000001b /uni00000019/uni00000017 /uni00000016/uni00000015\n/uni0000000b/uni00000024/uni0000000c/uni00000003/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000051/uni00000056/uni0000000c/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000020/uni00000015/uni00000029/uni00000033\n/uni00000029/uni00000037/uni00000033/uni0000002f/uni00000035\n/uni00000033/uni0000002f/uni00000028/uni0000003b/uni00000035/uni00000036\n/uni00000035/uni00000030/uni0000002c/uni00000033/uni0000002a/uni00000030\n/uni00000014/uni00000015/uni0000001b /uni00000019/uni00000017 /uni00000016/uni00000015\n/uni0000000b/uni00000025/uni0000000c/uni00000003/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000020/uni00000014/uni00000015/uni0000001b\n/uni00000014/uni00000015/uni0000001b /uni00000019/uni00000017 /uni00000016/uni00000015\n/uni0000000b/uni00000026/uni0000000c/uni00000003/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000020/uni00000018/uni00000014/uni00000015\n/uni00000014/uni00000015/uni0000001b /uni00000019/uni00000017 /uni00000016/uni00000015\n/uni0000000b/uni00000027/uni0000000c/uni00000003/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000025/uni00000052/uni00000058/uni00000051/uni00000047/uni00000044/uni00000055/uni0000005c/uni00000015/uni00000014/uni00000019/uni00000015/uni00000014/uni0000001c/uni00000015/uni00000015/uni00000015\n/uni00000038/uni00000056/uni00000048/uni00000047/uni00000003/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni00000025/uni0000000c\n/uni00000029/uni00000033\n/uni00000029/uni00000037/uni00000033/uni0000002f/uni00000035\n/uni00000033/uni0000002f/uni00000028/uni0000003b/uni00000035/uni00000036\n/uni00000035/uni00000030/uni0000002c/uni00000033/uni0000002a/uni00000030\nFigure 11: Performance of range lookup under different lookup range and position boundary.\n/uni00000014/uni00000013/uni00000019\n/uni00000014/uni00000013/uni0000001b\n/uni00000038/uni00000056/uni00000048/uni00000047/uni00000003/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni00000025/uni0000000c/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013/uni0000002f/uni00000052/uni00000052/uni0000004e/uni00000058/uni00000053/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000058/uni00000056/uni0000000c\n/uni0000003c/uni00000026/uni00000036/uni00000025/uni00000010/uni00000024\n/uni00000014/uni00000013/uni00000019\n/uni00000014/uni00000013/uni0000001b\n/uni00000038/uni00000056/uni00000048/uni00000047/uni00000003/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni00000025/uni0000000c/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013\n/uni0000003c/uni00000026/uni00000036/uni00000025/uni00000010/uni00000025/uni00000029/uni00000033 /uni00000029/uni00000037 /uni00000033/uni0000002f/uni00000035 /uni00000033/uni0000002f/uni00000028/uni0000003b /uni00000035/uni00000036 /uni00000035/uni00000030/uni0000002c /uni00000033/uni0000002a/uni00000030\n/uni00000014/uni00000013/uni00000019\n/uni00000014/uni00000013/uni0000001b\n/uni00000038/uni00000056/uni00000048/uni00000047/uni00000003/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni00000025/uni0000000c/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013\n/uni0000003c/uni00000026/uni00000036/uni00000025/uni00000010/uni00000026\n/uni00000014/uni00000013/uni00000019\n/uni00000014/uni00000013/uni0000001b\n/uni00000038/uni00000056/uni00000048/uni00000047/uni00000003/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni00000025/uni0000000c/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013\n/uni0000003c/uni00000026/uni00000036/uni00000025/uni00000010/uni00000027\n/uni00000014/uni00000013/uni00000019\n/uni00000014/uni00000013/uni0000001b\n/uni00000038/uni00000056/uni00000048/uni00000047/uni00000003/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni00000025/uni0000000c/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013\n/uni0000003c/uni00000026/uni00000036/uni00000025/uni00000010/uni00000028\n/uni00000014/uni00000013/uni00000019\n/uni00000014/uni00000013/uni0000001b\n/uni00000038/uni00000056/uni00000048/uni00000047/uni00000003/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni00000025/uni0000000c/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013\n/uni0000003c/uni00000026/uni00000036/uni00000025/uni00000010/uni00000029\nFigure 12: Average operation time of indexes under six YCSB workloads.\nThis imbalance suggests that allocating more memory to lev-\nels where read overhead exceeds the index memory proportion\ncould improve the memory-latency tradeoff. In this sense, setting a\nuniform position boundary across different levels is not always a\ngood choice. This insight is aligned with setting a different mem-\nory budget (i.e., bit-per-key) for bloom filters across levels in the\nLSM-tree [8].\n5.5 Impact on Range Lookup\nObservation 6 : Learned indexes offer a superior memory-\nlatency tradeoff compared to fence pointers during short-range\nlookups. However, this advantage diminishes as the length of\nthe lookup range increases.\nRange lookups are a fundamental operation in LSM-tree systems,\nresponsible for retrieving a specified span of key-value pairs. Theprocess begins by locating the starting key of the range at each\nlevel, a task facilitated by learned indexes. Once the starting point\nis identified, the system sequentially retrieves the remaining entries\nwithin the target range.\nTo evaluate the effectiveness of learned indexes for range queries,\nwe test various index types using 64MB SSTables under different\nrange lengths. As shown in Figure 11, the memory-latency tradeoff\nfor small ranges resembles that of point lookups. In these cases, all\nlearned indexes outperform the traditional fence pointer in terms\nof memory efficiency while maintaining comparable query latency.\nHowever, as the range length increases, the latency across different\nindexing methods begins to converge, reducing the performance\nadvantage of learned indexes. For example, in Figure 11(A), increas-\ning the position boundary significantly improves performance in\nshort-range queries. In contrast, as shown in Figure 11(C), the same\nadjustment yields little benefit for longer ranges. This is because, for\nshort ranges, the dominant overhead lies in seeking the initial block,\n\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY Junfeng Liu, Jiarui Ye, Mengshi Chen, Meng Li, and Siqiang Luo\nwhere learned indexes are most effective. In long-range queries,\nhowever, the main cost shifts to scanning and retrieving a large\nnumber of entries, which diminishes the relative impact of index\nprecision. As a result, the memory-latency benefits of learned in-\ndexes diminish as range length increases, highlighting a limitation\nof their effectiveness in long-range query scenarios.\n5.6 Impact on Mixed Workload\nObservation 7 : The memory-latency tradeoff does not vary a\nlot under update-lookup mixed workload compared to read-only\nworkloads. Prioritizing read performance is a practical approach\nwhen evaluating a learned index in LSM-tree systems.\nTo evaluate the performance of learned indexes under more complex\nand real-world conditions, we test them with six YCSB workloads:\nA is read-write balanced, B is point lookup heavy, C is point lookup\nonly, D focuses on recent point lookup, E is range lookup heavy\n(with ranges less than 100), and F is read-modify-write (50% lookup\nand 50% update). The result is shown in Figure 12.\nOverall, the memory-latency tradeoff remains consistent with\nthe results from the point lookup workloads tested earlier. Specifi-\ncally, across all workloads, PGM continues to offer the best tradeoff,\nwhile FITing-tree lags behind other learned indexes in performance.\n6 Discussion\nIn this section, we are going to conclude about the performance\ntradeoff of learned indexes on LSM-tree systems and how we can\nmake them suitable for LSM-trees. The evaluation and analysis in\nthe previous section have already found several observations regard-\ning learned indexes in various aspects in the LSM-tree systems. To\nconclude, learned indexes showcase satisfactory memory-latency\ntradeoff in both range lookup and point lookup‚Äìusing less memory\nand delivering stronger throughput, while the extra computation\nconsumption for training is not remarkable, which demonstrates a\nsuperior potential of learned indexes to integrate learned indexes\ninto LSM-tree systems. To help users understand more about having\nthe best learned indexes in their LSM-tree systems, we are going to\ngive three decision guidelines in the following.\n6.1 Insights and Tuning Guide\nPrioritize Position Boundary. In Section 4, we identify three\nkey factors that influence the performance of learned indexes in\nLSM-tree systems, with position boundary having the most sig-\nnificant impact on read performance while different index types\nonly impact the memory-latency tradeoff. Though some learned in-\ndexes focus on optimizing prediction or segment lookup, which can\nshow excellent results in memory-based systems, these advantages\nbecome less noticeable in LSM-tree environments. For example,\neven though PLR employs the simplest inner index structure, its\nmemory-latency tradeoff remains desirable in most cases. This un-\nderscores that position boundary plays a more critical role in overall\nperformance compared to prediction optimizations, suggesting that\noptimizing the model position boundary to a smaller range within\nthe same memory budget is more important than improving the\nefficiency of indexing these learned models in LSM-trees.Increase the Index Granularity. While index granularity (i.e.,\nSSTable size) has less impact on performance than the position\nboundary, increasing granularity can still yield up to a 10% im-\nprovement in the memory-performance tradeoff across all learned\nindexes. By reducing the memory required to store index structures,\nlarger SSTables make it possible to allocate more memory toward\ndecreasing the position boundary, thereby enhancing lookup per-\nformance. However, adopting a level-granularity model must be\napproached with caution, as it is only feasible when full merges are\nperformed (i.e., merging an entire level into the next). Although\nfull merges do not increase the overall write amplification [ 11],\nthey can lead to short-term spikes in resource usage, which may\ntemporarily degrade foreground performance.\nWisely Allocate the Memory Budget. Since the performance\ngains diminish once the index size surpasses a certain threshold,\nallocating excessive memory to learned indexes is not optimal. In-\nstead, allocate a balanced portion of memory to the indexes based on\nyour total budget, and dedicate the remainder to other in-memory\ncomponents, such as Bloom filters and write buffers, to enhance\noverall performance. Additionally, because read overhead at each\nlevel is not always proportional to the level‚Äôs capacity, using a\nuniform position boundary across all levels may not be the best\napproach. It is more effective to adjust the boundaries according to\nthe specific query distribution.\n6.2 Future Direction\nSimply integrating existing learned indexes into LSM-tree systems\nis not the final step in learned index research. Based on our findings,\nthere are two promising directions for further exploration: (1) de-\nveloping a more sophisticated algorithm for dynamically allocating\nmemory budgets for learned indexes, taking into account work-\nloads, query distribution, and dataset characteristics to optimize\nthe memory-latency tradeoff, and (2) incorporating learned indexes\ninto the broader optimization of LSM-tree design space, such as in\nsystems like Dostoevsky [ 9], Wacky [ 10], and Moose [ 35]. The first\ndirection arises from our observation of the imbalance between\nread overhead and memory consumption at different levels. The\nsecond direction addresses a gap in recent LSM-tree design studies,\nwhich largely overlook the role of indexes, especially learned in-\ndexes. Given the strong memory-latency tradeoff demonstrated by\nlearned indexes, considering them in the LSM-tree design configu-\nration may lead to valuable optimization insights.\n7 Related Work\nLSM-tree Stores. Extensive research has focused on optimizing\nLSM-tree stores through comprehensive theoretical analysis and pa-\nrameter tuning, such as size ratio, compaction policies, and Bloom\nfilters [ 8‚Äì11,21,22,35,39,43]. These studies have significantly im-\nproved the performance of LSM-tree systems. Additionally, works\nlike Dostoevsky [ 9], Wacky [ 10], and Moose [ 35] define distinctive\nLSM-tree structures and derive optimal configurations by theoreti-\ncally modeling the cost of various LSM-tree operations. Integrating\nlearned indexes into these designs could offer valuable insights.\nFurthermore, self-tuning systems such as Cosine [ 3], Data Calcu-\nlator [ 24], Design Continuums [ 23], and Limousine [ 4] model the\ncosts of different index structures, including learned indexes and\n\n[Experiments & Analysis]\nEvaluating Learned Indexes in LSM-tree Systems: Benchmarks, Insights and Design Choices Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY\nLSM-tree indexes, to calculate the optimal storage structure within\na given budget. While these works provide broad insights into stor-\nage design, they lack fine-grained guidelines specifically tailored\nfor LSM-tree storage and learned indexes, with some focusing pri-\nmarily on cloud storage [ 3,4]. Our study aims to complement this\nresearch by offering more targeted design insights for LSM-tree\nsystems and learned indexes.\nLearned Indexes. Our study lies in the improvement of learned\nindexes techniques. In addition to the learned indexes discussed ear-\nlier, we review other notable approaches. MADEX [ 20] redesigns\nB+-tree nodes, incorporating CDF and correction models to en-\nhance point lookups. RUSLI [ 20] modifies RadixSpline [ 26] to sup-\nport updates, while FINEdex [ 32] introduces a buffer, building on\nXIndex [ 52], to handle updates more efficiently. LSI [ 25] is the first\nto model unsorted data. Some learned indexes, such as AULID [ 31],\nare specifically designed for disk-based systems. Additionally, re-\ncent studies have applied learned indexing to string, spatial, and\nmulti-dimensional queries [ 13,19,33,44,45,50,53,56]. Several eval-\nuations [ 30,41,54] and surveys [ 17,36] provide valuable insights\ninto tuning issues and the evolving landscape of learned indexes.\nIntegrating a wider variety of learned indexes into LSM-trees in\nthe future could provide us with more valuable insights.\nLearned Indexes in LSM-tree Systems. Due to the compati-\nbility between learned indexes and LSM-tree systems, and the\npromising memory-latency tradeoff they offer, several recent stud-\nies [2,7,37,47‚Äì49] explore integrating learned indexes into LSM-\ntrees to improve lookup performance. Abu-Libdeh et al. [2] are\nthe first to evaluate the feasibility of learned indexes in LSM-tree\nsystems, though their study does not fully cover different configura-\ntion options or index types. Dai et al. [7] integrate piecewise linear\nregression models into their LSM-tree system [ 38] and propose\nBourbon, achieving significant lookup improvements. Lu et al. [37]\npropose TridentKV, which integrates RMI [ 28] as the learned in-\ndex and claims better performance than Bourbon in read-heavy\nworkloads. Ramadhan et al. [47] further improve Bourbon by re-\nplacing binary search with exponential search, yielding moderate\nperformance gains. However, these works do not fully explore the\nentire configuration space that affects learned index performance in\nLSM-trees, nor do they thoroughly investigate the memory-latency\ntradeoff. Our work aims to bridge this gap, providing additional\ninsights and extending these foundational studies.\n8 Conclusion\nIn this study, we have conducted a comprehensive theoretical and\npractical evaluation of integrating learned indexes into LSM-tree\nsystems. We begin by revisiting existing learned indexes and an-\nalyzing their expected costs to identify key factors that influence\nLSM-tree performance. Through rigorous evaluations under vari-\nous conditions, we have derived several design guidelines tailored\nto LSM-tree systems and provide practical insights for optimizing\ntheir performance.\n9 Artifacts\nTo facilitate reproducibility and further exploration, we provide the\nfull implementation, including source code, workload generators,\nand experiment scripts, in our public GitHub repository: https://github.com/qingshanlanshan/LearnedIndexInLSM. The reposi-\ntory includes detailed instructions on how to configure, build, and\nrun the experiments described in this paper. Please refer to the\nREADME.md file in the repository for setup instructions, system re-\nquirements, and usage examples.\nReferences\n[1]-. Learned-Index-for-LSM-tree technical report. https://github.com/\nqingshanlanshan/LearnedIndexInLSM/TechnicalReport.pdf.\n[2]Hussam Abu-Libdeh, Deniz Altƒ±nb√ºken, Alex Beutel, Ed H Chi, Lyric Doshi,\nTim Kraska, Andy Ly, Christopher Olston, et al .2020. Learned indexes for a\ngoogle-scale disk-based database. arXiv preprint arXiv:2012.12501 (2020).\n[3]Subarna Chatterjee, Meena Jagadeesan, Wilson Qin, and Stratos Idreos. 2021. Co-\nsine: a cloud-cost optimized self-designing key-value storage engine. Proceedings\nof the VLDB Endowment 15, 1 (2021), 112‚Äì126.\n[4]Subarna Chatterjee, Mark F Pekala, Lev Kruglyak, and Stratos Idreos. 2024. Limou-\nsine: Blending Learned and Classical Indexes to Self-Design Larger-than-Memory\nCloud Storage Engines. Proceedings of the ACM on Management of Data 2, 1 (2024),\n1‚Äì28.\n[5] Source Code. 2024. WiredTiger. https://github.com/wiredtiger/wiredtiger.\n[6]James C Corbett, Jeffrey Dean, Michael Epstein, Andrew Fikes, Christopher Frost,\nJeffrey John Furman, Sanjay Ghemawat, Andrey Gubarev, Christopher Heiser,\nPeter Hochschild, et al .2013. Spanner: Google‚Äôs globally distributed database.\nACM Transactions on Computer Systems (TOCS) 31, 3 (2013), 1‚Äì22.\n[7]Yifan Dai, Yien Xu, Aishwarya Ganesan, Ramnatthan Alagappan, Brian Kroth,\nAndrea Arpaci-Dusseau, and Remzi Arpaci-Dusseau. 2020. From WiscKey to\nBourbon: A Learned Index for Log-Structured Merge Trees. In 14th USENIX\nSymposium on Operating Systems Design and Implementation (OSDI 20) . 155‚Äì171.\n[8]Niv Dayan, Manos Athanassoulis, and Stratos Idreos. 2017. Monkey: Optimal\nnavigable key-value store. In Proceedings of the 2017 ACM International Conference\non Management of Data . 79‚Äì94.\n[9]Niv Dayan and Stratos Idreos. 2018. Dostoevsky: Better Space-Time Trade-\nOffs for LSM-Tree Based Key-Value Stores via Adaptive Removal of Superfluous\nMerging. In Proceedings of the 2018 International Conference on Management of\nData (Houston, TX, USA) (SIGMOD ‚Äô18) . Association for Computing Machinery,\nNew York, NY, USA, 505‚Äì520. https://doi.org/10.1145/3183713.3196927\n[10] Niv Dayan and Stratos Idreos. 2019. The log-structured merge-bush & the wacky\ncontinuum. In Proceedings of the 2019 International Conference on Management of\nData . 449‚Äì466.\n[11] Niv Dayan, Tamar Weiss, Shmuel Dashevsky, Michael Pan, Edward Bortnikov,\nand Moshe Twitto. 2022. Spooky: granulating LSM-tree compactions correctly.\nProceedings of the VLDB Endowment 15, 11 (2022), 3071‚Äì3084.\n[12] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\net al.2020. ALEX: an updatable adaptive learned index. In Proceedings of the 2020\nACM SIGMOD International Conference on Management of Data . 969‚Äì984.\n[13] Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020.\nTsunami: A learned multi-dimensional index for correlated data and skewed\nworkloads. arXiv preprint arXiv:2006.13282 (2020).\n[14] Facebook. 2024. RocksDB. https://github.com/facebook/rocksdb.\n[15] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds. Proceedings of the\nVLDB Endowment 13, 8 (2020), 1162‚Äì1175.\n[16] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim\nKraska. 2019. Fiting-tree: A data-aware index structure. In Proceedings of the 2019\ninternational conference on management of data . 1189‚Äì1206.\n[17] Jiake Ge, Boyu Shi, Yanfeng Chai, Yuanhui Luo, Yunda Guo, Yinxuan He, and\nYunpeng Chai. 2023. Cutting Learned Index into Pieces: An In-depth Inquiry\ninto Updatable Learned Indexes. In 2023 IEEE 39th International Conference on\nData Engineering (ICDE) . IEEE, 315‚Äì327.\n[18] Google. 2024. LevelDB. https://github.com/google/leveldb/.\n[19] Tu Gu, Kaiyu Feng, Gao Cong, Cheng Long, Zheng Wang, and Sheng Wang. 2023.\nThe rlr-tree: A reinforcement learning based r-tree for spatial data. Proceedings\nof the ACM on Management of Data 1, 1 (2023), 1‚Äì26.\n[20] Ali Hadian and Thomas Heinis. 2020. MADEX: Learning-augmented Algorithmic\nIndex Structures.. In AIDB@ VLDB .\n[21] Andy Huynh, Harshal Chaudhari, Evimaria Terzi, and Manos Athanassoulis. 2021.\nEndure: A Robust Tuning Paradigm for LSM Trees Under Workload Uncertainty.\narXiv preprint arXiv:2110.13801 (2021).\n[22] Andy Huynh, Harshal A Chaudhari, Evimaria Terzi, and Manos Athanassoulis.\n2024. Towards flexibility and robustness of LSM trees. The VLDB Journal (2024),\n1‚Äì24.\n[23] Stratos Idreos, Niv Dayan, Wilson Qin, Mali Akmanalp, Sophie Hilgard, Andrew\nRoss, James Lennon, Varun Jain, Harshita Gupta, David Li, et al .2019. Design\nContinuums and the Path Toward Self-Designing Key-Value Stores that Know\n\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY Junfeng Liu, Jiarui Ye, Mengshi Chen, Meng Li, and Siqiang Luo\nand Learn.. In CIDR .\n[24] Stratos Idreos, Kostas Zoumpatianos, Brian Hentschel, Michael S Kester, and Demi\nGuo. 2018. The data calculator: Data structure design and cost synthesis from\nfirst principles and learned cost models. In Proceedings of the 2018 International\nConference on Management of Data . 535‚Äì550.\n[25] Andreas Kipf, Dominik Horn, Pascal Pfeil, Ryan Marcus, and Tim Kraska. 2022.\nLSI: a learned secondary index structure. In Proceedings of the Fifth International\nWorkshop on Exploiting Artificial Intelligence Techniques for Data Management .\n1‚Äì5.\n[26] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass learned\nindex. In Proceedings of the third international workshop on exploiting artificial\nintelligence techniques for data management . 1‚Äì5.\n[27] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings of the 2018 international\nconference on management of data . 489‚Äì504.\n[28] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings of the 2018 international\nconference on management of data . 489‚Äì504.\n[29] Avinash Lakshman and Prashant Malik. 2010. Cassandra: a decentralized struc-\ntured storage system. ACM SIGOPS Operating Systems Review 44, 2 (2010), 35‚Äì40.\n[30] Hai Lan, Zhifeng Bao, J. Shane Culpepper, and Renata Borovica-Gajic. 2023.\nUpdatable Learned Indexes Meet Disk-Resident DBMS - From Evaluations to\nDesign Choices. Proc. ACM Manag. Data 1, 2, Article 139 (June 2023), 22 pages.\nhttps://doi.org/10.1145/3589284\n[31] Hai Lan, Zhifeng Bao, J Shane Culpepper, Renata Borovica-Gajic, and Yu Dong.\n2023. A simple yet high-performing on-disk learned index: Can we have our\ncake and eat it too? arXiv preprint arXiv:2306.02604 (2023).\n[32] Pengfei Li, Yu Hua, Jingnan Jia, and Pengfei Zuo. 2021. FINEdex: a fine-grained\nlearned index scheme for scalable and concurrent memory systems. Proceedings\nof the VLDB Endowment 15, 2 (2021), 321‚Äì334.\n[33] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. 2020. LISA: A\nlearned index structure for spatial data. In Proceedings of the 2020 ACM SIGMOD\ninternational conference on management of data . 2119‚Äì2133.\n[34] Pengfei Li, Hua Lu, Rong Zhu, Bolin Ding, Long Yang, and Gang Pan. 2023.\nDILI: A Distribution-Driven Learned Index (Extended version). arXiv preprint\narXiv:2304.08817 (2023).\n[35] Junfeng Liu, Fan Wang, Dingheng Mo, and Siqiang Luo. 2024. Structural De-\nsigns Meet Optimality: Exploring Optimized LSM-tree Structures in A Colossal\nConfiguration Space. Proceedings of the ACM on Management of Data 2, 3 (2024),\n1‚Äì26.\n[36] Yu Liu, Hua Wang, Ke Zhou, ChunHua Li, and Rengeng Wu. 2022. A survey\non AI for storage. CCF Transactions on High Performance Computing 4, 3 (2022),\n233‚Äì264.\n[37] Kai Lu, Nannan Zhao, Jiguang Wan, Changhong Fei, Wei Zhao, and Tongliang\nDeng. 2021. TridentKV: A read-optimized LSM-tree based KV store via adaptive\nindexing and space-efficient partitioning. IEEE Transactions on Parallel and\nDistributed Systems 33, 8 (2021), 1953‚Äì1966.\n[38] Lanyue Lu, Thanumalayan Sankaranarayana Pillai, Hariharan Gopalakrishnan,\nAndrea C Arpaci-Dusseau, and Remzi H Arpaci-Dusseau. 2017. Wisckey: Sepa-\nrating keys from values in ssd-conscious storage. ACM Transactions on Storage\n(TOS) 13, 1 (2017), 1‚Äì28.\n[39] Siqiang Luo, Subarna Chatterjee, Rafael Ketsetsidis, Niv Dayan, Wilson Qin, and\nStratos Idreos. 2020. Rosetta: A robust space-time optimized range filter for key-\nvalue stores. In Proceedings of the 2020 ACM SIGMOD International Conference on\nManagement of Data . 2071‚Äì2086.\n[40] Marcel Maltry and Jens Dittrich. 2022. A Critical Analysis of Recursive Model\nIndexes. Proc. VLDB Endow. 15, 5 (2022), 1079‚Äì1091.\n[41] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra,\nAlfons Kemper, Thomas Neumann, and Tim Kraska. 2020. Benchmarking learned\nindexes. arXiv preprint arXiv:2006.12804 (2020).\n[42] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra,\nAlfons Kemper, Thomas Neumann, and Tim Kraska. 2020. Benchmarking Learned\nIndexes. Proc. VLDB Endow. 14, 1 (2020), 1‚Äì13.\n[43] Dingheng Mo, Fanchao Chen, Siqiang Luo, and Caihua Shan. 2023. Learning to\nOptimize LSM-trees: Towards A Reinforcement Learning based Key-Value Store\nfor Dynamic Workloads. Proc. ACM Manag. Data 1, 3, Article 213 (Nov. 2023),\n25 pages. https://doi.org/10.1145/3617333\n[44] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning multi-dimensional indexes. In Proceedings of the 2020 ACM SIGMOD interna-\ntional conference on management of data . 985‚Äì1000.\n[45] Jianzhong Qi, Guanli Liu, Christian S Jensen, and Lars Kulik. 2020. Effectively\nlearning spatial indices. Proceedings of the VLDB Endowment 13, 12 (2020), 2341‚Äì\n2354.\n[46] Pandian Raju, Rohan Kadekodi, Vijay Chidambaram, and Ittai Abraham. 2017.\nPebblesdb: Building key-value stores using fragmented log-structured merge\ntrees. In Proceedings of the 26th Symposium on Operating Systems Principles . 497‚Äì\n514.[47] Agung Rahmat Ramadhan, Min-guk Choi, Yoojin Chung, and Jongmoo Choi.\n2023. An Empirical Study of Segmented Linear Regression Search in LevelDB.\nElectronics 12, 4 (2023), 1018.\n[48] Subhadeep Sarkar and Manos Athanassoulis. 2022. Dissecting, designing, and op-\ntimizing LSM-based data stores. In Proceedings of the 2022 International Conference\non Management of Data . 2489‚Äì2497.\n[49] Subhadeep Sarkar, Niv Dayan, and Manos Athanassoulis. 2023. The LSM design\nspace and its read optimizations. In 2023 IEEE 39th International Conference on\nData Engineering (ICDE) . IEEE, 3578‚Äì3584.\n[50] Benjamin Spector, Andreas Kipf, Kapil Vaidya, Chi Wang, Umar Farooq Minhas,\nand Tim Kraska. 2021. Bounding the last mile: Efficient learned string indexing.\narXiv preprint arXiv:2111.14905 (2021).\n[51] Mihail Stoian, Andreas Kipf, Ryan Marcus, and Tim Kraska. 2021. PLEX: Towards\nPractical Learned Indexing. CoRR abs/2108.05117 (2021). arXiv:2108.05117 https:\n//arxiv.org/abs/2108.05117\n[52] Chuzhe Tang, Youyun Wang, Zhiyuan Dong, Gansen Hu, Zhaoguo Wang, Minjie\nWang, and Haibo Chen. 2020. XIndex: a scalable learned index for multicore data\nstorage. In Proceedings of the 25th ACM SIGPLAN symposium on principles and\npractice of parallel programming . 308‚Äì320.\n[53] Youyun Wang, Chuzhe Tang, Zhaoguo Wang, and Haibo Chen. 2020. SIndex: a\nscalable learned index for string keys. In Proceedings of the 11th ACM SIGOPS\nAsia-Pacific Workshop on Systems . 17‚Äì24.\n[54] Chaichon Wongkham, Baotong Lu, Chris Liu, Zhicong Zhong, Eric Lo, and\nTianzheng Wang. 2022. Are updatable learned indexes ready? arXiv preprint\narXiv:2207.02900 (2022).\n[55] Jiacheng Wu, Yong Zhang, Shimin Chen, Jin Wang, Yu Chen, and Chunxiao\nXing. 2021. Updatable learned index with precise positions. 14, 8 (April 2021),\n1276‚Äì1288. https://doi.org/10.14778/3457390.3457393\n[56] Shangyu Wu, Yufei Cui, Jinghuan Yu, Xuan Sun, Tei-Wei Kuo, and Chun Jason\nXue. 2022. NFL: robust learned index via distribution transformation. arXiv\npreprint arXiv:2205.11807 (2022).",
  "textLength": 101727
}