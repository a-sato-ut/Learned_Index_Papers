{
  "paperId": "31ea0b02a433169556c9cad1b3ba51f738b278f1",
  "title": "A Scalable Learned Index Scheme in Storage Systems",
  "pdfPath": "31ea0b02a433169556c9cad1b3ba51f738b278f1.pdf",
  "text": "arXiv:1905.06256v1  [cs.DB]  8 May 2019A Scalable Learned Index Scheme in Storage Systems\nPengfei Li, Yu Hua, Pengfei Zuo, Jingnan Jia\nHuazhong University of Science and Technology\nAbstract\nIndex structures are important for efﬁcient data access,\nwhich have been widely used to improve the performance\nin many in-memory systems. Due to high in-memory\noverheads, traditional index structures become difﬁcult t o\nprocess the explosive growth of data, let alone providing\nlow latency and high throughput performance with limited\nsystem resources. The promising learned indexes leverage\ndeep-learning models to complement existing index struc-\ntures and obtain signiﬁcant memory savings. However,\nthe learned indexes fail to become scalable due to the\nheavy inter-model dependency and expensive retraining.\nTo address these problems, we propose a scalable learned\nindex scheme to construct different linear regression mode ls\naccording to the data distribution. Moreover, the used\nmodels are independent so as to reduce the complexity of\nretraining and become easy to partition and store the data\ninto different pages, blocks or distributed systems. Our\nexperimental results show that compared with state-of-the -\nart schemes, AIDEL improves the insertion performance\nby about 2 ×and provides comparable lookup performance,\nwhile efﬁciently supporting scalability.\n1 Introduction\nEfﬁcient data storage and access are important for both\nindustry and academia, and the explosive growth of data\nexacerbates this problem. Index structures, such as B+-\ntree, Hash-map, and Bloom ﬁlters usually support today’s in -\nmemory systems to handle data processing tasks according\nto different requirements [ 1,14,17,31]. Traditional index\nstructures have been improved to be more memory-efﬁcient\nover the past decades [ 17,21,29,8].\nTree-based structures keep all ordered data for range\nrequests, which aim to identify the items within a given\nrange. A common approach to build a low-latency and\nhigh-throughput storage system is to maintain all data and\nmetadata completely in the main memory, which eliminates\nthe expensive disk I/O operations [ 38]. In fact, the indexes,\ne.g., tree-based structures, consume around 55% of the tota l\nmemory in a state-of-the-art in-memory systems [ 39]. The\nexpensive space overhead becomes exacerbated when the\nindex structures are too large to ﬁt into the limited-size\nmemory.\nExisting works attempt to improve the performance andreduce storage overhead. For example, since the B+-tree\nexhibits good cache-line locality, many cache-conscious\nvariants of B+-tree including CSS-tree [ 29], CSB-tree [ 30]\nand FAST [ 21] have been developed. Some schemes also\npropose to use hybrid index structures to further improve\nthe performance via GPUs [ 20,21,34]. Moreover, in order\nto reduce memory overhead of the B+-tree, compression\nschemes, including preﬁx/sufﬁx truncation, dictionary co m-\npression and key normalization have been proposed [ 16,4,\n6,29,28,40]. Some schemes use approximate structures to\nprocess the indexes [ 2,15,24].\nHowever, all above schemes are designed for general-\npurpose data structures and mainly focus on the index\nstructures themselves, thus overlooking the patterns of\ndata distribution. Kraska et al [ 24] argue that exact\ndata distribution enables high optimization for almost any\nindex structure. For example, a linear regression function\nis sufﬁcient for a system to store and access a set of\ncontinuous integer keys (e.g., the keys from 1 to 100M),\nwhich has signiﬁcant advantages over traditional B+-trees\nin terms of lookup performance and memory overhead. The\npatterns of data distribution become important for storage\nsystems to deliver high performance. However, in real-\nworld applications (e.g., web servers), it’s usually difﬁc ult\nto accurately obtain the patterns of data distribution in\nadvance and some patterns may be extremely complex or\neven impossible to represent via known patterns. We hence\nconsider machine learning (ML) approaches to learn a model\nthat exhibits the patterns of data distribution, called learned\nindexes [24].\nIn the context of our paper, the models are interpreted\nas regression ML models with bounded prediction errors,\nwhich are used to predict the positions of the keys by\nlearning the data distribution. However, the data generate d\nfrom the real-word applications, e.g., web servers, Intern et\nof Things (IoT), autonomous vehicles, are extremely hard to\nlearn. Inspired by the learned indexes, we partition the dat a\ninto different parts and use multiple linear regression mod els\nto learn each part well [ 24]. In order to meet range requests,\nall data are sorted for further training. Unlike the traditi onal\ntree-based indexes, the searching process in learned index es\nis mainly achieved by calculations. For example, the traine d\nlinear regression models offer a prediction range based on\nthe queried key, and guarantee that the prediction range\n1\n\ncontains the key if existing.\nThe learned indexes open up a new perspective on\nindexing issues: indexes can be considered as ML models .\nWe use cost-efﬁcient computation to speed up traditional\ncomparison operations, thereby increasing access speed an d\nsaving memory resources. However, it is non-trivial to\nefﬁciently leverage learned indexes due to the following\nchallenges.\n1) Poor Scalability. The poor scalability comes from\nthe heavy inter-model dependency and expensive retraining .\nSpeciﬁcally, to keep all ordered data for range requests,\ninserting new data into learned indexes will change the\npositions of some data, thus leading to many data movements\nor even increasing the probabilities that some data can’t\nbe found. To ensure the accuracy of the models, all the\nmodels have to be retrained even if only one model needs\nto be updated, since all the models are highly dependent.\nFurthermore, it is hard for learned indexes to partition and\nstore the data into different regions, since learned indexe s\nassume that all the data are stored in one continuous block,\nand we can’t move any models unless retraining.\n2) High Overheads. Learned indexes build a delta-\nindex [ 32] to handle inserts, which however produces high\noverhead. For example, the index structure needs to be\nretrained when the delta-buffer is full. Moreover, handlin g\nrange requests is inefﬁcient, since the data are stored in\ndifferent structures and hence all the data are not in order.\nLearned indexes argue to leverage the recursive model index\nstructure [ 24] or build an additional translation table to\npartition the data, but the two schemes are inefﬁcient and\ncause extra costs. Such designs require extra space and need\nto be rebuilt during retraining.\nIn order to address these challenges, our paper presents a\nscalable and adaptive learned index scheme with a bounded\nprediction error, called AIDEL. Unlike existing learned\nindexes, our models are completely independent in inter-\nor inner-layers. All models are generated through our\nmain component, Learning Probe Algorithm (LPA), which\nadaptively assigns linear regression models according to t he\ndata distribution. AIDEL achieves scalability through the\nsorted lists, which can easily handle inserts and keep all\nordered data to efﬁciently meet range requests. Such design s\nin AIDEL allow to update any models without affecting the\nentire structure.\nThe key contributions of this paper are summarized.\n•High Scalibility. We present a scalable learned index\nscheme, AIDEL, which eliminates the dependency of models\nand handles inserts through sorted lists. Unlike existing\nlearned indexes, AIDEL partitions the data and stores\nthem into different regions with low overheads in terms of\nretraining, thus efﬁciently supporting system scalabilit y.\n•Strong Adaptivity. We propose learning probe\nalgorithm to adaptively assign different linear regressio n\nmodels according to the data distribution. This algorithmnot only ensures all the models are independent, but also\nreduces the number of models to save more space than\nlearned indexes. Our scheme hence obtains adaptivity to\nefﬁciently handle various requests.\n•Low Overheads. Retraining and updating AIDEL are\ncost-efﬁcient, since we can retrain and update any models\nwithout affecting the entire structure due to the independe ncy\nbetween models. AIDEL not only efﬁciently meets range\nrequests, but also saves time on re-sorting the data during\nretraining, since AIDEL ensures all the data are kept in orde r,\nwhile also signiﬁcantly reducing the overheads.\nThe rest of this paper is organized as below. Section\n2 introduces the background. In Section 3, we present\nthe learning probe algorithm, and then demonstrate the\nidea and main operations of AIDEL. Section 4 shows the\nexperimental results and analysis. Section 5 discusses the\nrelated work, and Section 6 concludes our paper.\n2 Background and Motivation\n2.1 Index Structures for Range Requests\nIn general, an index structure is able to support point query\nthat searches for a given item. Unlike it, a range query aims\nto identify the items within a given range, which requires th e\ndata to be sorted, thus facilitating efﬁcient data accessin g.\nDue to the salient features of efﬁciency and scalability, B+-\ntree [ 12] is able to meet the needs of requirements of real-\nworld applications.\nFirst, B+-tree is efﬁcient for range requests. B+-tree\nstores all data in the leaf nodes and keeps the data in order,\nwhich enables efﬁcient range requests. In order to ﬁnd the\nqueried data, inner nodes are used to indicate which nodes to\nbe accessed next, until the leaf nodes are found. During the\nsearching, all the data in a node will be accessed if this node\nis selected by the previous one. Rao el al [ 29] showed that\nthe B+-tree exhibits good cache behaviors, since all the data\nin a node are accessed and used in more comparisons by one\ncache line if the length of a node is aligned with the cache\nline. Thus, CSS-tree [ 29] and CSB+-tree [ 30] are proposed\nto provide efﬁcient lookup performance by exploiting the\ncache. Recently, FAST [ 21] tries to make use of SIMD to\nfurther improve the performance. However, these optimized\nB+-trees need to allocate more memory for the inner nodes.\nThe nodes need to be realigned with cache and SIMD when\nnew insertions occur.\nSecond, B+-tree achieves scalability by dynamically\nbalancing the tree size. New data can be inserted if the found\nleaf node is not full, and otherwise more empty positions\nwill be generated by splitting and merging the nodes. Based\non the dynamic-size feature, B+-tree can easily handle\ninserts and keep all ordered data to efﬁciently support rang e\nrequests [ 19]. However, the size of B+-tree keeps growing\nwith the growth of the inserts and inner nodes consume a\nsigniﬁcant amount of available memory, which dramatically\n2\n\ndecrease the lookup performance once the index structure\noverfolws the memory.\nThird, B+-tree provides correctness guarantee that the\ndata are promised to be found once inserted. This correctnes s\nguarantee seems to be a fundamental feature of the index\nstructures, which however is not easy to be satisﬁed by\nlearned indexes, especially for the newly inserted data.\nBecause the inserted data will change the positions of some\ndata, leading to a failure probability that some data can’t b e\nfound since their new positions exceed the predicted range.\nMore details are analyzed in Section 3.3.\n2.2 New Perspective on Index Issues\nFrom the perspective of machine learning, range index\nstructures are regression models [ 24], which can predict\nthe position of a given look-up key as shown in Figure\n1(a). The index structures in Figure 1(a) can be the B+-\ntree or learned indexes. In the B+tree, the data are stored\nin leaf nodes and can be found through checking the tree.\nLearned indexes [ 24] view this process as a prediction, and\nthe records between [pred+minerr,pred+max err](where\nminerrmay be a negative) can be considered as the same\nconcept with the leaf nodes in the B+-tree. Obviously, the\nlength of [pred+minerr,pred+max err]will affect the\nlookup performance, and we term this length as prediction\ngranularity .\nFor making the prediction practical, the sorted keys and\nthe true positions are considered as inputs and outputs,\nrespectively. The relationship between keys and positions is\nsimilar to a cumulative distribution function (CDF) as show n\nin Figure 1(b). The dataset used in Figure 1(b) is the same\nas that used in Section 4, which is synthesized by lognormal\ndistribution with µ=0 and σ=2. Based on this observation,\nthe prediction accuracy can be improved by learning the\npatterns of data distribution according to the CDF. Numerou s\nschemes have been proposed to estimate the distribution of\ndata [ 13,27,18], which can be used in our work.\nWhen the CDF between keys and positions can be\naccurately represented via the known regression models,\nthe lookup complexity is O(1)since each position can be\ncalculated by the regression models. For example, a set\nof continuous integer keys (e.g., the keys from 1 to 100M)\nare stored in a piece of continuous positions (e.g., positio ns\nfrom 1 to 100M). The CDF can be represented as y=x,\nwhere xand yare keys and positions respectively. The\nprediction granularity is 1, which means that this regressi on\nmodel is accurate enough without any prediction error.\nHowever, in real-world applications (e.g., web servers), t he\nCDFs can’t be obtained in advance and some CDFs may\nbe extremely complex or even impossible to represent via\nknown regression models [ 24]. In these situations, we don’t\nneed to accurately represent the CDF to reduce the predictio n\ngranularity to 1, since the length of the leaf nodes in the B+-\ntree has never been set to 1, which simpliﬁes the predictionproblem: regression models only need to approximately\nrepresent the CDF and reduce the prediction granularity to\nthe same size like the leaf nodes in the B+-tree.\nIn this paper, we evaluate several different methods to\napproximately represent the CDF and the results are shown\nin Figure 3. Our proposed LPA algorithm learns the\nCDF best with the same number of models, which will be\nelaborated in Section 3.2.\n1-\n2.\n0\n\u0000E\n+\n\u0001\n\u00027\u0003.\n\u0004\n\u0005E\n\u0006\n\u0007\n\b\n\t\n.\n\u000b\n\fE\n\r\n\u000e\n\u000f74.\n\u0010\n\u0011E\n\u0012\n\u0013\n\u00147\n6.00E+007 8.00E+007 1.00E+008 1.20E+008 1.40E+008 1.60E+008 1.80E+0080.02.0k4.0k6.0k8.0k10.0k Positions\nKeys\n(a) Range Index Model (b) CDF of Lognormal DistributionKeyIndex Structures\n(e.g., B+-trees or learned indexes )\nprediction\npred+min_err pred+max_errleaf node\n· · · · · · · · · · · ·\nleaf node\nFigure 1: Range index and CDF models.\n2.3 Learned Indexes\nOne of the key challenges of using the learned model\nas an index structure is how to provide a small prediction\ngranularity to ﬁnd the lookup key quickly as a B+-tree does.\nUsing a single ML model to reduce the prediction granularity\n(e.g., from 100M to 10) is difﬁcult, which may result in\nan extremely complex ML model. It is hard to design and\ntrain this type of models. However, it is easy to reduce the\nprediction granularity from 100M to 10K, then from 10K to\n100, via various small ML models. Based on this observation\nand inspired by the mixture of experts work (a type of ML\nmodels for complex tasks) [ 35], learned indexes propose to\nuse a recursive model index (RMI) to improve the prediction\naccuracy.\nThe main idea of RMI is to build a hierarchy of models\nwhere at each stage the model picks another model based\non the intermediate prediction results, until the ﬁnal stag e\npredicts the position [ 24]. As shown in Figure 2, a RMI\nconsists of 3 stages, respectively containing 1, 2, 3 ML\nmodels. These models are trained in the order of hierarchica l\nrelationship, each of which is trained with different train ing\ndata. For example, Model 1.1 in the top level is trained\nﬁrst with the whole dataset that contains Nentries as shown\nin Figure 2. Based on the prediction results of Model 1.1,\neither Model 2.1 or 2.2 is selected and the entire dataset is\nalso divided into two subdatasets according to the selectio n\nresults. Then, the two models in the second stage are trained\nwith their respective subdatasets. The next stage follows t he\nsimilar training process.\nIn order to provide the correctness guarantee that learned\nindexes can accurately ﬁnd the queried key, learned indexes\nstore the min- and max-error for every model in the last\n3\n\nPositionKey\n02\n12345679881519232735435568\nStage 1\nStage 2\nStage 3Model 1.1\nModel 2.1 Model 2.2\nModel 3.1 Model 3.2 Model 3.3Input\nOutput 012 3456 798Key\n2\n8\n15\n19\n23\n27\n35\n43\n55\n68Position\n0\n1\n2\n3\n4\n5\n6\n7\n98\nFigure 2: The RMI in learned indexes to improve the\nprediction accuracy.\nstage, which can be calculated as follows:\nminerr=min(yi−fj\nL(x))∀i∈SL.j,j∈ML\nmax err=max(yi−fj\nL(x))∀i∈SL.j,j∈ML(1)\nwhere yirepresents the true position of each key in the\nsubdataset SL.j,fj\nL(x)represents the prediction result of jth\nmodel in the last stage L and there are Mlmodels in stage l.\nIf absolute min-/max-error is above the predeﬁned threshol d,\nthe ML model becomes invalid to be replaced with a B+-\ntree. Finally, learned indexes show the prediction granula rity\n[pred+minerr,pred+max err]if the picked ML model is\nvalid, and otherwise the lookup key will be searched in the\nB+-tree.\nLearned indexes implement a 2-stage RMI index with a\nsmall neural network (NN) on the top and a large amount\nof linear regression models at the bottom. Because learned\nindexes observe that a simple (0 hidden layers) to semi-\ncomplex NN model (2 hidden layers) on the top works better\nthan other conﬁgured NN models (i.e., with more hidden\nlayers). It is not cost-efﬁcient to execute complex models\nat the bottom since the simple linear regression models are\naccurate enough to learn the small subdatasets.\nOur experiments demonstrate that there exists invalid and\nredundant models in learned indexes, as shown in Sections\n3.1and4.4. Speciﬁcally, RMI needs to be conﬁgured in\nadvance (e.g., ML models, threshold of error, etc.), the\nmodels with smaller prediction errors than the threshold\nare considered to be valid, and otherwise traditional index\nstructures have to be used. However, the prediction errors\nof some models will exceed the threshold if we don’t\nconﬁgure enough models, causing these models to be invalid.\nSimply increasing the number of models to eliminate invalid\nmodels will result in many redundant models. Because the\nprediction errors of valid models have been smaller than\nthe threshold, leading to the newly increased models to\nbe redundant. The main reason for generating invalid andredundant models is that learned indexes can’t dynamically\nallocate models according to data distribution.\n2.4 Performance Guarantee\nOne concern about using ML models for indexing is the\ncalculation overhead because ML models are used to handle\ncomplex tasks including image recognition, natural langua ge\nprocessing, robotics, etc. They are usually considered\ncomputation-intensive and storage-intensive due to heavy\ncomputation consumption in training and inference [ 25,36].\nHowever, in learned indexes, ﬁnite arithmetic operations\nare faster than that traveling a B+-tree even if the B+-\ntree is in the cache [ 24]. In this paper, AIDEL only con-\ntains linear regression models whose arithmetic operation s\nare simple. In the meantime, besides the wide use of\nGraphics Processing Unit (GPU) and Tensor Processing Unit\n(TPU), there are also many researches on machine learning\naccelerators including GPUs, FPGAs, ASICs, PIMs and\nNVMs [ 9,10,22,37,11,33].\nIt is worth noting that the design goal of AIDEL is not to\ncompletely replace the traditional index structures. AIDE L\nis orthogonal to the traditional index optimization method s\nsuch as compression techniques and cache-conscious ap-\nproaches [ 29,30,21,16,4,6,29,28,40].\n3 The AIDEL Design\nIn this section, we elaborate the design of Adaptive\nInDEpendent Linear regression models (AIDEL) for scal-\nable learned indexes, which can learn the CDF of data\ndistribution.\nThe design goal of AIDEL is to eliminate the dependency\nbetween models in learned indexes, while providing efﬁcien t\nscalability. One of the key insights is to train linear\nregression models according to data distribution, which\neliminates the invalid and redundant models. Another insig ht\nis to handle inserts via sorted lists, which not only avoids t he\nprobability that some data can’t be found but also efﬁcientl y\nmeets range requests.\nThe AIDEL consists of two stages. The ﬁrst stage are\nkey-value pairs which are used to indicate which model is\nchosen according to the lookup key, where the key is the ﬁrst\ndata covered by each model and the value is a pointer to the\nmodel. The second stage are linear regression models which\nare used to predict the positions. All models are generated\nby learning probe algorithm to eliminate the invalid and\nredundant models.\nThe reason for using <key,model>pairs in the ﬁrst\nstage is that such design eliminates the dependency between\nmodels. We can hence retrain or move any models without\naffecting the entire structure. The second stage uses linea r\nregression models instead of complex neural networks,\nbecause the whole CDF can be represented via a large\namount of models and the linear regression models are\nenough to learn each part well [ 24]. Moreover, linear\n4\n\nregression models have few parameters, which are easy to\nbe trained and more space-efﬁcient.\nAIDEl achieves scalability through sorted lists which are\nappended behind existing data. The new data can be inserted\ninto sorted lists without changing the positions of the data\nthat have been learned by the model. Such design not only\navoids the probability that some data fail to be found due\nto data movements but also keeps all ordered data for range\nrequests.\n3.1 Learning Strategies\nIn the learned indexes, the lookup performance depends\nlargely on the size of prediction granularity. To bound the\nworst-case performance of learned indexes to that of the\nB+-tree, the invalid model whose error is larger than the\npredeﬁned threshold is replaced with a B+-tree [ 24]. One\nway to improve the lookup performance is to eliminate the\ninvalid models, which can be achieved by learning the CDF\nof data distribution to improve the prediction accuracy as\nanalyzed in Section 2.2.\nWe use several different strategies to learn the CDF in\nFigure 1(b) and the results are shown in Figure 3. We\nobserve that it’s impossible to represent the lognormal\ndistribution perfectly by only using one regression model\nas shown of the red line in Figure 3(a). Because the\ndistributions in real-world applications are more complex\nthan linear distribution [ 15,24]. According to the idea of\nlearned indexes that we can approximately represent the\ndata distribution by dividing the data into different parts\nand representing them with different regression models, we\nexamine other strategies to learn the CDF.\nIn order to implement a 2-stage RMI, we use a linear\nregression model in the ﬁrst stage since learned indexes\nidentify that a simple neural network (with hidden layers\nfrom 0 to 2) for the ﬁrst stage works well. A zero hidden-\nlayer NN is equivalent to a linear regression model. We only\nconﬁgure 10 linear regression models in the second stage\nbecause the used dataset in this evaluation only contains 10 K\nrecords and 10 models are enough to show the strengths of\ndifferent strategies. Our experiments demonstrate that th e\nway to select the models affects the prediction accuracy.\nBecause this selection process also determines how to divid e\nthe dataset as analyzed in Section 2.3. The linear regression\nmodels in the second stage can’t ﬁt the data distribution wel l\nif the obtained subdatasets have poor linear patterns.\nWe ﬁrst examine the selection process of learn indexes.\nFormally, each ML model can be essentially treated as a\nmathematical function f(x), in which xis the queried key.\nIf we use fl(x)to denote ML models in different stages, the\nselection process can be described as follows:\nfl(x)=f(⌊Mlfl−1(x)/N⌋)\nl(x) f1(x)=y (2)\nwhere xrepresents input, Nrepresents the total positions\nof the stage, y∈(0,M2]represents prediction result of the/s48/s49/s48/s107/s50/s48/s107/s51/s48/s107/s52/s48/s107\n/s45/s50/s46/s48/s69/s43/s48/s55/s48/s46/s48/s69/s43/s48/s48 /s50/s46/s48/s69/s43/s48/s55 /s52/s46/s48/s69/s43/s48/s55 /s54/s46/s48/s69/s43/s48/s55 /s56/s46/s48/s69/s43/s48/s55 /s49/s46/s48/s69/s43/s48/s56 /s49/s46/s50/s69/s43/s48/s56 /s49/s46/s52/s69/s43/s48/s56 /s49/s46/s54/s69/s43/s48/s56 /s49/s46/s56/s69/s43/s48/s56/s48/s46/s48/s50/s46/s48/s107/s52/s46/s48/s107/s54/s46/s48/s107/s56/s46/s48/s107/s49/s48/s46/s48/s107/s49/s50/s46/s48/s107\n/s45/s50/s46/s48/s69/s43/s48/s55/s48/s46/s48/s69/s43/s48/s48 /s50/s46/s48/s69/s43/s48/s55 /s52/s46/s48/s69/s43/s48/s55 /s54/s46/s48/s69/s43/s48/s55 /s56/s46/s48/s69/s43/s48/s55 /s49/s46/s48/s69/s43/s48/s56 /s49/s46/s50/s69/s43/s48/s56 /s49/s46/s52/s69/s43/s48/s56 /s49/s46/s54/s69/s43/s48/s56 /s49/s46/s56/s69/s43/s48/s56/s112/s111/s115/s105/s116/s105/s111/s110\n/s40/s97/s41/s32/s85/s115/s105/s110/s103/s32/s111/s110/s101/s32/s108/s105/s110/s101/s97/s114/s32/s109/s111/s100/s101/s108/s32/s116/s114/s117/s116/s104\n/s32/s115/s105/s110/s103/s108/s101\n/s107/s101/s121 /s107/s101/s121 /s112/s111/s115/s105/s116/s105/s111/s110\n/s40/s98/s41/s32/s78/s111/s114/s109/s97/s108/s105/s122/s101/s100/s32/s112/s97/s114/s116/s105/s116/s105/s111/s110/s105/s110/s103/s32/s116/s114/s117/s116/s104\n/s32/s110/s111/s114/s109/s97/s108/s105/s122/s97/s116/s105/s111/s110\n/s107/s101/s121 /s112/s111/s115/s105/s116/s105/s111/s110\n/s40/s99/s41/s32/s85/s110/s105/s102/s111/s114/s109/s108/s121/s32/s112/s97/s114/s116/s105/s116/s105/s111/s110/s105/s110/s103/s32/s116/s114/s117/s116/s104\n/s32/s117/s110/s105/s102/s111/s114/s109\n/s107/s101/s121 /s112/s111/s115/s105/s116/s105/s111/s110\n/s40/s100/s41/s32/s76 /s101/s97/s114/s110/s105/s110/s103/s80/s114/s111/s98/s101/s32/s112/s97/s114/s116/s105/s116/s105/s111/s110/s105/s110/s103/s32/s116/s114/s117/s116/s104\n/s32/s108/s101/s97/s114/s110/s105/s110/s103/s80/s114/s111/s98\nFigure 3: The efﬁciency of AIDEL in learning CDF.\nﬁrst model, and there are Mlmodels in stage l. From this\nformulation, we ﬁnd that the core idea to select the next\nmodel and partition the dataset is normalization, represen ted\nas⌊Mlfl−1(x)/N⌋.\nWe examine the approach (i.e., normalization) of learned\nindexes to select the next model and the results are shown\nin Figure 3(b). The matching effect of each model\nvaries signiﬁcantly depending on the data distribution. Fo r\nexample, the densely distributed data are not well learned\nwhile it’s much better for the sparse part. The reason is that\ndensely distributed data are likely to be divided into the sa me\nsubdataset according to Equation 2, even if these data are\nnot linearly distributed, resulting in poor learning accur acy.\nIncreasing the number of models allows these densely\ndistributed data to be partitioned into multiple subdatase ts,\nthus allowing more models to be used to improve the\nlearning accuracy. However, the strategy of adding models\nwill also be applied to the sparsely distributed data, while\nresulting in some models to be redundant. Because the\nsparsely distributed data have been well learned and there a re\nno needs to increase models for this part. Thus, the number\nof models in learned indexes is not optimal. Moreover, we\nhave no priori knowledge of the data distribution in advance ,\nwhich increases the difﬁculty for conﬁguring the number of\nmodels.\nWe use the same conﬁguration as the implemented 2-stage\nRMI, except modifying the selection strategy between the\nmodels. In this strategy, we divide the dataset evenly so\nthat each subdataset has the same amount of data and the\nresults are shown in Figure 3(c). This strategy improves\nthe learning accuracy for densely distributed data since th ese\ndata are divided into multiple subdatasets and can be learne d\nby more models. However, this strategy reduces the learning\n5\n\naccuracy for sparsely distributed data, since we have to add\nsome data from densely distributed data into sparse part to\nachieve the same amount of data, even if these data are not\nlinearly distributed.\nNeither of these two strategies can learn CDF well.\nThe main reason is that the two methods can’t adaptively\nconﬁgure models based on the data distribution, which\nmotivates us to propose the learning probe algorithm (LPA).\nFor fair comparisons, we also use 10 models and the\nresults are shown in Figure 3(d). The proposed learning\nprobe algorithm learns the CDF better than the previous\nstrategies. Because the dataset is divided according to the\ndata distribution, only the linearly distributed data will be\ndivided into the same subdatast which are easy to be learned\nby a linear regression model. The details of this algorithm\nare described in Section 3.2.\n3.2 The Learning Probe Algorithm\nTo overcome the shortcomings of previous strategies, this\npaper proposes the learning probe algorithm (LPA), which\nuses the greedy strategy to adaptively partition the data\naccording to the data distribution. Unlike existing work,\nin LPA, only the data with the same linear distribution\ncan be divided into the same subdataset. Therefore, each\nsubdataset can be easily represented by a linear regression\nmodel. The criterion for judging whether the data have the\nsame distribution is whether the error of the obtained model\nexceeds a predeﬁned threshold. LPA will add more data to\nthe subdataset if the error of obtained model is smaller than\nthe threshold. Otherwise, we remove a small amount of data\nin the order from back to front, since reducing the amount\nof data can decrease the prediction error of the regression\nmodel. The complete process of LPA algorithm is shown in\nAlgorithm 1.\nBefore using LPA, we need to conﬁgure some parameters\nincluding the threshold ,learning step and learning rate,\nwhere threshold is the max error of the model we can toler-\nate,learning step andlearning rate are used to determine\nthe learning speed. As shown in Algorithm 1, the main\ncomponent of LPA works like a probe, which ﬁrst walks\nforward for a large step of length learning step, i.e., add\nlearning step data from the training dataset record into a\nsmall dataset S(line 2). We can obtain a linear regression\nmodel on dataset Sand calculate the prediction error of\nthe model (lines 3 and 4), where minerrandmax errcan\nbe calculated by Equation 1. The prediction error of the\nobtained model determines the next operation of the probe.\nIferror<threshold , the probe keeps moving forward to\nanother learning step to obtain a new model until the error\nof obtained model is not smaller than threshold (lines 5-8).\nWhen error>threshold , the probe keeps moving backward\nwith a smaller step until the prediction error of the obtaine d\nmodel is not larger than threshold (lines 9-13). Finally, LPA\nappends the model to AIDEL and cleans the dataset SforAlgorithm 1: LPA Algorithm\nInput: intthreshold ,intlearning step,ﬂoat\nlearning rate,dataType record[N]\nOutput: trained AIDEL\n1while not reach the end of the dataset record[N] do\n2 addlearning step data into dataset Sfrom record ;\n3 train a linear regression model onS;\n4 error = max(|minerror|,|max error|);\n5 while error<threshold do\n6 add next learning step data into dataset Sfrom\nrecord ;\n7 train a new model onS;\n8 end\n9 while error>threshold do\n10 step=int(learning step∗learning rate);\n11 remove step data from the end of dataset S;\n12 train a new model onS;\n13 end\n14 AIDEL .append( model );\n15 clean data from dataset Sfor next probing;\n16end\nnext probing (lines 14 and 15).\nCompared with learned indexes, all the models generated\nby LPA are valid since only the model whose prediction error\nis not larger than threshold can be appended to AIDEL. And\nthe max error of each obtained model can be controlled by\nthe predeﬁned parameter threshold . Beneﬁt from the greedy\nstrategy, LPA eliminates redundant models at the same time.\nBecause each model covers as many continuous data as\npossible, where these data have the same linear distributio n.\nFurthermore, we don’t need to conﬁgure the number of\nmodels in advance, even if we have no priori knowledge of\nthe data distribution. Because LPA can adaptively partitio n\nthe data according to the data distribution. By eliminating\ninvalid and redundant models, the number of models is far\nsmaller than that of learned indexes, as shown in Section 4.4.\nAIDEL is constructed through LPA and the structure is\nshown of the left part in Figure 4. All the linear regression\nmodels are stored in the form of key-value, where the key is\nthe ﬁrst data covered by the model and the value is a pointer\nto the model.\n3.3 Scalability\nTraditional B+-tree achieves scalability by rebalancing the\ntree, which can insert new data while efﬁciently meeting\nrange requests over sorted data. However, the scalability i s\nnot easy for learned indexes since inserting new data may\nincur an error that some data can’t be found. Because the\nnewly inserted data will change the positions of some data\nto keep all data in order, leading some data to exceed the\nprediction granularity. As shown in Figure 4, the red line\nrepresents one of the linear regression models generated by\nLPA, and the black points are the data covered by this model.\n6\n\nFigure 4: The case of failing to identifying data in insertio n.\nSince minerrandmax errof the model are calculated via\nEquation 1as described in Section 2.3, the error xaof point\nameets the condition:\nminerr≤xa≤max err\nTherefore, point amust be found by the model since the true\nposition of ameet the condition:\na∈[pred a+minerr,pred a+max err]\nwhere pred arepresents the prediction result of the linear\nmodel. Obviously, all the covered points can be found by the\nmodel, which however is not true when there are some newly\ninserted data. For example, we have to move point atoa′if\nwe directly insert a new data before point a, which leads to\nthe error:\na′/∈[pred a′+minerr,pred a′+max err]\nsince the new error x′\na>max err.\nLearned indexes [ 24] argue to build a delta-index [ 32] to\nhandle new inserts, which has been widely used in other\nstructures such as Bigtable [ 7] and A-tree [ 15]. However,\nthe design of delta-index incurs additional issues. We\nhave to retrain the entire structure when delta-index is ful l.\nMoreover, range requests are inefﬁcient, because the data a re\nnot in order due to being stored in delta-index and learned\nindexes respectively.\nThe design goal of our scalable AIDEL structure is\nto avoid the errors that some data can’t be found, while\nefﬁciently meeting range requests over sorted data. AIDEL\nachieves scalability through a structure of sorted lists wh ich\nare appended behind the existing data. Figure 5illustrates the\ninsertion process. The data in <key,position>represent the\nexisting data , while the data in the sorted lists are the newly\ninserted data. The insertion process can be divided into\ntwo steps: (1)Find a position in the prediction granularity\n[pred+minerr,pred+max err]which is given by the linear\nregression model. AIDEL will return a position whose key\nis ﬁrst smaller than the new data if the prediction granulari ty\ndoesn’t contain this key. (2)Insert the data into sortedKey\n2\n8\n15\n17\n19\n29\n35\n43\n55\n68Position insert 10\n4852 542021230\n1\n2\n3\n4\n5\n6\n7\n989\ninsert 22\n23\n444546insert 47\n5323\n48\n525453\n54cache line\nFigure 5: The insertion process of AIDEL.\nlists behind this position. For example, AIDEL returns the\nposition 4 in the ﬁrst step when we insert 22, since 19 is ﬁrst\nsmaller than 22 in the existing data. Then, 22 is inserted int o\nthe sorted list behind the data 19. To access data efﬁciently ,\nthe length of each list is aligned with a cache line to leverag e\nthe cache. AIDEL will assign a new sorted list when there\nare no empty positions in existing lists, as shown in Figure\n5.\nIt is worth noting that such designs meet two criteria of\nthe design goal. First, the insertion process in sorted list s\ndoesn’t change the positions of existing data, which avoids\nthe movements of existing data and hence guarantees that\nthe data can always be found. Second, existing data and the\ninserted data in sorted lists are kept in order, which efﬁcie ntly\nmeets range requests.\n3.4 Retraining\nAlthough AIDEL achieves scalability through the sorted\nlists, the performance will decrease if the sorted lists are too\nlong. Because AIDEL has to spend a lot of time searching\non sorted lists. One way to improve the performance is to\nretrain AIDEL. There are two types of retraining in AIDEL.\nOne is to retrain all models and the other is to retrain part of\nthe models.\nThe process of retraining all models can be divided into\nthree steps. (1)Put all data into <key,position>pairs and\nkeep them in order, which are served as new existing data for\ntraining new models. This step is easy to be achieved since\nall data have been kept in order via sorted lists as described\nin Section 3.3. What we need to do is just to put the data\nfrom sorted lists after the corresponding existing data. Fo r\nexample, data 9 and 10 in the sorted list can be inserted\nafter 8 without re-sorting as shown in Figure 5. (2)Retrain\nthe models via LPA algorithm on the new existing data.\n(3)Construct key-value pairs for the ﬁrst stage to indicate\nhow to choose each model, where the key is the ﬁrst data\ncovered by each model and the value is a pointer to the\n7\n\nmodel.\nOne advantage of retraining in AIDEL is to retrain any\nmodels without affecting the whole structure, since all the\nmodels are independent. The process of retraining part of\nthe models can also be divided into three steps. (1)Differen t\nfrom retraining all models, we only need to put the data\nthat are covered by the model to be retrained into <\nkey,position>pairs. In Figure 5, suppose the model to be\nretrained covers data 2, 8, 15, 17, 19, and we only need to\nput the data from two covered sorted lists (i.e., 9, 10 and 20,\n21, 22, 23) into <key,position>pairs. (2)Retrain the model\nthat covers these data via LPA algorithm. LPA may generate\nmultiple models if the error of obtained model is larger than\nthe predeﬁned threshold . (3) Update the information of\nretrained models to the ﬁrst stage.\nCompared with learned indexes, retraining AIDEL is\nmore cost-efﬁcient. First, learned indexes need to re-sort\nall data since the new data are stored separately from the\nexisting ones and these data are not in order. However,\nAIDEL doesn’t need to spend time on re-sorting since\nAIDEL guarantees that all data have been kept in order via\nsorted lists as described in Section 3.3. Second, AIDEL\ncan retrain any model without affecting the whole structure .\nLearned indexes have to retrain the entire structure even if\nonly one model needs to be retrained. Because each model\nin learned indexes is selected by another model according to\nEquation 2, all models are highly dependent. In AIDEL, all\nmodels are independent since they are selected according to\nkey-value pairs in the ﬁrst stage. We can modify any models\nby updating the key-value pairs.\nAlthough the training for ML models is usually considered\nto be time consuming, learned indexes indicate that the\ntraining doesn’t consume much longer than a few seconds for\n200M records with simple RMI [ 24]. The core components\nin AIDEL are linear regression models which are simpler\nthan learned indexes. Moreover, the training for ML models\ncan be accelerated by powerful hardware such as GPUs and\nTPUs as describe in Section 2.4.\n3.5 Data Partition\nIn real-world applications, it is common to partition the\ndata into different blocks and store them in separate region s,\nsuch as disks and distributed systems. However, learned\nindexes only consider the case where all data are stored in\none contiguous block. The RMI structure in learned indexes\nis unsuitable to partition the data into different regions s ince\nthe models are dependent. The entire structure of learned\nindexes needs to be retrained even if only one model needs\nto be modiﬁed as analyzed in Section 3.4.\nLearned indexes outline several options to overcome this\nissue. First, it is possible to ﬁgure out the regions that are\noverlapped by multiple models through RMI and duplicate\nthese data. Second, we can create an additional translation\ntable for the conversion of different addresses. However,both methods are complicated and difﬁcult to achieve the\nscalability. Moreover, all components in RMI have to be\nrebuilt once retraining is required.\nAIDEL is easy to partition and store the data in different\nregions, because all the models are independent and the data\nare non-overlapped by each model. AIDEL can update (i.e.,\nadd, delete and change) each model by simply updating\nthe<key,model>pairs in the ﬁrst stage. During the\npartitioning, AIDEL only needs to remove the corresponding\nmodels which cover the partition data, and train new models\non these data. Interestingly, training on these data is\nparticularly easy because the original models have already\ndivided the data according to the data distribution. What we\nneed to do is to train new models on each divided part.\n4 Performance Evaluation\nIn this section, we evaluate the performance of AIDEL and\ncompare it with state-of-the-art index structures includi ng\nB+-tree [ 23], FAST [ 21] and learned indexes [ 24].\nAs the baseline, we use a popular B+-tree implementation\n(stx::tree [ 5] version 0.9). In the B+-tree, we use a fan-out\nof 128 since B+-tree provides the best lookup performance\nwith this conﬁguration as described in [ 24]. We use\nthe same conﬁguration (i.e. a page size of 128) for\nlearned indexes and AIDEL. FAST [ 21] is the state-of-\nthe-art SIMD optimized B+-tree. However, the structure\nin FAST requires the tree size to be the power of 2,\nthus leading to larger space overhead. Additionally, it is\ndifﬁcult to update indexes in FAST because this structure ha s\nbeen optimized for cache and SIMD instructions. Learned\nindexes [ 24] use ML models to address the index issues.\nSince no public implementations of learned indexes [ 24] are\navailable, we implement a 2-stage recursive model index\nwith the same conﬁguration as learned indexes. In order\nto compare the insertion performance, we build a delta-\nindex to handle new inserts for learned indexes. Moreover,\ndifferent conﬁgurations of the second stage in learned\nindexes, including 10K, 50K, 100K, 200K, are denoted as\nLI10K, LI 50K, LI 100K, LI 200K, respectively.\nWe use several datasets with different distributions to\nevaluate the performance of index structures. Among them,\nwe choose 2 real-world datasets (1)Weblogs, (2)DocId, and\n1 synthetic dataset (3)Lognormal.\n•Weblogs dataset contains 200 million log entries and\nwe use the timestamps as the indexes.\n•DocId contains ﬁve text collections in the form of\nbags-of-words, which has nearly 10 million instances\nin total. The DocID and WordID are used to identify\nunique documents and words, which are all non-linearly\ncontinuous.\n•Lognormal dataset is generated similarly with learned\nindexes, which contains 190 million unique values that\nfollow the lognormal distribution with µ=0 and σ=2,\nand each value is scaled up to be an integer up 1B.\n8\n\n/s32/s76/s73/s95/s50/s48/s48/s75/s32 /s32/s65/s73/s68/s69/s76\nFigure 6: Insertion throughput in different datasets.\nWe run experiments on a server that is equipped with an\nIntel 2.8 GHz 16-core CPU, 16 GB DRAM and 500 GB\nhard disk. The L1 and L2 caches of the CPU are 32KB and\n256KB, respectively. The prototypes are developed under\nthe Linux kernel 2.6.18 environment and we compile all\nimplementations using g++ 8.1.0 with -O3 option.\n4.1 Insertion Performance\nIn the experiments for measuring insertion performance,\nwe compare AIDEL with B+-tree and learned indexes\nwith different conﬁgurations. Since there are no insertion\nfunctions in learned indexes, we build a delta-index to hand le\nthe new inserts. We don’t compare AIDEL with FAST,\nbecause FAST can’t handle new inserts unless reconstruct\nthe entire structure.\nUnlike the traditional index structures, both learned\nindexes and AIDEL based on ML models, require ofﬂine\ntraining, and we choose 10% of the total data to execute\nthe training in the experiments. Then we disorder the data\nto eliminate the impact of cache and the results of insertion\nthroughput are shown in Figure 6. AIDEL improves the\ninsertion throughput by 1.3 ×to 2.7×compared with the\ntraditional B+-trees. The insertion performance of learned\nindexes is low, because learned indexes have to check the\nlearned ML models and delta-buffer to conﬁrm they don’t\ncontain the data before insertion. We also observe that\nincreasing the number of models is not useful for improving\nthe insertion throughput, because the bottleneck of insert ion\nis the delta-buffer instead of ML models. AIDEL has high\ninsertion performance because after learning the patterns ,\nAIDEL can quickly locate the approximate location of the\nnew data and append the data into the sorted lists.\nAs shown in Figure 7, we evaluate the insertion latency\nof different index structures with the increase of the load\nfactors. We deﬁne the load factor as the ratio of the inserted\ndata to the training data. We observe that the insertion spee d\nof AIDEL is the fastest, and the main reason is that AIDEL\ncan quickly ﬁnd the corresponding sorted lists after learni ng\nthe patterns. If the newly inserted data follow the same\npatterns, AIDEL is most likely to deliver the data to each/s48/s46/s49 /s48/s46/s53 /s48/s46/s57 /s50 /s52 /s48/s46/s51 /s48/s46/s55 /s49 /s51 /s53/s52/s48/s48/s53/s48/s48/s54/s48/s48/s55/s48/s48/s56/s48/s48/s57/s48/s48/s76/s97/s116/s101/s110/s99/s121/s32/s40/s110/s115/s41\n/s76/s111/s97/s100/s32/s70/s97/s99/s116/s111/s114/s32/s66/s43/s116/s114/s101/s101\n/s32/s76/s101/s97/s73/s110/s100/s101/s120\n/s32/s65/s73/s68/s69/s76\nFigure 7: Insertion latency with different load factors.\nsorted list. However, the sorted lists may become longer as\nthe inserted data increase, which will affect the insertion and\nquery performance as described in Section 3.3. We then need\nto retrain the learned structures.\n4.2 Lookup Performance\nIn the evaluation of lookup performance, we ﬁrst insert\nthe workloads into each index structure and then generate\na new random workload which contains existing keys and\nnon-existing keys to evaluate the lookup performance. We\ncompare AIDEL against binary search, B+-tree, FAST and\nlearned indexes which conﬁgure different amounts of models\nin the second stage (i.e. 10K, 50K, 100K and 200K). We\nincluded binary search since this method represents the wor st\ncase where the prediction granularity is equal to the size of\nthe whole dataset. We also compare the lookup performance\nof the index structures with and without insertion operatio ns.\nAs shown in Section 4.1, we build a delta-index to achieve\nthe scalability for learned indexes.\nWe evaluate the lookup performance of these different\nindex structures with no inserts and the results are shown\nin Figure 8(a). When there are no inserts, learned indexes\nimprove the lookup performance by 1.3 ×to 3.1×compared\nwith the B+-tree, because learned indexes can quickly ﬁnd\nthe corresponding model (i.e. the model knows where\nthe look-up key may locate) through the recursive model\nindex (RMI). AIDEL improves the lookup performance by\n1.2×to 2.1×compared with the B+-tree, since the models\nin AIDEL are independent, which have to leverage the\ntraditional methods to ﬁnd the corresponding model. The\nlookup performance of AIDEL is nearly the same as FAST\nand is comparable to learned indexes without inserts.\nWe further add some inserts to evaluate the lookup\nperformance (e.g. load factors of 0.5 and 1) and the results\nare shown in Figures 8(b) and (c). We don’t include\nFAST since FAST can’t handle inserts unless reconstruct\nthe entire structure. Compared with the experimental resul ts\nin Figure 8, learned indexes can’t achieve the same lookup\nperformance as no inserts. The main reason is that learned\nindexes handle the inserts with a delta-index and have to\n9\n\n/s32/s76/s73/s95/s49/s48/s48/s75 /s32/s76/s73/s95/s50/s48/s48/s75/s32 /s32/s65/s73/s68/s69/s76\n(a) Lookup throughput with no inserts\n/s32/s76/s73/s95/s50/s48/s48/s75/s32 /s32/s65/s73/s68/s69/s76\n(b) Lookup throughput with the insert factor of 0.5\n/s32/s76/s73/s95/s50/s48/s48/s75 /s32/s65/s73/s68/s69/s76\n(c) Lookup throughput with the insert factor of 1\nFigure 8: Lookup throughput in different situations.\nlookup both structures in one lookup operation. In contrast ,\nAIDEL improves the lookup performance by 1.3 ×to 2.1×\nwhen inserting new data, since AIDEL achieves scalability\nthrough the sorted lists and can deliver the data to each sort ed\nlist. Moreover, the independent structures make retrainin g\neasy as described in Section 3.4.\n4.3 Memory Overhead\nCompared with the traditional index structures, learned\nstructures consume less memory. B+-tree stores all the data\nin the nodes while learned structures manage the data by\nML models. In the learned indexes, the number of models\nis conﬁgured by the user in advance. Memory overhead\ncan be calculated according to this conﬁguration. However,\nAIDEL uses the learning probe algorithm to adaptively\nassign different ML models to learn the data distribution, a nd\nthe number of the ML models is to be known after learning.\nWe evaluate the metadata overhead of each index struc-\nture, i.e., the intermediate nodes in the B+-tree and the\nML models in learned structures. The experimental results\nare shown in Figure 9. Both learned indexes and AIDEL\nconsume less memory than B+-tree by 14 ×to 130×. The\nmain reason is that one trained ML model can cover lots of\ndata, and we only need to store the parameters of the trained\nML model. In the context of our paper, all models are linear\nregression models, which only contain two parameters (i.e. ,\nthe slop and intercept). Moreover, AIDEL can save more\nmemory than the leaned indexes, since the models in AIDEL\nare trained according to the data distribution.\n4.4 Model Numbers\nCompared with learned indexes, AIDEL eliminates the\ninvalid and redundant models by using the learning probe\nalgorithm, which uses less models than learned indexes. We\nevaluate the used models with different threshold values,\nwhere the threshold is the max prediction error that we can\ntolerate.\nThe number of models in learned indexes can be de-\ntermined in advance according to the conﬁgurations, and\nthe main components in learned indexes are the models in\nthe second stage whose number is 10K, 50K, 100K, 200K,/s76/s111/s103/s110/s111/s114/s109/s97/s108 /s87 /s101/s98/s108/s111/s103/s115 /s68/s111/s99/s73/s100/s48/s46/s48/s48/s46/s53/s49/s46/s48/s49/s46/s53/s50/s46/s48/s50/s46/s53/s51/s46/s48/s51/s46/s53/s51/s46/s50/s56 /s49/s50/s46/s57/s56/s77/s101/s109/s111/s114/s121/s32/s79/s118/s101/s114/s104/s101/s97/s100/s32/s40/s77/s66/s41/s32/s66/s43/s116/s114/s101/s101/s32 /s32/s76/s73/s95/s49/s48/s75/s32 /s32/s76/s73/s95/s53/s48/s75/s32 /s32/s76/s73/s95/s49/s48/s48/s75 /s32/s76/s73/s95/s50/s48/s48/s75 /s32/s65/s73/s68/s69/s76\n/s49/s50/s46/s52/s54\nFigure 9: Memory overheads.\nrespectively. However, the models whose errors are larger\nthan threshold can’t be used, since these trained models are\nnot able to offer a small enough range, which contains the\nlook-up key. We have to use the traditional index structures\nsuch as the B+-tree to replace the invalid models as described\nin learned indexes [ 24].\nWe evaluate the numbers of the invalid models in the\nexperiments, and the results that evaluates on Lognormal\nare shown in Table 1. AIDEL can use fewer models than\nlearned indexes under the same threshold , since the models\nin AIDEL can cover the data that have the same patterns\nas many as possible according to the data distribution.\nMoreover, the models in AIDEL are all valid (i.e. error<=\nthreshold ), since the learning probe algorithm guarantees\nthat only the models meet the condition error<=threshold\ncan be appended to AIDEL as described in Section 3.2.\nHowever, learned indexes use the strategy of normaliza-\ntion to partition the data. Once we fail to conﬁgure sufﬁcien t\nmodels to learn the data, lots of trained models are invalid.\nIn contrast, if the learned indexes use sufﬁcient models, e. g.,\n200K, to learn the data, most of the models are redundant\nsince learned indexes can’t assign models according to the\ndata distribution as analyzed in Section 3.1. For example,\nfrom Table 1, the learned indexes use 200K models to allow\nmost models to be valid, but there still exists 17 invalid\nmodels with the threshold of 64. However, AIDEL only\nneeds nearly 15K models to learn the data distribution and\n10\n\nTable 1: The numbers of models in learned indexes and AIDEL on the dataset of lognormal.\nThreshold TypeLearned IndexesAIDEL10K 50K 100K 200K\n32total 10,000 50,000 100,000 200,000 58,695\nunsatisﬁed 9,934 23,569 19,510 9,287 0\n64total 10,000 50,000 100,000 200,000 15,301\nunsatisﬁed 5,905 2,467 396 17 0\n128total 10,000 50,000 100,000 200,000 4,132\nunsatisﬁed 896 8 0 0 0\n256total 10,000 50,000 100,000 200,000 991\nunsatisﬁed 13 0 0 0 0\nguarantees that all these models are valid, since our propos ed\nlearning probe algorithm ensures that only the model whose\nprediction error is smaller than the predeﬁned threshold ca n\nbe appended into AIDEL. The main reason is that AIDEL\ncan adaptively assign different models according to the dat a\ndistribution.\n5 Related Work\nB+-trees [ 12] are designed to accelerate searches on disk-\nbased database systems and different variants have been\nproposed over the past few decades [ 17]. B+-trees [ 3] are\nused for disk based systems and T-trees [ 26] are proposed\nto be a replacement since the main memory sizes become\nlarge enough to store entire database. In order to provide\nfaster query performance in the database, several cache\nconscious B+-tree variants are proposed since Rao et al. [ 29]\nshowed that B+-trees have good cache behavior on modern\nprocessors. They propose CSS-tree [ 29] and CSB+-tree [ 30]\nto efﬁciently support index operations by exploiting the\ncache. More recently, with powerful hardware, there are\nsome schemes aiming to provide higher performance by\nusing SIMD instructions such as FAST [ 21] or GPUs [ 20,\n21,34].\nThe above methods mainly focus on the lookup time\nof the B+-trees while overlooking the memory overhead.\nHowever, B+-tree often consumes much storage space.\nThere are several schemes on compressing indexes to reduce\nthe size of keys via preﬁx/sufﬁx truncation, dictionary\ncompression and key normalization [ 16,6,29,28], or hybrid\nhot/cold indexes [ 39], which use a two-stage index to reduce\nthe memory overhead. Learned indexes [ 24] present a\ndifferent way to compress indexes, which depend on the data\ndistribution and achieve orders-of-magnitude less storag e\nconsumption compared with traditional B+-trees.\nApproximate indexes are used to reduce the memory\noverhead, such as BF-tree [ 2], A-tree [ 15] and Learned\nindexes [ 24]. BF-tree and A-tree use a B+-tree to store\ninformation about a region of the dataset, but the leaf nodes\nin a BF-tree are Bloom ﬁlters while in an A-tree are linear\nsegments. Learned indexes[ 24] proposed recursive modelindex to narrow the search range of a record, by using a\nsimple neural network in the ﬁrst stage, while many linear\nmodels in the second stage. Additionally, BF-trees fail to\nconsider data distribution, while A-tree, learned indexes and\nAIDEL all exploit the properties about the data distributio n.\nSince there are many researches on machine learning\naccelerators including GPUs, FPGAs, ASICs, PIMs and\nNVMs [ 9,10,22,37,11,33], we can use more advanced ML\nmodels such as convolutional neural network and the mixture\nof experts [ 25,36,35] for learned indexes. Moreover, the\nrelationship between the keys and positions is similar to\nthe CDF, and there has many researches on estimating the\ndistribution of data [ 13,27,18].\nUnlike them, the design goal of our paper is not to com-\npletely replace the traditional B+-trees, but to complement\nthe existing schemes. At the same time, the scheme in\nthis paper also needs to use traditional index structures to\nstore the linear regression models and thus all these relate d\ntechniques are orthogonal to AIDEL.\n6 Conclusion\nIn order to address the problem of scalability of learned\nindexes, we present a scalable learned index scheme, called\nAIDEL. In the context of our paper, the models are inter-\npreted as regression ML models with bounded prediction\nerrors, which are used to predict the positions of the keys\nby learning the data distribution. Unlike existing learned\nindexes, our models are independent in inter- or inner-\nlayers, which are generated by our LPA algorithm. The\nLPA algorithm eliminates invalid and redundant models\nin the learned indexes, since this algorithm adaptively\nassigns different regression models according to the data\ndistribution. The independency between models enables\nAIDEL to partition the data and store them into different\nregions with low overheads in terms of retraining. AIDEL\nhandles inserts through a structure of sorted lists, which\nkeep all ordered data to efﬁciently meet range requests.\nUnlike the traditional tree-based indexes, the searching\nprocess in our scheme is mainly achieved by calculations.\nOur experimental results show that compared with the B+-\n11\n\ntree, AIDEL improves 1.3 ×to 2.7×insertion throughput\nand about 2 ×lookup throughput. Compared with learned\nindexes, AIDEL provides comparable lookup performance\nwhile efﬁciently supporting scalability.\nReferences\n[1] A LEXIOU , K., K OSSMANN , D., AND LARSON , P.- ˚A.\nAdaptive range ﬁlters for cold data: Avoiding trips to\nsiberia. Proceedings of the VLDB Endowment 6 , 14\n(2013), 1714–1725.\n[2] A THANASSOULIS , M., AND AILAMAKI , A. Bf-tree:\napproximate tree indexing. Proceedings of the VLDB\nEndowment 7 , 14 (2014), 1881–1892.\n[3] B AYER , R., AND MCCREIGHT , E. Organization and\nmaintenance of large ordered indexes. In Software\npioneers . Springer, 2002, pp. 245–262.\n[4] B AYER , R., AND UNTERAUER , K. Preﬁx b-trees.\nACM Transactions on Database Systems (TODS) 2 , 1\n(1977), 11–26.\n[5] B INGMANN , T. Stx b+ tree c++ template classes, 2007.\n[6] B OEHM , M., S CHLEGEL , B., V OLK, P. B., F ISCHER ,\nU., H ABICH , D., AND LEHNER , W. Efﬁcient in-\nmemory indexing with generalized preﬁx trees. In BTW\n(2011), vol. 180, pp. 227–246.\n[7] C HANG , F., D EAN, J., G HEMAWAT , S., H SIEH ,\nW. C., W ALLACH , D. A., B URROWS , M., C HAN -\nDRA, T., F IKES , A., AND GRUBER , R. E. Bigtable:\nA distributed storage system for structured data. ACM\nTransactions on Computer Systems (TOCS) 26 , 2\n(2008), 4.\n[8] C HANG , Y.-C., C HANG , Y.-W., W U, G.-M., AND\nWU, S.-W. B*-trees: a new representation for non-\nslicing ﬂoorplans. In Proceedings of the 37th Annual\nDesign Automation Conference (2000), ACM, pp. 458–\n463.\n[9] C HEN, T., D U, Z., S UN, N., W ANG , J., W U, C.,\nCHEN, Y., AND TEMAM , O. Diannao: A small-\nfootprint high-throughput accelerator for ubiquitous\nmachine-learning. ACM Sigplan Notices 49 , 4 (2014),\n269–284.\n[10] C HEN, Y.-H., E MER , J., AND SZE, V. Eyeriss:\nA spatial architecture for energy-efﬁcient dataﬂow for\nconvolutional neural networks. In ACM SIGARCH\nComputer Architecture News (2016), vol. 44, IEEE\nPress, pp. 367–379.\n[11] C HI, P., L I, S., X U, C., Z HANG , T., Z HAO, J.,\nLIU, Y., W ANG , Y., AND XIE, Y. Prime: Anovel processing-in-memory architecture for neural\nnetwork computation in reram-based main memory. In\nACM SIGARCH Computer Architecture News (2016),\nvol. 44, IEEE Press, pp. 27–39.\n[12] C OMER , D. Ubiquitous b-tree. ACM Computing\nSurveys (CSUR) 11 , 2 (1979), 121–137.\n[13] D VORETZKY , A., K IEFER , J., AND WOLFOWITZ ,\nJ. Asymptotic minimax character of the sample\ndistribution function and of the classical multinomial\nestimator. The Annals of Mathematical Statistics\n(1956), 642–669.\n[14] F AN, B., A NDERSEN , D. G., K AMINSKY , M., AND\nMITZENMACHER , M. D. Cuckoo ﬁlter: Practically\nbetter than bloom. In Proceedings of the 10th ACM\nInternational on Conference on emerging Networking\nExperiments and Technologies (2014), ACM, pp. 75–\n88.\n[15] G ALAKATOS , A., M ARKOVITCH , M., B INNIG , C.,\nFONSECA , R., AND KRASKA , T. A-tree: A\nbounded approximate index structure. arXiv preprint\narXiv:1801.10207 (2018).\n[16] G OLDSTEIN , J., R AMAKRISHNAN , R., AND SHAFT ,\nU. Compressing relations and indexes. In Data\nEngineering, 1998. Proceedings., 14th International\nConference on (1998), IEEE, pp. 370–379.\n[17] G RAEFE , G., AND LARSON , P.-A. B-tree indexes\nand cpu caches. In Data Engineering, 2001.\nProceedings. 17th International Conference on (2001),\nIEEE, pp. 349–358.\n[18] H UANG , J. C., AND FREY, B. J. Cumulative\ndistribution networks and the derivative-sum-product\nalgorithm: Models and inference for cumulative\ndistribution functions on graphs. Journal of Machine\nLearning Research 12 , Jan (2011), 301–348.\n[19] H WANG , D., K IM, W.-H., W ON, Y., AND NAM, B.\nEndurable transient inconsistency in byte-addressable\npersistent b+-tree. In 16th USENIX Conference on File\nand Storage Technologies (2018), p. 187.\n[20] K ACZMARSKI , K. B+-tree optimized for gpgpu.\nInOTM Confederated International Conferences” On\nthe Move to Meaningful Internet Systems” (2012),\nSpringer, pp. 843–854.\n[21] K IM, C., C HHUGANI , J., S ATISH , N., S EDLAR ,\nE., N GUYEN , A. D., K ALDEWEY , T., L EE, V. W.,\nBRANDT , S. A., AND DUBEY , P. Fast: fast\narchitecture sensitive tree search on modern cpus and\ngpus. In Proceedings of the 2010 ACM SIGMOD\nInternational Conference on Management of data\n(2010), ACM, pp. 339–350.\n12\n\n[22] K IM, D., K UNG , J., C HAI, S., Y ALAMANCHILI ,\nS., AND MUKHOPADHYAY , S. Neurocube: A\nprogrammable digital neuromorphic architecture with\nhigh-density 3d memory. In Computer Architecture\n(ISCA), 2016 ACM/IEEE 43rd Annual International\nSymposium on (2016), IEEE, pp. 380–392.\n[23] K NUTH , D. E. The art of computer programming:\nsorting and searching , vol. 3. Pearson Education, 1997.\n[24] K RASKA , T., B EUTEL , A., C HI, E. H., D EAN, J.,\nAND POLYZOTIS , N. The case for learned index\nstructures. In Proceedings of the 2018 International\nConference on Management of Data (2018), ACM,\npp. 489–504.\n[25] K RIZHEVSKY , A., S UTSKEVER , I., AND HINTON ,\nG. E. Imagenet classiﬁcation with deep convolutional\nneural networks. In Advances in neural information\nprocessing systems (2012), pp. 1097–1105.\n[26] L EHMAN , T. J., AND CAREY , M. J. A study of index\nstructures for main memory database management\nsystems. In Proc. VLDB (1986), vol. 1.\n[27] M AGDON -ISMAIL , M., AND ATIYA , A. F. Neural\nnetworks for density estimation. In Advances in Neural\nInformation Processing Systems (1999), pp. 522–528.\n[28] N EUMANN , T., AND WEIKUM , G. Rdf-3x: a risc-style\nengine for rdf. Proceedings of the VLDB Endowment\n1, 1 (2008), 647–659.\n[29] R AO, J., AND ROSS, K. A. Cache conscious indexing\nfor decision-support in main memory. In VLDB (1999),\nvol. 99, Citeseer, pp. 78–89.\n[30] R AO, J., AND ROSS, K. A. Making b+-trees cache\nconscious in main memory. In Acm Sigmod Record\n(2000), vol. 29, ACM, pp. 475–486.\n[31] R ICHTER , S., A LVAREZ , V., AND DITTRICH , J. A\nseven-dimensional analysis of hashing methods and its\nimplications on query processing. Proceedings of the\nVLDB Endowment 9 , 3 (2015), 96–107.\n[32] S EVERANCE , D. G., AND LOHMAN , G. M.\nDifferential ﬁles: their application to the maintenance\nof large databases. ACM Transactions on Database\nSystems (TODS) 1 , 3 (1976), 256–267.\n[33] S HAFIEE , A., N AG, A., M URALIMANOHAR , N.,\nBALASUBRAMONIAN , R., S TRACHAN , J. P., H U,\nM., W ILLIAMS , R. S., AND SRIKUMAR , V. Isaac:\nA convolutional neural network accelerator with in-\nsitu analog arithmetic in crossbars. ACM SIGARCH\nComputer Architecture News 44 , 3 (2016), 14–26.[34] S HAHVARANI , A., AND JACOBSEN , H.-A. A hybrid\nb+-tree as solution for in-memory indexing on cpu-gpu\nheterogeneous computing platforms. In Proceedings of\nthe 2016 International Conference on Management of\nData (2016), ACM, pp. 1523–1538.\n[35] S HAZEER , N., M IRHOSEINI , A., M AZIARZ , K.,\nDAVIS , A., L E, Q., H INTON , G., AND DEAN,\nJ. Outrageously large neural networks: The\nsparsely-gated mixture-of-experts layer. arXiv preprint\narXiv:1701.06538 (2017).\n[36] S IMONYAN , K., AND ZISSERMAN , A. Very\ndeep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 (2014).\n[37] W ANG , Y., Z HANG , M., AND YANG , J. Towards\nmemory-efﬁcient processing-in-memory architecture\nfor convolutional neural networks. In ACM SIGPLAN\nNotices (2017), vol. 52, ACM, pp. 81–90.\n[38] W U, X., N I, F., AND JIANG , S. Wormhole: A fast\nordered index for in-memory data management. arXiv\npreprint arXiv:1805.02200 (2018).\n[39] Z HANG , H., A NDERSEN , D. G., P AVLO , A.,\nKAMINSKY , M., M A, L., AND SHEN, R. Reducing\nthe storage overhead of main-memory oltp databases\nwith hybrid indexes. In Proceedings of the 2016\nInternational Conference on Management of Data\n(2016), ACM, pp. 1567–1581.\n[40] Z UKOWSKI , M., H EMAN , S., N ES, N., AND BONCZ ,\nP. Super-scalar ram-cpu cache compression. In Data\nEngineering, 2006. ICDE’06. Proceedings of the 22nd\nInternational Conference on (2006), IEEE, pp. 59–59.\n13",
  "textLength": 65125
}