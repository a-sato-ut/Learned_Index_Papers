{
  "paperId": "d4638086aec2afd8e1db644e74aa8eb76b6d3021",
  "title": "Robust Learning-Augmented Caching: An Experimental Study",
  "pdfPath": "d4638086aec2afd8e1db644e74aa8eb76b6d3021.pdf",
  "text": "Robust Learning-Augmented Caching: An Experimental Study\nJakub Chł˛ edowski* 1Adam Polak* 2Bartosz Szabucki* 1Konrad ˙Zołna* 1 3\nAbstract\nEffective caching is crucial for the performance\nof modern-day computing systems. A key op-\ntimization problem arising in caching – which\nitem to evict to make room for a new item – can-\nnot be optimally solved without knowing the fu-\nture. There are many classical approximation\nalgorithms for this problem, but more recently\nresearchers started to successfully apply machine\nlearning to decide what to evict by discovering\nimplicit input patterns and predicting the future.\nWhile machine learning typically does not pro-\nvide any worst-case guarantees, the new ﬁeld\nof learning-augmented algorithms proposes so-\nlutions that leverage classical online caching al-\ngorithms to make the machine-learned predic-\ntors robust. We are the ﬁrst to comprehensively\nevaluate these learning-augmented algorithms\non real-world caching datasets and state-of-the-\nart machine-learned predictors. We show that\na straightforward method – blindly following ei-\nther a predictor or a classical robust algorithm,\nand switching whenever one becomes worse than\nthe other – has only a low overhead over a well-\nperforming predictor, while competing with clas-\nsical methods when the coupled predictor fails,\nthus providing a cheap worst-case insurance.\n1. Introduction\nCaching is an important part of almost any modern-day\ncomputing system because it can vastly speed up memory\naccess. A major optimization problem arising with regard\nto caching is: Which item to evict from the cache in order\nto make room for a new item when the cache is full? The\n*Equal contribution1Jagiellonian University, Kraków,\nPoland2EPFL, Lausanne, Switzerland3DeepMind, Lon-\ndon, United Kingdom. Correspondence to: Jakub\nChł˛ edowski <jakub.chledowski@gmail.com>, Adam\nPolak <adam.polak@epﬂ.ch>, Bartosz Szabucki <bar-\ntosz.szabucki@gmail.com>, Konrad ˙Zołna <kon-\nrad.zolna@gmail.com>.\nProceedings of the 38thInternational Conference on Machine\nLearning , PMLR 139, 2021. Copyright 2021 by the author(s).optimization goal is to maximize the number of cache hits ,\ni.e., situations when the requested item is still present in the\ncache. If we choose a wrong item to evict and it is requested\nagain soon after, a cache miss occurs, and the item has to\nbe reloaded to the cache from the main memory1, which is\nusually orders of magnitude slower than reading it directly\nfrom the cache.\nClassical caching algorithms. In the ofﬂine scenario,\ni.e., when we know in advance the sequence of requested\nitems, the problem is easy to solve optimally. Indeed, Belady\n(1966) proved that the number of cache misses is minimized\nby a greedy eviction policy – always evict the item which\nwill reappear the furthest in the future (or which will never\nreappear, if there is such item).\nIn the more realistic online scenario, we do not know the\nfuture requests. For the cache size of kitems, the clas-\nsical MARKER algorithm (Fiat et al., 1991) is O(logk)-\ncompetitive, i.e. it incurs at most O(logk\u0001OPT)cache\nmisses on inputs which the optimal ofﬂine algorithm\nserves with OPT misses. There is also a matching lower\nbound (Fiat et al., 1991), showing that no algorithm can do\nbetter, up to a constant factor. On the other hand, real-world\napplications employ simple heuristics, such as the gold stan-\ndard Least Recently Used ( LRU), which happens to perform\nbetter in practice.\nRecent machine learning approaches try to discover implicit\naccess patterns speciﬁc to individual applications, which\nare likely to occur in future request sequences and use that\nknowledge to make better eviction decisions (Jain & Lin,\n2016; Shi et al., 2019; Liu et al., 2020; Yan & Li, 2020).\nLearning-augmented caching algorithms. In general,\nalgorithms based on machine learning models tend to work\nwell on typical inputs but can perform arbitrarily badly\nwhen, e.g. training data is scarce, or input patterns change\nunexpectedly over time. Lykouris & Vassilvitskii (2018)\nproposed a workaround to that issue. Their online caching\nalgorithm, PredictiveMarker, takes as an additional input\nfor each requested item a prediction (e.g. generated by an\nML model) when this item will be requested again. The\n1Even if a requested item is unlikely to be ever reused, it has to\nbe put into the cache.arXiv:2106.14693v1  [cs.LG]  28 Jun 2021\n\nRobust Learning-Augmented Caching: An Experimental Study\nalgorithm is consistent , i.e., it incurs an almost optimal num-\nber of cache misses when given nearly perfect predictions,\nandrobust , i.e., it isO(logk)-competitive (just like the op-\ntimal MARKER algorithm) even when the predictions are\ncompletely wrong.2Formally, PredictiveMarker incurs at\nmost\nO\u0012\nOPT\u0001min\u0012r\u0011reuse\nOPT+ 2;logk\u0013\u0013\ncache misses, where \u0011reuse denotes the total L1error of\nthe predictor. Rohatgi (2020) and Wei (2020) came up\nwith more caching algorithms working in this setup, with\nimproved dependencies on the prediction error \u0011reuse.\nAntoniadis et al. (2020) proposed a different setup for\nlearning-augmented caching algorithms. In their setup,\na predictor has to keep guessing what the optimal (knowing\nthe future) policy would do. In principle, any caching algo-\nrithm can serve as a predictor itself. The prediction error\n\u0011cache is measured as the size of the symmetric difference be-\ntween cache conﬁgurations of the optimal algorithm and the\npredictor, summed over all time steps. Actually, the setup\nworks for any metrical task system (MTS), a general class\nof online problems that includes caching. Antoniadis et al.\n(2020) provide two consistent and robust algorithms (one\ndeterministic and one randomized) for general MTS, and\nspeciﬁcally for caching, they propose an algorithm, dubbed\nTrust&Doubt, with a better dependence on prediction error.\nMotivation. A comprehensive comparison of the above\nlearning-augmented caching algorithms is a difﬁcult task.\nAdmittedly, worst-case competitive ratios of algorithms\nwithin the same setup can be compared, but such theoretical\ncomparison between the two setups is implausible. It fol-\nlows from the fact that the ﬁnal competitive ratios depend\non coupled predictors, with different error measures that can\nnot be translated between the setups.\nTo the best of our knowledge, so far there is no experimental\nevaluation of these algorithms using real-world datasets nor\npredictors. Experiments were included only in works by\nLykouris & Vassilvitskii (2018) and Antoniadis et al. (2020).\nHowever, these are small-size proof-of-concept experiments\non datasets adapted from other problems (not related to\ncaching) and using simple ad-hoc predictors instead of fully-\nﬂedged machine learning models. The following question\nremains wide open.\nAre learning-augmented caching algorithms practical?\nOur study. In this paper, we set out to answer this ques-\ntion experimentally. We use benchmark dataset from the\n2The exact meaning of consistency and robustness seems to be\nsomewhat inconsistent throughout the literature.2nd Cache Replacement Championship (CRC, 2017), also\nused by Shi et al. (2019) and Liu et al. (2020). As a predic-\ntor, we use state-of-the-art machine-learning-based caching\nalgorithm Parrot (Liu et al., 2020). Conveniently, Parrot\nalso predicts when items reappear in the request sequence,\non top of predicting which item the optimal policy discards.\nIt can thus be used as a predictor for learning-augmented al-\ngorithms in both setups, by Lykouris & Vassilvitskii (2018)\nand by Antoniadis et al. (2020), respectively.\nFirst, we test how the algorithms perform with a fully-\nﬂedged predictor so that we can see if the overhead they\nincur is an acceptable cost to pay in exchange for the worst-\ncase guarantees they provide. Second, we also run a sce-\nnario with a predictor under-performing due to the scarcity\nof available training data. That lets us evaluate if the theo-\nretical robustness guarantees play a role in practice.\nOur secondary contribution is a ready-to-use benchmark,\nwhich facilitates an easier future comparison with our results.\nIt is based on a dataset which was previously used by Liu\net al. (2020). However, since the method they use to generate\ninput instances tends to have high variance, by providing the\nready-to-use inputs we make it possible to compare directly\nwith the numbers we report, without having to rerun all the\nexperiments.\n2. Background and Algorithms\nIn this section, we aim to concisely describe the background\nneeded for the understanding of the following sections.\nWe will start by explaining the two prediction setups for\nlearning-augmented caching. Next, we will continue with a\nshort overview of the learning-augmented algorithms used\nin each setup. At the end, we will describe Parrot (Liu et al.,\n2020), the neural network predictor that we use to generate\npredictions for the algorithms.\n2.1. Prediction setups for caching\nLykouris & Vassilvitskii (2018) proposed the ﬁrst prediction\nsetup for online caching. In their setup, after each request,\nthe predictor has to forecast when the requested item will\nbe requested again – the value called reuse distance . This\nis a very natural choice since the reuse distance is the only\nstatistic that Belady’s optimal ofﬂine algorithm looks at.\nThey deﬁne the prediction error \u0011reuse to be theL1distance,\ni.e., the sum over all requests of the absolute difference\nbetween the actual and predicted reuse distances.\nAntoniadis et al. (2020) noticed a limitation of this setup\n– it does not generalize to other online problems. Already\nfor the weighted caching problem (where the cost of load-\ning each item can be different) even perfect reuse dis-\ntance predictions are not sufﬁcient to beat the best clas-\nsical prediction-less algorithm. To address this limitation,\n\nRobust Learning-Augmented Caching: An Experimental Study\nTable 1. Classical and learning-augmented caching algorithms. Constants in competitive ratios are omitted for brevity.\nAlgorithm Prediction Competitive ratio Combiner Reference\nOPT n/a 1 n/a Belady (1966)\nLRU n/a k n/a folklore\nMARKER n/a logk n/a Fiat et al. (1991)\nPredictiveMarker reuse distance min(logk;p\n\u0011reuse=OPT) n/a Lykouris & Vassilvitskii (2018)\nLMarker reuse distance min(logk;log(\u0011reuse=OPT)) n/a Rohatgi (2020)\nLNonMarkerDreuse distance min(logk;logk=k\u0001\u0011reuse=OPT)deterministic Rohatgi (2020)\nLNonMarkerRreuse distance min(logk;logk=k\u0001\u0011reuse=OPT)randomized Rohatgi (2020)\nBlindOracleDreuse distance min(logk;1=k\u0001\u0011reuse=OPT) deterministic Wei (2020)\nBlindOracleRreuse distance min(logk;1=k\u0001\u0011reuse=OPT) randomized Wei (2020)\nRobustFtPDoptimal policy min(logk;\u0011cache=OPT) deterministic Antoniadis et al. (2020)\nRobustFtPRoptimal policy min(logk;\u0011cache=OPT) randomized Antoniadis et al. (2020)\nTrust&Doubt optimal policy min(logk;log(\u0011cache=OPT)) n/a Antoniadis et al. (2020)\nthey proposed an alternative setup that works for metrical\ntask systems – a general class of online problems, which\nincludes caching. In their setup, after each request, the pre-\ndictor has to guess what an optimal ofﬂine algorithm would\ndo. The prediction error \u0011cache is the size of the symmetric\ndifference between the caches maintained by the optimal\nalgorithm and the predictor, summed over time.\nA direct comparison of the two setups is problematic for\nat least two reasons. First, even though predictions for the\nﬁrst setup can be translated (by following Belady’s rule)\nto predictions for the second setup (but not the other way\nround), the respective errors cannot be related to each other,\nas shown by the two instructive examples in Antoniadis\net al. (2020, Sect. 1.3). Second, a priori, it is not clear which\nof the two types of predictors is easier to train well. On\nthe one hand, predicting reuse distances can be framed as\na standard supervised learning task, while predicting optimal\npolicy seems to require more advanced approaches such as\nimitation learning or reinforcement learning. On the other\nhand, one can imagine an input distribution such that it is\nhard to accurately predict reuse distances while it is still\neasy to always ﬁnd an item with a reuse distance likely so\nlarge that it is safe to evict.\n2.2. Augmented algorithms\nWe evaluate the six learning-augmented caching algo-\nrithms proposed up to date. PredictiveMarker (Lykouris\n& Vassilvitskii, 2018), LMarker and LNonMarker (Ro-\nhatgi, 2020), and BlindOracle (Wei, 2020) are all aug-\nmented with reuse distance predictions, while RobustFtP\nand Trust&Doubt (Antoniadis et al., 2020) utilize optimal\npolicy predictions. See Table 1 for a summary of these\nalgorithms.\nCombiners. One way to achieve robustness is to combine\na non-robust learning-augmented algorithm with a robustclassical algorithm, e.g. MARKER , using a combiner . A com-\nbiner is a procedure that takes two algorithms and uses them\nin a black-box way to perform on par with the better of the\ntwo algorithms on each input (up to a constant factor).\nA straightforward combiner can simulate the two algorithms,\nkeep track of their respective costs up to date, and switch\nbetween them whenever one heavily outperforms the other.\nThe idea dates back to Fiat et al. (1994), and it was adapted\nto the learning-augmented setting by Lykouris & Vassilvit-\nskii (2018). We will call the above combiner deterministic .\nIn contrast, Wei (2020) and Antoniadis et al. (2020) use an\nidea of Blum & Burch (2000) to provide an alternative ran-\ndomized combiner. The more intricate combining algorithm\nallows to bring down the multiplicative overhead to 1 +\"at\nthe cost of an extra additive constant depending on \".\nSome learning-augmented algorithms are built using com-\nbiners, while others achieve robustness out-of-the-box. In\nprinciple, each of the algorithms implementing a combiner\ncan have (at least) four variants. It can be paired with either\nMARKER orLRU3, and, independently of that choice, it can\nuse either the deterministic or the randomized combiner.\nAlgorithms with reuse distance. PredictiveMarker,\nLMarker, and LNonMarker all work by keeping track of\neviction chains . An eviction chain is a sequence of sub-\noptimal evictions where each eviction is forced by the cache-\nmiss that can be blamed on the previous eviction in the\nsequence. The three algorithms differ with respect to (1)\nfor how long they trust predictions in each eviction chain,\nand (2) what they do instead when they eventually lose the\n3Formally, combining with LRU does not yield robustness,\nsince LRU is notO(logk)-competitive. However, it has at least a\nprovably bounded competitive ratio, and its good practical perfor-\nmance is well understood. Hence, in practice, it makes sense to\nuse LRU as a fallback option for a potentially arbitrarily inaccurate\nmachine-learned algorithm.\n\nRobust Learning-Augmented Caching: An Experimental Study\nTable 2. Characteristics of our datasets. The ﬁrst row shows the total sizes of all used datasets. These sizes are later split into\ntrain/valid/test sets with 80%/10%/10% splits. Further rows display the cache hit rates of pure (non-learning-augmented) algorithms,\nillustrating varying difﬁculties of the datasets.\nastar bwaves bzip cactusadm gems lbm leslie3d libq mcf milc omnetpp sphinx3 xalanc\nSize 1,154,048 570,368 167,680 221,952 723,456 782,080 716,032 579,840 2,965,504 556,800 555,520 328,704 69,120\nCache hits\nOPT 37.4% 4.9% 80.8% 33.7% 12.7% 24.8% 30.9% 5.3% 44.6% 1.4% 42.4% 74.8% 56.9%\nRANDOM 8.3% 0.2% 56.5% 4.5% 3.9% 2.2% 9.5% 0.0% 20.5% 0.0% 17.6% 52.8% 36.8%\nLRU 4.0% 0.0% 63.8% 0.0% 2.9% 0.0% 9.5% 0.0% 27.1% 0.0% 20.4% 12.7% 45.4%\nMARKER 4.7% 0.0% 62.7% 1.3% 4.1% 0.0% 9.3% 0.0% 24.9% 0.0% 20.3% 42.2% 43.5%\nPARROT -REUSE 29.0% 0.1% 57.9% 21.8% 0.4% 0.5% 4.9% 5.3% 32.5% 1.1% 11.9% 67.6% 37.3%\nPARROT -CACHE 32.2% 0.3% 68.4% 32.9% 3.1% 0.0% 11.4% 0.0% 43.9% 0.0% 21.9% 70.6% 49.7%\ntrust. We refer to the original papers for detailed discussions\nof these strategies, designed to allow better and better for\nworst-case competitive ratios, expressed as a function of\nthe normalized error \u0011reuse=OPT . PredictiveMarker and\nLMarker are provably robust out-of-the-box. In contrast,\nLNonMarker achieves robustness thanks to the classical\nMARKER algorithm and a black-box combiner.\nThe next learning-augmented caching algorithm, BlindOra-\ncle, simply applies Belady’s rule to the prediction – i.e. it\nevicts the item with the predicted next arrival time furthest\nin the future – and adds the guarantee of a robust algorithm\n(e.g. MARKER ) using either of the aforementioned combin-\ners. We note that, despite being the simplest out of the four,\nBlindOracle also has the best competitive ratio, though the\nproof is much more involved than the algorithm itself.\nAlgorithms with policy predictions. RobustFtP stands\nforRobust Follow-the-Prediction . It is based on essentially\nthe same algorithmic idea as BlindOracle – i.e. apply a\ncombiner to (1) robust MARKER and (2) consistent following\nthe predictions – but in the alternative prediction setup and\nwith a quite different theoretical analysis.\nTrust&Doubt utilizes predictions in a more intricate way in\norder to achieve better dependence on the prediction error\n\u0011cache . Similar to PredictiveMarker and LMarker, it is ro-\nbust out-of-the-box and hence does not require a combiner.\n2.3. Predictor\nWe couple the learning-augmented algorithms with the state-\nof-the-art predictors. Speciﬁcally, Liu et al. (2020) proposed\nParrot, an imitation learning algorithm that tries to mimic\nBelady’s oracle policy with a deep neural network.\nAt each step, the model is input the last H= 30 requested\nitems (including the current one) and the k= 16 items\nthat are at the time in the cache (in the relevant set). All\nthese items are encoded to ﬁxed-size embedding, and the re-\nquested items are additionally processed by LSTM (Hochre-\niter & Schmidhuber, 1997), which is continuously fed with\nthem one by one. That leads to obtaining H+kembed-dings, which are further processed with attention (speciﬁ-\ncally, Transformer (Vaswani et al., 2017) and BiDAF (Seo\net al., 2017)), which ﬁnally outputs one vector per cache\nitem. These elements are passed through a linear layer with\nsoftmax to form a prediction.\nThe model uses two different prediction heads, which are\noptimized simultaneously during training. The ﬁrst one\npredicts which element would be evicted from the cache\nby Belady’s oracle and deﬁnes the PARROT -CACHE predic-\ntor, which is exactly what the RobustFtP and Trust&Doubt\nalgorithms need.\nThe second head estimates, for each element in the cache,\nthe number of steps before the element is requested again.\nWe use that estimate for the last requested item to con-\nstruct the second predictor, which we call PARROT -REUSE .\nSuch predictions are compatible with the PredictiveMarker,\nLMarker, LNonMarker, and BlindOracle algorithms.\n3. Datasets\nOur datasets come from the 2nd Cache Replacement Cham-\npionship (CRC, 2017) and consist of real-world memory\naccess traces from the SPEC CPU2006 benchmark (Hen-\nning, 2006).\nThere are two ways to obtain the traces to begin with. One\ncan either download the traces released in the Cache Re-\nplacement Championship or collect custom traces using\na dynamic binary instrumentation tool DynamoRIO (Bruen-\ning et al., 2003). Liu et al. (2020) used the second method to\nevaluate Parrot and shared their procedure, but they were un-\nable to release their exact traces. We follow this procedure\nto create datasets from the publicly available traces (CRC,\n2017). In particular, we subsample the traces by choosing\n64 out of 2048 sets4and ﬁltering the accesses5to those sets.\nThe ﬁrst 80% of this sequence is used for training, followed\n4Aset-associative cache is split into nsets, each holding k\nitems, called lines . Each line’s address uniquely predetermines\nthe set it can be cached in. In that sense, each trace constitutes n\nindependent instances of the caching problem. (Here k= 16 .)\n5Following Shi et al. (2019) and Liu et al. (2020) we evaluate\nour approach on the last level of a three-level cache hierarchy.\n\nRobust Learning-Augmented Caching: An Experimental Study\nby 10% used for validation, and the last 10% for testing. We\nrefer to Table 2 for details of the datasets.\nWe noticed that even slight differences in the data collec-\ntion and postprocessing might create datasets of signiﬁ-\ncantly different characteristics. To account for that, we have\nreevaluated Parrot on our inputs, and made them publicly\navailable at https://github.com/chledowski/Robust-Learning-\nAugmented-Caching-An-Experimental-Study-Datasets.\n4. Experimental Setup\nFor both Parrot and the learning-augmented algo-\nrithms, we use the code made available by Liu et al.\n(2020) and Antoniadis et al. (2020), respectively. We\nuniﬁed both codebases and connected them into a sin-\ngle pipeline. The source code is publicly available at\nhttps://github.com/chledowski/ml_caching_with_guarantees.\nDue to constrained computing resources, we limited the\nnumber of training steps to 20 000 . The second change is\nto ablate DAgger (Ross et al., 2011), as it did not yield\nany improvements in our case. No other Parrot’s hyper-\nparameter was changed.\nAs a result, our models are trained for 20 000 steps on each\ndataset, with a batch size of 32. The best model is chosen to\nbe the one with the highest validation cache hit rate among\nthe evaluated checkpoints (done every 5000 steps).\nTo measure the practicality of learning-augmented caching\nalgorithms, we will weaken the Parrot model by training\nonly on preﬁxes (e.g. 1%) of the original datasets. We will\nanalyze how the learning-augmented algorithms perform\nunder such a change.\nAs we mention in Section 2.2, each of the algorithms im-\nplementing a combiner (LNonMarker, BlindOracle, and\nRobustFtP) have four variants, using the deterministic or\nthe randomized combiner, with MARKER or with LRU. We\nnoticed that MARKER and LRU perform on par with each\nother on most of the datasets that we use. Hence, for sim-\nplicity, we analyze only the two variants with MARKER\n(deterministic and randomized). For completeness, we also\nran experiments with LRU, but as expected the choice turns\nout to be irrelevant.\n5. Results\nIn this section, we analyze our experiments on the practical-\nity of the learning-augmented algorithms. We begin with\nfull training sets to assess algorithms’ overhead over fully-\nﬂedged predictors. Then we move to predictors trained on\nonly1%of data to check whether the algorithms are robust\nin practice. We end with a closer look at two datasets, which\nwe further subsample to better illustrate the behavior of thealgorithms coupled with predictors of varying accuracy.\nTo make comparison across datasets easier, we normalize\nscores to both LRU and Belady’s OPT at the same time.\nSpeciﬁcally, for an algorithm ALG with empirical competi-\ntive ratio6crALG, we report the LRU-normalized empirical\ncompetitive ratio , i.e.,\ncrALG\u00001\ncrLRU\u00001:\nThe lower the value, the better, meaning that the algorithm’s\nperformance is closer to Belady’s optimal oracle. For the\nsake of completeness, the raw unnormalized scores are in-\ncluded in the supplementary material.\n5.1. State-of-the-art predictor\nIn order to be applicable in practice, a learning-augmented\ncaching algorithm should have a low overhead. In other\nwords, when the underlying predictor is accurate, the ﬁnal\nperformance should be as close to the predictor’s perfor-\nmance as possible. In our ﬁrst experiment, we aim to test\nwhich of the learning-augmented caching algorithms fulﬁll\nthis key requirement.\nTo this end, for each dataset considered, we train the Parrot\nmodel on the full training set, which leads to obtaining two\nstate-of-the-art predictors – PARROT -REUSE and PARROT -\nCACHE . Then, we couple each learning-augmented caching\nalgorithm with one of the predictors, depending on the na-\nture of the algorithm. The results are presented in Figure 1.\nThe bars correspond to the augmented algorithms, while\nhorizontal lines reﬂect the performance of non-augmented\nmethods. PARROT -CACHE is almost always better than the\nclassical LRU baseline and even approaches optimal behav-\nior for a few datasets. PARROT -REUSE performs signiﬁ-\ncantly worse than PARROT -CACHE and even lags behind\nLRU in a few cases. It makes the use of algorithms coupled\nwith PARROT -CACHE preferable, at least with the currently\nbest available predictors.\nWhen a predictor is better than MARKER , all learning-\naugmented algorithms leverage its accurate predictions to\nimprove over the classical baseline.\nHowever, only the two simplest algorithms – BlindOracleD\nand RobustFtPD– have overheads over predictors low\nenough to be considered practical. Note that these two\nare based on the same rule, just applied in two different\nsettings. The rule is to blindly follow either a predic-\ntor or MARKER , and switch whenever one heavily outper-\nforms the other (see Section 2.2). The versions of these\n6The empirical competitive ratio of the algorithm ALG is\ndeﬁned as crALG = cost ALG=costOPT , where costALG de-\nnotes the number of cache misses that algorithm ALG incurs\non a dataset.\n\nRobust Learning-Augmented Caching: An Experimental Study\nastar bwaves bzip cactusadm gems lbm leslie libq mcf milc omnetpp sphinx xalancOPTLRU\n0.20.40.60.81.21.41.6MARKER\nPARROT-REUSE\nPARROT-CACHEPredictiveMarker\nLMarker\nLNonMarkerRLNonMarkerD\nBlindOracleR\nBlindOracleDTrust&Doubt\nRobustFtPR\nRobustFtPD\nFigure 1. Normalized cost (the lower the better) of learning-augmented algorithms compared to coupled predictors and classical\nalgorithms. The bars reﬂect the performance of the augmented algorithms, while horizontal lines correspond to non-augmented\nmethods. The scores reﬂect LRU-normalized empirical competitive ratio (deﬁned in Section 5). The predictor PARROT -CACHE (blue line)\noutperforms PARROT -REUSE (red line) and even approaches optimal policy for a few datasets. MARKER (black line) is comparable to LRU.\nThe best augmented methods are RobustFtPD(yellow bar) and BlindOracleD(brown bar) which have very low overhead while stay robust\nwhen the coupled predictor performs worse than MARKER . RobustFtPDis, however, coupled with better predictors and hence signiﬁcantly\noutperforms BlindOracleD. The remaining augmented algorithms are overly conservative and hence fail to leverage good predictions.\nastar\n(1%)bwaves\n(1%)bzip\n(1%)cactusadm\n(1%)gems\n(1%)lbm\n(1%)leslie3d\n(1%)libq\n(1%)mcf\n(1%)milc\n(1%)omnetpp\n(1%)sphinx3\n(1%)xalanc\n(1%)OPTLRU\n0.20.40.60.81.21.41.6MARKER\nPARROT-REUSE\nPARROT-CACHEPredictiveMarker\nLMarker\nLNonMarkerRLNonMarkerD\nBlindOracleR\nBlindOracleDTrust&Doubt\nRobustFtPR\nRobustFtPD\nFigure 2. Normalized cost (the lower the better). Underperforming predictors trained on 1% of data. The predictors (red and blue\nlines) signiﬁcantly outperform MARKER (black line) for only 4 datasets due to limited training data. Augmented algorithms (bars) are\nrobust – they approach MARKER (black line), even when coupled predictors are wildly inaccurate. The robustness of RobustFtPR(violet\nbar) and BlindOracleR(gray bar) seems the weakest (especially for bzip and xalanc datasets). At the same time, RobustFtPD(yellow bar)\nand BlindOracleD(brown bar) prove to be the best, as they are able to leverage accurate predictions, while the remaining augmented\nalgorithms are overly conservative. Predictors’ scores for bzip and xalanc are not shown as they are above 2.0. Most augmented algorithms,\nhowever, still perform comparably to MARKER for these two datasets.\n\nRobust Learning-Augmented Caching: An Experimental Study\nastar bwaves bzip cactusadm gems lbm leslie3d libq mcf milc omnetpp sphinx3 xalancOPTLRU\n0.20.40.60.81.21.41.6MARKER\nPARROT-REUSE\nPARROT-CACHEPredictiveMarker\nLMarker\nLNonMarkerRLNonMarkerD\nBlindOracleR\nBlindOracleDTrust&Doubt\nRobustFtPR\nRobustFtPD\n0.01% 0.10% 1% 10% 100%OPTLRU\n0.20.40.60.81.21.41.61.82.02.2\nFigure 3. Normalized cost (the lower the better). A closer look\nat mcf subsamples. Performance of the PARROT -REUSE and\nPARROT -CACHE predictors improve along with the growing size of\nthe trained dataset, while the two best augmented algorithms in the\nrespective setups are able to follow the minimum of the results of\nthe combined predictor and classical algorithms.\n0.1% 1% 10% 100%OPTLRU\n0.20.40.60.8Figure 4. Normalized cost (the lower the better). A closer look\nat sphinx3 subsamples. The PARROT -REUSE is more robust to the\nscarcity of data when trained on subsamples of sphinx3. The predic-\ntors improve along with the growing size of the trained dataset. The\naugmented algorithms perform signiﬁcantly better than MARKER ,\nbut only RobustFtPDand BlindOracleDapproach the predictor.\nmethods with randomized combinations (i.e., BlindOracleR\nand RobustFtPR) seem to overly rely on MARKER , and their\nperformance is clearly worse.\nAll remaining methods offer only a slight improvement\nover MARKER , even if the predictor excels. These complex\nstrategies on how to use predictions, developed to prove\nworst-case bounds, do not work in practice for the analyzed\ncases.\nIn a nutshell, PARROT -CACHE signiﬁcantly outperforms all\nalgorithms and RobustFtPDuses it with only a small over-\nhead. We will check how these methods work when coupled\nwith underperforming predictors in the next subsection.\n5.2. Underperforming predictor\nAs mentioned before, we impair the training procedure to\nobtain underperforming predictors. Speciﬁcally, we heavily\nsubsample the training sets, leaving the ﬁrst 1% of requests\navailable to the models. The results are presented in Fig-\nure 2.\nInterestingly, even when trained on only 1% of the data,\nPARROT -REUSE and PARROT -CACHE are still sometimes\nable to ﬁnd policies better than MARKER . In these cases, the\nbest learning-augmented caching algorithms perform better\nthan MARKER , with a small overhead, similar to the results\nin the previous subsection.\nIn most cases, however, the predictors overﬁt to severely\nlimited training data, and the resulting caching strategy\nis no better than MARKER . Both PARROT -CACHE and\nPARROT -REUSE lag behind the classical method for a few\ntasks (bzip, gems, leslie3d, omnetpp and xalanc). In thesecases, the augmented algorithms – with the exception of\nBlindOracleRand RobustFtPRfor two datasets – perform\ncomparably with MARKER , which empirically proves their\nrobustness. Notably, the robustness of the two simplest\nmethods (BlindOracleDand RobustFtPD), shown in the\nprevious subsection to be much better at leveraging good\npredictions, is comparable to the rest of the algorithms.\nIn short, together with conclusions from the previous sub-\nsection, the two methods closely track the better of their\ntwo components – the predictor or MARKER . The remain-\ning methods add too much overhead when coupled with\nwell-performing predictors, while it does not result in better\nrobustness.\n5.3. A closer look into speciﬁc datasets\nTo illustrate changes in the performance of the compared\nalgorithms, we further subsample the largest dataset in our\nsuite – mcf – in order to obtain series of training sets of\nvarying sizes from 0.01% to 100% of the original size.\nWe train PARROT -REUSE and PARROT -CACHE on each of\nthem, and then couple augmented algorithms. The results\nare present in Figure 3.\nAs expected, the performance of the PARROT -CACHE and\nPARROT -REUSE models improves along with the growth of\nthe available data. We can observe that the performance of\nall the augmented algorithms continues to improve along\nwith the performance of the coupled predictor. However,\nwhen the predictor underperforms MARKER , the perfor-\nmances of the augmented algorithms remain on par with\nthe classical algorithm, with a notable exception of the\n\nRobust Learning-Augmented Caching: An Experimental Study\nTrust&Doubt, which is clearly the worst.\nHowever, as the PARROT -CACHE and PARROT -REUSE pre-\ndictors start to outperform LRU and MARKER , the perfor-\nmance of the combining algorithms improves along with\nthem. The most notable result here is how closely the\nRobustFtPDfollows improvements in the performance of\nPARROT -CACHE , while overhead for remaining algorithms\nremains signiﬁcantly larger.\nAt the ﬁrst glance it might be surprising that sometimes, e.g.\nmcf (1%), augmented algorithms outperform both MARKER\nand their coupled predictors. However, an augmented algo-\nrithm can take advantage of the fact that each of its ingredi-\nents may perform better on a different part of the dataset.\nAnother dataset that we investigate in more detail is sphinx3.\nAs it is smaller than mcf, we can only train models on\nthe range from 0.1% to 100%. Interestingly here on small\nsubsamples (0.1% and 1%) the PARROT -REUSE performs\nbetter than PARROT -CACHE , suggesting it might be more\nrobust to scarcity of training data. After careful examination\nthe same trend can be observed comparing Figures 1 and 2.\nSince even the weakest predictors easily outperform\nMARKER on this dataset, the learning-augmented algorithms\nperform similar to what we see already in Section 5.1.\n5.4. Towards performance explanation\nTo better understand the differences in performance between\nthe algorithms, we ran additional experiments (on a subset\nof datasets) in which we measured how much the algorithms\nfollowed the predictors. That notion is straightforward for\nBlindOracle and RobustFtP, as they, in each time step, either\ndo exactly what the predictor advises, or follow MARKER ,\nwhich in turn is completely independent of the predictor.\nHowever, the notion becomes more subtle for other algo-\nrithms, which, e.g., apply predictions only to a varying\nsubset of items. To overcome that difﬁculty, we used a mea-\nsure of prediction usage , which is algorithm independent.\nSpeciﬁcally, for each algorithm, we computed the Jaccard\nsimilarity between the caches maintained by the algorithm\nand the predictor (if followed blindly), averaged over time.\nOur general conclusion is that algorithms’ performance is\ncorrelated with how much they choose to follow the underly-\ning predictor. The correlation is positive on datasets where\nthe respective predictor performs better than MARKER , and\nnegative otherwise, see Figure 5. Most algorithms correctly\ndecide for each dataset whether it is on average better to\nfollow the predictor or not. It is, however, the level of com-\nmitment to that decision that differs: With fully-ﬂedged\npredictors, deterministic BlindOracleDand RobustFtPDfol-\nlowed them >97% of the time, randomized BlindOracleR\nand RobustFtPR– around 90% of the time, and remaining\nalgorithms – at most 85%, often much less. For most algo-\n0.0 0.2 0.4 0.6 0.8 1.0\nPrediction usageOPTLRU\n0.20.40.60.81.21.41.6Normalized competitive ratio\nMarker is better\nPredictor is betterFigure 5. Prediction usage . Each data point represents a pair\n(dataset, learning-augmented algorithm). Orange dots correspond\nto pairs such that MARKER performed better on that dataset than\nthe predictor used by that algorithm (if followed blindly). Green\nstars correspond to pairs where the predictor was better. See Sec-\ntion 5.4 for the deﬁnition of prediction usage . Intuitively, a good\nlearning-augmented algorithm should have a large prediction usage\nfor green stars and a small one for orange dots.\n0 500 1000 1500 2000 2500PredictorMarker\nastar 100% (predictors are better than Marker)\n0 1000 2000 3000 4000 5000 6000\nTimePredictorMarker\nmcf 0.01% (Marker is better than predictors)BlindOracleR\nBlindOracleDRobustFtPR\nRobustFtPD\nFigure 6. Switching behavior of the combiners . For each time\nstep, we draw a point in the upper part of the subplot if the com-\nbiner follows MARKER , and in the lower part if it follows the\npredictor. For astar 100% dataset, the deterministic combiner, af-\nter a few switches, quickly infers that the predictor is better than\nMARKER and follows it until the end. On the other hand, the\nrandomized combiner needs more time to make a decision. As\na result, even though it follows the predictor most of the time,\nthe initial hesitancy jeopardizes its overall performance. For mcf\n0.01% dataset, where the predictors are outperformed by MARKER ,\nthe deterministic combiner again outperforms randomized variant,\nas it quickly identiﬁes what to follow.\n\nRobust Learning-Augmented Caching: An Experimental Study\nrithms, how much the algorithm follows the predictor seems\nto depend on the predictor’s performance – the better the\npredictor, the more it is followed. Only for Trust&Doubt\nthese two numbers seem uncorrelated, which may explain\nits lower robustness.\nNext, we analyzed the switching behavior of the combiners,\nsee Figure 6. The deterministic combiner, after few brief\nswitches, quickly infers what to follow for each dataset. The\nrandomized combiner also eventually follows the better of\nthe two underlying algorithms but needs much more time to\nﬁgure out that it has to switch, which jeopardizes its overall\nperformance. That explains why randomized BlindOracleR\nand RobustFtPRtend to perform worse than their determin-\nistic counterparts, BlindOracleDand RobustFtPD.\n6. Related Work\nLearning-augmented algorithms. The idea of augment-\ning an algorithm with hints or predictions coming from a\npotentially untrusted oracle is not new. The recent variant,\nclearly inspired by the now omnipresent machine-learned\npredictors for various tasks, seems to spark from Lykouris\n& Vassilvitskii (2018) and Purohit et al. (2018). The idea\nhas been applied to many problems, also beyond the online\nalgorithms, e.g. to Bloom ﬁlters (Kraska et al., 2018). For an\noverview of the ﬁeld, see the recent survey by Mitzenmacher\n& Vassilvitskii (2021).\nRobust machine learning. Robustness of machine learn-\ning methods is sometimes deﬁned as robustness to adversar-\nial examples – approximately estimated worst-case inputs\nlaying controllably far from training distribution (Carlini\n& Wagner, 2017; Weng et al., 2019; Szegedy et al., 2014).\nMore broadly, however, robustness in machine learning can\nbe seen as an ability to generalize, i.e., to perform well\non unseen examples (Bishop, 2007), where a distribution\nshift between training and testing examples poses a chal-\nlenge (Moreno-Torres et al., 2012; Geirhos et al., 2020).\nWe experiment with heterogeneous sequential datasets (Hen-\nning, 2006; CRC, 2017). Their characteristics change over\ntime, as they are generated by real-world programs. We\nleverage this property to test generalization ability – and\nhence robustness – of state-of-the-art machine learning pre-\ndictors (Liu et al., 2020). We vary amount of data available\nduring training to analyze pessimistic cases and use learning-\naugmented algorithms to incorporate robustness to caching\npolicies based on neural network predictions.\n7. Conclusions\nWe ﬁll a critical gap in the learning-augmented literature.\nWe evaluated the learning-augmented caching algorithms\nusing the state-of-the-art predictors on real-world datasets.In a nutshell, we conclude that learning-augmented algo-\nrithms can have only a low overhead over a well-performing\npredictor, while competing with classical methods when the\ncoupled predictor fails, thus providing a cheap worst-case\ninsurance.\nOur experiments show that when the training data is scarce,\nthe performance of the state-of-the-art Parrot model tends\nto degrade quickly, depending on the dataset. Hence, it\njustiﬁes looking for a way to beneﬁt from the robustness\nof the classical online algorithms. As learning augmented\nalgorithms do exactly that, we test their performance in\npractice.\nTwo algorithms – BlindOracleDand RobustFtPD– turn\nout to be the best. They provide a very low overhead over\ngood predictions but still compete with the robust classical\nmethods even when the predictors fail. The remaining four\ntested algorithms are robust, but they seem to be overly\nconservative and do not fully utilize good predictions.\nWe show that the theoretical asymptotic competitive ratio\nis not a good proxy for the practical performance of the\nlearning-augmented algorithms. In the reuse distance setup,\nit correctly points to the leader but incorrectly distinguishes\nbetween the remaining algorithms. In the policy setup, the\ntheoretically inferior algorithm turns out the best in practice.\nMoreover, according to the theoretical analysis, the random-\nized combiner should perform better than the deterministic\none, while in our experiments we observe the opposite.\nOn most datasets, the predictor for the optimal policy\nsetup outperforms the predictor for the reuse distance setup.\nHence, learning-augmented algorithms in the latter setup\ncan hardly compete with the RobustFtPD, designed for the\nformer. That conclusion is valid with respect to current state-\nof-the-art predictors, and future improvements to reuse dis-\ntance predictors may invalidate it. However, in the view of\nour results, a direct empirical comparison between the two\nalternative setups becomes less relevant. Indeed, theoretical\ndevelopments in both setups independently led to the same\nalgorithmic idea behind BlindOracleDand RobustFtPD. The\nidea excels in practice and can presumably be applied to\nany type of predictor. As the examples of BlindOracleR,\nRobustFtPR, and Trust&Doubt show, optimizing the meth-\nods further towards an objective speciﬁc to a particular\nsetup does not necessarily lead to improved performance\nin practice.\nAcknowledgements\nAdam Polak was supported by the Swiss National Sci-\nence Foundation (SNF) within the project Lattice Algo-\nrithms and Integer Programming (185030) . Konrad ˙Zołna\nwas supported by the National Science Center, Poland\n(2017/27/N/ST6/00828, 2018/28/T/ST6/00211).\n\nRobust Learning-Augmented Caching: An Experimental Study\nReferences\nThe 2nd Cache Replacement Championship, 2017. URL\nhttps://crc2.ece.tamu.edu/ .\nAntoniadis, A., Coester, C., Eliás, M., Polak, A., and Simon,\nB. Online metric algorithms with untrusted predictions.\nInInternational Conference on Machine Learning , 2020.\nBelady, L. A. A study of replacement algorithms for virtual-\nstorage computer. IBM Systems Journal , 1966. doi:\n10.1147/sj.52.0078.\nBishop, C. M. Pattern recognition and machine learning,\n5th Edition . Springer, 2007.\nBlum, A. and Burch, C. On-line learning and the metrical\ntask system problem. Machine Learning , 2000. doi:\n10.1023/A:1007621832648.\nBruening, D., Garnett, T., and Amarasinghe, S. An infras-\ntructure for adaptive dynamic optimization. In Interna-\ntional Symposium on Code Generation and Optimization:\nFeedback-Directed and Runtime Optimization , 2003. doi:\n10.1109/CGO.2003.1191551.\nCarlini, N. and Wagner, D. A. Towards evaluating the\nrobustness of neural networks. In IEEE Symposium on\nSecurity and Privacy , 2017. doi: 10.1109/SP.2017.49.\nFiat, A., Karp, R. M., Luby, M., McGeoch, L. A., Sleator,\nD. D., and Young, N. E. Competitive paging algorithms.\nJournal of Algorithms , 1991. doi: 10.1016/0196-6774(91)\n90041-V.\nFiat, A., Rabani, Y ., and Ravid, Y . Competitive k-server\nalgorithms. Journal of Computer and System Sciences ,\n1994. doi: 10.1016/S0022-0000(05)80060-1.\nGeirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Bren-\ndel, W., Bethge, M., and Wichmann, F. Shortcut learning\nin deep neural networks. Nature Machine Intelligence ,\n2020. doi: 10.1038/s42256-020-00257-z.\nHenning, J. L. SPEC CPU2006 benchmark descriptions.\nSIGARCH Computer Architecture News , 2006. doi: 10.\n1145/1186736.1186737.\nHochreiter, S. and Schmidhuber, J. Long short-term memory.\nNeural Computation , 1997. doi: 10.1162/neco.1997.9.8.\n1735.\nJain, A. and Lin, C. Back to the future: Leveraging Belady’s\nalgorithm for improved cache replacement. In IEEE/ACM\nAnnual International Symposium on Computer Architec-\nture, 2016. doi: 10.1109/ISCA.2016.17.Kraska, T., Beutel, A., Chi, E. H., Dean, J., and Polyzotis,\nN. The case for learned index structures. In SIGMOD\nInternational Conference on Management of Data , 2018.\ndoi: 10.1145/3183713.3196909.\nLiu, E. Z., Hashemi, M., Swersky, K., Ranganathan, P.,\nand Ahn, J. An imitation learning approach for cache\nreplacement. In International Conference on Machine\nLearning , 2020.\nLykouris, T. and Vassilvitskii, S. Competitive caching with\nmachine learned advice. In International Conference on\nMachine Learning , 2018.\nMitzenmacher, M. and Vassilvitskii, S. Algorithms with\nPredictions . Cambridge University Press, 2021. doi:\n10.1017/9781108637435.037.\nMoreno-Torres, J. G., Raeder, T., Alaíz-Rodríguez, R.,\nChawla, N. V ., and Herrera, F. A unifying view on dataset\nshift in classiﬁcation. Pattern Recognition , 2012. doi:\n10.1016/j.patcog.2011.06.019.\nPurohit, M., Svitkina, Z., and Kumar, R. Improving online\nalgorithms via ML predictions. In Advances in Neural\nInformation Processing Systems , 2018.\nRohatgi, D. Near-optimal bounds for online caching\nwith machine learned advice. In ACM-SIAM Sympo-\nsium on Discrete Algorithms , 2020. doi: 10.1137/1.\n9781611975994.112.\nRoss, S., Gordon, G., and Bagnell, D. A reduction of\nimitation learning and structured prediction to no-regret\nonline learning. In International Conference on Artiﬁcial\nIntelligence and Statistics , 2011.\nSeo, M. J., Kembhavi, A., Farhadi, A., and Hajishirzi, H.\nBidirectional attention ﬂow for machine comprehension.\nInInternational Conference on Learning Representations ,\n2017.\nShi, Z., Huang, X., Jain, A., and Lin, C. Applying deep\nlearning to the cache replacement problem. In IEEE/ACM\nInternational Symposium on Microarchitecture , 2019.\ndoi: 10.1145/3352460.3358319.\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan,\nD., Goodfellow, I. J., and Fergus, R. Intriguing proper-\nties of neural networks. In International Conference on\nLearning Representations , 2014.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Atten-\ntion is all you need. In Advances in Neural Information\nProcessing Systems , 2017.\n\nRobust Learning-Augmented Caching: An Experimental Study\nWei, A. Better and simpler learning-augmented online\ncaching. In Approximation, Randomization, and Combi-\nnatorial Optimization. Algorithms and Techniques , 2020.\ndoi: 10.4230/LIPIcs.APPROX/RANDOM.2020.60.\nWeng, L., Chen, P.-Y ., Nguyen, L., Squillante, M., Boopathy,\nA., Oseledets, I., and Daniel, L. PROVEN: Verifying ro-\nbustness of neural networks with a probabilistic approach.\nInInternational Conference on Machine Learning , 2019.\nYan, G. and Li, J. RL-Bélády: A uniﬁed learning framework\nfor content caching. In ACM International Conference\non Multimedia , 2020. doi: 10.1145/3394171.3413524.\n\nRobust Learning-Augmented Caching: An Experimental Study\nA. Supplementary material\nAdditional information about experiments. We ran the\ntotal of 53 trainings and evaluations. All but two runs de-\nscribed below were run for 20 000 steps, with evaluation\nevery 5000 steps.\nFor datasets from {astar, bwaves, bzip, cactusadm, gems,\nleslie3d, omnetpp, sphinx3}, we ran 4 experiments on the\nﬁrst 0.1%, 1%, 10% and 100% records of the dataset.\nFor datasets from {lbm, libq, milc, xalanc} we ran only two\nexperiments, on 1% and 100% of the dataset.\nFor the mcf dataset, we ran 11 experiments in total. The\nﬁrst ﬁve of them were runs on 0.01%, 0.1%, 1%, 10% and\n100% of the dataset, without DAgger. For comparison,\nwe ran additional six experiments with a larger number of\nsteps and with DAgger. We achieved comparable cache hit\nrates, and therefore we did not evaluate learning-augmented\nalgorithms on that additional data.\nOn average, each experiment took around one day on a Tesla\nV100 GPU, occupying at most 3GB of the GPU memory.\nFull results. We present the full cache hit rates of PARROT -\nREUSE across all evaluated datasets and fractions in Table 3.\nThe raw unnormalized competitive ratios are shown in Ta-\nble 4.\n\nRobust Learning-Augmented Caching: An Experimental Study\nTable 3. Cache hit rates of PARROT -CACHE .Detailed results.\nDATASET FRACTION\nDAGGER\nSTEPS\nEVAL FREQ\nCACHE REQUESTS\nCACHE REQUESTS IN TRAIN SET\nHITRATE CKPT 0ON VAL\nBEST NON 0CHECKPOINT\nHITRATE CKPT BEST ON VAL\nLAST CHECKPOINT\nHITRATE CKPT LAST ON VAL\nHITRATE CKPT BEST NON 0ON TEST\nASTAR 0.1% FALSE 20001 5000 640,032 1,154 8.25% 5000 13.32% 20000 11.83% 14.05%\nASTAR 1% FALSE 20001 5000 640,032 11,540 8.25% 5000 25.09% 20000 24.55% 25.75%\nASTAR 10% FALSE 20001 5000 640,032 115,404 8.25% 10000 30.26% 20000 30.05% 30.99%\nASTAR 100% FALSE 20001 5000 640,032 1,154,048 8.25% 20000 31.53% 20000 31.53% 32.17%\nBWAVES 0.1% FALSE 20001 5000 640,032 570 0.02% 5000 0.03% 20000 0.02% 0.00%\nBWAVES 1% FALSE 20001 5000 640,032 5,703 0.02% 10000 1.93% 20000 1.62% 0.30%\nBWAVES 10% FALSE 20001 5000 640,032 57,036 0.02% 20000 0.03% 20000 0.03% 0.00%\nBWAVES 100% FALSE 20001 5000 640,032 570,368 0.02% 10000 3.21% 20000 1.21% 0.32%\nBZIP 0.1% FALSE 20001 5000 640,032 167 52.05% 20000 48.02% 20000 48.02% 53.23%\nBZIP 1% FALSE 20001 5000 640,032 1,676 52.05% 5000 42.69% 20000 41.94% 45.12%\nBZIP 10% FALSE 20001 5000 640,032 16,768 52.05% 5000 57.09% 20000 52.84% 64.28%\nBZIP 100% FALSE 20001 5000 640,032 167,680 52.05% 5000 63.50% 20000 61.29% 68.44%\nCACTUSADM 0.1% FALSE 20001 5000 640,032 221 0.32% 20000 4.81% 20000 4.81% 0.45%\nCACTUSADM 1% FALSE 20001 5000 640,032 2,219 0.32% 15000 8.43% 20000 8.42% 20.16%\nCACTUSADM 10% FALSE 20001 5000 640,032 22,195 0.32% 5000 25.88% 20000 24.17% 30.93%\nCACTUSADM 100% FALSE 20001 5000 640,032 221,952 0.32% 5000 25.30% 20000 24.98% 32.97%\nGEMS 0.1% FALSE 20001 5000 640,032 723 2.96% 15000 3.14% 20000 3.11% 3.09%\nGEMS 1% FALSE 20001 5000 640,032 7,234 2.96% 20000 3.12% 20000 3.12% 3.09%\nGEMS 10% FALSE 20001 5000 640,032 72,345 2.96% 20000 3.13% 20000 3.13% 3.08%\nGEMS 100% FALSE 20001 5000 640,032 723,456 2.96% 20000 1.77% 20000 1.77% 3.10%\nLBM 1% FALSE 20001 5000 640,032 7,820 0.02% 5000 0.19% 20000 0.13% 0.00%\nLBM 100% FALSE 20001 5000 640,032 782,080 0.02% 5000 1.22% 20000 0.29% 0.00%\nLESLIE 3D 0.1% FALSE 20001 5000 640,032 716 0.81% 15000 1.29% 20000 1.01% 9.10%\nLESLIE 3D 1% FALSE 20001 5000 640,032 7,160 0.81% 5000 1.62% 20000 1.21% 9.31%\nLESLIE 3D 10% FALSE 20001 5000 640,032 71,603 0.81% 5000 2.09% 20000 1.00% 11.73%\nLESLIE 3D 100% FALSE 20001 5000 640,032 716,032 0.81% 5000 4.99% 20000 4.89% 11.41%\nLIBQ 1% FALSE 20001 5000 640,032 5,798 0.00% 5000 0.00% 20000 0.00% 0.00%\nLIBQ 100% FALSE 20001 5000 640,032 579,840 0.00% 10000 0.01% 20000 0.00% 0.01%\nMCF 0.01% FALSE 20001 5000 640,032 296 2.61% 5000 15.79% 20000 11.02% 15.27%\nMCF 0.01% TRUE 20001 5000 640,032 296 2.61% 5000 15.79% 20000 11.02% 15.28%\nMCF 0.1% FALSE 20001 5000 640,032 2,965 2.61% 15000 22.19% 20000 22.16% 27.33%\nMCF 0.1% TRUE 20001 5000 640,032 2,965 2.61% 10000 21.64% 20000 21.40% 27.02%\nMCF 1% FALSE 20001 5000 640,032 29,655 2.61% 5000 40.00% 20000 39.44% 41.31%\nMCF 1% TRUE 20001 5000 640,032 29,655 2.61% 5000 40.35% 20000 39.93% 41.42%\nMCF 10% FALSE 20001 5000 640,032 296,550 2.61% 5000 42.09% 20000 41.49% 43.45%\nMCF 100% FALSE 20001 5000 640,032 2,965,504 2.61% 20000 43.05% 20000 43.05% 43.89%\nMCF 100% TRUE 20001 5000 640,032 2,965,504 2.61% 15000 42.65% 20000 42.49% 43.46%\nMCF 100% FALSE 120001 30000 3,840,032 2,965,504 2.61% 30000 43.10% 120000 43.10% 43.89%\nMCF 100% TRUE 120001 30000 3,840,032 2,965,504 2.61% 30000 42.60% 120000 42.60% 43.11%\nMILC 1% FALSE 20001 5000 640,032 5,568 0.22% 10000 0.23% 20000 0.23% 0.01%\nMILC 100% FALSE 20001 5000 640,032 556,800 0.22% 5000 0.34% 20000 0.23% 0.01%\nOMNETPP 0.1% FALSE 20001 5000 640,032 555 8.24% 5000 16.92% 20000 16.60% 17.90%\nOMNETPP 1% FALSE 20001 5000 640,032 5,555 8.24% 10000 13.85% 20000 13.62% 16.85%\nOMNETPP 10% FALSE 20001 5000 640,032 55,552 8.24% 5000 18.60% 20000 14.83% 19.28%\nOMNETPP 100% FALSE 20001 5000 640,032 555,520 8.24% 5000 22.23% 20000 15.46% 21.91%\nSPHINX 3 0.1% FALSE 20001 5000 640,032 328 36.60% 20000 36.33% 20000 36.33% 53.78%\nSPHINX 3 1% FALSE 20001 5000 640,032 3,287 36.60% 5000 43.84% 20000 42.71% 59.48%\nSPHINX 3 10% FALSE 20001 5000 640,032 32,870 36.60% 5000 45.68% 20000 45.49% 66.55%\nSPHINX 3 100% FALSE 20001 5000 640,032 328,704 36.60% 20000 55.18% 20000 55.18% 70.54%\nXALANC 1% FALSE 20001 5000 640,032 691 14.73% 15000 15.02% 20000 14.46% 32.38%\nXALANC 100% FALSE 20001 5000 640,032 69,120 14.73% 5000 27.01% 20000 26.42% 49.71%\n\nRobust Learning-Augmented Caching: An Experimental Study\nTable 4. Competitive ratios. Detailed results.\nDATASET FRACTION\nOPT\nLRU\nMARKER\nRANDOM\nPARROT -REUSE\nPREDICTIVE MARKER\nLMARKER\nLNONMARKER R\nLNONMARKER D\nBLIND ORACLE R\nBLIND ORACLE D\nPARROT -CACHE\nTRUST &D OUBT\nROBUST FTPR\nROBUST FTPD\nASTAR 0.1% 1.00 1.53 1.52 1.47 1.26 1.51 1.51 1.51 1.51 1.32 1.27 1.37 1.53 1.42 1.38\nASTAR 1% 1.00 1.53 1.52 1.47 1.16 1.50 1.50 1.50 1.49 1.22 1.17 1.19 1.47 1.24 1.19\nASTAR 10% 1.00 1.53 1.52 1.47 1.15 1.50 1.50 1.50 1.49 1.19 1.15 1.10 1.45 1.14 1.11\nASTAR 100% 1.00 1.53 1.52 1.46 1.14 1.50 1.50 1.50 1.49 1.19 1.14 1.08 1.45 1.14 1.09\nBWAVES 0.1% 1.00 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05\nBWAVES 1% 1.00 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05\nBWAVES 10% 1.00 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05\nBWAVES 100% 1.00 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05\nBZIP 0.1% 1.00 1.88 1.93 2.23 2.96 2.00 1.98 2.00 1.97 2.36 1.92 2.43 2.00 2.17 1.98\nBZIP 1% 1.00 1.88 1.96 2.22 2.92 1.98 1.97 1.98 1.98 2.26 1.92 2.85 2.03 2.32 2.00\nBZIP 10% 1.00 1.88 1.93 2.26 2.39 1.93 1.93 1.97 1.95 2.12 1.92 1.86 1.81 1.90 1.84\nBZIP 100% 1.00 1.88 1.94 2.26 2.19 1.94 1.94 1.98 1.96 2.06 1.92 1.64 1.69 1.78 1.66\nCACTUSADM 0.1% 1.00 1.51 1.49 1.44 1.46 1.47 1.47 1.48 1.47 1.48 1.47 1.50 1.51 1.49 1.50\nCACTUSADM 1% 1.00 1.51 1.49 1.44 1.36 1.46 1.46 1.47 1.47 1.42 1.37 1.20 1.46 1.34 1.22\nCACTUSADM 10% 1.00 1.51 1.49 1.44 1.19 1.45 1.45 1.47 1.45 1.33 1.20 1.04 1.40 1.21 1.05\nCACTUSADM 100% 1.00 1.51 1.49 1.44 1.18 1.44 1.44 1.46 1.45 1.31 1.19 1.01 1.40 1.15 1.02\nGEMS 0.1% 1.00 1.11 1.09 1.09 1.13 1.09 1.09 1.09 1.09 1.11 1.11 1.10 1.10 1.10 1.09\nGEMS 1% 1.00 1.11 1.09 1.10 1.13 1.09 1.09 1.09 1.09 1.11 1.11 1.10 1.10 1.10 1.09\nGEMS 10% 1.00 1.11 1.09 1.09 1.13 1.08 1.09 1.09 1.09 1.11 1.11 1.10 1.10 1.10 1.09\nGEMS 100% 1.00 1.11 1.09 1.10 1.13 1.09 1.09 1.09 1.09 1.11 1.11 1.10 1.10 1.10 1.09\nLBM 1% 1.00 1.33 1.33 1.30 1.32 1.33 1.33 1.33 1.33 1.33 1.33 1.33 1.33 1.33 1.33\nLBM 100% 1.00 1.33 1.33 1.30 1.32 1.33 1.33 1.33 1.33 1.33 1.33 1.33 1.33 1.33 1.33\nLESLIE 3D 0.1% 1.00 1.31 1.31 1.31 1.39 1.32 1.32 1.32 1.32 1.34 1.32 1.32 1.32 1.31 1.31\nLESLIE 3D 1% 1.00 1.31 1.31 1.31 1.39 1.31 1.31 1.31 1.31 1.34 1.32 1.31 1.31 1.31 1.31\nLESLIE 3D 10% 1.00 1.31 1.31 1.31 1.37 1.31 1.31 1.31 1.31 1.33 1.32 1.28 1.30 1.29 1.28\nLESLIE 3D 100% 1.00 1.31 1.31 1.31 1.38 1.31 1.31 1.31 1.31 1.33 1.32 1.28 1.30 1.30 1.29\nLIBQ 1% 1.00 1.06 1.06 1.06 1.00 1.06 1.06 1.06 1.06 1.02 1.01 1.06 1.06 1.06 1.06\nLIBQ 100% 1.00 1.06 1.06 1.06 1.00 1.06 1.06 1.06 1.06 1.03 1.01 1.06 1.06 1.06 1.06\nMCF 0.01% 1.00 1.32 1.36 1.43 1.72 1.34 1.34 1.35 1.35 1.37 1.32 1.53 1.50 1.36 1.34\nMCF 0.1% 1.00 1.32 1.35 1.43 1.58 1.31 1.32 1.33 1.32 1.37 1.32 1.31 1.26 1.29 1.28\nMCF 1% 1.00 1.32 1.35 1.43 1.29 1.26 1.26 1.28 1.26 1.27 1.26 1.06 1.21 1.08 1.06\nMCF 10% 1.00 1.32 1.36 1.43 1.22 1.25 1.25 1.27 1.24 1.21 1.20 1.02 1.20 1.05 1.02\nMCF 100% 1.00 1.32 1.36 1.43 1.22 1.25 1.25 1.26 1.25 1.22 1.20 1.01 1.20 1.03 1.01\nMILC 1% 1.00 1.01 1.01 1.01 1.00 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01\nMILC 100% 1.00 1.01 1.01 1.01 1.00 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01\nOMNETPP 0.1% 1.00 1.38 1.39 1.43 1.56 1.38 1.38 1.39 1.39 1.44 1.39 1.43 1.39 1.40 1.39\nOMNETPP 1% 1.00 1.38 1.39 1.43 1.56 1.38 1.38 1.39 1.39 1.44 1.39 1.44 1.42 1.41 1.40\nOMNETPP 10% 1.00 1.38 1.39 1.43 1.55 1.38 1.38 1.39 1.38 1.45 1.39 1.40 1.40 1.40 1.39\nOMNETPP 100% 1.00 1.38 1.39 1.43 1.53 1.38 1.38 1.39 1.38 1.46 1.39 1.36 1.37 1.37 1.35\nSPHINX 3 0.1% 1.00 3.46 2.29 1.87 1.43 1.56 1.59 1.93 1.68 1.69 1.44 1.83 2.09 2.00 1.86\nSPHINX 3 1% 1.00 3.46 2.30 1.88 1.40 1.84 1.92 1.98 1.85 1.62 1.41 1.60 1.87 1.82 1.62\nSPHINX 3 10% 1.00 3.46 2.30 1.87 1.36 1.58 1.65 1.89 1.66 1.59 1.37 1.32 1.63 1.69 1.33\nSPHINX 3 100% 1.00 3.46 2.3 1.88 1.28 1.52 1.56 1.87 1.61 1.63 1.29 1.16 1.47 1.46 1.17\nXALANC 1% 1.00 1.27 1.32 1.48 1.87 1.34 1.34 1.34 1.33 1.61 1.30 1.56 1.37 1.42 1.34\nXALANC 100% 1.00 1.27 1.32 1.47 1.45 1.29 1.28 1.31 1.30 1.38 1.29 1.17 1.17 1.23 1.19",
  "textLength": 56190
}