{
  "paperId": "05105bef253d38bf2c591f8905bd886cc42bdf3a",
  "title": "Pavo: A RNN-Based Learned Inverted Index, Supervised or Unsupervised?",
  "pdfPath": "05105bef253d38bf2c591f8905bd886cc42bdf3a.pdf",
  "text": "Received October 30, 2018, accepted November 14, 2018, date of publication December 7, 2018,\ndate of current version January 4, 2019.\nDigital Object Identifier 10.1 109/ACCESS.2018.2885350\nPavo: A RNN-Based Learned Inverted Index,\nSupervised or Unsupervised?\nWENKUN XIANG1, HAO ZHANG1, RUI CUI2, XING CHU\n1, KEQIN LI\n3, (Fellow, IEEE),\nAND WEI ZHOU\n1\n1School of Software, Yunnan University , Kunming 650091, China\n2Tencent Technology, Beijing 100081, China\n3Department of Computer Science, State University of New York, New Paltz, NY 12561, USA\nCorresponding author: Wei Zhou (zwei@ynu.edu.cn)\nThis work was supported in part by the National Natural Science Foundation of China under Grant 61762089, Grant 61663047, Grant\n61863036, and Grant 61762092, and in part by the Science and Technology Innovation Team Project of Yunnan Province under Grant\n2017HC012.\nABSTRACT The booms of big data and graphic processing unit technologies have allowed us to explore\nmore appropriate data structures and algorithms with smaller time complexity. However, the application of\nmachine learning as a potential alternative for the traditional data structures, especially using deep learning,\nis a relatively new and largely uncharted territory. In this paper, we propose a novel recurrent neural network-\nbased learned inverted index, called Pavo, to ef\u001cciently and \u001dexibly organize inverted data. The basic hash\nfunction in the traditional inverted index is replaced by a hierarchical neural network, which makes Pavo\nbe able to well adapt for various data distributions while showing lower collision rate as well as higher\nspace utilization rate. A particular feature of our approach is that a novel unsupervised learning strategy to\nconstruct the hash function is proposed. To the best of our knowledge, there are no similar results, in which\nthe unsupervised learning strategy is employed to design hash functions, in the existing literature. Extensive\nexperimental results show that the unsupervised model owns some advantages than the supervised one. Our\napproaches not only demonstrate the feasibility of deep learning-based data structures for index purpose but\nalso provide bene\u001cts for developers to make more accurate decisions on both the design and the con\u001cguration\nof data organization, operation, and parameters tuning of neural network so as to improve the performance\nof information searching.\nINDEX TERMS Learning index, RNN, hash function, LSTM.\nI. INTRODUCTION\nThe rapid developments of GPU (Graphic Processing Unit)\nhave allowed us to explore newer data structure and faster\nalgorithm so as to extremely improve our capacity for orga-\nnizing and searching data. In the meantime, some AI (Arti-\n\u001ccial Intelligence) technologies, such as the deep learning,\nhave already shown their advantages in intelligent informa-\ntion processing. However, as fundamental parts of computer\nsystems, the traditional data structures are designed more\nspeci\u001ccally to adapt for the CPU, where the data is orga-\nnized according to a \u001cxed pattern, regardless of various data\ndistributions. Therefore, an intuitive question raises, could\nwe design more appropriate data structures based on deep\nlearning and GPU? Fortunately, one latest paper has provided\nsome interesting results about using deep learning to build\nindex structures, see [1]. This work inspires us to explore\nmore possibilities.It is well known that the inverted index is widely used as\na form of data organization. The basic principle of inverted\nindex is to aggregate and index according to the attributes of\nitems such that similar items can be quickly found for the\nrecommendation. Generally, companies construct hundreds\nof inverted indexes using the same hash function, regardless\nof various data distributions. However, with the advent of\nthe era of big data, how to design an ef\u001ccient and \u001dexible\ndata organization method to improve the space utilization\nrate of the inverted list has become a signi\u001ccant problem that\nneeds to be solved urgently. Hence, this constructs the main\nmotivation of us to explore a more intelligent inverted index\nwhich can adapt for different data distributions.\nAs a personalized reading product of Tencent, Daily\nExpress relies on the powerful content output capabilities of\nTencent News and Tencent Video, backed by QQ, WeChat\nand other large social networks, and quickly grows up to the\nVOLUME 7, 20192169-3536 \n2018 IEEE. Translations and content mining are permitted for academic research only.\nPersonal use is also permitted, but republication/redistribution requires IEEE permission.\nSee http://www.ieee.org/publications_standards/publications/rights/index.html for more information.293\n\nW. Xiang et al. : Pavo: RNN-Based Learned Inverted Index, Supervised or Unsupervised?\nFIGURE 1. The framework of recommend systems in Daily Express of Tencent.\nthird largest information reading application in China within\njust a few years. As a real-time content recommendation sys-\ntem, how to \u001cnd the content that users are interested in among\ntens of millions of resources is the main tasks of the system.\nIn order to improve the accuracy of the recommendation,\ncurrent Daily Express use commodity-based and interest-\nbased collaborative \u001cltering strategies shown as below.\nAs shown in Fig. 1, in the current developing group\nof Daily Express, many recommendation algorithms work\ntogether which leads to consume huge computing resources,\nespecially there exists many trivial calculations in the process\nof computing similarity. Therefore, a large number of inverted\nindexes are built up to speed up similarity computing. And the\nnumber of these inverted index tables is roughly around 350.\nBased on our long period of implementation and a large\nnumber of different business applications, we obtain some\ninteresting observations as follows.\n(1) The inverted index generally uses the hash function\nto construct the index where the hash function can \u001cnd the\nlocation of the random key with O(1) time complexity. Taking\nthe popular in-memory database Redis as an example, Redis\nuses the Murmur hash function is used. When the maximum\nload factor is 1.0, a hash table with length 2nis created for\na data set with Npieces of data ( nis a positive integer,\n2n>n>N>2n\u00001. Although Murmur hash is an ef\u001ccient\nhash function with a low collision rate, its average number of\nlookups is typically between 1.35 and 1.5 in a data set. While\nthe ideal situation is that the data can be uniformly mapped\nby hash function, and the lowest average search time can bereduced to 1.0., which indicates that the method exists some\npotential optimization space.\n(2) The distribution of data has also a very signi\u001ccant\nimpact on the outputs of the hash. In the english word list,\nthe number of words, which start with ``s'' and the second\nplace is ``h'', is far more than the ones which start with\n``s'' and the second place is ``v''. It is well known that this\nkind of data is unevenly distributed in most of the data.\nHowever, the existing hash functions, such as Murmur hash,\nBKDRHash, etc., treat each character in the string separately,\nwithout considering the links between the characters. It will\nbe helpful for us to evenly map the data to the hash table if we\ncan previously learn the distribution of data in advance, and\nincrease the distances between these similar data in the hash\ntable.\n(3) In most inverted indexes constructed by current recom-\nmendation systems, each index table has its own unique data\ndistribution. Moreover, traditional indexing schemes often\nadopt the same hashing function (even if they can have\ndifferent random number seeds). Such an indexing scheme\nmay result in a lower collision rate in partial inverted tables,\nbut is not universally applicable due to :::. Therefore, cus-\ntomized inverted indexes are highly appreciated and can\ngreatly reduce the possibility of con\u001dict.\nBased on the above discussions, we propose a hierarchical\nneural network based inverted index named as Pavo. The\nmain contributions of this paper are re\u001dected as follows:\nIA novel RNN-based learned inverted index ``Pavo'' is\nproposed. To the best of our knowledge, it is the \u001crst\n294 VOLUME 7, 2019\n\nW. Xiang et al. : Pavo: RNN-Based Learned Inverted Index, Supervised or Unsupervised?\nwork to design an intelligent inverted index by means of\nRNN, which is used to split the data set into sub-data\nsets. In this way, the complexity of each model can be\nreduced while the index accuracy can be improved.\nISupervised and unsupervised learning strategies, in the\nlearned inverted index, are constructed, respectively.\nBased on our experiments, it is found that unsupervised\nlearning strategy can \u001cnd a more uniform distribution\nand reduce the data collision rate in most data sets\ncompared with the supervised inverted index. This is a\nnew reveal after the seminal work [1].\nIThe framework and algorithms of Pavo are designed in\ndetail, while the extensive experiments are conducted to\nevaluate the performance of Pavo. With different param-\neters and con\u001cgurations, the ef\u001cciency and \u001dexibility of\nPavo are demonstrated.\nThe rest of the paper is organized as follows: The related\nworks are introduced in Section II. Section III discusses the\nsupervised and unsupervised model. The experimental results\nare shown and analyzed in Section IV. Finally, the paper is\nconcluded in Section V.\nII. RELATED WORK\nAlthough using data structures in machine learning is not a\nnew idea, the application of machine learning as a potential\nreplacement for traditional data structures, especially using\ndeep learning, is a relatively new and largely uncharted\nterritory. Our work builds upon a wide range of previous\noutstanding research. In the following, we intend to outline\nseveral important interactions between machine learning and\nindexes. An inverted index is an index data structure con-\nsisting of a list of words or numbers, which is a mapping\nfrom documents [2]. The purpose of an inverted index is\nto ef\u001cciently generate a list of keyword vectors, in which\nhash functions are widely used. Afterwards, texts are stored\nin the data structure that allows for very ef\u001ccient and fast\nfull-text searches. For the moment, inverted index has been\nintensively studied and used in many different \u001celds, such as\nsearch engine [3], information retrieval systems [4], bioin-\nformatics [5], etc.. However, as far as we know there is\nno attempts to build an inverted index by using neural net-\nwork instead of the hash functions in it. The explosion of\nworkload complexity and the development of AI call for\nnew approaches to achieve more ef\u001ccient and intelligent\ncomputing. Machine learning has emerged as a powerful\ntechnique to address computer optimization. Most recently,\nresearchers have been beginning to employ machine learning\ntechniques for the optimization of indexes and hash functions.\nThere has existed a lot of research on emulating locality-\nsensitive hash (LSH) functions, to build Approximate Near-\nest Neighborhood (ANN) indexes, ranging from supervised\n[6]\u0015[11] and unsupervised [12]\u0015[16] to semi-supervised set-\ntings [17]. These kinds of methods incorporate data-driven\nlearning methods in the development of advanced hash func-\ntions. The principle of these works is a kind of ``learn tohash'', which means to learn the information of data distri-\nbutions or class labels in order to guide the design of hash\nfunction. However, the basic construction of the hash function\nitself is not been changed. Therefore, those methods cannot\nbe directly used to construct the fundamental data structures.\nAs far as we know, paper [1] is the seminal work to develop\na ``learned index'' which explores how neural networks, can\nbe used to enhance, or even replace, traditional index struc-\ntures. It provides a neural network based learned index to\nreplace the B-Tree index and further discusses the differences\nbetween learned hash-map index and the traditional hash-\nmap index. Moreover, experimental results show that the\nlearned index owns signi\u001ccant advantages over traditional\nindexes. Inspired by the paper [1], we propose an RNN-based\nlearned inverted index. Further, different with the supervised\napproach in the paper [1], an unsupervised scheme is studied.\nIn addition, experiments are conducted and the results show\nthat our unsupervised solution can \u001cnd a more uniform dis-\ntribution and reduce the data collision rate in most data sets.\nIII. THE FRAMEWORK OF RNN-BASED LEARNED INDEX\nThe index structure and the machine learning model are\ntraditionally thought to be different approaches. The index\nstructure is constructed in a \u001cxed manner, and the machine\nlearning model is established on the probability forecasting.\nHowever, the principles of these two methods both con-\ncern some kinds of spatial position locating or searching.\nThe hash index can approximately be seen as a regression,\nbecause it works based on where the key prediction data is\nlocated, or a classi\u001ccation in which box the prediction data\ncan be placed based on the key. Therefore, essentially hash\nindexes and neural networks possess some potential relation-\nship. In this section, we present our RNN-based indexing\nframework, mainly focusing on analyzing the proposed Dis-\nperse Stage, Mapping stage, and comparing the differences\nbetween supervised and unsupervised learning strategies.\nA. THE FRAMEWORK OF RNN-BASED LEARNED INDEX\nThe ideal hash table requires to have ef\u001ccient query ef\u001ccien-\ncies and space utilizations. Intuitively, it is worth to adopt an\nend-to-end neural network approach to simulate the whole\nhash function. In this kind of approach, for any arbitrarily\ncomplex data set distribution, the key values in theory can\nbe sorted and inputted into a neural network with appropriate\nparameters. After a suf\u001ccient number of iterations, a very\ngood space utilization rate can be obtained. However, with\nincreasing parameters it is not only dif\u001ccult for training,\nbut also greatly reduces the ef\u001cciency of the search speed.\nAs an index structure, \u001cnding data quickly and accurately is\nthe most important issue, so we intend to use a hierarchical\nframework instead of a single large-scale neural network to\nbuild the index.\nAs Fig. 2 shows, we propose a hierarchical neural network\nto build the index. It \u001crstly uses a RNN neural network to split\nthe data set into sub-data sets. In this way, the complexity\nof each model is reduced and the index accuracy is also\nVOLUME 7, 2019 295\n\nW. Xiang et al. : Pavo: RNN-Based Learned Inverted Index, Supervised or Unsupervised?\nFIGURE 2. The framework of RNN-based learned index.\nimproved. Then, in the last step as shown in Join Stage,\nthe data in the sub-data set is mapped from the smallest to\nthe largest in the dictionary sequence to the hash table to\ncomplete the function of the entire hash model. Each RNN\nsub-model in each layer of the model randomly initializes\nparameters, so that the cyclic neural network learns different\nrelationships between characters, splits the data set from\nmultiple angles, and achieves a lower con\u001dict rate.\nAs Fig. 2 shown above, the entire framework is divided into\nfour stages: Input Stage, Disperse Stage, Mapping Stage and\nJoin Stage. The supervised strategy is used in both Disperse\nStage and Mapping Stage. And the unsupervised strategy can\nonly be used in Mapping Stage.\n1) INPUT STAGE\nThe input data is the key values (mainly string type) in the\nexisting inverted list of Tencent Daily Express. We introduce\nthen-gram split method to preprocess the input data. One\ninput data IDkg1\nn;kg2\nn;\u0001\u0001\u0001;kgs\nnis de\u001cned as a string\nfragments set from the same key-value string, where s is the\nwindow size and there is overlap between every two input\nfragments (i.e. kgi\nn\\kgiC1\nn6D;). Therefore the entire input\ndata are I\u0003DI1;I2;\u0001\u0001\u0001;Im, where mis the size of the data\nset. In our approach, nis selected as 2, which means that it is\nprocessed in bigram mode.\n2) DISPERSE STAGE\nIn this stage the split layer is composed of one or more\nRNN models. Our purpose is to encode preprocessed input\nstrings and evenly distribute the codes in the vector space.\nThe function of each model of the Disperse stage is to split the\ndata set into multiple sub-data sets so that it is bene\u001ct for the\nsmaller subsequent models to learn the mapping relationship.For example, suppose we need to split the total spatial space c\nintorsub-space, where there are c=rsub-data sets. Two-tier\nDisperse stages are used in Fig. 2. The \u001crst layer splits the\ndata set into lsub-data sets. Then there will be a lnumber\nof models in the second layer. Each model is responsible to a\nseparate data set. Each model in the second layer then splits\nthe respective data set into jsub-data sets, where l\u0003jDr\n3) MAPPING STAGE\nThe mapping layer is also implemented by multiple RNN\nmodels. The purpose of this layer is to map the sub-data\nsets output from the splitting stage into the local hash space,\nand \u001cnd local hash location for each data in the output data\nset. We provide an unsupervised strategy in this mapping\nstage which makes our practice quite different from the \u001crst\nwork [1].\n4) JOIN STAGE\nFinally, after the entire framework is setup, each RNN model\nin the mapping stage is serially connected from low to high,\nand the local hash space is connected to form the \u001cnal hash\ntable. Suppose we totally have i sub-data sets in the last layer\nof mapping stage, which means we have imapping models in\nthe mapping stage. Each sub-data set has dinumber of data,\nthen the p\u0000thposition in the i\u0000thlocal space will be mapped\ninto the global hash table space:\nPosDi\u00001X\njD1djCp\nAs a whole, the entire framework consists of multiple RNN\nmodels. Each RNN model consists of a single-layer LSTM\nlayer and one or two full-connection layers. The entire frame-\nwork is trained stage by stage from left to right, and multiple\n296 VOLUME 7, 2019\n\nW. Xiang et al. : Pavo: RNN-Based Learned Inverted Index, Supervised or Unsupervised?\nmodels in each stage can be parallel trained. The \u001crst level\nin Disperse Stage uses the entire data set as training data, and\nthe latter layer uses the sub-data set splitted from the previous\nlayer. In the end, the data locates the position in the last layer\nof the Mapping Stage. During training, parameters are initial-\nized randomly for each separate RNN model. Therefore, each\nRNN can extract different features from key values and split\ndata sets from multiple dimensions.\nThe red line in the Fig. 2 above describes a complete\nprediction process of the proposed framework: (1) The word,\nshown as ``Documentaries'', is splitted into do, oc, cu,...,\nes using bigram, and are sent to the input layer RNN1.1 in\nthe Disperse stage. The output of RNN1.1 is a number,\nwhich indicates the next model. In this case the calculated\nresult is referring to RNN2.1 (2) The ``Documentaries'' word\nis transferred to RNN2.1 and recalculated using bigram.\n(3) Afterwards, the ``Documentaries'' word is transferred to\nRNN3.2 and the key Pos1 in the local hash space is located.\n(4) Pos1 is mapped to the global space in the entire hash table\nat last. Although the entire framework has a large number of\nparameters, in a key value prediction process, only one sub-\nmodel will be selected for calculation in each layer. There-\nfore, compared to a single model, the calculation amount\nof hierarchical model in the process of prediction is greatly\nreduced.\nB. MODELS IN DISPERSE STAGE\nIn this subsection we describe the details of the supervised\nlearning strategy in the Disperse Stage\nThe purpose of Disperse Stage is to encode preprocessed\ninput strings, evenly separate the codes in the vector space,\nand then evenly split them into sub-data sets. Due to the fact\nthat the key distribution in each sub-data set may be different.\nTherefore, we adopt multiple neural network modules to learn\nthe key distribution. As shown in Fig. 2, we have RNN2.1,\nRNN2.2,... etc. Since each sub-model in Disperse Stage is\nrandomly initialized, the neural network learns different rela-\ntionships between characters, splits the data set from multiple\nangles, and achieves a lower con\u001dict rate.\nIn the traditional hash index, the speci\u001cc location of each\ndata is determined by a hash function. Therefore, after the key\nhas undergone operations such as multiplication and shifting,\nthe hash functions modulo the length of the hash table to\nobtain the \u001cnal position value. Generally speaking, only one\n\u001cxed hash function can be used for an inverted index. How-\never, no matter what kinds of hash functions are selected, it is\nimpossible to give full consideration for all data distribution,\nso as to avoid con\u001dicts. In the recommended applications\n\u001celd, in order to quickly read the inverted data, it often\nstores the inverted tables in the Redis, which uses a high-\nperformance, low-impact Murmur hash to create indexes.\nThe neural networks in the Disperse Stage are applied\nsupervised learning strategy. We need to give a standard\nposition value for each key as a learning target, and use the\ngradient descent optimization parameter to reduce the gap\nbetween the standard position and the output position until themodel converges. For the convenience of comparison, we use\nthe Murmur hash to construct standard values for training\nduring the Disperse Stage.\nFIGURE 3. The supervised learning strategy in Disperse Stage.\nAs shown in the Fig. 3, we need to \u001crst determine the\nnumber Mof models in the next layer according to actual\nrequirements, that is, we need to split the data set into M\ncopies. Each model contains a layer of LSTM and a layer\nof full connections, which can also be set to 2 layers. After\ninputting the key value sequences, LSTM \u001crst encodes the\ninput string to obtain the feature vector F, then \u001cts feature F\nthrough the \u001crst layer full-connection and \u001cnally output an\ninteger value i2[0;M), which means that the piece of data\nis assigned to the i\u0000thmodel in the next layer for processing.\nThe training process uses Murmur hash to generate the target\nvalue and calculates the gap between the target value and the\npredicted value to optimize the network model.\nLoss Function Selection: The purpose of Disperse Stage is\nto learn a series of parameters so as to \u001cnd the next layer as\nbest as possible to process the data. After long term explo-\nration, two loss functions are used in our training process.\nFirst of all, we hope that the prediction value of the model\ncan be as close as possible to the target value generated by\nMurmur hash. We measure this in terms of mean squared\nerror, as shown in Eq. (1):\nloss1D1\nNNX\niD1\u0000\nyi\u0000Oyi\u00012(1)\nIn Eq. (1), yiis the Murmur hash target value which shows\nas label in Fig. 3 and the Oyithe prediction value.\nOn the other hand, we hope that the amount of data handled\nby each model in the Disperse Stage will be as uniform as\npossible. In order to avoid the extremely imbalanced amount\nof data processed in the later layer, an additional loss2 is\ndesigned, which we call uniform distribution loss. During the\ncalculation of the loss2, the data contained in each batch is\n\u001crstly counted the amount of data k assigned in each slot\nVOLUME 7, 2019 297\n\nW. Xiang et al. : Pavo: RNN-Based Learned Inverted Index, Supervised or Unsupervised?\ntime, and then is computed to obtain the difference between\nk and the theoretical average distribution in each slot. This\ndifference works as an optimization goal, as shown in Eq. (2):\nloss1DMX\njD1\u0012\nki\u0000N\nM\u00132\n(2)\nwhere Nis the training batch size, Mis the number of split\nslots (or number of next-level models) and k(j) is the amount\nof data in the j-th slot for this batch of data.\nDuring the training process, we integrate the above two loss\nfunctions as the overall optimization goal of the model:\nLD(w1loss1Cw2loss2) (3)\nwhere w1,w2are used to control the degree of \u001ct hyper-\nparameter. As Eq. (3) shows the overall loss is divided into\ntwo parts: the \u001crst part is to \u001ct the Murmur hash and the sec-\nond part is to reduce the gap between each slot and the\ntheoretical average distribution.\nC. MODELS IN MAPPING STAGE\nIn this subsection, we introduce our implementation of two\nmapping strategies: supervised mapping and unsupervised\nmapping.\nThe purpose in this stage is to map each piece of data into\nthe local hash space, and try to avoid the con\u001dict. Note here\nwe don't care about the order of the mapped data. This is\ndifferent from B-tree, where it is necessary to maintain the\norder of data. We also hope the con\u001dict rate can be as small\nas possible or not at all perfectly.\nD. SUPERVISED MAPPING\nIn the Mapping Stage, when the loading factor is set to 1,\nthe number of possible output values is equal to the amount\nof data. It is well known that there will be a considerable\npart of the hash space without data if we use hash function\nmapping, which causes space waste and increases the average\nnumber of searches. Here we choose to arrange the key values\nfrom the smallest to the largest, and place them one by one in\nthe hash space to ensure that the key values and hash space\nlocations are in one-to-one correspondence.\nIn the mapping layer, the dataset is the result of the splitting\nof the previous layer. The RNN used here is the same as\nthe disperse stage. However, the label is different. The loss\nfunction is described as below:\nlossD1\nNNX\niD1\u0000\nyi\u0000Oyi\u0001\n(4)\nwhere Nis the training batch size, Oyiis the output value of the\nrecurrent neural network and yiis the sorting result of the key\nvalue. As shows in the right side of Fig. 4, words are sorted\nand the labels are generated by the order number of the list.\nSo the label of ``Adventure'' is 1, and the label of ``Buddy''\nis 3, and so on.\nTheoretically, if there are enough parameters in the neural\nnetwork and after enough iterative optimization, it can ensure\nFIGURE 4. The supervised strategy in mapping stage.\nthat the neural network can perfectly learn this mapping\nrelationship. However, in order to speed up the ef\u001cciency of\nthe index, the amount of parameters in the neural network\ncannot be too much, so we need to balance the parameters\nand the con\u001dict rate in the mapping layer.\nE. UNSUPERVISED MAPPING\nThe previously supervised approach, which tags data accord-\ning to a random hash function or a dictionary sequence of\ndata key values, is essentially man-made method to specify\nthe distribution of data. And it does not naturally satisfy the\ndistribution of the data itself. Therefore, we consider whether\nit is possible to specify the location of each key and \u001cnd the\ndistribution that best \u001cts the data automatically through the\nneural network.\nFIGURE 5. The unsupervised strategy in mapping stage.\nAs Fig. 5 shown, we propose an unsupervised neural net-\nwork approach, which consider the process of locating the\nposition of the key value on the hash table as a classi\u001ccation\nproblem. The numbers of categories are equal to the length of\nthe hash table. As shows in Fig. 5, the keys are preprocessed\nby bigram and inputted into the RNN networks. Each RNN\n298 VOLUME 7, 2019\n\nW. Xiang et al. : Pavo: RNN-Based Learned Inverted Index, Supervised or Unsupervised?\nnetwork outputs a vector and we take the position of the\nmaximum value of the vector as the classi\u001ccation of the key\nvalue. Then it calculates the classi\u001ccation of each key value\non the entire data set and sum the results to get the numbers of\nkey values in each category. For example as shown in Fig. 5,\nafter operating the key0, the \u001crst RNN network outputs a\nvector, which is the \u001crst column in the matrix. Then we select\nthe maximum value, which is 0.5, in this column and setup\nit to 1. Other values in this column are set to 0. And so on\nkey1 to key n. After this, we sum the values in each row to get\nthe \u001cnal categories distribution.\nThe loss function of the unsupervised approach is de\u001cned\nas:\nlossD1\nN\u0002Y\u0002(D\u00001) (5)\nAmong them, Nis the data set size, Yis the output value\nof the neural network, and Dis the numbers of key values in\nthe category. Gradient descent is used to optimize the sum.\nWhen the number in each category equal 1, the loss value is\n0, which means that it \u001cnds a hash model that makes the data\nset evenly distributed. The training algorithm is described as\nbelow:\nAlgorithm 1 Unsupervised Hash Function Training Strategy\nInput: training sample Keys, iterations I\nOutput: trained Index\nTraining:\n1 build a network model with one LSTM layer\nand one to two fully-connected layers\n2 For i = 1 to I do\n3 logits = network_model(Keys)\n4 index = argmax(softmax(logits), axis=1)\n5 hash_results = sum(index, axis=0);\n6 loss = logits *hash_results;\n7 back propogation to minimize loss;\n8 return index\nAs above algorithm1 shows that it \u001crst establishes a\nrecurrent neural network containing one LSTM layer and\none or two full-connection layers with random parameters ini-\ntialization. Keys are inputted during forward propagation, and\nthe output logits (line 3) of the neural network is calculated,\nthe output logits is a list of vectors shape of [M,N], where\nM stands for the size of the dataset, and N stands for size\nof \u001cnal hash table. For each output vector, \u001cnds the position\nof the largest value in the vector as the index value of the\nKey (line 4). Then accumulates all the vectors, results in the\nhash result of these keys in the i thiteration(line 5). After this\nthe hash results is used to construct the loss function, and the\ngradient descent is used to optimize the loss function to get\nthe \u001cnal index model.\nIV. EXPERIMENTS AND ANALYSIS\nA. EXPERIMENT SETUP\nThe experiments were setup on a machine with 64GB\nmain memory and one 2.6GHZ Intel(R) i7 processors. Two\nGTX1060 GPU card are installed and each of them has 16GGPU memory. RedHat Enterprise Server 6.3 with Linux core\n2.6.32 was installed, and we use tensor\u001dow for experiments.\nFor each experiment, we ran ten cases, where the median of\nthe ten cases was used as the real performance.\nIn the experiments, we randomly generated data sets of\nthree common distributions (uniform distribution, normal dis-\ntribution, and long tail distribution) to test the learned index\nmodel and validate our results using a real data set. We hope to\ncompare the advantages and disadvantages of learned index\nmodels with traditional hash functions when the data size go\nup to one million or more.\nWe compare four kinds of data structures in our\nexperiments:\n1) BKDR HASH\nBKDR Hash is an algorithm invented by Brian Kernighan,\nDennis Ritchie, which is widely used for string processing.\n2) MURMUR HASH\nMurmur Hash is a non-cryptographic hash function suitable\nfor general hash-based lookup. Redis uses the Murmur hash\nfunction as default.\n3) SUPERVISED RNN-BASED INDEX\nOur proposed RNN-based index, where the supervised algo-\nrithm used in both the mapping stage and the disperse stage.\n4) UNSUPERVISED RNN-BASED INDEX\nOur proposed RNN-based index, where the unsupervised\nalgorithm used in the mapping stage, while it is still use super-\nvised algorithm in the disperse stage. The Murmur hash and\nBKDR Hash are implemented in Python, and the intelligent\nindexes are implemented in Tensor\u001dow and called directly in\nPython.\nB. EXPERIMENT ANALYSIS\nDuring the process of design and evaluation, it is observed\nthat the size of the mapping layer has signi\u001ccant impact on\nthe performance of the learned inverted index. To verify the\nobservations, the average search time and the remaining space\nunder different mapping size are obtained by experiments,\nwhere the data are generated with four different distributions\n(i.e. uniform distribution, normal distribution, long tail distri-\nbution and Tencent real inverted index). Here we choose the\ncommon character hash function BKDRHash and Murmur\nhash used in Redis as a comparison. Fig. 6(a) and Fig. 6(e)\nrespectively re\u001dect the average number of lookups and the\nremaining space for data sets of different mapping layers.\nAs shows in \u001cg. 6a, when the mapping size of supervised\nindex is 100, the average number of search times is 1.0,\nwhich means that one-to-one mapping of data is implemented\nwithout con\u001dict. Accordingly, the remaining space in the\nFig. 6(e) is 0.0. Also, the average number of lookups for\nsupervised index has risen slowly between size 200 and size\n2000 in Fig. 6(a). However, it grows up sharply after 2000.\nBut even at 5000, the lookup number is still lower than that\nVOLUME 7, 2019 299\n\nW. Xiang et al. : Pavo: RNN-Based Learned Inverted Index, Supervised or Unsupervised?\nFIGURE 6. Evaluation of mapping layer's size with different data distributions.\nof BKDRHash and Murmur hash. As for the unsupervised\nindex in the same Fig. 6(a), its performance is relatively stable\ncompared to the supervised model. Until 1000, the average\nnumber of searching is still 1.0, the corresponding space in\nthe Fig. 6(e) is 0.0. At the size 5000, unsupervised index\ncan still be well maintained at 1.05, and in the corresponding\nFig. 6(e) the remaining space is 0.07, which is signi\u001ccantly\nlower than supervised index with 0.8 remaining space. Sim-\nilarly, on the normal and long-tailed distribution datasets\n(Fig. 6(b), Fig. 6(c), Fig. 6(f), and Fig. 6(g)), supervised and\nunsupervised approaches reach the ideal value when the map-\nping size is less than 1000, where average number of searches\nis 1.0, and the remaining space is 0.0. When the mapping\nsize is greater than 1000, supervised learning rises rapidly,\nwhile unsupervised learning grows much slower. In the real\ndata set (Fig. 6(d), Fig. 7(h)) which has more complicated\ndata distribution, the average search time of supervised and\nunsupervised index are slightly higher than these in Fig. 6(a)-\nFig. 6(c), Fig. 6(e)-Fig. 6(g) between 100 and 1000. However,\ncurve of supervised index rises faster after size 2000, and\nit even exceeds BKDRHash and Murmur hash at size 5000,\nwhich mean supervised index get worst performance at size\n5000.\nFrom Fig. 6, we come to know that both the supervised\nand unsupervised index have lower search time and higher\nspace utilization when the mapping size less than 2000.\nSpeci\u001ccally, performance of the unsupervised index is better\nthan the supervised one. It is also observed that the average\nsearch time of learned indexes is almost 1, which closes to the\nideal state, when the mapping layer's size is less than 1000.\nTherefore, we choose 1000 mapping size in our \u001cnal network.\nThe average number of lookups is an important indicator\nfor evaluating the hash function. After selecting the mapping\nsize 1000, we further test the average search time and give\nboxplots. From Fig. 7(a)-Fig. 7(d), we can see that the average\nsearch number of supervised or non-supervised strategies\nare signi\u001ccantly lower than the one of traditional BKDR\nHash and Murmur hash. Among them, in the randomly\nFIGURE 7. Comparison of the average number of lookups in different\ndata distributions.\ngenerated uniform distribution Fig. 7(a), the average number\nfor supervised and unsupervised strategies is almost 1.0,\nwhile BKDR Hash and Murmur hash are both around 1.5.\nAnd this trend is basically maintained in the normal distri-\nbution and long-tail distribution data sets. In the real data\nset, BKDR Hash and Murmur hash still remained around\n1.5, while the average number of supervised and unsuper-\nvised strategies rose slightly up to 1.15. In the three datasets\nfrom Fig. 7(a)-Fig. 7(c), the learning indexes (i.e. super-\nvised and unsupervised) \u001ductuate very light and the results\nare extremely stable. In the real inverted index data set on\nFig. 7(d), the \u001ductuate effect of traditional hash function is\nbasically the same as the previous three simulated data sets,\nhowever the learning models are more volatile. This shows\nthat for a regular distribution of Fig. 7(a)-Fig. 7(c), neural\nnetwork can learn well, but the distribution of real data is\nmore complex and the \u001ctting ability of neural network will\ndecrease. This also suggests that a potential method is to\nincrease the number of parameters in the neural network to\nreduce the average number of search times and free space of\n300 VOLUME 7, 2019\n\nW. Xiang et al. : Pavo: RNN-Based Learned Inverted Index, Supervised or Unsupervised?\nthe model in large data sets, but the cost of increasing the\namount of parameters will increase the training and testing\ntime of the model. The results of Fig. 7(a)-Fig. 7(d) also show\nthat unsupervised index has less \u001ductuation than supervised\nindex, which means that unsupervised learning strategy has\nbetter abilities to adapt the data distribution. Neural network\nparameters are important indicators that affect the speed of\nnetwork training and \u001cnal searching. We further evaluate the\nrelationship between the network parameter amount and the\naverage number of lookups. Because in Fig. 6, when the\ndata is 1000, the average number of lookups under the three\nideal distribution conditions is almost the same, so we only\nselect the long tail distribution and the real data for testing.\nIn the ideal long-tail distribution map Fig. 8(a), it can be\nseen that when the model parameter is 2000, the average\nnumber of supervised index reaches 1.4, indicating that the\ncon\u001dict rate is relatively high. While unsupervised index only\nhas 1.08 at this time. With the increasing of parameters,\nthe con\u001dict rate of supervised index is signi\u001ccantly reduced.\nAt 8500 point, supervised learning was slightly higher than\nunsupervised index. When the model parameters reached\n18,000, supervised index reaches the ideal value of 1.0. The\naverage search times of unsupervised index is very low at\n\u001crst. When the parameter reaches 4000, it already get its\nideal value of 1.0. and then remain its curve trend afterwards.\nIn the real data Fig. 8(b), unsupervised index shows a lower\ncon\u001dict rate at the beginning, and the average number of\nlookups is already close to 1.0 at 2000 point. In supervised\nindex, when the number of parameters is 2000, the aver-\nage number of searches is still about 1.4. At a parameter\nof 10,000, both supervised and unsupervised indexes achieve\ngood results close to 1.0. Although the parameter amount is\n18000 in the ideal distribution Fig. 8(a), the average number\nof lookup times for supervised index is lower. But when the\nparameter is 8500, the model size will be enlarged 2.1 times\n(18000/8500D2.1), and the training time of the model is\nalso increased. Therefore, according to the test results of\nFig. 8, in the subsequent experiments, we choose 10000 of\nour parameters in the real dataset as showed in Fig. 8(b), and\nwe choose 8500 parameters under the ideal distribution as\nshowed in Fig. 8(a).\nFIGURE 8. Evaluation of neural network parameters in the mapping layer.\nThe effect of mapping layer will be affected by the size\nof the data set, so in the Disperse Stage it is necessary to\nsplit the data as evenly as possible to ensure the learning\nability of mapping layer. Therefore, in this set of experiments,\nFIGURE 9. Evaluation of performance of Disperse Stage.\nwe analyze the performance of Disperse Stage, where one\nhundred thousand data are split into 100 parts by differ-\nent methods. In Fig. 9 abscissa represents 1 to 100 data,\nand ordinates represent the number of data in each part.\nAs Fig. 9 shows, the maximum value of Murmur hash is\n1070 and the minimum value is 921. And the \u001ductuation is\nvery large. The highest value of BKDR hash is 1066 and\nthe lowest value is 928, which slightly better than Murmur\nhash. While if we look at our proposed disperse layer, its\nperformance is very stable. The highest value is 1004 and the\nlowest value is 997. The amount of data split by it is close to\n1000. This experiment shows that the learned index structure\ncan well divide the data evenly.\nWe conduct a series of experiments for comparing the\nRNN-based index with BKDR hash and Murmur hash based\nindexes. 1 million randomly generated long tail distributed\ndata are used to construct different indexes. Table 1 shows\nthe average search number of learning index is slightly higher\nthan 1, while the average search number of both Murmur hash\nand BKDR hash based indexes are near 1.5. the unsupervised\nindex gains the smallest spare space with only 0.20%. The\nspare space of supervised index is about 4.4%, which is\nslight higher than unsupervised index. While Murmur hash\nbased index is37%. This shows that, comparing to Murmur\nhash based index, unsupervised index save space 99.46%\n((37-0.2)/37D99.46). Table 1 also shows that the RKDR hash\nTABLE 1. Evaluation of performance of RNN-based learned index.\nVOLUME 7, 2019 301\n\nW. Xiang et al. : Pavo: RNN-Based Learned Inverted Index, Supervised or Unsupervised?\nbased index has the smallest search time with 8610ns, and\nthe Murmur hash based index is the second small search\ntime with 20637ns. The search time of unsupervised index\nis 72181ns, which is 3.5 (72181/20637) times more than\nMurmur hash. The reason is that learned index is com-\nposed of multi layers network, which unavoidably spends\nmore time to locating the data. Comparing to the super-\nvised index, the unsupervised index save search time 2.4 %\n((73987-72181)/93987).\nV. CONCLUSIONS AND FUTURE WORK\nIn this paper, we presented a novel RNN-based learned\ninverted index, which uses hierarchical neural network to\nsimulate hash functions. Experimental results show that both\nthe supervised and unsupervised approaches have lower col-\nlision rate as well as higher space utilization, compared with\nthe traditional hash functions. Although the con\u001dict rates\nboth in the supervised learning and the unsupervised learning\nindex model are very low on some data sets, the unsupervised\nlearning model can better \u001ct the relatively complex data and\nis less affected by the outliers. When the data distribution is\nunknown, unsupervised learning can train better models with\nthe same model parameter quantities. Although the search\ntime of learned index is at least 3-4 times more than the\ntraditional hash function based index on our python imple-\nmentation, considering the quick developed GPU, there is\nstrong evidence to show that neural network based index is\npromising in future.\nREFERENCES\n[1] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, ``The case for\nlearned index structures,'' in Proc. Int. Conf. Manage. Data , Houston, TX,\nUSA, Jun. 2018, pp. 489\u0015504, doi: 10.1145/3183713.3196909.\n[2] J. Zobel, A. Moffat, and K. Ramamohanarao, ``Inverted \u001cles versus sig-\nnature \u001cles for text indexing,'' ACM Trans. Database Syst. , vol. 23, no. 4,\npp. 453\u0015490, Dec. 1998.\n[3] J. Zobel and A. Moffat, ``Inverted \u001cles for text search engines,'' ACM Com-\nput. Surv. , vol. 38, no. 2, p. 6, Jul. 2006, doi: 10.1145/1132956.1132959.\n[4] G. Adomavicius and A. Tuzhilin, ``Toward the next generation of recom-\nmender systems: A survey of the state-of-the-art and possible extensions,''\nIEEE Trans. Knowl. Data Eng. , vol. 17, no. 6, pp. 734\u0015749, Jun. 2005.\n[5] A. Morgulis, G. Coulouris, Y. Raytselis, T. L. Madden, R. Agarwala, and\nA. A. Sch√§ffer, ``Database indexing for production MegaBLAST\nsearches,'' Bioinformatics , vol. 24, no. 16, pp. 1757\u00151764, Jun. 2008.\n[6] B. Kulis, P. Jain, and K. Grauman, ``Fast similarity search for learned\nmetrics,'' IEEE Trans. Pattern Anal. Mach. Intell. , vol. 31, no. 12,\npp. 2143\u00152157, Dec. 2009.\n[7] W. Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang, ``Supervised hash-\ning with kernels,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. ,\nProvidence, RI, USA, Jun. 2012, pp. 2074\u00152081.\n[8] M. Norouzi, D. Fleet, and R. Salakhutdinov, ``Hamming distance metric\nlearning,'' in Proc. Adv. Neural Inf. Process. Syst. , 2012, pp. 1061\u00151069.\n[9] A. Torralba, R. Fergus, and Y. Weiss, ``Small codes and large image\ndatabases for recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit. , Anchorage, AK, USA, Jun. 2008, pp. 1\u00158.\n[10] L. Fan, ``Supervised binary hash code learning with jensen Shannon diver-\ngence,'' in Proc. IEEE Int. Conf. Comput. Vis. , Dec. 2013, pp. 2616\u00152623.\n[11] F. Shen, C. Shen, W. Liu, and H. T. Shen, ``Supervised discrete hashing,'' in\nProc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. , Boston, MA, USA,\nJun. 2015, pp. 37\u001545.\n[12] Y. Weiss, A. Torralba, and R. Fergus, ``Spectral hashing,'' in Advances in\nNeural Information Processing Systems , vol. 21, D. Koller, D. Schuurmans,\nY. Bengio, and L. Bottou, Eds. Cambridge, MA, USA: MIT Press, 2009,\npp. 1753\u00151760.[13] W. Liu, J. Wang, S. Kumar, and S.-F. Chang, ``Hashing with graphs,'' in\nProc. ICML , Bellevue, WA, USA, 2011, pp. 1\u00158.\n[14] Y. Gong and S. Lazebnik, ``Iterative quantization: A procrustean approach\nto learning binary codes,'' in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit. , Colorado Springs, CO, USA, Jun. 2011, pp. 817\u0015824.\n[15] W. Kong and W.-J. Li, ``Isotropic hashing,'' in Proc. Adv. Neural Inf.\nProcess. Syst. , vol. 25, 2012, pp. 1655\u00151663.\n[16] Y. Gong, S. Kumar, V. Verma, and S. Lazebnik, ``Angular quantization\nbased binary codes for fast similarity search,'' in Proc. Adv. Neural Inf.\nProcess. Syst. , vol. 25, 2012, pp. 1646\u00151654.\n[17] J. Wang, S. Kumar, and S.-F. Chang, ``Semi-supervised hashing for large-\nscale search,'' IEEE Trans. Pattern Anal. Mach. Intell. , vol. 34, no. 12,\npp. 2393\u00152406, Dec. 2012.\nWENKUN XIANG received the B.S. and M.S.\ndegrees in software engineering from Yunnan\nUniversity, in 2015 and 2018, respectively. He\nis currently a Researcher with Tencent Technol-\nogy, Beijing. His current research interests include\nmachine learning, big data computing, and recom-\nmender system.\nHAO ZHANG is currently pursuing the graduate\ndegree with Yunnan University. His main research\ninterests include machine learning, deep learning,\nand bioinformatics.\nRUI CUI received the B.S. and M.S. degrees in\ncomputer application technology from Northwest-\nern Polytechnical University, in 2007 and 2010,\nrespectively. She is currently a Senior Researcher\nof recommendation algorithm with Tencent. She is\nmainly devoted to the research and development of\nshort video personalized recommendation of daily\nexpress.\nXING CHU received the B.E. degree from the\nKunming University of Science and Technology,\nin 2011, the M.E. degree from Hunan University,\nChina, in 2014, and the Ph.D. degree in control\ntheory and engineering from the Ecole Centrale de\nLille, in 2017. He is currently a Lecturer with the\nSchool of Software, Yunnan University, China. His\nresearch interests lie in the distributed cooperative\ncontrol of multi-agent/robot systems and intelli-\ngent automobile.\n302 VOLUME 7, 2019\n\nW. Xiang et al. : Pavo: RNN-Based Learned Inverted Index, Supervised or Unsupervised?\nKEQIN LI is currently a SUNY Distinguished Pro-\nfessor of computer science with the State Univer-\nsity of New York. He has published over 590 jour-\nnal articles, book chapters, and refereed confer-\nence papers. His current research interests include\nparallel computing and high-performance comput-\ning, distributed computing, energy-ef\u001ccient com-\nputing and communication, heterogeneous com-\nputing systems, cloud computing, big data com-\nputing, CPU\u0015graphic processing unit hybrid and\ncooperative computing, multicore computing, storage and \u001cle systems, wire-\nless communication networks, sensor networks, peer-to-peer \u001cle sharing\nsystems, mobile computing, service computing, the Internet of Things,\nand cyber-physical systems. He was a recipient of several best paper\nawards. He is currently serving or has served on the editorial boards of\nthe IEEE T RANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS , the IEEE\nTRANSACTIONS ON COMPUTERS , the IEEE T RANSACTIONS ON CLOUD COMPUTING ,\nthe IEEE T RANSACTIONS ON SERVICES COMPUTING , and the IEEE T RANSACTIONS\nONSUSTAINABLE COMPUTING .\nWEI ZHOU received the Ph.D. degree from the\nUniversity of Chinese Academy of Sciences. He\nis currently a Full Professor with the Software\nSchool, Yunnan University. His current research\ninterests include the distributed data intensive\ncomputing and bio-informatics.\nVOLUME 7, 2019 303",
  "textLength": 47769
}