{
  "paperId": "3a3e9648ccc800ce91fac33f426022e1ca3374d0",
  "title": "A Distributed Learned Hash Table",
  "pdfPath": "3a3e9648ccc800ce91fac33f426022e1ca3374d0.pdf",
  "text": "A Distributed Learned Hash Table\nShengze Wang1, Yi Liu1, Xiaoxue Zhang2, Liting Hu1, Chen Qian1\n1University of California Santa Cruz,2University of Nevada Reno\n{shengze, yliu634, liting, cqian12}@ucsc.edu, xiaoxuez@unr.edu\nAbstract —Distributed Hash Tables (DHTs) are pivotal in numer-\nous high-impact key-value applications built on distributed net-\nworked systems, offering a decentralized architecture that avoids\nsingle points of failure and improves data availability. Despite their\nwidespread utility, DHTs face substantial challenges in handling\nrange queries, which are crucial for applications such as LLM\nserving, distributed storage, databases, content delivery networks,\nand blockchains. To address this limitation, we present LEAD, a\nnovel system incorporating learned models within DHT structures\nto significantly optimize range query performance. LEAD utilizes a\nrecursive machine learning model to map and retrieve data across\na distributed system while preserving the inherent order of data.\nLEAD includes the designs to minimize range query latency and\nmessage cost while maintaining high scalability and resilience to\nnetwork churn. Our comprehensive evaluations, conducted in both\ntestbed implementation and simulations, demonstrate that LEAD\nachieves tremendous advantages in system efficiency compared to\nexisting range query methods in large-scale distributed systems,\nreducing query latency and message cost by 80% to 90%+. Fur-\nthermore, LEAD exhibits remarkable scalability and robustness\nagainst system churn, providing a robust, scalable solution for\nefficient data retrieval in distributed key-value systems.\nI. I NTRODUCTION\nKey-value data management across distributed computing\nsystems plays a crucial role in supporting large-scale Internet\napplications, including the emerging area of large language\nmodel (LLM) serving [1]–[6]. Distributed Hash Tables (DHTs)\nhave been widely used for decentralized data management [7]–\n[9]. A DHT is a distributed data structure adept at performing\nstorage and retrieval operations of key-value pairs across a de-\ncentralized network of nodes. DHTs mitigate the limitations of\ncentralized architectures by eliminating single points of failure\nand distributing data loads across numerous nodes, thereby en-\nhancing data availability and network efficiency [10]. State-of-\nthe-art systems like InterPlanetary File System (IPFS) [11], Cas-\nsandra [12], [13], Tor [14], Namecoin [15], and Bittorrent [16],\nhave exemplified the integration of DHTs in ensuring scalable\nand fault-tolerant data management in distributed networked\nsystems.\nThe problem. Despite their widespread adoption and in-\nherent advantages, DHT-based systems encounter significant\nchallenges, particularly when handling complex queries such\nas range queries, which are important functions in applica-\ntions such as KV-cache sharing in LLM serving [2]–[4], [17],\ndistributed file systems and databases [11], [12], edge-cloud\nsystems [18], [19], and blockchain systems [20], [21]. Current\nDHT systems are primarily optimized for single-key lookups.\nDHTs use a uniformly random hash function to distribute keys\ninto random locations, hence similar keys will be mapped tocompletely different storage locations. This feature of DHT\nwill introduce two major limitations for range queries. First,\nall keys in the queried range need to be searched to ensure the\ncompleteness of the query. Second, these keys will be mapped\nto different locations based on the hash function. Accessing\nthese locations will cause a high cost of network traffic. In\nthe literature, efforts to improve range query performance in\ndistributed systems have led to limited solutions. Armada [22]\nuses a partition tree model within the FissionE topology [23].\nDBST [24] integrates binary search trees for range queries.\nMARQUES [25] employs space-filling curves in a multi-level\noverlay structure, bringing increased overhead and scalability\nissues. RQIOT [26] explores the idea of using order-preserving\nhashing to improve range query efficiency, yet how to design\nsuch a hash method, especially in a dynamic distributed system,\nis unclear. These solutions cannot completely resolve the two\nlimitations of range queries in DHT.\nOur solution. To address the critical issue – enabling efficient\nrange queries for distributed networked systems – we introduce\nLEAD (LEArned DHT), a novel system that first integrates\nmachine learning models with DHT frameworks to enhance\nthe performance of range queries evidently. Drawing on the\nlearned indexes proposed in recent years [27], which suggests\nthat indexes could be conceptualized as \"models\" that predict\nthe position of a key within a dataset, we argue that a learned\nmodel can replace the hash function to distribute keys in\nnetworked systems . By learning the cumulative distribution\nfunction (CDF) of keys, we can maintain the inherent order\nof these keys while mapping them to a decentralized group\nof nodes, making similar keys be placed in close locations.\nHence the two limitations of random hash functions can be\ncompletely resolved. To minimize inference overhead and re-\nduce the prediction error, we adapt the Recursive Model Index\n(RMI) structure [28] to train the learned model.\nHowever, the idea of learning models to maintain the key\nrelationships while disturbing keys consistently in DHT-based\nsystems poses several challenges. First, we need to devise a\nstrategy for managing key mapping and peer addressing, as\nwell as utilizing the relationships between keys to conduct\nrange queries efficiently. Second, the distributed environment is\nhighly dynamic and characterized by frequent network churns;\nthis requires the protocol to quickly adapt to network changes.\nThird, as the network expands and new data are introduced, the\npreviously established Cumulative Distribution Function (CDF)\non which the model was trained may no longer accurately\nrepresent the new data distribution. Consequently, the learned\n1arXiv:2508.14239v1  [cs.NI]  19 Aug 2025\n\nmodel might not distribute data as uniformly as traditional hash\nfunctions, posing additional challenges for load balancing.\nIn response to these challenges, our protocol, LEAD, elabo-\nrates on the methodologies for applying learned models within\nDHT-based systems, focusing on the following aspects:\n(1) We first introduce the concept of the Learned Hash Func-\ntion under the realm of distributed key-value systems. We\ndetail the strategy to map and retrieve keys with learned\nmodels for DHT-based systems. This approach renovates\ntraditional hash functions that map keys to random posi-\ntions, allowing LEAD to maintain the inherent order of\nkeys and enhance range query performance.\n(2) LEAD is designed to adapt dynamically to frequent\nchanges in the system such as database size increases,\nnode joins, and departures. It employs mechanisms that\nrapidly update the overlay routing tables and maintain the\nlearned models, ensuring the system remains robust and\nefficient even in highly volatile environments. We propose\na distributed model update method termed the Federated\nRecursive Model (FRM).\n(3) LEAD incorporates a load-balancing model called Shadow\nBalancer using virtual nodes to allocate keys in an even\nmanner that prevents overloading specific nodes, thus en-\nhancing overall system performance and scalability.\n(4) We conduct comprehensive evaluations of LEAD’s perfor-\nmance in both implementation on real networked systems\nand simulations. The evaluation spans various network\nconditions, scales, and topologies, along with diverse\ndatasets and data volumes. Our assessment demonstrates\nLEAD significantly outperforms existing baseline methods\nin range query efficiency, reducing latency by more than\ntenfold compared to traditional methods in current DHT-\nbased systems. Additionally, LEAD exhibits remarkable\nscalability and resilience to network churn, maintaining\nlogarithmic efficiency in single-key query performance.\n(5) We conduct two timely case studies demonstrating LEAD’s\neffectiveness in real-world applications that require effi-\ncient range queries: key-value cache management for LLM\nserving and the InterPlanetary File System (IPFS).\nBeyond the immediate motivation of accelerating\nrange queries in classical DHT deployments, the same\norder-preserving learned hash that powers LEAD unlocks\na diverse set of emerging workloads: it can collocate\nsemantically close embeddings in vector databases that serve\nretrieval-augmented LLMs, shard the rapidly growing key–value\ncaches and adapter weights of distributed transformer inference\nwithout a central router, deliver geo-temporal IoT telemetry and\nedge-AI models to nearby gateways for low-latency analytics,\nadapt CDN object placement to shifting popularity skew in\nreal time, and provide an range index across heterogeneous\nblockchains. These broader scenarios underscore LEAD’s\npotential as a general storage substrate for next-generation,\ndata-intensive distributed networked systems and motivate the\ndesign choices detailed in the rest of the paper.\nDHT C-Table RP-BST100100025005000Memory (Mb)(a) Memory cost\nDHT C-Table RP-BST10100500010000Number of Messages (b) # of messages\nFig. 1. Micro benchmark of Range Query Performance\nII. B ACKGROUND AND MOTIVATION\nWe conducted a focused micro-benchmark to expose the\ninherent trade-offs of three representative baselines: DHT-based\nkey-value system with Chord [7] (DHT), the centralized range-\nlocation mapping table (C-Table), and the Range-Partition Bi-\nnary Search Tree (RP-BST), whose details are presented in\nAppendix-A. Chord utilizes a ring-like hashing space to manage\nkey-value pairs and is highly efficient for single-key lookups due\nto its logarithmic routing efficiency. However, it struggles with\nrange queries, which often require traversing multiple nodes\nsequentially, thereby increasing latency and message cost. We\nalso implemented DBST [24] as an RP-BST overlay. Each node\nmaintained a BST interval and two routing pointers (left/right).\nWe evaluated the number of messages required to complete\nrange queries and their memory overhead — critical metric\naffecting response time and the efficiency of data retrieval in\ndistributed environments. The experimented system includes\n100 nodes with 200 million key-value pairs from the ‘osmc64’\ndataset (described in Section IV-A) and executed range queries\nfor a range covering 2,000 keys after a given key. As depicted in\nFig. 1, the centralized table substantially reduces the number of\nmessages required to resolve range queries compared to DHT;\nhowever, it imposes a higher memory burden on the system. RP-\nBST–style overlays also improve messaging efficiency; how-\never, they incur considerable memory consumption and control-\nplane complexity. Crucially, they offer limited resilience to\nnetwork churn and impose high costs for index maintenance.\nFurthermore, both the centralized table and RP-BST overlay\nrequire a dedicated coordinator to maintain and synchronize\nmetadata, introducing an additional bottleneck in distributed\ndeployments. This underscores the necessity for a solution like\nLEAD, which aims to merge the advantages of both solutions.\nIII. LEAD D ESIGN\nA. System Overview\nThe section describes the LEAD system. It details the\nmethodologies employed in LEAD for key mapping using its\nLearned Hash Function , addressing peers during node joins\nand departures, data retrieval mechanisms tailored for queries,\nand balancing the loads. Additionally, the protocol outlines\nstabilization and recovery strategies to handle system dynamics.\nFig. 2 presents the system design of LEAD. At a high level,\nphysical nodes within the system are virtualized into multiple\nvirtual nodes, each functioning as independent peers within a\nstructured overlay network. Central to each peer is the learned\n2\n\nr1_v3r1_v1\nr1_v2r2_v3\nr2_v2\nr2_v3r3_v3\nr3_v1\nr3_v2Node r1\nVirtual Node r1_v1\nVirtual Finger Table\nLearned\nHash\nFunction\nIn-memory\nDatabaseRecursive\nModel\nVirtual Node r1_v2\nVirtual Node r1_v3LEAD\nOverlayFig. 2. LEAD System Design\nKeyPosition\nCDF of Key-value PairsLearned Hash Function\nHashing\nSpaceMapRecursive ModelTrainKey\nDistribute\nHash V alueLEAD Overlay Fig. 3. Key mapping with a learned hash function\nModel\nInitialization \nr1_v2r2_v3r2_v2\nr3_v1LEAD\nOverlay...Update \nLeaf Model Parameters Key K  \nModel Aggregation\nModel AggregationModel Aggregation Fig. 4. Decentralized Model Update\nmodel utilized for efficient and in-order key mapping. This\nis complemented by a consistent hashing function employed\nspecifically for peer addressing. Each peer also maintains a\nvirtual finger table, the component for storing updated routing\ninformation and facilitating effective data queries. Additionally,\npeers are equipped with an in-memory database dedicated to\nthe storage and rapid retrieval of key-value pairs.\nB. Key Mapping with a Learned Hash Function\nLEAD uses a learned hash function for key mapping, as\nshowed in Fig. 3. Unlike traditional hash functions, which\naim to map keys to random values within a specified range,\nthe learned hash function strategically maps keys to order-\npreserving values in a hashing space. Utilizing the cumulative\ndistribution function (CDF) of keys managed on the network,\nit maintains the inherent order of these keys while mapping\nthem to a hashing space. This preservation of key relationships\nenhances systems with the capability for in-order data retrieval.\nWe employ the Recursive Model Indexes (RMI) structure [28]\nto implement the learned hash function in LEAD. In Section\nIV-C9, we will show that RMI provides the lowest latency\ncompared to other learned models. The RMI structure is a\nhierarchy of models, where at each stage the model determines\nthe appropriate child model to engage for a specified key. At the\nleaf level, models predict the relative position of a key within a\ndataset. A scale factor, S, is then applied to translate this relative\nposition into a hashing space comprising Hhash values. For\ninstance, considering a two-stage RMI trained on Nkey-value\npairs, the learned hash function, denoted as LearnedHASH ,\ncan be articulated as follows:\nLearnedHASH (key) =⌊N\nH×f⌊Ba×f1(x)b\nN⌋\n2 (K)⌋(1)\naB referred to as the branching factor that determimines the number of\n\"buckets\" that data is divided into by the stage-one model\nbfireferred to as the ith stage model\nLearnedHASH is trained by optimizing the parameters\nof the given model by minimizing the squared error of\nits predictions. Specifically, a model kat stage ρ, denoted\nbyf(k)\nρ, is trained with the following loss function [27]:\nLρ=X\n(x,y)a(f⌊Mρb×fρ−1(x)\nN⌋\nρ (x)−y)2(2)\naxis the key, y∈[0, N)is its relative position within a dataset\nbNumber of models at stage ρWe introduce three systems-level optimizations that are\ncritical for a fully-decentralized overlay for the vanilla\nRMI: (i) Auto-Model Selection. At bootstrap time a peer\nruns a lightweight mountain-climbing probe adapted from\nthe learned-index tuner —that trains candidate leaf predictors\non a 1% sketch of its local key sample, ranks them by\n99-th-percentile prediction error and instantiates the for an\noptimal trade-off between model size and prediction error. (ii)\nThe vanilla RMI assumes a fixed target domain. In practice,\nnode joins and virtual-node churn change the effective density\nof the overlay, so a leaf that once mapped to may need only\nhalf that span an amount of updates later. Each peer therefore\nattaches a 2-field anchor 〈offset,scale 〉to its leaf model: the\non-line gradient update adjusts offset to keep the median key\ncentered and dials scale up/down with a 2-bit PID controller\nso the 95 % key-quantile always ends near the right edge of\nthe peer’s virtual-ID window. (iii) LEAD integrates a Federated\nRecursive Model (FRM) within its Learned Hash Function,\nenabling collaborative learning among peers for dynamic model\nupdates. This decentralized design ensures load balancing and\nseamless request handling during model updates, as detailed\nin Section III-E3. LearnedHASH maps each key to a hash\nvalue within the same hashing space used for peer addressing.\nWhile hash collisions for different keys are permissible, the\nhash value’s primary role is to distribute the key across the\nnetwork, not to serve as a unique identifier. Each key-value\npair, identified by the key K, is assigned to the first peer whose\nV ID (as detailed in Section III-D) either equals or follows the\nhash value produced by LearnedHASH (K).\nModel initialization and re-training. We assume the system\nstarts with a small number (<10) of nodes with a limited amount\nof data. Hence, the very first model training can be conducted on\nan arbitrary node without causing a scalability problem. Then\nmore nodes and data join the system, hence one of the key\ncontribution of LEAD is to adjust the network for newly\njoined nodes and re-train the learned model for new data .\nThe re-training mechanism will be detailed in Sec. III-E3.\nC. Load balancing with virtual nodes\nAchieving balanced load distribution in distributed key-value\nsystems remains challenging. These systems contain heteroge-\nneous nodes, with varied storage capacity and network band-\n3\n\nwidth. Additionally, nodes may experience resource shortages\ndue to higher-priority tasks or hotspots (popular data items that\nattract many requests). These factors undermine the randomiza-\ntion and uniformity that consistent hashing aims for, leading to\nuneven load distribution, bottlenecks, and inefficiencies within\nthe system. To address these challenges, LEAD employs a\nload balancing model called Shadow Balancer , which utilizes\nvirtual nodes to optimize key distribution across the network\nand alleviate hotspot effects. As illustrated in Fig. 2, each\nphysical node is virtualized into multiple virtual nodes, with\neach operating as an independent peer within the network. To\nfacilitate efficient peer addressing and data retrieval processes,\nthis design also leverages consistent hashing to ensure that these\nvirtual peers are distributed as evenly as possible across the\nhashing space. The operational policy of the Shadow Balancer\nis formalized as follows:\n(1) Each node virtualizes itself into kvirtual peers, where k\nis adjustable according to the node’s capabilities.\n(2) In response to resource bottlenecks, a node plans the\ndeparture of virtual peers that manage fewer requests.\nEven in resource-constrained environments, the Shadow Bal-\nancer adds minimal overhead. See Appendix -B for its detailed\nanalysis.\nD. Peer Addressing\nAlong with the learned hash function, LEAD employs a\nconsistent hashing mechanism known as PeerHASH to\nassign an m-bit identifier, denoted as V ID , to each peer\nin the network. Specifically, our implementation of LEAD\nutilizes a universal hash function as PeerHASH . Each\nphysical node, referred to as N, hosts one or more virtual\nnodes, collectively called V. These virtual nodes are assigned\nunique port numbers, enabling direct inter-peer communication\nwithout intermediaries. The V ID for each peer can be derived\nby hashing a concatenation of the corresponding node’s IP\naddress and its port number using PeerHASH . Every V\nmaintains its own set of network routing information in a\nstructure known as virtual finger table . In a hashing space\nholding hhashing values, the table holds ⌊logh⌋entries, with\neach entry comprising a V ID and the corresponding node’s\nIP address. Similar to Chord, each ithentry in the virtual\nfinger table of a virtual node Videntifies the first node, S, that\nsucceeds Vby at least 10i−1positions in the hashing space for\npeer addressing. We defines the Successor (x)as the first peer\nwhose V ID is equal to or follows a hash value xin the peer\naddressing space. Consequently, the ith entry of the virtual\nfinger table of V, denoted as vfinger (i), can be formalized as\nvfinger (i) =successor (V ID + 10i−1) (3)\n1) Node Joins and Departs: To maintain the status of Vin\na dynamic network, each peer Vmust preserve the status of its\nsuccessor. The process for a node ( N) to join the network is\noutlined in the following procedures:\n(1)Initialization: A new node initializes itself either as the\nfirst node in an empty network or by obtaining information\nabout an existing peer ( V0) that is part of the network.(2)Node virtualization: The node Ncreates nvirtual nodes\n(Vs) and assigns them nunique ports. Their Virtual IDs\n(V ID s) are then generated using the PeerHASH .\n(3)Successor Discovery: Each virtual node Vdispatches\na Remote Procedure Call (RPC) to V0to lookup for\nSuccessor (V ID )and obtain its knowledge of the net-\nwork, including the successor’s predecessor, successor,\nand virtual finger table. The lookup mechanics for\nSuccessor (V ID )are further detailed in Section III-E1.\n(4)Status Acknowledge: Upon identifying its successor peer\nSuccessor (V ID ), the virtual node Vestablishes itself\nwithin the network by setting Successor (V ID )as its\nimmediate successor and adopting Successor (V ID )’s\ncurrent predecessor as its own. Subsequently, Vissues\nRPC to Successor (V ID )instructing it to update its\npredecessor record to V. Concurrently, Vsends another\nRPC to the predecessor of Successor (V ID ), requesting\nan update of its successor record to V. Moreover, Vcopies\nthe 2nd to ithentries of the virtual finger table from\nSuccessor (V ID ), which accelerates its initialization and\nstabilizes its initial operations within the network.\n(5)Key Transfer: Once the virtual node Vhas successfully\njoined the network, it initiates the key transfer process:\nVrequests Successor (V ID )to transfer the appropriate\nkey-value pairs that fall within its responsibility range.\nFor planned node departures, the node Nnotifies the imme-\ndiate successors and predecessors of its managed virtual nodes\nV. Subsequently, these virtual nodes Vtransfer their key-value\npairs to their predecessors.\n2) Virtual Finger Table Update: Accurate and up-to-date\nrouting information is crucial for the efficiency and reliability\nof LEAD. LEAD maintains the peer addressing information in\nvirtual finger tables. Periodically, each peer updates its virtual\nfinger table by sending RPCs across the network to obtain each\nentry’s latest successor and their status. Additionally, events\nsuch as node joins, departures, and failures trigger the affected\nnodes to update their virtual finger tables.\nE. Data Retrieval\n1) Single Key Lookup: The distributed single key lookup\nprocess in LEAD aims to locate the immediate successor of\na key by identifying the first peer on the network whose VID\nequals or follows the hash value of the given key in the hashing\nspace. Pconsults its virtual finger table to execute an optimal\njump towards the key’s hash identifier. This involves selecting\nthe farthest preceding peer in the finger table that does not\nexceed the key’s identifier, assuming this peer possesses closer\nor direct knowledge of the key. The query is then routed to this\nselected node, which follows the same procedure. This iterative\nprocess continues until the query reaches the peer responsible\nfor managing the key, denoted as S. Upon locating the key,\nSdispatches an RPC directly back to Pwith the requested\ndata, effectively completing the retrieval process with enhanced\nefficiency and minimized latency.\n2) Range query: LEAD leverages the Learned Hash Function\nto distribute keys across the network while preserving their\n4\n\nReply\nr1_v5\nr1_v4\nr1_v3r1_v1\nLookup Key KLookup Key K r1_v2r2_v3If didn't get\nall N keys\nr2_v2\nr2_v3Lookup Key K\nr2_v5\nr2_v4r3_v3\nr3_v1\nr3_v5r3_v2r3_v4\nLEAD\nOverlay\nRoutingVirtual\nFinger Table\nKey K  \nRoutingVirtual \nFinger TableVirtual \nFinger TableHash K using\nLearned Hash FunctionRange Query for \nN keys after Key K  \nIn-Memory\nRange QueryFig. 5. Range Query in LEAD\nrelationships in order-preserving hash values. Range queries in\nLEAD are handled based on the order-preserving key mapping\nby the Learned Hash Function. To execute a range query for a\nsequence of nsuccessive key-value pairs starting from key K,\nthe initiating peer Pfirst applies the Learned Hash Function to\nhashK(as shown in Fig. 5), yielding the hash value LK. Using\nthe single-key lookup mechanism described in Section III-E1, P\nlocates the peer Sresponsible for K. Once the query reaches S,\nSperforms a local range query within its in-memory database\nto retrieve the sequence of key-value pairs. If Sholds only\na portion of the required sequence, it forwards the remaining\nquery to its successor. This forwarding process is repeated,\nmoving through the chain of successors, until all nkeys are\nretrieved. The final peer to fulfill the range query then sends\nthe complete set of results back to the initiating peer P.\n3) Model Update: While the learned hash function in LEAD\nefficiently distributes new key-value pairs across the network,\nchallenges arise when this model no longer aligns with the\noverall Cumulative Distribution Function (CDF) of the keys\nmanaged across the network. Such misalignment can lead to\nincreased hash collisions and an uneven distribution of key-\nvalue pairs, potentially overloading specific network peers. As\ndetailed in Section IV-C10, the Learned Hash does not neces-\nsitate updates until new key-value pairs constitute up to 40%\nof the network’s storage for the tested datasets. Sub-optimized\nlearned hash functions do not impact the correctness of\nsystem operations , but they may affect load balancing if there\nis a significant logarithmic discrepancy between the learned\nhash function and the current data distribution. Model updates\ncan help optimize the workload balancing across the network.\nTo effectively manage these discrepancies, LEAD is proposed\nwith the Federated Recursive Model (FRM) within its Learned\nHash Function, promoting decentralized and cooperative learn-\ning among peers for dynamic model updates. As showed in\nFig. 4, FRM incorporates the hierarchical structure of Recursive\nModels, with each peer in the network incrementally refining\nits segment of the leaf models based on locally observed data\nchanges. The L0 layer in the FRM structure performs approxi-\nmate predictions to identify the leaf model for specific keys. The\nhierarchical structure maintains stability in the L0 parameters\nwhen the model captures the approximate CDF of existing data.\nAs such, when new keys are integrated into the network, the\nfocus of FRM is on refining the corresponding leaf modelsfor the unlearned keys. Each peer operates with two versions\nof the Recursive Model: one active in the current Learned\nHash Function and another reserved for updates. The system\ncontinuously monitors key distribution across the network in\na decentralized manner through the tracking of the proportion\nof new key-value pairs integrated since the last model update\nat the peer level. When a new key-value pair is introduced\nto the network, the corresponding peer calculates the median\nindex of keys it manages to determine the relative index for\ntraining, using its copy of the model designated for updates. The\npeer then selects the appropriate leaf model based on L0 layer\npredictions. Once the leaf model is identified, the peer refines\nthis model. The relative index for training each new key is\ncalculated by determining the median index of immediate keys\ncurrently managed by the peer. Given a network comprising\nnpeers, with kkeys distributed through the Learned Hash\nFunction, we explore the scenario where madditional keys\nare introduced. To ascertain the proportion of these new keys\nobserved by any given peer causes the total new keys on the\nnetwork to exceed a predefined threshold t, we can model this\nexpectation asm\nk+m, assuming a relatively balanced load across\nthe network. Then, we can achieve the threshold at tof the\nnew key-value pairs observed by a peer, where the total new\nkeys on the network exceed tof the total keys managed on\nthe network since the last update with high probability. During\nthe early phase of the LEAD network, when only a few peers\nare present, a randomly selected training peer is designated to\ninitialize FRM. This initial coordinator is selected based on\ncriteria such as computational power and network load. Once\nchosen, all peers in the network transfer their key-value pairs\nto this node. The central node then performs batch training to\nestablish the initial parameters for the learned hash function.\nThe process begins with a lightweight Model-Scout module\nthat benchmarks multiple candidate leaf families (e.g., Linear,\nRadixSpline). A quick mountain-climbing search is then used\nto tune the model parameters, aiming for an optimal trade-\noff between model size and prediction error. Upon successful\ntraining, the model is adopted by other peers on the network\nthrough the stabilization process as discussed below. Peers are\nactively monitoring the proportion of new key-value pairs joined\nsince the last model update. Once the proportion of new key-\nvalue pairs observed exceeds a threshold - specifically, 40%\nas identified in our empirical study in Section IV-C10, the\npeer flags the readiness status for the model update as true in\nits heartbeat message. Upon a peer being ready for a model\nupdate and detecting that a majority of its neighbors on the\nsuccessor and predecessor list (e.g., 90%) are also flagged for\nupdates, it takes the role of a transient coordinator. Then, it\nsends the flagged neighbors a Remote Procedure Call (RPC)\nto request confirmation of status and transfer of parameters.\nWhen such RPC is received by a peer, it pushes the updated leaf\nparameters to the transient coordinator, acknowledges readiness\nfor the model update, and then resets its update-ready status,\nensuring no redundant or conflicting update processes occur.\nDuring the parameter transfer, only the segments that have\n5\n\nchanged are pushed to minimize data transfer size—for\ninstance, only about 12 KB for approximately 1000 linear\nleaf model parameters and their segments stored in 32-bit\nformat . After receiving acknowledgments from its neighbors,\nthe transient coordinator aggregates the updated leaf model\nparameters from these peers through the averaging operation.\nOnce the new model is consolidated, a new version number will\nbe assigned to facilitate network-wide recognition and adoption.\nPeers in LEAD periodically check for the latest model version\nvia heartbeat messages with their neighbors. The sectional\ntransient inconsistency caused by updates does not compromise\nthe continuous service of the LEAD system, as peer-addressing\nrelies on an independent hash function. Moreover, during\ncooperative model updates, the system remains operational ;\nonly a subset of peers performs asynchronous updates on the\nleaf models. This is targeted at specific key segments and occurs\nuntil significant data changes are detected. Thus, the integrity\nof the system is preserved.\nF . Stabilization and Failures Recovery\nHandling system churn – where nodes frequently join or\ndepart – is crucial for sustaining system integrity and perfor-\nmance. LEAD is designed to adapt rapidly to these changes\nthrough robust stabilization and failure recovery mechanisms.\nThe correctness of LEAD is dependent on the current knowl-\nedge of its successors and predecessors within the network.\nAdditionally, the efficiency of query handling is contingent\nupon the timeliness and accuracy of the virtual finger tables.\nTo maintain this information, each peer periodically stabilizes\nthemselves in the network through successor and predecessor\nverification, heartbeat communications, and virtual finger table\nmaintenance. Building on its stabilization mechanisms, LEAD\nincorporates resilient failure recovery strategies to address peer\nfailures. The details are presented in the Appendix -C.\nIV. E VALUATION\nThis section presents the evaluation of LEAD through both\ntestbed implementation and large-scale simulations, along with\nreal-world case studies.\nA. Methodology\nHardware and environments. The testbed implementation\ncomprises nine virtual machines in public clouds, including\nthree types of machines: one with two Intel Xeon Silver 4314\n2.40 GHz 16-Core CPUs and 128GB of DDR4 2666MHz\nmemory; one with an Intel Xeon E5-2687W v4 3.00GHz 12-\nCore CPU and 32GB of DDR4 2400MHz memory; and the\nother with an Intel Core i7-7700 3.60GHz 4-Core CPU and\n16GB of DDR4 2400MHz memory. They communicate through\nthe Internet. Each virtual machine runs 10 virtual nodes in the\noverlay hence the overlay includes 90 peers in total. We utilize\nRedis for in-memory key-value storage on peers.\nThe simulator we built, called p2psim+, is based on a\npublicly-available discrete event-driven simulator p2psim [29]\nrunning on an Ubuntu 22.04 LTS desktop with an AMD Ryzen 7\n3700X 3.6 GHz 8-Core CPU, complemented by 32GB of DDR4\n3200MHz RAM across two 16GB modules. P2psim is widelyrecognized and utilized within the community [30]. We added\nover 3,000 lines of C++ code to enhance the simulator. These\nextensions include the integration of LEAD, the support for\nuser-defined network topologies, customized network behavior\nobservers, and scalability enhancements for large experiments.\nWe utilize the implementation of RMIs in Rust [28]. We will\npublish p2psim+ upon the acceptance of this paper.\nDatasets. We leverage four real-world datasets from the SOSD\nbenchmark [31], each consisting of 200 million 64-bit unsigned\nintegers as keys. The datasets encapsulate a broad spectrum of\ndata distributions and sources, described as follows:\n(1) ‘osmc64’: uniformly sampled OpenStreetMap Cell IDs\n(2) ‘face64’: randomly sampled Facebook user IDs\n(3) ‘amzn64’: Amazon book sale popularity data\n(4) ‘wiki64’: Wikipedia article edit timestamps\nTo accurately emulate real-world network topologies in our\nsimulations, we incorporate the PlanetLab Dataset from the\nNetwork Latency Datasets [32]. This dataset captures round-trip\ntimes (RTTs) between 490 nodes dispersed across the PlanetLab\nnetwork. Specifically, we employ the \"PlanetLabData_1\" as the\nlatency model to construct the PlanetLab topology.\nBaselines. We use four baseline methods in our experiments:\nthe batch query approach on Chord [7] DHT with batch sizes\nof either 100 or 1000, and the recent work Marques [25].\nWe let Chord batch single-key queries together and send them\nas one or multiple consolidated requests across the network.\nMarques [25] is a recent enhancement on Chord [7] for range\nquery efficiency. We exclude DBST [24] from direct com-\nparison, as it relies on a centrally constructed binary search\ntree and incurs high overhead—analyzed in Section II—that\nrenders it unsuitable for decentralized environments. Similarly,\nRQIOT [26] assumes centralized order-preserving hashing with-\nout providing a decentralized construction mechanism. Neither\nDBST nor RQIOT offer open-source implementations, further\nlimiting their applicability in reproducible and fair comparison\nwithin our distributed system framework.\nB. Testbed Performance\nFig. 6 presents the latency benchmark results obtained from\nthe real-machine testbed implementation. For LEAD, a pre-\ntrained two-layer model incorporating both linear and cubic\nlayers is employed. For each experimental run, we inserted\n200 million 64-bit unsigned integers from each of the four\ndatasets. Then, we conducted range queries for ranges with\nvarying numbers of keys, from 500 to 10,000, subsequent to\na specified key. To ensure the reliability of the results, each\nquery was repeated ten times, and we calculated the average\nlatency for each data point. As demonstrated in Fig. 6, as the\nquery range expands, LEAD maintains near-constant latency\nfor range queries. In contrast, both the Batch Query method\nand Marques exhibit rapidly increasing latencies. For instance,\nin the experiment using the ’osmc64’ dataset, a range query\nfor 500 keys resulted in latencies of 259 ms for Batch Query\nwith a batch size of 1000 and 557 ms for Marques, while\nLEAD efficiently resolved the query in just 145 ms. As the\nquery range extended to 4,000 keys, the latency for Batch Query\n6\n\n/uni00000026/uni0000004b/uni00000052/uni00000055/uni00000047/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000057/uni0000004b/uni00000048/uni00000003/uni00000054/uni00000058/uni00000048/uni00000055/uni0000005c/uni00000003/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000014/uni00000013/uni00000013 /uni00000026/uni0000004b/uni00000052/uni00000055/uni00000047/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000057/uni0000004b/uni00000048/uni00000003/uni00000054/uni00000058/uni00000048/uni00000055/uni0000005c/uni00000003/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000014/uni00000013/uni00000013/uni00000013 /uni00000030/uni00000044/uni00000055/uni00000054/uni00000058/uni00000048/uni00000056 LEAD\n500 1000 2000 4000 8000 10000\nQuery Range2505001000250050001000020000Query Latency (ms)(a) osmc64\n500 1000 2000 4000 8000 10000\nQuery Range2505001000250050001000020000Query Latency (ms) (b) face64\n500 1000 2000 4000 8000 10000\nQuery Range2505001000250050001000020000Query Latency (ms) (c) amzn64\n500 1000 2000 4000 8000 10000\nQuery Range2505001000250050001000020000Query Latency (ms) (d) wiki64\nFig. 6. Latency of range queries on various datasets in the real-machine testbed\n/uni00000026/uni0000004b/uni00000052/uni00000055/uni00000047/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000057/uni0000004b/uni00000048/uni00000003/uni00000054/uni00000058/uni00000048/uni00000055/uni0000005c/uni00000003/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000014/uni00000013/uni00000013 /uni00000026/uni0000004b/uni00000052/uni00000055/uni00000047/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000057/uni0000004b/uni00000048/uni00000003/uni00000054/uni00000058/uni00000048/uni00000055/uni0000005c/uni00000003/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000014/uni00000013/uni00000013/uni00000013 /uni00000030/uni00000044/uni00000055/uni00000054/uni00000058/uni00000048/uni00000056 LEAD\n500 1000 2000 4000 8000 10000\nQuery Range2505001000250050001000020000Query Latency (ms)\n(a) osmc64\n500 1000 2000 4000 8000 10000\nQuery Range2505001000250050001000020000Query Latency (ms) (b) face64\n500 1000 2000 4000 8000 10000\nQuery Range2505001000250050001000020000Query Latency (ms) (c) amzn64\n500 1000 2000 4000 8000 10000\nQuery Range2505001000250050001000020000Query Latency (ms) (d) wiki64\nFig. 7. Latency of range queries on various datasets from large-scale simulations\nChord with the query batch size of 100 Chord with the query batch size of 1000 Marques LEAD\n500 1000 2000 4000 8000 10000\nQuery Range10255010020050010005000Number of Messages\n(a) osmc64\n500 1000 2000 4000 8000 10000\nQuery Range10255010020050010005000Number of Messages\n (b) face64\n500 1000 2000 4000 8000 10000\nQuery Range10255010020050010005000Number of Messages\n (c) amzn64\n500 1000 2000 4000 8000 10000\nQuery Range10255010020050010005000Number of Messages\n (d) wiki64\nFig. 8. Number of messages of each range query on various datasets.\n/uni00000026/uni0000004b/uni00000052/uni00000055/uni00000047/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000057/uni0000004b/uni00000048/uni00000003/uni00000054/uni00000058/uni00000048/uni00000055/uni0000005c/uni00000003/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000014/uni00000013/uni00000013 /uni00000026/uni0000004b/uni00000052/uni00000055/uni00000047/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000057/uni0000004b/uni00000048/uni00000003/uni00000054/uni00000058/uni00000048/uni00000055/uni0000005c/uni00000003/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000014/uni00000013/uni00000013/uni00000013 /uni00000030/uni00000044/uni00000055/uni00000054/uni00000058/uni00000048/uni00000056 LEAD\n100 200 300 400 490\nNumber of nodes6008001000Mean Latency (ms)\nChord\nMarques\nLEAD\n(a) Single Key Lookup\nUniform-distributedExponential-distributedPareto-distributed\nNetwork Churn Conditions2505001000250050001000020000Query Latency (ms) (b) Network Churns\n100 200 300 400 490\nNumber of Nodes2505001000250050001000020000Query Latency (ms) (c) Network Scales\nEuclideanRandom GraphRandom Sampling\nTopology2505001000250050001000020000Query Latency (ms) (d) Network Topology\nFig. 9. Latency of range queries under various conditions\nescalated to over 1,300 ms and for Marques to over 750 ms.\nSuch latencies become prohibitive for most high-throughput\napplications. LEAD continued to deliver results in less than\n150 ms, showcasing its superior performance and scalability.\nC. Simulation Results\nThe simulated system consists of 490 nodes configured\naccording to the PlanetLab topology. Each node operates 10\nvirtual nodes. By default, we employed the pretrained two-\nlayer models for LEAD, which incorporates both linear and\ncubic layers. Each simulation spanned a logical duration of\n120 minutes. To emulate the dynamic nature of real-world\ndistributed systems, node lifetimes were modeled with a uni-form distribution, averaging 80 logical minutes. The network\ndynamics were initiated by exiting nodes from the network\nafter their lifespan concluded and rejoining them following a\nuniformly distributed interval, averaging 10 logical minutes.\nEach time a node exited and rejoined, its routing state was\nreset to preserve network integrity. Furthermore, to adapt to\nnetwork changes effectively, the stabilization timer for each\npeer was set to 1000 logical ms, enabling regular updates to\ntheir finger tables and stabilization of their successor states.\nRange queries were conducted at regular intervals of five logical\nminutes throughout the simulation. Each query aimed to retrieve\na sequence of Nkeys subsequent to a specified key M. For\neach query, we documented both the latency and the number\n7\n\nof routing steps incurred. Following the completion of each test\nrun, we calculated the average values for these metrics.\n1) Range query performance: Fig. 7 illustrates the range\nquery latency obtained from the simulation. Each experimen-\ntal cycle involved inserting 200 million 64-bit unsigned in-\ntegers from one of four distinct datasets: ’osmc64’, ’face64’,\n’amzn64’, and ’wiki64’. As the query range extends, the near-\nconstant latency exhibited by LEAD underscores its substantial\nsuperiority in query latency compared to other baseline methods\nacross all datasets tested. Again, the results show that LEAD\nsignificantly reduces the range query latency.\n2) Query messages: To complement our latency analysis,\nwe quantified the number of messages for each range query\nexecuted. Fig. 8 depicts the number of messages for range\nqueries required across various test configurations, elucidating\nLEAD’s optimized path efficiency for range queries. LEAD\ncosts much fewer messages compared to the other baselines.\nFor example, when the query range is 5000, LEAD only costs\n<15messages per query, while Marques needs >50messages\nand Chord, even with batching, requires >200 messages for\nsize 1000 and >1000 messages for size 100. LEAD reduces\nthe query messages by over 80%. We observe that LEAD\ntypically incurs an amount of messages similar to those of a\nsingle-key lookup, which is logarithmic relative to network size.\n3) Single-key performance: Alongside evaluating range\nquery performance, we scrutinized the single-key lookup perfor-\nmance of each baseline method, utilizing the ‘osmc64’ dataset\nas a representative example. Fig. 9(a) demonstrates LEAD up-\nholds competitive performance with Chord in single-key query\nlatency. This is attributed to its adherence to the foundational\ndesign of Chord. On the other hand, Marques’s multi-level\noverlay structure introduces more than a 50% increase in latency\nfor single-key queries compared to the original Chord.\n4) Network churn resistance: As illustrated in Fig. 9(b),\nthe resilience of LEAD is demonstrated through its ability to\nmaintain continuous service performance under various network\nchurn conditions. The test setup involved populating the system\nwith 200 million key-value pairs from the ‘osmc64’ dataset and\nexecuting range queries for 4,000 keys. Then we emulated the\nnetwork dynamics through exiting nodes from the network after\ntheir lifespan concluded and rejoining them in intervals that\nfollowed uniform, exponential, or Pareto distributions.\n5) Network scale: To assess scalability, we varied the net-\nwork size from 100 to 490 nodes. We utilized the ’osmc64’\ndataset, which consists of 200 million key-value pairs, to\nmeasure latency by executing range queries for 4,000 keys.\nFig. 9(c) illustrates that LEAD consistently outperforms other\nbaseline methods across all network sizes evaluated.\n6) Network topology: Fig. 9(d) demonstrates that LEAD\nconsistently surpasses other baseline methods in range query\nlatency across all evaluated network topologies. In continuation\nof our scalability testing, with the network size held constant\nat 490 nodes, we assessed the performance of LEAD across\nthree synthetic network topologies: 1) Euclidean, in which the\nlatencies between nodes were modeled by their distances in\na two-dimensional Euclidean space; 2) Random graph; and\n/uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013 /uni00000014/uni00000015 /uni00000014/uni00000017 /uni00000014/uni00000019 /uni00000014/uni0000001b /uni00000015/uni00000013 /uni00000015/uni00000015\n/uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000026/uni00000052/uni00000052/uni00000055/uni00000047/uni0000004c/uni00000051/uni00000044/uni00000057/uni00000048/uni00000003/uni0000003b/uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013/uni00000014/uni00000015/uni00000014/uni00000017/uni00000014/uni00000019/uni00000014/uni0000001b/uni00000015/uni00000013/uni00000015/uni00000015/uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000026/uni00000052/uni00000052/uni00000055/uni00000047/uni0000004c/uni00000051/uni00000044/uni00000057/uni00000048/uni00000003/uni0000003c\n/uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002e/uni00000048/uni0000005c/uni00000010/uni00000039/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000003/uni00000053/uni00000044/uni0000004c/uni00000055/uni00000056 /uni00000014/uni00000048/uni00000019\n/uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013/uni00000014/uni00000015/uni00000014/uni00000017/uni00000014/uni00000019/uni00000014/uni0000001b/uni00000015/uni00000013/uni00000015/uni00000015\n/uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000026/uni00000052/uni00000052/uni00000055/uni00000047/uni0000004c/uni00000051/uni00000044/uni00000057/uni00000048/uni00000003/uni0000003b/uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b /uni00000014/uni00000013/uni00000014/uni00000015/uni00000014/uni00000017/uni00000014/uni00000019/uni00000014/uni0000001b/uni00000015/uni00000013/uni00000015/uni00000015/uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000026/uni00000052/uni00000052/uni00000055/uni00000047/uni0000004c/uni00000051/uni00000044/uni00000057/uni00000048/uni00000003/uni0000003c\n/uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002e/uni00000048/uni0000005c/uni00000010/uni00000039/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000003/uni00000053/uni00000044/uni0000004c/uni00000055/uni00000056 /uni00000014/uni00000048/uni00000019(a) Chord\n/uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013/uni00000014/uni00000015/uni00000014/uni00000017/uni00000014/uni00000019/uni00000014/uni0000001b/uni00000015/uni00000013/uni00000015/uni00000015\n/uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000026/uni00000052/uni00000052/uni00000055/uni00000047/uni0000004c/uni00000051/uni00000044/uni00000057/uni00000048/uni00000003/uni0000003b/uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b /uni00000014/uni00000013/uni00000014/uni00000015/uni00000014/uni00000017/uni00000014/uni00000019/uni00000014/uni0000001b/uni00000015/uni00000013/uni00000015/uni00000015/uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000026/uni00000052/uni00000052/uni00000055/uni00000047/uni0000004c/uni00000051/uni00000044/uni00000057/uni00000048/uni00000003/uni0000003c\n/uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002e/uni00000048/uni0000005c/uni00000010/uni00000039/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000003/uni00000053/uni00000044/uni0000004c/uni00000055/uni00000056 /uni00000014/uni00000048/uni00000019 (b) LEAD\nFig. 10. Comparison of Key-Value Pair Distribution\n3) Random sampling, in which the inter-node latencies were\nrandomly assigned within a range.\n7) Load balancing: Figure 10 compares the load distribution\nof a traditional Chord DHT setup against LEAD integrated\nwith our Shadow Balancer , which enables 10 virtual nodes\nper physical node. The heat map representation shows that\nLEAD significantly enhances load balancing within the net-\nwork. LEAD with Shadow Balancer exhibits a more uniform\ngreen color across the network, indicating a well-balanced load\namong nodes. As depicted in Fig. 11, increasing the number\nof virtual nodes decreases the standard deviation, suggesting\nbetter load dispersion. Specifically, the inflection point at 10\nvirtual nodes per node in the PlanetLab topology indicates an\noptimized balance. Beyond this point, additional virtual nodes\ndo not significantly improve load balancing, thereby identifying\n10 virtual nodes per node as an ideal configuration for the\nestablished network.TABLE I\nRECURSIVE MODEL EVALUATION\nModel Maximum Log2 Error Average Log2 Error Size (Mb)\nLinear 25.79 18.51 0.75\nRadix 21.28 12.79 1.75\nCubic 18.63 9.82 12.00\n8) Learned models: LEAD leverages the Recursive Mode\nstructure for fast and accurate order-preserving key mapping\nwith the learned hash function. Selecting the optimal model\ntype during the training phase is crucial to minimize the\nprediction error, thereby ensuring LEAD achieves optimal load\ndistribution. Fig. 12 illustrates the results of range query latency\non the ‘osmc64’ dataset for queries ranging from 500 to 10,000\nkeys using three typical models as detailed in Table I. For the\n‘osmc64’ dataset, our evaluations revealed that both the Radix\nand Cubic models can aptly fit its distribution, showcasing\neffective performance in managing range queries. In contrast,\nalthough the linear model offers benefits in terms of smaller\nmodel size, it results in increased error bounds, which can\nadversely affect the system performance.\n9) Learned Indexes with LEAD: In formulating the learned\nmodel for LEAD, we assessed various learned index structures,\nincluding RMI [27], Radix Spline Indexes [33], and Piece-\nwise Geometric Model Indexes (PGM) [34]. Our evaluations,\ndepicted in Figure 13, highlight the Recursive Model structure’s\n8\n\n/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000039/uni0000004c/uni00000055/uni00000057/uni00000058/uni00000044/uni0000004f/uni00000003/uni00000031/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni00000033/uni00000048/uni00000055/uni00000003/uni00000031/uni00000052/uni00000047/uni00000048/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000013/uni00000013/uni00000016/uni00000018/uni00000013/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000013/uni00000013/uni00000036/uni00000057/uni00000044/uni00000051/uni00000047/uni00000044/uni00000055/uni00000047/uni00000003/uni00000027/uni00000048/uni00000059/uni0000004c/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002e/uni00000048/uni0000005c/uni00000003/uni00000036/uni00000057/uni00000052/uni00000055/uni00000044/uni0000004a/uni00000048\nFig. 11. Node Storage Standard Deviation (SD.)\n/uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013\n/uni00000034/uni00000058/uni00000048/uni00000055/uni0000005c/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni0000001b/uni00000013/uni00000013/uni0000001b/uni00000014/uni00000013/uni0000001b/uni00000015/uni00000013/uni0000001b/uni00000016/uni00000013/uni0000001b/uni00000017/uni00000013/uni0000001b/uni00000018/uni00000013/uni0000001b/uni00000019/uni00000013/uni0000001b/uni0000001a/uni00000013/uni0000001b/uni0000001b/uni00000013/uni0000001b/uni0000001c/uni00000013/uni0000001c/uni00000013/uni00000013/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000003/uni00000034/uni00000058/uni00000048/uni00000055/uni0000005c/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c\n/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000003/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f\n/uni00000035/uni00000044/uni00000047/uni0000004c/uni0000005b/uni00000003/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f\n/uni00000026/uni00000058/uni00000045/uni0000004c/uni00000046/uni00000003/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f Fig. 12. Latency vs. RMI Models\n/uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013\n/uni00000034/uni00000058/uni00000048/uni00000055/uni0000005c/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni0000001b/uni00000013/uni00000013/uni0000001b/uni00000014/uni00000013/uni0000001b/uni00000015/uni00000013/uni0000001b/uni00000016/uni00000013/uni0000001b/uni00000017/uni00000013/uni0000001b/uni00000018/uni00000013/uni0000001b/uni00000019/uni00000013/uni0000001b/uni0000001a/uni00000013/uni0000001b/uni0000001b/uni00000013/uni0000001b/uni0000001c/uni00000013/uni0000001c/uni00000013/uni00000013/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000003/uni00000034/uni00000058/uni00000048/uni00000055/uni0000005c/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c\n/uni00000035/uni00000030/uni0000002c\n/uni00000033/uni0000002a/uni00000030\n/uni00000035/uni00000044/uni00000047/uni0000004c/uni0000005b/uni00000036/uni00000053/uni0000004f/uni0000004c/uni00000051/uni00000048 Fig. 13. Latency vs Learned Indexes\nconsistent performance advantage across various query ranges\nwhen integrated with LEAD. Consequently, the Recursive\nModel structure is the preferred choice for LEAD, ensuring\nefficient and accurate range query handling.\n10) Model Update: We randomly selected a portion of the\n’osm’ dataset as the training set, treating the remainder as\nunlearned, new key-value pairs. These new pairs were then\ndistributed across the network, alongside the existing data. To\nquantify the impact of introducing new data on network load\nbalancing, we recorded the standard deviation of key-value pairs\nstored per node as the proportion of new entries increased. This\nmetric was assessed for both versions of LEAD: one without\nmodel updates and one with dynamic model updates enabled.\nFig. 14 illustrates the effects of new key-value pair integration\non load balancing across the network. The results indicate that\nthe key mapping with our learned hash function remains stable,\nwith minimum impacts on load balancing, even as new key-\nvalue pairs constitute up to 40% of the network’s storage.\nFig. 14 also shows that our dynamic model update mechanism\nsustains optimal load balance across the network, demonstrating\nLEAD’s robust adaptability to data changes within the network.\nOur real-world testbed confirms that maintaining LEAD’s\nlearned hash function adds only marginal overhead to a\nvanilla Chord control plane . At the evaluated scale—one\nRMI instance per peer collectively managing 200 million 64-bit\nkey–value pairs—the Linear and Radix models occupy under\n2 MB of DRAM, while the Cubic model remains below 12\nMB. Model Update is likewise cheap and invoked only\nafter the system observes a 40 % drift in new keys :\nevery key insertion triggers a gradient update that averages\n2.1 µs on our testbed—roughly 0.2 % of end-to-end insertion\nlatency and completely hidden by network delay. The model re-\nsynchronisation exchanging ≤60KB—about one-third of the\nbytes a peer already spends during a finger-table refresh cycle.\nOverall, CPU, memory, and network overheads for the RMI\ninstance stay below 4 %, 0.1 %, and 3 %, respectively.\nD. Case study I : KV Cache Management for LLM Severing.\nIn LLM serving, key–value (KV) caches retain the atten-\ntion keys and values of earlier tokens for reuse rather than\nrecomputation. Hence how to share and re-use existing KV\ncaches is a crucial problem [2]–[4], [17], [35]. We consider a\ndistributed LLM inference system where multiple nodes (GPU\nworkers) collaboratively serve incoming queries. Each node\ncaches KV blocks [35] from sequences it has processed, and\nnodes cooperate to serve future queries that may need those\ncached blocks, similar to a CDN. The goal is to leverage theexisting KV cache instead of recomputing from scratch, by\nretrieving cached KV blocks from the network. KV blocks\nbelonging to the same shared prefix can be stored on the\nsame or nearby nodes. Since an LLM needs the KV for all\nprior tokens in the sequence, LEAD can fetch a whole span\nof positions in one efficient range query. That query is routed\nonly to the node(s) responsible for that contiguous key range,\nobviating any need for system-wide broadcasts or gathers. We\nexperiment a distributed inference system consisting of eight\nworker nodes—each equipped with a single NVIDIA A100 80\nGB GPU running the Llama-3 8B model. The KV cache is\nmanaged using the PagedAttention scheme with a fixed 16-\ntoken block size [35]. Every token is issued a composite key.\nOur evaluation workload is Long-DocQA [36], which comprises\n776 lengthy documents paired with 6,400 questions. LooGLE\nconstructs each prompt by prefixing a full document to its\nassociated question; after Zipf-0.6 sampling, prompts average\n10,985 tokens [17], [37]. We simulate a production environ-\nment where cold KV blocks can be offloaded to host RAM,\nresulting in over 500 million KV blocks under management.\nIn the absence of local shared-prefix caching, each inference\nrequest must retrieve on average more than 700,000 blocks\nfrom the network. We compare LEAD against two baselines:\n(i) Centralized KV Router: maintains a global index of KV\nlocations across all workers. (ii) Chord-based DHT: uses a\ndistributed hash table overlay for KV storage. Assuming a\n10 GbE TCP/IP data-center network with a 100 µs average\nround-trip latency, we measure each system’s average block-\nlookup latency, total message count, and cache-hit rate. As\nFig. 15 illustrates, LEAD only add a marginal latency increase\nto the ideal centralized case, while delivering the resilience\nand scalability of a fully decentralized design—dramatically\noutperforming a traditional DHT. Notably, the centralized router\nsuffers from severe query load imbalance, as evidenced by a\nhigh query CoV—the coefficient of variation in the number\nof messages handled per node during range queries. All three\nsystems achieve comparable cache hit rates under identical\nquery correctness guarantees. These results highlight LEAD’s\nfoundation for enabling workload-aware, multi-tiered caching\nacross heterogeneous GPU clusters, supporting dynamic in-\nference pipelines and fault-tolerant, network-aware KV-cache\nmanagement in scalable LLM serving infrastructures.\nE. Case study II : InterPlanetary File System (IPFS).\nInterPlanetary File System (IPFS) [11] is a distributed content\ndelivery network that stores, retrieves, and locates data based on\nthe Content Identifiers (CIDs) of its actual content rather than\n9\n\n/uni00000013/uni00000008 /uni00000014/uni00000013/uni00000008 /uni00000015/uni00000013/uni00000008 /uni00000016/uni00000013/uni00000008 /uni00000017/uni00000013/uni00000008 /uni00000018/uni00000013/uni00000008\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000052/uni00000049/uni00000003/uni00000031/uni00000048/uni0000005a/uni00000003/uni0000002e/uni00000048/uni0000005c/uni00000010/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000003/uni00000033/uni00000044/uni0000004c/uni00000055/uni00000056/uni00000014/uni00000015/uni00000018/uni00000013/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni0000001a/uni00000018/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000013/uni00000013/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002e/uni00000048/uni0000005c/uni00000003/uni00000036/uni00000057/uni00000052/uni00000055/uni00000044/uni0000004a/uni00000048\n/uni0000002f/uni00000028/uni00000024/uni00000027/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000052/uni00000058/uni00000057/uni00000003/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000038/uni00000053/uni00000047/uni00000044/uni00000057/uni00000048\n/uni0000002f/uni00000028/uni00000024/uni00000027/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000038/uni00000053/uni00000047/uni00000044/uni00000057/uni00000048Fig. 14. SD. of Storage with Model Update\nCentralized\nKV RouterChord LEAD020406080100Avg Latency (ms)485.00×\n1.05×\nCentralized\nKV RouterChord LEAD02505007501000125015001750Query Messages180.22×\n0.56×\nCentralized\nKV RouterChord LEAD0.00.51.01.52.02.53.0Query Load COV\n0.07×0.02× Fig. 15. KV Cache Management\n0 10 20 30 40 50\nNumber of Messages0.000.250.500.751.00CDF\nIPFS\nLEAD Fig. 16. CDFs of the message cost in IPFS\nits name or location. With millions of daily content retrievals,\nIPFS supports numerous third-party applications, demonstrating\nits broad utility and impact. However, the traditional DHTs\nused in IPFS, i.e., Kademlia [38], face challenges in handling\nrange queries, which are essential for efficiently retrieving\nsequences of data blocks or related files. To gauge LEAD’s\nbenefit in a production-style CDN, we forked the reference\ngo-ipfs daemon [39] and replaced only its routing module with\na LEAD overlay, leaving libp2p, Bitswap, and the block-store\nunmodified. We emulated an IPFS network with 100 peers\nusing the PlanetLab topology. We generated 100 million syn-\nthetic key-value pairs to represent the logical units structuring\nthe metadata of resources (files). For LEAD, the CIDs were\nmanaged by the learned hash function within SHA-1’s hashing\nspace. The key operation tested was a typical user request\nfor a resource. In this scenario, an edge server retrieves all\nthe blocks containing the metadata for the requested resource,\nwhich consists from 1,000 to 3,000 blocks in our test case. For\nthe emulated IPFS, the server divided the 1,000 lookup queries\ninto 10 batches and sent them over the network. In contrast, for\nLEAD, the query was optimized as a supported meta-operation\nrange query. We recorded the number of hops required to resolve\nthe request for both systems. As illustrated in Fig. 16, the CDFs\nof the retrieval hops for the emulated IPFS and LEAD indicate\na significant reduction in the number of messages required to\ncomplete data block sequences retrieval when using LEAD.\nV. R ELATED WORK\nRange query in DHTs. Current DHT systems have sig-\nnificant limitations in handling range queries. These systems\nare inherently designed for exact key-based queries, and there-\nfore, their hashing mechanisms lose the semantic relationship\nbetween keys—necessary for range queries. Significant efforts\nto facilitate efficient range queries in distributed networked\nsystems have introduced innovative concepts while also reveal-\ning inherent limitations. Early attempts to reconcile hash-based\nload-balancing with ordered access bolted auxiliary data struc-\ntures onto a vanilla DHT: Prefix-Hash-Trees (PHT) [40] and\nRange Search Trees [41] layer Chord-style fingers with a try\nthat must be eagerly split and merged on every insert, leading\nto high control traffic per update and poor churn tolerance.\nArmada [22] utilizes a partition tree model and a tailored\nalgorithm within the FissionE [23] topology to enhance range\nquery efficiency. Nevertheless, its reliance on a customized\nDHT scheme restricts its broader applicability. Similarly, DBST\nsystem [24] integrates binary search tree structures to provide\nefficient range queries for ordered data. These tree constructionsare assumed to be centralized and are not applicable to large-\nscale distributed systems. MARQUES [25] employs space-\nfilling curves within a multi-level overlay structure derived from\nChord [7], targeting enhanced performance for range queries.\nNonetheless, the complexity involved in managing this struc-\ntured network overlay can substantially introduce overheads and\npose scalability challenges. The latest work, RQIOT [26], tried\nto employ order-preserving hashing to handle range queries.\nHowever, how to implement such a hash method, especially in\na dynamic distributed system, is unclear.\nLearned Index Structures and Hash Functions. Recent\nresearch has reimagined traditional indexing by conceptualizing\nindexes as predictive models that estimate the position of a\nkey within a dataset [27], [34], [42]–[46]. These learned index\nstructures combine machine learning techniques with classical\ndata structures to accelerate key lookups. Kraska et al. [27]\nproposed the Recursive Model Index (RMI) to address the\ninaccuracy of using a single model to approximate the dataset’s\nCDF. There has been growing interest in learned hash functions,\nwhere models are trained to map keys to hash buckets in a\ndata-aware manner. Prior works on locality-sensitive hashing\n(LSH) [47]–[49] have explored model-driven hash functions for\napproximate nearest neighbor search. More recently, Sabek et al.\n[50] demonstrated that learned models can achieve comparable\nor even fewer hash collisions than traditional hash functions.\nHowever, while these approaches show promise, integrating\nlearned index structures or hash functions into decentralized sys-\ntems such as Distributed Hash Tables remains unexplored—an\nopportunity that LEAD seeks to address.\nVI. C ONCLUSION\nThis paper introduces LEAD, a novel distributed key-value\nstorage and lookup system designed to enhance the efficiency\nof range queries by incorporating learned models with DHTs.\nLEAD includes the detailed design of training and updating\nlearned models, implementing single-key and range queries,\nachieving load balancing, and dealing with system churns.\nExtensive evaluations on both testbed implementation and simu-\nlations demonstrate that LEAD significantly reduces the latency\nand message cost of performing range queries by by 80%\nto90%+ , compared to existing DHT-based solutions. LEAD\ncan maintain system consistency under dynamic changes and\nvarious system conditions.\nWe believe LEAD opens a completely new field for fur-\nther research on integrating learned models with distributed\nsystems. The implementation details are publicly available at\nhttps://github.com/ShengzeWang/LEAD.\n10\n\nACKNOWLEDGMENT\nThe authors were partially supported by NSF Grants\n2322919, 2420632, 2426031, and 2426940. We thank the\nanonymous shepherd and reviewers for their valuable com-\nments.\nREFERENCES\n[1] Y . Wang, Y . Chen, Z. Li, Z. Tang, R. Guo, X. Wang, Q. Wang, A. C.\nZhou, and X. Chu, “Towards efficient and reliable llm serving: A real-\nworld workload study,” arXiv e-prints , pp. arXiv–2401, 2024.\n[2] W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. H. Yu, J. Gonzalez,\nH. Zhang, and I. Stoica, “Efficient memory management for large language\nmodel serving with pagedattention,” in Proceedings of the 29th Symposium\non Operating Systems Principles , 2023, pp. 611–626.\n[3] L. Zheng et al. , “Sglang: Efficient execution of structured language model\nprograms,” in Proc. of NeurIPS , 2024.\n[4] Y . Cheng, K. Du, J. Yao, and J. Jiang, “Do large language models need\na content delivery network?” arXiv preprint arXiv:2409.13761 , 2024.\n[5] Y . Liu, H. Li, Y . Cheng, S. Ray, Y . Huang, Q. Zhang, K. Du, J. Yao,\nS. Lu, G. Ananthanarayanan, M. Maire, H. Hoffmann, A. Holtzman, and\nJ. Jiang, “Cachegen: Kv cache compression and streaming for fast large\nlanguage model serving,” in Proceedings of the ACM SIGCOMM 2024\nConference , 2024.\n[6] F. Fang, Y . Hua, S. Wang, R. Zhou, Y . Liu, C. Qian, and X. Zhang,\n“Gentorrent: Scaling large language model serving with an overley\nnetwork,” 2025. [Online]. Available: https://arxiv.org/abs/2504.20101\n[7] I. Stoica, R. Morris, D. Karger, M. F. Kaashoek, and H. Balakrishnan,\n“Chord: A scalable peer-to-peer lookup service for internet applications,”\nACM SIGCOMM , vol. 31, no. 4, 2001.\n[8] J. Zarrin, R. L. Aguiar, and J. P. Barraca, “Resource discovery for\ndistributed computing systems: A comprehensive survey,” Journal of\nparallel and distributed computing , vol. 113, pp. 127–166, 2018.\n[9] A. Passarella, “A survey on content-centric technologies for the current\ninternet: Cdn and p2p solutions,” Computer Communications , vol. 35,\n2012.\n[10] M. Coluzzi, A. Brocco, P. Contu, and T. Leidi, “A survey and comparison\nof consistent hashing algorithms,” in 2023 IEEE ISPASS , 2023.\n[11] D. Trautwein, A. Raman, G. Tyson, I. Castro, W. Scott, M. Schubotz,\nB. Gipp, and Y . Psaras, “Design and evaluation of ipfs: a storage layer\nfor the decentralized web,” in ACM SIGCOMM 2022 , 2022, pp. 739–752.\n[12] Apache, “Apache cassandra: Open source nosql database.” [Online].\nAvailable: https://cassandra.apache.org/\n[13] A. Lakshman and P. Malik, “Cassandra: a decentralized structured storage\nsystem,” ACM SIGOPS , vol. 44, no. 2, pp. 35–40, 2010.\n[14] “The Tor Project | Privacy & Freedom Online — torproject.org,” https:\n//www.torproject.org/, [Accessed 13-05-2025].\n[15] Namecoin, “Namecoin.” [Online]. Available: https://www.namecoin.org/\n[16] L. BitTorrent, “Bittorrent: The world’s most popular torrent client.”\n[Online]. Available: https://www.bittorrent.com/\n[17] V . Srivatsa, Z. He, R. Abhyankar, D. Li, and Y . Zhang, “Preble: Ef-\nficient distributed prompt scheduling for llm serving,” arXiv preprint\narXiv:2407.00023 , 2024.\n[18] M. Chiang and T. Zhang, “Fog and IoT: An Overview of Research\nOpportunities,” IEEE Internet of Things Journal , 2016.\n[19] X. Li, M. Wang, S. Shi, and C. Qian, “VERID: Towards Verifiable IoT\nData Management,” in Proceedings of ACM/IEEE IoTDI , 2019.\n[20] D. Marijan and C. Lal, “Blockchain verification and validation: Tech-\nniques, challenges, and research directions,” Computer Science Review ,\nvol. 45, p. 100492, 2022.\n[21] Z. Nie, J. Li, F. Duan, and Y . Lu, “A collaborative ledger storing\nmodel for lightweight blockchains based on chord ring,” The Journal of\nSupercomputing , vol. 80, no. 4, pp. 5593–5615, 2024.\n[22] D. S. Li, J. Cao, X. C. Lu, and K. C. C. Chan, “Efficient range query\nprocessing in peer-to-peer systems,” IEEE Transactions on Knowledge\nand Data Engineering , vol. 21, no. 1, pp. 78–91, 2009.\n[23] D. Li, X. Lu, and J. Wu, “Fissione: a scalable constant degree and low\ncongestion dht scheme based on kautz graphs,” in IEEE INFOCOM 2005 ,\n2005.\n[24] S. Ahmed, A. Shome, and M. Biswas, “Dbst: A scalable peer-to-peer\ndistributed information system supporting multi-attribute range query,” in\n2021 ICSCT , 2021, pp. 1–6.[25] A. Sen, A. S. M. S. Islam, and M. Y . S. Uddin, “Marques: Distributed\nmulti-attribute range query solution using space filling curve on dths,” in\n2015 NSysS , 2015, pp. 1–9.\n[26] B. Djellabi, M. Younis, and M. Amad, “Effective peer-to-peer design\nfor supporting range query in internet of things applications,” Computer\nCommunications , vol. 150, pp. 506–518, 2020.\n[27] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case for\nlearned index structures,” in ACM SIGMOD 2018 , 2018, pp. 489–504.\n[28] R. Marcus, E. Zhang, and T. Kraska, “Cdfshop: Exploring and optimizing\nlearned index structures,” in ACM SIGMOD 2020 , ser. SIGMOD ’20.\nNew York, NY , USA: Association for Computing Machinery, 2020.\n[29] T. Gil, F. Kaashoek, J. Li, R. Morris, and J. Stribling, “p2psim, a simulator\nfor peer-to-peer protocols,” 2003.\n[30] J. Risson and T. Moors, “Survey of research towards robust peer-to-peer\nnetworks: Search methods,” Computer Networks , vol. 50, no. 17, 2006.\n[31] A. Kipf, R. Marcus, A. van Renen, M. Stoian, A. Kemper, T. Kraska, and\nT. Neumann, “Sosd: A benchmark for learned indexes,” arXiv preprint\narXiv:1911.13014 , 2019.\n[32] R. Zhu, B. Liu, D. Niu, Z. Li, and H. V . Zhao, “Network latency esti-\nmation for personal devices: A matrix completion approach,” IEEE/ACM\nTransactions on Networking , vol. 25, no. 2, pp. 724–737, 2017.\n[33] A. Kipf, R. Marcus, A. van Renen, M. Stoian, A. Kemper, T. Kraska, and\nT. Neumann, “Radixspline: a single-pass learned index,” in Proceedings\nof the Third International Workshop on Exploiting Artificial Intelligence\nTechniques for Data Management , ser. aiDM ’20. New York, NY , USA:\nAssociation for Computing Machinery, 2020.\n[34] P. Ferragina and G. Vinciguerra, “The pgm-index: a fully-dynamic com-\npressed learned index with provable worst-case bounds,” VLDB 2020 ,\n2020.\n[35] W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. H. Yu, J. Gonzalez,\nH. Zhang, and I. Stoica, “Efficient memory management for large language\nmodel serving with pagedattention,” in Proceedings of the 29th Symposium\non Operating Systems Principles , 2023, pp. 611–626.\n[36] J. Li, M. Wang, Z. Zheng, and M. Zhang, “Loogle: Can long-\ncontext language models understand long contexts?” arXiv preprint\narXiv:2311.04939 , 2023.\n[37] F. Fang, Y . Hua, S. Wang, R. Zhou, Y . Liu, C. Qian, and X. Zhang, “Gen-\ntorrent: Scaling large language model serving with an overley network,”\narXiv preprint arXiv:2504.20101 , 2025.\n[38] P. Maymounkov and D. Mazieres, “Kademlia: A peer-to-peer information\nsystem based on the xor metric,” in International Workshop on Peer-to-\nPeer Systems . Springer, 2002, pp. 53–65.\n[39] Ipfs, “Ipfs/kubo: An ipfs implementation in go.” [Online]. Available:\nhttps://github.com/ipfs/kubo\n[40] S. Ramabhadran, S. Ratnasamy, J. M. Hellerstein, and S. Shenker, “Prefix\nhash tree: An indexing data structure over distributed hash tables,” in\nProceedings of the 23rd ACM symposium on principles of distributed\ncomputing , vol. 37. St. John’s Newfoundland, Canada, 2004.\n[41] J. Gao and P. Steenkiste, “An adaptive protocol for efficient support of\nrange queries in dht-based systems,” in Proceedings of the 12th IEEE\nInternational Conference on Network Protocols, 2004. ICNP 2004. IEEE,\n2004, pp. 239–250.\n[42] J. Ding, U. F. Minhas, J. Yu, C. Wang, J. Do, Y . Li, H. Zhang,\nB. Chandramouli, J. Gehrke, D. Kossmann et al. , “Alex: an updatable\nadaptive learned index,” in ACM SIGMOD 2020 , 2020, pp. 969–984.\n[43] B. Lu, J. Ding, E. Lo, U. F. Minhas, and T. Wang, “Apex: a\nhigh-performance learned index on persistent memory,” arXiv preprint\narXiv:2105.00683 , 2021.\n[44] J. Wu, Y . Zhang, S. Chen, J. Wang, Y . Chen, and C. Xing, “Updatable\nlearned index with precise positions,” arXiv preprint arXiv:2104.05520 ,\n2021.\n[45] C. Tang, Y . Wang, Z. Dong, G. Hu, Z. Wang, M. Wang, and H. Chen,\n“Xindex: a scalable learned index for multicore data storage,” in 25th\nACM SIGPLAN , 2020, pp. 308–320.\n[46] P. Li, Y . Hua, J. Jia, and P. Zuo, “Finedex: a fine-grained learned index\nscheme for scalable and concurrent memory systems,” VLDB 2021 , 2021.\n[47] J. Wang, T. Zhang, N. Sebe, H. T. Shen et al. , “A survey on learning to\nhash,” IEEE transactions on pattern analysis and machine intelligence ,\nvol. 40, no. 4, pp. 769–790, 2017.\n[48] M. Tur ˇcaník and M. Javurek, “Hash function generation by neural\nnetwork,” in 2016 NTSP , 2016, pp. 1–5.\n11\n\n[49] J. Wang, J. Wang, N. Yu, and S. Li, “Order preserving hashing for\napproximate nearest neighbor search,” in 21st ACM Multimedia , 2013,\npp. 133–142.\n[50] I. Sabek, K. Vaidya, D. Horn, A. Kipf, M. Mitzenmacher, and T. Kraska,\n“Can learned models replace hash functions?” Proc. VLDB Endow. ,\nvol. 16, no. 3, p. 532–545, nov 2022.\n[51] S. Richter, V . Alvarez, and J. Dittrich, “A seven-dimensional analysis of\nhashing methods and its implications on query processing,” PVLDB , vol. 9,\nno. 3, pp. 96–107, 2015.\n[52] L. Shi, H. Zhang, Y . Yao, Z. Li, and H. Zhao, “Keep the cost down:\nA review on methods to optimize llm’s kv-cache consumption,” arXiv\npreprint arXiv:2407.18003 , 2024.\n[53] J. Yao, H. Li, Y . Liu, S. Ray, Y . Cheng, Q. Zhang, K. Du, S. Lu, and\nJ. Jiang, “Cacheblend: Fast large language model serving for rag with\ncached knowledge fusion,” in Proceedings of the Twentieth European\nConference on Computer Systems , 2025, pp. 94–109.\n[54] J. Wang, X. Yi, R. Guo, H. Jin, P. Xu, S. Li, X. Wang, X. Guo, C. Li,\nX. Xu et al. , “Milvus: A purpose-built vector data management system,”\ninProceedings of the 2021 International Conference on Management of\nData , 2021, pp. 2614–2627.\n[55] Pinecone, “Pinecone.” [Online]. Available: https://www.pinecone.io/\n[56] [Online]. Available: https://ai.meta.com/tools/faiss/\n[57] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,\nH. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel et al. , “Retrieval-\naugmented generation for knowledge-intensive nlp tasks,” Advances in\nneural information processing systems , vol. 33, pp. 9459–9474, 2020.\n[58] J. J. Pan, J. Wang, and G. Li, “Vector database management techniques\nand systems,” in Companion of the 2024 International Conference on\nManagement of Data , 2024, pp. 597–604.\n[59] B. Lin, C. Zhang, T. Peng, H. Zhao, W. Xiao, M. Sun, A. Liu,\nZ. Zhang, L. Li, X. Qiu et al. , “Infinite-llm: Efficient llm service for\nlong context with distattention and distributed kvcache,” arXiv preprint\narXiv:2401.02669 , 2024.\n[60] M. Cho, M. Rastegari, and D. Naik, “Kv-runahead: Scalable causal llm\ninference by parallel key-value cache generation,” in Forty-first Interna-\ntional Conference on Machine Learning , 2024.\n[61] A. C. Yüzügüler, J. Zhuang, and L. Cavigelli, “Preserve: Prefetching\nmodel weights and kv-cache in distributed llm serving,” arXiv preprint\narXiv:2501.08192 , 2025.\n[62] R. Singh and S. S. Gill, “Edge ai: a survey,” Internet of Things and Cyber-\nPhysical Systems , vol. 3, pp. 71–92, 2023.\n[63] Y . Shi, K. Yang, T. Jiang, J. Zhang, and K. B. Letaief, “Communication-\nefficient edge ai: Algorithms and systems,” IEEE Communications Surveys\n& Tutorials , vol. 22, no. 4, pp. 2167–2191, 2020.\n[64] B. M. Maggs and R. K. Sitaraman, “Algorithmic Nuggets in Content\nDelivery,” ACM SIGCOMM Computer Communication Review , 2015.\n[65] Z. Zheng, S. Xie, H.-N. Dai, X. Chen, and H. Wang, “Blockchain\nchallenges and opportunities: A survey,” International journal of web and\ngrid services , vol. 14, no. 4, pp. 352–375, 2018.\n[66] A. A. Monrat, O. Schelén, and K. Andersson, “A survey of blockchain\nfrom the perspectives of applications, challenges, and opportunities,” Ieee\nAccess , vol. 7, pp. 117 134–117 151, 2019.\n12\n\nAPPENDIX\nA. Benchmark Details\nIn our microbenchmark, the Centralized Table (C-Table)\nbaseline represents an approach where a single node maintains\na global index mapping key ranges to node locations. This\nreduces lookup hops but at the cost of storing a huge mapping\ntable on one node – leading to the high memory usage seen\nin Fig.1(a) – and introducing a single point of failure. The\nRange-Partition BST (RP-BST) baseline refers to a distributed\noverlay where nodes are organized in a binary search tree by\nkey ranges. Each node must maintain pointers to tree neighbors\n(left/right child pointers) and possibly additional routing state,\nincreasing memory overhead and control complexity. IRP-BST\nreduced messages compared to a plain DHT, but required\nsignificantly more memory per node (storing large tree routing\ntables or interval state) and struggled under churn (due to costly\nrebalancing of the tree). Fig.1 shows that while these approaches\ncan improve query hops, they exact a high memory/control cost\nor require central coordination, which motivates the need for a\nmore efficient and fully decentralized solution like LEAD.\nA naive strategy to enhance range queries is parallelizing\nlookups on traditional DHTs by simultaneously issuing multiple\nkey queries. The \"Chord with batch size 100/1000\" baselines\nrepresent scenarios where 100 or 1000 parallel lookups are\nissued concurrently. However, this parallelization introduces\nsubstantial practical overhead: (i) The initiating node must man-\nage numerous simultaneous responses, potentially saturating its\nnetwork interface or CPU. (ii) The overall query latency is still\nconstrained by the slowest individual lookup, limiting scalability\nand efficiency in realistic conditions.\nB. Shadow Balancer Analysis\nConsider a LEAD deployment with nphysical nodes, each\nhosting kvirtual nodes. The Shadow Balancer introduces addi-\ntional message complexity proportional only to k, resulting in\nan overall complexity still logarithmic with respect to n. Exper-\nimentally, we found that using 10 virtual nodes per physical\nnode incurred minimal overhead—less than 4% CPU usage,\nunder 0.1% additional memory per node for routing tables,\nand negligible network impact—while significantly enhancing\nload balancing. This approach aligns with established industry\npractices; for instance, Apache Cassandra recommends a similar\nvirtual-node strategy to evenly distribute load with minimal\noverhead.\nConsider a LEAD network composed of nnodes, each\nof which is virtualized into mvirtual nodes. The system is\ndesigned to accommodate a hashing space capable of handling\nhhash values. Each virtual node oversees a virtual finger table\ncontaining b=⌊logh⌋entries. When a peer P0with a virtual\nnode ID HP0initiates a range query for nsuccessive key-value\npairs starting from key Kwith a hash value HK, the query\nwill reach Swithin log(mn)hops with high probability. With\nVkey-value pairs managed in the network, each peer manages\nan average ofV\nmnkey-value pairs. Upon reaching S, the query\nretrieves an average ofV\n2mnkeys. If the query is not completed,it continues on S’s successors, each of which retrievesV\nmnkeys.\nConsequently, after reaching S, the query needs an additional\n⌈mn2\nV−1\n2⌉hops with high probability. Given that n2≪V\nandm≪V, it follows thatmn2\nV<1with high probability.\nTherefore, the range query requires less than one additional hop\nafter reaching S, and it can be resolved within log(mn)hops\nwith high probability.\nFurthermore, our design naturally accommodates heteroge-\nneous environments by assigning more virtual nodes (thus\ngreater hash space) to more capable nodes, while assigning\nfewer to resource-constrained ones. We acknowledge that “hot\nkeys,” receiving disproportionately high request volumes, con-\nstitute a distinct load-balancing concern at the request level.\nCurrently, LEAD does not explicitly handle hot-key replication\nor caching, but such strategies can be integrated orthogonally,\nfor example, via application-level replication. We plan to extend\nLEAD with standard replication strategies for hot keys in future\nreleases.\nC. Handling New Keys and Failure Recovery\nLEAD is designed so that correctness is never compromised\nby model error. Even if new keys arrive that the model hasn’t\nseen, those keys are still inserted and found correctly. This\nis because the system always uses the current learned model\noutput in combination with the DHT’s finger tables to place and\nlocate keys. In the worst case, an outdated or inaccurate model\ncould result in uneven load distribution, but the correctness\nof lookups remains guaranteed by fundamental DHT routing\nprinciples. Our evaluation showed that the learned model re-\nmained efficient even as up to 40% new keys were added;\nbeyond that, we triggered a model update to realign with the\nnew CDF. By following successors until N keys are gathered,\nthe query inherently stops only after covering the continuous\nrange requested. As long as the keys are stored in sorted order\nacross the ring, which our learned hash function aims to ensure,\norder consistency is maintained. If the key distribution is highly\nirregular or adversarial, a learned model might not yield much\nbenefit. In such a scenario, the performance of LEAD would\ngradually revert toward the baseline DHT: even if the model’s\npredictions are poor, the system will still find keys correctly\nusing DHT routing. Our evaluations included diverse real-world\ndatasets — uniform-like distributions, highly skewed popularity\ndata, temporal data, etc. — and LEAD handled all with strong\nperformance gains.\nLEAD is designed to adapt rapidly to system churn through\nrobust stabilization and failure recovery mechanisms. The cor-\nrectness of LEAD is dependent on the current knowledge of\nits successors and predecessors within the network. Addition-\nally, the efficiency of query handling is contingent upon the\ntimeliness and accuracy of the virtual finger tables. To maintain\nthis information, each peer periodically stabilizes itself in the\nnetwork through the following mechanisms:\n(1)Successor and Predecessor Verification: Peers regularly\ninitiate verification requests to update their immediate\nsuccessors and predecessors.\n13\n\n(2)Heartbeat Communications: Regular heartbeat messages\nare exchanged between a peer and its network neighbors\non their successor and predecessor lists to affirm their\npresence and operational status.\n(3)Virtual Finger Table Maintenance: Each peer undertakes\nsystematic verification and updates of its virtual finger table\nto align with the current network state.\nEach peer continuously monitors the operational status of\nits immediate neighbors through regular heartbeat messages.\nA missed series of heartbeats triggers an immediate suspicion\nof peer failure, prompting further verification actions. After\na given timeout, the peer flags it as a failure and initiates\nthe recovery process. In addition to maintaining a primary\nsuccessor and predecessor, LEAD implements a successor list\nand a predecessor list containing several backup peers. Upon\ndetection of a failure in the immediate successor, the next\navailable peer from this list is promoted to take over as the\nimmediate successor or predecessor. Concurrently, the peer\nwaits for the next stabilization phase to verify the updated\ncorrectness of the immediate successor and predecessor. At\nthe same time, its virtual finger table is updated seamlessly\nwith the successor recovery. Meanwhile, the node managing the\nvirtual peers regularly checks their liveliness as a higher-level\nobserver. Should a virtual peer fail to reestablish its status within\nthe network following specified timeouts, the node initiates a\ncontrolled removal of the peer from the network and facilitates\nthe rejoining of the peer via its other operational virtual peers.\nFurthermore, while the core focus of this paper is to delineate\nthe fundamental operations of LEAD, the implementation of\ndata redundancy and replication can be enabled from a higher\nlevel, providing an additional layer of data protection.\nCorrectness guarantee under asynchronous updates.\nLEAD preserves query-correctness despite (i) concurrent model\nupdates, (ii) finger-table maintenance, and (iii) churn, because\nthese three mechanisms are decoupled and monotone:\n(1)Decoupled address spaces: Ownership is defined solely\nby the PeerHASH , never by the evolving learned model.\nEven if two peers temporarily disagree on the newest FRM\nversion, they still agree on who ultimately owns any hash\nidentifier.\n(2)Version-monotone models: Each peer tags its leaf-model\nblob with a monotonically increasing version vector; rout-\ning messages carry the sender’s current version. A peer\nnever rolls back to an older version, so once all predeces-\nsors of an identifier hadopt version v, no later update can\nmapKoutside that predecessor interval.\n(3)Safe forwarding rule: When a query for Kreaches a\npeerPwhose local RMI predicts a different successor\nthan its finger table, P forwards the query along the\nfinger-table edge—which, by Chord’s proven invariants, al-\nways advances the query at least halfway to the true owner.\nThus every hop strictly decreases the identifier distance\nin the canonical ring metric, guaranteeing convergence in\n≤ ⌈log2N⌉hops even while RMI versions are in flux.\n(4)Eventual convergence: The heartbeat messages ensurethat any live peer receives strictly newer RMI versions\nfrom at least one neighbor within ∆≤O(logN)message\ndelays, hence the overlay becomes lookup-correct again\nafterO(logN)rounds following the last FRM update.\nFrom the perspective of an active peer, there may be occa-\nsions where it attempts to route a query through a peer that has\nfailed but is still considered alive prior to the completion of the\nstabilization process. In such cases, the query will either proceed\nafter a timeout – prompting the peer to retry the query – or it\nmay continue through a suboptimal path during this particular\nhop. Despite these interruptions, the system is designed to sta-\nbilize rapidly. This ensures that even a non-optimal routing hop\ndoes not compromise the logarithmic routing step guarantee for\ndata retrieval, provided that the overall integrity and correctness\nof the system are maintained.\nD. Fully Decentralized Model Update\nLEAD’s federated update mechanism is designed to be\nlightweight and distributed. The term “coordinator” used in the\npaper does not imply a persistent leader or central node; rather,\nrather, it describes a transient role adopted temporarily by a peer\nto merge local model updates within its immediate neighbor-\nhood. This design ensures that coordinators communicate only\nwith a small subset of nodes, thereby limiting communication\noverhead and preventing any single node from becoming a\nglobal bottleneck. Additionally, as detailed in Section III.E,\nmultiple coordinators can concurrently aggregate updates in\ndifferent network regions. This parallel and localized process\nensures that the model update mechanism scales gracefully. The\nFRM within the LEAD protocol provides a robust mechanism\nfor decentralized and cooperative model updates among peers,\naimed at optimizing load balancing when a significant influx of\nnew key-value pairs is introduced to the network.\nE. Security Considerations\nWhile a full adversarial analysis was outside our original\nscope, we acknowledge that the learned model could be a\npotential attack surface if an adversary controls some peers.\nImportantly, since peer addressing and routing rely on cryp-\ntographic hashing independent of the learned model, attackers\ncannot misroute queries, although they could attempt to degrade\nload balancing. To mitigate risks, we already utilize gradient\naggregation during model updates to prevent any single peer\nfrom dominating the training phase and manipulating data\navailability. In future iterations, we plan to collaborate with the\nbroader community to further harden LEAD by incorporating\nestablished security mechanisms such as Sybil-resistant identi-\nties and robust aggregation techniques to counteract malicious\nnodes.\nF . Additional Evaluation Results\n1) Case study III: Blockchain application: We perform a\ncase study to show how LEAD can improve data retrieval\nperformance in practice for blockchain applications. Specifi-\ncally, we apply LEAD on a distributed collaborative ledger sys-\ntem [21]. Blockchain applications, particularly those involving\n14\n\n/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000039/uni0000004c/uni00000055/uni00000057/uni00000058/uni00000044/uni0000004f/uni00000003/uni00000031/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni00000033/uni00000048/uni00000055/uni00000003/uni00000031/uni00000052/uni00000047/uni00000048/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000013/uni00000013/uni00000016/uni00000018/uni00000013/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000013/uni00000013/uni00000036/uni00000057/uni00000044/uni00000051/uni00000047/uni00000044/uni00000055/uni00000047/uni00000003/uni00000027/uni00000048/uni00000059/uni0000004c/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002e/uni00000048/uni0000005c/uni00000003/uni00000036/uni00000057/uni00000052/uni00000055/uni00000044/uni0000004a/uni00000048\nFig. 12. Node Storage Standard Deviation (SD.)\n/uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013\n/uni00000034/uni00000058/uni00000048/uni00000055/uni0000005c/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni0000001b/uni00000013/uni00000013/uni0000001b/uni00000014/uni00000013/uni0000001b/uni00000015/uni00000013/uni0000001b/uni00000016/uni00000013/uni0000001b/uni00000017/uni00000013/uni0000001b/uni00000018/uni00000013/uni0000001b/uni00000019/uni00000013/uni0000001b/uni0000001a/uni00000013/uni0000001b/uni0000001b/uni00000013/uni0000001b/uni0000001c/uni00000013/uni0000001c/uni00000013/uni00000013/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000003/uni00000034/uni00000058/uni00000048/uni00000055/uni0000005c/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c\n/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000003/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f\n/uni00000035/uni00000044/uni00000047/uni0000004c/uni0000005b/uni00000003/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f\n/uni00000026/uni00000058/uni00000045/uni0000004c/uni00000046/uni00000003/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f Fig. 13. Latency vs. RMI Models\n/uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013\n/uni00000034/uni00000058/uni00000048/uni00000055/uni0000005c/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni0000001b/uni00000013/uni00000013/uni0000001b/uni00000014/uni00000013/uni0000001b/uni00000015/uni00000013/uni0000001b/uni00000016/uni00000013/uni0000001b/uni00000017/uni00000013/uni0000001b/uni00000018/uni00000013/uni0000001b/uni00000019/uni00000013/uni0000001b/uni0000001a/uni00000013/uni0000001b/uni0000001b/uni00000013/uni0000001b/uni0000001c/uni00000013/uni0000001c/uni00000013/uni00000013/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000003/uni00000034/uni00000058/uni00000048/uni00000055/uni0000005c/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c\n/uni00000035/uni00000030/uni0000002c\n/uni00000033/uni0000002a/uni00000030\n/uni00000035/uni00000044/uni00000047/uni0000004c/uni0000005b/uni00000036/uni00000053/uni0000004f/uni0000004c/uni00000051/uni00000048 Fig. 14. Latency vs Learned Indexes\n/uni00000013/uni00000008 /uni00000014/uni00000013/uni00000008 /uni00000015/uni00000013/uni00000008 /uni00000016/uni00000013/uni00000008 /uni00000017/uni00000013/uni00000008 /uni00000018/uni00000013/uni00000008\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000052/uni00000049/uni00000003/uni00000031/uni00000048/uni0000005a/uni00000003/uni0000002e/uni00000048/uni0000005c/uni00000010/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000003/uni00000033/uni00000044/uni0000004c/uni00000055/uni00000056/uni00000014/uni00000015/uni00000018/uni00000013/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni0000001a/uni00000018/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000013/uni00000013/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002e/uni00000048/uni0000005c/uni00000003/uni00000036/uni00000057/uni00000052/uni00000055/uni00000044/uni0000004a/uni00000048\n/uni0000002f/uni00000028/uni00000024/uni00000027/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000052/uni00000058/uni00000057/uni00000003/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000038/uni00000053/uni00000047/uni00000044/uni00000057/uni00000048\n/uni0000002f/uni00000028/uni00000024/uni00000027/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000038/uni00000053/uni00000047/uni00000044/uni00000057/uni00000048\nFig. 15. Standard deviation of storage with model update\n/uni00000037/uni00000055/uni00000044/uni00000047/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000044/uni0000004f /uni0000005a/uni00000012/uni00000003/uni00000026/uni0000004b/uni00000052/uni00000055/uni00000047 /uni0000005a/uni00000012/uni00000003/uni0000002f/uni00000028/uni00000024/uni00000027\n/uni0000002f/uni00000048/uni00000047/uni0000004a/uni00000048/uni00000055/uni00000003/uni00000036/uni00000057/uni00000052/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000014/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000045/uni0000000c\n(a) Memory Utilization\n/uni00000037/uni00000055/uni00000044/uni00000047/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000044/uni0000004f /uni0000005a/uni00000012/uni00000003/uni00000026/uni0000004b/uni00000052/uni00000055/uni00000047 /uni0000005a/uni00000012/uni00000003/uni0000002f/uni00000028/uni00000024/uni00000027\n/uni0000002f/uni00000048/uni00000047/uni0000004a/uni00000048/uni00000055/uni00000003/uni00000036/uni00000057/uni00000052/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000014/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001c/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000013/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c (b) Ledger Query Latency\nFig. 16. Memory cost and latency for range queries of a blockchain\nnaïve mechanisms that lack support for efficient range queries.\nLEAD can be seamlessly integrated into blockchain systems to\nalleviate bottlenecks associated with range query latency. In this\ncase study, we demonstrate this by substituting the traditional\nDHTs used in the ledger storage system [19] with LEAD. We\nloaded the system with a blockchain ledger containing 100\nmillion organic timestamps as keys linked to their corresponding\ntransaction values. Subsequently, we conducted range queries\nfor 4,000 transactions following a specific timestamp. We\ncompared LEAD with two other baseline methods: the Chord\ncollaborative ledger storage model as proposed by [19] and\na naïve full ledger storage model that maintains a complete\ncopy of the ledger on each peer, as depicted in Fig. 16. The\nnaïve full ledger storage model, while offering minimal in-\nmemory query latency due to its localized data access, incurs\nsubstantial memory overhead by storing the entire ledger. This\napproach, although straightforward, becomes impractical as the\nledger size grows, leading to scalability issues. In contrast,\nLEAD demonstrates a significant improvement in reducing\nledger query latency while concurrently optimizing memory\nusage when compared to the traditional DHT storage model.\nE. Case study II : InterPlanetary File System (IPFS).\nThe InterPlanetary File System (IPFS) [2] is a distributed\ncontent delivery network that stores, retrieves, and locates data\nbased on the Content Identifiers (CIDs) of its actual content\nrather than its name or location. With millions of daily content\nretrievals, IPFS supports numerous third-party applications,\n0 10 20 30 40 50\nNumber of Messages0.000.250.500.751.00CDF\nIPFS\nLEADFig. 17. CDFs of the message cost in IPFS\ndemonstrating its broad utility and impact. However, the tradi-\ntional DHTs used in IPFS, i.e., Kademlia [35], face challenges\nin handling range queries, which are essential for efficiently\nretrieving sequences of data blocks or related files.\nIn this case study, we explore how integrating LEAD into\nIPFS can improve the efficiency of range queries, thereby\nenhancing overall data retrieval performance. We simulate an\nIPFS network with 100 peers using the PlanetLab topology\nand emulated the core networking layer of the IPFS protocol\non top of the Kademlia DHT in p2psim+. We generated 100\nmillion synthetic key-value pairs to represent the logical units\nstructuring the metadata of resources (files). For the emulated\nIPFS, the CIDs of these key-value pairs were named using SHA-\n1. In contrast, for LEAD, the CIDs were managed by the learned\nhash function within SHA-1’s hashing space. The key operation\ntested was a typical user request for a resource. In this scenario,\nan edge server retrieves all the blocks containing the metadata\nfor the requested resource, which consists from 1,000 to 3,000\nblocks in our test case. For the emulated IPFS, the server divided\nthe 1,000 lookup queries into 10 batches and sent them over the\nnetwork. In contrast, for LEAD, the query was optimized as a\nsupported meta-operation range query. We recorded the number\nof hops required to resolve the request for both systems. As\nillustrated in Fig. 17, the CDFs of the retrieval hops for the\nemulated IPFS and LEAD indicate a significant reduction in the\nnumber of messages required to complete data block sequences\nretrieval when using LEAD.\nV. R ELATED WORK\nRange query in DHTs. Current DHT systems have signif-\nicant limitations in handling range queries. These systems are\ninherently designed for exact key-based queries, and therefore,\ntheir hashing mechanisms lose the semantic relationship be-\ntween keys—necessary for range queries. Significant efforts to\nfacilitate efficient range queries in distributed networked sys-\ntems have introduced innovative concepts while also revealing\ninherent limitations. Armada [20] utilizes a partition tree model\nand a tailored algorithm within the FissionE [21] topology to\n10Fig. 17. Range Query Benchmark in Blockchain\ncollaborative ledgers, require efficient and reliable data access\nto ensure consistency and speed in transaction processing. For\nexample, verification tasks [20] often require the retrieval of\nspecific ranges of blocks or transactions to confirm their validity,\nensure they adhere to the chain rules, or perform security\nchecks. Traditional DHT implementations provide a robust\nstructure for single key queries but often struggle with latency\nissues as the query range scales up due to their reliance on naive\nmechanisms that lack support for efficient range queries. LEAD\ncan be seamlessly integrated into blockchain systems to alleviate\nbottlenecks associated with range query latency. In this case\nstudy, we demonstrate this by substituting the traditional DHTs\nused in the ledger storage system [21] with LEAD. We loaded\nthe system with a blockchain ledger containing 100 million\norganic timestamps as keys linked to their corresponding trans-\naction values. Subsequently, we conducted range queries for\n4,000 transactions following a specific timestamp. We compared\nLEAD with two other baseline methods: the Chord collaborative\nledger storage model as proposed by [21] and a naive full ledger\nstorage model that maintains a complete copy of the ledger\non each peer, as depicted in Fig. 17. The naive full ledger\nstorage model, while offering minimal in-memory query latency\ndue to its localized data access, incurs substantial memory\noverhead by storing the entire ledger. This approach, although\nstraightforward, becomes impractical as the ledger size grows,\nleading to scalability issues. In contrast, LEAD demonstrates a\nsignificant improvement in reducing ledger query latency while\nconcurrently optimizing memory usage when compared to the\ntraditional DHT storage model.\nG. Additional Related Work\nLearning Hash Functions. To date, the concept of the\n\"Learned Hash Function\" has not been formally established in\nthe literature. Nonetheless, there have been attempts to develop\ndata-dependent hash functions that learn from the characteristics\nof the data itself. For instance, research such as [47]–[49] has\nexplored learning locality-sensitive hash (LSH) functions to\nconstruct Approximate Nearest Neighborhood (ANN) indexes.\nThese studies are primarily focused on clustering similar items\ninto buckets to facilitate nearest neighborhood queries, yet their\nmethodologies are not adaptable to a broader range of applica-\ntions [27], [47], [51]. Additionally, a recent study by Sabek et al.\n[50] discusses the potential to replace traditional hash functions\nwith learned models. It indicates that learned models producea similar or reduced number of hash collisions compared to\ntraditional hash functions. There has been no exploration into\nintegrating learned index structures within distributed networked\nsystems yet.\nKV Cache Management for LLM Severing. Inference with\nlarge language models over extended contexts incurs substantial\nmemory and latency overhead because self-attention scales\nquadratically with sequence length. To mitigate this, key–value\n(KV) caching retains the attention keys and values of earlier\ntokens for reuse rather than recomputation, trading additional\nmemory for reduced compute—an especially worthwhile com-\npromise when long shared prefixes would otherwise demand\nexorbitant recomputation [35], [52], [53]. However, because the\nKV cache grows linearly with sequence length, it can quickly\nexhaust a single node’s GPU memory , and coordinating caches\nacross multiple nodes introduces considerable data-management\noverhead. In LLM serving, key–value (KV) caches retain the\nattention keys and values of earlier tokens for reuse rather than\nrecomputation. Hence how to share and re-use existing KV\ncaches is a crucial problem [2]–[4], [17], [35]. Our case study\nI is analogous to a content delivery network (CDN), where\neach node can serve cache lookups for others. We assume a\nrealistic scenario of long-context reuse: queries arrive in some\ntemporal order (here following a Zipf-0.6 popularity distribution\nin our evaluation), and if the shared prefix was recently used\non one node, another query for the same document might land\non a different node. The goal is to leverage the existing KV\ncache instead of recomputing from scratch, by retrieving cached\nKV blocks from the network. LEAD can organize the KV\ncache across nodes to optimize such reuse. Rather than hashing\nthese KV blocks arbitrarily across the system, LEAD maintains\ncontiguous ranges of keys.\nH. Expanding LEAD into Emerging Domains\nLEAD opens several avenues for further research. The\narchitectural principles behind LEAD—order-preserving\nlearned hashing, cooperative model updates, and lightweight\nvirtual-node balancing—extend well beyond classical key-value\nstores. We outline several promising, high-impact directions\nthat we plan to prototype:\n1) Vector Databases and Retrieval-Augmented Generation:\nVector search engines such as Milvus [54], Pinecone [55],\nand Meta’s FAISS [56] shard billions of embeddings but rely\non static partitioners or HNSW graphs that must be rebuilt\nas data drift. By hashing product-quantization or IVF bucket\nIDs through LEAD, semantically adjacent vectors would land\non neighboring peers, enabling locality-aware ANN probes\nwhile retaining the high-efficient routing guarantee. Because the\nCDF is retrained cooperatively, the overlay can track concept\ndrift without global re-indexing—an attractive property for\nRetrieval-Augmented-Generation (RAG) [57] stacks that in-\ngest fresh documents continuously. Early work on peer-to-peer\nvector search underscores the demand for such decentralized\nindexes [58].\n2) Large-Scale LLM Serving and Decentralized Inference:\nLooking forward, we see LEAD as a compelling sub-\n15\n\nstrate for next-generation AI serving stacks, especially for\nsharding the ever-growing KV caches and model adapters\nthat dominate large-language-model (LLM) inference. By\nhashing tuples 〈sequence-ID,layer,token-pos 〉through LEAD’s\norder-preserving learned function, temporally adjacent tokens\nland on neighbouring peers, enabling sliding-window eviction\nand locality-aware reuse without a central KV router—an attrac-\ntive alternative to recent distributed-attention proposals such as\nDistAttention [59], KV-Runahead [60], PRESERVE [61].\n3) Edge-AI and IoT Data Lakes: Massive edge deploy-\nments generate time-series telemetry that must be queried by\ngeographic window or recency [62], [63]. LEAD can hash\ncomposite keys 〈device-ID, timestamp 〉so that adjacent time\nranges map to adjacent roadside or gateway peers. The learned\nCDF absorbs workload skew caused by bursty sensors, while\nFRM keeps models current without cloud coordination—ideal\nfor bandwidth-constrained, privacy-sensitive edge fabric.\n4) Content-Delivery Networks and caches: Modern CDNs\nalready rely on consistent hashing to place objects across Point\nof Presences (PoPs) [9], but these static hashes ignore temporal\npopularity skew, leading to cache imbalance and cold-start\nmisses. Integrating LEAD at the cache-mapping layer would\nallow the overlay to learn the content popularity CDF and adap-\ntively remap hot prefixes to additional PoPs, reducing origin\nfetches. This direction aligns with the long-standing goal of\n“demand-responsive” CDNs envisioned by CDN providers [64].\n5) Cross-chain and indexing: Inter-ledger stacks need\nfast, ordered look-ups across heterogeneous blockchains [65],\n[66]. A LEAD overlay per chain—bridged via light-client\nproofs—would supply uniform range queries over block-height\nor asset-ID without a trusted indexer, complementing recent\ncross-chain initiatives.\n6) Harden LEAD against adversarial behaviour: A promis-\ning complementary effort is to harden LEAD against adversarial\nbehaviour. Because the learned hash derives from live data,\nattackers might (i) inject skewed keys to distort the CDF, (ii)\nreturn bogus model parameters to misroute queries, or (iii)\njoin the overlay with large Sybil swarms. We plan to explore\nlightweight counter-measures—replicated model cross-checks,\ncapped learning rates, signed model digests, and standard\nSybil-throttling rules—that preserve LEAD’s efficiency while\nadding strong resilience.\nImplementation details, as well as simulation code, are pub-\nlicly available at https://github.com/ShengzeWang/LEAD. We\nintend to release LEAD as a robust, accessible library, facilitat-\ning continued research within the community and encouraging\nbroader industry adoption.\n16",
  "textLength": 114506
}