{
  "paperId": "60441de68623abfd43d8d85525f34a26308652bf",
  "title": "NeuroDB: A Neural Network Framework for Answering Range Aggregate Queries and Beyond",
  "pdfPath": "60441de68623abfd43d8d85525f34a26308652bf.pdf",
  "text": "NeuroDB: A Neural Network Framework for\nAnswering Range Aggregate Queries and Beyond\nSepanta Zeighami\nUSC\nzeighami@usc.eduCyrus Shahabi\nUSC\nshahabi@usc.edu\nAbstract —Range aggregate queries (RAQs) are an integral\npart of many real-world applications, where, often, fast and\napproximate answers for the queries are desired. Recent work\nhas studied answering RAQs using machine learning models,\nwhere a model of the data is learned to answer the queries.\nHowever, such modelling choices fail to utilize any query speciﬁc\ninformation. To capture such information, we observe that RAQs\ncan be represented by query functions , which are functions that\ntake a query instance (i.e., a speciﬁc RAQ) as an input and output\nits corresponding answer. Using this representation, we formulate\nthe problem of learning to approximate the query function, and\npropose NeuroDB, a query specialized neural network framework,\nthat answers RAQs efﬁciently. We experimentally show that\nNeuroDB answers RAQs orders of magnitude faster than the\nstate-of-the-art on real-world, benchmark and synthetic datasets.\nFurthermore, NeuroDB is query-type agnostic (i.e., it does not\nmake any assumption about the underlying query type) and\nour observation that queries can be represented by functions\nis not speciﬁc to RAQs. Thus, we investigate whether NeuroDB\ncan be used for other query types, by applying it to distance\nto nearest neighbour queries. We experimentally show that\nNeuroDB outperforms the state-of-the-art for this query type,\noften by orders of magnitude. Moreover, the same neural network\narchitecture as for RAQs is used, bringing to light the possibility\nof using a generic framework to answer any query type efﬁciently.\nI. I NTRODUCTION\nRange aggregate queries (RAQs) are an integral part of\nmany real world applications. Calculating the total proﬁt over\na period from sales records or the average pollution level for\ndifferent regions for city planing [1] are examples of their use\ncases. Often, due to large volume of data, an exact answer\ntakes too long to compute and a fast approximate answer is\npreferred. In such scenarios, there exists a time/space/accuracy\ntrade-off, where algorithms can sacriﬁce accuracy for time or\nspace. For example, consider a geospatial database containing\nlatitude and longitude of location signals of individuals and,\nfor each location signal, the duration the individual stayed in\nthat location. A potential RAQ on this database is to calculate\nthe average time spent by users in an area, which can be useful\nfor analyzing different Points of Interest (POIs). Approximate\nanswers within a few minutes of the exact answer can be\nacceptable in such applications. We use such a scenario as\nour running example, with the database shown in Fig. 1 (left).\nResearch on RAQs has focused on improving the time/s-\npace/accuracy trade-offs. Existing methods can be divided into\nsampling-based [2]–[5] and model-based methods [1], [6]–\n[9]. Sampling-based methods sample a subset of the database\nFig. 1. Running example: (left) Database of location signals, and (right) the\nquery function of average visit duration. Color shows visit duration in hours.\nand answer the queries based on the samples. Model-based\nmethods develop a model of the data to answer the queries.\nModels can be of the form of histograms, wavelets and data\nsketches (see [6] for a survey). More recently, learning-based\nregression and density data models [1], [8], [9] were proposed\nwhich have shown improvements over existing techniques.\nA major drawback of the recent learning-based approaches\nis that they learn a model from the data that is oblivious\nto the queries being asked. Learning from the data misses\nthe opportunity to learn information about the queries that\ncan help answer them faster. This information can be of two\nforms. (1) It can be about the query workload. In real-world\ndatabases, certain queries are far more common than others.\nIn fact, OLAP systems (e.g., [10], [11]) divide attributes into\nmeasure anddimension categories where common RAQs have\nan aggregation function on a measure attribute and range\npredicates on dimension attributes. In our running example,\nvisit duration can be deﬁned as a measure attribute and lat.\nand lon. as dimension attributes. Other queries, such as the\nquery of average latitude given a visit duration range may\nnot make any semantic sense. Even given the measure and\ndimension attributes, some ranges are queried more often than\nothers. In our running example, more RAQs may be asked in\ndowntown vs. a residential area. Diverting model capacity to\nlearn about the queries that will actually be asked can improve\nthe performance of a system. (2) The information can also be\nabout the patterns in the answer to the query. The average visit\nduration for different places in the downtown of a city may\nbe similar, while the answer will be different for suburban\nareas. A model can ﬁnd such patterns, so that a compact\nrepresentation of the data relevant to the queries is learned.\nA. NeuroDB for Range Aggregate Queries\nWe propose a fundamentally different approach towards\nanswering RAQs. First, we introduce the notion of a queryarXiv:2107.04922v1  [cs.DB]  10 Jul 2021\n\nfunction . Intuitively, a range aggregate query function takes as\nan input a range predicate and outputs a real number, which is\nan aggregation of a measure attribute. In our running example,\nconsider the query of average visit duration for a 50m \u000250m\nrectangle with bottom left corner at the geo-coordinate (x;y).\nFor such an RAQ, a query function, fD(x;y), can be deﬁned\nthat takes as input the geo-coordinate of the rectangle and\noutputs the average visit duration of data points in the rectan-\ngle. This query function is plotted in Fig. 1 (right). We call\na particular input to the query function a query instance . For\nexample, Fig. 1 (right) shows that for query instance (-95.3615,\n29.758) the answer is 9, i.e., fD(\u000095:3615;29:758) = 9 .\nThe observation that RAQs can be represented as query\nfunctions allows us to learn models to answer them efﬁciently.\nTo that end, we propose a novel framework, called Neural\nDatabases (NeuroDB), to efﬁciently answer RAQs. Specif-\nically, letqbe a query instance issued by a user, and let\nfD(x)be the range aggregate query function, deﬁning what\nthe answer is for any query instance x(and thus the ground\ntruth answer for qisfD(q)). During a pre-processing step,\nNeuroDB uses solved query instances from a known algorithm\nto learn a model that approximates the query function well.\nThe model takes a query instance as an input and outputs an\nanswer, and the training objective is to optimize the model\nso that its answer is as close to ground truth as possible. At\ntest time, NeuroDB outputs ^fD(q;\u0012)as its answer for query q,\neliminating both the database and the algorithm it learned\nfrom . In our running example, NeuroDB learns a model that\nlooks similar to Fig. 1 (right), but can be evaluated fast and\nwithout accessing the database at all. We use neural networks\nas our model and propose a neural network architecture that\ncan be easily trained, answer the queries fast and accurately,\nand scale with dimensionality and data size.\nNeuroDB improves the state-of-the-art in two ways. First,\nwe experimentally observed that NeuroDB answers RAQs\nmultiple orders of magnitude faster than state-of-the-art and\nwith better accuracy. Thus, in traditional data management\nsystems, NeuroDB can be an addition to the query processing\nengine and can considerably improve the performance of\nRAQs, while a default query processing engine can answer\nqueries for which NeuroDB is not trained. In this regard,\nNeuroDB can be used similar to how indexes are used\nin database systems. Second, NeuroDB trained for a query\nfunction is typically much smaller than data size, while the\nmodel learned can be used to answer the queries without\naccessing the data. This is beneﬁcial for applications requiring\nefﬁcient release and storage of data. For instance, location\ndata aggregators (e.g., SafeGraph [12]) can train NeuroDB\nto answer the average visit duration query, and release it to\ninterested parties instead of the dataset. This can improve the\nstorage, transmission and query processing costs for all parties.\nB. Beyond Range Aggregate Queries\nOur observation that queries can be represented by query\nfunctions is not speciﬁc to RAQs, and is true for any query\ntype. Considering such representation opens the path for usinglearned models to answer different database query types.\nHence, an important question is to understand the utility of\nlearned models for different query types. We take the ﬁrst\nstep in this direction by applying NeuroDB to another query\ntype. Speciﬁcally, we consider two variants of k-th nearest\nneighbour queries. This is possible because the training of\nNeuroDB is query type agnostic, that is, it does not make\nassumptions about the query type during training.\nFirst, we consider k-th nearest neighbour queries ( k-NN),\nwhich, for a d-dimensional database D, can be represented\nby a function, fNN\nD(:)that takes a d-dimensional point as\nan input, and outputs another d-dimensional point. NeuroDB\ncan be used to train a model, ^fNN\nD(:;\u0012)that mimics the\nfunctionfNN\nD(:). We observed that NeuroDB can provide an\nanswer for the k-NN query that is spatially close to the true\nanswer. However, for a query q, the output of ^fNN\nD(q;\u0012)is not\nnecessarily in the database (but rather some point in Rdwhose\ndistance to fNN\nD(q)is small), while the answer to nearest\nneighbour query needs to be a point in the database. Such an\nanswer may not have much utility in real-world applications.\nNext, we consider the distance to k-th nearest neighbour\nquery, where the query answer is the distance to and not the\nactual point in the database. This query can be represented\nby a function that takes a d-dimensional query point as an\ninput and outputs a real number. Distance to nearest neighbour\nquery is useful for various applications such as active learning\n[13]–[15] (e.g., as a diversity score for selecting samples\nfor training a model), outlier detection [16] and assessing\nprobability of getting infected from a disease (e.g., COVID-19)\ncalculated based on proximity to the nearest infected person.\nWe observed that NeuroDB is able to answer distance to k-\nNN queries orders of magnitude faster than state-of-the-art,\nwhile only taking space that is a small fraction of data size.\nNeuroDB achieves this because it answers distance to k-NN\nqueries without calculating any distances or ﬁnding any of the\nneighbours at query time. Instead, NeuroDB approximately\nmaterializes the query results, with higher probability regions\n(according to query distribution) being materialized more\naccurately. This allows the framework to perform especially\nwell for high-dimensional data where queries are from a small\nportion of the total high dimensional space (e.g. images [17]).\nThe results in this paper suggest dividing database queries\nintoitem queries andstatistic queries categories. Item queries\nare queries where the answer is an actual database item, while\nstatistic queries are queries where the answer is a numerical\nstatistic calculated based on the database. We conjecture that\nNeuroDB, on its own, is useful for statistic queries. Studying\nthis conjecture for different statistic queries is an interesting\nfuture research direction, where a generic framework can\nbe used to answer multiple different query types in this\ncategory. This paper is a ﬁrst step in that direction, where\na framework designed for range aggregate queries is shown to\nalso efﬁciently answer distance to the k-th nearest neighbour\nqueries and outperform the state-of-the-art for both of them.\nAlthough NeuroDB outperforming the state-of the-art in either\nof the two query types is signiﬁcant on its own, this paper\n\nalso sheds light on the possibility that a learned framework\ncan be generalizable to multiple query types. As such, it can\nimprove the performance of a system while also saving time\nwhen designing it. For item queries, NeuroDB may be useful\nas part of other algorithms designed for such queries, the study\nof which we leave for future work.\nC. Contributions and Roadmap\n\u000fWe formulate the problem of learning RAQs with function\napproximators (Sec. II);\n\u000fPropose NeuroDB, the ﬁrst neural network framework to\nanswer RAQs efﬁciently (Secs. III, IV); and\n\u000fShow how NeuroDB can also answer distance to nearest\nneighbour queries (Sec. V).\n\u000fOur experiments show that\n1) NeuroDB enjoys orders of magnitude gain in query time\nand provide better accuracy over state-of-the-art for an-\nswering RAQs using real-world, TPC-benchmark and syn-\nthetic datasets (Sec. VI-A); and\n2) The same architecture is used to answer distance to nearest\nneighbour query with orders of magnitude gain in query\ntime over state-of-the-art on real datasets. NeuroDB’s\nquery time is not affected by kand only marginally\nimpacted by data dimensionality (Sec. VI-B).\nSec. VII presents related work and we conclude in Sec. VIII.\nII. P ROBLEM DEFINITION\nRange Aggregate Queries . Consider a dataset (or table) D\nwithnrecords and with dattributes,X1, ...,Xd. For ease\nof discussion, we start by considering range aggregate queries\nthat can be represented by the following SQL query. Such\na query captures many real-world RAQs [1]. We discuss the\nextension of our solution to more general RAQs in Sec. IV.\nSELECTAGG (Xm) FROMD\nWHERE (xl\n1\u0014X1<xu\n1) AND ... AND ( xl\nd\u0014Xd<xu\nd)\nIn the above SQL statement, for any i,xl\niandxu\niare\nquery variables , and they, respectively, deﬁne lower and upper\nbounds on the range of the attributes. For any i,xl\niand\nxu\nican be\u00001 and1respectively, in which case there\nare no restriction on the values of Xiin the query. We say\nthat an attribute is not active in the query in that case, and\nisactivate otherwise. Furthermore, AGG is a user deﬁned\naggregation function (typical examples include SUM ,AVG\nandCOUNT ), andXm, for 1\u0014m\u0014d, is the measure\nattribute . For ease of discussion, we assume the measure\nattribute and the aggregation function are ﬁxed, that is, we\nare only interested in answering RAQs with measure attribute\nXmand aggregation function AGG . We relax this assumption\nwhen discussing general RAQs in Sec. IV. Furthermore, let\nq= (xl\n1;:::;xl\nd;xu\n1;:::;xu\nd)be adpred-dimensional vector for\ndpred = 2\u0002d. We callqaquery instance and we assume\nquery instances follows some distribution Q. Thus, different\nquery instances correspond to different range predicates for\nthe measure attribute Xmand aggregation function AGG .\nWe use a real-world database of location signals as our\nrunning example. The database, shown in Fig. 1 (left) containslatitude and longitude of GPS signals obtained from cell-phone\ndevices and, for each location signal, the duration the user\nstayed in that location (more details about the dataset are\nprovided in Sec. VI). The dataset is for downtown Houston,\nplotted on the map obtained from maps.google.com. Setting\nX1,X2andX3to represent lat., lon. and visit duration\nattributes, respectively and m= 3 andAGG asAVG , the\nSQL statement above is the query of average visit duration\nfor check-ins that fall in a rectangle with bottom left corner\nat(xl\n1;xl\n2)and top right corner at (xu\n1;xu\n2), and whose visit\nduration is between xu\n3andxl\n3. The distribution Qdecides\nwhich query instance are more common than others. Qmay\nensurexl\n3=\u00001 andxu\n3=1with probability 1, since visit\nduration is typically not an active attribute, or, (xl\n1;xl\n2)and\n(xu\n1;xu\n2)may follow a distribution so that the range is more\noften around POIs in downtown rather than residential areas.\nQuery Functions . Such RAQs can be represented by a\nfunction of the query instance. Deﬁne the function fD(:)so\nthat for a query q,fD(q)is the answer to the above SQL\nstatement. We call fD(:)aquery function . Aquery instance\ndeﬁnes a particular query on a database, and a query function\nis a function that maps a query instance to its answer. In our\nrunning example, the function fD(:)takes as input the ranges\non three attributes (and thus, its input dimensionality is 6)\nand outputs a real number. In Fig. 1 (right), we show this\nfunction for a subset of query instances. Speciﬁcally, it shows\nthe query of average visit duration given a square of side length\n0.00043 in geo-coordinates (which is about 50m) with bottom\nleft corner at location (xl\n1;xl\n2), which is achieved by setting\nxl\n3=\u00001,xu\n3=1,xu\n1=xl\n1+ 0:00043 ,xu\n2=xl\n2+ 0:00043 .\nProblem Statement . Our goal in this paper is to learn a\nfunction approximator, ^fD(:;\u0012)that approximates the query\nfunction,fD(:), well. In the general problem setting, ^fD(q;\u0012)\ncan be any algorithm, from a combinatorial methods that\noperates on the data to a neural network. For any such function\napproximator, let \u0006(^fD)be its storage cost (e.g., for neural\nnetworks, number of parameters) and \u001c(^fD)be its evaluation\ntime or query time (e.g., for neural networks, the time it takes\nfor a forward pass). Furthermore, let \u0001(f(q);y)\u00150be an\nerror function that measures how bad a solution yis when the\nactual answer is f(q)for a query q, e.g., it can be deﬁned as\n0-1 loss orkfD(q)\u0000yk. The problem studied in this paper\nis learning to answer range aggregate queries with time and\nspace constraints, formulated as follows.\nProblem 1: Given a query function fD(:), query distribution\nQ, class of function approximators, \u0002, and time and space\nrequirements tands, ﬁnd\narg min\n\u00122\u0002Eq\u0018Q[\u0001(fD(q);^fD(q;\u0012))]s.t.\u0006(^fD)\u0014s,\u001c(^fD)\u0014t\nIn the problem formulation, \u0002shows our modelling\nchoice. Furthermore, since we usually only have access to\na setQof samples from Qbut not the distribution, we\naim at optimizing1\njQjP\nq2Q\u0001(fD(q);^fD(q;\u0012)instead of\nEq\u0018Q[\u0001(fD(q);^fD(q;\u0012)].\n\nFig. 2. NeuroDB Framework\nIII. N EURO DB\nTo solve Problem 1, we design and train a function approx-\nimator in Sec. III-A. We show how it can answer queries in\nSec. III-B and analyze its performance in Sec. III-C.\nA. Model Architecture and Training\n1) Challenges: Using neural networks to answer RAQs\ncomes with its own challenges. We ﬁrst discuss these chal-\nlenges that motivate our design choices.\n(1)Query time/accuracy trade-offs . To improve the accuracy\nof a neural network, we need to increase the number of its\nparameters. Meanwhile, a forward pass of a neural network\ntakes time linear in the number of its parameters (assuming\nno parallelization). A design is needed that can limit the\nincrease in query time while improving accuracy. We avoid\nparallelization for fair comparison with existing methods, but\nits implications are discussed in Appendix A.\n(2)Dependence on n. The query function fDis dependant\non the dataset Dand the complexity of approximating fD\ndepends on the dataset and its size n.fDis a piece-wise\nconstant function and increasing ncan increase the number\nof pieces. Intuitively, this is because the answer to a query\nchanges only when the set of matching points to the range\npredicate changes. Thus, points of discontinuity can happen\nwhenever a data point is on the boundary of the range\npredicate, as in those scenarios changing the range predicate\nby any non-zero amount can change the value of the query\nfunction by a ﬁxed amount. This also implies that the number\nof discontinuities is larger when there are more points in a\ndatabase. Thus, our proposed architecture should be able to\ntake data size into account. Fig. 1 (right) shows the piece-wise\nconstant nature of the query function in our running example.\n(3)Training time . Since number of parameters needs to\nincrease with accuracy and data size, training could become\nharder and take more time and space. Although training time\nis a preprocessing step and does not affect query time, training\nof the network should be feasible with existing GPUs.\n2) Architecture and Training: NeuroDB consists of multi-\nple neural networks with identical structures. At a preprocess-\ning step, we split the query space into a number of partitions\nand learn a different neural network to answer the queries for\neach partition. At query time, to answer a new query, we ﬁnd\nwhich neural network is responsible for it and then perform a\nforward pass of that neural network. The answer to the query\nis the output of the neural network.Fig. 2 shows this framework for our average visit distribu-\ntion RAQ example. Here, based on the formulation in Sec. II,\nthe query space is the possible bottom-left co-ordinates of the\nquery rectangle. Fig. 2 shows that in a preprocessing step, the\nquery space is partitioned into 4 partitions. Then, for the i-th\npartition, a model, ^fi\nD(:;\u0012), is learned that mimics the query\nfunctionfD(:)for that part of the query space. Subsequently,\nthe partitions are indexed. Finally, given a new query, qn,\nthe index is traversed to ﬁnd which partition it belongs to. A\nforward pass of the corresponding neural network is performed\nand the output is returned as the answer, in this case ^f2\nD(qn;\u0012).\nBefore explaining the speciﬁcs of our design, we discuss\nhow this architecture addresses the challenges of Sec. III-A1.\n(1) Query time depends on the time it takes to ﬁnd the partition\nthe query belongs to, tp, and the time of a forward pass for the\nneural networks, tn. Using our architecture, increasing number\nof partitions increases tpbut nottn. Given that indexing can\nbe used to search the partitions, tpis generally very small and\nincreasing it has negligible impact on query time. As a results,\nwe can increase number of parameters and model capacity at a\nlow cost for query time. (2) Number of partitions can be used\nto increase the size of the architecture with data size and can be\nset as a function of n. (3) Training can be done independently\nfor each neural network used. The beneﬁts of this is twofold.\nFirst, neural networks can be trained in parallel and even on\ndifferent devices, which speeds up training. Second, training\nrequires less memory because all the networks do not need to\nbe loaded at once. Thus, we can train only as many networks\nas there is memory for, as opposed to having to train all the\nnetwork at once which requires larger memory.\nTo summarize, this approach requires a method for parti-\ntioning the space and indexing them, as well as designing a\nneural network and training it for each partition. Our method\nuses a kd-tree to partition the space and index them, and these\nsteps are performed together. Thus, we ﬁrst discuss those two\nsteps and then discuss training of the neural networks.\nPartitioning . For this method to be successful, a good par-\ntitioning method needs to be chosen. Although it may be\npossible to learn the partitioning, our experiments showed\nthat learning the partitioning is difﬁcult and computationally\nintensive in practice. Instead, we take a different approach\ntowards partitioning the space. Recall that our objective\nis to minimize Eq\u0018Q[\u0001fD(q;^fD(q;\u0012)]which isP\nip(q2\nPi)Eq\u0018Qi[\u0001fD(q;^fD(q;\u0012)], wherePiis thei-th partition and\nQiis the distribution of queries in partition Pi(i.e., ifgQ(q)is\n\nAlgorithm 1 getindex (N;h;i )\nRequire: A kd-tree node N, tree height hand dimension, i\nto split the node, Non\nEnsure: A kd-tree with height hrooted atN\n1:ifh= 0 then\n2:N:model trainmodel (N:Q;fD(N:Q))\n3: return\n4:N:val median ofN:Q alongi-th dimension\n5:N:dim i\n6:Qleft=fqjq2N:Q;q [N:dim ]\u0014N:valg\n7:Qright =fqjq2N:Q;q [N:dim ]>N:valg\n8:forx2fleft;rightgdo\n9:Nx new node\n10:Nx:Q Qx\n11:N:x Nx.AddingNxas left or right child of N\n12:getindex (Nx;h\u00001;(N:dim + 1) moddpred)\np.d.f. ofQ, p.d.f. ofQiisgQ(q)\np(q2Pi)ifq2Piand 0 otherwise).\nThus, for each partition, its contribution to our objective is\ndependant on probability of it being queried as well as the\naverage approximation error, where the former depends on the\nquery distribution while the latter depends on the complexity\nof the function being approximated. Hence, we should select\npartitions such that high probability areas are approximated\naccurately, while the error for low probability partitions can be\nhigher. We use the general observation that reducing the size of\nthe space approximated by a neural network allows for better\napproximations in the smaller space. We choose partitions that\nare smaller where the queries are more frequent and larger\nwhere they are less frequent. This can be done by partitioning\nthe space such that all partitions are equally probable.\nTo this end, we build a kd-tree on our sampled query set, Q.\nNote that the split points in the kd-tree can be considered as\nestimates of the median of the distribution Q(conditioned on\nthe current path from the root) along one of its dimensions\nobtained from the samples in Q. We build the kd-tree by\nspecifying a maximum height, h, and splitting every node\nuntil all leaf nodes have height h, which creates 2hpartitions.\nSplitting of a node Nis done based on median of one of the\ndimensions of the subset, N:Q , of the queries, Q, that fall in\nN. Thus, a leaf node will have at least bjQj\n2h\u00001cqueries. The\ncomplete procedure is shown in Alg. 1. To build an index with\nheighthrooted at a node, Nroot(note thatNroot:Q=Q), we\ncallgetindex (Nroot;h;0)deﬁned in Alg. 1.\nTraining . We train an independent model for each of the 2h\nleaf nodes. For a leaf node, N, this happens in Line 2 of Alg. 1,\nwhere the function trainmodel (N:Q;fD(N:Q))returns a\ntrained model on the training data N:Q andfD(N:Q).N:Q\nare query samples that fall within the part of the query space\nthat nodeNis responsible for. fD(N:Q)is used as the\ntraining label for queries in N:Q . Note that the answers can\nbe collected through any known algorithm, where a typical\nalgorithm iterates over the points in the database, pruned by\nan index, and for a candidate data point checks whether it\nmatches the RAQ predicate or not. We emphasize that this is\na pre-processing step. That is, this sample collection step isAlgorithm 2 getanswer (N;q)\nRequire: kd-tree root node Nand queryq\nEnsure: Answer toq\n1:whileNis not leaf do\n2: ifq[N:dim ]\u0014N:val then\n3:N N:left\n4: else\n5:N N:right\n6:returnN:model:forward pass(q)\nonly performed once and is only used to train our model. Fur-\nthermore, the process is embarrassingly parallelizable across\ntraining queries, if preprocessing time is a concern.\nThe process of training is similar to a typical supervised\ntraining of a neural network with stochastic gradient descent\n(SGD). We use Adam optimizer [18] for training and use\na squared error loss function, that is, for the i-th partition\ncorresponding to the leaf node N, the minimization objective\nis1\njN:QjP\nq2N:Q(fD(q)\u0000^fi\nD(q;\u0012))2.\nNeural Network Architecture . We use a fully connected\nneural network for each of the partitions. The architecture\nis the same for all the partitions and consists of nllayers,\nwhere the input layer has dimensionality dpred, the ﬁrst layer\nconsists of lfirst units, the next layers have lrest units and\nthe last layer has 1 unit. We use relu activation all the layers\n(except the output layer). Note that nl,lfirst andlrest are\nhyper-parameters of our model. Although approaches in neural\narchitecture [19] search can be applied to ﬁnd them, they are\ngenerally computationally expensive. In this paper, we use a\nsimple heuristic. We select one of the partitions, and do a\ngrid search on the hyper-parameters. Since our neural network\narchitecture for each partition is small, this grid search can\nbe done in a practical time frame. The grid search ﬁnds the\nhyper-parameters so that NeuroDB satisﬁes the space and time\nconstraints in Problem 1 while maximizing its accuracy.\nB. Answering Queries\nAnswering queries using our NeuroDB framework is sim-\nple. The pseudocode is shown in Alg. 2. For a query, q, ﬁrst,\nthe kd-tree is traversed to ﬁnd the leaf node that the query q\nfalls into. The answer to the query is a forward pass of the\nneural network corresponding to the leaf node.\nC. Analysis\nGiven a value of h, there are 2hpartitions and each contains\na neural network. We let h= logn\ncfor some user parameter\ncdenoting the capacity of a neural network, so that the\nnumber of partitions, Np, isn\nc. That is, we increase the\nnumber of partitions linearly in n. Intuitively, the capacity of\na neural network denotes how complex of a function it can\napproximate well, which depends on the number of neural\nnetwork parameters, number of points in the database (as well\nas their distribution) and number of training samples available.\nWe leave a theoretical study of the capacity of a neural network\nto the future work, but brieﬂy mention that recent work on\nmemorization capacity of a neural network [20] can be seen\nas a ﬁrst step in this direction. We revisit the topic of what\n\nvalue ofhshould be chosen empirically in our experiments.\nRegarding time and space complexity, for a ﬁxed neural\nnetwork architecture but variable data size and dimensionality,\nthere will be O(dpredn)number of parameters, which means\nthe space complexity is O(dpredn). Furthermore, at query\ntime, the kd-tree needs to be traversed which takes O(logn),\nand a forward pass of the neural network takes O(dpred). Thus,\ntime complexity of the algorithm is O(logn+dpred).\nWe acknowledge that, similar to recent learning-based ap-\nproaches [1], [9] we do not provide an analytical formulation\nof the accuracy of NeuroDB. Nevertheless, we experimentally\nshow that this architecture can provide good accuracy in\npractice. In our experiments, we discuss how model accuracy\ndepends on number of training samples available, number of\npartitions and the size of each neural network.\nRegarding training time, given a network architecture, and\nassumingTiterations of stochastic gradient descent, training\ntime can be quantiﬁed as O(dprednT), where each iteration\nfor each network takes O(dpred)and there are O(n)networks.\nIV. N EURO DB FOR GENERAL RAQ S\nTo train NeuroDB, we require a query function fD(:)and a\nquery setQ. NeuroDB treats the query function as a black box,\nonly utilizing it to collect the labels for training samples in\nQ. Thus, we call NeuroDB a query-type agnostic framework.\nSubsequently, after NeuroDB is trained for a query function,\nit can be used to answer corresponding query instances. As\nsuch, to extend NeuroDB to other RAQs, we only need to\nshow how they can be represented as a query function. After\nsuch a query function representation is created, we can learn\nNeuroDB to answer the corresponding query instances. Below,\nwe ﬁrst discuss how this representation can be obtained in\ngeneral RAQ settings, then we discuss how NeuroDB can be\napplied to answer RAQs in real-world databases.\nA. Query Representation\nGeneral RAQs . First, we generalize our deﬁnition of RAQs.\nAn RAQ consists of a range predicate, and an aggregation\nfunctionAGG . We consider range predicates that can be\nrepresented by a query instance q, adpred dimensional vector,\nand a binary predicate function ,Pf(q;x), that takes as inputs a\npoint in the database, x,x2D, and the query instance q, and\noutputs whether xmatches the predicate or not. Furthermore,\nAGG is a function that takes the set of matching data points\nto the query as an input and outputs a real number. Note that\nthe notion of a measure attribute is implicitly captured in the\ndeﬁnition of aggregation function (e.g., AVG (Xm)can be\ndeﬁned by the aggregation function AGG (S) =P\ns2Ss[Xm]\njSj,\nwheres[Xm]is the value of Xmattribute for a record s). Then,\ngiven a predicate function and an aggregation function, range\naggregate queries can be represented by the query function\nfD(q) =AGG (fxjx2D;p(x;q) = 1g).\nThe above formulation divides the set of all possible RAQs\ninto a set of different query functions, each deﬁned for a\nspeciﬁc aggregation and predicate function. In a real-world\napplication, relevant query functions can be created from thequery workload, where RAQs with the same predicate and\naggregation function but different query instances can be used\nto deﬁne a query function. We avoid specifying how the\npredicate function should be deﬁned to keep our discussion\ngeneric to arbitrary predicate functions, but some examples\nfollow. To represent the RAQs of the form discussed in Sec. II,\nqcan be deﬁned as lower and upper bounds on the attributes\nandPf(q;x)deﬁned as the WHERE clause in Sec. II. We\ncan also have Pf(q;x) =x[1]> x[0]\u0002q[0] +q[1], so that\nPf(q;x)andqdeﬁne a half-space above a line speciﬁed by\nq. Furthermore, for many applications, WHERE clauses in\nSQL queries are written in a parametric form [21]–[23] (e.g.,\nWHEREX1>?param 1ORX2>?param 2, where ?param\nis the common SQL syntax for parameters in a query). Such\nSQL queries can readily be represented as query functions by\nsettingqto be the parameters of the WHERE clause.\nJoin and Group By Clauses . Finally, although answering\narbitrary SQL queries is not the focus of this papers, we\ndiscuss how RAQs with Join and Group By clauses can also be\nrepresented as query functions. If an SQL statement contains\nJoin of tables, we can consider the dataset, D, on which the\nquery function is deﬁned to be the Joined tables. Then, the\nquery function can be deﬁned the same way as before, but\noperating on the joined tables. Furthermore, answering RAQs\nwith a Group By clause using NeuroDB can be done similar\nto [1]. Consider an RAQ with predicate function Pf(q;x)and\nwith the clause Group By G, where G is an attribute that\ntakes one of g1, ...,gkvalues. Such a query can be treated as\nkdifferent query functions (for each a different model can be\nlearn) where the range predicate for the i-th query function,\nPi\nf(q;x), is deﬁned as Pf(q;x)^x[G] ==gi.\nB. Applying NeuroDB\nIn our approach, possible RAQs on a database are divided\ninto various query function. Subsequently, we learn different\nmodels for different query functions. We call this query\nspecialization , where specialized models are trained to answer\nspeciﬁc query functions. This is beneﬁcial because, as shown\nin our experiments, small specialized models can answer a\nquery function within microseconds, when the state-of-the-art\nnon-specialized models take milliseconds. Intuitively, this huge\nadvantage is due to a specialized model being able to divert\nall its capacity to learn patterns for a speciﬁc query function.\nWhen a system requires answers to multiple query func-\ntions, the choice of which query functions to learn a model\nfor can be done by the database administrator, akin to the\nchoice of which attribute to index. Intuitively, models should\nbe learned for queries that are frequently performed on the\ndatabase. Similar to existing work [1], [5], if a model has\nnot been learned for a particular query function, we assume\nthat a default query processing engine can answer the query.\nOverall, Learning a model for a query function increases space\nconsumption (for model storage) but improves query time\n(since learned models can answer a query faster). Given that,\nin our approach, models take space equal to only a fraction\nof data size but speed up query time by orders of magnitude,\n\nfD(q)\n01 2 345 6123456fD(q)1\n2\nQuery InputQuery's Answer: Neares Neigbour \n: Distance to Nearest Neigbour \nq\nfD(q) \n01 2 345 61234563\nq\nQuery InputQuery's Answer: Count QueryFig. 3. Nearest neighbour, f1\nD(q), distance to nearest neighbour, f2\nD(q), (left)\nand range aggregate, f3\nD(q), (right) query functions\nlearning multiple models to efﬁciently answer different query\nfunctions proves to be a realistic and beneﬁcial choice.\nFurthermore, in contrast to recent learning-based methods\n[1], [9], our approach can learn to answer different RAQs\nwithout making assumptions about the query function (besides\nthe fact that the range predicate can be represented by a\npredicate function). This allows learning arbitrary and poten-\ntially application speciﬁc RAQs that generic database systems\naren’t optimized for (e.g., an arbitrary aggregation function\non a polygon-shaped range predicate), and can substantially\nimprove the performance of such systems.\nV. N EURO DB FOR NEAREST NEIGHBOUR QUERIES\nIn this section, we show how the NeuroDB framework can\napplied to nearest neighbour queries. As discussed in Sec. IV,\nNeuroDB framework is query type agnostic, and to apply\nNeuroDB to a query type, it is only required that the query\ntype is represented by query functions. Thus, below we discuss\nthe query representation for two variants of nearest neighbour\nqueries. After specifying the query representation, NeuroDB\ncan readily be applied to answer the queries.\nk-th nearest neighbour query . Given a d-dimensional\ndatabase, an integer k, and a query point q2Rd,k-th nearest\nneighbour query is to ﬁnd the point in Dwhose distance to qis\nthek-th smallest. For this query, we deﬁne fD(q) :Rd!Rd\nas the query function that takes in a point in Rdand outputs\nanother point inRd\\D.\nFor the one dimensional database D=f1;2;3;5;6g, Fig. 3\n(left) shows the k-th nearest neighbour query function for\nk= 1 (Shown as f1\nD(q)in the left ﬁgure). Observe that\nf1\nD(q)is a step function, where the output of f1\nD(q)is always\na point in the database. f1\nD(q)is constant over the query\ninputs whose nearest neighbour is the same, and points of\ndiscontinuity occur when the nearest neighbour changes (this\nhappens at the mid-point between consecutive data points,\nshown by dashed lines in the ﬁgure). We have also included\nFig. 3 (right) for comparison with an RAQ. In Fig. 3 (right),\nwe consider the count aggregation function and the range\npredicate is deﬁned by the query variable q,q2R, and\npredicate function Pf(q;x), wherePf(q;x)is set to 1 if x>q\nand 0 otherwise (i.e., the query input speciﬁes the beginning\nof the range and all ranges are of the form (q;1)).\nDistance to k-th nearest neighbour . In addition to k-th\nnearest neighbour query, we also consider the distance to thek-th nearest neighbour query. That is, if pis thek-th nearest\nneighbour of a query point q, thenfD(q) =d(p;q)for some\ndistance metric, d(p;q). Note thatfD:Rd!R . As discussed\nin Sec. I-B, this query function is useful for various applica-\ntion, but has not been traditionally studied separately from the\nnearest neighbour query, because combinatorial methods use a\nnearest neighbour algorithms to ﬁnd the distance to the nearest\nneighbour. However, using neural networks we can directly\ncalculate distance to the nearest neighbour without ever ﬁnding\nany of the nearest neighbours. The red curve in Fig. 3 shows\nthis query for d= 1 andk= 1 (on the same database\nmentioned above). Observe that, in contrast to the nearest\nneighbour query, this query is continuous. Furthermore, its\nrange is a single real number, as opposed to Rd.\nFor bothk-NN and distance to k-NN query types, following\nour framework of query specialization discussed in Sec. IV-B,\nwe deﬁnefD(q)to be the query function for a speciﬁc value\nofk. Thus,k-NN query type is divided into different query\nfunctions, each for a value of k. The same discussion as in\nSec. IV-B for RAQs applies to k-NN and distance to k-NN\nqueries, e.g., we can learn NeuroDB for common values of\nkand a default nearest neighbour algorithm can be used to\nanswer the queries for values of kwhere no model is learned.\nVI. E MPIRICAL STUDY\nIn this section, we ﬁrst empirically evaluate NeuroDB for\nRAQs in Sec. VI-A. Then, we apply the same NeuroDB\narchitecture to answer distance to nearest neighbour queries\nin Sec. VI-B.\nSystem Setup. The experiments are performed on a machine\nrunning Ubuntu 18.04 LTS equipped with an Intel i9-9980XE\nCPU (3GHz), 128GB RAM and a GeForce TRX 2080 Ti\nNVIDIA GPU.\nModel Training and Evaluation . For all the experiments,\nbuilding and training of NeuroDB is performed in Python 3.7\nand Tensorﬂow 2.1. Training of the model is done on GPU.\nThe model is saved after training. For evaluation, a separate\nprogram written in C++ and running on CPU loads the saved\nmodel, and for each query performs a forward pass on the\nmodel. Thus, model evaluation is done with C++ and on CPU,\nwithout any parallelism for any of the algorithms. We refer to\nour algorithm as NeuroDB. Unless otherwise stated, we set\nthe model depth to 5 layers, with the ﬁrst layer consisting of\n60 units and the rest with 30 units. The height of the kd-tree\nis set to 4. We found this architecture to provide both good\naccuracy and query time, but we also present results on impact\nof each of the hyperparamters.\nA. Range Aggregate Query\n1) Setup:\nDataset . We use real, benchmark and synthetic datasets for\nevaluation. The data size and dimensionality for each dataset\nis shown in Table I. For each dataset, we specify a measure\nattribute on which the aggregation function operates.\nPM2.5 . PM2.5 [24] contains PM2.5 statistics for locations\nin Beijing. Similar to [1], we let PM2.5 to be the measure\nattribute.\n\nTPC-DS . We used TPC-DS [25] with scale factors 1 and 10,\nrespectively referred to as TPC1 and TPC10. Since we study\nrange aggregate queries, we consider the numerical attributes\nin store sales table as our dataset. We use net proﬁt as the\nmeasure attribute.\nSynthetic datasets . To study the impact of data dimension-\nality and distribution, we generated synthetic 5, 10 and 20\ndimensional data from Gaussian mixture models (GMM) with\n100 components whose mean and co-variance are selected\nuniformly at random, respectively referred to as G5, G10 and\nG20. GMMs are often used to model real data distributions\n[26]. We set one of the columns to be the measure attribute.\nVeraset . As was used in our running example, we use Veraset\ndataset, which contains anonymized location signals of cell-\nphones across the US collected by Veraset [27], a data-as-a-\nservice company. Each location signal contains an anonymized\nid, timestamp and the latitude and longitude of the location.\nWe performed stay point detection [28] on this dataset (to,\ne.g., remove location signals when a person is driving), and\nextracted location visits where a user spent at least 15 minutes\nand for each visit, also recorded its duration. 100,000 of the\nextracted location visits in downtown Houston were sampled\nto form the dataset used in our experiments, which contains\nthree columns: latitude, longitude and visit duration. We let\nvisit duration to be the measure attribute.\nQuery Distribution . Our experiments consider query func-\ntions consisting of average, sum, standard deviation and me-\ndian aggregation attributes together with two different predi-\ncate functions. First, similar to [1], our experiments show the\nperformance on the predicate function deﬁned by the WHERE\nclause in Sec.II. We consider up to 3 active attributes in the\npredicate function. To generate a query instance with ractive\nattributes, we ﬁrst select, uniformly at random, ractivate\nattributes (from a total of dpossible attributes). Then, for\nthe selected active attributes, we randomly generate a range.\nUnless otherwise stated, the range for each active attribute\nis uniformly distributed. This can be thought of as a more\ndifﬁcult scenario for NeuroDB as it requires approximating\nthe query function equally well over all its domain, while\nalso giving a relative advantage to other baselines, since they\nare unable to utilize the query distribution. Unless otherwise\nstated, for all dataset except Veraset, we report the results for\none active attributes and use the average aggregation function.\nFor Veraset, we report the results setting latitude and longitude\nas active attributes. Second, to show how NeuroDB can be\napplied to application speciﬁc RAQs, in Sec.VI-A4, we discuss\nanswering the query of median visit duration given a general\nrectangle on Veraset dataset.\nMeasurements . In addition to query time and space used, we\nreport the normalized absolute error for a query in the set\nof test queries, T, deﬁned asjfD(q)\u0000^fD(q;\u0012)j\n1\njTjP\nq2TjfD(q)j. The error is\nnormalized by average query result magnitude to allow for\ncomparison over different data sizes when the results follow\ndifferent scales.\nBaseline Algorithms . We considered DBEst [1] and DeepDBDataset # Points Dim.\nG5, G10\nG20100,0005, 10\n20\nPM2.5 [24] 41,757 4\nTPC1 [25] 2,653,123 13\nTPC10 [25] 26,532,166 13\nVeraset (VS) 100,000 3\nTABLE I\nDATASETS FOR RANGE AGGREGATE\nQUERIESDataset # Points Dim.\nGloVe (GV)\n[29]1,191,88725, 50\n100, 200\nGIST [30] 1,000,000 960\nKDD [31] 1,009,745 36\nIPUMS [32] 70,187 60\nTABLE II\nDATASETS FOR DISTANCE TO\nNEAREST NEIGHBOUR QUERY\n[9] as the state-of-the-art model-based approximate query\nprocessing engines. Both algorithms learn models of the data\nand answer speciﬁc queries based on the models of the data.\nWe use the open-source implementation of DBEst available\nat [33] and DeepDB at [34]. In a preliminary experiment, we\ncompared the performance of DBEst and DeepDB on TPC1,\nfor queries with one active attribute and observe that DeepDB\nperforms much better than DBEst. Furthermore, DBEst’s\nimplementation does not support multiple active attributes.\nThus, we chose DeepDB as our model-based baseline. We use\nthe default parameter setting for DeepDB. Furthermore, we\nuse VerdictDB [5] as our sampling-based baseline, using its\npublicly available implementation [35]. We also implemented\na sampling-based baseline designed speciﬁcally for range ag-\ngregate queries, referred to as TREE-AGG. In a pre-processing\nstep and for a parameter k, TREE-AGG samples kdata\npoints from the database uniformly. Then, for performance\nenhancement and easy pruning, it builds an R-tree index on the\nsamples, which is well-suited for range predicates. At query\ntime, by using the R-tree, ﬁnding data points matching the\nquery is done efﬁciently, and most of the query time is spent\non iterating over the points matching the predicate to compute\nthe aggregate attribute required. For both TREE-AGG and\nVerdictDB, we set the number of samples so that the error\nis similar to that of DeepDB, as we assume the-state-of-the-\nart algorithm, DeepDB, answers the queries with an acceptable\nerror rate.\n2) Results Across Datasets: Fig. 4 (a) shows the error on\ndifferent datasets, where NeuroDB provides a lower error rate\nthan the baselines. Fig. 4 (b) shows that NeuroDB achieves this\nwhile providing multiple orders of magnitude improvement in\nquery time. NeuroDB has a relatively constant query time.\nThis is because, across all datasets, NeuroDB’s architecture\nonly differs in its input dimensionality, which only impacts\nnumber of parameters in the ﬁrst layer of the model and thus\nchanges model size by very little. Due to our use of small\nneural networks, we observe that model inference time for\nNeuroDB is very small and in the order of few microseconds,\nwhile the modeling choices of DeepDB leads to query time\noften multiple orders of magnitude larger. Furthermore, the\nresults on G5 to G20 show the impact of data dimensionality\non the performance of the algorithms. We observe that, for\nNeuroDB, the error increases as dimensionality increases,\nwhich is because the model needs to learn more information.\nThe similar impact can be seen for DeepDB, manifesting itself\nin increased query time. Furthermore, the R-tree index of\nTREE-AGG often allows it to perform better than the other\n\n PM2.5  VSTPC1 TPC10 G5 G10 G20\nDatasets0.000.020.040.06Normalized MAE(a) Error\n PM2.5  VSTPC1 TPC10 G5 G10 G20\nDatasets102103104105Time (/uni03BCs)(b) Quer( timeNeuroDB TREEμAGG VerdictDB DeepDBFig. 4. RAQs on different datasets\n0.010.030.05 0.10\nQuary Range0.000.020.040.06Normalized MSE(a) Error\n0.010.030.05 0.10\nQuar( Range102103104ti e (/uni03BCs)(b) Quer( Ti eNeuroDB VerdictDB DeepDB TREEμAGG Fig. 5. Impact of query range on RAQs\n1 2 3\nNo. active att ibutes0.000.020.040.06No malized MAE(a) E  o \n1 2 3\nNo. active att ibutes102103104Time (/uni03BCs)(b) Que y timeNeu oDB TREEμAGG DeepDB Ve dictDB\nFig. 6. Impact of number of active attributes on RAQ\nAVG SUM STD\nAgg. function0.000.010.020.03Normalized MAE(a) Error\nAVG SUM STD\nAgg. f(nction102103104105Time (/uni03BCs)(b) Q(er) timeNe(roDB TREEμAGG DeepDB VerdictDB Fig. 7. Impact of agg. function on RAQs\nbaselines, especially for lower dimensional data.\n3) Different Workloads on TPC1:\nImpact of Query Range . We experimented with setting the\nquery range to xpercent of the domain range, for x2\nf1;3;5;10g, presented in Fig. 5. We observe that error rate\nof NeuroDB increases for smaller query ranges since for\nsmaller ranges NeuroDB needs to memorize where exactly\neach data point, rather then learning the overall distribution of\ndata points which can be done for larger ranges. Nevertheless,\nNeuroDB provides better accuracy than the baselines for query\nranges at least 3 percent, while performing queries orders of\nmagnitude faster for all ranges. We note that if more accurate\nanswers are needed for smaller ranges in an application, in-\ncreasing the model size of NeuroDB can improve its accuracy\nat the expense of query time (as studied in Sec. VI-A4).\nImpact of no. of active attributes on Query Time . We vary\nthe number of active attributes in the range predicate from one\nto three. The accuracy for all the algorithms drops when there\nare more active attributes, with NeuroDB outperforming the\nalgorithms both in accuracy and query time. For NeuroDB, the\ndrop in accuracy can be due to larger training size needed for\nhigher dimensional data to achieve the same accuracy, while\nour training procedure is the same for all dimensions.\nImpact of Aggregation Function . Fig. 7 shows how different\naggregation functions impact performance of the algorithms.\nOverall, NeuroDB is able to outperform the algorithms for\nall aggregation functions. VerdictDB and DeepDB implemen-\ntation did not contain the implementation for STD and thus\ntheir results are not reported for that aggregation function.\n4) Results on Veraset:\nTime/Space/Accuracy Trade-Offs . We study different time\n/space/accuracy trade-offs achievable by NeuroDB and other\nmethods, shown in Fig. 8. For NeuroDB, we vary number\nof layers (referred to as depth of the neural network), d,\nnumber of units per layer (referred to as width of the neural\nnetwork),w, and height of the kd-tree, h, to see their impact\non its time/space/accuracy. Fig. 8 shows several possible\ncombinations of the hyperparameters. For each line in Fig. 8,\nNeuroDB is run with two of those hyperparameters kept thesame and another one changing. Labels of the lines can\nbe interpreted as follows. The line labels are of the form\n(height;width;depth ), where two of height, width ordepth\nhave numerical values and are the constant hyperparameters\nfor that particular line. Furthermore, the value of one of height ,\nwidth ordepth isfd;w;hgand is the variable hyperparameter\nfor the plotted line. For example, line labelled (h, 120, 5)\nmeans the experiments for the corresponding line are with a\nNeuroDB architecture with 120 number of units per layer,\n5 layers and each point plotted corresponds to a different\nvalue for the kd-tree height, and label (0, 30, d) means the\nexperiments are run with varying depth of the neural network,\nwith kd-tree height 0 (i.e. only one partition) and the width\nof the neural network is 30. The hyperparameter values are as\nfollows. For lines (h, 120, 5) and (h, 30, 50), kd-tree height\nis varied from 0 to 4, for the line labelled (0, w, 5) neural\nnetwork width is in the set f15;30;60;120gand for lines\nlabelled (0, 120, d) and (0, 30, d) neural network depth is\nin the setf2;5;10;20g.\nFinally, TREE-AGG and VerdictDB are plotted for sam-\npling sizes of 100%, 50%, 20% and 10% of data size. For\nDeepDB only one result is reported since we observed in our\nexperiments that changing the hyperparameters only impacts\naccuracy but not query time. Thus, we only report the result\nfor the setting with the best accuracy.\nFig. 8 (a) shows the trade-off between query time and\naccuracy. Overall, NeuroDB performs well when fast answers\nare required but some accuracy can be sacriﬁced, while if\naccuracy close to an exact answer is required, TREE-AGG\ncan perform better. Furthermore, Fig. 8 (b) shows the trade-off\nbetween space consumption and accuracy. Similar to time/ac-\ncuracy trade-offs, we observe that when the error requirement\nis not too stringent, NeuroDB can answer queries by taking\na very small fraction of data size. Finally, NeuroDB outper-\nforms DeepDB in all the metrics. Finally, comparing TREE-\nAGG with VerdictDB shows that, on this particular dataset,\nthe sampling strategy of VerdictDB does not improve upon\nuniform sampling of TREE-AGG while the R-tree index of\nTREE-AGG improves the query time by orders of magnitude\n\n101102103104\nTime (/uni03BC(μ0.000.020.040.060.08Normalized MSE (a) Accuracy/Time Trade -off\n10−210−1100\nSpace (fraction of data (i−e )0.000.020.040.060.08Normalized MSE(b) Accuracy/Space Trade -off(h, 120, 5)\nDeepDB(h, 30, 5) (0, w, 5) (0, 30, d) (0, 120, d) TREE-AGG VerdictDBFig. 8. Time/Space/Accuracy Trade-Off on Veraset\n103104105106\nTraining Size0.0000.0250.0500.0750.1000.125Normalized MAE(a) Impact of Trai i g Size\n010002000300040005000\nEpoch No.10(1100Normalized MAE(b) Impact of Trai i g Duratio width=120 width=30 Fig. 9. Impact of training size and duration\nFig. 10. Learned NeuroDB with depth 5 (left) and 10 (right) for the avg.\nvisit duration example. Color shows the visit duration in hours.\nof VerdictDB.\nMoreover, Fig 8 shows the interplay between different\nhyperparameters of NeuroDB. We can see that increasing\ndepth and width of the neural networks improves the accuracy,\nbut after a certain accuracy level the improvement plateaus\nand accuracy even worsens if depth of the neural network is\nincreased but the width is too small (i.e., the purple line).\nNevertheless, using our kd-tree partitioning method allows for\nfurther improving the time/accuracy trade-off as it improves\nthe accuracy at almost no cost to query time. We also observe\nthat kd-tree improves the space/accuracy trade-off, compared\nwith increasing the width or depth of the neural networks.\nThis shows that our paradigm of query specialization is in\nfact beneﬁcial, as learning multiple specialized models each\nfor a different part of the query space performs better than\nlearning a single model for the entire space.\nMedian visit duration query function . Here, we consider\nthe query of median visit duration given a general rectangular\nrange. Speciﬁcally, we consider the predicate function that\ntakes as its input coordinates of two points p1andp2, that\nrepresent the location of two non-adjacent vertices of the\nrectangle, and an angle, \u001e, that deﬁnes the angle the rectangle\nmakes with the x-axis. Then, given q= (p1;p2;\u001e), the query\nfunction is to return median of visit duration of records falling\nin the rectangle deﬁned by q. This is a common query for real-\nworld location data, and data aggregators such as SafeGraph\n[12] publish such information.\nNeither DeepDB nor DBEst can answer this query, since\nthe predicate function is not supported by those methods, and\nextending those methods to support them is not trivial. On the\nother hand, NeuroDB can readily be used to answer this query\nfunction. Although VerdictDB can be extended to support this\nquery function, the current implementation does not support\nthe aggregation function, so we do not report the results on\nVerdictDB. Table III shows the results for this query function.\nOverall, the performance of the methods is similar to otherMetric NeuroDB TREE-AGG DeepDB VerdictDB\nNorm. MAE 0.045 0.052 N/A N/A\nQuery time ( \u0016s) 25 601 N/A N/A\nTABLE III\nMEDIAN VISIT DURATION FOR GENERAL RECTANGLES\nresults on Veraset dataset, reported in Fig. 8.\nImpact of Training Size and Duration . Fig. 9 shows the\nperformance of NeuroDB as number of training samples and\nnumber of epochs to train changes. The results are for a\nNeuroDB with tree height 0 (i.e., no partitioning), neural\nnetwork depth 5 and with neural network widths of 30 and\n120. In Fig. 9 (a), we observe that at training size of about\n100,000 sampled query points, both architectures achieve close\nto their lowest error and further increasing number of samples\nonly marginally improves the performance. Furthermore, when\nsample size is small, both architectures perform similar to\neach other and the neural network with larger width only\nstarts outperforming the smaller neural network when enough\nsamples are available. Finally, Fig. 9 (b) shows that both\nmodels converge in less than one thousand epochs, for training\nsize 5\u0002106and where each epoch contains 50 batches.\nTo reach 5000 epochs, the total training time for neural\nnetwork with widths 120 and 30 was 39.7 min and 37.15 min\nrespectively.\nVisualizing NeuroDB . Fig. 10 shows the function NeuroDB\nhas learned for our running example, for two neural networks\nwith the same architecture, but with depths 5 and 10. Com-\nparing Fig. 10 with Fig. 3, we observe that NeuroDB learns a\nfunction with similar patterns as the ground truth but the sharp\ndrops in the output are smoothened out. We also observe that\nthe learned function becomes more similar to the ground truth\nas we increase the number of parameters. Note that the neural\nnetworks are of size about 9% and 3.8% of the data size.\nB. Distance to k-th Nearest Neighbour Query\n1) Setup: In this section, we use the NeuroDB framework\nwith the same architecture as for RAQs to answer distance to\nnearest neighbour queries as deﬁned in Sec. V. More results\non hyper-parameter tuning for distance to nearest neighbour\nqueries and a case study on nearest neighbour queries to\nprovide more insight into what NeuroDB learns are provided\nin Secs. B and C, respectively.\nDatasets. We use four common real datasets in our exper-\niments whose size and dimensionality are reported in Table\nII. For GloVe dataset, unless otherwise stated, we use the\n25 dimensional version and refer by GV < d > to thed\ndimensional version of the dataset.\n\nGIST IPUMS KDD GV25 GV50 GV100 GV200\nDataset102103104Time ((sμ(aμ Query time\nGIST IPUMS KDD GV25 GV50 GV100 GV200\nDataset10)210)1Re . Error(bμ ErrorNeuroDB (distμ NSG ANN NeuroDB (NNμ+NSG NeuroDB (NNμFig. 11. Results across datasets for NN queries\n2550 100 200\nk101102103Time (/uni03BCsμ(aμ Query  ime\n2550 100 200\nk0.000.020.040.06Rel. error(bμ ErrorNeuroDB (dis μ ANN NSG Fig. 12. Varying kfor dist. NN query\nQuery Distribution . We assume the queries follow the data\ndistribution, which can be true for the datasets considered. For\ninstance, the GloVe dataset contains learned word representa-\ntions. A distance to nearest neighbour query on this dataset\ncan be interpreted as checking whether a set of words, D,\ncontains a similar word to some query word q, or to check if\na queryqwould be an outlier in D(e.g. to check whether a\ngood representation for qis learned or not).\nTo generate the queries the points in the dataset are split\ninto a training set of size Ntrain and a testing set of size\nNtest(points in the dataset are also training or testing queries).\nUnless otherwise stated, we set, k= 100 ,Ntest= 10;000and\nNtrain to the size of all the dataset except the test set.\nMeasurements . We report time taken to answer a query,\nspace used and average relative error . Relative error, for the\nk-th nearest neighbour query q, when an algorithm returns\nthe pointpkwhile the correct answer is p\u0003\nkis deﬁned as\njd(pi;q)\u0000d(pi\u0003;q)j\nd(pi\u0003;q), whered(x;y)is the Euclidean distance be-\ntweenxandy. Furthermore, since the KDD dataset contained\nnear duplicate records, the distance to the nearest neighbour\ncould be very close to zero and relative error could be very\nlarge for small absolute error values. Thus, only for this\ndataset, we present the results for normalized mean absolute\nerror (as deﬁned in Sec. VI-A) instead of relative error.\nBaseline Algorithms . We compare our algorithm with NSG\n[36] and ANN [37]. NSG is a state-of-the-art graph-based\nalgorithm, shown in [36] to outperform various existing meth-\nods. We use their publicly available implementation [38].\nWe also use ANN which is a wort-case optimal algorithm\nas implemented in ANN library [39]. The algorithm, given\na worst-case relative error parameter, \u000f, returnsknearest\nneighbours of qsuch that the relative error for the k-th nearest\nneighbour in Dis at most \u000f. For both methods to select\nalgorithm parameters, we perform a grid search to ﬁnd the\nparameters such that the query time is minimum while relative\nerror is around 0.05.\n2) Comparison Across Datasets: Fig. 11 shows the results\non different datasets for both distance to k-NN andk-NN\nqueries. Note that for the baseline algorithms, the error and\nquery time for answering both k-NN and distance to k-NN is\nthe same, since the algorithms, in both cases, return knearest\nneighbours as their output. In the ﬁgure, NeuroDB (dist) is\nNeuroDB trained to answer distance to k-NN queries and Neu-\nroDB (NN) is trained to answer k-NN queries. Furthermore,\nwe implemented NeuroDB (NN)+NSG, which is an algorithm\nthat takes the output of NeuroDB (NN), and ﬁnds its ﬁrst\nnearest neighbour in the database using NSG.Distance to k-NN. First, observe that NeuroDB is able to\nanswer distance to k-NN queries orders of magnitude faster\nthan the state-of-the-art while achieving similar accuracy.\nFurthermore, its query time is only marginally affected by data\ndimensionality (e.g., see GV25 to GV200 results) and changes\nvery little for datasets with different sizes. We emphasize that\nthese results are obtained without any parameter tuning for\nNeuroDB, and NeuroDB’s architecture is exactly the same as\nwhen performing RAQs. This shows that NeuroDB can be\napplied to two different query types with minimal effort, and\nthus, can save time when designing a system.\nk-NN. Fig. 11 also shows the results for k-NN queries. Note\nthat the output of NeuroDB (NN) is a d-dimensional point,\nbut the point is not necessarily in the dataset. Thus, even\nthough Fig. 11 shows that NeuroDB (NN) can answer k-\nNN queries with accuracy similar to NSG but much faster\nfor high dimensional datasets, such an answer may not be\nuseful for real-world applications. Thus, we investigated using\nNSG together with NeuroDB (NN). Such an algorithm ﬁrst\nruns NeuroDB (NN), and then runs NSG to ﬁnd the nearest\nneighbour of the output of NeuroDB (NN). Intuitively, this can\nimprove query time because NeuroDB (NN) is very fast, and\nNSG only needs to ﬁnd the ﬁrst nearest neighbour instead of\nk-th nearest neighbour. We observe that, for high dimensional\ndatasets, such an algorithm can improve the query time over\nstandalone NSG while obtaining similar accuracy. Thus, such\na hybrid approach can be useful, where NeuroDB is used as a\nheuristic to search the space, but we leave a full investigation\nof such hybrid approaches to the future work.\n3) Impact of kon GloVe dataset: Fig. 12 shows how\nchangingkimpacts the performance. Overall khas no impact\non query time of NeuroDB, because for any value of kthe\ncost of using NeuroDB is just a forward pass of the neural\nnetwork. However, NSG and ANN need to ﬁnd all the k\nnearest neighbours, and thus their performance deteriorates\nwithk. Better support for larger values of kis one of our\nmotivations for using NeuroDB, as it can ﬁnd the distance\ntok-th nearest neighbour without unnecessarily ﬁnding all k\nnearest neighbours, which other methods do.\nVII. R ELATED WORK\nRelated works can be classiﬁed into 3 groups: algorithms\nfor RAQs and nearest neighbour queries and machine learning\nmethods for database queries.\nAlgorithms for Approximate Query Processing . Approxi-\nmate query processing (AQP) has many applications in data\nanalytics, with queries that contain an aggregation function\n\nand a selection predicate used to report statistics from the\ndata. Broadly speaking, the methods can be divided into\nsampling-based methods [2]–[5] and model-based methods\n[1], [6]–[9]. Sampling-based methods use different sampling\nstrategies (e.g., uniform sampling, [2], stratiﬁed sampling [4],\n[5]) and answer the queries based on the samples. Model-\nbased methods develop a model of the data that is used to\nanswer queries. The models can be of the form of histograms,\nwavelets, data sketches (see [6] for a survey) or regression\nand density based models [1], [8], [9]. Generally, these works\nfollow two steps. First, a model of the data is created. Then,\na method is proposed to use these data models to answer the\nqueries. The important difference between these works and\nours is that the models are created based on the database\nto answer speciﬁc queries. That is, a model is created that\nexplains the data, rather than a model that predicts the query\nanswer. For instance, regression and density based models\nof [1] or the sum-product network of [9] are models of\nthe data that are created independent of potential queries.\nWe experimentally showed that our modeling choice allows\nfor orders of magnitude performance improvement. Secondly,\nspeciﬁc models can answer speciﬁc queries, (e.g. [1] answers\nonly COUNT, SUM, A VG, V ARIANCE, STDDEV and PER-\nCENTILE aggregations). However, our framework is query-\ntype agnostic and can be applied to any aggregation function.\nIn this respect, our approach is similar to sampling-based\nmethods that can be applied to any aggregation function and\nselection predicate. However, sampling-based methods fail to\ncapitalize on patterns available in either data points or query\ndistribution, which results in worse performance.\nAlgorithms for Nearest neighbour query . Nearest neighbour\nquery has been studied for decades in the computer science\nliterature [40]–[42], and is a key building block for various\napplications in machine learning and data analysis [16], [43]–\n[46]. For various applications, such as similarity search on\nimages, it is important to be able to perform the query fast,\na problem that becomes hard to address in high-dimensional\nspaces [37], [47], [48]. As a result, more recent research has\nfocused on approximate nearest neighbour query [36], [49]–\n[52]. Generally speaking, all the methods iterate through a\nset of candidate approximate nearest neighbours and prune\nthe candidate set to ﬁnd the ﬁnal nearest neighbours. The\nalgorithms can be categorized into locality-sensitive hashing\n(LSH) [48]–[51], [53], product quantization [54], [55], tree-\nbased methods [37], [56], and graph-based searching [36],\n[52], [57]. LSH-based and quantization-based methods map\nthe query point to multiple buckets which are expected to\ncontain similar points.\nFinding a small candidate set is difﬁcult, and as dimen-\nsionality increases more candidate points need to be checked.\nHowever, our NeuroDB framework avoids accessing points\naltogether, and learns a function based on the data that can\nanswer the queries accurately. Moreover, the size of the\ncandidate set increases with k, for all the algorithms. That is,\nmore points need to be searched when kincreases. However,\nin an application which only requires distance to the k-thnearest neighbour, NeuroDB can directly output the answer,\nwithout the value of kaffecting query time. We are unaware of\nany other work that studies distance to k-th nearest neighbour\nwithout ﬁnding any of the nearest neighbours.\nMachine Learning for Databases . There has been a recent\ntrend to replace different database components with learned\nmodels [1], [8], [9], [58]–[66], [66]. Most of the effort has been\nin either indexing (on one dimensional indexes are studied in\n[66]–[68], with extensions to multiple dimensions studied in\n[61], using Bloom ﬁlters in [60], [65] and key-value stores\nin [62], [63]) or approximate query processing (learning data\ndistribution with a model [8], using reinforcement learning for\nquery processing [58], learning models based on the data to\nanswer queries [1], [9] and cardinality estimation [69], [70])\nwith [64] discussing how they can be put together to design a\ndatabase system. The main observation in these bodies of work\nis that a certain database operation (e.g. retrieving the location\nof a record for the case of indexing in [67]) can be replaced\nby a learned model. In our paper, we observe the overarching\nidea that answering anyquery can be performed by a model,\nsince any query is a function that can be approximated. In\nthis respect, our work can be seen as a generalization of the\nrecent work in this area. Solving this more general problem\nrequires a learning method with strong representation power,\nwhich motivates our use of neural networks. This is in contrast\nwith simpler models used in [1], [64], [67].\nVIII. C ONCLUSION\nWe introduced NeuroDB, a neural network framework for\nefﬁciently answering RAQs and distance to nearest neighbour\nqueries, with orders of magnitude improvement in query time\nover the state-of-the-art algorithms. We further showed that\nthe same framework and neural network architecture can be\nused to answer both query types, showing the potential for\nutilizing a single framework to answer different query types\nand minimizing human time spent on designing algorithms\nfor different query types. To improve NeuroDB, future work\ncan focus on parallelism, better partitioning methods and\nunderstanding theoretical guarantees of the accuracy of the\nmethod. Model pruning methods [71] that remove some of\ntheunimportant model weights can also be considered to\nreduce model size and evaluation time for faster performance,\nat the cost of accuracy. Additionally, studying NeuroDB for\ndynamic data and changing query distribution can be an\ninteresting future direction. A straightforward approach can be\nto frequently test NeuroDB on a (potentially changing) test set,\nand re-train the neural networks whose accuracy fall below a\ncertain threshold, but it may be possible to update the neural\nnetworks more cleverly. The general problem of updating a\nneural network is studied in the machine learning literature and\nis an active area of research [72]–[75]. However, interesting\nproblems arise in the case of NeuroDB, since insertion of a\nnew data point changes the query function in a speciﬁc and\nquery dependent way.\n\nREFERENCES\n[1] Q. Ma and P. Triantaﬁllou, “Dbest: Revisiting approximate query\nprocessing engines with machine learning models,” in Proceedings of\nthe 2019 International Conference on Management of Data , 2019, pp.\n1553–1570.\n[2] J. M. Hellerstein, P. J. Haas, and H. J. Wang, “Online aggregation,”\ninProceedings of the 1997 ACM SIGMOD international conference on\nManagement of data , 1997, pp. 171–182.\n[3] S. Agarwal, B. Mozafari, A. Panda, H. Milner, S. Madden, and I. Stoica,\n“Blinkdb: queries with bounded errors and bounded response times on\nvery large data,” in Proceedings of the 8th ACM European Conference\non Computer Systems , 2013, pp. 29–42.\n[4] S. Chaudhuri, G. Das, and V . Narasayya, “Optimized stratiﬁed sampling\nfor approximate query processing,” ACM Transactions on Database\nSystems (TODS) , vol. 32, no. 2, pp. 9–es, 2007.\n[5] Y . Park, B. Mozafari, J. Sorenson, and J. Wang, “Verdictdb: Univer-\nsalizing approximate query processing,” in Proceedings of the 2018\nInternational Conference on Management of Data , 2018, pp. 1461–1476.\n[6] G. Cormode, M. Garofalakis, P. J. Haas, and C. Jermaine, “Synopses\nfor massive data: Samples, histograms, wavelets, sketches,” Found.\nTrends Databases , vol. 4, no. 1–3, p. 1–294, Jan. 2012. [Online].\nAvailable: https://doi.org/10.1561/1900000004\n[7] R. R. Schmidt and C. Shahabi, “Propolyne: A fast wavelet-based algo-\nrithm for progressive evaluation of polynomial range-sum queries,” in\nInternational Conference on Extending Database Technology . Springer,\n2002, pp. 664–681.\n[8] S. Thirumuruganathan, S. Hasan, N. Koudas, and G. Das, “Approxi-\nmate query processing using deep generative models,” arXiv preprint\narXiv:1903.10000 , 2019.\n[9] B. Hilprecht, A. Schmidt, M. Kulessa, A. Molina, K. Kersting, and\nC. Binnig, “Deepdb: Learn from data, not from queries!” Proceedings\nof the VLDB Endowment , vol. 13, no. 7, 2019.\n[10] “Oracle9i olap user’s guide,” https://docs.oracle.com/cd/B10501 01/\nolap.920/a95295/designd6.htm, 2021, accessed Jun 15th, 2021.\n[11] “Dimensions and measures, blue and green,” https://help.tableau.com/\ncurrent/pro/desktop/en-us/dataﬁelds typesandroles.htm, 2021, accessed\nJun 15th, 2021.\n[12] “Safegraph dataset,” https://docs.safegraph.com/v4.0/docs/\nplaces-schema#section-patterns, 2020, accessed Dec 29th, 2020.\n[13] B. Settles, “Active learning literature survey,” University of Wisconsin-\nMadison Department of Computer Sciences, Tech. Rep., 2009.\n[14] A. Fujii, K. Inui, T. Tokunaga, and H. Tanaka, “Selective sampling for\nexample-based word sense disambiguation,” arXiv preprint cs/9910020 ,\n1999.\n[15] Z. Xu, R. Akella, and Y . Zhang, “Incorporating diversity and density\nin active learning for relevance feedback,” in European Conference on\nInformation Retrieval . Springer, 2007, pp. 246–257.\n[16] V . Chandola, A. Banerjee, and V . Kumar, “Anomaly detection: A survey,”\nACM computing surveys (CSUR) , vol. 41, no. 3, pp. 1–58, 2009.\n[17] S. T. Roweis and L. K. Saul, “Nonlinear dimensionality reduction by\nlocally linear embedding,” science , vol. 290, no. 5500, pp. 2323–2326,\n2000.\n[18] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980 , 2014.\n[19] B. Zoph and Q. V . Le, “Neural architecture search with reinforcement\nlearning,” arXiv preprint arXiv:1611.01578 , 2016.\n[20] C. Yun, S. Sra, and A. Jadbabaie, “Small relu networks are powerful\nmemorizers: a tight analysis of memorization capacity,” in Advances in\nNeural Information Processing Systems , 2019, pp. 15 558–15 569.\n[21] “Parameter queries (visual database tools),” https:\n//docs.microsoft.com/en-us/sql/ssms/visual-db-tools/\nparameter-queries-visual-database-tools?view=sql-server-ver15, 2021,\naccessed Jun 30th, 2021.\n[22] “Parameterized query,” https://node-postgres.com/features/queries, 2021,\naccessed Jun 30th, 2021.\n[23] “Parameterized query,” https://docs.data.world/documentation/sql/\nconcepts/dw speciﬁc/parameterized queries.html, 2021, accessed Jun\n30th, 2021.\n[24] X. Liang, T. Zou, B. Guo, S. Li, H. Zhang, S. Zhang, H. Huang, and\nS. X. Chen, “Assessing beijing’s pm2. 5 pollution: severity, weather\nimpact, apec and winter heating,” Proceedings of the Royal Society A:\nMathematical, Physical and Engineering Sciences , vol. 471, no. 2182,\np. 20150257, 2015.[25] R. O. Nambiar and M. Poess, “The making of tpc-ds,” ser. VLDB ’06.\nVLDB Endowment, 2006, p. 1049–1058.\n[26] D. A. Reynolds, “Gaussian mixture models.” Encyclopedia of biomet-\nrics, vol. 741, 2009.\n[27] “Veraset website,” https://www.veraset.com/about-veraset, 2020, ac-\ncessed: 2020-10-25.\n[28] Y . Ye, Y . Zheng, Y . Chen, J. Feng, and X. Xie, “Mining individual life\npattern based on location history,” in 2009 tenth international conference\non mobile data management: Systems, services and middleware . IEEE,\n2009, pp. 1–10.\n[29] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors\nfor word representation,” in Proceedings of the 2014 conference on\nempirical methods in natural language processing (EMNLP) , 2014, pp.\n1532–1543.\n[30] H. Jegou, M. Douze, and C. Schmid, “Gist dataaset,” http://\ncorpus-texmex.irisa.fr/, 2020, accessed Dec 29th, 2020.\n[31] “Kdd dataset,” https://archive.ics.uci.edu/ml/datasets/kdd+cup+1999+\ndata, 2020, accessed Dec 29th, 2020.\n[32] “Ipums dataset,” https://archive.ics.uci.edu/ml/datasets/IPUMS+Census+\nDatabase, 2020, accessed Dec 29th, 2020.\n[33] Q. Ma and P. Triantaﬁllou, “Dbest implementation,” https://github.com/\nqingzma/DBEst MDN, 2020, accessed Dec 21th, 2020.\n[34] B. Hilprecht, A. Schmidt, M. Kulessa, A. Molina, K. Kerst-\ning, and C. Binnig, “Deepdb implementation,” https://github.com/\nDataManagementLab/deepdb-public, 2021, accessed May 21th, 2021.\n[35] Y . Park, B. Mozafari, J. Sorenson, and J. Wang, “Verdictdb implementa-\ntion,” https://github.com/verdict-project/verdict, 2021, accessed Jul 6th,\n2021.\n[36] C. Fu, C. Xiang, C. Wang, and D. Cai, “Fast approximate\nnearest neighbor search with the navigating spreading-out graphs,”\nPVLDB , vol. 12, no. 5, pp. 461 – 474, 2019. [Online]. Available:\nhttp://www.vldb.org/pvldb/vol12/p461-fu.pdf\n[37] S. Arya, D. M. Mount, N. S. Netanyahu, R. Silverman, and A. Y . Wu,\n“An optimal algorithm for approximate nearest neighbor searching ﬁxed\ndimensions,” Journal of the ACM (JACM) , vol. 45, no. 6, pp. 891–923,\n1998.\n[38] C. Fu, C. Xiang, C. Wang, and D. Cai, “Nsg implementation,” https:\n//github.com/ZJULearning/nsg, 2020, accessed Sept 16th, 2020.\n[39] D. M. Mount and S. Arya, “Ann: A library for approximate nearest\nneighbor searching,” http://www.cs.umd.edu/ \u0018mount/ANN/, 2020, ver-\nsion 1.1.2 Release Date: Jan 27, 2010.\n[40] J. H. Friedman, F. Baskett, and L. J. Shustek, “An algorithm for ﬁnding\nnearest neighbors,” IEEE Transactions on computers , vol. 100, no. 10,\npp. 1000–1006, 1975.\n[41] L. Guan and M. Kamel, “Equal-average hyperplane partitioning method\nfor vector quantization of image data,” Pattern Recognition Letters ,\nvol. 13, no. 10, pp. 693–699, 1992.\n[42] C.-D. Bei and R. Gray, “An improvement of the minimum distortion\nencoding algorithm for vector quantization,” IEEE Transactions on\ncommunications , vol. 33, no. 10, pp. 1132–1133, 1985.\n[43] S. Shalev-Shwartz and S. Ben-David, Understanding machine learning:\nFrom theory to algorithms . Cambridge university press, 2014.\n[44] A. Alfarrarjeh, S. H. Kim, V . Hegde, C. Shahabi, Q. Xie, S. Ravada\net al. , “A class of r*-tree indexes for spatial-visual search of geo-tagged\nstreet images,” in 2020 IEEE 36th International Conference on Data\nEngineering (ICDE) . IEEE, 2020, pp. 1990–1993.\n[45] L. Chen, M. T. ¨Ozsu, and V . Oria, “Robust and fast similarity search for\nmoving object trajectories,” in Proceedings of the 2005 ACM SIGMOD\ninternational conference on Management of data , 2005, pp. 491–502.\n[46] T. Yu, Y . Yang, Y . Li, X. Chen, M. Sun, and P. Li, “Combo-attention\nnetwork for baidu video advertising,” in Proceedings of the 26th ACM\nSIGKDD International Conference on Knowledge Discovery & Data\nMining , 2020, pp. 2474–2482.\n[47] R. Weber, H.-J. Schek, and S. Blott, “A quantitative analysis and\nperformance study for similarity-search methods in high-dimensional\nspaces,” in VLDB , vol. 98, 1998, pp. 194–205.\n[48] A. Gionis, P. Indyk, R. Motwani et al. , “Similarity search in high\ndimensions via hashing,” in Vldb , vol. 6, 1999, pp. 518–529.\n[49] Y . Zheng, Q. Guo, A. K. Tung, and S. Wu, “Lazylsh: Approximate near-\nest neighbor search for multiple distance functions with a single index,”\ninProceedings of the 2016 International Conference on Management of\nData , 2016, pp. 2023–2037.\n\n[50] Q. Huang, J. Feng, Y . Zhang, Q. Fang, and W. Ng, “Query-aware\nlocality-sensitive hashing for approximate nearest neighbor search,”\nProceedings of the VLDB Endowment , vol. 9, no. 1, pp. 1–12, 2015.\n[51] Y . Wang, A. Shrivastava, J. Wang, and J. Ryu, “Randomized algorithms\naccelerated over cpu-gpu for ultra-high dimensional similarity search,”\ninProceedings of the 2018 International Conference on Management of\nData , 2018, pp. 889–903.\n[52] W. Zhao, S. Tan, and P. Li, “Song: Approximate nearest neighbor\nsearch on gpu,” in 2020 IEEE 36th International Conference on Data\nEngineering (ICDE) . IEEE, 2020, pp. 1033–1044.\n[53] P. Indyk and R. Motwani, “Approximate nearest neighbors: towards\nremoving the curse of dimensionality,” in Proceedings of the thirtieth\nannual ACM symposium on Theory of computing , 1998, pp. 604–613.\n[54] H. Jegou, M. Douze, and C. Schmid, “Product quantization for nearest\nneighbor search,” IEEE transactions on pattern analysis and machine\nintelligence , vol. 33, no. 1, pp. 117–128, 2010.\n[55] J. Johnson, M. Douze, and H. J ´egou, “Billion-scale similarity search\nwith gpus,” IEEE Transactions on Big Data , 2019.\n[56] N. Roussopoulos, S. Kelley, and F. Vincent, “Nearest neighbor queries,”\ninProceedings of the 1995 ACM SIGMOD international conference on\nManagement of data , 1995, pp. 71–79.\n[57] K. Hajebi, Y . Abbasi-Yadkori, H. Shahbazi, and H. Zhang, “Fast\napproximate nearest-neighbor search with k-nearest neighbor graph,” in\nTwenty-Second International Joint Conference on Artiﬁcial Intelligence ,\n2011.\n[58] J. Ortiz, M. Balazinska, J. Gehrke, and S. S. Keerthi, “Learning\nstate representations for query optimization with deep reinforcement\nlearning,” in Proceedings of the Second Workshop on Data Management\nfor End-To-End Machine Learning , 2018, pp. 1–4.\n[59] M. Hashemi, K. Swersky, J. Smith, G. Ayers, H. Litz, J. Chang,\nC. Kozyrakis, and P. Ranganathan, “Learning memory access\npatterns,” ser. Proceedings of Machine Learning Research, J. Dy\nand A. Krause, Eds., vol. 80. Stockholmsm ¨assan, Stockholm\nSweden: PMLR, 10–15 Jul 2018, pp. 1919–1928. [Online]. Available:\nhttp://proceedings.mlr.press/v80/hashemi18a.html\n[60] S. Macke, A. Beutel, T. Kraska, M. Sathiamoorthy, D. Z. Cheng,\nand E. Chi, “Lifting the curse of multidimensional data with learned\nexistence indexes,” in Workshop on ML for Systems at NeurIPS , 2018.\n[61] V . Nathan, J. Ding, M. Alizadeh, and T. Kraska, “Learning multi-\ndimensional indexes,” in Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data , 2020, pp. 985–1000.\n[62] S. Idreos, N. Dayan, W. Qin, M. Akmanalp, S. Hilgard, A. Ross,\nJ. Lennon, V . Jain, H. Gupta, D. Li et al. , “Learning key-value store\ndesign,” arXiv preprint arXiv:1907.05443 , 2019.\n[63] S. Idreos, K. Zoumpatianos, S. Chatterjee, W. Qin, A. Wasay,\nB. Hentschel, M. Kester, N. Dayan, D. Guo, M. Kang, and Y . Sun,\n“Learning data structure alchemy,” Bulletin of the IEEE Computer\nSociety Technical Committee on Data Engineering , vol. 42, no. 2, pp.\n46–57, 2019.\n[64] T. Kraska, M. Alizadeh, A. Beutel, E. H. Chi, J. Ding, A. Kristo,\nG. Leclerc, S. Madden, H. Mao, and V . Nathan, “Sagedb: A learned\ndatabase system,” 2019.\n[65] M. Mitzenmacher, “A model for learned bloom ﬁlters and optimizing by\nsandwiching,” in Advances in Neural Information Processing Systems ,\n2018, pp. 464–473.\n[66] A. Galakatos, M. Markovitch, C. Binnig, R. Fonseca, and T. Kraska,\n“A-tree: A bounded approximate index structure,” arXiv preprint\narXiv:1801.10207 , 2018.\n[67] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” in Proceedings of the 2018 International\nConference on Management of Data , 2018, pp. 489–504.\n[68] J. Ding, U. F. Minhas, H. Zhang, Y . Li, C. Wang, B. Chandramouli,\nJ. Gehrke, D. Kossmann, and D. Lomet, “Alex: an updatable adaptive\nlearned index,” arXiv preprint arXiv:1905.08898 , 2019.\n[69] A. Kipf, T. Kipf, B. Radke, V . Leis, P. Boncz, and A. Kemper, “Learned\ncardinalities: Estimating correlated joins with deep learning,” CIDR\n2019, 9th Biennial Conference on Innovative Data Systems Research ,\n2018.\n[70] P. Wu and G. Cong, “A uniﬁed deep model of learning from both\ndata and queries for cardinality estimation,” in Proceedings of the 2021\nInternational Conference on Management of Data , 2021, pp. 2009–2022.\n[71] D. Blalock, J. J. G. Ortiz, J. Frankle, and J. Guttag, “What is the state\nof neural network pruning?” arXiv preprint arXiv:2003.03033 , 2020.[72] J. Quionero-Candela, M. Sugiyama, A. Schwaighofer, and N. D.\nLawrence, Dataset shift in machine learning . The MIT Press, 2009.\n[73] M. Long, H. Zhu, J. Wang, and M. I. Jordan, “Deep transfer learning\nwith joint adaptation networks,” in International conference on machine\nlearning . PMLR, 2017, pp. 2208–2217.\n[74] X. Wang, L. Li, W. Ye, M. Long, and J. Wang, “Transferable attention for\ndomain adaptation,” in Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence , vol. 33, 2019, pp. 5345–5352.\n[75] H. Liu, M. Long, J. Wang, and M. Jordan, “Transferable adversarial\ntraining: A general approach to adapting deep classiﬁers,” in Interna-\ntional Conference on Machine Learning , 2019, pp. 4013–4022.\n[76] G. Loosli, S. Canu, and L. Bottou, “Training invariant support\nvector machines using selective sampling,” in Large Scale Kernel\nMachines , L. Bottou, O. Chapelle, D. DeCoste, and J. Weston, Eds.\nCambridge, MA.: MIT Press, 2007, pp. 301–320. [Online]. Available:\nhttp://leon.bottou.org/papers/loosli-canu-bottou-2006\n[77] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv\npreprint arXiv:1312.6114 , 2013.\n\nAPPENDIX\nA. Parallelism\nAn important aspect of using neural networks is the ability\nto perform matrix multiplications in parallel. In this paper,\nto allow for fair comparison with existing non-parallelizable\nmethods, we discuss our results in a single threaded model and\nperform our experiments on CPUs. Meanwhile, parallelizing\nNeuroDB has its own challenges. The forwarded passes on\nour networks consists of only one query, thus, no batching is\nperformed at query time. Furthermore, each neural network\nused is relatively small. As a result, a lot of synchronization\nneeds to be done among different threads during the forward\npass (we note that performing nearest neighbour query in\nthe batch setting is also studied in the literature [55], in\nwhich parallelization becomes easier for our approach, with\nless synchronization needed). For instance, assigning a thread\nto each perceptron means output of each core needs to be\nsent to all other cores at every layer, which slows down the\nforward pass. This is despite the fact this parallelization can\ntheoretically reduce the time complexity by the number of\ncores. We believe more studies need to be conducted to reap\nthe beneﬁts of parallelization.\nB. Distance to k-NN Complementary Experimental Results\n1) Impact of hyperparameters on accuracy: Experiments\nin this subsection are performed on uniform data and query\ndistribution, with n= 10;000,d= 20 andk= 20 .\nImpact of Training Size. Fig. 13 (a) shows the impact of\ntraining size on model accuracy. The results are average of\nthree runs (three models are trained, where the randomness is\ndue to the models being initialized randomly at the beginning\nof the training, as well as using SGD for training), and the\nshaded area shows the standard deviation. In this experiment,\nto keep the training time the same, the number of updates\napplied to the models, as well as the batch size, are kept\nthe same across different training sizes (fewer epochs are run\nfor larger training sizes, because each epoch contains more\nupdates). We observe that as training size increases model\naccuracy improves. Furthermore, larger training size is more\nimportant for larger values of n. The increase in standard\ndeviation for larger training sizes is due to the algorithm\nrunning for fewer epochs on larger training sizes.\nImpact of Model Depth . Fig. 13 (b) shows the impact of\nmodel depth on accuracy. First, we observe that a linear\nmodel (e.g. a neural network with only one layer) provides\nvery poor accuracy, which justiﬁes our use of deeper neural\nnetworks. Second, increasing model depth beyond a certain\npoint does not necessarily improve accuracy. Increasing model\ndepth can cause over-ﬁtting, which explains the worsening of\nperformance observed in the ﬁgure for larger model sizes.\nImpact of Tree Height . Fig. 13 (c) shows the impact of\nthe height of the kd-tree, which determines number of par-\ntitions used. We observe that increasing the height generally\nincreases accuracy, but with larger models beneﬁting more.\nWe note that in this experiment, for each partition, we keepthe number of training samples used ﬁxed (i.e., there are more\ntraining samples as more partitions are created). Overall, we\nobserve that larger depth and more training size improve model\naccuracy. However, if the training size is ﬁxed (e.g., if we\ndon’t have access to the query distribution), there is a limit to\nimprovements obtained by increasing tree height, as number\nof training samples per model will be reduced.\nWe also note that the standard deviation shown in Fig. 13 (c)\nis over different models in the NeuroDB and not multiple runs.\nThe low standard deviation shows that all models responsible\nfor different partitions obtain similar accuracy.\n2) Accuracy/Time Trade-Off: Fig. 12 shows the accuracy/-\ntime trade-off of the algorithms on a subset of GV25 dataset.\nSpeciﬁcally, the experiments here were run on 10,000 data\npoints sampled from GV25. NSG and ANN are plotted at dif-\nferent error levels, and as can be observed query time increases\nas lower error is required. For NeuroDB, each point in the\nﬁgure corresponds to a different neural network architecture.\nFrom left to right, query time of NeuroDB increases because\na larger network architecture is used (we used a combination\nof increasing depth of the network as well as its width).\nWe observe that, initially, as larger architectures are used,\nthe ability of the model to learn increases and the accuracy\nimproves. However, after a certain point, the model accuracy\nstops improving and even deteriorates. This can be attributed\nto two facts. First, as model size increases, it becomes more\ndifﬁcult to train the model (i.e., more training samples and\nmore training iterations will be needed). Second, the model\nbecomes more prone to over-ﬁtting and may not perform well\nat evaluation time.\nAn interesting observation is that NeuroDB outperforms\nNSG and ANN in the low accuracy regime by an order of\nmagnitude. However, after a certain accuracy level it becomes\ndifﬁcult to learn a NeuroDB that learns the query with that\naccuracy. Thus, the beneﬁt of NeuroDB can be seen when fast\nanswers are required, but some accuracy can be sacriﬁced.\nC. Learned patterns for Nearest Neighbour Query\nWe use a smaller data set to study how NeuroDB performs\nnearest neighbour queries to provide interesting insights into\nwhat NeuroDB learns.\nDataset . We used the mnist dataset created by [76] which\ncontains 28\u000228 gray-scale pixel hand-written digits (i.e. each\nimage has 784 dimensions). We use a variational auto-encoder\n(V AE) [77] to ﬁrst learn a 30-dimensional representation of\neach image. Then, we create a databases, D, containing 10\ndifferent digits. We use the rest of the images in the mnist\ndataset to be our training and testing sets. 5 of the images\n(digits) in the database are shown in Fig. 15 (note that the\ndatabase contains 5 more images not shown).\n1) NeuroDB and Feature Learning:\nGoal . In this experiment our goal is to (1) gain insight\nabout the output of NeuroDB and (2) discuss the potential of\nNeuroDB in helping machine learning methods perform better\nfeature learning. We emphasize that this experiment is not a\nsimple application of V AEs, but rather shows the potential of\n\nFig. 13. Impact of various hyperparameters on distance to nearest neighbour query\n101102103\ntime ( sμ0.000.020.040.060.080.10Relative Error(aμ Time vs. relative error\nNeuroDB\nANN\nNSG Fig. 14. Accuracy/time trade-off\nFig. 15. Some of the images in the database\n Fig. 16. Queries (ﬁrst row) and answers (second row) of NeuroDB and true answers (third row)\nFig. 17. Queries (ﬁrst row) and answers (second row) of NeuroDB trained on a dataset without the digit 9\nNeuroDB in helping V AEs learn better features. The query\ncan be thought of as a style transfer task where the goal is\nto replace an image with the same digit in D. This can be\ndone with a nearest neighbour query if a good representation\nof each image is learned.\nResults and Discussion . Fig. 16 shows multiple input queries,\ntheir corresponding output of NeuroDB and the true nearest\nneighbour (the images are plotted as the output of the V AE).\nFirst, we observe that, the output of NeuroDB and the true re-\nsults are visually indistinguishable when decoded, even though\ntheir representations aren’t exactly the same. For instance,\nin the ﬁrst row, the relative error (as deﬁned in Sec. VI-B)is 0.026. This shows that small approximation error can be\ntolerated in practice.\nSecond, we observe that in the fourth column, the digit 1 is\nmapped to the digit 4. This can be attributed to the fact that\nthe V AE has not learned a good representation for the digit\n1, and has mapped it to a location closer to the digit 4 in\nthe feature space. We note that this is not caused by the error\nin NeuroDB as performing nearest neighbour search with no\napproximation error (shown in the third row) also returns digit\n4 and not 1. That is, the nearest neighbour query shows the\nproblem in the feature learning.\nAlthough NeuroDB isn’t at fault for observing this issue\n\nin feature learning, it can be useful in ﬁxing it. NeuroDB\nprovides a differentiable nearest neighbour query operator, and\nthus can be backpropogated through (in contrast with combi-\nnatorial methods that perform nearest neighbour search). For\ninstance, a loss on distance to nearest neighbour (which can\nbe calculated with NeuroDB) can enforce the representations\nbeing similar to to the digits in the database. We leave the\npotential of using NeuroDB in feature learning for the future\nwork, but we brieﬂy mention that another simple potential\nuse-case is to use NeuroDB as part of the encoder, that is, to\nconsider the output of the NeuroDB as the ﬁnal encoding. If\ngood enough representations are learnt by the V AE, NeuroDB\ncan help create a unique representation for each digit, which\ncan make a downstream classiﬁcation task easier.\n2) What is being learnt?:\nGoal . In this section we address the question of whether the\nneural network is learning any interesting patterns between the\ninput and its nearest neighbour. To do so, we perform a similar\nexperiment as above, but remove the digit 9 from the dataset\nduring training. That is, digit 9 is removed from the database\nas well as the training set. The rest of the training is done\nas before. At test time, we examine what the neural network\noutputs when digit 9 is being input.\nResults and Discussion . Fig. 17 shows the results of this\nexperiment. The ﬁrst row shows the queries and the second\nrow shows the output of the neural network. An interesting\nobservations can be made from the results. The neural network\nis able to output a digit 9, when 9 is input to the model.\nAlthough this dose not always happen (e.g., in the last two\ncolumns of Fig. 17) the fact that it is possible is in itself\nsigniﬁcant. This is because a combinatorial method used to\nanswer the query will always output another digit, i.e., a digit\nfrom 0-8 given the digit 9 as query (because 9 is not in\nthe database, and the output of the combinatorial method is\nalways in the database). In contrast, NeuroDB has learned a\nmapping for the nearest neighbour query, which is general\nenough so that a digit 9 as an input is still mapped to a\ndigit 9. This behaviour is beneﬁcial when there is missing\ndata in the database. This also shows that NeuroDB is not\nmerely memorizing training instances, but rather learning\ngeneralizable patterns. Another interesting observation is that\nthe digit 9s that are output by the model are similar (e.g., the\nimages depicted in the ﬁrst 4 columns of Fig. 17)",
  "textLength": 94555
}