{
  "paperId": "bc3c85274a65541fe0826a8956055b0f03251f67",
  "title": "Learning Correlation Space for Time Series",
  "pdfPath": "bc3c85274a65541fe0826a8956055b0f03251f67.pdf",
  "text": "Learning Correlation Space for Time Series\nHan Qiu\nMassachusetts Institute of Technology\nCambridge, MA, USA\nhanqiu@mit.eduHoang Thanh Lam\nIBM Research\nDublin, Ireland\nt.l.hoang@ie.ibm.com\nFrancesco Fusco\nIBM Research\nDublin, Ireland\nfrancfus@ie.ibm.comMathieu Sinn\nIBM Research\nDublin, Ireland\nmathsinn@ie.ibm.com\nABSTRACT\nWe propose an approximation algorithm for efficient correlation\nsearch in time series data. In our method, we use Fourier transform\nand neural network to embed time series into a low-dimensional\nEuclidean space. The given space is learned such that time series\ncorrelation can be effectively approximated from Euclidean distance\nbetween corresponding embedded vectors. Therefore, search for\ncorrelated time series can be done using an index in the embedding\nspace for efficient nearest neighbor search. Our theoretical analysis\nillustrates that our method’s accuracy can be guaranteed under\ncertain regularity conditions. We further conduct experiments on\nreal-world datasets and the results show that our method indeed\noutperforms the baseline solution. In particular, for approximation\nof correlation, our method reduces the approximation loss by a\nhalf in most test cases compared to the baseline solution. For top- k\nhighest correlation search, our method improves the precision from\n5% to 20% while the query time is similar to the baseline approach\nquery time.\nKEYWORDS\nTime series, correlation search, Fourier transform, neural network\nACM Reference Format:\nHan Qiu, Hoang Thanh Lam, Francesco Fusco, and Mathieu Sinn. 2018.\nLearning Correlation Space for Time Series. In Proceedings of ACM Confer-\nence (Conference’17). ACM, New York, NY, USA, 10 pages. https://doi.org/10.\n1145/nnnnnnn.nnnnnnn\n1 INTRODUCTION\nGiven a massive number of time series, building a compact index of\ntime series for efficient correlated time series search queries is an\nimportant research problem [ 2,8]. The classic solutions [ 2,30,42]\nin the literature use Discrete Fourier Transformation (DFT) to trans-\nform time series into the frequency domain and approximate the\ncorrelation using only the first few coefficients of the frequency\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nConference’17, July 2017, Washington, DC, USA\n©2018 Copyright held by the owner/author(s).\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM.\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\nFigure 1: Solution Framework\nvectors. Indeed, people have shown that using only the first 5 coef-\nficients of the DFT is enough to approximate the correlation among\nstock indices with high accuracy [42].\nApproximation of a time series using the first few coefficients\nof its Fourier transformation can be considered as a dimension-\nreduction method that maps long time series into a lower dimen-\nsional space with minimal loss of information. An advantage of\nsuch dimension-reduction approaches is that they are unsupervised\nmethods and are independent from use-cases; therefore, they can\nbe used for many types of search queries simultaneously. However,\nthey might not be ideal for applications on particular use-cases\nwhere the index is designed just for a specific type of query.\nIn practice, depending on situations we may serve different types\nof search queries such as top- khighest correlation search [ 14],\nthreshold-based (range) correlation search [ 2,30] or even simple\napproximation of the correlation between any pair of time series.\nDifferent objective function might be needed to optimize perfor-\nmance of different query types. For instance, for the first two query\ntypes it is important to preserve the order of the correlation in\nthe approximation, while for the third one it is more important to\nminimize the approximation error.\nTo overcome such problems, in this paper we propose a general\nframework to optimize query performance for specific combination\nof datasets and query types. The key idea behind this framework\nis sketched in Figure 1. Denote sas a time series in the set Sand\ns=DFT(s)as the discrete Fourier transformation of s. We use\na neural network f(·|θ)with parameters θto map sinto a low-\ndimensional space vector f(s|θ). This neural network tries to ap-\nproximate the correlation between a pair of time series sandrusing\nthe Euclidean distance ∥f(s|θ)−f(r|θ)∥2on the embedding space\nf(S). We can then utilize classic nearest neighbor search algorithmsarXiv:1802.03628v3  [cs.LG]  15 May 2018\n\nConference’17, July 2017, Washington, DC, USA Han Qiu, Hoang Thanh Lam, Francesco Fusco, and Mathieu Sinn\nNotations Meanings\ns,r,u time series\ns DFT of s\n¯s Mean value of s\nσs Standard deviation of s\nˆs Normalized series of s\nN+the set of all positive integers\nM time series length\nm embedding size\nk selection set size\nη selection threshold\nF top-kselection set\nθ network parameters\nf(s|θ) neural network with parameter θ\nρ query precision\nδ query approximation gap\n∥s−r∥2 Euclidean distance between sandr\ndm(s,r) Euclidean distance between sand r\nconsidering only the first melements\nTable 1: Notations\non this embedding space to index time series and improve query\nefficiency.\nWe also notice that specific loss function is needed in training\nthe network to optimize performance of specific type of query. In\nexperiments, we will see that using the right loss function for a\nspecific type of query is crucial in achieveving good results. There-\nfore, in this paper we also propose relevant loss functions for all\naforementioned search query types.\nIn summary, the contributions of this work are listed as follows:\n•we propose a framework to approximate time series correla-\ntion using a time series embedding method;\n•we do theoretical analysis to show non-optimality of the\nbaseline approach from which it motivates this work. We\nproposed appropriate loss function for each type of query\nand provide theoretical guarantee for these approaches;\n•we conduct experiments on real-world datasets to show the\neffectiveness of our approach compared to the classic solu-\ntions [ 2]: our approach reduces the approximation error at\nleast by a half in the experiments, and improves the precision\nfor top- kcorrelation search from 5% to 20%;\n•we open the source code for reproducibility of our research1.\nThe rest of the paper is organized as follows. We discuss back-\nground of the paper in section 2. Three important problems are\nformulated in section 3. In section 4 we show the non-optimality\nof the baseline approach from which it motivates this work. We\nalso show theoretical guarantee of proposed loss functions for each\nproblem formulated in section 3. Sections 5 and 6 show how to build\nand train the embedding neural networks. Experimental results are\ndiscussed in section 8 and section 9 concludes the paper.2 BACKGROUND\nLets=[s1,s2,···,sM],r=[r1,r2,···,rM]be two time series. The\nPearson correlation between randsis defined as:\ncorr(s,r)=1\nMÍM\nj=1sjrj−¯s¯r\nσsσr, (1)\nwhere ¯s=ÍM\nj=1sj/Mand¯r=ÍM\nj=1rj/Mare the mean values of s\nandr, whileσs=qÍM\nj=1(sj−¯s)2/Mandσr=qÍM\nj=1(rj−¯r)2/M\nare the standard deviations. If we further define the l2-normalized\nvector of the time series s,ˆs=[ˆs1,ˆs2,···,ˆsM], as:\nˆsj=1√\nMsj−¯s\nσs=sj−¯s\nqÍM\nj=1(sj−¯s)2, (2)\nwe can reduce corr(s,r)asˆsTˆr=1−∥ˆs−ˆr∥2\n2/2. That is, the correla-\ntion between two time series can be expressed as a simple function\non the corresponding l2-normalized time series. Therefore, in fol-\nlowing discussion, we always consider l2-normalized version ˆsof a\ntime series s.\nThe scaled discrete Fourier transformation of the time series s,\ns=[s1,s2,···,sM], is defined as:\nsj=1√\nMM−1Õ\nl=0sle−2jlπi\nM (3)\nwhere i2=−1. We use “scaled” here for the factor1√\nM, which made\nshaving the same scale as s. By the Parseval’s theorem [ 2,34],\nwe have sTr=sTr∗and∥s∥2=∥s∥2. We can then recover the\nfollowing property in the literature [30, 42]:\n•ifcorr(s,r)=sTr>1−ε2/2then dm(s,r)<ε, where dm\nis the Euclidean distance mapped on the first mdimensions\nwith m<M\n2\nd2\nm([s1,s2,···,sM],[r1,r2,···,rM])=mÕ\nj=1∥sj−rj∥2\n2.(4)\nBased on this property, authors in [ 2] develop a method to search\nfor highest correlated time series in a database utilizing d2m(s,r)≈\n2−2·corr(s,r). This method is considered as the baseline solution\nin this paper.\n3 PROBLEM FORMULATION\nAssume we are given a dataset of regular time series S∈RM,\nwhere M∈N+is a large number. By “regular” we mean that the\ntime series are sampled from the same period and with the same\nfrequency (and therefore, having same length). Some correlation\nsearch problems can be formulated as follows:\nProblem 1 (Top- kCorrelation Search) .Find a method Fthat for\nany given time series s∈Sand count k∈N+,F(s,k|S)provides\na subset of Sthat includes the khighest correlated time series with\nrespect to s. More formally,\nF(s,k|S)=arg max\n{S′|S′⊂S,|S′|=k}Õ\nr∈S′corr(s,r)\n=arg min\n{S′|S′⊂S,|S′|=k}Õ\nr∈S′∥s−r∥2\n2(5)\n1Omitted for blind review\n\nLearning Correlation Space for Time Series Conference’17, July 2017, Washington, DC, USA\nProblem 2 (Threshold Correlation Search) .Find a method Gthat\nfor any given time series s∈Sand threshold value η∈(0,1),G(s,η|S)\nprovides a subset of Sthat consists of all time series rwith corr(s,r)≥\nη. More formally,\nG(s,δ|S)={r∈S|corr(s,r)≥η} (6)\nProblem 3 (Correlation Approximation) .Find an embedding func-\ntionf:RM→Rmthat minimizes the expected approximation error\nin correlations\nf(·|S)=arg min\nfEs,r∈S|∥f(s)−f(r)∥2\n2−2(1−corr(s,r))| (7)\nSolutions of these problems are related with each other. First,\nProblem 1 and Problem 2 are almost equivalent. For instance, if\nthe set{corr(s,r)|r∈S}contains no identical elements, and if we\nletη(s,k|S)to be the kth largest element in this set, we will have\nG(s,η(s,k|S)|S)=F(s,k|S). Therefore, we only need to discuss one\nof them; in this paper we will focus on Problem 1.\nSecond, solutions of Problem 3 can lead to good approximation\nfor Problem 1. If there is an fsuch that the objective value of\nProblem 3 is close to 0, or\n∥f(s)−f(r)∥2\n2≈2(1−corr(s,r)),\nwe can consider the following approximation of Problem 1\nˆF(s,k|f,S)=arg min\n{S′|S′⊂S,|S′|=k}Õ\nr∈S′∥f(s)−f(r)∥2\n2(8)\ncombined with top- knearest neighbor search structures on the\nembedding space f(S). If the dimension mof the embedded space\nis small, we can use k-d tree [ 4]. With a balanced k-d tree, the com-\nplexity of searching the top knearest neighbors among ncandidates\nreduces to O(klogn)[19].\nAll problems defined above have several important applications.\nIn time series forecasting, one can search for strongly correlated\ntime series in historical database and then use them to predict\nthe target time series. In data storage, when the whole time series\ndataset is very large and does not fit available memory, one can\ncompress the time series significantly using the embedding with\nsmall trade-off for loss due to correlation approximation.\n4 THEORETICAL ANALYSIS\nThis section discusses the theoretical analysis to support our meth-\nods. We first show that DFT is not an optimal solution for Problem\n1 and Problem 3 in general. This result motivates our work as there\nis room for improvement over the baseline if data has specific struc-\nture that our learning algorithm can leverage. Next we show that\nin order to solve Problem 1, it suffices to approximate the l2norm\nonSwith l2norm on f(S). We then propose an appropriate loss\nfunction to solve Problem 1 effectively.\n4.1 Fourier transform’s non-optimality\nIn the literature [ 42], it has been shown that using only the first 5\nDFT coefficients, it is enough to approximate solutions for Problem\n1 on stocks data with high accuracy. However, there is still much\nroom for improvement over the methods based on DFT as shown\nin the experimental section. This is because DFT approximations\nblindly assumes that most of the energy (information) in the time\nseries is in the low-frequency components. In the two followingexamples, we illustrate that the naive DFT method might neither\naccurately approximate the correlation function nor extract the\nmost important information explaining dataset variability, if the\ninformation in the high-frequency components of the time series\nis non-negligible. Use-case-specific dimension reduction methods\nsuch as neural network can avoid such bias and therefore are more\npreferable.\nExample 1. Consider a set Sof real-valued time series s, where the\ncorresponding DFT vector sof each shas following property\nsi=sM/4+i,∀i=1,···,M/4.\nRecall that for DFT vector sof real-valued time series s, we have\nconjugacy si=s∗\nM+1−ifori=1,···,M. That implies 4d2m(s,r)=\n∥s−r∥2\n2=2−2corr(s,r)form=M/4. The optimal embedding with\nsizemfor the correlation function approximation should therefore be\nf(s)=[2s1,2s2,···,2sm]which differs from the embedding using\nDFT a constant factor of 2.\nExample 2. Consider a set Sof similar time series s, where the\ncorresponding DFT vector sof each sis generated as follows:\nsi=µ0\ni+ziσ0\ni,∀i=1,···,M\nzi∼(\nN(0,ε)∀i=1,···,m\nN(0,1)∀i=m+1,···,M(9)\nwhereε≪1,σ0\ni,0, and ziare sampled independently. It is obvious\nthat for Problem 1, DFT method has almost same precision as random\nguess.\n4.2 Approximating top- kcorrelated series\nIn this subsection, we analyze the theoretical performance guaran-\ntee of approximation (8). Using notation F(s,k|S)and ˆF(s,k|f,S),\nProblem 1 is equivalent to maximize the precision ρonfgiven s\nandk\nρ(s,k,f|S)=1\nk|ˆF(s,k|f,S)Ù\nF(s,k|S)|, (10)\nor to minimize the approximation gap δonf\nδ(s,k,f|S)=1\nk[Õ\nr∈ˆF(s,k|f,S)∥s−r∥2\n2−Õ\nr∈F(s,k|S)∥s−r∥2\n2].(11)\nIf the mapping fwell preserves the metric, that is, the gap\nbetween∥s−r∥2\n2and∥f(s)−f(r)∥2\n2is uniformly small, we can\nprove that the approximation gap δis also quite small. Formally,\nwe have:\nTheorem 4.1. If for a function f:RM→Rm, we have\n|∥f(s)−f(r)∥2\n2−∥s−r∥2\n2|≤ε,∀s,r∈S,\nwhereε∈R+, then we also have\nδ(s,k,f|S)≤2ε,∀s∈S,k∈N+.\n\nConference’17, July 2017, Washington, DC, USA Han Qiu, Hoang Thanh Lam, Francesco Fusco, and Mathieu Sinn\nProof. In fact, we haveÕ\nr∈ˆF(s,k|f,S)∥s−r∥2\n2\n≤Õ\nr∈ˆF(s,k|f,S)∥f(s)−f(r)∥2\n2+kε\n≤Õ\nr∈F(s,k|S)∥f(s)−f(r)∥2\n2+kε\n≤Õ\nr∈F(s,k|S)∥s−r∥2\n2+2kε(12)\nholds for every s∈Sandk∈N+. The second inequality above\nholds since ˆF(s,k|f,S)solves the corresponding minimization prob-\nlem (8). The conclusion then follows trivially. □\nTheorem 4.1 provides us a sanity check that approximation (8)\nbecomes more accurate (in terms of δ) as function approximation\nfgets better. We can therefore design the following loss function\non parametric mapping f(·|θ)for optimization\nLδ(θ)=Es,r∈S|∥f(s|θ)−f(r|θ)∥2\n2−∥s−r∥2\n2|. (13)\nTheorem 4.1 can also be extended for more general distance\nd(·,·)onRm×Rm.\nTheorem 4.2. Consider distance d(·,·)onRm×Rmand corre-\nsponding set approximation\nˆFd(s,k|f,S)=arg min\n{S′|S′⊂S,|S′|=k}Õ\nr∈S′d(f(s),f(r)). (14)\nIf for a function f:RM→Rm, we have\n|d(f(s),f(r))−∥ s−r∥2\n2|≤ε,∀s,r∈S,\nwhereε∈R+, then we also have\nδd(s,k,f|S)≤2ε,∀s∈S,k∈N+,\nwhereδdis defined as\nδd(s,k,f|S)=1\nk[Õ\nr∈ˆFd(s,k|f,S)∥s−r∥2\n2−Õ\nr∈F(s,k|S)∥s−r∥2\n2].(15)\nProof. This theorem can be proved by proceeding with the\nsame argument as in the proof of Theorem 4.1. □\nAlthough we have confidence that a good metric-approximating\nfcan lead to low approximation gap δ, there is no much to say\nabout the precision ρ. For instance, assume we are given a group of\ntime series Sand a target series s, and for each r∈Sthe correlation\ncorr(s,r)is close to others. Or, we can assume |corr(s,r)−c0|<ε\nwhere c0,εare fixed values. In this case, an fwith low approxi-\nmation error|∥f(s)−f(r)∥2\n2−∥s−r∥2\n2|, and therefore low gap δ,\nmight still confuse the relative order of the correlation corr(s,r)\nand distance∥s−r∥2\n2|. This will result in a low precision ρ. To\nimprove practical approximation performance on ρfor Problem 1,\nwe propose a loss function that tries to approximate correlation\norder\nLρ(θ)=Es,r,u∈S|(∥f(r)−f(s)∥2\n2−∥f(r)−f(u)∥2\n2)\n−(∥r−s∥2\n2−∥r−u∥2\n2)|.(16)The following corollary from Theorem 4.1 shows that we can\nalso obtain performance guarantee on δfor network optimizing\nLρ.\nCorollary 4.3. If for a function f:RM→Rm, we have\n|(∥f(r)−f(s)∥2\n2−∥f(r)−f(u)∥2\n2)\n−(∥r−s∥2\n2−∥r−u∥2\n2)|≤ε,∀r,s,u∈S,\nwhereε∈R+, then we also have\nδ(s,k,f|S)≤2ε,∀s∈S,k∈N+.\nProof. This reduce to Theorem 4.1 if we set r=u. □\nAt the end of this section, we notice that the uniform property\nonϵs,r=∥s−r∥2\n2−d(f(s),f(r))might be too restrictive, while in\ngeneral neural network can only attain some probabilistic bounds.\nSince we have\nδd(s,k,f|S)≤1\nk[Õ\nr∈ˆFd(s,k|f,S)ϵs,r−Õ\nr∈F(s,k|S)ϵs,r]\n≤1\nk|Õ\nr∈ˆFd(s,k|f,S)ϵs,r|+1\nk|Õ\nr∈F(s,k|S)ϵs,r|,(17)\nwe can apply concentration inequalities to obtain following proba-\nbilistic bound\nTheorem 4.4. Assume for any given s,ϵs,rare independently and\nidentically distributed zero-mean random variables on R. If we further\nassume\nEϵ2\ns,r≤ε,|ϵs,r|≤M a.s.,∀r∈S,\nwe have\nPr(δ(s,k,f|S)≥2cε)≤4 exp(−kc2ε\n2+2\n3Mc),∀c≥1.\nProof. We can then apply the Bernstein inequality on each part\nof the right hand side of (17):\nPr(δ(s,k,f|S)≥2cε)≤Pr(|Õ\nr∈ˆFd(s,k|f,S)ϵs,r|≥kcε)\n+Pr(|Õ\nr∈F(s,k|S)ϵs,r|≥kcε)\n≤4 exp(−(kcε)2\n2kε+2\n3Mkcε)\n≤4 exp(−kc2ε\n2+2\n3Mc).\n□\nSuch probabilistic bound is less ideal than the deterministic\nbound in Theorem 4.1. It is more effective for accuracy of threshold-\nbased queries since the corresponding kwill be much greater than\nin the top- kqueries and the bound will be much tighter.\n\nLearning Correlation Space for Time Series Conference’17, July 2017, Washington, DC, USA\n5 DESIGN OF EMBEDDING STRUCTURE\nIn last section, we show that if we have an accurate embedding\nfunction f, the approximation of our correlation search will also\nbe accurate. Nevertheless, it is not easy to find such an f. In this\nsection we will discuss several potential neural network structures\nthat we considered.\n5.1 Embedding on the time-domain\nFor direct embedding of time series, a natural selection is the Recur-\nrent neural network (RNN). RNN is a suitable model for sequential\ndata, and recently it has been widely applied in sequence-related\ntasks such as machine translation [ 3] and video processing [ 17].\nSpecifically, given a sequence [X1,···,XM], a (single-layer) RNN\ntries to encode first mobservations[X1,···,Xm]into state hmwith\nthe following dynamic processes\nhm+1=д(Xm+1,hm) (18)\nwhereдis the same function across mand is called a “unit”. Com-\nmon choices of RNN units include Long-short term memory (LSTM)\n[22] and Gate recurrent unit (GRU) [ 12]. We can then take the last\nstate hMas the final embedding vector\nf(s)=hM. (19)\nThis approach has a disadvantage that the training algorithm is\nvery slow when the time series is long because of the recurrent\ncomputations. Moreover, in the experiments we observed that this\nmethod does not give good results for long time series.\n5.2 Embedding on the frequency-domain\nTo avoid the time-consuming and ineffective recurrent computa-\ntions, we first use DFT to transform a time series sinto a frequency\ndomain s=DFT(s). Next, we use a multi-layer fully connected\n(dense) network with ReLU activation functions\nReLU(x)=max{x,0}\nto transform the frequency vector sinto an embedded vector with a\ndesirable size. At last, we applied l2normalization to the embedding\nvector to project it on a unit sphere. This normalization step realign\nthe scale of the Euclidean distances between embedding vectors\nwith the correlation we aim to approximate.\n6 TRAINING EMBEDDING NEURAL\nNETWORKS\nIn this section we describe how we train the embedding neural\nnetworks for different objective functions.\n6.1 Approximation of correlation\nFor Problem 3, we sample a random batch of pair of time series (s,r)\nand minimize the following loss function at each training iteration:\nLapproximate =|∥f(s|θ)−f(r|θ)∥2\n2−2(1−corr(s,r))|.\nWe call this method Chronos2Approximation algorithm in the\nexperiments.\n2Chronos in Greek means time.6.2 Top- kcorrelation search\nFor Problem 1, we sample a random batch of tuple of time series\n(s,r,u)and minimize the following loss function at each training\niteration:\nLorder =|(∥f(r|θ)−f(s|θ)∥2\n2−∥f(r|θ)−f(u|θ)∥2\n2)\n−2(corr(r,u)−corr(r,s))|\nWe call this method Chronos Order algorithm in the experiments\nbecause it tries to approximate the correlation order. According\nto Corrolary 4.3, if we optimize Lorder we also approximate the\nsolutions for Problem 1.\n6.3 Symmetry correction\nRecall that when sis a real number time series, its DFT sis symmet-\nric, i.e. siis a complex conjugate of sM−i. Because of this reason, if\nM=2∗m, we should have dM(s,r)=2∗dm(s,r)=2−2corr(s,r).\nThanks to the symmetry property, people only need to use m<M\n2\ncoefficients to approximate the correlation. In such case the approx-\nimation of 2−2corr(s,r)is corrected to 2∗dm(s,r).\nTherefore, in our implementation of the Chronos algorithms we\nuse a corrected version of the loss functions as follows to take into\naccount the symmetry of the embedding:\nLapproximate =|2∥f(s|θ)−f(r|θ)∥2\n2−2(1−corr(s,r))|.\nLorder =|2(∥f(r|θ)−f(s|θ)∥2\n2−∥f(r|θ)−f(u|θ)∥2\n2)\n−2(corr(r,u)−corr(r,s))|\n6.4 Datasets\nWe used four real-world datasets to validate our approaches. One\nof these datasets is the daily stock indices crawled from the Yahoo\nfinance [ 1] using public python API. The other datasets are chosen\nfrom the UCR time series classification datasets [ 11]. These datasets\nwere chosen because each time series is long (at least 400 readings)\nand the number of time series is large enough (at least a few thou-\nsands). Overall the characteristics of these datasets are described in\nTable 2. We preprocessed the data by adding dummy timestamps\nto the UCR time series classification datasets since our scripts do\nnot assume time series sampled with the same resolution. The size\nof the data reported in Table 2 is the disk size calculated based on\nthe du command in Linux.\n6.5 Experiment settings\nFor each dataset, we randomly split the data into training, validation\nand test with the ratios 80-10-10%. We use the training set to train\nthe embedding neural networks and validate it on the validation\nset. The test set is used to estimate the query accuracy. In particular,\nfor the top- khighest correlation queries, each time series in the test\nset is considered as a target series, and all the other series in the\ntraining set is considered as the database from which we look for\ncorrelated time series to the target. For correlation approximation\nqueries, we randomly permute the test series and create pairs of\ntime series from the permutation. Since the test set is completely\nindependent from training and validation, the reported results are\nthe proper estimate of the query accuracy in practice.\nWe fix parameters of the neural network to the default values\ndescribed in Table 3. Although we didn’t fine-tune the network\n\nConference’17, July 2017, Washington, DC, USA Han Qiu, Hoang Thanh Lam, Francesco Fusco, and Mathieu Sinn\nData # time series length size\nStock 19420 1000 400 MB\nYoga 3300 426 26 MB\nUWaveGesture 4478 945 71 MB\nStarLightCurves 9236 1024 181 MB\nTable 2: Datasets used in experiments\nhyper-parameters and explore deeper neural network structures,\nthe obtained results are already significantly better than the baseline\nalgorithms. In practice, automatic network search and tuning can\nbe done via Bayesian optimization [ 37]. Since search for the optimal\nnetwork structure and optimal network hyper-parameters is out of\nthe scope of this work, we leave this problem as future work.\nTable 3: Parameter settings\nparameter value\noptimization algorithm ADAM [26]\nlearning rate 0.01\nweight initialization Xavier [21]\nnumber of hidden layers 1\nhidden layer size 1024\nmini-batch size 256\ntraining iterations 10000 mini-batches\nThe network was implemented in TensorFlow and run in a Linux\nsystem with two Intel cores, one NVDIA Tesla K40 GPU. All the\nrunning time is reported in the given system.\n6.6 Results on correlation approximation\nFigure 2 shows the comparison between the Chronos approximation\nand DFT for approximated solutions to Problem 3. Each subplot\nin the figure corresponds to a dataset. The vertical axis shows the\napproximation loss in the test set, while the horizontal axis shows\nthe embedding size m, varying from 2 to 16. As can be observed, our\nmethod outperformed DFT in all the test cases with a significant\nmargin. In particular, the approximation error was reduced at least\nby a half, especially for small embedding size. These empirical\nresults confirm our theoretical analysis on the sub-optimality of\nDFT for Problem 3.\nIt is interesting that the down-sample approach, although very\nsimple, works very well in UWave and Stock datasets. Both Chronos\nand down-sample outperformed DFT in all cases. One possible\nexplanation is that DFT seems to preserve approximation accuracy\nfor high correlation (see Lemma 2 in [ 42]) rather than optimize\ncorrelation approximation for a randomly picked pair of time series.\n6.7 Results on top- kcorrelation search\nFigure 2 shows the comparison between the Chronos order , the\nChronos approximation , DFT and down-sample for approximated\nsolutions to Problem 1. Each subplot in the figure corresponds to a\npair of dataset and query size k(varied from 10 to 100). The vertical\naxis shows the precision at kin the test set, while the horizontal\naxis shows the embedding size m, varying from 2 to 16.Data Chronos order\nStock 947\nYoga 914\nUWaveGesture 894\nStarLightCurves 950\nTable 4: Training time (seconds) with 10000 mini-batches\nData Chronos order DFT\nStock 0.9 0.8\nYoga 0.7 0.3\nUWaveGesture 0.7 0.3\nStarLightCurves 0.6 0.4\nTable 5: Average top- kquery time (milliseconds)\nAlthough Theorem 4.1 and Corollary 4.3 provide the same theo-\nretically guarantee for the approximation bound of Chronos order\nandChronos order on Problem 1, our results show that the Chronos\norder outperforms the Chronos approximation in terms of the preci-\nsion. These algorithms differ only in the loss function in training\nthe networks: while Chronos approximation directly minimizes the\nloss of correlation approximation, the Chronos order also tries to\npreserve order information. This result implies that using the right\nloss function for a given query type is crucial in achieving good\nresults.\nIn all cases, the Chronos order algorithm outperforms the DFT\nbaseline algorithms. On the other hand, except for the Stock dataset,\nChronos approximation algorithm outperforms the DFT baseline\nalgorithm. The overall improvement varies from 5% to 20% depend-\ning on the value of kandm. The down-sample algorithm although\nworks very well in the Yoga dataset, its accuracy is very low for\nthe other cases.\n6.8 Training & Query time\nAnother important evaluation metric on our methods’ performance\nis the time efficiency in training the embedding network f(s|θ)\nand in subsequent function evaluation for incoming queries. Table\n4 shows the training time of the Chronos Order method in four\ndatasets when k=100,m=16. For cases with other values of k\nandm, the results are similar. We can see that it only takes about\n15 minutes to train the network with 10000 mini-batches.\nTable 5 shows the query time of different algorithms when\nk=100,m=16. Again, for cases with other values of kand\nm, the results are similar. As can be seen, the queries time is not\nsignificantly influenced by the additional tasks for evaluating the\nfunction f(s|θ). Since the queries time is dominated by the k-d\ntree traversal, time for evaluating the function f(s|θ)is negligible.\nQuery time using DFT is slightly faster than the the query time of\nChronos order. Both methods have very quick response time per\nquery being less than one millisecond in all experiments.\n\nLearning Correlation Space for Time Series Conference’17, July 2017, Washington, DC, USA\n●\n●\n●\n●●\n●● ●●\n●\n●\n●0.000.250.500.75\n4 8 12 16\nEmbedding size mAverage approximation lossvariable DFT chronos_app sampleStar Light Curve\n●●\n●\n●●\n●\n●\n●●\n●\n●\n●0.51.01.5\n4 8 12 16\nEmbedding size mAverage approximation lossvariable DFT chronos_app sampleUWave Gesture\n●\n●\n●\n●●\n●\n● ●●\n●\n●●0.00.51.01.5\n4 8 12 16\nEmbedding size mAverage approximation lossvariable DFT chronos_app sampleYoga\n●\n●\n●\n●●\n●\n●●●\n●\n●●0.00.51.01.52.0\n4 8 12 16\nEmbedding size mAverage approximation lossvariable DFT chronos_app sampleStock Indices\nFigure 2: Results on correlation approximation\n●●●●\n●●●●\n●●●●\n●●●●\n0.00.20.40.6\n4 8 12 16\nEmbedding size mPrecision @ 10variable chronos_order chronos_app DFT sampleStar Light Curve k=10\n●●●●\n●●●●\n●●●●\n●●●●\n0.00.20.40.60.8\n4 8 12 16\nEmbedding size mPrecision @ 50variable chronos_order chronos_app DFT sampleStar Light Curve k=50\n●●●●\n●●●●\n●●●●\n●●●●\n0.000.250.500.75\n4 8 12 16\nEmbedding size mPrecision @ 100variable chronos_order chronos_app DFT sampleStar Light Curve k=100●●●●\n●●●●\n●●●●\n●●●●\n0.00.20.40.6\n4 8 12 16\nEmbedding size mPrecision @ 10variable chronos_order chronos_app DFT sampleUWave Gesture k=10\n●●●●\n●●●●\n●●●●\n●●●●\n0.00.20.40.60.8\n4 8 12 16\nEmbedding size mPrecision @ 50variable chronos_order chronos_app DFT sampleUWave Gesture k=50\n●●●●\n●●●●\n●●●●\n●●●●\n0.00.20.40.60.8\n4 8 12 16\nEmbedding size mPrecision @ 100variable chronos_order chronos_app DFT sampleUWave Gesture k=100●●●●\n●●●●\n●●●●\n●●●●\n0.00.20.40.6\n4 8 12 16\nEmbedding size mPrecision @ 10variable chronos_order chronos_app DFT sampleYoga k=10\n●●●●\n●●●●\n●●●●\n●●●●\n0.000.250.500.75\n4 8 12 16\nEmbedding size mPrecision @ 50variable chronos_order chronos_app DFT sampleYoga k=50\n●●●●\n●●●●\n●●●●\n●●●●\n0.000.250.500.75\n4 8 12 16\nEmbedding size mPrecision @ 100variable chronos_order chronos_app DFT sampleYoga k=100●●●●\n●●●●\n●●●●\n●●●●\n0.00.20.4\n4 8 12 16\nEmbedding size mPrecision @ 10variable chronos_order chronos_app DFT sampleStock indices k=10\n●●●●\n●●●●\n●●●●\n●●●●\n0.00.20.40.6\n4 8 12 16\nEmbedding size mPrecision @ 50variable chronos_order chronos_app DFT sampleStock indices k=50\n●●●●\n●●●●\n●●●●\n●●●●\n0.00.20.40.6\n4 8 12 16\nEmbedding size mPrecision @ 100variable chronos_order chronos_app DFT sampleStock indices k=100\nFigure 3: Results on top- kcorrelation search\n7 RELATED WORKS\nCorrelation related queries on time series. During the past sev-\neral decades, people investigated a massive amount of correlation-\nrelated query problems on time series datasets in the literature.As our review will be far from exhaustive, here we rather provide\nseveral problem categories and some corresponding examples.\nOne major classification on these problems is whether a spe-\ncific time series is given in the query. For example, some works\n\nConference’17, July 2017, Washington, DC, USA Han Qiu, Hoang Thanh Lam, Francesco Fusco, and Mathieu Sinn\n[28,30,36,42] are interested in computing correlation among all se-\nquence pairs or providing all one that are correlated (over a certain\nthreshold), while others [ 18,33] aims at finding most correlated\nsequences in the dataset for the given one. There are also some\npapers addressing both sides [ 2]. In this paper, we focus on the later\ncategory that a specific time series is assumed to be given in the\nquery.\nAnother major distinction is whether the object they consider\nis the whole sequence or a subsequence. For problems focusing\non whole sequence [ 2,30,33,42], not only the query time series\nbut also all sequences from the dataset are assume to have (almost)\nthe same length. Our problems also make this assumption. For\nproblems concerning subsequences [ 18], the query time series is\ngenerally much shorter than those in the dataset, which are also\nassumed to have arbitrary lengths.\nFinally, an important distinction is whether the data is provided\noffline or collected online. On one hand, problems in the online\nsetting [ 36,42] typically address the varying nature in data streams\nand discover stationary submodule in the computation procedure\nto boost query efficiency. On the other hand, problems in the offline\nsettings [ 30,33], including ours, usually deal with the difficulties\nbrought by the massive data size.\nDimension reduction. As far as we know, we are the first to\nsuggest using function approximation, rather than algorithms for\nfeature extraction, for dimension reduction in time series indexing\nand correlation searching problems. The application of dimension\nreduction techniques to improve time series correlation search effi-\nciency is first introduced by Agrawal et al. [2]. DFT and discrete\nwavelet transform (DWT) are the two most commonly used meth-\nods. The usage of DFT was introduced by Agrawal et al. [2], who\nalso constructed R∗-tree for indexing on the transformed Fourier\ncoefficients. Chan and Fu [ 8], on the other hand, were the first\nto use wavelet analysis for time series embedding. Regardless of\nthe significant difference between DFT and DWT approaches, it\nis shown that they have similar performance on query precision\nin a later paper by Wu et al. [39]. Therefore, in this paper we only\nconsider DFT as our baseline method.\nWe also notice that since Agrawal et al. [2] researchers suggested\nmany different dimensionality reduction techniques to provide\ntighter approximation bound for pruning. To name a few, there are\nAdaptive Piecewise Constant Approximation (APCA) [ 24], Piece-\nwise Aggregate Approximation (PAA) [ 23], multi-scale segment\nmean (MSM) [ 29], Chebyshev Polynomials (CP) [ 7], and Piece-\nwise Linear Approximation (PLA) [ 10]. However, according to the\ncomparison work by Ding et al. [15], none of these significantly\noutperform DFT and DWT.\nSimilarity measures. Though simple, Pearson correlation and\nEuclidean distance ( l2-norm) are still the most commonly used\nsimilarity measure in the literature. We also focus on this measure\nin this paper. However, Euclidean distance suffers from drawbacks\nsuch as fixed time series length and its applications in general are\nmuch restricted. Therefore, in many applications, more flexible\nsimilarity measure such as Dynamic time warping (DTW) [ 35] is\nmore appropriate. DTW is an algorithm for similarity computation\nbetween general time series and can deal with time series with\nvarying lengths and local time shifting. One of the major drawbacks\nof DTW is its computation complexity; therefore, most relatedresearch on DTW develop and discuss its scalable application in\nmassive datasets [5, 25, 33, 40].\nThere are also other similarity measures such as Longest Com-\nmon Subsequence (LCSS) [ 6,38], but rather than concerning gener-\nalizability, they either focus on a specific type of data or problem\nrelated to the l2-norm and DTW. They therefore did not attract\nmuch attention in the past decade. Interested readers are again re-\nferred to the comparison work by [ 15] for more details on similarity\nmeasures.\nNeural network approximation. With the recent successes\nin deep learning, neural networks again become the top choice for\nfunction approximators. Recent work by Kraska et al. [27] further\nillustrates that neural networks, with proper training, can beat\nbaseline database indexes in terms of speed and space requirement.\nThere are also more neural-network-based methods in time se-\nries related problems. Cui et al. [13] design an multi-scale convo-\nlution neural network for time series classification. Zheng et al.\n[41] apply convolution neural network for distance metric learning\nand classification. Both works show that the designed structures\ncan achieve baseline performance when data is sufficiently large.\nHowever, in these works the networks are supervised to learn the\nmapping from time series to the given labels, while in this paper\nthe network is developed for unsupervised dimension reduction\nand feature extraction of dataset structure.\nApproximation and exact algorithm When the data is small\nwhich fits memory entirely, the exact algorithm such as MASS\nby Mueen et al. [ 31] can be used to efficiently retrieve the exact\ncorrelations. However, in applications like the ones running at edge\nwhen the data size is so big that does not fit available memory, we\nneed a small index of the data which is several orders of magnitude\nsmaller than the original size. One possible way to resolve the\nresource limitation issue is to down-sample the data to a desirable\nsize that fits memory. This approach is similar to the embedding\napproach proposed in this paper which trades between accuracy\nand resource usage. However, as we will see in the experiments,\ndown-sampling does not work well because it is not designed to\noptimize the objective of the interested problems.\nMetric learning for time-series Recently, there is a trend to\nuse deep neural networks for learning metric for different time-\nseries mining tasks. For instance, Garreau et al. proposed to learn a\nMahalanobis distance to perform alignment of multivariate time\nseries [ 20], Zhengping et al. [ 9] proposed a method for learning\nsimilarity between multi-variate time-series. Pei et al. [ 32] used re-\ncurrent neural networks to learn time series metric for classification\nproblems. Do et al. [ 16] proposed a metric learning approach for\nk-NN classification problems. Our method is tightly related to these\nworks because it can be considered as a metric learning approach.\nHowever, the key difference between our work and the prior work\nis in the objective function we optimize. While the objectives in the\nprior arts are designed for classification or time-series alignment,\nour objectives are designed to approximate correlation between\ntime series in similarity search problems. Moreover, our significant\ncontribution lies in the theoretical analysis of the approximation\nbounds for each specific type of the similar search queries.\n\nLearning Correlation Space for Time Series Conference’17, July 2017, Washington, DC, USA\n8 EXPERIMENTS\nIn this section, we discuss the experiment results. The DFT method\nis considered as the baseline and compared with Chronos approx-\nimation and Chronos order methods in several different types of\nqueries. Besides, we also compared the proposed approaches to the\ndown-sample method in which the time series were embedded to a\nsmall embedding space by downs-sampling. We first introduce the\ndatasets and experimental settings.\n9 CONCLUSIONS AND FUTURE WORK\nIn this paper, we propose a general approximation framework for a\nvariety of correlation search queries in time series datasets. In this\nframework, we use Fourier transform and neural network to embed\ntime series into low-dimensional Euclidean space, and construct\nnearest neighbor search structures in the embedding space for\ntime series indexing to speed up queries. Our theoretical analysis\nillustrates that our method’s accuracy can be guaranteed under\ncertain regularity conditions, and our experiment results on real\ndatasets indicate the superiority of our method over the baseline\nsolution.\nSeveral observations in this work are interesting and require\nmore attention. First, the approximation accuracy of our method\nvaries significantly across datasets. Therefore, it is crucial in later\nreal-world applications to more systematically evaluate the appli-\ncability of our method and design network structure for specific\ndatasets. Second, the selection of distance function dcan be impor-\ntant in improving the performance of our embedding method, since\nit might be able to balance between the approximation capability of\nthe neural network and the internal similarity within the datasets.\nOur work can be further extended in several directions. First,\nour method can only deal with time series with equal length. How-\never, a large number of real-world time series are sampled with\nirregular time frequency or during arbitrary time period. There-\nfore, embedding methods that can deal with general time series\nshould be explored in the future. Second, currently we only consider\nPearson correlation as the evaluation metric for similarity. We can\ninvestigate the applicability of our framework on other types of\nsimilarity measure such as the dynamic time wrapping (DTW) mea-\nsure. Finally, in some of the aforementioned applications such as\ntime series forecasting, correlation search for the target time series\nmight not provide the most essential information in improving the\nultimate objectives. End-to-end systems explicitly connecting cor-\nrelation search techniques and application needs might therefore\nbe more straightforward and effective.\nREFERENCES\n[1] Yahoo finance. https://finance.yahoo.com/. Accessed: 2017-06-30.\n[2]Agrawal, R., Faloutsos, C., and Swami, A. Efficient similarity search in se-\nquence databases. In International Conference on Foundations of Data Organization\nand Algorithms (1993), Springer, pp. 69–84.\n[3]Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint arXiv:1409.0473 (2014).\n[4]Bentley, J. L. Multidimensional binary search trees used for associative searching.\nCommunications of the ACM 18 , 9 (1975), 509–517.\n[5]Berndt, D. J., and Clifford, J. Using dynamic time warping to find patterns in\ntime series. In KDD workshop (1994), vol. 10, Seattle, WA, pp. 359–370.\n[6]Boreczky, J. S., and Rowe, L. A. Comparison of video shot boundary detection\ntechniques. Journal of Electronic Imaging 5 , 2 (1996), 122–129.\n[7]Cai, Y., and Ng, R. Indexing spatio-temporal trajectories with chebyshev poly-\nnomials. In Proceedings of the 2004 ACM SIGMOD international conference onManagement of data (2004), ACM, pp. 599–610.\n[8]Chan, K.-P., and Fu, A. W.-C. Efficient time series matching by wavelets. In\nData Engineering, 1999. Proceedings., 15th International Conference on (1999), IEEE,\npp. 126–133.\n[9]Che, Z., He, X., Xu, K., and Liu, Y. Decade: A deep metric learning model for\nmultivariate time series.\n[10] Chen, Q., Chen, L., Lian, X., Liu, Y., and Yu, J. X. Indexable pla for efficient\nsimilarity search. In Proceedings of the 33rd international conference on Very large\ndata bases (2007), VLDB Endowment, pp. 435–446.\n[11] Chen, Y., Keogh, E., Hu, B., Begum, N., Bagnall, A., Mueen, A., and Batista,\nG.The ucr time series classification archive, July 2015. https://www.cs.ucr.edu/\n~eamonn/time_series_data/.\n[12] Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F.,\nSchwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-\ndecoder for statistical machine translation. arXiv preprint arXiv:1406.1078 (2014).\n[13] Cui, Z., Chen, W., and Chen, Y. Multi-scale convolutional neural networks for\ntime series classification. arXiv preprint arXiv:1603.06995 (2016).\n[14] Dallachiesa, M., Palpanas, T., and Ilyas, I. F. Top-k nearest neighbor search\nin uncertain data series. Proceedings of the VLDB Endowment 8 , 1 (2014), 13–24.\n[15] Ding, H., Trajcevski, G., Scheuermann, P., Wang, X., and Keogh, E. Querying\nand mining of time series data: experimental comparison of representations and\ndistance measures. Proceedings of the VLDB Endowment 1 , 2 (2008), 1542–1552.\n[16] Do, C., Chouakria, A. D., Marié, S., and Rombaut, M. Temporal and frequen-\ntial metric learning for time series knn classification. In Proceedings of the 1st\nInternational Workshop on Advanced Analytics and Learning on Temporal Data,\nAALTD 2015, co-located with The European Conference on Machine Learning and\nPrinciples and Practice of Knowledge Discovery in Databases (ECML PKDD 2015),\nPorto, Portugal, September 11, 2015. (2015).\n[17] Donahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach, M., Venu-\ngopalan, S., Saenko, K., and Darrell, T. Long-term recurrent convolutional\nnetworks for visual recognition and description. In Proceedings of the IEEE\nconference on computer vision and pattern recognition (2015), pp. 2625–2634.\n[18] Faloutsos, C., Ranganathan, M., and Manolopoulos, Y. Fast subsequence\nmatching in time-series databases , vol. 23. ACM, 1994.\n[19] Friedman, J. H., Bentley, J. L., and Finkel, R. A. An algorithm for finding\nbest matches in logarithmic expected time. ACM Transactions on Mathematical\nSoftware (TOMS) 3 , 3 (1977), 209–226.\n[20] Garreau, D., Lajugie, R., Arlot, S., and Bach, F. Metric learning for temporal\nsequence alignment. In Advances in Neural Information Processing Systems 27 ,\nZ. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger,\nEds. Curran Associates, Inc., 2014, pp. 1817–1825.\n[21] Glorot, X., and Bengio, Y. Understanding the difficulty of training deep feedfor-\nward neural networks. In Proceedings of the Thirteenth International Conference\non Artificial Intelligence and Statistics (2010), pp. 249–256.\n[22] Hochreiter, S., and Schmidhuber, J. Long short-term memory. Neural compu-\ntation 9 , 8 (1997), 1735–1780.\n[23] Keogh, E., Chakrabarti, K., Pazzani, M., and Mehrotra, S. Dimensionality\nreduction for fast similarity search in large time series databases. Knowledge and\ninformation Systems 3 , 3 (2001), 263–286.\n[24] Keogh, E., Chakrabarti, K., Pazzani, M., and Mehrotra, S. Locally adaptive\ndimensionality reduction for indexing large time series databases. ACM Sigmod\nRecord 30 , 2 (2001), 151–162.\n[25] Keogh, E., and Ratanamahatana, C. A. Exact indexing of dynamic time warping.\nKnowledge and information systems 7 , 3 (2005), 358–386.\n[26] Kingma, D., and Ba, J. Adam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980 (2014).\n[27] Kraska, T., Beutel, A., Chi, E. H., Dean, J., and Polyzotis, N. The case for\nlearned index structures. arXiv preprint arXiv:1712.01208 (2017).\n[28] Li, Y., Yiu, M. L., and Gong, Z. Discovering longest-lasting correlation in se-\nquence databases. Proceedings of the VLDB Endowment 6 , 14 (2013), 1666–1677.\n[29] Lian, X., Chen, L., Yu, J. X., Han, J., and Ma, J. Multiscale representations for\nfast pattern matching in stream time series. IEEE transactions on knowledge and\ndata engineering 21 , 4 (2009), 568–581.\n[30] Mueen, A., Nath, S., and Liu, J. Fast approximate correlation for massive time-\nseries data. In Proceedings of the 2010 ACM SIGMOD International Conference on\nManagement of data (2010), ACM, pp. 171–182.\n[31] Mueen, A., Zhu, Y., Yeh, M., Kamgar, K., Viswanathan, K., Gupta, C., and\nKeogh, E. The fastest similarity search algorithm for time series subse-\nquences under euclidean distance, August 2017. http://www.cs.unm.edu/~mueen/\nFastestSimilaritySearch.html.\n[32] Pei, W., Tax, D. M. J., and van der Maaten, L. Modeling time series similarity\nwith siamese recurrent networks. CoRR abs/1603.04713 (2016).\n[33] Rakthanmanon, T., Campana, B., Mueen, A., Batista, G., Westover, B., Zhu,\nQ., Zakaria, J., and Keogh, E. Searching and mining trillions of time series\nsubsequences under dynamic time warping. In Proceedings of the 18th ACM\nSIGKDD international conference on Knowledge discovery and data mining (2012),\nACM, pp. 262–270.\n\nConference’17, July 2017, Washington, DC, USA Han Qiu, Hoang Thanh Lam, Francesco Fusco, and Mathieu Sinn\n[34] Rudin, W., et al. Principles of mathematical analysis , vol. 3. McGraw-hill New\nYork, 1964.\n[35] Sakoe, H., and Chiba, S. Dynamic programming algorithm optimization for\nspoken word recognition. IEEE transactions on acoustics, speech, and signal\nprocessing 26 , 1 (1978), 43–49.\n[36] Sakurai, Y., Papadimitriou, S., and Faloutsos, C. Braid: Stream mining through\ngroup lag correlations. In Proceedings of the 2005 ACM SIGMOD international\nconference on Management of data (2005), ACM, pp. 599–610.\n[37] Snoek, J., Larochelle, H., and Adams, R. P. Practical bayesian optimization\nof machine learning algorithms. In Advances in neural information processing\nsystems (2012), pp. 2951–2959.\n[38] Vlachos, M., Kollios, G., and Gunopulos, D. Discovering similar multidi-\nmensional trajectories. In Data Engineering, 2002. Proceedings. 18th International\nConference on (2002), IEEE, pp. 673–684.\n[39] Wu, Y.-L., Agrawal, D., and El Abbadi, A. A comparison of dft and dwt based\nsimilarity search in time-series databases. In Proceedings of the ninth international\nconference on Information and knowledge management (2000), ACM, pp. 488–495.\n[40] Yi, B.-K., Jagadish, H., and Faloutsos, C. Efficient retrieval of similar time\nsequences under time warping. In Data Engineering, 1998. Proceedings., 14th\nInternational Conference on (1998), IEEE, pp. 201–208.\n[41] Zheng, Y., Liu, Q., Chen, E., Zhao, J. L., He, L., and Lv, G. Convolutional\nnonlinear neighbourhood components analysis for time series classification. In\nPacific-Asia Conference on Knowledge Discovery and Data Mining (2015), Springer,\npp. 534–546.\n[42] Zhu, Y., and Shasha, D. Statstream: Statistical monitoring of thousands of data\nstreams in real time. In VLDB’02: Proceedings of the 28th International Conference\non Very Large Databases (2002), Elsevier, pp. 358–369.",
  "textLength": 48678
}