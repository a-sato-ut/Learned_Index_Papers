{
  "paperId": "c9139c1ed0c6483a6eabac764fae82e1d588f4bb",
  "title": "Differentially Private Learned Indexes",
  "pdfPath": "c9139c1ed0c6483a6eabac764fae82e1d588f4bb.pdf",
  "text": "DIFFERENTIALLY PRIVATE LEARNED INDEXES\nJianzhang Duâˆ—\nIndiana University\ndu5@iu.eduTilak Mudgalâˆ—& Rutvi Rahul Gadreâˆ—\nUMass Dartmouth\n{tmudgal,rgadre }@umassd.edu\nYukui Luo\nBinghamton University\nyluo11@binghamton.eduChenghong Wang\nIndiana University\ncw166@iu.edu\nABSTRACT\nIn this paper, we address the problem of efficiently answering predicate queries on\nencrypted databasesâ€”those secured by Trusted Execution Environments (TEEs),\nwhich enable untrusted providers to process encrypted user data without reveal-\ning its contents. A common strategy in modern databases to accelerate predicate\nqueries is the use of indexes, which map attribute values (keys) to their corre-\nsponding positions in a sorted data array. This allows for fast lookup and retrieval\nof data subsets that satisfy specific predicates. Unfortunately, indexes cannot be\ndirectly applied to encrypted databases due to strong data-dependent leakages.\nRecent approaches apply differential privacy (DP) to construct noisy indexes that\nenable faster access to encrypted data while maintaining provable privacy guar-\nantees. However, these methods often suffer from large storage costs, with index\nsizes typically scaling linearly with the key space. To address this challenge, we\npropose leveraging learned indexesâ€”a trending technique that repurposes ma-\nchine learning models as indexing structuresâ€”to build more compact DP indexes.\nOur contributions are threefold: (i) We first propose a strawman method that di-\nrectly applies DP training techniques to classical learned indexes. We then show\nthat this general paradigm can result in significant utility concerns, with an error\nno better than existing DP indexes. (ii) We then introduce a completely differ-\nent paradigmâ€”learning a private index structure on already distorted noisy key-\nposition mappings. Specifically, we propose a range tree-based private mechanism\nto generate distorted key-position mappings, followed by error-bounded piece-\nwise linear regression (PLR) models to learn a compact representation of the noisy\nmapping. (iii) We provide a formal analysis of our DP-PLR based learned indexes.\nResults show that our DP-PLR can achieve near-lossless processing with bounded\noverhead. Additionally, DP-PLR significantly reduces index size, from the linear\nkey-size scaling of traditional methods to potentially constant size.\n1 I NTRODUCTION\nOver the past decade, there has been a significant increase in the use of cloud computing for data\nstorage and analysis. Its low cost, high availability, scalability, and ease of use make it an appealing\noption for businesses and scientific research. However, organizations that handle sensitive data, such\nas hospitals, banks, government agencies, and energy companies, may hesitate to use cloud services\ndue to privacy concerns. The shared nature of cloud resources, coupled with potential vulnerabilities\nin the privileged software stack, has already led to various privacy breaches (Security, 2024). As\na result, there is a critical need for robust measures to safeguard data-in-use privacy in the cloud\nenvironment. This is essential not only for policy compliance (HIPAA, 2003; GDPR, 2017), but\nalso for maintaining public trust and advancing national priorities (The White House, 2022)\nThis need has given rise to a long line of research in an area known as Encrypted Databases\n(EDBs)(Eskandarian & Zaharia, 2017; Wang et al., 2021; Qiu et al., 2023), which enable untrusted\ncloud providers to manage and process encrypted user data. To achieve this, EDBs leverage Trusted\nâˆ—Co-first author, equal contribution.\n1arXiv:2410.21164v1  [cs.DB]  28 Oct 2024\n\nExecution Environments (TEEs)(Costan & Devadas, 2016) to establish secure hardware enclaves\non cloud machines, ensuring that any execution within these enclaves remains strongly isolated\nfrom the rest of the software stack, including the privileged OS and hypervisors. Usersâ€™ data is\nonly decrypted and processed inside these enclaves, and remains encrypted and integrity-protected\nwhenever it leaves the enclave. Despite the strong encryption and isolation provided by TEEs, re-\nsearchers have identified various side-channel threats associated with TEEs, leading to significant\nreal-world data breaches (Kocher et al., 2020). For example, different query processing can result\nin distinguishable memory access patterns and read/write volumes, which attackers can exploit to\nreconstruct substantial portions of the data (Kellaris et al., 2016), even if it is placed inside a TEE\nor encrypted elsewhere. As a result, modern EDBs combine TEEs with oblivious algorithms, which\nimplement branchless processing methods and pad the complexity to the worst-case maximum to\nensure complete data independence.\nWhile oblivious algorithms provide strong and provable privacy guarantees for todayâ€™s EDBs, they\nclash with modern database optimization techniques, which often rely on leveraging data-dependent\npatterns for fine-grained performance improvements. A prime example is the use of indexes, which\nmap attribute values (or keys) to their positions in a sorted array. Indexes enable rapid access to\nspecific data subsets, reducing the need for frequent full table scans and minimizing excessive I/Os.\nUnfortunately, this promising technique is not directly compatible with EDBâ€™s privacy guarantees\nas they can leak exact information about the data distribution. As a result, EDBs must sequentially\nload all encrypted data into the TEE for every query processing, even when only a small portion is\nneeded. To bridge this gap, Sahin et al. (2018) and Chowdhury et al. (2019) independently intro-\nduced DP indexes, which distort key-position mappings with DP noise to privately index encrypted\ndata. For instance, Sahin et al. (2018) use a B+tree-based index, where (pointers to) private data\nare stored at leaf nodes, each managing data with the same attribute value. They then apply DP\nby randomly adding or removing tuples at leaf nodes to create plausible deniability. Chowdhury\net al. (2019) propose a more storage-efficient design without dummy data, which directly distorts\nindex endpoints. For example, if the true index range for an attribute value is D[v0, v1), the DP\nindex generates D[Ëœv0,Ëœv1), where each endpoint Ëœv0andËœv1is distorted by independent DP noise.\nAlthough D[Ëœv0,Ëœv1)may include irrelevant data, a TEE can filter it inside the enclave. However, the\nsymmetric DP noise can still cause data loss when D[v0, v1)\\D[Ëœv0,Ëœv1)Ì¸=âˆ…. To address this, Wang\net al. (2024) use one-sided DP noise to ensure lossless indexing by enforcing Ëœv1> v1andËœv0< v0.\nNevertheless, all these DP indexes share a common limitation: storage costs typically O(N), where\nNis the number of keys. For instance, a B+tree requires at least 2Nâˆ’1nodes, and methods\nlike (Roy et al., 2020; Wang et al., 2024) will store at least one noisy endpoint per attribute value.\nKraska et al. (2018) argue that indexes are inherently models, sparking a growing research area\nknown as learned indexes (Wu et al., 2024), which repurpose machine learning (ML) models as\ndatabase indexes. Recent advancements in learned indexes have demonstrated their powerful storage\nefficiency, showing that even with a limited number of models, accurate key-position mappings can\nbe approximated. When models are simple, such as linear models, the total storage for learned\nindexes can be reduced to constant sizes. This leads to the fundamental question of this work:\nCan we leverage learned index techniques to build new DP indexes for EDBs that are both\nprovably private and compact in storage size?\nTo address this question, we initiate the first study on designing DP learned indexes. Our major\ncontributions are as follows: (i) We begin by proposing a strawman method that directly applies\nexisting DP training techniques to the training of classical learned indexes. Nevertheless, we show\nthat this general paradigm can lead to significant utility concerns due to the large sensitivity in\ngradients. We provide an empirical risk lower bound for this method, demonstrating that it per-\nforms no better than existing DP indexes. (ii) To achieve practical DP learned indexes with strong\nutility, we advocate for a completely different paradigmâ€”applying standard training on distorted\nkey-position mappings. To accomplish this, we propose a range tree-based mechanism that gener-\nates noisy key-position mappings, with privacy-induced error bounded by O(log3/2N). We then\napply error-bounded piece-wise linear regression (PLR) to learn a compact representation of these\nnoisy mappings. (iii) We provide a formal analysis of our proposed DP-PLR method and compare\nits guarantees with existing DP indexes. Results show that the PLR yields index sizes in O(1)and\ncan, with high probability, achieve lossless indexing, while keeping the index lookup overhead (the\nnumber of irrelevant tuples indexed) bounded by O(log3/2N).\n2\n\n2 B ACKGROUND AND RELATED WORK\nGeneral notations for relational databases. We define database instances Das relational tables\nwith attributes attr (D), where each attribute Aâˆˆattr (D)has a discrete domain of dom(A) =\n{xi}N\ni=1. By default, we consider dom(A)is sorted by ascending order. We define the frequency\n(count) of an attribute value xofAinDasF(x, D, A ) =P\ntâˆˆDâˆ§t.A=x1. The frequencies for\nall attribute values of A, is defined as the histogram of AinD, denoted as H(D, A i) ={ck=\nF(xk, D, A i)}âˆ€xkâˆˆdom(Ai). For simplicity, we may omit the arguments DandAofF(Â·),H(Â·)if\ntheyâ€™re already defined. In this work, we focus on linear predicate queries, denoted as qÏ•(D), which\nretrieve tuples from Dsatisfying a predicate Ï•then compute aggregated statistics on the fetched\ndata. A predicate Ï•is a logical expression with conditions on attributes, formed using conjunctions\n(âˆ§) or disjunctions ( âˆ¨). Each condition is a logical comparison such as Ai=aorAj> b.\nEDB system model. We consider a standard EDB model (Zheng et al., 2017; Eskandarian & Za-\nharia, 2017) in a cloud environment with two entities (as shown in Figure 1): the service provider\n(SP), managing the cloud infrastructure including the TEE, and the data owner (DO), who securely\noutsources storage and processing of private data to the SP. In the standard EDB model, the SP is\nconsidered honest-but-curious (Paverd et al., 2014), meaning they follow the pre-defined EDB pro-\ntocol without deviation but may attempt to learn sensitive information about the ownerâ€™s data by\nobserving execution transcripts.\nTo initiate the EDB, the TEE first creates an enclave on SPâ€™s cloud machine, generates encryption\nkeys(sk,pk), keeps skinside the enclave, and sends pkto the owner. The DO encrypts their data\ntuple-wisely using pkalong with a result key Kr, then uploads the encrypted data and Krto the SP.\nWhen the DO issues a query, the TEE loads, decrypts, and processes the data, reencrypts the result\nusing decrypted Kr, and sends it back to the DO for decryption. Moreover, we assume the EDB\nemploys oblivious algorithms for data processing to prevent side-channel leakages. For predicate\nqueries, an example of the oblivious processing algorithm is as follows: (i) Upon receiving the\nquery, the TEE sequentially reads the entire dataset into the enclave and performs a linear scan,\nlabeling each tuple as a â€matchâ€ or â€non-matchâ€ based on the predicate. To maintain obliviousness,\nthe label is updated for every tuple, regardless of whether it matches the predicate; (ii) The TEE then\nmakes a second pass over the labeled data to compute the desired statistics as specified by the query.\nSimilarly, during the statistics computation, every tuple is accessed, including non-matching ones,\nfor which a dummy read is applied to maintain obliviousness.\nData OwnerService Provider\nAttestation\npk\npk\nDatabase& ð¾ð‘Ÿ\nUntrusted Memory \npk\nDatabase& ð¾ð‘ŸTEE (pk, sk)\nsk\nð¾ð‘Ÿð¾ð‘Ÿ\nQueries\nð¾ð‘Ÿ\nFigure 1: EDB workflow\n0 20 40 60 80 100\nAttribute Values020040060080010001200Sorted Array Offset (CF)Raw CF (true indexes)\nCrypt CF (DP indexes)\nSPECIAL CF (lossless DP indexes)Figure 2: CF model indexes\n(a) RMI  architecture (b) FMI (our approach ) architectureModel 1.1\nModel 2.2 Model 2.1 Model 2.3\nModel 3.2 Model 3.1 Model 3.3......Key\nPositionStage 1\nStage 3Stage 2 KeyRoot Model ( ð‘“0)\nExpert Model 1 ( ð‘“1)\n...\nExpert Model M ( ð‘“ð‘€)\nConcatenation SoftMax\nMultiplicationPosition...\n...Figure 3: RMI example\nIndex model and learned indexes. Given Dsorted by A, and an attributed value xiâˆˆdom(A) =\n{x1, x2, . . . , x n}, a index structure maps xito an interval [v0, v1)such that D[v0, v1)contains all\ntuples tâˆˆDthatt.A=xi. Kraska et al. (2018) suggest that indexes can essentially be abstracted as\na cumulative frequency (CF) model. For instance, the lower and upper bound of [v0, v1)mentioned\nabove can actually computed by the CFs of v0=CF(xiâˆ’1) =Piâˆ’1\nj=0F(xj), and v1=CF(xi)1.\nTo better show this, we provide a visual example in Figure 3. Kraska et al. (2018) argue that tra-\nditional indexes (e.g., B+trees) are inherently data structures that compute or approximate a CF\ncurve (CFC), which is an ordered collection {xi, yi}N\ni=1, where yi=CF(xi). They propose that\nthese classical data structures can be replaced by ML models, namely learned indexes, which learn\nto approximate the CFC. A well-known example of a learned index is the Recursive Model Index\n(RMI) Kraska et al. (2018). The RMI mimics the structure of a B+tree but replaces its internal\nnodes with models. For a lookup operation, the root model predicts which child model should han-\n1With out loss of generality, we implicitly assume F(x0) = 0 .\n3\n\ndle the request, and this process continues through successive stages until a leaf model is reached.\nThe final (leaf) model then estimates the keyâ€™s position by fitting it to the CFC (Figure 3).\nDifferential privacy. DP is a well-established privacy framework that is rooted from the property\nof algorithm stability. Specifically, a randomnized mechanism Mis said to satisfies (Ïµ, Î´)-DP if for\nany pair of neighboring databases DandDâ€², differing by at most one tuple, and for all âˆ€OâŠ‚ O ,\nwhere Odenotes all possible outputs, the following holds\nPr[M(D)âˆˆO]â‰¤eÏµPr[M(Dâ€²)âˆˆO] +Î´\nThis definition ensures that the probability of producing a specific output does not change signifi-\ncantly (up to a multiplicative factor of eÏµ) when any single tuple in the dataset is modified. The slack\nÎ´introduces a practical relaxation, allowing the privacy guarantee to fail with probability at most Î´.\nNote that when Î´= 0, then the above definition is commonly referred as (Ïµ)-DP or pure DP.\nDP indexes. Two independent works Sahin et al. (2018); Chowdhury et al. (2019) introduced the\nconcept of DP indexes to enable EDBs to use indexing techniques. Specifically, Sahin et al. (2018)\nuse a variant B+to implement DP indexes, where a binary tree routes keys to a leaf that stores\n(pointers to) all records with a specific attribute value. To ensure DP, they adjust the leaf data by\nadding (positive noise) or removing (negative noise) tuples based on randomly sampled DP noise.\nTo prevent important records from being removed due to negative noise, a pessimistic amount of\ndummy tuples is added to each leaf data before the DP adjustment. To answer predicate queries,\nthe entire tree is searched, and all leaf data, including dummy ones, within the predicate range is\nreturned. While this method ensures basic DP guarantees, it results in significant storage costs due\nto the large amount of dummy data involved.\nChowdhury et al. (2019) proposed the Crypt Ïµthat do not require dummy data. Specifically, they\nconsider to directly release a noisy CFC {xi,Ëœyi}N\ni=1such that each Ëœyiâ†CF(xi) +Lap(N\nÏµ)is\ndistorted by independent Laplace noise. For processing queries, the EDB first obliviously sorts the\nentire data by the attribute, then uses the noisy CF to index tuples. We show an example of Crypt Ïµ\nin Figure 3. While Crypt Ïµindexes avoid adding dummy data, they suffer from significant data loss.\nThe noisy index range, distorted by DP, may be smaller than the true range, potentially excluding a\nlarge volume of matching tuples.\nWang et al. (2024) improves upon Crypt Ïµby introducing SPECIAL indexes. Specifically, they create\ntwo noisy CFCs: one that overestimates the true CFC with one-sided positive Laplace noise noise\nand another that underestimates it with negative noise. To answer index lookup, the lower end\npoint is taken from the underestimated curve, and the upper end point from the overestimated curve,\nensuring all matching data is included. However, this approach can result in significant overhead, as\nthe noisy index range can be much larger than the true range, leading to substantial I/O costs. We\nshown an example of this in Figure 3. To our knowledge, SPECIAL index is the only project that\nguarantees deterministic accuracy (no missing data).\n3 DP L EARNED INDEXES\nIn this section, we present the technical details of our proposed DP learned indexes. Before delving\ninto the specifics, we first formulate the concrete problem to be addressed.\nProblem formulation. We consider the problem of private training and inference of learned indexes\non static data. Formally, given Dsorted by Aâˆˆattr (D), and{(xi, yi)}N\ni=1to be the CFC of A\noverD. We consider an idealized index to be I DX(xi) = [yiâˆ’1, yi)for all xi, and our goal is to build\na private learned index model, PI DX, such that PI DX(xi) = [Ë†y(xiâˆ’1),Ë†y(xi)), where Ë†y(xi)denotes\nthe predicted position of xi. In addition, we consider PI DXshould satisfies the following properties:\nâ€¢P-1. (Ïµ, Î´)-DP at the tuple level. Given Ïµ >0, and 0â‰¤Î´ <1. For any neighboring\ndatabases DandDâ€², differing by a single tuple and both sorted by the same attribute Ai,\nwhere dom(Ai)âˆˆD=dom(Ai)âˆˆDâ€², it holds for all outputs OâŠ‚ O that\nPr[PIDXDâˆˆO]â‰¤eÏµÂ·Pr[PIDXDâ€²âˆˆO] +Î´.\nThis notion describes privacy at the tuple level, for instance the information related to the\npresence of each individual tuple by observing the index outcome, is bounded by DP. We\n4\n\ndo not consider key privacy, for example, an attacker might know whether a specific key\nxjis included in dom(Ai)âˆˆD. This aligns with the privacy guarantees of existing DP\nindex approaches (Roy et al., 2020; Wang et al., 2024) and the logic of traditional indexes,\nwhere a lookup should abort if a key is not present. Note that tuple-level privacy can also be\nextended to user-level privacy. If a user owns multiple tuples in the dataset and we wish to\npreserve privacy at the user level, we can achieve this by applying the group privacy (Dwork\n& Rothblum, 2016) mechanism with appropriately adjusted privacy parameters.\nâ€¢P-2. Practical utility. We require PI DXshould offer practical utility guarantees, including\nhigh accuracy (e.g., for any xi,|IDX(xi)\\PIDX(xi)|is small), and low query overhead\n(âˆ€xi,|PIDX(xi)\\IDX(xi)|is small).\nâ€¢P-3. Storage efficient. One key motivation for exploring learned indexes is the poten-\ntial to use lightweight ML models instead of large data structures. Therefore, we believe\nDP indexes should retain this advantage, offering more storage efficiency than traditional\nmethods, such as noisy CF-based classical DP indexes.\n3.1 N AÂ¨IVE DESIGNS\nA straightforward approach is to directly apply private training methods, such as DP-SGD, to RMIs.\nHowever, we notice that the hierarchical training process considered by RMIs can cause significant\nutility loss when paired with DP-SGD. Specifically, consider an RMI with â„“stages, comprising\none root model and Mâ„“sub-models at each stage â„“. We use f0(x;Î¸0)to denote the root model,\nwhere Î¸0are its parameters. A sub-model at stage â„“, indexed by k, is denoted by f(k)\nâ„“(x;Î¸(k)\nâ„“), with\ncorresponding parameters Î¸(k)\nâ„“. The training of the RMI proceeds sequentially, stage by stage. A\nmodel in stage â„“cannot begin training until all models in stage â„“âˆ’1have been trained. Each model\nat stage â„“is trained on a subset of data that is routed by the models from the previous stage â„“âˆ’1.\nFor example, the sub-model f(1)\nâ„“at stage â„“will only receive the training data where the model in\nstage â„“âˆ’1predicts a value that routes the data to sub-model k= 1. Formally, the training data for\nsub-model f(k)\nâ„“is given by:\nCF(k)\nâ„“=\u001a\n(xi, yi)âˆˆCF\f\f\f\f\u0016Mâ„“fâ„“âˆ’1(xi;Î¸â„“âˆ’1)\nN\u0017\n=k\u001b\n.\nIn this context, fâ„“âˆ’1(xi;Î¸â„“âˆ’1)refers to the prediction from the previous stage and determines which\nsub-model f(k)\nâ„“will be responsible for training on the given data point (xi, yi). To incorporate\nDP-SGD into the aforementioned training process, independent DP noise must be added at each\nstage, however, this can potentially lead to significant error accumulation in the final-stage models.\nMoreover, tracking and managing the privacy budget across different stages becomes challenging.\nTo address the challenge that traditional RMI structures can be hard to compatible with DP-SGD, we\ndraw inspiration from the Mixture of Experts framework to introduce FMIs. Unlike the hierarchical\nstructure of classical RMI, which necessitates independent training of each sub-model, the FMI\nconsists of multiple models that can be trained at the same time, which significantly simplifies\nintegration with DP-SGDâ€”as one can derive a unified gradient during training to which DP noise\ncan be applied. Specifically, we consider a FMI consists of a root model f0(x;Î¸0)andMexpert-\nmodels {fk(x;Î¸k)}M\nk=1operating at the same level. Given a key xiâˆˆCF={(xi, yi)}N\ni=1, the\nindex prediction of FMI is then given by\nPIDX(xi) = (Ë†y(xiâˆ’1),Ë†y(xi))s.t.Ë†y(x) =MX\nk=1wk(x;Î¸0)Â·fk(x;Î¸k),\nwherew(x;Î¸0) = [ w1(x;Î¸0), . . . , w M(x;Î¸0)]denotes the weight vector generated by the root\nmodel. We say that FMI can be logically similar to RMI: FMI can be logically formed as a two\nstaged RMI, but instead of picking one model for prediction, multiple last stage models collabora-\ntively predict the output for every key, with a root model f0(x;Î¸0)adjusting the contributions of\nthese predictions through a weight vector w(xi;Î¸0). Notably, if we generalize the root model to\noutput a one-hot-encoded weight vector, the FMI effectively reduces to a two-stage RMI model.\nTo enable simultaneous training of all components, the model parameters are concatenated into a\nunified parameter vector Î˜ = [ Î¸0;Î¸1;. . .;Î¸M]. During each training iteration, we sample a mini-\nbatch BâŠ‚D, where the per-example unified gradient is computed as\ngi=âˆ‡Î˜â„“i(Î˜) = [ âˆ‡Î¸0â„“i(Î˜),âˆ‡Î¸1â„“i(Î˜), . . .âˆ‡Î¸Mâ„“i(Î˜)]âŠ¤,\n5\n\nIn this way, a unified per-example gradient vector gican be obtained. Next, we apply DP-SGD to\nupdate the overall model parameters, and using the obtained unified gradients, shown as follows:\nÎ˜â†Î˜âˆ’Î·ï£«\nï£­1\n|B|ï£«\nï£­X\niâˆˆBgi\nmax\u0010\n1,âˆ¥giâˆ¥2\nC\u0011+N\u0000\n0, Ïƒ2|B|2C2I\u0001ï£¶\nï£¸ï£¶\nï£¸,\nThe above process can be summarized as follows: At each iteration, we sample a mini-batch BâŠ‚\n{(xi, yi)}N\ni=1for training. For each example in the mini-batch, the per-example gradient giis first\nclipped to a maximum â„“2-norm of Cto bound the sensitivity. After that, the clipped gradients\nare averaged and perturbed with Gaussian noise N(0, Ïƒ2|B|2C2I), where Iis the identity matrix\ncorresponding to the dimensionality of Î˜, and Ïƒ >0is the noise multiplier. Finally, the DP model\nupdate is applied to the overall model parameters with a learning rate Î·.\nAlthough FMI enables seamless integration with DP-SGD, a key limitation lies in the high sensitivity\nof gradients. Specifically, altering a single tuple in the dataset Dcan affect all entries in {xi, yi}N\ni=1.\nThis, in turn, may cause every per-example gradient in a mini-batch Bto change, resulting in a\nsensitivity of |B|. Consequently, Gaussian noise must be scaled by the batch size, which signifi-\ncantly reduces the utility of DP-SGD and can lead to poorly trained models. For example, applying\nthe excess risk lower bounds of DP-SGD (Bassily et al., 2014), and accounting for the Gaussian\nnoise multiplier Ïƒbeing scaled by |B|, the excess risk is lower bounded by â„¦\u0010\np\nN(Ïµ/|B|)2\u0011\neven\nfor strongly convex functions, where pis the dimensionality of the model. Furthermore, since the\nbatch size |B|is typically proportional to the sample size, i.e., |B|=O(N), the error can be lower\nbounded by â„¦\u0010\nNp\nÏµ2\u0011\n. In contrast, classical DP indexes, such as Crypt Ïµ, bound the privacy-induced\nerror with high probability at O\u0010âˆš\nN\nÏµ\u0011\n(see Roy et al. (2020) or our formal analysis in Table 1). As\nsuch, the utility of DP-SGD learned indexes would have a lower bound that is asymptotically worse\nthan that of traditional DP indexes.\n3.2 P ROPOSED METHOD\nTo prevent large utility loss, we advocate a completely different paradigm for building DP learned\nindexes: instead of training on the original CFCs, we train on CFCs that have already been distorted\nby DP mechanisms. Since the training is performed as post-processing on the DP-distorted CFCs,\nno additional noise is required. However, two key challenges remain. First, the high sensitivity of\nCFCs can result in significant noise being introduced when releasing the noisy CFCs, leading to\nsubstantial utility loss in the final learned indexes, even if the training is noise-free. Second, model\nselection can be crucial, especially when one of our primary goals is achieving storage efficiency\n(P-3). Hence, we must carefully choose models that can learn a compact representation of the noisy\nCFC while maintain high utility. Below, we discuss our approaches to address these challenges.\nFigure 4: Range tree and DP range tree.\nRange tree based noisy CFC generation. To address the first challenge, we drew inspiration\nfrom prior work on the continual release of time-series data (Chan et al., 2011; Wang et al., 2023)\nand propose a similar range tree-based approach for deriving noisy CFCs. This method allow us to\nbound the DP induced errors within O((log N)3/2). At a high level, we construct a binary range tree\nover the sorted attribute domain dom(A) ={xi}N\ni=1. Each leaf node represents a single attribute\nvalue xiâˆˆdom(A)and stores the frequency of xi. Each internal node represents the combined\nattribute range of its two child nodes and stores the frequency of records whose attribute Afalls\nwithin that range. The tree construction proceeds upward recursively until we reach the root node,\n6\n\nwhich represents the entire attribute range [x1, xN]. We show an example in Figure 4. Next, we\nconvert this tree into a DP range tree by adding Laplace noise to each frequency count stored in\nevery node. Since tuples with a certain attribute value contribute to at most logNfrequency counts\nin the tree, thus by the parallel composition theorem of DP (Dwork et al., 2014), the per-instance\nDP noise can be set as Lap(logN\nÏµ). This ensures that after adding noise to all frequency counts, the\nentire mechanism still satisfies Ïµ-DP. Note that each Ëœyican be derived using at most logNnoisy\nfrequencies from the DP range tree, with each acting as a partial sum (p-sum) contributing to the\nfinal value of Ëœyi. We show an example of this approach in Figure 4 (right).\nTheorem 1. For any CFC {xi, yi}N\ni=1, and let {xi,Ëœyi}N\ni=1to be the corresponding noisy CFC\nreleased by the range tree mechanism. Then, âˆ€yi, the error |Ëœyiâˆ’yi|is bounded by O(Ïµâˆ’1(logN)3\n2).\nThe privacy-induced error in the range-tree mechanism is determined by the combined effect of\nLaplace noise. Since each Ëœyiis computed using at most logNnoisy counts, each perturbed by\nLap(logN\nÏµ)noise, hence, Theorem 1 can be proven by applying the bound on the sum of Laplace\nrandom variables (Dwork et al., 2014; Wang et al., 2021). In algorithm 1, we outline the formal\nprocedure for implementing the range-tree based noisy CFC releasing, including additional opti-\nmizations for dynamically recycling unused noisy frequencies.\nAlgorithm 1 Optimized noisy range tree mechanism for generating noisy CFC\nInput :Dsorted by A; Privacy parameter Ïµ >0.\n1:Compute histogram H(D, A) ={ci=F(xi, D, A )}N\ni=1, and{xi}N\ni=1=dom(A)\n2:Set p-sum vectors {Î±i= 0}N\ni=1,{ËœÎ±i= 0}N\ni=1, and per instance privacy budget Ïµâ€²=Ïµ/log2N.\n3:foreacht= 1,2, . . . , N do\n4: Write tin binary form t=P\nj2jbinj(t), where binj(t)âˆˆ {0,1}.\n5: Leti= min {j:Binj(t)Ì¸= 0}, update the p-sum Î±i=P\nj<iÎ±j+ct\n6: forj < i doresetÎ±j= 0andËœÎ±j= 0 â–·Dynamic recycle unused p-sums.\n7: Generate noisy p-sum: ËœÎ±i=Î±i+Lap\u00001\nÏµâ€²\u0001\n.\n8: Ëœyt=P\nj:Binj(t)=1ËœÎ±j â–·Find noisy p-sums for covering Ëœyt.\n9:return {(xi,Ëœyi)}N\ni=1.\nPiece-wise linear regression (PLR) on noisy CFCs. Once we have the noisy CFC, the next step is\nto find proper model to learn a compact representation of this CFC. Specifically, we select PLR mod-\nels for this task, for two primary reasons. First, as the CFC is monotonic and low-dimensional (2d\ninteger data), a PLR model can straightforward fit the trend without overcomplicating the represen-\ntation. This allows us to represent the CFC with a minimal number of parameters, ensuring model\nsimplicity and storage efficiency (P-3). Second, recent advancements in error-bounded PLR (Xie\net al., 2014) allow one to approximate each data point in a given dataset within a fixed error bound\nÏ„. This enables us to enforce a utility goal by controlling the approximation accuracy. Furthermore,\none can also manage the model complexity by adjusting the error bound Ï„; smaller values of Ï„\ntypically result in fewer segments and, consequently, a smaller model size. This flexibility allows\nus to balance between utility (P-2) and storage efficiency (P-3) goals, depending on the specific\napplication requirements. We summarize key steps for applying PLR on noisy CFC in Algorithm 2.\nAlgorithm 2 Linear model learned indexes from noisy CFCs\nInput : A sequence of noisy CFC {(xi,Ëœyi)}N\ni=1.\n1:{xi,Ëœyi}N\ni=1â†Isotonic Regression ({xi,Ëœyi}N\ni=1) â–·monotonic increasing.\n2:fori= 1,2, . . . , N doËœyi= min(max(0 ,Ëœyi),|D|) â–·clipping.\n3:(seg= (v1, v2, ..., v k=xN), f={fi}k\ni=1)â†ErrBounded PLR({xi,Ëœyi}N\ni=1, Ï„)\n4:return (seg, f)\nIn general, we first apply constrained post-processing to the released noisy CFC to ensure that the\nnoisy CFC adheres to its meaningful context. Specifically, this includes applying isotonic regression\nto enforce the monotonic increasing property of the CFC, followed by a clipping to ensure that\nall CF values fall within the valid range (0,|D|]. Next, we adopt PLR on the processed CFC,\n7\n\nwhich segments the domain dom(Ai)into a limited number of disjoint intervals [vj, vj+1), where\nj= 1,2, . . . , M andMis the total number of segments. For each segment j, an individual linear\nmodel fj(x) =ajx+bjis learned to approximate the subset of the CFC within the interval [vj, vj+1)\nby minimizing the squared error between the modelâ€™s predictions and the noisy CFC values. Finally,\nwe output the overall partition, seg= (v1, v2, . . . , v M), as well as the set of linear models for each\nsegment, f={fi}M\ni=1. Given any input xiâˆˆdom(A), to make predictions, the PLR model outputs\nf(xi) =fj(xi)forxiâˆˆ[vj, vj+1),where j < M.\nNearly lossless index lookup. We now show how to make index lookup using the learned PLR.\nA straightforward approach could be directly use the PLRâ€™s predicted position for constructing in-\ndexing ranges, for instance, for any xiâˆˆdom(A), we output PI DX(xi) = [f(xiâˆ’1), f(xi)). Note\nthat due to the potential noises from both DP CFC generation and the PLR learning process, it is\npossible that [yiâˆ’1, yi)\\[f(xiâˆ’1), f(xi))Ì¸=âˆ…and thus leading to a set of matching data to be lost if\nwe follow PI DX(xi)to fetch data. To address this issue, we propose a pessimistic indexing method\nsuch that with high probability, it holds that [yiâˆ’1, yi)âŠ†PIDX(xi). Specifically, we first compute\nthe inference errors, ei=|f(xi)âˆ’Ëœyi|of PLR on noisy CFC, for all xiâˆˆdom(A), and subsequently\ndetermine the maximum error, emax= arg max {1,...,N}(ei). When using error-bounded PLR, one\ncan approximate emaxdirectly by the error bound Ï„without additional computation. However, in\npractice, the empirical emaxcan be much smaller than Ï„(Xie et al., 2014), so tha it is important to\ncompute the exact emax. Then, for any index lookup, say with key xi, we compute a pessimistic\noverestimated indexing range as\nPIDX(xi) = [min (0 , f(xiâˆ’1)âˆ’emaxâˆ’Z),max(|D|, f(xi) +emax+Z))\nwhere Z=Î±sÏµâˆ’1(logN)3/2where Î±sâ‰¥1. Since Zis proportional to the noisy CFCâ€™s error\nbound, and the inference error of PLR (on noisy CFC) is bounded by emax. Hence, when Î±sis set\nto properly large, then it holds PI DXcovers [yiâˆ’1, yi)with high probability, which implies strong\nlossless indexing guarantees. Moreover, as emax is computed entirely as a post-processing step\non the noisy CFC, and Zis determined by public parameters. As a result, this process does not\nintroduce any privacy loss.\n4 F ORMAL ANALYSIS\nIn this section, we provide a formal analysis of our proposed DP PLR indexes and compare them\nagainst the SOTA DP indexes. Our focus will be on their critical guarantees including privacy (P-1),\nutility (P-2) and storage efficiency (P-3).\nComparison baselines. We select three existing DP indexes for comparison: the DP B+tree (Sahin\net al., 2018), Crypt Ïµ(Chowdhury et al., 2019), and SPECIAL indexes (Wang et al., 2024). Since our\nfocus is on static data indexes, for fair comparison reasons, we exclude DP index variants that are\noptimized primarily for dynamic data (Zhang et al., 2023).\nGeneral Setting. We consider a sorted database Dby attribute A, with the true CFC represented\nas{xi, yi}N\ni=1. Our analysis evaluates four key metrics related to index lookups for answering\npredicate queries. Specifically, we focus on a predicate query that retrieves data matching a specific\nattribute value (e.g., point query). We note that the techniques presented here can be extended to\nother types of predicate queries, such as range queries or arbitrary conjunctive queries, suggesting\nthat our analysis of point queries is sufficient. Specifically, we consider the following metrics:\n(i)Query error , which represents the total number of missing data tuples for the index lookup;\n(ii)Query overhead , indicating the total number of irrelevant tuples indexed; (iii) Index storage ,\nwhich measures the storage required for the index structures; and (iv) Data overhead , reflecting the\nnumber of dummy tuples that must be inserted to the underlying data to support the proposed index.\nOur analysis will focus on the probabilistic upper bounds, say Î±, for each of the aforementioned\nmeasures, such that with probability at least 1âˆ’Î²(forÎ² >0), these measures do not exceed the\ncorresponding Î±.\n4.1 R ESULTS\nWe present our formal analysis results in Table 1, with the detailed derivation and computing steps\nof these upper bounds deferred to A.1 for brevity. In the following, we will focus on discussing key\nobservations from the formal analysis result.\n8\n\nTable 1: Formal comparison between DP-PLR and existing DP indexes\nDPB+Tree Crypt Ïµ SPECIAL DP-PLR (ours)\nQuery error O\u00102 logNln(2\nÎ²)\nÏµ\u0011\nâˆ’Bâ€¡O\u00102Nln(2\nÎ²)\nÏµ\u0011\n0 O \n(2 log N)3\n2q\nln(2\nÎ²)\nÏµ!\nâˆ’Î±s(logN)3\n2\nÏµ\nQuery overhead O\u00102 logNln(2\nÎ²)\nÏµ\u0011\n+B O\u00102Nln(2\nÎ²)\nÏµ\u0011\nO\u00104Nln(1\nÎ²)\nÏµ\u0011\n+Âµ O \n(2 log N)3\n2q\nln(2\nÎ²)\nÏµ!\n+ 2Î±s(logN)3\n2\nÏµ+ 2Ï„\nIndex size (bits) 64(2Nâˆ’1) 64N 128N 128MorO(1)when Mis small\nData overhead O \n2 logNq\nNln(2\nÎ²)\nÏµ!\n+NB 0 0 0\nPrivacy (Ïµ)-DP (Ïµ)-DP (Ïµ, Î´)-DP (Ïµ)-DP\nâ€¡.Bdenotes the overflow array size, which denotes the fixed number of extra dummy injected to each leaf node data (Sahin et al., 2018).\nObservation 1. Our DP-PLR offers significantly lower storage costs, potentially constant in\nsize, for the index structure compared to existing DP indexes, which typically require storage\nlinear to the key size N.The storage costs of existing DP indexes (all baselines) are linear in\nthe key size, O(N), which can be substantial for attributes with large domain sizes (e.g., salary,\nmortgage). In contrast, the storage cost of DP-PLR is determined by the number of PLR models,\nM. Notably, we can adjust the error bound Ï„during PLR training or enforce an upper limit on M\nto control the maximum number of models. Thus, it is reasonable to assume that Mâˆ¼O(1). This\ncompact storage size also results in faster lookup times. For example, given xi, the lookup time\nin DP-PLR is O(logM)âˆ¼O(1), assuming a binary search to locate the segment linear model for\npredicting xiâ€™s position. In contrast, the other three methods either require searching an array of size\nN(Crypt Ïµand SPECIAL) or traversing a tree with Nleaf nodes to locate xi, making their lookup\ntime no better than O(logN).\nObservation 2. DP-PLR is the only method that achieves (nearly) lossless indexing without\nrequiring the insertion of dummy data, while still maintaining pure Ïµ-DP guarantees. From the\nindexing error bound, we observe that when Î±sis properly set, for instance Î±sâ‰¥O(p\nln(Î²âˆ’1)), the\nprobability of achieving no indexing error is at least 1âˆ’Î²with DP-PLR. This near-lossless guarantee\n(with high probability) is possible because DP-PLR accurately estimates the error bounds for both\nthe noisy CFC and the PLR prediction, smoothing these errors with pessimistic overestimation.\nWhile the DP B+tree has similar potential if the overflow array size Bis set large enough, it requires\na significant amount of dummy data, at least proportional to O(âˆš\nNlogN). In contrast, DP-PLR\navoids the need to inject any dummy data. Moreover, while SPECIAL achieves a deterministic\nlossless guarantee, it uses one-sided Laplace noise, which limits it to providing only (Ïµ, Î´)-DP.\nObservation 3. DP-PLR shows significantly lower indexing overhead compared to Crypt Ïµand\nSPECIAL, and is only marginally larger than DP B+by an asymptotic factor of O(âˆšlogN).\nHowever, DP-PLR eliminates the need to inject any dummy data into the underlying data.\nWhen setting Î±s=O(p\nln (Î²âˆ’1))andBproportional to O(2Ïµâˆ’1logNln(Î²âˆ’1)), both DP B+\nand DP-PLR provide the same accuracy guarantee (i.e., no error with probability at least 1âˆ’Î²).\nWhen comparing their overhead, DP-PLR has a slightly larger asymptotic factor of O(âˆšlogN)\nthan DP B+. However, in terms of data storage overhead, DP B+isO(logNâˆš\nN)larger than DP-\nPLR. When compared to other noisy CFC-based DP indexes, such as Crypt Ïµand SPECIAL, these\nmethods have an overhead of at least O(N/log3/2N)relative to DP-PLR. This demonstrates that\nDP-PLR also minimizes overhead compared to state-of-the-art noisy CFC-based DP indexes.\n5 C ONCLUSION\nIn this work, we propose DP-PLR, the first differentially private (DP) learned index designed to\nefficiently process predicate queries on encrypted databases. Our formal analysis demonstrates that\nDP-PLR significantly reduces index storage costs, shifting from the traditional linear dependence\non key sizes to potentially constant size. Furthermore, to the best of our knowledge, DP-PLR is\nthe only approach that achieves (nearly) lossless indexing without requiring the insertion of dummy\ndata, while maintaining strict Ïµ-DP guarantees. Its query overhead also exhibits asymptotically better\nperformance compared to the current state-of-the-art lossless private index, SPECIAL. Additionally,\nwe identify several exciting opportunities for future optimization, such as enabling private and effi-\ncient updates to DP-PLR, supporting concurrent yet private lookups, and leveraging available public\ndata to fine-tune the index for improved accuracy and reduced query overhead.\n9\n\nREFERENCES\nRaef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient\nalgorithms and tight error bounds. In 2014 IEEE 55th annual symposium on foundations of\ncomputer science , pp. 464â€“473. IEEE, 2014.\nT-H Hubert Chan, Elaine Shi, and Dawn Song. Private and continual release of statistics. ACM\nTransactions on Information and System Security (TISSEC) , 14(3):1â€“24, 2011.\nAmrita Roy Chowdhury, Chenghong Wang, Xi He, Ashwin Machanavajjhala, and Somesh Jha.\nCrypte: Crypto-assisted differential privacy on untrusted servers. arXiv e-prints , pp. arXivâ€“1902,\n2019.\nVictor Costan and Srinivas Devadas. Intel sgx explained. Cryptology ePrint Archive , 2016.\nCynthia Dwork and Guy N Rothblum. Concentrated differential privacy. arXiv preprint\narXiv:1603.01887 , 2016.\nCynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations\nand TrendsÂ® in Theoretical Computer Science , 9(3â€“4):211â€“407, 2014.\nSaba Eskandarian and Matei Zaharia. Oblidb: Oblivious query processing for secure databases.\narXiv preprint arXiv:1710.00458 , 2017.\nGDPR. The eu general data protection regulation (gdpr). A Practical Guide, 1st Ed., Cham: Springer\nInternational Publishing , 10(3152676):10â€“5555, 2017.\nHIPAA. Summary of the hipaa privacy rule. Office for Civil Rights , 2003.\nGeorgios Kellaris, George Kollios, Kobbi Nissim, and Adam Oâ€™neill. Generic attacks on secure\noutsourced databases. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and\nCommunications Security , pp. 1329â€“1340, 2016.\nPaul Kocher, Jann Horn, Anders Fogh, Daniel Genkin, Daniel Gruss, Werner Haas, Mike Hamburg,\nMoritz Lipp, Stefan Mangard, Thomas Prescher, et al. Spectre attacks: Exploiting speculative\nexecution. Communications of the ACM , 63(7):93â€“101, 2020.\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 international conference on management of data ,\npp. 489â€“504, 2018.\nAndrew Paverd, Andrew Martin, and Ian Brown. Modelling and automatically analysing privacy\nproperties for honest-but-curious adversaries. Tech. Rep , 2014.\nLina Qiu, Georgios Kellaris, Nikos Mamoulis, Kobbi Nissim, and George Kollios. Doquet: Differ-\nentially oblivious range and join queries with private data structures. Proceedings of the VLDB\nEndowment , 16(13):4160â€“4173, 2023.\nAmrita Roy, Chenghong Wang, Xi He, Ashwin Machanavajjhala, and Somesh Jha. Crypt: Crypto-\nassisted differential privacy on untrusted servers. In Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data , pp. 603â€“619, 2020.\nCetin Sahin, Tristan Allard, Reza Akbarinia, Amr El Abbadi, and Esther Pacitti. A differentially\nprivate index for range query processing in clouds. In 2018 IEEE 34th International Conference\non Data Engineering (ICDE) , pp. 857â€“868. IEEE, 2018.\nIBM Security. Cost of a data breach report 2024. https://www.ibm.com/reports/\ndata-breach , 2024. Accessed: 2024-09-30.\nThe White House. National strategy to advance privacy-preserving data sharing and analytics,\n2022. URL https://www.whitehouse.gov/wp-content/uploads/2022/03/\nNational-Strategy-to-Advance-Privacy-Preserving-Data-Sharing-/\nand-Analytics-1.pdf .\n10\n\nChenghong Wang, Johes Bater, Kartik Nayak, and Ashwin Machanavajjhala. Dp-sync: Hiding\nupdate patterns in secure outsourced databases with differential privacy. In Proceedings of the\n2021 International Conference on Management of Data , pp. 1892â€“1905, 2021.\nChenghong Wang, Johes Bater, Kartik Nayak, and Ashwin Machanavajjhala. Incshrink: architecting\nefficient outsourced databases using incremental mpc and differential privacy. In Proceedings of\nthe 2022 International Conference on Management of Data , pp. 818â€“832, 2022.\nChenghong Wang, David Pujol, Kartik Nayak, and Ashwin Machanavajjhala. Private {Proof-of-\nStake}blockchains using {Differentially-Private }stake distortion. In 32nd USENIX Security\nSymposium (USENIX Security 23) , pp. 1577â€“1594, 2023.\nChenghong Wang, Lina Qiu, Johes Bater, and Yukui Luo. Special: Synopsis assisted secure collab-\norative analytics. arXiv preprint arXiv:2404.18388 , 2024.\nYang Wu, Xuanhe Zhou, Yong Zhang, and Guoliang Li. Automatic database index tuning: A survey.\nIEEE Transactions on Knowledge and Data Engineering , 2024.\nQing Xie, Chaoyi Pang, Xiaofang Zhou, Xiangliang Zhang, and Ke Deng. Maximum error-bounded\npiecewise linear representation for online stream approximation. The VLDB journal , 23:915â€“937,\n2014.\nYanping Zhang, Johes Bater, Kartik Nayak, and Ashwin Machanavajjhala. Longshot: Indexing\ngrowing databases using mpc and differential privacy. Proceedings of the VLDB Endowment , 16\n(8):2005â€“2018, 2023.\nWenting Zheng, Ankur Dave, Jethro G Beekman, Raluca Ada Popa, Joseph E Gonzalez, and Ion\nStoica. Opaque: An oblivious and encrypted distributed analytics platform. In 14th USENIX\nSymposium on Networked Systems Design and Implementation (NSDI 17) , pp. 283â€“298, 2017.\nA A PPENDIX\nA.1 F ORMAL ANALYSIS\nDPB+tree indexes (Sahin et al., 2018). Given an index lookup with attribute value k, the DP B+\nmethod will return (pointers to) all tuples linked to the kmatching leaf nodes. Since Laplace noise\nZin the scale of Zâˆ¼Lap(logN/Ïµ)is added to distort number of records to be returned. So that we\nknow that Zwill be the primary factor that causes query error (missing tuples due to negative values\nofZâˆ’B < 0) or overhead ( when Zâˆ’B > 0). Hence, the following analysis will be primarily\nbased on the upper bounds of Z. Since Zâˆ¼Lap(logN/Ïµ), and thus the probability density function\n(PDF) of ZisfZ(z) =1\n2bexp\u0010\nâˆ’|z|\nb\u0011\nforzâˆˆR. The tail probability of the Laplace distribution is\ngiven by P(|Z|> Î±) = exp\u0000\nâˆ’Î±\n2b\u0001\n. When setting exp\u0000\nâˆ’Î±\nb\u0001\nâ‰¤Î², and taking the natural logarithm\nof both sides, one can obtain Î±â‰¥ âˆ’bln(Î²). Substituting b=logN\nÏµgives:\nÎ±â‰¥logN\nÏµln\u00121\nÎ²\u0013\n.\nBy symmetry of the Laplace noises, we know that with probability at least Î²such that Z >\nlogN\nÏµln\u0010\n2\nÎ²\u0011\norZ <âˆ’logN\nÏµln\u0010\n2\nÎ²\u0011\n. So that we can derive the query error bound as max(0 ,âˆ’(Z+\nB)), which is O\u0012\nlogNln(2\nÎ²)\nÏµ\u0013\nâˆ’B, and similarly the overhead is O\u0012\nlogNln(2\nÎ²)\nÏµ\u0013\n+B.\nNext, we analyze the total storage overhead of the DP B+tree indexes. So in general, the total\noverhead is determined by Z1+...+ZN, where each Zidenotes the noise added to the ithleaf\nnode for populating the tree node storage. To derive the upper bounds, here we need the Lemma 10\nin Wang et al. (2022) (or Lemma 12.2 in Dwork et al. (2014)). So for completeness, we reproduce\nthe lemmas as below:\n11\n\nLemma 2. Given Mi.i.d. Laplace random variables, Z1, ..., Z N, where each Yiis sampled from\nLap(âˆ†\nÏµ). Let0< Î±â‰¤Nâˆ†\nÏµ, the following inequality holds\nPr\"NX\n1Ziâ‰¥Î±#\nâ‰¤e\u0010\nâˆ’Î±2Ïµ2\n4Nâˆ†2\u0011\nProof. The complete proof of Lemma 2 can be found in the Appendix C.1 of Wang et al. (2021) and\nin Dwork et al. (2014).\nBy setting eâˆ’Î±2Ïµ2\n4Mâˆ†2=Î², and when M > 4 ln1\nÎ²the following inequality holds\nPr\"NX\ni=1Ziâ‰¥2âˆ†\nÏµr\nNln1\nÎ²#\nâ‰¤Î²\nHence, we can derive the total storage overhead upper bound as O \n2 logNq\nNln2\nÎ²\nÏµ!\n+NB\nThe DP B+tree has a total of 2Nâˆ’1nodes, where each node needs to store at least an integer2.\nThus, the index storage requires 64(2Nâˆ’1)bits.\nCrypt Ïµindexes (Chowdhury et al., 2019). The Crypt Ïµindexes use the noisy CFC model, so they\ndirectly release {xi,Ëœyi}N\ni=1. Errors occur when Ëœyi< yi, determined by yiâˆ’Ëœyi=âˆ’Lap(N/Ïµ).\nUsing the same upper bound technique as the DP B+tree, both the query error and overhead of\nCrypt Ïµindexes are within O\u0012\n2Nln(2\nÎ²)\nÏµ\u0013\n. As Crypt Ïµstore only the noisy CFC, the index storage is\n64Nbits, and there is no data storage overhead, as it does not use dummy data.\nSPECIAL indexes (Wang et al., 2024). The SPECIAL indexes use the same noisy CFC model\nas Crypt Ïµ, but release two CFCs with one-sided Laplace noiseâ€”shifted to have mean Âµ > 0for\npositive (or Âµ <0for negative) values and truncated at 0. This method achieves (Ïµ, Î´)-DP instead\nofÏµ-DP. As SPECIAL consistently overestimate the indexing range, no query errors occur, but\nquery overhead may arise. Since the Laplace noises are with shifted means, and both CFC can\nlead to overheads. Since SPECIAL introduces two noisy curves, so that the overhead is bounded by\nO\u0012\n4Nln(2\nÎ²)\nÏµ\u0013\n+Âµ. The index storage is doubled compared to the Crypt Ïµwith128Nbits. Similarly,\nthere is no data storage overhead.\nDP-PLR Indexes (Our Method). In our DP-PLR indexes, we will first look into the privacy in-\nduced errors in the binary mechanism for releasing noisy CFCs. Since each noisy CFC data point\nis released by using at most logNnodes, where each node is distorted by Laplace noise in the\nscale of Lap(logN/Ïµ), so that by Lemma 2, we can conclude that the privacy induced error bound\nof binary mechanism (for estimating each CFC data points) is within O \n2 logNq\nlogNln2\nÎ²\nÏµ!\n, and\nfurther we can derive the query error bound as O \n2 logNq\nlogNln2\nÎ²\nÏµ!\nâˆ’Î±slog3/2N\nÏµ. Note that the\nPLR process has bounded estimation error in Ï„, and in the indexing, we use Ï„to carry out pes-\nsimistic overestimation, so that the query error bounds will not be affected but the overhead now\nbecomes O \n2 logNq\nlogNln2\nÎ²\nÏµ!\n+ 2Î±slog3/2N\nÏµ+ 2Ï„, consider the overestimation is applied to both\nend points. The index storage for PLR is bounded by 2Ã—64Ã—Mbits, where Mis the number of\nlinear models in the PLR. Similarly, DP-PLR does not use dummy data, resulting in no additional\ndata storage overhead.\n2We assume 64 bits for all integers and address spaces\n12",
  "textLength": 48520
}