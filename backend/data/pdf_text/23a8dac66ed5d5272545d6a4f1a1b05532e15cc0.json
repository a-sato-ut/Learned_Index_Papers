{
  "paperId": "23a8dac66ed5d5272545d6a4f1a1b05532e15cc0",
  "title": "Spatial Temporal Analysis of 40,000,000,000,000 Internet Darkspace Packets",
  "pdfPath": "23a8dac66ed5d5272545d6a4f1a1b05532e15cc0.pdf",
  "text": "Spatial Temporal Analysis of 40,000,000,000,000\nInternet Darkspace Packets\nJeremy Kepner1, Michael Jones1, Daniel Andersen2, Aydın Buluc ¸3, Chansup Byun1, K Claffy2, Timothy Davis4,\nWilliam Arcand1, Jonathan Bernays1, David Bestor1, William Bergeron1, Vijay Gadepally1,\nMicheal Houle1, Matthew Hubbell1, Anna Klein1, Chad Meiners1, Lauren Milechin1, Julie Mullen1,\nSandeep Pisharody1, Andrew Prout1, Albert Reuther1, Antonio Rosa1, Siddharth Samsi1,\nDoug Stetson1, Adam Tse1, Charles Yee1, Peter Michaleas1\n1MIT,2CAIDA,3LBNL,4Texas A&M\nAbstract —The Internet has never been more important to\nour society, and understanding the behavior of the Internet\nis essential. The Center for Applied Internet Data Analysis\n(CAIDA) Telescope observes a continuous stream of packets\nfrom an unsolicited darkspace representing 1/256 of the Internet.\nDuring 2019 and 2020 over 40,000,000,000,000 unique packets\nwere collected representing the largest ever assembled public\ncorpus of Internet trafﬁc. Using the combined resources of the\nSupercomputing Centers at UC San Diego, Lawrence Berkeley\nNational Laboratory, and MIT, the spatial temporal structure of\nanonymized source-destination pairs from the CAIDA Telescope\ndata has been analyzed with GraphBLAS hierarchical hyper-\nsparse matrices. These analyses provide unique insight on this\nunsolicited Internet darkspace trafﬁc with the discovery of many\npreviously unseen scaling relations. The data show a signiﬁcant\nsustained increase in unsolicited trafﬁc corresponding to the\nstart of the COVID19 pandemic, but relatively little change in\nthe underlying scaling relations associated with unique sources,\nsource fan-outs, unique links, destination fan-ins, and unique\ndestinations. This work provides a demonstration of the practical\nfeasibility and beneﬁt of the safe collection and analysis of\nsigniﬁcant quantities of anonymized Internet trafﬁc.\nIndex Terms —Internet modeling, packet capture, streaming\ngraphs, power-law networks, hypersparse matrices\nI. I NTRODUCTION\nFor over ﬁve billion people the Internet has become as\nessential as land, sea, air, and space for enabling activities as\ndiverse as commerce, education, health, and entertainment [1].\nUnderstanding the Internet is likewise as important as studying\nthese other domains [2]. Developing scientiﬁc insights on\nhow the Internet behaves requires observation and data [3]–\n[6]. The largest public Internet observatory is the Center\nfor Applied Internet Data Analysis (CAIDA) Telescope that\noperates a variety of sensors including a continuous stream\nof packets from an unsolicited darkspace representing 1/256\nThis material is based upon work supported by the Assistant Secretary of\nDefense for Research and Engineering under Air Force Contract No. FA8702-\n15-D-0001, National Science Foundation CCF-1533644, and United States\nAir Force Research Laboratory Cooperative Agreement Number FA8750-19-\n2-1000. Any opinions, ﬁndings, conclusions or recommendations expressed\nin this material are those of the author(s) and do not necessarily reﬂect the\nviews of the Assistant Secretary of Defense for Research and Engineering,\nthe National Science Foundation, or the United States Air Force. The U.S.\nGovernment is authorized to reproduce and distribute reprints for Government\npurposes notwithstanding any copyright notation herein.\n123456789101112\nMonth00.511.522.53Total Packets1012 CAIDA Telescope\n2020\n2019Fig. 1. Packets per month. Number of packets collected per month by the\nCAIDA telescope. The number of packets increased signiﬁcantly during the\nCOVID19 pandemic.\nof the Internet. This network telescope monitors an Internet\ndarkspace (also referred to as a black hole, Internet sink,\nor darknet) that is a globally routed /8 network that carries\nalmost no legitimate trafﬁc because there are few allocated\naddresses in this Internet preﬁx. After discarding the small\namount of legitimate trafﬁc from the incoming packets, the\nremaining data represent a continuous view of anomalous\nunsolicited trafﬁc, or Internet background radiation. Almost\nevery computer on the Internet will receive some form of\nthis background trafﬁc. This unsolicited trafﬁc results from\na wide range of events, such as backscatter from randomly\nspoofed source denial-of-service attacks, the automated spread\nof Internet worms and viruses, scanning of address space\nby attackers or malware looking for vulnerable targets, and\nvarious misconﬁgurations (e.g. mistyping an IP address).\nIn recent years, trafﬁc destined to darkspace has evolved\nto include longer-duration, low-intensity events intended to\nestablish and maintain botnets. CAIDA personnel maintainarXiv:2108.06653v1  [cs.NI]  15 Aug 2021\n\nand expand the telescope instrumentation, collects, curates,\narchives, and analyzes the data, and enables data access for\nvetted researchers.\nDuring 2019 and 2020 over 40,000,000,000,000 unique\npackets were collected by the CAIDA Telescope. This data\nset represents the largest ever assembled public corpus of\nInternet trafﬁc, and is perhaps the largest public collection\nof streaming network events of any type. Figure 1 shows the\nnumber of packet in each month and indicates a signiﬁcant\nincrease aligning with the COVID19 pandemic. Analysis of\nsuch a large network data set is computationally challenging\n[7]–[9]. Using the combined resources of the Supercomputing\nCenters at UC San Diego, Lawrence Berkeley National Labo-\nratory, and MIT, the spatial temporal structure of anonymized\nsource-destination pairs from the CAIDA Telescope data has\nbeen analyzed leveraging prior work on massively parallel\nGraphBLAS hierarchical hypersparse matrices [10]–[16]. Ap-\nplying this type of analysis to other collections of billions\nof network packets has revealed power-law distributions [17],\nnovel scaling relations [18], and inspired new models of\nnetwork trafﬁc [19]. The goal of this paper is to understand\nthe scaling relations revealed by the CAIDA telescope data\nset. These scaling relations can provide fundamental insights\ninto Internet background trafﬁc. This work can also provide a\ndemonstration of the practical feasibility and beneﬁt of the safe\ncollection and analysis of signiﬁcant of quantities anonymized\nInternet trafﬁc.\nThe outline of the rest of the paper is as follows. First,\nthe network quantities and their distributions are deﬁned in\nterms of trafﬁc matrices. Second, multi-temporal analysis of\nhypersparse hierarchical trafﬁc matrices is described. Third,\nthe method for determining scaling relations as a function of\nthe packet window NVis presented along with the resulting\nscaling relations observed in the gateway trafﬁc data. Finally,\nour conclusions and directions for further work are presented.\nII. N ETWORK QUANTITIES AND DISTRIBUTIONS FROM\nTRAFFIC MATRICES\nThe CAIDA Telescope monitors the trafﬁc into and out of a\nset of network addresses providing a natural observation point\nof network trafﬁc. These data can be viewed as a trafﬁc matrix\nwhere each row is a source and each column is a destination.\nThe CAIDA Telescope trafﬁc matrix can be partitioned into\nfour quadrants (see Figure 2). These quadrants represent\ndifferent ﬂows between nodes internal and external to the set of\nmonitored addresses. Because the CAIDA Telescope network\naddresses are a darkspace, only the upper left (external !\ninternal) quadrant will have data. Internet data must be handled\nwith care, and CAIDA has pioneered standard trusted data\nsharing best practices that include [2]\n\u000fData is made available in curated repositories\n\u000fUsing standard anonymization methods where needed:\nhashing, sampling, and/or simulation\n\u000fRegistration with a repository and demonstration of le-\ngitimate research need\nsourcesdestinationssourcesdestinationssparse traffic matrix A\ninternalàexternalinternalàinternalexternalàexternalexternalàinternal\ninternal                       externalinternal                       externalFig. 2. Network trafﬁc matrix. The trafﬁc matrix can be divided into\nquadrants separating internal and external trafﬁc. The CAIDA Telescope\nmonitors a darkspace, so only the upper left (external !internal) quadrant\nwill have data.\nvalidsource packetwindowNV=217,218,…,227(time window)validdestination packetwindowNV=217,218,…,227(time window)source packets(packets from a source)\ndestination packets(packets to a destination)uniquedestinationsuniquesourcesuniquelinkssourcefan-out\ndestinationfan-inlinkpackets\nFig. 3. Streaming network trafﬁc quantities. Internet trafﬁc streams of\nNVvalid packets are divided into a variety of quantities for analysis: source\npackets, source fan-out, unique source-destination pair packets (or links),\ndestination fan-in, and destination packets.\n\u000fRecipients legally agree to neither repost a corpus nor\ndeanonymize data\n\u000fRecipients can publish analysis and data examples nec-\nessary to review research\n\u000fRecipients agree to cite the repository and provide pub-\nlications back to the repository\n\u000fRepositories can curate enriched products developed by\nresearchers\nStreams of interactions between entities are found in many\ndomains. For Internet trafﬁc these interactions are referred to\nas packets [20]. Figure 3 illustrates essential quantities found\nin all streaming dynamic networks. These quantities are all\ncomputable from anonymized trafﬁc matrices created from\nthe source and destinations found in packet headers. These\nsources and destinations are referred as Internet Protocol (IP)\naddresses.\nProcessing such a large volume of data requires additional\ncomputational innovations. The advent of GraphBLAS hyper-\nsparse hierarchical trafﬁc matrices has enabled the processing\n\nof hundreds of billions of packets in minutes [16], [21]–\n[23]. The CAIDA Telescope archives its trillions of collected\npackets at the supercomputing center at Lawrence Berkeley\nNational Laboratory (LBNL) where the packets are aggregated\ninto CryptoPAN [24] anonymized GraphBLAS trafﬁc matrices\nofNV= 217contiguous packets. This process compresses\nthe data down to approximately 3 bytes/packet. The resulting\nmatrices are stored and sent to the MIT SuperCloud where\nthe network quantities shown in Figure 3 are computed. Using\n384 64-core compute nodes (24,576 cores total) on the MIT\nSuperCloud all 40,000,000,000,000 packets were processed in\nfour days.\nThe code was implemented using Matlab/Octave with the\npMatlab parallel library [10]. A typical run could be launched\nin a few seconds using the MIT SuperCloud triples-mode\nhierarchical launching system [13]. Typical launch param-\neters were [384 16 4], corresponding to 384 nodes, 16\nMatlab/Octave processes per node, and 4 OpenMP threads\nper process. On each node, the 16 processes were pinned\nto 4 adjacent cores to minimize interprocess contention and\nmaximize cache locality for the GraphBLAS OpenMP threads\n[25]. Three levels of parallelism were used within the pro-\ngram. At the top level each month in a year was processed\nindependently using 384/12 = 32 compute nodes. Within each\nmonth, the packet windows were were split among the 32 \u000216\n= 512 Matlab/Octave processes with some overlap to allow\nfor contiguous processing of the streaming data. Within each\nMatlab/Octave process, the underlying GraphBLAS OpenMP\nparallelism was used on 4 cores. At the end of the processing\nthe results were aggregated using asynchronous ﬁle-based\nmessaging [26].\nThe contiguous nature of these data allows the exploration\nof a wide range of packet windows from NV= 217(sub-\nsecond) toNV= 227(minutes), providing a unique view into\nhow network quantities depend upon time. These observations\nprovide new insights into normal network background trafﬁc\nthat could be used for anomaly detection, AI feature engineer-\ning, polystore index learning, and testing theoretical models of\nstreaming networks [27]–[29].\nThe network quantities depicted in Figure 3 are computable\nfrom anonymized origin-destination matrices that are widely\nused to represent network trafﬁc [30]–[33]. It is common\nto ﬁlter the packets down to a valid set for any particular\nanalysis. Such ﬁlters may limit particular sources, destinations,\nprotocols, and time windows. To reduce statistical ﬂuctuations,\nthe streaming data should be partitioned so that for any chosen\ntime window all data sets have the same number of valid\npackets [15]. At a given time t,NVconsecutive valid packets\nare aggregated from the trafﬁc into a sparse matrix At, where\nAt(i;j)is the number of valid packets between the source i\nand destination j. The sum of all the entries in Atis equal to\nNVX\ni;jAt(i;j) =NV\nAll the network quantities depicted in Figure 3 can be readilyTABLE I\nNETWORK QUANTITIES FROM TRAFFIC MATRICES\nFormulas for computing network quantities from trafﬁc matrix Atat timetin\nboth summation and matrix notation. 1is a column vector of all 1’s,Tis the\ntranspose operation, and j j0is the zero-norm that sets each nonzero value of\nits argument to 1 [34]. These formulas are unaffected by matrix permutations\nand will work on anonymized data.\nAggregate Summation Matrix\nProperty Notation Notation\nValid packets NVP\niP\njAt(i;j)1TAt1\nUnique linksP\niP\njjAt(i;j)j01TjAtj01\nLink packets from itoj At(i;j) At\nMax link packets ( dmax) maxijAt(i;j) max( At)\nUnique sourcesP\nijP\njAt(i;j)j01TjAt1j0\nPackets from source iP\njAt(i;j) At1\nMax source packets ( dmax) maxiP\njAt(i;j) max( At1)\nSource fan-out from iP\njjAt(i;j)j0jAtj01\nMax source fan-out ( dmax) maxiP\njjAt(i;j)j0max(jAtj01)\nUnique destinationsP\njjP\niAt(i;j)j0j1TAtj01\nDestination packets to jP\niAt(i;j)1TjAtj0\nMax destination packets ( dmax)maxjP\niAt(i;j) max( 1TjAtj0)\nDestination fan-in to jP\nijAt(i;j)j01TAt\nMax destination fan-in ( dmax) maxjP\nijAt(i;j)j0max( 1TAt)\n100101102103104105source packets10-510-410-310-210-1100differential cumulative probabilityNV = 217\nsupernodeleaf nodes\ndmaxsource packetsdifferential cumulative probability\n100101102103104source packets10-610-510-410-310-210-1100differential cumulative probabilityNV = 217\nsupernodeleaf nodes\ndmaxsource packetsdifferential cumulative probability\n100102104106108source packets10-610-510-410-310-210-1100differential cumulative probabilityNV = 227\nsupernodeleaf nodes\ndmaxsource packetsdifferential cumulative probability\n100101102103104105106107source packets10-610-510-410-310-210-1100differential cumulative probabilityNV = 227\nsupernodeleaf nodes\ndmaxsource packetsdifferential cumulative probability\nFig. 4. Source packet degree distribution (left 2019-06, right 2020-06).\nExample differential cumulative probability (normalized histogram) for two\npacket windows ( NV= 217andNV= 227) of the number (degree) of\nsource packets from each source using logarithmic bins di= 2i. Sources\nsending out a single packet are denoted “leaf nodes”. The source with the\nlargest number of packets dmax is referred to as the “supernode”.\ncomputed from Atusing the formulas listed in Table I. Be-\ncause matrix operations are generally invariant to permutation\n(reordering of the rows and columns), these quantities can\nreadily be computed from anonymized data.\nEach network quantity will produce a distribution of values\nwhose magnitude is often called the degree d. The correspond-\ning histogram of the network quantity is denoted by nt(d). The\nlargest observed value in the distribution is denoted dmax. The\n\nnormalization factor of the distribution is given by\nX\ndnt(d)\nwith corresponding probability\npt(d) =nt(d)=X\ndnt(d)\nand cumulative probability\nPt(d) =X\ni=1;dpt(d)\nBecause of the relatively large values of dobserved, the mea-\nsured probability at large doften exhibits large ﬂuctuations.\nHowever, the cumulative probability lacks sufﬁcient detail to\nsee variations around speciﬁc values of d, so it is typical to\npool the differential cumulative probability with logarithmic\nbins ind\nDt(di) =Pt(di)\u0000Pt(di\u00001)\nwheredi= 2i[35]. All computed probability distributions\nuse the same binary logarithmic binning to allow for con-\nsistent statistical comparison across data sets [35], [36]. The\ncorresponding mean and standard deviation of Dt(di)over\nmany different consecutive values of tfor a given data set\nare denoted D(di)and\u001b(di). Figure 4 provides an example\ndistribution of external !internal source packets with packet\nwindows of NV= 217andNV= 227at two different times.\nThe resulting distribution exhibits the power-law frequently\nobserved in network data [37]–[43].\nIII. M ULTI -TEMPORAL ANALYSIS\nNetwork trafﬁc is dynamic and exhibits varying behavior\non a wide range of time scales. A given packet window\nsizeNVwill be sensitive to phenomena on its corresponding\ntimescale. Determining how network quantities scale with NV\nprovides insight into the temporal behavior of network trafﬁc.\nEfﬁcient computation of network quantities on multiple time\nscales can be achieved by hierarchically aggregating data in\ndifferent time windows [15]. Figure 5 illustrates a binary\naggregation of different streaming trafﬁc matrices. Computing\neach quantity at each hierarchy level eliminates redundant\ncomputations that would be performed if each packet window\nwas computed separately. Hierarchy also ensures that most\ncomputations are performed on smaller matrices residing in\nfaster memory. Correlations among the matrices mean that\nadding two matrices each with NVentries results in a matrix\nwith fewer than 2NVentries, reducing the relative number of\noperations as the matrices grow.\nOne of the important capabilities of the SuiteSparse Graph-\nBLAS library is support for hypersparse matrices where the\nnumber of nonzero entries is signiﬁcantly less than either\ndimensions of the matrix. If the packet source and destination\nidentiﬁers are drawn from a large numeric range, such as those\nused in the Internet protocol, then a hypersparse representation\nofAteliminates the need to keep track of additional indices\nand can signiﬁcantly accelerate the computations [16].\nstream of trafficmatricesNV=217sourcessparsetrafficmatrixAt:t+TNV=218sourcessparsetrafficmatrixAt:t+2T++++\nNV=219sourcessparsetrafficmatrixAt:t+4T++sourcesdestinations\nsourcesdestinations\nsourcesdestinationsFig. 5. Multi-temporal streaming trafﬁc matrices. Efﬁcient computation of\nnetwork quantities on multiple time scales can be achieved by hierarchically\naggregating data in different time windows.\nIV. R ESULTS\nOver 40,000,000,000,000 CAIDA Telescope anonymized\npacket headers from 2019 and 2020 have been collected\nand analyzed. The network quantities in Table I have been\ncomputed for window sizes corresponding to\nNV= 217;218;219;220;221;222;223;224;225;226;227\nThe averages and standard deviations of these quantities have\nbeen computed over sets of 227packets. A key property\nis how the various network quantities scale as a function\nof the window size NV. Figure 6 (top) shows the average\ntotal number of unique sources as a fraction of the packet\nwindowNVmeasured for the months of 2019-06 and 2020-\n06. Figure 6 (bottom) is the result of scaling the data using\nthe formula\n\fN\u000b\nV\nPerforming a similar analysis across all the network quanti-\nties produced the scaling relations for each month. Figures 7,\n8, and 9 show the scaling exponents \u000bfor the sources,\ndestinations, and links. In most cases, these scaling exponents\nare remarkably consistent and lie in the range 0:9<\u000b< 1:0.\nThree notable exceptions are the scaling of the unique sources,\nthe unique destinations, and the destination supernode fan-in\n(destination with the most unique sources). The unique sources\nshown in Figure 7 (top) appear to scale as N0:5\nVin 2019, which\nincreases to N0:7\nVappearing in 2020, implying that the relative\ndiversity of observed darkspace sources grew during 2020. The\nunique destinations shown in Figure 8 (top) consistently scaled\nasN0:8\nVthroughout 2019 and 2020 indicating that although\nthe source variety may have increased the set of destination\naddresses they were reaching out to remained similar. The des-\ntination supernode fan-in scaling shows signiﬁcant ﬂuctuations\nthroughout 2019 and 2020 spanning 0:5<\u000b< 1:0.\nTable II summarizes the median scaling relations of both the\naverages and standard deviations of all the network quantities\nfor 2019 and 2020. These results reveal a strong dependence\non these quantities as a function of the packet window size\nNVas well as remarkable consistency between 2019 and\n2020. To our knowledge, these scaling relations have not\nbeen previously reported and represent a new view on the\nbackground behavior of network trafﬁc. The scaling relations\n\nFig. 6. (top) Unique source fraction (left 2019-06, right 2020-06). Average total number of unique sources in a packet window of width NVmeasured at\neach time over a month. (bottom) Normalized unique source fraction (left 2019-06, right 2020-06). Scaling (top) data by (NV=217)0:5and(NV=217)0:49\naligns the different packet windows, indicating that the number of uniques sources is proportional to N1\u00000:5\nV=N0:5\nVandN1\u00000:49\nV=N0:51\nV.\u0001\u000bis the\ndifference between the best ﬁt \u000bobtained using the different error norms j j2,j j, andj j0\nTABLE II\nAPPROXIMATE SCALING RELATIONS .\nAnalysis of network quantities over packet windows NV= 217;:::; 227reveals a strong dependence of many of these quantities on NVas well as remarkable\nconsistency between 2019 and 2020.\n2019Average2019Deviation2020Average2020DeviationUnique links1.3 x NV0.980.010 x NV0.981.3 x NV0.980.011 x NV0.93Max link packets (dmax)0.0019 x NV0.990.019 x NV0.840.0014 x NV0.990.051 x NV0.76Unique sources79 x NV0.508.7 x NV0.4113 x NV0.580.062 x NV0.69Max source packets (dmax)0.080 x NV0.980.030 x NV0.910.069 x NV0.870.016 x NV0.90Max source fan-out (dmax)0.11 x NV0.940.031 x NV0.900.12 x NV0.930.014 x NV0.89Unique destinations12 x NV0.790.066 x NV0.8212 x NV0.790.024 x NV0.89Max destination packets (dmax)0.0025 x NV10.011 x NV0.860.0023 x NV10.0080 x NV0.89Max destination fan-in (dmax)0.0027 x NV0.890.074 x NV0.530.0023 x NV0.840.00035 x NV0.79\n\n123456789101112\nMonth00.10.20.30.40.50.60.70.80.91Scaling Exponent unique source fraction\n2020\n2019\n123456789101112\nMonth00.10.20.30.40.50.60.70.80.91Scaling Exponent source packet supernode degree fraction\n2020\n2019\n123456789101112\nMonth00.10.20.30.40.50.60.70.80.91Scaling Exponent source fan-out supernode degree fraction\n2020\n2019Fig. 7. Source scaling exponents .\n123456789101112\nMonth00.10.20.30.40.50.60.70.80.91Scaling Exponent unique destination fraction\n2020\n2019\n123456789101112\nMonth00.10.20.30.40.50.60.70.80.91Scaling Exponent destination packet supernode degree fraction\n2020\n2019\n123456789101112\nMonth00.10.20.30.40.50.60.70.80.91Scaling Exponent destination fan- in supernode degree fraction\n2020\n2019 Fig. 8. Destination scaling exponents .\n\n123456789101112\nMonth00.10.20.30.40.50.60.70.80.91Scaling Exponent unique link fraction\n2020\n2019\n123456789101112\nMonth00.10.20.30.40.50.60.70.80.91Scaling Exponent link supernode degree fraction\n2020\n2019Fig. 9. Link scaling exponents .\nprovide a new baseline for predicting and reasoning about the\nnature of this trafﬁc.\nV. C ONCLUSIONS AND FUTURE WORK\nUnderstanding the behavior of the Internet is essential as\nthe Internet has never been more important to our society.\nThe CAIDA Telescope provides a unique perspective on the\nInternet by observing a continuous stream of darkspace trafﬁc\nrepresenting 1/256 of the Internet. Over 40,000,000,000,000\nunique packets were collected during 2019 and 2020. This is\nthe largest public corpus of Internet trafﬁc ever collected. The\nSupercomputing Centers at UC San Diego, Lawrence Berkeley\nNational Laboratory, and MIT have combined their resources\nto analyze the spatial and temporal structure of anonymized\nsource-destination pairs leveraging GraphBLAS hierarchical\nhypersparse matrices. Analysis of this unsolicited Internet\ndarkspace trafﬁc has revealed many previously unseen scaling\nrelations. The data show a signiﬁcant sustained increase in\nunsolicited trafﬁc corresponding to the start of the COVID19\npandemic, but relatively little change in the underlying scaling\nrelations associated with unique sources, source fan-outs,\nunique links, destination fan-ins, and unique destinations. This\nwork provides a demonstration of the practical feasibility andbeneﬁt of the safe collection and analysis of signiﬁcant of\nquantities of anonymized Internet trafﬁc.\nACKNOWLEDGMENTS\nThe authors wish to acknowledge the following individu-\nals for their contributions and support: Bob Bond, Ronisha\nCarter, Cary Conrad, Alan Edelman, Tucker Hamilton, Jeff\nGottschalk, Nathan Frey, Chris Hill, Hayden Jananthan, Mike\nKanaan, Tim Kraska, Andrew Morris, Charles Leiserson,\nDave Martinez, Mimi McClure, Joseph McDonald, Christian\nProthmann, John Radovan, Steve Rejto, Daniela Rus, Allan\nVanterpool, Matthew Weiss, Marc Zissman.\nREFERENCES\n[1] “ Cisco Visual Networking Index: Forecast and Trends, 2018–2023 .”\nhttps://www.cisco.com/c/en/us/solutions/collateral/executive-\nperspectives/annual-internet-report/white-paper-c11-741490.html.\n[2] J. Kepner, J. Bernays, S. Buckley, K. Cho, C. Conrad, L. Daigle,\nK. Erhardt, V . Gadepally, B. Greene, M. Jones, R. Knake, B. Maggs,\nP. Michaleas, C. Meiners, A. Morris, A. Pentland, S. Pisharody,\nS. Powazek, A. Prout, P. Reiner, K. Suzuki, K. Takhashi, T. Tauber,\nL. Walker, and D. Stetson, “Zero botnets: An observe-pursue-counter\napproach.” Belfer Center Reports, 6 2021.\n[3] K. Claffy, “Measuring the internet,” IEEE Internet Computing , vol. 4,\nno. 1, pp. 73–75, 2000.\n[4] B. Li, J. Springer, G. Bebis, and M. H. Gunes, “A survey of network ﬂow\napplications,” Journal of Network and Computer Applications , vol. 36,\nno. 2, pp. 567–581, 2013.\n[5] M. Rabinovich and M. Allman, “Measuring the internet,” IEEE Internet\nComputing , vol. 20, no. 4, pp. 6–8, 2016.\n[6] k. claffy and D. Clark, “Workshop on internet economics (wie 2019)\nreport,” SIGCOMM Comput. Commun. Rev. , vol. 50, p. 53–59, May\n2020.\n[7] A. Lumsdaine, D. Gregor, B. Hendrickson, and J. Berry, “Challenges in\nparallel graph processing,” Parallel Processing Letters , vol. 17, no. 01,\npp. 5–20, 2007.\n[8] T. G. Kolda and B. W. Bader, “Tensor decompositions and applications,”\nSIAM review , vol. 51, no. 3, pp. 455–500, 2009.\n[9] M. Hilbert and P. L ´opez, “The world’s technological capacity to store,\ncommunicate, and compute information,” Science , p. 1200970, 2011.\n[10] J. Kepner, Parallel MATLAB for Multicore and Multinode Computers .\nSIAM, 2009.\n[11] J. Kepner and J. Gilbert, Graph algorithms in the language of linear\nalgebra . SIAM, 2011.\n[12] J. Kepner and H. Jananthan, Mathematics of big data: Spreadsheets,\ndatabases, matrices, and graphs . MIT Press, 2018.\n[13] A. Reuther, J. Kepner, C. Byun, S. Samsi, W. Arcand, D. Bestor,\nB. Bergeron, V . Gadepally, M. Houle, M. Hubbell, M. Jones, A. Klein,\nL. Milechin, J. Mullen, A. Prout, A. Rosa, C. Yee, and P. Michaleas,\n“Interactive supercomputing on 40,000 cores for machine learning and\ndata analysis,” in 2018 IEEE High Performance extreme Computing\nConference (HPEC) , pp. 1–6, 2018.\n[14] V . Gadepally, J. Kepner, L. Milechin, W. Arcand, D. Bestor, B. Bergeron,\nC. Byun, M. Hubbell, M. Houle, M. Jones, P. Michaleas, J. Mullen,\nA. Prout, A. Rosa, C. Yee, S. Samsi, and A. Reuther, “Hyperscaling\ninternet graph analysis with d4m on the mit supercloud,” in 2018 IEEE\nHigh Performance extreme Computing Conference (HPEC) , pp. 1–6,\nSep. 2018.\n[15] J. Kepner, V . Gadepally, L. Milechin, S. Samsi, W. Arcand, D. Bestor,\nW. Bergeron, C. Byun, M. Hubbell, M. Houle, M. Jones, A. Klein,\nP. Michaleas, J. Mullen, A. Prout, A. Rosa, C. Yee, and A. Reuther,\n“Streaming 1.9 billion hypersparse network updates per second with\nd4m,” in 2019 IEEE High Performance Extreme Computing Conference\n(HPEC) , pp. 1–6, 2019.\n\n[16] J. Kepner, T. Davis, C. Byun, W. Arcand, D. Bestor, W. Bergeron,\nV . Gadepally, M. Hubbell, M. Houle, M. Jones, A. Klein, P. Michaleas,\nL. Milechin, J. Mullen, A. Prout, A. Rosa, S. Samsi, C. Yee, and\nA. Reuther, “75,000,000,000 streaming inserts/second using hierarchical\nhypersparse graphblas matrices,” in 2020 IEEE International Parallel\nand Distributed Processing Symposium Workshops (IPDPSW) , pp. 207–\n210, 2020.\n[17] J. Kepner, K. Cho, K. Claffy, V . Gadepally, P. Michaleas, and\nL. Milechin, “Hypersparse neural network analysis of large-scale internet\ntrafﬁc,” in 2019 IEEE High Performance Extreme Computing Confer-\nence (HPEC) , pp. 1–11, 2019.\n[18] J. Kepner, C. Meiners, C. Byun, S. McGuire, T. Davis, W. Arcand,\nJ. Bernays, D. Bestor, W. Bergeron, V . Gadepally, R. Harnasch,\nM. Hubbell, M. Houle, M. Jones, A. Kirby, A. Klein, L. Milechin,\nJ. Mullen, A. Prout, A. Reuther, A. Rosa, S. Samsi, D. Stetson, A. Tse,\nC. Yee, and P. Michaleas, “Multi-temporal analysis and scaling relations\nof 100,000,000,000 network packets,” in 2020 IEEE High Performance\nExtreme Computing Conference (HPEC) , pp. 1–6, 2020.\n[19] P. Devlin, J. Kepner, A. Luo, and E. Meger, “Hybrid power-law models\nof network trafﬁc,” arXiv preprint arXiv:2103.15928 , 2021.\n[20] D. Huang, A. Chowdhary, and S. Pisharody, Software-Deﬁned network-\ning and security: from theory to practice . CRC Press, 2018.\n[21] J. Kepner, P. Aaltonen, D. Bader, A. Buluc ¸, F. Franchetti, J. Gilbert,\nD. Hutchison, M. Kumar, A. Lumsdaine, H. Meyerhenke, S. McMillan,\nC. Yang, J. D. Owens, M. Zalewski, T. Mattson, and J. Moreira, “Math-\nematical foundations of the graphblas,” in 2016 IEEE High Performance\nExtreme Computing Conference (HPEC) , pp. 1–9, 2016.\n[22] A. Buluc ¸, T. Mattson, S. McMillan, J. Moreira, and C. Yang, “Design\nof the graphblas api for c,” in 2017 IEEE International Parallel and\nDistributed Processing Symposium Workshops (IPDPSW) , pp. 643–652,\n2017.\n[23] T. A. Davis, “Algorithm 1000: Suitesparse:graphblas: Graph algorithms\nin the language of sparse linear algebra,” ACM Trans. Math. Softw. ,\nvol. 45, Dec. 2019.\n[24] J. Fan, J. Xu, M. H. Ammar, and S. B. Moon, “Preﬁx-preserving ip\naddress anonymization: measurement-based security evaluation and a\nnew cryptography-based scheme,” Computer Networks , vol. 46, no. 2,\npp. 253–272, 2004.\n[25] C. Byun, J. Kepner, W. Arcand, D. Bestor, W. Bergeron, M. Hubbell,\nV . Gadepally, M. Houle, M. Jones, A. Klein, L. Milechin, P. Michaleas,\nJ. Mullen, A. Prout, A. Rosa, S. Samsi, C. Yee, and A. Reuther,\n“Optimizing xeon phi for interactive data analysis,” in 2019 IEEE High\nPerformance Extreme Computing Conference (HPEC) , pp. 1–6, 2019.\n[26] C. Byun, J. Kepner, W. Arcand, D. Bestor, B. Bergeron, V . Gadepally,\nM. Houle, M. Hubbell, M. Jones, A. Klein, P. Michaleas, J. Mullen,\nA. Prout, A. Rosa, S. Samsi, C. Yee, and A. Reuther, “Large scale\nparallelization using ﬁle-based communications,” in 2019 IEEE High\nPerformance Extreme Computing Conference (HPEC) , pp. 1–7, 2019.\n[27] A. J. Elmore, J. Duggan, M. Stonebraker, M. Balazinska, U. Cetintemel,\nV . Gadepally, J. Heer, B. Howe, J. Kepner, T. Kraska, et al. , “A\ndemonstration of the bigdawg polystore system,” Proceedings of the\nVLDB Endowment , vol. 8, no. 12, p. 1908, 2015.\n[28] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” in Proceedings of the 2018 International\nConference on Management of Data , SIGMOD 18, (New York, NY ,\nUSA), pp. 489–504, Association for Computing Machinery, 2018.\n[29] E. H. Do and V . N. Gadepally, “Classifying anomalies for network\nsecurity,” in ICASSP 2020 - 2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , pp. 2907–2911,\n2020.\n[30] A. Soule, A. Nucci, R. Cruz, E. Leonardi, and N. Taft, “How to\nidentify and estimate the largest trafﬁc matrix elements in a dynamic\nenvironment,” in ACM SIGMETRICS Performance Evaluation Review ,\nvol. 32, pp. 73–84, ACM, 2004.\n[31] Y . Zhang, M. Roughan, C. Lund, and D. L. Donoho, “Estimating point-\nto-point and point-to-multipoint trafﬁc matrices: an information-theoretic\napproach,” IEEE/ACM Transactions on Networking (TON) , vol. 13,\nno. 5, pp. 947–960, 2005.\n[32] P. J. Mucha, T. Richardson, K. Macon, M. A. Porter, and J.-P. Onnela,\n“Community structure in time-dependent, multiscale, and multiplex\nnetworks,” science , vol. 328, no. 5980, pp. 876–878, 2010.\n[33] P. Tune, M. Roughan, H. Haddadi, and O. Bonaventure, “Internet trafﬁc\nmatrices: A primer,” Recent Advances in Networking , vol. 1, pp. 1–56,\n2013.[34] J. Karvanen and A. Cichocki, “Measuring sparseness of noisy signals,”\nin4th International Symposium on Independent Component Analysis\nand Blind Signal Separation , pp. 125–130, 2003.\n[35] A. Clauset, C. R. Shalizi, and M. E. Newman, “Power-law distributions\nin empirical data,” SIAM review , vol. 51, no. 4, pp. 661–703, 2009.\n[36] A.-L. Barab ´asiet al. ,Network science . Cambridge university press,\n2016.\n[37] W. E. Leland, M. S. Taqqu, W. Willinger, and D. V . Wilson, “On the\nself-similar nature of ethernet trafﬁc (extended version),” IEEE/ACM\nTransactions on Networking (ToN) , vol. 2, no. 1, pp. 1–15, 1994.\n[38] M. Faloutsos, P. Faloutsos, and C. Faloutsos, “On power-law relation-\nships of the internet topology,” in ACM SIGCOMM computer commu-\nnication review , vol. 29-4, pp. 251–262, ACM, 1999.\n[39] R. Albert, H. Jeong, and A.-L. Barab ´asi, “Internet: Diameter of the\nworld-wide web,” Nature , vol. 401, no. 6749, p. 130, 1999.\n[40] A.-L. Barab ´asi and R. Albert, “Emergence of scaling in random net-\nworks,” Science , vol. 286, no. 5439, pp. 509–512, 1999.\n[41] L. A. Adamic and B. A. Huberman, “Power-law distribution of the world\nwide web,” science , vol. 287, no. 5461, pp. 2115–2115, 2000.\n[42] A.-L. Barab ´asi, “Scale-free networks: a decade and beyond,” science ,\nvol. 325, no. 5939, pp. 412–413, 2009.\n[43] A. Mahanti, N. Carlsson, A. Mahanti, M. Arlitt, and C. Williamson, “A\ntale of the tails: Power-laws in internet measurements,” IEEE Network ,\nvol. 27, no. 1, pp. 59–64, 2013.",
  "textLength": 33689
}