{
  "paperId": "e2c84550d04d2bb198ae38728c3d97030a746578",
  "title": "Prefetching Using Principles of Hippocampal-Neocortical Interaction",
  "pdfPath": "e2c84550d04d2bb198ae38728c3d97030a746578.pdf",
  "text": "Prefetching Using Principles of\nHippocampal-Neocortical Interaction\nMichael Wu\nYale University\nmw976@yale.eduKetaki Joshi\nYale University\nketaki.joshi@yale.eduAndrew Sheinberg\nYale University\nandrew.sheinberg@gmail.com\nGuilherme Cox\nNVIDIA\ngcox@nvidia.comAnurag Khandelwal\nYale University\nanurag.khandelwal@yale.eduRaghavendra Pradyumna\nPothukuchi\nYale University\nraghav.pothukuchi@yale.edu\nAbhishek Bhattacharjee\nYale University\nabhishek@cs.yale.edu\nAbstract\nMemory prefetching improves performance across many\nsystems layers. However, achieving high prefetch accuracy\nwith low overhead is challenging, as memory hierarchies\nand application memory access patterns become more com-\nplicated. Furthermore, a prefetcher‚Äôs ability to adapt to new\naccess patterns as they emerge is becoming more crucial\nthan ever. Recent work has demonstrated the use of deep\nlearning techniques to improve prefetching accuracy, albeit\nwith impractical compute and storage overheads. This paper\nsuggests taking inspiration from the learning mechanisms\nand memory architecture of the human brain‚Äîspecifically,\nthe hippocampus and neocortex‚Äîto build resource-efficient,\naccurate, and adaptable prefetchers.\nCCS Concepts\n‚Ä¢Computer systems organization ‚ÜíArchitectures; Neu-\nral networks; Heterogeneous (hybrid) systems; ‚Ä¢Com-\nputing methodologies ‚ÜíBio-inspired approaches.\nKeywords\nPrefetching, Memory Organization, Brain-Inspired Learning\nHOTOS ‚Äô23, June 22‚Äì24, 2023, Providence, RI, USA\n¬© 2023 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0195-5/23/06.\nhttps://doi.org/10.1145/3593856.3595901ACM Reference Format:\nMichael Wu, Ketaki Joshi, Andrew Sheinberg, Guilherme Cox,\nAnurag Khandelwal, Raghavendra Pradyumna Pothukuchi, and Ab-\nhishek Bhattacharjee. 2023. Prefetching Using Principles of Hippocampal-\nNeocortical Interaction. In Workshop on Hot Topics in Operating Sys-\ntems (HOTOS ‚Äô23), June 22‚Äì24, 2023, Providence, RI, USA. ACM, New\nYork, NY, USA, 8 pages. https://doi.org/10.1145/3593856.3595901\n1 Introduction\nMemory prefetching is a performance optimization used\nwidely across several hardware and software layers of mod-\nern computer systems. Prefetching proactively brings data\nfrom slower levels of memory to faster ones, anticipating its\nfuture use. Although well-studied, prefetching continues to\nbe explored, especially as emerging memory hierarchies em-\nbrace heterogeneity [ 22], disaggregation [ 27], vertical/hori-\nzontal tiering [31], and compute in memory [48].\nEarly prefetchers targeted patterns that were easy to cap-\nture, such as strides, and were sufficient for well-understood\napplications, such as those in SPEC [ 4]. However, systems\nand applications today are far more complex and dynamic,\nrendering simple approaches ineffective. There is a growing\ninterest in developing prefetchers that can adapt to the dy-\nnamic execution by learning memory access patterns instead\nof detecting pre-programmed rules [11, 18, 40].\nRecent studies have begun exploring the viability of deep\nlearning (DL) for prefetching [ 11,18,30,40]. In theory, DL\nshould improve prefetching because it is inherently data-\ndriven, and should naturally adapt to applications and their\nenvironments. Indeed, these studies show that, in idealized\nsimulations, DL outperforms non-learning prefetch methods\nin accuracy. However, all of these approaches have three\nmain shortcomings that impede their real-world adoption.\n53This work is licensed under a Creative Commons Attribution International 4.0 License.\n\n\nFirst, the deep neural networks (DNNs) in prior work use\nimpractically high compute and storage resources, even as\nmuch as entire applications (¬ß2.1). A more efficient and light-\nweight learning structure is crucial for real-world systems.\nSecond, prior approaches train their models offline, with-\nout learning continuously from the system execution. This\nis partly necessary due to the overhead of DNN training.\nHowever, such models can then fail to adapt to evolving\napplication phases, configurations, inputs, and other sys-\ntem dynamics. Offline training also requires labeled datasets,\nwhich can be impractical to collect at scale.\nThird, even if we were to optimize resource usage, online\nDNNs still face the challenge of catastrophic interference.\nThis issue is well known in machine learning (ML) [ 23,28],\nand describes the tendency of DNNs to drastically forget pre-\nviously learned information when learning new information.\nWe consider the fundamental challenge of accurately learn-\ning memory access patterns online without interference. We\nargue that we can develop a robust solution by incorporating\nprinciples of learning found in the human brain. We note\nthat the problem of learning access patterns is similar to a\ntask that the human brain encounters continuously, which\nis to discover generalizable structure from its experiences.\nIn particular, we take inspiration from the cognitive theory\nof Complementary Learning Systems (CLS) [ 32], which mod-\nels the human brain as an online learning system. CLS theory\nposits that the brain avoids catastrophic interference using\ninterleaved replay, a process where it interleaves the learning\nof new and old information. Combining this insight with\nbio-inspired Hebbian networks, which use far less resources\nthan DNNs, helps us build efficient, online prefetchers.\nMany principles from CLS have already seen use in the ML\ncommunity [ 13,16,44,49]. Their objectives and constraints,\nhowever, differ from ours. In addition to maximizing model\naccuracy, we must further account for metrics like train-\ning and inference latency, prefetch timeliness, and usage\nof memory/network bandwidth. The combination of these\nconstraints motivates a more domain-specific solution.\nAs we discuss how CLS principles can improve upon DL\napproaches, we also present expected implementation chal-\nlenges with two real contexts: disaggregated systems [ 27]\nand CPU-GPU systems [ 3]. We specifically choose these\nbecause they experience high cross-node communication\nlatency and operate in relatively resource-constrained set-\ntings. Thus, they stand to greatly benefit from intelligent\nmemory prefetching. They also provide the opportunity to\nimplement CLS principles in software, offering a faster path\nto immediate impact in production systems.\n2 DL for Prefetching and Limitations\nPrior DL techniques for cache or memory prefetching used\ntransformers [ 30,45] or Long Short-Term Memories (LSTMs)[18,20,40]. These DNNs use an application‚Äôs recent cache/mem-\nory accesses and instructions to predict its next accesses, and\nhave been shown to achieve high accuracy. However, they\nhave unreasonable resource overheads and are only trained\noffline, thus remaining only simulated ideals.\nCertain works have explored more lightweight learning\nalgorithms for systems, such as reinforcement learning [ 11],\ngradient boosting [ 41], and small DNNs [ 24]. Unfortunately,\nthese approaches too, were only evaluated in simulation,\nand we find in our testing that the lightweight models fail to\nmatch the prefetching accuracy of larger DNNs. Our goal is to\ndevelop online, accurate and resource-efficient prefetchers.\n2.1 Overheads of DL-based Prefetching\nDL-based prefetching incurs untenable compute and storage\noverheads. A state-of-the-art LSTM-based cache prefetcher\n[40] requires over 1 GB of storage using 32-bit parameters.\nThis exceeds the memory capacity of some nodes in our tar-\nget systems [ 27,39]. We model this DNN for memory-page\nprefetching in a disaggregated system [ 27], and aggressively\ncompress it to nearly 1 MB by reducing its input-embedding\ndimension, and the number of output classes. We compile this\nmodel on an Intel i7-8700 CPU, and measure its performance.\nWe evaluate CPU performance because, for the inference\ntimes we target, which are around 1-10 ùúás [7,27,39], accel-\nerator offloading is not clearly beneficial. Figure 1 shows the\nprefetcher‚Äôs deployment.\nFigure 1: A DNN memory prefetcher. The gray area and\ndashed lines indicate steps during training.\n1 2 3 4 5 6 7 8\nNumber of Predictions Made100101102103104Latency (Œºs)\nHebb INT32 1T\nLSTM FP32 1T\nLSTM FP32 2T\nLSTM INT8 1T\nLSTM INT8 2T\n(a) Inference time for various number of future predictions.\n0 100 200 300 400 500\nNumber of Predictions Made101102103104105Latency (Œºs)\nHebb INT32 1T\nLSTM FP32 1T\nLSTM FP32 2T\n(b) Training time for various batch sizes\nFigure 2: Inference and training latency of a state-of-\nthe-art LSTM prefetcher on an Intel i7-8700 CPU.\n54\n\n0 20 40 60 80 100\nIterations0.000.250.500.751.00Confidence\nStride +1\nStride -1(a) Strides without replay\n0 20 40 60 80 100\nIterations0.000.250.500.751.00Confidence\nPointer Chasing\nIndirect Strides (b) Ptrs and Indirects without replay\n0 20 40 60 80 100\nIterations0.000.250.500.751.00Confidence\nIndirect Index\nPointer Offset (c) Index and Ptr Offsets without replay\n0 20 40 60 80 100\nIterations0.000.250.500.751.00Confidence\nStride +1\nStride -1\n(d) Strides with replay\n0 20 40 60 80 100\nIterations0.000.250.500.751.00Confidence\nPointer Chasing\nIndirect Strides (e) Ptrs and Indirects with replay\n0 20 40 60 80 100\nIterations0.000.250.500.751.00Confidence\nIndirect Index\nPointer Offset (f) Index and Ptr Offsets with replay\nFigure 3: Catastrophic interference (a-c) and the effect of replay (d-f) during online prefetch learning. Confidence\non the older pattern is shown in red, while the current one is in blue.\nFigure 2 shows the LSTM latencies with one and two\nthreads, and with parameter quantization (e.g., FP32 to INT8)\nduring inference [ 29]. The LSTM takes >150 ùúás per inference\nand >1 ms per example for training, which are orders of\nmagnitude higher than our target. Multi-threading is ineffec-\ntive because LSTMs have poor parallelism [ 42]. Even after\nquantization, LSTM inference still takes >60 ùúás.\nTable 1: Memory Access Patterns\nPattern Code Behavior\nStride a[i] Accessing data at regular delta such as streaming\npatterns or array traversal.\nPointer chase *ptr Pseudorandom accesses to different parts of the\nmemory. E.g. linked list traversal\nIndirect stride *(a[i]) Accessing data at regular delta from a base address.\nE.g. array of object pointers.\nIndirect index b[a[i]] Accessing data at indices that are at regular delta\nfrom a constant base address.\nPointer offset *ptr\n*(ptr+i)Pseudorandom accesses and adjacent data. E.g.\ntransform on a list/tree.\n2.2 Difficulty of Online Prefetch Learning\nThe scale of modern systems and dynamism of their applica-\ntions makes it impractical to collect representative datasets\nwith which DNN prefetchers can be trained offline. Instead,\nit is ideal if the model can adapt to changing memory access\nbehavior by learning online. Unfortunately, learning online\nmakes the DNN vulnerable to catastrophic interference [32].\nCatastrophic interference occurs when a DNN trained on\none pattern begins learning a different, unrelated pattern.\nThe weight updates made when learning the new pattern\noverwrite the values that were critical in learning the older\npattern, causing the DNN to completely forget the older\none. Such interference is avoided during offline training by\nmixing training data and learning over it in multiple passes.\nThis is not the case when learning online.\nWe use the LSTM from ¬ß2.1 to illustrate catastrophic in-\nterference with online prefetch learning. We first train the\nLSTM on a single memory access pattern (e.g., a constant\nstride) until it achieves perfect accuracy, simulating learningover a single application phase. Next, we present the LSTM\nwith another access pattern (e.g., pointer chasing) to learn.\nWe monitor the LSTM‚Äôs confidence on the current and previ-\nous patterns, which is the probability it assigns to the correct\nprediction. We select different pairs of patterns from those in\nTable 1, adapted from prior work [ 10]. For a pair of patterns,\nwe generate a trace of 1000 accesses with each pattern. We\nuse these data structure -level prefetching patterns to avoid\nconfounding effects possible in page-level prefetching.\nFigures 3a-3c show our results. As the LSTM learns the\ncurrent task, its confidence on the older task drops abruptly,\ndemonstrating catastrophic interference. In some cases, the\nconfidence on the first pattern recovers partially, indicating\nsome knowledge transfer. Nonetheless, the final confidence\nis poor. Such a prefetcher will be ineffective when patterns\nrepeat, which is the case with many applications.\n3 Hippocampal-Neocortical Inspired\nPrefetching\nFigure 4 shows the brain‚Äôs learning architecture in CLS the-\nory. Learning occurs through a complementary relationship\nbetween two regions of the brain: the neocortex and the\nhippocampus. The neocortex, similar to DNNs, slowly learns\nthe structure underlying the information it encounters; i.e.,\nthe rules behind a memory access pattern. Meanwhile, the\nhippocampus quickly memorizes the information it encoun-\nters ‚Äî i.e., the exact memory accesses ‚Äî in a compressed\nformat, likely by separating each access and storing them in\nan associative memory [ 35,36]. Over time, this information\nis decompressed and replayed in the neocortex, interleaving\nthe learning of old and new tasks, thus mitigating interfer-\nence [ 25,32]. Furthermore, CLS theory models the networks\nin the brain using biologically-inspired Hebbian networks,\nwhich have much lower resource needs compared to stan-\ndard DNNs. We show how each of these ideas help overcome\nthe limitations of standard DNNs for prefetching.\n55\n\nStructure \nLearner\n(Neocortex)Pattern \nCompletion\n(CA3)\nDecoding\n(CA1)Pattern Separation\n(Dentate gyrus)\nInterface\n(EC)Input\nOutputHippocampus\nStructure \nLearnerPattern \nCompletion\n(CA3)\nDecoding\n(CA1)Pattern Separation\n(Dentate gyrus)\nInterface\n(EC)Input\nOutputHippocampus NeocortexFigure 4: Memory architecture of the brain in CLS the-\nory. Each block is modeled with a Hebbian neural net-\nwork. Solid lines show information storage paths and\ndashed lines show recall and replay.\n3.1 Overcoming Resource Overheads\nBrain-inspired prefetch networks are resource efficient for\ntwo reasons. The first reason is that they use a Hebbian\nupdate rule, which is much simpler than that of standard\nDNN learning. When learning with Hebbian networks, a\nnetwork weight ùë§ùëñ ùëó‚Äîconnecting an input neuron ùëéùëñand\noutput neuron ùëèùëó‚Äîis increased if both neurons are non-zero,\nand decreased if the input neuron ( ùëéùëñ) is inactive while the\noutput ( ùëèùëó) is active. The simplified update rule [15, 36] is:\nŒîùë§ùëñ ùëó=(ùëèùëó‚â†0)[(ùëéùëñ‚â†0)‚àí( ùëéùëñ==0)] (1)\nThis update scheme requires far fewer operations than con-\nventional training of DNNs.\nThe second advantage of brain-inspired networks comes\nfrom their use of sparsity. These networks are sparse in\ntheir connectivity, meaning a node connects to only 1-25% of\nthe nodes in adjacent layers unlike all-to-all connections in\nDNNs. Additionally, they are sparse in their representations,\nin that only 1-25% of the network‚Äôs hidden layer neurons are\nactivated on an input. As a result, the storage and compute\nneeds of these networks are a fraction of those for DNNs.\nWe prototype a sparse Hebbian neural network for prefetch-\ning. The network has a single hidden layer of 1000 neurons,\nwith 12.5% connectivity between layers, and 10% sparsity\nin the hidden layer. Like an LSTM, our network also uses a\nrecurrent state to capture sequence memory.\nTable 2 compares the resource needs of our Hebbian net-\nwork with that of the LSTM from ¬ß2.2. We list the lower\nbound for the number of operations (Ops) in the LSTM, as its\nexact value varies with the implementation of transcendental\nfunctions (e.g., sigmoid and tanh). The Hebbian network is\nnearly 3√ósmaller than the LSTM with nearly an order of\nmagnitude fewer Ops. Hence, the Hebbian training and in-\nference times, shown in Figure 2, are proportionately lower.\nTable 2: Resource Needs of Hebbian vs LSTM networks\nModel Parameters #Ops (inference) #Ops (training)\nLSTM 170 k > 170k FP > 400k FP\nHebbian 49 k 14k INT 64k INTWe also compare the networks‚Äô prefetching accuracy with\nmultiple applications that have various memory access pat-\nterns: TensorFlow [ 6] training ResNet-50 [ 19], PageRank [ 34]\nusing GraphChi [ 26], mcf [ 4], and graph500 [ 1]. For each\napplication, we collect a trace of 2 billion memory accesses\nand simulate them with a memory sized at 50% of the trace‚Äôs\nfootprint. We deploy both prefetchers as shown in Figure 1,\nwith a miss history length of 1 input to the networks, and\nmeasure the percentage of misses removed compared to a\nbaseline without prefetching. Figure 5 shows the results. Our\nnetwork has comparable accuracy to the LSTM, even while\nconsuming far fewer resources, showing the effectiveness of\nHebbian learning.\ntensorflow mcf graph500 pagerank0255075100% Misses RemovedHebbian\nLSTM\nFigure 5: Online memory prefetching performance of\nHebbian and LSTM networks.\n3.2 Overcoming Catastrophic Interference\nIn CLS theory, catastrophic interference is mitigated by re-\nplaying past memories stored in the hippocampus. Building\na full hippocampal-like storage is an open problem requiring\nconsideration on selectively storing and sampling accesses\ninto it. Therefore, in this work, we will focus on showing\nthe benefits of replay for online prefetch learning without\nresource limitations on the hippocampal storage.\nWe perform the experiments in ¬ß2.2 that showed cata-\nstrophic interference again, but with replay. We implement\nreplay by retraining the network on the first pattern using\na 0.1√ósmaller learning rate after each training/inference of\nthe second. Figures 3d-3f shows the new results. While the\nmodels without replay experienced catastrophic interference,\nperforming replay lets the network learn the new pattern\nwithout forgetting the old one. Even if the old pattern were\nto repeat, the network would maintain prediction accuracy\nwithout needing to re-learn it.\n4 Target Systems for Online Prefetching\nWe target two environments for an initial deployment of\nour brain-inspired prefetcher: disaggregated systems [ 27]\nand CPU-GPU unified virtual memory (UVM) systems [ 3].\nBoth systems, shown in Figure 6, have data movement that\nincurs high communication latencies [ 7,27], and stand to\nbenefit from intelligent prefetching. However, the systems\ndiffer in a few critical ways that lead to requiring different\ntypes of prefetch strategies. One difference is the impact\nof a page miss. In disaggregated systems, CPU cores fault\n56\n\nGPU GPU GPUMEM MEM MEMMEMCPU\nUVM Driver\nPrefetcher(a) UVM system\nMEM MEM\nSwitch\nMemory\nBladeMemory\nBladeMemory\nBladeCPU\nKernel\nPr efetcherCPU\nKernel\nPr efetcher (b) Disaggregated system\nFigure 6: Architectures of our target systems. Black\nlines indicate page miss notifications and prefetch re-\nquests. Red lines indicate data movement paths.\nonly on one page at a time, indicating that the prefetcher\nshould be optimized to hide latency. In GPUs, the SIMT (sin-\ngle instruction multiple thread) execution can produce many\nconcurrent faults, and the lockstep execution model means\nthat a single fault can stall many threads. This suggests that a\nthroughput-optimized prefetcher would be more appropriate\nfor this system.\nOur target systems also differ in the location from which\nthe prefetchers can obtain information about memory ac-\ncesses, and where the compute and memory resources to\nrun prefetching are available. These parameters determine\nwhere the prefetcher should be placed. In the UVM system,\nsoftware-level information on memory accesses is only avail-\nable in the CPU-located driver. Therefore, all prefetching\ndecisions must ultimately be made from this centralized lo-\ncation. This contrasts with the disaggregated system, where\nscarce resources on the switch necessitate a decentralized\napproach with a separate prefetcher per node.\nThe different placement of the prefetcher in each system,\nin turn, results in unique design spaces. For example, the\nprefetcher in the UVM system can take advantage of its\nglobal view to make better-informed decisions, but may re-\nquire more processing to ensure that it can isolate the individ-\nual access patterns in the combined access streams. However,\nsuch interleaving of access streams may naturally offer more\nresistance to catastrophic interference, reducing replay costs.\nIn contrast, the prefetcher in the disaggregated system is\nless likely to see interleaved access streams since it has a\nseparate prefetcher per CPU. Therefore, the prefetcher net-\nwork could be smaller to learn the access patterns. With the\ndecentralized architecture, it is easier to integrate application-\nspecific, profile-guided prefetch optimizations.\n5 Future Research Challenges\nOur initials results with prefetchers based on the hippocampal-\nneocortical interaction are promising. Harnessing the full\nbenefits of such a prefetcher requires solving important chal-\nlenges to replicate the CLS architecture in Figure 4 in com-\nputer systems. We present these issues, beyond the target\nsystem-specific ones discussed in ¬ß4.5.1 Training Instances\nTraining on every prefetch inference, as done in our experi-\nments (¬ß3.1), can be unnecessary and resource-consuming,\nespecially because training is more expensive than inference.\nPossible alternatives are to train on a batch of samples at\nonce, and/or to only train on certain misses. Training only on\nsome misses reduces overall training costs, but requires care.\nSimple approaches, such as randomly deciding which sam-\nples to train on, may miss cases that are critical for the model\nto understand the application. A more intelligent sampling\nprocess could use confidence measures from the model to\nfilter less-information carrying samples, or to avoid training\non well-learned cases.\n5.2 Prefetch Output and Miss History\nThere are two main parameters for the prefetcher‚Äôs output:\nlength, which is the number steps predicted into the future;\nand width, which is the number of predictions made at each\nstep. The ideal values for these parameters are determined by\nthe system architecture and application behavior. Consider\nour design from ¬ß3.1, where the network is trained only\nto predict the next miss. If the time between misses is less\nthan the inference latency, even a perfect model will always\nprefetch too late. In that case, a more effective method is to\npredict a sequence of misses further into the future.\nRegarding prefetch width, throughput-bound environments\nlike the UVM system might benefit more from predicting\nmultiple prefetches at a time, even if they have slightly less ac-\ncuracy. The same could be said for read-heavy workloads. On\nthe other hand, systems where the network is the bottleneck\nrequire a prefetcher that is highly selective and confident\nabout bringing in data to minimize communication.\nIn order to learn how to predict with a given length and\nwidth, a prefetcher must maintain a miss history. For ex-\nample, when prefetching multiple steps into the future, a\nwindow of past misses is required to construct appropriate\ntraining examples. Thus, the prefetch length determines a\nminimum history size. Beyond this, the ideal history size\ndepends on the reuse distances in the access patterns of the\napplication. If the current pattern has short reuse distances,\nthen only a few entries in the miss history are necessary,\nwhile it is the opposite for patterns with longer reuse dis-\ntances. Thus, configuring the prefetch length, width, and the\naccess history will require intelligent co-design.\n5.3 Encoding Data for Prefetching\nMost prior work on ML-based prefetching encodes addresses\nand strides as one-hot vectors, which are then indexed into\nan embedding table to obtain a dense representation that is in-\nput to the prefetcher [ 18,30,40]. This aligns with approaches\nused for words in natural language processing, but storing\n57\n\nembeddings can become expensive (e.g., >500 MB [ 40]). It\nalso inflates compute costs, as the output layer grows linearly\nwith the number of embedding vectors.\nA more fundamental issue is that addresses and strides\ncan be a poor proxy for understanding the inherent behavior\nof an application. We found that, as of now, neither the\nLSTM nor the Hebbian network perform well on caching\napplications like memcached [ 17] and cachebench [ 12]. This\nis because these applications are almost entirely pointer-\nbased, and the access patterns are difficult to learn from\naddresses or strides.\nIdeally, the representation of the input data should more\nclosely resemble how addresses ‚Äúflow‚Äù at the data structure\nlevel (e.g. tree nodes, pointer chains). We have found that\ninsights from the cognitive theories can provide inspiration\nhere as well. There is evidence that the hippocampus en-\ncodes locations optimizing for vector-based navigation [ 33].\nA similar encoding for addresses could better represent paths\nthrough data structures. Other brain-inspired work has ex-\nplored ways of representing symbols that allow the efficient\ndetection of relations in neural networks [ 8]. An analog of\nsuch an approach for prefetching would be an address em-\nbedding optimized for detecting pointers that are logically\n(as opposed to numerically) close to one another.\n5.4 Implementing Replay\nIn our experiments to demonstrate the utility of replay, we\nassumed that we could store all past examples, and interleave\nthem later. A full implementation must trade the storage/-\ncompute costs of replay with its benefits for learning. One\nsimple approach is to use a fixed-size buffer. This, however,\ncould lose important information as entries are evicted. A\nmore principled approach could save space by filtering less\nimportant examples, perhaps using confidence as a measure,\nor freeing entries that have already been consolidated due\nto replay, thus not needed further learning [ 32]. Yet another\nalternative is to average similar examples, producing single\nrepresentative cases.\nAnother challenge in incorporating replay is to define ap-\nplication phases so that they can be replayed. However, phase\ncharacteristics can vary significantly between applications,\nmaking it difficult to manage replay with a single parameter\nsetting (buffer size, time limits, miss counts, etc.). This could\nmotivate an interface for application developers to directly\ntune replay parameters, or to indirectly indicate phase behav-\nior and timings. Another approach, also inspired by cognitive\ntheories, is to identify contexts or phases using clustering of\nabstract representations learned by the network [14].\nFinally, our preliminary studies also show only one form\nof replay. Cognitive literature describes many forms of re-\nplay [ 46], each with their own benefits and challenges. Some\nmethods such as hindsight orsimulation replay, where theprefetcher artificially generates memory sequences and learns\nthem, can be helpful in avoiding replay storage costs alto-\ngether. Such sequences could be generated by rules that\nare determined with profile-guided techniques, or through\ngenerative networks i.e., trading off compute for memory.\nAnother alternative could generate hidden layer values, com-\nplete the forward pass, and train on the output to reinforce\nexisting behavior. We leave such ideas for future research.\n5.5 Availability\nSince training actively changes the weights of a neural net-\nwork, it may be important to block inference during train-\ning. This is an availability issue, which motivates a protocol\nwhere training is applied to a separate model copy, which is\nlater redeployed when the live model‚Äôs confidence/accuracy\ndecreases. However, it is possible that simpler approaches\ncould also suffice for two reasons. One, because prefetch-\ning is not correctness-critical, inference requests are safe\nto drop. Second, neural networks can have noise-robust or\nnoise-smoothing effects, meaning that small perturbations to\nweights do not cause large changes in the network‚Äôs output,\nespecially since our training method explicitly seeks to pre-\nserve the network‚Äôs prior performance [ 21,35]. This could\nallow inference to remain accurate even when concurrent\nwith training. We expect this property to require further\nstudy, as it could significantly optimize a real prefetcher\ndeployment, but may be sensitive to the many details of\npractical implementation.\n6 Conclusion\nRecent deep learning approaches to prefetching have shown\npromising results, but only in idealized simulations. Their\nreal implementation is impeded due to resource overheads\nand learning limitations. This paper explores how one might\naddress these issues using principles and models of the hip-\npocampus and neocortex, as well as some challenges we\nshould expect in implementing them. We ultimately expect\nmore challenges to appear in designing and deploying such\nmodels, but we hope this paper serves as a starting point for\nimplementing efficient and intelligent learning algorithms\nfor all relevant systems contexts.\n7 Acknowledgements\nWe thank Jonathan D. Cohen for his insights on complemen-\ntary learning systems. We also thank Seung-seob Lee for his\nhelp with generating the traces used in this work. This work\nwas made possible in part via funding from the National Sci-\nence Foundation awards 2118851, 2040682, 2112562, 2047220\nand a Computing Innovation Fellowship for Raghavendra\nPradyumna Pothukuchi (under NSF grant 2127309 to the\nComputing Research Association).\n58\n\nReferences\n[1] 2010. Graph 500 | large-scale benchmarks. https://graph500.org/.\n[2]2016. Introduction to Cache Allocation Technology in the Intel¬Æ\nXeon¬Æ Processor E5 v4 Family. https://www.intel.com/content/www/\nus/en/developer/articles/technical/introduction-to-cache-allocation-\ntechnology.html.\n[3]2017. Nvidia Unified Memory. https://developer.nvidia.com/blog/\nunified-memory-cuda-beginners/.\n[4]2017. SPEC Benchmarks and Tools. https://www.spec.org/benchmarks.\nhtml.\n[5]2018. libtorch. https://github.com/pytorch/pytorch/blob/master/docs/\nlibtorch.rst.\n[6]Mart√≠n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng\nChen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu\nDevin, et al .2016. Tensorflow: Large-scale machine learning on hetero-\ngeneous distributed systems. arXiv preprint arXiv:1603.04467 (2016).\n[7]Tyler Allen and Rong Ge. 2021. Demystifying GPU UVM Cost\nwith Deep Runtime and Workload Analysis. In 2021 IEEE Interna-\ntional Parallel and Distributed Processing Symposium (IPDPS). 141‚Äì150.\nhttps://doi.org/10.1109/IPDPS49936.2021.00023\n[8]Awni Altabaa, Taylor Webb, Jonathan Cohen, and John Lafferty. 2023.\nAbstractors: Transformer Modules for Symbolic Message Passing and\nRelational Reasoning. arXiv preprint arXiv:2304.00195 (2023).\n[9]Berk Atikoglu, Yuehai Xu, Eitan Frachtenberg, Song Jiang, and Mike\nPaleczny. 2012. Workload Analysis of a Large-Scale Key-Value Store.\nInProceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint\nInternational Conference on Measurement and Modeling of Computer\nSystems (London, England, UK) (SIGMETRICS ‚Äô12). Association for\nComputing Machinery, New York, NY, USA, 53‚Äì64. https://doi.org/\n10.1145/2254756.2254766\n[10] Grant Ayers, Heiner Litz, Christos Kozyrakis, and Parthasarathy Ran-\nganathan. 2020. Classifying Memory Access Patterns for Prefetching.\nInProceedings of the Twenty-Fifth International Conference on Archi-\ntectural Support for Programming Languages and Operating Systems\n(Lausanne, Switzerland) (ASPLOS ‚Äô20). Association for Computing\nMachinery, New York, NY, USA, 513‚Äì526. https://doi.org/10.1145/\n3373376.3378498\n[11] Rahul Bera, Konstantinos Kanellopoulos, Anant Nori, Taha Shahroodi,\nSreenivas Subramoney, and Onur Mutlu. 2021. Pythia: A Customiz-\nable Hardware Prefetching Framework Using Online Reinforcement\nLearning. In MICRO-54: 54th Annual IEEE/ACM International Sympo-\nsium on Microarchitecture (Virtual Event, Greece) (MICRO ‚Äô21). Asso-\nciation for Computing Machinery, New York, NY, USA, 1121‚Äì1137.\nhttps://doi.org/10.1145/3466752.3480114\n[12] Benjamin Berg, Daniel S. Berger, Sara McAllister, Isaac Grosof, Sathya\nGunasekar, Jimmy Lu, Michael Uhlar, Jim Carrig, Nathan Beckmann,\nMor Harchol-Balter, and Gregory R. Ganger. 2020. The CacheLib\nCaching Engine: Design and Experiences at Scale. In 14th USENIX\nSymposium on Operating Systems Design and Implementation (OSDI 20).\nUSENIX Association, 753‚Äì768. https://www.usenix.org/conference/\nosdi20/presentation/berg\n[13] Prabuddha Chakraborty and Swarup Bhunia. 2022. BINGO: brain-\ninspired learning memory. Neural Computing and Applications 34 (02\n2022), 1‚Äì25. https://doi.org/10.1007/s00521-021-06484-8\n[14] Anne GE Collins and Michael J Frank. 2013. Cognitive control over\nlearning: creating, clustering, and generalizing task-set structure. Psy-\nchological review 120, 1 (2013), 190.\n[15] Georgios Detorakis, Travis Bartley, and Emre Neftci. 2019. Contrastive\nHebbian learning with random feedback weights. Neural Networks\n114 (2019), 1‚Äì14. https://doi.org/10.1016/j.neunet.2019.01.008[16] William Fedus, Prajit Ramachandran, Rishabh Agarwal, Yoshua Bengio,\nHugo Larochelle, Mark Rowland, and Will Dabney. 2020. Revisiting\nFundamentals of Experience Replay. In Proceedings of the 37th Interna-\ntional Conference on Machine Learning (ICML‚Äô20). JMLR.org, Article\n287, 11 pages.\n[17] Brad Fitzpatrick. 2004. Distributed caching with memcached. Linux\nJournal (August 2004).\n[18] Milad Hashemi, Kevin Swersky, Jamie Smith, Grant Ayers, Heiner Litz,\nJichuan Chang, Christos Kozyrakis, and Parthasarathy Ranganathan.\n2018. Learning Memory Access Patterns. In Proceedings of the 35th\nInternational Conference on Machine Learning (Proceedings of Machine\nLearning Research, Vol. 80), Jennifer Dy and Andreas Krause (Eds.).\nPMLR, 1919‚Äì1928.\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep\nresidual learning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition. 770‚Äì778.\n[20] Sepp Hochreiter and J√ºrgen Schmidhuber. 1997. Long Short-Term\nMemory. Neural Comput. 9, 8 (nov 1997), 1735‚Äì1780. https://doi.org/\n10.1162/neco.1997.9.8.1735\n[21] Heiko Hoffmann. 2019. Sparse Associative Memory. Neu-\nral Computation 31, 5 (05 2019), 998‚Äì1014. https://doi.org/\n10.1162/neco_a_01181 arXiv:https://direct.mit.edu/neco/article-\npdf/31/5/998/1052590/neco_a_01181.pdf\n[22] Sudarsun Kannan, Ada Gavrilovska, Vishal Gupta, and Karsten Schwan.\n2017. HeteroOS: OS Design for Heterogeneous Memory Management\nin Datacenter. SIGARCH Comput. Archit. News 45, 2 (jun 2017), 521‚Äì534.\nhttps://doi.org/10.1145/3140659.3080245\n[23] Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and\nChristopher Kanan. 2018. Measuring catastrophic forgetting in neural\nnetworks. In Proceedings of the AAAI conference on artificial intelligence,\nVol. 32.\n[24] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis.\n2018. The Case for Learned Index Structures. In Proceedings of the 2018\nInternational Conference on Management of Data (Houston, TX, USA)\n(SIGMOD ‚Äô18). Association for Computing Machinery, New York, NY,\nUSA, 489‚Äì504. https://doi.org/10.1145/3183713.3196909\n[25] Dharshan Kumaran, Demis Hassabis, and James L McClelland. 2016.\nWhat Learning Systems do Intelligent Agents Need? Complementary\nLearning Systems Theory Updated. Trends in Cognitive Sciences 20, 7\n(2016), 512‚Äì524. https://doi.org/10.1016/j.tics.2016.05.004\n[26] Aapo Kyrola, Guy Blelloch, and Carlos Guestrin. 2012. GraphChi:\nLarge-Scale Graph Computation on Just a PC. In Proceedings of the 10th\nUSENIX Conference on Operating Systems Design and Implementation\n(Hollywood, CA, USA) (OSDI‚Äô12). USENIX Association, USA, 31‚Äì46.\n[27] Seung-seob Lee, Yanpeng Yu, Yupeng Tang, Anurag Khandelwal, Lin\nZhong, and Abhishek Bhattacharjee. 2021. MIND: In-Network Memory\nManagement for Disaggregated Data Centers. In Proceedings of the\nACM SIGOPS 28th Symposium on Operating Systems Principles (Virtual\nEvent, Germany) (SOSP ‚Äô21). Association for Computing Machinery,\nNew York, NY, USA, 488‚Äì504. https://doi.org/10.1145/3477132.3483561\n[28] Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-\nTak Zhang. 2017. Overcoming catastrophic forgetting by incremental\nmoment matching. Advances in neural information processing systems\n30 (2017).\n[29] Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet,\nand Tom Goldstein. 2017. Training Quantized Nets: A Deeper\nUnderstanding. In Advances in Neural Information Processing Sys-\ntems, I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fer-\ngus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran Asso-\nciates, Inc. https://proceedings.neurips.cc/paper_files/paper/2017/file/\n1c303b0eed3133200cf715285011b4e4-Paper.pdf\n59\n\n[30] Xinjian Long, Xiangyang Gong, Bo Zhang, and Huiyang Zhou. 2023.\nDeep learning based data prefetching in CPU-GPU unified virtual\nmemory. J. Parallel and Distrib. Comput. 174 (Apr 2023), 19‚Äì31. https:\n//doi.org/10.1016/j.jpdc.2022.12.004\n[31] Hasan Al Maruf, Hao Wang, Abhishek Dhanotia, Johannes Weiner,\nNiket Agarwal, Pallab Bhattacharya, Chris Petersen, Mosharaf Chowd-\nhury, Shobhit Kanaujia, and Prakash Chauhan. 2022. Tpp: Transpar-\nent page placement for cxl-enabled tiered memory. arXiv preprint\narXiv:2206.02878 (2022).\n[32] James L McClelland, Bruce L McNaughton, and Randall C O‚ÄôReilly.\n1995. Why there are complementary learning systems in the hip-\npocampus and neocortex: insights from the successes and failures of\nconnectionist models of learning and memory. Psychological review\n102, 3 (1995), 419.\n[33] B. L. McNAUGHTON, C. A. Barnes, J. L. Gerrard, K. Gothard,\nM. W. Jung, J. J. Knierim, H. Kudrimoti, Y. Qin, W. E. Skaggs, M.\nSuster, and K. L. Weaver. 1996. Deciphering The Hippocampal Poly-\nglot: the Hippocampus as a Path Integration System. Journal of\nExperimental Biology 199, 1 (01 1996), 173‚Äì185. https://doi.org/\n10.1242/jeb.199.1.173 arXiv:https://journals.biologists.com/jeb/article-\npdf/199/1/173/2532105/jexbio_199_1_173.pdf\n[34] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd.\n1999. The PageRank citation ranking: Bringing order to the web. Techni-\ncal Report. Stanford InfoLab.\n[35] Edmund Rolls. 2018. The storage and recall of memories in the\nhippocampo-cortical system. Cell and Tissue Research 373 (09 2018).\nhttps://doi.org/10.1007/s00441-017-2744-3\n[36] Edmund T. Rolls. 2016. Pattern separation, completion, and categorisa-\ntion in the hippocampus and neocortex. Neurobiology of Learning and\nMemory 129 (2016), 4‚Äì28. https://doi.org/10.1016/j.nlm.2015.07.008\nPattern Separation and Pattern Completion in the Hippocampal Sys-\ntem.\n[37] Geet Sethi, Bilge Acun, Niket Agarwal, Christos Kozyrakis, Caroline\nTrippel, and Carole-Jean Wu. 2022. RecShard: Statistical Feature-Based\nMemory Optimization for Industry-Scale Neural Recommendation. In\nProceedings of the 27th ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems (Lausanne,\nSwitzerland) (ASPLOS ‚Äô22). Association for Computing Machinery,\nNew York, NY, USA, 344‚Äì358. https://doi.org/10.1145/3503222.3507777\n[38] Mohammad Shahrad, Rodrigo Fonseca, √ç√±igo Goiri, Gohar Chaudhry,\nPaul Batum, Jason Cooke, Eduardo Laureano, Colby Tresness, Mark\nRussinovich, and Ricardo Bianchini. 2020. Serverless in the Wild:\nCharacterizing and Optimizing the Serverless Workload at a Large\nCloud Provider. In Proceedings of the 2020 USENIX Conference on Usenix\nAnnual Technical Conference (USENIX ATC‚Äô20). USENIX Association,\nUSA, Article 14, 14 pages.\n[39] Yizhou Shan, Yutong Huang, Yilun Chen, and Yiying Zhang. 2018.\nLegoos: A disseminated, distributed {OS}for hardware resource disag-\ngregation. In 13th{USENIX}Symposium on Operating Systems Design\nand Implementation ( {OSDI}18). 69‚Äì87.\n[40] Zhan Shi, Akanksha Jain, Kevin Swersky, Milad Hashemi,\nParthasarathy Ranganathan, and Calvin Lin. 2021. A Hierar-\nchical Neural Model of Data Prefetching. In Proceedings of the\n26th ACM International Conference on Architectural Support for\nProgramming Languages and Operating Systems (Virtual, USA)\n(ASPLOS ‚Äô21). Association for Computing Machinery, New York, NY,\nUSA, 861‚Äì873. https://doi.org/10.1145/3445814.3446752\n[41] Zhenyu Song, Daniel S. Berger, Kai Li, and Wyatt Lloyd. 2020. Learning\nRelaxed Belady for Content Distribution Network Caching. In Proceed-\nings of the 17th Usenix Conference on Networked Systems Design and\nImplementation (Santa Clara, CA, USA) (NSDI‚Äô20). USENIX Association,\nUSA, 529‚Äì544.[42] Marijn F. Stollenga, Wonmin Byeon, Marcus Liwicki, and Juergen\nSchmidhuber. 2015. Parallel Multi-Dimensional LSTM, with Applica-\ntion to Fast Biomedical Volumetric Image Segmentation. In Proceedings\nof the 28th International Conference on Neural Information Processing\nSystems - Volume 2 (Montreal, Canada) (NIPS‚Äô15). MIT Press, Cam-\nbridge, MA, USA, 2998‚Äì3006.\n[43] Elvira Teran, Zhe Wang, and Daniel A. Jim√©nez. 2016. Perceptron\nlearning for reuse prediction. In 2016 49th Annual IEEE/ACM In-\nternational Symposium on Microarchitecture (MICRO). 1‚Äì12. https:\n//doi.org/10.1109/MICRO.2016.7783705\n[44] Simon Thomann, Paul R. Genssler, and Hussam Amrouch. 2022.\nHW/SW Co-design for Reliable In-memory Brain-inspired Hyper-\ndimensional Computing. https://doi.org/10.48550/ARXIV.2202.04789\n[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is all you need. Advances in neural information processing\nsystems 30 (2017).\n[46] Lennart Wittkuhn, Samson Chien, Sam Hall-McMaster, and Nicolas W.\nSchuck. 2021. Replay in minds and machines. Neuroscience & Biobehav-\nioral Reviews 129 (2021), 367‚Äì388. https://doi.org/10.1016/j.neubiorev.\n2021.08.002\n[47] Juncheng Yang, Yao Yue, and K. V. Rashmi. 2021. A Large-Scale\nAnalysis of Hundreds of In-Memory Key-Value Cache Clusters at\nTwitter. ACM Trans. Storage 17, 3, Article 17 (aug 2021), 35 pages.\nhttps://doi.org/10.1145/3468521\n[48] Shimeng Yu, Wonbo Shim, Xiaochen Peng, and Yandong Luo. 2021.\nRRAM for compute-in-memory: From inference to training. IEEE\nTransactions on Circuits and Systems I: Regular Papers 68, 7 (2021),\n2753‚Äì2765.\n[49] Xuejiao Zhao, Huanhuan Chen, Zhenchang Xing, and Chunyan Miao.\n2020. Brain-inspired Search Engine Assistant based on Knowledge\nGraph. https://doi.org/10.48550/ARXIV.2012.13529\n60",
  "textLength": 42168
}