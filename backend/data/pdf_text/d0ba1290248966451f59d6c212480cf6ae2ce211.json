{
  "paperId": "d0ba1290248966451f59d6c212480cf6ae2ce211",
  "title": "Cuckoo index",
  "pdfPath": "d0ba1290248966451f59d6c212480cf6ae2ce211.pdf",
  "text": "MIT Open Access Articles\nCuckoo index\nThe MIT Faculty has made this article openly available. Please share\nhow this access benefits you. Your story matters.\nCitation: Kipf, Andreas, Chromejko, Damian, Hall, Alexander, Boncz, Peter and Andersen, David \nG. 2020. \"Cuckoo index.\" 13 (13).\nAs Published: 10.14778/3424573.3424577\nPublisher: VLDB Endowment\nPersistent URL: https://hdl.handle.net/1721.1/136700\nVersion: Final published version: final published article, as it appeared in a journal, conference \nproceedings, or other formally published context\nTerms of use: Creative Commons Attribution-NonCommercial-NoDerivs License\n\n\nCuckoo Index: A Lightweight Secondary Index Structure\nAndreas Kipf1⇤, Damian Chromejko2, Alexander Hall3⇤,\nPeter Boncz4, David G. Andersen5⇤\n1MIT CSAIL2Google3RelationalAI4CWI5CMU\nkipf@mit.edu dchromejko@google.com alex.hall@relational.ai boncz@cwi.nl dga@cs.cmu.edu\nABSTRACT\nIn modern data warehousing, data skipping is essential for\nhigh query performance. While index structures such as B-trees or hash tables allow for precise pruning, their large\nstorage requirements make them impractical for indexing\nsecondary columns. Therefore, many systems rely on ap-\nproximate indexes such as min/max sketches (ZoneMaps) orBloom ﬁlters for cost-e↵ective data pruning. For example,Google PowerDrill skips more than 90% of data on average\nusing such indexes.\nIn this paper, we introduce Cuckoo Index (CI), an approx-\nimate secondary index structure that represents the many-\nto-many relationship between keys and data partitions in a\nhighly space-e\u0000cient way. At its core, CI associates variable-\nsized ﬁngerprints in a Cuckoo ﬁlter with compressed bitmapsindicating qualifying partitions. With our approach, we tar-get equality predicates in a read-only (immutable) settingand optimize for space e\u0000ciency under the premise of prac-\ntical build and lookup performance.\nIn contrast to per-partition (Bloom) ﬁlters, CI produces\ncorrect results for lookups with keys that occur in the data.\nCI allows to control the ratio of false positive partitions for\nlookups with non-occurring keys. Our experiments with\nreal-world and synthetic data show that CI consumes sig-niﬁcantly less space than per-partition ﬁlters for the samepruning power for low-to-medium cardinality columns. Forhigh cardinality columns, CI is on par with its baselines.\nPVLDB Reference Format:\nAndreas Kipf, Damian Chromejko, Alexander Hall, Peter Boncz,\nand David G. Andersen. Cuckoo Index: A Lightweight Secondary\nIndex Structure. PVLDB ,1 3 ( 1 3 ) :3 5 5 9 - 3 5 7 2 ,2 0 2 0 .\nDOI: https://doi.org/10.14778/3424573.3424577\n1. INTRODUCTION\nMany multi-level or sharded storage systems use sketches\nsuch as Bloom ﬁlters [7] to reduce the number of accesses\nneeded to ﬁnd data items of interest. Examples include\n⇤Work done while at Google.\nThis work is licensed under the Creative Commons Attribution-\nNonCommercial-NoDerivatives 4.0 International License. To view a copy\nof this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. For\nany use beyond those covered by this license, obtain permission by emailing\ninfo@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, V ol. 13, No. 13\nISSN 2150-8097.\nDOI: https://doi.org/10.14778/3424573.3424577the LSM-tree LevelDB [1] and PowerDrill [21]. In each of\nthese cases, searching for a data item dmay have to look at\nmany di↵erent “stripes” (chunks of data) where dcould be\nstored, and the goal is to reduce the number of stripes that\nmust be touched. Bloom ﬁlters and related approximate\nset-membership data structures are a good match for thisproblem, as they combine e \u0000cient space use with one-sided\nerror appropriate for the problem at hand: The ﬁlter may\noccasionally require accessing a stripe that does not contain\nthe data, but will always return a “true positive” result ifthe data is indeed there.\nWhile most systems maintain one Bloom ﬁlter per stripe\nand may test each of them at query time, SlimDB [37] (an\nLSM-tree) suggests replacing per-level Bloom ﬁlters with a\nsingle Cuckoo ﬁlter [15] that maps each key ﬁngerprint to themost recent level containing that key. Using this approach,SlimDB avoids lookups in multiple such ﬁlters. While this is\na valid strategy for LSM-trees where data items (i.e., keys)\nare only associated with a single level, it does not handle the\ngeneral case where data items may occur in multiple stripes.\nIn this paper, we extend this idea to the case of index-\ning secondary columns by associating each key indexed in a\nCuckoo ﬁlter [15] with a bitmap indicating qualifying stripes\n(i.e., stripes containing that data item). We term this newdata structure Cuckoo Index (CI). In contrast to a Cuckooﬁlter that uses a ﬁxed number of ﬁngerprint bits per key,\nCI stores variable-sized ﬁngerprints to avoid key shadow-\ning[39]. A ﬁngerprint stored in CI uniquely identiﬁes its\nkey. With this design, CI provides correct results for lookups\nwith keys that occur in the data (positive lookups). Aswith all schemes that do not store full keys, there can be\nfalse positives for lookups with non-occurring keys (nega-\ntive lookups). To mitigate the impact of those, CI allows toﬁne-tune the expected number of false positive stripes.\nWe store the variable-sized ﬁngerprints in a space-e\u0000cient\nblock-based storage. Each block stores (bitpacked) ﬁnger-prints with a ﬁxed number of bits. Additional bitmaps in-dicate the membership of a certain ﬁngerprint in a block.Further, CI maximizes primary bucket assignments at con-\nstruction time, which reduces space consumption.\nFor low-to-medium cardinality columns, CI is signiﬁcantly\nsmaller than per-stripe ﬁlters. The rationale behind thememory savings is simple: for secondary columns such as“country” each data item such as “US” will only be indexed\nonce in CI while previously “US” would have been stored\nin multiple per-stripe ﬁlters. The extra space required bythe bitmaps is insigniﬁcant compared to the space savingsby only indexing each data item once.\n3559\n\n\u0004>>PM<>T\n\u0016K<>@\u0006\f\u000b<NC\u0003\u0017<=G@\n\u0005GJJH\u0003\tDGO@MN\n\u001dJI@\u0010<KN\u0003\u0016\u0010\u0004N \n\u0011J\u0003\fI?@SFigure 1: Cuckoo Index (CI) strikes a good balance\nbetween space and pruning power.\nIn our work, we assume an analytical storage system such\nas Mesa [20] or PowerDrill [21] that partitions columns intostripes of up to 65 K rows. Further, we assume a secondary\nindexing setting in which we cannot control the data place-\nment (sort order). Data may be sorted by the primary key,by arrival time, or not at all, and we will show the e↵ect ofdi↵erent sort orders on ﬁlter sizes and pruning power. Our\nfocus is on improving the space e\u0000ciency (i.e., the trade-o ↵\nbetween space and pruning power) of the ﬁlter. We assume\nthat the costs for retrieving a false positive stripe from stor-\nage and scanning it in memory dominate performance. Wefurther restrict the scope of this work to the “write once-read\nmany” case common in both warehousing and log-structured\nsystems, where data is only written once into an immutablestate. Finally, we target equality predicates (e.g., countryequals “US”) which are very common in our use cases.\nWe evaluate CI with real-world (IMDb [30] and DMV [2])\nand synthetic data and show that in many cases it con-sumes less space for the same pruning power than existingapproaches. For example, for a medium cardinality column(with 6.34% of values being unique) and 8 K rows per stripe,\nCI consumes 23.7% less space than the best baseline, per-\nstripe Xor ﬁlters [19].\nIn summary, CI creates a new Pareto-optimal solution to\nthe data skipping problem, striking a good balance between\nspace e\u0000 ciency and pruning power (cf. Figure 1).\nContributions. Our contributions include:\n•A new lightweight secondary index structure with large\nspace savings for low-to-medium cardinality columns\nand on-par space consumption for high cardinalities\ncompared to per-stripe ﬁlters.\n•A new heuristic-based construction of Cuckoo hash ta-bles that maximizes the ratio of items assigned to theirprimary bucket.\n•A comparison of this heuristic against an optimal so-\nlution that maximizes primary bucket assignments by\nsolving a maximum weight matching problem.\n•An unaligned bitmap compression scheme that is more\nspace e\u0000cient than Roaring [8] in many cases while still\nallowing for partial decompression.•An open-source implementation of our approach and abenchmarking framework for (approximate) secondaryindexes. Both artifacts are available on GitHub\n1.\n•An evaluation of our approach using this frameworkon real-world and synthetic datasets.\n2. BACKGROUND\nIn this section, we describe the storage layout used in this\nwork, provide a problem deﬁnition, and give some back-ground on Cuckoo ﬁlters.\nStorage Layout. On a high level, we assume a PAX-style\nstorage layout [5] in which a table is horizontally partitioned\ninto segments (essentially ﬁles stored in distributed stor-\nage), which are organized in a columnar fashion (i.e., same\nattributes are grouped together) for better cache localityand compression. Such a data layout is typical for clouddatabases such as Snowﬂake [10].\nIn our setting, a segment typically consists of 1-5 M rows.\nEach column in a segment is further divided into logical“stripes”, consisting of up to 2\n16(65,536) rows each. We\nconsider a stripe the unit of access, meaning we always scanall values within a stripe. In other words, an index would\nallow us to skip over stripes, but not further reduce scan\nranges within a stripe. Throughout this paper, our focus is\non indexing a single segment (ﬁle). We further assume thatthe ﬁle API allows us to retrieve individual stripes from stor-age without reading the entire ﬁle. Such an API is supported\nby cloud storage systems such as Amazon S3.\nProblem Deﬁnition. We deﬁne Sas the set of stripes that\nrepresents a column within a segment. Given a data item d\nand a set of stripes S, the task is identify a subset R✓S\nof stripes that potentially contain d(i.e., false positives are\nallowed), without missing out on stripes that docontain d\n(i.e., no false negatives). The goal is to make Rcontain as\nfew stripes as possible. In the best case, R=Cwith C✓S\nbeing the stripes that actually contain d(true positives).\nWe measure the quality of an index using the scan rate\nmetric. We deﬁne the scan rate as the ratio of false positive\nstripes FP✓Rto true negative stripes TN✓S(|TN |>0):\nscan rate =|FP |\n|TN |(1)\nIn other words, the scan rate is the ratio of true negativestripes that is misclassiﬁed as positive. The idea behind\ndeﬁning scan rate like this is to align it with the false positive\nrate of per-stripe ﬁlters, assuming that lookups in di↵erentﬁlters are independent of each other. When |TN |=0 ,i . e . ,\nwhen a given value is contained all stripes, we set the scan\nrate to zero.\nEach false positive stripe fp2FPcomes with a penalty,\nmost notably an unnecessary I/O. For queries that only se-\nlect a few stripes (i.e., are very selective), an e↵ective indexcan save a lot of unnecessary work, potentially leading to\nlarge speedups in overall query performance.\nFor practical purposes, we want the index structure to be\nsigniﬁcantly smaller than the compressed column. Thus, the\noverarching goal is to strike a good balance between space\nconsumption and pruning power. Lookups in this structure\nshould still be practical, i.e., signiﬁcantly faster than a diskI/O which we assume to be in the order of milliseconds.\n1https://github.com/google/cuckoo-index/\n3560\n\n»¼»¼» ¼»¼»»\n»» »¼ ¼» ¼¼£S\u0003T¤\n=P>F@O\u0003DI?@S \u0003ADIB@MKMDIOCS \u0003Ó\u0003 »¼»¼»¼»\u0003 CT \u0003Ó\u0003 ¼¼¼»¼»»\u0003Figure 2: A Cuckoo ﬁlter.\nCuckoo Filter. Similar to a Bloom ﬁlter [7], a Cuckoo\nﬁlter [15] is a data structure that can answer approximate\nset-membership queries: Given a lookup key, the ﬁlter de-termines whether the key might be contained in the set or is\ndeﬁnitely not part of the set. Like a Cuckoo hash table [36],\naC u c k o oﬁ l t e rc o n s i s t so fm buckets. Figure 2 shows an ex-\nample of a ﬁlter with four buckets. In contrast to a Cuckoohash table, a Cuckoo ﬁlter does not store full keys but small\n(e.g., 8-bit) ﬁngerprints that are extracted from hashes:\nfingerprint (key)=extract (hash( key)) (2)\nFor example, the least signiﬁcant bits of the hash value can\nbe used as ﬁngerprint bits.\nSimilar to keys in a Cuckoo hash table, each ﬁngerprint\nhas two possible buckets that it can be placed in: its primaryand its secondary bucket. If a bucket is full, the insertion\nalgorithm “kicks” a random item to its alternate bucket and\ninserts the to-be-inserted item into the freed spot. There isa (conﬁgurable) maximum number of kicks that are allowedto occur during an insert after which construction fails. In\nsuch cases, a ﬁlter needs to be rebuilt from scratch with a\nmore conservative conﬁguration (i.e., more buckets).\nCompared to a Cuckoo hash table that uses two inde-\npendent hash functions on full keys, a Cuckoo ﬁlter usespartial-key cuckoo hashing [15] to ﬁnd an alternative bucket\ni\n2for ﬁngerprints stored in bucket i1:\ni2=hash( fingerprint )\u0000i1 (3)\nHowever, the restriction of not being able to use two in-dependent hash functions only applies in an online setting,i.e., already inserted items are no longer accessible. In thiswork, we are targeting the o ✏ine setting where all items are\navailable at once, hence we are free to use independent hashfunctions on the full keys.\nA Cuckoo ﬁlter typically reserves space for multiple items\nper bucket. This parameter is called bucket size b(or num-\nber of slots). With b= 1, the maximum load factor of the\nﬁlter (ratio of occupied slots) is 50% and increases to 84%and 95% with b=2a n d b= 4, respectively [15].\n3. CUCKOO INDEX\nIn this section, we ﬁrst provide an overview over the design\nof Cuckoo Index (CI) before describing several optimizationsto reduce space consumption and increase pruning power.\nWe note that all design decisions are made with practical\nbuild and probe performance in mind.\n6WULSH\u0003\u0013\n6WULSH\u0003\u0014\n6WULSH\u0003\u0015\n6WULSH\u0003\u0016Figure 3: A Cuckoo Index is built for each column\nin a data ﬁle (segment). Each ﬁngerprint in the\nCuckoo ﬁlter is associated with a stripe bitmap thatindicates qualifying stripes. The ﬁgure illustrates a\nquery for the country “US”.\n3.1 Overview\nOn a high level, CI consists of a Cuckoo ﬁlter where each\nkey ﬁngerprint is associated with a ﬁxed-size bitmap indi-\ncating qualifying stripes as shown in Figure 3. In contrast\nto traditional per-stripe ﬁlters, in this design there is only a\nsingle index per column and segment. To query CI, we ﬁrst\nprobe its Cuckoo ﬁlter with a hash of the lookup key. The\nﬁlter lookup returns the o↵set of the matching ﬁngerprint\nstored in the ﬁlter (if such a ﬁngerprint exists) which we thenuse to locate the corresponding bitmap. Note that there isa one-to-one mapping between ﬁngerprints and bitmaps.\nFor low cardinality columns, such as “country”, having\nonly one such ﬁlter per column has the advantage that ev-ery unique value will only be indexed once. For example,assume that “US” is contained in every single stripe. Withthe per-stripe ﬁlter design, “US” would occupy around 10bits per stripe (assuming a 1% target false positive rate).\nWith say 20 stripes, it would require 200 bits. With CI’s\ndesign, space consumption would be reduced to 10 bits forstoring “US” in the ﬁlter and another 20 bits for the corre-\nsponding bitmap and thus save 85% in memory. For high\ncardinality columns, this design may consume more memory\nthan the per-stripe ﬁlter design. However, this e↵ect can bemitigated by cross-optimizing ﬁngerprints and bitmaps aswe will show later. Besides the potentially large space sav-ings, another advantage of this design is that lookups only\nneed to probe a single ﬁlter.\nOn the other hand, this design also introduces a few chal-\nlenges. When inserting entries into a Cuckoo ﬁlter, there can\nbe ﬁngerprint collisions among keys. That occurs when two\nkeys share the same bucket and have the same ﬁngerprint.\nIn a regular Cuckoo ﬁlter, we would simply notinsert the\nredundant ﬁngerprint. In CI, however, each ﬁngerprint hasan associated value (i.e., a bitmap). Thus, we need a strat-egy to resolve such collisions. We discuss di↵erent options\nand their trade-o ↵s in Section 3.2.\nAnother challenge is the representation of bitmaps. The\nmost straightforward approach would be to allocate a sepa-\nrate array of uncompressed bitmaps that has the same size\n(i.e., number of entries) as the array that represents the\nCuckoo ﬁlter, enabling a simple o ↵set access from one into\nthe other (cf. Figure 3). There are many possible optimiza-\ntions to this design, such as bitmap compression or adding\na level of indirection and only maintaining distinct bitmaps.\nWe discuss our bitmap encoding scheme in Section 3.6.\n3561\n\nFigure 4: Storing 64-bit hashes of colliding keys in\nas e c o n d a r yt a b l et oe n s u r eac o l l i s i o n - f r e ep r i m a r y\ntable that only stores short key ﬁngerprints.\nOne of our key techniques is to cross-optimize (i.e., holis-\ntically optimize) ﬁngerprints and bitmaps to satisfy a givenscan rate (cf. Section 2). The idea is to store fewer ﬁn-\ngerprint bits (for false positive reduction) for ﬁngerprintsassociated with sparse bitmaps, and likewise use more bits\nfor ﬁngerprints associated with dense bitmaps. The ratio-\nnale behind that is that for sparse bitmaps we need to scanfew stripes anyways, thus a false positive that matches witha sparse bitmap is less expensive than a false positive that\nmatches with a dense bitmap.\nWe choose a Cuckoo over a Bloom ﬁlter for the following\nreasons: (i) a Cuckoo ﬁlter has a lower space consumption\nthan a Bloom ﬁlter for false positive rates under 3% [15],\nand (ii) in a Cuckoo ﬁlter, it is straightforward to associate\nvalues (bitmaps) with keys (ﬁngerprints). We note that the\nrecently proposed Xor ﬁlter [19] was not yet available at thedesign phase of this work. However, we expect it to havesimilar drawbacks as Invertible Bloom Lookup Table [18]\n(cf. Section 5) when associating it with bitmaps, including\nthe possibility of unsuccessful lookups.\n3.2 Collision Handling\nWe consider three di↵erent strategies to resolve ﬁngerprint\ncollisions. While Union Bitmaps introduces false positives,\nSecondary Table andVariable-Sized Fingerprints guar-\nantee correct results for positive lookups. That is, for a\nlookup key that exists in the data, the latter two strategies\nalways return the correct set of stripes, while Union Bitmaps\nmay return false positives.\nUnion Bitmaps. This approach solves ﬁngerprint colli-\nsions solely on the bitmap side. When we ﬁnd that a key\nﬁngerprint already exists in the ﬁlter, we union the bitmaps\nof the new and the existing ﬁngerprint. Such a conﬂict res-\nolution would not be possible if we would associate each\nﬁngerprint with a regular value such as an SSTable ID in anLSM-tree [37] (i.e., one cannot union IDs without requiring\nextra space). While this strategy does not require any ex-tra space, it introduces additional false positive stripes: Inthe event that a lookup maps to a bitmap that has beenunioned with another bitmap, we may introduce false pos-\nitive stripes. In the worst case, we may access S\u00001 false\npositive stripes with Sbeing the total number of stripes\n(which is the case when a bitmap with one set bit is com-bined with a bitmap with Sset bits). Since we aim for\nmaximum pruning power, we now study strategies that do\nnot introduce false positive stripes.\n<\u0003»»\n=\u0003¼»\n>\u0003»¼<\u0003»»\n=\u0003¼»\n>\u0003»¼\n\u0005P>F@O\u0003»\u000e@T\u0003\u0003\u000b<NC\u0003\u0003\u0013MDH<MT\u0003\u0003\u0016@>JI?<MT\n\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003 \u0005P>F@O\u0003\u0003\u0003\u0005P>F@O\n<\u0003\u0003\u0003\u0003»»\u0003\u0003»\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003¼\n=\u0003\u0003\u0003\u0003¼»\u0003\u0003»\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003¼\n>\u0003\u0003\u0003\u0003»¼\u0003\u0003»\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003¼\n\u0005P>F@O\u0003¼Figure 5: Three keys indexed in a CI with two slots\nper bucket (bitmaps omitted from ﬁgure). All keys\nhave Bucket 0 as their primary bucket. Only cis\nplaced in its secondary bucket, aand bare stored in\ntheir primary bucket. While the blue bit would be\nsu\u0000cient to di↵ erentiate between aandb,w en e e dt o\nstore an extra bit (marked in green). Without this\nbit, a lookup for cwould falsely get matched with\na(a lookup always ﬁrst checks the primary bucket).\nHence, to determine the minimum number of ﬁn-\ngerprint bits for Bucket 0, we need to consider all\nkeys that have this bucket as primary bucket.\nColliding Entries in Secondary Table. The multi-level\nCuckoo ﬁlter in SlimDB [37] addresses the problem of col-\nliding ﬁngerprints by introducing a secondary table (a hash\ntable storing full keys) to store conﬂicting entries. Lookups\nﬁrst search the secondary table and only consult the pri-\nmary table (the Cuckoo ﬁlter) if the key was not found in\nthe secondary table. This approach not only a ↵ects lookup\nperformance, but also may require a signiﬁcant amount ofextra space for the secondary table. For example, assuminga key length of 128 bits and a collision probability of 1%,\nthe secondary table adds an overhead of 1.28 bits per key.\nThis storage overhead of course increases with longer keyssuch as strings.\nTo bound the required space overhead, we could only store\n64-bit hashes of keys in the secondary table (cf. Figure 4).Since our ﬁlter is intended as an immutable (“write once-read many”) data structure, we know all keys at build timeand can therefore ensure that their hashes are unique (i.e.,\nthere are no 64-bit hash collisions). In the unlikely event\nof a hash collision, we use a di↵erent hash function. This\noptimization allows us to e ↵ectively use this technique with\nstring keys. The increased lookup cost, however, remains.\nVariable-Sized Fingerprints. This design avoids the over-\nhead of a secondary table by storing collision-free, variable-\nsized ﬁngerprints in the primary table. We compute the\nminimum number of hash bits required to ensure uniqueﬁngerprints on a per-bucket basis. We make all ﬁngerprintsin a bucket share the same length (number of bits) for spacee\u0000ciency reasons.\nTo correctly match a lookup key with its stored ﬁnger-\nprint, we need to carefully determine the minimum numberof ﬁngerprint bits per bucket as shown in Figure 5. In par-ticular, we need to consider all items that have a certain\nbucket as primary bucket instead of only considering those\nthat are actually stored in that bucket. Otherwise, a lookupfor an item that is stored in its secondary bucket mightget falsely matched with an item in its primary bucket. Tomitigate the size impact of this e↵ect, it is important to\nmaximize the ratio of items that are stored in their primary\n3562\n\nbuckets since those do not inﬂuence the ﬁngerprint lengths\nof other buckets (as opposed to items stored in their sec-\nondary buckets). In Section 3.4 we will introduce two key-\nto-bucket-assignment algorithms that yield the optimal and\nan almost optimal primary ratio, respectively.\n3.3 Scan Rate Optimization\nAs mentioned earlier, we use the scan rate metric (cf. Sec-\ntion 2) to measure the quality of our index, which is the ratio\nof false positive to true negative stripes. While our variable-length ﬁngerprint design yields correct results for positive\nlookups (a guaranteed scan rate of 0%), it may return false\npositive stripes for lookups with non-existing keys.\nCI therefore takes the target scan rate (for the entire in-\ndex) as a parameter. Since optimizing the scan rate across\nall buckets can be expensive, we optimize the scan rate on a\nper-bucket basis. In particular, we may increase the ﬁnger-print length of a bucket such that the expected scan rate ofthat bucket stays below a certain threshold.\nConsidering a single ﬁngerprint/bitmap pair stored in a\ncertain bucket, there are two components that determinethe scan rate of this particular entry. First, every additionalﬁngerprint bit halves the probability that a random lookupﬁngerprint matches with the stored ﬁngerprint. Second, as-\nsuming the ﬁngerprints match, the associated bitmap can\ncause a further scan rate reduction. In the best case, the\nbitmap has only one bit set, limiting the number of false\npositive stripes to one. Hence, the formula for the expectedscan rate of a ﬁngerprint/bitmap pair is:\nlocal scan rate =1\n2ﬁngerprint bits⇤bitmap density (4)\nwithbitmap density being deﬁned as the ratio of set bits.\nFor high cardinality columns, we observe sparse bitmaps\n(depending on the data distribution among stripes) and thuswe tend to require fewer ﬁngerprint bits in these cases.\nAlgorithm 1 shows the complete process of determining\nthe number of ﬁngerprint bits for a certain bucket such that\nwe avoid ﬁngerprint collisions and satisfy a given target scan\nrate (for the entire index). We ﬁrst compute the bucketdensity of the table (i.e., the ratio of non-empty buckets):\ntable density =num non-empty buckets\nnum buckets(5)\nThen we determine the minimum number of hash bits toavoid ﬁngerprint collisions (cf. Section 3.2). Essentially, weincrease the number of ﬁngerprint bits until the ﬁngerprintsare unique. We now increase the number of hash bits un-til we satisfy the given target scan rate (cf. Lines 3 to 14).\nTo check for that condition, we compute the actual scan\nrate of the bucket by averaging the local scan rates of the\nindividual ﬁngerprint/bitmap pairs (cf. Lines 5 to 9). Wealso account for the table density: If a lookup “ends up” in\nan empty bucket, there is no probability of a false match\nat all and it will correctly be identiﬁed as a negative. Fur-thermore, we account for the fact that a lookup “checks”up to two buckets: The primary and the secondary bucketof the lookup key. By multiplying by two in this step, we\ntreat both lookups (primary and secondary) as independent\nrandom experiments. Thereby, we slightly overestimate theprobability of a false match since these lookups are actuallynot independent. That is, if a lookup matches with a ﬁn-\ngerprint in the primary bucket, it cannot also match with a\nﬁngerprint in the secondary bucket.Algorithm 1: Returns the minimum number ofﬁngerprint bits required to avoid ﬁngerprintcollisions and to satisfy a given scan rate.\nInput: keys,table,bucket, target scan rate\nOutput: num bits\n// Get bucket density of table (ratio of\nnon-empty buckets).\n1table density GetBucketDensity (table)\n// Get minimum number of hash bits to avoid\nfingerprint collisions of keys that havebucket as primary bucket.\n2num bits GetCollFreeHashLength( keys,bucket )\n3while truedo\n4 false match probability 1/2num bits\n// Compute scan rate of bucket by averaging\nlocal scan rates.\n5 sum scan rate 0.0\n6 forentry inbucket do\n7 bitmap density \nGetBitmapDensity (entry.bitmap)\n8 sum scan rate +=\nfalse match probability⇤bitmap density\n9 actual scan rate \nsum scan rate/bucket.num entries()\n// Account for table density (reduces\nactual scan rate based on the fact that\nlookups may end up in an empty bucket).\n10 actual scan rate *=table density\n// Double actual scan rate to account for\nthe secondary lookup.\n11 actual scan rate *= 2\n12 ifactual scan rate < =target scan ratethen\n13 break\n14 ++num bits\n15return num bits\n3.4 Assigning Keys to Buckets\nWe follow two goals when assigning keys to buckets. First,\nand most importantly, we want to ﬁnd a placement whereall keys have a bucket. To ensure that this is possible, weneed to size the Cuckoo table accordingly. That is, we need\nto allocate a su\u0000cient number of buckets to accommodate\nthe key set considering maximum load factors (that depend\non the number of slots per bucket, cf. Section 2). Second,\nwe aim to maximize theprimary ratio :\nprimary ratio =num keys in primary buckets\nnum keys(6)\nWe refer to items that reside in their primary or secondarybuckets as primary or secondary items, respectively. Witha high primary ratio, we mitigate the impact of secondary\nitems inﬂuencing the ﬁngerprint length of their primary buck-\nets (cf. Section 3.2).\nWe have experimented with three di↵erent algorithms.\nFirst, we use the well-known kicking procedure that is alsoused by the Cuckoo ﬁlter. The only di↵erence is that we\ndo not use partial-key Cuckoo hashing and instead use two\nindependent hash functions, assuming we have access to all\n3563\n\nTable 1: Performance of di ↵erent algorithms for\nkey-to-bucket assignment.\nAlgorithm AVG primary % AVG matching time\nKicking 64% 192ns\nMatching 77% 101µs\nBiased Kicking 76% 1.2µs\nkeys at build time. Second, we use a matching algorithm\nthat ﬁnds the optimal placement of keys that maximizes the\nprimary ratio (cf. Section 3.4.1). This approach is similar to\nprior work on using matching in a Cuckoo hashing context\nby Dietzfelbinger et al. [11] that aims to speed up lookups\nrather than decreasing footprint size. Lastly, in Section 3.4.2we introduce a biased kicking algorithm that achieves almost\noptimal primary ratios in short time. We now describe thematching and the biased kicking algorithms.\n3.4.1 Matching Algorithm\nThe kicking algorithm might produce a key-to-bucket as-\nsignment that assigns many keys to their secondary bucketor even fail to ﬁnd an assignment altogether. To mitigatethat, we model the problem as a minimum-cost unbalancedlinear assignment problem. In order to maximize the ratio of\nprimary items, we assign a cost of 1 to a matching between\na key and its primary bucket and a cost of 2 to a matchingwith its secondary bucket. The problem can be solved us-ing an algorithm proposed by Goldberg and Kennedy ([16],\nwith an optimization from [17]) in O(np\nnlog(n)), where n\nis the total number of keys and buckets.\n3.4.2 Biased Kicking Algorithm\nTo avoid the high runtime of the matching-based approach,\nwe modify the Cuckoo kicking algorithm to optimize the pri-\nmary ratio. In particular, we consider both the primary andthe secondary bucket of the current in-ﬂight item to ﬁnd asecondary item. We leave a small chance (an experimen-\ntally obtained likelihood) to kick primary items to resolve\ncycles. We set the maximum number of allowed kicks to asu\u0000ciently high value (50 K kicks) to mitigate the impact of\nbuild failures in practice. Using this algorithm, we achieve\nprimary ratios that are within 1% of the optimum found by\nthe matching algorithm.\nTable 1 and Figure 6 show the impact of using di↵ erent\nassignment algorithms. We ran the experiments with 1 Munique values and 1, 2, 4, and 8 slots per bucket, using a\nsingle thread on a machine equipped with an Intel Xeon\nSkylake CPU (3.70 GHz).\n3.5 Storing Variable-Sized Fingerprints\nTo store variable-sized ﬁngerprints in a compact manner,\nwe have devised a custom storage layout. Fingerprints canhave from 0 (including\n2) up to 64 bits, even though we rarely\nsee cases with more than 20 bits for real-world columns (as-suming a 1% target scan rate). Figure 7 shows an example\nwith six ﬁngerprints and three di↵erent bit widths.\nFixed-Bit-Width Blocks. For each bit width, we allocate\noneblock that stores ﬁngerprints of a ﬁxed bit width (in the\nexample, Block 0 stores all 1-bit ﬁngerprints). Each block\n2That is the case when there is only a single key that has a\nbucket as primary bucket.606770717378798586\n1 2 4 8\nNumber of slots per bucket% in primary bucketKicking\nBiased KickingMatching\nFigure 6: Performance of di↵erent algorithms for\nkey-to-bucket assignment, broken down by the num-ber of slots per bucket. Note the zoomed in y-axis.\n¼\tDIB@MKMDION\u0003\t\u0013N  ¼\u0003¼»¼\u0003»¼\u0003»\u0003»»¼\u0003¼\n\u0012AAN@ON »\u0003\u0003¼\u0003\u0003\u0003\u0003½\u0003\u0003\u0003¾\u0003\u0003¿\u0003\u0003\u0003\u0003À\n¾¾\n½\n½¼¼»¼\n¼»¼»»¼\n»¼¼»»¼»¼\n»¼»»¼»\n»»¼»»»\u0011PH\u0003\u0005DON \u0011PH\u0003\t\u0013N \u0005DOK<>F@?\u0003\t\u0013N \u0005GJ>F\u0003\u0005DOH<K\n¼»¼¼»»¼»¼\n¼\u0006JHK<>O@? \u0003\n\u0005GJ>F\u0003\u0005DOH<K\nFigure 7: Dense storage of six variable-sized ﬁn-\ngerprints in three di↵erent blocks. Each blockstores ﬁxed-bit-width ﬁngerprints in bitpacked for-mat. Block bitmaps indicate membership of a ﬁn-\ngerprint in a block. We compact block bitmaps by\n“leaving out” set bits in subsequent bitmaps. By\nordering blocks based on decreasing cardinality we\nincrease the e↵ect of this optimization.\nstores the bit width and the number of ﬁngerprints (both\n32-bit integers), followed by the bitpacked ﬁngerprints.\nBlock Bitmaps. To identify the block of a given slot in\nthe Cuckoo table, we maintain one bitmap per block. Each\nsuch block bitmap indicates which slots are stored in the\ncorresponding block. In addition, we maintain a bitmap\nthat indicates empty slots. When encoding the bitmaps,\nwe do not di↵erentiate between this bitmap and the actualblock bitmaps. The only di↵erence is that this bitmap doesnot have a corresponding block.\nTo retrieve a ﬁngerprint at a certain slot index ,w ec h e c k\nthe corresponding bit of all block bitmaps until we encountera set bit. We then extract the ﬁngerprint from the corre-sponding block. In particular, to compute the o ↵set of the\nﬁngerprint in the bitpacked storage, we perform a rank op-\neration on its block bitmap and multiply that count with thebit width of the block. To accelerate rank calls, we maintain\na rank lookup table that stores precomputed ranks (repre-\nsented as uint32\nt) for 512-bit blocks, which causes a space\noverhead of 6.25% [44].\nCompacting Block Bitmaps. Based on the insight that\nblock bitmaps are pairwise disjoint, we “leave out” setbits\nin subsequent bitmaps to save space (as shown in the middle\nof Figure 7). Using rank calls we can still reconstruct the\nbit o↵ set of a slot at lookup time.\nThe complete ﬁngerprint lookup algorithm (with com-\npacted block bitmaps) is shown in Algorithm 2.\n3564\n\nAlgorithm 2: Returns the ﬁngerprint stored in\nthe slot with the given ID.\nInput: blocks, block bitmaps, num blocks, slotidx\nOutput: fingerprint\n1forblock idxin1tonum blocks do\n2 block blocks[block idx]\n3 block bitmap block bitmaps[block idx]\n4 ifGetBit (block bitmap, slot idx)then\n5 ifIsInactive (block )then\n6 return NULL\n7 idxinblock \nRank (block bitmap, compacted slotidx)\n8 return GetFingerprint( block, idx inblock )\n// As we advance to the next block, offset\nslot ID by the number of slots present inthe current block.\n9 slotidx slotidx\u0000Rank (block bitmap, slot idx)\n3.6 Bitmap Encoding\nIn this section, we describe our approach to representing\nbitmaps in CI. We store one bitmap per slot in the Cuckootable allowing for an o↵set access between the two.\nBitmap compression is essential to reduce the footprint of\nCI on disk and in memory. We use a custom bitwise (un-aligned) run-length encoding (dense) scheme that can savesigniﬁcant space over word- or byte-aligned approaches such\nas WAH [41] or Roaring [8]. Our dense scheme consists of\ntwo vectors. The ﬁrst stores 8-bit “run lengths” that consistof one bit to di↵erentiate between a run and a literal, fol-lowed by 7 bits to denote the run length or the literal length.The second vector stores the compressed bit sequences. For\nsparse bitmaps that exceed an empirically obtained density\nthreshold, we use bitpacked position lists (sparse scheme).That is, we encode the positions of the set bits with as fewbits as possible.\nWe compress all bitmaps of a single CI at once: That is, we\nencode all bitmaps (back-to-back) in a single bitmap, whichwe call global bitmap , to achieve better compression. How-\never, to access individual bitmaps, we require fast randomaccess, without decompressing the entire global bitmap.\nTo allow for partial decompression, we maintain another\nvector that storesp\n#runs many skip o↵sets .O n e s u c h\no↵set entry contains the number of bits covered by the pre-\nviousp#runs run lengths and the corresponding number\nof bits in the compressed bit sequence (recall that we storethe run lengths and the compressed bits in two separate vec-tors). Thus, this structure e↵ectively allows us to skip over\nchunks of run length entries. This approach is similar to\nfence pointers proposed by UpBit [6] with two di↵erences:\nFirst, skip o ↵sets contain relative bit counts whereas fence\npointers contain absolute counts. Second, the size of ourstructure is relative to the size of the compressed bitmapwhile the size of fence pointers is relative to the size of the\nuncompressed bitmap. Hence, fence pointers favor lookup\nperformance (O (1) vs. O(p\n#runs )) whereas our structure\nfavors space consumption.\nFor on-disk storage, we additionally apply Zstandard com-\npression (zstd) [4] which further reduces the size and allowsfor e\u0000cient encoding and decoding.Table 2: Cardinalities of selected columns.\nDataset Column #rows Cardinality\nIMDb country code 3,538,744 218\nIMDb company name 3,538,744 224,385\nIMDb title 3,538,744 1,483,626\nDMV color 11,429,681 224\nDMV city 11,429,681 31,349\n4. EVALUATION\nWe evaluate our approach with IMDb, DMV, and syn-\nthetic data and compare against ZoneMaps and per-stripe\nﬁlters. In the following, we ﬁrst describe the experimentalsetup, before we study the space consumption of the individ-\nual indexes. We also show the impact of di↵erent workloads\n(varying ratios of positive/negative lookups) and sort orders.We further experiment with synthetic data and report buildand lookup times. Finally, we integrate CI into PostgreSQL\nand show its impact on query performance. We run all ex-\nperiments on a machine with an Intel Xeon Gold 6230 CPUand 256 GiB of RAM.\nBaselines. We compare our approach to ZoneMaps and to\nper-stripe approximate set-membership data structures. For\nthe latter, we choose a space-optimized Bloom ﬁlter as well\nas the recently announced Xor ﬁlter [19] as representatives.\nAZ o n e M a pm a i n t a i n st h em i n i m u ma n dt h em a x i m u m\nvalue of each stripe. Its space consumption is constant\nand independent from the concrete data distribution. A\nZoneMap is most e ↵ective when data is unique and sorted.\nA per-stripe ﬁlter is a collection of ﬁlters, and as the name\nsuggests, one per stripe. By default, we construct each per-stripe ﬁlter with 10 bits per element, which yields a falsepositive probability of around 1% and 0.3% for Bloom and\nXor, respectively. Thus, for a negative lookup, we expect\n1% or 0.3% of stripes being misclassiﬁed as positive (as-suming independence). As a Bloom ﬁlter implementation,\nwe use LevelDB’s built-in Bloom ﬁlter, which is a classi-\ncal space-optimized Bloom ﬁlter. In the following, we refer\nto this approach as Bloom. For the Xor ﬁlter, we use its\nopen-source implementation [3] and refer to it as Xor.\nDatasets. We have denormalized the IMDb dataset [30]\nfor our experiments. The resulting denormalized table has3.5 M rows and 14 columns. DMV [2] contains vehicle, snow-\nmobile, and boat registrations in New York State and con-\nsists of 11.9 M rows and 19 columns (we have excluded ex-cluded the unique vincolumn, which is a primary index can-\ndidate). To simplify the experimental setup, we dictionary-encode strings as dense integers in an order-preserving way.\nWe focus our evaluation on the following representative\ncolumns: For IMDb, we choose country\ncode, company name,\nand title, and for DMV, we choose color and city.T a -\nble 2 shows their cardinalities. The IMDb column title\nhas the highest ratio of unique values (41.9%). We use the\ndatasets in their original tuple order unless noted other-\nwise. In addition to this original order, we will study the\ncases when data is randomly shu ✏ed or sorted.\nStripe Sizes. We load these datasets with four di↵erent\nstripe sizes, ranging from 213(8,192) to 216(65,536) rows\nper stripe (cf. Table 3 for the number of stripes).\n3565\n\nTable 3: Number of stripes.\nRows per stripe 213214215216\nIMDb 431 215 107 53\nDMV 1,395 697 348 174\nThis parameter a ↵ects both the space consumption and\nthe pruning power of the di↵erent indexes. For example,\nCI’s per-ﬁngerprint bitmaps have as many bits as there arestripes. Thus, with fewer rows per stripe (and thus more\nstripes), the number of bits increases and CI may require\nmore space. Similarly, and in particular for low cardinal-ity columns, per-stripe ﬁlters may consume more space withfewer rows per stripe. Frequent values that occur in allstripes, are redundantly stored in every single per-stripe ﬁl-\nter. On the other hand, a lower number of rows per stripe\nalso means higher precision and has the advantage of lessoverhead in case of a false positive.\n4.1 Space Consumption\nWe evaluate the sizes of the individual ﬁlters on our rep-\nresentative columns for di↵erent stripe conﬁgurations. Weinstantiate both Bloom and Xor with 10 bits per elementwhich yields false positive stripe ratios (scan rates) of around\n1% and 0.3%, respectively. We use CI in its most space-\ne\u0000cient conﬁguration with one slot per bucket and a loadfactor of 49%. Recall that empty slots in CI are encoded us-ing a single bit. We further use the biased kicking approachto maximize the primary ratio of items, which minimizes in-\ndex size (cf. Section 3.4.2). To control the impact of lookups\nwith non-occurring keys, we instantiate CI with two di↵er-ent scan rates: 0.1% (CI/0.1) and 1% (CI/1). We leave out\nZoneMap from this experiment since its space consumption\nsolely depends on the number of stripes.\nIn addition to the in-memory sizes, we report the sizes\nof the ﬁlters when compressed with Zstandard (zstd) [4].\nCompressed ﬁlter sizes are interesting for data warehousinguse cases where ﬁlters are stored alongside the actual (com-\npressed) data on blob storage such as Google Cloud Storage.\nCompression may also reduce ﬁlter load times (I/O). We use\nzstd in its fastest mode (zstd=1) which still allows for decentcompression ratios in our experience.\nFigure 8a shows the ﬁlter sizes for the country\ncode IMDb\ncolumn. The dashed line in the plot indicates 2% of the zstd-compressed column size. This column contains 218 uniquevalues, representing 0.01% of the entire column. With more\nrows per stripe, the space consumption decreases for all ap-\nproaches. Notably, CI consumes less space than Bloom andXor for all stripe sizes. With zstd compression, the sizesof Xor and CI are further reduced. For example, with 2\n13\nrows per stripe, Xor and CI/1 are 1.25 ⇥and 1.13⇥ smaller\nwhen zstd-compressed, respectively. The reason that Xorcompresses so well is its sparse table layout. Likewise, CI’sbitmaps have additional compression potential. Bloom, onthe other hand, does not beneﬁt from compression. This\nis expected due to its already high entropy. When zstd-\ncompressed, CI/1 consumes 8.90 ⇥(2\n13rows per stripe) and\n6.64⇥ (216) less space than Xor, which achieves a compa-\nrable scan rate. The ﬁlter sizes for the medium cardinality\ncolumn company name are shown in Figure 8b. With zstd-\ncompression, CI/1 is again smaller than Xor in all cases.Table 4: Total size in MiB of compressed ﬁlters for\nIMDb (14 columns) and DMV (19 columns).\nDataset CI/0.1 CI/1 Bloom Xor\nIMDb (213)5 . 3 9 5.12 6.63 6.45\nIMDb (216)3 . 9 0 3.26 4.10 3.99\nDMV (213)1 . 6 6 1.63 6.11 7.45\nDMV (216)0 . 6 4 0.60 1.70 1.85\nFor example, for 213rows per stripe, CI/1 and Xor consume\n1,121 KiB and 1,470 KiB, respectively. For the high cardi-nality column title the results look di↵erent (cf. Figure 8c).\nHere, CI/1 consumes more space than Xor. However, we ar-\ngue that CI is still favorable over Xor in such cases, since it\ndoes not produce any false positives for lookups with occur-ring keys. A similar picture presents itself for the low andhigh cardinality columns in the DMV dataset (cf. Figure 9).\nTable 4 shows the overall space consumption of the zstd-\ncompressed ﬁlters over allIMDb/DMV columns. For both\ndatasets, CI/1 consumes signiﬁcantly less space than itscompetitors. Since DMV has lower cardinalities than IMDb\n(0.05% vs. 3.74% unique values across all columns), CI’s\nadvantage is higher on DMV.\nIn summary, CI is competitive with Bloom and Xor in\nterms of size for high cardinality columns, and is more space-\ne\u0000cient for low-to-medium cardinality columns. Also, we\nobserve that CI’s scan rate conﬁguration does not have a\nlarge impact on its size.\n4.2 Mixed Workloads\nWe have shown that CI can be much smaller than per-\nstripe ﬁlters for a ﬁxed scan rate (assuming only negative\nlookups). However, its real beneﬁt only becomes clear when\nconsidering workloads that contain a mix of positive and\nnegative lookups.\nWe therefore now we vary the hit rate of lookups. For\nexample, a hit rate of 10% means that 10% of the lookups\nare with keys that occur in at least one stripe. We draw thesekeys uniformly from the set of column values and generatenon-existing keys for negative lookups. We again excludeZoneMap here, since it yields almost no pruning for unsorted\ndata (cf. Section 4.3).\nFigure 10 shows the results for the representative IMDb\ncolumns and 2\n13rows per stripe. We omit DMV from this\nexperiment since the results look similar to IMDb. Bloom\nand Xor are largely una ↵ected by the increase in positive\nlookups and achieve scan rates close to their conﬁgured false\npositive probabilities of around 1% and 0.3%, respectively.\nNotably, Xor uses approximately the same amount of mem-ory than Bloom, and is thus to be preferred over Bloom in\nthis setting. CI strongly beneﬁts from an increasing hit rate\nsince it guarantees exact results for positive lookups. Hence,depending on the workload characteristics, CI can also bebeneﬁcial for high cardinality columns such as title,d e -\nspite its possibly larger size.\nThis experiment also shows that our defensive (per-bucket)\nscan rate estimation (cf. Section 3.3) underestimates the\nscan rate. For example, with a target scan rate of 1%, CIactually achieves a scan rate of less than 0.7% in all cases.\nWe leave improvements to this estimation for future work.\n3566\n\nuncompressed zstd\n13 14 15 16 13 14 15 160204060\n0.00%0.50%1.00%1.50%2.00%2.50%\nRows per stripe (2^x)KiBCI/0.1 CI/1 Bloom Xor\n(a)country code (0.01% unique)uncompressed zstd\n13 14 15 16 13 14 15 16050010001500\n0.0%3.0%6.0%9.0%\nRows per stripe (2^x)KiBCI/0.1 CI/1 Bloom Xor\n(b) company name (6.34% unique)uncompressed zstd\n13 14 15 16 13 14 15 160100020003000\n0.0%5.0%10.0%15.0%20.0%\nRows per stripe (2^x)KiBCI/0.1 CI/1 Bloom Xor\n(c)title (41.9% unique)\nFigure 8: Uncompressed and zstd compressed ﬁlter sizes for IMDb columns. The right y-axis shows the size\nrelative to the compressed column size. The dashed line is at 2% of the compressed column size.\n050100150\n0.00%0.50%1.00%1.50%2.00%\n13 14 15 16\nRows per stripe (2^x)KiBCI/0.1 CI/1 Bloom Xor\n(a)color (<0.00% unique)0500100015002000\n0.0%2.0%4.0%6.0%\n13 14 15 16\nRows per stripe (2^x)KiBCI/0.1 CI/1 Bloom Xor\n(b) city (0.27% unique)\nFigure 9: Uncompressed ﬁlter sizes for DMV.\ncountry_code company_name title\n0 25 50 75 100 0 25 50 75 100 0 25 50 75 1000.00.30.60.9\nHit rate (%)Scan rate (%)CI/0.1 CI/1 Bloom Xor\nFigure 10: Scan rate with an increasing hit rate (ra-\ntio of positive lookups) and 213rows per stripe.\n4.3 Different Sort Orders\nNow we study the impact of sort orders on index size\nand precision. We use the medium-cardinality column city\n(DMV dataset) and 213rows per stripe for this experiment.\nWe note that the e↵ect of di↵erent sort orders is similarfor other columns. Table 5 shows the results. The size\nof the di↵erent (zstd-compressed) indexes stays about the\nsame when shu ✏ing the data. This is expected since cities\nare not particularly clustered in the original data. ZoneMap\nclearly consumes the least amount of space. When sortingthe data, the space consumption of CI/1, Bloom, and Xor\ndecreases. ZoneMap’s size increases since there is now a\nhigher number of di↵erent min/max values to encode. Thereason that Bloom/Xor beneﬁt from sorting is simple: withsorted data, there is less redundancy of unique values across\nstripes. That is, a single unique value is indexed in fewer\nBloom/Xor ﬁlters. CI, on the other hand, indexes everyunique value only once in any case (independent of the sortorder). However, it is well known that sorting can have alarge impact on bitmap compression [31], as we have con-\nﬁrmed with this experiment. While the scan rates (only\npositive lookups) of CI/1 and Xor are rather una ↵ected byTable 5: DMV city column with di↵ erent sort or-\nders and 2\n13rows per stripe.\nCI/1 Bloom Xor ZoneMap\nSize (KiB, zstd)Original 552 2,093 1,986 1.66Shu✏ ed 556 2,360 2,239 1.42\nSorted 80 41 61 1.85\nScan rate (%)Original 0.00 0.89 0.39 99.9Shu✏ ed 0.00 0.87 0.38 99.9\nSorted 0.00 0.60 0.41 0.00\n2^13 rows per stripe 2^16 rows per stripe\n0.01 0.1 1 10 100 0.01 0.1 1 10 100050010001500\nUnique ratio (%)Size (KiB)CI/0.1 CI/1 Bloom Xor\nFigure 11: Compressed ﬁlter sizes for uniform data\nwith 1 M rows and increasing cardinality.\nthe sort order, Bloom becomes more e↵ective with clustered\ndata. ZoneMap even achieves near-optimal accuracy.\n4.4 Uniform Data\nNext, we experiment with uniform data and vary the car-\ndinality and the number of rows. We ﬁrst ﬁx the number ofrows to 1 M and increase the cardinality. Figure 11 shows\nthe e↵ect of increasing cardinality on the compressed ﬁlter\nsizes. Up to 10% unique values CI consumes signiﬁcantly\nless space than Bloom and Xor. The space savings are morepronounced for fewer rows per stripe. While CI consumesmore space than Bloom and Xor with 100% unique values\n(i.e., a unique column), it may still be preferable in that\nsetting due to its higher accuracy under positive and mixedworkloads (cf. Section 4.2). We now ﬁx the cardinality to10 K and increase the number of rows (and implicitly the\nnumber of stripes). As can be seen in Figure 12, ﬁlter sizes\nincrease with more rows (stripes).\n3567\n\n2^13 rows per stripe 2^16 rows per stripe\n1 10 100 1 10 100020000400006000080000\nNumber of rows (M)Size (KiB)CI/0.1 CI/1 Bloom Xor\nFigure 12: Compressed ﬁlter sizes for uniform data\nwith 10 K unique values and an increasing numberof rows.\nTable 6: Build times in milliseconds with 1 M rows\nand 10 K unique values.\nRows per stripe CI/1 Bloom Xor\n21376.3 89.6 41.8\n21645.0 54.9 13.9\nThe size increase for both Bloom/Xor and CI can be ex-\nplained by the increased redundancy of unique values amongstripes. Put di ↵erently, a unique value will be present in\nmore stripes. Since Bloom and Xor maintain one ﬁlter per\nstripe, each unique value is indexed in more ﬁlters. In con-\ntrast, CI indexes each unique value only once, independentof the number of stripes. However, the number of stripes af-fects CI’s bitmaps, which maintain one bit per stripe. The\nreason that CI is less a ↵ected by the row count is that the\nadded bitmap bits cause less space overhead than the re-\ndundant indexing in Bloom/Xor. CI’s bitmap compressionalso mitigates the e↵ect of additional bits to some degree.\nLikewise, both CI and Bloom/Xor are a ↵ected by the num-\nber of rows per stripe, with Bloom/Xor being more a ↵ected\nthan CI. The reason is again the redundancy of unique val-\nues among stripes. Fewer rows per stripe mean more stripesand thus there is more redundancy, which is less of an issue\nfor CI than for Bloom/Xor.\n4.5 Build and Lookup Performance\nWe now study the build and lookup performance of CI\ncompared to Bloom and Xor. We omit CI/0.1 here since it\nhas very similar build and lookup times as CI/1. While thelookup code is partially optimized (with a rank lookup table\nfor each block bitmap), the build code is unoptimized.\nBuild Performance. Table 6 shows the build times of\nthe di↵erent ﬁlters with 1 M rows and 10 K unique values.\nAll ﬁlters are a ↵ected by the number of rows per stripe.\nFor Bloom and Xor the reason is that they need to createmore (albeit possibly smaller) ﬁlters with more stripes. ForCI, in contrast, the reason is the larger bitmaps that need\nto be compressed. In a breakdown analysis of CI’s build\ntime with 2\n13rows per stripe we found that most time is\nspent on scanning the data and collecting unique values withtheir corresponding bitmaps (23.6%), ﬁnding the minimum\nrequired ﬁngerprint length per slot to ensure no collisions\nand to satisfy the given scan rate (32.5%), and compressingTable 7: Lookup latency in nanoseconds with 1 M\nrows and 10 K unique values.\nLookups Rows per stripe CI/1 Bloom Xor\nPositive 21313,458 7,815 1,715\nPositive 2165,564 964 108\nNegative 213769 6,942 814\nNegative 216610 836 96.3\nTable 8: Size and lookup latency of our compression\nscheme compared to Roaring (R).\nColumn Ours-KiB R-KiB Ours-ns R-ns\ncountry code 5.77 16.0 3,810 3,396\ncompany name 1,425 2,495 2,589 3,704\ntitle 3,157 2,966 4,879 3,464\nthe global bitmap (34.7%). Against our own intuition, wedid not ﬁnd distributing values among buckets (kicking) to\nbe expensive. We now vary the cardinality. With 2\n13rows\nper stripe, the build times in milliseconds of CI (Bloom) with1 K (0.1%) and 1 M (100%) unique values are 16.6 (47.6) and\n5,448 (104), respectively.\nLookup Performance. Table 7 shows the lookup latency\nof CI/1 compared to Bloom and Xor. For positive lookups,\nCI has a signiﬁcant lookup overhead over Bloom/Xor. Un-der negative lookups, CI shows much better numbers. Onlyin the event of a false positive match in the Cuckoo table, it\nhas to extract a bitmap from the compressed global bitmap\n(cf. Section 3.6). With our current bitmap encoding scheme,\nthe cost of this operation is linear in the size of the com-\npressed bitmap. With more read-optimized schemes such\nas Roaring, one could trade size for lookup latency (cf. Sec-\ntion 4.6). Note that we use a rank lookup table to speed uplookups in the block bitmaps that indicate the membershipof a ﬁngerprint in a block (cf. Section 3.5). Without this op-timization, negative lookups would be signiﬁcantly slower.\nBloom and Xor, on the other hand, are less a ↵ected by the\nlookup type. As expected, both per-stripe ﬁlters show a lin-\near increase in lookup latency with fewer rows per stripe.CI, on the other hand, is rather una ↵ected by the number\nof stripes. With 2\n13rows per stripe and positive lookups,\n93.0% of CI’s lookup time is spent on extracting the bitmapthat indicates qualifying stripes. Under negative lookups,the most expensive operation is querying the Cuckoo tablewith 73.5%. Now we again vary the cardinality. With 2\n13\nrows per stripe, the latencies in ms for positive lookups ofCI (Bloom) with 1 K (0.1%) and 1 M (100%) unique valuesare 5,673 (7,608) and 3,006 (7,442), respectively.\n4.6 Bitmap Compression\nWe now compare the size and decompression speed of our\nbitmap encoding scheme (cf. Section 3.6) with Roaring. We\nuse the IMDb columns country code, company name,a n d\ntitle with 213rows per stripe for this experiment. The spe-\nciﬁc task is to encode the global bitmap and to extract sub-\nbitmaps. Recall that each sub-bitmap indicates the qualify-\ning stripes of a given key ﬁngerprint stored in the Cuckoo\ntable. We construct a CI/1 index, query with random keys\n3568\n\nTable 9: Index sizes in MiB for TPC-H columns (CI\nis zstd-compressed).\nIndex ocustkey l orderkey l partkey l suppkey\nbtree 322 1,286 1,287 1,287\nhash 476 1,973 1,897 2,379\nCI (213)1 5 . 6 4 9 . 5 6 8 . 5 3 5 . 2\nCI (216)1 0 . 2 3 1 . 4 4 6 . 7 1 1 . 0\nfrom the set of keys, and measure the time taken to extractthe sub-bitmap associated with the ﬁngerprint. For Roar-ing, we intersect the global bitmap with a query bitmap withthe relevant bits set. Note that storing individual Roaringbitmaps (as opposed to encoding them back-to-back) would\nincrease its total size by up to 4 ⇥(presumably due to the\nheader that is stored with every bitmap). Hence, encoding\nindividual bitmaps as Roaring is not an option. Table 8shows the results. For country\ncode and company name,\nour compression scheme consumes the least space whereasRoaring is more space-e\u0000cient for the sparse title bitmaps.\nIn terms of extracting sub-bitmaps, our encoding scheme is\ncompetitive with Roaring.\n4.7 Impact on Query Performance\nFinally, we integrate CI/1 and Bloom into PostgreSQL\n(PG) version 12.3. Speciﬁcally, we partition lineitem and\norders byorderdate with 213and 216rows per stripe (par-\ntition). We build the indexes on all columns that are subject\nto (join) predicates. At query time, we probe the respectiveindex and pass the returned partition IDs to PostgreSQL’spartition pruner.\nWe set the size of Bloom to be similar to the size of CI\n(when zstd-compressed). Xor’s implementation does not al-low to set the number of bits per element, hence we excludeXor from this experiment. Since PG does not parallelize\nqueries with partitioning pruning, we disable parallelism to\nensure a fair comparison against several baselines, includinga full scan and PG’s built-in btree and hash indexes. We\nrun three di↵erent queries on TPC-H with SF10:\nQ1 select sum(l\nextendedprice*(1-l discount))\nrevenue from lineitem, orders\nwhere l orderkey=o orderkey\nand o custkey=36901 and l returnflag=’R’;\nQ2 select sum(l extendedprice*(1-l discount))\nrevenue from lineitem where l partkey=155190;\nQ3 select sum(l extendedprice*(1-l discount))\nrevenue from lineitem where l suppkey=7706;\nTable 9 shows the index sizes in MiB and Table 10 shows the\nquery latencies in milliseconds. CI is signiﬁcantly smaller\nthan PG’s built-in indexes, which index individual tuplesrather than stripes. As expected, CI’s size decreases with\nmore rows per stripe.\nIn terms of query latency, PG’s hash index achieves the\nbest performance, followed by btree.C I a n d B l o o m c o v e r\nthe middle ground between the full-ﬂedged indexes and a\nfull scan. Compared to Bloom and with 2\n13rows per stripe,\nCI returns on average 2.10 ⇥fewer partitions and achieves a\n2.45⇥ speedup in query performance.Table 10: Query latencies in milliseconds.\nIndex Q1 Q2 Q3\nNo index 6,628 4,116 4,236\nbtree 0.48 0.25 0.79\nhash 0.37 0.17 0.68\nCI (213)5 0 . 4 3 4 . 8 8 9 4\nBloom (213)9 5 . 6 1 1 0 2 , 0 4 5\nCI (216)4 5 6 3 0 6 5 , 2 8 4\nBloom (216)6 0 2 8 8 7 7 , 4 0 5\n5. RELATED WORK\nThere have been many proposals to secondary indexing\nstriking di↵erent balances between space and pruning power.\nSmall Materialized Aggregates. A popular approach\nis to maintain Small Materialized Aggregates (SMAs) [33]\n(e.g., min and max values) per stripe. SMAs (or ZoneMaps)consume little space and are most e↵ective when data is(partially) sorted. Besides sorting, outliers can impact the\nprecision of ZoneMaps. Positional SMAs (PSMAs) [28] ex-\ntend SMAs with a compact lookup table that maps bytes ofvalues to scan ranges. In contrast to both SMA variants, CIis not impacted by outliers and does not require the data to\nbe (partially) sorted to be e↵ective. Although, data cluster-\ning can decrease its size as we have shown.\nApproximate Set Membership Structures. Another\nwell-known approach is to maintain one set-membership ﬁl-\nter (e.g., a Bloom ﬁlter [7, 29]) per stripe .W i t h a b o u t 1 0 b i t s\nper unique value, Bloom ﬁlters produce around 1% false pos-itives. While per-stripe ﬁlters are a good strategy for high\ncardinality columns, their size consumption is far from opti-\nmal for low cardinality columns. In addition, to identify allqualifying stripes for a given lookup key, we need to probe all\nper-stripe ﬁlters, which becomes increasingly expensive with\nhigher resolutions (more stripes). Other set membership ﬁl-\nters such as the Cuckoo ﬁlter [15] or the recently proposedXor ﬁlter [19, 12] improve space e\u0000ciency but the problemremains: with many redundant values across stripes, their\nspace consumption is sub-optimal. Also, and most notably,\nper-stripe ﬁlters may return false positive stripes under posi-\ntive lookups. This is in contrast to CI, which is 100% correct\nunder positive lookups.\nApproximate Lookup Tables. Approximate lookup ta-\nbles allow for associating keys with values with a low prob-\nability of false mappings. The most popular example is the\nBloomier ﬁlter [9] and its follow-up Invertible Bloom LookupTable (IBLT) [18]. In contrast to our work, Bloomier aims toimprove lookup performance. It consists of multiple Bloomﬁlters and requires all ﬁlters to use the same number of bits,\nwhich leads to sub-optimal space utilization under skew.\nIBLT allows to list its contents with some probability. A keyissue with IBLT is that a lookup may return “not found” for\na key. Since we cannot tolerate false negatives, IBLT is not\napplicable to our use case.\nClosely related to our work, Ren et al. [37] propose to\nmaintain a single higher-level Cuckoo ﬁlter instead of many\n“per-LSM-tree-node” ﬁlters in an LSM-tree. Each key in theﬁlter maps to the ID of the most recent LSM-tree node con-\ntaining that key. In contrast, we are interested in all of the\n3569\n\ndata partitions (stripes) that contain a given key. To resolve\nﬁngerprint collisions in the ﬁlter, Ren et al. store colliding\nentries in an external table. Our variable-sized ﬁngerprints\no↵er additional size beneﬁts over that approach.\nCuckoo Hashing Extensions. Our idea of maximizing\nprimary bucket assignments by solving a maximum weightmatching problem (cf. Section 3.4) is not entirely new. Infact, it is well known that matching algorithms on bipar-\ntite graphs (e.g., the Hopcroft-Karp algorithm [23]) can be\nused to assign values to buckets in the setting of Cuckoohashing. The thesis work by Michael Rink [38] provides a\ngood overview in that respect. Furthermore, Dietzfelbinger\net al. [11] show that matching algorithms can be used to\nmaximize the ratio of items that end up in their primary\nlocation (in the context of Cuckoo hash tables that are di-vided into pages ). In contrast to this work, we are interested\nin reducing the average ﬁngerprint length rather than mini-\nmizing page accesses. Reducing the likelihood that we need\nto access an item’s secondary bucket on lookup is rather a\npositive side e↵ect for us. The same work [11] also showsthat a biased insertion procedure that prefers primary loca-tions in case of full buckets achieves almost the same result\nin short time. Similar to this work, we have developed a bi-\nased kicking algorithm that prefers kicking items that reside\nin their secondary locations to maximize primary bucket as-signments. To the best of our knowledge, our work is the\nﬁrst to apply such an algorithm in the context of Cuckoo\nﬁlters (cf. Section 3.4.2).\nMinimal Perfect Hashing. Minimal perfect hashing is\napplicable to the problem studied in this work. The idea\nis to compute a minimal perfect hash function that maps n\nkeys to the dense integers [0,n \u00001]. In our context, these\nintegers would be o ↵sets into an array of bitmaps indicating\nqualifying stripes. Esposito et al. [14] have shown that sucha hash function only requires storing 1.56 bits per key inpractice. While this sounds attractive, constructing such a\nfunction remains computationally expensive. Nevertheless,\nusing a minimal perfect hash function such as RecSplit [14]is an attractive alternative to the “Cuckoo part” of our index(which can be seen as a variant of perfect hashing) to fur-ther reduce its footprint. The challenge of e\u0000ciently storing\nvariable-sized ﬁngerprints and bitmaps, however, remains.\nData Skipping Techniques. In general, there is plenty of\nresearch on (secondary) indexing and data skipping. Most\nrelated is bitmap indexing which maintains a bitmap foreach unique column value indicating qualifying tuples [8,\n35, 43]. There are two straightforward extensions to classi-\ncal bitmap indexing. One is to make bitmaps more coarse\ngrained which is similar to our setting. Another is to com-bine the bitmaps of multiple values, typically values withina certain range (binning ). Both extensions trade precision\nfor reduced footprint.\nColumn Imprints [40] and Column Sketches [22] both al-\nlow to accelerate in-memory scans by maintaining lossy in-dex structures in the form of bit vectors or ﬁxed-width codes.\nAs opposed to our work, both techniques aim to reduce the\ncost of in-memory scans and not the number of disk accesses.Speciﬁcally, Column Imprints exploit the idea that real datanaturally exhibits very localized correlations. Taking themto a much larger granularity than cache lines would make\nthem similar to ZoneMaps.Recently, machine learning (ML) found its way into in-\ndexing. While learned indexes [26, 24, 32, 25] may consume\nsigniﬁcantly less space than traditional indexes such as B-trees, that observation mostly applies to primary indexing\nwhere the base data is already sorted. In a secondary index-\ning setting such as ours, additional permutation vectors haveto be maintained which will likely account for most of thespace consumption of the learned index. Follow-up work on\nlearned multi-dimensional indexes [34, 13] features a small\nfootprint and allows for ﬁltering on secondary columns but—in contrast to our approach—requires full control over thesort order of the data. Furthermore, learned indexes have\nnot been adapted to the approximate indexing use case as\nstudied in this work. Other ML-based work by Wu et al. [42]\nallows to construct succinct secondary indexes by exploitingcolumn correlations. However, in contrast to our work, itrequires access to a primary index.\n6. CONCLUSIONS\nWe have introduced Cuckoo Index, a lightweight second-\nary index structure that is targeted at “write once-readmany” environments. We have extended Cuckoo ﬁlters withvariable-sized ﬁngerprints to avoid key shadowing. Withthis design, our approach allows for correct results for posi-\ntive lookups and further allows for tuning the scan rate for\nnegative lookups. Our novel cross-optimization of ﬁnger-\nprints and bitmaps elegantly makes use of the fact that forsparse bitmaps one needs to store fewer ﬁngerprint bits to\nachieve a desired scan rate. We have described a heuris-\ntic that achieves an almost optimal key-to-bucket assign-ment which minimizes index size. Using real-world and syn-\nthetic data, we have demonstrated that our approach con-\nsumes less space than per-stripe ﬁlters for low-to-medium\ncardinality columns that represent the majority of secondary\ncolumns in our experience. While we have focused on spacee\u0000ciency, lookup performance can matter, particularly whenthe cost for a false positive is relatively low. The cost of a\nlookup in CI is dominated by rank calls on block bitmaps\nand decompressing stripe bitmaps. We have accelerated\nrank calls using a rank lookup table. Further, we have shownthat our bitwise bitmap encoding compresses better thanRoaring [8] while also allowing for partial decompression.\nWhile we have designed CI for immutable environments,\nusing an update-friendly bitmap encoding such as Roaringor Tree-Encoded Bitmaps [27] CI could support updates anddeletions since both operations only require updating the\nbitmap part of the index. To support inserts, we would\nalso need to update the Cuckoo table. One strategy is tooverprovision the Cuckoo table, i.e., allocate more slots thanrequired. Another is to allocate an additional kbits per\nﬁngerprint, which would allow for doubling the size of the\nCuckoo table ktimes.\nCurrently, our index is limited to equality predicates. To\nsupport range predicates, one may consider replacing the\nCuckoo table with a range ﬁlter such as SuRF [44]. Variable-\nsized key su \u0000xes in SuRF would allow for correct results\nunder positive lookups. However, since key su \u0000xes follow a\nreal rather than a uniform (hash) distribution, the conﬂict\nprobability of two keys may be higher.\nIn future work, we plan to extend our approach to in-\ndexing multiple columns at once which opens up interestingopportunities such as exploiting column correlations.\n3570\n\n7. REFERENCES\n[1]LevelDB. https://github.com/google/leveldb/.\n[2]Vehicle, snowmobile, and boat registrations.\nhttps://catalog.data.gov/dataset/\nvehicle-snowmobile-and-boat-registrations.\n[3]Xor Filter. https:\n//github.com/FastFilter/xor\\_singleheader/.\n[4]Zstandard Compression.\nhttps://facebook.github.io/zstd/.\n[5]A. Ailamaki, D. J. DeWitt, M. D. Hill, and\nM. Skounakis. Weaving relations for cache\nperformance. In VLDB 2001, Proceedings of 27th\nInternational Conference on Very Large Data Bases,\nSeptember 11-14, 2001, Roma, Italy ,p a g e s1 6 9 – 1 8 0 ,\n2001.\n[6]M. Athanassoulis, Z. Yan, and S. Idreos. UpBit:\nScalable in-memory updatable bitmap indexing. In\nProceedings of the 2016 International Conference on\nManagement of Data, SIGMOD Conference 2016, San\nFrancisco, CA, USA, June 26 - July 01, 2016,p a g e s1319–1332, 2016.\n[7]B. H. Bloom. Space/time trade-o ↵s in hash coding\nwith allowable errors. Commun. ACM,1 3 ( 7 ) : 4 2 2 – 4 2 6 ,\n1970.\n[8]S. Chambi, D. Lemire, O. Kaser, and R. Godin.\nBetter bitmap performance with Roaring bitmaps.Softw., Pract. Exper. ,4 6 ( 5 ) : 7 0 9 – 7 1 9 ,2 0 1 6 .\n[9]B. Chazelle, J. Kilian, R. Rubinfeld, and A. Tal. The\nBloomier Filter: An e\u0000cient data structure for static\nsupport lookup tables. In SODA ,p a g e s3 0 – 3 9 ,2 0 0 4 .\n[10]B. Dageville, T. Cruanes, M. Zukowski, V. Antonov,\nA. Avanes, J. Bock, J. Claybaugh, D. Engovatov,\nM. Hentschel, J. Huang, A. W. Lee, A. Motivala,A. Q. Munir, S. Pelley, P. Povinec, G. Rahn,S. Triantafyllis, and P. Unterbrunner. The Snowﬂake\nelastic data warehouse. In Proceedings of the 2016\nInternational Conference on Management of Data,\nSIGMOD Conference 2016, San Francisco, CA, USA,June 26 - July 01, 2016,p a g e s2 1 5 – 2 2 6 ,2 0 1 6 .\n[11]M. Dietzfelbinger, M. Mitzenmacher, and M. Rink.\nCuckoo hashing with pages. In Algorithms - ESA 2011\n-1 9 t hA n n u a lE u r o p e a nS y m p o s i u m ,S a a r b r ¨ u c k e n ,Germany, September 5-9, 2011. Proceedings ,p a g e s\n615–627, 2011.\n[12]M. Dietzfelbinger and R. Pagh. Succinct data\nstructures for retrieval and approximate membership\n(extended abstract). In Automata, Languages and\nProgramming, 35th International Colloquium, ICALP\n2008, Reykjavik, Iceland, July 7-11, 2008, Proceedings,\nPart I: Tack A: Algorithms, Automata, Complexity,and Games ,p a g e s3 8 5 – 3 9 6 ,2 0 0 8 .\n[13]J. Ding, V. Nathan, M. Alizadeh, and T. Kraska.\nTsunami: A learned multi-dimensional index forcorrelated data and skewed workloads. CoRR,\nabs/2006.13282, 2020.\n[14]E. Esposito, T. M. Graf, and S. Vigna. RecSplit:\nMinimal perfect hashing via recursive splitting. InProceedings of the Symposium on Algorithm\nEngineering and Experiments, ALENEX 2020, Salt\nLake City, UT, USA, January 5-6, 2020 ,p a g e s\n175–185, 2020.\n[15]B. Fan, D. G. Andersen, M. Kaminsky, andM. Mitzenmacher. Cuckoo Filter: Practically betterthan Bloom. In CoNEXT,p a g e s7 5 – 8 8 ,2 0 1 4 .\n[16]A. V. Goldberg and R. Kennedy. An e\u0000cient cost\nscaling algorithm for the assignment problem. Math.\nProgram.,7 1 : 1 5 3 – 1 7 7 ,1 9 9 5 .\n[17]A. V. Goldberg and R. Kennedy. Global price updates\nhelp. SIAM J. Discrete Math. ,1 0 ( 4 ) : 5 5 1 – 5 7 2 ,1 9 9 7 .\n[18]M. T. Goodrich and M. Mitzenmacher. Invertible\nBloom Lookup Tables. CoRR,a b s / 1 1 0 1 . 2 2 4 5 ,2 0 1 1 .\n[19]T. M. Graf and D. Lemire. Xor Filters: Faster and\nsmaller than Bloom and Cuckoo Filters. CoRR,\nabs/1912.08258, 2019.\n[20]A. Gupta, F. Yang, J. Govig, A. Kirsch, K. Chan,\nK. Lai, S. Wu, S. G. Dhoot, A. R. Kumar, A. Agiwal,\nS. Bhansali, M. Hong, J. Cameron, M. Siddiqi,\nD. Jones, J. Shute, A. Gubarev, S. Venkataraman, andD. Agrawal. Mesa: Geo-replicated, near real-time,scalable data warehousing. PVLDB ,7 ( 1 2 ) : 1 2 5 9 – 1 2 7 0 ,\n2014.\n[21]A. Hall, O. Bachmann, R. B¨ ussow, S. Ganceanu, and\nM. Nunkesser. Processing a trillion cells per mouse\nclick. PVLDB ,5 ( 1 1 ) : 1 4 3 6 – 1 4 4 6 ,2 0 1 2 .\n[22]B. Hentschel, M. S. Kester, and S. Idreos. Column\nSketches: A scan accelerator for rapid and robust\npredicate evaluation. In Proceedings of the 2018\nInternational Conference on Management of Data,SIGMOD Conference 2018, Houston, TX, USA, June10-15, 2018,p a g e s8 5 7 – 8 7 2 ,2 0 1 8 .\n[23]J. E. Hopcroft and R. M. Karp. An n5/2algorithm\nfor maximum matchings in bipartite graphs. SIAM J.\nComput. ,2 ( 4 ) : 2 2 5 – 2 3 1 ,1 9 7 3 .\n[24]A. Kipf, R. Marcus, A. van Renen, M. Stoian,\nA. Kemper, T. Kraska, and T. Neumann. SOSD: Abenchmark for learned indexes. NeurIPS Workshop on\nMachine Learning for Systems ,2 0 1 9 .\n[25]A. Kipf, R. Marcus, A. van Renen, M. Stoian,\nA. Kemper, T. Kraska, and T. Neumann.\nRadixSpline: a single-pass learned index. In\nProceedings of the Third International Workshop on\nExploiting Artiﬁcial Intelligence Techniques for DataManagement, aiDM@SIGMOD 2020, Portland,\nOregon, USA, June 19, 2020 ,p a g e s5 : 1 – 5 : 5 ,2 0 2 0 .\n[26]T. Kraska, A. Beutel, E. H. Chi, J. Dean, and\nN. Polyzotis. The case for learned index structures. In\nSIGMOD,p a g e s4 8 9 – 5 0 4 ,2 0 1 8 .\n[27]H. Lang, A. Beischl, V. Leis, P. A. Boncz,\nT. Neumann, and A. Kemper. Tree-Encoded Bitmaps.InProceedings of the 2020 International Conference\non Management of Data, SIGMOD Conference 2020,online conference [Portland, OR, USA], June 14-19,2020,p a g e s9 3 7 – 9 6 7 ,2 0 2 0 .\n[28]H. Lang, T. M¨ uhlbauer, F. Funke, P. A. Boncz,\nT. Neumann, and A. Kemper. Data Blocks: HybridOLTP and OLAP on compressed storage using both\nvectorization and compilation. In Proceedings of the\n2016 International Conference on Management of\nData, SIGMOD Conference 2016, San Francisco, CA,\nUSA, June 26 - July 01, 2016 ,p a g e s3 1 1 – 3 2 6 ,2 0 1 6 .\n[29]H. Lang, T. Neumann, A. Kemper, and P. A. Boncz.\nPerformance-optimal ﬁltering: Bloom overtakes\nCuckoo at high-throughput. PVLDB ,1 2 ( 5 ) : 5 0 2 – 5 1 5 ,\n2019.\n3571\n\n[30]V. Leis, A. Gubichev, A. Mirchev, P. Boncz,\nA. Kemper, and T. Neumann. How good are query\noptimizers, really? PVLDB ,9 ( 3 ) ,2 0 1 5 .\n[31]D. Lemire, O. Kaser, and K. Aouiche. Sorting\nimproves word-aligned bitmap indexes. Data Knowl.\nEng.,6 9 ( 1 ) : 3 – 2 8 ,2 0 1 0 .\n[32]R. Marcus, A. Kipf, A. van Renen, M. Stoian,\nS. Misra, A. Kemper, T. Neumann, and T. Kraska.\nBenchmarking learned indexes. CoRR,\nabs/2006.12804, 2020.\n[33]G. Moerkotte. Small Materialized Aggregates: A light\nweight index structure for data warehousing. InVLDB ,p a g e s4 7 6 – 4 8 7 ,1 9 9 8 .\n[34]V. Nathan, J. Ding, M. Alizadeh, and T. Kraska.\nLearning multi-dimensional indexes. In Proceedings of\nthe 2020 International Conference on Management of\nData, SIGMOD Conference 2020, online conference\n[Portland, OR, USA], June 14-19, 2020 ,p a g e s\n985–1000, 2020.\n[35]P. E. O’Neil and D. Quass. Improved query\nperformance with variant indexes. In SIGMOD 1997,\nProceedings ACM SIGMOD International Conference\non Management of Data, May 13-15, 1997, Tucson,\nArizona, USA ,p a g e s3 8 – 4 9 ,1 9 9 7 .\n[36]R. Pagh and F. F. Rodler. Cuckoo hashing. J.\nAlgorithms ,5 1 ( 2 ) : 1 2 2 – 1 4 4 ,2 0 0 4 .\n[37]K. Ren, Q. Zheng, J. Arulraj, and G. Gibson.\nSlimDB: A space-e\u0000cient key-value storage engine for\nsemi-sorted data. PVLDB ,1 0 ( 1 3 ) : 2 0 3 7 – 2 0 4 8 ,2 0 1 7 .\n[38]M. Rink. Thresholds for Matchings in Random\nBipartite Graphs with Applications to Hashing-Based\nData Structures. PhD thesis, Technische Universit¨ atIlmenau, Germany, 2015.[39]S. Shi, C. Qian, and M. Wang. Re-designing\ncompact-structure based forwarding for programmablenetworks. In 27th IEEE International Conference on\nNetwork Protocols, ICNP 2019, Chicago, IL, USA,\nOctober 8-10, 2019 ,p a g e s1 – 1 1 ,2 0 1 9 .\n[40]L. Sidirourgos and M. L. Kersten. Column Imprints: a\nsecondary index structure. In Proceedings of the ACM\nSIGMOD International Conference on Management of\nData, SIGMOD 2013, New York, NY, USA, June\n22-27, 2013,p a g e s8 9 3 – 9 0 4 ,2 0 1 3 .\n[41]K. Wu, E. J. Otoo, and A. Shoshani. Optimizing\nbitmap indices with e \u0000cient compression. ACM\nTrans. Database Syst.,3 1 ( 1 ) : 1 – 3 8 ,2 0 0 6 .\n[42]Y. Wu, J. Yu, Y. Tian, R. Sidle, and R. Barber.\nDesigning succinct secondary indexing mechanism by\nexploiting column correlations. In Proceedings of the\n2019 International Conference on Management of\nData, SIGMOD Conference 2019, Amsterdam, TheNetherlands, June 30 - July 5, 2019 ,p a g e s1 2 2 3 – 1 2 4 0 ,\n2019.\n[43]E. T. Zacharatou, F. Tauheed, T. Heinis, and\nA. Ailamaki. RUBIK: e\u0000cient threshold queries onmassive time series. In Proceedings of the 27th\nInternational Conference on Scientiﬁc and Statistical\nDatabase Management, SSDBM ’15, La Jolla, CA,\nUSA, June 29 - July 1, 2015 ,p a g e s1 8 : 1 – 1 8 : 1 2 ,2 0 1 5 .\n[44]H. Zhang, H. Lim, V. Leis, D. G. Andersen,\nM. Kaminsky, K. Keeton, and A. Pavlo. SuRF:\nPractical range query ﬁltering with fast succinct tries.\nInProceedings of the 2018 International Conference\non Management of Data, SIGMOD Conference 2018,\nHouston, TX, USA, June 10-15, 2018 ,p a g e s3 2 3 – 3 3 6 ,\n2018.\n3572",
  "textLength": 75690
}