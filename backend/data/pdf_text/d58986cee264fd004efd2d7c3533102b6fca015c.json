{
  "paperId": "d58986cee264fd004efd2d7c3533102b6fca015c",
  "title": "AutoSpec: Automated Generation of Neural Network Specifications",
  "pdfPath": "d58986cee264fd004efd2d7c3533102b6fca015c.pdf",
  "text": "AutoSpec: Automated Generation of Neural Network Specifications\nShuowei Jin§, Francis Y. Yan†‡, Cheng Tan¶, Anuj Kalia†, Xenofon Foukas†, Z. Morley Mao§\n§University of Michigan,†Microsoft Research,‡UIUC,¶Northeastern University\nAbstract\nThe increasing adoption of neural networks in\nlearning-augmented systems highlights the impor-\ntance of model safety and robustness, particularly\nin safety-critical domains. Despite progress in\nthe formal verification of neural networks, current\npractices require users to manually define model\nspecifications—properties that dictate expected\nmodel behavior in various scenarios. This manual\nprocess, however, is prone to human error, lim-\nited in scope, and time-consuming. In this paper,\nwe introduce AutoSpec , the first framework to\nautomatically generate comprehensive and accu-\nrate specifications for neural networks in learning-\naugmented systems. We also propose the first set\nof metrics for assessing the accuracy and coverage\nof model specifications, establishing a benchmark\nfor future comparisons. Our evaluation across four\ndistinct applications shows that AutoSpec outper-\nforms human-defined specifications as well as two\nbaseline approaches introduced in this study.\n1. Introduction\nLearning-augmented systems, which integrate neural net-\nworks into computer systems, have demonstrated notable\nimprovements in a range of real-world applications, e.g.,\ndatabase indexing (Kraska et al., 2018), resource manage-\nment (Mao et al., 2019), video streaming (Yan et al., 2020),\nand intrusion detection (Liu & Lang, 2019). Nevertheless,\nthese systems face a fundamental challenge: while neural\nnetworks may learn to achieve good average performance,\ntheir behavior in worst-case scenarios remains uncertain\nand could potentially wreak havoc. For example, a learned\ncongestion-control algorithm lowers the packet sending rate\neven in favorable network conditions (Eliyahu et al., 2021).\nProviding formal guarantees for the worst-case performance\nof learning-augmented systems has received growing inter-\nest (Eliyahu et al., 2021; Wei et al., 2023; Wu et al., 2022).\nThese methods, however, require human experts to provide\nthe properties to verify, known as neural network specifica-tions . Each specification defines the expected model output\nfor a given input space (§2.1).\nSpecifically, researchers have relied on their domain knowl-\nedge and intuition about individual applications to manually\ncreate specifications. For instance, in adaptive video stream-\ning, where a neural network is employed to determine the\nbitrate for the next video chunk based on recent network\nconditions, Eliyahu et al. (2021) define a specification as,\n“[if video] chunks were downloaded quickly (more quickly\nthan it takes to play a chunk), the DNN should eventually\nnot choose the worst resolution.” Similar manual specifica-\ntions are devised for other learning-augmented systems, e.g.,\ndatabase indexes (Tan et al., 2021), memory allocators (Wei\net al., 2023), and job schedulers (Wu et al., 2022).\nHowever, crafting specifications manually is a complex task\nand prone to several issues. First, human-designed specifi-\ncations often lack comprehensiveness, as they typically ad-\ndress only specific aspects of the input domain. This limited\ncoverage in the input space leaves a broad range of scenar-\nios unverified. Second, manually created specifications are\noften ambiguous—being overly loose or incorrect in edge\ncases, rendering them less useful. In the case of adaptive\nvideo streaming, if the network bandwidth is consistently\nhigh, it would be more effective to dictate a correspondingly\nhigh video bitrate rather than merely avoiding “the worst\nresolution,” a constraint that is too loose to verify. Third,\nspecifications designed by humans are often based on their\npartial understanding of the system, which can sometimes\nbe incorrect. Lastly, the significant human effort and deep\nexpertise required to handcraft specifications make this ap-\nproach inherently non-scalable.\nThe above challenges in manual specification creation moti-\nvate the need for an automated approach. In this paper, we\nseek to answer the question: Can we automatically generate\nspecifications directly from data?\nTo illustrate the problem, consider a concrete example—\nnetwork throughput prediction, which estimates future net-\nwork throughput using past throughputs. As a useful module,\nnetwork throughput prediction has wide-ranging applica-\ntions, such as congestion control (Jay et al., 2019), adaptive\n1arXiv:2409.10897v2  [cs.LG]  23 Oct 2024\n\nAutoSpec: Automated Generation of Neural Network Specifications\ndownstream tasksTQFDTspec gen dataseteval datasetspec gen algorithmspec evaluationquality metricsNN verificationNN repairsanomaly discoverydevelopers…AutoSpec\nFigure 1: AutoSpec ’s workflow: AutoSpec starts by splitting\nthe input data into specification generation and evaluation\ndatasets. AutoSpec employs a specification generation al-\ngorithm ( §4) to generate candidate specifications. These\nspecifications are assessed on the evaluation dataset ( §3),\nproducing a set of quality metrics that describe their com-\nprehensiveness and accuracy.\nvideo streaming (Yan et al., 2020; Mao et al., 2017; Jin\net al., 2024), and resource allocation in radio access net-\nworks (Chien et al., 2019). For example, a straightforward\nneural network designed for this task may use the through-\nput values from previous four time slots ( x0, . . . , x 3) as\ninput to predict the throughput for the next slot (denoted\nasy). To verify the neural network, a specification can be\ndefined as: if the historical throughputs are within certain\nranges ( xi∈[xmin\ni, xmax\ni], fori= 0, . . . , 3), then the output\nthroughput should fall in another range ( y∈[ymin, ymax]).\nOur goal is to identify a collection of such specifications\nthat define the lower and upper bounds for the inputs and\noutput. Meanwhile, these specifications should be (1) com-\nprehensive , covering a broad range of the input space (in the\nexample, a four-dimensional space of throughputs), and (2)\naccurate , ensuring that the specifications closely align with\nthe ground-truth data (in our case, the specifications should\nconsistently match any five consecutive throughput values\nin the history, with the first four as xand the fifth as y).\nAchieving this goal, however, is faced with two major chal-\nlenges. First, there are no standard criteria established yet for\nevaluating specifications. Without clear evaluation metrics,\nit is impossible to compare different specifications. Design-\ning these metrics is subtle as they should be able to handle\ncorner-case specifications that are correct but useless. For\ninstance, in the throughput prediction task, a specification\ncould be made “comprehensive” by encompassing the entire\ninput ranges (i.e., xi∈[−∞,∞], fori= 0, . . . , 3) and the\nentire output range (i.e., y∈[−∞,∞]). While this specifi-\ncation is comprehensive and correct, it can be easily satisfied\nby any neural network and thus essentially useless for veri-\nfication. Similarly, to increase accuracy, one may construct\na specification set that tightly encloses every single data\npoint in training: for every five consecutive ground-truth\nthroughputs [t0, . . . , t 4], the specification can dictate that\nifxi∈[ti, ti], i= 0, . . . , 3, then yi∈[t4, t4]. This speci-fication set is “accurate” on the training set but cannot be\ngeneralized to describe the desired behavior of new data\npoints. It lacks coverage to guarantee the model’s perfor-\nmance across a broader range of the input space. Therefore,\nwe need to develop metrics that penalize impractical specifi-\ncations and reward useful ones.\nThe second challenge lies in generating specifications that\nare both comprehensive and accurate. This is difficult due\nto the fundamental conflict between comprehensiveness and\naccuracy. For example, enhancing comprehensiveness re-\nquires covering a broader range of inputs, which likely com-\npromises the accuracy due to encompassing too many data\npoints that could violate the specifications. Conversely, im-\nproving accuracy tends to diminish comprehensiveness as\nwell. As a result, striking a balance between the two aspects\npresents a significant hurdle in specification generation.\nTo address these two challenges, we present AutoSpec , a\nframework designed to automatically generate comprehen-\nsive and accurate specifications. Figure 1 outlines its work-\nflow. To the best of our knowledge, this work is the first\nto automatically generate neural network specifications di-\nrectly from data for verifying learning-augmented systems.\nAdditionally, it defines the first set of metrics to evaluate the\nefficacy of specifications. Our main contributions are:\n•A set of quantitative metrics for specifications ( §3):We\nestablish the first set of metrics for evaluating specifi-\ncations. These metrics extend familiar concepts such as\nprecision and recall to the context of neural network spec-\nifications. While these metrics are conceptually similar\nto their traditional counterparts, their definitions have\nbeen carefully calibrated for specifications to account for\ncorner cases.\n•A set of specification generation algorithms ( §4):We\ninitiate the exploration of automated generation of spec-\nifications, introducing two baselines and our own algo-\nrithm in AutoSpec : (a) a fixed-sized grid algorithm that\ndivides the input space into grids and generates a spec-\nification for each grid; (b) a clustering-based algorithm\nthat clusters data points and generates a specification\nfor each cluster; and (c) a tree traversal algorithm used\ninAutoSpec that employs a decision tree to define the\nboundaries of specifications.\n•A study of the framework and downstream task ( §5):We\nexperiment with four diverse applications and perform a\ncase study for the downstream task of detecting anoma-\nlies in neural networks through verification. Our experi-\nments show that AutoSpec ’s F1 score is 39% higher than\nhuman-defined specifications, and 18% higher than the\nnext best baseline algorithm.\nAutoSpec has two limitations. First, the specifications gen-\nerated by AutoSpec are limited to hyperrectangle inputs\n2\n\nAutoSpec: Automated Generation of Neural Network Specifications\nin the format of [x∈Rn:ai≤xi≤bi,∀i∈1, . . . , n ]. Ex-\ntending AutoSpec to other types of polytopes (zonotopes,\npolyhedra, etc.) requires future work. Second, AutoSpec\nonly handles reachability specifications. This leaves out\nother categories of specifications, such as monotonic and\nprobabilistic specifications. Further research is required to\nincorporate these types.\n2. Background and Related Work\n2.1. Providing Worst-Case Guarantees via Verification\nSpecifications. Specifications are constraints that describe\nthe operational expectations of deep neural networks\n(DNNs). Formally, given a DNN, N:Rn→Rm, a specifi-\ncation ϕdefines a set of pre-conditions ϕXover the input,\nand a corresponding set of post-conditions ϕYover the out-\nput. The specification prescribes a conditional statement:\n∀x∈Rn:ϕX(x)→ϕY(N(x)),\nwhere ϕX(x)specifies the pre-condition that captures input\ndomain constraints, while ϕY(N(x))represents the post-\ncondition that the DNN’s output, N(x), must satisfy. The\ntypical form of the post-condition is ϕmin\nY≤ N(x)≤ϕmax\nY,\nwhere ϕmin\nYandϕmax\nYare the specified boundaries. The veri-\nfication of ϕ(N)seeks to ensure that for all xadhering to\nϕX, the corresponding outputs adhere to ϕY.\nNeural network verifiation. DNN verification plays a piv-\notal role in developing reliable and trustworthy learning-\naugmented systems. The main objective of verification is to\nprovably guarantee that a given neural network behaves as\nintended, satisfying desired specifications. This is achieved\nwith various methods, including exhaustive search (Bak &\nTran, 2022), specialized SMT solvers (Katz et al., 2019),\nand reachability analysis (Wang et al., 2021). Although\nthese methods are effective, they rely on users to manu-\nally create the specifications to verify. This manual process\nis labor-intensive and error-prone, underscoring the need for\nautomated generation of specifications.\n2.2. Automated Specification Generation\nDespite the issues in handcrafting specifications, there has\nbeen little research on automating this process. Existing\nsystems (Eliyahu et al., 2021; Wei et al., 2023; Wu et al.,\n2022) still require humans to devise specifications based on\ndomain expertise and intuitions, presenting an opportunity\nto replace this manual process. Geng et al. (Geng et al.,\n2023) propose mining Neural Activation Patterns (NAPs)\nas specifications. However, this method is limited by its\ndependence on fixed neural network architectures and can-\nnot serve as a universal specification approach for arbitrary\nneural networks.\nBeyond neural networks, the most closely related research\ntopic is specification mining (Ammons et al., 2002; Lemieuxet al., 2015; Le & Lo, 2018) in program verification. It is\naimed at automatically formulating specifications to verify\ncomputer programs, rather than neural networks.\n2.3. Specification Quality Evaluation\nWhile specification generation is crucial, equally important\nis the evaluation of these generated specifications. Neverthe-\nless, this problem still remains unexplored. There lacks an\nestablished method to systematically assess the comprehen-\nsiveness and accuracy of specifications.\nOur proposal in §3 aims to fill this gap and provides a fair\ncomparison between specifications. Drawing inspirations\nfrom objective detection, we carefully define a set of stan-\ndard metrics, such as precision and recall, to gauge the ef-\nfectiveness of specifications.\n3. Specification Evaluation\nIn this section, we introduce the specification evaluation in\nAutoSpec . This process carefully defines a set of standard-\nized metrics, such as precision and recall, to quantitatively\nassess the quality of specifications, allowing for the compar-\nison of different specification generation algorithms (§4).\nOur metric design follows three principles:\n1.Comprehensive coverage : The aggregate input ranges\nof specifications should span a wide range of the input\nspace. This broad coverage ensures that the model’s be-\nhavior is effectively regulated across various scenarios,\nthereby providing a more reliable guarantee for learning-\naugmented systems.\n2.High accuracy : Each specification should closely align\nwith ground-truth data points. This alignment ensures\nthat the specifications are both theoretically correct and\npractically relevant to real-world scenarios.\n3.Tight constraint : Each specification should impose a rela-\ntively narrow output range to restrict the model’s behav-\nior. An overly large output range is too permissive and\nthus not useful in the model verification.\nSpecification evaluation’s inputs and outputs. Specifica-\ntion evaluation takes two inputs: (1) a specification set and\n(2) an evaluation dataset. The specification set Φis a col-\nlection of specifications, with each specification ϕidefined\nby an input range ϕXiand a corresponding expected output\nrange ϕYi(see §2.1). The evaluation dataset, comprising\ndata points Xevaland their corresponding ground-truth la-\nbelsYeval, is used to assess the specification set Φ. The\noutput of specification evaluation encompasses a set of met-\nrics that measure the overall quality and effectiveness of Φ.\nThese metrics provide a comprehensive view of the coverage\nand accuracy of the specifications.\nSpecification metrics. Next, we define the metrics of true\n3\n\nAutoSpec: Automated Generation of Neural Network Specifications\npositives (TP), false positives (FP), false negatives (FN), and\ntrue negatives (TN), in the context of specifications. These\nmetrics are used to further calculate precision, recall, and\nF1 score. Precision and recall measure the accuracy and\ncoverage of the specification set, respectively, while the F1\nscore consolidates precision and recall into a single metric.\nWe first provide the basic definitions of TP, FP, FN, TN for\nspecifications, and then adapt these definitions to account\nfor corner cases.\n•True Positive (TP): A true positive is defined as a data\npoint (xi, yi)in the evaluation dataset that satisfies a\nspecification ϕj∈Φ(with a corner case described later).\nFormally, (xi, yi)∈(Xeval, Yeval)is a true positive if\nxi∈ϕXj∧yi∈ϕYj.\n•False Positive (FP): A false positive is defined as a data\npoint (xi, yi)that does not meet a specification ϕj∈Φ\n(corner case to be described). In other words, (xi, yi)∈\n(Xeval, Yeval)is a false positive if xi∈ϕXj∧yi/∈ϕYj.\n•False Negative (FN): A false negative is defined as a data\npoint (xi, yi)that is not covered by any specification in\nΦ, i.e., xi/∈ϕXjfor any ϕj∈Φ.\n•True Negative (TN): In our context, we leave true nega-\ntives undefined as they do not carry practical meanings.\nPrecision, recall, and F1 score. We adopt the standard\ndefinitions of precision, recall, and F1 score based on the\npreviously defined TP, FP, and FN.\n•Precision = TP / (TP + FP), which measures the over-\nall accuracy of the specification set. High precision is\nachieved through a high TP rate and a low FP rate.\n•Recall = TP / (TP + FN), which quantifies the coverage\nof the specification set in the input space. High recall\ncorresponds to a low FN rate.\n•F1 score is the harmonic mean of precision and recall. It\ncombines both precision and recall and provides a single\nmetric for ranking different specification sets effectively.\nAddressing corner cases. As noted in §1, corner cases pose\nchallenges in our metric design. They represent atypical or\nextreme scenarios that can skew evaluation results or yield\nmisleading insights into the effectiveness of specifications.\nTo address this, we introduce two additional rules to refine\nthe basic metric definitions.\nOverlapping specifications. The basic definitions of TP and\nFP above do not account for specifications that overlap,\neither partially or completely. When a data point (xi, yi)\nis covered by two or more overlapping specifications (i.e.,\nthere are multiple ϕj∈Φsuch that xi∈ϕXj), it is clas-\nsified as a TP only if its output aligns with all the output\nranges of the overlapping specifications (i.e., yi∈ϕYjfor\neachϕj). Otherwise, it is considered a FP.Unbounded output ranges. Specifications with excessively\nlarge output ranges are not helpful for verification due to be-\ning overly permissive. In the extreme case, if a specification\ndefines an infinite output range, it would be trivially satisfied\nby any neural network, rendering verification useless. To\nrectify this, we introduce additional constraints to the output\nranges and filter out unwanted specifications prior to the\nevaluation procedure. For classification tasks where misclas-\nsification is a critical concern, we require the minimum and\nmaximum values of the output range, ϕmin\nYjandϕmax\nYj, to be\nidentical. This constraint ensures that each specification pre-\ncisely dictates the desired output class. For regression tasks,\nwe require that ϕmax\nYj−ϕmin\nYj≤α×(Ymax−Ymin), where\nα∈(0,1),YmaxandYminare the maximum and minimum\nvalues of the entire dataset. This ensures that the output\nrange of each specification is sufficiently narrow to provide\nmeaningful and actionable insights while still allowing for\nreasonable variability in the model’s predictions.\n4. Specification Generation Algorithms\nIn this section, we explore a suite of specification generation\nalgorithms tailored for classification and regression tasks.\nWe start from the straightforward fixed-size grid algorithm\nand build up to more sophisticated methods like clustering\nalgorithms and a tree traversal algorithm, the core of the\nAutoSpec generation process.\n4.1. Problem Statement\nInput and expected outputs. The input of our specification\ngeneration process is the generation dataset (Xgen, Ygen).\nEach data point in this dataset, represented as a pair (xi, yi),\nconsists of input features xiand labels yi. Specification gen-\neration algorithms are supposed to capture common patterns\nwithin this dataset, transforming them into a coherent set of\nspecifications, denoted as Φ.\nScope and focus. The algorithms we study specifically fo-\ncus on generating hyperrectangle based specifications ( ΦX\nshould be in hyperrectangle shape). We prioritize hyperrect-\nangles due to their simplicity and computational efficiency.\nHyperrectangles are easy to represent and manipulate, lead-\ning to more efficient algorithms for specification generation\nand verification. The generation of other convex polytopes\nis left as future work to expand the method’s applicability\nand precision, albeit at the cost of increased complexity. An\nexample of the generated specification looks like this:\nΦ :{(x1, x2)∈R2|0≤x1≤1,−1≤x2≤2}\n→ {y∈R|3≤y≤5}(1)\nThis specification defines a hyperrectangle in the input space\nand a corresponding range for the output.\nKey problem. The key challenge lies in using the dataset\nto construct meaningful hyperrectangles to split the input\n4\n\nAutoSpec: Automated Generation of Neural Network Specifications\nAlgorithm 1: Specification Extraction Function\n1Function extractSpecification( R,Xgen,Ygen,task):\n2 Collect data points set (XD, YD)⊆(Xgen, Ygen)where XD∈R;\n3 if(XD, YD)is not empty then\n4 iftask is classification then\n5 Compute the most common label ycommon inYD;\n6 return (R, y common );\n7 else\n8 Compute mean andstdof labels in YD;\n9 Define output range\nYrange←[mean−std, mean +std];\n10 return (R, Y range);\n11 end\n12 end\n13 return None\ndata space. Once the hyperrectangles are generated, we can\ndescribe the hyperrectangles boundaries as the specifica-\ntions input constraint ΦX. The corresponding output con-\nstraints are then derived based on the labels of the data\npoints inside each hyperrectangle as described in Algorithm\n1. In classification tasks, the output constraint is determined\nby the most common label present within the hyperrect-\nangle. Additionally, for regression tasks, we calculate the\nmean and standard deviation ( std) of the data points label\ninformation located inside the hyperrectangle. The resulting\noutput constraint for each specification is defined by the\nrange [mean −std, mean +std], providing a statistically\ngrounded interval.\n4.2. Grid-based Generation\nWe start with a straightforward generation algorithm, the\ngrid-based method. The idea is simple: it segments the input\nspace into fixed-size cells; each cell representing a hyper-\nrectangle. We then filter out any cells that are empty (i.e.,\ncontaining no data points from the generation dataset), and\ngenerate specifications only for those with data.\nAlgorithm 3 in Appendix A provides the details. For a\ndataset Xgen, we calculate the minimum ( xmin) and max-\nimum ( xmax) vectors across features. A step size vector t,\ncomputed as t= (xmax−xmin)/β, is then employed to\nsegment each feature dimension, creating a grid structure.\nSubsequently, our specification extraction algorithms are\napplied to each grid cell to formulate the output constraints\nof the specifications.\nLimitations. Despite its structural simplicity, the grid-based\nmethod has several limitations. Primarily, it may not align\nwell with the natural data distribution. It could encompass\nmultiple classes of data points in one cell, leading to incor-\nrect or overly loose output constraints for classification or re-\ngression task respectively. Furthermore, due to its fixed-size\nnature, some input spaces without data points will be filtered,\nreducing the specification set’s coverage. Additionally, the\nmethod’s computational complexity is O(βk), where kis\nthe number of features. It increases exponentially with thenumber of input features, making it impractical for learning\nfrom high-dimensional data.\n4.3. Clustering-based Generation\nThe clustering-based method aims to address the limitations\ninherent in the fixed-size grid approach, especially its rigid-\nity in adapting to diverse data distributions. Algorithm 4 in\nAppendix A details its workflow. It applies clustering algo-\nrithms to the dataset Xgen, facilitating the identification of\nnatural clusters within the data. Each cluster C1, C2, . . . , C k\nrepresents a unique grouping based on data similarity, and\nis used to form an individual specification. For each clus-\nterCi,XCiis defined as the hyperrectangle formed by the\nextremities of the data points within the cluster. Mathemati-\ncally, we determine the minimum xmin\nCiand maximum xmax\nCi\nboundaries across each dimension of the feature space for\nthe data points in cluster Ci. The resulting hyperrectangle\nRCiis then given by RCi=Qn\nj=1[xmin\nCi,j, xmax\nCi,j], where n\nis the number of features. The clustering algorithm’s time\ncomplexity of O(N2), where Nis the data input size. Com-\npared with the grid method, this clustering algorithm is sig-\nnificantly more scalable.\nLimitations. The clustering-based method, adept at captur-\ning the input feature space, encounters specific limitations.\nIts primary challenge is the inadequate utilization of label\ninformation ( Ygen). During clustering, only Xgenis used,\nleading to groupings based solely on proximity in the input\nfeature space. This approach may result in clusters that in-\nclude data points from multiple classes or with significant\nvariance in the label space, leading to incorrect or overly\nbroad output constraints for classification or regression tasks.\nAdditionally, the clustering method does not cover the in-\nput space outside of clusters, potentially leaving substantial\nareas unaddressed.\n4.4.AutoSpec : Tree Traversing Generation\nTo overcome the limitations of previous algorithms, espe-\ncially their insufficient utilization of label information and\nincomplete data space coverage, we introduce AutoSpec .\nAutoSpec integrates decision tree mechanics into the speci-\nfication generation process, striking a balance between ac-\ncuracy and comprehensiveness.\nWe begin with an in-depth understanding of the decision\ntree-based classifier training procedure. In training a deci-\nsion tree, we start with the entire dataset at the root. The\ndata is then split based on a selected feature that effectively\nseparates the classes. This selection is typically based on\nmeasures like information gain or Gini impurity, which quan-\ntify how well a feature separates the classes. The process\nrecursively continues on each derived subset in a greedy\nmanner. At each step, the algorithm selects the best split at\nthat particular node rather than looking ahead and planning\nfuture splits.\n5\n\nAutoSpec: Automated Generation of Neural Network Specifications\nAlgorithm 2: Tree Traversing Specification Generation\nInput : Dataset Xgen,Ygen, Task Type task\nOutput : Set of Specifications Specs\n1Initialize Specs← ∅ ;\n2Train a decision tree on Xgen;\n3foreach leaf node Liin the decision tree do\n4 Initialize empty lists minBounds ,maxBounds ;\n5 foreach condition Ckin the path to Lido\n6 Extract feature fand threshold tfromCk;\n7 Update minBounds [f]andmaxBounds [f]based on Ck;\n8 end\n9 SetR←Qn\nj=1[minBounds [j], maxBounds [j]];\n10 spec←extractSpecification( R,Xgen,Ygen,task);\n11 ifspec is not None then\n12 Addspec toSpecs ;\n13 end\n14end\n15return Specs\nThis approach naturally lends itself to specification gen-\neration in AutoSpec . In our context, each split made by\nthe decision tree helps in partitioning the data space into\nsmaller sections, each aimed at reducing label variance. This\nis achieved by ensuring that data points in each internal node\nare as similar as possible, leading to a more homogenous\ndistribution in each resulting hyperrectangle. These hyper-\nrectangles correspond to the leaf nodes of the decision tree\nwhere the label variance is minimized. By traversing the tree\nfrom the root to a leaf node using Depth-First Search (DFS),\nwe can delineate the input space for each specification, en-\nsuring that the specifications are as precise and meaningful\nas possible. We then use our specification extraction func-\ntion to extract specifications from these hyperrectangles.\nThe details are included in Algorithm 2.\nRegarding computational efficiency, the time complexity\nofAutoSpec is primarily influenced by two components:\nthe decision tree training and the DFS traversal. The time\ncomplexity for training a decision tree is generally O(Nkd),\nwhere Nis the number of training examples, kis the num-\nber of features, and dis the depth of the tree, which in the\nworst-case scenario can approach O(N). The DFS traver-\nsal, which operates on the structure of the tree, has a com-\nplexity of O(v+e), where vis the number of nodes (ver-\ntices) and eis the number of edges in the tree. Since the\nnumber of nodes and edges in a decision tree is linearly\nproportional to the number of data points, the DFS com-\nponent also scales linearly with the data. Therefore, the\noverall time complexity of AutoSpec can be approximated\nasO(Nkd +N) =O(Nkd), which allows for efficient\nscaling to larger datasets.\nWe present a visualization of the specifications generated by\ndifferent algorithms on a 2D dataset in Figure 2 (specifica-\ntions as rectangles, data points as dots, and different colors\nfor different points’ corresponding classes). This illustration\nhighlights that AutoSpec achieves the most comprehensive\ncoverage of the input space compared to other methods. The\n2\n1\n01234\nX02\n1\n012X1\nGrid\n2\n1\n01234\nX02\n1\n012\nClustering\n2\n1\n01234\nX02\n1\n012\nAutoSpecClass 0 Class 1 Class 2 Class 3Figure 2: Visualization of the specification set generated by\ndifferent algorithms.\neffectiveness and accuracy of these generated specifications\nare further evaluated in Section §5.\n5. Experimental Evaluation\nWe answer three questions in this section:\n•How does AutoSpec perform on diverse datasets, and are\nthe metrics capable of distinguishing different specifica-\ntion generation algorithms?\n•How do auto-generated specifications compare with\nhuman-designed specifications?\n•Are auto-generated specifications useful in downstream\ntasks, and can they detect anomalies in trained DNNs?\nDatasets. We choose four different datasets: one toy dataset\nfor visualization and three across various systems.\n•Toy spiral dataset. The spiral dataset is a synthetic 2D\nclassification dataset (spi, 2022), designed for easy visu-\nalization. The dataset has three classes, where each class\nhas 300 data points. The task is to predict one of three\nclasses based on a 2D location (X0, X1).\n•Uplink throughput prediction dataset. Throughput pre-\ndiction is a critical task in a range of network systems,\nsuch as for resource management (Kumar & Singh, 2018;\nChien et al., 2019; Fu & Wang, 2022). We use the Colos-\nseum O-RAN Dataset (Bonati et al., 2021), which con-\ntains the base station uplink throughput in various situa-\ntions. We extract the uplink throughput information out\nand form the dataset. The targeted task is a time series\nforecasting job, which uses four historical values as input\nto predict the throughput at the current timestamp. Thus,\nthe specification’s input constraint has four dimensions,\nwhile the output constraint has one dimension.\n•Intrusion detection dataset. Intrusion detection and pre-\nvention systems (IDSs/IPSs) are important in safeguard-\ning against attacks in cyber environments. We experiment\nwith the CIC-IDS2017 dataset (Sharafaldin et al., 2018),\na dataset on intrusion detection through classification\ntasks. The dataset comprises both benign and malicious\nnetwork traffic, encompassing a variety of attack types.\nThe dataset features labeled network flows with detailed\nmetadata like timestamps and IP addresses. The model\n6\n\nAutoSpec: Automated Generation of Neural Network Specifications\nApplication Input Dimension MethodsMetrics\n#TP #FP #FN Precision (%) Recall (%) F1 (%)\nToy Spiral Data 2Grid 88 1 1 98.87 98.87 98.87\nClustering 73 2 15 97.33 82.95 89.57\nAutoSpec 89 1 0 98.88 100.00 99.44\nThroughput Prediction 4Human 73,249 7,520 90,606 90.68 44.70 59.88\nGrid 134,280 36,475 620 78.63 99.54 87.86\nClustering 124,458 46,506 411 72.79 99.67 84.14\nAutoSpec 168,726 2,649 0 98.45 100.00 99.22\nIntrusion Detection 78Grid — — — — — —\nClustering 2,117,547 314,434 2,970 87.07 99.85 93.02\nAutoSpec 2,395,672 39,279 0 98.39 100.00 99.18\nBeam Management 4096Grid — — — — — —\nClustering 4,197 24,370 16,433 14.69 20.34 17.06\nAutoSpec 19,451 25,549 0 43.22 100.00 60.46\nTable 1: Evaluation results across datasets. Bold and underline indicate the best and second-best results for each dataset.\ninput represents these labeled flows as a feature vector\nof 78 dimensions, and aims to classify them into either\nbenign or malicious categories (e.g., brute force FTP\nand brute force SSH). The output is typically a single\nlabel for each flow, out of nine labels in total. Thus, we\nwill form the specification with an input constraint of 78\ndimensions and an output constraint of 1 dimension.\n•Beam management dataset. Highly directional millimeter\nwave (mmWave) radios perform beam management to\nestablish and maintain reliable links. DeepBeam (Polese\net al., 2021) proposes to use deep learning models for\ncoordination-free beam management. It releases a dataset\nthat includes signal samples (I/Q samples) under vari-\nous conditions, with features like different transmitter\nbeams and receiver gain levels. The DeepBeam model\nuses these samples as input feature vectors, which have\n4096 dimensions in total for beam classification. The\noutput will be classified as transmit beam information, a\nsingle-dimension value about the label information.\nData processing. All the above datasets are originally de-\nsigned for training deep learning models. Instead, we use\nthem to automatically generate specifications that could\nregularize model behaviors. We split each dataset into two\nfolds in a 9:1 ratio: the first is used for specification genera-\ntion, and the remainder is used for evaluating the generated\nspecifications.\nMetrics. In the following experiments, we use the metrics\ndefined by our framework ( §3): TP, FP, FN, precision, recall,\nand F1 score.\n5.1.AutoSpec ’s performance\nBaselines. We employ the fix-sized grid (“Grid”) and\nclustering-based methods (“Clustering”) as the baseline al-\ngorithms. For the fix-sized grid algorithm, we choose βto be10; for the clustering algorithm, we employ k-means as the\nclustering method. Regarding the number of clusters, after\ntuning the parameters, we use 30, 100, 1000, 1000 for spiral,\nuplink throughput, intrusion detection, and beam manage-\nment tasks, respectively. We also ask a human expert to\ndefine specifications for the throughput prediction task. The\nexpert is unable to define specifications for other tasks like\nintrusion detection and beam management because the input\nfeature spaces are too complicated. To ensure the generated\nspecification has a tight output range, we set αto10%.\nResults. We run AutoSpec and baselines on the four\ndatasets. Table 1 shows the results. We observe that Au-\ntoSpec performs consistently the best across all datasets and\napplications. Overall, AutoSpec improves the F1 score over\nthe second best by 18% on average.\nIn the first two datasets, where input dimensions are small,\nthe Grid algorithm also achieves good performance. How-\never, for intrusion detection and beam management, due to\nthe high-dimensional input space, the Grid’s time complex-\nity grows exponentially, and it times out when producing\nspecifications. Specifically, the algorithm was allowed to run\nfor 6 hours on a 64-Core Processor before being terminated,\nyet it failed to produce specifications within this timeframe.\nClustering algorithm performs worse than the Grid and Au-\ntoSpec , but it runs fast (in polynomial time) and can generate\nspecifications for high-dimensional spaces. We also observe\non the beam management dataset, that both Clustering and\nAutoSpec ’s performance drops significantly. Our hypothesis\nis that the problem is fundamentally hard with an increased\nnumber of input dimensions. This suggests future work of\nbuilding algorithms for high-dimensional inputs.\nA closer look at auto-generated specifications. To better\nillustrate the generated specifications, we visualize the toy\nspiral dataset: Figure 3 and Figure 4 depict specifications\n7\n\nAutoSpec: Automated Generation of Neural Network Specifications\n1.0\n 0.5\n 0.0 0.5 1.0\nX01.0\n0.5\n0.00.51.0X1\nGrid\n1.0\n 0.5\n 0.0 0.5 1.0\nX01.0\n0.5\n0.00.51.0\nClustering\n1.0\n 0.5\n 0.0 0.5 1.0\nX01.0\n0.5\n0.00.51.0\nAutoSpecClass 0 Class 1 Class 2\nFigure 3: Specification visualization on spec gen dataset.\n1.0\n 0.5\n 0.0 0.5 1.0\nX01.0\n0.5\n0.00.51.0X1\nGrid\n1.0\n 0.5\n 0.0 0.5 1.0\nX01.0\n0.5\n0.00.51.0\nClustering\n1.0\n 0.5\n 0.0 0.5 1.0\nX01.0\n0.5\n0.00.51.0\nAutoSpecTP (True Positive) FP (False Positive) FN (False Negative) Figure 4: Specification visualization on spec eval dataset.\n(rectangles), data points (dots), and points’ corresponding\nclasses (colors). Figure 3 plots the specification generation\ndataset. From the figure, we observe that neither Clustering\nnor Grid considers the information in label space while they\nare splitting the feature space to formulate the specifica-\ntions. Some specifications contain data points from multiple\nclasses, leading to FPs in the specification evaluation phase.\nWe also observe that AutoSpec splits the input space into\nsubspaces and generates specifications using each subspace,\nleading to a high recall. Meanwhile, all specifications cre-\nated by AutoSpec only contain data from one class and thus\nachieve a high precision.\n5.2. Comparison with human-defined specifications\nWe compare AutoSpec -generated specifications against\nhuman-defined ones for the throughput prediction dataset.\nTo accommodate the tight output constraint in specifica-\ntions, continuous throughput values are divided into ten\nintervals (0–9), with each interval representing a percentage\nrange of the maximum throughput (e.g., 0 for 0–10% of\nThroughput max). This allows us to only have to choose a\nlabel to represent the output range for a specification.\nThe human-defined specifications are designed with two\nprinciples: (a) monotonic trend : when historical throughputs\nshow a monotonic increase or decrease, it is expected that the\ncurrent throughput will follow this trend. Linear regression\nis employed to fit the previous data and predict the output to\na label; (b) stable trend : if the historical throughput values\nremain steady, it is highly probable that the next throughput\nwill be similar.\nThe human-defined specification set is generated by iter-\nating through all possible combinations and selecting the\nones that satisfy the principles. The details are included\nin Appendix B, where Table 2 presents three specification\nexamples. Then, we compare the human-defined specifica-\ntions with AutoSpec -generated ones and show in the results\nFigure 5. We observe that AutoSpec outperforms human-\ndefined specifications by 39% on F1 score. Human-defined\nspecifications, despite having a high precision, also yield a\nlow recall. This is not surprising, as human experts can only\nthink of specifications for a small number of scenarios andType x[0] x[1] x[2] x[3] y(Prediction)\nMonotonic Increase 0 2 4 6 8\nMonotonic Decrease 9 7 5 3 1\nStable 0 0 0 0 0\nTable 2: Examples of human-defined specifications.\n#TP #FP #FN01e52e5Human AutoSpec\nPrecision Recall F10%50%100%Human AutoSpec\nFigure 5: Human-defined specifications vs. AutoSpec -\ngenerated specifications.\nhave difficulty covering all possible cases.\n5.3. Verifying DNNs with generated specifications\nVerification engine. To verify neural networks with the\ngenerated specifications, we use auto LiRPA (Xu et al.,\n2020; 2021; Wang et al., 2021) as our verification engine,\nwhich allows automatic bound derivation and computation\nfor general computational graphs.\nExperiment setup. The goal is to verify a trained neural\nnetwork against AutoSpec -generated specifications to iden-\ntify potential vulnerabilities. For the experiment, we use\nthe same dataset—throughput prediction dataset—for both\ntraining the model and generating the specifications. The\nmodel is a simple four-layer fully connected neural network\nwith ReLU, with 4, 10, 5, and 1 neurons in each layer. We\nterminate the training when its loss stabilizes and loss varia-\ntion≤1×10−3. Then, we start the verification. If the model\nfails the verification, we extract data points that violate the\nspecifications. These points, termed as counterexamples,\nhighlight cases when the model’s behavior deviates from\nthe expected outcomes.\nDetected anomalies. An illustrative counterexample is pre-\nsented in Figure 6. Analysis of the historical throughput\nvalues suggests a monotonically increasing trend. However,\nthe model’s predictions, contrastingly, indicate a substantial\ndecrease, as depicted by the red point (the specification’s\n8\n\nAutoSpec: Automated Generation of Neural Network Specifications\nT1 T2 T3 T4Tcurrent\nTimestamp0200000400000600000Throughput (Mbps)\nHistorical Throughput\nCurrent Predicted Throughput\nSpecification Output Range\nFigure 6: Monotonic increase counterexample.\nexpected output range is described by the green line). This\ndiscrepancy not only flags an anomaly but also exposes\nunderlying vulnerabilities within the model. We discuss\nadditional counterexamples in Appendix C.\n6. Discussion\nIn this section, we discuss some of the motivations and\nquestions in this work.\n6.1. Why learn from data?\nIn the context of machine learning systems, some previous\nworks propose utilizing specifications to verify neural net-\nwork safety before deployment into the system (Eliyahu\net al., 2021; Wei et al., 2023). However, all these previous\nworks heavily rely on human experts to define the specifica-\ntion set, and these specification sets are often loose and can\nhave errors. Considering the procedure of human experts in\ndefining the specifications, they rely on their understanding\nof the system and their historical observation of the system\nbehavior. Such an observation procedure can be considered\nas a step to learn from experience/data, which is a natural\ntask for machine learning to automatically learn from data.\n6.2. Correctness of learned specifications\nTraditionally, specifications defined by domain experts have\nbeen considered as ground-truth laws that must not be vio-\nlated. However, these expert-defined specifications are in-\nherently limited by human experience and may occasionally\nbe incorrect or incomplete. In contrast, our work proposes a\ndata-driven approach to specification generation. While this\nmethod offers the potential to uncover patterns overlooked\nby human experts, it also introduces the risk of generating\nincorrect or spurious specifications, especially if the dataset\ncontains noise or biases. To mitigate this risk, we could em-\nploy rigorous evaluation and filtering processes, selecting\nonly those specifications that demonstrate a high probabil-\nity of correctness based on our evaluation criteria. These\nhigh-confidence, data-derived specifications are then used\nto guide and constrain neural network behavior, potentially\nleading to more robust and reliable models.6.3. Why Decision Trees?\nOur specification generation methods utilize decision trees\ndue to their inherent advantages in this context. Deci-\nsion trees naturally partition the input space, aligning with\nour goal of generating hyperrectangle based specifications.\nTheir hierarchical structure offers interpretability, crucial for\nunderstanding and validating derived rules. Recent studies\nhave shown decision trees can perform comparably to tradi-\ntional neural networks in certain tasks (Meng et al., 2019;\nZhuang et al., 2024).\n7. Conclusion\nFormal verification of neural networks offers significant\npotential for ensuring the safety and reliability of learning-\naugmented systems. However, current verification frame-\nworks rely on human experts to manually design specifica-\ntions, a process that is error-prone, incomplete, and unscal-\nable. This paper presents AutoSpec , the first framework to\nautomatically generate specifications for neural networks in\nlearning-augmented systems, alongside a set of evaluation\nmetrics to quantify the quality of these generated specifi-\ncations. Experimental results across four diverse datasets\ndemonstrate that AutoSpec outperforms both expert-defined\nspecifications and baseline methods.\nReferences\nSpiral toy dataset, 2022. URL https://cs231n.git\nhub.io/neural-networks-case-study/ .\nAmmons, G., Bodik, R., and Larus, J. R. Mining specifica-\ntions. ACM Sigplan Notices , 37(1):4–16, 2002.\nBak, S. and Tran, H.-D. Neural network compression of\nacas xu early prototype is unsafe: Closed-loop verification\nthrough quantized state backreachability. In NASA Formal\nMethods Symposium , pp. 280–298. Springer, 2022.\nBonati, L., D’Oro, S., Polese, M., Basagni, S., and Melodia,\nT. Intelligence and learning in o-ran for data-driven nextg\ncellular networks. IEEE Communications Magazine , 59\n(10):21–27, 2021.\nChien, W.-C., Lai, C.-F., and Chao, H.-C. Dynamic resource\nprediction and allocation in c-ran with edge artificial in-\ntelligence. IEEE Transactions on Industrial Informatics ,\n15(7):4306–4314, 2019.\nEliyahu, T., Kazak, Y ., Katz, G., and Schapira, M. Verifying\nlearning-augmented systems. In Proceedings of the 2021\nACM SIGCOMM 2021 Conference , pp. 305–318, 2021.\nFu, Y . and Wang, X. Traffic prediction-enabled energy-\nefficient dynamic computing resource allocation in cran\nbased on deep learning. IEEE Open Journal of the Com-\nmunications Society , 3:159–175, 2022.\n9\n\nAutoSpec: Automated Generation of Neural Network Specifications\nGeng, C., Le, N., Xu, X., Wang, Z., Gurfinkel, A., and Si,\nX. Towards reliable neural specifications. In Inter-\nnational Conference on Machine Learning , pp. 11196–\n11212. PMLR, 2023.\nJay, N., Rotman, N., Godfrey, B., Schapira, M., and Tamar,\nA. A deep reinforcement learning perspective on inter-\nnet congestion control. In International Conference on\nMachine Learning , pp. 3050–3059. PMLR, 2019.\nJin, S., Zhu, R., Hassan, A., Zhu, X., Zhang, X., Mao, Z. M.,\nQian, F., and Zhang, Z.-L. Oasis: Collaborative neural-\nenhanced mobile video streaming. In Proceedings of the\n15th ACM Multimedia Systems Conference , pp. 45–55,\n2024.\nKatz, G., Huang, D. A., Ibeling, D., Julian, K., Lazarus, C.,\nLim, R., Shah, P., Thakoor, S., Wu, H., Zelji ´c, A., et al.\nThe marabou framework for verification and analysis of\ndeep neural networks. In Computer Aided Verification:\n31st International Conference, CAV 2019, New York City,\nNY, USA, July 15-18, 2019, Proceedings, Part I 31 , pp.\n443–452. Springer, 2019.\nKraska, T., Beutel, A., Chi, E. H., Dean, J., and Polyzotis,\nN. The case for learned index structures. In Proceedings\nof the 2018 international conference on management of\ndata, pp. 489–504, 2018.\nKumar, J. and Singh, A. K. Workload prediction in cloud\nusing artificial neural network and adaptive differential\nevolution. Future Generation Computer Systems , 81:41–\n52, 2018.\nLe, T.-D. B. and Lo, D. Deep specification mining. In\nProceedings of the 27th ACM SIGSOFT International\nSymposium on Software Testing and Analysis , pp. 106–\n117, 2018.\nLemieux, C., Park, D., and Beschastnikh, I. General ltl speci-\nfication mining (t). In 2015 30th IEEE/ACM International\nConference on Automated Software Engineering (ASE) ,\npp. 81–92. IEEE, 2015.\nLiu, H. and Lang, B. Machine learning and deep learn-\ning methods for intrusion detection systems: A survey.\napplied sciences , 9(20):4396, 2019.\nMao, H., Netravali, R., and Alizadeh, M. Neural adaptive\nvideo streaming with pensieve. In Proceedings of the\nconference of the ACM special interest group on data\ncommunication , pp. 197–210, 2017.\nMao, H., Schwarzkopf, M., Venkatakrishnan, S. B., Meng,\nZ., and Alizadeh, M. Learning scheduling algorithms\nfor data processing clusters. In Proceedings of the ACM\nspecial interest group on data communication , pp. 270–\n288. 2019.Meng, Z., Chen, J., Guo, Y ., Sun, C., Hu, H., and Xu, M.\nPitree: Practical implementation of abr algorithms using\ndecision trees. In Proceedings of the 27th ACM Interna-\ntional Conference on Multimedia , pp. 2431–2439, 2019.\nPolese, M., Restuccia, F., and Melodia, T. DeepBeam: Deep\nWaveform Learning for Coordination-Free Beam Man-\nagement in mmWave Networks. Proc. of ACM Interna-\ntional Symposium on Mobile Ad Hoc Networking and\nComputing (MobiHoc) , 2021.\nSharafaldin, I., Lashkari, A. H., and Ghorbani, A. A. Toward\ngenerating a new intrusion detection dataset and intrusion\ntraffic characterization. ICISSp , 1:108–116, 2018.\nTan, C., Zhu, Y ., and Guo, C. Building verified neural net-\nworks with specifications for systems. In Proceedings of\nthe 12th ACM SIGOPS Asia-Pacific Workshop on Systems ,\npp. 42–47, 2021.\nWang, S., Zhang, H., Xu, K., Lin, X., Jana, S., Hsieh, C.-J.,\nand Kolter, J. Z. Beta-CROWN: Efficient bound prop-\nagation with per-neuron split constraints for complete\nand incomplete neural network verification. Advances in\nNeural Information Processing Systems , 34, 2021.\nWei, T., Liu, C., Jia, Z., and Tan, C. Building verified neural\nnetworks for computer systems with ouroboros. Proceed-\nings of Machine Learning and Systems , 5, 2023.\nWu, H., Barrett, C., Sharif, M., Narodytska, N., and Singh,\nG. Scalable verification of gnn-based job schedulers.\nProceedings of the ACM on Programming Languages , 6\n(OOPSLA2):1036–1065, 2022.\nXu, K., Shi, Z., Zhang, H., Wang, Y ., Chang, K.-W., Huang,\nM., Kailkhura, B., Lin, X., and Hsieh, C.-J. Automatic\nperturbation analysis for scalable certified robustness and\nbeyond. Advances in Neural Information Processing\nSystems , 33, 2020.\nXu, K., Zhang, H., Wang, S., Wang, Y ., Jana, S., Lin, X., and\nHsieh, C.-J. Fast and Complete: Enabling complete neural\nnetwork verification with rapid and massively parallel\nincomplete verifiers. In International Conference on\nLearning Representations , 2021. URL https://open\nreview.net/forum?id=nVZtXBI6LNn .\nYan, F. Y ., Ayers, H., Zhu, C., Fouladi, S., Hong, J., Zhang,\nK., Levis, P., and Winstein, K. Learning in situ: a ran-\ndomized experiment in video streaming. In 17th USENIX\nSymposium on Networked Systems Design and Implemen-\ntation (NSDI 20) , pp. 495–511, 2020.\nZhuang, Y ., Liu, L., Singh, C., Shang, J., and Gao, J. Learn-\ning a decision tree algorithm with transformers. arXiv\npreprint arXiv:2402.03774 , 2024.\n10\n\nAutoSpec: Automated Generation of Neural Network Specifications\nA. Pseudo code for specification generation algorithms\nIn this section, we present the pseudo-code for grid-based and clustering-based specification generation algorithms.\nAlgorithm 3: Grid-based Generation\nInput : Dataset Xgen,Ygen, Step size vector t, Task Type task , Input Feature Dimension: N\nOutput : Set of Specifications Specs\n1Initialize Specs ← ∅;\n2Compute xminandxmaxfrom Xgen;\n3forp1in range xmin\n1toxmax\n1with step size t1do\n4 . . .\n5 forpNin range xmin\nNtoxmax\nNwith step size tNdo\n6 Define rectangle R←QN\nj=1[pj, pj+tj];\n7 spec←extractSpecification( R,Xgen,Ygen,task);\n8 ifspec is not None then\n9 Addspec toSpecs ;\n10 end\n11 end\n12 . . .\n13end\n14return Specs\nAlgorithm 4: Clustering-based Generation\nInput : Feature set Xgen, Labels Ygen, Number of clusters k, Task Type task\nOutput : Set of Specifications Specs\n1Initialize Specs ← ∅;\n2Apply clustering algorithm to Xgenforming clusters C= [C1, C2, . . . , C k];\n3foreach cluster Cido\n4 Compute xmin\nCiandxmax\nCifor cluster CiinC;\n5 Define rectangle RCi←Qn\nj=1[xmin\nCi,j, xmax\nCi,j];\n6 spec←extractSpecification( RCi,Xgen,Ygen,task);\n7 ifspec is not None then\n8 Addspec toSpecs ;\n9 end\n10end\n11return Specs\nB. Human Defined Specification\nThis section outlines examples of human-designed specifications. Each specification has been translated into actual through-\nput speeds, based on the highest observed throughput of 919264 Mbps. The labels from “0” to “9” represent different ranges\nof throughput. For example, label “0” means the throughput is between 0 and 91926.4 Mbps. Each label after “0” covers an\nequally sized higher range of speeds, all the way up to the top speed. We list parts of our specifications in Table 3.\nC. Counterexamples\nWe list two main counterexamples revealed by our generated specification set. Figure 7 illustrates an unexpected model\nbehavior: although the anticipated trend for the current timestamp throughput is a continuous increase, the model’s predicted\noutput exhibits a substantial decrease. This deviation from the expected behavior is an anomaly. Conversely, Figure 8 presents\nthe reverse scenario. In this case, despite the projected trend indicating a decrease, the model’s output shows an increase,\nfurther underscoring the discrepancies between the expected and actual model behaviors, revealing the vulnerabilities.\n11\n\nAutoSpec: Automated Generation of Neural Network Specifications\nType x[0] x[1] x[2] x[3] y (Prediction)\nStable 0 0 0 0 0\nIncreasing 0 1 2 3 4\nIncreasing 0 1 2 4 5\nIncreasing 0 1 2 5 6\nIncreasing 0 1 2 6 7\nIncreasing 2 4 5 7 8\nDecreasing 5 4 1 0 0\nDecreasing 5 4 2 0 0\nDecreasing 5 4 2 1 0\nDecreasing 9 8 7 6 5\nStable 9 9 9 9 9\nTable 3: Examples of human-defined specifications.\nT1 T2 T3 T4Tcurrent\nTimestamp0200000400000600000Throughput (Mbps)\nHistorical Throughput\nCurrent Predicted Throughput\nSpecification Output Range\nFigure 7: Monotonic increase counterexample.\nT1 T2 T3 T4Tcurrent\nTimestamp0200000400000600000800000Throughput (Mbps)\nHistorical Throughput\nCurrent Predicted Throughput\nSpecification Output Range Figure 8: Monotonic decrease counterexample.\n12",
  "textLength": 53422
}