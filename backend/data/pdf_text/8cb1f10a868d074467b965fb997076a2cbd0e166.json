{
  "paperId": "8cb1f10a868d074467b965fb997076a2cbd0e166",
  "title": "Apiary: A DBMS-Backed Transactional Function-as-a-Service Framework",
  "pdfPath": "8cb1f10a868d074467b965fb997076a2cbd0e166.pdf",
  "text": "Apiary: A DBMS-Integrated Transactional Function-as-a-Service Framework\nPeter Kraft1∗, Qian Li1*, Kostis Kaffes1, Athinagoras Skiadopoulos1, Deeptaanshu Kumar3, Danny Cho1,\nJason Li2, Robert Redmond2, Nathan Weckwerth2, Brian Xia2, Peter Bailis1, Michael Cafarella2,\nGoetz Graefe4, Jeremy Kepner2, Christos Kozyrakis1, Michael Stonebraker2, Lalith Suresh5,\nXiangyao Yu6, and Matei Zaharia1\n1Stanford,2MIT,3CMU,4Google,5VMware,6University of Wisconsin-Madison\nAbstract\nDevelopers increasingly use function-as-a-service (FaaS) plat-\nforms for data-centric applications that perform low-latency\nand transactional operations on data, such as for microser-\nvices or web serving. Unfortunately, existing FaaS platforms\nsupport these applications poorly because they physically\nand logically separate application logic, executed in cloud\nfunctions, from data management, done in interactive trans-\nactions accessing remote storage. Physical separation harms\nperformance while logical separation complicates efficiently\nproviding transactional guarantees and fault tolerance.\nThis paper introduces Apiary, a novel DBMS-integrated\nFaaS platform for deploying and composing fault-tolerant\ntransactional functions. Apiary physically co-locates and log-\nically integrates function execution and data management by\nwrapping a distributed DBMS engine and using it as a unified\nruntime for function execution, data management, and opera-\ntional logging, thus providing similar or stronger transactional\nguarantees as comparable systems while greatly improving\nperformance and observability. To allow developers to write\ncomplex stateful programs, we leverage this integration to\nenable efficient and fault-tolerant function composition, build-\ning a frontend for orchestrating workflows of functions with\nthe guarantees that each workflow runs to completion and\neach function in a workflow executes exactly once. We eval-\nuate Apiary against research and production FaaS platforms\nand show it outperforms them by 2–68 ×on microservice\nworkloads by reducing communication overhead.\n1 Introduction\nFunction-as-a-service (FaaS), or serverless, cloud offerings\nare becoming popular in both industry and research appli-\ncations [23]. In widely-used FaaS platforms like AWS Step\nFunctions [7] and Azure Durable Functions [31], developers\nwrite programs as workflows of stateless functions whose\ndeployments are managed by the service provider. FaaS radi-\ncally reduces the operational complexity of cloud deployment\nby eliminating the need to manage application servers.\nThe FaaS model is increasingly popular for data-centric\napplications: low-latency and transactional applications such\nas an e-commerce web service. Unfortunately, existing FaaS\n*Both authors contributed equally to this paper.\nClientsClientsa) Existing FaaS Platformsb) ApiaryRemote Cloud DatabaseClientsApiaryDB QueriesDB QueriesInvocations\nλ\nFaaS RuntimeλλλλInvocationsλλλλλλλλClientsClientsClients\nDBMS-Integrated FaaS Runtime\nλ\nλ\nλFigure 1: Existing FaaS platforms separate application logic,\nexecuted in cloud functions, from data management, done in\ninteractive transactions accessing a remote database. Apiary\ninstead tightly integrates application logic and data management,\nexecuting functions in DBMS stored procedures.\nplatforms support these applications poorly because they both\nphysically and logically separate function execution from data\nmanagement, adopting the architecture of Figure 1a and call-\ning a remote DBMS once per data operation. Physical separa-\ntion causes high communication overhead: in our experiments\nwith OpenWhisk (Figure 2), communication accounts for as\nmuch as 98% of function runtime. Logical separation com-\nplicates fault tolerance and transactional guarantees because\nfunctions may be arbitrarily re-executed and transactions can-\nnot span across functions. There have been several attempts\nto tackle these problems, such as [15, 22, 42, 47, 48], pro-\nviding either physical co-location for good performance or\nlogical integration for strong gurantees, but not both. Some,\nlike Cloudburst [42], physically co-locate compute and data\nusing local caches, but do not provide transactions. Others,\nlike Boki [22] and Beldi [47], provide transactional functions\nusing an external transaction manager over remote storage,\nincreasing already-high storage access times by 3 ×.\nIn this paper, we present Apiary, a transactional, high-\nperformance FaaS platform for data-centric applications. Un-\nlike existing platforms, Apiary physically co-locates and logi-\ncally integrates function execution and data management by\nwrapping a distributed DBMS engine and using it as a uni-\nfied runtime for function execution, data management, and\noperational logging (Figure 1b). We compile functions to\nstored procedures, routines in a non-SQL language that run\nnatively as DBMS transactions, thus making functions basic\nunits of both control flow and atomicity. We then leverage this\nintegration to support efficient, fault-tolerant function com-\n1arXiv:2208.13068v2  [cs.DB]  30 Jun 2023\n\nEstablish a DB connection (777 μs)Issue query to remote DB (340 μs)DB exec.(23 μs)Function execution (1117μs)Figure 2: Latency breakdown for an OpenWhisk function per-\nforming a point database update. Query execution accounts for\nonly 2% of the overall function execution time.\nposition and new observability capabilities. We demonstrate\nthat Apiary provides (a)similar or stronger guarantees than\ncomparable platforms, (b)performance improvements of 2–\n68×compared to state-of-the-art systems, and (c)automatic\ntracing of application-database interactions for observability.\nThe major challenge in designing a FaaS platform for data-\ncentric applications is providing efficient, fault-tolerant func-\ntion composition. FaaS developers write complex programs\nby composing functions into workflows . To be robust to fail-\nures, workflows require strong execution guarantees: they\nmust always run to completion and each of their functions\nmust execute exactly once. Existing platforms provide these\nguarantees by requiring functions to be idempotent [6] or\nbuilding costly external transaction managers [47]. By con-\ntrast, we can leverage Apiary’s integration of functions and\ndata to build a fault-tolerant frontend providing these guaran-\ntees with minimal developer requirements and low overhead.\nBecause functions are stored procedures, Apiary can instru-\nment them to transactionally record their executions in the\nDBMS. Therefore, if a workflow execution fails, the frontend\ncan safely resume it by invoking its functions from the begin-\nning, skipping functions that have already executed. However,\nnaively instrumenting all functions is expensive, degrading\nperformance up to 2.2 ×. Thus, we develop a novel algorithm\nto identify when functions can be safely re-executed, reducing\noverhead to <5%.\nApiary also tackles a common challenge faced by FaaS\ndevelopers: obtaining observability into how applications in-\nteract with data. Existing platforms can record function execu-\ntions, but it is hard for them to capture interactions with data\nbecause they lack visibility into how functions manage data,\nso developers must resort to expensive and error-prone man-\nual logging across many functions. Apiary naturally has this\nvisibility because it tightly integrates functions and data and\ncan leverage existing techniques for database provenance cap-\nture. Therefore, we build a tracing layer that traces application\ncontrol flow across functions through workflow instrumenta-\ntion, then records which data items each function accesses\nor updates through query instrumentation and change data\ncapture to produce a complete history of application interac-\ntions with data. Obtaining this detailed information through\nmanual logging incurs overhead of up to 92%, but our tracing\nlayer reduces this to <15% by building a high-performance\nin-memory buffer and exporting its contents asynchronously.\nWe evaluate Apiary with commonly used microservice\nand web serving benchmarks such as social networks and\ne-commerce sites [18, 19]. We show that by reducing com-\n0 20 40 60 80 100\nExecution Time Percentage (%)RetwisHotelShopApplication Logic DB Query Exec. Service-DB CommunicationFigure 3: Average execution time breakdown for three data-\ncentric applications used in evaluation (§7). All three spend\nmost of their runtime communicating with the remote database\n(over long-lived connections) or executing database operations.\nmunication overhead, it outperforms the popular open-source\nFaaS platform OpenWhisk [34] by 7–68 ×and recent research\nsystems like Cloudburst [42] and Boki [22] by 2–27 ×.\nIn summary, our contributions are:\n•We propose Apiary, a transactional FaaS platform that phys-\nically co-locates and logically integrates functions with\ndata by wrapping a distributed DBMS. Apiary outperforms\nresearch and production platforms by 2–68 ×while provid-\ning similar or stronger guarantees.\n•We leverage Apiary’s architecture to design a fault-tolerant\nfrontend for orchestrating workflows of functions. It guar-\nantees that regardless of failures, workflows always run to\ncompletion and their functions each execute exactly once.\n•We use Apiary’s architecture to enhance observability by\nautomatically instrumenting applications and their inter-\nactions with data, achieving <15% overhead compared to\n92% with manual logging.\n2 Apiary Overview\n2.1 System Architecture\nApiary’s design is motivated by a key observation: data-\ncentric applications spend most of their runtime either com-\nmunicating with a DBMS or executing DBMS operations. As\nwe show in Figure 3, these account for 93-99% of the runtime\nof the microservice applications we use in our evaluation (§7).\nTo reduce communication overhead and improve performance,\nwe architect Apiary to physically co-locate compute and data\nby wrapping a distributed DBMS and compiling functions\nto database stored procedures. Because stored procedures\nare transactional, this architecture also logically integrates\nfunction execution and data management; we leverage this\nintegration to efficiently provide transactional guarantees (§3),\nfault tolerance (§3, §4), and observability (§5).\nApiary’s architecture assumes developers write programs\nas workflows of functions; we discuss the programming in-\nterface and its semantics in §3. We sketch the architecture in\nFigure 4. It has three layers: the clients, frontend, and back-\nend.\nClients. Clients send requests through a client library to the\nfrontend to invoke workflows and functions, which execute in\nthe backend. Developers write functions and compose work-\nflows using our programming interface.\n2\n\nBackend\nAnalytical DBMSOLTP DBMSFrontendClientDispatcherRegistrarλλλλInvoke Func/DAGInvoke Func/DAGInvoke Func/DAGInvoke Func/DAGInvoke Func/DAGInvoke Func/DAGRegister Func/WorkflowInvoke Func/WorkflowObservability QueriesTracing Layer\nλ\nλ\nλStored ProceduresFigure 4: Architecture of Apiary.\nFrontend. Frontend servers route and authenticate client re-\nquests to the backend. Each server has a dispatcher, which\nmanages workflow execution by invoking each function in the\nbackend, passing in its inputs, collecting its outputs to send to\nlater functions, and enforcing fault-tolerance guarantees (§3,\n§4). Servers also contain registrars, which handle function\nand workflow registration, instrumentation, and compilation.\nBackend. The backend executes functions and manages data.\nIt wraps a distributed DBMS and its stored procedures. Apiary\nexecutes functions transactionally on DBMS servers, instru-\nmenting them to provide fault tolerance (§3, §4) and capture\ninformation on application-database interactions for observ-\nability (§5). Because functions are physically co-located with\nthe DBMS, we rely on the DBMS’s native elastic scaling\ncapabilities to scale the backend.\n2.2 Non-Goals\nWe want to emphasize two objectives that are excluded from\nthe scope of this paper.\nCompute-Heavy Workloads. Apiary’s design focuses\non short-lived data-centric applications, not long-running\ncompute-intensive workloads such as video processing [17]\nor batch analytics [37]. These do not require its features and\nguarantees, such as transactional functions and exactly-once\nsemantics. If users wish to execute compute-heavy tasks, we\nexpect them to leverage an external service, such as AWS\nRekognition [5] for text detection in images.\nNon-Relational Data Models. Apiary currently only sup-\nports a relational data model. We believe it is possible to ex-\ntend Apiary to support transactional non-relational databases,\nsuch as MongoDB, but this is beyond the scope of this paper.\nMost comparable data-centric FaaS platforms are also re-\nstricted to relational or key-value data, e.g. [22,39,42,47,48].\n3 Apiary Semantics\nApiary aims to make it easy for developers to write stateful\napplications which are performant, robust to failures, and\ncorrectly handle concurrent operations. To make this possible,\nwe provide a familiar FaaS programming interface (§3.1)\nsimilar to widely-used platforms like AWS Step Functions [7],\nbut offer much stronger guarantees, including transactional\nsemantics (§3.2) and fault tolerance (§3.3).\nWorkflow InterfacecreateWorkflow(List[Func], Spec)groupFunctions(List[Func])Function InterfaceexecUpdate(Query, List[Arg])execQuery(Query, List[Arg])→ResultreturnOutput(Name, Object)retrieveInput(Name)→ObjectCreate a workflow from functions and a spec mapping named inputs and outputs.Group multiple functions into one transaction.Execute a database update.Execute a database query, return its results.Return a named output.Retrieve a named input.Figure 5: The Apiary workflow and function interfaces.\n3.1 Programming Interface\nBefore discussing Apiary’s semantics, we sketch its program-\nming interface, using as an example a hotel reservation service\nthat checks if a room is available, books it, then sends a con-\nfirmation email. Developers write functions in a high-level\nlanguage (Java) using SQL to access data stored in a relational\nDBMS, then construct programs as workflows of functions.\nFunction Interface. Functions take in and return any num-\nber of named serializable objects. They can embed SQL\nqueries to access or modify data in the DBMS. We show\nthe function interface in Figure 5. In the hotel reservation ex-\nample, we implement checking availability, booking a room,\nand sending an email in separate functions; we show the code\nfor checking availability in Figure 6 (lines 1–10).\nApiary requires functions to follow three rules:\n1.All SQL queries in functions must be defined statically as\nparameterized prepared statements.\n2. Functions must be deterministic.\n3. External service or API calls must be idempotent.\nThe first rule enables static analysis (§4) and data tracing for\nobservability (§5); the last two rules enable practical imple-\nmentation of exactly-once semantics (§4).\nWorkflow Interface. Developers construct programs as\nworkflows of functions using the interface in Figure 5. Each\nworkflow is a directed acyclic graph (DAG) where nodes are\nfunctions, edges are data flow, and the input to a function\nis the output of its parents. Developers construct a work-\nflow from a list of functions and a specification mapping\noutputs of earlier functions to inputs of later functions. Re-\ncursive or cyclic dependencies are not allowed. Each work-\nflow has a single sink function that has no children; its\noutput is returned to the client. To guarantee the correct-\nness of complex workflows, Apiary lets developers desig-\nnate a group of functions in a workflow as a single transac-\ntion; we discuss this in detail in §3.2. We can implement\nthe hotel reservation service as a three-function workflow\n(checkAvail ⇒reserve ⇒sendEmail ), where we group\nthe first two functions into one transaction; we sketch this in\nFigure 6 (lines 12–15).\n3.2 Transactional Semantics\nFaaS programs often require transactional guarantees; for\nexample, our hotel reservation workflow needs transactions to\nguarantee that rooms are never double-booked. Thus, Apiary\n3\n\n1def checkAvail ():\n2 query = new SQL (\" SELECT numAvail FROM HotelAvail\nWHERE hotelID =? AND date =?\")\n3 inp = retrieveInput (\" availIn \")\n4 avail = true ;\n5 for (dt = inp . start ; dt < inp. end ; dt ++) :\n6 num = execQuery (query , inp . hotelID , dt)\n7 if ( num < inp . numRooms ):\n8 avail = false\n9 break\n10 returnOutput (\" availOut \", avail )\n11// Omit reserve and sendEmail due to space limit .\n12w = createWorkflow ([checkAvail ,reserve ,sendEmail ],\n13 {\"in\": \" availIn \", \" availOut \": \" reserveIn \",\n14 \" reserveOut \": \" emailIn \", \" emailOut \": \"out \"})\n15w.groupFunctions ([checkAvail ,reserve ])\nFigure 6: Pseudocode implementing the three-function hotel\nworkflow using the Apiary interface (highlighted in blue).\nlogically integrates function execution and data management:\neach function executes as a serializable ACID transaction in\nthe database. A key implication of this design is that functions\nare units of both control flow and atomicity; we leverage\nthis to implement fault tolerance (§4) and track application-\ndatabase interactions for observability (§5).\nAn important question is what transactional semantics we\nprovide for workflows. Naively, we could execute entire work-\nflows in a single transaction, but this leads to unnecessar-\nily large transactions with poor performance. Alternatively,\nwe could provide no transactional guarantees for workflows\nand allow functions from concurrent workflows to arbitrarily\ninterleave, but developers often require transactional guar-\nantees across multiple functions. For example, in the hotel\nworkflow, the first two functions ( checkAvail andreserve )\nmust execute in one transaction to ensure the room is actually\navailable when it is booked. To balance these tradeoffs, we\nprovide multi-function transactions : developers can group\nmultiple functions in a workflow to execute as a single ACID\ntransaction, provided they form a connected subgraph of the\nworkflow graph. This design gives developers the flexibil-\nity to transactionally execute related operations, but separate\nunrelated operations to avoid the performance overhead of\nexcessively large transactions.\n3.3 Fault-Tolerance Guarantees\nTransactions are not sufficient to guarantee robust workflow\nexecution in the presence of failures because they do not\nprevent issues such as workflows being executed partially\nor individual functions being executed multiple times. Thus,\nApiary provides two guarantees for robust workflow execu-\ntion. First, workflows run to completion , so even if the failure\nof a dispatcher or DBMS server causes workflow execution to\nhalt, the workflow is eventually resumed. Second, functions\nin workflows execute exactly once , so even if a workflow or\nany of its functions fails and is restarted multiple times, the\neffect of the workflow on application state (in the database) is\nthe same as if every function in the workflow were executed\nexactly once. For example, in the hotel workflow, Apiary guar-antees that a room is only booked once and that if it is booked\nsuccessfully, a confirmation email is always sent.\n3.4 Comparison with Related Systems\nIn Table 1, we compare the semantics of Apiary to those\nof related systems. Apiary provides substantially stronger\nguarantees than commercial systems such as AWS Step Func-\ntions [7] and Azure Durable Functions [31]. These support\nrun-to-completion workflows but neither provide transactional\nfunction guarantees nor have visibility into the application\ndatabase. As a result, they cannot provide exactly-once func-\ntion execution, instead providing the weaker at-least-once\nguarantee and allowing arbitrary function re-execution [6].\nAWS Step Functions claims it can also offer exactly-once\nsemantics, but this is actually an at-most-once guarantee: it\ndoes not retry on failure, but instead guarantees tasks never\nrun more than once [8].\nApiary provides similar guarantees to transactional FaaS\nsystems such as Beldi [47], Boki [22], and Transactional State-\nfun [15]. They all allow developers to provide ACID trans-\nactional guarantees for individual functions, though Transac-\ntional Statefun implements a limited “one-shot” model where\nthe outputs of earlier queries in a transaction cannot be used\nas inputs to later queries in the same transaction. All provide\nexactly-once semantics for functions and run-to-completion\nworkflows. None provide transactional guarantees for entire\nworkflows, but all support running multiple functions in a sin-\ngle transaction similar to Apiary multi-function transactions.\nIn Beldi and Boki, a transactional function can synchronously\ncall other functions so they all execute in one large transaction.\nIn Transactional Statefun, a “coordinator function” can coor-\ndinate multiple other functions through two-phase commit so\nthey execute as a single transaction. However, while these sys-\ntems build costly external transaction managers over remote\nstorage, Apiary instead minimizes transactional overhead by\nco-locating compute and data.\nAn important related class of systems is FaaS platforms\nbuilt around causal consistency, such as Cloudburst [42], Hy-\ndrocache [45], and FaaSTCC [27]. These store data in a re-\nmote key-value store and use local caches to improve perfor-\nmance. The strongest guarantee they provide is transactional\ncausal consistency (TCC) for entire workflows. TCC guaran-\ntees that workflows cannot see the effects of other workflows\nuntil they are entirely complete, a guarantee only provided\nby Apiary or other transactional FaaS systems if the entire\nworkflow is executed in a single multi-function transaction.\nHowever, for individual functions, TCC provides relatively\nweak guarantees, allowing serious anomalies such as stale\nreads and write-write conflicts. By contrast, Apiary runs each\nfunction as an ACID transaction with serializable isolation,\ndisallowing these anomalies. Moreover, these systems only\nprovide a key-value API for data management, while Apiary\nsupports a relational model.\n4\n\nPlatformTransactional\nFunctionsMulti-Func.\nTxns.Exactly-Once\nSemanticsRun-to-\nCompletionData\nLocality\nStep Functions [7] No No At-Least-Once Yes No\nDurable Functions [31] No No At-Least-Once Yes No\nCloudburst [42] CC CC No No Caching\nFaaSTCC [27] TCC TCC At-Least-Once Yes Caching\nHydrocache [45] TCC TCC At-Least-Once Yes Caching\nStateFun-Txns [15] Yes Yes Yes Yes No\nBeldi [47] Yes Yes Yes Yes No\nBoki [22] Yes Yes Yes Yes Caching\nApiary Yes Yes Yes Yes Co-location\nTable 1: Apiary provides similar or stronger guarantees than\ncomparable platforms while improving performance by co-\nlocating compute and data. CC means causal consistency; TCC\nmeans transactional causal consistency.\n4 Fault-Tolerant Workflows\nWe now describe how we leverage Apiary’s integration of\nfunctions and data to efficiently implement the workflow fault-\ntolerance guarantees defined in §3: run-to-completion work-\nflow execution and exactly-once function execution.\n4.1 Handling Machine Failures\nApiary must enforce its execution guarantees despite failures\nof frontend dispatchers or DBMS backend servers. Most dis-\ntributed DBMSs can recover from failures of their servers,\ntypically using replication and logging. We assume that if\nany single server fails, the DBMS can recover without loss of\navailability by failing over to a replica, so workflow execution\nis unaffected. If multiple servers fail, the DBMS can recover\nwithout data loss from durable logs, so dispatchers must wait\nuntil recovery is complete, then resume execution.\nIf a dispatcher fails during workflow execution, clients with\na pending workflow invocation time out, then resubmit their\ninvocation to resume the partially executed workflow on a new\ndispatcher. To uniquely identify workflows for resumption,\nclients generate a unique ID for each workflow invocation and\nprefix it with a database-generated unique client ID. The new\ndispatcher must resume workflow execution from where the\nfailed one left off, finishing the workflow without re-executing\nany functions that have already been completed. Because Api-\nary runs functions transactionally as stored procedures, we\ncan make this possible by instrumenting functions to transac-\ntionally record their outputs (serialized in a binary format) in\nthe DBMS before returning. Each recorded output is associ-\nated with a unique function invocation ID, derived from the\nworkflow ID, and is retained only for the lifetime of the work-\nflow. During retry, the dispatcher resumes a workflow from\nthe beginning and (re-)dispatches each function. Functions\nfirst check for a record from the earlier execution and, if they\nfind one, return it instead of executing.\nA limitation of this current implementation is that it relies\non clients to detect dispatcher failures. In future work, we\nplan to fix this by having dispatchers write ahead workflow\nmetadata to the DBMS and ping each other in a decentralized\nmanner to detect failures (using the DBMS for discovery).\nThen, a dispatcher which detects another’s failure could re-\nF1\nRead -OnlyF3\nRead + WriteF4\nRead -OnlyF2\nRead -OnlyNot Recorded\nRecorded\n(source) (sink)Figure 7: Example of Apiary’s selective recording algorithm\nSFR. F3is recorded as it performs a write. F1must also be\nrecorded to avoid inconsistent outputs to F2andF3.\ntrieve its pending workflows from the DBMS and complete\ntheir execution without client involvement.\n4.2 Optimizing Function Recording\nThe protocol described in the previous section records every\nfunction’s output in the database, but doing this naively incurs\noverhead of up to 2.2 ×(as we show in §7.7) because it re-\nquires performing additional database lookups and updates in\neach function. However, we can reduce this overhead to <5%\n(across all workloads we tested) by recognizing that some\nfunctions can be safely re-executed without violating exactly-\nonce semantics, so their outputs need not be recorded. For\nexample, if an entire workflow is read-only, it can be safely re-\nexecuted if its original execution failed, so we need not record\nany of its functions. Therefore, we develop a new algorithm,\ncalled selective function recording (SFR), to determine, using\nstatic analysis when a workflow is registered, which functions\nmust be recorded and which can be safely re-executed.\nWe must record any function performing a DBMS write to\nensure writes are not re-executed (re-executing external calls\nis fine as they must be idempotent). Moreover, we must record\na read-only function if there exist disjoint paths from it to mul-\ntiple different recorded functions, or to at least one recorded\nfunction and the sink. This guarantees that two recorded func-\ntions which depend on a value computed by an ancestor will\nalways observe the same value from that ancestor, even if\nworkflow execution is restarted. To determine whether dis-\njoint paths exist, we search for all recorded functions (or the\nsink) reachable without traversing another recorded function;\nif there are more than one of these, there are disjoint paths,\nand the function must be recorded.\nWe sketch SFR in Algorithm 1 and provide an example\nin Figure 7. F3is recorded for performing writes, but F1is\nalso recorded despite being read-only. Suppose F1was not\nrecorded and a dispatcher crashed after executing F1andF3.\nUpon re-execution, F1may return a different value than it\noriginally did because some data was changed by an unrelated\nfunction. If that happened, F2would return an output based\non the new output of F1, but F3would return its recorded\noutput based on the original output of F1. This causes an in-\nconsistency that violates exactly-once semantics, so we must\nrecord F1to prevent it.\nCorrectness. SFR identifies a set of recorded functions such\nthat if a workflow is resumed following the protocol described\nin the previous section (re-executing non-recorded functions,\nreturning recorded outputs of recorded functions), the effect of\n5\n\nAlgorithm 1 SFR: Selective Function Recording\n1:function SFR( W) ▷Input W: the workflow graph.\n2: {f1,...,fn} = topoSort( W) ▷f1is source, fnis sink.\n3: Recorded ={}\n4: forfi∈ {fn...f1}do ▷Traverse from sink back to source.\n5: ifhasWrite( fi)then\n6: Recorded .add( fi)\n7: else\n▷BFS search all recorded functions (or the sink)\n▷reachable without traversing a recorded function.\n8: RF=BFSFindReachable (fi,Recorded ∪{fn})\n9: ifRF.size() > 1 then\n10: Recorded .add( fi)\n11: return Recorded\nthe workflow on application state is the same as if every func-\ntion in the workflow were executed exactly once. Recording\nextra functions cannot provide stronger guarantees because\nin the absence of disjoint paths to different recorded func-\ntions, a non-recorded function must have a single recorded\nfunction descendant that is the ancestor of all other recorded\nfunction (or sink) descendants and can provide a consistent\noutput during failure recovery. We do not guarantee we find\nthe minimal set of recorded functions as semantic information\nabout functions may obviate the need to record a function (for\nexample, if we knew a function returned a constant to another\nfunction, we could ignore that edge for this algorithm), but\nthis is outside the scope of this paper.\nComplexity. Because SFR traverses a workflow from the sink\nback to the source, we can memoize workflow graph search,\nso we only need to traverse each workflow graph edge once.\nTherefore, the time complexity of SFR is O(V+E)where\nVis the number of functions, and Eis the number of edges\nin the workflow graph. We only run this algorithm once per\nworkflow when the workflow is registered.\n4.3 Handling Function Failures\nApiary must additionally enforce its execution guarantees de-\nspite failures or errors in individual functions. If a function\nfails due to recoverable or transient errors (e.g., the failure of a\nDB server that can fail over to a replica), the dispatcher retries\nit. However, if an error is unrecoverable (e.g., a constraint\nviolation or function runtime exception), there is no choice\nbut for the DBMS to abort and roll back its containing trans-\naction. From there, Apiary continues workflow execution, but\npropagates a failure notification to any downstream functions\nwhich had as inputs the output of the failed function. This\nenforces run-to-completion workflow execution while giving\ndevelopers control over how their workflows handle failures.\n5 Observability\nDevelopers often require information on how applications\ninteract with data for debugging, monitoring, and auditing use\ncases, for example to verify a program did not improperly\naccess private data. In existing FaaS platforms, this informa-tion is fundamentally difficult to collect because platforms\nlack visibility into how functions manage data, so developers\nmust perform extensive manual logging across many short-\nlived functions. In this section, we discuss how we leverage\nApiary’s tight integration with the database to build a trac-\ning layer that automatically records these interactions with\nminimal cost.\n5.1 Observability Interface\nApiary instruments workflows to trace the history of workflow\nand function executions, instruments queries to log database\noperations, then combines this information to create a com-\nplete record of application interactions with data. Specifically,\nApiary records for each data item all function executions that\naccessed or modified it. The tracing layer automatically spools\nthis information to an analytical database (in our implementa-\ntion, Vertica [43]) for long-term storage and analysis. We use\na separate analytical database because it is better optimized\nfor large observability queries than a transactional DBMS.\nStorage policies such as data retention rules are implemented\nby this database. Within the analytical database, information\nis organized into tables. For captured workflow information,\nApiary creates a function invocations table per application:\nFunctionInvocations (func_id, timestamp,\nfunction_name, workflow_name, workflow_id)\nfunc_id , the primary key, is a unique ID per function invoca-\ntion. workflow_id is a unique ID per workflow invocation.\nBoth are defined in §4.1.\nFor each table used by an application, Apiary creates an\nevent table for captured operations on that table:\nTableEvents (func_id, timestamp, event_type,\nquery, [record_data...])\nevent_type can be insert, delete, update , orread ;\nquery is the query string; func_id is a foreign key referenc-\ningFunctionInvocations .\nThe information stored in these tables enables efficient exe-\ncution of useful observability queries for debugging, monitor-\ning, and auditing FaaS applications. For example, developers\ncould query if a function improperly accessed private data and\nwhat it did with the data. We evaluate case studies in §7.8.\n5.2 Implementing Tracing Layer\nWe leverage Apiary’s tight integration with data to adapt\ndatabase techniques like change data capture and query\nrewrites [4, 20] to a FaaS setting, building a tracing layer\nto capture application interactions with data efficiently.\nWhen a function executes, the tracing layer adds an entry\ntoFunctionInvocations . When a function performs a\ndatabase operation, it automatically records metadata such as\nthe function ID to TableEvents . For write operations, it also\nrecords updated data. For read operations, the tracing layer\nmodifies the query to return the primary keys of all retrieved\nrows (in addition to the requested information), then records\n6\n\nthem to identify each accessed record. To minimize overhead,\nit only captures rows that are actually retrieved, not rows that\nare accessed incidentally (e.g., by an aggregation that may\naccess thousands of rows). The tracing layer also logs the\nqueries themselves so this additional read information may\nbe reconstructed later if it is needed for an investigation.\nThe challenge in capturing information on database opera-\ntions is performing it efficiently while providing guarantees\nabout what information is captured. To capture writes, the\ntracing layer relies on DBMS change data capture to transac-\ntionally export information. However, read capture is more\ndifficult, both because existing DBMSs do not have built-in\nread capture capability and because reads are numerous so\noverhead may be higher. Reads are captured by instrumenting\ntheexecQuery function (Figure 5). The tracing layer main-\ntains a circular buffer inside each DBMS server’s memory.\nWhenever a read occurs in execQuery , the tracing layer ap-\npends its information to this buffer. Periodically, it flushes the\nbuffer to the remote analytical database. Tracing is robust to\nfunction failures and re-executions because re-executions are\nrecognized and deduplicated using their shared IDs. However,\ncaptured read information may be lost if the database server\ncrashes while information is in the buffer; if developers can-\nnot tolerate data loss, we can optionally place the buffer on\ndisk to eliminate loss at some performance cost.\n6 Implementation\nWe implement Apiary’s tightly integrated architecture (§2)\nby wrapping a distributed DBMS and compiling functions to\nDBMS stored procedures.\n6.1 Choosing a DBMS\nApiary requires a distributed DBMS with four properties:\n• Supports ACID transactions.\n•Supports running user code in a non-SQL language trans-\nactionally in stored procedures.\n• Supports change data capture (for observability, §5.2).\n• Supports elastic DBMS cluster resizing.\nWhile many DBMSs have these properties (e.g., Single-\nStore [40], Yugabyte [46]), we chose V oltDB as it could most\nefficiently execute our target workloads. Most distributed\nDBMSs, including V oltDB, scale by partitioning data. We\nobserve that in our target workloads, almost all transactions\nare single-sited [24] and access data in only a single partition.\nV oltDB executes these transactions efficiently, running them\nto completion in memory without needing locks. However, a\nlimitation of V oltDB is that it is less efficient at multi-sited\ntransactions; a transaction must hold a global lock to access\ndata on multiple partitions. There has been recent research on\naddressing this [49], but we leave the efficient implementation\nof multi-sited transactions to future work.6.2 Compilation\nWhen developers register functions and workflows in Apiary,\nit compiles each function to a stored procedure, a routine in a\nnon-SQL language that runs natively as a DBMS transaction.\nIn our implementation, functions provide the same guarantees\nas V oltDB transactions: they are ACID and serializable. To\nimplement multi-function transactions, Apiary compiles all\ninvolved functions to a single stored procedure. Compilation\nhappens in two steps. First, Apiary instruments each function\nto capture application-database interactions for observability\n(§5) and to record its execution for exactly-once semantics\n(§4). Then, Apiary compiles the instrumented function (or\nmulti-function transaction) into a stored procedure and regis-\nters it in the DBMS.\nApiary extends the DBMS stored procedure interface, so it\ncan compile any function that uses its programming interface\n(Figure 5) and follows the rules outlined in §3.1. Additionally,\nin our V oltDB-based implementation, because V oltDB can\nefficiently execute single-sited transactions, we let developers\nspecify if a function (or multi-function transaction) is single-\nsited and, if so, which function input specifies the site.\n7 Evaluation\nWe evaluate Apiary with widely-used microservice and web\nserving workloads as well as microbenchmarks, showing that:\n1.By physically co-locating compute and data, Apiary out-\nperforms production FaaS systems by 7–68 ×and research\nsystems by 2–27 ×(Figures 8, 11).\n2.By selectively instrumenting functions using the SFR al-\ngorithm, Apiary provides fault tolerance with overhead of\n<5% compared to 2.2 ×for a naive solution (Figure 12).\n3.By instrumenting database operations and functions, Api-\nary captures information on application-database interac-\ntions critical to observability with overhead of <15% as\ncompared to 92% with manual logging (Figure 13).\n7.1 Experimental Setup\nWe implement Apiary in ~10K lines of Java code, which we\nwill open source upon paper acceptance. We use V oltDB [44]\nv9.3.2 as our DBMS backend and Vertica [43] v10.1.1 for an-\nalytics data. For communication between clients and frontend\nservers, we use JeroMQ [36] v0.5.2 over TCP.\nIn all experiments where not otherwise noted, we run on\nGoogle Cloud using c2-standard-8 VM instances with 8\nvCPUs and 32GB DRAM. We use as a DBMS backend a\ncluster of 40 V oltDB servers with 8 V oltDB partitions per VM.\nFor high availability, we replicate each partition once, as is\ncommon in production. For fairness, we use the same V oltDB\ncluster as the storage backend for our baselines. To ensure\nwe can fully saturate this DBMS backend, we run 45 Apiary\nfrontend VMs and generate requests using 15 remote client\nVMs. Each client VM runs on a c2-standard-60 instance\nwith 60 vCPUs and 240GB DRAM. We spool observability\n7\n\ndata to a cluster of 10 Vertica servers running on a separate\nset of VMs from the V oltDB cluster. All experiments run for\n300 seconds after a 5-second warmup.\n7.2 Baselines\nWe compare Apiary to four baselines, ranging from produc-\ntion platforms to the latest research systems.\nOpenWhisk. OpenWhisk (OW) [34] is a popular open-source\nproduction FaaS platform. We implement each of our work-\nloads in the OW Java runtime, performing all business logic in\nan OW function but storing and querying data in an external\nV oltDB cluster. We coordinated with OW developers to tune\nour OW setup. Since OW cannot efficiently run workflows, we\nimplement each workload in a single OW function, eliminat-\ning communication between functions. Additionally, we pre-\nwarm OW function containers and only measure warm-start\nperformance. In our experiments, we use 45 c2-standard-8\nVMs as OW workers. To maximize OW performance, we use\n5 controllers, each on a c2-standard-60 instance that man-\nages 9 workers, load balancing between sub-clusters.\nRPC Servers. Most microservices today are deployed in long-\nrunning RPC servers with separate application and DBMS\nserver machines [18, 26]. We implement each of our work-\nloads this way, running all business logic in RPC servers\nbut storing data in an external V oltDB cluster and access-\ning it using V oltDB stored procedures. For fairness, we use\nthe same communication library as Apiary (JeroMQ, chosen\nbecause we found it outperforms alternatives like gRPC), re-\nimplement each microservice in Java following its original\narchitecture, and use long-lived connections with the DBMS.\nIn our experiments, we use a setup identical to Apiary but\nwith all frontend servers replaced with RPC servers.\nBoki. Boki [22] is a recent research system supporting trans-\nactional FaaS. We use Boki as a baseline because it is repre-\nsentative of a class of transactional FaaS systems (discussed\nin §3.4), but is additionally co-designed with in-memory local\ncaches for high performance. We use the experimental setup\ndescribed in the Boki paper, deploying 8 storage nodes, 3 se-\nquences, and 8 workers, each on an AWS EC2 c5d.2xlarge\ninstance with 8 vCPUs and 16GB DRAM. We coordinated\nwith the Boki authors to tune our setup.\nCloudburst. Cloudburst [42] is a recent research system for\nstateful FaaS that provides causal consistency. We use Cloud-\nburst as a baseline because it is an influential system that\nrepresents a class of FaaS platforms which are built around\ncausal consistency and improve performance with in-memory\nlocal caches (discussed in §3.4). To maximize Cloudburst\nperformance and ensure consistency of experimental results,\nwe disabled its autoscaler and manually pinned function ex-\necutors to every available worker thread. We otherwise run\nCloudburst unmodified in its most-performant last-writer wins\nmode. Similar to the Boki setup and following recommenda-\ntions from the Cloudburst authors, we deploy 1 Anna KVSWorkload Operation RatioRead-\nOnly ?Access\nRowsRPCs for\nµServices# of\nTxns.# of SQL\nQueries\nBrowsing 80% Yes 8 2 1 1\nCartUpdate 10% No 1 2 1 2 Shop\nCheckout 10% No 5 6 3 5\nSearch 60% Yes 30 4 6 22\nRecommend 39% Yes 1 2 1 1 Hotel\nReservation 1% No 5 2 2 5\nGetTimeline 90% Yes 550 3 51 51RetwisPost 10% No 1 2 1 1\nTable 2: Microservice benchmark information. RPCs are for\nthe RPC Servers baseline; Apiary and OW only require one\nclient-server RPC. Apiary only requires one DB round trip per\ntransaction, but the baselines require one per SQL query.\n0 500K 1.0M 1.5M110100Latency [ms; log10]\na) Shop p50\n0 50K 100K 150K\nb) Hotel p50\n0 10K 20K\nc) Retwis p50\n0 500K 1.0M 1.5M101001000Latency [ms; log10]\nd) Shop p99\n0 50K 100K 150K\nThroughput (queries/sec)\ne) Hotel p99\n0 10K 20K\nf) Retwis p99OpenWhisk RPC Servers Apiary\nFigure 8: Throughput versus latency for Apiary and the Open-\nWhisk (OW) and RPC Servers baselines on all benchmarks.\nnode, 4 scheduler nodes, and 8 worker nodes, each on an AWS\nEC2 c5.2xlarge instance with 8 vCPUs and 16GB DRAM.\nFor fairness, when comparing Apiary to Boki and Cloud-\nburst, we use 8 V oltDB servers and 8 frontend servers.\n7.3 Microservice Workloads\nWe evaluate Apiary using three microservice benchmarks,\neach commonly used in previous microservices and FaaS\npapers. As shown in Table 2, these workloads cover a large\ndesign space for data-centric FaaS applications.\nShop. This benchmark, adapted from a Google Cloud\ndemo [19], simulates a service where users browse an on-\nline store, update their shopping cart, and check out items.\nHotel. This benchmark, from DeathStarBench [18], simulates\nsearching and reserving hotel rooms. Our implementation con-\ntains a multi-function transaction similar to Figure 6, where\nvalidation and reservation are performed transactionally.\nRetwis. This benchmark, from Redis [38], simulates a Twitter-\nlike social network, where users follow other users, make\nposts, and read a “timeline” of the most recent posts of all\nusers they follow. We use the same Retwis parameters as\nCloudburst [42]: we create 1000 users, each following 50\nother users, and pre-load 5000 posts.\n8\n\n7.4 End-to-End Benchmarks\nWe first compare Apiary performance with the OW and RPC\nServers baselines on our three microservice workloads, show-\ning results in Figure 8. For each benchmark, we vary offered\nload (sent asynchronously following a uniform distribution)\nand observe throughput and latency. For all three workloads,\nmaximum throughput achieved by Apiary is greater for Shop\n(1.2M RPS) than Hotel (144K RPS) and for Hotel than Retwis\n(20K RPS). This is because most Shop operations access a\nsingle customer’s cart, while most Hotel operations look up\ndata for several hotels and most Retwis operations access data\nfor several dozen users (“Access Rows” in Table 2).\nWe find that Apiary significantly outperforms the RPC\nServers baseline on two benchmarks and performs on par\non the third – even though Apiary offers more features (like\nobservability information capture) and stronger guarantees\n(like ACID functions and fault-tolerant workflows). Apiary\noutperforms the RPC Servers baseline due to reduced com-\nmunication overhead: because it compiles services to stored\nprocedures that run in the database server, it requires fewer\nround trips to perform database operations (Table 2). Apiary\nachieves 1.6–3.4 ×better median and tail latency than RPC\nservers on Shop and Hotel, where each transaction executes\nmany database queries which each require an RTT in the\nbaseline but not in Apiary. It matches the baseline latency for\nRetwis, where each transaction executes only a single query.\nApiary and RPC servers achieve similar maximum throughput\nfor Hotel and Retwis, where throughput is bottlenecked by\nthe database, but Apiary improves throughput by 1.75 ×for\nShop, where the bottleneck is communication.\nApiary dramatically outperforms OW on all three bench-\nmarks. Due to a combination of scheduling, container ini-\ntialization, message passing, and communication overhead\n(analyzed in §7.5), Apiary improves throughput by 7–68 ×\nand median and tail latency by 5–14 ×compared to OW.\nThese results establish that, for our target applications, sepa-\nrating function execution from data management is inefficient.\nA conventional FaaS platform not only requires the same num-\nber of storage servers as Apiary to host and manage data, but\nalso needs compute workers to run application logic. However,\nbecause application logic is computationally bottlenecked by\ndatabase operations (as shown in Figure 3), these compute\nworkers contribute little but add significant communication\noverhead. Thus, Apiary’s architecture reduces communica-\ntion overhead, uses resources more efficiently, and, as we will\nshow in Section 7.9, reduces the cost of deployment.\nScalability. We also evaluate the scalability of Apiary, measur-\ning the maximum throughput Apiary can achieve with varying\nnumbers of database servers. We show results for the Hotel\nbenchmark in Figure 9, but obtained similar results for Shop\nand Retwis. We measure from 2 to 40 database servers (16 to\n320 data partitions), beginning with 2 servers because each\nserver needs a replica. We find that Apiary scales well; with\n0 5 10 15 20 25 30 35 40\nNumber of Database Servers0100K200KThroughput (RPS)\nApiary Throughput Ideal ScalingFigure 9: Maximum throughput for Apiary on Hotel with a\nvarying number of database servers. We extrapolate “ideal scal-\ning” linearly from a single replicated server (two servers total).\nEnd-to-end OpenWhiskinvocation (6.1ms)Controller (2.6 ms)Kafka(0.5 ms)Invoker Execution(2.5 ms)Kafka (0.5 ms)Prepare Container(1.4 ms)Execute Func.(1.1 ms)Access Metadata(1 ms)Schedule Func.(1.6 ms)\nFigure 10: Latency breakdown for an OpenWhisk function\ninvocation performing a point database update.\nlarger numbers of servers, performance was mainly limited\nby V oltDB’s overhead of managing a large network mesh.\n7.5 OpenWhisk Performance Analysis\nTo further investigate the performance difference between\nApiary and production FaaS systems like OW, we analyze OW\nperformance on a microbenchmark of a single OW function\nthat retrieves and increments a counter stored in V oltDB. We\ninvoke this function 100K times and measure the average\nlatency of each step.\nAs we show in Figure 10, OW adds significant overhead\nto a function invocation. Each invocation is first handled by\na controller which performs bookkeeping operations using\nfunction metadata stored in CouchDB (1 ms) before schedul-\ning the invocation to an invoker/worker node (1.6 ms). OW\nuses Apache Kafka for controller-invoker communication, in-\ncurring 1 ms of round-trip latency. Once the invoker receives\na request, it resumes the execution of an already-warm con-\ntainer (1.4 ms). The function then executes in 1.1 ms. We\nemphasize that this high overhead is not unique to OW; other\npopular production FaaS systems have similar architecture\nand performance characteristics. Apiary avoids this overhead\nbecause it integrates function execution and data management\nand stores all state in the backend DBMS, reducing commu-\nnication overhead and avoiding external state management.\n7.6 Comparing with Boki and Cloudburst\nWe next compare Apiary performance with Boki and Cloud-\nburst. We use the Retwis benchmark because both Cloudburst\nand Boki use it in their evaluation and provide open-source\nimplementations of it, so we can be sure our comparison is\nfair. Retwis is read-heavy, so to evaluate the performance im-\npact of writes, we use a microbenchmark which retrieves and\nincrements a counter associated with a key. We use 80K coun-\nters to minimize aborts due to write-write conflicts in Boki\nwhile still ensuring all counters fit into in-memory cache.\nLooking first at Boki, we find that Apiary improves through-\n9\n\n0 2000 4000 6000110100Latency [ms; log10]\na) Retwis p50\n0 100K 200K 300K\nb) Microbench p50\n0 2000 4000 6000\nThroughput (queries/sec)101000Latency [ms; log10]\nc) Retwis p99\n0 100K 200K 300K\nThroughput (queries/sec)\nd) Microbench p99Boki Cloudburst ApiaryFigure 11: Throughput versus latency for Apiary, Boki, and\nCloudburst on Retwis and on a microbenchmark.\nput by 2.1 ×on Retwis and 10.1 ×on the microbenchmark.\nWe see similar improvements for median and tail latencies.\nUnlike Apiary, Boki must frequently perform non-local reads\nin the presence of writes to update its caches and enforce its\nsnapshot isolation guarantee. Thus, we expect Boki to per-\nform relatively better on read-heavy Retwis and relatively\nworse on the write-heavy microbenchmark, where most reads\nare non-local. Our experiments confirm this hypothesis.\nLooking next at Cloudburst, we find that Apiary improves\nthroughput by 5.2 ×on Retwis and 27.7 ×on the microbench-\nmark, with similar trends for latencies. The performance dif-\nference is surprising because both Apiary and Cloudburst\nperform all data access and updates locally, using stored pro-\ncedures in Apiary and asynchronously-synchronized local\ncaches in Cloudburst (though Apiary provides stronger guar-\nantees than Cloudburst: ACID transactions versus causal con-\nsistency). Digging deeper, we find the performance difference\ncomes largely from the more efficient implementation of Api-\nary: a read from a local cache in Cloudburst takes 300 µs\n(this is high because Cloudburst is implemented in Python) as\ncompared to <20 µs for a V oltDB read in Apiary (this adds up\nbecause Retwis contains several reads and Cloudburst does\nnot batch them) and additionally Cloudburst incurs 2.2 ms of\nexecutor and scheduler overhead for each function execution\nas compared to 300 µs in Apiary.\n7.7 Fault-Tolerant Workflows Performance Analysis\nWe now analyze the performance impact of Apiary’s work-\nflow fault-tolerance guarantees. Apiary uses SFR (§4.1) to\nselectively record function outputs in the DBMS to avoid\nre-executing them when resuming a failed workflow. We eval-\nuate the overhead of our guarantee and compare it to a more\nnaive implementation (similar to prior work [47]) that records\nall function executions, showing results in Figure 12. We find\nour guarantee incurs overhead of <5%, but this low overhead\nis only possible because SFR lets us record selectively: only\n25% of Shop, 0.25% of Hotel, and 0.2% of Retwis transaction\nexecutions must be recorded. By contrast, the naive imple-\nmentation reduces throughput by 1.3–2.2 ×.\n0500K1.0M1.5MThroughput (RPS)1.22M 1.18M\n824Ka) Shop\n050K100K150K144K 144K\n65Kb) Hotel\n010K20K20K 20K\n15Kc) RetwisNo Guarantees Apiary NaiveFigure 12: Maximum achievable throughput for Apiary without\nworkflow fault-tolerance guarantees,with the guarantees, and\nwith a naive implementation of the guarantees.\n0 500K 1.0M 1.5M110100Latency [ms; log10]\na) Shop p50\n0 50K 100K 150K\nb) Hotel p50\n0 5K 10K 15K 20K\nc) Retwis p50\n0 500K 1.0M 1.5M101001000Latency [ms; log10]\nd) Shop p99\n0 50K 100K 150K\nThroughput (queries/sec)\ne) Hotel p99\n0 5K 10K 15K 20K\nf) Retwis p99No Capture Apiary Manual Logging\nFigure 13: Throughput versus latency for Apiary, no observabil-\nity capture, and a manual logging baseline on all benchmarks.\n7.8 Enhancing Observability with Apiary\nData Tracing Performance Analysis. To analyze the per-\nformance impact of Apiary’s observability tracing (§5), we\nmeasure Apiary performance with and without capture. We\nalso measure the performance of a “manual logging” baseline\nthat represents how observability information is captured by\napplication developers utilizing existing FaaS platforms: by\nmanually logging to local disk files that are later exported by\nmonitoring software like AWS Cloudwatch.\nWe show results for all workloads in Figure 13. At low\nload, both Apiary and manual logging slightly increase latency\n(both median and tail) by up to 10%. At high load, Apiary adds\nthroughput overhead of up to 15% while manual logging adds\noverhead of up to 92%. Apiary data tracing overhead is low\nbecause we minimize the cost on the critical path, buffering\ncaptured observability data in the database’s memory and\nasynchronously exporting it in large batches.\nCase Studies. We next evaluate the value and practicality\nof Apiary’s data tracing. We execute 150M Shop operations,\ngenerating 1.2B rows of traced data, and export this data to a\nsingle Vertica server, finding it compresses to just 12.4GB of\ndisk space. We then use this dataset to show how Apiary can\nhandle queries from tasks of interest to our industrial partners.\nDebugging. Our first query is “What was the state of some\nrecord X when it was read by this particular function exe-\ncution?” This query might be used to determine what input\ncaused a function abort. Apiary can answer this query be-\n10\n\ncause it records in TableEvents all changes to data, so we\ncan retrieve the last update to record X before the problematic\nfunction execution began. For instance, we can use a single\nSQL query to find the state of record X at a time TS:\nSELECT reco_data FROM TableEvents\nWHERE event_type IN (’insert’, ’update’) AND rec_id=X\nAND timestamp <= TS ORDER BY timestamp DESC LIMIT 1;\nWe execute this query on the largest event table in our Shop\ndataset (840M rows and 7GB storage) to find the exact state of\na record when it was retrieved by a particular Shop execution;\nthe average query latency across five runs is 4.3 seconds.\nDownstream Provenance. Our second query is “Find all\nrecords updated by a workflow that earlier read record X.”\nThis query is useful for taint tracking, for example if record\nX contains misplaced sensitive information. We can answer\nthis query by scanning for functions that read record X, then\nreturning the write sets of later functions in their workflows:\nSELECT DISTINCT(record_id)\nFROM TableEvents AS T, FunctionInvocations AS F\nON T.func_id = F.func_id\nWHERE T.event_type IN (’insert’, ’update’)\nAND F.function_name in SUCCESSOR_FUNC_NAMES\nAND F.workflow_id in WORKFLOW_IDS;\nWe execute this query on our Shop dataset to find all orders\nmade by users who earlier browsed a potentially problematic\nitem; the average query latency is 4.8s across five runs.\n7.9 Cost Analysis\nFinally, we evaluate the cost of deploying Apiary to the cloud.\nIn Table 3, we estimate the monthly total cost of serving the\nShop workload (Hotel and Retwis trend similarly) on GCP\nfor Apiary, OW, and a commercial FaaS platform (Google\nCloud Functions) using a serverless database (Firestore). We\nevaluate four different load patterns: low, medium, and high\npatterns with 10 QPS, 1K QPS, and 100K QPS, plus a mixed\npattern of 50% low load, 49% medium load, and 1% high\nload. For all systems except GCF+Firestore (Firestore is pay-\nper-request and we exclude its storage cost), we provision\nthe database cluster based on peak load and conservatively\nassume no DBMS scaling. We assume OW can scale its\nworkers and controllers to minimize cost at a given load and\nApiary can scale its frontend servers.\nWe find that Apiary minimizes cost at scale compared to\nother systems. At low load, GCF+Firestore is the cheapest\nbecause it can scale to near-zero. However, at medium and\nhigh load OW is 4.8–25.2 ×costlier and GCF+Firestore is 2.9–\n44×costlier because their high overhead means they require\nmore resources to support the same load. Even for mixed load,\nOW is 2 ×costlier and GCF+Firestore is 1.2 ×costlier because\nthis overhead outweighs any benefit derived from separating\nfunction execution and data management. Therefore, Apiary\nnot only is faster and provides more features, but is also more\ncost-efficient than comparable systems.\n1Prices were retrieved from the Google Cloud Pricing Calculator onSystemLow Load\n10 QPSMid Load\n1K QPSHigh Load\n100K QPSMixed\nLoad\nOW + V oltDB $1,221 $4,422 $153,956 $6,732\nGCF + Firestore $22 $2,679 $268,380 $4,008\nApiary + V oltDB $917 $917 $6,099 $3,383\nTable 3: Estimated monthly cost for OW, GCF with Firestore,\nand Apiary serving the Shop workload on GCP1, varying loads.\n8 Related Work\nData-Centric FaaS Platforms. Many recent research sys-\ntems seek to improve FaaS performance and functionality for\ndata-centric applications. We have already discussed several\nFaaS systems which provide transactional guarantees in §3.\nSimilar systems include AFT [41], which interposes between\na FaaS platform and a remote data store to enforce read atom-\nicity, and Netherite [10], which uses a reliable message queue\nabstraction to provide exactly-once semantics for serverless\nworkflows, though unlike Apiary both systems physically sep-\narate functions from data. Also related are FaaSM [39], which\nallows functions to share memory regions, Shredder [48],\nwhich provides low-latency storage functions by physically\nco-locating with a key-value store, and LambdaObjects [29]\nwhich co-locates FaaS storage and compute, though unlike\nApiary none of these supports transactions. Orleans [9, 11]\nvirtual actors resemble stateful functions, though their data\nmodel is based on local objects instead of a database and, un-\nlike Apiary, Orleans does not provide exactly-once semantics.\nAnother set of relevant systems includes Pocket [25], Lo-\ncus [37], and Sonic [28], which propose multi-tier cloud stor-\nage backends designed for FaaS applications. These systems\nare largely designed for compute-intensive tasks on large\namounts of data (e.g., batch analytics) and trade off trans-\nactions and low latency for data storage cost. However, this\ntradeoff is not suitable for our target applications where each\nrequest accesses smaller amounts of data but demand the low\nlatency and strong transactional guarantees of Apiary.\nData Tracing for Observability. Apiary’s tracing layer is\nrelated to prior research on workflow provenance [21] and\ndata provenance [12]; for example, it uses query rewrites [4,\n20] to capture data accesses. Workflow provenance traces the\nflow of data through different modules (e.g., functions) in a\nlarger program, but assumes each is stateless. Data provenance\ntraces the origin of individual data items, which models the\nstate operations of a program, though tracing fine-grained\ndata provenance is out of scope for Apiary.\nThe key challenge in tracing application interactions with\ndata is capturing control flow information and linking it to\ndata operations. Most existing systems rely on manual an-\nnotation, but some have proposed automatically capturing\nprovenance information through kernel interposition [32] or\ndynamic analysis [33], though this information by itself is of-\n2022-12-28, using the us-west1 region as it was the cheapest: https://\ncloudpricingcalculator.appspot.com/ .\n11\n\nten too low-level for users [16,35] so it must be supplemented\nwith information from manual annotations [3, 16, 30, 32, 35].\nOther systems have proposed automatically combining work-\nflow and data provenance for scientific and analytics appli-\ncations [2, 13], but tolerate high latencies. Apiary interposes\nbetween functions and the DBMS and leverages control flow\ninformation inherent in its FaaS programming model to au-\ntomatically capture both workflow provenance and data op-\nerations without manual annotations. While prior systems\nprovide information flow control-based security for FaaS [1],\nand secure container-based FaaS applications by tracing sys-\ntem calls and network activity (and using these to infer data\nmovement) [14], we do not know of any prior work which\ncan produce a similarly complete record of application inter-\nactions with data in a FaaS environment.\n9 Conclusion\nWe presented Apiary, a novel transactional FaaS framework\nfor data-centric applications. Apiary physically co-locates and\nlogically integrates function execution and data management\nby wrapping a distributed DBMS and its stored procedures.\nIt guarantees functions run as ACID transactions, provides\nmulti-function transactions and fault-tolerant workflows, and\noffers advanced observability capabilities. In addition to pro-\nviding more features and stronger guarantees than existing\nFaaS platforms, Apiary outperforms them by 2–68 ×on mi-\ncroservice workloads by reducing communication overhead.\nReferences\n[1]Kalev Alpernas, Cormac Flanagan, Sadjad Fouladi,\nLeonid Ryzhyk, Mooly Sagiv, Thomas Schmitz, and\nKeith Winstein. Secure serverless computing using dy-\nnamic information flow control. Proceedings of the\nACM on Programming Languages , 2(OOPSLA):1–26,\n2018.\n[2]Yael Amsterdamer, Susan B. Davidson, Daniel Deutch,\nTova Milo, Julia Stoyanovich, and Val Tannen. Putting\nLipstick on Pig: Enabling Database-Style Workflow\nProvenance. Proc. VLDB Endow. , 5(4):346–357, dec\n2011.\n[3]Elaine Angelino, Daniel Yamins, and Margo Seltzer.\nStarflow: A script-centric data analysis environment.\nInInternational Provenance and Annotation Workshop ,\npages 236–250. Springer, 2010.\n[4]Bahareh Arab, Dieter Gawlick, Venkatesh Radhakrish-\nnan, Hao Guo, and Boris Glavic. A Generic Provenance\nMiddleware for Queries, Updates, and Transactions. In\n6th USENIX Workshop on the Theory and Practice of\nProvenance (TaPP 2014) , 2014.\n[5]AWS. AWS Rekognition, 2021. https://aws.\namazon.com/rekognition/ .[6]AWS. How do I make my Lambda function\nidempotent?, 2021. https://aws.amazon.\ncom/premiumsupport/knowledge-center/\nlambda-function-idempotent/ .\n[7]AWS. AWS Step Functions, 2022. https://aws.\namazon.com/step-functions/ .\n[8]AWS. Standard vs. Express Workflows, 2022. https:\n//docs.aws.amazon.com/step-functions/\nlatest/dg/concepts-standard-vs-express.\nhtml .\n[9]Phil Bernstein, Sergey Bykov, Alan Geller, Gabriel Kliot,\nand Jorgen Thelin. Orleans: Distributed virtual actors\nfor programmability and scalability. Technical Report\nMSR-TR-2014-41, March 2014.\n[10] Sebastian Burckhardt, Badrish Chandramouli, Chris\nGillum, David Justo, Konstantinos Kallas, Connor\nMcMahon, Christopher S. Meiklejohn, and Xiangfeng\nZhu. Netherite: Efficient execution of serverless work-\nflows. Proc. VLDB Endow. , 15(8):1591–1604, apr 2022.\n[11] Sergey Bykov, Alan Geller, Gabriel Kliot, James R.\nLarus, Ravi Pandya, and Jorgen Thelin. Orleans: Cloud\ncomputing for everyone. In Proceedings of the 2nd ACM\nSymposium on Cloud Computing , SOCC ’11, New York,\nNY , USA, 2011. Association for Computing Machinery.\n[12] James Cheney, Laura Chiticariu, and Wang-Chiew Tan.\nProvenance in databases: Why, how, and where. Foun-\ndations and Trends ®in Databases , 1(4):379–474, 2009.\n[13] Fernando Chirigati and Juliana Freire. Towards Inte-\ngrating Workflow and Database Provenance. In Inter-\nnational Provenance and Annotation Workshop , pages\n11–23. Springer, 2012.\n[14] Pubali Datta, Isaac Polinsky, Muhammad Adil Inam,\nAdam Bates, and William Enck. ALASTOR: Recon-\nstructing the provenance of serverless intrusions. In\n31st USENIX Security Symposium (USENIX Security\n22), Boston, MA, August 2022. USENIX Association.\n[15] Martijn de Heus, Kyriakos Psarakis, Marios Fragkoulis,\nand Asterios Katsifodimos. Distributed transactions\non serverless stateful functions. In Proceedings of the\n15th ACM International Conference on Distributed and\nEvent-based Systems , pages 31–42. ACM New York,\nNY , USA, 2021.\n[16] Saumen Dey, Khalid Belhajjame, David Koop, Meghan\nRaul, and Bertram Ludäscher. Linking prospective and\nretrospective provenance in scripts. In 7th{USENIX }\nWorkshop on the Theory and Practice of Provenance\n(TaPP 15) , 2015.\n12\n\n[17] Sadjad Fouladi, Riad S. Wahby, Brennan Shacklett,\nKarthikeyan Vasuki Balasubramaniam, William Zeng,\nRahul Bhalerao, Anirudh Sivaraman, George Porter, and\nKeith Winstein. Encoding, fast and slow: Low-latency\nvideo processing using thousands of tiny threads. In\n14th USENIX Symposium on Networked Systems Design\nand Implementation (NSDI 17) , pages 363–376, Boston,\nMA, March 2017. USENIX Association.\n[18] Yu Gan, Yanqi Zhang, Dailun Cheng, Ankitha Shetty,\nPriyal Rathi, Nayan Katarki, Ariana Bruno, Justin Hu,\nBrian Ritchken, Brendon Jackson, Kelvin Hu, Meghna\nPancholi, Yuan He, Brett Clancy, Chris Colen, Fukang\nWen, Catherine Leung, Siyuan Wang, Leon Zaruvinsky,\nMateo Espinosa, Rick Lin, Zhongling Liu, Jake Padilla,\nand Christina Delimitrou. An open-source benchmark\nsuite for microservices and their hardware-software im-\nplications for cloud and edge systems. In Proceedings\nof the Twenty-Fourth International Conference on Ar-\nchitectural Support for Programming Languages and\nOperating Systems , ASPLOS ’19, page 3–18, New York,\nNY , USA, 2019. Association for Computing Machinery.\n[19] GCP. Google Cloud Microservices Demo (On-\nline Boutique), 2021. https://github.com/\nGoogleCloudPlatform/microservices-demo .\n[20] Boris Glavic and Gustavo Alonso. Perm: Processing\nProvenance and Data on the Same Data Model through\nQuery Rewriting. In 2009 IEEE 25th International\nConference on Data Engineering , pages 174–185. IEEE,\n2009.\n[21] Melanie Herschel, Ralf Diestelkämper, and\nHoussem Ben Lahmar. A survey on provenance:\nWhat for? what form? what from? The VLDB Journal ,\n26(6):881–906, 2017.\n[22] Zhipeng Jia and Emmett Witchel. Boki: Stateful server-\nless computing with shared logs. In Symposium on\nOperating Systems Principles (SOSP 21) . USENIX As-\nsociation, November 2021.\n[23] Eric Jonas, Johann Schleier-Smith, Vikram Sreekanti,\nChia-Che Tsai, Anurag Khandelwal, Qifan Pu, Vaishaal\nShankar, Joao Carreira, Karl Krauth, Neeraja Yadwad-\nkar, et al. Cloud programming simplified: A berke-\nley view on serverless computing. arXiv preprint\narXiv:1902.03383 , 2019.\n[24] Robert Kallman, Hideaki Kimura, Jonathan Natkins,\nAndrew Pavlo, Alexander Rasin, Stanley Zdonik, Evan\nP. C. Jones, Samuel Madden, Michael Stonebraker, Yang\nZhang, John Hugg, and Daniel J. Abadi. H-store: A high-\nperformance, distributed main memory transaction pro-\ncessing system. Proc. VLDB Endow. , 1(2):1496–1499,\naug 2008.[25] Ana Klimovic, Yawen Wang, Patrick Stuedi, Animesh\nTrivedi, Jonas Pfefferle, and Christos Kozyrakis. Pocket:\nElastic ephemeral storage for serverless analytics. In\n13th USENIX Symposium on Operating Systems Design\nand Implementation (OSDI 18) , pages 427–444, Carls-\nbad, CA, October 2018. USENIX Association.\n[26] Rodrigo Laigner, Yongluan Zhou, Marcos Antonio Vaz\nSalles, Yijian Liu, and Marcos Kalinowski. Data Man-\nagement in Microservices: State of the Practice, Chal-\nlenges, and Research Directions. Proc. VLDB Endow. ,\n14(13):3348–3361, sep 2021.\n[27] Taras Lykhenko, Rafael Soares, and Luis Rodrigues.\nFaastcc: Efficient transactional causal consistency for\nserverless computing. In Proceedings of the 22nd Inter-\nnational Middleware Conference , Middleware ’21, page\n159–171, New York, NY , USA, 2021. Association for\nComputing Machinery.\n[28] Ashraf Mahgoub, Karthick Shankar, Subrata Mitra,\nAna Klimovic, Somali Chaterji, and Saurabh Bagchi.\nSONIC: Application-aware data passing for chained\nserverless applications. In 2021 USENIX Annual Tech-\nnical Conference (USENIX ATC 21) , pages 285–301.\nUSENIX Association, July 2021.\n[29] Kai Mast, Andrea C Arpaci-Dusseau, and Remzi H\nArpaci-Dusseau. Lambdaobjects: re-aggregating stor-\nage and execution for cloud computing. In Proceedings\nof the 14th ACM Workshop on Hot Topics in Storage\nand File Systems , pages 15–22. ACM New York, NY ,\nUSA, 2022.\n[30] Timothy McPhillips, Tianhong Song, Tyler Kolisnik,\nSteve Aulenbach, Khalid Belhajjame, Kyle Bocinsky,\nYang Cao, Fernando Chirigati, Saumen Dey, Juliana\nFreire, et al. Yesworkflow: a user-oriented, language-\nindependent tool for recovering workflow information\nfrom scripts. arXiv preprint arXiv:1502.02403 , 2015.\n[31] Microsoft. Azure Durable Functions,\n2021. https://docs.microsoft.com/\nen-us/azure/azure-functions/durable/\ndurable-functions-overview .\n[32] Kiran-Kumar Muniswamy-Reddy, David A Holland, Uri\nBraun, and Margo I Seltzer. Provenance-aware storage\nsystems. In Usenix annual technical conference, general\ntrack , pages 43–56, 2006.\n[33] Leonardo Murta, Vanessa Braganholo, Fernando Chiri-\ngati, David Koop, and Juliana Freire. noworkflow: cap-\nturing and analyzing provenance of scripts. In Inter-\nnational Provenance and Annotation Workshop , pages\n71–83. Springer, 2014.\n13\n\n[34] OpenWhisk. Apache OpenWhisk, 2021. https://\nopenwhisk.apache.org/ .\n[35] João Felipe Pimentel, Saumen Dey, Timothy McPhillips,\nKhalid Belhajjame, David Koop, Leonardo Murta,\nVanessa Braganholo, and Bertram Ludäscher. Yin\n& yang: demonstrating complementary provenance\nfrom noworkflow & yesworkflow. In International\nProvenance and Annotation Workshop , pages 161–165.\nSpringer, 2016.\n[36] The ZeroMQ project. JeroMQ, Pure Java implementa-\ntion of libzmq, 2021. https://github.com/zeromq/\njeromq .\n[37] Qifan Pu, Shivaram Venkataraman, and Ion Stoica. Shuf-\nfling, fast and slow: Scalable analytics on serverless in-\nfrastructure. In 16th USENIX Symposium on Networked\nSystems Design and Implementation (NSDI 19) , pages\n193–206, Boston, MA, February 2019. USENIX Asso-\nciation.\n[38] Salvatore Sanfilippo. Retwis, 2021. https://github.\ncom/antirez/retwis .\n[39] Simon Shillaker and Peter Pietzuch. Faasm: Lightweight\nisolation for efficient stateful serverless computing. In\n2020 USENIX Annual Technical Conference (USENIX\nATC 20) , pages 419–433. USENIX Association, July\n2020.\n[40] SingleStore. SingleStore: The Single Database for\nAll Data-Intensive Applications, 2021. https://www.\nsinglestore.com/ .\n[41] Vikram Sreekanti, Chenggang Wu, Saurav Chhatrapati,\nJoseph E. Gonzalez, Joseph M. Hellerstein, and Jose M.\nFaleiro. A fault-tolerance shim for serverless computing.\nInProceedings of the Fifteenth European Conference on\nComputer Systems , EuroSys ’20, New York, NY , USA,\n2020. Association for Computing Machinery.\n[42] Vikram Sreekanti, Chenggang Wu, Xiayue Charles Lin,\nJohann Schleier-Smith, Joseph E. Gonzalez, Joseph M.\nHellerstein, and Alexey Tumanov. Cloudburst: State-\nful functions-as-a-service. Proc. VLDB Endow. ,\n13(12):2438–2452, July 2020.\n[43] Vertica. Vertica, 2021. https://www.vertica.com/ .\n[44] V oltDB. V oltDB, 2021. https://www.voltdb.com/ .\n[45] Chenggang Wu, Vikram Sreekanti, and Joseph M.\nHellerstein. Transactional causal consistency for server-\nless computing. In Proceedings of the 2020 ACM SIG-\nMOD International Conference on Management of Data ,\nSIGMOD ’20, page 83–97, New York, NY , USA, 2020.\nAssociation for Computing Machinery.[46] YugabyteDB. YugabyteDB: The Global Scalable Re-\nsilient distributed SQL Database, 2021. https://www.\nyugabyte.com/ .\n[47] Haoran Zhang, Adney Cardoza, Peter Baile Chen, Se-\nbastian Angel, and Vincent Liu. Fault-tolerant and trans-\nactional stateful serverless workflows. In 14th USENIX\nSymposium on Operating Systems Design and Imple-\nmentation (OSDI 20) , pages 1187–1204. USENIX As-\nsociation, November 2020.\n[48] Tian Zhang, Dong Xie, Feifei Li, and Ryan Stutsman.\nNarrowing the gap between serverless and its state with\nstorage functions. In Proceedings of the ACM Sympo-\nsium on Cloud Computing , SoCC ’19, page 1–12, New\nYork, NY , USA, 2019. Association for Computing Ma-\nchinery.\n[49] Xinjing Zhou, Xiangyao Yu, Goetz Graefe, and Michael\nStonebraker. Lotus: Scalable multi-partition transac-\ntions on single-threaded partitioned databases. Proc.\nVLDB Endow. , 15(11):2939–2952, 2022.\n14",
  "textLength": 73005
}