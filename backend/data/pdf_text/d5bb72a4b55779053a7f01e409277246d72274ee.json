{
  "paperId": "d5bb72a4b55779053a7f01e409277246d72274ee",
  "title": "FLAT: Fast, Lightweight and Accurate Method for Cardinality Estimation",
  "pdfPath": "d5bb72a4b55779053a7f01e409277246d72274ee.pdf",
  "text": "FLAT : Fast, Lightweight and Accurate Method\nfor Cardinality Estimation\nRong Zhu1,#, Ziniu Wu1,#, Yuxing Han1, Kai Zeng1,âˆ—, Andreas Pfadler1,\nZhengping Qian1, Jingren Zhou1, Bin Cui2\n1Alibaba Group,2Peking University\n1{red.zr, ziniu.wzn, yuxing.hyx, zengkai.zk, andreaswernerrober, zhengping.qzp,\njingren.zhou}@alibaba-inc.com2bin.cui@pku.edu.cn\nABSTRACT\nQuery optimizers rely on accurate cardinality estimation ( CardEst )\nto produce good execution plans. The core problem of CardEst is\nhow to model the rich joint distribution of attributes in an accurate\nand compact manner. Despite decades of research, existing meth-\nods either over-simplify the models only using independent fac-\ntorization which leads to inaccurate estimates, or over-complicate\nthem by lossless conditional factorization without any indepen-\ndent assumption which results in slow probability computation.\nIn this paper, we propose FLAT , aCardEst method that is simulta-\nneously fastin probability computation, lightweight in model size\nandaccurate in es timation quality. The key idea of FLAT is a novel\nunsupervised graphical model, called FSPN . It utilizes both inde-\npendent and conditional factorization to adaptively model different\nlevels of attributes correlations, and thus combines their advan-\ntages. FLAT supports efficient online probability computation in\nnear linear time on the underlying FSPN model, provides effective\noffline model construction and enables incremental model updates.\nIt can estimate cardinality for both single table queries and multi-\ntable join queries. Extensive experimental study demonstrates the\nsuperiority of FLAT over existing CardEst methods: FLAT achieves\n1â€“5 orders of magnitude better accuracy, 1â€“3 orders of magnitude\nfaster probability computation speed and 1â€“2 orders of magnitude\nlower storage cost. We also integrate FLAT into Postgres to perform\nan end-to-end test. It improves the query execution time by 12.9%\non the well-known IMDB benchmark workload, which is very close\nto the optimal result 14.2%using the true cardinality.\nPVLDB Reference Format:\nRong Zhu, Ziniu Wu, Yuxing Han, Kai Zeng, Andreas Pfadler, Zhengping\nQian, Jingren Zhou, Bin Cui. FLAT: Fast, Lightweight and Accurate Method\nfor Cardinality Estimation. PVLDB, 14(9): 1489 - 1502, 2021.\ndoi:10.14778/3461535.3461539\n#The first two authors contribute equally to this paper.\nâˆ—Corresponding author.\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 14, No. 9 ISSN 2150-8097.\ndoi:10.14778/3461535.34615391 INTRODUCTION\nCardinality estimation (CardEst ) is a key component of query op-\ntimizers in modern database management systems (DBMS) and\nanalytic engines [ 1,53]. Its purpose is to estimate the result size of\na SQL query before its actual execution, thus playing a central role\nin generating high-quality query plans.\nGiven a table ğ‘‡and a query ğ‘„, estimating the cardinality of\nğ‘„is equivalent to computing ğ‘ƒâ€”the probability of records in ğ‘‡\nsatisfyingğ‘„. Therefore, the core task of CardEst is to condense\nğ‘‡into a model ğ‘€to compute ğ‘ƒ. In general, such models could be\nobtained in two ways: query-driven anddata-driven . Query-driven\napproaches learn functions mapping a query ğ‘„to its predicted\nprobabilityğ‘ƒ, so they require large amounts of executed queries as\ntraining samples. They only perform well if future queries follow the\nsame distribution as the training workload. Data-driven approaches\nlearn unsupervised models of Pr(ğ‘‡)â€”the joint probability density\nfunction (PDF) of attributes in ğ‘‡. As they can generalize to unseen\nquery workload, data-driven approaches receive more attention\nand are widely used for CardEst .\nChallenge and Status of CardEst .An effectual CardEst method\nshould satisfy three criteria [ 13,21,57,60], namely high estimation\naccuracy, fast inference time and lightweight storage overhead, at\nthe same time. Existing methods have made some efforts in finding\ntrade-offs between the them. However, they still suffer from one or\nmore deficiencies when modeling real-world complex data.\nIn a nutshell, there exist three major strategies for building un-\nsupervised models of Pr(ğ‘‡)on data table ğ‘‡. The first strategy di-\nrectly compresses and stores all entries in Pr(ğ‘‡)[15,46], whose\nstorage overhead is intractable and the lossy compression may sig-\nnificantly impact estimation accuracy. The second strategy utilizes\nsampling [ 29,65] or kernel density based methods [ 18,23], where\nsamples from ğ‘‡are fetched on-the-fly to estimate probabilities.\nFor high-dimensional data, they may be either inaccurate without\nenough samples or inefficient due to a large sample size.\nThe third strategy, factorization based methods, is to decom-\npose Pr(ğ‘‡)into multiple low-dimensional PDFs Pr(ğ‘‡â€²)such that\ntheir suitable combination can approximate Pr(ğ‘‡). However, ex-\nisting methods often fail to balance the three criteria. Some meth-\nods, including deep auto-regression [ 17,62,63] and Bayesian Net-\nwork [ 12,57], can losslessly decompose Pr(ğ‘‡)using conditional\nfactorization . However, their probability computation speed is re-\nduced drastically. Other methods, such as 1-D histogram [ 51] and\nsum-product network [ 20], assume global or local independencearXiv:2011.09022v5  [cs.DB]  19 May 2021\n\nbetween attributes to decompose Pr(ğ‘‡). They attain high compu-\ntation efficiency but their estimation accuracy is low when the\nindependence assumption does not hold. We present a detailed\nanalysis of existing data-driven CardEst methods in Section 2.\nOur Contributions. In this paper, we address the CardEst problem\nmore comprehensively in order to satisfy all three criteria. We ab-\nsorb the advantages of existing models and design a novel graphical\nmodel, called factorize- sum-split-product network ( FSPN ). Its key\nidea to adaptively decompose Pr(ğ‘‡)according to the dependence\nlevel of attributes. Specifically, the joint PDF of highly and weakly\ncorrelated attributes will be losslessly separated by conditional\nfactorization and modeled accordingly. The joint PDF of highly\ncorrelated attributes can be easily modeled as a multivariate PDF.\nFor the weakly correlated attributes, their joint PDF is split into\nmultiple small regions where attributes are mutually independent\nin each. We prove that FSPN subsumes 1-D histogram, sum-product\nnetwork and Bayesian network, and leverages their advantages.\nBased on the FSPN model, we propose a CardEst method called\nFLAT , which is fast,lightweight andaccurate . On a single table, FLAT\napplies an effective offline method for the structure construction\nofFSPN and an efficient online probability computation method\nusing the FSPN . The probability computation complexity of FLAT\nis almost linear w.r.t. the number of nodes in FSPN . Moreover, FLAT\nenables fast incremental updates of the FSPN model.\nFor multi-table join queries, FLAT uses a new framework, which\nis more general and applicable than existing work [ 17,20,24,63].\nIn the offline phase, FLAT clusters tables into several groups and\nbuilds an FSPN for each group. In the online phase, FLAT combines\nthe probabilities of sub-queries in a fast way to get the final result.\nIn our evaluation, FLAT achieves state-of-the-art performance\non both single table and multi-table cases in comparison with all\nexisting methods [ 20,23,24,29,46,57,62,64]. On single table, FLAT\nachieves up to 1â€“5orders of magnitude better accuracy, 1â€“3orders\nof magnitude faster probability computation speed (near 0.2ğ‘šğ‘ )\nand1â€“2orders of magnitude lower storage cost (only tens of KB).\nOn the JOB-light benchmark [ 28,30] and a more complex crafted\nmulti-table workload, FLAT also attains the highest accuracy and\nan order of magnitude faster computation time (near 5ğ‘šğ‘ ), while\nrequiring only 3.3MB storage space. We also integrate FLAT into\nPostgres. It improves the average end-to-end query time by 12.9%on\nthe benchmark workload, which is very close to the optimal result\n14.2%using the true cardinality. This result confirms with a positive\nanswer to the long-existing question whether and how much a\nmore accurate CardEst can improve the query plan quality [ 44]. In\naddition, we have deployed FLAT in the production environment of\nour company. We also plan to release to the community an open-\nsource implementation of FLAT .\nIn summary, our main contributions are listed as follows:\n1) We analyze in detail the status of existing data-driven CardEst\nmethods in terms of the above three criteria (in Section 2).\n2) We present FSPN , a novel unsupervised graphical model,\nwhich combines the advantages of existing methods in an adaptive\nmanner (in Section 3).\n3) We propose FLAT , aCardEst method with fast probability\ncomputation, high estimation accuracy and low storage cost, on\nboth single table and multi-table join queries (in Section 4 and 5).4) We conduct extensive experiments and end-to-end test on\nPostgres to demonstrate the superiority and practicality of our\nproposed methods (in Section 6).\n2 PROBLEM DEFINITION AND BACKGROUND\nIn this section, we formally define the CardEst problem and analyze\nthe status of data-driven CardEst methods. Based on the analysis,\nwe summarize some key findings that inspire our work.\nCardEst Problem. Letğ‘‡be a table with a set of ğ‘˜attributesğ´=\n{ğ´1,ğ´2,...,ğ´ğ‘˜}.ğ‘‡could either be a single or a joined table. Each\nattributeğ´ğ‘–inğ‘‡is assumed to be either categorical, so that values\ncan be mapped to integers, or continuous. Without loss of generality,\nwe assume that the domain of ğ´ğ‘–is[ğ¿ğµğ‘–,ğ‘ˆğµğ‘–].\nIn this paper, we do not consider â€œLIKEâ€ queries on strings. Any\nselection query ğ‘„onğ‘‡may be represented in canonical form:\nğ‘„=(ğ´1âˆˆ[ğ¿1,ğ‘ˆ1]âˆ§ğ´2âˆˆ[ğ¿2,ğ‘ˆ2]âˆ§Â·Â·Â·âˆ§ğ´ğ‘˜âˆˆ[ğ¿ğ‘˜,ğ‘ˆğ‘˜]), where\nğ¿ğµğ‘–â‰¤ğ¿ğ‘–â‰¤ğ‘ˆğ‘–â‰¤ğ‘ˆğµğ‘–for allğ‘–. W.l.o.g., the endpoints of each\ninterval can also be open. We call ğ‘„apoint query ifğ¿ğ‘–=ğ‘ˆğ‘–for allğ‘–\nandrange query otherwise. If ğ‘„has no constraint on the left or right\nhand side of ğ´ğ‘–, we simply set ğ¿ğ‘–=ğ¿ğµğ‘–orğ‘ˆğ‘–=ğ‘ˆğµğ‘–, respectively.\nFor any query ğ‘„â€²where the constraint of an attribute ğ´ğ‘–contains\nseveral intervals, we may split ğ‘„â€²into multiple queries satisfying\nthe above form.\nLetCard(ğ‘‡,ğ‘„)denote the exact number of records in ğ‘‡satis-\nfying all predicates in ğ‘„. Generally, the CardEst problem asks to\nestimate the value of Card(ğ‘‡,ğ‘„)as accurately as possible without\nexecutingğ‘„onğ‘‡.CardEst is often modeled and solved from a statis-\ntical perspective. We can regard each attribute ğ´ğ‘–inğ‘‡as a random\nvariable. The table ğ‘‡essentially represents a set of i.i.d. records\nsampled from the joint PDF Prt(ğ´)=Prt(ğ´1,ğ´2,...,ğ´ğ‘˜). For any\nqueryğ‘„, letPrt(ğ‘„)denote the probability of records in ğ‘‡satisfy-\ningğ‘„. We have Card(ğ‘‡,ğ‘„)=Prt(ğ‘„)Â·|ğ‘‡|. Therefore, estimating\nCard(ğ‘‡,ğ‘„)is equivalent to estimating the probability Prt(ğ‘„). Un-\nsupervised CardEst solves this problem in a purely data-driven\nfashion, which can be formally stated as follows:\nOffline Training: Given a table ğ‘‡with a setğ´of attributes as input,\noutput a model bPrt(ğ´)forPrt(ğ´)such that bPrt(ğ´)â‰ˆPrt(ğ´).\nOnline Probability Computation: Given the model bPrt(ğ´)and\na queryğ‘„as input, output bPrt(ğ‘„)Â·|ğ‘‡|as the estimated cardinality.\nData-Driven CardEst Methods Analysis. We use three criteria,\nnamely model accuracy ,probability computation speed andstorage\noverhead , to analyze existing methods. The results are as follows:\n1)Lossy FullStore [15] stores all entries in Prt(ğ´)using compres-\nsion techniques, whose storage grows exponentially in the number\nof attributes and becomes intractable [62, 63].\n2)Sample and Kernel-based methods [18,23,29,65] do not store\nPrt(ğ´)but rather sample records from ğ‘‡on-the-fly, or use aver-\nage kernels centered around sampled points to estimate Prt(ğ‘„).\nFor high-dimensional data, they may be either inaccurate without\nenough samples, or inefficient due to a large sample size.\nAlternatively, a more promising way is to factorize Prt(ğ´)into\nmultiple low-dimensional PDFs Prt(ğ´â€²)such that: 1)|ğ´â€²|<<|ğ´|so\nPrt(ğ´â€²)is easier to store and model; and 2) a suitable combination,\ne.g. multiplication, weighted sum and etc, of Prt(ğ´â€²)approximates\nPrt(ğ´). Some representative methods are listed in the following:\n\n3)1-D Histogram [51] assumes all attributes are mutually inde-\npendent, so that bPrt(ğ´)=Ãğ‘˜\nğ‘–=1bPrt(ğ´ğ‘–). EachbPrt(ğ´ğ‘–)is built as a\n(cumulative) histogram, so bPrt(ğ‘„)may be obtained in ğ‘‚(|ğ´|)time.\nHowever, the estimation errors may be high, since correlations\nbetween attributes are ignored.\n4)M-D Histogram [7,14,46,59] builds multi-dimensional his-\ntograms to model the dependency of attributes. They identify sub-\nsets of correlated attributes using models such as Markov network,\nbuild histograms on each subset and assume the independence\nacross different subsets. It improves the accuracy but the decompo-\nsition is still lossy. Meanwhile, it is space consuming.\n5)Deep Auto-Regression (DAR) [17,62,63] decomposes the joint\nPDF according to the chain rule, i.e., Prt(ğ´)=Prt(ğ´1)Â·Ãğ‘˜\nğ‘–=2Prt(ğ´ğ‘–|\nğ´1,ğ´2,...,ğ´ğ‘–âˆ’1). Each conditional PDF can be parametrically mod-\neled by a deep neural network (DNN). While the expressiveness\nof DNNs allows Prt(ğ´)to be approximated well, probability com-\nputation time and space cost increase with the width and depth of\nthe DNN. Moreover, for range query ğ‘„, computing Prt(ğ‘„)requires\naveraging the probabilities of lots of sample points in the range.\nThus, the probability computation on DAR is relatively slow.\n6)Bayesian Network (BN) [4,12,57] models the dependence\nstructure between all attributes as a directed acyclic graph and\nassumes that each attribute is conditionally independent of the\nremaining attributes given its parents. The probability Prt(ğ´)is\nfactorized as Prt(ğ´)=Ãğ‘˜\nğ‘–=1Prt(ğ´ğ‘–|ğ´pa(ğ‘–)), where pa(ğ‘–)is the par-\nent attributes of ğ´ğ‘–inBN. Learning the BNstructure from data and\nprobability computation on BNare both NP-hard [5, 50].\n7)Sum-Product Network (SPN) [20] approximates Prt(ğ´)using\nseveral local and simple PDFs. An SPN is tree structure where each\nnode stands for an estimated PDF bPrtâ€²(ğ´â€²)of the attribute subset\nğ´â€²on record subset ğ‘‡â€²âŠ†ğ‘‡[45]. The root node represents bPrt(ğ´).\nEach inner node is: 1) a sum node which splits all records (rows) in\nğ‘‡â€²intoğ‘‡â€²\nğ‘–on each child such that bPrtâ€²(ğ´â€²)=Ã\nğ‘–ğ‘¤ğ‘–bPrtâ€²\nğ‘–(ğ´â€²)with\nweightsğ‘¤ğ‘–; or 2) a product node which splits attributes (columns)\ninğ´â€²on each child as bPrtâ€²(ğ´â€²)=Ã\nğ‘—bPrtâ€²(ğ´â€²\nğ‘—)when allğ´â€²\nğ‘—are\nmutually independent in ğ‘‡â€². Each leaf node then maintains a (cu-\nmulative) PDF on a singleton attribute. The probability bPrt(ğ‘„)can\nbe computed in a bottom-up manner using the SPN node opera-\ntions for both point and range queries. The storage overhead and\nprobability computation cost are linear in the number of nodes of\nSPN.\nThe performance of SPN heavily relies on the local independence\nassumption. When it holds, the generated SPN is compact and\nexhibits superiority over other methods [ 20,62]. However, real-\nworld data often possesses substantial skew and strong correlations\nbetween attributes [ 57]. In this situation, SPN can not split these\nattributes using the product operation and might repeatedly apply\nthesum operation to split records into extremely small volumes [ 39],\ni.e.,|ğ‘‡â€²|=1. This would heavily increase the SPN size, degrade its\nefficiency and make the model inaccurate [6, 39].\nInspirations. Based on the analysis, there does not exist a compre-\nhensively effectual CardEst method since each method only utilizes\none factorization approach. However, independent factorization\nhas low storage cost and supports fast inference but may incur\nhuge estimation errors; conditional factorization can accuratelydecompose the PDF but the inference is costly. This leads to our\nkey question: if we could, in an adaptive manner, apply both kinds of\nfactorization, would it be possible to obtain a CardEst method that can\nsimultaneously satisfy all three criteria ? We answer this question\naffirmatively with a new unsupervised model, called factorize-split-\nsum-product network (FSPN) , which integrates the strength of both\nfactorization approaches.\n3 THE FSPN MODEL\nIn this section, we present FSPN , a new tree-structured graphical\nmodel representing the joint PDF of a set of attributes in an adaptive\nmanner. We first explain the key ideas of FSPN with an example\nand then present its formal definition. Finally, we compare FSPN\nwith aforementioned models.\nKey Ideas of FSPN .FSPN can factorize attributes with different\ndependence levels accordingly. The conditional factorization ap-\nproach is used to split highly and weakly correlated attributes. Then,\nhighly correlated attributes are directly modeled together while\nweakly correlated attributes are recursively approximated using the\nindependent factorization approach. Figure 1(a) gives an example\nof tableğ‘‡with a setğ´of four attributes water turbidity (ğ´1),tem-\nperature (ğ´2),wave height (ğ´3) and wind force (ğ´4). We elaborate\nthe process to construct its FSPN in Figure 1(b) as follows:\nAt first, we examine the correlations between each pair of at-\ntributes inğ‘‡.ğ´3andğ´4are globally highly correlated, so they can\nnot be decomposed as independent attributes unless we split ğ‘‡\ninto extremely small clusters as SPN. Instead, we can losslessly\nseparate them from other attributes as early as possible and process\neach part respectively. Let ğ»={ğ´3,ğ´4}andğ‘Š={ğ´1,ğ´2}. We\napply the conditional factorization approach and factorize Prt(ğ´)=\nPrt(ğ‘Š)Â·Prt(ğ»|ğ‘Š)(as nodeğ‘1in step 1â—‹).Prt(ğ‘Š)andPrt(ğ»|ğ‘Š)\nare then modeled in different ways.\nThe two attributes ğ´1andğ´2inğ‘Šare not independent on ğ‘‡but\nthey are weakly correlated. Thus, we can utilize the independent\nfactorization approach on small subsets of ğ‘‡. In our example, if we\nsplit all records in ğ‘‡intoğ‘‡1andğ‘‡2based on whether ğ´1is less than\n50(as nodeğ‘2in step 2â—‹),ğ´1andğ´2are independent on both ğ‘‡1and\nğ‘‡2. This situation is called contextually independent , whereğ‘‡1andğ‘‡2\nrefer to the specific context. Since Prt1(ğ‘Š)=Prt1(ğ´1)Â·Prt1(ğ´2)\n(as nodeğ‘4in step 3â—‹), we then simply use two univariate PDFs\n(such as histograms in leaf nodes ğ¿1andğ¿2in step 3â—‹) to model\nPrt1(ğ´1)andPrt1(ğ´2)onğ‘‡1, respectively. Similarly, we also model\nPrt2(ğ‘Š)=Prt2(ğ´1)Â·Prt2(ğ´2)onğ‘‡2(as nodeğ‘5).\nFor the conditional PDF Prt(ğ»|ğ‘Š), we do not need to specify\nPr(ğ»|ğ‘¤)for each value ğ‘¤ofğ‘Š. Instead, we can recursively split ğ‘‡\ninto multiple regions ğ‘‡ğ‘—in terms ofğ‘Šsuch thatğ»is independent\nofğ‘Šin each context ğ‘‡ğ‘—, i.e., Prtğ‘—(ğ»)=Prtğ‘—(ğ»|ğ‘Š). At this time, for\nany valueğ‘¤ofğ‘Šfalling in the same region, Prtğ‘—(ğ»|ğ‘¤)stays the\nsame, so we only need to maintain Prtğ‘—(ğ»)for each region. We refer\nto this situation as contextual condition removal . In our example,\nwe splitğ‘‡intoğ‘‡3andğ‘‡4(as nodesğ‘3in step 4â—‹) by whether\nthe condition attribute ğ´1is less than 0.9.ğ‘Šis independent of ğ»\non each leaf node region, so we only need to model Prt3(ğ»)and\nPrt4(ğ»). Thus, we model them as two multivariate leaf nodes ğ¿5and\nğ¿6in step 4â—‹. Note that, attribute values in ğ»are interdependent\nand their joint PDFs Prt3(ğ»)andPrt4(ğ»)are sparse in the two-\ndimensional space, so they are easy modeled as a multivariate PDF.\n\nsplit\tğ‘¨ğŸğ‘¨ğŸ\tğ‘¨ğŸ‘ğ‘¨ğŸ’0.2743.70.95.50.6346.33.716.20.5551.63.214.9â€¦â€¦â€¦â€¦2.6162.81.58.90.9773.62.111.7W = { ğ‘¨ğŸ, ğ‘¨ğŸ}weakly correlatedH = { ğ‘¨ğŸ‘, ğ‘¨ğŸ’}highly correlatedT1T2T3T4â‘ â‘¡â‘£â‘¢factorizeğ‘µğŸğ‘µğŸğ‘µğŸ‘ğ‘µğŸ’ğ‘µğŸ“splitproduct univariate leaf0.30.7sumğ‘¨ğŸ< 50?ğ‘¨ğŸ< 0.9? multivariate leafğ‘³ğŸ“ğ‘³ğŸ”ğ‘³ğŸğ‘³ğŸğ‘³ğŸ‘ğ‘³ğŸ’ (a) Ocean Observation Data Table (c) The FSPNStructureğ‘µğŸğ‘µğŸ’sumğ‘¨ğŸ< 50?ğ‘µğŸ“PrT(W)PrT1(W)PrT2(W)ğ‘µğŸ’product univariate leafğ‘³ğŸğ‘³ğŸPrT1(W)PrT1(ğ‘¨ğŸ)PrT1(ğ‘¨ğŸ) ğ‘¨ğŸ‘ ğ‘¨ğŸ’ ğ‘¨ğŸ‘ ğ‘¨ğŸ’ğ‘µğŸğ‘µğŸfactorizeğ‘µğŸ‘PrT(A)PrT(W)PrT(H|W)ğ‘µğŸ‘ multivariate leafğ‘³ğŸ“ğ‘³ğŸ”PrT(H|W)PrT4(H)â‘¡â‘£â‘¢ (b) Detailed Steps to Construct the FSPNStructureâ‘ ğ‘¨ğŸ< 0.9?Figure 1: An ocean observation data table and its corresponding FSPN .\nFinally, we obtain an FSPN in Figure 1(c) containing 11 nodes,\nwhere 5 inner nodes represent different operations to split data and\n6 leaf nodes keep PDFs for different parts of the original data.\nFormulation of FSPN .LetFdenote a FSPN modeling the joint\nPDF Prt(ğ´)for recordsğ‘‡with attributes ğ´.Fis a tree structure.\nEach nodeğ‘inFis a 4-tuple(ğ´n,ğ¶n,ğ‘‡n,ğ‘‚n)where:\nâ€¢ğ‘‡nâŠ†ğ‘‡represents a set of records where the PDF is built on. It\nis called the context of nodeğ‘.\nâ€¢ğ´n,ğ¶nâŠ†ğ´represent two set of attributes. We call ğ´nandğ¶n\nthescope andcondition of nodeğ‘, respectively. If ğ¶n=âˆ…,ğ‘rep-\nresents the PDF Prtn(ğ´n); otherwise, it represents the conditional\nPDF Prtn(ğ´n|ğ¶n). The root ofF, such asğ‘1in Figure 1(c), is a\nnode withğ´n=ğ´,ğ¶n=âˆ…andğ‘‡n=ğ‘‡representing the joint PDF\nPrt(ğ´).\nâ€¢ğ‘‚nstands for the operation specifying how to split data to\ngenerate its children in different ways:\n1) A Factorize (|â—‹)node, such as ğ‘1in step 1â—‹, splits highly\ncorrelated attributes from the remaining ones by conditional factor-\nization only when ğ¶n=âˆ…. Letğ»âŠ†ğ´nbe a subset of highly corre-\nlated attributes. It generates the left child ğ‘l=(ğ´nâˆ’ğ»,âˆ…,ğ‘‡n,ğ‘‚l)\nand the right child ğ‘r=(ğ»,ğ´ nâˆ’ğ»,ğ‘‡ n,ğ‘‚r). We have Prtn(ğ´n)=\nPrtn(ğ´nâˆ’ğ»)Â·Prtn(ğ»|ğ´nâˆ’ğ»).\n2) A Sum (+â—‹)node, such as ğ‘2in step 2â—‹, splits the records in\nğ‘‡nin order to enforce contextual independence only when ğ¶n=âˆ….\nWe partition ğ‘‡ninto subsets ğ‘‡1,ğ‘‡2,...,ğ‘‡ğ‘›. For each 1â‰¤ğ‘–â‰¤ğ‘›,ğ‘\ngenerates the child ğ‘ğ‘–=(ğ´n,âˆ…,ğ‘‡ğ‘–,ğ‘‚ğ‘–)with weight ğ‘¤ğ‘–=|ğ‘‡ğ‘–|/|ğ‘‡n|.\nWe can regard ğ‘as a mixture of models on all of its children, i.e.,\nPrtn(ğ´n)=Ãğ‘›\nğ‘–=1ğ‘¤ğ‘–Prtğ‘–(ğ´n), whereğ‘¤ğ‘–represents the proportion\nof theğ‘–-th subset.\n3) A Product (Ã—â—‹)node, such as ğ‘4in step 3â—‹, splits the scope\nğ´nofğ‘only whenğ¶n=âˆ…and contextual independence holds.\nLetğ´1,ğ´2,...,ğ´ğ‘šbe the mutually independent partitions of ğ´n.\nğ‘generates children ğ‘ğ‘—=(ğ´ğ‘—,âˆ…,ğ‘‡n,ğ‘‚ğ‘—)for all 1â‰¤ğ‘—â‰¤ğ‘šsuch\nthat Prtn(ğ´n)=Ãğ‘š\nğ‘—=1Prtn(ğ´ğ‘—).\n4) A Split (âˆ’â—‹)node, such as ğ‘3in step 4â—‹, partitions the records\nğ‘‡ninto disjoint subsets ğ‘‡1,ğ‘‡2,...,ğ‘‡ğ‘‘only whenğ¶nâ‰ âˆ…. For each\n1â‰¤ğ‘–â‰¤ğ‘‘,ğ‘generates the child ğ‘ğ‘–=(ğ´n,ğ¶n,ğ‘‡ğ‘–,ğ‘‚ğ‘–). Note that\nfor any value ğ‘ofğ¶n, there exists exactly one ğ‘—such thatğ‘falls in\nthe region of ğ‘‡ğ‘—. The semantic of split is different from sum. The\nsplit node divides a large model of Prğ‘‡n(ğ´n|ğ¶n)into several parts\nby the values of ğ¶n. Whereas, the sum node decomposes a large\nmodel of Prğ‘‡n(ğ´n)to small models on the space of ğ´n.\n5) A Uni-leaf (â–¡)node, such as ğ¿1andğ¿2in step 3â—‹, keeps the\nunivariate PDF Prğ‘‡n(ğ´n), such as histogram or Gaussian mixturemodel, only when|ğ´n|=1andğ¶n=âˆ….\n6) A Multi-leaf (â–¡ â–¡)node, such as ğ¿5andğ¿6in step 4â—‹, main-\ntains the multivariate PDF Prğ‘‡n(ğ´n)only whenğ¶nâ‰ âˆ…andğ´nis\nindependent of ğ¶nonğ‘‡n.\nThe above operations are recursively used to construct Fwith\nthree constraints: 1) for a factorize node, the right child must be\nasplit node or multi-leaf ; the left child can be any type in sum,\nproduct ,factorize anduni-leaf ; 2) the children of a sum orproduct\nnode could be any type in sum,product ,factorize anduni-leaf ; and\n3) the children of a split node can only be split ormulti-leaf nodes.\nDifferences with SPN.As the name suggests, FSPN is inspired by\nSPN and its successful application in CardEst [20]. However, FSPN\ndiffers from SPN in two fundamental aspects. First, in terms of the\nunderlying key ideas, FSPN tries to adaptively model attributes\nwith different levels of dependency, which is not considered in\nSPN. Second, in terms of the fundamental design choices, FSPN can\nsplit weakly and highly correlated attributes, and models each class\ndifferently: 1) weakly correlated attributes are modeled by sum and\nproduct operations; and 2) for highly correlated attributes, FSPN\nuses split andmulti-leaf nodes. SPN only uses the first technique\non all attributes. As per our analysis in Section 2, this can generate\na large structure since local independence can not easily hold.\nMoreover, a simple extension of SPN with multi-leaf nodes also\nseems unlikely to mitigate its inherent limitations. This is because\nmulti-leaf nodes can only efficiently model highly correlated at-\ntributes, as their joint PDF can be easily reduced to and modeled\nin a low dimensional space. Otherwise, their storage cost grows\nexponentially so the model size would be very large. FSPN guar-\nantees that multi-leaf nodes are only applied on highly correlated\nattributes, whereas SPN and its extensions lack such mechanism.\nOur experimental results in Section 6.1 exhibit that the model size\nofSPN with multi-leaf nodes are much larger than FSPN and may\nexceed the memory limit on highly correlated table.\nGenerality of FSPN .We show that FSPN generalizes 1-D His-\ntogram ,SPN andBNmodels. First, when all attributes are mutually\nindependent, FSPN becomes 1-D Histogram . Second, FSPN degen-\nerates to SPN by disabling the factorize operation. Third, FSPN\ncould equally represent a BNmodel on discrete attributes by itera-\ntively factorizing each attribute having no parents from others. We\nput the transformation process in Appendix A.1. Based on it, we\nobtain Lemma 1 (proved in Appendix A.2) stating that the FSPN is\nno worse than SPN andBNin terms of expressive efficiency.\nLemma 1 Given a table ğ‘‡with attributes ğ´, if the joint PDF Prt(ğ´)\nis represented by an SPNSor aBNBwith space cost ğ‘‚(ğ‘€), then\n\nthere exists an FSPNFthat can equivalently model Prt(ğ´)with no\nmore thanğ‘‚(ğ‘€)space.\n4 SINGLE TABLE CardEst METHOD\nIn this section, we propose FLAT , afast,lightweight and accurate\nCardEs talgorithm built on FSPN . We first introduce how FLAT\ncomputes the probability on FSPN online in Section 4.1. Then, we\nshow how FLAT constructs the FSPN from data offline in Section 4.2.\nFinally, we discuss how FLAT updates the model in Section 4.3.\n4.1 Online Probability Computation\nFLAT can obtain the probability (cardinality) of any query ğ‘„in\na recursive manner on FSPN . We first show the basic strategy of\nprobability computation with an example, and then present the\ndetailed algorithm and analyze its complexity.\nBasic Strategy. As stated in Section 2, the query ğ‘„can be repre-\nsented in canonical form: ğ‘„=(ğ´1âˆˆ[ğ¿1,ğ‘ˆ1]âˆ§ğ´2âˆˆ[ğ¿2,ğ‘ˆ2]âˆ§Â·Â·Â·âˆ§\nğ´ğ‘˜âˆˆ[ğ¿ğ‘˜,ğ‘ˆğ‘˜]), whereğ¿ğ‘–â‰¤ğ´ğ‘–â‰¤ğ‘ˆğ‘–is the constraint on attribute\nğ´ğ‘–. Obviously, ğ‘„represents a hyper-rectangle range in the attribute\nspace whose probability needs to be computed. In Figure 2, we give\nan example query ğ‘„on the FSPN in Figure 1(c).\nFirst, considering the root node ğ‘1, computing the probability of\nğ‘„on this factorize node is a non-trivial task. For each point ğ‘Ÿâˆˆğ‘„,\nwe can obtain its probability Prğ‘Ÿ(ğ´1,ğ´2)from nodeğ‘2and the\nconditional probability Prğ‘Ÿ(ğ´3,ğ´4|ğ´1,ğ´2)from nodeğ‘3. However,\nfor different ğ‘Ÿ,Prğ‘Ÿ(ğ´3,ğ´4|ğ´1,ğ´2)is modeled by different PDFs on\nmulti-leaf nodesğ¿5orğ¿6ofğ‘3. Thus, we must split ğ‘„into two\nregions to compute the probability of ğ‘„(as step 1â—‹in Figure 2). To\nthis end, we push ğ‘„ontoğ‘3, whose splitting rule on the condition\nattributes (ğ´1<0.9) would divide ğ‘„into two hyper-rectangle\nrangesğ‘„1andğ‘„2onmulti-leaf nodesğ¿5orğ¿6, respectively. For\nğ‘„1(orğ‘„2), the probability Pr(ğ´3,ğ´4|ğ´1,ğ´2)=Pr(ğ´3,ğ´4)can be\ndirectly obtained from the multivariate PDF on ğ¿5(orğ¿6).\nThen, we can compute the probability Pr(ğ´1,ğ´2)for each region\nğ‘„1andğ‘„2fromğ‘2. Obviously, for the sum node (e.g.ğ‘2) and prod-\nuctnode (e.g.ğ‘4), the probability of each region can be recursively\nobtained by summing (as step 3â—‹) or multiplying (as step 2â—‹) the\nprobability values of its children, respectively. In the base case, the\nprobability on the singleton attribute ğ´1(orğ´2) is obtained from\ntheuni-leaf nodesğ¿1andğ¿3(orğ¿2andğ¿4). Finally, since Pr(ğ´1,ğ´2)\nandPr(ğ´3,ğ´4)are independent in ğ‘„1andğ‘„2, we can multiply and\nsum them together ((as step 4â—‹)) to obtain the probability of ğ‘„.\nAlgorithm Description. Next, we describe the online probability\ncomputation algorithm FLAT-Online . It takes as inputs a FSPNF\nmodeling Prt(ğ´)and the query ğ‘„, and outputs Prt(ğ‘„)onF. Let\nğ‘be the root node of F(line 1). For any node ğ‘â€²inF, letFnâ€²\ndenote the FSPN rooted atğ‘â€².FLAT-Online recursively computes\nthe probability of ğ‘„by the following rules:\nRule 1 (lines 2â€“3) : Basically, if ğ‘is auni-leaf node, we directly\nreturn the probability of ğ‘„on the univariate PDF of the attribute.\nRule 2 (lines 4â€“11) : ifğ‘is asum node (lines 4â€“7) or a product\nnode (lines 8â€“11), let ğ‘1,ğ‘2,...,ğ‘ğ‘¡be all of its children. We can\nfurther call FLAT-Online onFnğ‘–for each 1â‰¤ğ‘–â‰¤ğ‘¡to obtain the\nprobability on the PDF represented by each child. Then, node ğ‘\ncomputes a weighted sum (for sum node) or multiplication (for\nproduct node) of these probabilities.Range ğ‘¨1â–  ğ‘¨2â–  ğ‘¨3â–  ğ‘¨4â– \nBound [0, 10] [0, 100] [0, 100] [0, 100]\nLeafğ¿5 [0, 0.9) [0, 100] [0, 100] [0, 100]\nLeafğ¿6 [0.9, 10] [0, 100] [0, 100] [0, 100\nQuery ğ‘¸ [0.6, 1.4] [35, 65] [2, 3] [60, 70]\nQueryğ‘„1[0.6, 0.9) [35, 65] [2, 3] [60, 70]\nQueryğ‘„2[0.9, 1.4] [35, 65] [2, 3] [60, 70]\n=+ğ‘!ğ‘\"ğ‘#ğ‘$ğ‘%0.30.7ğ¿!ğ¿\"ğ¿#ğ¿$ğ¿%ğ¿&ğ‘„ğ‘„#ğ‘„$Step â‘¢ğ‘„#ğ‘„#ğ‘„$ğ‘„Step â‘ Step â‘¡Step â‘£\nFigure 2: An example of the FLAT probability computation.\nAlgorithm FLAT-Online(F,ğ‘„)\n1:letğ‘be the root node ofF\n2:ifğ‘isuni-leaf node then\n3: return Prt(ğ‘„)by the univariate PDF on the attribute modeled by ğ‘\n4:else ifğ‘is asum node then\n5: letğ‘1,ğ‘2,...,ğ‘ğ‘¡be the children of ğ‘with weights ğ‘¤1,ğ‘¤2,...,ğ‘¤ğ‘¡\n6:ğ‘ğ‘–â†FLAT-Online(Fnğ‘–,ğ‘„)for each 1â‰¤ğ‘–â‰¤ğ‘¡\n7: returnÃğ‘¡\nğ‘–=1ğ‘¤ğ‘–ğ‘ğ‘–\n8:else ifğ‘is aproduct node then\n9: letğ‘1,ğ‘2,...,ğ‘ğ‘¡be the children of ğ‘\n10:ğ‘ğ‘–â†FLAT-Online(Fnğ‘–,ğ‘„)for each 1â‰¤ğ‘–â‰¤ğ‘¡\n11: returnÃğ‘¡\nğ‘–=1ğ‘ğ‘–\n12:else\n13: letğ¿ğ¶be the left child modeling Prt(ğ‘Š)andğ‘…ğ¶be the right child modeling Prt(ğ»|ğ‘Š)\n14: letğ¿1,ğ¿2,...,ğ¿ğ‘¡be all the multi-leaf descendants of ğ‘…ğ¶\n15: splitğ‘„intoğ‘„1,ğ‘„2,...,ğ‘„ğ‘¡by ranges ofğ¿1,ğ¿2,...,ğ¿ğ‘¡\n16: getâ„ğ‘–ofğ‘„ğ‘–on variablesğ»from the multivariate PDF on ğ¿ğ‘–for each 1â‰¤ğ‘–â‰¤ğ‘¡\n17:ğ‘¤ğ‘–â†FLAT-Online(Flc,ğ‘„ğ‘–)for each 1â‰¤ğ‘–â‰¤ğ‘¡\n18: returnÃğ‘¡\nğ‘–=1â„ğ‘–ğ‘¤ğ‘–\nRule 3 (lines 12â€“18) : ifğ‘is afactorize node, letğ¿ğ¶andğ‘…ğ¶be its\nleft and right child modeling Prt(ğ‘Š)andPrt(ğ»|ğ‘Š), respectively.\nAll descendants of ğ‘…ğ¶aresplit ormulti-leaf nodes. Letğ¿1,ğ¿2,...,ğ¿ğ‘¡\nbe all multi-leaf descendants of ğ‘…ğ¶. We assume that each split\nnode divides the attribute domain space in a grid manner, which is\nensured by the FSPN structure construction method in Section 4.2.\nThen, eachğ¿ğ‘–maintains a multivariate PDF on a hyper-rectangle\nrange specified by all split nodes on the path from ğ‘…ğ¶toğ¿ğ‘–. Based on\nthese ranges, we can divide the range of query ğ‘„intoğ‘„1,ğ‘„2,...,ğ‘„ğ‘¡.\nFor eachğ‘„ğ‘–, the probability â„ğ‘–on highly correlated attributes ğ»\ncould be directly obtained from ğ¿ğ‘–. The probability ğ‘¤ğ‘–on attributes\nğ‘Šcould be recursively obtained by calling FLAT-Online onFlc,\ntheFSPN rooted atğ¿ğ¶, andğ‘„ğ‘–. After that, since ğ»is independent\nofğ‘Šon the range of each ğ‘„ğ‘–, we sum all products â„ğ‘–ğ‘¤ğ‘–together\nas the probability of ğ‘„.\nComplexity Analysis. We assume that, on each leaf node, the\nprobability of any range can be computed in ğ‘‚(1)time, which\ncan be easily implemented by a cumulative histogram or Gaussian\nmixture functions. Let ğ‘›be the number of nodes in FSPN . Letğ‘“\nandğ‘šbe the number of factorize andmulti-leaf nodes in FSPN ,\nrespectively. The maximum number of ranges to be computed on\neach node is ğ‘‚(ğ‘šğ‘“), so the time cost of FLAT-Online isğ‘‚(ğ‘šğ‘“ğ‘›).\nBy our empirical testing, the actual time cost of FLAT-Online is\nalmost linear w.r.t. the number of nodes in FSPN for two reasons.\nFirst, FSPN is compact on real-world data so both ğ‘“andğ‘›are small.\nSecond, the computation on many ranges in each node could be\n\nâ‘¡product node:split attributesâ‘¢sum node: split recordsâ‘ factorize node:conditionally  spl itti nguni-leaf nodeâ‘£split node: split records by conditionsraw datacondition attributesrecordsattributeshighly correlated attributesFigure 3: FLAT Structure Construction Process.\neasily done in parallel. In our testing, the speed of FLAT-Online\nis even near the histogram method and 1â€“3orders of magnitude\nfaster than other methods (See Section 6.1).\n4.2 Offline Structure Construction\nWe present the detailed procedures to build an FSPN in the al-\ngorithm FLAT-Offline . Its general process is shown in Figure 3.\nFLAT-Offline works in a top-down manner. Each node ğ‘takes the\nscope attributes ğ´n, the condition attributes ğ¶nand the context of\nrecordsğ‘‡nas inputs, and recursively decompose the joint PDF to\nbuild the FSPN rooted atğ‘. To build the FSPNFmodeling table\nğ‘‡with attributes ğ´, we can directly call FLAT-Offline(ğ´,âˆ…,ğ‘‡). We\nbriefly scan its main procedures as follows:\n1. Separating highly correlated attributes with others (lines 2â€“8) :\nwhenğ¶n=âˆ…,FLAT-Offline firstly detects if there exists a set ğ»of\nhighly correlated attributes since the principle of FSPN is to sepa-\nrate them with others as early as possible (step 1â—‹in Figure 3). We\nfindğ»by examining pairwise correlations, e.g. RDC [ 35], between\nattributes and iteratively group attributes whose correlation value is\nlarger than a threshold ğœâ„. Ifğ»â‰ âˆ…, we setğ‘to be a factorize node.\nThe left child and right child of ğ‘recursively call FLAT-Offline to\nmodel Prtn(ğ´nâˆ’ğ»)andPrğ‘‡n(ğ»|ğ´nâˆ’ğ»), respectively.\n2. Modeling weakly correlated attributes (lines 9â€“19) : ifğ¶n=âˆ…\nandğ»=âˆ…, we try to split Prtn(ğ´n)into small regions such that\nattributes in ğ´nare locally independent. Specifically, if |ğ´n|=1,ğ‘\nis auni-leaf node (line 10). We call the Leaf-PDF procedure to model\nunivariate PDF Prtn(ğ´n)(line 11) using off-the-shelf tools. In our\nimplementation, we choose histograms [ 46] and parametric Gauss-\nian mixture functions [ 47] to model categorical and continuous\nattributes, respectively.\nOtherwise, we try to partition ğ´ninto mutually independent\nsubsets based on their pairwise correlations (step 2â—‹in Figure 3).\nTwo attributes are regarded as independent if their correlation\nvalue is no larger than than a threshold ğœğ‘™. Ifğ´ncan be split to\nmutually independent subsets ğ´1,ğ´2,...,ğ´ğ‘š, we setğ‘to be a\nproduct node and call FLAT-Offline to model Prğ‘‡n(ğ´ğ‘–)for each\n1â‰¤ğ‘–â‰¤ğ‘š(lines 12â€“14). If not, the local independency does not\nexist, so we need to split the data (step 3â—‹in Figure 3). Similar to [ 11],\nwe apply a clustering method, such as ğ‘˜-means [ 26], to cluster ğ‘‡n\ntoğ‘‡1,ğ‘‡2,...,ğ‘‡ğ‘›according to ğ´n(line 17). The records in the same\ncluster are similar, so the corresponding PDF becomes smoother\nand attributes are more likely to be independent. At this time, we\nsetğ‘to be a sum node and call FLAT-Offline to model Prğ‘‡ğ‘–(ğ´n)\nwith weight ğ‘¤ğ‘–=|ğ‘‡ğ‘–|/|ğ‘‡n|for each 1â‰¤ğ‘–â‰¤ğ‘›(lines 16â€“19).Algorithm FLAT-Offline(ğ´n,ğ¶n,ğ‘‡n)\n1:ifğ¶n=âˆ…then\n2: callRDC(ğ‘,ğ‘,ğ‘‡ n)for each pair of attributes ğ‘,ğ‘âˆˆğ´n\n3:ğ»â†{ğ‘,ğ‘|RDC(ğ‘,ğ‘,ğ‘‡ n)â‰¥ğœâ„}\n4: recursively enlarge ğ»â†ğ»âˆª{ğ‘|RDC(ğ‘,ğ‘,ğ‘‡ n)â‰¥ğœâ„,ğ‘âˆˆğ»,ğ‘âˆ‰ğ»}\n5: ifğ»â‰ âˆ…then\n6:ğ‘‚nâ†factorize\n7: callFLAT-Offline(ğ´nâˆ’ğ»,âˆ…,ğ‘‡n)on the left child of node ğ‘\n8: callFLAT-Offline(ğ»,ğ´ nâˆ’ğ»,ğ‘‡ n)on the right child of node ğ‘\n9: else if|ğ´n|=1then\n10:ğ‘‚nâ†uni-leaf\n11: Prtn(ğ´n)â† Leaf-PDF(ğ´n,ğ‘‡n)\n12: else if subsetsğ´1,ğ´2,...,ğ´ğ‘šare mutually indepedent then\n13:ğ‘‚nâ†product\n14: callFLAT-Offline(ğ´ğ‘–,âˆ…,ğ‘‡n[ğ´n])on each child of ğ‘for all 1â‰¤ğ‘–â‰¤ğ‘š\n15: else\n16:ğ‘‚nâ†sum\n17:ğ‘‡1,ğ‘‡2,...,ğ‘‡ğ‘›â†Cluster(ğ‘‡n,ğ´n)\n18:ğ‘¤ğ‘–â†|ğ‘‡ğ‘–|/|ğ‘‡n|for all 1â‰¤ğ‘–â‰¤ğ‘›\n19: callFLAT-Offline(ğ´n,âˆ…,ğ‘‡ğ‘–)with weightğ‘¤ğ‘–on each child of ğ‘for1â‰¤ğ‘–â‰¤ğ‘›\n20:else\n21:ğ‘šâ†maxğ‘âˆˆan,ğ‘âˆˆcnRDC(ğ‘,ğ‘,ğ‘‡ n)\n22: ifğ‘šâ‰¤ğœğ‘™then\n23:ğ‘‚nâ†multi-leaf\n24: Prtn(ğ´n)â† Leaf-PDF(ğ´n,ğ‘‡n)\n25: keep the range of ğ‘in the attribute domain space\n26: else\n27:ğ‘‚nâ†split\n28:ğ‘â†arg maxğ‘âˆˆan,ğ‘âˆˆcnRDC(ğ‘,ğ‘,ğ‘‡ n)\n29: divideğ‘‡nintoğ‘‡1,ğ‘‡2,...,ğ‘‡ğ‘‘by the range on attribute ğ‘\n30: callFLAT-Offline(ğ´n,ğ¶n,ğ‘‡ğ‘–)on each child of ğ‘for1â‰¤ğ‘–â‰¤ğ‘‘\n3. Modeling conditional PDF (lines 21â€“30) : whenğ¶nâ‰ âˆ…, we try\nto model the conditional PDF Prğ‘‡n(ğ´n|ğ¶n). First, we compute pair-\nwise correlations across all attributes in ğ´nandğ¶n(line 21). Ifğ´nis\nindependent of ğ¶n,ğ‘is amulti-leaf node. We model the multivari-\nate PDF Prğ‘‡n(ğ´n)using the piecewise regression technique [ 40]\nand maintain its range in the attribute domain space (lines 23â€“25).\nOtherwise, we further split records in ğ‘‡n(step 4â—‹in Figure 3).\nProbability computation requires ğ‘‡nto be divided into grids in terms\nofğ¶n. We apply a heuristic ğ‘‘-way partition method where ğ‘‘is a\nhyper-parameter. We choose the attribute ğ‘âˆˆğ¶nthat maximizes\nthe pairwise correlations between ğ´nandğ¶n(line 28). Intuitively,\ndividing the space by ğ‘would largely break their correlations. We\nsetğ‘to be a split node, evenly divide the range of ğ‘onğ‘‡nintoğ‘‘\nparts and get the clusters ğ‘‡1,ğ‘‡2,...,ğ‘‡ğ‘‘(line 29). After that, we call\nFLAT-Offline to model Prğ‘‡ğ‘–(ğ´n|ğ¶n)for each 1â‰¤ğ‘–â‰¤ğ‘‘(line 30).\nComplexity Analysis. Letğ‘›be the number of nodes in the re-\nsulting FSPN andğ‘ be the number of sum nodes. On each inner\nnode, we can sample a set of ğ‘Ÿrecords from table ğ‘‡to compute\nthe RDC scores between attributes. The time cost of calling RDC\nisğ‘‚(ğ‘Ÿlogğ‘Ÿ), so the total time cost is ğ‘‚(ğ‘›|ğ´|2ğ‘Ÿlogğ‘Ÿ). On each sum\nnode, we can also use the sampled records to compute the central\npoints of the clusters and then assign each record to the nearest\ncluster. We denote the maximum iteration time in ğ‘˜-means asğ‘¡.\nThe total clustering time cost on all sum nodes isğ‘‚(ğ‘ ğ‘¡ğ‘˜ğ‘Ÿ). Besides,\non each node, we need to scan all records in ğ‘‡to assign them to the\nchildren (for inner nodes) or building the PDFs (for leaf nodes). The\ntotal scanning time cost is ğ‘‚(ğ‘›|ğ‘‡|). Therefore, the time complexity\nofFLAT-Offline isğ‘‚(ğ‘›|ğ´|2ğ‘Ÿlogğ‘Ÿ+ğ‘›|ğ‘‡|+ğ‘ ğ‘¡ğ‘˜ğ‘Ÿ). Asğ‘›is often small,\nit is efficient. By our testing, learning the structure of an FSPN is\nfaster than SPN andDAR to model the same joint PDF.\n4.3 Incremental Updates\nWhen the table ğ‘‡changes, we apply an incremental update method\nFLAT-Update to ensure the underlying FSPN model can fit the new\ndata. To attain high estimation accuracy while saving update cost,\n\nwe try to preserve the original FSPN structure to the maximum\nextent while fine-tuning its parameters for better fitting.\nLetÎ”ğ‘‡be the new data inserted into (or deleted from) ğ‘‡. We\ncould traverse the FSPN in a top-down manner to fit ğ‘‡+Î”ğ‘‡(or\nğ‘‡âˆ’Î”ğ‘‡). Specifically, for each factorize nodeğ‘, since the conditional\nfactorization is a lossless decomposition of the joint PDF, we directly\npropagate Î”ğ‘‡to its children. For each split node, we propagate each\nrecord in Î”ğ‘‡to the corresponding child according to its splitting\ncondition.\nOn each original multi-leaf nodeğ¿, we recheck whether the\nconditional independence still holds after adding (or deleting) some\nrecords. If so, we just update the parameters of its multivariate PDF\nbyÎ”ğ‘‡. Otherwise, we reset it as a split node and run lines 28â€“30 of\nFLAT-Offline to further divide its domain space.\nFor each sum node, we store the centroids of all clusters in struc-\nture construction. We could assign each record in Î”ğ‘‡to the nearest\ncluster (or remove each record from its original cluster), propagate\nit to that child and update the weight of each child accordingly.\nFor each product node, we also recheck whether the indepen-\ndence between attributes subset still holds after adding (or deleting)\nsome records. If not, we run lines 12â€“19 of FLAT-Offline to recon-\nstruct the sub-structure of the FSPN . Otherwise, we directly pass\nÎ”ğ‘‡to its children. On each uni-leaf node, we update its parame-\nters of the univariate PDF by Î”ğ‘‡. Obviously, after updating, the\ngenerated FSPN can accurately fit the PDF of ğ‘‡+Î”ğ‘‡(orğ‘‡âˆ’Î”ğ‘‡).\nDue to space limits, we put the pseudocode of FLAT-Update\nin Appendix B.1 of the technical report [ 66]. It can run in the\nbackground of the DBMS. Note that, FLAT-Update does not change\nthe original FSPN model when the data distribution keeps the same.\nIn case of significant change of data or data schema changes, such\nas inserting or deleting attributes, the FSPN could be rebuilt by\ncalling FLAT-Offline in Section 4.2.\nAlgorithm FLAT-Multi(ğ·,ğ‘„)\n1:organize all tables in ğ·as a join tree ğ½% offline\n2:foreach edge(ğ´,ğµ)âˆˆğ½do\n3: ifRDC(ğ‘,ğ‘)â‰¥ğœğ‘™for any attribute ğ‘ofğ´andğ‘ofğµthen\n4:ğ´â†{ğ´,ğµ}\n5:foreach nodeğ‘‡inğ½with attributes ğ´tofTdo\n6: add scattering coefficient columns ğ‘†tinğ‘‡\n7:Ftâ†FLAT-Offline(ğ´tâˆªğ‘†t,âˆ…,T)\n8:letğ¸={ğ‘‡1,ğ‘‡2,...,ğ‘‡ğ‘‘}denote all nodes in touched by ğ‘„% online\n9:forğ‘–â†1toğ‘‘do\n10: computeğ‘ğ‘–in Eq. (1) by Technique II\n11:return|E|Â·Ãğ‘‘\nğ‘–=1ğ‘ğ‘–\n5 MULTI-TABLE CardEst METHOD\nIn this section, we discuss how to extend FLAT algorithm to multi-\ntable join queries. We first describe our approach on a high level,\nand then elaborate the key techniques in details.\nMain Idea. To avoid ambiguity, in the following, we use printed\nletters, such as ğ‘‡,ğ·, to represent a set of tables, and calligraphic\nletters, such asT,D, to represent the corresponding full outer join\ntable. Given a database ğ·, all information of ğ·is contained inD.\nDAR -based approach [ 62] builds a single large model on D. It is\neasy to use and applicable to any type of joins between tables in ğ·\nbut suffer from significant limitations. First, no matter how many\ntables are involved in a query, the entire model has to be used forprobability computation, which may be inefficient. Second, the size\nofDgrow rapidly w.r.t. the number of tables in ğ·, so its training\ncost is high even using samples from D. Third, in case of data\nupdate of any table in ğ·, the entire model needs to be retrained.\nAnother approach [ 20] builds a set of small models, where each\ncaptures the joint PDF of several tables ğ·â€²âŠ†ğ·. The joint PDF of\nattributes inDâ€²(the full outer join table of ğ·â€²) is different from\nthat inDsince each record in Dâ€²can appear multiple times in D.\nTherefore, the local model of Dâ€²needs to involve some additional\ncolumns to correct such PDF difference. When a query touches\ntables in multiple models, all local probabilities are corrected and\nmerged together to estimate the final cardinality. This approach\nis more efficient and flexible, but it only supports the primary-\nforeign key join. This is not practical as many-to-many joins are\nvery common in query optimization (see Section 6.3 for examples\non the benchmark workload).\nTo overcome their drawbacks, our approach absorbs the key\nideas of [ 20] and also builds a set of small local models. However,\nwe extend this method to be more general and applicable. First,\nwe develop a new PDF correction paradigm, inspired by [ 20], to\nsupport more types of joins, e.g., inner or outer and many-to-many\n(See the following Technique I). Second, we specifically optimize\nthe probability computation and correction process based on our\nFSPN model (See Technique II). Third, we develop incremental\nmodel updates method for data changes (See Technique III).\nAlgorithm Description. We present a high-level description of\nour approach in the FLAT-Multi algorithm, which takes a database\nğ·and a query ğ‘„as inputs. The main procedures are as follows:\n1. Offline Construction (lines 1â€“7) :We first organize all tables in\nğ·as a treeğ½based on their joins. Initially, each node in ğ½is a table\ninğ·, and each edge in ğ½is a join between two tables. We do not\nconsider self-join and circular joins in this paper. Based on ğ½, we\ncan partition all tables in ğ·into multiple groups such that: tables\nare highly correlated in the same group but weakly correlated in\ndifferent groups. Specifically, for each edge (ğ´,ğµ)inğ½, we sample\nsome records from ğ´âŠ²âŠ³ğµ, the outer join table, and examine the\npairwise attribute correlation values between ğ´andğµ. If some\ncorrelation values are higher than a threshold, we learn the model\nonğ´âŠ²âŠ³ğµtogether, so we merge {ğ´,ğµ}to a single node. We repeat\nthis process until no pair of nodes needs to be merged. After that,\nthe probability across different nodes can roughly be assumed as\nindependent on their full outer join table.\nAfter the partition, each node ğ‘‡inğ½represents a set of one or\nmore single tables. We add some scattering coefficient columns in\nits outer join tableTfor PDF correction. The details are explained\nin the following Technique I. Then, we construct a FSPNFtonT\nusing FLAT-Offline in Section 4.2. IfTis large, we do not explicitly\nmaterialize it. Instead, we draw some samples from Tusing the\nmethod in [65] and train the FSPN model on them.\nFigure 4 depicts a example database with three tables. The join\nbetweenğ‘‡bandğ‘‡cis a many-to-many join. ğ‘‡aandğ‘‡bare highly\ncorrelated so they are merged together into node ğ‘‡1. Then, we build\ntwoFSPN sFt1andFt2on tableğ‘‡aâŠ²âŠ³ğ‘‡bandğ‘‡c, respectively.\n2. Online Processing (lines 8â€“12): Letğ¸={ğ‘‡1,ğ‘‡2,...,ğ‘‡ğ‘‘}denote\nall nodes in ğ½touched by the query ğ‘„andğ‘„ğ‘–be the sub-query on\nğ‘‡ğ‘–. By our assumption, the probability of each ğ‘„ğ‘–is independent on\n\n(a) Tableğ‘‡a\nğ´1ğ´2\nğ‘ 0\nğ‘ 2\nğ‘ 3\nğ‘ 4(b) Tableğ‘‡b\nğµ1ğµ2ğµ3\n1 0.3ğ‘€\n0 0.6ğ·\n3 0.4ğ·\n2 0.7ğ‘€\n4 0.5ğ¾\n3 0.2ğ¾(c) Tableğ‘‡cof\nnodeğ‘‡2\nğ¶1ğ¶2ğ‘†t2,{t1,t2}\nğ· 0.2 2\nğ‘€ 0.7 2\nğ· 0.8 2\nğ¾ 0.9 2(d) Tableğ‘‡aâŠ²âŠ³ğ‘‡bof nodeğ‘‡1\nğ´1ğ´2ğµ1ğµ2ğµ3ğ‘†a,bğ‘†b,ağ‘†t1,{t1,t2}\nnull null 1 0.3ğ‘€ 0 0 1\nğ‘ 0 0 0.6 ğ· 1 1 2\nğ‘ 3 3 0.4 ğ· 2 1 2\nğ‘ 2 2 0.7 ğ‘€ 1 1 1\nğ‘ 4 4 0.5 ğ¾ 1 1 1\nğ‘ 3 3 0.2 ğ¾ 2 1 1(e) Join Tree ğ½and Queryğ‘„\nğ‘‡!ğ‘‡\"ğ‘‡#ğ´!=ğµ\"ğµ#=ğ¶\"node ğ‘‡\"node ğ‘‡!\nğ‘¸:select count(*) from ğ‘‡bfull\nouter joinğ‘‡conğ‘‡b.b3=ğ‘‡c.c1\nwhereğ‘‡b.b2>0.5andğ‘‡c.c2<0.3\nFigure 4: Example databases and join query.\nthe tableE=T1âŠ²âŠ³T2âŠ²âŠ³Â·Â·Â·âŠ²âŠ³Tğ‘‘. We can efficiently correct the\nprobability from the local model Ftğ‘–onTğ‘–toEby a new paradigm.\nFinally, we multiply all probabilities to get the final result.\nTechnique I: Probability Correction Method. We need to cor-\nrect the probability to account for the effects of joining from two\naspects. We elaborate the details with the example query ğ‘„in Fig-\nure 4(e).ğ‘„is divided into two sub-queries: ğ‘„1(ğ‘‡b.b2>0.5on node\nğ‘‡1) andğ‘„2(ğ‘‡c.c2<0.3on nodeğ‘‡2). First, on node ğ‘‡1, the FSPNFt1\nis built on table ğ‘‡aâŠ²âŠ³ğ‘‡binstead of table ğ‘‡bindividually. As each\nrecord inğ‘‡bcan occur multiple times in ğ‘‡aâŠ²âŠ³ğ‘‡b, the probability\nobtained byFt1needs to be down-scaled to remove the effects of\nğ‘‡a. Second, the probability obtained on node ğ‘‡2is defined on table\nğ‘‡cindividually but not on ğ‘‡bâŠ²âŠ³ğ‘‡c. Therefore, the probability of\nğ‘„2(and alsoğ‘„1) needs to be up-scaled to add the effects of joining.\nThe above corrections are achieved by adding extra columns in\ntableTğ‘–of each node ğ‘‡ğ‘–. These columns track the number of times\nthat a record in a single table ğ´appears inTğ‘–, i.e., the scattering\neffect. Previous works [ 20,62] add columns to process the scattering\neffects of each join in only one side. However, our solution considers\nthe scattering effects on two sides of each join. It is more practical\nby supporting more join types in one framework, and more general\nby processing down-scale and up-scale effects at the same time.\nFor each pair of joined tables (ğ´,ğµ)in a nodeğ‘‡ğ‘–, we add two\nadditional attributes ğ‘†a,bandğ‘†b,ainTğ‘–.ğ‘†a,bindicates how many\nrecords inğµcan join with this record in ğ´and vice versa. We call\nsuchğ‘†a,bscattering coefficient . In Figure 4(d), we add two columns\nğ‘†a,bandğ‘†b,ain the tableğ‘‡aâŠ²âŠ³ğ‘‡bofğ‘‡1. These columns are be used\nto down-scale the effects of untouched tables inside each node.\nSimilarly, for up-scale correction, we can regard node ğ‘‡ğ‘–as the\nroot of the join tree ğ½. For each distinct sub-tree of ğ½rooted at\nğ‘‡ğ‘–containing nodes ğ¸â€²={ğ‘‡â€²\n1,ğ‘‡â€²\n2,...,ğ‘‡â€²\nğ‘‘}, we add a column ğ‘†tğ‘–,ğ¸â€²\nin tableTğ‘–indicating the scattering coefficient of each record in\nTğ‘–to the outer join table Eâ€²=Tâ€²\n1âŠ²âŠ³Tâ€²\n2âŠ²âŠ³Â·Â·Â·âŠ²âŠ³Tâ€²\nğ‘‘. For the\nnodeğ‘‡2in Figure 4(c), we add the column ğ‘†t2,{t1,t2}indicating\nthe scattering coefficient of each record in ğ‘‡cwhen joining with\nğ‘‡aâŠ²âŠ³ğ‘‡b. The method to compute the values of these scattering\ncoefficient columns has been proposed in [ 65]. Briefly speaking, we\ncan obtain the values of ğ‘†tğ‘–,ğ¸â€²by recursively aggregating over all\nsub-trees rooted at ğ‘‡ğ‘–â€™s children. Using dynamic programming, the\ntime cost of computing scatter coefficient values over all nodes is\nlinear w.r.t. table size.\nAs all tables form a join tree, the number of added scattering\ncolumns in each node is linear w.r.t. its number of tables. In each\nnodeğ‘‡ğ‘–, all scattering coefficient columns are learned together with\nother attributes when constructing the FSPNFtğ‘–.We can estimate the cardinality by the following lemma. We\nput the detailed correctness proof in Appendix C of the technical\nreport [ 66]. In a high order, for each record with down-scale value\nğ‘ and up-scale value ğ‘’, we correct its probability satisfying ğ‘„ğ‘–by\na factor ofğ‘’/ğ‘ . We setğ‘’orğ‘ to1if it is 0since records with zero\nscattering coefficient also occur once in the full outer join table.\nLemma 2 Given a query ğ‘„, letğ¸={ğ‘‡1,ğ‘‡2,...,ğ‘‡ğ‘‘}denote all\nnodes inğ½touched byğ‘„. On each node ğ‘‡ğ‘–, letğ‘†={ğ‘†a1,b1,ğ‘†a2,b2,...,\nğ‘†ağ‘›,bğ‘›}, where each(ğ´ğ‘—,ğµğ‘—)is a distinct join such that ğµğ‘—is not inğ‘„.\nLetğ‘ =(ğ‘ 1,ğ‘ 2,...,ğ‘ ğ‘›)whereğ‘†ağ‘—,bğ‘—=ğ‘ ğ‘—âˆˆNfor all 1â‰¤ğ‘–â‰¤ğ‘›denote\nan assignment to ğ‘†and dlm(ğ‘ )=Ãğ‘›\nğ‘—=1max{ğ‘ ğ‘—,1}. Let\nğ‘ğ‘–=|Tğ‘–|\n|E|Â·âˆ‘ï¸\nğ‘ ,ğ‘’\u0012\nPrTğ‘–(ğ‘„ğ‘–âˆ§ğ‘†=ğ‘ âˆ§ğ‘†tğ‘–,e=ğ‘’)Â·max{ğ‘’,1}\ndlm(ğ‘ )\u0013\n.(1)\nThen, the cardinality of ğ‘„is|E|Â·Ãğ‘‘\nğ‘–=1ğ‘ğ‘–.\nConsider again query ğ‘„in Figure 4(e). For the sub-query ğ‘„1on\nnodeğ‘‡1, we need to down-scale by ğ‘†b,aand up-scale by ğ‘†t1,{t1,t2}.\nBy Eq. (1), we haveğ‘1=(1âˆ—2+1+1)/8=1/2. Similarly, we\nhaveğ‘2=1/4for sub-query ğ‘„2, so the final cardinality of ğ‘„is\n8âˆ—(1/8)=1.\nAs a remark, if two tables ğ´andğµare inner joined in ğ‘„, we can\nadd the constraint ğ‘†a,b>0andğ‘†b,a>0(orğ‘†a,e>0andğ‘†b,e>0ifğ´\nandğµin different nodes) in Eq. (1)to remove all records in ğ´orğµ\nthat have no matches. Similarly, we only add ğ‘†b,a>0orğ‘†a,b>0\ntoğ‘„for left and right join, respectively.\nTechnique II: Fast Probability Computation: Notice that, the\nvalueğ‘ğ‘–in Eq. (1)involves summing over the probabilities of each\nassignment to the down-scale value ğ‘ and up-scale value ğ‘’. If we\ndirectly obtain all these probabilities, the time cost is very high.\nInstead, we present an optimized method to compute ğ‘ğ‘–, which\nonly requires a single traversal on the underlying FSPN model.\nSpecifically, on any node ğ‘‡in the join tree, let ğ‘†tandğ´tdenote\nthe scattering coefficient and attribute columns in T, respectively.\nWhen constructing the FSPNFt, we first use a factorize root node\nto split the joint PDF PrT(ğ‘†t,ğ´t)into PrT(ğ´t)on the left child\nğ¿ğ¶andPrT(ğ‘†t|ğ´t)on the right child ğ‘…ğ¶. Each leaf node ğ¿ofğ‘…ğ¶\nmodels a PDF of ğ‘†t. ByFSPN â€™s semantic, the probabilities of any\nqueryğ‘„onğ´tandğ‘†tare independent on each ğ¿. Then, we have\nPrâ€²\nT(ğ‘„)=âˆ‘ï¸\nl \nPrl(ğ´t)Â·âˆ‘ï¸\nğ‘ ,ğ‘’\u0012\nPrl(ğ‘†=ğ‘ âˆ§ğ‘†t,e=ğ‘’)Â·max{ğ‘’,1}\ndlm(ğ‘ )\u0013!\n=âˆ‘ï¸\nl\u0012\nPrl(ğ´t)Â·E\u0014max{ğ‘’,1}\ndlm(ğ‘ )\u0015\u0013\n.(2)\nFor the left part, the probability Prl(ğ´t)could be computed with\ntheFSPN rooted at node ğ¿ğ¶using the method in Section 4.1. For\n\nthe right part, it is a fixed expected value of max{ğ‘’,1}/dlm(ğ‘ )ofğ‘†t.\nTherefore, we can pre-compute the expected value for each possible\nğ‘†,ğ‘†t,eâŠ†ğ‘†ton each leaf ğ¿. After that, each ğ‘ğ‘–in Eq. (1)could be\nobtained by traversing the FSPNFtğ‘–only once . By our empirical\nanalysis in Section 4.1, the CardEst time cost for multi-table queries\nis also near linear w.r.t. the number of nodes in FSPN s.\nTechnique III: Incremental Updates. Next, we introduce how\nto update the underlying FSPN models in multi-table cases. We\nput the pseudocode of our algorithm FLAT-Update-Multi in Appen-\ndix B.2 [66] and describe the procedures as follows.\nFirst, we consider the case of inserting some records Î”ğ¶in a\ntableğ¶of the nodeğ‘‡. It affectsTin three aspects: 1) each record in\nÎ”ğ¶can join with other tables in ğ‘‡. We use Î”T+to denote all new\nrecords inserted into T; 2) each record inT, which does not find\na match in table ğ¶(null) but can join with the new records in Î”ğ¶,\nneeds to be removed. We denote them as Î”Tâˆ’; and 3) the scattering\ncoefficient of each record in T, which can join with new records in\nÎ”ğ¶, needs to be enlarged. We denote these records as Î”Tâˆ—. We can\ndirectly join Î”ğ¶withTto identify Î”T+,Î”Tâˆ’andÎ”Tâˆ—accordingly.\nNext, we describe how to incrementally update the FSPNFtbuilt\nby Technique II. Recall that the root node ğ‘ofFtis afactorize node\nseparating attributes and scattering coefficient columns, which\nenables fast incremental update. The left child ğ¿ğ¶ofğ‘models\nPrT(ğ´t)on all attribute columns. We could update it to fit the\ndataT+Î”T+âˆ’Î”Tâˆ’by directly calling the FLAT-Update method\nin Section 4.3. The right child ğ‘…ğ¶ofğ‘models PrT(ğ‘†t|ğ´t)on all\nscattering coefficients columns. Each multi-leaf ğ¿ofğ‘…ğ¶only stores\nsome expected values of ğ‘†tdefined by Eq. (2). We can pre-build\na hash table on the probability of each assignment ğ‘ ofğ‘†t. Then,\nbased on the changes of scattering columns in Î”T+,Î”Tâˆ’andÎ”Tâˆ—,\nwe can incrementally update all expected values.\nFinally, asTchanges, we need to propagate the effects to other\nnodesğ‘‡â€²to update all scattering columns ğ‘†tâ€²,e. For efficiency, it can\nrun in the background asynchronously. Specifically, after each time\ninterval such as one day, we scan all tables and recompute the scat-\ntering coefficients using the method in [ 65]. Then we incrementally\nupdate the expected values stored in FSPNFtâ€².\nFor the case of deleting some records Î”ğ¶in a tableğ¶of the\nnodeğ‘‡, the updating could be done in a very similar way. At this\ntime, we obtain Î”Tâˆ’containing all removed tuples joining with Î”ğ¶\npreviously, Î”T+containing all added tuples having no matches in\ntableğ¶andÎ”Tâˆ—containing all original records whose scattering\ncoefficients are reduced. Then we update the FSPNFtandFtâ€²of\nother nodes ğ‘‡â€²in the same way as the insertion case. Notice that,\nthe data insertion and deletion can also be done simultaneously as\nlong as we maintain the proper set of records Î”T+,Î”Tâˆ’andÎ”Tâˆ—. In\nthe complex case of creating new tables or deleting existing tables\nin the database, the model could be retrained offline.\n6 EVALUATION RESULTS\nWe have conducted extensive experiments to demonstrate the su-\nperiority of our proposed FLAT algorithm. We first introduce the\nexperimental settings, and then report the evaluation results of\nCardEst algorithms on the single table and multi-table cases in\nSection 6.1 and 6.2, respectively. Section 6.3 reports the effects of\nupdates. Finally, in Section 6.4, we integrate FLAT into the queryoptimizer of Postgres [ 8] and evaluate the end-to-end query opti-\nmization performance.\nBaselines. We compare FLAT with a variety of representative\nCardEst algorithms, including:\n1)Histogram : the simplest 1-D histogram based CardEst method\nwidely used in DBMS such as SQL Server [34] and Postgres [8].\n2)Naru : aDAR based algorithm proposed in [ 63]. We adopt the\nauthorsâ€™ source code from [ 64] with the var-skip speeding up tech-\nnique [ 31]. It utilizes a DNN with 5 hidden layers (512, 256, 512, 128,\n1024 neuron units) to approximate the PDFs. The sampling size is\nset to 2,000as the authorsâ€™ default. We do not compare with the\nsimilar method in [17], since their performance is close.\n3)NeuroCard [62]: an extension of Naru onto the multi-table\ncase. We also adopt the authorsâ€™ source code from [ 36] and set the\nsampling size to 8,000as the authorsâ€™ default.\n4)BN: a Bayesian network based algorithm. We use the Chow-\nLiu Tree [ 4,16] based implementation to build the BNstructure,\nsince its performance is better than others [12, 57].\n5)DeepDB : aSPN based algorithm proposed in [ 20]. We adopt\nthe authorsâ€™ source code from [ 19] and apply the same hyper-\nparameters, which set the RDC independence threshold to 0.3and\nsplit each node with at least 1%of the input data.\n6)SPN-Multi : a simple extension of SPN with multivariate leaf\nnodes. It maintains a multi-leaf node if the data volume is below\n1%and attributes are still not independent.\n7)MaxDiff : a representative M-D histogram based method [ 46].\nWe use the implementation provided in the source code repository\nof [64]. We do not compare with the improved methods DBHist [ 7],\nGenHist [14] and VIHist [59] are they are not open-sourced.\n8)Sample : the method uniformly samples a number of records\nto estimate the cardinality. We set the sampling size to 1%of the\ndataset. It is used in DBMS such as MySQL [ 48] and MariaDB [ 52].\nWe do not compare with other method such as IBJS [ 29] since their\nperformance has been verified to be less competitive [20, 62, 64].\n9)KDE: kernel density estimator based method for CardEst . We\nhave implemented it using the scikit-learn module [33].\n10)MSCN : a state-of-the-art query-driven CardEst algorithm\ndescribed in [ 24]. For each dataset, we train it with 105queries\ngenerated in the same way as the workload.\nRegarding FLAT hyper-parameters as described in Section 4.2, we\nset the RDC threshold ğœğ‘™=0.3andğœâ„=0.7for filtering independent\nand highly correlated attributes, respectively, and set ğ‘‘=2forğ‘‘-\nway partition of records. Similar to DeepDB , we also do not split a\nnode when it contains less than 1%of the input data. The sensitivity\nanalysis of hyper-parameters are put in Appendix D [66].\nEvaluation Metrics. Based on our discussion in Section 1, we con-\ncentrate on examining three key metrics: estimation accuracy, time\nefficiency and storage overhead. For estimation accuracy, we adopt\nthe widely used q-error metric [ 17,20,24,28,30,63] defined as the\nlarger value of Card(ğ‘‡,ğ‘„)/ÂšCard(ğ‘‡,ğ‘„)andÂšCard(ğ‘‡,ğ‘„)/Card(ğ‘‡,ğ‘„),\nso its optimal value 1. We report the whole q-error distribution\n(50%,90%,95%,99%and100% quantile) of each workload. For time\nefficiency, we report the estimation latency and model training time.\nFor storage overhead, we report the model size.\nEnvironment. All above algorithms have been implemented in\nPython. All experiments are performed on a CentOS Server with\n\nan Intel Xeon Platinum 8163 2.50GHz CPU having 64 cores, 128GB\nDDR4 main memory and 1TB SSD.\n6.1 Single Table Evaluation Results\nWe use two single table datasets: 1) GAS is real-world gas sensing\ndata obtained from the UCI dataset [ 49] and contains 3,843,159\nrecords. We extract the most informative 8 columns ( Time ,Humid-\nity,Temperature ,Flow_rate ,Heater_voltage ,R1,R5andR7); and 2)\nDMV [ 42] is a real-world vehicle registration information dataset\nand contains 11,591,877 tuples. We use the same 11 columns as [ 64].\nFor each dataset, we generate a workload containing 105ran-\ndomly generated queries. For each query, we use a probability of\n0.5to decide whether an attribute should be contained. As stated in\nSection 2, the domain of each attribute ğ´is mapped into an interval,\nso we uniformly sample two values ğ‘™andâ„from the interval such\nthatğ‘™â‰¤â„and setğ´âˆˆ[ğ‘™,â„].\nEstimation Accuracy. Table 1 reports the q-error distribution for\ndifferent CardEst algorithms. As main take-away, their accuracy\ncan be ranked as FLATâ‰ˆNaruâ‰ˆSPN-Multi >BN>DeepDB >>\nSample/MSCN >>KDE >>MaxDiff/Histogram . The details are\nas follows:\n1) Overall, FLAT â€™s estimation accuracy is very high. On both\ndatasets, the median q-error (1.001 and 1.002) is very close to 1,\nthe optimal value. On GAS, FLAT attains the highest accuracy. The\naccuracy of Naru andSPN-Multi is comparable to FLAT , which is\nmarginally better than FLAT on DMV. The high accuracy of Naru\nand stems from its ARbased decomposition and the large DNN\nrepresenting the PDFs. SPN-Multi achieves high accuracy as it\nmodels the PDFs of attributes without independence assumption.\n2) The accuracy BNandDeepDB is worse than FLAT . At the 95%\nquantile, FLAT outperforms BNby3.6Ã—andDeepDB by71Ã—on\nGAS. The error of BNmainly arises from its approximate structure\nconstruction. DeepDB appears to fail at splitting highly correlated\nattributes. Thus, it causes relatively large estimation errors for\nqueries involving these attributes.\n3) The accuracy of MSCN andSample appears unstable. FLAT\noutperforms MSCN by109Ã—and1.8Ã—on GAS and DMV, respec-\ntively. As MSCN is query-driven, its accuracy relies on if the work-\nload is â€œsimilarâ€ to the training samples. Whereas, FLAT outper-\nforms Sample by4.5Ã—and56Ã—on GAS and DMV, respectively as\nthe sampling space of DMV is much larger than GAS.\n4)FLAT largely outperforms Histogram ,MaxDiff andKDE since\nHistogram andMaxDiff makes coarse-grained independence as-\nsumption and KDE may not well characterize high-dimensional\ndata by tuning a good bandwidth for kernel functions [23].\nEstimation Latency. Figure 5 reports the average latency of all\nCardEst methods. Since only MSCN andNaru provide the imple-\nmentation optimized for GPUs, we compare all CardEst methods\non CPUs for fairness. We provide the comparison results on GPUs\nin Appendix E.1 [ 66]. In summary, their speed on CPUs can be\nranked as Histogramâ‰ˆFLAT >MSCN >SPN-Multi/DeepDB >\nKDE/Sample >>Others. The details are as follows:\n1)Histogram runs the fastest, it requires around 0.1ğ‘šğ‘ for each\nquery. FLAT is close with a latency around 0.2ğ‘šğ‘ and0.5ğ‘šğ‘ on DMV\nand GAS, respectively. Both are much faster than all other methods.\nThis can be credited to the FSPN model used in FLAT being bothTable 1: Performance of CardEst algorithms on single table.\nTrainingDataset Algorithm 50% 90% 95% 99% Max Size (KB)Time (Min)\nGASHistogram 2.732 53.60 163.0 2Â·1063Â·10734 1.3\nNaru 1.007 1.145 1.340 2.960 16.50 6, 365 216\nBN 1.011 1.208 1.550 4.780 36.80 108 8.2\nDeepDB 1.039 1.765 2.230 95.12 619.2 218 54\nSPN-Multi 1.005 1.169 1.289 1.461 3.702 31,253 62\nMaxDiff 2.211 86.7 196.0 3Â·1048Â·1053Â·105310\nSample 1.046 1.625 2.064 6.017 3, 410 - -\nKDE 3.307 5.469 6.742 471.0 2Â·104- 27\nMSCN 2.610 68.47 129.0 1Â·1057Â·1052, 663 662\nFLAT (Ours) 1.001 1.127 1.183 1.325 3.178 198 19\nDMVHistogram 1.184 2.541 41.72 710.0 2Â·10524 1.6\nNaru 1.006 1.184 1.368 6.907 49.03 7, 564 146\nBN 1.003 1.264 1.818 9.800 176.0 59 5.4\nDeepDB 1.005 1.574 2.604 27.90 534.0 247 48\nSPN-Multi 1.004 1.163 1.347 7.225 58.37 53,267 53\nMaxDiff 1.802 6.304 28.81 4, 320 3Â·1047Â·105249\nSample 1.122 1.619 9.010 551.0 7, 077 - -\nKDE 3.493 15.07 104.0 589.0 5Â·104- 48\nMSCN 1.215 2.612 4.420 17.90 1, 192 2, 566 744\nFLAT (Ours) 1.002 1.255 1.795 9.805 76.50 53 2.4\nFLAT\nFigure 5: Estimation latency of CardEst algorithms.\ncompact and easy to traverse for probability computation. MSCN\nis also fast since it only requires a forward pass over DNNs.\n2)DeepDB ,SPN-Multi ,KDE andSample need up to 10ğ‘šğ‘ for\neach query. FLAT is1â€“2orders of magnitude faster than them\nbecause the FSPN model used in our FLAT is more compact than\ntheSPN model in DeepDB andSPN-Multi . In addition, KDE and\nSample need to examine large amount of samples, thus less efficient.\n3)MaxDiff ,BNandNaru need 10â€“100ğ‘šğ‘ for each query. FLAT\nis2â€“3orders of magnitude faster than them, e.g., 213Ã—and599Ã—\nfaster than Naru on GAS and DMV, respectively. The time cost of\nMaxDiff is spent on decompressing the joint PDF. The inference\nonBNis NP-hard and hence inefficient. Naru requires repeated\nsampling for range querie so it is computationally demanding.\nModel Training Time. As shown in the last column in Table 1,\nFLAT is very efficient in training. Specifically, on DMV, FLAT is61Ã—\nand20Ã—faster than Naru andDeepDB in training. This is due to\nthe structure of FSPN is much smaller than SPN, and our training\nprocess does not require iterative gradient updates as required for\nSGD-based training of DNNs [2].\nStorage Overhead. Storage costs are given in Table 1. The storage\ncost of Histogram andBNis proportional to the attribute number so\nthey require the smallest storage. FLAT is also very small requiring\nabout 2Ã—ofHistogram .DeepDB requires more storage space than\nFLAT since the learned SPN has more nodes. They consume 10â€“\n100KB of storage. MSCN andNaru consume several MB since they\nstore large DNN models. SPN-Multi requires tens of MB as it needs\nto maintain the multi-leaf nodes on not highly correlated attributes,\nas we discussed in Section 3. The storage cost of MaxDiff is the\nhighest since it stores the compressed joint PDF.\n\nTrue Cardinality of queriesFigure 6: Cardinality distribution of workload on IMDB.\nModel Node Number. To give more details, we also compare the\nnumber of nodes (or neurons) in DeepDB ,SPN-Multi andNaru .\nThe 5-layer DNN in Naru is fully connected and contains 2,432\nneurons. The SPN used in DeepDB contains 873and823nodes\non GAS and DMV, respectively. SPN-Multi contains 825and787\nnodes on GAS and DMV, respectively. Whereas, the FSPN inFLAT\nonly uses 210and20nodes on GAS and DMV, respectively. FSPN\nuses 21Ã—,7.4Ã—and7Ã—less nodes than DNN, SPN andSPN-Multi\nto model the same joint PDF.\nStability. We also examine FLAT on synthetic datasets. The results\nin Appendix E.2 show that FLAT is stable to varied correlations and\ndistributions and relatively robust to varied domain size.\n6.2 Multi-Table Evaluation Results\nWe evaluate the CardEst algorithms for the multi-table case on\nthe IMDB benchmark dataset. It has been extensively used in prior\nwork [ 20,28,30,62] for cardinality estimation. We use the provided\nJOB-light query workload with 70 queries and create another more\ncomplex and comprehensive workload JOB-ours with 1,500queries.\nJOB-light â€™s schema contains six tables ( title,cast_info ,movie_info ,\nmovie_companies ,movie_keyword ,movie_info_idx ) where all other\ntables can only join with title. Each JOB-light query involves 3â€“6\ntables with 1â€“4filtering predicates on all attributes. JOB-ours uses\nthe same schema as JOB-light but each query is a range query\nusing 4â€“6tables and 2â€“7filtering predicates. The predicate of each\nattribute is set in the same way as on single table. Figure 6 illustrates\nthe true cardinality distribution of the two workloads. The scope\nof cardinality for JOB-ours is wider than JOB-light . Note that, the\nmodel of each CardEst method is the same for the two workloads.\nAs the attributes are highly correlated on IMDB, the model size of\nSPN-Multi exceeds our memory limit, so we can not evaluate it.\nResults on JOB-light .Table 2 reports the q-error and storage cost\nofCardEst methods on the JOB-light workload. We observe that:\n1) The accuracy of FLAT is the highest among all algorithms.\nNeuroCard is only a bit better w.r.t the maximum q-error, which\nreflects only one query in the workload. At the 95%quantile, FLAT\noutperforms NeuroCard by2.6Ã—,BNby33Ã—,DeepDB by1.7Ã—and\nMSCN by43Ã—. The reasons have been explained in Section 6.1.\n2) In terms of storage size, Histogram andBNare still the smallest\nandMaxDiff is still the largest. FLAT â€™s space cost is 3.3MB, which\nis10.8Ã—and2.1Ã—less than DeepDB andNeuroCard , respectively.\nIn comparison with the single table case, FLAT â€™s space cost is\nrelatively large. This is because for the multi-table case, FSPN needs\nto process more attributesâ€”the scattering coefficients columns and\nmaterialize some values for fast probability computation. However,\nit is still reasonable and affordable for modern DBMS.Table 2: Performance of CardEst algorithms on JOB-light .\nAlgorithm 50% 90% 95% 99% Max Size (KB)\nHistogram 8.310 1, 386 6, 955 8Â·1052Â·107131\nNeuroCard 1.580 4.545 5.910 8.480 8.510 7, 076\nBN 2.162 28.00 74.60 241.0 306.0 237\nDeepDB 1.250 2.891 3.769 25.10 31.50 3.7Â·104\nMaxDiff 32.31 5, 682 5Â·1044Â·1064Â·1074Â·105\nSample 2.206 65.80 1, 224 5Â·1041Â·106-\nKDE 10.56 563.0 4, 326 4Â·1058Â·106-\nMSCN 2.750 19.70 97.60 622.0 661.0 3, 421\nFLAT (Ours) 1.150 1.819 2.247 7.230 10.86 3, 430\nResults on JOB-ours .On this workload, FLAT is also the most\naccurate CardEst method. As reported in Table 3, we observe that:\n1) The performance of FLAT is better than NeuroCard and still\nmuch better than others. At the 95%quantile, FLAT outperforms\nNeuroCard ,DeepDB andMSCN by1.4Ã—,4.3Ã—and7.8Ã—, respec-\ntively. The performance of other algorithms drops significantly on\nthis workload. A similar observation is also reported in [ 62]. This\nonce again demonstrates the shortcomings of these approaches,\nespecially for complex data and difficult queries.\n2) The q-error of FLAT onJOB-ours is relatively larger than that\nonJOB-light because JOB-ours is a harder workload. As shown\nin Figure 6, the true cardinality of the tail 5%queries in JOB-ours\nis often less than 100. However, the performance of FLAT is still\nreasonable since the median value is only 1.2.\nWe also examine the detailed q-errors of FLAT and other CardEst\nmethods with different number of tables and predicates in queries.\nDue to space limits, we put the results in Appendix E.3 of the\ntechnical report [ 66]. The results show that the accuracy of our\nFLAT is more stable with number of joins and predicates.\nTime Efficiency. Figure 7 exhibits the average estimation latency\non the two workload. Obviously, Histogram is still the fastest while\nMaxDiff is still the slowest. FLAT requires around 5ğ‘šğ‘ for each\nquery, which is still much faster than others. It outperforms BN\nby5Ã—,Sample by12.4Ã—,KDE by4.8Ã—andDeepDB by5.2Ã—. The\ntraining time on the IMDB dataset is given in the last column of\nTable 3. FLAT is faster than NeuroCard and close to DeepDB .\n6.3 Effects of Updates\nWe examine the performance of our incremental update method.\nSpecifically, for data insertion evaluation, we train the base model\non a subset of IMDB data before 2004 ( 80%of data) and insert the\nrest data for updating. For data deletion, we train the base model\non all data and delete the data after 1991. We compare the accuracy\non the JOB-light workload and the update time cost of our update\nmethod with two baselines: he original stale model and the new\nmodel retained on the whole data. From Table 4, we observe that:\n1) The accuracy of the retrained model is the highest but it re-\nquires the highest updating time. The accuracy of the non-updated\nmodel is the lowest since the data distribution changes.\n2) Our update method makes a good trade-off: its accuracy is\nclose to the retrained model but its time cost is much lower. This\nshows that our FSPN model can be incrementally updated on its\nstructure and parameters to fit the new data in terms of both inser-\ntion and deletion. This is a clear advantage since the entire model\ndoes not need to be frequently retrained in presence of new data.\n\nFLATFigure 7: Estimation latency on IMDB.\nTable 3: Performance of CardEst algorithms on JOB-ours .\nTrainingAlgorithm 50% 90 95% 99% MaxTime (Min)\nHistogram 15.71 7480 4Â·1041Â·1064Â·1082.7\nNeuroCard 1.538 9.506 81.23 8012 1Â·105173\nBN 2.213 25.60 2456 2Â·1057Â·1067.3\nDeepDB 1.930 28.30 248.0 1Â·1041Â·10568\nMaxDiff 45.50 8007 2Â·1059Â·1061Â·10979\nSample 2.862 116.0 3635 3Â·1054Â·107-\nKDE 8.561 1230 1Â·1049Â·1052Â·10815\nMSCN 4.961 45.7 447.0 8576 1Â·1051, 744\nFLAT (Ours) 1.202 6.495 57.23 1120 1Â·10453\n6.4 End-to-End Evaluation on Postgres\nTo examine the performance of ML-based CardEst algorithms in\nreal-world DBMS, we integrate our FLAT andNeuroCard into the\nquery optimizer of Postgres 9.6.6 to perform an end-to-end test. We\ndo not compare with DeepDB since it can not support many-to-\nmany join. However, for many star-join queries between a primary\nkey and multiple foreign keys in the workload, the sub-queries\non joining foreign keys are many-to-many joins. Meanwhile, we\nadd the method which uses the true cardinality of each sub-query\nduring query optimization as the baseline. We report the results\nof the JOB-light workload on the IMDB benchmark dataset. The\nresults on JOB-ours are similar and put in the Appendix E.4 [66].\nWe disable parallel computing in Postgres and only allow pri-\nmary key indexing to minimize the impact of other factors [ 28,\n58].We report the total query time excluding the CardEst time cost\nin Figure 8(a) and the end-to-end query time (including plan com-\npiling and execution) in Figure 8(b). We observe that:\n1) Accurate CardEst results can help the query optimizer gener-\nate better query plans. Without considering the CardEst latency,\nboth NeuroCard andFLAT improve over Postgres by near 13%. Their\nimprovement is very close to the optimal result using true cardinal-\nity in query compiling ( 14.2%). This verifies that the accuracy of\nFLAT is sufficient to generate high-quality query plans.\n2) For the end-to-end query time, the improvement of FLAT is\nmore significant than NeuroCard . Overall, FLAT improves the query\ntime by 12.9%while NeuroCard only improves 4.6%. This is due to\ntheCardEst needs to do multiple times in query optimization. The\nlatency of NeuroCard is much longer than FLAT and degrades its\nend-to-end performance.\n3) The improvement of FLAT becomes more significant on queries\nwith more joins. On queries joining 4tables, FLAT improves the\nend-to-end query time by 26.5%because the search space of the\nquery plans grows exponentially w.r.t. the join number. If a query\nonly joins 2or3tables, its query plan is almost fixed. When it\njoins more tables, the inaccurate Postgres results may lead to aTable 4: Effects of updates on IMDB.\nUpdate Method 50% 90% 95% 99% Max Time (Min)\nInsertionNon-Updated 1.201 2.297 3.862 18.93 47.14 0\nRetrained 1.150 1.819 2.247 7.230 10.86 53\nOur Method 1.153 1.821 2.480 8.914 13.72 1.2\nDeletionNon-Updated 1.218 2.263 3.905 15.47 56.21 0\nRetrained 1.129 1.763 2.253 6.815 15.3 49\nOur Method 1.134 1.791 2.432 8.285 19.78 1.0\nsub-optimal query plan while our FLAT providing more accurate\nCardEst results can find a better plan. This phenomenon has also\nobserved and explained in [44].\n7 RELATED WORK\nWe briefly review prior work on query-driven CardEst methods and\nmachine learning (ML) applied to problems in databases. The data-\ndriven CardEst methods have already been discussed in Section 2.\nQuery-Driven CardEst Methods. Initially, prior research has ap-\nproached query-driven CardEst by utilizing feedback of past queries\nto correct generated models. Representative work includes correct-\ning and self-tuning histograms with query feedbacks [ 3,10,22,54],\nupdating statistical summaries in DBMS [ 55,61], and query-driven\nkernel-based methods [ 18,23]. Later on, with the advance of deep\nlearning, focus shifted to learning complex mappings from â€œfea-\nturizedâ€ queries to their cardinalities. Different types of models,\nsuch as deep networks [ 32], tree-based regression models [ 9] and\nmulti-set convolutional networks [ 24], were applied. In general,\nclear drawbacks of query-driven CardEst methods are as follows:\n1) their performance heavily relies on the particular choice of how\ninput queries are transformed into features; 2) they require large\namounts of previously executed queries for training; and 3) they\nonly behave well, when future input queries follow the same dis-\ntribution as the training query samples. Therefore, query-driven\nCardEst methods are not flexible and generalizable enough.\nML Applied in Databases. Recently, there has been a surge of\ninterest in using ML-based methods in order to enhance the perfor-\nmance of database components, e.g. indexing [ 41], data layout [ 25],\nquery execution [ 43] and scheduling [ 37]. Among them, learned\nquery optimizers are a noteworthy hot-spot. [ 38] proposed a query\nplan generation model by learning embeddings for all queries. [ 27]\napplied reinforcement learning to optimize the join order. We are\ncurrently trying to integrate FLAT with these two approaches to\ndesign an end-to-end solution for query optimization in databases.\nMoreover, it is worth mentioning that the proposed FSPN model\nis a very general unsupervised model, whose scope of application\nis not limited to CardEst . We are in the process of trying to apply\nto other scenarios in databases that also require modeling the joint\nPDF of high-dimensional data, such as approximate group-by query\nprocessing [ 56], hashing [ 25] and multi-dimensional indexing [ 41].\n8 CONCLUSIONS\nIn this paper, we propose FLAT , an unsupervised CardEst method\nthat is simultaneously fast in probability computation, lightweight\nin storage cost and accurate in estimation quality. It supports queries\non both single table and multi-tables. FLAT is built on FSPN , a new\ngraphical model which adaptively models the joint PDF of attributes\nand combines the advantages of existing CardEst models. Extensive\n\n2 3 4 5 Overall\nNumber of tables in query0100200300400Average execution time (s)+1.1%-1.2%-27.0%-10.6%-13.4%\n+0.9%-2.2%-24.2%-10.5%-12.7%\n-1.7%-2.3%-28.5%-11.1%-14.2%Postgres\nFLAT\nNeuroCard\nTrue Card(a)Query Time Excluding CardEst Latency\n2 3 4 5 Overall\nNumber of tables in query0100200300400Average execution time (s)+1.2%-0.6%-26.5%-10.3%-12.9%\n+8.2%+7.7%-10.6%-6.0%-4.6%Postgres\nFLAT\nNeuroCard (b)End-to-End Query Time\nFigure 8: Comparison of CardEst algorithms integrated into Postgres.\nexperimental results on benchmarks and the end-to-end evaluation\non Postgres have demonstrated the superiority of our proposed\nmethods. In the future work, we believe in that FLAT could serve\nas a key component in an end-to-end learned query optimizer for\nDBMS and the general FSPN model can play larger roles in more\ndatabase-related tasks.\n\nREFERENCES\n[1]Michael Armbrust, Reynold S Xin, Cheng Lian, Yin Huai, Davies Liu, Joseph K\nBradley, Xiangrui Meng, Tomer Kaftan, Michael J Franklin, Ali Ghodsi, et al .2015.\nSpark sql: Relational data processing in spark. In SIGMOD . 1383â€“1394.\n[2]LÃ©on Bottou. 2012. Stochastic Gradient Descent Tricks. Neural networks: Tricks\nof the trade (2012), 421â€“436.\n[3]Nicolas Bruno, Surajit Chaudhuri, and Luis Gravano. 2001. STHoles: a multidi-\nmensional workload-aware histogram. In SIGMOD . 211â€“222.\n[4]C. Chow and Cong Liu. 1968. Approximating discrete probability distributions\nwith dependence trees. IEEE transactions on Information Theory 14, 3 (1968),\n462â€“467.\n[5]Paul Dagum and Michael Luby. 1993. Approximating probabilistic inference in\nBayesian belief networks is NP-hard. Artificial intelligence 60, 1 (1993), 141â€“153.\n[6]Mattia Desana and Christoph SchnÃ¶rr. 2020. Sumâ€“product graphical models.\nMachine Learning 109, 1 (2020), 135â€“173.\n[7]Amol Deshpande, Minos Garofalakis, and Rajeev Rastogi. 2001. Independence is\ngood: Dependency-based histogram synopses for high-dimensional data. ACM\nSIGMOD Record 30, 2 (2001), 199â€“210.\n[8]Postgresql Documentation 12. 2020. Chapter 70.1. Row Estimation Examples.\nhttps://www.postgresql.org/docs/current/row-estimation-examples.html (2020).\n[9]Anshuman Dutt, Chi Wang, Azade Nazi, Srikanth Kandula, Vivek Narasayya,\nand Surajit Chaudhuri. 2019. Selectivity estimation for range predicates using\nlightweight models. PVLDB 12, 9 (2019), 1044â€“1057.\n[10] Dennis Fuchs, Zhen He, and Byung Suk Lee. 2007. Compressed histograms with\narbitrary bucket layouts for selectivity estimation. Information Sciences 177, 3\n(2007), 680â€“702.\n[11] Robert Gens and Domingos Pedro. 2013. Learning the structure of sum-product\nnetworks. In ICML . PMLR, 873â€“880.\n[12] Lise Getoor, Benjamin Taskar, and Daphne Koller. 2001. Selectivity estimation\nusing probabilistic models. In SIGMOD . 461â€“472.\n[13] G. Graefe and W. J. Mckenna. 1993. The Volcano optimizer generator: extensibility\nand efficient search. In ICDE . 209â€“218.\n[14] Dimitrios Gunopulos, George Kollios, Vassilis J Tsotras, and Carlotta Domeni-\nconi. 2000. Approximating multi-dimensional aggregate range queries over real\nattributes. In SIGMOD . 463â€“474.\n[15] Dimitrios Gunopulos, George Kollios, Vassilis J Tsotras, and Carlotta Domeni-\nconi. 2005. Selectivity estimators for multidimensional range queries over real\nattributes. The VLDB Journal 14, 2 (2005), 137â€“154.\n[16] Max Halford, Philippe Saint-Pierre, and Franck Morvan. 2019. An approach based\non bayesian networks for query selectivity estimation. DASFAA 2 (2019).\n[17] Shohedul Hasan, Saravanan Thirumuruganathan, Jees Augustine, Nick Koudas,\nand Gautam Das. 2019. Multi-attribute selectivity estimation using deep learning.\nInSIGMOD .\n[18] Max Heimel, Martin Kiefer, and Volker Markl. 2015. Self-tuning, gpu-accelerated\nkernel density models for multidimensional selectivity estimation. In SIGMOD .\n1477â€“1492.\n[19] Benjamin Hilprecht. 2019. Github repository: deepdb public.\nhttps://github.com/DataManagementLab/deepdb-public (2019).\n[20] Benjamin Hilprecht, Andreas Schmidt, Moritz Kulessa, Alejandro Molina, Kristian\nKersting, and Carsten Binnig. 2019. DeepDB: learn from data, not from queries!.\nInPVLDB .\n[21] Yannis E Ioannidis and Stavros Christodoulakis. 1991. On the propagation of\nerrors in the size of join results. In SIGMOD . 268â€“277.\n[22] Andranik Khachatryan, Emmanuel MÃ¼ller, Christian Stier, and Klemens BÃ¶hm.\n2015. Improving accuracy and robustness of self-tuning histograms by subspace\nclustering. IEEE TKDE 27, 9 (2015), 2377â€“2389.\n[23] Martin Kiefer, Max Heimel, Sebastian BreÃŸ, and Volker Markl. 2017. Estimating\njoin selectivities using bandwidth-optimized kernel density models. PVLDB 10,\n13 (2017), 2085â€“2096.\n[24] Andreas Kipf, Thomas Kipf, Bernhard Radke, Viktor Leis, Peter Boncz, and\nAlfons Kemper. 2019. Learned cardinalities: Estimating correlated joins with\ndeep learning. In CIDR .\n[25] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In SIGMOD . 489â€“504.\n[26] K Krishna and M Narasimha Murty. 1999. Genetic K-means algorithm. IEEE\nTransactions on Systems, Man, and Cybernetics, Part B (Cybernetics) 29, 3 (1999),\n433â€“439.\n[27] Sanjay Krishnan, Zongheng Yang, Ken Goldberg, Joseph Hellerstein, and Ion\nStoica. 2018. Learning to optimize join queries with deep reinforcement learning.\narXiv preprint arXiv:1808.03196 (2018).\n[28] Viktor Leis, Andrey Gubichev, Atanas Mirchev, Peter Boncz, Alfons Kemper, and\nThomas Neumann. 2015. How good are query optimizers, really? PVLDB 9, 3\n(2015), 204â€“215.\n[29] Viktor Leis, Bernhard Radke, Andrey Gubichev, Alfons Kemper, and Thomas\nNeumann. 2017. Cardinality Estimation Done Right: Index-Based Join Sampling.\nInCIDR .[30] Viktor Leis, Bernhard Radke, Andrey Gubichev, Atanas Mirchev, Peter Boncz,\nAlfons Kemper, and Thomas Neumann. 2018. Query optimization through the\nlooking glass, and what we found running the Join Order Benchmark. The VLDB\nJournal 27, 5 (2018), 643â€“668.\n[31] Eric Liang, Zongheng Yang, Ion Stoica, Pieter Abbeel, Yan Duan, and Peter Chen.\n2020. Variable Skipping for Autoregressive Range Density Estimation. In ICML .\n6040â€“6049.\n[32] Henry Liu, Mingbin Xu, Ziting Yu, Vincent Corvinelli, and Calisto Zuzarte. 2015.\nCardinality estimation using neural networks. In Proceedings of the 25th Annual\nInternational Conference on Computer Science and Software Engineering . 53â€“59.\n[33] Luch Liu. 2020. Github repository: scikit-learn. https://github.com/scikit-\nlearn/scikit-learn (2020).\n[34] Pedro Lopes, Craig Guyer, and Milener Gene. 2019. Sql docs: cardinality\nestimation (SQL Server). https://docs.microsoft.com/en-us/sql/relational-\ndatabases/performance/cardinality-estimation-sql-server?view=sql-server-ver15\n(2019).\n[35] David Lopez-Paz, Philipp Hennig, and Bernhard SchÃ¶lkopf. 2013. The randomized\ndependence coefficient. In NIPS . 1â€“9.\n[36] Frank Luan, Amog Kamsetty, Eric Liang, and Zongheng Yang. 2020. Github\nrepository: neurocard project. https://github.com/neurocard/neurocard (2020).\n[37] Hongzi Mao, Malte Schwarzkopf, Shaileshh Bojja Venkatakrishnan, Zili Meng,\nand Mohammad Alizadeh. 2019. Learning scheduling algorithms for data pro-\ncessing clusters. In SIGCOMM . 270â€“288.\n[38] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh,\nTim Kraska, Olga Papaemmanouil, and Nesime Tatbul. 2019. Neo: A learned\nquery optimizer. arXiv preprint arXiv:1904.03711 (2019).\n[39] James Martens and Venkatesh Medabalimi. 2014. On the expressive efficiency of\nsum product networks. arXiv:1411.7717 (2014).\n[40] Victor E McZgee and Willard T Carleton. 1970. Piecewise regression. J. Amer.\nStatist. Assoc. 65, 331 (1970), 1109â€“1124.\n[41] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning Multi-dimensional Indexes. In SIGMOD . 985â€“1000.\n[42] State of New York. 2020. Vehicle, snowmobile, and boat registrations.\nhttps://catalog.data.gov/dataset/vehicle-snowmobile-and-boat-registrations (2020).\n[43] Yongjoo Park, Ahmad Shahab Tajik, Michael Cafarella, and Barzan Mozafari.\n2017. Database learning: Toward a database that becomes smarter every time. In\nSIGMOD . 587â€“602.\n[44] Matthew Perron, Zeyuan Shang, Tim Kraska, and Michael Stonebraker. 2019.\nHow I learned to stop worrying and love re-optimization. In ICDE . 1758â€“1761.\n[45] Hoifung Poon and Pedro Domingos. 2011. Sum-product networks: A new deep\narchitecture. In ICCV Workshops . 689â€“690.\n[46] Viswanath Poosala and Yannis E Ioannidis. 1997. Selectivity estimation without\nthe attribute value independence assumption. In VLDB , Vol. 97. 486â€“495.\n[47] Carl Edward Rasmussen. 2000. The infinite Gaussian mixture model. In NIPS .\n554â€“560.\n[48] MySQL 8.0 Reference Manual. 2020. Chapter 15.8.10.2 Configuring Non-Persistent\nOptimizer Statistics Parameters. https://dev.mysql.com/doc/refman/8.0/en/innodb-\nstatistics-estimation.html (2020).\n[49] UCI ML Repository. 2020. Gas sensor array temperature modulation Data Set.\nhttps://archive.ics.uci.edu/ml/datasets/Gas+sensor+array+temperature+modulation\n(2020).\n[50] Mauro Scanagatta, Antonio SalmerÃ³n, and Fabio Stella. 2019. A survey on\nBayesian network structure learning from data. Progress in Artificial Intelligence\n(2019), 1â€“15.\n[51] P Griffiths Selinger, Morton M Astrahan, Donald D Chamberlin, Raymond A\nLorie, and Thomas G Price. 1979. Access path selection in a relational database\nmanagement system. In SIGMOD . 23â€“34.\n[52] MariaDB Server Documentation. 2020. Statistics for optimizing queries: InnoDB\npersistent statistics. https://mariadb.com/kb/en/innodb-persistent-statistics/ (2020).\n[53] Raghav Sethi, Martin Traverso, Dain Sundstrom, David Phillips, Wenlei Xie,\nYutian Sun, Nezih Yegitbasi, Haozhun Jin, Eric Hwang, Nileema Shingte, et al .\n2019. Presto: Sql on everything. In ICDE . 1802â€“1813.\n[54] Utkarsh Srivastava, Peter J Haas, Volker Markl, Marcel Kutsch, and Tam Minh\nTran. 2006. Isomer: Consistent histogram construction using query feedback. In\nICDE . 39â€“39.\n[55] Michael Stillger, Guy M Lohman, Volker Markl, and Mokhtar Kandil. 2001. LEO-\nDB2â€™s learning optimizer. In PVLDB , Vol. 1. 19â€“28.\n[56] Saravanan Thirumuruganathan, Shohedul Hasan, Nick Koudas, and Gautam Das.\n2020. Approximate query processing for data exploration using deep generative\nmodels. In ICDE . 1309â€“1320.\n[57] Kostas Tzoumas, Amol Deshpande, and Christian S Jensen. 2011. Lightweight\ngraphical models for selectivity estimation without independence assumptions.\nPVLDB 4, 11 (2011), 852â€“863.\n[58] Dana Van Aken, Andrew Pavlo, Geoffrey J Gordon, and Bohan Zhang. 2017.\nAutomatic database management system tuning through large-scale machine\nlearning. In SIGMOD . 1009â€“1024.\n[59] Hai Wang and Kenneth C Sevcik. 2003. A multi-dimensional histogram for\nselectivity estimation and fast approximate query answering. In Proceedings of\n\nthe 2003 conference of the Centre for Advanced Studies on Collaborative research .\n328â€“342.\n[60] Xiaoying Wang, Changbo Qu, Weiyuan Wu, Jiannan Wang, and Qingqing Zhou.\n2020. Are we ready for learned cardinality estimation? arXiv:2012.06743 [cs.DB]\n[61] Chenggang Wu, Alekh Jindal, Saeed Amizadeh, Hiren Patel, Wangchao Le, Shi\nQiao, and Sriram Rao. 2018. Towards a learning optimizer for shared clouds.\nPVLDB 12, 3 (2018), 210â€“222.\n[62] Zongheng Yang, Amog Kamsetty, Sifei Luan, Eric Liang, Yan Duan, Xi Chen, and\nIon Stoica. 2021. NeuroCard: One Cardinality Estimator for All Tables. PVLDB\n14, 1 (2021), 61â€“73.\n[63] Zongheng Yang, Eric Liang, Amog Kamsetty, Chenggang Wu, Yan Duan, Xi Chen,\nPieter Abbeel, Joseph M Hellerstein, Sanjay Krishnan, and Ion Stoica. 2019. Deep\nunsupervised cardinality estimation. PVLDB (2019).\n[64] Zongheng Yang and Chenggang Wu. 2019. Github repository: naru project.\nhttps://github.com/naru-project/naru (2019).\n[65] Zhuoyue Zhao, Robert Christensen, Feifei Li, Xiao Hu, and Ke Yi. 2018. Random\nsampling over joins revisited. In SIGMOD . 1525â€“1539.\n[66] Rong Zhu, Ziniu Wu, Yuxing Han, Kai Zeng, Andreas Pfadler, Zhengping Qian,\nJingren Zhou, and Bin Cui. 2020. FLAT: Fast, Lightweight and Accurate Method\nfor Cardinality Estimation [Technical Report]. arXiv preprint arXiv:2011.09022\n(2020).\n\nAPPENDIX\nA RELATIONS BETWEEN FSPN AND OTHER\nMODELS\nWe present here the details on how FSPN subsumes SPN, as well\nasBNmodels. We assume that all attributes are discrete, i.e., for\ncontinuous attributes, we can discretize them by binning, and all\n(conditional) PDFs are stored in a tabular form.\nA.1 Transforming to FSPN\nWe show the details on how to transform SPN andBNto the equiv-\nalent FSPN model.\nTransforming from SPN toFSPN .Given a data table ğ‘‡with at-\ntributesğ´, ifPrt(ğ´)could be represented by an SPN S, we can\neasily construct an FSPN Fthat equally represent Prt(ğ´). Specif-\nically, we disable the factorize operation in FSPN by setting the\nfactorization threshold to âˆ, and then follow the same steps of S\nto constructF. Then, the FSPNFis exactly the same of S.\nTransforming from BNtoFSPN .Given a data table ğ‘‡with at-\ntributesğ´, ifPrt(ğ´)could be represented by a discrete BNB, we\ncan also build an FSPNFthat equally represent Prt(ğ´). Without\nambiguity, we also use Bto refer to its DAG structure. We present\nthe procedures in the BN-to-FSPN algorithm. It takes as inputs a\ndiscrete BNBand the root node ğ‘ofFand outputs ğ¹nrepresent-\ning the same PDF of B. In general BN-to-FSPN works in a recursive\nmanner by executing the following steps:\n1â—‹(lines 1â€“3) IfBcontains more than one connected compo-\nnentB1,B2,...,Bğ‘¡, the variables in each are mutually indepen-\ndent. Therefore, we set ğ‘to be a product node with children\nğ‘1,ğ‘2,...,ğ‘ğ‘¡intoF, and call BN-to-FSPN onBğ‘–and nodeğ‘ğ‘–\nfor eachğ‘–.\n2â—‹(lines 5â€“7) IfBcontains only one connected component, let\nğ´ğ‘–be a node (variable) in Bthat has no out-neighbor. If ğ´ğ‘–also has\nno in-neighbor (parent) in B, it maintains the PDF Prt(ğ´ğ‘–). At that\ntime, we set ğ‘to be a uni-leaf node representing the univariate\ndistribution Prt(ğ´ğ‘–).\n3â—‹(lines 9â€“16) If the parent set ğ´pa(ğ‘–)ofğ´ğ‘–is not empty, ğ´ğ‘–\nhas a conditional probability table (CPT) defining Prt(ğ´ğ‘–|ğ´pa(ğ‘–))=\nPrt(ğ´ğ‘–|ğ´\\{ğ´ğ‘–}). At this time, we set ğ‘to be a factorize node\nwith the left child representing Prt(ğ´\\{ğ´ğ‘–})and right child ğ‘r\nrepresenting Prt(ğ´ğ‘–|ğ´\\{ğ´ğ‘–}). For the right child ğ‘r, we set it\nto be a split node. For each entry ğ‘¦ofğ´pa(ğ‘–)in the CPT of ğ´ğ‘–,\nwe add a multi-leaf nodeğ¿ğ‘¦ofğ‘rcontaining all data ğ‘‡ğ‘¦inğ‘‡\nwhose value on ğ´pa(ğ‘–)equalsğ‘¦. On each leaf ğ¿ğ‘¦, by the first-order\nMarkov property of BN, ğ´ğ‘–is conditionally independent of variables\nğ´\\{ğ´ğ‘–}\\ğ´pa(ğ‘–)given its parents ğ´pa(ğ‘–). Therefore, we can simplify\nthe PDF represented by ğ¿ğ‘¦asPrt(ğ´ğ‘–|ğ‘¦)=Prtğ‘¦(ğ´ğ‘–). Therefore, ğ‘r\ncharacterizes the CPT of Prt(ğ´ğ‘–|ğ´pa(ğ‘–))=Prt(ğ´ğ‘–|ğ´\\{ğ´ğ‘–}). Later,\nwe remove the node ğ´ğ‘–fromBto obtainBâ€², which represents the\nPDF Prt(ğ´\\{ğ´ğ‘–}). We call BN-to-FSPN onBâ€²and nodeğ‘l, the\nleft child of ğ‘to further model the PDF.\nFinally, we obtain the FSPN Frepresenting the same PDF of B.Algorithm BN-to-FSPN(B,ğ‘)\n1:ifBcontains connected components B1,B2,...,Bğ‘¡then\n2: setğ‘to be a product node with children ğ‘1,ğ‘2,...,ğ‘ğ‘¡\n3: callBN-to-FSPN(Bğ‘–,ğ‘ğ‘–)for eachğ‘–\n4:else\n5: letğ´ğ‘–be a node inBcontaining no out-neighbor\n6: ifğ´ğ‘–has no in-neighbor in Bthen\n7: setğ‘to be a uni-leaf node representing Prt(ğ´ğ‘–)\n8: else\n9: setğ‘to be a factorize node with left child ğ‘land right child ğ‘r\n10: setğ‘rto be a split node\n11: foreach valueğ‘¦ofğ´pa(ğ‘–)in the CPT of ğ´ğ‘–do\n12: add a multi-leaf node ğ‘ğ‘¦as child ofğ‘r\n13: letğ‘‡ğ‘¦â†{ğ‘¡âˆˆğ‘‡|ğ´pa(ğ‘–)ofğ‘¡isğ‘¦}\n14: letğ‘ğ‘¦represent Prtğ‘¦(ğ´ğ‘–)\n15: removeğ´ğ‘–fromBto obtainBâ€²\n16: callBN-to-FSPN(Bâ€²,ğ‘l)\nA.2 Proof of Lemma 1\nLemma 1 Given a table ğ‘‡with attributes ğ´, if the joint PDF Prt(ğ´)\nis represented by an SPNSor aBNBwith space cost ğ‘‚(ğ‘€), then\nthere exists an FSPNFthat can equivalently model Prt(ğ´)with no\nmore thanğ‘‚(ğ‘€)space.\nProof. For the SPNSand the FLATF, by Section A.1, their\nstructures are exactly the same. In the simplest case, if Frepresents\nthe distribution in the same way as Son each leaf nodes, their\nspace cost is the same.\nFor the BNBand the FLATF, we analyze their space cost. The\nstorage cost of each node ğ´ğ‘–inBis the number of entries in CPT\nofPrt(ğ´ğ‘–|ğ´pa(ğ‘–)). The FSPNFrepresents Prt(ğ´ğ‘–)in step 2â—‹of\nalgorithm BN-to-FSPN whenğ´pa(ğ‘–)is empty and Prt(ğ´ğ‘–|ğ‘¦)for\neach valueğ‘¦ofğ´pa(ğ‘–)in step 3â—‹. In the simplest case, if Falso\nrepresents the distribution in a tabular form, the storage cost is the\nsame asB. Therefore, the model size of Fcan not be larger than\nthat ofB.\nTherefore, this lemma holds. â–¡\nB DETAILED UPDATING ALGORITHM\nWe put the pseudocodes of our incremental updating algorithms.\nB.1 FLAT-Update Algorithm\nTheFLAT-Update algorithm described in Section 4.3 is used for\nupdating FSPN built on single table. It works in a recursive manner\nand traverse the original FSPN in a top-down manner. FLAT-Update\ntries to preserve the original FSPN structure to the maximum extent\nwhile fine-tuning its parameters for better fitting. The process to\neach type of nodes have been explained clearly in Section 4.3.\nB.2 FLAT-Update-Multi Algorithm\nTheFLAT-Update-Multi algorithm described in Technique III of\nSection 5 is used to update the FSPN models built on multi-tables.\nWe first identify the impact of updating Î”ğ¶ontoT, and then update\ntheFSPN modeling attribute columns and scattering coefficients\ncolumns, respectively. The details are clearly presented in Section 5.\n\nAlgorithm FLAT-Update(F,Î”ğ‘‡)\n1:letğ‘be the root node of F\n2:ifğ‘isfactorize node then\n3: callFLAT-Update(Fnğ‘–,Î”ğ‘‡)for each child ğ‘ğ‘–ofğ‘\n4:else ifğ‘issplit node then\n5: foreach childğ‘ğ‘–ofğ‘do\n6: letÎ”ğ‘‡ğ‘–be the set of records in the splitting condition of ğ‘ğ‘–\n7: callFLAT-Update(Fnğ‘–,Î”ğ‘‡ğ‘–)\n8:else ifğ‘ismulti-leaf node then\n9: ifthe conditional indepence still holds on ğ‘‡nthen\n10: update the parameters of the PDF by Î”ğ‘‡\n11: else\n12: resetğ‘to be a split node\n13: call lines 28â€“30 of FLAT-Offline( ğ´n,ğ¶n,ğ‘‡n)\n14:else ifğ‘issum node then\n15: foreach childğ‘ğ‘–ofğ‘do\n16: letÎ”ğ‘‡ğ‘–be the set of records assigned to or removed from ğ‘ğ‘–\n17: update the weights of ğ‘ğ‘–accordingly\n18: callFLAT-Update(Fnğ‘–,Î”ğ‘‡ğ‘–)\n19:else ifğ‘isproduct node then\n20: ifthe indepence between attributes still holds on ğ‘‡nthen\n21: callFLAT-Update(Fnğ‘–,Î”ğ‘‡)for each child ğ‘ğ‘–ofğ‘\n22: else\n23: call lines 12â€“19 of FLAT-Offline( ğ´n,ğ¶n,ğ‘‡n)\n24:else ifğ‘isuni-leaf node then\n25: update the parameters of the PDF by Î”ğ‘‡\nAlgorithm FLAT-Update-Multi (Ft,Î”ğ¶)\n1:ifÎ”ğ¶is inserted into table ğ¶then\n2: obtain Î”T+with all new records joining with Î”ğ¶\n3: obtain Î”Tâˆ’with all records with nullattributes of table ğ¶but can\njoin with Î”ğ¶\n4: obtain Î”Tâˆ—with all original records can join with Î”ğ¶\n5:else\n6: obtain Î”T+with all new records joining with nullattributes of table\nğ¶\n7: obtain Î”Tâˆ’with all removed tuples joining with Î”ğ¶\n8: obtain Î”Tâˆ—with all original records can join with Î”ğ¶\n9:letğ‘be the root node of Fwith left child ğ¿ğ¶and right child ğ‘…ğ¶\n10:callFLAT-Update(Flc,Î”T+)\n11:callFLAT-Update(Flc,Î”Tâˆ’)\n12:foreach multi-leaf nodeğ¿ofğ‘…ğ¶do\n13: incrementally update all expected values based on the scattering\ncolumns in Î”T+,Î”Tâˆ’andÎ”Tâˆ—\n14:ifreaches the periodical updating time then\n15: recompute all scattering columns ğ‘†t,efor all nodes ğ‘‡\n16: ifğ‘†t,echanges then\n17: incrementally update all expected values in all multi-leaf nodes\nC PROOF OF LEMMA 2\nLemma 2 Given a query ğ‘„, letğ¸={ğ‘‡1,ğ‘‡2,...,ğ‘‡ğ‘‘}denote all\nnodes inğ½touched byğ‘„. On each node ğ‘‡ğ‘–, letğ‘†={ğ‘†a1,b1,ğ‘†a2,b2,...,\nğ‘†ağ‘›,bğ‘›}, where each(ğ´ğ‘—,ğµğ‘—)is a distinct join such that ğµğ‘—is not inğ‘„.\nLetğ‘ =(ğ‘ 1,ğ‘ 2,...,ğ‘ ğ‘›)whereğ‘†ağ‘—,bğ‘—=ğ‘ ğ‘—âˆˆNfor all 1â‰¤ğ‘–â‰¤ğ‘›denote\nan assignment to ğ‘†and dlm(ğ‘ )=Ãğ‘›\nğ‘—=1max{ğ‘ ğ‘—,1}. Let\nğ‘ğ‘–=|Tğ‘–|\n|E|Â·âˆ‘ï¸\nğ‘ ,ğ‘’\u0012\nPrTğ‘–(ğ‘„ğ‘–âˆ§ğ‘†=ğ‘ âˆ§ğ‘†tğ‘–,e=ğ‘’)Â·max{ğ‘’,1}\ndlm(ğ‘ )\u0013\n. (1)Then, the cardinality of ğ‘„is|E|Â·Ãğ‘‘\nğ‘–=1ğ‘ğ‘–.\nProof. Given the query ğ‘„, letğ‘denote all the tables touched\nbyğ‘„andğ‘ğ‘–be the tables touched by ğ‘„in the node ğ‘‡ğ‘–. Obviously,\nwe can obtain the cardinality of ğ‘„on tableZasCard(Z,ğ‘„)=\nPrZ(ğ‘„)Â·|Z| .\nFirst, we have ğ‘âŠ†ğ‘‡=âˆªğ‘‘\nğ‘–=1ğ‘‡ğ‘–. LetE=T1âŠ²âŠ³T2âŠ²âŠ³Â·Â·Â·âŠ²âŠ³Tğ‘‘\ndenote the full outer join table over all nodes in ğ¸. We show how to\nobtain the cardinality of ğ‘„on tableE. For any single table ğ´âˆˆğ‘‡\\ğ‘,\nsuppose that we have the scattering coefficient column ğ‘†a,einE.\nğ‘†a,edenotes the scattering number from each record in ğ´toE.\nLetğ‘†e={ğ‘†a1,e,ğ‘†a2,e,...,ğ‘† ağ‘˜,e}be a collection of columns for any\nğ´ğ‘—âˆˆğ‘‡. Letğ‘ e=(ğ‘ a1,e,ğ‘ a2,e,...,ğ‘  ağ‘˜,e), whereğ‘ ağ‘—,eâˆˆNfor all\n1â‰¤ğ‘—â‰¤ğ‘˜, be an assignment to ğ‘†e. By [ 20,62], we can down-scale\nEby removing the effects of untouched tables ğ‘‡\\ğ‘to obtain the\ncardinality of ğ‘„. We have\nCard(Z,ğ‘„)=PrZ(ğ‘„)Â·|Z|\n= âˆ‘ï¸\nğ‘ ePrE(ğ‘„âˆ§ğ‘†e=ğ‘ e)\ndlm(ğ‘ e)!\nÂ·|E|,(2)\nwhere dlm(ğ‘ e)=Ã\nğ‘ âˆˆğ‘ emax{ğ‘ ,1}. Here, PrE(ğ‘„andğ‘†e=ğ‘ e)Â·|E|\nimplies the number of records in Esatisfying the predicate specified\ninğ‘„but scattered dlm(ğ‘ e)more times by other tables in ğ‘‡\\ğ‘.\nTherefore, we eliminate the scattering effects by dividing dlm(ğ‘ e).\nWe set each ğ‘ âˆˆğ‘ eto1when it is 0since records with zero scattering\ncoefficient, i.e., having no matching, also occur once in E. Since\nwe do not explicitly maintain the full outer join table Eand all\ncolumnsğ‘†e, we need to further simplify Eq. (2) as follows.\nSecond, for each node ğ‘‡ğ‘–âˆˆğ¸, letğ´ğ‘–=ğ‘‡ğ‘–\\ğ‘denote all tables in\nnodeğ‘‡ğ‘–untouched by ğ‘„. Letğ‘†eğ‘–={ğ‘†aâ€²,e|ğ´â€²âˆˆğ´ğ‘–}denote all the\nscattering columns from single table ğ´â€²âˆˆğ´ğ‘–toEinğ‘†e. Similarly,\nletğ‘ eğ‘–be the assignment of ğ‘†eğ‘–. We could rewrite Eq. (2) as\nCard(Z,ğ‘„)=Â©Â­\nÂ«âˆ‘ï¸\nğ‘ e1,ğ‘ e2,...,ğ‘  eğ‘‘PrE(ğ‘„âˆ§ğ‘†e1=ğ‘ e1âˆ§Â·Â·Â·âˆ§ğ‘†eğ‘‘=ğ‘ eğ‘‘Ãğ‘‘\nğ‘–=1dlm(ğ‘ eğ‘–)ÂªÂ®\nÂ¬Â·|E|\n=Â©Â­\nÂ«âˆ‘ï¸\nğ‘ e1,ğ‘ e2,...,ğ‘  eğ‘‘PrE(ğ‘„1âˆ§ğ‘†e1=ğ‘ e1âˆ§Â·Â·Â·âˆ§ğ‘„ğ‘‘âˆ§ğ‘†eğ‘‘=ğ‘ eğ‘‘Ãğ‘‘\nğ‘–=1dlm(ğ‘ eğ‘–)ÂªÂ®\nÂ¬Â·|E|.\n(3)\nBy our assumption on the join tree, the probability on different\nnodes are independent on their full outer join table, so we derive\nCard(Z,ğ‘„)=Â©Â­\nÂ«âˆ‘ï¸\nğ‘ e1,ğ‘ e2,...,ğ‘  eğ‘‘ ğ‘‘Ã–\nğ‘–=1PrE(ğ‘„ğ‘–âˆ§ğ‘†eğ‘–=ğ‘ eğ‘–)\ndlm(ğ‘ eğ‘–)!\nÂªÂ®\nÂ¬Â·|E|\n=Â©Â­\nÂ«ğ‘‘Ã–\nğ‘–=1Â©Â­\nÂ«âˆ‘ï¸\nğ‘ eğ‘–PrE(ğ‘„ğ‘–âˆ§ğ‘†eğ‘–=ğ‘ eğ‘–)\ndlm(ğ‘ eğ‘–)ÂªÂ®\nÂ¬ÂªÂ®\nÂ¬Â·|E|.(4)\nThird, based on Eq. (4), we could compute each term in the\nproduct using PDF maintained in each local node ğ‘‡ğ‘–. As stated in\nthe Lemma, for each node ğ‘‡ğ‘–, we have scattering columns ğ‘†=\n{ğ‘†a1,b1,ğ‘†a2,b2,...,ğ‘† ağ‘›,bğ‘›}, where each(ğ´ğ‘—,ğµğ‘—)is a distinct join\nwhereğµğ‘—is not inğ‘„. For each assignment ğ‘ =(ğ‘ 1,ğ‘ 2,...,ğ‘ ğ‘›)of\n\nğ‘†,ğ‘’âˆˆNofğ‘†tğ‘–,{e}andğ‘ Â·ğ‘’=(ğ‘ 1Â·ğ‘’,ğ‘ 2Â·ğ‘’,...ğ‘ ğ‘›Â·ğ‘’), we have\nPrTğ‘–(ğ‘„ğ‘–âˆ§ğ‘†=ğ‘ âˆ§ğ‘†tğ‘–,e=ğ‘’)\ndlm(ğ‘ )Â·max{ğ‘’,1}Â·|Tğ‘–|\n=PrE(ğ‘„ğ‘–âˆ§ğ‘†eğ‘–=ğ‘ Â·ğ‘’)\ndlm(ğ‘ Â·ğ‘’)Â·|E|.(5)\nWe could interpret Eq. (5)as follows. Let ğ‘ğ‘–=ğ‘‡ğ‘–âˆ©ğ‘be all tables\nbeing queried in node ğ‘‡ğ‘–. For the left hand side, we have\nPrZğ‘–(ğ‘„ğ‘–)=PrTğ‘–(ğ‘„ğ‘–âˆ§ğ‘†=ğ‘ )\ndlm(ğ‘ )Â·|Tğ‘–|,\nwhich corrects the probability from Tğ‘–toZğ‘–. Next, we should up-\nscaleZğ‘–to the full outer join table of ğ‘ğ‘–âˆª(ğ¸\\{ğ‘‡ğ‘–})=ğ¸\\(ğ‘‡ğ‘–\\ğ‘ğ‘–),\ni.e., all tables in ğ¸excluding untouched tables of ğ‘„inğ‘‡ğ‘–. Therefore,\nwe multiplyPrTğ‘–(ğ‘„ğ‘–âˆ§ğ‘†=ğ‘ âˆ§ğ‘†tğ‘–,e=ğ‘’)\ndlm(ğ‘ )Â·|Tğ‘–|by a factor of max{ğ‘’,1}.\nFor the right hand side,PrE(ğ‘„ğ‘–âˆ§ğ‘†eğ‘–=ğ‘ Â·ğ‘’)\ndlm(ğ‘ Â·ğ‘’)Â·|E| also down-scales\nthe probability from Eto the full outer join table of ğ¸\\(ğ‘‡ğ‘–\\ğ‘ğ‘–), so\nit is equivalent to the left hand side.\nBased on this, we have\nğ‘ğ‘–=|Tğ‘–|\n|E|Â·âˆ‘ï¸\nğ‘ ,ğ‘’\u0012\nPrTğ‘–(ğ‘„ğ‘–âˆ§ğ‘†=ğ‘ âˆ§ğ‘†tğ‘–,e=ğ‘’)Â·max{ğ‘’,1}\ndlm(ğ‘ )\u0013\n=âˆ‘ï¸\nğ‘ Â·ğ‘’PrE(ğ‘„ğ‘–âˆ§ğ‘†eğ‘–=ğ‘ Â·ğ‘’)\ndlm(ğ‘ Â·ğ‘’)\n=âˆ‘ï¸\nğ‘ eğ‘–PrE(ğ‘„ğ‘–âˆ§ğ‘†eğ‘–=ğ‘ eğ‘–)\ndlm(ğ‘ eğ‘–).\nThus, the cardinality of ğ‘„can be represented as\nCard(Z,ğ‘„)=|E|Â·ğ‘‘Ã–\nğ‘–=1ğ‘ğ‘–\nand the lemma holds. â–¡\nD SENSITIVITY ANALYSIS OF\nHYPER-PARAMETERS\nWe analyze the sensitivity of hyper-parameters of our FLAT\nmethod. The hyper-parameters include:\n(1)the RDC [ 35] threshold for deciding if two attributes are\nindependent, denoted as ğœğ‘™;\n(2)the RDC threshold for deciding if two attributes are highly\ncorrelated, denoted as ğœâ„;\n(3)the number of intervals for ğ‘‘-way partitioning in split node;\n(4)the minimum amount of input data to stop further slitting\nthe data on a node, denoted as ğ‘;\n(5)the number of bins of histograms in uni-leaf andmulti-leaf\nnodes, denoted as ğ‘;\n(6)the number of pieces for piecewise regression in multi-leaf\nnodes, denoted as ğ‘;\n(7)the number of samples from the full outer join table to train\nthe model, denoted as ğ‘›.\nAll these hyper-parameters represent trade-offs between estima-\ntion accuracy and learning and/or inference efficiency. We provide\nthe detailed analysis for each of them qualitatively as follows:(1)Smallerğœğ‘™would represent stricter criteria for discovering\nindependent attributes. Thus, the model is more accurate\nbut a large number of data-splitting operations ( sum and\nsplit nodes) will be created, increasing the model size and\ndecreasing the learning and/or inference efficiency.\n(2)With larger ğœâ„, the discovered highly correlated attributes\nare more inter-dependent and their values are easier to model\nwith piecewise regression, leading to more compact right\nbranch of factorize nodes in FSPN . However, there might\nstill exist some undiscovered highly correlated attributes\nthat are not factorized, leading to a less compact left branch\noffactorize nodes in FSPN .\n(3)A split node with larger ğ‘‘is more likely to break the at-\ntributes correlation. Therefore, with larger ğ‘‘, the model is\npotentially more compact and accurate at the same time.\nHowever, the training process would take much longer for\ncreating each split node.\n(4)Largerğ‘can prevent FSPN from creating a long chain of\nsum orsplit nodes, leading to more compact model with\nfaster inference latency. However, the attributes might not\nbe independent when a uni-leaf node is created, leading to\nless accurate estimation.\n(5)Intuitively, larger ğ‘will help the (multi-)histograms capture\nthe data distribution more accurately but will be more time-\nconsuming and less space-efficient.\n(6)Analogously, larger ğ‘will also help the multi-leaf nodes\ncapture the data distribution of multiple correlated attributes\nmore accurately but will be more time-consuming and less\nspace-efficient.\n(7)A larger sample ğ‘›will closely represent the actually table,\nprevent the model from overfitting, thus more accurate es-\ntimation. However, gathering the large number of sampled\nrecords might be time-consuming and might not be afford-\nable in memory.\nOverall, unlike DNN-based models, each hyper-parameter in\nFSPN represents a trade-off, whose influence on the model is pre-\ndictable. The users can easily find a set of hyper-parameters that are\nsuitable for their datasets and affordable to the computing resources.\nE ADDITIONAL EVALUATION RESULTS\nWe provide some additional evaluation results in this section.\nE.1 Estimation Latency on GPUs\nIn all examined CardEst methods, MSCN andNaru provide im-\nplementations specifically optimized on GPUs. We examine their\nquery latency on a NVIDIA Tesla V100 SXM2 GPU with 64GB GPU\nmemory. The comparison results on the dataset GAS and DMV is\nreported in Table 5. We find that:\n1) Both MSCN andNaru can benefit a lot from GPU since the\ncomputation on their underlying DNNs can be significantly speed\nup.\n2) Even without GPU acceleration, our FLAT on CPUs still runs\nmuch faster than MSCN andNaru on GPUs. Specifically, it runs\n\nTable 5: Estimation latency on GPUs.\nAverageDataset Algorithm EnvironmentLatency (ms)\nGASMSCN CPU 3.4\nMSCN GPU 1.3\nNaru CPU 59.2\nNaru GPU 9.7\nFLAT CPU 0.5\nDMVMSCN CPU 3.8\nMSCN GPU 1.4\nNaru CPU 78.7\nNaru GPU 11.5\nFLAT CPU 0.2\n19Ã—and58Ã—faster than Naru using GPUs on GAS and DMV, re-\nspectively. This once again verifies the high estimation accuracy of\nourFLAT method.\nFor the multi-table join query case, the NeuroCard algorithm\nprovides a GPU implementation in the repository [ 36]. However,\nthey require a higher version of CUDA, which is not supported in\nour machine, so we do not compare with it.\nE.2 Performance Stability\nWe evaluate the stability of FLAT using our FSPN model in terms\nof the varied data criteria from four aspects: number of attributes,\ndata distribution, attribute correlation and domain size.\nWe generate the synthetic datasets using the similar approach\nin a recent benchmark study [ 60]. Specifically, suppose we would\nlike to generate a table ğ‘‡with attributes{ğ´1,ğ´2,...,ğ´ğ‘›}and106\ntuples, where ğ‘›denotes the number of attributes. We generate the\nfirst column for ğ´1using a Pareto distribution1, with a value ğ‘ \ncontrolling the distribution skewness ğ‘ and a value ğ‘‘representing\nthe domain size. For each of the rest attribute ğ´ğ‘–, we generate a\ncolumn based on a previous attribute ğ´ğ‘—whereğ‘—<ğ‘–, to control the\ncorrelation ğ‘. Specifically, for each tuple ğ‘¡=(ğ‘1,ğ‘2,...,ğ‘ğ‘›)inğ‘‡,\nwe setğ‘ğ‘–toğ‘ğ‘—with a probability of ğ‘, and setğ‘ğ‘–to a random value\ndrawn from the Pareto distribution with the probability of 1âˆ’ğ‘.\nOn each synthetic dataset, we generate a query workload with\n1,000queries using the same method in Section 6.1. We report the\n95%-quantile q-error, the average inference latency, model size and\ntraining time in Table 6 and Table 7. We observe that:\n1. Correlation ( ğ‘ ):the varied correlation has very mild impact\nonFLAT â€™s performance. This is because FLAT makes no inde-\npendence assumption and can adaptively model the joint PDF of\nattributes. However, as shown in previous study [ 60], increasing the\ndata correlation severely degrades the performance of Histogram\nandDeepDB .\n2. Distribution ( ğ‘ ):the varied distribution skewness has slight\nimpact on FLAT â€™s performance. This is because FLAT applies (multi-\n)histograms to represent distributions, which are robust against\ndistribution changes. However, as shown in previous study [ 60],\n1In our implementation, we use the Python library scipy.stats.pareto functionincreasing the Pareto distribution skewness severely degrades the\nperformance of Naru andSample .\n3. Domain Size ( ğ‘‘):the increase in the domain size degrades the\nperformance of FLAT . In fact, as shown in [ 60], the performance of\nallCardEst methods degrade with the growth of domain size. This\nis because increasing ğ‘‘may rapidly increase the data complexity as\nthere areğ‘‘ğ‘›possible values that a record can take. Fortunately, the\nperformance degrades of FLAT are still within a reasonable range.\n4. Number of attributes ( ğ‘›):similar to domain size, the in-\ncrease in the number of attributes also degrades the performance\nofFLAT . However, the performance of FLAT is still reasonable\nand affordable with tens of attributes. This is because increasing\nthe number of attributes also increases the data complexity expo-\nnentially. In fact, the curse of dimensionality is a long-standing\nand common problem for almost all ML tasks. We would consider\nincreasing the robustness of FLAT in the future work.\nIn summary, FLAT is very stable to data correlation and distribu-\ntion while relative robust to domain size. However, it is sensitive to\nthe number of attributes.\nE.3 Performance with Varied Number of Joins\nand Predicates\nIn Figure 9 we report the detailed q-error of NeuroCard ,DeepDB\nand our FLAT with different number of tables and predicates in\nqueries. Clearly, when increasing the number of predicates, the\nq-error of DeepDB significantly increases while the q-error of\nNeuroCard andFLAT does not change too much. When increasing\njoin size, the performance of DeepDB degrades significantly while\nthe performance of NeuroCard andFLAT is affected marginally.\nThis suggests that the joint PDF represented by FSPN inFLAT is\nmore precise and robust compared to the representation via SPN\ninDeepDB , so its performance is more stable. The key reasons is\nthat FSPN â€™s design choices overcome the drawbacks of SPN. It is\nable to model the joint PDF of attributes with different dependency\nlevels accordingly.\n4_24_34_44_54_65_25_35_45_55_65_76_26_36_46_56_66_7\n# of joined tables_# of filter predicates0246810121416Median q-errorFLAT\nDeepDB\nNeuroCard\nFigure 9: Q-error with different joining and predicate size.\nE.4 End-to-End Test on JOB-ours Workload\nWe also perform the end-to-end test on Postgres using the harder\nworkload JOB-ours . Since JOB-ours contains 1,500queries and the\nexecution time of some queries is extremely long, we randomly\nsample 50queries from this workload for testing. The overall results\nare reported in Table 8.\n\nTable 6: Performance stability of FLAT w.r.t. varied data distribution skewness and correlation.\nDistribution Skewness (s) Attribute Correlation (c)\nData Criteria c=0.4,d=100,n=10 s=1.0,d=100,n=10\ns=0 s=0.3 s=0.6 s=1.0 s=1.5 s=2.0 c=0 c=0.2 c=0.4 c=0.6 c=0.8 c=1.0\nAccuracy (95% q-error) 1.06 1.15 1.23 1.76 2.25 2.11 1.32 1.27 1.76 2.11 1.73 1.00\nInference Latency (ms) 0.6 0.9 0.6 0.5 1.5 1.7 0.1 0.7 0.5 4.1 17.8 0.2\nModel size (KB) 76 101 80 75 430 580 9.5 103 75 1201 1889 4.7\nTraining time (Sec) 91 93 127 142 240 253 5.5 133 244.2 629 1370 17.0\nTable 7: Performance stability of FLAT w.r.t. varied data domain size and number of attributes.\nDomain Size (d) Number of Attributes (n)\nData Criteria s=1.0,c=0.4,n=10 s=1.0,c=0.4,d=100\nd=10 d=100 d=500 d=1,000 d=5,000 d=10,000 n=2 n=5 n=10 n=20 n=50\nAccuracy (95% q-error) 1.08 1.76 1.35 1.17 27.6 44.0 1.02 1.09 1.76 12.4 255\nInference Latency (ms) 0.5 0.6 1.5 18.0 15.9 49.7 0.4 0.5 0.5 3.3 25.9\nModel size (KB) 16.1 75.3 310 2701 1980 5732 15.0 49.9 75.3 1780 6908\nTraining time (Sec) 15.5 142 198 2670 1535 9721 9.7 48.6 142 761 4017\nTable 8: End-to-end testing results on JOB-ours workload.\nAverage QueryItem AlgorithmTime (Sec)Improvement\nQuery Time Postgres 431.7 â€”\nExcluding NeuroCard 379.2 12.2%\nCardEst FLAT 383.3 11.2%\nLatency True Cardinality 373.1 13.6%\nEnd-to-End Postgres 432.3 â€”\nQuery NeuroCard 405.7 6.2%\nTime FLAT 386.9 10.5%We observe similar phenomenon on the JOB-ours workload.\nSpecifically, for the query time excluding the CardEst query latency,\nthe improvements of NeuroCard and our FLAT are close to the\nmethod using the true cardinality. For the end-to-end query time,\nthe improvement of our FLAT is more significant than NeuroCard .",
  "textLength": 107652
}