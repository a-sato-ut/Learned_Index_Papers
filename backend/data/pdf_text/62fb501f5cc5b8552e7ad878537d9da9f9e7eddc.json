{
  "paperId": "62fb501f5cc5b8552e7ad878537d9da9f9e7eddc",
  "title": "Similarity query processing for high-dimensional data",
  "pdfPath": "62fb501f5cc5b8552e7ad878537d9da9f9e7eddc.pdf",
  "text": "Similarity Query Processing for High-Dimensional Data\nJianbin Qin\nShenzhen Institute of Computing Sciences,\nShenzhen University\njqin@sics.ac.cnWei Wang\nUniversity of New South Wales\nweiw@cse.unsw.edu.au\nChuan Xiao\nOsaka University and Nagoya University\nchuanx@ist.osaka-u.ac.jpYing Zhang\nUniversity of Technology Sydney\nYing.Zhang@uts.edu.au\nABSTRACT\nSimilarity query processing has been an active research topic for\nseveral decades. It is an essential procedure in a wide range of\napplications. Recently, embedding and auto-encoding methods\nas well as pre-trained models have gained popularity. They\nbasically deal with high-dimensional data, and this trend brings\nnew opportunities and challenges to similarity query process-\ning for high-dimensional data. Meanwhile, new techniques\nhave emerged to tackle this long-standing problem theoreti-\ncally and empirically. In this tutorial, we summarize existing\nsolutions, especially recent advancements from both database\n(DB) and machine learning (ML) communities, and analyze\ntheir strengths and weaknesses. We review exact and approx-\nimate methods such as cover tree, locality sensitive hashing,\nproduct quantization, and proximity graphs. We also discuss\nthe selectivity estimation problem and show how researchers\nare bringing in state-of-the-art ML techniques to address the\nproblem. By highlighting the strong connections between DB\nand ML, we hope that this tutorial provides an impetus towards\nnew ML for DB solutions and vice versa.\nPVLDB Reference Format:\nJianbin Qin, Wei Wang, Chuan Xiao, and Ying Zhang. Similarity\nQuery Processing for High-Dimensional Data. PVLDB , 13(12):\n3437-3440, 2020.\nDOI: https://doi.org/10.14778/3415478.3415564\n1. INTRODUCTION\nSimilarity query processing is a fundamental and essential\nprocedure in applications of many domains, including databases\n(DB), machine learning (ML), multimedia, and computer vision.\nNumerous query processing algorithms have been proposed in\nthe last few decades to deal with various kinds of data types and\nsimilarity functions. With the proliferation of deep learning,\nespecially the prevalence of embedding, auto-encoders, and pre-\ntrained models, similarity query processing for high-dimensional\ndata become increasingly important. They bene\ft DB applica-\ntions such as entity matching [48] and concept linking [12] as\nThis work is licensed under the Creative Commons Attribution-\nNonCommercial-NoDerivatives 4.0 International License. To view a copy\nof this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. For\nany use beyond those covered by this license, obtain permission by emailing\ninfo@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 13, No. 12\nISSN 2150-8097.\nDOI: https://doi.org/10.14778/3415478.3415564well as ML applications such as multimedia retrieval [7] and\nadversarial machine learning [1]. In ML community, similarity\nqueries are also studied under the name of knearest neighbors\n(k-NN) queries. k-NN itself is also an instance-based learning\nmethod for classi\fcation and regression.\nOur tutorial discusses the importance of similarity query\nprocessing for high-dimensional data in a wide range of appli-\ncations. We summarize existing solutions, especially recent\nadvancements from both DB and ML communities, thereby\nhighlighting the interplay between modern DB and ML tech-\nnologies. We review technical challenges and various exact and\napproximate algorithms, including cover tree, locality sensitive\nhashing, product quantization, and proximity graphs. More-\nover, we discuss the selectivity estimation of similarity query\nprocessing for high-dimensional data, and show how researchers\nare bringing in state-of-the-art ML techniques to address this\nproblem. We expect that this tutorial will provide an impetus\ntowards new ML for DB solutions and vice versa.\nScope . This tutorial aims to provide a comprehensive review of\nsimilarity query processing methods for high-dimensional data.\nIn particular, we explain the reason why processing similarity\nqueries for high-dimensional data { in contrast to sets and strings\n(see [45, 73] for survey) which have been extensively studied by\nthe DB community { has become more important. We introduce\nvarious existing algorithms and analyze their strengths and\nweaknesses. We highlight the connections between DB and ML\nas well as their foci on this topic. Finally, we outline future\nresearch directions and open problems to be solved.\nIntended Length and Target Audience . This is a three-\nhour tutorial targeting researchers, developers, and practition-\ners interested in managing high-dimensional data and ML for\nDB topics. We assume that the target audience is generally fa-\nmiliar with basic DB and ML terms, but there is no requirement\nfor prior knowledge of speci\fc algorithms.\nRelated Tutorials in Recent Years . This will be the\n\frst time that the authors present a tutorial on similarity\nquery processing for high-dimensional data. To the best of\nour knowledge, two topically related tutorials were presented\nat recent data-centric research venues (WISE 2017 [55] and\nCIKM 2019 [40]). These two tutorials target sets and strings,\nrespectively, whose query processing methods are substantially\ndi\u000berent from those will be presented at this tutorial.\n2. TUTORIAL OUTLINE\nThis tutorial consists of \fve parts. The \frst part motivates\nthe need for similarity query processing on high-dimensional\ndata and introduces basic concepts. The second and third\n 3437\n\nparts delve into query processing algorithms. The fourth part\ncovers selectivity estimation algorithms. The \ffth part discusses\nmiscellaneous issues such as the use of similarity queries with\nrespect to the entire work\row of real applications, deployment\nin a distributed environment, as well as future directions and\nopen problems.\n2.1 Background and Preliminaries\nIn the introductory part of the tutorial, we \frst introduce\napplications and explains the increasing importance of simi-\nlarity query processing on high-dimensional data, as stated in\nSection 1. Then we describe basic concepts: (1) data models\nand the way of which we convert raw data (text, images, etc.)\nto high-dimensional data; (2) similarity/distance functions,\nmainly Hamming distance for binary vectors, Euclidean dis-\ntance, cosine similarity (angular distance), and inner product\nfor real-valued vectors; (3) query types, i.e., search and join\nqueries, or thresholded and k-NN queries, depending on the\ndimension of categorization; (4) a summary of the solutions\nthat will be elaborated in the rest of the tutorial.\n2.2 Exact Query Processing\nExact query processing methods aim to \fnd all the results\nthat satisfy the similarity constraint. Researchers are interested\nin this type of solutions as it does not pose any uncertainty\nto the pipelines that apply similarity query processing as a\ncomponent. It also simpli\fes empirical comparison as only speed\nand space consumptions are key evaluation criteria. Existing\nexact methods usually answer queries by looking up one or\nmore (overlapping or non-overlapping) regions in the original\nor a transformed space. Partitioning techniques are often\nemployed. These methods can be classi\fed into the following\nthree categories:\nTree-based Methods . These methods partition the database\nin a hierarchical manner. To process queries, triangle inequality\nis often used to determine the nodes to be traversed. Represen-\ntative methods are M-tree [11] and cover tree [5, 27].\nSpace Partitioning Methods . These methods partition\nthe original space and bound the overall distance using the\ndistance in each subspace. Some methods require a sequential\nscan of the database, e.g., the vector approximation \fle (VA-\n\fle) [67]. For fast retrieval, indexing methods were proposed to\ndeal with Hamming distance using the pigeonhole principle [51,\n54, 56, 57].\nDimensionality Reduction Methods . These methods\nproject objects to another space to reduce dimensionality. They\nare basically early attempts that deal with the disk-resident\ncase and aim at reducing disk I/O [3, 52, 9, 74, 28]. Most of\nthem transform the original space to a 1-dimensional space and\nutilize B+-trees for indexing.\n2.3 Approximate Query Processing\nIt is commonly believed that it is hard to compute the exact\nresults of queries with a sub-linear cost. Instead, computing\napproximate results is su\u000eciently useful for many practical\nproblems, and these solutions empirically achieve signi\fcantly\nhigher e\u000eciency and scalability than exact ones [37]. Approxi-\nmate methods either adopt a space-\frst (i.e., looking up regions\nin a space) or an object-\frst (i.e., looking up objects directly)\nstrategy to \fnd query results.\nLocality Sensitive Hashing . Locality sensitive hashing (LSH)\nis a data-independent space-\frst approach with probabilistic\nguarantees on the worst-case performance [26, 20, 15, 62]. Itrelies on a family of hash functions that maps objects to another\nspace such that similar objects are mapped to the same hash\ncodes with higher probability than dissimilar objects. Recent de-\nvelopment focuses on supporting various similarity measures [46,\n76] and space-e\u000ecient indexing [61, 25, 75].\nLearning to Hash . Learning to hash (L2H) is a data-\ndependent space-\frst approach that maps data to another\nspace by exploiting the data distribution. The main principle\nof most methods in this category is to preserve the similarity\ninformation within an appropriate neighborhood. Additional\nheuristics and optimizations are often added to further reduce\nthe information loss caused by the mapping or increase general-\nization to unseen data. According to the optimization objective\nto preserve similarity, L2H algorithms can be grouped into\npairwise-similarity persevering class [68, 22, 39], multiwise-\nsimilarity persevering class [64, 63], and implicitly-similarity\npersevering class [30, 31]. Recently, deep learning-based L2H\nmethods were proposed, in both supervised and unsupervised\nmanner [6, 41, 70, 59].\nSpace Partitioning Methods . This category is a space-\frst\napproach that divides the high-dimensional space into multiple\nregions. Partition is often carried out in a recursive way, so\nthe index is represented by a tree or a forest. Based on the\nway of partitioning, there are mainly three classes of methods:\nPivoting methods divide the objects based on the distance\nfrom the object to some (usually randomly chosen) pivots;\ne.g., VP-Tree [71] and ball tree [8]. Hyperplane partitioning\nmethods recursively divide the space by a hyperplane with a\nrandom direction (e.g. Annoy [4], random projection tree [14])\nor an axis-aligned separating hyperplane (e.g., randomized\nkd-trees [60, 49]). Compact partitioning methods either divide\nthe objects into clusters [18] or create possibly approximate\nVoronoi partitions [50, 5] to exploit locality. Another line of\nmethods is based on product quantization [29, 19, 32, 24], with\nthe unique ability to handle billions of objects.\nNeighborhood-based Methods . This category is an object-\n\frst approach that constructs a proximity graph where nodes\nrepresent objects and edges connect nearby objects. The main\nidea is to perform a search for similar objects atop the prox-\nimity graph. They achieve top accuracy and speed trade-o\u000b\nin many empirical evaluations [37, 2]. The \frst class of these\nmethods tries to build a k-NN graph [16] or its variant [37]\nwhich records the k-NN of each object. Then nearest neighbor\nsearch is conducted by the hill-climbing strategy. The second\nclass employs the navigable small world graph [43, 44], an undi-\nrected graph that contains an approximation of the Delaunay\ngraph and has long-range links with the small world navigation\nproperty [34]. Hierarchical navigable small world [44] is one of\nthe most e\u000ecient algorithms thus far and support incremental\nupdate. Recently, learning-based methods were proposed to\nprovide a more e\u000ecient search path in the graph [53]. The third\nclass is based on the relative neighborhood graph [17], which\nconsiders connectivity, degree, shortest path length, and index\nsize to achieve robust empirical performance.\n2.4 Selectivity Estimation\nSelectivity estimation outputs the approximate number of\nobjects that satisfy a selection criterion. Due to its use in\ndensity estimation, outlier detection, image retrieval, and query\noptimization, this problem has received considerable attention\nrecently. For example, hands-o\u000b entity matching systems [21,\n13] extract paths from random forests and take each path (a\nconjunction of similarity predicates over multiple attributes)\n 3438\n\nas a blocking rule, and thus selectivity estimation is useful for\nchoosing the execution order of query plans that involve multiple\nsimilarity predicates. A traditional database method is based\non importance sampling [69]. Kernel density estimation [23, 47]\ntailored to this problem has also been developed. A recent trend\nis to formalize it as a regression task and utilize ML methods,\ne.g., by XGBoost [10], LightGBM [33], the mixture of expert\nmodel [58], or the recursive model indexes [35]. Another line of\nwork targets monotonic estimation by employing deep lattice\nnetwork [72], deep regression with incremental prediction [65],\nor piece-wise linear functions [66].\n2.5 Future Opportunities\nWe highlight a number of promising directions for future\nresearch: (1) It is interesting to explore ML models as solu-\ntions to query processing (e.g., learned indexing or sampling).\n(2) Whilst many existing studies target search queries, we ex-\npect that join queries will be explored, especially for the cold\nstart case. (3) Answering composite queries (e.g., conjunctive\nqueries) over multiple attributes will receive more attention,\nsince many DB tasks deal with multi-attribute data and the\nadvancement of deep learning methods will enable us to embed\nmore attributes for semantic comparison. (4) Another direction\nis to develop e\u000ecient algorithms for query processing in data\nscience platforms such as Pandas/R dataframe.\n3. BIOGRAPHIES OF PRESENTERS\nThe four presenters have rich experience in the research on\nsimilarity queries for high-dimensional data, and have made\nsigni\fcant contributions [36, 37, 38, 54, 56, 57, 61, 42, 65, 66].\nJianbin Qin is a Research Scientist with Shenzhen Institute\nof Computing Sciences, Shenzhen University. He received the\nPh.D. degree from the University of New South Wales in 2013.\nHis research interests include similarity query processing, data\nintegration, textual databases, and information retrieval. He\nhas given a tutorial at WISE 2017.\nWei Wang is a Professor with the University of New South\nWales. He received the Ph.D. degree from the Hong Kong\nUniversity of Science and Technology in 2004. His research\ninterests include high-dimensional data management, similarity\nquery processing, data integration, knowledge graphs, natural\nlanguage processing, and adversarial machine learning. He has\ngiven tutorials at SIGMOD 2009 and ICDE 2011.\nChuan Xiao is an Associate Professor with Osaka University\nand Nagoya University. He received the Ph.D. degree from the\nUniversity of New South Wales in 2010. His research interests\ninclude similarity query processing, data integration, textual\ndatabases, spatio-temporal databases, and graph databases.\nHe has given a tutorial at WISE 2017.\nYing Zhang is a Professor and ARC Future Fellow with\nthe University of Technology, Sydney, and the Head of the\nDatabase Group at the Centre for Arti\fcial Intelligence. He\nreceived the Ph.D. degree from the University of New South\nWales in 2008. His research interests include high-dimensional\ndata management, scalable data analytics, data streams, and\ngraph databases. He has given a tutorial at ICDE 2019.\nAcknowledgments . This work was supported by JSPS\n17H06099, 18H04093, and 19K11979, NSFC 61702409, Guang-\ndong Basic and Applied Basic Research Foundation 2019A1515111047,\n2019A1515011064, Guangdong Project 2017B030314073 and\n2018B030325002, ARC DP170103710, DP180103096, DP180103411,\nand FT170100128, and D2D CRC DC25002 and DC25003. We\nthank Yaoshu Wang (SICS) for his kind advice.4. REFERENCES\n[1]M. Alzantot, Y. Sharma, A. Elgohary, B. Ho, M. B. Srivastava,\nand K. Chang. Generating natural language adversarial\nexamples. In EMNLP , pages 2890{2896, 2018.\n[2] M. Aum uller, E. Bernhardsson, and A. J. Faithfull.\nAnn-benchmarks: A benchmarking tool for approximate\nnearest neighbor algorithms. Inf. Syst. , 87, 2020.\n[3]S. Berchtold, C. B ohm, and H. Kriegel. The pyramid-technique:\nTowards breaking the curse of dimensionality. In SIGMOD ,\npages 142{153, 1998.\n[4] E. Bernhardsson. Annoy at github\nhttps://github.com/spotify/annoy , 2015.\n[5] A. Beygelzimer, S. Kakade, and J. Langford. Cover trees for\nnearest neighbor. In ICML , pages 97{104, 2006.\n[6] D. Cai, X. Gu, and C. Wang. A revisit on deep hashings for\nlarge-scale content based image retrieval. CoRR ,\nabs/1711.06016, 2017.\n[7]Y. Cao, M. Long, B. Liu, and J. Wang. Deep cauchy hashing for\nhamming space retrieval. In CVPR , pages 1229{1237, 2018.\n[8] L. Cayton. Fast nearest neighbor retrieval for bregman\ndivergences. In ICML , pages 112{119, 2008.\n[9] K. Chakrabarti and S. Mehrotra. Local dimensionality\nreduction: A new approach to indexing high dimensional spaces.\nInVLDB , pages 89{100, 2000.\n[10] T. Chen and C. Guestrin. Xgboost: A scalable tree boosting\nsystem. In KDD , pages 785{794, 2016.\n[11] P. Ciaccia, M. Patella, and P. Zezula. M-tree: An e\u000ecient\naccess method for similarity search in metric spaces. In VLDB ,\npages 426{435, 1997.\n[12] J. Dai, M. Zhang, G. Chen, J. Fan, K. Y. Ngiam, and B. C. Ooi.\nFine-grained concept linking using neural networks in\nhealthcare. In SIGMOD , pages 51{66, 2018.\n[13] S. Das, P. S. G. C., A. Doan, J. F. Naughton, G. Krishnan,\nR. Deep, E. Arcaute, V. Raghavendra, and Y. Park. Falcon:\nScaling up hands-o\u000b crowdsourced entity matching to build\ncloud services. In SIGMOD , pages 1431{1446, 2017.\n[14] S. Dasgupta and Y. Freund. Random projection trees and low\ndimensional manifolds. In STOC , pages 537{546, 2008.\n[15] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni.\nLocality-sensitive hashing scheme based on p-stable\ndistributions. In SoCG , pages 253{262, 2004.\n[16] W. Dong. High-dimensional similarity search for large datasets .\nPrinceton University, 2011.\n[17] C. Fu, C. Xiang, C. Wang, and D. Cai. Fast approximate\nnearest neighbor search with the navigating spreading-out\ngraph. PVLDB , 12(5):461{474, 2019.\n[18] K. Fukunaga and P. M. Narendra. A branch and bound\nalgorithms for computing k-nearest neighbors. IEEE Trans.\nComputers , 24(7):750{753, 1975.\n[19] T. Ge, K. He, Q. Ke, and J. Sun. Optimized product\nquantization. IEEE Trans. Pattern Anal. Mach. Intell. ,\n36(4):744{755, 2014.\n[20] A. Gionis, P. Indyk, and R. Motwani. Similarity search in high\ndimensions via hashing. In VLDB , pages 518{529, 1999.\n[21] C. Gokhale, S. Das, A. Doan, J. F. Naughton, N. Rampalli,\nJ. W. Shavlik, and X. Zhu. Corleone: hands-o\u000b crowdsourcing\nfor entity matching. In SIGMOD , pages 601{612, 2014.\n[22] J. He, W. Liu, and S. Chang. Scalable similarity search with\noptimized kernel hashing. In KDD , pages 1129{1138, 2010.\n[23] M. Heimel, M. Kiefer, and V. Markl. Self-tuning,\nGPU-accelerated kernel density models for multidimensional\nselectivity estimation. In SIGMOD , pages 1477{1492, 2015.\n[24] J. Heo, Z. Lin, X. Shen, J. Brandt, and S. Yoon. Shortlist\nselection with residual-aware distance estimator for k-nearest\nneighbor search. In CVPR , pages 2009{2017, 2016.\n[25] Q. Huang, J. Feng, Q. Fang, W. Ng, and W. Wang.\nQuery-aware locality-sensitive hashing scheme for l pnorm.\nVLDB J. , 26(5):683{708, 2017.\n[26] P. Indyk and R. Motwani. Approximate nearest neighbors:\nTowards removing the curse of dimensionality. In STOC , pages\n604{613, 1998.\n 3439\n\n[27] M. Izbicki and C. R. Shelton. Faster cover trees. In ICML ,\npages 1162{1170, 2015.\n[28] H. V. Jagadish, B. C. Ooi, K.-L. Tan, C. Yu, and R. Zhang.\nidistance: An adaptive b+-tree based indexing method for\nnearest neighbor search. ACM Trans. Database Syst. ,\n30(2):364{397, 2005.\n[29] H. J\u0013 egou, M. Douze, and C. Schmid. Product quantization for\nnearest neighbor search. IEEE Trans. Pattern Anal. Mach.\nIntell. , 33(1):117{128, 2011.\n[30] Z. Jin, Y. Hu, Y. Lin, D. Zhang, S. Lin, D. Cai, and X. Li.\nComplementary projection hashing. In ICCV , pages 257{264,\n2013.\n[31] A. Joly and O. Buisson. Random maximum margin hashing. In\nCVPR , pages 873{880, 2011.\n[32] Y. Kalantidis and Y. Avrithis. Locally optimized product\nquantization for approximate nearest neighbor search. In\nCVPR , pages 2329{2336, 2014.\n[33] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye,\nand T. Liu. Lightgbm: A highly e\u000ecient gradient boosting\ndecision tree. In NIPS , pages 3149{3157, 2017.\n[34] J. M. Kleinberg. Navigation in a small world. Nature ,\n406(6798):845, 2000.\n[35] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The\ncase for learned index structures. In SIGMOD , pages 489{504,\n2018.\n[36] M. Li, Y. Zhang, Y. Sun, W. Wang, I. W. Tsang, and X. Lin.\nI/O e\u000ecient approximate nearest neighbour search based on\nlearned functions. In ICDE , 2020.\n[37] W. Li, Y. Zhang, Y. Sun, W. Wang, M. Li, W. Zhang, and\nX. Lin. Approximate nearest neighbor search on high\ndimensional data-experiments, analyses, and improvement.\nIEEE Trans. Knowl. Data Eng. , 2019.\n[38] W. Liu, H. Wang, Y. Zhang, W. Wang, and L. Qin. I-LSH: I/O\ne\u000ecient c-approximate nearest neighbor search in\nhigh-dimensional space. In ICDE , pages 1670{1673, 2019.\n[39] W. Liu, J. Wang, S. Kumar, and S. Chang. Hashing with\ngraphs. In ICML , pages 1{8, 2011.\n[40] J. Lu, C. Lin, J. Wang, and C. Li. Synergy of database\ntechniques and machine learning models for string similarity\nsearch and join. In CIKM , pages 2975{2976, 2019.\n[41] J. Lu, V. E. Liong, and J. Zhou. Deep hashing for scalable image\nsearch. IEEE Trans. Image Processing , 26(5):2352{2367, 2017.\n[42] K. Lu, H. Wang, W. Wang, and M. Kudo. VHP: approximate\nnearest neighbor search via virtual hypersphere partitioning.\nPVLDB , 13(9):1443{1455, 2020.\n[43] Y. Malkov, A. Ponomarenko, A. Logvinov, and V. Krylov.\nApproximate nearest neighbor algorithm based on navigable\nsmall world graphs. Inf. Syst. , 45:61{68, 2014.\n[44] Y. A. Malkov and D. A. Yashunin. E\u000ecient and robust\napproximate nearest neighbor search using hierarchical\nnavigable small world graphs. CoRR , abs/1603.09320, 2016.\n[45] W. Mann, N. Augsten, and P. Bouros. An empirical evaluation\nof set similarity join techniques. PVLDB , 9(9):636{647, 2016.\n[46] G. Mar\u0018 cais, D. F. DeBlasio, P. Pandey, and C. Kingsford.\nLocality-sensitive hashing for the edit distance. Bioinform. ,\n35(14):i127{i135, 2019.\n[47] M. Mattig, T. Fober, C. Beilschmidt, and B. Seeger.\nKernel-based cardinality estimation on metric data. In EDBT ,\npages 349{360, 2018.\n[48] S. Mudgal, H. Li, T. Rekatsinas, A. Doan, Y. Park,\nG. Krishnan, R. Deep, E. Arcaute, and V. Raghavendra. Deep\nlearning for entity matching: A design space exploration. In\nSIGMOD , pages 19{34, 2018.\n[49] M. Muja and D. G. Lowe. Scalable nearest neighbor algorithms\nfor high dimensional data. IEEE Trans. Pattern Anal. Mach.\nIntell. , 36(11):2227{2240, 2014.\n[50] G. Navarro. Searching in metric spaces by spatial\napproximation. VLDB J. , 11(1):28{46, 2002.\n[51] M. Norouzi, A. Punjani, and D. J. Fleet. Fast exact search in\nhamming space with multi-index hashing. IEEE Trans. Pattern\nAnal. Mach. Intell. , 36(6):1107{1119, 2014.[52] B. C. Ooi, K. Tan, C. Yu, and S. Bressan. Indexing the edges -\nA simple and yet e\u000ecient approach to high-dimensional\nindexing. In PODS , pages 166{174, 2000.\n[53] L. Prokhorenkova. Graph-based nearest neighbor search: From\npractice to theory. CoRR , abs/1907.00845, 2019.\n[54] J. Qin, Y. Wang, C. Xiao, W. Wang, X. Lin, and Y. Ishikawa.\nGPH: similarity search in hamming space. In ICDE , pages\n29{40, 2018.\n[55] J. Qin and C. Xiao. Set similarity query processing. In WISE ,\n2017.\n[56] J. Qin and C. Xiao. Pigeonring: A principle for faster\nthresholded similarity search. PVLDB , 12(1):28{42, 2018.\n[57] J. Qin, C. Xiao, Y. Wang, W. Wang, X. Lin, Y. Ishikawa, and\nG. Wang. Generalizing the pigeonhole principle for similarity\nsearch in hamming space. IEEE Trans. Knowl. Data Eng. ,\n2019.\n[58] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. V. Le, G. E.\nHinton, and J. Dean. Outrageously large neural networks: The\nsparsely-gated mixture-of-experts layer. CoRR , abs/1701.06538,\n2017.\n[59] F. Shen, Y. Xu, L. Liu, Y. Yang, Z. Huang, and H. T. Shen.\nUnsupervised deep hashing with similarity-adaptive and\ndiscrete optimization. IEEE Trans. Pattern Anal. Mach. Intell. ,\n40(12):3034{3044, 2018.\n[60] C. Silpa-Anan and R. I. Hartley. Optimised kd-trees for fast\nimage descriptor matching. In CVPR , 2008.\n[61] Y. Sun, W. Wang, J. Qin, Y. Zhang, and X. Lin. SRS: solving\nc-approximate nearest neighbor queries in high dimensional\neuclidean space with a tiny index. PVLDB , 8(1):1{12, 2014.\n[62] Y. Tao, K. Yi, C. Sheng, and P. Kalnis. Quality and e\u000eciency\nin high dimensional nearest neighbor search. In SIGMOD ,\npages 563{576, 2009.\n[63] J. Wang, W. Liu, A. X. Sun, and Y. Jiang. Learning hash codes\nwith listwise supervision. In ICCV , pages 3032{3039, 2013.\n[64] J. Wang, J. Wang, N. Yu, and S. Li. Order preserving hashing\nfor approximate nearest neighbor search. In MM, pages\n133{142, 2013.\n[65] Y. Wang, C. Xiao, J. Qin, X. Cao, Y. Sun, W. Wang, and\nM. Onizuka. Monotonic cardinality estimation of similarity\nselection: A deep learning approach. In SIGMOD , pages\n1197{1212, 2020.\n[66] Y. Wang, C. Xiao, J. Qin, R. Mao, M. Onizuka, W. Wang, and\nR. Zhang. Consistent and \rexible selectivity estimation for\nhigh-dimensional data. CoRR , abs/2005.09908, 2020.\n[67] R. Weber, H. Schek, and S. Blott. A quantitative analysis and\nperformance study for similarity-search methods in\nhigh-dimensional spaces. In VLDB , pages 194{205, 1998.\n[68] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In\nNIPS , pages 1753{1760, 2008.\n[69] X. Wu, M. Charikar, and V. Natchu. Local density estimation\nin high dimensions. In ICML , pages 5293{5301, 2018.\n[70] Z. Xia, X. Feng, J. Peng, and A. Hadid. Unsupervised deep\nhashing for large-scale visual search. In IPTA , pages 1{5, 2016.\n[71] P. N. Yianilos. Data structures and algorithms for nearest\nneighbor search in general metric spaces. In SODA , pages\n311{321, 1993.\n[72] S. You, D. Ding, K. R. Canini, J. Pfeifer, and M. R. Gupta.\nDeep lattice networks and partial monotonic functions. In\nNIPS , pages 2981{2989, 2017.\n[73] M. Yu, G. Li, D. Deng, and J. Feng. String similarity search and\njoin: a survey. Frontiers Comput. Sci. , 10(3):399{417, 2016.\n[74] R. Zhang, B. C. Ooi, and K.-L. Tan. Making the pyramid\ntechnique robust to query types and workloads. In ICDE , pages\n313{324, 2004.\n[75] B. Zheng, X. Zhao, L. Weng, N. Q. V. Hung, H. Liu, and C. S.\nJensen. PM-LSH: A fast and accurate LSH framework for\nhigh-dimensional approximate NN search. PVLDB ,\n13(5):643{655, 2020.\n[76] E. Zhu, F. Nargesian, K. Q. Pu, and R. J. Miller. LSH ensemble:\nInternet-scale domain search. PVLDB , 9(12):1185{1196, 2016.\n 3440",
  "textLength": 27213
}