{
  "paperId": "afa47bac0aaf4ec93fa9b666aa73a24829257d6e",
  "title": "Predicted Variables in Programming",
  "pdfPath": "afa47bac0aaf4ec93fa9b666aa73a24829257d6e.pdf",
  "text": "SmartChoices: Hybridizing Programming and Machine Learning\nVictor Carbune1Thierry Coppey1Alexander Daryin1Thomas Deselaers1Nikhil Sarda1Jay Yagnik1\nAbstract\nWe present SmartChoices , an approach to making\nmachine learning (ML) a ﬁrst class citizen in pro-\ngramming languages which we see as one way to\nlower the entrance cost to applying ML to prob-\nlems in new domains. There is a growing divide in\napproaches to building systems: on the one hand,\nprogramming leverages human experts to deﬁne a\nsystem while on the other hand behavior is learned\nfrom data in machine learning. We propose to hy-\nbridize these two by providing a 3-call API which\nwe expose through an object called SmartChoice.\nWe describe the SmartChoices-interface, how it\ncan be used in programming with minimal code\nchanges, and demonstrate that it is an easy to use\nbut still powerful tool by demonstrating improve-\nments over not using ML at all on three algo-\nrithmic problems: binary search, QuickSort, and\ncaches. In these three examples, we replace the\ncommonly used heuristics with an ML model en-\ntirely encapsulated within a SmartChoice and thus\nrequiring minimal code changes. As opposed to\nprevious work applying ML to algorithmic prob-\nlems, our proposed approach does not require to\ndrop existing implementations but seamlessly in-\ntegrates into the standard software development\nworkﬂow and gives full control to the software\ndeveloper over how ML methods are applied. Our\nimplementation relies on standard Reinforcement\nLearning (RL) methods. To learn faster, we use\nthe heuristic function, which they are replacing, as\naninitial function . We show how this initial func-\ntion can be used to speed up and stabilize learning\nwhile providing a safety net that prevents perfor-\nmance to become substantially worse – allowing\nfor a safe deployment in critical applications in\nreal life.\n1Google Research. Correspondence to: Victor Carbune <vcar-\nbune@google.com>.\nReinforcement Learning for Real Life (RL4RealLife) Workshop in\nthe36thInternational Conference on Machine Learning , Long\nBeach, California, USA, 2019. Copyright 2019 by the author(s).1. Introduction\nMachine Learning (ML) has had many successes in the\npast decade in terms of techniques and systems as well as\nin the number of areas in which it is successfully applied.\nHowever, using ML has some cost that comes from the addi-\ntional complexity added to software systems (Sculley et al.,\n2014). There is a fundamental impedance mismatch be-\ntween the approaches to system building. Software systems\nhave evolved from the idea that experts have full control\nover the behavior of the system and specify the exact steps\nto be followed. ML on the other hand has evolved from\nlearning behavior by observing data. It allows for learning\nmore complex but implicit programs leading to a loss of con-\ntrol for programmers since the behavior is now controlled\nby data. We believe it is very difﬁcult to move from one\nto another of these approaches, but that a hybrid between\nthem needs to exist which allows to leverage both the devel-\noper’s domain-speciﬁc knowledge and the adaptability of\nML systems.\nWe propose to hybridize ML with programming. We expose\na new object called SmartChoice exposing a 3-call API\nwhich is backed by ML-models and determines its value at\nruntime. A developer will be able to use a SmartChoice just\nlike any other object, combine it with heuristics, domain\nspeciﬁc knowledge, problem constraints, etc. in ways that\nare fully under the developer’s control. This represents\naninversion of control compared to how ML systems are\nusually built. SmartChoices allow to integrate ML tightly\ninto systems and algorithms whereas traditional ML systems\nare built around the model.\nOur approach combines methods from reinforcement learn-\ning (RL), online learning, with a novel API and aims to make\nusing ML in software development easier by avoiding the\noverhead of going through the traditional steps of building\nan ML system: (1) collecting and preparing training data,\n(2) deﬁning a training loss, (3) training an initial model,\n(4) tweaking and optimizing the model, (5) integrating the\nmodel into their system, and (6) continuously updating and\nimproving the model to adjust for drift in the distribution of\nthe data processed.\nWe show how these properties allow for applying ML in\ndomains that have traditionally not been using it and that\nthis is possible with minimal code changes. We demonstrate\nthat ML can help improve the performance of “classical”arXiv:1810.00619v3  [cs.LG]  13 Jun 2019\n\nSmartChoices: Hybridizing Programming and Machine Learning 2\nalgorithms that typically rely on a heuristic. The concrete\nimplementation of SmartChoices in this paper is based on\nstandard deep RL. We emphasize that this is just one possi-\nble implementation.\nIn this paper we show SmartChoices in the context of the\nPython programming language (PL) using concepts from\nobject oriented PLs. The same ideas can be transferred di-\nrectly to functional or imperative PLs, where a SmartChoice\ncould be modelled after a function or a variable.\nWe show how SmartChoices can be used in three algo-\nrithmic problems – binary search, QuickSort, and caches –\nto improve performance by replacing the commonly used\nheuristic with an ML model with minimal code changes,\nleaving the structure of the original code (including poten-\ntial domain-speciﬁc knowledge) untouched. We chose these\nproblems as ﬁrst applications for ease of reproducibility but\nbelieve that this demonstrates that our approach could bene-\nﬁt a wide range of applications, e.g. systems-applications,\ncontent recommendations, or modelling of user behavior.\nFurther, we show how to use the heuristics that are replaced\nas “initial functions ” as means to guide the initial learning,\nhelp targeted exploration, and as a safety net to prevent very\nbad performance.\nThe main contributions of this paper are: (i) we propose\na way to integrate ML methods directly into the software\ndevelopment workﬂow using a novel API; (ii) we show how\nstandard RL and online learning methods can be leveraged\nthrough our proposed API; (iii) we demonstrate that this\ncombination of ideas is simple to use yet powerful enough\nto improve performance of standard algorithms over not\nusing ML at all.\n2. Software Development with SmartChoices\nA SmartChoice has a simple API that allows the developer\nto provide enough information about its context, predict its\nvalue, and provide feedback about the quality of its predic-\ntions. SmartChoices invert the control compared to common\nML approaches that are model centric. Here, the developer\nhas full control over how data and feedback are provided to\nthe model, how inference is called, and how predictions are\nused.\nTo create a SmartChoice, the developer chooses its\noutput type (ﬂoat, int, category, ...), shape, and range;\ndeﬁnes which data the SmartChoice is able to observe\n(type, shape, range); and optionally provides an initial\nfunction. In the following example we instantiate a scalar\nﬂoat SmartChoice taking on values between 0and1,\nwhich can observe three scalar ﬂoats (each in the range\nbetween 0and10), and which uses a simple initial function:\nchoice = SmartChoice(\noutput_def=(float,shape=[1],range=[0,1]),\nobservation_defs={’low’:(float,[1],[0,10]),\n’high’:(float,[1],[0,10]),\n’target’:(float,[1],[0,10])},\ninitial_function=lambda observations:0.5)The SmartChoice can then be used. It determines its value\nwhen read using inference in the underlying ML model, e.g.\nvalue = choice. Predict ()\nSpeciﬁcally, developers should be able to use a SmartChoice\ninstead of a heuristic or an arbitrarily chosen constant.\nSmartChoices can also take the form of a stochastic variable,\nshielding the developer from the underlying complexity of\ninference, sampling, and explore/exploit strategies.\nThe SmartChoice determines its value on the basis of\nobservations about the context that the developer passes in:\nchoice. Observe (’low’, 0.12)\nchoice. Observe ({’high’:0.56,’target’:0.43})\nA developer might provide additional side-information into\nthe SmartChoice that an engineered heuristic would not be\nusing but which a powerful model is able to use in order to\nimprove performance.\nThe developer provides feedback about the quality of previ-\nous predictions once it becomes available:\nchoice. Feedback (reward=10)\nIn this example we provide numerical feedback. Follow-\ning common RL practice a SmartChoice aims to maximize\nthe sum of reward values received over time (possibly dis-\ncounted). In other setups, we might become aware of the\ncorrect value in hindsight and provide the “ground truth” an-\nswer as feedback, turning the learning task into a supervised\nlearning problem. Some problems might have multiple met-\nrics to optimize for (run time, memory, network bandwidth)\nand the developer might want to give feedback for each\ndimension.\nThis API allows for integrating SmartChoices easily and\ntransparently into existing applications with little overhead.\nSee listing 1 for how to use the SmartChoice created above\nin binary search. In addition to the API calls described\nabove, model hyperparameters can be speciﬁed through ad-\nditional conﬁguration, which can be tuned independently.\nThe deﬁnition of the SmartChoice only determines its inter-\nface (i.e. the types and shapes of inputs and outputs).\n3. Initial Functions in SmartChoices\nWe allow for the developer to pass an initial function to the\nSmartChoice. We anticipate that in many cases the initial\nfunction will be the heuristic that the SmartChoice is replac-\ning. Ideally it is a reasonable guess at what values would\nbe good for the SmartChoice to return. The SmartChoice\nwill use this initial function to avoid bad performance in the\ninitial predictions, and observe the behavior of the initial\nfunction to guide its own learning process, similar to imi-\ntation learning (Hussein et al., 2017). The existence of the\ninitial function should strictly improve the performance of\na SmartChoice. In the worst case, the SmartChoice could\nchoose to ignore it completely, but ideally it will allow the\nSmartChoice to explore solutions which are not easily reach-\nable from a random starting point. Further, the initial func-\ntion plays the role of a heuristic policy which explores the\n\nSmartChoices: Hybridizing Programming and Machine Learning 3\nstate and action space generating initial trajectories which\nare then used for learning. Even though such exploration is\nbiased, off-policy RL can train on this data. In contrast to\nimitation learning where an agent tries to become as good\nas the expert, we explicitly aim to outperform the initial\nfunction as quickly as possible, similar to (Schmitt et al.,\n2018).\nFor a SmartChoice to make use of the initial heuristic, and\nto balance between learning a good policy and the safety of\nthe initial function, it relies on a policy selection strategy .\nThis strategy switches between exploiting the learned policy,\nexploring alternative values, and using the initial function.\nIt can be applied at the action or episode level depending\non the requirements. Finally, the initial function provides\na safety net: in case the learned policy starts to misbehave,\nthe SmartChoice can always fallback to the initial function\nwith little cost.\n4. SmartChoices in Algorithms\nIn this section, we describe how SmartChoices can be used\nin three different algorithmic problems and how a developer\ncan leverage the power of machine learning easily with just\na few lines of code. We show experimentally how using\nSmartChoices helps improving the algorithm performance.\nThe interface described above naturally translates into an\nRL setting: the inputs to Observe calls are combined into\nthe state, the output of the Predict call is the action, and\nFeedback is the reward.\nTo evaluate the impact of SmartChoices we measure cu-\nmulative regret over training episodes. Regret measures\nhow much worse (or better when it is negative) a method\nperforms compared to another method. Cumulative regret\ncaptures whether a method is better than another method\nover all previous decisions. For practical use cases we\nare interested in two properties: (1) Regret should never\nbe very high to guarantee acceptable performance of the\nSmartChoice under all circumstances. (2) Cumulative regret\nshould become permanently negative as early as possible.\nThis corresponds to the desire to have better performance\nthan the baseline model as soon as possible.\nUnlike the usual setting which distinguishes a training and\nevaluation mode, we perform evaluation from the point of\nview of the developer without this distinction. The devel-\noper just plugs in the SmartChoice and starts running the\nprogram as usual. Due to the online learning setup in which\nSmartChoices are operating, overﬁtting does not pose a\nconcern (Dekel & Singer, 2005). The (cumulative) regret\nnumbers thus do contain potential performance regressions\ndue to exploration noise. This effect could be mitigated by\nperforming only a fraction of the runs with exploration.\nIn our experiments we do not account for the computational\ncosts of inference in the model. The goal of our study\nis to demonstrate that the proposed approach is generally\nProgram Binary SmartChoice \nRL Policy \nEpisode Log Observed Context Client Code \nchoice.Observe \nchoice.Predict \nchoice.Feedback Models \n(definitions & checkpoints) \nReplay Buffer Model Trainer \n(agent code, optimizer, …) Figure 1. An overview of the architecture for our experiments how\nclient code communicates with a SmartChoice and how the model\nfor the SmartChoice is trained and updated.\nfeasible and that with minimal code changes ML can be\nused in programming. While for algorithms, like those\nwe are experimenting with here, the actual run time does\nmatter we believe that advances in specialized hardware\nwill enable running machine learning models at insigniﬁ-\ncant cost (Kraska et al., 2018). Further, even if such cost\nseem high, we see SmartChoices applicable to a wide vari-\nety of problems: e.g. relying on expensive approximation\nheuristics or working with inherently slow hardware, such\nas ﬁlesystems where the inference time is less relevant. And\nlastly, our approach is applicable to a wide variety of prob-\nlems ranging from systems problems, over user modelling,\nto content recommendation where the computational over-\nhead for ML is not as problematic.\nOur implementation currently is a small library exposing\nthe SmartChoice interface to client applications (ﬁg. 1). A\nSmartChoice assembles observations, actions, and feedback\ninto episode logs that are passed to a replay buffer. The\nmodels are trained asynchronously. When a new check-\npoint becomes available the SmartChoice loads it for use in\nconsecutive steps.\n4.1. Experiment Setup\nTo enable SmartChoices we leverage recent progress in RL\nfor modelling and training. It allows to apply SmartChoices\nto the most general use cases. While we are only look-\ning at RL methods here, SmartChoices could be used with\nother learning methods such as multi-armed bandits or su-\npervised learning. We are building our models on DDQN\n(Hasselt et al., 2016) for categorical outputs and on TD3 (Fu-\njimoto et al., 2018) for continuous outputs. DDQN is a de\nfacto standard in RL since its success in AlphaGo (Silver\net al., 2016). TD3 is a recent modiﬁcation to DDPG (Lil-\nlicrap et al., 2015) using a second critic network to avoid\noverestimating the expected reward. We summarize the\nhyperparameters used in our experiments in (table 1).\nWhile these hyperparameters are now new parameters that\nthe developer can tweak, we hypothesize that on the one\nhand tuning hyperparameters is often simpler than manu-\nally deﬁning new problem-speciﬁc heuristics, and on the\nother hand that improvements on automatic model tuning\nfrom the general machine learning community will be easily\napplicable here too.\nOur policy selection strategy starts by only evaluating the\ninitial function and then gradually starts to increase the\n\nSmartChoices: Hybridizing Programming and Machine Learning 4\nTable 1. Parameters for the different experiments described below\n(FC=fully connected layer, LR=learning rate). See (Henderson\net al., 2018) for details on these parameters.\nBinary search QuickSort Caches (discrete) Caches (continuous)\nLearning algorithm TD3 DDQN DDQN TD3\nActor network FC16!tanh – – FC10!tanh\nCritic/value network FC16 (FC16;ReLU )2!FC (FC10;ReLU )2!FC FC10\nKey embedding size – – 8\nDiscount 0:8,0 0 0:8\nLR actor 10\u00003– – 10\u00004\nInitial function decay yes no\nBatch size 256 1024\nAction noise \u001b 0:03 – – 0:01\nTarget noise \u001b 0:2 – – 0:01\nTemperature – 0:1 –\nUpdate ratio ( \u001c) 0.05 0:001\nCommon: Optimizer: Adam; LR critic: 10\u00004; Replay buffer: Uniform, FIFO, size 20000; Update period: 1.\nuse of the learned policy. It keeps track of the received\nrewards of these policies adjusts the use of the learned policy\ndepending on its performance. We show the usage rate of the\ninitial function when we use it (ﬁg. 2, bottom) demonstrating\nthe effectiveness of this strategy.\n4.2. Binary Search\nBinary search (Williams, 1976) is a standard algorithm for\nﬁnding the location lxof a target value xin a sorted array\nA=fa0;a1;:::;a N\u00001gof sizeN. Binary search has a\nworst case runtime complexity of dlog2(N)esteps when no\nfurther knowledge about the distribution of data is available.\nPrior knowledge of the data distribution can help reduce the\naverage runtime: e.g. in case of an uniform distribution,\nthe location of xcan be approximated using linear inter-\npolationlx\u0019(N\u00001)(x\u0000a0)=(aN\u00001\u0000a0). We show\nhow SmartChoices can be used to speed up binary search by\nlearning to estimate the position lxfor a more general case.\nThe simplest way of using a SmartChoice is to directly\nestimate the location lxand incentivize the search to do\nso in as few steps as possible by penalizing each step by\nthe same negative reward (listing 1). At each step, the\nSmartChoice observes the values aL,aRat both ends of the\nsearch interval and the target x. The SmartChoice output\nqis used as the relative position of the next read index m,\nsuch thatm=qL+ (1\u0000q)R.\nIn order to give a stronger learning signal to the model,\nthe developer can incorporate problem-speciﬁc knowledge\ninto the reward function or into how the SmartChoice is\nused. One way to shape the reward is to account for prob-\nlem reduction. For binary search, reducing the size of the\nremaining search space will speed up the search proportion-\nally and should be rewarded accordingly. By replacing the\nstep-counting reward in listing 1 (line 9) with the search\nrange reduction (Rt\u0000Lt)=(Rt+1\u0000Lt+1), we directly re-\nward reducing the size of the search space. By shaping the\nreward like this, we are able to attribute the feedback signal\nto the current prediction and to reduce the problem from\nRL to contextual bandit (which we implement by using a\ndiscount factor of 0).\nAlternatively we can change the way the prediction is used\nto cast the problem in a way that the SmartChoice learns\n 5 10 15 20 25 30 35 40Cost (# of lookups), smoothedCost of binary search\nHeuristics mix, shaped reward\nHeuristics mix, simple reward\nPosition, shaped reward\nPosition, simple reward\nPosition, simple, no init. func.\nInterpolation (baseline)\nVanilla (baseline)1000 2000 3000 4000 5000-15000-10000-500005000100001500020000\nCumulative regret\nEpisodeCumulative regret of SmartChoice against vanilla\n0%20%40%60% 80%\n0 1000 2000 3000 4000 5000Initial function usage\nEpisodeFigure 2. The cost of different variants of binary search (top left),\ncumulative regret compared to vanilla binary search (right), and\ninitial function usage (bottom).\nfaster and is unable to predict very bad values. For many\nalgorithms (including binary search) it is possible to pre-\ndict a combination of (or choice among) several existing\nheuristics rather than predicting the value directly. We use\ntwo heuristics: (a) vanilla binary search which splits the\nsearch rangefaL;:::;a Rginto two equally large parts\nusing the split location lv= (L+R)=2, and (b) inter-\npolation search which interpolates the split location as\nli= ((aR\u0000v)L+ (v\u0000aL)R)=(aR\u0000aL). We then use\nthe valueqof the SmartChoice to mix between these heuris-\ntics to get the predicted split position lq=qlv+ (1\u0000q)li.\nSince in practice both of these heuristics work well on many\ndistributions, any point in between will also work well. This\nreduces the risk for the SmartChoice to pick a value that is\nreally bad which in turn helps learning. A disadvantage is\nthat it is impossible to ﬁnd the optimal strategy if its values\nlie outside of the interval between lvandli.\nToevaluate our approaches we use a test environment where\nin each episode, we search a random element in a sorted\narray of 5000 elements taken from a randomly chosen distri-\nbution (uniform, triangular, normal, pareto, power, gamma\nand chisquare), with values in [\u0000104;104].\nFigure 2 shows the results for the different variants of bi-\nnary search using a SmartChoice and compares them to the\nvanilla binary search baseline. The results show that the sim-\nplest case (pink line) where we directly predict the relative\nposition with the simple reward and without using an initial\nfunction performs poorly initially but then becomes nearly\nas good as the baseline (cumulative regret becomes nearly\nconstant after an initial bad period). The next case (yellow\nline) has an identical setup but we are using the initial func-\ntion and we see that the initial regret is substantially smaller.\nBy using the shaped reward (blue line), the SmartChoice\nis able to learn the behavior of the baseline quickly. Both\napproaches that are mixing the heuristics (green and red\nlines) signiﬁcantly outperform the baselines.\n4.3. QuickSort\nQuickSort (Hoare, 1962) sorts an array in-place by partition-\ning it into two sets (smaller/larger than the pivot) recursively\n\nSmartChoices: Hybridizing Programming and Machine Learning 5\nListing 1. Standard binary search (left) and a simple way to use a SmartChoice in binary search (right).\n1def bsearch(x, a, l=0, r=len(a)-1):\n2 if l > r: return None\n3\n4\n5 q = 0.5\n6 m = int(q *l + (1-q) *r)\n7 if a[m] == x:\n8 return m\n9\n10 if a[m] < x:\n11 return bsearch(x, a, m+1, r)\n12 return bsearch(x, a, l, m-1)1 def bsearch(x, a, l=0, r=len(a)-1):\n2 if l > r: return None\n3 choice. Observe ({’target’:x,\n4 ’low’:a[l], ’high’:a[r]})\n5 q = choice. Predict ()\n6 m = int(q *l + (1-q) *r)\n7 if a[m] == x:\n8 return m\n9 choice. Feedback (-1)\n10 if a[m] < x:\n11 return bsearch(x, a, m+1, r)\n12 return bsearch(x, a, l, m-1)\nuntil the array is fully sorted. QuickSort is one of the most\ncommonly used sorting algorithms where many heuristics\nhave been proposed to choose the pivot element. While the\naverage time complexity of QuickSort is \u0012(Nlog(N)), a\nworst case time complexity of O(N2)can happen when the\npivot elements are badly chosen. The optimal choice for\na pivot is the median of the range, which splits it into two\nparts of equal size.\nTo improve QuickSort using a SmartChoice we aim at tuning\nthe pivot selection heuristic. To allow for sorting arbitrary\ntypes, we use the SmartChoice to determine the number of\nrandom samples to pick from the array to sort, and use their\nmedian as the partitioning pivot (listing 2). As feedback\nsignal for a recursion step, we estimate the impact of the\npivot selection on the computational cost \u0001c.\n\u0001c=cpiv+ \u0001crec\ncexpected=cpiv+ (aloga+blogb\u00002n\n2logn\n2)\nnlogn;\n(1)\nwherenis the size of the array, aandbare the sizes of the\npartitions with n=a+bandcpiv=cmedian +cpartition is the\ncost to compute the median of the samples and to partition\nthe array. \u0001crectakes into account how close the current\npartition is to the ideal case (median). The cost is a weighted\nsum of number of reads, writes, and comparisons. Similar\nto the shaped reward in binary search, this reward allows us\nto reduce the RL problem to a contextual bandit problem\nand we use a discount of 0.\nForevaluation we are using a test environment where we\nsort randomly shufﬂed arrays. Results of the experiments\nare presented in ﬁg. 3 and show that the learned method\noutperforms all baseline heuristics within less than 100\nepisodes. ‘Vanilla’ corresponds to a standard QuickSort\nimplementation that picks one pivot at random in each step.\n‘Random3’ and ‘Random9’ sample 3 and 9 random elements\nrespectively and use the median of these as pivots. ‘Adap-\ntive’ uses the median of max(1;blog2(n)\u00001c)randomly\nsampled elements as pivot when partitioning a range of size\nn. It uses more samples at for larger arrays, leading to a bet-\nter approximation of the median, and thus to faster problem\nsize reduction.\nFig. 4 shows that the SmartChoice learns a non-trivial pol-\nicy. The SmartChoice learns to select more samples at larger\narray sizes which is similar to the behavior that we hand-\n 23500 24000 24500 25000 25500 26000 26500 27000\n 0  200  400  600  800  1000Cost (read=1,write=1,cmp=0.5)\nEpisode(a)\nSmartChoice samples\nVanilla (baseline)\nRandom3 (baseline)\nRandom9 (baseline)\nAdaptive (baseline)\n 0 200  400  600  800 1000-2.5e+06-2e+06-1.5e+06-1e+06-5000000500000\nCumulative regret\nEpisode(b)Figure 3. Results from using a SmartChoice for selecting the num-\nber of pivots in QuickSort. (a) shows the overall cost for the\ndifferent baseline methods and for the variant with a SmartChoice\nover training episodes. (b) shows the cumulative regret of the\nSmartChoice method compared to each of the baselines over train-\ning episodes.\n 0% 20% 40% 60% 80% 100%\n 10  100  1000Fraction of samples\nSize of the array to sort (log scale)Predicted number of samples to use, such that pivot=median(samples)\n15 samples (19%)\n13 samples (20%)\n11 samples (22%)\n9 samples (24%)\n7 samples (27%)\n5 samples (31%)\n3 samples (37%)\n1 sample (50%)\nFigure 4. Number of pivots chosen by the SmartChoice in Quick-\nSort after 5000 episodes. The expected approximation error of the\nmedian is given in the legend, next to the number of samples.\ncoded in the adaptive baseline but in this case no manual\nheuristic engineering was necessary and a better policy was\nlearned. Also, note that a SmartChoice-based method is\nable to adapt to changing environments which is not the\ncase for engineered heuristics. One surprising result is that\nthe SmartChoice prefers 13 over 15 samples at large ar-\nray sizes. We hypothesize this happens because relatively\nfew examples of large arrays are seen during training (one\nper episode, while arrays of smaller sizes are seen multiple\ntimes per episode).\n4.4. Caches\nCaches are a commonly used component to speed up com-\nputing systems. They use a cache replacement policy (CRP)\nto determine which element to evict when the cache is\nfull and a new element needs to be stored. Probably the\nmost popular CRP is the least recently used (LRU) heuristic\nwhich evicts the element with the oldest access timestamp.\nA number of approaches have been proposed to improve\ncache performance using machine learning (see sec. 5). We\npropose two different approaches how SmartChoices can be\nused in a CRP to improve cache performance.\nDiscrete (listing 3): A SmartChoice directly predicts which\nelement to evict or chooses not to evict at all (by predict-\ning an invalid index). That is, the SmartChoice learns to\n\nSmartChoices: Hybridizing Programming and Machine Learning 6\nListing 2. A QuickSort implementation that uses a SmartChoice to choose the number of samples to compute the next pivot. As feedback,\nwe use the cost of the step compared to the optimal partitioning.\n1def qsort(a, l=0, r=len(a)):\n2 if r <= l+1:\n3 return\n4 m = pivot(a, l, r)\n5 qsort(a, l, m-1)\n6 qsort(a, m+1, r)\n7\n8def delta_cost(c_pivot, n, a, b):\n9 # See eq. 11 def pivot(a, l, r):\n2 choice. Observe ({’left’:l, ’right’:r})\n3 q = min(1+2 *choice. Predict (), r-l)\n4 v = median(sample(a[l:r], q))\n5 m = partition(a, l, r, v)\n6 c = cost_of_median_and_partition()\n7 d = delta_cost(c, r-l, m-l, r-m)\n8 choice. Feedback (1/d)\n9 return m\nListing 3. Cache replacement policy directly predicting eviction decisions ( Discrete ).\n1keys = ... # keys now in cache.\n2\n3# Returns evicted key or None.\n4def miss(key):\n5 choice. Feedback (-1) # Miss penalty.\n6 choice. Observe (’access’, key)\n7 choice. Observe (’memory’, keys)\n8 return evict(choice. Predict ())1 def evict(i):\n2 if i >= len(keys): return None\n3 choice. Feedback (-1) # Evict penalty.\n4 choice. Observe (’evict’, keys[i])\n5 return keys[i]\n6 def hit(key):\n7 choice. Feedback (1) # Hit reward.\n8 choice. Observe (’access’, key)\nListing 4. Cache replacement policy using a priority queue ( Continuous ).\n1q = min_priority_queue(capacity)\n2def priority(key):\n3 choice. Observe (...)\n4 score = choice. Predict ()\n5 score *= capacity *scale\n6 return time() + score1 def hit(key):\n2 choice. Feedback (1) # Hit reward.\n3 q.update(key, priority(key))\n4 def miss(key):\n5 choice. Feedback (-1) # Miss penalty.\n6 return q.push(key, priority(key))\nObservation Network \n(when using key embeddings) \nActor \nAccess  \n(history) \nEmbed Action \n    Critic \nConcat \nMemory \nEviction \n(history) \nConcat FC FC FCConv1D \nConv1D \nConv1D \nFCFCFC\nFC\nValue \nFigure 5. The architecture of the neural networks for TD3 with key\nembedding network.\nbecome a CRP itself. While this is the simplest way to use a\nSmartChoice, it makes it more difﬁcult to learn a CRP better\nthan LRU (in fact, even learning to be on par with LRU is\nnon-trivial in this setting).\nContinuous (listing 4): A SmartChoice is used to enhance\nLRU by predicting an offset to the last access timestamp.\nHere, the SmartChoice learns which items to keep in the\ncache longer and which items to evict sooner. In this case it\nbecomes trivial to be as good as LRU by predicting a zero\noffset. The SmartChoice value in (\u00001;1)is scaled to get a\nreasonable value range for the offsets. It is also possible to\nchoose not to store the element by predicting a sufﬁciently\nnegative score.\nIn both approaches the feedback given to the SmartChoice\nis whether an item was found in the cache ( +1) or not (\u00001).\nIn the discrete approach we also give a reward of \u00001if the\neviction actually takes place.\nIn our implementation the observations are the history of\naccesses, memory contents, and evicted elements. The\nSmartChoice can observe (1) keys as a categorical input\nor (2) features of the keys.\nObserving keys as categorical input allows to avoid feature\nengineering and enables directly learning the properties of\n 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0.82 0.84\n0 0.5M 1M 2M 3MHit Ratio\nGlobal Step(a)\ndiscrete keys\ncontinuous keys\ncontinuous frequency\nlru\noracle\n0 5k 10k 15k-1e+06-50000005000001e+061.5e+062e+06\nCumulative regret\nEpisode(b)\n 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5\n0 0.5M 1M 2M 3MHit Ratio\nGlobal Step(a)\ndiscrete keys\ncontinuous keys\ncontinuous frequency\nlru\noracle\n0 5k 10k 15k-1e+06-800000-600000-400000-20000002000004000006000008000001e+06\nCumulative regret\nEpisode(b)Figure 6. Cache performance for power law access patterns. Top:\n\u000b= 0:1, bottom:\u000b= 0:5. (a) Hit Ratio (w/o exploration) and (b)\nCumulative Regret (with exploration)\nparticular keys (e.g. which keys are accessed the most) but\nmakes it difﬁcult to deal with rare and unseen keys. To\nhandle keys as input we train an embedding layer shared\nbetween the actor and critic networks (ﬁg. 5).\nAsfeatures of the keys we observe historical frequencies\ncomputed over a window of ﬁxed size. This approach re-\nquires more effort from the developer to implement such\nfeatures, but pays off with better performance and the fact\nthat the model does not rely on particular key values.\nWe experiment with three combinations of these options:\n(1) discrete caches observing keys, (2) continuous caches\nobserving keys, (3) continuous caches observing frequen-\ncies. For evaluation we use a cache with size 10and integer\nkeys from 1to100. We use two synthetic access patterns\nof length 1000 , sampled i.i.d. from a power law distribution\nwith\u000b= 0:1and\u000b= 0:5. Fig. 6 shows results for the\n\nSmartChoices: Hybridizing Programming and Machine Learning 7\nthree variants of predicted caches, a standard LRU cache,\nand an oracle cache to give a theoretical, non-achievable,\nupper bound on the performance.\nWe look at the hit ratio without exploration to understand\nthe potential performance of the model once learning has\nconverged. However, cumulative regret is still reported\nunder exploration noise.\nBoth implementations that work directly on key embed-\ndings learn to behave similar to the LRU baseline without\nexploration (comparable hit ratio). However, the continuous\nvariant pays a higher penalty for exploration (higher cumula-\ntive regret). Note that this means that the continuous variant\nlearned to predict constant offsets (which is trivial), however\nthe discrete implementation actually learned to become an\nLRU CRP which is non-trivial. The continuous implementa-\ntion with frequencies quickly outperforms the LRU baseline,\nmaking the cost/beneﬁt worthwhile long-term (negative cu-\nmulative regret after a few hundred episodes).\n4.5. Reproducibility: Goals and Metrics\nNonetheless, Similar to many works that build on RL tech-\nnology, we are faced with the reproducibility issues de-\nscribed by (Henderson et al., 2018). Among multiple runs\nof any experiment, only some runs exhibit the desired be-\nhavior, which we report. In the “failing” runs, we observe\nbaseline performance because the initial function acts as a\nsafety net. Thus, our experiments show that we can outper-\nform the baseline heuristics without a high risk to fail badly.\nThe design construct speciﬁc to SmartChoices and what dis-\ntinguishes it from standard Reinforcement Learning is that\nit is applied in software control where often developers are\nable to provide safe initial functions or write the algorithm\nin a way that limits the cost of a poorly performing policy.\nWhile we do not claim to have the solution to address re-\nproducibility, the use of the initial function can mitigate it\nand any solution to better reproducibility and higher stabil-\nity developed by the community will be applicable in our\napproach as well.\nIn table 2, we provide details on the reproducibility and per-\nformance of our experiments over 100 identical experiments\nfor each of the problems described earlier. The table shows\nthe cumulative regret and the break even point for our exper-\niments for various quantiles and as the mean. Cumulative\nregret indicates how much worse our method is than not\nusing ML at all – if it’s negative it means that it’s better than\nnot using it. The break even point is the number of episodes\nafter which cumulative regret becomes negative and never\npositive anymore. In some experiments the break even point\nis not reached. We report the percentage of runs in which it\nwas reached in the ‘mean’ column.\nWe want to highlight that, while the experiments for some\nproblems are more reproducible than others, our approach\ndoes not perform substantially worse than the initial func-tion provided by the developer, e.g. cumulative regret for\nnone of the problems grows very large, indicating that per-\nformance remains acceptable. This is very visible for the\ncache experiments: While only for 26% of the runs the\nbreak even point was reached, meaning that the cache per-\nforms strictly better than before, it only performs worse than\nbefore in 14% of the runs. For 60% of the runs, the use of\nML does neither help nor hurt compared to using the LRU\nheuristic.\n5. Related work\nThe most relevant work to our proposed interface is (Chang\net al., 2016) where a programming interface is proposed\nfor joint prediction and a method that allows for unifying\nthe implementation for training and inference. Similarly,\nProbabilistic programming (Gordon et al., 2014) introduces\ninterfaces which simplify the developer complexity when\nworking with statistical models and conditioning variable\nvalues on run-time observations. Our proposed interfaces\nare at a higher level in that the user does not need to know\nabout the inner workings of the underlying models. In fact,\nto implement our proposed APIs, techniques from proba-\nbilistic programming might be useful. Similarly, (Sampson\net al., 2011) propose a programming interface for approxi-\nmate computation.\nSimilar in spirit to our approach is (Kraska et al., 2018)\nwhich proposes to incorporate neural models into database\nsystems by replacing existing index structures with neural\nmodels that can be both faster and smaller. In contrast, we\naim not to replace existing data structures or algorithms\nbut transparently integrate with standard algorithms and\nsystems. Our approach is general enough to be used to im-\nprove the heuristics in algorithms (as done here), to optimize\ndatabase systems (similar to (Kraska et al., 2018)), or to sim-\nply replace an arbitrarily chosen constant. Another approach\nthat is similar to SmartChoices is Spiral (Bychkovsky et al.,\n2018) but it is far more limited in scope than SmartChoices\nin that it aims to predict boolean values only and relies on\nground truth data for model building.\nSimilarly, a number of papers apply machine learning to\nalgorithmic problems, e.g. Neural Turing Machines (Graves\net al., 2014) aims to build a full neural model for program\nexecution. (Kaempfer & Wolf, 2018; Kool et al., 2018;\nBello et al., 2016) propose end-to-end ML approaches to\ncombinatorial optimization problems. In contrast to our\napproach these approaches replace the existing methods\nwith an ML-system rather than augmenting them. These are\na good demonstration of the inversion of control problem\nmentioned above: using ML requires to give full control to\nthe ML system.\nThere are a few approaches that are related to our use of\nthe initial function, however most common problems where\nRL is applied do not have a good initial function. Generally\n\nSmartChoices: Hybridizing Programming and Machine Learning 8\nTable 2. Reproducibility data for our experiments: We report cumulative regret for different quantiles of experiments at different training\nepisodes as well as the average over all episodes. We also report the respective break even point as a number of episodes, which is the\nnumber of training episodes at which cumulative regret becomes negative and never positive anymore. For the break even point we report\nthe percentage of runs in which the break even point was reached in the column “mean”.\nProblem Percentile 1 5 10 25 50 75 90 95 99 mean\nBinary Search (N=120)Cum. Regret @5K episodes -2.71 -2.66 -2.62 -2.45 -2.03 -1.01 0.44 0.70 0.78 -1.59\nCum. Regret @50K episodes -3.99 -3.83 -3.76 -3.64 -3.34 -2.85 3.80 3.86 3.92 -2.20\nBreak-even (episodes) 127 201 271 417 758 2403 111 85%\nQuickSort (N=115)Cum. Regret @1K episodes -1273 -1248 -1214 -1146 -1029 -916 -409 372 425 -913\nCum. Regret @10K episodes -1356 -1306 -1267 -1219 -1146 -1034 -945 -285 393 -1064\nBreak-even (episodes) 0 0 0 37 93 141 307 7370 1 94%\nCache (N=100)Cum. Regret @20K episodes -8.25 -5.88 -3.49 -0.00 0.00 0.02 0.34 0.84 2.17 -0.52\nBreak even (episodes) 32 157 472 111111 26%\nrelated is the idea of imitation learning (Hussein et al., 2017)\nwhere the agent aims to replicate the behavior of an expert.\nTypically the amount of training data created by an expert is\nvery limited. Based on imitation learning is the idea to use\npreviously trained agents to kickstart the learning of a new\nmodel (Schmitt et al., 2018) where the authors concurrently\nuse a teacher and a student model and encourage the student\nmodel to learn from the teacher through an auxiliary loss\nthat is decreased over time as the student becomes better.\nIn some applications it may be possible to obtain additional\ntraining data from experts from other sources, e.g. (Hester\net al., 2018; Aytar et al., 2018) leverage YouTube videos of\ngameplay to increase training speed of their agents. These\napproaches work well in cases where it is possible to lever-\nage external data sources.\nCaches are an interesting application area where multiple\nteams have shown in the past that ML can improve cache\nperformance (Zhong et al., 2018; Lykouris & Vassilvitskii,\n2018; Hashemi et al., 2018; Narayanan et al., 2018; Gramacy\net al., 2002). In contrast to our approach, all ML models\nare built for task-speciﬁc caches, and do not generalize to\nother tasks. Algorithm selection has been an approach to\napply RL for improving sorting algorithms (Lagoudakis &\nLittman, 2000). Search algorithms have also been improved\nusing genetic algorithms to tweak code optimization (Li\net al., 2005).\n6. Conclusion\nWe have introduced a new programming concept called a\nSmartChoice aiming to make it easier for developers to use\nmachine learning from their existing code in new application\nareas. Contrary to other approaches, SmartChoices can eas-\nily be integrated and hand full control to the developer over\nhow ML models are used and trained. Our approach bridge\nthe chasm between the traditional approaches of software\nsystems building and machine learning modeling, and thus\nallow for the developer to focus on reﬁning their algorithm\nand metrics rather than working on building pipelines to in-\ncorporate machine learning. We achieve this by proposing a\nnew object called SmartChoice which provides a 3-call API.\nA SmartChoice observes information about its context andreceives feedback about the quality of predictions instead of\nbeing assigned a value directly.\nWe have studied the feasibility of SmartChoices in three\nalgorithmic problems. For each we show how easy\nSmartChoices can be incorporated and how performance\nimproves in comparison to not using a SmartChoice at all.\nSpeciﬁcally, through our experiments we highlight both\nadvantages and disadvantages that reinforcement learning\nbrings when used as a solution for a generic interface as\nSmartChoices.\nNote that we do notclaim to have the best possible machine\nlearning model for each of these problems but our contribu-\ntion lies in building a framework that allows for using ML\neasily, spreading its use, and improving the performance in\nplaces where machine learning would not have been used\notherwise. SmartChoices are applicable to more general\nproblems across a large variety of domains from system\noptimization to user modelling. Our current implementation\nof SmartChoices is built on standard RL methods but other\nML methods such as supervised learning are in scope as\nwell if the problem is appropriate.\nFuture Work. In this paper we barely scratch the surface\nof the new opportunities created with SmartChoices. The\ncurrent rate of progress in ML will enable better results and\nwider applicability of SmartChoices to new applications.\nWe hope that SmartChoices will inspire the use of ML in\nplaces where it has not been considered before.\nAcknowledgements. The authors are part of a larger ef-\nfort aiming to hybridize machine learning and programming.\nWe would like to thank all other members of the team for\ntheir contributions to this work: George Baggott, Gabor\nBartok, Jesse Berent, Eugene Brevdo, Andrew Bunner, Jeff\nDean, Arkady Epshteyn, Sanjay Ghemawat, Daniel Golovin,\nAlex Grubb, Ramki Gummadi, Wei Huang, Eugene Kir-\npichov, Effrosyni Kokiopoulou, Ketan Mandke, Luciano\nSbaiz, Benjamin Solnik, Weikang Zhou.\nFurther we would like to thank the authors and contributors\nof the TF-agents (Guadarrama et al., 2018) library: Ser-\ngio Guadarrama, Julian Ibarz, Anoop Korattikara, Oscar\nRamirez.\n\nSmartChoices: Hybridizing Programming and Machine Learning 9\nReferences\nAytar, Y ., Pfaff, T., Budden, D., Paine, T. L., Wang, Z., and de Fre-\nitas, N. Playing hard exploration games by watching YouTube.\nInNIPS , 2018.\nBello, I., Pham, H., Le, Q. V ., Norouzi, M., and Bengio, S. Neural\ncombinatorial optimization with reinforcement learning. ArXiV ,\n2016.\nBychkovsky, V ., Cipar, J., Wen, A., Hu, L., and Mo-\nhapatra, S. Spiral: Self-tuning services via real-\ntime machine learning. Technical report, Facebook,\n2018. https://code.fb.com/data-infrastructure/spiral-self-tuning-\nservices-via-real-time-machine-learning/.\nChang, K.-W., He, H., Ross, S., Daumé, H., and Langford, J. A\ncredit assignment compiler for joint prediction. In NIPS , 2016.\nDekel, O. and Singer, Y . Data-driven online to batch conversions.\nInNIPS , 2005.\nFujimoto, S., van Hoof, H., and Meger, D. Addressing function\napproximation error in actor-critic methods. In ICML , 2018.\nGordon, A. D., Henzinger, T. A., Nori, A. V ., and Rajamani, S. K.\nProbabilistic programming. In Proc. FOSE , 2014.\nGramacy, R. B., Warmuth, M. K., Brandt, S. A., and Ari, I. Adap-\ntive caching by refetching. In NIPS , 2002.\nGraves, A., Wayne, G., and Danihelka, I. Neural Turing machines.\nArXiV , 2014.\nGuadarrama, S., Korattikara, A., Ramirez, O., Castro, P., Holly, E.,\nFishman, S., Wang, K., Gonina, E., Harris, C., Vanhoucke, V .,\nand Brevdo, E. TF-Agents: A library for reinforcement learn-\ning in tensorﬂow. https://github.com/tensorflow/\nagents , 2018.\nHashemi, M., Swersky, K., Smith, J. A., Ayers, G., Litz, H., Chang,\nJ., Kozyrakis, C. E., and Ranganathan, P. Learning memory\naccess patterns. In ICML , 2018.\nHasselt, H. v., Guez, A., and Silver, D. Deep reinforcement learn-\ning with double q-learning. In AAAI , 2016.\nHenderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and\nMeger, D. Deep reinforcement learning that matters. In AAAI ,\n2018.\nHester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul, T., Piot,\nB., Sendonaris, A., Dulac-Arnold, G., Osband, I., Agapiou, J.,\nLeibo, J. Z., and Gruslys, A. Learning from demonstrations for\nreal world reinforcement learning. In AAAI , 2018.\nHoare, C. A. R. Quicksort. The Computer Journal , 5(1):10–16,\n1962.\nHussein, A., Gaber, M. M., Elyan, E., and Jayne, C. Imitation\nlearning: A survey of learning methods. ACM Comput. Surv. ,\n2017.\nKaempfer, Y . and Wolf, L. Learning the multiple traveling sales-\nmen problem with permutation invariant pooling networks.\nArXiV , 2018.\nKool, W., van Hoof, H., and Welling, M. Attention solves your\nTSP, approximately. ArXiV , 2018.Kraska, T., Beutel, A., hsin Chi, E. H., Dean, J., and Polyzotis, N.\nThe case for learned index structures. In SIGMOD , 2018.\nLagoudakis, M. G. and Littman, M. L. Algorithm selection using\nreinforcement learning. In ICML , 2000.\nLi, X., Garzarán, M. J., and Padua, D. A. Optimizing sorting\nwith genetic algorithms. Int. Sym. on Code Generation and\nOptimization , 2005.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa,\nY ., Silver, D., and Wierstra, D. Continuous control with deep\nreinforcement learning. ArXiV , 2015.\nLykouris, T. and Vassilvitskii, S. Competitive caching with ma-\nchine learned advice. In ICML , 2018.\nNarayanan, A., Verma, S., Ramadan, E., Babaie, P., and Zhang,\nZ.-L. Deepcache: A deep learning based framework for content\ncaching. In NetAI’18 , 2018.\nSampson, A., Dietl, W., Fortuna, E., Gnanapragasam, D., Ceze, L.,\nand Grossman, D. Enerj: Approximate data types for safe and\ngeneral low-power computation. In ACM SIGPLAN Notices ,\nvolume 46, pp. 164–174. ACM, 2011.\nSchmitt, S., Hudson, J. J., Zídek, A., Osindero, S., Doersch, C.,\nCzarnecki, W., Leibo, J. Z., Küttler, H., Zisserman, A., Si-\nmonyan, K., and Eslami, S. M. A. Kickstarting deep reinforce-\nment learning. ArXiV , 2018.\nSculley, D., Holt, G., Golovin, D., Davydov, E., Phillips, T., Ebner,\nD., Chaudhary, V ., and Young, M. Machine learning: The high\ninterest credit card of technical debt. In SE4ML: Software Engi-\nneering for Machine Learning (NIPS 2014 Workshop) , 2014.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den\nDriessche, G., Schrittwieser, J., Antonoglou, I., Panneershel-\nvam, V ., Lanctot, M., Dieleman, S., Grewe, D., Nham, J.,\nKalchbrenner, N., Sutskever, I., Lillicrap, T. P., Leach, M.,\nKavukcuoglu, K., Graepel, T., and Hassabis, D. Mastering the\ngame of Go with deep neural networks and tree search. Nature ,\n2016.\nWilliams, Jr., L. F. A modiﬁcation to the half-interval search (bi-\nnary search) method. In Proc. 14th Annual Southeast Regional\nConference , 1976.\nZhong, C., Gursoy, M. C., and Velipasalar, S. A deep reinforcement\nlearning-based framework for content caching. In CISS , 2018.",
  "textLength": 48082
}