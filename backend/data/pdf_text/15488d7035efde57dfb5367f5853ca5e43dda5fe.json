{
  "paperId": "15488d7035efde57dfb5367f5853ca5e43dda5fe",
  "title": "Learning-Based Heavy Hitters and Flow Frequency Estimation in Streams",
  "pdfPath": "15488d7035efde57dfb5367f5853ca5e43dda5fe.pdf",
  "text": "Learning-Based Heavy Hitters and Flow Frequency Estimation in\nStreams\nRana Shahout1and Michael Mitzenmacher1\n1Harvard University, USA\nAbstract\nIdentifying heavy hitters and estimating the frequencies of flows are fundamental tasks in various network\ndomains. Existing approaches to this challenge can broadly be categorized into two groups, hashing-based and\ncompeting-counter-based. The Count-Min sketch is a standard example of a hashing-based algorithm, and the\nSpace Saving algorithm is an example of a competing-counter algorithm. Recent works have explored the use of\nmachine learning to enhance algorithms for frequency estimation problems, under the algorithms with prediction\nframework. However, these works have focused solely on the hashing-based approach, which may not be best\nfor identifying heavy hitters.\nIn this paper, we present the first learned competing-counter-based algorithm, called LSS, for identifying\nheavy hitters, top k, and flow frequency estimation that utilizes the well-known Space Saving algorithm. We\nprovide theoretical insights into how and to what extent our approach can improve upon Space Saving, backed\nby experimental results on both synthetic and real-world datasets. Our evaluation demonstrates that LSS can\nenhance the accuracy and efficiency of Space Saving in identifying heavy hitters, top k, and estimating flow\nfrequencies.\n1 Introduction\nThe paradigm of learning-augmented algorithms, also known as algorithms with predictions, combines machine\nlearning and traditional algorithms. This approach aims to enhance algorithms and data structures by leveraging\npredictions from machine learning models. Such learning-augmented algorithms have demonstrated theoretical\nand empirical benefits across numerous areas, including scheduling [2, 3, 32, 38], caching [27, 36], and approximate\nfrequency estimation [21]. In the realm of approximate frequency estimation, the goal is to utilize predictive models\nto improve the accuracy of the estimation.\nA stream of packets flowing through a network or a specific switch can be divided into multiple flows, each\nidentified by a unique set of attributes. For example, the 5-tuple consists of source IP, source port, destination IP,\ndestination port, and protocol is often used as a flow identifier. Two fundamental problems in network statistics\nare determining the frequency of a flow, i.e., counting the number of packets with a given flow identifier that passes\nthrough the network or switch, and identifying the heavy hitter flows (the most frequent ones), as these flows often\nhave the most significant impact. These monitoring capabilities are desirable for various network algorithms and\ndomains, such as load balancing [12], routing, fairness [24], intrusion detection [34], [17], caching [14], and policy\nenforcement [39].\nIn a modern network, as the number of flows can be massive, providing precise answers to queries regarding flow1\nfrequencies and identifying heavy hitters2is often prohibitively costly. To that end, streaming algorithms process\nthe data in a single pass while efficiently estimating flow frequencies with minimal storage space are essential. These\nalgorithms build a sketch ; a compact data structure that extracts statistical information in one pass on the entire\ndata stream.\nStandard frequency estimation algorithms can be broadly classified into two categories: hashing-based and\ncompeting-counter-based approaches. The hashing-based approach [6, 10, 23] hashes data items into buckets, po-\ntentially leading to collisions among different items. It then calculates item frequencies based on the number of\nitems hashed into each bucket. In contrast, the competing-counter-based approach [28,30] maintains a fixed num-\nber of counters that at any time tracks a subset of the input items, without hashing and corresponding collisions.\n1The terms flow and item are used interchangeably.\n2The terms heavy hitters and frequent items are used interchangeably.arXiv:2406.16270v1  [cs.DS]  24 Jun 2024\n\nFigure 1: In many practical distributions (such as Zipfian, shown here using logarithmic scales on both axes), there\nare many low-frequency items.\nThese data stream items “compete” for the limited counters, with the algorithm aiming to allocate counters to the\nmost frequent items. For identifying heavy hitters and top k, while hashing-based approaches require additional\nstructures like heaps, competing-counter-based approaches do not necessitate such auxiliary components, making\nthem more space-efficient. Indeed, competing-counter-based algorithms have been shown to be more space-efficient\nthan hashing-based algorithms, both empirically and asymptotically [8, 9], as hashing-based algorithms require\nallocating significantly more counters and space than theoretical lower bounds to mitigate the impact of hash colli-\nsions on counter accuracy. Additionally, competing-counter-based algorithms offer deterministic error guarantees,\nin contrast to hashing-based, which are randomized.\nHsu et al. [21] introduced a learned hashing-based algorithm for frequency estimation that utilizes a heavy hitter\npredictor. When an incoming item is predicted to be a heavy hitter, it is assigned a unique bucket for accurate\ncounting. Items not predicted as heavy hitters are processed using a conventional sketching algorithm. This\napproach reduces collisions between heavy hitters and less frequent items, leading to improved overall accuracy. It\nis important to note that heavy hitter predictors may exhibit errors, and thus relying solely on them to identify heavy\nhitters can violate error guarantees. To our knowledge, there is a lack of research on learned competing-counter-\nbased algorithms. Unfortunately, directly applying the same strategy to competing-counter-based algorithms by\nsimply assigning all counters to predicted heavy hitters leads to unbounded error estimation. For instance, a falsely\npredicted heavy hitter item occupies a counter that could have been used for a real heavy hitter. (Recall that\ncompeting-counter-based approaches have a fixed number of counters.) Even worse, a true heavy hitter might be\nmistakenly identified as a non-heavy hitter and consequently ignored. Unlike learned hashing-based algorithms\nthat focus only on heavy-hitter items, we propose an approach that employs a predictor to filter out noise caused\nby items predicted to have low frequencies as in many network traces [20] and practical distributions (e.g. Zipfian),\nmany items are with one occurrence. (Figure 1).\nIn this paper, we introduce a learned competing-counter-based algorithm for identifying heavy hitters, top k,\nand flow frequency estimation. We focus on the Space Saving algorithm, although our approach could be applied to\nany competing-counter-based algorithm, such as the MG algorithm [30]. We present Learned Space Saving (LSS),\na novel technique that leverages machine learning predictions to guide the competition for limited counters among\ndata items. Specifically, LSS aims to exclude predicted “weak” or low-frequency items from the competition, while\nensuring that predicted “strong” or heavy-hitter items remain in the competition. In this way, the Space Saving\naccuracy is improved.\nLSS is designed to be resilient against prediction errors. Our LSS method employs a filtering mechanism\nto exclude predicted low-frequency items, resulting in a “cleaner” data structure. To ensure robustness against\nincorrectly predicting a high-frequency item as a low-frequency item, we employ a Counting Bloom filter that tracks\nthese items and ensures an item is not consistently ignored just because it is (repeatedly) predicted as low frequency.\nWhen using the heavy hitter predictor, we divide the counters into fixed and mutable counters. A predicted heavy\nhitter can be allocated a fixed counter, to achieve an accurate count. But fixed counters are limited, so if there\nare excessive incorrect heavy hitter predictors, the mutable counters allow for a standard Space Saving algorithm\non items after the fixed counters are filled. We break LSS into two variants, LSS-LF and LSS-HH, each utilizes a\ndistinct learning model and is designed to exhibit resilience against prediction errors.\nOur contributions are:\n\n•We propose LSS, a learning-based approach for identifying heavy hitters, top kand frequency estimation,\nimproving Space Saving’s accuracy and designed to be robust against prediction errors (Section 3).\n•We break down LSS into LSS-LF (Section 4) and LSS-HH (Section 5), showing each component’s robustness.\n•We present theorems providing insight into potential gains\n•We propose LSS+, a variation of LSS offering higher update throughput by relaxing deterministic to proba-\nbilistic approximation guarantees (Section 6).\n•We implement and evaluate LSS and variants (Section 7) on synthetic and real-world datasets (Internet\ntraffic, web search); LSS demonstrated up to 18% better top-k precision, 24% higher heavy hitter recall, and\n34% lower RMSE for frequency estimation compared to Space Saving under certain configurations.\n2 Background\n2.1 Preliminaries\nGiven a universe U, astream S=u1, u2, . . .∈ UNis a sequence of arrivals from the universe. (We assume the\nstream is finite here.) We define the frequency of an item iinSas the number of items corresponding to iinS\nand denote this quantity by fi.\nWe seek algorithms that support the following operations:\n•ADD( i): given an element i∈ U, append itoS.\n•Query( i): return an estimate bfioffi\nA weighted stream consists of tuples of the form ( ui, wi), where uirepresents the item’s id and wiits (non-\nnegative) weight. At each step, a new tuple is added to the stream. In this setting, the weight wiis added to the\ncorresponding item’s frequency. Frequency estimation algorithms typically assume unweighted updates, such as\nclick streams, while others assume weighted updates, such as network traffic volumes. In this paper, we focus on\nthe unweighted updates model for ease of exposition, although our approach applies to weighted data streams as\nwell, with straightforward modifications.\nDefinition 1. An algorithm solves ϵ-Frequency if given any Query (i)it returns bfisatisfying\nfi≤bfi≤fi+Nϵ.\nA (randomized) algorithm is said to solve ϵ-Frequency with probability 1−δif, for any ichosen before the stream is\nprocessed, Query (i)returns bfisatisfying the above bound with probability 1−δ. Similarly, a (randomized) algorithm\nsolves ϵ-Frequency in expectation if given any Query (i)it returns bfisatisfying\nfi≤E[bfi]≤fi+Nϵ.\nDefinition 2. An algorithm solves (ϵ, θ)-HeavyHitters if it returns a set of items B, such that for every item i: if\nfi≥θN, then i∈B, and if fi≤(θ−ϵ)Nthen i /∈B. An algorithm solves (ϵ, θ)-HeavyHitters with probability\n1−δif it returns a set of items Bsatisfying the above conditions with probability 1−δ.\nDeterministic solutions (competing-counter-based) [28, 30] for the ϵ-HeavyHitters problem ensure the identifi-\ncation of all items with sufficiently large counts, but potentially may include some items with counts smaller than\nthe given heavy hitter threshold. In contrast, hashing-based [6,10] for the ϵ-HeavyHitters problem may introduce\na probability of failure.\n2.2 Robustness in Learning-Augmented Algorithms\nAs mentioned, a learned-augmented algorithm combines traditional algorithms with machine learning models. (It\nshould be noted, however, that this approach generally treats the machine learning models as black boxes, allowing\nit to work with any model that provides usable predictions.) However, machine learning methods are inherently\nimperfect and may exhibit errors, including substantial and unexpected errors. A key question is how can we use\npredictions while maintaining robustness, which refers to ability of an algorithm to maintain reasonable performance\n\nTable 1: List of Symbols\nSymbol Meaning\nS The data stream\nU The universe of elements\nN The stream size\nu[x...y]The partial stream ux, ux+1,···, uy\nϵ An estimate accuracy parameter\nθ The heavy hitters threshold\nτ LSS+ robustness probability\nt LSS-LF threshold\nδ(u1···x)SS error vector after processing u1...x\nℓ Number of single occurrence items\nfpr Bloom filter false positive rate\neven if the predictions are simply wrong [27,33]. Ensuring robustness is essential because machine learning models\nare rarely perfect in practice. There are several reasons why learned models may exhibit errors. First, most models\nare trained to perform well on average by minimizing expected losses. In doing so, they may reduce errors on the\nmajority of inputs at the expense of increasing errors on outlier cases. Additionally, generalization error guarantees\nonly hold when the training and test samples are drawn from the same distribution. If this assumption is violated,\ndue to distribution drift or adversarial samples, the predictions can vary from the ground truth.\nOne general approach for learned-augmented algorithms to try to achieve robustness is to fall back on the\ntraditional algorithm when the model is inaccurate. This requires being able to notice inaccurate predictions and\nchange to the traditional algorithm quickly and effectively. Another related approach, which we use here, is to\nfind ways to limit the damage that can be caused by incorrect predictions by using additional algorithm or data\nstructure.\n2.3 Space Saving\nThe Space Saving (SS) algorithm [28] is a competing-counter algorithm that provides frequency estimation for data\nstream items. Space Saving maintains a set of kentries, denoted by T, each entry has an associated item iand\ncounter, and we use cito denote the counter value associated with item iif any exists. When k=1\nϵ, Space Saving\nestimates the frequency of any item with an additive error less than Nϵwhere Nis the stream size.\nWhen an item iis encountered within the stream that is in the set T, the algorithm increments its corresponding\ncounter ci. In cases where the item iis not present in Tand the size of Tis less than k, the algorithm adds ito the\nsetTand initializes its count to 1 ( ci= 1). Otherwise, when the item iis not in Tand the size of Thas reached\nk, Space Saving identifies an item jwithin Twith the minimum non-zero count cj, denoted by minCount . The\nalgorithm then executes an update in which ciis assigned the value of cj+ 1, and the set Tis updated to replace\nitem jwith item i. To estimate the frequency of an item, if the item is in Tthen we report its count. Otherwise,\nwe report the smallest counter stored in T. Pseudocode for SS is presented (in black) in Algorithm 1.\nSpace Saving satisfies the following properties (the first two properties are proved in [28] while the latter is\nproved in [43]):\nLemma 1. Space Saving with k=ϵ−1counters ensures that after processing Ninsertions, the minimum count of\nall monitored items is no more thanN\nk=Nϵ, i.e, minCount < Nϵ .\nLemma 2. All items with frequency larger than or equal to minCount are in the set T, the set of items with\nassociated counters.\nLemma 3. Space Saving with ϵ−1counters can estimate the frequency of any item with an additive error less than\nNϵ.\n3 Learned Space Saving\nWe propose a learning-based approach to enhance the accuracy of the Space Saving algorithm, called Learned Space\nSaving (LSS). Our approach allows for two types of predictors: one for identifying low-frequency items and another\nfor identifying heavy hitters. Either or both can be used. Alternatively, a single stronger predictor capable of\n\nFigure 2: Overview of LSS, combining the approaches of LSS-LF and LSS-HH. LSS-LF utilizes a low-frequency\npredictor to exclude infrequent items. LSS-HH divides entries into fixed and mutable entries, using a heavy hitters\npredictor to allocate fixed entries for frequent items while processing remaining items through the mutable entries.\nTo mitigate the impact of prediction errors, LSS-LF utilizes a Counting Bloom Filter (CBF), while LSS-HH sets a\nlimit ( khh) on the number of fixed entries (represented in green color).\npredicting item frequencies could be employed. To accommodate the availability of predictors in different systems,\nwe divide LSS into two components: LSS with low frequency predictor (LSS-LF) when only the low-frequency\npredictor is available, and LSS with heavy hitter predictor (LSS-HH) when only the heavy hitters predictor is\navailable. When both predictors are available, or when a single predictor can predict item frequencies, we utilize\nthe full LSS, which combines the strengths of the individual components.\nLSS-LF employs a filtering mechanism to exclude low-frequency items, resulting in a “cleaner” data structure. It\nutilizes a Counting Bloom Filter to ensure robustness against prediction errors. LSS-HH takes a different approach\nby dividing the Space Saving counters into two categories: fixed counters and mutable counters. It uses a heavy\nhitters predictor to allocate fixed counters for items identified as heavy hitters. The remaining items are processed\nusing the traditional Space Saving algorithm, using mutable counters for maintaining their counts. This overall\napproach is illustrated in Figure 2.\nOur design decouples the robustness logic (highlighted in green in Figure 2) from the remaining algorithm\nlogic. Figure 3 illustrates the algorithms presented in this paper. We have employed a color-coding scheme to aid\nreaders in visualizing and distinguishing between the presented algorithms. We represent four algorithms in the\npseudo-code of Algorithm 1, each uniquely represented using specific colors, as follows: Space Saving is represented\nin black. LSS-LF combines black and brown colors. LSS-HH combines black and blue colors. Finally LSS combines\nall of them.\n4 LSS-LF: Learned Space Saving without Low Frequencies\nWe claim that by selectively removing low-frequency items and properly adjusting their estimation, we can enhance\nthe accuracy of the Space Saving algorithm. We first consider the case of items appearing only once in a stream\nand “remembering” their count as 1 as a special case. Then, we present the general case of removing low-frequency\nitems up to toccurrences.\nOur intuition is that occurrences of items that appear once “disturb” the accuracy of items that are in T. Space\nSaving could set a counter of an item that is potentially frequent (i.e. heavy hitter) to zero by encountering a single\noccurrence item. Eventually, the replaced frequent item will return to the Space Saving table (from correctness),\nbut in this case, the “counter history” will be lost, causing inaccuracy in the counters.\nWe accordingly introduce LSS-LF (Learned Space Saving without Low Frequencies), which aims to exclude\nitems that are predicted to have up to toccurrences from being inserted into Space Saving. We refer to the special\ncase when t= 1 by LSS-LFS (Learned Space Saving Low Frequency Singles).\n4.1 Addressing Items with Single Occurrence\nWe focus on this particular case for two primary reasons: First, for some distributions (see Figure 4), the majority\nof low-frequency items are items that occur only once. Second, in certain setups, obtaining a predictor for single\n\nAlgorithm 1 LSS\n1:Initialization :T←∅,\n2:CBF – Counting Bloom filter ,t– CBF threshold\n3:freqPredictor – frequency predictor\n4:fixedEntries ←0\n5:HH - heavy hitters predictor, khh- number of allowed fixed entries\n6:function Insert (i)\n7: iffreqPredictor(i) > torCBF.GET (i)> tthen\n8: ifi∈Tthen\n9: ci←ci+ 1\n10: else if |T|< kthen\n11: T←T∪ {i}\n12: ci←1\n13: ifHH(i) and fixedEntries ≤khhthen\n14: mark entry of cias fixed entry\n15: fixedEntries ←fixedEntries + 1\n16: end if\n17: else\n18: j←argminj∈Tunasncj\n19: ci←cj+ 1\n20: T←T∪ {i} \\ {j}\n21: ifHH(i) and fixedEntries ≤khhthen\n22: mark entry of cias fixed entry\n23: fixedEntries ←fixedEntries + 1\n24: end if\n25: end if\n26: else\n27: CBF.ADD (i)\n28: end if\n29:end function\n30:function Query (i)\n31: ifi∈Tthen\n32: return ci+t\n33: else\n34: return minj∈Tcj+t\n35: end if\n36:end function\noccurrence items is more achievable than a general low-frequency predictor. Even by addressing this special case\nof single occurrences, we demonstrate improved performance.\nLSS-LFS employs a predictor that, for a specific item i, predicts if item ihas a single occurrence. Note that we\nperform predictions for every incoming arrival.\nUnless some mitigating structure is added, ignoring predicted single items can lead to unbounded errors. For\nexample, if a heavy hitter is predicted incorrectly as a single item, this item is excluded from the Space Saving,\nviolating the error guarantee (Lemma 3).\nWe therefore add a structure to ensure that when the predictor suggests an item be ignored, a verification process\nis used to determine if the item has been previously ignored, and ignores the prediction if that has occurred. Our\napproach uses a Bloom Filter (BF) [4,5]3(we consider further generalizations later). A BF is a simple and efficient\nprobabilistic data structure that determines whether an item is part of a set, which does not yield false negatives\nbut can yield false positives.\nSpecifically, we keep a Bloom filter of predicted single occurrence items previously ignored to ensure robustness.\nIf the predictor predicts an item is a single occurrence, but the Bloom filter returns that it has been previously\nignored, this indicates the item is not a single occurrence item, we disregard the predictor’s suggestion and instead\ninsert the item into the Space Saving. Otherwise, if the item is not found in the Bloom filter, we add it to the filter\n3The BF could be replaced with any similar filter structure, such as a cuckoo filter or ribbon filter [11,15].\n\nFigure 3: An illustration of our algorithms. Consider an input stream Sof 11 items as shown, where Ais predicted\nas a heavy hitter, the first arrival of Bis predicted (incorrectly) as low frequency, D, E, F, G, H, I, J, K are predicted\n(correctly) as low-frequency items. In this example, low-frequency items take over the SS counters. In LSS-HH,\nby allocating a fixed entry for the predicted heavy hitter A, the remaining counters again are taken over by low-\nfrequency items. LSS-LF, however, ensures that low-frequency items do not dominate by filtering them. When B\narrives after being previously stored in the filter, it is tracked again. Note that LSS-LF uses fewer counters than\nSS and LSS-HH to account for the additional memory required for the filter.\nand ignore it. For example, in Figure 3, the items B, D, E, F, G, H, I, J, K are predicted as low-frequency items\nand are eliminated from being inserted into the table. Instead, they are placed in the filter. Bis tracked again\nas it was incorrectly predicted as a low frequency item. As a result, the counters remain dedicated to tracking\nnon-low-frequency items.\nThe effect is that faulty predictions will not cause us to ignore an item more than once, because Bloom filters\nare designed so that there are no false negatives. However, we may underestimate the count of mispredicted items\nby 1, because an item is not included in the Space Saving when it is inserted in the Bloom filter. We compensate\nfor this by adding 1 to the query result of the Space Saving.\n4.1.1 Robustness Result\nWe first present theoretical results showing that LSS-LFS is robust, in the sense that it cannot behave too much\nworse than the corresponding algorithm that does not use predictions (denoted by SS).\nTheorem 1. LetSSbe an algorithm for (ϵ−1\nN)-Frequency. Then LSS-LFS (Algorithm 1) solves ϵ-Frequency.\nProof. For any item iand stream size N, we have\nbfi≜SS(ϵ−1\nN).Query (i) + 1. (1)\nThat is, our estimate is the query result for ifrom SS, which is an algorithm for ( ϵ−1\nN)-Frequency, with at most one\noccurence of iremoved from the stream SSprocesses and an extra count of 1 added back in. It follows the smallest\npossible return value is ( fi−1) + 1 = fi,and the largest possible return value is\u0000\nfi+\u0000\nϵ−1\nN\u0001\nN\u0001\n+ 1 = fi+ϵN,\nproving the claim.\nNote that we alternatively could have used a ϵ-Frequency algorithm for SSand not added 1 to the query result,\nyielding a largest possible return value bfioffi+ϵN, but might yield a smallest value of fi−1 (as a lower bound).\nWe chose the presentation of Theorem 1 to maintain the same guarantees as without predictions.\nTheorem 2. Given k=ϵ−1available counters and the availability of perfect predictions for items with a single\noccurrence, let ℓrepresents the count of such items. By utilizing this predictor specifically to filter out single-\noccurrence items, Space Saving can estimate the frequency of any item with additive error less than (N−ℓ)ϵ, where\nNdenotes the size of the stream.\nProof. In the Space Saving algorithm, the minimal counter value is always greater than or equal to the ratio of\nthe number of inserted items to the number of counters. Since we eliminate items with single occurrences, the\ntotal number of inserted items becomes N−ℓ. By using ϵ−1counters, we ensure that the minimal counter value\nis greater than or equal to ( N−ℓ)ϵ. The rest of the proof follows immediately from Lemma 1, 3.\nOur LSS-LFS algorithm will, of course, not be as good as Theorem 2 even with perfect predictions, because it\ndoes not assume predictions are perfect, and uses a Bloom filter for robustness. However, the following theorem\nprovides insight into why we expect strong benefits, given suitably good predictors. In stating the theorem, we\n\nrecall that a Bloom filter has a false positive rate, which corresponds to the probability a non-set item creates a\nfalse positive, and further, for any sequence of MBloom filter queries, the fraction of false positives is at most\n(1+ν)Mfor any constant νwith high probability (that is, O(M−α) for any constant α; note the result may require\nsufficiently large M).\nTheorem 3. Given SSwithk=ϵ−1to solve the ϵ-Frequency problem and given perfect predictions of whether an\nitem is a single occurrence. LSS-LFS using kcounters can estimate the frequency of any item with additive error\nless than (N−ℓ·(1−(1 +ν)fpr)ϵwith high probability, where fpris the false positive rate of its Bloom filter and\nν >0can be any suitable constant.\nProof. Based on Algorithm 1, LSS-LFS uses a predictor to filter out arrivals with a single occurrence. A perfect\npredictor is assumed, but without knowing that it is perfect. As LSS-LFS utilizes a Bloom filter to handle the\npredictor’s imperfections, which may yield false positives that include unnecessary items in its Space Saving instance.\nThe expected number of false positives can be expressed as ℓ·fpr, and with high probability the number of false\npositives is at most (1 + ν)ℓ·fpr. Following the same logic as in Theorems 2, it can be deduced that the total\nnumber of inserted items, with high probability, is N−ℓ·(1−(1 +ν)fpr).\n4.2 Addressing Items Up to t Occurrences\nFor given t, the previous approach can be generalized by addressing items with up to toccurrences, where tis a\nthreshold that depends on the specific distribution.\nWe now present LSS-LF, a generalization of LSS-LFS. We use a predictor that, for a specific item iand threshold\nt, predicts if item ihas more than toccurrences. LSS-LF replaces the standard Bloom filter with a Counting Bloom\nFilter (CBF) [16] which expands on the Bloom filter capabilities by tracking how many times each item appears in\na multi-set. The CBF tracks the number of times an item is inserted into it, so we can (approximately) count how\nmany times an item that is predicted to have less than toccurrences within the stream, and then we place items\ninto the Space Saving if their CBF count reaches a predefined threshold t. This generalizes the previous algorithm\nusing the Bloom filter which corresponds to the case t= 1. Now, however, our underestimation might be as much\nast, from tinsertions until reaching the CBF threshold. We correct this by adding tto the value returned by SS\nin LSS-LF.\nTheorem 4. Given a threshold t, letSSbe an algorithm for (ϵ−t\nN)-Frequency. Then LSS-LF solves ϵ-Frequency.\nProof. The proof follows the same logic as that of Theorem 1. Here, however, we may lose up to tentries for an\nitem iwithin the stream.\n5 LSS-HH: Learned Space Saving with Heavy Hitters\nHere, we explore the case where a predictor for heavy hitters is included. We propose LSS-HH as an approach that\nexploits the heavy hitter predictor for better performance. LSS-HH assigns a set number of entries specifically for\npredicted heavy hitters in the Space Saving algorithm. As a result, these entries are protected from replacement,\neven when a new item appears. The other remain mutable, as with the original Space Saving algorithm; in\nparticular, when an item appears that replaces the item with the lowest counter value, it does so only with respect\nto the mutable entries. We discuss additions to this approach to ensure it remains robust in the face of potential\nprediction errors.\nInaccurate predictions can lead to a scenario where small items erroneously take up fixed entries and reduce the\navailable fixed entries originally intended for actual heavy hitters. As a result, heavy hitters are subject to frequent\ninclusion and removal from the structure, resulting in unbounded errors. To ensure the robustness of our approach,\nwe set a limit on the maximum number of fixed entries ( khh). In cases where the number of predicted heavy hitters\nexceeds the defined threshold, the initial heavy hitters encountered in the data stream fill the fixed entries, while\nthe remaining heavy hitters compete on regular mutable entries. In contrast, if the number of predicted heavy\nhitters is less than the fixed entries, those entries will be given to non-heavy hitters. This is due to the fact that\nwe only mark entries as fixed when a heavy hitter shows up. In Figure 3, there is one fixed entry allocated for A,\nwhich is predicted as a heavy hitter. This prevents low-frequency items from replacing the counter tracking the\nheavy hitter A.\nTheorem 5. Given the availability of perfect predictions for heavy hitters, LSS-HH using kcounters, where khh\namong them are fixed entries, provides exact frequencies for khhheavy hitters (zero errors) and estimates the\nfrequency of the other items with additive error less thanN−khhθN\nk−khhwhere θis the heavy hitters’ threshold.\n\nAlgorithm 2 LSS+\n1:Initialization :T←∅,\n2:BF–Bloomfilter\n3:function Insert (i)\n4: ifitem iis not predicted as low-frequency then\n5: Learned Space Saving Insertion\n6: else\n7: ifUniform (0,1)≤τthen ▷With probability τ\n8: ifBF.CONTAINS (i)then\n9: ifi∈Tthen\n10: ci←ci+τ−1\n11: else if |T|< kthen\n12: T←T∪ {i}\n13: ci←τ−1\n14: else\n15: j←argminj∈Tcj\n16: ci←cj+τ−1\n17: T←T∪ {i} \\ {j}\n18: end if\n19: else\n20: BF.ADD (i)\n21: end if\n22: end if\n23: end if\n24:end function\n25:function Query (i)\n26: result = Learned Space Saving Query\n27: return result +τ−1\n28:end function\nProof. Since heavy hitters have dedicated counters, their error is zero. For the remaining items, we have k−khh\n(mutable) counters. As before, the minimal counter value among the mutable counters is always greater than or\nequal to the ratio of the number of inserted items to the number of counters. However, since we eliminate heavy\nhitter items, each appearing at least θNtimes, the total number of inserted items becomes N−khhθN. Thus, the\nrest of the proof follows immediately from Lemma 1 and Lemma 3.\nIf the frequency distribution of items is Zipfian, then the number of θ-heavy hitters is at most1\nθlnn, where n\nrepresents the total number of unique items in the stream (Remark 9.13 in [21]). In this case, khhcan be set to\n1\nθlnn.\n6 LSS+: Faster LSS\nThe inclusion of the filter (BF or CBF), while crucial for robustness, can potentially slow down overall performance\nand increase memory usage. We present LSS+ that offers potentials for speedup at the cost of loosening our\ndeterministic approximation guarantees to probabilistic guarantees. For simplicity, we describe LSS+ for the case\nof filtering out single occurrences, but the approach also applies to filtering out items with up to toccurrences.\nWhenever the predictor suggests an item will have a single occurrence, LSS checks for the item’s presence in\nthe Bloom filter, and either adds it to the Bloom filter if it is not there, or adds 1 to the item count in the Space\nSaving data structure if it is. LSS+ mitigates this overhead by selectively executing this step with probability τ,\nand adding τ−1to the item count in the Space Saving data structure if needed. (For convenience, we assume here\nthatτ−1is an integer, to avoid floating point arithmetic, but one could develop alternative implementations.)\nIn the special case where τequals 1, LSS+ coincides with LSS. When τis less than 1, it takes on average τ−1\npredictions that an item will have a single occurrence before checking the Bloom filter, reducing accesses to and\nmemory required for the Bloom filter. However, now the guarantees for the algorithm are only in expectation;\n\n(a) Web search\n (b) IP\n (c) Zipf α= 1.3\nFigure 4: Histogram of predicted frequencies up to 50 of the used datasets (Log scale). For Web search and IP\ndatasets, we use the learned model described in Section 7, for the Zipf distribution, we used the simulated predictor\nwith p= 0.9.\nwhen an item is in the Bloom filter, its expected count increases by one each time such a prediction occurs. The\nvalue of the parameter τin LSS+ can be chosen to balance between robustness and efficiency. This optimization\nis handled in Line 7 of Algorithm 2.\nAs LSS+ checks the Bloom filter probabilistically, it makes sense to consider the expected value of the query\nresponse; we refer to this as solving the query problem in expectation.\nTheorem 6. LetSSbe an algorithm for (ϵ−τ−1\nN)-Frequency. Then LSS+ solves ϵ-Frequency in expectation.\nProof. The proof follows similar logic as that of Theorem 1. The return value of LSS+ is\nbfi≜SS(ϵ−τ−1\nN).Query (i) +τ−1. (2)\nEach time item iis encountered, if the prediction says it will appear again in the stream, its count is increased\nby 1 deterministically. If the prediction says it has a single occurrence, but it is in the Bloom filter, its expected\ncount increases by 1, as it goes up by τ−1with probability τ(when the Bloom filter is checked). The only times\nthe expected count does not increase by 1 on each appearance is when the item is not in the Bloom filter and it is\npredicted to be a single occurrence; this occurs an expected τ−1times before the item is put in the Bloom filter.\nIt follows that E[bfi]≥fiandE[bfi]≤fi+\u0010\nϵ−τ−1\nN\u0011\nN+τ−1=fi+ϵN.\n7 Evaluation\nOur evaluation examines the effectiveness of LSS in comparison to SS. To this end, we aim to address the following\nquestions:\n•How robust is LSS to prediction errors? We consider two extreme cases: all items are predicted as low\nfrequency, and all items are predicted as heavy hitters. We also examine the effect of prediction error\nprobability on LSS accuracy.\n•How does LSS accuracy compare against SS on various applications, such as frequency estimation, identifying\ntop-k items, and finding heavy hitters?\n•How should we adjust parameters (e.g. the number of fixed entries, filter size) depending on the specific task:\nfrequency estimation, identifying top-k items, and identifying heavy hitters)?\n•How much does LSS+ improve the update throughput compared to LSS?\nDatasets:\n•IP Trace Datasets: : We use the anonymized IP trace streams collected from CAIDA [20]. The traffic data\nis collected at a backbone link of a Tier 1 ISP between Chicago and Seattle in 2016. Each recording session\nlasts approximately one hour, with around 30 million packets and 1 million unique flows observed within each\nminute.\n\n•Web Search Query Datasets: We use the AOL query log dataset [1], which comprises 21 million search\nqueries collected from 650 thousand anonymized users over a 90-day period. The dataset contains 3.8 million\nunique queries, each consisting of a multi-word search phrase.\n•Synthetic Datasets: We generated synthetic datasets following the Zipf [35] distribution with varying\nskewness levels, each dataset contains 10 million items.\nImplementation and Computation Platform: We implemented the learned versions of Space Saving in\nPython 3.7.6. The evaluation was performed on an AMD EPYC 7313 16-Core Processor with an NVIDIA A100\n80GB PCIe GPU, running Ubuntu 20.04.6 LTS with Linux kernel 5.4.0-172-generic, and TensorFlow 2.4.1.\nThe Learned Model: We follow the implementation of the learned model in [21] and adapt it using TensorFlow\n2.4.1 for the discussed datasets CAIDA [20] and AOL [1]. For the CAIDA dataset, we train a neural network to\npredict the log of the packet counts for each flow. The model takes as input the IP addresses, ports, and protocol\ntype of each packet. We employ Recurrent Neural Networks (RNNs) with 64 hidden units to encode IP addresses\nand extract the final states as features. Ports are encoded by two-layer fully-connected networks with 16 and 8\nhidden units. The encoded IP and port vectors are concatenated with the protocol type, and this combined feature\nvector is used for frequency estimation via a two-layer fully-connected network with 32 hidden units. For the AOL\ndataset, we construct the predictor by training a neural network to predict the number of times a search phrase\nappears. To process the search phrase, we train an RNN with LSTM cells that take the characters of a search\nphrase as input. The final states encoded by the RNN are fed to a fully connected layer to predict the query\nfrequency. The model is trained on a subset of data to identify properties that correlate with item frequencies\ninstead of memorizing specific items. This trained model is then tested on a separate dataset.\nSimulated Frequency Prediction: For synthetic datasets, given true frequencies, we generate predicted\nfrequencies.We simulate a predictor as follows: given a threshold tand probability p, items are classified as either\nsmall (true count < t) or big (true count ≥t). With probability 1 −p, an item is mispredicted where small (big)\nitems are mispredicted as big (small) items, and their predicted count is randomly chosen from the set of big (small)\nitem counts. With probability p, the prediction of an item is its true count multiplied by a factor that slightly varies\naround 1, within a range defined by the noise level (default 5%).In addition, for items below the threshold t, a small\nprobability (default 1%) controls the likelihood that the added noise will cause these items to be predicted above\nt. For CAIDA and AOL datasets, we use the learned predictor from Section 7. When demonstrating robustness\non real datasets, we employ controlled prediction with a simulated predictor to introduce non-perfect predictions.\nParameter Setting: Our approach does not aim to optimize every parameter thoroughly. In practice, param-\neters can be fine-tuned based on knowledge of data distribution, or by iterative refinement over time, benefiting\nfrom our scheme’s inherent robustness. Here we state the default parameters we used in the experiments unless\nstated otherwise. For a fair comparison with the same memory consumption as SS, we allocated 90% of the memory\nconsumed by SS counters to the LSS counters and the remaining 10% to the filter (CBF), we refer to this as filter\nratio in the experiments. When using fixed counters, particularly for finding heavy hitters, we designated 10% of\nthe total counters as fixed counters. (We also explore the effects of varying the number of fixed counters.) Since\nwe do not assume a prior knowledge of the dataset, we set a low frequency threshold t= 4. To set the threshold\nfor identifying heavy hitters, we used the theoretical error guarantee ϵbased on the used memory (Lemma 1). The\ndefault value for this threshold is chosen to be θ= 0.25ϵ. To simulate periodic queries throughout the data stream,\nwe execute a query at intervals of every 1000 arrivals. The reported accuracy is the average across all windows.\n7.1 Problems and Metrics\nWe explored three problems: (1) detecting heavy hitters in the stream, i.e., items whose frequency exceeds a\ngiven threshold; (2) finding the top kmost frequent items in the stream, where kis given; and (3) estimating the\nfrequencies of individual items in the stream. Our error metrics are:\nRoot Mean Square Error (RMSE) for Frequency Estimation : measures the square root of the average\nsquared differences between the estimated frequency and actual frequency. RMSE =q\n1\nnPn\ni=1(fi−ˆfi)2.\nPrecision for Top-k : ratio of the number of correctly reported instances to the number of reported instances\n(TP\nTP+FP), where TPis the true positive and FPis the false positive.\nRecall for Heavy Hitters : ratio of the number of correctly reported instances to the number of correct\ninstances (TP\nTP+FN) where FNis the false negative.\nOperations performance: insertions or queries per second.\n\n(a) All predictions\nare 1\n(b) All predictions\nare heavy hitters\n(c) Predictions accu-\nracy\n(d) Top-k vs. fixed\ncounters\n(e) Heavy hitters vs.\nfixed counters\nFigure 5: (a-c) Robustness of LSS using web search dataset (a) precision of top-k ( k= 10) with all predictions\nas 1 (b) recall of finding heavy hitters when all predictions are heavy hitters (c) precision of top-k vs. prediction\naccuracy p. (d-e) Impact of fixed counters on top-k ( k= 64) and heavy hitters using web search dataset.\n7.2 End-to-End Performance\nData Skew To show how the numerous less frequent tail items could collectively dominate the counters in a space-\nsaving algorithm, we present in Figure 4 histograms for the frequency range of 1 to 50, plotted on a logarithmic\nscale. The scale highlights that there are a large number of low-frequency items. We observe a sharp peak at the\nlowest frequency (1) and as the predicted frequencies increase, the number of items decreases roughly exponentially.\nPredictions overhead The inference time of the used predictors is 2.8 microseconds per item on a single GPU\nwithout optimization, which ensures minimal impact on throughput. To evaluate memory overhead, we calculated\nthe total number of parameters and corresponding memory requirements for each predictor model, which depends on\nthe dataset. For the network flow predictor, the model has 167,521 parameters: 76,800 for IP address embeddings,\n82,176 for encoding ports, and 8,545 for the frequency estimation network. With 32-bit float representations,\nthis translates to around 0.67 MB of memory. The AOL dataset predictor is even more lightweight, with 7,680\nparameters: 4,000 for character embeddings (RNN with LSTM cells), and 3,680 for the fully-connected layer,\nusing approximately 0.03 MB of memory. Thus, the prediction overhead, which includes inference and memory, is\ntherefore small. Note that inference overhead is expected to be less significant in the future [25] due to specialized\nhardware such as Google TPUs, hardware accelerators, and network compression [18], [40], [7], [19]. Furthermore,\nNvidia has predicted that GPUs will get 1000x faster by 2025. In terms of memory, there is a growing research\nfield, TinyML [13], focused on creating tiny machine learning models for efficient on-device execution. It involves\nmodel compression techniques and high-performance system design for efficient ML. Here we present one example\nof predictors, which can be treated as black boxes without focusing on their internal functioning; our approach\ncan therefore be used with any suitable and efficient learning scheme that yields a predictor, given the rapid\nadvancements in machine learning research.\nRobustness Figure 5 shows the robustness of LSS that our theorems suggest using the web search dataset with\na simulated predictor. Figure 5a evaluates the precision of top-k as function of the consumed space when k= 10,\nwhere the prediction for every item arrival is 1. Even in this extreme case, we observe only a small degradation\nin accuracy compared to SS. This degradation arises because we do not benefit from filtering since the filter is\noverloaded with items, causing items to be frequently replaced in the SS table. Additionally, the filter consumes\nsome memory, so less memory is available for tracking items compared to SS. At the other extreme, Figure 5b\nshows the recall of finding heavy hitters when every item is predicted as a heavy hitter. In this scenario, the fixed\nentries in the counters could be filled with non-heavy hitter items, leaving fewer counters available for tracking the\nactual heavy hitters. This results in a decreased recall rate. However, once again, the decrease is small. Figure 5c\nillustrates the precision rate of the top-k task as a function of the prediction accuracy ( p), as explained in the\nsimulated predictor. When p= 0, it implies that all items are mispredicted, in which case SS yields a higher recall\nrate than LSS because the predictions provide no benefit. As pincreases, LSS outperforms SS, and in the other\nextreme, when p= 1 (perfect prediction), LSS achieves around 50% improvement in precision compared to SS.\nAccuracy vs. Fixed Counters Figures 5d, 5e show the recall of finding top-k items and heavy hitters as a\nfunction of the number of fixed counters using the web search dataset with the learned model. As this number\nincreases, the recall for identifying top-k items decreases because the fixed counters may be populated with heavy\nhitters that are not among the top-k items. Since these entries are fixed in a first-come manner, the top items may\n\n(a) Web, Top-k\n (b) Web, HH\n (c) IP, Top-k\n (d) IP, HH\n (e) Zipf, Top-k\n (f) Zipf, HH\nFigure 6: Precision and recall vs. memory of identifying top-k ( k= 64) and heavy hitters. LSS+ configured with\nτ= 0.5. We use web search, IP and Zipf ( α= 1.3) datasets.\n(a) Web search\n (b) IP\n (c) Zipf α= 1.3\n (d) Update\n (e) as function of τ\nFigure 7: RMSE vs. memory of frequency estimation using web search, IP and Zipf ( α= 1.3) datasets.\nnot be placed in the fixed counters. Allocating fixed counters for heavy hitters that are not top-k items results\nin fewer mutable counters for tracking top-k. However, as the number of fixed counters increases, the recall for\ndetecting heavy hitters improves since having fixed counters dedicated to tracking heavy hitters aligns with this\nobjective. Thus, for finding top-k items, we set the number of fixed entries to zero, while for finding heavy hitters,\nwe allocated 10% of the counters as fixed counters.\nAccuracy vs. Memory We examine the accuracy of SS, LSS, and LSS+ (with τ= 0.5) across three tasks:\nfinding top-k items, identifying heavy hitters, and frequency estimation using web search, IP, and synthetic datasets.\nThe accuracy is evaluated as a function of the memory used. Figure 6 shows the results for top-k and heavy hitter\nidentification tasks. As expected, higher available memory results in improved precision and recall rates. For the\ntop-k task, no fixed entries were used. LSS and LSS+ achieve better precision than SS when finding top-k items\nand better recall when identifying heavy hitters. LSS slightly outperforms LSS+, as one might expect; here for\nboth the filters we allocate allocate 10% of the memory. Figure 7 illustrates the RMSE of frequency estimation and\nshows the accuracy of each variant, LSS-LF and LSS-HH, separately. As expected, increasing memory consumption\ndecreases RMSE. We observe that each variant improves the accuracy compared to SS, and the combined usage of\ntechniques in LSS achieves higher accuracy in frequency estimation.\nAccuracy vs. Filter Size Figure 8a presents the impact of the filter ratio on the precision of top-k ( k= 64)\nitems using the IP dataset. In this experiment, we keep the threshold tfixed to default ( t= 4) and vary only the\nfilter ratio. Allocating less space to the filter does not affect LSS’s correctness but it leads to a higher false positive\nrate for the filter. As a result, more items are included in the space-saving table, which affects accuracy and makes\nthe approach more similar to falling back on the traditional space-saving algorithm.\nAccuracy vs. Stream Length Figure 8b shows the recall of finding heavy hitters using 217bit memory using\nweb search dataset. At the beginning of the stream, LSS maintains an accurate result (high recall rate 1) by\neffectively filtering out low-frequency items. As the stream grows larger, medium items also accumulate, leading\nto a decrease in the recall rate for both methods.\nAccuracy vs. tFigure 8c displays the RMSE vs. t using a synthetic dataset with α= 1.3 and p= 0.9. The\nRMSE decreases until a certain point and then increases. This behavior is due to the increasing false positive rate\nof the filter at larger values of t, which is related to the number of low-frequency items and the filter size. In general,\nwith larger memory (here we used 219bit memory), the filter size can be increased proportionally, allowing it to\n\n(a) IP, Top-k\n (b) Web search, HH\n (c) Synthetic, Fre-\nquency\n(d) Synthetic, Top-k\nFigure 8: Impact of parameters (a) Precision vs. filter ratio of identifying top-k frequent item using IP dataset\n(b) recall rate for identifying heavy hitters in a web search dataset, focusing on the initial part of the stream (c)\nRMSE vs. t using synthetic dataset ( α= 1.3) (d) Precision vs. αof identifying top-k frequent item using synthetic\ndataset (d) Update operation runtime using IP dataset. We use the BF implementation from [29], t= 1 for LSS\nand LSS-CBF, τ= 0.5 for LSS+; (e) in relation with τ.\nhandle more low-frequency items. However, the number of low-frequency items depends on the data distribution.\nIf prior knowledge of the distribution is available, the filter size and tcan be adjusted.\nAccuracy vs. αUsing synthetic datasets with p= 0.9, Figure 8d shows the precision of top-k ( k= 124) vs. α,\nthe skewness parameter of a Zipfian distribution. As αincreases, the distribution becomes more skewed, with a\nhigher concentration of low-frequency items (“heavier tail”). LSS has improvements over SS until a certain point\n(α= 2). After this point, LSS’s precision starts to decrease due to the saturation of the filter with low-frequency\nitems, resulting in higher false positive rates. When α≥2.2, LSS has lower precision than SS since the filter\nbecomes ineffective and fewer counters are allocated to the Space Saving table compared to SS.\nPerformance Comparison Figure 7d examines the update performance of SS, LSS, LSS-HH, LSS-CBF and\nLSS+ algorithms using the IP dataset. We set t= 1 and use [29] and have not optimized further. A key\nconsideration in comparing LSS to SS is the computational cost of inserting elements into the Bloom filter versus\nintegrating them into the Space-Saving data structure. When an item is inserted or queried within a Bloom filter,\nadditional hash computations take place. The performance of LSS degrades slightly due to the fact that insertion\ninto the Bloom filter is less efficient than updating the Space-Saving data structure. Meanwhile, LSS+, configured\nwith τ= 0.5, noticeably outperforms both the aforementioned versions. This superior performance is due to its\nability to minimize the number of insertions to the Bloom filter and to the SS. We skip query speed below since the\ndiscussed algorithms have the same query process. Figure 7e examines the update performance of LSS and LSS+\nas a function of the parameter τ. (These experiments all use 32-bit floating point counters.) As τincreases, LSS+\nsaves more Bloom filter operations, resulting in improved update performance up to τ= 0.8. Following this, LSS+\nshows a slight drop in performance compared to LSS.\n8 Related Works\nThere are many algorithms proposed in the literature for frequency estimation. top-k, the frequent elements prob-\nlem, and their variations. See [8] for a survey. Algorithms for these problems fall into two main classes [28]: (de-\nterministic) competing-counter-based techniques and (randomized) hashing-based techniques. competing-counter-\nbased techniques (e.g. Space Saving) maintain a separate counter for each item within the monitored set, a subset\nof the stream. The counters for monitored items are updated when they appear in the stream. In contrast, when\nthere is no counter for the observed item, the item is either ignored or some algorithm-dependent action is taken.\nhashing-based techniques (e.g. Count-Min Sketch [10]) use competing-counter-based bitmaps to estimate all items’\nfrequencies rather than monitoring a subset. Each item is hashed into a space of counters using a family of hash\nfunctions, and every arrival within the stream updates the counters. [26] proposed discarding approximately in-\nfrequent items in the entire data stream setup using multiple LRU queues that code the item IDs. This approach\nis based on the assumption that items that have been infrequent for a long period are unlikely to become frequent\nlater. However, this assumption may not hold true for every dataset.\nAlgorithms with predictions is, as we have stated, a rapidly growing area. The site [41] contains a collection of\nover a hundred papers on the topic. The seminal work [25] focused on employing machine learning to refine indexing\ndata structures and devise enhanced algorithms for data management, and there are now numerous works on this\n\ntheme. For example, [31] analyzed and enhanced learned Bloom filters as described in [25]. The work presented\nin [42] introduced a learned range filter for range queries on numerical datasets. The study in [37] explored the\npotential benefits of replacing traditional hash functions with learned models, aiming to minimize collisions and\nboost performance.\nThe idea of using predictions to specifically improve frequency estimation algorithms appears to have originated\nwith [21], where they augmented a learning oracle of the heavy hitters into frequency estimation hashing-based\nalgorithms. Later [22] explored the power of such an oracle, showing that it can be applied to a wide array of\nproblems in data streams.\n9 Conclusion\nIdentifying heavy hitters and estimating the frequencies of flows are fundamental tasks in various network domains.\nRecent works have explored the use of machine learning techniques to enhance algorithms for approximate frequency\nestimation problems. However, these studies have focused only on the hashing-based approach, which may not be\nbest for identifying heavy hitters. In this work, we have presented a novel learning-based approach for identifying\nheavy hitters, top k, and flow frequency estimation. We have applied this approach to the well-known Space\nSaving algorithm, which we have called Learned Space Saving (LSS). Our approach is designed to be resilient\nagainst prediction errors, as machine learning methods are inherently imperfect and may exhibit errors. We have\ndemonstrated the benefits of our design both analytically and empirically. Experimental results on real-world\ndatasets highlight that LSS achieves higher recall and precision rates, as well as improved root mean squared error\n(RMSE) compared to the traditional Space Saving algorithm.\nAcknowledgments\nWe thank Sandeep Silwal and ChonLam Lao for their assistance with the evaluation setup. Rana Shahout was\nsupported in part by Schmidt Futures Initiative and Zuckerman Institute. Michael Mitzenmacher was supported\nin part by NSF grants CCF-2101140, CNS-2107078, and DMS-2023528.\n\nReferences\n[1] Web Search Query Log . https://jeffhuang.com/search_query_logs/ .\n[2] Yossi Azar, Stefano Leonardi, and Noam Touitou. Flow time scheduling with uncertain processing time. In\nSamir Khuller and Virginia Vassilevska Williams, editors, STOC ’21: 53rd Annual ACM SIGACT Symposium\non Theory of Computing, Virtual Event, Italy, June 21-25, 2021 , pages 1070–1080. ACM, 2021.\n[3] Yossi Azar, Stefano Leonardi, and Noam Touitou. Distortion-oblivious algorithms for minimizing flow time. In\nJoseph (Seffi) Naor and Niv Buchbinder, editors, Proceedings of the 2022 ACM-SIAM Symposium on Discrete\nAlgorithms, SODA 2022, Virtual Conference / Alexandria, VA, USA, January 9 - 12, 2022 , pages 252–274.\nSIAM, 2022.\n[4] Burton H Bloom. Space/time trade-offs in hash coding with allowable errors. Communications of the ACM ,\n13(7):422–426, 1970.\n[5] Andrei Broder and Michael Mitzenmacher. Network applications of Bloom filters: A survey. Internet mathe-\nmatics , 1(4):485–509, 2004.\n[6] Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams. In Interna-\ntional Colloquium on Automata, Languages, and Programming , pages 693–703. Springer, 2002.\n[7] Yu-Hsin Chen, Tushar Krishna, Joel S Emer, and Vivienne Sze. Eyeriss: An energy-efficient reconfigurable\naccelerator for deep convolutional neural networks. IEEE journal of solid-state circuits , 52(1):127–138, 2016.\n[8] Graham Cormode and Marios Hadjieleftheriou. Finding frequent items in data streams. Proceedings of the\nVLDB Endowment , 1(2):1530–1541, 2008.\n[9] Graham Cormode and Marios Hadjieleftheriou. Methods for finding frequent items in data streams. The\nVLDB Journal , 19:3–20, 2010.\n[10] Graham Cormode and Shan Muthukrishnan. An improved data stream summary: the count-min sketch and\nits applications. Journal of Algorithms , 55(1):58–75, 2005.\n[11] Peter C Dillinger and Stefan Walzer. Ribbon filter: practically smaller than bloom and xor. arXiv preprint\narXiv:2103.02515 , 2021.\n[12] Gero Dittmann and Andreas Herkersdorf. Network processor load balancing for high-speed links. In Proceed-\nings of the 2002 International Symposium on Performance Evaluation of Computer and Telecommunication\nSystems , volume 735. Citeseer, 2002.\n[13] Lachit Dutta and Swapna Bharali. Tinyml meets iot: A comprehensive survey. Internet of Things , 16:100461,\n2021.\n[14] Gil Einziger, Roy Friedman, and Ben Manes. Tinylfu: A highly efficient cache admission policy. ACM\nTransactions on Storage (ToS) , 13(4):1–31, 2017.\n[15] Bin Fan, Dave G Andersen, Michael Kaminsky, and Michael D Mitzenmacher. Cuckoo filter: Practically\nbetter than bloom. In Proceedings of the 10th ACM International on Conference on emerging Networking\nExperiments and Technologies , pages 75–88, 2014.\n[16] Li Fan, Pei Cao, Jussara Almeida, and Andrei Z Broder. Summary cache: a scalable wide-area web cache\nsharing protocol. IEEE/ACM transactions on networking , 8(3):281–293, 2000.\n[17] Pedro Garcia-Teodoro, Jesus Diaz-Verdejo, Gabriel Maci´ a-Fern´ andez, and Enrique V´ azquez. Anomaly-based\nnetwork intrusion detection: Techniques, systems and challenges. computers & security , 28(1-2):18–28, 2009.\n[18] Song Han, Junlong Kang, Huizi Mao, Yiming Hu, Xin Li, Yubin Li, Dongliang Xie, Hong Luo, Song Yao,\nYu Wang, et al. Ese: Efficient speech recognition engine with sparse lstm on fpga. In Proceedings of the 2017\nACM/SIGDA International Symposium on Field-Programmable Gate Arrays , pages 75–84, 2017.\n[19] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A Horowitz, and William J Dally. Eie:\nEfficient inference engine on compressed deep neural network. ACM SIGARCH Computer Architecture News ,\n44(3):243–254, 2016.\n\n[20] Paul Hick. CAIDA Anonymized Internet Trace, equinix-chicago, 2016.\n[21] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation algorithms.\nInInternational Conference on Learning Representations , 2019.\n[22] Tanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, and David P Woodruff. Learning-augmented data stream\nalgorithms. In International Conference on Learning Representations , 2019.\n[23] Cheqing Jin, Weining Qian, Chaofeng Sha, Jeffrey X Yu, and Aoying Zhou. Dynamically maintaining frequent\nitems over a data stream. In Proceedings of the twelfth international conference on Information and knowledge\nmanagement , pages 287–294, 2003.\n[24] Abdul Kabbani, Mohammad Alizadeh, Masato Yasuda, Rong Pan, and Balaji Prabhakar. Af-qcn: Approxi-\nmate fairness with quantized congestion notification for multi-tenanted data centers. In 2010 18th ieee sym-\nposium on high performance interconnects , pages 58–65. IEEE, 2010.\n[25] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned index structures.\nInProceedings of the 2018 international conference on management of data , pages 489–504, 2018.\n[26] Yuanpeng Li, Feiyu Wang, Xiang Yu, Yilong Yang, Kaicheng Yang, Tong Yang, Zhuo Ma, Bin Cui, and Steve\nUhlig. Ladderfilter: Filtering infrequent items with small memory and time overhead. Proceedings of the ACM\non Management of Data , 1(1):1–21, 2023.\n[27] Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice. Journal of the\nACM (JACM) , 68(4):1–25, 2021.\n[28] Ahmed Metwally, Divyakant Agrawal, and Amr El Abbadi. Efficient computation of frequent and top-k\nelements in data streams. In Database Theory-ICDT 2005: 10th International Conference, Edinburgh, UK,\nJanuary 5-7, 2005. Proceedings 10 , pages 398–412. Springer, 2005.\n[29] Michael Axiak. pybloomfiltermmap3. https://github.com/prashnts/pybloomfiltermmap3 .\n[30] Jayadev Misra and David Gries. Finding repeated elements. Science of computer programming , 2(2):143–152,\n1982.\n[31] Michael Mitzenmacher. A model for learned bloom filters and optimizing by sandwiching. Advances in Neural\nInformation Processing Systems , 31, 2018.\n[32] Michael Mitzenmacher. Queues with small advice. In Michael Bender, John Gilbert, Bruce Hendrickson, and\nBlair D. Sullivan, editors, Proceedings of the 2021 SIAM Conference on Applied and Computational Discrete\nAlgorithms, ACDA 2021, Virtual Conference, July 19-21, 2021 , pages 1–12. SIAM, 2021.\n[33] Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. Communications of the ACM ,\n65(7):33–35, 2022.\n[34] Biswanath Mukherjee, L Todd Heberlein, and Karl N Levitt. Network intrusion detection. IEEE network ,\n8(3):26–41, 1994.\n[35] David MW Powers. Applications and explanations of zipf’s law. In New methods in language processing and\ncomputational natural language learning , 1998.\n[36] Dhruv Rohatgi. Near-optimal bounds for online caching with machine learned advice. In Shuchi Chawla,\neditor, Proceedings of the 2020 ACM-SIAM Symposium on Discrete Algorithms, SODA 2020, Salt Lake City,\nUT, USA, January 5-8, 2020 , pages 1834–1845. SIAM, 2020.\n[37] Ibrahim Sabek, Kapil Vaidya, Dominik Horn, Andreas Kipf, Michael Mitzenmacher, and Tim Kraska. Can\nlearned models replace hash functions? Proceedings of the VLDB Endowment , 16(3):532–545, 2022.\n[38] Ziv Scully, Isaac Grosof, and Michael Mitzenmacher. Uniform bounds for scheduling with job size estimates. In\nMark Braverman, editor, 13th Innovations in Theoretical Computer Science Conference, ITCS 2022, January\n31 - February 3, 2022, Berkeley, CA, USA , volume 215 of LIPIcs , pages 114:1–114:30. Schloss Dagstuhl -\nLeibniz-Zentrum f¨ ur Informatik, 2022.\n\n[39] Joel Sommers, Paul Barford, Nick Duffield, and Amos Ron. Accurate and efficient sla compliance monitoring.\nInProceedings of the 2007 conference on Applications, technologies, architectures, and protocols for computer\ncommunications , pages 109–120, 2007.\n[40] Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. Efficient processing of deep neural networks: A\ntutorial and survey. Proceedings of the IEEE , 105(12):2295–2329, 2017.\n[41] https://algorithms-with-predictions.github.io/ . Algorithms with predictions github repository.\n[42] Kapil Vaidya, Subarna Chatterjee, Eric Knorr, Michael Mitzenmacher, Stratos Idreos, and Tim Kraska. Snarf:\na learning-enhanced range filter. Proceedings of the VLDB Endowment , 15(8):1632–1644, 2022.\n[43] Fuheng Zhao, Divyakant Agrawal, Amr El Abbadi, and Ahmed Metwally. Spacesaving+-: An optimal\nalgorithm for frequency estimation and frequent items in the bounded deletion model. arXiv preprint\narXiv:2112.03462 , 2021.",
  "textLength": 63682
}