{
  "paperId": "d506faedcf30dc47b25d715bf1c42f6bb9bf2f87",
  "title": "SmartPQ: An Adaptive Concurrent Priority Queue for NUMA Architectures",
  "pdfPath": "d506faedcf30dc47b25d715bf1c42f6bb9bf2f87.pdf",
  "text": "SmartPQ : An Adaptive Concurrent Priority Queue\nfor NUMA Architectures\n*Christina Giannoula†*Foteini Strati‡†Dimitrios Siakavaras†Georgios Goumas†Nectarios Koziris†\n†National Technical University of Athens‡ETH Zürich\nConcurrent priority queues are widely used in important\nworkloads, such as graph applications and discrete event\nsimulations. However, designing scalable concurrent priority\nqueues for NUMA architectures is challenging. Even though\nseveral NUMA-oblivious implementations can scale up to a\nhigh number of threads, exploiting the potential parallelism\nofinsert operation, NUMA-oblivious implementations scale\npoorly in deleteMin -dominated workloads. This is because\nall threads compete for accessing the same memory locations,\ni.e., the highest-priority element of the queue, thus incurring\nexcessive cache coherence traffic and non-uniform memory\naccesses between NUMA nodes. In such scenarios, NUMA-\naware implementations are typically used to improve system\nperformance on a NUMA system.\nIn this work, we propose an adaptive priority queue,\ncalled SmartPQ .SmartPQ tunes itself by switching between a\nNUMA-oblivious and a NUMA-aware algorithmic mode to\nachieve high performance under all various contention sce-\nnarios. SmartPQ has two key components. First, it is built\non top of NUMA Node Delegation ( Nuddle ), a generic low-\noverhead technique to construct efficient NUMA-aware data\nstructures using any arbitrary concurrent NUMA-oblivious\nimplementation as its backbone. Second, SmartPQ integrates\na lightweight decision making mechanism to decide when to\nswitch between NUMA-oblivious andNUMA-aware algorith-\nmic modes. Our evaluation shows that, in NUMA systems,\nSmartPQ performs best in all various contention scenarios\nwith 87.9% success rate, and dynamically adapts between\nNUMA-aware andNUMA-oblivious algorithmic mode, with\nnegligible performance overheads. SmartPQ improves perfor-\nmance by 1.87 ×on average over SprayList, the state-of-the-\nartNUMA-oblivious priority queue.\n1. Introduction\nConcurrent data structures are widely used in the soft-\nware stack, i.e., kernel, libraries and applications. Prior\nworks [9, 10, 14, 65] discuss the need for efficient and scalable\nconcurrent data structures for commodity Non-Uniform Mem-\nory Access (NUMA) architectures. Pointer chasing data struc-\ntures such as linked lists, skip lists and search trees have inher-\nently low contention, since concurrent threads search for differ-\nent elements during their operations. Recent works [10,16,70]\nhave shown that lock-free algorithms [22, 24, 29, 36, 52, 55]\nof such data structures can scale to hundreds of cores. On the\nother hand, data structures such as queues and stacks typically\nincur high contention, when accessed by many threads. In\nthese data structures, concurrent threads compete for the same\nmemory elements (locations), incurring excessive traffic and\n* Christina Giannoula and Foteini Strati are co-primary authors.\n100-0 70-30 50-50 30-70 0-100\nInsert - DeleteMin (%)051015202530Throughput (Mops/sec)Initial Size = 1024, Key Range = 2048, Running Threads = 64\n214.2\nNUMA-oblivious\nNUMA-awareFigure 1: Throughput achieved by a NUMA-oblivious [2, 34]\nand a NUMA-aware [65] priority queue, both initialized with\n1024 keys. We use 64 threads that perform a mix of insert and\ndeleteMin operations in parallel, and the key range is set to 2048\nkeys. We use allNUMA nodes of a 4-node NUMA system, the\ncharacteristics of which are presented in Section 4.\nnon-uniform memory accesses between nodes of a NUMA\nsystem.\nIn this work, we focus on priority queues, which are widely\nused in a variety of applications, including task scheduling in\nreal-time and computing systems [79], discrete event simula-\ntions [49, 75] and graph applications [39, 43, 76], e.g., Single\nSource Shortest Path [12] and Minimum Spanning Tree [60].\nSimilarly to skip-lists and search trees, priority queues have\ntwo main operations: insert anddeleteMin . The insert opera-\ntion, concurrent priority queues typically exhibits high levels\nof parallelism and low-contention, since threads may work\non different parts of the data structure. Therefore, concurrent\nNUMA-oblivious implementations [6, 45, 47, 64, 67, 74, 77, 80]\ncan scale up to a high number of threads. In contrast, in\ndeleteMin operation, allthreads compete for deleting the\nhighest-priority element of the queue, thus competing for the\nsame memory locations (similarly to queues and stacks), and\ncreating a contention spot. In deleteMin -dominated workloads,\nconcurrent priority queues typically incur high-contention and\nlow parallelism. To achieve higher parallelism, relaxed pri-\nority queues have been proposed in the literature [2, 30], in\nwhich deleteMin operation returns an element among the first\nfew(high-priority) elements of the priority queue. However,\nsuch NUMA-oblivious implementations are still inefficient in\nNUMA architectures, as we demonstrate in Section 4. There-\nfore, to improve performance in NUMA systems, NUMA-\naware implementations have been proposed [10, 65].\nWe examine NUMA-aware andNUMA-oblivious concur-\nrent priority queues with a wide variety of contention scenarios\nin NUMA architectures, and find that the performance of a\npriority queue implementation is becoming increasingly de-\npendent on both the contention levels of the workload and the\nunderlying computing platform. This is illustrated in Figure 1,\n1arXiv:2406.06900v1  [cs.DC]  11 Jun 2024\n\nwhich shows the throughput achieved by a NUMA-oblivious\nand a NUMA-aware priority queue using a 4-node NUMA\nsystem. Even though in a insert -dominated scenario, e.g.,\nwhen having 100% insert operations, the NUMA-oblivious im-\nplementation achieves significant performance gains over the\nNUMA-aware one, when contention increases, i.e., the percent-\nage of deleteMin operations increases, the NUMA-oblivious\nimplementation incurs non-negligible performance slowdowns\nover the NUMA-aware priority queue. We conclude that none\nof the priority queues performs best across allcontention\nworkloads.\nOur goal in this work is to design a concurrent priority\nqueue that (i) achieves the highest performance under all\nvarious contention scenarios, and (ii) performs best even when\nthe contention of the workload varies over time.\nTo this end, our contribution is twofold. First, we introduce\nNUMA Node Delegation (Nuddle ), a generic technique to ob-\ntainNUMA-aware data structures, by effectively transforming\nanyconcurrent NUMA-oblivious data structure into the corre-\nsponding NUMA-aware implementation. In other words, Nud-\ndleis a framework to wrap any concurrent NUMA-oblivious\ndata structure and transform it into an efficient NUMA-aware\none. Nuddle extends ffwd [65] by enabling multiple server\nthreads, instead of only one, to execute operations in parallel\non behalf of client threads. In contrast to ffwd, which aims to\nprovide single threaded data structure performance, Nuddle\ntargets data structures which are able to scale up to a number\nof threads such as priority queues.\nSecond, we propose SmartPQ , an adaptive concurrent pri-\nority queue that achieves the highest performance under all\ncontention workloads and dynamically adapts itself over time\nbetween a NUMA-oblivious and a NUMA-aware algorithmic\nmode. SmartPQ integrates (i) Nuddle toefficiently switch be-\ntween the two algorithmic modes with very low overhead, and\n(ii) a simple decision tree classifier , which predicts the best-\nperforming algorithmic mode given the expected contention\nlevels of a workload.\nFigure 2 presents an overview of SmartPQ , where we use\nthe term base algorithm to denote anyarbitrary concurrent\nNUMA-oblivious data structure. SmartPQ relies on three key\nideas. First, client threads can execute operations using ei-\ntherNuddle (NUMA-aware mode) or its underlying NUMA-\noblivious base algorithm ( NUMA-oblivious mode). Second,\nSmartPQ incorporates a decision making mechanism to de-\ncide upon transitions between the two modes. Third, SmartPQ\nexploits the fact that the actual underlying implementation\nofNuddle is a concurrent NUMA-oblivious data structure.\nClient threads in both algorithmic modes access the data struc-\nture with the same concurrency strategy, i.e., with no actual\nchange in the way data is accessed, and synchronization is\nimplemented. Therefore, SmartPQ switches from one mode\nto another with nosynchronization points between transitions.\nWe evaluate a wide range of contention scenarios and\ncompare Nuddle andSmartPQ with state-of-the-art NUMA-\noblivious [2, 47] and NUMA-aware [65] concurrent priority\nqueues. We also evaluate SmartPQ using synthetic bench-\nmarks that dynamically vary their contention workload over\ntime. Our evaluation shows that SmartPQ adapts between\nits two algorithmic modes with negligible performance over-\nNuddle\nBase \nAlgorithmNuddle\nBase \nAlgorithmBase \nAlgorithmNUMA -aware\nMode\nNUMA -oblivious\nMode\nclassifierFigure 2: High-level overview of SmartPQ .SmartPQ dynamically\nadapts its algorithm to the contention levels of the workload\nbased on the prediction of a simple classifier.\nheads, and achieves the highest performance in allcontention\nworkloads with 87.9% success rate.\nThis paper makes the following contributions:\n–We propose Nuddle , a generic technique to obtain NUMA-\naware concurrent data structures.\n–We design a simple classifier to predict the best-performing\nimplementation among NUMA-oblivious andNUMA-aware\npriority queues given the contention levels of a workload.\n–We propose SmartPQ , an adaptive concurrent priority queue\nthat achieves the highest performance, even when con-\ntention varies over time.\n–We evaluate Nuddle andSmartPQ with a wide variety of\ncontention scenarios, and demonstrate that SmartPQ per-\nforms best over prior state-of-the-art concurrent priority\nqueues.\n2. NUMA Node Delegation ( Nuddle )\n2.1. Overview\nNUMA Node Delegation ( Nuddle ) is a generic technique\nto obtain NUMA-aware data structures by automatically trans-\nforming anyconcurrent NUMA-oblivious data structure into\nan efficient NUMA-aware implementation. Nuddle extends\nffwd [65], a client-server software mechanism which is based\non the delegation technique [8, 38, 48, 57, 73].\nFigure 3 left shows the high-level overview of ffwd, which\nhas three key design characteristics. First, alloperations per-\nformed by multiple client threads are delegated to one single\ndedicated thread, called server thread. Server thread performs\noperations in the data structure on behalf of its client threads.\nThis way, the data structure remains in the memory hierarchy\nof asingle NUMA node, avoiding non-uniform memory ac-\ncesses to remote data. Second, ffwd eliminates the need for\nsynchronization, since the shared data structure is no longer\naccessed by multiple threads: only a single server thread di-\nrectly modifies the data structure, and therefore, ffwd uses a\nserial asynchronized implementation of the underlying data\nstructure. Third, ffwd provides an efficient communication\nprotocol between the server thread and client threads that min-\nimizes cache coherence overheads. Specifically, ffwd reserves\ndedicated cache lines to exchange request and response mes-\nsages between the client threads and sever thread. Multiple\nclient threads are grouped together to minimize the response\nmessages from the server thread: one response cache line is\nshared among multiple client threads belonging at the same\nclient thread group. For more details, we refer the reader to\nthe original paper [65].\nFigure 3 right presents the high-level overview of Nuddle ,\nwhich is based on three key ideas. First, Nuddle deploys\n2\n\nSerial\nData \nStructureServer \nThreadServer \nThread\nNUMA NodeServer \nThread\nNUMA NodeServer \nThreadServer \nThread. . . Server \nThreadServer \nThread. . . \nNUMA NodeServer \nThreadServer \nThread. . . \nNUMA Node. . . Client \nThreadClient \nThreadClient \nThread\n. . . Client \nThreadClient \nThreadClient \nThread\n. . . Client \nThreadClient \nThreadClient \nThread\n. . . Client Thread Group\nClient \nThreadClient \nThreadClient \nThread\n. . . Client Thread Group\nClient \nThreadClient \nThreadClient \nThread\n. . . Client \nThreadClient \nThreadClient \nThread\n. . . Client \nThreadClient \nThreadClient \nThread\n. . . Client Thread Group\nClient \nThreadClient \nThreadClient \nThread\n. . . Client Thread Group\n. . . Client \nThreadClient \nThreadClient \nThread\n. . . Client \nThreadClient \nThreadClient \nThread\n. . . Client \nThreadClient \nThreadClient \nThread\n. . . Client Thread Group\nClient \nThreadClient \nThreadClient \nThread\n. . . Client Thread Group\nClient \nThreadClient \nThreadClient \nThread\n. . . Client \nThreadClient \nThreadClient \nThread\n. . . Client \nThreadClient \nThreadClient \nThread\n. . . Client Thread Group\nClient \nThreadClient \nThreadClient \nThread\n. . . Client Thread Groupffwd Nuddle\nConcurrent\nData \nStructureFigure 3: High-level design of ffwd [65] and Nuddle .Nuddle\nlocates allserver threads at the same NUMA node to design a\nNUMA-aware scheme, and associates each of them to multiple\nclient thread groups. Nuddle uses the communication protocol\nproposed in ffwd [65].\nmultiple servers to perform operations on behalf of multiple\nclient threads. Specifically, client threads are grouped in client\nthread groups, and each sever thread serves multiple client\nthread groups. This way, multiple server threads concurrently\nperform operations on the data structure, achieving high lev-\nels of parallelism up to a number of server threads. Second,\nNuddle locates all server threads to the same NUMA node to\nkeep the data structure in the memory hierarchy of one single\nNUMA node, and propose a NUMA-aware approach. Client\nthreads can be located at any NUMA node. Third, since multi-\nple servers can concurrently update the shared data structure,\nNuddle uses the concurrent NUMA-oblivious implementation\n(i.e., which includes synchronization primitives when access-\ning the shared data) of the underlying data structure to ensure\ncorrectness. Third, Nuddle employs the same client-server\ncommunication protocol with ffwd to carefully manage mem-\nory accesses and minimize cache coherence traffic and latency.\nffwd targets inherently serial data structures, whose concur-\nrent performance cannot be better than that of single threaded\nperformance. In contrast, Nuddle targets data structures that\ncan scale up to a number of concurrent threads. Priority queue\nis a typical example of such a data structure. In insert opera-\ntion, priority queue can scale up to multiple threads which can\nconcurrently update the shared data. In contrast, deleteMin\noperation is inherently serial: at each time only onethread\ncan update the shared data, since allthreads compete for the\nhighest-priority element of the queue. However, as we men-\ntioned, in relaxed priority queues (e.g., SprayList [2]), even\ndeleteMin operation can be parallelized to some extent.\n2.2. Implementation Details\nFigures 4, 5 and 6 present the code of a priority queue\nimplementation using Nuddle . We denote with red color the\ncore operations of the base algorithm , which is used as the\nunderlying concurrent NUMA-oblivious implementation of\nNuddle . Note that even though in this work we focus on\npriority queues, Nuddle is ageneric framework for any type\nof concurrent data structure.\nHelper Structures. Nuddle includes three helper struc-\ntures (Figure 4), which are needed for client-server commu-\nnication. First, the main structure of Nuddle , called struct\nnuddle_pq , wraps the base algorithm (nm_oblv_set ), andincludes a few additional fields, which are used to associate\nclient thread groups to server threads in the initialization step.\nSecond, each client thread has its own struct client struc-\nture with a dedicated request and a dedicated response cache\nline. The request cache line is exclusively written by the\nclient thread and read by the associated server thread, while\nthe response cache line is exclusively written by the server\nthread and read by all client threads that belong in the same\nclient thread group. Third, each server thread has its own\nstruct server structure that includes an array of requests\n(my_clients ), each of them is shared with a client thread,\nand an array of responses ( my_responses ), each of them is\nshared with all client threads of the same client thread group.\n1#define cache_line_size 128\n2typedef char cache_line[cache_line_size];\n3\n4struct nuddle_pq {\n5 nm_oblv_set ∗base_pq;\n6 int servers , groups , clnt_per_group;\n7 int server_cnt , clients_cnt , group_cnt;\n8 cache_line ∗requests[groups][clnt_per_group];\n9 cache_line ∗responses[groups];\n10 lock ∗global_lock;\n11};\n12\n13struct client {\n14 cache_line ∗request , ∗response;\n15 int clnt_pos;\n16};\n17\n18struct server {\n19 nm_oblv_set ∗base_pq;\n20 cache_line ∗my_clients[], ∗my_responses[];\n21 int my_groups , clnt_per_group;\n22};\nFigure 4: Helper structures of Nuddle .\nInitialization Step. Figure 5 describes the initialization\nfunctions of Nuddle .initPQ() initializes (i) the underlying\ndata structure using the corresponding function of the base\nalgorithm (line 25), and (ii) the additional fields of struct\nnuddle_pq . For this function, programmers need to spec-\nify the number of server threads and the maximum number\nof client threads to properly allocate cache lines needed for\ncommunication among them. Programmers also specify the\nsize of the client thread group (line 27), which is typically\n7 or 15, if the cache line is 64 or 128 bytes, respectively.\nAs explained in ffwd [65], assuming 8-byte return values,\na dedicated 64-byte (or 128-byte) response cache line can\nbe shared between up to 7 (or 15) client threads, because it\nalso has to include one additional toggle bit for each client\nthread. After initializing struct nuddle_pq , each running\nthread calls either initClient() orinitServer() depend-\ning on its role. Each thread initializes its own helper struc-\nture ( struct client orstruct server ) with request and\nresponse cache lines of the corresponding shared arrays of\nstruct nuddle_pq . Server threads undertake client thread\ngroups with a round robin fashion, such that the load associ-\nated with client threads is balanced among them. In function\ninitServer() , it is the programmer’s responsibility to prop-\nerly pin software server threads to hardware contexts (line 56),\nsuch that server threads are located in the same NUMA node,\n3\n\nand the programmer fully benefits from the Nuddle technique.\nMoreover, given that client threads of the same client thread\ngroup share the same response cache line, the programmer\ncould pin client threads of the same client thread group to hard-\nware contexts of the same NUMA node to minimize cache\ncoherence overheads. Finally, since the request and response\narrays of struct nuddle_pq areshared between all threads,\na global lock is used when updating them to ensure mutual\nexclusion.\n23struct nuddle_pq ∗initPQ( int servers , int\nmax_clients) {\n24 struct nuddle_pq ∗pq = allocate_nuddle_pq();\n25 __base_init(pq->base_pq);\n26 pq−>servers = servers;\n27 pq−>clnt_per_group = client_group(\ncache_line_size);\n28 pq−>groups = (max_clients +\n29 pq−>clnt_per_group −1) / pq −>clnt_per_group;\n30 pq−>server_cnt = 0;\n31 pq−>client_cnt = 0;\n32 pq−>group_cnt = 0;\n33 pq−>requests = malloc(groups ∗clnt_per_group);\n34 pq−>responses = malloc(groups);\n35 init_lock(pq −>global_lock);\n36 return pq;\n37}\n38\n39struct client ∗initClient( struct nuddle_pq ∗pq) {\n40 struct client ∗cl = allocate_client();\n41 acquire_lock(pq −>global_lock);\n42 cl−>request = &(pq −>requests[group_cnt][\nclients_cnt]);\n43 cl−>response = &(pq −>responses[group_cnt]);\n44 cl−>pos = pq −>client_cnt;\n45 pq−>client_cnt++;\n46 if(pq−>client_cnt % pq −>clnt_per_group == 0) {\n47 pq−>clients_cnt = 0;\n48 pq−>group_cnt++;\n49 }\n50 release_lock(pq −>global_lock);\n51 return cl;\n52}\n53\n54struct server ∗initServer( struct nuddle_pq ∗pq,\nint core)\n55{\n56 set_affinity(core);\n57 struct server ∗srv = allocate_server();\n58 srv−>base_pq = pq −>base_pq;\n59 srv−>my_groups = 0;\n60 srv−>clnt_per_group = pq −>clnt_per_group;\n61 acquire_lock(pq −>global_lock);\n62 int j = 0;\n63 for(i = 0; i < pq −>groups; i++)\n64 if(i % pq −>servers == pq −>server_cnt) {\n65 srv−>my_clients[j] = pq −>requests[i][0..\ngr_clnt];\n66 srv−>my_responses[j++] = pq −>responses[i];\n67 srv−>my_groups++;\n68 }\n69 pq−>server_cnt++;\n70 release_lock(pq −>global_lock);\n71 return srv;\n72}\nFigure 5: Initialization functions of Nuddle .\nMain API. Figure 6 shows the core functions of Nuddle ,\nwhere we omit the corresponding functions for deleteMin\noperation, since they are very similar to that of insert operation.\nBoth insert anddeleteMin operations of Nuddle have similarAPI with the classic API of prior state-of-the-art priority queue\nimplementations [2, 45, 47, 67]. However, we separate the\ncorresponding functions for client threads and server threads.\nA client thread writes its request to a dedicated request cache\nline (line 75) and then waits for the server thread’s response. In\ncontrast, a server thread directly executes operations in the data\nstructure using the core functions of the base algorithm (line\n82). Moreover, a server thread can serve client threads using\ntheserve_requests() function. A server thread iterates\nover its own client thread groups and executes the requested\noperations in the data structure. The server thread buffers\nindividual return values for clients to a local cache line ( resp\nin lines 92 and 94) until it finishes processing all requests for\nthe current client thread group. Then, it writes all responses to\ntheshared response cache line of that client thread group (line\n96), and proceeds to its next client thread group.\n73int insert_client( struct client ∗cl,\nint key, int64_t value)\n74{\n75 cl−>request = write_req(\"insert\", key, value);\n76 while (cl−>response[cl −>pos] == 0) ;\n77 return cl−>response[cl −>pos];\n78}\n79\n80int insert_server( struct server ∗srv,\nint key, int64_t value)\n81{\n82 return __base_insrt(srv −>base_pq , key, value);\n83}\n84\n85void serve_requests( struct server ∗srv) {\n86 for(i = 0; i < srv −>mygroups; i++) {\n87 cache_line resp;\n88 for(j = 0; j < srv −>clnt_per_group; j++) {\n89 key = srv −>my_clients[i][j].key;\n90 value = srv −>my_clients[i][j].value;\n91 if(srv −>my_clients[i][j].op == \"insert\")\n92 resp[j] = __base_insrt(srv −>base_pq , key,\nvalue);\n93 else if (srv −>my_clients[i][j].op == \"\ndeleteMin\")\n94 resp[j] = __base_delMin(srv −>base_pq);\n95 }\n96 srv−>my_responses[i] = resp;\n97 }\n98}\nFigure 6: Functions used by server threads and client threads to\nperform operations using Nuddle .\n3.SmartPQ\nWe propose SmartPQ , an adaptive concurrent priority queue\nwhich tunes itself by dynamically switching between NUMA-\noblivious andNUMA-aware algorithmic modes, in order to\nperform best in allcontention workloads and at anypoint in\ntime, even when contention varies over time.\nDesigning an adaptive priority queue involves addressing\ntwo major challenges: (i) how to switch from one algorithmic\nmode to the other with low overhead , and (ii) when to switch\nfrom one algorithmic mode to the other.\nTo address the first challenge, we exploit the fact that the\nactual underlying implementation of Nuddle is aconcurrent\nNUMA-oblivious implementation. We select Nuddle , as the\nNUMA-aware algorithmic mode of SmartPQ , and its underly-\n4\n\n15 29\nNumber of Threads0481216Throughput (Mops/sec)Initial Size = 10K, Key Range = 1M\n Insert - DeleteMin (%) = 80-20214.2\nalistarh_herlihy\nNuddle(a)\n2K 10K 100K\nKey Range048121620Throughput (Mops/sec)Initial Size = 100K, Number of Threads = 50\n Insert - DeleteMin (%) = 70-30214.2\nalistarh_herlihy\nNuddle (b)\nFigure 7: Throughput achieved by Nuddle (using 8 server threads) and its underlying NUMA-oblivious base algorithm , i.e., alis-\ntarh_herlihy [2, 34], when we vary (a) the number of threads that perform operations in the shared data structure, and (b) the key\nrange of the workload.\ningbase algorithm , as the NUMA-oblivious algorithmic mode\nofSmartPQ . Threads can perform operations in the data struc-\nture using either Nuddle or its underlying base algorithm , with\nno actual change in the way data is accessed. As a result,\nSmartPQ can switch between the two algorithmic modes with-\noutneeding a synchronization point between transitions, and\nwithout violating correctness.\nTo address the second challenge, we design a simple de-\ncision tree classifier (Section 3.1.2), and train it to select the\nbest-performing algorithmic mode between Nuddle , as the\nNUMA-aware algorithmic mode of SmartPQ , and its underly-\ningbase algorithm , as the NUMA-oblivious mode of SmartPQ .\nFinally, we add a lightweight decision making mechanism in\nSmartPQ (Section 3.2) to dynamically tune itself over time\nbetween the two algorithmic modes. We describe more details\nin next sections.\n3.1. Selecting the Algorithmic Mode\n3.1.1. The Need for a Machine Learning Approach\nSelecting the best-performing algorithmic mode can be\nsolved in various ways. For instance, one could take an empir-\nical exhaustive approach: measure the throughput achieved by\nthe two algorithmic modes for all various contention scenarios\non the target NUMA system, and then use the algorithmic\nmode that achieves the highest throughput on future runs of\nthe same contention workload on the target NUMA system.\nEven though this is the most accurate method, it (i) incurs\nsubstantial overhead and effort to sweep over allvarious con-\ntention workloads, and (ii) would need a large amount of\nmemory to store the best-performing algorithmic mode for all\nvarious scenarios. Furthermore, it is not trivial to construct\na statistical model to predict the best-performing algorithmic\nmode, since the performance of an algorithm is also affected\nby the characteristics of underlying computing platform. Fig-\nure 7 summarizes these observations by comparing Nuddle\nwith its underlying base algorithm in a 4-node NUMA sys-\ntem. For the base algorithm , we use alistarh_herlihy priority\nqueue [2, 34], since this is the NUMA-oblivious implementa-\ntion that achieves the highest performance, according to our\nevaluation (Section 4).\nFigure 7a demonstrates that the best-performing algorithmic\nmode depends on multiple parameters, such as the number of\nthreads that perform operations in the shared data structure.\nWe find that the algorithmic also depends on the size of thedata structure, and the operation workload, i.e., the percent-\nage of insert /deleteMin operations. Specifically, when the\nnumber of threads increases, we may expect that the perfor-\nmance of the NUMA-oblivious alistarh_herlihy degrades due\nto higher contention. In contrast, with 80% insert operations\nwhen increasing the number of threads to 29, alistarh_herlihy\noutperforms Nuddle . This is because the size of the priority\nqueue and the range of keys used in the workload are relatively\nlarge, while the percentage of deleteMin operations is low. In\nthis scenario, threads may not compete for the same elements,\nworking on different parts of the data structure, and thus, the\nNUMA-oblivious alistarh_herlihy achieves higher throughput\ncompared to the NUMA-aware Nuddle .\nFigure 7b demonstrates that the best-performing algorith-\nmic mode cannot be straightforwardly predicted, and also\ndepends on the characteristics of the workload and of the un-\nderlying hardware. In insert -dominated workloads, as the\nkey range increases, threads may update different parts of the\nshared data structure. We might, thus, expect that after a cer-\ntain point of increasing the key range, the NUMA-oblivious\nalistarh_herlihy will always outperform Nuddle , since the\ncontention decreases. However, we note that, even though\nthe performance of Nuddle remains constant, as expected, the\nperformance of alistarh_herlihy highly varies as the key range\nincreases due to the hyperthreading effect. When using more\nthan 32 threads, hyperthreading is enabled in our NUMA sys-\ntem (Section 4). The hyperthreading pair of threads shares the\nL1 and L2 caches, and thus, these threads may either thrash or\nbenefit from each other depending on the characteristics of L1\nand L2 caches (e.g., size, eviction policy), and the elements\naccessed in each operation.\nConsidering the aforementioned non-straightforward behav-\nior, we resort to a machine learning approach as the basis of\nour prediction mechanism.\n3.1.2. Decision Tree Classifier\nWe formulate the selection of the algorithmic mode as\na classification problem, and leverage supervised learning\ntechniques to train a simple classifier to predict the best-\nperforming algorithmic mode for each contention workload.\nFor our classifier, we select decision trees, since they are\ncommonly used in classification models for multithreaded\nworkloads [3, 17, 19, 21, 51, 59, 69, 72], and incur low training\nand inference overhead. Moreover, they are easy to interpret\nand thus, be incorporated to our proposed priority queue (Sec-\n5\n\ntion 3.2). We generate the decision tree classifier using the\nscikit-learn machine learning toolkit [56].\n1) Class Definition: We define the following classes: (a) the\nNUMA-oblivious class that stands for the NUMA-oblivious\nalgorithmic mode, (b) the NUMA-aware class that stands for\ntheNUMA-aware algorithmic mode, and (c) the neutral class\nthat stands for a tie, meaning that either a NUMA-aware or a\nNUMA-oblivious implementation can be selected, since they\nachieve similar performance. We include a neutral class for\ntwo reasons: (i) when using only one socket of a NUMA sys-\ntem, NUMA-aware implementations deliver similar through-\nput with NUMA-oblivious implementations, and (ii) in an\nadaptive data structure, which dynamically switches between\nthe two algorithmic modes, we want to configure a transition\nfrom one algorithmic mode to another to occur when the differ-\nence in their throughput is relatively high, i.e., greater than a\ncertain threshold. Otherwise, the adaptive data structure might\ncontinuously oscillate between the two modes, without deliv-\nering significant performance improvements or even causing\nperformance degradation.\n2) Extracted Features : Table 1 explains the four features of\nthe contention workload which are used in our classifier target-\ning priority queues. We assume that the contention workload\nis known a priori, and thus, we can easily extract the features\nneeded for classification. Section 5 discusses how to on-the-fly\nextract these features.\nFeature Definition\n#ThreadsThe number of active threads\nthat perform operations in the data structure\nSize The current size of the priority queue\nKey_range The range of keys used in the workload\n%insert /deleteMin The percentage of insert /deleteMin operations\nTable 1: The features of the contention workload which are used\nfor classification.\n3) Generation of Training Data: To train our classifier,\nwe develop microbenchmarks, in which threads repeatedly\nexecute random operations on the priority queue for 5 seconds.\nWe select Nuddle , as the NUMA-aware implementation, and\nalistarh_herlihy , as its underlying NUMA-oblivious imple-\nmentation, since this is the best-performing NUMA-oblivious\npriority queue (Section 4). We use a variety of values for the\nfeatures needed for classification (Table 1). Our training data\nset consists of 5525 different contention workloads. Finally,\nwe pin software threads to hardware contexts of the evalu-\nated NUMA system in a round-robin fashion, and thus, the\nclassifier is trained with this thread placement. We leave the\nexploration of the thread placement policy for future work.\n4) Labeling of Training Data: Regarding the labeling of\nour training data set, we set the threshold for tie between the\ntwo algorithmic modes to an empirical value of 1.5 Million\noperations per second. When the difference in throughput\nbetween the two algorithmic modes is less than this threshold,\ntheneutral class is selected as label. Otherwise, we select the\nclass that corresponds to the algorithmic mode that achieves\nthe highest throughput.\nThe final decision tree classifier has only 180 nodes, half of\nwhich are leaves. It has a very low depth of 8, that is the lengthof the longest path in the tree, and thus, a very low traversal\ncost (2-4 ms in our evaluated NUMA system).\n3.2. Implementation Details\nFigure 8 presents the modified code of Nuddle adding\nthe decision making mechanism (using green color) to im-\nplement SmartPQ . We extend the main structure of Nud-\ndle, renamed to struct smartpq , by adding an additional\nfield, called algo , to keep track the current algorithmic\nmode, (either NUMA-oblivious orNUMA-aware ). Simi-\nlarly, struct client andstruct server structures are\nextended with an additional algo field (e.g., line 111), which\nis a pointer to the algo field of struct smartpq . Each ac-\ntive thread initializes this pointer either in initClient() or\ninitServer() depending on its role (e.g., line 119). This\nway, all threads share the same algorithmic mode at any\npoint in time. In struct client , we also add a pointer\nto the shared data structure (line 110), which is used by client\nthreads to directly perform operations in the data structure\nin case of NUMA-oblivious mode. Specifically, we modify\nthe core functions of client threads, i.e., insert_client()\nanddeleteMin_client() , such that client threads either\ndirectly execute their operations in the data structure (e.g., line\n126), or delegate them to server threads (e.g., line 127-128),\nwith respect to the current algorithmic mode. In contrast, the\ncore functions of server threads do not need any modification.\nFinally, we wrap the code of serve_requests function, i.e.,\nthe lines 86-97 of Figure 6, with an if/else statement on the\nalgo field (lines 133, 146 in Fig. 8), such that server threads\npoll at client threads’ requests only in NUMA-aware mode. In\nNUMA-oblivious mode, serve_requests function returns\nwithout doing nothing. This way, programmers do not need\nto take care of calls on this function in their code, when the\nNUMA-oblivious mode is selected.\nThedecisionTree() function describes the interface with\nour proposed decision tree classifier, where the input argu-\nments are associated with its features. In frequent time lapses,\none or more threads may call this function to check if a tran-\nsition to another algorithmic mode is needed. If this is the\ncase, the algo field of struct smartpq is updated (line 154\nin Fig. 8), and SmartPQ switches algorithmic mode, i.e., all\nactive threads start executing their operations using the new\nalgorithmic mode. If the classifier predicts the neutral class\n(line 153), the algo field is not updated, and thus SmartPQ\nremains at the currently selected algorithmic mode.\n4. Experimental Evaluation\nIn our experimental evaluation, we use a 4-socket Intel\nSandy Bridge-EP server equipped with 8-core Intel Xeon CPU\nE5-4620 processors providing a total of 32 physical cores and\n64 hardware contexts. The processor runs at 2.2GHz and each\nphysical core has its own L1 and L2 cache of sizes 64KB\nand 256KB, respectively. A 16MB L3 cache is shared by all\ncores in a NUMA socket and the RAM is 256GB. We use\nGCC 4.9.2 with -O3 optimization flag enabled to compile all\nimplementations.\nOur evaluation includes the following concurrent priority\nqueue implementations:\n6\n\n99struct smartpq {\n100 nm_oblv_set ∗base_pq;\n101 int servers , groups , clnt_per_group;\n102 int server_cnt , clients_cnt , group_cnt;\n103 cache_line ∗requests[groups][clnt_per_group];\n104 cache_line ∗responses[groups];\n105 lock ∗global_lock;\n106 int *algo; // 1: NUMA-oblivious (default),\n2: NUMA-aware\n107 };\n108\n109 struct client {\n110 nm_oblv_set *base_pq;\n111 int *algo;\n112 cache_line ∗request , ∗response;\n113 int clnt_pos;\n114 };\n115\n116 struct client ∗initClient( struct smartpq ∗pq) {\n117 ... lines 40 −49 of Fig. 5 ...\n118 cl->base_pq = pq->base_pq;\n119 cl->algo = &(pq->algo);\n120 release_lock(pq −>global_lock);\n121 return cl;\n122 }\n123\n124 int insert_client( struct client ∗cl,\nint key, float value) {\n125 if(*(cl->algo) == 1) {\n126 return __base_insert(cl->base_pq,key,value);\n127 } else { // *(cl->algo) == 2\n128 ... lines 75 −77 of Fig. 6 ...\n129 }\n130 }\n131\n132 void serve_requests( struct server ∗srv) {\n133 if(*(srv->algo) == 2){\n134 for(i = 0; i < srv −>mygroups; i++) {\n135 cache_line resp;\n136 for(j = 0; j < srv −>clnt_per_group; j++) {\n137 key = srv −>my_clients[i][j].key;\n138 value = srv −>my_clients[i][j].value;\n139 if(srv −>my_clients[i][j].op == \"insert\")\n140 resp[j] = __base_insrt(srv −>base_pq , key,\nvalue);\n141 else if (srv −>my_clients[i][j].op == \"\ndeleteMin\")\n142 resp[j] = __base_delMin(srv −>base_pq);\n143 }\n144 srv−>my_responses[i] = resp;\n145 }\n146 } else\n147 return;\n148 }\n149\n150 void decisionTree( struct server /\nstruct client ∗str, int nthreads ,\nint size , int key_range ,\ndouble insert\\_deleteMin) {\n151 int algo = 0;\n152 ... code for decision tree classifier ...\n153 if (algo != 0) // 0: neutral\n154 *(str->algo) = algo;\n155 }\nFigure 8: The modified code of Nuddle adding the decision mak-\ning mechanism to implement SmartPQ .–alistarh_fraser [2, 24]: A NUMA-oblivious , relaxed prior-\nity queue [2] based on Fraser’s skip-list [24] available at\nASCYLIB library [16].\n–alistarh_herlihy [2, 34]: A NUMA-oblivious , relaxed prior-\nity queue [2] based on Herlihy’s skip-list [34] available at\nASCYLIB library [16].\n–lotan_shavit [47]: A NUMA-oblivious priority queue avail-\nable at ASCYLIB library [16].\n–ffwd [65]: A NUMA-aware priority queue based on the\ndelegation technique [8, 38, 48, 57, 73], which includes only\none server thread to perform operations on behalf of all\nclient threads.\n–Nuddle : Our proposed NUMA-aware priority queue, which\nuses alistarh_herlihy as the underlying base algorithm .\n–SmartPQ : Our proposed adaptive priority queue, which uses\nNuddle as the NUMA-aware mode, and alistarh_herlihy as\ntheNUMA-oblivious base algorithm .\nWe evaluate the concurrent priority queue implementations\nin the following way:\n–Each execution lasts 5 seconds, during which each thread\nperforms randomly chosen operations. We also tried longer\ndurations and got similar results.\n–Between consecutive operations in the data structure each\nthread executes a delay loop of 25 pause instructions. This\ndelay is intentionally added in our benchmarks to better\nsimulate a real-life application, where operations in the data\nstructure are intermingled with other instructions in the\napplication.\n–At the beginning of each run, the priority queue is initialized\nwith elements the number of which is described at each\nfigure.\n–Each software thread is pinned to a hardware context. Hy-\nperthreading is enabled when using more than 32 software\nthreads. When exceeding the number of available hardware\ncontexts of the system, we oversubscribe software threads\nto hardware contexts.\n–We pin the first 8 threads to the first NUMA node, and\nconsecutive client thread groups of 7 client threads each, to\nNUMA nodes in a round-robin fashion.\n–InNUMA-oblivious implementations, any allocation needed\nin the operation is executed on demand, and memory affinity\nis determined by the first touch policy.\n–InNUMA-aware implementations, since our NUMA system\nhas 64-byte cache lines, the response cache line is shared\nbetween up to 7 client threads, using 8-byte return values.\n–InNuddle , the first 8 threads represent server threads. Server\nthreads repeatedly execute the serve_requests function,\nand then a randomly chosen operation until time is up.\n–We have disabled the automatic Linux Balancing [42] to get\nconsistent and stable results.\n–All reported results are the average of 10 independent exe-\ncutions with no significant variance.\n4.1. Throughput of Nuddle\nFigure 9 presents the throughput achieved by concurrent\npriority queue implementations for various sizes and oper-\nation workloads. NUMA-aware priority queue implemen-\ntations, i.e., ffwd andNuddle , achieve high throughput in\ndeleteMin -dominated workloads: Nuddle performs best in\n7\n\n248152229364350571061550246810Insert - DeleteMin (%) = 20-80\n100K elements\n248152229364350571061550246810\n5M elements\n248152229364350571061550246810\n10M elements\n248152229364350571061550246810Insert - DeleteMin (%) = 40-60\n248152229364350571061550246810\n248152229364350571061550246810\n24815222936435057106155051015202530Insert - DeleteMin (%) = 80-20\n24815222936435057106155051015202530\n24815222936435057106155051015202530\nNumber of ThreadsThroughput (Mops/sec)alistarh_fraser alistarh_herlihy lotan_shavit FFWD NuddleFigure 9: Throughput of concurrent priority queue implementations. The columns show different priority queue sizes using the key\nrange of double the elements of each size. The rows show different operation workloads. The vertical line in each plot shows the point\nafter which we oversubscribe software threads to hardware contexts.\nall deleteMin -dominated workloads, while ffwd outperforms\nNUMA-oblivious implementations in the small-sized priority\nqueues (e.g., 100K elements). In large-sized priority queues,\ninsert operations have a larger impact on the total execution\ntime (due to a longer traversal), and thus Nuddle andNUMA-\noblivious implementations perform better than ffwd, since they\nprovide higher thread-level parallelism. Note that ffwd has\nsingle-threaded performance, since at any point in time only\none(server) thread performs operations in the data structure.\nMoreover, as it is expected, the performance of both ffwd\nandNuddle saturates at the number of server threads used\n(e.g., 8 server threads for Nuddle ) to perform operations in\nthe data structure. Finally, we note that the communication\nbetween server and client threads used in NUMA-aware im-\nplementations has negligible overhead; when the number of\nclient threads increases, even though the communication traf-\nfic over the interconnect increases, there is noperformance\ndrop. Overall, we conclude that Nuddle achieves the highest\nthroughput in all deleteMin -dominated workloads, and is the\nmost efficient NUMA-aware approach, since it provides high\nthread-level parallelism.\nOn the other hand, NUMA-oblivious implementations incur\nhigh performance degradation in high-contention scenarios,\nsuch as deleteMin -dominated workloads, when using more\nthan one NUMA node (i.e., after 8 threads). As already dis-cussed in prior works [5,15,25,48,54,81], the non-uniformity\nin memory accesses and cache line invalidation traffic signifi-\ncantly affects performance in high-contention scenarios. In in-\nsert-dominated workloads, which incur lower contention, even\nthough lotan_shavit priority queue still incurs performance\ndegradation when using more than one NUMA nodes of the\nsystem, the relaxed NUMA-oblivious implementations, i.e.,\nalistarh_fraser andalistarh_herlihy priority queues, achieve\nhigh scalability. This is because relaxed priority queues de-\ncrease both (i) the contention among threads, and (ii) the\ncache line invalidation traffic: the deleteMin operation returns\n(with a high probability) an element among the first few (high-\npriority) elements of the queue, and thus, threads do not fre-\nquently compete for the same elements. Finally, we observe\nthatalistarh_herlihy priority queue achieves higher perfor-\nmance benefits over alistarh_fraser priority queue, when we\noversubscribe software threads to the available hardware con-\ntexts of our system. Overall, we find that in insert -dominated\nworkloads, the relaxed NUMA-oblivious implementations sig-\nnificantly outperform the NUMA-aware ones.\nTo sum up, we conclude that there is no one-size-fits-all so-\nlution, since none of the priority queues performs best across\nallcontention workloads. Nuddle achieves the highest through-\nput in high contention scenarios, while alistarh_herlihy per-\nforms best in low and medium contention scenarios. It is thus\n8\n\ndesirable to design a new approach for a concurrent priority\nqueue to perform best under allvarious contention scenarios.\n4.2. Throughput of SmartPQ\n4.2.1. Classifier Accuracy\nWe evaluate the efficiency of our proposed classifier (Sec-\ntion 3.1.2) using two metrics: (i) accuracy, and (ii) mispre-\ndiction cost. First, we define the accuracy of the classifier\nas the percentage of correct predictions, where a prediction\nis considered correct, if the classifier predicts the algorith-\nmic mode (either the NUMA-aware Nuddle or the NUMA-\noblivious alistarh_herlihy ) that achieves the best performance\nbetween the two. We use a test set of 10780 different con-\ntention workloads, where we randomly select the values of\nthe features in each workload. In the above test set, our clas-\nsifier has 87.9% accuracy, i.e., it mispredicts 1300 times in\n10780 different contention workloads. Second, we define the\nmisprediction cost as the performance difference between the\ncorrect (best-performing) algorithmic mode and the wrong\npredicted mode normalized to the performance of the wrong\npredicted mode. Specifically, assuming the throughput of the\nwrong predicted and correct (best-performing) algorithmic\nmode is Yand Xrespectively, the misprediction cost is de-\nfined as ((X–Y)/Y)∗100% . In 1300 mispredicted workloads,\nthe geometric mean of misprediction cost for our classifier is\n30.2%. We conclude that the proposed classifier has high ac-\ncuracy, and in case of misprediction, incurs low performance\ndegradation.\n4.2.2. Varying the Contention Workload\nWe present the performance benefit of SmartPQ in syn-\nthetic benchmarks, in which we vary the contention workload\nover time, and compare it with Nuddle and its underlying\nbase algorithm , i.e., alistarh_herlihy priority queue. In all\nbenchmarks, we change the contention workload every 25\nseconds. In SmartPQ , we set one dedicated sever thread to\ncall the decision tree classifier every second, in order to check\nif a transition to another algorithmic mode is needed. Fig-\nure 10 and Figure 11 show the throughput achieved by all\nthree schemes, when we vary one and multiple features in\nthe contention workload, respectively. Table 2 and Table 3\nshow the features of the workload as they vary during the\nexecution for the benchmarks evaluated in Figure 10 and Fig-\nure 11, respectively. Note that the current size of the priority\nqueue changes during the execution due to successful insert\nanddeleteMin operations.\nWe make three observations. First, as already shown in\nSection 4.1, there is no one-size-fits-all solution, since nei-\ntherNuddle noralistarh_herlihy performs best across all vari-\nous contention workloads. For instance, in Figure 10b, even\nthough the performance of Nuddle remains constant, it out-\nperforms alistarh_herlihy , when having 15 running threads,\ni.e., using 2 NUMA nodes of the system. Second, we observe\nthatSmartPQ successfully adapts to the best-performing algo-\nrithmic mode, and performs best in allcontention scenarios.\nIn Figure 11, even when multiple features in the contention\nworkload vary during the execution, SmartPQ outperforms\nalistarh_herlihy andNuddle by 1.87 ×and 1.38 ×on aver-\nage, respectively. Note that any of the contention workloadsTime (sec) Current Size Key Range Number of Threads Insert - DeleteMin (%)\n0 1149 100K 50 75-25\n25 812 2K 50 75-25\n50 485 1M 50 75-25\n75 2860 10K 50 75-25\n100 2256 50M 50 75-25\n(a) Varying the key range in the workload.\nTime (sec) Current Size Key Range Number of Threads Insert - DeleteMin (%)\n0 1166 20M 57 65-35\n25 15567 20M 29 65-35\n50 15417 20M 15 65-35\n75 15297 20M 43 65-35\n100 15346 20M 15 65-35\n(b) Varying the number of threads that perform operations in the data\nstructure.\nTime (sec) Current Size Key Range Number of Threads Insert - DeleteMin (%)\n0 1M 5M 22 50-50\n25 140 5M 22 100-0\n50 7403 5M 22 30-70\n75 962 5M 22 100-0\n100 8236 5M 22 0-100\n(c) Varying the percentage of insert /deleteMin operations.\nTable 2: Features of the contention workload for benchmarks\nevaluated in Figure 10. We use bold font on the features that\nchange in each execution phase.\nevaluated in Figures 10 and 11 belongs in the training data\nset used for training our classifier. Third, we note that the\ndecision making mechanism of SmartPQ has very low perfor-\nmance overheads. Across all evaluated benchmarks, SmartPQ\nachieves only up to 5.3% performance slowdown (i.e., when\nusing a range of 50M keys in Figure 10a) over the correspond-\ning baseline implementation ( alistarh_herlihy priority queue).\nNote that since the proposed decision tree classifier has very\nlow traversal cost (Section 3.1.2), we intentionally set a fre-\nquent time interval (i.e., one second) for calling the classifier,\nsuch that SmartPQ detects the contention workload change on\ntime, and quickly adapts itself to the best-performing algorith-\nmic mode. We also tried large time intervals, and observed\nthatSmartPQ slightly delays to detect the contention work-\nload change, thus achieving lower throughput in the transition\npoints.\nOverall, we conclude that SmartPQ performs best across all\ncontention workloads and at anypoint in time, and incurs neg-\nligible performance overheads over the corresponding baseline\nimplementation.\n5. Discussion & Future Work\nIn Section 3.1.2, we assume that the contention workload is\nknown a priori to extract the features needed for classification.\nTo extract these features on-the-fly , and dynamically detect\nwhen contention changes, the main structure of SmartPQ ,\ni.e.,struct smartpq , needs to be enriched with additional\nfields to keep track of workload statistics (e.g., the number\nof completed insert /deleteMin operations, the number of ac-\ntive threads that perform operations on the data structure, the\nminimum and/or maximum key that has been requested so\nfar). Active threads that perform operations on the data struc-\nture could atomically update these statistics. In frequent time\nlapses, either a background thread or an active thread could\nextract the features needed for classification based on the work-\n9\n\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\n80\n85\n90\n95\n100\n105\n110\n115\n120\n12505101520\nNumber of Threads = 50, Insert (%) - DeleteMin (%) = 75-25\nalistarh_herlihy Nuddle SmartPQ\nTime (sec)Throughput (Mops/sec)(a)\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\n80\n85\n90\n95\n100\n105\n110\n115\n120\n1250.02.55.07.510.012.515.017.5\nKey Range = 20M, Insert (%) - DeleteMin (%) = 65-35\nalistarh_herlihy Nuddle SmartPQ\nTime (sec)Throughput (Mops/sec) (b)\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\n80\n85\n90\n95\n100\n105\n110\n115\n120\n1250510152025303540\nKey Range = 5M, Number of Threads = 22\nalistarh_herlihy Nuddle SmartPQ\nTime (sec)Throughput (Mops/sec) (c)\nFigure 10: Throughput achieved by SmartPQ ,Nuddle and its underlying base algorithm (alistarh_herlihy ), in synthetic benchmarks,\nin which we vary a) the key range, b) the number of threads that perform operations in the data structure, and c) the percentage of\ninsert /deleteMin operations in the workload.\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\n80\n85\n90\n95\n100\n105\n110\n115\n120\n125\n130\n135\n140\n145\n150\n155\n160\n165\n170\n175\n180\n185\n190\n195\n200\n205\n210\n215\n220\n225\n230\n235\n240\n245\n250\n255\n260\n265\n270\n275\n280\n285\n290\n295\n300\n305\n310\n315\n320\n325\n330\n335\n340\n345\n350\n355\n360\n365\n370\n3750102030405060\nalistarh_herlihy Nuddle SmartPQ\nalistarh_herlihyNuddleSmartPQ0246810Throughput (Mops/sec)\nTime (sec)Throughput (Mops/sec)Harmonic Mean\nFigure 11: Throughput achieved by SmartPQ ,Nuddle and its underlying base algorithm (alistarh_herlihy ), in synthetic benchmarks, in\nwhich we vary multiple features in the contention workload.\nTime (sec) Current Size Key Range Number of Threads Insert - DeleteMin (%)\n0 1M 10M 57 50-50\n25 26 10M 36 70-30\n50 12 20M 36 50-50\n75 79 20M 36 80-20\n100 29K 20M 50 80-20\n125 319K 100M 50 50-50\n150 13 100M 57 50-50\n175 524K 100M 22 100-0\n200 524K 100M 22 50-50\n225 1142 100M 22 50-50\n250 463 200M 57 0-100\n275 253 200M 57 100-0\n300 33K 20M 57 0-100\n325 142 20M 29 80-20\n350 25K 20M 29 50-50\nTable 3: Features of the contention workload for benchmarks\nevaluated in Figure 11. We use bold font on the features that\nchange in each execution phase.\nload statistics, and call the classifier to predict if a transition\nto another algorithmic mode is needed. Finally, an additional\nparameter could be also added in SmartPQ to configure how\noften to collect workload statistics.\nIn our experimental evaluation, we pin server threads on a\nsingle NUMA node and client threads on all nodes. We have\nchosen to do so (i) for simplicity, given that this approach\nfits well with our microbenchmark-based evaluation, and (ii)\nbecause this is par with prior works on concurrent data struc-\ntures [2, 7, 9 –11, 14, 16, 22, 27, 37, 46, 65 –67, 71, 78]. In a\nreal-life scenario, where SmartPQ is used as a part of an ap-\nplication, client threads do notneed to be pinned in hardware\ncontexts, and they can be allowed to run in any core of the\nsystem. However, for our approach to be meaningful server\nthreads need to be limited on a single NUMA node. This can\neasily be done by creating the server threads when SmartPQis initialized, and pinning them to hardware contexts that are\nlocated at the same NUMA node. In this case, server threads\nare background threads that only accept and serve requests\nfrom various client threads, which are part of the high-level\napplication.\nFinally, even though we focus on a microbenchmark-based\nevaluation to cover a wide variety of contention scenarios,\nit is one of our future directions to explore the efficiency of\nSmartPQ in real-life applications, such as web servers [28,63],\ngraph traversal applications [12, 67] and scheduling in oper-\nating systems [1]. As future work, we also aim to investigate\nthe applicability of our approach in other data structures, that\nmay have similar behavior with priority queues (e.g., skip lists,\nsearch trees), and extend our proposed classifier (e.g., adding\nmore features) to cover a variety of NUMA CPU-centric sys-\ntems with different architectural characteristics.\n6. Related Work\nTo our knowledge, this is the first work to propose an adap-\ntive priority queue for NUMA systems, which performs best\nunder allvarious contention workloads, and even when con-\ntention varies over time. We briefly discuss prior work.\nConcurrent Priority Queues. A large corpus of work\nproposes concurrent algorithms for priority queues [2, 6,\n9, 45, 47, 62, 64, 66 –68, 74, 77, 80], or generally for skip\nlists [11, 13, 18, 23, 24, 34, 35, 46, 61]. Recent works [45, 47]\ndesigned lock-free priority queues that separate the logical\nand the physical deletion of an element to increase paral-\nlelism. Alistarh et al. [2] design a relaxed priority queue,\ncalled SprayList , in which deleteMin operation returns with\na high probability, an element among the firstO(plog3 p)ele-\nments of the priority queue, where pis the number of threads.\n10\n\nSagonas et al [66] design a contention avoiding technique, in\nwhich deleteMin operation returns the highest-priority element\nof the priority queue under lowcontention, while it enables\nrelaxed semantics when high contention is detected. Specifi-\ncally, under high-contention a few deleteMin operations are\nqueued, and later several elements are deleted from the head\nof the queue at once via a combined deletion operation. Hei-\ndarshenas et al. [30] design a novel architecture for relaxed\npriority queues. These prior approaches are NUMA-oblivious\nimplementations. Thus, in NUMA systems, they incur signif-\nicant performance degradation in high-contention scenarios\n(e.g., deleteMin -dominated workloads in Section 4.1). In con-\ntrast, Calciu et al. [9] propose a NUMA-friendly priority queue\nemploying the combining and elimination techniques. Elimi-\nnation allows the complementary operations, i.e., insert with\ndeleteMin , to complete without updating the data structure,\nwhile combining is a technique similar to the delegation tech-\nnique [8, 38, 48, 57, 73] of Nuddle andffwd [65]. Finally, Daly\net al. [14] propose an efficient technique to obtain NUMA-\naware skip lists, which however, can only be applied to skip\nlist-based data structures. In contrast, Nuddle is ageneric\ntechnique to obtain NUMA-aware data structures.\nBlack-Box Approaches. Researchers have proposed black-\nbox approaches: any data structure can be made wait-free or\nNUMA-aware without effort or knowledge on parallel pro-\ngramming or NUMA architectures. Herlihy [33] provides a\nuniversal method to design wait-free implementations of any\nsequential object. However, this method remains impractical\ndue to high overheads. Hendler et al. [31] propose flat com-\nbining; a technique to reduce synchronization overheads by\nexecuting multiple client threads’ requests at once . Despite\nsignificant improvements [32], this technique provides high\nperformance only for a few data structures (e.g., synchronous\nqueues). ffwd [65] is black-box approach, which uses the del-\negation technique [8, 38, 48, 57, 73] to eliminate cache line\ninvalidation traffic over the interconnect. However, ffwd is lim-\nited to single threaded performance. Calciu et al. [10] propose\na black-box technique, named Node Replication , to obtain\nconcurrent NUMA-aware data structures. In Node Replication ,\nevery NUMA node has replicas of the shared data structure,\nwhich are synchronized via a shared log. Although ffwd and\nNode Replication are generic techniques to obtain NUMA-\naware data structures, similarly to Nuddle , both of them use a\nserial asynchronized implementation as the underlying base\nalgorithm . Thus, if they are used as the NUMA-aware algo-\nrithmic mode in an adaptive data structure, which dynamically\nswitches between a NUMA-oblivious and a NUMA-aware\nmode, both ffwd andNode Replication need a synchronization\npoint between transitions to ensure correctness. Consequently,\nthey would incur high performance overheads, when transi-\ntions between algorithmic modes happen at a non-negligible\nfrequency.\nMachine learning in Data Structures. Even though ma-\nchine learning is widely used to improve performance in many\nemerging applications [3, 4, 17, 21, 26, 41, 44, 50, 51, 53, 58,\n69, 82], there is a handful of works [20, 40] that leverage\nmachine learning to design highly-efficient concurrent datastructures. Recently, Eastep et al. [20] use reinforcement\nlearning to on-the-fly tune a parameter in the flat combining\ntechnique [31, 32], which is used in skip lists and priority\nqueues. Kraska et al. [40] demonstrate that machine learning\nmodels can be trained to predict the position or existence of\nelements in key-value lookup sets, and discuss under which\nconditions learned index models can outperform the traditional\nindexed data structures (e.g., B-trees).\n7. Conclusion\nWe propose SmartPQ , an adaptive concurrent priority queue\nfor NUMA architectures, which performs best under allvar-\nious contention scenarios, and even when contention varies\nover time. SmartPQ has two key components. First, it is built\non top of Nuddle ; a generic low-overhead technique to obtain\nefficient NUMA-aware data structures using anyconcurrent\nNUMA-oblivious implementation as its backbone. Second,\nSmartPQ integrates a lightweight decision making mecha-\nnism, which is based on a simple decision tree classifier, to\ndecide when to switch between Nuddle , i.e., a NUMA-aware\nalgorithmic mode, and its underlying base algorithm , i.e.,\naNUMA-oblivious algorithmic mode. Our evaluation over a\nwide range of contention scenarios demonstrates that SmartPQ\nswitches between the two algorithmic modes with negligible\noverheads, and significantly outperforms prior schemes, even\nwhen contention varies over time. We conclude that SmartPQ\nis an efficient concurrent priority queue for NUMA systems,\nand hope that this work encourages further study on adaptive\nconcurrent data structures for NUMA architectures.\nReferences\n[1]L. A. Torrey, J. Coleman, and B. Miller, “A Comparison of Interactivity in the\nLinux 2.6 Scheduler and an MLFQ Scheduler,” Software Practice and Experience. ,\n2007.\n[2]D. Alistarh, J. Kopinsky, J. Li, and N. Shavit, “The SprayList: a Scalable Relaxed\nPriority Queue,” in PPoPP , 2015.\n[3]A. Benatia, W. Ji, Y . Wang, and F. Shi, “Sparse Matrix Format Selection with\nMulticlass SVM for SpMV on GPU,” ICPP , 2016.\n[4]A. Benatia, W. Ji, Y . Wang, and F. Shi, “Sparse Matrix Format Selection with\nMulticlass SVM for SpMV on GPU,” in ICPP , 2016.\n[5]S. Boyd-Wickizer, A. T. Clements, Y . Mao, A. Pesterev, M. F. Kaashoek, R. Morris,\nand N. Zeldovich, “An Analysis of Linux Scalability to Many Cores,” in OSDI ,\n2010.\n[6]G. S. Brodal, J. L. Träff, and C. D. Zaroliagis, “A Parallel Priority Queue with\nConstant Time Operations,” J. Parallel Distrib. Comput. , 1998.\n[7]N. G. Bronson, J. Casper, H. Chafi, and K. Olukotun, “A Practical Concurrent\nBinary Search Tree,” in PPoPP , 2010.\n[8]I. Calciu, D. Dice, T. Harris, M. Herlihy, A. Kogan, V . J. Marathe, and M. Moir,\n“Message Passing or Shared Memory: Evaluating the Delegation Abstraction for\nMulticores,” in OPODIS , 2013.\n[9]I. Calciu, H. Mendes, and M. Herlihy, “The Adaptive Priority Queue with Elimina-\ntion and Combining,” in DISC , 2014.\n[10] I. Calciu, S. Sen, M. Balakrishnan, and M. K. Aguilera, “Black-box Concurrent\nData Structures for NUMA Architectures,” ASPLOS , 2017.\n[11] J. Choe, A. Huang, T. Moreshet, M. Herlihy, and R. I. Bahar, “Concurrent Data\nStructures with Near-Data-Processing: An Architecture-Aware Implementation,” in\nSPAA , 2019.\n[12] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction to Algorithms,\nThird Edition . The MIT Press, 2009.\n[13] T. Crain, V . Gramoli, and M. Raynal, “No Hot Spot Non-blocking Skip List,” in\nICDCS , 2013.\n[14] H. Daly, A. Hassan, M. F. Spear, and R. Palmieri, “NUMASK: High Performance\nScalable Skip List for NUMA,” in DISC , 2018.\n11\n\n[15] T. David, R. Guerraoui, and . V . Trigonakis, “Everything You Always Wanted to\nKnow About Synchronization but Were Afraid to Ask,” in SOSP , 2013.\n[16] T. A. David, R. Guerraoui, and V . Trigonakis, “Asynchronized Concurrency: The\nSecret to Scaling Concurrent Search Data Structures,” ASPLOS , 2015.\n[17] L. Dhulipala, C. Hong, and J. Shun, “ConnectIt: A Framework for Static and\nIncremental Parallel Graph Connectivity Algorithms,” in VLDB Endowment , 2020.\n[18] I. Dick, A. Fekete, and V . Gramoli, “A Skip List for Multicore,” Concurrency and\nComputation: Practice and Experience , 2017.\n[19] L. Doddipalli and K. Rani, “Ensemble Decision Tree Classifier For Breast Cancer\nData,” International Journal of Information Technology Convergence and Services ,\n2012.\n[20] J. Eastep, D. Wingate, and A. Agarwal, “Smart Data Structures: An Online Machine\nLearning Approach to Multicore Data Structures,” in ICAC , 2011.\n[21] A. Elafrou, G. I. Goumas, and N. Koziris, “Performance Analysis and Optimiza-\ntion of Sparse Matrix-Vector Multiplication on Modern Multi- and Many-Core\nProcessors,” in ICPP , 2017.\n[22] F. Ellen, P. Fatourou, E. Ruppert, and F. van Breugel, “Non-blocking Binary Search\nTrees,” in PODC , 2010.\n[23] M. Fomitchev and E. Ruppert, “Lock-free Linked Lists and Skip Lists,” in PODC ,\n2004.\n[24] K. Fraser., “Practical Lock-Freedom,” PhD thesis, University of Cambridge , 2004.\n[25] C. Giannoula, N. Vijaykumar, N. Papadopoulou, V . Karakostas, I. Fernandez,\nJ. Gómez-Luna, L. Orosa, N. Koziris, G. Goumas, and O. Mutlu, “ SynCron : Effi-\ncient Synchronization Support for Near-Data-Processing Architectures,” in HPCA ,\n2021.\n[26] P. Grönquist, C. Yao, T. Ben-Nun, N. Dryden, P. Dueben, S. Li, and T. Hoefler,\n“Deep Learning for Post-Processing Ensemble Weather Forecasts,” Philosophical\nTransactions of the Royal Society A , 2021.\n[27] R. Guerraoui and V . Trigonakis, “Optimistic Concurrency with OPTIK,” in PPoPP ,\n2016.\n[28] M. Harchol-Balter, B. Schroeder, N. Bansal, and M. Agrawal, “Size-based Schedul-\ning to Improve Web Performance,” TOCS , 2003.\n[29] T. Harris, “A Pragmatic Implementation of Non-Blocking Linked Lists,” in DISC ,\n2001.\n[30] A. Heidarshenas, T. Gangwani, S. Yesil, A. Morrison, and J. Torrellas, “Snug: Ar-\nchitectural support for relaxed concurrent priority queueing in chip multiprocessors,”\ninICS, 2020.\n[31] D. Hendler, I. Incze, N. Shavit, and M. Tzafrir, “Flat Combining and the\nSynchronization-parallelism Tradeoff,” in SPAA , 2010.\n[32] D. Hendler, I. Incze, N. Shavit, and M. Tzafrir, “Scalable Flat-Combining Based\nSynchronous Queues,” in DISC , 2010.\n[33] M. Herlihy, “Wait-free Synchronization,” TOPLAS , 1991.\n[34] M. Herlihy, Y . Lev, V . Luchangco, and N. Shavit, “A Simple Optimistic Skiplist\nAlgorithm,” in SIROCCO , 2007.\n[35] M. Herlihy and N. Shavit, The Art of Multiprocessor Programming . Morgan\nKaufmann Publishers Inc., 2008.\n[36] S. V . Howley and J. Jones, “A Non-blocking Internal Binary Search Tree,” in SPAA ,\n2012.\n[37] S. V . Howley and J. Jones, “A Non-Blocking Internal Binary Search Tree,” in SPAA ,\n2012.\n[38] D. Klaftenegger, K. Sagonas, and K. Winblad, “Delegation Locking Libraries for\nImproved Performance of Multithreaded Programs,” in EuroPar , 2014.\n[39] V . Kolmogorov, “Blossom V: A new Implementation of a Minimum Cost Perfect\nMatching Algorithm,” 2009.\n[40] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The Case for Learned\nIndex Structures,” in SIGMOD , 2018.\n[41] A. Kusum, I. Neamtiu, and R. Gupta, “Safe and Flexible Adaptation via Alternate\nData Structure Representations,” in CC, 2016.\n[42] L. T. Schermerhorn. (2007) Automatic Page Migration for Linux [A Matter of\nHygiene].\n[43] D. Lasalle and G. Karypis, “Multi-threaded Graph Partitioning,” in IPDPS , 2013.\n[44] P. Li, H. Lu, Q. Zheng, L. Yang, and G. Pan, “Lisa: A learned index structure for\nspatial data,” in SIGMOD , 2020.\n[45] J. Lindén and B. Jonsson, “A Skiplist-Based Concurrent Priority Queue with\nMinimal Memory Contention,” in OPODIS , 2013.\n[46] Z. Liu, I. Calciu, M. Herlihy, and O. Mutlu, “Concurrent Data Structures for\nNear-Memory Computing,” in SPAA , 2017.\n[47] I. Lotan and N. Shavit., “Skiplist-Based Concurrent Priority Queues,” in IPDPS ,\n2000.[48] J.-P. Lozi, F. David, G. Thomas, J. Lawall, and G. Muller, “Remote Core Locking:\nMigrating Critical-Section Execution to Improve the Performance of Multithreaded\nApplications,” in USENIX ATC , 2012.\n[49] R. Marotta, M. Ianni, A. Pellegrini, and F. Quaglia, “A Non-Blocking Priority\nQueue for the Pending Event Set,” in SIMUTOOLS , 2016.\n[50] S. Memeti, S. Pllana, A. Binotto, J. Kołodziej, and I. Brandic, “Using Meta-\nHeuristics and Machine Learning for Software Optimization of Parallel Computing\nSystems: A Systematic Literature Review,” Computing , 2019.\n[51] K. Meng, J. Li, G. Tan, and N. Sun, “A Pattern Based Algorithmic Autotuner for\nGraph Processing on GPUs,” in PPoPP , 2019.\n[52] M. M. Michael, “High Performance Dynamic Lock-free Hash Tables and List-based\nSets,” in SPAA , 2002.\n[53] D. Michie, “’Memo’ Functions and Machine Learning,” in Nature , 1968.\n[54] D. Molka, D. Hackenberg, R. Schone, and M. S. Muller, “Memory Performance and\nCache Coherency Effects on an Intel Nehalem Multiprocessor System,” in PACT ,\n2009.\n[55] A. Natarajan and N. Mittal, “Fast Concurrent Lock-free Binary Search Trees,” in\nPPoPP , 2014.\n[56] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blon-\ndel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,\nM. Brucher, M. Perrot, and E. Duchesnay, “Scikit-learn: Machine Learning in\nPython,” J. Mach. Learn. Res. , 2011.\n[57] D. Petrovi ´c, T. Ropars, and A. Schiper, “On the Performance of Delegation over\nCache-Coherent Shared Memory,” in ICDCN , 2015.\n[58] J. C. Pichel and B. Pateiro-López, “A New Approach for Sparse Matrix Classifica-\ntion Based on Deep Learning Techniques,” in CLUSTER , 2018.\n[59] K. Polat and S. Güne¸ s, “A Novel Hybrid Intelligent Method Based on C4.5 Deci-\nsion Tree Classifier and One-against-All Approach for Multi-Class Classification\nProblems,” Expert Syst. Appl. , 2009.\n[60] R. C. Prim, “Shortest Connection Networks and some Generalizations,” The Bell\nSystems Technical Journal , 1957.\n[61] W. Pugh, “Skip Lists: A Probabilistic Alternative to Balanced Trees,” Commun.\nACM , 1990.\n[62] M. Rab, R. Marotta, M. Ianni, A. Pellegrini, and F. Quaglia, “NUMA-Aware\nNon-Blocking Calendar Queue,” in DS-RT , 2020.\n[63] M. Rawat and A. D. Kshemkalyani, “SWIFT: Scheduling in Web Servers for Fast\nResponse Time,” in NCA , 2003.\n[64] H. Rihani, P. Sanders, and R. Dementiev, “MultiQueues: Simpler, Faster, and Better\nRelaxed Concurrent Priority Queues,” 2015.\n[65] S. Roghanchi, J. Eriksson, and N. Basu, “Ffwd: Delegation is (Much) Faster Than\nYou Think,” in SOSP , 2017.\n[66] K. Sagonas and K. Winblad, “The Contention Avoiding Concurrent Priority Queue,”\ninLCPC , 2016.\n[67] K. Sagonas and K. Winblad, “The Contention Avoiding Concurrent Priority Queue,”\ninLCPC , 2017.\n[68] P. Sanders, “Randomized Priority Queues for Fast Parallel Access,” J. Parallel\nDistrib. Comput. , 1998.\n[69] N. Sedaghati, T. Mu, L.-N. Pouchet, S. Parthasarathy, and P. Sadayappan, “Auto-\nmatic Selection of Sparse Matrix Representation on GPUs,” in ICS, 2015.\n[70] D. Siakavaras, K. Nikas, G. Goumas, and N. Koziris, “RCU-HTM: A Generic\nSynchronization Technique for Highly Efficient Concurrent Search Trees,” CCPE ,\n2021.\n[71] D. Siakavaras, K. Nikas, G. I. Goumas, and N. Koziris, “RCU-HTM: Combining\nRCU with HTM to Implement Highly Efficient Concurrent Binary Search Trees,”\nPACT 2017.\n[72] J. Sloan, R. Kumar, and G. Bronevetsky, “Algorithmic Approaches to Low Overhead\nFault Detection for Sparse Linear Algebra,” in DSN , 2012.\n[73] M. A. Suleman, O. Mutlu, M. K. Qureshi, and Y . N. Patt, “Accelerating Critical\nSection Execution with Asymmetric Multi-Core Architectures,” in ASPLOS , 2009.\n[74] H. Sundell and P. Tsigas, “Fast and Lock-free Concurrent Priority Queues for\nMulti-thread Systems,” Journal of Parallel and Distributed Computing , 2005.\n[75] W. T. Tang, R. S. M. Goh, and I. L.-J. Thng, “Ladder Queue: An O(1) Priority\nQueue Structure for Large-scale Discrete Event Simulation,” TOMACS. , 2005.\n[76] M. Thorup, “Integer Priority Queues with Decrease Key in Constant Time and the\nSingle Source Shortest Paths Problem,” in STOC , 2003.\n[77] M. Wimmer, J. Gruber, J. L. Träff, and P. Tsigas, “The Lock-free k-LSM Relaxed\nPriority Queue,” in PPoPP , 2015.\n[78] K. Winblad, K. Sagonas, and B. Jonsson, “Lock-Free Contention Adapting Search\nTrees,” in SPAA , 2018.\n12\n\n[79] Y . Xu, K. Li, and J. Hu, “A Genetic Algorithm for Task Scheduling on Heteroge-\nneous Computing Systems using Multiple Priority Queues,” Information Sciences ,\n2014.\n[80] D. Zhang and D. Dechev, “A Lock-Free Priority Queue Design Based on Multi-\nDimensional Linked Lists,” TPDS , 2016.[81] M. Zhang, H. Chen, L. Cheng, F. C. M. Lau, and C. Wang, “Scalable Adaptive\nNUMA-Aware Lock,” TPDS , 2017.\n[82] Y . Zhao, J. Li, C. Liao, and X. Shen, “Bridging the Gap between Deep Learning\nand Sparse Matrix Format Selection,” in PPoPP , 2018.\n13",
  "textLength": 70767
}