{
  "paperId": "bf2c9e04ad1963c38f9c4bf84efb6df77093bd81",
  "title": "Learning Sublinear-Time Indexing for Nearest Neighbor Search",
  "pdfPath": "bf2c9e04ad1963c38f9c4bf84efb6df77093bd81.pdf",
  "text": "Learning Space Partitions for Nearest Neighbor Search\nYihe Dong\u0003\nMicrosoftPiotr Indyk\nMITIlya Razenshteyn\nMicrosoft ResearchTal Wagner\nMIT\nSeptember 30, 2020\nAbstract\nSpace partitions of Rdunderlie a vast and important class of fast nearest neighbor search\n(NNS) algorithms. Inspired by recent theoretical work on NNS for general metric spaces [ANN+18a,\nANN+18b], we develop a new framework for building space partitions reducing the problem to\nbalanced graph partitioning followed by supervised classi\fcation. We instantiate this general\napproach with the KaHIP graph partitioner [SS13] and neural networks, respectively, to obtain\na new partitioning procedure called Neural Locality-Sensitive Hashing (Neural LSH). On sev-\neral standard benchmarks for NNS [ABF17], our experiments show that the partitions obtained\nby Neural LSH consistently outperform partitions found by quantization-based and tree-based\nmethods as well as classic, data-oblivious LSH.\n1 Introduction\nThe Nearest Neighbor Search (NNS) problem is de\fned as follows. Given an n-point dataset Pin\nad-dimensional Euclidean space Rd, we would like to preprocess Pto answerk-nearest neighbor\nqueries quickly. That is, given a query point q2Rd, we want to \fnd the kdata points from P\nthat are closest to q. NNS is a cornerstone of the modern data analysis and, at the same time, a\nfundamental geometric data structure problem that led to many exciting theoretical developments\nover the past decades. See, e.g., [WLKC16, AIR18] for an overview.\nThe main two approaches to constructing e\u000ecient NNS data structures are indexing andsketch-\ning. The goal of indexing is to construct a data structure that, given a query point, produces a\nsmall subset of P(called candidate set ) that includes the desired neighbors. Such a data structure\ncan be stored on a single machine, or (if the data set is very large) distributed among multiple\nmachines. In contrast, the goal of sketching is to compute compressed representations of points to\nenable computing approximate distances quickly (e.g., compact binary hash codes with the Ham-\nming distance used as an estimator, see the surveys [WSSJ14, WLKC16]). Indexing and sketching\ncan be (and often are) combined to maximize the overall performance [WGS+17, JDJ17].\nBoth indexing and sketching have been the topic of a vast amount of theoretical and empirical\nliterature. In this work, we consider the indexing problem. In particular, we focus on indexing\nbased on space partitions . The overarching idea is to build a partition of the ambient space Rdand\nsplit the dataset Paccordingly. Given a query point q, we identify the bin containing qand form the\nresulting list of candidates from the data points residing in the same bin (or, to boost the accuracy,\nnearby bins as well). Some of the popular space partitioning methods include locality-sensitive\nhashing (LSH) [LJW+07, AIL+15, DSN17]; quantization-based approaches, where partitions are\n\u0003Author names are ordered alphabetically.\n1arXiv:1901.08544v4  [cs.LG]  29 Sep 2020\n\nobtained via k-means clustering of the dataset [JDS11, BL12]; and tree-based methods such as\nrandom-projection trees or PCA trees [Spr91, BCG05, DS13, KS18].\nCompared to other indexing methods, space partitions have multiple bene\fts. First, they\nare naturally applicable in distributed settings, as di\u000berent bins can be stored on di\u000berent ma-\nchines [BGS12, NCB17, LCY+17, BW18]. Moverover, the computational e\u000eciency of search can\nbe further improved by using any nearest neighbor search algorithm locally on each machine. Sec-\nond, partition-based indexing is particularly suitable for GPUs due to the simple and predictable\nmemory access pattern [JDJ17]. Finally, partitions can be combined with cryptographic techniques\nto yield e\u000ecient secure similarity search algorithms [CCD+19]. Thus, in this paper we focus on\ndesigning space partitions that optimize the trade-o\u000b between their key metrics: the number of\nreported candidates, the fraction of the true nearest neighbors among the candidates, the number\nof bins, and the computational e\u000eciency of the point location.\nRecently, there has been a large body of work that studies how modern machine learning tech-\nniques (such as neural networks) can help tackle various classic algorithmic problems (a partial list\nincludes [MPB15, BLS+16, BJPD17, DKZ+17, MMB17, KBC+18, BDSV18, LV18, Mit18, PSK18]).\nSimilar methods|under the name \\learn to hash\"|have been used to improve the sketching ap-\nproach to NNS [WLKC16]. However, when it comes to indexing , while some unsupervised tech-\nniques such as PCA or k-means have been successfully applied, the full power of modern tools like\nneural networks has not yet been harnessed. This state of a\u000bairs naturally leads to the following\ngeneral question: Can we employ modern (supervised) machine learning techniques to\n\fnd good space partitions for nearest neighbor search?\n1.1 Our contribution\nIn this paper we address the aforementioned challenge and present a new framework for \fnding\nhigh-quality space partitions of Rd. Our approach consists of three major steps:\n1. Build the k-NN graph Gof the dataset by connecting each data point to knearest neighbors;\n2. Find a balanced partition Pof the graph Gintomparts of nearly-equal size such that the\nnumber of edges between di\u000berent parts is as small as possible;\n3. Obtain a partition of Rdby training a classi\fer on the data points with labels being the parts\nof the partitionPfound in the second step.\nSee Figure 1 for illustration. The new algorithm directly optimizes the performance of the\npartition-based nearest neighbor data structure. Indeed, if a query is chosen as a uniformly random\ndata point, then the average k-NN accuracy is exactly equal to the fraction of edges of the k-NN\ngraphGwhose endpoints are separated by the partition P. This generalizes to out-of-sample\nqueries provided that the query and dataset distributions are close, and the test accuracy of the\ntrained classi\fer is high.\nAt the same time, our approach is directly related to and inspired by recent theoretical work [ANN+18a,\nANN+18b] on NNS for general metric spaces. In particular, using the framework of [ANN+18a,\nANN+18b], we prove that, under mild conditions on the dataset P, thek-NN graph of Pcan be\npartitioned with a hyperplane into two parts of comparable size such that only few edges get split\nby the hyperplane. This gives a partial theoretical justi\fcation of our method.\nThe new framework is very \rexible and uses partitioning and learning in a black-box way. This\nallows us to plug various models (linear models, neural networks, etc.) and explore the trade-o\u000b\nbetween the quality and the algorithmic e\u000eciency of the resulting partitions. We emphasize the\nimportance of balanced partitions for the indexing problem, where all bins contain roughly the same\n2\n\nnumber of data points. This property is crucial in the distributed setting, since we naturally would\nlike to assign a similar number of points to each machine. Furthermore, balanced partitions allow\ntighter control of the number of candidates simply by varying the number of retrieved parts. Note\nthat a priori, it is unclear how to partition Rdso as to induce balanced bins of a given dataset. Here\nthe combinatorial portion of our approach is particularly useful, as balanced graph partitioning is a\nwell-studied problem, and our supervised extension to Rdnaturally preserves the balance by virtue\nof attaining high training accuracy.\nWe speculate that the new method might be potentially useful for solving the NNS problem for\nnon-Euclidean metrics, such as the edit distance [ZZ17] or optimal transport distance [KSKW15].\nIndeed, for any metric space, one can compute the k-NN graph and then partition it. The only\nstep that needs to be adjusted to the speci\fc metric at hand is the learning step.\nLet us \fnally put forward the challenge of scaling our method up to billion-sized or even larger\ndatasets. For such scale, one needs to build an approximate k-NN graph as well as using graph\npartitioning algorithms that are faster than KaHIP. We leave this exciting direction to future work.\nFor the current experiments (datasets of size 106points), preprocessing takes several hours. Another\nimportant challenge is to obtain NNS algorithms based on the above partitioning with provable\nguarantees in terms of approximation and running time. However, we expect it to be di\u000ecult,\nin particular, since all the current state-of-the-art NNS algorithms lack such guarantees (e.g., k-\nmeans-based [JDS11] or graph methods [MY18], see also [ABF17] for a recent SOTA survey).\nEvaluation We instantiate our framework with the KaHIP algorithm [SS13] for the graph par-\ntitioning step, and either linear models or small-size neural networks for the learning step. We\nevaluate it on several standard benchmarks for NNS [ABF17] and conclude that in terms of quality\nof the resulting partitions, it consistently outperforms quantization-based and tree-based partition-\ning procedures, while maintaining comparable algorithmic e\u000eciency. In the high accuracy regime,\nour framework yields partitions that lead to processing up to 2 :3\u0002fewer candidates than the\nstrongest baseline.\nAs a baseline method we use k-means clustering [JDS11]. It produces a partition of the dataset\nintokbins, in a way that naturally extends to all of Rd, by assigning a query point qto its\nnearest centroid. (More generally, for multi-probe querying, we can rank the bins by the distance\nof their centroids to q). This simple scheme yields very high-quality results for indexing. Besides\nk-means, we evaluate LSH [AIL+15], ITQ [GLGP13], PCA tree [Spr91], RP tree [DS13], and Neural\nCatalyzer [SDSJ19].\n1.2 Related work\nOn the empirical side, currently the fastest indexing techniques for the NNS problem are graph-\nbased [MY18]. The high-level idea is to construct a graph on the dataset (it can be the k-NN\ngraph, but other constructions are also possible), and then for each query perform a walk, which\neventually converges to the nearest neighbor. Although very fast, graph-based approaches have\nsuboptimal \\locality of reference\", which makes them less suitable for several modern architectures.\nFor instance, this is the case when the algorithm is run on a GPU [JDJ17], or when the data is stored\nin external memory [SWQ+14] or in a distributed manner [BGS12, NCB17]. Moreover, graph-based\nindexing requires many rounds of adaptive access to the dataset, whereas partition-based indexing\naccesses the dataset in one shot. This is crucial, for example, for nearest neighbor search over\nencrypted data [CCD+19]. These bene\fts justify further study of partition-based methods.\nMachine learning techniques are particularly useful for the sketching approach, leading to a vast\nbody of research under the label \\learning to hash\" [WSSJ14, WLKC16]. In particular, several\n3\n\n(a) Dataset\n (b)k-NN graph together\nwith a balanced partition\n(c) Learned partition\nFigure 1: Stages of our framework\nrecent works employed neural networks to obtain high-quality sketches [LLW+15, SDSJ19]. The\nfundamental di\u000berence from our work is that sketching is designed to speed up linear scans over\nthe dataset, by reducing the cost of distance evaluation, while indexing is designed for sublinear\ntime searches, by reducing the number of distance evaluations. We note that while sketches are\nnot designed for indexing, they can be used for that purpose, since a b-bit hashing scheme induces\na partition of Rdinto 2bparts. Nonetheless, our experiments show that partitions induced by\nthese methods (such as Iterative Quantization [GLGP13]) are not well-suited for indexing, and\nunderperform compared to quantization-based indexing, as well as to our methods.\nWe highlight in particular the recent work of [SDSJ19], which uses neural networks to learn\na mapping f:Rd!Rd0that improves the geometry of the dataset and the queries to facilitate\nsubsequent sketching. It is natural to ask whether the same family of maps can be applied to\nenhance the quality of partitions for indexing. However, as our experiments show, in the high\naccuracy regime the maps learned using the algorithm of [SDSJ19] consistently degrade the quality\nof partitions.\nFinally, we mention that here is some prior work on learning space partitions: [CD07, RG13,\nLNC+11]. However, all these algorithms learn hyperplane partitions into two parts (then applying\nthem recursively). Our method, on the other hand, is much more \rexible, since neural networks\nallow us to learn a much richer class of partitions.\n2 Our method\nGiven a dataset P\u0012Rdofnpoints, and a number of bins m> 0, our goal is to \fnd a partition R\nofRdintombins with the following properties:\n1.Balanced: The number of data points in each bin is not much larger than n=m.\n2.Locality sensitive: For a typical query point q2Rd, most of its nearest neighbors belong to the\nsame bin ofR. We assume that queries and data points come from similar distributions.\n3.Simple: The partition should admit a compact description and, moreover, the point location\nprocess should be computationally e\u000ecient. For example, we might look for a space partition\ninduced by hyperplanes.\nFormally, we want the partition Rthat minimizes the loss EqhP\np2Nk(q)1R(p)6=R(q)i\ns.t.8p2PjR(p)j\u0014\n(1 +\u0011)(n=m), whereqis sampled from the query distribution, Nk(q)\u001aPis the set of its knearest\nneighbors in P,\u0011>0 is a balance parameter, and R(p) denotes the part of Rthat contains p.\n4\n\nFirst, suppose that the query is chosen as a uniformly random data point ,q\u0018P. LetGbe\nthek-NN graph of P, whose vertices are the data points, and each vertex is connected to its k\nnearest neighbors. Then the above problem boils down to partitioning vertices of the graph Ginto\nmbins such that each bin contains roughly n=m vertices, and the number of edges crossing between\ndi\u000berent bins is as small as possible (see Figure 1(b)). This balanced graph partitioning problem\nis extremely well-studied, and there are available combinatorial partitioning solvers that produce\nvery high-quality solutions. In our implementation, we use the open-source solver KaHIP [SS13],\nwhich is based on a sophisticated local search.\nMore generally, we need to handle out-of-sample queries, i.e., which are not contained in P. Let\neRdenote the partition of G(equivalently, of the dataset P) found by the graph partitioner. To\nconvert eRinto a solution to our problem, we need to extend it to a partition Rof the whole space\nRdthat would work well for query points. In order to accomplish this, we train a model that, given\na query point q2Rd, predicts which of the mbins of eRthe pointqbelongs to (see Figure 1(c)).\nWe use the dataset Pas a training set, and the partition eRas the labels { i.e., each data point\nis labeled with the ID of the bin of eRcontaining it. The method is summarized in Algorithm 1.\nThe geometric intuition for this learning step is that { even though the partition eRis obtained by\ncombinatorial means, and in principle might consist of ill-behaved subsets of Rd{ in most practical\nscenarios, we actually expect it to be close to being induced by a simple partition of the ambient\nspace. For example, if the dataset is fairly well-distributed on the unit sphere, and the number of\nbins ism= 2, a balanced cut of Gshould be close to a hyperplane.\nThe choice of model to train depends on the desired properties of the output partition R. For\ninstance, if we are interested in a hyperplane partition, we can train a linear model using SVM or\nregression. In this paper, we instantiate the learning step with both linear models and small-sized\nneural networks. Here, there is natural tension between the size of the model we train and the\naccuracy of the resulting classi\fer, and hence the quality of the partition we produce. A larger\nmodel yields better NNS accuracy, at the expense of computational e\u000eciency. We discuss this in\nSection 3.\nMulti-probe querying Given a query point q, the trained model can be used to assign it to\na bin of a partition R, and search for nearest neighbors within the data points in that part. In\norder to achieve high search accuracy, we actually train the model to predict several bins for a\ngiven query point, which are likely to contain nearest neighbors. For neural networks, this can be\ndone naturally by taking several largest outputs of the last layer. By searching through more bins\n(in the order of preference predicted by the model) we can achieve better accuracy, allowing for a\ntrade-o\u000b between computational resources and accuracy.\nHierarchical partitions When the required number of bins mis large, in order to improve the\ne\u000eciency of the resulting partition, it pays o\u000b to produce it in a hierarchical manner. Namely, we\n\frst \fnd a partition of Rdintom1bins, then recursively partition each of the bins into m2bins,\nand so on, repeating the partitioning for Llevels. The total number of bins in the overall partition\nism=m1\u0001m2\u0001:::m L. See Figure 2 for illustration. The advantage of such a hierarchical partition\nis that it is much simpler to navigate than a one-shot partition with mbins.\nNeural LSH with soft labels In the primary instantiation of our framework, we set the su-\npervised learning component to a a neural network with a small number of layers and constrained\nhidden dimensions (the exact parameters are speci\fed in the next section). In order to support\ne\u000bective multi-probe querying, we need to infer not just the bin that contains the query point, but\n5\n\nR0\nR1\nR2\nR3\nP1\nP2\nP3\nP4\nP5\nP6\nP7\nP8\nP9Figure 2: Hierarchical partition into 9 bins with m1=m2= 3.Ri's are partitions, Pj's are the\nbins of the dataset. Multi-probe query procedure, which descends into 2 bins, may visit the bins\nmarked in bold.\nPreprocessing\nInput: Dataset P\u001aRd, integer parameter k>0, number of bins m> 0\n1:Build ak-NN graph GofP.\n2:Run a balanced graph partitioning algorithm on Gintomparts. Number the parts arbitrarily\nas 1;:::;m . Let\u0019(p)2f1;:::;mgdenote the part containing p, for everyp2P.\n3:Train a machine learning model Mwith training set Pand labelsf\u0019(p)gp2P. For every x2Rd,\nletM(x)2f1;:::;mgdenote the prediction of Monx.\nM(\u0001) de\fnes our m-way partition of Rd. Note that it is possible that \u0019(p)6=M(p) for somep2P,\nifMattains imperfect training accuracy.\nQuery\nInput: query point q2Rd, number of bins to search b\n1:Run inference on Mto compute M(q).\n2:Search for a near neighbor of qin the binM(q), i.e., among the candidates fp2P:M(p) =\nM(q)g.\n3:IfMfurthermore predicts a distribution over bins, search for a near neighbor in the btop-\nranked bins according to the ranking induced by the distribution (i.e., from the most likely bin\nto less likely ones).\nAlgorithm 1: Nearest neighbor search with a learned space partition\nrather a distribution over bins that are likely to contain this point and its neighbors. A T-probe\ncandidate list is then formed from all data points in the Tmost likely bins. In order to accomplish\nthis, we use soft labels for data points generated as follows. For S\u00151 and a data point p, the\nsoft labelP= (p1;p2;:::;p m) is a distribution over the bin containing a point chosen uniformly\nat random among Snearest neighbors of p(includingpitself). Now, for a predicted distribution\nQ= (q1;q2;:::;q m), we seek to minimize the KL divergence between PandQ:Pm\ni=1pilogpi\nqi.\nIntuitively, soft labels help guide the neural network with information about multiple bin ranking.\nSis a hyperparameter that needs to be tuned; we study its setting in the appendix (cf. Figure 6b).\n6\n\n3 Sparse hyperplane-induced cuts in k-NN graphs\nWe state and prove a theorem that shows, under certain mild assumptions, that the k-NN graph of\na datasetP\u0012Rdcan be partitioned by a hyperplane such that the induced cut is sparse (i.e., has\nfew crossing edges while the sizes of two parts are similar). The theorem is based on the framework\nof [ANN+18a, ANN+18b] and uses spectral techniques.\nWe start with some notation. Let Nk(p) be the set of knearest neighbors of pinP. The\ndegree ofpin thek-NN graph is deg( p) =jNk(p)[fp02Pjp2Nk(p0)gj. LetDbe the\ndistribution over the dataset P, where a point p2Pis sampled with probability proportional to\nits degree deg( p). LetDclosebe the distribution over pairs ( p;p0)2P\u0002P, wherep2Pis uniformly\nrandom, and p0is a uniformly random element of Nk(p). Denote\u000b= E (p;p0)2Dclose[kp\u0000p0k2\n2] and\n\f= Ex1\u0018D;x2\u0018D[kp1\u0000p2k2\n2]. We will proceed assuming that \u000b(typical distance between a data point\nand its nearest neighbors) is noticeably smaller than \f(typical distance between two independent\ndata points).\nThe following theorem implies, informally speaking, that if \u000b\u001c\f, then there exists a hyper-\nplane which splits the dataset into two parts of not too di\u000berent size while separating only few\npairs of (p;p0), wherep0is one of the knearest neighbors of p. For the proof of the theorem, see\nAppendix C.\nTheorem 3.1. There exists a hyperplane H=fx2Rdjha;xi=bgsuch that the following holds.\nLetP=P1[P2be the partition of Pinduced by H:P1=fp2Pjha;pi\u0014bg,P2=fp2Pj\nha;pi>bg. Then, one has:\nPr(p;p0)\u0018Dclose[pandp0are separated by H]\nminfPrp\u0018D[p2P1];Prp\u0018D[p2P2]g\u0014r2\u000b\n\f: (1)\n4 Experiments\nDatasets For the experimental evaluation, we use three standard ANN benchmarks [ABF17]:\nSIFT (image descriptors, 1M 128-dimensional points), GloVe (word embeddings [PSM14], ap-\nproximately 1.2M 100-dimensional points, normalized), and MNIST (images of digits, 60K 784-\ndimensional points). All three datasets come with 10 000 query points, which are used for evalua-\ntion. We include the results for SIFT and GloVe in the main text, and MNIST in Appendix A.\nEvaluation metrics We mainly investigate the trade-o\u000b between the number of candidates gen-\nerated for a query point, and the k-NN accuracy, de\fned as the fraction of its knearest neighbors\nthat are among those candidates. The number of candidates determines the processing time of\nan individual query. Over the entire query set, we report both the average as well as the 0 :95-th\nquantile of the number of candidates. The former measures the throughput1of the data structure,\nwhile the latter measures its latency.2We focus on parameter regimes that yield k-NN accuracy of\nat least 0:75, in the setting k= 10. Additional results with broader regimes of accuracy and of k\nare included in the appendix.\nOur methods We evaluate two variants of our method, with two di\u000berent choices of the super-\nvised learning component:\n1Number of queries per second.\n2Maximum time per query, modulo a small fraction of outliers.\n7\n\n\u000fNeural LSH: In this variant we use small neural networks. We compare this method with\nk-means clustering, Iterative Quantization (ITQ) [GLGP13], Cross-polytope LSH [AIL+15],\nand Neural Catalyzer [SDSJ19] composed over k-means clustering. We evaluate partitions into\n16 bins and 256 bins. We test both one-level (non-hierarchical) and two-level (hierarchical)\npartitions. Queries are multi-probe.\n\u000fRegression LSH: This variant uses logistic regression as the supervised learning component\nand, as a result, produces very simple partitions induced by hyperplanes. We compare this\nmethod with PCA trees [Spr91, KZN08, AAKK14], random projection trees [DS13], and recur-\nsive bisections using 2-means clustering. We build trees of hierarchical bisections of depth up to\n10 (thus total number of leaves up to 1024). The query procedure descends a single root-to-leaf\npath and returns the candidates in that leaf.\n4.1 Implementation details\nNeural LSH uses a \fxed neural network architecture for the top-level partition, and a \fxed ar-\nchitecture for all second-level partitions. Both architectures consist of several blocks, where each\nblock is a fully-connected layer + batch normalization [IS15] + ReLU activations. The \fnal block\nis followed by a fully-connected layer and a softmax layer. The resulting network predicts a dis-\ntribution over the bins of the partition. The only di\u000berence between the top-level network the\nsecond-level network architecture is their number of blocks ( b) and the size of their hidden layers\n(s). In the top-level network we use b= 3 ands= 512. In the second-level networks we use\nb= 2 ands= 390. To reduce over\ftting, we use dropout with probability 0 :1 during training.\nThe networks are trained using the Adam optimizer [KB15] for under 20 epochs on both levels.\nWe reduce the learning rate multiplicatively at regular intervals. The weights are initialized with\nGlorot initialization [GB10]. To tune soft labels, we try di\u000berent values of Sbetween 1 and 120.\nWe evaluate two settings for the number of bins in each level, m= 16 andm= 256 (leading\nto a total number of bins of the total number of bins in the two-level experiments are 162= 256\nand 2562= 65 536, respectively). In the two-level setting with m= 256 the bottom level of Neural\nLSH usesk-means instead of a neural network, to avoid over\ftting when the number of points per\nbin is tiny. The other con\fgurations (two-levels with m= 16 and one-level with either m= 16 or\nm= 256) we use Neural LSH at all levels.\nWe slightly modify the KaHIP partitioner to make it more e\u000ecient on the k-NN graphs. Namely,\nwe introduce a hard threshold of 2000 on the number of iterations for the local search part of the\nalgorithm, which speeds up the partitioning dramatically, while barely a\u000becting the quality of the\nresulting partitions.\n4.2 Comparison with multi-bin methods\nFigure 4 shows the empirical comparison of Neural LSH with k-means clustering, ITQ, Cross-\npolytope LSH, and Neural Catalyzer composed over k-means clustering. It turns out that k-means\nis the strongest among these baselines.3The points depicted in Figure 4 are those that attain\naccuracy\u00150:75. In the appendix (Figure 10) we include the full accuracy range for all methods.\nIn all settings considered, Neural LSH yields consistently better partitions than k-means.4\n3It is important to note that ITQ is not designed to produce space partitions; as explained in Section 1, it does\nso as a side-e\u000bect. Simiarly, Neural Catalyzer is not designed to enhance partitions. The comparison is intended to\nshow that they do not outperform indexing techniques despite being outside their intended application.\n4We note that two-level partitioning with m= 256 is the best performing con\fguration of k-means, for both SIFT\nand GloVe, in terms of the minimum number of candidates that attains 0 :9 accuracy. Thus we evaluate this baseline\n8\n\nGloVe SIFT\nAverages 0 :95-quantiles Averages 0 :95-quantiles\nOne level 16 bins 1.745 2.125 1.031 1.240\n256 bins 1.491 1.752 1.047 1.348\nTwo levels 16 bins 2.176 2.308 1.113 1.306\n256 bins 1.241 1.154 1.182 1.192\nFigure 3: Largest ratio between the number of candidates for Neural LSH and k-means over the\nsettings where both attain the same target 10-NN accuracy, over accuracies of at least 0 :85. See\ndetails in Section 4.2.\nDepending on the setting, k-means requires signi\fcantly more candidates to achieve the same\naccuracy:\n\u000fUp to 117% more for the average number of candidates for GloVe;\n\u000fUp to 130% more for the 0 :95-quantiles of candidates for GloVe;\n\u000fUp to 18% more for the average number of candidates for SIFT;\n\u000fUp to 34% more for the 0 :95-quantiles of candidates for SIFT;\nFigure 3 lists the largest multiplicative advantage in the number of candidates of Neural LSH\ncompared to k-means, for accuracy values of at least 0 :85. Speci\fcally, for every con\fguration of\nk-means, we compute the ratio between the number of candidates in that con\fguration and the\nnumber of candidates of Neural LSH in its optimal con\fguration, among those that attained at\nleast the same accuracy as that k-means con\fguration.\nWe also note that in all settings except two-level partitioning with m= 256,5Neural LSH\nproduces partitions for which the 0 :95-quantiles for the number of candidates are very close to\nthe average number of candidates, which indicates very little variance between query times over\ndi\u000berent query points. In contrast, the respective gap in the partitions produced by k-means is\nmuch larger, since unlike Neural LSH, it does not directly favor balanced partitions. This implies\nthat Neural LSH might be particularly suitable for latency-critical NNS applications.\nModel sizes. The largest model size learned by Neural LSH is equivalent to storing about \u00195700\npoints for SIFT, or \u00197100 points for GloVe.This is considerably larger than k-means with k\u0014\n256, which stores at most 256 points. Nonetheless, we believe the larger model size is acceptable\nfor Neural LSH, for the following reasons. First, in most of the NNS applications, especially\nfor the distributed setting, the bottleneck in the high accuracy regime is the memory accesses\nneeded to retrieve candidates and the further processing (such as distance computations, exact or\napproximate). The model size is not a hindrance as long as does not exceed certain reasonable\nlimits (e.g., it should \ft into a CPU cache). Neural LSH signi\fcantly reduces the memory access\ncost, while increasing the model size by an acceptable amount. Second, we have observed that\nthe quality of the Neural LSH partitions is not too sensitive to decreasing the sizes the hidden\nlayers. The model sizes we report are, for the sake of concreteness, the largest ones that still lead\nto improved performance. Larger models do not increase the accuracy, and sometimes decrease it\ndue to over\ftting.\nat its optimal performance.\n5As mentioned earlier, in this setting Neural LSH uses k-means at the second level, due to the large overall number\nof bins compared to the size of the datasets. This explains why the gap between the average and the 0 :95-quantile\nnumber of candidates of Neural LSH is larger for this setting.\n9\n\n(a) GloVe, one level, 16 bins\n (b) SIFT, one level, 16 bins\n(c) GloVe, one level, 256 bins\n (d) SIFT, one level, 256 bins\n(e) GloVe, two levels, 16 bins\n (f) SIFT, two levels, 16 bins\n(g) GloVe, two levels, 256 bins, k-means at 2nd level\n (h) SIFT, two levels, 256 bins, k-means at 2nd level\nFigure 4: Comparison of Neural LSH with baselines; x-axis is the number of candidates, y-axis is\nthe 10-NN accuracy\n10\n\nFigure 5: Comparison of decision trees built from hyperplanes: x-axis { number of candidates,\ny-axis { 10-NN accuracy\n4.3 Comparison with tree-based methods\nNext we compare binary decision trees, where in each tree node a hyperplane is used to determine\nwhich of the two subtrees to descend into. We generate hyperplanes with the following methods:\nRegression LSH, the Learned KD-tree of [CD07], the Boosted Search Forest of [LNC+11], cutting\nthe dataset into two equal halves along the top PCA direction [Spr91, KZN08], 2-means clustering,\nand random projections of the centered dataset [DS13, KS18]. We build trees of depth up to 10,\nwhich correspond to hierarchical partitions with the up to 210= 1024 bins. Results for GloVe and\nSIFT are summarized in Figure 5 (see appendix). For random projections, we run each con\fguration\n30 times and average the results.\nFor GloVe, Regression LSH signi\fcantly outperforms 2-means, while for SIFT, Regression LSH\nessentially matches 2-means in terms of the average number of candidates, but shows a noticeable\nadvantage in terms of the 0 :95-percentiles. In both instances, Regression LSH signi\fcantly outper-\nforms PCA tree, and all of the above methods dramatically improve upon random projections.\nNote, however, that random projections have an additional bene\ft: in order to boost search\naccuracy, one can simply repeat the sampling process several times and generate an ensemble of\ndecision trees instead of a single tree. This allows making each individual tree relatively deep,\nwhich decreases the overall number of candidates, trading space for query time. Other considered\napproaches (Regression LSH, 2-means, PCA tree) are inherently deterministic, and boosting their\naccuracy requires more care: for instance, one can use partitioning into blocks as in [JDS11], or\nalternative approaches like [KS18]. Since we focus on individual partitions and not ensembles, we\nleave this issue out of the scope.\n4.4 Additional experiments\nIn this section we include several additional experiments.\nFirst, we study the e\u000bect of setting k. We evaluate the 50-NN accuracy of Neural LSH when\nthe partitioning step is run on either the 10-NN or the 50-NN graph.6We compare both algorithms\ntok-means with k= 50. Figure 6a compares these three algorithms on GloVe for 16 bins reporting\naverage numbers of candidates. From this plot, we can see that for k= 50, Neural LSH convincingly\noutperforms k-means, and whether we use 10-NN or 50-NN graph matters very little.\nSecond, we study the e\u000bect of varying S(the soft labels parameter) for Neural LSH on GloVe\nfor 256 bins. See Figure 6b where we report the average number of candidates. As we can see\n6Neural LSH can solve k-NNS by partitioning the k0-NN graph, for any k; k0; they do not have to be equal.\n11\n\n0.70.80.9\n100000 200000 300000 400000 500000Neural LSH, 10−NN graph\nNeural LSH, 50−NN graph\nk−means(a) GloVe, one level, 16 bins, 50-NN accuracy using 10-NN\nand 50-NN graphs\n0.800.850.900.95\n40000 80000 120000 160000S = 1\nS = 5\nS = 15\nS = 50(b) GloVe, one level, 256 bins, varying S\nFigure 6: E\u000bect of various hyperparameters\nfrom the plot, the setting S= 15 yields much better results compared to the vanilla case of S= 1.\nHowever, increasing Sbeyond 15 brings diminishing returns on the overall accuracy.\n5 Conclusions and future directions\nWe presented a new technique for \fnding partitions of Rdwhich support high-performance indexing\nfor sublinear-time NNS. It proceeds in two major steps: (1) We perform a combinatorial balanced\npartitioning of the k-NN graph of the dataset; (2) We extend the resulting partition to the whole\nambient space Rdby using supervised classi\fcation (such as logistic regression, neural networks,\netc.). Our experiments show that the new approach consistently outperforms quantization-based\nand tree-based partitions. There is a number of exciting open problems we would like to highlight:\n\u000fCan we use our approach for NNS over non-Euclidean geometries, such as the edit distance [ZZ17]\nor the optimal transport distance [KSKW15]? The graph partitioning step directly carries\nthrough, but the learning step may need to be adjusted.\n\u000fCan we jointly optimize a graph partition and a classi\fer at the same time? By making the\ntwo components aware of each other, we expect the quality of the resulting partition of Rdto\nimprove. A related approach has been successfully applied in [LNC+11] for hyperplane tree\npartitions.\n\u000fCan our approach be extended to learning several high-quality partitions that complement each\nother? Such an ensemble might be useful to trade query time for memory usage [ALRW17].\n\u000fCan we use machine learning techniques to improve graph-based indexing techniques [MY18] for\nNNS? (This is in contrast to partition-based indexing, as done in this work).\n\u000fOur framework is an example of combinatorial tools aiding \\continuous\" learning techniques. A\nmore open-ended question is whether other problems can bene\ft from such symbiosis.\nAcknowledgments. Supported by NSF TRIPODS awards No. 1740751 and No. 1535851, Simons\nInvestigator Award, and MIT-IBM Watson AI Lab collaboration grant.\n12\n\nReferences\n[AAKK14] Amirali Abdullah, Alexandr Andoni, Ravindran Kannan, and Robert Krauthgamer,\nSpectral approaches to nearest neighbor search , arXiv preprint arXiv:1408.0751 (2014).\n[ABF17] Martin Aum uller, Erik Bernhardsson, and Alexander Faithfull, Ann-benchmarks: A\nbenchmarking tool for approximate nearest neighbor algorithms , International Confer-\nence on Similarity Search and Applications, Springer, 2017, pp. 34{49.\n[AIL+15] Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig\nSchmidt, Practical and optimal lsh for angular distance , Advances in Neural Infor-\nmation Processing Systems, 2015, pp. 1225{1233.\n[AIR18] Alexandr Andoni, Piotr Indyk, and Ilya Razenshteyn, Approximate nearest neighbor\nsearch in high dimensions , arXiv preprint arXiv:1806.09823 (2018).\n[ALRW17] Alexandr Andoni, Thijs Laarhoven, Ilya Razenshteyn, and Erik Waingarten, Optimal\nhashing-based time-space trade-o\u000bs for approximate near neighbors , Proceedings of the\nTwenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, Society for\nIndustrial and Applied Mathematics, 2017, pp. 47{66.\n[ANN+18a] Alexandr Andoni, Assaf Naor, Aleksandar Nikolov, Ilya Razenshteyn, and Erik Wain-\ngarten, Data-dependent hashing via nonlinear spectral gaps , Proceedings of the 50th\nAnnual ACM SIGACT Symposium on Theory of Computing (2018), 787{800.\n[ANN+18b] ,H older homeomorphisms and approximate nearest neighbors , 2018 IEEE\n59th Annual Symposium on Foundations of Computer Science (FOCS), IEEE, 2018,\npp. 159{169.\n[BCG05] Mayank Bawa, Tyson Condie, and Prasanna Ganesan, Lsh forest: self-tuning indexes\nfor similarity search , Proceedings of the 14th international conference on World Wide\nWeb, ACM, 2005, pp. 651{660.\n[BDSV18] Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik, Learning\nto branch , International Conference on Machine Learning, 2018.\n[BGS12] Bahman Bahmani, Ashish Goel, and Rajendra Shinde, E\u000ecient distributed locality\nsensitive hashing , Proceedings of the 21st ACM international conference on Informa-\ntion and knowledge management, ACM, 2012, pp. 2174{2178.\n[BJPD17] Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis, Compressed sensing\nusing generative models , International Conference on Machine Learning, 2017, pp. 537{\n546.\n[BL12] Artem Babenko and Victor Lempitsky, The inverted multi-index , Computer Vision and\nPattern Recognition (CVPR), 2012 IEEE Conference on, IEEE, 2012, pp. 3069{3076.\n[BLS+16] Luca Baldassarre, Yen-Huan Li, Jonathan Scarlett, Baran G ozc u, Ilija Bogunovic, and\nVolkan Cevher, Learning-based compressive subsampling , IEEE Journal of Selected\nTopics in Signal Processing 10(2016), no. 4, 809{822.\n[BW18] Aditya Bhaskara and Maheshakya Wijewardena, Distributed clustering via lsh based\ndata partitioning , International Conference on Machine Learning, 2018, pp. 569{578.\n13\n\n[CCD+19] Hao Chen, Ilaria Chillotti, Yihe Dong, Oxana Poburinnaya, Ilya Razenshteyn, and\nM Sadegh Riazi, Sanns: Scaling up secure approximate k-nearest neighbors search ,\narXiv preprint arXiv:1904.02033 (2019).\n[CD07] Lawrence Cayton and Sanjoy Dasgupta, A learning framework for nearest neighbor\nsearch , Advances in Neural Information Processing Systems, 2007, pp. 233{240.\n[Chu96] Fan RK Chung, Laplacians of graphs and cheegers inequalities , Combinatorics, Paul\nErdos is Eighty 2(1996), no. 157-172, 13{2.\n[DKZ+17] Hanjun Dai, Elias Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song, Learning combina-\ntorial optimization algorithms over graphs , Advances in Neural Information Processing\nSystems, 2017, pp. 6351{6361.\n[DS13] Sanjoy Dasgupta and Kaushik Sinha, Randomized partition trees for exact nearest\nneighbor search , Conference on Learning Theory, 2013, pp. 317{337.\n[DSN17] Sanjoy Dasgupta, Charles F Stevens, and Saket Navlakha, A neural algorithm for a\nfundamental computing problem , Science 358(2017), no. 6364, 793{796.\n[GB10] Xavier Glorot and Yoshua Bengio, Understanding the di\u000eculty of training deep feedfor-\nward neural networks , International Conference on Arti\fcial Intelligence and Statistics,\n2010, pp. 249{256.\n[GLGP13] Yunchao Gong, Svetlana Lazebnik, Albert Gordo, and Florent Perronnin, Iterative\nquantization: A procrustean approach to learning binary codes for large-scale image\nretrieval , IEEE Transactions on Pattern Analysis and Machine Intelligence 35(2013),\nno. 12, 2916{2929.\n[IS15] Sergey Io\u000be and Christian Szegedy, Batch normalization: Accelerating deep network\ntraining by reducing internal covariate shift , arXiv preprint arXiv:1502.03167 (2015).\n[JDJ17] Je\u000b Johnson, Matthijs Douze, and Herv\u0013 e J\u0013 egou, Billion-scale similarity search with\ngpus, arXiv preprint arXiv:1702.08734 (2017).\n[JDS11] Herve J\u0013 egou, Matthijs Douze, and Cordelia Schmid, Product quantization for nearest\nneighbor search , IEEE transactions on pattern analysis and machine intelligence 33\n(2011), no. 1, 117{128.\n[KB15] Diederik Kingma and Jimmy Ba, Adam: A method for stochastic optimization , Inter-\nnational Conference for Learning Representations, 2015.\n[KBC+18] Tim Kraska, Alex Beutel, Ed H Chi, Je\u000brey Dean, and Neoklis Polyzotis, The case for\nlearned index structures , Proceedings of the 2018 International Conference on Man-\nagement of Data, ACM, 2018, pp. 489{504.\n[KS18] Omid Keivani and Kaushik Sinha, Improved nearest neighbor search using auxiliary\ninformation and priority functions , International Conference on Machine Learning,\n2018, pp. 2578{2586.\n[KSKW15] Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger, From word embeddings\nto document distances , International Conference on Machine Learning, 2015, pp. 957{\n966.\n14\n\n[KZN08] Neeraj Kumar, Li Zhang, and Shree Nayar, What is a good nearest neighbors algo-\nrithm for \fnding similar patches in images? , European conference on computer vision,\nSpringer, 2008, pp. 364{378.\n[LCY+17] Jinfeng Li, James Cheng, Fan Yang, Yuzhen Huang, Yunjian Zhao, Xiao Yan, and Rui-\nhao Zhao, Losha: A general framework for scalable locality sensitive hashing , Proceed-\nings of the 40th International ACM SIGIR Conference on Research and Development\nin Information Retrieval, ACM, 2017, pp. 635{644.\n[LJW+07] Qin Lv, William Josephson, Zhe Wang, Moses Charikar, and Kai Li, Multi-probe lsh:\ne\u000ecient indexing for high-dimensional similarity search , Proceedings of the 33rd inter-\nnational conference on Very large data bases, VLDB Endowment, 2007, pp. 950{961.\n[LLW+15] Venice Erin Liong, Jiwen Lu, Gang Wang, Pierre Moulin, and Jie Zhou, Deep hashing\nfor compact binary codes learning , Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2015, pp. 2475{2483.\n[LNC+11] Zhen Li, Huazhong Ning, Liangliang Cao, Tong Zhang, Yihong Gong, and Thomas S\nHuang, Learning to search e\u000eciently in high dimensions , Advances in Neural Infor-\nmation Processing Systems, 2011, pp. 1710{1718.\n[LV18] Thodoris Lykouris and Sergei Vassilvitskii, Competitive caching with machine learned\nadvice , International Conference on Machine Learning, 2018.\n[Mit18] Michael Mitzenmacher, A model for learned bloom \flters and optimizing by sandwich-\ning, Advances in Neural Information Processing Systems, 2018.\n[MMB17] Chris Metzler, Ali Mousavi, and Richard Baraniuk, Learned d-amp: Principled neural\nnetwork based compressive image recovery , Advances in Neural Information Processing\nSystems, 2017, pp. 1772{1783.\n[MPB15] Ali Mousavi, Ankit B Patel, and Richard G Baraniuk, A deep learning approach to\nstructured signal recovery , Communication, Control, and Computing (Allerton), 2015\n53rd Annual Allerton Conference on, IEEE, 2015, pp. 1336{1343.\n[MY18] Yury A Malkov and Dmitry A Yashunin, E\u000ecient and robust approximate nearest\nneighbor search using hierarchical navigable small world graphs , IEEE transactions on\npattern analysis and machine intelligence (2018).\n[NCB17] Y Ni, K Chu, and J Bradley, Detecting abuse at scale: Locality sensitive hashing at\nuber engineering , 2017.\n[PSK18] Manish Purohit, Zoya Svitkina, and Ravi Kumar, Improving online algorithms via ml\npredictions , Advances in Neural Information Processing Systems, 2018, pp. 9661{9670.\n[PSM14] Je\u000brey Pennington, Richard Socher, and Christopher Manning, Glove: Global vectors\nfor word representation , Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), 2014, pp. 1532{1543.\n[RG13] Parikshit Ram and Alexander Gray, Which space partitioning tree to use for search? ,\nAdvances in Neural Information Processing Systems, 2013, pp. 656{664.\n15\n\n[SDSJ19] Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Herve J\u0013 egou, Spreading\nvectors for similarity search , International Conference on Learning Representations,\n2019.\n[Spr91] Robert F Sproull, Re\fnements to nearest-neighbor searching ink-dimensional trees ,\nAlgorithmica 6(1991), no. 1-6, 579{589.\n[SS13] Peter Sanders and Christian Schulz, Think Locally, Act Globally: Highly Balanced\nGraph Partitioning , Proceedings of the 12th International Symposium on Experimental\nAlgorithms (SEA'13), LNCS, vol. 7933, Springer, 2013, pp. 164{175.\n[SWQ+14] Yifang Sun, Wei Wang, Jianbin Qin, Ying Zhang, and Xuemin Lin, Srs: solving c-\napproximate nearest neighbor queries in high dimensional euclidean space with a tiny\nindex , Proceedings of the VLDB Endowment 8(2014), no. 1, 1{12.\n[WGS+17] Xiang Wu, Ruiqi Guo, Ananda Theertha Suresh, Sanjiv Kumar, Daniel N Holtmann-\nRice, David Simcha, and Felix Yu, Multiscale quantization for fast similarity search ,\nAdvances in Neural Information Processing Systems, 2017, pp. 5745{5755.\n[WLKC16] Jun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang, Learning to hash for indexing\nbig data - a survey , Proceedings of the IEEE 104(2016), no. 1, 34{57.\n[WSSJ14] Jingdong Wang, Heng Tao Shen, Jingkuan Song, and Jianqiu Ji, Hashing for similarity\nsearch: A survey , arXiv preprint arXiv:1408.2927 (2014).\n[ZZ17] Haoyu Zhang and Qin Zhang, Embedjoin: E\u000ecient edit similarity joins via embed-\ndings , Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining, ACM, 2017, pp. 585{594.\nA Results for MNIST\nWe include experimental results for the MNIST dataset, where all the experiments are performed\nexactly in the same way as for SIFT and GloVe. Consistent with the trend we observed for SIFT\nand GloVe, Neural LSH consistently outperforms k-means (see Figure 7) both in terms of average\nnumber of candidates and especially in terms of the 0 :95-th quantiles. We also compare Regression\nLSH with recursive 2-means, as well as PCA tree and random projections (see Figure 8), where\nRegression LSH consistently outperforms the other methods.\nB E\u000bect of Neural Catalyzer on Space Partitions\nIn this section we compare vanilla k-means with k-means run after applying a Neural Catalyzer\nmap [SDSJ19]. The goal is to check whether the Neural Catalyzer { which is designed to boost\nup the performance of sketching methods for NNS by adjusting the input geometry { could also\nimprove the quality of space partitions for NNS. See Figure 9 for the comparison on GloVe and\nSIFT with 16 bins. On both datasets (especially SIFT), Neural Catalyzer in fact degrades the\nquality of the partitions. We observed a similar trend for other numbers of bins than the setting\nreported here. These \fndings support our observation that while both indexing and sketching for\nNNS can bene\ft from learning-based enhancements, they are fundamentally di\u000berent approaches\nand require di\u000berent specialized techniques.\n16\n\n0.800.850.900.951.00\n5000 10000 15000Neural LSH (average)\nNeural LSH (0.95−quantile)\nk−means (average)\nk−means (0.95−quantile)(a) One level, 16 bins\n0.800.850.900.951.00\n0 2500 5000 7500Neural LSH (average)\nNeural LSH (0.95−quantile)\nk−means (average)\nk−means (0.95−quantile) (b) Two levels, 16 bins\nFigure 7: MNIST, comparison of Neural LSH with k-means; x-axis { number of candidates, y-axis\n{ 10-NN accuracy\n●\n●\n●\n●\n●\n●0.20.40.60.8\n300 500 1000 2000 5000 10000 30000●2−means (0.95−quantile)\n2−means (average)\nPCA tree (average, 0.95−quantile)\nRandom projections (0.95−quantile)\nRandom projections (average)\nRegression LSH (0.95−quantile)\nRegression LSH (average)\nFigure 8: MNIST, comparison of trees built from hyperplanes; x-axis { number of candidates, y-axis\n{ 10-NN accuracy\nC Proof of Theorem 3.1\nProof. Consider an undirected graph G= (V;E), where the set of vertices VisP, and the (multi-\n)set of edges contains an edge ( p;p0) for every p02Nk(p). The graph contains nvertices and kn\nedges, and some of the edges might be double (if p02Nk(p) andp2Nk(p0) at the same time).\nLetAGbe the symmetric adjacency matrix of Gnormalized by 2 kn(so that the sum of all the\nentries equals to 1, thus giving a probability distribution over P\u0002P, which can be seen to be\nequal toDclose). The rows and columns of AGcan naturally be indexed by the points of P. Denote\n\u001aG(p) =P\np0(AG)p;p0. It is immediate to check that \u001aGyields a distribution over P, which can be\nseen to be equal to D. DenoteDG= diag(\u001aG). DenoteLG=DG\u0000AGthe Laplacian of AG. Due\nto the equivalence of \u001aGandDandAGandDclose, we have:\n\u000b\n\f=P\np;p02P(AG)p;p0\u0001kp\u0000p0k2\n2P\np;p02P\u001aG(p)\u001aG(p0)\u0001kp\u0000p0k2\n2: (2)\n17\n\n0.800.850.900.95\n200000 400000 600000k−means (average)\nk−means (0.95−quantile)\nCatalyzer + k−means (average)\nCatalyzer + k−means (0.95−quantile)(a) GloVe, one level, 16 bins\n0.750.800.850.900.95\n200000 400000 600000k−means (average)\nk−means (0.95−quantile)\nCatalyzer + k−means (average)\nCatalyzer + k−means (0.95−quantile) (b) SIFT, one level, 16 bins\nFigure 9: Comparison of k-means and Catalyzer + k-means\nBy considering all possible coordinate projections and using additivity of k\u0001k2\n2over coordinates,\nwe conclude that there exists a coordinate i\u00032[d] such that:\nP\np;p02P(AG)p;p0\u0001(pi\u0003\u0000p0\ni\u0003)2\nP\np;p02P\u001aG(p)\u001aG(p0)\u0001(pi\u0003\u0000p0\ni\u0003)2\u0014\u000b\n\f: (3)\nDe\fne a vector y2RPbyyp=pi\u0003. We now apply the following standard fact from spectral graph\ntheory: IfAis the weighted adjacency matrix of a graph, and Lis its Laplacian matrix, then\nxtLx=Pn\ni;j=1Aij(xi\u0000xj)2for allx2Rn. Thus the numerator of (3) becomes ytLGy. For the\ndenominator, consider the graph HonPin which every pair p;p0is connected by an edge of weight\n\u001aG(p)\u001aG(p0).\n\u000fIts weighted adjacency matrix AHis given by ( AH)p;p0=\u001aG(p)\u001aG(p0) forp6=p0, and with zeros\non the diagonal. Thus AH=\u001aG\u001at\nG\u0000D2\nG(recall that DG= diag(\u001aG)).\n\u000fThe degree of each node pinHequals\u001aG(p)P\np02Pnfpg\u001aG(p0) =\u001aG(p)\u0000(\u001aG(p))2(recall thatP\np02P\u001aG(p0) = 1). Therefore the diagonal degree matrix of HisDH=DG\u0000D2\nG.\nTogether, the Lapacian of HisLH=DH\u0000AH=DG\u0000\u001aG\u001at\nG. Therefore the denominator of (3)\nbecomesyt(DG\u0000\u001aG\u001at\nG)y. Overall, we have:\nytLGy\nyt(DG\u0000\u001aG\u001at\nG)y\u0014\u000b\n\f:\nNext, we de\fne ey=y\u0000c\u00011, where 1is the all-1's vector, and cis the scalar c= (yt\u001aG)=(1t\u001aG).\nThis scalar is chosen to render ey?\u001aG. Furthermore, since 1is in the kernel of every Laplacian\nmatrix, we have LGey=LGyandLHey=LHy. Together, we get\neytLGey\neytDGey=ytLGy\nyt(DG\u0000\u001aG\u001at\nG)y\u0014\u000b\n\f;\nNow by the Cheeger's inequality [Chu96], we conclude that there exists a threshold y02Rsuch\nthat:P\np1;p2:eyp1\u0014y0;eyp2>y0(AG)p1;p2\nminfP\np:eyp\u0014y0\u001aG(p);P\np:eyp>y0\u001aG(p)g\u0014s\n2\u0001eytLGey\neytDGey\u0014r2\u000b\n\f: (4)\n18\n\n(a) GloVe, one level, 16 bins\n (b) SIFT, one level, 16 bins\n(c) GloVe, one level, 256 bins\n (d) SIFT, one level, 256 bins\n(e) GloVe, two levels, 16 bins\n (f) SIFT, two levels, 16 bins\n(g) GloVe, two levels, 256 bins, k-means at 2nd level\n (h) SIFT, two levels, 256 bins, k-means at 2nd level\nFigure 10: Results from Figure 4 with broader candidate and accuracy regimes. The \\Learned RCS-\nLSH\" baseline is the learned rectilinear cell structure locality sensitive hashing method of [CD07].\n19\n\nOne can trace back all the de\fnitions and observe that the set fp2P:eyp\u0014y0gis induced by\nan (axis-aligned) hyperplane, and the left-hand side of (4) is nothing else but the left-hand side\nof (1).\n20",
  "textLength": 51029
}