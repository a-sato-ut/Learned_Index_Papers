{
  "paperId": "257fa485809f04088c07fec8720d9f9daab82d5a",
  "title": "Parallel Batched Interpolation Search Tree",
  "pdfPath": "257fa485809f04088c07fec8720d9f9daab82d5a.pdf",
  "text": "Parallel-batched Interpolation Search Tree\nVitaly Aksenov1, Ilya Kokorin1, and Alena Martsenyuk\nITMO University, Russia\nAbstract. A sorted set (or map) is one of the most used data types in\ncomputer science. In addition to standard set operations, like Insert,\nRemove, and Contains , it can provide set-set operations such as Union,\nIntersection ,and Difference .Eachoftheseset-setoperationsisequiv-\nalent to some batched operation: the data structure should be able to\nexecute Insert,Remove, and Contains on a batch of keys. It is obvious\nthat we want these “large” operations to be parallelized. These sets are\nusually implemented with the trees of logarithmic height, such as 2-3\ntrees, treaps, AVL trees, red-black trees, etc. Until now, little attention\nwas devoted to data structures that work asymptotically better under\nseveral restrictions on the stored data. In this work, we parallelize Inter-\npolation Search Tree which is expected to serve requests from a smooth\ndistribution in doubly-logarithmic time. Our data structure of size n\nperforms a batch of moperations in O(mlog log n)work and poly-log\nspan.\nKeywords: Parallel Programming ·Data Structures ·Parallel-Batched\nData Structures.\n1 Introduction\nASorted set is one of the most ubiquitous Abstract Data Types in Computer\nScience, supporting Insert,Remove, and Contains operations among many oth-\ners. The sorted set can be implemented using different data structures: to name\na few, skip-lists [20], red-black trees [10], splay trees [21], or B-trees [9,8].\nSince nowadays most of the processors have multiple cores, we are interested\nin parallelizing these data structures. There are two ways to do that: write a con-\ncurrent version of a data structure or allow one to execute a batch of operations\nin parallel. The first approach is typically very hard to implement correctly and\nefficiently due to different problems with synchronization. Thus, in this work we\nare interested in the second approach: parallel-batched data structures .\nSeveral parallel-batched data structures implementing a sorted set are pre-\nsented: e.g., 2-3 trees [17], red-black trees [16], treaps [5], (a, b) trees [2], AVL-\ntrees [14], and generic joinable binary search trees [4,22].\nAlthough many parallel-batched trees were presented, we definitely lack im-\nplementations that can execute separate queries in o(logn)time (where nis\nthe number of keys in the set) under some reasonable assumptions. However,\nthere exist several sequential data structures with this property. One example is\nInterpolation Search Tree , orIST.arXiv:2306.13785v1  [cs.DC]  23 Jun 2023\n\n2 Vitaly Aksenov, Ilya Kokorin, and Alena Martsenyuk\nDespite the fact that concurrent IST is already presented [7,19] we still lack\nparallel-batched version of the IST: indeed, it differs from the concurrent ver-\nsion because concurrent IST allows many processes to execute scalar requests\nsimultaneously, while parallel-batched IST utilizes multiprocessing to parallelize\nlarge non-scalar requests. In this work, we design and test Parallel-batched In-\nterpolation Search Tree .\nThe work is structured as follows: in Section 2 we describe the important\npreliminaries; in Section 3 we present the original Interpolation Search Tree; in\nSections 4, 5, and 6 we present the parallel-batched contains, insert and remove\nalgorithms; in Section 7 we present a parallelizable method to keep the IST\nbalanced; in Section 8 we present a theoretical analysis; in Section 9 we discuss\nthe implementation and present experimental results; we conclude in Section 10.\n2 Preliminaries\n2.1 Sorted set\nSorted set stores a set of keys of the same comparable type Kand allows one to\nexecute, among others, the following operations:\n–Set.Contains(key) — returns trueifkeybelongs to the set, falseother-\nwise.\n–Set.Insert(key) — if keydoes not exist in the set, adds keyto the set\nand returns true. Otherwise, it leaves the set unmodified and return false.\n–Set.Remove(key) — if keyexists in the set, removes keyfrom the set and\nreturns true. Otherwise, it leaves the set unmodified and return false.\n2.2 Parallel-batched data structures\nDefinition 1. Consider a data structure Dstoring a set of keys and an opera-\ntionOp. IfOpinvolves only one key (e.g., it checks whether a single key exists\nin the set, or inserts a single key into the set) it is called a scalar operation .\nOtherwise (i.e., if Opinvolves multiple keys) it is called a batched operation .\nA data structure Dthat supports at least one batched operation is called a\nbatched data structure .\nWe want a sorted set to implement the following batched operations:\n–Set.ContainsBatched(keys[]) — the operation takes an array of keys of\nsizemand returns an array Resultof the same size. For each i∈[0;m),\nResult[i] is true if keys[i] exist in the set, and false otherwise.\n–Set.InsertBatched(keys[]) — the operation takes an array of keys of size\nm. For each i∈[0;m), ifkeys[i] does not exist in the set, the operation\nadds it to the set.\n–Set.RemoveBatched(keys[]) — the operation takes an array of keys of size\nm. For each i∈[0;m), ifkeys[i] exists in the set, the operation removes it\nfrom the set.\n\nParallel-batched Interpolation Search Tree 3\nNote, that:\n–InsertBatched calculates the union of two sets. Indeed, A←A∪Bcan be\nwritten as A.InsertBatched(B) ;\n–RemoveBatched calculates the difference of two sets. Indeed, A←A\\Bcan\nbe written as A.RemoveBatched(B) ;\n–ContainsBatched calculates the intersection of two sets. Indeed, A∩Bcan\nbe calculated by identifying all keys, belonging in both AandB. As follows\nfrom the ContainsBatched definition, A.ContainsBatched(B) allows us to\nidentify all such keys;\nWecanemployparallelprogrammingtechniques(e.g., fork-joinparallelism [6,13])\nto execute batched operations faster.\nDefinition 2. A batched data structure Dthat uses parallel programming to\nspeed up batched operations is called a parallel-batched data structure .\n2.3 Time complexity model\nIn our work, we assume the standard work-span complexity model [1] for fork-\njoincomputations. We model each computation as a directed acyclic graph ,\nwhere nodes represent operations and edges represent dependencies between\nthem: if there exists a path v→a1→a2→. . .→an→ufrom operation\nvto operation u, then operation vmust be executed before operation u. If nei-\nther path v⇝unor path u⇝vexists, operations uand vcan be executed\nin any order, even in parallel (see Fig 1). Moreover, the directed acyclic graph ,\nmodeling the parallel computation, has exactly one source node (i.e., the node\nwith zero incoming edges) and exactly one sink node (i.e., the node with zero\noutcoming edges). The only source node corresponds to the start of the execu-\ntion, while the only sink node corresponds to the end of the execution. Some\nnodes have two outcoming edges — such operations spawn two parallel tasks\nand are called fork operations . Some nodes have two incoming edges — such\noperations wait for two corresponding parallel tasks to complete and are called\njoin operations .\nFig.1: Example of an directed acyclic graph , representing a parallel execution\n\n\n4 Vitaly Aksenov, Ilya Kokorin, and Alena Martsenyuk\nFor example, in Fig. 1 operation Amust be executed before operation Hand\noperation Cmust be executed before operation J, while operations EandGcan\nbe executed in any order, even in parallel. Node Ais thesource node , denoting\nthe beginning of the execution, while node Kis thesink node , denoting the end\nof the execution. Nodes Band Ccorrespond to fork operations , while nodes J\nandKcorrespond to join operations\nConsidering the execution graph of the algorithm, our target complexities\nare:\n–workdenotes the number of nodes in the graph, i.e., the total number of\nexecuted operations. The graph in Fig. 1 has workof value 10.\n–spandenotes the number of nodes on the longest path from the source node\ntothesink node ,i.e.,thelengthofthecriticalpathinthegraph.Thegraphin\nFig. 1 has spanof value 7, since the longest path from the source node (node\nA) to thesink node (node K) is the path A→B→D→H→G→J→K.\n2.4 Standard parallel primitives\nIn this work, we use several standard parallel primitives. Now, we give their\ndescriptions. Their implementations are provided, for example, in [11].\nParallel loop. It executes a loop body for nindex values (from leftto\nright, exclusive) in parallel. This operation costs O(n)work and O(logn)span\ngiven that the loop body has time complexity O(1).\npfor i in left..right:\nloop_body(i)\nScan. Result := Scan(Arr) calculates exclusive prefix sums of array Arr\nsuch that\nResult [i] =\n\n0 i= 0\ni−1P\nj=0Arr[j]i >0\nScanhasO(n)work and O(logn)span given that the sum of two values can be\ncalculated in O(1).\nFilter. Filter(Arr, predicate) returns an array, consisting of elements\nof the given array Arrsatisfying predicate keeping the order (e.g., Filter([1,\n3, 8, 6, 7, 2], is_even) returns [8, 6, 2] ).FilterhasO(n)work and\nO(logn)span given that predicate has time complexity O(1).\nMerge. Merge(A, B) merges two sorted arrays AandBkeeping the result\nsorted. MergehasO(|A|+|B|)work and O(log2(|A|+|B|))span.\nDifference. Difference(A, B) takes two sorted arrays AandBand returns\nallelementsfrom Athatarenotpresentin B,insortedorder:e.g., Difference([2,\n4, 5, 7, 9], [2, 5, 9]) = [4, 7] .Difference takes O(|A|+|B|)work and\nO(log2(|A|+|B|))span.\nRank.Giventhat Aisasortedarrayand xisavalue,wedenote ElemRank(A,\nx)=|{e∈A|e≤x}|asthenumberofelementsin Athatarelessthanorequalto\n\nParallel-batched Interpolation Search Tree 5\nx(note,thatitisalsotheinsertionpositionof xinAsothat Aremainssortedafter\nthe insertion). For example, ElemRank([1, 3, 5, 7], 2) = 1 ,ElemRank([1,\n3, 5, 7], 5) = 3 ,ElemRank([1, 3, 5, 7], -1) = 0 . Given that Aand B\nare sorted arrays, we denote Rank(A, B) = [r 0, r 1,. . ., r|B|−1], where ri=\nElemRank(A, B[i]) .Rankoperation can be computed in O(|A|+|B|)work and\nO(log2(|A|+|B|))span.\n3 Interpolation Search Tree\n3.1 Interpolation Search Tree Definition\nInterpolation Search Tree (IST) is a multiway internal search tree proposed\nin [15]. IST for a set of keys x0< x 1< . . . < x n−1can be either leafornon-leaf.\nDefinition 3. Leaf IST with a set of keys x0< x 1< . . . < x n−1consists of\narray RepwithRep[i] =xi, i.e., it keeps all the keys in a sorted array Rep.\nDefinition 4. Non-leaf IST with a set of keys x0< x 1< . . . < x n−1consists of\n(Fig. 2 and 3):\n–An array Repstoring an ordered subset of keys xi0, xi1, . . . x ik−1(i.e., 0≤\ni0< i 1< . . . < i k−1< n).\n–Child ISTs C0, C1. . . C k(each can be either leaf or non-leaf), such that:\n•C0is an IST storing a subset of keys x0, x1. . . x i0−1;\n•for1≤j≤k−1,Cjis an IST storing a subset of keys xij−1+1, . . . x ij−1;\n•Ckis an IST storing a subset of keys xik−1+1, . . . x n−1;\nFig.2: Example of a non-leaf IST. Here n= 13, k= 3, i0= 3, i1= 6, i2= 10.\nThus, Rep[0] = x3, Rep [1] = x6, Rep [2] = x10.C0stores keys x0. . . x 2,C2stores\nkeysx4. . . x 5,C3stores keys x7. . . x 9,C4stores keys x11. . . x 12.\n\n\n6 Vitaly Aksenov, Ilya Kokorin, and Alena Martsenyuk\nFig.3: Example of an IST built on array in Fig. 2. Two nodes (root and its\nleftmost child) are non-leaf ISTs, while other nodes are leaf ISTs.\nAnynon-leaf IST has the following properties:\n–all keys less than Rep[0]are located in C0;\n–all keys in between Rep[j−1]andRep[j]are located in Cj;\n–all keys greater than Rep[k−1]are located in Ck;\n3.2 Interpolation search and the lightweight index\nWe can optimize operations on ISTs with numeric keys, by leveraging the inter-\npolation search technique [18,15,23]. Each node of an IST (both leaf and non-leaf\nones) has an approximate index that can point to some place in the Reparray\ncloseto the position of the key being searched. The structure of a non-leaf IST\nwith an index is shown in Fig. 4.\n\nParallel-batched Interpolation Search Tree 7\nFig.4: Non-leaf IST contains: (1) Reparray with stored keys; (2) an array of\npointers to child ISTs C; (3) an index, allowing for fast lookups of keys in the\nReparray.\nThe original IST paper [15] proposes to use an array IDof size m∈Θ(nε)\nwith some ε∈[1\n2; 1)as an index. IDarray is defined the following way: let aand\nbbe reals such that a < b. In that case, ID[i] = j iffRep[j]< a+i·b−a\nm≤\nRep[j+ 1]. As stated in [15], ID[⌊x−a\nb−a·m⌋]is the approximate position of x\n∈(a;b)inRep.\nAfter finding the approximate location of xinRep, we can find its exact\nlocation by using the linear search, as described in [15]. Let us denote i :=\nID[⌊x−a\nb−a·m⌋].\n–ifRep[i] = x , we conclude that we have found the exact position of x;\n–ifRep[i] < x , we conclude that xcan only be found to the right of the\ni-th element in Reparray (since the Reparray is sorted). Therefore, we start\nincrementing iuntil we either: (1) find x; (2) find the element that is strictly\ngreater than xor reach the end of Reparray — in that case we conclude\nthat xis not contained in Reparray (Fig. 5a);\n–ifRep[i] > x , we conclude that xcan only be found to the left of the i-\nth element in Reparray (since the Reparray is sorted). Therefore, we start\ndecrementing iuntil we either: (1) find x; (2) find the element that is strictly\nless than xor reach the beginning of Reparray — in that case we conclude\nthat xis not contained in Reparray (Fig. 5b).\n\n8 Vitaly Aksenov, Ilya Kokorin, and Alena Martsenyuk\nFig.5: Determining the exact location of the key given the approximate location\n(a) Searching for the key on the right\nto the approximate position.\n(b) Searching for the key on the left to\nthe approximate position.\nNote, we can use more complex techniques instead of the linear search, e.g.,\nexponential search [3]. However, they are often unnecessary, since the index usu-\nally provides an approximation good enough to finish the search only in a couple\nof operations. We can also use a machine learning model as an approximate\nindex structure [12].\n3.3 Search in IST\nSuppose we want to find a keyin an IST. The search algorithm is iterative: on\neach iteration we look for the keyin a subtree of some node v. To look for the\nkey in the whole IST we begin the algorithm with setting v := IST.Root , since\nthe IST is the subtree of its root.\nTo find keyinvsubtree, we do the following (here kis the length of v.Rep):\n1. If vis empty, we conclude that keyis not there;\n2. If keyis found in v.Reparray, then we have found the key;\n3. If key < v.Rep[0] , the keycan be found only in v.C[0]subtree (as ex-\nplained in Section 3, all such keys are stored in the leftmost child subtree).\nThus, we set v←v.C[0]and continue our search in the leftmost child;\n4. If key > v.Rep[k - 1] , the keycan be found only in v.C[k]subtree (as\nexplainedinSection3,allsuchkeysarestoredintherightmostchildsubtree).\nThus, we set v←v.C[k]and continue our search in the rightmost child;\n5. Otherwise,wefind jsuchthat v.Rep[j - 1] < key < v.Rep[j] .Asstated\nin Section 3 keycan be found only in v.C[j]. Thus, we set v←v.C[j]\nand continue our search in the j-th child.\nWe can implement the aforementioned algorithm the following way (List-\ning 1.1):\n\nParallel-batched Interpolation Search Tree 9\nListing 1.1: An algorithm to search a key in an IST\nfun IST.Contains(key):\nv := IST.Root\nwhile true:\nif v = nil: return false\nk :=|v.Rep |\nif key < v.Rep[0]: v ←v.C[0]\nelif key > v.Rep[k - 1]: v ←v.C[k]\nelse:\nj := interpolation_search(v.Rep, key)\nif key = v.Rep[j]: return true\nv←v.C[j]\n3.4 Executing update operations and maintaining balance\nThe algorithm for inserting a key into IST is very similar to the search algorithm\nabove. To execute IST.Insert(key) we do the following (Fig. 6):\n1. Initialize v := IST.Root :weinsertthedesiredkeyinthesubtreeof v,which\nis initially the whole IST;\n2. For the current node v, ifkeyappears in v.Reparray, we finish the opera-\ntion — the key already exists.\n3. If vis a leaf and keydoes not exists in v.Rep, insert keyinto v.Repkeeping\nit sorted (we can find the insertion position for the new key as described in\nSection 3.2);\n4. If vis an inner node and keydoes not exists in v.Rep, determine in which\nchild the insertion should continue (we do it the same way as in Section 3.3),\nsetv←v.C[next_child_idx] and go to step (2).\n\n10 Vitaly Aksenov, Ilya Kokorin, and Alena Martsenyuk\nFig.6: Insert 15: proceed from the root to the second child since 2<15<50\nand then to the first child since 15<20\nToremoveakeyfromISTweintroduce Existsarrayineachnodethatshows\nwhether the corresponding key in Repis in the set or not. Thus, we just need\nto mark a key as removed without physically deleting it. We have to take into\naccount such marked keys during contains and insert operations. The removal\nalgorithm is discussed in more detail in [15,7,19].\nThe problem with these update algorithms is that all the new keys may be\ninserted to a single leaf, making the IST unbalanced (Fig. 7). In order to keep\nthe execution time low, we should keep the tree balanced.\n\nParallel-batched Interpolation Search Tree 11\nFig.7: Example of an unbalanced IST where the left leaf stores a single key while\nthe right leaf stores many keys.\nDefinition 5. Suppose His some small integer constant, e.g., 10. An IST T,\nstoring keys x0< x 1< . . . < x n−1, is said to be ideally balanced if either :\n–Tis a leaf IST and n≤H;\n–Tis a non-leaf IST and (here kis the size of the of the Reparray):\n•n > H;\n•k∈Θ(√n);\n•elements in Repare equally spaced: Rep[0]must equal to x⌊n\nk⌋,Rep[1]\nmust equal to x2·⌊n\nk⌋,Rep[2]must equal to x3·⌊n\nk⌋, and so on: Rep[i]\nmust equal to x(i+1)·⌊n\nk⌋;\n•all child ISTs {Ci}k\ni=0are ideally balanced;\nConsider an ideally balanced IST storing nkeys (Fig. 8). As stated in [15]:\n–The root of IST contains Θ(√n) =Θ(n1\n2)keys in its Reparray;\n–Any node on the second level has Reparray of size Θ(p\nn1\n2) =Θ(n1\n4);\n–Any node on the third level of has Reparray of size Θ(p\nn1\n4) =Θ(n1\n8)keys\nin its Reparray;\nGenerally, any node on the t-th level has Reparray of size Θ(n1\n2t). Thus, an\nideally balanced IST with nkeys has a height of O(log log n).\n\n12 Vitaly Aksenov, Ilya Kokorin, and Alena Martsenyuk\nFig.8: Height of an ideal IST\nIn order to keep IST balanced we maintain the number of modifications\n(both insertions and removals) applied to each subtree T. When the number of\nmodifications applied to some subtree Texceeds the initial size of Tmultiplied\nby some constant C, we rebuild Tfrom scratch making it ideally balanced. This\nrebuilding approach has a proper amortized bounds and is adopted from papers\nabout IST [15,7,19].\n3.5 Time and space complexity\nMehlhornandTsakalidis[15]giveadefinitionofa smooth probability distribution .\nFor example, uniform distribution U(a;b)is smooth for every −∞< a < b <\n+∞.\nDefinition 6. An insertion is µ-random if the key to be inserted is drawn from\nprobability density µ.\nA removal is random if every key present in the set is equally likely to be\nremoved.\nMehlhornandTsakalidis[15]statethatifprobabilitydistribution µissmooth:\n–IST with nkeys takes O(n)space;\n–The expected amortized insertion and removal cost is O(log log n);\n–The amortized insertion and removal cost is O(logn);\n–The expected search time on IST, generated by µ-random insertions and\nrandom removals, is O(log log n);\n\nParallel-batched Interpolation Search Tree 13\n–the worst-case search time is O(log2n);\nThese estimations are based on the fact, that the expected cost of the inter-\npolation search on a sorted array, generated by µ-random insertions and random\nremovals given that µis smooth, is O(1), and the ideally balanced IST storing\nnkeys has depth O(log log n).\nTherefore, IST can execute operations in o(logn)time under reasonable as-\nsumptions. As our goal, we want to design a parallel-batched version of the IST\nthat processes operations asymptotically faster than known sorted set imple-\nmentations (e.g., red-black trees).\n4 Parallel-batched Contains\nIn this section, we describe the implementation of ContainsBatched(keys[])\noperation, introduced in Section 2.2. We suppose that keysarray is sorted. For\nsimplicity, we assume that IST does not support removals. In Section 6, we\nelaborate on how to support removal operations and what changes should we do\nin the ContainsBatched implementation in order to support removals.\nWe implement ContainsBatched operation the following way. At first, we in-\ntroduceafunction BatchedTraverse(node, keys[], left, right, result[]) .\nThe purpose of this function is to determine for each index left≤i<right,\nwhether keys[i] is stored in the nodesubtree. If so, set result[i] = true ,\notherwise, result[i] = false . Given the operation BatchedTraverse , we can\nimplement ContainsBatched with almost zero effort (Listing 1.2):\nListing 1.2: Implementation of ContainsBatched on top of BatchedTraverse\nroutine\nfun ContainsBatched(keys[]):\nresult[] := [array of size |keys|]\n// search for all keys in the root subtree (i.e., in the whole IST)\nBatchedTraverse(IST.Root, keys, 0, |keys|, result)\nreturn result\nNow,wedescribe BatchedTraverse(node, keys[], left, right, result[])\nimplementation in leaf IST nodes in Section 4.1 and in non-leaf IST nodes in\nSection 4.2.\n4.1 BatchedTraverse in a leaf node\nIfnodeis a leaf node, we determine for each key in keys[left..right) whether\nit exists in node.Rep . Since nodeis a leaf, keys cannot be found anywhere else\ninnodesubtree.\nWemayuse Rankfunctiontofindthe rankofeachelementof keys[left..right)\ninnode.Rep and, thus, determine for each key whether it exists in node.Rep\n(Fig.9).AspresentedinSection2.4,ranksofallkeysfromsubarray keys[left..right)\n\n14 Vitaly Aksenov, Ilya Kokorin, and Alena Martsenyuk\ninnode.Rep may be computed in parallel in O(right−left+|node.Rep |)work\nandO(log2(right−left+|node.Rep |))span.\nLet us denote the rank of the key being searched r := RankElem(node.Rep,\nkey). Note that requals to |{x∈node.Rep :x≤key}|— the number of\nelements of node.Rep , that are less than or equal to the key.\n–Ifrequals zero, it means that all the elements of node.Rep are strictly\ngreater than key. Thus, keyis not in node.Rep and the result is false;\n–Otherwise, relementsof node.Rep arelessthanorequalto key.Inthatcase,\nwe check node.Rep[r - 1] : if it equals key, the key is found in node.Rep ,\notherwise, the key is not found (Fig. 9).\nFig.9: Execution of BatchedTraverse in an IST leaf. Here Rank(node.Rep,\nkeys[left..right)) = [0, 1, 2, 2, 3, 4] .\nWe can check the existence of all the keys and set all the results in parallel\nusing parallel loop the following way (Listing 1.3):\nListing 1.3: Using Rankto find keys in a leaf node in parallel\nrank := Rank(node.Rep, keys[left..right))\npfor i in left..right:\nr := rank[i - left]\nif r = 0 or node.Rep[r - 1] ̸=keys[i]:\nresult[i] ←false\nelse:\nresult[i] ←true\n\nParallel-batched Interpolation Search Tree 15\n4.2 BatchedTraverse in an inner node\nConsider now the BatchedTraverse procedure on an inner node (Fig. 10).\nFig.10: Execution of BatchedTraverse in an inner node of an IST.\nWebeginitsexecutionwithfindingthepositionforeachkeyfrom keys[left..right)\ninnode.Rep (i.e., the number of node.Rep elements less than each given key).\nWe may do it using Rankfunction as in Section 4.1 — it will take O(right−\nleft+|node.Rep |)work and O(log2(right−left+|node.Rep |))span.\nHowever, we can also use the interpolation search (described in Section 3.2)\nand parallel loop (described in Section 2.4) for that purpose: see Listing 1.4.\nListing 1.4: Using interpolation search to find keys in IST node in parallel\nkey_indexes := [array of size (right - left)]\npfor i in left..right:\nkey_indexes[i - left] ←interpolation_search(node.Rep, keys[i])\nDenoting Sas an interpolation search time in node.Rep , this algorithm takes\nO((right−left)·S)work and O(log ( right−left)·T)span. As stated in Sec-\ntion 3.5, Sis expected to be O(1)if the IST is obtained using µ-random inser-\ntions and random removals given that µissmooth. Thus, under this assumptions\ninterpolation-based traverse is expected to be faster than Rank-based one (from\nSection 4.1) when node.Rep array is big.\nSome keys of the input array keys[left..right) (e.g., 5and11in Fig. 10)\nare found in the Reparray. For such keys, we set Result[i] ←true(indeed,\nthey exist in Reparray, therefore they exist in nodesubtree). All other keys can\nbe divided into three categories:\n–Keys that are strictly less than Rep[0](e.g., 0and 2in Fig. 10) can only\nbe found in C[0]subtree (as explained in Section 3.1). Therefore, for such\nkeys we should continue the traversal in C[0].\n–Keys that are strictly greater than Rep[k - 1] (e.g., 100and101in Fig. 10)\ncan only be found in C[k](as explained in Section 3.1). Therefore, for such\nkeys we continue the traversal in C[k].\n\n16 Vitaly Aksenov, Ilya Kokorin, and Alena Martsenyuk\n–Keys that lie between Rep[i]andRep[i + 1] for some i∈[0;k−2](e.g., 6\nand7fori = 1or9and10fori = 2in Fig. 10) can only be found in C[i\n+ 1](as explained in Section 3.1). Therefore, for such keys we continue the\ntraversal in C[i + 1] .\nNote that some child nodes (e.g., C[1]andC[4]in Fig. 10) are guaranteed\nnot to contain any key from keys[left..right) thus we do not continue the\nsearch in such nodes.\nAfterdetermininginwhichchildthesearchofeachkeyfrom keys[left..right)\nshould continue we proceed to searching for keys in children in parallel: for ex-\nample in Figure 10 we would continue searching:\n–Forkeys[0..2) inC[0], thus we execute BatchedTraverse(node.C[0],\nkeys[], 0, 2, result[]) ;\n–Forkeys[3..5) inC[2], thus we execute BatchedTraverse(node.C[2],\nkeys[], 3, 5, result[]) ;\n–Forkeys[5..7) inC[3], thus we execute BatchedTraverse(node.C[3],\nkeys[], 5, 7, result[]) ;\n–Forkeys[8..10) inC[5], thus we execute BatchedTraverse(node.C[5],\nkeys[], 8, 10, result[]) ;\nAll these BatchedTraverse calls can be done in parallel, since there is no\ndependencies between them.\n5 Parallel-batched Insert\nWe now consider the implementation of the operation InsertBatched(keys[]) ,\nintroduced in Section 2.2. Again, we suppose that array keys[]is sorted. For\nsimplicity, we consider InsertBatched implementation on an IST without re-\nmovals.InSection6,weexplainhowwhatchangesshouldwedointhe InsertBatched\nimplementation to support remove operations.\nFor simplicity now we consider batch-insertion into unbalanced IST. In Sec-\ntion 7 we elaborate on how to make the IST balanced.\nWe begin the insertion procedure with filtering out keys already present in\nthe IST since there is no point in inserting such keys to the set. We can do this\nusing the ContainsBatched routine (see Section 4) together with the Filter\nprimitive (see Section 2.4): we filter out all the keys for which ContainsBatched\nreturns true. For example, if the IST contains keys 1, 3, 5, 7, 9 and we are\ngoing to insert keys [2, 4, 5, 7, 8] in it, we filter out keys 5and 7(since\nthey already exist in the IST) and insert only keys [2, 4, 8] .\nWeimplementourprocedurerecursivelyinthesamewayas BatchedTraverse .\nNote, that each keybeing inserted is not present in IST, thus for each keyour\ntraversal finishes in some leaf (Fig. 11) since we cannot find that keyin some\nReparray on the traversal path.\n\nParallel-batched Interpolation Search Tree 17\nFig.11: Inserting a batch of keys in the IST\nAfter we finish the insertion traversal, we end up with a collection of leaves\n{leafi}t\ni=1and in each leaf leafifrom that collection we should insert some cor-\nresponding subarray keys[left i. . .righti)of the batch being inserted. Note,\nthat in some leaves we do not insert any keys (e.g., the leaf in the middle from\nFig. 11). For example, in Fig. 11 we insert keys[0..2) (i.e., 0and 3) to the\nleftmost leaf, while inserting keys[2..4) (i.e., 18and19) to the rightmost leaf.\nTo finish the insertion, we just merge keys[left i. . .righti)with leaf i.Rep\nandgetthenew Reparray: leafi.Rep←Merge(keys[ lefti. . . right i),leafi.Rep).\nNow, each target leaf leaf icontains all the keys that should be inserted into it.\n\n18 Vitaly Aksenov, Ilya Kokorin, and Alena Martsenyuk\n6 Parallel-batched Remove\nWe now consider the implementation of the operation RemoveBatched(keys[]) ,\nintroduced in Section 2.2. Again, we suppose that array keysis sorted.\nWe begin the removal procedure with filtering out keys, not presented in the\nIST, since there is no point in removing such keys from the IST. We can do this\nusing the ContainsBatched routine (see Section 4) together with the Filter\nprimitive (see Section 2.4): we filter out all the keys for which ContainsBatched\nreturns false. For example, if the IST contains keys 1, 3, 5, 7, 9 and we are\ngoing to remove keys [2, 3, 6, 7, 9] from it, we filter out keys 2and6(since\nthey do not exist in the IST) and thus we remove only the keys [3, 7, 9] .\nThe remove traversal is executed similarly to the traversal from Section 4\nwith one major difference: when we encounter keyto be removed in Reparray of\nsome node vwe mark the key as logically removed without physically deleting\nit from v.Rep(Fig 12).\n\nParallel-batched Interpolation Search Tree 19\nFig.12: Removing a batch of keys from the IST\nTo allow marking keys as removed we augment each node vwith an array\nExists.v.Exists array has the same size as v.Rep(say k) and all its elements\nare initially set to true. When we need to remove i-th key from v.Rep, we just\nsetv.Exists[i] ←false(similar technique was proposed e.g., in [15,7,19]).\nNote that now we may have keys, that physically exist in some Reparray\nin the tree, but such keys do not exists in the tree logically, since they have\nbeen removed by some previous RemoveBatched operation (see e.g., keys 1,10\nand 23in Fig. 12). Since now we have a logical removal, we should modify\nthe implementations of ContainsBatched (from Section 4) and InsertBatched\n(from Section 5) so that these operations would take into account the possible\nexistence of such logically removed keys.\n\n20 Vitaly Aksenov, Ilya Kokorin, and Alena Martsenyuk\nDuring the execution of ContainsBatched when we encounter the keybe-\ning searched in the Reparray of some node v(say v.Rep[i] = key ), we check\nv.Exists[i] :\n–Ifv.Exists[i] = true then we conclude that keyexists in the set;\n–Otherwise, we conclude that keydoes not exist in the set, since it has been\nremoved by previous RemoveBatched operation;\nNow, we explain the updates to InsertBatched procedure. As was stated in\nSection 5, we cannot encounter any of the keys being inserted in the Reparray\nof any node of the IST, since we filter out all the keys existing in the IST before\nthe insertion actually begins. However, when keys can be logically removed this\nis not true anymore: we can encounter logically removed keys in the Reparray of\nsome node v. Of course, such keys have the corresponding entries in v.Exists\narray set to false, since such keys do not logically exist in IST (Fig. 13a).\nSuppose we are inserting keyto the IST and we encounter it in the Rep\narray of some node v(i.e., v.Rep[i] = key for some i). Thus, we can just\nsetv.Exists[i] ←true(Fig. 13b). This way the insert operation “revives” a\npreviously removed key.\n(a) Keys 1,10and 23are marked as\nremoved\n(b) Keys 10and 23are “revived“ by a\nsubsequent insert operation\nFig.13: Insertion of a key, that still exists in the IST physically, but is removed\nlogically\n7 Parallel tree rebuilding\n7.1 Rebuilding principle\nFollowing the removal algorithm from Section 6, we face the following problem:\nlogically removed keys still reside in the IST wasting memory space (Fig. 14),\nsincewedonothaveanymechanismforphysicallydeletingremovedkeys.Wecan\neven end up in a situation when all the keys in the IST are logically removed, but\n\nParallel-batched Interpolation Search Tree 21\nthe IST still occupies a great amount of memory without being able to reclaim\nany of the occupied space.\nFig.14: IST, that is logically empty, still occupies some space for removed keys\nWe can employ subtree rebuilding approach to reclaim space, occupied by\nlogically removed keys. Moreover, as stated in Section 3.4, we employ the very\nsame approach to keep the IST balanced (and thus keep the operation execution\ntime bounded).\nOur rebuilding algorithm is conceptually similar to the algorithm proposed\ne.g., in [15,7,19]: For each node of the ISTwe maintain Mod_Cnt — the number of\nmodifications (successful insertions and removals) applied to that node subtree.\nMoreover, each node stores Init_Subtree_Size — the number of keys in that\nnode subtree when the node was created.\nSupposeweareexecutinganupdatebatchoperation Op(i.e.,either InsertBatched\norRemoveBatched ) in node vand Opincreases v.Mod_Cnt byk(i.e., it either\ninserts knew keys to vsubtree or removes kexisting keys from vsubtree).\nIfv.Mod_Cnt + k ≤C·v.Init_Subtree_Size (where Cis a predefined\ninteger constant, e.g., 2) we increment v.Mod_Cnt bykand continue the execu-\ntion of Opin an ordinary way (as described in Sections 5 and 6); Otherwise,\nwe rebuild the whole subtree of v. The subtree rebuilding works in the following\nway:\n1. Weflattenthe subtree into an array: we collect all non-removed keys from\nthe subtree to array subtree_keys in ascending order. This operation is\ndescribed in more detail in Section 7.2;\n2. We apply the operation, that triggered the rebuilding, to the subtree_keys\narray:\n\n22 Vitaly Aksenov, Ilya Kokorin, and Alena Martsenyuk\n(a) If InsertBatched(v, keys[], left, right) operation triggered the\nrebuilding,wemerge(seeSection2.4) keys[left..right) intosubtree_keys :\nsubtree_keys ←Merge(subtree_keys, keys[left..right)) ;\n(b) If RemoveBatched(v, keys[], left, right) operation triggered the\nrebuilding, we filter out keys[left..right) from the subtree_keys via\ntheDifference operation(seeSection2.4): subtree_keys ←Difference(\nsubtree_keys, keys[left..right)) ;\n3. Finally, we build an ideal IST v’, containing all entries from subtree_keys .\nThis operation is described in more detail in Section 7.3. Note, that v’will\ncontain all the keys we were inserting into v(if we are inserting new keys)\nor will not contain the keys were removing from v(if we are removing the\nexisting keys).\n7.2 Flattening an IST into an array in parallel\nFirst of all, we need to know how many keys are located in each node subtree. We\nstore this number in a Sizevariable in each node and maintain it the following\nway:\n–When creating new node v, initialize v.Sizewith the initial number of keys\ninvsubtree;\n–When inserting mnew keys to v’s subtree, increment v.Sizebym;\n–When removing mexisting keys from v’s subtree, decrement v.Sizebym.\nTo flatten the whole subtree of nodewe allocate an array subtree_keys of\nsizenode.Size where we shall store all the keys from nodesubtree. We imple-\nment the flattening recursively, via the Flatten(v, subtree_keys[], left,\nright)procedure, which fills subtree_keys[left..right) subarray with all\nthe keys from v’s subtree. To flatten the whole subtree of nodeinto newly-\nallocatedarray subtree_keys ofsize node.Size weuse Flatten(node, subtree_keys,\n0, node.Size) .\nNote that non-leaf node vhas2k + 1sources of keys:\n–v.C[0]containing v.C[0].Size keys;\n–v.Rep[0] containing 1key if v.Exists[0] = true and0keys otherwise;\n–v.C[1]containing v.C[1].Size keys;\n–v.Rep[1] containing 1key if v.Exists[1] = true and0keys otherwise;\n–...\n–v.Rep[k - 1] containing 1key if v.Exists[k - 1] = true and 0keys\notherwise;\n–v.C[k]containing v.C[k].Size keys.\nHere C[i]is(2·i)-th key source and Rep[i]is(2·i + 1)-th key source.\nNote that for a leaf node all children just contain zero keys.\nNow for each key source we must find the position in the subtree_keys\narray where that source will place its keys. To do this we calculate array sizes\n\nParallel-batched Interpolation Search Tree 23\nof size 2k + 1:i-th source of keys stores its keys count in sizes[i] . After that\nwe calculate positions := Scan(sizes) to find the prefix sums of sizes array.\nAfter that\npositions [i] =\n\n0 i= 0\ni−1P\nj=0sizes [j]i >0\nConsidernow i-thkeysource.Allpriorkeysourceswillcontain positions[i]\nkeys in total, thus i-th key source should place its keys into subtree_keys array\nstarting from left + positions[i] position (Fig. 15). Therefore:\n–v.C[i]places its keys in the subtree_keys array starting from left +\npositions[2 ·i](since v.C[i]is(2·i)-th key source). Thus we exe-\ncute Flatten(v.C[i], subtree_keys, left + positions[2 ·i], left\n+ positions[2 ·i] + v.C[i].Size) tolet C[i]placeitskeysinthe subtree_keys\narray;\n–Ifv.Exists[i] = false then v.Rep[i] shouldnotbeputinthe subtree_keys\narray since v.Rep[i] is logically removed;\n–Otherwise, v.Exists[i] = true andv.Rep[i] shouldbeplacedat subtree_keys[left\n+ positions[2 ·i + 1]] since v.Rep[i] isthe (2·i + 1)-thkeysource.\nEach key source can be processed in parallel, since there are no data depen-\ndencies between them.\nFig.15: Parallel flattening of an IST node\nIn Fig. 15, C[0]will place its keys in keys[left .. left + 3) subarray,\nRep[0]will be placed in keys[left + 3] ,C[1]will place its keys in keys[left\n+ 4 .. left + 5) subarray, Rep[1]will not be placed in keysarray since its\nlogically removed, C[2]will place its keys in keys[left + 5 .. left + 9)\nsubarray, Rep[2]will be placed in keys[left + 9] andC[3]will place its keys\ninkeys[left + 10 .. left + 12) subarray.\n\n24 Vitaly Aksenov, Ilya Kokorin, and Alena Martsenyuk\n7.3 Building an ideal IST in parallel\nSuppose we have a sorted array of keys and we want to build an ideally bal-\nanced IST (see Section 3.4) from these keys. We implement this procedure\nrecursively via build_IST_subarray(keys[], left, right) procedure — it\nbuilds an ideal IST containing keys from the keys[left..right) subarray and\nreturns the root of the newly-built subtree. Thus, to build a new subtree from ar-\nraykeyswe just use new_root := build_IST_subarray(keys[], 0, |keys|)\nto obtain the root of the IST built from the whole keysarray.\nIf the size of the subarray (i.e., right - left ) is less than some constant\nH, we return a leaf node containing all the keys from keys[left..right) in its\nReparray.\nOtherwise (i.e., if right - left is big enough), we have to build a non-\nleaf node. Let us denote m := right - left; k := ⌊√m⌋−1. As follows from\nDefinition 5, Reparray should have size Θ(√m)and its elements must be equally\nspaced keys of the initial array. Thus, we copy k-th key into Rep[0],(2·k)-th\nkey into Rep[1],(3·k)-th key into Rep[2]and so on — in general, we copy\n(i+ 1)·k-th key into Rep[i]. Note, our subarray keys[left..right) begins\nat position leftof the initial array keys. Thus, we copy key keys[left + (i\n+ 1)·k]into Rep[i]. All the copying can be done in parallel since there are\nno data dependencies. This way we obtain Reparray of size Θ(√m)filled with\nequally-spaced keys of the initial subarray (Fig. 16).\nNow we should build the children of the newly-created node (Fig. 16):\n–Rep[0] = keys[left + k] . Thus, all keys less than keys[left + k] will\nbe stored in C[0]subtree (see Section 3 for details). Since keysarray\nis sorted, C[0]must be built from keys[left..left + k) , thus C[0] =\nbuild_IST_subarray(keys[], left, left + k) ;\n–for1≤i≤k−2,Rep[i - 1] = keys[left + i ·k]andRep[i] = keys[left\n+ (i + 1) ·k]. Thus, all keys xsuch that Rep[i-1] < x < Rep[i] should\nbestoredin C[i]subtree.Since keysarrayissorted, C[i]mustbebuiltfrom\nthesubarray keys[left + i ·k + 1..left + (i + 1) ·k),thus C[i] =\nbuild_IST_subarray(keys[], left + i ·k + 1, left + (i + 1) ·k);\n–Rep[k - 1] = keys[left + k2](note,that left + k2= left + (⌊√right −left⌋ −1)2<\nright). Thus, all keys greater than keys[left + k2]are stored in C[k]\nsubtree. Since keysarray is sorted, C[k]must be built from keys[left +\nk2..right) ,thus C[k] = build_IST_subarray(keys[], left + k2, right) .\nWe can build all children in parallel, since there is no data dependencies\nbetween them.\n\nParallel-batched Interpolation Search Tree 25\nFig.16: Building children of a new node\nTo finish the construction of a node we need to calculate node.ID array\ndescribed in Section 3.2. We can build it in the following way:\n1. Create an array boundsof size m + 1such that bound[i] =keys[left] +i·\n(keys[right - 1] −keys[left] )/mwhere m=Θ(nε)withsome ε∈[1\n2; 1);\n2. Use ID := Rank(bounds, node.Rep) . After that ID[i] = j iffRep[j]≤\nbounds[i] < Rep [j+ 1]\n8 Theoretical results\nIn this section, we present the theoretical bounds for our data structure. These\nbounds are quite trivial, so, we just explain the intuition behind them.\nTheorem 1. The flatten operation of an IST with nkeys has O(n)work and\nO(log3n)span. The building procedure of an ideal IST from an array of size n\nhasO(n)work and O(logn·log log n)span. Thus, the rebuilding of IST with n\nkeys costs O(n)work and O(log3n)span.\nProof (Sketch). While the work bounds are trivial, we are more interested in\nspan bounds. From [15] we know that in the worst case, the height of IST with n\nkeys does not exceed O(log2n). Thus, the flatten operation just goes recursively\nintoO(log2n)levels and spends O(logn)span at each level. This gives O(log3n)\nspan in total. The construction of an ideal IST has O(log log n)recursive levels\nwhile each level can be executed in O(logn)time, i.e., copy the elements into\nReparray. This gives us the result.\nThis brings us closer to our main complexity theorem.\nTheorem 2. The work of a batched operation on our parallel-batched IST has\nthe same complexity as if we apply all moperations from this batch sequen-\ntially to the original IST of size n(from [15], the expected execution time is\nO(mlog log n)). The total span of a batched operation is O(log4n).\n\n26 Vitaly Aksenov, Ilya Kokorin, and Alena Martsenyuk\nProof (Sketch). The work bound is trivial — the only difference with the original\nIST is that we can rebuild the subtree in advance before applying some of the\noperations. Now, we get to the span bounds. From [15], we know that the height\nof IST with nkeys does not exceed O(log2n). On each level, we spend: 1) at\nmost O(log2n)span for merge and rank operations; or 2) we rebuild a subtree\nat that level and stop. The first part gives us O(log4n)span, while rebuilding\ntakes just O(log3n)span. This leads us to the result of the total O(log4n)span.\n9 Experiments\nWe have implemented the Parallel Batched IST in C++ using OpenCilk [6] as\na framework for fork-join parallelism.\nWe tested our parallel-batched IST on three workloads. We initialize the tree\nwith elements from the range [−108; 108], each taken with probability 1/2. Thus,\nthe expected size of the tree is 108. Then we execute:\n–Search for a batch of 107keys, taken uniformly at random from the range;\n–Insert a batch of random 107keys, taken uniformly at random from the\nrange;\n–Remove a batch of random 107keys, taken uniformly at random from the\nrange.\nThe experimental results are shown in Fig. 17. The OX axis corresponds to\nthenumberofworkerprocessesandtheOYaxiscorrespondstothetimerequired\nto execute the operation in milliseconds. Each point of the plot is obtained as\nan average of 10runs. We run our code on an Intel Xeon Gold 6230 machine\nwith 16cores.\nFig.17: Benchmark results for Parallel-batched Interpolation Search Tree\nAs shown in the results, we achieve good scalability. Indeed:\n–14x scaling on ContainsBatched operation for 16 processes;\n\nParallel-batched Interpolation Search Tree 27\n–11x scaling on InsertBatched operation for 16 processes;\n–13x scaling on RemoveBatched operation for 16 processes.\nWe also compared our implementation in a sequential mode with std::set\nimplemented via red-back tree. On our machine std::set took 9257 ms to check\nthe existence of 107keys in a set with 108elements while our IST implemen-\ntation took only 3561 ms using one process. We achieve such speedup by using\ninterpolation search as described in Section 3.2 and by processing high levels of\nthe tree only once for the whole keys batch.\n10 Conclusion\nIn this work, we presented the first parallel-batched version of the interpolation\nsearch tree that has an optimal work in comparison to the sequential imple-\nmentation and has a polylogarithmic span. We implemented it and got very\npromising results. We believe that this work will encourage others to look into\nparallel-batched data structures based on something more complex than binary\nsearch trees.\nReferences\n1. Acar, U.A., Blelloch, G.E.: Algorithms: Parallel and sequential.\nhttps://www.umut-acar.org/algorithms-book 6(2019)\n2. Akhremtsev, Y., Sanders, P.: Fast parallel operations on search trees. In: 2016\nIEEE 23rd International Conference on High Performance Computing (HiPC). pp.\n291–300. IEEE (2016)\n3. Bentley, J.L., Yao, A.C.C.: An almost optimal algorithm for unbounded searching.\nInformation processing letters 5(SLAC-PUB-1679) (1976)\n4. Blelloch, G.E., Ferizovic, D., Sun, Y.: Just join for parallel ordered sets. In: Pro-\nceedings of the 28th ACM Symposium on Parallelism in Algorithms and Architec-\ntures. pp. 253–264 (2016)\n5. Blelloch, G.E., Reid-Miller, M.: Fast set operations using treaps. In: Proceedings\nof the tenth annual ACM symposium on Parallel algorithms and architectures. pp.\n16–26 (1998)\n6. Blumofe, R.D., Joerg, C.F., Kuszmaul, B.C., Leiserson, C.E., Randall, K.H., Zhou,\nY.: Cilk: An efficient multithreaded runtime system. Journal of parallel and dis-\ntributed computing 37(1), 55–69 (1996)\n7. Brown, T., Prokopec, A., Alistarh, D.: Non-blocking interpolation search trees\nwith doubly-logarithmic running time. In: Proceedings of the 25th ACM SIG-\nPLAN Symposium on Principles and Practice of Parallel Programming. pp. 276–\n291 (2020)\n8. Comer, D.: Ubiquitous b-tree. ACM Computing Surveys (CSUR) 11(2), 121–137\n(1979)\n9. Graefe, G., et al.: Modern b-tree techniques. Foundations and Trends ®in\nDatabases 3(4), 203–402 (2011)\n10. Guibas, L.J., Sedgewick, R.: A dichromatic framework for balanced trees. In: 19th\nAnnual Symposium on Foundations of Computer Science (sfcs 1978). pp. 8–21.\nIEEE (1978)\n\n28 Vitaly Aksenov, Ilya Kokorin, and Alena Martsenyuk\n11. JáJá, J.: An introduction to parallel algorithms. Reading, MA: Addison-Wesley\n10, 133889 (1992)\n12. Kraska,T.,Beutel,A.,Chi,E.H.,Dean,J.,Polyzotis,N.:Thecaseforlearnedindex\nstructures. In: Proceedings of the 2018 international conference on management of\ndata. pp. 489–504 (2018)\n13. Lea, D.: A java fork/join framework. In: Proceedings of the ACM 2000 conference\non Java Grande. pp. 36–43 (2000)\n14. Medidi, M., Deo, N.: Parallel dictionaries using avl trees. Journal of Parallel and\nDistributed Computing 49(1), 146–155 (1998)\n15. Mehlhorn, K., Tsakalidis, A.: Dynamic interpolation search. Journal of the ACM\n(JACM) 40(3), 621–634 (1993)\n16. Park, H., Park, K.: Parallel algorithms for red–black trees. Theoretical Computer\nScience 262(1-2), 415–435 (2001)\n17. Paul, W., Vishkin, U., Wagener, H.: Parallel dictionaries on 2–3 trees. In: Inter-\nnational Colloquium on Automata, Languages, and Programming. pp. 597–609.\nSpringer (1983)\n18. Peterson, W.W.: Addressing for random-access storage. IBM journal of Research\nand Development 1(2), 130–146 (1957)\n19. Prokopec, A., Brown, T., Alistarh, D.: Analysis and evaluation of non-blocking\ninterpolation search trees. arXiv preprint arXiv:2001.00413 (2020)\n20. Pugh, W.: Skip lists: a probabilistic alternative to balanced trees. Communications\nof the ACM 33(6), 668–676 (1990)\n21. Sleator, D.D., Tarjan, R.E.: Self-adjusting binary search trees. Journal of the ACM\n(JACM) 32(3), 652–686 (1985)\n22. Sun, Y., Ferizovic, D., Belloch, G.E.: Pam: parallel augmented maps. In: Proceed-\nings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel\nProgramming. pp. 290–304 (2018)\n23. Willard, D.E.: Searching unindexed and nonuniformly generated files in \\log\\logn\ntime. SIAM Journal on Computing 14(4), 1013–1029 (1985)",
  "textLength": 47468
}