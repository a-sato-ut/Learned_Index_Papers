{
  "paperId": "84eaef2c9e6263d27e25aa67096ff365e6c267b0",
  "title": "Are Updatable Learned Indexes Ready?",
  "pdfPath": "84eaef2c9e6263d27e25aa67096ff365e6c267b0.pdf",
  "text": "Are Updatable Learned Indexes Ready?\nChaichon Wongkham\nThe Chinese University of Hong Kong\nwongkha21@cse.cuhk.edu.hkBaotong Lu\nThe Chinese University of Hong Kong\nbtlu@cse.cuhk.edu.hkChris Liu\nThe Chinese University of Hong Kong\ncyliu@cse.cuhk.edu.hk\nZhicong Zhong\nThe Chinese University of Hong Kong\nzczhong@cse.cuhk.edu.hkEric Lo\nThe Chinese University of Hong Kong\nericlo@cse.cuhk.edu.hkTianzheng Wang\nSimon Fraser University\ntzwang@sfu.ca\nABSTRACT\nRecently, numerous promising results have shown that updatable\nlearned indexes can perform better than traditional indexes with\nmuch lower memory space consumption. But it is unknown how\nthese learned indexes compare against each other and against the\ntraditional ones under realistic workloads with changing data distri-\nbutions and concurrency levels. This makes practitioners still wary\nabout how these new indexes would actually behave in practice. To\nfill this gap, this paper conducts the first comprehensive evaluation\non updatable learned indexes. Our evaluation uses ten real datasets\nand various workloads to challenge learned indexes in three aspects:\nperformance, memory space efficiency and robustness. Based on\nthe results, we give a series of takeaways that can guide the future\ndevelopment and deployment of learned indexes.\n1 INTRODUCTION\nRecent advances in machine learning (ML) have sparked a flurry\nof research on using ML to improve various database components.\nHowever, it remains unclear (1) what the relative merits are of the\nvarious techniques and (2) how those impressive results will hold\nup under various workloads. Practitioners are often wary about\nusing these new techniques in their systems without knowing how\nthey would perform in practice. In this paper, we hope to fill the\ngap by studying learned indexes [12,14‚Äì18,20‚Äì22,25,32,35,45,\n48,54,56,57,61,64,66], the arguably most well-studied learned\ndatabase engine component.\nAlthough preliminary evaluations on learned indexes exist, they\nlargely focused on read-only workloads [ 38,40]. A system built\nfor practical adoption, however, must also consider dynamic work-\nloads. Hence, we focus on updatable learned indexes. Moreover, to\nprovide a holistic evaluation, we focus not only on (1) performance\n(throughput and latency), but also (2) space efficiency and (3) ro-\nbustness under dynamic workloads. Next, we briefly introduce our\nevaluation methodology and highlight our findings.\n1.1 Performance\nSince learned indexes can easily overfit to a particular synthetic\ndistribution [ 2], the SOSD benchmark [ 23] has taken the initiative\nwith four real datasets for evaluating learned indexes. Recent eval-\nuations have started using SOSD‚Äôs real datasets [ 38,40], but they\nonly focused on read-only workloads. Among the four real datasets\nin SOSD, osmandfacebook are known to be more challenging for\nlearned indexes [ 38,40]. In contrast, learned indexes are able to out-\nperform traditional indexes on the wiki andbooks datasets [ 38,40].\nHowever, it is unclear how these results (i.e., which type of indexperforms better under which scenarios) may extend beyond the four\nconcrete datasets, leading to the question: can a learned index still\noutperform all traditional indexes on a variety of data? We observe\nthe fundamental reason is the lack of a common, quantitative metric\nto evaluate how ‚Äúhard‚Äù (or ‚Äúeasy‚Äù) a dataset is for a learned index to\nhandle. Furthermore, without a quantitative measure, it is hard for\npractitioners to know whether their target workload/production\ndata is ‚Äúeasy‚Äù or ‚Äúhard‚Äù, which may eventually impact which index\nshould be chosen to achieve the best performance.\nIn this paper, we propose to use piecewise linear approximation\n(PLA) [ 47] as an approximate metric to quantify the hardness of a\ndataset. PLA has a clear theoretical foundation from computational\ngeometry, which captures the minimal number of linear models\nrequired to fit a data distribution. It has been used to build a space-\noptimal learned index [ 18]. We use PLA to approximate the hardness\nof a dataset in two dimensions: global hardness andlocal hardness .\nThe former challenges the index‚Äôs structure design choices and the\ncost models that govern structural modification operations (SMOs);\nthe latter challenges the accuracy of various ML models.\nThe global and local hardness metrics based on PLA enable us\nto conduct a comprehensive study that pragmatically examines up-\ndatable learned indexes along three aspects: (1) data (from easy to\nhard ), (2)workload (from read-only towrite-only ), and (3) concur-\nrency (from single-threaded/single-socket tomulti-threaded/NUMA ).\nOur results revealed a number of interesting insights. For example,\nalthough our results show that recent updatable learned indexes\n(ALEX [ 15], LIPP [ 61]) perform better than traditional indexes (ART\n[27], Masstree [ 39], HOT [ 4] and Wormhole [ 62]) in over 80% of\nthe data-workload space under a single thread, some learned index\ndesign choices are actually at odd with concurrency control and\nshow regression under multiple threads.\n1.2 Space Efficiency\nMemory consumption is an important factor in production envi-\nronments. Learned indexes have been reported to use 4 √ó-2000√ó\nless memory compared to traditional indexes because they store\nsuccinct models instead of keys in nodes [ 15,25]. While their space\nadvantage has been proven in read-only environments, we found\nthat it might not hold up in a realistic environment with data modi-\nfications. In read-only workloads, the data array to be indexed can\nbe fully ordered and packed, so even the leaf nodes can store no\nkeys: they can store and use a model to point straight to the under-\nlying data array. In contrast, a real system often needs to handle\ndynamic workloads and builds indexes on non-primary keys. In\nboth cases, the underlying data array could be unsorted or the nodesarXiv:2207.02900v2  [cs.DB]  4 Sep 2022\n\nhave to leave spaces to accommodate insertions. Consequently, the\nindex often needs a leaf layer that explicitly stores the key-position\ninformation, bloating memory usage.\nIn this paper, we examine the end-to-end memory space efficiency\nof learned indexes including their leaf layer with key-position pairs.\nWe found that learned indexes are only 3.2 √ósmaller than traditional\nindexes at best and might even use more space than some traditional\nindexes. Note that traditional indexes can consume around 55%\nof the total memory in transactional in-memory databases [ 65].\nTherefore, our results suggest that memory saving is not a definitive\nadvantage of the current generation of updatable learned indexes\nand highlight the need for further improvements in this area.\n1.3 Robustness\nModern data systems often require predictable performance. Low\ntail latency is particularly important for user-facing\napplications [ 30]. In this paper, we examine whether the high\nperformance of learned indexes comes with a price on poor tail\nlatency. Furthermore, many workloads require efficient and robust\nsupport for range queries. In addition to point queries, we\ntherefore also test learned indexes‚Äô behavior under varying range\nscan sizes and compare them with state-of-the-art traditional\nindexes. Finally, the power of learned indexes is that they allow\nspecialization to a given data distribution, hence becoming\ninstance-optimized [2]. However, in a realistic environment, the\ndata can continuously evolve. It then becomes important to study\nhow well and how fast a learned index can adapt to changes. Yet\nexisting performance studies [ 23,38,40] have not looked into this\nissue. Some recent updatable learned indexes, such as ALEX [ 15]\nand XIndex [ 54], have briefly touched upon this issue but did so\nwithout comparing with a wide range of traditional/learned\nindexes as well as on different data hardness. In this paper, we\nstudy the impact of data distribution changes on learned indexes\nand compare their robustness with that of the traditional indexes.\nOur results show that although a few learned indexes generally\nshowed good resilience to changes in data distribution, most\ntraditional indexes exhibit even better, extremely robust\nperformance across different workloads and data distributions.\n1.4 Contributions, Limitations, and Roadmap\nWe have created and open-sourced a benchmarking suite, GRE1\nwhich makes it easy to compare against learned and traditional\nindexes under the evaluation spectrum proposed in this paper. To\nfacilitate future studies, GRE includes scripts to run the benchmark\nand visualize all experiments presented in this paper. While earlier\neffort [ 40] has presented a leaderboard for learned indexes, GRE\nfurther expands the dimension of the comparisons by presenting\nheatmaps to visualize which ‚Äúarea‚Äù in the data-workload spectrum\nhas already been ‚Äúconquered‚Äù by learned indexes and which area\nthe traditional indexes are still in possession.\nTo the best of our knowledge, this is the first comprehensive\nstudy of updatable learned indexes‚Äô performance, space efficiency\nand robustness. However, since learned indexes are still fast evolv-\ning with new proposals on handling multi-dimensional data [ 14,\n1Akin to the GRE test for a learned index to ‚Äúpass and graduate‚Äù for practical use.\nAvailable at https://github.com/gre4index/GRE .16,32,45,48,56,64], persistence [ 35] and string keys [ 52,57], we\nfocus on one-dimensional updatable indexes on numeric data and\nanticipate future work to cover the other aspects.\nIn the rest of this paper, Section 2 provides the background on\nupdatable learned indexes. Section 3 presents our experimental\nsetup, including how we approximate the hardness of a dataset.\nSections 4‚Äì7 then present the detailed empirical results. Section 8\nsummarizes our lessons learned and then we answer the theme\nquestion Are updatable learned indexes ready? in Section 9. Due to\nspace limit, more discussions and results could be found in [59].\n2 LEARNED INDEXES\nThe main intuition behind learned indexes is that if the data is a set\nof continuous integer keys (e.g., the keys 0 to 100M) in an array a,\nthe key itself can be directly used as an offset into the array (e.g.,\nthe value of key 77 can be accessed at a[77] ). This can potentially\nallow O(1) rather than O(log n) lookup complexity and significantly\nreduce index storage overhead. However, implementing a practi-\ncally useful learned index is challenging in terms of supporting\nnon-contiguous data, dynamic workloads, and concurrency.\n2.1 Structure and Last-Mile Search\nThe ideal case with O(1) complexity is based on a crucial assumption\nthat keys are continuous, which is not necessarily true in reality.\nTherefore, learned indexes usually train multiple ML models to\napproximate the cumulative distribution function (CDF) of the data,\nforming a hierarchy of models akin to the structure of B+-trees\nwith inner and leaf nodes. However, instead of laying out keys\nphysically, inner nodes in learned indexes store models. The model\nguides traversals to reach leaf nodes which store the actual data.\nIf simple linear models are used, the node only needs to store the\nslope and intercept, reducing storage footprint. There are two inner\nnode designs for updatable learned indexes:\nML for Child Search. In this design, an inner node ùêºuses a\nmodel to predict which child node of ùêºcontains the key. Hence,\nthe model of ùêºis trained using the information from its children\n(e.g., maximum and minimum keys of each child). Since a model\nmight be imperfect and give wrong predictions, it is necessary\nto search around the predicted position to reach the exact child.\nPGM-Index [18], XIndex [54] and FINEdex [31] adopt this design.\nML for Subspace Lookup. Indexes using this design\n(ALEX [ 15], APEX [ 35] and LIPP [ 61]) recursively partition the key\nspace, and a partition is represented by a slot in an inner node. A\nmodel in each inner node is used to decide which partition a key\nbelongs to. Thus, a traversal simply uses the model to compute\nwhich child (partition) shall be visited next, instead of predicting.\nWhen reaching a leaf node, there is always a ‚Äúlast-mile‚Äù search\naround the model predicated position because models may not be\nperfect. Last-mile search often is a performance bottleneck. Differ-\nent learned indexes use different designs to mitigate it:\nError-Driven Designs. This type of updatable learned indexes\nrequires as input an error threshold to bound the distance of the last-\nmile search; it is adopted by PGM-Index [ 18], XIndex [ 54], FITing-\nTree [ 20] and FINEdex [ 31]. In case a model‚Äôs error margin goes\nbeyond the given threshold, the index will carry out adjustment\nsuch as increasing the granularity of the models [ 22] or increasing\n\nAre Updatable Learned Indexes Ready?\nthe number of nodes (and hence increasing the number of models\nsince each node hosts one model) [18, 20, 54].\nPerformance-Driven Designs. ALEX [ 15] and APEX [ 35]\nmaintain runtime statistics (e.g., the number of keys searched in\nthe last-mile and tree height) to detect suboptimal behaviors due\nto model inaccuracies. These indexes maximize the accuracy by\ntriggering the best action (e.g., SMOs) based on an empirical-based\ncost model.\nCollision-Driven Designs. The key idea is to model inaccura-\ncies as ‚Äúcollisions.‚Äù Specifically, a model-based CDF can be seen as\nan order-preserving function where different keys could be ‚Äúhashed‚Äù\n(predicted) to the same ‚Äúbucket‚Äù (array position), causing collisions.\nHence, a last-mile search could be handled by an open-addressing\ncollision resolution scheme that gets or puts a collided key else-\nwhere in the index. Based on this idea, LIPP [ 61] attempts to elimi-\nnate last-mile search by finding a model that minimizes collisions.\nWhen a collision is inevitable, it uses a chaining scheme that creates\nnew nodes (or recursively a sub-tree of new nodes) to transform\nthe last-mile search problem to be a sub-tree traversal problem.\n2.2 Dynamic Workloads\nBeyond lookups, updatable learned indexes also need to support in-\nserts/deletes/updates which can change data distributions. Existing\nsolutions usually use tree-merge, delta-merge or sparse nodes.\nTree-Merge. As represented by PGM-Index [ 18], tree-merged\nbased approaches are inspired by LSM-trees [ 46] which build mul-\ntiple sub-indexes, each of which covers a subset of keys. Inserts\nare then handled by creating new sub-indexes by merging smaller\nsub-indexes with the new key to ensure the keys stay sorted. Up-\ndates are done in-place and deletes are implemented as inserts of\ntombstones. Consequently, tree-merge based designs inherit the\nperformance characteristics of LSM-trees, for example, searching\nfor a key may visit multiple sub-indexes of different sizes.\nDelta-Merge. Unlike tree-merge designs, in XIndex [ 54], FITing-\nTree [ 20] and FINEdex [ 31], new keys are collected in delta nodes\nand are merged periodically with the main data array with potential\nmodel retraining. Updates and deletes are done in-place. The main\ndifference among the various indexes under this design is their delta\ngranularity. For XIndex, new inserts are absorbed by a per-node\ndelta. FINEdex [ 31] maintains a delta per record to reduce conflicts\nwithin a node and facilitate parallel retraining.\nSparse-Nodes. To absorb future inserts, ALEX [ 15], APEX [ 35]\nand LIPP [ 61] leave space in tree nodes, making them sparse. Up-\ndates and deletes are also done in-place. The index then needs to\nconduct SMOs such as node merges and splits, similar to traditional\nindexes. Moreover, to balance between the tree‚Äôs fill factor and\nmodel accuracy, the index should consider learned-index specific\noperations, such as node resizing and model retraining when the\nworkload and data distribution change.\n2.3 Concurrency\nSince most learned indexes employ a hierarchy of models, classic\nconcurrency control approaches for trees can be adapted to work on\nlearned indexes. Among the surveyed in-memory learned indexes,\nonly FINEdex [ 31] and XIndex [ 54] support concurrency. To cope\nwith the in-memory environment, both of them use optimisticTable 1: Configurations of learned indexes. Indexes named\nwith a ‚Äú+‚Äù sign are our concurrent implementations.\nIndex Parameters\nALEX Max inner/data node size: 16MB\nMin/avg/max node density: 0.6/0.7/0.8\nALEX+ Max inner/data node size: 16MB/512KB\nMin/avg/max node density: 0.6/0.7/0.8\nLIPP(+) Node density: 0.5; max node size: 16MB\nSubtree inserted/conflict ratio: 2/0.1\nPGM-Index2Error bound: 16\nXIndex Error bound: 32; delta size: 256\nError tolerance: 1/4; max number of models per group: 4\nFINEdex Error bound: 32\nlocking, which associates a versioned lock per node. The lock word\ncarries a version number, such that readers only need to verify\nthe version did not change before and after their accesses; writers\nwould acquire locks as usual but also increment the version number.\nThis extracts more concurrency by allowing readers to proceed\n(especially, to traverse inner nodes) without holding locks.\nThe use of optimistic concurrency has impact on memory recla-\nmation and delta merging procedures. For example, in XIndex, be-\nfore merging a node‚Äôs delta with its main data array, it uses RCU [ 19]\nto ensure the existing readers of the delta have all finished. During\nthe merge, writers follow the normal write path but inserts would\nwork on yet another temporary per-node delta (so readers also need\nto read this temporary delta in additional to the index before merge;\nXIndex‚Äôs current implementation uses Masstree [ 39] to construct\nthe delta). Finally, the temporary delta would be promoted as the\nofficial delta after the merge is done. Using this approach, both\nreaders and writers are non-blocking.\n3 BENCHMARKING SETUP\nWe conduct experiments using various datasets and workloads, and\ncompare them against state-of-the-art traditional indexes. To set\nthe stage, we describe the necessary changes done to each index,\ndatasets and workloads, and our environment.\n3.1 Index Implementations\nSince the implementation of a learned index can highly influence ex-\nperimental results [ 23], we follow previous work [ 29,38,40,63] to\nuse the original authors‚Äô or widely-used open-source implementa-\ntions. Unless otherwise stated, we use the recommended parameters\nthat can be found in each index‚Äôs original implementation. All the\nevaluated learned indexes along with their configurations are listed\nin Table 1. Among the included learned indexes, LIPP and ALEX do\nnot support concurrency out-of-the-box, but our evaluation shows\nthat they are the most competitive under a single thread. So we\nimplemented concurrent versions of them (LIPP+ and ALEX+) to\nmake comprehensive comparisons and reason about their design\ndecisions in realistic multi-core environments.\n2PGM-index cannot support key in value 264‚àí1.fbcontains that key. Following\nhttps://github.com/gvinciguerra/PGM-index/issues/29 , we shifted all fbkeys by -1\nwhen using PGM-Index. Furthermore, the codebase of PGM-Index has continuously\nevolved since its publication. The latest version exposes 3 extra knobs. We tuned\nPGM-Index based on the authors‚Äô recommendation.\n\nLIPP recommended lock coupling [ 1] for concurrency\ncontrol [ 61]. However, coupling using traditional locks may\nseverely limit concurrency [ 28]. Although using optimistic lock\ncoupling [ 28] can mitigate the impact, we observe that LIPP\nactually requires no coupling because when following a pointer\nfrom a parent node to a child node, the pointer would not be\ninvalidated by another concurrent thread. Yet, LIPP does not\ndifferentiate inner node and leaf node but uses a unified node\nlayout: a node can store both data and pointers to child nodes.\nUnfortunately, this design choice limits scalability because (i) inner\nnodes now contain keys and must also maintain statistics for SMO\ndecision making (where ALEX only needs to maintain statistics for\nleaf nodes) and (ii) even a simple key insert that triggers no SMOs\ncould lock a node at any level (e.g., the root). We mitigate the latter\nby using item-level optimistic locks (without coupling) for LIPP\n(denoted as LIPP+), where reads proceed without taking the lock\nbut only need to verify that the read item (i.e. data or child pointer)\ndid not change; only writers are required to take the lock in\nexclusive mode. Furthermore, we uncovered two bugs in LIPP‚Äôs\noriginal implementation, fixing which led to slightly lower\nperformance compared to what was originally reported [61].3\nThe original implementation of ALEX did not support concur-\nrency. We implemented a concurrent version (ALEX+) by adapting\nthe concurrency protocol of ALEX‚Äôs persistent memory variant,\nAPEX [ 35]. APEX employs optimistic locks at leaf node level for\nevery 256 records. By employing out-of-place-based SMOs (i.e.,\nalways allocating new nodes), APEX traverses tree without hold-\ning locks. Inner nodes are synchronized using per-node shared-\nexclusive locks. ALEX+ adopts all the concurrency design from\nAPEX except it uses a single optimistic lock per data node, which\nexhibits better performance in DRAM environments. We excluded\nFITing-tree [20] in our experiments since it is not open-source.\nFor traditional indexes, we include STX B+-tree [ 3], ART [ 27] and\nHOT [ 4] for single-threaded experiments. For multi-threaded ex-\nperiments, we include B+-TreeOLC [ 58], ART-OLC (ART with opti-\nmistic lock coupling) [ 28], HOT-ROWEX (HOT with ROWEX [ 28]),\nMasstree [ 39] and Wormhole [ 62]. Due to space limitation, we omit\ntheir details and only cover the necessary changes made by us.\nWe added side-links in leaf nodes for B+-TreeOLC for better range\nscan performance. Two ART implementations support concurrency;\nwe use the best performing one based on OLC. We ported HOT‚Äôs\nepoch-based memory reclamation (EBMR) to ART for better scala-\nbility. Although HIST-tree [ 11] shows that as a traditional index,\nit can achieve promising speedups by leveraging certain implicit\nassumptions made by learned indexes (e.g., sortedness), its open-\nsource implementation does not support dynamic workloads and\nwe exclude it from our study.\n3.2 Datasets\nTable 2 shows the real datasets used in our benchmarks. The first\nfour datasets are from SOSD [ 23].osmfrom SOSD is known to\nbe a one-dimensional projection of multi-dimensional spatial data.\nWe include it for stress testing under the worst case scenarios.\nExcept wiki , each dataset consists of 200M unique 8-byte unsigned\n3Our fixes have been accepted by the original authors (details at https://github.com/\nJiacheng-WU/lipp/pull/11 andhttps://github.com/Jiacheng-WU/lipp/pull/12 ).Table 2: Datasets used in experiments.\nDataset Description Source\nbooks Amazon book sales popularity [23]\nfb Upsampled Facebook user ID [23]\nosm Uniformly sampled OpenStreetMap locations [23]\nwiki Wikipedia article edit timestamps [23]\ncovid Uniformly sampled Tweet ID with tag COVID-19 [34]\ngenome Loci pairs in human chromosomes [49]\nstack Vote ID from Stackoverflow [53]\nwise Partition key from the WISE data [60]\nlibio Repository ID from libraries.io [33]\nhistory History node ID in OpenStreetMap [8]\nplanet Planet ID in OpenStreetMap [8]\n0 2 4 6 8\nKey domain 1e90%25%50%75%100%planet\n0 1 2 3 4\nKey domain 1e110%25%50%75%100%genome\nFigure 1: CDFs of planet and genome\ninteger keys, and we pair each key with an 8-byte payload. wiki\nhas duplicated keys.\nSome of these datasets have been used by prior studies but it re-\nmains hard to quantify their potential impact on index performance.\nFor example, some studies have shown that learned indexes cannot\noutperform traditional indexes on the osmdataset [ 40]. Other re-\nsults have shown that learned indexes can outperform traditional\nindexes under wiki and books in read-only workloads [ 38,40].\nThis leads to questions such as how ‚Äúhard‚Äù is osmactually? andis it\nmuch harder or just a bit harder than books ?We thus observe the\nneed to quantify the ‚Äúhardness‚Äù of a dataset. As we detail next, we\nfind that piecewise linear approximation (PLA) [ 18] is a good start.\nAll 1D learned indexes regard indexing as mappings from keys to\npositions. If the data distribution is more non-linear, learned indexes\nwill use more linear models to approximate the distribution. Hence,\nwe could use the minimum number of linear models required to fit\nthe distribution to approximate the data hardness:\nDEFINITION ( ùúñ-approximate [ 18]).Given an array ùê∑=[ùëò1,ùëò2,\n...,ùëòùëõ], whereùëòùëñis a key with rank ùëüùëñin the array, if a model ùêπfor\narrayùê∑isùúñ-approximate, then |ùêπ(ùëòùëñ)‚àíùëüùëñ|‚â§ùúñ,‚àÄùëñ‚àà[1,ùëõ].\nA given array ùê∑may not be perfectly fitted by a single\nùúñ-approximate linear model. But one could split ùê∑into multiple\nsmall segments ùê∑1,ùê∑2,...,ùê∑ùëösuch that for each ùê∑ùëñthere exists\nanùúñ-approximate linear model ùêπùëñ,ùëñ‚àà [ 1,ùëö]. The minimal\ncollection of those models is then the PLA of ùê∑. Computing the\noptimal PLA model of ùê∑can be done in linear time using the\nalgorithm in [ 18]. For a given array ùê∑and an error bound ùúñ, we\ndefine the data hardness ùêªas the number of segments in ùê∑‚Äôs\noptimal PLA model, which includes the least number of segments.\nFor the same dataset, ùêªshould increase as ùúñdecreases. In the rest\nof this paper, we use ùúñ=4096 andùúñ=32to quantify the hardness\nof a dataset in two dimensions.\n\nAre Updatable Learned Indexes Ready?\nWhenùúñis large (4096), the PLA is more coarse-grained and can\ncapture the dataset‚Äôs global non-linearity, which mainly challenges\nthe structural aspect of a learned index. Specifically, since learned\nindexes conceptually break down a CDF as a tree of models, its\nstructure (e.g., fanout and height) is strongly influenced by the\nglobal non-linearity of the CDF. For example, our experimental\nresults (Section 3) indicate that many learned indexes cannot out-\nperform the traditional ones on the planet dataset. By looking at\nits CDF (Figure 1a), we see that its hardness comes from its sharp\ndeflection of distribution at key value around 1M, giving it high\nglobal non-linearity. On datasets like this, learned indexes that\ninsist on a balanced tree structure (e.g., PGM-Index) would give\nthe dense key region (keys <1M) and the sparse key region (keys\n>1M) the same height, requiring more layers to be traversed in the\nsparse key region than needed. ALEX and LIPP combat the issue by\nadopting unbalanced trees (hence the dense region, if hard-to-fit,\ncan be approximated with more models). Nonetheless, a CDF like\nthe one in Figure 1a is still challenging since it can increase their\npath length variance (affecting latency) and stress their decision\ncomponents (e.g., the cost model in ALEX).\nGlobal non-linearity alone is insufficient to fully characterize\nthe hardness of a dataset to learned indexes. For example, we found\nthat the genome dataset also gives a hard time to many learned\nindexes. However, if looking at its CDF (Figure 1b), its CDF looks\nsmooth. But if we zoom into its CDF, it is found that genome has\na very bumpy distribution locally. Non-linearities in local regions\nchallenge the individual machine learning models of a learned index.\nHence, we use PLA with a small ùúñ(32) to capture local non-linearity.\nGlobal non-linearity (PLA ùúñ-4096) and local non-linearity (PLA\nùúñ-32) together form our data space that quantifies the hardness of\na dataset. The ùúñvalues 4096 and 32 are empirically decided. We\nhave also tried other metrics to approximate a dataset‚Äôs hardness,\ne.g., by measuring the mean square error of fitting only one linear\nregression line. But we found that PLA ùúñ-4096 and PLA ùúñ-32 aligned\nthe best with the actual index performance when there are no other\nfactors (e.g., NUMA, concurrency control) in play.\n3.3 Workloads\nWe devise synthetic workloads that issue requests using the afore-\nmentioned datasets. For each dataset, we first randomly shuffle all\nthe 200 million keys, and then issue lookup/insert requests accord-\ning the specified ratios below.\n‚Ä¢Read-Only (0% Write) : Bulk load all the 200M keys and ran-\ndomly lookup for 800M keys.\n‚Ä¢Read-Intensive (20% Write) : Bulk load 100M random keys,\nthen issue requests where 80% are lookup operations and 20%\nare insert operations that insert all the remaining keys.\n‚Ä¢Balanced (50% Write) : Same as Read-Intensive but 50% are\nlookup operations and 50% are insert operations.\n‚Ä¢Write-Heavy (80% Write) : Same as Balanced but the\nlookup/insert ratio is 20%:80%.\n‚Ä¢Write-Only (100% Write) : Issue 100 million insertion after\nbulk loading 100 million keys.\nFor each workload and index, we repeat the experiment three\ntimes and report the average throughput (operations per second)\nand the average latency after bulk loading.\nLocal hardness HPLA(Œµ=\n32)1e30\n200\n400\n600\n800\n1000\n1200\n1400Global hardness HPLA(Œµ=4096)1e2\n01020304050Write Ratio\n0%20%50%80%100%stackwisecovidhistory\nlibiobooks\nplanetosm\nfbgenome\nEasy\nDifficultLIPP\nALEX\nARTFigure 2: Throughput heatmap (single-threaded) that shows\nthe throughput ratio between the best performing learned\nindex and the best performing traditional index. A positive\nratio (in red) means a traditional index is the winner un-\nder that particular workload and dataset; a negative ratio (in\nblue) indicates that a learned index is the winner.\n3.4 Hardware and Platform\nAll the experiments are conducted on a quad-socket machine with\nfour 24-core Intel Xeon Platinum 8268 CPUs clocked at 2.9 GHz.\nThe machine in total has 96 cores and 768GB of main memory. By\ndefault, hyper-threading is disabled. All the code is compiled using\ngcc 8.3.0 under the O3optimization level.\n4 PERFORMANCE\nIn this section, we study on the performance of updatable learned\nindexes in single-core and multi-core settings.\n4.1 Single-Threaded Experiments\nWe start with single-threaded settings to compare state-of-the-\nart updatable learned indexes with the traditional ones. Figure 2\nshows a heatmap of throughput ratios between learned indexes\nand traditional indexes. The color of each point in the heatmap\nindicates the throughput ratio between the bestlearned index and\nthebesttraditional index under that specific data-workload. A blue\npoint indicates a positive ratio, i.e., there exists a learned index that\noutperforms all the state-of-the-art traditional indexes. A red point\n(negative ratio) indicates otherwise, i.e., there exists a traditional\nindex that outperforms all the state-of-the-art learned indexes. The\ndarkness of a point represents the ‚Äúwinning‚Äù ratio with a darker\ncolor indicating the winner outperforms the other indexes by a\nhigher margin. Figure 2 shows that 80% of the data points are in\nblue color, leading to our first message:\nMessage 1. In a single-core environment, updatable learned\nindexes outperform traditional indexes over 80% of our data-\nworkload combinations.\n\nART\nB+treeLIPPALEX0500Insert Latency (ns)covid\nART\nB+treeLIPPALEXlibio\nART\nB+treeLIPPALEXgenome\nART\nB+treeLIPPALEXosmInsert Latency = Pre-insertion key lookup\n+ other steps of insertion\nlookup\nothers\nART\nB+treeLIPPALEX0100200300Time (ns)\nART\nB+treeLIPPALEXART\nB+treeLIPPALEXART\nB+treeLIPPALEXRemaining steps of insertion after key lookup\ninsert\nsmo\nstat\nshift\nchainFigure 3: Time breakdown of insert operations.\nIn fact, we have another four datasets but they are also easy. To\navoid cluttering the easy region, we include only 10 out of the 14\nreal datasets.\nMessage 2. Most real datasets are easy.\nIn Figure 2, ALEX, LIPP and ART are the overall winners. Strictly\nspeaking, PGM-Index could be the winner on 100% write workloads\nas it exhibits the highest insert throughput when there is no lookup.\nNonetheless, its lookup performance is dominated by ALEX and\nLIPP; and its good insert performance is not attributed to its core\ndesign but its LSM-styled approach to handling inserts. We there-\nfore do not show PGM-Index in the heatmap. The main factors\nthat lead ALEX and LIPP perform better than the other learned\nindexes are (1) the use of ML for subspace lookup in their inner\nnodes that eliminate search in inner nodes and (2) the sparse-node\ndesign avoids visiting multiple trees or delta trees as in PGM-Index,\nXIndex, and FINEdex. ART as a traditional index outperforms the\nother traditional indexes because of its cache friendliness [ 27] and\nperforms especially well on integer keys (whereas the more recent\nones like Wormhole and HOT are specialized for long string keys).\nYet, ALEX and LIPP as learned indexes can outperform ART except\non very hard data because they are instance-optimized. Focusing\non the write-intensive workloads leads to the third message:\nMessage 3. In a single-core environment, updatable learned\nindexes only cannot outperform traditional indexes on hard\ndatasets with‚â•50% writes.\nWhile it is known that hard datasets could give a hard time to\nlearned indexes, Figure 2 shows that recent learned indexes like\nALEX and LIPP have already overcome that on lookup operations:\nMessage 4. In a single-core environment, updatable learned\nindexes outperform traditional indexes from read-only to\nread-intensive workloads, regardless of the data hardness.\nHence, with the fact that lookup is the first step of insert (an insert\nof keyùëòwould first lookup ùëòto locate the slot to be inserted), what\nelse inside a learned index‚Äôs insert operation can out-bleed the\nspeed gain from its first step?Table 3: Statistics of an insert operation in ALEX and LIPP.\nDataset ALEX LIPP\nNode traversed Keys shifted Node traversed Node created\ncovid 1.02 8.07 1.23 0.4\nlibio 1.04 19.92 1.09 0.4\ngenome 1.01 42.62 2.12 0.32\nosm 1.62 35.84 2.33 0.28\nTo answer this question, we break down the average latency\nof the insertions of ALEX and LIPP in the write-only workload.\nFigure 3 shows the result. We include ART and B+-tree as references.\nWe show the results for two easy datasets ( covid ) and ( libio ), the\nlocally hardest dataset ( genome ), and the globally hardest dataset\n(osm). Figure 3(top) confirms that learned indexes generally get a\nmore efficient first-step (lookup) in an insertion (except osm), but\nthe remaining steps of an insert perform much worse than the best\ntraditional index (ART) and are no better than B+-trees.\nFigure 3(bottom) details the remaining insert steps, where a large\npart of the latency in learned indexes is due to collision resolution.\nFor ALEX, that is the time spent on shifting the elements in data\nnodes and carrying out SMOs (e.g., node resizing); for LIPP, that is\nthe time spent on creating and chaining new nodes and its SMO-\nlike adjustment procedure to bound the tree height. Although the\nshifting and SMO costs in ALEX are not specific to learned indexes\n(also seen in B-tree variants), they are more expensive than in B-\ntrees and worsen in harder datasets. In contrast, although LIPP also\nexhibits higher collision resolution costs on harder datasets, it is\nstill smaller than that of ALEX.\nTable 3 further shows the detailed statistics per insert in ALEX\nand LIPP. As the table lists, a harder dataset does not particularly\nincrease the number of chaining operations (node creation) in LIPP\nbut only slightly increase tree traversal time to reach the desig-\nnated node. That is because LIPP creates at most one new node on\ncollision, which successfully bounds the write amplification to be\none node allocation per collision. In contrast, ALEX has a number\nof key shifts and the number of shifts increases with the data hard-\nness, because a harder dataset challenges ALEX in multiple aspects\nincluding its cost models, fill factor, and model accuracy. ALEX‚Äôs\nwrite amplification (the number of key shifts due to a collision)\nis large because it is only bound by its huge node size (maximum\n16MB). Figure 3 also reveals a unique component in learned indexes\ninsertion: the update of the various statistics on insertions. The cost\nis non-negligible and is particularly pronounced in LIPP because it\nupdates the statistics in every node on the insertion path.\nMessage 5. In a single-core environment, LIPP‚Äôs node chain-\ning collision resolution has a lower write amplification than\nALEX‚Äôs key shifting collision resolution.\nWith the understanding of where the time goes in insertions, it\nseems that LIPP as a learned index is ‚Äúready‚Äù by having competi-\ntive insert performance and excellent lookup on a majority of real\ndatasets. However, we observe its design is mainly optimized for\nsingle-threaded execution and is often at odds with other important\naspects, including multi-core scalability as we discuss next.\n\nAre Updatable Learned Indexes Ready?\nLocal hardness HPLA(Œµ=\n32)1e30\n200\n400\n600\n800\n1000\n1200\n1400Global hardness HPLA(Œµ=4096)1e2\n01020304050Write Ratio\n0%20%50%80%100%stackwisecovidhistory\nlibiobooks\nplanetosm\nfbgenome\nEasy\nDifficultLIPP+\nALEX+\nART-OLC\nFigure 4: Throughput heatmap under 24 threads (one\nsocket).\n4.2 Multi-Threaded Experiments\nWe now move on to the multi-threading environment and begin\nwith the heatmap that shows the best indexes in our data-workload\nspace under 24 threads without hyperthreading in Figure 4. LIPP+,\nALEX+ and ART-OLC are the only winners. However, two notable\nchanges are that LIPP+ has lost its leading position to ALEX+ except\non read-only workloads; and ART-OLC also has taken over some\neasy datasets on write-intensive workloads. The latter is because\nART has been very competitive ‚Äî sometimes its performance is\nclose to LIPP and is better than ALEX in the single-threaded setting.\nWhen LIPP+ loses its edge in the multi-threading setting, ART-OLC\ntakes over as the best performing index, followed by ALEX+.\nFigure 5 (white area) shows the scalability from 2 to 24 cores of\nthe read-only, balanced, and write-only workloads. All the learned\nindexes scale well on read-only workloads. However, once the\nworkloads include writes, LIPP+ can no longer sustain its scalability\ndespite its use of optimistic concurrency because every insert thread\nhas to update the per-node statistic on its insertion path. That\ninduces high contention and cacheline ping-pong, especially at the\nroot node. This is a drawback of using a unified node layout where\nstatistics have to be maintain in every node rather than in only the\nleaf nodes like ALEX+. In contrast, ALEX+ scales well by taking the\nadvantage of lock-free lookup and optimistic locking in leaf nodes,\nuntil its write amplification becomes severe under hard data.\nMessage 6. After parallelization, some single-threaded up-\ndatable learned indexes (e.g., ALEX) can scale and perform\nbetter than native concurrent learned indexes (e.g., XIndex),\nbut some (e.g., LIPP) cannot and perform worse.\n4.3 Impact of Hyper-threading/NUMA\nThe grey area of Figure 5 shows the scalability with 36 and 48\nthreads with hyper-threading (48 hardware threads per socket).\nWhen writes are involved, LIPP+ cannot scale and Wormhole‚Äôssingle lock for the inner layer severely limits concurrency. Except\nthem, all indexes benefit from hyper-threading but show different\ndegree of performance saturation. For example, ALEX+ exhibit\nslow down from memory bandwidth exhaustion due to its high\nwrite amplification and long last-mile search, especially on harder\ndata. In fact, our profiling results indicate that ALEX+ has already\nsaturated the memory bandwidth with 24 threads in one socket.\nMoving on to NUMA, Figure 6 shows the scalability of each\nindex when scaling from 1 to 4 sockets. We use the Interleave\nNUMA memory allocation policy that allocates memory pages from\ndifferent sockets in a round-robin fashion. This is also the setting\nwhich yields the best performance for all indexes.\nAll indexes show diminishing return once we use more than one\nsocket due to cross-socket bandwidth is lower than the intra-socket\n(24 cores) bandwidth. ART-OLC has difficulty scaling on easier\ndatasets ( libio andcovid ). That is because those datasets have\ndense keys that result in dense nodes and hence, greatly increase\ncontention on high thread counts. Masstree crumbles when there\nare writes because its write amplifications and concurrency control\ntogether have exhausted the cross-socket bandwidth [ 44]. ALEX+,\nperforms worse either with insertion or harder data on two sockets\nbecause distributing the memory accesses over two sockets would\nexperience the tighter cross-socket bandwidth bottleneck despite\nthe increased bandwidth in aggregation. ALEX+ however can scale\nagain with more sockets as by then there are more bandwidth\nchannels. Overall, this experiment shows that:\nMessage 7. Hyper-threading and NUMA have influence on\nthe scalability of learned indexes to various degree, and\ntheir core designs play a role in their scalability under\nhyper-threading/NUMA.\n4.4 Deletion Performance\nBefore we move on to the next section, we complement this section\nusing deletion workloads. LIPP, Masstree, Wormhole, B+TreeOLC,\nand HOT-ROWEX do not cover deletions. We exclude them from\nour study except for LIPP (and LIPP+), which we implemented\ndeletion for it. We also extended ALEX‚Äôs deletion for ALEX+.\nIn this experiment, for each dataset, we bulk load all the 200M\nkeys, and randomly issue lookup/delete requests until 100M keys\nare deleted. Figure 7 shows the single-threaded throughput heatmap\nunder five different deletion workloads, from read-only to delete-\nonly (100% delete). The results of multi-threading are similar and\nare omitted for space. From the results, we see that ALEX, LIPP, and\nART (as well as their concurrent versions) are still the most compet-\nitive ones. Note that when compared with the insertion workloads\n(Figure 2), the learned indexes take over more territory from ART\neven on hard data. Although the deletion path of a learned index is\nanalogous to its insertion path, where deleting a key may also incur\nwrite amplification (filling up the gaps) and trigger SMOs (node\nresizing), one crucial difference is that deleting a key from a data\nnode would not ‚Äúpollute‚Äù a node‚Äôs ML model. Consequently, dele-\ntion and lookup could continue to enjoy high quality model-based\nsearch without model pollution or retraining overhead. This makes\n\n050100150RO Throughput (Mop/s)\ncovid\n050100150200\nlibio\n020406080\ngenome\n020406080\nosm\n020406080BAL Throughput (Mop/s)\n0255075100\n0204060\n020406080\n248 16 24 36 48\n# of cores0204060WO Throughput (Mop/s)\n248 16 24 36 48\n# of cores020406080100\n248 16 24 36 48\n# of cores0204060\n248 16 24 36 48\n# of cores020406080\nALEX+\nLIPP+\nXIndex\nFINEdex\nART-OLC\nB+treeOLC\nHOT-ROWEX\nMasstree\nWormholeFigure 5: Throughput of the read-only (top), balanced (middle) and write-only (bottom) workloads using one socket. The grey\narea indicates hyper-threading is enabled.\n0100200RO Throughput (Mop/s)\ncovid\n0100200300400\nlibio\n050100150\ngenome\n050100150\nosm\n0255075100BAL Throughput (Mop/s)\n050100150\n0255075100\n050100\n2 cores 1 2 3 4\n# of sockets0204060WO Throughput (Mop/s)\n2 cores 1 2 3 4\n# of sockets050100150\n2 cores 1 2 3 4\n# of sockets0255075100\n2 cores 1 2 3 4\n# of sockets050100\nALEX+\nLIPP+\nXIndex\nFINEdex\nART-OLC\nB+treeOLC\nHOT-ROWEX\nMasstree\nWormhole\nFigure 6: Throughput under varying socket counts.\nlearned indexes perform even better and outperforms traditional\nindexes on more datasets and workloads.\nMessage 8. Deletions in learned indexes are lightweight be-\ncause there is no model pollution.\n5 MEMORY SPACE EFFICIENCY\nMost previous work on learned indexes excluded the size of the\nleaf layer when evaluating memory space efficiency. We aim to\nstudy the end-to-end space consumption (i.e., the size of the whole\nindex including both non-leaf and leaf layers) of the indexes. Wereport the size of the indexes after running the write-only workload\nwhere the first 100M keys are bulk loaded and the rest of them are\nindividually inserted into the index.\nFigure 8 shows the end-to-end space consumption of the indexes.\nFor clarity, we present only indexes that are the best either in terms\nof throughput or space. From the figure, we observe that:\nMessage 9. When considering the index size end-to-end, up-\ndatable learned indexes only have at most 3.2 √óspace saving\nover state-of-the-art traditional indexes, and all of them use\neven more space than HOT.\n\nAre Updatable Learned Indexes Ready?\nLocal hardness HPLA(Œµ=\n32)1e30\n200\n400\n600\n800\n1000\n1200\n1400Global hardness HPLA(Œµ=4096)1e2\n01020304050Delete Ratio\n0%20%50%80%100%stackwisecovidhistory\nlibiobooks\nplanetosm\nfbgenome\nEasy\nDifficultLIPP\nALEX\nART\nFigure 7: Throughput heatmap (single-threaded) under dele-\ntion workloads.\n01020Size (GB)covid libio genome osm\nALEX LIPP PGM-Index ART B+tree HOT\nFigure 8: Memory space efficiency of the best indexes.\nThe factor 3.2√ócomes from measuring the difference between\nthe sizes of the most space-efficient learned index (PGM-Index) and\nthe least space-efficient traditional index (ART). Although learned\nindexes do have small non-leaf layers, memory-optimized tradi-\ntional index like HOT can also be very space-efficient.\nAlthough PGM-Index and HOT are space-efficient, they did not\nshow outstanding performance. Concerning the three winners in\nthe performance heatmaps, ALEX is the most space-efficient be-\ncause its inner layers store no keys. Yet it is slightly larger than\nPGM-Index because it leaves gaps for insertions. ART as a trie,\nhas low space utilization in its nodes (many null child pointers)\nwhen the key space is not dense enough [ 4]. Despite its excellent\nsingle-threaded performance, LIPP‚Äôs memory consumption is 4‚Äì5 √ó\nlarger than ALEX‚Äôs, and is the highest in this experiment. Having\nthis observation, one might wonder (1) what would the performance\nof LIPP and ALEX be if they were given the same memory space? and\n(2)is LIPP‚Äôs single-thread advantage a fundamental achievement or\nmore a space-performance tradeoff?\nTo answer these questions, we carried out another experiment\nwhich tuned the fill factor of ALEX data nodes such that the re-\nsulting index (ALEX-M) uses roughly the same amount of memory\n01020Size (GB)covid\n01020Size (GB)libio\n01020Size (GB)genome\n01020Size (GB)osm\n050100\nWrite ratio (%)0510Throughput (Mop/s)050100\nWrite ratio (%)0510Throughput (Mop/s)050100\nWrite ratio (%)0510Throughput (Mop/s)050100\nWrite ratio (%)0510Throughput (Mop/s)\nALEX-M LIPPFigure 9: ALEX-M vs. LIPP (when ALEX is tuned to use\nroughly the same amount of memory as LIPP).\nspace as LIPP.4The resulting fill factor of ALEX-M is 0.2‚Äì0.25,\nwhereas ALEX has an original fill factor of 0.7. Figure 9 shows the\nthroughput of ALEX-M and LIPP under this new setting. ALEX‚Äôs\nlookup performance has improved significantly and dominated\nLIPP on both easy and hard datasets. The reason is that with lower\ndensity data nodes, an insert in ALEX can often find a gap, maintain-\ning high model accuracy and incurring fewer key shifting. These\nresults indicate that LIPP‚Äôs collision-driven design is trading space\nfor speed.\n6 ROBUSTNESS\nA robust index shall have low tail latency despite any concurrency\ndegree or heavy-lifting part (e.g., SMO). A reliable index shall also\nhave a robust performance as the underlying data distribution\nchanges. Furthermore, it is important for the indexes to perform\nrobustly under a variety of range scan sizes.\n6.1 Tail Latency\nIn this experiment, we report the tail latencies (variance and 99.9\npercentile) of the indexes under both single-threaded and multi-\nthreaded (24 cores) settings. The lookup and insert latencies are sam-\npled from 1% of the operations from the read-only and write-only\nworkloads, respectively. As Figure 10a shows, the updatable lear-\nned indexes (except XIndex) exhibit comparable tail latency with\nthe traditional indexes under single-threaded lookup operations.\nXIndex‚Äôs latency variance is especially high due to the expensive\ncontext switching between its foreground and background threads.\nSpecifically, unlike other indexes, XIndex requires an extra back-\nground thread to merge deltas. For fair comparison, we pinned its\noperational and background threads to the same core so that all the\nindexes are evaluated using the same CPU budget. This experiment\nreveals that using background threads to handle dynamic work-\nloads would hurt latency variance. We have confirmed this reason\nby pinning the background thread to an extra physical core, after\nwhich its tail latency is back to normal. The results using multiple\nthreads (Figure 10b) are similar. It is also worth-noting that LIPP+‚Äôs\ntail latencies remain low even under multi-threading, although it\ndid not scale well on throughput. The reason is that LIPP+ uses\n4We cannot do the other way round because of LIPP‚Äôs implementation sets its fill\nfactor as an integer; hence we could not set any fill factor smaller than 2, and a fill\nfactor of 1 means no gap is allowed.\n\n02000400099.9% (ns)covid libio genome osm\n0500010000STD (ns)\n14.1x\n9.6x\n10.9x\n12.6x(a) Single core\nALEX\nLIPP\nPGM-IndexXIndex\nFINEdex\nARTHOT\nB+treeWormhole\nMasstree\n02000400099.9% (ns)covid libio genome osm\n02000STD (ns)\n16.1x\n11.3x\n13.7x\n14.9x(b) 24 cores\nALEX+\nLIPP+\nXIndexFINEdex\nART-OLCHOT-ROWEX\nB+treeOLCWormhole\nMasstreeFigure 10: Tail latency of lookup operations.\n0500099.9% (ns)covid libio\n2.3xgenome\n2.9x3.1xosm\n0500010000STD (ns)\n15.8x\n14.3x\n13.5x\n15.9x(a) Single core\nALEX\nLIPP\nPGM-IndexXIndex\nFINEdex\nARTHOT\nB+treeWormhole\nMasstree\n0200004000099.9% (ns)\n2.0x\n2.9xcovid\n3.0xlibio\n1.4x\n2.8xgenome\n2.4x\n2.9xosm\n02500050000STD (ns)\n14.0x\n14.2x(b) 24 cores\nALEX+\nLIPP+\nXIndexFINEdex\nART-OLCHOT-ROWEX\nB+treeOLCWormhole\nMasstree\nFigure 11: Tail latency of insert operations.\n‚àí50‚àí2502550Throughput % changecovid->osm osm->covid covid->genome genome->covid\nALEX\nLIPPPGM-Index\nXIndexFINEdex\nARTHOT\nB+treeWormhole\nMasstreeALEX\nLIPPPGM-Index\nXIndexFINEdex\nARTHOT\nB+treeWormhole\nMasstreeALEX\nLIPPPGM-Index\nXIndexFINEdex\nARTHOT\nB+treeWormhole\nMasstreeALEX\nLIPPPGM-Index\nXIndexFINEdex\nARTHOT\nB+treeWormhole\nMasstree\nFigure 12: Throughput changes as data distributions change.\natomic instructions to update statistics, affecting average latency\nrather than tail latency.\nFigure 11 shows the tail latency for inserts. With a single thread\n(Figure 11a), updatable learned indexes except XIndex generally\nhave similar tail latencies as the traditional ones. ALEX and LIPP\nare sensitive to data hardness. They have high 99.9% tail insert\nlatency on the hard osmandgenome datasets due to the increased\nnumber of SMOs. Similar to its lookup operations, XIndex‚Äôs insert\noperations exhibit very high variance in tail latencies regardless\nof the data hardness because of the context switching overheadbetween its 24 foreground threads and 3 background threads using\n24 physical cores.5Overall, the multi-threaded results are similar\nto the single-threaded ones, except that Wormhole has a higher tail\nlatency for inserts due to its use of a single exclusive lock for the\nwhole inner layer.\nMessage 10. Except XIndex, updatable learned indexes ex-\nhibit low tail latency on both single- and multi-threaded set-\ntings. Yet, some traditional indexes (ART, HOT and B-tree)\nexhibit impeccable robustness in this aspect.\n6.2 Shifting Data Distributions\nThe goal of our next experiment is to study how learned indexes\nbehave and adapt to changes of data distribution after the index\nhas been deployed. We follow earlier work [ 15] to (1) bulk load\nan index using 100 million keys in one dataset ùëã, and (2) start a\nread-write balanced workload using 100 million keys of another\ndatasetùëåfor insertions and lookups for the keys in ùëã. The keys\nof both datasets are scaled to the same domain. With the ability\n5The foreground and background thread ratio follows the recommendation from\nXIndex‚Äôs original paper [54].\n\nAre Updatable Learned Indexes Ready?\nto quantify data hardness, we shift from easy data ( covid ) to two\ndifferent kinds of hard data ( genome andosm) and vice versa.\nFigure 12 shows the change of the throughput on the balanced\nworkload with respect to the original workload with no change\nof the dataset. The result shows that learned indexes are sensitive\nto data distribution changes while traditional indexes are not. The\nchanges, however, can be both positive and negative. For example,\nALEX‚Äôs throughput can drop by up to 52% when it is bulk-loaded\nwith easy data covid , followed by inserts of hard data ( osm). How-\never, its performance can improve by up to 15% when the hard data\nosmis first bulk-loaded, followed by the easier covid . The result\naligns with the observations made by prior work [ 15], where the\nindex starts with easy data can incur significant overhead to adapt\nto the new, harder distribution. Yet, an index started with harder\ndata requires less/no overhead to adapt to easy data. LIPP behaves\nsimilarly to ALEX. PGM-Index and XIndex are more resilient to\ndistribution changes. In PGM-Index, the different distributions are\nlikely to be stored in different trees in its LSM structure. XIndex‚Äôs\nthroughput is less sensitive to the increased SMOs as they are\nhandled by background threads, which impact tail latency.\nMessage 11. Learned indexes are sensitive to data distribu-\ntion changes, while traditional indexes are not. Corroborat-\ning with prior work, it is harder (easier) for a learned index\nthat is pre-filled with easy (hard) data to adapt to a harder\n(easier) dataset.\n6.3 Range Queries\nThis experiment evaluates range query performance. We bulk load\neach index using the whole dataset of 200M keys and start a read-\nonly scan workload. Each query picks a random start key ùêæand\nfetches a fixed number of keys starting from ùêæ. Each workload\nissues 10 million range queries in total and we measure the through-\nput in number of keys accessed per second. Figure 13 shows the\nresults (for indexes that implemented range scan only) under a\nvarying range query size from 10 to 10,000 under a single thread.\nAs shown by the figure, all indexes exhibit higher throughput as\nthe query size increases because a larger scan size involves less tree\ntraversal and more efficient in-node scan. However, this experiment\nalso reveals yet another drawback of LIPP‚Äôs unified node design:\nwith a node layout that interleaves child pointers and data in the\nnode array, a range scan on the array would inevitably encounter\na lot of branches. Specifically, to continue to the next entry in the\ndata array, LIPP needs a branching instruction to decide whether\nthe entry is a child entry (hence recursively visiting the subtree) or\na data entry. This largely cancels out the potential improvement\nbrought by using fewer traversals under larger scan sizes.\nMessage 12. Learned indexes are good on range queries. Yet,\nthe unified node design is not range-scan friendly.\n7 COMPLEMENTING REAL DATA WITH\nSYNTHETIC DATA\nThe global and local hardness defined based on PLA are mildly\ncorrelated because they are extracted from the same CDF after\nall, just in different granularity. This also explains why we seldom\nfind real datasets that are positioned near the ‚Äúhard‚Äù corners (e.g.,\nglobally-hard-locally-easy) in our heatmaps. Although we find no\n12345\nRange 10x0100200300Throughput (M keys/s)\ncovid\n12345\nRange 10x0100200300\nlibio\n12345\nRange 10x0100200300\nosm\n12345\nRange 10x0100200300\ngenome\nALEX\nLIPPFINEdex\nPGM-IndexB+tree\nHOTFigure 13: Range query throughput under varying scan\nsizes.\nLocal hardness HPLA(Œµ=\n32)1e30\n200\n400\n600\n800\n1000\n1200\n1400Global hardness HPLA(Œµ=4096)1e2\n0102030405060Write Ratio\n0%20%50%80%100%syn_stacksyn_ghard_leasy\nsyn_planetsyn_osm\nsyn_fb syn_geasy_lhardsyn_ghard_lhard\nsyn_genome\nEasy\nDifficultALEX\nLIPP\nART\nFigure 14: Throughput heatmap (single-thread) from syn-\nthetic datasets.\nsyn_ghard_leasy\n(globally hard\nand locally easy)\nsyn_geasy_lhard\n(globally easy\nand locally hard)\nsyn_ghard_lhard\n(globally hard\nand locally hard)\nFigure 15: Synthetic datasets from heatmap corners.\nreal dataset that exhibits those extreme hardness, we have built a\nsynthetic data generator that generates data using local and global\nhardness as inputs. Figure 14 is the corresponding single-thread\nthroughput heatmap based on the generated data. We selected lo-\ncal/global hardness values ùêªfrom the ‚Äúhard‚Äù corners of the heatmap.\nFigure 15 illustrates their CDFs.\nOur synthetic data generator samples data from a set of random\nlinear models. Specifically, we first randomly generate a positive\nslopeùëöand an intercept ùëèfor a segment‚Äôs linear model. Given that a\nlinear model maps keys to positions, we have ùë¶=ùëöùëò+ùëè, whereùëòis\na key andùë¶is its rank. For a given rank and ùúñ, a key is then uniformly\nsampled from[ùëöùëéùë•(ùë¶‚àíùúñ‚àíùëè\nùëö,ùëùùëüùëíùë£+1),ùë¶+ùúñ‚àíùëè\nùëö], whereùëùùëüùëíùë£ is the\nkey from rank ùë¶‚àí1. Keys are incrementally generated from rank\n\n1 to rank 200M. A segment size is controllable but currently we\nsimply give each segment an equal number of keys. Following the\nalgorithm we described in [ 59] for computing data hardness, we\ncreate a convex hull for all keys of a generated segment. Then, we\ngenerate the first key ùëòùëñof the next segment by incrementing the\nvalue ofùëòùëñuntil(ùëòùëñ,ùëüùëñ)goes beyond the bounding box of the convex\nhull of the previous segment. The process is iterative and recursive.\nWe first generate a global segment and then its local segments, and\nrepeat the process until all segments are generated.\nAlthough primitive, we can see that the resulting heatmap from\nthe synthetic data is similar to the one using real data. From the\nheatmap of synthetic data, learned indexes can also do well when\nonly one dimension is hard while the other one remains easy. In\nother words, learned indexes lose their edge only when both dimen-\nsions are hard and with intensive-writes, corroborating Message 3.\n8 LESSONS LEARNED\nIn this section, we summarize six lessons learned from the study:\n1. Using ML for subspace lookup in internal layers and using\nsparse-node as node design generally well balance perfor-\nmance, space, and robustness. This observation inspires future\nwork to continue in this direction to refine cost models and address\nlimitations (e.g., write amplification).\n2. Use hard datasets judiciously. During the experimentation,\nwe found that most real datasets are easy. In contrast, fbhas been\nupsampled and osmis not one-dimensional in nature [ 8]. Therefore,\nwhile we can use those hard datasets to stress test the corner cases,\nindex designers may not need to put too much weights on those.\n3. Concurrency control and robustness should be first-class\ncitizens when designing a learned index, i.e., designs should\nbe holistic. Otherwise, different design choices may inherently\ncontradict each other later. For example, LIPP‚Äôs unified node lay-\nout which mainly considered single-threaded scenarios can signifi-\ncantly hurt scalability and add many branches to range scans.\n4. Future learned indexes can benefit from cache-friendly\nand NUMA-aware designs. Since we found that good learned\nindexes like ALEX+ can saturate memory bandwidth, reducing\ncache misses and NUMA-aware data placement are two promising\noptimization directions.\n5. Memory efficiency is nota clear advantage of updatable\nlearned indexes. Simply replacing traditional indexes with lear-\nned indexes may not help much in space saving. We recommend\nfuture learned indexes must report the end-to-end memory con-\nsumption and explore alternatives (e.g., compression [ 5], persistent\nmemory [35]) to reduce DRAM pressure.\n6. Traditional indexes are not good-for-nothing. Traditional\nindexes should not be completely abandoned because of their effi-\nciency in write-intensive workload and hard datasets, rich function-\nality (e.g., support for variable-length keys) and extreme robustness.\n9 SO, ARE UPDATABLE LEARNED INDEXES\nREADY?\nIn this section, we try to answer this question from three different\nangles: yesterday ,today , and tomorrow .\nYesterday. If the question were asked before this benchmark\nstudy, the answer would have been no. Specifically, a modern index\nstack wisecovidhistorylibiobooks planetosmfb\ngenome\nDataset0%50%100%Write RatioART-OLCFigure 16: The world without this benchmark study.\nHeatmap on 24 cores with all workloads. The datasets can‚Äôt\nbe ordered.\nmust support concurrency, making XIndex [ 54] and FINEdex [ 31]\nthe only choices. Figure 16 shows what the 24-core performance\nheatmap would have been without our study ‚Äî ART-OLC domi-\nnates the heatmap. Furthermore, without a quantitative metric on\ndata hardness, practitioners would have no way to make informed\ndecisions on when to use a learned index or not. In addition to\nissues with concurrency, we observed that the supposedly fastest\n(single-threaded) learned index LIPP actually trades space for speed.\nToday. Despite being a performance study, we have made con-\ntributions to parallelizing two state-of-the-art updatable learned\nindexes (ALEX+ and LIPP+). We show that LIPP by design is not\nconcurrency-friendly nor space-efficient, yet ALEX+ is promising in\nterms of performance, scalability, space, and robustness. Although\nALEX+ has a harder time on hard datasets when the workloads are\nwrite-intensive, most real datasets are easy and most real-world\nOLTP workloads are actually read-intensive [ 10,26,36,50]. Hence,\nwe conclude that ALEX+ as an updatable learned index that is al-\nmost ready . For write-intensive workloads, we recommend LSM-\nstyle indexing. Some early efforts have already started to explore\nthis direction [12].\nTomorrow. Despite the promising results, ALEX+ has limita-\ntions and we recommend using it in a hardness-conscious manner.\nFor example, the hardness of a dataset can be added as a new\nfeature/dimension in index selection tools [ 6,7,24,37,55] and\nquery optimizers [ 13,41‚Äì43,51]. When those components are ready,\nALEX+ would also be ready.\nACKNOWLEDGMENTS\nThis work is partially supported by Hong Kong General Research\nFund (14200817), Hong Kong AoE/P-404/18, Innovation and Tech-\nnology Fund (ITS/310/18, ITP/047/19LP) and Centre for Perceptual\nand Interactive Intelligence (CPII) Limited under the Innovation\nand Technology Fund.\n\nAre Updatable Learned Indexes Ready?\nREFERENCES\n[1]R. Bayer and M. Schkolnick. 1977. Concurrency of Operations on B-Trees. Acta\nInf.(1977).\n[2]Laurent Bindschaedler, Andreas Kipf, Tim Kraska, Ryan Marcus, and Umar Farooq\nMinhas. 2021. Towards a Benchmark for Learned Systems. In 2021 IEEE 37th\nInternational Conference on Data Engineering Workshops (ICDEW) . 127‚Äì133.\n[3]Timo Bingmann. 2013. STX B+ Tree 0.9. https://panthema.net/2007/stx-btree/ ,\nretrieved Sep. 1, 2021.\n[4]Robert Binna and et al. 2018. HOT: A Height Optimized Trie Index for Main-\nMemory Database Systems. In Proceedings of the 2018 International Conference on\nManagement of Data .\n[5]Antonio Boffa, Paolo Ferragina, and Giorgio Vinciguerra. 2022. A Learned\nApproach to Design Compressed Rank/Select Data Structures. ACM Transactions\non Algorithms (2022). https://doi.org/10.1145/3524060\n[6]Surajit Chaudhuri, Mayur Datar, and Vivek R. Narasayya. 2004. Index Selection\nfor Databases: A Hardness Study and a Principled Heuristic Solution. IEEE Trans.\nKnowl. Data Eng. 16, 11 (2004), 1313‚Äì1323.\n[7]Surajit Chaudhuri and Vivek R. Narasayya. 1997. An Efficient Cost-Driven Index\nSelection Tool for Microsoft SQL Server. In VLDB . 146‚Äì155.\n[8]Google Cloud. 2017. OpenStreetMap. (2017). https://console.cloud.google.com/\nmarketplace/details/openstreetmap/geo-openstreetmap .\n[9]Brian F. Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell\nSears. 2010. Benchmarking Cloud Serving Systems with YCSB. In SoCC . 143‚Äì154.\n[10] James C. Corbett and et al. 2012. Spanner: Google‚Äôs Globally-Distributed Database.\nInOSDI , Chandu Thekkath and Amin Vahdat (Eds.).\n[11] Andrew Crotty. 2021. Hist-Tree: Those Who Ignore It Are Doomed to Learn. In\nCIDR .\n[12] Yifan Dai, Yien Xu, Aishwarya Ganesan, Ramnatthan Alagappan, Brian Kroth,\nAndrea Arpaci-Dusseau, and Remzi Arpaci-Dusseau. 2020. From WiscKey to\nBourbon: A Learned Index for Log-Structured Merge Trees. In 14th USENIX\nSymposium on Operating Systems Design and Implementation (OSDI 20) .\n[13] Dinesh Das, Jiaqi Yan, Mohamed Za√Øt, Satyanarayana R. Valluri, Nirav Vyas, Ra-\nmarajan Krishnamachari, Prashant Gaharwar, Jesse Kamp, and Niloy Mukherjee.\n2015. Query Optimization in Oracle 12c Database In-Memory. Proc. VLDB Endow.\n8, 12 (2015), 1770‚Äì1781.\n[14] Angjela Davitkova, Evica Milchevski, and Sebastian Michel. 2020. The ML-Index:\nA Multidimensional, Learned Index for Point, Range, and Nearest-Neighbor\nQueries. In Proceedings of the 23rd International Conference on Extending Database\nTechnology, EDBT .\n[15] Jialin Ding and et al. 2020. ALEX: An Updatable Adaptive Learned Index. In\nProceedings of the 2020 ACM SIGMOD International Conference on Management of\nData .\n[16] Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020.\nTsunami: A learned multi-dimensional index for correlated data and skewed\nworkloads. PVLDB (2020).\n[17] Paolo Ferragina and Giorgio Vinciguerra. 2020. Learned Data Structures. In\nRecent Trends in Learning From Data . Springer International Publishing, 5‚Äì41.\n[18] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-Index: A Fully-Dynamic\nCompressed Learned Index with Provable Worst-Case Bounds. Proc. VLDB Endow.\n(2020).\n[19] Keir Fraser. 2004. Practical lock-freedom . Ph.D. Dissertation. University of Cam-\nbridge, UK.\n[20] Alex Galakatos and et al. 2019. FITing-Tree: A Data-Aware Index Structure. In\nProceedings of the 2019 International Conference on Management of Data (SIGMOD\n‚Äô19).\n[21] Ali Hadian and Thomas Heinis. 2021. Shift-Table: A Low-latency Learned Index\nfor Range Queries using Model Correction. In EDBT . 253‚Äì264.\n[22] Andreas Kipf and et al. 2020. RadixSpline: A Single-Pass Learned Index. In\nProceedings of the Third International Workshop on Exploiting Artificial Intelligence\nTechniques for Data Management (aiDM ‚Äô20) .\n[23] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2019. SOSD: A Benchmark for Learned\nIndexes. NeurIPS Workshop on Machine Learning for Systems (2019).\n[24] Jan Kossmann, Stefan Halfpap, Marcel Jankrift, and Rainer Schlosser. 2020. Magic\nmirror in my hand, which is the best in the land? An Experimental Evaluation of\nIndex Selection Algorithms. Proc. VLDB Endow. 13, 11 (2020), 2382‚Äì2395.\n[25] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In Proceedings of the 2018 International\nConference on Management of Data (SIGMOD ‚Äô18) .\n[26] Jens Krueger, Changkyu Kim, Martin Grund, Nadathur Satish, David Schwalb,\nJatin Chhugani, Hasso Plattner, Pradeep Dubey, and Alexander Zeier. 2011. Fast\nUpdates on Read-Optimized Databases Using Multi-Core CPUs. Proc. VLDB\nEndow. 5, 1 (sep 2011), 61‚Äì72.\n[27] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. The Adaptive Radix\nTree: ARTful Indexing for Main-Memory Databases. In Proceedings of the 2013\nIEEE International Conference on Data Engineering .\n[28] Viktor Leis, Florian Scheibner, Alfons Kemper, and Thomas Neumann. 2016.\nThe ART of Practical Synchronization. In Proceedings of the 12th InternationalWorkshop on Data Management on New Hardware (DaMoN ‚Äô16) .\n[29] Lucas Lersch and et al. 2019. Evaluating Persistent Memory Range Indexes. Proc.\nVLDB Endow. (2019).\n[30] Lucas Lersch, Ivan Schreter, Ismail Oukid, and Wolfgang Lehner. 2020. Enabling\nLow Tail Latency on Multicore Key-Value Stores. Proc. VLDB Endow. (2020).\n[31] Pengfei Li, Yu Hua, Jingnan Jia, and Pengfei Zuo. 2021. FINEdex: A Fine-grained\nLearned Index Scheme for Scalable and Concurrent Memory Systems. Proc. VLDB\nEndow. 15, 2 (2021), 321‚Äì334.\n[32] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. 2020. LISA: A\nlearned index structure for spatial data. In Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data . 2119‚Äì2133.\n[33] Libraries.io. 2017. Repository ID. (2017). https://libraries.io/data .\n[34] Christian E Lopez and Caleb Gallemore. 2021. An augmented multilingual Twitter\ndataset for studying the COVID-19 infodemic. Social Network Analysis and Mining\n11, 1 (2021), 1‚Äì14.\n[35] Baotong Lu, Jialin Ding, Eric Lo, Umar Farooq Minhas, and Tianzheng Wang.\n2021. APEX: A High-Performance Learned Index on Persistent Memory. Proc.\nVLDB Endow. 15, 3 (2021), 597‚Äì610.\n[36] Haonan Lu, Siddhartha Sen, and Wyatt Lloyd. 2020. Performance-Optimal Read-\nOnly Transactions. In 14th USENIX Symposium on Operating Systems Design and\nImplementation (OSDI 20) . 333‚Äì349.\n[37] Vincent Y. Lum and Huei Ling. 1971. An optimization problem on the selection of\nsecondary keys. In Proceedings of the 26th ACM annual conference . ACM, 349‚Äì356.\n[38] Marcel Maltry and Jens Dittrich. 2022. A Critical Analysis of Recursive Model\nIndexes. Proc. VLDB Endow. (2022).\n[39] Yandong Mao, Eddie Kohler, and Robert Tappan Morris. 2012. Cache craftiness\nfor fast multicore key-value storage. In EuroSys . ACM, 183‚Äì196.\n[40] Ryan Marcus and et al. 2020. Benchmarking Learned Indexes. Proc. VLDB Endow.\n(2020).\n[41] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Nesime Tatbul, Mohammad Al-\nizadeh, and Tim Kraska. 2021. Bao: Making Learned Query Optimization Practical.\nInSIGMOD . 1275‚Äì1288.\n[42] Ryan C. Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh,\nTim Kraska, Olga Papaemmanouil, and Nesime Tatbul. 2019. Neo: A Learned\nQuery Optimizer. Proc. VLDB Endow. 12, 11 (2019), 1705‚Äì1718.\n[43] Volker Markl, Guy M. Lohman, and Vijayshankar Raman. 2003. LEO: An auto-\nnomic query optimizer for DB2. IBM Syst. J. (2003).\n[44] Ajit Mathew and Changwoo Min. 2020. HydraList: A Scalable in-Memory Index\nUsing Asynchronous Updates and Partial Replication. Proc. VLDB Endow. (2020).\n[45] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning Multi-Dimensional Indexes. In SIGMOD . 985‚Äì1000.\n[46] Patrick O‚ÄôNeil, Edward Cheng, Dieter Gawlick, and Elizabeth O‚ÄôNeil. 1996. The\nLog-Structured Merge-Tree (LSM-Tree). Acta Inf. (1996).\n[47] Joseph O‚ÄôRourke. 1981. An On-Line Algorithm for Fitting Straight Lines between\nData Ranges. Commun. ACM (1981).\n[48] Jianzhong Qi, Guanli Liu, Christian S. Jensen, and Lars Kulik. 2020. Effectively\nLearning Spatial Indices. PVLDB 13, 12 (2020), 2341‚Äì2354.\n[49] Suhas S.P. Rao and et al. 2014. A 3D Map of the Human Genome at Kilobase\nResolution Reveals Principles of Chromatin Looping. Cell(2014).\n[50] Robin Rehrmann, Carsten Binnig, Alexander B√∂hm, Kihong Kim, Wolfgang\nLehner, and Amr Rizk. 2018. OLTPshare: The case for sharing in OLTP workloads.\nProceedings of the VLDB Endowment 11, 12 (2018), 1769‚Äì1780.\n[51] Srinath Shankar and et al. 2012. Query optimization in microsoft SQL server PDW.\nInProceedings of the ACM SIGMOD International Conference on Management of\nData, SIGMOD . ACM, 767‚Äì776.\n[52] Benjamin Spector, Andreas Kipf, Kapil Vaidya, Chi Wang, Umar Farooq Minhas,\nand Tim Kraska. 2021. Bounding the Last Mile: Efficient Learned String Indexing\n(Extended Abstracts). In 3rd International Workshop on Applied AI for Database\nSystems and Applications, AIDB Workshops .\n[53] Stackoverflow. 2021. Vote ID. (2021). https://archive.org/download/\nstackexchange .\n[54] Chuzhe Tang and et al. 2020. XIndex: A Scalable Learned Index for Multicore\nData Storage. In Proceedings of the 25th ACM SIGPLAN Symposium on Principles\nand Practice of Parallel Programming (PPoPP ‚Äô20) .\n[55] Gary Valentin, Michael Zuliani, Daniel C. Zilio, Guy M. Lohman, and Alan Skelley.\n2000. DB2 Advisor: An Optimizer Smart Enough to Recommend Its Own Indexes.\nInProceedings of the 16th International Conference on Data Engineering . 101‚Äì110.\n[56] Haixin Wang, Xiaoyi Fu, Jianliang Xu, and Hua Lu. 2019. Learned Index for Spatial\nQueries. In 2019 20th IEEE International Conference on Mobile Data Management\n(MDM) . 569‚Äì574.\n[57] Youyun Wang, Chuzhe Tang, Zhaoguo Wang, and Haibo Chen. 2020. SIndex: a\nscalable learned index for string keys. In Proceedings of the 11th ACM SIGOPS\nAsia-Pacific Workshop on Systems . 17‚Äì24.\n[58] Ziqi Wang, Andrew Pavlo, Hyeontaek Lim, Viktor Leis, Huanchen Zhang, Michael\nKaminsky, and David G. Andersen. 2018. Building a Bw-Tree Takes More Than\nJust Buzz Words. In Proceedings of the 2018 International Conference on Manage-\nment of Data (Houston, TX, USA) (SIGMOD ‚Äô18) . Association for Computing Ma-\nchinery, New York, NY, USA, 473‚Äì488. https://doi.org/10.1145/3183713.3196895\n\n[59] Chaichon Wongkham, Baotong Lu, Chris Liu, Zhicong Zhong, Eric Lo, and\nTianzheng Wang. 2022. Are Updatable Learned Indexes Ready? (Extended Ver-\nsion). arXiv (2022).\n[60] Edward L Wright, Peter RM Eisenhardt, Amy K Mainzer, Michael E Ressler, Roc M\nCutri, Thomas Jarrett, J Davy Kirkpatrick, Deborah Padgett, Robert S McMillan,\nMichael Skrutskie, et al. 2010. The Wide-field Infrared Survey Explorer (WISE):\nmission description and initial on-orbit performance. The Astronomical Journal\n140, 6 (2010), 1868.\n[61] Jiacheng Wu and et al. 2021. Updatable Learned Index with Precise Positions.\nProc. VLDB Endow.\n[62] Xingbo Wu, Fan Ni, and Song Jiang. 2019. Wormhole: A Fast Ordered Index for\nIn-memory Data Management. In EuroSys . 18:1‚Äì18:16.[63] Zhongle Xie, Qingchao Cai, Gang Chen, Rui Mao, and Meihui Zhang. 2018. A\nComprehensive Performance Evaluation of Modern In-Memory Indices. In 2018\nIEEE 34th International Conference on Data Engineering (ICDE) . 641‚Äì652.\n[64] Zongheng Yang and et al. 2020. Qd-Tree: Learning Data Layouts for Big Data\nAnalytics. In ACM SIGMOD International Conference on Management of Data .\n[65] Huanchen Zhang and et al. 2016. Reducing the Storage Overhead of Main-\nMemory OLTP Databases with Hybrid Indexes. In Proceedings of the 2016 Inter-\nnational Conference on Management of Data (SIGMOD ‚Äô16) .\n[66] Songnian Zhang, Suprio Ray, Rongxing Lu, and Yandong Zheng. 2021. SPRIG: A\nLearned Spatial Index for Range and kNN Queries. In 17th International Sympo-\nsium on Spatial and Temporal Databases .\n\nAre Updatable Learned Indexes Ready?\n248 16 24\n# of cores02550Throughput (Mop/s)\ncovid\n248 16 24\n# of cores\nlibio\n248 16 24\n# of cores\ngenome\n248 16 24\n# of cores\nosm\nper-node locking per-256-records locking\nFigure A: Throughput of ALEX+ using different lock granu-\nlarities under the Balanced workload.\n248 16 24\n# of cores204060Throughput (Mop/s)\nRead Only\n248 16 24\n# of cores\nBalanced\nALEX+\nALEX+-LL\n248 16 24\n# of cores\nWrite Only\nFigure B: Throughput of ALEX+ on the wiki dataset with\nnon-unique keys under different workloads.\nAPPENDIX\nA LOCK GRANULARITY IN ALEX+\nWe implemented two versions of ALEX+ using different lock granu-\nlarities. We finally adopted the version that uses a single optimistic\nlock per data node over the one that uses one optimistic lock per\n256 records in a data node.\nFigure A shows the throughput of the two versions under the\nBalanced workload on four real datasets. It shows that using a\nsingle optimistic lock per data node is consistently better regardless\nof the data hardness. The overhead of using a lock per 256 records\nis higher although it admits more concurrency. Specifically, ALEX+\ninherits ALEX‚Äôs design to use exponential search within a data node\nand when using one lock per 256 records, locks could be acquired\nin different orders as a search can go either direction within a data\narray. This can lead to deadlocks. To prevent deadlock, per-256-\nrecord locking requires ALEX+ to release all the acquired locks once\nit fails to acquire a lock and restarts. In contrast, per-node locking\nis deadlock-free. Moreover, the lock can be inlined within a data\nnode‚Äôs header, accessing which incurs no extra cost if the header is\nsmaller than a cacheline because the node header contains essential\nmetadata (e.g., the ML model) that must be accessed anyway.\nB SUPPORT FOR NON-UNIQUE KEYS\nOur evaluation has been mainly using workloads with unique keys\nonly. But one may also build indexes on non-primary attributes with\nduplicated keys. In this section, we explore the impact of duplicate\nkeys on learned indexes.\nThere are two main approaches for an index to handle duplicated\nkeys: (1) using a linked list and (2) inlining. For (1), values of the\nsame key form a linked list, and only the first occurrence of key\nwould be presented in the nodes. For (2), all occurrences of the\nvalues of the same key would be stored (inlined) in the index node.Generally, the use of inlining or linked list is a tradeoff between\nlookup and insert. Inlining favors lookup because the values are\nco-located in memory, traversing which does not require additional\npointer chasing. However, inlining is less friendly to insertion and\nmay require sophisticated space management (e.g., for variable-size\npayload) in index nodes, increasing the write amplification as new\ninserts may cause the payloads and keys to be moved around in the\nnode (e.g., to keep key-value pairs sorted). In contrast, the linked\nlist approach favors insertion because the values are stored out-\nof-place; the index node only needs to store a pointer to the list\nof values per key. Yet, the linked list approach may reduce lookup\nthroughput because of pointer chasing.\nAmong all the learned indexes surveyed, none but ALEX (hence\nALEX+) supports duplicated keys using inlining. Hence, to study\nwhether learned indexes would follow the tradeoff discussed above\nwhen facing duplicated keys, we implemented a version of ALEX+\nthat uses linked list to handle duplicates (denoted as ALEX+LL).\nFigure B shows the performance of ALEX+ (that uses inlining) and\nALEX+LL on SOSD‚Äôs wiki dataset, which contains duplicates. The\nresults show that ALEX+ as the overall best concurrent learned\nindex also exhibits the aforementioned tradeoff, where the use of\nlinked list makes it better in insert but the use of inlining makes in\nbetter it lookup.\nC COMPUTING THE OPTIMAL PLA-MODEL\nWe use the algorithm in [ 18] to compute the optimal PLA-model\nof a given dataset. It belongs to a family of online algorithms [ 47]\nthat aim to fit straight lines for time-series data, where each data\npoint(ùë°,ùë£)has a timestamp ùë°with an error range ùúñon the value ùë£.\nTheir goal is to maintain a set of straight lines that can fit all the\ndata points within their error ranges. Since the data is time-series,\nùë°ùëñis smaller than ùë°ùëóifùëñ<ùëó. In [ 18], it views each value in a data\narray as a tuple(ùëò,ùëü), whereùëòis the key and ùëüis the position in the\ndata array. In range indexes, it is natural that ùëòùëñis smaller than ùëòùëó\nifùëñ<ùëó. Hence, the algorithm in [ 18] leverages the time-series-like\ntotal order to achieve linear time and space complexity. Briefly, the\nidea is to incrementally construct a convex hull for a set of points\n(ùëòùëñ,ùëüùëñ)forùëñ=1...ùëõ. The rankùëüùëñwould be within¬±ùúñas long as\ntheir convex hull can be enclosed by a bounding box with height\n2ùúñ. Incrementally updating the convex hull admits linear time and\nspace complexity. If a point (ùëòùëñ,ùëüùëñ)goes beyond the bounding box,\nthat means the current set of data cannot be fitted by any straight\nline, then the algorithm increments the number of segments by one\nand moves on to construct the next segment starting with ùëòùëñ. For\nmore details, interested readers may refer to [18, 47].\nD APPROXIMATING DATA HARDNESS\nAll single-dimensional indexes, to our best knowledge, are de-\nsigned as a tree of linear models. Therefore, the PLA of a single-\ndimensional dataset that captures the minimal number of linear\nmodelsùêªrequired to fit the data distribution is a natural candidate\nto approximate the data hardness. Yet, PLA has a subtle parameter ùúñ\nthat governs the approximation quality. Intuitively, a good approxi-\nmation (i.e., a good choice of ùúñ) shall align best with any learned\nindex performance. In other words, if a data hardness approxima-\ntion metric determines that dataset ùê¥is easier than dataset ùêµ(i.e.,\n\n0 2 4 6 8\nKey domain 1e90%20%40%60%80%100%planet(a)planet\n0 25 50 75 100 125\nKey domain 1e170%20%40%60%80%100%osm (b)osm\nFigure E: Fitting one linear regression model (red line) and\nusing the MSE as the global hardness.\n0 50000 100000 150000 200000 250000 300000\nGlobal hardness HLRMSE\n 1e1002468Throughput (Mop/s)\nosm\nplanetgenome\nfbALEX\nLIPP\nFigure F: Throughput of the Balanced workload under vary-\ning global hardness based on MSEs.\n0 200000 400000 600000 800000 1000000 1200000\nLocal hardness HPLA(=32)\n02468Throughput (Mop/s)\nosmplanet\ngenomefbALEX\nLIPP\nFigure C: Throughput of the Balanced workload under vary-\ning PLA local hardness with a small ùúñvalue (32) .\n0 1000 2000 3000 4000 5000\nGlobal hardness HPLA(=4096)\n02468Throughput (Mop/s)\nosmplanetgenome\nfbALEX\nLIPP\nFigure D: Throughput of the Balanced workload under vay-\ning PLA (ùúñ=4096) global hardness.\nùêªùê¥<ùêªùêµ), then it would be a good approximation if all learned\nindexes perform better on ùê¥than onùêµ. That forms our basis for\nchoosing the ùúñvalues.\nFrom a high-level, our goal is to choose a pair of ùúñvalues for\nlocal and hardness approximation, respectively. The best choiceshould lead to data hardness (both local and global) that aligns well\nwith the index performance expectations (i.e., harder leads to lower\nperformance). In general, a small/large ùúñin PLA is a fine-grained ap-\nproximation of the CDF that captures the local/global non-linearity\nbetter. Therefore, to pick a pair of suitable ùúñvalues, we first fix\nthe small value, and try different large values to observe whether\nthe resulting approximation would align with index performance\nbehaviors. We repeat this process with different small values (for\neach of which we subsequently test different large values) to find\na desirable pair eventually. We empirically experimented various\nvalues and found that many choices would work and provide simi-\nlar results (e.g., 32/4096, 64/2048, etc.). For brevity, below we take\n32/4096 as the small/large ùúñvalues (which led to the best results\namong our tests) to explain the rationale and process in more detail.\nFigure C shows the throughput of ALEX and LIPP under different\ndata hardness based on a small ùúñ=32value (other learned indexes\nbehave similarly; omitted for clarity). It is a fairly good approxi-\nmation because the performance of the surveyed learned indexes\nalign with the data hardness pretty well ‚Äî the index throughput\ngenerally degrades when ùêªincreases. However, we observe that\nthe use of PLA ( ùúñ=32) alone is insufficient because the indexes\nshow similar throughput even though ùêªùëùùëôùëéùëõùëíùë° andùêªùëúùë†ùëöare much\nsmaller than ùêªùëìùëèandùêªùëîùëíùëõùëúùëöùëí under that metric. That essentially\nimplies that planet andosmpossess some other hardness that is\nnot captured by PLA ( ùúñ=32).\nWith a small ùúñcapturing local hardness, now we experiment\nwith different large values for global non-linearity. Figure D shows\nthe throughput of ALEX and LIPP by arranging a dataset‚Äôs global\nhardness using PLA with ùúñ=4096 . A highùúñvalue means a large\nstep size when segmenting a CDF, which can capture a dataset‚Äôs\nglobal non-linearity. Furthermore, PLA is less sensitive to outliers\n(e.g., the few outliers in fbcan all be represented by a few new\nmodels, hence adding the hardness value only by a bit). By using\nPLA with a large ùúñ, the approximation correctly ranks planet and\nosmas globally harder than fbandgenome , complementing the\nlocal non-linearity approximation obtained using a small ùúñvalue.\nOne may notice that there are other alternatives to complement\nlocal non-linearity. For example, we can fit the whole CDF using\none linear regression model and use its mean-square-error (MSE)\nto measure its global non-linearity, as shown in Figure E. However,\nwe note using a large ùúñvalue in PLA can capture the global non-\nlinearity better than using MSEs. Figure F shows the throughput of\nALEX and LIPP by arranging the datasets‚Äô global hardness using\ntheir MSEs. In the figure, planet andosmare harder than genome in\nglobal non-linearity, indicating the approach can complement the\nlocal non-linearity dimension. However, it still cannot explain the\nbehavior of the indexes on fb. Specifically, fbis regarded as much\nharder than planet andosmboth locally (Figure C) and ‚Äúglobally‚Äù\n(Figure F). One would therefore expect an index to perform worse on\nfbthan on osmandplanet . However, we observe that for the same\nindex (e.g., ALEX), it performs more or less the same across planet ,\nosmand fb. In other words, fb‚Äôs overall hardness (considering\nboth local and global) should be similar to the other two datasets,\notherwise the index should not perform similarly across all the\nthree datasets. This shows the limitation of MSE: it is too sensitive\nto outliers, giving fb(which has a few outliers with very large keys)\n\nAre Updatable Learned Indexes Ready?\n010A (Mop/s)covid libio genome osm\n010B (Mop/s)\n010C (Mop/s)\nALEX\nLIPPPGM-Index\nXIndexFINEdex\nARTMasstree\nWormhole\n(a) Single core\n0200A (Mop/s)covid libio genome osm\n0200B (Mop/s)\n0200C (Mop/s)\nALEX+\nLIPP+XIndex\nFINEdexART-OLC\nMasstreeWormhole (b) 24 cores\nFigure G: Throughput of indexes under YCSB workload with Zipfian distribution.\nan overly-high global non-linearity. Yet using PLA as we explained\nabove can model the data hardness better.\nE YCSB RESULTS\nOur evaluation has mainly focused on index performance under\nworkloads whose keys are uniformly sampled from real data. In\nthis section, we study the index performance using workloads with\nnon-uniform key distributions. Specifically, we use three workloads\nfrom the industrial-strength YCSB benchmark [9]:\n‚Ä¢YCSB-A: an update-heavy workload that contains 50% lookup\nrequests and 50% update requests where keys are chosen under\na Zipfian distribution.\n‚Ä¢YCSB-B: similar to YCSB-A except it is read-heavy with 95% of\nrequests are lookups and 5% are updates.‚Ä¢YCSB-C: similar to the other two except it is a read-only workload\nwith 100% lookup.\nThe default Zipfian constant in YCSB is 0.99. Figure G shows that\nour findings in the main discussion can be extended to non-uniform\nworkloads. For example, ALEX and LIPP are still the leaders in most\ncases while ART comes close on hard data. It is worth noting that\nLIPP+ remains competitive in YCSB workloads even using multiple\ncores (Figure Gb). YCSB workloads have no key insertions but only\nupdates the payloads of existing keys. Hence, it can scale because\nthere are no updates on the per-node statistics along its update path\nand triggering no atomic instructions. Yet, that does not change\nthe fact that LIPP consumes huge space, cannot scale with inserts,\nand is not range-scan friendly.",
  "textLength": 85855
}