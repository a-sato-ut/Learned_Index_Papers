{
  "paperId": "219af6ca8d476bd2d8ac0cf0eac3dc0297ce6c00",
  "title": "FASTune: Towards Fast and Stable Database Tuning System with Reinforcement Learning",
  "pdfPath": "219af6ca8d476bd2d8ac0cf0eac3dc0297ce6c00.pdf",
  "text": "Citation: Shi, L.; Li, T.; Wei, L.; Tao,\nY.; Li, C.; Gao, Y. FASTune: Towards\nFast and Stable Database Tuning\nSystem with Reinforcement Learning.\nElectronics 2023 ,12, 2168.\nhttps://doi.org/10.3390/\nelectronics12102168\nAcademic Editors: Franco Cicirelli\nand Manohar Das\nReceived: 10 March 2023\nRevised: 2 May 2023\nAccepted: 7 May 2023\nPublished: 10 May 2023\nCopyright: © 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nelectronics\nArticle\nFAST une: Towards Fast and Stable Database T uning System\nwith Reinforcement Learning\nLei Shi1,2,3,*\n, Tian Li2, Lin Wei1\n, Yongcai Tao2, Cuixia Li1,* and Yufei Gao1,3\n1School of Cyber Science and Engineering, Zhengzhou University, Zhengzhou 450002, China;\nyfgao@zzu.edu.cn (Y.G.)\n2School of Computer and Artiﬁcial Intelligence, Zhengzhou University, Zhengzhou 450001, China;\n202022172013213@gs.zzu.edu.cn (T.L.)\n3Songshan Lab, Zhengzhou 450046, China\n*Correspondence: shilei@zzu.edu.cn (L.S.); lcxxcl@zzu.edu.cn (C.L.)\nAbstract: Conﬁguration tuning is vital to achieving high performance for a database management\nsystem (DBMS). Recently, automatic tuning methods using Reinforcement Learning (RL) have been\nexplored to ﬁnd better conﬁgurations compared with database administrators (DBAs) and heuristics.\nHowever, existing RL-based methods still have several limitations: (1) Excessive overhead due to\nreliance on cloned databases; (2) trial-and-error strategy may produce dangerous conﬁgurations\nthat lead to database failure; (3) lack the ability to handle dynamic workload. To address the above\nchallenges, a fast and stable RL-based database tuning system, FASTune, is proposed. A virtual\nenvironment is proposed to evaluate conﬁgurations which is an equivalent yet more efﬁcient scheme\nthan the cloned database. To ensure stability during tuning, FASTune adopts an environment proxy\nto avoid dangerous conﬁgurations. In addition, a Multi-State Soft Actor–Critic (MS-SAC) model\nis proposed to handle dynamic workloads, which utilizes the soft actor–critic network to tune\nthe database according to workload and database states. The experimental results indicate that,\ncompared with the state-of-the-art methods, FASTune can achieve improvements in performance\nwhile maintaining stability in the tuning.\nKeywords: database tuning; reinforcement learning; decision making; deep learning\n1. Introduction\nA database has numerous tunable parameters [ 1], which can signiﬁcantly affect perfor-\nmance metrics, such as latency and throughput. Appropriate conﬁgurations can improve\nthe database performance. Database tuning is an NP-hard problem [ 2,3], making the search\nfor optimal conﬁgurations a challenging task for DBAs. In recent years, some studies have\nfocused on automatic database tuning, including rule-based methods [ 4–6] and learning-\nbased methods [ 3,7–12]. Rule-based methods search for optimal conﬁgurations based\non ﬁxed rules, which have previously been observed to improve database throughput\ncompared to default conﬁgurations on OLTP (Online Transaction Processing) workloads.\nHowever, the rule-based method fails to utilize experience from previous tuning processes\nand thus needs to restart the search process for each new tuning request. Learning-based\nmethods, e.g., Ottertune [ 8], utilize a machine-learning model to select knobs, map the\nworkload, and recommend conﬁgurations to improve database performance. However,\nthese methods have two limitations.\nFirstly, their reliance on the pipelined approach can result in sub-optimal performance,\nas the optimal solution for a particular stage may not guarantee the optimal solution for\nthe subsequent stage, and different stages may not complement each other effectively.\nConsequently, an end-to-end optimization of overall performance becomes unfeasible.\nSecondly, the models depend on large-scale, high-quality training samples, which can\nbe difﬁcult to access. For instance, the performance of cloud databases is inﬂuenced by\nElectronics 2023 ,12, 2168. https://doi.org/10.3390/electronics12102168 https://www.mdpi.com/journal/electronics\n\nElectronics 2023 ,12, 2168 2 of 22\nvarious factors, such as memory size, disk capacity, and workloads. Capturing all these\nconditions and accumulating high-quality samples present challenging tasks.\nIn this case, conventional machine learning approaches have poor adaptability and\nthe model requires retraining to adapt to the new environment.\nAnother family of learning-based methods, e.g., Qtune [ 2], CDBTune [ 7] and HUNTER [3]\naddress the tuning problem using reinforcement learning [ 13,14]. They consider the\ndatabase an environment and use an agent to ﬁnd optimal conﬁgurations through a\ntrial-and-error strategy, which alleviates the burden of collecting a large number of samples\nin the initial modeling stage. However, applying these methods in the real world still has\nseveral challenges:\nFirst, the agent updates the policy according to an evaluation of conﬁgurations, which\ndepends on a time-consuming stress test on the database.\nSecond, a trial-and-error strategy is adopted in RL to exploit optimal conﬁgurations.\nThus, the agent may recommend dangerous conﬁgurations that can cause performance\ndegradation or database crashes which is unacceptable.\nThird, the workload is assumed constant, so the tuning aims to improve performance\non a speciﬁc workload. However, as shown in Figure 1, the real-world workload can be\nvaried so that the tuning result may be delayed.\nFigure 1. Statistics of different query types over time in the workload of the BusTracker application\n(a mobile phone application for live-tracking of the public transit bus system).\nTo address the above problems, FASTune is proposed to prevent dangerous conﬁgura-\ntions, accelerate the tuning process and adapt to the dynamic workload. To boost efﬁciency\nand ensure the stability of the database during the tuning process, FASTune implements an\nenvironment proxy. Environment proxy achieves this goal through: (i) Discarding actions\nthat could cause a dramatic drop in database performance or database failure. (ii) Reducing\ninefﬁcient evaluation of the action. In contrast to the existing methods that enable direct\ninteraction between the agent and the environment, our research incorporates a wrapped\nenvironment with a proxy, through which the agent interacts with the environment. The\nevaluation of action is also handled by environment proxy. Environment proxy has three\nkey components: Filter, Virtual Environment, and Dispatcher. Filter excludes dangerous\nactions to avoid ﬂuctuations and database failures. The Filter uses rule-based and learning-\nbased methods to evaluate dangerous actions. The Filter extracts rules from the documents\n(e.g., the database manual), and actions that match these rules will be considered dangerous\nand excluded. Using rules is straightforward and effective but can not cover all situations\nbecause there are non-linear correlations between knobs and database performance. So\nFilter utilizes a classiﬁcation model to detect a dangerous action, reduces the dangerous\naction signiﬁcantly, and contributes to the stability of the database.\n\nElectronics 2023 ,12, 2168 3 of 22\nFASTune employs a virtual environment to evaluate conﬁgurations more effectively.\nA virtual environment is a model that mimics the behavior of the environment as closely\nas possible while being computationally feasible. The virtual environment estimates the\nevaluation of the action, which reduces unnecessary stress tests on the database. Virtual\nEnvironment can reduce tuning time more efﬁciently and does not require additional\nmemory and storage compared to cloned database. However, since the estimation is based\non historical data, predicting the performance of actions can be difﬁcult when there are\ninsufﬁcient relevant data. To address this issue, the dispatcher is proposed, dispatcher\ndivides the actions into two groups: “common” and “uncommon”. Common actions\nmean that the action is numerically close to the previous action so that the performance\nof the action can be predicted based on historical data. Conversely, for an uncommon\naction, deploying it on the database and performing a stress test is necessary to acquire\nits performance. Therefore, the environment (i.e., the database instance) is required. The\ndetails of the Virtual Environment will be discussed in Section 5.\nSince several studies have shown that different workloads are sensitive to differ-\nent knobs [ 10,15], it is necessary for agent to consider workload characteristics during\nthe tuning. A Multi-State Soft Actor–Critic model (MS-SAC) is proposed to handle dy-\nnamic workloads. Different from previous work, MS-SAC ﬁnds optimal conﬁgurations\naccording to both environment state and workload state. FASTune continuously collects\nworkload arriving at the database and constructs a model to predict future workload. The\npredicted results will be provided to the agent as workload state. The paper makes the\nfollowing contributions:\n1. Environment Proxy is proposed in Reinforcement Learning for database tuning, which\nimproves the efﬁciency and stability of tuning;\n2. FASTune utilizes a combined approach to exclude dangerous conﬁgurations;\n3. MS-SAC model is proposed, which utilizes the soft actor–critic network to ﬁnd optimal\nconﬁgurations based on both the environment and workload state;\n4. Experimental evidence demonstrates that FASTune can considerably reduce tuning\ntime and ensure the stability of database tuning.\n2. Related Works\n2.1. Database Tuning\nAutomatic database tuning systems have been studied for decades [ 2,8,16–36]. They\ncan be broadly classiﬁed into the following categories.\n2.1.1. Rule-Based Methods\nRule-based methods use rules or heuristics to ﬁnd optimal conﬁgurations for the\ndatabase. iTuned [ 12] utilizes statistical methods to select essential knobs and ﬁnd correla-\ntions between these knobs and performance. Wei et al. [ 6] propose a method that generates\nrules and uses them for tuning. BestConﬁg [ 4] splits the high-dimension search space into\nsub-spaces and recommends conﬁgurations using a recursive bound-and-search algorithm.\nDB2 proposes a self-tuning memory model that uses heuristics to allocate proper memory\nto the components of database [ 37], Tran et al. [ 38] used linear and quadratic regression\nmodels for buffer tuning. The DBSherlock helps DBA to diagnose faults by comparing slow\nregions and normal regions [ 39]. However, rule-based methods rely on historical data or\nrules and fail to utilize previous tuning processes’ experiences.\n2.1.2. Learning-Based Methods\nOttertune [ 10] and ResTune [ 15] map the tuning problem to black-box optimization\nissues and uses the traditional Machine learning (ML) method to obtain optimal solutions.\nHowever, they depend on a large amount of high-quality training data. Another family\nof learning-based methods, e.g., CDBTune [ 7] and Hunter [ 3], utilize RL to tune database\nand the conﬁgurations. They train an agent that uses a trial-and-error strategy to search for\noptimal conﬁgurations. In the beginning, agent randomly recommends conﬁgurations and\n\nElectronics 2023 ,12, 2168 4 of 22\napplies them to the database, and then agent adjusts its policy based on the feedback from\nthe database. RL can achieve high performance and does not require training data, but\nagent may recommend risky conﬁgurations that cause database failure. In addition, the\ninteraction with the database is time-consuming, which prevents them from being applied\nin the production environment. Kanellis [ 40] proposed a knobs importance ranking method\nto automatically ﬁlter the most important knobs to reduce tuning time, which is a limited\nimprovement. CDBTune uses cloned database instances to accelerate the tuning process.\nHowever, it brings a huge cost.\n2.2. Reinforcement Learning\nReinforcement learning is proposed to solve multi-person decision problems [ 13,41].\nRL is a model that uses an agent (learner) to execute an action (make a decision) in a\nspeciﬁc environment (scenario) and learn from the interactions with the environment [ 42].\nDifferent from supervised learning, RL does not require much training data. Instead, the\nagent updates its policy of outputting actions to maximize a reward function. Usually, a\nhigher reward means a better decision(action).The essential part of reinforcement learning\nis learning from interactions with the environment to maximize a cumulative reward\nsignal over a period of time. In other words, an agent learns to take actions based on the\nfeedback received from the environment, with the goal of receiving the highest possible\nreward in the long run. The agent’s goal is not to predict a speciﬁc output but to maximize\nthe expected cumulative reward. This trial-and-error learning process is a key aspect of\nreinforcement learning. Reinforcement learning has been successfully utilized in different\noptimal problems [ 43–47], and there is a large number of published studies that use RL\nto address database problems. Neo [ 48] developed a learnable query optimizer with RL.\nSageDB [ 49] suggests a vision that learning-based models can replace some components\nin a DBMS. Basu et al. use RL to implement index tuning. OpenGauss [ 50] uses a deep\nreinforcement learning model to optimize the query plan.\n3. System Overview\nIn this section, The architecture and the workﬂow of our database tuning system are\ndemonstrated.\n3.1. Architecture\nFASTune is an database tuning system with reinforcement learning, and an overview\nof it is shown in Figure 2.\nFASTune consists of three parts. First, Agent interacts with the Environment Proxy and\nrecommends conﬁgurations. Agent is a Soft Actor–Critic (SAC) [ 51] network containing\ntwo independent neural networks: actor and critic. Actor takes multi-state as input and\noutputs an action (i.e., conﬁgurations), and the Proxy computes a reward based on the\nevaluation of action. Critic takes multi-state and action as input and outputs a Q-value\n(score). Critic updates according to the reward, and actor updates according to the Q-value.\nDetails of SAC will be discussed in Section 4. Second, the Environment Proxy contains three\ncomponents—(a) Filter using rules and machine learning methods to exclude dangerous\naction for safety tuning. (b) Virtual Environment alleviates the burden of evaluating the\naction in tuning. (c) Dispatcher receives the action and deploys it to environment or sends it\nto Virtual Environment. Third, Workload Collector records recently arrived workloads and\npredict the expected queries in the future. The interactions between Agent and Environment\nProxy generate trajectories stored in memory; trajectories are a set of tuples <Sw,Sd,A,r>.\nSwis the feature of predicted future workload, and Sdrepresents database state such as\nlock_row_lock_current _waits , etc. Ais an action generated by Agent, and ris the reward of\naction calculated from throughput and latency. Note that, different from existing methods,\nSwand Sdare combined and provided to the agent as multi-state.\n\nElectronics 2023 ,12, 2168 5 of 22\nFigure 2. Overview architecture of FASTune.\n3.2. Workﬂow\nFASTune works in 4 stages:\n1. Initialize. FASTune stress tests the database with default conﬁgurations to build an\ninitial performance baseline. Then the weights of the actor and critic network are\ninitialized by the stochastic distribution. Workload Collector starts to collect workload\nuntil the tuning process ends;\n2. Recommend. As the tuning request arrives. Agent takes environment state and workload\nstate as input and then outputs an action based on the policy . The Agent sends the action\nto Environment Proxy and waits for a reward (i.e., the evaluation of action);\n3. Evaluate. The ﬁlter in the environment proxy excludes potentially harmful actions\nbefore the evaluation process. Once action passes through the ﬁlter, Dispatcher\ndetermines whether the action is “common” or “uncommon” based on historical\ndata, “common” and “uncommon” action will be sent to Virtual Environment and\nEnvironment, respectively. If action is sent to Virtual Environment, Agent will get\nthe predicted evaluation. Otherwise, the Proxy deploys the action on database and\nreturns an evaluation based on a stress test. Every interaction between the proxy and\nthe agent generates a sample, which is then stored in memory;\n4. Update. Environment Proxy calculates the reward according to the action evaluation\nand returns it to Agent. Since a higher reward indicates better performance, Agent\nupdates the policy network (i.e., actor) to earn a higher reward and updates value\nnetwork (i.e., critic) to provide more accurate feedback for the actor. The detail of\nactor and critic are described in Section 4.\nStage 2–4 is repeated until the model converges or meets other stop conditions (e.g.,\ndatabase throughput reaches a given threshold). Finally, the Agent deploys conﬁgurations\nwith ideal performance on the database instance.\n\nElectronics 2023 ,12, 2168 6 of 22\n4. RL for T uning\nThis section introduces RL to solve the tuning problem, and then the proposed MS-\nSAC model is presented.\nUsually, there are hundreds of tunable knobs in databases, some of which are in\ncontinuous space [ 1]. So it is hard for traditional machine learning methods to ﬁnd optimal\nconﬁgurations [ 2]. As both the rule-based and conventional learning-based approaches\nhave limitations, designing a more efﬁcient tuning system is necessary. Reinforcement\nLearning (RL) is a learning method that can effectively operate with limited samples during\ninitial training.Because RL makes decisions through the interaction process between an\nagent and its environment, relying on accumulated rewards rather than labels to perform\nlearning. Some popular RL methods include Q-learning [ 52], DQN [ 53], and DDPG [ 54].\nQ-learning uses Q-tables deﬁned as Q(s,a), where the rows represent the Q-value of states\nand the columns represent actions. The Q-value measures how beneﬁcial it would be to\ntake a particular action in the current state. Q(s,a)is iteratively deﬁned as follows:\nQ(st,at) Q(st,at) +a[r+gmax at+1Q(st+1,at+1)\u0000Q(st,at)] (1)\nQ-Learning updates the Q-table based on the Bellman Equation, where a represents\nthe learning rate, gis a discount factor, and ris the performance at time t+1. However, it\nis impractical to solve database tuning problems with a large state space because Q-table\ncan hardly store so many states. Even with ten metrics discretized into ten equal bins\neach, it results in 1010states. DQN uses neural networks to replace Q-tables but can only\noutput discrete actions, while knob combinations in a database are high-dimensional and\ncontinuous. CDBtune employs DDPG, which utilizes two deep neural networks: an actor\nand critic networks to address the issue. The actor network maps states to actions, and\nthe critic network approximates the state-action value function. The actor network learns\nthe policy, while the critic network estimates the value function of the policy. With the\nactor–critic architecture, DDPG immediately acquires the value of the current action based\non the current state without having to compute and store Q-values for all actions like DQN.\nAs a result, DDPG can learn the policy with high-dimensional states and actions, making it\na more suitable choice for solving database tuning problems.\nHowever, the interaction between the deterministic policy network (i.e., actor) and the\nvalue network (i.e., critic) makes DDPG brittle to hyper-parameter: discount factor, learning\nrates, and other parameters must be set carefully to achieve ideal results. Consequently,\nusing DDPG on complex high-dimensional tasks is hard to stabilize. These issues limit the\napplication of RL to real-world tasks. The Multi-State Soft Actor–Critic model (MS-SAC) is\nproposed to overcome these disadvantages.\n4.1. MS-SAC Model\nMS-SAC uses the soft actor–critic networks [ 51] to develop an agent speciﬁcally for\ndatabase tuning. The SAC algorithm is a variant of the actor–critic algorithm that optimizes\nthe actor’s policy objective by introducing entropy regularization. This regularization\nhelps to encourage exploration and prevent the policy from becoming too deterministic,\nwhich can lead to sub-optimal solutions. The concept of Multi-State refers to the agent\nrecommending conﬁgurations based on both the environment and the workload state,\nunlike previous works. This approach enhances FASTune’s adaptability and stability, and\nwe plan to introduce more states (such as network states) in the future. It is worth noting\nthat, unlike traditional RL, the environment is enveloped by our proposed proxy, which\nmeans that the agent interacts with the proxy rather than the environment itself. More\ninformation about the environment proxy can be found in Section 5. Table 1 illustrates the\nmapping from MS-SAC to the tuning task and clariﬁes the notion presented.\n\nElectronics 2023 ,12, 2168 7 of 22\nTable 1. Mapping from Reinforcement Learning to Database Tuning.\nVariables MS-SAC Database to Be T uned\nE Environment Database to be tuned\nS Database and workload state Metrics of database and workload\na Action Conﬁgurations of database\nr Reward Performance improvement\n- Agent The soft actor–critic network\np Policy -\nQpCritic -\nVpActor -\nq The weights in actor -\nw The weights in critic -\ng Discount factor set to 0.9\na Coefﬁcient of explore and exploit set to 0.2\nr Coefﬁcient of soft update target network set to 0.01\nL Loss function -\n- Environment proxy -\n1. Environment. The Environment is the tuning target. Precisely, a database instance;\n2. Database state. The Database State records the metrics of database, which consist\nof cumulative value (e.g., lock_deadlocks ) and state value (e.g., f ile_num _open _f iles );\nboth reﬂect the situation inside the database;\n3. Workload state. Workload State represents the characteristics of the upcoming work-\nload. FASTune combines the Workload State with and Database State as the Multi-\nState, which is provided to the Agent when generating an Action;\n4. Action. Action is database conﬁgurations generated by Agent. From a mathematical\nviewpoint, Action and Multi-State are both vectors. Agent maps Multi-State to Action,\ngiven the State, Agent deterministically outputs an Action;\n5. Reward. Reward is a scalar that reﬂects the quality of Action. FASTune calculates the\nreward according to performance change after the database deploys a new action. A\nhigher reward means greater Action. Note that the MS-SAC model optimizes policies\nto maximize the expected entropy of the policy, so both performance change and\nentropy of the action are considered in the calculation of the reward;\n6. Agent. FASTune utilizes the actor–critic networks as an Agent to tune knobs. Agent\nreceives a Reward and Multi-State from Environment Proxy and updates the policy to\nlearn how to recommend high-quality Action that can earn a higher reward.\n4.2. Training\nAs described above, the agent is to maximize a cumulative reward signal over time\nwhich can be deﬁned as the function:\np\u0003=max\nqE[¥\nå\nt=0R(st,at) +aH(p(\u0001jst))] (2)\nwhere the policy of the agent is represented by p.R(st,at)is the reward function, and ais\nthe coefﬁcient. H(p(˙jst))represents the entropy of the actions. The entropy of a random\nvariable xwith probability density p(x)is deﬁned as:\nH(P) =E\nx\u0018P[\u0000logP(x)] (3)\nEntropy reﬂects the degree of disorder in a system. In database tuning optimization,\nthis term represents the diversity of the agent’s output conﬁgurations. Entropy maximiza-\ntion leads to more exploration and thus prevents the model from converging to a bad\nlocal optimum.\n\nElectronics 2023 ,12, 2168 8 of 22\nThe policy function with entropy item is deﬁned as:\nVp(s) =Ep\u0014¥\nå\nt=0gt\u0012\nR(st,at,st+1) +aH(p(\u0001jst))\u0013\u0015\n(4)\nThe action-value function (Q-function) with entropy item is deﬁned as:\nQp(s,a) =Ep\"\n¥\nå\nt=0gtR(st,at,st+1) +a¥\nå\nt=1gtH(p(\u0001jst))#\n(5)\nWith the equations above, the Vpand Qpcan be connected by\nVp(s) =Ea\u0018p[Qp(s,a)] +aH(p(\u0001js)) (6)\nand the Bellman equation for Qpis:\nQp(s,a) = E\ns0\u0018P,a0\u0018pR(s,a,s0) +g\u0000\nQp(s0,a0) +aH\u0000\np(\u0001js0)\u0001\u0001\n(7)\n= E\ns0\u0018PR(s,a,s0) +gVp(s0). (8)\nTo alleviate the overestimation problem, SAC concurrently has two q-functions with\nparameters w1andw2and one policy function with parameter q. SAC selects the one with\na lower value between two Q-functions, and the loss functions for the Q-networks(critic) is:\nLQ(w) = E\n(s,a,r,s0,d)\u0018Dh\n(Qw(s,a)\u0000y(r,s0,d))2i\n(9)\nthe loss of the policy network(actor) is :\nLp(q) = E\ns\u0018R,a\u0018pq[alog(pq(ajs))\u0000Qw(s,a)] (10)\nThe training process are summarized in pseudo-code in Algorithm 1.\nThe network structure and parameter of agent have been described in Table 2 to make\nit easier to understand and implementation.\nDuring the training process, the MS-SAC model learns a stochastic policy that maxi-\nmizes the expected reward while also maximizing entropy, leading to a more diverse set\nof actions and better exploration. The model also learns a Q-function that estimates the\nstate-action value and can be used to guide the policy towards more optimal actions. The\nuse of a replay buffer and target networks helps stabilize the learning process and prevent\noverﬁtting to recent experiences. With proper design and training, MS-SAC performs\nwell with high-dimension data. Since there are many tunable knobs in a database with\ncontinuous space, MS-SAC is suitable for database tuning problems.\nTable 2. Network and parameters of agent.\nActor Layer Actor Param Critic Layer Critic Param\nInput 33 Input (Number of knobs) + 33\nFull connection 128 Full connection 128\nActivation function ReLU Activation function ReLU\nFull connection 128 Full connection 128\nActivation function ReLU Activation function ReLU\nFull connection 64 Full connection 64\nOutput Number of knobs Data 1\n\nElectronics 2023 ,12, 2168 9 of 22\nAlgorithm 1 Train Agent\nInput: initial actor parameters q, critic parameters w1,w2, empty replay buffer D.\nlet target parameters equal to main parameters wtarget,1 f1,wtarget,2 w2.\nwhile !converged do\nobserve the initial state of the environment s1.\nexecute atand get reward rt\nthe state of environment change to st+1\nstore (st,at,rt,st+1)inD\nfortime = 1 to ¥do\nselect an action based on policy: at=p(q)st.\nifready to update then\nfork = 1 to Kdo\ngetNsamples fromD:f(si,ai,ri,si+1)gi=1,2,..., N\nfor each sample, compute target for the Q-functions:\nyi(r,s0,d) =ri+g\u0012\nmin\ni=1,2Qwtarg, i(si+1,ai+1)\u0000alogpq(ai+1jsi+1)\u0013\n,\nai+1\u0018pq(\u0001jsi+1)\nupdate critic to minimizing the loss function:\nrwi1\nNN\nå\ni=1(yi\u0000Qw(si,ai))2,f or i=1, 2\nupdate actor by gradient ascent using:\nrq=1\nNN\nå\ni=1\u0010\nalogpq(˜aijsi)\u0000min\nj=1,2Qwtarg,j(si,˜ai)\u0011\nupdate target network:\nwtarg, i rwtarg, i+ (1\u0000r)wi,f or i=1, 2\nend for\nend if\nend for\nend while\n5. Environment Proxy\nEarlier studies on reinforcement learning have perceived databases as an\nenvironment [2,3,10,15] and agent update according to the feedback from environment.\nHowever, in database tuning problems, the agent requires a non-trivial amount of time for\naction evaluation. Moreover, RL employs trial-and-error to devise a solution to the tuning\nproblem, which means actions from the agent may lead to performance degradation or\ndatabase crash. To address these problems, Environment Proxy has been suggested to act\nas an interface between the agent and the environment, adding stability and safety. The\nenvironment proxy contains the Filter, Virtual Environment, and Dispatcher. The environ-\nment (i.e., the database that needs to be tuned) is wrapped with an environment proxy.\nInstead of deploying conﬁgurations to the environment directly, a combined approach is\nused to check conﬁgurations. Filter drops dangerous conﬁgurations to bring stability to the\ndatabase. Proxy provides a more efﬁcient evaluation of conﬁgurations using a Dispatcher\nand Virtual Environment. Dispatcher categorizes the conﬁgurations as “uncommon action”\nif the evaluation of the action is hard to estimate. On the contrary, the “common action”\nmeans the Virtual Environment can estimate an evaluation according to historical data. The\nproxy replaces the original position of environment in RL, and the interior of proxy is a\nblack box for agent.\n\nElectronics 2023 ,12, 2168 10 of 22\n5.1. Filter\nThe Filter adopts a rule-based and learning-based method to exclude dangerous con-\nﬁgurations. If the conﬁgurations are considered dangerous, Filter notices environment\nproxy to discard it and return a negative reward to agent as punishment. To judge the\naction, FASTune ﬁrst extracts ﬁxed rules from documents (e.g., the manual) and adds\nthem to the rules library. The action matches any rule in the library is considered dan-\ngerous. For example, in MySQL, if com_stmt _prepare\u0000com_stmt _close is greater than\nmax_prepared _stmt _count , it may cause database failure because database cannot create\nmore than max_prepared _stmt _count statements. Extracting rules from documents can be\nburdensome. Fortunately, DB-BERT [11] brings some light to this problem.\nDB-BERT utilizes the BERT for document analysis, BERT (Bidirectional Encoder Rep-\nresentations from Transformers) is a transformer-based deep learning model. It is one of\nthe most popular and powerful models for natural language processing tasks, including\ntext classiﬁcation, question answering, and text generation. BERT is pre-trained on a large\namount of text data, using two unsupervised tasks: masked language modeling (MLM) and\nnext sentence prediction (NSP). This pre-training approach allows BERT to learn various\nlanguage tasks without requiring task-speciﬁc training data. That makes BERT a highly\nﬂexible and versatile NLP model that can be ﬁne-tuned for a wide range of NLP tasks with\nstate-of-the-art performance. DB-BERT is an extension of the BERT model that specializes\nin extracting explicit and implicit parameter references from the text. It does so by com-\nparing the BERT encoding of the text with those of the DBMS parameter names, selecting\nthe parameter with the smallest cosine distance. That allows DB-BERT to pair extracted\nvalues with parameters that are explicitly mentioned or are similar to the text. Additionally,\nDB-BERT can translate tuning hints into arithmetic formulas using a sequence of decisions\nand reinforcement learning.FASTune uses DB-BERT to exploit rules from a document and\nfeed these rules to ﬁlter.\nUsing rules to exclude dangerous action is effective, but rules can not explore potential\nrelations between knobs and the database performance. Thus, FASTune utilizes a classiﬁer\nbased on Support Vector Machines (SVM) [ 55,56]. SVM is one of the most robust binary\nclassiﬁer models with a wide range of applications in several ﬁelds. SVMs aim to ﬁnd the\nbest possible decision boundary that separates two or more classes of data(e.g., dangerous\nand safe). SVM maximize the margin, or the distance between the hyperplane and the\nclosest data points from both classes, and use the closest data points, called support vectors,\nto determine the hyperplane parameters. Once the hyperplane is determined, the SVM can\nclassify new data points based on which side of the hyperplane they fall. A newly collected\ndata point is classiﬁed as belonging to one or the other class, based on which side of the\nhyperplane it falls into. SVMs can handle complex data sets with high dimensions and\nnonlinear boundaries using kernel functions.\nFASTune adopts SVM to divide the actions into dangerous and safe groups. FASTune\nmaps each action to a vector, representing a point in the high-dimension coordinate system.\nThen FASTune uses SVM to ﬁnd a hyperplane to split these points into two groups. Since\nthese points are not linearly separable, we use Radial Basis Function (RBF) kernel [ 57,58]\nto map them to higher dimensional spaces.The RBF kernel function is a powerful tool for\nSVMs because it can handle complex, nonlinear relationships between input variables. By\ntransforming the input data into a higher-dimensional feature space using the RBF kernel,\nSVMs can ﬁnd decision boundaries that are highly ﬂexible and can accurately classify data\npoints that are not linearly separable in the original feature space.\nThere are two important parameters in RBF: gand c. Parameter cmakes a trade-off\nbetween the misclassiﬁcation of examples and the simplicity of hyperplane. A higher\nvalue of cincreases the correctness of classiﬁcation, while a lower cmakes the hyperplane\nsmooth. The gdetermines how inﬂuential an example can be. The greater the value of g\nis, the more it affects the neighboring data points. Choosing the right gand cis vital for\nthe performance of SVM. To address this problem, FASTune adopts the exhaustive grid\nsearch method that tries all combinations of candgfrom a grid of them. Then, each cand\n\nElectronics 2023 ,12, 2168 11 of 22\ngcombination is evaluated by k-fold cross-validation. Finally, we get the best combination\nofcandg.\nNote that the classiﬁer we introduced above needs labeled training data. To collect\ntraining data, conﬁgurations are generated using uniform random distribution, and each set\nof conﬁgurations is deployed to the database. The conﬁgurations are labeled as dangerous\nif database performance drops sharply or fails. Otherwise, FASTune marks it as safe.\nCollecting training data can be a hard task. In future work, We would like to explore more\nefﬁcient methods to generate training data and open the source code and dataset.\n5.2. Dispatcher\nIntuitively, if the action is similar to the previous (i.e., common action), it is possible\nto predict the performance changes in a database; thus, it is not necessary to deploy the\naction to the database and run a time-consuming stress test. On the contrary, If an action\nis rarely seen or never seen before (i.e., uncommon action), it will be hard to predict its\nimpact on database performance, so the Dispatcher will send the action to the real database\n(i.e., environment) to see what happens. The main challenge for Dispatcher is to deﬁne\nwhat is uncommon action. Fortunately, ODT (outlier detection technique) can address the\nchallenge effectively. Outlier detection has been studied for decades and is widely used\nin various ﬁelds. Outlier detection is used to identify data points or observations that are\nsigniﬁcantly different from the other data points in a dataset. There are several methods\nfor outlier detection, including statistical methods, distance-based methods, and machine\nlearning methods. In the context of FASTune, uncommon actions are considered outliers,\nand a distance-based method is used to identify them. Outlier detection relies on the idea\nthat uncommon actions are likely to be located far away from the other data points in\na dataset. The Mahalanobis distance is used to measure the distance between each data\npoint. Data points signiﬁcantly far away from most data points are identiﬁed as outliers. To\ndetermine where most data points are located, gaussian distribution is introduced, where\nthe majority of data points are located close to the mean, and the frequency of data points\ndecreases as they move away from the mean.Therefore, data points signiﬁcantly far away\nfrom the mean are likelier to be outliers.\nFASTune performs outlier detection by assuming that the regular data come from\na gaussian distribution. From this assumption, our work deﬁnes outliers that stand far\nenough from the gaussian distribution. For gaussian distribution, the distance of a sample\nxito the distribution can be computed using Mahalanobis distance. Mahalanobis distance\nmeasures the distance between a point and a distribution [ 59]. It performs well in the\nmultivariate outlier detection task. The classical Mahalanobis distance of sample xis\ndeﬁned as:\nd(x,m,S) =q\n(x\u0000m)0S\u00001(x\u0000m) (11)\nSandmare covariance and location of the gaussian distribution, respectively, and\nthey must be estimated among the speciﬁc data. FASTune uses The Minimum Covariance\nDeterminant (MCD) [ 60,61] estimator to estimate Sandmfor its simplicity and ease of\ncomputing. The MCD was introduced by P .J.Rousseuw [ 61]. MCD is a robust estimator of\ncovariance. The basic idea of MCD is to identify a subset of observations in a dataset with\nthe smallest determinant of their covariance matrix, which is less sensitive to the inﬂuence\nof outliers. The MCD estimator is useful for producing reliable estimates of the parameters\nof the distribution in datasets. According to the deﬁnition of MCD-based Mahalanobis\ndistance, the uncommon action detection is described in Algorithm 2.\n\nElectronics 2023 ,12, 2168 12 of 22\nAlgorithm 2 Uncommon Action Detection\nExecute Initialization to collect enough samples.\nInitialize P: an empty points collection.\nwhile Agent recommends conﬁgurations do\nMap conﬁgurations to a point p.\nEstimate corresponding sandmof gaussian distribution using MCD.\nCompute tolerance ellipse with Mahalanobis distance.\nifthe position of the sample is within tolerance ellipse then\nLable the new sample as an outlier and send it to Environment.\nelse\nSent it to Virtual Environment.\nend if\nAdd the point to the collection: P=P[p.\nend while\nThe Detection process can be summarized as follows:\n5.2.1. Collect Samples\nTo estimate the Sandmof the gaussian distribution of the samples, FASTune ﬁrst\ncollects actions generated by an agent as initial samples. Let Lrepresent the number of\nsamples. The agent interacts with a database Ltimes, producing Lnumber of samples. All\nactions are considered inliers at this stage because the number of samples has not reached\nthe given threshold, and detection is unavailable. Without enough samples, it is hard to\ndistinguish between an inlier and an outlier. Once Lreaches the manually set threshold,\nthis stage ends. Note that the threshold is a hyper-parameter.\n5.2.2. Estimate the Distribution\nMap the action to an n-dimension multivariate A= (a1,a2,a3. . .an), where airepre-\nsents the value of the i-th conﬁgurations of the database. Then estimate the location of\nthe gaussian distribution based on existing samples, speciﬁcally, estimates the Sandmof\ngaussian distribution using samples collected in the last stage.\n5.2.3. Set a Threshold\nA threshold is set to identify data points that are signiﬁcantly far away from the other\ndata points. FASTune computes the tolerance ellipse with a 97.5 percentile point. The 97.5%\ntolerance ellipse is deﬁned as:\nRD(x) =q\n(x\u0000ˆmMCD)tˆS\u00001\nMCD(x\u0000ˆm) =q\nc2\nr,0.975(12)\n[c2\nr,a]stands for the a-quantile of the [c2\nr]distribution. Here, variable ˆS\u00001\nMCDis the\nMCD estimated location, and the point outside the ellipse is considered an outlier.\n5.2.4. Identify\nData points that are above the threshold (i.e., outside the ellipse) are identiﬁed as\noutliers and are considered to be signiﬁcantly different from the other data points in the\ndataset. Note that, The Accurate MCD estimator is hard to compute since it requires the\nevaluation of all subsets [ 61]. The FAST-MCD [ 62] algorithm is borrowed to improve the\nestimator’s speed. Figure 3 shows the outlier detection results. There are 71 dimensions in\naction and we choose two for visualization.\n\nElectronics 2023 ,12, 2168 13 of 22\nFigure 3. Results of outlier detection on two dimensions, k1denotes table _open _cache and k2denotes\ninnodb _adaptive _max_sleep _delay .\nThe results show that the actions generated by the agent tend to be gaussian distributed\nwhen the number of iterations is large enough. Actions inside the tolerance ellipse (the red\npoints) are considered common actions dominating the majority. Note that the Dispatcher\ndoes not split the action; Dispatcher sends the whole action to the environment or the\nvirtual environment for each interaction.\n5.2.5. Update\nTo increase the accuracy of detection, the new action from an agent in each iteration\nwill be added to the samples collection, so the estimate of Sandmare also updated as\nthe collection is updated. In summary, outlier detection enables Dispatcher to distinguish\nbetween common and uncommon actions, and the Dispatcher distributes common actions\nto the virtual environment and uncommon actions to the environment. The sample in\nthe collection can also be used to train the virtual environment, and more details will be\ndiscussed below.\nThis Distance-based method can be computationally expensive for calculating the\ndistance. However, these methods can be effective at identifying outliers that are located\nfar away from the other data points.\n5.3. Virtual Environment\nIn the reinforcement learning framework, the update of agent depends on the reward.\nSpeciﬁcally, in database tuning problems, the existing methods calculate the reward by\nrunning a stress test on database, which is rather time-consuming. The tuning time of\nthe state-of-the-art approach for optimal conﬁgurations can take hours, which can be the\nbottleneck for tuning efﬁciency. CDBTune and Hunter [ 3,7] use cloned databases to make\nthe evaluation parallelization to reduce tuning time efﬁciently; however, it also brings\na heavy burden because each database instance requires additional threads, disk space,\nand memory.\nTo address this challenge, we proposed a virtual environment which is an equiva-\nlent but faster approach. The virtual environment and the environment are essentially\ndifferent; virtual environment is a neural network that estimates the evaluation of action\nbased on historical data. It does not have the functionality of a real database instance\n(i.e., execute queries). The environment is the real database to be tuned, the environment\nruns a benchmark to evaluate the action, and the reward is calculated from the evaluation.\nIf the database performance is improved after deploying an action, the reward will be\npositive; otherwise, it will be negative depending on the extent of performance improve-\nment or reduction. For the reinforcement learning framework, virtual environment and\nenvironment play the same role in the training process and have the same input and output.\nIn FASTune, both virtual environment and environment are wrapped with a proxy, and\nproxy providing an interface for the agent to interact. We now describe how to train the\nvirtual environment.\n\nElectronics 2023 ,12, 2168 14 of 22\nTraining the Virtual Environment\nTraining data of the virtual environment is a collection of tuples Tv=<Sw,Sd,A,r>,\nwhere Swis a vector that represents the states of workload, Sdis the state of database, Ais\nthe action from agent, and ris the reward. For each <Sw,Sd,A>, the virtual environment\naims to output a value r0which is close to r. The training data can be obtained between\nthe environment and the agent in each trajectory. Given Sdand Sw, then the agent output\nA(action) and sent to environment proxy. Proxy returns calculated r(reward) based on\nan evaluation of A, and the rwill be recorded.The virtual environment is a multi-layer\nneural network model consisting of four fully connected layers.The input layer receives a\nvector that combines Sw,Sd, and Aand output a higher dimension tensor to the two hidden\nlayers, using a non-linear function to transform data. The output is a vector representing\nthroughput and latency. The neural network can be viewed as a chain of functions that\nconvert the input into an output pattern [ 63,64]. To prevent the network from solely\nlearning linear transformations, Tanh, a commonly used activation function in neural\nnetworks, is introduced into the hidden layers to capture more complex patterns. The\nnetwork’s weights are initialized using a standard normal distribution. Given a training\ndataset U=<W,S,A,r>, The objective of training is to minimize the loss function, which\nis deﬁned as follows:\nL=1\n2åU\ni=1jr0\u0000rj2(13)\nThe output value r0produced by the Virtual Environment for a given <W,S,A>. To\ntrain the Virtual Environment, The adam optimization algorithm [ 65] is introduced, which\nis a stochastic optimization algorithm that updates the network weights by computing\nthe ﬁrst and second moments of the gradients using a stochastic objective function. The\ntraining process is terminated when the model has converged or has reached a speciﬁed\nnumber of steps. The training process spend about 200 s on average and the details on\nvirtual environment are list in Table 3.\nTable 3. Details of Virtual Environment.\nItem Description\nDimensions of layers 33 !32!16!1\nTrain data size 4094\nTest data size 1000\nValid ratio 0.2\nNumber of epochs 3000\nBatch size 32\nLearning rate 1\u000210\u00005\nEarly stop 300\n6. Workload State\nSeveral studies have shown that workloads are sensitive to various knobs. As shown\nin Figure 4, the performance of different workloads using the same conﬁgurations is varied,\nand performance does not change linearly in any direction because knobs have non-linear\ncorrelations. Qtune encodes queries to capture the workload information to provide a\nquery-aware tuning system. Hunter bounces back quickly from throughput plummet when\nworkload drifts by learning from historical data. They perform well on given workloads but\nfail to handle the dynamic workload. It can also cause dramatic ﬂuctuations in performance\nif the database tries to apply conﬁgurations when the workload drifts. It is well-known\nthat workload may shift over time in a production environment, which poses a challenge to\nthe stability of the system. To address this challenge, we extract features from the predicted\nfuture workload that comes in a short time (e.g., one minute). Furthermore, feed these\nfeatures as workload state to an agent. Agent not only considers the state of database but\nalso the state of the workload when generating conﬁgurations.\n\nElectronics 2023 ,12, 2168 15 of 22\nFigure 4. Throughput on different workload types using consistent conﬁgurations, k1denotes\ninnodb _change _bu f f er _max_size, and k2 denotes table _open _cache .\nOur work builds a forecasting model to predict the types of queries and how many of\nthem will arrive in the database. The predicted results are used as workload state, which\nwill be sent to the agent as part of the multi-state. The agent can then use this data to\nﬁnd optimal conﬁgurations for the dynamic workload. Our approach ﬁrst encodes each\nquery into a vector and aggregates a batch of queries together to approximate the workload\npattern. Then similar patterns will be combined into several groups using a clustering\nalgorithm. Finally, models predict how many quires will arrive at each group. Note that\npredicting exact SQL (Structured Query Language) statements may lead to expensive\ncomputing, so our method forecasts the number of queries in each group.\n6.1. Encoder\nIn general, a SQL statement can be divided into four types: insert, delete, select,\nand update, different queries may involve different tables. Both query type and tables\nrelated to the query signiﬁcantly impact the database performance. For example, OLAP\n(Online Analytical Processing) usually involves large numbers of records, while OLTP only\ninvolves a few records and executes simple updates, insertions, and deletions in databases.\nTables with different structures and sizes also affect the database performance. FASTune\ncapture that information in the vector. The Encoder ﬁrst extracts the template (i.e., prepared\nstatements) from queries. FASTune uses DBMS’s SQL parser (e.g., PostgreSQL-parser) to\nmap SQL statements to an abstract syntax tree to get a standard query template. Encoder\ncounts the number of queries in an established time interval and saves the ﬁnal result at the\nend of each time interval. The time interval can be a hyper-parameter. Too short an interval\ncan lead to expensive calculations and fewer performance gains. Conversely, If the time\ninterval is too long, it will be difﬁcult to promptly detect workload drift. FASTune makes a\ntrade-off between speed and accuracy and manually sets the time interval to 10 s. We will\nleave choosing the time interval automatically for future work.\n6.2. Cluster\nAlthough Query Encoder decreases queries by converting queries to templates, it is\nstill a heavy burden to predict how many and what kind of templates will arrive in the\nfuture. Thus, FASTune further clusters similar templates into a group to reduce the number\nof those templates. After gaining the template of queries, many algorithms can cluster\nthe templates. We chose DBSCAN [ 66] and made some improvements to it. Compared\nto the original DBSCAN algorithm, our approach made a trade-off between accurate and\ncomputational costs by setting a threshold t. The threshold tdetermined how similar\nthe templates must be to be in the same group. Higher tmeans more templates will be\nclustered together so the result can be more precise, yet it will lead to longer computational\ntime. We map each template to a point and use DBSCAN to group these points close to\nnearby neighbors according to the distance measurement.\n\nElectronics 2023 ,12, 2168 16 of 22\n6.3. Forecaster\nThe Encoder and Cluster convert SQL statements to templates and cluster templates\ninto groups. The forecaster predicts the arrival rate of each group’s queries. The forecaster\naims to predict queries in a near-term (e.g., 10 s) so that the agent can take future workload\ninto account when recommending conﬁgurations. Linear models earn our trust since they\nconsume fewer computing resources, require fewer samples, and usually perform well in\nthe near term predicting [67].\n7. Evaluation\n7.1. Evaluation of Efﬁciency\nIn this section, we compare FASTune with state-of-the-art methods [ 2,3,7,15] on Open-\nGauss and MySQL, and the method we compared are listed below:\n1. QTune is a query-aware tuning system that supports three database tuning granulari-\nties [12]. The evaluation uses its workload-level tuning;\n2. CDBTune adopts a reinforcement learning model to tune the database [ 9]. The agent\ninputs internal metrics of the database and outputs optimal conﬁgurations;\n3. ResTune uses Bayesian Optimization to optimize resource utilization with\nconstraints [15]. It uses a meta-learning approach to extract useful knowledge from\nhistorical experiences;\n4. HUNTER designed a hybrid tuning model that uses samples from a genetic algo-\nrithm to accelerate the exploration of deep reinforcement learning [ 10]. Meanwhile,\nHUNTER uses Random Forest, Principal Component Analysis, and Fast Exploration\nStrategy to reduce the action space.\nDetails of the hardware are listed in Table 4.\nTable 4. Details of hardware used in the evaluation.\nDBMS CPU RAM Disk Speed Disk Latency\nOpenGauss 4 core 16 GB 220 Mb/s 12.3 ms\nMySQL 4 core 16 GB 220 Mb/s 12.3 ms\nBoth MySQL and OpenGauss run on a server with a 4 core CPU, 16 GB RAM, and a\n200 GB disk. A virtual machine (VM) is deployed to keep the experimental environment\nconsistent. We ﬁrst create a snapshot that records the initial state of the operating system\n(OS) and hardware. At each evaluation, the virtual machine rolls back to a snapshot to\nprovide the same experimental conditions. A static workload (e.g., TPC-C and Sysbench) is\nused to evaluate FASTune and compare it to the methods mentioned above. The evaluation\nis based on two dimensions: tuning time and database performance improvement. In\nSection 7.2, a dynamic workload is used to evaluate the stability of FASTune. We count the\nnumber of dangerous conﬁgurations and database ﬂuctuations during the tuning process.\nFigure 5 shows the results of Sysbench and TPC-C running on MySQL and openGauss.\nFigure 5 illustrates the similarity between the tuning results of MySQL and openGauss.\nIn comparison to other tuning systems, FASTune achieves optimal performance in a shorter\nduration. By ﬁnding the optimal conﬁgurations within 3 h, FAStune signiﬁcantly reduces\nthe tuning time. It is worth noting that all methods resulted in a TPC-C throughput of\n6% to 18% higher on openGauss which could be due to database version limits. The\nexperiment results depict that the proposed model enhances database performance by\n1–1.5 times within 5 h, requiring substantially lower time (60–80%) for optimal performance\nattainment as compared to FastTune conventional models. During the TPC-C test, the\ndatabase platform was able to achieve a throughput of up to 6000 txn/s, while openGauss\nwas able to secure up to 7800 txn/s. The higher throughput on openGauss may have\nresulted from inherent database variations. Due to databases’ complex and random nature,\nﬂuctuations may occur after the intelligent agent’s convergence, as observed in Figure 5a,\nwhere performance often oscillates between 2500 and 3000.\n\nElectronics 2023 ,12, 2168 17 of 22\nFigure 5. Performance Evaluation—Comparisons of state-of-the-art database tuning systems,\n(a–d) demonstrate throughput of MySQL on Sysbench and TPC-C. ( e–h) demonstrate throughput of\nOpenGauss on Sysbench and TPC-C.\nFigure 6 shows the count of conﬁgurations that lead to a sharp performance decline\n(#Dangerous) and database failure (#Failure) during the tuning process.\nFigure 6. Dangerous conﬁgurations statistics during the tuning process. ( a–c) show the number of\ndangerous and failure conﬁgurations under workloads from Bank, Wiki, and TPC-C on openGauss.\nThe state-of-the-art methods have 59 to 349 dangerous conﬁgurations within 1000 tun-\ning intervals. Instead, the dangerous conﬁgurations occurred less than ten times. FASTune\nachieves this by using an action ﬁlter to evaluate and discards dangerous conﬁgurations di-\nrectly. To sum up, FASTune can recommend optimal conﬁgurations and keep the database\nstable during the tuning. The best result of FASTune is slightly better than other methods,\nbut the time used is much less.\n7.2. Evaluation of Dynamic Workload\nIn this section, the dynamic workloads are used to evaluate the stability of the tuning\nsystem, and BenchBase is used to construct dynamic workloads by switching transaction\ntypes, BenchBase is a SQL benchmark framework. The workload shift between Sysbench\n(RW) and Sysbench (RO); Sysbench (RO) is an OLTP workload that contains heavy write\nqueries, and Sysbench (RW) is a mixed workload with reading and writing. Workload\nstarts with Sysbench (RO) and continues for hours, then switches to Sysbench (RW) and\ncontinues for hours. Note that our work train workload forecasters for FASTune before\nevaluation. Figure 7 shows the throughput of database.\n\nElectronics 2023 ,12, 2168 18 of 22\nFigure 7. The changes of throughput with workload drift—Comparisons of state-of-the-art\ntuning systems.\nWorkload ﬁrst holds on Sysbench (RO) for one hour, and then switches to Sysbench\n(RW). The workload drift causes a performance degradation, and all throughput drops\nbelow 1200 txn/s except for FASTune. FASTune suffers the least from the drift because FAS-\nTune predicts workload drift and adjusts in advance. Note that FASTune has experienced\na slight performance degradation as a by-product ahead of the drift. Another potential\nadvantage of FASTune is that FASTune can provide warning before workload drift arrives.\nWe would like to implement this feature in future work.\n8. Conclusions\nThe paper introduces a novel tuning system called FASTune, which is designed to\nrecommend optimal database conﬁgurations that can enhance database performance. FAS-\nTune employs soft actor–critic networks as an agent to achieve fast and stable exploration.\nEnvironment Proxy has been proposed to provide a gateway between the agent and envi-\nronment, preventing dangerous action arising from the agent. Additionally, FASTune uses a\nvirtual environment to enhance the effectiveness of evaluating conﬁgurations. To maintain\nstability, a workload forecaster based on machine learning is proposed, and the expected\nqueries are fed to the agent to handle the dynamic workload. The experimental results\nshow that FASTune can ﬁnd optimal database conﬁgurations to improve performance\nwhile maintaining stability in the tuning. However, FASTune requires collecting workloads\nin users’ environment, which may result in data privacy issues, and the training of the\nforecaster incurs a non-negligible extra cost. We plan to explore more effective approaches\nto better support dynamic workload tuning.\nAuthor Contributions: Conceptualization, T.L. and C.L.; Data curation, T.L.; Formal analysis, T.L.\nand C.L.; Funding acquisition, L.S., L.W., Y.T., C.L. and Y.G.; Investigation, T.L.; Methodology, T.L.;\nProject administration, L.S., C.L. and Y.G.; Resources, L.S., L.W. and Y.T.; Software, T.L.; Supervision,\nL.S., C.L. and Y.G.; Validation, T.L.; Visualization, T.L.; Writing—original draft, T.L.; Writing—review\nand editing, L.S. and C.L. All authors have read and agreed to the published version of the manuscript.\nFunding: This work was supported in part by the the National Key Technologies R&D Program\n(2018YFB1701401), Key Project of Public Beneﬁt in Henan Province of China (201300210500), Nature\nScience Foundation of China (62006210, 62001284, 62206252), Key Scientiﬁc Research Projects of\nColleges and Universities in Henan Province (23A520015), Key Project of Collaborative Innovation in\nNanyang (22XTCX12001), Key Technology Project of Henan Province of China (221100210100), and\nResearch Foundation for Advanced Talents of Zhengzhou University (32340306).\nData Availability Statement: Not applicable.\nConﬂicts of Interest: The authors declare no conﬂict of interest.\n\nElectronics 2023 ,12, 2168 19 of 22\nAbbreviations\nThe following abbreviations are used in this manuscript:\nDBMS Database Management System\nRL Reinforcement Learning\nDBAs Database Administrators\nOLTP Online Transaction Processing\nOLAP Online Analytical Processing\nMS-SAC Multi-State Soft Actor–Critic\nML Machine Learning\nRBF Radial Basis Function\nSAC Soft Actor–Critic\nDDGP Deep Deterministic Policy Gradient\nSVM Support Vector Machine\nMCD Minimum Covariance Determinant\nMLE Maximum Likelihood Estimate\nRO Read Only\nRW Read Write\nReferences\n1. Belknap, P .; Dageville, B.; Dias, K.; Yagoub, K. Self-Tuning for SQL Performance in Oracle Database 11g. In Proceedings of the\n2009 IEEE 25th International Conference on Data Engineering, Shanghai, China, 29 March–2 April 2009; pp. 1694–1700. [CrossRef]\n2. Li, G.; Zhou, X.; Li, S.; Gao, B. QTune: A Query-Aware Database Tuning System with Deep Reinforcement Learning. Proc. VLDB\nEndow. 2019 ,12, 2118–2130. [CrossRef]\n3. Cai, B.; Liu, Y.; Zhang, C.; Zhang, G.; Zhou, K.; Liu, L.; Li, C.; Cheng, B.; Yang, J.; Xing, J. HUNTER: An Online Cloud Database\nHybrid Tuning System for Personalized Requirements. In Proceedings of the 2022 International Conference on Management of\nData. ACM, Philadelphia, PA, USA, 12–17 June 2022; pp. 646–659. [CrossRef]\n4. Zhu, Y.; Liu, J.; Guo, M.; Bao, Y.; Ma, W.; Liu, Z.; Song, K.; Yang, Y. BestConﬁg: Tapping the Performance Potential of Systems via\nAutomatic Conﬁguration Tuning. In Proceedings of the 2017 Symposium on Cloud Computing. Association for Computing\nMachinery, SoCC ’17, Santa Clara, CA, USA, 24–27 September 2017; pp. 338–350. [CrossRef]\n5. Marco, A.; Berkenkamp, F.; Hennig, P .; Schoellig, A.P .; Krause, A.; Schaal, S.; Trimpe, S. Virtual vs. Real: Trading off Simulations\nand Physical Experiments in Reinforcement Learning with Bayesian Optimization. In Proceedings of the 2017 IEEE International\nConference on Robotics and Automation (ICRA), Singapore, 29 May–3 June 2017; pp. 1557–1563. [CrossRef]\n6. Wei, Z.; Ding, Z.; Hu, J. Self-Tuning Performance of Database Systems Based on Fuzzy Rules. In Proceedings of the 2014 11th\nInternational Conference on Fuzzy Systems and Knowledge Discovery (FSKD), Xiamen, China, 19–21 August 2014; pp. 194–198.\n[CrossRef]\n7. Zhang, J.; Zhou, K.; Li, G.; Liu, Y.; Xie, M.; Cheng, B.; Xing, J. CDBTune+: An Efﬁcient Deep Reinforcement Learning-Based\nAutomatic Cloud Database Tuning System. VLDB J. 2021 ,30, 959–987. [CrossRef]\n8. Van Aken, D.; Pavlo, A.; Zhang, B.; Gordon, G.J. Automatic Database Management System Tuning Through Large-scale Machine\nLearning. In Proceedings of the 2017 ACM International Conference on Management of Data, ACM, Chicago, IL, USA, 14–19\nMay 2017; pp. 1009–1024. [CrossRef]\n9. Zhang, X.; Wu, H.; Li, Y.; Tan, J.; Li, F.; Cui, B. Towards Dynamic and Safe Conﬁguration Tuning for Cloud Databases. In\nProceedings of the 2022 International Conference on Management of Data, ACM, Philadelphia, PA, USA, 12–17 June 2022;\npp. 631–645. [CrossRef]\n10. Zhang, J.; Liu, Y.; Zhou, K.; Li, G.; Xiao, Z.; Cheng, B.; Xing, J.; Wang, Y.; Cheng, T.; Liu, L.; et al. An End-to-End Automatic\nCloud Database Tuning System Using Deep Reinforcement Learning. In Proceedings of the 2019 International Conference on\nManagement of Data, ACM, Amsterdam, The Netherlands, 30 June–5 July 2019; pp. 415–432. [CrossRef]\n11. Trummer, I. DB-BERT: A Database Tuning Tool That “Reads the Manual”. In Proceedings of the 2022 International Conference on\nManagement of Data. Association for Computing Machinery, SIGMOD ’22, Philadelphia, PA, USA, 12–17 June 2022; pp. 190–203.\n[CrossRef]\n12. Duan, S.; Thummala, V .; Babu, S. Tuning Database Conﬁguration Parameters with iTuned. Proc. VLDB Endow. 2009 ,2, 1246–1257.\n[CrossRef]\n13. Francois-Lavet, V .; Henderson, P .; Islam, S.; Bellemare, M.G.; Pineau, J. An Introduction to Deep Reinforcement Learning. Found.\nTrends® Mach. Learn. 2018 ,11, 219–354. [CrossRef]\n14. Sutton, R.S.; Barto, A.G. Reinforcement Learning: An Introduction , 2nd ed.; Adaptive Computation and Machine Learning Series;\nThe MIT Press: Cambridge, MA, USA, 2018.\n15. Zhang, X.; Wu, H.; Chang, Z.; Jin, S.; Tan, J.; Li, F.; Zhang, T.; Cui, B. ResTune: Resource Oriented Tuning Boosted by Meta-\nLearning for Cloud Databases. In Proceedings of the 2021 International Conference on Management of Data, Virtual Event, 20–25\nJune 2021; pp. 2102–2114. [CrossRef]\n\nElectronics 2023 ,12, 2168 20 of 22\n16. Basu, D.; Lin, Q.; Chen, W.; Vo, H.T.; Yuan, Z.; Senellart, P .; Bressan, S. Regularized Cost-Model Oblivious Database Tuning with\nReinforcement Learning. In Transactions on Large-Scale Data- and Knowledge-Centered Systems XXVIII: Special Issue on Database- and\nExpert-Systems Applications ; Hameurlain, A., Küng, J., Wagner, R., Chen, Q., Eds.; Lecture Notes in Computer Science; Springer:\nBerlin/Heidelberg, Germany, 2016; pp. 96–132. [CrossRef]\n17. Gelbart, M.A.; Snoek, J.; Adams, R.P . Bayesian Optimization with Unknown Constraints. arXiv 2014 , arXiv:1403.5607. [CrossRef]\n18. Berkenkamp, F.; Krause, A.; Schoellig, A.P . Bayesian Optimization with Safety Constraints: Safe and Automatic Parameter Tuning\nin Robotics. arXiv 2020 , arXiv:cs/1602.04450. [CrossRef]\n19. Sui, Y.; Gotovos, A.; Burdick, J.; Krause, A. Safe Exploration for Optimization with Gaussian Processes. In Proceedings of the\n32nd International Conference on Machine Learning. PMLR, Lille, France, 6–11 July 2015; pp. 997–1005.\n20. Zolaktaf, Z.; Milani, M.; Pottinger, R. Facilitating SQL Query Composition and Analysis. In Proceedings of the 2020 ACM\nSIGMOD International Conference on Management of Data. Association for Computing Machinery, SIGMOD ’20, Portland, OR,\nUSA, 14–19 June 2020; pp. 209–224. [CrossRef]\n21. Liberty, E.; Karnin, Z.; Xiang, B.; Rouesnel, L.; Coskun, B.; Nallapati, R.; Delgado, J.; Sadoughi, A.; Astashonok, Y.; Das, P .; et al.\nElastic Machine Learning Algorithms in Amazon SageMaker. In Proceedings of the 2020 ACM SIGMOD International Conference\non Management of Data. Association for Computing Machinery, SIGMOD ’20, Portland, OR, USA, 14–19 June 2020; pp. 731–737.\n[CrossRef]\n22. Tan, J.; Nayman, N.; Wang, M. CobBO: Coordinate Backoff Bayesian Optimization with Two-Stage Kernels. arXiv 2022 ,\narXiv:2101.05147. [CrossRef]\n23. Mockus, J. Global Optimization and the Bayesian Approach. In Bayesian Approach to Global Optimization: Theory and Applications ;\nMockus, J., Ed.; Mathematics and Its Applications; Springer: Cham, The Netherlands, 1989; pp. 1–3. [CrossRef]\n24. Tan, J.; Zhang, T.; Li, F.; Chen, J.; Zheng, Q.; Zhang, P .; Qiao, H.; Shi, Y.; Cao, W.; Zhang, R. iBTune: Individualized Buffer Tuning\nfor Large-Scale Cloud Databases. Proc. VLDB Endow. 2019 ,12, 1221–1234. [CrossRef]\n25. Yan, J.; Jin, Q.; Jain, S.; Viglas, S.D.; Lee, A. Snowtrail: Testing with Production Queries on a Cloud Database. In Proceedings of\nthe Workshop on Testing Database Systems, DBTest’18, Houston, TX, USA, 15 June 2018; Association for Computing Machinery:\nNew York, NY, USA, 2018; pp. 1–6. [CrossRef]\n26. Liu, J.; Zhang, C. Distributed Learning Systems with First-order Methods. arXiv 2021 , arXiv:2104.05245. [CrossRef]\n27. Galakatos, A.; Markovitch, M.; Binnig, C.; Fonseca, R.; Kraska, T. FITing-Tree: A Data-aware Index Structure. In Proceedings of\nthe 2019 International Conference on Management of Data, SIGMOD ’19, Amsterdam, The Netherlands, 30 June–5 July 2019;\nAssociation for Computing Machinery: New York, NY, USA, 2019; pp. 1189–1206. [CrossRef]\n28. Kraska, T.; Beutel, A.; Chi, E.H.; Dean, J.; Polyzotis, N. The Case for Learned Index Structures. In Proceedings of the 2018\nInternational Conference on Management of Data, SIGMOD ’18, Houston, TX, USA, 10–15 June 2018; Association for Computing\nMachinery: New York, NY, USA, 2018; pp. 489–504. [CrossRef]\n29. Ma, L.; Van Aken, D.; Hefny, A.; Mezerhane, G.; Pavlo, A.; Gordon, G.J. Query-Based Workload Forecasting for Self-Driving\nDatabase Management Systems. In Proceedings of the 2018 International Conference on Management of Data, ACM, Houston,\nTX, USA, 10–15 June 2018; pp. 631–645. [CrossRef]\n30. Ma, L.; Zhang, W.; Jiao, J.; Wang, W.; Butrovich, M.; Lim, W.S.; Menon, P .; Pavlo, A. MB2: Decomposed Behavior Modeling\nfor Self-Driving Database Management Systems. In Proceedings of the 2021 International Conference on Management of Data,\nSIGMOD ’21, Virtual Event, 20–25 June 2021; Association for Computing Machinery: New York, NY, USA, 2021; pp. 1248–1261.\n[CrossRef]\n31. Sadri, Z.; Gruenwald, L.; Leal, E. Online Index Selection Using Deep Reinforcement Learning for a Cluster Database. In\nProceedings of the 2020 IEEE 36th International Conference on Data Engineering Workshops (ICDEW), Dallas, TX, USA, 20–24\nApril 2020; pp. 158–161. [CrossRef]\n32. Schnaitter, K.; Polyzotis, N. Semi-Automatic Index Tuning: Keeping DBAs in the Loop. arXiv 2010 , arXiv:1004.1249. [CrossRef]\n33. Van Aken, D.; Yang, D.; Brillard, S.; Fiorino, A.; Zhang, B.; Bilien, C.; Pavlo, A. An Inquiry into Machine Learning-Based\nAutomatic Conﬁguration Tuning Services on Real-World Database Management Systems. Proc. VLDB Endow. 2021 ,14, 1241–1253.\n[CrossRef]\n34. Kunjir, M.; Babu, S. Black or White? How to Develop an AutoTuner for Memory-based Analytics. In Proceedings of the 2020\nACM SIGMOD International Conference on Management of Data, SIGMOD ’20, Portland, OR, USA, 14–19 June 2020; Association\nfor Computing Machinery: New York, NY, USA, 2020; pp. 1667–1683. [CrossRef]\n35. Fekry, A.; Carata, L.; Pasquier, T.; Rice, A.; Hopper, A. Tuneful: An Online Signiﬁcance-Aware Conﬁguration Tuner for Big Data\nAnalytics. arXiv 2020 , arXiv:2001.08002. [CrossRef]\n36. Fekry, A.; Carata, L.; Pasquier, T.; Rice, A.; Hopper, A. To Tune or Not to Tune? In Search of Optimal Conﬁgurations for Data\nAnalytics. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD ’20,\nVirtual Event, 6–10 July 2020; Association for Computing Machinery: New York, NY, USA, 2020; pp. 2494–2504. [CrossRef]\n37. Storm, A.J.; Garcia-Arellano, C.; Lightstone, S.S.; Diao, Y.; Surendra, M. Adaptive Self-Tuning Memory in DB2. In Proceedings of\nthe 32nd International Conference on Very Large Data Bases, VLDB Endowment, VLDB ’06, Seoul, Republic of Korea, 12–15\nSeptember 2006; pp. 1081–1092.\n38. Tran, D.N.; Huynh, P .C.; Tay, Y.C.; Tung, A.K.H. A New Approach to Dynamic Self-Tuning of Database Buffers. ACM Trans.\nStorage (TOS) 2008 ,4, 3:1–3:25. [CrossRef]\n\nElectronics 2023 ,12, 2168 21 of 22\n39. Yoon, D.Y.; Niu, N.; Mozafari, B. DBSherlock: A Performance Diagnostic Tool for Transactional Databases. In Proceedings of the\n2016 International Conference on Management of Data, SIGMOD ’16, San Francisco, CA, USA, 26 June–1 July 2016; Association\nfor Computing Machinery: New York, NY, USA, 2016; pp. 1599–1614. [CrossRef]\n40. Kanellis, K.; Alagappan, R.; Venkataraman, S. Too Many Knobs to Tune? Towards Faster Database Tuning by Pre-Selecting\nImportant Knobs. In Proceedings of the 12th USENIX Conference on Hot Topics in Storage and File Systems HotStorage’20,\nVirtul, 13–14 July 2020; p. 16.\n41. Ni, Z.; He, H.; Zhao, D.; Prokhorov, D.V . Reinforcement Learning Control Based on Multi-Goal Representation Using Hierarchical\nHeuristic Dynamic Programming. In Proceedings of the The 2012 International Joint Conference on Neural Networks (IJCNN),\nBrisbane, Australia, 10–15 June 2012; pp. 1–8. [CrossRef]\n42. Nowé, A.; Brys, T. A Gentle Introduction to Reinforcement Learning. In Proceedings of the Scalable Uncertainty Management-\n10th International Conference, SUM 2016, Nice, France, 21–23 September 2016; Schockaert, S., Senellart, P ., Eds.; Springer:\nBerlin/Heidelberg, Germany, 2016; Volume 9858, pp. 18–32. [CrossRef]\n43. Mazyavkina, N.; Sviridov, S.; Ivanov, S.; Burnaev, E. Reinforcement Learning for Combinatorial Optimization: A Survey. Comput.\nOper. Res. 2021 ,134, 105400. [CrossRef]\n44. Shen, R.; Zhong, S.; Wen, X.; An, Q.; Zheng, R.; Li, Y.; Zhao, J. Multi-Agent Deep Reinforcement Learning Optimization\nFramework for Building Energy System with Renewable Energy. Appl. Energy 2022 ,312, 118724. [CrossRef]\n45. Deng, J.; Sierla, S.; Sun, J.; Vyatkin, V . Reinforcement Learning for Industrial Process Control: A Case Study in Flatness Control in\nSteel Industry. Comput. Ind. 2022 ,143, 103748. [CrossRef]\n46. He, Z.; Tran, K.P .; Thomassey, S.; Zeng, X.; Xu, J.; Yi, C. A Deep Reinforcement Learning Based Multi-Criteria Decision Support\nSystem for Optimizing Textile Chemical Process. Comput. Ind. 2021 ,125, 103373. [CrossRef]\n47. Zhang, H.; Peng, Q.; Zhang, J.; Gu, P . Planning for Automatic Product Assembly Using Reinforcement Learning. Comput. Ind.\n2021 ,130, 103471. [CrossRef]\n48. Mikhaylov, A.; Mazyavkina, N.S.; Salnikov, M.; Troﬁmov, I.; Qiang, F.; Burnaev, E. Learned Query Optimizers: Evaluation and\nImprovement. IEEE Access 2022 ,10, 75205–75218. [CrossRef]\n49. Kraska, T.; Alizadeh, M.; Beutel, A.; Chi, E.H.; Ding, J.; Kristo, A.; Leclerc, G.; Madden, S.R.; Mao, H.; Nathan, V . SageDB: A\nLearned Database System. Available online: https://dspace.mit.edu/handle/1721.1/132282 (accessed on 18 July 2022).\n50. Li, G.; Zhou, X.; Sun, J.; Yu, X.; Han, Y.; Jin, L.; Li, W.; Wang, T.; Li, S. openGauss: An Autonomous Database System. Proc. VLDB\nEndow. 2021 ,14, 3028–3042. [CrossRef]\n51. Haarnoja, T.; Zhou, A.; Hartikainen, K.; Tucker, G.; Ha, S.; Tan, J.; Kumar, V .; Zhu, H.; Gupta, A.; Abbeel, P .; et al. Soft Actor-Critic\nAlgorithms and Applications. arXiv 2019 , arXiv:1812.05905.\n52. Watkins, C.J.C.H.; Dayan, P . Q-Learning. Mach. Learn. 1992 ,8, 279–292. [CrossRef]\n53. Hester, T.; Vecerik, M.; Pietquin, O.; Lanctot, M.; Schaul, T.; Piot, B.; Horgan, D.; Quan, J.; Sendonaris, A.; Osband, I.; et al. Deep\nQ-learning from Demonstrations. In Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence and Thirtieth\nInnovative Applications of Artiﬁcial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artiﬁcial\nIntelligence, AAAI’18/IAAI’18/EAAI’18, New Orleans, LA, USA, 2–7 February 2018; pp. 3223–3230.\n54. Silver, D.; Lever, G.; Heess, N.; Degris, T.; Wierstra, D.; Riedmiller, M. Deterministic Policy Gradient Algorithms. In Proceedings\nof the 31st International Conference on International Conference on Machine Learning—Volume 32, JMLR.org, ICML’14, Beijing,\nChina, 21–26 June 2014; pp. I-387–I-395.\n55. Pisner, D.A.; Schnyer, D.M. Chapter 6—Support Vector Machine. In Machine Learning ; Mechelli, A., Vieira, S., Eds.; Academic\nPress: Cambridge, MA, USA, 2020; pp. 101–121. [CrossRef]\n56. Cervantes, J.; Garcia-Lamont, F.; Rodríguez-Mazahua, L.; Lopez, A. A Comprehensive Survey on Support V ector Machine Classifica-\ntion: Applications, Challenges and Trends. Neurocomputing 2020 ,408, 189–215. [CrossRef]\n57. Buhmann, M.D. Radial Basis Functions. Acta Numer. 2000 ,9, 1–38. [CrossRef]\n58. Scholkopf, B.; Sung, K.-K.; Burges, C.; Girosi, F.; Niyogi, P .; Poggio, T.; Vapnik, V . Comparing Support Vector Machines with\nGaussian Kernels to Radial Basis Function Classifiers. IEEE Trans. Signal Process 1997 ,45, 2758–2765. [CrossRef]\n59. Simpson, D.G. Introduction to Rousseeuw (1984) Least Median of Squares Regression. In Breakthroughs in Statistics ; Kotz, S.,\nJohnson, N.L., Eds.; Springer Series in Statistics; Springer: Berlin/Heidelberg, Germany, 1997; pp. 433–461. [CrossRef]\n60. Hubert, M.; Debruyne, M. Minimum Covariance Determinant. Wiley Interdiscip. Rev. Comput. Stat. 2010 ,2, 36–43. [CrossRef]\n61. Hubert, M.; Debruyne, M.; Rousseeuw, P .J. Minimum Covariance Determinant and Extensions. Wiley Interdiscip. Rev. Comput.\nStat. 2018 ,10, e1421. [CrossRef]\n62. Rousseeuw, P .J.; Driessen, K.V . A Fast Algorithm for the Minimum Covariance Determinant Estimator. Technometrics 1999 ,\n41, 212–223. [CrossRef]\n63. Dikaleh, S.; Xiao, D.; Felix, C.; Mistry, D.; Andrea, M. Introduction to Neural Networks. In Proceedings of the 27th Annual\nInternational Conference on Computer Science and Software Engineering CASCON ’17, Markham, ON, Canada, 6–8 November\n2017; p. 299.\n64. Abiodun, O.I.; Jantan, A.; Omolara, A.E.; Dada, K.V .; Mohamed, N.A.; Arshad, H. State-of-the-Art in Artiﬁcial Neural Network\nApplications: A Survey. Heliyon 2018 ,4, e00938. [CrossRef]\n65. Kingma, D.P .; Ba, J. Adam: A Method for Stochastic Optimization. In Proceedings of the 3rd International Conference on\nLearning Representations, ICLR 2015, San Diego, CA, USA, 7–9 May 2015; Bengio, Y., LeCun, Y., Eds.\n\nElectronics 2023 ,12, 2168 22 of 22\n66. Ester, M.; Kriegel, H.P .; Sander, J.; Xu, X. A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with\nNoise. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, KDD’96, Portland, OR,\nUSA, 2–4 August 1996; pp. 226–231.\n67. Akdere, M.; Çetintemel, U.; Riondato, M.; Upfal, E.; Zdonik, S.B. Learning-Based Query Performance Modeling and Prediction.\nIn Proceedings of the 2012 IEEE 28th International Conference on Data Engineering. IEEE Computer Society, ICDE ’12, Arlington,\nVA, USA, 1–5 April 2012; pp. 390–401. [CrossRef]\nDisclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.",
  "textLength": 74844
}