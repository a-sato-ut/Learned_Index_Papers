{
  "paperId": "3ec488c1a5da8fabdbc1745b6fb81621e635ca59",
  "title": "DBA bandits: Self-driving index tuning under ad-hoc, analytical workloads with safety guarantees",
  "pdfPath": "3ec488c1a5da8fabdbc1745b6fb81621e635ca59.pdf",
  "text": "DBA bandits: Self-driving index tuning under\nad-hoc, analytical workloads with safety guarantees\nR. Malinga Perera, Bastian Oetomo, Benjamin I. P. Rubinstein, Renata Borovica-Gajic\nfmalinga.perera, b.oetomo g@student.unimelb.edu.au, fbrubinstein, renata.borovica g@unimelb.edu.au\nSchool of Computing and Information Systems\nUniversity of Melbourne\nAbstract —Automating physical database design has remained\na long-term interest in database research due to substantial\nperformance gains afforded by optimised structures. Despite\nsigniﬁcant progress, a majority of today’s commercial solutions\nare highly manual, requiring ofﬂine invocation by database\nadministrators (DBAs) who are expected to identify and supply\nrepresentative training workloads. Even the latest advancements\nlike query stores provide only limited support for dynamic envi-\nronments. This status quo is untenable: identifying representative\nstatic workloads is no longer realistic; and physical design tools\nremain susceptible to the query optimiser’s cost misestimates\n(stemming from unrealistic assumptions such as attribute value\nindependence and uniformity of data distribution).\nWe propose a self-driving approach to online index selection\nthat eschews the DBA and query optimiser, and instead learns\nthe beneﬁts of viable structures through strategic exploration\nand direct performance observation. We view the problem as\none of sequential decision making under uncertainty, speciﬁcally\nwithin the bandit learning setting. Multi-armed bandits balance\nexploration and exploitation to provably guarantee average per-\nformance that converges to a ﬁxed policy that is optimal with\nperfect hindsight. Our comprehensive empirical results demon-\nstrate up to 75% speed-up on shifting and ad-hoc workloads and\n28% speed-up on static workloads compared against a state-of-\nthe-art commercial tuning tool.\nIndex Terms —Automated Indexing, Autonomous Databases,\nReinforcement Learning, Multi-armed Bandits\nI. I NTRODUCTION\nWith the increasing complexity and variability of database\napplications and their hosting platforms (e.g., multi-tenant\ncloud environments), automated physical design tuning, par-\nticularly automated index selection, has re-emerged as a con-\ntemporary challenge for database management systems. Most\ndatabase vendors offer automated tools for physical design\ntuning within their product suites [1]–[3]. Such tools form an\nintegral part of broader efforts toward fully automated database\nmanagement systems which aim to: a) decrease database\nadministration costs and thus total costs of ownership [4],\n[5]; b) help non-experts use database systems; and c) facilitate\nhosting of databases on dynamic environments such as cloud-\nbased services [6]–[9]. Most physical design tools take an off-\nline approach, where the representative training workload is\nprovided by the database administrator (DBA) [10]. Where\nonline solutions are provided [8], [11]–[15], questions remain:\nHow often should the tools be invoked? And more importantly,\nis the quality of proposed designs in any way guaranteed?\nHow can tools generalise beyond queries seen to dynamicad-hoc workloads, where queries are unpredictable and non-\nstationary?\nModern analytics workloads are dynamic in nature with\nad-hoc queries common [16] e.g., data exploration workloads\nadapt to past query responses [17], [18]. Such ad-hoc work-\nloads hinder automated tuning since: a) inputting represen-\ntative information to design tools is infeasible under time-\nevolving workloads; and b) reacting too quickly to changes\nmay result in undesirable performance variability, where in-\ndices are continuously dropped and created. Any proposed\nrobust automated physical design solution must address such\nchallenges [11].\nTo compare alternative physical design structures, auto-\nmated design tools use a cost model employed by the query\noptimiser, typically exposed through a “what-if” interface [19],\nas the sole source of truth. However such cost models make\ninappropriate assumptions about data characteristics [20], [21]:\ncommercial DBMSs often assume uniform data distributions\nand attribute value independence. As a result, estimated bene-\nﬁts of proposed designs may diverge signiﬁcantly from actual\nworkload performance [8], [9], [22]–[24]. Even with more\ncomplex data distribution statistics such as single- and multi-\ncolumn histograms, the issue remains for complex workloads,\nas demonstrated in our experiments (Section V).\nIn this paper, we demonstrate that even in ad-hoc environ-\nments where queries are unpredictable, there are opportunities\nfor index optimisation. We argue that the problem of online\nindex selection under ad-hoc, analytical workloads can be\nefﬁciently formulated within the multi-armed bandit (MAB)\nlearning setting—a tractable form of Markov decision process.\nMABs take arms or actions (selecting indices) to maximise\ncumulative rewards, trading off exploration of untried actions\nwith exploitation of actions that maximise rewards observed so\nfar (see Figure 1). MABs permit learning from observations of\nactual performance, and need not rely on potentially misspec-\niﬁed cost models. Unlike initial efforts with applying learning\nfor physical design, e.g., more general forms of reinforcement\nlearning [25], bandits offer regret bounds that guarantee the\nﬁtness of dynamically-proposed indices [26].\nThe key contributions of the paper are summarised next:\n\u000fWe model index tuning as a multi-armed bandit, propos-\ning design choices that lead to a practical, competitive\nsolution;arXiv:2010.09208v2  [cs.DB]  20 Oct 2020\n\n\u000fOur proposed design achieves a worst-case safety guar-\nantee against any optimal ﬁxed policy, as a consequence\nof a corrected regret analysis of the C2UCB bandit; and\n\u000fOur comprehensive experiments demonstrate MAB’s su-\nperiority over a state-of-the-art commercial physical de-\nsign tool, with up to 75% speed-ups under dynamic,\nanalytical workloads.\nII. P ROBLEM FORMULATION\nThe goal of the online database index selection problem\nis to choose a set of indices (henceforth referred to as a\nconﬁguration ) that minimises the total running time of a\nworkload sequence within a given memory allowance. Neither\nthe workload sequence, nor system run times, are known in\nadvance.\nWe adopt the problem deﬁnition of [13]. Let the workload\nW= (w1;w2;:::;wT)be a sequence of mini-workloads (e.g.,\na sequence of single queries), Ithe set of secondary indices ,\nCmem(s)represent the memory space required to materialise a\nconﬁguration s\u0012I, andS=fs\u0012IjCmem(s)\u0014Mg\u00122I\nbe the class of index conﬁgurations feasible within our total\nmemory allowance M. Our goal is to propose a conﬁguration\nsequenceS= (s0;s1;:::;sT), withst2S as the conﬁgura-\ntion in round tands0=;as the starting conﬁguration, which\nminimises the total workload time Ctot(W;S)deﬁned as:\nCtot(W;S) =TX\nt=1Crec(t) +Ccre(st\u00001;st) +Cexc(wt;st):\nHereCrec(t)refers to the recommendation time in round\nt(deﬁned as running time of the recommendation tool)\nandCcre(st\u00001;st)refers to the incremental index creation\ntime in transitioning from conﬁguration st\u00001tost. Finally,\nCexc(wt;st)denotes the execution time of mini-workload wt\nunder the conﬁguration st, namely the sum of response times\nof individual queries.\nAt roundt, the system:\n1) Chooses a set of indices st2 S in preparation for\nupcoming workload wt, without direct access to wt.\nstonly depends on observation of historical workloads\n(w1;:::;wt\u00001), corresponding sets of chosen indices,\nand resulting performance;\n2) Materialises the indices in stwhich do not exist yet, that\nis, all indices in the set difference stnst\u00001; and\n3) Receives workload wt, executes all the queries therein,\nand measures elapsed time of each individual query and\neach operator in the corresponding query plan.\nIII. C ONTEXTUAL COMBINATORIAL BANDITS\nIn this paper, we argue that online index selection can\nbe successfully addressed using multi-armed bandits (MABs)\nfrom statistical machine learning, where different arms cor-\nrespond to chosen indices. We ﬁrst present necessary back-\nground on MABs, outlining the essential properties that we\nexploit in our work (i.e., bandit context and combinatorial\narms) to converge to a highly performant index conﬁguration.We use the following notation. We denote non-scalar values\nwith boldface: lowercase for vectors and uppercase for matri-\nces. We also write [k] =f1;2;:::;kgfork2N, and denote\nthe transpose of a matrix or a vector with a prime.\nThe contextual combinatorial bandit setting under semi-\nbandit feedback involves repeated selections from kpossible\nactions, over rounds t= 1;2;:::, in which the MAB:\n1) Observes a context feature vector (possibly random or\nadversarially chosen) of each action or armi2[k],\ndenoted asXt=fxt(i)gi2[k], forxt(i)2Rd, along\nwith their costs, ci;\n2) Selects or pulls a set of arms (often referred to as super\narm)st2St, where we restrict the class of possible\nsuper armsSt\u0012S0\nt=\b\ns\u0012[k]\f\fP\ni2sci\u0014C\t\n\u00122[k];\nand\n3) For each it2st, observes random scoresrt(it)drawn\nfrom ﬁxed but unknown arm distribution which depends\nsolely on the arm itand its context xt(it), whose true\nexpected values are contained in the unknown variable\nr?\nt=fE[rt(i)]gi2[k]\nA MAB’s goal is to maximise the cumulative expected rewardP\ntE[Rt(st)] =P\ntg(r?\nt;Xt;st)for a known function g.\nThis function gneed not be a simple summation of all the\nscores. The core challenge in this problem is that the expected\nscores for all arms i2[k]are unknown. Reﬁnement of a bandit\nlearner’s approximation for arm iisgenerally only possible\nby including arm iin the super arm, as the score for arm iis\nnot observable when iis not played. This suggests solutions\nthat balance exploration andexploitation . Even though at ﬁrst\nglance it may seem that each arm needs to be explored at least\nonce, placing practical limits on large numbers of arms, there\nis a remedy to this as will be discussed shortly.\nThe C2UCB algorithm. Used to solve the contextual com-\nbinatorial bandit problem, the C2UCB Algorithm [26] models\nthe arms’ scores as linearly dependent on their contexts:\nrt(i) =\u00120xt(i) +\"t(i)for unknown zero-mean (subgaussian)\nrandom variable \"t, unknown but ﬁxed parameter \u00122Rd, and\nknown context xt(i). It is crucial to notice that this implies\nthatall learned knowledge is contained in estimates of \u0012,\nwhich is shared between all arms, obviating the need to\nexplore each arm . Estimation of \u0012can be achieved using\nridge regression,1withjstjnew data pointsf(xt(i);rt(i))gi2st\navailable at round t, further accelerating the convergence rate\nof the estimator ^\u0012, over observing only one example as might\nbe na ¨ıvely assumed.\nPoint estimates on the expected scores can be made with\n\u0016rt(i) = ^\u00120\ntxt(i), where ^\u0012tare trained coefﬁcients of a\nridge regression on arm i’s observed rewards against contexts.\nHowever, this quantity is oblivious to the variance in the\nscore estimation. Intuitively, to balance out the exploration\nand exploitation, it is desirable to add an exploration boost\nto the arms whose score we are less sure of (i.e., greater\n1A standard linear regression with L2regularisation on the \u0012coefﬁcients,\nleading to well-posed optimisation. Equivalent to max a posteriori Bayesian\nlinear regression with a Gaussian prior.\n\nSELECT A.C1 FROM A\nWHERE A.C2 = 5 AND \nA.C3 = 6\nTable A(C1, C2, C3)\nTable B(C4, C5)0 0.1 1 0 0\n0 1 0.1 0 00 0 1 0 00 1 0 0 0\nPart 1 Part 2\n0 5 0\n0 5 0\n1 15 0\n1 15 0(3) Identify Arms\n(7) Creation time, Execution time w/ Index0 5 0.5 0 0 5 -5 0Shared Weight ( θ)\n(8) 10sec gain, 20sec creation time, \n30MB size\n(2) Query details & \nExecution time w/o IndexArms\n(5) Materialize IX6IX1 (C2)\nIX2 (C3)\nIX5 (C3, C2, C1)\nIX6 (C2, C3, C1)C1 C2 C3 C4 C5 D1 D2 D3C1 C2 C3 C4 C5 D1 D2 D3\n(4) Arm SelectedFirst Round Second Round\n(1) New \nQuery (6) Returning\nQueryContext (x)Fig. 1. An abstract view of the proposed bandit learning-based online index selection.\nestimate variance). This suggests that the upper conﬁdence\nbound (UCB) should be used, in place of the expected value,\nand which can be calculated [27] as:\n^rt(i) = ^\u00120\ntxt(i) +\u000btq\nxt(i)0V\u00001\nt\u00001xt(i); (1)\nwhere\u000btis the exploration boost factor, and Vt\u00001is the\npositive-deﬁnite d\u0002dscatter matrix of contexts for the chosen\narms up to and including round t\u00001. The ﬁrst term of ^rt(i)\ncorresponds to arm i’s immediate reward, whereas its second\nterm corresponds to its exploration boost, as its value is larger\nwhen the arm is sensitive to the context elements we are\nless conﬁdent of (i.e., the underexplored context dimension).\nHence, by using ^rt(i)in place of \u0016rt(i), arms with contexts\nlying in the underexplored regions of context space are more\nlikely to be chosen, as higher scores yield higher g, assuming\nthatgis monotonic increasing in the arm rewards.\nIdeally, the super arm st2 Stis chosen such that\ng(^rt;Xt;st)is maximised. However, it is sometimes compu-\ntationally expensive to ﬁnd such super arms. In such cases, it is\noften good enough to obtain a solution via some approximation\nalgorithm where g(^rt;Xt;st)isnear maximum. With this\ncriterion in mind, we now deﬁne an \u000b-approximation oracle .\nDeﬁnition 1. An\u000b-approximation oracle is an algorithm\nAthat outputs a super arm s=A(r;X)with guarantee\ng(s;r;X)\u0015\u000b\u0001maxsg(s;r;X), for some\u000b2[0;1]and\ngiven inputrandX.\nNote that knapsack-constrained submodular programs are\nefﬁciently solved by the greedy algorithm (iteratively select a\nremaining cost-feasible arm with highest available score) with\n\u000b= 1\u00001=e. C2UCB is detailed in Algorithm 1.\nThe performance of a bandit algorithm is usually measured\nby its cumulative regret , deﬁned as the total expected differ-\nence between the reward of the chosen super arm E[Rt(st)]\nand an optimal super arm maxs2StE[Rt(s)]overTrounds.\nHowever, such a metric is unfair to C2UCB since its perfor-\nmance depends on the oracle’s performance. This suggests toAlgorithm 1 The C2UCB Algorithm\n1:Input:\u0015;\u000b1;:::;\u000bT\n2:InitializeV0 \u0015Id,b0 0d\n3:fort 1;:::;T do\n4: ObserveSt\n5: ^\u0012t V\u00001\nt\u00001bt\u00001.estimate via ridge regression\n6: fori2[k]do\n7: Observe context xt(i)\n8: ^rt(i) ^\u00120\ntxt(i) +\u000btq\nxt(i)0V\u00001\nt\u00001xt(i)\n9: end for\n10:st A(^rt;Xt).using\u000b-approximation oracle\n11: Playstand observe rt(i)for alli2st\n12:Vt Vt\u00001+P\ni2stxt(i)xt(i)0.regression update\n13:bt bt\u00001+P\ni2strt(i)xt(i).regression update\n14:end for\nmeasure C2UCB’s performance with a new metric using the\noracle’s performance guarantee as its measuring stick, deﬁned\nas follows.\nDeﬁnition 2. Cumulative \u000b-regret is the sum of expected\ninstantaneous regret, Reg\u000b\nt=\u000b\u0001maxsg(s;r?\nt;Xt)\u0000\ng(st;r?\nt;Xt), wheresis a super arm returned by an \u000b-\napproximation oracle as a part of the bandit algorithm, while\nr?\ntis a vector containing each arms’ true expected scores.\nWhengis assumed to be monotonic and Lipschitz con-\ntinuous, [26] claimed that C2UCB enjoys ~O(p\nT)\u000b-regret.2\nWe have corrected an error in the original proof, as seen in\nour technical note [28], conﬁrming the ~O(p\nT)\u000b-regret. This\nexpression is sub-linear in T, implying that the per-round\naverage cumulative regret approaches zero after sufﬁciently\nmany rounds. Consequently, online index selection based on\nC2UCB comes endowed with a safety guarantee on worst-\ncase performance: selections become at least as good as an\n2The notation ~O(\u0001)is equivalent to O(\u0001)while ignoring logarithmic factors.\n\n\u000b-optimal policy (with perfect access to true scores); and\npotentially much better than any ﬁxed policy.\nIV. MAB FOR ONLINE INDEX SELECTION\nPerformant bandit learning for online index tuning demands\narms covering important actions and no more, rewards that\nare observable and for which regret bounds are meaningful,\nand contexts and oracle that are efﬁciently computable and\npredictive of rewards. Each workload query is monitored for\ncharacteristics such as running time, query predicates, payload,\netc. (Figure 1). These observations feed into generation of\nrelevant arms and their contexts. The learner selects a desired\nconﬁguration which is materialised. After query return, the\nsystem identiﬁes beneﬁts of the materialised indices, which\nare then shaped into the reward signal for learning.\nDynamic arms from workload predicates. Instead of\nenumerating all column combinations, relevant arms (indices)\nmay be generated based on queries: combinations and permu-\ntations of query predicates (including join predicates), with\nand without inclusion of payload attributes from the selec-\ntion clause. Such workload-based arm generation drastically\nreduces the action space, and exploits natural skewness of\nreal-life workloads that focus on small subsets of attributes\nover full tables [17], [18]. Workload-based arm generation is\nonly viable due to dynamic arm addition (reﬂecting a dynamic\naction space) and is allowed by the bandit setting: we may\ndeﬁne the set of feasible arms for each round at the start of\nthe round.\nContext engineering. Effective contexts are predictive of\nrewards, efﬁciently computable, and promote generalisation to\npreviously unseen workloads and arms. We form our context\nin two parts (see Figure 1).\nContext Part 1: Indexed column preﬁx. We encode one\ncontext component per column. However unlike a bag-of-\nwords or one-hot representation appropriate for text, similarity\nof arms depends on having similar column preﬁxes; common\nindex columns is insufﬁcient. This reﬂects a novel bandit\nlearning aspect of the problem. A context component has\nvalue 10\u0000jwherejis the corresponding column’s position in\nthe index, provided that the column is included in the index\nand is a workload predicate column. The value is set to 0\notherwise, including if its presence only covers the payload.\nExample 3. Under the simplest workload (single query) in\nFigure 1, our system generates six arms: four using different\ncombinations and permutations of the predicates, two includ-\ning the payload (covering indices). Index IX5 includes column\nC1, but the context for C1 is valued as 0, as this column is\nconsidered only due to the query payload.\nContext Part 2: Derived statistical information. We repre-\nsent statistical and derived information about the arms and\nworkload, details available from the optimiser during query\nexecution, and sufﬁcient statistics for unbiased estimates.\nThis statistical information includes: a Boolean indicating a\ncovering index, the estimated size of the index divided by the\ndatabase size (if not materialised already, 0 otherwise), andusage information of the index from previous rounds. This is\nshown in Figure 1 under D1, D2 and D3, respectively.\nReward shaping. As the goal of physical design tuning\ntools is to minimise end-to-end workload time, we incorporate\nindex creation time and query execution time into the reward\nfor a workload. We omit index recommendation time, as it is\nindependent of arm selection. However, we measure and report\nrecommendation time of the MAB algorithm in our experi-\nments. Recall that MAB depends only on observed execution\nstatistics from implemented conﬁgurations and generalisation\nof the learned knowledge to unseen arms thereafter.\nThe implementation of the reward signal for an arm includes\nthe execution time as a gainGt(i;wt;st)for a workload wt\nby each arm iunder conﬁguration st. By deﬁningU(s;q)as\nthe list of indices used by the query optimiser for query qfor\na given conﬁguration s, the gain by index ifor a query qis\ndeﬁned by:\nGt(i;fqg;st)\n= [Ctab(\u001c(i);q;;)\u0000Ctab(\u001c(i);q;fig)] 1U(s;q)(i);\nwhere\u001c(i)is the table which ibelongs to and Ctab(\u001c(i);q;;)\nrepresents the full table scan time for table \u001c(i)and queryq.3\nThe gain for a workload is related to the gain for individual\nquery by:\nGt(i;wt;st) =X\nq2wtGt(i;fqg;st):\nBy this deﬁnition, gain Gt(i;wt;st)will be 0ifiis not used\nby the optimiser in the current round tand can be negative if\nthe index creation leads to a performance regression. Creation\ntime ofiis taken as a negative reward, only if iis materialised\nin roundt, and is 0otherwise:\nrt(i) =Gt(i;wt;st)\u0000Ccre(st\u00001;fig):\nMinimising the end-to-end workload time, or rather, max-\nimising the end-to-end workload time gained, is the goal of\nthe bandit. As deﬁned earlier, the total workload time Ctot\nis the sum of execution ,recommendation andcreation times\naccumulated over rounds. As such, minimising each round’s\nsummand is an equivalent problem. Modifying the execution\ntime to the time gain while ignoring the recommendation time\nyields per-round super arm reward of:\nRt(st) =Cexc(wt;;)\u0000[Cexc(wt;st) +Ccre(st\u00001;st)]\n\u0019X\ni2stGt(i;wt;st)\u0000X\ni2stCcre(st\u00001;fig)\n=X\ni2strt(i):\nSelection of the execution plan depends on the query optimiser,\nand as noted, the query optimiser may resolve to a sub-optimal\nquery plan. As we show, the bandit is nonetheless resilient as\nit can quickly recover from any such performance regressions.\n3Due to the reactive nature of multi-armed bandits, we mostly observe a\nfull table scan time for each table \u001c(i)and queryq. When we do not observe\nthis, we estimate it with the maximum secondary index scan/seek time.\n\nAlgorithm 2 MAB Simulation for Index Tuning\n1:QS QueryStore ().keeps query information\n2:while (TRUE) do\n3: queries getLastRoundWorkload ()\n4: for allqueries do\n5: if(isNewTemplate) then\n6: QS:add (query )\n7: else\n8: QS:update (query )\n9: end if\n10: end for\n11: QoI QS:getQoI ().get queries of interest\n12: arms generateArms (QoI)\n13: contexts generateContext (arms;QoI )\n14: st C2UCB:recommend (arms;contexts )\n15: Ccre materialise (st)\n16: Cexc executeCurrentWorkload ()\n17: C2UCB:updateReward (Ccre;Cexc)\n18:end while\nObserved execution times encapsulate real-world effects e.g.,\nthe interaction between queries, application properties, run-\ntime parameters, etc. Since the end-to-end workload time\nincludes the index creation and query execution times, we\nare indirectly optimising for both efﬁciency and the quality\nof recommendations.\nA greedy oracle for super-arm selection. Recall that\nC2UCB leverages a near-optimal oracle to select a super arm,\nbased on individual arm scores [26]. As a sum of individual\narm rewards, our super-arm reward has a (sub)modular ob-\njective function and (as easily proven) exhibits monotonicity\nand Lipschitz continuity. Approximate solutions to maximise\nsubmodular (diminishing returns) objective functions can be\nobtained with greedy oracles that are efﬁcient and near-\noptimal [29]. Our implementation uses such an oracle com-\nbined with ﬁltering to encourage diversity. Initially, arms with\nnegative scores are pruned. Then arm selection and ﬁltering\nsteps alternate, until the memory budget is reached. In the\nselection step, an arm is selected greedily based on individual\nscores. The ﬁltering step ﬁlters out arms that are no longer\nviable under the remaining memory budget, or those that are\nalready covered by the selected arms based on preﬁx matching.\nIf a covering index is selected for a query, all other arms\ngenerated for that query will be ﬁltered out. Note that ﬁltering\nis a temporary process that only impacts the current round.\nBandit learning algorithm. Algorithm 2 shows the MAB\nalgorithm which summarises workload information using tem-\nplates; these track frequency, average selectivity, ﬁrst seen and\nlast seen times of the queries which help to generate the best\nset of arms per round (i.e., QoI). The context is updated after\neach round based on the workload and selected set of arms.\nThe bandit then selects a set of arms for this round. The set\nof arms chosen form a conﬁguration to be materialised within\nthe database. Once the new conﬁguration is in place, a new\nset of queries will be executed. To support shifting workloads,where users’ interests change over time, the learner can forget\nlearned knowledge depending on the workload shift intensity\n(i.e., number of newly introduced query templates).\nV. E XPERIMENTAL EVALUATION\nWe evaluate our MAB framework across a range of widely\nused industrial benchmarks, comparing it to a state-of-the-\nart physical design tool shipped with a commercial database\nproduct referred to as the Physical Design Tool (PDTool).\nThis is a mature product, proven to outperform other physical\ndesign tools available on the market.\nA. Experimental Setup\nBenchmarks. We use ﬁve publicly available benchmarks:\nTPC-H (with uniform distribution) [30] and TPC-H Skew [31]\nwith zipﬁan factor 4, allowing the reader to understand the\nimpact of data skewness when all the other aspects are\nkept identical; TPC-DS [32], which demonstrates the solution\nﬁtness under a large number of candidate conﬁgurations;\nSSB [33] with easily achievable high index beneﬁts; and\nﬁnally, Join Order Benchmark (JOB) with IMDb dataset (a\nreal-world dataset) [21] (henceforth referred to as IMDb) a\nchallenging workload for index recommendations, with index\noveruse leading to performance regressions.\nUnless stated otherwise, all experiments use scale fac-\ntor (SF) 10, resulting in approximately 10GB of data per\nworkload, except in the case of the IMDb dataset which\nhas a ﬁxed size of 6GB. We consider three broad types of\nworkloads, allowing us to compare different aspects of the\nrecommendation process:\nStatic : The workload sequence is known in advance, and\nrepeating over time (modelling workloads used for reporting\npurposes). In absence of dynamic environment complexities,\nthis simpler setting allows us to single-out the effectiveness\n(ability to ﬁnd a better conﬁguration) and the efﬁciency\n(additional overhead) of the MAB search strategy.\nDynamic shifting : The region of interest shifts over time\nfrom one group of queries to another (modelling data ex-\nploration). This experiment evaluates the adaptation speed to\nworkload shifts and the cost of exploration when adapting.\nDynamic random : A query sequence is chosen entirely\nat random (modelling more dynamic settings, such as cloud\nservices). Dynamic random experiments test the delicate bal-\nance between swift and careful adaptation under returning\nworkloads, which can lead to unwanted index oscillations.\nBoth PDTool and MAB are given a memory budget ap-\nproximately equal to the size of the data (1x; 10GB for\nSF 10 datasets and 6GB for IMDb dataset) for the creation\nof secondary indices. We have experimented with different\nmemory budgets ranging from 0.25x to 2x (since beneﬁts\nof additional memory seem to diminish beyond a 2x limit)\nunder TPC-H and TPC-H skew benchmarks, and observed\nthe same patterns throughout that range. We have naturally\npicked the middle of the active region (1x) as our memory\nbudget. All these workloads come with original primary and\n\n050100150200250300350400\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB\n0100200300400500600700\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB\n050100150200250300\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB(a)\n0100200300400500600700\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB\n050100150200250300350400\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB (b)\n0100200300400500600700\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB (c)\n05001000150020002500300035004000\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB\n050100150200250300\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB (d)\n05001000150020002500300035004000\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB\n050100150200250300\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB (e)\nFig. 2. MAB vs. PDTool Convergence for static workloads: (a) SSB, (b) TPC-H, (c) TPC-H Skew, (d) TPC-DS and (e) IMDb.\n0500010000150002000025000300003500040000\nNoIndex\nPDTool\nMAB\nNoIndex\nPDTool\nMAB\nNoIndex\nPDTool\nMAB\nNoIndex\nPDTool\nMAB\nNoIndex\nPDTool\nMAB\nSSB TPC-H TPC-H Skew TPC-DS IMDbTotal Workload Time (sec)\nWorkload\nFig. 3. MAB vs. PDTool total end-to-end workload time for static workloads.\nforeign keys that inﬂuence the choice of indices. We grant the\naforementioned memory budget on top of this.\nIn search of the best possible design, we do not constrain\nthe running time of PDTool, with one exception: In TPC-\nDS dynamic random, PDTool was uncompetitive due to long\nrunning times,4hence the PDTool running time of each\ninvocation was restricted to 1 hour. All proposed indices are\nmaterialised and queries invoked over the same commercial\nDBMS in both cases (MAB and PDTool).\nAcross experiments, each group of templatized queries is\ninvoked over rounds, producing different query instances. For\nstatic and dynamic settings, PDTool is invoked every time\nafter the ﬁrst round of new queries, with those queries given\nas the training workload, since this workload will become\nrepresentative of future rounds. This setting is somewhat\nunrealistic and favourable for PDTool, since in real-life the\nPDTool will seldom truly have knowledge of the representative\nworkload (i.e., what is yet to arrive in the future), advantaging\nthe PDTool in our experiments. However, it presents a viable\ncomparison against the workload-oblivious MAB. Bandits, do\nnot use any workload information ahead of time but instead\nobserve workload sequence and react accordingly.\nHardware. All experiments are performed on a server\nequipped with 2x 24 Core Xeon Platinum 8260 at 2.4GHz,\n4A single PDTool invocation took around 8 hours (default limit). The\ntotal recommendation time was around 40 hours, which is not competitive\ncompared to the end-to-end workload time of 4 hours under MAB.1.1TB RAM, and 50TB disk (10K RPM) running Windows\nServer 2016. We report cold runs, clearing database buffer\ncaches prior to every query execution.\nMetrics. In addition to reporting total end-to-end workload\ntime for all rounds, we also report the total workload time\nper round used to demonstrate the convergence of different\ntools. Additionally, we present the total workload time broken\ndown by recommendation time (when invoking the PDTool\nor the MAB framework), index creation time, and workload\nexecution time. For completeness, we show original query\ntimes, without any secondary indices (denoted as NoIndex). In\naddition to convergence graphs of individual benchmarks, we\npresent a summary graph with total end-to-end workload time\nfor all rounds under MAB and PDTool tuning of SSB, TPC-H\n(uniform), TPC-H skew, TPC-DS and IMDb benchmarks.\nB. Experimental Results\n1) MAB versus the existing physical design tool: We\nreport on wide ranging empirical comparisons of MAB and\nPDTool.\nStatic workloads. Static workloads over uniform datasets\nare the best case for ofﬂine physical design tools, as a\npre-determined workload sequence may perfectly represent\nfuture queries. However, when underlining data is skewed,\nrecommendations based on a pre-determined workload alone\ncan have unfavourable outcomes. While used for reporting,\nstatic workloads do not reﬂect modern dynamic workloads\n(e.g., data exploration). In static workloads, all query templates\nin the benchmark (22, 13, 99 and 33 templates for TPC-H,\nSSB, TPC-DS and IMDb, respectively) are invoked once every\nround, each with a different query instance of the template,\nfor a total of 25 rounds, providing sufﬁcient time to observe\nconvergence.\nFigure 3 displays overall workload time (including rec-\nommendation and index creation time) for all 25 rounds\nunder MAB and PDTool. For skewed datasets (TPC-H Skew,\nTPC-DS, IMDb) MAB outperforms PDTool. MAB shows\nover 17%, 28% and 11% performance gain against PDTool,\nunder TPC-H Skew, TPC-DS, IMDb benchmarks, respectively.\nUnder uniform datasets (TPC-H and SSB), both MAB and\nPDTool provide signiﬁcant performance gains over NoIndex\n\n010203040506070\n0 20 40 60 80Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB\n050100150200250300\n0 20 40 60 80Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB\n01002003004005006007008009001000\n0 20 40 60 80Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB(a)\n010203040506070\n0 20 40 60 80Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB\n050100150200250300\n0 20 40 60 80Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB\n01002003004005006007008009001000\n0 20 40 60 80Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB (b)\n010203040506070\n0 20 40 60 80Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB\n050100150200250300\n0 20 40 60 80Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB\n01002003004005006007008009001000\n0 20 40 60 80Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB (c)\n0200400600800100012001400\n0 20 40 60 80Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB\n020406080100120\n0 20 40 60 80Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB (d)\n0200400600800100012001400\n0 20 40 60 80Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB\n020406080100120\n0 20 40 60 80Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB (e)\nFig. 4. MAB vs. PDTool Convergence for dynamic shifting workloads: (a) SSB, (b) TPC-H, (c) TPC-H Skew, (d) TPC-DS and (e) IMDb.\n05000100001500020000250003000035000\nNoIndex\nPDTool\nMAB\nNoIndex\nPDTool\nMAB\nNoIndex\nPDTool\nMAB\nNoIndex\nPDTool\nMAB\nNoIndex\nPDTool\nMAB\nSSB TPC-H TPC-H Skew TPC-DS IMDbTotal Workload Time (sec)\nWorkload\nFig. 5. MAB vs. PDTool total end-to-end workload time for dynamic shifting\nworkloads.\n(over 50% and 85%, respectively), while PDTool outperforms\nthe MAB (by 19% and 5%). This is not surprising since\nfor uniform, static experiments usually align with PDTool\nassumptions and the future can be perfectly represented by\na pre-determined workload.\nConvergence plots in Figure 2(a–e), show MAB’s gradual\nimprovement over 25 rounds. Both MAB and PDTool have\nlarge spikes after the ﬁrst round for all the workloads (spikes\nare less visible in SSB due to the small number of simple\nindices created). For both tools, this is due to recommendation\nand creation of indices. However, MAB might drop proposed\nindices and create new ones later on, generating relatively\nsmaller spikes in subsequent rounds. Nonetheless, MAB ef-\nﬁciently balances the exploration of new indices, reducing\nexploration with time.\nWhat is the best search strategy? Comparison of execution\ntimes in the ﬁnal round of the static experiment provides a\nclear idea about the beneﬁt of using execution cost guided\nsearch. As evident from Figure 2(a–e), in 4 out of 5 cases,\nMAB converges to a better conﬁguration than PDTool. MAB\nprovides over 5%, 84%, 31% and 19% better execution time\nby the last round (25th) compared to PDTool under SSB, TPC-\nH Skew, TPC-DS and IMDb, respectively.\nFor TPC-H skew, PDTool misses an index on\nOrders:Ocustkey . This index boosts the performance\nof some queries (Q22 in particular) which MAB correctly\ndetects and materialises. Missing this leads to large executiontimes in a few rounds including the last round (8, 12, 17, 20\nand 25) for PDTool. These experiments illustrate the risk of\nrelying on the query optimiser and imperfect statistics as a\nsingle source of truth.\nThe only case when MAB is outperformed by the PDTool\nis under TPC-H (PDTool delivers over 21% better execution\ntime by the last round): different indices are proposed, as our\ncurrent MAB framework does not support an index merging\nphase employed by some physical design tools [34]. Instead,\nMAB uses individual queries to propose index candidates.\nWe plan to address index merging in future work.\nDynamic shifting workloads. Under the dynamic shifting\nworkloads, all query templates in the benchmark are randomly\ndivided into 4 equal-sized groups. A group of query templates\nis then executed for 20 rounds, after which the workload\nswitches to a new group of unseen queries (no overlap with\nthe previous queries). When the workload switches, PDTool\nis invoked and trained on the new sequence of queries (whose\ntemplates will be used in the next 19 rounds).5Thus, PDTool\nis invoked four times in total (in rounds 2, 22, 42, 62). On\nthe other hand, the MAB framework does not assume any\nworkload knowledge.\nFigure 5 displays MAB’s end-to-end workload time as\nsubstantially lower compared to the alternatives, under all\nbenchmarks. MAB provides over 3%, 6%, 58%, 14% and 34%\nspeed-up compared to PDTool, under SSB, TPC-H, TPC-H\nSkew, TPC-DS and IMDb, respectively.\nInterestingly, NoIndex performs better than PDTool against\nthe IMDb workload. PDTool has a higher total workload\ntime as well as higher execution time compared to NoIndex.\nNoIndex provides 3.5% (24 seconds) speedup in execution\ntime over PDTool. This is mainly due to misestimates of the\noptimiser [8]. As an example (out of many), query 18 takes\nless than 1 second under NoIndex, whereas with the created\nindices by PDTool some instances of this query take around 7-\n8 seconds due to a suboptimal plan chosen favoring the index\nusage. This affects both MAB and PDTool, but MAB identiﬁes\n5Again, this relaxation assumes a DBA with knowledge that the previous\nworkload will not be repeated, placing PDTool at an advantage. In reality,\nproposing training workloads might be much more challenging for dynamic\nworkloads.\n\n050100150200250300350\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB\n0100200300400500600\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB\n0100200300400500600700800\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB(a)\n050100150200250300350\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB\n0100200300400500600\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB\n0100200300400500600700800\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB (b)\n050100150200250300350\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB\n0100200300400500600\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB\n0100200300400500600700800\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB (c)\n0500100015002000250030003500400045005000\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB\n0100200300400500600\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB (d)\n0500100015002000250030003500400045005000\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB\n0100200300400500600\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberNoIndex\nPDTool\nMAB (e)\nFig. 6. MAB vs. PDTool Convergence for dynamic random workloads: (a) SSB, (b) TPC-H, (c) TPC-H Skew, (d) TPC-DS and (e) IMDb\n050001000015000200002500030000350004000045000\nNoIndex\nPDTool\nMAB\nNoIndex\nPDTool\nMAB\nNoIndex\nPDTool\nMAB\nNoIndex\nPDTool\nMAB\nNoIndex\nPDTool\nMAB\nSSB TPC-H TPC-H Skew TPC-DS IMDbTotal Workload Time (sec)\nWorkload\nFig. 7. MAB vs. PDTool total end-to-end workload time for dynamic random\nworkloads.\nthe indices with a negative impact based on the reward and\ndrops them. For the IMDb workload which does not get much\nsupport from indices, MAB provides 3% total performance\ngain and 26% execution time gain compared to NoIndex.\nOne can easily observe the workload shifts in Figure 4(a–e)\ndue to the spikes in rounds 2, 22, 42, and 62. For PDTool,\nthis is due to the invocation of PDTool and index creation\nafter the workload shifts. Similar spikes can be seen in\nthe MAB line with automatic detection of workload shifts.\nFurther random spikes can be observed, for PDTool, from\nrounds 20-40 in TPC-H skew and rounds 30-40 under IMDb,\ndue to the issues discussed in the previous paragraphs (Q22\nin TPC-H Skew, Q18 in IMDb).\nDynamic random workloads. We simulate modern data\nanalytics workloads that are truly ad-hoc in nature. For in-\nstance, cloud providers, hosting millions of databases, sel-\ndom can detect representative queries, since they frequently\nchange [8]. In such cases, it is common to invoke the PDTool\nperiodically (e.g., nightly or weekly) using queries since the\nlast invocation as the training workload. In this setting, we\ninvoke the PDTool every 4 rounds, using queries from the\nlast 4 rounds as the representative workload. In the dynamic\nrandom setting, the number of total training queries in the\ncomplete sequence is similar to the number of queries we\nhad in the static setting. However, we have no control over\nthe selection of queries for the workload and they are chosencompletely randomly. The sequence is then divided into 25\nequal-sized rounds. In all cases, the round-to-round repeat\nworkload was between 45-54%.\nAs shown in Figure 7, again we see a considerably lower\ntotal workload time of MAB compared to PDTool. MAB\nprovides over 37%, 17%, 11%, 61% and 75% speed-up\ncompared to PDTool, under SSB, TPC-H, TPC-H Skew, TPC-\nDS and IMDb, respectively. It is notable that in Figure 7, the\ntotal workload time of PDTool climbs higher than NoIndex on\ntwo occasions, in TPC-DS and IMDb. In IMDb, this is due\nto the same issue discussed previously under dynamic shifting\nworkloads (due to the optimiser’s misestimates, favouring the\nusage of sub-optimal indices, e.g., IMDb Q18). While PDTool\nhas a much better execution time than NoIndex under TPC-DS\n(execution time of 5.3h under PDTool vs 9.3h under NoIndex),\ndue to high recommendation time (5.1 hours, see Table I),\nPDTool ends up with a higher total workload time. Under\nthese 2 benchmarks (TPC-DS and IMDb), MAB provides over\n55% and 1.5% performance gain over NoIndex, respectively.\nIn Figure 6(a–e), we can see ﬁve major spikes for PDTool due\nto the tuning invocations (in rounds 5, 9, 13, 17, 21).\n2) The impact of database size: To examine the impact of\ndatabase size, we run TPC-H uniform and TPC-H Skew static\nexperiments on SF 1, 10 and 100 databases. As previously\ndiscussed, under SF 10, MAB performed better in the case\nof TPC-H Skew and PDTool performed better on TPC-H\n(see Table II). The impact of sub-optimal index choices is\neven more evident for larger databases, leading to a huge gap\nbetween total workload times of MAB and PDTool for TPC-\nH Skew (44 hours in the former vs 20 hours in the latter\ncase). In TPC-H, PDTool results in a higher total workload\ntime (14.8 hours vs. 13.2 hours for MAB). This is mainly\ndue to sub-optimal optimiser decisions, where the optimiser\nfavours the usage of indices (coupled with nested loops joins)\nwhen alternative plans would be a better option. For instance,\nunder the recommended indices from PDTool, some instances\nof Q5 run longer than 8 minutes (using index nested loops\njoin), where others ﬁnish in 1.5 minutes (using a plan based\non hash joins). We notice that, with larger database sizes,\nexecution time dominates contributing more than 91% to the\ntotal workload time. We observe faster and more accurate\n\nTABLE I\nTOTAL TIME BREAKDOWN (MIN):THE BEST CHOICE IS IN BOLD TEXT\nWorkloadRecommendation Creation Execution Total\nPDTool MAB PDTool MAB PDTool MAB PDTool MABStaticSSB 0.34 0.02 0.95 1.86 12.9 13.15 14.19 15.03\nTPC-H 0.6 0.08 2.45 5.66 46.35 55.64 49.4 61.38\nTPC-H Skew 0.58 0.11 8.37 19.82 54.17 32.06 63.12 51.99\nTPC-DS 44.86 1.53 1.45 5.94 302.63 242.15 348.94 249.62\nIMDB 0.34 0.31 1.1 1.3 11.01 9.42 12.41 11.03DynamicSSB 1.28 0.05 1.5 2.21 5.42 5.69 8.2 7.95\nTPC-H 1.55 0.12 9.36 9.74 26.35 25.14 37.25 35\nTPC-H Skew 1.65 0.16 14.98 20.96 85.49 21.44 102.11 42.56\nTPC-DS 11.13 1.66 6.08 16.48 187.08 155.65 204.29 173.79\nIMDB 3.09 0.29 1.59 2.24 11.21 7.93 15.89 10.46RandomSSB 2.83 0.02 1.77 2.37 26.59 16.83 30.85 19.22\nTPC-H 7.55 0.08 14.68 7.06 84.14 80.43 106.37 87.57\nTPC-H Skew 3.3 0.08 31.74 34.68 48.71 39.44 83.75 74.2\nTPC-DS 310.22 1.4 8.23 19.81 323.57 227.02 642.01 248.24\nIMDB 14.74 0.28 2.72 1.14 48.55 14.47 66.01 15.89TABLE II\nStatic WORKLOADS UNDER DIFFERENT\nDATABASE SIZES (MIN)\nWorkload SF PDTool MAB\nTPC-H1 2.02 2.03\n10 49.4 61.38\n100 891.01 793.40\nTPC-H\nSkew1 4.17 3.83\n10 63.12 51.99\n100 2640.64 1219.33\nconvergence of MAB under larger databases, due to a clear\ndifference between rewards for different arms, highlighting\nMAB’s excellent potential beneﬁts for larger databases.\n3) Hypothetical index creation vs actual index creation:\nManaging the exploration-exploitation balance under a large\nnumber of candidate indices, and an enormous number of\ncombinatorial choices is not trivial. PDTool explores using the\n“what-if” analysis, which comes under the tool’s recommen-\ndation time, whereas MAB explores using index creations.\nCost of hypothetical index creation: When analysing\nPDTool’s recommendation times, it’s evident that average time\nof a single PDTool invocation grows noticeably with training\nworkload size. As an example, PDTool tuning of the TPC-DS\nbenchmark grows from 2.78 minutes in the dynamic shifting\nsetting (25-query workload) to 62.02 minutes in the dynamic\nrandom setting (400-query workload). Furthermore, multiple\ninvocations required in dynamic random and shifting settings\naggravate the problem further for PDTool (see Table I). On the\nother hand, PDTool recommendation time rapidly increases\nwith the complexity of the workloads (e.g., under 100-query\nTPC-H workload, PDTool takes below 8 minutes for recom-\nmendation, whereas the same size TPC-DS workload tuning\ntakes more than 45 minutes).\nHowever, MAB recommendation times stay signiﬁcantly\nlower and stable despite the workload shifts and changes in\ncomplexity or size (see Table I). In all experiments, MAB takes\nless than 1% of the total workload time for recommendation,\nexcept for IMDb where it takes around 2% (due to low total\nworkload time and higher number of query templates). More\nthan 80% of this recommendation time is spent on the initial\nsetup (1stround) and the continuous overhead is negligible.\nCost of actual index creation: While actual execution\nstatistics based search allows the MAB to converge to better\nconﬁgurations, as a down side, MAB spends more time on\nindex creation (see Table I). For instance, under TPC-H and\nTPC-H Skew static experiments, MAB spends 5.6 and 19.8\nminutes on index creation where PDTool only spends 2.4\nand 8.3 minutes, respectively. Under skewed data, rewardsshow more variability which delays the convergence for MAB.\nThis leads to higher exploration and greater creation costs.\nWhile MAB is still competitive due to efﬁcient exploration,\nwe consider ways to improve its convergence in future work\n(see Section VII).\nFinal verdict: Comparing the total of recommendation and\nindex creation times (henceforth refereed to as exploration\ncost) between MAB and PDTool presents a clear picture\nabout these two exploration methods. From Table I we can\nobserve that, in most cases (9 out of 15) MAB archives a\nbetter exploration cost compared to PDTool. However when\nthe workload is small (e.g., dynamic shifting) PDTool tends to\nperform better. TPC-DS, with the highest number of candidate\nindices (over 3200 indices) among these benchmarks, provides\na great test case for exploration efﬁciency. Under TPC-DS,\nMAB exploration cost is signiﬁcantly lower in shifting and\nrandom settings, and marginally higher in the static setting.\nDespite the efﬁcient exploration, MAB does not sacriﬁce\nrecommendation quality in any way (better execution costs\nin 12 out of 15 cases, with signiﬁcantly better execution costs\nunder all cases of TPC-DS).\nThis efﬁcient exploration is promoted by the linear reward-\ncontext relationship along with C2UCB’s weight sharing (Sec-\ntion III), resulting in a small number of parameters to learn.\nAn arm’s identity becomes irrelevant and context (Section IV)\nbecomes the sole determining factor of each arm’s expected\nscore, which allows MAB to predict the UCB of a newly\narriving arm with known context without trying it even once.\nC. Why Not (General) Reinforcement Learning?\nPast efforts have considered more general reinforcement\nlearning (RL) for physical design tuning [25], [35]. Compared\nto most MAB approaches, deep RL invites over parameteri-\nsation, which can slow convergence (see Figure 8), whereas\nMAB typically provides better convergence, simpler imple-\nmentation, and safety guarantees via strategic exploration and\nknowledge transfer (see Section III). Due to its randomisation,\n\n050100150200250300350400450500\n0 20 40 60 80 100Total Time Per Round (sec)\nRound NumberPDTool\nMAB\nDDQN\nDDQN_SC\n050001000015000200002500030000\nPDTool MAB DDQN DDQN_SCTotal Workload Time (sec)\nMethodsRecommendation\nIndex Creation\nExecution(a)\n01002003004005006007008009001000\n0 20 40 60 80 100Total Time Per Round (sec)\nRound NumberPDTool\nMAB\nDDQN\nDDQN_SC\n05000100001500020000250003000035000\nPDTool MAB DDQN DDQN_SCTotal Workload Time (sec)\nMethodsRecommendation\nIndex Creation\nExecution (b)\n050100150200250300350400450500\n0 20 40 60 80 100Total Time Per Round (sec)\nRound NumberPDTool\nMAB\nDDQN\nDDQN_SC\n050001000015000200002500030000\nTA MAB DDQN DDQN_SCTotal Workload Time (sec)\nMethodsRecommendation Cost\nIndex Creation Cost\nExecution Cost (c)\n01002003004005006007008009001000\n0 20 40 60 80 100Total Time Per Round (sec)\nRound NumberPDTool\nMAB\nDDQN\nDDQN_SC\n05000100001500020000250003000035000\nPDTool MAB DDQN DDQN_SCTotal Workload Time (sec)\nMethodsRecommendation Cost\nIndex Creation Cost\nExecution Cost (d)\nFig. 8. DDQN vs. MAB for static workloads: (a) End-to-end workload time for TPC-H, (b) End-to-end workload time for TPC-H Skew, (c) TPC-H\nconvergence, (d) TPC-H Skew convergence.\nRL can also suffer from performance volatility as compared\nto C2UCB, a deterministic algorithm.\nExperimental setup. The above intuition is supported by\nexperiments with more general RL, where we evaluate the\npopular DDQN RL agent [36]. We run the static 10GB TPC-\nH and TPC-H Skew benchmark over 100 rounds and present\nresults in Figure 8. For a fair comparison, we combine all of\nMAB’s arms’ contexts as DDQN state. We also present the\nsame set of candidate indices to the DDQN. For the DDQN’s\nneural network hyperparameters, we followed the experiment\nof [25] by setting 4 hidden layers, with 8 neurons each. The\ndiscount factor \ris set to 0:99and the exploration parameter is\nset to 1at the ﬁrst sample, decaying to 0 with exponential rate\nreaching 0.01 in the 2400thsample. One sample corresponds to\none index chosen by the agent. In the beginning of the round,\nif the agent decides to explore, then the choice of the set of\nindices will be randomly made for that entire round. These\nexperiments are repeated ten times, reporting either average\nvalue (Figure 8 (a) and (b)) or median ((c) and (d)) along with\ninter-quartile range. For completeness, we include the case of\nonly using single column indices (DDQN-SC in Figure 8), as\noriginally proposed in [25].\nEvaluation. Due to DDQN-SC’s reduced search space in\nsome scenarios, it might not be possible to ﬁnd the optimal\nconﬁguration for a workload. This is evident from Figure 8 (a)\nand (b) where DDQN shows 33% and 21% speedup, compared\nto DDQN-SC, in execution time under TPC-H and TPC-H\nSkew, respectively. Interestingly, under TPCH-Skew, where\nthe demand for exploration is higher, DDQN-SC has a lower\ntotal workload time than DDQN due to the noticeably small\nindex creation times of single column indices (1.5 hours vs\n5.8 hours, respectively). Under both TPC-H and TPC-H Skew,\nMAB performs signiﬁcantly better, providing 35% and 58%\nspeed-up against the better RL alternative, respectively.\nNo state transitions. A strength of more general re-\ninforcement learning is its ability to take into account\nstate transitions—usually represented as transition probability\nmatrices–when actions are taken. However, in the online index\nselection problem, the importance of state transition is unclear.\nAn approach to deﬁning state is to think of it as the collection\nof indices that exist in the system. That is, when an action\nis taken (i.e., choosing an index), it will determine whetherthat index exists at the start of the next round. This fact,\nhowever, does not require a probability estimate since we\nknow with certainty what indices will exist at the beginning\nof the next round, which by itself determines the next state\n(i.e., deterministic state transition). Another notion of state\nis the characteristic of the queries asked in the next round.\nThose queries do not depend on the action taken at the end\nof the current round, thus it is appropriate to take successor\nquery state as independent of actions taken. Hence, adopting\nmore general RL provides no clear beneﬁt over MAB, while\nimposing delay to convergence as demonstrated in Figure 8 (c)\nand (d). The fact that there is no progression of state justiﬁes\nthe choice of using MAB over DNN-based general RL. MABs\nalso enjoy performance guarantees (see Section III).\nHyperparameter search space. General RL approaches are\nnotorious for challenging hyperparameter tuning. For example,\nin this experiment, we have to decide: the number of layers of\nthe neural network, the neurons per layer, activation functions\nand loss, the exploration parameter \", and discount factor \r.\nInvoking grid search is highly time consuming, considering\nthat one repetition of the experiment for one possible combi-\nnation of hyperparameters takes hours to complete. In contrast,\nthe C2UCB bandit only requires the hyperparameter \u0015—which\nbecomes less relevant as rounds are observed—and \u000bwhich\ncontrols exploration.\nVolatility of RL. Most modern RL algorithms, such as\nDDQN which we have used here, require randomisation in\norder to explore vast state-action spaces. There is no hint\nof which arms or context elements are underexplored, so\narms are chosen randomly when the agent decides to enter\nexploration, which also occurs at random. While we might\nbe lucky and happen upon the optimal set of arms, we could\nbe unfortunate and the algorithm might not encounter useful\narms. This is not the case with C2UCB. Extending UCB,\ndeterministic C2UCB is capable of identifying underexplored\narms through their context vectors. The only (rare and as such\nnot strictly necessary) case when C2UCB is random is where\nthe MAB must tie-break arms. A more signiﬁcant cause of\nstability of our MAB is its small parametrisation compared to\ndeep learner-based RL. Combined, the stable MAB yields a\nmore consistent result, as can be seen in Figure 8(a) and (b).\nMuch wider variance (highlighted by the error bars) on the\n\nDDQN plot demonstrates how the performance of DDQN can\nvary signiﬁcantly for the same exact experiments, compared\nto the narrow error bars on the MAB, which demonstrates the\nalgorithm’s stability.\nVI. R ELATED WORK\nAutomated physical design tuning. Most commercial\nDBMS vendors nowadays offer physical design tools in\ntheir products [1]–[3]. These tools rely heavily on the query\noptimiser to compare beneﬁts of different design structures\nwithout materialisation [19]. Such an approach is ineffective\nwhen base data statistics are unavailable, skewed, or change\ndynamically [10]. In these dynamic environments, the problem\nof physical design is aggravated: a) deciding when to call a\ntuning process is not straightforward; and b) deciding what is\na representative training workload is a challenge.\nOnline physical design tuning. Several research groups\nhave recognised these problems and have offered lightweight\nsolutions to physical design tuning [11]–[14]. While such\nsolutions are more ﬂexible and need not know the workload\nin advance, they are typically limited in terms of applicability\nto new unknown workloads (generalisation beyond past), and\ndo not come with theoretical guarantees that extend to actual\nruntime conditions. Moreover, by giving the optimiser a central\nrole, the tools remain susceptible to its mistakes [9]. [8]\nextends [1] with the use of additional components, in a\nnarrowed scope of index selection to mimic an online tool.\nThis takes corrective actions against the optimiser mistakes\nthrough a validation process.\nAdaptive and learning indices. Another dimension of\nonline physical design tuning is database cracking and adaptive\nindexing that smooth the creation cost of indices by piggy-\nbacking on query execution [37], [38]. Recent efforts have\ngone a step further and proposed replacing data structures\nwith learned models that are smaller in size and faster to\nquery [39]–[41]. Such approaches are complementary to our\nefforts: once the data structures (or models) are materialised\ninside a DBMS, the MAB framework can be used to automate\nthe decision making as to which data structure should be used\nto speed-up query analysis.\nLearning approaches to optimisation and tuning. Recent\nyears have witnessed new machine learning approaches to\nautomate decision-making processes within databases. For in-\nstance, reinforcement learning approaches have been used for\nquery optimisation and join ordering [42]–[45]. In [9], simpler\napproaches like regression have mitigated the optimiser’s cost\nmisestimates as a path toward more robust index selection.\nWhile [9] shows promising results when avoiding query re-\ngressions, according to the authors, when it starts without\nany knowledge on tuning database (AdaptiveDB), it only\nprovides a marginal workload execution cost improvement\n(and sometimes deterioration) over the traditional optimiser.\nFurthermore, this classiﬁer incurs up to 10% recommendation\ntime, impacting recommendation cost in all cases, especially\nwhere recommendation cost already dominates the cost for\nPDTool (TPC-DS, IMDb). When it comes to tuning, theclosest approaches employ variants of RL for index selection\nor partitioning [25], [35], [46] or conﬁguration tuning [5].\n[35] describes RL-based index selection, which depends solely\non the recommendation tool for query-level recommendations\nand is affected by decision combinatorial explosion, both\nissues addressed in our work. Unlike its more general coun-\nterpart (RL), MABs have advantages of faster convergences\nas demonstrated in Section V-C, simple implementation, and\ntheoretical guarantees.\nVII. D ISCUSSION AND FUTURE AVENUES\nThis paper scratches the surface of the numerous opportu-\nnities for applying bandit learners to performance tuning of\ndatabases. In the following, we discuss a rich research vision\nfor the area.\nReal-world use and integration . Even though we use\nsynthetic benchmarks, they are comprehensive in summarising\nfundamental properties and known pain points of real-life\nworkloads (complexity, data skewness, numerous joins, etc.).\nTherefore our ﬁndings and results generalise to real-life use\ncases as well. Furthermore, MAB only requires execution\nstatistics to function and can be easily integrated with any\nexisting DBMS.\nMulti-tenant and HTAP environments . A crucial advan-\ntage of the MAB setting is theoretical guarantees on the\nﬁtness of proposed indices to observed run-time conditions.\nThis is critical for production systems in the cloud and multi-\ntenant environments [7]–[9], where analytical modelling is\nimpossible due to unpredictable changes in run-time con-\nditions. Similar requirements hold for hybrid OLTP/OLAP\nprocessing environments (HTAP) where the presence of trans-\nactions hinders the usefulness of indices, making analytical\nmodelling next to impossible. The MAB approach on the\ncontrary eschews the optimiser and modelling completely,\nchoosing indices based on observed query performance and\nis thus equally applicable to these challenging environments.\nBeyond index choices . Despite focusing solely on the\ntask of index selection in this paper, the MAB framework\nis equally applicable to other physical design choices, such\nas materialised views selection, statistics collection, or even\nselection of design structures that are a mix of traditional and\napproximate data structures, such as learned models [39] or\nother ﬁne-grained design primitives [41].\nComplementing the recommendation tool . Even though\nthe MAB solution was presented as an alternative to the\nrecommendation tool (PDTool), it can work in concert with\nexisting recommendation systems. MAB can be used as a\nvalidator that starts with the PDTool’s recommendations and\nperforms further run-time optimisations based on observed\nperformance, in a similar vein to [8].\nCold-start problem. Under the current setup, MAB starts\nwithout any secondary indices or knowledge about their\nbeneﬁts, forming a cold-start problem and leading to higher\ncreation costs. While MAB is already superior against PDTool\neven with the creation cost burden (see Section V-B3), even\nfaster convergence and better creation costs can be provided by\n\npre-training models in hypothetical rounds [47] (using what-if)\nor workload forecasting [15] to improve context quality.\nOpportunities for bandit learning. The increased search\nspace of possible design choices calls for advancements to\nbandits algorithms and theory, where unbounded/inﬁnite num-\nbers of arms will be increasingly important. Similarly, various\nﬂavours of physical design might ask for novel bandits that\nadopt the notion of heterogeneous arms (indices, views, or\nstatistics), or hierarchical models where individual choices at\nlower levels (e.g., the choice of indices or materialised views)\ninﬂuence decisions at a higher level (e.g., index merging due\nto memory constraints).\nVIII. C ONCLUSIONS\nThis paper develops a multi-armed bandit learning frame-\nwork for online index selection. Beneﬁts include eschewing\nthe DBA and the (error-prone) query optimiser by learn-\ning the beneﬁts of indices through strategic exploration and\nobservation. We justify our choice of MAB over general\nreinforcement learning for online index tuning, comparing\nMAB against DDQN, a popular RL algorithm based on deep\nneural networks, demonstrating signiﬁcantly faster conver-\ngence of the MAB. Furthermore, our extensive experimental\nevaluation demonstrates advantages of MAB over an existing\ncommercial physical design tool (up to 75% speed up, and\n23% on average), and exempliﬁes robustness to data skew and\nunpredictable ad-hoc workloads.\nREFERENCES\n[1] S. Agrawal, S. Chaudhuri, L. Koll ´ar, A. P. Marathe, V . R. Narasayya,\nand M. Syamala, “Database tuning advisor for Microsoft SQL Server\n2005,” in VLDB , 2004.\n[2] D. C. Zilio, J. Rao, S. Lightstone, G. M. Lohman, A. J. Storm, C. Garcia-\nArellano, and S. Fadden, “DB2 design advisor: Integrated automatic\nphysical database design,” in VLDB , 2004.\n[3] B. Dageville, D. Das, K. Dias, K. Yagoub, M. Za ¨ıt, and M. Ziauddin,\n“Automatic SQL tuning in oracle 10g,” in VLDB , 2004.\n[4] D. Zilio, S. Lightstone, K. Lyons, and G. Lohman, “Self-managing\ntechnology in IBM DB2 universal database,” in ACM CIKM , 2001.\n[5] A. Pavlo, G. Angulo, J. Arulraj, H. Lin, J. Lin, L. Ma, P. Menon, T. C.\nMowry, M. Perron, I. Quah et al. , “Self-driving database management\nsystems,” in CIDR , 2017.\n[6] C. Curino, E. P. C. Jones, R. A. Popa, N. Malviya, E. Wu, S. Madden,\nH. Balakrishnan, and N. Zeldovich, “Relational cloud: a database service\nfor the cloud,” in CIDR , 2011.\n[7] V . R. Narasayya, S. Das, M. Syamala, B. Chandramouli, and\nS. Chaudhuri, “SQLVM: Performance isolation in multi-tenant relational\ndatabase-as-a-service,” in CIDR , 2013.\n[8] S. Das, M. Grbic, I. Ilic, I. Jovandic, A. Jovanovic, V . R. Narasayya,\nM. Radulovic, M. Stikic, G. Xu, and S. Chaudhuri, “Automatically\nindexing millions of databases in microsoft azure sql database,” in\nSIGMOD , 2019.\n[9] B. Ding, S. Das, R. Marcus, W. Wu, S. Chaudhuri, and V . R. Narasayya,\n“AI Meets AI: Leveraging Query Executions to Improve Index Recom-\nmendations,” in SIGMOD , 2019.\n[10] S. Chaudhuri and V . Narasayya, “Self-tuning database systems: A decade\nof progress,” in VLDB , 2007.\n[11] K. Schnaitter, S. Abiteboul, T. Milo, and N. Polyzotis, “On-Line Index\nSelection for Shifting Workloads,” in ICDEW , 2007.\n[12] K.-U. Sattler, E. Schallehn, and I. Geist, “Autonomous query-driven\nindex tuning,” in IDEAS , 2004.\n[13] N. Bruno and S. Chaudhuri, “An Online Approach to Physical Design\nTuning,” in ICDE , 2007.\n[14] ——, “To tune or not to tune?: A lightweight physical design alerter,”\ninVLDB , 2006.[15] L. Ma, D. Van Aken, A. Hefny, G. Mezerhane, A. Pavlo, and G. J.\nGordon, “Query-based workload forecasting for self-driving database\nmanagement systems,” in SIGMOD , 2018.\n[16] M. L. Kersten, S. Idreos, S. Manegold, and E. Liarou, “The researcher’s\nguide to the data deluge: Querying a scientiﬁc database in just a few\nseconds,” VLDB Endow. , 2011.\n[17] S. Idreos, Big Data Exploration . Taylor and Francis, 2013.\n[18] I. Alagiannis, R. Borovica, M. Branco, S. Idreos, and A. Ailamaki,\n“NoDB: Efﬁcient query execution on raw data ﬁles,” in SIGMOD , 2012.\n[19] S. Chaudhuri and V . Narasayya, “AutoAdmin “what-if”; index analysis\nutility,” in SIGMOD , 1998.\n[20] S. Christodoulakis, “Implications of certain assumptions in database\nperformance evaluation,” TODS , 1984.\n[21] V . Leis, A. Gubichev, A. Mirchev, P. A. Boncz, A. Kemper, and\nT. Neumann, “How good are query optimizers, really?” PVLDB , 2015.\n[22] R. Borovica, I. Alagiannis, and A. Ailamaki, “Automated physical\ndesigners: What you see is (not) what you get,” in DBTest , 2012.\n[23] K. E. Gebaly and A. Aboulnaga, “Robustness in automatic physical\ndatabase design,” in EDBT , 2008.\n[24] R. Borovica-Gajic, S. Idreos, A. Ailamaki, M. Zukowski, and C. Fraser,\n“Smooth scan: Robust access path selection without cardinality estima-\ntion,” The VLDB Journal , 2018.\n[25] A. Sharma, F. M. Schuhknecht, and J. Dittrich, “The case for automatic\ndatabase administration using deep reinforcement learning,” 2018, un-\npublished.\n[26] L. Qin, S. Chen, and X. Zhu, “Contextual combinatorial bandit and its\napplication on diversiﬁed online recommendation,” in SDM , 2014.\n[27] L. Li, W. Chu, J. Langford, and R. E. Schapire, “A contextual-bandit\napproach to personalized news article recommendation,” in WWW , 2010.\n[28] B. Oetomo, M. Perera, R. Borovica-Gajic, and B. I. P. Rubinstein, “A\nnote on bounding regret of the C2UCB contextual combinatorial bandit,”\narXiv preprint arXiv:1902.07500 , 2019.\n[29] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher, “An analysis of ap-\nproximations for maximizing submodular set functions–i,” Mathematical\nprogramming , 1978.\n[30] TPC, “TPC-H benchmark,” http://www.tpc.org/tpch/.\n[31] Microsoft, “TPC-H skew benchmark,” https://www.microsoft.com/\nen-us/download/details.aspx?id=52430.\n[32] R. O. Nambiar and M. Poess, “The making of tpc-ds,” in VLDB , 2006.\n[33] P. O. Neil, B. O. Neil, and X. Chen, “Star schema benchmark,” 2009,\nunpublished.\n[34] S. Chaudhuri and V . Narasayya, “Index merging,” in ICDE , 1999.\n[35] D. Basu, Q. Lin, W. Chen, H. T. V o, Z. Yuan, P. Senellart, and S. Bressan,\n“Regularized cost-model oblivious database tuning with reinforcement\nlearning,” in TLDKS XXVIII , 2016.\n[36] H. v. Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning\nwith double q-learning,” in AAAI , 2016.\n[37] S. Idreos, M. L. Kersten, and S. Manegold, “Database cracking,” in\nCIDR , 2007.\n[38] G. Graefe and H. Kuno, “Self-Selecting, Self-Tuning, Incrementally\nOptimized Indexes,” in EDBT , 2010.\n[39] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” in SIGMOD , 2018.\n[40] A. Galakatos, M. Markovitch, C. Binnig, R. Fonseca, and T. Kraska,\n“Fiting-tree: A data-aware index structure,” in SIGMOD , 2019.\n[41] S. Idreos, K. Zoumpatianos, B. Hentschel, M. S. Kester, and D. Guo,\n“The data calculator: Data structure design and cost synthesis from ﬁrst\nprinciples and learned cost models,” in SIGMOD , 2018.\n[42] T. Kaftan, M. Balazinska, A. Cheung, and J. Gehrke, “Cuttleﬁsh: A\nlightweight primitive for adaptive query processing,” 2018, unpublished.\n[43] I. Trummer, S. Moseley, D. Maram, S. Jo, and J. Antonakakis, “Skin-\nnerDB: Regret-bounded query evaluation via reinforcement learning,”\nPVLDB , 2018.\n[44] A. Kipf, T. Kipf, B. Radke, V . Leis, P. A. Boncz, and A. Kemper,\n“Learned cardinalities: Estimating correlated joins with deep learning,”\ninCIDR , 2019.\n[45] R. Marcus and O. Papaemmanouil, “Towards a hands-free query opti-\nmizer through deep learning,” in CIDR , 2019.\n[46] B. Hilprecht, C. Binnig, and U. R ¨ohm, “Towards learning a partitioning\nadvisor with deep reinforcement learning,” in aiDM , 2019.\n[47] C. Zhang, A. Agarwal, H. Daum ´e III, J. Langford, and S. N. Negahban,\n“Warm-starting contextual bandits: Robustly combining supervised and\nbandit feedback,” 2019, unpublished.",
  "textLength": 68025
}