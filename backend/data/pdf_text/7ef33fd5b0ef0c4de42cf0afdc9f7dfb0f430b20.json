{
  "paperId": "7ef33fd5b0ef0c4de42cf0afdc9f7dfb0f430b20",
  "title": "From Natural Language Processing to Neural Databases",
  "pdfPath": "7ef33fd5b0ef0c4de42cf0afdc9f7dfb0f430b20.pdf",
  "text": "From NaturalLanguage Processing to NeuralDatabases\nJames Thorne\nUniversityof Cambridge\nFacebookAI\njt719@cam.ac.ukMajid Yazdani\nFacebookAI\nmyazdani@fb.comMarzieh Saeidi\nFacebookAI\nmarzieh@fb.com\nFabrizio Silvestri\nFacebookAI\nfsilvestri@fb.comSebastian Riedel\nFacebookAI\nUniversityCollegeLondon\nsriedel@fb.comAlon Halevy\nFacebookAI\nayh@fb.com\nABSTRACT\nIn recent years, neural networks have shown impressive perfor-\nmance gains on long-standing AI problems, such as answering\nqueries from text and machine translation. These advances raise\nthequestionofwhetherneuralnetscanbeusedatthecoreofquery\nprocessingtoderiveanswersfromfacts,evenwhenthefactsare\nexpressed in natural language. If so, it is conceivable that we could\nrelaxthefundamentalassumptionofdatabasemanagement,namely,\nthat ourdataisrepresented asfieldsofapre-defined schema.Fur-\nthermore,suchtechnologywouldenablecombininginformation\nfrom text,images, andstructureddata seamlessly.\nThispaperintroduces neuraldatabases ,aclassofsystemsthat\nuseNLPtransformersaslocalizedanswerderivationengines.We\nground the vision in NeuralDB , a system for querying facts repre-\nsentedasshortnaturallanguagesentences.Wedemonstratethat\nrecent naturallanguage processing models,specificallytransform-\ners, can answer select-project-join queries if they are given a set of\nrelevant facts. However, they cannot scale to non-trivial databases\nnoranswerset-basedandaggregationqueries.Basedonthesein-\nsights, weidentifyspecific researchchallengesthatare neededto\nbuild neural databases. Some of the challenges require drawing\nupon the rich literature in data management, and others pose new\nresearch opportunities to the NLP community. Finally, we show\nthat with preliminary solutions, NeuralDB can already answer\nqueriesover thousandsofsentences withvery high accuracy.\nPVLDBReference Format:\nJames Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri,Sebastian\nRiedel, andAlon Halevy. From Natural LanguageProcessingto Neural\nDatabases. PVLDB, 14(6):1033-1039, 2021.\ndoi:10.14778/3447689.3447706\nThisworkis licensed under the CreativeCommons BY-NC-ND4.0International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to viewacopyof\nthislicense. Forany usebeyond thosecovered by thislicense, obtainpermissionby\nemailing info@vldb.org. Copyrightis heldby theowner/author(s).Publication rights\nlicensed to the VLDBEndowment.\nProceedingsof the VLDBEndowment, Vol. 14,No. 6 ISSN2150-8097.\ndoi:10.14778/3447689.34477061 INTRODUCTION\nResearchers have long considered the application of neural nets to\ndata management problems, including learning indices [ 16], query\noptimization, data cleaning and entity matching [ 20,23,32]. In\napplying neural networks to data management, research has so far\nassumedthat the data wasmodeledbyadatabaseschema.\nThe success of neural networks in processing unstructured data\nsuchasnaturallanguageandimagesraisesthequestionofwhether\ntheir use can be extended to a point where we can relax the fun-\ndamentalassumptionofdatabasemanagement,whichisthatthe\ndata we process is represented as fields of a pre-defined schema.\nWhat if, instead, data and queries can be represented as short nat-\nurallanguagesentences,and queriescanbeansweredfromthese\nsentences? Furthermore, what if relevant data from images can be\nseamlesslycombinedwithtextandstructureddata?\nThis paper describes a vision for neural databases and prelim-\ninary empirical evidence of its potential. Neural databases offer\nseveral benefits that database systems have struggled to support\nfordecades.The first,andmostimportantbenefit, isthataneural\ndatabase has no pre-defined schema. Therefore, the scope of the\ndatabase does not need to be defined in advance, and any data\nthatbecomesrelevantas the application isused canbe storedand\nqueried.Thesecondbenefitisthatupdatesandqueriescanbeposed\nin a variety of natural language forms, as is convenient to any user.\nIncontrast,atraditionaldatabasequeryneedstobebasedonthe\ndatabaseschema.Evenwhenthedataismodeledwithamoreflexi-\nbleformalismsuchasRDF,thereisstillasinglenameforanygiven\nrelation, and that name needs to be used in updates and queries.\nThird,withrecentadvancesinmachinetranslation,thelanguage\nofqueriesandanswerscanbedifferentfromthelanguageofthe\ndata in the neural database. A final benefit comes from the fact\nthattheneuraldatabaseisbased onapre-trainedlanguagemodel\nthat already contains a lot of knowledge which can be exploited to\ngeneratebetteranswers to more diversequeries.\nTo ground our vision, we built NeuralDB , a database system\ninwhichupdatesandqueriesaregivenasshortnaturallanguage\nsentences.Figure1showsexamplefactsandqueriesthat NeuralDB\ncan answer. Our preliminary experiments show that NeuralDB\ncananswerselect-project-join-aggregatequeriesoverthousandsof\nnaturallanguagesentences withvery high accuracy.\nBynature,neuraldatabasesarenotmeanttoprovidethesame\ncorrectnessguaranteesofatraditionaldatabasesystem,i.e.,thatthe\nanswerstoqueriessatisfytheprecisebinarysemanticsofthequery\n1033\n\n\nlanguage.Hence,tobeclearaboutthescopeofthevision,neural\ndatabases should not be considered as an alternative to traditional\ndatabasesinapplicationswhere such guarantees are required.\nGiven their benefits, neural databases are well suited for emerg-\ningapplicationswheretheschemaofthedatacannotbedetermined\nin advance and data can be stated in a wide range of linguistic pat-\nterns. A family of such applications arise in the area of storing\nknowledgeforpersonalassistantsthatarecurrentlyavailablefor\nhome use and in the future will accompany Augmented Reality\nglasses. In these applications, users store data about their habits\nand experiences, their friends and their preferences, and designing\na schema for such an application is impractical. Another class of\napplicationsisthemodelingandqueryingofpoliticalclaims[ 34]\n(with the goal of verifying their correctness). Here too, claims can\nbe aboutawide varietyoftopics andexpressedinmanyways.\nThe key technical insight underlying neural databases is that\nstate of the art transformer models [ 36] can be used for query pro-\ncessing.Morespecifically,transformerscancombinemultiplefacts,\neach expressed as a short natural language sentence, to answer\nsimple queries. In effect, transformers can execute joins, selections,\nandprojections.However,transformersarelimitedinthesizeof\nthe inputs they can realistically process, and they fail when higher-\nlevelmathematicalreasoningisinvolved,suchasperformingag-\ngregation. Hence, to answer database-like queries, we need to run\nmultipletransformerinstancesandcombinetheiranswersappro-\npriately.Theselimitationsraisethefirsttwotechnicalchallenges\nfor our vision: (1) finding suitable sets of facts from the database to\nfeed to each transformer instance, and (2) further processing the\nanswers of each transformer instance (e.g., union and aggregation)\nto produce the answer to the query.\nInSection2wedefinethefunctionalityofaneuraldatabasethat\nanswers queries over facts described as natural language sentences\nand describe the underlying NLP technology and its limitations. In\nSection3wedescribe NeuralDB ,ourfirstprototypeneuraldata-\nbasethatprovidesinitialsolutionstothechallengesdescribedabove.\nWealsodescribeasetofexperimentsthat validatethepromiseof\ntransformersandof NeuralDB .Section4describesadditionalre-\nsearchchallenges for realizingthe vision ofneuraldatabases.\n2 A NEURAL DATABASE FORTEXT\nWe ground our vision for neural databases by building a specific\ninstance, NeuralDB , a system for querying facts described in text.\nNeuralDB will demonstrate the challenges involved in building\nneuraldatabases.Section2.1definesthefunctionalityof NeuralDB .\nIn Section 2.2, we explain how NLP technology, namely, transform-\nersoverpre-trainedlanguagemodels,canprovideakeyprocessing\nunitforrealizing NeuralDB ,andpointoutthekeychallengesin\nadaptingtransformersto data management.\n2.1 Problemdefinition\nInNeuralDB ,dataandqueriesarerepresentedassentencesinnat-\nurallanguage,providingtwoofthekeybenefitsofneuraldatabases.\nFirst, the database has no pre-defined schema ≈õ users can mention\nany relationship of interest. Second, the database is usable by a\nbroadersetofusersbecauseupdatesandqueriescanbespecified\ninwhatever linguisticform ismostconvenientto the user.Facts: (4of50 shown)\nNicholaslives inWashington D.C. withSheryl.\nSheryl isNicholas‚Äôs spouse.\nTeuvowasbornin1912 inRuskala.\nIn 1978, Sheryl‚Äôsmother gave birth to her inHuntsville.\nQueries:\nDoes Nicholas‚Äôs spouselive inWashington D.C.?\n(Boolean Join) ‚àí‚ÜíTRUE\nWho isSheryl‚Äôshusband?\n(Lookup) ‚àí‚ÜíNicholas\nWho isthe oldest person inthe database?\n(Max)‚àí‚ÜíTeuvo\nWho isSheryl‚Äôsmother?\n(Lookup) ‚àí‚ÜíNULL\nFigure1:InNeuralDB,factsandqueriesareposedwithshort\nnaturallanguagesentences.Theabovequeriesareanswered\nby our prototype.\nData:Formally, the data in NeuralDB is a set of sentences. In-\ntuitively, a sentence corresponds to a single fact, such as Sue is\nMary‚Äôs mom , orGustavo likes espresso . But in many situations,\nespeciallywhenupdatestothedatabasearespokenbyusers,itis\nmore convenient for sentences to express multiple facts, such as\nKiara is married to Kyrone and they have 3 kids . We refer to the\nlatter sentences as composite and the former as atomic. We mostly\nconsideratomicsentencesinthispaper.Initscurrentform, Neu-\nralDBisaimedtosupportknowledgeexpressedinshortsentences,\nnot entire paragraphs.\nQueries: Formally, a query ùëÑover a database, ùê∑, produces a set\nofanswers: ùëÑ(ùê∑)={ùëé1,...,ùëéùëô}.Whilequeriesareformulatedin\nnatural language, we only consider queries that, if translated to\nSQL, would involve a select-project-join (SPJ) component followed\nbyanaggregation(predicatesinvolvingnumbercomparisonsare\nleft for future work). In our discussion, join queries are ones that\nrequire combining two or more sentences to generate the answers\n(e.g.,WhoworksinacompanyinFrance? ),andaggregationqueries\nareonesthatendwithanaggregationoperator(e.g., howmanykids\ndoesPathave? ).Alookupquery isaquerywhereeachanswercomes\nfrom a singlefact in the database(e.g., Who is Susan‚Äôshusband? ),\nwhether there is a single answer or several. If the query returns\nTrue/False,werefertoitasaBooleanquery.Notethatinourcontext,\nlookup queries are non-trivial because facts in the database are\nexpressed in a variety of linguistic forms. We use the term support\nsetforananswer toaquery(orasub-query)torefer toaminimal\nset of database facts that are needed in order to derive that answer.\n2.2 Answeringqueries usingNLP\nThe NLP problem of question answering from external knowledge\nsuchasWikipedia,(a.k.a. open-bookQA ,orQA)isthemostnatu-\nralpointtoexploretheapplicationofNLPmodelsto NeuralDB .\n1034\n\n\nCommon QA tasks, such as SQuAD, [ 28] and DROP [ 9], require\nmodels to answer reading comprehension queries such as When\nwerethe Normans inNormandy? andWhichkicker had the most\nfieldgoals? byextractingacontiguousspanoftextorperforming\nreasoningover agiven passageoftext.\nIt is important to highlight two key differences between QA and\nNeuralDB , which will have a significant impact on the ultimate\nsolutions. First, in QA, the fact needed to answer a given ques-\ntion is typically located in a paragraph or document with multiple\nsentences about the same subject where this additional context\nmay help information recall. NeuralDB s do not enjoy this locality\nproperty because a query may be dependent on multiple facts that\ncan be anywhere in the database. In fact, models in retrieval-based\ntasks that use Wikipedia as a knowledge source may also leverage\nthe(unique)documenttitletoaidretrieval(forexample,predicting\nthe document key withautoregressive retrieval[7]).\nTheseconddifferenceisthatQAmethodsaredesignedtoanswer\nqueries where a value is given as an input, and the expected result\nis small (e.g., foreign minister of Malaysia ). In contrast, database\nqueries can return sets (e.g., cities of Malaysia ) and aggregations\nappliedto thesesets(such as Count andMax).\nKeepingthesedifferencesinmind,wenowdescribehowNLP\ntechniquescanbeusedtoanswerlimitedformsofdatabasequeries.\n2.2.1 Pre-trainedlanguage modelsand transformers. NaturalLan-\nguage Processing (NLP) has made impressive progress in recent\nyears by building on transformer architectures and pre-trained lan-\nguage models. In addition to QA, such models have led to major\nimprovementsontaskssuchastextsummarizationandmachine\ntranslation. Pre-trained language models such as BERT [ 8], GPT-\n3 [6], RoBERTa [ 21], T5 [27] are neural models trained on large\ncorpora of text. The models are trained by randomly removing cer-\ntaintokensfromthecorpusandtrainingtheneuralnettopredict\nthemissingtoken, givenitscontext (i.e.,thewords precedingand\nfollowing the missing token). At an intuitive level, these models\nobtain two kindsof knowledge (1)theability to makepredictions\nindependent of the exact linguistic form in which knowledge is\nstated [24,33], and (2) some world knowledge that is mentioned\nfrequently in text (e.g., London is the capital of the UK) [ 6,26].1\nPre-trained language models are usually fine-tuned to a given task.\nForexample,forquestionansweringfromtext,thesystemwouldbe\nfurthertrainedwithasetof(question,answer)pairsinordertopro-\nducethefinalmodel.Importantly,sincethepre-trainedlanguage\nmodels capture world knowledge [ 26], fewer examples are needed\nfor the fine-tuningcomparedto training amodelfrom scratch.\nTheTransformermodel[ 36]isthemostcommonneuralarchitec-\nture to operate on pre-trained language models. Transformers [ 36]\ntake as input a sequence of symbols x=(ùë•1,...,ùë•ùëõ). They are\ntypically trained in one of two configurations: encoder only or\nencoder-decoder.Intheformer,eachtokenis encodedtoavector\nrepresentation that is used to predict a label. In the latter, used\ninsequence-to-sequenceapplications(e.g.,questionansweringor\nmachine translation), the decoder produces the outputsequence.\n1Note that the second type of knowledge can also lead to implicit biases. We address\nthispointin Section4John works at Shell\nSarah is a doctor\nMarcello lives in USASarah married JohnFacts\nNeural Query\nProcessor\n(Transformer)\nSarah is from FranceResult set\nSarah\nQuery: Who is\nFrench?\nFigure2:PrototypeneuralqueryprocessorusingaT5Trans-\nformer.Thefactsandthequeryareconcatenatedandgiven\nasinputtothetransformer.Ourresultsshowthattransform-\nerscananswerlookupandjoinquerieswhengiventhesup-\nport set of facts needed to generate an answer, but that the\narchitecture doesnotscaleto many facts.\nIn both configurations, the transformer works in two phases.\nIn the firstphase, thetransformer encodesthe input into aninter-\nmediate representation Z=(ùëß1,...,ùëßùëõ)where the dimension of\nthevectoris fixed,typicallywhere ùëßùëñ‚ààR768.Inthesecondphase,\nthetransformerdecodes ztoproducetheoutput.Forexample,in\nsequence-to-sequence generation, the outputwould be a sequence\noftokens y=(ùë¶1,...,ùë¶ùëô),endingwithaspecialtoken.\n2.2.2 Usingtransformerstoanswerqueries. Transformershavethe\ncapability to answer simple queries over a small amount of data\nexpressed in natural language. To adapt them to NeuralDB , we\nwould first train the transformer with triples of the form (ùê∑,ùëÑ,ùê¥),\nwhereùê∑is a set of facts, ùëÑis a query, and ùê¥is the answer to ùëÑ\noverùê∑.AsshowninFigure2,atquerytimewefeedthefactsand\nthe query to the transformer, and the output is the result set. In\nSection3weshowthattransformersproducehighqualityresults\neven for queriesthat require joining multiple sentences.\nGiven the potential of transformers, we need to address two\nchallengesinordertoadapttransformerstolarge-scalequeryan-\nswering: organizing the data so it can be fed into transformers\ninsmallerchunks,andplanningtoansweraqueryfrommultiple\nsmallerquery processing components.We considereachinturn.\nChallenge1:Supportsetgeneration. State-of-the-arttransformer\nmodelscannotacceptlargeinputsbecausetheirmemoryrequire-\nmentsarequadraticinrepsecttothesizeoftheirinputs.Inpractice,\nit is common to use a maximum input size of 512or1024tokens.\nHence,we cannotencode the entire databaseusing thesemodels.\nThe implication for neural databases is that we need to run\nmultiple transformer instances. Each instance needs a subset of\nfactsfromthedatabaseasinput≈õasupportset≈õfromwhichthe\noutput can be derived. For example, for the query people who live\ninLondon ,eachsupportsetwouldincludeenoughfactstoderive\nwhether aperson lives inLondon ornot.\n1035\n\n\nJohn works at Shell\nSarah is a doctor\nSarah married JohnJohn works at Shell\nSarah is a doctor\nSarah married JohnSarah married JohnFacts Support sets\nNULL\nJohnQuery-based\nderivation\nNeural SPJ\nSupport Set\nGenerator\nQuery : \nHow many peoples'\nspouses are doctors?Neural SPJResult set\nAggregation 1\nFigure3:QueryansweringinNeuralDB.Thesupportsetgeneratorcreatessmallsetsoffactsthatareeachfedintoaseparate\nNeural SPJ operator that runs a single transformer. The results of the individual Neural SPJ operators are either unioned to\nproduce theresultorpassed on to atraditionalaggregation operator.\nInthecontextofQA,thischallengeisaddressedbyemploying\nan information retrieval component that extracts a small subset\nof the relevant facts from the text corpus and feeds them to the\ntransformer.The informationretrievalcomponent caneitherbe a\nsimpleone(e.g.,BM25),ortrainedjointlywiththemodeltolearnto\nextractrelevantpartsofthecorpus[ 12,15,18,25,34,39].However,\nin our context, we need to create multiple support sets, each of\nwhichisfedintoatransformer.\nThe retrieval problem is complicated by the fact that answering\na query may require conditional retrieval from the database. For\nexample, to answer the query Whose spouse is a doctor? we‚Äôd\nfirstneedtofetchspousesandthentheirprofessions.IntheNLP\ncommunity,thisisknownasmulti-hopqueryanswering[ 5],which\nhas recently become an active area of research but restricted to the\ncase of retrieving or generating a single answer. While research in\nNLPfocusesonexpandingasinglesetofinformationtogenerate\na single answer for a question, in NeuralDB we may need to\nperformmulti-hopretrievalforsetsoffactstosupportaggregation.\nForexample,inthequery Howmanypeople‚Äôsspousesaredoctors?\nmulti-hopretrievalisrequiredfor every spousefact.\nChallenge2:Answercomposition. Sincethetransformerisper-\nforming local query answering, we need a component that can\nassembletheresultsoftheindividualtransformerstoproducean\nanswer for the full query. Furthermore, as we show in Section 3,\ntransformersare not adept at computingaggregates. Hence, if the\nquerycontainsaggregation,thequeryplanneedstocomputean\nintermediateresultsetthatthengetsaggregatedusingatraditional\noperator. In the next section, we describe an architecture that com-\nbines multiple transformers and is able to answer queries that can\nbe expressed as a select-project-join followed by an aggregate. For\nmorecomplicatedqueries,wemayneedacomponentthatbuilds\nan explicit query planas inatraditional databasesystem.\n3 A PROOFOFCONCEPT\nWe now describe NeuralDB ‚Äôs architecture that includes initial\nsolutions to the two challenges outlined above. We experimentally\nvalidate that transformers can locally answer certain classes of(1)DoesNicholas‚Äôsspouse livein WashingtonD.C.?\n{Nicholaslivesin WashingtonD.C. with Sheryl.,\nSheryl isNicholas‚Äôsspouse.} ‚àí‚ÜíTRUE\n(2)Whoisthe oldestperson in the database?\n{Teuvowas born in 1912.} ‚àí‚Üí(Teuvo, 1912)\n(3)DoesNicholas‚Äôsspouse livein WashingtonD.C.?\n{Teuvowas born in 1912.} ‚àí‚ÜíNULL\nFigure 4: Examples of the intermediate results that are pro-\nduced by theNeuralSPJ operator.\ndatabase queries and then show that NeuralDB can build on that\ntoanswerqueriesonlargedatabases.Thearchitectureof NeuralDB\nisshowninFigure 3andisbasedonthe following ideas.\nRunningmultipletransformersinparallel: Asnotedearlier,in\npractice,transformerscanonlytakearelativelysmallinput.Hence,\nto scale to larger data sets, NeuralDB runs multiple copies of a\nneuralSPJ operatorin parallel, each outputtingstructured results.\nWhenqueriesdon‚Äôt involveaggregation,theunionoftheoutputs\noftheneuralSPJoperatorsistheanswertothequery.Whenthe\nquerydoesinvolveaggregation,thesemachine-readableoutputs\narefedintotheaggregationoperator.Figure4showsexamplesof\ntheoutputoftheneuralSPJoperator(andthereforetheformofthe\ntraining data itrequires).\nThefactsgiventoeachtransformeraregivenbyasupportset\ngenerator (SSG). Intuitively, the support set of an answer to an SPJ\nqueryincludestheleavesofitsderivationtree.TheSSGcreatessets\noffactsthatcontainsupportforananswer,withminimalirrelevant\nfacts.\nAggregation with a conventional operator: Since the neural\nSPJ was designed to output structured results, our architecture can\nuse a separate conventional aggregation operator. Using a separate\naggregation overcomes thelimitation on transformersconcerning\naggregation queries. The aggregation operator is selected through\naclassifier that mapsaquery to an aggregation function.\n1036\n\n\n3.1 Implementation\nWeimplementedaversionof NeuralDB followingtheabovearchi-\ntecture.Wedevelopedasupportsetgenerator(describedindetail\nin [35]) that incrementally builds support sets by starting with sin-\nglefactsandaddingmore.Thedecisionaboutwhichfactstoadd\ntoasupportsetisformulatedasaclassificationproblemthatcan\nbetrainedbytheneuralSPJitself.Note thattrainingthissupport\nset generator did not require additional labeled data as a distant\nsupervision signal comes from measuring how changing the input\nfactschangethe answer generatedbythe model.\n3.1.1 Experimental setup. Training any neural system requires\na supervision signal. We generate training data in a controlled\nmanner byconvertingknowledgebase tuplesfromWikidata [ 37]\n(e.g.,(Zuckerberg,ceoOf,Facebook) )tonaturallanguagesentences\n(e.g,The CEO of Facebook is Zuckerberg ). Because of the scale\nof Wikidata, it is possible to generate large numbers of training\ninstancesabouta wide rangeof relationships andentities. For our\nfirstexperiment(testingthetransformersthemselves,Section3.2.1),\nitissufficienttotrainwithtriplesoftheform (ùê∑,ùëÑ,ùê¥),whereùê∑\nis a set of facts, ùëÑis a query, and ùê¥is the answer to ùëÑoverùê∑.\nHowever, when testing NeuralDB (Section 3.2.2) we need slightly\nmore refined labels in order to train the neural SPJ to generate\nintermediate results (see Figure 4).\n3.1.2 Trainingdatageneration. Forthis proofofconcept,we con-\nsider simple template-based data generation over 27 relations from\nWikidata and their inverse. For every Wikidata relationship we\nconsider,wecreatelinguisticvariationsinboththefactsandthe\nqueriesbyconstructingmultiplenaturallanguagetemplatesthat\nvary pronouns, temporal expressions, and phrasing of the fact and\nquery. Each template has a placeholder for the subject and the\nobject, e.g., $O is the place of birth of $S . We then generate dif-\nferent data instances by substituting entities from Wikidata for\nthe placeholdersin creating training, validation, and held-out test\nsets,whicharedisjointbysubject.Ourdatawasgeneratedusing\n632 templates: 115 for facts and 517 for the different query types\noverthe27relations,foreachfactandquery,randomlychoosing\nan appropriate template for the relation or its inverse. The held-\nout test database contains 8400 facts and 14000 queries over this\ndatabasewithreferenceanswersthatareusedforscoring.Thisis\nordersofmagnitudelargerthancanbefeasiblyencodedwiththe\nNLP-only model indicated in Figure 2. For training, we generate\napproximately80,000traininginstances,ensuringanequalbalance\nfor each of the aggregation operations. For aggregation, we con-\nsiderCount,Min,andMax.Thesupervisionfortheclassifierwhich\nselectsthe aggregation functionispart ofthe templates.\n3.2 Results\n3.2.1 Testing transformers. We demonstrate the limitations of a\nstandalonetransformermodeldesignedforconditionallanguage\ngeneration when applied to neural query processing (Figure 2).\nWithout adaptation, this model can only feasibly encode a maxi-\nmumof50facts.Weuseasubsetofourtrainingdatatofine-tune\na T5 transformer model[27], constructing databases of 50facts to\navoid memory limitations when encoding the entire database. We\njointly encode allfactsbyconcatenating themwiththe query.Table1:Exactmatchscoresfor NeuralDB .NotethatIR(k=5)\nwith a single T5 transformer cannot accurately answer ag-\ngregation and join queries. We use 200 parallel SPJ opera-\ntors(4xV100GPUs,batchsize=50)overadatabasewith8400\nfacts, averaged over14000queries.\nMethodExact Match (%)\nCount Min/Max Sets Atomic Joins\nNeuralDB 79.45 100.00 91.91 97.90 79.29\nTF¬∑IDF+T5 31.06 0.00 44.25 98.05 68.02\nDPR+T5 38.07 21.19 54.55 97.38 58.64\nOur results show that for lookup and join queries the model at-\ntainednearperfectscores(above99%exactmatch)onthetemplate-\ngenerated data. The fact that the model had high scores for queries\nthat require the combination of multiple facts indicates that the\ntransformeris ableto combine information from multiple sources\nwhen generating an answer to the user query. Furthermore, gener-\nating correctanswers whenencoding theentire database without\nfiltering out the irrelevant facts suggests that the model is resilient\ntoexposuretoirrelevantfacts.However,themodelperformspoorly\nforqueriesthatrequireanaggregation(forqueriesrequiringacount\noperation,exactmatchwas57%)orwhenthequeryresultisalarge\nset.Importantly,theresultsindicatethatthemodelcanberobust\nto simplelinguisticvariations when processing queries.\n3.2.2 Large-scaleNeuralDBs. Table1compares NeuralDB with\na baseline (bottom two rows) that first applies IR techniques to\nretrieve the relevantfactsandfeedsthose to atransformer. Inthe\nNeuralDB version,thesupportsetsgeneratedbytheSSGareinput\ntoSPJoperatorsinparallelandthenaggregated.Asthetableshows,\nNeuralDB achievesthehighestEMaccuracyonalltypesofqueries.\nWhile thesenumbers cannotbe directlycompared (as NeuralDB\nrequires different supervision to generate intermediate results),\nNeuralDB makes substantial improvements over several query\ntypes from the same sourcedata.\n3.2.3 Summary. Theinitialexperimentsconfirmthebasictenetsof\nourapproachtoneuraldatabasesthatareembodiedinthearchitec-\nture ofNeuralDB :(1)if there were a way tofeedthe transformer\nthe relevant facts from the database, it can produce results with\nreasonable accuracy, (2) aggregation queries need to be performed\noutside of the neural machinery, and (3) in order to handle queries\nthat result in sets of answers and in order to prepare sets for subse-\nquent aggregation operators, we need to develop a neural operator\nthat canprocess individual (or small sets of)facts in isolationand\nwhoseresultsoutputtedastheanswerorfedintoaconventional\n(i.e.non-neural) aggregation operator.\n4 A RESEARCH AGENDA\nOurNeuralDB implementation provided initial solutionsto Chal-\nlenges 1 & 2. This demonstrated that it is possible to build a neural\ndatabase that accurately answers queries over large numbers of\nfactsstatedwithshortsentences.FurtheraddressingChallenges1&\n2drawsupontherichliteraturedevelopedinthedatamanagement\ncommunity regarding indexing, view materialization, and query\n1037\n\n\nprocessing.Inparticular,supportsetgenerationcangreatlybenefit\nfrom indexing and view materialization techniques that enable the\nsystem to efficiently refine the support sets at query time. With\nadditional advances, we should be able to scale neural databases\nto larger data, support more complex queries, and increase their\naccuracy.This section outlinesadditionalresearchchallenges.\nChallenge3: Deeperunderstandingofsemantics. Tosupportawider\nrangeofapplications,aneuraldatabaseneedstocorrectlyhandle\nthesemanticsofnumbers(comparisonsandoperations)anddata\ntypes(e.g.,addresses,timepoints).Anadditionalchallengeconcerns\nunderstanding dependencies and hence identifying which updates\nshouldreplacepreviousfactsandwhichshouldnot.Forexample,\nif the fact, Mariah is unemployed , was in the database and later\nthe fact,Mariah works for Apple , was added, then the first fact\nshould be removed (or at least, apply only to queries about the\npast).However,thesamedoesnotholdforthefacts Kasperlikes\nteafollowedbythefact Kasperlikescoffee .Alloftheseissuespose\nchallenges for the development of transformers in NLP, but they\nare guidedbythe needsofneuraldatabases.\nChallenge 4: Multi-modalneuraldatabases. Combiningmultiple\nmodalitiessuchasimages,text,structureddataandaudioinaneural\ndatabase is an exciting area of investigation, building on the fact\nthattransformershavealsobeenappliedtootherdatatypes.For\nexample,consideraquerythatcombinesanimagedatabasewith\nknowledgefromtext,askingfor photosthatincludeanobjectused\nto brew coffee . Such extensions would benefit from recent progress\non visual query answering systems [ 2,4], that can already answer\ncertainclassesofqueriesonthe imagesthemselves.\nChallenge5: Obtainingtrainingdataandtransferlearning. Finding\ntraining data for a neural database can be challenging in some\ncontexts. We expect that over time, the community will create\npublic sets of training data for common relationships. A promising\napproach for creating training data sets is to find sentences in\nWikipedia that express known triples from Wikidata or to use\nWikidatatriplestogeneratesyntheticsentencesasinT-REx[ 10]\nand KELM [ 1] datasets, respectively. Leveraging similar parallel\ncorporaoftextandstructureddatacanyieldadditionaltrainingdata.\nAdvancesinNLPonlearningwithoutparalleldata[ 17],few-shot\nlearningandlargelanguagemodelsshouldconsiderablyreducethe\nnumber oftraining instancesneeded.\nAkeyfactorinassessingthecostoftrainingdataiswhetherit\nneeds to cover all the relationships that occur in the queries. For\nexample, if the training data has no mention of people‚Äôs hobbies,\ncan we still answer a query such as, what are Ruth‚Äôs hobbies?\nDevelopments in transfer learning [ 14] could be applied to this\ncontext. It is also reasonable to expect that as we obtain training\ndata for many relations, transfer will occur more naturally because\nthe linguistic patterns in queries and facts are limited. An initial\nexperiment describedin[35] substantiates this intuition.\nChallenge6: Mitigatingbiases. Apossibledownsideofusingneural\ntechniquesinadatabaseisthepotentialforbiasfromtheunderlying\nlanguage model. For example, suppose our database included facts\nsayingthatJohnandJaneworkatahospital,butwhenweaskedfor\ntheirprofession,thesystemanswers doctorforJohnand nursefor\nJane.Currently,thereisnogoodwayofdistinguishingbiasedfromunbiasedknowledgeinalanguagemodel.Apossibleapproachto\naddressing this issue is to design a separate module that attacks\nthedatabasewithqueriesthatattempttodiscoverbiases.Then,we\ncould devise safeguards within the database that ensure that we\ndon‚Äôtuse such biasedknowledge inansweringqueries.\nChallenge 7: Applying neural components to existing data man-\nagementarchitectures. InNeuralDB ,theentiredatabasewastext.\nHowever, there are settings in existing DBMSs where structured\ndata is mixed with text and images. The challenge is to extend\nexistingDMBSsandtheirqueryprocessorstoincorporateneural\noperators, thereby allowing a wider range of queries over these\ndata.Forexample,considerqueryingaproductuserreportdatabase\nfor reports about a malfunctioning battery that also have an image\nofabattery accompanying them.\n5 RELATED WORK\nSpacedoesnotpermitacomprehensivedescriptionofrelatedwork.\nThe differences between neural databases and question answering\nover text was covered in Section 2.2. Bridging the gap between\nunstructured natural language data and database-style querying\nhas been a long-standing theme in database research [ 3,13,19,30,\n39,40].Unliketheaboveworks,neuraldatabasesdonottrytomap\ndataorqueriesintoapre-definedschemabutoperatedirectlyon\nfacts describedas short naturallanguagesentences.\nInthespirittoNeuralTuringMachines[ 11]andMemoryNet-\nworks [31] architectures, an alternative way of building neural\ndatabasesistoencodeallthefactsinthedatabasetoaneuralmem-\noryandbuildmachinerytoread,write,andreasonontopofthis\nneuralmemory.However,suchanapproachwouldnothavecon-\ntrol and transparency: It is challenging to remove facts from the\ndatabaseorcheckwhetheraparticularfactexists.Also,itwould\nnotbepossibletoexplainqueryresults.Furthermore,thesearchi-\ntecturesperformwellonbAbI[ 38]taskswherethenumberoffacts\nis limited, and mainly lookup or simple reasoning is needed. There\nalso have been considerable efforts inmixing traditional symbolic\nreasoning or data management algorithms with neural network\narchitectures, such as differentiable backward chaining [ 29] and\ndifferentiableprolog[ 22].Insteadof≈Çneuralizing≈æexistingsymbolic\nreasoners, in our work, we start with a scalable neural architecture\nandsupportitwithsymboliccomputationonlywherenecessary.\nThis enables us to directly leverage the rapid progress made in\nretrievalaugmentedQA models andensures scalability.\n6 CONCLUSIONS\nWeintroducedneuraldatabases,anewclassofdatabasesystems\nthat use neural reasoning, and are therefore able to answer queries\nfrom data expressed as natural language sentences that do not\nconform to a pre-defined schema. The design of the NeuralDB\narchitecturewasbasedonacarefulexaminationofthestrengthsand\nweaknessesofcurrentmodelsusedfornaturallanguageprocessing,\nnamely transformers. Neural databases open up many avenues\nof research because they provide a path for leveraging the latest\nadvances in NLP and computer vision to enable queries over more\nflexibleandvariedrepresentations of data.\n1038\n\n\nREFERENCES\n[1]Oshin Agarwal, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. Large scale\nknowledgegraphbasedsyntheticcorpusgenerationforknowledge-enhanced\nlanguagemodelpre-training,2020.\n[2]Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module\nnetworks. In 2016IEEEConferenceonComputerVisionandPatternRecognition,\nCVPR2016,LasVegas,NV,USA,June27-30,2016 ,pages39≈õ48.IEEEComputer\nSociety, 2016.\n[3]I Androutsopoulos, G D Ritchie, and P Thanisch. Natural Language Interfaces to\nDatabases- anIntroduction. Natural Language Engineering , 1(1):29≈õ81, 1995.\n[4]Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,\nC. Lawrence Zitnick, and Devi Parikh. VQA: visual question answering. In 2015\nIEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile,\nDecember 7-13,2015 , pages2425≈õ2433.IEEE Computer Society, 2015.\n[5]AkariAsai,KazumaHashimoto,HannanehHajishirzi,RichardSocher,andCaim-\ningXiong.Learningtoretrievereasoningpathsoverwikipediagraphforquestion\nanswering. In InternationalConference onLearning Representations , 2020.\n[6]Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafullaDhariwal,ArvindNeelakantan, Pranav Shyam, GirishSastry,Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter,\nChristopherHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and DarioAmodei. Language Models areFew-Shot Learners. 2020.\n[7]Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. Autore-\ngressive entity retrieval. In International Conference on Learning Representations ,\n2021.\n[8]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:\nPre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding. In\nProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociation\nforComputationalLinguistics:HumanLanguageTechnologies,Volume1(Longand\nShort Papers) , Minneapolis, Minnesota,2019.\n[9]DheeruDua,YizhongWang,PradeepDasigi,GabrielStanovsky,SameerSingh,\nand Matt Gardner. DROP: A Reading Comprehension Benchmark Requiring\nDiscrete Reasoning Over Paragraphs. In Proceedings of the 2019 Conference\noftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:\nHumanLanguageTechnologies,Volume1(LongandShortPapers) ,pages2368≈õ2378,\nMinneapolis, Minnesota,jun2019.Associationfor Computational Linguistics.\n[10]Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon\nHare, Frederique Laforest, and Elena Simperl. T-REx: A large scale alignment\nofnaturallanguage withknowledge basetriples. In Proceedingsof theEleventh\nInternational Conference on Language Resources and Evaluation (LREC 2018) ,\nMiyazaki, Japan,May 2018.European Language ResourcesAssociation(ELRA).\n[11]AlexGraves,GregWayne,andIvoDanihelka. Neuralturingmachines. CoRR,\nabs/1410.5401, 2014.\n[12]KelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMing-weiChang.\nREALM : Retrieval-Augmented Language ModelPre-Training,2020.\n[13]AlonY.Halevy,OrenEtzioni,AnHaiDoan,ZacharyG.Ives,JayantMadhavan,\nLukeK.McDowell,andIgorTatarinov. Crossingthestructurechasm. In CIDR\n2003,FirstBiennialConferenceonInnovativeDataSystemsResearch,Asilomar,CA,\nUSA, January5-8,2003, Online Proceedings . www.cidrdb.org,2003.\n[14]NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,BrunaMorrone,Quentin\nDe Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.\nParameter-efficient transfer learning for NLP. In Kamalika Chaudhuri and Rus-\nlan Salakhutdinov, editors, Proceedings of the 36th International Conference on\nMachine Learning , volume 97 of Proceedings of Machine Learning Research , pages\n2790≈õ2799,LongBeach,California, USA,09≈õ15Jun2019.PMLR.\n[15]Vladimir Karpukhin, Barlas Oƒüuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey\nEdunov,DanqiChen,andWen-tauYih.DensePassageRetrievalforOpen-Domain\nQuestion Answering. 2020.\n[16]Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. The\ncase for learned index structures. In Gautam Das, Christopher M. Jermaine, and\nPhilipA.Bernstein,editors, Proceedingsofthe2018InternationalConferenceon\nManagement of Data, SIGMOD Conference 2018, Houston, TX, USA, June 10-15,\n2018, pages489≈õ504. ACM,2018.\n[17]Guillaume Lample, Alexis Conneau, Marc‚ÄôAurelio Ranzato, Ludovic Denoyer,\nand Herv√© J√©gou. Word translation without parallel data. In 6th International\nConference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April\n30 -May 3, 2018, Conference Track Proceedings . OpenReview.net, 2018.\n[18]Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir\nKarpukhin,NamanGoyal,HeinrichK√ºttler,MikeLewis,Wen-tauYih,TimRock-\nt√§schel,SebastianRiedel,andDouweKiela. Retrieval-AugmentedGenerationfor\nKnowledge-IntensiveNLPTasks. In NeurIPS, 2020.\n[19]Fei Li and H V Jagadish. Constructing an Interactive Natural Language Interface\nfor Relational Databases. Proceedings of the VLDB Endowment2 , 8(1):73≈õ84, 2014.\n[20]Yuliang Li, Jinfeng Li, Yoshihiko Suhara, AnHai Doan, and Wang-Chiew Tan.\nDeep entity matching with pre-trained language models. Proc. VLDB Endow. ,14(1):50≈õ60,September 2020.\n[21]YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,Omer\nLevy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov. RoBERTa:ARobustly\nOptimized BERT PretrainingApproach. 2019.\n[22]Pasquale Minervini, Matko Bosnjak, Tim Rockt√§schel, Sebastian Riedel, and\nEdward Grefenstette. Differentiable reasoning on large knowledge bases and\nnatural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence,\nAAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence\nConference,IAAI2020,TheTenthAAAISymposiumonEducationalAdvancesin\nArtificialIntelligence,EAAI2020,NewYork,NY,USA,February7-12,2020 ,pages\n5182≈õ5190.AAAI Press,2020.\n[23]Sidharth Mudgal, Han Li, Theodoros Rekatsinas, AnHai Doan, Youngchoon\nPark,GaneshKrishnan,RohitDeep,EstebanArcaute,andVijayRaghavendra.\nDeeplearning forentitymatching:Adesign spaceexploration. InGautamDas,\nChristopher M. Jermaine, and Philip A. Bernstein, editors, Proceedings of the\n2018International Conferenceon Management ofData,SIGMOD Conference2018,\nHouston, TX, USA, June 10-15, 2018 , pages19≈õ34. ACM,2018.\n[24]MatthewPeters,MarkNeumann,LukeZettlemoyer,andWen-tauYih. Dissecting\nContextual Word Embeddings: Architecture and Representation. In Proceedings\nofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing ,pages\n1499≈õ1509,Brussels, Belgium,2018.Associationfor Computational Linguistics.\n[25]Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani,\nNicolaDeCao,JamesThorne,YacineJernite,VassilisPlachouras,TimRockt√§schel,\netal. Kilt: a benchmark forknowledge intensive language tasks. arXiv preprint\narXiv:2009.02252 , 2020.\n[26]Fabio Petroni, Tim Rockt√§schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\nAlexander H. Miller, and Sebastian Riedel. In Proceedings of EMNLP-IJCNLP ,\nHongKong, China.\n[27]Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the Limits\nof Transfer Learning with a Unified Text-to-Text Transformer. Journal of Ma-\nchine Learning Research , 21:1≈õ67, 2020.\n[28]Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don‚Äôt know:\nUnanswerable questions for SQuAD. ACL 2018 - 56th Annual Meeting of the\nAssociation for Computational Linguistics, Proceedings of the Conference (Long\nPapers), 2:784≈õ789, 2018.\n[29]Tim Rockt√§schel and Sebastian Riedel. End-to-end Differentiable Proving. In\nI Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, and\nR Garnett,editors, Advances in Neural Information Processing Systems30 ,pages\n3788≈õ3800.Curran Associates,Inc.,2017.\n[30]MarziehSaeidi,MaxBartolo,PatrickLewis,SameerSingh,TimRockt√§schel,Mike\nSheldon, Guillaume Bouchard, and Sebastian Riedel. Interpretation of natural\nlanguage rules in conversational machine reading. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing . Association for\nComputational Linguistics,2018.\n[31]Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory\nnetworks. In Advancesinneuralinformation processingsystems .\n[32]NanTang,JuFan,FangyiLi,JianhongTu,XiaoyongDu,GuoliangLi,SamMadden,\nand Mourad Ouzzani. Relational pretrained transformers towards democratizing\ndata preparation [vision]. CoRR, abs/2012.02469,2020.\n[33]IanTenney,PatrickXia,BerlinChen,AlexWang,AdamPoliak,R.ThomasMcCoy,\nNajoungKim,BenjaminVanDurme,SamuelRBowman,DipanjanDas,andEllie\nPavlick. What do you learn from context? Probing for sentence structure in\ncontextualized word representations. ICLR, pages1≈õ17,2019.\n[34]JamesThorne, AndreasVlachos, ChristosChristodoulopoulos, and ArpitMittal.\nFEVER: a large-scale dataset for Fact Extraction and VERification. In Proceedings\nof the 2018 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Volume 1 (Long Papers) ,\npages 809≈õ819, New Orleans, Louisiana, 2018. Association for Computational\nLinguistics.\n[35]JamesThorne,MajidYazdani,MarziehSaeidi,FabrizioSilvestri,SebastianRiedel,\nand AlonY. Halevy. Neural databases. CoRR, abs/2010.06973,2020.\n[36]AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LilonJones,Aidan\nGomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In 31st\nConference on Neural Information Processing Systems (NIPS 2017) , Long Beach,\nCA, USA,2017.\n[37]DennyVrandeƒçiƒáandMarkusKr√∂tzsch. Wikidata:afreecollaborativeknowl-\nedgebase. Communications ofthe ACM , 57(10):78≈õ85, 2014.\n[38]Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van\nMerri√´nboer,ArmandJoulin,andTomasMikolov. Towardsai-completequestion\nanswering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698 , 2015.\n[39]TomerWolfson,MorGeva,AnkitGupta,MattGardner,YoavGoldberg,Daniel\nDeutch, and Jonathan Berant. Break It Down: A Question Understanding Bench-\nmark.TransactionsoftheAssociationforComputationalLinguistics ,8:183≈õ198,\n2020.\n[40]JichuanZeng,XiVictoriaLin,CaimingXiong,RichardSocher,MichaelR.Lyu,\nIrwin King, and Steven C. H. Hoi. Photon: A robust cross-domain text-to-sql\nsystem.\n1039\n",
  "textLength": 43524
}