{
  "paperId": "973007aef333350731bbf173e9d7abd312ab00d9",
  "title": "Learning-Augmented Streaming Codes are Approximately Optimal for Variable-Size Messages",
  "pdfPath": "973007aef333350731bbf173e9d7abd312ab00d9.pdf",
  "text": "1\nLearning-Augmented Streaming Codes are\nApproximately Optimal for Variable-Size Messages\nMichael Rudow and K.V . Rashmi\nAbstract\nReal-time streaming communication requires a high quality-of-service despite contending with packet loss. Streaming codes are\na class of codes best suited for this setting. A key challenge for streaming codes is that they operate in an “online” setting in which\nthe amount of data to be transmitted varies over time and is not known in advance. Mitigating the adverse effects of variability\nrequires spreading the data that arrives at a time slot over multiple future packets, and the optimal strategy for spreading depends\non the arrival pattern. Algebraic coding techniques alone are therefore insufﬁcient for designing rate-optimal codes. We combine\nalgebraic coding techniques with a learning-augmented algorithm for spreading to design the ﬁrst approximately rate-optimal\nstreaming codes for a range of parameter regimes that are important for practical applications.\nI. I NTRODUCTION\nReal-time communication arises in many popular applications, including V oIP, online gaming, and videoconferencing. These\napplications involve a sender transmitting packets of information to a receiver over a lossy channel. The receiver must decode\nthe data within a strict playback deadline. In many scenarios, one cannot retransmit the lost packets because doing so requires\nan extra round trip time and can thus exceed the maximum tolerable latency [2]. Instead, one can use erasure coding to recover\nlost packets.\nWhile erasure coding has been well studied, real-time communication has several unique aspects that require a new “streaming\nmodel,” as was introduced by Martinian and Sundberg [3]. During each time slot, i, a “message packet,” denoted as S[i], of size\nkarrives at a sender. The sender then transmits a “channel packet,” denoted as X[i], of sizento a receiver over a burst-only\nloss channel. The sender must recover S[i]by time slot (i+\u001c). An overview of the model is presented in Figure 1, with\nthe sender, channel, and receiver appearing in blue. (The component “side information” is introduced later.) Coding schemes\nthat recover lost symbols from each message packet \u001ctime slots later have signiﬁcantly higher rates than alternatives, such\nas interleaved maximal distance separable codes, that recover all lost symbols together by \u001ctime slots after the ﬁrst message\npacket for which the symbols are lost [3].\nNumerous works [4]–[25] have employed tools from algebraic coding theory to design optimal streaming codes for various\nsettings where the sizes of message packets and channel packets are ﬁxed in advance. These regimes are suitable for applications\nthat involve sending ﬁxed quantities of data, such as V oIP when audio packets are sent uncompressed.\nIn contrast, many applications, such as videoconferencing, involve transmitting variable amounts of data. A new streaming\nmodel incorporating message packets of varying sizes was introduced in [26]. Two factors affect the optimal rate in this setting.\nFirst is the sequence of the sizes of all message packets of a transmission, called the “message packet size sequence.” Second\nis the maximum number of time slots the receiver can wait to decode each message packet under a lossless transmission, called\n“lossless delay” and denoted as \u001cL.\nThe optimal rate for “ofﬂine” schemes (which know the message packet size sequence) exceeds that of “online” schemes\n(which cannot access the sizes of message packets for future time slots) in all but two settings [27]. In one setting where \u001cL\nhas its maximum possible value, a technique for spreading the symbols of each message packet over several channel packets\nindependently of all other message packets is rate optimal. The other setting, where \u001cLhas its minimum possible value (i.e.,\n0), requires sending the symbols of each message packet in the corresponding channel packet. Therefore, information about the\nsizes of the future message packets does not help. Rate-optimal constructions, or even approximately rate-optimal constructions,\nare not known even for the ofﬂine setting for all remaining parameter regimes.\nWe consider the setting of \u001cL= 1, which is important since it is the smallest value for which the symbols of a message\npacket can be spread over multiple channel packets. Spreading helps to signiﬁcantly mitigate the adverse effect of variability of\nthe sizes of message packets on the rate [26]. On the other hand, maintaining a small value of \u001cLis crucial for latency-sensitive\napplications, as the delay of \u001cLextra time slots may be incurred for decoding every message packet.\nWe ﬁrst consider the ofﬂine setting and decompose the code design into two distinct challenges. First, how can we best\nspread message symbols over channel packets? Second, how can we send the minimum necessary number of parity symbols to\nensure that each message packet is decoded in time, given any choice of how to spread message symbols? We use an integer\nprogram ofﬂine to determine how to optimally spread message symbols over channel packets. We then introduce a building\nM. Rudow and K.V . Rashmi are with the Computer Science Department, Carnegie Mellon University, Pittsburgh, PA, 15213 USA.\nThis is an extended version of the IEEE ISIT 2022 paper [1] with the same titlearXiv:2205.08521v1  [cs.IT]  17 May 2022\n\n2\nSenderPacket loss\nchannel\nChannel\nPacketMessage\nPacketDecode  \n \nwithin    time\nslotsor\nReceiver\nOracle\nFig. 1: Overview of the model for streaming codes.\nblock for constructing a rate-optimal streaming code for any given choice of how to spread message symbols over channel\npackets. One ﬁnal challenge remains: how can we construct a rate-optimal online streaming code?\nWe address the problem by combining machine learning with tools from algebraic coding theory. We take a learning-\nbased approach, relying on techniques similar to empirical risk minimization to convert the optimal ofﬂine solution into an\napproximately optimal online one that maximizes the expected rate. Our proposed method determines how to spread symbols\nonline, and then the building block construction is applied.\nOur methodology can be viewed as using a “learning-augmented algorithm”—a topic that has recently surged to prominence,\ntackling problems in other domains, such as caching [28], metric task systems [29], bloom ﬁlters [30], learned index struc-\ntures [31], scheduling [32], etc. [33]–[37]. However, to the best of our knowledge, the powerful paradigm of learning-augmented\nalgorithms has not been applied to design coding schemes until now.\nUsing machine learning models to perform encoding and/or decoding has received considerable attention in the recent\npast, including for channel coding [38]–[46], decoding with feedback [47], [48], approximate coded computation [49]–[51],\nMIMO [52], [53], etc. [54]–[58]. Most of these works use neural networks to handle encoding and or decoding in a black-box\nmanner. In contrast, our work applies machine learning only to a small portion of the problem: to determine how to spread\nmessage symbols to address the uncertainty in the sizes of the future message packets. We then leverage tools from classical\nalgebraic coding theory to solve the rest of the problem. This makes the learning part lightweight and interpretable and allows\nfor theoretical guarantees on the rate.\nII. M ODEL AND BACKGROUND\nWe now present the system model used in this work, which is built on top of the model of streaming codes for variable-size\nmessages [26], [27].\nA transmission occurs over (t+ 1) time slots for a non-negative integer t. During the ith time slot for i2f0;:::;tg, the\nsender obtains a message packet, S[i]2Fki, where Fis a ﬁnite ﬁeld, and ki2f0;:::;mgfor a positive integer m. The\nsender also receives “side information,” O[i], that captures the differences between the online and ofﬂine settings. In the ofﬂine\nsetting, which assumes knowledge of the future, the side information is the sizes of the future message packets. In the online\nsetting, the side information is independent Ssamples from the distribution of the sizes of future message packets for some\npositive integerS. LetDk0;:::;kibe the conditional distribution of ki+1;:::;ktgivenk0;:::;ki. Then,\nO[i] =(\n(ki+1;:::;kt) if ofﬂine\n\u0010\nk(j)\ni+1;:::;k(j)\nt\u0011\n\u0018Dk0;:::;kijj2f0;:::;S\u00001g\u000b\nif online:(1)\nDuring theith time slot, the sender transmits a channel packet, X[i]2Fnito a receiver, where niis a non-negative integer.\nThe receiver obtains Y[i]2fX[i];\u0003g, which reﬂects packet reception or packet loss, respectively. When all channel packets\nare received, S[i]must be decoded by the receiver by time slot (i+\u001cL)for a parameter \u001cL\u00150. This requirement is called\nthe “lossless-delay” constraint and represents the maximum tolerable latency for lossless transmission. Recall that our work\nconsiders\u001cL= 1 (as discussed in Section I). In addition, S[i]must be decoded by time slot (i+\u001c)when losses occur for\na parameter \u001c\u0015\u001cL. This reﬂects the maximum acceptable latency in the worst case and is called the “worst-case-delay”\nconstraint.\nOur work uses the packet loss model from previous work [3], [4], [27]. Packets are lost in bursts of up to bconsecutive\nlosses followed by at least \u001csuccessful receptions. Formally, for any i2f0;:::;t\u0000\u001c\u0000b+ 1g, if(Y[i] =\u0003)then8j2\nfi+b;:::;i +b+\u001c\u00001g(Y[j] =X[j]):To ensure that the worst-case-delay constraint is satisﬁable, \u001c\u0015b. Furthermore,\nb>0because coding is not needed otherwise. Finally, \u001c >b , since\u001c\u0015(\u001cL+b)[26] and\u001cL= 1 in our work.\nWhen the sizes of message packets and channel packets are ﬁxed, the rate is simply the ratio of their sizes. However, a\nmore nuanced notion of rate is needed due to the variability in the sizes of message packets and channel packets. The rate for\na message packet size sequence k0;:::;ktis deﬁned as\nRt=Pt\ni=0kiPt\ni=0ni: (2)\nFor any integer i,f0;:::;igis denoted by [i]. For convenience of notation, ki= 0 fori2[2\u001c\u00001][ft\u00002\u001c+ 1;:::;tg;\nandtis taken to be at least 4\u001c. This assumption is satisﬁed by adding zero padding, which does not affect the rate. As such,\n\n3\nTime slotSet meets the number of lost so that \nmessage symbols ofLost if f Lost if f\nFig. 2: Selecting pi+\u001cby considering each burst starting in time slot j2fi\u0000b+ 1;:::;i + 1g(shown with lightning bolts).\nS[0];:::;S [2\u001c\u00001]are known to be of size zero, and X[0];:::;X [2\u001c\u00001]are empty. Encoding and decoding depends on the\nhistory of the transmission, which is summarized as follows.\nDeﬁnition 1 (State): For anyt;\u001c; andi2f2\u001c;:::;tg, the state is denotedXi= (k0;:::;ki;X[0];:::;X [i\u00001]).\nThis section considers systematic codes for clarity, but the results also hold for general codes. To meet the lossless-delay\nconstraint, the symbols of S[i]must be sent by time slot (i+\u001cL)(i.e., inX[i]andX[i+ 1]). The “policy” of a construction,\nas deﬁned below, speciﬁes how to spread the message symbols.\nDeﬁnition 2 (Policy): The policy of a construction for any i2[t]and stateXiis the number of symbols of S[i]sent in\nchannel packet X[i]. The policy is denoted as Fi(Xi)(orfifor conciseness) and lies in [ki].\nFor anyi>0,X[i]comprises (a) the ﬁrst fisymbols ofS[i], (b) the ﬁnal (ki\u00001\u0000fi\u00001)symbols ofS[i\u00001], and (c)piparity\nsymbols, denoted as P[i]. The encoding is given by X[i] =\nEnc(Xi;S[i\u0000\u001c];:::;S [i\u00001];S0[i];:::;Sfi\u00001[i];O[i]) (3)\nfori\u00152\u001c, andX[i]is empty for i<2\u001c. This section assumes that X[i]is independent of the message symbols of S[i]sent\ninX[i+ 1] for clarity, although the results hold without this assumption. The receiver obtains Y[i]2fX[i];\u0003gdepending on\nwhether channel packet X[i]is received or dropped.1Under lossless transmission, S[i]is available in uncoded form. Otherwise,\nS[i]is decoded as\nDec(hY[j];kj;fjjj2[i+\u001c]i): (4)\nThe following notation is used throughout our work. A vector Vhas lengthv, comprises symbols (V0;:::;Vv\u00001), and is a\nrow vector. For any i\u0014j2[v\u00001],Vj\ni= (Vi;:::;Vj).\nIII. A B UILDING BLOCK CONSTRUCTION\nIn this section, we present a rate-optimal construction, called the “ (\u001c;b;t;hfiji2[t]i)\u0000Spread Code,” for any given policies,\ni.e., choice of how to spread the message symbols over channel packets. Speciﬁcally, for any given hfiji2[t]i, at leastfi\nsymbols ofS[i]will be sent in X[i]under the construction for each i2[t].\nThe ﬁrst 2\u001cchannel packets are empty. For each i2[t]n[2\u001c\u00001],X[i]comprises (a) the ﬁrst f0\nisymbols ofS[i]for some\nf0\ni\u0015fi, (b) the ﬁnal (ki\u00001\u0000f0\ni\u00001)symbols of S[i\u00001], and (c)piparity symbols called P[i]. Next, we deﬁne f0\ni,pi, and\nP[i]for any message packet size sequence, k0;:::;kt.\nDeﬁning each f0\niandpi.For time slots i2[2\u001c\u00001][ft\u00002\u001c+1;:::;tg,f0\ni=ki= 0. For time slots i2[3\u001c\u00001][ft\u0000\u001c+1;:::;tg,\npi= 0. For alli= 2\u001c;:::; (t\u00002\u001c), we deﬁne pi+\u001cto be as small as possible while ensuring that S[i]is decoded by time\nslot(i+\u001c)under any lossy transmission. Speciﬁcally, pi+\u001c=\nmax\nj2fi\u0000b+1;:::;i+1g\u0010\n0;1[j+b\u00001\u0015i+ 1] (ki\u0000fi) +1[j\u0014i]fi+iX\nl=j(kl\u00001\u0000f0\nl\u00001) +i\u00001X\nl=jf0\nl\u0000i+\u001c\u00001X\nl=j+bpl\u0011\n; (5)\nas is illustrated in Figure 2. We then use pi+\u001cto deﬁne\nf0\ni= max (fi;pi+\u001c): (6)\nConstructing parity symbols. The parity symbols are deﬁned analogously to those of the construction from [27] (which builds\non the construction from [5]). For i2f2\u001c;:::;t\u0000\u001cg, the message symbols sent in X[i]are partitioned into (a) symbols\nofS[i]that are recovered during time slot (i+\u001c)under a lossy transmission, and (b) symbols of S[i\u00001]andS[i]that are\n1The receiver needs the sizes of the message packets to decode. Thus, a small header with up to 2Pi\nj=i\u0000blog(kj)\u00142(b+1) log( m)symbols containing\n(ki\u0000b; : : : ; k i)is added to the header of X[i].\n\n4\nFig. 3: Deﬁning W[i]by placing the symbols of V[i\u0000\u001c]in positions 2m(jmod\u001c);:::; (2m(jmod\u001c) +vj\u00001)forj2\nfi\u0000\u001c;:::;i\u00001g. The remaining positions are ﬁlled with 0’s.\nrecovered by time slot (i\u00001 +\u001c)under a lossy transmission. The two components are of sizes uiandviand are denoted as\nU[i]andV[i], respectively. Thus, X[i] = (U[i];V[i];P[i]), where\nU[i] =Spi+\u001c\u00001\n0 [i] (7)\nV[i] =\u0000\nSki\u0000f0\ni\u00001\npi+\u001c[i];Ski\u00001\u00001\nf0\ni\u00001[i\u00001]\u0001\n(8)\nP[i] =U[i\u0000\u001c] +P(v)[i]: (9)\nEach symbol of P(v)[i]is a linear combination of the symbols of (V[i\u0000\u001c];:::;V [i\u00001]), where the linear equations are\nchosen using a (2m\u001c)\u0002(2m\u001c)Cauchy matrix, A, as follows. Let W[i]be a length 2m\u001cvector where positions 2m(jmod\n\u001c);:::; (2m(jmod\u001c) + 2m\u00001)compriseV[j]followed by (2m\u0000vj) 0’s forj2fi\u0000\u001c;:::;i\u00001g, as is illustrated in\nFigure 3. Finally, we deﬁne\nP(v)[i] =W[i]A(i); (10)\nwhereA(i)isArestricted to columns 2m(imod\u001c);:::; (2m(imod\u001c) +pi\u00001). The (\u001c;b;t;hfiji2[t]i)\u0000Spread Code is\nshown recovering a burst in Figure 4.\nDecoding. Fori2[t],S[i]is decoded (a) from X[i]andX[i+ 1] under lossless conditions, and (b) by solving a system of\nlinear equations corresponding to the symbols of S[i\u0000\u001c];:::;S [i\u00001];Y[i];:::;Y [i+\u001c]when losses occur.\nNext, we show that the (\u001c;b;t;hfiji2[t]i)\u0000Spread Code meets the lossless-delay and worst-case-delay constraints.\nLemma 1: For any parameters (\u001c;b), an arbitrary message packet size sequence k0;:::;kt, and any policy fifori2[t], the\n(\u001c;b;t;hfiji2[t]i)\u0000Spread Code satisﬁes the lossless-delay constraint and worst-case-delay constraint.\nProof: For anyi2[t\u00001],S[i] =\u0010\nSf0\ni\u00001\n0[i];Ski\u00001\nf0\ni[i]\u0011\n, whereSf0\ni\u00001\n0[i]is sent inX[i], andSki\u00001\nf0\ni[i]is sent inX[i+ 1].\nForS[t],kt= 0 is known. Thus, the lossless-delay constraint is met.\nNext, we show that the worst-case-delay is satisﬁed for any burst. Satisfaction is immediate if the burst starts after (t\u0000\u001c\u0000\nb+ 1), sincekt\u0000\u001c\u0000b= 0;:::;kt= 0is known. Otherwise, suppose X[i];:::;X [i+b\u00001]are lost for some i2[t\u0000\u001c\u0000b+ 1].\nWe assume that i\u00152\u001c, sincek0= 0;:::;k 2\u001c\u00001= 0 is known, and no symbols are sent in X[0];:::;X [2\u001c\u00001]. Each\nP(v)[i+b] = (P[i+b]\u0000U[i+b\u0000\u001c]);:::;P(v)[i+\u001c\u00001] = (P[i+\u001c\u00001]\u0000U[i\u00001])is known.\nWe show that enough parity symbols are received after the burst by time slot (i+\u001c\u00001)to recoverV[i];:::;V [i+b\u00001]\nas follows:\nfi+b\u00001+i+b\u00001X\nj=i\u0000\nkj\u00001\u0000f0\nj\u00001\u0001\n+i+b\u00002X\nj=if0\nj\u0014i+b+\u001c\u00001X\nj=i+bpj (11)\ni+b\u00001X\nj=i\u0000\nkj\u00001\u0000f0\nj\u00001+f0\nj\u0001\n\u0014i+b+\u001c\u00001X\nj=i+bpj (12)\ni+b\u00001X\nj=ivj+uj\u0014i+b+\u001c\u00001X\nj=i+bpj (13)\ni+b\u00001X\nj=ivj\u0014i+\u001c\u00001X\nj=i+bpj; (14)\nwhere Eq. (11) follows from Eq. (5), Eq. (12) follows from (a) f0\ni+b\u00001=fi+b\u00001, or (b) combining f0\ni+b\u00001=pi+b\u00001+\u001cwith\nEq. (5) to show\ni+b\u00001X\nj=i\u0000\nkj\u00001\u0000f0\nj\u00001\u0001\n+i+b\u00002X\nj=if0\nj=ki\u00001\u0000f0\ni\u00001+i+b\u00002X\nj=ikj\u0014i+b+\u001c\u00002X\nj=i+bpj;\nEq. (13) follows from Eqs. (7) and (8), and Eq. (14) follows from Eqs. (7) and (9).\n\n5\nTime slotRecover\nusingRecover for\nusing\nFig. 4: An example of how the (\u001c;b;t;hfiji2[t]i)\u0000Spread Code recovers a burst of length bstarting in time slot (i+ 1) .\nNext, we show that P[i+b];:::;P [i+\u001c\u00001]sufﬁce to recover V[i];:::;V [i+b\u00001]Recall thatP(v)[j] =W[j]A(j)for\nj2fi+b;:::;i +\u001c\u00001g, whereW[j]containsV[l]in positions\nI(i;j)=[\nl2fi;:::;i +b\u00001gf2m(lmod\u001c);:::; (2m(lmod\u001c) +vl\u00001)g;\nas deﬁned in Eq. (10) and illustrated in Figure 3. Let W0[j]be the vector of length 2m\u001cwith (a) 0’s in positions in I(i;j), (b)\nWr[j]for positions r2[2m\u001c\u00001]nI. The receiver can compute W0[j]and use it to determine P\u0003[j] =\u0000\nP(v)[j]\u0000W0[j]A(j)\u0001\n.\nLetl0=i;:::;lb\u00001= (i+b\u00001), andr= arg min l2fi;:::;i +b\u00001g(lmod\u001c). Letl0\n0= (i+b);:::;l\u001c\u0000b\u00001= (i+\u001c\u00001), and\nr0= arg min l2fi+b;:::;i +\u001c\u00001g(lmod\u001c). Then\n2\n666666664P\u0003[l0\nr0\u0000(i+b)]T\n...\nP\u0003[l0\n\u001c\u0000b\u00001]T\nP\u0003[l0\n0]T\n...\nP\u0003[l0\nr0\u0000(i+b)\u00001]T3\n777777775T\n=2\n666666664V[l(r\u0000i)]T\n...\nV[lb\u00001]T\nV[l0]T\n...\nV[l(r\u0000i)\u00001]T3\n777777775T\nA0\n(i); (15)\nwhereTmeans transpose, and A0\n(i)is a submatrix of a Cauchy matrix of dimensions\u0010Pi+b\u00001\nj=ivj\u0011\n\u0002\u0010Pi+\u001c\u00001\nj=i+bpj\u0011\n. As such,\nA0\n(i)is full rank, allowing the receiver to solve for V[i];:::;V [i+b\u00001].\nFinally, forj=i;:::; (i+b\u00001), the receiver uses the symbols of V[j];:::;V [j+\u001c\u00001]to compute P(v)[j+\u001c], yielding\nU[j] =\u0000\nP[j+\u001c]\u0000P(v)[j+\u001c]\u0001\n. AsS[j]is sent over V[j];U[j];andV[j+ 1], it is recovered by time slot (j+\u001c).\nNext, we provide a lower bound on the number of parity symbols sent by any streaming code that satisﬁes the lossless-delay\nand worst-case-delay constraints. The bound is illustrated in Figure 5\nLemma 2: Consider any \u001c;t;b , and any streaming code that satisﬁes the lossless-delay and worst-case-delay constraints.\nSuppose for l2[t], the construction sends py\nlparity symbols and uses policy fl. For anyi2f3\u001c;:::;tgandj2fi\u0000\u001c\u0000b+\n1;:::;i\u0000\u001c+ 1g, the number of parity symbols satisﬁes\n\u0000fj\u00001\u00001[j+b\u00001 =i\u0000\u001c]\u0000\nki\u0000\u001c\u0000fi\u0000\u001c\u0001\n+i\u0000\u001cX\nl=j\u00001kl\u0014iX\nl=j+bpy\nl: (16)\nProof: SupposeX[j];:::;X [j+b\u00001]are lost. Due to the worst-case-delay constraint, S[j\u00001];:::;S [i\u0000\u001c]must\nbe recovered by time slot i. Thus,Pi\u0000\u001c\nl=j\u00001klsymbols need to be decoded. Because the message packets are independent,\nthe symbols of X[0];:::;X [j\u00002]contain no information about S[j\u00001];:::;S [i\u0000\u001c]. By deﬁnition of encoding (that is,\nEq. (3)),X[j\u00001]containsfj\u00001symbols of S[j\u00001], no additional information about S[j\u00001], and no information about\nS[j];:::;S [i\u0000\u001c]. When (j+b\u00001) == (i\u0000\u001c),X[i\u0000\u001c+ 1] is received, and its message symbols include (ki\u0000\u001c\u0000fi\u0000\u001c)\nsymbols of S[i\u0000\u001c]. The remaining message symbols of X[j+b];:::;X [i]correspond to S[i\u0000\u001c+ 1];:::;S [i]and cannot\nbe used to recover S[j\u00001];:::;X [i\u0000\u001c](independence of message packets). Altogether,\n\u0000fj\u00001\u00001[j+b\u00001 =i\u0000\u001c]\u0000\nki\u0000\u001c\u0000fi\u0000\u001c\u0001\n+i\u0000\u001cX\nl=j\u00001kl\nsymbols corresponding to S[j\u00001];:::;S [i\u0000\u001c]need to be recovered by time slot i. These symbols can only be recovered\n\n6\nTime slotusing received parity symbolsRecover\nlost message symbols of  by time slot isymbols of                 are sent in \nare sent in remaining\nFig. 5: Illustration of the bound on the number of symbols sent under any streaming code satisfying the delay constraints.\nFor anyi2f3\u001c;:::;tgandj2fi\u0000\u001c\u0000b+ 1;:::;i\u0000\u001c+ 1g,S[j\u00001];:::;S [i\u0000\u001c]are recovered by time slot iwhen\na burst of length bstarting in time slot j(shown with lightning bolts), under the relaxation of receiving the lost symbols of\nS[i\u0000\u001c+ 1];:::;S [j+b\u00001](boxes with thick black outline).\nusing the parity symbols of X[j+b];:::;X [i], of which there are\niX\nl=j+bpy\nl:\nThe symbols of message packets are drawn uniformly at random from the underlying ﬁeld. Thus, the total number of parity\nsymbols must match the number of message symbols to be decoded.\nWe show the rate of the (\u001c;b;t;hfiji2[t]i)\u0000Spread Code matches that of any streaming code with policy fifori2[t].\nLemma 3: For any\u001c;t;b , and message packet size sequence k0;:::;kt, the (\u001c;b;t;hfiji2[t]i)\u0000Spread Code matches the\nrate of any streaming code with policy fifori2[t]that satisﬁes the lossless-delay and worst-case-delay constraints.\nProof: Under the (\u001c;b;t;hfiji2[t]i)\u0000Spread Code,Pt\nl=0(kl+pl)symbols are sent. Consider any streaming code\nconstruction that satisﬁes the lossless-delay and worst-case-delay constraints, and for each i2[t], employs policy fiand sends\npy\niparity symbols. This streaming code construction sendsPt\nl=0\u0010\nkl+py\nl\u0011\nsymbols in total.\nWe show by induction on i= 0;:::;t thatPi\nl=0pl\u0014Pi\nl=0py\nl. The base case holds for j <3\u001cbecausep0= 0;:::;p 3\u001c\u00001=\n0. For the inductive hypothesis, we note for all j <i :\njX\nl=0pl\u0014jX\nl=0py\nl: (17)\nIn the inductive step, consider i= 3\u001c;:::;t\u0000\u001c. By Eq. (17), the proof holds if pi\u0014py\ni. Otherwise, pi>py\ni\u00150. Due to\nEq. (17), we only need to show for j\u0014ithatPi\nl=jpl\u0014Pi\nl=jpy\nl.\nLeti\u0003= (i\u0000\u001c). By Eq. (5), there exists j\u00032fi\u0003\u0000b+ 1;:::;i\u0003+ 1g(speciﬁcally, taking j\u0003as the value of jused to\ndeﬁnepi) such that\niX\nl=j\u0003+bpl= (18)\ni\u0003+\u001cX\nl=j\u0003+bpl=1[j\u0003+b\u00001\u0015i\u0003+ 1] (ki\u0003\u0000fi\u0003) +1[j\u0003\u0014i\u0003]fi\u0003+i\u0003X\nl=j\u0003(kl\u00001\u0000f0\nl\u00001) +i\u0003\u00001X\nl=j\u0003f0\nl (19)\n=\u00001[j\u0003=i\u0003+ 1]fj\u0003\u00001\u00001[j\u0003>(i\u0003+ 1)]f0\nj\u0003\u00001\u00001[j\u0003+b\u00001 =i\u0003] (ki\u0003\u0000fi\u0003) +i\u0003X\nl=j\u0003\u00001kl (20)\n=\u0000f0\nj\u0003\u00001\u00001[j\u0003+b\u00001 =i\u0003] (ki\u0003\u0000fi\u0003) +i\u0003X\nl=j\u0003\u00001kl (21)\n=\u0000f0\nj\u0003\u00001\u00001[j\u0003+b\u00001 =i\u0000\u001c] (ki\u0000\u001c\u0000fi\u0000\u001c) +i\u0000\u001cX\nl=j\u0003\u00001kl (22)\n\n7\nTime\nslotusing received parity symbolsRecover\nLost if flost message symbols of  by time slot isymbols of                 are sent in \nare sent in remainingLost if f\nFig. 6: Modeling the transmission and constraints using the variables of the integer program. For any i2f3\u001c;:::;t\u0000\u001cgand\nburst (lightning bolts) of length bstarting inj2fi\u0000\u001c\u0000b+ 1;:::;i\u0000\u001c+ 1g,S[j\u00001];:::;S [i\u0000\u001c], are recovered by time\nslotjunder the relaxation of receiving the lost symbols of S[i\u0000\u001c+ 1];:::;S [j+b\u00001](boxes with thick black outline).\n\u0014\u0000fj\u0003\u00001\u00001[j\u0003+b\u00001 =i\u0000\u001c] (ki\u0000\u001c\u0000fi\u0000\u001c) +i\u0000\u001cX\nl=j\u0003\u00001kl: (23)\nEq. (19) follows from the fact that i\u0003= (i\u0000\u001c)and Eq. (5). Eq. (20) follows from rearranging terms. Eq. (21) is immediate if\nj\u0003>i\u0003and otherwise follows from pi\u0003+\u001c\u0014fi\u0003(byj\u0003>i\u0003and Eq. (5)) leading to f0\ni\u0003=fi\u0003(by Eq. (6)). Eq. (22) follows\nfrom substituting i\u0003= (i\u0000\u001c). Eq. (23) follows from Eq. (6).\nBy Lemma 2,\niX\nl=j\u0003+bpy\nl\u0015\u0000fj\u0003\u00001\u00001[j\u0003+b\u00001 =i\u0000\u001c]\u0000\nki\u0000\u001c\u0000fi\u0000\u001c\u0001\n+i\u0000\u001cX\nl=j\u0003\u00001kl: (24)\nCombining Eqs. (18), (23) and (24) leads to\niX\nl=j\u0003+bpy\nl\u0015iX\nl=j\u0003+bpl: (25)\nApplying Eq. (17) (for j=j\u0003+b\u00001) to Eq. (25) leads to\nj\u0003+b\u00001X\nl=0py\nl+iX\nl=j\u0003+b\u00001py\nl=iX\nl=0py\nl\u0015j\u0003+b\u00001X\nl=0pl+iX\nl=j\u0003+b\u00001pl=iX\nl=0pl; (26)\nproving the inductive step for l2f3\u001c;:::;t\u0000\u001cg. Recall that pt\u0000\u001c+1= 0;:::;pt= 0, leading to\ntX\nl=0py\nl\u0015t\u0000\u001cX\nl=0py\nl\u0015t\u0000\u001cX\nl=0pl=tX\nl=0pl:\nThe(\u001c;b;t;hfiji2[t]i)\u0000Spread Code sendsPt\nl=0(kl+pl)symbols, which is no more than the number sent under the\nalternative construction (i.e.,Pt\nl=0(kl+py\nl)).\nIV. O FFLINE -OPTIMAL STREAMING CODES\nIn this section, we design the ﬁrst rate-optimal ofﬂine construction for the setting of \u001cL= 1. We build the construction with\ntwo steps for an arbitrary message packet size sequence, k0;:::;kt. First, we design an integer program (IP) to use constraints\nto model satisfying the lossless-delay and Lemma 2. The IP determines the optimal policy for each time slot: hfiji2[t]i, as\nis illustrated in Figure 6. Second, we employ the (\u001c;b;t;hfiji2[t]i)\u0000Spread Code given the polices. The objective function\nof the integer program is to minimize the total number of parity symbols transmitted, which maximizes the rate.\nNext, we introduce Algorithm 1 to determine an optimal policy, fi, for each time slot i2[t]and then verify that the\n(\u001c;b;t;hfiji2[t]i)\u0000Spread Code is rate optimal.\nTheorem 1: For any (\u001c;b;t )and message packet size sequence k0;:::;kt, suppose Algorithm 1 outputs hfiji2[t]i. Then\nthe(\u001c;b;t;hfiji2[t]i)\u0000Spread Code is rate optimal.\nProof: Due to Lemma 3, the rate of the (\u001c;b;t;hfiji2[t]i)\u0000Spread Code is the same as a construction that for i2[t]\nemploys the policy fiand sendsp(IP)\ni parity symbols in X[i]. We will show that no coding scheme can send fewer thanPt\ni=0p(IP)\ni parity symbols.\n\n8\nAlgorithm 1 Computeshfiji2[t]ifor which the (\u001c;b;t;hfiji2[t]i)\u0000Spread Code matches the ofﬂine-optimal-rate.\nInput: (\u001c;b;t;k 0;:::;kt)\nMinimizePt\ni=0p(IP)\ni subject to\n\u000f8i2[t],f(IP)\ni\u00150;f(IP)\ni\u0014ki;p(IP)\ni\u00150.\n\u000f8i2f3\u001c;:::;t\u0000\u001cg;j2fi\u0000\u001c\u0000b+ 1;:::;i\u0000\u001c+ 1g,\n\u0000f(IP)\nj\u00001\u00001[j+b\u00001 =i\u0000\u001c]\u0010\nki\u0000\u001c\u0000f(IP)\ni\u0000\u001c\u0011\n+i\u0000\u001cX\nl=j\u00001kl\u0014iX\nl=j+bp(IP)\nl:\nOutput:hfiji2[t]i\nSender\nLearning-based\napproachSide\ninformation\nSpread code\nFig. 7: Illustration of the (\u001c;b;t )\u0000Spread ML Code. A learning-based approach is used to determine a policy, f(\u000f)\ni, during the\nith time slot, which is then used by the\u0010\n\u001c;b;t;D\nf(\u000f)\niji2[t]E\u0011\n\u0000Spread Code.\nAn arbitrary rate-optimal construction must satisfy the ﬁrst constraint because for all i2[t]between 0andkisymbols of\nS[i]are sent inX[i]along with a non-negative number of parity symbols. The construction must satisfy the second constraint\ndue to Lemma 2. Using each policy of this rate-optimal construction along with the number of parity symbols it sends is a\nvalid solution to the integer program. Correctness follows from minimization.\nAlthough Algorithm 1 applies to the entire message packet size sequence, it is trivial to modify the algorithm to apply to\nthe remainder of a transmission after channel packets X[0];:::;X [l]have been sent for some l2[t]. This involves adding\nconstraints for all j2[l](a)f(IP)\nj =fjand (b)p(IP)\nj=pj. We call the modiﬁed algorithm “Algorithm 2.”\nCorollary 1: For any (\u001c;b;t ), message packet size sequence k0;:::;kt, andl2[t\u0000\u001c], suppose that for all j2[l], policyfj\nwas used and pjparity symbols were sent in X[j], and Algorithm 2 outputs hfiji2[t]i. Then the (\u001c;b;t;hfiji2[t]i)\u0000Spread\nCode attains the best possible rate given the prior transmission of X[0];:::;X [l].\nV. L EARNING -BASED ONLINE STREAMING CODES\nWe now present an online code construction, dubbed the “ (\u001c;b;t )\u0000Spread ML Code,” whose expected rate is within \u000fof\nthe online-optimal-rate. The construction uses a learning-based approach to specify the policy of spreading the symbols of\nS[i], denotedf(\u000f)\ni, for eachi2[t], and then applies the\u0010\n\u001c;b;t;D\nf(\u000f)\niji2[t]E\u0011\n\u0000Spread Code, as is shown at a high level\nin Figure 7.\nTo determine how to spread message symbols, we use the side information of Ssamples the distribution of the sizes of the\nfuture message packets, which was deﬁned in Eq. (1) as\n(k(j)\ni+1;:::;k(j)\nt)\u0018Dk0;:::;kijj2f0;:::;S\u00001g\u000b\n. We use a similar\ntechnique to empirical risk minimization over the Ssamples to set f(\u000f)\nito the value leading to lowest expected number of\nsymbols being sent by a rate-optimal ofﬂine code. Speciﬁcally, for any i= 0;:::;t ,j2[S\u00001]andl2[ki], let\nzi;j;l=tX\nr=ip(IP)\nr;\nwherep(IP)\ni;:::;p(IP)\nt are variables used by the IP of Algorithm 2 given X[0];:::;X [i\u00001];andf(IP)\ni =l. Then\nf(\u000f)\ni= arg min\nl2[ki]1\nS\u00001X\nj2[S\u00001]zi;j;l: (27)\nWe demonstrate how f(\u000f)\niis deﬁned in Figure 8.\nThe key observation to interpret the choice of f(\u000f)\niis that the number of parity symbols sent corresponding to message packet\nS[i], namelypi+\u001c, is monotonically non-decreasing as f(\u000f)\niincreases. Thus, smaller values of f(\u000f)\nilead to smaller values of\npi+\u001c, which exploits the parity symbols already sent before time slot (i+\u001c). This strategy is effective when the next several\nmessage packets are likely small. Therefore, a small P[i+\u001c]sufﬁces to ensure that the next several message packets are\nrecovered when some of X[i+ 2];:::;X [i+\u001c\u00001]are lost. In contrast, larger values of f(\u000f)\nipromote larger values of pi+\u001c,\n\n9\nSimulatorSide\ninformation\nSpread code\nMinimizer\nFig. 8: Illustration of the learning-based approach (green) to determine how to spread message symbols.\nwhich is suitable when the next several message packets are likely to be large. Hence, a large P[i+\u001c]will not go to waste\neven if a burst starts after receiving S[i].\nTo show that the (\u001c;b;t )\u0000Spread ML Code is approximately rate optimal, we analyze the number of extra symbols it sends\ncompared to an optimal scheme as follows.\nDeﬁnition 3 (Regret): The regret,Rki;:::;kt\u0010\nf(\u000f)\ni\u0011\n, for the message packet size sequence k0;:::;ktis the number of extra\nsymbols sent under Algorithm 2 when f(\u000f)\niis used compared to the best ofﬂine policy, f0\ni.\nThe number of extra symbols sent compared to an optimal ofﬂine scheme isPt\ni=0Rki;:::;kt\u0010\nf(\u000f)\ni\u0011\n, as is shown next for\ncompleteness.\nLemma 4: For message packet size sequence k0;:::;kt, the (\u001c;b;t )\u0000Spread ML Code transmitsPt\ni=0Rki;:::;kt(fi)more\nsymbols than a scheme meeting the ofﬂine-optimal-rate.\nProof: We can sequentially improve the (\u001c;b;t )\u0000Spread ML Code for i=t\u00002\u001c;:::; 2\u001cby switching f(\u000f)\njforj2\nfi;:::;tgto the one computed by Algorithm 2 given f(\u000f)\n0;:::;f(\u000f)\ni\u00001;p0;:::;pi\u00001. For each value of i, the improvement in\ntotal number of symbols sent is Rki;:::;kt(fi)by Deﬁnition 3. After reaching i= 2\u001c, the output is simply Algorithm 2, as\nk0= 0;:::;k 2\u001c\u00001= 0.\nNext, we bound the expected regret of the (\u001c;b;t )\u0000Spread ML Code from spreading message symbols for any time slot.\nLemma 5: For any (\u001c;b;t ),S\u0015q\nln(8m2\n\u000f)2p\n2m3\n\u000fsamples from the side information, where mis the maximum size of a\nmessage packet, i2[t],k0;:::;ki\u00001;andf0\ni2[ki],\nEki+1;:::;kth\nRki;:::;kt\u0010\nf(\u000f)\ni\u0011\n\u0000Rki;:::;kt(f0\ni)i\n\u0014\u000f: (28)\nProof: Ifki= 0,fi=f(\u000f)\ni, concluding the proof. Otherwise, the choice of f(\u000f)\nileads to sending kiextra parity symbols\ninX[i+\u001c]compared to an optimal scheme, so\nRki;:::;kt\u0010\nf(\u000f)\ni\u0011\n\u0014m (29)\nand\nEki+1;:::;kth\nRki;:::;kt\u0010\nf(\u000f)\ni\u0011i\n\u0014m;Varki+1;:::;kt\u0010\nRki;:::;kt\u0010\nf(\u000f)\ni\u0011\u0011\n\u0014m2:\nAt a high level, we apply the Hoeffding bound [59] to show that the expected regret for each possible policy is well\napproximated using the empirical mean over the Ssamples from the side information. The unlikely event that the expected\nvalue deviates greatly from the mean will have negligible impact due to Eq. (29).\nNext, we use O[i]to determine the values of Srandom variables, (zi;j;0;:::;zi;j;S\u00001), equalingRki;:::;kt(j)in distribution.\nThe empirical mean,1\nSPS\u00001\nl=0zi;j;l, is used to estimate Eki;:::;kt[Rki;:::;kt(j)]. By the Hoeffding bound [59],\f\f\f\f\f1\nS S\u00001X\nl=0zi;j;l!\n\u0000Ek0;:::;kt[Rki;:::;kt(j)]\f\f\f\f\f<\u000fy\nwith probability at least \u0012\n1\u00002e\u00002S2(\u000fy)2\nm2\u0013\n\u0015(1\u0000\u000e)\nas long as\n\u000e\n2\u0015e\u00002S2(\u000fy)2\nm2\n2S2(\u000fy)2\nm2\u0015ln\u00122\n\u000e\u0013\n\n10\nS2\u0015ln\u00122\n\u000e\u0013m2\n2(\u000fy)2\nS\u0015s\nln\u00122\n\u000e\u0013mp\n2\u000fy:\nUsing\u000fy=\u000e=\u000f\n4m2and applying the union bound over the at most mvalues ofkishows with probability (1\u0000m\u000e)for\nallj2[ki],\f\f\f\f\f1\nS S\u00001X\nl=0zi;j;l!\n\u0000Ek0;:::;kt[Rki;:::;kt]\u0010\nf(\u000f)\ni;j\u0011\n]\f\f\f\f\f<\u000fy: (30)\nWe setf(\u000f)\niaccording to Eq. (27) as\nf(\u000f)\ni= arg min\nj1\nSS\u00001X\nl=0zi;j;l:\nWith probability (1\u0000m\u000e)Eq. (30) holds, leading to\nEki;:::;kth\nRki;:::;kt\u0010\nf(\u000f)\ni\u0011\n\u0000Rki;:::;kt(f0\ni)i\n\u0014\nEki;:::;kth\nRki;:::;kt\u0010\nf(\u000f)\ni\u0011\n\u0000Rki;:::;kt(f0\ni)i\n+ \n1\nS S\u00001X\nl=0zi;f0\ni;l!\n\u00001\nS S\u00001X\nl=0zi;f(\u000f)\ni;l!!\n\u0014\n \nEki;:::;kt\"\u0010\nRki;:::;kt\u0010\nf(\u000f)\ni\u0011i\n\u00001\nS S\u00001X\nl=0zi;f(\u000f)\ni;l!!!\n+  \n1\nS S\u00001X\nl=0zi;f0\ni;l!\n\u0000Eki;:::;kt[Rki;:::;kt(f0\ni)]!!\n\u0014\n\f\f\f\f\fEki;:::;kth\nRki;:::;kt\u0010\nf(\u000f)\ni\u0011i\n\u00001\nS S\u00001X\nl=0zi;f(\u000f)\ni;l!\f\f\f\f\f+\f\f\f\f\f \n1\nS S\u00001X\nl=0zi;f0\ni;l!\n\u0000Eki;:::;kt[Rki;:::;kt(f0\ni)]!\f\f\f\f\f\u00142\u000fy;(31)\nwhich used the fact that Eq. (27) led to\n1\nS S\u00001X\nl=0zi;f0\ni;l!\n\u00151\nS S\u00001X\nl=0zi;f(\u000f)\ni;l!\n:\nOtherwise, with probability m\u000e\nEki;:::;kth\nRki;:::;kt(f(\u000f)\ni)\u0000Rki;:::;kt(f0\ni)i\n\u0014m: (32)\nCombining Eqs. (31) and (32) leads to\nEki;:::;kth\nRki;:::;kt(f(\u000f)\ni)\u0000Rki;:::;kt(f0\ni)i\n\u0014\u000em2+ 2\u000fy\u0014\u000f:\nFinally, we show that the expected online-optimal-rate, denoted as “ R(E;Opt),” is within \u000fof the expected rate of the\n(\u001c;b;t )\u0000Spread ML Code, denoted as\nR(E)= Ek0;:::;kt\"Pt\ni=0kiPt\ni=0ki+pi#\n: (33)\nTheorem 2: For any (\u001c;b;t )and forS\u0015q\nln(8m2\n\u000f)2p\n2m3\n\u000fsamples from the side information, where mis the maximum\nsize of a message packet, (R(E;Opt)\u0000R(E))<\u000f.\nProof: We consider an online scheme with the optimal expected rate, namely the (\u001c;b;t;hf0\niji2[t]i)\u0000Spread Code,\nwhich must exist by Lemma 3. Let nibe the number of symbols sent in X[i]under the optimal scheme and\nn\u000f=tX\ni=0Rki;:::;kt\u0010\nf(\u000f)\ni\u0011\n\u0000Rki;:::;kt(f0\ni) (34)\nbe the number of additional symbols sent under the (\u001c;b;t;hf0\niji2[t]i)\u0000Spread Code.\nBy deﬁnition\nR(E;Opt)\u0000R(E)\u0014Ek0;:::;kt\"\f\f\f\f\fPt\ni=0kiPt\ni=0ni\u0000Pt\ni=0ki\nn\u000f+Pt\ni=0ni\f\f\f\f\f#\n(35)\n\u0014Ek0;:::;kt2\n4\f\f\f\f\f\fjn\u000fjPt\ni=0ki\u0010Pt\ni=0ni\u0011\u0010\nn\u000f+Pt\ni=0ni\u0011\f\f\f\f\f\f3\n5\n\u0014Ek0;:::;kt\"\njn\u000fjPt\ni=0ni#\n(36)\n\n11\n\u0014Ek0;:::;kt\"\njn\u000fjPt\ni=01[ki>0]#\n; (37)\nwhere Eq. (36) follows from sendingPt\ni=0ni\u0015Pt\ni=0kisymbols to satisfy the lossless-delay constraint, and Eq. (37) follows\nfromPt\ni=0ki\u0015Pt\ni=01[ki>0].\nIfki= 0, thenRki;:::;kt\u0010\nf(\u000f)\ni\u0011\n=Rki;:::;kt(f0\ni) = 0 . Thus, we can simplify Eq. (34) as\nn\u000f=tX\ni=01[ki>0]\u0010\nRki;:::;kt\u0010\nf(\u000f)\ni\u0011\n\u0000Rki;:::;kt(f0\ni)\u0011\n: (38)\nApplying Eq. (38) to Eqs. (35) and (37) leads to\nR(E;Opt)\u0000R(E)\u0014Ek0;:::;kt2\n4Pt\ni=01[ki>0]\u0010\nRki;:::;kt\u0010\nf(\u000f)\ni\u0011\n\u0000Rki;:::;kt(f0\ni)\u0011\nPt\ni=01[ki>0]3\n5\n\u0014max\ni2[t]Ek0;:::;kth\nRki;:::;kt\u0010\nf(\u000f)\ni\u0011\n\u0000Rki;:::;kt(f0\ni)i\n= max\ni2[t]Ek0;:::;ki\u00001h\nEki;:::;kth\nRki;:::;kt\u0010\nf(\u000f)\ni\u0011\n\u0000Rki;:::;kt(f0\ni)ii\nAs such, it sufﬁces to show for all i2[t]that\nEk0;:::;ki\u00001h\nEki;:::;kth\nRki;:::;kt\u0010\nf(\u000f)\ni\u0011\n\u0000Rki;:::;kt(f0\ni)ii\n\u0014\u000f: (39)\nBecauseS\u0015q\nln(8m2\n\u000f)2p\n2m3\n\u000f, Lemma 5 guarantees for any k0;:::;ki\u00001,\nEki;:::;kth\nRki;:::;kt\u0010\nf(\u000f)\ni\u0011\n\u0000Rki;:::;kt(f0\ni)i\n\u0014\u000f;\nconcluding the proof.\nWe have designed an online code that uses a black box algorithm to determine how to spread message symbols and bounded\nhow close the rate is to optimal based on the regret due to the choices of how to spread. To show that the code is approximately\nrate optimal, we presented an explicit learning-based approach of leveraging samples to the distribution of the sizes of future\nmessage packets to spread message symbols (i.e., Eq. (27)) and showed in Lemma 5 that it has a sufﬁciently small expected\nregret. More generally, any criteria with a sufﬁciently small expected regret could be used, leading to the following result.\nCorollary 2: Theorem 2 holds when any criteria for spreading message symbols is substituted for Eq. (27) if the criteria\nsatisﬁes Eq. (28) for all i2[t],k0;:::;kt;andf0\ni2[ki].\nVI. C ONCLUSION\nInspired by the growing ﬁeld of learning-augmented algorithms, this work introduces a new methodology for constructing\nonline streaming codes that combines machine learning with algebraic coding theory tools. The approach is to (a) isolate the\ncomponent that can beneﬁt from machine learning, (b) solve the ofﬂine version of the problem by integrating optimization with\nalgebraic coding theory techniques, and (c) convert the ofﬂine scheme into an online one using a learning-based approach. This\nstrategy is applicable beyond the setting considered in this paper, including numerous other settings for real-time streaming\ncommunication.\nACKNOWLEDGMENT\nThis work was funded in part by an NSF grant (CCF-1910813).\nREFERENCES\n[1] M. Rudow and K. Rashmi, “Learning-based streaming codes are approximately optimal for variable-size messages,” to be published 2022 IEEE\nInternational Symposium on Information Theory (ISIT). IEEE, 2022.\n[2] A. Badr, A. Khisti, W. Tan, and J. Apostolopoulos, “Perfecting protection for interactive multimedia: A survey of forward error correction for low-delay\ninteractive applications,” IEEE Signal Processing Magazine , vol. 34, no. 2, pp. 95–113, March 2017.\n[3] E. Martinian and C. . W. Sundberg, “Burst erasure correction codes with low decoding delay,” IEEE Trans. Inf. Theory , vol. 50, no. 10, pp. 2494–2502,\nOct 2004.\n[4] E. Martinian and M. Trott, “Delay-optimal burst erasure code construction,” in ISIT, June 2007, pp. 1006–1010.\n[5] A. Badr, P. Patil, A. Khisti, W. Tan, and J. Apostolopoulos, “Layered constructions for low-delay streaming codes,” IEEE Trans. Inf. Theory , vol. 63,\nno. 1, pp. 111–141, Jan 2017.\n[6] S. L. Fong, A. Khisti, B. Li, W. Tan, X. Zhu, and J. Apostolopoulos, “Optimal streaming codes for channels with burst and arbitrary erasures,” IEEE\nTrans. Inf. Theory , vol. 65, no. 7, pp. 4274–4292, July 2019.\n[7] M. N. Krishnan and P. V . Kumar, “Rate-optimal streaming codes for channels with burst and isolated erasures,” in ISIT, June 2018, pp. 1809–1813.\n[8] M. N. Krishnan, D. Shukla, and P. V . Kumar, “Rate-optimal streaming codes for channels with burst and random erasures,” IEEE Trans. Inf. Theory ,\nvol. 66, no. 8, pp. 4869–4891, 2020.\n[9] E. Domanovitz, S. L. Fong, and A. Khisti, “An explicit rate-optimal streaming code for channels with burst and arbitrary erasures,” IEEE Transactions\non Information Theory , vol. 68, no. 1, pp. 47–65, 2022.\n\n12\n[10] A. Badr, D. Lui, A. Khisti, W. Tan, X. Zhu, and J. Apostolopoulos, “Multiplexed coding for multiple streams with different decoding delays,” IEEE\nTrans. Inf. Theory , vol. 64, no. 6, pp. 4365–4378, June 2018.\n[11] S. L. Fong, A. Khisti, B. Li, W. Tan, X. Zhu, and J. Apostolopoulos, “Optimal multiplexed erasure codes for streaming messages with different decoding\ndelays,” IEEE Trans. Inf. Theory , vol. 66, no. 7, pp. 4007–4018, 2020.\n[12] A. Badr, A. Khisti, and E. Martinian, “Diversity embedded streaming erasure codes (de-sco): Constructions and optimality,” IEEE J. Sel. Areas Inf.\nTheory , vol. 29, no. 5, pp. 1042–1054, May 2011.\n[13] A. Badr, D. Lui, and A. Khisti, “Streaming codes for multicast over burst erasure channels,” IEEE Trans. Inf. Theory , vol. 61, no. 8, pp. 4181–4208,\nAug 2015.\n[14] M. Haghifam, M. N. Krishnan, A. Khisti, X. Zhu, W.-T. Tan, and J. Apostolopoulos, “On streaming codes with unequal error protection,” IEEE J. Sel.\nAreas Inf. Theory , 2021.\n[15] N. Adler and Y . Cassuto, “Burst-erasure correcting codes with optimal average delay,” IEEE Trans. Inf. Theory , vol. 63, no. 5, pp. 2848–2865, May\n2017.\n[16] D. Leong and T. Ho, “Erasure coding for real-time streaming,” in ISIT, July 2012, pp. 289–293.\n[17] D. Leong, A. Qureshi, and T. Ho, “On coding for real-time streaming under packet erasures,” in ISIT, July 2013, pp. 1012–1016.\n[18] A. Badr, A. Khisti, W. Tan, and J. Apostolopoulos, “Streaming codes with partial recovery over channels with burst and isolated erasures,” IEEE J. Sel.\nTopics Signal Process. , vol. 9, no. 3, pp. 501–516, April 2015.\n[19] S. L. Fong, A. Khisti, B. Li, W.-T. Tan, X. Zhu, and J. Apostolopoulos, “Optimal streaming erasure codes over the three-node relay network,” IEEE\nTrans. Inf. Theory , vol. 66, no. 5, pp. 2696–2712, 2020.\n[20] A. Badr, A. Khisti, W.-t. Tan, X. Zhu, and J. Apostolopoulos, “Fec for voip using dual-delay streaming codes,” in IEEE INFOCOM 2017 - IEEE\nConference on Computer Communications , 2017, pp. 1–9.\n[21] Z. Li, A. Khisti, and B. Girod, “Correcting erasure bursts with minimum decoding delay,” in Conf. Rec. Asilomar Conf. Signals Syst. Comput. , Nov\n2011, pp. 33–39.\n[22] Y . Wei and T. Ho, “On prioritized coding for real-time streaming under packet erasures,” in Allerton . IEEE, 2013, pp. 327–334.\n[23] M. N. Krishnan, V . Ramkumar, M. Vajha, and P. V . Kumar, “Simple streaming codes for reliable, low-latency communication,” IEEE Communications\nLetters , pp. 1–1, 2019.\n[24] V . Ramkumar, M. Vajha, M. N. Krishnan, and P. Vijay Kumar, “Staggered diagonal embedding based linear ﬁeld size streaming codes,” pp. 503–508,\n2020.\n[25] P.-W. Su, Y .-C. Huang, S.-C. Lin, I.-H. Wang, and C.-C. Wang, “Random linear streaming codes in the ﬁnite memory length and decoding deadline\nregime,” in ISIT, 2021, pp. 730–735.\n[26] M. Rudow and K. Rashmi, “Streaming codes for variable-size messages,” IEEE Transactions on Information Theory , pp. 1–1, 2022.\n[27] ——, “Online versus ofﬂine rate in streaming codes for variable-size messages,” in ISIT. IEEE, 2020, pp. 509–514.\n[28] T. Lykouris and S. Vassilvtiskii, “Competitive caching with machine learned advice,” in International Conference on Machine Learning . PMLR, 2018,\npp. 3296–3305.\n[29] A. Antoniadis, C. Coester, M. Elias, A. Polak, and B. Simon, “Online metric algorithms with untrusted predictions,” in International Conference on\nMachine Learning . PMLR, 2020, pp. 345–355.\n[30] M. Mitzenmacher, “A model for learned bloom ﬁlters and optimizing by sandwiching,” Advances in Neural Information Processing Systems , vol. 31,\n2018.\n[31] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case for learned index structures,” in Proceedings of the 2018 international conference\non management of data , 2018, pp. 489–504.\n[32] S. Lattanzi, T. Lavastida, B. Moseley, and S. Vassilvitskii, “Online scheduling via learned weights,” in Proceedings of the Fourteenth Annual ACM-SIAM\nSymposium on Discrete Algorithms . SIAM, 2020, pp. 1859–1877.\n[33] É. Bamas, A. Maggiori, and O. Svensson, “The primal-dual method for learning augmented algorithms,” Advances in Neural Information Processing\nSystems , vol. 33, pp. 20 083–20 094, 2020.\n[34] M. Mitzenmacher and S. Vassilvitskii, “Algorithms with predictions,” arXiv preprint arXiv:2006.09123 , 2020.\n[35] C.-Y . Hsu, P. Indyk, D. Katabi, and A. Vakilian, “Learning-based frequency estimation algorithms.” in International Conference on Learning\nRepresentations , 2019.\n[36] T. Jiang, Y . Li, H. Lin, Y . Ruan, and D. P. Woodruff, “Learning-augmented data stream algorithms,” in International Conference on Learning\nRepresentations , 2019.\n[37] K. Anand, R. Ge, and D. Panigrahi, “Customizing ml predictions for online algorithms,” in International Conference on Machine Learning . PMLR,\n2020, pp. 303–313.\n[38] T. Gruber, S. Cammerer, J. Hoydis, and S. t. Brink, “On deep learning-based channel decoding,” in 2017 51st Annual Conference on Information Sciences\nand Systems (CISS) , 2017, pp. 1–6.\n[39] E. Nachmani, E. Marciano, L. Lugosch, W. J. Gross, D. Burshtein, and Y . Be’ery, “Deep learning methods for improved decoding of linear codes,”\nIEEE J. Sel. Topics Signal Process. , vol. 12, no. 1, pp. 119–131, 2018.\n[40] L. Lugosch and W. J. Gross, “Neural offset min-sum decoding,” in ISIT, 2017, pp. 1361–1365.\n[41] S. Dörner, S. Cammerer, J. Hoydis, and S. t. Brink, “Deep learning based communication over the air,” IEEE J. Sel. Topics Signal Process. , vol. 12,\nno. 1, pp. 132–143, 2018.\n[42] S. Cammerer, T. Gruber, J. Hoydis, and S. ten Brink, “Scaling deep learning-based decoding of polar codes via partitioning,” in GLOBECOM 2017 -\n2017 IEEE Global Communications Conference , 2017, pp. 1–6.\n[43] Y . Jiang, H. Kim, H. Asnani, S. Kannan, S. Oh, and P. Viswanath, “Turbo autoencoder: Deep learning based channel codes for point-to-point\ncommunication channels,” Advances in neural information processing systems , vol. 32, pp. 2758–2768, 2019.\n[44] Y . Jiang, S. Kannan, H. Kim, S. Oh, H. Asnani, and P. Viswanath, “Deepturbo: Deep turbo decoder,” in 2019 IEEE 20th International Workshop on\nSignal Processing Advances in Wireless Communications (SPAWC) . IEEE, 2019, pp. 1–5.\n[45] E. Nachmani and L. Wolf, “Hyper-graph-network decoders for block codes,” Advances in Neural Information Processing Systems , vol. 32, pp. 2329–2339,\n2019.\n[46] A. V . Makkuva, X. Liu, M. V . Jamali, H. Mahdavifar, S. Oh, and P. Viswanath, “Ko codes: inventing nonlinear encoding and decoding for reliable\nwireless communication via deep-learning,” in International Conference on Machine Learning . PMLR, 2021, pp. 7368–7378.\n[47] H. Kim, Y . Jiang, S. Kannan, S. Oh, and P. Viswanath, “Deepcode: Feedback codes via deep learning,” Advances in neural information processing\nsystems , vol. 31, 2018.\n[48] D. B. Kurka and D. Gündüz, “Deepjscc-f: Deep joint source-channel coding of images with feedback,” IEEE J. Sel. Areas Inf. Theory , vol. 1, no. 1, pp.\n178–193, 2020.\n[49] J. Kosaian, K. Rashmi, and S. Venkataraman, “Parity models: erasure-coded resilience for prediction serving systems,” in Proceedings of the 27th ACM\nSymposium on Operating Systems Principles , 2019.\n[50] J. Kosaian, K. V . Rashmi, and S. Venkataraman, “Learning-based coded computation,” IEEE J. Sel. Areas Inf. Theory , vol. 1, no. 1, pp. 227–236, 2020.\n[51] H. V . Krishna Giri Narra, Z. Lin, G. Ananthanarayanan, S. Avestimehr, and M. Annavaram, “Collage inference: Using coded redundancy for lowering\nlatency variation in distributed image classiﬁcation systems,” in 2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS) ,\n2020, pp. 453–463.\n\n13\n[52] Y .-S. Jeon, S.-N. Hong, and N. Lee, “Blind detection for mimo systems with low-resolution adcs using supervised learning,” in 2017 IEEE International\nConference on Communications (ICC) , 2017, pp. 1–6.\n[53] N. Samuel, T. Diskin, and A. Wiesel, “Learning to detect,” EEE Trans. Signal Process. , vol. 67, no. 10, pp. 2554–2564, 2019.\n[54] N. Farsad and A. Goldsmith, “Detection algorithms for communication systems using deep learning,” 2017.\n[55] G. Gui, H. Huang, Y . Song, and H. Sari, “Deep learning for an effective nonorthogonal multiple access scheme,” IEEE Trans. Veh. Technol. , vol. 67,\nno. 9, pp. 8440–8450, 2018.\n[56] T. O’Shea and J. Hoydis, “An introduction to deep learning for the physical layer,” IEEE Trans. Cogn. Commun. Netw. , vol. 3, no. 4, pp. 563–575, 2017.\n[57] T. J. O’Shea, K. Karra, and T. C. Clancy, “Learning to communicate: Channel auto-encoders, domain speciﬁc regularizers, and attention,” in 2016 IEEE\nInt. Symp. Signal Process. Inf. Technol. , 2016, pp. 223–228.\n[58] K. Choi, K. Tatwawadi, T. Weissman, and S. Ermon, “Necst: neural joint source-channel coding,” 2018.\n[59] W. Hoeffding, Probability Inequalities for sums of Bounded Random Variables . New York, NY: Springer New York, 1994, pp. 409–426. [Online].\nAvailable: https://doi.org/10.1007/978-1-4612-0865-5_26",
  "textLength": 45416
}