{
  "paperId": "a7f40dd51c626005d34a252b73856a6c927682a7",
  "title": "Trimmer: Cost-Efficient Deep Learning Auto-tuning for Cloud Datacenters",
  "pdfPath": "a7f40dd51c626005d34a252b73856a6c927682a7.pdf",
  "text": "Trimmer: Cost-Efficient Deep Learning Auto-tuning\nfor Cloud Datacenters\nPaper ID: 40\nAbstract —Cloud datacenters capable of provisioning high\nperformance Machine Learning-as-a-Service (MLaaS) at\nreduced resource cost is achieved via auto-tuning: automated\ntensor program optimization of Deep Learning models to\nminimize inference latency within a hardware device. However\ngiven the extensive heterogeneity of Deep Learning models,\nlibraries, and hardware devices, performing auto-tuning within\nCloud datacenters incurs a significant time, compute resource,\nand energy cost of which state-of-the-art auto-tuning is not\ndesigned to mitigate. In this paper we propose Trimmer, a high\nperformance and cost-efficient Deep Learning auto-tuning\nframework for Cloud datacenters. Trimmer maximizes DL\nmodel performance and tensor program cost-efficiency by\npreempting tensor program implementations exhibiting poor\noptimization improvement; and applying an ML-based filtering\nmethod to replace expensive low performing tensor programs\nto provide greater likelihood of selecting low latency tensor\nprograms. Through an empirical study exploring the cost of\nDL model optimization techniques, our analysis indicates that\n26–43% of total energy is expended on measuring tensor\nprogram implementations that do not positively contribute\ntowards auto-tuning. Experiment results show that Trimmer\nachieves high auto-tuning cost-efficiency across different DL\nmodels, and reduces auto-tuning energy use by 21.8–40.9% for\nCloud clusters whilst achieving DL model latency equivalent to\nstate-of-the-art techniques.\nIndex Terms —Deep Learning, Cloud datacenter, MLaaS,\nMachine Learning systems, Energy, Sustainable AI\nI. I NTRODUCTION\nDeep Learning (DL) has become increasingly important\nacross industry and academia, with numerous DL models\ncreated to perform sophisticated Computer Vision and\nNatural Language Processing [1]. Creation of new DL model\narchitectures combined with growing user demand has\nresulted in the formation of Cloud datacenters containing\nspecialized hardware devices (GPUs, FPGAs, etc.) dedicated\nto provisioning Machine Learning-as-a-Service (MLaaS) [2].\nThere is a major impetus for providers to ensure that such\nCloud datacenters are capable of provisioning, as well as\ncreating, DL models with high accuracy and low latency\ninference to achieve high system throughput and\ncost-efficiency in terms of compute resource and energy\nconsumption. An effective means to attain this goal is to\noptimize the individual computational components of DL\nmodels - tensor programs - towards specific target-device\ncharacteristics, spanning cache/memory access patterns,\nthread processing, and hardware-intrinsic functions [3].\nPerforming tensor program optimization is a complex and\ntime-consuming task, which has in turn resulted in the creation\nofauto-tuning : automated DL model optimization of tensorprogram implementations towards a target-device. Facilitated\nby DL compilers such as TVM [3] and Halide [4], auto-tuning\nminimizes tensor program latency via an iterative exploration\nof a large tensor program parameter space, and necessitates\ntens of thousands of costly iterative measurements of candidate\nimplementations per target-device. Auto-tuning is frequently\ndeployed within DL-focused Cloud datacenters [5]–[7].\nWhilst numerous auto-tuning techniques have been\nproposed to accelerate the candidate search of tensor\nprograms [4], [8]–[11], the auto-tuning process requires\nconsiderable time and compute resource cost to complete,\nwhereby even a small DL model must occupy an isolated\ntarget-device for hours to yield reasonable performance\nimprovements [10]. This becomes a considerable issue in the\ncontext of Cloud datacenters which must provision high\nperformance and cost-efficient MLaaS (including user\ndefined auto-tuning [5]) for a large user base each with\nunique DL model configurations and hardware device\nconstraints. This translates into monetary loss for Cloud\nproviders stemming from longer auto-tuning duration and\nwaiting times, lower MLaaS throughput and availability due\nto auto-tuners requiring exclusive access to a target-device\nfor extended time periods, and incurs higher energy\nconsumption, representing a barrier towards creating\nenvironmentally sustainable AI systems [12].\nIn this paper we present Trimmer: A high performance\nand cost-efficient DL auto-tuning framework that reduces the\ntotal time and energy cost required to perform tensor\nprogram optimization for MLaaS deployed within Cloud\ndatacenters. Driven by an empirical study of DL optimization\nperformance and energy cost, Trimmer proposes a ML-based\ncandidate filtering to reduce the number of hardware\nmeasurements of candidates identified as likely to fail, and\nre-sample the candidate search space to replace expensive\nlong-running cold candidates with faster, hot candidates\nincreasing the likelihood of finding faster tensor programs.\nTrimmer is capable of performing hardware measurements of\ntensor program implementations as batches in parallel that\nare periodically compiled to measure improvements to model\ninference latency, rather than probing the search space\nserially. Such an approach allows Trimmer to prioritize or\ndiscard tensor program optimizations across a cluster of\nmachines based on relative performance speed-up across\nmultiple DL models whilst achieving equivalent inference\nlatency compared to state of the art.\n\nThe core contributions of our work are as follows:\n•We analyse the heterogeneous energy and performance\ncharacteristics of DL model optimization frequently used\nfor MLaaS, and show that cold candidates generate 26–\n43% of energy waste within the auto-tuning process;\n•We demonstrate that by extracting the intermediate\nlayer of the neural network, we can query similar tensor\nprogram candidates by using cosine similarity measure\nto filter poor performing candidates;\n•We show via experimentation that Trimmer provides\ncost-efficient auto-tuning, and within cloud DL clusters\nreduces the system energy cost of auto-tuning by up to\n21.8% and 40.9%, respectively.\nSection II presents the background; Section III analyzes\nDL optimization performance and energy cost; Section IV\ndetails the Trimmer auto-tuner design; Section V presents\nthe evaluation results of Trimmer; Section VI discusses\nrelated work; and Section VII our research conclusions.\nII. B ACKGROUND\nA. Cloud Datacenters for Deep Learning\nDeep Learning (DL) models: DL Models provide\ncognitive-like capabilities in areas such as image recognition\nor language learning [13], [14]. DL models composed of\nmulti-layered Deep Neural Networks (DNNs) are represented\nas Directed Acyclic Graphs (DAG), where nodes represent\nDLoperators (Convolution, Batch Normalization) and edges\nthe operator data dependencies. DL operators are expressed\nastensor programs comprising both CPU code (e.g. data\nfetching, submission) and accelerator code (e.g. GPU\nkernels) that perform tensor manipulation instantiated and\nexecuted within a target-device as an implementation . DL\nmodels with greater number of large operators exhibit\nimproved accuracy by performing a higher number of\nFloating Point Operations (FLOP) [15].\nDL Cloud datacenters: Creation of new DL models, data\nvolume growth, and user demand has resulted in the\nformation of Cloud datacenters dedicated to provisioning\nMLaaS as shown in Figure 1. These datacenters are formed\nby clusters of machines equipped with accelerator devices\nsuch as Graphical Processing Units (GPUs) to perform DL\nmodel inference (encompassing tensor program execution)\nsubstantially faster than conventional CPUs. With production\nsystems such as Facebook performing trillions of DL\ninferences daily [2], it is paramount that DL-focused Cloud\ndatacenters achieve high performance model inference to\nsatisfy Service Level Agreement (SLA) and maintain Quality\nof Service (QoS) in a cost-efficient manner. Such cost\ncomprises compute resource (CPU, GPU, network) and time\nthat together drive system energy consumption: a critical\nissue due to high monetary and environmental cost of\ndatacenter operation [16].\nB. DL Model Optimization\nAttaining high performance and cost-efficient DL model\ninference for Cloud datacenters is achieved via DL model\nTraining Infrastructure Inference InfrastructureResource  \nManager  Training  \nEngine\nInference  \nEngine  Cloud Interface  \nGraph OptimizerAuto-tuner\nCompilerOptimization Engine  MLaaS  \nService  \nManager  SchedulerCloud  \nInfra.  User DL Models Provider DL ModelsFig. 1: MLaaS Cloud Infrastructure\noptimization to reduce tensor program latency in a\ntarget-device [8]. Such optimization is attained by leveraging\nframework specific tensor program implementations [17],\n[18] or DL compilers such a TVM [3]. DL compilers enable\ncompilation of high-level DL models definitions onto\ndevice-specific binaries providing greater control over\nimplementation behavior. These optimizations consist of both\nhigh-level : target-device independent transformations of DL\nmodel structure (e.g. operator fusion, algebraic simplification,\ndata buffer reuse), and low-level : target-device dependent\ntensor program transformations (alignment to device cache\nsize, tensorization, mapping tensor compute regions [3]).\nManually performing tensor program optimization is a\ntime-consuming task that must be performed for hundreds of\ntensor programs per target-device, and produces sub-optimal\ntensor program latency improvement [9].\nC. Auto-Tuning\nAuto-tuning is a method for automatic DL model\noptimization of tensor program implementations towards a\ntarget-device. Auto-tuning automates low-level optimization\nby: (1) searching through a space of possible implementation\nparameters (i.e. loop tiling extents, unroll factors) known as\ntemplate-based auto-tuning (e.g. AutoTVM, Chameleon [8],\n[10]); or (2) autoscheduling : automatic generation of\noperator implementations given a set of rules, producing\ndifferent versions of low-level Intermediate Representation\n(IR) [4], [9]. Both approaches propose candidates - operator\nimplementations in IR compiled towards target-device\nlanguages (C, CUDA, x86, PTX), assembled into binaries\nand executed in isolation to measure their latency. Latency\nmeasurements are used to re-train auto-tuner cost models [8],\n[19] that propose new candidates which are leveraged by\nsearch algorithms [3], [10] to navigate the implementation\nspace, avoiding candidates that exhibit high latency.\nD. Auto-tuning Cost in Cloud Datacenters\nAuto-tuning is a time and energy intensive process, where\neven small DL models [20] require ≳10hours to achieve\nsizable latency improvements for a single target-device [3],\n[10]. During auto-tuning, the platform CPU experiences\nheavy load spikes resulting from candidate space search,\ncode lowering, and compilation of tensor programs.\nFurthermore, due to the tensor program implementation\nspace is particularly large and non-linear for DL models with\n\nTABLE I: Hardware setup\nAbbrev. Hardware Specification\nA2x (16-core) Intel Xeon 5218 [2.3GHz],\n196GB DDR4, Nvidia V100 (V olta) 32GB\nB(6-core) Intel i7-8700K [3.7GHz],\n16GB DDR4, Nvidia GTX2080 (Turing): 8GB\nC(12-core) AMD Ryzen 1920X [3.5GHz],\n128GB DDR4, Nvidia GTX2080 (Turing): 8GB\nD(6-core) Intel i7-6850K [3.8GHz],\n32GB DDR4, Nvidia GTX1080 (Pascal) 8GB\nTABLE II: Software and workload setup.\nType Software Specification DL Auto-Tuner\nSupport Ubuntu 20.04, Docker 20.10.7 RD: Random\nCompute CUDA 11.3.1 [21] AT: AutoTVM [8]\nDriver Nvidia Driver 465.31 GA: Genetic [8]\nLibrary Pytorch [18],Apache MXNet [17] GR: Grid [8]\nCompiler TVM 0.8 & LLVM 11 [3], [22]\nType Workload Specification\nDL Model MobileNet-V1/V2 [23], ResNet-18 [20], DenseNet-121\n[24], VGG-13/16/19 [13], {batch 1, 3x224x224 }\nmany complex operators, auto-tuning entails repeatedly\nexecuting thousands of candidate tensor programs in an\nisolated target-device to ensure accurate latency\nmeasurement. With the growing number of larger DL\nmodels, auto-tuning engages the host platform and\ntarget-device for extended periods of time. This is\nexacerbated by the need to repeat auto-tuning for any\narchitectural changes to the DL model and target-devices.\nThese aforementioned issues are amplified when\nconsidering the scale of Cloud datacenters, whereby DL\nproviders will manually or automatically (via auto-tuning)\noptimize the many DL models underpinning MLaaS, as well\nas allow users to directly perform auto-tuning (Amazon\nSageMaker Neo [5], Alibaba MNN [6], Glow [7]). Thus\nwhilst DL model optimization is effective at reducing the\nmodel inference latency for MLaaS, auto-tuning requires a\nconsiderable time, compute resources, incurring subsequent\nenergy costs to complete. These optimization costs result in\nreduced availability and system throughput for DL-focused\nCloud datacenters, and incur higher energy consumption - a\nbarrier towards designing sustainable AI infrastructure.\nIII. DL O PTIMIZATION PERFORMANCE & C OST STUDY\nA. Analysis Setup\nWe have conducted an experimental study pertaining to\nthe performance improvement and energy cost of various DL\nmodels when applying different optimization techniques.\nDeployment: Several prominent, well established CNN\nDL models [25] were examined using a variety of DL\nframeworks, target-devices and platforms as shown in Tables\nI & II. We applied both graph-level optimization and\nauto-tuning, comparing with both baseline implementations\nand pre-optimized configurations. In all cases, we configured\nauto-tuners and graph optimizers as reported in related\npublications and code bases from [26].\nMeasurement: We measure DL model latency and energy\nconsumption after applying optimization techniques and\nDenseNet-121 MobileNet-V2ResNet-18 VGG-19\nModel024Latency (ms)MXNet Runtime\nPytorch Runtime\n0.00.10.20.3\nGPU Energy (J)MXNet Energy\nPytorch Energy(a) Latency & Run-time Energy\nDenseNet-121 MobileNet-V2ResNet-18 VGG-19\nModel012Compilation Energy (kJ)MXNet Pytorch (b) Compilation Energy\nFig. 2: DL model latency, run-time & compilation energy with\ndifferent DL libraries [opt. level: 3]\nDenseNet-121VGG-19\nModel0246Latency (ms)Platf: A\nPlatf: BPlatf: C\nPlatf: D\n0.00.10.20.3\nGPU Energy (J)GPU Energy\nRuntime\n(a) Latency & GPU energy\nDenseNet-121 MobileNet-V2ResNet-18VGG-19\nModel024Compilation Energy (kJ)Platf: A\nPlatf: BPlatf: C\nPlatf: D (b) Compilation Energy\nFig. 3: DL Model latency, run-time & compilation energy with\ndifferent platforms [opt. level: 3]\nDenseNet-121 ResNet-18 VGG-19\nModel0.00.10.2GPU Energy (J)Lvl. 0\nLvl. 1Lvl. 2\nLvl. 3Lvl. 4\n(a) GPU Energy\nDenseNet-121 ResNet-18 VGG-19\nModel0123Compilation Energy (kJ)Lvl. 0\nLvl. 1Lvl. 2\nLvl. 3Lvl. 4 (b) Compilation Energy\nFig. 4: DL Model run-time & compilation energy with varied graph\noptimization levels. [framework: MxNet / platform: A]\nquantify their cost implications. We observe GPU and CPU\npower dissipation using a custom profiler based on Nvidia’s\nManagegement Library (NVML) [27] and RAPL/MSR\ninterface for Intel/AMD CPUs. Albeit not explicitly stated in\nprior studies, all experiments were conducted using\ngraph-level optimization level 3 (TVM), with exception to\nstudying graph optimization levels.\nB. Library, Platform, and Graph Optimization\nLibrary: As shown in Figure 2a & 2b, the majority of DL\nmodels converted from PyTorch to TVM IR perform on\naverage 1.05× faster than MXNet, however incur a 1.11×\nmean energy increase during compilation, with a strong\npositive correlation (Pearson: 0.9987) between model latency\nand energy. Differences amongst DL frameworks stem from\nvaried DL model IR and compiler parsing approaches; i.e.\nthe compiler maps equivalent operator definitions due to lack\nof direct matches within the operator library.\nPlatform: As shown in Figures 3a & 3b, DL model\nlatency and energy profiles vary across platforms and\ntarget-devices. Observably, compiling DenseNet-121 on\nplatforms Aand Cconsumed up to 4.16× more energy\ncompared to B, despite slower CPUs with higher number of\ncores and lower Thermal Design Power. Unique CPU and\ntarget device combinations during compilation and model\n\n3540No Tuning\nRDGR\nGAAT\nResNet-18 VGG-16\nModel0510Latency (ms)(a) Latency (ms)\n23No Tuning\nRDGR\nGAAT\nResNet-18 VGG-16\nModel0.00.20.4Energy (J) (b) Runtime Energy (J)\nResNet-18 VGG-16\nModel0246Tuning Energy (MJ)RD GR GA AT (c) Total Tuning Energy (MJ)\nResNet-18 VGG-16\nModel01000020000Tuning Time (s)RD GR GA AT (d) Total Tuning Time (s)\nFig. 5: Energy and performance impact of auto-tuning: [Platform: A], [HW measurements: 500], [Graph opt. level: 3]\n012345678\nOp. ID0100200300400500CandidatesAutoTVM\n012345678\nOp. IDGenetic\n012345678\nOp. IDGrid Search\n012345678\nOp. IDRandomNo error Compile Timeout Instantiation Error Runtime Error\nFig. 6: Error occurrences across different auto-tuners [Hardware\nmeasurements: 500, Platform: A, Model: ResNet-18]\nTABLE III: Graph-level optimization primitives\nLvl. Primitives\n0 Simplify/Partition Graph, Simplify Expressions, Infer Types\n1Fuse Operators, To BBNF, To ANF, To GNF,\nEliminate Dead Code, Partially Evaluate, Inline Ops\n2 Fold Constants, Split Arguments, Lazy Gradients, DynToStat\n3Canonicalize Operators, Forward/Backward Scale Axis,\nEliminate Common Sub-expressions\n4 Combine Parallel: Conv2D, Dense, Matmul; Math Approx.\nexecution exhibit different cost and performance patterns\ndependant on the platform composition, the choice of DL\nframework, model architecture, operator computational\ncomplexity, as well as the suitability of tensor program\nimplementations towards given target-device.\nGraph: As shown in Figure 4a, applying further graph\noptimization levels (see. Table III) decreases model latency\nand energy consumption by 25-50%, yet also increases\ncompilation energy by 29-60% for levels 0 to 4 as shown in\nFigure 4b. This stems from strong correlation between model\nlatency, energy cost, and computation required to apply\nconsecutive graph optimizations. We observe that latency\nimprovements at the same optimization level vary across\nmodels; i.e. applying level 4 to VGG-19 improved latency by\n22.5% while for DenseNet-121 a 50.1% improvement,\nhowever required 60.6% more energy during compilation.\nC. Auto-tuning\nPerformance: Our experiments indicate that the choice of\nauto-tuner alters latency improvement and energy profiles of\nDL models, as shown in Figures 5a & 5b. Compared to a\nbaseline model (model compiled with default operator\nimplementations), sophisticated auto-tuners (AutoTVM,\nGenetic Search) result in latency decrease of 9.22 - 10.35×TABLE IV: Cold candidate impact across platforms, average of all\ntuned operators when tuned with Auto-TVM\nModel Platform Num. Cold Time Cold % Energy Cold\nResNet-18A 133±21 26.74±4.3 52.1±10.4kJ\nB 134±18 26.86±2.8 36.7±7.9kJ\nC 130±11 26.13±2.2 37.1±4.2kJ\nD 139±14 27.92±2.9 29.0±4.6kJ\nVGG-16A 202±29 40.51±5.9 76.7±2.0kJ\nB 201±28 40.24±5.6 49.3±2.4kJ\nC 200±29 40.16±5.9 49.1±4.2kJ\nD 216±25 43.33±5.0 41.2±3.1kJ\nand energy cost reduction of 1.91 - 9.98× (Figure 5b). In\ncontrast, brute-force auto-tuners (Random, Grid Search) can\nresult in moderately faster or sometimes slower latency\ncompared to baseline as per Figure 5a. Sophisticated\nauto-tuners explore implementation spaces more efficiently,\nusing cost models to guide search towards faster candidates.\nEnergy: Similar to DL model latency, there exists a\nstrong positive correlation between auto-tuning time and\nenergy cost (0.9880 Pearson coefficient), albeit different\nauto-tuners incur varied time and energy costs across DL\nmodels as shown in Figures 5c & 5d. When applied to\nResNet-18, AutoTVM produced the lowest latency of\n0.89ms, however incurred an additional 7200–8300s and\n240–1800kJ compared to Grid Search and Genetic Search,\nwhich stems from querying and updating AutoTVM’s cost\nmodel and performing Simulated Annealing. Surprisingly,\nRandom Search was significantly costlier (23,250s &\n6,020kJ) compared to other approaches due to the random\nimplementation space traversal, inadvertently compiling and\nexecuting both slow and erroneous candidates significantly\nincreasing cost. The observed differences in cost amongst\nauto-tuners stem from operator heterogeneity, implementation\nspace size, and auto-tuners varying effectiveness at space\ntraversal and sampling promising candidates.\nFailed Candidates: We discovered that all auto-tuners\nexperienced a sizeable number of compilation and run-time\nerrors when generating candidates as shown in Figure 6.\nInstantiation (compilation) errors were the most common\nfailures observed, caused by stochasticity of auto-tuners,\nparticularly in Genetic (15%), AutoTVM (7.2%), and\nRandom (36%). Run-time errors and timeouts represented\nfurther 1.9% and 2.6% of failures. Existing auto-tuners\ndetermine when to stop optimizing based on the total number\nof performed measurements (inclusive of failures) requested\nby the user. Whilst erroneous candidates only consumed\n\nFig. 7: Performance trends of different tuning frameworks with 500 HW measurements, depicting: best-case performance trend, LOESS\nregression of candidate performance, hotcandidates and cold candidates\nA\nB\nC\nD28453551\n24383251\n25403252\n30403455Op ID: 0\n18412956\n25422654\n23412756\n23403260Op ID: 1\n21402858\n21392656\n24402756\n27383759Op ID: 2\n20382757\n17362454\n22372455\n18333757Op ID: 3\nATGAGRRDA\nB\nC\nD24524465\n29434869\n25434867\n27476074Op ID: 4\nATGAGRRD31463878\n32444375\n23464373\n31465978Op ID: 5\nATGAGRRD27473567\n23413664\n24423563\n27465073Op ID: 6\nATGAGRRD32564380\n28524477\n30504374\n37465779Op ID: 7\n020406080100Cold Candidates Energy %\nFig. 8: Percentage of energy consumed by cold candidates across\neight Conv2D operators of ResNet-18.\nadditional 11-18% energy, they reduced the opportunity for\ndesirable candidate exploration and incurred additional costs;\nthus reducing auto-tuning effectiveness and cost-efficiency.\nCold Candidates: A sizeable portion of candidates exhibit\nhigh latency, yet do not meaningfully contribute to\noptimization progress. We categorize these so called cold\ncandidates by applying LOESS regression [28]. With higher\nlatency, cold candidates incur more auto-tuning cost, as\nshown in Figure 7 and Table IV. We observe that AutoTVM\nproduced the most cold candidates early on due to cost\nmodel initialization, where Grid and Random Search\nstrategies exhibit larger diffusion. Particularly problematic\ngiven their time and energy costs, cold candidates\ncontributed on average to 50.5% of total auto-tuning cost, as\nshown in Table IV, varying across operators and platforms,\nas shown for ResNet-18 in Figure 8. Cold candidates\nproduced by AutoTVM were responsible for 17-38% of total\nauto-tuning energy cost, and 80% in worst case for Random\nSearch. As per Table IV, the choice of platform had impact\nproportional to its compute capabilities.\nConvergence: The choice of an auto-tuner determines\noptimization convergence patterns, indicated by the next\nminimum candidate trend line shown in Figure 7. More\nsophisticated frameworks (AutoTVM, Genetic Search)converge early compared to Grid Search or Random Search.\nCrucially, whilst most frameworks progressively propose\nfaster candidates, we observed that hot candidates (low\nlatency candidates that contribute positively to auto-tuning\nprogress) could be found relatively early during auto-tuning.\nFor example, as shown in Figure 7, auto-tuning Operator 7\nwith AutoTVM discovered candidates merely 100ns (3-5%)\nslower than globally fastest candidate within the first 87 out\nof 500 total measured candidates.\nD. Design Directions\nFrom our study, we identify multiple important design\ndescisions required for cost effective DL auto-tuning.\n(1) Understand energy diversity: We observe that the\ninterplay between DL models, frameworks, auto-tuners,\ntarget-devices and platforms uniquely impacts the cost and\nperformance of auto-tuners, while existing works focus\nprimarily on reducing cost w.r.t. target-devices [3], [10]. We\nalso observe that no single optimization approach exhibits a\nguaranteed latency improvement at reduced cost. Leveraging\nthese insights is useful to determine models and optimizers\nthat work well for specific target-devices and host platforms.\n(2) Avoid erroneous and cold candidates: Observably,\nall examined auto-tuners generate erroneous candidates,\nwhich do not contribute towards performance convergence\nnor explore favourable candidates. We also observe that cold\ncandidates exhibit high operator latency without contributing\ntowards successful candidate selection, incurring high time\nand energy cost. Avoiding both erroneous and cold\ncandidates is useful to reduce optimization costs whilst\nmaintaining reasonable latency improvements.\n(3) Leverage hot candidates: Our analysis suggests that\ncandidates with acceptably low latencies can be found early\nduring the candidate search. Whilst increased number of\nmeasurements results in operator latency improvement, it is\npossible to ascertain hotcandidates (exhibit low latency and\npositively contribute to convergence) soon after auto-tuning\ncommences. Leveraging hot candidates could help to rapidly\ndetermine the efficacy of an optimization technique.\n\nIV. T RIMMER FRAMEWORK\nA. Overview\nTrimmer is designed to improve the cost-efficiency of\nauto-tuning and operates as a component for optimizing\ntrained DL models that are ready to be deployed within a\nCloud datacenter, as shown in Figure 9. Such models\noriginate from both the provider (as part of their MLaaS\noffering) and users (submitting models for training or\ndeployment). Trimmer achieves fast and low-cost auto-tuning\nvia enhancements at both operator and DL model levels.\nAt operator-level, Trimmer performs neural network based\ncold candidate filtering that predictively excludes candidate\nimplementations that exhibit poor performance ( cold\ncandidates) and re-sampling for more favourable candidates\n(hotcandidates) to accelerate auto-tuning convergence onto\nsufficiently optimal implementations quicker. At DL\nmodel-level, Trimmer performs Survey-tuning whereby each\noperator is partially optimized using a small number of\nhardware measurements and periodic latency measurement of\nthe complete DL model, thus allowing early completion\nbased on measured latency improvements. Combining these\napproaches, Trimmer can optimize models simultaneously\nacross a Cloud cluster with multiple hardware platforms,\nranking their progress and suspending optimization based on\ncomparative latency improvements. We implement this\nfunctionality using Docker and leverage custom RPC-based\nroutines to execute model auto-tuning remotely.\nFormally, Trimmer’s goal is to minimize DL model\ninference latency ftby auto-tuning each operator\n{oi∈O|i= 1···K}, where Kis the number of operators\nin the model. The amount of time spent on optimizing each\noidepends on its rate of its latency improvement relative to\nother ojin the model and overall improvement to model\nlatency. Overall, Trimmer’s objective is to optimize the DL\nmodel ft∗, given a (user-specified) latency goal g, where\ng≤latency of the unoptimized model.\nminKX\ni=1ois.t.fo∗≤g (1)\nB. Cold Candidate Filtering\nInspired by learned index structure approaches [29], we\ncreated a DL model and leveraged outputs of its intermediate\nlayer to query candidates based on their similarity, and filter\nout top- ncold candidates ahead of hardware measurement.\nOverview & Training: We designed a three-layer fully\nconnected (FC) network with ReLU activation function [30]\nas outlined in Table V. We implemented and trained our\nmodel using PyTorch [18]. During training of our FC\nnetwork, we minimize the Mean Squared Error (MSE)\nagainst individual operator latency, using 20,000 data points\ncollected during our study (Section III-C) as the dataset. We\nprepare our dataset by splitting it into <train:validation:test >\nsets with a 7:1:2 ratio and leverage the validation set for\nhyperparameter tuning following prior work in predicting\nCloud DL  \nInference NodesCloud Management \nInfrastructure\nModel Profiler  Inference EngineTraining Engine\nCandidate Profiler  \nMLaaS  \nTraining EngineSurvey T uning Manager\nNN REGRESSOR\nCOLD FIL TER RE-SAMPLER\nCost\nModelSearch\nStrategyCandidate  \nSpace  Infer  Train Neural  \nSampler  TRIMMER  \nTVM\nCOMPILER  AUTO \nTUNER  Optimization EngineCloud InterfaceProvider Models User Models Trained Models Untrained ModelsFig. 9: Trimmer system architecture\nTABLE V: FC Network Architecture. Tuple (x,y) is layer dimension,\nx and y are the number of input and output neurons, respectively.\nLayer Type Layer Dimensions\nEmbedding ei(Rspace , 10)\nInput (EP\ni=1ei, 32)\nMiddle (32, 32)\nOutput (32, 1)\noperator latency [31]. Our model was trained for 10 epochs\nutilizing the Adam Optimizer [32], with batch size 1024,\nlearning rate of 1e−3(min 1e−9) and Plateau Patience of 1.\nThe model learns relationships between operator latency and\nthe operator, candidate and target-device characteristics.\nModel Inputs: To capture non-linearity of the mentioned\ncharacteristics, the input to our FC network is a vector that\nincludes: (1) implementation candidate configuration, where\neach entry is an integer describing low-level optimization\nparameters of the configuration; (2) features of the\ntarget-device and its host platform; (3) the theoretically\nachievable operator FLOPs; (4) a unique operator identifier;\nand (5) a unique representation of operator arguments (for\nconvolution - kernel size, padding, stride...). During filtering\nwe extract the number of outputs Rspace for each operator\nconfiguration by accessing its total configuration space and\nconstruct an embedding , where each integer entry is a vector\nof parameters of size k, and kis a hyperparameter that we\nset to 10based on empirical findings. Categorical features\nsuch as target-device or operator characteristics are\ntransformed into integers and have their own embeddings.\nWe then feed the concatenated features into our FC network.\nExploration: As described in Section III-C, it is beneficial\nto reduce costs associated with measuring cold candidates,\nhowever, the auto-tuner should explore a wide range of\npoints in the implementation space to identify hot\ncandidates. To achieve this, Trimmer uses an inverse\nϵ-greedy strategy to incentivize exploitation of filtered\ncandidates early on during auto-tuning and progressively\nreduces the probability of candidate filtering after each cost\nmodel update, such that space exploration is favoured as\nsoon as auto-tuner search strategy and cost model are stable.\n\nAlgorithm 1 Cold Candidate Filtering\nInput: (Samples ,ϵ, k,model )\n1: // batch inference the task configuration samples\n2:embeddings ←MODEL .PREDICT (Samples )\n3: // for each candidate task’s embedding\n4:foreinembeddings do\n5: rand←RAND\n6: // ϵis a decreasing parameter\n7: ifrand < ϵ then\n8: // return the number of cold candidates within ksimilar samples\n9: o←SIMILAR INDATABASE (k, e)\n10: ifo==kthen\n11: // remove sample from the Set S\n12: REMOVE (s)\nQuerying configuration: Trimmer heuristically prunes the\nproposed candidates if the top- nsimilar candidates are cold\ncandidates, as described in Section III-C. We leverage the\nmiddle layer output of our FC network to predict candidate\nlatency by querying top- nsimilar configurations within an\noperator tuning database using Cosine Similarity (commonly\nused to identify similar samples in a vector space [33]). We\nreplace the pruned high-latency candidates with new unseen\nsamples to ensure effective exploration. These probabilistic\nand heuristic strategies are then combined in a single\nprocedure, as shown in Algorithm 1.\nC. Survey Tuning\nExisting auto-tuners optimize operators sequentially,\nperforming Nhardware measurements before optimizing the\nnext operator. Auto-tuning is considered complete once all\noperators have been optimized fully (up to specified number\nof hardware measurements N), as shown in Figure 10. With\nthis assumption, latency of a DL model can be ascertained\nonly after all operators are optimized, with the user having\nno knowledge of achieved DL model latency whilst\nauto-tuning is underway. Existing auto-tuners enable early\nstopping of optimization by an arbitrarily set threshold,\nhowever, it is performed on a per-operator basis, with no\nmeans to compare improvements relative to other operators.\nOur study in Section III-C suggests that well-performing\n(hot) candidates can be found relatively early during the\nauto-tuning process. We leverage this in Trimmer and reduce\noptimization cost by providing Survey-tuning , as depicted in\nFigure 10, both at operator and model level.\nOperator Level. Trimmer places all tunable operators into\na queue and performs auto-tuning in batches ofnhardware\nmeasurements. After a complete tuning epoch (i.e. each\noperator has completed a batch of measurements), we\ncompile and evaluate the DL model using best candidate\noperator configurations found so far. This enables Trimmer\nto suspend or early-stop auto-tuning based on the trade-off\nbetween projected model latency improvement and\noptimization cost so far. The early-stop or suspension\nmechanism uses achieved model latency as ≤goalg. If goal\ngis not reached, or not reached within a desired cost budget\n(time, energy, monetary...), Trimmer compares the rate of\nlatency reduction compared to last batch, as:\nOP.1\nOP.2\nOP.NTUNING ST OP SEQUENTIAL  \nTUNING MEASUREMENT  MODEL EV ALUA TION  \nOP.1\nOP.2\nOP.NSUR VEY \nTUNINGMODEL EV ALUA TION  \nBATCH TUNING STOPTUNING  \nSTART\nTUNING  \nSTART Fig. 10: Sequential and Survey tuning, [OP = Operator].\nstop =ω1xt+xt−1\nxt−1≤ϕ (2)\nxt=ω2δft\nδOt(3)\nWhere xtis the ratio of difference in model latency f\nbetween tandt−1over the auto-tuning time Otfor the\nepoch. To account for the cases (1) where both xtandxt−1\nare positive (slower), and (2) where xtis faster but xt−1was\nslower, ω1is set to −1and1otherwise; ω2is set to −1if\nboth δftandδOtare negative, 1otherwise. These cases\noccur due to high non-linearity of the operator candidate\nspace that may cause the auto-tuning to diverge in\ndiscovered latency abruptly. The intuition is that Trimmer\ncan maximize optimization efficiency by checking whether\nthe model latency is decreasing every epoch at a sufficient\nrate given its time and energy cost, where ϕis a\nhyperparameter that specifies whether the change in the ratio\nshould be greater than ϕ% to avoid excess tuning, where the\nincremental performance change between models per batch\nis not significant. This hyperparameter allows practitioners to\nprioritize either achieved performance or cost accordingly. In\nour case, we have set ϕto -0.25 providing an appropriate\nbalance between performance improvement and cost in\noptimization, motivated by empirical study.\nModel Level: The suspension approach described above\ncan also be applied at model level (measuring model latency\nat each batch). This transition from per-operator to per-model\nallows for meta-tuning, where the relative speed-up of\nmodels is compared to concurrent auto-tuning processes of\nother models within a cluster of machines. Inspired by [34],\nwe implement multi-model auto-tuning by ranking models\nby their most recently achieved latency improvement in an\nauto-tuning batch, and suspend models that perform the\nworst. The intuition behind this population-based approach is\nto focus optimization effort onto the most promising models\nwhen tuning multiple models within the cluster; and\ndepending on user set criterion of latency threshold or cost\nobjective. The suspension interval can include a number of\nconfigurable plateau iterations (i.e. poor performance\nimprovement in consecutive batch intervals). We synchronize\nmulti-model auto-tuning processes across a cluster of\nmachines at intervals of completed batches of individual\nmodels - permitting each model to be optimized for several\nbatches before being compared.\n\nTABLE VI: Single Platform evaluation results (latency, auto-tuning\ntime and total energy consumed during auto-tuning\nModel TR AT RL CH Base.\nLatency\n(ms)Alexnet 0.79±0.02 0.85±0.05 0.84±0.09 0.82±0.08 4.42\nVGG-16 4.68±0.49 4.78±0.28 5.85±0.45 5.83±0.38 9.66\nMobilenet 0.65±0.03 0.67±0.02 0.76±0.06 0.74±0.05 1.24\nResNet-18 1.39±0.29 0.86±0.08 1.11±0.06 1.03±0.08 8.48\nTuning\nTime\n(m)Alexnet 119±0.29 116±0.25 121±0.42 127±0.40\nVGG-16 194±0.49 207±0.22 296±0.98 298±0.30\nMobilenet 213±0.48 286±0.52 216±0.58 214±0.42\nResNet-18 228±0.24 401±0.59 353±0.74 279±0.44\nTuning\nEnergy\n(MJ)Alexnet 1.6±0.22 1.9±0.22 2.3±0.46 2.4±0.50\nVGG-16 3.4±0.59 3.4±0.21 5.5±1.19 5.6±0.32\nMobilenet 3.6±0.81 4.5±0.48 3.6±0.58 3.6±0.41\nResNet-18 21.2±1.6 26.6±2.4 29.5±3.4 31.8±3.1\nV. E VALUATION\nA. Setup\nEnvironment: We utilized four unique DL models and\nperformed our experiments on hardware platforms described\nin Table I. Where appropriate, models selected for\noptimization share identical model configurations and\nhyperparameters with prior work [8], [10], and were\nconverted from Pytorch using TVM. Throughout our\nevaluation, we selected graph optimization level 3 in line\nwith code bases online to avoid value approximation\noptimizations at higher levels (see Table III) that may affect\nmodel accuracy. In this work, we also assumed access to an\noffline database of historical optimization data. In the case\nwhere there is no historical data, auto-tuning can be executed\nto collect niterations of data for our FC network training.\nAuto-tuners: We compared Trimmer (TR) against three\nstate-of-the-art auto-tuners: AutoTVM (AT): utilizes XGBoost\nto avoid excessive hardware measurements and Simulated\nAnnealing (SA) optimizer to search through the candidate\nspace given feedback from the cost model [3]; Reinforcement\nLearning (RL): uses the XGBoost cost model and Proximal\nPolicy Optimization [10] and Q-Learning [11] as the\noptimizer to propose candidates; and Chameleon (CH):\nextending the RL approach, and uses K-means clustering to\nreduce similar candidates in the search space to accelerate\nauto-tuning [10]. The auto-tuners were configured to perform\n500 hardware measurements (candidate batch size = 64), as\nper default configurations of the respective auto-tuners.\nMetrics : We measured effectiveness of Trimmer using DL\nmodel latency (before and after applying auto-tuning), as\nwell as total optimization time and energy consumption that\nconstitute the auto-tuning cost per model. We also monitored\nthe platforms under test for CPU and GPU metrics such as\nutilization or memory usage and measured any resource\noverheads incurred from Trimmer’s mechanisms.\nExperiment scenarios : We have conducted experiments\nthat evaluate Trimmer w.r.t. reduction in, and cost-efficiency\nof, auto-tuning time and energy cost. We evaluated Trimmer\non Single-platform DL model auto-tuning, performing\nisolated experiments on individual target-devices, as well as\nCloud cluster where we coordinate four auto-tuning\nTR AT RL CH\nAuto-tuning Approach0123T otal Tuning Energy (MJ)AlexNet\nTR AT RL CH\nAuto-tuning Approach0246VGG-16\nTR AT RL CH\nAuto-tuning Approach024MobileNet\nTR AT RL CH\nAuto-tuning Approach0102030ResNet-18Fig. 11: Total auto-tuning energy consumption\nTR AT RL CH\nAuto-Tuning Approach0.00.20.40.6Energy(MJ) / 1ms improvementAlexNet\nTR AT RL CH\nAuto-Tuning Approach0.00.51.01.5VGG-16\nTR AT RL CH\nAuto-Tuning Approach02468MobileNet\nTR AT RL CH\nAuto-Tuning Approach01234ResNet-18\nFig. 12: Ratio of average total energy spent (MJ) during auto-tuning\nto achieve 1ms of model latency improvement\ninstances across four platform Amachines to compare\nSurvey tuning against sequential auto-tuning in parallel.\nB. Experiment Results: Single Platform, Sequential Tuning\nPerformance: Trimmer achieved a greater model latency\nreduction compared to other auto-tuning approaches for the\nmajority of scenarios. As shown in Table VI, Trimmer\nachieved the fastest inference time at 0.79ms, 4.68ms and\n0.65ms for AlexNet, VGG-16, MobileNet, respectively. In\nthe case of ResNet-18, Trimmer achieved comparable results\nto RL and Chameleon approaches. The reason for these\nresults is due to Trimmer’s cold candidate filtering, allowing\nfor quicker identification of hotcandidates relatively early\nwithin the auto-tuning process. Such phenomena is\nobservable when inspecting the shape of operator candidates\nmeasurement patterns as shown in Figure 13.\nCost-efficiency: As shown in Table VI, Trimmer\ncompleted auto-tuning with a lower time cost than other\nauto-tuners for VGG-16, ResNet-18, and performed within\nmargin of error of the fastest framework for remaining\nmodels. Moreover, Trimmer’s system energy consumption is\nlower during auto-tuning for AlexNet and ResNet-18 and\nscored on par with the least energy-hungry framework\n(AutoTVM) in remaining models cases. Crucially, such\nauto-tuning costs should be considered in the context of their\ncost-efficiency (i.e. when an auto-tuner produces low model\nlatency, yet consumes significantly more energy). Across all\nmodels, Trimmer achieves the highest cost-efficiency for the\namount of energy required to achieve 1ms latency reduction\nas depicted in Figure 12 with an average improvement over\nother auto-tuners of 14-33% (AlexNet), 2-54% (VGG-16),\n16-24% (MobileNet) and 14-29% (ResNet-18). This stems\nfrom cold candidate filtering and early suspension upon\ndetection of insufficient latency reduction, and is linked to\nthe design of our neural sampler, which was trained on a\nrange of model and target-device samples as per Table I.\nCandidates: Our results suggest that the auto-tuner cost\nmodel guided by Trimmer proposes fewer globally poor and\nfailed candidates compared to RL and Chameleon, and on\naverage explores more candidates than AutoTVM within the\n\n0 100 200 300 400 500\nCandidate Hardware Measurement0.00000.00250.00500.00750.01000.01250.0150Candidate Latency (ms)Trimmer\n0 100 200 300 400 500\nCandidate Hardware MeasurementAutoTVMFig. 13: Comparison of candidate hardware measurement patterns in\nTrimmer and AutoTVM in VGG-16 (Operator-5)\nMobileNet AlexNet\nModel051015T otal Tuning Energy (MJ)Survey Tuning Sequential Tuning\n(a) Total tuning energy\nTrimmer AutoTVM RL Chameleon\nTuning Algorithm0500100015002000Number of failed candidates (b) Total failed candidates\nFig. 14: Total tuning energy & number of failed candidates when\ntuning VGG-16 intel.v100 – Survey vs. sequential in parallel\nsame time (see Figure 14). This stems from the design of the\nTrimmer re-sampling mechanism, which filters out cold\ncandidates from the currently measured batch, whilst\ngreedily reintroducing a portion of new candidates proposed\nby the cost model to diversify the batch. This produces more\ncost-efficient exploration (as more overall candidates are\nexplored) and inadvertently explores more cold candidates\ncompared to filter-less AutoTVM approach. The additional\nprocessing of candidate filtering resulted in 3% CPU\nutilization increase compared to AutoTVM, which given the\nlatency improvement and cost-efficiency attained by\nTrimmer, we deem an acceptable resource cost overhead.\nC. Experiment Results: Cloud Clusters\nPerformance & Energy: Trimmer was able to achieve a\n21.8% and 40.9% reduction to total auto-tuning energy cost\nfrom Survey tuning in comparison to parallel auto-tuning for\nAlexNet(1.8MJ) and MobileNet(5.97MJ), respectively as\nshown in Figure 14. The reason for this energy reduction is\ndue to Survey tuning capturing improvements to model\ninference latency across all operators as well as other models\nat regular batch interval (and if insufficient performance\ngains were detected, optimization is suspended). A key\nadvantage of Trimmer Survey tuning is that it allows for\nstrongly performing auto-tuning frameworks to conduct\nadditional batches over less effective algorithms, improving\nit’s cost-efficiency. This is particularly the case for\nMobileNet where Trimmer achieved an inference time 0.3ms\n(8%) faster than AutoTVM running to full completion per\nTable VII.\nAs shown in Figure 15, when using the Survey tuning to\nperiodically evaluate the model at each batch interval, we\nobserved that each framework exhibited different\nconvergence patterns across both models. For the case of\nMobilenet, we observed that whilst all frameworks producedTABLE VII: Cloud cluster Survey tuning vs. parallel auto-tuning\nModel Survey Parallel Improve\nTuning Energy (MJ)MobileNet 10.76 18.21 40.9%\nAlexNet 6.80 8.70 21.8%\nModel Run-time (ms)MobileNet 0.629 0.684 8%\nAlexNet 0.797 0.805 1%\n0 1 2 3 4 5 6 7\nSurvey Batch #0.70.80.91.01.1Model Latency (ms)\nMobileNetTrimmer RL Chameleon AutoTVM\n0 1 2 3 4 5 6 7\nSurvey Batch #0.81.01.21.41.6\nAlexNet\nFig. 15: Survey tuning at different batches (64 candidates per batch)\na 10.1–30.2% inference latency improvement between\nBatch-0 and Batch-1, both RL and Chameleon were\nsuspended at a relatively early batch interval due to reduced\nperformance improvements detected at Batch-3 – both in\nterms of its own convergence gradient and in comparison to\nother frameworks. AutoTVM was suspended at Batch-5 for\nachieving relatively little improvement over two consecutive\nintervals, as well as in relation to Trimmer, which was able\nto produce best inference time at Batch-5 (0.62ms).\nThese results affirm that Trimmer: (1) enables the\nauto-tuner search strategy to explore further into the\ncandidate space given the same amount of time and energy,\nproviding more cost-efficient auto-tuning; and (2) via the\nSurvey tuning meta-strategy, Trimmer achieves overall faster\noptimization times compared to Sequential tuning at\noperator-level, and Parallel optimization of multiple models\nsimultaneously due to its ability to suspend tuning given less\nfavourable performance improvement across a Cloud cluster.\nAdditionally, given Survey tuning is a meta-tuning\nmechanism that works in conjunction with the auto-tuner and\ndoes not modify its operation and can integrate with any\nauto-tuner that reports latency measurements in real-time\nincluding ones that do not rely on templates for\nimplementation scheduling [9], [11].\nVI. R ELATED WORK\nCost-efficient DL for Cloud datacenters. Creating\ncost-efficient DL systems has been gaining traction within\nthe ML community, most notably for system energy\nconsumption [12], [35]. Frameworks designed to improve\nDL-focused Cloud datacenter cost-efficiency have recently\nbeen proposed to accelerate DL model hyperparameter\ntuning, efficient architecture search and training [36].\nAmazon SageMaker [37] focuses on training,\nhyperparameter tuning and optimization (Neo). Lorien [38]\nsupports tuning of DL models across clusters of machines\nand collects best performing schedules to achieve faster\ntraining of predicting model accuracy.\nDL Tensor Program Optimization. Leveraging search\nalgorithms and ML to automatically optimize programs is an\n\nactive research area. Search-based approaches have been\nshown to optimize complex FFT or BLAS routines [39] and\nperform I/O parameter search [40]. DL compilers\nincreasingly adopt auto-tuning; AutoTVM [8] uses Simulated\nAnnealing and Gradient-boosting to parameterize templates\nfor tensor program implementations. Deep Reinforcement\nLearning (DRL) is used for implementation search and\nfiltering by both Chameleon [10] and AdaTune [19], whilst\nAutophase [41] leverages DRL to generate a compilation\noptimization order. Autoschedulers such as Ansor [9]\ngenerate implementations hierarchically and fine-tune their\nparameters via evolutionary search, whilst FlexTensor [11]\nexplores program space using DRL and heuristics to\ngenerate candidates. One-shot-tuner [42] modifies AutoTVM\nwith a Transformer cost model to predict operator\nperformance. Trimmer proposes a sampling mechanism\nbased on an FC neural network to encourage measurements\nof fast candidates whilst filtering out candidates that are\nlikely to be slow or erroneous. Trimmer also introduces an\nϵ-greedy meta-tuning strategy – Survey tuning at both\noperator and model level that reduces auto-tuning cost by\nenabling early finish, periodically measuring DL-model-level\nperformance. Survey tuning further enables auto-tuning\nacross a Cloud cluster and multiple models simultaneously.\nVII. C ONCLUSIONS\nIn this paper we propose Trimmer: a DL model\nauto-tuning framework that performs high-performance and\ncost-efficient tensor program optimization for Cloud\ndatacenters. We have empirically analyzed the diverse energy\nand performance characteristics of different DL model\nauto-tuning and optimization techniques across various\nhardware devices. Through conducting experimentation we\nhave demonstrated that Trimmer is capable of improving DL\nmodel performance whilst reducing auto-tuning energy cost\nconsiderably. We hope that our framework provides new\ninsights to aid the study – and creation of – cost-efficient\nCloud datacenters designed to provision MLaaS.\nREFERENCES\n[1] I. Goodfellow, B. Yoshua, and C. Aaron, Deep learning . MIT press\nCambridge, MA, USA, 2016.\n[2] K. Hazelwood et al. , “Applied machine learning at facebook: A\ndatacenter infrastructure perspective,” in HPCA , 2018.\n[3] T. Chen et al. , “TVM: An automated end-to-end optimizing compiler\nfor deep learning,” in USENIX OSDI , 2018, pp. 578–594.\n[4] A. Adams et al. , “Learning to optimize halide with tree search and\nrandom programs,” ACM Transactions on Graphics , pp. 1–12, 2019.\n[5] A. Inc. (2020) Amazon sagemaker neo. [Online]. Available: https:\n//aws.amazon.com/sagemaker/\n[6] X. Jiang, H. Wang, Y . Chen, Z. Wu, L. Wang, B. Zou, Y . Yang, Z. Cui,\nY . Cai et al. , “MNN: A universal and efficient inference engine,” MLSys ,\n2020.\n[7] N. Rotem, J. Fix, S. Abdulrasool, G. Catron, S. Deng, R. Dzhabarov,\nN. Gibson, J. Hegeman, M. Lele, R. Levenstein et al. , “Glow: Graph\nlowering compiler techniques for neural networks,” arXiv preprint\narXiv:1805.00907 , 2018.\n[8] T. Chen et al. , “Learning to optimize tensor programs,” in NeurIPS ,\n2018, pp. 3389–3400.\n[9] L. Zheng et al. , “Ansor: Generating high-performance tensor programs\nfor deep learning,” OSDI , 2020.[10] B. H. Ahn et al. , “Chameleon: Adaptive code optimization for expedited\ndeep neural network compilation,” in ICLR , 2020.\n[11] S. Zheng et al. , “Flextensor: An automatic schedule exploration\nand optimization framework for tensor computation on heterogeneous\nsystem,” in ACM ASPLOS , 2020, p. 859–873.\n[12] R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni, “Green ai,” arXiv\npreprint arXiv:1907.10597 , 2019.\n[13] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” ICLR , 2015.\n[14] C. Raffel et al. , “Exploring the limits of transfer learning with a unified\ntext-to-text transformer,” arXiv e-prints , 2019.\n[15] M. Tan and Q. V . Le, “Efficientnet: Rethinking model scaling for\nconvolutional neural networks,” PMLR , 2019.\n[16] M. Zakarya, “Energy, performance and cost efficient datacenters: A\nsurvey,” Renewable and Sustainable Energy Reviews , vol. 94, 2018.\n[17] Apache Software Foundation. (2020) Apache MXNet | a flexible\nand efficient library for deep learning. [Online]. Available: https:\n//mxnet.apache.org\n[18] PyTorch. (2020) Pytorch. Facebook’s AI Research lab (FAIR). [Online].\nAvailable: https://pytorch.org/\n[19] M. Li et al. , “Adatune: Adaptive tensor program compilation made\nefficient,” Advances in Neural Information Processing Systems , 2020.\n[20] K. He et al. , “Deep residual learning for image recognition,” in IEEE\nconference on Computer Vision and Pattern Recognition , 2016.\n[21] NVIDIA. (2020) Nvidia cuda. [Online]. Available: https://developer.\nnvidia.com/cuda-toolkit\n[22] C. Lattner, “Llvm and clang: Next generation compiler technology,” in\nThe BSD conference , vol. 5, 2008.\n[23] A. G. Howard, M. Zhu et al. , “Mobilenets: Efficient convolutional\nneural networks for mobile vision applications,” arXiv preprint\narXiv:1704.04861 , 2017.\n[24] G. Huang, Z. Liu, L. Van Der Maaten, K. Q. Weinberger et al. , “Densely\nconnected convolutional networks,” in CVPR , 2017.\n[25] M. Co¸ skun et al. , “An overview of popular deep learning methods,”\nEuropean Journal of Technique (EJT) , vol. 7, no. 2, pp. 165–176, 2017.\n[26] “github - apache/tvm: open deep learning compiler stack for\ncpu, gpu and specialized accelerators,” 2022. [Online]. Available:\nhttps://github.com/apache/tvm/\n[27] NVIDIA. (2020) Nvidia management library. [Online]. Available:\nhttps://developer.nvidia.com/nvidia-management-library-nvml\n[28] W. S. Cleveland and S. J. Devlin, “Locally weighted regression: an\napproach to regression analysis by local fitting,” Journal of the American\nstatistical association , vol. 83, no. 403, pp. 596–610, 1988.\n[29] T. Kraska et al. , “The case for learned index structures,” in International\nconference on management of data , 2018, pp. 489–504.\n[30] V . Nair and G. E. Hinton, “Rectified linear units improve restricted\nboltzmann machines,” in ICML , 2010.\n[31] L. Zhang et al. , “Nn-meter: Towards accurate latency prediction of deep-\nlearning model inference on diverse edge devices,” in MobiSys , 2021.\n[32] K. Xu et al. , “Show, attend and tell: Neural image caption generation\nwith visual attention,” in ICML , 2015.\n[33] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet, “Front-\nend factor analysis for speaker verification,” IEEE/ACM TASLP , 2011.\n[34] M. Jaderberg, V . Dalibard, S. Osindero, W. M. Czarnecki, J. Donahue,\nA. Razavi, O. Vinyals, T. Green, I. Dunning, K. Simonyan, C. Fernando,\nand K. Kavukcuoglu, “Population based training of neural networks,”\narXiv preprint arXiv:1711.09846 , 2017.\n[35] T.-J. Yang et al. , “Netadapt: Platform-aware neural network adaptation\nfor mobile applications,” in ECCV , 2018.\n[36] D. Golovin, B. Solnik, S. Moitra et al. , “Google vizier: A service for\nblack-box optimization,” in ACM SIGKDD , 2017.\n[37] E. Liberty, Z. Karnin, B. Xiang, L. Rouesnel et al. , “Elastic machine\nlearning algorithms in amazon sagemaker,” in SIGMOD , 2020.\n[38] C. H. Yu et al. , “Lorien: Efficient deep learning workloads delivery,” in\nProceedings of the ACM Symposium on Cloud Computing , 2021.\n[39] M. Frigo and S. G. Johnson, “The design and implementation of fftw3,”\nProceedings of the IEEE , vol. 93, no. 2, pp. 216–231, 2005.\n[40] J. Ansel, S. Kamil, K. Veeramachaneni et al. , “Opentuner: An extensible\nframework for program autotuning,” in PACT , 2014.\n[41] Q. Huang et al. , “Autophase: Compiler phase-ordering for hls with deep\nreinforcement learning,” in FCCM , 2019.\n[42] J. Ryu, E. Park, and H. Sung, “One-shot tuner for deep learning\ncompilers,” in ACM SIGPLAN CC , 2022.",
  "textLength": 54285
}