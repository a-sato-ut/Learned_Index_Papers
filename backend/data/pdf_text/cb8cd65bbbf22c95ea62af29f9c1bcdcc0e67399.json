{
  "paperId": "cb8cd65bbbf22c95ea62af29f9c1bcdcc0e67399",
  "title": "Privacy-Preserving Learning-Augmented Data Structures",
  "pdfPath": "cb8cd65bbbf22c95ea62af29f9c1bcdcc0e67399.pdf",
  "text": "Privacy-Preserving Learning-Augmented Data Structures\nPrabhav Goyal\nprabhavg@uci.edu\nUniversity of California, Irvine\nIrvine, USAVinesh Sridhar\nvineshs1@uci.edu\nUniversity of California, Irvine\nIrvine, USAWilson Zheng\nwilsonz5@uci.edu\nUniversity of California, Irvine\nIrvine, USA\nAbstract\nLearning-augmented data structuresuse predicted frequency esti-\nmates to retrieve frequently occurring database elements faster\nthan standard data structures. Recent work has developed data\nstructures that optimally exploit these frequency estimates while\nmaintaining robustness to adversarial prediction errors. However,\nthe privacy and security implications of this setting remain largely\nunexplored.\nIn the event of a security breach, data structures should reveal\nminimal information beyond their current contents. This is even\nmore crucial for learning-augmented data structures, whose layout\nadapts to the data. A data structure ishistory independentif its mem-\nory representation reveals no information about past operations\nexcept what is inferred from its current contents. In this work, we\ntake the first step towards privacy and security guarantees in this\nsetting by proposing the first learning-augmented data structure\nthat is strongly history independent, robust, and supports dynamic\nupdates.\nTo achieve this, we introduce two techniques:thresholding, which\nautomatically makes any learning-augmented data structure ro-\nbust, andpairing, a simple technique that provides strong history\nindependence in the dynamic setting. Our experimental results\ndemonstrate a tradeoff between security and efficiency but are still\ncompetitive with the state of the art.\nCCS Concepts\nâ€¢Information systems â†’Information retrieval query pro-\ncessing;â€¢Security and privacy â†’Database and storage secu-\nrity.\nKeywords\nlearning-augmented data structures, frequency estimation, privacy,\nhistory independence\nACM Reference Format:\nPrabhav Goyal, Vinesh Sridhar, and Wilson Zheng. 2025. Privacy-Preserving\nLearning-Augmented Data Structures. In.ACM, New York, NY, USA, 6 pages.\n1 Introduction\nDictionaries are a fundamental class of data structures that sup-\nport updates and retrievals of key-value pairs. Many classic data\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nConferenceâ€™17, Washington, DC, USA\nÂ©2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.structures, such as binary search trees and hash tables, efficiently\nimplement dictionaries. In general, data structures that support\ninexact searches, such as predecessor and range queries, achieve\nretrieval times of at best ğ‘‚(logğ‘›) . However, these structures often\nignore information about the keysâ€™ access frequencies.\nRecently, Linet al. [ 27] introduced the firstlearning-augmented\nsearch tree. Here, each key ğ‘˜ğ‘–is assigned an access frequency esti-\nmateğ‘“ğ‘–âˆˆ(0,1]by a machine learning model. The authors design a\nsearch tree that can retrieve any key ğ‘˜ğ‘–inğ‘‚(log 1/ğ‘“ğ‘–)time while\nsupporting inexact queries [ 27]. This property is known asconsis-\ntencyw.r.t the assigned frequency distribution. Zeynaliet al. [ 43]\nsubsequently proposed a learning-augmented dictionary that also\nachievesrobustnessagainst adversarial models; that is, it guarantees\nthat no key ever exceeds a depth of ğ‘‚(logğ‘›) even if an adversar-\nial model can arbitrarily edit frequency estimates. Other recent\nworks that study consistent and robust learning-augmented data\nstructures include [ 11,16]. See also, e.g., [ 5,6,10,12,15,24,26,28â€“\n30,33,34,36â€“38], which apply machine learning predictions to\nimprove classical algorithms in general.\nIn contrast to numerous works on performance guarantees, few\nworks have studied privacy and security needs in this setting. Since\nthese structures adapt to their input distributions, they leak more in-\nformation about the underlying data than their classic counterparts,\nmaking them more vulnerable to data breaches. Even the splay\ntree [ 41], another structure which achieves robustness and consis-\ntency among other nice properties, fails to prevent such leakage as\nits layout prioritizes recent insertions and queries.\nThis issue is amplified in todayâ€™s computing landscape, where\nlarge-scale data breaches are increasingly common. In 2024 alone,\nmore than 3,000 confirmed breaches in the United States affected\nan estimated 1.35 billion people, more than 91% of which were\nattributed to cyberattacks [ 25]. These risks are projected to grow\nas artificial intelligence continues to integrate deeply into critical\ninfrastructure.\nOne way to mitigate this growing problem is by implementing\nhistory independence[ 23,32]. A data structure is history indepen-\ndent if its internal memory representation depends solely on its\ncurrent key-value contents and not on the sequence of past opera-\ntions. This restricts the leakage of historical information, providing\nprotection against data breaches.\nIn this work, we take the first step towards privacy protection in\nthe learning-augmented setting. We present the first robust, consis-\ntent, dynamic, and history independent learning-augmented data\nstructure. We give results for both weak and strong history inde-\npendence which presents a tradeoff between security and efficiency.\nTo achieve these guarantees, we introduce two general techniques.\nFirst,thresholding, which makes any consistent learning-augmentedarXiv:2510.00165v1  [cs.IR]  30 Sep 2025\n\nConferenceâ€™17, July 2017, Washington, DC, USA Prabhav Goyal, Vinesh Sridhar, and Wilson Zheng\ndata structure robust at little cost. Second,pairing, a new construc-\ntion that maintains two coordinated copies of the dataset to ensure\nstrong history independence in the dynamic setting.\nOur experimental results show that thresholding produces learned\ndata structures that match the performance of the state-of-the-art\nwhile offering improvements in space usage, and that pairing gives\nstrongly history independent data structures at the cost of a2 Ã—\nincrease in search times. This establishes thresholding and pair-\ning as simple mechanisms to construct learning-augmented data\nstructures that are both efficient and secure.\n2History Independent Learned Data Structures\nFor a review of history independence, see Appendix A. We begin\nby showing that the only robust learning-augmented data structure\nin the literature that supports dynamic updates, RobustSL [43],\nisnothistory independent. Specifically, we exploit its amortized\nupdate scheme by constructing a counterexample where two dis-\ntinct operation sequences produce the same external state but yield\ndistinguishable internal representations.\nWe first review RobustSL â€™s update scheme. Let ğ‘›denote the\ncurrent number of elements and ğ‘denote a cutoff value used in the\nupdate scheme.1ğ‘is initialized to 4 and ğ‘›begins at 0. Elements with\nfrequencies less than1 /ğ‘are handled by RobustSL in a special way.\nTo achieve robustness, ğ‘‚(logğ‘) must equalğ‘‚(logğ‘›) at all times,\nso the structure is periodically destroyed and rebuilt to updateğ‘.\nIf, after an insertion, ğ‘›equalsğ‘, then we set ğ‘â†ğ‘2and\nreconstruct the structure using this new cutoff value in ğ‘‚(ğ‘›logğ‘›)\ntime. If, after a deletion, ğ‘›equalsğ‘1/4, then we set ğ‘â†âˆš\nğ‘and do\nthe same. An amortized analysis shows that this produces ğ‘‚(logğ‘›) -\ntime insertion and deletion with high probability inğ‘›[43].2\nNow consider any arbitrary state, ğ´, of the algorithm and two\ndistinct sets of operations, ğ‘‹andğ‘Œ, which nevertheless take ğ´to\nthe same final state, ğµ. Letğ‘â†(ğ‘âˆ’ğ‘›) . Inğ‘‹, we insertğ‘identical\nelements into the structure and then remove 1 of those elements.\nInğ‘Œ, we insertğ‘âˆ’1identical elements into the structure. Both sets\nof operations take us from ğ´toğµ, howeverğ‘‹makesğ‘change,\nwhereasğ‘Œdoes not. Since ğ‘influences the location of elements\nin the structure, one can distinguish the memory representations\ncreated byğ‘‹andğ‘Œdespite the fact that both add the same content\ntoğ´. Thus, RobustSL , under the given updating scheme, is not\nhistory independent.\n2.1 A Weakly History Independent Update\nScheme\nNow we present the first history independent, robust, and dynamic\nlearning-augmented data structure. We do so by modifying the\nupdate scheme of RobustSL such that it is now history independent\n(in the static setting, skip-lists such as RobustSL , have been shown\nto be history independent [ 18]. Thus, it is sufficient to provide a\nhistory independent update scheme to obtain history independence\nin the dynamic setting).\n1Zeynaliet al.[43] useğ‘› ğ‘¡andğ‘›in place ofğ‘›andğ‘.\n2An event succeeds with high probability (w.h.p.) in ğ‘›if it succeeds with probability\ngreater than1âˆ’1/ğ‘›.Our scheme is based off of a scheme by Hartlineet al.[ 23]\nthat constructs a weakly history independent dynamic hash ta-\nble. We appear to be the first that extends it to a data structure\nwith logarithmic-time updates as opposed to constant-time updates.\nTheir scheme has two basic ideas. First, rather than having an upper\nand lower cutoff for changing ğ‘, each change in ğ‘is associated\nwith a single value of ğ‘›(e.g., every time ğ‘›doubles, double ğ‘and\nrebuild). This way, every value of ğ‘›is associated with a canonical\nğ‘, avoiding the issue we raise in the previous section.\nHowever, this cutoff scheme can be abused by an adversary to\ntrigger repeated rebuilds by inserting and deleting elements around\na given cutoff value. Thus, their second contribution introduces\nrandomness in the location of these cutoffs. Specifically,ğ‘is now\nconceived as a random variable that has a ğ‘‚(1/ğ‘›)chance of updat-\ning after each operation. As in, e.g., [ 10,20,21,23,31], we assume\nthat this source of randomness is kept private from an adversary,\nwhich prevents the above issue.\nTheir scheme works like so. Before each insertion, if ğ‘=ğ‘› , then\nrandomly choose a value in {ğ‘›+ 1,..., 2(ğ‘›+ 1)âˆ’1}and rebuild\nthe structure with ğ‘set to that value. Otherwise ğ‘>ğ‘› , and they\ndo the following. With probability1 /(ğ‘›+ 1), rebuild the structure\nwithğ‘â† 2ğ‘›and with probability1 /(ğ‘›+ 1)do the same with\nğ‘â† 2ğ‘›+1. Otherwise, do not rebuild. After this the new item is\ninserted. Deletions work similarly. We initially delete the element.\nThen, ifğ‘›â‰¤ğ‘/ 2, resizeğ‘to a value uniformly chosen between\n{ğ‘›,... 2ğ‘›âˆ’1}. Ifğ‘›>ğ‘/ 2, then setğ‘â†ğ‘› with probability1/ğ‘›.\nOtherwise, do nothing.\nHartlineet al.show that each update has an ğ‘‚(1/ğ‘›)chance of\nrebuilding [ 23]. The cost of rebuilding a robust learning-augmented\ndata structure is ğ‘‚(ğ‘›logğ‘›) , since the structure has depth ğ‘‚(logğ‘›) .\nThus, in our setting, the expected update cost is ğ‘‚((ğ‘›logğ‘›)/ğ‘›)=\nğ‘‚(logğ‘›). We conclude the following.\nTheorem 2.1.There exists a weakly history independent, consis-\ntent, and robust learning-augmented data structure that supports\nğ‘‚(logğ‘›)-time dynamic updates in expectation.\nProof. Follows from the above updating scheme applied to\nRobustSL [43]. At all times in the above scheme, ğ‘differs from ğ‘›\nby at most a constant factor, so ğ‘‚(logğ‘)âŠ†ğ‘‚(logğ‘›) . Therefore,\nRobustSLstill achieves robustness under this scheme.â–¡\nNext, we develop several more robust, weakly history indepen-\ndent, and dynamic data structures using our new thresholding\nscheme.\n3 Thresholding\nWe now present our second contribution,thresholding. This tech-\nnique is a simple modification of the learning-augmented frame-\nwork that allows any consistent learning augmented data structure\nDto become robust with a negligible effect on search times. We\ncontrast our general method with prior approaches that focus on\nmaking individual data structures robust (cf. [16, 43]).\nThe two prior works on robust learning-augmented data struc-\ntures both group together all keys with a frequency below a thresh-\nold (e.g.,1/ğ‘›) into a non-learned structure that has a standard\nğ‘‚(logğ‘›) retrieval time [ 16,43]. Our new insight is to instead mod-\nify the frequencies themselves according to thresholds, allowing\n\nPrivacy-Preserving Learning-Augmented Data Structures Conferenceâ€™17, July 2017, Washington, DC, USA\nthe data structure to act as a black box. It has been conjectured that\nmethods similar to ours may work in practice (see OpenReviews\nof [11,16]). We are the first to show that this is true theoretically.\nFor now, we assume a static setting, where the total number of keys,\nğ‘›, is known in advance. We later lift this assumption.\nDefinition 3.1 (Threshold Frequency Scheme).In the learning-\naugmented setting, each key ğ‘˜ğ‘–has a corresponding frequency ğ‘“ğ‘–.\nA threshold frequency scheme defines a new frequency\nğ‘“â€²\nğ‘–â†max{ğ‘“ ğ‘–/2,1/(2ğ‘›)}.\nEach key is then inserted with frequencyğ‘“â€²\nğ‘–rather thanğ‘“ ğ‘–.\nTheorem 3.2.Any consistent learning-augmented data structure\nDcan be made robust via our threshold frequency scheme.\nProof. By definition, for any assignment of frequencies that\nsum to no more than 1, Dguarantees that each ğ‘˜ğ‘–is retrieved in\ntime at most ğ‘‚(log 1/ğ‘“ğ‘–). We first confirm that our new thresholding\nscheme has frequencies that sum to no more than 1. Indeed, since\nat mostğ‘›keys can increase in frequency by at most1 /(2ğ‘›), we\nhave thatÃğ‘›\nğ‘–=1ğ‘“â€²\nğ‘–â‰¤Ãğ‘›\nğ‘–=1ğ‘“ğ‘–/2+ğ‘›Ã—1/(2ğ‘›)â‰¤1/2+1/2=1.\nUnder this scheme, each ğ‘˜ğ‘–is retrieved in time at most ğ‘‚(log 2/ğ‘“ğ‘–)âŠ†\nğ‘‚(log 1/ğ‘“ğ‘–), so consistency w.r.t. the original frequency assignment\nis preserved. All ğ‘“â€²\nğ‘–â‰¥1/(2ğ‘›), so any keyâ€™s depth is at worst ğ‘‚(logğ‘›) .\nThus,Dis now robust.â–¡\nTheorem 3.2 and prior work implies new consistent and robust\nimplementations of treaps [ 11,27] and B-treaps [ 11]. We emphasize\nthat any new work in this field now only needs to satisfy consistency\nas our thresholding scheme can immediately be applied to achieve\nrobustness.\nNext, we extend our result to another field of work calledbiased\ndata structuresand show that any biased data structure implies a\nconsistent, robust learning-augmented data structure via threshold-\ning. In this field, each key ğ‘˜ğ‘–is associated with a weight ğ‘¤ğ‘–>0. We\ncall a data structuredbiasedif it is a dynamic data structure with the\nfollowing guarantee. After inserting it into the structure, retriev-\ning keyğ‘˜ğ‘–takesğ‘‚(logğ‘Š/ğ‘¤ ğ‘–)time, where ğ‘Š=Ãğ‘›\nğ‘–=1ğ‘¤ğ‘–. Several\ndata structures, including the binary search tree [ 3,7], skip-list [ 4],\ntreap [ 39], zip tree [ 17,42], zip-zip tree [ 17], skip-list tree [ 4,13],\nB-tree [ 14], and energy-balanced tree [ 19] can be implemented as\nbiased data structures.3See also, e.g., [2, 8, 9, 22].\nLemma 3.3.Any biased data structure Bis consistent in the learning-\naugmented setting.\nProof. In the learning-augmented setting, we set ğ‘¤ğ‘–â†ğ‘“ğ‘–. Thus,\nğ‘Š=Ãğ‘›\nğ‘–=1ğ‘“ğ‘–â‰¤1. Therefore, each key ğ‘˜ğ‘–can be found in time\nğ‘‚(log 1/ğ‘“ ğ‘–)by properties ofB.â–¡\nTheorem 3.4.Any biased data structure can be made into a con-\nsistent, robust learning-augmented data structure via thresholding\nProof.Follows from Lemma 3.3 and Theorem 3.2.â–¡\n3Some of the cited structures hold the result in expectation.This result implies several new data structures that match state-\nof-the-art guarantees in the learning-augmented setting. In partic-\nular, our results also imply the first consistent and robust external-\nmemory learning-augmented data structure by applying threshold-\ning to [ 11,14]. Our scheme also satisfies an open question of [ 43].\nThey asked whether one can maintain a consistent and robust data\nstructure in which some keys have frequency estimates and others\ndo not. We can easily handle this by assuming any key without an\nestimate has frequency 0. These keys will be thresholded and have\nğ‘‚(logğ‘›) search time without affecting the search time of other\nkeys.\nLastly, any of the above structures can made dynamic, i.e. allow\nfor changes in ğ‘›, either by using the amortized update scheme pro-\nposed by Zeynaliet al.for RobustSL [43] or, if the underlying data\nstructure is history independent, using the history independent\nupdating scheme we propose in Section 2.1. In this case, we would\nsetğ‘“â€²\nğ‘–â†max{ğ‘“ ğ‘–/2,1/(2ğ‘)} and achieve similar performance and\nrobustness guarantees as [ 43]. Thus, we can also implement Theo-\nrem 2.1 using the following history independent data structures: the\nzip-tree [ 42], zip-zip tree [ 17], skip-list tree [ 13,17], treap [ 11,27],\nand B-treap [11, 18].\n4 Pairing: Obtaining Strong History\nIndependence\nOur results in Sections 2.1 and 3 implied the first weakly history\nindependent, dynamic, consistent, and robust learning-augmented\ndata structures. Here, we develop an orthogonal technique we call\npairingthat also transforms any consistent data structure into a ro-\nbust one while additionally providingstronghistory independence.\nDefine apaired data structure Dğ‘ƒas two copies of the data stored\nin two separate data structures Dğ¶andD.4Dğ¶is any strongly\nhistory independent consistent learning-augmented data structure\n(e.g., the biased zip-zip tree [ 17] via Lemma 3.3) and Dis a strongly\nhistory independent non-learned data structure with support for\ninexact queries and ğ‘‚(logğ‘›) -retrieval time (e.g., a standard skip-list\nor zip-zip tree [ 17]), whereğ‘›is the number of keys in the structure\ncurrently.\nWe insert and delete elements from both structures in tandem.\nTo search, we do the following. We begin a tentative search in Dğ¶\nforğ›¾logğ‘› steps for some constant ğ›¾> 0. If we find the element,\nwe are done. If we have made ğ›¾logğ‘› steps and have not found\nthe element, we terminate the search in Dğ¶and searchDfor the\nelement. For some query key ğ‘˜ğ‘–with frequency ğ‘“ğ‘–that exists in\nDğ‘ƒ, it immediately follows that this takes ğ‘‚(min{log 1/ğ‘“ğ‘–,logğ‘›})\ntime, satisfying both consistency and robustness. Using standard\ntechniques, we can implement predecessor and range queries in\nğ‘‚(logğ‘›)-time inD, matching the results of [43].\nUnlike the robust learning-augmented data structures of [ 16,43]\nand our thresholding scheme from above, the underlying structure\nofDğ‘ƒdoes not depend on the number of keys inserted, ğ‘›(apart\nfrom its size). Thus, we do not need an amortized updating scheme\nin which the structure periodically rebuilds itself to maintain its\ncorrectness. As a result, the strong history independence of Dğ¶and\n4To save space,Dğ¶andDmay store pointers to shared data rather than maintaining\nactual copies.\n\nConferenceâ€™17, July 2017, Washington, DC, USA Prabhav Goyal, Vinesh Sridhar, and Wilson Zheng\nDis preserved in the dynamic setting and we immediately have\nthe following.\nTheorem 4.1.There exists a strongly history independent, dy-\nnamic, consistent, and robust learning-augmented data structure.\nProof. Follows from instantiating a paired data structure using\na strongly history independent consistent data structure Dğ¶, e.g. [ 4,\n11,17,42], and a strongly history independent non-learned data\nstructureDwhich supports inexact queries, e.g. [ 11,17,18,42].â–¡\nThus, we have achieved a stronger privacy guarantee at the\ncost of doubling space usage and increasing search times by a\nconstant factor for less-frequent elements. In addition, unlike our\nthresholding scheme, pairing is unable to support keys that have\nnot been assigned a frequency. In the following section, we collect\nexperimental data to determine the impact of these tradeoffs in\npractice.\n5 Experiments\nIn this section, we discuss experimental results in the static setting,\ni.e., with fixed ğ‘›. We compare the biased zip-zip tree [ 17] with\nour thresholding scheme against other learning-augmented data\nstructures, including RobustSL [43]. In general, we find that our\nconsistent, robust zip-zip tree has comparable or better performance\ntoRobustSL while also being much easier to implement. Indeed,\nRobustSL requires a ground-up rewrite of skip-lists and introduces\nseveral tunable parameters that must be optimized. In contrast,\nconverting a zip-zip tree implementation to a biased zip-zip tree\nwith thresholding requires changing fewer than five lines of code.\nSimilarly, implementing a paired zip-zip tree is straightforward: one\ninitializes two zip-zip trees, one standard and one biased, performs\nall updates in tandem, and applies the fall-back search procedure\ndescribed above.\nWe test the RobustSL [43], biased zip-zip tree [ 17], biased zip-zip\ntree with thresholding (threshold zip-zip tree), paired data structure\nwithğ›¾=1implemented via zip-zip trees (paired zip-zip tree), the\nlearning-augmented treaps of Linet al.[ 27] (L-Treap) and Chen\net al.[ 11] (C-Treap), as well as the (non-learned) AVL tree [ 1].\nFollowing [ 43], we consider the Zipfian distribution [ 35], commonly\nused to model text frequencies, in which each key of rank ğ‘–(1 to\nğ‘›) is assigned frequency1 /ğ‘–ğ›¼, for some parameter ğ›¼â‰¥1. The keys\nare inserted from1to ğ‘›into the respective structure with these\nfrequencies, 100000 queries are made, where each query of key ğ‘–\noccurs with frequency1 /ğ‘–ğ›¼, and we count the average number of\ncomparisons made over all queries.\nTo test robustness, [ 43] use the following â€œnoisy frequencyâ€\nscheme. They define a parameter ğ›¿âˆˆ[ 0,1]and adversarial rank\nË†ğ‘–â†ğ‘–Ã—( 1âˆ’ğ›¿)+ğ›¿Ã—(ğ‘›âˆ’ğ‘–+ 1). We insert the keys under these\nadversarial ranks (exactly reversed when ğ›¿=1) but perform queries\nusing the original ranks. Thus, the learning-augmented structures\nare tested with adversarially bad frequency estimates. To further\nemphasize the influence of adversarially-chosen frequencies, we\nalso define the inverse power distribution, in which each key of\nrankğ‘–is assigned frequency1/ğ›¼ğ‘–for someğ›¼â‰¥1.\nIn power law relations, such as the Zipfian distribution, the\nsmallest frequencies are still inverse polynomial in ğ‘›. Thus, any\nnon-robust learning-augmented data structure should still expect tohaveğ‘‚(logğ‘›) retrieval times. In contrast, the smallest frequencies\nin the inverse power distribution are inverse exponential in ğ‘›, which\nmay degrade to linear search times in non-robust structures under\nadversarially-chosen frequencies. As a result, this distribution better\ndemonstrates the power of robustness in the learning-augmented\nsetting.\nOur tests are as follows. The Zipf parameter test examines dif-\nferent values of ğ›¼under perfect frequency estimates ( ğ›¿=0) with a\nfixedğ‘›=2000; the Noisy Zipfian test uses ğ›¼=2andğ›¿=0.9; and\nthe Inverse Power test uses ğ›¼= 1.01(chosen for computational\ntractability) and ğ›¿=0.9. Lastly, our size test measures RobustSL â€™s\nnode count when its keys follow the Zipfian distribution with ğ›¼=2.\nThe other structures considered have exactly ğ‘›nodes, except the\npaired zip-zip tree, which has2ğ‘›nodes.\n5.1 Experimental Results\nWe find that the threshold zip-zip tree achieves query performance\nwithin 2 comparisons of RobustSL on average while only using\ntwo-thirds of the space, demonstrating an advantageous balance\nbetween efficiency and memory cost. We also find that the paired\nzip-zip tree achieves query performance within at most2 Ã—the\ncomparisons of RobustSL and the threshold zip-zip tree on average,\nwhile usingâ‰ˆ25%more space thanRobustSL.\nZipfian Workloads.In Figure 1a, we perform the Zipfian Pa-\nrameter Test, which varies ğ›¼under perfect predictions. As the\nskew increases, we can see that all learned structures are indeed\nconsistent and take advantage of the frequency distribution. The\nnon-robust structures (biased zip-zip, L-Treap, and C-Treap) bene-\nfit the most from a higher skew. Nevertheless, RobustSL and the\nthreshold zip-zip tree trail by just 2-3 comparisons on average and\nremain closely matched with each other. The paired zip-zip tree also\nperforms as well as the others as ğ›¼increases, though has poorer\nconstant factors forğ›¼=1.\nNext, we introduce noise into the predictions with ğ›¼= 2and\nğ›¿=0.9(see Figure 1b). We observe that the non-robust structures\nC-Treap and L-Treap degrade sharply, with average comparisons\ngrowing linearly with ğ‘›.5In contrast, both RobustSL and threshold\nzip-zip tree stay below7comparisons on average, with the threshold\nzip-zip tree trailing by at most 2-3 comparisons. They also still\ntake advantage of the frequency distribution despite the noise,\nconsistently performing better than the AVL tree. The paired zip-\nzip tree remains within 2 Ã—the cost of the threshold zip-zip tree\nthroughout. Interestingly, the biased zip-zip tree is less affected\nthan both treaps, though it still performs worse than the threshold\nzip-zip tree andRobustSL.\nInverse Power Workloads.We designed the inverse power test\nto show that, under schemes in which frequencies grow exponen-\ntially small in ğ‘›, adversarial predictions force linear performance\nin any consistent, non-robust learned data structure (see Figure 1c).\nThe data supports this hypothesis, as all three non-robust struc-\ntures (biased zip-zip, L-Treap, and C-Treap) appear to scale linearly\ninğ‘›. In contrast, threshold zip-zip tree and RobustSL match the\nroughly logğ‘› growth of the AVL tree. Furthermore, despite per-\nforming worse than the biased zip-zip tree for small ğ‘›, the paired\n5The L-Treapâ€™s generally poor performance is also due to the fact that it requires the\ninput to be inserted in uniformly permuted order to remain balanced [11, 27, 43].\n\nPrivacy-Preserving Learning-Augmented Data Structures Conferenceâ€™17, July 2017, Washington, DC, USA\nFigure 1: (a) Zipf Parameter test ( ğ‘›=2000,ğ›¿=0), (b) Noisy Zipfian test ( ğ›¼=2,ğ›¿=0.9), and (c) Inverse Power test ( ğ›¼=1.01,ğ›¿=0.9).\nValues overflowing 25 comparisons indicated with a number next to the bar.\nFigure 2: Size test (Zipfian,ğ›¼=2)\nzip-zip tree has a clear logarithmic growth of its query cost com-\npared to the linear growth of the biased zip-zip tree. Thus, we have\nexperimental evidence that our methods provide robustness even\nin an extreme adversarial setting.\nSpace Overhead.Another consideration in choosing an appro-\npriate database structure is space usage. All tree-based structures\nconsidered maintain exactly ğ‘›nodes, while RobustSL incurs signif-\nicant overhead due to its skip-list layering structure. Accordingly,\nwe find that RobustSL uses roughly1 .5Ã—space, which may be a\nlimiting factor if applied to large-scale datasets. The paired zip-zip\ntree maintains two representations of the dataset, yielding a space\ncost of2ğ‘›(see Figure 2).\nOur results suggest that the threshold zip-zip tree may offer a sim-\npler and practical alternative to RobustSL for learning-augmented\nworkloads, achieving comparable robustness and performance un-\nder synthetic workloads while significantly reducing space usage.\nIn addition, our results for the paired zip-zip tree show a clear trade-\noff between security and efficiency. The paired zip-zip tree offers\nstrong history independence in the dynamic setting, yet at the cost\nof a 2Ã—increase in space and time usage compared to our threshold\nzip-zip tree.6 Conclusion\nIn this paper, we initiated a study of privacy and security in learning-\naugmented data structures by proposing the first dynamic, consis-\ntent, robust, and history independent learned data structures using\nour new techniques, thresholding and pairing. Our techniques also\nmore than triple the number of practical learning-augmented data\nstructures that are consistent and robust.\nFuture work could experimentally validate our new data struc-\ntures in the dynamic setting or examine whether better choices for\nğ›¾exist in the paired zip-zip tree (e.g., ğ›¾={ 3.82,1.3863}for height\nand expected depth of a standard zip-zip tree respectively [ 17]).\nOne could also consider how to narrow the efficiency gap between\nour strongly and weakly history independent robust learned data\nstructures.\nWe also observe that, in general, any biased data structure in\nwhich a bound ğ‘Šâˆ—onğ‘Šis known in advance can be implemented\nwith thresholding such that each key with weight ğ‘¤ğ‘–has depth\nğ‘‚(min{logğ‘Šâˆ—/ğ‘¤ğ‘–,logğ‘›}) . We wonder if this could be useful else-\nwhere, e.g., in variants of Sleator and Tarjanâ€™s link-cut tree [ 40] or\nto generalize biased data structures which require polynomially-\nbounded weights such as Goodrich and Strashâ€™s priority range\ntree [22].\nReferences\n[1]G. Adelson-Velskii and E. Landis. An algorithm for the organization of informa-\ntion, 1962.\n[2]Sunil Arya, Theocharis Malamatos, and David M Mount. A simple entropy-based\nalgorithm for planar point location.ACM Transactions on Algorithms (TALG),\n3(2):17â€“es, 2007.\n[3]Mikhail J Atallah, Michael T Goodrich, and Kumar Ramaiyer. Biased finger trees\nand three-dimensional layers of maxima: (preliminary version). InProceedings of\nthe tenth annual symposium on Computational geometry, pages 150â€“159, 1994.\n[4]Amitabha Bagchi, Adam L Buchsbaum, and Michael T Goodrich. Biased skip\nlists.Algorithmica, 42(1):31â€“48, 2005.\n[5]Ziyad Benomar and Christian Coester. Learning-augmented priority queues.\nAdvances in Neural Information Processing Systems, 37:124163â€“124197, 2024.\n[6]Ziyad Benomar and Vianney Perchet. On tradeoffs in learning-augmented algo-\nrithms.arXiv preprint arXiv:2501.12770, 2025.\n[7]Samuel W Bent, Daniel D Sleator, and Robert E Tarjan. Biased search trees.SIAM\nJournal on Computing, 14(3):545â€“568, 1985.\n[8]Prosenjit Bose, Rolf Fagerberg, John Howat, and Pat Morin. Biased predecessor\nsearchğ›¿.LATIN 2014: Theoretical Informatics LNCS 8392, page 755, 2014.\n[9]Prosenjit Bose, John Howat, and Pat Morin. A history of distribution-sensitive\ndata structures. InSpace-Efficient Data Structures, Streams, and Algorithms: Papers\nin Honor of J. Ian Munro on the Occasion of His 66th Birthday, pages 133â€“149.\n\nConferenceâ€™17, July 2017, Washington, DC, USA Prabhav Goyal, Vinesh Sridhar, and Wilson Zheng\nSpringer, 2013.\n[10] Bo Chen and Radu Sion. Hiflash: A history independent flash device.arXiv\npreprint arXiv:1511.05180, 2015.\n[11] Jingbang Chen, Xinyuan Cao, Alicia Stepin, and Li Chen. On the power of\nlearning-augmented search trees. InForty-second International Conference on\nMachine Learning, 2025.\n[12] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\net al. Alex: an updatable adaptive learned index. InProceedings of the 2020 ACM\nSIGMOD international conference on management of data, pages 969â€“984, 2020.\n[13] Jeff Erickson. Lecture notes on treaps., 2017.\n[14] Joan Feigenbaum and Robert E Tarjan. Two new kinds of biased search trees.\nBell System Technical Journal, 62(10):3139â€“3158, 1983.\n[15] Paolo Ferragina and Giorgio Vinciguerra. Learned data structures. InRecent\nTrends in Learning From Data: Tutorials from the INNS Big Data and Deep Learning\nConference (INNSBDDL2019), pages 5â€“41. Springer, 2020.\n[16] Chunkai Fu, Brandon G Nguyen, Jung Hoon Seo, Ryan Zesch, and Samson Zhou.\nLearning-augmented search data structures.arXiv preprint arXiv:2402.10457,\n2024.\n[17] Ofek Gila, Michael T Goodrich, and Robert E Tarjan. Zip-zip trees: Making zip\ntrees more balanced, biased, compact, or persistent. InAlgorithms and Data\nStructures Symposium, pages 474â€“492. Springer, 2023.\n[18] Daniel Golovin.Uniquely represented data structures with applications to privacy.\nCarnegie Mellon University, 2008.\n[19] Michael T Goodrich. Competitive tree-structured dictionaries. InProceedings of\nthe eleventh annual ACM-SIAM symposium on Discrete algorithms, pages 494â€“495,\n2000.\n[20] Michael T Goodrich, Evgenios M Kornaropoulos, Michael Mitzenmacher, and\nRoberto Tamassia. More practical and secure history-independent hash tables.\nInEuropean symposium on Research in Computer Security, pages 20â€“38. Springer,\n2016.\n[21] Michael T. Goodrich, Evgenios M. Kornaropoulos, Michael Mitzenmacher, and\nRoberto Tamassia. Auditable data structures. In2017 IEEE European Symposium\non Security and Privacy (EuroS&P), pages 285â€“300, 2017.\n[22] Michael T Goodrich and Darren Strash. Priority range trees. InInternational\nSymposium on Algorithms and Computation, pages 97â€“108. Springer, 2010.\n[23] Jason D Hartline, Edwin S Hong, Alexander E Mohr, William R Pentney, and\nEmily C Rocke. Characterizing history independent data structures.Algorithmica,\n42(1):57â€“74, 2005.\n[24] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based fre-\nquency estimation algorithms. InInternational Conference on Learning Represen-\ntations, 2019.\n[25] ITRC. Itrc 2024 annual data breach report, Jan 2025.\n[26] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case\nfor learned index structures. InProceedings of the 2018 international conference\non management of data, pages 489â€“504, 2018.\n[27] Honghao Lin, Tian Luo, and David Woodruff. Learning augmented binary search\ntrees. InInternational Conference on Machine Learning, pages 13431â€“13440. PMLR,\n2022.\n[28] Michael Mitzenmacher. A model for learned bloom filters and optimizing by\nsandwiching.Advances in neural information processing systems, 31, 2018.\n[29] Michael Mitzenmacher. Scheduling with predictions and the price of mispredic-\ntion.arXiv preprint arXiv:1902.00732, 2019.\n[30] Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions.\nCommunications of the ACM, 65(7):33â€“35, 2022.\n[31] David Molnar, Tadayoshi Kohno, Naveen Sastry, and David Wagner. Tamper-\nevident, history-independent, subliminal-free data structures on prom storage-\nor-how to store ballots on a voting machine. In2006 IEEE Symposium on Security\nand Privacy (S&Pâ€™06), pages 6â€“pp. IEEE, 2006.\n[32] Moni Naor and Vanessa Teague. Anti-persistence: History independent data\nstructures. InProceedings of the thirty-third annual ACM symposium on Theory\nof computing, pages 492â€“501, 2001.\n[33] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. Learning\nmulti-dimensional indexes. InProceedings of the 2020 ACM SIGMOD international\nconference on management of data, pages 985â€“1000, 2020.\n[34] Adam Polak and Maksym Zub. Learning-augmented maximum flow.Information\nProcessing Letters, 186:106487, 2024.\n[35] David M. W. Powers. Applications and explanations of Zipfâ€™s law. InNew Methods\nin Language Processing and Computational Natural Language Learning, 1998.\n[36] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms\nvia ml predictions.Advances in Neural Information Processing Systems, 31, 2018.\n[37] Jiayong Qin, Xianyu Zhu, Qiyu Liu, Guangyi Zhang, Zhigang Cai, Jianwei Liao,\nSha Hu, Jingshu Peng, Yingxia Shao, and Lei Chen. Piecewise linear approxi-\nmation in learned index structures: Theoretical and empirical analysis.arXiv\npreprint arXiv:2506.20139, 2025.\n[38] Atsuki Sato and Yusuke Matsui. Fast partitioned learned bloom filter.Advances\nin Neural Information Processing Systems, 36:39119â€“39146, 2023.[39] Raimund Seidel and Cecilia R Aragon. Randomized search trees.Algorithmica,\n16(4):464â€“497, 1996.\n[40] Daniel D Sleator and Robert Endre Tarjan. A data structure for dynamic trees.\nInProceedings of the thirteenth annual ACM symposium on Theory of computing,\npages 114â€“122, 1981.\n[41] Daniel Dominic Sleator and Robert Endre Tarjan. Self-adjusting binary search\ntrees.Journal of the ACM (JACM), 32(3):652â€“686, 1985.\n[42] Robert E Tarjan, Caleb Levy, and Stephen Timmel. Zip trees.ACM Transactions\non Algorithms (TALG), 17(4):1â€“12, 2021.\n[43] Ali Zeynali, Shahin Kamali, and Mohammad Hajiesmaili. Robust learning-\naugmented dictionaries. InInternational Conference on Machine Learning, pages\n58470â€“58483. PMLR, 2024.\nA History Independence\nA data structure is history independent [ 23,32] if its history of op-\nerations, such as insertions, deletions, or queries, cannot be inferred\nfrom its internal memory representation beyond what is implied by\nits current data contents, known as itsstate. There are two types of\nhistory independence,strongandweak. Intuitively, one can think of\nstrong history independence as preventing data leakage across mul-\ntiple data breaches, whereas weak history independence guarantees\nprotection for only a single breach.\nFormally, let us consider two states ğ´andğµ, and two sets of\noperationsğ‘‹andğ‘Œ. Letğ´be the initial state and ğµbe the final state\nof our data structure.\nDefinition A.1 (Strong History Independence).A data structure\nisstrongly history independentif, for any two sets of operations ğ‘‹\nandğ‘Œthat result in the same logical state ğµ, the resulting inter-\nnal memory representation induced by each set of operations is\nindistinguishable.\nDefinition A.2 (Weak History Independence).A data structure is\nweakly history independentif this indistinguishable property holds\nonly whenğ´is the initial (e.g., empty) state.",
  "textLength": 36423
}