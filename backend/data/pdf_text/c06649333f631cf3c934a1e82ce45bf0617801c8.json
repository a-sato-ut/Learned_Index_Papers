{
  "paperId": "c06649333f631cf3c934a1e82ce45bf0617801c8",
  "title": "KML: Using Machine Learning to Improve Storage Systems",
  "pdfPath": "c06649333f631cf3c934a1e82ce45bf0617801c8.pdf",
  "text": "KML: Using Machine Learning to Improve Storage Systems\nIbrahim Umit Akgun, Ali Selman Aydin, Andrew Burford, Michael McNeill, Michael Arkhangelskiy,\nAadil Shaikh, Lukas V elikov, and Erez Zadok\nStony Brook University\nAbstract\nOperating systems include many heuristic algorithms designed\nto improve overall storage performance and throughput. Because\nsuch heuristics cannot work well for all conditions and workloads,\nsystem designers resorted to exposing numerous tunable param-\neters to users—thus burdening users with continually optimizing\ntheir own storage systems and applications. Storage systems are\nusually responsible for most latency in I/O-heavy applications, so\neven a small latency improvement can be significant. Machine\nlearning (ML) techniques promise to learn patterns, generalize\nfrom them, and enable optimal solutions that adapt to changing\nworkloads. We propose that ML solutions become a first-class\ncomponent in OSs and replace manual heuristics to optimize\nstorage systems dynamically. In this paper, we describe our pro-\nposed ML architecture, called KML. We developed a prototype\nKML architecture and applied it to two case studies: optimizing\nreadahead and NFS read-size values. Our experiments show that\nKML consumes less than 4KB of dynamic kernel memory, has a\nCPU overhead smaller than 0.2%, and yet can learn patterns and\nimprove I/O throughput by as much as 2.3 \u0002and 15 \u0002for two\ncase studies—even for complex, never-seen-before, concurrently\nrunning mixed workloads on different storage devices.\n1 Introduction\nComputer hardware, software, storage, and workloads are\nconstantly changing. Storage performance heavily depends on\nworkloads and the precise system configuration [11,76]. Storage\nsystems and OSs include many parameters that can affect overall\nperformance [10,12,97]. Yet, users often do not have the time\nor expertise to tune these parameters. Worse, the storage and OS\ncommunities are fairly conservative and resist making significant\nchanges to systems to prevent instability or data loss. Thus,\nmany techniques currently used were historically developed\nwith human intuition after studying a few workloads; but such\ntechniques cannot easily adapt to ever-changing workloads and\nsystem diversities.\nFor example, readahead values, while tunable, are often fixed\nand left at their defaults. Correctly setting them is important\nand difficult when workloads change: too little readahead wastes\npotential throughput and too much pollutes caches—both hurt-\ning performance. Some OSs let users pass hints ( e.g.,fadvise ,\nmadvise ) to help recognize files that will be used sequentially or\nrandomly, but these often fail to find optimal values for complex,mixed, or changing workloads. We experimented with a variety\nof modern workloads and mainy different values of readahead: in\nprior work, we confirmed that no single readahead value is opti-\nmal for all workloads [4]. Another example of tunable parameters\nin the network storage settings is the default read-size ( rsize )\nparameter in NFS: if set too small or large, performance suffers.\nMachine Learning (ML) techniques can address this complex\nrelationship between workloads and tunable parameters by ob-\nserving actual behavior and adapting on-the-fly, and hence may\nbe more promising than fixed heuristics. ML techniques were\nrecently used to predict index structures in KV stores [22,48],\nfor database query optimization [47], improved caching [84],\ncache eviction policies [90], I/O scheduling [38], and more.\nIn this paper, we describe our ML approach to improve\nstorage performance by dynamically adapting to changing I/O\nworkloads. We designed and developed a versatile, low-overhead,\nlight-weight system called KML, for conducting ML training and\nprediction for storage systems. KML defines generic ML APIs\nthat can be used for a variety of subsystems; we currently support\nseveral deep neural networks and decision tree models. We\ndesigned KML to be embeddable inside an OS or the critical path\nof the storage system: KML imposes low CPU and memory over-\nheads. KML can run synchronously or asynchronously, giving\nusers the ability to trade-off prediction accuracy vs. overhead.\nDeveloping and tuning ML-based applications can be its own\nchallenge. Therefore, we designed KML to run identically in\nuser- or kernel-level. Users can develop and debug ML solutions\neasily in the user level, then upload the same model to run\nidentically in the kernel.\nWe demonstrate KML’s usefulness with two case studies:\n(i) adapting readahead values dynamically and (ii) setting NFS\nrsize values automatically. In both cases, we aim to adapt these\nvalues within one second under changing and even mixed work-\nloads. Overall, our approach to storage systems optimization\nusing ML is a continuous observe-and-tune paradigm.\nThis paper makes five contributions:\n1.We show that lightweight ML can indeed become a\nfirst-class citizen inside storage systems and OSs;\n2.We offer flexibility through synchronous or asynchronous\ntraining and the ability to offload training to the user-level;\n3.We introduce the idea of generic ML APIs that can be\nexpanded to support additional and future ML techniques;\n1arXiv:2111.11554v2  [cs.OS]  26 Jan 2022\n\nUser \nKernel \nML enhanced \nOS/Storage component App.ko \nk-MLib.ko Data collection / \ntraining \ninference/ \npredictions \nML enhanced \nOS/Storage component \nApp\nu-MLib.a \nData collection \ninference/ \npredictions Trace \nk-MLib.ko App.ko training \n[ ] w [ ] w \n[ ] w (a)\n(b)User \nKernel Figure 1: Two different operational modes that we built to achieve a\nhigh efficiency ML framework for tuning OS-level storage systems: (a)\nkernel space training and inference and (b) offline user space training\nand kernel space inference.\n4.We apply KML to two important optimization problems\n(readahead and NFS rsize values); and\n5.We evaluate our solutions using multiple, complex, and\neven mixed workloads, as well as two different storage\ndevices. We demonstrate throughput improvements up to\n2.3\u0002for readhead and up to 15 \u0002forrsize . We show\nthat ML models trained on a few workloads can generalize\nand optimize throughput for never-before-seen workloads\nor devices. And finally, we show that KML has small CPU\noverheads (<0:2%) and dynamic memory footprint (4KB),\nwell worth the overall I/O improvements.\nNext, Section 2 describes KML’s design. Section 3 describes\nour two use cases (readahead and NFS rsize ). Detailed\nevaluation of KML and two use cases are in Section 4. We\nsurvey related work in Section 5 and conclude in Section 6.\n2 KML’s Architecture\nModern ML libraries are often general-purpose, rely on many\nlarge third-party libraries ( e.g., in C++ or Python), and designed\nto process lots data using massive processing power ( e.g., GPU\nclusters). Porting such ML systems to an OS kernel would be im-\npractical, because an OS is a highly constrained and unforgiving\nenvironment. Thus, we chose to develop an ML framework from\nscratch—designed for low-overhead, light-weight, and highly\ntailored to OSs and storage systems and developers.\nKML high-level design choices. Figure 1 demonstrates two\ndifferent operating modes that we built. KML supports (a)\nin-kernel training and inference and (b) user space offline\ntraining and in-kernel inference. Once a model is built in\nuser space, it can be loaded into the kernel as is. KML has a\nhighly modular design: the core ML code base is shared by\nboth user and kernel space. Operation mode (a) is designed\nfor performance and accuracy, especially under high-I/O rates,\nbecause collecting and copying lots of I/O event data out of thekernel imposes high overheads. Operation mode (b) is designed\nto simplify ML model development for OS/storage developers.\nUsers can develop and test an ML model design more easily\nin user space, testing different features, ML architectures, and\nhyper-parameters to reach a stable and accurate model.\n2.1 Design Overview\nEasy to develop and extend. In Figure 1(b), KML is\ncompiled and linked with an application for both kernel and\nuser space. u-MLib.a andk-MLib.ko are built using the\nsame KML source code. We developed a wrapper layer for the\nKML development API: KML’s core code is uniform across\nboth user and kernel APIs. This identical abstraction speeds up\ndevelopment, eases debugging, and facilitates extensibility (see\nSection 2.3). Nevertheless, we recognize that while we aim to\nmake ML-based solutions easier to use, developers still require\na good understanding of OS and storage system internals.\nLow overhead. To make ML approaches practical for storage\nsystems, they must have low computational and memory\noverheads. ML solutions have three phases that consume much\nmemory/CPU resources: (i) inference ( i.e., prediction), (ii)\ntraining, and (iii) data processing & normalization. We support\nasynchronous training and inference capabilities to reduce in-\nterference on the data path; KML also uses efficient communica-\ntions between the data collection and model training & inference\ncomponents, to help scalability and stability of ML-based designs.\nTo reduce the data collection overheads, developers can facilitate\nsubsampling techniques that are provided in KML. We detail\nour design choices to reduce these overheads in Section 2.4.\n2.2 Fundamentals of Core ML library\nKML provides primitives for building and extending ML models.\nThis involves building algorithms for training ML models ( e.g.,\nback-propagation, decision-tree induction) and building the\nmathematical functions needed to implement them. The library\ndesign allows for seamless extensibility of library functionality.\nAdditionally, our ML functionality is easily debugged in user\nspace as it uses identical code and APIs in kernel space.\nMathematical and matrix operations. Most ML algorithms\nrely heavily on basic mathematical functions and matrix algebra.\nFor example, a neural network classifier uses functions such as\nmatrix multiplication/addition, softmax , and exponentiation.\nHence, we implemented kernel versions of such common ML\nfunctions using well known approximation algorithms.\nLayer and loss-function implementations. One can think of\na neural network as a composition of layers and one or more loss\nfunctions. Many of these building blocks are used across many\ndifferent neural network architectures. Layers like a fully con-\nnected layer, ReLU [63], or sigmoid are essential building blocks\nof many neural networks; loss functions are also fairly common\nacross many applications. Both layers and loss functions imple-\nment two main functionalities, one during the inference (forward)\nphase and another during the back-propagation (training) phase.\n2\n\nUser \nKernel \nMemory Management \n(filemap, page-writeback) \ndata collection \nreadahead.ko k-MLib.ko \nData \nnormalization KML interface Block device Applications \nsystem calls / \nmemory mapped I/O \nfeature data \nprocessing \ntrain / \ninference struct page *, \nstruct inode * #pages \n#sectors \nModel \ndata 1\n2\n354Figure 2: KML kernel space training and inferencing architecture.\nWe implemented these common components and their forward\nand back-propagation functionality from scratch in KML:\nlayer/loss functions, data structures related to the layer/loss, etc.\nInference and training. When stacked together, the elements\nof a conventional neural network can form a DAG. Thus, a neural\nnetwork inference means traversing the DAG starting from\nthe initial node(s) (where the inputs are provided), toward the\nresulting nodes (where the neural network output is produced).\nKML implements a standard training method used in neural\nnetworks—back-propagation [73]. KML also includes Stochas-\ntic Gradient Descent (SGD) which uses the gradients computed\nusing back-propagation to optimize the neural network weights.\n2.3 KML’s Modular Design\nWe now elaborate on KML’s operation modes: (i) in-kernel\ntraining and inference (see Figure 1(a)), and (ii) user space\ntraining and in-kernel inference (see Figure 1(b)).\nTraining in kernel space. We use the readahead use case\nto describe how KML works in kernel training and inference\nmode. Figure 2 shows KML’s framework ( k-MLib.ko ), a KML\napplication ( readahead.ko ), and target storage components\n(Block device and Memory Management subsystems). The\nyellow background denotes KML related components. The\nblue background depicts the target storage components, which\nare specific to the readahead case. The green line represents\nexecution and data flow. Numbered boxes refer to transitions\nhappening between the components.\nAs we mentioned in Section 1, we designed our use cases\nbased on a continuous observe-and-tune principle. In its first\nstage, the readahead module observes and collects data. Since\nour target component is the memory management ( e.g., page\ncache) system, the readahead module starts collecting data\nfrom this component (Figure 2 ¬). The readahead module then\nextracts features and transfers them to the KML framework\nto be normalized (Figure 2 ­). After the data processing and\nnormalization stage is done, if the readahead module is operating\nin training mode, it trains on the normalized data, and the\nexecution flow ends here. However, if the readahead module\nis operating in inference mode, it feeds the normalized data\nto the readahead neural network model and tunes the target\ncomponents based on the model’s prediction (Figure 2 ®).\nUser \nKernel I/O Workloads \nreadahead.ko training \ndata collection \n Train data KML interface \ntrain Data processing and \nnormalization kml.a \nsave model [ ] w [ ] w \n[ ] w data \nprocessing \n Trace data Memory Management  \n(filemap, page-writeback) readahead \ninstance Figure 3: KML user-space training & kernel-space inference\narchitecture.\nHow a KML application optimizes a target component\ndepends on the problem and its solution. Here, the readahead\nmodule updates readahead sizes on a per-file basis (Figure 2 ¯)\nor a per-device basis (Figure 2 °). When the readahead module\nis inferencing, execution flow forms a closed-circuit . After\nthe readahead module changes readahead sizes, OS memory\nstate changes; thereafter, new inputs go to the readahead neural\nnetwork model, leading to updated predictions. Therefore, ML\nis particularly suitable to solve problems that require an ongoing\ncycle of observing and tuning.\nIn the ML ecosystem, data collection is a crucial part. One\nreason we offer kernel training is to train on data collected with\na high sampling rate. Tracing OSs and storage systems with high\naccuracy and sampling rates is challenging [5]. Nevertheless,\ntracing tools like LTTng [60] can bring overhead down to as little\nas 5%. Additionally, traces may still be inaccurate due to data\nloss. LTTng collects trace data in shared user/kernel lockless\ncircular buffers; under heavy sampling loads, some trace events\ncan be dropped if LTTng’s user-level processing threads do\nnot consume the samples fast enough. However, operating in\nkernel space gives KML more control over thread scheduling to\nreduce loss of sampled events. Since our use cases may require\nhigh sampling rates for I/O events, placing data processing and\nnormalization in user space would lose too much valuable data\nthan in the kernel. Still, we believe a user-kernel co-operated\ndesign may be beneficial in some cases (part of our future work).\nTraining in user space. Building ML solutions is an iterative\nprocess. To find the essential features and build accurate models,\nwe need to run multiple data analyses, train, then test an ML\nmodel with different architectures and hyper-parameters. To\nspeed up model development and debugging, KML offers\noffline user-space training and kernel inferencing mode (see\nFigure 1(b)). As KML’s user- and kernel-space libraries use the\nsame APIs and code base, models trained in user space can be\nloaded into the kernel as is.\nFigure 3 shows how the readahead model works in operation\nmode. Components highlighted in yellow represent KML-\nspecific implementations. The red arrows denote the offline data\ncollection and training paths.\nWe started by collecting training data using in-kernel tracing\nof the target storage components [5]. Next was feature-extraction;\nthis is where user-space training was useful, because we could\nrun various analyses, test different features, and implement\n3\n\nmany data-normalization techniques without re-running I/O\nexperiments. After we finalized the feature selection, we trained\nand tested the readahead ML model in user space, varying\nseveral hyper-parameters; we used Tune [57] to optimize our\nhyper-parameters. When the readahead ML model was ready for\nreal-time testing, the only remaining step was to save the trained\nmodel to a KML-specific file and load it into the readahead kernel\nmodule. KML APIs facilitate all the functionality necessary for\nbuilding, training, saving, and deploying ML models in-kernel.\nTo ensure identical kernel and user APIs, we use wrappers\nto abstract external functionality. KML’s development API\nprovides 30 functions that fall into five categories: (i) memory\nmanagement, (ii) threading, (iii) logging, (iv) atomic operations,\nand (v) file operations. For example, we have a simple wrapper\ncalled kml_malloc that calls malloc in user-level and\nkmalloc in kernel space. For brevity, API details and prototypes\nare omitted, but part of our released code (see Section 2.6).\n2.4 Computational & Memory Overheads\nOSs and storage systems are susceptible to performance degra-\ndation and increased latency if computational and memory re-\nsources are not carefully managed. Therefore, we designed KML\nwith efficient CPU and memory usage in mind. There is often\na positive correlation between the computational and memory\nfootprint of an ML model and its training and inference accuracy.\nHence, KML is highly configurable, letting users trade-off over-\nheads vs. prediction accuracy to best suit the problem at hand.\nReducing computational overheads. Matrix manipulation\nis a computationally intensive ML building block that relies\non floating-point (FP) operations. OSs often disable the\nfloating-point unit (FPU) in the kernel to reduce context-\nswitching overheads. To address this, we considered three\napproaches: (1) quantization, (2) fixed-point representations,\nand (3) temporarily enabling the FPU unit in kernel space.\nQuantization provides compact representation, allows developers\nto compute matrix manipulation operations, and does not require\nan FPU [19,23,35,38,74]. Quantization can help reduce com-\nputational and memory overheads, but it reduces accuracy [41].\nFixed-point representation computes FP operations using integer\nregisters. Since all FP operations are emulated, integration of\nfixed-point representation is fairly easy and even faster in certain\ncases [17,58]. However, fixed-point representation works within\nfixed ranges which can result in numerical instability [51]. Since\nboth accuracy and stability are vital KML design goals, we chose\na third alternative: KML temporarily enable the FPU in the Linux\nkernel using kernel_fpu_begin andkernel_fpu_end .\nTo avoid context-switch overheads, we minimize the number\nof code blocks that use FPs and keep these blocks small.\nReducing memory overheads. Three factors affect KML’s\ndynamic memory consumption: (1) ML model-specific data,\n(2) KML’s internal memory allocations at training and inference,\nand (3) data collection for both training and inference. ML model-\nspecific data and KML’s internal memory usage depend on thenumber of layers, layer sizes, and layer types. KML uses dynamic\nmemory allocation for all internal usage purposes ( e.g., layer\ngradients); this helps reduce interference and memory pressure.\nKML gathers input data in a lock-free circular buffer; then, an\nasynchronous training thread trains on gathered data. When\ncollecting data with a high sampling rate, the size of the lock-free\ncircular buffer is important to the ML model’s performance and\naccuracy. Users need to configure the size of the circular buffer\nto account for the data sampling rate such that the asynchronous\ntraining thread can catch up with processing. If the size of the\ncircular buffer is misconfigured, KML may lose useful training\ndata, which can reduce the resulting ML model’s accuracy.\nOperating under resource-constrained conditions. KML\nexposes a memory allocation and reservation API for ML\ninternals. The primary motivation behind KML’s memory\nreservation capabilities is to ensure predictable performance and\naccuracy, even under memory pressure. This allows KML to\noperate without worry of memory allocation lagging or failing,\nwhich would hurt performance and accuracy.\nData processing & asynchronous training. To make ML\nsolutions generalizable, data normalization is often utilized.\nKML supports data normalization functionalities such as moving\naverage, standard deviation, and Z-score calculation. However,\ndata normalization often requires heavy FP computation. Thus,\nKML supports offloading training, inference, and data normaliza-\ntion to a separate asynchronous thread —away from the data path\nitself. This thread communicates with other KML components\n(e.g., data collection) using a lock-free circular buffer. By default,\nwe let Linux schedule this kthread as needed; KML also supports\npinning the kthread to a CPU core, to ensure it gets higher\nscheduling priority when high sampling rates are required.\nSubsampling is another viable solution to reduce data collec-\ntion overheads, which KML supports. However, subsampling\ncan reduce prediction accuracy, so care is needed to select a\nsuitable sampling rate. In Section 4.3 we evaluate the impact\nof subsampling windows on overheads, prediction accuracy, and\noverall I/O performance.\n2.5 Stability & Explainability\nBoth the training and inference phases for ML solutions can be\ncomputationally intensive. Except for model initialization and\nsaving models to files, KML APIs involve no other I/Os. KML’s\nimpact on the stability of storage performance is limited to\nmemory-allocation latency and concurrency. Memory allocations\nin both user and kernel space can use locking mechanisms,\nwhich could incur unexpected latencies. To minimize these\nproblems, KML allocates memory only in the asynchronous\ntraining thread. KML uses a lock-free circular buffer for data\ncommunication and reserves 512 bytes of additional memory\nto further ensure stability under memory-pressure conditions.\nLastly, we applied standard k-fold cross validation techniques\nto ensure the stability of our ML solutions.\nML solutions can suffer from unexpected behavior and\n4\n\nare harder to explain. Conversely, traditional heuristics have\nwell-defined behaviors often expressed as closed-form formulas.\nAn ML algorithm may behave erratically when used in new,\nunforeseen settings, which could hurt system performance where\nML is deployed. This type of issue is difficult to troubleshoot\ndue to the long-standing explainability problems that affect ML\nmodels [3]. KML currently supports two ML models: neural\nnetworks and decision trees. Decision tree predictions are more\nexplainable because they are represented as a tree of successive\nIF-THEN statements, bisecting the range of the features con-\nsidered. Deep neural networks, however, are more challenging\nto explain and verify. Nevertheless, recent work focuses on\nexplainability in ML [3,42,70,75]. While we plan to improve\nKML model stability using feedback-based control algorithms\nin the future, we currently focus on demonstrating that ML can\ntune storage system parameters better than existing heuristics.\n2.6 Implementation\nKML contains 12,213 lines of C/C++ code (LoC). KML’s core\nML part has 5,539 LoC, which can be compiled in both user and\nkernel space. Our readahead neural network model code is nearly\n1K LoC long: 486 LoC for collecting data, initializing the model,\ncreating an inference thread, and changing block-level and file-\nlevel readahead sizes; and another 351 LoC for model definition,\ndata processing, and normalization. Our NFS neural network\nmodel also includes nearly 1K LoC: 435 LoC for data collection,\nmodel initialization, and running inference to predict workload\ntype; and 338 LoC for creating the model and manipulating data.\nAll of our code has been released on github ( https:\n//github.com/sbu-fsl/kernel-ml [4]), which includes examples, sam-\nple data, models, and full API documentation (all 30 methods).\n3 Use Cases\nWe now detail our two use cases: (1) readahead neural network\nand decision tree models and (2) NFS neural network model.\nWe describe the following for each: (i) problem definition,\n(ii) data collection for training, (iii) data preprocessing and\nfeature extraction, and (iv) building the ML model.\n3.1 Use Case: Readahead\nProblem definition. Readahead is a technique to prefetch\nan additional amount of storage data into the OS caches in\nanticipation of its use in the near term. Determining how much to\nread ahead has always been challenging: too little readahead ne-\ncessitates more disk reads later and too much readahead pollutes\ncaches with useless data—both hurt performance. The readahead\nvalue is a typical example of a storage system parameter: while\ntunable, it is often fixed and left at its default. Some OSs let users\npass hints via fadvise andmadvise to help the OS recognize\nfiles that will be used purely sequentially or randomly, but these\noften fail to find optimal values for varied, mixed, or changing\nworkloads. Next, we detail our readahead neural network design\n(following Figure 3). Our goal is to predict optimal readahead\nsizes while running under dynamic I/O workloads.Studying the problem. We experimented with running 4\ndifferent RocksDB [32] benchmarks, each with 20 different\nreadahead sizes (8–1024), and attempted to determine the\nreadahead sizes that yield the best performance (in ops/sec) for\neach workload. This became our training data, which can help\npredict readahead values for other workloads and environments.\nThis investigation revealed that each workload has a unique\nbehavior and requires a different readahead size to reach optimal\nperformance. We further investigated the correlations between\nfile access patterns, RocksDB workload labels, and performance.\nThis helped us determine the information and features needed\nto build a good model, as described below.\nData collection. We used LTTng [60] to collect trace data,\nwhich we then used for finding useful features for the readahead\nproblem. We captured most page cache tracepoints [26] ( e.g.,\nadd_to_page_cache ,writeback_dirty_page ). We\ncollected and processed over 20GB of traces by running multiple\n10-minute RocksDB benchmarks on an NVMe-SSD device.\nTen minutes was sufficient for RocksDB to reach a steady state.\nAfter examining these traces, we selected a set of candidate\nfeatures based on our domain expertise. We then picked the\nfeatures of interest and decided where to call hook functions\nwhich are responsible for gathering necessary information ( e.g.,\nstruct page ) for inference. Our hook functions provide three\nimportant raw values: (1) time difference from the beginning of\nexecution, (2) inode number, and (3) page offsets of the files\nthat were accessed in locations where the hooks were called.\nData preprocessing & normalization. We summarize the\ninput data at one-second intervals to ensure we can quickly\nadapt to changing I/O workloads while ensuring stability under\nshort-term activity spikes. Based on our domain expertise, and\nthrough model experimentation, we selected the following five\nfeatures for our model: the number of transactions taking place\neach second, the calculated cumulative moving mean and the\ncumulative moving standard deviation of page offsets, the mean\nabsolute page offset differences for successive transactions, and\ntheinode number (to ensure we process only RocksDB file\naccesses). Before we fed these features to our readahead neural\nnetwork, we applied Z-score normalization to each feature.\nWhy we chose machine learning for this use case. After\nstudying the readahead problem, we wanted to explore whether\nmachine learning would be suitable for solving this problem or\nwhether more traditional heuristics could still work. Therefore,\nwhile extracting features from collected traces, we visualized\nthe features to investigate what type of patterns and clusters\nthe data has. Figure 4 shows a t-SNE [89] visualization of\nnormalized features that are generated from both NVMe-SSD\nand SA TA-SSD traces. t-SNE is a visualization technique that\napplies dimension reduction and is often used for representing\nhigh-dimensional data and looking for clusters. We can observe\nthat sequential and random workloads are somewhat separated;\nalas, data points from the same workload type are distributed\nover multiple clusters, overlapping clusters of other types. Worse,\n5\n\n/uni0000005e/uni00000004/uni00000064/uni00000004/uni00000372/uni0000005e/uni0000005e/uni00000018/uni00000003/uni0000018c/uni0000011e/uni00000102/uni0000011a/uni0000018c/uni00000102/uni00000176/uni0000011a/uni0000017d/uni00000175\n/uni0000005e/uni00000004/uni00000064/uni00000004/uni00000372/uni0000005e/uni0000005e/uni00000018/uni00000003/uni0000018c/uni000001c1/uni0000018c/uni00000102/uni00000176/uni0000011a/uni0000017d/uni00000175\n/uni0000005e/uni00000004/uni00000064/uni00000004/uni00000372/uni0000005e/uni0000005e/uni00000018/uni00000003/uni0000018c/uni0000011e/uni00000102/uni0000011a/uni00000190/uni0000011e/uni0000018b\n/uni0000005e/uni00000004/uni00000064/uni00000004/uni00000372/uni0000005e/uni0000005e/uni00000018/uni00000003/uni0000018c/uni0000011e/uni00000102/uni0000011a/uni0000018c/uni0000011e/uni000001c0/uni0000011e/uni0000018c/uni00000190/uni0000011e\n/uni00000045/uni00000073/uni00000044/uni0000011e/uni00000372/uni0000005e/uni0000005e/uni00000018/uni00000003/uni0000018c/uni0000011e/uni00000102/uni0000011a/uni0000018c/uni00000102/uni00000176/uni0000011a/uni0000017d/uni00000175\n/uni00000045/uni00000073/uni00000044/uni0000011e/uni00000372/uni0000005e/uni0000005e/uni00000018/uni00000003/uni0000018c/uni000001c1/uni0000018c/uni00000102/uni00000176/uni0000011a/uni0000017d/uni00000175\n/uni00000045/uni00000073/uni00000044/uni0000011e/uni00000372/uni0000005e/uni0000005e/uni00000018/uni00000003/uni0000018c/uni0000011e/uni00000102/uni0000011a/uni00000190/uni0000011e/uni0000018b\n/uni00000045/uni00000073/uni00000044/uni0000011e/uni00000372/uni0000005e/uni0000005e/uni00000018/uni00000003/uni0000018c/uni0000011e/uni00000102/uni0000011a/uni0000018c/uni0000011e/uni000001c0/uni0000011e/uni0000018c/uni00000190/uni0000011eFigure 4: t-SNE visualization of readahead normalized features that\nare generated from both NVMe-SSD and SATA-SSD traces. Axes are\nintentionally omitted because the dimensions are generated by t-SNE\nand do not represent any specific data.\neven random workloads’ clusters overlap with some sequential\nworkloads’ clusters, because RocksDB’s warm-up phases involve\nmostly sequential accesses—another source of dynamism. All\nthese findings strongly suggest that workload classification\nfor the readahead problem would be fairly challenging using\ntraditional heuristics. Hence, we felt motivated to explore ML\nsolutions to solving the readahead problem.\nBuilding neural network model. We modeled the readahead\nproblem as a classification problem and designed a neural\nnetwork with three linear layers (with hidden layer sizes of 5 and\n15), using sigmoid non-linearities in between layers, and with\na cross-entropy loss method as the loss function. We used an\nSGD optimizer [45,72], and set a learning rate of 0.01 and a mo-\nmentum of 0.99 after trying different values; all these values are\ncommon in the literature [8]. We also used Tune [57] to optimize\nthe learning rate and momentum. Our readahead neural network\ntrains on the aforementioned input data and predicts the workload\ntype. We trained on the following four types of RocksDB\nworkloads on NVMe-SSD because they provide a diverse com-\nbination of random and sequential operations: (i) readrandom,\n(ii) readseq, (iii) readrandomwriterandom, and (iv) readreverse.\nClass frequencies were close, suggesting that classification accu-\nracy is a good metric to evaluate the performance, with the least\nfrequent class being 21.4% and most frequent class being 28.8%.\nWe tested the neural network’s performance with the aforemen-\ntioned data via k-fold cross validation with k=10, and found out\nthat it achieved an average accuracy of 95.5%. We also analyzed\nthe contribution of each feature to the classification performance;\nwe randomized the order of a feature of interest across samples in\nthe validation dataset, and then calculated the 10-fold validation\nperformance [9]. Using Pearson correlation analysis [68], we\nfound that two features were highly correlated: the cumulative\nmoving standard deviation and the cumulative moving mean\nof page offsets. Including both would have over-emphasized\ntheir importance in this analysis, so we excluded the cumulative\nstandard deviation of page offsets. Cross validation results were69.6%, 76.4%, 42.6%, and 89.1% for number of transactions,\ncumulative moving mean of page offsets, mean absolute page\noffset differences, and current readahead value, respectively.\nThis shows that mean absolute page offset differences is the\nmost important feature, because randomizing its order reduced\nthe validation results the most (down to 42.6%)—followed by\nnumber of transactions, cumulative moving mean of page offsets,\nand finally the currently used readahead value.\nAfter obtaining classification predictions, we set the em-\npirically determined optimal readahead sizes according to the\npredicted workload type. In Section 4.4, we evaluate the read-\nahead neural network not only on workloads we trained on but\nalso on workloads that were notincluded in the training data and\nworkloads running on different devices (NVMe vs. SA TA SSDs).\nFigure 4 shows that the same type of workloads for SA TA-SSD\nvs. NVMe-SSD are not placed in the same clusters all the time.\nWe use neural network input data that is generated only from\nan NVMe-SSD to train readahead neural network; nevertheless,\nwe still get significant performance improvement even for\nSA TA-SSDs (see Section 4.4). This indicates that our readahead\nneural network is indeed learning higher-level abstractions about\nthe workloads, one that traditional heuristics would struggle with.\nFinally, we also experimented with the readahead neural\nnetwork using TPC-H [87] queries running on MySQL [66]\nto show how our readahead neural network behaves on\ncompletely different types of workloads and applications and\nhow generalizable the models are.\nDecision-tree models. We also built a decision-tree (DT)\nmodel for workload type classification based on the same\nfeatures and training data. The readahead DT model contains 59\nnodes with a maximum depth of 9. We omit the full DT figure\nfor brevity (available as part of our source/data release), but for\nexample, the decision at the root node is whether the Z-score of\nthe mean absolute page offset was less than \u00000:349. We tested\nthe prediction accuracy of this DT using the same procedure\nwith the readahead neural network (10-fold cross-validation),\nand observed that it results in an average prediction accuracy of\nonly 75.4%. As mentioned in Section 2.5, KML supports DTs\nbecause DTs trees are more explainable than neural networks\nand run considerably faster. We evaluated the readahead DT\nusing the same procedure as the neural networks (Section 4.4).\nReadahead in per-file basis. So far, we have shown how we\napproach the readahead problem when a single I/O workload\nis accessing one device. Storage system developers recognize\nthe challenge of handling mixed storage workloads running\non the same system—a common occurrence [7]. In that case,\nreadahead values cannot be set at the device level, as that would\nbe suboptimal in mixed workloads. Instead, readahead values\nshould be set at a higher abstraction level, on a per-file basis. To\nshow our neural network’s versatility, we use the same model\nto tune readahead sizes not only on a per-disk basis but also on\na per-file basis. Whereas before we ran inference every second\nand set one readahead value for an entire device, here we ran\n6\n\ninference every second on each open file and set a readahead\nvalue directly in Linux’s struct file . We evaluated the\nper-file basis approach and found that it could predict and\nimprove I/O throughput for mixed workloads better than both\nthe vanilla and per-disk basis approaches (see Section 4.4).\n3.2 Use Case: NFS rsize\nProblem definition. Networked storage systems such as NFS\nare popular and heavily used. NFS is used for storing virtual\nmachine disks [62], hosting NoSQL databases [83], and more.\nA misconfiguration of NFS can hurt performance. We experi-\nmented with different applications using NFS and found out that\none critical NFS configuration parameter is the rsize —default\nnetwork read-unit size. Hence, we focus predicting an optimal\nNFS rsize value based on workload characteristics.\nStudying the problem. We tested NFS using the same\nmethodology as for readahead. The only difference here is\ntuning rsize instead of readahead. We used NFSv4 for all of\nour tests. The NFSv4 implementation we used supports only\nseven different rsize values (4K–256K). However, in the\nNFS use case, there are additional external factors not present\nin the readahead problem that can affect I/O performance\n(e.g., NFS server configuration, network speed, and number\nof clients connected to the same server). We experimented\nwith four different RocksDB benchmarks under different NFS\nserver configurations and network conditions. We configured\nour server with two different NFS mount point options—one\nbacked by NVMe-SSD and one backed by SA TA-SSD. We\ninjected artificial network delays to simulate slower networks.\nOur experiments revealed that random and sequential workloads\nrequire different rsize values to achieve optimal performance.\nData collection. We enabled NFS and page-cache related\nkernel tracepoints to collect training data ( e.g.,nfs4_read ,\nnfs4_readpage_done ,vmscan_lru_shrink_inactive ,\nandadd_to_page_cache ). Unlike the readahead neural\nnetwork model, we collected data from tracepoints not only\nto model page cache behavior, but also network conditions.\nSimilarly studying these traces, we chose our feature set and\nplaced our hook functions. Our feature set includes eight features\n(described below) which are calculated using the following five\ndata points: (i) time difference from the beginning of execution\nfor each tracepoint transaction, (ii) NFS file handles, (iii) file\noffsets in NFS requests, (iv) page offsets of the files that were\naccessed, and (v) number of reclaimed pages during LRU scans.\nData preprocessing & normalization. We applied the same\ndata preprocessing and normalization techniques that we used for\nthe readahead neural network. The NFS neural network model\nconsists of eight features which are computed every second:\n(1) number of tracepoint transactions, (2) average time difference\nbetween each nfs4_read andnfs_readpage_done match-\ning pair, (3) average time difference between each consecutive\nnfs4_read request, (4) average time difference between\neach consecutive nfs4_readpage_done request, (5) meanabsolute requested offset difference between each consecutive\nnfs4_read request, (6) mean absolute page offset difference\nbetween each consecutive add_to_page_cache , (7) average\nnumber of reclaimed pages, and (8) current rsize .\nNeural network model. We trained and tested our NFS neural\nnetwork model using exactly the same methodology as the read-\nahead problem; for brevity, we detail only the differences between\nthe neural network models. We approached the NFS problem as a\nworkload characterization problem and constructed our NFS neu-\nral network model with four linear layers (with hidden layer sizes\nof 25, 10, and 5) with sigmoid activation functions in between.\nSimilar to the readahead neural network, we used cross entropy\nas the loss function and SGD as the optimizer. We evaluated\nthe NFS neural network model and found out that it results in a\nprediction accuracy of 98.6% (using 10-fold cross-validation).\n4 Evaluation\nOur evaluation proceeds as follows: First, we explain our\nevaluation goals in Section 4.1. We then describe the testbed\ndesign and benchmarks that we used to evaluate the readahead\nand NFS rsize neural networks in Section 4.2. In Section 4.3\nwe provide performance details regarding KML’s training and\ninference. Section 4.4 shows how the readahead ML models\nimprove performance. Finally, in Section 4.5, we present our\nevaluation of the rsize neural network model for NFS.\n4.1 Evaluation Goals\nOur primary evaluation goal is to show that using ML techniques\ninside the OS can be used to to tune parameters dynamically and\nimprove storage systems’ performance.\nWe start by showing the practicality of using ML in kernel\nspace. We evaluate KML’s system overheads in terms of (i) data\ncollection overhead, (ii) training cost, (iii) inference cost, and\n(iv) memory usage. Then, we evaluate both readahead and NFS\nneural network models to show how they improve the I/O perfor-\nmance and quickly adapt the system in the presence of changing\nworkloads and conditions. To show that our models can learn\nabstract workload patterns, we first present the generalization\npower of our models by testing it on workloads notincluded in\nthe training dataset. Next, we present benchmarks on a device\ntype that was notused in the data collection phase or training. We\nalso built a decision tree model for the readahead problem to have\ncomparable results since decision trees are more explainable,\nstill popular, and closer in operation to traditional heuristics.\nFurthermore, we evaluate KML’s versatility by applying\nthe readahead neural network model on a per-file basis. This\ndemonstrates KML’s ability to optimize individual I/Os in a\nmixed workload. Lastly, we evaluate our readahead ML models’\nbehavior when they mispredict and how quickly they recover.\n4.2 Testbed\nWe ran the benchmarks on two identical Dell R-710 servers, each\nwith two Intel Xeon quad-core CPUs (2.4GHz, 8 hyper-threads),\n7\n\n24GB of RAM and an Intel 10GbE NIC. In some experiments,\nwe intentionally configured the system with only 1GB of mem-\nory to force more memory pressure on the I/O system; but we\nalso show experiments with the full 24GB of system RAM. We\nused the CentOS 7.6 Linux distribution. We developed KML for\nLinux kernel version 4.19.51, the long-term support stable kernel;\nwe added our readahead ML models to this kernel and used it\nin all experiments. Because HDDs are becoming less popular\nin servers, especially when I/O performance is a concern, we\nfocused all of our experiments on SA TA and NVMe SSDs. We\nused Intel SSDSC2BA200G3 200GB as our SA TA-SSD device\nand a Samsung MZ1LV960HCJH-000MU 960GB as our NVMe-\nSSD device, both formatted with Ext4. These two devices were\nused exclusively for RocksDB databases. To avoid interference\nwith the installed CentOS, the two servers have a dedicated\nSeagate ST9146852SS 148GB SAS boot drive for CentOS,\nutilities, and RocksDB benchmark software. We used 10GbE\nswitches to connect the machines (useful for NFS experiments).\nWe observed an average RTT time of 0.2 milliseconds.\nBenchmarks. We chose RocksDB’s db_bench tool to gen-\nerate diverse workloads for evaluating the readahead and NFS\nrsize neural networks. RocksDB [32] is a popular key-value\nstore and covers an important segment of realistic storage sys-\ntems; db_bench is a versatile benchmarking tool that includes a\ndiverse set of realistic workloads. Workloads can be run individu-\nally or in series, and the working set size can be easily configured\nto generate more I/O pressure on a system. On the 1GB RAM\nsystems, we configured a RocksDB of twice the size (2GB).\nTo demonstrate that our ML models can learn from and\noptimize for different types of real-world workloads, we\nchose the following six popular yet different db_bench\nworkloads: (1) readrandom, (2) readseq, (3) readrandomwriteran-\ndom (alternating random reads and writes), (4) readreverse,\n(5) updaterandom (read-modify-write in random offsets), and\n(6)mixgraph (a complex mix of sequential and random\naccesses, based on Facebook’s realistic data that follow certain\nPareto and power-law distributions [14]).\nWe trained our readahead neural network on traces that\ncontain only four of these workloads: readrandom, readseq,\nreadreverse, readrandomwriterandom—all running only on\nthe NVMe-SSD. These four tend to be the simpler workloads,\nbecause we wanted to see whether KML can train on simpler\nworkloads yet accurately predict on more complex workloads\nnot trained on. This also ensures a balanced representation of\nrandomness and sequentiality in the training dataset.\nAfter the training phase completed, we tested our models on\nall six workloads as well as different devices. This was done\nto show that our models not only perform accurate predictions\non the training set samples, but they also generalize to two new\nand complex workloads (updaterandom and mixgraph as well\nas a different device (SA TA-SSD))—which were excluded from\nthe training data. We evaluated mixed workloads by running two\nconcurrent db_bench instances, each on a separate RocksDB\nNormalized to no sampling (X=1)A\nSubsampling window size (log)BCFigure 5: Performance (A), prediction accuracy (B) and CPU overheads\n(C) in seven different subsampling window sizes for the per-disk read-\nahead neural network. Upward green arrows denote that higher is better .\ndatabase and using a different workload profile, both stored on\nthe same device. We kept the hardware configuration the same as\nbefore (1GB RAM) to increase system and page-cache pressure.\nWe also experimented with our readahead network model\nusing TPC-H [87] queries running on MySQL [66], to evaluate\nhow generalizable and effective the readahead neural network is\nto an entirely different workload. In this paper we do notclaim\nthat our readahead neural network model will work universally\nto optimize readahead values for all possible workloads. Rather,\nthese use cases are meant to demonstrate the KML framework’s\nversatility. With more workloads and datasets, one can build a\nwide range of ML models to optimize many storage problems.\n4.3 KML’s Overheads\nAn ML model’s overhead depends on its architecture. Generally,\ndeeper or higher-dimensional neural networks consume more\nmemory and CPU than, say, decision-tree models. It is vital\nthat an ML component, especially one that may run inside the\nkernel, consume as little CPU and memory as possible. Next,\nwe evaluate the readahead neural network overheads.\nData gathering overheads. The only inline operations that\nreadahead neural network inserts directly in the data path are\ndata collection probes. Hence it is vital for these probes to be\noptimized. Figure 5(C) shows how the data collection CPU\noverheads (percentage) change with subsampling window\nsizes. When there is no subsampling in the system ( X= 1),\nthe CPU overheads of data collection probes is as high as\n0.18%. Although this is a fairly low overhead considering\nthe multiplicative I/O benefits we report, this overhead can\nbe reduced further by increasing the subsampling window.\nHowever, increasing the subsampling windows size can hurt\nprediction accuracy and performance improvements, as less data\nis available to make rapid predictions. see Figure 5(A) and (B).\nFigure 5(B) shows that workloads with a lot of randomness\n8\n\nFigure 6: Running four back-to-back RocksDB workloads in order from left to right: readsequential, readrandom, readreverse, then mixgraph. Here,\nwe started with the default readahead value; thereafter , the last value set in one workload was the one used in the next run. For each of the four\ngraphs, we show their Y axes (throughput, different scales). The readahead value is shown as the Y2 axis for the rightmost graph (d) and is common for\nall four . Each workload ran 15–50 times in a row, to ensure we ran it long enough to observe patterns of mis/prediction and reach steady-state. Again,\nwe see KML adapting, picking optimal readahead values, occasionally mis-predicting but quickly recovering, hence overall throughput was better .\nin them were the least affected, because randomness is still\npredicted as random even with fewer samples; yet we can reduce\nthe already small CPU overheads even more.\nThe figure further shows that only sequential workloads are\naffected by subsampling window changes: generally, as the\nsampling window widens, prediction accuracy and normalized\nperformance worsen. However, we noticed an unexpected be-\nhavior for the readseq workload. Increasing the subsampling\nwindow size from one to five or ten actually improved both\nprediction accuracy and performance; this is because readseq\nkeeps the I/O subsystem busy at near maximum bandwidth, and\nincreasing subsampling window size reduced short-term noise\nthat resulted in more frequent mispredictions.\nWe can also observe that the data collection overheads depend\non the workload type. For example, readseq workload’s av-\nerage data sampling frequency per-second is around 30K but its\ndata collection overhead is still lower than mixgraph workload\nwhich has 20K average data sampling frequency per-second.\nThe reason that data collection overheads change based on the\nworkload type, is due to the sudden I/O bursts resulting in some\ncache misses (bi-modal histograms omitted for brevity).\nInference/training overheads. The readahead neural network\nperforms inference (prediction) and changes the block-layer\nreadahead value in 21 \u0016s on average (std. dev. <10%). This\naction executes in a separate, asynchronous kernel thread, once in\nevery second. Hence, it has negligible impact on the overall OS\nperformance. When the readahead neural network runs in per-file\nmode, KML runs inferences an average 135 times a second\n(i.e., one per open file): inferencing for all open files consumes\n1.7ms on average. We measured that the readahead decision\ntree inference takes only 8 \u0016s (using the same feature vector).\nThe readahead neural network and decision tree have the same\ndata pre-processing and normalization implementation—the\nonly difference between them is in the inference part. Overall,\nthese overheads are fairly small and acceptable, considering the\nmultiplicative I/O performance benefits they enable.\nAs discussed in Section 3.1, our readahead neural network\nprototype offloads training to the user level. We measured\nthe time to perform one training iteration in user level at 51 \u0016s\non average; this training iteration includes the forward pass,\nback-propagation, and weight update stages.Memory overheads. The readahead neural network allocates\n3,916 bytes of dynamic memory during the model’s initialization\nphase. While inferencing, KML temporarily allocates 676 bytes\nbefore returning the inference results. This overall memory foot-\nprint is negligible in today’s multi-GB systems. The readahead\ndecision tree occupies only 2,432 bytes of dynamic memory\nduring initialization. The decision tree model does not allocate\ndynamic memory during inference. Lastly, the kernel module\nreadhead.ko has a binary memory footprint of 432KB and the\nkernel module nfs.ko is 636KB, while the KML framework\nitself ( k-Mlib.ko ) has a memory footprint of 5.5MB.\nPracticality and scalability. Our vision is KML could enable\na future where traditional heuristics are gradually replaced with\nML-based approaches to improve storage and network I/O\nperformance. In Section 4.4 we demonstrate, for example, that\nour readahead neural network model improves I/O performance\nby as much as 2.3 \u0002, but consumes less than 0.2% additional\nCPU cycles: we believe this is a fairly acceptable trade-off\nfor most users. Nevertheless, we tested this model with 100\nconcurrent inferences and found that both overheads and I/O\nimprovements have scaled linearly; hence KML’s benefits still\noutweigh its overheads.\n4.4 Readahead Evaluation\nReadahead background. There are two places in the Linux\nkernel where readahead is defined: the block layer and the file\nsystem level. When a file is opened, the VFS initializes an\nopen struct file and copies the readahead value for that\nfile from the corresponding block layer. Upon a page fault for\nthat file, the page-cache layer uses the value stored in the file to\ninitiate reading-ahead the desired number of sectors of that file.\nHowever, the readahead value in the file structure is initialized\nonly once when the file is opened. So when KML changes the\nblock layer readahead value, the Linux kernel does not copy\nthe new value to any file already opened. This means that open\nfiles may continue to use a sub-optimal readahead value, even if\nbetter values are available ( e.g., due to workload changes). That\nis why we implemented a mechanism that changes the readahead\nsize for open files when KML changes the disk-level readahead\nvalue. This propagates newer readahead values to each open file,\nimproving our adaptability. Conversely, if KML mispredicts the\n9\n\n3.36.35.93.46.25.83.35.75.53.56.35.93.66.25.43.56.05.21988.8669.82014.91955.1666.62011.81846.5659.91999.0801.3462.4935.3811.6464.4922.8793.8457.6920.11992.1676.12020.81958.5672.82017.61849.8665.62004.5804.8468.7941.1815.3470.6928.3797.3463.6925.30.10.55.050.0500.05000.0\nvanillakmldiskkmlfilevanillakmldiskkmlfilevanillakmldiskkmlfilevanillakmldiskkmlfilevanillakmldiskkmlfilevanillakmldiskkmlfilereadrandomrw-randommixgraphreadrandomrw-randommixgraphreadseqreadreverse1000s ops/sec (log10)\n++Figure 7: Mixed workloads results. We ran sequential and random workload combinations on the same NVMe-SSD device. Each unique combination\nis tested with the readahead neural network running in per-disk basis (kml disk) and per-file basis (kml file) and compared against vanilla results.\nThe model running in per-file basis outperformed both vanilla and per-disk modes.\n\u0013\u0014\u0013\u0013\u0013UHDGUHYHUVH\u0003.0/UHDGUHYHUVH\u00039DQLOOD\n\u0013\u0015\u0017\u0019\u001b\u0014\u0013\u0013\u0011\u0013\u0015\u0011\u0018\u0018\u0011\u0013PL[JUDSK\u0003.0/PL[JUDSK\u00039DQLOOD7LPH\u0003\u000bPLQXWHV\f7KURXJKSXW\u0003\u000b\u0014\u0013\u0013\u0013V\u0003RSV\u0012VHF\f\n\u0013\u0014\u0013\u0013\u0013UHDGUHYHUVH\u0003.0/UHDGUHYHUVH\u00039DQLOOD\n\u0013\u0015\u0017\u0019\u001b\u0014\u0013\u0013\u0011\u0013\u0015\u0011\u0018\u0018\u0011\u0013PL[JUDSK\u0003.0/PL[JUDSK\u00039DQLOOD7LPH\u0003\u000bPLQXWHV\f7KURXJKSXW\u0003\u000b\u0014\u0013\u0013\u0013V\u0003RSV\u0012VHF\fA\nB\nFigure 8: Mixed workloads results on a timeline, comparing the\nreadahead neural network model running on per-file basis (’A’, top)\nvs. per-disk basis (’B’, bottom).\nworkload type and changes the readahead size to a suboptimal\nvalue, short-term performance degradation can happen, which\nmight hurt overall performance.\nBack-to-back workloads on NVMe. Figure 6 shows four\nworkloads running back to back with each subfigure comparing a\nvanilla run (colored orange) to our KML-enabled readahead run\n(colored blue). The readahead value was left at the default value\n(i.e., 256) at the start of both vanilla and KML-enabled runs, but\nwhen the next workload started, it used the last readahead value\nfrom the previous workload’s run ( e.g., the readahead value at the\nend of the leftmost subfigure is the same at the start of the subfig-\nure immediately to its right). This experiment evaluates KML’s\nability to optimize the readahead values when the I/O workload\nmay change every few minutes. The X axes indicate the run\ntime in minutes. The Y axes indicate throughput in thousands of\nops/sec (higher is better), and have different scales for each exper-\niment. The Y2 axes show the readahead values used or predicted\nby KML over time in terms of number of sectors (denoted with\na green line and using the same scale). Each workload ran 15–50\ntimes in a row, so it ran long enough to observe mis/predictionspatterns. As seen in Figure 6, KML adapts quickly to changing\nworkloads by tuning the readahead value in about one second.\nAlthough we observe some mis/prediction patterns, seen as\nsudden spikes, overall throughput still improved across all four\nruns, averaging 63.25% improvement: 140% improvement for\nreadrandom, 2% for readsequential, 109% for mixgraph , and\n12% for readreverse. We note that even a small improvement\nin throughput can yield significant cumulative energy and\neconomic cost savings for long-running servers [54].\nRead-sequential workloads. Out of the six workloads we\nran, Figure 9 shows the one where KML performed the worst:\nread-sequential. Reading data sequentially directly from the\nraw SA TA-SSD is nearly 1,000 \u0002faster than the mixgraph\nworkload, and nearly 400 \u0002faster with the NVMe-SSD. Here,\nthere is little opportunity for KML to improve throughput for\na sequential workload that reads at speeds near the maximum\nthroughput of the physical device.\nRead-reverse workloads. As we can see from the fluctuating\ngreen line (readahead values in Figure 6) KML mispredicts\nreadreverse as readseq and changes the readahead value to\nsomething suboptimal. These two workloads both access files\nsequentially—one reading forward and one backward. Inter-\nestingly, readseq and readreverse are quite close from a feature\nrepresentation perspective, which explains the mispredictions.\nBut since both of these workloads access files sequentially, their\noptimal readahead values are also quite close to each other. Thus,\neven when KML mispredicts readreverse as readseq or vice\nversa, this had a small overall impact on performance.\nSummary of readahead neural network results. We sum-\nmarize all readahead neural network results in Figure 9. We ob-\nserve that the average throughput improvement for NVMe-SSD\nis ranging from 0% to 65%. We saw greater improvements in the\nSA TA-SSD case, ranging from 2% to 130% (2.3 \u0002). Lastly, we\nran the complex mixgraph workload on NVMe-SSD with the\nsystem memory set to the maximum ( i.e., 24GB) and the database\nsize set to be relatively large, 65GB (compared to a 2GB baselines\ndatabase size). This experiment ran for nearly an hour (48.5 min-\nutes) and resulted in an average throughput improvement of 38%.\n10\n\n/uni0000018c/uni0000011e/uni00000102/uni0000011a/uni00000190/uni0000011e/uni0000018b /uni0000018c/uni0000011e/uni00000102/uni0000011a/uni0000018c/uni00000102/uni00000176/uni0000011a/uni0000017d/uni00000175 /uni0000018c/uni0000011e/uni00000102/uni0000011a/uni0000018c/uni0000011e/uni000001c0/uni0000011e/uni0000018c/uni00000190/uni0000011e /uni0000018c/uni000001c1/uni00000372/uni0000018c/uni00000102/uni00000176/uni0000011a/uni0000017d/uni00000175 /uni000001b5/uni00000189/uni0000011a/uni00000102/uni0000019a/uni0000011e/uni0000018c/uni00000102/uni00000176/uni0000011a/uni0000017d/uni00000175 /uni00000175/uni0000015d/uni000001c6/uni00000150/uni0000018c/uni00000102/uni00000189/uni0000015a /uni000003ec/uni00000358/uni000003ec/uni000003ec/uni00000358/uni000003f1/uni000003ed/uni00000358/uni000003ec/uni000003ed/uni00000358/uni000003f1/uni000003ee/uni00000358/uni000003ec/uni000003ee/uni00000358/uni000003f1/uni00000057/uni0000011e/uni0000018c/uni00000128/uni0000017d/uni0000018c/uni00000175/uni00000102/uni00000176/uni00000110/uni0000011e/uni00000003/uni0000002f/uni00000175/uni00000189/uni0000018c/uni0000017d/uni000001c0/uni0000011e/uni00000175/uni0000011e/uni00000176/uni0000019a/uni00000003/uni0000037e/uni00000079/uni0000037f/uni000003ed/uni00000358/uni000003ec/uni000003ee/uni000003ee/uni00000358/uni000003ef\n/uni000003ed/uni00000358/uni000003ed/uni000003ee/uni000003ee/uni00000358/uni000003ee /uni000003ee/uni00000358/uni000003ee/uni000003ee/uni000003ee/uni00000358/uni000003ec/uni000003f5\n/uni000003ec/uni00000358/uni000003f5/uni000003f2/uni000003ed/uni00000358/uni000003f2/uni000003f1\n/uni000003ed/uni00000358/uni000003ec/uni000003f0/uni000003ed/uni00000358/uni000003f1/uni000003f1 /uni000003ed/uni00000358/uni000003f1/uni000003ef /uni000003ed/uni00000358/uni000003f1/uni000003ed/uni0000005e/uni00000004/uni00000064/uni00000004/uni00000372/uni0000005e/uni0000005e/uni00000018 /uni00000045/uni00000073/uni00000044/uni0000011e/uni00000372/uni0000005e/uni0000005e/uni00000018Figure 9: Readahead neural network performance improvements ( \u0002)\nfor RocksDB benchmarks on SATA-SSD and NVMe-SSD across all six\nworkloads, normalized to vanilla (1.0 \u0002).\n/uni0000018c/uni0000011e/uni00000102/uni0000011a/uni00000190/uni0000011e/uni0000018b /uni0000018c/uni0000011e/uni00000102/uni0000011a/uni0000018c/uni00000102/uni00000176/uni0000011a/uni0000017d/uni00000175 /uni0000018c/uni0000011e/uni00000102/uni0000011a/uni0000018c/uni0000011e/uni000001c0/uni0000011e/uni0000018c/uni00000190/uni0000011e /uni0000018c/uni000001c1/uni00000372/uni0000018c/uni00000102/uni00000176/uni0000011a/uni0000017d/uni00000175 /uni000001b5/uni00000189/uni0000011a/uni00000102/uni0000019a/uni0000011e/uni0000018c/uni00000102/uni00000176/uni0000011a/uni0000017d/uni00000175 /uni00000175/uni0000015d/uni000001c6/uni00000150/uni0000018c/uni00000102/uni00000189/uni0000015a /uni000003ec/uni00000358/uni000003ec/uni000003ec/uni00000358/uni000003f1/uni000003ed/uni00000358/uni000003ec/uni000003ed/uni00000358/uni000003f1/uni000003ee/uni00000358/uni000003ec/uni000003ee/uni00000358/uni000003f1/uni00000057/uni0000011e/uni0000018c/uni00000128/uni0000017d/uni0000018c/uni00000175/uni00000102/uni00000176/uni00000110/uni0000011e/uni00000003/uni0000002f/uni00000175/uni00000189/uni0000018c/uni0000017d/uni000001c0/uni0000011e/uni00000175/uni0000011e/uni00000176/uni0000019a/uni00000003/uni0000037e/uni00000079/uni0000037f/uni000003ec/uni00000358/uni000003ee/uni000003f3/uni000003ee/uni00000358/uni000003ed/uni000003f5\n/uni000003ec/uni00000358/uni000003f2/uni000003f0/uni000003ee/uni00000358/uni000003ed /uni000003ee/uni00000358/uni000003ed/uni000003ef/uni000003ed/uni00000358/uni000003f5/uni000003f5\n/uni000003ec/uni00000358/uni000003f2/uni000003ed/uni00000358/uni000003f1/uni000003f5\n/uni000003ec/uni00000358/uni000003f4/uni000003f1/uni000003ed/uni00000358/uni000003f1/uni000003f1/uni000003ed/uni00000358/uni000003f0/uni000003f5 /uni000003ed/uni00000358/uni000003f0/uni000003f4/uni0000005e/uni00000004/uni00000064/uni00000004/uni00000372/uni0000005e/uni0000005e/uni00000018 /uni00000045/uni00000073/uni00000044/uni0000011e/uni00000372/uni0000005e/uni0000005e/uni00000018\nFigure 10: Readahead decision tree performance improvements ( \u0002)\nfor RocksDB benchmarks on SATA-SSD and NVMe-SSD devices across\nall six workloads, normalized to vanilla (1.0 \u0002).\nMixed workloads. Mixed workloads are considered a\nchallenging optimization problem [7]. In Figure 8, we present a\ntimeline performance comparison using the readahead neural net-\nwork model running on a per-disk vs. per-file basis. The per-file\nmode performs better overall because readahead values are set for\neach open file independently. Conversely, in the per-disk mode, a\nsingle readahead value is set at the disk level and hence uniformly\non all open files: a readahead value good for one workload is\nlikely to be suboptimal for other open files. One reason why the\nper-disk mode cannot predict workload types correctly is that\nwhen different workloads are mixed—even sequential ones or\nones with regular patterns—the mix looks more like a purely\nrandom workload at the disk level. Figure 7 shows overall mixed\nworkloads performance comparisons. Per-file mode performed\noverall better in every combination of mixed workloads. If\nwe compare only the sequential parts of the mixed workload\ncombination (orange bars in Figure 7), in per-disk mode, we\nobserve significant performance degradation. However, in\nper-file mode, we can observe performance improvements for\nboth the sequential and random (blue bars in Figure 7) parts of\nthe mixed workload combination. The reason why per-disk mode\nperforms better for the random parts of the mixed workload com-\nbinations is for the same reason: mixing workloads looks more\nrandom-like at the disk level. KML predicts these as readrandom\nor readrandomwriterandom which coincidentally fits this part\nof the workload, but significantly hurts non-random workloads.\nDecision tree evaluation. In addition to the neural network\nmodel, we implemented a decision tree model for the readahead\nproblem to compare the two ML approaches on the same\nproblem. We tested the readahead decision tree the same way.\nFigure 10 shows that there is a performance improvement\nfor workloads with a random component. For the readahead\n0 2 4 6 8 10 12 14\nRuntime (minutes)500750100012501500175020002250Throughput (1000s ops/sec)readseq_vanilla readseq_kml readahead\n02004006008001000\nReadahead size (sectors)Figure 11: Performance timeline graph for tuning with KML decision\ntree while running readseq workload on NVMe-SSD.\n/uni000003ed /uni000003f2 /uni000003f5 /uni000003ed/uni000003ed /uni000003ed/uni000003ef /uni000003ed/uni000003f0 /uni000003ed/uni000003f1 /uni000003ee/uni000003ee\n/uni00000064/uni00000057/uni00000012/uni00000372/uni0000002c/uni00000003/uni00000059/uni000001b5/uni0000011e/uni0000018c/uni000001c7/uni00000003/uni00000045/uni000001b5/uni00000175/uni0000010f/uni0000011e/uni0000018c/uni000003ec/uni00000358/uni000003ec/uni000003ec/uni00000358/uni000003f1/uni000003ed/uni00000358/uni000003ec/uni000003ed/uni00000358/uni000003f1/uni000003ee/uni00000358/uni000003ec/uni00000057/uni0000011e/uni0000018c/uni00000128/uni0000017d/uni0000018c/uni00000175/uni00000102/uni00000176/uni00000110/uni0000011e/uni00000003/uni0000002f/uni00000175/uni00000189/uni0000018c/uni0000017d/uni000001c0/uni0000011e/uni00000175/uni0000011e/uni00000176/uni0000019a/uni00000003/uni0000037e/uni00000079/uni0000037f/uni000003ed/uni00000358/uni000003ef/uni000003f5\n/uni000003ed/uni00000358/uni000003ec/uni000003f2/uni000003ed/uni00000358/uni000003ec/uni000003ef\n/uni000003ec/uni00000358/uni000003f0/uni000003f3/uni000003ed/uni00000358/uni000003ec/uni000003ed/uni000003ed/uni00000358/uni000003ec/uni000003f2 /uni000003ed/uni00000358/uni000003ec/uni000003f3/uni000003ed/uni00000358/uni000003ef/uni000003f4\n/uni000003ed/uni00000358/uni000003ec/uni000003f4/uni000003ed/uni00000358/uni000003ed/uni000003ed/uni000003ed/uni00000358/uni000003ec/uni000003f0\n/uni000003ec/uni00000358/uni000003f2/uni000003f0/uni000003ec/uni00000358/uni000003f5/uni000003f5/uni000003ed/uni00000358/uni000003ed/uni000003ed/uni00000358/uni000003ed/uni000003f4\n/uni000003ec/uni00000358/uni000003f5/uni000003f1/uni0000005e/uni00000004/uni00000064/uni00000004/uni00000372/uni0000005e/uni0000005e/uni00000018 /uni00000045/uni00000073/uni00000044/uni0000011e/uni00000372/uni0000005e/uni0000005e/uni00000018\nFigure 12: Readahead neural network performance improvements ( \u0002)\nfor TPC-H queries on SATA-SSD and NVMe-SSD devices, normalized\nto vanilla (1.0 \u0002).\ndecision tree, we measure average throughput improvement\nfor random workloads on NVMe-SSD as ranging from 48% to\n59%; and in the SA TA-SSD case, ranging from 99% to 119%\n(2.19\u0002). While good, the neural network model yielded greater\nimprovements, as discussed above.\nThe DT model, however, degraded performance for sequential\nworkloads. it degraded performance for sequential workloads\non NVMe-SSD by 15–40%; and in the SA TA-SSD case, by\n36–73% worse. We investigated this performance degradation.\nFigure 11 shows the readseq workload running on a RocksDB\ninstance stored on an NVMe-SSD. Here, the readahead decision\ntree predicts the workload correctly in the first three minutes,\ndespite some fluctuations. Afterwards, the decision tree\nmodel’s predictions fluctuate wildly, and at around minute\n10 it consistently makes wrong predictions. Overall, this was\nsomewhat expected for our I/O optimization problem: neural\nnetwork models, while more complex to train and use, are more\nadaptable than decision-trees [36]. Specifically, when the DT\nmodel mispredicts, and system conditions change ( i.e., I/O\nactivity), the DT model continues to mispredict, and it cannot\nrecover as quickly as the more adaptable neural network model.\nTPC-H benchmarks. As we mentioned in Section 4.2,\nwe evaluated our readahead neural network model—trained\non RocksDB workloads—on TPC-H queries (both with\nNVMe-SSD). This intends to show the model’s accuracy\nlimitations when presented with vastly different workload\nand application combinations. Figure 12 shows performance\nimprovements as much as 39% for most query types. For query\n11, however, the readahead neural network failed to characterize\nthe workload correctly and resulted in a 53% performance\nreduction. Nevertheless, overall TPC-H performance still\nimproved by 6%. We expect that neural network models trained\non more traditional SQL database workloads would likely yield\neven better predictions across most similar databases.\n11\n\n/uni0000018c/uni0000011e/uni00000102/uni0000011a/uni00000190/uni0000011e/uni0000018b /uni0000018c/uni0000011e/uni00000102/uni0000011a/uni0000018c/uni00000102/uni00000176/uni0000011a/uni0000017d/uni00000175 /uni0000018c/uni0000011e/uni00000102/uni0000011a/uni0000018c/uni0000011e/uni000001c0/uni0000011e/uni0000018c/uni00000190/uni0000011e /uni0000018c/uni000001c1/uni00000372/uni0000018c/uni00000102/uni00000176/uni0000011a/uni0000017d/uni00000175 /uni00000175/uni0000015d/uni000001c6/uni00000150/uni0000018c/uni00000102/uni00000189/uni0000015a /uni000003ec/uni000003f1/uni000003ed/uni000003ec/uni000003ed/uni000003f1/uni000003ee/uni000003ec/uni00000057/uni0000011e/uni0000018c/uni00000128/uni0000017d/uni0000018c/uni00000175/uni00000102/uni00000176/uni00000110/uni0000011e/uni00000003/uni0000002f/uni00000175/uni00000189/uni0000018c/uni0000017d/uni000001c0/uni0000011e/uni00000175/uni0000011e/uni00000176/uni0000019a/uni00000003/uni0000037e/uni00000079/uni0000037f/uni000003ec/uni00000358/uni000003f3/uni000003f2/uni000003ee/uni00000358/uni000003ef/uni000003ee\n/uni000003ec/uni00000358/uni000003ee/uni000003ed/uni000003ee/uni00000358/uni000003ef/uni000003f5/uni000003ed/uni00000358/uni000003ef/uni000003f5/uni000003ec/uni00000358/uni000003f2/uni000003ef/uni000003ed/uni000003f1/uni00000358/uni000003f3/uni000003f4\n/uni000003ec/uni00000358/uni000003f0/uni000003f5/uni000003ed/uni000003f1/uni00000358/uni000003ef/uni000003f1 /uni000003ed/uni000003f1/uni00000358/uni000003ec/uni000003f0/uni0000005e/uni00000004/uni00000064/uni00000004/uni00000372/uni0000005e/uni0000005e/uni00000018 /uni00000045/uni00000073/uni00000044/uni0000011e/uni00000372/uni0000005e/uni0000005e/uni00000018Figure 13: Performance improvements ( \u0002) for RocksDB benchmarks\non SATA-SSD and NVMe-SSD devices across all six workloads running\non NFS, normalized to vanilla (1.0 \u0002).\n4.5 NFS Evaluation\nFigure 13 shows the NFS rsize neural network performance\nimprovements using the same evaluation techniques of readahead.\nThroughout these experiments, we ran multiple iterations of\nthe same workloads. Since rsize is a mount point parameter\nfor NFS, our NFS neural network can tune rsize values only\nin the beginning of the iteration. (We plan to fix the Linux\nkernel to permit rsize to change dynamically.) Hence, in\nsequential workloads, if the NFS neural network makes even\none misprediction, it will affect the entire iteration, leading to\nperformance degradation. Nevertheless, in random workload\ncases, we still measured around 15 \u0002performance improvement;\nin separate experiments (not shown for brevity), performance\nimprovements for random workloads reached up to 20 \u0002. This\ndemonstrates the significant potential of KML.\n5 Related Work\nMachine learning in systems and storage. In follow-up\nwork to Mittos [37], a custom neural network was built that\nmakes inferences inside the OS’s I/O scheduler queue. The neu-\nral network decides synchronously whether to submit requests\nto the device using binary classification [38]. There are notable\ndifferences between that system and our KML. That system\nwas trained offline using TensorFlow and exclusively trained\nin user space. Additionally, each of their two layers were custom\nbuilt. Conversely, KML provides a more flexible architecture.\nKML training, retraining, normalization, repeated inference—all\nare possible and accomplished with ease in any combination of\nonline, offline, synchronous, or asynchronous settings. Lastly,\nKML easily supports an arbitrary number of generalizable neural\nnetwork layers; our experiments demonstrate more expressive\nclassification abilities on a more diverse set of devices.\nLaga et al. [50] improved readahead performance in the\nLinux Kernel with Markov chain models, netting a 50%\nI/O performance improvement in TPC-H [87] queries on\nSA TA-SSDs. In contrast, our experiments ran on a wider\nselection of storage media (NVMe-SSD and SA TA-SSD) and\nworkloads. In TPC-H, we show improvements up to 39% despite\nTPC-H being a completely new workload for our readahead\nmodel. Moreover, our results illustrate that our readahead model\ncan improve I/O throughput by as much as 2.4 \u0002—all while\nkeeping memory consumption under 4KB, in comparison to\nLaga et al.’s much larger 94MB Markov chain model.\nParameter tuning for storage and operating systems has been achallenge and researchers approached this problem using control\ntheory [80] and data distribution analysis for storage clusters [2].\nSome research has attempted to apply ML techniques to OS\ntask scheduling [17, 65], with small reported performance im-\nprovements (0.1–6%). Nevertheless, it is becoming increasingly\npopular to apply ML techniques to storage and OS problems in-\ncluding: tuning SSD configurations [53], memory allocation [61],\nTCP congestion [30], building smart NICs [79], predicting index\nstructures in key-value stores [22,48], offline black-box storage\nparameter optimization [13], reconfigurable kernel datapaths [69],\nlocal and distributed caching [84,90], database query optimiza-\ntion [47], and cloud resource management [21,24,25,82].\nMachine learning libraries for resource-constraint systems.\nA myriad of ML libraries exist—some general purpose and oth-\ners more specialized. Popular general-purpose ML libraries in-\nclude Tensorflow [1], PyTorch [67], and CNTK [20]. Conversely,\nlibraries like ELL [31], Tensorflow Lite [85], SOD [81], and\nDlib [28] specialize to run on resource-constrained or on-device\nenvironments, KML differentiates itself by targeting OS-level ap-\nplications and designed for OS and storage systems specifically.\nInside the OS, resources are highly constrained, prediction accu-\nracy is vital, and even small data-path overheads are unacceptable.\nAdapting readahead and prefetching. Readahead\nand prefetching methods are both well-studied prob-\nlems [27, 49, 77, 78] and see use in distributed sys-\ntems [16, 18, 29, 52, 55, 56, 64, 86]. Many have attempted to\nbuild statistical models to optimize and tune systems [33,77,78].\nHowever, the main limitation of statistical models is their\ninability to adapt to novel new workloads and devices. We have\nshown that our model can adapt to never-before-seen workloads\nand devices. Another way to improve a readahead system is to\npredict individual I/O requests and file accesses by observing\nworkload patterns [6, 27, 40, 49, 88, 91, 94, 96]. Predicting file\naccesses using hand-crafted algorithms is a reasonable first\napproach. However, such manual labor simply cannot keep\nup with the diversity and complexity of ever-changing modern\nworkloads. Conversely, as long as we have training data, ML\nmodels can adapt, retrained as needed, and optimize much\nfaster. Simulations are also viable solutions for readahead and\nprefetching problems [15,34,71,95,99]. However, simulations\nare computationally expensive and are limited to the datasets\nthat the models are trained and tested with. Additionally, the\nmodels produced in simulations are not designed for resource-\nconstrained environments, making it non-trivial to migrate such\nmodels to the kernel. It is possible to use a user-space library\nto intercept file accesses [93] or to require application-level\nchanges [98]. In contrast, KML requires no application changes\nand is capable of intercepting mmap -based file accesses.\nFinally, while techniques exist to improve NFS performance,\nwe are unaware of automated ones that use ML [43].\n12\n\n6 Conclusion\nOperating systems and storage systems have to support many\never-changing workloads and devices. To provide the best per-\nformance, we have to configure storage system knobs based on\nworkloads’ needs and device characteristics. Unfortunately, cur-\nrent heuristics cannot adapt to workload changes quickly enough\nand require constant development efforts to support new devices.\nWe propose KML to solve these problems—an ML framework\ninside the OS that adapts quickly to optimize storage performance.\nKML enables finer granularity optimizations for individual\nfiles in even mixed workloads—a challenging problem. Our\npreliminary results show that, for a readahead problem, we can\nboost I/O throughput by up to 2.3 \u0002without imposing significant\nCPU/memory overheads. For the NFS rsize problem, the im-\nprovement was up to 15 \u0002. These I/O throughput improvements\nfar outweigh the small memory and CPU consumption of KML.\nFuture work. We plan on using KML to tune knobs for\nother OS subsystems: e.g., packet and I/O schedulers, and\nnetworking. We are adding ML techniques to KML, such as\nreinforcement learning [44], which can be a better fit for solving\ncertain OS problems. To support more advanced ML approaches\n(e.g., Recurrent Neural Networks (RNNs) [92]) and Long\nShort-Term Memory (LSTM) [39]), we are extending KML to\nsupport arbitrary computation DAGs. We also plan to integrate\nuser-kernel co-operated design into KML. Finally, loading an\nunverified ML model into a running kernel opens up new attack\nsurfaces. We are exploring known techniques to digitally sign\nand certify loadable models [46,59].\n7 Acknowledgments\nThis work was made possible in part thanks to Dell-EMC,\nNetApp, and IBM support; and NSF awards CCF-1918225,\nCNS-1900706, CNS-1729939, and CNS-1730726.\nReferences\n[1]Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng\nChen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay\nGhemawat, Geoffrey Irving, Michael Isard, Manjunath\nKudlur, Josh Levenberg, Rajat Monga, Sherry Moore,\nDerek Gordon Murray, Benoit Steiner, Paul A. Tucker,\nVijay V asudevan, Pete Warden, Martin Wicke, Y uan\nY u, and Xiaoqiang Zheng. TensorFlow: A system for\nlarge-scale machine learning. In 12th USENIX Symposium\non Operating Systems Design and Implementation (OSDI\n2016) , pages 265–283, Savannah, GA, November 2016.\n[2]Michael Abd-El-Malek, William V . Courtright II, Chuck\nCranor, Gregory R. Ganger, James Hendricks, Andrew J.\nKlosterman, Michael P . Mesnier, Manish Prasad, Brandon\nSalmon, Raja R. Sambasivan, Shafeeq Sinnamohideen,\nJohn D. Strunk, Eno Thereska, Matthew Wachs, and\nJay J. Wylie. Ursa minor: V ersatile cluster-based storage.\nInProceedings of the F AST ’05 Conference on Fileand Storage Technologies, December 13-16, 2005, San\nFrancisco, California, USA . USENIX, 2005.\n[3]Rishabh Agarwal, Nicholas Frosst, Xuezhou Zhang, Rich\nCaruana, and Geoffrey E Hinton. Neural additive models:\nInterpretable machine learning with neural nets. arXiv\npreprint arXiv:2004.13912 , 2020.\n[4]Ibrahim ’Umit’ Akgun, Ali Selman Aydin, Aadil Shaikh,\nLukas V elikov, and Erez Zadok. A machine learning\nframework to improve storage system performance. In\nProceedings of the 13th ACM Workshop on Hot Topics in\nStorage (HotStorage ’21) , Virtual, July 2021. ACM.\n[5]Ibrahim Umit Akgun, Geoff Kuenning, and Erez Zadok.\nRe-animator: V ersatile high-fidelity storage-system\ntracing and replaying. In Proceedings of the 13th ACM\nInternational Systems and Storage Conference (SYSTOR\n’20), Haifa, Israel, June 2020. ACM.\n[6]Ahmed Amer, Darrell DE Long, J-F Pâris, and Randal C\nBurns. File access prediction with adjustable accuracy. In\nConference Proceedings of the IEEE International Perfor-\nmance, Computing, and Communications Conference (Cat.\nNo. 02CH37326) , pages 131–140. IEEE, 2002.\n[7]George Amvrosiadis, Ali R. Butt, V asily Tarasov, Erez\nZadok, Ming Zhao, Irfan Ahmad, Remzi H. Arpaci-\nDusseau, Feng Chen, Yiran Chen, Y ong Chen, Y ue\nCheng, Vijay Chidambaram, Dilma Da Silva, Angela\nDemke-Brown, Peter Desnoyers, Jason Flinn, Xubin He,\nSong Jiang, Geoff Kuenning, Min Li, Carlos Maltzahn,\nEthan L. Miller, Kathryn Mohror, Raju Rangaswami,\nNarasimha Reddy, David Rosenthal, Ali Saman Tosun,\nNisha Talagala, Peter V arman, Sudharshan V azhkudai,\nAvani Waldani, Xiaodong Zhang, Yiying Zhang, and\nMai Zheng. Data storage research vision 2025: Report\non NSF visioning workshop held may 30–june 1, 2018.\nTechnical report, National Science Foundation, February\n2019. https://dl.acm.org/citation.cfm?id=3316807 .\n[8]Y oshua Bengio. Practical recommendations for gradient-\nbased training of deep architectures. In Neural Networks:\nTricks of the Trade , pages 437–478. Springer, 2012.\n[9]Leo Breiman. Random forests. Machine learning ,\n45(1):5–32, 2001.\n[10] Zhen Cao, Geoff Kuenning, and Erez Zadok. Carver:\nFinding important parameters for storage system tuning.\nInProceedings of the 18th USENIX Conference on File\nand Storage Technologies (F AST ’20) , Santa Clara, CA,\nFebruary 2020. USENIX Association.\n[11] Zhen Cao, V asily Tarasov, Hari Raman, Dean Hildebrand,\nand Erez Zadok. On the performance variation in modern\nstorage stacks. In Proceedings of the 15th USENIX\nConference on File and Storage Technologies (F AST ’17) ,\npages 329–343, Santa Clara, CA, February-March 2017.\nUSENIX Association.\n13\n\n[12] Zhen Cao, V asily Tarasov, Sachin Tiwari, and Erez Zadok.\nTowards better understanding of black-box auto-tuning: A\ncomparative analysis for storage systems. In Proceedings\nof the Annual USENIX Technical Conference , Boston, MA,\nJuly 2018. USENIX Association. Data set at http://download.\nfilesystems.org/auto-tune/A TC-2018-auto-tune-data.sql.gz .\n[13] Zhen Cao, V asily Tarasov, Sachin Tiwari, and Erez Zadok.\nTowards better understanding of black-box auto-tuning:\nA comparative analysis for storage systems. In USENIX\nAnnual Technical Conference, (ATC) , pages 893–907,\nBoston, MA, July 2018.\n[14] Zhichao Cao, Siying Dong, Sagar V emuri, and David HC\nDu. Characterizing, modeling, and benchmarking\nRocksDB key-value workloads at Facebook. In 18th\nUSENIX Conference on File and Storage Technologies\n(F AST) , pages 209–223, 2020.\n[15] Chandranil Chakraborttii and Heiner Litz. Learning i/o ac-\ncess patterns to improve prefetching in ssds. ICML-PKDD ,\n2020.\n[16] Hui Chen, Enqiang Zhou, Jie Liu, and Zhicheng Zhang.\nAn rnn based mechanism for file prefetching. In 2019\n18th International Symposium on Distributed Computing\nand Applications for Business Engineering and Science\n(DCABES) , pages 13–16. IEEE, 2019.\n[17] Jingde Chen, Subho S. Banerjee, Zbigniew T. Kalbarczyk,\nand Ravishankar K. Iyer. Machine learning for load\nbalancing in the linux kernel. In Proceedings of the 11th\nACM SIGOPS Asia-Pacific Workshop on Systems , APSys\n’20, Tsukuba, Japan, 2020. Association for Computing\nMachinery.\n[18] Giovanni Cherubini, Y usik Kim, Mark Lantz, and Vinodh\nV enkatesan. Data prefetching for large tiered storage\nsystems. In 2017 IEEE International Conference on Data\nMining (ICDM) , pages 823–828, November 2017.\n[19] Jungwook Choi, Swagath V enkataramani, Vijayalakshmi\nSrinivasan, Kailash Gopalakrishnan, Zhuo Wang, and\nPierce Chuang. Accurate and efficient 2-bit quantized\nneural networks. In Proceedings of the 2nd SysML\nConference , 2019.\n[20] CNTK, September 2020. https://github.com/microsoft/\nCNTK .\n[21] Eli Cortez, Anand Bonde, Alexandre Muzio, Mark Russi-\nnovich, Marcus Fontoura, and Ricardo Bianchini. Resource\ncentral: Understanding and predicting workloads for\nimproved resource management in large cloud platforms.\nInProceedings of the 26th Symposium on Operating\nSystems Principles , pages 153–167, Shanghai, China, 2017.\n[22] Yifan Dai, Yien Xu, Aishwarya Ganesan, Ramnatthan Ala-\ngappan, Brian Kroth, Andrea Arpaci-Dusseau, and Remzi\nArpaci-Dusseau. From WiscKey to bourbon: A learnedindex for log-structured merge trees. In 14th USENIX Sym-\nposium on Operating Systems Design and Implementation\n(OSDI) . USENIX Association, November 2020.\n[23] Christopher De Sa, Megan Leszczynski, Jian Zhang, Alana\nMarzoev, Christopher R. Aberger, Kunle Olukotun, and\nChristopher Ré. High-accuracy low-precision training,\n2018. arXiv preprint arXiv:1803.03383.\n[24] Christina Delimitrou and Christos Kozyrakis. Paragon:\nQos-aware scheduling for heterogeneous datacenters.\nACM SIGPLAN Notices , 48(4):77–88, 2013.\n[25] Christina Delimitrou and Christos Kozyrakis. Quasar:\nResource-efficient and qos-aware cluster management.\nACM SIGPLAN Notices , 49(4):127–144, 2014.\n[26] Mathieu Desnoyers. Using the Linux kernel tracepoints,\n2016. https://www.kernel.org/doc/Documentation/trace/\ntracepoints.txt .\n[27] Xiaoning Ding, Song Jiang, Feng Chen, Kei Davis, and\nXiaodong Zhang. DiskSeen: Exploiting disk layout and\naccess history to enhance I/O prefetch. In USENIX Annual\nTechnical Conference , pages 261–274, 2007.\n[28] dlib C++ Library, September 2020. http://dlib.net/ .\n[29] Bo Dong, Xiao Zhong, Qinghua Zheng, Lirong Jian,\nJian Liu, Jie Qiu, and Ying Li. Correlation based file\nprefetching approach for hadoop. In 2010 IEEE Second\nInternational Conference on Cloud Computing Technology\nand Science , pages 41–48. IEEE, 2010.\n[30] Mo Dong, Tong Meng, Doron Zarchy, Engin Arslan,\nY ossi Gilad, Brighten Godfrey, and Michael Schapira.\nPCC vivace: Online-learning congestion control. In 15th\nUSENIX Symposium on Networked Systems Design and\nImplementation (NSDI 18) , pages 343–356, 2018.\n[31] Embedded Learning Library (ELL), January 2020.\nhttps://microsoft.github.io/ELL/ .\n[32] Facebook. RocksDB. https://rocksdb.org/ , September 2019.\n[33] Cory Fox, Dragan Lojpur, and An-I Andy Wang. Quan-\ntifying temporal and spatial localities in storage workloads\nand transformations by data path components. In 2008\nIEEE International Symposium on Modeling, Analysis and\nSimulation of Computers and Telecommunication Systems ,\npages 1–10. IEEE, 2008.\n[34] Gaddisa Olani Ganfure, Chun-Feng Wu, Y uan-Hao\nChang, and Wei-Kuan Shih. Deepprefetcher: A deep\nlearning framework for data prefetching in flash storage\ndevices. IEEE Transactions on Computer-Aided Design of\nIntegrated Circuits and Systems , 39(11):3311–3322, 2020.\n[35] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan,\nand Pritish Narayanan. Deep learning with limited\nnumerical precision. In Proceedings of the 32nd Interna-\ntional Conference on Machine Learning (ICML) , pages\n1737–1746, Lille, France, 2015.\n14\n\n[36] Lawrence O Hall, Xiaomei Liu, Kevin W Bowyer, and\nRobert Banfield. Why are neural networks sometimes\nmuch more accurate than decision trees: an analysis\non a bio-informatics problem. In SMC’03 Conference\nProceedings. 2003 IEEE International Conference on\nSystems, Man and Cybernetics. Conference Theme-System\nSecurity and Assurance (Cat. No. 03CH37483) , volume 3,\npages 2851–2856. IEEE, 2003.\n[37] Mingzhe Hao, Huaicheng Li, Michael Hao Tong, Chrisma\nPakha, Riza O. Suminto, Cesar A. Stuardo, Andrew A.\nChien, and Haryadi S. Gunawi. MittOS: Supporting\nmillisecond tail tolerance with fast rejecting SLO-aware\nOS interface. In Proceedings of the 26th Symposium on\nOperating Systems Principles , pages 168–183, Shanghai,\nChina, October 2017.\n[38] Mingzhe Hao, Levent Toksoz, Nanqinqin Li, Edward\nEdberg, Henry Hoffmann, and Haryadi S. Gunawi.\nLinnOS: Predictability on unpredictable flash storage. In\n14th USENIX Symposium on Operating Systems Design\nand Implementation (OSDI) , Banff, Alberta, November\n2020. USENIX Association.\n[39] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term\nmemory. Neural computation , 9(8):1735–1780, 1997.\n[40] Haiyan Hu, Yi Liu, and Depei Qian. I/o feature-based\nfile prefetching for multi-applications. In 2010 Ninth\nInternational Conference on Grid and Cloud Computing ,\npages 213–217. IEEE, 2010.\n[41] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran\nEl-Yaniv, and Y oshua Bengio. Quantized neural networks:\nTraining neural networks with low precision weights and\nactivations. The Journal of Machine Learning Research ,\n18(1):6869–6898, 2017.\n[42] Jeya Vikranth Jeyakumar, Joseph Noor, Y u-Hsi Cheng,\nLuis Garcia, and Mani Srivastava. How can i explain\nthis to you? an empirical study of deep neural network\nexplanation methods. Advances in Neural Information\nProcessing Systems , 2020.\n[43] Chet Juszczak. Improving the write performance of\nan NFS server. In Proceedings of the USENIX Winter\n1994 Technical Conference , WTEC’94, San Francisco,\nCalifornia, 1994. USENIX Association.\n[44] Leslie Pack Kaelbling, Michael L. Littman, and Andrew W.\nMoore. Reinforcement learning: a survey. Journal of\nArtificial Intelligence Research , pages 237–285, 1996.\n[45] Jack Kiefer, Jacob Wolfowitz, et al. Stochastic estimation\nof the maximum of a regression function. The Annals of\nMathematical Statistics , 23(3):462–466, 1952.\n[46] Doowon Kim, Bum Jun Kwon, Kristián Kozák, Christo-\npher Gates, and Tudor Dumitras. The broken shield:\nMeasuring revocation effectiveness in the windowscode-signing PKI. In 27th USENIX Security Symposium\n(USENIX Security 18) , pages 851–868, 2018.\n[47] Tim Kraska, Mohammad Alizadeh, Alex Beutel, Ed H. Chi,\nAni Kristo, Guillaume Leclerc, Samuel Madden, Hongzi\nMao, and Vikram Nathan. SageDB: A learned database\nsystem. In 9th Biennial Conference on Innovative Data\nSystems Research (CIDR) , Asilomar, CA, January 2019.\n[48] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and\nNeoklis Polyzotis. The case for learned index structures.\nInProceedings of the 2018 International Conference on\nManagement of Data , pages 489–504. ACM, 2018.\n[49] Thomas M. Kroeger and Darrell D. E. Long. Design and\nimplementation of a predictive file prefetching algorithm.\nInUSENIX Annual Technical Conference , pages 105–118,\nBoston, MA, June 2001.\n[50] Arezki Laga, Jalil Boukhobza, M. Koskas, and Frank\nSinghoff. Lynx: A learning Linux prefetching mechanism\nfor SSD performance model. In 5th Non-V olatile Memory\nSystems and Applications Symposium (NVMSA) , pages\n1–6, August 2016.\n[51] Liangzhen Lai, Naveen Suda, and Vikas Chandra. Deep\nconvolutional neural network inference with floating-point\nweights and fixed-point activations, 2017. arXiv preprint\narXiv:1703.03073.\n[52] Sangmin Lee, Soon J Hyun, Hong-Yeon Kim, and\nY oung-Kyun Kim. Aps: adaptable prefetching scheme to\ndifferent running environments for concurrent read streams\nin distributed file systems. The Journal of Supercomputing ,\n74(6):2870–2902, 2018.\n[53] Daixuan Li and Jian Huang. A learning-based approach\ntowards automated tuning of ssd configurations. arXiv\npreprint arXiv:2110.08685 , 2021.\n[54] Z. Li, A. Mukker, and E. Zadok. On the importance of\nevaluating storage systems’ $costs. In Proceedings of the\n6th USENIX Conference on Hot Topics in Storage and File\nSystems , HotStorage’14, 2014.\n[55] Shuang Liang, Song Jiang, and Xiaodong Zhang. Step:\nSequentiality and thrashing detection based prefetching\nto improve performance of networked storage servers. In\n27th International Conference on Distributed Computing\nSystems (ICDCS’07) , pages 64–64. IEEE, 2007.\n[56] Jianwei Liao, Francois Trahay, Guoqiang Xiao, Li Li, and\nY utaka Ishikawa. Performing initiative data prefetching\nin distributed file systems for cloud computing. IEEE\nTransactions on cloud computing , 5(3):550–562, 2015.\n[57] Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz,\nJoseph E Gonzalez, and Ion Stoica. Tune: A research\nplatform for distributed model selection and training. arXiv\npreprint arXiv:1807.05118 , 2018.\n[58] Darryl D. Lin, Sachin S. Talathi, and V . Sreekanth Anna-\npureddy. Fixed point quantization of deep convolutional\n15\n\nnetworks. In International Conference on Machine\nLearning , pages 2849–2858, June 2016.\n[59] Linux. Linux kernel module signing facility.\nhttps://www.kernel.org/doc/html/v4.19/admin-guide/\nmodule-signing.html?highlight=signing , January 2021.\n[60] LTTng. LTTng: an open source tracing framework for\nLinux. https://lttng.org , April 2019.\n[61] Martin Maas, David G. Andersen, Michael Isard, Moham-\nmad Mahdi Javanmard, Kathryn S. McKinley, and Colin\nRaffel. Learning-based memory allocation for C++ server\nworkloads. In Proceedings of the Twenty-Fifth International\nConference on Architectural Support for Programming\nLanguages and Operating Systems (ASPLOS) , pages\n541–556, Lausanne, Switzerland, March 2020.\n[62] Paul Manning. Best practices for running vmware vsphere\non network attached storage. https://www.vmware.com/\ncontent/dam/digitalmarketing/vmware/en/pdf/techpaper/\nvmware-nfs-bestpractices-white-paper-en.pdf , 2009.\n[63] Vinod Nair and Geoffrey E. Hinton. Rectified linear units\nimprove restricted boltzmann machines. In Proceedings\nof the 27th International Conference on Machine Learning\n(ICML-10) , pages 807–814, Haifa, Israel, June 2010.\n[64] Anusha Nalajala, T Ragunathan, Sri Harsha Tavidisetty\nRajendra, Nagamlla V enkata Sai Nikhith, and Rathnamma\nGopisetty. Improving performance of distributed file\nsystem through frequent block access pattern-based\nprefetching algorithm. In 2019 10th International Con-\nference on Computing, Communication and Networking\nTechnologies (ICCCNT) , pages 1–7. IEEE, 2019.\n[65] Atul Negi and P Kishore Kumar. Applying machine\nlearning techniques to improve Linux process scheduling.\nInTENCON 2005-2005 IEEE Region 10 Conference ,\npages 1–6. IEEE, 2005.\n[66] Oracle Corporation. MySQL. http://www.mysql.com , May\n2020.\n[67] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\nAndreas Köpf, Edward Yang, Zachary DeVito, Martin\nRaison, Alykhan Tejani, Sasank Chilamkurthy, Benoit\nSteiner, Lu Fang, Junjie Bai, and Soumith Chintala.\nPyTorch: An imperative style, high-performance deep\nlearning library. In Advances in Neural Information\nProcessing Systems 32: Annual Conference on Neural\nInformation Processing Systems (NeurIPS 2019) , pages\n8024–8035, V ancouver, BC, Canada, December 2019.\n[68] Karl Pearson. Note on regression and inheritance in the\ncase of two parents. Proceedings of the Royal Society of\nLondon , 58(347-352):240–242, 1895.\n[69] Yiming Qiu, Hongyi Liu, Thomas Anderson, Yingyan Lin,\nand Ang Chen. Toward reconfigurable kernel datapathswith learned optimizations. In Proceedings of the Workshop\non Hot Topics in Operating Systems , pages 175–182, 2021.\n[70] Gabriëlle Ras, Marcel van Gerven, and Pim Haselager.\nExplanation methods in deep learning: Users, values,\nconcerns and challenges. In Explainable and interpretable\nmodels in computer vision and machine learning , pages\n19–36. Springer, 2018.\n[71] Natarajan Ravichandran and Jehan-François Pâris. Making\nearly predictions of file accesses . PhD thesis, University\nof Houston, 2005.\n[72] Herbert Robbins and Sutton Monro. A stochastic\napproximation method. The annals of mathematical\nstatistics , pages 400–407, 1951.\n[73] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.\nWilliams. Learning representations by back-propagating\nerrors. Nature , 323(6088):533–536, 1986.\n[74] Christopher De Sa, Matthew Feldman, Christopher Ré,\nand Kunle Olukotun. Understanding and optimizing\nasynchronous low-precision stochastic gradient descent. In\nProceedings of the 44th Annual International Symposium\non Computer Architecture, (ISCA) , pages 561–574,\nToronto, ON, Canada, June 2017.\n[75] Wojciech Samek, Grégoire Montavon, Sebastian La-\npuschkin, Christopher J Anders, and Klaus-Robert Müller.\nToward interpretable machine learning: Transparent\ndeep neural networks and beyond. arXiv e-prints , pages\narXiv–2003, 2020.\n[76] Priya Sehgal, V asily Tarasov, and Erez Zadok. Evaluating\nperformance and energy in file system server workloads.\nInProceedings of the USENIX Conference on File and\nStorage Technologies (F AST ’10) , pages 253–266, San Jose,\nCA, February 2010. USENIX Association.\n[77] Elizabeth Shriver, Arif Merchant, and John Wilkes. An\nanalytic behavior model for disk drives with readahead\ncaches and request reordering. In SIGMETRICS , June 1998.\n[78] Elizabeth AM Shriver, Christopher Small, and Keith A\nSmith. Why does file system prefetching work? In\nUSENIX Annual Technical Conference, General Track ,\npages 71–84, 1999.\n[79] Giuseppe Siracusano, Salvator Galea, Davide Sanvito,\nMohammad Malekzadeh, Hamed Haddadi, Gianni Antichi,\nand Roberto Bifulco. Running neural networks on the nic.\narXiv preprint arXiv:2009.02353 , 2020.\n[80] Filippo Sironi, Davide B Bartolini, Simone Campanoni,\nFabio Cancare, Henry Hoffmann, Donatella Sciuto,\nand Marco D Santambrogio. Metronome: operating\nsystem level performance management via self-adaptive\ncomputing. In Proceedings of the 49th Annual Design\nAutomation Conference , pages 856–865, 2012.\n16\n\n[81] SOD - An Embedded, Modern Computer Vision\nand Machine Learning Library, September 2020.\nhttps://sod.pixlab.io/ .\n[82] Gagan Somashekar and Anshul Gandhi. Towards optimal\nconfiguration of microservices. In Proceedings of the 1st\nWorkshop on Machine Learning and Systems , pages 7–14,\n2021.\n[83] Kalyanasundaram Somasundaram. The impact of slow nfs\non data systems. https://engineering.linkedin.com/blog/2020/\nthe-impact-of-slow-nfs-on-data-systems , June 2020.\n[84] Pradeep Subedi, Philip Davis, Shaohua Duan, Scott\nKlasky, Hemanth Kolla, and Manish Parashar. Stacker: An\nautonomic data movement engine for extreme-scale data\nstaging-based in-situ workflows. In SC18: International\nConference for High Performance Computing, Networking,\nStorage and Analysis , pages 920–930. IEEE, 2018.\n[85] TensorFlow lite, January 2020. https://www.tensorflow.org/\nlite.\n[86] Nancy Tran and Daniel A Reed. Automatic arima time\nseries modeling for adaptive i/o prefetching. IEEE Trans-\nactions on parallel and distributed systems , 15(4):362–377,\n2004.\n[87] Transaction Processing Performance Council. TPC\nbenchmark H (decision support). www.tpc.org/tpch , 1999.\n[88] Ahsen J Uppal, Ron C Chiang, and H Howie Huang.\nFlashy prefetching for high-performance flash drives. In\n2012 IEEE 28th Symposium on Mass Storage Systems and\nTechnologies (MSST) , pages 1–12. IEEE, 2012.\n[89] Laurens van der Maaten and Geoffrey Hinton. Visualizing\ndata using t-sne. Journal of Machine Learning Research ,\n9(86):2579–2605, 2008.\n[90] Giuseppe Vietri, Liana V . Rodriguez, Wendy A. Martinez,\nSteven Lyons, Jason Liu, Raju Rangaswami, Ming Zhao,\nand Giri Narasimhan. Driving cache replacement with\nML-based LeCaR. In Proceedings of the 10th USENIX\nWorkshop on Hot Topics in Storage (HotStorage ’18) ,\nBoston, MA, July 2018. USENIX.\n[91] Gary AS Whittle, J-F Pâris, Ahmed Amer, Darrell DE\nLong, and Randal Burns. Using multiple predictors\nto improve the accuracy of file access predictions. In\n20th IEEE/11th NASA Goddard Conference on Mass\nStorage Systems and Technologies, 2003.(MSST 2003).\nProceedings. , pages 230–240. IEEE, 2003.\n[92] Wikipedia. Recurrent neural network. https:\n//en.wikipedia.org/wiki/Recurrent_neural_network .\n[93] Jiwoong Won, Oseok Kwon, Junhee Ryu, Dongeun Lee,\nand Kyungtae Kang. ifetcher: User-level prefetching\nframework with file-system event monitoring for linux.\nIEEE Access , 6:46213–46226, 2018.[94] Fengguang Wu, Hongsheng Xi, and Chenfeng Xu. On the\ndesign of a new Linux readahead framework. Operating\nSystems Review , 42:75–84, 2008.\n[95] Chenfeng Xu, Hongsheng Xi, and Fengguang Wu.\nEvaluation and optimization of kernel file readaheads\nbased on markov decision models. The Computer Journal ,\n54(11):1741–1755, 2011.\n[96] Xiaofei Xu, Zhigang Cai, Jianwei Liao, and Y utaka\nIshiakwa. Frequent access pattern-based prefetching inside\nof solid-state drives. In 2020 Design, Automation & Test in\nEurope Conference & Exhibition (DATE) , pages 720–725.\nIEEE, 2020.\n[97] Gala Yadgar, MOSHE Gabel, Shehbaz Jaffer, and Bianca\nSchroeder. Ssd-based workload characteristics and their\nperformance implications. ACM Transactions on Storage\n(TOS) , 17(1):1–26, 2021.\n[98] Chuan-Kai Yang, Tulika Mitra, and Tzi-cker Chiueh.\nA decoupled architecture for application-specific file\nprefetching. In Chris G. Demetriou, editor, Proceedings\nof the FREENIX Track: 2002 USENIX Annual Technical\nConference, June 10-15, 2002, Monterey, California, USA ,\npages 157–170. USENIX, 2002.\n[99] Shengan Zheng, Hong Mei, Linpeng Huang, Yanyan Shen,\nand Yanmin Zhu. Adaptive prefetching for accelerating\nread and write in nvm-based file systems. In 2017 IEEE\nInternational Conference on Computer Design (ICCD) ,\npages 49–56. IEEE, 2017.\n17",
  "textLength": 100180
}