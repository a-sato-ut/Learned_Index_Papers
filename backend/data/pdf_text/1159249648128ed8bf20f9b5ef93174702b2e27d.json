{
  "paperId": "1159249648128ed8bf20f9b5ef93174702b2e27d",
  "title": "Learning-Augmented Frequent Directions",
  "pdfPath": "1159249648128ed8bf20f9b5ef93174702b2e27d.pdf",
  "text": "Published as a conference paper at ICLR 2025\nLEARNING -AUGMENTED FREQUENT DIRECTIONS\nAnders Aamand\nUniversity of Copenhagen\nandersaamanda@gmail.comJustin Y. Chen\nMIT\njustc@mit.eduSiddharth Gollapudi\nIndependent\nsgollapu@berkeley.edu\nSandeep Silwal\nUW-Madison\nsilwal@cs.wisc.eduHao WU\nUniversity of Waterloo\nhao.wu1@uwaterloo.ca\nABSTRACT\nAn influential paper of Hsu et al. (ICLR’19) introduced the study of learning-augmented stream-\ning algorithms in the context of frequency estimation. A fundamental problem in the streaming\nliterature, the goal of frequency estimation is to approximate the number of occurrences of items\nappearing in a long stream of data using only a small amount of memory. Hsu et al. develop a\nnatural framework to combine the worst-case guarantees of popular solutions such as CountMin\nand CountSketch with learned predictions of high frequency elements. They demonstrate that\nlearning the underlying structure of data can be used to yield better streaming algorithms, both\nin theory and practice.\nWe simplify and generalize past work on learning-augmented frequency estimation. Our first\ncontribution is a learning-augmented variant of the Misra-Gries algorithm which improves upon\nthe error of learned CountMin and learned CountSketch and achieves the state-of-the-art perfor-\nmance of randomized algorithms (Aamand et al., NeurIPS’23) with a simpler, deterministic al-\ngorithm. Our second contribution is to adapt learning-augmentation to a high-dimensional gen-\neralization of frequency estimation corresponding to finding important directions (top singular\nvectors) of a matrix given its rows one-by-one in a stream. We analyze a learning-augmented\nvariant of the Frequent Directions algorithm, extending the theoretical and empirical under-\nstanding of learned predictions to matrix streaming.\n1 INTRODUCTION\nLearning-augmented algorithms combine the worst-case analysis of traditional algorithm design\nwith machine learning to exploit structure in the specific inputs on which the algorithm is deployed.\nA burgeoning line of work in this context has studied algorithms furnished with predictions given\nby domain experts or learned from past data. This general methodology has been applied to create\ninput-optimized data structures (Kraska et al., 2018; Mitzenmacher, 2018), graph algorithms (Dinitz\net al., 2021; Chen et al., 2022c), online algorithms (Lykouris & Vassilvitskii, 2021; Gollapudi &\nPanigrahi, 2019), streaming algorithms (Hsu et al., 2019; Jiang et al., 2020; Chen et al., 2022a)\namong many other applications1. Within the context of streaming algorithms, where the input arrives\nin an online fashion and the algorithm has too little memory to store everything, predictors can\nhighlight data which are worth remembering. This intuition was formalized in an influential work\nof Hsu et al. (2019) in the context of frequency estimation, a fundamental streaming problem where\nthe goal is to provide an estimate of how many times any element appeared in the stream.\nGiven access to a heavy-hitter oracle identifying the highest frequency elements, Hsu et al. (2019)\ngive a natural framework where the heavy-hitters are counted exactly while the rest of the frequen-\ncies are approximated using standard algorithms such as CountMin (Cormode & Muthukrishnan,\n2005) or CountSketch (Charikar et al., 2002). They study the setting where the frequencies follow a\npower law distribution, commonly seen in practice and therefore well-studied for frequency estima-\ntion (Cormode & Muthukrishnan, 2005; Metwally et al., 2005; Minton & Price, 2012). Given access\n1There are hundreds of papers written on this topics. See the survey of Mitzenmacher & Vassilvitskii (2022)\nor the website https://algorithms-with-predictions.github.io/ .\n1arXiv:2503.00937v1  [cs.LG]  2 Mar 2025\n\nPublished as a conference paper at ICLR 2025\nto an oracle which can recover the heaviest elements, they give improved error bounds where error is\ntaken in expectation over the empirical distribution of frequencies. A sequence of follow-up works\ninvestigate how to learn good predictors (Du et al., 2021; Chen et al., 2022b), apply the results to\nother streaming models (Shahout et al., 2024), and give improved algorithms (Aamand et al., 2023).\nAlgorithms Weighted Error Predictions ? Analysis\nFrequent Direction Θ\n\u0012\nlnm\nln2d\nm\u0013\n·lnd\nm\n(lnd)2·∥A∥2\nF\nm\n No Theorem 3.3\nLearned Frequent Direction Θ\u0010\n1\n(lnd)2·∥A∥2\nF\nm\u0011\nYes Theorem 3.4\nTable 1: Error bounds for Frequent Directions with ninput vectors from the domain Rdusing m×d\nwords of memory, assuming that the matrix consisting of input vectors has singular value σ2\ni∝1/i.\nThe weighted error is defined by Equation (2).\nIn this work, we define and analyze the corresponding problem in the setting where each data point\nis a vector rather than an integer, and the goal is to find frequent directions rather than elements (cap-\nturing low-rank structure in the space spanned by the vectors). This setting of “matrix streaming”\nis an important tool in big data applications including image analysis, text processing, and numer-\nical linear algebra. Low-rank approximations via SVD/PCA are ubiquitous in these applications,\nand streaming algorithms for this problem allow for memory-efficient estimation of these approxi-\nmations. In the matrix context, we define a corresponding notion of expected error and power-law\ndistributed data. We develop a learning-augmented streaming algorithm for the problem based on\nthe Frequent Directions (FD) algorithm (Ghashami et al., 2016) and give a detailed theoretical anal-\nysis on the space/error tradeoffs of our algorithm given predictions of the important directions of the\ninput matrix. Our framework captures and significantly generalizes that of frequency estimation in\none dimension. When the input vectors are basis vectors, our algorithm corresponds to a learning-\naugmented version of the popular Misra-Gries (Misra & Gries, 1982) heavy hitters algorithm. In this\nspecial case of our model, our algorithm achieves state-of-the-art bounds for learning-augmented\nfrequency estimation, matching that of Aamand et al. (2023). In contrast to prior work, we achieve\nthis performance without specializing our algorithm for power-law distributed data.\nWe experimentally verify the performance of our learning-augmented algorithms on real data. Fol-\nlowing prior work, we consider datasets containing numerous problem instances in a temporal order\n(each instance being either a sequence of items for frequency estimation or a sequence of matrices\nfor matrix streaming). Using predictions trained on past data, we demonstrate the power of incorpo-\nrating learned structure in our algorithms, achieving state-of-the-art performance in both settings.\nOur Contributions\n• We generalize the learning-augmented frequency estimation model to the matrix streaming setting:\neach stream element is a row vector of a matrix A. We define corresponding notions of expected\nerror and power-law distributed data with respect to the singular vectors and values of A.\n• In this setting, we develop and analyze a learning-augmented version of the Frequent Directions\nalgorithm of Ghashami et al. (2016). Given predictions of the important directions (correspond-\ning to the top right singular vectors of A), we demonstrate an asymptotically better space/error\ntradeoff than the base algorithm without learning. See Table 1.\n• As a special case of our setting and a corollary of our analysis, we bound the performance of\nlearning-augmented Misra-Gries for frequency estimation. In the learning-augmented setting,\npast work has analyzed randomized algorithms CountMin and CountSketch as well as specialized\nvariants (Hsu et al., 2019; Aamand et al., 2023). To our knowledge, no analysis has been done\nprior to our work for the popular, deterministic Misra-Gries algorithm. Our analysis shows that\nlearned Misra-Gries achieves state-of-the-art learning-augmented frequency estimation bounds,\nwithout randomness or specializing the algorithm for Zipfian data. See Table 2.\n• We empirically validate our theoretical results via experiments on real data. For matrix streaming,\nour learning-augmented Frequent Directions algorithm outperforms the non-learned version by 1-\n2 orders of magnitude on all datasets. For frequency estimation, our learned Misra-Gries algorithm\nachieves superior or competitive performance against the baselines.\n2\n\nPublished as a conference paper at ICLR 2025\nAlgorithm Weighted Error Rand ?Pred ? Reference\nCountMin Θ\u0000n\nm\u0001\nYes No (Aamand et al., 2023)\nLearned CountMin Θ\u0012\n(lnd\nm)2\nmn\n(lnd)2\u0013\nYes Yes (Hsu et al., 2019)\nCountSketch O\u00001\nmn\nlnd\u0001\nYes No (Aamand et al., 2023)\nLearned CountSketch O\u0010lnd\nm\nmn\n(lnd)2\u0011\nYes Yes (Aamand et al., 2023)\nCountSketch++ O\u0010\nlnm+poly(ln ln d)\nmn\n(lnd)2\u0011\nYes No (Aamand et al., 2023)\nLearned CountSketch ++ O\u0010\n1\nmn\n(lnd)2\u0011\nYes Yes (Aamand et al., 2023)\nMisra-Gries Θ\u0010\u0010\nlnm\nln2d\nm\u0011lnd\nm\nmn\n(lnd)2\u0011\nNo No Theorem 3.1\nLearned Misra-Gries Θ\u0010\n1\nmn\n(lnd)2\u0011\nNo Yes Theorem 3.2\nTable 2: Error bounds for frequency estimation with ninput elements from the domain [d]using m\nwords of memory, assuming that the frequency of element i∈[d]follows f(i)∝1/i. The weighted\nerror indicates that element iis queried with a probability proportional to 1/i.\nRelated Work The learning-augmented frequency estimation problem was introduced in Hsu et al.\n(2019). They suggest the model of predicted frequencies and give the first analysis of learning-\naugmented CountMin and CountSketch with weighted error and Zipfian frequencies. Du et al.\n(2021) evaluate several choices for the loss functions to use to learn the frequency predictor and\nChen et al. (2022b) develop a procedure to learn a good predictor itself with a streaming algorithm.\nShahout et al. (2024) extend the model to sliding window streams where frequency estimation is re-\nstricted to recently appearing items. Shahout & Mitzenmacher (2024) analyze a learning-augmented\nversion of the SpaceSaving algorithm (Metwally et al., 2005) which is a deterministic algorithm for\nfrequency estimation, but, unlike our work, they do not give space/error tradeoffs comparable to\nHsu et al. (2019). Aamand et al. (2023) give tight analysis for CountMin and CountSketch both with\nand without learned predictions in the setting of weighted error with Zipfian data. Furthermore,\nthey develop a new algorithm based on the CountSketch, which we refer to as learning-augmented\nCountSketch++, which has better asymptotic and empirical performance.\nMatrix sketching and low-rank approximations are ubiquitous in machine learning. The line of work\nmost pertinent to our work is that on matrix streaming where rows arrive one-by-one, and in small\nspace, the goal is to maintain a low-rank approximation of the full matrix. The Frequent Directions\nalgorithm for the matrix streaming problem was introduced by Liberty (2013). Subsequent work of\nGhashami & Phillips (2014a) and Woodruff (2014) refined the analysis and gave a matching lower\nbound. These works were joined and developed in Ghashami et al. (2016) with an even simpler\nanalysis given by Liberty (2022).\nA related line of work is on learning sketching matrices for low-rank approximation, studied in\nIndyk et al. (2019; 2021). Their goal is to learn a sketching matrix Swith few rows so that the\nlow-rank approximation of Acan be recovered from SA. The main guarantee is that the classical\nlow-rank approximation algorithm of Clarkson & Woodruff (2013), which uses a random S, can be\naugmented so that only half of its rows are random, while retaining worst-case error. The learned\nhalf of Scan be optimized empirically, leading to a small sketch SAin practice. The difference\nbetween these works and us is that their overall procedure cannot be implemented in a single pass\nover the stream. We discuss other related works in Appendix A.\nOrganization Section 2 delves into the necessary preliminaries for our algorithm. We define the\nproblems of frequency estimation, and its natural higher dimensional version, introduce our notion of\nestimation error for these problems, and discuss the two related algorithms Misra-Gries and Frequent\nDirections for these problems. In Section 3, we introduce our learning-augmented versions of Misra-\nGries and Frequent Directions. We also analyse the performance of learned Misra-Gries algorithms,\npostponing the the analysis of learned Frequent Directions to Appendix D. Section 4 presents our\nexperiment results with extensive figures given in Appendix F.\n3\n\nPublished as a conference paper at ICLR 2025\n2 P RELIMINARIES\nFrequency Estimation. Letn, d∈N+, and consider a sequence a1, a2, . . . , a n∈[d]arriving\none by one. We are interested in the number of times each element in [d]appears in the stream.\nSpecifically, the frequency f(i)of element i∈[d]is defined as f(i).=|{t∈[n] :at=i}|. Thus,P\ni∈[d]f(i) = n.Given estimates ˜f(i)for each , i∈[d], we focus on the following weighted\nestimation error (Hsu et al., 2019; Aamand et al., 2023): Err.=P\ni∈[d]f(i)\nn·\f\f\ff(i)−˜f(i)\f\f\f.(1)\nThe weighted error assigns a weight to each element’s estimation error proportional to its frequency,\nreflecting the intuition that frequent elements are queried more often than less frequent ones.\nDirection Frequency. The frequency estimation problem has a natural high-dimensional exten-\nsion. The input now consists of a stream of vectors A1,A2, . . . ,An∈Rd. For each unit vector\n⃗ v∈Rd, we define its ”frequency,” f(⃗ v), as the sum of the squares of the projected lengths of each\ninput vector onto ⃗ v. Specifically, let A∈Rn×ddenote the matrix whose rows are AT\n1, . . . ,AT\nn.\nThen f(⃗ v).=∥A⃗ v∥2\n2.\nTo see the definition is a natural extension of the element frequency in the frequency estimation\nproblem, suppose each input vector Atis one of the standard basis vectors ⃗ e1, . . . ,⃗ e dinRd. Further,\nwe restrict the frequency query vector ⃗ vto be one of these standard basis vectors, i.e., ⃗ v=⃗ eifor\nsome i∈[d].Then f(⃗ ei) =∥A⃗ ei∥2\n2=P\nt∈[n]⟨At,⃗ ei⟩2=P\nt∈[n]1[At=⃗ ei],which is simply the\nnumber of times ⃗ eiappears in A.\nEstimation Error. Consider an algorithm that can provide an estimate ˜f(⃗ v)off(⃗ v)for any unit\nvector ⃗ v∈Rd. The estimation error of a single vector is given by\f\f\ff(⃗ v)−˜f(⃗ v)\f\f\f=\f\f\f∥A⃗ v∥2\n2−˜f(⃗ v)\f\f\f.\nSince the set of all unit vectors in Rdis uncountably infinite, we propose to study the following\nweighted error:\nErr=X\ni∈[d]σ2\ni\n∥A∥2\nF·\f\f\f∥A⃗ vi∥2\n2−˜f(⃗ vi)\f\f\f, (2)\nwhere σ1, . . . , σ ddenote the singular values of A,⃗ v1, . . . ,⃗ v dare the corresponding right singular\nvectors, and ∥A∥Fis its Frobenius norm.\nTo see how this generalizes Equation (1), assume again that the rows of Aconsist of standard basis\nvectors and that f(⃗ e1)≥f(⃗ e2)≥ ··· ≥ f(⃗ ed). In this case, it is straightforward to verify that\nσ2\ni=f(⃗ ei)and⃗ vi=⃗ eifor all i∈[d]. Consequently, ∥A⃗ vi∥2\n2=f(⃗ ei), and∥A∥2\nF=P\ni∈[d]σ2\ni=\nn. Therefore, Equation (2) reduces to Equation (1) in this case. Moreover, for a specific class of\nalgorithms, we can offer an alternative and intuitive interpretation of the weighted error.\nLemma 2.1. For algorithms that estimate ˜f(⃗ v)by first constructing a matrix Band then applying\nthe formula ˜f(⃗ v) =∥B⃗ v∥2\n2such that 0≤˜f(⃗ v)≤f(⃗ v), the weighted error defined in Equation (2)\nsatisfies Err∝E⃗ v∼N(0,ATA)h\n∥A⃗ v∥2\n2−˜f(⃗ v)i\n.(3)\nThe conditions stated in the lemma apply to the Frequent Directions algorithm (Ghashami et al.,\n2016), discussed later in the section. The lemma asserts that the weighted error is proportional to\nthe expected difference between ∥A⃗ v∥2\n2and˜f(⃗ v), where ⃗ vis sampled from a multivariate normal\ndistribution with mean 0and covariance matrix ATA. The proof is included in the Appendix B.\nZipfian Distribution. We follows the assumption that in the frequency estimation problem, the\nelement frequencies follow a Zipfian distribution (Hsu et al., 2019; Aamand et al., 2023), i.e., f(i)∝\n1/i∀i∈[d]. Naturally, for the high dimensional counterpart, we assume that σ2\ni∝1/i.\nMisra-Gries and Frequent Directions Algorithms. The Misra-Gries algorithm (Misra & Gries,\n1982) is a well-known algorithm developed for frequency estimation in the streaming setting with\nlimited memory. Its high-dimensional counterpart is the Frequent Directions algorithm (Ghashami\net al., 2016). We focus on presenting the Frequent Directions algorithm here along with a brief\nexplanation of how Misra-Gries can be derived from it.\n4\n\nPublished as a conference paper at ICLR 2025\nThe algorithm is described in Algorithm 1. The matrix Bcreated during the initialization phase can\nbe viewed as an array of mbuckets, where each bucket can store a vector in Rd. As each input vector\nAiarrives, the algorithm updates Busing an ”update” procedure, inserting AT\niinto the first avail-\nable bucket in B. IfBis full, additional operations are triggered (Lines 7 - 10): essentially, the algo-\nrithm performs a singular value decomposition (SVD) of B, such that B=P\nj∈[d]σ(i)\nj·⃗ u(i)\nj\u0010\n⃗ v(i)\nj\u0011T\n,\nwhere ⃗ u(i)\njand⃗ v(i)\njare the columns of matrices U(i)andV(i), respectively, and σ(i)\njare the diagonal\nentries of ΣΣΣ(i). The algorithm then retains only the first τ−1right singular vectors, ⃗ v(i)\n1, . . . ,⃗ v(i)\nτ−1,\nscaled by the factors\u0000\n(σ(i)\n1)2−(σ(i)\nτ)2\u00011/2, . . . ,\u0000\n(σ(i)\nτ−1)2−(σ(i)\nτ)2\u00011/2respectively.\nAlgorithm 1 Frequent Direction AFD\n1:Procedure INITIALIZATION\n2: Input: sketch parameters m, τ, d ∈N+, s.t., τ≤m≤d\n3: Reserve m×dspace for an empty matrix B\n4:Procedure UPDATE\n5: Input: an input vector Ai∈Rd\n6: B←[B;AT\ni]matrix obtained by appending AT\niafter the last row B\n7: ifBhasmrows then\n8: U(i),ΣΣΣ(i),V(i)←SVD(B)\n9: ΣΣΣ(i)←q\nmax{ΣΣΣ(i)2−(σ(i)\nτ)2I,0}, where σ(i)\nτis the τ(th)largest singular value\n10: B←ΣΣΣ(i)V(i)T\n11:Procedure RETURN\n12: return B\nTo reduce the algorithm to Misra-Gries, we make the following modifications: each input vector Ai\nis an element in [d], andBis replaced by a dynamic array with a capacity of m. The SVD operation\nis replaced by an aggregation step, where identical elements in Bare grouped together, retaining\nonly one copy of each along with its frequency in B. Consequently, lines 7–10 now correspond to\nselecting the top- (τ−1)elements and reducing their frequencies by f(τ)2.\nBased on recent work by Liberty (2022), Algorithm 1 possesses the following properties. For com-\npleteness, we provide a brief proof in the Appendix.\nProposition 2.2 ((Liberty, 2022)) .Algorithm 1 uses O(md)space, operates in O\u0010\nnm2d\nm+1−τ\u0011\ntime,\nand ensures that ATA−BTB⪰0. Moreover, it guarantees the following error bound:\n∥ATA−BTB∥2≤ min\nk∈[0. . τ−1]∥A−[A]k∥2\nF\nτ−k, (4)\nwhere ∥·∥2is the spectral norm of a matrix, and [A]kis the best rank- kapproximation of A.\nNote that the error in this context is defined by the maximum distortion rather than a weighted\none. If τ= (1−Ω(1)) m, the running time reduces to O(nmd). Furthermore, for k= 0, the\nerror bound simplifies to the original bound established by Liberty (2013). These bounds can be\nadapted for the Misra-Gries algorithm, where ATA−BTB⪰0implies that the algorithm never\noverestimates element frequencies. Additionally, when implemented with a hash table, the running\ntime for Misra-Gries can be further optimized to O(n).\n3 L EARNING -AUGMENTED FREQUENT DIRECTION\nWe aim to augment the Frequent Directions algorithm with learned predictions. The framework is\npresented in Algorithm 2. Given space for storing mvectors in Rd, the algorithm reserves mL≤m\n2A common implementation of Misra-Gries sets τ=m, and the aggregation step can be optimized using\nhash tables.\n5\n\nPublished as a conference paper at ICLR 2025\nslots for the predicted ”frequent directions” ⃗ w1, . . . , ⃗ w mL, which are orthonormal vectors returned\nby a learned oracle. The algorithm then initializes two seperate instances of Algorithm 1, denoted\nbyA↓\nFDandA⊥\nFD, with space usage mLandm−2·mL, respectively.\nAfter initialization, when an input vector Aiarrives, the algorithm decomposes it into two com-\nponents, Ai=Ai,↓+Ai,⊥, where Ai,↓is the projection of Aionto the subspace spanned by\n⃗ w1, . . . , ⃗ w mL, andAi,⊥is the component orthogonal to this subspace. The vector Ai,↓is passed to\nA↓\nFD, while Ai,⊥is passed to A⊥\nFD. Intuitively, A↓\nFDis responsible to compute a sketch matrix for the\nsubspace predicted by the learned oracle, whereas A⊥\nFDis responsible to compute a sketch matrix for\nthe orthogonal subspace. When the algorithm terminates, the output matrix is obtained by stacking\nthe matrices returned by A↓\nFDandA⊥\nFD. To adapt this framework for the learning-augmented Misra-\nGries algorithm, A↓\nFDcorresponds to an array to record the exact counts of the predicted elements\nandA⊥\nFDcorresponds to a Misra-Gries algorithm over all other elements.\nAlgorithm 2 Learning-Augmented Frequent Direction ALFD\n1:Procedure INITIALIZATION\n2: Input: sketch parameters m, d∈N+; learned oracle parameter mLs.t.,mL≤m\n3: LetPH= [⃗ w1|. . .|⃗ wmL]∈Rd×mLbe the matrix consisting of the mLorthonormal\ncolumns, which are the frequent directions predicted by the learned oracle\n4: Initialize an instance of Algorithm 1: A↓\nFD.initialization (mL,0.5·mL, d)\n5: Initialize an instance of Algorithm 1: A⊥\nFD.initialization (m−2·mL,0.5·(m−2·mL), d)\n6:Procedure UPDATE\n7: Input: an input vector Ai\n8: Ai,↓←PHPT\nHAi\n9: Ai,⊥←Ai−Ai,↓\n10: A↓\nFD.update (Ai,↓)\n11: A⊥\nFD.update (Ai,⊥)\n12:Procedure RETURN\n13: B↓← A↓\nFD.return ()\n14: B⊥← A⊥\nFD.return ()\n15: B←\u0002\nB↓;B⊥\u0003T\n16: return B\n3.1 T HEORETICAL ANALYSIS\nWe present the theoretical analysis for the (learned) Misra-Gries and (learned) Frequent Directions\nalgorithms under a Zipfian distribution. The error bounds for the (learned) Misra-Gries algorithms\nare detailed in Theorems 3.1 and 3.2. The corresponding results for the (learned) Frequent Direc-\ntions algorithm are provided in Theorems 3.3 and 3.4. The complete proofs for are provided in\nAppendix C and Appendix D, respectively.\nDue to space constraints, we provide sketch proofs for the (learned) Misra-Gries algorithm only. The\nproofs for the (learned) Frequent Directions algorithm follow similar techniques. Since the structure\nof Misra-Gries is simpler, analyzing its bounds first offers clearer insights into the problems.\nTheorem 3.1 (Expected Error of the Misra-Gries Algorithm) .Given a stream of nelements from a\ndomain [d], where each element ihas a frequency f(i)∝1/ifori∈[d], the Misra-Gries algorithm\nusing mwords of memory achieves expected error of Err∈Θ\u0010\u0010\nlnm\nlnd\nm\u0011\n·lnd\nm\n(lnd)2·n\nm\u0011\n.(5)\nProof Sketch. At a high level, we first derive an upper bound on the maximum estimation error\nusing Fact 2.2 under the Zipfian distribution assumption. We then partition the elements into two\ngroups: those with frequencies exceeding this error and those that do not. For the first group, the\nestimation error for each element is bounded by the maximum error. For the second group, since\nMisra-Gries never overestimates their frequencies, the error is limited to the actual frequency of\neach element. For each group, we can show that the weighted error is bounded above by the RHS\n6\n\nPublished as a conference paper at ICLR 2025\nof (5). For the lower bound, we construct an adversarial input sequence such that the weighted error\nof elements in the first group indeed matches the upper bound, proving that the bound is tight. □\nTheorem 3.2 (Expected Error of the Learned Misra-Gries Algorithm) .Given a stream of nelements\nfrom a domain [d], where each element ihas a frequency f(i)∝1/ifori∈[d], and assuming a\nperfect oracle, the learning-augmented Misra-Gries algorithm using mwords of memory achieves\nexpected error of Err∈Θ\u0010\n1\nm·n\n(lnd)2\u0011\n.\nHere, a perfect oracle is defined as one that makes no mistakes in predicting the top frequent ele-\nments. The scenario where the learning oracle is not perfect will be discussed later in this section.\nProof Sketch. Under the assumption of access to a perfect oracle, the algorithm does not make\nestimation error on the top- mLelements. For the remaining elements, the Misra-Gries algorithm\nnever overestimates its frequency: ˜f(i)∈[0, f(i)].Hence the weighted error is at most\nErr=dX\ni=mL+1f(i)\nn·\f\f\f˜f(i)−f(i)\f\f\f≤dX\ni=mL+11\ni·lnd·n\ni·lnd∈O\u00121\nm·n\n(lnd)2\u0013\n. (6)\nThe lower bound is obtained using a similar technique as in Theorem 3.1, by constructing an input\nsequence such that the error incurred by the non-predicted elements matches the upper bound. □\nComparison with Previous Work. This guarantee matches that of the learning-augmented fre-\nquency estimation algorithm of Aamand et al. (2023) but with significant simplifications. Aamand\net al. (2023) also reserve separate buckets for the predicted heavy hitters, but to get a robust algo-\nrithm in case of faulty predictions, they maintain O(log log n)additional CountSketch tables for\ndetermining if an arriving element (which is not predicted to be heavy) is in fact a heavy hitter with\nreasonably high probability. If these tables deem the element light, they output zero as the estimate,\nand otherwise, they use the estimate of a separate CountSketch table. In contrast, our algorithm uses\njust a single implementation of the simple and classic Misra-Gries algorithm. This approach has the\nadditional advantage of being deterministic in contrast to CountSketch, which is randomized.\nRobustness and Resilience to Prediction Errors. We note that the learned Misra-Gries algorithm\nisrobust in the sense that it essentially retains the error bounds of its classic counterpart regardless\nof predictor quality. Indeed, the learned version allocates m/2space to maintain exact counts of\nelements predicted to be heavy, and uses a classic Misra-Gries sketch of size m/2for the remaining\nelements. Thus, it incurs no error on the elements predicted to be heavy and on the elements pre-\ndicted to be light, we get the error guarantees of classic Misra-Gries (using space m/2instead of\nm). It is further worth noting that the error bound of Theorem 3.2 holds even for non-perfect learn-\ning oracles or predictions as long as their accuracy is high enough. Specifically, assume that the\nalgorithm allocates some mL∈Ω(m)buckets for the learned oracle. Further, assume that only the\ntopc·mLelements with the highest frequencies are included among the mLheavy hitters predicted\nby the oracle, for some c≤1(e.g., c= 0.1). In this case, Inequality (6) still holds: the summation\nnow starts from c·mL+ 1instead of mL+ 1, which does not affect the asymptotic error.\nThe corresponding theorems for (learned) Frequent Directions are below with proofs in Appendix D.\nTheorem 3.3 (Expected Error of the F REQUENT DIRECTIONS Algorithm) .Assume that the singu-\nlar values of the input matrix Ato the Algorithm 1 satisfies σ2\ni∝1\ni, for all i∈[d], it achieves an\nexpected error of Err(AFD)∈Θ\u0010\u0010\nlnm\nln2d\nm\u0011\n·lnd\nm\n(lnd)2·∥A∥2\nF\nm\u0011\n.\nTheorem 3.4 (Expected Error of the Learned F REQUENT DIRECTIONS Algorithm) .Assume that\nthe singular values of the input matrix Ato Algorithm 2 satisfies σ2\ni∝1\ni, for all i∈[d], and that\nlearning oracle is perfect, it achieves an expected error of Err(AFD)∈Θ\u0010\n1\n(lnd)2·∥A∥2\nF\nm\u0011\n.\nRobustness of Learned Frequent Directions. It turns out that Algorithm 2 does not come with\na robustness guarantee similar to that of Learned Misra-Gries discussed above. In fact, we can\nconstruct adversarial inputs for which the expected error is much worse than in the classic setting.\nFortunately, there is a way to modify the algorithm slightly using the fact that the residual error\n∥A−[A]k∥2\nFcan be computed within a constant factor using an algorithm from Li et al. (2024).\n7\n\nPublished as a conference paper at ICLR 2025\nSince the error of the algorithm scales with the residual error, this essentially allows us to determine\nif we should output the result of a learned or standard Frequent Directions algorithm. The result\nis Theorem E.1 on robustness. Combined with Theorem E.2, which explicitly bounds the error of Al-\ngorithm 2 in terms of the true and predicted frequent directions, we obtain consistency/robustness\ntradeoffs for the modified algorithm. Details are provided in Appendix E.\n4 EXPERIMENTS\nWe complement our theoretical results with experiments on real data both in the frequency estima-\ntion (1-dimensional stream elements) and frequent directions (row vector stream elements) settings.\nWe highlight the main experimental results here and include extensive figures in Appendix F.\n4.1 F REQUENT DIRECTIONS EXPERIMENTS\nDatasets and Predictions We use datasets from Indyk et al. (2019) and Indyk et al. (2021), prior\nworks on learning-based low rank approximation not in the streaming setting. The Hyper dataset\n(Imamoglu et al., 2018) contains a sequence of hyperspectral images of natural scenes. We consider\n80images each of dimension 1024×768. The Logo, Friends, and Eagle datasets come from high-\nresolution Youtube videos3. We consider 20frames from each video each with dimension 3240×\n1920 . We plot the distribution of singular values for each dataset in Appendix F. For each dataset, we\nuse the top singular vectors of the first matrix in the sequence to form the prediction via a low-rank\nprojection matrix (see Algorithm 2).\nBaselines We compare two streaming algorithms and one incomparable baseline. In the streaming\nsetting, we compare the Frequent Directions algorithm of Ghashami et al. (2016) with our learning-\naugmented variant. Both implementations are based on an existing implementation of Frequent Di-\nrections4. We additionally plot the performance of the low-rank approximation given by the largest\nright singular vectors (weighted by singular values). This matrix is not computable in a stream as it\ninvolves taking the SVD of the entire matrix Abut we evaluate it for comparison purposes. Results\nare displayed based on the rank of the matrix output by the algorithm, which we vary from 20to200.\nFor both Frequent Directions and our learned variant, the space used by the streaming algorithm is\ntwice the rank: this is a choice made in the Frequent Directions implementation to avoid running\nSVD on every insertion and thus improve the update time. We use half of the space for the learned\nprojection component and half for the orthogonal component in our algorithm.\nResults For each of the four datasets, we plot tradeoffs between median error (across the sequence\nof matrices) and rank as well as error across the sequence for a fixed rank of 100(see Figure 1).\nWe include the latter plots for choices of rank in Appendix F. Our learning-augmented Frequent\nDirections algorithm improves upon the base Frequent Directions by 1-2 orders of magnitude on\nall datasets. In most cases, it performs within an order of magnitude of the (full-memory, non-\nstreaming) SVD approximation. In all cases, increasing rank, or equivalently, space, yields signifi-\ncant improvement in the error. These results indicate that learned hints taken from the SVD solution\non the first matrix in the sequence can be extremely powerful in improving matrix approximations in\nstreams. As the sequences of matrices retain self-similarity (e.g., due to being a sequence of frames\nin a video), the predicted projection allows our streaming algorithm to achieve error closer to that of\nthe memory-intensive SVD solution than that of the base streaming algorithm.\n4.2 F REQUENCY ESTIMATION EXPERIMENTS\nDatasets and Predictions We test our algorithm and baselines on the CAIDA (CAIDA, 2016)\nand AOL (Pass et al., 2006) datasets used in prior work (Hsu et al., 2019; Aamand et al., 2023).\nThe CAIDA dataset contains 50 minutes of internet traffic data, with a stream corresponding to the\nIP addressed associated with packets passing through an ISP over a minute of data. Each minute\nof data contains approximately 30 million packets with 1 million unique IPs. The AOL dataset\n3Originally downloaded from http://youtu.be/L5HQoFIaT4I ,http://youtu.be/\nxmLZsEfXEgE andhttp://youtu.be/ufnf_q_3Ofg and appearing in Indyk et al. (2019).\n4https://github.com/edoliberty/frequent-directions\n8\n\nPublished as a conference paper at ICLR 2025\n20 40 60 80 100 120 140 160 180 200\nRank106\n105\n104\n103\nError\nError/Rank Tradeoff\nSVD\nFD\nLearned FD (ours)\n0 10 20 30 40 50 60 70 80\nMatrices106\n105\n104\n103\nError\nRank: 100\nSVD\nFD\nLearned FD (ours)\n(a) Hyper\n20 40 60 80 100 120 140 160 180 200\nRank102104106108Error\nError/Rank Tradeoff\nSVD\nFD\nLearned FD (ours)\n0 2 4 6 8 10 12 14 16 18\nMatrices102103104105106Error\nRank: 100\nSVD\nFD\nLearned FD (ours)\n(b) Logo\n20 40 60 80 100 120 140 160 180 200\nRank103104105106107Error\nError/Rank Tradeoff\nSVD\nFD\nLearned FD (ours)\n0 2 4 6 8 10 12 14 16 18\nMatrices108\n105\n102\n101104Error\nRank: 100\nSVD\nFD\nLearned FD (ours)\n(c) Eagle\n20 40 60 80 100 120 140 160 180 200\nRank102104106108Error\nError/Rank Tradeoff\nSVD\nFD\nLearned FD (ours)\n0 2 4 6 8 10 12 14 16 18\nMatrices103104105106Error\nRank: 100\nSVD\nFD\nLearned FD (ours)\n(d) Friends\nFigure 1: Comparison of matrix approximations. The Frequent Directions and learning-augmented\nFrequent Directions algorithms are streaming algorithms while the exact SVD stores the entire ma-\ntrix to compute a low-rank approximation (so it cannot be implemented in a stream). For each\ndataset, the left plot shows median error (error formula from Equation (2)) as the rank of the approx-\nimation varies while the right plot shows error over the sequence of matrices with a fixed rank of\n100. The sudden drop in error in Eagle corresponds to several frames of a black screen in the video.\ncontains 80 days of internet search query data with each stream (corresponding to a day) having\naround 300k total queries and 100k unique queries. We plot the frequency distribution for both\n9\n\nPublished as a conference paper at ICLR 2025\ndatasets in Appendix F. We use recurrent neural networks trained in past work of Hsu et al. (2019)\nas the predictor for both datasets.\nAlgorithms We compare our learning-augmented Misra-Gries algorithm with learning-augmented\nCountSketch (Hsu et al., 2019) and learning-augmented CountSketch++ (Aamand et al., 2023). As\nin Aamand et al. (2023), we forego comparisons against CountMin as it has worse performance both\nin theory (Aamand et al., 2023) and practice (Hsu et al., 2019). For the prior state-of-the-art, learned\nCS++, the implemented algorithm does not exactly correspond to the one which achieves the best\ntheoretical bounds as only a single CountSketch table is used (as opposed to two) and the number\nof rows of the sketch is 3(as opposed to O(log log n)). There is a tunable hyperparameter Cwhere\nelements with estimated frequency less than Cn/w have their estimates truncated to zero (where n\nis the stream length and wis the sketch width). The space stored by the sketch corresponds to 3w\nas there are 3rows. For Misra-Gries, the space corresponds to double the number of stored counters\nas each counter requires storing a key as well as a count. As in prior work, for all algorithms, their\nlearned variants use half of the space for the normal algorithm and half of the space to store exact\ncounts for the elements with top predicted frequencies.\n500 1000 1500 2000 2500 3000\nSpace0.20.40.60.81.01.21.4Weighted Error1e12\n Error/Space Tradeoff\nCS\nCS++ (C=1)\nCS++ (C=2)\nCS++ (C=5)\nMG (ours)\n0 10 20 30 40 50\nStreams1234567Weighted Error1e11\n Space: 750\nCS\nCS++ (C=1)\nCS++ (C=2)\nCS++ (C=5)\nMG (ours)\n500 1000 1500 2000 2500 3000\nSpace012345Weighted Error1e7\n Error/Space Tradeoff\nCS\nCS++ (C=1)\nCS++ (C=2)\nCS++ (C=5)\nMG (ours)\n0 10 20 30 40 50 60 70 80\nStreams0.00.51.01.52.02.53.0Weighted Error1e7\n Space: 750\nCS\nCS++ (C=1)\nCS++ (C=2)\nCS++ (C=5)\nMG (ours)\nFigure 2: Comparison of learning-augmented frequency estimation algorithms. Top: CAIDA, Bot-\ntom: AOL. For both datasets, the left plot show the median error of each method (across all 50\nstreams) with varying space budgets. The right plot shows the performance of each algorithm across\nstreams with fixed space of 750words. Randomized algorithms are averaged across 10 trials and\none standard deviation is shaded.\nResults For both datasets, we compare the learning-augmented algorithms by plotting the tradeoff\nbetween median error and space as well as error across the sequence of streams for a fixed space of\n750(see Figure 2). In Appendix F, we include the latter plots for all choices of space, as well as all\ncorresponding plots both without predictions (to compare the base CS, CS++, and MG algorithms)\nand under unweighted error (taken as the unweighted sum of absolute errors over all stream items re-\ngardless of frequency) which was also evaluated in prior work. The learning-augmented Misra-Gries\nalgorithm improves significantly over learning-augmented CountSketch, as implied by our theoret-\nical bounds. Furthermore, it is competitive with the state-of-the-art learning-augmented CS++ al-\ngorithm. Sometimes our algorithm outperforms the best hyperparameter choice CS++ and often\noutperforms several of the hyperparameter choices of CS++. Furthermore, learning-augmented MG\nhas no equivalent tunable parameter and is simpler to deploy (especially as CS++ is already a sim-\nplification of the theoretical algorithm of Aamand et al. (2023)). As learning-augmented MG is the\nonly deterministic algorithm with provable guarantees in the setting of Hsu et al. (2019), our results\nindicate that there is essentially no cost to derandomization.\n10\n\nPublished as a conference paper at ICLR 2025\nACKNOWLEDGMENTS\nJustin Y . Chen is supported by an NSF Graduate Research Fellowship under Grant No. 17453.\nHao WU was a Postdoctoral Fellow at the University of Copenhagen, supported by Providentia, a\nData Science Distinguished Investigator grant from Novo Nordisk Fonden. Siddharth Gollapudi is\nsupported in part by the NSF (CSGrad4US award no. 2313998).\nREFERENCES\nAnders Aamand, Justin Y . Chen, Huy L ˆe Nguyen, Sandeep Silwal, and Ali Vakilian. Im-\nproved frequency estimation algorithms with and without predictions. In Alice Oh, Tris-\ntan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Ad-\nvances in Neural Information Processing Systems 36: Annual Conference on Neural Infor-\nmation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,\n2023 , 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/\n2e49934cac6cb8604b0c67cfa0828718-Abstract-Conference.html .\nCAIDA. Caida internet traces, chicago. http://www.caida.org/data/monitors/passive-equinix-\nchicago.xml, 2016.\nMoses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams.\nInInternational Colloquium on Automata, Languages, and Programming , pp. 693–703. Springer,\n2002.\nJustin Y . Chen, Talya Eden, Piotr Indyk, Honghao Lin, Shyam Narayanan, Ronitt Rubinfeld,\nSandeep Silwal, Tal Wagner, David Woodruff, and Michael Zhang. Triangle and four cycle count-\ning with predictions in graph streams. In International Conference on Learning Representations ,\n2022a.\nJustin Y . Chen, Piotr Indyk, and Tal Wagner. Streaming algorithms for support-aware histograms.\nInProceedings of the 39th International Conference on Machine Learning , 2022b.\nJustin Y . Chen, Sandeep Silwal, Ali Vakilian, and Fred Zhang. Faster fundamental graph algo-\nrithms via learned predictions. In International Conference on Machine Learning , pp. 3583–3602.\nPMLR, 2022c.\nKenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in input spar-\nsity time. In Dan Boneh, Tim Roughgarden, and Joan Feigenbaum (eds.), Symposium on Theory\nof Computing Conference, STOC’13, Palo Alto, CA, USA, June 1-4, 2013 , pp. 81–90. ACM,\n2013. doi: 10.1145/2488608.2488620. URL https://doi.org/10.1145/2488608.\n2488620 .\nGraham Cormode and Shan Muthukrishnan. An improved data stream summary: the count-min\nsketch and its applications. Journal of Algorithms , 55(1):58–75, 2005.\nMichael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii.\nFaster matchings via learned duals. ArXiv , abs/2107.09770, 2021. URL https://api.\nsemanticscholar.org/CorpusID:236154892 .\nElbert Du, Franklyn Wang, and Michael Mitzenmacher. Putting the “learning” into learning-\naugmented algorithms for frequency estimation. In Marina Meila and Tong Zhang (eds.), Pro-\nceedings of the 38th International Conference on Machine Learning , volume 139 of Proceed-\nings of Machine Learning Research , pp. 2860–2869. PMLR, 18–24 Jul 2021. URL https:\n//proceedings.mlr.press/v139/du21d.html .\nMina Ghashami and Jeff M. Phillips. Relative errors for deterministic low-rank matrix approxima-\ntions. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms ,\nSODA ’14, pp. 707–717, USA, 2014a. Society for Industrial and Applied Mathematics. ISBN\n9781611973389.\nMina Ghashami and Jeff M Phillips. Relative errors for deterministic low-rank matrix approxima-\ntions. In Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms ,\npp. 707–717. SIAM, 2014b.\n11\n\nPublished as a conference paper at ICLR 2025\nMina Ghashami, Edo Liberty, Jeff M. Phillips, and David P. Woodruff. Frequent directions: Simple\nand deterministic matrix sketching. SIAM Journal on Computing , 45(5):1762–1792, 2016. doi:\n10.1137/15M1009718. URL https://doi.org/10.1137/15M1009718 .\nSreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with expert\nadvice. In International Conference on Machine Learning , 2019. URL https://api.\nsemanticscholar.org/CorpusID:174800680 .\nChen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation\nalgorithms. In 7th International Conference on Learning Representations, ICLR 2019, New Or-\nleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019. URL https://openreview.net/\nforum?id=r1lohoCqY7 .\nNevrez Imamoglu, Yu Oishi, Xiaoqiang Zhang, Guanqun Ding, Yuming Fang, Toru Kouyama, and\nRyosuke Nakamura. Hyperspectral image dataset for benchmarking on salient object detection.\nIn2018 Tenth international conference on quality of multimedia experience (qoMEX) , pp. 1–3.\nIEEE, 2018.\nPiotr Indyk, Ali Vakilian, and Yang Yuan. Learning-based low-rank approximations. In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc, E. Fox, and R. Garnett (eds.),\nAdvances in Neural Information Processing Systems , volume 32. Curran Associates, Inc.,\n2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/\nfile/1625abb8e458a79765c62009235e9d5b-Paper.pdf .\nPiotr Indyk, Tal Wagner, and David Woodruff. Few-shot data-driven algorithms for low rank ap-\nproximation. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan\n(eds.), Advances in Neural Information Processing Systems , volume 34, pp. 10678–10690. Cur-\nran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/\npaper/2021/file/588da7a73a2e919a23cb9a419c4c6d44-Paper.pdf .\nTanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, and David P Woodruff. Learning-augmented data\nstream algorithms. ICLR , 2020.\nTim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 International Conference on Management of Data ,\nSIGMOD ’18, pp. 489–504, New York, NY , USA, 2018. Association for Computing Machin-\nery. ISBN 9781450347037. doi: 10.1145/3183713.3196909. URL https://doi.org/10.\n1145/3183713.3196909 .\nYi Li, Honghao Lin, and David P. Woodruff. Optimal sketching for residual error estimation for\nmatrix and vector norms. In The Twelfth International Conference on Learning Representa-\ntions, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net, 2024. URL https:\n//openreview.net/forum?id=RsJwmWvE6Q .\nEdo Liberty. Simple and deterministic matrix sketching. In Inderjit S. Dhillon, Yehuda Koren,\nRayid Ghani, Ted E. Senator, Paul Bradley, Rajesh Parekh, Jingrui He, Robert L. Grossman,\nand Ramasamy Uthurusamy (eds.), The 19th ACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, KDD 2013, Chicago, IL, USA, August 11-14, 2013 , pp. 581–\n588. ACM, 2013. doi: 10.1145/2487575.2487623. URL https://doi.org/10.1145/\n2487575.2487623 .\nEdo Liberty. Even simpler deterministic matrix sketching. CoRR , abs/2202.01780, 2022. URL\nhttps://arxiv.org/abs/2202.01780 .\nThodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice. J.\nACM , 68(4), July 2021. ISSN 0004-5411. doi: 10.1145/3447579. URL https://doi.org/\n10.1145/3447579 .\nAhmed Metwally, Divyakant Agrawal, and A. Abbadi. Efficient computation of frequent and top-\nk elements in data streams. In International Conference on Database Theory , pp. 398–412, 01\n2005. ISBN 978-3-540-24288-8. doi: 10.1007/978-3-540-30570-5 27.\n12\n\nPublished as a conference paper at ICLR 2025\nRaphael A Meyer, Cameron Musco, Christopher Musco, and David P Woodruff. Hutch++: Optimal\nstochastic trace estimation. In Symposium on Simplicity in Algorithms (SOSA) , pp. 142–155.\nSIAM, 2021.\nGregory T. Minton and Eric Price. Improved concentration bounds for count-sketch. In ACM-SIAM\nSymposium on Discrete Algorithms , 2012. URL https://api.semanticscholar.org/\nCorpusID:11724394 .\nJayadev Misra and David Gries. Finding repeated elements. Sci. Comput. Program. , 2(2):143–\n152, 1982. doi: 10.1016/0167-6423(82)90012-0. URL https://doi.org/10.1016/\n0167-6423(82)90012-0 .\nMichael Mitzenmacher. A model for learned bloom filters and optimizing by sandwiching. Advances\nin Neural Information Processing Systems , 31, 2018.\nMichael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. Commun. ACM , 65\n(7):33–35, June 2022. ISSN 0001-0782. doi: 10.1145/3528087. URL https://doi.org/\n10.1145/3528087 .\nGreg Pass, Abdur Chowdhury, and Cayley Torgeson. A picture of search. In Proceedings of the 1st\ninternational conference on Scalable information systems , pp. 1–es, 2006.\nTamas Sarlos. Improved approximation algorithms for large matrices via random projections. In\n2006 47th annual IEEE symposium on foundations of computer science (FOCS’06) , pp. 143–152.\nIEEE, 2006.\nRana Shahout and Michael Mitzenmacher. Learning-based heavy hitters and flow frequency esti-\nmation in streams. In arXiv preprint , 06 2024. doi: 10.48550/arXiv.2406.16270.\nRana Shahout, Ibrahim Sabek, and Michael Mitzenmacher. Learning-augmented frequency estima-\ntion in sliding windows. In arXiv preprint , 09 2024. doi: 10.48550/arXiv.2409.11516.\nJoel A Tropp, Alp Yurtsever, Madeleine Udell, and V olkan Cevher. Practical sketching algorithms\nfor low-rank matrix approximation. SIAM Journal on Matrix Analysis and Applications , 38(4):\n1454–1485, 2017.\nDavid Woodruff. Low rank approximation lower bounds in row-update streams. In\nZ. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (eds.), Ad-\nvances in Neural Information Processing Systems , volume 27. Curran Associates, Inc.,\n2014. URL https://proceedings.neurips.cc/paper_files/paper/2014/\nfile/58e4d44e550d0f7ee0a23d6b02d9b0db-Paper.pdf .\nDavid P Woodruff et al. Sketching as a tool for numerical linear algebra. Foundations and Trends®\nin Theoretical Computer Science , 10(1–2):1–157, 2014.\n13\n\nPublished as a conference paper at ICLR 2025\nAppendix\nTable of Contents\nA Other Related Works 15\nB Missing Proofs for Preliminaries 15\nC Analysis of Misra-Gries 18\nD Analysis of Frequent Directions 21\nD.1 Frequent Directions Under Zipfian . . . . . . . . . . . . . . . . . . . . . . . . 21\nD.2 Learned Frequent Directions Under Zipfian . . . . . . . . . . . . . . . . . . . . 22\nE Consistency/Robustness trade offs 25\nE.1 The Error of Non-Perfect Oracles. . . . . . . . . . . . . . . . . . . . . . . . . . 26\nF Additional Experiments 28\nF.1 Dataset Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\nF.2 Noise Analysis in Frequent Directions . . . . . . . . . . . . . . . . . . . . . . 28\nF.3 Additional Frequent Directions Experiments . . . . . . . . . . . . . . . . . . . 29\nF.4 Additional Frequency Estimation Experiments . . . . . . . . . . . . . . . . . . 30\nTable of Notations\nSymbol Definition\nn Number of Inputs in the Stream\ndDomain Size (Frequency Estimation)\nDimension (Frequent Directions)\nf(·) Frequency\nai Theithinput element in the stream (Frequency Estimation)\nAi Theithinput vector in the stream (Frequent Directions)\nA Stream Input Matrix\n⃗ ei Standard Basis Vector\nσi Singular Value of a Matrix\nN(·,·) Normal Distribution\nU(i),ΣΣΣ(i),V(i)The SVD decomposition matrices at the ithiteration of Algorithm 1\nTable 3: Definitions of Main Notation.\n14\n\nPublished as a conference paper at ICLR 2025\nA O THER RELATED WORKS\nThere is also a vast literature on sketching based algorithms for low-rank approximation without any\nlearned-augmentation (Sarlos, 2006; Ghashami & Phillips, 2014b; Liberty, 2013; Tropp et al., 2017;\nMeyer et al., 2021). We refer to the monograph Woodruff et al. (2014) for more details.\nB M ISSING PROOFS FOR PRELIMINARIES\nWe refer the reader to Section 2 for the full statements.\nProof of Lemma 2.1.\nE\n⃗ v∼N(0,ATA)h\n∥A⃗ v∥2\n2−˜f(⃗ v)i\n=E\u0002\n⃗ vTATA⃗ v\u0003\n−E\u0002\n⃗ vTBTB⃗ v\u0003\n=E\u0002\ntr(⃗ vTATA⃗ v)\u0003\n−E\u0002\ntr(⃗ vTBTB⃗ v)\u0003\n=E\u0002\ntr(ATAxxT)\u0003\n−E\u0002\ntr(BTBxxT)\u0003\n= tr(ATAE\u0002\nxxT\u0003\n)−tr(BTBE\u0002\nxxT\u0003\n)\n= tr(( ATA)2)−tr(BTBATA)\nLet⃗ vibe the right singular vectors of A:\ntr\u0010\u0000\nATA\u00012\u0011\n= tr\n\nX\ni∈[d]σ2\ni·⃗ vi⃗ vT\ni\n2\n (7)\n= tr\nX\ni∈[d]σ4\ni·⃗ vi⃗ vT\ni\n=X\ni∈[d]σ4\ni, (8)\ntr\u0000\nBTBATA\u0001\n= tr\nBTB\nX\ni∈[d]σ2\ni·⃗ vi⃗ vT\ni\n\n (9)\n=X\ni∈[d]σ2\ni·tr\u0000\nBTB⃗ vi⃗ vT\ni\u0001\n=X\ni∈[d]σ2\ni·tr\u0000\n⃗ vT\niBTB⃗ vi\u0001\n. (10)\nTherefore,\nE\n⃗ v∼N(0,ATA)\u0002\n⃗ vT(ATA−BTB)⃗ v\u0003\n=X\ni∈[d]σ4\ni−X\ni∈[d]σ2\ni·tr\u0000\n⃗ vT\niBTB⃗ vi\u0001\n. (11)\nFurther, since ˜f(⃗ v) =∥B⃗ v∥2\n2and0≤˜f(⃗ v)≤f(⃗ v), Equation (2) can be written as\nX\ni∈[d]σ2\ni\n∥A∥2\nF·⃗ vT\ni(ATA−BTB)⃗ vi. (12)\nThe first term is given by\nX\ni∈[d]σ2\ni·⃗ vT\ni\nX\nj∈[d]σ2\nj⃗ vj⃗ vT\nj\n⃗ vi=X\ni∈[d]σ4\ni·⃗ vT\ni⃗ vi=X\ni∈[d]σ4\ni. (13)\nFurther note that\nE\n⃗ v∼N(0,ATA)h\n∥⃗ v∥2\n2i\n=E\nz∼N(0,I)\u0002\nzTAATz\u0003\n=E\nz∼N(0,I)\u0002\ntr\u0000\nzTAATz\u0001\u0003\n(14)\n=E\nz∼N(0,I)\u0002\ntr\u0000\nAATzzT\u0001\u0003\n= tr\u0000\nAAT\u0001\n=∥A∥2\nF. (15)\n15\n\nPublished as a conference paper at ICLR 2025\nTherefore,\nX\ni∈[d]σ2\ni\n∥A∥2\nF·⃗ vT\ni(ATA−BTB)⃗ vi=E⃗ v∼N(0,ATA)\u0002\n⃗ vT(ATA−BTB)⃗ v\u0003\nE⃗ v∼N(0,ATA)h\n∥⃗ v∥2\n2i . (16)\nProof of Fact 2.2. For consistence, at each iteration, if Bhas less than mrows after insertion, we\nstill define\nU(i),ΣΣΣ(i),V(i)←SVD(B),andΣΣΣ(i)= ΣΣΣ(i).\nTo analyze the error, let B(i)denote the value of Bafter the i(th)iteration, and define\n∆(i).=AT\niAi+B(i−1)TB(i−1)−B(i)TB(i)\n=V(i)ΣΣΣ(i)TU(i)TU(i)ΣΣΣ(i)V(i)T−V(i)ΣΣΣ(i)T\nΣΣΣ(i)V(i)T\n=V(i)\u0012\nΣΣΣ(i)TΣΣΣ(i)−ΣΣΣ(i)T\nΣΣΣ(i)\u0013\nV(i)T.\nThen\nATA−B(n)TB(n)=X\ni∈[n]\u0010\nAT\niAi+B(i−1)TB(i−1)−B(i)TB(i)\u0011\n=X\ni∈[n]∆(i). (17)\nSince each ∆(i)⪰0, we prove that\nATA−B(n)TB(n)⪰0. (18)\nLet⃗ v1, . . . ,⃗ v d∈Rkbe the right singular vectors of A. For each k= 0, . . . , τ −1, define the\nprojection matrix Pk= [⃗0|. . .|⃗0|⃗ vk+1|. . .|⃗ vd]∈Rd×d, consisting of columns vectors\n⃗0, . . . ,⃗0,⃗ vk+1, . . . ,⃗ v d. The null space is thus spanned by the top- kright singular vectors of A. We\nclaim the following holds:\n\r\r\r∆(i)\r\r\r\n2≤1\nτ−k·tr\u0010\nPkT∆(i)Pk\u0011\n, ∀k= 0, . . . , τ −1. (19)\nBefore proving it, we complete the proof of the error:\n\r\r\rATA−B(n)TB(n)\r\r\r\n2=\r\r\r\r\r\rX\ni∈[n]∆(i)\r\r\r\r\r\r\n2≤X\ni∈[n]\r\r\r∆(i)\r\r\r\n2≤1\nτ−k·X\ni∈[n]tr\u0010\nPkT∆(i)Pk\u0011\n(20)\n=1\nτ−k·tr\nPkT X\ni∈[n]∆(i)!\nPk\n (21)\n=1\nτ−k·tr\u0010\nPkTATAPk\u0011\n=1\nτ−k· ∥A−[A]k∥2\nF. (22)\nProof of Inequality (19): First,\n\r\r\r∆(i)\r\r\r\n2=\r\r\r\rV(i)\u0012\nΣΣΣ(i)TΣΣΣ(i)−ΣΣΣ(i)T\nΣΣΣ(i)\u0013\nV(i)T\r\r\r\r\n2\n=\r\r\r\rΣΣΣ(i)TΣΣΣ(i)−ΣΣΣ(i)T\nΣΣΣ(i)\r\r\r\r\n2\n=\r\r\r\rmin{ΣΣΣ(i)2,\u0010\nσ(i)\nτ\u00112\nI}\r\r\r\r\n2=\u0010\nσ(i)\nτ\u00112\n,\n16\n\nPublished as a conference paper at ICLR 2025\nwhere σ(i)\nτis the τ(th)largest singular value of ΣΣΣ(i). Next,\ntr\u0010\n∆(i)\u0011\n= tr\u0012\nV(i)\u0012\nΣΣΣ(i)TΣΣΣ(i)−ΣΣΣ(i)T\nΣΣΣ(i)\u0013\nV(i)T\u0013\n(23)\n= tr\u0012\nΣΣΣ(i)TΣΣΣ(i)−ΣΣΣ(i)T\nΣΣΣ(i)\u0013\n=X\nj∈[d]\u0010\nσ(i)\nj\u00112\n≥τ·\u0010\nσ(i)\nτ\u00112\n. (24)\nNext, let Pk= [⃗ v1|. . .|⃗ vk|⃗0|. . .|⃗0]∈Rd×dbe the projection matrix to the space spanned by\nthe top- kright singular vectors of A. ThenPk+Pk=I, and\ntr\u0010\n∆(i)\u0011\n= tr\u0010\u0000\nPk+Pk\u0001T∆(i)\u0000\nPk+Pk\u0001\u0011\n= tr\u0010\nPT\nk∆(i)Pk\u0011\n+ tr\u0010\nPkT∆(i)Pk\u0011\n.(25)\nExpanding tr\u0000\nPT\nk∆(i)Pk\u0001\nwe get\ntr\u0010\nPT\nk∆(i)Pk\u0011\n=X\nj∈[k]⃗ vT\nj∆(i)⃗ vj≤k·\u0010\nσ(i)\nτ\u00112\n, (26)\nwhich implies that\ntr\u0010\nPkT∆(i)Pk\u0011\n≤(τ−k)·\u0010\nσ(i)\nτ\u00112\n= (τ−k)·\r\r\r∆(i)\r\r\r2\n2. (27)\nRunning Time: For each Ai, inserting it into Btakes O(d)time. When Breaches its capacity of\nmrows, the operations in Lines 7-10 are triggered, and performing the SVD requires O(m2d)time.\nAfter completing this step, Bhas at least m−τ+ 1 empty rows. Thus, the algorithm can ac-\ncommodate at least m−τ+ 1additional insertions before Lines 7-10 need to be executed again.\nConsequently, the total running time is:\nO\u0012n\nm−τ+ 1·m2d\u0013\n. (28)\n17\n\nPublished as a conference paper at ICLR 2025\nC A NALYSIS OF MISRA -GRIES\nProof of Theorem 3.1. We need to prove both upper bounds and lower bounds for the theorem.\nUpper Bound for Theorem 3.1. W.L.O.G., assume that f(1)≥f(2)≥ ··· ≥ f(d). Since f(i)∝1\nifor each i∈[d], and the input stream consists of nelements, it follows that f(i)≈n\ni·lnd.\nAssume that we have a Misra-Gries sketch of size m∈N+. Then by translating the error guarantee\nfrom Fact 2.2 for Misra-Gries, we have\nmax\ni∈[d]\f\f\f˜f(i)−f(i)\f\f\f≤ min\nk∈[0. . m−1]n−P\nj∈[k]f(i)\nτ−k≤n−P\nj∈[m/2]f(i)\nm/2=2·n·ln2d\nm\nm·lnd.(29)\nFurther,\ni≥m\n2·ln2d\nm=⇒f(i) =n\ni·lnd≤2·n·ln2d\nm\nm·lnd(30)\nSince we also know that 0≤˜f(i)≤f(i), it follows that\nX\ni∈[df(i)\nn·\f\f\f˜f(i)−f(i)\f\f\f=m\n2·ln2d\nmX\ni=1f(i)\nn·\f\f\f˜f(i)−f(i)\f\f\f+dX\ni=m\n2·ln2d\nm+1f(i)\nn·\f\f\f˜f(i)−f(i)\f\f\f\n≤m\n2·ln2d\nmX\ni=1f(i)\nn·2·n·ln2d\nm\nm·lnd+dX\ni=m\n2·ln2d\nm+1f(i)\nn·f(i)\n≤m\n2·ln2d\nmX\ni=11\ni·lnd·2·n·ln2d\nm\nm·lnd+dX\ni=m\n2·ln2d\nm+11\ni·lnd·n\ni·lnd\n∈O lnm\n2·ln2d\nm\nlnd·n·lnd\nm\nm·lnd+1\nm\n2·ln2d\nm+ 1·n\n(lnd)2!\n=O lnm\n2·ln2d\nm\n1·n·lnd\nm\nm·(lnd)2+lnd\nm\nm·n\n(lnd)2!\n=O  \nlnm\nln2d\nm!\n·lnd\nm\nm·n\n(lnd)2!\nLower Bound for Theorem 3.1. To prove the lower bound, we assume there is an adversary which\ncontrols the order that the input elements arrive, under the constraints thatP\ni∈[d]f(i) =nand\nf(i)∝1/i, to maximize the error of the Misra-Gries algorithm.\nDenote Bthe array maintained by the Misra-Gries algorithm, containing mbuckets. Initially, all\nbuckets are empty.\nFirst, the adversary inserts elements 1, . . . , t to the Misra-Gries algorithm, with multiplicities\nf(1), . . . , f (t), where t=m\nln2d\nm. After this, Bcontains tnon-empty buckets (for simplicity, here\nwe assume thatP\ni∈[t]f(i)is a multiple of m), which stores elements 1, . . . , t , associated with their\nrecorded frequencies f(1), . . . , f (t), which we call their counters.\nNext, let Cbe the multi-set consisting of elements t+1, . . . , d , such that each element i∈[t+ 1. . d]\nhas multiplicity f(i)inC. Consider the following game:\n• Adversary: pick an element ifromCthat is not in B. If such element exists, remove one\ncopy of it from C, and send it to the Misra-Gries algorithm as the next input. If there is no\nsuch element, stop the game.\n18\n\nPublished as a conference paper at ICLR 2025\n• Misra-Gries Algorithm: process the input i.\nDuring this game, Bare filled up and contains no empty bucket after at most every minput, and the\ncounters of elements 1, . . . , t decrease by 1(if they are still above zero) when Bis updated to make\nempty buckets.\nFurther, when the game stops, there can be at most mdistinct elements in C, with frequency sum at\nmostPt+m\ni=t+1f(i). It follows that the counters of elements 1, . . . , t inBdecrease at least by\n1\nm·dX\ni=m+t+1f(i)∈Ω \nn·lnd\nm\nm·lnd!\n,since m+t+ 1≤2m.\nTherefore, the weighted error introduced by these counters is at least\nΩ\nX\ni∈[t]1\ni·lnd·n·lnd\nm\nm·lnd\n= Ω \nlnt\nlnd·n·lnd\nm\nm·lnd!\n= Ω  \nlnm\nln2d\nm!\n·lnd\nm\n(lnd)2·n\nm!\n.\nProof of Theorem 3.2. We need to prove both upper bounds and lower bounds for the theorem.\nUpper Bound for Theorem 3.2. It suffices to show that, there exists a parameter setting of mL\nwhich enables the algorithm to achieve the desired error bound.\nAssume that the algorithm reserves mL=m/3words for the learned oracle. Then for each element\ni∈[mL], its frequency estimate ˜f(i) =f(i). And for each i /∈[mL], the Misra-Gries algorithm\nnever overestimate its frequency: ˜f(i)∈[0, f(i)].Hence\nErr=dX\ni=mL+1f(i)\nn·\f\f\f˜f(i)−f(i)\f\f\f≤dX\ni=mL+11\ni·lnd·n\ni·lnd∈O\u00121\nm·n\n(lnd)2\u0013\n. (31)\nLower Bound for Theorem 3.2. To establish the lower bound, we consider an adversarial sce-\nnario where an adversary controls the order in which elements arrive, subject to the constraintsP\ni∈[d]f(i) =nandf(i)∝1/i. The adversary’s goal is to maximize the error of the learned\nMisra-Gries algorithm.\nAccording to the framework presented in Algorithm 2 for the learned Frequent Directions, the\nlearned Misra-Gries algorithm initializes two separate Misra-Gries instances: one for the mLel-\nements predicted to be frequent and one for elements predicted to be non-frequent.\nSince mLmemory words are already reserved for storing the frequencies of the predicted frequent\nelements, we do not need to run a full Misra-Gries algorithm on the these elements. Instead, we\nonly record their observed frequencies.\nBy overloading the notation a little, let us denote Bas the array used by the Misra-Gries instance\nmanaging the predicted non-frequent elements, which has a capacity of m−mLbuckets. Initially,\nall buckets in Bare empty.\nSince the learned Misra-Gries algorithm incurs no estimation error for the predicted frequent ele-\nments, our analysis focuses on the non-frequent elements and the potential error introduced by the\nMisra-Gries instance that processes them.\nLetCdenote the multi-set of elements mL+1, . . . , d , where each element i∈[mL+ 1. . d]appears\nwith multiplicity f(i)inC. Consider the following adversarial game:\n• Adversary’s Role: At each step, the adversary selects an element ifromCthat is not cur-\nrently stored in the array B. If such an element exists, the adversary removes one occur-\nrence of ifromCand sends it to the Misra-Gries algorithm as the next input. If there is no\nsuch element left, the adversary halts the game.\n19\n\nPublished as a conference paper at ICLR 2025\n• Misra-Gries Algorithm’s Role: The Misra-Gries algorithm processes the incoming element\nias it would normally, using the array Bof capacity m−mL.\nAfter the game, the remaining elements in Care fed to the Misra-Gries algorithm in arbitrary order\nby the adversary.\nNow, consider the estimation error made by the Misra-Gries algorithm on the elements mL+\n1, . . . , m L+ 2(m−mL). Since the array Bcan only store up to m−mLelements, the algo-\nrithm must estimate the frequency of at least m−mLelements from this range as zero. Therefore,\nthe error is at least\nmL+2(m−mL)X\ni=mL+(m−mL)+1f(i)\nn·f(i) =mL+2(m−mL)X\ni=m−mL+11\ni·lnd·n\ni·lnd∈Ω\u0012n\nm·ln2d\u0013\n,\nwhich finishes the proof.\n20\n\nPublished as a conference paper at ICLR 2025\nD A NALYSIS OF FREQUENT DIRECTIONS\nD.1 F REQUENT DIRECTIONS UNDER ZIPFIAN\nProof of Theorem 3.3. We need to prove both upper bounds and lower bounds for the theorem.\nUpper Bound for Theorem 3.3. First, based on the assumption that σ2\ni∝1\ni, and Fact 2.2, we have\n∥ATA−BTB∥2≤ min\nk∈[0. . m−1]∥A−[A]k∥2\nF\nτ−k(32)\n= min\nk∈[0. . m−1]Pd\ni=k+1σ2\ni\nτ−k(33)\n= min\nk∈[0. . m−1](Hd−Hk)· ∥A∥2\nF\n(τ−k)·lnd(34)\n≤2·(Hd−Hm/2)· ∥A∥2\nF\nm·lnd(35)\n∈O \n∥A∥2\nF·ln2d\nm\nm·lnd!\n, (36)\nwhere Hm.=P\nj∈[m]1\nj,∀m∈N+are the harmonic numbers. Further,\ni≥m\nln2d\nm=⇒σ2\ni=∥A∥2\nF\ni·lnd≤∥A∥2\nF·ln2d\nm\nm·lnd(37)\nSinceBTB⪰0, andATA−BTB⪰0by Fact 2.2, it follows that for each right singular vector\n⃗ viofA\n0≤⃗ vT\ni(ATA−BTB)⃗ vi≤⃗ vT\niATA⃗ vi≤σ2\ni, (38)\nwhere σiis the singular value associated with ⃗ vi.\nTherefore, the expected error is given by\nErr(AFD).=X\ni∈[d]σ2\ni\n∥A∥2\nF·⃗ vT\ni(ATA−BTB)⃗ vi (39)\n=m\nln2d\nmX\ni=1σ2\ni\n∥A∥2\nF·⃗ vT\ni(ATA−BTB)⃗ vi+dX\ni=m\nln2d\nm+1σ2\ni\n∥A∥2\nF·⃗ vT\ni(ATA−BTB)⃗ vi(40)\n∈O\nm\nln2d\nmX\ni=1σ2\ni\n∥A∥2\nF·∥A∥2\nF·ln2d\nm\nm·lnd+dX\ni=m\nln2d\nm+1σ2\ni\n∥A∥2\nF·σ2\ni\n (41)\n=O\nm\nln2d\nmX\ni=11\ni·lnd·∥A∥2\nF·ln2d\nm\nm·lnd+dX\ni=m\nln2d\nm+11\ni·lnd·∥A∥2\nF\ni·lnd\n (42)\n=O lnm\nln2d\nm\nlnd·∥A∥2\nF·ln2d\nm\nm·lnd+1\nm\nln2d\nm+ 1·∥A∥2\nF\n(lnd)2!\n(43)\n=O lnm\nln2d\nm\n1·∥A∥2\nF·ln2d\nm\nm·(lnd)2+ln2d\nm\nm·∥A∥2\nF\n(lnd)2!\n(44)\n=O  \nlnm\nln2d\nm!\n·lnd\nm\nm·∥A∥2\nF\n(lnd)2!\n(45)\n21\n\nPublished as a conference paper at ICLR 2025\nLower Bound for Theorem 3.3. The proof of the lower bound follows the same approach as the one\nfor Theorem 3.1, in Appendix C.\nAssume that Aconsists of standard basis vectors ⃗ e1, . . . ,⃗ e d∈Rd. Let f(⃗ ei)denote the number of\noccurrences of ⃗ eiinA. Without loss of generality, assume that f(⃗ e1)≥. . .≥f(⃗ ed). In this case,\nwe have f(⃗ ei) =σ2\niandP\ni∈[d]f(⃗ ei) =P\ni∈[d]σ2\ni=∥A∥2\nF=n. Further, we can then view the B\nmaintained by the Frequent Directions algorithm as an array of mbuckets.\nNow the setting is exactly the same as the Misra-Gries algorithm. Consequently, the constructive\nlower bound proof from Theorem 3.1 directly applies to Frequent Directions.\nD.2 L EARNED FREQUENT DIRECTIONS UNDER ZIPFIAN\nWe need an additional result to prove Theorem 3.4. Recall that PHin Algorithm 2 consists of or-\nthonormal column vectors ⃗ w1, . . . , ⃗ w mL∈Rd. Extending this set of vectors to form an orthonormal\nbasis of Rd:⃗ w1, . . . , ⃗ w mL, ⃗ wmL+1, . . . , ⃗ w d.WritePH= [⃗ wmL+1|. . .|⃗ wd]the projection ma-\ntrix to the orthogonal subspace. Let A↓.=APHPT\nHbe the matrix of projecting the rows of Ato\nthe predicted subspace, and A⊥.=A−A↓=A(I−PHPT\nH).\nThe following lemma holds.\nLemma D.1. For a vector ⃗ x∈Rd, we have\n⃗ xTATA⃗ x=⃗ xTAT\n↓A↓⃗ x+⃗ xTAT\n⊥A⊥⃗ x+ 2·X\ni∈[d]σ2\ni· ⟨PT\nH⃗ vi,PT\nH⃗ x⟩ · ⟨PT\nH⃗ vi,PT\nH⃗ x⟩. (46)\nThe proof of the lemma is included at the end of the section.\nProof of Theorem 3.4. We need to prove both upper bounds and lower bounds for the theorem.\nUpper Bound for Theorem 3.4. It suffices to show that, there exists a parameter setting of mL\nwhich enables the algorithm to achieve the desired error bound. We assume that the algorithm uses\nmL=m/3predicted directions from the learned oracle.\nRecall that Algorithm 2 maintains two instances of Algorithm 1: A↓\nFDandA⊥\nFD. The former pro-\ncesses the vectors projected onto the subspace defined by PH, while the latter handles the vectors\nprojected onto the orthogonal subspace. Therefore, the input to A↓\nFDisA↓=APHPT\nH, and the\ninput to A⊥\nFDisA⊥=A−A↓=A(I−PHPT\nH) =APHPT\nH. Ultimately, the resulting matrix\nBis a combination of the matrices returned by A↓\nFDandA⊥\nFD, specifically denoted as B↓andB⊥,\nrespectively.\nCombined with Lemma D.1, for each right singular vector ⃗ vjofA, we have\n⃗ vT\nj(ATA−BTB)⃗ vj=⃗ vT\njAT⃗ vjA−⃗ vT\nj(B↓)TB↓⃗ vj−⃗ vT\nj(B⊥)TB⊥⃗ vj (47)\n=⃗ vT\njAT\n↓A↓⃗ vj−⃗ vT\nj(B↓)TB↓⃗ vj (48)\n+⃗ vT\njAT\n⊥A⊥⃗ vj−⃗ vT\nj(B⊥)TB⊥⃗ vj (49)\n+ 2·X\ni∈[d]σ2\ni· ⟨PT\nH⃗ vi,PT\nH⃗ vj⟩ · ⟨PT\nH⃗ vi,PT\nH⃗ vj⟩. (50)\nFirst, observe that since A↓\nFDis allocated mL×dspace for the matrix A↓with rank at most mL, by\nthe error guarantee of Frequent Direction algorithm (Fact 2.2), it is guaranteed that ⃗ vT\njAT\n↓A↓⃗ vj−\n⃗ vT\nj(B↓)TB↓⃗ vj= 0.\nSecond, note that ⟨PT\nH⃗ vi,PT\nH⃗ vj⟩is the inner product, between the projected vectors ⃗ viand⃗ vj\nto the subspace Hspecified by the predicted frequent directions, and that ⟨PT\nH⃗ vi,PT\nH⃗ vj⟩is the\ninner product, between the projected vectors ⃗ viand⃗ vjto the orthogonal complement of H.\n22\n\nPublished as a conference paper at ICLR 2025\nIn particular, when the machine learning oracle makes perfect predictions of ⃗ v1, . . . ,⃗ v mL, i.e.,\n⃗ w1=⃗ v1, . . . , ⃗ w mL=⃗ vmL, then for each i, either PT\nH⃗ viorPT\nH⃗ viwill be zero.\nTherefore, it holds that\n⃗ vT\nj(ATA−BTB)⃗ vj=⃗ vT\njAT\n⊥A⊥⃗ vj−⃗ vT\nj(B⊥)TB⊥⃗ vj. (51)\nFurther, by the property of Frequent Direction algorithm A⊥\nFD,AT\n⊥A⊥−(B⊥)TB⊥⪰0.And since\nA⊥is the projection of Ato the subspace spanned by the right singular vectors ⃗ vmL+1, . . . ,⃗ v d, it\nstill has right singular vectors ⃗ vmL+1, . . . ,⃗ v d, associated with singular values σmL+1, . . . , σ d. It\nfollows that\n0≤⃗ vT\nj(ATA−BTB)⃗ vj (52)\n=⃗ vT\njAT\n⊥A⊥⃗ vj−⃗ vT\nj(B⊥)TB⊥⃗ vj (53)\n≤⃗ vT\njAT\n⊥A⊥⃗ vj (54)\n≤\u001aσ2\nj, j > m L\n0 j≤mL. (55)\nTherefore, the weighted error is given by\nErr(AFD).=X\ni∈[d]σ2\ni\n∥A∥2\nF·⃗ vT\ni(ATA−BTB)⃗ vi (56)\n=mLX\ni=1σ2\ni\n∥A∥2\nF·⃗ vT\ni(ATA−BTB)⃗ vi+dX\ni=mL+1σ2\ni\n∥A∥2\nF·⃗ vT\ni(ATA−BTB)⃗ vi(57)\n=dX\ni=mL+1σ2\ni\n∥A∥2\nF·⃗ vT\ni(ATA−BTB)⃗ vi (58)\n∈O dX\ni=mL+1σ2\ni\n∥A∥2\nF·σ2\ni!\n(59)\n=O dX\ni=mL+11\ni·lnd·∥A∥2\nF\ni·lnd!\n(60)\n=O \n1\nmL+ 1·∥A∥2\nF\n(lnd)2!\n(61)\nNoting that mL∈Θ(m)finishes the proof of upper bound.\nLower Bound for Theorem 3.4. The proof of the lower bound follows the same approach as the one\nfor Theorem 3.2, in Appendix C.\nAssume that Aconsists of standard basis vectors ⃗ e1, . . . ,⃗ e d∈Rd. Let f(⃗ ei)denote the number of\noccurrences of ⃗ eiinA. Without loss of generality, assume that f(⃗ e1)≥. . .≥f(⃗ ed). In this case,\nwe have f(⃗ ei) =σ2\niandP\ni∈[d]f(⃗ ei) =P\ni∈[d]σ2\ni=∥A∥2\nF=n. Further, we can then view the B\nmaintained by the Frequent Directions algorithm as an array of mbuckets.\nNow the setting is exactly the same as the Misra-Gries algorithm. Consequently, the constructive\nlower bound proof from Theorem 3.2 directly applies to learned Frequent Directions.\nWe next prove Lemma D.1.\nProof of Lemma D.1. First, observe that\n⃗ xTATA⃗ x=⃗ xT(A↓+A⊥)T(A↓+A⊥)⃗ x (62)\n=⃗ xTAT\n↓A↓⃗ x+⃗ xTAT\n⊥A⊥⃗ x+⃗ xTAT\n↓A⊥⃗ x+⃗ xTAT\n⊥A↓⃗ x. (63)\n23\n\nPublished as a conference paper at ICLR 2025\nIt suffices to show that\n⃗ xTAT\n↓A⊥⃗ x=⃗ xTAT\n⊥A↓⃗ x=X\ni∈[d]σ2\ni· ⟨PT\nH⃗ vi,PT\nH⃗ x⟩ · ⟨PT\nH⃗ vi,PT\nH⃗ x⟩ (64)\nNote that\n⃗ xTAT\n↓A⊥⃗ x=⃗ xT\u0010\nPT\nHPHAT\u0011\u0010\nA(I−PHPT\nH)⃗ x\u0011\n(65)\n=⃗ xTPT\nHPHATA(I−PHPT\nH)⃗ x (66)\n=⃗ xT(I−PHPT\nH)TATAPHPT\nH⃗ x=⃗ xTAT\n⊥A↓⃗ x. (67)\nHence, it remains to study ⃗ xTAT\n↓A⊥⃗ xor⃗ xTAT\n⊥A↓⃗ x. Keeping expanding one of them\n⃗ xTAT\n↓A⊥⃗ x=⃗ xT(I−PHPT\nH)TATAPHPT\nH⃗ x (68)\n=⃗ xT(I−PHPT\nH)T\nX\ni∈[d]σ2\ni⃗ vi⃗ vT\ni\nPHPT\nH⃗ x (69)\n=X\ni∈[d]σ2\ni· ⟨⃗ vi,(I−PHPT\nH)⃗ x⟩ · ⟨⃗ vi,PHPT\nH⃗ x⟩. (70)\nSinceI−PHPT\nH=PHPT\nH,\n⃗ xTAT\n↓A⊥⃗ x=X\ni∈[d]σ2\ni· ⟨⃗ vi,PHPT\nH⃗ x⟩ · ⟨⃗ vi,PHPT\nH⃗ x⟩ (71)\n=X\ni∈[d]σ2\ni· ⟨PT\nH⃗ vi,PT\nH⃗ x⟩ · ⟨PT\nH⃗ vi,PT\nH⃗ x⟩ (72)\n24\n\nPublished as a conference paper at ICLR 2025\nE C ONSISTENCY /ROBUSTNESS TRADE OFFS\nIf the predictions are perfect, the sketch Boutput by Algorithm 2 satisfies that BTB⪯ATA. This\nproperty in particular gives the quantity ⃗ xTBTB⃗ x≤⃗ xTATA⃗ xfor any vector ⃗ xand as further\nBTB⪰0, we get that the error |⃗ vT\niBTB⃗ vi−⃗ vT\niATA⃗ vi| ≤σ2\nifor any of the singular vectors ⃗ vi\n(and with perfect predictions the errors on the predicted singular vectors are zero). Unfortunately,\nwith imperfect predictions, the guarantee that BTB⪯ATAis not retained. To take a simple\nexample, suppose that d= 2 and that the input matrix A= (1,1)has just one row. Suppose we\ncreate two frequent direction sketches by projecting onto the standard basis vectors e1ande2and\nstack the resulting sketches B1andB2to get a sketch matrix B. It is then easy to check that B\nis in fact the identify matrix. In particular, if ⃗ x=e1−e2, then∥B⃗ x∥2\n2= 2 whereas ∥A⃗ x∥2\n2= 0\nshowing that ATA−BTBis not positive semidefinite. The absence of this property poses issues\nin proving consistency/robustness trade offs for the algorithm. Indeed, our analysis of the classic\nfrequent directions algorithm under Zipfian distributions, crucially uses that the error incurred in the\nlight directions ⃗ vifori≥m\nlnd\nmis at most σ2\ni.\nIn this section, we address this issue by presenting a variant of Algorithm 2 that does indeed provide\nconsistency/robustness trade-offs with only a constant factor blow up in space. To do so, we will\nmaintain three different sketches of the matrix A. The first sketch is the standard frequent directions\nsketch Liberty (2013) in Algorithm 1, the second one is the learning-augmented sketch produced\nby Algorithm 2, and the final sketch computes an approximation to the residual error ∥A−[A]k∥2\nF\nwithin a constant factor using an algorithm from Li et al. (2024). Let B1be the output of Algorithm 1\non input AandB2be the output of Algorithm 2 on input A. Suppose for simplicity that we knew\n∥A−[A]k∥2\nFexactly. Then, the idea is that when queried with a unit vector ⃗ x, we compute ∥B1x∥2\n2\nand∥B2x∥2\n2. If these are within 2∥A−[A]k∥2\nF\nm−kof each other, we output ∥B2x∥2\n2as the final estimate\nof⃗ xTATA⃗ x, otherwise, we output ∥B1x∥2\n2. The idea behind this approach is that in the latter\ncase, we know that the learning-based algorithm must have performed poorly with an error of at\nleast∥A−[A]k∥2\nF\nm−kand by outputting the estimate from the classic algorithm, we retain its theoretical\nguarantee. On the other hand, in the former case, we know that the error is at most 3∥A−[A]k∥2\nF\nm−kbut\ncould be much better if the learning augmented algorithm performed well.\nTo state our the exact result, we recall that the algorithm from Li et al. (2024) using space O(k2/ε4)\nmaintains a sketch of Asuch that from the sketch we can compute an estimate αsuch that ∥A−\n[A]k∥2\nF≤α≤(1 +ε)∥A−[A]k∥2\nF. We denote this algorithm Ares(k, ε). Our final algorithm is\nAlgorithm 3 for which we prove the following result.\nTheorem E.1. [Worst-Case guarantees] For any unit vector ⃗ x, the estimate Γof∥A⃗ x∥2\n2returned\nby Algorithm 3 satisfies\n|∥A⃗ x∥2\n2−Γ| ≤min\u0012\f\f∥A⃗ x∥2\n2− ∥B2x∥2\n2\f\f,6∥A−[A]k∥2\nF\nm−k\u0013\n.\nIn other words, the Error of Algorithm 3 is asymptotically bounded by the minimum of Algorithm 2\nand the classic Frequent Direction algorithm.\nProof. Suppose first that |∥B2⃗ x∥2\n2− ∥B1⃗ x∥2\n2| ≤2α. Then |∥A⃗ x∥2\n2−Γ|=\f\f∥A⃗ x∥2\n2− ∥B2x∥2\n2\f\f.\nMoreover, by the approximation guarantees of AresandAFD,\n\f\f∥A⃗ x∥2\n2− ∥B2x∥2\n2\f\f≤\f\f∥A⃗ x∥2\n2− ∥B1x∥2\n2\f\f+\f\f∥B1x∥2\n2− ∥B2x∥2\n2\f\f≤α+ 2α≤6∥A−[A]k∥2\nF\nm−k,\nas desired.\nSuppose on the other hand that |∥B2⃗ x∥2\n2− ∥B1⃗ x∥2\n2|>2α. Since by Fact 2.2, we always have that\f\f∥A⃗ x∥2\n2− ∥B1x∥2\n2\f\f≤∥A−[A]k∥2\nF\nm−k≤α, it follows that\f\f∥A⃗ x∥2\n2− ∥B2x∥2\n2\f\f> α≥∥A−[A]k∥2\nF\nm−k.\nBut since in this case, we output ∥B1⃗ x∥2\n2, the estimate of the standard frequent direction, we again\nhave by Fact 2.2 that\f\f∥A⃗ x∥2\n2− ∥B2x∥2\n2\f\f≤∥A−[A]k∥2\nF\nm−kas desired.\nWe note that the constant 6in the theorem can be replaced by any constant >3by increasing the\nspace used for Ares.\n25\n\nPublished as a conference paper at ICLR 2025\nAlgorithm 3 Robust Learning-based Frequent Direction ARLFD\n1:Procedure INITIALIZATION\n2: Input: sketch parameters m, d∈N+; learned oracle parameter mLs.t.,mL≤m; predicted\nfrequent directions PH= [⃗ w1|. . .|⃗ wmL]∈Rd×mL, query vector ⃗ x.\n3: Initialize an instance of Algorithm 1: AFD.initialization (m,0.5·m, d)\n4: Initialize an instance of Algorithm 2: ALFD.initialization (m,0.5·m, d)\n5: Initialize the residual error estimation algorithm Li et al. (2024) Ares(m/2,1)\n6:Procedure UPDATE\n7: AFD.update (Ai)\n8: ALFD.update (Ai)\n9: Ares.update (Ai)\n10:Procedure RETURN\n11: B1← A FD.return ()\n12: B2← A FD.return ()\n13: α0← A res.return ()\n14: α←α0\nm−k\n15: return (B1,B2, α)\n16:Procedure QUERY (⃗ x)\n17: if|∥B2⃗ x∥2\n2− ∥B1⃗ x∥2\n2| ≤2αthen\n18: return ∥B2⃗ x∥2\n2\n19: else\n20: return ∥B1⃗ x∥2\n2\nE.1 T HEERROR OF NON-PERFECT ORACLES .\nWe will now obtain a more fine-grained understanding of the consistency/robustness trade off of Al-\ngorithm 3. Consider the SVD A=P\ni∈[d]σi⃗ ui⃗ vT\ni.LetA↓.=APHPT\nHbe the matrix of projecting\nthe rows of Ato the predicted subspace, and A⊥.=A−A↓=A(I−PHPT\nH). Recall that PH\nconsists of orthonormal column vectors ⃗ w1, . . . , ⃗ w mL∈Rd. Extending this set of vectors to form\nan orthonormal basis of Rd:⃗ w1, . . . , ⃗ w mL, ⃗ wmL+1, . . . , ⃗ w d.WritePH= [⃗ wmL+1|. . .|⃗ wd]the\nprojection matrix to the orthogonal subspace.\nBased on Lemma D.1, for each vector, we can write\n⃗ xTATA⃗ x=⃗ xTAT\n↓A↓⃗ x+⃗ xTAT\n⊥A⊥⃗ x+ 2·X\ni∈[d]σ2\ni· ⟨PT\nH⃗ vi,PT\nH⃗ x⟩ · ⟨PT\nH⃗ vi,PT\nH⃗ x⟩. (73)\nTo understand the significance of Lemma D.1, note that our algorithm attempts to approximate the\nfirst two terms (through either exact or approximate Frequent Direction sketches), but ignores the\nfinal one. Therefore, regardless of how successful it is in approximating ⃗ xTAT\n↓A↓⃗ x+⃗ xTAT\n⊥A⊥⃗ x,\nwe will have 2·P\ni∈[d]σ2\ni· ⟨PT\nH⃗ vi,PT\nH⃗ x⟩ · ⟨PT\nH⃗ vi,PT\nH⃗ x⟩occurring as an additional added error.\nNote that ⟨PT\nH⃗ vi,PT\nH⃗ vj⟩is the inner product, between the projected vectors ⃗ viand⃗ vjto the sub-\nspace Hspecified by the predicted frequent directions, and that ⟨PT\nH⃗ vi,PT\nH⃗ vj⟩is the inner product,\nbetween the projected vectors ⃗ viand⃗ vjto the orthogonal complement of H. In particular, if PH\nconsists of a set of correctly predicted singular vectors of A, then for any i, either PT\nH⃗ viorPT\nH⃗ vi\nwill be zero and in particular the additional added error will be zero. In order to obtain an algo-\nrithm performing as well as if we had perfect predictions, it therefore suffices that the predictions\nare accurate enough that\n\f\f\f\f\f\fX\ni∈[d]σ2\ni· ⟨PT\nH⃗ vi,PT\nH⃗ vj⟩ · ⟨PT\nH⃗ vi,PT\nH⃗ vj⟩\f\f\f\f\f\f∈O\u0012∥A−[A]k∥2\nF\nm.\u0013\n(74)\n26\n\nPublished as a conference paper at ICLR 2025\nTo obtain a more general smoothness/robustness trade off, one can plug into Theorem E.1. Doing\nso in the setting of Theorem 3.4 where the singular values follow a Zipfian distribution, we obtain\nthe following immediate corollary.\nCorollary E.2. Consider the setting of Theorem 3.4, but where we run Algorithm 3 instead of\nAlgorithm 2 and where we make no assumptions on the quality of the oracle. Then the error\nErr(ARLFD )is at most\nO \n1\n(lnd)2·∥A∥2\nF\nm!\n+ 2X\ni∈[d]σ2\ni\n∥A∥2\nF·X\nj∈[d]σ2\nj· ⟨PT\nH⃗ vj,PT\nH⃗ vi⟩ · ⟨PT\nH⃗ vj,PT\nH⃗ vi⟩,\nbut also always bounded by\nO\n\u0010\nlnm\nlnd\nm\u0011\n·lnd\nm\n(lnd)2·∥A∥2\nF\nm\n.\nWe finish by showing an example demonstrating that even with very accurate predictions, the extra\nadded error can be prohibitive. Assume that the input space is R2, and the input vectors are either\n(1,0)or(0,1). Assume that σ2\n1= 107,σ2\n2= 1,,⃗ v1= (1,0), and⃗ v2= (0,1).\nIn this case, assume that mL= 1. A perfect PHshould be PH= (1,0), but we will assume\nthat the actual prediction we get is a little perturbed, say we change it to PH= (cos1\n100,sin1\n100).\nTherefore, PH= (sin1\n100,−cos1\n100),\nX\ni∈[2]σ2\ni· ⟨PT\nH⃗ vi,PT\nH⃗ v1⟩ · ⟨PT\nH⃗ vi,PT\nH⃗ v1⟩= 107· ⟨cos1\n100,cos1\n100⟩ · ⟨sin1\n100,sin1\n100⟩(75)\n+ 1· ⟨sin1\n100,cos1\n100⟩ · ⟨− cos1\n100,sin1\n100⟩(76)\n≈107cos21\n100·sin21\n100(77)\n≈107·1\n1002≈103. (78)\nIn general, assume that PH= (cos θ,sinθ)for small θ.The\nX\ni∈[2]σ2\ni· ⟨PT\nH⃗ vi,PT\nH⃗ v1⟩ · ⟨PT\nH⃗ vi,PT\nH⃗ v1⟩=σ2\n1cos2θ·sin2θ≈σ2\n1θ2(79)\nSo we need θ≈1/√m, in order that this bound is comparable with the normal FD bound.\n27\n\nPublished as a conference paper at ICLR 2025\nF A DDITIONAL EXPERIMENTS\nIn this section, we include figures which did not fit in the main text.\nF.1 D ATASET STATISTICS\n100101102103104105106\nSorted Elements101103105FrequencyCAIDA Log-Log Frequencies\n100101102103104105\nSorted Elements100101102103FrequencyAOL Log-Log Frequencies\nFigure 3: Log-log plot of frequencies for the CAIDA and AOL datasets.\n100101102103\nSorted Index102103104Singular ValueHyper Log-Log Singular Values\n100101102103\nSorted Index101102103104105Singular ValueLogo Log-Log Singular Values\nFigure 4: Log-log plot of singular values for the first Hyper and Logo matrices.\n100101102103\nSorted Index1010\n107\n104\n101\n102105Singular ValueEagle Log-Log Singular Values\nFigure 5: Log-log plot of singular values for the first Eagle and Friends matrices.\nF.2 N OISE ANALYSIS IN FREQUENT DIRECTIONS\nWe present the following figure for the\n28\n\nPublished as a conference paper at ICLR 2025\n105\n104\n103\n102\n101\nNoise103105107109Error\nNoise Analysis (Rank: 100)\nSVD\nFD\nLearned FD (ours)\nFigure 6: Analysis of prediction noise in matrix streaming on the first matrix of the Logo dataset.\nThe rank of the algorithms is 100. The baselines of Frequent Directions and the true SVD are shown\nas dashed lines. Our learned Frequent Directions algorithm uses perfect predictions corrupted by a\nmatrix of Gaussian noise with standard deviation σ/√\ndwhere σis displayed as the amount of noise\non the horizontal axis. The linear relationship on the log-log plot indicates that the performance of\nour algorithm decays polynomially with the amount of noise.\nF.3 A DDITIONAL FREQUENT DIRECTIONS EXPERIMENTS\nWe present plots of error/rank tradeoffs and error across sequences of matrices with fixed rank for\nall four datasets Hyper, Logo, Eagle, and Friends.\nF.3.1 H YPER DATASET\n0 10 20 30 40 50 60 70 80\nMatrices105\n104\n103\nError\nRank: 20\nSVD\nFD\nLearned FD (ours)\n0 10 20 30 40 50 60 70 80\nMatrices106\n105\n104\nError\nRank: 200\nSVD\nFD\nLearned FD (ours)\nFigure 7: Frequent directions results on the Hyper dataset.\nF.3.2 L OGO DATASET\n0 2 4 6 8 10 12 14 16 18\nMatrices105106107108Error\nRank: 20\nSVD\nFD\nLearned FD (ours)\n0 2 4 6 8 10 12 14 16 18\nMatrices100101102103104105Error\nRank: 200\nSVD\nFD\nLearned FD (ours)\nFigure 8: Frequent directions results on the Logo dataset.\n29\n\nPublished as a conference paper at ICLR 2025\nF.3.3 E AGLE DATASET\n0 2 4 6 8 10 12 14 16 18\nMatrices108\n105\n102\n101104107Error\nRank: 20\nSVD\nFD\nLearned FD (ours)\n0 2 4 6 8 10 12 14 16 18\nMatrices108\n105\n102\n101104Error\nRank: 200\nSVD\nFD\nLearned FD (ours)\nFigure 9: Frequent directions results on the Eagle dataset.\nF.3.4 F RIENDS DATASET\n0 2 4 6 8 10 12 14 16 18\nMatrices105106107Error\nRank: 20\nSVD\nFD\nLearned FD (ours)\n0 2 4 6 8 10 12 14 16 18\nMatrices101102103104105Error\nRank: 200\nSVD\nFD\nLearned FD (ours)\nFigure 10: Frequent directions results on the Friends dataset.\nF.4 A DDITIONAL FREQUENCY ESTIMATION EXPERIMENTS\nHere, we present all frequency estimation results comparing our Learned Misra-Gries algorithm with\nLearned CountSketch of Hsu et al. (2019) and Learned CountSketch++ of Aamand et al. (2023). We\npresent results both with and without learned predictions. Additionally, we present results both with\nstandard weighted error discussed in this paper as well as unweighted error also evaluated in the\nexperiments of prior work. The unweighted error corresponds to taking the sum of absolute errors\nacross all items appearing in the stream (not weighted by their frequencies).\nF.4.1 N OPREDICTIONS , W EIGHTED ERROR\n500 1000 1500 2000 2500 3000\nSpace0.20.40.60.81.0Weighted Error1e12\n Error/Space Tradeoff\nCS\nCS++ (C=1)\nCS++ (C=2)\nCS++ (C=5)\nMG (ours)\n0 10 20 30 40 50\nStreams2468Weighted Error1e11\n Space: 300\nCS\nCS++ (C=1)\nCS++ (C=2)\nCS++ (C=5)\nMG (ours)\nFigure 11: Frequency estimation on the CAIDA dataset with weighted error and no predictions.\n30\n\nPublished as a conference paper at ICLR 2025\n500 1000 1500 2000 2500 3000\nSpace012345Weighted Error1e7\n Error/Space Tradeoff\nCS\nCS++ (C=1)\nCS++ (C=2)\nCS++ (C=5)\nMG (ours)\n0 10 20 30 40 50 60 70 80\nStreams12345Weighted Error1e7\n Space: 300\nCS\nCS++ (C=1)\nCS++ (C=2)\nCS++ (C=5)\nMG (ours)\nFigure 12: Frequency estimation on the AOL dataset with weighted error and no predictions.\nF.4.2 W ITHPREDICTIONS , W EIGHTED ERROR\n500 1000 1500 2000 2500 3000\nSpace0.20.40.60.81.01.21.4Weighted Error1e12\n Error/Space Tradeoff\nCS\nCS++ (C=1)\nCS++ (C=2)\nCS++ (C=5)\nMG (ours)\n0 10 20 30 40 50\nStreams0.20.40.60.81.01.2Weighted Error1e12\n Space: 300\nCS\nCS++ (C=1)\nCS++ (C=2)\nCS++ (C=5)\nMG (ours)\nFigure 13: Frequency estimation on the CAIDA dataset with weighted error and learned predictions.\n500 1000 1500 2000 2500 3000\nSpace012345Weighted Error1e7\n Error/Space Tradeoff\nCS\nCS++ (C=1)\nCS++ (C=2)\nCS++ (C=5)\nMG (ours)\n0 10 20 30 40 50 60 70 80\nStreams012345Weighted Error1e7\n Space: 300\nCS\nCS++ (C=1)\nCS++ (C=2)\nCS++ (C=5)\nMG (ours)\nFigure 14: Frequency estimation on the AOL dataset with weighted error and learned predictions.\nF.4.3 N OPREDICTIONS , UNWEIGHTED ERROR\n500 1000 1500 2000 2500 3000\nSpace0123Unweighted Error1e10\n Error/Space Tradeoff\nCS\nCS++ (C=1)\nCS++ (C=2)\nCS++ (C=5)\nMG (ours)\n0 10 20 30 40 50\nStreams0.00.51.01.52.02.53.0Unweighted Error1e10\n Space: 300\nCS\nCS++ (C=1)\nCS++ (C=2)\nCS++ (C=5)\nMG (ours)\nFigure 15: Frequency estimation on the CAIDA dataset with unweighted error and no predictions.\n31\n\nPublished as a conference paper at ICLR 2025\n500 1000 1500 2000 2500 3000\nSpace0.00.51.01.52.02.53.0Unweighted Error1e7\n Error/Space Tradeoff\nCS\nCS++ (C=1)\nCS++ (C=2)\nCS++ (C=5)\nMG (ours)\n0 10 20 30 40 50 60 70 80\nStreams0.00.51.01.52.02.53.0Unweighted Error1e7\n Space: 300\nCS\nCS++ (C=1)\nCS++ (C=2)\nCS++ (C=5)\nMG (ours)\nFigure 16: Frequency estimation on the AOL dataset with unweighted error and no predictions.\nF.4.4 W ITHPREDICTIONS , UNWEIGHTED ERROR\n500 1000 1500 2000 2500 3000\nSpace012345Unweighted Error1e10\n Error/Space Tradeoff\nCS\nCS++ (C=1)\nCS++ (C=2)\nCS++ (C=5)\nMG (ours)\n0 10 20 30 40 50\nStreams01234Unweighted Error1e10\n Space: 300\nCS\nCS++ (C=1)\nCS++ (C=2)\nCS++ (C=5)\nMG (ours)\nFigure 17: Frequency estimation on the CAIDA dataset with unweighted error and learned predic-\ntions.\n500 1000 1500 2000 2500 3000\nSpace0.00.51.01.52.02.53.0Unweighted Error1e7\n Error/Space Tradeoff\nCS\nCS++ (C=1)\nCS++ (C=2)\nCS++ (C=5)\nMG (ours)\n0 10 20 30 40 50 60 70 80\nStreams0.00.51.01.52.02.53.0Unweighted Error1e7\n Space: 300\nCS\nCS++ (C=1)\nCS++ (C=2)\nCS++ (C=5)\nMG (ours)\nFigure 18: Frequency estimation on the AOL dataset with unweighted error and learned predictions.\n32",
  "textLength": 80480
}