{
  "paperId": "4084114115b5e209b727265de5a0617a53be907a",
  "title": "Learning to Control Latent Representations for Few-Shot Learning of Named Entities",
  "pdfPath": "4084114115b5e209b727265de5a0617a53be907a.pdf",
  "text": "Learning to Control Latent Representations\nfor Few-Shot Learning of Named Entities\nOmar U. Florez and Erik Mueller\nConversational AI Research\nCapital One\nomar.florezchoque@capitalone.com\nAbstract\nHumans excel in continuously learning with small data without forgetting how to\nsolve old problems. However, neural networks require large datasets to compute\nlatent representations across different tasks while minimizing a loss function. For\nexample, a natural language understanding (NLU) system will often deal with\nemerging entities during its deployment as interactions with users in realistic\nscenarios will generate new and infrequent names, events, and locations. Here, we\naddress this scenario by introducing an RL trainable controller that disentangles\nthe representation learning of a neural encoder from its memory management role.\nOur proposed solution is straightforward and simple: we train a controller to\nexecute an optimal sequence of reading and writing operations on an external\nmemory with the goal of leveraging diverse activations from the past and provide\naccurate predictions. Our approach is named Learning to Control (LTC) and allows\nfew-shot learning with two degrees of memory plasticity. We experimentally show\nthat our system obtains accurate results for few-shot learning of entity recognition\nin the Stanford Task-Oriented Dialogue dataset.\n1 Motivation\nToday, supervised models have problems incorporating new tasks over time while protecting previ-\nously acquired knowledge. This is because these algorithms require that all data is given prior to\ntraining. This becomes a problem in the presence of more general scenarios in which new classes\nemerge during training or data exhibit long-tailed distributions. Hence, classes with large sup-\nport dominate the learning of gradient-based representations causing the catastrophic forgetting of\nunder-represented classes (Kirkpatrick et al. (2016)).\nUnlike deep neural networks, humans and other mammals excel in learn incrementally with small\ndata. Biological evidence suggests that the process of acquiring new skills occurs in different brain\nareas with at least two degrees of plasticity (Wixted et al .(2018)): 1) A slow memory that is dense and\nrequires extensive practice. 2) A fastmemory that is sparse and stores volatile information. Inspired\nby these observations, we introduce a trainable controller, Learning to Control (LTC), that learns to\ninteract with both a slow memory (consisting of neural encoders) and a fastmemory (consisting of an\nassociative array storing key-value pairs). We experimentally show the advantage of using LTC for a\nfew-shot learning setup. Figure 1 depicts the network architecture of our model.\nWe present the following contributions:\n\u000fWe propose a novel architecture that uses a trainable controller to manipulate latent repre-\nsentations in external memory (Section 3).\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv:1911.08542v1  [cs.LG]  19 Nov 2019\n\n\u000fWe introduce the use of a reward signal that is proportional to the average reduction of\nentropy when attending the memory entries. This enables us to propose a reinforcement\nlearning approach that learns a policy based on interactions with external memory.\n\u000fWe show the generality of our solution for the few-shot learning of entities in the Stanford\nTask-Oriented Dialogue dataset.\n2 Dense and Sparse Memories\nThe proposed architecture considers the use of two types of memories that behave differently during\nbackpropagation. First, we deﬁne memory as follows.\nDeﬁnition 2.1. (Memory) Given a neural model f(x;y;\u0012 )that maps an observation xto the learning\ntaskyand is parameterized by \u0012. Then, the memory of f(x;y;\u0012 )is a collection of co-activations\nresponding similarly to the reocurring patterns associated to the learning task y.\nA Neural Network uses most of its memory to remember patterns in the dataset. However, our\napproach distinguishes between dense andsparse memories.\nDeﬁnition 2.2. (Dense Memory) A dense memory is a type of memory, in which all its learnable\nparameters\u0012have the possibility of being updated during training.\nWe want to preserve latent representations of the input between training steps, but a dense memory\nwill likely overwrite them based on frequent global updates. We then need a sparse memory with\naddressable memory entries and sparse updates.\nDeﬁnition 2.3. (Sparse Memory) Given a model f(x;y;\u0012 ), leta(x;y)be a memory operation\nwhich returns kupdating parameters to incorporate the pair (x;y). Then, a sparse memory is a type\nof memory that satisﬁes the sparse updating constraint :\n0<ja(x;y)j\u001cj\u0012j\nwherej\u0012jis the total number of trainable parameters.\nWe use the sparse memory introduced in Kaiser et al .(2017) which consists of two arrays of size\nmem_size for storing keys ( K) and values ( V). Each key has a dimensionality of key_dim and\neach value is a scalar representing a class label. In our experiments, we ﬁxed the number of updating\nparameters, i.e. keys, to be very small ( k= 10 ) in comparison to mem_size in order to satisfy the\nsparse updating constraint ,k\u001cmem_size . The ﬁnal form of the sparse memory is as follows.\nM:= (Kmem _size\u0002key_dim;Vmem _size)\nThe state of a sparse memory changes based on the following memory operations.\nDeﬁnition 2.4. (Memory Operations) Letsbe a input embedding that we want to incorporate into\nM,ybe a class label, and ibe the memory index of the most similar key to sbased on the Cosine\nsimilarity.\n1. IfV[i]6=y, thewrite (i;s;y )operation registers a new class with M[i] =hs;yi.\n2.IfV[i] =y, theupdate (i;s;y )operation modiﬁes the stored key-based representation K[i]\nwithM[i] =hkK[i] +sk;yi.\n3 Learning to Control\nThe goal of Learning To Control (LTC) is to learn a strategy to manage the entries of a sparse memory.\nWe characterize this decision process with a memory policy \u0019\u0012, which represents the probability of\nrunning a memory operation given the current state of the memory. Rather than following a heuristics\n(e.g., overwriting in the oldest memory entry as in Kaiser et al .(2017)), learning such policy has\nthe advantage of optimizing the allocation of infrequent classes considering the limited number of\nmemory entries in M. Indeed,\u0019\u0012transitionsMto a new conﬁguration in which its keys are more\nadapted to store and maintain informative latent vectors. To clarify the connection between the\ncomponents, let us consider the forward and backward computations involved in LTC.\n2\n\nFigure 1: Architecture of our approach based on a neural encoder that generates input representations\nand a trainable controller that learns to store and maintain them in an external memory formed by\nkey-value pairs. The memory is modiﬁed via operations a, each of which returns a reward signal r.\n3.1 Forward Computation\nAs illustrated in Figure 1, information ﬂows through the hidden layers of the neural encoder, feeds the\ncontroller, and reaches the external memory M. The encoder is a Long-Short Term Memory (LSTM)\nneural network that transforms the raw input xinto the last hidden state of the following recurrence\nhi=LSTM (x;hi\u00001), wheres=hiis the state of the environment prior to any memory operation\nand corresponds to the last hidden state of the LSTM encoder.\nWhich memory entries should the controller update to incorporate the learning task (s;y)? Our\napproach is to induce the probablity distribution \u0019\u0012(ijs)over the memory entries given the state s\nand sample from there the most likely memory index i\u0019\u0012\n\u0019\u0012(ijs) =Softmax (s\u0001Ki)\ni\u0019\u0012\u0018argmaxi(\u0019\u0012(ijs)):\nThen, the controller executes the memory operation a(i\u0019\u0012;s;y)at positioni\u0019\u0012considering the\nDeﬁnition 2.4. During inference, ^y=V[i\u0019\u0012]returns the predicted class of the raw input x.\n3.2 Backward Computation\nThe forward step performs an operation a(i\u0019\u0012;s;y)in the sparse memory and modiﬁes its keys\nto integrate latent representations. The backward step estimates the objective function J(\u0012)and\ncomputes its gradients with respect to the trainable parameters of the encoder and controller modules.\nIn the case of the controller, we want to compute the gradient vector r\u0012J(\u0012)that updates the\nstochastic policy \u0019\u0012in a direction that maximizes J(\u0012)during few-shot learning. For our purposes,\neach memory operation areturns a reward rthat measures the efﬁcacy of this operation to reduce\nuncertainty about which of the keys in Kwill lead the transition between memory states. This means\ngoing from a state in which no operation took place in memory ( Kt\u00001) to an updated state ( Kt).\nMore formaly, our reward function rt(s;a)corresponds to the reduction of entropy Hwhen softly\nattending over the memory entries before and after executing the operation a.\nhKt=s\u0001Kt\nattKt=Softmax (hKt)\nWe use the dot-product attention score (Luong, Pham, and Manning (2015)) to compute the logits\nhKtover the memory keys and use Softmax to generate the probability distribution attKt. Zhu et\nal.(2005) demonstrates that models that show high entropy in their output distributions often fail to\ndiscriminate classes because of exhibiting uniform-like distributions and a high degree of uncertainty.\nThus, we use the following entropy-based reward signal to encourage the use of few memory entries\nto modify the sparse memory M.\nrt=\u0000\u0000\nH(attKt)\u0000H(attKt\u00001)\u0001\n3\n\nModel 5-way 1-shot 5-way 5-shot 20-way 1-shot 20-way 5-shot\nMemory Augmented NN 63.1% 66.7% 57.2% 61.3%\nMatching Networks 67.3% 71.4% 62.2% 68.3%\nLearning to Control 71.5% 77.1% 65.3% 75.8%\nTable 1: Average accuracy for few-shot learning in the STDO dataset.\nBased on memory operations, we can approximate the objective function from a batch of sampled\nepisodes of length Tas the expected value of the cumulative sum of rewards under the policy \u0019\u0012,\nJ(\u0012) =E\u0019\u0012\"TX\nt=1rt(s;a)#\n:\nThe REINFORCE algorithm (Williams (1992)) allows us to directly take the gradients of the objective\nfunctionJ(\u0012)as described in Equation 1 and used them to perform backpropagation.\nr\u0012J(\u0012)\u00191\nNNX\nn=1 TX\nt=1r\u0012log\u0019\u0012(it\nnjst\nn)! TX\nt0=trt0(st0\nn;at0\nn)!\n(1)\n4 Experiments\nWe evaluate our proposed method on the Stanford Task-Oriented Dialogue Dataset (STDO) (Eric et al .\n(2017)), which consists of 3;031dialogues in the domain of an in-car assistant that provides automatic\nresponses to the requests of a driver considering the weather, point-of-interest, and scheduled domains.\n4.1 Baselines\nWe compare Learning to Control (LTC) with the following baselines: 1) Matching Networks\n(MN) (Vinyals et al .(2016)): This algorithm provides one-shot learning capabilities by jointly\nmapping a small labeled support set and an unlabeled example to its most likely label, and 2)\nMemory Augmented Neural Networks (MANN) (Santoro et al .(2016)): This algorithm is designed\nto provide one-shot learning based on a Neural Turing Machine and a curriculum training regime.\n4.2 Few-Shot Learning for Entity Recognition\nWe study the problem of sequence labeling to ﬁnd the best label sequence (named entities) for a given\ninput sentence considering an LSTM encoder augmented with an external memory to store hidden\nrepresentations for each recurrent unit. We use the Stanford Named Entity Recognizer (NER)1to\naugment the STDO dataset with 7classes (Location, Person, Organization, Money, Percent, Date,\nTime) and a non-entity class. This results in 15;928observations, 8classes, and an average sequence\nlength of 44 words.\nWe change the format of the STOD dataset to simulate a scenario in which classes obtain incremental\nsupport during training. This is done by forming episodes of labeled examples that show a uniform\ndistribution over kclasses (e.g., 5-way), so we can track the number of times a particular class was\npresented to the model during training (e.g., 1-shot learning). This format was proposed by Santoro\net al.(2016) to study few-shot learning . The goal of this experiment is to learn with small data, so\nwe study the learning behavior of LTC with an external memory of 1;000entries and in relation\nto two models designed to provide few-shot learning capabilities: Memory Augmented Neural\nNetworks (Santoro et al. (2016)) and Matching Networks (Vinyals et al. (2016)).\nTable 1 shows that LTC provides an average improvement of +5:2in the STOD dataset with respect\nto the MN neural model, which is consistently the second most accurate option for both 5-way and\n20-way classiﬁcation and when the number of classes ranges from small (5-way) to large (20-way).\n1https://nlp.stanford.edu/software/CRF-NER.html\n4\n\nSimilarly, LTC also shows an average improvement of +10:35% with respect to the MANN model.\nFor the scenario of learning with infrequent classes, LTC outperforms MN when only 1 training\nobservation is presented (1-shot) and 5 and 20 classes are available during training by +4:2%and\n+3:1%, respectively. The performance advantage in the results can be explained by the extra memory\ncapacity of LTC and represent an opportunity to explore later what is the minimum memory size to\ncontinuously learn with small data, an even more challenging problem to study in the future.\n5 Related Work\nThe idea of training a controller that interacts with a memory to incrementally allocate distributional\nrepresentations builds upon a wide range of research in machine learning and memory augmented\nalgorithms.\nKraska et al .(2018) introduced the concept of learned indices structures as a learning architecture\nthat predicts the position or existence of records. Their main goal is to prevent too many collisions\nfrom being mapped to the same position inside the hash index. This is because collisions often\ncause memory overhead when traversing a linked list or require the allocation of additional memory\nfor storing more records. They approach this problem as supervised learning of the cumulative\ndistribution function of hash keys, which leads to minimizing the number of collisions. In contrast,\nwe propose the use of an associative array as an indexed structure that supports generalization, so\nthe collision of multiple observations to a similar hash key if adequate for predicting the class of\nsimilar input data. While the idea of learning hash functions as neural networks is not new, existing\nwork mainly focused on learning a better hash-function to map observations into low-dimensional\nembeddings for similarity search assuming a ﬁxed data distribution (Qian et al .(2014)). To our\nknowledge, it has not been explored yet if it is possible to learn a hash index according to a data\ndistribution in which few training observations are presented per class during training.\nDeep Neural Networks are models that solve classiﬁcation problems with non-linearly separable\nclasses. These are shown to be good for problems such as object detection (Krizhevsky, Sutskever, and\nHinton (2017)), language translation (Johnson et al .(2016)) and image generation (Goodfellow et al .\n(2014)). Recently, recurrent neural networks (RNN) have been studied to manage the states between\ntime steps using an attention mechanism that addresses similar content (Bahdanau, Cho, and Bengio\n(2014)). Some similar approaches have also studied the problem of few-shot learning (Santoro et al .\n(2016), Koch, Zemel, and Salakhutdinov (2015), Vinyals et al .(2016)). A representative example is\nMatching Networks (Vinyals et al .(2016)), which uses an attention mechanism to augment neural\nnetworks for set-to-set learning.\nA relevant research topic to this work is the idea of memory augmented networks that extend its\ncapabilities by coupling an external memory. For example, Kaiser et al .(2017) also proposes a\nkey-value memory, but with no controller mechanism for adequate training of few-shot learning tasks.\nAlso, Neural Turing Machines (NTM) (Graves, Wayne, and Danihelka (2014)) are differentiable\narchitectures allowing efﬁcient training with gradient descend and showing important properties for\nassociative recall for learning different sequential patterns. Although they have important properties\nfor one-shot learning, given their sequential memory management, the supervised nature of its training\nstill shows issues related to catastrophic forgetting.\n6 Conclusions\nLearning new tasks without forgetting the previously ones references the plasticity of our own brain to\nretain previously acquired knowledge. In this work, we show a learning system that stores information\nabout named entities with two levels of representation: dense and sparse. While a dense memory\ncaptures features that are shared across tasks, sparse memory incorporates embedding vectors and\nprevents them from being overwritten by global updates during backpropagation. To do this, we rely\non the sparse structure of associative arrays to locally update a small set of memory entries and on the\nuse of a trainable controller that manages the transition of the sparse memory to a state of low entropy\nthat also addresses the learning task. This enables a model to incrementally learn in the presence\nof a few observations per class. We experimentally show that our system obtains accurate results\nfor few-shot learning of entities in the Stanford Task-Oriented Dialogue dataset in comparison with\n5\n\nstate-of-the-art few-shot learning algorithms (Memory Augmented Neural Networks and Matching\nNetworks).\nReferences\nBahdanau, D.; Cho, K.; and Bengio, Y . 2014. Neural machine translation by jointly learning to align\nand translate. CoRR abs/1409.0473.\nEric, M.; Krishnan, L.; Charette, F.; and Manning, C. D. 2017. Key-value retrieval networks for\ntask-oriented dialogue. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and\nDialogue , 37–49. Saarbrücken, Germany: Association for Computational Linguistics.\nGoodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.;\nand Bengio, Y . 2014. Generative adversarial nets. In Advances in Neural Information Processing\nSystems 27 . 2672–2680.\nGraves, A.; Wayne, G.; and Danihelka, I. 2014. Neural turing machines. CoRR abs/1410.5401.\nJohnson, M.; Schuster, M.; Le, Q. V .; Krikun, M.; Wu, Y .; Chen, Z.; Thorat, N.; Viégas, F.;\nWattenberg, M.; Corrado, G.; Hughes, M.; and Dean, J. 2016. Google’s multilingual neural\nmachine translation system: Enabling zero-shot translation. Technical report, Google.\nKaiser, L.; Nachum, O.; Roy, A.; and Bengio, S. 2017. Learning to remember rare events.\nKirkpatrick, J.; Pascanu, R.; Rabinowitz, N. C.; Veness, J.; Desjardins, G.; Rusu, A. A.; Milan, K.;\nQuan, J.; Ramalho, T.; Grabska-Barwinska, A.; Hassabis, D.; Clopath, C.; Kumaran, D.; and\nHadsell, R. 2016. Overcoming catastrophic forgetting in neural networks. CoRR .\nKoch, G.; Zemel, R.; and Salakhutdinov, R. 2015. Siamese neural networks for one-shot image\nrecognition. PhD Thesis.\nKraska, T.; Beutel, A.; Chi, E. H.; Dean, J.; and Polyzotis, N. 2018. The case for learned index\nstructures.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2017. Imagenet classiﬁcation with deep convolutional\nneural networks. Commun. ACM 60(6).\nLuong, T.; Pham, H.; and Manning, C. D. 2015. Effective approaches to attention-based neural\nmachine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural\nLanguage Processing , 1412–1421. Lisbon, Portugal: Association for Computational Linguistics.\nQian, Q.; Hu, J.; Jin, R.; Pei, J.; and Zhu, S. 2014. Distance metric learning using dropout: A\nstructured regularization approach. In Proceedings of the 20th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining , KDD ’14, 323–332.\nSantoro, A.; Bartunov, S.; Botvinick, M.; Wierstra, D.; and Lillicrap, T. 2016. Meta-learning with\nmemory-augmented neural networks. In Proceedings of the 33rd International Conference on\nInternational Conference on Machine Learning - Volume 48 , ICML’16, 1842–1850.\nVinyals, O.; Blundell, C.; Lillicrap, T.; kavukcuoglu, k.; and Wierstra, D. 2016. Matching networks\nfor one shot learning. In Advances in Neural Information Processing Systems .\nWilliams, R. J. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Mach. Learn. 8(3-4).\nWixted, J. T.; Goldinger, S. D.; Squire, L. R.; Kuhn, J. R.; Papesh, M. H.; Smith, K. A.; Treiman,\nD. M.; and Steinmetz, P. N. 2018. Coding of episodic memory in the human hippocampus.\nProceedings of the National Academy of Sciences .\nZhu, S.; Ji, X.; Xu, W.; and Gong, Y . 2005. Multi-labelled classiﬁcation using maximum entropy\nmethod. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research\nand Development in Information Retrieval .\n6",
  "textLength": 20552
}