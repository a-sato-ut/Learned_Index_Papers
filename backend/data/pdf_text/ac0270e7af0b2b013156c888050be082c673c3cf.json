{
  "paperId": "ac0270e7af0b2b013156c888050be082c673c3cf",
  "title": "A Survey of Learned Indexes for the Multi-dimensional Space",
  "pdfPath": "ac0270e7af0b2b013156c888050be082c673c3cf.pdf",
  "text": "A Survey of Learned Indexes for the Multi-dimensional Space\nABDULLAH-AL-MAMUN, Purdue University, USA\nHAO WU, Keystone Strategy, USA\nQIYANG HE, Snowflake, USA\nJIANGUO WANG, Purdue University, USA\nWALID G. AREF, Purdue University, USA\nA recent research trend involves treating database index structures as Machine Learning (ML) models. In this domain, single or multiple\nML models are trained to learn the mapping from keys to positions inside a data set. This class of indexes is known as ‚ÄúLearned\nIndexes. \" Learned indexes have demonstrated improved search performance and reduced space requirements for one-dimensional data.\nThe concept of one-dimensional learned indexes has naturally been extended to multi-dimensional (e.g., spatial) data, leading to the\ndevelopment of ‚ÄúLearned Multi-dimensional Indexes\". This survey focuses on learned multi-dimensional index structures. Specifically,\nit reviews the current state of this research area, explains the core concepts behind each proposed method, and classifies these methods\nbased on several well-defined criteria. We present a taxonomy that classifies and categorizes each learned multi-dimensional index, and\nsurvey the existing literature on learned multi-dimensional indexes according to this taxonomy. Additionally, we present a timeline to\nillustrate the evolution of research on learned indexes. Finally, we highlight several open challenges and future research directions in\nthis emerging and highly active field.\n1 INTRODUCTION\nRecently, due to tremendous progress in the field of machine learning, two research trends have emerged in the area of\nDatabase Systems (DB, for short): ML for DB, and DB for ML. In ML for DB, the goal is to develop ML-enhanced core\ndatabase systems components, e.g., learned indexes [ 97], learned query optimizers [ 132], and self-driving databases [ 149].\nThese initial learned systems components have demonstrated superior performance compared to their traditional\ncounterparts. On the other hand, in DB for ML, the objective is to extend the traditional DB architecture, components,\nand query languages to support efficient in-database ML workloads [29, 85, 112].\nThis survey paper focuses on ML for DB, particularly on the idea of replacing traditional database index structures\n(e.g., B-tree [ 17,35]) with ML models, first proposed in [ 97]. The proposed Recursive Model Index (RMI, for short) can\nbe considered as the first instance of a ‚ÄúLearned Index\". Note that traditional indexes provide theoretical guarantees\non performance, are well-studied, and have been successfully integrated into practical data systems. In contrast, pure\nlearned indexes learn the key-to-position mapping with some error-correction mechanisms to achieve better search\nperformance, and reduce space requirements. On the other hand, there are hybrid learned indexes that optimize\ntraditional indexes with helper ML models. The spectrum of learned indexes is given in Figure 1.\nAlthough the term ‚ÄúLearned Indexes\" has been coined very recently, the concept of using a learning mechanism in\nthe context of database indexing has been studied previously. An example of an earlier index structure that combines\nML techniques in the context of database indexing is the handwritten trie [ 13]. The handwritten trie employs Hidden\nMarkov Models (HMM) [ 157] on a trie[ 58] structure to index the learned models. Notice that this handwritten trie\nfocuses on the idea of indexing the learned models instead of learning the index . On the other hand, one of the earliest\nAuthors‚Äô addresses: Abdullah-Al-Mamun, Purdue University, West Lafayette, USA, mamuna@purdue.edu; Hao Wu, Keystone Strategy, San Francisco,\nUSA, Hao Wu contributed to this paper while studying at Purdue University, haowu3@alumni.cmu.edu; Qiyang He, Snowflake, Bellevue, USA, Qiyang\nHe contributed to this paper while studying at Purdue University, qiyang.he@snowflake.com; Jianguo Wang, Purdue University, West Lafayette, USA,\ncsjgwang@purdue.edu; Walid G. Aref, Purdue University, West Lafayette, USA, aref@purdue.edu.\n1arXiv:2403.06456v1  [cs.DB]  11 Mar 2024\n\n2 Abdullah-Al-Mamun, et al.\nHybrid Learned Index Traditional IndexInput: Search \nKey\n(e.g., Id)\nIndexData Input: Search \nKey\n(e.g., Id)\nIndexDataHelping ML\nModel\nDataInput: Search \nKey\n(e.g., Id)ML \nModel\nPredict the position of \nkey on  disk/memory\nPure Learned Index\nFig. 1. Spectrum of learned indexes\npapers on a distribution-aware index structure for spatial (i.e., multi-dimensional) data can be found in [ 16]. In [ 16],\nthe proposed technique combines an R-tree [ 69] with a self-organizing map [ 93]. Moreover, the initial promising\nresults of one-dimensional learned indexes [ 97] have inspired researchers to extend the concept in the context of\nmulti-dimensional data. As a result, various methods for learned multi-dimensional indexes have been introduced in\nthe recent years. One of the key assumptions in one-dimensional learned indexes is that the data can be sorted (i.e.,\ntotally ordered). However, there is no obvious total sort order for multi-dimensional data. As a result, it is challenging\nto define an error-correction mechanism in case of mis-predictions. Moreover, the layout of the multi-dimensional data\nmight need to be re-arranged based on a pre-defined mechanism so that it can be easily learned by ML models. The\nchoice of ML model might also vary from learned one-dimensional indexes due to the impact of dimensionality. In\nsummary, learned multi-dimensional indexes need to address additional research challenges that are not associated\nwith learned one-dimensional indexes. In this survey, we distinguish between multi- and high-dimensional data. In the\ncontext of multi-dimensional data, we assume that the dimension of the data is typically between 2to10. On the other\nhand, the dimension of the data can be very high (e.g., 100) in the context of high-dimensional data. Although there are\nstudies related to ML-enhanced high-dimensional indexes, we have not included them in this study unless some of the\ntechniques in this domain have influenced the research of learned multi-dimensional indexes.\nIn this paper, we present a comprehensive survey of recent advances in the area of learned multi-dimensional indexes\nusing that taxonomy given in Figure 2. In the taxonomy, we distinguish between two concepts: indexing the learned\nmodels vs. learning the index . By indexing the learned models, we refer to the following problem. Assume that we\nare given a collection of learned models, where each model can recognize a certain object class, e.g., a learned model\nthat recognizes the class car, another recognizes the class bike, etc. Given a query object, one needs to execute each of\nthe models to identify the model that produces the highest recognition score. The problem can be formulated as ‚ÄúCan\nwe index the learned models to accelerate the model identification process?‚Äù In contrast, by learning the index , we refer to\nthe problem of replacing a traditional database index structure with an ML model. For example, instead of searching a\nB+-tree to locate the leaf page that contains an input search key, one uses an ML model that predicts the location that\ncontains the search key.\nIn the taxonomy, in the context of learning the index, we further distinguish among learned indexes along the\nfollowing dimension: learned indexes that support static datasets (i.e., Immutable) vs. learned indexes that\nsupport inserts/updates (i.e., Mutable) . The issue of supporting static vs. dynamic datasets is crucial because learning\nan index requires offline training that is relatively slow in nature. Thus, learned indexes that support inserts/updates\nneed to accommodate this fact, yet still offer online responses. In our taxonomy, mutable learned indexes are further\n\nA Survey of Learned Indexes for the Multi-dimensional Space 3\nCDFShop  [133]\nOne-d\nMulti -dImmutable\nMutable\nDynamic Data LayoutFixed Data Layout\nALEX[44]Pavo[203]\nDrift Model[71]\nIFB-tree[72]Doraemon[182]\nLISA[110]Case Spatial[147]\nPGM[57]\nMADEX[73]AccB+tree[123]RMI[97]\nHandwrittenTrie[13]\nBenchmarkingASLM[113]\nHybrid -LR[155]XIndex *[183]SIndex *[194]\nBF-Sandwich[136]LEARNED INDEX ESLearning the \nIndex\nIndexing \nLearned Models\nRSMI[153]FITing -tree[60]SageDB -LMI[96]ZM-index[190] ML-index[41]\nLearnedBF[129]Pure\nHybrid\nPure\nHybridProjected space\nNative space\nProjected space\nNative space\nOne-d\nMulti -dOne-d\nOne-d\nMulti -dMulti -dPure\nHybrid\nPure\nHybrid\nPure\nHybrid\nPure\nHybridProjected space\nNative space\nProjected space\nNative spaceProjected space\nProjected space\nNative space\nMTO[43]ML-enhanced[88]‚ÄúAI+R‚Äù -tree[7]\nPeriod -index[19]Distance -bounded[212]\nPolyFit[114]HAP[120]\nSpatial -LS[148]\nWaffle[34]Z-index[146]WAZI[145]\nCOAX[70]FI-index[173]\nIF-X[75]\nRW-tree[48]LSTI[46]\nWISK[174]\nRLR -tree[67]GLIN_ALEX[189] BayesTree[171]LIFOSS[211]\nBOURBON[38]TridentKV[125]SA-LSM[220]CARMI*[216]APEX*[124]\nLIPP[200]FINEdex *[108]MetaBF[158]\nStableBF[121]\nAdaBF[39]PLBF[187]ConcurrentLI *[196]\nSieve[185]TALI[68]\nDILI*[111]FILM[128]DiffLex[37]PLIN*[222]\nELSI[116]MdlReuse[117]\nTowards Practical Systems \nImplementationOne-d\nMulti -dGoogle -index[4]HM-index[192]\nSLBRIN[191]EWALI*[119]SALI*[63]Google -index[4]\nBOURBON[38]SA-LSM[220]\nSageDB -LMI [96]FHSIE[180]Compress -LBF [40]\nACR -tree[78]LeanedKD[209]\nLIMS[184]LMI-1[12]\nLPBF[227] PA-LBF[214]SNARF[186]IA-LBF[23] PDDBF[215]PLBF++[169]\nLIPP+*[199]ALEX +*[199]Hybrid_RMI[97]\nLBF[97]HMI[97]\nRS [92]LSI[89]\nRSS[177]PLEX[178] ShiftTable[74]B-tree\nBloom filterHist-tree\nRadix tableLookup table\nRadix table\nGrid -based\nR-tree Quad -tree KD-tree M-tree\nIn place\nDelta buffer\nB+-tree\nLSM -tree\nMusic Retrieval [86]Bloom Filter\nS3-SkipList[217] HERMIT[202]FLIRT[205]RUSLI[134] Skip list Radix table Circular Queue TRS -tree\nIn place\nDelta bufferNative space\nBRIN Radix table\nR-tree Grid -based\nIn place\nDelta buffer\nIn place\nDelta buffer\nProjected space\nNative space\nCounting Bloom Filter\nR-tree\nGrid -basedSOSD[91] Critical -RMI[130]\nBenchmark -1[131]Updateable[199]\nMicro -Archi[11] ExpEval -1[181]\nExpEval -2[99] Lhash -1[163]\nLhash -2[164] CuttingLI[62]\nTowardsBenchmark[24]BMT[106]LMSFC[61]\nPLATON[206]\nPLATON [206]TONE[221]HLI[47]\nCOLIN[223]AELH[115]\nWIPE*[197]B-tree\nSieve[185]Hybrid -LR[155]Qd-tree[207]LMI-2[176]\nSPRIG[218]SPRIG+[219]Flood[141]Tsunami[45]\nAIDEL[109]\nNFL[201]\nFig. 2. Taxonomy of learned indexes. The end of a branch indicates that there are no papers in that category as of the time this article\nhas been written. An asterisk (*) symbol is used if the index natively supports concurrency. The hybrid learned indexes are categorized\nbased on their underlying traditional data structures.\ndivided into fixed vs. dynamic data layout . A fixed data layout refers to the class of indexes where the layout of the\ndata and the structure of the index are fixed before the index-building phase. On the other hand, if the layout of the\ndata is arranged/re-arranged by the ML models while building the learned index, we refer to them as having a dynamic\ndata layout. For example, in Figure 3a, the initial fixed data layout is re-arranged using an ML model.\nNext, we distinguish between learned indexes along the dimensionality of the data: learned indexes for one-\ndimensional data vs. learned indexes for multi-dimensional data .\nUnder each category, we further classify the indexes into pure vs. hybrid learned indexes . Pure learned indexes\nare designed without leveraging a traditional index structure (e.g., a B-tree or an R-tree). Moreover, pure learned indexes\nare designed to replace a traditional index structure. In contrast, a hybrid learned index combines a traditional index\nstructure with ML models to build an ML-enhanced index structure. As a result, hybrid learned indexes are categorized\nfurther based on their underlying traditional data structure .\nOn the other hand, mutable pure learned indexes are classified based on their policy for supporting new data\ninsertions: in-place vs. delta buffer insertion policies . The in-place insertion policy uses ML models to find the\nposition where the new data item should be inserted. Additionally, an in-place insertion policy might reserve some\ngaps in the index so that insert operations can be accommodated immediately. Conversely, the delta buffer insertion\npolicy employs fixed-size buffers to hold the new data items for a short period of time. These temporary buffers are\nsynchronized with the index structure periodically (e.g., during ML model re-training). However, we do not classify\nmutable hybrid indexes based on insertion strategies due to the presence of a traditional index, where that might\n\n4 Abdullah-Al-Mamun, et al.\n(a) Fixed vs. Dynamic data layout\n(b) In-place vs. Delta buffer-based insertion\n(c) Native vs. Projected space\nFig. 3. Illustration of the taxonomy criteria\ninfluence the choice of the insertion strategy. The difference between the two insertion policies are illustrated with the\nexample given in Figure 3b.\nWe further classify the learned multi-dimensional indexes into two classes: indexes operating in the native space\nof the data vs. a projected space of the data . Here, if the ML models are trained on the original representation of\nthe multi-dimensional data, we refer to them as indexes built in native space. Conversely, if the multi-dimensional\ndata is projected or linearized into a projected space using a projection function or a Space Filling Curve (SFC, for\nshort) [138, 165], we refer to them as indexes operating in a projected space (Figure 3c).\nAdditionally, in the taxonomy, we highlight studies related to benchmarking learned one-dimensional indexes. At\nthe time of writing this article, we are not aware of any comprehensive benchmarking study in the area of learned\nmulti-dimensional indexes. We also highlight and discuss studies related to the integration of learned indexes into\npractical systems.\nContributions . The main contributions of this survey are as follows. We provide an up-to-date coverage of learned\nmulti-dimensional indexes (until the end of 2023). We present a taxonomy of existing learned indexes in both the\none- and multi-dimensional spaces. We expect that upcoming learned index structures can be categorized within the\ntaxonomy based on the proposed criteria.\nPaper Organization . The rest of this paper is organized as follows: Section 2 provides an overview of related\nsurveys and tutorials. Section 3 presents a timeline of the evolution of learned indexes. Section 4 presents existing work\nin the area of indexing the learned models. Sections 5, 6, and 7 present both one-dimensional and multi-dimensional\nindexes according to the taxonomy in Figure 2. The properties of immutable and mutable learned multi-dimensional\nindexes are summarized in Tables 1, 2, and 3. Section 8 highlights the various ML techniques used in existing learned\nmulti-dimensional indexes. Section 9 discusses studies related to benchmarking learned indexes. Section 10 presents\nsteps taken towards the integration of learned indexes in practical systems. Section 11 provides an overview of open\nchallenges and discusses future research directions. Finally, Section 12 concludes the paper.\n\nA Survey of Learned Indexes for the Multi-dimensional Space 5\n2 RELATED SURVEYS AND TUTORIALS\nThe topic of learned indexes is relatively new, and hence there are only a few related surveys in the literature. Most of\nthe existing studies focus on learned one-dimensional indexes [ 56,122,225]. Related conference-based tutorials on the\nsubject of learned one-dimensional indexes can be found in [ 80,103‚Äì105,193]. Moreover, progress reports on ML in\ndatabases, and instance optimized data systems have been presented in [ 95,98]. A recent book on the topic of designing\nData Structures with discussion on learned indexes for one-dimensional data can be found in [15].\nThere is a 4-page conference tutorial on the subject of learned multi-dimensional indexes [ 8] that is an earlier version\nof this survey article by the same authors. That tutorial covers methods only until Year 2020. Moreover, the tutorial\ndoes not include descriptions of the core concepts of the learned index structures listed in that short tutorial. This\nsurvey serves as an extension to that earlier conference tutorial and adds new criteria for classifying new indexes into a\nmore sophisticated and comprehensive taxonomy as well as covers many more multi-dimensional learned indexes that\nhave been developed until 2024.\nAnother related survey on multi-dimensional indexes can be found in [ 107]. However, in [ 107], the survey focuses\non the impact of modern hardware platforms (e.g., non-volatile memory, GPU), and ML algorithms in the context\nof multi-dimensional indexes. There is also a series of related tutorials on big spatial data [ 160‚Äì162]. However, the\nfocus of this series of tutorials is not on learned multi-dimensional index structures. There is also a related tutorial\nthat covers ML-enhanced indexes particularly designed for high-dimensional data [ 49]. On the other hand, there\nare several comprehensive surveys for traditional multi-dimensional indexes [ 59,166,167]. As the topic of learned\nindexes is relatively new, to the best of our knowledge, this survey paper is the first to focus particularly on learned\nmulti-dimensional index structures.\n3 EVOLUTION OF LEARNED INDEXES\nFigure 4, presents a chronological diagram to depict the evolution of both one- and multi-dimensional learned indexes.\nHere, we have grouped the papers on learned one- and multi-dimensional indexes based on their publication years.\nMoreover, we have used the ‚Üísymbol to indicate if a later paper is related to an earlier one. The dis used when\nthere is a line crossing. Additionally, a ‚ó¶symbol is used to denote a copy of an earlier paper in a later year. One- and\nmulti-dimensional indexes are differentiated using the ‚ñ°and‚ñ≥symbols, respectively.\n4 INDEXING THE LEARNED MODELS\nAssume that we have a collection of learned models, where each model represents a certain class of objects, e.g.,\nthe class cat, dog, etc. Given an input instance, e.g., an image of an object, we need to execute each of the learned\nmodels to identify which model this input is likely to belong to, and hence identify the class of the input instance, e.g.,\nbeing a cat with high probability. To avoid running each of the models on the instance, it would be good if we can\nindex these learned models to distinguish between these models in a scalable way. We present this idea in the context\nof handwritten words and the Handwritten Trie [ 13] that highlight an example of the body of work for the class of\nIndexing the Learned Models. Other similar indexes in the same class can be found in [14].\nHandwritten Trie [13] Given a collection of learned models, where each model corresponds to one handwritten\nword, e.g., a person‚Äôs signature, handwritten indexes are used to speed up the search for a matching model given an\ninput handwritten text, e.g., to help identify whose person this signature belongs to. The core idea of the handwritten\ntrie involves a trie structure as well as a collection of Hidden Markov Models (HMMs) that serve as the alphabet symbols\n\n6 Abdullah-Al-Mamun, et al.\n1990 -2001 2002 2019 2018RMI \n[97]SOSD  [91]\nIFB-Tree[72]ASLM [113]\nHybrid -LR[155]AIDEL [109]ALEX[44]\nSageDB -LMI [96]\nZM-Index[190]Tsunami [45]PGM [57]\nIF-X [75]RS [92]LISA [110]\nMADEX [73]Benchmark -1 \n[131]\nCDFShop  [133]CaseSpatial  \n[147]FLOOD [141]MTO [43]\nXIndex  [183]QD-tree [207]\nPolyFit  [114]LearnedBF  \n[129]ML-Index \n[41]Self-\norganizing \n[16]\nPavo  \n[203]Doraemon [182]DriftModel  [71]\nFITing -Tree[60]BF_Sandwich  \n[136]\nIndexing the learned models\nLearning the indexOne-dimensional\nMulti -dimensionalHand -\nwritten\nTrie[13]Music\nRetrieval\n [86]\n2008 2010BAYES\nTree\n[171]\nHERMIT[202]\nPeriod -index [19]\nGoogle -index [4]Bourbon [38]Distance -\nbounded\n[212]\nRSMI[153]ML-enhanced \n[88]\nSPRIG [218]\nSIndex  \n[194]RUSLI[134]\nAPEX [124]\nLIPP [200]PLEX [178]RSS [177]\nFINEdex  [108]TridentKV  [125]‚ÄúAI+R‚Äù -tree \n[7]RW-tree\n  [48]\nConcurrentLI [196]LSI[89]HAP[120]\nMicro -\narchi [11]\nCritical -\nRMI[130]Updateable [199]NFL[201]Waffle \n[34]\nSNARF[186]\nSA-LSM[220]Z-Index[146]\n2024RLR-tree \n[67]\nLSTI \n[46]\nWISK \n[174]\nExpEval -2[99]ExpEval -1[181]\nCuttingLI [62]PLIN[222]FILM[128]LMSFC \n[61]\nDILI [111]\nSpatial -LS [148]\nGLIN[189]Sieve [185]\nLIFOSS [211]\nFLIRT[205]\nDiffLex  [37]IA-LBF[23]\nStableBF  [121]\nPLBF [187]\nLhash -1[163]SPRIG+ [219]\nS3-SkipList [217]\nAcc B+Tree [123]WAZI \n[145]\nTALI[68]FI-index[173]COAX \n[70]\nTowards\nBenchmark \n[24]\n2020 2021 2022 2023ELSI\n[116]\nMdlReuse [117]\nHM-index\n[192]Shift -table \n[74] SLBRIN[191]CARMI\n[216]ACR-tree [78]\nLearnedKD\n [209]FHSIE[180]LMI-1 [12]LMI-2[176]\nLPBF\n [227]PA-LBF\n[214] CompressLBF\n [40]   PLATON\n      [206]\nMeta -BF [158]\nPLBF++\n[169]PDDBF\n[215]BMT\n[106]\nTONE[221]\nCOLIN[213]AELH[115]\nWIPE[197]LIMS\n [184]AdaBF\n[39]\nHLI[47]Lhash -2 \n[164]\nRelated to\nConnector to avoid line crossingCopy of an earlier index in a later yearEWALI \n[119]SALI [63]\nFig. 4. Timeline of the evolution of learned indexes. Lines connecting between the various learned multi-dimensional indexes reflect\ndependence of these indexes on earlier work.\nin the trie nodes. The input to the handwritten trie is handwritten text. This input is segmented into alphabet symbols,\nand the index is traversed by executing the HMM models associated with each of the trie node. Each HMM is trained to\nrepresent a pictogram class so that each HMM accepts a specific pictogram with high probability. By traversing nodes\nat each level, the model chooses nodes with the highest probability for a particular letter (pictogram) and eventually\nobtains a set of nodes with the highest combined probability. Since the handwritten trie does not actually learn the\ndistribution of key inputs, but rather indexes the HMM models, it is categorized as an index for the Learned Models\nrather than a learned index by itself.\nOther indexes exist in this category. In the context of music retrieval [ 86], an R*-tree [ 18] is used to index the HMM\nmodels. In the BayesTree [ 171], an R-tree is employed to index a hierarchy of mixture density models. More indexes,\ne.g., as in [14], fall under this category, but will not be discussed further in this survey.\n5 LEARNED IMMUTABLE INDEXES\nImmutable learned indexes are built on static datasets and do not support dynamic inserts/updates. In this section, we\npresent the Learned Immutable Indexes in both the one- and multi-dimensional spaces.\n5.1 The One-dimensional Case\nWhile one-dimensional learned indexes are not the focus of this survey, we present a few foundational indexes in the\ncategory as many indexes in the case of multi-dimensional learned indexes build on their one-dimensional counterparts.\n\nA Survey of Learned Indexes for the Multi-dimensional Space 7\n5.1.1 Pure Learned Indexes .Here, we present the core concepts of RMI [ 97] to highlight the class of Immutable\nOne-dimensional Pure Learned Indexes.\nRMI [97].The key idea in [ 97] is: ‚ÄúIndexes are models.\" For example, given a key ùëò, an index simply predicts the\nposition of ùëòin a sorted array. Also, it has also been observed that ‚Äùa model predicting the position of a key within\na sorted array effectively approximates the Cumulative Distribution Function (CDF).\" As a result, indexes can be\nlearned. In [ 97], it has been demonstrated how learned index structures can be implemented for three types of indexes:\nB-tree [ 17,35], Hash map [ 32,139], and Bloom filter [ 25]. Similar to a B-tree, for range queries, the Recursive Model\nIndex (RMI) structure is introduced that recursively chooses a model at multiple levels. With an input key, the root\nmodel chooses a child model based on its output. This continues until it reaches the leaf model that predicts the actual\nposition in the underlying sorted array.\nThe prediction by the leaf-level ML model might not be accurate. As a result, an error correction mechanism is\nemployed to correct the misprediction within a predefined error bound (i.e., min and max error). Because data in the\none-dimensional case is sorted, and hence a total order exists among the elements of the indexed array, any misprediction\ncan be corrected by searching either to the left or to the right of the mispredicted value. A binary or exponential\nsearch operation can be performed to search for the correct value from the mispredicted location. The proposed RMI\nstructure can be built using a mixture of ML models (e.g., neural networks [ 6] or linear regression [ 84]). Moreover, at\nthe bottom level of RMI, a traditional B-tree can also be used as a model, introducing the idea of Hybrid Learned Indexes\n(i.e., Hybrid RMI). For point queries, the idea of Hash-Model Index (HMI) has been proposed in the same paper. For\nminimizing collisions, HMI leverages the CDF of the dataset by employing learned hash functions. For existence indexes,\nthe Learned Bloom Filter (LBF) has been introduced [ 97]. LBF is formulated as a classification problem. It proposes\nadopting a hybrid approach by integrating a traditional bloom filter with an ML model. During query processing, the\nkey is first passed into a learned model to decide on existence, and a traditional bloom filter is used to catch any false\nnegative cases from the learned model.\nOther example indexes in this category are as follows. CDFShop [ 133] is a tool that guides users on tuning the\nparameters of the RMI index structure. Pavo [ 203] leverages Recurrent Neural Networks (RNN) [ 33] to replace the\nhash function in a traditional inverted index structure. In MdlReuse [ 117], pre-trained ML models are used instead of\ntraining a model during the construction of the learned index. HAP [ 120] is a Hamming space indexing framework that\nleverages ML models for cost estimation, query processing, and index compression. The FI-index [ 173] investigates\nthe use of function interpolation models as an alternative choice of ML model in the context of learned indexes. In\nGoogle-index [4], a learned index is introduced for a disk-based distributed system. In MetaBF [158], a learned bloom\nfilter is proposed by leveraging meta-learning [ 168]. HLI [ 47] uses ML models with error bounds for leaf nodes and\nmodels without error bounds for inner nodes to achieve the benefits of both approaches. AELH [ 115] is a learned hash\nindex structure that leverages Autoencoders [188] for binary hash codes.\n5.1.2 Hybrid Learned Indexes .Hybrid learned indexes combine traditional index structures with ML models to\nbuild ML-enhanced index structures. In this section, we present the core concepts of RS [ 92] as an example to highlight\nthe class of Immutable One-dimensional Hybrid Learned Indexes. Then, in the section to follow, we survey in more\ndetail, the multi-dimensional case.\nRS[92].RadixSpline (RS, for short) is a hybrid learned index structure that combines the linear spline model with\na traditional radix table [ 101]. Given a search key, the traditional radix table is used to locate the two spline points\n\n8 Abdullah-Al-Mamun, et al.\nbounding the search key. RS can be constructed in a single pass if the data is sorted. The steps to construct the RS index\nare as follows: i) Fit a linear spline to the CDF of the data to ensure a certain error-bound, and collect a set of spline\npoints, and ii) Construct an approximate index of the spline points using a radix table. The proposed index supports\nboth equality and range predicates. However, performance of the RS index can be negatively impacted by skewed data\ndistributions. To address this, a tree-structured radix table can be utilized. Note that the RS structure can be tuned using\nonly two hyperparameters for a given data and memory budget.\nThe other indexes in this category are as follows. Hybrid_RMI [ 97] combines a traditional B-tree with an RMI\nstructure to create a hybrid index structure. PLEX [ 178] leverages a traditional Hist-tree [ 36] to build a learned index\nwith a single tunable parameter. LSI [ 89] also leverages a traditional compact Hist-tree to construct a learned index for\nunsorted data. ShiftTable [ 74] is a helper error correction layer that can be combined with the ML models of a learned\nindex. LBF [ 97] introduces the learned bloom filter structure by combining ML models with traditional bloom filters.\nRSS [177] extends the methods of RS [92] for indexing strings.\n5.2 The Multi-dimensional Case\nIn this section, we explain the learned multi-dimensional indexes along the various dimensions of our taxonomy.\nThe properties of the class of immutable learned multi-dimensional indexes are presented in Table 1. We highlight the\ntypes of queries supported by each of the learned multi-dimensional indexes. For point queries, some indexes do not\nexplicitly provide a query processing algorithm or experimental results. However, if an index can be easily extended\nto support point queries, we highlight that by listing that the index supports point query processing. On the other\nhand, due to nature of the application domain, an index might be designed emphasizing efficiency over accuracy. As a\nresult, an index can support either exact or approximate query processing. Here, approximate query processing refers\nto the event of missing some results from the set of exact answers to the query. We indicate whether a supported query\ntype returns an exact or an approximate answer. Notice that we have excluded the learned multi-dimensional bloom\nfilters (e.g., LearnedBF [ 129], CompressLBF [ 40]) in the context of supported query types because they are considered\nas probabilistic existence index structures.\n5.2.1 Pure Learned Indexes in the Projected Space .\nSageDB-LMI [96].SageDB is an ML-enhanced database system that has been envisioned in [ 96]. SageDB includes\na learned multi-dimensional index (SageDB-LMI, for short) that is one of the earliest attempts to extend techniques\nsimilar to RMI [ 97] in the context of multi-dimensional data. SageDB-LMI projects the multi-dimensional data points\ninto one-dimensional space by consecutively sorting and partitioning the points along a sequence of dimensions (e.g.,\nfirst x-dimension then y-dimension) into uniformly-sized cells. This produces a layout that is easily learnable compared\nto the more complex space-filling curves, e.g., the Z-order SFC [ 138]. Notably, any one-dimensional learned indexing\nmethod can be applied to the projected space. Consequently, in the second step, SageDB-LMI uses a trained CDF model\n(e.g., RMI) to predict the physical location of the point. In an in-memory, read-only experimental setup, SageDB-LMI\noutperforms a traditional R-tree in terms of average query time and index size.\nZM-index [190].Similar to SageDB-LMI [ 96], the ZM-index projects multi-dimensional spatial data into a one-\ndimensional projected space. Particularly, the ZM-index involves two essential components: 1. A Z-order [ 140,144,150]\ncurve to linearize the multi-dimensional space, and 2. A multi-staged model index to support search. The Z-order curve\n\nA Survey of Learned Indexes for the Multi-dimensional Space 9\nTable 1. Properties of the Immutable Learned Multi-dimensional Indexes\nIndex Pure\nLearnedHybrid\nLearnedData\nSpacePoint\nQueryRange\nQuerykNN\nQueryJoin\nQuery\nSageDB-LMI [96] ‚úì√ó Projected Exact Exact √ó √ó\nZM-index [190] ‚úì√ó Projected Exact Exact √ó √ó\nML-index [41] ‚úì√ó Projected Exact Exact Exact √ó\nHM-index [192] ‚úì√ó Projected Exact Exact √ó √ó\nZ-index [146] ‚úì√ó Projected Exact Exact √ó √ó\nWAZI [145] ‚úì√ó Projected Exact Exact √ó √ó\nLearnedBF [129] ‚úì√ó Native ‚Äî ‚Äî ‚Äî ‚Äî\nCompressLBF [40] ‚úì√ó Native ‚Äî ‚Äî ‚Äî ‚Äî\nQd-tree [207] ‚úì√ó Native Exact Exact √ó √ó\nLMI-2 [176] ‚úì√ó Native Approx. Approx. Approx. √ó\nDistance-bounded [ 212]√ó Radix table Projected Approx. Approx. √ó Approx.\nFlood [141] √ó Grid Native Exact Exact √ó √ó\nTsunami [45] √ó Grid Native Exact Exact √ó √ó\nSPRIG [218] √ó Grid Native Exact Exact Exact √ó\nSPRIG+ [219] √ó Grid Native Exact Exact Exact √ó\nML-enhanced [88] √ó R-tree Native √ó √ó Approx.√ó\n‚ÄúAI + R‚Äù-tree [7] √ó R-tree Native Exact Exact √ó √ó\nCaseSpatial [147] √ó Grid Native Exact Exact √ó √ó\nSpatial-LS [148] √ó Grid Native Exact Exact √ó Exact\nCOAX [70] √ó Grid Native Exact Exact √ó √ó\nPolyFit [114] √ó Quad-tree Native Approx. Approx. √ó √ó\nLearnedKD [209] √ó KD-tree Native √ó √ó Approx.√ó\nLMI-1 [12] √ó M-tree Native Approx. Approx. Approx. √ó\nsorts the multi-dimensional data according to the corresponding Z-addresses computed through bit interleaving. The\nmulti-staged model index consists of neural network models at multiple stages that recursively partition the data space\ninto sub-regions. Similar to RMI [ 97], given a query key, the model at Stage ùëñpredicts a position based on the CDF\nand the number of all keys. After that, it chooses a model at Stage ùëñ+1according to the predicted position or directly\noutputs the predicted position if already at the leaf level. The steps for range query processing using ZM-index are\nas follows: (i) Computing Z-addresses of the start and end points of the range query. (ii) Predicting the positions of\nthe start and end points using the multi-staged model index structure. (iii) Finding the exact positions of the start and\nend points using model-based search. (iv) Scanning through the points within the range. Although the ZM-index can\nachieve query processing time similar to an R-tree, it can significantly reduce the index size compared to its traditional\ncounterpart.\nML-index [41].In the case of learned multi-dimensional indexes in the projected space, it is desirable for the\nmulti-dimensional data to be projected in an order that can be easily learned by ML models. To achieve this goal,\nthe proposed Multi-dimensional Learned index (ML-index, for short) generalizes the idea of the previously proposed\niDistance [ 82]. The ML-index partitions and transforms the data into one-dimensional values based on distribution-\naware reference points. It proposes an efficient scaling method so that, after projecting the multi-dimensional points\ninto the one-dimensional space, the spatial proximity in the native space is well-preserved in the lower dimension.\n\n10 Abdullah-Al-Mamun, et al.\nSimilar to the learned B-tree index in [ 97], a recursive model index is applied to the projected space. The ML-index can\nsupport point, range, and kNN queries.\nHM-index [192].The Hilbert Model index (HM-index, for short) follows the similar principle that has been used\nfor the ZM-index [ 190]. Here, the core idea is to project the multi-dimensional data into the one-dimensional space\nusing the Hilbert SFC [ 77,165] and applying one-dimensional learned indexing techniques on the projected space.\nParticularly, a two-stage model has been used for learning the underlying data distribution. During the prediction\nstep, given a query point, the model either predicts the position of the query point or a range in which the position of\nthe query points can be searched. Moreover, due to the application of the Hilbert linear ordering, an error correction\nmechanism can be used in case of an incorrect prediction with an error bound. Notice that the nearby data points in the\nprojected Hilbert space may be far apart in the native space. As a result, it can impact the performance of range query\nprocessing. Thus, a query partitioning technique based on n-order Hilbert regions has been adopted to minimize the\nimpact on performance.\nZ-index [146].Many spatial indexes use a pre-defined SFC, e.g., the Z-curve or Hilbert-curve, to order multi-\ndimensional data. However, existing SFCs are not designed to be instance-optimized [ 95] for given data and query\nworkloads. The idea of an instance-optimized SFC-based index has been investigated [ 146]. Particularly, for a given\ndata and query workloads, an instance-optimized variant of the Z-index has been proposed by varying the partitioning\nand the ordering [ 159]. The goal is to reduce the number of false positives during range query processing. In the\nproposed algorithm, two heuristic-based approaches are explored: the Independence-based Heuristic, and the Sampling-\nbased Heuristic. For the sampling-based heuristic, after the partitioning step, a learned density model is applied to\napproximately calculate the number of data points falling into each of the children-cells. Notice that an instance-\noptimized SFC is directly applicable to the class of learned multi-dimensional indexes operating on a projected space.\nWAZI [145].The Workload-aware Z-Index (WAZI, for short) is an extension of the previously proposed instance-\noptimized Z-index [ 146]. Given a spatial dataset and range query workloads, the proposed index optimizes the parti-\ntioning and Z-ordering. WAZI‚Äôs partitioning technique produces cells that are accessed by a similar set of range queries.\nThis enables the proposed index to reduce extraneous cell accesses during query processing. The benefit will persist\nwith the proposed partitioning and ordering as long as the distribution of the query workload remains unchanged.\nDuring the index construction phase, a greedy algorithm is proposed for the partitioning and ordering of the cells. After\nthe index construction phase, a particular partitioning and ordering are chosen by minimizing a pre-defined objective\nfunction based on the number of accessed data points. Here, Random Forest Density Estimation (RFDE) [ 198] models\nare used to approximate the exact data and query distributions.\n5.2.2 Pure Learned Indexes in Native Space .\nLearnedBF [129].LearnedBF extends the concept of Learned Bloom Filter (LBF) [ 97] to design a learned multi-\ndimensional bloom filter. For designing LearnedBF, firstly, the strings with k-tuples are converted into an embedding\nvector. Notice that Recurrent Neural Networks (RNN) with gated recurrent units [ 33], and direct embedding technique\nare used for high- and low-cardinality attributes, respectively. Moreover, it has been observed that a learned bloom\nfilter can handle multidimensional data (e.g., k-tuple) effectively if there is a ‚Äúco-occurrence structure\" between in-index\nk-tuples that are different from out-of-index tuples. Based on the observation, the embedding vectors are concatenated\n\nA Survey of Learned Indexes for the Multi-dimensional Space 11\nand sent as an input to a densely connected neural network layer. The output of this layer is used as an input to a\nsigmoid function. After that, a sandwich structure [136] is employed to get the final output of the LearnedBF.\nCompressLBF [40].CompressLBF has been introduced to reduce the space consumption of the previously proposed\nLearnedBF [ 129]. It is observed that the size of the trained ML model is significantly impacted by the number of distinct\nvalues in the input. Particularly, for a column with a high number of unique values, the size of the corresponding\nembedding matrix will grow linearly. As a result, it has been proposed to compress the input embedding by splitting\na column into several sub-columns where the number of sub-columns is pre-defined. This split operation creates\nsub-columns with fewer dimensions. As a result, the embedding of the input will be smaller in size, and hence the\nproposed filter requires less space. Moreover, the compressed filter achieves high accuracy and saves model training\ntime.\nQd-tree [207].It has been observed that modern big data analytical systems partition data mainly by two approaches:\ni) Hash/time-based, and ii) Clustering-based [ 83]. These existing methods do not consider the distribution of the query\nworkload while partitioning the data. Given a particular data and query workloads, the Query-data Routing Tree\n(Qd-tree, for short) leverages Reinforcement Learning [ 87] (RL, for short) to partition the data so that the number of\nblocks accessed by a particular query workload is minimized. The Qd-tree can be considered as a workload-aware\nmulti-dimensional index structure where each non-leaf node partitions the data using a particular query predicate.\nMoreover, the data in a leaf node is routed to the same disk block. For the formulation of the Qd-tree construction\nprocess as a Markov Decision Process (MDP) [ 152], the set of nodes are represented as states, the action space is\nrepresented as the set of query predicates (i.e., allowed cuts), and the reward is calculated using the number of skipped\nblocks over all queries. Notice that calculating the actual reward (i.e., number of skipped blocks) by executing queries is\na costly process. As a result, a sampling method has been leveraged to avoid the costly query execution. On the other\nhand, the proposed RL agent, namely ‚ÄúWoodblock\", consists mainly of two learnable networks: i) Policy network, and\nii) Value network. Moreover, Proximal Policy Optimization [ 170] (PPO, for short) is used as the underlying learning\nalgorithm.\nLMI-2 [176].LMI-1 [ 12] is a Learned Metric Index that leverages predictions from a hierarchical structure with ML\nmodels for query processing. Notice that LMI-1 is a hybrid structure that requires a pre-existing index structure to\npartition the data. LMI-2 extends the LMI-1 by eliminating the requirement of a pre-existing index structure to partition\nthe data objects. Moreover, LMI-2 adopts an unsupervised approach for ML model training by leveraging clustering [ 83]\ntechniques. As a result, LMI-2 requires less index construction time than LMI-1 and outperforms LMI-1 in terms of\nquery processing time.\n5.2.3 Hybrid Learned Indexes in the Projected Space .\nDistance-bounded [212].Most traditional spatial data processing methods follow a pipeline of coarse-grained\nfiltering followed by exact geometric tests. In contrast, the method proposed in [ 212] advocates for fine-grained raster\napproximations to eliminate the need for exact geometric tests, prioritizing efficiency over accuracy. As a result, the\nproposed method leverages a raster-based approximation of spatial objects for approximate query processing. Moreover,\na user-defined distance bound (i.e., error bound) is used to control the accuracy of the spatial approximation. Based\non the user defined distance bound, the space is divided into uniform two-dimensional raster cells. Subsequently, the\ntwo-dimensional raster cells are projected into a one-dimensional array using a SFC. After that a one-dimensional\n\n12 Abdullah-Al-Mamun, et al.\nlearned index (e.g., RadixSpline [ 92]) is constructed to learn the position of the cells in the one-dimensional array.\nNotice that the learned index is used for approximate point query processing. On the other hand, to enhance query\nperformance for polygons, an Adaptive Cell Trie structure (ACT, for short) [90] is used.\n5.2.4 Hybrid Learned Indexes in Native Space .\nFlood [141].Flood is a clustered in-memory read-only learned multi-dimensional index (for column stores) optimized\nfor specific datasets and query workloads. During the index construction phase, Flood takes query workloads as input\nand learns to automatically create a data layout optimized for the given query distribution. Given a dataset with d\ndimensions, Flood models the empirical Cumulative Distribution Function (CDF) of each dimension using RMI [ 97].\nThen, Flood uses these models to create partitions of equal size, ensuring each partition contains an equal number\nof data points. These partitions are grouped to form a grid structure. Flood optimizes the data layout for particular\nworkloads by adjusting the number of partitions in each dimension. It uses d-1 dimensions for partitioning, designating\nthe last dimension as the ‚Äúsort dimension\". Flood fine-tunes the parameters of its grid structure using a cost model, and\napplies a gradient-descent algorithm to the cost model to optimize its parameters jointly. During query processing, for\neach query, Flood identifies all grid cells intersected by the query, and follows a projection-refinement-scan approach.\nTsunami [45].Traditional multi-dimensional indexes, e.g., the ùëò-d tree [ 21], partition the underlying space based on\nthe data distribution alone. In contrast, learned indexes, e.g., Flood [ 141], optimize for both data and query workloads.\nHowever, Flood‚Äôs performance suffers under skewed query workloads and correlated data. To overcome Flood‚Äôs\nlimitations, Tsunami is proposed. Tsunami introduces a Grid Tree structure and an Augmented Grid to address the\nissues of skewed query workloads and correlated data, respectively. The proposed grid tree is a lightweight Decision\nTree [ 156], and the augmented grid uses conditional CDFs and functional mappings. For the grid tree, Tsunami clusters\nqueries by selectivity and builds a Grid Tree for each query type. Each node in the Grid Tree is split into several ranges\nalong one dimension (except leaf nodes) until it reaches a minimum threshold or exhibits low query skew. For the\naugmented grid structure, Tsunami employs three strategies: i) Partitioning Dimension X independently by its CDF\n(similar to Flood), ii) Eliminating Dimension X by constructing a mapping from X to Y (if they are monotonically\ncorrelated), and iii) Partitioning X dependent on Dimension Y by CDF(X|Y). To find the best augmented grid structure,\nTsunami defines the number of partitions and a search strategy including the above three approaches. Then, it uses\nadaptive gradient descent to iteratively search for an optimal strategy. Tsunami outperforms the previously proposed\nFlood in scenarios with skewed query workloads and correlated data.\nSPRIG [218].Spatial Interpolation Function based Grid (SPRIG, for short) is a grid-based learned multi-dimensional\nindex designed for read-only workloads. SPRIG applies a sampling technique to the dataset, and uses the sampled data\nto build an adaptive grid structure. Then, it leverages the sampled data to fit a spatial interpolation function [ 135],\nspecifically using the bilinear interpolation function. During query processing, given a search key, SPRIG predicts the\napproximate position of the key using the learned interpolation function. Notice that SPRIG requires a local search as\nan error correction mechanism following the prediction step. Moreover, to provide an error bound, SPRIG introduces a\nmaximum estimation error calculated using the query workload. SPRIG can process both range and kNN queries.\nSPRIG+ [219].Observing the imprecision of a single spatial interpolation function and the large prediction error in\nSPRIG [ 218], SPRIG+ combines the idea of space-partitioning trees with the prediction mechanism. Firstly, SPRIG+\ndivides the two-dimensional grids into four sub-regions recursively. Next, one spatial interpolation function is learned\n\nA Survey of Learned Indexes for the Multi-dimensional Space 13\nfor each region. Furthermore, to optimize storage space, SPRIG+ stores only the integer coordinates (used for calculating\nthe interpolation function coefficients) instead of keeping the double coefficients (for lazy calculation). Additionally,\nSPRIG+ compresses all data into a bit vector, ensuring that the number of bits occupied by each integer remains as\nsmall as possible.\nCaseSpatial [147].The techniques proposed in Flood [ 141] have been applied to five in-memory traditional spatial\nindex structures in CaseSpatial [ 147]: Fixed-grid [ 22], Adaptive-grid [ 142], k-d Tree [ 21], Quad-tree [ 166], and STR-\ntree [ 102]. Moreover, the query processing in all these techniques is performed in three steps: Index lookup, Refinement,\nand Scan. Similar to Flood, data in each partition are sorted using one dimension. During range query processing, while\nsearching within a partition, it is proposed to use a one-dimensional learned index (e.g., RadiXSpline [ 92]) on the sorted\ndimension instead of a binary search. As a result, performance has improved for range queries with low selectivity.\nHowever, there is less improvement in the case of range queries with high selectivity. This has led to the conclusion that\nthe performance of a range query with low selectivity can be significantly improved by employing learned indexing\ntechniques. It has also been observed that filtering in one dimension and refining in the other dimension using a\none-dimensional learned index can outperform methods that filter on two dimensions.\nSpatial-LS [148].Spatial-LS is an extension of the techniques proposed in CaseSpatial [ 147]. Here, six different\ntraditional spatial partitioning techniques have been considered for experiments, and these techniques are applied to\nachieve instance-optimization. Notice that in CaseSpatial [ 147], only the range query is supported. On the other hand,\nSpatial-LS supports point, range, distance, and spatial join queries. Moreover, their extensive experimental studies show\nthat: i) Properly tuned grid-based structures can outperform tree-based structures due to fewer random accesses and\nthe benefit of learned search within larger partitions; ii) Within a large partition, a learned model performs better than\nbinary search; and iii) The impact of ML-enhanced grid-based index structures is less in the case of queries with high\nselectivity. Notice that all these techniques are developed considering an in-memory setup. As a result, the benefits of\nthe proposed techniques might not be directly applicable in a disk-based setup due to the diminished importance of\nsearching within each partition.\nCOAX [70].It has been observed that two or more attributes are correlated in many real-world multi-dimensional\ndatasets. As a result, these correlations can be leveraged for dimensionality reduction. Correlation-Aware Indexing\n(COAX, for short) learns the correlations among the attributes. The main idea is to construct a multi-dimensional index\nby excluding the highly-correlated attributes. As a result, the constructed index will be significantly reduced in size,\nand that can enhance query processing performance. Moreover, if a query involves a non-indexed attribute, say ùëé1, that\nis correlated with another indexed attribute ùëé2, COAX uses only the correlated indexed attribute to process the query.\nCOAX learns the correlation by learning Soft Functional Dependency (softFD) [ 81]. Furthermore, COAX is implemented\nwith Grid Files [142] by adopting a hybrid approach and supports point and range queries.\nML-enhanced [88]. It has been observed that the performance of existing pure learned multi-dimensional indexes\nmight degrade for high-dimensional data due to the ‚Äúcurse of dimensionality\". As a result, ML models have been\nincorporated into different traditional high-dimensional indexes (e.g., VA+Index [ 52], DS-tree [ 195], iSAX [ 175]) to build\nthe ML-enhanced variants of these traditional index structures. Notice that an R-tree has been used for implementing\nthe VA+Index structure. Moreover, the proposed ML-enhanced indexes focus on improving recall for approximate kNN\nquery processing in high-dimensional data. The underlying intuition behind all the ML-enhanced indexes is ‚Äúonce an\nindex is built, the distribution of the nearest neighbors given a query object has been fixed, and therefore is learnable\".\n\n14 Abdullah-Al-Mamun, et al.\nAs a result, the proposed ML-enhanced indexes use deep neural networks to guide k-nearest-neighbor (KNN) search on\ntraditional tree-based indexes. Particularly, the problem of kNN query processing has been formulated as a multi-class\nclassification problem [ 5], where the goal is to predict the leaf nodes that contain the nearest neighbors given an input\nquery. Hence, by leveraging the predictions of the ML models, the ML-enhanced indexes improve the leaf node access\norder of their traditional counterparts.\nAI+R-tree [ 7].Similar to the ML-enhanced index [ 88], the AI+R-tree incorporates ML models to enhance the\nperformance of a traditional R-tree. As areas covered by R-tree nodes overlap in space, searching for a single object may\nrequire exploring multiple paths from root to leaf. This overlapping issue negatively impacts R-tree query processing\nperformance. In the AI+R-tree, for a range query, an overlap ratio is proposed to quantify the degree of unnecessary\nleaf node accesses. Moreover, range query processing in an R-tree has been formulated as a multi-label classification\nproblem [ 76]. Specifically, a new AI-tree is designed that trains multiple ML models to directly predict the true leaf node\nIDs for an input range query. To improve the query processing time of a traditional R-tree in the case of high-overlap\nqueries, a hybrid AI+R-tree is proposed that processes high- and low-overlap queries using the AI-tree and the regular\nR-tree, respectively. The AI+R-tree uses the overlap ratio to train an ML model, enabling it to classify an input query as\nhigh- or low-overlap. It assumes that the data and query workloads are fixed. For efficient query processing, it also\nleverages a traditional grid structure to index the learned ML models [13].\nPolyFit [114].The PolyFit method has been proposed to process approximate range aggregate queries (e.g., SUM,\nCOUNT). This method leverages piecewise polynomial functions for query processing, and constructs the index based\non polynomial fitting for intervals of data. The choice of polynomial functions is due to their observed benefits over\nlinear regression. Multiple polynomial functions are used to minimize the fitting error, as a single polynomial function\nmight not fit the entire dataset accurately. Additionally, a greedy segmentation method has been proposed to reduce\nthe number of polynomial functions. PolyFit is extended to support queries over multi-dimensional data by estimating\ncumulative functions for multiple keys and employing greedy segmentation techniques. However, due to the quadratic\ntime complexity of the greedy approach in the multi-dimensional case, a Quad-tree [ 166] has been incorporated to\nidentify segments.\nLearnedKD [209].LearnedKD integrates an ML model with a traditional k-d tree [ 21] to speed up kNN query\nprocessing. For ML model training, kNN queries are initially processed using the traditional k-d tree, and the positions\nof the k nearest neighbors are recorded as labels. Then, a deep neural network model is trained on this dataset to assign\nhigh probabilities to data objects that are nearest neighbors of a given kNN query. This supervised learning process\nenables the model to predict whether an object is a nearest neighbor for an input query. However, the ML model‚Äôs\noutput does not directly identify the k nearest neighbors. Therefore, a post-processing step is necessary to determine\nthe nearest neighbors after the prediction. This method is applied specifically to two-dimensional k-d trees.\nLMI-1 [12].Inspired by the benefits of learned multi-dimensional indexes (e.g., Flood [ 141]), a Learned Metric Index\n(LMI-1, for short) has been proposed in [ 12]. LMI-1 is specifically designed for indexing data in metric spaces, where\nsimilar data objects are clustered together using a distance metric. Moreover, LMI-1 suggests using predictions from a\nhierarchical structure with ML models for query processing rather than relying on their distance metric. Notice that\nLMI-1 is a hybrid structure that requires a pre-existing index structure to partition the data. Afterwards, it uses the\npartitions as labels to train a hierarchy of supervised ML models. LMI-1 is one of the earliest works to extend the\nconcept of learned multi-dimensional indexes into the metric space.\n\nA Survey of Learned Indexes for the Multi-dimensional Space 15\n6 LEARNED MUTABLE FIXED DATA LAYOUT INDEXES\nIn this section, we present the Mutable Fixed Data Layout Learned Indexes in both the one- and multi-dimensional\nspaces.\n6.1 The One-dimensional Case\n6.1.1 Pure Learned Indexes with In-Place Insertion Strategy .We present the core concepts of ALEX [ 44], a\none-dimensional index that highlights the class of Mutable Pure Learned Indexes with Fixed Data Layout and an In-place\nInsertion Strategy.\nALEX [44].ALEX is an in-memory updatable learned index structure. Its core elements include: i) RMI as the ML\nmodel hierarchical structure, and ii) A Gapped Array (GA, for short) and A Packed Memory Array (PMA, for short) [ 20]\nas leaf node layouts in RMI. ALEX adopts an adaptive RMI structure capable of handling the insertion of new keys.\nDuring initialization, the root model is executed first, partitioning the key space among its child nodes, that in turn are\nrecursively initialized. Each non-root node is assigned a fixed number of partitions. When a partition size is appropriate,\nleaf nodes are created in the form of either GA or PMA to support inserts. The main idea behind the proposed GA is to\nmaintain space/gaps at the leaf level so the structure can accommodate in-place ML model-based insertions. Moreover,\nif the partition size exceeds the maximum number of keys, a new inner node will be created. On the other hand, if the\npartition size is too small, adjacent partitions will be merged to avoid wasting leaf nodes. An extended version of ALEX,\ntermed ALEX+, with concurrency support has been introduced in [199].\nOther indexes in this category are as follows. AIDEL [ 109] leverages independence among ML models to build a\nscalable index structure. TALI [ 68] uses the update distribution of data for efficient lookup and insertion operations.\nStableBF [ 121] designs a learned bloom filter for handling data streams. DriftModel [ 71] addresses the issue of drift\ncorrection (i.e., model error due to updates) in updatable learned indexes. SALI [ 63] employs probability models to\nconstruct a scalable index structure. APEX [ 124] extends ALEX to develop an index optimized for persistent memory.\nCARMI [ 216] offers a cache-friendly extension of the RMI index. EWALI [ 119] improves the write performance of a\nlearned index by using dual buffers. TONE [ 221] reduces the tail latency of a learned index through a two-level leaf\ndesign. COLIN [ 223] is a cache-friendly learned index structure that leverages a mixture of learned and simple nodes\n(i.e., a heterogeneous node structure).\n6.1.2 Pure Learned Indexes with Delta-Buffer Insertion Strategy .We present the core concepts of PGM [ 57],\na one-dimensional index that highlights the class of Mutable Pure Learned Indexes with Fixed Data Layout and a\nDelta-Buffer Insertion Strategy.\nPGM [57].It has been observed that the initially proposed one-dimensional learned indexes (e.g., RMI) provide\nan empirical error bound without any formal worst-case bound. As a result, the Piecewise Geometric Model index\n(PGM, for short) is proposed with a guaranteed worst-case bound. The steps for constructing the PGM index are as\nfollows: i) Computing optimal piecewise linear segments with a pre-fixed model error bound ùúñ, ii) Storing the segments\nas a triplet of key, slope, and intercept, iii) Repeat the previous steps recursively. PGM uses a delta buffer insertion\nstrategy to support dynamic inserts. Moreover, three variants of PGM have been proposed: i) Compressed PGM for\nspace efficiency, ii) Self-adaptive PGM for a particular query distribution, and iii) Multi-criteria PGM that optimizes for\na user-given requirement, e.g., query time.\n\n16 Abdullah-Al-Mamun, et al.\nThe other indexes in this category are as follows. ASLM [ 113] employs simple single-layer ML models for efficient\nupdate handling. Doraemon [ 182] re-uses pre-trained ML models for similar data distributions to reduce the model\nre-training cost. XIndex [ 183] supports concurrency natively by leveraging a two-phase compaction scheme. In\nConcurrentLI [ 196], XIndex has been extended to XIndex-R and XIndex-H as range and hash indexes, respectively.\nSimilar to XIndex, SIndex [ 194] is a concurrent learned index specifically designed for indexing strings. FINEdex [ 108]\navoids using a large delta buffer and employs instead a delta per training record to support data insertion. FILM [ 128]\nuses a cold data identification mechanism that enables efficient data swapping between disk and memory. DiffLex [ 37]\nis a NUMA-aware learned index that leverages sparse and dense arrays based on the hotness (i.e., cold vs. hot) of the\nkeys. PLIN [ 222] utilizes Optimal Piecewise Linear Representation [ 204] to build an index optimized for non-volatile\nmemory.\n6.1.3 Hybrid Learned Indexes .In this section, we present the core concepts of Bourbon [ 38], a One-dimensional\nthat highlights the class of Hybrid Learned Indexes with Mutable Fixed Data Layout.\nBourbon [38].Bourbon incorporates ML models to enhance the performance of an LSM-tree [ 127]. Moreover,\nBourbon is an updatable ML-enhanced LSM-tree that has been integrated inside WiscKey, a commercial key-value\nstore [ 126]. A key observation in Bourbon is that immutable SSTables are suitable for learning because there are no\nin-place updates. Particularly, the search index block component of the LSM-tree has been replaced with ML models.\nHowever, the learning of the SSTables has been categorized based on whether the SSTables are short- or long-lived. As\na result, at runtime, a simple cost-benefit analyzer has been used to decide whether learning is beneficial.\nThe other indexes in this category are as follows. TridentKV [ 125] adopts an optimized learned index block structure\nto build an ML-enhanced LSM-tree. SA-LSM [ 220] exploits a survival analysis technique in an LSM-tree based key-\nvalue store. FITing-tree [ 60] combines a traditional B+-tree with piecewise linear functions. AccB+tree [ 123] employs\nlinear models to enhance the search operation of a traditional B+-tree. MADEX [ 73] accelerates the intra-page lookup\nperformance of a traditional B+-tree by leveraging ML models. Sieve [ 185] incorporates piecewise linear models with a\ntraditional B+-tree to design an index for efficient block-skipping. LIFOSS [ 211] exploits a two-layer learning model to\ndesign an index targeted for data streams. IFB-tree [ 72] designs interpolation-friendly nodes to enhance the performance\nof a traditional B-tree. Hybrid-LR [ 155] exploits the benefit of linear regression models and B-trees to design a hybrid\nstructure. PLBF [ 187], PLBF++ [ 169], SNARF [ 186], IA-LBF [ 23], PDDBF [ 215], BF-Sandwich [ 136], and AdaBF [ 39]\ncombine ML models with traditional bloom filters to build learned bloom filters. FLIRT [ 205] exploits a circular queue\nstructure to design a parameter-free index for data streams. HERMIT [ 202] uses an ML-enhanced Tiered Regression\nSearch Tree (TRS-Tree) to detect column correlations. S3-SkipList [ 217] employs ML models to select guard entries in a\ntraditional skip-list structure [151] . RUSLI [134] extends RS [92] to support data updates in real time.\n6.2 The Multi-dimensional Case\n6.2.1 A Summary of the Properties of the Mutable Fixed Data Layout Indexes .The properties of the class of\nmutable fixed data layout learned multi-dimensional indexes are presented in Table 2. We highlight the types of queries\nsupported by each of the indexes. For point queries, some indexes do not explicitly provide a query processing algorithm\nor experimental results. However, if an index can easily be extended to support point queries, we have considered that\nthe index supports point query processing. On the other hand, due to nature of the application domain, an index might\nbe designed emphasizing efficiency over accuracy. As a result, an index can support either exact/approximate query\n\nA Survey of Learned Indexes for the Multi-dimensional Space 17\nprocessing. Here, approximate query processing refers to the event of missing some results from the set of exact answer.\nHere, we also indicate whether a supported query type returns an exact or an approximate answer.\nTable 2. Properties of the Mutable Fixed Data Layout Learned Multi-dimensional Indexes\nIndex Data\nLayoutPure\nLearnedHybrid\nLearnedData\nSpacePoint\nQueryRange\nQuerykNN\nQueryJoin\nQuery\nGLIN-ALEX [189] Fixed In-place √ó Projected Exact Exact √ó √ó\nSLBRIN [191] Fixed √ó BRIN Projected Exact Exact Exact √ó\nLSTI [46] Fixed √ó Radix Table Projected Exact Exact Exact √ó\nIF-X [75] Fixed √ó R-tree Native Exact Exact √ó √ó\nPeriod Index [19] Fixed √ó Grid Native Exact Exact √ó √ó\n6.2.2 Pure Learned Indexes with In-Place Insertion in the Projected Space .\nGLIN-ALEX [189].Generic Learned Indexing (GLIN, for short) is a lightweight structure to index complex geometric\nobjects (e.g., polygons). GLIN projects the multi-dimensional geometric objects into one-dimensional intervals of\nZ-values. After that, the objects are sorted based on their minimum Z-value. As a result, any existing order-preserving\none-dimensional traditional or learned indexing technique can be applied to the sorted values. Here, GLIN uses both\nB-tree and ALEX [ 44] indexes to produce GLIN-BTREE and GLIN-ALEX, respectively. Moreover, for a particular leaf\nnode, GLIN maintains the Minimum Bounding Rectangles (MBRs) of all objects of that node. As a result, during the\nML model-based refinement step, a leaf node can be skipped if its MBR does not overlap with the query rectangle‚Äôs\nMBR. GLIN can support containment andintersect queries for complex geometric objects. However, GLIN can have true\nnegative results for intersect queries. As a result, GLIN adopts a query augmentation technique to widen the Z-value\ninterval. This query augmentation technique introduces a trade-off between correctness and pruning time. As ALEX\nadopts an in-place insertion strategy, GLIN-ALEX also uses the same strategy to support inserts.\n6.2.3 Hybrid Learned Indexes in the Projected Space .\nSLBRIN [191].Spatial Learned Index Based on BRIN (SLBRIN, for short) extends the ideas of Block Range Index (BRIN,\nfor short) [ 210] in the context of learned spatial indexing. It aims to achieve high performance during query processing\nwhile maintaining efficient update operation. The main idea is to divide the index objects into two components: History\nRange (HR) and Current Range (CR). HR and CR are beneficial for query processing and update handling, respectively.\nFor creating HR and index entries, SLBRIN first projects the multi-dimensional data into the one-dimensional space\nusing Geohash [ 118]. This enables imposing a linear ordering using the Geohash values. After that, HR with index\nentries are partitioned recursively into ranges so that a one-dimensional learned index can be built for each range.\nSLBRIN employs Multi-layer Perceptron (MLP) as the ML model for training. Notice that CR remains empty during this\nindex construction phase because there are no update operations at this step. During an update operation, the data will\nbe encoded using Geohash to create the index entries. After that, the new index entries will be stored in CR for a short\nperiod of time. The new entries will be moved to HR during the periodic merge operation of CR. After a merge CR\noperation, a model re-training process is invoked to correct the error bound. SLBRIN can support point, range, and\nk-NN queries.\n\n18 Abdullah-Al-Mamun, et al.\nLSTI [46].LSTI is a learned multi-dimensional index for processing spatio-textual data. Here, the multi-dimensional\ndata is first projected into the one-dimensional space using Z/Morton-order [ 140,144]. After that, techniques similar\nto the one-dimensional learned index RadixSpline [ 92] are applied to the projected data. The proposed index can\nsupport data updates using techniques similar to the one-dimensional updateable index ALEX [ 44]. Moreover, it can\nprocess four types of queries: boolean point, boolean range, boolean kNN, and top-k text similarity. Given a specific\ntext description. the boolean point, range, kNN, and top-k queries retrieve a specific location, the objects in a specific\nregion, the objects closest to a specific location, and multiple objects (with approximate text descriptions) in a specific\nspatial region, respectively. Notice that two parameters: max_err and the number of radix bits significantly impact\nthe index performance. As a result, a Random Forest [ 27] regression model has been used to learn the optimal index\nparameters for specific data and query workloads.\n6.2.4 Hybrid Learned Indexes in Native Space .Hybrid learned indexes combine traditional index structures with\nML models to build ML-enhanced index structures. Here, we present the hybrid learned indexes for the multi-dimensional\nspace where these indexes operate in the native data space.\nInterpolation Friendly Indexes [75].Interpolation Friendly Indexes (IF-X, for short) have been introduced in [ 75].\nIF-X leverages the idea of augmenting traditional index structures (e.g., the R-tree) with ML models. Particularly, a\nlightweight technique, e.g., linear interpolation [ 65], has been used for one of the dimensions. The use of a simple\nlinear interpolation method offers the following benefits: faster computation time and fewer parameters. Moreover, to\nminimize interpolation error, the proposed index sorts the data entries on the most suitable dimension that is selected\nbased on the least model prediction error. Furthermore, for performing interpolation, the structure of the leaf nodes of\nthe index has also been modified so that the nodes can accommodate the additional information needed for the ML\nmodels. Notice that the proposed technique does not consider the query workload when selecting the sort dimension.\nPeriod Index [19] .Period Index has been proposed to index intervals (i.e., temporal period data) by position and\nduration. A grid-based data structure with constant time lookup is utilized. The core idea of the proposed index can be\ndivided into two parts: First, splitting the timeline into buckets of fixed size, where each bucket is further partitioned\ninto cells (referring to the position of the interval); Second, arranging the cells in levels (referring to the duration of\nthe interval). Moreover, it proposes finding the length of the buckets by adapting to the underlying distribution using\na cumulative histogram. Hence, an improved version of the Period index, termed Period Index*, has been proposed\nwith this adaptive bucket length. Period Index* can be viewed as a learning-enhanced version of the Period Index. The\nproposed index can process range, duration, and range-duration queries.\n6.2.5 Open Branches in the Taxonomy .As of the time of writing this article, there are no mutable learned multi-\ndimensional indexes in any of the following categories: i) Mutable Fixed Data Layout Multi-dimensional Pure Learned\nIndexes with In-place Insertion Strategy in Native Space, and ii) Mutable Fixed Data Layout Multi-dimensional Pure\nLearned Indexes with Delta-Buffer Insertion Strategy. We envision that the above mentioned open branches will be\nuseful for classifying learned multi-dimensional indexes in the future.\n7 LEARNED MUTABLE INDEXES WITH DYNAMIC DATA LAYOUT\nFor the class of mutable learned indexes, if the layout of the data is arranged/re-arranged by the ML models while\nbuilding the learned index, we refer to them as having a dynamic data layout. In this section, we present the class of\nMutable Learned Indexes with Dynamic Data Layout in both the one- and the multi-dimensional spaces.\n\nA Survey of Learned Indexes for the Multi-dimensional Space 19\n7.1 The One-dimensional Case\n7.1.1 Pure Learned Indexes with In-place Insertion Strategy .In this section, we present the core concepts of\nLIPP [ 200] to highlight the class of Mutable Pure Learned Indexes with with Dynamic Data Layout and In-place Insertion\nStrategy for the One-dimensional case.\nLIPP [200].It has been observed that the performance of an updatable one-dimensional learned index, e.g., ALEX [ 44],\ndegrades in the presence of the ML model‚Äôs misprediction. This degradation occurs because searching after an imprecise\nprediction always incurs overhead. To address this issue, the Learned Index with Precise Positions (LIPP, for short) has\nbeen proposed. The main idea behind LIPP is to avoid misprediction by ensuring that the key-to-position mapping is\nprecise and eliminating the need for localized search following each misprediction. Consequently, the lookup cost is\nbounded by the tree height. Additionally, each LIPP node contains an ML model, a bit vector indicating entry type, and\nan array of entries. Each entry can be of the following types: NULL, DATA, or NODE. The NULL type indicates an\nunused slot, the DATA type represents a single entry, and the NODE type denotes a child node at the next level. Notably,\nLIPP does not distinguish between non-leaf and leaf nodes. During the construction phase of LIPP, if multiple keys map\nto the same position, a new child node is created. To ensure even distribution of mapping, kernelized linear models\nare utilized. Since LIPP is a sorted index, the selected kernel function must be monotonically increasing. The Fastest\nMinimum Conflict Degree (FMCD) algorithm is proposed for computing the model of each LIPP node. Furthermore,\nLIPP employs an in-place insertion strategy akin to ALEX. An enhanced version of LIPP supporting concurrency, LIPP+,\nhas been introduced [199].\nThe other indexes in this category are as follows. NFL [ 201] exploits a key distribution-transformation method so\nthat the learned index can be built in the easy-to-learn transformed space. DILI [ 111] employs linear regression models\nto design a distribution-driven index structure. WIPE [ 197] is designed for reducing the issue of write-amplification in\nnon-volatile memory.\n7.1.2 Open Branches in the Taxonomy .As of now, there are no mutable learned one-dimensional indexes in any\nof the following categories in the one-dimensinoal case: i) Mutable Pure Learned Indexes with Dynamic Data Layout\nand Delta-Buffer Insertion Strategy, and ii) Mutable Hybrid Learned Indexes with Dynamic Data Layout. We envision\nthat the above mentioned open branches will be useful for classifying learned one-dimensional indexes in the future.\n7.2 The Multi-dimensional Case\n7.2.1 A Summary of the Properties of the Mutable Indexes with Dynamic Data Layout .The properties of the\nclass of mutable learned multi-dimensional indexes with dynamic data layout are presented in Table 3. In the table,\nwe present the types of queries supported by each of the learned multi-dimensional indexes. For point queries, some\nindexes do not explicitly provide a query processing algorithm or experimental results. However, if an index can easily\nbe extended to support point queries, we have considered that the index supports point query processing. Moreover, we\nalso indicate whether a supported query type returns an exact or approximate answer. Note that we have excluded\nlearned multi-dimensional bloom filters (e.g., LPBF [ 227], PA-LBF [ 214]) in the context of supported query types because\nthey are considered probabilistic existence index structures.\n7.2.2 Pure Indexes with In-place Insertion in the Projected Space .\n\n20 Abdullah-Al-Mamun, et al.\nTable 3. Properties of the Mutable Learned Indexes with Dynamic Data Layout in the Multi-dimensional Space\nIndex Data\nLayoutPure\nLearnedHybrid\nLearnedData\nSpacePoint\nQueryRange\nQuerykNN\nQueryJoin\nQuery\nLISA [110] Dynamic In-place√ó Projected Exact Exact Exact √ó\nRSMI [153] Dynamic In-place√ó Projected Exact Approx. Approx.√ó\nBMT [106] Dynamic In-place√ó Projected Exact Exact Exact √ó\nLMSFC [61] Dynamic In-place√ó Projected Exact Exact √ó √ó\nLIMS [184] Dynamic In-place√ó Native Exact Exact Exact √ó\nMTO [43] Dynamic In-place√ó Native Exact Exact √ó Exact\nWISK [174] Dynamic Delta Buffer √ó Native Exact Exact Exact √ó\nLPBF [227] Dynamic√ó CBF Projected ‚Äî ‚Äî ‚Äî ‚Äî\nPA-LBF [214] Dynamic√ó CBF Projected ‚Äî ‚Äî ‚Äî ‚Äî\nRLR-tree [67] Dynamic√ó R-tree Native Exact Exact Exact Exact\nRW-tree [48] Dynamic√ó R-tree Native Exact Exact Exact √ó\nACR-tree [78] Dynamic√ó R-tree Native Exact Exact Exact √ó\nPLATON [206] Dynamic√ó R-tree Native Exact Exact Exact √ó\nWaffle [34] Dynamic√ó Grid Native Exact Exact Exact √ó\nFHSIE [180] Dynamic√ó Grid Native Exact Exact Exact √ó\nLISA [110].Considering the issues of storage consumption and high I/O cost of the R-Tree, LISA, a disk-based\nLearned Index for Spatial Data, has been introduced. The main idea of LISA is to generate a searchable data layout\nin disk pages for an arbitrary spatial dataset using a machine learning model. LISA consists of four parts: i) Space\npartitioning into a series of grid cells; ii) A mapping function M for mapping search keys to 1D values; iii) A monotonic\nprediction function SP that takes the output of M as input and predicts the shard ID; iv) Local models for each shard for\nallocating, splitting, and merging pages. During query processing, given a query rectangle ùëûùëü, it is decomposed into\nsmaller rectangles. Then, the shard prediction function ùëÜùëÉis used to select the shard in each cell (overlapping with ùëûùëü).\nSubsequently, local models are utilized to find the address of pages overlapping with ùëûùëü. LISA supports range queries,\nKNN queries, insertions, and deletions. For KNN query processing, each KNN query is converted into a series of range\nqueries. LISA supports dynamic insertions by employing a model-based in-place insertion policy.\nRSMI [153].Recursive Spatial Model Index (RSMI, for short) is a disk-based updateable learned spatial index. Due to\nuneven gaps in the empirical CDF of the Z-order space-filling curve, RSMI leverages rank space-based ordering [ 154]\nto project multi-dimensional points into the one-dimensional space. Data points are sorted in ascending order based\non their one-dimensional projection values. Then, a set of points are packed into a block based on a predefined block\nsize. To scale with larger datasets, RSMI recursively partitions the data and trains models for each partition. Moreover,\nmultiple ML models are trained to learn the mapping (i.e., from search key to disk block ID) in the projected space.\nRSMI can support point, range, and kNN queries on point data. The proposed RSMI produces approximate answers with\nhigh accuracy. For exact query processing, RSMI advocates for a search operation similar to an R-tree. Note that all the\nproposed techniques are primarily focused on point data. Query processing performance may be negatively impacted\nif the proposed method is applied to data objects with extension. RSMI supports dynamic inserts by employing an\nin-place insertion policy.\n\nA Survey of Learned Indexes for the Multi-dimensional Space 21\nBMT [106].It has been observed that each existing SFC has a pre-fixed projection function that neither considers\nthe underlying data distribution nor the query workloads. As a result, a specific SFC cannot perform well across a\nvariety of datasets and query workloads because a fixed Bit Merging Pattern (BMP, for short) is applied to the entire\ndataset to build a particular SFC (e.g., bit interleaving for Z-order). Thus, for a given data and query workloads, the\ncore idea is to apply different BMPs to different subspaces to construct a Piecewise SFC. Hence, a tree-based structure\nhas been designed where each leaf node refers to a subspace. Moreover, for each subspace (i.e., leaf node), the path\nfrom root to leaf creates the BMP by representing each internal node as a bit value for different dimensions. The\nproposed Bit Merging Tree (BMT, for short) preserves two important properties of an SFC: Injection (i.e., generating\na unique value for a given input), and Monotonicity [ 100]. The BMT construction process has been formulated as a\nsequential decision-making process so that reinforcement-learning-based (RL-based) techniques can be leveraged to\nlearn an effective BMT construction policy. Particularly, a greedy policy has been incorporated into a Monte Carlo\nTree Search (MCTS, for short) [ 28]. The main idea of the greedy policy is to choose an action (i.e., choose a bit for each\nnode from a pool of candidate bits) so that the RL agent can maximize its reward (e.g., query performance). Notice that\nexecuting queries on BMT to calculate the reward is a costly process. As a result, a new metric called ScanRange has\nbeen proposed to calculate the reward efficiently. The proposed BMT has been integrated into two previously proposed\nlearned indexes as a replacement of a traditional SFC: BMT+RSMI [153] and BMT+ZM-index [190].\nLMSFC [61].Learned Monotonic Space Filling Curve (LMSFC, for short) has been proposed to learn a parameterized\nmonotonic Z-order for given data and query workloads. The outcome of learning a parameterized Z-order is to find\nparameters for a particular instance to minimize query processing cost. To minimize the cost of query processing for a\ngiven instance, the problem of learning an optimal parameterized SFC is formulated as an optimization problem. A\nBayesian optimization method termed SMBO [ 79] is used to approximately solve the optimization problem. Then, an\noffline dynamic programming-based (and a heuristic-based approach) is proposed to pack data points into disk pages\nbased on a density-based cost function. The goal is to reduce the dead space of the MBRs of the disk pages by packing\nthe data points as tightly as possible. After packing the points into the pages, the minimum z-value from each page is\nused to create a sorted array. Then, a one-dimensional learned index (e.g., PGM [ 57]) is used to find the page (given a\nz-value as input). However, even with an instance-optimized Z-order SFC, the issue of false positives remains. As a\nresult, recursive splitting of the query rectangle is proposed to further reduce the number of false positives during the\nsearch operation. The proposed LMSFC supports inserts by adopting an in-place insertion strategy.\n7.2.3 Pure Indexes with In-place Insertion in Native Space .\nLIMS [184].It has been observed that most existing techniques for learned multi-dimensional indexes (in vector\nspace) may not be directly applicable for indexing in metric space due to the lack of coordinate structure and dimension\ninformation. Moreover, only the property of triangular inequality can be leveraged for pruning in a metric space. As a\nresult, a Learned Index for Metric Space (LIMS, for short) has been proposed to efficiently support point, range, and\nkNN queries in the metric space. To achieve these goals, LIMS clusters the data objects into multiple groups, and builds\na learned index for each cluster. Additionally, it selects a set of data objects as pivots for each cluster so that the distance\nbetween the pivot and the data objects can be pre-computed. Notice that the pre-computation of distances in each\ncluster enables LIMS to build learned indexes on these distance values. It has been observed that the selection of pivots\ncan significantly impact the performance during query processing.\n\n22 Abdullah-Al-Mamun, et al.\nMTO [43].Multi-Table Optimizer (MTO, for short) is designed to reduce I/O cost for analytical workloads in multi-\ntable datasets. Similar to the previously proposed Qd-tree [ 207], MTO leverages reinforcement learning to route each\nrecord to the corresponding data block, ensuring that all elements in one block are from the same table. Besides using\nsimple predicates (e.g., A.X <100), MTO proposes using join-induced predicates to optimize the layout for all tables.\nFor join-induced predicates, it evaluates subqueries in these predicates to obtain indexes that are candidates for join\noperations. Then, these predicates and the table are sent to the Qd-tree as input to train the model. During query\nprocessing, MTO uses the Qd-tree to guide block access. To support dynamic workloads, MTO adopts an in-place\ninsertion policy while taking into account block access reduction and sub-tree reorganization cost in the reward\nfunction.\n7.2.4 Pure Indexes with Delta Buffer Insertion in Native Space .\nWISK [174].Workload-aware Learned Index for Spatial Keyword (WISK, for short) is a learned spatial index for\nkeyword queries. Given data and query workloads, WISK employs ML models that can learn using both spatial and\ntextual information. This is achieved in two phases: firstly, it learns an optimal data layout for a particular query\nworkload, and builds the index based on the learned layout. The index construction steps are as follows: i) ML models\nare trained to approximate the CDF of the spatio-textual objects, ii) A cost estimation function is defined based on\nthe learned CDF, iii) SGD is applied to learn the optimal partition by minimizing the cost, iv) The hierarchy of WISK\nis constructed using a bottom-up packing approach, v) The packing process has been transformed into a sequential\ndecision-making process so that it can be solved using a reinforcement learning technique. As the query processing\ncost of WISK largely depends on how the data objects are partitioned during the construction of the index, an optimal\npartitioning problem has been formulated to construct the leaf nodes of the WISK. It has been shown that the problem\nof partitioning with optimal cost is NP-hard. As a result, a heuristic algorithm using Stochastic Gradient Descent\n(SGD) [26] has been proposed. WISK supports dynamic inserts by employing a delta-buffer insertion strategy.\n7.2.5 Hybrid Learned Indexes in the Projected Space .\nLPBF [227].Learned Prefix Bloom Filter (LPBF, for short) is a learned bloom filter that is particularly designed for\nspatial data. The core idea of the proposed LPBF is to project multi-dimensional spatial data into one-dimensional\nbinary code using a space-filling curve (e.g., Z-order). After that, the prefixes of binary z-values are grouped into several\nclusters based on a pre-defined parameter. Moreover, the suffixes of the same prefix are learned using sub-Learned\nBloom Filters (sub-LBF) [ 129]. LPBF leverages a traditional Counting Bloom Filter (CBF, for short) [ 51] as a backup\nfilter to support data updates. The use of CBF and sub-LBF helps minimize the false positive rate and the model training\ntime for the proposed method.\nPA-LBF [214].Prefix-Based and Adaptive Learned Bloom Filter (PA-LBF) is an extended version of the previously\nproposed LPBF [ 227]. Notice that the concept of adaptation has been introduced during the process of sub-LBF model\ntraining. For each sub-LBF, the main extension is the use of an adaptive learning process to calculate the number of\nfilter layers based on a pre-defined threshold. This adaptive learning approach enables PA-LBF to further minimize the\nfalse positive rate compared to the previously proposed LPBF.\n7.2.6 Hybrid Learned Indexes in Native Space .\n\nA Survey of Learned Indexes for the Multi-dimensional Space 23\n7.2.7 RLR-tree [67].During the traditional R-tree construction process, there are several existing node splitting\nstrategies (e.g., linear, quadratic) [ 69]. For a chosen node splitting strategy, the query processing performance will vary\nfor different data and query workloads. Reinforcement Learning-based R-Tree (RLR-tree, for short) formulates the\nChooseSubtree and Split operations of a traditional R-tree as a Markov Decision Process (MDP) [ 152] (i.e., sequential\ndecision-making process). It employs RL techniques to learn models for the ChooseSubtree and Split operations. Notice\nthat the RLR-tree does not need to modify the existing query processing algorithms; instead, it aims to construct a better\nR-tree by optimizing the ChooseSubtree and Split operation. As a result, it can support all the existing query processing\nalgorithms designed for the R-tree. Moreover, the RLR-tree presents the design of state space, action space, transition,\nand reward for MDP formulation. For example, in the context of MDP formulation for ChooseSubtree, while inserting a\nnew object, a state is a tree node with the following features: area increase, perimeter increase, overlap increase, and\noccupancy rate. For the action space, given a current state, the RL agent picks a child node into which the new entry\nwill be inserted. For the transition, given a state and an action, the agent moves to a child node. If the child node is a leaf\nnode, the agent reaches a terminal state. In the context of designing the reward function, the RLR-tree proposes using a\nreference tree with a pre-defined ChooseSubtree and Split strategy. As a result, the reward is calculated based on the\ndifference between the query processing cost of the RLR-tree and the reference R-tree. Based on the above-mentioned\nMDP formulation, the RLR-tree adopts a Deep Q-Network learning method (DQN, for short) [ 137] for training the\nagent.\nRW-tree [48].A traditional R-tree is constructed using one of the following methods: bulk loading vs. one-by-one\ninsertion. In the context of one-by-one insertion, most existing techniques do not take the query workload into account\nwhile constructing the R-tree. As a result, a learning-based framework has been proposed for query workload-aware\nR-tree construction. The goal is to achieve better query processing performance for future queries generated from a\nsimilar query distribution. The proposed RW-tree is built on the following core concepts: learning the query workload\ndistribution and leveraging a cost model for the accurate approximation of query execution time. For learning the\nworkload distribution, in the pre-processing step, the entire space is partitioned into grid cells, and all the queries\nare mapped into their corresponding grid cell. After that, the statistics of the queries are collected per cell. Next, the\ngrid cells are clustered based on queries with similar statistics. Given the learned workload distribution and a new\ndata insertion choice, a cost model is proposed to approximately measure the query execution time. This enables the\nRW-tree to select the insertion strategy with the lowest cost. The RW-tree can process both range and k-NN queries.\nACR-tree [78].It has been observed that existing methods for R-tree node packing can be categorized into the\nfollowing types: i) Heuristics to pack objects into parents in a bottom-up manner, or ii) A greedy approach to partition\nnodes into child nodes in a top-down fashion. The goal of the existing methods is to optimize the R-tree construction\nfor the short-term (i.e., without considering the long-term tree construction cost). Moreover, these heuristic-based or\ngreedy methods also try to pack the nodes as full as possible. As the optimal R-tree node packing problem is NP-hard,\nan RL-based method has been proposed to construct the R-tree for optimizing the long-term tree cost. Particularly, the\ntop-down R-tree construction methods have been formulated as an MDP [ 152]. Then, an Actor-Critic [ 66] based RL\nmethod has been applied to construct the Actor-Critic R-tree (ACR-tree, for short). The Critic part is designated to\nestimate the long-term tree cost, and the Actor part is designated to make decisions (e.g., split or pack). The ACR-tree\nuses a grid-based method to better encode the spatial information. Moreover, based on the estimation of the long-term\ncost, it either splits a node or packs an object onto a single child node. Notice that the Actor-Critic training process is\n\n24 Abdullah-Al-Mamun, et al.\ntime-consuming. As a result, a bottom-up model training process with training sharing has been proposed to speed up\nthe training process. The proposed ACR-tree supports both range and kNN queries.\nPLATON [206].Similar to the ACR-tree [ 78], it has been observed that both top-down and bottom-up R-tree packing\nmethods cannot adapt to particular data and query workloads due to the use of fixed heuristics. Moreover, the top-down\npacking method adheres to a sub-optimal node partitioning policy by ignoring dependencies between partition decisions\namong different R-tree nodes. As a result, for given data and query workloads, a top-down Packing with Learned\nPartition (PLATON, for short) policy has been proposed to overcome the mentioned limitations. Notice that the problem\nof optimal R-tree packing has been proved to be NP-hard. As a result, the proposed node partitioning policy is learned by\nleveraging an RL-based MCTS [ 28] technique. However, in the presence of a large state space and long action sequence,\nthe existing MCTS algorithm suffers from slow convergence. Hence, a divide-and-conquer strategy has been proposed\nto reduce the size of the state space. Moreover, optimization techniques, e.g., early termination and level-wise sampling,\nhave been applied for faster convergence of the MCTS algorithm. During the R-tree construction phase, for particular\ndata and query workloads, the top-down packing is performed based on the RL-based learned partitioning policy.\nWaffle [34].Waffle is an in-memory grid-based indexing system for moving objects. Waffle is built on the following\nmajor components: grid index manager, lock manager, transaction manager, re-grid manager, and an online configuration\ntuning component. Notice that the proposed indexing system contains many different configuration knobs. As a result,\nthe performance of Waffle heavily depends on the configuration of the knobs. Moreover, due to the dynamic nature of\nmoving objects, the settings of different knobs require online adjustments. To address this issue, an online configuration\ntuning component, termed Wafflemaker, has been proposed. Wafflemaker is an RL-based component that suggests an\noptimized knob configuration for Waffle in an online fashion.\nFHSIE [180].The Fast Hybrid Spatial Index with External Memory Support (FHSIE, for short) has been proposed to\nefficiently extend the concept of learned spatial indexing to secondary storage. FHSIE clusters spatial objects using\nunsupervised ML techniques based on a few parameters. During the index construction phase, the K-means clustering\nmethod is applied recursively to the spatial data to build a height-balanced hierarchical structure. Then, a grid structure\nis incorporated at the bottom level of the previously created hierarchical structure. The purpose of the grid structure\nis to accurately find the intersections of bottom-level clusters with an input query. For extending FHSIE to external\nmemory, the main idea is to allocate disk blocks for each of the following components separately: i) Internal models,\nii) Bottom-level models, and iii) Cells of the grid. FHSIE supports inserts using an in-place insertion strategy. Although\nthe proposed index can efficiently support point, range, and KNN queries in secondary storage, the index construction\ntime can be significantly higher than that of traditional spatial indexes.\n7.3 Open Branches in the Taxonomy\nAs of the time of writing this survey paper, there are no learned multi-dimensional indexes in the following category:\nMutable Dynamic Data Layout Pure Learned Indexes with Delta-Buffer Insertion Strategy in Projected Space. We\nenvision that the above mentioned open branch will be useful for classifying learned multi-dimensional indexes in the\nfuture.\n8 A SUMMARY OF ML TECHNIQUES FOR LEARNED MULTI-DIMENSIONAL INDEXES\nA summary of ML techniques used for learned multi-dimensional indexes is presented in Table 4.\n\nA Survey of Learned Indexes for the Multi-dimensional Space 25\nTable 4. A Summary of ML Techniques for Learned Multi-dimensional Indexes\nIndex ML Techniques\nLSTI [46] RadixSpline, Random Forest Regression\nLISA [110] Lattice Regression Model\nLIMS [184] Ploynomial Regression\nSageDB-LMI [96] Linear Model\nTsunami [45] Linear Model\nCOAX [70] Linear Model\nGLIN_ALEX [189] Linear Model\nZ-index [146] Density Model\nFLOOD [141] Piecewise Linear Model, RMI\nPolyFit [114] Polynomial Function\nIF-X [75] Linear Interpolation\nSPRIG [218] Spatial Interpolation Function\nSPRIG+ [219] Spatial Interpolation Function\nPeriod-index* [19] Cumulative Histogram\nLMI-2 [176] K-means Clustering, Gaussian Mixture Model\nFHSIE [180] K-means Clustering\nRW-tree [48] Density based Clustering\nWAZI [145] Random Forest Density Estimation Model\nLMSFC [61] Random Forest\n‚ÄúAI+R‚Äù-tree [7] Random Forest, Decision Tree\nCaseSpatial [147] RadixSpline\nSpatial-LS [148] RadixSpline\nDistance-bounded [212] RadixSpline\nLPBF [227] Gradient Boosting\nPA-LBF [214] Gradient Boosting\nZM-index [190] Neural Network, Linear Model\nLearnedKD [209] Neural Network\nLMI-1 [12] Neural Network\nML-enhanced [88] Neural Network, Linear Model\nCompressedBF [40] Neural Network\nLearnedBF [129] Neural Network\nML-index [41] Neural Network\nHM-index [192] Neural Network\nRSMI [153] Neural Network\nSLBRIN [191] Neural Network\nQD-tree [207] Reinforcement Learning\nMTO [43] Reinforcement Learning\nRLR-tree [67] Reinforcement Learning\nWaffle [34] Reinforcement Learning\nBMT [106] Reinforcement Learning\nWISK [174] Reinforcement Learning\nACR-tree [78] Reinforcement Learning\nPLATON [206] Reinforcement Learning\n9 BENCHMARKING LEARNED INDEXES\n\n26 Abdullah-Al-Mamun, et al.\nIn this section, we present the studies conducted for benchmarking of either the one- or the multi-dimensional\nlearned indexes.\n9.1 The One-dimensional Case\nSeveral studies on benchmarking learned one-dimensional indexes are available in the literature. In [ 24], several ideas\nhave been proposed for designing new benchmarks that are better suited to evaluate learned systems. The Search\nOn Sorted Data Benchmark (SOSD) has been presented in [ 91,131], and However, SOSD only considers in-memory\nread-only workloads. Moreover, SOSD covers three learned indexes: RMI [ 97], RS [ 92], and PGM [ 57]. A critical analysis\nof RMI has been presented in [ 130]. This analysis investigates the impact of each hyperparameter of RMI on prediction\naccuracy, lookup time, and build time. Moreover, it provides guidelines to configure RMI. In [ 11], a micro-architectural\nanalysis of ALEX has been presented. This study goes beyond high-level metrics (e.g., latency and index size) towards\nlower-level metrics (e.g., cache misses per cache level, average number of execution cycles). The experiments are\nbased on Intel‚Äôs Top-Down Micro-architecture Analysis Method (TMAM) [ 208]. Wongkham, et al. [ 199], evaluate\nupdateable learned indexes. Five one-dimensional learned indexes have been included in this study: ALEX, PGM,\nLIPP [ 200], XIndex [ 183], and FINEdex [ 108]. Moreover, this study proposes to extend ALEX and LIPP to support\nconcurrency as ALEX+ and LIPP+. In [ 181], a testbed has been developed to facilitate the design and testing of existing\nand upcoming learned index structures. It also presents the design choices of key components in learned indexes (e.g.,\ninsertion, concurrency, bulk loading). The experiments cover eight one-dimensional learned indexes. In [ 99], it has\nbeen shown how to extend four in-memory updatable one-dimensional learned indexes (e.g., Fitting-tree [ 60], ALEX,\nPGM, LIPP) in a disk-based setting. An extensive experimental evaluation is presented for the proposed extensions.\nIn [62], several one-dimensional updateable learned indexes (e.g., RMI, RS, ALEX, XIndex, FITing-tree, and PGM) have\nbeen evaluated along four criteria: approximation algorithm, index structure, insertion strategy, and model re-training\nstrategy. Moreover, it also provides several guidelines for designing learned indexes. The concept of using ML models as\na replacement for a hash function has been introduced initially as a hash model index [ 97]. In [ 163] and [ 164], extensive\nexperimental studies have been conducted to analyze the performance of learned hash index structures. The main\ncontribution of these studies is to provide insights on choosing a learned hash index versus a traditional hash index\nstructure.\n9.2 The Multi-dimensional Case\nTo the best of our knowledge, there is no comprehensive benchmarking on learned multi-dimensional indexes. As a\nresult, it is still an important and open area of research.\n10 TOWARDS THE INTEGRATION OF LEARNED INDEXES INTO PRACTICAL SYSTEMS\nA few studies have been conducting on integrating learned index structures into practical systems. Although a few\none-dimensional learned indexes have been successfully integrated into practical database/storage engines, similar\nefforts are still in progress in the context of learned multi-dimensional indexes. In this section, we discuss the studies\nthat take a step forward towards the integration of learned indexes into practical systems.\n10.1 The One-dimensional Case\nIn Google-index [ 4], a learned index has been integrated into a distributed, disk-based database system: Google‚Äôs\nBigtable [ 30]. The integrated learned index has demonstrated significant improvements in read latency and memory\n\nA Survey of Learned Indexes for the Multi-dimensional Space 27\nfootprint. In BOURBON [ 38], the proposed ML-enhanced LSM-tree has been incorporated into a production-quality\nsystem, and has achieved high performance compared with its traditional counterpart. In SA-LSM [ 220], the proposed\nML-enhanced LSM-tree has been incorporated into a commercial-strength LSM-tree storage engine, namely X-Engine.\nSieve [ 185] has been integrated in a distributed SQL query engine, namely Presto [ 172]. Hybrid-LR [ 155] has been\nimplemented in PostgreSQL [179].\n10.2 The Multi-dimensional Case\nIn ELSI [ 116], a system for efficiently building and rebuilding learned multi-dimensional indexes has been proposed.\nELSI supports any learned spatial index that projects the multi-dimensional points into one dimension (i.e., termed as\nprojected space in this survey) to impose an ordering and process queries using the predict-and-scan method. Two\nindex-building methods have been proposed based on space partitioning and reinforcement learning. Moreover, ELSI\nhas been combined with four multi-dimensional indexes: ZM-index [ 190], ML-index [ 41], RSMI [ 153], and LISA [ 110].\nHowever, the integration of ELSI into a commercial database system (e.g., PostgreSQL) has been mentioned as future\nwork. In SageDB [ 42], the ideas of the Qd-tree [ 207] have been implemented for creating the data layout inside a\npractical system. In PLATON [ 206], the proposed ML-enhanced R-tree has been implemented on top of a real-world\nspatial library: libspatialindex [ 1], and a practical database system: PostgreSQL 14.3[3], with the PostGIS[ 2] extension.\n11 OPEN CHALLENGES AND FUTURE RESEARCH DIRECTIONS\nIn this section, we highlight the open research challenges and future research directions in the context of learned\nmulti-dimensional indexes.\n11.1 Total Ordering and Error Bound\nIn the context of one-dimensional learned indexes, in most cases, the ML models are trained with the assumption that\nthe underlying data is sorted with a total order. However, prediction-based ML models inherently do not provide any\nguarantee of accuracy when applied to unseen test data. As a result, if the ML models make an error during prediction,\nit needs to be corrected by a local search based on a predefined error bound. Notice that this error bound can be easily\nachieved in the case of one-dimensional data because we can sort the data (i.e., total ordering). However, this is not\nthe case for multi-dimensional data because there is no total ordering in the multi-dimensional space. As a result, a\nclass of learned multi-dimensional indexes projects the data into one-dimensional space to impose an ordering. Various\nspace-filling curves have been used in this context, each with different advantages and limitations. On the other hand,\nthe class of learned multi-dimensional indexes in native space does not employ a projection function. However, some\nlearned multi-dimensional indexes might select one of the dimensions to impose an order in native space. Each of the\nlearned multi-dimensional indexes in native space, for exact query processing, has proposed a different mechanism for\nerror correction. However, providing an error bound is challenging for the latter class of indexes, and further research\naddressing this aspect is needed.\n11.2 Choice of ML Models\nDue to recent advances in the area of machine learning, there are numerous types of ML models available with various\nlevels of model complexity. In general, a complex model is expected to learn the underlying data distribution better\nthan a simpler model. However, in the context of learned indexes, it is normally suggested to adopt a simple ML model\n(e.g., linear regression, decision tree) whenever possible. The main reason behind this suggestion is performance (e.g.,\n\n28 Abdullah-Al-Mamun, et al.\nless training time, smaller model size, low model prediction latency). Notice that traditional database indexes (e.g., B+\ntree, R-tree) are highly optimized data structures. As a result, a learned index should avoid using complex ML models\nso that the model building time and prediction latency do not become bottlenecks to achieving low index construction\ntime and high query processing performance, respectively. However, it has also been observed that a customized ML\nmodel yields better performance than basic ML models. For example, in [ 50], a tailored regression model has been\nproposed to achieve better performance. Moreover, there is a growing interest in designing and using deep neural\nnetwork-based ML models [ 9,10,53] in the context of learned indexes. As a result, the choice of ML models needs to be\nfurther investigated for designing learned multi-dimensional indexes.\n11.3 Model Training and Re-training\nTraining ML models is one of the most important parts of constructing learned indexes. Most of the proposed learned\nindexes try to minimize the model training time as much as possible. Moreover, changes in the underlying input\ndata/query distribution should be detected as soon as possible, and a model re-training process should be triggered\nwhen necessary. Although these challenges are addressed in many of the proposed indexes, there is still ample room for\nfuture research in this area.\n11.4 Supporting Dynamic Inserts/Updates\nThe initial learned indexes only considered read-only workloads in a static scenario. After that, new methods have been\nproposed to support insert/update operations. However, supporting inserts/updates comes with the cost of periodic\nre-organization (e.g., re-training) of the learned index structures or incorporating mechanisms that require additional\nspace (e.g., a gapped array). In this survey, we have mainly observed two main strategies to support dynamic inserts:\nIn-place and Delta buffer. Although these challenges are addressed in the class of mutable multi-dimensional learned\nindexes, further research is needed to investigate the advantages and disadvantages of each insertion strategy.\n11.5 Concurrency\nIn the context of designing learned indexes, for most cases, supporting concurrency has come as an afterthought. Only\na few proposed methods (marked with an * symbol in the taxonomy 2) discuss the issue of concurrency and propose\ntechniques to support concurrency natively. As a result, future research in this area should treat the issue of concurrency\nas a first-class citizen while designing or implementing learned indexes.\n11.6 Index Compression\nBoth one- and multi-dimensional indexes have demonstrated significant benefits in terms of reduced storage require-\nments. Moreover, in the context of inverted indexes [ 226], the benefits of index compression by incorporating a learned\none-dimensional index into an inverted index structure have been studied in [ 143]. Additionally, the advantages of a\nlearned multi-dimensional bloom filter for index compression are studied in CompressLBF [ 40]. However, exploring the\npotential benefits of index compression using learned multi-dimensional indexes in the context of other index types is\nan interesting direction for future research.\n11.7 Security\nThe issue of security in the context of learned indexes is also little explored. The impact of poisoning attacks has been\ndiscussed in a recent study [ 94]. It has been shown that ‚Äúevery injection into the training dataset has a cascading impact\n\nA Survey of Learned Indexes for the Multi-dimensional Space 29\non multiple data values.\" This attack can significantly reduce the performance of learned indexes. As a result, robust\nlearned indexing methods should be designed to tackle issues related to security. Exploring this direction in the context\nof multi-dimensional learned indexes is an open research topic.\n11.8 Benchmarking\nAlthough several studies have been performed to benchmark the performance of one-dimensional learned indexes,\na comprehensive benchmark on learned multi-dimensional indexes is still missing. There are many open-sourced\nmulti-dimensional datasets available in various repositories (e.g., UCR-STAR [ 64]). However, collecting real query\nworkloads for multi-dimensional and spatial data is challenging. As a result, many studies use real datasets but synthetic\nquery workloads. A benchmarking study on multi-dimensional learned indexes with real data and query workloads\nwill likely fill this gap.\n11.9 Theoretical Analysis\nThere are a few theoretical studies [ 31,54,55,213] that mathematically analyze the reasons behind the performance gain\nof learned one-dimensional indexes over traditional indexes. In [ 54,55], it has been shown that learned one-dimensional\nindexes are provably better than traditional indexes. Moreover, it has been proven that under certain assumptions,\nlearned one-dimensional indexes can answer queries in ùëÇ(log log ùëõ)expected query time [ 213]. Furthermore, for the\nclass of learned one-dimensional indexes adopting piece-wise linear segments, the theoretical analysis on the choice\nof the parameter ùúñhas been presented in [ 31]. As a result, theoretical analysis of the various components of learned\nmulti-dimensional indexes is needed to better understand the benefits and limitations of existing techniques.\n11.10 Leveraging GPUs for Learned Indexes\nThe benefit of natively implementing a learned one-dimensional index on a GPU has been presented in [ 224]. Particularly,\nthe PGM index has been implemented on a GPU so that the index can exploit the high concurrency of the GPU. During\nquery processing, GPU-PGM achieves an order of magnitude performance gain over the CPU-PGM. Similar investigation\nin the context of learned multi-dimensional indexes is an interesting direction for future research.\n12 CONCLUSION\nIn this survey, we provide an up-to-date coverage of the state-of-the-art in learned multi-dimensional indexes. We have\nintroduced a taxonomy to categorize both one- and multi-dimensional indexes using the following criteria: i) Learning\nthe index vs. Indexing the learned models, ii) Immutable vs. mutable learned indexes, iii) Mutable fixed data layout\nvs. Mutable dynamic data layout learned indexes, iv) One-dimensional vs. multi-dimensional learned indexes, v) Pure\nlearned indexes vs. hybrid learned indexes, vi) In-place vs. delta-buffer insertion strategy for learned indexes, and\nvii) Projected vs. native space learned indexes. Moreover, we expect that future learned index structures can be placed\nin the introduced taxonomy in this paper based on the proposed criteria. We summarize the core ideas of 43learned\nmulti-dimensional indexes, and highlight the similarities/differences of more than 60learned one-dimensional indexes.\nFurthermore, we have listed the supported query types and the underlying ML techniques for each of the learned\nmulti-dimensional indexes. We include a timeline diagram that shows the evolution of both one- and multi-dimensional\nlearned index structures. Finally, we have discussed several open challenges and future research opportunities in the\narea of learned multi-dimensional indexes.\n\n30 Abdullah-Al-Mamun, et al.\n13 ACKNOWLEDGEMENTS\nWalid G. Aref acknowledges the support of the National Science Foundation under Grant Number IIS-1910216.\nREFERENCES\n[1] Accessed in 2023. libspatialindex 1.9.3. https://libspatialindex.org/en/latest/\n[2] Accessed in 2023. PostGIS. https://postgis.net/\n[3] Accessed in 2023. PostgreSQL 14.3. https://www.postgresql.org/\n[4] Hussam Abu-Libdeh, Deniz Altƒ±nb√ºken, Alex Beutel, Ed H Chi, Lyric Doshi, Tim Kraska, Andy Ly, Christopher Olston, et al .2020. Learned indexes\nfor a google-scale disk-based database. arXiv preprint arXiv:2012.12501 (2020).\n[5] Charu C Aggarwal. 2015. Data classification. In Data Mining . Springer, 285‚Äì344.\n[6] Charu C Aggarwal et al. 2018. Neural networks and deep learning. Springer 10, 978 (2018), 3.\n[7] Abdullah Al-Mamun, Ch Md Rakin Haider, Jianguo Wang, and Walid G Aref. 2022. The ‚ÄúAI+ R‚Äù-tree: An Instance-optimized R-tree. In 2022 23rd\nIEEE International Conference on Mobile Data Management (MDM) . IEEE, 9‚Äì18.\n[8] Abdullah Al-Mamun, Hao Wu, and Walid G Aref. 2020. A Tutorial on Learned Multi-dimensional Indexes. In Proceedings of the 28th International\nConference on Advances in Geographic Information Systems . 1‚Äì4.\n[9]Domenico Amato, Giosu√© Lo Bosco, and Raffaele Giancarlo. 2022. On the Suitability of Neural Networks as Building Blocks for the Design of\nEfficient Learned Indexes. In International Conference on Engineering Applications of Neural Networks . Springer, 115‚Äì127.\n[10] Domenico Amato, Giosu√© Lo Bosco, and Raffaele Giancarlo. 2023. Neural networks as building blocks for the design of efficient learned indexes.\nNeural Computing and Applications 35, 29 (2023), 21399‚Äì21414.\n[11] Mikkel M√∏ller Andersen and Pinar T√∂z√ºn. 2022. Micro-architectural analysis of a learned index. In Proceedings of the Fifth International Workshop\non Exploiting Artificial Intelligence Techniques for Data Management . 1‚Äì12.\n[12] Matej Antol, Jaroslav Ol‚Äôha, Ter√©zia Slanin√°kov√°, and Vlastislav Dohnal. 2021. Learned metric index‚Äîproposition of learned indexing for\nunstructured data. Information Systems 100 (2021), 101774.\n[13] Walid Aref, Daniel Barbar√°, and Padmavathi Vallabhaneni. 1995. The Handwritten Trie: Indexing Electronic Ink. SIGMOD Rec. 24, 2 (May 1995),\n151‚Äì162.\n[14] Walid G. Aref. 2009. Electronic Ink Indexing . Springer US, Boston, MA, 972‚Äì978. https://doi.org/10.1007/978-0-387-39940-9_143\n[15] Manos Athanassoulis, Stratos Idreos, Dennis Shasha, et al .2023. Data Structures for Data-Intensive Applications: Tradeoffs and Design Guidelines.\nFoundations and Trends ¬Æin Databases 13, 1-2 (2023), 1‚Äì168.\n[16] G Phanendra Babu. 1997. Self-organizing neural networks for spatial data. Pattern Recognition Letters 18, 2 (1997), 133‚Äì142.\n[17] Rudolf Bayer and Edward McCreight. 1970. Organization and maintenance of large ordered indices. In Proceedings of the 1970 ACM SIGFIDET (Now\nSIGMOD) Workshop on Data Description, Access and Control . 107‚Äì141.\n[18] Norbert Beckmann, Hans-Peter Kriegel, Ralf Schneider, and Bernhard Seeger. 1990. The R*-tree: an efficient and robust access method for points\nand rectangles. In Proceedings of the 1990 ACM SIGMOD international conference on Management of data . 322‚Äì331.\n[19] Andreas Behrend, Anton Dign√∂s, Johann Gamper, Philip Schmiegelt, Hannes Voigt, Matthias Rottmann, and Karsten Kahl. 2019. Period Index: A\nLearned 2D Hash Index for Range and Duration Queries. In Proceedings of the 16th International Symposium on Spatial and Temporal Databases .\n100‚Äì109.\n[20] Michael A Bender and Haodong Hu. 2007. An adaptive packed-memory array. ACM Transactions on Database Systems (TODS) 32, 4 (2007), 26‚Äìes.\n[21] Jon Louis Bentley. 1975. Multidimensional Binary Search Trees Used for Associative Searching. Commun. ACM 18, 9 (1975), 509‚Äì517.\n[22] Jon Louis Bentley and Jerome H Friedman. 1979. Data structures for range searching. ACM Computing Surveys (CSUR) 11, 4 (1979), 397‚Äì409.\n[23] Arindam Bhattacharya, Srikanta Bedathur, and Amitabha Bagchi. 2020. Adaptive learned bloom filters under incremental workloads. In Proceedings\nof the 7th ACM IKDD CoDS and 25th COMAD . 107‚Äì115.\n[24] Laurent Bindschaedler, Andreas Kipf, Tim Kraska, Ryan Marcus, and Umar Farooq Minhas. 2021. Towards a Benchmark for Learned Systems. In\n2021 IEEE 37th International Conference on Data Engineering Workshops (ICDEW) . IEEE, 127‚Äì133.\n[25] Burton H Bloom. 1970. Space/time trade-offs in hash coding with allowable errors. Commun. ACM 13, 7 (1970), 422‚Äì426.\n[26] L√©on Bottou. 1998. Online algorithms and stochastic approximations. Online learning in neural networks (1998).\n[27] Leo Breiman. 2001. Random forests. Machine learning 45 (2001), 5‚Äì32.\n[28] Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez,\nSpyridon Samothrakis, and Simon Colton. 2012. A survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and\nAI in games 4, 1 (2012), 1‚Äì43.\n[29] Chengliang Chai, Jiayi Wang, Yuyu Luo, Zeping Niu, and Guoliang Li. 2022. Data management for machine learning: A survey. IEEE Transactions\non Knowledge and Data Engineering 35, 5 (2022), 4646‚Äì4667.\n[30] Fay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C Hsieh, Deborah A Wallach, Mike Burrows, Tushar Chandra, Andrew Fikes, and Robert E\nGruber. 2008. Bigtable: A distributed storage system for structured data. ACM Transactions on Computer Systems (TOCS) 26, 2 (2008), 1‚Äì26.\n\nA Survey of Learned Indexes for the Multi-dimensional Space 31\n[31] Daoyuan Chen, Wuchao Li, Yaliang Li, Bolin Ding, Kai Zeng, Defu Lian, and Jingren Zhou. 2022. Learned Index with Dynamic ùúñ. InThe Eleventh\nInternational Conference on Learning Representations .\n[32] Lianhua Chi and Xingquan Zhu. 2017. Hashing techniques: A survey and taxonomy. ACM Computing Surveys (Csur) 50, 1 (2017), 1‚Äì36.\n[33] Kyunghyun Cho, Bart Van Merri√´nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078 (2014).\n[34] Dalsu Choi, Hyunsik Yoon, Hyubjin Lee, and Yon Dohn Chung. 2022. Waffle: in-memory grid index for moving objects with reinforcement\nlearning-based configuration tuning system. Proceedings of the VLDB Endowment 15, 11 (2022), 2375‚Äì2388.\n[35] Douglas Comer. 1979. Ubiquitous B-tree. ACM Computing Surveys (CSUR) 11, 2 (1979), 121‚Äì137.\n[36] Andrew Crotty. 2021. Hist-Tree: Those Who Ignore It Are Doomed to Learn.. In CIDR .\n[37] Lixiao Cui, Kedi Yang, Yusen Li, Gang Wang, and Xiaoguang Liu. 2023. DiffLex: A High-Performance, Memory-Efficient and NUMA-Aware\nLearned Index using Differentiated Management. In Proceedings of the 52nd International Conference on Parallel Processing . 62‚Äì71.\n[38] Yifan Dai, Yien Xu, Aishwarya Ganesan, Ramnatthan Alagappan, Brian Kroth, Andrea Arpaci-Dusseau, and Remzi Arpaci-Dusseau. 2020. From\n{WiscKey}to Bourbon: A Learned Index for {Log-Structured}Merge Trees. In 14th USENIX Symposium on Operating Systems Design and\nImplementation (OSDI 20) . 155‚Äì171.\n[39] Zhenwei Dai and Anshumali Shrivastava. 2019. Adaptive learned Bloom filter (Ada-BF): Efficient utilization of the classifier. arXiv preprint\narXiv:1910.09131 (2019).\n[40] Angjela Davitkova, Damjan Gjurovski, and Sebastian Michel. 2021. Compressing (Multidimensional) Learned Bloom Filters. In Workshop on\nDatabases and AI .\n[41] Angjela Davitkova, Evica Milchevski, and Sebastian Michel. 2020. The ML-Index: A Multidimensional, Learned Index for Point, Range, and\nNearest-Neighbor Queries.. In EDBT . 407‚Äì410.\n[42] Jialin Ding, Ryan Marcus, Andreas Kipf, Vikram Nathan, Aniruddha Nrusimha, Kapil Vaidya, Alexander van Renen, and Tim Kraska. 2022. SageDB:\nAn Instance-Optimized Data Analytics System. Proceedings of the VLDB Endowment 15, 13 (2022), 4062‚Äì4078.\n[43] Jialin Ding, Umar Farooq Minhas, Badrish Chandramouli, Chi Wang, Yinan Li, Ying Li, Donald Kossmann, Johannes Gehrke, and Tim Kraska. 2021.\nInstance-Optimized Data Layouts for Cloud Analytics Workloads. In SIGMOD Conference . ACM, 418‚Äì431.\n[44] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li, Hantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald\nKossmann, et al .2020. ALEX: an updatable adaptive learned index. In Proceedings of the 2020 ACM SIGMOD International Conference on Management\nof Data . 969‚Äì984.\n[45] Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020. Tsunami: A Learned Multi-dimensional Index for Correlated Data and\nSkewed Workloads. Proc. VLDB Endow. 14, 2 (2020), 74‚Äì86.\n[46] Xiaofeng Ding, Yinting Zheng, Zuan Wang, Kim-Kwang Raymond Choo, and Hai Jin. 2023. A learned spatial textual index for efficient keyword\nqueries. Journal of Intelligent Information Systems 60, 3 (2023), 803‚Äì827.\n[47] Yuquan Ding, Xujian Zhao, and Peiquan Jin. 2022. An Error-Bounded Space-Efficient Hybrid Learned Index with High Lookup Performance. In\nInternational Conference on Database and Expert Systems Applications . Springer, 216‚Äì228.\n[48] Haowen Dong, Chengliang Chai, Yuyu Luo, Jiabin Liu, Jianhua Feng, and Chaoqun Zhan. 2022. RW-Tree: A Learned Workload-aware Framework\nfor R-tree Construction. In 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2073‚Äì2085.\n[49] Karima Echihabi, Kostas Zoumpatianos, and Themis Palpanas. 2021. New trends in high-d vector similarity search: al-driven, progressive, and\ndistributed. Proceedings of the VLDB Endowment 14, 12 (2021), 3198‚Äì3201.\n[50] Martin Eppert, Philipp Fent, and Thomas Neumann. 2021. A Tailored Regression for Learned Indexes: Logarithmic Error Regression. In Fourth\nWorkshop in Exploiting AI Techniques for Data Management . 9‚Äì15.\n[51] Li Fan, Pei Cao, Jussara Almeida, and Andrei Z Broder. 2000. Summary cache: a scalable wide-area web cache sharing protocol. IEEE/ACM\ntransactions on networking 8, 3 (2000), 281‚Äì293.\n[52] Hakan Ferhatosmanoglu, Ertem Tuncel, Divyakant Agrawal, and Amr El Abbadi. 2000. Vector Approximation based Indexing for Non-uniform\nHigh Dimensional Data Sets. In CIKM . ACM, 202‚Äì209.\n[53] Paolo Ferragina, Marco Frasca, Giosu√® Cataldo Marin√≤, and Giorgio Vinciguerra. 2023. On nonlinear learned string indexing. IEEE Access (2023).\n[54] Paolo Ferragina, Fabrizio Lillo, and Giorgio Vinciguerra. 2020. Why are learned indexes so effective?. In International Conference on Machine\nLearning . PMLR, 3123‚Äì3132.\n[55] Paolo Ferragina, Fabrizio Lillo, and Giorgio Vinciguerra. 2021. On the performance of learned data structures. Theoretical Computer Science 871\n(2021), 107‚Äì120.\n[56] Paolo Ferragina and Giorgio Vinciguerra. 2020. Learned data structures. In Recent Trends in Learning From Data . Springer, 5‚Äì41.\n[57] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic compressed learned index with provable worst-case bounds.\nProceedings of the VLDB Endowment 13, 8 (2020), 1162‚Äì1175.\n[58] Edward Fredkin. 1960. Trie memory. Commun. ACM 3, 9 (1960), 490‚Äì499.\n[59] Volker Gaede and Oliver G√ºnther. 1998. Multidimensional access methods. ACM Computing Surveys (CSUR) 30, 2 (1998), 170‚Äì231.\n[60] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim Kraska. 2019. FITing-Tree: A Data-aware Index Structure. In\nSIGMOD Conference . ACM, 1189‚Äì1206.\n\n32 Abdullah-Al-Mamun, et al.\n[61] Jian Gao, Xin Cao, Xin Yao, Gong Zhang, and Wei Wang. 2023. LMSFC: A Novel Multidimensional Index Based on Learned Monotonic Space\nFilling Curves. Proc. VLDB Endow. 16, 10 (aug 2023), 2605‚Äì2617.\n[62] Jiake Ge, Boyu Shi, Yanfeng Chai, Yuanhui Luo, Yunda Guo, Yinxuan He, and Yunpeng Chai. 2023. Cutting Learned Index into Pieces: An In-depth\nInquiry into Updatable Learned Indexes. In 2023 IEEE 39th International Conference on Data Engineering (ICDE) . IEEE, 315‚Äì327.\n[63] Jiake Ge, Huanchen Zhang, Boyu Shi, Yuanhui Luo, Yunda Guo, Yunpeng Chai, Yuxing Chen, and Anqun Pan. 2023. SALI: A Scalable Adaptive\nLearned Index Framework based on Probability Models. arXiv preprint arXiv:2308.15012 (2023).\n[64] Saheli Ghosh, Tin Vu, Mehrad Amin Eskandari, and Ahmed Eldawy. 2019. UCR-STAR: The UCR spatio-temporal active repository. SIGSPATIAL\nSpecial 11, 2 (2019), 34‚Äì40.\n[65] Goetz Graefe. 2006. B-tree indexes, interpolation search, and skew. In Proceedings of the 2nd international workshop on Data management on new\nhardware . 5‚Äìes.\n[66] Ivo Grondman, Lucian Busoniu, Gabriel AD Lopes, and Robert Babuska. 2012. A survey of actor-critic reinforcement learning: Standard and\nnatural policy gradients. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 42, 6 (2012), 1291‚Äì1307.\n[67] Tu Gu, Kaiyu Feng, Gao Cong, Cheng Long, Zheng Wang, and Sheng Wang. 2023. The RLR-Tree: A Reinforcement Learning Based R-Tree for\nSpatial Data. Proceedings of the ACM on Management of Data 1, 1 (2023), 1‚Äì26.\n[68] Na Guo, Yaqi Wang, Haonan Jiang, Xiufeng Xia, and Yu Gu. 2022. TALI: An Update-Distribution-Aware Learned Index for Social Media Data.\nMathematics 10, 23 (2022), 4507.\n[69] Antonin Guttman. 1984. R-trees: A dynamic index structure for spatial searching. In Proceedings of the 1984 ACM SIGMOD international conference\non Management of data . 47‚Äì57.\n[70] Ali Hadian, Behzad Ghaffari, Taiyi Wang, and Thomas Heinis. 2023. COAX: Correlation-Aware Indexing. In 2023 IEEE 39th International Conference\non Data Engineering Workshops (ICDEW) . IEEE, 55‚Äì59.\n[71] Ali Hadian and Thomas Heinis. 2019. Considerations for handling updates in learned index structures. In Proceedings of the Second International\nWorkshop on Exploiting Artificial Intelligence Techniques for Data Management . ACM, 3.\n[72] Ali Hadian and Thomas Heinis. 2019. Interpolation-friendly B-trees: Bridging the Gap Between Algorithmic and LearnedIndexes. (2019).\n[73] Ali Hadian and Thomas Heinis. 2020. Madex: Learning-augmented algorithmic index structures. In Proceedings of the 2nd International Workshop\non Applied AI for Database Systems and Applications .\n[74] Ali Hadian and Thomas Heinis. 2021. Shift-Table: A low-latency learned index for range queries using model correction. arXiv preprint\narXiv:2101.10457 (2021).\n[75] Ali Hadian, Ankit Kumar, and Thomas Heinis. 2020. Hands-off Model Integration in Spatial Index Structures. In Proceedings of the 2nd International\nWorkshop on Applied AI for Database Systems and Applications .\n[76] Francisco Herrera, Francisco Charte, Antonio J Rivera, and Mar√≠a J Del Jesus. 2016. Multilabel classification. In Multilabel Classification . Springer,\n17‚Äì31.\n[77] David Hilbert. 1891. Ueber die stetige Abbildung einer Linie auf ein Fl√§chenst√ºck. Math. Ann. 38 (1891), 459‚Äì460.\n[78] Shuai Huang, Yong Wang, and Guoliang Li. 2023. ACR-Tree: Constructing R-Trees Using Deep Reinforcement Learning. In International Conference\non Database Systems for Advanced Applications . Springer, 80‚Äì96.\n[79] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. 2011. Sequential model-based optimization for general algorithm configuration. In\nLearning and Intelligent Optimization: 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers 5 . Springer, 507‚Äì523.\n[80] Stratos Idreos and Tim Kraska. 2019. From auto-tuning one size fits all to self-designed and learned data-intensive systems. In Proceedings of the\n2019 International Conference on Management of Data . 2054‚Äì2059.\n[81] Ihab F Ilyas, Volker Markl, Peter Haas, Paul Brown, and Ashraf Aboulnaga. 2004. CORDS: Automatic discovery of correlations and soft functional\ndependencies. In Proceedings of the 2004 ACM SIGMOD international conference on Management of data . 647‚Äì658.\n[82] Hosagrahar V Jagadish, Beng Chin Ooi, Kian-Lee Tan, Cui Yu, and Rui Zhang. 2005. iDistance: An adaptive B+-tree based indexing method for\nnearest neighbor search. ACM Transactions on Database Systems (TODS) 30, 2 (2005), 364‚Äì397.\n[83] Anil K Jain, M Narasimha Murty, and Patrick J Flynn. 1999. Data clustering: a review. ACM computing surveys (CSUR) 31, 3 (1999), 264‚Äì323.\n[84] Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. 2023. Linear regression. In An Introduction to Statistical\nLearning: With Applications in Python . Springer, 69‚Äì134.\n[85] Matthias Jasny, Tobias Ziegler, Tim Kraska, Uwe Roehm, and Carsten Binnig. 2020. DB4ML-an in-memory database kernel with machine learning\nsupport. In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data . 159‚Äì173.\n[86] Hui Jin and HV Jagadish. 2002. Indexing Hidden Markov Models for Music Retrieval.. In ISMIR .\n[87] Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. 1996. Reinforcement learning: A survey. Journal of artificial intelligence research\n4 (1996), 237‚Äì285.\n[88] Rong Kang, Wentao Wu, Chen Wang, Ce Zhang, and Jianmin Wang. 2021. The Case for ML-Enhanced High-Dimensional Indexes. In Proceedings\nof the 3rd International Workshop on Applied AI for Database Systems and Applications .\n[89] Andreas Kipf, Dominik Horn, Pascal Pfeil, Ryan Marcus, and Tim Kraska. 2022. Lsi: A learned secondary index structure. In Proceedings of the Fifth\nInternational Workshop on Exploiting Artificial Intelligence Techniques for Data Management . 1‚Äì5.\n[90] Andreas Kipf, Harald Lang, VN Pandey, Raul Alexandru Persa, Christoph Anneser, Eleni Tzirita Zacharatou, Harish Doraiswamy, Peter A Boncz,\nThomas Neumann, and Alfons Kemper. 2020. Adaptive main-memory indexing for high-performance point-polygon joins. (2020).\n\nA Survey of Learned Indexes for the Multi-dimensional Space 33\n[91] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper, Tim Kraska, and Thomas Neumann. 2019. SOSD: A Benchmark\nfor Learned Indexes. ArXiv abs/1911.13014 (2019).\n[92] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper, Tim Kraska, and Thomas Neumann. 2020. RadixSpline: a\nsingle-pass learned index. In Proceedings of the Third International Workshop on Exploiting Artificial Intelligence Techniques for Data Management .\n1‚Äì5.\n[93] Teuvo Kohonen. 1990. The self-organizing map. Proc. IEEE 78, 9 (1990), 1464‚Äì1480.\n[94] Evgenios M Kornaropoulos, Silei Ren, and Roberto Tamassia. 2022. The price of tailoring the index to your data: Poisoning attacks on learned\nindex structures. In Proceedings of the 2022 International Conference on Management of Data . 1331‚Äì1344.\n[95] Tim Kraska. 2021. Towards instance-optimized data systems. Proceedings of the VLDB Endowment 14, 12 (2021).\n[96] Tim Kraska, Mohammad Alizadeh, Alex Beutel, Ed H Chi, Jialin Ding, Ani Kristo, Guillaume Leclerc, Samuel Madden, Hongzi Mao, and Vikram\nNathan. 2019. Sagedb: A learned database system. (2019).\n[97] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018. The case for learned index structures. In Proceedings of the 2018\nInternational Conference on Management of Data . ACM, 489‚Äì504.\n[98] Tim Kraska, Umar Farooq Minhas, Thomas Neumann, Olga Papaemmanouil, Jignesh M Patel, Christopher R√©, and Michael Stonebraker. 2021.\nML-In-Databases: Assessment and Prognosis. IEEE Data Eng. Bull. 44, 1 (2021), 3‚Äì10.\n[99] Hai Lan, Zhifeng Bao, J Shane Culpepper, and Renata Borovica-Gajic. 2023. Updatable Learned Indexes Meet Disk-Resident DBMS-From Evaluations\nto Design Choices. Proceedings of the ACM on Management of Data 1, 2 (2023), 1‚Äì22.\n[100] Ken CK Lee, Baihua Zheng, Huajing Li, and Wang-Chien Lee. 2007. Approaching the skyline in Z order.. In VLDB , Vol. 7. 279‚Äì290.\n[101] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. The adaptive radix tree: ARTful indexing for main-memory databases. In 2013 IEEE 29th\nInternational Conference on Data Engineering (ICDE) . IEEE, 38‚Äì49.\n[102] Scott T Leutenegger, Mario A Lopez, and Jeffrey Edgington. 1997. STR: A simple and efficient algorithm for R-tree packing. In Proceedings 13th\ninternational conference on data engineering . IEEE, 497‚Äì506.\n[103] Guoliang Li and Xuanhe Zhou. 2022. Machine learning for data management: A system view. In 2022 IEEE 38th International Conference on Data\nEngineering (ICDE) . IEEE, 3198‚Äì3201.\n[104] Guoliang Li, Xuanhe Zhou, and Lei Cao. 2021. AI meets database: AI4DB and DB4AI. In Proceedings of the 2021 International Conference on\nManagement of Data . 2859‚Äì2866.\n[105] Guoliang Li, Xuanhe Zhou, and Lei Cao. 2021. Machine learning for databases. In Proceedings of the First International Conference on AI-ML Systems .\n1‚Äì2.\n[106] Jiangneng Li, Zheng Wang, Gao Cong, Cheng Long, Han Mao Kiah, and Bin Cui. 2023. Towards Designing and Learning Piecewise Space-Filling\nCurves. Proceedings of the VLDB Endowment 16, 9 (2023), 2158‚Äì2171.\n[107] Mingxin Li, Hancheng Wang, Haipeng Dai, Meng Li, Rong Gu, Feng Chen, Zhiyuan Chen, Shuaituan Li, Qizhi Liu, and Guihai Chen. 2024. A\nSurvey of Multi-Dimensional Indexes: Past and Future Trends. IEEE Transactions on Knowledge and Data Engineering (2024).\n[108] Pengfei Li, Yu Hua, Jingnan Jia, and Pengfei Zuo. 2021. FINEdex: a fine-grained learned index scheme for scalable and concurrent memory systems.\nProceedings of the VLDB Endowment 15, 2 (2021), 321‚Äì334.\n[109] Pengfei Li, Yu Hua, Pengfei Zuo, and Jingnan Jia. 2019. A Scalable Learned Index Scheme in Storage Systems. CoRR abs/1905.06256 (2019).\narXiv:1905.06256\n[110] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. 2020. LISA: A Learned Index Structure for Spatial Data. SIGMOD (2020).\n[111] Pengfei Li, Hua Lu, Rong Zhu, Bolin Ding, Long Yang, and Gang Pan. 2023. DILI: A Distribution-Driven Learned Index. 16, 9 (2023), 2212‚Äì2224.\n[112] Xupeng Li, Bin Cui, Yiru Chen, Wentao Wu, and Ce Zhang. 2017. Mlog: Towards declarative in-database machine learning. Proceedings of the\nVLDB Endowment 10, 12 (2017), 1933‚Äì1936.\n[113] Xin Li, Jingdong Li, and Xiaoling Wang. 2019. ASLM: Adaptive single layer model for learned index. In International Conference on Database\nSystems for Advanced Applications . Springer, 80‚Äì95.\n[114] Zhe Li, Tsz Nam Chan, Man Lung Yiu, and Christian S Jensen. 2021. PolyFit: Polynomial-based indexing approach for fast approximate range\naggregate queries. In Advances in Database Technology-24th International Conference on Extending Database Technology, EDBT 2021 . OpenProceedings.\norg, 241‚Äì252.\n[115] Yuming Lin, Zhengguo Huang, and You Li. 2023. Learning hash index based on a shallow autoencoder. Applied Intelligence 53, 12 (2023),\n14999‚Äì15010.\n[116] Guanli Liu, Jianzhong Qi, Christian S. Jensen, James Bailey, and Lars Kulik. 2023. Efficiently Learning Spatial Indices. In 2023 IEEE 39th International\nConference on Data Engineering (ICDE) . 1572‚Äì1584.\n[117] Guanli Liu, Jianzhong Qi, Lars Kulik, Kazuya Soga, Renata Borovica-Gajic, and Benjamin IP Rubinstein. 2023. Efficient Index Learning via Model\nReuse and Fine-tuning. In 2023 IEEE 39th International Conference on Data Engineering Workshops (ICDEW) . IEEE, 60‚Äì66.\n[118] Jiajun Liu, Haoran Li, Yong Gao, Hao Yu, and Dan Jiang. 2014. A geohash-based index for spatial data management in distributed memory. In 2014\n22Nd international conference on geoinformatics . IEEE, 1‚Äì4.\n[119] Li Liu, Chunhua Li, Zhou Zhang, Yuhan Liu, Ke Zhou, and Ji Zhang. 2022. A Data-aware Learned Index Scheme for Efficient Writes. In Proceedings\nof the 51st International Conference on Parallel Processing . 1‚Äì11.\n\n34 Abdullah-Al-Mamun, et al.\n[120] Qiyu Liu, Yanyan Shen, and Lei Chen. 2022. HAP: an efficient hamming space index based on augmented pigeonhole principle. In Proceedings of\nthe 2022 International Conference on Management of Data . 917‚Äì930.\n[121] Qiyu Liu, Libin Zheng, Yanyan Shen, and Lei Chen. 2020. Stable learned bloom filters for data streams. Proceedings of the VLDB Endowment 13, 12\n(2020), 2355‚Äì2367.\n[122] Yu Liu, Hua Wang, Ke Zhou, ChunHua Li, and Rengeng Wu. 2022. A survey on AI for storage. CCF Transactions on High Performance Computing 4,\n3 (2022), 233‚Äì264.\n[123] A Llavesh, Utku Sirin, R West, and A Ailamaki. 2019. Accelerating b+ tree search by using simple machine learning techniques. In Proceedings of\nthe 1st International Workshop on Applied AI for Database Systems and Applications .\n[124] Baotong Lu, Jialin Ding, Eric Lo, Umar Farooq Minhas, and Tianzheng Wang. 2021. APEX: a high-performance learned index on persistent memory.\nProceedings of the VLDB Endowment 15, 3 (2021), 597‚Äì610.\n[125] Kai Lu, Nannan Zhao, Jiguang Wan, Changhong Fei, Wei Zhao, and Tongliang Deng. 2021. TridentKV: A Read-Optimized LSM-Tree Based KV\nStore via Adaptive Indexing and Space-Efficient Partitioning. IEEE Transactions on Parallel and Distributed Systems 33, 8 (2021), 1953‚Äì1966.\n[126] Lanyue Lu, Thanumalayan Sankaranarayana Pillai, Hariharan Gopalakrishnan, Andrea C Arpaci-Dusseau, and Remzi H Arpaci-Dusseau. 2017.\nWisckey: Separating keys from values in ssd-conscious storage. ACM Transactions on Storage (TOS) 13, 1 (2017), 1‚Äì28.\n[127] Chen Luo and Michael J Carey. 2020. LSM-based storage techniques: a survey. The VLDB Journal 29, 1 (2020), 393‚Äì418.\n[128] Chaohong Ma, Xiaohui Yu, Yifan Li, Xiaofeng Meng, and Aishan Maoliniyazi. 2022. Film: A fully learned index for larger-than-memory databases.\nProceedings of the VLDB Endowment 16, 3 (2022), 561‚Äì573.\n[129] Stephen Macke, Alex Beutel, Tim Kraska, Maheswaran Sathiamoorthy, Derek Zhiyuan Cheng, and Ed H Chi. 2018. Lifting the curse of multidimen-\nsional data with learned existence indexes. In Workshop on ML for Systems at NeurIPS . 1‚Äì6.\n[130] Marcel Maltry and Jens Dittrich. 2022. A critical analysis of recursive model indexes. Proceedings of the VLDB Endowment 15, 5 (2022), 1079‚Äì1091.\n[131] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra, Alfons Kemper, Thomas Neumann, and Tim Kraska. 2020.\nBenchmarking learned indexes. Proceedings of the VLDB Endowment 14, 1 (2020), 1‚Äì13.\n[132] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh, Tim Kraska, Olga Papaemmanouil, and Nesime Tatbul. 2019. Neo:\na learned query optimizer. Proceedings of the VLDB Endowment 12, 11 (2019), 1705‚Äì1718.\n[133] Ryan Marcus, Emily Zhang, and Tim Kraska. 2020. CDFShop: Exploring and Optimizing Learned Index Structures. SIGMOD (2020).\n[134] Mayank Mishra and Rekha Singhal. 2021. RUSLI: real-time updatable spline learned index. In Fourth Workshop in Exploiting AI Techniques for Data\nManagement . 1‚Äì8.\n[135] Lubos Mitas and Helena Mitasova. 1999. Spatial interpolation. Geographical information systems: principles, techniques, management and applications\n1, 2 (1999).\n[136] Michael Mitzenmacher. 2018. A Model for Learned Bloom Filters, and Optimizing by Sandwiching. In Proceedings of the 32nd International\nConference on Neural Information Processing Systems (Montr√©al, Canada) (NIPS‚Äô18) . Curran Associates Inc., Red Hook, NY, USA, 462‚Äì471.\n[137] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K\nFidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement learning. nature 518, 7540 (2015), 529‚Äì533.\n[138] Mohamed F Mokbel, Walid G Aref, and Ibrahim Kamel. 2003. Analysis of multi-dimensional space-filling curves. GeoInformatica 7 (2003), 179‚Äì209.\n[139] Robert Morris. 1968. Scatter storage techniques. Commun. ACM 11, 1 (1968), 38‚Äì44.\n[140] Guy M Morton. 1966. A computer oriented geodetic data base and a new technique in file sequencing. (1966).\n[141] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learning multi-dimensional indexes. In Proceedings of the 2020 ACM\nSIGMOD International Conference on Management of Data . 985‚Äì1000.\n[142] J√ºrg Nievergelt, Hans Hinterberger, and Kenneth C Sevcik. 1984. The grid file: An adaptable, symmetric multikey file structure. ACM Transactions\non Database Systems (TODS) 9, 1 (1984), 38‚Äì71.\n[143] Harrie Oosterhuis, J Shane Culpepper, and Maarten de Rijke. 2018. The potential of learned index structures for index compression. In Proceedings\nof the 23rd Australasian Document Computing Symposium . 1‚Äì4.\n[144] Jack A Orenstein and Tim H Merrett. 1984. A class of data structures for associative searching. In Proceedings of the 3rd ACM SIGACT-SIGMOD\nSymposium on Principles of Database Systems . 181‚Äì190.\n[145] Sachith Pai, Michael Mathioudakis, and Yanhao Wang. 2023. Workload-aware and Learned Z-Indexes. arXiv preprint arXiv:2310.04268 (2023).\n[146] Sachith Gopalakrishna Pai, Michael Mathioudakis, and Yanhao Wang. 2022. Towards an Instance-Optimal Z-Index. AIDB@VLDB (2022).\n[147] Varun Pandey, Alexander van Renen, Andreas Kipf, Ibrahim Sabek, Jialin Ding, and Alfons Kemper. 2020. The Case for Learned Spatial Indexes.\narXiv preprint arXiv:2008.10349 (2020).\n[148] Varun Pandey, Alexander van Renen, Eleni Tzirita Zacharatou, Andreas Kipf, Ibrahim Sabek, Jialin Ding, Volker Markl, and Alfons Kemper. 2023.\nEnhancing In-Memory Spatial Indexing with Learned Search. arXiv preprint arXiv:2309.06354 (2023).\n[149] Andrew Pavlo, Gustavo Angulo, Joy Arulraj, Haibin Lin, Jiexi Lin, Lin Ma, Prashanth Menon, Todd C Mowry, Matthew Perron, Ian Quah, et al .\n2017. Self-Driving Database Management Systems.. In CIDR , Vol. 4. 1.\n[150] Peano. 1890. Sur une courbe, qui remplit toute une aire plane. Math. Ann. 36 (1890), 157‚Äì160.\n[151] William Pugh. 1990. Skip lists: a probabilistic alternative to balanced trees. Commun. ACM 33, 6 (1990), 668‚Äì676.\n[152] Martin L Puterman. 1990. Markov decision processes. Handbooks in operations research and management science 2 (1990), 331‚Äì434.\n\nA Survey of Learned Indexes for the Multi-dimensional Space 35\n[153] Jianzhong Qi, Guanli Liu, Christian S Jensen, and Lars Kulik. 2020. Effectively learning spatial indices. Proceedings of the VLDB Endowment 13, 12\n(2020), 2341‚Äì2354.\n[154] Jianzhong Qi, Yufei Tao, Yanchuan Chang, and Rui Zhang. 2018. Theoretically optimal and empirically efficient r-trees with strong parallelizability.\nProceedings of the VLDB Endowment 11, 5 (2018), 621‚Äì634.\n[155] Wenwen Qu, Xiaoling Wang, Jingdong Li, and Xin Li. 2019. Hybrid indexes by exploring traditional B-tree and linear regression. In International\nConference on Web Information Systems and Applications . Springer, 601‚Äì613.\n[156] J. Ross Quinlan. 1986. Induction of decision trees. Machine learning 1, 1 (1986), 81‚Äì106.\n[157] Lawrence Rabiner and Biinghwang Juang. 1986. An introduction to hidden Markov models. ieee assp magazine 3, 1 (1986), 4‚Äì16.\n[158] Jack Rae, Sergey Bartunov, and Timothy Lillicrap. 2019. Meta-learning neural bloom filters. In International Conference on Machine Learning . PMLR,\n5271‚Äì5280.\n[159] Raghu Ramakrishnan, Johannes Gehrke, and Johannes Gehrke. 2003. Database management systems . Vol. 3. McGraw-Hill New York.\n[160] Ibrahim Sabek and Mohamed F Mokbel. 2019. Machine learning meets big spatial data. Proceedings of the VLDB Endowment 12, 12 (2019),\n1982‚Äì1985.\n[161] Ibrahim Sabek and Mohamed F Mokbel. 2020. Machine learning meets big spatial data. In 2020 IEEE 36th International Conference on Data\nEngineering (ICDE) . IEEE, 1782‚Äì1785.\n[162] Ibrahim Sabek and Mohamed F Mokbel. 2021. Machine learning meets big spatial data (revised). In 2021 22nd IEEE International Conference on\nMobile Data Management (MDM) . IEEE, 5‚Äì8.\n[163] Ibrahim Sabek, Kapil Vaidya, Dominik Horn, Andreas Kipf, and Tim Kraska. 2021. When Are Learned Models Better Than Hash Functions? arXiv\npreprint arXiv:2107.01464 (2021).\n[164] Ibrahim Sabek, Kapil Vaidya, Dominik Horn, Andreas Kipf, Michael Mitzenmacher, and Tim Kraska. 2022. Can Learned Models Replace Hash\nFunctions? Proceedings of the VLDB Endowment 16, 3 (2022), 532‚Äì545.\n[165] Hans Sagan. 2012. Space-filling curves . Springer Science & Business Media.\n[166] Hanan Samet. 1984. The quadtree and related hierarchical data structures. ACM Computing Surveys (CSUR) 16, 2 (1984), 187‚Äì260.\n[167] Hanan Samet. 2006. Foundations of multidimensional and metric data structures . Morgan Kaufmann.\n[168] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. 2016. Meta-learning with memory-augmented neural\nnetworks. In International conference on machine learning . PMLR, 1842‚Äì1850.\n[169] Atsuki Sato and Yusuke Matsui. 2023. Fast Partitioned Learned Bloom Filter. arXiv preprint arXiv:2306.02846 (2023).\n[170] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint\narXiv:1707.06347 (2017).\n[171] Thomas Seidl, Ira Assent, Philipp Kranen, Ralph Krieger, and Jennifer Herrmann. 2009. Indexing density models for incremental learning and\nanytime classification on data streams. In Proceedings of the 12th international conference on extending database technology: advances in database\ntechnology . 311‚Äì322.\n[172] Raghav Sethi, Martin Traverso, Dain Sundstrom, David Phillips, Wenlei Xie, Yutian Sun, Nezih Yegitbasi, Haozhun Jin, Eric Hwang, Nileema\nShingte, et al. 2019. Presto: SQL on everything. In 2019 IEEE 35th International Conference on Data Engineering (ICDE) . IEEE, 1802‚Äì1813.\n[173] Naufal Fikri Setiawan, Benjamin IP Rubinstein, and Renata Borovica-Gajic. 2020. Function interpolation for learned index structures. In Databases\nTheory and Applications: 31st Australasian Database Conference, ADC 2020, Melbourne, VIC, Australia, February 3‚Äì7, 2020, Proceedings 31 . Springer,\n68‚Äì80.\n[174] Yufan Sheng, Xin Cao, Yixiang Fang, Kaiqi Zhao, Jianzhong Qi, Gao Cong, and Wenjie Zhang. 2023. WISK: A Workload-aware Learned Index for\nSpatial Keyword Queries. Proceedings of the ACM on Management of Data 1, 2 (2023), 1‚Äì27.\n[175] Jin Shieh and Eamonn J. Keogh. 2009. iSAX: disk-aware mining and indexing of massive time series datasets. Data Min. Knowl. Discov. 19, 1 (2009),\n24‚Äì57.\n[176] Ter√©zia Slanin√°kov√°, Matej Antol, Jaroslav O ÀáIha, Vojtƒõch Ka≈àa, and Vlastislav Dohnal. 2021. Data-driven learned metric index: an unsupervised\napproach. In International Conference on Similarity Search and Applications . Springer, 81‚Äì94.\n[177] Benjamin Spector, Andreas Kipf, Kapil Vaidya, Chi Wang, Umar Farooq Minhas, and Tim Kraska. 2021. Bounding the last mile: Efficient learned\nstring indexing. arXiv preprint arXiv:2111.14905 (2021).\n[178] Mihail Stoian, Andreas Kipf, Ryan Marcus, and Tim Kraska. 2021. PLEX: towards practical learned indexing. In 3rd International Workshop on\nApplied AI for Database Systems and Applications .\n[179] Michael Stonebraker, Lawrence A Rowe, and Michael Hirohama. 1990. The implementation of POSTGRES. IEEE transactions on knowledge and\ndata engineering 2, 1 (1990), 125‚Äì142.\n[180] Xinyu Su, Jianzhong Qi, and Egemen Tanin. 2023. A Fast Hybrid Spatial Index with External Memory Support. In 2023 IEEE 39th International\nConference on Data Engineering Workshops (ICDEW) . IEEE, 67‚Äì73.\n[181] Zhaoyan Sun, Xuanhe Zhou, and Guoliang Li. 2023. Learned Index: A Comprehensive Experimental Evaluation. Proceedings of the VLDB Endowment\n16, 8 (2023), 1992‚Äì2004.\n[182] Chuzhe Tang, Zhiyuan Dong, Minjie Wang, Zhaoguo Wang, and Haibo Chen. 2019. Learned Indexes for Dynamic Workloads. arXiv preprint\narXiv:1902.00655 (2019).\n\n36 Abdullah-Al-Mamun, et al.\n[183] Chuzhe Tang, Youyun Wang, Zhiyuan Dong, Gansen Hu, Zhaoguo Wang, Minjie Wang, and Haibo Chen. 2020. XIndex: a scalable learned index\nfor multicore data storage. In Proceedings of the 25th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming . 308‚Äì320.\n[184] Yao Tian, Tingyun Yan, Xi Zhao, Kai Huang, and Xiaofang Zhou. 2022. A learned index for exact similarity search in metric spaces. IEEE\nTransactions on Knowledge and Data Engineering (2022).\n[185] Yulai Tong, Jiazhen Liu, Hua Wang, Ke Zhou, Rongfeng He, Qin Zhang, and Cheng Wang. 2023. Sieve: A Learned Data-Skipping Index for Data\nAnalytics. Proceedings of the VLDB Endowment 16, 11 (2023), 3214‚Äì3226.\n[186] Kapil Vaidya, Subarna Chatterjee, Eric Knorr, Michael Mitzenmacher, Stratos Idreos, and Tim Kraska. 2022. SNARF: a learning-enhanced range\nfilter. Proceedings of the VLDB Endowment 15, 8 (2022), 1632‚Äì1644.\n[187] Kapil Vaidya, Eric Knorr, Michael Mitzenmacher, and Tim Kraska. 2020. Partitioned Learned Bloom Filters. In International Conference on Learning\nRepresentations .\n[188] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising\nautoencoders. In Proceedings of the 25th international conference on Machine learning . 1096‚Äì1103.\n[189] Congying Wang and Jia Yu. 2022. GLIN: A Lightweight Learned Indexing Mechanism for Complex Geometries. arXiv preprint arXiv:2207.07745\n(2022).\n[190] Haixin Wang, Xiaoyi Fu, Jianliang Xu, and Hua Lu. 2019. Learned Index for Spatial Queries. In 2019 20th IEEE International Conference on Mobile\nData Management (MDM) . IEEE, 569‚Äì574.\n[191] Lijun Wang, Linshu Hu, Chenhua Fu, Yuhan Yu, Peng Tang, Feng Zhang, and Renyi Liu. 2023. SLBRIN: A Spatial Learned Index Based on BRIN.\nISPRS International Journal of Geo-Information 12, 4 (2023), 171.\n[192] Ning Wang and Jianqiu Xu. 2021. Spatial queries based on learned index. In Spatial Data and Intelligence: First International Conference, SpatialDI\n2020, Virtual Event, May 8‚Äì9, 2020, Proceedings 1 . Springer, 245‚Äì257.\n[193] Wei Wang, Meihui Zhang, Gang Chen, HV Jagadish, Beng Chin Ooi, and Kian-Lee Tan. 2016. Database meets deep learning: Challenges and\nopportunities. ACM Sigmod Record 45, 2 (2016), 17‚Äì22.\n[194] Youyun Wang, Chuzhe Tang, Zhaoguo Wang, and Haibo Chen. 2020. SIndex: a scalable learned index for string keys. In Proceedings of the 11th\nACM SIGOPS Asia-Pacific Workshop on Systems . 17‚Äì24.\n[195] Yang Wang, Peng Wang, Jian Pei, Wei Wang, and Sheng Huang. 2013. A Data-adaptive and Dynamic Segmentation Index for Whole Matching on\nTime Series. Proc. VLDB Endow. 6, 10 (2013), 793‚Äì804.\n[196] Zhaoguo Wang, Haibo Chen, Youyun Wang, Chuzhe Tang, and Huan Wang. 2022. The concurrent learned indexes for multicore data storage.\nACM Transactions on Storage (TOS) 18, 1 (2022), 1‚Äì35.\n[197] Zhonghua Wang, Chen Ding, Fengguang Song, Kai Lu, Jiguang Wan, Zhihu Tan, Changsheng Xie, and Guokuan Li. 2023. WIPE: a Write-Optimized\nLearned Index for Persistent Memory. ACM Transactions on Architecture and Code Optimization (2023).\n[198] Hongwei Wen and Hanyuan Hang. 2022. Random forest density estimation. In International Conference on Machine Learning . PMLR, 23701‚Äì23722.\n[199] Chaichon Wongkham, Baotong Lu, Chris Liu, Zhicong Zhong, Eric Lo, and Tianzheng Wang. 2022. Are updatable learned indexes ready?\nProceedings of the VLDB Endowment 15, 11 (2022), 3004‚Äì3017.\n[200] Jiacheng Wu, Yong Zhang, Shimin Chen, Jin Wang, Yu Chen, and Chunxiao Xing. 2021. Updatable Learned Index with Precise Positions. Proc.\nVLDB Endow. 14, 8 (apr 2021), 1276‚Äì1288.\n[201] Shangyu Wu, Yufei Cui, Jinghuan Yu, Xuan Sun, Tei-Wei Kuo, and Chun Jason Xue. 2022. NFL: robust learned index via distribution transformation.\nProceedings of the VLDB Endowment 15, 10 (2022), 2188‚Äì2200.\n[202] Yingjun Wu, Jia Yu, Yuanyuan Tian, Richard Sidle, and Ronald Barber. 2019. Designing Succinct Secondary Indexing Mechanism by Exploiting\nColumn Correlations. arXiv preprint arXiv:1903.11203 (2019).\n[203] Wenkun Xiang, Hao Zhang, Rui Cui, Xing Chu, Keqin Li, and Wei Zhou. 2018. Pavo: A RNN-Based Learned Inverted Index, Supervised or\nUnsupervised? IEEE Access 7 (2018), 293‚Äì303.\n[204] Qing Xie, Chaoyi Pang, Xiaofang Zhou, Xiangliang Zhang, and Ke Deng. 2014. Maximum error-bounded piecewise linear representation for\nonline stream approximation. The VLDB journal 23 (2014), 915‚Äì937.\n[205] Guang Yang, Liang Liang, Ali Hadian, and Thomas Heinis. 2023. FLIRT: A Fast Learned Index for Rolling Time frames. (2023).\n[206] Jingyi Yang and Gao Cong. 2023. PLATON: Top-down R-tree Packing with Learned Partition Policy. Proceedings of the ACM on Management of\nData 1, 4 (2023), 1‚Äì26.\n[207] Zongheng Yang, Badrish Chandramouli, Chi Wang, Johannes Gehrke, Yinan Li, Umar Farooq Minhas, Per-√Öke Larson, Donald Kossmann, and\nRajeev Acharya. 2020. Qd-tree: Learning Data Layouts for Big Data Analytics. In SIGMOD Conference . ACM, 193‚Äì208.\n[208] Ahmad Yasin. 2014. A top-down method for performance analysis and counters architecture. In 2014 IEEE International Symposium on Performance\nAnalysis of Systems and Software (ISPASS) . IEEE, 35‚Äì44.\n[209] Peng Yongxin, Zhou Wei, Zhang Lin, and Du Hongle. 2020. A Study of Learned KD Tree Based on Learned Index. In 2020 International Conference\non Networking and Network Applications (NaNA) . IEEE, 355‚Äì360.\n[210] Jia Yu and Mohamed Sarwat. 2017. Indexing the pickup and drop-off locations of NYC taxi trips in PostgreSQL‚Äìlessons from the road. In Advances\nin Spatial and Temporal Databases: 15th International Symposium, SSTD 2017, Arlington, VA, USA, August 21‚Äì23, 2017, Proceedings 15 . Springer,\n145‚Äì162.\n\nA Survey of Learned Indexes for the Multi-dimensional Space 37\n[211] Tong Yu, Guanfeng Liu, An Liu, Zhixu Li, and Lei Zhao. 2023. LIFOSS: a learned index scheme for streaming scenarios. World Wide Web 26, 1\n(2023), 501‚Äì518.\n[212] Eleni Tzirita Zacharatou, Andreas Kipf, Ibrahim Sabek, Varun Pandey, Harish Doraiswamy, and Volker Markl. 2021. The Case for Distance-Bounded\nSpatial Approximations. In CIDR . www.cidrdb.org.\n[213] Sepanta Zeighami and Cyrus Shahabi. 2023. On Distribution Dependent Sub-Logarithmic Query Time of Learned Indexing. arXiv preprint\narXiv:2306.10651 (2023).\n[214] Meng Zeng, Beiji Zou, Xiaoyan Kui, Chengzhang Zhu, Ling Xiao, Zhi Chen, Jingyu Du, et al .2023. PA-LBF: Prefix-Based and Adaptive Learned\nBloom Filter for Spatial Data. International Journal of Intelligent Systems 2023 (2023).\n[215] Meng Zeng, Beiji Zou, Wensheng Zhang, Xuebing Yang, Guilan Kong, Xiaoyan Kui, and Chengzhang Zhu. 2023. Two-layer partitioned and\ndeletable deep bloom filter for large-scale membership query. Information Systems 119 (2023), 102267.\n[216] Jiaoyi Zhang and Yihan Gao. 2022. CARMI: a cache-aware learned index with a cost-based construction algorithm. Proceedings of the VLDB\nEndowment 15, 11 (2022), 2679‚Äì2691.\n[217] Jingtian Zhang, Sai Wu, Zeyuan Tan, Gang Chen, Zhushi Cheng, Wei Cao, Yusong Gao, and Xiaojie Feng. 2019. S3: A scalable in-memory skip-list\nindex for key-value store. Proceedings of the VLDB Endowment 12, 12 (2019), 2183‚Äì2194.\n[218] Songnian Zhang, Suprio Ray, Rongxing Lu, and Yandong Zheng. 2021. SPRIG: A Learned Spatial Index for Range and kNN Queries. In SSTD . ACM,\n96‚Äì105.\n[219] Songnian Zhang, Suprio Ray, Rongxing Lu, and Yandong Zheng. 2022. Efficient Learned Spatial Index With Interpolation Function Based Learned\nModel. IEEE Transactions on Big Data (2022).\n[220] Teng Zhang, Jian Tan, Xin Cai, Jianying Wang, Feifei Li, and Jianling Sun. 2022. SA-LSM: optimize data layout for LSM-tree based storage using\nsurvival analysis. Proceedings of the VLDB Endowment 15, 10 (2022), 2161‚Äì2174.\n[221] Yong Zhang, Xinran Xiong, and Oana Balmau. 2022. TONE: cutting tail-latency in learned indexes. In Proceedings of the Workshop on Challenges\nand Opportunities of Efficient and Performant Storage Systems . 16‚Äì23.\n[222] Zhou Zhang, Zhaole Chu, Peiquan Jin, Yongping Luo, Xike Xie, Shouhong Wan, Yun Luo, Xufei Wu, Peng Zou, Chunyang Zheng, et al .2022. Plin:\na persistent learned index for non-volatile memory with high performance and instant recovery. Proceedings of the VLDB Endowment 16, 2 (2022),\n243‚Äì255.\n[223] Zhou Zhang, Pei-Quan Jin, Xiao-Liang Wang, Yan-Qi Lv, Shou-Hong Wan, and Xi-Ke Xie. 2021. COLIN: a cache-conscious dynamic learned index\nwith high read/write performance. Journal of Computer Science and Technology 36 (2021), 721‚Äì740.\n[224] Xun Zhong, Yong Zhang, Yu Chen, Chao Li, and Chunxiao Xing. 2022. Learned Index on GPU. In 2022 IEEE 38th International Conference on Data\nEngineering Workshops (ICDEW) . IEEE, 117‚Äì122.\n[225] Xuanhe Zhou, Chengliang Chai, Guoliang Li, and Ji Sun. 2020. Database meets artificial intelligence: A survey. IEEE Transactions on Knowledge\nand Data Engineering 34, 3 (2020), 1096‚Äì1116.\n[226] Justin Zobel and Alistair Moffat. 2006. Inverted files for text search engines. ACM computing surveys (CSUR) 38, 2 (2006), 6‚Äìes.\n[227] Beiji Zou, Meng Zeng, Chengzhang Zhu, Ling Xiao, and Zhi Chen. 2022. A learned prefix bloom filter for spatial data. In International Conference\non Database and Expert Systems Applications . Springer, 336‚Äì350.",
  "textLength": 155538
}