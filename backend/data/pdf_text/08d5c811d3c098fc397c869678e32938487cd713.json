{
  "paperId": "08d5c811d3c098fc397c869678e32938487cd713",
  "title": "Online Primal-Dual Algorithms with Predictions for Packing Problems",
  "pdfPath": "08d5c811d3c098fc397c869678e32938487cd713.pdf",
  "text": "arXiv:2110.00391v1  [cs.DS]  1 Oct 2021Online Primal-Dual Algorithms with Predictions for Packin g\nProblems\nNguyễn Kim Thắng\nUniversity Paris-Saclay, IBISC, France\nChristoph D¨ urr\nSorbonne University, CNRS, LIP6, France\nOctober 4, 2021\nAbstract\nThe domain of online algorithms with predictions has been ex tensively studied for\ndifferent applications such as scheduling, caching (pagin g), clustering, ski rental, etc. Re-\ncently, Bamas et al., aiming for an unified method, have prov ided a primal-dual framework\nfor linear covering problems. They extended the online prim al-dual method by incorpo-\nrating predictions in order to achieve a performance beyond the worst-case case analysis.\nIn this paper, we consider this research line and present a fr amework to design algorithms\nwith predictions for non-linear packing problems. We illus trate the applicability of our\nframework in submodular maximization and in particular ad- auction maximization in\nwhich the optimal bound is given and supporting experiments are provided.\n1 Introduction\nOnline computation [ 7] is a well-established field in theoretical computer scien ce. In online\ncomputation, inputs are released one-by-one in form of a reque st sequence. Requests arrive\nover time and each request must be answered by the algorithm wi th an irrevocable action,\nwithout any information about future requests to come. These actions define a solution to\nthe problem at hand, and the algorithm wants to optimize its o bjective value. Usually this\nvalue is normalized by the optimum, which could have been com puted if all the requests were\nknown in beforehand. The resulting competitive ratio measures the price of not knowing the\nfuture requests.\nSeveral models have been studied, which relax some of the ass umptions in the standard\nonline computation model, see [ 31,32] for a recent survey.\nOne of the models is motivated by spectacular advances in mac hine learning (ML). In\nparticular, the capability of ML methods in predicting patt erns of future requests would\nprovide useful information to be exploited by online algorith ms. A general framework for\nincorporating ML predictions into algorithm design in orde r to achieve a better performance\nthan the worst-case one is formally introduced by [ 23]. This is rapidly followed by work\nstudying online algorithms with predictions [ 28] in a large spectrum of problems such as\nscheduling [ 21,27], caching (paging) [ 23,29,2], ski rental [ 15,20,1], etc. In contrast to\nonline computation with advice (see [ 17] for a survey), the aforementioned models handle\npredictions in a careful manner, guaranteeing good perform ance even if the predictions is\n1\n\ncompletely wrong. More precisely, on the one hand, if the pre dictions are accurate, one expects\nto achieve a good performance as in the offline setting where the input are completely given\nin advance. On the other hand, if the predictions are mislead ing, one still has to maintain a\ncompetitive solution as in the online setting where no predi ctive information is available.\nThis issue has been addressed (for example the aforementione d work) by subtly incor-\nporating predictions and exploiting specific problems’ str uctures. The primal-dual method\nis an elegant and powerful technique for the design of algorit hms [ 36], especially for on-\nline algorithms [ 8]. In the objective of unifying previous ad-hoc approaches, [ 5] presented a\nprimal-dual approach to design online algorithms with pred ictions for covering problems with\na linear objective function. In this paper, we build on previo us work and provide a primal-\ndual method with predictions for packing problems with non-l inear objectives; following a\nresearch direction and answering some open questions mentio ned in [ 5].\n1.1 Problem and Model\nPacking problem In the packing problem studied in this paper we are given of a se quence\nofnelementsE, andmunit capacity resources, numbered from 1tom. Each element e∈E\nhas a profit weand a resource requirement bi,e≥0for each resource i. The goal is to select a\nset of elements of maximum total profit, such that no resource capacity is exceeded. Formally\nthe goal is to chose a vector x∈{0,1}|E|maximizing/summationtext\ne∈Ewexesuch that for every resource\ni∈{1,...,m}we have/summationtext\ne∈Ebi,exe≤1.\nFractional variant In the fractional variant of this problem, the integrality c onstraint on x\nis removed, and the goal is to optimize over vectors x∈[0,1]|E|. ByIwe denote the instance\nof a packing problem, which consists of /a\\}b∇acketle{tE,(we)e∈E,m,(bi,e)1≤i≤m,e∈E/a\\}b∇acket∇i}ht.\nNon-linear variant We consider a generalization of this problem, where the obje ctive is\nnot to maximize the sum/summationtext\ne∈Ewexe, butf(x)for a given function f:{0,1}|E|→R+. It is\nassumed that fis monotone, in the sense that f(x)≤f(y), whenever yequals xexcept for\na single element ewithxe= 0andye= 1. Describing fexplicitly needs space O(2|E|), hence\nit is assumed that fis given as a black-box oracle. For the fractional variant, th e multilinear\nextension F: [0,1]|E|→R+is used instead of f, which is defined as\nF(x) =/summationdisplay\nS⊆E/productdisplay\ne∈Sxe/productdisplay\ne/\\e}atio\\slash∈S(1−xe)f(1S),\nwhere1Sis the characteristic vector of the set S. Alternatively, F(x) =E/bracketleftbig\nf(1T)/bracketrightbig\nwhereTis\na random set such that every element eappears independently in Twith probability xe. Note\nthatF(1S) =f(1S)for every S⊆E—Fis truly an extension of f— and if fis monotone\nthen so is F, in the sense that F(x)≤F(y)for all vectors x,y∈[0,1]|E|such that xe≤ye\nfor all elements e.\nOnline packing problem. In the paper, we consider the model presented in [ 5] (with\nseveral definitions rooted in [ 23,20]). In the online packing problem, elements e∈Earrive\nonline. When element e∈Earrives, the algorithm learns its resource requirements bi,e. At\nthe arrival of eonlyxecan be set by the algorithm, and cannot be changed later on. On lym,\nthe number of resources, are initially known to the algorithm . The algorithm does not know\n2\n\nEinitially, not even its cardinality. Formally an online alg orithmAreceives the elements of\nEin arbitrary order, and upon reception of element e∈Eneeds to choose a value for xe.\nAt any moment, the vector xmust satisfy all mresource capacity constraints of the packing\nproblem.\nWe denote byA(I)the objective value of the solution produced by Aon an instance\nI. A prediction is an integral value xpred\ne∈{0,1}, which is given to the algorithm with\nevery element e. Confidence in the prediction is described by a parameter η∈(0,1]. For\nconvenience we choose larger ηvalue to represent smaller confidence. Similarly to A(I), we\ndenote byP(I)the objective value of the vector xpredproduced by the prediction, and set\nP(I) = 0 whenever xpreddoes not satisfy all constraints. Finally we denote by O(I)the\noptimal fractional solution to the packing problem, which co uld have been computed if the\nwhole instance Iwere available from the beginning.\nAlgorithmAisc(η)-consistent andr(η)-robust if for every instance I,\nA(I)≥max{c(η)·P(I),r(η)·O(I)}.\nIdeally we would like that c(η)tends to 1, whenηapproaches 0, meaning that with high\nconfidence, the algorithm performs at least as good as the pr ediction. Additionally we would\nlike that r(η)tends to the same competitive ratio as in the standard online setting (without\nprovided predictions), when ηapproaches 1.\nObjective function At any moment in time, the algorithm maintains a vector xover the\narrived elements. Conceptually this vector is padded with z eros to create a vector belonging\nto[0,1]|E|, which for convenience we also denote by x.\n1.2 Approach and Contribution\nWe use the primal-dual method in our approach. At a high level , it formulates a given problem\nas a (primal) linear program. There is a corresponding dual l inear program, which plays an\nimportant role in the method. By weak duality, a solution to t he dual bounds the optimal\nsolution to the primal. Every arrival of an element in the onl ine problem, translates in the\narrival of a variable in the primal linear program, and a corr esponding constraint in the dual\nlinear program. The algorithm updates the fractional solut ions to both the primal and dual in\norder to maintain their feasibility. Then, the competitive ratio of the algorithm is established\nby showing that every time an update of primal and dual soluti ons is made, the increase of\nthe primal can be bounded by that of the dual up to some desired factor.\nNote that, similar to [ 5], we focus on constructing fractional solution which is the main\nstep in the primal-dual method. In order to derive algorithm s for specific problems, online\nrounding schemes are needed and in many problems, such schem es already exist. We will\nprovide references for the rounding schemes tailor designed for different applications, but do\nnot develop them in this paper.\nWe follow the primal-dual framework presented in [ 34] to design competitive algorithms\nfor fractional non-linear packing problems. Given a functio nf:{0,1}n→R+, itsmultilinear\nextension F: [0,1]n→R+is defined as F(x) :=/summationtext\nS⊆E/producttext\ne∈Sxe/producttext\ne/∈S(1−xe)·f(1S)where\n1Sis the characteristic vector of S(i.e., the eth-component of 1Sequals1ife∈Sand equals\n0 otherwise).\nThe performance of an algorithm will depend on the following measures on the objective\nfunction.\n3\n\nDefinition 1 ([ 34])A differentiable function F: [0,1]|E|→R+is(λ,µ)-locally-smooth for\nsome parameters λ,µ≥0if for any set S⊆E, and for every vector x∈[0,1]|E|, the following\ninequality holds:\n/a\\}b∇acketle{t∇F(x),1S/a\\}b∇acket∇i}ht=/summationdisplay\ne∈S∇eF(x)≥λF/parenleftbig\n1S/parenrightbig\n−µF/parenleftbig\nx/parenrightbig\n.\nwhere∇eF(x)denotes∂F(x)/∂xe.\nNote that the (λ,µ)-smoothness notion here differs from the usual notion of smo othness of\nfunctions in convex optimization. The former, introduced i n [34], is related to the definition of\nsmooth games [ 30] in the context of algorithmic game theory. The parameters (λ,µ)somehow\ndescribe how far the function is from being linear. For exampl e a non-decreasing linear\nfunction is (1,1)-locally-smooth. Another example is the multilinear extensio n of monotone\nsubmodular functions, which is (1,2)-locally-smooth [ 34].\nThe dual of a linear packing problem is a linear covering probl em and vice-versa. However\nthis breaks when considering non-linear objective function s. The usual trick is to linearize the\nmodel, by introducing auxiliary variables and work with what is called a configuration linear\nprogram. Now, how are we going to exploit the prediction? In ou r approach, we incorporate\nthe predictive information in the primal-dual approach by m odifying the coefficients of the\nconstraints. This in turn influences the increase of the pri mal variables and hence the solution\nproduced by the algorithm.\nThis approach guarantees that: (i) if the confidence on the p rediction is high ( ηis close\nto 0) then value of variables corresponding to the elements s elected by the prediction would\nget a large value; (ii) and inversely, if the confidence on th e prediction is low ( ηis close to 1)\nthen the variables would be constructed as in the standard on line primal-dual method. The\nconstruction follows the multiplicative weights update ba sed on the gradient of the multilinear\nextension [ 34], which generalizes the multiplicative update in [ 9,3]. Using the concept of local-\nsmoothness, we show the feasibility of our primal/dual solu tions (even when the prediction\nprovides an infeasible one) and also analyze the performance o f the algorithm, as a function\nof locally-smoothness and confidence parameters.\nTheorem 1 LetFbe the multilinear extension of the non-decreasing objecti ve function f.\nDenote the row sparsity d:= max i|{bie:bie>0}|and the divergence ρ:= max imaxe,e′:bie′>0bie/bie′.\nAssume that Fis(λ,µ)-locally-smooth for some parameters λ >0andµ. Then, for every\n0< η≤1, there exists a r(η)-consistent and r(η)·/parenleftbigλ\n2ln(1+dρ/η)+µ/parenrightbig\n-robust algorithm for the\nfractional packing problem where r(η) = min 0≤u≤1F(u\n1+η)/F(u).\nIn the following, we describe the applications of our framew ork to different classes of\nproblems.\n•Linear objectives. Numerous combinatorial problems can be formalized by linea r\nprograms with packing constraints to which our framework app lies. When fis a\nmonotone linear function, the smooth parameters of Fareλ= 1andµ= 1. In\nthis case, our algorithm guarantees the consistency C(η) =1\n1+ηand the robustness\nR(η) =Ω(1)\n2(1+η)ln(1+dρ/η). Note that in this case, the multilinear extension Fis rather\neasy to compute, as it satisfies F(x) =/summationtext\ne∈Exef(1{e}).\n•Submodular objectives. Submodular maximization constitutes a major research\nagenda and has been widely studied in optimization, machine learning [ 4,19,6] and\n4\n\nalgorithm design [ 13,10]. Using our framework, we derive an algorithm that yields\na(1−η)-consistent and Ω/parenleftbig1\n2ln(1+dρ/η)/parenrightbig\n-robust fractional solution for online submod-\nular maximization with packing constraints. This performanc e is similar to that of\nlinear functions up to constant factors. Note that using the online contention resolution\nrounding schemes [ 14], generalizing the offline counterpart [ 11], one can obtain random-\nized algorithms for several specific constraint polytopes s uch as knapsack polytopes,\nmatching polytopes and matroid polytopes.\nAd-auctions problem. An interesting particular packing problem is the ad-auction s rev-\nenue maximization. Informally, one needs to allocate ads in a n online manner to advertisers\nwith their budget constraints in order to maximize the total r evenue from allocated ads. Any\nalgorithm improving the ad allocation even by a small fractio n would have a non-negligible\nimpact. Building on the salient ideas of our framework, we giv e a(1−η)-consistent and\n(1−ηe−η)-robust algorithm for this problem.\n1.3 Related work\nThe primal-dual method is a powerful tool for online computa tion.\nA primal-dual framework for linear programs with packing/co vering constraints was given\nin [9]. Their method unifies several previous potential-functio n-based analyses and give a\nprincipled approach to design and analyze algorithms for pro blems with linear relaxations.\nA framework for covering/packing problems with convex/conca ve objectives whose gradients\nare monotone was provided by [ 3]. Subsequently, [ 34] presented algorithms dealing with\nnon-convex functions and established the competitive rati o as a function of the smoothness\nparameters of the objective function. The smoothness notio n introduced in [ 34] has rooted\nin smooth games defined by [ 30] in the context of algorithmic game theory.\nThe domain of algorithms with predictions [ 28], or learning augmented algorithms, has\nrecently emerged and rapidly grown at the intersection of (d iscrete) algorithm design and\nmachine learning (ML). The idea is to incorporate learning p redictions, together with ML\ntechniques into algorithm design, in order to achieve perfor mance guarantees beyond the\nworst-case analysis and provide specifically adapted soluti ons to different problems. Interest-\ning results have been shown over a large spectrum of problems such as scheduling [ 21,27],\ncaching (paging) [ 23,29,2], ski rental [ 15,20,1], counting sketches [ 16], bloom filters [ 18,26],\netc. Recently, Bamas et al. [ 5] have proposed a primal-dual approach to design online al-\ngorithms with predictions for linear problems with coverin g constraints. In this paper, by\ncombining their ideas and the ideas from [ 9,3,34], we present a primal-dual framework for\nmore general problems with non-linear objectives and packin g/covering constraints (moti-\nvated by a question mentioned in [ 5]).\nOnline matching and ad-auctions have been widely studied (s ee [25] and references therein).\nMotivated by Internet advertising applications, several w orks have considered the ad-auctions\nproblem in various settings where forecast/prediction is a vailable/learnable. Esfandiari et al.\n[12] considered a model in which the input is stochastic and a for ecast for the future items\nis given. The accuracy of the forecast is intuitively measur ed by the fraction of the value\nof an optimal solution that can be obtained from the stochast ic input. They provide algo-\nrithms with provable bounds in this setting. Schild et al. [ 33] introduced the semi-online\nmodel in which unknown future has a predicted part and an adver sarial part. They gave\nalgorithms with competitive ratios depending on the fracti on of the adversarial part (in the\n5\n\ninput). Closely related to our work is the model by Mahdian et al. [24] in which given two\nalgorithms, one needs to design a (new) algorithm which is ro bust to both given algorithms.\nThey derived an algorithm for ad-auctions problems that ach ieves a fraction of the maximum\nrevenue of the given algorithms. The main difference to ours model is that their algorithm\nisnotrobust in case a given algorithm provides infeasible solutio ns (which would happen for\nthe prediction) whereas our algorithm is.\n2 Primal-Dual Framework for Packing Problems\nFormulation. First, we model the packing problem as a configuration linear program. For\nthe integral variant of the packing problem, the decision var iablexe∈{0,1}indicates whether\nelementeis selected in the solution. A configuration is a set of eleme ntsS⊆Eand could be\nfeasible or not. In addition to xthe linear program contains variables zS∈{0,1}for every\nconfiguration S. The idea is that zS= 1solely for the set Scontaining all selected elements\ne, i.e. for which xe= 1. In this case Sis feasible by the constraints imposed on x.\nThe fractional variant of the packing problem is modeled with the same variables and\nconstraints, but without the integrality constraints. Now xespecifies the fraction with which\neis selected. The zvariables represent now a distribution on configurations Sand have to\nbe consistent with xin the following sense. When Sis selected with probability zS, thene\nbelongs to the selected set Swith probability xe. Unlike for the integral linear program, z\nis not unique for given x. In our algorithm we chose a particular vector z. Note that the\nsupport of zmight contain non-feasible configurations.\nWe consider the following linear program and its dual.\nbmax/summationdisplay\nSf(1S)zS\n/summationdisplay\nebi,e·xe≤1∀i(αi)\n/summationdisplay\nS:e∈SzS=xe∀e(βe)\n/summationdisplay\nSzS= 1 ( γ)\nxe,zS≥0∀e,Smin/summationdisplay\niαi+γ\n/summationdisplay\nibi,e·αi≥βe∀e(xe)\nγ+/summationdisplay\ne∈Sβe≥f(1S)∀S(zS)\nαi≥0∀i\nIn the primal, (αi)are the packing constraints of the given problem. Constraint s(βe)\nforce the aforementioned relation between xandz. Constraint (γ)ensures that zrepresents\na distribution. Note that the primal constraints (γ)and(βe), imply the box constraints\nxe≤1∀e.\nAlgorithm. Assume that function F(·)is(λ,µ)-locally smooth. Let dbe the maximal\nnumber of positive entries in a row, i.e., d= max i|{bie:bie>0}|. Letρthe maximum\ndivergence between positive row coefficients, i.e. ρ= max imaxe,e′:bie′>0bie/bie′. The algorithm\nis given a prediction xpred, i.e. with every arriving element e, it receives the values xpred\niefor\neach resource i. It uses this prediction to specify coefficients b, which are scaled from bin a\n6\n\nspecific manner. The maximum divergence ρis defined similarly as ρwithbreplacing b, i.e.,\nρ= max imaxe,e′:bie′>0bi,e/bie′.\nThe algorithm maintains a primal solution y∈[0,1]E, computed with the primal-dual\nmethod with respect to the coefficients b. Its decision for element eeither follows the predicted\nsolution xpred\ne, or it follows yein case infeasibility of the predicted solution xpredhas been\ndetected.\nThe value of bi,edepends on coefficient bi,eand the prediction xpred\ne. Specifically, bi,e=bi,e\nifxpred\ne= 1and the predictive solution is notfeasible; bi,e=1\nηbi,eotherwise. In both cases,\npacking constraints using bare stronger than they would be with coefficients b.\nIntuitively, if we do not trust the prediction at all, i.e. η= 1, thenbi,e=bi,eand therefore\nxe,yewould get a value proportional to the one returned by a primal -dual algorithm in the\nclassic setting. Inversely, if we trust the prediction (i.e .,ηis close to 0) and the prediction\nis feasible, then bi,e=bi,ewhenxpred\ne= 1andbi,e=bi,e/ηwhenxpred\ne= 0. Therefore, the\nmodified constraint/summationtext\ne′bi,e′ye′≤1will likely prevent ye, foresuch that xpred\ne= 0, from\ngetting a large value. Hence, yeforesuch that xpred\ne= 1could get a large value. In the\nend of each iteration, we set the output solution xeroughly by scaling a factor1\n1+ηtoye\norxpred\ne(depending on cases) in order to maintain the feasibility an d the consistency to the\nprediction.\nThe construction of yfollows the scheme in [ 34]. We recall the definition of the divergence\nfactorρ= max imaxe,e′:bie′>0bi,e/bie′. So in particular, ρ≤ρ/η. Recall that the gradient in\ndirection eis∇eF(y) =∂F(y)/∂ye. By convention, when an element eis not released,\n∇eF(y) = 0 . While∇eF(y)>0— i.e., increasing yeimproves the objective value —\nand/summationtext\nibi,eαe≤1\nλ∇eF(y), the primal variable yeand dual variables αi’s are increased by\nappropriate rates. We will argue in the analysis that the prim al and dual solutions returned\nby the algorithm are feasible.\nRecall that by definition of the multilinear extension,\n∇eF(y) =F((y−e,1))−F((y−e,0)) =ER/bracketleftbig\nf/parenleftbig\n1R∪{e}/parenrightbig\n−f/parenleftbig\n1R/parenrightbig/bracketrightbig\nwhere(y−e,1)denotes a vector which is identical to yon every coordinate different to eand\nthe value at coordinate eis 1. The vector (y−e,0)is defined similarly. The expectation is\ntaken over random subset R⊆E\\{e}such that e′is included with probability ye′. Therefore,\nduring the iteration of the while loop with respect to elemen te, onlyyeis modified and\nye′remains fixed for all e′/\\e}atio\\slash=e, as a consequence ∇eF(y)is constant during the iteration.\nMoreover, for every e,F(y)and∇eF(y)can be efficiently approximated up to any required\nprecision [ 35].\nOne important aspect of the primal-dual algorithm presente d below, is that it works only\nwith primal variables xand dual variables α, and uses the multilinear extension Finstead of\nf. However for the analysis, we will later show how to extend thes e solutions with variables z\nandβ,γboth to show feasibility of the constructed solution and to a nalyze consistency and\nrobustness of the algorithm.\nNote that once the prediction becomes infeasible, the algor ithm works with the given b\ncoefficients and outputs the solution computed by yescaled by 1/(1+η).\nPrimal variables. The vector xis completed by zto form a complete solution to the\nprimal linear program, by setting zS=/producttext\ne∈Sxe/producttext\ne/∈S(1−xe).\n7\n\nAlgorithm 1 Algorithm for Packing Problem.\n1:All primal and dual variables are initially set to 0.\n2:Lety∈[0,1]Ebe such that ye= 0∀e.\n3:At every step, always maintain zS=/producttext\ne∈Sxe/producttext\ne/∈S(1−xe).\n4:foreach arrival of a new element edo\n5:ifxpred\ne= 1orthe predictive solution xpredis infeasible then\n6: setbi,e=bi,e\n7:else\n8: setbi,e=bi,e/η\n9:end if\n10: while/summationtext\nibi,eαi≤1\nλ∇eF(y)and∇eF(y)>0do\n11: Some of the primal and dual variables are increased continuo usly as follows, where τ\nis the time during this process.\n12: Increase yeat a rate such thatdye\ndτ←1\n∇eF(y)·ln(1+dρ).\n13: end while\n14: ifxpred\ne= 1andthe predictive solution is still feasible then\n15: setxe←1\n1+ηxpred\ne=1\n1+η\n16: else\n17: setxe←1\n1+ηye\n18: end if\n19:end for\nDual variables. Variables αi’s are constructed in the algorithm. Define γ=µ\nλF(y)and\nβe=1\nλ·∇eF(y).\nThe following lemma provides a lower bound on the αvariables.\nLemma 1 At any moment during the iteration related to element e, for every constraint iit\nalways holds that\nαi≥∇eF(y)\nmaxe′bi,e′·dλ/bracketleftBigg\nexp/parenleftbigg\nln/parenleftbig\n1+dρ/parenrightbig\n·/summationdisplay\ne′bi,e′·ye′/parenrightbigg\n−1/bracketrightBigg\n. (1)\nProof Fix a constraint i. We prove the claimed inequality by induction. In the very beg inning\nof the algorithm, when no elements are released yet, the inequ ality holds since both sides are 0.\nConsider any moment τduring the loop corresponding to an arriving element e. Assume that\nthe inequality holds at the beginning of the iteration step. W e will show that the inequality\nstill holds after the step.\nAsFis a multilinear extension, by its very definition, Fis linear in ye. Hence∇eF(y)is\nindependent of ye. Moreover, during the loop corresponding to resource e, onlyyeis modified,\nleavingye′unchanged for all e′/\\e}atio\\slash=e. As as a result, d∇eF(y)/dτ= 0. Therefore, the rate at\ntimeτof the the right-hand-side of Inequality ( 1) is:\n8\n\n∇eF(y)\nmaxe′bi,e′·dλ·ln/parenleftbig\n1+dρ/parenrightbig\n·bi,e·dye\ndτ·exp/parenleftbigg\nln/parenleftbig\n1+dρ/parenrightbig\n·/summationdisplay\ne′bi,e′·ye′/parenrightbigg\n≤∇eF(y)\nmaxe′bi,e′·dλ·ln/parenleftbig\n1+dρ/parenrightbig\n·bi,e·1\n∇eF(y)·ln(1+dρ)·/parenleftbiggmaxe′bi,e′·dλ·αi\n∇eF(y)+1/parenrightbigg\n≤bi,e·αi\n∇eF(y)+1\ndλ=dαi\ndτ,\nwhere in the first inequality we use the induction hypothesis a nd the increasing rate of ye.\nSo at any time during the iteration related to e, the increasing rate of the left-hand side is\nalways larger than that of the right-hand side. Hence, the lem ma follows. /square\nLemma 2 The primal variables constructed by the algorithm are feasi ble.\nProof First observe that if during the execution of the algorithm in the iteration related to\nsome element e, we have/summationtext\ne′bi,e′ye′>1for some constraint ithen by Lemma 1,\nαi>∇eF(y)\nmaxe′bi,e′·dλ/bracketleftbigg\nexp/parenleftbigg\nln/parenleftbig\n1+dρ/parenrightbig/parenrightbigg\n−1/bracketrightbigg\n=ρ·∇eF(y)\nλmaxe′bi,e′≥∇eF(y)\nλbi,e\nTherefore,/summationtext\nibi,eαi>1\nλ∇eF(y)and hence the algorithm would have stopped increasing ye\nat some earlier point. Therefore, every constraint/summationtext\ne′bi,e′ye′≤1is always maintained during\nthe execution of the algorithm.\nSecondly, we show primal feasibility (even when the predict ion oracle provides an infeasible\nsolution). If the prediction oracle provides a feasible solu tion, we set S1={e:xpred\ne= 1}and\nS2=∅. Otherwise, let e∗be the the first element for which the prediction oracle provi des an\ninfeasible solution. Let S1be the set of all resources esuch that xpred\ne= 1andeis released\nbeforee∗. Further let S2={e:xpred\ne= 1}\\S1. For every constraint i,\n/summationdisplay\nebi,exe=/summationdisplay\ne:xpred\ne=1bi,exe+/summationdisplay\ne:xpred\ne=0bi,exe\n=/summationdisplay\ne∈S1bi,exe+/summationdisplay\ne∈S2bi,exe+/summationdisplay\ne:xpred\ne=0bi,exe\n≤1\n1+η·1+η·/summationdisplay\ne∈S2bi,exe+η·/summationdisplay\ne:xpred\ne=0bi,exe\n=1\n1+η+η\n1+η/summationdisplay\ne∈S2bi,eye+η\n1+η/summationdisplay\ne:xpred\ne=0bi,eye\n≤1\n1+η+η\n1+η/summationdisplay\nebi,eye\n≤1\n1+η+η\n1+η·1 = 1.\nThe first inequality is due to: (1) the feasibility of the pred iction restricted on S1, i.e.,/summationtext\ne∈S1bi,expred\ne≤1; and (2) xe=1\n1+η=1\n1+ηxpred\nefore∈S1; and (3) the definitions of bi,ein\n9\n\nAlgorithm 1. The third equality follows by the algorithm: xe=1\n1+η·yefore /∈S1. The last\ninequality holds since/summationtext\nebi,eye≤1by the observation made at the beginning of the lemma.\nHence,/summationtext\nebi,exe≤1for every constraint i.\nBesides, by definition of zS=/producttext\ne∈Sxe/producttext\ne/∈S(1−xe)where0≤xe≤1for alle, the\nidentity/summationtext\nSzS= 1always holds. In fact, if one chooses an element ewith probability xe\nthenzSis the probability that the set of selected elements is S. So the total probability/summationtext\nSzSmust be 1. Similarly,/summationtext\nS:e∈SzS=xe/summationtext\nS′⊂E\\{e}/producttext\ne′∈S′xe′/producttext\ne′/∈S′(1−xe′) =xesince/summationtext\nS′⊂E\\{e}/producttext\ne′∈S′xe′/producttext\ne′/∈S′(1−xe′) = 1 (by the same argument).\nTherefore, the solution (x,z)is primal feasible. /square\nLemma 3 The dual variables defined as above are feasible.\nProof The first dual constraint/summationtext\nibi,eαi≥βeis satisfied by the while loop condition of the\nalgorithm and the definition of βe. The second dual constraint γ+/summationtext\ne∈Sβe≥f(1S)reads\n1\nλ/summationdisplay\ne∈S∇eF(y)+µ\nλF(y)≥F(1S),\nwhich is, by arranging terms, exactly the (λ,µ)-max-local smoothness of F. (Recall that\nF(1S) =f(1S).) Hence, the lemma follows. /square\nTheorem 1 LetFbe the multilinear extension of the non-decreasing objecti ve function f.\nDenote the row sparsity d:= max i|{bie:bie>0}|and the divergence ρ:= max imaxe,e′:bie′>0bie/bie′.\nAssume that Fis(λ,µ)-locally-smooth for some parameters λ >0andµ. Then, for every\n0< η≤1, there exists a r(η)-consistent and r(η)·/parenleftbigλ\n2ln(1+dρ/η)+µ/parenrightbig\n-robust algorithm for the\nfractional packing problem where r(η) = min 0≤u≤1F(u\n1+η)/F(u).\nProof\nRobustness. First, we bound the increases of F(y)and of the dual objective value — which\nwe denote by D— at any time τin the execution of Algorithm 1. The derivative of F(y)\nwith respect to τis:\n∇eF(y)·dye\ndτ=∇eF(y)·1\n∇eF(y)·ln(1+dρ)=1\nln(1+dρ)\nBesides, the rate of the dual at time τis:\ndD\ndτ=/summationdisplay\nidαi\ndτ+dγ\ndτ=/summationdisplay\ni:bi,e>0/parenleftbiggbi,e·αi\n∇eF(y)+1\ndλ/parenrightbigg\n+µ\nλdF(y)\ndτ\n=/summationdisplay\ni:bi,e>0bi,e·αi\n∇eF(y)+/summationdisplay\ni:bi,e>01\ndλ+µ\nλ·1\nln(1+dρ)\n≤2\nλ+µ\nλ·ln(1+dρ)=2ln(1+dρ)+µ\nλ·ln(1+dρ),\nwhere the inequality holds since during the algorithm/summationtext\nibi,e·αi≤1\nλ∇eF(y). Hence, the\nratio between F(y)and the dual Dis at leastλ\n2ln(1+dρ)+µ.\n10\n\nBesides, xe=1\n1+η≥1\n1+ηyeifxpred\ne= 1and the predictive solution is still feasible; and\nxe=1\n1+ηyeotherwise. Therefore, x≥y\n1+ηand soF(x)≥F/parenleftbigy\n1+η/parenrightbig\nby monotonicity1ofF.\nHence the robustness is at least\nF(x)\nF(y)·λ\n2ln(1+dρ)+µ≥min\n0≤u≤1F(1\n1+ηu)\nF(u)·λ\n2ln(1+dρ/η)+µ\nwhere the latter is due to ρ≤ρ/η.\nConsistency. By our algorithm, for every element e, ifxpred\ne= 1(and the prediction\noracle provides a feasible solution) then xe=1\n1+η. Hence, the consistency of the algorithm\nF(x)/F(xpred)≥F(xpred\n1+η)/F(xpred)≥r(η). /square\n2.1 Applications\n2.1.1 Applications to linear functions\nWhen the objective fcan be expressed as a monotone linear functions, its multilin ear ex-\ntensionFis(1,1)-locally-smooth. Moreover, r(η) = 1/(1 +η). Consequently, Algorithm 1\nprovides a 1/(1+η)-consistent and O/parenleftbig\n1/ln(1+dρ/η)/parenrightbig\n-robust fractional solution for the online\nlinear packing problem.\n2.1.2 Applications to online submodular maximization\nConsider the online problem of maximizing a monotone submodu lar function subject to pack-\ning constraints. A set-function f: 2E→R+issubmodular iff(S∪e)−f(S)≥f(T∪e)−f(T)\nfor allS⊂T⊆E. LetFbe the multilinear extension of a monotone submodular functi onf.\nFunction Fadmits several useful properties: (i) if fis monotone then so is F; (ii)Fis concave\nin any positive direction, i.e., ∇F(x)≥∇F(y)for allx≤y(x≤ymeansxe≤ye∀e).\nLemma 4 Letfbe an arbitrary monotone submodular function. Then, its mul tilinear ex-\ntensionFis (1,1)-locally-smooth.\nProof AsFis the linear extension of a submodular function, ∇eF(x) =ER/bracketleftbig\nf/parenleftbig\n1R∪{e}/parenrightbig\n−\nf/parenleftbig\n1R/parenrightbig/bracketrightbig\nwhereRis a random subset of E\\{e}such that e′is included with probability xe′.\nFor any subset S={e1,...,eℓ}, we have\nF(x)+/summationdisplay\ne∈S∇eF(x) =F(x)+/summationdisplay\ne∈SER/bracketleftbig\nf/parenleftbig\n1R∪{e}/parenrightbig\n−f/parenleftbig\n1R/parenrightbig/bracketrightbig\n=ER/bracketleftbigg\nf(1R)+/summationdisplay\ne∈S/bracketleftbig\nf/parenleftbig\n1R∪{e}/parenrightbig\n−f/parenleftbig\n1R/parenrightbig/bracketrightbig/bracketrightbigg\n≥ER/bracketleftbigg\nf(1R)+ℓ/summationdisplay\ni=1/bracketleftbig\nf(1R∪{e1,...,ei})−f(1R∪{e1,...,ei−1})/bracketrightbig/bracketrightbigg\n=ER/bracketleftbig\nf(1R∪S)/bracketrightbig\n≥ER/bracketleftbig\nf(1S)/bracketrightbig\n=F/parenleftbig\n1S/parenrightbig\n1Note that this is the only step in the analysis we use the monot onicity of f, which implies the monotonicity\nofF.\n11\n\nthe first inequality is due to the submodularity fand the second one due to its monotonicity.\nThe lemma follows. /square\nProposition 1 For any 0< η≤1, Algorithm 1gives a(1−η)-consistent and O/parenleftbig\n1/ln(1+\ndρ/η)/parenrightbig\n-robust fractional solution to the problem of online submod ular maximization under\npacking constraints.\nProof We first bound r(η) = min 0≤u≤1F(1\n1+ηu)/F(u). By the non-negativity and the\nconcavity in positive direction of F, for any set S⊆E,\nF/parenleftbiggu\n1+η/parenrightbigg\n≥F(u)−/angbracketleftbigg\n∇F/parenleftbiggu\n1+η/parenrightbigg\n,u−u\n1+η/angbracketrightbigg\n(2)\nF/parenleftbiggu\n1+η/parenrightbigg\n≥F(0)+/angbracketleftbigg\n∇F/parenleftbiggu\n1+η/parenrightbigg\n,u\n1+η/angbracketrightbigg\n≥/angbracketleftbigg\n∇F/parenleftbiggu\n1+η/parenrightbigg\n,u\n1+η/angbracketrightbigg\n. (3)\nTherefore,\nF/parenleftbigg\nu\n1+η/parenrightbigg\nF(u)≥1−η\n1+η·/angbracketleftbigg\n∇F/parenleftbigg\nu\n1+η/parenrightbigg\n,u/angbracketrightbigg\nF(u)(by (2))\n≥1−η\n1+η·/angbracketleftbigg\n∇F/parenleftbigg\nu\n1+η/parenrightbigg\n,u/angbracketrightbigg\nF/parenleftbigg\nu\n1+η/parenrightbigg (by monotonicity of F)\n≥1−η\n1+η·/angbracketleftbigg\n∇F/parenleftbigg\nu\n1+η/parenrightbigg\n,u/angbracketrightbigg\n/angbracketleftbigg\n∇F/parenleftbigg\nu\n1+η/parenrightbigg\n,u\n1+η/angbracketrightbigg (by (3))\n= 1−η.\nSo,r(η)≥1−η. Therefore, the proposition holds by Theorem 1and by the (1,1)-locally-\nsmoothness of F(Lemma 4). /square\nOne can derive online randomized algorithms for the integra l variants of these problems\nby rounding the fractional solutions. For example, using the online contention resolution\nrounding schemes [ 14], one can obtain randomized algorithms for several specifi c constraint\npolytopes, for example, knapsack polytopes, matching polytopes and matroid polytopes.\n3 Ad-Auction Revenue Maximization\nProblem. In the problem, we are given mbuyers, each buyer 1≤i≤mhas a budget Bi.\nItems arrive online and at the arrival of item e, each buyer 1≤i≤nprovides a bid bi,e≪Bi\nfor buying e. In the integral variant of the problem, one needs to allocat e the item to at most\none of the buyers. For every buyer, the total bids of the alloc ated items should not exceed the\nbudget. Note that this forms a packing constraint, similar to the ones studied in the previous\nsection, except that we don’t normalize the constraints by th e factor1/Bi. In the fractional\nvariant of the problem, items can be allocated in fractions t o buyers, not exceeding a total\n12\n\nof1among the fractions. In both variants, the objective is to ma ximize the total revenue\nreceived from all buyers, which is the sum over all allocatio ns of corresponding bid, possibly\nmultiplied with the fraction of the allocation.\nIn out setting, every item earrives with a prediction, suggesting a buyer to whom to\nallocate that item, if any. For convenience we denote the ite ms by the integers from 1ton.\n3.1 The algorithm\nFormulation. The fractional variant of problem can be expressed as the foll owing linear\nprogram, where xi,eindicates the fraction at which item eis allocated to buyer i.\nmaxn/summationdisplay\ne=1m/summationdisplay\ni=1bi,exi,e\nm/summationdisplay\ni=1xi,e≤1∀e(βe)\nn/summationdisplay\ne=1bi,exi,e≤Bi∀i(αi)\nxi,e≥0∀i,eminn/summationdisplay\ni=1Biαi+m/summationdisplay\ne=1βe\nbi,eαi+βe≥bi,e∀i,e(xi,e)\nαi,βe≥0∀i,e\nIn the primal linear program, the first constraint ensures t hat an item can be allocated to at\nmost one buyer and the second constraint ensures that a buyer idoes not spend more than\nthe budget Bi. The dual of the relaxation is presented in the right.\nAlgorithm. For the sake of simplicity in the algorithm description, we i ntroduce a fictitious\nbuyer, denoted as 0, such that b0,e= 0for all items e. The non-assignment of an item eto\nany buyer can be seen as being assigned to the fictitious buye r0with revenue 0. The purpose\nof buyer 0is to simplify the description of the algorithm. We define th e constant C=\n(1+Rmax)η\nRmaxwhereRmax= max i,ebi,e\nBi. Adapting the primal-dual algorithm presented in\nthe previous section, we obtain the following algorithm for t he fractional ad-auction problem.\nLemma 5 For every i, it always holds that\nαi≥1\nC−1/parenleftbigg\nC/summationtext\ne∈M(i)bi,e\nηBi−1/parenrightbigg\n.\nProof We adapt the proof in [ 9] with a slight modification. The primal inequality αiis\nproved by induction on the number of released items. Initial ly, when no item is released, the\ninequality is trivially true. Assume that the inequality holds right before the arrival of an\niteme. The inequality remains unchanged for all but the buyer iselected in line 5 of the\nalgorithm, i.e. imaximizes bi,e(1−αi). We denote by αithe value before the update triggered\n13\n\nAlgorithm 2 Algorithm for Ad-Auctions Revenue Maximization.\n1:All primal and dual variables are initially set to 0.\n2:For the analysis, we maintain for every buyer itwo sets N(i),M(i), both initially empty.\n3:foreach arrival of a new item edo\n4:Leti∗be the buyer such that xpred\ni∗,e= 1. If either the prediction is not feasible or there\nis no such i∗(i.e.,xpred\ni′,e= 0∀i′) then set i∗= 0.\n5:Letibe the buyer that maximizes bi′,e(1−αi′). Ifbi,e(1−αi)≤0then set i= 0.\n6:Setβe←max/braceleftbig\n0,bi,e(1−αi)/bracerightbig\n# for the purpose of analysis\n7:ifbi,e< bi∗,ethen\n8: Setxi,e←ηandxi∗,e←1−η.\n9: Definebi,e=bi,e/η\n10:N(i∗)←N(i∗)∪{e}. # for the purpose of analysis\n11: else\n12: Setxi,e←1and define bi,e=bi,e # includes case xpredis infeasible\n13: end if\n14: UpdateM(i)←M(i)∪{e}. # for the purpose of analysis\n15: Updateαi←αi/parenleftBig\n1+bi,e\nBi/parenrightBig\n+bi,e\nBi·1\nC−1.\n16:end for\nby the arrival of eandα′\niits value after the update. We have\nα′\ni=αi·/parenleftbigg\n1+bi,e\nBi/parenrightbigg\n+bi,e\nBi·1\nC−1\n≥1\nC−1/parenleftbigg\nC/summationtext\ne′∈M(i)\\ebi,e\nηBi−1/parenrightbigg\n·/parenleftbigg\n1+bi,e\nBi/parenrightbigg\n+bi,e\nBi·1\nC−1\n=1\nC−1/parenleftbigg\nC/summationtext\ne′∈M(i)\\ebi,e\nηBi·/parenleftbigg\n1+bi,e\nBi/parenrightbigg\n−1/parenrightbigg\n≥1\nC−1/parenleftbigg\nC/summationtext\ne′∈M(i)\\ebi,e\nηBi·Cbi,e\nηBi−1/parenrightbigg\n=1\nC−1/parenleftbigg\nC/summationtext\ne∈M(i)bi,e\nηBi−1/parenrightbigg\n.\nThe first inequality is due to the induction hypothesis. The se cond inequality is due to this\nsequence of transformations. For any 0< y≤z≤1we have\nln(1+y)\ny≥ln(1+z)\nz\n⇔ln(1+y)≥ln(1+z)y/z\n⇔1+y≥(1+z)y/z,\nwhich we apply with y=bi,e/Biandz=Rmax. By the definition of Cwe obtain\n/parenleftbigg\n1+Rmax/parenrightbigg1\nRmax·bi,e\nBi=Cbi,e\nηBi.\n14\n\nThis completes the induction step. /square\nLemma 6 The primal solution is feasible up to a factor (1+Rmax).\nProof The first primal constraint/summationtext\nexi,e≤1follows the values of xi,eandxi∗,ein Algo-\nrithm 2. For the second primal constraint, we first prove that/summationtextm\ne=1bi,e≤Bifor every i. By\nLemma 5, for every i,\nαi≥1\nC−1/parenleftbigg\nC/summationtext\ne∈M(i)bi,e\nηBi−1/parenrightbigg\nSo whenever/summationtext\ne∈M(i)bi,e≥ηBi, we have αi≥1; so the algorithm stops allocating items to\nbuyeri. Hence, the buyer ican be allocated at most one additional item once her budget i s\nalready saturated. Hence,/summationtext\ne∈M(i)bi,e< ηBi+max ebi,e. Therefore,\nm/summationdisplay\ne=1bi,exi,e=/summationdisplay\ne∈M(i)bi,exi,e+/summationdisplay\ne∈N(i)bi,exi,e< Bi+max\nebi,e\nwhere the latter is due to the feasibility of N(i), the set of items assigned by the prediction\nto buyer i, i.e.,/summationtext\ne∈N(i)bi,exi,e≤(1−η)/summationtext\nebi,expred\ni,e≤(1−η)Bi. So,/summationtextm\ne=1bi,exi,e≤\nBi(1+Rmax). /square\nTheorem 2 The algorithm is (1−η)-consistent and1−1/C\n1+Rmax-robust. The robustness tends to\n1−e−ηwhenRmaxtends to 0.\nProof First, we establish robustness. At the arrival of item e, the increase in the primal is\n/braceleftBigg\n(1−η)·bi∗,e+η·bi,eifbi,e< bi∗,e,\nbi,e ifbi,e≥bi∗,e\nwhich is always larger than bi,e. Besides, the increase in the dual is\nBi∆αi+βe=bi,eαi+bi,e\nC−1+bi,e(1−αi) =/parenleftbigg\n1+1\nC−1/parenrightbigg\nbi,e=C\nC−1.\nHence, by Lemma 6, the robustness isC−1\nC·1\n1+Rmax.\nSecondly we establish consistency quite easily. At every tim e the prediction solution gets\na profitbi∗,e, the algorithm achieves a profit at least (1−η)bi∗,e. /square\n3.2 Experiments\nWe run an experimental evaluation of Algorithm 2. Source code is publicly available2. For\nthis purpose we constructed a random instance on 100 buyers a nd 10,000 items, adapting\nthe model described in [ 22]. Every item ereceives a bid from exactly 6 random buyers.\nThis means that in average every bidder makes 600 bids. The va lue of the bid follows a\nlognormal distribution with mean 1/2and deviation 1/2as well. By choosing the budget of\nthe bidders we can tune the hardness of the instance, under th e constraint that Rmaxremains\n2http://www.lip6.fr/christoph.durr/packing\n15\n\nreasonably small. We choose the budget as a 1/10fraction of the total bids, leading to a\nvalueRmax≈0.19.\nThe competitive ratio is determined using the fractional of fline solution. The integrality\ngap of our instance is very close to 1, roughly 1.0001in fact. For the prediction, we computed\nfirst the optimal integral solution, which is a partial mapp ing from items to buyers. This\nsolution was perturbed as suggested by [ 5]: Every item was mapped, with probability ǫ, to a\nuniformly chose random buyer, among the buyers who bid on the item.\nThe experiments are shown in Figure 1, and indicate clearly the benefit of the prediction\nwhen its perturbation is small. Note that the ratio is not clo se to1in that case for ηclose\nto zero, because the algorithm does not follow blindly the pr ediction. Infeasibility of the\nprediction was detected at 82% of the input sequence for ǫ= 0.01and at 64% for ǫ= 0.1.\nAs expected, with the performance degrades with the perturba tion of the prediction. Note\nthat the observed robustness is not monotone in η, unlike the bound shown in this paper.\nWe think that this performance degradation for ηaround1/2is due to the rather simplistic\nmixture between the primal-dual solution and the predicted s olution.\n0 0.2 0.4 0.6 0.8 10.70.80.9\nηrobustnessǫ= 0\nǫ= 0.01\nǫ= 0.1\nFigure 1: Experimental evaluation\n4 Conclusion\nIn the paper, we have presented primal-dual frameworks to des ign algorithms with predictions\nfor non-linear problems with packing constraints. Through a pplications, we show the poten-\ntial of our approach and provide useful ideas/guarantees in i ncorporating predictions into\nsolutions to problems with high impact such as submodular op timization and ad-auctions.\nAn interesting direction is to prove lower bounds for non-li near packing problems in terms of\nsmoothness and confidence parameters.\n16\n\nReferences\n[1] Spyros Angelopoulos, Christoph D¨ urr, Shendan Jin, Shah in Kamali, and Marc Renault.\nOnline Computation with Untrusted Advice. In 11th Innovations in Theoretical Com-\nputer Science Conference (ITCS 2020) , volume 151, pages 52:1–52:15, 2020.\n[2] Antonios Antoniadis, Christian Coester, Marek Elias, A dam Polak, and Bertrand Simon.\nOnline metric algorithms with untrusted predictions. In International Conference on\nMachine Learning , pages 345–355, 2020.\n[3] Yossi Azar, Niv Buchbinder, TH Hubert Chan, Shahar Chen, Ilan Reuven Cohen, Anu-\npam Gupta, Zhiyi Huang, Ning Kang, Viswanath Nagarajan, Jose ph Seffi Naor, and\nDebmalya Panigrahi. Online algorithms for covering and pac king problems with convex\nobjectives. In Proc. Symposium on Foundations of Computer Science (FOCS) , 2016.\n[4] Francis Bach et al. Learning with submodular functions: A convex optimization per-\nspective. Foundations and Trends ®in Machine Learning , 6(2-3):145–373, 2013.\n[5] Etienne Bamas, Andreas Maggiori, and Ola Svensson. The p rimal-dual method for learn-\ning augmented algorithms. In Proc. 34th Conference on Neural Information Processing\nSystems , 2020.\n[6] Andrew An Bian, Kfir Levy, Andreas Krause, and Joachim M. B uhmann. Continu-\nous DR-submodular maximization: Structure and algorithms. InNeural Information\nProcessing Systems (NIPS) , 2017.\n[7] Allan Borodin and Ran El-Yaniv. Online computation and competitive analysis . Cam-\nbridge University Press, 2005.\n[8] Niv Buchbinder and Joseph Naor. Online primal-dual algo rithms for covering and pack-\ning.Mathematics of Operations Research , 34(2):270–286, 2009.\n[9] Niv Buchbinder and Joseph Naor. The design of competitiv e online algorithms via a\nprimal-dual approach. Foundations and Trends in Theoretical Computer Science , 3(2-3):\n93–263, 2009.\n[10] Niv Buchbinder, Moran Feldman, Joseph Seffi, and Roy Sc hwartz. A tight linear time\n(1/2)-approximation for unconstrained submodular maximiza tion.SIAM Computing , 44\n(5):1384–1402, 2015.\n[11] Chandra Chekuri, Jan Vondrak, and Rico Zenklusen. Submodu lar function maximization\nvia the multilinear relaxation and contention resolution sch emes. SIAM Journal on\nComputing , 43(6):1831–1879, 2014.\n[12] Hossein Esfandiari, Nitish Korula, and Vahab Mirrokni. Allocation with traffic spikes:\nMixing adversarial and stochastic models. ACM Trans. Econ. Comput. , 6(3–4), 2018.\n[13] Moran Feldman, Joseph Naor, and Roy Schwartz. A unified continuous greedy algo-\nrithm for submodular maximization. In Symposium on Foundations of Computer Science\n(FOCS) , pages 570–579, 2011.\n17\n\n[14] Moran Feldman, Ola Svensson, and Rico Zenklusen. Online contention resolution\nschemes. In Proc. 27th Symposium on Discrete Algorithms , pages 1014–1033, 2016.\n[15] Sreenivas Gollapudi and Debmalya Panigrahi. Online al gorithms for rent-or-buy with\nexpert advice. In International Conference on Machine Learning , pages 2319–2327, 2019.\n[16] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Le arning-based frequency\nestimation algorithms. In Proc. Conference on Learning Representations , 2019.\n[17] Dennis Komm. Introduction to Online Computation . Springer, 2016.\n[18] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neokl is Polyzotis. The case for\nlearned index structures. In Proc. Conference on Management of Data , pages 489–504,\n2018.\n[19] Andreas Krause and Daniel Golovin. Submodular function maximization. In Tractability:\nPractical Approaches to Hard Problems , pages 71–104. Cambridge University Press, 2014.\n[20] Ravi Kumar, Manish Purohit, and Zoya Svitkina. Improving on line algorithms via ML\npredictions. In Proc. 32nd Conference on Neural Information Processing Sys tems, pages\n9684–9693, 2018.\n[21] Silvio Lattanzi, Thomas Lavastida, Benjamin Moseley, a nd Sergei Vassilvitskii. Online\nscheduling via learned weights. In Proc. Symposium on Discrete Algorithms , pages 1859–\n1877, 2020.\n[22] Thomas Lavastida, Benjamin Moseley, R Ravi, and Chenyan g Xu. Learnable and\ninstance-robust predictions for online matching, flows an d load balancing, 2020.\n[23] Thodoris Lykouris and Sergei Vassilvtiskii. Competitive caching with machine learned\nadvice. In International Conference on Machine Learning , pages 3296–3305, 2018.\n[24] Mohammad Mahdian, Hamid Nazerzadeh, and Amin Saberi. O nline optimization with\nuncertain information. ACM Trans. Algorithms , 8(1), 2012.\n[25] Aranyak Mehta. Online matching and ad allocation. Foundations and Trends ®in\nTheoretical Computer Science , 8(4):265–368, 2013.\n[26] Michael Mitzenmacher. A model for learned bloom filter s, and optimizing by sandwich-\ning. In Proc. Conference on Neural Information Processing Systems , pages 464–473,\n2018.\n[27] Michael Mitzenmacher. Scheduling with predictions an d the price of misprediction. In\nProc. 11th Innovations in Theoretical Computer Science Con ference , 2020.\n[28] Michael Mitzenmacher and Sergei Vassilvitskii. Beyond the Worst-Case Analysis of Al-\ngorithms , chapter Algorithms with Predictions. Cambridge Universi ty Press, 2020.\n[29] Dhruv Rohatgi. Near-optimal bounds for online caching with machine learned advice.\nInProc. Symposium on Discrete Algorithms , pages 1834–1845, 2020.\n[30] Tim Roughgarden. Intrinsic robustness of the price of a narchy. Journal of the ACM , 62\n(5):32, 2015.\n18\n\n[31] Tim Roughgarden. Beyond worst-case analysis. Communications of the ACM , 62(3):\n88–96, 2019.\n[32] Tim Roughgarden. Beyond the Worst-Case Analysis of Algorithms . Cambridge Univer-\nsity Press, 2020.\n[33] Aaron Schild, Erik Vee, Manish Purohit, Ravi Kumar Ravikum ar, and Zoya Svitkina.\nSemi-online bipartite matching. In Proc. 10th Innovations in Theoretical Computer Sci-\nence Conference , 2019.\n[34] Nguyen Kim Thang. Online primal-dual algorithms with c onfiguration linear programs.\nInProc. 31st International Symposium on Algorithms and Compu tation , 2020.\n[35] Jan Vondrák. Polyhedral techniques in combinatorial opti mization. Lecture notes, Nov\n18 2010.\n[36] David P Williamson and David B Shmoys. The design of approximation algorithms .\nCambridge University Press, 2011.\n19",
  "textLength": 48638
}