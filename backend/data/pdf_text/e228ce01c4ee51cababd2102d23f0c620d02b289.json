{
  "paperId": "e228ce01c4ee51cababd2102d23f0c620d02b289",
  "title": "The Second International Verification of Neural Networks Competition (VNN-COMP 2021): Summary and Results",
  "pdfPath": "e228ce01c4ee51cababd2102d23f0c620d02b289.pdf",
  "text": "The Second International Veri\fcation of Neural Networks\nCompetition (VNN-COMP 2021): Summary and Results\nStanley Bak\u0003, Changliu Liuy, Taylor Johnsonz\nAbstract\nThis report summarizes the second International Veri\fcation of Neural Networks Com-\npetition (VNN-COMP 2021), held as a part of the 4th Workshop on Formal Methods\nfor ML-Enabled Autonomous Systems that was collocated with the 33rd International\nConference on Computer-Aided Veri\fcation (CAV). Twelve teams participated in this\ncompetition. The goal of the competition is to provide an objective comparison of the\nstate-of-the-art methods in neural network veri\fcation, in terms of scalability and speed.\nAlong this line, we used standard formats (ONNX for neural networks and VNNLIB for\nspeci\fcations), standard hardware (all tools are run by the organizers on AWS), and tool\nparameters provided by the tool authors. This report summarizes the rules, benchmarks,\nparticipating tools, results, and lessons learned from this competition.\n1 Introduction\nMethods based on machine learning are increasingly being deployed for a wide range of prob-\nlems, including recommendation systems, machine vision and autonomous driving. While ma-\nchine learning has made signi\fcant contributions to such applications, few tools provide formal\nguarantees about the behaviours of neural networks.\nIn particular, for data-driven methods to be usable in safety-critical applications, including\nautonomous systems, robotics, cybersecurity, and cyber-physical systems, it is essential that the\nbehaviours generated by neural networks are well-understood and can be predicted at design\ntime. In the case of systems that are learning at run-time it is desirable that any change to the\nunderlying system respects a given safety-envelope for the system.\nWhile the literature on veri\fcation of traditionally designed systems is wide and successful,\nthere has been a lack of results and e\u000borts in this area until recently. The International Veri\fca-\ntion of Neural Networks Competition (VNN-COMP) was established in 2020, aiming to bring\ntogether researchers working on techniques for the veri\fcation of neural networks. In 2021,\nVNN-COMP1was held as a part of the 4th Workshop on Formal Methods for ML-Enabled\nAutonomous Systems (FoMLAS) that was collocated with the 33rd International Conference\non Computer-Aided Veri\fcation (CAV).\nWhile the \frst VNN-COMP in 2020 was a friendly competition where the participants tested\ntheir tools and reported the results in parallel, the second VNN-COMP in 2021 aims to provide a\nfair comparison and standardize the pipeline of the competition. Such standardization includes\n1) standard formats where we use ONNX for neural networks and VNNLIB for speci\fcations;\nand 2) standard hardware where all tools are run by the organizers on AWS either on CPU\ninstances or cost-equivalent GPU instances. The competition was kicked o\u000b in January and the\nsolicitation for participation was sent in February 2021. By March, several teams registered\n\u0003S. Bak is with Stony Brooks, stanley.bak@stonybrook.edu.\nyC. Liu is with Carnegie Mellon University, cliu6@andrew.cmu.edu.\nzT. Johnson is with Vanderbilt University, taylor.johnson@vanderbilt.edu.\n1https://sites.google.com/view/vnn2021/homearXiv:2109.00498v1  [cs.LO]  31 Aug 2021\n\nVNN-COMP 2021 Report\nFigure 1: Timeline for VNN-COMP2021.\nto participate in this competition. The rule discussion was \fnalized in March 2021 where the\n\fnalized rules are summarized in section 2. From April to May 2021, the benchmarks to test\nthe tools were solicited. Meanwhile, after comparing di\u000berent choices, the organizing team\n\fnally decided to use AWS as our testing platform. On June 1st, 2021, all participants had an\nonline meeting to agree on the rules and benchmarks. By the end of June 2021, twelve teams\nsubmitted their tools and the organizers spent two weeks running the tools on AWS to obtain\nthe \fnal results. The \fnal results were reported in FoMLAS on July 19, 2021. The timeline\nis summarized in \fg. 1. Most discussions took place on the repository https://github.com/\nstanleybak/vnncomp2021 . There are three issues: rules discussion, benchmarks discussion,\nand tool submission, serving as the venues for the corresponding discussions. Moreover, the\nrepository hosts all the submitted benchmarks and the scripts to run the tests.\nThe remainder of this report is organized as follows. Section 2 discusses the competition\nrules. Section 3 lists all participating tools, Section 4 lists all benchmarks, and Section 5 summa-\nrizes the results. Section 6 concludes the report and discusses potential future improvements.\n2 Rules\nTerminology Aninstance is de\fned as (speci\fcation (pre- and post-condition), network, time-\nout). For example: an MNIST classi\fer with one input image, a given local robustness threshold\n\u000f, and a speci\fc timeout. A benchmark is de\fned as a set of instances. For example: a speci\fc\nMNIST classi\fer with 100 input images, a given robustness threshold \u000f, and one timeout per\ninput.\nRun-time caps Per instance: any veri\fcation instance will timeout after at most X minutes,\ndetermined by the benchmark proposer. These can be di\u000berent for each instance. Per bench-\nmark: there is an upper runtime limit of 6 hours per benchmark. For example, a benchmark\nproposal could have six instances with a one hour timeout, or 100 instances each with a 3.6\nminute timeout. To provide a fair comparison, we quantify the startup overhead for each tool\nby running it on small networks; and then we subtract the overhead from the total runtime.\nInstance score Each instance is scored is as follows:\n•Correct hold: 10 points;\n•Correct violated (where random tests or simple adversarial example generation did not\nsucceed): 10 points;\n2\n\nVNN-COMP 2021 Report\n•Correct violated (where random tests or simple adversarial example generation suc-\nceeded): 1 point;\n•Incorrect result: -100 points.\nTime bonus Time bonus is computed as follows.\n•The fastest tool for each solved instance will receive +2 points.\n•The second fastest tool will receive +1 point.\nAll runtimes below 1.0 seconds after overhead correction (explained below) are considered\nto be 1.0 seconds exactly for scoring purposes. If two tools have runtimes within 0.2 seconds\n(after all corrections), for scoring purposes we will consider them the same runtime.\nOverhead Correction According to the rules discussion, we decided to subtract tool overhead\ntime from the results. For example, simply importing tensor\row from Python and acquiring\nthe GPU can sometimes take about 5 seconds, which would be unfortunate for benchmarks like\nACASXu where some veri\fcation times are under a second.\nTo subtract overhead, we created trivial network instances and included those in the mea-\nsurements. We then observed the minimum veri\fcation time along all instances, and considered\nthat to be the overhead time for the tool.\nOne issue with that was that some tools had di\u000berent overhead depending on if they were\nrun in CPU mode or GPU mode, and this type of measurement penalized the GPU mode\nunintentionally. In the score reporting, we include a multi-overhead result where we apply the\noverhead measured for the mode the tool was actually run in.\nBenchmark score The benchmark score of each category is a percentage. It is computed as\n100 times the sum of the individual instance scores for that benchmark category divided by the\nmaximum sum of instance scores of any tool for that benchmark category. For example, the\ntool with the highest sum of instance scores for a category should get 100%.\nFormat This year we standardized the inputs to be onnx neural networks and vnnlib speci\f-\ncation \fles. Tool authors were also required to provide scripts to install their tool as much as\npossible, as well as run their tool on a speci\fc instance provided the network \fle, speci\fcation\n\fle, and timeout. Speci\fcations included simple disjunctions in both pre- and post-conditions\nto encode properties like an unchanged classi\fcation for an input in one of multiple hyper-\nboxes. The speci\fcation is regarded as encoding a counter-example, meaning that a property\nis proven \\correct\" if the speci\fcation is shown to be unsatis\fable, while the property is shown\nto be violated, if a counterexample ful\flling the speci\fcation is found. Hence, robustness with\nrespect to inputs in a hyper-box has to be encoded as disjunctive property, where any of the\nother classes is constrained to become the maximum output.\n3 Participants\nThe following tools and teams participated in VNN-COMP. They are summarized in table 1.\n3.1 Marabou\nTeam Guy Amir1, Clark Barrett2, Ahmed Irfan3, Guy Katz1, Teruhiro Tagomori4, Alex\nUsvyatsov1, Haoze Wu2, Alexsandar Zeljic2.\n1Hebrew University of Jerusalem,2Stanford University,3Amazon Web Services,4NRI\nSecure.\n3\n\nVNN-COMP 2021 Report\nTool GPU? Floating Point Accuracy Use of External Solvers\nMarabou - LP-default Gurobi\nVeriNet - Either Xpress Solver\nERAN Yes Sound Gurobi\n\u000b,\f-CROWN Yes Either Gurobi\nDNNF - Either None\nNNV - 64bit Matlab\nOVAL Yes Either None\nRPM - 64bit None\nNV.jl - 64bit None\nVenus - 64bit Gurobi\nDebona - 32bit Gurobi\nnnenum - Either None\nTable 1: Summary of key features of participating tools. For the \\GPU\" column, we only\nmark the tools that were tested on GPU during this competition. Tools that support GPU but\nwere tested on CPU during this competition are not marked. For the \roating point accuracy,\n\\either\" means the tool can handle either 32 or 64 bit \roating point, depending on the onnx\nnetwork; \\LP-default\" means the tool uses the default settings for LP solver; \\sound\" means\nthe tool is fully \roating-point-sound arithmetic with respect to IEEE-754 semantics up to the\nLP solver (up to 64x slower than non-fp-sound 32 bit arithmetic).\nDescription Marabou [22] is a Neural Network Veri\fcation toolkit that can answer queries\nabout a network's properties by encoding and solving these queries as constraint satisfaction\nproblems. It can accommodate networks with di\u000berent activation functions (including ReLU,\nLeaky ReLU, Sign [1], Max, Absolute value) and topologies (e.g., FFNNs, CNNs, residual\nconnections). It also uses the Split-and-Conquer algorithm [51] for parallelization to further\nenhance scalability. Marabou accepts multiple input formats, including protocol bu\u000ber \fles\ngenerated by the popular TensorFlow framework for neural networks, and the ONNX format.\nThe core of Marabou resolves around the Reluplex procedure [21], but it also supports\nmultiple new techniques and solving modes. In particular, it incorporated the DeepPoly analysis\nintroduced in [38] and the (MI)LP-based bound tightening \frst seen in [42]. In terms of complete\nveri\fcation procedure, in addition to the Reluplex procedure, Marabou also supports solving\nthe veri\fcation query with a MILP encoding.\nFor the competition, Marabou uses the DeepPoly analysis to tighten the variable bounds\nand then uses a portfolio strategy for complete veri\fcation, with a fraction of the CPUs solving\na MILP-encoding of the veri\fcation problem with Gurobi, and the rest running a new complete\nveri\fcation procedure that is currently under submission.\nLink https://github.com/NeuralNetworkVeri\fcation/Marabou\nCommit For reproduction of VNN-Comp results, use https://github.com/anwu1219/Marabou_\nprivate/commit/81e9f14f7ae9f6a2097524ea1291e86434ef42dc .\nHardware and licenses CPU, Gurobi License.\nParticipated benchmarks ACASXu ,cifar10 resnet ,eran ,marabou-cifar10 ,mnistfc ,\noval21 ,verivital .\n4\n\nVNN-COMP 2021 Report\n3.2 VeriNet\nTeam Patrick Henriksen, Alessio Lomuscio (Imperial College London).\nDescription VeriNet [17, 18] is a complete Symbolic Interval Propagation (SIP) based ver-\ni\fcation toolkit for feed-forward neural networks. The underlying algorithm utilises SIP to\ncreate a linear abstraction of the network, which, in turn, is used in an LP-encoding to anal-\nyse the veri\fcation problem. A branch and bound-based re\fnement phase is used to achieve\ncompleteness.\nVeriNet implements various optimisations, including a gradient-based local search for coun-\nterexamples, optimal relaxations for Sigmoids, adaptive node splitting [17], succinct LP-\nencodings, and a novel splitting heuristic that takes into account indirect e\u000bects splits have\non succeeding relaxations [18].\nVeriNet supports a wide range of layers and activation functions, including Relu, Sigmoid,\nTanh, fully connected, convolutional, max and average pooling, batch normalisation, reshape,\ncrop and transpose operations, as well as additive residual connections.\nNote that VeriNet subsumes the Deepsplit method presented in [18].\nLink Scheduled for release late August 2021 at: https://vas.doc.ic.ac.uk/software/neural/.\nHardware and licences CPU and GPU, Xpress Solver license required for large networks.\nParticipated benchmarks ACASXu ,cifar10 resnet ,cifar2020 ,eran ,marabou-cifar10 ,\nmnistfc ,nn4sys ,oval21 ,verivital .\n3.3 ERAN\nTeam Mark Niklas M uller (ETH Zurich), Gagandeep Singh (UIUC), Markus P uschel (ETH\nZurich), Martin Vechev (ETH Zurich)\nDescription ERAN [30,33,36{39] is a neural network veri\fer based on leveraging abstract in-\nterpretations to encode a network, pre- and post-condition as an LP or MILP problem. ERAN\nsupports both incomplete and complete veri\fcation and can handle fully-connected, convo-\nlutional, and residual network architectures containing ReLU, Sigmoid, Tanh, and Maxpool\nnon-linearities. It uses 64-bit precision (up to 16 times slower than 32-bit on some GPUs) and\nan arithmetic which is \roating-point-sound (performs 4 times more operations) with respect\nto IEEE-754 semantics up to the LP solver. Single- and multi-neuron relaxations of non-linear\nactivations [30] computed using GPUPoly [33] are combined with a partial MILP encoding\nand neuron-wise bound-re\fnement [39] to obtain a precise network encoding. The analyzer is\nwritten in Python, uses ELINA [40] for numerical abstractions, and Gurobi for solving LP and\nMILP instances. While the use of both GPU and CPU enables ERAN to utilize all resources\nof a system it also requires both a relatively strong GPU and CPU to be available in order to\navoid one bottlenecking the other (which occurred with the GPU AWS instance).\nWhen run in complete mode, ERAN generates concrete counterexamples. In incomplete\nmode, ERAN attempts to falsify a property by running a PGD attack before attempting veri\f-\ncation using increasingly more expensive and precise abstractions. As we use the same abstrac-\ntions for all instances of a benchmark, ERAN might fail to verify a property before exceeding\nthe timeout. In these cases, a more expensive abstraction might have been able to verify the\nproperty in time.\nLink https://github.com/eth-sri/eran\nCommit Please use the main repository for anything but reproducing VNN-COMP results.\nhttps://github.com/mnmueller/eran_vnncomp2021.git\n(1e474c77f72f86f450df9f0a860b4d35c490ea7c)\n5\n\nVNN-COMP 2021 Report\nHardware and licences CPU and GPU, GUROBI License\nParticipated benchmarks ACASXu ,cifar10 resnet ,cifar2020 ,eran ,marabou-cifar10 ,\nmnistfc ,nn4sys ,oval21 ,verivital .\n3.4\u000b;\f-CROWN\nTeam Huan Zhang*(Carnegie Mellon), Kaidi Xu*(Northeastern), Shiqi Wang*(Columbia),\nZhouxing Shi (UCLA), Yihan Wang (UCLA), Xue Lin (Northeastern), Suman Jana (Columbia),\nCho-Jui Hsieh (UCLA), Zico Kolter (Carnegie Mellon); * indicates equal contribution.\nDescription The\u000b;\f-CROWN ( alpha-beta-CROWN ) veri\fer is based on an e\u000ecient bound\npropagation algorithm, CROWN [55], with a few crucial extensions [49, 53, 54]. We use the\ngeneralized version of CROWN in the auto LiRPA library [53] which supports general neural\nnetwork architectures (including convolutional layers, residual connections, recurrent neural\nnetworks and Tranformers) and a wide range of activation functions (e.g., ReLU, tanh, sig-\nmoid, max pooling and average pooling), and is e\u000eciently implemented on GPUs. We jointly\noptimize intermediate layer bounds and \fnal layer bounds using gradient ascent (referred to as\n\u000b-CROWN or optimized CROWN/LiRPA [54]). Additionally, we use branch and bound [10]\n(BaB) and incorporate split constraints in BaB into the bound propagation procedure e\u000eciently\nvia the\f-CROWN algorithm [49]. The combination of e\u000ecient, optimizable and GPU acceler-\nated bound propagation with BaB produces a powerful and scalable neural network veri\fer.\nOur veri\fer also utilizes a mixed integer programming (MIP) solver (Gurobi) for networks\nwhere MIP runs relatively fast, following the formulation in [42]. We use MIP to solve the\ntightest possible intermediate layer bounds for as many neurons as possible on CPUs within\nthe timeout budget, and use \u000b-CROWN to solve the remaining ones on GPUs. Finally, we\nconduct BaB with \f-CROWN using tightened bounds. Although the GPU AWS instance has\nweak CPUs, we still \fnd that MIP is helpful for some benchmarks, and it can become more\nbene\fcial on a machine with both strong CPUs and GPUs. Note that \u000b-CROWN can exceed\nthe power of a typical LP veri\fer when intermediate layer bounds are jointly optimized [32,54],\nso we do not use a LP solver to tighten bounds.\nLink https://github.com/huanzhang12/alpha-beta-CROWN\nCommit c12e6eeaf6b16f99a99b65f377d0f450d6466a83 (only for reproducing competition\nresults; please use the main branch version for other proposes)\nHardware and licenses CPU and GPU with 32-bit or 64-bit \roating point; Gurobi license\nrequired for mnistfc ,eran ,marabou-cifar10 andverivital benchmarks.\nParticipated benchmarks ACASXu ,cifar10 resnet ,cifar2020 ,eran ,marabou-cifar10 ,\nmnistfc ,nn4sys ,oval21 ,verivital .\n3.5 DNNF\nTeam David Shriver, Sebastian Elbaum, Matt Dwyer (University of Virginia).\nDescription DNNF [35] is a tool for neural network property falsi\fcation. It only attempts\nto \fnd counter-examples to a property speci\fcation, and will not prove that a property holds.\nDNNF reduces properties and networks to robustness problems which can then be falsi\fed using\nmany di\u000berent adversarial attack methods. Our reduction approach enables us to support much\nmore complex property and network speci\fcations, such as speci\fcations with direct input and\noutput relations, such as the relation y >x . The backend falsi\fcation method used for VNN-\nCOMP is a custom implementation of PGD, but DNNF can also be run with several o\u000b-the-shelf\n6\n\nVNN-COMP 2021 Report\nmethods from foolbox or cleverhans, two popular python packages for adversarial attacks.\nDNNF makes use of the DNNV framework [34] to load networks and properties, as well as to\nperform network simpli\fcations. The current implementation of DNNF supports many di\u000berent\nnetwork operations. In particular it supports ONNX models with the following operations: Add,\nAtan, AveragePool, BatchNormalization, Concat, Conv, ConvTranspose, Elu, Flatten, Gather,\nGemm, LeakyRelu, MatMul, MaxPool, Mul, Relu, Reshape, Resize, Shape, Sigmoid, Softmax,\nSub, Tanh, Transpose, Unsqueeze.\nDNNF can also be run on GPUs, which can speed up falsi\fcation for large models.\nLink https://github.com/dlshriver/DNNF\nCommit VNN-COMP results can be reproduced with commit\nd4f08b43e4ad622157c65ac071183a3a0f4e6fe0 . For other uses, we suggest the main branch.\nHardware and licences DNNF can be run on the CPU or GPU, with no additional licenses\nrequired.\nParticipated benchmarks ACASXu ,cifar10 resnet ,cifar2020 ,eran ,marabou-cifar10 ,\nmnistfc ,nn4sys ,oval21 ,verivital .\n3.6 NNV\nTeam Neelanjana Pal (Vanderbilt University), Taylor T Johnson (Vanderbilt University)\nDescription The Neural Network Veri\fcation Tool (NNV) [43{46, 52] is written primarily\nwith Matlab and implements reachability-analysis methods for neural network veri\fcation with\na particular focus on applications of closed-loop neural network control systems in autonomous\ncyber-physical systems. NNV uses geometric representations such as star sets that allows for\na layer-by-layer computation of the exact reachable set for feed-forward deep neural networks.\nIn the event that a particular safety property is violated, NNV can construct and visualize the\ncomplete set of counterexample inputs for a neural network.\nLink https://github.com/verivital/nnv.\nCommit 3ca2629aaceb9080e4d08a0f9c6b51854f9c7b7f (for reproducing competition re-\nsults; otherwise please use the master version).\nHardware and licences GPU and CPU with 64-bit \roating point. A license for Matlab will\nbe required.\nParticipated benchmarks ACASXu ,cifar2020 ,eran ,mnistfc ,oval21 ,verivital .\n3.7 OVAL\nTeam Alessandro De Palma (University of Oxford), Florian Jaeckle (University of Oxford),\nM. Pawan Kumar (University of Oxford)\nDescription The OVAL veri\fcation tool is an optimization-based complete veri\fer that relies\non a specialized Branch and Bound (BaB) framework for neural network veri\fcation. In this\ncontext, a BaB method is composed of three main components (see [9] for an overview): a\nbranching strategy to divide the veri\fcation property into easier subproblems, a bounding algo-\nrithm to compute over-approximation bounds for each subproblem, and a falsi\fcation algorithm\nto look for counter-examples to the property.\nFor networks with medium to large input dimensionality, OVAL relies on the e\u000ecient\nFSB [13] branching strategy, which combines a dual-based scoring of ReLU neurons [9] with\ninexpensive strong branching approximations to select an activation to split upon. For networks\nof small input dimensionality, OVAL can revert to input splitting as in [10]. Bounds on the\n7\n\nVNN-COMP 2021 Report\nsubproblems are obtained by adaptively choosing [12] between bounding algorithms of varying\ndegrees of tightness: the competition entry relies on Beta-CROWN [49], which e\u000bectively solves\nthe convex hull of element-wise activations, and Active Set [11], which operates on the tighter\nrelaxation from [2] to tackle harder properties. In addition, the framework supports a variety\nof bounding algorithms [8, 12, 15, 54]. The search for counter-examples is performed using the\nMI-FGSM [14] adversarial attack, which we adapted to perform general property falsi\fcation.\nThe implementation of the OVAL framework, written in PyTorch [31], exploits GPU accel-\neration and is massively parallel over both the BaB subproblems and the relative intermediate\ncomputations [8]. It currently supports fully connected and convolutional networks, with ReLU,\nmaxpool and average pooling layers.\nLink https://github.com/oval-group/oval-bab.\nCommit 014b6ee5071508430c8e515bbae725306db68fe1 in order to reproduce competition\nresults. We otherwise suggest to employ the master version.\nHardware and licences GPU and CPU with 32-bit or 64-bit \roating point. No license is\nrequired.\nParticipated benchmarks ACASXu ,cifar2020 ,eran ,marabou-cifar10 ,mnistfc ,nn4sys ,\noval21 ,verivital .\n3.8 RPM\nTeam Joe Vincent (Stanford), Mac Schwager (Stanford)\nDescription The Reachable Polyhedral Marching (RPM) tool [47] is a method for computing\nexact forward and backward reachable sets of feedforward neural networks with ReLU acti-\nvation. Veri\fcation problems are posed as backward reachability problems. A unique feature\nof the RPM tool is its incremental computation of reachable sets. For veri\fcation this means\nthat unsafe inputs may be found before computing the complete reachable set, leading to early\ntermination. RPM does not currently have a parallel implementation, although the algorithm\nis amenable to parallelization.\nLink https://github.com/StanfordMSL/Neural-Network-Reach/tree/vnn_comp_2021\nCommit 861ce6e380e3cc2d439a7bca87b59817e4624af6 for reproducing competition results.\nFor other purposes the most recent commit is suggested.\nHardware and licences CPU, no license is required.\nParticipated benchmarks ACASXu\n3.9 ComposableNeuralVeri\fcation (NV.jl)\nTeam Tianhao Wei (Carnegie Mellon), Chen Tan (Northeastern), Changliu Liu (Carnegie\nMellon)\nDescription This tool is adapted from the original NeuralVeri\fcation.jl [25] developed at the\nStanford Intelligent System Lab. This Julia toolbox implemented a wide variety of veri\fcation\nalgorithms that use reachability, optimization, and search, which are summarized in [26]. We\nadded support for onnx format networks and vnnlib format speci\fcations,\nLink https://github.com/intelligent-control-lab/NeuralVerification.jl\nHardware and licences CPU, no licence\nCommit 4e612602ba4b34b42416742d85476d9b0dcdcb51 (for reproducing competition results;\notherwise please use the master branch)\n8\n\nVNN-COMP 2021 Report\nParticipated benchmarks nn4sys and AcasXu\n3.10 Venus\nTeam Panagiotis Kouvaros (Imperial College London), Alessio Lomuscio (Imperial College\nLondon)\nDescription Venus is a complete veri\fcation tool for Relu-based feed-forward neural net-\nworks. Venus implements a MILP-based veri\fcation method whereby it leverages dependency\nrelations between the ReLU nodes to reduce the search space that needs to be considered dur-\ning branch-and-bound. The dependency relations are exploited via callback cuts [6] and via\na branching method that divides the veri\fcation problem into a set of sub-problems whose\nMILP formulations require fewer integrality constraints [23]. To derive tight MILP encodings,\nVenus additionally implements a symbolic interval propagation method for computing the pre-\nactivation bounds of the ReLU nodes; the method optimises the linear relaxation of each of the\nReLU nodes towards minimising the over-approximation error in subsequent layers.\nLink https://github.com/vas-group-imperial/venus2\nCommit For the reproduction of the VNN-COMP2021 results please use the repository https:\n//github.com/pkouvaros/venus2_vnncomp21 (57e9608041d230b5d78c4f2afb890b81035436a1).\nHardware and licenses CPU, GUROBI License.\nParticipated benchmarks ACASXU, mnistfc, nn4sys.\n3.11 Debona\nTeam Christopher Brix (RWTH Aachen University), Thomas Noll (RWTH Aachen University)\nDescription Debona is a fork of VeriNet [17]. However, the abstract domain used by VeriNet\nde\fnes symbolic linear upper and lower bounds that are parallel to each other, i.e., o\u000bset only\nby some scalar value. On the contrary, Debona utilizes independent upper and lower bounds.\nThis allows for a tighter relaxation especially for ReLU operations, where a lower bound of zero\nmay be better than bounds that are negative in large regions of the input space. This idea has\nbeen described in [7] but was independently previously published in [38].\nLink https://github.com/ChristopherBrix/Debona\nCommit f000f3d483b2cc592233d0ba2a1a0327210562c8\nHardware and licences CPU, Gurobi licence\nParticipated benchmarks AcasXu, eran, mnistfc and nn4sys\n3.12 nnenum\nTeam Stanley Bak (Stony Brook Univeristy)\nDescription The nnenum tool uses multiple levels of abstraction to achieve high-performance\nveri\fcation of ReLU networks without sacri\fcing completeness [3]. Analysis combines three\ntypes of zonotopes with star set (triangle) overapproximations [45], and uses e\u000ecient parallelized\nReLU case splitting [5]. The ImageStar method [43] allows sets to be quickly propagated\nthrough all layers supported by the ONNX runtime, such as convolutional layers with arbitrary\nparameters. The tool is written in Python 3, uses GLPK for LP solving. New this year we\nadded support for vnnlib \fles, and optimized some of the LP timeout parameters for acasxu [4].\nLink https://github.com/stanleybak/nnenum\nCommit c93a39cb568f58a26015bd151acafab34d2d4929\n9\n\nVNN-COMP 2021 Report\nBenchmark Application Network Types Largest NN\nAcasxu Control Feedforward + ReLU Only 54.6k\nCifar10 resnet Image Classi\fcation ResNet 487k\nCifar2020 (unscored) Image Classi\fcation Conv + ReLU 9.41M\nEran Image Classi\fcation Feedforward + non-ReLU 1.68M\nMarabou-cifar10 Image Classi\fcation Conv + ReLU 1.29M\nMnistfc Image Classi\fcation Feedforward + ReLU Only 2.03M\nnn4sys Database Indexing Feedforward + ReLU Only 336.5M*\nOval21 Image Classi\fcation Conv + ReLU 840k\nVerivital Image Classi\fcation Conv + maxpool / avgpool 46.3k\n*After zipping, the network is of 1.79M.\nTable 2: Overview of all benchmarks.\nHardware and licences CPU, No licenses required\nParticipated benchmarks AcasXu, cifar2020, mnistfc, oval\n4 Benchmarks\n4.1 ACAS Xu\nNetworks The ACASXu benchmarks consists of ten properties de\fned over 45 neural networks\nused to issue turn advisories to aircraft to avoid collisions. The neural networks have 300 neurons\narranged in 6 layers, with ReLU activation functions. There are \fve inputs corresponding to\nthe aircraft states, and \fve network outputs, where the minimum output is used as the turn\nadvisory the system ultimately produces.\nSpeci\fcations We use the original 10 properties [21], where properties 1-4 are checked on all\n45 networks as was done in later work by the original authors [22]. Properties 5-10 are checked\non a single network. The total number of benchmarks is therefore 186. The original veri\fcation\ntimes ranged from seconds to days|including some benchmark instances that did not \fnish.\nThis year we used a timeout of around two minutes (116 seconds) for each property, in order\nto \ft within a total maximum runtime of six hours.\n4.2 Cifar10 resnet\nProposed by the\u000b;\f-CROWN team.\nMotivations Currently, many tools are hard-coded to handle feedforward networks only. To\nmake neural network veri\fcation more useful in practical scenarios, we advocate that tools\nshould handle more general architecture. Residual networks [16] (ResNet) is the \frst step\ntowards this goal due to its relatively simple structure and practical signi\fcance. The propose\nof this benchmark is to provide some incentives for the community to develop more generic\ntools.\nNetworks We provided two ResNet models on CIFAR-10 image classi\fcation task with the\nfollowing structures:\n•ResNet-2B with 2 residual blocks: 5 convolutional layers + 2 linear layers\n10\n\nVNN-COMP 2021 Report\n•ResNet-4B with 4 residual blocks: 9 convolutional layers + 2 linear layers\nThe networks are trained via adversarial training with an `1perturbation norm of \u000f=2\n255.\nFor simplicity, these networks do not contain batch normalization or pooling layers, and use\nReLU activation functions. The ResNet-4B model is relatively large with over 10K neurons.\nWe evaluated both networks using a 100-step projected gradient descent (PGD) attack with\n5 random restarts, and a simple bound propagation based veri\fcation algorithm CROWN [55]\n(mathematically equivalent to the abstract interpretation used in DeepPoly [38]) under `1\nnorm perturbations. The results are listed in Table 3.\nModel # ReLUs Clean acc.\u000f= 2=255 \u000f= 1=255\nPGD acc. Veri\fed acc. PGD acc. Veri\fed acc.\nResNet-2B 6244 69.25% 54.82% 26.88% 62.24% 57.16%\nResNet-4B 14436 77.20% 61.41% 0.24% 69.75% 23.28%\nTable 3: Clean accuracy, PGD accuracy and CROWN veri\fed accuracy for ResNet models.\nNote that the veri\fed accuracy is obtained via the vanilla version of CROWN/DeepPoly which\nhas been widely used as a simple baseline, not the \u000b;\f-CROWN tool used in the competition.\nTo ensure the appropriate level of di\u000eculty, we use \u000f=2\n255for the ResNet-2B model and\n\u000f=1\n255for the ResNet-4B model.\nSpeci\fcations We randomly select 48 images from the CIFAR-10 test set for the ResNet-2B\nmodel and 24 images for the ResNet-4B model. The images are classi\fed correctly and cannot\nbe attacked by a 100-step PGD attack with 5 random restarts. For each image, we specify the\nproperty that the logit of the ground-truth label is always greater than the logits of all other 9\nlabels within `1norm input perturbation of \u000f=2\n255for ResNet-2B and \u000f=1\n255for ResNet-4B.\nThe per-example timeout is set to 5 minutes and the overall runtime is guaranteed to be less\nthan 6 hours.\n4.3 Cifar2020 (unscored)\nMotivation This benchmark combines two convolutional CIFAR10 networks from last year's\nVNN-COMP 2020 with a new, larger network with the goal to evaluate the progress made by\nthe whole \feld of Neural Network veri\fcation.\nNetworks The two ReLU networks cifar 102255and cifar 108255with two convolu-\ntional and two fully-connected layers were trained for `1perturbations of \u000f=2\n255and8\n255,\nrespectively, using COLT [29] and the larger ConvBig with four convolutional and three fully-\nconnected networks, was trained using adversarial training [28] and \u000f=2\n255.\nSpeci\fcations We draw the \frst 100 images from the CIFAR10 test set and for every network\nreject incorrectly classi\fed ones. For the remaining images, the speci\fcations describe a correct\nclassi\fcation under an `1-norm perturbation of at most2\n255and8\n255forcifar 102255and\nConvBig andcifar 108255, respectively and allow a per sample timeout of 5 minutes.\n4.4 eran\nProposed by: The ERAN team\n11\n\nVNN-COMP 2021 Report\nMotivation While most Neural Network Veri\fcaiton methods focus their analysis on ReLU\nbased networks, many modern network architectures, e.g., E\u000ecientNet [41], are based on non-\npiecwise-linear activation functions. To begin to understand how the choice of activation func-\ntion a\u000bects certi\fability, the eran benchmark aims at comparing the certi\fabilty of networks\nbased on piecewise-linear and non-piecwise-linear activation functions under an `1-norm based\nadversary.\nNetworks We consider a ReLU network with 8 hidden layers of width 200 and a Sigmoid\nnetwork with 6 hidden layers of width 200. Both networks were trained using standard training.\nSpeci\fcations We sample random images from the MNIST test set until we obtain 36 cor-\nrectly classi\fed images per network. For these images, the speci\fcations describe a correct\nclassi\fcation under an `1-norm perturbation of at most 0.015 and 0.012 for the ReLU and\nSigmoid network, respectively, and allow a per sample timeout of 5 minutes.\n4.5 Marabou-cifar10\nProposed by The Marabou team.\nNetworks This benchmark contains three convolutional networks, cifar10 small.onnx ,\ncifar10 medium.onnx , and cifar10 large.onnx , trained on the CIFAR10 dataset. Each net-\nwork has 2 convolutional layers followed by two fully connected feed-forward layers. Each layer\nuses the ReLU activation functions. The networks are all trained with Adam optimizer for 120\nepochs with learning rate 0.0002. The three networks contain 2568, 4944, and 10528 ReLUs,\nrespectively. The test accuracy are 63 :14%, 70:21%, and 74 :16%, respectively. The networks\nexpect the input image to be normalized between 0 and 1.\nSpeci\fcations We randomly sample correctly classi\fed images from the CIFAR10 test set.\nThe speci\fcations are targeted adversarial robustness, which states that the network does not\nmis-classify an image as a given adversarial label under l1-norm perturbations. The target\nlabel is chosen as ( correctLabel + 1) mod 10. We propose two perturbation bounds: 0.012 and\n0.024, and allow a per-query timeout of 5 minutes.\n4.6 Mnistfc\nProposed by The VeriNet team.\nMotivation This benchmark contains fully connected networks with ReLU activation func-\ntions and varying depths.\nNetworks The benchmark set consists of three fully-connected classi\fcation networks with 2,\n4 and 6 layers and 256 ReLU nodes in each layer trained on the MNIST dataset. The networks\nwere \frst presented in a benchmark in VNN-COMP 2020 [48].\nSpeci\fcations We randomly sampled 15 correctly classi\fed images from the MNIST test set.\nFor each network and image, the speci\fcation was a correct classi\fcation under l1perturbations\nof at most \u000f= 0:03 and\u000f= 0:05. The timeouts were 2 minutes per instance for the 2-layer\nnetwork and 5 minutes for the remaining two networks.\n4.7 NN4Sys\nProposed by The ComposableNeuralveri\fcation team\n12\n\nVNN-COMP 2021 Report\nApplication The benchmark contains networks for database indexing, which is a 1D to 1D\nmapping.\n•Background : learned index is a neural network (NN) based database index proposed by\nKraska et al. [24], 2018. It shows great potential but has one drawback|for non-existing\nkeys (i.e., the keys that do not exist in the database), the outputs of a learned index can\nbe arbitrary.\n•What we do : to provide safety guarantees for allkeys, we design a speci\fcation to dictate\nhow \\far\" one predicted position can be, compared to its actual position (or the positions\nthat non-existing keys should be).\n•What to verify : our benchmark provides multiple pairs of (1) learned indexes (trained\nNNs) and (2) corresponding speci\fcations. We design these pairs with di\u000berent parame-\nters such that they cover a variety of user needs and have varied di\u000eculties for veri\fers.\n•Translating learned indexes to a VNN benchmark : the original learned index [24] contains\ntwo stages (of NNs) for high precision. However, this cascading structure is inconvenien-\nt/unsupported to verify because there is a \\switch\" operation|choosing one NN in the\nsecond stage based on the prediction of the \frst stage's NN. To convert learned indexes to\na standard form, we merge the NNs in both stages into an integrated network by adding\nsome hand crafted layers.\n•A note on broader impact : using NNs for systems is a broad topic, but many existing\nworks lack strict safety guarantees. We believe that NN Veri\fcation can help system\ndevelopers gain con\fdence to apply NNs to critical systems. We hope our benchmark can\nbe an early step towards this vision.\nNetworks The networks are feedforward with ReLU activations, and they are sparse networks.\nThere are six networks in this benchmark. Three of them have original size 194.2M and zipped\nsize 1.79M; the other three have original size 336.5M and zipped size 790k. This is because\nonnx does not support directly encoding of sparse matrices, hence the networks are stored as\nfully connected networks.\nSpeci\fcations The speci\fcation aims to check if the prediction error is bounded. The spec-\ni\fcation is a collection of pairs of input and output intervals such that any input in the input\ninterval should be mapped to the corresponding output interval.\n4.8 Oval21\nProposed by The OVAL team.\nMotivations The majority of adversarial robustness benchmarks consider image-independent\nperturbation radii, possibly resulting in some properties that are either easily veri\fed by all\nveri\fcation methods, or too hard to be veri\fed (for commonly employed timeouts) by any of\nthem. In line with the OVAL veri\fcation dataset from VNN-COMP 2020 [48], whose versions\nhave already been used in various recent works [8,11{13,19,20,27,49,54], the OVAL21 bench-\nmark associates to each image-network pair a perturbation radius found via binary search to\nensure that all properties are challenging to solve.\n13\n\nVNN-COMP 2021 Report\nNetworks The benchmark includes 3 ReLU-based convolutional networks which were robustly\ntrained [50] against `1perturbations of radius \u000f= 2=255 on CIFAR10. Two of the networks,\nnamed base and wide , are composed of 2 convolutional layers followed by 2 fully connected\nlayers and have respectively 3172 and 6244 activations. The third model, named deep , has 2\nadditional convolutional layers and a total of 6756 activations.\nSpeci\fcations The veri\fcation properties represent untargeted adversarial robustness (with\nrespect to all possible misclassi\fcations) to `1perturbations of varying \u000f, with a per-instance\ntimeout of 720 seconds. The property generation procedure relies on commonly employed\nlower and upper bounds to the adversarial loss to exclude perturbation radii that yield trivial\nproperties. 10 correctly classi\fed images per network are randomly sampled from the entire\nCIFAR10 test set, and a distinct \u000f2[0;16=255] is associated to each. First, a binary search is\nrun to \fnd the largest \u000fvalue for which a popular iterative adversarial attack [14] fails to \fnd\nan adversarial example. Then, a second binary search is run to \fnd the smallest \u000fvalue for\nwhich bounds [54] from the element-wise convex hull of the activations (with \fxed intermediate\nbounds from [50, 55]) fail to prove robustness. Both binary search procedures are run with a\ntolerance of \u000ftol= 0:1. Denoting \u000flbas the smallest output from the two routines, and \u000fubas\nthe largest, the following perturbation radius is chosen: \u000f=1\n3\u000flb+2\n3\u000fub.\nLink https://github.com/stanleybak/vnncomp2021/tree/main/benchmarks/oval21\n4.9 Verivital\nProposed by The VeriVITAL team.\nMotivation Neural networks with pooling layers are vastly used in several applications. The\nmain motivation for proposing this benchmark was to include the pooling layers as part of this\nyear's VNN-Comp.\nNetworks This benchmark contains two MNIST classi\fers with pooling layers, one with av-\neragepooling layers and the other with maxpooling.\nSpeci\fcations We randomly sampled 20 correctly classi\fed images from the MNIST test set.\nFor the network with averagepooling layers, the speci\fcation was to correctly classify those\nrandomly chosen images with an l1perturbation radii( \u000f) of 0:02 and 0:04 and a timeout of 5\nminutes. For the network with maxpooling layers the corresponding radius was 0 :004 with a\ntimeout of 7 minutes.\nLink https://github.com/stanleybak/vnncomp2021/tree/main/benchmarks/verivital\n14\n\nVNN-COMP 2021 Report\n5 Results\nEach tool was run on all benchmarks which produced a csv\fle of results. This was sent to\nthe authors for review, which sometimes required rerunning certain benchmarks to make sure\nthey match the expected results. Python code was then written to process the results and\ncompute the scores. The \fnal csv\fles for each tool as well as scoring scripts are available\nonline: https://github.com/stanleybak/vnncomp2021_results .\nThis also includes detailed log \fles for each benchmark showing the speci\fc runtime for each\ntool and the score awarded. This can be used to \fnd challenging instances to help with tool\ndevelopment. For example, in the output \fles in the repo you may see things like:\nRow: ['ACASXU_run2a_4_2_batch_2000-prop_2', '-', '6.4 (h)', '10.5 (h)',\n'timeout', '41.1 (h)', 'timeout', 'timeout', '64.8 (h)', '62.5 (h)',\n'timeout', 'timeout', 'timeout', '-']\n73: nnv score: 0\n73: nnenum score: 12\n73: venus2 score: 11\n73: NN-R score: 0\n73: VeriNet score: 10\n73: DNNF score: 0\n73: Debona score: 0\n73: a-b-CROWN score: 10\n73: oval score: 10\n73: Marabou score: 0\n73: ERAN score: 0\n73: NV.jl score: 0\n73: randgen score: 0\nThe tools are listed in order, and row is the times and result for each tool. So nnenum should\nbe holds with a time of 6.4 (after subtracting overhead). The scores are also listed for each tool.\nSince nnenum was the fastest on this instance, it got 12 points, 10 for correct plus 2 for time\nbonus as fastest. The second fastest was venus2 at 10.5 seconds, so they get 11 points. None\nof the remaining tools were within 0.2 seconds, so they all received 10 points if they completed\nanalysis successfully.\n5.1 Overall Score\nThe overall score for VNN-COMP 2021 is shown in Table 4. Two tables are included, based\non the two ways to measure overhead discussed in Section 2. Overall, \u000b;\f-CROWN performed\nbest, followed by VeriNet. We awarded two third place results, one to oval and one to ERAN, as\ntheir ranking depended upon factors like overhead as well as how incorrect results were judged,\ndiscussed next. For all the remaining tables in this section, the numbers reported correspond\nto the multi-overhead measurements.\nOne unexpected aspect was how to judge incorrect results, since tools currently are not\nrequired to produce a counter-example when an instance is falsi\fed. We considered two rea-\nsonable options, which was voting (majority is assumed to be correct), and odd-one-out. In\nodd-one-out, only if a single tool's output di\u000bers from all the others is the result is considered\nincorrect. If multiple tools produce the same result or if only two tools completed the instance\nand their results di\u000ber, then the instance is ignored for scoring purposes. This generally had a\n15\n\nVNN-COMP 2021 Report\nslight e\u000bect on the score, which was signi\fcant enough to a\u000bect the order in the total ranking.\nSpeci\fcally, the positions of ERAN and oval for third place could be a\u000bected by the scoring\nparameters, when using the \"Single Overhead\" overhead correction. Notice that, however, both\nvoting and odd-one-out are imperfect ways to judge incorrect results, and it may be the case\nthat the mismatching tool was in fact correct. In future editions of VNN-COMP we may stan-\ndardize counter-example outputs and require they are produced when instances can be falsi\fed,\nto remedy this shortcoming. For the reported category scores, we display odd-one-out scores\nand results.\nOther statistics and the individual benchmark scores are also included below. In general,\nthe GPU tools did better, as well as tools that could support a large number of benchmarks and\nveri\fed a large number of benchmark instances. Several tools produced mismatching results,\nand an interesting followup study could identify the underlying reasons for this unsoundness.\nTable 4: VNN-COMP 2021 Overall Score\n(a) Single Overhead\n# Tool Score\n1\u000b,\f-CROWN 779.2\n2 VeriNet 701.2\n3 oval 582.0\n4 ERAN 581.1\n5 Marabou 335.3\n6 Debona 201.9\n7 venus2 189.2\n8 nnenum 184.5\n9 nnv 57.2\n10 NV.jl 48.1\n11 RPM 25.4\n12 DNNF 24.3\n13 randgen 1.9(b) Multi-Overhead\n# Tool Score\n1\u000b,\f-CROWN 779.7\n2 VeriNet 705.0\n3 ERAN 643.4\n4 oval 581.8\n5 Marabou 339.0\n6 Debona 201.9\n7 venus2 189.2\n8 nnenum 184.6\n9 nnv 57.2\n10 NV.jl 48.1\n11 RPM 25.4\n12 DNNF 24.3\n13 randgen 1.9\n16\n\nVNN-COMP 2021 Report\n5.2 Other Stats\nThis section presents other statistics related to the measurements that are interesting, but did\nnot play a direct role in scoring this year. With ERAN, which had di\u000berent overheads, the\n`CPU Mode' column in Table 5 corresponds to the overhead used for the ACASXu and ERAN\nbenchmarks, whereas the `Seconds' column corresponds to when the GPU was used (all others).\nTable 5: Overhead\n# Tool Seconds CPU Mode\n1 Marabou 0.2 -\n2 randgen 0.3 -\n3 nnenum 1.0 -\n4 venus2 1.7 -\n5 DNNF 2.0 -\n6 VeriNet 2.2 -\n7 Debona 2.5 -\n8 oval 5.1 -\n9\u000b,\f-CROWN 6.1 -\n10 ERAN 7.1 3.7\n11 nnv 8.4 -\n12 NV.jl 20.9 -\n13 RPM 52.2 -\nTable 6: Num Benchmarks Participated\n# Tool Count\n1 VeriNet 9\n2 ERAN 9\n3\u000b,\f-CROWN 9\n4 oval 8\n5 Marabou 7\n6 nnv 6\n7 DNNF 5\n8 randgen 4\n9 nnenum 4\n10 Debona 4\n11 venus2 3\n12 NV.jl 2\n13 RPM 1\n17\n\nVNN-COMP 2021 Report\nTable 7: Num Instances Veri\fed\n# Tool Count\n1\u000b,\f-CROWN 766\n2 VeriNet 717\n3 ERAN 656\n4 oval 636\n5 Marabou 364\n6 nnenum 310\n7 Debona 280\n8 venus2 266\n9 nnv 141\n10 NV.jl 97\n11 RPM 72\n12 DNNF 55\n13 randgen 33\nTable 8: Num Violated\n# Tool Count\n1 ERAN 177\n2\u000b,\f-CROWN 177\n3 oval 175\n4 VeriNet 175\n5 Marabou 103\n6 nnenum 73\n7 Debona 70\n8 venus2 63\n9 DNNF 55\n10 RPM 44\n11 randgen 33\n12 NV.jl 11\n18\n\nVNN-COMP 2021 Report\nTable 9: Num Holds\n# Tool Count\n1\u000b,\f-CROWN 589\n2 VeriNet 542\n3 ERAN 479\n4 oval 461\n5 Marabou 261\n6 nnenum 237\n7 Debona 210\n8 venus2 203\n9 nnv 141\n10 NV.jl 86\n11 RPM 28\nTable 10: Mismatched (Incorrect) Results\n# Tool Count\n1 Marabou 26\n2 Debona 14\n3 nnv 12\n4 NV.jl 5\n5 venus2 1\n19\n\nVNN-COMP 2021 Report\n5.3 Benchmark Scores\nThe results for the individual categories are shown below. For overall score, the tools which\nparticipated in all or almost all of the benchmarks did best. Within individual benchmarks,\nsome tools performed well despite not ranking high in the overall score. The results presented\nhere are for multi-overhead setup, with incorrect results scored using the odd-one-out strategy.\nAdjusting these parameters produced minor changes in the overall score and rankings, but was\nomitted for clarity. If these alternate scores are of interest, the discussion at the beginning of\nSection 5 outlines where to access the scripts used to compute scores.\nTable 11: Benchmark acasxu\n# Tool Veri\fed Falsi\fed Fastest Score Percent\n1 nnenum 138 47 155 1910 100.0%\n2 VeriNet 138 47 117 1852 97.0%\n3 Marabou 137 46 115 1809 94.7%\n4 oval 138 47 98 1794 93.9%\n5 venus2 138 46 94 1778 93.1%\n6\u000b,\f-CROWN 138 47 67 1732 90.7%\n7 ERAN 125 46 24 1506 78.8%\n8 Debona 84 42 39 1086 56.9%\n9 RPM 28 44 9 486 25.4%\n10 nnv 29 0 29 348 18.2%\n11 DNNF 0 41 12 182 9.5%\n12 randgen 0 28 0 28 1.5%\n13 NV.jl 45 9 0 -23 0%\nTable 12: Benchmark cifar10-resnet\n# Tool Veri\fed Falsi\fed Fastest Score Percent\n1\u000b,\f-CROWN 58 0 12 623 100.0%\n2 VeriNet 48 0 29 548 88.0%\n3 ERAN 43 0 36 502 80.6%\n4 Marabou 39 0 0 390 62.6%\n20\n\nVNN-COMP 2021 Report\nTable 13: Benchmark cifar2020\n# Tool Veri\fed Falsi\fed Fastest Score Percent\n1 oval 146 41 174 2209 100.0%\n2\u000b,\f-CROWN 148 42 43 1996 90.4%\n3 VeriNet 139 42 5 1822 82.5%\n4 ERAN 107 43 132 1749 79.2%\n5 nnenum 62 13 0 741 33.5%\n6 randgen 0 2 0 2 0.1%\n7 nnv 6 0 0 -140 0%\nTable 14: Benchmark eran\n# Tool Veri\fed Falsi\fed Fastest Score Percent\n1\u000b,\f-CROWN 60 1 26 670 100.0%\n2 VeriNet 48 1 49 588 87.8%\n3 ERAN 46 1 0 470 70.1%\n4 Debona 47 2 39 375 56.0%\n5 oval 21 0 18 247 36.9%\n6 Marabou 19 0 0 190 28.4%\n7 nnv 10 0 0 100 14.9%\nTable 15: Benchmark marabou-cifar10\n# Tool Veri\fed Falsi\fed Fastest Score Percent\n1\u000b,\f-CROWN 1 52 52 625 100.0%\n2 ERAN 0 52 51 613 98.1%\n3 oval 0 53 44 611 97.8%\n4 VeriNet 0 52 16 543 86.9%\n5 Marabou 0 29 0 281 45.0%\n6 DNNF 0 2 0 11 1.8%\n7 randgen 0 1 0 1 0.2%\n21\n\nVNN-COMP 2021 Report\nTable 16: Benchmark mnistfc\n# Tool Veri\fed Falsi\fed Fastest Score Percent\n1\u000b,\f-CROWN 49 21 35 772 100.0%\n2 VeriNet 39 21 57 716 92.7%\n3 Debona 37 22 47 688 89.1%\n4 oval 37 21 46 676 87.6%\n5 ERAN 34 22 47 654 84.7%\n6 Marabou 35 19 0 540 69.9%\n7 venus2 31 16 26 522 67.6%\n8 nnenum 35 12 21 512 66.3%\n9 nnv 27 0 8 186 24.1%\n10 DNNF 0 7 0 70 9.1%\nTable 17: Benchmark nn4sys\n# Tool Veri\fed Falsi\fed Fastest Score Percent\n1\u000b,\f-CROWN 70 5 73 878 100.0%\n2 VeriNet 68 3 0 719 81.9%\n3 ERAN 67 4 0 704 80.2%\n4 oval 56 4 0 589 67.1%\n5 NV.jl 41 2 0 422 48.1%\n6 venus2 34 1 0 250 28.5%\n7 DNNF 0 4 0 22 2.5%\n8 randgen 0 2 0 2 0.2%\n9 Debona 42 4 0 -722 0%\nTable 18: Benchmark oval21\n# Tool Veri\fed Falsi\fed Fastest Score Percent\n1 oval 12 2 11 164 100.0%\n2\u000b,\f-CROWN 12 2 2 146 89.0%\n3 VeriNet 11 2 6 145 88.4%\n4 ERAN 6 2 3 86 52.4%\n5 Marabou 4 2 1 63 38.4%\n6 nnenum 2 1 0 30 18.3%\n7 nnv 16 0 4 -31 0%\n22\n\nVNN-COMP 2021 Report\nTable 19: Benchmark verivital\n# Tool Veri\fed Falsi\fed Fastest Score Percent\n1\u000b,\f-CROWN 53 7 52 704 100.0%\n2 oval 51 7 57 694 98.6%\n3 ERAN 51 7 56 693 98.4%\n4 VeriNet 51 7 0 580 82.4%\n5 DNNF 0 1 0 10 1.4%\n6 nnv 53 0 0 -168 0%\n7 Marabou 27 7 0 -2260 0%\n23\n\nVNN-COMP 2021 Report\n6 Conclusion and Ideas for Future Competitions\nThis report summarizes the 2ndVeri\fcation of Neural Networks Competition (VNN-COMP)\nheld in 2021. Improvements to the competition structure have been made, including standard-\nization of common input formats ( onnx and vnnlib ), and common measurement hardware.\nBased on the common benchmarks (CIFAR2020 and ACASXU), tools exhibited signi\fcant\nprogress in terms of scalability and speed compared with previous years. The comparison is\nimperfect, as last year we did not have standardized hardware, so it is unclear how much of\nthe speed improvement is due to algorithmic improvements and how much is due to better\nhardware. Future editions of the competition may better judge the year-to-year improvements\nin neural network veri\fcation methods.\nThe benchmarks and tool execution scripts are openly available for others to replicate:\nhttps://github.com/stanleybak/vnncomp2021 . We hope this serves as a fair comparison\nfor evaluating future improvements to veri\fcation methods in upcoming publications. From\nan applicability perspective, having a common input format hopefully reduces the barriers of\nindustry participants to use the developed tools.\nIn future editions of VNN-COMP, some improvements we can make would be to allow more\ntypes of hardware. For example, any AWS EC2 machine could be chosen by the tool authors\nwith roughly the same cost ( $3 an hour this year). Multiple tool authors expressed that their\ntool could work faster if given both a strong CPU and GPU together. Alternatively, we could\nallow custom hardware per benchmark. This would likely require additional automation with\nthe competition measurements, especially dealing with installing license \fles. Some ideas for\nautomating this could be to have authors provide the Gurobi licences to use, rather than the\ncompetition organizers. Another improvement would be to improve overhead measurement, as\ndetailed in Section 2. Finally, we should standardize the counter-example format, so that the\nground truth for mismatched veri\fcation results can be provided.\nAcknowledgements\nThis competition (including the cost of AWS and the cash prizes) is supported by a gift from\nthe Lu Jin Family Foundation.\nThis research was supported in part by the Air Force Research Laboratory Information\nDirectorate, through the Air Force O\u000ece of Scienti\fc Research Summer Faculty Fellowship\nProgram, Contract Numbers FA8750-15-3-6003, FA9550-15-0001 and FA9550-20-F-0005. This\nmaterial is based upon work supported by the Air Force O\u000ece of Scienti\fc Research un-\nder award numbers FA9550-19-1-0288 and FA9550-21-1-0121, the National Science Founda-\ntion (NSF) under grant number FMitF 1918450, and the Defense Advanced Research Projects\nAgency (DARPA) Assured Autonomy program through contract number FA8750-18-C-0089.\nAny opinions, \fnding, and conclusions or recommendations expressed in this material are those\nof the author(s) and do not necessarily re\rect the views of the United States Air Force, DARPA,\nnor NSF.\nTool authors listed in section 3 participated in the preparation and review of this report.\nReferences\n[1] Guy Amir, Haoze Wu, Clark Barrett, and Guy Katz. An smt-based approach for verifying binarized\nneural networks. arXiv preprint arXiv:2011.02948 , 2020.\n24\n\nVNN-COMP 2021 Report\n[2] Ross Anderson, Joey Huchette, Will Ma, Christian Tjandraatmadja, and Juan Pablo Vielma.\nStrong mixed-integer programming formulations for trained neural networks. Mathematical Pro-\ngramming , 2020.\n[3] Stanley Bak. Execution-guided overapproximation (ego) for improving scalability of neural network\nveri\fcation, 2020.\n[4] Stanley Bak. nnenum: Veri\fcation of relu neural networks with optimized abstraction re\fnement.\nInNASA Formal Methods Symposium , pages 19{36. Springer, 2021.\n[5] Stanley Bak, Hoang-Dung Tran, Kerianne Hobbs, and Taylor T. Johnson. Improved geometric path\nenumeration for verifying ReLU neural networks. In 32nd International Conference on Computer-\nAided Veri\fcation (CAV) , July 2020.\n[6] E. Botoeva, P. Kouvaros, J. Kronqvist, A. Lomuscio, and R. Misener. E\u000ecient veri\fcation of\nneural networks via dependency analysis. In Proceedings of the 34th AAAI Conference on Arti\fcial\nIntelligence (AAAI20) . AAAI Press, 2020.\n[7] Christopher Brix and Thomas Noll. Debona: Decoupled boundary network analysis for tighter\nbounds and faster adversarial robustness proofs. CoRR , abs/2006.09040, 2020.\n[8] Rudy Bunel, Alessandro De Palma, Alban Desmaison, Krishnamurthy Dvijotham, Pushmeet\nKohli, Philip HS Torr, and M Pawan Kumar. Lagrangian decomposition for neural network\nveri\fcation. Conference on Uncertainty in Arti\fcial Intelligence , 2020.\n[9] Rudy Bunel, Jingyue Lu, Ilker Turkaslan, P Kohli, P Torr, and M Pawan Kumar. Branch and\nbound for piecewise linear neural network veri\fcation. Journal of Machine Learning Research ,\n21(2020), 2020.\n[10] Rudy Bunel, Ilker Turkaslan, Philip HS Torr, Pushmeet Kohli, and M Pawan Kumar. A uni\fed\nview of piecewise linear neural network veri\fcation. Advances in Neural Information Processing\nSystems , 2018.\n[11] Alessandro De Palma, Harkirat Singh Behl, Rudy Bunel, Philip H. S. Torr, and M. Pawan Kumar.\nScaling the convex barrier with active sets. In International Conference on Learning Representa-\ntions , 2021.\n[12] Alessandro De Palma, Harkirat Singh Behl, Rudy Bunel, Philip H. S. Torr, and M. Pawan Kumar.\nScaling the convex barrier with sparse dual algorithms. arXiv preprint arXiv:2101.05844 , 2021.\n[13] Alessandro De Palma, Rudy Bunel, Alban Desmaison, Krishnamurthy Dvijotham, Pushmeet\nKohli, Philip HS Torr, and M Pawan Kumar. Improved branch and bound for neural network\nveri\fcation via lagrangian decomposition. arXiv preprint arXiv:2104.06718 , 2021.\n[14] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li.\nBoosting adversarial attacks with momentum. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 9185{9193, 2018.\n[15] Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, and Pushmeet Kohli.\nA dual approach to scalable veri\fcation of deep networks. Conference on Uncertainty in Arti\fcial\nIntelligence , 2018.\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 770{778, 2016.\n[17] P. Henriksen and A. Lomuscio. E\u000ecient neural network veri\fcation via adaptive re\fnement\nand adversarial search. In Proceedings of the 24th European Conference on Arti\fcial Intelligence\n(ECAI20) , 2020.\n[18] P. Henriksen and A. Lomuscio. Deepsplit: An e\u000ecient splitting method for neural network veri-\n\fcation via indirect e\u000bect analysis. In Proceedings of the 30th International Joint Conference on\nArti\fcial Intelligence (IJCAI21) , To appear, August 2021.\n[19] Florian Jaeckle and M Pawan Kumar. Generating adversarial examples with graph neural net-\nworks. Conference on Uncertainty in Arti\fcial Intelligence , 2021.\n25\n\nVNN-COMP 2021 Report\n[20] Florian Jaeckle, Jingyue Lu, and M Pawan Kumar. Neural network branch-and-bound for neural\nnetwork veri\fcation. arXiv preprint arXiv:2107.12855 , 2021.\n[21] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An\ne\u000ecient smt solver for verifying deep neural networks. In International Conference on Computer\nAided Veri\fcation , pages 97{117. Springer, 2017.\n[22] Guy Katz, Derek A Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus, Rachel Lim, Parth\nShah, Shantanu Thakoor, Haoze Wu, Aleksandar Zelji\u0013 c, et al. The marabou framework for ver-\ni\fcation and analysis of deep neural networks. In International Conference on Computer Aided\nVeri\fcation , pages 443{452. Springer, 2019.\n[23] P. Kouvaros and A. Lomuscio. Towards scalable complete veri\fcation of relu neural networks\nvia dependency-based branching. In Proceedings of the 30th International Joint Conference on\nArti\fcial Intelligence (IJCAI21) , To Appear, 2021.\n[24] Tim Kraska, Alex Beutel, Ed H Chi, Je\u000brey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 International Conference on Management of Data ,\n2018.\n[25] Changliu Liu, Tomer Arnon, Christopher Lazarus, and Mykel J Kochenderfer. Neuralveri\fcation.jl:\nAlgorithms for verifying deep neural networks. In ICLR 2019 Debugging Machine Learning Models\nWorkshop , 2019.\n[26] Changliu Liu, Tomer Arnon, Christopher Lazarus, Christopher Strong, Clark Barrett, and Mykel J.\nKochenderfer. Algorithms for verifying deep neural networks. Foundations and Trends ®in\nOptimization , 4(3-4):244{404, 2021.\n[27] Jingyue Lu and M Pawan Kumar. Neural network branching for neural network veri\fcation. In\nInternational Conference on Learning Representations , 2020.\n[28] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.\nTowards deep learning models resistant to adversarial attacks. In Proc. International Conference\non Learning Representations (ICLR) , 2018.\n[29] Martin Vechev Mislav Balunovic. Adversarial training and provable defenses: Bridging the gap.\nInProc. International Conference on Learning Representations (ICLR) , 2020.\n[30] Mark Niklas M uller, Gleb Makarchuk, Gagandeep Singh, Markus P uschel, and Martin Vechev.\nPrima: Precise and general neural network certi\fcation via multi-neuron convex relaxations. arXiv\npreprint arXiv:2103.03638 , 2021.\n[31] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic di\u000berentiation in py-\ntorch. NIPS Autodi\u000b Workshop , 2017.\n[32] Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relax-\nation barrier to tight robustness veri\fcation of neural networks. Advances in Neural Information\nProcessing Systems , 32:9835{9846, 2019.\n[33] Fran\u0018 cois Serre, Christoph M uller, Gagandeep Singh, Markus P uschel, and Martin Vechev. Scaling\npolyhedral neural network veri\fcation on GPUs. In Proc. Machine Learning and Systems (MLSys) ,\n2021.\n[34] David Shriver, Sebastian G. Elbaum, and Matthew B. Dwyer. DNNV: A framework for deep\nneural network veri\fcation. In Alexandra Silva and K. Rustan M. Leino, editors, Computer\nAided Veri\fcation - 33rd International Conference, CAV 2021, Virtual Event, July 20-23, 2021,\nProceedings, Part I , volume 12759 of Lecture Notes in Computer Science , pages 137{150. Springer,\n2021.\n[35] David Shriver, Sebastian G. Elbaum, and Matthew B. Dwyer. Reducing DNN properties to enable\nfalsi\fcation with adversarial attacks. In 43rd IEEE/ACM International Conference on Software\nEngineering, ICSE 2021, Madrid, Spain, 22-30 May 2021 , pages 275{287. IEEE, 2021.\n[36] Gagandeep Singh, Rupanshu Ganvir, Markus P uschel, and Martin Vechev. Beyond the single neu-\nron convex barrier for neural network certi\fcation. In Advances in Neural Information Processing\n26\n\nVNN-COMP 2021 Report\nSystems 32 , pages 15098{15109. Curran Associates, Inc., 2019.\n[37] Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus P uschel, and Martin Vechev. Fast and\ne\u000bective robustness certi\fcation. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-\nBianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31 , pages\n10802{10813. Curran Associates, Inc., 2018.\n[38] Gagandeep Singh, Timon Gehr, Markus P uschel, and Martin Vechev. An abstract domain for\ncertifying neural networks. Proc. ACM Program. Lang. , 3(POPL):41:1{41:30, 2019.\n[39] Gagandeep Singh, Timon Gehr, Markus P uschel, and Martin Vechev. Boosting robustness cer-\nti\fcation of neural networks. In International Conference on Learning Representations (ICLR) .\n2019.\n[40] Gagandeep Singh, Markus P uschel, and Martin Vechev. Fast polyhedra abstract domain. In Proc.\nPrinciples of Programming Languages (POPL) , pages 46{59, 2017.\n[41] Mingxing Tan and Quoc Le. E\u000ecientnet: Rethinking model scaling for convolutional neural\nnetworks. In International Conference on Machine Learning , pages 6105{6114. PMLR, 2019.\n[42] Vincent Tjeng, Kai Y. Xiao, and Russ Tedrake. Evaluating robustness of neural networks with\nmixed integer programming. In ICLR , 2019.\n[43] Hoang-Dung Tran, Stanley Bak, Weiming Xiang, and Taylor T. Johnson. Veri\fcation of deep\nconvolutional neural networks using imagestars. In 32nd International Conference on Computer-\nAided Veri\fcation (CAV) . Springer, July 2020.\n[44] Hoang-Dung Tran, Patrick Musau, Diego Manzanas Lopez, Xiaodong Yang, Luan Viet Nguyen,\nWeiming Xiang, and Taylor T. Johnson. Parallelizable reachability analysis algorithms for feed-\nforward neural networks. In Proceedings of the 7th International Workshop on Formal Methods\nin Software Engineering (FormaliSE'19) , FormaliSE '19, pages 31{40, Piscataway, NJ, USA, May\n2019. IEEE Press.\n[45] Hoang-Dung Tran, Patrick Musau, Diego Manzanas Lopez, Xiaodong Yang, Luan Viet Nguyen,\nWeiming Xiang, and Taylor T. Johnson. Star-based reachability analysis for deep neural networks.\nIn23rd International Symposium on Formal Methods (FM'19) . Springer International Publishing,\nOctober 2019.\n[46] Hoang-Dung Tran, Xiaodong Yang, Diego Manzanas Lopez, Patrick Musau, Luan Viet Nguyen,\nWeiming Xiang, Stanley Bak, and Taylor T. Johnson. NNV: The neural network veri\fcation\ntool for deep neural networks and learning-enabled cyber-physical systems. In 32nd International\nConference on Computer-Aided Veri\fcation (CAV) , July 2020.\n[47] Joseph A. Vincent and Mac Schwager. Reachable polyhedral marching (rpm): A safety veri\fcation\nalgorithm for robotic systems with deep neural network components, 2021.\n[48] VNN-COMP. International veri\fcation of neural networks competition (VNN-COMP). Veri\fca-\ntion of Neural Networks workshop at the International Conference on Computer-Aided Veri\fcation ,\n2020.\n[49] Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and Zico Kolter.\nBeta-CROWN: E\u000ecient bound propagation with per-neuron split constraints for complete and\nincomplete neural network veri\fcation. arXiv preprint arXiv:2103.06624 , 2021.\n[50] Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer\nadversarial polytope. 2018.\n[51] Haoze Wu, Alex Ozdemir, Aleksandar Zeljic, Kyle Julian, Ahmed Irfan, Divya Gopinath, Sadjad\nFouladi, Guy Katz, Corina Pasareanu, and Clark Barrett. Parallelization techniques for verifying\nneural networks. In # PLACEHOLDER PARENT METADATA VALUE# , volume 1, pages 128{\n137. TU Wien Academic Press, 2020.\n[52] W. Xiang, H. Tran, and T. T. Johnson. Output reachable set estimation and veri\fcation for multi-\nlayer neural networks. IEEE Transactions on Neural Networks and Learning Systems , 29(11):5777{\n5783, 2018.\n[53] Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya\n27\n\nVNN-COMP 2021 Report\nKailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis for scalable certi\fed\nrobustness and beyond. Advances in Neural Information Processing Systems , 33, 2020.\n[54] Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, and Cho-Jui Hsieh. Fast\nand Complete: Enabling complete neural network veri\fcation with rapid and massively parallel\nincomplete veri\fers. In International Conference on Learning Representations , 2021.\n[55] Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. E\u000ecient neural\nnetwork robustness certi\fcation with general activation functions. Advances in Neural Information\nProcessing Systems , 31:4939{4948, 2018.\n28",
  "textLength": 65501
}