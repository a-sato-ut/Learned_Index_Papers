{
  "paperId": "3f3b35fa0b8b942bcae7c755603ce56de616da93",
  "title": "Deep learning modelling techniques: current progress, applications, advantages, and challenges",
  "pdfPath": "3f3b35fa0b8b942bcae7c755603ce56de616da93.pdf",
  "text": "Vol.:(0123456789)Artificial Intelligence Review (2023) 56:13521–13617\nhttps://doi.org/10.1007/s10462-023-10466-8\n1 3\nDeep learning modelling techniques: current progress, \napplications, advantages, and challenges\nShams Forruque Ahmed1 · Md. Sakib Bin Alam2 · Maruf Hassan1 · \nMahtabin Rodela Rozbu3 · Taoseef Ishtiak4 · Nazifa Rafa5 · M. Mofijur6,7 · \nA. B. M. Shawkat Ali8,9 · Amir H. Gandomi10,11 \nPublished online: 17 April 2023 \n© The Author(s) 2023\nAbstract\nDeep learning (DL) is revolutionizing evidence-based decision-making techniques that can \nbe applied across various sectors. Specifically, it possesses the ability to utilize two or more \nlevels of non-linear feature transformation of the given data via representation learning in \norder to overcome limitations posed by large datasets. As a multidisciplinary field that is \nstill in its nascent phase, articles that survey DL architectures encompassing the full scope \nof the field are rather limited. Thus, this paper comprehensively reviews the state-of-art DL \nmodelling techniques and provides insights into their advantages and challenges. It was \nfound that many of the models exhibit a highly domain-specific efficiency and could be \ntrained by two or more methods. However, training DL models can be very time-consum-\ning, expensive, and requires huge samples for better accuracy. Since DL is also suscepti-\nble to deception and misclassification and tends to get stuck on local minima, improved \noptimization of parameters is required to create more robust models. Regardless, DL has \nalready been leading to groundbreaking results in the healthcare, education, security, com-\nmercial, industrial, as well as government sectors. Some models, like the convolutional \nneural network (CNN), generative adversarial networks (GAN), recurrent neural network \n(RNN), recursive neural networks, and autoencoders, are frequently used, while the poten-\ntial of other models remains widely unexplored. Pertinently, hybrid conventional DL archi-\ntectures have the capacity to overcome the challenges experienced by conventional models. \nConsidering that capsule architectures may dominate future DL models, this work aimed to \ncompile information for stakeholders involved in the development and use of DL models in \nthe contemporary world.\nKeywords Deep learning · Deep learning architecture · Neural network · Boltzmann \nmachine · Deep belief network · Autoencoders\n * Shams Forruque Ahmed \n shams.ahmed@auw.edu.bd; shams.f.ahmed@gmail.com\n * Amir H. Gandomi \n gandomi@uts.edu.au\nExtended author information available on the last page of the article\n\n13522 S. F. Ahmed et al.\n1 3\n1 Introduction\nDeveloping machines with the ability to ‘think’ has been a long-running aspiration of \ninventors throughout history. The popular idea of replicating intelligent human behavior \narranged as processes in machines (Dick 2019) has fueled researchers’ imaginations. In the \npresent time, artificial intelligence (AI) is a thriving and rapidly changing field with vari-\nous applications in society and economics, such as understanding speech or images, textual \nanalysis, and in supporting an actively growing research body (Lu et al. 2018). Machine \nlearning (ML), a part of AI, is a multidisciplinary field spanning computer science, sta-\ntistics, and data science that addresses the need for computers to improve automatically \nthrough experience and by the use of data (Jordan and Mitchell 2015). ML is advancing \nevidence-based decision-making in the fields of healthcare, education, national security, \nfinance, economics, manufacturing, and marketing (Jordan and Mitchell, 2015), specifi-\ncally by implementing various approaches to teach computers to achieve tasks. However, \nconventional ML techniques cannot efficiently process raw data and require mindful engi-\nneering and great expertise (Lecun et al. 2015). In the real world, every piece of data may \nbe influenced by different factors of variations, thus requiring humans to factor in those \nvariations and decide whether to incorporate them or not. Overcoming such flaws, deep \nlearning (DL) has recently emerged as a promising approach in ML (Lecun et al. 2015), \ncurrently dominating the majority of the works in the field of ML (Alpaydin 2020).\nWhile it may appear as a seemingly new concept, the idea of DL can be traced back to \nthe 1940s and subsequently underwent roughly three waves of development with the most \nrecent current revival beginning in 2006 (Goodfellow et al. 2016). During the first wave \nbetween 1940 and 1960, DL was known as cybernetics, then it gained popularity again in \nthe 1980s-1990s as connectionism. Fundamental methods such as radial basis function net-\nworks and multilayer perceptrons were employed in 2014 to solve the problem of design-\ning mobile adaptive tracking controllers (Tzafestas 2014). These two neural networks were \nfound suitable for decision-making and control. Later, Sengupta et  al. (Sengupta et  al. \n2020) pointed out a few reasons why DL rose to prominence in the twenty-first century, \nincluding the surge of “big data” with quality labels, improvements in regularization tech-\nniques, development of near-perfect optimization algorithms, creation of niche software \nplatforms that can enable the integration of architectures, and advancements in paral-\nlel computing power and multi-core, multi-threaded execution. In fact, big data became \na huge issue for conventional ML algorithms along with the increasing size of the net-\nwork, whereby the performance of old algorithms either became overloaded or deteriorated \n(Khamparia and Singh 2019). The enhanced performance of DL can be attributed to its \nability to utilize two or more levels of non-linear feature transformation of the given data \n(Zeiler and Fergus 2014).\nDeep learning allows computational models with multiple layers to gradually extract \nhigher-level features from the raw input (Alpaydin 2020; Deng and Yu 2014). The “deep” \nin DL, therefore, denotes a high credit assignment path (CAP) depth, which has been \nassigned a value of 2 by most researchers (Sugiyama 2019; Telikani et al. 2021; Kashyap \net al. 2021; Mousavi and Gandomi 2021; Tahmassebi et al. 2018a, b, 2019, 2020; Jayara-\nman et  al. 2020; Kumar et  al. 2019). Deep learning enables computers to learn com-\nplex concepts by forming them out of simple ones. Goodfellow et al. (2016) adequately \nexplained that, “Deep learning is a particular kind of machine learning that achieves great \npower and flexibility by learning to represent the world as a nested hierarchy of concepts, \nwith each concept defined in relation to simpler concepts, and more abstract representations \n\n13523\nDeep learning modelling techniques: current progress,…\n1 3\ncomputed in terms of less abstract ones” (Goodfellow et al. 2016). DL is primarily based \non artificial neural networks, a type of computing system roughly mimicking the biological \nneural networks of animal brains (Chen et al. 2019), and may employ supervised, unsuper -\nvised, or semi-supervised representation learning (Bengio et al. 2013; Lecun et al. 2015; \nSchmidhuber 2015). Representation learning, also known as feature learning, sets DL apart \nfrom other techniques in ML. Unlike manual feature engineering, feature learning enables \ncomputers to spontaneously find the representations required for the classifications from \nraw data (Bengio et al. 2013). DL, therefore, relies on very little hand-tuning and has the \nability to analyze the rapidly increasing computations and data. The requirement for man-\nual engineering is only restricted to operations, such as altering the numbers and sizes of \nlayers, to yield different degrees of abstraction (Bengio et al. 2013; Lecun et al. 2015).\nThe applications of DL span various disciplines and sectors. To begin with, DL has \nexhibited remarkable performance in image recognition (Carrio et  al. 2017; Krizhevsky \nand Hinton 2017; Lecun et al. 2015; Szegedy et al. 2015; Tompson et al. 2014; Wei et al. \n2019), displayed potential in image restoration (Schmidt 2014), and demonstrated ground-\nbreaking results in speech recognition (Cireşan et al. 2012; Deoras et al. 2011; Hinton et al. \n2012; Lecun et al. 2015; Sainath et al. 2013). It is currently used in the speech recognition \nsystems of major day-to-day products (Case et al. 2014; Deng and Yu 2014; Lemley et al. \n2017) as well as in the operation of unmanned vehicles (Carrio et al. 2017). The area of \nlanguage processing has also been harnessing the benefits of DL (Deng and Yu 2014), in \nwhich DL contributes to natural language understanding and translation (Collobert et al. \n2011; Mesnil et al. 2015; Sutskever et al. 2014), query response (Bordes et al. 2014), sen-\ntiment analysis, text classification, information recovery (Huang et  al. 2013; Shen et  al. \n2014), and writing style recognition (Brocardo et al. 2017), just to name a few. DL has \nalso been revolutionizing the health sector (Miotto et al. 2018), particularly yielding far-\nreaching implications for drug discovery and design and in effectively predicting interac-\ntions of potential drugs with molecules of interest (Ma et al. 2015). DL’s ability to acquire \nend-to-end learning models from complex, unstructured, diverse, and poorly annotated \ndata has also led to advancements in biomedical research (Collobert et al. 2011; Naylor \n2018; Ravì et  al. 2017). With its high image recognition skills, DL has been applied in \nclinical imaging, such as neuroimaging (Sui et al. 2020), and has shown great promise in \nthe identification and detection of lesions, cancer cells, and different organs, as well as in \nimage enhancement (Cao et  al. 2019; Litjens et  al. 2017; Wieslander et  al. 2017). Bio-\ninformatics has also applied DL for predicting gene ontology annotations, understanding \nthe functions of different genes (Chicco et al. 2014), and most importantly for anticipating \nhow mutations in non-coding DNA affect gene expressions and susceptibility to diseases \n(Leung et al. 2014; Xiong et al. 2016).\nDL’s applications range far beyond science. For example, the military has taken advan-\ntage of the highly efficient image and object recognition ability of DL for various opera-\ntions (Mendis et al. 2016; Yang et al. 2018). Businesses have applied DL for improving \ntheir customer relationship management, where it allows for the estimation of the customer \nlifetime value that would result from possible direct marketing activities (Tkachenko 2015). \nThe recommendation systems in various commercial products utilize DL to understand and \npredict user preferences (Da’u and Salim 2020; Feng et al. 2019; Oord et al. 2013). Simi-\nlarly, it has been also used in targeting an appropriate audience for mobile advertisements \n(De et al. 2017). Furthermore, DL utilizes both supervised and unsupervised learning in \nfinancial fraud detection and anti-money laundering by identifying anomalies and abnor -\nmal money transactions (Paula et al. 2016). While DL has been helping to advance several \nfields of research, society, and the economy, it can also be exploited for malicious attempts. \n\n13524 S. F. Ahmed et al.\n1 3\nFor instance, DL has been drawing criticisms for compromising cybersecurity as it is sus-\nceptible to attacks by hackers and to deceit (Li et al. 2019a, b, c, d; Norton and Qi 2017; \nPapernot et al. 2016). Nevertheless, DL modelling architectures suffer from some errors; \nin several instances, DL was found to misclassify or randomly classify images (Nguyen \net al. 2015; Szegedy et al. 2015). To tackle these issues, it is pertinent to design models that \ninternally create states that are equivalent to image-grammar (Zhu and Mumford 2006). In \naddition, Mühlhoff (2020) has argued that despite its much-extolled advantage of requiring \nminimal hand-tuning, DL, in fact, relies on microwork by humans, thereby calling it “a \nform of distributed orchestration of human cognition through networked media technol-\nogy” (Mühlhoff 2020).\nThe uses of DL technologies in the contemporary world and their potential for further \napplications cannot be disregarded, despite some limitations. Many scholarly works have \nbeen undertaken to comprehensively review the applications of DL technologies across \ndifferent sectors. Most review works focus on specific areas and implementations of DL \n(Arulkumaran et al. 2017; Gheisari et al. 2017; Pouyanfar et al. 2018; Vargas et al. 2017). \nOther reviews have surveyed DL architectures and algorithms in the context of specific \napplications, such as speech emotion recognition (Fayek et al. 2017; Pandey et al. 2019), \ntext classification (Zulqarnain et  al. 2020), early diagnosis of Alzheimer’s (Ortiz et  al. \n2016), electronic health records (Roberto et  al. 2020; Xiao et  al. 2018), medical image \nanalysis (Akkus et al. 2017; Cao et al. 2019; Liu et al. 2019; Shoeibi et al. 2020), time \nseries forecasting (Lara-ben and Carranza-garc 2021), aircraft maintenance, repair, and \noverhaul (Rengasamy et al. 2018), and land cover mapping (Pashaei and Kamangir 2020). \nWith DL having gained momentum only recently, review articles on DL architectures \nencompassing the full scope of the field are still lacking. Dixit et al. (Dixit et al. 2018) \nprovided a brief overview of seven of the most widely used DL architectures (deep neu-\nral networks, deep belief networks, recurrent neural networks, deep Boltzmann machine, \nrestricted Boltzmann machine, deep autoencoders, and convolutional neural networks), a \nlist of DL libraries, and some of the most common applications. However, as is perceiv -\nable, their paper is not a comprehensive review of existing architectures. In addition to the \nmodels discussed by Dixit et  al. (2018), Sengupta et  al. (2020) have covered generative \nadversarial neural networks and highlighted tests that can be undertaken before implement-\ning different neural networks in safety–critical systems. Shrestha et al. (Shrestha 2019) pro-\nvided a rigorous overview of the neural networks and DNNs and found certain limitations \nthat constrain training, such as overfitting, long training time, and high susceptibility to \ngetting stuck in the local minima.\nKhamparia and Singh (2019) contributed perhaps one of the most important studies \non DL architectures, even though it is limited to neural networks. Their meta-analysis \ncritically reviewed twelve DL modelling techniques and found that advanced DL archi-\ntectures that are combinations of a few conventional architectures are far more robust \nthan their conventional counterparts. A comprehensive list of DL architectures and \ntheir related applications was also presented. Nevertheless, as a continuously expand-\ning and developing field, there is a need to critically review and compile information \non the state-of-art DL modelling techniques. Therefore, by first delving into a brief \ndiscussion on DL as a subset of ML, this paper comprehensively reviews all of the \navailable DL modelling techniques. While these modelling techniques have many ben-\nefits across multiple disciplines, they are not without limitations. Therefore, this paper \nalso highlights the advantages and drawbacks of these models and concludes with \nfuture perspectives on DL models, providing directions for enhancing the architecture \ndesigns and increasing the implementation of DL technologies across more sectors. \n\n13525\nDeep learning modelling techniques: current progress,…\n1 3\nOverall, this paper aims to disseminate essential information on the constantly evolv -\ning field of deep learning and direct future research towards improving existing model-\nling techniques.\n2  Methodology for selecting, collecting, and analyzing pertinent data\nThis review utilized an integrative literature method to analyze almost all available \ndeep learning modelling approaches, as well as their current progress, applications, \nadvantages, and challenges. Throughout this method, relevant and reliable papers were \nselected, collected, filtered, carefully evaluated and analyzed. The database was found \nusing credible websites, e.g. Scopus and refereed journals from reputable publish-\ners such as Nature, Elsevier, Taylor & Francis, Springer, Wiley, ACS, Inderscience, \nMDPI, Frontiers, and Sage. Relevant keywords such as “Deep learning”, “Deep learn-\ning architecture”, “Deep learning modelling”, “Advantages of deep learning”, “Chal-\nlenges of deep learning”, “Future of deep learning”, and each deep learning model \nsuch as “Vector space model”, “Convolutional neural network”, “Recurrent neural \nnetwork” and so on, were used to find out publications related to the present work. \nThrough the Scopus database, 186,154 papers published within the last five years were \nidentified. The references and bibliographies of the aforementioned publications were \nsifted and compiled in order to locate more relevant papers. The following criteria \nwere used to thoroughly scan and categorize the abstract, introduction, and conclusion \nfrom selected papers:\n (i) Preliminary consideration was given to only peer-reviewed articles from reputable \npublishers and websites\n (ii) Researchers who are actively engaged in the relevant research field were chosen and \ncollected\n (iii) The selected publications were evaluated for their balance between modern studies \nand prior research\n (iv) Referring to websites that employ the aforementioned keywords commercially\n (v) The most recent and cutting-edge algorithms relevant to the present work were \nemphasised\n (vi) Some publications of relevance that were cited in recent studies were rigorously \nretrieved as the original source of the studies.\nThe above criteria assisted in selecting 748 papers that are more relevant. Through-\nout the entire review process of available relevant papers, several questions were \nraised. To answer these questions, some other references were sourced and examined \nfor further clarification and improvement. Papers for the present study were chosen \nbased on a set of inclusion and exclusion criteria, which are illustrated in Table  1. A \ntotal of 419 articles were finally selected by applying the exclusion criteria. Although \nthe exclusion criteria appeared to provide a solid foundation to find peer-reviewed \nand high-quality academic articles, some of the characteristics of the exclusion crite-\nria appeared to be biased and skewed, making it difficult to discover high-quality aca-\ndemic journals. The authors conducted a test–retest procedure to overcome this issue.\n\n13526 S. F. Ahmed et al.\n1 3\n3  Deep learning\nDeep learning (DL) is considered an evolution of machine learning (ML) that incorpo-\nrates algorithms to learn from data to accomplish some tasks without being explicitly \nprogrammed (Lecun et al. 2015). Both ML and DL are a subset of artificial intelligence \n(AI). ML powers a wide range of automated functions in various businesses, from data \nsecurity services hunting down malware to finance specialists looking for trade warn-\nings. It has also a wide variety of applications in modern society, such as: developing \nintelligent personal assistants for finding helpful information (Dhyani and Kumar 2019), \nrecommender systems that can suggest relevant items to the users (Zhang et al. 2019), \nmachine translation to provide the most accurate translation of any text in any language \n(Poliak et al. 2018), and predicting the class of object in an image (Chen et al. 2018a; \nb). The way machines can learn new techniques becomes interesting whenever deep \nlearning techniques are employed. The effectiveness of traditional machine-learning \napproaches is comparatively lower than DL techniques, as illustrated in Fig.  1, consid-\nering that they require a large volume of data to provide significant results. For a long \ntime, designing a feature extractor for machine learning systems demanded hand-crafted \nfeatures to simplify the learning process. However, such feature extraction techniques \nneed human expertise and significant domain understanding.Table 1  Inclusion and exclusion criteria to select papers for the present review\nInclusion criteria Exclusion criteria\nThe publications chosen are all academic and peer-\nreviewedEven if they are academic or peer-reviewed, publica-\ntions lacking a robust discussion are not included\nThe publications should be pertinent to the topic of \nthe present studyPublications that lack information about the afore-\nmentioned keywords are not selected\nThe papers should be capable of answering the \nresearch questionsThe results of any literature that showed a high level \nof repetition were filtered out\nAny additional information that appears pertinent \nand valuable is also selectedAny literature with insufficient references and con-\ntexts was not considered\nFig. 1  Performance of deep \nlearning against traditional learn-\ning (Ng 2015)\n\n\n13527\nDeep learning modelling techniques: current progress,…\n1 3\nDeep learning allows machines to learn from their mistakes and comprehend the world \nas a hierarchy of concepts. In this learning process, the machines learn from data using \na general-purpose learning algorithm, thus needing less human expertise to describe all \nthe knowledge that the machine requires expressly. The models of DL employ a layered \nnetwork architecture, known as an artificial neural network (ANN) (Schmidhuber 2015), \nwhich is modelled after the human brain’s analogous networks. The embedding of lay -\ners results in a significantly more efficient learning experience than traditional machine \nlearning models. The ability of deep learning to achieve high-level features from a mas-\nsive amount of input data, referred to as feature engineering, distinguishes it from machine \nlearning. As a result, deep learning is gaining popularity with innovative applications in \nnatural language processing (NLP), computer vision, and predictive modelling (Ahmad \net al. 2019).\n4  Deep learning modelling techniques\nDeep learning modelling techniques enable computational models to learn feature \nrepresentation in data using multiple processing layers and several levels of abstrac-\ntion (Lecun et al. 2015). Artificial neural networks (ANNs) provide the foundation of \nadvanced deep learning models (Schmidhuber 2015) and perform well in a variety of \ndomains. However, ANNs suffer from certain drawbacks, such as no guaranteed con-\nvergence to an optimal solution and being prone to overfitting the training data. There-\nfore, researchers have tried to find solutions using deep architecture. The term “deep” \nin “deep learning” was motivated by the number of processing layers through which \nthe data must pass in the network. A deep learning model is made up of multiple layers \nthat stack up on top of each other (Fig.  2). The first layer (input) consists of units con-\ntaining values fed to every neuron in the first hidden layer, then the predicted results \ncome out of the model from the output (final) layer. The number of units in this layer \nequals the number of output classes desired. The hidden layers placed between the \ninput and output layers apply weights to the inputs and pass them through an activa-\ntion function. The activation function is used to help the network add non-linearity and \nlearn complex relationships in the data. The backpropagation algorithm computes the \nFig. 2  a Conventional neural \nnetwork b Deep learning neural \nnetwork (Oka et al. 2021)\n\n\n13528 S. F. Ahmed et al.\n1 3\nerror between the predicted result and the desired class in the output layer, then pro-\nceeds to the hidden layer to reduce the loss by adjusting the weights. This process is \nrepeated until the output is accurate enough to be useful.\nConsidering the concepts of neural networks discussed above, several deep learn-\ning modelling techniques are built as described in the following subsections. These \ntechniques have various applications, such as the detection, classification of objects \nin images and video data (Lea et al. 2016), finding sentiment and emotion from text \ndata (Jin Wang et  al. 2016a; Hassan et  al. 2018; Majumder et  al. 2019), audio pro-\ncessing applications like speech recognition (Rao et al. 2018a; b), and neural machine \ntranslation (NMT) with translation between different languages (Sutskever et al. 2014). \nDeveloping a deep learning-based model in these fields requires the pre-processing \nof raw data, feature selection, optimal parameter determination, and the evaluation of \nclassification accuracy and convergence speed. This section covers different types of \ndeep learning modelling approaches and explains their underlying mathematical con-\ncepts, advancements, latest implementations, and applications in various fields.\n4.1  Vector space model\nThe vector space model (VSM) is an arithmetic model in which texts are represented as \nvectors. It has been successfully applied in information filtering, information retrieval, \nand other areas (Abualigah and Hanandeh, 2015; Van Gysel et  al. 2018; Mitra and \nCraswell 2017). The vector elements describe the weights or importance of every word \nin a document. The cosine similarity technique can be applied to find the degree of \nsimilarity between two documents (Günther et  al. 2016). In the vector space model \nshown below, documents are described as a term-document matrix (Shi et al. 2018) or \na term-frequency matrix, where the rows represent the documents and the terms are \ndefined by the columns. Words, sentences, or phrases are often used as terms, each of \nwhich depends on the application and context. Each cell signifies the term’s weight in \na document, and if a term is present in the document, the cell value will be non-zero.\nSuppose there is a document Dk and a query q . The cosine similarity formula can be \nused to find the similarity between Dk and q using the formula:\nThe query and document vectors are not correlated if the cosine value gives zero in \nEq. (1 ). The vector space model assumes that the terms are independent of each other. \nAs a result, the model ignores the possibility of semantically related index terms.⎡\n⎢\n⎢\n⎢\n⎢\n⎢⎣D1\nD2\n⋯\nDnT1\nw11\nw12\n⋯\nw1nT2…Tt\nw21…wt1\nw22…w11\n⋯⋯⋯\nw2n⋯wtn⎤\n⎥\n⎥\n⎥\n⎥\n⎥⎦\n(1)/u1D450/u1D45C/u1D460�Dk,q�=∑N\ni=1wi,jwi,q�∑N\ni=1wi,j2�∑N\ni=1wi,q2\n\n13529\nDeep learning modelling techniques: current progress,…\n1 3\n4.1.1  Word embedding\nIn recent years, research interest in the concept of using a vector representation of words \nand word embedding has increasingly progressed. The latter has been often utilized in \nadvanced natural language processing applications, such as information retrieval, question \nanswering (Zhou et  al. 2016), and machine translation (Zhang et  al. 2017a; b, c). Word \nembedding is a method of generating vectors and mapping them to associated words. \nTomas Mikolov’s word2vec (Mikolov et al. 2013) models can generate high-dimensional \nvector representations of words when training on a large text dataset (Demeester et  al. \n2016). These vectors are capable of capturing syntactic and semantic information. In its \nsimplest form, a word2vec model involves the training of a simple neural network to com-\nplete a task and includes only one hidden layer in the neural network, as shown in Fig.. \nThe goal is to simply learn the hidden layers’ weights, which are used as word vectors in \nmany applications (Zhang et al. 2015). The size of the input layer depends on the number \nof words in the vocabulary for training, where one neuron represents one word. The hidden \nlayer size is defined by how many dimensions we want to keep in the resulting word vec-\ntors. It is suggested that the dimensionality of the vectors be set between 100 and 1000 in \nthe original model (Demeester et al. 2016). Higher dimensionality provides high quality of \nword embedding, while the output layer has the same size as the input layer.\nTo train the embedded weights, the continuous bag of words (CBOW) and skip-gram \nare two useful techniques. Given a target word, the skip-gram model attempts to predict \nalternative context words. Here, input to hidden layer connections remains the same as the \nword2vec fully connected network. However, a simple modification is made in the hid-\nden to output layer connection to give space for the selected number of context words. \nContrariwise, the CBOW model aims to predict target words given a set of context words, \nthe number of which depends on the setting of the window size. For example, in the sen-\ntence, “the quick brown fox jumped over the lazy dog.”, ‘the’ and ‘brown’ might be used \nas context words and ‘quick’ as the target word. A tweak to the neural network architecture \nis required in this scenario as is a simple modification to adjust the input to hidden layer \nconnection C times. Here, C is the number of context words. By adding these configura-\ntions to the network, the hidden layer’s output can be found by taking the mean of the \ncontext words. The steps after calculating the hidden layer remain precisely the same. A \ntext classification system was proposed by Ali et  al. (2019) for retrieving transportation \nsentiment from social networking and news sites. The authors combined a topic2vec and \nword2vec to create a word embedding model that describes the documents using a low \ndimensional vector but keeps the semantic meanings. The model obtains a sentiment clas-\nsification accuracy of 93% with transportation datasets, outperforming topic2vec document \nrepresentation approaches. The model treats the unimportant words as sentiment words \nthat cause decreasing classification performance. However, sophisticated data pre-process-\ning is needed to improve classification accuracy (see Figs. 2, 3).\n4.1.2  Sentence embedding\nThe sentence embedding model aims to produce a fixed-length continuous vector repre-\nsenting the entire input. A rough sense of the relative locations of the sentence vectors \nin the original vector space can be obtained from the figure. Similar sentences are close \ntogether in summary-vector space. Skip-thought is one of the popular sentence embedding \n\n13530 S. F. Ahmed et al.\n1 3\nmodels that demonstrates significant results in several tasks, including semantic similarity, \nparaphrase detection, image annotation (how well the sentences describe an image), and \nclassifications (Kiros et al. 2015).\nVector representation, which is used for words, phrases, sentences, paragraphs, docu-\nments, or even images, can be generalized as representing “thoughts.” On the other hand, \nthe skip-thought model abstracts skip-gram architecture to the sentence level (Kiros et al. \n2015). The idea behind this model is that the context words embed a word’s meaning. The \nmodel tries to map sentences with common syntactic and semantic information into simi-\nlar vectors by reconstructing the neighbouring sentence. The skip-thought model has three \nmain parts: encoder, previous decoder, and next decoder, as shown in Fig. 4.\nIn Fig.  4, given a sentence si at index i , the encoder produces a fixed-length representa-\ntion zi . It needs to access the word embedding layer (also called the lookup table layer) that \nmaps each word into a corresponding vector. Inside an encoder, a recurrent neural network \n(RNN) with the gated recurrent unit (GRU) or long short-term memory (LSTM) activation \nis fed every word sequentially in a sentence. This encoder captures the temporal patterns \nof sequential word vectors. The previous decoder takes the embedding zi from the encoder \nand “tries” to generate the proceeding sentence si−1 . This decoder uses another recurrent \nnetwork that generates the sentence sequentially and shares the same lookup table layer \nfrom the encoder. The next decoder takes the embedding zi from the encoder and “tries” to \ngenerate the subsequent sentence si+1 . This decoder also uses a recurrent network similar to \nthe previous decoder. The encoder is the end result of the skip-thought model as it contains \nsyntactic and semantic information.\nFig. 3  Illustration of word2vec fully connected neural network (Orkphol and Yang 2019)\nFig. 4  Skip-thought model overview (Hassan et al. 2018)\n\n13531\nDeep learning modelling techniques: current progress,…\n1 3\nDue to the vast amount of textual data surfacing online, the demand for text summa-\nrization is continuously increasing worldwide. As a result, the necessity of natural lan-\nguage processing (NLP) models arises to extract the essential and valuable information \nfrom the long text while maintaining critical information. Mohd et al. (2020) introduced \na text summarizer that obtains the features of a long text document using different tech-\nniques, such as Latent Dirichlet Allocation (LDA) and Term Frequency-Inverse Docu-\nment Frequency (TF-IDF), which represents each sentence as a numerical vector. Simi-\nlar vectors are aggregated together using a genetic algorithm. Lastly, the LDA technique \nwas utilized to obtain the center sentence of each cluster to be included in the resulting \nsummary. The macro-average of precision from the experimental results was found to \nbe 34%, which is higher than the benchmark standard. However, the technique was per -\nformed on only one dataset, and thus the precision may not be feasible.\nDifferent types of difficulties, such as combining syntactic information or identi-\nfying different labels for the document classification task, are acknowledged using \nDocBERT. The DocBERT is a document classification model based on Bidirectional \nEncoder Representations from Transformers (BERT) (Adhikari et al. 2019). The gen-\neral idea is to use a fully connected layer to filter the representation obtained from the \ncommon language specification (CLS) token and then employ a SoftMax layer to con-\nvert 768-dimensional encoding to class distribution. Adhikari et  al. (2019) reported \nthe state-of-the-art results on four popular datasets, attempting to address the BERT \nmodel’s high computational expense and reduce the parameters by 30-fold. The average \ndocument length was found to be less than BERT, while the maximum length was 512. \nHowever, BERT can outperform non-contextual embeddings on various tasks, such as \nthe clinical domain. Si et al. (2019) explored the performance of classic word embed-\nding approaches (word2vec, GloVe) and contextualized methods (BERT) on a clinical \nconcept extraction task. The output of the BERT model was fed into a bi-LSTM, which \nshowed that contextual embeddings play a significant role in achieving better perfor -\nmance (F1-measures of 93.18) on various benchmark tests in the datasets like SemEval.\n4.1.3  Graph embedding\nGraph embedding is a technique for transforming a whole graph into a single vector \nwhile preserving the graph’s relevant information. The resulting vectors contain highly \ninformative features that can be used for the task, such as node classification, ranking, \nalignment, link prediction, clustering, and visualization. The primary goal of graph \nembedding techniques is to reflect high-dimensional points into a residual continuous \nvector space with low dimensions (Fig.  5). As a result, it is easy to compute the node \nsimilarity using the dot product or cosine distance formula. Graph analytics is also con-\nsiderably faster and more accurate than computing in the high-dimensional complex \ngraph domain.\nAlthough matrix-factorization approaches have been proposed to represent a node \nearlier, they are significantly affected by conventional dimension reduction techniques. \nComparatively, recent techniques focus on learning node embeddings using random \nwalk characteristics. A graph structure can be translated into a sample collection of lin-\near sequences using the DeepWalk model (Perozzi et al. 2014), which employs hierar -\nchical SoftMax techniques as the loss function. The primary concept underlying this \nmethod is to learn embeddings, and therefore (Hamilton et al. 2017):\n\n13532 S. F. Ahmed et al.\n1 3\nwhere Pg,T(vj/uni007C.varvi) denotes the probability of visiting from vertex vi to vj on a length-T \n;andDEC/parenleft.s1zi,zj/parenright.s1\n is a function that takes the node embeddings zi and zj and uses them to \ndecode the graph metrics.\nA hypergraph embedding method, LBSN2Vec, was developed by Yang et al. (2019) for \nlocation-based social network (LBSN) data that enhances friendship and location predic-\ntion task effectiveness. LBSN provides services to the users to publish their location and \nlocation-related contents like photos or notes. Encoding both users and places into low-\ndimensional vectors produces hyperedges by sampling friendships and checking-in using \na random walk. The model chooses two nodes from the sample graph and then feeds the \nnodes into a model similar to skip-gram to generate low-dimensional vectors represent-\ning the nodes. The authors revealed that the LBSN2Vec model outperforms the baseline \ngraph embeddings in predicting the friendship of two individuals and location predic-\ntion by 32.95% and 25.32%, respectively. However, the study was limited to random walk \napproaches for the location prediction task in the hypergraph. Further research is thus \nrequired to take advantage of the meta-graph or hypergraph for the deep learning-based \nrecommendation model.\n4.2  Convolutional neural network\nConvolutional neural networks (CNNs) are particularly useful to reduce the number of \nparameters in an ANN. This has inspired researchers and practitioners to consider adopt-\ning larger models to accomplish tasks that were previously difficult to handle with regular \nANNs. The CNN model is influenced by an animal’s visual cortex and is intended to learn \nlow-level to high-level features from the data received gradually. For example, the model \nfirst detects the low-level edge in the first layer in the image classification task and then the \nhigh-level features like shapes and faces in an image (see Fig. 5).(2)DEC�zi,zj�≜ezT\nizj\n∑\nvk∈VezT\nizk≈Pg,T(vj�vi)\nFig. 5  Graph embedding (Xu 2020)\n\n13533\nDeep learning modelling techniques: current progress,…\n1 3\nTo understand the architecture of CNN, we explain the essential CNN model compo-\nnents. A CNN model is comprised of three primary layers: convolution, pooling, and fully \nconnected layers. The first two layers generate features from the input, while the third layer, \nthe fully connected layer, connects the extracted features to the final output. The convo-\nlution layers retrieve the high-level characteristics from the data provided. The primary \nobjective is to compute different feature maps by projecting a tiny array of numbers called \na \"kernel\" to the input data. The input is also known as a tensor. An element-wise product \nbetween each kernel element and the input tensor is performed at each position of the ten-\nsor. Then, the summation of these values is calculated and applied to the associated index \nof the output tensor (Fig.  6). Multiple kernels are used to repeat this process to produce an \narbitrary number of feature maps. Each feature map represents distinct input tensors’ char -\nacteristics, and each kernel can be considered as a different feature generator. The size and \nnumber of kernels are two primary hyperparameters that describe the convolution opera-\ntion. Usually, the kernels’ size is 3 × 3, but it can also be 5 × 5 or 7 × 7. The number of ker -\nnels is chosen arbitrarily depending on the depth of the output feature maps. Mathemati-\ncally, convolution operation can be defined by the following equation (Khan et al. 2020):\nwhere fk\nl is the output feature map of the k-th convolution operation of the l-th layer. This \ncan be computed as Fk\nl=[fk\nl(1,1),…fk\nl(p,q),…fk\nl(P,Q)] , where ic is the input tensor \nand ic(x,y) is an element of that tensor. These values will be element-wise multiplied by \nek\nl(u,v) , the k-th convolutional kernel of the l-th layer.\nCNN introduces non-linearity to the network by applying a non-linear activation func-\ntion. Previously, the popular choice was non-linear activation functions, including sig-\nmoid or tangent functions (LeCun et al. 2012). However, to resolve the vanishing gradi-\nent problem (Nwankpa et al. 2018) of the sigmoid and tangent function, Rectified Linear \nUnit (ReLU) and its variants, such as leaky ReLU and Parametric Rectified Linear Unit \n(PReLU), are used. One of the recently proposed activation functions named Mish outper -\nforms ReLU and other typical activation functions in many deep networks across bench-\nmark datasets (Misra 2019). The activation function of the convolutional feature map can \nbe computed as:(3)fk\nl(p,q)=/uni2211.s1\nc/uni2211.s1\nx,yic(x,y)⊙ek\nl(u,v)\nFig. 6  Convolution operation\n\n13534 S. F. Ahmed et al.\n1 3\nwhere Fk\nl is the output of a convolutional operation that goes to an activation function g(); \nand ak\nl is the non-linear output of the k-th input feature map in the l-th layer.\nThe extracted features from the convolutional and pooling layers are flattened to \na one-dimensional array of numbers. Those features are then fed into the traditional \nneural network, where each input is connected to its subsequent layer neurons by a \nlearnable weight. The main drawback of a fully CNN is that it requires training many \nparameters, which contributes to its high computational expense and possible overfit-\nting. The dropout technique is used to overcome such difficulties, in which a few nodes \nand connections are removed (Goodfellow et al. 2013). The output layer is the final \nlayer of CNNs, where softmax  function is widely used to provide probability distribu-\ntion (Russakovsky et al. 2015). Another classifier, the support vector machine (SVM), \ncan also classify data (Tang 2013).\nParallel computing has made CNNs more efficient than humans in recognizing vis-\nual patterns, making them a desirable alternative for wide-area monitoring because of \ntheir advantages over humans. Mukherjee et al. (2020) proposed a CNN-based genera-\ntive model, namely “GenInSAR”, for combined coherence estimation and phase filter -\ning which directly learns interferometric synthetic aperture radar (InSAR) data distri-\nbution. InSAR is a developing and extremely successful remote sensing method for \nmonitoring a variety of geophysical parameters, including surface deformation. The \nunsupervised training on simulated and satellite InSAR images of the proposed model \n(GenInSAR) outperformed the other comparable methods (CNN-InSAR(as-is), CNN-\nInSAR(retrained), NLSAR, NLInSAR, Goldstein, Boxcar) in reducing the total resi-\ndue (by more than 16.5% on average), with fewer over-smoothing/artifacts surrounding \nbranch cuttings. Compared to the related methods, the phase cosine error, coherence \nand phase root-mean-square-error of GenInSAR were improved by 0.05, 0.07 and 0.54, \nrespectively. As a result, the InSAR machine learning can be improved by GenInSAR’s \nability to produce new interferograms.\n4.2.1  CNN‑LSTM\nLong short-term memory (LSTM) can learn long-term relationships in data. However, \nspatial data like images are challenging to model with the standard LSTM. The convo-\nlutional neural network combined with long short-term memory (CNN-LSTM) is based \non an LSTM network that is primarily designed for sequence prediction tasks where \nthe input is spatial data, such as images, videos, or temporal structure of words in a \nsentence, paragraph, or document. The model shown in Fig.  7 illustrates the combined \nregional CNN and LSTM to identify the sentiment of text (Wang et al. 2016a), which \nconsiders an individual sentence as a region and long-distance relationship of sentences \nin the prediction task.\nThe main architecture of the CNN-LSTM model consists of the input layer, convolu-\ntion layer, pooling layer, sequential layer (LSTM hidden layer), and fully connected layer. \nThe first three layers are the CNN layers. The CNN layer’s output data is transferred to the \nLSTM layer. Following temporal modelling, the data from the LSTM layers are sent to a \nfully connected layer. These layers are well-suited to produce higher-order features that are (4) ak\nl=g(Fk\nl)\n\n13535\nDeep learning modelling techniques: current progress,…\n1 3\neasy to distinguish within distinct categories. The CNN model is used for feature extrac-\ntion, while the LSTM model is employed for data interpretation over time.\n4.2.2  Temporal convolutional network (TCN)\nThe novel work on the temporal convolutional networks (TCNs) was first proposed \nby Lea et al. (2016) for video-based action segmentation. This approach involves two \nphases: (i) CNN computes the low-level features that encapsulate spatial–temporal \ninformation, and (ii) RNN feeds the low-level features into a classifier to extract the \nhigh-level temporal information. Although TCN demands the integration of two differ -\nent models, it offers a unified technique to capture all two layers of information in a \nhierarchical manner. The original TCN model possesses a convolutional encoder and \ndecoder architecture. The model captures a set of video features as the input and then \nextracts a D -dimensional feature vector for each video frame. If a video has T  frames, \nthe input X  appends all the frame-wise features in a way that X∈ℝT×D . Similar to other \nCNN architectures, the networks apply some filters followed by non-linear activation of \nthe input to extract features. The convolution consists of l  layers, where the collection \nof filters in each layer is defined as {W(i)}Fl\ni=1 for W(i)∈ℝd×Fl−1 . Here, Fl is the number of \nconvolution filters in the l  layer with a temporal window d. If Xl−1 is an output from the \nprevious layer, the l -th layer output, Xl , can be calculated as follows (Kim and Reiter \n2017):\nwhere f denotes any non-linear activations functions, e.g., ReLu.\nConvolutional neural networks and their variants are used in various applica-\ntions, such as the detection, classification of objects in images and video data, finding (5) Xl=f(W∗Xl−1)\nFig. 7  Regional CNN-LSTM model for sentiment analysis (Jin Wang et al. 2016a)\n\n13536 S. F. Ahmed et al.\n1 3\nsentiment and emotions in natural language data, and audio processing applications like \nvoice recognition. A CNN-based architecture named LeafNet was developed by Barré \net al. (2017) to identify plant species from the leaf images. The authors experimented \nwith their model on three publicly available state-of-the-art datasets of leaf images: \nLeafSnap, Foliage, and Flavia. The previous studies on these datasets were based on \nthe hand-crafted feature extraction technique. After data augmentation, approximately \n270,000 leaf images were used on a 17-layer CNN to train the LeafNet model with \nimage sizes of 256 × 256 pixels. Improved accuracies (by 0.8–13.3%) of 86.3%, 95.8%, \nand 97.9% were found on the LeafSnap, Foliage, and Flavia datasets, respectively, com-\npared to previous studies. However, this method is comparatively slow (training takes \nabout 32 h) and lacks context due to the small, cropped window sizes.\nIn another work, a region-based convolutional neural network (R-CNN) has been \napplied in the computer vision field for the object detection task. Li et al. (2019a, b, c, d) \nproposed the stereo R-CNN method that can perform three-dimensional (3D) object detec-\ntion in autonomous vehicle navigation. The method identifies and integrates objects in both \nthe left and right images simultaneously and uses a region-based object detection align-\nment to retrieve the correct 3D bounding box. The stereo R-CNN captures input images \nwith a resolution of 600 × 2000 and takes advantage of ImageNet’s pre-trained ResNet-101. \nThe model was evaluated on the KITTI object detection benchmark. The proposed method \noutperformed a previous study (Chen et  al. 2018a, b) for 3D object proposals by over \n25–30%. Due to the absence of precise depth information, the model can only produce \nshallow 3D detection results. Variations in appearance can also have a significant impact.\nChen et al. (2018a, b) introduced an unsupervised domain adaptation model for cross-\ndomain object detection based on the faster R-CNN model (Zhang et al. 2016a, b). They \nemployed two domain classifiers: one for high-level features at the global image scale and \nanother for features clipped by the region proposal network at the instance (object) scale. \nThe model was validated for different domain shift datasets. Via experiments, the authors \nfound that the domain adaptive faster R-CNN model outperforms the faster R-CNN model \nby over 8.8%. This improvement was found consistent across the categories, thus indicat-\ning that the suggested method can minimize domain mismatch between object categories. \nHowever, the model was not trained to recognize traffic in darkness and is only adaptable \nto specific scenarios.\nA dynamic CNN-based system was proposed by Chu et al. (2017) for tracking objects in \nvideos. Using shared CNN features and Region of Interest Pooling, the model takes advan-\ntage of single object trackers. The experimental results showed that the proposed online \nmulti-object tracking algorithm outperforms Markov decision processes by 4%. Although \nthe model performed well in tracking objects, it is unsuitable for applications with limited \nresources. Also, the model may consume a lot of memory and time as it constructs a net-\nwork for individual objects and performs online learning. Since CNN works well for both \nimage classification and natural language processing tasks, CNN-based text classification \nmodels are gaining popularity. For instance, multi-layer CNN produces optimal features \nduring the training process to reflect the semantics of the sentence being evaluated. These \nsemantic constructs can be applied to a variety of applications, including text classification, \ntext summarization, and information retrieval.\nA CNN-based method was suggested by Hughes et  al. (2017) for classifying clinical \ntexts into one of 26 categories, such as “Brain” or “Cancer.” The model classifies texts \nby converting each document into a sentence-level representation. The authors used \ntwo stacked convolutional layers followed by a pooling layer. The experimental analy -\nsis revealed that the model improves the word embedding-based methods by accuracy of \n\n13537\nDeep learning modelling techniques: current progress,…\n1 3\naround 15%. However, the model was trained with a relatively small dataset (4000 sen-\ntences). To improve the model performance, domain adaptation techniques can be used to \ntransfer knowledge from another domain to the medical field (Sun et al. 2016).\nCNN-based models have also been successfully applied in the sentiment analysis of \nTwitter data. To predict user behavior via sentiment analysis, Liao et al. (2017) examined \ndifferent deep learning techniques. They employed CNN and word embedding techniques \nto get better results than traditional learning algorithms, such as SVM and Naive Bayes \nclassifiers. Their approach interpreted the sentence matrix to be the same as an image \nmatrix. A linear kernel was convoluted to that sentence matrix, and a max-pooling func-\ntion was applied to each feature to find the fixed-length representation of the sentence. \nThe model was assessed on several benchmark datasets, including MR and STS Gold. The \nmaximum development accuracy was found to be up to 74.5%. To improve the model accu-\nracy, a multilayer CNN may be used instead of a simple CNN (single channel) for sentence \nclassification.\nCNN-based approaches are also becoming more prominent in cosmology because of \ntheir noticeable performance. DeepSphere is a graph-based CNN that works on cosmologi-\ncal data analysis (Perraudin et al. 2019) to predict a class from a map and classify pixels. \nThe data often come as spherical maps represented as a graph in the network so that the \nmodel can perform the convolution and pooling operations. In the latter work, DeepSphere \noutperformed all the baselines by 10% in terms of classification accuracy. However, the \nmodel was applied to only the classification problem performed on scalar fields. To further \ndemonstrate the performance of DeepSphere, it would be useful to make comparisons to \nvarious spherical CNN implementations with different sampling techniques.\n4.3  Recurrent neural network (RNN)\nRecurrent neural networks (RNNs) have recently demonstrated promising performance \non various natural language processing tasks and have produced superior results on mul-\ntiple tasks, such as sentiment classification (Wang et  al. 2016c), image captioning (Yao \net al. 2017), and language translation (Li et al. 2017a; b). There are numerous situations in \nwhich data sequences describe the case itself. For example, in a language modelling task, \na sequence of words defines their meaning. If the sequences are disturbed, the informa-\ntion makes no sense. In a traditional neural network, the assumption is that there has no \ndependency between the input and output. Considering this case, a network connecting to \nprior information is needed to fully comprehend the data. As a response, RNNs are useful, \nwhich are termed from the fact that they execute the same computation for each sequence \nelement. The output in every state is dependent on the previous calculation. RNNs keep \na \"memory\" that captures the information about what has been computed so far (Tomaš \nMikolov et al. 2010, 2011). An RNN can be unfolded into an entire network, as illustrated \nin Fig. 8.\nThe computation flows running in an RNN for the text processing task are as follows:\n• xt denotes the present input at time step t, where input is given as a one-hot encoded \nvector. For example, x1=[10 00 0]t is the initial word in a sentence.\n• st signifies the hidden state at time step t, captures the “memory of the network, and is \ncomputed using the previous hidden state and the present step’s input:\n\n13538 S. F. Ahmed et al.\n1 3\nwhere f is an element-wise non-linear function, such as tanh or ReLU  . In the case of calcu-\nlating the first hidden state, st−1 is typically set to all zeros. W and U are the weight matrix \nof the hidden state and input, respectively.\n• ot represents the output at time step t. For instance, to predict the next word in a sen-\ntence, the probability can be calculated by applying the softmax  function.\nAn RNN can, in theory, summarize all historical information up to time step st . Unfor -\ntunately, the accuracy of RNNs is significantly inhibited by the vanishing gradient problem \n(Bengio et al. 1994). To address this problem, gated recurrent units and long short-term \nmemory have become more powerful models and gained acceptance in recent years as the \nbest strategy to implement recurrent neural networks.(6) st=f(Uxt+Wst−1)\n(7) ot=softmax (Vst)\nFig. 8  Unfolded recurrent neural network (Lecun et al. 2015)\nFig. 9  A schematic for a long short-term memory cell (Jenkins et al. 2018)\n\n13539\nDeep learning modelling techniques: current progress,…\n1 3\n4.3.1  Long short‑term memory (LSTM)\nA long short-term memory (LSTM) network is comprised of different memory blocks \nreferred to as cells. A cell is constructed by gates that control the flow of information: \nforget, input, and output gates (Fig.  9). A forget gate removes information from a cell con-\nfiguration, and the input gate updates the newly entered data to the cell. The input gate \ndetermines the rate at which new data enter the cell, whereas the output gate limits the data \nin the cell and computes the output activation of the LSTM unit.\nThe gating mechanism in a LSTM can be defined by the following equations:\nwhere it is input gate; ft denotes forget gate; ot is output gate at a time step t; /uni0303.s1Ct is a new \nmemory cell vector; and W and U are parameter matrices.\n4.3.2  Bidirectional long‑short time memory (BiLSTM)\nRegular recurrent neural networks with LSTM cells can be extended to bidirectional recur -\nrent neural networks in which the data is passed through two LSTMs (Graves et al. 2013; \nGraves and Schmidhuber 2005). One forward LSTM offers the input sequence in the cor -\nrect order (forward layer), and another backward LSTM provides the input sequence in \nreverse order (backward layer). This technique improves the model’s accuracy by captur -\ning the long-term dependencies of the input sequence in both directions. In the BiLSTM, \nthe forward layer computation is identical to those in the regular LSTM that computes \nthe sequences ( � �⃗ht,� �⃗ct ) from t=1toT . On the other hand, the backward layer computes the \nsequences ⃖� � �ht,⃖� �ct from t=Tto1 as described below:(8) it=/u1D70E(Wixt+Uiht−1+bi)\n(9) ft=/u1D70E(Wfxt+Ufht−1+bf)\n(10) ot=/u1D70E(Woxt+Uoht−1+bo)\n(11) /uni0303.s1Ct=tanh(Wgxt+Ught−1+bg)\n(12) Ct=ft⊗Ct−1+it⊗�Ct\n(13) ht=ot⊗tanh(ct)\n(14) ⃖�it=𝜎(Wixt+Uiht+1+bi)\n(15) ⃖�ft=𝜎(Wfxt+Ufht+1+bf)\n(16) ⃖� �ot=𝜎(Woxt+Uoht+1+bo)\n(17)⃖ ����Ct=tanh(Wgxt+Ught+1+bg)\n\n13540 S. F. Ahmed et al.\n1 3\nIn a study conducted by Siami-Namini, Tavakoli, and Namin (2019), LSTM and BiL -\nSTM were compared in terms of time series data modelling. The prediction accuracy of the \nBiLSTM-based model was 37.78% higher than that of standard LSTM-based models after \ntraining with both directions of input data. However, BiLSTM-based models achieved slower \nperformance than the LSTM-based models. Another study (Brahma 2018) introduced a new \nmodel suffix bidirectional LSTM (SuBiLSTM) that improved BiLSTM for sentiment classifi-\ncation and question classification tasks (see Figs. 7 , 8, 9).\n4.3.3  Gated recurrent unit (GRU)\nThe architectures of a gated recurrent unit (GRU) and long short-term memory (LSTM) \nare closely related, since both are crafted similarly and, in some situations, generate equally \noutstanding results (Murali and Swapna 2019). The GRU cell is comprised of two gates: an \nupdate gate z  and a reset gate r . It addresses the vanishing gradient problem of a regular RNN \nby using the update gate to determine how much historical memory (from earlier time steps) \nshould be maintained and proceed to the future and the reset gate to pair the new input with \nthe prior memory, as shown in Fig. 10.\nThe gating mechanism in GRU is expressed by the following equations:(18) ⃖� � �Ct=⃖�ft⊗⃖ ������Ct+1+⃖�it⊗⃖ ����Ct\n(19) ⃖� �ht=⃖� �ot⊗tanh( ⃖� �ct)\n(20) z=/u1D70E(Wzht−1+Uzxt)\n(21) r=/u1D70E(Wrht−1+Urxt)\n(22) c=𝑡𝑎𝑛h/parenleft.s1Wc(ht−1⊗r/parenright.s1+Ucxt)\nFig. 10  Gated recurrent unit cell (Zhao et al. 2019)\n\n13541\nDeep learning modelling techniques: current progress,…\n1 3\nwhere xt is the input vector; ht is output vector; W and U are parameter matrices; /u1D70E is the \nsigmoid function; and ⊗ denotes the Hadamard product (entry-wise product).\nDue to their versatility in various applications, RNNs have been successfully used in \nmultiple tasks, including language modelling, speech-to-text processing, caption generator, \nmachine translation, and other fields. RNN has also been applied in the sentiment analysis \ntask to produce effective outcomes. For instance, Basiri et al. (2021) proposed a model to \ndetermine the sentiment from long reviews and short tweet text. In the model, the attention \nmechanism in RNNs is used to pay more attention to certain factors by assigning different \nweights when processing the data. The linguistic structures become more descriptive by \napplying the attention mechanism. Two bidirectional LSTM and GRU are also employed \nto generate the input text’s previous and next contexts feature representation. The proposed \nmodel improved the accuracy from 1.85% to 3.63% for five long review datasets and from \n0.25% to 0.54% for three short tweet datasets. While the study emphasized sentiment clas-\nsification at the document level, there is potential to investigate sentiment classification at \nthe sentence and aspect levels.\nAnother RNN model based on dialogue was built with an attention mechanism for emo-\ntion detection in textual conversations with six emotion labels (Majumder et al. 2019). The \nmodel has several variants, including DialogueRNN + Att and BiDialgouRNN, and con-\nsiders both context and speaker information. The network employs three GRUs to track \nindividual speaker states, global context from the preceding utterances, and the emotional \nstate through the conversations. The data are provided and fed into the GRU for emotion \nrepresentation, depending on the context. Although the DialogueRNN model achieved a \nbetter f1-score of 6.62% on several experiments, which is above the baselines (Majumder \net al. 2019), it is time-consuming for training and not parameter-efficient for global or local \ncontexts.\nIn RNN-based neural machine translation (NMT), sequence-to-sequence (seq2seq) \narchitectures are used to deal with translation between languages. These seq2seq archi-\ntectures apply two RNNs, namely an encoder and decoder. A study (Camgoz et al. 2018) \nutilized the standard seq2seq model to recognize sign language gestures from a video of \nsomeone performing continuous signs. In the study, the CNN was trained on the sentence \nlevel annotation to extract features from the video before translating it to text. These fea-\ntures were fed to the seq2seq model. The model scored 18.13 on the BLEU-4 matric (Pap-\nineni et al. 2001) and 43.80 on the ROUGE matric (Lin 2004). The model assumed that \nthe CNN could learn good feature representation, but this hypothesis’s validity was not \nevaluated.\nTo model long texts for generating semantic relations between sentences, researchers \nface challenges in sentiment analysis. Rao et al. (2018a, b) handled the problem by propos-\ning the State Refinement-LSTM (SR-LSTM) and SSR-LSTM models based on deep RNN. \nThe models have two hidden layers: the first one uses LSTM to represent the semantic rela-\ntionship of sentences, and the second one encodes those sentence relationships at the docu-\nment level. The SR-LSTM model outperformed other models by obtaining an accuracy of \n44% and 63.9% on the IMDB and yelp2015 datasets, respectively, while the SSR-LSTM \nmodel achieved an accuracy of 44.3% and 63.8% on the same datasets. However, the mod-\nels considered only the sequential order of the documents. In future works, it may possible \nto represent the documents using tree-structured LSTM.\nRNNs have also been successfully applied in intelligent health care systems. For example, \nUddin et al. (2020) presented a multi-sensors data fusion network that relies on a recurrent (23) ht=(z⊗c)⊗((1−z)⊗ht−1)\n\n13542 S. F. Ahmed et al.\n1 3\nneural network to recognize human activities and behavior. They extracted features from mul-\ntiple body sensors and enhanced the features using Kernel Principal Component Analysis \n(KPCA) techniques. Then, human activities were recognized by training a deep RNN. The \nproposed method was assessed on three publicly available datasets. The average performance \nwas found to be 99% using precision, recall, and F1-score matrices. It is possible to extend the \nwork by developing a real-time human behavior tracking system with considering more com-\nplex human activities.\nThe RNN-LSTM approach for time series modelling has recently attracted much interest. \nThe applicability of RNN-LSTM was analyzed by Sahoo et  al. (2019) for predicting daily \nflows during the low-flow periods. The model effectively used the time series data by taking \nadvantage of the LSTM memory cell to learn features from both the current and past values \nof an observable object. The model’s performance (root-mean-square error RMSE = 0.487) \non hydrological data outperformed the traditional RNN model (RMSE = 0.516) and naive \nmethod (RMSE = 0.793). Nevertheless, multiple hidden LSTM layers can be used to enhance \nthe performance of the model. Experts are also attempting to use deep learning approaches \nin typhoon prediction as deep learning techniques become more sophisticated. Alemany \net al. (2019) proposed a fully connected RNN to predict hurricane trajectories from historical \ncyclone data that could learn from all types of hurricanes. The model produced better predic-\ntion accuracy than the previous models. For example, the mean absolute error (0.0842) of the \nRNN model was better than that of the previous sparse RNN average model (0.4612) to track \nHurricane Sandy in 2012. The model may take advantage of converting the grid locations to \nlatitude–longitude coordinates to reduce the conversion error.\n4.3.4  Deep echo state network\nThe deep echo state network (DeepESN) is a recently proposed technique to enhance the effi-\nciency of a general echo state network (ESN) in several domains. ESN is a reservoir comput-\ning model in which the reservoir computing shows efficiency to train RNNs by preserving \nmemory using its recurrent nature. A dynamic reservoir is incorporated in ESN, presenting a \nsparsely linked recurrent network of neurons that differs from a traditional multilayered neu-\nral network. The reservoir is the network’s only hidden layer, and its input connections are \nassigned at random and cannot be trained. On the other hand, the weights between the reser -\nvoir and output are the only ones that can be trained. The system learns the weights by linear \nregression rather than backpropagation. DeepESN is simply the ESN model’s application of \nthe deep learning architecture.\nThe DeepESN output is produced using a linear structure of the recurrent units across \nall recurrent layers. After initialization, the DeepESN reservoir component is left untrained. \nTherefore, the usual ESN technique is subject to stability limitations. Such limits are stated \nin DeepESN by the criteria for the ESN of the deep reservoir computing network. In the deep \necho state network, input is processed by the first layer, and the previous layers’ outputs pro-\ncess the successive layers’ inputs. Therefore, the state transition function of a DeepESN can be \npresented by the following equation (Lukoševičius and Jaeger 2009):\nwhere l represents the number of layers; W(l)\nin refers to the input matrix for l ; /u1D703(l) denotes \nbias weight vector; and /uni0302.s1W(l) expresses the recurrent weight matrix for layer l . Here, i(l)(t) \nsignifies the input for the l th layer of the network at time t . The output of the model can be \nexpressed by the following equation:(24) x(l)(t)=/parenleft.s11−a(l)/parenright.s1x(l)(t−1)+a(l)tanh(W(l)\nini(l)(t)+/u1D703(l)+/uni0302.s1W(l)x(l)(t−1))\n\n13543\nDeep learning modelling techniques: current progress,…\n1 3\nwhere Wout is the weight matrix between the reservoir and output y(t).\nBased on the DeepESN, a novel technique was developed by Gallicchio et al. (2018a) \nfor diagnosing Parkinson’s disease. This is a significant initial work in the DeepESN \ndomain that shows the superiority of DeepESN over the shallow echo state network model. \nThe proposed technique identified Parkinson’s disease by using the time series data gath-\nered from a tablet device while subjects performed sketching spiral tests with a pen. The \nacquired data contain x and y components of the pen, pen pressure, and grip angle. These \nsignals were used to feed the model with no feature extraction and data pre-processing. The \nproposed model was evaluated on a public spiral test dataset and showed to perform better \nthan the shallow ESN and other state-of-the-art methods.\nGallicchio et  al. (2018b) proposed a DeepESN technique based on additive decom-\nposition for predicting the time series data where the additive decomposition technique \nwas used as a pre-processing step to the model. Data are split into three parts by additive \ndecomposition (trend, seasonality, and residual) and then fed to the DeepESN. The perfor -\nmance of the additive decomposition-DeepESN was compared with LSTM, GRU, ESN, \nand DeepESN algorithms on six different datasets. The proposed model demonstrated \nsignificant performance for large, multidimensional data. Although ESN was found to be \ncomputationally efficient, it delivered a poor performance in prediction. LSTM and GRU \nrequired five times more computational time than DeepESN and additive decomposition-\nDeepESN. The additive decomposition-DeepESN model showed a low standard deviation, \nproving its stability, whereas other reservoir algorithms were unstable, i.e., with a higher \nstandard deviation. Thus, the additive decomposition technique has the ability to improve \nthe stability and performance of the DeepESN.\n4.3.5  Elman recurrent neural network\nThe difference between the Elman recurrent neural network (ERNN) (Elman 1990) and \nother recurrent networks is that the hidden layer’s output is used as input for the context \nlayer in the former. The architecture of ERNN consists of four layers: input layer, recurrent \nlayer, hidden, and output layer. Each layer has one or multiple neurons that use a non-linear \nfunction of their weighted sum of inputs to transfer information from one layer to the next. \nEach hidden neuron is linked to a neuron of the single recurrent layer with the constant \nweight of one. As a result, the recurrent layer contains a copy of the hidden layer’s state \none instant ago. The benefit of using ERNN is that it emphasizes the relationship between \nfuture and previous values even when it is difficult to learn from them. The ERNN can be \ndescribed by the following equations (Achanta and Gangashetty 2017):\nwhere Wi signifies hidden weight’s input; W denotes the recurrent weight matrix of the hid-\nden layer; bh represents the hidden bias; U refers to the hidden output matrix; bo is the Bias \nVector of the output layer; and f and g are the non-linear functions of hidden and output \nlayers, respectively. Input is represented by xt , the state of ht and yt refer to the outputs at \ntime t.(25) y(t)=Wout[x(1)(t)x(2)(t)…x(Nl)]T+/u1D703out\n(26) ht=f/parenleft.s1Wixt+Wht−1+bh/parenright.s1\n(27) yt=g(Uht+bo)\n\n13544 S. F. Ahmed et al.\n1 3\nAn ERNN model with a stochastic time effective function (ST-ERNN) was developed \nby Jie Wang et al. (2016b) to forecast stock indices. The architecture is built by combin-\ning ERNN, multilayer perceptron, and stochastic-time-effective function, where a sto-\nchastic process is used to describe the level of historical data impact in the market. The \ntime-strength function includes a drift function and Brownian motion to model the appear -\nance of random changes while keeping the primary trend. The proposed neural network \nperforms better than other existing neural networks in financial time series forecasting. \nConsidering the rapid changes in the stock market data that make the field non-linear and \nnonstationary, predicting this kind of data is very challenging. Nevertheless, ST-ERNN \nshowed a significant performance that can be crucial for future experiments in this domain. \nKrichene et al. (2017) applied ERNN for forecasting Mackey Glass time-series elements. \nThe performance of ERNN was evaluated via comparison with two other existing models \n(Al-Jumeily et al. 2014; Park 2010) using the same dataset, where ERNN showed better \nperformance. It is worth noting that optimal performance was achieved when the weights \nof the context units were randomly initialized.\n4.4  Recursive neural network\nA recursive neural network (RvNN) is a nonlinear model that can function on structured \ninputs and is applicable to parse trees in natural language processing (NLP), image anal-\nysis, protein topologies, among other applications in structured domains. For instance, \nRvNN performs extremely well in the NLP tasks. Despite their deep structure, the archi-\ntecture of RvNN lacks the capacity for hierarchical representation (Irsoy and Cardie 2014) \nand contains complex informative processing models. Because they acquire high-level rep-\nresentations from explicit inputs, recursive networks are effective in many deep learning \ntasks where the input is a structure. RvNNs are normally defined on a directed positional \nacyclic graph (Micheli et al. 2007). The form of RvNN is shown in Fig.  11, referring to the \nparse tree on the left side (Ma et al. 2018). If the parent node’s feature vector is p, and c1 \nand c2 are its children, then\n(28) p=f(w.[c1;c2 ]+b)\nFig. 11  The tree and its associated RvNN architecture (Ma et al. 2018). In the figure, S represents a sen-\ntence, NP is a noun phrase, VP is a verb phrase, D denotes determiner, N signifies noun, and V is a verb\n\n13545\nDeep learning modelling techniques: current progress,…\n1 3\nwhere f(.) is the activation function. The computation is recursively done for all nodes, and \nthe hidden vectors of nodes’ can then be used for different classification tasks.\nTree-structured recursive neural networks (RvNNs) were used to perform rumor detec-\ntion on Twitter by Jing Ma, Gao, and Wong (2018). This study constructed two recursive \nnetworks on top-down and bottom-up tree-structured neural networks. Rather than a sen-\ntence’s parse tree, the model’s input is a propagation tree rooted from a source post, and \neach node is a sensitive post rather than individual words. Recursive feature learning can \ncapture the content semanticization of posts along with the tree structure and the receptive \nrelationship between them. The basic concept of the bottom-up model is to create a feature \nvector by traversing each node recursively from the leaves to the root on the top. On the \nother hand, the concept of the top-down approach is to create an enhanced feature vector \nfor each post, considering its propagation direction, in which rumor indicators are com-\nbined along the path of propagation. However, for the non-rumor class, the proposed mod-\nels did not perform well. Yet, they could add other types of data into the structured neural \nmodels, such as user properties, to boost representation learning even further.\nBiancofiore et al. (2017) analyzed atmospheric particulate matter (PM) and forecasted \ndaily averaged concentrations of PM10 and PM2.5 up to 1–3 days. Particulate matter is \na significant pollutant that affects human health, thus studies on reducing PM are critical. \nThe latter researchers implemented a multiple linear regression model, feed-forward neu-\nral network, and neural networks with the recursive structure and found that the recursive \nneural network model outperforms the other methods. The total number of input variables \nand neurons in the second layer in the model determines how many neurons are in the first \nlayer. The network’s output, the predicted particulate matter concentration, is represented \nby a single neuron in the final layer. In the latter work, the RvNN model correctly predicted \n95% of the days, but this decreased to 57% when considering only the days where the lim-\nits were exceeded. In addition, the false-positive rate was 30% in this study.\nLim and Kang (2018) extracted the relation between chemical compounds and genes. \nThey experimented with three methods, a tree-LSTM model with a position feature and \na subtree containment feature, and implemented an ensemble process. The authors also \nimplemented a stack augmented parser interpreter neural network (SPNN). The study \nrevealed that the SPNN with ensemble technique outperformed the tree-LSTM with \nensemble technique, which means that the extra tracking layer is beneficial. However, \nthe proposed model is unable to comprehend the structure of a sentence. More training \ninstances are needed to resolve this error. Also, coordination was not detected, whereby a \ncomma, parenthesis, or special term like “and” or “or” is used to express coordination rela-\ntions. This form of error may be avoided with the use of a separate module that looks for \nterms of equal emphasis.\n4.5  Neural tensor network\nIn several natural language processing tasks, neural tensor networks (NTNs) have been \nsuccessful. However, they need to estimate a considerable number of parameters, often \nresulting in overfitting (Yang et  al. 2015) and excessive training times. An NTN model \nconstructed by Socher, Chen, et al. (2013) implements a 3D tensor for combining two input \nvectors as bellow:\n(29) f(xT\n1W[1∶k]x2+V/bracketleft.s3\nx1\nx2/bracketright.s3\n+b)\n\n13546 S. F. Ahmed et al.\n1 3\nwhere W[1∶k ]∈ℝnxnxk is the tensor ( W is a slice matrix); V∈ℝk×2n is the linear mapping to \ncombine input vectors x1 and x2; b refers to a bias term; f is the non-linear activation func-\ntion; and xT\n1W[1∶k ]x2 is an array of k bilinear products.\nIn contrast to the regular neural network model, NTN can connect two input vectors \nwith a tensor directly. Although the NTN model is efficient, it takes considerable time \nto compute. Several studies were done to reduce the time complexity using parameter \nreduction techniques. For instance, Ishihara et  al. (2018) introduced two-parameter \nreduction techniques based on the matrix decomposition method, while Y. Zhao, Liu, \nand Sun (2015) and P. Liu, Qiu, and Huang (2015) proposed simple matrix decom-\nposition techniques for reducing parameters. A neural tensor model named the con-\nvolutional NTN converts all word tokens into vectors with the help of a lookup layer, \nencode questions and answers with coevolutionary, pooling layers to fixed-length vec-\ntors, and finally modelling their interactions with a tensor layer. Therefore, in a seman-\ntic vector space, this model will group related questions and answers to avoid the prob-\nlem of lexical distances.\nQiu and Huang (2015) proposed a convolutional NTN for community-based ques-\ntion answering, integrating sentence modelling and semantic matching into one model. \nThey implemented contrastive max-margin criterion to train the model. This study \nevaluated two different datasets for English and Chinese languages and found that the \nproposed model can handle more complex interactions with tensor layers than exist-\ning models. However, texts were converted into fixed-length vectors with the proposed \nconvolutional layer, saving the essential information lost in bag-of-words. The experi-\nments on the Chinese dataset demonstrated worse performance than the English data-\nset, which may be due to some mistakes in the segmentation of the Chinese expression.\nA deep attention NTN for visual question answering was introduced by Bai et al. \n(2018). In this approach, tensor-based representations are used to find the joint rela-\ntionship between images, questions, and responses. The authors used bilinear features \nto model images and questions that were further encoded by third dimension, i.e. the \nresponse as a triplet. The correlation between various triplets was broken down by dif-\nferent types of answers and questions. For the most discriminatory inference reasoning \nmethod, a slice-wise attention module was developed. The model was optimized by \nlearning a label regression with Kullback–Leibler divergence losses. This designing \ntechnique enabled fast convergence and scalable training across a wide range of answer \nsets. The proposed model structure was integrated into the known visual question \nanswering models MLB (Kim et al. 2017) and MUTAN (Ben-Younes et al. 2017). The \nproposed technique showed more accuracy than independent MLB and MUTAN mod-\nels. This study compared GloVe word embedding with the word embedding learned \nfrom the proposed model and demonstrated that the model could be applied to more \nvisual question answering models for further verification.\nHu et  al. (2017) proposed enhanced face recognition performance by combining \nface recognition features and facial attribute features in a variety of tasks. They created \na robust tensor-based model that develops fusion as a problem of tensor optimization. \nDue to the great number of parameters, the model was not effective in explicitly opti-\nmizing this tensor, and therefore a rich fusion architecture was proposed on the basis \nof the tensor. The results revealed that this tensor-based fusion’s Tucker-Low-Rank \ndecomposition has the same Gated Two Stream neural network, making neural network \nlearning simple but effective. The authors experimented on three well-known data-\nbases (MultiPIE, CASIA NIR-VIS2.0, and LFW) and found that the fusion approach \n\n13547\nDeep learning modelling techniques: current progress,…\n1 3\nsignificantly increased the face recognition performance. This technique can be \nexpanded to large-scale data utilizing effective Mini Batch SGD-based learning since \nthey set the equivalence between tensor-factorization and gated neural network archi-\ntecture. Another advantage is that this model can be expanded to deeper architectures.\n4.6  Continuous‑bag‑of‑word with denoising autoencoder‑logistic regression\nTo analyze sentiments, a Multimodal Learning technique was presented by Baecchi et al. \n(2016) by implementing neural network-based models for microblogging contents that \nmight consist of texts and images. The proposed architecture is based on the continuous-\nbag-of-word (CBOW) model (Mikolov et al. 2013) and was further extended to include \na denoising autoencoder (DA) to include visual data. Thus, CBOW-logistic regres-\nsion (LR) is the extended version of CBOW. The difference between CBOW and the \nextended model is that the new architecture can perform classification and representa-\ntion concurrently. The idea behind this approach is that the multi-tasking technique can \ndevelop the performance of a neural network, while the proposed model can incorporate \nsemantic and sentiment polarity. The model was further extended to CBOW-DA-LR to \ninclude visual data, such as images in tweets. The descriptor acquired by the denoising \nautoencoder, along with the regular word presentation, provides a new descriptor for a \nword window in the tweet and learns a logistic regressor at the same time. The proposed \nCBOW-DA-LR technique was compared to SentiBank, a commonly-used approach in \nthis domain, and showed higher accuracy (79% accuracy on text + image data vs. 72% of \nSentiBank). Although this specific technique shows significant improvements, it should \nbe further evaluated to ensure its validity.\n4.7  Deep belief network\nA deep belief network (DBN) is used to stack several unsupervised networks utiliz-\ning the hidden layer of each network for the next layer’s input. A stack of restricted \nBoltzmann machines (RBMs) is typically used in the DBN. The benefit of the restricted \nBoltzmann machine is to fit the sample features (Hinton 2009). Therefore, a hidden lay -\ner’s output in an RBM can be used as another RBM’s visible layer input. This method \nmay be considered as the further extraction of the features from the extracted features of \nthe samples.\nSuppose that W is the generative weights of the hidden layers learned by an RBM denote \np(v/uni007C.varh, W) and prior distribution over hidden vectors p(h/uni007C.varW ) . If v is the visible vector, then \nthe probability of v can be expressed by the equation:\nwhere p(v/uni007C.varh, W) is kept after learning W; and p(h/uni007C.varW ) is replaced by a more reliable model \nof the grouped following distribution on hidden vectors.\nA computer-aided diagnosis system was built by Abdel-Zaher and Eldeib (2016) for \ndetecting breast cancer, utilizing a weight-initialized backpropagation neural network from \na trained DBN having identical architecture. The authors implemented DBN in an unsuper -\nvised state for acquiring the input features from the main Wisconsin breast cancer dataset. (30)p(v)=/uni2211.s1\nhp(h/uni007C.varW)p(v/uni007C.varh,W)\n\n13548 S. F. Ahmed et al.\n1 3\nThe obtained network weight matrix of DBN was then shifted into the backpropagation \nneural network to enroll the supervised state. In the supervised form, the backpropaga-\ntion neural network was evaluated on Levenberg Marquardt and Conjugate Gradient algo-\nrithms. The proposed methodology showed 99.68% accuracy, which outperforms prior \nstudies. Therefore, this work proposes an efficient system to construct an accurate breast \ncancer classification model. However, a deep belief network needs significant computa-\ntional effort on hardware, and thus building a real-life computer-aided diagnosis system \nbased on DBN is very challenging.\nZhao et al. (2017) proposed a feature learning technique named discriminant DBN for \nsynthetic aperture radar (SAR) image classification. In the study, discriminant features \nwere obtained in an unsupervised way by integrating the ensemble-learning technique with \na DBN. Some SAR image patch subsets were organized and labelled for training weak \nclassifiers, then the particular patch was defined by projection vectors. The SAR image \npatch was projected into each weak decision space covered by weak classifiers. The mod-\nel’s performance was found to be better than other proposed approaches in this domain. \nHowever, since fixed neighbors govern the model, the weak classifier’s training strategy’s \nneighbor selection process may cause significant variance in pseudo-labelling. Some adap-\ntive strategies can be utilized to choose specific samples for training the weak classifiers.\nAnother deep belief network, namely convolutional deep belief network (CDBN) is a \nhierarchical generative model for a real size image. RBM and DBN find it challenging to \nscale to complete pictures since they do not take into account the 2D form of the image, \nand therefore, the weights for detecting a specific feature must be acquired separately for \neach position. CDBN addresses this issue by scaling to the size of real images. The key to \nthis solution is probabilistic max-pooling, a new strategy for shrinking higher layer rep-\nresentations in a probabilistically sound manner. This model stacks convolutional RBMs \n(CRBMs) to construct a multilayer structure similar to DBNs. The CRBM is analogous \nto RBM, except the weight among the hidden and visible layers is distributed over each \nposition in the image. By integrating the energy functions of all individual layer pairs, the \nsystem generates an energy function. After training the given layer, the weights and activa-\ntions of the layer are frozen and passed on to the next layer as input.\nAssume a CDBN with detection layer ( H ), visible layer ( V ), pooling layer ( P ), and \nanother higher detection layer ( H/uni2032.var ); H/uni2032.var and K/uni2032.var has groups, shared weights Γ={ Γ1,1…Γk,k�} \nconnects pooling unit Pk and detection unit Hl . The energy function can be described as \n(Lee et al. 2009):\nBased on CNN structure, Wu et al. (2018) presented a novel technique for pathological \nvoice detection, in which the weights of the CNN are pre-trained by a CDBN. The model \nuses statistical approaches to detect the structure of the input data. The performance of \nthe proposed technique was compared with the existing techniques using the Saarbrucken \nvoice database. Generative models are generally used to develop the deep learning models \non a small dataset and avoid overfitting. The study reported an accuracy of 68% and 71% \non the validation set, respectively. This is a slight improvement compared to other exist-\ning methods. The results demonstrated that CNN can be tuned more robustly by applying \nCDBN to initiate the weights and can avoid the overfitting issue. However, the accuracy for \nthe testing set decreased, which proves that a more robust system might affect the accuracy.(31)E/parenleft.s1v,h,p,h�/parenright.s1=−/uni2211.s1\nkv◦/parenleft.s1Wk∗hk/parenright.s1−/uni2211.s1\nkbk/uni2211.s1\nijhk\nij−/uni2211.s1\nk,lpk◦/parenleft.s1Γkl∗h�l/parenright.s1−/uni2211.s1\nlb�\nl/uni2211.s1\nijh�l\nij\n\n13549\nDeep learning modelling techniques: current progress,…\n1 3\nA Gaussian Bernoulli-based CDBN (GCDBN) model is made up of many coevolution-\nary layers that are built on Gaussian Bernoulli restricted Boltzmann machines (GBRBM). \nTherefore, the architecture takes the benefit of GBRBM and convolution neural networks. \nAfter each convolutional layer, the feature maps are down-sampled using a stochastic \npooling layer. Using a convolutional neural network and GRBM, the proposed system \ncan extract relevant features from a real-sized image using generative convolution filters, \nreducing the amount of connecting weights, and improving the learning of spatial informa-\ntion from nearby picture patches. Li et al. (2019a, b, c, d) proposed the GCDBN model \nfor image feature extraction, which can reduce the computational cost significantly by \nreplacing fully connected weights with the convolutional filter. However, as a limitation of \nthis study, only one GCDBN was built with five layers. The recognition accuracy can be \nincreased by adding more convolutional and pooling layers in the proposed architecture.\nDBNs are also widely used in the analysis of hyperspectral imaging (HSI). However, \nthey fail to examine training samples’ prior knowledge, limiting the discriminant capacity \nof retrieved features for classification. MMDBN, a manifold-based multi-DBN was thus \nproposed by Li et  al. (2022) in order to acquire deep manifold characteristics of hyper -\nspectral imaging. The MMDBN created a hierarchical initiation approach that initializes \nthe network based on the data’s hidden local geometric structure. The MMDBN algorithm \nefficiently extracted the deep characteristics from each HSI class. Experimental findings \non the Salinas, Botswana and Indian Pines datasets reach 90.48%, 97.35%, and 78.25%, \nrespectively, demonstrating that MMDBN outperforms some state-of-the-art algorithms in \nclassification performance. MMDBN’s classification performance can be further improved \nby designing the combined spectral-spatial deep manifold networks.\n4.8  Hybrid neural network\nThe process of artificial neural network (ANN) learning entails predicting values for a set \nof parameters and an architecture (Guti 2011). After choosing an architecture, supervised, \nunsupervised, or reinforcement learning is often accomplished by repetitively modifying \nthe connection weights using a gradient descent-based optimization method. The signifi-\ncant challenges with this type of technique are the need for a prior-determined architecture \nfor the neural net, its sensitivity to early training conditions, and its local nature. Several \nactivations or transfer methods have been used for the hidden layer nodes in hybrid mod-\nels. Many studies have suggested hybridizing various basis functions via a single hybrid \nhidden layer or different linked pure layers. A hybrid neural network (HNN) was initially \nintroduced to model a fed-batch bioreactor [36]. The hybrid model is comprised of a partial \nfirst principal model that provides previous information about the process with a neural \nnetwork, which acts as an estimate of unmeasured process arguments.\nA genetic algorithm is a type of evolutionary algorithm that uses evolutionary biology \nconcepts like inheritance and mutation. A number of operators (selection operator, substi-\ntution operator, recombination operator, and mutation operator) are used in genetic algo-\nrithms to bring together the current generation’s eligible members to produce new eligi-\nble members. Arabasadi et al. (2017) developed a hybrid technique that combines genetic \nalgorithms with neural networks for diagnosing coronary artery disease, using Gini-index, \nprincipal component analysis, information-gain, and weight-by-SVM for feature selection. \nThe initial weights of a neural network were determined with a genetic algorithm, then \nthe neural network was trained using training data. The proposed technique implements a \n\n13550 S. F. Ahmed et al.\n1 3\nfeed-forward topology with one hidden layer in the neural network. The experiment con-\ntained 22 inputs and five neurons in a hidden layer to produce an output that indicates \nwhether the patient has CAD or not. The suggested approach improved the performance of \na neural network by around 10% by upgrading its initial weights with a genetic algorithm \nthat offers better weights for the neural network. However, several limitations were found \nin this study. Instead of genetic algorithms, other established evolutionary algorithms \nlike evolution strategy and Particle-Swarm-Optimization (PSO) could be implemented to \nensure the validity of the model. Some parameters, such as momentum factor and learning \nrate, could also be optimized.\nA novel metaheuristic method was suggested for improving the free parameters of the \nPV generation forecasting engine. Using this metaheuristic optimization approach, the \nshark-smell-optimization (SSO) technique has been enhanced. The metaheuristic algorithm \nincorporates efficient operators to improve its global and local search capabilities. A new \nforecasting methodology was applied to a hybrid forecasting engine that combines a neu-\nral network with a metaheuristic algorithm (Abedinia, Amjady, and Ghadimi 2018). This \nmethod includes a two-stage feature selection filter that filters out inefficient inputs using \ninformation-theoretic criteria, such as mutual information and interaction gain. For PV \ngeneration prediction, a three-stage neural network-based forecasting engine was designed \nand trained via a combination of a metaheuristic algorithm and the Levenberg–Marquardt \nlearning method. With the help of this hybrid technique, the neural network-based forecast-\ning engine eliminated underfitting and overfitting problems.\nAn HNN with Wavelet Transform and Bayesian Optimization was used in a study con-\nducted by (Liu et al. 2022) to predict the copper price for the short-term and long-term. \nWavelet Transform was applied to the data to reduce noise and remove extraneous informa-\ntion whereas the algorithm of Bayesian Optimization was utilized on the searching task’s \nhyperparameter. For training and forecasting copper price, GRU and LSTM were used. \nThe results showed that the proposed approaches, GRU or LSTM, can accurately forecast \nthe copper price in the short and long term with the mean squared errors of less than 3% \nin both cases. With this HNN, the unnecessary data can be filtered out while the optimal \nhyperparameter set is searched. It is simple and straightforward to use in predicting the \nprice of other commodities such as the stock market.\n4.8.1  Probabilistic neural network (PNN) and two‑layered restricted Boltzmann (RBM)\nA hybrid deep learning model was presented by Ghosh, Ravi, and Ravi (2016) for senti-\nment classification that combines a two-layered restricted Boltzmann machine (RBM) and \nprobabilistic neural network (PNN). Sentiment classification is a sub-domain of sentiment \nanalysis that identifies positive and negative sentiments from a review. In the proposed \narchitecture, RBM was used for dimensionality reduction, and PNN classified the senti-\nment. The hybrid model was assessed in five datasets and performed better than other exist-\ning models in this domain. The technique achieved a sensitivity of 92.7%,93.3%, 93.1%, \n94.9%, and 93.2% for a book dataset, movie dataset, electronics, and kitchen appliance \ndataset, respectively. The study revealed that the model does not rely on external resources, \nsuch as sentiment dictionaries, reducing the system’s complexity. It also does not perform \nPOS tagging, which, although is typically needed in this domain, reduces the system’s \ntime complexity. In future works, the model should be evaluated with more experiments to \nensure its validity.\n\n13551\nDeep learning modelling techniques: current progress,…\n1 3\n4.8.2  Dynamic artificial neural network\nIn the field of deep learning, dynamic neural networks (DNNs) are an emerging technique \nthat can outperform traditional static models in terms of accuracy, adaptiveness, and compu-\ntational complexity (Han et al. 2021). Static models have limited parameters and computa-\ntional graphs at the inference stage, whereas DNN architecture and parameters are flexible \nto different inputs. The outputs of static models are computed based on their link with feed-\nforward inputs, as there is no feedback. However, the outputs of dynamic neural networks are \ndetermined by the present and previous values of inputs, outputs, and the network architec-\nture (Abbas Ali Abounoori Esmaeil Naderi Nadiya Gandali Alikhani Hanieh Mohammadali \n2016). DNNs can be divided into three types (Tavarone et al 2018): (i) instance-wise dynamic \nmodels that process each instance individually using data-dependent structures or parameters, \n(ii) spatial-wise dynamic models that perform adaptive computing on image data at various \nspatial locations, and (iii) temporal-wise dynamic models that accomplish adaptive inference \nfor sequential data, such as movies and texts along the temporal dimension. Instance-wise and \nspatial-wise methods are used specifically in image recognition, whereas temporal-wise mod-\nels show emerging improvements in text and audio data. These three types can be combined \nsimultaneously for video-related research domains (Li et al. 2017a, b; Niklaus et al. 2017).\nGodarzi et al. (2014) improved an artificial neural network (ANN), specifically named a \nnonlinear autoregressive model with eXogenous input (NARX), to predict oil prices by devel-\noping a dynamic neural network. For the validation and improvements of results, the method-\nology followed three stages: ANN static, time series, and NARX. For identifying the signifi-\ncant factors that affect the oil price, a time series model was developed in the first stage. Then, \na static ANN model was built to verify the acquired data from the first stage to ensure the opti-\nmal performance of the NARX model. In the last phase, the NARX model was implemented \nfor the prediction. The methodology was found to be a novel approach for oil price prediction \nand can be used for other domains like predicting coal or natural gas price.\n4.9  Generative adversarial networks\nGoodfellow et al. (2014) was the pioneer of adversarial training for image generation, whereby \ntraining is formulated as a minimax adversarial game and a discriminator is used to distin-\nguish fake data from real samples. The generator works by generating fake samples based \non a probabilistic model with the given data. Then, a classification model is applied to verify \nwhether the generated samples belong to the expected class. The generator aims to fool the \ndiscriminator, whereas the discriminator works to detect the false samples generated by the \ngenerator. Generative models have been used in a wide range of research domains and have \nundergone numerous advances since their introduction (Bau et al. 2019; Odena et al. 2017; \nBrock et al. 2019; Ledig et al. 2017; Miyato et al. 2018; Karras et al. 2018). In every adversar -\nial approach, there are two models working simultaneously: (i) the generative model acquires \nthe data distribution, and (ii) the discriminative model measures the probability of sample \npoint whether it is coming from the training samples. Generative adversarial networks (GAN) \nlearning concerns finding the optimal parameters /u1D703∗\nG for a generator function G/parenleft.s1Z;/u1D703G/parenright.s1 by a \nminimax game. This relation can be represented by the following expressions (32– 35), as sug-\ngested by Goodfellow et al. (2014):\n(32) /u1D703∗\nG=/u1D703Gargmin\n/u1D703Dmaxf/parenleft.s1/u1D703G,/u1D703D/parenright.s1\n\n13552 S. F. Ahmed et al.\n1 3\nwhere f is determined by:\nFor ensuring maximum loss in the above equation, the optimal discriminator D∗(x) is \na known smooth function for the generator probability pG(x) , as described in (Goodfel-\nlow et al. 2014). The smooth function can be formulated as:\nGAN has been in a wide range of applications since its emergence. Generative \napproaches are being applied to validate machine learning models’ robustness and to \ngenerate new data for rare examples and for image-to-image translation (Park et  al. \n2019; Taigman et al. 2017; Xu et al. 2018), image super-resolution (Ledig et al. 2017; \nSønderby et al. 2017), synthesis training (Brock et al. 2019; Tang et al. 2019), text-to-\nimage synthesis (Hong et al. 2018; Zhang et al. 2017a, b, c), and many more. However, \nthe training of generative models is very sensitive to the selected hyperparameters. New \nnetwork architectures have been introduced on a regular basis to this research paradigm \nin order to maintain training stability.\n4.9.1  Unrolled generative adversarial networks\nTo solve the problems of mode collapse, instability of GANs network training with \ncomplex recurrent generators, and increasing diversity, Pfau (2017) introduced a method \nfor reducing complexity in GANs training. The proposed algorithm defines the genera-\ntor’s objective in order to achieve an unrolled optimization of the discriminator. The \nauthors argued to use a local optimum of the discriminator parameters /u1D703∗\nD(as presented \nin Eq.  34) to be demonstrated as a fixed point, which comes from an iterative optimiza-\ntion procedure. Pfau (2017) developed the complex recurrent generators increasing the \ndiversity and scope of the data distribution. To explain the unrolled GAN, the authors \nused the discriminator parameter /u1D703∗\nD to express the fixed mark of an iterative optimiza-\ntion process. The expression continues in the following order (15–17):\nwhere /u1D702k represents the learning rate scheduler. Equation  (37) is the full batch steepest \ngradient ascent equation, and Eqs. (36) and (38) supplement the expression to explain the (33) =/u1D703Gargminf(/u1D703G,/u1D703∗\nD(/u1D703G))\n(34) /u1D703∗\nD/parenleft.s1/u1D703G/parenright.s1=/u1D703Gargmaxf/parenleft.s1/u1D703G,/u1D703D/parenright.s1\n(35) f/parenleft.s1/u1D703G,/u1D703D/parenright.s1=/u1D53Cx∼pdata/bracketleft.s1log/parenleft.s1D/parenleft.s1x;/u1D703D/parenright.s1/parenright.s1/bracketright.s1+/u1D53CZ∼N(0,1)/bracketleft.s1log(1−D(G(Z ;/u1D703G/parenright.s1;/u1D703D))]\n(36) D∗(x)=pdata(x)\npdata(x)+pG(x)\n(37) /u1D7030\nD=/u1D703D\n(38) =/u1D703k\nD+/u1D702kdf(/u1D703G,/u1D703k\nD)\nd/u1D703K\nD\n(39) /u1D703∗\nD/parenleft.s1/u1D703G/parenright.s1= lim\nk→∞/u1D703k\nD\n\n13553\nDeep learning modelling techniques: current progress,…\n1 3\niterative optimization process. This approach is different from that presented in (Good-\nfellow et  al. 2014), which indicates that the generator requires that the discriminator be \nupdated via several steps to run every update step for the generator. Some drawbacks of \nthis algorithm include high computational cost and cost for each training period as well as \nincreased complexity with respect to the number of steps.\n4.9.2  Style‑based generator architecture for generative adversarial networks\nMotivated by the style-transfer model presented in (Huang and Belongie 2017), an alterna-\ntive generator architecture was proposed in (Karras et al. 2019) for GANs. The presented \ngenerator improved the state-of-the-art work with regard to traditional distribution matri-\nces, which continued towards finding better interpolation properties and latent factors vari-\nation. The authors stated that compared to the traditional generators (Karras et al. 2018) \nthat are used to feed the latent code within the input layer, their architecture (Karras et al. \n2019) allows input to be mapped through an intermediate space. This latent space then \nallows control of the generator through the adaptive instance normalization or AdaIN \n(Dumoulin et al. 2018, 2017; Ghiasi et al. 2017; Huang and Belongie 2017) within every \nconvolutional layer. The proposed automated linear separability and perceptual path met-\nrics quantified the aspects needed for the generator.\nThe affine transformations learned from the 8-layer MLP was specialized by a parame-\nter w to styles y,  where y = (ys, yb), which led to AdaIN (Dumoulin et al. 2018, 2017; Ghiasi \net al. 2017; Huang and Belongie 2017). Followed by the synthesis network g of each con-\nvolution layer, AdaIN function performs the computation as follows:\nwhere each feature map xi is normalized individually, the feature matrix is scaled, and bias \nis added by applying the respective scalar components from the style y . Karras et al. (2019) \nredesigned the generator architecture, which exposed new approaches for image synthesis \ntasks. It is clear from the obtained results that style-based generators outperform the tradi-\ntional GAN generators.\n4.9.3  Multi‑Level generative models for partial label learning (MGPLL) \nwith non‑random label noise\nThe presented MGPLL method (Yan and Guo 2020) learns a problem through a feature \nlevel and label level generator. It follows a bidirectional mapping of data points and label \nvectors. A noise label generation is also used while developing the network to form non-\nrandom noise and to execute label denoising. The model architecture has a multi-class pre-\ndictor to locate the training samples to denoise label vectors. Afterwards, a conditional \nfeature generator is applied to perform the inverse mapping. Yan and Guo (2020) adopted \nadversarial loss from Wasserstein Generative Adversarial Network (WGAN) to formulate \ntheir learning. They claimed their model to be the pioneering work that exploited multi-\nlevel generative architecture models. Moreover, the network was modelled with non-ran-\ndom noise labels in order to learn the partial label (Zeng et al. 2013). The noise label gen-\nerator was responsible for exploiting non-random characteristics of noise labels, whereas (40) AdaIN (xi,y)=ys,ixi−/u1D707/parenleft.s1\nxi/parenright.s1\n/u1D70E/parenleft.s1xi/parenright.s1+yb,i\n\n13554 S. F. Ahmed et al.\n1 3\nthe data feature generator was accountable for executing the conditioning upon the data \nsamples based on the particular ground data. Later, the prediction model performed inverse \nmapping between these labels and features. The GAN architecture was designed particu-\nlarly for label learning partially. The conditional label level generator pointed to the advent \nof the label-dependent non-random noise, whereas the feature level generator was used to \nproduce data from the denoised label vectors. As a partial label learning generative archi-\ntecture, the authors tested the model against both synthetic PL and real-world (FG-NET, \nLost, MSRCv2, Birdsong, Yahoo! News) datasets, where they achieved satisfactory state-\nof-the-art performance.\n4.9.4  Dual adversarial co‑learning for multi‑domain text classification\nMulti-domain sentiment classification was performed by Wu and Guo (2020) through the \nnovel dual adversarial co-learning method. The authors explored a number of real-world \nsentiment analysis tasks and demonstrated how multi-domain text classification (MDTC) \naddresses the problem of a model constructed for one domain failing when tested on \nanother domain. The methodology focuses on domain-invariant and domain-specific fea-\ntures by shared-private networks, and two classifiers were trained to extract features. Both \nthe classifiers and feature extractors were designed to work in an adversarial manner, which \nresulted in the basis of prediction discrepancy on unlabeled data. A multinomial multi-\ndomain adversarial discriminator was developed to enhance the effectiveness of feature \nextraction of the domain invariant features. This technique separates the domain-specific \nfeatures from the domain invariant features. The presented methodology is novel in such a \nway that the network tries to align data across domains within the extracted feature space \nand labelled and unlabeled data within each domain. This technique also contributes to \navoiding overfitting the limited labelled data.\nAccording to Wu and Guo (2020), if each of the M domains has a limited number of \nlabelled instances, then Lm= {(xi,yi)}lm\ni=1 and unlabeled instances Um= {(xi)}um\ni=1 . In the \nstudy, the challenge was to make use of all available resources of the M domains. The \nauthors reported that this helped to improve the multi-domain classification performance. \nThey furthermore introduced separation regularizer (Bousmalis et al. 2016; Liu et al. 2016) \nto ensure that domain-specific extractors remained distinct from the extractors, which are \ndomain-invariant. The introduced methodology was designed to pull features from domain \ninvariant and domain-specific literature. The shared private network was used to pass \nthe extracted features from the texts, followed by two classifiers that work together in an \nadversarial fashion. A multinomial multi-domain discriminator was applied to increase \nthe effectiveness of domain-invariant feature extraction. The authors tested this model on \ntwo MDTC benchmark datasets and for unsupervised domain adaptability. The generative \nmodel positions data with respect to extracted feature space and distinguishes labelled and \nunlabeled data between each domain. However, the model should be more robust to avoid \noverfitting for limited data samples.\n4.9.5  Capsule neural network\nA capsule neural network (CapsNet) was first introduced by Sabour et al. (2017) to address \na few drawbacks of the convolutional neural network (CNN). For instance, the sub-sam-\npling layers involved in CNN provide less translation invariance. Also, CNN loses the \n\n13555\nDeep learning modelling techniques: current progress,…\n1 3\ninformation about location and position estimation and is more prone to overfitting train-\ning data for these reasons. It learns the features without understanding the spatial infor -\nmation. Thus, most of the CNN models are not effective to avoid misclassification. Cap-\nsNet addresses these issues by avoiding the sub-sampling layers, which helps the model to \nmaintain the spatial and pose information. The idea of capsules was introduced by Hinton \net al. (2011). CapsNets use these “capsule” neural units to encode the relationship between \nfeatures and location with capsules as well as transformation matrices. Since this approach \nacquires translation equivariance, CapsNets are more powerful than CNN for samples with \nmisled spatial and pose information.\nThe dynamic routing algorithm (Sabour et al. 2017) also helps CapsNets to overcome \nthe inability of features to acquire spatial information and scarcity of rotational invariance. \nCapsNets also encode part-whole relationships like orientations, brightness, and scales \namong different entities that are objects’ features or feature parts. They use shallow CNN \nto acquire spatial information. However, CapsNets perform poorly on classification tasks \nfor missing semantic information. For shallow convolutional architecture, a high number of \nconvolutional kernels are used to provide the network with a broad receptive field, but this \napproach is also prone to overfitting. Since their inception, CapsNets has been employed \nin various researches, including cancer and tumor cell detection (Mobiny and Van Nguyen \n2018; Afshar et al. 2018), generative adversarial network (Jaiswal et al. 2019), monitor -\ning machine health (Zhu et  al. 2019), object height classification (Popperli et  al. 2019), \nrice image recognition (Li et al. 2019a, b, c, d), protein translational analysis (Wang et al. \n2019), hyperspectral images (Landgrebe 2002), and many more.\nHyperspectral images are used for agriculture (Gevaert et al. 2015), land coverage clas-\nsification (Yan et al. 2015), vegetation and water resource studies (Govender et al. 2007), \nscene classification (Hu et al. 2015), and other environmental monitoring related activities. \nDeng et al. (2018) presented two-layered CapsNet, which was trained on less training sam-\nples than Hyperspectral Image (HSI) classification. The work was motivated by the sim-\nplicity and comparability of shallower deep networks. The model was trained on two real-\nlife HSI data: PaviaU (PU) and Salins A. Upon the observation, CapsNet gave an overall \naccuracy of 94% and an average accuracy of 95.90% on the PU dataset, whereas CNN had \n93.45% and 95.63% accuracy, respectively. The study also made a comparison among Ran-\ndom Forests, Support Vector Machines, and CNN with CapsNet in terms of network archi-\ntecture. The authors stated that traditional deep learning-based models would not be suit-\nable for HSI datasets (Zhong et al. 2018) and that CNN could achieve higher performance \nwith more training samples, but for limited training data, CapsNet worked better. Figure 12 \nshows the native logic for Hyperspectral Image (HSI) classification in its conceptual form.\nCapsNet was also used in another HSI study (Jiang et al. 2020), in which a new model \ncalled Conv-Caps was designed by integrating CNN and a capsule network with Markov \nRandom Fields (MRF) for possessing spectral as well as spatial information. With MRF, \nthe study used graph cut expansion for more efficient classification performance. A CNN-\nbased feature extractor was also used in the network design. In the model, the layer was \nfollowed by a feature map in order to obtain a probability map. In the last stage, MRF \nwas used to find subdivision labels. This method takes proper advantage of the spectral \nand spatial information that hyperspectral images provide. The model was evaluated with \na Bayesian framework perspective and produced satisfactory results. To make capsule net-\nworks more robust, various research approaches have been introduced over time, a few of \nwhich are presented below.\n\n13556 S. F. Ahmed et al.\n1 3\n4.9.6  Multi‑lane capsule network\nMulti-Lane Capsule Network (MLCN) was introduced by Do Rosario et  al. (2019) to \naddress the limitation of traditional Capsule Networks. The algorithm was tested on the \nreputed FashionMNIST and CIFAR10 datasets. When compared to traditional CapsNet \narchitectures, the authors achieved satisfactory outcomes with their novel lane proposals. \nThe experimental baseline was similar to the original configuration employed in (Sabour \net al. 2017). According to the findings of do Rosario et al. (2021), MLCN was found to be \ntwo times more efficient, on average, than the typical capsule network. The authors intro-\nduced the problem of load balancing that occurs when distributing heterogeneous lanes \nwithin both homogeneous and heterogeneous accelerators. They addressed this issue with \na greedy approach, which was argued to be 50% more efficient than the brute force naive \napproach. Furthermore, the load balancing issue was handled by the neural architecture \nsearch created by their MLCN models, which matched device memory.\nChang and Liu (2020) improved the MLCN algorithm by addressing the issue of cap-\nsule networks creating undesirable priorities in the background, which usually results in \npoor performance if the background contains too much variance. The authors proposed a \nnewly configured multi-lane capsule network architecture with a strict-squash (MLSCN) \nfunction for image classification with a complex background to solve this issue. The novel \narchitecture replaced the traditional squash function and optimized the dropout function \nd. The strict-squash algorithm was proposed to prevent the vulnerability of dynamic rout-\ning while also limiting the uselessness of the capsule initialization features. For meaning-\nful feature extraction, the authors also proposed a coherent dynamic weighting assignment \nstrategy in the multi-lane module. By combining these two methods, the authors recom-\nmended MLSCN on the basis of MLCN. The research work focused on addressing the \nFig. 12  HSI classification overview presented in (Deng et al. 2018)\n\n13557\nDeep learning modelling techniques: current progress,…\n1 3\nissue of misclassification of images with complex backgrounds. This issue can be repre-\nsented with the input formalized as below (Chang and Liu 2020):\nwhere gij is the input pixel value in location ( i,j ). After the convolutional network pro-\ncesses, the feature map can be obtained using:\nwhere  ̂gij is the output pixel value in the location ( i,j ); and Fi×j\nout is the capsule layer input, \nwhich is responsible to finish the classification step. Following this step, the output layer \ncan be defined as:\nwhere pi is the probability for each category. Most of the regions of an input image have \na background as the content or information; however, this information is useless as it is \nthe background of the image. Yet, the capsule network provides redundant attention to the \ninformation. As a result, it was identified as the fundamental cause of poor performance in \ntraditional capsule networks. This problem was solved using the aforementioned network \ncombined with the original capsule network along with multi-lane architectures. Chang \nand Liu (2020) improved their work by making three major contributions: the strict-squash \nfunction, lanes filter, and drop-circuit.\nIf ui is activation vector of the capsule i  of the previous layer, Vj/uni007C.vari is the inclination \nof the capsule i moving to be clustered in capsule j . The relation between these two \nparameters can be formalized by the following equation (Chang and Liu 2020):\nThe summation of coupling coefficients between i  and the other previous capsules \nequals 1, which was achieved by a ‘routing SoftMax’ in which the initial logits bij are \nprior probabilities and the capsule i  must be coupled with capsule j. Equations (44)-(47) \nare used to perform the necessary computations for the model architecture (Chang and \nLiu 2020).\nThe squash function is interpreted as a normalization step upon the weighted sum \nfrom the previous layers and is presented as:(41) Gi×j\nin= ((g11, ...,g1j),(g21, ...,g2j), ..., (gi1, ...,gij))\n(42) Fi×j\nout= (( ̂g11, ..., ̂g1j),(̂g21, ..., ̂g2j), ..., (̂gi1, ..., ̂gij))\nP={p1,p2, ...pj}\n(43) Vj/uni007C.vari=wj/uni007C.vari×ui\n(44) cij=exp�\nbij�\n∑\nkbik\n(45)sj=/uni2211.s1\ni(cj/uni007C.vari×vj/uni007C.vari)\n(46) uj=Squash/parenleft.s1\nSj/parenright.s1\n=/uni007C.var/uni007C.x/uni007C.x/uni007C.xSj/uni007C.x/uni007C.x/uni007C.x/uni007C.var2\n1+/uni007C.var/uni007C.x/uni007C.x/uni007C.xSj/uni007C.x/uni007C.x/uni007C.x/uni007C.var2Sj\n/uni007C.var/uni007C.x/uni007C.x/uni007C.xSj/uni007C.x/uni007C.x/uni007C.x/uni007C.var\n\n13558 S. F. Ahmed et al.\n1 3\nFinally, to compute cj/uni007C.vari and update ui or vj/uni007C.vari , the following equation is used, where uj is \nthe result of the first iteration:\nBased on the best classification performance on four benchmark image classification \ndatasets, Chang and Liu (2020) found that, in comparison with a single input type, mul-\ntiple input types can help the multi-lane architecture to achieve better results. One short-\ncoming of their research was the drop-circuit, which could not recognize the combined \nadapted lanes. Consequently, the dropout algorithm would need further research as it \nestablishes randomness in the experimental results.\n4.9.7  Complex‑valued capsule network (Cv‑CapsNet)\nTo adjust complex datasets, He et  al. (2019) focuses on the extraction of multi-scale, \ncomplex-valued, and high-level features. Moreover, they introduced an algorithm with a \nrestricted encoding unit of the complex-valued capsule and dense network, with a generali-\nzation of the dynamic routing in the complex-valued realm. The generalized dynamic rout-\ning algorithm was used to fuse the real- and imaginary values of complex-valued primary \ncapsules. The parameters trained for complex-valued routing were lowered when compared \nto real-valued routing of the same dimensional capsules. He et al. (2019) also introduced \nCv-CapsNet +  + as an extended framework utilizing a 3-level Cv-CapsNet model. It was \ndesigned for multi-scale high-level complex-value feature extraction and merging the low-\nlevel capsules information that represents the features of instantiation. In addition, Trabelsi \net al. (2018) presented a method to simulate complex and real-valued convolution, which \nwas demonstrated for a complex-valued filter matrix W=(A+iB) and a complex-valued \nvector h=(x+iy) using the following computation:\nReal-valued matrices were also presented to introduce the real and imaginary parts in \nEq. (49)\nHere, the real and the imaginary components of the output convolutions are two sepa-\nrate parts. Moreover, the real and the imaginary part for all complex-valued convolutions \nare detached from each other but concatenated with respect to the real and complex parts \nfor the following complex-part layer. He et al. (2019) argued that this modelling guaran-\ntees the sustainability of the complex-valued convolutions and ensures the complex-valued \nencoding. Thus, the architecture was employed to fetch multi-scale features, including orig-\ninal, semantic, and structure features. In the model, CReLU (complex-valued) (Trabelsi \net  al. 2018) was chosen as the activation function. The authors implemented the model \non CIFAR10 Fashion and MNIST datasets. The model performed well by achieving fewer \ntrainable parameters with a smaller number of iterations. The generalized dynamic routing \nalgorithm helped to combine the real values with the imaginary values, greatly reducing \nthe number of trainable parameters for the same dimensional complex routing model as \ncompared to the real-valued routing models. However, they could not reduce the computa-\ntional complexity for training the model.(47) bj/uni007C.vari=bj/uni007C.vari+vj/uni007C.vari×uj\n(48) W∗h=(A+iB)∗(x+iy)\n(49)/bracketleft.s3\nℜ(W ∗h)\nℑ(W ∗h)/bracketright.s3\n=/bracketleft.s3\nA−B\nBA/bracketright.s3\n∗/bracketleft.s3\nx\ny/bracketright.s3\n\n13559\nDeep learning modelling techniques: current progress,…\n1 3\n4.9.8  Multi‑scale CapsNet\nA novel variation of capsule networks was introduced by Xiang et al. (2018), focusing on \ncomputational efficacy and representation capacity. In the leading stage of the presented \nmulti-scale architecture, information was extracted following the multi-scale information \nextraction method. However, on the second stage hierarchy, the features were encoded into \nmulti-dimensional capsules. An improved drop-out was also introduced in the research \nwork to enhance the robustness of the capsule network. The authors considered the hier -\narchical features of the dataset and exploited multi-dimensional capsules for encoding \nthose features. The multi-scale capsule encoding consists of two stages, where the first \nstage obtains the semantic and structural information through multi-scale feature acqui-\nsition. Another top branch of the two layers retrieved the semantic information from the \ndata as well. The foremost hierarchy of the middle branch of the architecture performed \nthe medium-level feature extraction process. The last branch took on the actual original \nfeatures that were obtained without trainable parameters. In the second stage of the archi-\ntecture, feature hierarchies were encoded into multi-dimensional capsules. The final branch \nlayer was encoded to high-level features of 12D, medium level features of 8D and low-level \nfeatures of 4D. The following weight matrices were used to compute the predicted vectors \n(Xiang et al. 2018):\nEquation (50) is used as the objective function of the multi-category capsule network \n(Xiang et al. 2018):\nThe length of a capsule portrays the probability of the entity, where the length is argued \nto be compressed to [0,1]. Equation  (51) represents that the length can be compressed \nwithout changing its direction and helps in translating the length as the capsule detects the \nactual probability of a given data feature:\nwhere vj represents the capsule output of the j-th unit; and sj is the total input. Dynamic \nrouting was used as a form of the information selection method, which ensures that the \noutputs of the children capsules are sent to their respective parent capsules (Xiang et al. \n2018). On the other side, the routing coefficients are adjusted by the update() function \nshown below:̂u1\nj/uni007C.vari=Wiju1\ni\n̂u2\nj/uni007C.vari=Viju2\ni\n̂u3\nj/uni007C.vari=Uiju3\ni\n̂u=concat (̂u1,̂u2,̂u3)\n(50) LM=J�\nj=1Tj/u1D45A/u1D44E/u1D465�0,m+−‖Vj‖�2+/u1D706�1−Tj�max(0,‖Vj‖−m−)2\n(51) vj=‖sj‖2\n1+‖sj‖2sj\n‖sj‖\n\n13560 S. F. Ahmed et al.\n1 3\nThe authors achieved state-of-the-art performance of the model on FashionMNIST and \nCIFAR10 datasets. MS-CapsNet was also used in the Synthetic Aperture Radar (SAR) \nimage detection task (Xiang et al. 2018). Gao et al. (2021) addressed the issue of noise \ndetection and deformation sensing in traditional CNN architectures with their implemented \nmultiscale capsule network for feature extraction in SAR image pixels. The multiscale \nmodule exploited spatial information from the image features. The authors also applied an \nadaptive fusion convolution module to address the issue of noise detection and tested the \nmodel’s architecture on three real-life SAR datasets.\n4.9.9  Attention mechanism\nThe attention mechanism is described as a mapping mechanism to query and set a key-\nvalue pair to the output. In the output, all of the elements in values, keys, query, and out-\nput are vectors. The output values are produced as a weighted sum of the input values, \nand the weight values are assigned using a compatibility function. The query with respect \nto the associated key generates this compatibility function. Self-attention, also known as \nintra-attention, is such an attention-based mechanism that relates various positions of a unit \nsequence to compute the representation of that sequence input. The self-attention algorithm \nhas been used for reading comprehension (Cheng et al. 2016), textual entailment (Paulus \net  al. 2018), summarization (Parikh et  al. 2016), task-dependent sentence representation \n(Lin et al. 2017), and in many other fields.\nVaswani (2017) introduced the transformer-based attention mechanism for sequence \ntransduction, replacing the recurrent units to employ in encoder-decoder network archi-\ntectures for multi-headed self-attention units. The transformer was trained significantly \nfor translation tasks and was found to be faster than the recurrent and convolutional-based \narchitectures. The model was applied to 2014 WMT English-to-German and 2014 WMT \nEnglish-to-French machine translation work. The encoder was used to map and input \nsequence for symbol representations and to generate an output sequence given the con-\ntinuous representation. The transformer was employed to follow the overall architecture \nwith the help of self-attention as well as the point-wise fully connected layers within the \nencoder-decoder network architecture.\nVaswani (2017) proposed a self-attention algorithm to perform two machine translation \nwork and achieved satisfactory and parallelizable results. The model obtained a 28.4 score \non BLEU for the 2014 WMT English-German machine translation task and a 41.8 score \non the 2014 WMT English-French machine translation work. The model was generalized \nthrough the transformer-based attention mechanism on words, which proved to be advanta-\ngeous over previous researches (Gehring et al. 2017; Kaiser and Sutskever 2016). It was \nsuccessfully implemented to the English constituency parsing task with both large and lim-\nited training samples. However, the authors did not evaluate this model for image, audio, \nand video data.bi+1=bi+̂uj⋅uj\nci+1=softmax (bi+1)\n\n13561\nDeep learning modelling techniques: current progress,…\n1 3\n4.9.10  Deep Boltzmann machines\nDeep Boltzmann Machine (DBM) (Srivastava and Salakhutdinov 2014), a deep neu-\nral network architecture, is trained in a semi-supervised approach. The architecture of \nDBM allows the network to acquire knowledge about complex feature-based relation-\nships. DBMs have a wide range of applications like facial expression recognition (He \net al. 2013), text recognition (Srivastava and Salakhutdinov 2014), person identification \nfrom audio-visual data (Alam et al. 2017), 3D model recognition (Leng et al. 2015), and \nmany more. DBM consists of units that are respective to input data. The hidden units in \na DBM consist of symmetrical-coupled stochastic binary units. Different layers of the \nDBM architecture hold the binary hidden units. Coupling is enabled in consecutive two \nlayers in a top-down and bottom-up approach. Such structure allows DBM to understand \ncomplicated internal representations of input data.\n4.9.11  Deep‑FS: A feature selection algorithm for deep Boltzmann machines\nA deep feature selection algorithm was presented by Taherkhani et  al. (2018), which \nwas argued to have the ability to remove unwanted features from extensively large data-\nsets. Considering that a feature selection algorithm can help improve the performance of \na machine learning model significantly, this algorithm was developed for DBM domain \nwork. The algorithm was used by a Deep Boltzmann Machine and gathered the data \ndistribution in a network. Such an algorithm is capable of embedding feature selection \nwithin a Restricted Boltzmann Machine, as presented in Fig. 13.\nConsidering an RBM of D binary units, if V  is a vector containing states of the D \nunits, there is the set V  ∈ {0,1}D and a vector h, which contains states of the hidden \nunits. If an RBM has F hidden neurons, the F dimensional hidden variables are h ∈ \n{0,1}F. Taherkhani et al. (2018) expressed the joint configuration of V  and h  as defined \nin the following Eq. (52):\n(52) E(/u1D415,h)=−D/uni2211.s1\ni=1F/uni2211.s1\nj=1Wijvihj−D/uni2211.s1\ni=1bivi−F/uni2211.s1\nj=1ajhj\nFig. 13  Representation of a restricted Boltzmann machine comprised of two layers of hidden and visible \nneurons. In the network, there are D visible and F hidden neural units (Taherkhani et al. 2018)\n\n13562 S. F. Ahmed et al.\n1 3\nwhere Wij is the weight connecting the ith visible component vi and the jth hidden compo-\nnent hj ; and bi and aj are the biases connecting to the ith visible units and the jth hidden \nunits, respectively. An energy function was employed by Taherkhani et al. (2018) for the \njoint distribution of the visible and hidden variables, which assignment is demonstrated in \nEq. (53):\nwhere Z is a partition function, also known as the normalizing term. The function Z is \ndefined below:\nThe overall sum was calculated for all pairs (V,h). If V is a D dimensional vector and h \nis an F dimensional binary vector, there are  2D+F different pairs of (V,h) that are possible. \nAdditionally, the visible units are considered to be binary. Moreover, the conditional prob-\nabilities of P(h/uni007C.varV)  and P(V/uni007C.varh) were calculated in (Taherkhani et al. 2018) by the following \nequations:\nFurthermore, these conditional probabilities can be extended as:\nBased on the results of Taherkhani et al. (2018), the novel feature selection algorithm \nwas designed to handle feature selection from large datasets. The algorithm was embedded \ninto DBM classifiers, which helped to handle a reduced quantity of input features with less \nlearning errors from large datasets. The algorithm performed well because of its ability to \nremove irrelevant features from large data. The results demonstrated that more than 45% \nof the features can be reduced from the FashionMNIST dataset, which helped to reduce \nthe network error from 0.97 to 0.90%. In addition, the time of execution was reduced by \nmore than 5.5% for classification tasks. The model was tested on GISETTE, PANCAN, \nand MADELON datasets and showed to be highly effective for all datasets. Specifi-\ncally, it reduced the input features by 81% for GISETTE, 77% for PANCAN, and 57% for \nMADELON datasets.(53) P(/u1D415,h)=1\nZexp(−E(/u1D415,h))\n(54)Z=/uni2211.s1\nV/uni2211.s1\nhexp(−E (/u1D415,h))\n(55) P(h/uni007C.varV)=F/uni220F.s1\nj=1p/parenleft.s1hj/uni007C.varV/parenright.s1\n(56) P(V/uni007C.varh)=D/uni220F.s1\ni=1p/parenleft.s1vi/uni007C.varh/parenright.s1\n(57) p/parenleft.s1hj=1/uni007C.var/u1D415/parenright.s1= g(D/uni2211.s1\ni=1Wijvi+aj)\n(58) p/parenleft.s1vi=1/uni007C.var/u1D421/parenright.s1= g(F/uni2211.s1\nj=1Wijhj+bi)\n\n13563\nDeep learning modelling techniques: current progress,…\n1 3\n4.9.12  Restricted Boltzmann machine\nRestricted Boltzmann machine (RBM) is a variant of the Boltzmann Machine, contain-\ning a stochastic neural network (generally) for unsupervised learning (Guo et al. 2016). \nUnlike other Boltzmann machines, RBMs have a defining trait of providing a bipartite \ngraph for its visible and hidden layers, enabling the implementation of a gradient-based \ncontrastive divergence algorithm for training. Developed RBM models use noisy recti-\nfied units (linear) to store data on intensities. To create learning modules, RBMs can \nbe efficiently applied to compose deep networking models, such as Deep Energy Mod-\nels (DBNs), Deep Boltzmann Machines (DBMs), and Deep Belief Networks (DBNs). \nGenerally, RBMs are not a popular choice for computer vision-based applications; how -\never, in recent times, a few RBM models have been structured to perform vision tasks. \nFor example, Shape Boltzmann Machine, proposed by Eslami et al. (2014), can learn \nto apply the probability distribution method on object shapes to model binary shape \nimages.\nAnother prominent use of RBMs, suggested by Kae et al. (2013), is in combination \nwith CRF to model local and global structures for face segmentation with improved \nperformance in face labelling. Furthermore, another novel method based on DBN archi-\ntecture and mean-covariance RBM was employed for phone recognition. Various frame-\nworks and models for RBMs have been intensively studied and developed, each having \nits own sets of merits and demerits. Although most RBMs that are utilized for vision \ntasks exhibit remarkable capability in performing image and object classification/iden-\ntifying tasks, such models must be a hybrid of one or more networks to be efficient. \nAs of yet, standard RBMs alone are not adopted for memory associative or computer \nvision-based tasks and are usually in compliance with more than one other deep learn-\ning framework.\n4.9.13  Sequence classification restricted Boltzmann machines with gated units\nThe intractability of learning and inference in RBM was investigated by Tran et  al. \n(2020) considering the exponential complexity of the gradient computation while \nmaximizing the log-likelihoods. The algorithm optimized a conditional probability \ndistribution in place of a joint probability distribution for sequence classification. The \nauthors also introduced gated-Sequence Classification Restricted Boltzmann Machine \n(gSCRBM), in which an information processing gate is integrated alongside long short-\nterm memory (LSTM) networks. The network architecture was evaluated in an optical \ncharacter recognition (OCR) task and for multi-resident activity recognition in smart \nhomes. It was argued that gSCRBM requires much fewer parameters compared to other \nrecurrent architectures with memory gates. The SCRBM was constructed by the roll-\ning RBMs along with the class label over the time of training. The network architecture \ninterpreted the probability distribution with the following equation:\nwhere /u1D4311∶T , /u1D4211∶T are the time series corresponding to the visible and hidden states; y1∶T is a \nsequence of class labels; and /u1D4210 are the hidden unit biases.(59) p/parenleft.s1y1∶T,x1∶T,h1∶T/parenright.s1=T/uni220F.s1\nt=1p(yt,xt,ht/uni007C.varht−1)\n\n13564 S. F. Ahmed et al.\n1 3\nThe model faced difficulty with an intractable inference, as explained in (Sutskever and \nHinton 2007). The authors also suggested that this problem could be solved through the \naddition of recurrent units, as done for RTRBM (Sutskever et al. 2009). For RTRBM, the \nclass labels were excluded, while in the case of SCRBMs, local distribution at time t was  \np(yt,/u1D431t,/u1D421t/uni007C.var/u1D421t−1) . This was replaced by the expression presented in Eq. (60):\nwhere ̂/u1D421t−1 is the expected values vector for the hidden units at time t-1 and is calculated as:\nThe local energy function is given by:\nThe algorithm was designed to achieve better learning and dynamic interference in \nsequence classification. For long-term information retrieval, the algorithm followed the \nstructure of rolling RBMs, and gated units (gSCRBM) were introduced. The gSCRBM \nperformed better in terms of parameters because it was trained with fewer parameters \nthan traditional LSTMs and GRUs. The model was evaluated to prove its superior perfor -\nmance over advanced LSTM structures (Yu et al. 2019), Bidirectional LSTM (BiLSTM), \nand Stacked LSTM (StackedLSTM) (Graves and Schmidhuber 2005). It was found that \nSCRBM outperformed the other models in terms of generalization. Although GRUs and \nLSTMs generated better results in a few circumstances, the authors explained that those \narchitectures demand more sophisticated structures, longer processing time, and more hid-\nden units. SCRBM was found to be more compact with fewer parameters but with the same \namount of neurons as another RNN network containing the same hyperparameters. How -\never, the SCRBM was not able to capture long-term information, which led to a vanishing \ngradient or exploding gradient problem. This issue was later resolved by the gated unit \n(gSCRBM).\n4.10  Stacked denoising autoencoders\n4.11  Autoencoders\nAutoencoder neural networks were designed for unsupervised learning by applying a back -\npropagation algorithm of the target values for equalizing the inputs. The autoencoder learns \nthe approximation between the output and identity function when the input is compared to \nthe output. When the autoencoder discovers the features or data structure, the hidden units \nare subjected to a sparsity constraint. Autoencoder models require knowledge of the geom-\netry of the data to properly understand the input data. Constraining the node in the hidden \nlayer allows autoencoders to learn the low-dimensional representation of the model.(60) p(yt,/u1D431t,/u1D421t���̂/u1D421t−1�\n=exp(−E𝜃(yt,/u1D431t,/u1D421t;̂/u1D421t−1)\n∑\nyt,/u1D431t,/u1D421texp(−E𝜃(y�,/u1D431�,/u1D421�;̂/u1D421t−1)\n(61) ̂/u1D421t−1=/u1D53C[/u1D407t−1/uni007C.var/u1D4311∶t−1,y1∶t−1]\n(62) E𝜃/parenleft.s2\ny�,/u1D431�,/u1D421�;̂/u1D421t−1/parenright.s2\n=−/bracketleft.s3/parenleft.s1/u1D431t/parenright.s1⊤/u1D416xh+/u1D42E⊤\nyt+/parenleft.s2\n̂/u1D421t−1/parenright.s2⊤\n/u1D416hh/bracketright.s3\n/u1D421t−/u1D41A⊤/u1D431t−byt−/u1D41C⊤/u1D421t\n\n13565\nDeep learning modelling techniques: current progress,…\n1 3\n4.11.1  Autoencoders for Words\nLiou et al. (2014) presented the Elman network for encoding each word of a different vec-\ntor in semantic space, which is related to corresponding entropy coding (Elman 1990, \n1998) and is operated on an encoder for training. The authors utilized the Elman network \nas a super Turing machine for powerful computation work (Siegelmann 1995). Figure  14 \nillustrates the Elman network employed by a simple recurrent network, which was designed \nfor semantic word categorization. However, because it could not handle the encoding task, \nthe Elman network was redesigned in order to encode the words into the semantic space \nFig. 14  Representation of Elman network (Liou et al. 2014)\nFig. 15  Schematic representation of deep energy model (Samaniego et al. 2020)\n\n13566 S. F. Ahmed et al.\n1 3\ndomain. The achieved codes were utilized in indexing, ranking, and categorizing literary \ntasks.\nLiou et al. (2014) encoded each word for individual vectors while training. The ward \nwas bound with corresponding entropy coding in semantic space. The training methodol-\nogy also included ranking, indexing and categorizing literacy steps from the training data. \nThe model was trained on the basis of acquired datasets from two Chinese novels: Romance \nof the Three Kingdom and Dream of the Red Chamber. However, they still needed to inves-\ntigate whether a low error rate could be achieved without the renewed coding units and the \nsame network architecture.\n4.11.2  Deep energy models\nThe deep energy model (DEM) is a deep learning training technique for deep networks and \narchitects based on the restrictive Boltzmann machine learning methodology (I. Goodfel-\nlow et al. 2016; Guo et al. 2016). It includes a feed-forward neural network that transforms \ndata inputs deterministically rather than modelling the output via a layer of stochastic hid-\nden units (perceptron/neuron), as shown in Fig.  15. The feedforward network (g/u1D703) acts on \nthe universal approximation theorem in order to approximate a continuous function, map-\nping corresponding inputs and outputs (Nguyen-Thanh et al. 2020).\nUnlike deep belief networks and deep Boltzmann machines that have multiple stochas-\ntic hidden layers, DEM consists of a single stochastic hidden layer (h), which allows effi-\ncient inference and simultaneous training of all the layers within the network (Ngiam et al. \n2011). Hopfield energy models were one of the earlier developed DEMs that, in their sim-\nplistic nature, allow closed-form modelling (Bartunov et al. 2019). However, the Hopfield \nmodel has significant demerits and is unable to work with the quadratic dimensionality of \nmemory patterns. The capacity for more patterns is also limited by the number of param-\neters in the network. Since real-world data consist of higher-order dependencies, the Hop-\nfield energy model cannot be used (Bartunov et  al. 2019). Ngiam et  al. (2011) utilized \nthe DEM approach to process natural images, demonstrating significant improvements in \ndata outputs when compared to greedy layer-wise training. In recent years, the develop-\nment of energy-based models meta-learning (EBMM) has been observed to show better \nperformance as a memory model that is capable of recalling training, memorizing patterns, \nand performing compression (Bartunov et al. 2019; Kraska et al. 2018; Parkhi et al. 2015; \nSun et al. 2015; Zhang et al. 2016a, b). Meta-based learning primarily operates on the read \n(x;θ ) and write (X) functions by means of truncated gradient descent, as follows:\n(63) read(̃x;𝜃)= x(K+1)=xk−𝛾(k)∇xE/parenleft.s1x(k)/parenright.s1,x(0)=̃x\n(64) L(X,/u1D703)=1\nNN/uni2211.s1\ni=1E[/uni007C.var/uni007C.x/uni007C.x/uni007C.xxi−read/parenleft.s1xi;/u1D703/parenright.s1/uni007C.x/uni007C.x/uni007C.x/uni007C.var2\n2]\n(65) W(x ,θ) = E(x;θ )+α /uni007C.var/uni007C.x/uni007C.x∇xE(x;θ)/uni007C.x/uni007C.x/uni007C.var2\n2+β /uni007C.var/uni007C.varθ−θ/uni007C.var/uni007C.var2\n2\n(66) write (X)/u1D703(T),/u1D703(t+1)=/u1D703(t)−/u1D702(t)1\nNN/uni2211.s1\ni=1∇/u1D703W(xi,/u1D703(t)),/u1D703(0)=/u1D703\n\n13567\nDeep learning modelling techniques: current progress,…\n1 3\nwhere x is the input (the deterministic dynamics); X represents the Nth set of input pat-\nterns compressed into parameters, θ , by the writing rule; N is the number of stored patterns; \nk = 1, 2,…K (number of sequences required to be updated to perform gradient descent \nfor optimization for reading function); t = 1, 2,…T (number of sequences required to be \nupdated to perform gradient descent for optimization for write function), respectively; \nγ(k)andη(t) are the learned stepped sizes for reading and writing functions respectively; E(x) \nrepresents the energy function; ∇x is the derivative operator; and W(x,θ) is the writing loss \nfunction, consisting of meta parameters α andβ , representing the energy function at a local \nminimum that must be two-fold and requires the hessian term to be positive. The later part \nof the writing loss function performs optimization, limiting deviation of prior parameter, θ , \nfrom the initial parameter, θ . Finally, implementing gradient descent tunes the writing func-\ntion as Eq. (34); where L(X,θ) denotes the score matching objective, or the reconstruction \nerror for the read function.\nCompared to past DEMs, EBMMs can utilize slow gradient learning, having effec-\ntive convolutional memories, particularly due to fast writing rules (Bartunov et al. 2019). \nEBMMs also adhere to and manage memory capacity efficiently, even for non-compress-\nible inputs, such as binary strings to natural images of high compression. It also has the \nability to differentiate different patterns (energy levels). The method proposed by Bartunov \net al. (2019) resolves the functioning pace of EBMMs with fast writing and limited param-\neter updates (a maximum of 5 steps), adding new inputs for the weights. Another advan-\ntage of this method is the association of faster reading and fewer gradient descent steps. \nThe employability of the proposed operations, which store N patterns in memory and do \nnot require additional assumptions, further adds to the efficiency of the model (Bartunov \net al. 2019). However, batch writing assumption is a challenge for EBMM and could be \nimproved with more elaborate architecture.\nIt is also difficult to find the optimum balance between writing speed and the model’s \ncapacity (a commonality for most deep learning energy models) (Ba et al. 2016; Bartunov \net al. 2019). In addition, the characterized properties of the learning attractor models are \nnot yet known, and EBMM cannot return different associations when under uncertainty, \nwhich occurs due to compression. Furthermore, with the general application of gradient-\nbased meta-learning, it is difficult to evaluate the expected outcome of EBMMs, mainly \nbecause of the high dimensionality pattern space of inputs that increases the resulting dis-\ntortion of the model and decreases the output reliability after adaptation. Therefore, a dif-\nferent gradient descent functionality is necessary. Also, parametric gradient-based optimi-\nzation requires significant updates (for memory/recalling applications) and, hence, is slow. \nResolving these existing issues, together with the observation and exploration of more sto-\nchastic variants for EBMMs would lead to significant improvements for DEM.\nStatistical learning and construction of an inference-free hierarchical framework offer \na viable solution for density estimation, consisting of higher dimensional challenges. By \nutilizing Bayesian (Eq. (67)) and Parzen score matching functions (Eq. (68)) (Saremi et al. \n2018; Vincent 2011) together with a multilayer perceptron of scalable energy learning \noperation (Eq. (69)), the deep energy estimator network (DEEN) can be modelled and fur -\nther optimized (Saremi et al. 2018), as follows:\nwhere σ denotes any level of noise; ψ represents the score function; ξ is the noisy measure-\nment of underlying random variable x; and θ is the parameter vector.(67) /uni0302.s1x(ξ)=ξ+σ 2ψ(ξ;θ)\n\n13568 S. F. Ahmed et al.\n1 3\nwhere P represents the Parzen density estimator; S signifies the smoothing kernel; k rep-\nresents the nth x of a dataset, x = {x1,x2…xn }; and n is the number of elements in the \ndataset, x.\nIn Eq. (69) (Saremi et al. 2018), E(x;θ) is linearly constructed from the preceding hid-\nden layer  hL, in which w is the weight of each data x and parameter θ , εα denotes the expert \n(corresponding products of expert, PoE) parametrized by the neural network, and α signi-\nfies the number of iterations.\nDeep energy estimator networks (DEENs) have been demonstrated to be effective \nwith high dimensionality data values (Saremi et  al. 2018). However, it is important \nto note that although DEEN can auto-regularize due to its Parzen function, it is not \nan autoencoder. In fact, DEEN can operate with a decoder by not directly estimating \nthe score functions (Alain et al. 2014) and, thus, skipping stability issues of denoising \nautoencoders. Being dataset-dependent, DEEN does not impose any bounds towards σ \nand can be effectively regularized. Apart from working with higher dimensionality data, \ndeep energy estimators are employed for semi-supervised, unsupervised learning, and \ngenerative modelling (Saremi et al. 2018). DEENs provide consistent estimations and, \ntherefore, acquire increasing interest; however, more testing is required to examine the \nnetwork’s performance for dynamic data as well as the scalability potential.\nAnother prominent application of DEM is the nonlinear finite deformation hyper-\nelasticity problem, operating on an energy and loss function. For instance, using Eule-\nrian motion description and transport deformation gradient formulation, the nonlinear \nresponse of elastic materials (in 3D) with a large deformation continuum can be mod-\nelled by employing DEM via DNNs. In a previous work, a neural network is structured \nusing Eq. (70), then optimized to minimize its potential energy using a loss function \n(Nguyen-Thanh et al. 2020):\nwhere /uni0302.s1z is the final output of the final layer l; w and b are weights and biases, respectively; \nand σ is the activation function acting on the kth neuron of the lth layer.\nDEM can also be utilized for nonlinear deformation, being faster with fewer cod-\ning and having the traction-free boundary conditions to be auto-filled. Training enables \nfaster solution retrieval, and the model can be easily coded in common machine learn-\ning operating platforms, such as TensorFlow and Pytorch (Nguyen-Thanh et al. 2020). \nHowever, the use of DEM has certain drawbacks due to the imposition of the boundary \ncondition of parameters and the associated integrations used for modelling and, there-\nfore, requires further study to improve the integration techniques. Moreover, the mod-\nelling tends towards non-convexity of loss function during the nonlinear evolution of \nnetwork neurons, and so, an enhanced theoretical understanding is required to better \nestablish the deep neural network architecture. DNNs for finite deformation hyper-elas-\nticity are trained using backpropagation, computing the gradient loss and minimizing (68)P(ξ)=1\nn/uni2211.s1\nkS(ξ/uni007C.varx(k))\n(69)E(x;θ)=/uni2211.s1\nαw(L+ 1)\nαh(L)\nα/parenleft.s1x;/braceleft.s1w(1),w(2),…,w(L)/braceright.s1/parenright.s1=/uni2211.s1\nαεα(x;θ)\n(70) ̂zl\nk=σ/parenleft.s4nl−1/uni2211.s1\nj=1wl\nkĵzl−1\nk+bl\nk/parenright.s4\n,0<1<L (the ﬁnal layer)\n\n13569\nDeep learning modelling techniques: current progress,…\n1 3\nthe function, using a standard optimizer. Considering the tendency of a gap to exist \nbetween backpropagation and energy-based models, Nguyen-Thanh et al. (2020) admin-\nistered forward propagation to approximate the solution with defined boundary condi-\ntions, which directs the prediction. Scellier and Bengio (2017) proposed equilibrium \npropagation to bridge gaps between backpropagation and the energy-based model. The \nmain objective of equilibrium propagation is to ensure a learning framework for the \nDEMs with a 0.00% training error. Provided the statistics of an excellent training error \nscore, it would be interesting to observe the performance of such a system for different \ndeep learning techniques and DEMs with complex non-linear data of high parameters \nand dimensions.\nReinforcement learning (RL) is another intensively studied deep learning method \nthat has unique connections with DEMs in terms of state and action spaces. RL sur -\npluses the shortcomings associated with DEMs, which are mostly sampling issues and \nunpopularity with regression models (Zhang et  al. 2020). For example, performing \nmolecular modelling using a DEM-based system would be difficult due to the absence \nof frameworks that do not involve a classification route for the dataset. Consequently, \nwhen it comes to modelling problems that do not involve density estimations or the \nnecessity for energy functions, a new neural network is required. Recently, Zhang et al. \n(2020) proposed a novel approach, where RL is reformulated into distribution learning \nto resolve sampling issues, using a minimax generative adversarial network to develop a \ntargeted adversarial learning optimized sampling (TALOS) methodology. Another tech-\nnique using entropy policy, called variational adversarial density estimation (VADE), \nwas also effective (for molecular modelling), demonstrating how cross-fertilization \nbetween EBMs/DEMs and RL can overcome the challenges of EBMs. Haarnoja et al. \n(2017) explored maximum RL via DEM using the Markov decision process (Eq. (71)) \nand modified the objective to maximize the entropy (Eq. (72)). Using soft Q learning \nand the Bellman equation, the model operated on learning maximum entropy policies \n(Eq. (73)).\nwhere S and a are state and action space, respectively; r denotes reward; p/u1D70B signifies the \nmarginals of state and state action for the policy, π(⋅/uni007C.varst) ; and α acts as a hyperparameter.\nwhere V∗\nsoft represents a partition log function; and Q∗\nsoft denotes a Q-function (proven and \ndetailed by Ziebart and Fox (2010) and Haarnoja et al. (2017), respectively).\nReinforcement learning energy modelling policies, which are suitable for high-\ndimensional values, have been observed to be robust and applicable to code robotic \ntasks and, hence, have become quite popular amid humanoid robots. Although the \nmodel requires pre-training of the general-purpose stochastic policies, when compared \nwith other deep energy modelling techniques, reinforcement learning via DEM seems \nmost promising, particularly by being able to solve inputs and sampling issues for \nenergy-based modellings.(71)πstd=argmaxπ/uni2211.s1\ntE(st+at)∼pπ[r(st,at)]\n(72)πMaxEnt= argmaxπ/uni2211.s1\ntE(st+at)∼pπ[r(st,at)] + αH(π(⋅ /uni007C.varst))]\n(73) πMaxEnt (at/uni007C.varst)= exp1\nα(Q∗\nsoft/parenleft.s1St,at/parenright.s1−V∗\nsoft(st)))\n\n13570 S. F. Ahmed et al.\n1 3\n4.11.3  Deep coding network\nDeep predictive coding network is a bio-inspired framework built on the theoretical \nunderstanding of how the brain infers sensory stimuli. The mechanism by which the \nbrain speculates decisions based on certain data (e.g. visual information) has been for -\nmulated as the baseline for predictive coding, followed by the adaptation of filter objec-\ntives and training of modules via gradient descent. However, due to the still very mis-\nunderstood functioning of neurons in the brain, it is likely that the connected neurons \nin the brain consist of a more complex architecture, significantly limiting existing deep \nlearning models. Incorporating a feedforward and feedback (prediction making) system \nalong with each layer of a neural network is a generative understanding of deep cod-\ning networks, particularly the deep predictive coding system. Such networks are heav -\nily studied and used for computer vision, where classification for images and videos is \nperformed.\nA base equation for a deep predictive coding network is given by (Dora et al. 2018):\nwhere lp is the calculated error in compliance with p-norm; yl\nm,n is a vector in a channel of \nthe lth layer, consisting of mth rows and nth columns; w(l)\nm,n,i ,j represents the filter through \nwhich neurons at m, n position of l layer is projected; Y1andX1 are the height and width of \nthe layer arranged in a 3D box shape; and /uni0302.s1y and y denote the predicted and actual activity \nof neurons, respectively.\nWith limited research and understanding of brain processing, particularly of events asso-\nciated with memory, learning, and attention, developing mature and complex deep predict-\ning coding network architectures remains challenging. Therefore, visual image mapping \nrequires further analysis. Nevertheless, many novel deep learning frameworks and applica-\ntions employ the use of predictive coding across various fields for plethora machine learn-\ning applications. For instance, Dora et al. (2018) developed a generative model based on \ndeep predictive coding and trained using unsupervised learning for processing real-world \nimages and to effectively capture the statistical regularities of the data. Such ability makes \nthe model suitable for various image classification and computer vision tasks. The applica-\ntion of a similar model in security is another prominent example.\nThe importance of machines to detect video anomalies is gaining popularity to enhance \nsecurity and surveillance. However, video anomalies are highly ambiguous and complex, \nwith high error margins and poor scores in existing reconstruction and prediction modules \n(Hasan et al. 2016; Liu et al. 2017; Ye et al. 2019). A recent application of deep learning \nby Ye et al. (2019) demonstrated an improved video anomaly detection. Using a predic-\ntive coding network with an error refinement module, the methodology was able to refine \ncoarse predictions, reconstruct errors, and create a framework that assembles reconstruc-\ntion and prediction modules. The modified predictive coding model uses a multilayer net-\nwork that extracts prediction error features (Eqs. (75) and (76)). The new predictions are \nthen generated to rectify prediction errors using the convolution of the ConvLSTM unit, \nenabling sequential dynamics modelling (Eq.  (77)). Afterwards, the system performs \nrefinement. To reach a refined estimation, score gaps between the frames (normal and \nabnormal) are reconstructed. Equation (79) represents the error refinement module based \non Eq.  (78). The objective functions were minimized and optimized. Metrics, including (74) E=N/uni2211.s1\nl=0(Y1,X1/uni2211.s1\nm,nlp/parenleft.s2\nyl\nm,n−/uni0302.s1yl\nm,n/parenright.s2\n+Y1,X1/uni2211.s1\nm,nlp/parenleft.s2\nyl\nm,n/parenright.s2\n+/uni2211.s1\nm,n,i ,jlp(w(l)\nm,n,i ,j))\n\n13571\nDeep learning modelling techniques: current progress,…\n1 3\nintensity (to measure pixel-wise difference), gradient (to prevent blurry predictions), and \nmotion constraints, were utilized as a part of the adversarial training strategy.\nwhere Ej−1 is the previous prediction error; Ij−1 represents the ground truth; and /uni0302.s1I(j−1) \ndenotes the previous prediction.\nIn Eq. (44), Rj−1 extracts deep features, and Ej−1 is the previous prediction error.\nwhere /uni0302.s1Ij is the updated prediction generated from the previous prediction error; and \nConvLSTM is a special LSTM operation (spatial convolutions placed for connected \ntransformations).\nwhere ΔS is the regularity score gap for error refinement (between normal and abnormal \nframe); St is the regularity score; t, time frame; N is the sequence number set for the nor -\nmal frame; A is the sequence number set for the abnormal frame; and Tn and Ta denote the \ntotal number of normal and abnormal frames, respectively.\nwhere /uni0302.s1Et and Et are the updated prediction error and preceding prediction error of time step \nt, respectively.\nAnother novel method was proposed by Tandiya et al. (2018) based on deep parse coding \nto detect radio frequency (RF) anomalies that are present in wireless connections. The neural \nnetwork was trained to recognize the anomaly when there is a potent deviation between the \npredicted and actual outcomes. The method performs real-time RF monitorization, which is \nboth non-intrusive and automated. Tandiya et al. (2018) demonstrated that the use of deep pre-\ndictive coding is faster and more efficient than other ML-based approaches. Sequenced images \nof the network’s normal operation were obtained using Prednet, a video frame detector, which \nteaches the network to make predictions and detect anomalies. Auto-tuning the hyperparam-\neters could be one significant improvement for the predictive coding networks, using:\nwhere α = cycle frequency as one axis.\nThe anomaly detection efficiency of this neural network was close to 100%. The seis-\nmocardiography-based detector showed to act relatively faster than the first detector, which \nis responsible for the detection anomaly in consecutive spectrogram images. The seis-\nmocardiography-based detector spots image anomalies almost instantaneously, and such a \nmethodology of anomaly detection can be employed for networks with variable constraints \nand devices. However, the robustness of detection can be further improved by working (75) PEP ∶ Ej−1=Ij−1−/uni0302.s1I(j−1)\n(76) Rj−1= PEP(Ej−1)\n(77) /uni0302.s1Ij= Conv(ConvLSTM/parenleft.s1Rj−1/parenright.s1)\n(78) ΔS =∑\nt∈NSt\nTn−∑\nt∈ASt\nTa\n(79) /uni0302.s1Et= ERM/parenleft.s1Et/parenright.s1\n(80) S/u1D6FC\nxx(n,f)=1\nNN/uni2211.s1\nr=11\nN�XN�(n,f+/u1D6FC∕2)X∗\nN�(n,f−/u1D6FC∕2)\n\n13572 S. F. Ahmed et al.\n1 3\nwith complex anomalies, evaluating longer run times, and employing machine learning \ntechniques to process raw data in different forms. Showing promising error rates and effi-\ncient predictive capacity, each framework has its own merits and demerits. Given that pre-\ndictive coding is an area that requires further understanding, the functions and frameworks \napplied by machine learning engineers to solve problems in various disciplines, with vari-\nous biases, can be significantly improved and optimized further.\nThe main objective of sparse coding, a special case of deep predictive coding, is to deter -\nmine a set of input vectors as a linear combination of basis vectors, which is then taught to \nefficiently represent data, as seen in Eq. (49) (for example, image data for classification). In \na study by Zhang et al. (2017a, b, c), deep sparse coding (a deep modelling technique) pro-\nduced effective results in extracting high distinct features from raw image pixels, for which \nthe process is based on unsupervised learning. The deep sparse coding network is constructed \nupon basic input, a sparse-coding and pooling layer, and a normalization and map reduc-\ntion layer. Such an algorithm uses heuristics to minimize non-convex functions. Although \nthe system is dependent on a CNN architecture and could have improved speed, the overall \nframework is easier to code and functions better than any independent CNNs. However, deep \nsparse coding suffers from not being mathematically rigorous and converging towards a local \nminimum. Arora et al. (2015) demonstrated how sparse coding can also converge to a global \nminimum, providing a novel-based initialization method that returns a better starting point.\nwhere  Xij is the receptive field at spatial location i, j; W represents the weight of the input; \nC is the number of colored channels (of the input layer), as well as the number of feature \nmaps for the feature map layer (Zhang et al. 2017a, b, c); K controls the sparsity of cij ; and \nL0 is a constraint under batch tree orthogonal matching pursuit.\nwhere wm is the kernel; and Cm is the sparse feature map.\nConvolutional sparse coding network (CSN), based on Eq. (50), incorporates the frame-\nwork of a convolutional system (Zhang et al. 2017c). Similar to a deep sparse coding net-\nwork that primarily performs patch-level approximation, CSN conducts image-level recon-\nstruction (approximation as well), but with more hindrance due to the convolution’s nature. \nTherefore, deep sparse coding was observed to propagate sharp information forward. The \nhierarchical sparse coding (HSC) framework is a similar working sparse coding network \nthat completes the patch operation using concatenation methodology. For HSCs, map \nreduction layers are essential to delve deeper. Utilizing multi-level optimization and non-\nnegative sparse coding, Sun et al. (2017) developed a multilayer sparse coding network. \nThe latter system is a deep learning framework consisting of bottleneck modules with an \nexpansion and reduction layer of sparse coding, consisting of wide and slim dictionaries \nthat are able to generate high- and low-dimensional distinct features and clustered repre-\nsentations, respectively. A supervised learning technique was also employed to train the \ndictionaries, optimizing regulatory parameters. Although the network requires fewer layers \nand parameters, the deep learning architecture should be further studied to improve pro-\ncessing efficiency. The general descriptions of each deep learning modelling technique, as (81) Cij=arg min1\n2/uni007C.var/uni007C.varxi,j−Wcij/uni007C.var/uni007C.var2, s.t /uni007C.var/uni007C.x/uni007C.x/uni007C.xcij/uni007C.x/uni007C.x/uni007C.x/uni007C.var\nL0≤K\n(82) C=arg min1\n2/uni2016.x/uni2016.x/uni2016.x/uni2016.x/uni2016.xI−/uni2211.s1M\nm=1wm∗Cm/uni2016.x/uni2016.x/uni2016.x/uni2016.x/uni2016.x2\nF+/u1D6FDM/uni2211.s1M\nm=1/uni007C.x/uni007C.xC1/uni007C.x/uni007C.x\nm=1\n\n13573\nDeep learning modelling techniques: current progress,…\n1 3Table 2  Overview of the studies conducted on deep learning modelling\nModel General description Study surveyed Main task(s) Outcome(s) Application(s) Remarks\nVector space \nmodel \n(VSM)VSM is an arithmetic model in which texts are \nrepresented as vectors. The vector elements \ncharacterize the weights or significance of \neach word within a document. It can identify \nsimilarities among distinct documents, and thus \nassists to detect plagiarism. However, due to \ntheir low similarity values, long documents/\npapers are poorly represented. The VSM has \nbeen effectively implemented in information \nfiltering and retrieval, among other applicationsAli et al. (2019) Propose a text clas-\nsification system for \nretrieving transporta-\ntion sentiment from \nsocial networking and \nnews sitesAchieved an accuracy of 93% for \nsentiment classification, which \noutperformed topic2vec docu-\nment representation methods \nwith transportation datasetsSentiment \nanalysis, \ntopic model-\nlingSophisticated data pre-\nprocessing is needed to \nimprove classification \naccuracy\nAdhikari et al. \n(2019)Development of \ndocument classification \nmodel based on BERT \nfor identifying different \nlabels for the document \nclassification taskReduced the number of parameters \nby 30 timesDocument \nclassificationThe average document \nlength is less than \nbidirectional encoder \nrepresentations from the \ntransformer (BERT); the \nmaximum length is 512\nSi et al. (2019) Assessment of how \nwell classic word \nembedding approaches \n(word2vec, GloVe) and \ncontextualized methods \n(BERT) perform on a \nclinical concept extrac-\ntion taskAchieving better performance \n(F1-measures of 93.18) on vari-\nous benchmark testsClinical \nconcept \nextractionNot investigates the \nperformance benefits of \nfine-tuning BioBERT \nwith clinical text\nMohd et al. \n(2020)Introduced a text summa-\nrizer that obtains the \nfeatures of a long text \ndocument to extract the \nessential and valuable \ninformation while \nmaintaining critical \ninformationThe macro-average of precision \nfrom the experimental results \nwas found 34%Text summari-\nzationTested the technique with \nonly one dataset\n\n13574 S. F. Ahmed et al.\n1 3Table 2  (continued)\nModel General description Study surveyed Main task(s) Outcome(s) Application(s) Remarks\nYang et al. \n(2019)Develop a hypergraph \nembedding method \nLBSN2Vec for \nlocation-based social \nnetwork data that \nenhanced friendship \nand location prediction \ntask effectivenessOutperforms the baseline graph \nembeddings an average growth \nof 32.95% and 25.32%, respec-\ntivelyLocation-\nbased social \nnetworkExamined only the random \nwalk in the hypergraph \nfor location prediction \ntasks\nConvolutional \nneural net-\nwork (CNN)A CNN model is comprised of three primary lay -\ners: convolution, pooling, and fully connected \nlayers. The first two layers generate features \nfrom the input, while the third layer, the fully \nconnected layer, connects the extracted features \nto the final output. The convolution layers \nextract high-level features from the provided \ndata. CNNs are especially beneficial for reduc-\ning the number of parameters in an artificial \nneural network. However, sometimes they take \na longer time to train data. These models are \ntypically used in object detection, text classifi-\ncation and sentiment analysisBarré et al. \n(2017)Construct LeafNet that \ncan be used to identify \nplant species from the \nleaf imagesFound an accuracy of 86.3%, \n95.8%, and 97.9% on the LeafS-\nnap, Foliage, and Flavia dataset, \nrespectivelyPlant clas-\nsification- Comparatively slow \n(training took about 32 h)\n- Lacks context due to the \nsmall, cropped window \nsizes\nLi et al. (2019a, \nb, c, d)Propose a method Stereo \nR-CNN that can per -\nform 3D object detec-\ntion in autonomous \ndrivingOutperformed one of the previous \nstudies by over 25%—30%Object detec-\ntion- Due to the absence of \nprecise depth informa-\ntion, the model can only \nproduce shallow 3D \ndetection results\n- Variations in appearance \ncan also have a signifi-\ncant impact\nChen et al. \n(2018a, b)Apply an unsupervised \ndomain adaptation \nmethod for object \ndetection in a variety \nof image domainsOutperformed the faster R-CNN \nmodel up to + 8.8%Object detec-\ntionThe model is adaptable to \nspecific scenarios only\n\n13575\nDeep learning modelling techniques: current progress,…\n1 3Table 2  (continued)\nModel General description Study surveyed Main task(s) Outcome(s) Application(s) Remarks\nChu et al. (2017) Propose a dynamic \nCNN-based framework \nfor tracking objects in \nvideosThe proposed online multi-object \ntracking algorithm performed \nbetter than Markov decision \nprocesses by 4%Object tracking - Unsuitable for applica-\ntions with limited \nresources\n- Consume a lot of memory \nand time\nHughes et al. \n(2017)Classify clinical text into \none of 26 categories, \nsuch as \"Brain\" or \n\"Cancer\"Showed better prediction accuracy \ncompared to the word embed-\nding based methods by about \n15%Text classifica-\ntionDomain Adaptation Tech-\nniques can be used to \ntransfer knowledge from \nanother domain to the \nmedical field\nLiao et al. \n(2017)Predict user behavior \nfrom Twitter dataDevelopment accuracy was maxi-\nmum up to 74.5%Sentiment \nanalysisA multilayer CNN may be \nused to boost the model\nPerraudin et al. \n(2019)Develop DeepSphere, a \ngraph-based convolu-\ntional neural network \nthat can predict a \nclass from a map and \nclassify pixels from \ncosmological dataBetter than the baselines by \n10% in terms of classification \naccuracyCosmological \ndata analysisMissing the comparison of \nDeepSphere to various \nspherical CNN imple-\nmentations with different \nsampling techniques\nMukherjee et al. \n(2020)Construct a CNN-based \ngenerative model, \nnamely “GenInSAR”, \nfor combined coher -\nence estimation and \nphase filtering which \ndirectly learns inter -\nferometric synthetic \naperture radar (InSAR) \ndata distributionCompared to the related methods, \nthe phase cosine error,\ncoherence and phase root-mean-\nsquare-error of GenInSAR were \nimproved by 0.05, 0.07 and \n0.54, respectivelySynthetic \naperture \nradar data \ndistributionThe InSAR machine learn-\ning can be improved by \nGenInSAR’s ability to \nproduce new interfero-\ngrams\n\n13576 S. F. Ahmed et al.\n1 3Table 2  (continued)\nModel General description Study surveyed Main task(s) Outcome(s) Application(s) Remarks\nRecurrent neu-\nral network \n(RNN)RNNs are neural networks with memories capable \nof capturing all information recorded sequen-\ntially in the preceding unit. It is advantageous in \nforecasting time series since the highlight point \nworks as a reminder of previous inputs. RNNs \nare useful due to the fact that they execute the \nsame computation for each sequence element. \nBut they suffer from gradient and exploding \nvanishing issues, which limits longer sequencesSiami-Namini \net al. (2019)Compare LSTM and \nBiLSTM in time series \ndata modellingEnhanced forecasting accuracy by \n37.78% to standard LSTM-based \nmodelsTime series \nmodellingBiLSTM based models \nappear to achieve slower \nperformance than the \nLSTM-based models\nBasiri et al. \n(2021)Build CNN-RNN model \nthat can find out the \nsentiment from long \nreviews as well as short \ntweet textImproved from 1.85% to 3.63% \nfor five long review datasets in \nterms of accuracySentiment \nanalysisThere is potential to investi-\ngate sentiment classifica-\ntion at the sentence and \naspect levels\nMajumder et al. \n(2019)Introduced Dia-\nlogueRNN built on \nRNN with attention \nmechanism for emotion \ndetection in textual \nconversations with one \nof six emotion labelsOutperformed the baseline meth-\nods by achieving a 6.62% higher \nF1-score on averageEmotion detec-\ntionTime-consuming for train-\ning and not parameter-\nefficient for global or \nlocal contexts\nCamgoz et al. \n(2018)Recognize sign language \ngestures from a video \nof someone performing \ncontinuous signsScored 18.13 on the BLEU-4 \nmatric and 43.80 on the ROUGE \nmatricNeural \nmachine \ntranslationCNN could learn good fea-\nture representation, but \nthis hypothesis’s validity \nwas not evaluated\nG. Rao et al. \n(2018a, b)Model long texts for \ngenerating semantic \nrelations between \nsentences for sentiment \nanalysisShowed better performance than \nother models by obtaining an \naccuracy of 44% and 63.9%Sentiment \nanalysis- Considered only the \nsequential order of the \ndocuments\n- It is possible to represent \nthe documents using tree-\nstructured LSTM\n\n13577\nDeep learning modelling techniques: current progress,…\n1 3Table 2  (continued)\nModel General description Study surveyed Main task(s) Outcome(s) Application(s) Remarks\nUddin et al. \n(2020)Present a multi-sensors \ndata fusion network \nto recognize human \nactivities and behaviorThe average performance was \nmeasured as 99% using \nprecision, recall, and F1-score \nmatricesIntelligent \nhealth care \nsystemsThe work can be extended \nto develop a real-time \nhuman behavior monitor -\ning system with consider -\ning more complex human \nactivities\nSahoo et al. \n(2019)Analyze the applicability \nof LSTM-RNN to fore-\ncast daily flows during \nlow-flow periodsLSTM-RNN model performance \n(RMSE = 0.487) on hydrological \ndata outperformed the traditional \nRNN model (RMSE = 0.516) and \nnaive method (RMSE = 0.793)Time series \nmodellingMultiple hidden LSTM \nlayers can be used to \nboost the performance of \nthe model\nAlemany et al. \n(2019)Propose a fully con-\nnected RNN to predict \nhurricane trajectories \nfrom historical cyclone \ndata that could learn \nfrom all types of hur -\nricanesFor hurricane SANDY, the mean \nabsolute error from the RNN \nmodel (0.0842) is better than the \nprevious sparse RNN average \n(0.4612) modelTyphoon \npredictionThe model may take \nadvantage of convert-\ning the grid locations \nto latitude–longitude \ncoordinates to reduce the \nconversion error\nRecursive neu-\nral network \n(RvNN)RvNN is a nonlinear model that operates on \nstructured inputs and is useful to parse trees \nin natural language processing (NLP), image \nanalysis, and protein topologies, among other \nstructured domain applications\nIt works exceptionally well in NLP-related tasksMa et al. (2018) Develop two recursive \nmodels for rumor \ndetection, based on \ntop-down and bottom-\nup tree-structured \nneural networksShowed a strong ability to identify \nrumors at an early stage. Accu-\nracy on two different datasets \nwas calculated as 72.3% and \n73.7%, better than other existing \napproachesRumor detec-\ntion on \nTwitterOther types of data can be \nadded into the structured \nneural models, such as \nuser properties, to boost \nrepresentation learning \neven further\nBiancofiore et al. \n(2017)Analyze atmospheric \nparticulate matter, and \nforecast daily averaged \nconcentration of PM10 \nand PM2.5 from one to \nthree days aheadCorrectly predicted 95% of the \ndays analyzed in this studyForecasting \nPM10 and \nPM2.5Actual prediction accuracy \nis decreased if only the \ndays where the exceeded \nlimits are considered\n\n13578 S. F. Ahmed et al.\n1 3Table 2  (continued)\nModel General description Study surveyed Main task(s) Outcome(s) Application(s) Remarks\nLim and Kang \n(2018)Extract relationships \nbetween chemical \ncompounds and genesF- scores for the model including \nextra pre-processing and the \nSPINN model were calculated at \n63.7 and 64.1%, respectivelyChemical \ncompounds \nand genesCoordination is not \ndetected which may be \navoided with the use of a \nseparate module\nNeural tensor \nnetwork \n(NTN)In contrast to conventional neural network mod-\nels, NTN can directly connect two input vectors \nto a tensor. NTNs have been successful in vari-\nous natural language processing applications\nAlthough the NTN model is effective, it is compu-\ntationally intensiveQiu and Huang \n(2015)Construct a convolutional \nNTN for community-\nbased question \nanswering, integrating \nsentence modelling and \nsemantic matching into \none modelOutperformed the state-of-the-arts \non two different languages data-\nsets, English and ChineseCommunity-\nbased \nquestion \nansweringThe convolutional NTN \ncan handle more complex \ninteractions with tensor \nlayers than existing \nmodels\nBai et al. (2018) Develop deep attention \nNTN for visual ques-\ntion answeringIncreased performance of 1.98% \nand 1.70% than existing MLB \nand MUTAN models respec-\ntivelyVisual question \nansweringThis method could be \napplied to more visual \nquestion answering mod-\nels for further verification\nHu et al. (2017) Demonstrate how the \ncombination of face \nrecognition features \nand facial attribute \nfeatures can improve \nface recognition per -\nformances in different \nchallengesThe model obtained almost 100% \naccuracy on three databases \nCASIA NIR-VIS2.0, MultiPIE, \nand LFWFace recogni-\ntionThe approach used in the \nstudy can be scaled to \nbig data using effective \nmini-batch SGD-based \nlearning\nDeep belief \nnetwork \n(DBN)In DBN, a stack of restricted Boltzmann machines \n(RBMs) is typically utilized. The DBN is used \nto stack multiple unsupervised networks, with \nthe hidden layer of each network serving as the \ninput for the subsequent layer. The RBM has the \nadvantage of fitting sample characteristicsAbdel-Zaher and \nEldeib (2016)Diagnose breast cancer \nthrough a weight-\ninitialized backpropa-\ngation neural network \nfrom a trained DBN \nhaving identical archi-\ntectureThe accuracy of the model was \nevaluated as 99.68% on the Wis-\nconsin breast cancer datasetBreast cancer \ndetectionSince DBN requires signifi-\ncant computational effort \non hardware, building a \nreal-life computer-aided \ndiagnosis system is quite \nchallenging\n\n13579\nDeep learning modelling techniques: current progress,…\n1 3Table 2  (continued)\nModel General description Study surveyed Main task(s) Outcome(s) Application(s) Remarks\nZhao et al. \n(2017)Propose a feature learn-\ning technique named \ndiscriminant DBN \nfor synthetic aperture \nradar (SAR) image \nclassificationSignificantly outperformed the \nstate-of-the-artSAR image \nclassificationThe neighbor selection \nprocess of the training \nstrategy of a weak classi-\nfier may cause significant \nvariance in pseudo-\nlabelling as it is governed \nby fixed neighbors. Some \nadaptive strategies can be \nfollowed to pick specific \nsamples for training the \nweak classifiers\nLi et al. (2022) To develop MMDBN \nmodel, a manifold-\nbased multi-DBN to \nacquire deep manifold \ncharacteristics of \nhyperspectral imagingExperimental findings on the Sali-\nnas, Botswana and Indian Pines \ndatasets reach 90.48%, 97.35%, \nand 78.25%, respectively, \ndemonstrating that MMDBN \noutperforms some state-of-the-\nart algorithms in classification \nperformanceHyperspectral \nimagingMMDBN’s classifica-\ntion performance can \nbe further improved by \ndesigning the combined \nspectral-spatial deep \nmanifold networks\nConvolutional \ndeep belief \nnetwork \n(CDBN)CDBN is a hierarchical generative model for a \nreal size image. This model stacks convolu-\ntional RBMs (CRBMs) to construct a multilayer \nstructure similar to DBNs. Unlike RBM, the \nCRBM distributes the weight of the hidden and \nvisible layers across the imageWu et al. (2018) Present a novel technique \nfor pathological voice \ndetection based on \nCNN structureFor the validation and testing sets \nthe accuracy of CNN was 66% \nand 77% respectively whereas \nCDBN achieved 68% and 71% \nrespectivelyPathological \nvoice detec-\ntionCNN can be tuned more \nrobustly by applying \nCDBN to initiate the \nweights, and it can keep \naway the overfitting issue\nLi et al. (2019a, \nb, c, d)Propose a model based \non Gaussian Bernoulli \nrestricted Boltzmann \nmachines (GBRBM) \nto take the benefit of \nGBRBM and convolu-\ntion neural networksBy considering variance and \nconvolution, feature extraction \nperformance was improved. The \nmodel showed low computa-\ntional cost compared with the \nexisting methodsImage feature \nextractionThis experiment just built \none GCDBN with only \nfive layers. The recogni-\ntion accuracy can be \nincreased by adding more \nconvolutional and pool-\ning layers\n\n13580 S. F. Ahmed et al.\n1 3Table 2  (continued)\nModel General description Study surveyed Main task(s) Outcome(s) Application(s) Remarks\nHybrid neural \nnetwork \n(HNN)The HNN is comprised of a partial first principal \nmodel that provides previous information about \nthe process with a neural network, which acts as \nan estimate of unmeasured process arguments. \nIt is useful for sentiment classification, energy \nforecasting, disease diagnosis, and many other \napplicationsArabasadi et al. \n(2017)Develop a hybrid tech-\nnique that combines \ngenetic algorithms with \nneural networks for \ndiagnosing coronary \nartery diseaseThe hybrid approach improves the \nperformance of a neural network \nby around 10% by upgrading \nits initial weights with a genetic \nalgorithmDiagnosing \ncoronary \nartery \ndiseaseSome other parameters \nsuch as learning rate and \nmomentum factor could \nbe optimized\nAbedinia et al. \n(2018)Suggest a new forecast-\ning methodology based \non a hybrid forecasting \nengine by integrat-\ning a neural network \nwith a Metaheuristic \nalgorithmThe proposed model provided \nbetter prediction accuracy than \nother models in the domainSolar energy \nforecastingThe neural network-based \nforecasting engine is able \nto prevent underfitting \nand overfitting issues \nwith the help of this \nhybrid method\nGhosh et al. \n(2016)Propose an architecture \nusing probabilistic neu-\nral network (PNN) and \nrestricted Boltzmann \nmachine (RBM) \ntogetherPerformed better than other mod-\nels in Books and DVD datasets \nbut could not perform better in \nElectronics, and Kitchen appli-\nance datasetsSentiment clas-\nsificationThis model does not rely on \nexternal resources, such \nas sentiment dictionar -\nies, and thus reduces the \nsystem’s complexity\n(Liu et al. 2022) Use HNN with Wavelet \nTransform and Bayes-\nian Optimization to \npredict the copper \nprice for the short term \nand long-termThe proposed approaches, GRU \nor LSTM, accurately forecasted \nthe copper price in the short \nand long term with the mean \nsquared errors of less than 3% in \nboth casesPrice forecast-\ningWith the HNN, the \nunnecessary data can \nbe filtered out while the \noptimal hyperparameter \nset is searched\nDynamic neu-\nral network \n(DNN)Dynamic neural networks (DNNs) are an emerg-\ning technique that can outperform traditional \nstatic models in terms of accuracy, adaptive-\nness, and computational complexityGodarzi et al. \n(2014)Improve the performance \nof an ANN to predict \noil prices by develop-\ning a dynamic neural \nnetworkBetter accuracy was achieved \nusing DNN than time-series \nand ANN models for oil price \npredictionOil price \npredictionThe model adjusts the \noutputs obtained from the \ntime-series model and \nincreases the prediction \naccuracy\n\n13581\nDeep learning modelling techniques: current progress,…\n1 3Table 2  (continued)\nModel General description Study surveyed Main task(s) Outcome(s) Application(s) Remarks\nCBOW-DA-LR CBOW-logistic regression (LR) is an extension \nof the CBOW algorithm. CBOW-DA-LR is an \nenhancement of CBOW-LR that incorporates \nvisual data, such as images in tweetsBaecchi et al. \n(2016)Analyze sentiments with \nthe use of multimodal \nlearning techniques by \nimplementing neural \nnetwork-based models \nfor microblogging con-\ntent that might consist \nof texts and imagesOutperformed the SentiBank \napproach, which is a well-\nestablished approachSentiment \nanalysisCan be performed well in \nsyntactic/semantic word-\nsimilarities\nDeep echo \nstate network \n(DeepESN)The DeepESN can enhance the efficiency of a \ngeneral echo state network (ESN) in several \ndomains. The DeepESN output is produced \nusing a linear structure of the recurrent units \nacross all recurrent layers. The usual ESN tech-\nnique is subject to stability limitations. Such \nlimits are stated in DeepESN by the criteria \nfor the ESN of the deep reservoir computing \nnetworkGallicchio et al. \n(2018a)Develop a novel tech-\nnique based on Deep-\nESN for diagnosing \nParkinson’s diseaseDeepESN showed significant \nperformance compared to the \necho state network (ESN). For \ntraining, validation and testing \nset, its accuracy was 2.67, 2.95 \nand 3.07% more than ESNDiagnosing \nParkinson’s \ndiseaseThis is a significant initial \nwork in the domain of \nDeepESN that shows the \nsuperiority of DeepESN \nover the shallow ESN \nmodel\nGallicchio et al. \n(2018b)Construct a DeepESN \nmodel denoted by \nAD-DeepESN for the \ntime series data predic-\ntion where additive \ndecomposition (AD) \ntechnique was used as \na pre-processing step \nto the modelAD-DeepESN model performed \nwell on six datasets with a low \nstandard deviationTime series \npredictionA low standard deviation \nproves the stability of the \nmodel. Significant per -\nformance can be achieved \nfor a large multidimen-\nsional data\nElman recur -\nrent neural \nnetwork \n(ERNN)In ERNN, the hidden layer’s output is used as \ninput for the context layer in the former. The \nERNN architecture comprises four layers: input \nlayer, recurrent layer, hidden and output layer. \nEach layer has one or multiple neurons that use \na non-linear function of their weighted sum of \ninputs to transfer information from one layer to \nthe next oneWang et al. \n(2016a, b, c)Build an architecture \nby combining ERNN, \nmultilayer perceptron, \nand stochastic-time-\neffective functionThe proposed model showed an \nimprovement in forecasting \nprecision in comparison with \nBPNN, STNN, and ERNNStock indices \nforecastingNonlinear and nonstation-\nary data can be used for \ngetting a noticeable per -\nformance of the model\n\n13582 S. F. Ahmed et al.\n1 3Table 2  (continued)\nModel General description Study surveyed Main task(s) Outcome(s) Application(s) Remarks\nKrichene et al. \n(2017)Apply a non-linear \nchaotic system named \nMackey–Glass to RNN \nthat shows a good \nbenchmark test since \nits elements are chal-\nlenging for predictionThe proposed model’s normalized \nroot mean square error value \nwas minimal (0.0165) compared \nto other traditional RNN modelsForecasting \nMackey \nGlass \ntime-series \nelementsRandomly initializing the \nweights of the context \nunits can produce the \noptimal results\nDeep energy \nmodel \n(DEM)DEM is a deep learning training technique for \ndeep networks and architects based on the RBM \nlearning methodology. It includes a feed-\nforward neural network that transforms data \ninputs deterministically rather than modelling \nthe output via a layer of stochastic hidden units \n(perceptron/neuron). In contrast to DBNs and \ndeep Boltzmann machines, DEM consists of a \nsingle stochastic hidden layer, which enables \nrapid inference and simultaneous training of all \nthe layers within the networkBartunov et al. \n(2019)Employment of \nmeta-based learning \nmethod to energy-\nbased memory model \n(EBMM) for storing \npatternsThe method resolved the function-\ning pace of EBMMs, having fast \nwriting and limited parameter \nupdates, adding new inputs for \nthe weightsBuilding \ncompressed \nmemoriesCompared to past DEMs, \nEBMMs can utilize slow \ngradient learning, having \neffective convolutional \nmemories, particularly \ndue to fast writing rules\nSaremi et al. \n(2018)Demonstrate the utility \nof deep learning \nestimator network \n(DEEN) in learning \nscoring function, the \nenergy, and single-step \ndenoising operations \nfor high-dimensional \nand synthetic dataDeep learning estimator network \nperforms well for consistent \nestimationDensity \nestimation \nin statistical \nlearningThe DEEN model is \nunexamined for linear \ncomplex higher dimen-\nsions and parameters\nHaarnoja et al. \n(2017)Explore maximum \nreinforcement learn-\ning via DEM, using \nthe Markov decision \nprocessThe model is capable of accurately \nrepresenting complicated multi-\nmodal behavior in a variety of \ncontextsRobotic tasks; \nBuilding \nhumanoid \nrobotsEffectively captures com-\nplex multimodal behavior \npre-training general-pur -\npose stochastic policies\n\n13583\nDeep learning modelling techniques: current progress,…\n1 3Table 2  (continued)\nModel General description Study surveyed Main task(s) Outcome(s) Application(s) Remarks\nNguyen-Thanh \net al. (2020)Directly minimizes \npotential energyFulfils equilibrium state when \npotential energy is minimizedFinite deforma-\ntion hyper-\nelasticity- Nonconvexity of the loss \nfunction when neurons \nare evaluated by a nonlin-\near activation\n- Imposition of boundary \nconditions, integration \ntechniques need improve-\nments\nScellier and \nBengio (2017)Propose equilibrium \npropagation to bridge \ngaps between back -\npropagation and the \nenergy-based modelThe study makes static back-\npropagation more conceivableAnalog cir -\ncuits; digital \nhardware\n(GPU)Stores every previous state \nin the training sequences; \nlengthy relaxation \ntowards a fixed point \ndemonstrates negative \nimpacts\nDeep coding \nnetwork \n(DCN)DCN is a bio-inspired framework based on \nthe theoretical knowledge of how the brain \ninterprets sensory data. The method through \nwhich the brain predicts judgments based on \nspecific facts (such as visual information) has \nbeen described as the foundation for predic-\ntive coding, followed by the adaption of filter \nobjectives and training of modules via gradient \ndescent. These networks are extensively utilized \nin computer vision, where image and video \nclassification is accomplished. The DCN is also \nused in many other sectors including building \nsecurity and surveillance, autonomous vehicle \ncontrol, communication services, object detec-\ntion and classificationZhang et al. \n(2017a, b, c)Image feature learning \nby deep sparse-coding \nnetworkDeep sparse-coding network \ndemonstrated effective results in \nextracting high distinct features \nfrom raw image pixelsImage clas-\nsification, \ncompression, \ndenoisingAlthough the deep sparse-\ncoding network detects \nodd features from raw \nimages automatically, the \nspeed of the deep sparse-\ncoding network needs to \nbe further improved\n\n13584 S. F. Ahmed et al.\n1 3Table 2  (continued)\nModel General description Study surveyed Main task(s) Outcome(s) Application(s) Remarks\nDora et al. \n(2018)Develop a generative \nmodel, based on deep \npredictive coding, \ntrained using unsu-\npervised learning, for \nprocessing real-world \nimagesThe model was found suitable for \nvarious image classification and \ncomputer vision tasksImage transla-\ntion; com-\nputer vision \ntasksMore studies are neces-\nsary to understand the \norganization of the brain \nto infer real-world images \nin order to improve the \nalgorithm\nSun et al. (2017) Intermediate representa-\ntions with non-negative \nsparse codingEfficiently extended\nthe conventional sparse coding to \nmultilayer architectures;\nexpanded learning capacityObject detec-\ntion; Image \nclassificationReduces the computational \ncosts of sparse coding \nnetwork; compatible \nwith batch normalization \nand other deep learning \ntools; Complies with few \nparameters and layers\nTandiya et al. \n(2018)Monitorization and \nanalysis of radiofre-\nquency by deep predic-\ntive codingThe use of deep predictive coding \nwas found faster and more \nefficient than other machine \nlearning-based approachesAutonomous \nvehicle \ncontrol; \ncommunica-\ntion servicesScalable to networks with \nmany devices robustness \nwould require improve-\nment for complex anoma-\nlies, evaluating longer \nrun-times and employing \nmachine learning \ntechniques to process raw \ndata in different forms\nYe et al. (2019) Improve video anomaly \ndetectionThe predictive coding network \nwith an error refinement \nmodule was able to refine coarse \npredictions, reconstruct errors, \nand create a framework that \nassembles reconstruction and \nprediction modulesBuilding \nsecurity and \nsurveillanceAuto-tuning the hyper -\nparameters could be a \nsignificant improvement \nfor the predictive coding \nnetworks\n\n13585\nDeep learning modelling techniques: current progress,…\n1 3Table 2  (continued)\nModel General description Study surveyed Main task(s) Outcome(s) Application(s) Remarks\nCapsule neural \nnetwork \n(CapsNet)CapsNets use “capsule” neural units to encode \nthe relationship between features and location \nwith capsules as well as transformation matri-\nces. Since this approach acquires translation \nequivariance, CapsNets are more powerful \nthan CNN for samples with misled spatial and \npose information. CapsNets encode part-whole \nrelationships like orientations, brightness, and \nscales among different entities that are objects’ \nfeatures or feature parts. They use shallow \nCNN to acquire spatial information. However, \nthe CapsNets perform poorly on classification \ntasks for missing semantic informationChang & Liu \n(2020)The strict-squash \n(MLSCN) solved the \nproblem of the tradi-\ntional capsule network \nof turning to account \nfor every property of \nan imageThe novel squash functions solved \nthe problem of poor perfor -\nmance issues; due to being \nsensitive to noise of traditional \ncapsule networksImage recogni-\ntionThe dropout mechanism \nneeds further research\nJ. He et al. \n(2019)Extracting the high-level \ninformation of multi-\nscale complex-valued \nfeatures in order to \nadopt complex datasetsNovel encoding unit of restricted \ncomplex-value dense network \nwith another complex-valued \ncapsule, generalizing the \ndynamic routing algorithm for \nimplementation in the complex-\nvalued domainInformation \nextractionApplying the generalized \ndynamic routing algo-\nrithm to fuse the real- \nand imaginary values of \nprimary capsules that are \ncomplex-valued highly \ndecreased the param-\neters to be trained for \ncomplex-valued models \ncompared to real-valued \nmodels of similar dimen-\nsion capsules. However, \nthe models had computa-\ntional complexity\n\n13586 S. F. Ahmed et al.\n1 3Table 2  (continued)\nModel General description Study surveyed Main task(s) Outcome(s) Application(s) Remarks\nDeng et al. \n(2018)A two-layer CapsNet \narchitecture was \npresented in this \npaper. The architecture \nwas designed to be \ntrained with limited \ntraining examples for \nHyperspectral Image \nClassification (HSI)The presented CapsNet architec-\nture achieved an overall 94% \naccuracy and 95.90% on average \nfor the PU and SalinsA datasets; \nwhereas CNN gave 93.45% and \n95.63% accuracies respectivelyHyperspectral \nImage Clas-\nsificationCapsNets was brought for \ntraining HSI classifica-\ntion and made a com-\nparison with the Random \nForests, Support Vector \nMachines and CNN-\nbased state-of-the-art \nclassifiers to prove that \nCapsNets work better for \nHSI classification\nXiang et al. \n(2018)Enhancing the compu-\ntational efficiency and \ncapacity of representa-\ntion of traditional \ncapsule networksBetween a two-staged architecture, \nthe first stage is responsible for \nobtaining semantic and structural \ninformation by employing multi-\nscale information learning; and \nthe second stage is responsible \nfor encoding the hierarchy levels \nof features for multi-dimensional \ncapsulesInformation \nextractionThe improved dropout \nenhanced the robustness \nof the traditional capsule \nnetwork and MS-Cap-\nsNets outperformed \ntraditional CapsNets. \nNo detailed analysis was \nperformed of the network \narchitectures over com-\nplex datasets\nGenerative \nadversarial \nnetwork \n(GAN)GAN is a machine learning algorithm in which \ntwo neural networks compete to increase accu-\nrate predictions. It often operates unsupervised \nand utilizes a framework based on coopera-\ntive zero-sum games to learn. It is capable of \nparallelizing the sampling of generated data. \nHowever, it is harder to train as various forms \nof data need to be provided constantly in order \nto determine whether GAN operates well or notPfau (2017) Stabilizing GANs \nthrough defining the \ngenerator objective \nregarding unrolled \noptimization of the \ndiscriminatorThe introduced method solved \nthe problems of mode collapse \nand stabilized GANs’ training \nwith recurrent generators. It also \nincreased the assortment and \nscope of data distributionStabilizing \nGAN train-\ningThe computational cost \nof each training step is \nas high as it increased \nlinearly with the amount \nof unrolling steps\n\n13587\nDeep learning modelling techniques: current progress,…\n1 3Table 2  (continued)\nModel General description Study surveyed Main task(s) Outcome(s) Application(s) Remarks\nKarras et al. \n(2019)A style transfer-based \nalternate version of the \ngenerative adversarial \nnetworkThe proposed generator improved \nthe general distribution of \nquality metrics, leading to \nunderstanding more accurate \ninterpolation. The generator also \ndisentangled the variation for \nthe latent factorsRedesigning \ngenerator \narchitectureThe style-based genera-\ntors can perform better \nthan traditional GAN \ngenerators\nY. Yan & Guo \n(2020)-The model worked \nthrough two levels \nof a label based \nand a feature-based \nadversarial generator, \ndesigned under a \nbidirectional mapping \nnetwork frameworkThe noise label generator model \nperformed non-random aspects \nof noise labels which are condi-\ntioned on the true label. Moreo-\nver, the data feature generator \nmodel performed conditioning \non data samples on the respec-\ntive true labels. A prediction \nmodel was also presented in the \npaper which performed inverse \nmappings between labels and \nfeaturesLabel learning Both of the generators \nworked simultaneously to \nidentify ground truth labels \nfrom the training samples. \nThese training samples \nwere perceived from the \nfeatures and the candidate \npoints. The authors \ntested the model across \nreal-world and synthesized \ndatasets and got state-of-\nthe-art result performance\nWu & Guo \n(2020)The authors proposed \na novel approach of \nco-learning with dual \nadversarial networks \nfor multi-domain senti-\nment classificationThe proposed approach pulled out \nfeatures of both domain-invari-\nant and domain-specific textsSentiment clas-\nsificationThe proposed method \naligned data across \ndomains through the \nextracted feature space \nand also situated labelled \nand unlabeled data \nbetween each domain. \nThe proposed methodol-\nogy was able to avoid \noverfitting in case of \nlimited data\n\n13588 S. F. Ahmed et al.\n1 3Table 2  (continued)\nModel General description Study surveyed Main task(s) Outcome(s) Application(s) Remarks\nDeep \nBoltzmann \nMachines \n(DBM)DBM is a deep neural network architecture that is \ntrained in a semi-supervised approach. Its archi-\ntecture allows the network to acquire knowledge \nabout complex feature-based relationships. \nDBMs have a wide range of applications like \nfacial expression recognition, text recognition, \nperson identification from audio-visual data, 3D \nmodel recognition, and many moreTaherkhani et al. \n(2018)The proposed research \nwork introduced a \nunique feature selec-\ntion method to embed \nin a Restricted Boltz-\nmann MachineA novel algorithm was proposed \nfor input feature selection from \nlarge datasets. The algorithm \nwas embedded into Deep Boltz-\nmann Machines classifiers to \nreduce input features and learn-\ning errors from large datasetsFeature selec-\ntionThe novel algorithm is \nvery effective for feature \nreduction purposes from \nlarge datasets. Along \nwith reducing the number \nof features, the algorithm \nalso reduced error rate \nand increased classifica-\ntion performance with \nrespect to time variation\nTran et al. \n(2020)To address the difficulty \nof learning and inter -\nference in Recurrent \nTemporal Restricted \nBoltzmann Machine \nmodels of the expo-\nnential character of \ngradient computingTo achieve better results in repre-\nsentation learning and dynamic \ninterference upon sequence \nclassification, the authors \nintroduced SCRBM. The model \nwas designed through rolling \nRBMs with their class nodes \nwith respect to timeLearning and \ninterferenceComparing to standard \nRNNs, SCRBM is more \ncompact with respect to \na number of parameters \nto learn with an equal \nnumber of hidden neural \nunits. However, SCRBM \ncould not accumulate \nlong-term information\nVaswani (2017) A transformer-based \nattention mechanism \ndispensed with recur -\nrence and convolution \nwas presented in the \npaperTwo machine translation tasks \nwere performed that showed bet-\nter results that were paralleliz-\nable and it required less time to \nbe trainedObject detec-\ntionTransformer-based models \nwere yet to experiment \non problems of input and \noutput modalities which \ninclude images, audio \nand video data\nDahl et al. \n(2010)Develop a novel method \nbased on DBN \narchitecture and \nmean-covariance RBM \n(mcRBM) for phone \nrecognitionThrough the use of mcRBM \nfeatures in conjunction with \nDBNs, a 20.5% phone error rate \nwas achievedPhone recogni-\ntion, face \nlabellingThe mcRBM is useful for a \nsmall training set but suf-\nfers from representational \ninefficiency issues\n\n13589\nDeep learning modelling techniques: current progress,…\n1 3Table 2  (continued)\nModel General description Study surveyed Main task(s) Outcome(s) Application(s) Remarks\nStacked \ndenoising \nautoencoders\n(SDAE)SDAE is an expansion of the stacked autoencoder. \nSeveral denoising autoencoders are connected \nin a chain to form a SDAE. An important \nfeature of SDAE is unsupervised pre-training, \nwhich occurs layer by layer as input data is \npassed through. However, it has a high compu-\ntational costLiou et al. \n(2014)Using Elman network \nto work with word \nsequences from litera-\nture workThe training method consisted of \nencoding each word into sepa-\nrate vectors in semantic space \nand it was related to correspond-\ning entropy coding. The trained \ncodes had reduced entropyIndexing, \nranking, and \ncategoriza-\ntion of liter -\nary tasksIt is necessary to investi-\ngate whether reduced \nerrors were attainable \nwithout utilizing the \nrevised codes and the \nsame methods\n\n13590 S. F. Ahmed et al.\n1 3\nwell as the main surveyed studies in terms of their main objectives, outcomes, and applica-\ntions, have been summarized in Table 2.\n5  Advantages and challenges of deep learning models\nThe several advantages underpinning deep learning models, including image processing \nand recognition, speech recognition, self-driving cars, and so on, have sparked such wide-\nspread attention. The main benefit of using deep learning models over machine learning \n(ML) technologies is their capacity to produce new features through a limited range of \nfeatures in the trained dataset (Kotsiopoulos et al. 2021). These models can generate new \ntasks for solving current ones as well as they also cover a variety of human life aspects. \nA significant amount of time can be saved using deep learning models when dealing with \nmassive datasets, as deep learning algorithms can generate features without the need for \nhuman intervention (Gupta et al. 2021).\nDespite their numerous advantages, deep learning models have a number of noticeable \nchallenges. First, they are unable to provide arguments supporting the fact that a particular \nconclusion is reached (Signorelli 2018). In addition, unlike typical machine learning, peo-\nple are not able to follow an algorithm to figure out why the system decides that the image \nportrayed is a dog rather than a cat. To correct these types of errors in deep learning algo-\nrithms, the entire algorithm must be revised, which requires additional time. Also, high-\nperformance computing units, high powerful GPUs and enormous quantities of storage are \nneeded to train the models. Therefore, deep learning models require more time compared \nto traditional ML methods (Palanichamy 2019). The challenges of applying the deep learn-\ning models are summarized in Table 3 along with their advantages.\nIn general, deep learning (DL) often produces better results as opposed to machine \nlearning. For example, the largest data portion of an institute/organization is unstructured \nsince it appears in so many different formats, including texts and images. Most machine \nlearning (ML) algorithms struggle to make sense of unstructured data, therefore this type \nof data is underutilized. Herein lies the strength of deep learning. The main benefit of using \nDL over other ML algorithms is its capacity to produce novel features from limited sets of \nfeatures already present in the training dataset. It follows that DL algorithms can devise \nnew challenges to address existing problems. DL enables full-cycle learning by using neu-\nral networks’ capability for featurization, from inputting raw data to producing an outcome. \nThis approach allows for the optimization of all relevant parameters, which ultimately \nresults in improved precision.\nA key advantage of using the DL approach is that it can perform feature engineering \non its own. In this method, the algorithm is not given any explicit instructions, but rather \nit automatically searches through the data for features that correlate and then combines \nthem to facilitate faster learning. Because of its ability to handle massive data, DL scales \nextremely well. The algorithms of DL can be learned on a wide range of data formats \nwhile still producing insights relevant to the objectives of the training. For instance, DL \nalgorithms can be utilized to identify correlations between social media activities, market \nresearch, and other factors in order to predict the future stock value of a particular firm.\nThere are a number of issues with DL models as well. In order to outperform alternative \nmethods, deep learning needs access to a massive dataset. Therefore managing data is the \nkey challenge that hinders DL in industrial implementations. Deep learning is currently \n\n13591\nDeep learning modelling techniques: current progress,…\n1 3Table 3  Advantages and challenges of deep learning modelling techniques\nDeep learning models Advantages Challenges\nVector space model (VSM) - Ranks retrieved documents by identifying and rating the most \nrelevant text documents for a specific query\n- Identifies similarities among distinct documents, and thus \nassists to detect plagiarism\n- Simple in structure as it is constructed on the basis of linear \nalgebra\n- Permits for partial matching\n- Term weights are not binary\n- Allows for the computation of the similarity degree between \ndocuments and queries on a continuous scale- Textual VSM is incapable of coping with linguistic ambiguity \nand a variety\n- Theoretically, terms are assumed to be statistically independent\n- In the presentation of vector space, the order of the terms that \nappeared in the documents is lost\n- Keywords search must exactly match the terms in the docu-\nment; word substrings can lead to a match of \"false positive \"\n- Due to their low similarity values, long documents/papers are \npoorly represented\n- Suffers from polysemy and synonym\n- The process of weighting is intuitive, although it is not very \nmuch formal\n- Sensitivity to semantics; papers with a similar context and a \ndistinct term vocabulary will not be associated, causing a match \nof \"false negative\"\nConvolutional neural network (CNN) - Feature engineering is a time-consuming and complicated \nprocess used traditionally in image processing that is not \nrequired in CNN\n- The models are considered robust under different challenging \ncircumstances such as complex background, system orienta-\ntion and size, various resolutions, and illumination\n- After training, the efficiency of testing time is substantially \nhigher than that of other approaches including SVM\n- Requires less time in classification\n- Without human supervision, automatically can detect critical \nparameters\n- High precision in the problems of image recognition- Poor data labelling, which can significantly reduce system \nperformance and accuracy\n- Comparatively larger data sets are required to train, as well as \ncorrect annotation, which requires domain expertise\n- Optimization challenges arising from the complexity of the \nmodels, and hardware limitations\n- Sometimes take a longer time to train data\n- Computational cost is high\n- Takes more time to train using a bad GPU\n\n13592 S. F. Ahmed et al.\n1 3Table 3  (continued)\nDeep learning models Advantages Challenges\nRecurrent neural network (RNN) - RNNs are frequently used in conjunction with convolutional \nlayers to extend the effective pixel neighborhood\n- It is advantageous in forecasting time series since the highlight \npoint works as a reminder of previous inputs\n- It takes a long time to train an RNN for computational prob-\nlems\n- Time series inputs allow RNNs to handle nonlinear dynamics, \nas well as long-term correlations\n- In RNN, weight remains the same over all the layers, limiting \nthe parameters the network needs to learn\n- Any length of input can be processed by RNN\n- The model dimension remains constant even the input dimen-\nsion is increased- Suffers from gradient and exploding vanishing issue, which \nlimits longer sequences\n- Unable to stack up\n- Training processes are complex and slow\n- RNN is less powerful than CNN\n- When Relu or Tanh is used as an activating feature, it cannot \nhandle exceedingly long sequences\n- The flow of information across the layers/levels makes it a \nnightmarish task\n- It cannot be progressed without knowing the structure of the \ntree for each input sample\n- The computation process is comparatively slow because of its \nrepeated/recurrent nature\nHierarchical bidirectional recurrent \nneural network (HBRNN)- Each layer in the HBRNN handles classification tasks and \nplays a critical role in the effectiveness of the entire network\n- Each layer in the network can constitute a classifier hierarchy\n- In most cases, its efficiency and accuracy are comparatively \nbetter than the other networks as the HBRNN is constructed \nthrough the extensions of bidirectional recurrent neural net-\nwork (BRNN) and RNN\n- Simultaneously forecasts both negative and positive time \ndirections- Must be known of both the beginning and end of the sequence\n- Suffers from computational complexity because of considering \nmore parameters than an RNN\n- May not be suitable in the applications of real-time speech \nrecognition\n- It is necessary to have access to the entire sequence before mak -\ning predictions\n- Since HBRNN anticipates future words, it may not be appropri-\nate to forecast the next word based on the prior ones. In this \ncase, using HBRNN will result in poor accuracy\n- Large datasets are needed for better prediction accuracy\n\n13593\nDeep learning modelling techniques: current progress,…\n1 3Table 3  (continued)\nDeep learning models Advantages Challenges\nRecursive neural network (RvNN) - Extremely beneficial for analyzing language and natural \nscenes\n- It can be utilized to learn the tree-like structure\n- Useful for categorization tasks such as classifying metagen-\nomic sample morphologies\n- RvNN is capable of identifying the samples which are com-\nparatively similar to one another on the basis of the scoring \nfunction\n- Future data can be annotated with class relationships using the \nrecursive neural network\n- Provides a representation of high-dimensional features\n- Capable of generating a tree-like hierarchical relationship \nbetween the samples\n- Suitable for both supervised and unsupervised learning tasks \nas it is capable of addressing both regression and classifica-\ntion problems- Recursive neural networks may not be as accurate as deep belief \nnetworks and graph neural networks\n- It faces a problem with vanishing gradient\n- Tree structures of the input samples need to be known during \nthe training period\n- Parsing is domain-specific and slow\n- RvNN is afflicted by the problem of long-distance reliance\n- Due to the intrinsic complexity, the recursive neural networks \nare intrinsically complicated\n- Computationally quite expensive during the learning phase\n- The process of obtaining labelled data for RvNNs is excruciat-\ningly difficult and time-consuming\n- Labels are required for every bigram and its supersets that is not \neasy to find in real world\nNeural tensor network (NTN) - NTN can effectively explain the complicated semantic link -\nages between relationships and entities\n- Minimize the entity representation learning sparseness \nproblem\n- Generalizes numerous previous models of the neural network\n- Provides a comparatively powerful way for modelling rela-\ntional data than a typical neural network layer does\n- It can multiply the two inputs rather than only implicitly via \nnon-linearity\n- Able to provide higher precision in the prediction of invisible \nconnections between entities via reasoning within a specified \nknowledge base\n- Allows database extension even when no exterior textual \nresources are available- The level of computational complexity is extremely high\n- Huge triplet samples are required to properly learn\n- Has a minimal impact on sparse knowledge graphs on a wide \nscale\n- Require the estimation of a huge amount of parameters, which \nfrequently leads to overfitting\n- Long training period is needed compared to the other neural \nnetwork models as it comprises so many parameters\n\n13594 S. F. Ahmed et al.\n1 3Table 3  (continued)\nDeep learning models Advantages Challenges\nDeep belief networks (DBN) - The greedy learning approach with DBNs can address the dif-\nficulty of appropriate parameter selection\n- No labelled data is required as it is also an unsupervised \nprocess\n- Have significant benefits in learning input features applied \nbroadly in numerous fields including disease diagnosis, \nspeech and face recognition, image processing, traffic flow \nforecasting, breast cancer classification, and interpretation of \nnatural language\n- DBNs benefit from the steady characteristic learning of ran-\ndomly input samples, enabling highly efficient application in \nthe areas of handwriting, face and speech recognition\n- It uses layer-by-layer training to effectively learn a deep hier -\narchy probabilistic model for the performance optimization of \nclassification problems- DBNs do not take into consideration the two-dimensional \nframework of input images, which could considerably impact \ntheir performance as well as application in multimedia analysis \nand computer vision problems\n- High computational cost to train a DBN\n- The lack of clarity regarding the processes necessary to further \noptimize the network using maximum training approximation\n- Due to the vast amount of data involved, the DBN training \nprocesses are time-intensive, and thus may not meet the needs \nof real-time application systems\n- Performance is poor due to the input data being clamped when \na contrastive divergence learning algorithm is used to pre-train \nDBN\nGenerative adversarial network (GAN) - It is capable of parallelizing the sampling of generated data\n- GAN does not require to estimate a probability distribution by \nthe introduction of a lower bound like a variational autoen-\ncoder\n- It has been empirically demonstrated to yield sharper and bet-\nter results than any other type of generative model, particu-\nlarly variational autoencoder\n- Capable to generate similar types of data, image, audio, video \nand texts to the original one\n- GANs delve into the minutiae of data and can quickly interpret \nit into many formats, making them useful for machine learn-\ning tasks\n- Different types of objects such as trees, bicyclists, parking car \non streets, and people can be recognized easily using GANs\n- Able to measure the distance between two different objects- The data generating process is inherently slow, which is exac-\nerbated when dealing with the generation of high-dimensional \ndata, such as voice recognition\n- GAN training is unstable and challenging to converge\n- It suffers from mode collapse issue\n- Harder to train as various forms of data need to be provided \nconstantly in order to determine whether GAN operates well \nor not\n- Producing outcomes from speech or text is an extremely com-\nplicated process\n- Due to the instability of training and the approach of unsuper -\nvised learning, it becomes more difficult to generate output\n- Lack of intrinsic evaluation metrics\n- Unable to forecast the density accuracy and identify an image is \ndense enough to proceed with\n- Inverting in GANS is not simple\n\n13595\nDeep learning modelling techniques: current progress,…\n1 3Table 3  (continued)\nDeep learning models Advantages Challenges\nCapsule neural network (CapsNet) - CapsNet can achieve state-of-the-art performance due to the \nless complexity in its structure\n- It does not require so many parameters like convolutional \nneural network\n- Well generalized ability on smaller datasets makes CapsNets \nsuitable for usage in a wide range of applications\n- The usage of pose matrices or parameter vectors allows Cap-\nsNets to recognize objects, irrespective of the viewpoint\n- Capable of capturing class object instantiation parameters\n- CapsNets represent more specific features to understand what \nand how the network is learning- CapsNets are not able to perform consistently across various \ndatasets, particularly large datasets such as ImageNet\n- The rigid concept of capsule entities may make the concept \ninappropriate for applications not related to computer vision\n- Not suitable for online-based training\n- Possesses higher complexity to implement compared to convo-\nlutional neural networks\n- As the CapsNet generates matrix or vector outputs it cannot \nsimply reuse previous loss functions\n- Not able to distinguish closer objects\n- The routing process is dynamic as well as difficult to paral-\nlelize, limiting GPUs from fully utilizing their computational \ncapability\nAttention mechanism - Allows guidance of any tasks of a complex system including \nprediction, modelling, and identification\n- It is incorporated with a recurrent neural network and long \nshort-term memory which enables the modelling of lengthy \ntemporal dependencies\n- Controls the cognitive process flexibly by concentrating on a \ncollection of elements\n- It is capable of focusing on spatial dimensions, temporal \ndimensions, or various features of the input vectors\n- Attention mechanism can link each input vector to generate \nthe output vector more directly and symmetrically\n- It can deduce information from an input that is most relevant \nto completing a task, which improves performance, particu-\nlarly in language processing- Attention mechanism adds additional weight parameters in the \nmodel, which might lengthen training time, particularly if the \nmodel’s input data contains long sequences of data\n- It is a long and tedious process to parallelize the system\n- Attention mechanisms often keep them distinct and disjoined \nusing the dedicated channel and spatial attention module, \npreventing interaction between these two modules, thus not \noptimal\n- Numerous attention methods do not prioritize channel interac-\ntion when computing attention weights, hence reducing infor -\nmation transmission\n- The majority of attention techniques introduce significant addi-\ntional computation in model parameter form, causing slower \nand larger architectures\n\n13596 S. F. Ahmed et al.\n1 3Table 3  (continued)\nDeep learning models Advantages Challenges\nDeep Boltzmann machine (DBM) - Capable of learning various levels of representation from input \ndata using multilayer structures\n- Promising in solving speech and object recognition issues\n- DBM model can effectively use large volumes of unlabeled \ndata\n- Can handle ambiguous inputs more robustly\n- Efficiently performs learning inferences and parameters with \ngreedy-layered training\n- Capable of identifying latent features in data\n- DBM model can integrate multiple data sources into a single \nrepresentation that incorporates useful retrieval and classifica-\ntion features- Maximum probabilistic learning in deep Boltzmann machines is \na challenge due to the hard inference issue caused by partition \nfunctions\n- Multiple hidden layers exacerbate the difficulty of learning in \ndeep Boltzmann machines\n- Approximate inference is noticeably slower than a single pass \n(bottom-up) as in DBNs\n- DBM training is computationally expensive compared to the \ntraining of the deep belief network\n- Less intuitive and difficult to train as they require layer-by-layer \nsampling and pre-training\nStacked denoising autoencoders (SDAE) - Using SDAE, the weight errors of the process of fine-tuning \ncan be reduced\n- Can effectively avoid gradient vanishing and over-fitting issues\n- Unsupervised learning process utilizing the SDAE is reliable \nin the load forecasting\n- Can be used in learning compact data representation\n- SDAE improves deep learning accuracy by embedding noisy \nautoencoders in the layers\n- It can be used to distort data and to introduce some noise to \ngeneralize throughout the test set\n- Delivers a raw data version with detailed as well as notewor -\nthy feature information- Optimizing a threshold that is sufficiently generalizable to \npreviously unseen test scenarios is challenging\n- Lose random control over the input\n- High computational cost\n- Inability to scale to the features with high-dimension\n- SDAE training is comparatively slower than the other compet-\ning algorithms as it relies on iterative as well as numerical \noptimization in learning model parameters\n- Complicated by the input data’s dimensionality and the \nrequirement for computationally demanding model selection \ntechniques to adjust hyperparameters\n- A lengthy training time may be required for highly optimized \nexecution\n\n13597\nDeep learning modelling techniques: current progress,…\n1 3Table 3  (continued)\nDeep learning models Advantages Challenges\nDeep energy model (DEM) - Capable to incorporate several hidden deterministic layers \nwith single hidden stochastic layers\n- Joint-based learning of DEM enhances generative perfor -\nmance as well as alters the representations learnt at every \nlevel\n- It can perform interface and learning efficiently using hidden \ndeterministic layers rather than hidden stochastic layers\n- DEMs are flexible in modelling\n- Useful in object recognition, sequence labelling and image \nrestoration\n- Useful tool to model probability distributions of high-dimen-\nsion- No direct sampling method in DEM like flow or autoregressive \nmodels as it is not able to compute easily how likely a probable \nsample is\n- DEMs can yield a longer period to converge although they work \nin theory\n- Less popular as a result of computational difficulties\n- It is difficult to evaluate likelihood (learning) in DEMs\n- Feature learning is not available\n- Inference in the deep energy-based models is quite difficult \ndue to the function of partition, which is often impossible to \ncompute precisely\nPredictive coding network (PCN) - Effective for several classification problems, such as image \nclassification\n- Can be trained using unsupervised learning\n- Faster and more efficient than other machine learning-based \napproaches\n- Automated adjustment of hyperparameters could improve \nPCNs considerably\n- Applicable in the field of computer vision, where natural \nimage and video classifications are performed\n- Single architecture can be reused in PCNs to run top-down \nand bottom-up processes recursively to refine their presenta-\ntion concerning more exact and conclusive object recognition- It is difficult to develop mature and complicated deep predictive \ncoding network designs\n- Computational time is longer than ordinary networks having the \nsame amount of layers\n- More layers are needed to model complicated and nonlinear \nrelations in data\n- A more difficult task simply requires the brain to process infor -\nmation more slowly through the same network\n- Suffers from the uncertainty about how the estimated error \nminimization functions\n- Each stage of the PCN framework’s sub computation may \nconceal an intractable computing challenge\n\n13598 S. F. Ahmed et al.\n1 3Table 3  (continued)\nDeep learning models Advantages Challenges\nRestricted Boltzmann Machine (RBM) - Modelling capacity can be enhanced by incorporating more \nhidden variables\n- The hidden and visible unit sets are not conditionally depend-\nent in RBMs\n- Suitable for classification, dimensionality reduction, regres-\nsion, topic modelling, feature learning, and collaborative \nfiltering\n- RBMs are able to be trained in both supervised and unsuper -\nvised processes based on the particular task\n- Capable to examine and determine several hidden variables \nincluding drama, action, and fantasy\n- Useful for learning unsupervised features\n- It is comparatively faster than a traditional DBM because of \nthe limitations in the number of connections among the nodes- Computationally intensive process to learn an RBM\n- The approximation inference is significantly slower than the \nsingle bottom-up pass used in RBMs\n- In the case of large datasets, the combined optimization of the \nparameters is not feasible due to the slower approximation \ninterface of RBMs\n- Training is quite complicated since calculating the function of \nthe energy gradient is not easy\n- Suffers from weight Adjustment\n- The training algorithms of RBMs such as contrastive diver -\ngence and parallel tempering have limitations for complex and \nhigh-dimensional data processing\n- With lower temperature chains, training can be converged more \nrapidly, but the accuracy suffers\n\n13599\nDeep learning modelling techniques: current progress,…\n1 3\nlimited in its applicability because of the extensive computer resources and training data-\nsets it necessitates. It is still a mystery as to how exactly DL models arrive at their conclu-\nsions. Not like in traditional ML, where we can trace back the reasoning behind a system’s \nidentification of a given image as representing a cat rather than a dog. To rectify errors in \nDL algorithms, the entire algorithm must be modified. However, no universally applicable \ntheory is available that can help us to choose the appropriate DL tools as it needs knowl-\nedge of training methods, topology, and other features.\n6  Comparative analysis of deep learning modelling techniques\nThrough the present review, it has been determined to what extent deep learning (DL) \nmodelling techniques can be used in real-world applications. In addition, the methods \nemployed, the outcomes, and the challenges of DL that have been modelled are identi-\nfied. The comparative study compares available DL techniques based on their strengths and \nweaknesses, as well as performance metrics. The advantages and challenges outlined in the \nprevious section make up the basis for the comparative study on strengths and weaknesses.\n6.1  Comparative study based on weakness and strength\nOne of the common DL models, namely the vector space model (VSM) is found simple \nin structure and allows the computation of the similarity degree between documents and \nqueries on a continuous scale. In contrast, the VSM assumes that words are statistically \nindependent. Additionally, documents with a similar context and distinct term vocabulary \nwill not be connected, resulting in a \"false negative\" match. Convolutional neural network \n(CNN), on the other hand, uses less time for classification and has good precision in image \nrecognition challenges. However, comparatively larger data sets are required to train for \nCNN. Poor data labeling is another disadvantage of CNN, which can dramatically affect \nsystem performance and precision. Several classification issues, including image classifica-\ntion, have been successfully addressed using a predictive coding network (PCN). One of its \ndrawbacks is that there is a lack of certainty regarding how the estimated error minimiza-\ntion functions.\nIt is observed that the recurrent neural network (RNN) is useful for time series forecast-\ning. In RNN, weight remains constant across all levels, minimizing the number of param-\neters the network must learn. However, gradient and explosion vanishing issues limit the \nlength of RNN sequences. Its computation process is comparatively slow because of its \nrepeated/recurrent nature. However, for highly optimal execution, a long training period \nmay be needed. In most cases, the efficiency and accuracy of the Hierarchical bidirectional \nrecurrent neural network (HBRNN) are comparatively better than the other networks as it \nis constructed through the extensions of bidirectional recurrent neural network (BRNN) \nand RNN. Before predictions can be made with HBRNN, the full sequence must be acces-\nsible. On the basis of the scoring function, the recursive neural network (RvNN) is capable \nof detecting samples that are relatively similar to one another. Obtaining labeled data for \nRvNNs is an incredibly challenging and time-consuming task. Compared to a typical neu-\nral network layer, a neural tensor network (NTN) is a powerful tool for modelling relational \n\n13600 S. F. Ahmed et al.\n1 3\ndata. Massive triplet samples are required for NTN to properly train, however, this has a \nlittle effect on sparse knowledge graphs on a global scale.\nDeep belief network (DBN) enables highly efficient applications in the domains of \nhandwriting, face, and speech recognition due to the model’s continual learning of the \ncharacteristics of randomly input samples. However, DBNs do not account for the two-\ndimensional structure of input images, which could significantly affect their performance. \nAttention mechanism can deduce information from an input that is most pertinent to \naccomplishing a task, hence enhancing performance, especially in language processing. \nAmbiguous inputs can be handled by Deep Boltzmann machine (DBM) more robustly. \nDBM is capable of identifying latent features in data. One of the limitations of DBM is \nthat maximum probabilistic learning in DBM is a challenge due to the hard inference issue \ncaused by partition functions. The restricted Boltzmann machine (RBM) is comparatively \nfaster than a traditional DBM because of the limitations in the number of connections \namong the nodes. But the process of learning an RBM is computationally intensive, and in \nthe case of large datasets, the combined optimization of the parameters is not feasible due \nto the slower approximation interface of RBMs.\nA well-generalized ability on smaller datasets makes capsule neural network (CapsNet) \nsuitable for use in a wide range of applications. CapsNets are not able to perform consist-\nently across various datasets, particularly large datasets such as ImageNet. Using hidden \ndeterministic layers as opposed to hidden stochastic layers, the deep energy model (DEM) \ncan perform interface and learn quickly. It is less popular due to computational difficulties \nand the difficulty of evaluating the likelihood (learning) in DEMs. A generative adversar -\nial network (GAN) does not require estimating a probability distribution by introducing a \nlower bound like a variational autoencoder. But GAN has a mode collapse problem, and its \ndata-generation process is intrinsically slow.\n6.2  Comparative study based on performance criteria\nThis section compares the performance of several deep learning modelling techniques \nbased on two key performance factors such as prediction accuracy and complexity level, \nwhich are crucial for suitable model selection. The study of the computational complex-\nity of deep learning models is important because it can answer the fundamental question \nof why deep learning architecture performs substantially better than traditional machine \nlearning algorithms. In addition, understanding the complexity is useful to analyze and \ncompare different deep learning models and improve their performance. The complexity \nanalysis of deep learning models highly depends on the model structure; on the other hand, \nthe models are structurally different. Therefore, they cannot be generalized and directly \ncompared to one another.\nOne of the recent studies (Hu et al. 2021) surveyed the latest research on model com-\nplexity in deep learning. In the study, four factors that influence the deep learning model \ncomplexity were surveyed: (i) model framework including activation functions such as tanh, \nReLu, and others, (ii) model size, including the depths of the neural network layers and the \nnumber of trainable parameters, (iii) optimization process such as the number of iterations \n(epochs) to optimize the model, optimization algorithms, hyperparameters, and (iv) data \ncomplexity, which includes class imbalance and high dimensional data. The performance \nof a DL model also relies on other parameters such as hardware platforms (high-end GPU), \ncompiler optimization, and implementation tools. Based on some of those factors and lit-\nerature availability, we analyze the performance and computational complexity of different \n\n13601\nDeep learning modelling techniques: current progress,…\n1 3\nTable 4  Comparison of different variants of deep learning architectures applied in different fields based on \nperformance criteria and complexity\nDeep learning model Applied field Performance (prediction accu-\nracy or other matrices)Compu-\ntational \ncomplexity\nAE Time Series Prediction High High\nBD-LSTM Sentiment Analysis High High\nBi-LSTM Time Series Prediction Medium High\nCNN Sentiment Analysis Medium High\nMalicious URLs Detection High Medium\nHuman Activity Recognition High High\nIntrusion Detection Systems Medium Medium\nCNN-LSTM Malicious URLs Detection High Medium\nGRU Time Series Prediction Medium Medium\nLSTM Time Series Prediction Medium Medium\nRNN Sentiment Analysis Low Low\nTime Series Prediction Low Medium\nIntrusion Detection Systems High Medium\nRNN-GRU Sentiment Analysis Medium Medium\nRNN-LSTM Sentiment Analysis High Medium\nRNN-LSTM Human Activity Recognition Medium Medium\nvariants of deep learning models across different application fields (Seo et al. 2020; Zeroual \net al. 2020; Cui et al. 2018; Vazhayil et al. 2018; Shakya et al. 2018), and classify them into \nthree categories: Low, Medium, and High, as illustrated in Table  4. The lack of relevant \ncomparative DL literature is identified as the key challenge behind this comparative survey.\nThe time complexity of an algorithm mainly depends on the input data, and it can be \ndescribed using the big-oh notation. Due to its complex nature of architecture, structural \ndifferences, and many other factors, the time complexity of the deep learning model is usu-\nally measured by how long it takes a model to solve a problem on specified hardware. \nAn empirical analysis of how the configuration settings affect the running time of deep \nlearning models was conducted by Lee and Chen (2020). The analysis demonstrated that \nmodel complexity increases the running time, but if the data quality is below average, it \nis not worthwhile to increase model complexity. In the sentiment analysis task, increasing \nthe CNN model’s complexity may not improve the performance, whereas increasing the \nRNN model’s complexity invariably improves the model performance. Bi-LSTM is found \nto be superior to other CNN and RNN models for sentiment analysis (Seo et al. 2020). In \nmalicious URL detection, CNN-LSTM gives comparatively high accuracy than ordinary \nCNN with a little more computational cost (Vazhayil et al. 2018). However, CNN shows a \nsignificant improvement over RNN-LSTM in computer vision tasks such as human activity \nrecognition (Shakya et al. 2018). CNN is a better choice in intrusion detection systems if \nit is a binary classification problem (Cui et al. 2018). For multi-class classification, regular \nCNN performs poor than others while RNN is a good choice because of the sequential \ndata. It is much more computationally expensive than RNN in its architecture. Compared \nto RNN, Auto Encoder (AE) shows superior performance in forecasting time-series data. \nBut RNN is relatively faster and needs less computational cost than LSTM, Bi-LSTM, and \nAE (Zeroual et al. 2020).\n\n13602 S. F. Ahmed et al.\n1 3\n7  Future of deep learning\nAs we step foot into a new era of surplus big data and information, the future of deep learn-\ning is not only prominent but vital for the advancement, resilience, and problem-solving \nendeavors of the globe. Deep learning has become a necessary tool across every discipline \nfrom science, engineering, humanity, and health to climate studies and many more. From \ndeveloping cybersecurity and surveillance to performing quantum computing, deep learn-\ning will be an evident constant of the future. With the great success of deep networks in \nthe field of computer vision and the development of artificial intelligence, being able to \nextract meaningful and correct features from data to generate necessary outcomes, without \ndiscrimination and being more tolerant of nuisance variations in data (Deng 2014; Guo \net al. 2016), deep learning is the basis for future innovations. As of yet, further knowledge \nand understanding are required to improve and construct deep learning networks that deal \nwith complex high dimensionality data and variations to characterize inputs and outputs \nefficiently (Kato et al. 2016).\nThe growing interest in investments, particularly of giant tech companies (Google, \nFacebook, Apple), represents and signals the value and potency of deep learning in the pre-\nsent and future. Although deep learning demands high computational power and constant \ntraining to generate reliable results, more work is yet to be done to ensure that deep learn-\ning networks are efficient and cost-effective in extracting and identifying distinct features \nfrom real-world data, mimicking the ability of biological intelligence. Therefore, when \nconstructing a deep learning methodology, it is important to ensure that the model can deal \nwith uncertainty, is scalable, and has transferable qualities to be implemented and applied \nto multiple problem systems (Zhang et al. 2020). Alongside the development of deep learn-\ning techniques, the availability of user-friendly hardware and software systems are signifi-\ncant future prospects for deep learning.\nLarger and more extensive datasets are necessary for enhancing the performance of \nDL models in a complex and dynamic construction environment including many human \nresources, several types of equipment, and a variety of human and equipment activities \n(Fink et al. 2020). As humanity surfs the wave of artificial intelligence and deep learning, \nethical frameworks must be developed to ensure the sound employment and enhancement \nof deep learning techniques in order to manage proper conduction and utilization of big \ndata that are fed into deep learning architectures, subsequently generating beneficial and \nsustainable solutions. Due to the small sample size of training and limited unsafe activities \nconsidered, several workers’ actions can not be recognized (Ding et al. 2018). With a larger \ndataset, the model can therefore improve and give more precise results. Nevertheless, there \nis presently no publicly available complete and standardized dataset, also for particular \ntasks like activity recognition, pose detection and object detection, as well as for different \nviews, a wide range of construction sites, occlusion circumstances, and lighting.\nCombining deep learning with expert knowledge can be a fruitful area of research since \nmodels may be dynamically augmented with acquired new data, resulting in effective digi-\ntal twins which can help in maintenance decision making. Despite the fact that physics-\ninduced deep learning is now pursuing multiple directions, there is no agreement or no \nconsolidation on various directions as well as how they can be translated to industrial appli-\ncations. There is a need for additional studies to refine and consolidate these techniques, \nwhich may help increase the generalization ability of the models developed. Another issue \nthat must be addressed in future studies is the effective selection and composition of sets of \ntraining data. This is especially important in environments that are constantly changing and \n\n13603\nDeep learning modelling techniques: current progress,…\n1 3\nhave extremely variable operating conditions, where the training dataset is not representa-\ntive of the whole range of predicted operating conditions. Continuous decisions must be \nmade as to whether new data needs to be included in training datasets and the algorithms \nupdated, or whether the information is repetitive and included already in the datasets used \nfor training the algorithms.\n8  Conclusion\nDeep learning (DL) is a thriving multidisciplinary field that is still in its nascent phase. \nWith the growing availability of data, DL architectures can be successfully applied to \nproblems across various sectors in the modern world. This paper provides a comprehen-\nsive systematic review of the state-of-the-art DL modelling techniques. Some models can \nbe trained by two or more methods, which means their efficiency relies on the domain in \nwhich they are used. The use of hierarchical layers for proper data classification, as well \nas supervision in learning to determine the importance of the database of interest, are \nboth important factors to develop robust DL models. While nearly all of the models dis-\nplay robustness to some extent, existing techniques are still flawed, which subjects them \nto criticisms. With the availability of big data across various domains, the quality of data \ncan become an issue when training DL models. Training DL models can also be very time-\nconsuming, expensive, and requires hundreds of correct examples for better accuracy, \nwhich can limit their use for everyday purposes or in sensitive security systems. The result-\ning models may also be domain-specific and, therefore, may have restricted applications. \nIn addition, DL is susceptible to deception and misclassification, which can threaten the \nsocial and financial securities of individuals and/or corporations. Getting stuck on local \nminima also makes most models unsuitable for online modes.\nCNNs, RNNs, GANs, and autoencoders are the more frequently used DL architectures \nacross various sectors. However, the potential application of other architectures in cur -\nrent areas that use DL is widely unexplored. This paper found that advanced DL models, \nwhich are essentially hybrid conventional DL architectures, have the potential to overcome \nthe challenges experienced by conventional models. Moreover, generative models exhibit \ngreater capabilities as they are less reliant on examples. Future networks should strive to \ngenerate a set of possible outcomes, instead of providing one final prediction for the input, \nwhich may help tackle the issue of distorted or unclear inputs. Developing new strategies \nto optimize parameters, particularly hyperparameters, is another possibility that requires \nfurther investigation. Capsule architectures may dominate future DL models as they offer \nan enhanced way of routing information between layers. If the current challenges can be \naddressed, DL models can potentially contribute to further innovations in the field of AI \nand for solving far more complex problems.\nAcknowledgements The authors highly express their gratitude to Asian University for Women, Chatto-\ngram, Bangladesh for their support in carrying out this study.\nFunding Open Access funding enabled and organized by CAUL and its Member Institutions.\nDeclarations \nConflict of interest The authors declare that they have no known competing financial interests or personal \nrelationships that could appear to have influenced the work reported in this study.\n\n13604 S. F. Ahmed et al.\n1 3\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly \nfrom the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\nAbbas AA, Naderi E, Gandali A, Hanieh M (2016) Comparative study of static and dynamic artificial neural \nnetwork models in forecasting of tehran stock exchange. Int J Bus Dev Stud 8:43–59. https:// doi. org/ \n10. 22111/ IJBDS. 2016. 2635\nAbdel-Zaher AM, Eldeib AM (2016) Breast cancer classification using deep belief networks. Expert Syst \nAppl 46:139–144. https:// doi. org/ 10. 1016/j. eswa. 2015. 10. 015\nAbedinia O, Amjady N, Ghadimi N (2018) Solar energy forecasting based on hybrid neural network and \nimproved metaheuristic algorithm. Comput Intell 34(1):241–260. https:// doi. org/ 10. 1111/ coin. 12145\nAchanta S, Gangashetty SV (2017) Deep Elman recurrent neural networks for statistical parametric speech \nsynthesis. Speech Commun 93:31–42. https:// doi. org/ 10. 1016/j. specom. 2017. 08. 003\nAdhikari A, Ram A, Tang R, Lin J (2019) DocBERT: BERT for document classification. arXiv: 1904. 08398\nAfshar P, Mohammadi A, Plataniotis KN (2018) Brain tumor type classification via capsule networks. In: \nProceedings - international conference on image processing, ICIP. https:// doi. org/ 10. 1109/ ICIP. 2018. \n84513 79\nAhmad J, Farman H, Jan Z (2019) Deep learning methods and applications. In: SpringerBriefs in computer \nscience. https:// doi. org/ 10. 1007/ 978- 981- 13- 3459-7_3\nAkkus Z, Galimzianova A, Hoogi A, Rubin DL, Erickson BJ (2017) Deep learning for brain MRI seg-\nmentation: state of the art and future directions. J Digit Imaging 30:449–459. https:// doi. org/ 10. 1007/ \ns10278- 017- 9983-4\nAlain G, Bengio Y, Courville A, Fergus R, Manning C (2014) What regularized auto-encoders learn from \nthe data-generating distribution. J Mach Learn Res 15(1):3563–3593\nAlam MR, Bennamoun M, Togneri R, Sohel F (2017) A joint deep Boltzmann machine (jDBM) model for \nperson identification using mobile phone data. IEEE Trans Multimed 19(2):317–326.  https:// doi. org/ \n10. 1109/ TMM. 2016. 26155 24\nAlemany S, Beltran J, Perez A, Ganzfried S (2019) Predicting hurricane trajectories using a recurrent neural \nnetwork. In: 33rd AAAI conference on artificial intelligence, AAAI 2019, 31st innovative applica-\ntions of artificial intelligence conference, IAAI 2019 and the 9th AAAI symposium on educational \nadvances in artificial intelligence, EAAI 2019.  https:// doi. org/ 10. 1609/ aaai. v33i01. 33014 68\nAli F, Kwak D, Khan P, El-Sappagh S, Ali A, Ullah S, Kim KH, Kwak KS (2019) Transportation sentiment \nanalysis using word embedding and ontology-based topic modeling. Knowledge-Based Syst. https:// \ndoi. org/ 10. 1016/j. knosys. 2019. 02. 033\nAl-Jumeily D, Ghazali R, Hussain A (2014) Predicting physical time series using dynamic ridge polynomial \nneural networks. PLoS ONE 9(8):e105766. https:// doi. org/ 10. 1371/ journ al. pone. 01057 66\nAlpaydin E (2020) Introduction to machine learning. MIT Press, Cambridge\nArabasadi Z, Alizadehsani R, Roshanzamir M, Moosaei H, Yarifard AA (2017) Computer aided decision \nmaking for heart disease detection using hybrid neural network-genetic algorithm. Comput Methods \nPrograms Biomed. https:// doi. org/ 10. 1016/j. cmpb. 2017. 01. 004\nArora S, Ma T, Moitra A (2015) Simple, efficient, and neural algorithms for sparse coding. PMLR, pp \n113–149\nArulkumaran K, Deisenroth MP, Brundage M, Bharath AA (2017) A brief survey of deep reinforcement \nlearning. In: IEEE signal processing magazine, special issue on deep learning for image understand-\ning pp 1–16\nBa J, Hinton G, Mnih V, Leibo JZ, Ionescu C (2016) Using fast weights to attend to the recent past. Adv \nNeural Inf Process Syst 29:4338–4346\nBaecchi C, Uricchio T, Bertini M, Del Bimbo A (2016) A multimodal feature learning approach for sen-\ntiment analysis of social network multimedia. Multimed Tools Appl. https:// doi. org/ 10. 1007/ \ns11042- 015- 2646-x\n\n13605\nDeep learning modelling techniques: current progress,…\n1 3\nBai Y, Fu J, Zhao T, Mei T (2018) Deep attention neural tensor network for visual question answering. In: \nLecture notes in computer science (including subseries lecture notes in artificial intelligence and lec-\nture notes in bioinformatics). https:// doi. org/ 10. 1007/ 978-3- 030- 01258-8_2\nBarré P, Stöver BC, Müller KF, Steinhage V (2017) LeafNet: a computer vision system for automatic plant \nspecies identification. Ecol Inform. https:// doi. org/ 10. 1016/j. ecoinf. 2017. 05. 005\nBartunov S, Rae JW, Osindero S, Lillicrap TP (2019) Meta-learning deep energy-based memory models. \nhttps:// arXiv. org/ 1910. 02720\nBasiri ME, Nemati S, Abdar M, Cambria E, Acharya UR (2021) ABCDM: an attention-based bidirectional \nCNN-RNN deep model for sentiment analysis. Futur Gener Comput Syst 115:279–294. https:// doi. \norg/ 10. 1016/j. future. 2020. 08. 005\nBau D, Zhu JY, Strobelt H, Zhou B, Tenenbaum JB, Freeman WT, Torralba A (2019) GaN dissection: visu-\nalizing and understanding generative adversarial networks. In: 7th international conference on learn-\ning representations, ICLR 2019\nBengio Y, Simard P, Frasconi P (1994) Learning long-term dependencies with gradient descent is difficult. \nIEEE Trans Neural Netw 5(2):157–166. https:// doi. org/ 10. 1109/ 72. 279181\nBengio Y, Courville A, Vincent P (2013) Representation learning : a review and new perspectives. IEEE \nTrans Pattern Anal Mach Intell 35:1798–1828\nBen-Younes H, Cadene R, Cord M, Thome N (2017) MUTAN: multimodal tucker fusion for visual question \nanswering. In: Proceedings of the IEEE international conference on computer vision. https:// doi. org/ \n10. 1109/ ICCV. 2017. 285\nBiancofiore F, Busilacchio M, Verdecchia M, Tomassetti B, Aruffo E, Bianco S, Di Tommaso S, Colangeli \nC, Rosatelli G, Di Carlo P (2017) Recursive neural network model for analysis and forecast of PM10 \nand PM25. Atmos Pollut Res. https:// doi. org/ 10. 1016/j. apr. 2016. 12. 014\nBordes A, Weston J, Chopra S (2014) Question answering with subgraph embeddings. https:// arXiv. org/ \n1406. 3676\nBousmalis K, Trigeorgis G, Silberman N, Krishnan D, Erhan D (2016) Domain separation networks. In: \nAdvances in neural information processing systems\nBrahma S (2018) Improved sentence modeling using suffix bidirectional LSTM. https:// arXiv. org/ 1805. \n07340\nBrocardo ML, Traore I, Woungang I, Obaidat MS (2017) Authorship verification using deep belief network \nsystems. Int J Commun Syst 30:e3259. https:// doi. org/ 10. 1002/ dac. 3259\nBrock A, Donahue J, Simonyan K (2019) Large scale GaN training for high fidelity natural image synthesis. \nIn: 7th international conference on learning representations, ICLR 2019\nCamgoz NC, Hadfield S, Koller O, Ney H, Bowden R (2018) Neural sign language translation. In: Proceed-\nings of the IEEE computer society conference on computer vision and pattern recognition. https:// doi. \norg/ 10. 1109/ CVPR. 2018. 00812\nCao Z, Duan L, Yang G, Yue T, Chen Q (2019) An experimental study on breast lesion detection and clas-\nsification from ultrasound images using deep learning architectures. BMC Med Imaging 19:1–9\nCarrio A, Sampedro C, Rodriguez-ramos A, Campoy P (2017) A review of deep learning methods and \napplications for unmanned aerial vehicles. J Sensors 14:2017. https:// doi. org/ 10. 1155/ 2017/ 32968 74\nCase C, Casper J, Catanzaro B, Diamos G, Elsen E (2014) Deep speech: scaling up end-to-end speech rec-\nognition. https:// arXiv. org/ 1412. 5567\nChang S, Liu J (2020) Multi-lane capsule network for classifying images with complex background. IEEE \nAccess 8:79876–79886. https:// doi. org/ 10. 1109/ ACCESS. 2020. 29907 00\nChen X, Kundu K, Zhu Y, Ma H, Fidler S, Urtasun R (2018a) 3D object proposals using stereo imagery for \naccurate object class detection. IEEE Trans Pattern Anal Mach Intell 40(5):1259–1272. https:// doi. \norg/ 10. 1109/ TPAMI. 2017. 27066 85\nChen Y, Li W, Sakaridis C, Dai D, Van Gool L (2018b) Domain adaptive faster R-CNN for object detection \nin the wild. In: Proceedings of the IEEE computer society conference on computer vision and pattern \nRecognition. https:// doi. org/ 10. 1109/ CVPR. 2018. 00352\nChen YY, Lin YH, Kung CC, Chung MH, Yen I (2019) Design and implementation of cloud analytics-\nassisted smart power meters considering advanced artificial intelligence as edge analytics in demand-\nside managment for smart homes. Sensors 19:2047\nCheng J, Dong L, Lapata M (2016) Long short-term memory-networks for machine reading. EMNLP con-\nference on empirical methods in natural language processing, proceedings. https:// doi. org/ 10. 18653/ \nv1/ d16- 1053\nChicco D, Sadowski P, Baldi P, Milano P, Elettronica D (2014) Deep autoencoder neural networks for gene \nontology annotation predictions. In: 5th ACM conference on bioinformatics, computational biology, \nand health informatics - BCB’14. pp 533–540. https:// doi. org/ 10. 1145/ 26493 87. 26494 42\n\n13606 S. F. Ahmed et al.\n1 3\nChu Q, Ouyang W, Li H, Wang X, Liu B, Yu N (2017) Online multi-object tracking using CNN-based \nsingle object tracker with spatial-temporal attention mechanism. In: Proceedings of the IEEE interna-\ntional conference on computer vision. https:// doi. org/ 10. 1109/ ICCV. 2017. 518\nCireşan D, Meier U, Masci J, Schmidhuber J (2012) Multi-column deep neural network for traffic sign clas-\nsification. Neural Netw 32:333–338. https:// doi. org/ 10. 1016/j. neunet. 2012. 02. 023\nCollobert R, Weston J, Bottou L, Karlen M, Kavukcuoglu K, Kuksa P (2011) Natural language processing \n(Almost) from scratch. J Mach Learn Res 12:2493–2537\nCui J, Long J, Min E, Liu Q, Li Q (2018) Comparative study of CNN and RNN for deep learning based \nintrusion detection system. In: Lecture notes in computer science (including subseries lecture notes in \nartificial intelligence and lecture notes in bioinformatics). https:// doi. org/ 10. 1007/ 978-3- 030- 00018-\n9_ 15\nDa’u A, Salim N (2020) Recommendation system based on deep learning methods: a systematic review and \nnew directions. Artif Intell Rev 53:2709–2748. https:// doi. org/ 10. 1007/ s10462- 019- 09744-1\nDahl GE, Ranzato M, Mohamed AR, Hinton G (2010) Phone recognition with the mean-covariance \nrestricted Boltzmann machine. Adv Neural Inf Process Syst 23:469–477\nDe S, Maity A, Goel V, Shitole S, Bhattacharya A (2017) Predicting the popularity of instagram posts for \na lifestyle magazine using deep learning. In: 2017 2nd international conference on communication \nsystems, computing and IT applications (CSCITA) pp 174–177\nDemeester T, Sutskever I, Chen K, Dean J, Corado G (2016) Distributed representations of words and \nphrases and their compositionality. EMNLP 2016 - Conference empirical methods natural language \nprocess processing\nDeng L (2014) A tutorial survey of architectures, algorithms, and applications for deep learning. APSIPA \nTrans Signal Inf Process 3:e2. https:// doi. org/ 10. 1017/ atsip. 2013.9\nDeng L, Yu D (2014) Deep learning: methods and applications. Found Trends Signal Process 7:197–387\nDeng F, Pu S, Chen X, Shi Y, Yuan T, Shengyan P (2018) Hyperspectral image classification with capsule \nnetwork using limited training samples. Sensors 18(9):3153. https:// doi. org/ 10. 3390/ s1809 3153\nDeoras A, Povey D, Mikolov T, Burget L, Černocký J (2011) Strategies for training large scale neural net-\nwork language models. In: IEEE workshop on automatic speech recognition and understanding pp \n196–201\nDhyani M, Kumar R (2019) An intelligent Chatbot using deep learning with Bidirectional RNN and atten-\ntion model. Mater Today Proceedings. https:// doi. org/ 10. 1016/j. matpr. 2020. 05. 450\nDick S (2019) Artificial intelligence. Harvard Data Sci Rev 1:1–8. https:// doi. org/ 10. 1162/ 99608 f92. 92fe1 \n50c\nDing L, Fang W, Luo H, Love PED, Zhong B, Ouyang X (2018) A deep hybrid learning model to detect \nunsafe behavior: integrating convolution neural networks and long short-term memory. Autom Constr \n86:118–124\nDixit M, Tiwari A, Pathak H, Astya R (2018) An overview of deep learning architectures, libraries and its \napplications areas. In 2018 international conference on advances in computing, communication con-\ntrol and networking. pp 293–297. https:// doi. org/ 10. 1109/ ICACC CN. 2018. 87484 42\nDo Rosario VM, Borin E, Breternitz M (2019) The multi-lane capsule network. IEEE Signal Process Lett \n26:1006–1010. https:// doi. org/ 10. 1109/ LSP. 2019. 29156 61\ndo Rosario VM, Breternitz M, Borin E (2021) Efficiency and scalability of multi-lane capsule networks \n(MLCN). J. Parallel Distrib Comput. 155:63–73. https:// doi. org/ 10. 1016/J. JPDC. 2021. 04. 010\nDora S, Pennartz C, Bohte S (2018) A deep predictive coding network for inferring hierarchical causes \nunderlying sensory inputs, Lecture notes in computer science (including subseries lecture notes in \nartificial intelligence and lecture notes in bioinformatics). Springer. https:// doi. org/ 10. 1007/ 978-3- \n030- 01424-7_ 45\nDumoulin V, Perez E, Schucher N, Strub F, Vries H, Courville A, Bengio Y (2018) Feature-wise transfor -\nmations. Distill. https:// doi. org/ 10. 23915/ disti ll. 00011\nDumoulin V, Shlens J, Kudlur M (2017) A learned representation for artistic style. In: 5th international con-\nference on learning representations, ICLR 2017 - conference track proceedings\nElman JL (1990) Finding structure in time. Cogn Sci 14(2):179–211. https:// doi. org/ 10. 1016/ 0364- 0213(90) \n90002-E\nElman JL (1998) Generalization, simple recurrent networks, and the emergence of structure. In: Proceedings \n20th annual conference cognitive science society\nEslami SMA, Heess N, Williams CKI, Winn J (2014) The shape boltzmann machine: a strong model of \nobject shape. Int J Comput Vis 107:155–176. https:// doi. org/ 10. 1007/ s11263- 013- 0669-1\nFayek HM, Lech M, Cavedon L (2017) Evaluating deep learning architectures for speech emotion recogni-\ntion. Neural Netw 92:60–68. https:// doi. org/ 10. 1016/j. neunet. 2017. 02. 013\n\n13607\nDeep learning modelling techniques: current progress,…\n1 3\nFeng X, Zhang H, Ren Y, Shang P, Zhu Y, Liang Y (2019) The deep learning-based recommender system \n“pubmender” for choosing a biomedical publication venue: development and validation study. J Med \nInternet Res 21:e12957. https:// doi. org/ 10. 2196/ 12957\nFink O, Wang Q, Svensen M, Dersin P, Lee W-J, Ducoffe M (2020) Potential, challenges and future direc-\ntions for deep learning in prognostics and health management applications. Eng Appl Artif Intell \n92:103678\nGallicchio C Micheli A Pedrelli L (2018a) Deep echo state networks for diagnosis of Parkinson’s disease. \nIn: ESANN 2018a - Proceedings, European symposium on artificial neural networks, computational \nintelligence and machine learning\nGallicchio C, Micheli A, Pedrelli L (2018b) Design of deep echo state networks. Neural Netw 108:33–47. \nhttps:// doi. org/ 10. 1016/j. neunet. 2018. 08. 002\nGao Y, Gao F, Dong J, Li HC (2021) SAR image change detection based on multiscale capsule network. \nIEEE Geosci Remote Sens Lett. 18(3):484–488  https:// doi. org/ 10. 1109/ LGRS. 2020. 29778 38\nGehring J, Auli M, Grangier D, Yarats D, Dauphin YN (2017) Convolutional sequence to sequence learn-\ning. In: 34th International conference on machine learning, ICML 2017\nGevaert CM, Suomalainen J, Tang J, Kooistra L (2015) Generation of spectral-temporal response surfaces \nby combining multispectral satellite and hyperspectral UAV imagery for precision agriculture appli-\ncations. IEEE J Sel Top Appl Earth Obs Remote Sens. https:// doi. org/ 10. 1109/ JSTARS. 2015. 24063 \n39\nGheisari M, Wang G, Bhuiyan ZA (2017) A survey on deep learning in big data. In: 2017 IEEE Interna-\ntional conference on computational science and engineering (CSE) and IEEE international conference \non embedded and ubiquitous computing (EUC) 2:173–180\nGhiasi G, Lee H, Kudlur M, Dumoulin V, Shlens J (2017) Exploring the structure of a real-time, arbitrary \nneural artistic stylization network. In: British machine vision conference 2017, BMVC 2017. https:// \ndoi. org/ 10. 5244/c. 31. 114\nGhosh R, Ravi K, Ravi V (2016) A novel deep learning architecture for sentiment classification. In: 2016 \n3rd International conference on recent advances in information technology, RAIT 2016. https:// doi. \norg/ 10. 1109/ RAIT. 2016. 75079 53\nGodarzi AA, Amiri RM, Talaei A, Jamasb T (2014) Predicting oil price movements: a dynamic artificial \nneural network approach. Energy Policy 68: 371–382. https:// doi. org/ 10. 1016/j. enpol. 2013. 12. 049\nGoodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, Courville A, Bengio Y (2014) \nGenerative adversarial nets. Adv Neural Inform Process Syst. https:// doi. org/ 10. 3156/ jsoft. 29.5_ \n177_2\nGoodfellow I, Bengio Y, Courville A (2016) Deep learning. MIT Press, Cambridge\nGoodfellow IJ, Warde-Farley D, Mirza M, Courville A, Bengio Y (2013) Maxout networks. In: 30th Inter -\nnational conference on machine learning, ICML 2013.\nGovender M, Chetty K, Bulcock H (2007) A review of hyperspectral remote sensing and its application in \nvegetation and water resource studies. Water SA 33(2):145–151. https:// doi. org/ 10. 4314/ wsa. v33i2. \n49049\nGraves A, Schmidhuber J (2005) Framewise phoneme classification with bidirectional LSTM and other neu-\nral network architectures. Neural Netw 18(5–6):602–610. https:// doi. org/ 10. 1016/j. neunet. 2005. 06. \n042\nGraves A, Jaitly N, Mohamed AR (2013) Hybrid speech recognition with Deep Bidirectional LSTM. In: \n2013 IEEE workshop on automatic speech recognition and understanding, ASRU 2013 - proceedings \nhttps:// doi. org/ 10. 1109/ ASRU. 2013. 67077 42\nGünther F, Dudschig C, Kaup B (2016) Latent semantic analysis cosines as a cognitive similarity measure: \nevidence from priming studies. Q J Exp Psychol. https:// doi. org/ 10. 1080/ 17470 218. 2015. 10382 80\nGuo Y, Liu Y, Oerlemans A, Lao S, Wu S, Lew MS (2016) Deep learning for visual understanding: a \nreview. Neurocomputing 187:27–48. https:// doi. org/ 10. 1016/j. neucom. 2015. 09. 116\nGupta A, Anpalagan A, Guan L, Khwaja AS (2021) Deep learning for object detection and scene perception \nin self-driving cars: survey, challenges, and open issues. Array 100057\nPA Gutiérrez and C Hervás-Martínez (2011) Hybrid artificial neural networks: models, algorithms and data. \nIn: 11th international work-conference on artificial neural networks\nHaarnoja T, Tang, H, Abbeel P, Levine S (2017) Reinforcement learning with deep energy-based policies\nHamilton WL, Ying R, Leskovec J (2017) Representation learning on graphs: methods and applications. \nhttps:// arXiv. org/ 1709. 05584\nHan Y, Huang G, Song S, Yang L, Wang H,  Wang Y (2021) Dynamic Neural networks: a survey. IEEE \nTransactions on Pattern Analysis and Machine Intelligence 44(11):7436–7456\nHasan M, Choi J, Neumann J, Roy-Chowdhury AK, Davis LS (2016) Learning temporal regularity in video \nsequences\n\n13608 S. F. Ahmed et al.\n1 3\nHassan M, Bin Alam MS, Ahsan, T (2018) Emotion detection from text using skip-thought vectors. In: \n2018 International conference on innovations in science, engineering and technology, ICISET 2018. \nhttps:// doi. org/ 10. 1109/ ICISET. 2018. 87456 15\nHe J, Cheng X, He J, Xu H (2019) Cv-CapsNet: Complex-valued capsule network. IEEE Access 7:85492–\n85499. https:// doi. org/ 10. 1109/ ACCESS. 2019. 29245 48\nHe S, Wang S, Lan W, Fu H, Ji Q (2013) Facial expression recognition using deep boltzmann machine from \nthermal infrared images. In: Proceedings - 2013 humaine association conference on affective comput-\ning and intelligent interaction, ACII 2013. https:// doi. org/ 10. 1109/ ACII. 2013. 46\nHinton GE (2009) Deep belief networks. Scholarpedia. https:// doi. org/ 10. 4249/ schol arped ia. 5947\nHinton G, Deng L, Yu D, Dahl GE, Mohamed AR, Jaitly N, Senior A, Vanhoucke V, Nguyen P, Sainath TN, \nKingsbury B (2012) Deep neural networks for acoustic modeling in speech recognition: the shared \nviews of four research groups. IEEE Signal Process Mag 29(6):82–97\nHinton GE, Krizhevsky A, Wang SD (2011) Transforming auto-encoders. In: Lecture notes in computer sci-\nence (including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics). \nhttps:// doi. org/ 10. 1007/ 978-3- 642- 21735-7_6\nHong S, Yang D, Choi J, Lee H (2018) Inferring semantic layout for hierarchical text-to-image synthesis. \nIn: Proceedings of the IEEE computer society conference on computer vision and pattern recognition. \nhttps:// doi. org/ 10. 1109/ CVPR. 2018. 00833\nHu F, Xia GS, Hu J, Zhang L (2015) Transferring deep convolutional neural networks for the scene classifi-\ncation of high-resolution remote sensing imagery. Remote Sens 7(11):14680–14707. https:// doi. org/ \n10. 3390/ rs711 14680\nHu X, Chu L, Pei J, Liu W, Bian J (2021) Model complexity of deep learning: a survey. Knowl Inf Syst. \nhttps:// doi. org/ 10. 1007/ s10115- 021- 01605-0\nHu G, Hua Y, Yuan Y, Zhang Z, Lu Z, Mukherjee SS, Hospedales TM, Robertson NM, Yang Y (2017) \nAttribute-enhanced face recognition with neural tensor fusion networks. In: Proceedings of the IEEE \ninternational conference on computer vision. https:// doi. org/ 10. 1109/ ICCV. 2017. 404\nHuang X, Belongie S (2017) Arbitrary style transfer in real-time with adaptive instance normalization. In: \nProceedings of the IEEE international conference on computer vision. https:// doi. org/ 10. 1109/ ICCV. \n2017. 167\nHuang P, He X, Gao J, Deng L, Acero A, Heck L (2013) Learning deep structured semantic models for web \nsearch using clickthrough data. In: Proceedings of the 22nd ACM international conference on infor -\nmation and knowledge management pp 2333–2338\nHughes M, Li I, Kotoulas S, Suzumura T (2017) Medical text classification using convolutional neu-\nral networks. Studies in Health Technology and Informatics, pp 246–250. https:// doi. org/ 10. 3233/ \n978-1- 61499- 753-5- 246\nIrsoy O, Cardie C (2014) Deep recursive neural networks for compositionality in language. In: Advances in \nneural information processing systems\nIshihara T, Hayashi K, Manabe H, Shimbo M, Nagata M (2018) Neural tensor networks with diagonal slice \nmatrices. In: NAACL HLT 2018 - 2018 conference of the North American chapter of the association \nfor computational linguistics: human language technologies - proceedings of the conference. https:// \ndoi. org/ 10. 18653/ v1/ n18- 1047\nJaiswal A, AbdAlmageed W, Wu Y, Natarajan P (2019) CapsuleGAN: generative adversarial capsule net-\nwork. In: Lecture notes in computer science (including subseries lecture notes in artificial intelligence \nand lecture notes in bioinformatics). https:// doi. org/ 10. 1007/ 978-3- 030- 11015-4_ 38\nJayaraman S, Ramachandran M, Patan R, Daneshmand M, Gandomi AH (2022) Fuzzy deep neural learn-\ning based on goodman and Kruskal’s Gamma for Search Engine Optimization. IEEE Trans Big Data \n8(1), 268–277\nJenkins IR, Gee LO, Knauss A, Yin H, Schroeder J (2018) Accident scenario generation with recurrent \nneural networks. In: 2018 21st International conference on intelligent transportation systems (ITSC). \nIEEE, pp 3340–3345\nJiang X, Zhang Y, Liu W, Gao J, Liu J, Zhang Y, Lin J (2020) Hyperspectral image classification with Cap-\nsnet and Markov random fields. IEEE Access 8:191956–191968. https:// doi. org/ 10. 1109/ ACCESS. \n2020. 30291 74\nJordan MI, Mitchell TM (2015) Machine learning: trends, perspectives, and prospects 349\nKae A, Sohn K, Lee H, Learned-Miller E (2013) Augmenting CRFs with Boltzmann machine shape priors \nfor image labeling 2019–2026. https:// doi. org/ 10. 1109/ CVPR. 2013. 263\nKaiser Ł, Sutskever I (2016) Neural GPUs learn algorithms. In: 4th International conference on learning \nrepresentations, ICLR 2016 - conference track proceedings\n\n13609\nDeep learning modelling techniques: current progress,…\n1 3\nKarras T, Aila T, Laine S, Lehtinen J (2018) Progressive growing of GANs for improved quality, stability, \nand variation. In: 6th international conference on learning representations, ICLR 2018 - conference \ntrack proceedings\nKarras T, Laine S, Aila T (2019) A style-based generator architecture for generative adversarial networks. \nIn: Proceedings of the IEEE conference on computer vision and pattern recognition 2019 pp 4396–\n4405. https:// doi. org/ 10. 1109/ CVPR. 2019. 00453\nKashyap PK, Kumar S, Jaiswal A, Prasad M, Gandomi AH (2021) Towards precision agriculture: iot-ena-\nbled intelligent irrigation systems using deep learning neural network. IEEE Sens J 21(16):17479–\n17491. https:// doi. org/ 10. 1109/ JSEN. 2021. 30692 66\nKato N, Fadlullah ZM, Mao B, Tang F, Akashi O, Inoue T, Mizutani K (2016) The deep learning vision for \nheterogeneous network traffic control: proposal, challenges, and future perspective. IEEE Wirel Com-\nmun 24:146–153 https:// doi. org/ 10. 1109/ MWC. 2016. 16003 17WC\nKhamparia A, Singh MM (2019) A systematic review on deep learning architectures and applications. \nExpert Syst 36:e12400. https:// doi. org/ 10. 1111/ exsy. 12400\nKhan A, Sohail A, Zahoora U, Qureshi AS (2020) A survey of the recent architectures of deep convolutional \nneural networks. Artif Intell Rev 53:5455–5516. https:// doi. org/ 10. 1007/ s10462- 020- 09825-6\nKim JH, On KW, Lim W, Kim J, Ha JW, Zhang BT (2017) Hadamard product for low-rank bilinear pool-\ning. In: 5th international conference on learning representations, ICLR 2017 - conference track \nproceedings\nKim TS, Reiter A (2017) Interpretable 3D human action analysis with temporal convolutional networks. \nIEEE Computer society conference on computer vision and pattern recognition workshops. https:// \ndoi. org/ 10. 1109/ CVPRW. 2017. 207\nKiros R, Zhu Y, Salakhutdinov R, Zemel RS, Torralba A, Urtasun R, Fidler S (2015) Skip-thought vectors. \nAdvances in neural information processing systems\nKotsiopoulos T, Sarigiannidis P, Ioannidis D, Tzovaras D (2021) Machine learning and deep learning in \nsmart manufacturing: the smart grid paradigm. Comput Sci Rev 40:100341\nKraska T, Beutel A, Chi EH, Dean J, Polyzotis N (2018) The case for learned index structures. In: Proceed-\nings of the ACM SIGMOD international conference on management of data. Association for comput-\ning machinery, New York, pp 489–504. https:// doi. org/ 10. 1145/ 31837 13. 31969 09\nKrichene E, Masmoudi Y, Alimi AM, Abraham A, Chabchoub H (2017) Forecasting using elman recur -\nrent neural network.  Advances in Intelligent Systems and Computing, pp 488–497. https:// doi. org/ 10. \n1007/ 978-3- 319- 53480-0_ 48\nKrizhevsky A, Hinton GE (2017) ImageNet classification with deep convolutional neural networks. Com-\nmun ACM 60:84–90\nKumar A, Ramachandran M, Gandomi AH, Patan R, Lukasik S, Soundarapandian RK (2019) A deep neural \nnetwork based classifier for brain tumor diagnosis. Appl Soft Comput 82:105528\nLandgrebe D (2002) Hyperspectral image data analysis. IEEE Signal Process Mag 19(1):17–28. https:// doi. \norg/ 10. 1109/ 79. 974718\nLara-ben P, Carranza-garc M (2021) An experimental review on deep learning architectures for time series \nforecasting. Int J Neural Syst 31:2130001. https:// doi. org/ 10. 1142/ S0129 06572 13000 11\nLea C, Vidal R, Reiter A, Hager GD (2016) Temporal convolutional networks: a unified approach to action \nsegmentation. In: Lecture notes in computer science (including subseries lecture notes in artificial \nintelligence and lecture notes in bioinformatics). https:// doi. org/ 10. 1007/ 978-3- 319- 49409-8_7\nLeCun YA, Bottou L, Orr GB, Müller K-R (2012) Efficient backprop BT - neural networks: tricks of the \ntrade. In: Neural networks: tricks of the trade\nLecun Y, Bengio Y, Hinton G (2015) Deep learning. Nature 521:463–444. https:// doi. org/ 10. 1038/ natur \ne14539\nLedig C, Theis L, Huszár F, Caballero J, Cunningham A, Acosta A, Aitken A, Tejani A., Totz J, Wang Z, \nShi W (2017) Photo-realistic single image super-resolution using a generative adversarial network. In: \nProceedings - 30th IEEE conference on computer vision and pattern recognition, CVPR 2017. https:// \ndoi. org/ 10. 1109/ CVPR. 2017. 19\nLee R, Chen IY (2020) The time complexity analysis of neural network model configurations. In: Pro-\nceedings - 2nd international conference on mathematics and computers in science and engineering, \nMACISE 2020. https:// doi. org/ 10. 1109/ MACIS E49704. 2020. 00039\nLee H, Grosse R, Ranganath R, Ng AY (2009) Convolutional deep belief networks for scalable unsuper -\nvised learning of hierarchical representations. In: Proceedings of the 26th international conference on \nmachine learning, ICML 2009 https:// doi. org/ 10. 1145/ 15533 74. 15534 53\nLemley J, Bazrafkan S, Corcoran P (2017) Deep learning for consumer devices and services. IEEE Consum \nElectron Mag 6:48–56\n\n13610 S. F. Ahmed et al.\n1 3\nLeng B, Zhang X, Yao M, Xiong Z (2015) A 3D model recognition mechanism based on deep Boltzmann \nmachines. Neurocomputing 151:593–602. https:// doi. org/ 10. 1016/j. neucom. 2014. 06. 084\nLeung MKK, Xiong HY, Lee LJ, Frey BJ (2014) Deep learning of the tissue-regulated splicing code. Bioin-\nformatics 30:i121–i129. https:// doi. org/ 10. 1093/ bioin forma tics/ btu277\nLi Z, Huang H, Zhang Z, Shi G (2022) Manifold-based multi-deep belief network for feature extraction of \nhyperspectral image. Remote Sens 14:1484\nLi J, Xiong D, Tu Z, Zhu M, Zhang M, Zhou G (2017a) Modeling source syntax for neural machine transla-\ntion. In: ACL 2017a - 55th annual meeting of the association for computational linguistics, proceed-\nings of the conference (Long Papers). https:// doi. org/ 10. 18653/ v1/ P17- 1064\nLi Z, Yang Y, Liu X, Zhou F, Wen S, Xu W (2017b) Dynamic computational time for visual attention. \nIn: Proceedings - 2017 IEEE international conference on computer vision workshops, ICCVW 2017. \nhttps:// doi. org/ 10. 1109/ ICCVW. 2017. 145\nLi JB, Schmidt FR, Kolter JZ (2019a) Adversarial camera stickers: a physical camera-based attack on deep \nlearning systems. In: International conference on machine learning. pp 3896–3904\nLi P, Chen X, Shen S (2019b) Stereo R-CNN based 3D object detection for autonomous driving. In: Pro-\nceedings of the IEEE computer society conference on computer vision and pattern recognition. \nhttps:// doi. org/ 10. 1109/ CVPR. 2019. 00783\nLi Y, Qian M, Liu P, Cai Q, Li X, Guo J, Yan H, Yu F, Yuan K, Yu J, Qin L, Liu H, Wu W, Xiao P, Zhou Z \n(2019c) The recognition of rice images by UAV based on capsule network. Cluster Comput. https:// \ndoi. org/ 10. 1007/ s10586- 018- 2482-7\nLi Z, Cai X, Liu Y, Zhu B (2019d) A Novel Gaussian-Bernoulli based convolutional deep belief net-\nworks for image feature extraction. Neural Process Lett 49:305–319. https:// doi. org/ 10. 1007/ \ns11063- 017- 9751-y\nLiao S, Wang J, Yu R, Sato K, Cheng Z (2017) CNN for situations understanding based on sentiment analy -\nsis of twitter data. Procedia Comput Sci. https:// doi. org/ 10. 1016/j. procs. 2017. 06. 037\nLim S, Kang J (2018) Chemical-gene relation extraction using recursive neural network. Database.https:// \ndoi. org/ 10. 1093/ datab ase/ bay060\nLin Z, Feng M, Dos Santos CN, Yu M, Xiang B, Zhou B, Bengio Y (2017) A structured self-attentive sen-\ntence embedding. In: 5th international conference on learning representations, ICLR 2017 - confer -\nence track proceedings\nLin CY (2004) Rouge: a package for automatic evaluation of summaries. Proc work text summ branches out \n(WAS 2004)\nLiou CY, Cheng WC, Liou JW, Liou DR (2014) Autoencoder for words. Neurocomputing 139:84–96. \nhttps:// doi. org/ 10. 1016/j. neucom. 2013. 09. 055\nLitjens G, Kooi T, Bejnordi BE, Arindra A, Setio A, Ciompi F, Ghafoorian M, Laak JAWMV, Der G, Van \nB, Clara IS (2017) A survey on deep learning in medical image analysis. Med Image Anal 42:60–88. \nhttps:// doi. org/ 10. 1016/j. media. 2017. 07. 005\nLiu S, Wang Y, Yang X, Lei B, Liu L, Xiang S, Ni D, Wang T (2019) Deep learning in medical ultra-\nsound analysis: a review. Engineering 5:261–275. https:// doi. org/ 10. 1016/j. eng. 2018. 11. 020\nLiu K, Cheng J, Yi J (2022) Copper price forecasted by hybrid neural network with Bayesian optimi-\nzation and wavelet transform. Resour Policy 75:102520. https:// doi. org/ 10. 1016/j. resou rpol. 2021.  \n102520\nLiu P, Qiu X, Xuanjing H (2016) Recurrent neural network for text classification with multi-task learn-\ning. In: IJCAI international joint conference on artificial intelligence\nLiu W, Luo W, Lian D, Gao S (2017) Future frame prediction for anomaly detection—a new baseline. \nIn: Proceedings of the IEEE conference on computer vision and pattern recognition 6536–6545\nLu H, Li Y, Chen M, Kim H, Serikawa S (2018) Brain intelligence: go beyond artificial intelligence. \nMobile Netw Appl 23:368–375\nLukoševičius M, Jaeger H (2009) Reservoir computing approaches to recurrent neural network training. \nComput Sci Rev 3(3):127–49. https:// doi. org/ 10. 1016/j. cosrev. 2009. 03. 005\nMa J, Sheridan RP, Liaw A, Dahl GE, Svetnik V (2015) Deep neural nets as a method for quantitative struc-\nture—activity relationships. J Chem Inf Model 55:263–274. https:// doi. org/ 10. 1021/ ci500 747n\nMa J, Gao W, Wong KF (2018) Rumor detection on twitter with tree-structured recursive neural net-\nworks. In: ACL 2018 - 56th annual meeting of the association for computational linguistics, pro-\nceedings of the conference (Long Papers). https:// doi. org/ 10. 18653/ v1/ p18- 1184\nMajumder N, Poria S, Hazarika D, Mihalcea R, Gelbukh A, Cambria, E (2019) DialogueRNN: an atten-\ntive RNN for emotion detection in conversations. In: 33rd AAAI Conference on artificial intel-\nligence, AAAI 2019, 31st innovative applications of artificial intelligence conference, IAAI 2019 \nand the 9th AAAI symposium on educational advances in artificial intelligence, EAAI 2019https://  \ndoi. org/ 10. 1609/ aaai. v33i01. 33016 818\n\n13611\nDeep learning modelling techniques: current progress,…\n1 3\nMendis GJ, Randeny T, Wei, J, Madanayake A (2016) Deep learning based doppler radar for micro VAS \ndetection and classification Gihan J. Mendis. In: MILCOM 2016–2016 IEEE military communica-\ntions conference pp 924–929\nMesnil G, Dauphin Y, Yao K, Bengio Y, Deng L, Hakkani-tur D, He X (2015) Using recurrent neural \nnetworks for slot filling in spoken language understanding. IEEE/ACM trans audio, speech Lang \nProcess 23:530–539\nMicheli A, Sperduti A, Starita A (2007) An introduction to recursive neural networks and kernel meth-\nods for cheminformatics. Curr Pharm Des 13(14):1469–1496. https:// doi. org/ 10. 2174/ 13816 12077  \n80765 981\nMikolov T, Karafiát M, Burget L, Jan C, Khudanpur, S (2010) Recurrent neural network based language \nmodel. In: Proceedings of the 11th annual conference of the international speech communication \nassociation, INTERSPEECH 2010\nMikolov T, Kombrink S, Burget L, Černocký J, Khudanpur S (2011) Extensions of recurrent neural net-\nwork language model In: ICASSP, IEEE international conference on acoustics, speech and signal \nprocessing - proceedings. https:// doi. org/ 10. 1109/ ICASSP. 2011. 59476 11\nMikolov T, Chen K, Corrado G Dean J (2013) Efficient estimation of word representations in vector \nspace. In: 1st international conference on learning representations, ICLR 2013 - workshop track \nproceedings\nMiotto R, Wang F, Wang S, Jiang X, Dudley JT (2018) Deep learning for healthcare: review, opportuni-\nties and challenges. Brief Bioinform 19:1236–1246. https:// doi. org/ 10. 1093/ bib/ bbx044\nMisra D (2019) Mish: a self regularized non-monotonic neural activation function. https:// arXiv. org/  \n1908. 08681\nMitra B, Craswell N (2017) Neural text embeddings for information retrieval (WSDM 2017 tutorial) \nIn: WSDM 2017 - Proceedings of the 10th ACM international conference on web search and data \nmining https:// doi. org/ 10. 1145/ 30186 61. 30227 55\nMiyato T, Kataoka T, Koyama M, Yoshida Y (2018) Spectral normalization for generative adversarial \nnetworks. In: 6th international conference on learning representations, ICLR 2018 - conference \ntrack proceedings\nMobiny A, Van Nguyen H 2018 Fast CapsNet for lung cancer screening. In: Lecture notes in computer \nscience (including subseries lecture notes in artificial intelligence and lecture notes in bioinfor -\nmatics). https:// doi. org/ 10. 1007/ 978-3- 030- 00934-2_ 82\nMohd M, Jan R, Shah M (2020) Text document summarization using word embedding. Expert Syst \nAppl. https:// doi. org/ 10. 1016/j. eswa. 2019. 112958\nMousavi M, Gandomi AH (2021) Deep learning for structural health monitoring under environmental \nand operational variations. In: Nondestructive characterization and monitoring of advanced mate-\nrials, aerospace, civil infrastructure, and transportation XV. International society for optics and \nphotonics p 115920H\nMühlhoff R (2020) Human-aided artificial intelligence: or, how to run large computations in human brains? \nToward a media sociology of machine learning. New Media Soc 22:1868–1884. https:// doi. org/ 10. \n1177/ 14614 44819 885334\nMukherjee S, Zimmer A, Sun X, Ghuman P, Cheng I (2020) An unsupervised generative neural approach \nfor InSAR phase filtering and coherence estimation. IEEE Geosci Remote Sens Lett 18:1971–1975\nMurali S, Swapna TR (2019) An empirical evaluation of temporal convolutional network for offensive text \nclassification. Int J Innov Technol Explor Eng 8(8)\nNaylor CD (2018) On the prospects for a (deep) learning health care system. J Am Med Assoc \n320:1099–1100\nNg A (2015) What data scientists should know about deep learning. www. slide share. net/ Extra ctCon f44\nNgiam J, Chen Z, Wei Koh P, Ng AY (2011) Learning deep energy models. In: Proceedings of the 28th \ninternational conference on machine learning (ICML-11)  pp 1105–1112\nNguyen A, Yosinski J, Clune J (2015) Deep neural networks are easily fooled: high confidence predictions \nfor unrecognizable images. In: Proceedings of the IEEE conference on computer vision and pattern \nrecognition pp 427–436\nNguyen-Thanh VM, Zhuang X, Rabczuk T (2020) A deep energy method for finite deformation hyperelas-\nticity. Eur J Mech 80:103874. https:// doi. org/ 10. 1016/j. eurom echsol. 2019. 103874\nNiklaus S, Mai L, Liu F (2017) Video frame interpolation via adaptive separable convolution. In: Proceed-\nings of the IEEE international conference on computer vision. https:// doi. org/ 10. 1109/ ICCV. 2017. 37\nNorton AP, Qi Y (2017) Adversarial-playground: a visualization suite showing how adversarial examples \nfool deep learning. In: 2017 IEEE symposium on visualization for cyber security (VizSec) pp 1–14\nNwankpa CE, Ijomah W, Gachagan A, Marshall S (2018) Activation functions: comparison of trends in \npractice and research for deep learning.   https:// arXiv. org/ 1811. 03378\n\n13612 S. F. Ahmed et al.\n1 3\nOdena A, Olah C, Shlens J (2017) Conditional image synthesis with auxiliary classifier gans. In: 34th Inter -\nnational conference on machine learning, ICML 2017\nOka A, Ishimura N, Ishihara S (2021) A new dawn for the use of artificial intelligence in gastroenterology. \nHepatol Pancreatol Diagn 11:1719\nOord VD, Dieleman S, Schrauwen B (2013) Deep content-based music recommendation. Neural Inform \nProcess Syst 26:1–9\nOrkphol K, Yang W (2019) Word sense disambiguation using cosine similarity collaborates with Word2vec \nand WordNet. Futur Internet 11:114\nOrtiz A, Munilla J, Gorriz JM, Ramirez J (2016) Ensembles of deep learning architectures for the early \ndiagnosis of the Alzheimer’s disease. Int J Neural Syst 26:1–23. https:// doi. org/ 10. 1142/ S0129 06571 \n65002 58\nPalanichamy K (2019) Integrative omic analysis of neuroblastoma. Computational epigenetics and diseases. \nElsevier, Amsterdam, pp 311–326\nPandey K, Shekhawat HS, Prasanna, SRM (2019) Deep learning techniques for speech emotion recogni-\ntion : a review. 2019 29th international conference radioelektronika pp 1–6\nPapernot N, Mcdaniel P, Jha S, Fredrikson M, Celik ZB, Swami A (2016) The limitations of deep learning \nin adversarial settings. In: 2016 IEEE European symposium on security and privacy (EuroS and P) pp \n372–387\nPapineni K, Roukos S, Ward T, Zhu W-J (2001) BLEU: a method for automatic evaluation of machine trans-\nlation. Assoc Comput Linguist. https:// doi. org/ 10. 3115/ 10730 83107 3135\nParikh AP, Täckström O, Das, D, Uszkoreit J (2016) A decomposable attention model for natural language \ninference. In: EMNLP 2016 - conference on empirical methods in natural language processing, pro-\nceedings. https:// doi. org/ 10. 18653/ v1/ d16- 1244\nPark T, Liu MY, Wang TC, Zhu JY (2019) Semantic image synthesis with spatially-adaptive normalization. \nIn: Proceedings of the IEEE computer society conference on computer vision and pattern recognition. \nhttps:// doi. org/ 10. 1109/ CVPR. 2019. 00244\nPark DC (2010) A time series data prediction scheme using bilinear recurrent neural network. In: 2010 \nInternational conference on information science and applications, ICISA 2010. https:// doi. org/ 10. \n1109/ ICISA. 2010. 54803 83\nParkhi OM, Vedaldi A, Zisserman A (2015) Deep face recognition. British machine vision association\nPashaei M, Kamangir H (2020) Review and evaluation of deep learning architectures for efficient land cover \nmapping with uas hyper-spatial imagery: a case study over a wetland. Remote Sens 12:959. https:// \ndoi. org/ 10. 3390/ rs120 60959\nPaula EL, Ladeira M, Carvalho RN, Marzag T (2016) Deep learning anomaly detection as support fraud \ninvestigation in Brazilian exports and anti-money laundering. In: 2016 15th IEEE International con-\nference on machine learning and applications (ICMLA) pp 954–960. https:// doi. org/ 10. 1109/ ICMLA. \n2016. 73\nPaulus R, Xiong C, Socher R (2018) A deep reinforced model for abstractive summarization. In: 6th inter -\nnational conference on learning representations, ICLR 2018 - conference track proceedings\nPerozzi B, Al-Rfou R, Skiena S (2014) DeepWalk: online learning of social representations. In: Proceedings \nof the ACM SIGKDD international conference on knowledge discovery and data mining. https:// doi. \norg/ 10. 1145/ 26233 30. 26237 32\nPerraudin N, Defferrard M, Kacprzak T, Sgier R (2019) DeepSphere: efficient spherical convolutional \nneural network with HEALPix sampling for cosmological applications. Astron Comput 27:130–46. \nhttps:// doi. org/ 10. 1016/j. ascom. 2019. 03. 004\nPfau D (2017) Unrolled GAN 1–25\nPoliak A, Belinkov Y, Glass J, Van Durme B (2018) On the evaluation of semantic phenomena in neural \nmachine translation using natural language inference. In: NAACL HLT 2018 - 2018 conference of the \nNorth American chapter of the association for computational linguistics: human language technolo-\ngies - proceedings of the conference. https:// doi. org/ 10. 18653/ v1/ n18- 2082\nPopperli M, Gulagundi R, Yogamani S, Milz S (2019) Capsule neural network based height classification \nusing low-cost automotive ultrasonic sensors. In: IEEE intelligent vehicles symposium, proceedings. \nhttps:// doi. org/ 10. 1109/ IVS. 2019. 88138 79\nPouyanfar S, Saad S., Yilin Y, Haiman T, Tao Y, Reyes MP, Shyu M, Chen S-C, Iyengar SS (2018) A sur -\nvey on deep learning: algorithms, techniques, and applications. ACM Comput Surv 51(5):1–36\nQasim Abualigah LM, Hanandeh ES (2015) Applying genetic algorithms to information retrieval using vec-\ntor space model. Int J Comput Sci Eng Appl. https:// doi. org/ 10. 5121/ ijcsea. 2015. 5102\nQiu X, Huang X (2015) Convolutional neural tensor network architecture for community-based question \nanswering. In: IJCAI International joint conference on artificial intelligence\n\n13613\nDeep learning modelling techniques: current progress,…\n1 3\nRao G, Huang W, Feng Z, Cong Q (2018a) LSTM with sentence representations for document-level senti-\nment classification. Neurocomputing 308:49–57. https:// doi. org/ 10. 1016/j. neucom. 2018. 04. 045\nRao K, Sak H, Prabhavalkar R (2018b) Exploring architectures, data and units for streaming end-to-end \nspeech recognition with RNN-transducer. In: 2017 IEEE automatic speech recognition and under -\nstanding workshop, ASRU 2017 - proceedings. https:// doi. org/ 10. 1109/ ASRU. 2017. 82689 35\nRavì D, Wong C, Deligianni F, Berthelot M, Andreu-perez J, Lo B (2017) Deep learning for health infor -\nmatics. IEEE J Biomed Heal Inform 21:4–21\nRengasamy D, Figueredo GP, Advanced T, Analysis D (2018) Deep learning approaches to aircraft mainte-\nnance, repair and overhaul: a review. In: 2018 21st International conference on intelligent transporta-\ntion systems (ITSC) pp 150–153\nRoberto J, Solares A, Elisa F, Raimondi D, Zhu Y, Rahimian F, Canoy D, Tran J, Catarina A, Gomes P, Pay -\nberah AH, Zottoli M, Nazarzadeh M, Conrad N (2020) Deep learning for electronic health records: a \ncomparative review of multiple deep neural architectures. J Biomed Inform 101:103337. https:// doi. \norg/ 10. 1016/j. jbi. 2019. 103337\nRussakovsky O, Deng J, Su H, Krause J, Satheesh S, Ma S, Huang Z, Karpathy A, Khosla A, Bernstein M, \nBerg AC, Fei-Fei L (2015) ImageNet large scale visual recognition challenge. Int J Comput Vis 115: \n211–252. https:// doi. org/ 10. 1007/ s11263- 015- 0816-y\nSabour S, Frosst N, Hinton GE (2017) Dynamic routing between capsules. Adv Neural Inform Processing \nSyst. https:// doi. org/ 10. 48550/ arXiv. 1710. 09829\nSahoo BB, Jha R, Singh A, Kumar D (2019) Long short-term memory (LSTM) recurrent neural network \nfor low-flow hydrological time series forecasting. Acta Geophys 67(5):1471–1481. https:// doi. org/ 10. \n1007/ s11600- 019- 00330-1\nSainath TN, Mohamed A, Kingsbury, B, Ramabhadran B, Watson IBMTJ, Heights Y (2013) Deep con-\nvolutional neural networks for LVCSR. In: Proceedings  acoustics, speech and signal processing pp \n8614–8618\nSamaniego E, Anitescu C, Goswami S, Nguyen-Thanh VM, Guo H, Hamdia K, Zhuang X, Rabczuk T \n(2020) An energy approach to the solution of partial differential equations in computational mechan-\nics via machine learning: concepts, implementation and applications. Comput Methods Appl Mech \nEng 362:112790\nSaremi S, Mehrjou A, Schölkopf B, Hyvärinen A (2018) Deep energy estimator networks. https:// arXiv. \n1805. 08306\nScellier B, Bengio Y (2017) Equilibrium propagation: bridging the gap between energy-based models and \nbackpropagation. Front Comput Neurosci 11:24. https:// doi. org/ 10. 3389/ fncom. 2017. 00024\nSchmidhuber J (2015) Deep learning in neural networks: an overview. Neural Netw 61:85–117. https:// doi. \norg/ 10. 1016/j. neunet. 2014. 09. 003\nSchmidt U (2014) Shrinkage fields for effective image restoration. In: Proceedings of the IEEE conference \non computer vision and pattern recognition https:// doi. org/ 10. 1109/ CVPR. 2014. 349\nSengupta S, Basak S, Saikia P, Paul S, Tsalavoutis V, Atiah FD, Ravi V, Alan R, Ii P (2020) A review \nof deep learning with special emphasis on architectures applications and recent trends. Knowledge-\nBased Syst 194:105596\nSeo S, Kim C, Kim H, Mo K, Kang P (2020) Comparative study of deep learning-based sentiment classifi-\ncation. IEEE Access 8:6861–6875. https:// doi. org/ 10. 1109/ ACCESS. 2019. 29634 26\nShakya SR, Zhang C, Zhou Z (2018) Comparative study of machine learning and deep learning architecture \nfor human activity recognition using accelerometer data. Int J Mach Learn Comput  8(6):577–582. \nhttps:// doi. org/ 10. 18178/ ijmlc. 2018.8. 6. 748\nShen Y, He X, Gao J, Deng L, Mesnil G (2014) A latent semantic model with convolutional-pooling struc-\nture for information retrieval. In: Proceedings of the 23rd ACM international conference on confer -\nence on information and knowledge management. pp 101–110\nShi T, Kang K, Choo J, Reddy CK (2018) Short-text topic modeling via non-negative matrix factoriza-\ntion enriched with local word-context correlations. In: The web conference 2018 - proceedings of the \nworld wide web conference, WWW 2018. https:// doi. org/ 10. 1145/ 31788 76. 31860 09\nShoeibi A, Ghassemi N, Khodatars M, Jafari M, Hussain S, Alizadehsani R (2020) Application of deep \nlearning techniques for automated detection of epileptic seizures: a Review.  https:// arXiv. org/ 2007. \n01276\nShrestha A (2019) Review of deep learning algorithms and architectures. IEEE Access 7:53040–53065. \nhttps:// doi. org/ 10. 1109/ ACCESS. 2019. 29122 00\nSi Y, Wang J, Xu H, Roberts K (2019) Enhancing clinical concept extraction with contextual embeddings. J \nAm Med Informatics Assoc. https:// doi. org/ 10. 1093/ jamia/ ocz096\n\n13614 S. F. Ahmed et al.\n1 3\nSiami-Namini S, Tavakoli N, Namin AS (2019) The performance of LSTM and BiLSTM in forecasting time \nseries. In: Proceedings - 2019 IEEE International conference on big data, big data. https:// doi. org/ 10. \n1109/ BigDa ta470 90. 2019. 90059 97\nSiegelmann HT (1995) Computation beyond the turing limit. Science 80:268. https:// doi. org/ 10. 1126/ scien \nce. 268. 5210. 545\nSignorelli CM (2018) Can computers become conscious and overcome humans? Front Robot AI 5:121\nSocher R, Chen D, Manning CD, Ng AY (2013) Reasoning with neural tensor networks for knowledge base \ncompletion. Adv Neural Inf Proc Syst 1:e2\nSønderby CK, Caballero J, Theis L, Shi W, Huszár F (2017) Amortised map inference for image super-\nresolution. In: 5th international conference on learning representations, ICLR 2017 - conference track \nproceedings\nSrivastava N, Salakhutdinov R (2014) Multimodal learning with deep Boltzmann machines. J Mach Learn \nRes 15\nSugiyama S (2019) Human behavior and another kind in consciousness: emerging research and opportuni-\nties. IGI Global, Hershey\nSui J, Liu M, Lee J, Zhang J, Calhoun V (2020) Deep learning methods and applications in neuroimaging. J \nNeurosci Methods 339:108718. https:// doi. org/ 10. 1016/j. jneum eth. 2020. 108718\nSun P, Hui C, Bai N, Yang S, Wan L, Zhang Q, Zhao Y (2015) Revealing the characteristics of a novel \nbioflocculant and its flocculation performance in Microcystis aeruginosa removal. Sci Rep 5:17465. \nhttps:// doi. org/ 10. 1038/ srep1 7465\nSun X, Nasrabadi NM, Tran TD (2017) Supervised deep sparse coding networks.  https:// arXiv. org/ 1701. \n08349\nSun B, Feng J, Saenko K (2016) Return of frustratingly easy domain adaptation. In: 30th AAAI conference \non artificial intelligence, AAAI 2016\nSutskever I, Hinton G, Taylor G (2009) The recurrent temporal restricted boltzmann machine. In: Advances \nin neural information processing systems 21 - proceedings of the 2008 conference\nSutskever I, Vinyals O, Le QV (2014) Sequence to sequence learning with neural networks. In: Advances in \nneural information processing systems\nSutskever I, Hinton G (2007) Learning multilevel distributed representations for high-dimensional \nsequences. J Machine Learn Res. 2:548–555\nSzegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan D, Vanhoucke V, Rabinovich A (2015) \nGoing deeper with convolutions. In: Proceedings of the IEEE conference on computer vision and pat-\ntern recognition. pp 1–9\nTaherkhani A, Cosma G, McGinnity TM (2018) Deep-FS: a feature selection algorithm for deep Boltz-\nmann machines. Neurocomputing 322:22–37. https:// doi. org/ 10. 1016/j. neucom. 2018. 09. 040\nTahmassebi A, Gandomi AH, Fong S, Meyer-Baese A, Foo SY (2018a) Multi-stage optimization of a \ndeep model: a case study on ground motion modeling. PLoS ONE 13:e0203829\nTahmassebi A, Gandomi AH, McCann I, Schulte MHJ, Goudriaan AE, Meyer-Baese A (2018b) Deep \nlearning in medical imaging: Fmri big data analysis via convolutional neural networks. In: Pro-\nceedings of the practice and experience on advanced research computing. pp 1–4\nTahmassebi A, Ehtemami A, Mohebali B, Gandomi AH, Pinker K, Meyer-Baese A (2019) Big data ana-\nlytics in medical imaging using deep learning. In: Big data: learning, analytics, and applications. \ninternational society for optics and photonics, p 109890E\nTahmassebi A, Martin J, Meyer-Baese A, Gandomi AH (2020) An interpretable deep learning frame-\nwork for health monitoring systems: a case study of eye state detection using EEG Signals. In: \n2020 IEEE symposium series on computational intelligence (SSCI). IEEE pp 211–218\nTaigman Y, Polyak A, Wolf L (2017) Unsupervised cross-domain image generation. In: 5th international \nconference on learning representations, ICLR 2017 - conference track proceedings\nTandiya N, Jauhar A, Marojevic V, Reed JH (2018) Deep predictive coding neural network for rf anom-\naly detection in wireless networks. arXiv:2018.8403654. https:// doi. org/ 10. 1109/ ICCW. 2018.  \n84036 54\nTang Y (2013) Deep learning using linear support vector machines. https:// arXiv. org/ 1306. 0239\nTang Z, Yang J, Pei Z, Song X, Ge B (2019) Multi-process training gan for identity-preserving face syn-\nthesis. IEEE Access 7:97641–97652. https:// doi. org/ 10. 1109/ ACCESS. 2019. 29302 03\nTavarone Raffaele,   Badino L (2018) Conditional-computation-based recurrent neural networks for \ncomputationally efficient acoustic modelling. Interspeech, pp 1274–1278\nTelikani A, Gandomi AH, Choo K-KR, Shen J (2021) A cost-sensitive deep learning based approach \nfor network traffic classification. IEEE Trans Netw Serv Manag 19(1):661–670. https:// doi. org/ 10.  \n1109/ TNSM. 2021. 31122 83\n\n13615\nDeep learning modelling techniques: current progress,…\n1 3\nTkachenko Y (2015) Autonomous CRM control via CLV approximation with deep reinforcement learn-\ning in discrete and continuous action space.  arXiv:1504.01840. https:// arXiv. org/ 1504. 01840\nTompson J, Jain A, Lecun Y, Bregler C (2014) Joint training of a convolutional network and a graphical \nmodel for human pose estimation. 27:1–9 https:// arXiv. org/ 1406. 2984\nTrabelsi C, Bilaniuk O, Zhang Y, Serdyuk D, Subramanian S, Santos JF, Mehri S, Rostamzadeh N, \nBengio, Y, Pal CJ (2018) Deep complex networks. In: 6th international conference on learning \nrepresentations, ICLR 2018 - conference track proceedings\nTran SN, Garcez ADA, Weyde T, Yin J, Zhang Q, Karunanithi M (2020) Sequence classifica-\ntion restricted boltzmann machines with gated units. IEEE Trans Neural Networks Learn Syst \n31:4806–4815. https:// doi. org/ 10. 1109/ TNNLS. 2019. 29581 03\nTzafestas SG (2014) Mobile robot control IV: fuzzy and neural methods. In: Tzafestas SG (ed) Introduc-\ntion to mobile robot control. Elsevier, Oxford, pp 269–317\nUddin MZ, Hassan MM, Alsanad A, Savaglio C (2020) A body sensor data fusion and deep recurrent \nneural network-based behavior recognition approach for robust healthcare. Inf Fusion 55:105–115. \nhttps:// doi. org/ 10. 1016/j. inffus. 2019. 08. 004\nVan Gysel C, De Rijke M, Kanoulas E (2018) Neural vector spaces for unsupervised information \nretrieval. ACM Trans Inf Syst 36(4):1–25. https:// doi. org/ 10. 1145/ 31968 26\nVargas R, Mosavi A, Ruiz R (2017) Deep learning: a review. Adv Intell Syst Comput\nVaswani A (2017) Attention is all you need . Adv Neural Inf Process Syst 2017 pp 5999–6009 arXiv:  \n1706. 03762 v5\nVazhayil A, Vinayakumar R, Soman K (2018) Comparative study of the detection of malicious URLs using \nshallow and deep networks. In: 2018 9th international conference on computing, communication and \nnetworking technologies, ICCCNT 2018. https:// doi. org/ 10. 1109/ ICCCNT. 2018. 84941 59\nVincent P (2011) A connection between scorematching and denoising autoencoders. Neural Comput \n23:1661–1674. https:// doi. org/ 10. 1162/ NECO_a_ 00142\nWang J Yu LC, Lai KR, Zhang X (2016a) Dimensional sentiment analysis using a regional CNN-LSTM \nmodel. In: 54th Annual meeting of the association for computational linguistics, ACL 2016 - Short \nPapers. https:// doi. org/ 10. 18653/ v1/ p16- 2037\nWang J, Wang J, Fang W, Niu H (2016b) Financial time series prediction using elman recurrent random \nneural networks. Comput Intell Neurosci. https:// doi. org/ 10. 1155/ 2016/ 47425 15\nWang X, Jiang, W, Luo Z (2016c) Combination of convolutional and recurrent neural network for sentiment \nanalysis of short texts. In: COLING 2016 - 26th international conference on computational linguis-\ntics, proceedings of COLING 2016: technical papers\nWang D, Liang Y, Xu D (2019) Capsule network for protein post-translational modification site predic-\ntion. Bioinformatics 35(14):2386–2394. https:// doi. org/ 10. 1093/ bioin forma tics/ bty977\nWei Q, Kasabov N, Polycarpou M, Zeng Z (2019) Deep learning neural networks: methods, systems, and \napplications. Neurocomputing 396:130–132. https:// doi. org/ 10. 1016/j. neucom. 2019. 03. 073\nWieslander H, Forslid G, Bengtsson E, Wahlby C, Hirsch J-M, Stark CR, Sadanandan SK (2017) Deep con-\nvolutional neural networks for detecting cellular changes due to malignancy. In: Proceedings of the \nIEEE international conference on computer vision workshops pp 82–89\nWu Y, Guo Y (2020) Dual adversarial co-learning for multi-domain text classification. In: AAAI 2020 - \n34th AAAI Conference artificial intelligence, pp 6438–6445. https:// doi. org/ 10. 1609/ aaai. v34i04. \n6115\nWu H, Soraghan J, Lowit A, Di Caterina G (2018) A deep learning method for pathological voice detection using \nconvolutional deep belief network. In: Proceedings of the annual conference of the international speech \ncommunication association, INTERSPEECH. https:// doi. org/ 10. 21437/ Inter speech. 2018- 1351\nXiang C, Zhang L, Tang Y, Zou W, Xu C (2018) MS-capsnet: a novel multi-scale capsule network. IEEE \nSignal Process Lett 25:1850–1854. https:// doi. org/ 10. 1109/ LSP. 2018. 28738 92\nXiao C, Choi E, Sun J (2018) Review Opportunities and challenges in developing deep learning models \nusing electronic health records data: a systematic review. J Am Med Informatics Assoc 25:1419–\n1428. https:// doi. org/ 10. 1093/ jamia/ ocy068\nXiong HY, Alipanahi B, Lee LJ, Bretschneider H, Yuen RKC, Hua Y, Gueroussov S, Hamed S, Hughes TR, \nMorris Q, Barash Y, Adrian R, Jojic N, Scherer SW, Blencowe BJ (2015) The human splicing code \nreveals new insights into the genetic determinants of disease. Science 347(6218):1254806. https:// doi. \norg/ 10. 1126/ scien ce. 12548 06\nXu M (2020) Understanding graph embedding methods and their applications. SIAM Rev 63(4):825–853. \nhttps:// doi. org/ 10. 1137/ 20M13 86062\nXu T, Zhang P, Huang Q, Zhang H, Gan Z, Huang X, He X (2018) AttnGAN: fine-grained text to image genera-\ntion with attentional generative adversarial networks. In: Proceedings of the IEEE computer society confer -\nence on computer vision and pattern recognition. https:// doi. org/ 10. 1109/ CVPR. 2018. 00143\n\n13616 S. F. Ahmed et al.\n1 3\nYan WY, Shaker A, El-Ashmawy N (2015) Urban land cover classification using airborne LiDAR data: a \nreview. Remote Sens Environ. 158:295–310. https:// doi. org/ 10. 1016/j. rse. 2014. 11. 001\nYan Y, Guo Y (2020) Multi-level generative models for partial label learning with non-random label noise. \nhttps:// doi. org/ 10. 24963/ ijcai. 2021/ 449\nYang Z, Yu W, Liang P, Guo H, Xia L, Zhang F, Ma Y, Ma J (2019) Deep transfer learning for military \nobject recognition under small training set condition. Neural Comput Appl 31:6469–6478. https:// doi. \norg/ 10. 1007/ s00521- 018- 3468-3\nYang B, Yih W tau, He X, Gao J, Deng L (2015) Embedding entities and relations for learning and infer -\nence in knowledge bases. In: 3rd international conference on learning representations, ICLR 2015 \n- conference track proceedings\nYang D, Qu B, Yang J, Cudre-Mauroux P (2019) Revisiting user mobility and social relationships in \nLBSNs: a hypergraph embedding approach. In: The web conference 2019 - proceedings of the world \nwide web conference, WWW 2019. https:// doi. org/ 10. 1145/ 33085 58. 33136 35\nYao T, Pan Y, Li Y, Mei T (2017) Incorporating copying mechanism in image captioning for learning novel \nobjects. In: Proceedings - 30th IEEE conference on computer vision and pattern recognition, CVPR \n2017. https:// doi. org/ 10. 1109/ CVPR. 2017. 559\nYe M, Peng X, Gan W, Wu W, Qiao Y (2019) AnoPCN: video anomaly detection via deep predictive coding \nnetwork. In:  MM 2019 - Proceedings 27th ACM international conference multimedia 1805–1813. \nhttps:// doi. org/ 10. 1145/ 33430 31. 33508 99\nYu Y, Si X, Hu C, Zhang J (2019) A review of recurrent neural networks: LSTM cells and network architec-\ntures. Neural Comput 31(7):1235–1270. https:// doi. org/ 10. 1162/ neco_a_ 01199\nZeiler MD, Fergus R (2014) Visualizing and understanding convolutional networks. European conference \non computer vision. Springer, Cham, pp 818–833\nZeng Z, Xiao S, Jia K, Chan TH, Gao S, Xu D, Ma Y (2013) Learning by associating ambiguously labeled \nimages. In: Proceedings of the IEEE computer society conference on computer vision and pattern \nrecognition. https:// doi. org/ 10. 1109/ CVPR. 2013. 97\nZeroual A, Harrou F, Dairi A, Sun Y (2020) Deep learning methods for forecasting COVID-19 time-series data: a \ncomparative study. Chaos, Solitons Fractals 140:110121. https:// doi. org/ 10. 1016/j. chaos. 2020. 110121\nZhang D, Xu H, Su Z, Xu Y (2015) Chinese comments sentiment classification based on word2vec and \nSVMperf. Expert Syst Appl 42(4):1857–1863. https:// doi. org/ 10. 1016/j. eswa. 2014. 09. 011\nZhang C, Bengio S, Hardt M, Recht B, Vinyals O (2016a) Understanding deep learning requires rethinking \ngeneralization. Commun ACM 64:107–115. https:// doi. org/ 10. 1145/ 34467 76\nZhang L, Lin L, Liang X, He K (2016b) Is faster R-CNN doing well for pedestrian detection?. In: Lecture \nnotes in computer science (including subseries lecture notes in artificial intelligence and lecture notes \nin bioinformatics). https:// doi. org/ 10. 1007/ 978-3- 319- 46475-6_ 28\nZhang B, Xiong D, Su J, Duan H (2017a) A context-aware recurrent encoder for neural machine translation. IEEE/\nACM Trans Audio Speech Lang Process 25(12):2424–2432. https:// doi. org/ 10. 1109/ TASLP. 2017. 27514 20\nZhang H, Xu T, Li H, Zhang S, Wang X, Huang X, Metaxas D (2017b) StackGAN: text to photo-realistic \nimage synthesis with stacked generative adversarial networks. In: Proceedings of the IEEE interna-\ntional conference on computer vision. https:// doi. org/ 10. 1109/ ICCV. 2017. 629\nZhang S, Wang J, Tao X, Gong Y, Zheng N (2017c) Constructing deep sparse coding network for image \nclassification. Pattern Recognit 64:130–140. https:// doi. org/ 10. 1016/j. patcog. 2016. 10. 032\nZhang S, Yao L, Sun A, Tay Y (2019) Deep learning based recommender system: a survey and new per -\nspectives. ACM Comput Surv 52(1):1–38. https:// doi. org/ 10. 1145/ 32850 29\nZhang J, Lei YK, Zhang Z, Chang J, Li M, Han X, Yang L, Yang YI, Gao YQ (2020) A perspective on deep \nlearning for molecular modeling and simulations. J Phys Chem A 124(34):6745–6763. https:// doi. org/ \n10. 1021/ acs. jpca. 0c044 73\nZhao Y, Liu Z, Sun M (2015) Phrase type sensitive tensor indexing model for semantic composition. In: \nProceedings of the national conference on artificial intelligence\nZhao Z, Jiao L, Zhao J, Gu J, Zhao J (2017) Discriminant deep belief network for high-resolution SAR \nimage classification. Pattern Recognit 61:686–701. https:// doi. org/ 10. 1016/j. patcog. 2016. 05. 028\nZhao H, Chen Z, Jiang H, Jing W, Sun L, Feng M (2019) Evaluation of three deep learning models for early \ncrop classification using Sentinel-1A imagery time series-a case study in Zhanjiang. China Remote \nSens 11(22):2673. https:// doi. org/ 10. 3390/ rs112 22673\nZhong Z, Li J, Luo Z, Chapman M (2018) Spectral-spatial residual network for hyperspectral image classifi-\ncation: a 3-D deep learning framework. IEEE Trans Geosci Remote Sens 56(2):847–858. https:// doi. \norg/ 10. 1109/ TGRS. 2017. 27555 42\nZhou G, Xie Z, He T, Zhao J, Hu XT (2016) Learning the multilingual translation representations for ques-\ntion retrieval in community question answering via non-negative matrix factorization. IEEE/ACM \nTrans Audio Speech Lang Process 5:5–6. https:// doi. org/ 10. 1109/ TASLP. 2016. 25446 61\n\n13617\nDeep learning modelling techniques: current progress,…\n1 3\nZhu S, Mumford D (2006) A stochastic grammar of images a stochastic grammar of images. Found Trends \nComput Graph Vis 2(4):2. https:// doi. org/ 10. 1561/ 06000 00018\nZhu Z, Peng G, Chen Y, Gao H (2019) A convolutional neural network based on a capsule network with \nstrong generalization for bearing fault diagnosis. Neurocomputing 323:62–75. https:// doi. org/ 10. \n1016/j. neucom. 2018. 09. 050\nZiebart BD, Fox D (2010) Modeling purposeful adaptive behavior with the principle of maximum causal \nentropy. Carnegie Mellon University\nZulqarnain M, Ghazali R, Mazwin Y, Hassim M, Rehan M (2020) A comparative review on deep learning \nmodels for text classification. Indones J Electr Eng Comput Sci 19:325–335. https:// doi. org/ 10. 11591/ \nijeecs. v19. i1. pp325- 335\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nAuthors and Affiliations\nShams Forruque Ahmed1 · Md. Sakib Bin Alam2 · Maruf Hassan1 · \nMahtabin Rodela Rozbu3 · Taoseef Ishtiak4 · Nazifa Rafa5 · M. Mofijur6,7 · \nA. B. M. Shawkat Ali8,9 · Amir H. Gandomi10,11 \n1 Science and Math Program, Asian University for Women, Chattogram 4000, Bangladesh\n2 Data Science and Artificial Intelligence, Asian Institute of Technology, Chang Wat 12120, \nPathum Thani, Thailand\n3 Department of Computational Biology, Carnegie Mellon University, Pittsburgh, PA 15213, USA\n4 School of Computer Science, Carleton University, Ottawa, ON K1S 5B6, Canada\n5 Department of Geography, University of Cambridge, Downing Place, Cambridge CB2 3EN, \nUnited Kingdom\n6 Centre for Technology in Water and Wastewater, School of Civil and Environmental Engineering, \nUniversity of Technology Sydney, Ultimo, NSW 2007, Australia\n7 Mechanical Engineering Department, Prince Mohammad Bin Fahd University, Al Khobar 31952, \nSaudi Arabia\n8 School of Engineering and Technology, Central Queensland University, Melbourne, VIC 300, \nAustralia\n9 School of Science and Technology, The University of Fiji, Lautoka, Fiji\n10 Faculty of Engineering and Information Technology, University of Technology Sydney, Sydney, \nNSW 2007, Australia\n11 University Research and Innovation Center (EKIK), Óbuda University, 1034 Budapest, Hungary",
  "textLength": 309066
}