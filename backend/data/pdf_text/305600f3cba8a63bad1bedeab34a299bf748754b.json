{
  "paperId": "305600f3cba8a63bad1bedeab34a299bf748754b",
  "title": "Northstar: An Interactive Data Science System",
  "pdfPath": "305600f3cba8a63bad1bedeab34a299bf748754b.pdf",
  "text": "Northstar: an interactive data science system\nThe MIT Faculty has made this article openly available. Please share \nhow this access benefits you. Your story matters.\nAs Published 10.14778/3229863.3240493\nPublisher VLDB Endowment\nVersion Final published version\nCitable link https://hdl.handle.net/1721.1/132273\nTerms of Use Creative Commons Attribution-NonCommercial-NoDerivs License\nDetailed Terms http://creativecommons.org/licenses/by-nc-nd/4.0/\n\n\nNorthstar: An Interactive Data Science System\nTim Kraska\nMassachusetts Institute of Technology\nkraska@mit.edu\nABSTRACT\nIn order to democratize data science, we need to fundamentally\nrethink the current analytics stack, from the user interface to\nthe “guts.” Most importantly, enabling a broader range of users\nto unfold the potential of (their) data requires a change in the\ninterface and the “protection” we offer them. On the one hand,\nvisual interfaces for data science have to be intuitive, easy,\nand interactive to reach users without a strong background in\ncomputer science or statistics. On the other hand, we need to\nprotect users from making false discoveries. Furthermore, it\nrequires that technically involved (and often boring) tasks have\nto be automatically done by the system so that the user can\nfocus on contributing their domain expertise to the problem. In\nthis paper, we present Northstar, the Interactive Data Science\nSystem, which we have developed over the last 4 years to ex-\nplore designs that make advanced analytics and model building\nmore accessible.\nPVLDB Reference Format:\nTim Kraska. Northstar: An Interactive Data Science System. PVLDB ,\n11 (12): 2150-2164, 2018.\nDOI: https://doi.org/10.14778/3229863.3240493\n1. INTRODUCTION\nTo truly democratize Data Science, we need to fundamen-\ntally change the way people interact with data. Astonishingly,\nthe interfaces people use to analyze data have not changed\nsince the 1990s, and most analytical tasks are still performed\nusing scripting languages and/or SQL. Of course, there have\nbeen fashion trends in the choice of programming language\n(e.g., from PERL to Python), algorithms (e.g., from neural nets\nto statistical learning and back to neural nets), and database\ntechnology (SQL to NoSQL to Not Only SQL). Yet, people still\ninteract with data primarily through writing scripts and SQL-\nlike languages, with up to hour-long wait times for results.\nThis work is licensed under the Creative Commons Attribution-\nNonCommercial-NoDerivatives 4.0 International License. To view a copy\nof this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. For\nany use beyond those covered by this license, obtain permission by emailing\ninfo@vldb.org.\nProceedings of the VLDB Endowment, V ol. 11, No. 12\nCopyright 2018 VLDB Endowment 2150-8097/18/8.\nDOI: https://doi.org/10.14778/3229863.3240493We argue that we should stop holding onto the past; rather,\nwe should start designing systems for how Data Science should\nbe done 10 years from now. With Northstar1, a system for\ninteractive Data Science, we’ve tried to do exactly that for the\nlast four years. Perhaps surprisingly, some aspects of our vision\nfor the system have been inspired by movies such as ”Minority\nReport” and the newer James Bond ﬁlms. All of these movies\nfeature highly collaborative visual environments with touch\n(and pen) interfaces for analyzing data; you see nobody coding\nin Python. With Northstar, we had a very similar goal: to\nprovide a highly collaborative visual Data Science environment\nbased on a touch and pen interface, and simplify its use so\nmuch that domain experts who are not trained in statistics or\ncomputer science can use it without any help.\nAt the same time, we wanted to use hardware that is already\navailable and not wait until holograms actually become reality\n(though, we were really excited about HoloLens [76]). We\ntherefore settled on interactive whiteboards – speciﬁcally, the\nMicrosoft Surface Hub – as our core target platform. Inter-\nactive whiteboards are essentially large multi-touch TVs, but\noften with highly reduced lag time and better resolution, which\nprovide a real alternative to whiteboards. Consequently, we\nwere not shocked that Microsoft struggled to fulﬁll the demand\nfor the Surface Hub [111], nor did it surprise us that other tech\ncompanies, such as Google, followed suit and now also offer\ntheir own interactive whiteboard solutions.\nSo far, interactive whiteboards are just better conferencing\nsystems, but they have the potential to be much more. We\nwant to put them at the center of every meeting that involves\nnumbers, from discussing sales ﬁgures to better understanding\nthe customer base, and even to building predictive models. We\nenvision a collaborative environment where domain experts and\ndata scientists can work together to arrive at initial solutions\nduring a single meeting – solutions which can then, if neces-\nsary, be reﬁned ofﬂine. This is in stark contrast to the current\ndreadful way that data scientists and domain experts interact:\nmeetings after meetings to ﬁnd a common base before real\nprogress is ﬁrst made. Consequently, to foster collaboration\nand results during a meeting, the system has to provide a vi-\nsual interface, because co-programming Python with a CEO is\nsimply not an option. Furthermore, we want to enable domain\n1Previously, it was named Interactive Data Exploration System\n(IDES), but this name no longer seemed adequate since we\nadded signiﬁcant support for model building.\n 2150\n\ndateavg(churn XYZ)feb 17 nov 17 jun 17\ndateavg(churn abc)feb 17 nov 17 jun 17\nchurn XYZcount\nYes No(1a)\nagenorm.  count0 20 40 60 80 100\n(1b)pattern support\ninputproducts\nused 100% XYZ\nDEF , XYZ 43%\nABC, XYZ 29%\nQRS, XYZ 13%uses XYZcount\nYes No\nchurn XYZcount\nYes No\nchurn XYZcount\nYes No(2a)\n(2b)F175%(3a)\nlabelchurn\nxyz\ntrue\ntrue falsepredictedfalseactual\nstudentcount\nYes No (3b)offline(3c)Figure 1: Illustration of Vizdom accompanying the use case described in Section 2. (1a) Example of progressive visualiza-\ntions. (1b) Example of a brushing operation initiated by putting two visualizations close to each other. (2a) Example of a\nfrequent itemset operator that is ﬁltered by a selection in the visualization in the top right area. (2b) Operators can also serve\nas ﬁlters for downstream visualizations. Dashed connection lines represent a NOT operator. The bottom left visualization\nis ﬁltered to everything BUT the item selected in the operator. (3a) Example of a prediction operator that uses Northstar’s\nautomated ML capabilities. (3b) Visualizations can be used as ﬁlters to subset the data a prediction operator works on. (3c)\nUsers can choose to transfer long-running prediction operators to an ofﬂine mode if more precision is desired.\nexperts to build models on their own without the help of a data\nscientist or within the meeting room setting. Thus, the user\nexperience on a domain expert’s laptop should be similar to that\non an interactive whiteboard and feature a virtual data scientist,\nwho watches over the process and prevents any major mistakes.\nInterestingly, by putting the user experience ﬁrst, we not only\nfound that existing systems do not work in this setup, but we\nalso ended up designing a system very different from one we\nwould have created using a systems-ﬁrst approach. Today, we\nalready have several pilot deployments of Northstar in industry\nand academia, among them Adobe and IGT.\nThe main goal of this system paper is to provide an overview\nof Northstar and explain the rationale behind its design, as well\nas outline interesting challenges, solutions, and future work for\ndesigning the next generation of Data Science systems, with\nthe goal of eventually truly democratizing Data Science.\nThe remainder of this paper is organized as follows: After\na motivating use case (Section 2), we provide an overview\nof Northstar (Section 3), and afterward discuss the different\ncomponents of Northstar in more detail (Section 4 - 8). Fi-\nnally, we discuss related work (Section 9) and future directions\n(Section 10), before concluding the paper (Section 11 & 12).\n2. A MOTIV ATING USE CASE\nTo motivate Northstar and illustrate its power, we present an\nintroductory use case of how we envision Northstar being used.\nThroughout this description, we refer to parts of Figure 1.\nPete, a product manager, and Dana, a data scientist, both\nwork at a large software company that offers various produc-\ntivity tools through a subscription-based model. The company\nhas recently released a new product, XYZ, and Pete has noticed\nthat it’s not meeting their expectations. More speciﬁcally, it\nseems like the churn rate (i.e., the numbers of customers who\nstop their subscription) for XYZ has been increasing steadily.\nPete asks Dana to meet with him to investigate the issue\ncollaboratively and explore how customers are behaving. They\nstart up Vizdom, the front end of Northstar, on an interactivewhiteboard (see also Figure 2) in one of the meeting rooms and\nstart out with a few data exploration queries. (1a) For example,\nthey plot the churn rates of other products and ﬁnd, that the\noverall trend for XYZ indeed looks worse. (1b) Using Vizdom’s\nbrush and normalization features, they follow up by looking at\nXYZ’s user demographics to see if particular subpopulations\n(different age groups, in the example) of users are more likely to\nstop their subscriptions. They conclude that no particular group\nof users shows any interesting trends. All of these visualizations\nare computed and reﬁned progressively. Despite their large data\nset, Dana and Pete see near-instantaneous results for all their\ninteractions, allowing them to explore many different questions\nin a short amount of time and without losing focus on their\ncurrent train of thought.\nDana suggests investigating interactions between different\ntools. (2a) She brings up a frequent itemset operator in Vizdom,\ninputs ”products used,” and ﬁlters it to include only users who\nhave used XYZ; they observe that users commonly use XYZ in\ncombination with other company tools. (2b) Pete has a hunch\nand selects all the users using XYZ together with ABC. He has\nlong suggested that the overlap in functionality between the\ntwo tools is fairly signiﬁcant and that they might compete with\neach other. Looking at the churn rate for this subpopulation\nand comparing it to the overall population of users who use\nXYZ, they see that there is a noticeable difference: users who\nuse both tools are more likely to end their subscription than\nusers who do not. Vizdom’s ﬁlter functionality supports the full\npower of boolean queries. In the example, Pete uses a gesture to\nproduce a NOT operator, represented by the dashed connection\nline. Pete writes a note: he wants to follow up with his team\nand discuss ideas for making the two tools more distinct.\nAfter exploring the data, our two protagonists decide to be\nmore proactive and install measures to reduce the number of\nfuture subscription cancellations. In the past, they had good\nsuccess with sending coupons to customers. They think adding\na new customer relation workﬂow that automatically sends this\ncoupons to users they are about to loose could help.\n 2151\n\n(3a) To check how feasible this is, they use Vizdom to build a\nmodel that predicts, given all available data, if a particular user\nis likely to cancel their subscription in the future. The system\ngoes off and initiates an automatic model search that validates\nits results by splitting the data into various training and testing\nfolds. Since this operation is again done progressively, Pete\nand Dana can see after just a few seconds that the system found\na model that does fairly well on this task (i.e., F1-Score = 75%).\nPete is not an expert in machine learning, so Dana toggles the\nview to a confusion matrix that shows the detailed performance\nof the best model the system has found so far. Pete wants some\nmore details, so they inspect the model in more depth. Among\nother things, they can inspect different training testing splits,\nsee the predictions for different user sub-populations, or look at\nattribute-based decision boundaries. In doing so, Pete notices\nthat one of the top decision criteria is whether a customer\nis a student. That seems ﬁshy to Pete. Given his domain\nbackground and experience, he knows there is high ﬂuctuation\namong students. They often cancel their subscriptions at the\nend of a semester then sign up again at the beginning of the\nnext. (3b) He asks Dana to exclude this population of users\nfrom their model. The system restarts its model search and\nPete and Dana see that the overall performance is a bit worse\nnow, but Pete thinks it’s better to exclude those users to avoid\nsending unnecessary promotions.\nThey decide that the workﬂow is reasonable and plan to\ndeploy it into the production environment. (3c) Dana puts the\nongoing model search into ofﬂine mode. The system will keep\nsearching for and improving models that solve this problem\nand will notify Dana of the results after a speciﬁed time. At\nthat point, Dana can export the best model the system found in\nPython and hand it off to one of her team members to set up\nthis new customer-relations workﬂow with some A/B testing.\nAfter a few weeks, Dana and Pete reconvene and use Vizdom\nagain to analyze if this prediction model worked well and if\nsending these promotions affected the churn rate.\n3. Northstar OVERVIEW\nPutting the targeted user experience ﬁrst, as sketched out in\nthe previous section, led us to a very different system design\nthan a systems-ﬁrst approach would have. In this section, we\nﬁrst discuss some of the key realizations that inﬂuenced the\nsystem design before presenting the overall system architecture.\n3.1 Key Requirements for Designing an In-\nteractive Data Science System\nOver the course of the project, we conducted several user\nstudies [113, 115, 114], which guided our system design to\naddress the aforementioned use case. While some of the real-\nizations appear to be trivial in hindsight, they were not when we\nstarted. In the following, we outline some of our key ﬁndings\nduring the course of the project.\n(1) Results have to be approximate : Results for any opera-\ntion, including machine learning, have to return in seconds, if\nnot milliseconds. Anything else disturbs the user’s experience,\nleading to fewer discoveries [69, 113] and causing domain\nexperts to walk away, saying, “better to do that ofﬂine.” Unsur-\nvizdomIDEAAlpine MeadowPrimitive LibrarySpark…PythonTuplewareQUDELegacy SystemLegacy SystemFigure 2: Northstar architecture overview\nprisingly, that implies that results have to be approximated, as\nit is otherwise often impossible to return answers fast enough.\n(2) Outliers matter : While a lot of work in approximate\nquery processing disregards the importance of outliers, we\nfound them extremely important. Users have a tendency to\nexplore anything that stands out – the most valuable customers,\nrecords with data errors, the top-k sold products, etc.\n(3) Progressive results are often better than error bars :\nWhile the ﬁrst version of our interface featured error bars, we\nnow support them only as an option for advanced users. As\nother studies have already shown [23], error bars are often mis-\ninterpreted by users or simply ignored in the ﬁrst place. We\nfound that progressive results, which are continuously updated\nin the background until they converge on the complete answer,\nprovide the user with a much better experience. Most impor-\ntantly, we found that ﬂuctuations in the visualization help the\nuser to better understand how reliable the approximation is, and\nthe guarantee that the result will eventually converge provides\nadditional conﬁdence.\n(4) Connect & explore over legacy systems : Companies\noften have a large landscape of legacy systems, from various\ndatabase installations (Oracle, DB2, Postgres, etc.) to dis-\ntributed ﬁlesystems (Hadoop, EMC, etc.), which hold most of\ntheir data and are extremely hard to change. Therefore, a key\nrequirement is that a new analytics system has to seamlessly\nintegrate with the existing infrastructure and that a user can\nsimply connect to a data source and start exploring without any\ntime-consuming pre-processing steps.\nThese requirements inﬂuenced one of the most important de-\nsign decisions for Northstar; instead of creating a new database\nsystem which replaces existing DWHs (e.g., as proposed in\n[106, 107]) to better support Interactive Data Science (IDS), we\ndecided to design an accelerator, which sits on top of DWHs,\nﬁlesystems, etc., and essentially functions as a giant intelligent\ncache for the data source.\n 2152\n\n(5) Visualizations are not like SQL queries : The work-\nload created by visualization systems is very different from\nwhat TPC-H and TPC-DS make us believe analytical work-\nloads look like. A single visualization often requires more than\none query, and even simple visualization are often extremely\nhard to express as SQL queries [35]. For example, histograms\nrequire binning the data into buckets and then performing an\naggregation per bucket, which is surprisingly complicated to\nexpress in SQL and not well supported by existing DBMSs.\n(6) Visualization can hide information : For months, we\ndemonstrated Northstar’s unsupervised learning feature using\nMIMIC II [77], a data set about critical care patients. Using\nfrequent itemset clustering, we showed that we can quickly ﬁnd\na large difference between the common diseases of young vs.\nold people. However, just by chance, one of us inspected the\ndata more closely and found that the age 0was used for patients\nwhose age was unknown. When those records were removed,\nthe difference in the clustering wasn’t that pronounced anymore,\nindicating that patients with missing patient information also\nhave different diseases. This simple discovery led us down a\nrabbit hole of research topics to investigate techniques that can\nautomatically point out potential problems in an analysis, from\nthe Simpson Paradox to Multiple-Hypothesis Error.\n(7) Taking a holistic view of Data Science is important:\nUsers frequently want to switch between looking at data (ex-\nploring it), transforming and analyzing it (building models,\nrunning stats). For example, with the MIMIC 2 data set, we\nwanted to test if patients with a fever also have a higher heart\nrate. We therefore quickly created a statistical test in our inter-\nface, but then noticed that the temperature of some patients was\nreported in Celcius and not Fahrenheit. Thus, a user should be\nable to quickly write a UDF to convert Celsius to Fahrenheit\nand rerun the whole analysis with the corrected input without\never switching to different tools.\n3.2 Overall System Architecture\nWe designed Northstar speciﬁcally to address the IDS re-\nquirements, and also kept redesigning it as we came across\nnew ones. For example, we ﬁrst designed Vizdom with the\ngoal of taking advantage of our existing analytics framework,\nTupleware [24, 25]. Tupleware is in many aspects similar to\nSpark, but was designed to run on small high-performance clus-\nters and was able to achieve one to three orders of magnitude\nbetter performance. However, while our initial belief was that a\nfast execution engine is key to achieving interactivity, it turned\nout that query approximation and progressive results and the\nability to quickly change the analytical workﬂow based on user\ninteraction are far more important and often contradict the goal\nof code compilation. Interestingly, the same observation was\nrecently made by the HyperDB team. Because of its excellent\nquery execution performance, HyperDB was sold to Tableau\nand now is primarily used as a backend for Tableau’s visual\nfront end. However, in a recent publication [61], the authors\nprimarily describe how to avoid code generation for the often\nshort running queries generated by the visual front end. In con-\ntrast, we decided to build IDEA, the interactive data exploration\naccelerator, and now only use Tupleware to pre-compile com-\nplex operations. Furthermore, during the turn of the project,\nPython became increasingly popular as the main language ofchoice for data scientist. Thus, it became pretty clear that we\ncan not rely on a single execution engine, but rather need to be\ncompatible with other frameworks.\nThe resulting system architecture is shown in Figure 2. The\nVizdom front end provides a visual data exploration environ-\nment speciﬁcally designed for pen and touch interfaces, such as\nthe Microsoft Surface Hub. Figure 2 includes an actual picture\nof the Microsoft Surface Hub in our lab running Vizdom to\nexplore a medical data set. A demo video of Vizdom can be\nfound here vimeo.com/139165014 . Currently, Vizdom\nconnects to IDEA using a standard REST interface, which in\nturn connects to the data sources using the appropriate proto-\ncols (e.g., ODBC). These data sources can include anything\nfrom legacy data warehouses to raw ﬁles to advanced analytics\nplatforms (e.g., Spark, Hadoop).\nIn turn, IDEA acts as an intelligent cache and streaming\napproximation engine that uses Tupleware [25, 24], Python,\nSpark or other engines as runtimes for more complex analytics\ntasks. To inform IDEA about which operations are available\nin which runtime, the primitive library provides a standard-\nized API and metadata information about them. For the ML\nauto-tuning, we built Alpine Meadow, a ”query” optimizer for\nmachine learning. Finally, QUDE, the component to Quantify\nthe Uncertainty in Data Exploration, monitors every interaction\nthe user does and tries to warn about common mistakes and\nproblems and, if possible, even prevents them from happening\nin the ﬁrst place.\nIn the following, we describe each component in more detail\nand how they address the aforementioned challenges.\n4. VIZDOM: AN NOVEL INTERFACE\nFOR DATA SCIENCE\nAs Fisher et al. [42] argued, a common way to perform data\nanalytics at the turn of the 21st century was to use spreadsheet\napplications and data sets that would ﬁt completely in mem-\nory. Computations were therefore fast and results were visible\nwithin seconds. Users could perform multiple analyses simul-\ntaneously, explore different aspects of the data, and iteratively\nand interactively reﬁne ﬁndings at a fast pace.\nToday, these conveniences are gone. Increasing data com-\nplexity that requires specialized query languages and transfor-\nmations, modern analytical scenarios that rely on advanced\nalgorithms (e.g., machine learning), and the sheer size of to-\nday’s data all force users to interact with data through custom\njobs written in scripting or programming languages. These\njobs run for minutes or hours in the cloud, without providing\ninsights on what goes on behind the scenes.\nThis mainframe-like interaction paradigm is an inherently\npoor ﬁt for Data Science. The work is exploratory by nature and\ndemands rapid iterations, and all but the simplest analysis tasks\nrequire domain experts, who often do not have programming\nskills, to be in the loop to effectively steer the process.\nWhile systems like Tableau are a step in the right direction,\noffering a visual interface for data exploration, they lack support\nfor creating sophisticated models. In our work to make Data\nScience more accessible, we saw user experience as a crucial\ncomponent. We consciously designed Northstar using a top-\ndown approach, where user needs drive the requirements for the\n 2153\n\nrest of our system. We therefore closely collaborated with Andy\nvan Dam’s group at Brown University to develop Vizdom [26],\na novel pen-and-touch interface for interactive Data Science\n(Figure 2 shows Vizdom running on a Microsoft Surface Hub).\nVizdom exhibits a ﬂuid [36] and novel interaction style that\nis designed to promote “ﬂow”—staying immersed in the cur-\nrent activity and not being distracted by the user interface—and\nrelies on prompt feedback and fast response times. Interac-\ntively analyzing multidimensional data sets requires frequent\nswitching between a range of distinct but interrelated tasks\n(e.g., producing different visuals based on different column\nsets, calculating new variables, observing interactions between\nsubsets of the data, creating statistical models, etc.). Vizdom\naddresses this challenge by unifying a comprehensive set of\ntools for visual data analysis into a hybrid pen-and-touch sys-\ntem designed to exploit the visualization advantages of large\ninteractive displays. Tools are either data views, placeholders\nfor visualizations, or operators that perform transformations or\ncomputations on data. User can interact with these elements\nthrough direct manipulation, and elements will act not only as\nresult-viewers but also, more importantly, also as controls for\nadjusting or steering ongoing computations. Leveraging an un-\nbounded whiteboard metaphor, users can combine these tools\nlike building blocks to create interactive and visual workﬂows.\nIn designing Vizdom, we put heavy emphasis on fast re-\nsponses to each and every user interaction regardless of the\nsize of the data being analyzed. Human-computer interaction\nliterature [80, 93] often states that a delay of one second is the\nupper bound for system responses, after which users lose focus\non their current train of thought. To ensure a highly interactive\nenvironment for data analysis, Vizdom makes use of progres-\nsive visualizations and approximate answers as computed by\nIDEA (see Section 6).\nWe ﬁrst demonstrated Vizdom at VLDB 2015, where it won\nthe Best Demo Award. Since then, we have worked with various\nacademic and industry partners to get Vizdom with its backend\ndeployed and learn more about the real needs of domain experts\nand data scientists across various domains. We worked with one\nof Adobe’s Data Science teams, who used Vizdom to analyze\ntheir product subscription data, and started a collaboration with\nIGT, among others. Similarly, we continuously use Vizdom\nfor focused user studies to better understand user behavior. For\nexample, we studied the impact of approximate visual results\non exploratory analysis [113] and examined the effect of the\nmultiple comparison problem in visual analysis [115].\n5. TUPLEWARE: BARE METAL SPEED\nFOR UDFS\nTodaya’s analytics frameworks are ill-suited to support in-\nteractive visual frontends, even for simple operations on small\ndata sets. Current frameworks (e.g., Hadoop, Spark) are de-\nsigned to process massive data sets distributed across huge\nclusters, which addresses the problems faced by giant Internet\ncompanies. With these frameworks, just scheduling a single\njob can often take longer than any reasonable interactivity la-\ntency threshold. With Tupleware, we explored the design of a\nnew analytical framework for interactive latencies and “normal”\nusers—not the Googles of the world. Two key contributions ofTupleware are (1) the close-to-zero execution and scheduling\noverhead and (2) new query compilation and optimization tech-\nniques. The latter fundamentally bridges the gap between query\noptimizers, which usually make high-level optimization deci-\nsion (e.g., join ordering), and compilers, which make low-level\noptimization decisions (e.g., loop-unrolling).\nWhile Tupleware was very important in the early stages of\nNorthstar, it lost its importance in the interactive Data Science\nstack over time. First, it turned out that even the fastest exe-\ncution engine can be too slow to provide interactive response\ntimes for complex operations. Thus, we started to implement\nmore and more algorithms as approximate and progressive al-\nternatives directly in IDEA (see next section). Second, code\ncompilation, as also noted by others [61], often has an up-front\ncost, which can quickly add up in cases where many small,\nshort-running queries dominate the workload. Hence, it be-\ncame more efﬁcient to use Tupleware mainly to pre-compile\ncomplex analytical operations (similar to stored procedures),\nwhich are then combined into complex workﬂows based on the\nuser interactions within IDEA using an iterator-based execution\nmodel. However, Tupleware was one of the very ﬁrst systems\nto compile complex analytical workﬂows, and many systems\nbuilt upon its results [1, 78, 83, 37]. Furthermore, we believe\nthat Tupleware’s compilation strategies might play an important\nrole in the next generation of Northstar as part of synthesizing\nbetter access methods [63].\n6. IDEA: AN INTERACTIVE DATA\nEXPLORATION ACCELERATOR\nAs mentioned in the previous section, even the fastest run-\ntime can be too slow to guarantee interactive response times\nover very large data sets. Approximate query processing (AQP)\ntechniques can help in these situations, but existing techniques\nfall short in providing good approximations over rare events.\nYet, users commonly explore those events, as they often contain\ninteresting insights (e.g., the habits of the few highest-valued\ncustomers, the suspicious outliers, etc.). Furthermore, exist-\ning AQP engines integrate poorly with legacy systems. We\ntherefore started to develop the ﬁrst Interactive Data Explo-\nration Accelerator (IDEA) [27] with the goal of building the\nﬁrst approximation engine for interactive Data Science, which\nseamlessly integrates with existing IT infrastructures.\n6.1 Neither a DB nor a Streaming Engine\nInterestingly, IDEA required a fundamental rethinking of\nthe query execution model; it is neither a system for one-shot\nqueries, nor traditional AQP engine, nor a streaming engine.\nRather, IDEA has entirely unique semantics. Fundamentally,\nIDEA is a database as a middle-tier and acts as an intelligent, in-\nmemory caching layer that sits in front of the much slower data\nsources, managing both progressive results and the samples\nused to compute them.\nBut even over cached samples, the query execution model\nis different. Unlike DBMSs, queries are not one-shot opera-\ntions that return batch results; rather, workﬂows are constructed\nincrementally, requiring fast response times and progressive\nresults that reﬁne over time. It is also not a traditional AQP\nengine, as users often incrementally compose operations into\n 2154\n\ncomplex workﬂows with one output being the input for one or\nmore other operations, like in a streaming system. Yet, in con-\ntrast to streaming engines, IDEA is meant to enable free-form\nexploration of data sampled from a deterministic system (e.g.,\na ﬁnite data source), whereas traditional streaming engines\ntypically assume a predeﬁned set of queries over inﬁnite data\nstreams and do not have samples as inputs.\nAlso in contrast to a traditional DBMS, IDEA can ofﬂoad\npreﬁltering and pre-aggregation operations to an underlying\ndata source (e.g., perform a predicate pushdown to a DBMS)\nor even transform the base data by executing custom UDFs in\nthe source. Most importantly, though, in contrast to traditional\nDBMSs, AQP engines, or streaming engines, IDEA users com-\npose queries incrementally, therefore resulting in simultaneous\nvisualizations of many component results with varying degrees\nof error. Maintaining different component partial results rather\nthan single, exact answers imposes a completely new set of\nchallenges for both expressing and optimizing these types of\nqueries. Currently, our IDEA prototype uses a preliminary\ninteraction algebra to deﬁne a user’s visual queries.\n6.2 Visual Indexes\nSimilar to the algebra and optimizer, we also found that tra-\nditional indexes are suboptimal for interactive data exploration\ntasks. Existing incremental techniques either sort the data (e.g.,\ndatabase cracking [51]) or do not naturally support summary\nvisualizations. However, sorting can destroy data randomness\nand, consequently, the ability to provide good estimates. Simi-\nlarly, existing techniques generally index every tuple without\nconsidering any properties of the frontend (e.g., human percep-\ntion limitations, visualization characteristics) and can require a\nlot of storage space, especially for highly dimensional data.\nFor example, some visualizations (e.g., histograms) require\nthe system to scan all leaf pages in a traditional B-tree, since this\nindex is designed for single-range requests rather than provid-\ning visual data summaries. We therefore proposed VisTrees [35],\na new dynamic index structure that can efﬁciently provide ap-\nproximate results speciﬁcally to answer visualization requests.\nThe core idea of VisTrees is that the nodes within the index are\n“visually-balanced” to better serve visual user interactions and\nthen compressed based on perception limitations.\n6.3 Sample Management\nAs previously mentioned, IDEA caches as much data as pos-\nsible from the underlying data sources in order to provide faster\napproximate results, since most data sources are signiﬁcantly\nslower. For example, the memory bandwidth of modern hard-\nware ranges from 40\u000050GB/s per socket [10, 110], whereas we\nrecently measured that PostgreSQL and a commercial DBMS\ncan only export 40\u0000120MB/s, even with a warm cache holding\nall data in memory. Although DBMS export rates may improve\nin the future, IDEA’s cache will still remain crucial for provid-\ning approximate answers to visual queries and supporting more\ncomplex analytics tasks (e.g., ML algorithms).\nConceptually, IDEA can be best referred to as a sample\nmanagement system and roughly divides the memory into\nthree parts: the Result Cache , the Sample Store , and space for\nIndexes . When triggered by an initial user interaction, IDEA be-\ngins ingesting data from the various data sources, speculativelyperforming operations and caching the results in the Result\nCache to support possible future interactions. At the same time,\nIDEA also caches all incoming data in the Sample Store using\na compressed row format. When the available memory for the\nSample Store is depleted, IDEA starts to update the cache using\na reservoir sampling strategy to eventually create a representa-\ntive sample over the whole data set even if the data stream is\nbiased. To further mitigate the impact of bias in the data stream,\nIDEA takes advantage of sampling operators most database\nsystems provide as well as reads from random offsets of the\ndata (e.g., when connected to a ﬁle). Furthermore, IDEA might\ndecide to split up the reservoir sample into several stratiﬁed\nsubsamples to overrepresent the tails of the distribution, or to\ncreate specialized indexes for potential future queries. This is\ndone based on the current visualizations on the screen as they,\nfor example, determine what ﬁlter chains the user is able to\ncreate. All these decisions are constantly optimized based on\nboth past and current user interactions. For example, if the user\ndrags a new attribute onto the canvas, the system will allocate\nmore resources to the new attribute in preparation for potential\nfollow-up queries. At the same time, IDEA constantly streams\nincreasingly precise results to the frontend as the computation\nprogresses over the data, along with indications about both the\ncompleteness and error estimates.\n6.4 Approximating Black Boxes\nIn contrast to most other AQP engines, IDEA has the goal of\napproximating complex analytics and machine-learning pipelines.\nThis is particularly challenging as many operations are black\nboxes to IDEA. For example, IDEA is fully compatible with\nscikit-learn [52] and even allows users to add new Python op-\nerations. In order to approximate results for these operations,\nIDEA uses a relatively simple idea: it executes the operation\nﬁrst over a small sample and then re-executes the operation over\nprogressively increasing sample sizes. However, this creates a\nwhole new set of challenges, e.g., what is a good sample size\nto start with and in what increments should it be made larger?\nWe address some of these questions in more detail when we\ndiscuss Northstar ML auto-tuning capabilities (Section 7).\n6.5 Result Reuse\nAs outlined earlier, query approximation for visual interac-\ntive Data Science has its own set of challenges but also pro-\nvides a vast array of opportunities, one of them being reuse.\nVisual tools have encouraged a more conversational interaction\nparadigm [43], whereby users incrementally compose and iter-\natively reﬁne queries throughout the data exploration process.\nMoreover, this style of interaction also results in several sec-\nonds (or even minutes) of user “think time” where the system\nis completely idle. Thus, these two key features provide an\nAQP system with ample opportunities to (1) reuse previously\ncomputed (approximate) results across queries during a session\nand (2) take actions to prepare for potential future queries.\nHowever, it turned out that there existed close to no work to\nefﬁciently reuse approximate results. To that end, we made an\ninteresting and, in hindsight, conceivably trivial observation:\nalmost all visualizations convey simple statistics about the data.\nBased on that observation, we developed a new AQP formula-\ntion that treats aggregate query answers as random variables\n 2155\n\nFigure 3: IDEBench results for four systems as a summary report. Results show the mean percentage of time violations\nand missing histogram bins, as well as the mean relative errors (MREs) and MRE CDF for the approximated results; the\ngreater the proportion of small errors, the smaller the area above the curve.\nto enable reuse of approximate results with formal reasoning\nabout error propagation across overlapping queries [43].\nFor example, consider a simple bar chart showing the count\nper category of the following SQL query:\nSELECT sex, COUNT (*)\nFROM census\nGROUP BY sex\nOne way to model a group-by attribute is to treat it as a\ncategorical random variable X, where Xcan assume any one\nof the possible outcomes from the sample space \nX. For\nexample, a random variable modeling the sexattribute can take\non an outcome in the sample space \nsex=fMale;Femaleg.\nMore interestingly, ﬁlter chains as shown in Figure 1 can be\nexpressed as conditional variables. For example, assume that a\nsexbar chart was linked to a salary bar chart as the downstream\noperation, and only ‘Female’ was selected. Let us further\nassume that Yis the random variable for the salary distribution.\nThen, we can determine the proportional height of the bar for\nthe salary range 0\u000010kasP(Y\u001410kjX=Female ).\nSurprisingly, this view of query results for visualizations not\nonly made it easier to estimate the quality of results for each\noperation in an incrementally composed workﬂow, but also\nopened up many new opportunities for result reuse. For exam-\nple, it allows “query rewrites” by means of Bayes’ Theorem or\nthe Law of Total Probability.\n6.6 Results\nWe recently evaluated IDEA against other systems to create\na benchmark for interactive data exploration, called IDEBench\n[33]. The key result of this study is shown in Figure 3 and\ncompares MonetDB, IDEA, approXimateDB, and a commer-\ncial in-memory AQP system (referred to as System X) withrespect to the data loading time and data quality after X sec-\nonds over 500MB of ﬂight data [81] for 10 different interactive\nexploration workﬂows (see [33] for more details). As the ﬁg-\nure shows, IDEA overall is able to return the fastest and even\nmeets a targeted return time of 500ms around 99% of the time\nwith signiﬁcantly less data prep time. In this case, the data\nwas stored on ﬁle for all systems and we gave idea 3 minutes\nfrom “connecting to” the ﬁle to exploring. IDEA also has fewer\nmissing bins in approximated histograms, and the mean relative\nerror over all returned results is signiﬁcantly less than approXi-\nmateDB ’s and marginally less than System X ’s’. One of the key\nreaons is, that the benchmark simulates a user who explores the\ndata set by incrementally building queries and that IDEA can\nexplore the incremental nature of requests. Finally, it should be\nnoted, that in contrast to the next best system, the commercial\nAQP engine System X, IDEA features progressive results, and\nin contrast to all other systems, it can also approximate the\nexecution of machine-learning algorithms.\n7. Alpine Meadow: A QUERY OPTIMI-\nZER FOR MACHINE LEARNING\nOne key promise of our work is to help users quickly ar-\nrive at an initial solution, often a model, in a collaborative\nmeeting. Unfortunately, selecting the right ML algorithm and\nhyper-parameter tuning is often a time-consuming and boring\nprocess. In 2013, with MLbase [64], we took the ﬁrst step\ntoward unlocking the power of ML for end users. MLbase\nprovided (1) a simple declarative way to specify ML tasks and\n(2) a novel optimizer to select and dynamically adapt the choice\nof learning algorithm. One of the key contributions of this work\n 2156\n\nwas TuPAQ [95], a novel bandit-based hyper-parameter tuning\nstrategy that was the predecessor of Hyperband [67]. How-\never, MLbase was not designed for interactive environments\nand the process of hyper-parameter tuning could take hours.\nTherefore, we started the Alpine Meadow project based on our\nexperience with MLbase. Alpine Meadow and MLbase have\nseveral commonalities: they both use a query-optimizer-based\napproach to ML auto-tuning and, for example, use bandits to\nefﬁciently explore the search space. But there are also signiﬁ-\ncant differences—most importantly, the focus on interactivity.\nIn the remainder of this section, we outline in greater detail\nwhat makes Alpine Meadow unique.\n7.1 Focus on End-to-End Learning\nAlpine Meadow was developed as part of the DARPA D3M\n[92] project, which aims to entirely automate machine learning.\nEvery six months, DARPA evaluates all systems on how well\nthey can autonomously solve a set of problems. The problems\nrange from text classiﬁcation tasks to building models that auto-\nmatically measure wrist length from images. Each task comes\nin the form of a JSON description containing the goal and a\ndescription of the available data for the task. The system then\nneeds to automatically ﬁnd the best pipeline in a given time\nframe (e.g., 10 minutes) and is evaluated using some quality\nmetric (e.g., F1 score). Therefore, in contrast to MLbase and\nmany other ML auto-tuning systems, Alpine Meadow’s goal\nis to automatically create the entire end-to-end workﬂow from\ndata cleaning operations to the model, including deep-learning\nmodels. Also in contrast to MLbase, Alpine Meadow is de-\nsigned to take user feedback into account. Meaning, a user can\nsteer the exploration, for example, by proposing new features\nor presetting certain operations, such as cleaning operations.\n7.2 Interactivity\nThe most notable difference between Alpine Meadow and\nother ML auto-tuning systems is the focus on interactivity.\nAlpine Meadow aims to provide a ﬁrst answer in seconds,\nwhich is then reﬁned in the background. Consequently, it\nusually tries simpler pipelines over small samples ﬁrst before\nincreasing the sample size and model complexity.\nFurthermore, we developed a cost-quality model to measure\nthe ”promisingness” of a pipeline over a given sample. By\nﬁltering out pipelines with high cost, we can prune the search\nspace, saving resources and reducing the overall search latency.\nFor now, our cost-quality model estimates three factors for a\ngiven pipeline over a sample of size m: (1) the time for training\nand testing the pipeline; (2) the expected quality gain of a\npipeline over the last best solution; (3) the risk of a pipeline,\nthat is, the variance of quality. These factors are then weighted\ndifferently over time. For example, at ﬁrst, the training time\nis given more weight as we want to return a good solution as\nquickly as possible to the user, potentially sacriﬁcing quality.\nLater we assume that the quality of a pipeline matters more\nand we allow proportionally more expensive pipelines to run\nto eventually ﬁnd the best performing model. . To actually\nbuild the cost model we use (1) learned rules (see next section)\nas well as (2) the history of past and ongoing training steps\n(see [9] for more details).7.3 Learned Rule-Based Optimization and\nTransfer Learning\nWe extensively use meta-learning techniques to utilize his-\ntory from similar problems. Most importantly, our optimizer de-\nrives best-practice rules from past experience and uses them to\ncreate and prune the search space as well as prioritize pipelines.\nTo jump-start the system, we trained it using publicly available\ncompetitions (e.g., Kaggle) as well as the sample problems\nthat DARPA provided (note that DARPA has a separate set\nof problems that we have never seen for evaluation purposes).\nFurthermore, we made the rules problem-speciﬁc. Based on\nthe techniques of [41], we determine the similarity of every\nproblem to previous problems and adjust the importance of\nrules based on this similarity.\nThis approach further allows us to transfer existing solutions\nto new problems. For example, for an image classiﬁcation\ntask, we might use an existing deep-learning model that was\ntrained on CIFAR, “chop off its head” (i.e., remove the top\nneuron layers), and replace it with a new set of layers for the\ngiven task (a common transfer learning technique). As a result,\nwe are often able to train more complex models in a short\namount of time, as our optimizer prefers to start from existing\nsolutions. A nice side effect of the rule- and transfer-learning-\nbased approach is that it also makes the ﬁnal pipelines simpler\nto explain to the user.\n7.4 User Interactions with Alpine Meadow\nWhile Alpine Meadow can be used without Vizdom and\nIDEA, it unfolds it full potentially when used together. For\nexample, using Vizdom users can at any time steer the search\nprocess, e.g., by restricting the model type or feature prepro-\ncessing steps, adding new features, and/or restricting the model\nbuilding to subpopulations of the data. Furthermore, IDEA en-\nsures that all operations, including the machine learning model,\nreturn ﬁrst results in sub-seconds after every user interaction,\nand that the key characteristics of the so far best models are\nalways visualized to the user. Furthermore, the best pipelines\ncan be interactively inspected and modiﬁed, and the output of a\nmodel can be used as an input for other operations in Vizdom,\nmaking it for example possible to quickly analyze on what data\nthe model does not perform well on, or to use the model itself\nto label unlabeled data, etc.\n7.5 Initial Results\nFigure 4 shows the performance of Alpine Meadow against\nother systems competing in D3M, which includes teams from\nStanford, NYU, UC Berkeley, and many others (anonymized\nas Systems 2-10). The ﬁgure shows how many of the DARPA-\nprovided prediction tasks each system is able to solve, how\noften they are better than the expert solution, as well as the\nnormalized utility score, which is deﬁned asP\nisi\u0000bi\njbij, with si\n(bi) being the performance of the system (baseline) on problem\ni. As of March 2018, Alpine Meadow solves not only all of\nthe DARPA-provided challenges, but is also able to outperform\nthe expert solution in 80% of the cases. Furthermore, the\nnormalized score shows that it provides overall good solutions\n(i.e., if the system were only a bit better in 80% of the cases\nbut otherwise much worse, the score would likely be negative).\n 2157\n\nSolved ProblemsBetter Than BaselineNormalized ScoreAlpine Meadow100%80%0.42System 240%27%0.09System 340%13%0.02DARPA Baseline100%0%0.00System 420%7%-0.07System 587%47%-0.16System 627%7%-0.22System 760%20%-0.59System 887%53%-0.75System 960%20%-1.14System 1060%20%-4.57Figure 4: DARPA D3M Competition Results on 03/2018\n8. QUDE: QUANTIFYING THE UNCER-\nTAINTY IN DATA EXPLORATION\nWhile visual tools are key to democratizing Data Science,\nthey also bring new risks. For example, data is often massaged,\nﬁltered, and visualized until the domain expert sees something\ninteresting, and only then is a statistical test performed. How-\never, this ignores the “ﬁshing expedition” before the test—and\nthe increased risk of a false discovery because of it. We there-\nfore believe that a system should automatically track potential\ncommon mistakes within a Data Science pipeline. But multi-\nhypothesis errors are just one type of potential problem among\nmany within Data Science pipelines. Others include the Yule-\nSimpson effect or, when training models, imbalance of labels\nor common problems in representing null values. If we want\nto empower a broader class of users without deep statistical or\nmachine-learning backgrounds to analyze data sets, we should\nwork toward automatically protecting them from these (com-\nmon) mistakes. Over the last one to two years, we have started\nto address this problem by developing QUDE (pronounced\n“cute”), a tool for Quantifying the Uncertainty in Data Explo-\nration. In the following, we highlight four areas where QUDE\nalready helps to detect, quantify, and sometimes even correct\ncommon problems.\n8.1 Uncertainty as Unknown Unknowns\nIncompleteness of data is one of the most common sources\nof uncertainty in practice. For instance, if unknown data items\nare missing from the unknown target population (i.e., we can’t\ntell if the database is complete or not ), even a simple aggregate\nquery result, like SUM, can be questionable.\nWe therefore started to develop techniques that estimate not\nonly the amount of missing data based on techniques from [100,\n101, 102] but also the impact those items might have on query\nresults [19, 20]. We assume a simple data integration scenario\nin which (semi-)independent data sources are integrated into\na single database. The overlap between the different data sets\nallows us to estimate the number of missing items using species\nestimation techniques [100]. Further, it is possible to make\nestimates about the values the missing items might have using\nour novel bucket estimator [19]. This way, Vizdom is able to\nindicate to the user how much impact missing data might have\non the visualization.\n8.2 Uncertainty as Undetected Data Errors\nFor a data scientist, it is important to know whether a data\nset is clean enough to begin analysis or if it is worthwhile toinvest more time and money in cleaning. In the best case, these\nunknown errors are unimportant corner cases, but often enough\nthey can be crucially overlooked problems that signiﬁcantly\naffect any subsequent analytics. This raises a fundamental\nquestion: Is it possible to quantify the data quality of a data set\nwith regard to the number of remaining errors in the data set?\nWhile this is a seemingly simple question, it is actually\nextremely challenging to deﬁne data quality without knowing\nthe ground truth [85, 14, 38, 91, 39, 58]. A simple approach\nis to extrapolate the number of errors from a small “perfectly\nclean” sample [104, 108, 11, 16, 22, 71]): (1) we take a small\nsample, (2) perfectly clean it manually or with the crowd, and\n(3) extrapolate our ﬁndings to the entire data set. For example,\nif we found 10 new errors in a sample of 1000 records out of 1M\nrecords, we would assume that the total data set contains 10000\nadditional errors. However, this na ¨ıvee approach presents a\nchicken-and-egg paradox. If we clean a small sample of data,\nit may not be representative and thus will give an inaccurate\nestimate. For larger samples, how can the analyst know that the\nsample itself is perfectly clean without a quality metric?\nWe therefore developed the Data Quality Metric (DQM) [18],\na statistical estimator based on the principle of diminishing re-\nturns, which basically states that every additional error is more\ndifﬁcult to detect. For example, with experts or crowdsourcing,\nthe ﬁrst (crowd-)worker to pass over the data set ﬁnds more\nnew errors than every subsequent worker, and so on. The key\ninsight is to estimate this diminishing return rate (i.e., fewer\nerrors are found in each pass) and project this rate forward to\nestimate the number of errors if there were an inﬁnite number\nof workers (i.e., all discoverable errors). The ratio of the current\nerrors to the estimated discoverable errors then functions as an\nindicator of the cleanliness of the data set.\n8.3 Uncertainty as False Discovery\nWhile interactivity is key to the usability of advanced visual\nanalytical tools [69], using them also signiﬁcantly increases the\nrisk of making spurious discoveries. Such risk has two aspects:\n(1)the statistical signiﬁcance of the visualized results is unclear,\nand(2)the growing number of hypotheses being tested during\nexploration increases with every single visualization.\nThe ﬁrst aspect of risk is important because visualizations\nhave the power to inﬂuence human perception and understand-\ning.Suppose that an ice cream company salesperson is exploring\na data set about sales. First, she wants to get a yearly distribu-\ntion of the sales ﬁgures. So, she compares the sales of the last\nﬁve years using a histogram of sales per year. In the second\nstep, she is interested in learning if sales differ signiﬁcantly\nacross states. She thus compares sales per state over the last\nﬁve years.\nSuppose the histogram shows that sales in Vermont were\nhigher than in Rhode Island. Consider how tempting it is for an\nunsophisticated user to conclude that Vermonters buy more ice\ncream just based on the visualization. Although a statistically\ninclined user would formally analyze this observation by using\nhypothesis testing, she would have to redirect her attention to\nwork with a different statistical tool (e.g., R) before proceeding\nto the next data exploration step. After such a context switch,\nthe insight might turn out completely wrong due to random\nnoise. At scale, the division of labor between data exploration\n 2158\n\nand hypothesis testing will cause even more waste of human\nefforts on such spurious insights. Thus, if a visualization pro-\nvides any insights, these should be tested immediately for their\nsigniﬁcance . If that is not be the case, the value of the visualiza-\ntion would be very limited, as the user would not be allowed to\nmake any conclusions based on the visualization. Thus, if we\nconsider a visualization as something more than a pretty picture\npresented to the user (i.e., more than just a listing of facts), we\nshould always test the insight the user gains from the visualiza-\ntion for its signiﬁcance and inform the user about it. A central\nchallenge of our work is the understanding of the hypothesis\nderived by the user given a certain data visualization. With\nrespect to the previous example, the hypothesis derived by the\nuser could be: (1) Vermonters buy more ice cream than Rhode\nIslanders, (2) Rhode Islanders buy more than Vermonters, or\n(3) they buy ice cream in the same amount.\nThe second aspect of risk is arguably even more severe. With\nevery additional hypothesis test, the chance of ﬁnding a false\ndiscovery increases. This problem is known as the “ multiple\ncomparisons problem ” (MCP) and has been studied extensively\nin statistics literature [12, 7, 46, 60].\nData exploration on systems such as Vizdom [26] or Tableau\nnot only increase the risk of false discovery, but also changes\nthe way that statistical tests are applied. Suppose in the pre-\nvious example the salesperson explores various relationships\nin the sales data set through visualizations until she sees a vi-\nsualization that she deems useful (e.g., signiﬁcantly more ice\ncream sales to males in Massachusetts compared to California).\nWith some background in statistics, she validates this insight\nby using an appropriate test with a signiﬁcance level of 5%.\nSuppose the observed p-value is below the signiﬁcance level,\nso she rejects the null hypothesis and believes that there is only\na5%chance that she incorrectly rejected the null hypothesis\nin case it was true. However, this way of applying statistical\ntesting is wrong. What the user ignores is that before she did\nthe test, she had already searched through the data set for a\nwhile and observed different insights and, implicitly, their cor-\nresponding hypotheses, albeit untested. Thus, by the time the\nuser applied the statistical test, she was already inadvertently\ntrapped in the multiple comparisons problem. This was also\nconﬁrmed in a recent user study we performed [115]. In our\nexperiment using synthetic data sets with known ground truth\nlabels, over 60% of user insights were false.\nUnfortunately, existing work to control for MCP are often\nnot directly applicable to interactive data exploration or have\nother severe drawbacks. For example, the most common ap-\nproach of using holdouts allows verifying the gathered insights\njustonce ; any additional data exploration session would require\na complete new holdout unless the testing over the holdout was\nMCP-controlled, so the problem remains (see [116] for more\ndetails). Furthermore, splitting a data set into exploration and\nholdout data sets can signiﬁcantly lower the power (i.e., the\nchance to ﬁnd real insights), especially for rare events. In con-\ntrast, statistical techniques such as the Benjamini-Hochberg\nprocedure to bound the False Discovery Rate (FDR) or the\nBonferroni procedure to bound the Family-Wise Error Rate\n(FWER) were not designed for incremental testing. We there-\nfore started to develop techniques which (1) try to infer based\non the visualizations what the user might be inferring/testingand (2) automatically control the FDR rate. For example, in\n[116, 117] we develop an automatic MCP control based on\nalpha-investing fully integrated into Vizdom. However, the\nwhole area is still in its infancy and many interesting research\nchallenges remain.\n8.4 Uncertainty as Hidden Facts\nVisualizations as a form of aggregation can “hide” data errors\nor the incompleteness of data, and sometimes even mislead. As\nmentioned in Section 3, we recently observed this phenomenon\nwhen analyzing the age distribution of patients in the MIMIC-\nII data set. The distribution was visualized using histograms\nwith a bucket size of 10 years. Nothing was suspicious about\nthe visualization, which showed that very young and older\npeople are in the emergency room slightly more often. Only\nafter zooming in did we ﬁnd that the data set did not contain\nany patients between 1 and 9 years; rather, all patients in this\nbucket had an age of 0. It turned out that the data came from\nan emergency room for adults and the value 0 was used if\nthe age was not known. Even more severe, ﬁltering out the\n0-aged patients signiﬁcantly changed our conclusions regarding\nyounger and older patients.\nInspired by this result, we developed techniques [47] to au-\ntomatically detect forms of Simpson’s Paradox, which is a\nspecial type of error in which a high-level aggregation leads\nto a wrong conclusion. The two main challenges to enabling\nefﬁcient online detection are sheer data size as well as the num-\nber of different attribute combinations that need to be tested.\nTherefore, for our algorithms, we applied two main techniques:\n(1) For dealing with large data sets, we developed a set of ap-\nproximate algorithms that can stream over the data and decide\nin a probabilistic manner if the data is likely to contain a Simp-\nson’s Paradox. This allows our algorithms to make a prediction\nat interactive speeds of how likely it is to ﬁnd a paradox after\nseeing only a small amount of data. (2) Since many different\nattribute combinations need to be tested to detect a Simpson’s\nParadox, we devised a technique that leverages ideas from mul-\ntiarmed bandits to ﬁnd a good trade-off between exploration\nand exploitation. These techniques allow us to scale out to\nlarge data sets or data sets with many different attributes.\nTo summarize, QUDE is a ﬁrst step toward building an as-\nsistant that can help domain experts who are not trained data\nscientists to make discoveries on their own. However, clearly\nwe are still at the beginning, and many interesting research\nchallenges remain open as described in Section 10.\n9. RELATED WORK\nOur work around Northstar spans many different research\nareas — in fact, so many areas that we need to refer to the\nindividual publications around Northstar for a more detailed\nlisting of related work and only highlight a few of them here.\nVizdom : the visualization community has produced various\nsystems that facilitate data exploration by domain experts. For\nexample, Tableau and its research predecessors Polaris [97] and\nimMens [70] are systems for analyzing data sets through visu-\nalizations with a high degree of customizability. To achieve the\nlow latencies required for user-driven data exploration, these\n 2159\n\nsystems either use heavily optimized DBMSs or precomputa-\ntion of results. Though we support similar data exploration\nworkﬂows, we apply an intuitive pen-&-touch interface in Viz-\ndom and focus on progressive computation and visualization\nto guarantee interactivity thresholds. Furthermore, we aim\nto support users in the entire Data Science process from data\npreparation over exploration to model building.\nIDEA : Most related to IDEA is the work in approximate\nquery processing using sampling [3] and online aggregation [50].\nHowever, systems that use biased sampling (e.g., AQUA [4],\nBlinkDB [5], DICE [56]) typically require extensive prepro-\ncessing or foreknowledge about the expected workload, which\ngoes against the ad hoc nature of interactive data exploration.\nOn the other hand, systems that perform online aggregation\n(e.g., CONTROL [49], DBO [55], HOP [21], FluoDB [112])\ntypically cannot deal with black-box operations and/or out-\nliers. Verdict [84] uses the results of past queries to improve\napproximations for future queries in a process called “database\nlearning.” However, Verdict requires up-front ofﬂine parameter\nlearning, as well as a sufﬁcient number of training queries, in\norder for users to begin seeing large beneﬁts. Also related to\nIDEA are techniques to improve the “visual experience;” for\nexample, approximated histogram [73], better smoothing of\ntimeseries data [89], or visualizing data transformations [59].\nThose techniques are largely orthogonal to the ideas presented\nhere, but it would be very interesting to integrate them into\nNorthstar. Most related to IDEA’s execution model are prob-\nably FluxQuery [31] and zenvisage [94], although their focus\nis on SQL workloads. Finally, in order to better support user\nsessions in DBMSs, various techniques have been developed\nto reuse results [98, 54, 79, 29]. Nonetheless, these techniques\ndo not consider reuse in the context of (partial) query results\nwith associated error.\nAlpine Meadow : Most related to Alpine Meadow is the\nwork on ML algorithm selection and hyper-parameter tuning\n[96, 8, 72, 68, 66, 45]. For example, TuPAQ [96] and Hy-\nperband [66] use variations of the multiarmed bandit (MAB)\nalgorithm to better allocate computational resources for hyper-\nparameter tuning. Other solutions like Auto-WEKA [99, 62]\nor its sister package Auto-sklearn [40] are more similar to our\napproach, as they also consider various feature selection and\ndata transformation algorithms with the intent of generating\nML pipelines. Still, these solutions are built for ofﬂine use and\nrun for a predeﬁned amount of time, rendering them unfeasible\nfor interactive settings. Furthermore, they also do not consider\ntransfer learning or data cleaning steps, and do not offer good\nsupport to take user input into account.\nQUDE : Obviously, QUDE builds upon the vast amount of\ntechniques developed in the statistics community (see [48] for\nan overview). Surprisingly, even though statistical errors are\nhighly common [53], there is very little work in automating\nthese techniques in the form of a Data Science assistant to\nprevent layman users. Some notable exceptions are the recent\nefforts to automatically detect bias in machine learning, algo-\nrithm building, and analytics [44, 109] or the use of perceptual\nmodels to quantify when approximations are safe [6].10. THE FUTURE OF INTERACTIVE\nDATA SCIENCE\nNorthstar provides one of the very ﬁrst interactive Data Sci-\nence environments with the goal of democratizing Data Science.\nIt addresses a wide range of research problems to make ana-\nlytics and model building more interactive, many of which we\ndiscovered only during the course of this project. However,\neven more challenges remain, and we believe interactive Data\nScience and the goal of making analytics more accessible for a\nbroader range of users creates a new research ﬁeld in itself. We\nhighlight a few potential future research challenges below.\n10.1 Formal Execution Model\nAs outlined in Section 6.1, the execution model of IDEA\nis not a traditional query processor, a pure AQP engine, or a\nstreaming engine. However, a precise model for the execution\nengine does not yet exist. Arguably, our AQP formulation\nof Section 6.5 goes in the right direction, but it is mainly fo-\ncused on the reuse of results. Even further complexity arises\nwhen considering more diverse data models, such as time series\ndata, which come with their own semantics and algebra [34].\nNonetheless, we believe that a strong, formalized foundation\ncan guide the development of future accelerators for interactive\nData Science and their optimizations.\n10.2 Other Data Types\nMany real-world use cases require dealing with more than\none data type. For example, a sales prediction model might\nstart out with structured data gathered from a data warehouse,\nbut then might include features from the product description,\nreviews, or the quality of product pictures. Thus, we need tools\nwhich allow data scientists to deal efﬁciently with different data\ntypes from semi-structured data to videos. This is particularly\nchallenging as many of the state-of-the-art models for concep-\ntual tasks (e.g., video object detection, audio transcription, or\nentity discovery in text) are neural nets, which are notoriously\nhard to train in sub-seconds. However, techniques like transfer\nlearning [105] might make it possible, as it allows us to start\nfrom a good existing solution. Yet it is not without challenges,\nas the problem now becomes how to efﬁciently ﬁnd the most\nrelevant already-trained model and how to best integrate it into\nthe current analytical pipeline.\n10.3 Data Integration and Transformation\nData scientists commonly want to integrate several data\nsources, which often requires an intensive data cleaning and\nmunging [57] to get all the data into the right format and consol-\nidated so it can actually be analyzed. While there has been a lot\nof work in data integration [28], most of the existing work is not\nnecessarily result-oriented, is computationally very expensive,\nand/or still imposes a lot of scripting on users. For example, it\nwould be great if the system could highlight how potential data\nerrors might inﬂuence the individual operations on the screen\n(see also 8) and what concrete cleaning steps a user might want\nto take. Similarly, the process of transformation should be\nmade easier. Systems like Trifacta [2] or BoostClean [65] are\nalready big steps in the right direction; however, we believe\nthe integration with the Data Science analytics system must\n 2160\n\nbe much closer to better support the iterative Data Science life\ncycle. Ideally, users should be able to seamlessly move between\nthe different tasks of data collecting, cleaning and integration,\nexplorative analysis, and building and evaluating models, all in\na visual manner.\n10.4 Better Approximation Techniques for\nLegacy Systems\nAs outlined throughout the entire paper, approximate query\nprocessing and progressive results are key to enabling a ﬂuid\nuser experience. While many techniques have been developed\nover the years [13], we found several issues keeping them from\nbeings fully applicable for us. Most importantly, commercial\ndatabase systems currently barely support AQP, and it is unre-\nalistic to assume that existing legacy systems, such as a data\nwarehouse or distributed ﬁle system, can easily be replaced\nwith an AQP engine. This was one of the reasons we imple-\nmented our AQP techniques in the middle layer, IDEA, which\nthen connects to the existing legacy system without actually\nchanging it. However, this design imposes a whole new set of\nchallenges, including: (1) how IDEA can guarantee that it gets\na sufﬁciently randomized data stream out of the legacy system\n(see also [82]), (2) how IDEA can best take advantage of the fea-\ntures the legacy system provides (e.g., predicate push-downs)\nwhile maintaining fast response times for initial results, and\n(3) the possibility of leveraging the underlying system to better\ndeal with extreme values/outliers (e.g., if the data warehouse\nhas an index, can the index be partially used?).\nAnother exciting future direction for AQP techniques is to\nlearn models that represent the data distribution, as done in\nlearned indexes [63]. [63] shows how a CDF model can be\nused to enhance index structures with quite promising results.\nNow assuming a CDF model exists, for example in the form of\na learned index, the same model could now also be directly used\nto answer queries. In fact, our AQP formulation for enabling\napproximate result reuse from Section 6.5 already provides the\nfoundation to do that, as it treats results as random variables.\n10.5 Risk Control\nAs outlined in Section 8, democratizing Data Science should\nalso mean protecting users from common (and not-so-common)\nmistakes. With QUDE, we took a ﬁrst step in this direction,\nbut many open challenges remain. For example, there is a\ngrowing trend toward creating recommendation engines, which\npropose interesting visualizations (e.g., [103, 74, 32, 90, 88])\nor exploration steps [75], or automatically test for correlations\n[15]. Those systems are potentially checking thousands of hy-\npotheses in just a few seconds and are smoking guns disguised\nas water pistols. As a result, it is almost guaranteed that the\nsystem will ﬁnd something “interesting” regardless of whether\nthe observed phenomenon is statistically relevant. Even worse,\nwithout knowing how exactly the system tried to ﬁnd some-\nthing “interesting” (e.g., a visualization, correlation, etc.) and\nhow many correlations were tested, it is later often impossible\nto correct for the MHP. In some cases, like data polygamy, even\na holdout technique does not work.\nThe same also holds true for automatically ﬁnding machine-\nlearning models as done with Alpine Meadow, which can beregarded as a model recommendation engine. That is, train-\ning machine-learning models can be seen as a form of testing,\nwith the hypothesis being that a given model generalizes over\nunseen data, which is usually evaluated using techniques like\ncross-validation. However, the more model pipelines and hyper-\nparameters we test, the greater the chance that we ﬁnd a model\nthat just by chance works well over the cross-validation data\nsets (see [30] for a more detailed description). Hence, control-\nling the MHP for recommendation engines is a largely unsolved\nresearch question and we only recently made some interesting\nprogress on that front by using VC dimensions.\nSimilarly, there exist many more types of errors that a system\ncould warn the user about. For example, there has been a lot\nof excitement recently about automatically detecting bias [44,\n109, 17] or helping the user to understand results using lineage\n[86, 87]. We therefore believe that there is a lot of potential for\nbuilding a virtual Data Science Assistant, a tool that helps the\ndomain expert in the discovery and model-building process and\nprevents him from making mistakes.\n11. CONCLUSION\nWe speculate that in the near future many conference rooms\nwill be equipped with an interactive whiteboard, like the Mi-\ncrosoft Surface Hub, and that we can use such whiteboards to\nenable domain experts and data scientists to work together dur-\ning a single meeting to visualize, transform, and analyze even\nthe most complex data on the spot. We showed that democra-\ntizing Data Science requires us to completely rethink the full\nanalytical stack, from the interface to the “guts,” as well as take\na more holistic view of problems and bridge various research\ncommunities. With Northstar, we explored a ﬁrst system design\nfor true interactive Data Science and collaboration using inter-\nactive whiteboards. Furthermore, Northstar’s deployments in\nindustry and academia, as well as our various user studies [113,\n115, 114], have shown that Northstar helps to gain insights\nfaster and avoid problematic discoveries, and adds signiﬁcant\nvalue in practice.\n12. ACKNOWLEDGMENTS\nThe work presented in this paper was mainly done by my\nfantastic PhD students and PostDocs, and they truly deserve\nall the credit. Most notably, I would like to thank my Post-\nDoc Emanuel Zgraggen and my students Yeounoh Chung,\nAndrew Crotty, Alex Galakatos, Ani Kristo, Zeyuan Shang,\nLeonhard Spiegelberg, and Erfan Zamanian. Furthermore, I\nwas very lucky to have many fantastic collaborators, most no-\ntably Carsten Binnig and Eli Upfal. I would also like to thank\nAndy Van Dam (who probably started this whole endeavor\nby introducing me to Emanuel), as well as Benedetto Buratti,\nUgur C ¸etintemel, Cyrus Cousins, Philipp Eichmann, Yue Guo,\nLorenzo De Stefani, Stan Zdonik, Robert C. Zeleznik, and\nZheguang Zhao. Finally, I would like to thank Eugene Wu\nand Aditya Parameswaran for their valuable comments on this\npaper. This research was funded in part by the DARPA Award\n16-43-D3M-FP-040, NSF CAREER Award IIS-1453171, NSF\nAward IIS-1514491, NSF Award IIS-1562657, Air Force YIP\nAWARD FA9550-15-1-0144, and gifts from Google, Microsoft,\nIntel, and Oracle.\n 2161\n\n13. REFERENCES\n[1] Project tungsten: Bringing apache spark closer to bare metal.\nhttps://databricks.com/blog/2015/04/28/\nproject-tungsten-bringing-spark-closer-to-bare-metal.\nhtml . Accessed: 2018-07-15.\n[2] Trifacta. http://www.trifacta.com .\n[3] S. Acharya, P. B. Gibbons, and V . Poosala. Congressional Samples for\nApproximate Answering of Group-By Queries. In SIGMOD , pages\n487–498, 2000.\n[4] S. Acharya, P. B. Gibbons, V . Poosala, and S. Ramaswamy. The Aqua\nApproximate Query Answering System. In SIGMOD , pages 574–576,\n1999.\n[5] S. Agarwal, B. Mozafari, A. Panda, H. Milner, S. Madden, and I. Stoica.\nBlinkDB: Queries with Bounded Errors and Bounded Response Times\non Very Large Data. In EuroSys , pages 29–42, 2013.\n[6] D. Alabi and E. Wu. Pfunk-h: approximate query processing using\nperceptual models. In HILDA@SIGMOD , page 10, 2016.\n[7] Y . Benjamini et al. Controlling the false discovery rate. Journal of the\nRoyal Statistical Society, Series B , 57(5), 1995.\n[8] J. Bergstra and Y . Bengio. Random search for hyper-parameter\noptimization. Journal of Machine Learning Research , 13(Feb):281–305,\n2012.\n[9] C. Binnig, B. Buratti, Y . Chung, C. Cousins, T. Kraska, Z. Shang,\nE. Upfal, R. Zeleznik, and E. Zgraggen. Towards interactive curation &\nautomatic tuning of ml pipelines. In Proceedings of the Second\nWorkshop on Data Management for End-To-End Machine Learning ,\nDEEM’18, pages 1:1–1:4, New York, NY , USA, 2018. ACM.\n[10] C. Binnig, A. Crotty, A. Galakatos, T. Kraska, and E. Zamanian. The end\nof slow networks: It’s time for a redesign. PVLDB , 9(7):528–539, 2016.\n[11] P. Bohannon, W. Fan, M. Flaster, and R. Rastogi. A cost-based model\nand effective heuristic for repairing constraints by value modiﬁcation. In\nProceedings of SIGMOD , pages 143–154, 2005.\n[12] C. E. Bonferroni. Teoria statistica delle classi e calcolo delle\nprobabilita . Libreria internazionale Seeber, 1936.\n[13] S. Chaudhuri, B. Ding, and S. Kandula. Approximate query processing:\nNo silver bullet. In SIGMOD , pages 511–519, New York, NY , USA,\n2017. ACM.\n[14] Y . Cheah and B. Plale. Provenance quality assessment methodology and\nframework. J. Data and Information Quality , 5(3):9:1–9:20, 2015.\n[15] F. Chirigati, H. Doraiswamy, T. Damoulas, and J. Freire. Data polygamy:\nThe many-many relationships among urban spatio-temporal data sets. In\nSIGMOD , pages 1011–1025, 2016.\n[16] J. Chomicki and J. Marcinkowski. Minimal-change integrity\nmaintenance using tuple deletions. Information and Computation ,\n197(1-2):90–121, 2005.\n[17] Y . Chung, T. Kraska, N. Polyzotis, and S. E. Whang. Slice ﬁnder:\nAutomated data slicing for model validation. CoRR , abs/1807.06068,\n2018.\n[18] Y . Chung, S. Krishnan, and T. Kraska. A data quality metric (DQM):\nhow to estimate the number of undetected errors in data sets. PVLDB ,\n10(10):1094–1105, 2017.\n[19] Y . Chung, M. L. Mortensen, C. Binnig, and T. Kraska. Estimating the\nimpact of unknown unknowns on aggregate query results. In SIGMOD ,\npages 861–876, 2016.\n[20] Y . Chung, M. L. Mortensen, C. Binnig, and T. Kraska. Estimating the\nimpact of unknown unknowns on aggregate query results. ACM Trans.\nDatabase Syst. , 43(1):3:1–3:37, 2018.\n[21] T. Condie, N. Conway, P. Alvaro, J. M. Hellerstein, K. Elmeleegy, and\nR. Sears. MapReduce Online. In NSDI , pages 313–328, 2010.\n[22] G. Cong, W. Fan, F. Geerts, X. Jia, and S. Ma. Improving data quality:\nConsistency and accuracy. In Proc. of VLDB , pages 315–326, 2007.\n[23] M. Correll and M. Gleicher. Error bars considered harmful: Exploring\nalternate encodings for mean and error. TVCG , 20(12):2142–2151, 2014.\n[24] A. Crotty, A. Galakatos, K. Dursun, T. Kraska, C. Binnig, U. C ¸ etintemel,\nand S. Zdonik. An architecture for compiling udf-centric workﬂows.\nPVLDB , 8(12):1466–1477, 2015.\n[25] A. Crotty, A. Galakatos, K. Dursun, T. Kraska, U. C ¸ etintemel, and S. B.\nZdonik. Tupleware: ”big” data, big analytics, small clusters. In CIDR ,\n2015.\n[26] A. Crotty, A. Galakatos, E. Zgraggen, C. Binnig, and T. Kraska. Vizdom:\ninteractive analytics through pen and touch. PVLDB , 8(12):2024–2027,\n2015.[27] A. Crotty, A. Galakatos, E. Zgraggen, C. Binnig, and T. Kraska. The\ncase for interactive data exploration accelerators (IDEAs). In\nHILDA@SIGMOD , 2016.\n[28] A. Doan, A. Halevy, and Z. Ives. Principles of Data Integration . Morgan\nKaufmann Publishers Inc., San Francisco, CA, USA, 1st edition, 2012.\n[29] K. Dursun, C. Binnig, U. C ¸ etintemel, and T. Kraska. Revisiting Reuse in\nMain Memory Database Systems. In SIGMOD , pages 1275–1289, 2017.\n[30] C. Dwork, V . Feldman, M. Hardt, T. Pitassi, O. Reingold, and A. Roth.\nGeneralization in adaptive data analysis and holdout reuse. In NIPS ,\npages 2350–2358, 2015.\n[31] R. Ebenstein, N. Kamat, and A. Nandi. FluxQuery : An execution\nframework for highly interactive query workloads. In SIGMOD , pages\n1333–1345, 2016.\n[32] H. Ehsan, M. A. Sharaf, and P. K. Chrysanthis. Muve: Efﬁcient\nmulti-objective view recommendation for visual data exploration. In\nICDE , pages 731–742, May 2016.\n[33] P. Eichmann, C. Binnig, T. Kraska, and E. Zgraggen. Idebench: A\nbenchmark for interactive data exploration. CoRR , abs/1804.02593,\n2018.\n[34] P. Eichmann, A. Crotty, A. Galakatos, and E. Zgraggen. Discrete time\nspeciﬁcations in temporal queries. In CHI, pages 2536–2542, 2017.\n[35] M. El-Hindi, Z. Zhao, C. Binnig, and T. Kraska. Vistrees: fast indexes\nfor interactive data exploration. In HILDA@SIGMOD , 2016.\n[36] N. Elmqvist, A. V . Moere, H.-C. Jetter, D. Cernea, H. Reiterer, and\nT. Jankun-Kelly. Fluid interaction for information visualization.\nInformation Visualization , page 1473871611413180, 2011.\n[37] G. M. Essertel, R. Y . Tahboub, J. M. Decker, K. J. Brown, K. Olukotun,\nand T. Rompf. Flare: Native compilation for heterogeneous workloads\nin apache spark. CoRR , abs/1703.08219, 2017.\n[38] A. Even and G. Shankaranarayanan. Dual assessment of data quality in\ncustomer databases. J. Data and Information Quality , 1(3), 2009.\n[39] W. Fan, F. Geerts, J. Li, and M. Xiong. Discovering conditional\nfunctional dependencies. IEEE Trans. Knowl. Data Eng. ,\n23(5):683–698, 2011.\n[40] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg, M. Blum, and\nF. Hutter. Efﬁcient and robust automated machine learning. In NIPS ,\npages 2962–2970, 2015.\n[41] M. Feurer, J. T. Springenberg, and F. Hutter. Initializing bayesian\nhyperparameter optimization via meta-learning. In AAAI , pages\n1128–1135, 2015.\n[42] D. Fisher, R. DeLine, M. Czerwinski, and S. Drucker. Interactions with\nbig data analytics. interactions , 19(3):50–59, 2012.\n[43] A. Galakatos, A. Crotty, E. Zgraggen, C. Binnig, and T. Kraska.\nRevisiting reuse for approximate query processing. PVLDB ,\n10(10):1142–1153, 2017.\n[44] S. Galhotra, Y . Brun, and A. Meliou. Fairness testing: testing software\nfor discrimination. In ESEC/FSE , pages 498–510, 2017.\n[45] D. Golovin, B. Solnik, S. Moitra, G. Kochanski, J. Karro, and D. Sculley.\nGoogle vizier: A service for black-box optimization. In KDD , pages\n1487–1495. ACM, 2017.\n[46] M. G. G’Sell et al. Sequential selection procedures and false discovery\nrate control. Journal of the Royal Statistical Society , 78(2), 2016.\n[47] Y . Guo, C. Binnig, and T. Kraska. What you see is not what you get!:\nDetecting simpson’s paradoxes during data exploration. In\nHILDA@SIGMOD , pages 2:1–2:5, 2017.\n[48] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical\nLearning . Springer Series in Statistics. Springer New York Inc., New\nYork, NY , USA, 2001.\n[49] J. M. Hellerstein, R. Avnur, A. Chou, C. Hidber, C. Olston, V . Raman,\nT. Roth, and P. J. Haas. Interactive Data Analysis: The Control Project.\nIEEE Computer , pages 51–59, 1999.\n[50] J. M. Hellerstein, P. J. Haas, and H. J. Wang. Online Aggregation. In\nSIGMOD , pages 171–182, 1997.\n[51] S. Idreos, M. L. Kersten, and S. Manegold. Database Cracking. In CIDR ,\npages 68–78, 2007.\n[52] INRIA. scikit-learn: Machine learning in python.\nhttp://scikit-learn.org . Accessed: 2018-07-15.\n[53] J. P. A. Ioannidis. Why most published research ﬁndings are false. Plos\nMed, 2(8), 2005.\n[54] M. Ivanova, M. L. Kersten, N. J. Nes, and R. Goncalves. An\nArchitecture for Recycling Intermediates in a Column-Store. In\nSIGMOD , pages 309–320, 2009.\n 2162\n\n[55] C. M. Jermaine, S. Arumugam, A. Pol, and A. Dobra. Scalable\nApproximate Query Processing with the DBO Engine. In SIGMOD ,\npages 725–736, 2007.\n[56] N. Kamat, P. Jayachandran, K. Tunga, and A. Nandi. Distributed and\nInteractive Cube Exploration. In ICDE , pages 472–483, 2014.\n[57] S. Kandel, A. Paepcke, J. Hellerstein, and J. Heer. Wrangler: interactive\nvisual speciﬁcation of data transformation scripts. In CHI, 2011.\n[58] K. Keeton, P. Mehra, and J. Wilkes. Do you know your iq?: a research\nagenda for information quality in systems. SIGMETRICS Performance\nEvaluation Review , 37(3):26–31, 2009.\n[59] M. A. Khan, L. Xu, A. Nandi, and J. M. Hellerstein. Data tweening:\nIncremental visualization of data transforms. PVLDB , 10(6):661–672,\n2017.\n[60] A. Kirsch, M. Mitzenmacher, A. Pietracaprina, G. Pucci, E. Upfal, and\nF. Vandin. An efﬁcient rigorous approach for identifying statistically\nsigniﬁcant frequent itemsets. Journal of the ACM (JACM) , 59(3):12,\n2012.\n[61] A. Kohn, V . Leis, and T. Neumann. Adaptive execution of compiled\nqueries. In ICDE , 2018.\n[62] L. Kotthoff, C. Thornton, H. H. Hoos, F. Hutter, and K. Leyton-Brown.\nAuto-weka 2.0: Automatic model selection and hyperparameter\noptimization in weka. The Journal of Machine Learning Research ,\n18(1):826–830, 2017.\n[63] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for\nlearned index structures. In SIGMOD , pages 489–504, 2018.\n[64] T. Kraska, A. Talwalkar, J. C. Duchi, R. Grifﬁth, M. J. Franklin, and M. I.\nJordan. Mlbase: A distributed machine-learning system. In CIDR , 2013.\n[65] S. Krishnan, M. J. Franklin, K. Goldberg, and E. Wu. Boostclean:\nAutomated error detection and repair for machine learning. arXiv\npreprint arXiv:1711.01299 , 2017.\n[66] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar.\nHyperband: A novel bandit-based approach to hyperparameter\noptimization. arXiv preprint arXiv:1603.06560 , 2016.\n[67] L. Li, K. G. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar.\nHyperband: A novel bandit-based approach to hyperparameter\noptimization. Journal of Machine Learning Research , 18:185:1–185:52,\n2017.\n[68] T. Li, J. Zhong, J. Liu, W. Wu, and C. Zhang. Ease. ml: towards\nmulti-tenant resource sharing for machine learning workloads. PVLDB ,\n11(5):607–620, 2018.\n[69] Z. Liu and J. Heer. The effects of interactive latency on exploratory\nvisual analysis. InfoVis , 2014.\n[70] Z. Liu, B. Jiang, and J. Heer. imMens : Real-time visual querying of big\ndata. Comput. Graph. Forum , 32(3):421–430, 2013.\n[71] A. Lopatenko and L. Bravo. Efﬁcient approximation algorithms for\nrepairing inconsistent databases. In ICDE , pages 216–225, 2007.\n[72] G. Luo. A review of automatic selection methods for machine learning\nalgorithms and hyper-parameter values. Network Modeling Analysis in\nHealth Informatics and Bioinformatics , 5(1):18, 2016.\n[73] S. Macke, Y . Zhang, S. Huang, and A. G. Parameswaran. Adaptive\nsampling for rapidly matching histograms. PVLDB , 11(10):1262–1275,\n2018.\n[74] J. Mackinlay. Automating the design of graphical presentations of\nrelational information. ACM Trans. Graph. , 5(2):110–141, Apr. 1986.\n[75] T. Milo and A. Somech. Deep reinforcement-learning framework for\nexploratory data analysis. In Workshop on Exploiting Artiﬁcial\nIntelligence Techniques for Data Management (aiDM@SIGMOD) ,\npages 4:1–4:4, 2018.\n[76] Microsoft hololens.\nhttps://www.microsoft.com/en-us/hololens . Accessed:\n2018-07-15.\n[77] Mimic ii data set. https://mimic.physionet.org/ . Accessed:\n2018-07-15.\n[78] B. Myers, M. Oskin, and B. Howe. Compiling queries for\nhigh-performance computing. https://dada.cs.washington.\nedu/research/tr/2016/02/UW-CSE-16-02-02.pdf .\nAccessed: 2018-07-15.\n[79] F. Nagel, P. A. Boncz, and S. Viglas. Recycling in Pipelined Query\nEvaluation. In ICDE , pages 338–349, 2013.\n[80] J. Nielsen. Powers of 10: Time scales in user experience. Retrieved\nJanuary , 5:2015, 2009.\n[81] B. of Transportation Statistics. Bureau of transportation statistics.\nhttp://www.transtats.bts.gov , 2017. Accessed: 2017-10-21.[82] F. Olken. Random Sampling from Databases . PhD thesis, University of\nCalifornia at Berkeley, 1993.\n[83] S. Palkar, J. J. Thomas, D. Narayanan, P. Thaker, R. Palamuttam,\nP. Negi, A. Shanbhag, M. Schwarzkopf, H. Pirk, D. saman Amarasinghe,\nS. Madden, and M. Zaharia. Evaluating end-to-end optimization for data\nanalytics applications in weld. PVLDB , 11(9):1002–1015, 2018.\n[84] Y . Park, A. S. Tajik, M. J. Cafarella, and B. Mozafari. Database\nLearning: Toward a Database that Becomes Smarter Every Time. In\nSIGMOD , pages 587–602, 2017.\n[85] L. Pipino, Y . W. Lee, and R. Y . Wang. Data quality assessment.\nCommun. ACM , 45(4):211–218, 2002.\n[86] F. Psallidas and E. Wu. Provenance for interactive visualizations. In\nHILDA@SIGMOD , pages 9:1–9:8, 2018.\n[87] F. Psallidas and E. Wu. Smoke: Fine-grained lineage at interactive speed.\nPVLDB , 11(6):719–732, 2018.\n[88] X. Qin, Y . Luo, N. Tang, and G. Li. Deepeye: An automatic big data\nvisualization framework. Big Data Mining and Analytics , 1(1):75–82,\nMarch 2018.\n[89] K. Rong and P. Bailis. ASAP: prioritizing attention via time series\nsmoothing. PVLDB , 10(11):1358–1369, 2017.\n[90] S. F. Roth, J. Kolojejchick, J. Mattis, and J. Goldstein. Interactive\ngraphic design using automatic presentation knowledge. In SIGCHI ,\npages 112–117, 1994.\n[91] V . Sessions and M. Valtorta. Towards a method for data accuracy\nassessment utilizing a bayesian network learning algorithm. J. Data and\nInformation Quality , 1(3), 2009.\n[92] W. Shen. Data-driven discovery of models (d3m).\nhttps://www.darpa.mil/program/\ndata-driven-discovery-of-models . Accessed: 2018-07-15.\n[93] B. Shneiderman. Response time and display rate in human performance\nwith computers. ACM Computing Surveys (CSUR) , 16(3):265–285,\n1984.\n[94] T. Siddiqui, A. Kim, J. Lee, K. Karahalios, and A. Parameswaran.\nEffortless data exploration with zenvisage: An expressive and interactive\nvisual analytics system. PVLDB , 10(4):457–468, 2016.\n[95] E. R. Sparks, A. Talwalkar, M. J. Franklin, M. I. Jordan, and T. Kraska.\nTupaq: An efﬁcient planner for large-scale predictive analytic queries.\nCoRR , abs/1502.00068, 2015.\n[96] E. R. Sparks, A. Talwalkar, D. Haas, M. J. Franklin, M. I. Jordan, and\nT. Kraska. Automating model search for large scale machine learning. In\nSOCC , pages 368–380, 2015.\n[97] C. Stolte, D. Tang, and P. Hanrahan. Polaris: A system for query,\nanalysis, and visualization of multidimensional relational databases.\nIEEE Trans. Vis. Comput. Graph. , 8(1):52–65, 2002.\n[98] K. Tan, S. Goh, and B. C. Ooi. Cache-on-Demand: Recycling with\nCertainty. In ICDE , pages 633–640, 2001.\n[99] C. Thornton, F. Hutter, H. H. Hoos, and K. Leyton-Brown. Auto-weka:\nCombined selection and hyperparameter optimization of classiﬁcation\nalgorithms. In KDD , pages 847–855. ACM, 2013.\n[100] B. Trushkowsky, T. Kraska, M. J. Franklin, and P. Sarkar. Crowdsourced\nenumeration queries. In ICDE , pages 673–684, 2013.\n[101] B. Trushkowsky, T. Kraska, M. J. Franklin, and P. Sarkar. Answering\nenumeration queries with the crowd. Commun. ACM , 59(1):118–127,\n2016.\n[102] B. Trushkowsky, T. Kraska, M. J. Franklin, P. Sarkar, and\nV . Ramachandran. Crowdsourcing enumeration queries: Estimators and\ninterfaces. IEEE Trans. Knowl. Data Eng. , 27(7):1796–1809, 2015.\n[103] M. Vartak, S. Madden, A. G. Parameswaran, and N. Polyzotis. SEEDB:\nautomatically generating query visualizations. PVLDB ,\n7(13):1581–1584, 2014.\n[104] J. Wang, S. Krishnan, M. J. Franklin, K. Goldberg, T. Kraska, and\nT. Milo. A sample-and-clean framework for fast and accurate query\nprocessing on dirty data. In SIGMOD , pages 469–480, 2014.\n[105] K. Weiss, T. M. Khoshgoftaar, and D. Wang. A survey of transfer\nlearning. Journal of Big Data , 3(1):9, May 2016.\n[106] E. Wu, L. Battle, and S. R. Madden. The case for data visualization\nmanagement systems: Vision paper. PVLDB , 7(10):903–906, 2014.\n[107] E. Wu, F. Psallidas, Z. Miao, H. Zhang, L. Rettig, Y . Wu, and T. Sellam.\nCombining design and performance in a data visualization management\nsystem. In CIDR , 2017.\n[108] M. Yakout, A. K. Elmagarmid, J. Neville, M. Ouzzani, and I. F. Ilyas.\nGuided data repair. PVLDB , 4(5):279–289, 2011.\n 2163\n\n[109] K. Yang and J. Stoyanovich. Measuring fairness in ranked outputs. In\nSSDBM , pages 22:1–22:6, 2017.\n[110] E. Zamanian, C. Binnig, T. Kraska, and T. Harris. The end of a myth:\nDistributed transaction can scale. PVLDB , 10(6):685–696, 2017.\n[111] zdnet. Microsoft: Surface hub demand is strong; product is now in stock.\nhttps://www.cs.waikato.ac.nz/ml/weka/ .\n[112] K. Zeng, S. Agarwal, A. Dave, M. Armbrust, and I. Stoica. G-OLA:\nGeneralized On-Line Aggregation for Interactive Analysis on Big Data.\nInSIGMOD , pages 913–918, 2015.\n[113] E. Zgraggen, A. Galakatos, A. Crotty, J.-D. Fekete, and T. Kraska. How\nprogressive visualizations affect exploratory analysis. TVCG , 2016.\n[114] E. Zgraggen, R. Zeleznik, and S. M. Drucker. Panoramicdata: data\nanalysis through pen & touch. TVCG , 20(12):2112–2121, 2014.\n[115] E. Zgraggen, Z. Zhao, R. Zeleznik, and T. Kraska. Investigating the\neffect of the multiple comparisons problem in visual analysis. In CHI,\npage 479. ACM, 2018.\n[116] Z. Zhao, L. D. Stefani, E. Zgraggen, C. Binnig, E. Upfal, and T. Kraska.\nControlling false discoveries during interactive data exploration. In\nSIGMOD , pages 527–540, 2017.\n[117] Z. Zhao, E. Zgraggen, L. D. Stefani, C. Binnig, E. Upfal, and T. Kraska.\nSafe visual data exploration. In SIGMOD , pages 1671–1674, 2017.\n 2164",
  "textLength": 91056
}