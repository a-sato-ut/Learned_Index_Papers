{
  "paperId": "419bede6ee312577239d5a0fdcd707d052526c0c",
  "title": "When Are Learned Models Better Than Hash Functions?",
  "pdfPath": "419bede6ee312577239d5a0fdcd707d052526c0c.pdf",
  "text": "When Are Learned Models Better Than Hash Functions?\n(Extended Abstracts)\nIbrahim Sabek‚àó\nMIT CSAIL\nsabek@mit.eduKapil Vaidya‚àó\nMIT CSAIL\nkapilv@mit.eduDominik Horn\nTUM\ndominik.horn@tum.deAndreas Kipf\nMIT CSAIL\nkipf@mit.eduTim Kraska\nMIT CSAIL\nkraska@mit.edu\nABSTRACT\nIn this work, we aim to study when learned models are better hash\nfunctions, particular for hash-maps. We use lightweight piece-wise\nlinear models to replace the hash functions as they have small\ninference times and are sufficiently general to capture complex\ndistributions. We analyze the learned models in terms of: the model\ninference time and the number of collisions. Surprisingly, we found\nthat learned models are not much slower to compute than hash\nfunctions if optimized correctly. However, it turns out that learned\nmodels can only reduce the number of collisions (i.e., the number\nof times different keys have the same hash value) if the model is\nable to over-fit to the data; otherwise, it can not be better than an\nordinary hash function. Hence, how much better a learned model\nis in avoiding collisions highly depends on the data and the ability\nof the model to over-fit. To evaluate the effectiveness of learned\nmodels, we used them as hash functions in the bucket chaining and\nCuckoo hash tables. For bucket chaining hash table, we found that\nlearned models can achieve 30% smaller sizes and 10% lower probe\nlatency. For Cuckoo hash tables, in some datasets, learned models\ncan increase the ratio of keys stored in their primary locations by\naround 10%. In summary, we found that learned models can indeed\noutperform hash functions but only for certain data distributions\nand with a limited margin.\nPVLDB Reference Format:\nIbrahim Sabek, Kapil Vaidya, Dominik Horn, Andreas Kipf, and Tim\nKraska. When Are Learned Models Better Than Hash Functions? (Extended\nAbstracts). PVLDB, 14(1): XXX-XXX, 2021.\ndoi:XX.XX/XXX.XX\n1 INTRODUCTION\nHashing is a fundamental operation in computer science and com-\nmonly used in databases [ 15]. They are mainly used to acceler-\nate point queries, perform joins and grouping, etc. (e.g., [ 2,7]). In\nhash tables, a key is mapped to a location in constant time (i.e.,\nùëÇ(1)). Compared to the traditional tree-structured main-memory\nindexes, hash tables have been proven to be much faster for point\nqueries. Meanwhile, a lot of data structures and algorithms are\nrecently being enhanced by learned models (e.g., [ 11,12]). These\nlearned structures can outperform their traditional counterparts\non practical workloads. Along this line of work, the authors of [ 11]\n‚àóBoth authors have equal contributions and their names are sorted alphabetically.\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 14, No. 1 ISSN 2150-8097.\ndoi:XX.XX/XXX.XXintroduced the idea of using learned models instead of hash func-\ntions, and supported that by some empirical evidence. In this paper,\nwe aim to study in more detail when learned models are better\nhash functions, particular for applications like hash-maps. We pri-\nmarily consider piece-wise linear models in our analysis as they\nhave small inference times and are sufficiently general to capture\ncomplex distributions.\nOur study investigates the performance of learned models in\nterms of: the number of collisions , and the computation time . A colli-\nsion between two keys occurs when they have the same hash value.\nSome hash functions are extremely fast to compute, yet, they might\nsuffer from a considerable collision rate in some scenarios (e.g.,\nMultiply-shift [ 4]). In general, there are known theoretical lower\nbounds on the number of collisions achieved by hash functions. We\nobserved that learned models might be able to do better than these\nlower bounds and outperform hash functions. In particular, it turns\nout that the amount of collisions for learned models is dependent\non the data distribution.\nRegarding computation time, we empirically found that learned\nmodels are slower to compute than most hash functions due to\nthe cache miss overhead from randomly accessing the model‚Äôs pa-\nrameters. However, with the help of vectorization and prefetching-\noptimized inter-task parallelism (e.g., AMAC [ 10]), the learned\nmodels computation time can come quite close to its hashing coun-\nterpart (around 2 ns difference using models with moderate size).\nTo show the effect of using learned models within hashing ap-\nplications, we built bucket chaining and Cuckoo hash tables using\ntwo efficient models, namely RMI [ 11] and RadixSpline [ 9], instead\nof hash functions. Typically, in Cuckoo hashing, a single hash func-\ntion is used to extract two hash sequences. In our experiments, we\ncomputed one hash sequence using the learned model and the other\nusing the hash function. We empirically evaluate the performance\nof these altered hash tables with various real-world and synthetic\ndatasets. For bucket chaining, we found that learned models can\nachieve 30% smaller hash tables and 10% lower probe latency. For\nCuckoo hashing, in some datasets, learned models can increase the\nratio of keys stored in their primary locations (primary key ratio)\nby around 10% and a small lookup time benefit in the probe phase.\n2 BACKGROUND\nHash Functions and Tables. Murmur [ 15] and XXH3 [ 3] are\namong the most widely-used hash functions, which have good\nbalance between computation time and collision rates. They are\nimplemented with arithmetic (e.g., multiply, add) and logical (e.g.,\nshift, XOR) operations. However, XXH3 is specifically designed\nfor streaming data. AquaHash [ 16] is another popular hash func-\ntion that leverages Advanced Encryption Standard (AES) instruc-\ntions [ 5]. In general, hashing schemes for handling collisions arearXiv:2107.01464v1  [cs.DB]  3 Jul 2021\n\ncategorized into two main categories: chaining and open addressing.\nBucket chaining [ 2] is a standard hash table implementation that\nfollows the chaining scheme. It contains a set of ùëõbuckets, where\neach bucket has a pre-allocated array of ùë†entries. On an insert,\nonce a collision occurs, the item is inserted in the current available\nentry in its corresponding bucket. If the current bucket is already\nfilled up, a new one is created, pre-allocated and chained to it. For\nopen addressing scheme, Cuckoo hash table [ 14] has become the\nrecent state-of-the-art. Every item has two possible locations: its\nprimary and its secondary bucket. When inserting an item and its\nprimary bucket is full, it gets placed into its secondary bucket. If\nthe secondary bucket is also full, a random item is kicked from the\nbucket and is placed into its alternative location (balanced kicking).\nIn contrast, biased kicking [ 8] prefers kicking items that reside\nin their secondary buckets. The idea behind this is to increase the\nratio of items in their primary buckets (primary ratio) and hence im-\nprove performance for positive lookups. Typically, Cuckoo hashing\nis implemented with two independent hash functions.\nLearned Models. Recently, the idea of using learned models to\npredict the location of keys in datasets has gained a great attention\nin the database community [ 11]. RMI [ 11] was the first proposed\nindex that uses multi-stage learned models. In RMI, the root model\ngives an initial prediction of the CDF for a specific key. Then, this\nprediction is recursively refined by more accurate models in the\nsubsequent stages. Interestingly, the authors of [ 11] also discussed\nthe idea of using CDF-based learned models as order-preserving\nhash functions, which is the main scope of this paper. An interesting\nindex that followed RMI, namely RadixSpline [ 9], employs a radix\ntable to quickly find the two spline points that approximate the CDF\nfor a specific key. Then, linear interpolation between the retrieved\nspline points is used to locate the key. In this paper, we only focus on\npiece-wise linear models that are built using a set of line segments,\nwhere each segment is represented by a slope and an intercept.\nBoth RMI and RadixSpline can be considered as piece-wise linear\nmodels, which are just trained/created in a different way.\n3 ANALYSIS OF LEARNED MODELS\n3.1 Can Learned Models Cause Less Collisions?\nIn this section, we first characterize collisions and then use this to\nidentify/analyze factors affecting collisions for learned models and\nhash functions. This analysis helps us to characterize situations\nwhere learned models outperform hash functions.\nNotation. We consider the task of mapping ùëÅkeys toùëÅloca-\ntions for ease of analysis. ùë•0,ùë•1,...is the sorted array of ùëÅkeys\n(ùë•ùëñ<=ùë•ùëñ+1) andùë¶0,ùë¶1,.., whereùë¶ùëñ‚àà[0,ùëÅ‚àí1], is the correspond-\ning sorted array of output values ( ùë¶ùëó=ùëì(ùë•ùëñ)) where f is a learned\nmodel or hash function) such that ùë¶ùëñ<=ùë¶ùëñ+1. Note thatùë•ùëñdoes not\nnecessarily relate to ùë¶ùëñ,ùë•ùëñ‚Äôs andùë¶ùëñ‚Äôs are just sorted versions of the\noriginal input keys and output locations. For learned models, ùë¶ùëñ‚Äôs\nare continuous values and the precise output location is the closest\ninteger toùë¶ùëñ‚Äôs. The sorted output values generate a set of gaps\nùëî1,ùëî2,...such thatùë¶ùëñ=\u0010√çùëñ\nùë°=1ùëîùë°\u0011\n+ùë¶0. These gaps form a distribu-\ntionùê∫with a probability density function (PDF) ùëìùê∫. The discussion\nand analysis in the rest of this section support the following points:\n‚Ä¢Collisions are dependent on the gaps between consecutive\nsorted output values ( ùëîùëñ‚Äôs).‚Ä¢For piece-wise linear models, the number of collisions is\ndependent on the key distribution, specifically the gaps be-\ntween consecutive sorted keys ( ùë•ùëñ‚àíùë•ùëñ‚àí1). Higher variation\nin the distribution of gaps between keys leads to more col-\nlisions. Having more linear models improves the accuracy\nbut may not reduce the collisions.\n‚Ä¢Collisions for a good hash function are independent of the\ninput key distribution (The distribution of ùë•ùëñ‚Äôs).\nCharacterizing Collisions. If two keys are mapped to the same\nlocation, then there is a collision. The key insight regarding colli-\nsions is that the collisions depend on the gaps between consecutive\noutput values ( ùë¶ùëñ‚àíùë¶ùëñ‚àí1). If the gap between two consecutive out-\nput values is greater than one (ùë¶ùëñ‚àíùë¶ùëñ‚àí1‚â•1), then they would\ndefinitely be placed in separate locations. On the other hand, if the\nthe gap is smaller than one (ùë¶ùëñ‚àíùë¶ùëñ‚àí1‚â§1), they may be mapped to\nthe same location depending on the location boundary. In addition,\nthe smaller the gap value the more the probability of the keys falling\nin the same location.\nThe gap values are constrained by the condition that the sum of\nall the gaps should be less than (ùëÅ‚àí1)1, and thus, the mean gap\nvalue turns out to be less than or equal to one ( ùê∏[ùê∫]‚â§1). Ideally,\nwe would want all the gaps to be exactly equal to one as this leads\nto zero collisions and also satisfies the constraint. Qualitatively\nspeaking, any increase in the variance of gap distribution ùê∫leads\nto an increase in the number of gaps below value one and thus, a\nsubsequent increase in the number of collisions.\nCollisions for Linear Models. The output distribution for linear\nmodels is dependent on the input distribution. Linear operations\nscale and offset the input values to obtain the output. For sorted\ninput values ( ùë•0,ùë•1,ùë•2,..), a simple linear model ( ùë¶=ùëö‚àóùë•+ùëè) will\njust scale the gaps of the input ( ùëîùëñ=ùë¶ùëñ+1‚àíùë¶ùëñ=(ùë•ùëñ+1‚àíùë•ùëñ)‚àóùëö). For\npiece-wise linear models, the gap distribution of the output values\nùê∫is a scaled version of the input. The scaling will be such that the\nmean ofùê∫is less than equal to one ( ùê∏[ùê∫]‚â§ 1). If the input gap\ndistribution has higher variance, this would be propagated to ùê∫,\nleading to more collisions.\nNext, we qualitatively argue why more models do not necessar-\nily reduce the number of collisions. Suppose the input data was\ngenerated using a gap distribution ùêªwith corresponding PDF ùëìùêª.\nPiece-wise linear models would simply scale different ranges of the\ninput and thus, the corresponding output gap distribution would\njust be a scaled version of ùëìùêª. Increasing the number of models\ndoes not alter the gap distribution of the output values and thus, the\nnumber of collisions stays the same. In an extreme case, when the\nnumber of models is close to the number of keys, then the collisions\nwould be low but the space overhead would make the structure\npractically unusable.\nHere, we visualize the gap distribution of the output values for\nvarious datasets and the corresponding proportion of empty slots.\nWe used piece-wise linear models for various datasets and obtained\nthe outputùë¶‚Ä≤\nùëñùë†(CDF) values for them. In Figure 1, we show the PDF\nof the gap distribution and the proportion of empty slots for three\nreal datasets from [ 13] and a synthetic uniform one. Clearly, the\ngap distribution is much more predictable for wiki than for uniform,\nfbandosm.wiki has a gap distribution concentrated more towards\n1Sum of gaps is:√çùëÅ‚àí1\nùë°=1(ùë¶ùë°‚àíùë¶ùë°‚àí1)=ùë¶ùëÅ‚àí1‚àíùë¶0‚â§ùëÅ‚àí1\n2\n\nFigure 1: Gap distribution and effect on collisions\none and so ends up having the least number of empty slots. osm\ntends to have a lot of gaps concentrated towards zero and ends up\nwith the most empty slots. We provide a formal analysis for the\ncollisions and its relation with gaps distribution in Appendix A.\nCollisions for Hash Functions. In case of a good hash function\n(e.g., Murmur), the output values will be uniformly distributed in the\nrange[0,ùëÅ‚àí1]irrespective of the input distribution. Therefore, the\ngap distribution of the output values is a fixed distribution which\ncorresponds to the uniform case in Figure 1.\nSummary. If the input keys are generated from a distribution, then\nthe CDF of the distribution maps the data uniformly randomly in\nthe range[0,1]. Hence, the CDF will behave as an order-preserving\nhash function in a hash table. A learned model that approximates\nthis underlying distribution would only be as effective as a hash\nfunction in terms of collisions. In this case, over-fitting to the data\nis necessary to reduce the collisions. If the data is generated in an\nincremental time series fashion ( ùë•0,ùë•1=ùë•0+ùëî0,ùë•2=ùë•1+ùëî1,....),\nthe predictability of the gaps determines the amount of collisions.\nIn certain cases, like the wiki distribution, a learned model can lead\nto fewer collisions. Auto generated IDs with some deletions are the\nother common case where learned models can beat hash functions.\n3.2 Can Learned Models be Computationally as\nFast as Traditional Hash Functions?\nIn this section, we first present the computation bottleneck of using\nlearned models as hash functions. Then, we discuss the opportunity\nof alleviating such bottleneck by using vectorization (i.e., Single\nInstruction Multiple Data (SIMD) instructions) and prefetching-\noptimized inter-task parallelism techniques (e.g., AMAC [10]).\nCache Misses Overhead. The computation of traditional hash\nfunctions is fast. It usually includes the execution of arithmetic,\nlogical, and shifting operations (e.g., Multiply-shift [ 4], and Mur-\nmur [ 15]). In contrast, using learned models, like RMI, as a hash\nfunction incurs higher latency. The main reason is that, although\nthe inference computation of these models (which is basically the\nhashing computation in our case) is completely based on arithmetic\noperations (e.g., add, multiply, max), there is an additional overhead\nin accessing the model parameters (e.g., intercepts and slopes) that\nwill be used during the computation. This overhead significantly\nincreases if the model size becomes large as its parameters will not\ncompletely fit in the cache, and randomly accessing them from the\nmemory will incur many cache misses.Performance Gain via SIMD. Interestingly, vectorizing the com-\nputation in learned models is more efficient than vectorizing some\nhash functions as long as the model parameters are kept warm in the\ncache . To backup this claim, we micro-benchmarked the throughput\nof hashing 128 million 64-bit integer keys using a single-threaded\nAVX512 SIMD implementation for both Murmur [ 1] and 2-levels\nRMI model [ 11], running on a machine with Intel(R) Xeon(R) pro-\ncessors (Skylake architecture). We made sure that all models‚Äô pa-\nrameters are kept in the cache by building an RMI with a total\nof 5 linear models only (1 root model, and 4 models in the sec-\nond level). Our results showed that the hashing throughputs for\nvectorized RMI and vectorized Murmur are 1000 and 800 million\nkeys/sec, respectively. This is expected because, with ignoring the\neffect of parameters‚Äô cache misses , the inference computation (i.e.,\nhashing) in RMI heavily relies on fast comparison (e.g., min/max)\nand fused instructions2(e.g., fmadd), each has a throughput of 2\ninstructions/cycle [ 6]. On the other hand, 60% of the total instruc-\ntions needed in the Murmur computation have a throughput of 1\ninstruction/cycle or less, such as logical shift (1 instruction/cycle)\nand multiplication (0.66 instruction/cycle).\nPerformance Gain via AMAC. As previously mentioned, the su-\nperiority of vectorized learned models quickly diminishes when we\nhave large models, which is a typical case in real settings. In this\ncase, the model parameters are frequently accessed from memory\nand not from cache. Even if some of the requested parameters from\na vectorized instruction hit in cache, the instruction cannot proceed\nuntil cache misses of the other parameters in the vector are resolved.\nObviously, direct software prefetching is not a feasible solution to\nthis issue, and will completely stall the performance, because the\nmodel parameters require immediate memory access. Therefore,\nwe propose to hide the cache misses latency by combining the vec-\ntorized learned models with a widely-used prefetching-optimized\ninter-task parallelism technique, namely AMAC [ 10]. This helps in\nmaking the overall latency of vectorized learned models very close\nto traditional hash functions as shown in our evaluation (Section 4).\nAppendix B shows our proposed batch-oriented hash function that\ncombines the benefits of SIMD and AMAC with learned models.\n4 EVALUATION\nFor the experiments, we use three real datasets from [ 13], namely\nwiki,osm, and fb, in addition to three variations of a synthetic\nsequential dataset with different x% elements removed randomly\n(x={0, 1, 10}). Each real or synthetic dataset has around 200 million\n64-bit keys. We de-duplicate the real datasets before using them.\nWe use AquaHash [ 16], XXH3 [ 3] and Murmur [ 1] with fast modulo\nreduction3as traditional hash functions and two efficient learned\nmodels: RMI [11] and RadixSpline [9].\nComputation Time. Figure 2(a) shows the median throughput for\nhashing the sequential dataset with 10% removed elements using\ntraditional hash functions and non-vectorized learned models, while\nvarying the count of line segments used in each model. As expected,\ntraditional functions are much better than non-vectorized learned\nmodels, even with small sizes. The throughput of learned models\ndecreases significantly for large sizes due to the increased number of\n2Intel(R) Xeon(R) Gold 6230 processor has two physical AVX512 FMA units.\n3Modulo reduction is based on efficient integer division (https://libdivide.com).\n3\n\nn/a 100\n103\n105\n101\n103\n105\n107\nModel count0.02.55.0Keys per second√ó108\nAquaHash\nMurmurXXH3\nRadixSplineRMI(a) Median keys per second\nseq\ngap 1%gap 10%wikifbosm\nDataset0%33%66%100%Empty slotsRadixSpline\nMurmur (b) Empty slots (percentage)\nFigure 2: Throughput and collisions comparisons.\nModel\nCountNon-Vect.\nMurmur (ns)Vect.\nMurmur (ns)Non-Vect.\nRMI (ns)Vect.\nRMI (ns)\n10 2.4 1.9 10 4\n1032.4 1.9 13 4\n1052.4 1.9 25 8\n1072.4 1.9 112 22\nTable 1: Computation time of vectorized RMI and Murmur.\ncache misses when accessing the model parameters. Table 1 shows\nthe performance of AMAC-based vectorized versions of both RMI\nand Murmur hashing for the same dataset. As shown, with 103\nmodels, the performance gap between non-vectorized RMI and\nMurmur is substantial (around 10 ns difference), however, using the\nAMAC-based vectorization reduces this gap to be 2 ns only. Note\nthat, at very large models (e.g., 107), RMI becomes much slower\nthan Murmur, even with vectorization.\nCollisions. Figure 2(b) shows the amount of collisions for RadixS-\npline against Murmur hashing on various datasets. RMI is omitted\nas it has similar results to RadixSpline. Here, we mapped ùëÅele-\nments intoùëÅslots and report the proportion of empty slots. The\ndashed line is the theoretically expected value for true uniform\nrandom hash functions. As shown in the figure, for many datasets,\nlearned models indeed have less empty slots than hash functions\n(i.e., less collisions). However, for fbandosmdatasets, the models\nmake the collisions worse. This confirms our analysis in Section 3.1.\nBucket Chaining Hash Tables. Bucket chaining hash tables deal\nwith collisions by creating linked lists for the keys mapped to the\nsame location. When retrieving a key, we traverse the linked list\nuntil we find the key. With increased collisions, the space needed\nfor the chained hash tables increases. Figure 3(a) shows the effect of\nusing different hash functions and RadixSpline as a representative\nlearned model when building bucket chaining hash tables with\ndifferent payload and bucket sizes. We observe that RadixSpline can\nlead to less probe times for all of the datasets, except fbandosmones.\nMoreover, larger payloads lead to larger cache miss penalties, and\nhence with increasing payload sizes, hash functions take slightly\nmore time than learned models.\nCuckoo Hash Tables. Having a high primary key ratio reduces\nthe amount of unnecessary lookups, as one avoids going to the\nsecond location, and hence improves the probe time. We show the\neffect of replacing one of the used multiple hash functions by a\nlearned model. We use a Cuckoo Hash with 2 hash functions, load\nfactor of 1, bucket size of 8, and two kicking strategies; balanced\nand biased [8]. As shown in Figure 3(b), using any two traditional\nhash functions consistently achieves primary key ratios of 62% and\n83%, for biased and balanced kicking, respectively, which are close\n(a) \n(b) Figure 3: (a) Bucket chaining hash table probe times for vary-\ning payload sizes and slots per bucket. (b) Primary key ratio\nand probe times for various kicking strategies and load fac-\ntors.\nseq\ngap 1% gap 10%wikifbosm200250500750 RadixSpline (cuckoo)\nMurmur (cuckoo)\nRadixSpline (chained)\nMurmur (chained)\nDatasetProbe time per key [ns]\n(a) 16 bytes payloads\nseq\ngap 1% gap 10%wikifbosm250300600800 RadixSpline (cuckoo)\nMurmur (cuckoo)\nRadixSpline (chained)\nMurmur (chained)\nDatasetProbe time per key [ns] (b) 64 bytes payload\nFigure 4: Probe times comparison between Cuckoo and\nbucket chaining hashing.\nto theoretically optimal. However, we observe that using learned\nmodels, e.g., RadixSpline, along with both kicking strategies can\nlead to better primary key ratio for all of the datasets, except fband\nosmones. With biased kicking, learned models get a much better\nprimary key ratio which leads to lower cache misses and thus a\nslightly better probe time.\nCombined Probe Times. Figure 4 shows the probe times achieved\nby employing each of RadixSpline and Murmur hashing inside both\nbucket chaining and Cuckoo hash tables on various datasets. We\nused bucket size of 4 for both tables. Also, biased kicking is used\nfor constructing the Cuckoo table. As shown, for all datasets except\nfbandosm, bucket chaining with RadixSpline is the best strategy.\nCuckoo tables are generally slower than their chained counterparts.\n4\n\nAlgorithm 1 Function HashViaLearnedModel (Instancesùë†, Keys\nùëñùëõùëùùë¢ùë° , KeysNumùëÅ, Output‚Ñéùëéùë†‚Ñéùëíùë† )\n1:ùëëùëúùëõùëí‚Üê0 /* Flag to end hashing computation */\n2:ùë†ùë°ùëéùë°ùëí‚ÜêInitializeFSMInstances (ùë†) /* Initializeùë†instances of a finite state machine*/\n3:whileùëëùëúùëõùëí<ùë†do\n4:ùëò= (ùëò==ùë†) ? 0 :ùëò\n5: switchùë†ùë°ùëéùë°ùëí[ùëò].ùë†ùë°ùëéùëîùëí do\n6: case P: /* Predict using the root model, and prefetching next model parameters */\n7: ifùëñ<ùëÅthen\n8: ùë†ùë°ùëéùë°ùëí[ùëò].ùë£ùëòùëíùë¶‚ÜêLoadKeyVec( ùëñùëõùëùùë¢ùë° ,ùëñ)\n9: ùëùùëüùëíùëë‚ÜêPredictNextLevelModelIndVec (ùë†ùë°ùëéùë°ùëí[ùëò].ùë£ùëòùëíùë¶ ,ùëüùëúùëúùë° )\n10: ùë†ùë°ùëéùë°ùëí[ùëò].ùë†ùë°ùëéùëîùëí =ùêª\n11: ùëñ+=ùëä/*ùëäis the vector width (Assume ùëÅmodùëä) = 0*/\n12: PrefetchModelParamsVec (ùëùùëüùëíùëë ,ùë†ùë°ùëéùë°ùëí[ùëò].ùë£ùëùùëéùëüùëéùëöùë† )\n13: else\n14: ùë†ùë°ùëéùë°ùëí[ùëò].ùë†ùë°ùëéùëîùëí =ùê∑\n15: ++ùëëùëúùëõùëí\n16: end if\n17: case H: /* Perform actual hashing */\n18: ‚Ñéùëéùë†‚Ñéùëíùë†‚ÜêHashKeysVec (ùë†ùë°ùëéùë°ùëí[ùëò].ùë£ùëòùëíùë¶ùë† ,ùë†ùë°ùëéùë°ùëí[ùëò].ùë£ùëùùëéùëüùëéùëöùë† )\n19: ùë†ùë°ùëéùë°ùëí[ùëò].ùë†ùë°ùëéùëîùëí =ùëÉ/* Initiate prefetching for a new set of keys in ùëÉ*/\n20:end while\nA COLLISION ANALYSIS\nIf we assume that ùëîùëñ‚Äôs are generated from an independent and iden-\ntically distributed (iid) variable with probability density function\nùëìùê∫(ùë•), then the expected number of empty slots ùëí, after mapping\nkeys to their hash outputs, is given by the formula below:\nE[ùëí]=ùëÅ¬∑‚à´1\n0(1‚àíùë•)¬∑ùëìùê∫(ùë•)ùëëùë•\nThis equation was derived using the fact that if the gap value ùë•is\nless than one, then with probability ùë•, a location boundary would\nfall between the two consecutive values. This is because the location\nboundaries are separated by unit values and the probability of a\nrandom boundary falling in a gap of size ùë•isùë•. The probability that\nno boundaries fall in a gap of size ùë•is(1‚àíùë•)and(1‚àíùë•)‚àóùëìùê∫(ùë•)ùëëùë•\nrepresents the proportion collisions for gap values from [ ùë•,ùë•+ùëëùë•].\nThis quantity is then integrated from 0to1, as consecutive keys\nwith gaps beyond one don‚Äôt collide.\nB ALGORITHM FOR HASHING VIA\nLEARNED MODELS\nAlgorithm 1 shows the pseudo code of our proposed batch-oriented\nhash function that combines AMAC with vectorized learned models.The core idea is sample: for a vector of keys, we map the hashing\ncomputation generated by 2-levels learned model into an FSM with\ntwo states, where the first state (Lines 6 to 16) uses the root model\nto predict the index of the second level model, and prefetches its\nparameters, and the second state (Lines 17 to 19) performs the actual\nhashing using the prefetched model parameters. The algorithm\nkeeps interleaving multiple running instances of the FSM till it\nfinishes hashing all input keys. The logic in each state is completely\nimplemented with SIMD instructions.\nREFERENCES\n[1]Austin Appleby. Murmurhash3 64-bit finalizer. https://code.google.com/p/\nsmhasher/wiki/MurmurHash3.\n[2]Cagri Balkesen, Jens Teubner, Gustavo Alonso, and M. Tamer √ñzsu. Main-memory\nhash joins on multi-core CPUs: Tuning to the underlying hardware. In ICDE ,\npages 362‚Äì373, 2013.\n[3] Yann Collet. xxHash. https://cyan4973.github.io/xxHash/.\n[4]Martin Dietzfelbinger, Torben Hagerup, Jyrki Katajainen, and Martti Penttonen.\nA Reliable Randomized Algorithm for the Closest-Pair Problem. Journal of\nAlgorithms , 25(1):19‚Äì51, 1997.\n[5]Shay Gueron. Intel Advanced Encryption Standard (AES) New Instructions\nSet. https://www.intel.com/content/dam/doc/white-paper/advanced-encryption-\nstandard-new-instructions-set-paper.pdf.\n[6]Intel Intrinsics Guide. https://software.intel.com/sites/landingpage/IntrinsicsGuide/.\nhttps://software.intel.com/sites/landingpage/IntrinsicsGuide/.\n[7]Christopher Jonathan, Umar Farooq Minhas, James Hunter, Justin Levandoski,\nand Gor Nishanov. Exploiting Coroutines to Attack the \"Killer Nanoseconds\".\nProc. VLDB Endow. , 11(11):1702‚Äì1714, 2018.\n[8]Andreas Kipf, Damian Chromejko, Alexander Hall, Peter A. Boncz, and David G.\nAndersen. Cuckoo index: A lightweight secondary index structure. Proc. VLDB\nEndow. , 13(13):3559‚Äì3572, 2020.\n[9]Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. RadixSpline: A Single-Pass Learned Index.\nInProc. of aiDM@SIGMOD , 2020.\n[10] Onur Kocberber, Babak Falsafi, and Boris Grot. Asynchronous Memory Access\nChaining. Proc. VLDB Endow. , 9(4):252‚Äì263, 2015.\n[11] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. The\nCase for Learned Index Structures. In SIGMOD , page 489‚Äì504, 2018.\n[12] Ani Kristo, Kapil Vaidya, Ugur √áetintemel, Sanchit Misra, and Tim Kraska. The\nCase for a Learned Sorting Algorithm. In SIGMOD , page 1001‚Äì1016, 2020.\n[13] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra,\nAlfons Kemper, Thomas Neumann, and Tim Kraska. Benchmarking Learned\nIndexes. Proc. VLDB Endow. , 14(1):1‚Äì13, 2020.\n[14] Rasmus Pagh and Flemming Friche Rodler. Cuckoo Hashing. Journal of Algo-\nrithms , 51(2):122‚Äì144, 2004.\n[15] Stefan Richter, Victor Alvarez, and Jens Dittrich. A Seven-Dimensional Analysis\nof Hashing Methods and Its Implications on Query Processing. Proc. VLDB Endow. ,\n9(3):96‚Äì107, 2015.\n[16] J. Andrew Rogers. AquaHash. https://github.com/jandrewrogers/AquaHash/.\n5",
  "textLength": 28564
}