{
  "paperId": "a68555a8d702ad5add8ffacb8b5d654df5c2f30c",
  "title": "Towards instance-optimized data systems",
  "pdfPath": "a68555a8d702ad5add8ffacb8b5d654df5c2f30c.pdf",
  "text": "MIT Open Access Articles\nTowards instance-optimized data systems\nThe MIT Faculty has made this article openly available. Please share\nhow this access benefits you. Your story matters.\nCitation: Kraska, Tim. 2021. \"Towards instance-optimized data systems.\" Proceedings of the \nVLDB Endowment, 14 (12).\nAs Published: 10.14778/3476311.3476392\nPublisher: VLDB Endowment\nPersistent URL: https://hdl.handle.net/1721.1/143734\nVersion: Final published version: final published article, as it appeared in a journal, conference \nproceedings, or other formally published context\nTerms of use: Creative Commons Attribution-NonCommercial-NoDerivs License\n\n\nTowards instance-optimized data systems\nTim Kraska\nMassachusetts Institute of Technology\nCambridge, MA\nkraska@mit.edu\nABSTRACT\nIn recent years, we have seen increased interest in applying machine\nlearning to system problems. For example, there has been work on\napplying machine learning to improve query optimization, index-\ning, storage layouts, scheduling, log-structured merge trees, sorting,\ncompression, and sketches, among many other data management\ntasks. Arguably, the ideas behind these techniques are similar: ma-\nchine learning is used to model the data and/or workload in order to\nderive a more efficient algorithm or data structure. Ultimately, these\ntechniques will allow us to build ‚Äúinstance-optimized‚Äù systems: that\nis, systems that self-adjust to a given workload and data distribution\nto provide unprecedented performance without the need for tun-\ning by an administrator. While many of these techniques promise\norders-of-magnitude better performance in lab settings, there is\nstill general skepticism about how practical the current techniques\nreally are.\nThe following is intended as a progress report on ML for Systems\nand its readiness for real-world deployments, with a focus on our\nprojects done as part of the Data Systems and AI Lab (DSAIL) at MIT.\nBy no means is it a comprehensive overview of all existing work,\nwhich has been steadily growing over the past several years not\nonly in the database community but also in the systems, networking,\ntheory, PL, and many other adjacent communities.\nPVLDB Reference Format:\nTim Kraska. Towards instance-optimized data systems. PVLDB, 14(12):\n3222 - 3232, 2021.\ndoi:10.14778/3476311.3476392\n1 INTRODUCTION\nDatabase systems have a long history of carefully selecting efficient\nalgorithms, e.g., a merge vs a hash-join, based on data statistics. Yet,\nmost databases are general-purpose systems that are not purpose-\nbuilt for a specific workload or data distribution. For this reason,\nthe architects and developers of a database system have to make\ncompromises when building the system. They have to decide the\ntype of index structures to implement, what scheduling algorithms\nto use, how to organize data on disk as well as in-memory, and so\non. All of these decisions come with their own trade-offs, which\nthey try to balance. As a result, the final system will likely perform\nwell but not achieve the best possible performance for any given\napplication.\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 14, No. 12 ISSN 2150-8097.\ndoi:10.14778/3476311.3476392In contrast, if they would design a system from scratch in C++ for\njust one particular application for one single customer and maybe\neven just one single situation, such as the transaction processing\nfor Walmart‚Äôs online shop during Black Friday, they would prob-\nably design the system very differently than a general purpose\ntransactional database system. Unfortunately, building a system\nlike a database is very time-consuming and expensive and involves\nhundreds of engineers, making it impossible to design a system for\nevery application and user from scratch.\nBut what if the system could automatically self-adjust to the\ndata and workload, to provide near-optimal performance? This is\nthe core idea behind a range of recently proposed techniques to\nautomatically adjust the system for a given application without\nhuman intervention. Here we refer to this idea as an instance-\noptimized system in analogy to instance-optimal algorithms [ 32].\nMachine learning plays an important role in this area, as it provides\nwell-known techniques to ‚Äúlearn‚Äù something about the (expected)\ndata, workload, or hardware behavior, which can then be used\nto optimize the system. However, instance-optimizations might\nnot require machine learning; often more traditional optimization\ntechniques suffice. This article outlines three different degrees of\ninstance-optimization and their readiness for industry.\n2 KNOB AND DESIGN TUNING\nThe simplest form of instance-optimization automatically tunes\nsystem parameters. Many systems provide a range of parameters,\ni.e., knobs, to tune the system, mostly in order to achieve better\nperformance for a particular application or hardware. For example,\nthe buffer pool size is often an important parameter in database\nsystems, which should be tuned depending on the available mem-\nory and workload. Similarly, database systems offer a range of\ndesign options, such as indexes or materialized views, which can\nsignificantly reduce the data-processing time.\nTraditionally, an administrator would tune the parameters of a\nsystem and create indexes or materialized views. Unfortunately, this\nin itself can be a time-consuming task and not every organization\ncan afford to pay a skilled database administrator. Moreover, work-\nloads tend to change, making the tuning process a never-ending\ntask.\nNot surprisingly, there is a long history of research to auto-\nmate this process [ 2,3,13‚Äì15,20,30,40,67,98,109] and several\ntechniques have been successfully deployed into production by\nMicrosoft [ 15] and Oracle [ 21], among many others. Tradition-\nally, these techniques require a (representative) training work-\nload provided by an administrator and address only one prob-\nlem (e.g., index selection) at a time. More recent techniques in\nthis area often use modern machine learning techniques, such\nas reinforcement learning, to be more proactive, make changes\n3222\n\n\nmore quickly (online), and/or address the problem more holistically\n[4, 5, 22, 41, 60, 61, 61, 62, 94, 94, 101].\nYet, the goal of knob and design tuning is to assist, and sometimes\nreplace, the database administrator while not making fundamental\nchanges to the database system itself. Hence, the tuning potential is\nrestricted to the knobs and design choices the systems have today.\nThis leads to the next category of instance-optimized components,\nwhich go beyond the more traditional algorithms and often deeply\nembed a model in the algorithm itself. Thus, for the purpose of\nthis paper, we consider a reinforcement-learning-based schedul-\ning algorithm to be an instance-optimized component, whereas a\nreinforcement-leaning-based model to select the best indexes is\nconsidered a knob and design tuning technique.\nStill, it should be pointed out that Pavlo et al. argue in [ 94] that\nin order to create ‚Äúself-driving‚Äù database systems, one also needs\nto reconsider many of the traditional design choices. In particular,\nthe authors argue that we require a new system design that allows\nconfiguration to be changed much more quickly without requir-\ning a restart of the database and potentially also to provide more\n‚Äúknobs‚Äù for automization. This ultimately blurs the line between\nknob/design tuning and what we refer to as instance-optimized\nsystems, as discussed further below.\n3 INSTANCE-OPTIMIZED COMPONENTS\nMore recently a wide range of learning-enhanced algorithms and\ndata structures to build instance-optimized components have been\nproposed. These techniques go far beyond knob tuning by deeply\nembedding models or optimization into the algorithms, data struc-\ntures, or entire component designs. In ‚ÄúThe Three Pillars of Machine\nProgramming‚Äù [ 39] these techniques fall mainly under the inven-\ntion and adaptation pillar. In the following, we highlight research\nresults on only two instance-optimized database components, in-\ndexing and query optimization, with a focus on techniques out\nof DSAIL [ 26] at MIT. However, many other components have\nbeen ‚Äúinstance-optimized,‚Äù including scheduling [ 72,73,103], sort-\ning [58,59], joins [ 38], and compression [ 12,51] (see [ 28] for a more\ncomprehensive list of papers).\n3.1 Building Instance-Optimized Components\nGenerally, we identify three different ways to develop instance-\noptimized data structures, algorithms, and components.\nDesign continuum: The first approach is to use a model to\nsynthesize or configure a traditional algorithm. For example, in\n[48] Stratos et al. proposed a design continuum that unifies major\ndistinct data structures/algorithms under the same model. The core\nidea is that every data structure consists of a few fundamental de-\nsign concepts and that different optimization techniques can be used\nto compose several of these concepts into an instance-optimized\ndata structure for a given data set and workload [ 50]. However, so\nfar these design concepts consist entirely of traditional techniques.\nMoreover, making all the design concepts compatible with each\nother‚Äîincluding ways to transition between them‚Äîpresents an\nimmense research and engineering effort.\nML-enhanced algorithms: Another approach to building inst-\nance-optimized algorithms, data structures, and components is bymeans of an oracle. That is, the algorithm assumes an oracle, typi-\ncally a machine learning model, that is capable of making a predic-\ntion that is imperfect but relevant to the problem; this prediction is\nthen used to derive a more efficient algorithm. Key to this approach\nis the model‚Äôs ability to capture something about the problem more\nefficiently than a data agnostic approach would. For example, a\nmodel that predicts the position of a key inside a sorted array can\nbe used to improve the efficiency of a range index [ 56] and sorting\nalgorithm [ 58]. In contrast, a model that predicts whether a key is in\na given set can be used to improve Bloom filters [ 56], and a model\nthat predicts the frequency of an item in a set can improve the\nestimation quality of counting sketches [ 29,47], whereas a model\nthat predicts the job execution time can be used to derive a more\nefficient scheduling algorithm [ 81,82]. Here we can distinguish\nbetween two cases: oracles that predict something about the future\n(e.g., the job arrival or execution time ) and oracles that ‚Äúpredict‚Äù (or\nestimate) something about known data (e.g., a empirical CDF model\nto predict the rank of an item). Interestingly, while it is generally\nimportant that prediction-based models generalize, overfitting can\nbe a good thing if the model only needs to capture something about\nthe known data more compactly.\nThe general category of ML-enhanced algorithms is also some-\ntimes referred to as algorithms with predictions [ 82], oracle-based\nalgorithms, learning-enhanced algorithms, or learned algorithms.\nMoreover, the idea of embedding models to improve existing algo-\nrithms opens up a whole new range of interesting opportunities\nand research challenges for a wide range of existing problems [ 82].\nFull-model replacement: For some restricted problems it is\npossible to entirely replace an algorithm or data structure through a\nmodel. This is most often the case if the original algorithm is already\na heuristic to solve an NP-hard problem and does not require strict\nguarantees (e.g., a guarantee to find the best solution). Examples\ninclude cardinality estimation and scheduling algorithms, among\nother problems.\nInterestingly, these three approaches are not exclusive to each\nother. For example, one could consider oracle-based approaches\nas part of a design continuum. Similarly, oracle-based algorithms\noften already use code synthesis to build more efficient models.\nFor example, in [ 56] the authors propose combining traditional\nB-trees with regression models to achieve better CDF estimates. It\nstill remains to be shown whether it is better to include a design\ncontinuum as part of a single dominant oracle-based algorithm or\nif oracle-based ideas are better as a part of a design continuum.\n3.2 Learned Indexes\nIn our SIGMOD 2018 paper ‚ÄúThe Case for Learned Index Structures‚Äù\n[55,56], we showed that traditional algorithms and data structures,\nparticularly B-trees, hash-maps, and Bloom filters, can be enhanced\nby learned models with significant space and performance benefits\nwhile providing the exact same semantic guarantees. To make this\npossible, we invented ways to combine probabilistic models with\ntraditional data structures and algorithms and a new type of model\nstructure, called the recursive model index (RMI), which can have\nnano-second inference time.\nWhile the results were promising, the paper also excited a lot of\ncontroversy because of its highly unintuitive result that machine\n3223\n\n\nlearning can improve data structures with (sub-)linear complexity.\nMost notably, three individual blog posts by Prof. Neumann, Prof.\nMitzenmacher, and a group at Stanford raised the question whether\nother more traditional approaches could outperform learned in-\ndexes. Moreover, we made several simplifying assumptions (e.g.,\nwe focused on read-only in-memory workloads) and the paper did\nnot show how a learned index would impact an end-to-end system.\nThis in turn raised the question of how significant the impact would\nbe in real-world systems. The lack of an open-source evaluation\nfurther contributed to the uncertainty.\n3.2.1 A progress report. Fortunately, many researchers around the\nglobe started to expand on our initial results to address many of the\nopen research questions and limitations (see [ 28] for a collection of\nthese papers). For example, in a joint evaluation with TU Munich\n[27,75], we showed that learned indexes are indeed much smaller\nwhile providing better or similar performance than traditional state-\nof-the-art indexes [ 27,75]. Similar results were also found by other\n(independent) research groups [ 69]1and several micro experiments\nin various follow-up papers (e.g., [ 24,36,107]). We open-sourced all\nour implementations and created a public index performance leader-\nboard [ 27], allowing for more fair comparisons going forward. Very\nexciting is also the theoretical result by Ferragina et al. [ 33,34],\nwhich proves why learned indexes are more space efficient under\ncertain data/workload assumptions.\nNote that [ 27] also contains a comparison against newer index\nvariants such as the radix-based binary search and the Compact\nHisTree (CHT) [ 16], a highly read-optimized and compressed data\nstructure. While the author of [ 16] argues that the Compact HisTree\nis not a learned index structure, we do consider it one, as it explic-\nitly models the histogram of the data rather than relying solely\non binary decisions to navigate a tree structure. Regardless of its\nclassification, [ 27] shows that RBS and CHT are only able to slightly\noutperform RMIs on very few datasets and that only at a very large\nindex size, often surpassing the size of the original dataset, render-\ning its impact on real-world systems more questionable (see below).\nThis is due to the fact that both approaches require a radix table to\nbe efficient; in the extreme the radix table would store a pointer for\neach possible key in the domain, not just all keys in the data set.\nLearned indexes have also been integrated into full disk-based\nsystems. Interestingly, the evaluations of this work have shown\nthat the advantage of learned indexes on disk-based systems often\ncomes through their smaller index size, which can be orders-of-\nmagnitude smaller, rather than the faster lookup time. For example,\nGoogle integrated learned index structures into their BigTable sys-\ntem, where it was able to improve the throughput by up to 2x [ 1].\nThis performance benefit can be attributed to the fact that the\nsmaller index makes it possible to save an IO request and the associ-\nated de-compression cost. Similarly, research work at the University\nof Wisconsin showed how learned indexes can improve ‚Äúlookup\nperformance by 1.23x-1.78x as compared to state-of-the-art produc-\ntion LSMs‚Äù [ 18], in this case due to both the index size and faster\nlookup performance.\n1Note that the evaluation in [ 69] uses the same traditional baselines as in [ 75], but\nre-implements the RMI-model-based index instead of using the one from [ 75], that\nseems less tuned than the other variants, with a training technique which is more\nsensitive to outliers.Many of the other previous limitations, such as the lack of update\nsupport, have also been addressed and, to date, we have counted\nover 50 new variants of learned indexes (see [ 28] for a list of papers).\nFor example, Alex [ 24], Bourbon [ 18], and PGM [ 36] extend the\nidea of learned index to support inserts. Interestingly, it has also\nbeen found that while overfitting is often a good property for read-\nonly workloads, supporting writes might require models that better\ngeneralize and consider potential future updates [ 42]. Similarly, dif-\nferent learning approaches for the models and cost functions have\nbeen proposed. Whereas our original RMI structure was trained\ngreedily top-down, FittingTree [ 37], RadixSpline (RS), and the PGM\nindex [ 36] are all trained bottom-up. FittingTree [ 37] uses linear\nsplines [ 89] to approximate the CDF of the data, whereas RS com-\nbines the idea of splines with an additional radix-based lookup table\nto achieve faster lookup performance. In [ 31] the authors show that\nthe right cost function for training should be the log-error, because\nof the subsequent exponential or binary search process. There exists\nalso several extension for learned indexes on string data [ 104,110].\nFinally, more recent projects explored how to adjust learned in-\ndexes for modern hardware, such as multi-core environments [ 107],\nNVM [66], and RDMA [111].\n3.2.2 Learned Bloom filters and hash-maps. Similarly there has\nbeen exciting follow-up work on the idea of Learned Bloom Fil-\nters. In [ 80] Prof. Mitzenmacher proposed the Sandwiched Learned\nBloom Filter (SBF) as an improved learned Bloom filter variant\non our original work [ 56]. It combines two Bloom filters (not just\none) with a model that predicts whether a key is in a given set.\nLater, Dai and Shrivastava proposed Adaptive Learned Bloom Filter\n(Ada-BF) [ 19], which leverages the certainty of the prediction to\nachieve smaller Bloom filter sizes. In contrast, our most recent work,\nPartitioned Learned Bloom Filters (PLBF) [ 108], can be regarded as\na generalization with provable guarantees of Sandwiched Learned\nBloom Filter (SBF) and Adaptive Learned Bloom Filter (Ada-BF) [ 19].\nHowever, there has been other work on improving Learned Bloom\nFilters, including improvements using meta-learning techniques\n[97] or learned Bloom filters for data streams [ 64]. Moreover, we\nalso theoretically and experimentally evaluated (RMI) models as\npotentially better hash functions for hash-maps [100].\nWe also addressed the questions raised around learned hash-\nmaps in [ 100], which theoretically and experimentally evaluates\nwhen learned models can and cannot outperform hash functions to\nbuild more efficient hash-maps.\n3.2.3 Multi-dimensional indexes and storage layouts. One of the\nmost exciting directions for learned index structures are techniques\nto index multi-dimensional data to improve upon index structures\nsuch as R- or KD-trees. With the Flood project [ 84] we demonstrated\nhow we can extend the idea of learned indexes to multi-dimensional\nindexes, which outperform alternative general-purpose data struc-\ntures by orders of magnitude by automatically adjusting to the\ndata distribution and workload through models. With Tsunami\n[25], we later extended the Flood technique to create the first self-\noptimizing in-memory storage manager, which also takes advantage\nof correlation within the data. Interestingly, in order to achieve the\norders-of-magnitude improvement in lookup performance benefits,\nboth techniques also require optimization of the storage layout;\nthey both are designed to be clustered indexes rather than being\n3224\n\n\nused as secondary indexes. Hence, we consider Flood and Tsunami\nstrongly related to techniques that help to automatically partition\nthe data based on the workload, such as Schism [ 17], QD-tree [ 115],\nor MTO [ 23]. Yet, traditionally automatic data partition algorithms\nfall more into the knob tuning category, as they do not fundamen-\ntally change the components of the database, whereas Flood and\nTsunami go further and introduce more opportunities for optimiza-\ntion (i.e., degrees of freedom) than a traditional system would. In\nfact, QD-tree [ 115] and MTO [ 23] were explicitly designed to be\nused with a traditional data-warehouse database.\n3.3 Learned Query Optimization\nQuery optimization remains one of the most challenging problems\nin data management systems. In general, a query optimizer has four\ncomponents: (1) a cost model, (2) a cardinality estimation model, (3)\ntransformation rules, and (4) a search strategy. As the name implies,\nthe cost model‚Äôs goal is to estimate the cost (e.g., latency) for a given\nquery plan. Here the cost model might depend on the hardware,\nthe schema, and even the state of the database (e.g., what pages are\nin the cache), but is generally independent of the data itself.2The\ncardinality estimation model depends on the data but is otherwise\nindependent of the workload. The transformation rules and search\nstrategy are independent of either. The rules define what rewrites\nare possible and how they can be combined, whereas the search\nstrategy determines the way the ‚Äúbest‚Äù plan out of all possible plans\nis found.\nGiven the importance and the complexity of the problem, there\nhave been several attempts to build ‚Äúlearned‚Äù query optimizers,\nmost often focusing on the cardinality estimation component. For\nexample, one of the earliest applications of learning to improve\nthe cardinality estimate model was Leo [ 105], which used succes-\nsive runs of similar queries to adjust histogram estimators. More\nrecent approaches [ 53,63,93,106,112] have used deep learning to\nlearn cardinality estimations or query costs in a supervised fashion.\nOther work [ 7‚Äì9] present a query-driven approach to cardinality\nestimation, using techniques such as self-organizing maps. Unsu-\npervised approaches, based on Monte Carlo integration, have also\nbeen proposed [ 116,117]. In [ 43], the authors present a scheme\ncalled CRN for estimating cardinalities via query containment rates.\nWhile all of these works demonstrate improved cardinality esti-\nmation accuracy (potentially useful in its own right, as in [ 7‚Äì9]),\nthey do not provide evidence that these improvements lead to bet-\nter query plans. In fact, it is possible that while the cardinality\nestimation accuracy (or Q-Error) improves overall, queries become\nslower [ 86,87]. Ortiz et al. [ 91] showed that certain learned cardi-\nnality estimation techniques may improve mean performance on\ncertain data sets, but tail latency is not evaluated.\nMore recent work has tried to address this shortcoming by train-\ning the cardinality estimation model while more fully considering\nits impact on the cost model. For example, Negi et al. [ 88] showed\nhow prioritizing training on cardinality estimations that have a\nlarge impact on query performance can improve estimation mod-\nels. Furthermore, [ 57,77] showed that, with sufficient training,\nreinforcement learning-based approaches could find plans with\n2This might not be true in cases where the data has a lot of variable length attributes\nand/or requires complex UDFs.lower costs (according to the PostgreSQL optimizer). In [ 86,87],\nwe proposed a new loss function, Flow-Loss, that approximates\nthe optimizer‚Äôs cost model, which it uses to optimize explicitly for\nbetter query plans. At the heart of Flow-Loss is a reduction of query\noptimization to a flow routing problem on a certain ‚Äúplan graph‚Äù\nin which different paths correspond to different query plans. The\nresult shows that, across different architectures and databases, a\nmodel trained with Flow-Loss improves the plan costs and query\nruntimes despite having worse estimation accuracy than a model\ntrained with Q-Error.\nWith Neo [ 79], we took this even further and used reinforcement\nlearning to directly improve the query latency by addressing cardi-\nnality estimation, cost models, and the search strategy all at once\n(Neo still assumes a given set of transformation rules). Thus, Neo\nis one if not the first end-to-end learned query optimizer. It uses\na value network based on tree convolution, employs row vectors\nto represent query predicates, and applies learning from demon-\nstration to shorten Neo‚Äôs training time. As a result, Neo could learn\noptimization strategies that were competitive with commercial\nsystems after 24 hours of training. However, none of these tech-\nniques are capable of handling changes in schema, data, or query\nworkload, and database administrators expressed concerns about\nrobustness and how much they could trust a completely learned\nquery optimizer.\nMotivated by these difficulties, we more recently introduced Bao\n(theBanditoptimizer) [ 76]. Bao takes advantage of the wisdom built\ninto existing query optimizers by providing per-query optimization\nhints. The core idea behind Bao is that we do not try to learn an\noptimizer from scratch: instead, we take an existing optimizer (e.g.,\nPostgreSQL‚Äôs optimizer) and learn when to activate (or deactivate)\nsome of its features on a query-by-query basis. In other words,\nBao seeks to build learned components on top of existing query\noptimizers in order to enhance query optimization, rather than\nreplacing or discarding traditional query optimizers altogether. This\nalso allows Bao to be used as an advisor for a database administrator.\nIn [76], we showed that Bao can offer both reduced costs and better\nperformance compared with commercial systems deployed in the\ncloud. We later informally extended the evaluation in [ 74] to include\nVertica, Azure Synapse, and Redshift in the evaluation, showing cost\nreductions of up to 25%. Finally, in [ 85], together with Microsoft,\nwe evaluated the benefits of Bao for Big Data workloads, showing\nup to 90% runtime latency savings for complex queries.\n3.4 Other Algorithms\nMachine learning techniques have been applied to a wide vari-\nety of other problems relevant for database systems. For example,\nreinforcement-learning-based techniques were proposed for (job)\nscheduling [ 72,73,103], garbage collection [ 52], and managing\nelastic clusters [ 65,92] (see also [ 71,102]). Similarly, oracle-based\nalgorithms were suggested for sorting [ 58], caching [ 68], count-min\nsketching [ 47], data compression [ 12,51], and optimizing the trans-\naction execution policy [ 95], among many other problems. While\nthere are already too many learning-enhanced algorithms to list\nhere, the whole area is also evolving quickly. Just to mention two in-\nteresting projects out of DSAIL: Vaidya et al. recently proposed the\nuse of a CDF model to improve the space efficiency of range filters\n3225\n\n\n(currently under submission), and Kristo et al. recently proposed\na new LearnedSort algorithm [ 59], which is more robust against\nduplicates and outperforms the best sorting algorithms we were\nable to find by up to 30% in sorting throughput on a wide range of\ndata sets, even when the model training time is included.\n3.5 Future Research Opportunities\nThe entire field of instance-optimized or learning-enhanced algo-\nrithms is evolving extremely rapidly across various communities.\nFor example, the idea of the learned index alone has also been ap-\nplied to other areas, including network package classification [ 99],\nDNA sequence search [ 46], longest prefix matches [ 44], and in-\nverted indexes [ 90,114], and follow-on has appeared in database\n(e.g., VLDB[ 113]), systems (e.g., OSDI [ 111]), machine learning (e.g.,\nNeurIPS [19]), networking (e.g., SIGCOMM [99]), and theory (e.g.,\nTheor. Comput. Sci. [34]) conferences. For an overview the reader\nis referred to a recent tutorials [ 6,49,70], surveys [ 35], and our\ncollection of papers [ 28]. However, a lot of interesting research\nquestions and opportunities remain and it will be exciting to see\nhow the entire field of ML for Systems will develop.\nIn the case of learned indexes, there is a range of open opportuni-\nties for study. For example, while [ 1,18] show that learned indexes\ncan improve the performance of large-scale disk-based systems, it\nwould be exciting to see to what degree learned indexes would im-\npact an in-memory OLTP and HTAP system or embedded database,\nlike DuckDB [ 96]. Integrating a learned index into those systems\nwould also require investigation into more advanced recovery tech-\nniques (e.g., is it possible to simplify ARIES [ 83]) and techniques to\nefficiently support range-locks using learned indexes).\nAs [75] shows, learned indexes are not beneficial for all data sets.\nIn particular, if the data set contains a lot of duplicates, traditional\nindexes might be better. Similarly, some models and training meth-\nods are not robust against outliers. Hence, an interesting research\ndirection is to make learned indexes more robust by considering\noutliers and duplicates more closely. Here, CDFShop [ 78] already\ntakes a step in the right direction, but many more improvements\nare possible. Especially interesting is how to better combine models\nwith the traditional techniques used in Tries and B-trees and explore\nthe implications. Similarly, it would be interesting to explore new\nend-to-end learning techniques. Current approaches either build\nan index bottom-up or top-down, often considering the placement\nof data as a secondary optimization goal. However, if we could\nconsider it as a holistic optimization problem, that co-optimizes\nthe model structure, model types, and data placement, it can be\nassumed that we could achieve faster lookup times while further\nreducing the index size.\nYet, to us the most exciting directions are applications of learned\nindexes in related fields and for multi-dimensional data. For exam-\nple, our latest results on LISA [ 46] build on and extend FM-index,\nwhich is the state-of-the-art technique widely deployed in genomics\ntools. Experiments with human, animal, and plant genome data sets\nindicate that LISA achieves up to 2.2ùë•and10.8ùë•speedups over the\nstate-of-the-art FM-index-based implementations for exact search\nand super-maximal exact match (SMEM) search, respectively. Sim-\nilarly, we believe that we are just at the beginning of rethinkingthe way we build multi-dimensional indexes and co-optimize in-\ndex structures with storage layouts for an entire workload with\ncomplex access paths.\nIn the same respect, there are myriads of open research chal-\nlenges for learned query optimizers. While Bao [ 76] provides a\npractical approach to improving existing query optimizers, it adds\nsignificant query optimization overhead. For long-running ana-\nlytical queries the overhead often plays no important role, but it\ncertainly would for short-running queries, as found in transactional\nworkloads. On the other hand, end-to-end learned query optimizers,\nsuch as Neo, still suffer from the long training time and even small\nchanges in the schema (or index configuration) might require the\nentire model to be retrained. A promising recent result by Hilprecht\nand Binnig [ 45] proposes the use of zero-shot learning to improve\nupon Neo [ 79]. This represents a first step toward making it pos-\nsible to transfer a learned model from one database configuration\nto another. However, the approach still has severe limitations, as\nthe cost model might still remember cardinality corrections and/or\nmight not be able to correct wrong cardinalities, depending on the\namount of training data and setup. Moreover, database vendors\nusually require that query optimizers are robust rather than having\nthe best possible performance. For example, a query that ran fast\nand suddenly runs significantly slower might result in customer\ncomplaints, regardless of performance. One interesting idea in that\nregard is to develop techniques that keep the query latency an-\nchored at some reference point, while passing on resource savings\nto the cloud provider. Many other related research challenges re-\nmain, including how to reduce query optimization times or how to\nbetter balance exploration and exploitation in a production setting,\nmodel management, explainability, etc.\nObviously, this only outlines a few selected remaining research\nchallenges and even does not touch upon other components, such\nas scheduling, distributed query processing, among other areas.\n4 INSTANCE-OPTIMIZED SYSTEMS\nThe previous section described how different components of a data-\nbase management system can be instance-optimized to ultimately\nprovide better performance. However, it remains an open question\nhow different learned components can actually be combined to\nbuild an entire instance-optimized system. This is the question we\nare exploring with SageDB [ 54]. While it is still too early to draw\nany conclusions about the expected performance benefits on actual\napplications and analytic workloads, initial results using real-world\nworkload traces on our current prototype are promising. Moreover,\nthey have already revealed some interesting trade-offs and insights\ninto how different learned components might affect each other.\nThe following is a brief update on the development of SageDB, as\nwell as a selected set of early lessons learned and future research\nopportunities.\n4.1 SageDB\nWhen we started to develop SageDB, the first question we had to\naddress was if we really wanted to build a database system from\nscratch. While building an entire system from the ground up would\ngive us the greatest flexibility, it would also increase the time before\nthe system would become stable and feature complete enough that\n3226\n\n\n0 50 100 150\nLatency (s)PostgreSQL (standard tuning)PostgreSQL (expert tuning)Cloud DB X (standard tuning)Cloud DB X (expert tuning)SageDB\n0 1000 2000 3000\nSize (GB)(a) Data Warehouse Workload\n0 200 400\nLatency (s)PostgreSQL (standard tuning)PostgreSQL (expert tuning)Cloud DB X (standard tuning)Cloud DB X (expert tuning)SageDB\n0 200 400\nSize (GB) (b) Bitcoin workload\nFigure 1: Preliminary SageDB results on two workloads. Results include block-layout optimization and automatic replica-\ntion, encoding, and partial materialization. However, the workload does NOT include any joins, nor does the system support\n(yet) learned query optimization, multi-dimensional indexes, or memory hierarchy optimizations. Hence, there exists a lot of\nopportunity for further optimizations.\nother users would be willing to try it out. At the same time, we\nconsidered the prospect of early deployments essential to this line\nof work.\nWe thus decided to build SageDB as an accelerator for Post-\ngreSQL. The core idea is that we leverage the concept of foreign\ndata wrappers in PostgreSQL, which allows SageDB to take control\nof managing the data of one or more tables. Further, external data\nwrappers allow us to push down queries, even those containing\njoins, to SageDB without changing the code of PostgreSQL. That\nway we were able to build a system that not only is fully com-\npatible to PostgreSQL but also provides an easier migration path\nfor existing PostgreSQL deployments. Users are able to explicitly\ndefine which tables should be managed by SageDB vs natively by\nPostgreSQL. At the same time, the loose connection between Post-\ngreSQL and SageDB allows us to build SageDB over time as part of\nPostgreSQL until it is ready to stand on its own. The main down-\nside is that we will have the PostgreSQL and foreign data wrapper\noverhead, forcing us to focus on long-running analytical workloads\nfor the moment.\nAs of July 2021, we have a first prototype of PostgreSQL with\nthe SageDB accelerator running, which is able to instance-optimize\nthe data layout of a single table in a multi-dimensional fashion, in-\ncluding partial materialization, replication, and encoding selection.\nMoreoever, SageDB uses the separation of storage and compute\nthat we proposed in 2008 [ 11] for cloud database systems and is\nnow commonly found in commercial offerings like Snowflake. That\nis, the data is stored in large blocks on cloud storage services like\nS3 and brought into a SageDB instance on demand. This allows\nSageDB to actually work with data, which goes beyond the capacity\nof a single compute node, and it is possible to have several SageDB\ninstances running on the same data for scalability.3However, in\ncontrast to current traditional cloud database systems, SageDB au-\ntomatically adjusts how the data is stored, indexed, and replicated\nin blocks based on the workload and data.\nMoreover, the system uses automatically partial materialization,\na concept similar to materialized views but on a finer granularity.\nWhile materialized views ‚Äúcache‚Äù the entire query result, SageDB‚Äôs\npartial materialization technique is able to build pre-aggregates,\nsuch as running sums, aggregations, and counts, on a block or at an\neven more fine-grained level. Then at run-time SageDB determines\nfor each query what partial materialized results can be used to avoid\n3Note that our current prototype does not yet allow a query to be processed in parallel\nby several machines. We do plan to add distributed query processing in the future.scanning certain parts of the data to speed up the query processing\ntime. Obviously, the benefits of partial materialization care highly\ndependent on the workload, available amount of memory, and the\namount of work required to keep them up to date. This is exactly\nwhere instance-optimization comes into play.\nSimilarly, SageDB provides support to (partial) replicate and reor-\nganize each replica for performance and select the most appropriate\ncompression technique for the data based on the data and workload.\nHowever, the current prototype does not yet integrate a learned\nquery optimizer; neither does it currently support joins, nor does\nit perform automatic optimization for the entire storage hierarchy\n(Cloud storage, SSD, NVM, vs Memory): all these are features on\nour roadmap.\nFigure 1 shows some preliminary results of SageDB against Post-\ngreSQL and a commercial cloud datawarehouse (Cloud DB X) for\ntwo workloads. The workloads are derived from real-world data\nand traces, but focus entirely on queries over a single table. The\nBitcoin dataset had a size of 500GB; the datewarehouse dataset was\n1TB in size. The server in this experiment had 32 vCPUs and 244\nGB of RAM. All SageDB data was stored on S3, whereas the Cloud\nDB used their own storage, and PostgreSQL local storage. We also\ntuned both systems: first, following standard best-practice tuning\ntechnique (standard tuning), and second, more aggressively, using\nthe workload including creating materialized views and aggressive\nindexing. In the case of the commercial cloud database we also had\na professional tuning expert from the cloud vendor help us tune\nthe system. While our latest prototype already integrates multi-\ndimensional indexes and storage, the results here do not include\nthose optimizations yet.\nAs Figure 1 shows, SageDB provides speed-ups (latency) of up\nto 8x compared to the expert tuned cloud-database and up to two\norders of magnitude improvement compared to PostgreSQL stan-\ndard tuned. SageDB achieves these speed-ups by using significantly\nless storage overhead compared to the expert-tuned configurations.\nMoreover, in the case of SageDB, no administrator was involved\nand the system self-optimized the layout, encoding, partial materi-\nalization, and replication based on the workload, saving significant\nhuman effort. Finally, we believe these numbers are only a glance\ninto what might be possible and we actually expect bigger speed-\nups as soon as SageDB fully supports multi-dimensional indexes\nand storage layouts as well as joins and partial materialization over\njoins.\n3227\n\n\n4.2 Lessons Learned\nWhile we are still in the process of fully building SageDB, we have\nalready learned several valuable lessons from our journey. In the\nfollowing, we highlight some of them.\nLesson 1: Taking a holistic approach is important. Tradi-\ntionally, systems components are developed in isolation. This com-\npartmentalization helps to reduce the complexity and allows smaller\nteams to work in parallel. However, it often also yields suboptimal\nperformance, as the components tend to make local decisions rather\nthan global ones. For example, it is tempting to implement a compo-\nnent to store and compress the data and another to index the data.\nHowever, from our work on multi-dimensional indexes [ 25,84] we\nalready know that it is best to co-optimize both together.\nLesson 2: There are exciting new opportunities for cross-\noptimization. Interestingly, we found several new optimizations\nwe hadn‚Äôt considered before we started the SageDB project. For\nexample, traditionally a database would use some partitioning strat-\negy to split the data into blocks or pages and then build a caching\nlayer on top of it. However, if the data and workload can be fore-\ncasted, it is also possible to determine which subsets of the data\nshould be always in the cache. This in turn allows us to optimize\nthe storage layout in a way that is more beneficial for caching. In\nfact, it even allows us to implement the cache entirely differently.\nEspecially if combined with other optimizations such as partial\nmaterialization and multi-dimensional in-memory indexes, this can\nlead to entirely new data layouts and significant speed-ups.\nLesson 3: More knobs are often better. In general, database\narchitects tend to minimize the number of tuning parameters. Every\ntuning parameter introduces complexity and database administra-\ntors already have a hard time tuning them correctly. Hence, the\ntrend towards ‚Äúknobless‚Äù systems. In many cases ‚Äúknobless‚Äù sys-\ntems are actually just systems where things that obviously should\nbe knobs have a fixed value that cannot be modified by the user.\nIt is easy to equate a system that is ‚Äúknobless‚Äù to one that is au-\ntonomous, but we should not take the \"easy route\" of simply gluing\nand hiding knobs‚Äîthis might make the administration easier, but\nit won‚Äôt improve system performance.\nHowever, if this complexity doesn‚Äôt pose an additional burden on\nthe administrator, increasing the number of knobs can actually be a\ngood thing. Additional knobs provide additional degrees of freedom\nand more ways to better tune a system for a given workload.\nLesson 4: Start small and revisit often. The complexity of\nbuilding an instance-optimized systems is significantly higher than\nbuilding a traditional database system. On one hand, we want to\nhave more knobs; on the other hand, we also don‚Äôt want to have\nabstracted components, which cannot be co-optimized. In addition,\noptimizing the system in a timely fashion using techniques like RL\nalso gets significantly harder as the space of potential optimization\nincreases. Thus, our approach is to not try to address all possible\noptimizations at once, but rather make simplifying assumptions\nthat we will revisit later. For example, we decided to treat the block-\nlevel data layout optimization separately from the in-memory data\nlayout optimization, even though ideally we would co-optimize\nthem.Lesson 5: Evaluate with real-world workloads. We already\nknew from our experience with building instance-optimized compo-\nnents that the standard benchmarks, like TPC-H, are actually a bad\nway to evaluate any of these components (see also [ 10]). For exam-\nple, for our Bao and Neo work on query optimization, we observed\nthat we were not able to achieve meaningful improvements for\nTPC-H for any of the commercial database systems. We speculate\nthat the reason for this is that database vendors make sure that their\nquery plans for these common workloads are ‚Äúperfect.‚Äù In other\nwords, the database vendors (manually?) ‚Äúoverfit‚Äù their systems\nto the benchmarks. Moreover, benchmarks tend to use synthetic\ndata and workload generators, which do not contain any of the\ninteresting patterns found in many real-world applications and are\neither too easy or sometimes too hard to learn, as they contain too\nmuch randomness. Those are just a few reasons, as outlined in [ 10],\nwhy standard database benchmarks are not suitable for evaluating\ninstance-optimized systems. At the same time, getting real-world\ndata is hard and, to date, we do not have a good solution to the\nproblem, except our ongoing attempts to get real-world data and\nworkload traces from industry.\nLesson 6: Robustness will be an issue. Over the course of the\npast six months, we had several conversations with cloud vendors\nabout what it would take to operationalize an instance-optimized\ndatabase. One, if not the biggest, concern is robustness and pre-\ndictability. In other words, cloud vendors are particularly afraid\nthat a query will run fast and suddenly slow down. Even more in-\nteresting, let‚Äôs assume a traditional database runs a complex query\nin100ùë†and an instance-optimized system runs the same query first\nin5ùë†but because of re-optimization and the prioritization of other\nqueries later at 60ùë†. Even though the entire workload might run\nfaster after the re-optimization, some cloud vendors would prefer to\nhave the predictable slower 100ùë†rather than the variation in latency.\nOne potential solution to the problem might be to artificially slow\ndown queries while passing on the resource savings to the cloud\nprovider. For example, if the reference system has a latency of 100ùë†,\nthe cloud provider might fix the latency at 80ùë†even though the\nquery takes only 5ùë†. However, arguably a 5ùë†query requires fewer\nresources, which would still be beneficial for the cloud provider,\nwhile the user gets a decent but not overwhelming performance\nincrease. Another alternative technique might be to optimize in a\nway that performance degressions are minimized. For example, for\nNeo [ 79] we proposed a weighted loss function, which penalized\nperformance degressions compared to PostgreSQL.\n4.3 Future Work\nWe are just at the beginning of understanding how instance-optimiz-\ned data management systems should be built and what techniques\nare required to do so. Open research questions in that space range\nfrom the right development tools (e.g., how do we efficiently de-\nvelop systems that have models at the core), debugging (e.g., how do\nwe debug a system that changes its behavior based on the observed\nworkload and data), design principles (e.g., how do we reduce the\ncomplexity of building such a system while still being able to do\nholistic optimization), benchmarking (e.g., can we build standard-\nized benchmarks for instance-optimized systems [ 10]), to entirely\nnew techniques (e.g., the above-mentioned storage optimizations\n3228\n\n\nfor caching). However, if successful, these techniques will provide\nan entirely new way to build (database) systems and unleash un-\nprecedented performance while significantly reducing the cost in\ncompute and human resources to keep the systems performant\nthrough administrators.\nACKNOWLEDGMENTS\nOur work in the area of ML for Systems would have not been possi-\nble without my excellent students, PostDocs, and numerous collab-\norators from academia and industry. In particular, Jialin Ding and\nRyan Marcus were so far instrumental in moving the SageDB project\nforward. In addition, many of the results shown here wouldn‚Äôt have\nbeen possible without Mohammad Alizadeh, Alex Beutel, Laurent\nBindschaedler, Ed Chi, Jeff Dean, Andreas Kipf, Donald Kossmann,\nEric Knorr, Ani Kristo, Markos Markakis, Umar Farooq Minhas,\nMichael Mitzenmacher, Parimarjan Negi, Amadou Ngom, Ibrahim\nSabek, Nesime Tatbul, Kapil Vaidya, and Geoffrey Xiangyu Yu.\nSome of the research out of MIT featured here was supported by\nGoogle, Intel, and Microsoft as part of the MIT Data Systems and\nAI Lab (DSAIL) at MIT, and NSF IIS 1900933. Part of the research\nout of DSAIL was also sponsored by the United States Air Force\nResearch Laboratory and the United States Air Force Artificial\nIntelligence Accelerator and was accomplished under Cooperative\nAgreement Number FA8750-19-2-1000. The views and conclusions\ncontained in this document are those of the authors and should not\nbe interpreted as representing the official policies, either expressed\nor implied, of the United States Air Force or the U.S. Government.\nThe U.S. Government is authorized to reproduce and distribute\nreprints for government purposes notwithstanding any copyright\nnotation herein.\nREFERENCES\n[1]Hussam Abu-Libdeh, Deniz Altinb√ºken, Alex Beutel, Ed H. Chi, Lyric Doshi,\nTim Kraska, Xiaozhou Li, Andy Ly, and Christopher Olston. 2020. Learned\nIndexes for a Google-scale Disk-based Database. In Proceedings of the Workshop\non ML for Systems at NeurIPS.\n[2]Sanjay Agrawal, Surajit Chaudhuri, and Vivek R. Narasayya. 2000. Automated\nSelection of Materialized Views and Indexes in SQL Databases. In VLDB 2000,\nProceedings of 26th International Conference on Very Large Data Bases, September\n10-14, 2000, Cairo, Egypt. 496‚Äì505.\n[3]Sanjay Agrawal, Vivek R. Narasayya, and Beverly Yang. 2004. Integrating\nVertical and Horizontal Partitioning Into Automated Physical Database Design.\nInProceedings of the ACM SIGMOD International Conference on Management of\nData, Paris, France, June 13-18, 2004. ACM, 359‚Äì370. https://doi.org/10.1145/\n1007568.1007609\n[4]Dana Van Aken, Andrew Pavlo, Geoffrey J. Gordon, and Bohan Zhang. 2017.\nAutomatic Database Management System Tuning Through Large-scale Machine\nLearning. In Proceedings of the 2017 ACM International Conference on Manage-\nment of Data, SIGMOD Conference 2017, Chicago, IL, USA, May 14-19, 2017. ACM,\n1009‚Äì1024. https://doi.org/10.1145/3035918.3064029\n[5]Dana Van Aken, Dongsheng Yang, Sebastien Brillard, Ari Fiorino, Bohan Zhang,\nChristian Billian, and Andrew Pavlo. 2021. An Inquiry into Machine Learning-\nbased Automatic Configuration Tuning Services on Real-World Database Man-\nagement Systems. Proc. VLDB Endow. 14, 7 (2021), 1241‚Äì1253.\n[6]Abdullah Al-Mamun, Hao Wu, and Walid G. Aref. 2020. A Tutorial on Learned\nMulti-dimensional Indexes. In SIGSPATIAL ‚Äô20: 28th International Conference on\nAdvances in Geographic Information Systems, Seattle, WA, USA, November 3-6,\n2020. ACM, 1‚Äì4. https://doi.org/10.1145/3397536.3426358\n[7]Christos Anagnostopoulos and Peter Triantafillou. 2015. Learning Set Cardinal-\nity in Distance Nearest Neighbours. In Proceedings of the 2015 IEEE International\nConference on Data Mining (ICDM) (ICDM ‚Äô15). IEEE Computer Society, USA,\n691‚Äì696. https://doi.org/10.1109/ICDM.2015.17\n[8]C. Anagnostopoulos and P. Triantafillou. 2015. Learning to Accurately COUNT\nwith Query-Driven Predictive Analytics. In 2015 IEEE International Conference\non Big Data (Big Data) (Big Data ‚Äô15). 14‚Äì23. https://doi.org/10.1109/BigData.\n2015.7363736[9]Christos Anagnostopoulos and Peter Triantafillou. 2017. Query-Driven Learning\nfor Predictive Analytics of Data Subspace Cardinality. ACM Trans. Knowl. Discov.\nData 11, 4 (June 2017), 47:1‚Äì47:46. https://doi.org/10.1145/3059177\n[10] Laurent Bindschaedler, Andreas Kipf, Tim Kraska, Ryan Marcus, and Umar Fa-\nrooq Minhas. 2021. Towards a Benchmark for Learned Systems. In 37th IEEE\nInternational Conference on Data Engineering Workshops, ICDE Workshops 2021,\nChania, Greece, April 19-22, 2021. IEEE, 127‚Äì133. https://doi.org/10.1109/\nICDEW53142.2021.00029\n[11] Matthias Brantner, Daniela Florescu, David A. Graf, Donald Kossmann, and Tim\nKraska. 2008. Building a database on S3. In Proceedings of the SIGMOD. ACM,\n251‚Äì264.\n[12] Lujing Cen, Andreas Kipf, Ryan Marcus, and Tim Kraska. 2021. LEA: A Learned\nEncoding Advisor for Column Stores. In aiDM ‚Äô21: Fourth Workshop in Exploiting\nAI Techniques for Data Management, Virtual Event, China, 25 June, 2021. ACM,\n32‚Äì35. https://doi.org/10.1145/3464509.3464885\n[13] Surajit Chaudhuri and Vivek R. Narasayya. 1997. An Efficient Cost-Driven\nIndex Selection Tool for Microsoft SQL Server. In VLDB‚Äô97, Proceedings of 23rd\nInternational Conference on Very Large Data Bases, August 25-29, 1997, Athens,\nGreece. Morgan Kaufmann, 146‚Äì155. http://www.vldb.org/conf/1997/P146.PDF\n[14] Surajit Chaudhuri and Vivek R. Narasayya. 1998. AutoAdmin ‚ÄôWhat-if‚Äô Index\nAnalysis Utility. In SIGMOD 1998, Proceedings ACM SIGMOD International Con-\nference on Management of Data, June 2-4, 1998, Seattle, Washington, USA. ACM\nPress, 367‚Äì378. https://doi.org/10.1145/276304.276337\n[15] Surajit Chaudhuri and Vivek R. Narasayya. 2007. Self-Tuning Database Systems:\nA Decade of Progress. In Proceedings of the 33rd International Conference on Very\nLarge Data Bases, University of Vienna, Austria, September 23-27, 2007. ACM,\n3‚Äì14. http://www.vldb.org/conf/2007/papers/special/p3-chaudhuri.pdf\n[16] Andrew Crotty. 2021. Hist-Tree: Those Who Ignore It Are Doomed to Learn. In\n11th Conference on Innovative Data Systems Research, CIDR 2021, Virtual Event,\nJanuary 11-15, 2021, Online Proceedings. www.cidrdb.org. http://cidrdb.org/\ncidr2021/papers/cidr2021_paper20.pdf\n[17] Carlo Curino, Evan Jones, Yang Zhang, and Sam Madden. 2010. Schism: A\nWorkload-Driven Approach to Database Replication and Partitioning. PVLDB 3,\n1 (2010), 48‚Äì57. https://doi.org/10.14778/1920841.1920853\n[18] Yifan Dai, Yien Xu, Aishwarya Ganesan, Ramnatthan Alagappan, Brian Kroth,\nAndrea Arpaci-Dusseau, and Remzi Arpaci-Dusseau. 2020. From WiscKey to\nBourbon: A Learned Index for Log-Structured Merge Trees. In 14th USENIX\nSymposium on Operating Systems Design and Implementation (OSDI 20). USENIX\nAssociation, 155‚Äì171. https://www.usenix.org/conference/osdi20/presentation/\ndai\n[19] Zhenwei Dai and Anshumali Shrivastava. 2020. Adaptive Learned Bloom Filter\n(Ada-BF): Efficient Utilization of the Classifier with Application to Real-Time\nInformation Filtering on the Web. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual. https://proceedings.neurips.cc/paper/\n2020/hash/86b94dae7c6517ec1ac767fd2c136580-Abstract.html\n[20] Biplob K. Debnath, David J. Lilja, and Mohamed F. Mokbel. 2008. SARD: A\nstatistical approach for ranking database tuning parameters. In Proceedings of\nthe 24th International Conference on Data Engineering Workshops, ICDE 2008,\nApril 7-12, 2008, Canc√∫n, Mexico. IEEE Computer Society, 11‚Äì18. https://doi.\norg/10.1109/ICDEW.2008.4498279\n[21] Karl Dias, Mark Ramacher, Uri Shaft, Venkateshwaran Venkataramani, and\nGraham Wood. 2005. Automatic Performance Diagnosis and Tuning in Oracle.\nInSecond Biennial Conference on Innovative Data Systems Research, CIDR 2005,\nAsilomar, CA, USA, January 4-7, 2005, Online Proceedings. www.cidrdb.org, 84‚Äì94.\nhttp://cidrdb.org/cidr2005/papers/P07.pdf\n[22] Bailu Ding, Sudipto Das, Ryan Marcus, Wentao Wu, Surajit Chaudhuri, and\nVivek R. Narasayya. 2019. AI Meets AI: Leveraging Query Executions to Improve\nIndex Recommendations. In Proceedings of the 2019 International Conference on\nManagement of Data, SIGMOD Conference 2019, Amsterdam, The Netherlands,\nJune 30 - July 5, 2019. ACM, 1241‚Äì1258. https://doi.org/10.1145/3299869.3324957\n[23] Jialin Ding, Umar Farooq Minhas, Badrish Chandramouli, Chi Wang, Yinan Li,\nYing Li, Donald Kossmann, Johannes Gehrke, and Tim Kraska. 2021. Instance-\nOptimized Data Layouts for Cloud Analytics Workloads. In SIGMOD ‚Äô21: Inter-\nnational Conference on Management of Data, Virtual Event, China, June 20-25,\n2021. ACM, 418‚Äì431. https://doi.org/10.1145/3448016.3457270\n[24] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid B. Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned\nIndex. In Proceedings of the 2020 International Conference on Management of\nData, SIGMOD Conference 2020, online conference [Portland, OR, USA], June 14-19,\n2020. ACM, 969‚Äì984. https://doi.org/10.1145/3318464.3389711\n[25] Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020.\nTsunami: A Learned Multi-dimensional Index for Correlated Data and Skewed\nWorkloads. Proc. VLDB Endow. 14, 2 (2020), 74‚Äì86. https://doi.org/10.14778/\n3425879.3425880\n[26] DSAIL. 2021. Data System and AI Lab. http://dsail.csail.mit.edu/.\n3229\n\n\n[27] DSAIL. 2021. (Learned Index Leaderboard. https://learnedsystems.github.io/\nSOSDLeaderboard/leaderboard/. [Online; accessed 7-July-2021].\n[28] DSAIL. 2021. ML for Systems Papers. http://dsg.csail.mit.edu/mlforsystems/\npapers/.\n[29] Elbert Du, Franklyn Wang, and Michael Mitzenmacher. 2021. Putting the \"Learn-\ning\" into Learning-Augmented Algorithms for Frequency Estimation. In Proceed-\nings of the 38th International Conference on Machine Learning, ICML 2021, 18-24\nJuly 2021, Virtual Event (Proceedings of Machine Learning Research, Vol. 139).\nPMLR, 2860‚Äì2869. http://proceedings.mlr.press/v139/du21d.html\n[30] Songyun Duan, Vamsidhar Thummala, and Shivnath Babu. 2009. Tuning Data-\nbase Configuration Parameters with iTuned. Proc. VLDB Endow. 2, 1 (2009),\n1246‚Äì1257. https://doi.org/10.14778/1687627.1687767\n[31] Martin Eppert, Philipp Fent, and Thomas Neumann. 2021. A Tailored Regression\nfor Learned Indexes: Logarithmic Error Regression. In aiDM ‚Äô21: Fourth Workshop\nin Exploiting AI Techniques for Data Management, Virtual Event, China, 25 June,\n2021. ACM, 9‚Äì15. https://doi.org/10.1145/3464509.3464891\n[32] Ronald Fagin, Amnon Lotem, and Moni Naor. 2003. Optimal aggregation al-\ngorithms for middleware. J. Comput. Syst. Sci. 66, 4 (2003), 614‚Äì656. https:\n//doi.org/10.1016/S0022-0000(03)00026-6\n[33] Paolo Ferragina, Fabrizio Lillo, and Giorgio Vinciguerra. 2020. Why Are Learned\nIndexes So Effective?. In Proceedings of the 37th International Conference on\nMachine Learning (Proceedings of Machine Learning Research, Vol. 119). PMLR,\n3123‚Äì3132. http://proceedings.mlr.press/v119/ferragina20a.html\n[34] Paolo Ferragina, Fabrizio Lillo, and Giorgio Vinciguerra. 2021. On the per-\nformance of learned data structures. Theor. Comput. Sci. 871 (2021), 107‚Äì120.\nhttps://doi.org/10.1016/j.tcs.2021.04.015\n[35] Paolo Ferragina and Giorgio Vinciguerra. 2020. Learned Data Structures. In\nRecent Trends in Learning From Data, . Springer International Publishing, 5‚Äì41.\nhttps://doi.org/10.1007/978-3-030-43883-8_2\n[36] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-Index: A Fully-\nDynamic Compressed Learned Index with Provable Worst-Case Bounds. Pro-\nceedings of the VLDB Endowment 13, 8 (April 2020), 1162‚Äì1175. https://doi.org/\n10.14778/3389133.3389135\n[37] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim\nKraska. 2019. FITing-Tree: A Data-aware Index Structure. In Proceedings of\nthe 2019 International Conference on Management of Data, SIGMOD Conference\n2019, Amsterdam, The Netherlands, June 30 - July 5, 2019. ACM, 1189‚Äì1206.\nhttps://doi.org/10.1145/3299869.3319860\n[38] Vahid Ghadakchi, Mian Xie, and Arash Termehchy. 2020. Bandit join: prelimi-\nnary results. In Proceedings of the Third International Workshop on Exploiting\nArtificial Intelligence Techniques for Data Management, aiDM@SIGMOD 2020,\nPortland, Oregon, USA, June 19, 2020. ACM, 1:1‚Äì1:4. https://doi.org/10.1145/\n3401071.3401655\n[39] Justin Gottschlich, Armando Solar-Lezama, Nesime Tatbul, Michael Carbin,\nMartin Rinard, Regina Barzilay, Saman P. Amarasinghe, Joshua B. Tenenbaum,\nand Tim Mattson. 2018. The three pillars of machine programming. In Proceed-\nings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and\nProgramming Languages, MAPL@PLDI 2018, Philadelphia, PA, USA, June 18-22,\n2018. ACM, 69‚Äì80. https://doi.org/10.1145/3211346.3211355\n[40] Himanshu Gupta, Venky Harinarayan, Anand Rajaraman, and Jeffrey D. Ullman.\n1997. Index Selection for OLAP. In Proceedings of the Thirteenth International\nConference on Data Engineering, April 7-11, 1997, Birmingham, UK. IEEE Com-\nputer Society, 208‚Äì219. https://doi.org/10.1109/ICDE.1997.581755\n[41] Yaniv Gur, Dongsheng Yang, Frederik Stalschus, and Berthold Reinwald. 2021.\nAdaptive Multi-Model Reinforcement Learning for Online Database Tuning.\nInProceedings of the 24th International Conference on Extending Database Tech-\nnology, EDBT 2021, Nicosia, Cyprus, March 23 - 26, 2021. OpenProceedings.org,\n439‚Äì444. https://doi.org/10.5441/002/edbt.2021.48\n[42] Ali Hadian and Thomas Heinis. 2019. Considerations for handling updates\nin learned index structures. In Proceedings of the Second International Work-\nshop on Exploiting Artificial Intelligence Techniques for Data Management,\naiDM@SIGMOD 2019, Amsterdam, The Netherlands, July 5, 2019. ACM, 3:1‚Äì3:4.\nhttps://doi.org/10.1145/3329859.3329874\n[43] Rojeh Hayek and Oded Shmueli. 2019. Improved Cardinality Estimation\nby Learning Queries Containment Rates. arXiv:1908.07723 [cs] (Aug. 2019).\narXiv:1908.07723 [cs]\n[44] Shunsuke Higuchi, Junji Takemasa, Yuki Koizumi, Atsushi Tagami, and Toru\nHasegawa. 2021. Feasibility of Longest Prefix Matching Using Learned Index\nStructures. SIGMETRICS Perform. Eval. Rev. 48, 4 (May 2021), 45‚Äì48. https:\n//doi.org/10.1145/3466826.3466842\n[45] Benjamin Hilprecht and Carsten Binnig. 2021. One Model to Rule them\nAll: Towards Zero-Shot Learning for Databases. CoRR abs/2105.00642 (2021).\narXiv:2105.00642 https://arxiv.org/abs/2105.00642\n[46] Darryl Ho, Jialin Ding, Sanchit Misra, Nesime Tatbul, Vikram Nathan, Vasimud-\ndin Md, and Tim Kraska. 2019. LISA: Towards Learned DNA Sequence Search.\nCoRR abs/1910.04728 (2019). arXiv:1910.04728 http://arxiv.org/abs/1910.04728\n[47] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. 2019. Learning-Based\nFrequency Estimation Algorithms. In 7th International Conference on LearningRepresentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.\nhttps://openreview.net/forum?id=r1lohoCqY7\n[48] Stratos Idreos, Niv Dayan, Wilson Qin, Mali Akmanalp, Sophie Hilgard, Andrew\nRoss, James Lennon, Varun Jain, Harshita Gupta, David Li, and Zichen Zhu.\n2019. Design Continuums and the Path Toward Self-Designing Key-Value Stores\nthat Know and Learn. In 9th Biennial Conference on Innovative Data Systems\nResearch, CIDR 2019, Asilomar, CA, USA, January 13-16, 2019, Online Proceedings.\nwww.cidrdb.org. http://cidrdb.org/cidr2019/papers/p143-idreos-cidr19.pdf\n[49] Stratos Idreos and Tim Kraska. 2019. From Auto-tuning One Size Fits All\nto Self-designed and Learned Data-intensive Systems. In Proceedings of the\n2019 International Conference on Management of Data, SIGMOD Conference\n2019, Amsterdam, The Netherlands, June 30 - July 5, 2019. ACM, 2054‚Äì2059.\nhttps://doi.org/10.1145/3299869.3314034\n[50] Stratos Idreos, Kostas Zoumpatianos, Subarna Chatterjee, Wilson Qin, Abdul\nWasay, Brian Hentschel, Mike S. Kester, Niv Dayan, Demi Guo, Minseo Kang,\nand Yiyou Sun. 2019. Learning Data Structure Alchemy. IEEE Data Eng. Bull.\n42, 2 (2019), 47‚Äì58. http://sites.computer.org/debull/A19june/p47.pdf\n[51] Amir Ilkhechi, Andrew Crotty, Alex Galakatos, Yicong Mao, Grace Fan, Xiran\nShi, and Ugur √áetintemel. 2020. DeepSqueeze: Deep Semantic Compression for\nTabular Data. In Proceedings of the 2020 International Conference on Management\nof Data, SIGMOD Conference 2020, online conference [Portland, OR, USA], June\n14-19, 2020. ACM, 1733‚Äì1746. https://doi.org/10.1145/3318464.3389734\n[52] Nicholas Jacek and J. Eliot B. Moss. 2019. Learning When to Garbage Collect\nwith Random Forests. In Proceedings of the 2019 ACM SIGPLAN International\nSymposium on Memory Management (ISMM 2019). Association for Computing\nMachinery, Phoenix, AZ, USA, 53‚Äì63. https://doi.org/10.1145/3315573.3329983\n[53] Andreas Kipf, Thomas Kipf, Bernhard Radke, Viktor Leis, Peter Boncz, and\nAlfons Kemper. 2019. Learned Cardinalities: Estimating Correlated Joins with\nDeep Learning. In 9th Biennial Conference on Innovative Data Systems Research\n(CIDR ‚Äô19).\n[54] Tim Kraska, Mohammad Alizadeh, Alex Beutel, Ed H. Chi, Ani Kristo, Guillaume\nLeclerc, Samuel Madden, Hongzi Mao, and Vikram Nathan. 2019. SageDB: A\nLearned Database System. In 9th Biennial Conference on Innovative Data Systems\nResearch, CIDR 2019, Asilomar, CA, USA, January 13-16, 2019, Online Proceedings.\nwww.cidrdb.org. http://cidrdb.org/cidr2019/papers/p117-kraska-cidr19.pdf\n[55] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis.\n2017. The Case for Learned Index Structures. CoRR abs/1712.01208 (2017).\narXiv:1712.01208 http://arxiv.org/abs/1712.01208\n[56] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In Proceedings of the 2018 International\nConference on Management of Data, SIGMOD Conference 2018, Houston, TX, USA,\nJune 10-15, 2018. ACM, 489‚Äì504. https://doi.org/10.1145/3183713.3196909\n[57] Sanjay Krishnan, Zongheng Yang, Ken Goldberg, Joseph Hellerstein, and Ion\nStoica. 2018. Learning to Optimize Join Queries With Deep Reinforcement\nLearning. arXiv:1808.03196 [cs] (Aug. 2018). arXiv:1808.03196 [cs]\n[58] Ani Kristo, Kapil Vaidya, Ugur √áetintemel, Sanchit Misra, and Tim Kraska.\n2020. The Case for a Learned Sorting Algorithm. In Proceedings of the 2020\nInternational Conference on Management of Data, SIGMOD Conference 2020,\nonline conference [Portland, OR, USA], June 14-19, 2020. ACM, 1001‚Äì1016. https:\n//doi.org/10.1145/3318464.3389752\n[59] Ani Kristo, Kapil Vaidya, and Tim Kraska. 2021. Defeating duplicates: A re-\ndesign of the LearnedSort algorithm. In International Workshop on Applied AI\nfor Database Systems and Applications (AIDB@VLDB) (AIDB ‚Äô21).\n[60] Mayuresh Kunjir and Shivnath Babu. 2020. Black or White? How to Develop an\nAutoTuner for Memory-based Analytics. In Proceedings of the 2020 International\nConference on Management of Data, SIGMOD Conference 2020, online conference\n[Portland, OR, USA], June 14-19, 2020. ACM, 1667‚Äì1683. https://doi.org/10.1145/\n3318464.3380591\n[61] Hai Lan, Zhifeng Bao, and Yuwei Peng. 2020. An Index Advisor Using Deep\nReinforcement Learning. In CIKM ‚Äô20: The 29th ACM International Conference on\nInformation and Knowledge Management, Virtual Event, Ireland, October 19-23,\n2020. ACM, 2105‚Äì2108. https://doi.org/10.1145/3340531.3412106\n[62] Guoliang Li, Xuanhe Zhou, Shifu Li, and Bo Gao. 2019. QTune: A Query-Aware\nDatabase Tuning System with Deep Reinforcement Learning. Proc. VLDB Endow.\n12, 12 (2019), 2118‚Äì2130. https://doi.org/10.14778/3352063.3352129\n[63] Henry Liu, Mingbin Xu, Ziting Yu, Vincent Corvinelli, and Calisto Zuzarte.\n2015. Cardinality Estimation Using Neural Networks. In Proceedings of the 25th\nAnnual International Conference on Computer Science and Software Engineering\n(CASCON ‚Äô15). IBM Corp., Riverton, NJ, USA, 53‚Äì59.\n[64] Qiyu Liu, Libin Zheng, Yanyan Shen, and Lei Chen. 2020. Stable Learned\nBloom Filters for Data Streams. Proc. VLDB Endow. 13, 11 (2020), 2355‚Äì2367.\nhttp://www.vldb.org/pvldb/vol13/p2355-liu.pdf\n[65] Konstantinos Lolos, Ioannis Konstantinou, Verena Kantere, and Nectarios\nKoziris. 2017. Elastic Management of Cloud Applications Using Adaptive Rein-\nforcement Learning. In IEEE International Conference on Big Data (Big Data ‚Äô17).\nIEEE, 203‚Äì212. https://doi.org/10.1109/BigData.2017.8257928\n[66] Baotong Lu, Jialin Ding, Eric Lo, Umar Farooq Minhas, and Tianzheng Wang.\n2021. APEX: A High-Performance Learned Index on Persistent Memory. CoRR\n3230\n\n\nabs/2105.00683 (2021). arXiv:2105.00683 https://arxiv.org/abs/2105.00683\n[67] Jiaheng Lu, Yuxing Chen, Herodotos Herodotou, and Shivnath Babu. 2019.\nSpeedup Your Analytics: Automatic Parameter Tuning for Databases and Big\nData Systems. Proc. VLDB Endow. 12, 12 (2019), 1970‚Äì1973. https://doi.org/10.\n14778/3352063.3352112\n[68] Thodoris Lykouris and Sergei Vassilvitskii. 2018. Competitive Caching with\nMachine Learned Advice. In Proceedings of the 35th International Conference\non Machine Learning, ICML 2018, Stockholmsm√§ssan, Stockholm, Sweden, July\n10-15, 2018 (Proceedings of Machine Learning Research, Vol. 80). PMLR, 3302‚Äì3311.\nhttp://proceedings.mlr.press/v80/lykouris18a.html\n[69] Marcel Maltry and Jens Dittrich. 2021. A Critical Analysis of Recursive Model\nIndexes. CoRR abs/2106.16166 (2021). arXiv:2106.16166 https://arxiv.org/abs/\n2106.16166\n[70] Abdullah Al Mamun, Hao Wu, and Walid G. Aref. 2020. A Tutorial on Learned\nMultidimensional Indexes. https://www.cs.purdue.edu/homes/aref/learned-\nindexes-tutorial.html.\n[71] Hongzi Mao, Parimarjan Negi, Akshay Narayan, Hanrui Wang, Jiacheng Yang,\nHaonan Wang, Ryan Marcus, Ravichandra Addanki, Mehrdad Khani Shirkoohi,\nSongtao He, Vikram Nathan, Frank Cangialosi, Shaileshh Bojja Venkatakrish-\nnan, Wei-Hung Weng, Song Han, Tim Kraska, and Mohammad Alizadeh. 2019.\nPark: An Open Platform for Learning-Augmented Computer Systems. In Ad-\nvances in Neural Information Processing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada. 2490‚Äì2502. https://proceedings.neurips.cc/paper/2019/hash/\nf69e505b08403ad2298b9f262659929a-Abstract.html\n[72] Hongzi Mao, Malte Schwarzkopf, Shaileshh Bojja Venkatakrishnan, Zili Meng,\nand Mohammad Alizadeh. 2018. Learning Scheduling Algorithms for Data\nProcessing Clusters. arXiv:1810.01963 [cs, stat] (2018). arXiv:1810.01963 [cs, stat]\n[73] Hongzi Mao, Malte Schwarzkopf, Shaileshh Bojja Venkatakrishnan, Zili Meng,\nand Mohammad Alizadeh. 2018. Learning Scheduling Algorithms for Data\nProcessing Clusters. arXiv:1810.01963 [cs, stat] (Oct. 2018). arXiv:1810.01963 [cs,\nstat]\n[74] Ryan Marcus. 2021. More Bao Results: Learned Distributed Query Optimization\non Vertica, Redshift, and Azure Synapse. https://learnedsystems.mit.edu/bao-\ndistributed/.\n[75] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra,\nAlfons Kemper, Thomas Neumann, and Tim Kraska. 2020. Benchmarking\nLearned Indexes. Proc. VLDB Endow. 14, 1 (2020), 1‚Äì13. https://doi.org/10.14778/\n3421424.3421425\n[76] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Nesime Tatbul, Mohammad Al-\nizadeh, and Tim Kraska. 2021. Bao: Making Learned Query Optimization Practi-\ncal. In SIGMOD ‚Äô21: International Conference on Management of Data, Virtual\nEvent, China, June 20-25, 2021. ACM, 1275‚Äì1288. https://doi.org/10.1145/3448016.\n3452838\n[77] Ryan Marcus and Olga Papaemmanouil. 2018. Deep Reinforcement Learning for\nJoin Order Enumeration. In First International Workshop on Exploiting Artificial\nIntelligence Techniques for Data Management (aiDM @ SIGMOD ‚Äô18). Houston,\nTX.\n[78] Ryan Marcus, Emily Zhang, and Tim Kraska. 2020. CDFShop: Exploring and\nOptimizing Learned Index Structures. In Proceedings of the 2020 International\nConference on Management of Data, SIGMOD Conference 2020, online conference\n[Portland, OR, USA], June 14-19, 2020. ACM, 2789‚Äì2792. https://doi.org/10.1145/\n3318464.3384706\n[79] Ryan C. Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Al-\nizadeh, Tim Kraska, Olga Papaemmanouil, and Nesime Tatbul. 2019. Neo:\nA Learned Query Optimizer. Proc. VLDB Endow. 12, 11 (2019), 1705‚Äì1718.\nhttps://doi.org/10.14778/3342263.3342644\n[80] Michael Mitzenmacher. 2018. A Model for Learned Bloom Filters and Optimiz-\ning by Sandwiching. In Advances in Neural Information Processing Systems 31:\nAnnual Conference on Neural Information Processing Systems 2018, NeurIPS 2018,\nDecember 3-8, 2018, Montr√©al, Canada. 462‚Äì471. https://proceedings.neurips.cc/\npaper/2018/hash/0f49c89d1e7298bb9930789c8ed59d48-Abstract.html\n[81] Michael Mitzenmacher. 2020. Scheduling with Predictions and the Price of\nMisprediction. In 11th Innovations in Theoretical Computer Science Conference,\nITCS 2020, January 12-14, 2020, Seattle, Washington, USA (LIPIcs, Vol. 151). Schloss\nDagstuhl - Leibniz-Zentrum f√ºr Informatik, 14:1‚Äì14:18. https://doi.org/10.4230/\nLIPIcs.ITCS.2020.14\n[82] Michael Mitzenmacher and Sergei Vassilvitskii. 2020. Algorithms with Predic-\ntions. In Beyond the Worst-Case Analysis of Algorithms, . Cambridge University\nPress, 646‚Äì662. https://doi.org/10.1017/9781108637435.037\n[83] C. Mohan and Frank E. Levine. 1992. ARIES/IM: An Efficient and High Con-\ncurrency Index Management Method Using Write-Ahead Logging. In Pro-\nceedings of the 1992 ACM SIGMOD International Conference on Management\nof Data, San Diego, California, USA, June 2-5, 1992. ACM Press, 371‚Äì380.\nhttps://doi.org/10.1145/130283.130338\n[84] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning Multi-Dimensional Indexes. In Proceedings of the 2020 International Con-\nference on Management of Data, SIGMOD Conference 2020, online conference[Portland, OR, USA], June 14-19, 2020. ACM, 985‚Äì1000. https://doi.org/10.1145/\n3318464.3380579\n[85] Parimarjan Negi, Matteo Interlandi, Ryan Marcus, Mohammad Alizadeh, Tim\nKraska, Marc Friedman, and Alekh Jindal. 2021. Steering Query Optimizers: A\nPractical Take on Big Data Workloads. In SIGMOD ‚Äô21: International Conference\non Management of Data, Virtual Event, China, June 20-25, 2021. ACM, 2557‚Äì2569.\nhttps://doi.org/10.1145/3448016.3457568\n[86] Parimarjan Negi, Ryan Marcus, Andreas Kipf, Hongzi Mao, Nesime Tatbul, Tim\nKraska, and Mohammad Alizadeh. to appear. Flow-Loss: Learning Cardinality\nEstimates That Matter. Proc. VLDB Endow. ( to appear).\n[87] Parimarjan Negi, Ryan Marcus, Hongzi Mao, Nesime Tatbul, Tim Kraska, and\nMohammad Alizadeh. 2020. Cost-Guided Cardinality Estimation: Focus Where\nit Matters. In 36th IEEE International Conference on Data Engineering Workshops,\nICDE Workshops 2020, Dallas, TX, USA, April 20-24, 2020. IEEE, 154‚Äì157. https:\n//doi.org/10.1109/ICDEW49219.2020.00034\n[88] Parimarjan Negi, Ryan Marcus, Hongzi Mao, Nesime Tatbul, Tim Kraska, and\nMohammad Alizadeh. 2020. Cost-Guided Cardinality Estimation: Focus Where\nIt Matters. In Workshop on Self-Managing Databases (SMDB @ ICDE ‚Äô20).\n[89] Thomas Neumann and Sebastian Michel. 2008. Smooth Interpolating Histograms\nwith Error Guarantees. In Sharing Data, Information and Knowledge, 25th British\nNational Conference on Databases (BNCOD ‚Äô08). 126‚Äì138. https://doi.org/10.\n1007/978-3-540-70504-8\\_12\n[90] Harrie Oosterhuis, J. Shane Culpepper, and Maarten de Rijke. 2018. The Poten-\ntial of Learned Index Structures for Index Compression. In Proceedings of the\n23rd Australasian Document Computing Symposium, ADCS 2018, Dunedin, New\nZealand, December 11-12, 2018. ACM, 7:1‚Äì7:4. https://doi.org/10.1145/3291992.\n3291993\n[91] Jennifer Ortiz, Magdalena Balazinska, Johannes Gehrke, and S. Sathiya Keerthi.\n2019. An Empirical Analysis of Deep Learning for Cardinality Estimation.\narXiv:1905.06425 [cs] (Sept. 2019). arXiv:1905.06425 [cs]\n[92] Jennifer Ortiz, Brendan Lee, Magdalena Balazinska, and Joseph L. Hellerstein.\n2016. PerfEnforce: A Dynamic Scaling Engine for Analytics with Performance\nGuarantees. arXiv:1605.09753 [cs] (May 2016). arXiv:1605.09753 [cs]\n[93] Yongjoo Park, Shucheng Zhong, and Barzan Mozafari. 2018. QuickSel: Quick\nSelectivity Learning with Mixture Models. arXiv:1812.10568 [cs] (Dec. 2018).\narXiv:1812.10568 [cs]\n[94] Andrew Pavlo, Gustavo Angulo, Joy Arulraj, Haibin Lin, Jiexi Lin, Lin Ma,\nPrashanth Menon, Todd C. Mowry, Matthew Perron, Ian Quah, Siddharth San-\nturkar, Anthony Tomasic, Skye Toor, Dana Van Aken, Ziqi Wang, Yingjun Wu,\nRan Xian, and Tieying Zhang. 2017. Self-Driving Database Management Sys-\ntems. In 8th Biennial Conference on Innovative Data Systems Research, CIDR 2017,\nChaminade, CA, USA, January 8-11, 2017, Online Proceedings. www.cidrdb.org.\nhttp://cidrdb.org/cidr2017/papers/p42-pavlo-cidr17.pdf\n[95] Andrew Pavlo, Evan P. C. Jones, and Stan Zdonik. 2011. On Predictive Modeling\nfor Optimizing Transaction Execution in Parallel OLTP Systems. PVLDB 5, 2\n(2011), 86‚Äì96. https://doi.org/10.14778/2078324.2078325\n[96] Mark Raasveldt and Hannes M√ºhleisen. 2019. DuckDB: an Embeddable Analyti-\ncal Database. In Proceedings of the 2019 International Conference on Management\nof Data, SIGMOD Conference 2019, Amsterdam, The Netherlands, June 30 - July 5,\n2019. ACM, 1981‚Äì1984. https://doi.org/10.1145/3299869.3320212\n[97] Jack W. Rae, Sergey Bartunov, and Timothy P. Lillicrap. 2019. Meta-Learning\nNeural Bloom Filters. In Proceedings of the 36th International Conference on\nMachine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA\n(Proceedings of Machine Learning Research, Vol. 97). PMLR, 5271‚Äì5280. http:\n//proceedings.mlr.press/v97/rae19a.html\n[98] Jun Rao, Chun Zhang, Nimrod Megiddo, and Guy M. Lohman. 2002. Automating\nphysical database design in a parallel database. In Proceedings of the 2002 ACM\nSIGMOD International Conference on Management of Data, Madison, Wisconsin,\nUSA, June 3-6, 2002. ACM, 558‚Äì569. https://doi.org/10.1145/564691.564757\n[99] Alon Rashelbach, Ori Rottenstreich, and Mark Silberstein. 2020. A Computa-\ntional Approach to Packet Classification. In Proceedings of the Annual Conference\nof the ACM Special Interest Group on Data Communication on the Applications,\nTechnologies, Architectures, and Protocols for Computer Communication (Virtual\nEvent, USA) (SIGCOMM ‚Äô20). Association for Computing Machinery, New York,\nNY, USA, 542‚Äì556. https://doi.org/10.1145/3387514.3405886\n[100] Ibrahim Sabek, Kapil Vaidya, Dominik Horn, Andreas Kipf, and Tim Kraska.\n2021. When Are Learned Models Better Than Hash Functions?. In International\nWorkshop on Applied AI for Database Systems and Applications (AIDB@VLDB)\n(AIDB ‚Äô21).\n[101] Zahra Sadri, Le Gruenwald, and Eleazar Leal. 2020. DRLindex: deep reinforce-\nment learning index advisor for a cluster database. In IDEAS 2020: 24th Interna-\ntional Database Engineering & Applications Symposium, Seoul, Republic of Korea,\nAugust 12-14, 2020. ACM, 11:1‚Äì11:8. https://dl.acm.org/doi/10.1145/3410566.\n3410603\n[102] Michael Schaarschmidt, Alexander Kuhnle, Ben Ellis, Kai Fricke, Felix Gessert,\nand Eiko Yoneki. 2018. LIFT: Reinforcement Learning in Computer Systems\nby Learning From Demonstrations. arXiv:1808.07903 [cs, stat] (Aug. 2018).\narXiv:1808.07903 [cs, stat]\n3231\n\n\n[103] Yangjun Sheng, Anthony Tomasic, Tieying Zhang, and Andrew Pavlo. 2019.\nScheduling OLTP transactions via learned abort prediction. In Proceedings of\nthe Second International Workshop on Exploiting Artificial Intelligence Techniques\nfor Data Management, aiDM@SIGMOD 2019, Amsterdam, The Netherlands, July\n5, 2019. ACM, 1:1‚Äì1:8. https://doi.org/10.1145/3329859.3329871\n[104] Benjamin Spector, Andreas Kipf, Kapil Vaidya, Chi Wang, Umar Farooq Minhas,\nand Tim Kraska. 2021. Bounding the Last Mile: Efficient Learned String Indexing.\nInInternational Workshop on Applied AI for Database Systems and Applications\n(AIDB@VLDB) (AIDB ‚Äô21).\n[105] Michael Stillger, Guy M. Lohman, Volker Markl, and Mokhtar Kandil. 2001. LEO\n- DB2‚Äôs LEarning Optimizer. In VLDB (VLDB ‚Äô01) . 19‚Äì28.\n[106] Ji Sun and Guoliang Li. 2019. An End-to-End Learning-Based Cost Estimator.\nProceedings of the VLDB Endowment 13, 3 (Nov. 2019), 307‚Äì319. https://doi.org/\n10.14778/3368289.3368296\n[107] Chuzhe Tang, Youyun Wang, Zhiyuan Dong, Gansen Hu, Zhaoguo Wang, Min-\njie Wang, and Haibo Chen. 2020. XIndex: A Scalable Learned Index for Mul-\nticore Data Storage. In Proceedings of the 25th ACM SIGPLAN Symposium on\nPrinciples and Practice of Parallel Programming (San Diego, California) (PPoPP\n‚Äô20). Association for Computing Machinery, New York, NY, USA, 308‚Äì320.\nhttps://doi.org/10.1145/3332466.3374547\n[108] Kapil Vaidya, Eric Knorr, Michael Mitzenmacher, and Tim Kraska. 2021. Parti-\ntioned Learned Bloom Filters. In 9th International Conference on Learning Rep-\nresentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.\nhttps://openreview.net/forum?id=6BRLOfrMhW\n[109] Gary Valentin, Michael Zuliani, Daniel C. Zilio, Guy M. Lohman, and Alan\nSkelley. 2000. DB2 Advisor: An Optimizer Smart Enough to Recommend Its Own\nIndexes. In Proceedings of the 16th International Conference on Data Engineering,\nSan Diego, California, USA, February 28 - March 3, 2000. IEEE Computer Society,\n101‚Äì110. https://doi.org/10.1109/ICDE.2000.839397\n[110] Youyun Wang, Chuzhe Tang, Zhaoguo Wang, and Haibo Chen. 2020. SIndex:\na scalable learned index for string keys. In APSys ‚Äô20: 11th ACM SIGOPS Asia-\nPacific Workshop on Systems, Tsukuba, Japan, August 24-25, 2020. ACM, 17‚Äì24.https://dl.acm.org/doi/10.1145/3409963.3410496\n[111] Xingda Wei, Rong Chen, and Haibo Chen. 2020. Fast RDMA-based Ordered\nKey-Value Store using Remote Learned Cache. In 14th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI 20). USENIX Association,\n117‚Äì135. https://www.usenix.org/conference/osdi20/presentation/wei\n[112] Lucas Woltmann, Claudio Hartmann, Maik Thiele, Dirk Habich, and Wolfgang\nLehner. 2019. Cardinality Estimation with Local Deep Learning Models. In Pro-\nceedings of the Second International Workshop on Exploiting Artificial Intelligence\nTechniques for Data Management (aiDM ‚Äô19). Association for Computing Machin-\nery, Amsterdam, Netherlands, 1‚Äì8. https://doi.org/10.1145/3329859.3329875\n[113] Jiacheng Wu, Yong Zhang, Shimin Chen, Yu Chen, Jin Wang, and Chunxiao\nXing. 2021. Updatable Learned Index with Precise Positions. Proc. VLDB Endow.\n14, 8 (2021), 1276‚Äì1288. http://www.vldb.org/pvldb/vol14/p1276-wu.pdf\n[114] Wenkun Xiang, Hao Zhang, Rui Cui, Xing Chu, Keqin Li, and Wei Zhou. 2019.\nPavo: A RNN-Based Learned Inverted Index, Supervised or Unsupervised? IEEE\nAccess 7 (2019), 293‚Äì303. https://doi.org/10.1109/ACCESS.2018.2885350\n[115] Zongheng Yang, Badrish Chandramouli, Chi Wang, Johannes Gehrke, Yinan Li,\nUmar Farooq Minhas, Per-√Öke Larson, Donald Kossmann, and Rajeev Acharya.\n2020. Qd-tree: Learning Data Layouts for Big Data Analytics. In Proceedings of\nthe 2020 International Conference on Management of Data, SIGMOD Conference\n2020, online conference [Portland, OR, USA], June 14-19, 2020. ACM, 193‚Äì208.\nhttps://doi.org/10.1145/3318464.3389770\n[116] Zongheng Yang, Amog Kamsetty, Sifei Luan, Eric Liang, Yan Duan, Xi Chen,\nand Ion Stoica. 2020. NeuroCard: One Cardinality Estimator for All Tables.\narXiv:2006.08109 [cs] (June 2020). arXiv:2006.08109 [cs]\n[117] Zongheng Yang, Eric Liang, Amog Kamsetty, Chenggang Wu, Yan Duan, Xi\nChen, Pieter Abbeel, Joseph M. Hellerstein, Sanjay Krishnan, and Ion Stoica. 2019.\nDeep Unsupervised Cardinality Estimation. Proceedings of the VLDB Endowment\n13, 3 (Nov. 2019), 279‚Äì292. https://doi.org/10.14778/3368289.3368294\n3232\n",
  "textLength": 82744
}