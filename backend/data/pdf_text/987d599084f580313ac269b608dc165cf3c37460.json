{
  "paperId": "987d599084f580313ac269b608dc165cf3c37460",
  "title": "A Survey of Machine Learning-Based System Performance Optimization Techniques",
  "pdfPath": "987d599084f580313ac269b608dc165cf3c37460.pdf",
  "text": "applied  \nsciences \nReview\nA Survey of Machine Learning-Based System Performance\nOptimization Techniques\nHyejeong Choi\n and Sejin Park *\n/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045 /gid00001\n/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046\nCitation: Choi, H.; Park, S. A Survey\nof Machine Learning-Based System\nPerformance Optimization\nTechniques. Appl. Sci. 2021 ,11, 3235.\nhttps://doi.org/10.3390/app11073235\nReceived: 23 February 2021\nAccepted: 2 April 2021\nPublished: 4 April 2021\nPublisher’s Note: MDPI stays neutral\nwith regard to jurisdictional claims in\npublished maps and institutional afﬁl-\niations.\nCopyright: © 2021 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).Department of Computer Science, Keimyung University, Daegu 1095, Korea; hyejeong12311@gmail.com\n*Correspondence: baksejin@kmu.ac.kr; Tel.: +82-53-580-5270\nAbstract: Recently, the machine learning research trend expands to the system performance opti-\nmization ﬁeld, where it has still been proposed by researchers based on their intuitions and heuristics.\nCompared to conventional major machine learning research areas such as image or speech recog-\nnition, machine learning-based system performance optimization ﬁelds are at the beginning stage.\nHowever, recent papers show that this approach is promising and has signiﬁcant potential. This\npaper reviews 11 machine learning-based system performance optimization approaches from nine\nrecent papers based on well-known machine learning models such as perceptron, LSTM, and RNN.\nThis survey provides a detailed design and summarizes model, input, output, and prediction method\nof each approach. This paper covers various system performance areas from the data structure\nto essential system components of a computer system such as index structure, branch predictor,\nsort, and cache management. The result shows that machine learning-based system performance\noptimization has an important potential for future research. We expect that this paper shows a wide\nrange of applicability of machine learning technology and provides a new perspective for system\nperformance optimization.\nKeywords: deep learning; machine learning; system performance; optimization\n1. Introduction\nAs the amount of data and the complexity of data structures increase, computer system\nperformance optimization is highly required. However, traditional optimization techniques\ndo not work adaptively because most of them are designed for speciﬁc data under a speciﬁc\nenvironment. Hybrid techniques [ 1–3] came out but are still time-consuming and difﬁcult\nto obtain good results on complex data distribution.\nPrefetching or prediction techniques have already existed concepts, but they are\nlimited in time and space. In addition, traditional techniques only work well under\ncertain patterns or situations. For example, stride prefetcher [ 4] cannot learn various data\ncharacteristics such as delta [ 5]. That is to say, there is a possibility of space waste and\nperformance decline when irregular data come in. Also, depending on the data distribution,\nthe conventional workload or pattern-based algorithm leads bad if the data distribution\ndoes not ﬁt to the algorithm. Furthermore, existing techniques for handling large data such\nas database management and sorting have reached speed limits.\nTherefore, since it is difﬁcult to intuitively design a system that satisﬁes the increas-\ningly complex workload and performance metrics (latency, prediction) that the system\ntargets, Machine Learning was introduced to design the system architecture more auto-\nmatically. Machine learning can be a good solution because machine learning can ﬁnd the\nvarious relationship among data, such as linear, non-linear. This is the biggest advantage\nof a machine learning-based approach, and it can automatically explore patterns for a\ngiven workload.\nThe goal of this survey paper is to show the state-of-the-art machine learning tech-\nniques for various areas in terms of system performance optimization. It covers various\nmachine learning-based approaches from CPU prefetching to basic data structure.\nAppl. Sci. 2021 ,11, 3235. https://doi.org/10.3390/app11073235 https://www.mdpi.com/journal/applsci\n\nAppl. Sci. 2021 ,11, 3235 2 of 19\nIn the papers we reviewed, each topic or designed model is unique, but the main\npurpose is the same-system performance optimization using machine learning techniques.\nThis paper reviews 11 ML-based techniques from nine papers that have applied\nmachine learning to optimize various systems such as traditional database management,\ndata structure, sorting algorithms, etc. Each system has a target optimization direction,\nand by applying machine learning, problem-solving possibilities and future potential are\nconﬁrmed. In addition, there may be various models, approaches, and available data for\na given problem. Therefore, based on the challenge found in the conventional system,\nwe explore the design space for the ML component and analyze how to apply it to the\nconventional system.\nThe paper is organized as follows. Section 3 introduces recent works that applied ma-\nchine learning for system performance optimization. Section 4 describes the characteristics\nand architecture design of the works introduced in Section 3. Finally, Section 5 discusses\nthe challenges and future directions of applying ML for system designs and concludes\nthis paper.\n2. Background\nWe brieﬂy summarize the models introduced in this survey. Most approaches applied\nfour models.\nPerceptron receives an input signal and outputs one signal. Each input signal has\na weight, and this weight represents the importance of the input signal. Single-layer\nperceptron have only input and output stages, and multi-layer perceptron add a hidden\nlayer between input and output.\nSupport Vector Machine (SVM) is an algorithm similar to perceptron and is a method\nof deﬁning a decision boundary to obtain a baseline for classiﬁcation and maximizing the\nmargin between the decision boundary and the actual data.\nRecurrent Neural Network (RNN) has input signals, output signals, and weights like\nperceptron, but has several hidden layers inside, and these hidden layers are not only\naffected by the current input but are cyclically connected to memorize previous input\ninformation. The left side of Figure 1a is the RNN cell. To calculate the hidden state of the\ncurrent time t, it is calculated as follows using the hidden state ht\u00001of the previous time\nt\u00001and the input xt. Next, the calculated htvalue is sent to the output layer and becomes\nan input value for calculating ht+1again.\nAppl. Sci. 2021 , 11, x FOR PEER REVIEW 3 of 20 \n \n  \n(a) RNN cell ( b) LSTM cell \nFigure 1. (a) The Architecture of Recurrent Neural Network (RNN) cell; ( b) The Architecture of \nLong Short-term Memory (LSTM) cell. \n3. Review of Machine Learning-based System Performance Optimization Research \nThis section reviews machine learning-bas ed system performance optimization re-\nsearch. Table 1 is a summary of the paper before surveying the paper. \nTable 1. Summary of the reviewer searches. \nResearch Model Description \nLearned In-\ndex Struc-\ntures [7] B-Tree index \nSingle and Multi-\nlayer Perceptron It replaces a B-Tree with a neural networ k to improve query sp eed. A regression-based \nmodel predicts the position of the key if it is given as the input. The model is hierarchi-\ncally organized for higher accuracy. \nHash-Map index \nSingle and Multi-\nlayer Perceptron It replaces a hash-function with a neural network to solve the collision problem, which \nis the major problem of hashing technique. When a key is given as an input value, a re-\ngression-based model predicts the position of the key. This model implemented model \nhierarchically to reduce the complexity of a single model. \nBloom filter \nRNN It includes a neural network to minimize the spatial overhead and false positive rate of \nBloom filter. It designed a classi fication model that treats Bloom filter as a binary classi-\nfication problem to predict 0 or 1 when a key comes in. \nPavo [8] LSTM It replaces a hash-function with LSTM to increase the space utilization rate of inverted \nindexing. The architecture consists of a total of four stages: Input Stage is pre-pro-\ncessing data, Disperse Stage distributes the sub-models uniformly, Mapping Stage \nmaps the sub-data to a local hash, and the Join Stage creates a global hash table. \nLearned \nBranch Pre-\ndictor [9] Perceptron It replaces the traditional two-level scheme counter table with perceptron for efficient \nbranch prediction. Given input is indexed into one of the perceptron tables, and the \nmodel classifies whether the branch will be ta ken or will not be taken by a weight vec-\ntor. \nNN-sort [10] Multi-layer \nPerceptron It is a regression-based neural network to  improve sorting performance. The overall \nframework consists of three stages: the input phase is a step of pre-processing the input \ndata, the sorting phase is a step of sorting the data by repeatedly inputting data into the \nmodel. Finally, there is a polish phase to correct inaccurately sorted elements to pro-\nduce an accurate result. \nMER-sort \n[11] Perceptron It is a machine learning sorting algorithm-based radix sort. Similar to radix sort, this \nmodel proceeds sorting by separating the unordered keys into partial buckets.  \nFigure 1. (a) The Architecture of Recurrent Neural Network (RNN) cell; ( b) The Architecture of Long\nShort-term Memory (LSTM) cell.\nLong Short-term Memory (LSTM) [ 6] is a proposed method to solve the disadvantage\nthat the current hidden layer loses the memory of the previous input information when\nthe input of the RNN becomes long. In order to solve the problem, unnecessary memories\nare deleted by adding a value called cell state, and things to remember are selected. The\n\nAppl. Sci. 2021 ,11, 3235 3 of 19\nright side of Figure 1b is an LSTM cell. LSTM has three gates to obtain hidden state and\ncell state values. The ﬁrst forget gate ( ft) is a gate to delete memories. As the second input\ngate ( it,gt), it is a gate to remember the current information. Finally, the output gate ( ot)\nis a gate to calculate the hidden state at the current point in time. The cell state ctcan be\nobtained by multiplying two values ( it,gt) calculated at the input gate for each element and\nadding the memory selected at the input gate to the result of the forget gate. The hidden\nstate htcan be obtained by performing an entrywise product for the cell state ctand the\noutput gate ( ot).\n3. Review of Machine Learning-Based System Performance Optimization Research\nThis section reviews machine learning-based system performance optimization re-\nsearch. Table 1 is a summary of the paper before surveying the paper.\nTable 1. Summary of the reviewer searches.\nResearch Model Description\nLearned Index\nStructures [7]B-Tree index\nSingle and Multi-layer\nPerceptronIt replaces a B-Tree with a neural network to improve query speed. A\nregression-based model predicts the position of the key if it is given as the input.\nThe model is hierarchically organized for higher accuracy.\nHash-Map index\nSingle and Multi-layer\nPerceptronIt replaces a hash-function with a neural network to solve the collision problem,\nwhich is the major problem of hashing technique. When a key is given as an input\nvalue, a regression-based model predicts the position of the key. This model\nimplemented model hierarchically to reduce the complexity of a single model.\nBloom ﬁlter\nRNNIt includes a neural network to minimize the spatial overhead and false positive\nrate of Bloom ﬁlter. It designed a classiﬁcation model that treats Bloom ﬁlter as a\nbinary classiﬁcation problem to predict 0 or 1 when a key comes in.\nPavo [8] LSTMIt replaces a hash-function with LSTM to increase the space utilization rate of\ninverted indexing. The architecture consists of a total of four stages: Input Stage is\npre-processing data, Disperse Stage distributes the sub-models uniformly,\nMapping Stage maps the sub-data to a local hash, and the Join Stage creates a\nglobal hash table.\nLearned Branch\nPredictor [9]PerceptronIt replaces the traditional two-level scheme counter table with perceptron for\nefﬁcient branch prediction. Given input is indexed into one of the perceptron\ntables, and the model classiﬁes whether the branch will be taken or will not be\ntaken by a weight vector.\nNN-sort [10]Multi-layer\nPerceptronIt is a regression-based neural network to improve sorting performance. The\noverall framework consists of three stages: the input phase is a step of\npre-processing the input data, the sorting phase is a step of sorting the data by\nrepeatedly inputting data into the model. Finally, there is a polish phase to correct\ninaccurately sorted elements to produce an accurate result.\nMER-sort [11] PerceptronIt is a machine learning sorting algorithm-based radix sort. Similar to radix sort,\nthis model proceeds sorting by separating the unordered keys into partial buckets.\nLearned Reuse\nPredictor [12]PerceptronIt is a perceptron-based reuse predictor to improve the accuracy of reuse\nprediction. However, an actual neural network is not applied, it only brought an\nidea of a perceptron. The multiple inputs are given, and it is indexed each weight\ntable using input and PC, and it conducts prediction by adding each weight.\nLearned Cache\nPrefetching [13,14]PerceptronIt is a perceptron-based cache prefetching technique to reduce unnecessary\nprefetching. The basic conﬁguration is the traditional prefetcher that proposes the\nblock. It keeps the latest cache miss address, and perceptron determines whether\nthe block is accepted or not.\nLSTMIt is a neural network-based prefetcher considering data distribution. Two\nLSTM-based architectures are proposed. The ﬁrst is embedding LSTM. If the\nembedded Program Counter (PC) and delta (a distance between two addresses)\nare given as input, the delta value is predicted. The second is Clustering + LSTM.\nIt separates various cluster regions according to addresses and predicts deltas\nwithin the cluster.\n\nAppl. Sci. 2021 ,11, 3235 4 of 19\nTable 1. Cont.\nResearch Model Description\nLearned Cache\nReplacement\nPolicy [15]LSTM, SVMIt proposed an attention-based LSTM for ofﬂine prediction and SVM for online\nprediction. Although there are differences in the architecture of the two models,\nboth models work with the same input. This model classiﬁes whether the input is\ncache-friendly or not.\n3.1. Machine Learning Based Index Structures\nTraditional data management systems use a heuristic-based algorithm, which means\nthat they do not utilize the characteristics of speciﬁc applications and data themselves [ 16].\nIn order to improve these problems, the papers [ 7,8] introduce a new index structure\nthat leverages machine learning. In paper [ 7], three well-known index structures (B-Tree,\nHash-Map, Bloom ﬁlter) are re-designed with machine learning-based techniques. Ref. [ 8]\nalso proposes an inverted index based on machine learning.\n3.1.1. Learned B-Tree\nB-Tree index model predicts a position of look-up key within a sorted set of keys.\nOriginal B-Tree guarantees that found the key is the ﬁrst key or higher than the look-up\nkey. In addition, B-Tree determines whether the look-up key is in the page through binary\nsearch. In this paper [7], a regression-based range index is proposed.\nFor the Learned B-Tree design, the Recursive Model Index (RMI) [ 7] was designed to\nincrease accuracy and reduce complexity rather than a single CDF model. Because RMI\ntrains only for each range of data, such as B-Tree, RMI can easily build with a simple\nmodel. In addition, this paper supports B-Tree. If learning data distribution does not ﬁt for\nprediction, they use B-Tree.\nIn Figure 2, the upper-level model receives the key as input and predicts the next-level\nmodel until it is located at the lowest level. Upon reaching the lowest level model, the\nmodel predicts the location of the queried key and ﬁnds the queried key between min_err\nand max_err of the predicted location.\nAppl. Sci. 2021 , 11, x FOR PEER REVIEW 5 of 20 \n \n \nFigure 2. Learned B-Tree [7]: The top-level model receiv es a key as an input and it predicts the \nfinal position between min_err and max_err. \n3.1.2. Learned Hash-Map \nA Hash-Map uses hash-functions to map the position in the array. The goal of the \nlearned Hash-Map is to reduce the hash conflict [7]. To address the problem, they replace \nthe hash-function with a machine learning-based model. They used a regression-based \nmodel. The model is also modeled as CDF for th e key distribution, and it also uses RMI \nlike the range index. In Figure 3, when the trained model receives a key as an input, it \npredicts the position of the key in the array, similar to a hash function.  When the learned \nhash-function was compared with the existing hash-function, the collision rate of the \nlearned hash-function for the given dataset was reduced by up to 77%. \n \nFigure 3. Learned Hash-Map [7]: The top-level model rece ived the key as an input and it predicts \nthe final position in the leaf model. \n3.1.3. Learned Bloom Filter \nA Bloom filter is a structure that determines whether the data is a member of the set \nor not. By design, a Bloom filter is space effi cient because it checks whether the key exists \nat the position that K hash-function returned to the bit-array, but Bloom filter still occupies \na large amount of memory. Also, it has a fals e-positive result because of hash function \nconflict. The goal of this learned existence index [7] is to minimize the space and false \nFigure 2. Learned B-Tree [ 7]: The top-level model receives a key as an input and it predicts the ﬁnal\nposition between min_err and max_err.\nCompared with the original B-Tree under various conditions, the learning index model\nachieved much less memory consumption and faster look-up speed than B-Tree.\n\nAppl. Sci. 2021 ,11, 3235 5 of 19\n3.1.2. Learned Hash-Map\nA Hash-Map uses hash-functions to map the position in the array. The goal of the\nlearned Hash-Map is to reduce the hash conﬂict [ 7]. To address the problem, they replace\nthe hash-function with a machine learning-based model. They used a regression-based\nmodel. The model is also modeled as CDF for the key distribution, and it also uses RMI\nlike the range index. In Figure 3, when the trained model receives a key as an input, it\npredicts the position of the key in the array, similar to a hash function. When the learned\nhash-function was compared with the existing hash-function, the collision rate of the\nlearned hash-function for the given dataset was reduced by up to 77%.\nAppl. Sci. 2021 , 11, x FOR PEER REVIEW 5 of 20 \n \n \nFigure 2. Learned B-Tree [7]: The top-level model receiv es a key as an input and it predicts the \nfinal position between min_err and max_err. \n3.1.2. Learned Hash-Map \nA Hash-Map uses hash-functions to map the position in the array. The goal of the \nlearned Hash-Map is to reduce the hash conflict [7]. To address the problem, they replace \nthe hash-function with a machine learning-based model. They used a regression-based \nmodel. The model is also modeled as CDF for th e key distribution, and it also uses RMI \nlike the range index. In Figure 3, when the trained model receives a key as an input, it \npredicts the position of the key in the array, similar to a hash function.  When the learned \nhash-function was compared with the existing hash-function, the collision rate of the \nlearned hash-function for the given dataset was reduced by up to 77%. \n \nFigure 3. Learned Hash-Map [7]: The top-level model rece ived the key as an input and it predicts \nthe final position in the leaf model. \n3.1.3. Learned Bloom Filter \nA Bloom filter is a structure that determines whether the data is a member of the set \nor not. By design, a Bloom filter is space effi cient because it checks whether the key exists \nat the position that K hash-function returned to the bit-array, but Bloom filter still occupies \na large amount of memory. Also, it has a fals e-positive result because of hash function \nconflict. The goal of this learned existence index [7] is to minimize the space and false \nFigure 3. Learned Hash-Map [ 7]: The top-level model received the key as an input and it predicts\nthe ﬁnal position in the leaf model.\n3.1.3. Learned Bloom Filter\nA Bloom ﬁlter is a structure that determines whether the data is a member of the set\nor not. By design, a Bloom ﬁlter is space efﬁcient because it checks whether the key exists\nat the position that K hash-function returned to the bit-array, but Bloom ﬁlter still occupies\na large amount of memory. Also, it has a false-positive result because of hash function\nconﬂict. The goal of this learned existence index [ 7] is to minimize the space and false\npositive. One way to construct a Bloom ﬁlter is to think of it as a binary classiﬁcation model.\nTherefore, a predictive model has been proposed to determine whether an element is a key\nor not. However, since this model has a false negative, unlike the Bloom ﬁlter, the authors\nadd an overﬂow Bloom ﬁlter to keep the false-negative rate (FNR) at zero. In Figure 4, if a\nkey is given as input to the trained model and the predicted value by the model exceeds the\nthreshold, queried key is determined that the key exists in the set. However, if it does not\nexceed the threshold, the existence of the key is checked through the actual bloom ﬁlter. In\nan experiment to track blacklisted phishing URLs, Bloom Filter of 1% FPR needed 2.04 MB\nof memory, while the learned Bloom ﬁlter reduced the size of the required memory with\n1.31 MB of memory.\n\nAppl. Sci. 2021 ,11, 3235 6 of 19\nAppl. Sci. 2021 , 11, x FOR PEER REVIEW 6 of 20 \n \npositive. One way to construct a Bloom filter is to think of it as a binary classification \nmodel. Therefore, a predictive model has been proposed to determine whether an element \nis a key or not. However, since this model has a false negative, unlike the Bloom filter, the \nauthors add an overflow Bloom filter to keep the false-negative rate (FNR) at zero. In Fig-\nure 4, if a key is given as input to the trai ned model and the predicted value by the model \nexceeds the threshold, queried key is determin ed that the key exists in the set. However, \nif it does not exceed the threshold, the existence of the key is checked through the actual \nbloom filter.  In an experiment to track blacklisted phishing URLs, Bloom Filter of 1% FPR \nneeded 2.04 MB of memory, while the learned Bloom filter reduced the size of the required memory with 1.31 MB of memory. \n \nFigure 4. Bloom Filter Architecture [7]:  The Learned model received a key as an input and predicts \na regression value to determine if the key exists in the set or not. \n3.1.4. Learned Inverted Index \nAn inverted index is a technique that is wi dely used in a large-scale text search. In \ncontrast to forward index, words become key and documents become value (so, inverted). However, when indexing such a large-scale te xt, it is generally generated through the \nsame hash function without understanding the distribution of various data. In order to \nincrease the space utilization rate of the inve rted list when the large amount of data is \ngiven, this paper presents Pavo [8],  an RNN-based learned inverted index. \n \nFigure 5. Learned Inverted Index [8]: The RNN model rece ived the preprocessed data string as an \ninput and predicts the position of the data in the leaf RNN. \nFigure 4. Bloom Filter Architecture [ 7]: The Learned model received a key as an input and predicts a\nregression value to determine if the key exists in the set or not.\n3.1.4. Learned Inverted Index\nAn inverted index is a technique that is widely used in a large-scale text search. In\ncontrast to forward index, words become key and documents become value (so, inverted).\nHowever, when indexing such a large-scale text, it is generally generated through the same\nhash function without understanding the distribution of various data. In order to increase\nthe space utilization rate of the inverted list when the large amount of data is given, this\npaper presents Pavo [8], an RNN-based learned inverted index.\nPavo conducts four stages as shown in Figure 5. The ﬁrst stage, the input stage, is\na stage for pre-processing input data. The input data string is processed as bigram and\nused as the input sequence of the RNN. For example, the word “Documentaries” can be\ndivided into do, oc, cu, . . ., es when using bigram. The second stage, the disperse stage,\nconsists of a hierarchical model and plays a role to distribute data evenly among multiple\nmodels. By dividing the dataset into several sub-data sets, the low-level model helps to\nlearn the mapping relationship of the dataset. The third stage, the mapping stage, maps\neach data in the dataset of models in the last layer to a local hash. The last stage, the join\nstage, is connected by a local hash to create the ﬁnal hash-table. All local hashes are serially\nconnected to create a global hash table, and as shown in Figure 5, pos1 located in the\nlocal hash of RNN 3.2 is mapped to the global hash table to ﬁnally ﬁnd the location of the\nkey. This paper introduces two learning methods in the mapping stage: supervised and\nunsupervised learning methods. First, supervised learning aligns keys, then sets labels\nin order, and learns to map keys one-to-one with hash entries. Supervised learning hash-\nfunction tends to overﬁtting the model when the number of labels is small, or the noise\nis high [ 17]. Therefore, unsupervised learning is introduced. The second unsupervised\nlearning is a method of self-learning the distribution of data without labels. When each\nkey is input to the model, the model outputs a vector of hash table length. Next, the vector\nis the input to the Softmax function [ 18]. It ﬁnds the position of the largest value in the\noutput value of Softmax and sets the position’s value to 1. At the end of the stage, to\nobtain a distribution, the value of each category is summed, and the model optimized the\ncomputed error through a loss function to distribute the data uniformly. When mapping\nsize is 1000, the average number of look-up of learned hash-function from the entire dataset\nwas approximately 1.0–1.15, which was lower than 1.5 of the existing hash-function.\n\nAppl. Sci. 2021 ,11, 3235 7 of 19\n1 \n  \n \n \n \nFigure 5. Learned Inverted Index [ 8]: The RNN model received the preprocessed data string as an\ninput and predicts the position of the data in the leaf RNN.\n3.2. Perceptron Based Branch Predictor\nThe existing 2-bit branch prediction [ 19] stores two-level scheme branch history infor-\nmation for branch prediction. In order to predict, a pattern history table (PHT) is used. To\nimprove branch accuracy, a larger table is needed. In order to reduce the space overhead, it\nshares counters, but duplicates may occur. Because of these disadvantages, history length\nis limited, and the predictor is difﬁcult to learn a long history. Perceptron is a simple\nneural network, but it has the advantage of learning a long history. This paper proposed a\ntwo-level scheme using perceptron instead of a 2-bit counter [ 9]. A detailed description of\nthis structure is shown in Figure 6. If a branch address is given, the address is hashed to\nobtain an index. The index obtained by hashing the branch address indexes the entry of\nthe Perceptron Vector Table. The entry has a weight and a bias indicating the correlation\nbetween branch addresses. The extracted weights and biases are dot products with the\nGlobal Branch History that stores the past branch address to calculate the output value y. If\ny is negative, the branch is non-taken, otherwise it is taken. Finally, when the actual branch\nresult appears, the weight is updated by comparing it with the predicted branch result.\nWhen comparing the prediction rate according to the hardware budget, learned branch\npredictor improved 14.7% over gshare [20] and 10% over bi-mode [21].\nAppl. Sci. 2021 , 11, x FOR PEER REVIEW 8 of 20 \n \n \nFigure 6. Perceptron Predictor Block Di agram [9]: After selecting th e parameter corresponding to \nthe branch address, compute dot product the parame ter with the previous br anch history, and the \npredictor decides if the branch is taken or not based on the result.3.3. Perceptron based Branch \nPredictor. \n3.3. Machine Learning based Sort \nSorting is one of the basic tasks of computing components used in many applications. \nRecently, machine learning is applied to solv e the problem because the speed of the algo-\nrithm has reached the limit. \n3.3.1. NN-Sort (Neural Network Sort) \nIn paper [10], a neural network-based sort ing algorithm called NN-sort [10] is pro-\nposed. NN-sort trains the model about past data and classifies incoming data in the future. \nThe overall structure of NN-sort is divided into  three phases as shown in Figure 7: Input \nPhase, Sorting Phase, and Polish Phase. \n \nFigure 7. NN-Sort (Neural Network Sort) [10]. \nInput Phase serves to convert data into a vector so that NN-sort can learn the various \ntype of data. \nSorting Phase serves to convert unsorted data  into roughly sorted data by repeatedly \nexecuting the proposed model. If different inpu t data result in the same position, NN-sort \nstores the input data in the conflicting array c. On the other hand, the non-conflicting key \nFigure 6. Perceptron Predictor Block Diagram [ 9]: After selecting the parameter corresponding to\nthe branch address, compute dot product the parameter with the previous branch history, and the\npredictor decides if the branch is taken or not based on the result.\n\nAppl. Sci. 2021 ,11, 3235 8 of 19\n3.3. Machine Learning Based Sort\nSorting is one of the basic tasks of computing components used in many applications.\nRecently, machine learning is applied to solve the problem because the speed of the\nalgorithm has reached the limit.\n3.3.1. NN-Sort (Neural Network Sort)\nIn paper [ 10], a neural network-based sorting algorithm called NN-sort [ 10] is pro-\nposed. NN-sort trains the model about past data and classiﬁes incoming data in the future.\nThe overall structure of NN-sort is divided into three phases as shown in Figure 7: Input\nPhase, Sorting Phase, and Polish Phase.\nAppl. Sci. 2021 , 11, x FOR PEER REVIEW 8 of 20 \n \n \nFigure 6. Perceptron Predictor Block Di agram [9]: After selecting th e parameter corresponding to \nthe branch address, compute dot product the parame ter with the previous br anch history, and the \npredictor decides if the branch is taken or not based on the result.3.3. Perceptron based Branch \nPredictor. \n3.3. Machine Learning based Sort \nSorting is one of the basic tasks of computing components used in many applications. \nRecently, machine learning is applied to solv e the problem because the speed of the algo-\nrithm has reached the limit. \n3.3.1. NN-Sort (Neural Network Sort) \nIn paper [10], a neural network-based sort ing algorithm called NN-sort [10] is pro-\nposed. NN-sort trains the model about past data and classifies incoming data in the future. \nThe overall structure of NN-sort is divided into  three phases as shown in Figure 7: Input \nPhase, Sorting Phase, and Polish Phase. \n \nFigure 7. NN-Sort (Neural Network Sort) [10]. \nInput Phase serves to convert data into a vector so that NN-sort can learn the various \ntype of data. \nSorting Phase serves to convert unsorted data  into roughly sorted data by repeatedly \nexecuting the proposed model. If different inpu t data result in the same position, NN-sort \nstores the input data in the conflicting array c. On the other hand, the non-conflicting key \nFigure 7. NN-Sort (Neural Network Sort) [10].\nInput Phase serves to convert data into a vector so that NN-sort can learn the various\ntype of data.\nSorting Phase serves to convert unsorted data into roughly sorted data by repeatedly\nexecuting the proposed model. If different input data result in the same position, NN-sort\nstores the input data in the conﬂicting array c. On the other hand, the non-conﬂicting key\nis stored in the array ok. In this case, c is used as input of model f at the next iteration. If\nc is below the threshold after one iteration, cis not supplied from model f again, but cis\nsorted by the existing algorithm.\nAt Polish Phase, all arrays of fo1,o2, . . . , ok, . . . , otg(0<t<e,eis the maximum\nnumber of iterations) are scanned and merged with w. If the position of an element in\noiis incorrect, NN-sort inserts the element in the correct position and if the position of\nthe element is correct, NN-sort appends the element in the result array. Compared to the\nreal-world dataset, NN-sort shows a sorting rate of 5950 data points per second, which is\n2.72 times of std::sort [22], 7.34 times of Redis sort [23], and 58% faster than std::heap [22].\n3.3.2. MER-Sort (Model-Enhanced Radix Sort)\nMachine learning sorting algorithm-based radix sort with almost linear time called\nMER-sort has been proposed [ 11]. When ndata are given, radix sort performs bucket\nsorting of k phases with time complexity of O(kn)[24]. MER-sort is also similar to radix\nsort because MER-sort divides the bucket until the input is sorted. But MER-sort learns the\nCDF model, and the learned model predicts the position [11].\nFigure 8 shows the structure of MER-sort. In Step 1A, the model predicts the bucket\nwhere the input will be located when the unordered input array comes in. At this time,\nwhen the bucket overﬂows, the input enters the spill bucket. In Step 1B, the model repeats\nthe same mapping operation until the number of inputs per bucket is less than the threshold\n(typically 100). Next, for a bucket of variable size, MER-sort determines how many inputs\n\nAppl. Sci. 2021 ,11, 3235 9 of 19\neach bucket has. The number of inputs per bucket is stored in Count Array. In addition, a\nbucket of variable size can be stored directly in the output array (Step 2). Finally, the spill\nbucket is merged with the output array, and insert sort is used for modiﬁcation (Step 3). At\nup to 1 billion keys of standard normal distribution, MER-sort showed only 30% of sorting\nspeed compared to other algorithms [11].\nAppl. Sci. 2021 , 11, x FOR PEER REVIEW 9 of 20 \n \nis stored in the array ݋௞. In this case, c is used as input of model f at the next iteration. If c \nis below the threshold after one iteration, ܿ is not supplied from model f again, but ܿ is \nsorted by the existing algorithm. \nAt Polish Phase, all arrays of {݋ଵ,݋ଶ,…,݋ ௞,…,݋ ௧}(0  <  ߳,߳ < ݐ is the maximum num-\nber of iterations) are scanned and merged with ݓ .If the position of an element in ݋௜ is \nincorrect, NN-sort inserts the element in the correct position and if the position of the \nelement is correct, NN-sort appends the element in the result array. Compared to the real-\nworld dataset, NN-sort shows a sorting rate of 5950 data points per second, which is 2.72 \ntimes of std::sort [22], 7.34 times of Redis sort [23], and 58% faster than std::heap [22]. \n3.3.2. MER-Sort (Model-Enhanced Radix Sort) \nMachine learning sorting algorithm-based radi x sort with almost linear time called \nMER-sort has been proposed [11]. When n data are given, radix sort performs bucket sort-\ning of k phases with time complexity of )݊݇(ܱ  [24]. MER-sort is also similar to radix sort \nbecause MER-sort divides the bucket until th e input is sorted. But MER-sort learns the \nCDF model, and the learned model predicts the position [11]. \n \nFigure 8. MER-sort (Model-Enhanced Radix Sort) [11]. \nFigure 8 shows the structure of MER-sort. In  Step 1A, the model predicts the bucket \nwhere the input will be located when the unordered input array comes in. At this time, \nwhen the bucket overflows, the input enters th e spill bucket. In Step 1B, the model repeats \nthe same mapping operation until the number of  inputs per bucket is less than the thresh-\nold (typically 100). Next, for a bucket of va riable size, MER-sort determines how many \ninputs each bucket has. The number of inputs per bucket is stored in Count Array. In \naddition, a bucket of variable size can be stored directly in the output array (Step 2). Fi-\nnally, the spill bucket is merged with the output array, and insert sort is used for modifi-cation (Step 3). At up to 1 billion keys of standard normal distribu tion, MER-sort showed \nonly 30% of sorting speed compared to other algorithms [11]. \n3.4. Machine Learning based Cache Management \nThe speed of a CPU is far ahead of the memory latency. This problem causes a bot-\ntleneck in many computer applications [25]. A hierarchical memory system was con-structed with the addition of cache memory  to overcome between the speed of memory \naccess and the speed of the CPU. Since cache memory is even smaller than main memory \ncapacity, cache memory must be  managed efficiently. Therefor e, various techniques such \nas cache replacement, prefetching, etc. were introduced to increase efficiency. Since these \ntraditional techniques are speculative, they should ideally predict future patterns. How-ever, predictors are almost impossible to know the pattern of the future, so one way is to \nlearn memory patterns of the past. \n3.4.1. Reuse Prediction \nFigure 8. MER-sort (Model-Enhanced Radix Sort) [11].\n3.4. Machine Learning Based Cache Management\nThe speed of a CPU is far ahead of the memory latency. This problem causes a bottle-\nneck in many computer applications [ 25]. A hierarchical memory system was constructed\nwith the addition of cache memory to overcome between the speed of memory access and\nthe speed of the CPU. Since cache memory is even smaller than main memory capacity,\ncache memory must be managed efﬁciently. Therefore, various techniques such as cache\nreplacement, prefetching, etc. were introduced to increase efﬁciency. Since these traditional\ntechniques are speculative, they should ideally predict future patterns. However, predictors\nare almost impossible to know the pattern of the future, so one way is to learn memory\npatterns of the past.\n3.4.1. Reuse Prediction\nCache block reuse prediction is a technique to predict whether the block currently\nresiding in the cache is likely to be accessed again before being replaced. Previous tech-\nniques have not signiﬁcantly combined various input [ 12]. The proposed model is possible\nto increase cache efﬁciency by weighting various inputs and analyzing correlations with\neach other. This paper does not use perceptron algorithm but uses a similar perceptron\nlearning algorithm [ 12]. In this paper [ 12], when a block is accessed a predictor works. If\nthe block was in the cache (i.e., cache hit), the predictor predicts the possibility of reusing\nin the near future. If the block is predicted as it will not be reused, the block will not be\nstored in the cache again and vice versa.\nFigure 9 shows the structure of the predictor presented in the paper [ 12]. The predictor\nuses six features: the memory access trace and the PC of the memory instruction that\ncaused the eviction, and the memory address bits. Each feature has its own table and\nindexes into the corresponding entry to the hash by applying the XOR gate to each feature\nwith the PC of the instruction. This predictor is similar to perceptron predictor known as\nhashed perceptron [ 26]. Next, when the weights of the feature table are extracted through\nthe calculated index and the weights extracted from each table are added to exceed the\nthreshold, it is determined that the accessed block is not reused. If the sum of corresponding\nweights exceeds the threshold, the predictor is predicted that the prediction is incorrect.\nIn a single-thread workload, the average Sampling Dead Block Prediction (SDBP) [ 27]\n(3.5%), Signature-based Hit Predictor (SHiP) [28] (3.8%), and Perceptron (6.1%) improved\ncompared to the LRU when the predictor’s introduction per cycle (IPC) by the LRU’s IPC.\n\nAppl. Sci. 2021 ,11, 3235 10 of 19\nAppl. Sci. 2021 , 11, x FOR PEER REVIEW 10 of 20 \n \nCache block reuse prediction is a technique to predict whether the block currently \nresiding in the cache is likely to be accesse d again before being re placed. Previous tech-\nniques have not significantly combined various input [12]. The proposed model is possi-\nble to increase cache efficiency by weightin g various inputs and analyzing correlations \nwith each other. This paper does not use perceptron algorithm but uses a similar percep-\ntron learning algorithm [12]. In this paper [12], when a block is accessed a predictor works. \nIf the block was in the cache (i.e., cache hit), the predictor predicts the possibility of reusing \nin the near future. If the block is predicted as it will not be reused, the block will not be \nstored in the cache again and vice versa. \n \nFigure 9. Perceptron Reuse Predictor [12]: After XORing  the PC and each fe ature and adding all \nindexed weights, the predictor predicts the reus ability of the block based on the added weights. \nFigure 9 shows the structure of the predictor presented in the paper [12]. The predic-\ntor uses six features: the memory access trace and the PC of the memory instruction that \ncaused the eviction, and the memory address bits. Each feature has its own table and in-\ndexes into the corresponding entry to the hash by applying the XOR gate to each feature with the PC of the instruction. This predictor is similar to perceptron predictor known as \nhashed perceptron [26]. Next, when the weight s of the feature table are extracted through \nthe calculated index and the weights extracted from each table are added to exceed the \nthreshold, it is determined that the accessed block is not reused. If the sum of correspond-\ning weights exceeds the threshold, the predictor is predicted that the prediction is incor-rect. \nIn a single-thread workload, the average Sampling Dead Block Prediction (SDBP) [27] \n(3.5%), Signature-based Hit Predictor (SHiP) [2 8] (3.8%), and Perceptron (6.1%) improved \ncompared to the LRU when the predictor’s introduction per cycle (IPC) by the LRU’s IPC. \n3.4.2. Perceptron based Prefetcher \nPrefetching is the process of fetching data  from memory into the cache before the \nprocessor demands it. If the prediction is wron g, cache pollution occurs, and the efficiency \nmay be reduced because additional work is  required. To remove this unnecessary \nmemory request, a two-level prefetching me chanism based on perceptron has been pro-\nposed [13]. The first level combines the existing  prefetchers (stride prefetcher [4], Markov \nprefetcher [29]) and Global History Buffer (GHB ) [2]. In Figure 10, if a cache miss occurs, \nthe model pushes the block to the GHB stack,  and the convolutional prefetcher proposes \na block. Next, after finding the proposed block in the index table, it is hashed to the GHB \nto extract the features related to the proposed block and input it to the perceptron. Per-\nceptron performs prediction based on the input features and does not prefetch the corre-\nsponding block if the predicted value is nega tive. Otherwise, it prefetches the proposed \nFigure 9. Perceptron Reuse Predictor [ 12]: After XORing the PC and each feature and adding all\nindexed weights, the predictor predicts the reusability of the block based on the added weights.\n3.4.2. Perceptron Based Prefetcher\nPrefetching is the process of fetching data from memory into the cache before the\nprocessor demands it. If the prediction is wrong, cache pollution occurs, and the efﬁ-\nciency may be reduced because additional work is required. To remove this unnecessary\nmemory request, a two-level prefetching mechanism based on perceptron has been pro-\nposed [13]. The ﬁrst level combines the existing prefetchers (stride prefetcher [4], Markov\nprefetcher [ 29]) and Global History Buffer (GHB) [ 2]. In Figure 10, if a cache miss occurs,\nthe model pushes the block to the GHB stack, and the convolutional prefetcher proposes a\nblock. Next, after ﬁnding the proposed block in the index table, it is hashed to the GHB to\nextract the features related to the proposed block and input it to the perceptron. Perceptron\nperforms prediction based on the input features and does not prefetch the corresponding\nblock if the predicted value is negative. Otherwise, it prefetches the proposed block. Also,\ndepending on the result, the block is added to the Deny Table or Accept Table. Both tables\nare used as perceptron0s training data to adjust memory access patterns. Compared with\nStride, Markov Prefetcher, an average of 80.84% of proposed requests by stride prefetcher\nwas rejected, and an average of 49.71% of proposed requests by Markov prefetcher was\nrejected. However, the cache hit rate was \u00001.67 to 2.46% because even useful proposals\ncould be rejected.\nAppl. Sci. 2021 , 11, x FOR PEER REVIEW 11 of 20 \n \nblock. Also, depending on the result, the block is added to the Deny Table or Accept Table. \nBoth tables are used as perceptron ′s training data to adjust memory access patterns. Com-\npared with Stride, Markov Prefetcher, an averag e of 80.84% of proposed  requests by stride \nprefetcher was rejected, and an average of 49.71% of proposed requests by Markov prefetcher was rejected. However, the cache hit rate was −1.67 to 2.46% because even use-\nful proposals could be rejected. \n \nFigure 10. Two Level Prefetcher [13]: In the first level prefetcher, the feature related to the block \nproposed by the existing prefet cher is transferred to the perc eptron, and in the second level \nprefetcher, the proposal is accepted or denied based on the value predic ted by the perceptron. \n3.4.3. LSTM based Prefetcher \nPrefetch’s role is to determine what data needs to be cached to ease the memory wall \n[30]. For prefetch performance, many existing tasks rely on tables [2,4,5,29]. However, as \nthe workload increases, the table also must expand, which puts pressure on the hardware. \nConsidering that the previous long history will affect the current prefetching, this paper \nproposes a prefetcher using a sequence-based neural network. In this paper [14], two de-\nsigns are introduced. \nThe first is Embedding LSTM, the second is Clustering + LSTM. In the Embedding \nLSTM (see Figure 11), the PC and the delt a are individually embedded, and the two fea-\ntures are connected to become a two-layer LSTM input. Then, the model outputs the top-\n10 deltas for each time stamp and selects the highest probability delta among the last \ntimestamp K. \n \nFigure 11. The Embedding LSTM Model [14]. \nClustering + LSTM (see Figure 12) is built on the assumption that the interaction be-\ntween addresses occurs locally. Therefore, th e address space is clustered using k-means. \nThe PC and Address are divided into each cl uster, and the delta between adjacent ad-\ndresses within each cluster is calculated. Assu ming that there are two clusters in Figure \n12, ܥܲଵ~ே and ݎ݀݀ܣ ଵ~ே are clustered, and the PC and De lta existing in the first cluster \nare called ܥܲ ଵ,௡ and ܽݐ݈݁ܦ ଵ,௠. This separate set of data is embedded with appropriate \nvalues, and the two embedding values are conc atenated to become the input of the LSTM. \nFigure 10. Two Level Prefetcher [ 13]: In the ﬁrst level prefetcher, the feature related to the block\nproposed by the existing prefetcher is transferred to the perceptron, and in the second level prefetcher,\nthe proposal is accepted or denied based on the value predicted by the perceptron.\n\nAppl. Sci. 2021 ,11, 3235 11 of 19\n3.4.3. LSTM Based Prefetcher\nPrefetch’s role is to determine what data needs to be cached to ease the memory\nwall [ 30]. For prefetch performance, many existing tasks rely on tables [ 2,4,5,29]. However,\nas the workload increases, the table also must expand, which puts pressure on the hardware.\nConsidering that the previous long history will affect the current prefetching, this paper\nproposes a prefetcher using a sequence-based neural network. In this paper [ 14], two\ndesigns are introduced.\nThe ﬁrst is Embedding LSTM, the second is Clustering + LSTM. In the Embedding\nLSTM (see Figure 11), the PC and the delta are individually embedded, and the two\nfeatures are connected to become a two-layer LSTM input. Then, the model outputs the\ntop-10 deltas for each time stamp and selects the highest probability delta among the last\ntimestamp K.\nAppl. Sci. 2021 , 11, x FOR PEER REVIEW 11 of 20 \n \nblock. Also, depending on the result, the block is added to the Deny Table or Accept Table. \nBoth tables are used as perceptron ′s training data to adjust memory access patterns. Com-\npared with Stride, Markov Prefetcher, an averag e of 80.84% of proposed  requests by stride \nprefetcher was rejected, and an average of 49.71% of proposed requests by Markov prefetcher was rejected. However, the cache hit rate was −1.67 to 2.46% because even use-\nful proposals could be rejected. \n \nFigure 10. Two Level Prefetcher [13]: In the first level prefetcher, the feature related to the block \nproposed by the existing prefet cher is transferred to the perc eptron, and in the second level \nprefetcher, the proposal is accepted or denied based on the value predic ted by the perceptron. \n3.4.3. LSTM based Prefetcher \nPrefetch’s role is to determine what data needs to be cached to ease the memory wall \n[30]. For prefetch performance, many existing tasks rely on tables [2,4,5,29]. However, as \nthe workload increases, the table also must expand, which puts pressure on the hardware. \nConsidering that the previous long history will affect the current prefetching, this paper \nproposes a prefetcher using a sequence-based neural network. In this paper [14], two de-\nsigns are introduced. \nThe first is Embedding LSTM, the second is Clustering + LSTM. In the Embedding \nLSTM (see Figure 11), the PC and the delt a are individually embedded, and the two fea-\ntures are connected to become a two-layer LSTM input. Then, the model outputs the top-\n10 deltas for each time stamp and selects the highest probability delta among the last \ntimestamp K. \n \nFigure 11. The Embedding LSTM Model [14]. \nClustering + LSTM (see Figure 12) is built on the assumption that the interaction be-\ntween addresses occurs locally. Therefore, th e address space is clustered using k-means. \nThe PC and Address are divided into each cl uster, and the delta between adjacent ad-\ndresses within each cluster is calculated. Assu ming that there are two clusters in Figure \n12, ܥܲଵ~ே and ݎ݀݀ܣ ଵ~ே are clustered, and the PC and De lta existing in the first cluster \nare called ܥܲ ଵ,௡ and ܽݐ݈݁ܦ ଵ,௠. This separate set of data is embedded with appropriate \nvalues, and the two embedding values are conc atenated to become the input of the LSTM. \nFigure 11. The Embedding LSTM Model [14].\nClustering + LSTM (see Figure 12) is built on the assumption that the interaction\nbetween addresses occurs locally. Therefore, the address space is clustered using k-means.\nThe PC and Address are divided into each cluster, and the delta between adjacent addresses\nwithin each cluster is calculated. Assuming that there are two clusters in Figure 12, PC1\u0018N\nand Addr 1\u0018Nare clustered, and the PC and Delta existing in the ﬁrst cluster are called\nPC1,nandDelta 1,m. This separate set of data is embedded with appropriate values, and the\ntwo embedding values are concatenated to become the input of the LSTM. At this time, to\nreduce the size of the model, multi-task LSTM is used to model each cluster. LSTMs are\nan independent model, but the weights of model tie and cluster ID provide to give a bias.\nBecause this structure can calculate delta within the address set, the vocabulary size can\nbe signiﬁcantly reduced compared to Embedding LSTM. However, because Clustering +\nLSTM trains the model in a separate address region, it has a disadvantage that access to\nother regions cannot be handled.\nAppl. Sci. 2021 , 11, x FOR PEER REVIEW 12 of 20 \n \nAt this time, to reduce the size of the model , multi-task LSTM is used to model each clus-\nter. LSTMs are an independent model, but the weights of model tie and cluster ID provide \nto give a bias. Because this structure can calc ulate delta within the address set, the vocab-\nulary size can be significantly reduced co mpared to Embedding LSTM. However, because \nClustering + LSTM trains the model in a separate address region, it has a disadvantage \nthat access to other regions cannot be handled. \n \nFigure 12. The Clustering + LSTM Model [14]. \n3.4.4. Cache Replacement Policy \nCache replacement is a problem of determini ng which blocks to remove when a new \naccess is requested under the “cache capacity is full” condition. Cache replacement is im-\nportant to replace the block so that cache hit ratio is not violated. Ex isting heuristic-based \nreplacement algorithms have a disadvantage th at they cannot adapt to data changes. To \nsolve this disadvantage, this paper [15] prop oses two neural network models as a solution. \nThe first model is an attention-based LSTM, which is an offline learning, and the second \nmodel is an integer Support Vector Machine (ISVM), called Glider, which enables offline \nand online learning. The two models are Hawkeye [31]-based prediction approach, which learns optimal caching behavior as supervised learning model. The first model, an atten-\ntion-based LSTM, sees the problem as a sequ ence labeling problem. The detailed architec-\nture consists of embedding, single layer LSTM, and attention layer as shown in Figure 13. \nThe embedding layer is the task of processing  the input into meaningful data, the LSTM \nlayer learns the behavior of the cache, and the attention layer learns the correlation be-\ntween PCs. The output is a binary value whic h predicts whether the PC sequence is cache-\nfriendly or not. \nFigure 12. The Clustering + LSTM Model [14].\n3.4.4. Cache Replacement Policy\nCache replacement is a problem of determining which blocks to remove when a new\naccess is requested under the “cache capacity is full” condition. Cache replacement is\nimportant to replace the block so that cache hit ratio is not violated. Existing heuristic-\n\nAppl. Sci. 2021 ,11, 3235 12 of 19\nbased replacement algorithms have a disadvantage that they cannot adapt to data changes.\nTo solve this disadvantage, this paper [ 15] proposes two neural network models as a\nsolution. The ﬁrst model is an attention-based LSTM, which is an ofﬂine learning, and the\nsecond model is an integer Support Vector Machine (ISVM), called Glider, which enables\nofﬂine and online learning. The two models are Hawkeye [ 31]-based prediction approach,\nwhich learns optimal caching behavior as supervised learning model. The ﬁrst model, an\nattention-based LSTM, sees the problem as a sequence labeling problem. The detailed\narchitecture consists of embedding, single layer LSTM, and attention layer as shown in\nFigure 13. The embedding layer is the task of processing the input into meaningful data, the\nLSTM layer learns the behavior of the cache, and the attention layer learns the correlation\nbetween PCs. The output is a binary value which predicts whether the PC sequence is\ncache-friendly or not.\n1 \n  \n \n \n \nFigure 13. Attention-based LSTM Model [ 15]: LSTM received a sequence of PC recurrently as an\ninput and predicts the cache priority for current PC.\nThe second model, Glider simpliﬁes feature by designing the PC sequence as a k-\nsparse binary to simplify the model as shown in Figure 14. This feature has the advantage\nof not having to learn the order of the sequences. Glider consists of a PC History Register\n(PCHR) and an ISVM table. PCHR keeps the last 5 PCs seen from each core in any order,\nand is modeled as an LRU cache to keep the most recent 5 PCs. ISVM stores the weight of\neach PC and consists of 16 weights. When the current PC is accessed, it creates an index by\nhashing the current PC and ﬁnds the corresponding ISVM entry in the ISVM table. And,\nbased on the indexed ISVM and the PC existing in the current PCHR, a 4-bit hash is created\nto ﬁnd the ﬁve weights. Five weights are extracted through ﬁve indexes. The output is\ncalculated by adding the extracted ﬁve weights. If the output is above the threshold, the\nmodel predicts the block as cache-friendly. In this case, it is inserted with a high-priority.\nIf the output is less than 0, the model predicts the block as cache-averse. In this case, the\nblock is inserted with low-priority. If the output is between 0 and the threshold, the model\npredicts the block as cache-friendly, but it is inserted with a mid-priority.\n\nAppl. Sci. 2021 ,11, 3235 13 of 19\nAppl. Sci. 2021 , 11, x FOR PEER REVIEW 13 of 20 \n \n \nFigure 13. Attention-based LSTM Model [15]: LSTM receiv ed a sequence of PC recurrently as an \ninput and predicts the cache priority for current PC. \n \nFigure 14. The Glider Predictor [15]: Glider selects the ISVM as a currently accessed PC and deter-\nmines the cache priority based on the sum of the weights associated with the previously accessed \nPC. \nThe second model, Glider simplifies featur e by designing the PC sequence as a k-\nsparse binary to simplify the model as shown in Figure 14. This feature has the advantage \nof not having to learn the order of the sequence s. Glider consists of a PC History Register \n(PCHR) and an ISVM table. PCHR keeps the last 5 PCs seen from each core in any order, \nand is modeled as an LRU cache to keep the most  recent 5 PCs. ISVM stores the weight of \neach PC and consists of 16 weights. When the current PC is accessed, it creates an index \nby hashing the current PC and finds the corresponding ISVM entry in the ISVM table. \nAnd, based on the indexed ISVM and the PC ex isting in the current PCHR, a 4-bit hash is \nFigure 14. The Glider Predictor [ 15]: Glider selects the ISVM as a currently accessed PC and\ndetermines the cache priority based on the sum of the weights associated with the previously\naccessed PC.\nWhen evaluating the online model, Glider showed an average accuracy improvement\nof 88.8 vs. 84.9% compared to the latest online model Hawkeye.\n4. Evaluation and Design Consideration\nThis section compares the learned models explained in Section 2 in various perspec-\ntives and explores the design considerations.\n4.1. Evaluation\nBased on the above-mentioned studies, we evaluate the characteristics that can be\nobserved by dividing into three. First, the machine learning model must be able to general-\nize not only the training data but also various data. Therefore, when applying machine\nlearning to an existing system, we evaluate the generalization ability of a machine learning\nmodel. Second, we compares the characteristics found by applying machine learning\nto the existing computer system with the characteristics of the existing system. Finally,\nwe compare the characteristics of each machine learning model applied to the studies\nmentioned above.\n4.1.1. Data Distribution\nAll the above models explain in Section 2 show that they obtain a lot of beneﬁts by\nlearning the distribution of the data. However, since these models are supervised learning\nmodels, they rely on the distribution of the training data.\nWe can compare NN-sort with std::sort by data distribution. For example, NN-\nSort trains well by uniform distribution data. When the target data set contains regular\ndistribution data more than 45%, we can see that std::sort shows better performance than\nNN-Sort since NN-sort was trained by uniform distribution data [10].\nThe perceptron-based branch predictor is a model in which perceptron trains a linear\nrelationship. When prediction data is non-linear, gshare [ 20] that can learn non-linear\ndistribution by applying address and history to XOR gate showed better performance than\nperceptron-based branch predictor.\n4.1.2. Machine Learning Model and Traditional Model\nBased on the reviewed papers, the advantages and disadvantages that can be identiﬁed\nwhen replacing the traditional model with machine learning are summarized. Compared\nto the traditional B-Tree, Learned B-Tree showed improved results that were up to 1.5~3 \u0002\nfaster and up to 2 \u0002less spatially [ 7], but since Learned B-Tree is trained with training\ndata, it works well only in read operations. Therefore, when a new input comes in, it is\nnecessary to retrain Learning B-Tree. This means that the trained model does not reﬂect the\n\nAppl. Sci. 2021 ,11, 3235 14 of 19\ndiversity of data. Two Level Prefetcher [ 13] uses perceptron to deny unnecessary proposals\nfrom traditional prefetcher. Although Two Level Prefetcher increased accuracy by denying\nunnecessary decisions, it reduced the cache hit rate slightly because the perceptron can\ndeny the required requests, and the prefetch may not be done during the perceptron\ntraining. Learned Prefetcher [ 14] designed an LSTM-based prefetcher and compared to two\nhardware prefetchers for experimentation. In various application traces, the LSTM-based\nprefetchers showed high accuracy and recall compared to two traditional prefetchers, but\nthere were no usability experiments to see if the training model could learn different types\nof traces. In view of this aspect, machine learning shows an improvement in accuracy\nwhen compared to traditional models, but machine learning is limited by the training data\nand may not be able to guarantee timeliness due to the training time. Table 2 summarizes\ntwo pros and cons.\nTable 2. Difference between machine learning model and traditional model.\nAdvantage Disadvantage\nMachine Learning Model(1) Can learn various data distribution\naccording to the complexity or\ncharacteristics of the model.(1) Results depends on training datasets\n(2) It takes time to generate a prediction model.\nTraditional Model(1) Fast result (The model is already\nimplemented, and results are quickly out).\n(2) Not sensitive to data distribution.(1) The model usually shows a consistent result,\nbut only a certain class shows better\nperformance.\n4.1.3. Perceptron and LSTM\nPerceptron and LSTM are used in the reviewed papers. It is difﬁcult to ﬁnd a suitable\nnetwork structure for the technology, but it is important to select a suitable neural network.\nBased on the paper introduced in Section 2, Table 3 summarizes the characteristics, advan-\ntages, and disadvantages of the two models. This is assumed that all models are computed\nin a single layer. The model parameter means a parameter whose value changes during\ntraining, such as weight. Operation refers to the computation method of neural networks.\nTable 3. Properties and Advantages and Disadvantages of Perceptron and LSTM.\nPerceptron LSTM\nModel\nparameterProperties Proportional to the number of inputs. Parameters exist for each gate as well as input.\nAdvantage\u000f Fast training\n\u000f Small model sizeThe relationship of inputs can be saved in\nmore detail.\nDisadvantage Difﬁcult to remember complex distribution.\u000f Model size increases.\n\u000f Slow training.\nOperationProperties Feed forward Hidden layer is used as input again.\nAdvantage\u000f Simple operation\n\u000f Back-propagation algorithm isn’t needed\nto update the weight.Can take advantage of the previous\nlong history.\nDisadvantageIt is not remembering long-term history\nbecause it is only affected by the current input.Not suitable for time-variant system.\nOfﬂine&\nOnline\nlearningProperties Ofﬂine & Online learning are available Ofﬂine learning is available\nAdvantageEven at runtime, the model can be ﬂexibly\nchanged according to the inﬂux of workload.Can give a variety of models and features.\nDisadvantageOnline learning requires limitations (storage\nspace, data operation), so a less complex model\nis required.When limited resources are given, learning\nbecomes impossible if the amount of\ndata increases.\n\nAppl. Sci. 2021 ,11, 3235 15 of 19\n4.2. Design Considerations\nTo design a new model with machine learning, the researcher needs to be able to\nthink of a new way of thinking and learning. At this time, the effort of the engineer is\nrequired. This section describes design considerations by comparing with reviewed papers.\nWe describe training and prediction for design considerations. Table 4 summarizes the\nreviewed papers.\nTable 4. Design Summary of the Reviewed Papers.\nSummary Input Output Learning Method Prediction\nLearned index\n[7]Learn the index\nstructure for efﬁcient\nrange request\nKeyRegression\n(key’s position)\nOfﬂine learningRMI network\n+ B-Tree\nLearn hash-function\nfor efﬁcient key\nlookupRMI network\nLearn structure to\nlearn whether a key\nis recordedClassiﬁcation\n(key’s existence\n(0/1))RNN + overﬂow\nBloom ﬁlter\nPavo [8]Learn hash-function\nfor efﬁcient indexing\nin space and speed.Word to be used as\na key1. Regression\n(supervised\nlearning)\n2. Classiﬁcation\n(unsupervised\nlearning)Ofﬂine learningLSTM + fully\nconnected layer\nLearned branch\npredictor [9]Learn a model that\nreplaces a 2-bit\ncounter in a\ntwo-level schemeGlobal History\nRegisterClassiﬁcation\n(taken or non-taken)Online learning\n(Adjust weights\naccording to loss\nof predictions and\nactual results)Perceptron +\ngshare/perceptron\nNN-sort [10]Data\ndistribution-aware\nsorting algorithmData elementsRegression\n(Return position)Ofﬂine learningMulti-layer NN +\nQuick sort\nMER-sort [11]Learn Radix\nsort-based sorting\nalgorithm, which is\nnot sensitive to dataData elementsRegression\n(Return position)Ofﬂine learningPerceptron + insert\nsort\nReuse predictor\n[12]Determine if the\naccess block is\nreusable or not\u000f PC\n\u000f memory addr\n\u000f compressed\ndata\ntime/reference\ncountClassiﬁcation\n(Reuse or not)Online learning\n(Learning by\ninputting part of\nthe access to a\nsampler that has a\npartial set of\ncache)Perceptron\nCache\nprefetcher [13]Learning-based\ntwo-level scheme to\nreduce prefetching\nof unnecessary\nrequests.\u000f prefetch\ndistance\n\u000f transition\nprobability\n\u000f address delta\n\u000f frequencyClassiﬁcation\n(Accept or Deny)Online learningTwo-level\npredictor\n(traditional\nprefetcher +\nperceptron)\nLearning\nmemory access\npatterns [14]Learn prefetcher to\nanalyze memory\naccess pattern\u000f PC\n\u000f address deltaClassiﬁcation\n(Delta with the\nhighest probability)Ofﬂine learning\n(Vocabulary\ngeneration during\ntraining for\nclassiﬁcation)(1) Embedding\nLSTM\n(2) Clustering +\nLSTM\n\nAppl. Sci. 2021 ,11, 3235 16 of 19\nTable 4. Cont.\nSummary Input Output Learning Method Prediction\nGlider [15]Learn hardware\ncache replacement\npolicy\u000f attention-\nbased LSTM:\nPC sequencer\n\u000f Glider:\nunordered\nunique PC\nsequenceClassiﬁcation\n(cache-friendly or\nnot)Ofﬂine learning\n(attention-based\nLSTM, ISVM)\nOnline learning\n(ISVM)(1) attention-\nbased\nLSTM\n(2) ISVM\n4.2.1. Training\nInput. Machine learning can use several features to learn patterns in data. Reuse pre-\ndictor uses features associated with block access, such as certain bits of memory address, to\ncorrelate with adjacent blocks, rather than using only Program Counter (PC) that generated\nthe current eviction [ 12]. Two Level Prefetcher [ 13] features prefetch distance, number of\noccurrences, address delta, etc. to prevent cache pollution. Learned Prefetcher [ 14] viewed\nmemory access as a sequential trace and used past PC sequence and delta sequence as\ninputs. However, no matter how many features the engineer chooses, the model may fall\nshort of the expected value. In addition, as more features are required to be implemented\nin real hardware, space for storage is required, and time for computation is required, so\nthe researcher should balance between hardware overhead and accuracy. There are three\nfeature selection methods to select meaningful data [ 32]. First, the ﬁlter is a method that\nselects a subset of variables in pre-processing as independent of the model. Second, the\nwrapper looks at the learning model as a black box and scores a subset of variables for\nperformance measurement. Finally, the embedded method performs a variable selection in\nthe training process. Also, by analyzing the meaning between the memory access pattern\nand natural language, such as Learning access pattern and attention-based LSTM [ 14,15],\nthe model input data is analyzed and modeled by NLP (Natural Language Processing).\nThis means that the use of domain knowledge that can interpret not only natural language\nbut also programming language with NLP plays an important role in the interpretation of\nthe ML model to apply ML to the system.\nRegression & Classiﬁcation. A supervised learning model needs to understand\nwhether the problem is a classiﬁcation or a regression before selecting. Brieﬂy speaking,\nclassiﬁcation is the task of predicting one of the pre-deﬁned class labels, and regression is\nthe task of outputting a series of values.\nSorting algorithm can be thought of as regression because the algorithm is a task that\npredicts a continuous position. Branch prediction can be thought of as a classiﬁcation\nbecause two classes are pre-deﬁned whether taken or non-taken a branch when a branch\naddress comes in. Prefetching can be thought of as regression because prefetcher predicts\nfuture memory access. However, since address space has a wide range of features when\nconsidered as a regression, there is a way to divide address space into a classiﬁcation\nproblem [ 14]. They also described a paper designing prefetching as classiﬁcation problem\nthat determines whether taken or non-taken prefetching as the auxiliary structure of\nprefetching [13].\nOnline & Ofﬂine Learning. The model can learn online or ofﬂine. Online learning is\nrequired to gradually learn model as the system changes, and ofﬂine learning is to learn the\nmodel only once and apply the model in the system. Online learning is proper for cache\ntasks with limited computing resources. This is because the cache needs to be adjusted\nfor the condition of the hit or miss. Glider [ 15] selected perceptron that consumes fewer\ncomputing resources for cache optimization. Additional feedback is needed to learn online.\nThe best way is to keep track of the entry in the long term, but this is not possible because\nit has time and space overhead. Perceptron-based Prefetching [ 13] uses a cache access\ninterval to tread as an incorrect prediction if an entry in the accept table is not referenced\nwithin a certain reference period and to read as a correct prediction if an entry in deny table\n\nAppl. Sci. 2021 ,11, 3235 17 of 19\nis not referenced within a certain reference period. Ofﬂine learning is properly for database\nsystems. This is because already stored data need only learn the correlation of the data.\nHowever, re-training is required to insert or delete tasks.\nConsequently, online learning generally uses SVM and shallow artiﬁcial neural net-\nwork (ANN), and ofﬂine learning shows the diversity of models. This is because online\nlearning requires only limited data availability and space in order to have low complex-\nity. However, ofﬂine learning requires a lot of data and overhead for model accuracy\nand generalization.\n4.2.2. Prediction\nSince machine learning is applied to improve performance, prediction accuracy must\nbe guaranteed. Some paper claims that replacing the traditional technique requires auxiliary\nassistance of traditional technique as well as machine learning. Branch predictor [ 9] replaces\nPHT (Pattern History Table) and maintains a global history register (GHR) to have the\nsame architecture as the traditional two-lever scheme. In the learned index [ 7], the learned\nB-Tree is supported by traditional B-Tree according to min/max-error in the last mile. The\nlearned Bloom ﬁlter adds an overﬂow Bloom ﬁlter to guarantee zero of false-negative rate\n(FNR) in the binary classiﬁcation task. NN-sort [ 10] uses a traditional sorting algorithm\nsuch as Quick Sort in Polish Phase to guarantee accuracy. MER-sort [ 11] uses an insert\nsort algorithm to merge Spill array and Output array. Instead of replacing all existing\ntechniques to provide guarantees similar to the speciﬁc functions of existing techniques\nand to increase predictive accuracy, the engineer can make a hybrid model, which mixed\nthe existing techniques and machine learning model.\n5. Discussion\nNew ML schemes. Despite the achievements through the ML-based system design,\na new ML-based system is still needed. The reviewed papers [ 7,10,11,13] proposed a\ncombined method of traditional algorithm and machine learning-based approach. This\napproach improves performance by addressing the pros and cons between machine learn-\ning and traditional techniques. However, the combination with traditional techniques\nhas a fundamental limitation of traditional techniques. Recently, there is an approach to\napproximating Belady0s MIN without a heuristic combination in cache replacement stud-\nies [33]. In addition, we improved the accuracy of the model by proposing a method that\nhierarchically combines multiple models rather than a single model [ 7,8,10,11]. Another\npromising approach that can be seen through this design is hierarchical reinforcement\nlearning (RL) [ 34], a method of hierarchically learning the goals by setting multiple goals.\nThe overhead of a single model can be reduced because the behavior can be coordinated\nbetween layers.\nScalability & New Application. The ML-based system must be applicable to the\nexisting architecture and new systems and needs to be continuously developed. Some\ndesigns are limited to speciﬁc workloads, and generalization can be compromised when\nfaced with new workloads. For that reason, new ML learning methods can be used. For\nexample, LSTM shows good performance in a lot of labeled data, but because it requires a\nlot of iteration for optimization, it is difﬁcult to optimize if only a few labeled examples are\nused for learning [ 35]. Therefore, the proposed method for model generalization with a\nfew data is Meta-learning [ 36], and it is possible to adapt to new tasks or environments not\nencountered during fast weight update and training time.\nImplementation Improvement. An efﬁcient strategy is needed for an ML-based\nsystem to be actually implemented with acceptable overhead. In terms of the model,\nGlider [ 15] reduced the hardware budget by removing redundant PCs and proposing\nan integer value ISVM. Perceptron Reuse Prediction [ 12] reduces the memory overhead\nfor the training dataset by applying the concept of a sampler that stores separate cache\nmetadata. In addition, promising methods that can be proposed are network pruning,\nmemory sharing method [ 37], and 16-bit ﬂoat representation [ 38], which can reduce the\n\nAppl. Sci. 2021 ,11, 3235 18 of 19\nnumber of operations and the model size. DNN acceleration can be used to improve\ncomputational performance versus power consumption. Recently, there is a DNN inference\nacceleration method that caches the output of the DNN’s hidden layer and consumes only\nthe computation required for inference [39].\nThis paper surveys recent papers applying machine learning to optimize various\ncomputer system. We evaluated the machine learning applied system and analyzed the\nmachine learning design method in several aspects. Most reviewed papers use a well-\nknown neural network like perceptron, LSTM. By just using a simple model, the proposed\nmethods show better performance than traditional techniques. Of course, machine learning\ntechniques are not a panacea. They face its own problems such as over-ﬁtting, scalability,\ncalculation overhead, and memory footprint, etc. However, in order to overcome these\nproblems, a more practical implementation is possible while proposing quantization,\npruning, and inference acceleration methods. Therefore, as the network introduced in\nthis paper, as well as the more expressive and complex model, is sought, a wide range\nof applications will be possible. These optimization opportunities will act as a positive\nfeedback loop between ML and system design as computer architecture/system advances\nfor ML acceleration and ML for computer architecture/system optimization are pursed.\nIn a near future, we believe that innovative machine learning-based system optimization\ntechniques will appear.\nAuthor Contributions: Conceptualization, H.C.; writing—original draft preparation, H.C.; supervi-\nsion and review, S.P . All authors have read and agreed to the published version of the manuscript.\nFunding: This research was funded by by the National Research Foundation of Korea (NRF) grant\nfunded by the Korea government (MSIT) (No. 2019R1G1A1100305).\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability Statement: Not applicable.\nConﬂicts of Interest: The authors declare no conﬂict of interest.\nReferences\n1. Lee, D.; Noh, S.H.; Mim, S.L.; Cho, Y. LRFU: A block replacement policy which exploits systems and theory. IEEE Trans. Comput.\n2001 ,50, 1352–1361.\n2. Nesbit, K.J.; Smith, J.E. Data cache prefetching using a global history buffer. In Proceedings of the Tenth Symposium on\nHigh-Performance Computer Architecture, Madrid, Spain, 14–18 February 2004.\n3. Musser, D.R. Introspective Sorting and Selection Algorithms. Softw. Pract. Exp. 1997 ,27, 983–993. [CrossRef]\n4. Fu, J.W.; Patel, J.H.; Janssens, B.L. Stride directed prefetching in scalar processors. ACM SIGMICRO Newsl. 1992 ,23, 102–110.\n[CrossRef]\n5. Kim, J.; Pugsley, S.H.; Gratz, P .V .; Reddy, A.N.; Wilkerson, C.; Chishti, Z. Path conﬁdence based lookahead prefetching. In\nProceedings of the 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), Taipei, Taiwan, 15–19\nOctober 2016.\n6. Hochreiter, S.; Schmidhuber, J. Long short-term memory. Neural Comput. 1997 ,9, 1735–1780. [CrossRef] [PubMed]\n7. Kraska, T.; Beutel, A.; Chi, E.H.; Dean, J.; Polyzotis, N. The case for learned index structures. In Proceedings of the International\nConferernce on Management of Data, Houston, TX, USA, 10–15 June 2018; pp. 489–504. [CrossRef]\n8. Xiang, W.; Zhang, H.; Cui, R.; Chu, X.; Li, K.; Zhou, W. Pavo: A RNN-Based Learned Inverted Index, Supervised or Unsupervised?\nIEEE Access 2019 ,7, 293–303. [CrossRef]\n9. Jimnez, D.A.; Lin, C. Dynamic branch prediction with perceptrons. In Proceedings of the HPCA Seventh International Symposium\non High-Performance Computer Architecture, Monterrey, Mexico, 19–24 January 2001; pp. 197–206.\n10. Zhu, X.; Cheng, T.; Zhang, Q.; Liu, L.; He, J.; Yao, S.; Zhou, W. NN-sort: Neural Network based Data Distribution-aware Sorting.\narXiv 2019 , arXiv:1907.08817.\n11. Kristo, A.; Vaidya, K.; Çetintemel, U.; Misra, S.; Kraska, T. The case for a learned sorting algorithm. In Proceedings of the 2020\nACM SIGMOD International Conference on Management of Data, Portland, OR, USA, 14–19 June 2020; pp. 1001–1016.\n12. Teran, E.; Wang, Z.; Jimnez, D.A. Perceptron learning for reuse prediction. In Proceedings of the 2016 49th Annual IEEE/ACM\nInternational Symposium on Microarchitecture (MICRO), Taipei, Taiwan, 15–19 October 2016; pp. 1–12.\n13. Wang, H.; Luo, Z. Data cache prefetching with perceptron learning. arXiv 2017 , arXiv:1712.00905.\n\nAppl. Sci. 2021 ,11, 3235 19 of 19\n14. Hashemi, M.; Swersky, K.; Smith, J.; Ayers, G.; Litz, H.; Chang, J.; Kozyrakis, C.; Ranganathan, P . Learning Memory Access\nPatterns. In Proceedings of the 35th International Conference on Machine Learning, in PMLR, Stockholmsmässan, Stockholm\nSWEDEN, 15 July 2018; Volume 80, pp. 1919–1928.\n15. Shi, Z.; Huang, X.; Jain, A.; Lin, C. Applying deep learning to the cache replacement problem. In Proceedings of the 52nd Annual\nIEEE/ACM International Symposium on Microarchitecture, Columbus, OH, USA, 12–16 October 2019; pp. 413–425.\n16. Kraska, T.; Alizadeh, M.; Beutel, A.; Chi, E.; Kristo, A.; Leclerc, G.; Madden, S.; Mao, H.; Nathan, V . SageDB: A learned database\nsystem. In Proceedings of the 9th Biennial Conference on Innovative Data Systems Research, CIDR ’19, Asilomar, CA, USA, 13–16\nJanuary 2019.\n17. Wang, J.; Kumar, S.; Chang, S.-F. Semi-supervised hashing for largescale search. IEEE Trans. Pattern Anal. Mach. Intel. 2012 ,34,\n2396–2406.\n18. Nwankpa, C.; Ijomah, W.; Gachagan, A.; Marshall, S. Activation Functions: Comparison of trends in Practice and Research for\nDeep Learning. arXiv 2018 , arXiv:181103378.\n19. Yeh, T.-Y.; Patt, Y. Two-level adaptive branch prediction. In Proceedings of the 24th ACM/IEEE Int’l Symposium on Microarchi-\ntecture, Albuquerque, NM, USA, 18–20 September 1991; pp. 51–61.\n20. McFarling, S. Combining Branch Predictors ; Technical Report TN-36; Digital Western Research Laboratory: Palo Alto, CA, USA,\nJune 1993.\n21. Lee, C.-C.; Chen, C.C.; Mudge, T.N. The bi-mode branch predictor. In Proceedings of the 30th Annual International Symposium\non Microarchitecture, Research Triangle Park, NC, USA, 3 December 1997.\n22. C++ Resources Network. Available online: http://www.cplusplus.com/ (accessed on 3 April 2021).\n23. Redis. Redis is an Open Source (BSD Licensed), In-Memory Data Structure Store, Used as a Database, Cache and Message Broker.\nAvailable online: https://redis.io/ (accessed on 10 May 2009).\n24. Andersson, A.; Hagerup, T.; Nilsson, S.; Raman, R. Sorting in linear time? In Proceedings of the 27th Annual ACM Symposium\non the Theory of Computing, Las Vegas, NV , USA, 29 May–1 June 1995; pp. 427–436.\n25. Manegold, S.; Boncz, P .A.; Kersten, M.L. Optimizing database architecture for the new bottleneck: Memory access. VLDB J. 2000 ,\n9, 231–246. [CrossRef]\n26. Tarjan, D.; Skadron, K. Merging path and gshare indexing in perceptron branch prediction. ACM Trans. Archit. Code Optim. 2005 ,\n2, 280–300. [CrossRef]\n27. Khan, S.M.; Tian, Y.; Jim /acute.ts1enez, D.A. Sampling dead block prediction for last-level caches. In Proceedings of the 2010 43rd Annual\nIEEE/ACM International Symposium on Microarchitecture, Atlanta, GA, USA, 4–8 December 2010; pp. 175–186.\n28. Wu, C.-J.; Jaleel, A.; Hasenplaugh, W.; Martonosi, M.; Steely, J.S.C.; Emer, J. SHiP: Signature-based hit predictor for high\nperformance caching. In Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO-44,\nNew York, NY, USA, 3–7 December 2011; pp. 430–441.\n29. Joseph, D.; Grunwald, D. Prefetching using markov predictors. ACM SIGARCH Comput. Archit. News 1997 ,25, 252–263.\n[CrossRef]\n30. Wulf, W.; McKee, S. Hitting the wall: Implications of the obvious. ACM SIGARCH Comput. Archit. News 1995 ,23, 20–24.\n[CrossRef]\n31. Jain, A.; Lin, C. Back to the future: Leveraging Belady’s algorithm for improved cache replacement. In Proceedings of the 43rd\nAnnual International Symposium on Computer Architecture (ISCA), Seoul, Korea, 18–22 June 2016; pp. 78–89.\n32. Guyon, I.; Elisseeff, A. An introduction to variable and feature selection. J. Mach. Learn. Res. 2003 ,3, 1157–1182.\n33. Song, Z.; Berger, D.S.; Li, K.; Lloyd, W. Learning relaxed belady for content distribution network caching. In Proceedings of\nthe 17th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 20), Santa Clara, CA, USA, 25–27\nFebruary 2020; pp. 529–544.\n34. Kulkarni, T.D.; Narasimhan, K.R.; Saeedi, A.; Tenenbaum, J.B. Hierarchical deep reinforcement learning: Integrating temporal\nabstraction and intrinsic motivation. arXiv 2016 , arXiv:1604.06057.\n35. Ravi, S.; Larochelle, H. Optimization as a model for few-shot learning. In Proceedings of the 5th International Conference on\nLearning Representations (ICLR), Toulon, France, 24–26 April 2017; pp. 1–11.\n36. Hospedales, T.; Antoniou, A.; Micaelli, p.; Storkey, A. Meta-learning in neural networks: A survey. arXiv 2020 , arXiv:2004.05439.\n37. Chen, T.; Li, M.; Li, Y.; Lin, M.; Wang, N.; Wang, M.; Xiao, T.; Xu, B.; Zhang, C.; Zhang, Z. Mxnet: A ﬂexible and efﬁcient machine\nlearning library for heterogeneous distributed systems. arXiv 2015 , arXiv:1512.01274.\n38. Courbariaux, M.; Bengio, Y.; David, J.-P . Low precision arithmetic for deep learning. arXiv 2014 , arXiv:1412.7024.\n39. Balasubramanian, A.; Kumar, A.; Liu, Y.; Cao, H.; Venkataraman, S.; Akella, A. Accelerating Deep Learning Inference via Learned\nCaches. arXiv 2021 , arXiv:2101.07344.",
  "textLength": 83762
}