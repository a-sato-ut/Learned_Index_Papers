{
  "paperId": "daec743e6477c5b304a495e1d32acbffda5621f7",
  "title": "minIL: A Simple and Small Index for String Similarity Search with Edit Distance",
  "pdfPath": "daec743e6477c5b304a495e1d32acbffda5621f7.pdf",
  "text": "minIL: A Simple and Small Index for String\nSimilarity Search with Edit Distance\nZhong Yang1, Bolong Zheng1, Xianzhi Wang2, Guohui Li1, Xiaofang Zhou3\n1Huazhong University of Science and Technology, Wuhan, China\nEmail:fzhongyang90, bolongzheng, guohuilig@hust.edu.cn\n2University of Technology Sydney, Sydney, Australia\nEmail: xianzhi.wang@uts.edu.au\n3The Hong Kong University of Science and Technology, Hong Kong, China\nEmail: zxf@cse.ust.hk\nAbstract ‚ÄîThe string similarity search is core functionality in\na range of applications, including data cleaning, near-duplicate\nobject detection, and data integration. We study the problem of\nthreshold similarity search with the edit distance, where given\na set of strings, a threshold k, and a query string q, we aim to\nÔ¨Ånd all strings in the set whose edit distances to qare no larger\nthan k. Extensive studies have been proposed for the threshold\nsimilarity search problem with the edit distance. However, they\nsuffer from a huge space consumption issue when achieving only\nan acceptable efÔ¨Åciency, especially for long strings. In this paper,\nwe propose a simple yet small index, called minIL, to eliminate\nthis issue. First, we adopt a minhash family to capture pivot\ncharacters and to construct sketch representations for strings.\nSecond, we develop a multi-level inverted index to search sketches\nwith a low space consumption. Finally, we apply a novel learned\nindex technique on top of the index that further improves the\nquery efÔ¨Åciency. Extensive experiments on real-world datasets\noffer insight into the performance of our method and show\nthat it substantially reduces the index size, and is capable of\noutperforming the baseline approaches.\nIndex Terms‚Äîthreshold string similarity search, edit distance,\ninvertd index, minhash\nI. I NTRODUCTION\nString similarity search, as one of the essential operating in\ndata processing, has been concentrated and studied extensively\nin recent years [13], [15], [18], [19], [22], [24], [25]. Given\na set of stringsSand a query string q, we aim to Ô¨Ånd all\nstrings inSthat satisfy a query criteria under a similarity\nmeasure. Various string similarity measures are used, such as\nthe cosine similarity, the jaccard similarity, and the overlap\nsimilarity. Among these similarity measures, the edit distance\nhas a key advantage that it preserves the character ordering\nand captures a best alignment of two strings, which is crucial\nfor applications, such as spell checking, plagiarism checking,\nspeech recognition and protein/DNA sequences detection. For\nexample, in the source tracking of COVID-19, the string\nsimilarity search with the edit distance is applied to Ô¨Ånd\ngene sequences similar to the virus in the genetic database.\nIn this paper, we focus on the problem of threshold-based\nstring similarity search under the edit distance that requires the\ndistances between the results and the query string are within\na given threshold.Existing studies [12], [13], [15], [19], [24], [28] on\nthreshold-based similarity search with the edit distance al-\nways suffer from either a low pruning rate issue or a huge\nspace consumption issue. As is well known, the time cost of\nedit distance computation is O(n2)with a string length n.\nTherefore, the query efÔ¨Åciency is low when a large number of\ncandidates need to be veriÔ¨Åed due to a low pruning rate issue,\nand the performance becomes even worse for long strings.\nAlthough approximate approaches [4], [5], [25], [27] guarantee\nthe query efÔ¨Åciency on long strings, they still have a huge\nspace consumption.\nGiven two strings sandq, the edit distance between s\nandqis the minimum number of edit operations needed to\ntransformstoq. After conducting a preliminary experiment\non existing datasets, we observe that the distribution of the\ncharacters to be edited in a string sis close to a uniform\ndistribution with high probability, especially when the string\nis long. For example, the distribution of spell mistakes in an\narticle, and the distribution of the mutated bases in a gene\nsequence. Intuitively, if we assume that the characters to be\nedited in strings are uniformly distributed, the probability\nthat a randomly selected character in a string sneeds to be\nedited isk\nn, wherekis the number of characters need to be\nedited andnis the string length. Accordingly, the probability\nthat the characters do not need to be edited is 1\u0000k\nn. The\nprobability is fairly high whenk\nnis small, i.e., the edit distance\nbetween string s and q is small. The probabilities remains \nthe same when the character is randomly selected from a \ncertain interval of strings. Therefore, if we randomly select \ncharacters from multiple intervals independently to construct \nsketch representations for strings, the sketches of similar \nstrings are likely to be similar. In other words, if the candidate \nsketches are similar to the query sketch, the candidate strings \nare similar to the query string with high possibility, and the \nresults found with sketch strings have a high accuracy.\nTherefore, we propose a novel approximate method that \nenables to Ô¨Ånd quickly the candidate strings whose characters \nneed to be edited are uniformly distributed. The method \nfetches a series of pivot characters to construct a sketch \nrepresentation for each string, and then computes the number \nof different pivots of the sketch representations between the\n\ns=stkilat dwcqkovgradbp q=stkil tdwcqkovgradap\nstkilatdw qkovgr adbp stkiltdw qkovgr adap\n(1)s‚Äô=cka\n(2) s‚Äô= caa (2) q ‚Äô=ctaAfter processed Ôºö\n(1) q‚Äô= ckaFig. 1. A Running Example\ncandidate strings and the query. To construct the sketch\nrepresentation, we Ô¨Årst apply an independent stochastic hash\nfunction (e.g., minhash [3]) on an interval in the middle of a\nstring to fetch a pivot. The string is divided into two substrings\nby the pivot. Then we recursively process the substrings to\nfetch more pivots. Based on the previous assumption, the\nprobability that a string and the query string produce a same\npivot at each recursion is 1\u0000k\nn, while the probability of\nproducing a different pivot each time isk\nn. Obviously, if two\nstrings are similar, their sketches are likely to be the same \nor to have only a few different pivots. In contrast, if two \nstrings are dissimilar, most of the pivots between the sketch \nrepresentations are different.\nConsider an example in Fig. 1. We have two strings q \nand s where jqj = 19 (the red underline in q represents a \nposition shift compared with s) and jsj = 20. As the edit \ndistance between q and s is 2, the probability of obtaining the \nsame pivot at each recursion approximates 0:9. First, we apply \nminhash to fetch a pivot from the middle 6-characters interval \nof s and q. The pivots are the same since s and q have the \nsame interval ‚Äúdwcqko‚Äù, and ‚Äúc‚Äù (in red) is captured from \nboth s and q. Afterwards, both s and q are divided to two \nsubstrings. The pivots are recursively fetched from the middle \nof the 6-characters intervals of the substrings. Finally, s and q \nare compacted to much shorter sketch strings s0 and q0, which \nare likely to be identical or differ by only one character. For \nexample, (1) s0=q0=‚Äúcka‚Äù, or (2) s0=‚Äúcaa‚Äù, q0=‚Äúcta‚Äù.\nBy combining the sketching method with two concise \nindexes, i.e., a trie-based index and a mutil-level inverted \nindex, respectively, we develop two methods minIL+trie and \nminIL that achieve improvements on both space and time costs. \nBenefiting from the sketch representation, the space costs  of \nthe two proposed indexes are reduced to O(LN), where N \nis the dataset cardinality and L is the sketch length. Since \nthe space cost is independent on the string length, the method \ngains better performance on long strings. Table I shows the \ncomparison on space costs  of existing methods. We can see \nthat the space cost of minIL is smaller than existing methods \n(The space cost of Bed-tree is not indicated explicitly in study \n[28], it requires more space than MinSearch [27]). Moreover, \nto further improve the query efficiency, we replace the length \nfilter with a learned index technique to quickly locate the \npositions of the candidates in inverted lists. The experiments \nresults show that both the space cost and query efficiency of \nthe proposed methods outperform the competitors.TABLE I\nSPACE COSTS OF EXISTING STUDIES\nAlgorithm Space Complexity\nminIL O(LN)\nminIL+trie O(LN)\nMinSearch [27] O(Nnlogn )\nBed-tree [28] NA\nHS-tree [24]O(Pl=lmax\nl=lminPn=n max\nn=n minl\u0003(jSlj+n))\nThe major contributions are summarized as follows:\n\u000fWe propose a novel sketching method that implicitly en-\ncodes alignments between strings and guarantees the sim-\nilarity between sketch representations of similar strings \nwith high probability.\n\u000fWe propose a simple and small index minIL that sub-\nstantially reduces the space cost compared with existing \nstudies. We innovatively apply the learned index to re-\nplace the length filter to improve the query efficiency.\n\u000fWe conduct extensive experiments on real-world datasets \nto offer insight into the performance of minIL that out-\nperforms the existing methods, and the results show high \nefficiency and low space consumption of the method.\nThe rest of the paper is organized as follows. In Section\nII, we formalize the problem deÔ¨Ånition. In Section III, we\nintroduce the sketch representation construction algorithm. We\ncover the index structure and the threshold search algorithm\nin Section IV. Section V introduces the optimizations for\nthe extreme string shift problem. Experimental studies are\npresented in Section VI. Finally, we review the related work\nin Section VII and conclude the paper in Section VIII.\nII. P RELIMINARIES\nWe proceed to formalize the problem deÔ¨Ånition. Frequently\nused notation is summarized in Table II.\nDeÔ¨Ånition 1 (Edit Distance). Given two strings sandq, the\nedit distance between sandq, denoted as ED(s;q), is the\nminimum number of edit operations, including substitution,\ninsertion and deletion on a single character, needed to trans-\nformstoq.\nDeÔ¨Ånition 2 (Threshold-based Similarity Search with Edit\nDistance). Given a set of string S=fs1;s2;:::;sNgand\na query string q, and a threshold k, the threshold-based\nsimilarity search with edit distance reports a set Rof all\nstringssi2S such thatED(si;q)\u0014k.\nExample 1. Consider a set of strings in Table III. Assume\na query string q=\\above\" with length of 5, and k= 1,\nthe threshold similarity search returns \\abode\" since the edit\ndistance between \\abode\" and the query \\above\" is1\u0014k.\nIII. S TRING SKETCH CONSTRUCTION\nWe proceed to introduce the sketching method, called min-\nhash compacting (MinCompact), that compacts long strings\n\nTABLE II\nSUMMARY OF NOTATIONS\nNotation DeÔ¨Ånition\nS A set of input strings\nN Number of strings in S\n\u0006 Alphabet of dataset\nsory A string in S\nqorx Query string\nn Length of a string\nk Threshold\nt Threshold factor t=k=n2[0;1]\nl Recursion times of the MinCompact method\nL Length of a sketch string\n\" Interval length parameter in MinCompact method\ns0ory0Sketch of a string in S\nq0orx0Sketch of the query string\n\u000b Number of different characters between x0andy0\nP\u000b Probability that x0andy0have\u000bdistinct characters\nR The result set\nTABLE III\nA S ET OF STRINGS\nID String Length\ns1 abode 5\ns2 about 5\ns3 abound 6\ns4 aboard 6\ns5 abstract 8\ninto short sketch strings by using an independent minhash \nfamily, which implicitly aligns the strings. MinCompact en-\nables to find approximate matches to the query with a perfect \naccuracy (> 0:99).\nA. Minhash Compacting\nMinCompact compacts a string with length nto a sketch\nstring with length 2l\u00001afterlrecursions (l is a small value).\nThe details of the method are described in Algorithm 1. It\nÔ¨Årst apples an independent minhash function on the middle\n[(1=2\u0000\")n: (1=2 +\")n] characters of an input string yto\nÔ¨Ånd a pivot that has the minimal hash value. The pivot is\nthen stored in the sketch string y0. The input string is divided\ninto two substrings by the pivot, then we recursively process\nthe substrings as input at the next recursion. The process\nis repeated by lrecursions. MinCompact is to some extent\ninspired by the study [5] that is designed for embedding strings\ninto a Hamming space. Different from [5] that embeds a string\ninto a long, sparse string, MinCompact compacts a string into\na short sketch string.\nExample 2. Considery=w1w2:::w18in Fig. 2. We set l= 2\nand2\"n= 4 (2\"n is the length of the interval for fetching\na pivot). At the Ô¨Årst recursion, the pivot w9is fetched from\n[w8:w11], then we push w9intoy0. At the second recursion,\npivotsw5andw13are obtained from [w3:w6]and[w13:Algorithm 1: MinCompact\nInput: A stringy=w1w2w3:::wn,l,\"\nOutput: A new string y0\n1Initialy0of size 2l\u00001;\n2Select an independent minhash function hand leti\nminimizeh(wi)out of the\ni2[(1=2\u0000\")n: (1=2 +\")n] iny. We callwithe\npivot ;\n3Pushwiintoy0;\n4Recursively process [w1;:::;wi\u00001]and[wi+1;:::;wn]\nuntill-th level;\n5Returny0;\nw16], respectively. Finally, after pushing the pivots into y0, we\nhavey0=w9w5w13.\nMinCompact has two advantages. One advantage is that\nit implicitly encodes alignments between strings. The string\nshifts always exist between similar strings, and a large string\nshift may decrease the probability of fetching a same pivot\nbetween similar strings, which leads to correct candidates\nmissing. Therefore, the alignment between strings during\nfetching the pivots is required. MinCompact aligns strings\nby taking the pivot as the boundary of input strings at the\nnext recursion. Once two strings produce a same pivot, the\nsubstrings are aligned from the location of the pivot. Therefore,\nthe string shift issue at the next recursion is reduced. For\nexample in Fig. 1, there is a position string shift between\nsandq(indicated by the red underline in q). After the Ô¨Årst\npivot \\c\"is fetched, the string shift remains between the Ô¨Årst\nsubstrings of sandq, while the second substrings of sandq\nhave no string shift, since they are aligned by the pivot \\c\",\ni.e., the string shift in the second substrings is eliminated.\nThe other advantage is that the method can readily control the\noutput length. The output length is a constant value L= 2l\u00001\ndetermined by the parameter l.\nB. Analysis\nnWe proceed to illustrate the idea of MinCompact. If two \nstrings x and y are similar, they are close naturally in length, jxj \n\u0019 jyj \u0014 n, where n is the string length after trans-forming one \nstring into the other. For ease of presentation, we first discuss \nedit operations of substitution and insertion. For example in \nFig. 1, instead of deleting \\a\" from s, we substitute \\b\" to \\a\" \nin s and insert \\a\" into q to transform two strings into the same. \nIf the edit distance between x and y is k (k \u001c n), there are k \ncharacters need to be edited. Assume that k characters are \nuniformly distributed in strings. Intuitively, the probability that \na stochastically selected character in the string that requires to \nbe edited approximates to k . The probability remains \nunchanged when the character\nis selected from an interval of the string. That is to say, if \nwe stochastically selected a character as a pivot from each of \ntwo similar strings, the probability P(differ)  that the pivots\nare different approximates to k . The probability P(same)  that\nn\n\nw13 w5w9\nw5l=1\nl=2\nOutputÔºöw1w2w3w4w5w6w7w8w9w10w11w12w13w14w15w16w17w18\nw1w2w3w4w5w6w7w8 w10w11w12w13w14w15w16w17w18\nw13 w5 w9Fig. 2. An Example of MinCompact Procedure\nthe pivots are identical approximates to 1\u0000  k\nn. For the deletion\noperation, the only difference is that n will be slightly smaller, \nwhich does not affect the conclusion.\nBased on the above conclusion, the sketch strings x0and y0\nthat have L independent pivots are likely to differ only in a \nfew pivots. The probability that x0 and y0 have \u000b different \npivots is:\nP\u000b\u0019C\u000b\nL(k\nn)\u000b(1\u0000k\nn)L\u0000\u000b(1)\nThe probability that x0andy0have no more than \u000bdifferent\npivots is:\nP(differ\u0014\u000b)\u0019\u000bX\n0Pi (2)\nFor instance, if l = 3, jxj = jyj = n, and ED(x; y) \u0014 0:1n, \nin this case, y is a result of the query x with a threshold \nk = 0:1n. According to the above derivation we have the \nprobabilities: P0 \u0019 0:478;  P1 \u0019 0:372;  P2 \u0019 0:124, P3 \u0019 \n0:023 and so on. Then probability that x0 and y0 have no more \nthan 3 different pivots is P = P0 + P1 + P2 + P3 \u0019 0:997. \nIn other words, if x0 and y0 have no more than 3 different \npivots, the probability that the original strings x and y are \nsimilar is 0:997. It indicates that the accuracy of the results \nwhose sketch string has no more than 3 different pivots to \nx0 is 0:997. Therefore, we enable to find the results whose\nPedit distances with x are no larger than k with an accuracy\u000b\n0 Pi. We can readily achieve a perfect accuracy (>  0:99) by adjusting \u000b .\nC. Effectiveness of Parameters\nMinCompact considers two parameters land\". The param-\neterldictates the maximum recursion depth and determines\nthe length of sketch strings. It affects the computation cost\nand the probability P\u000bof candidates being similar with the\nquery. Obviously, a small lis needed such that the length of\nthe input string at each recursion is no smaller than the number\nof characters to scan. At each recursion, the algorithm scans\n2\"n characters, and the input length at the i-th recursion is at\nleast (1=2\u0000\")i\u00001n. We need to choose lsuch that at the l-th\nrecursion, the input length is no smaller than 2\"n. Therefore,\nit sufÔ¨Åces to set lthat satisÔ¨Åes:\nl\u0014log1=2\u0000\"2\"+ 1: (3)\nThe parameter \"controls the computation cost of MinCom-\npact. When \"decreases, the input length at each recursion\ndecreases, and the computation cost is reduced. The methodscans 2\"n (1\n2n< \" <1\n2) characters at each time, and\n2l\u00001times in total. So the time cost is O(\fn), where\n\f= 2(2log1=2\u0000\" 2\"+1\u00001)\" < 1, and iff\"=1\n2nor\"=1\n2,\n\f= 1. The parameter \"also affects the ability of string shift\ntolerance. A larger \"implies greater tolerance. Although the\nmethod can tolerate string shift, if \"is too small while the\nstring shift is large, the characters in middle intervals between\nsimilar strings may be different. Therefore, the probability of\nproducing the same pivot in the intervals is decreased, which\nreduces the algorithm accuracy. So there is a trade-off between\nthe computation cost and the ability of string shift tolerance\nthat requires a proper setting of parameter \".\nD. Extreme String Shift Issue\nIt is worth noting that although a proper \" enables to improve \nthe string shift tolerance ability, the method suffers from \nextreme string shift cases, i.e., the shifts are all con-centrated at \nthe beginning or at the end of the string. To deal with the \nproblem, we propose two optimizations in this paper. The first \none is to utilize a slightly larger \" at the first recursion in \nMinCompact. As discussed, a large \" can tolerate  large string \nshift. If we apply a large \" at the first recursion, a high \nprobability of producing a same pivot in the intervals remains. \nIf a same pivot is obtained between similar strings at the first \nrecursion, the input strings in the next recursion are aligned, \nand so does the input strings in the subsequent recursions. \nTherefore, the string shift is reduced and the ability of string \nshift tolerance is improved. The second optimization developed \non top of the query algorithm and the index structure is \ndescribed in Sec. V.\nE. Other Issues\nMinCompact helps to Ô¨Ånd similar strings and Ô¨Ålter dis-\nsimilar ones with high probability. However, two types of\ninappropriate candidates may be produced. One type is the\ncandidates that have a large length difference that exceeds\nthe threshold with the query. The string length varies greatly,\nwhich can be observed from the dataset statistics in Table\nIV. After applying MinCompact, since the sketch strings are\nwith the same length, the strings with large length differences\nmay have similar sketch strings. The other type is the candi-\ndates that are dissimilar to the query, but contain pivots that\nhappens to be the same as the query. For instance, the string\nsegments \\acdfge \"and\\hkljma \"from two dissimilar strings\ncan produce the same pivot \\a\". To deal with these issues,\ntwo pruning strategies are introduced along with our index\nstructure in Sec. IV.\nIV. T HRESHOLD BASED SIMILARITY SEARCH\nAfter applying MinCompact, each string sis compacted into\na sketch string s0. To answer the threshold based similarity\nquery, we Ô¨Årst Ô¨Ånd the candidates fsigwhose sketch strings\ns0\nihave no more than \u000bdifferent pivots with the sketch\nstringq0of the query. Then, we verify whether ED(si;q)of\nall candidates sisatisÔ¨Åes the criteria. To prune inappropriate\ncandidates quickly, we propose a trie-based index. To further\n\nr\na b\nb c\na b aa b\nb abd\na\nRecord List0\n0 1 2 1 21\n0 1 1 2 3 1 3\nRecord List‚Ä¶‚Ä¶\nRecord Listd=1\nd=2\nd=3\n‚Ä¶‚Ä¶Fig. 3. An Example of Marked Equal-Depth Trie\nreduce the space cost, we develop a multi-level inverted index\nwith a learned index based length Ô¨Ålter.\nA. Trie-based Index\nA straight-forward way to Ô¨Ånd the candidates is the linear\nscan method that compares each s0\ni with q0 to compute the \nnumber of different pivots. Although the sketch strings are \nshort, it is costly to scan all strings especially when the dataset \ncardinality is large. The trie index is a well-adopted structure \nfor strings. Although it cannot be applied directly for searching \nthe original long strings under the edit distance, it is suitable for \nsearching the short  sketch strings. Unlike the traditional trie \nindex that stores strings with various lengths and searches for \nstrings with common prefixes, we need a structure to save \nstrings with equal length and to search strings that have no \nmore than \u000b different pivots to the query sketch q0. According \nto the requirements, we design a marked equal-depth trie index \nstructure.\nIndex Structure and Search Algorithm Fig. 3 shows an \nexample of the marked equal-depth trie with a depth of 3. \nEach node represents a character with a mark at the top right \nof the node. The mark records the number of different pivots \nup to the node during the search process. Each leaf node links \na record list of sketch strings whose characters are represented \nby the route from the root to the leaf node. The trie is built \nby inserting all sketch strings into the tree. And the marks \nattached on nodes are 0 initially. Algorithm 2 presents the \nrecursive searching procedure on the trie. We start from the \nroot node and traverse all the child  nodes of the root node. We \ncheck the mark on the child  node whether it is no larger than \n\u000b. If the mark \u000b^ exceeds \u000b, the successor nodes of the child  \ncan be pruned (Line 6-7). If the mark is within the limit of \u000b, \nthen we check the character represented by the node. If the \ncharacter is equal to the current character of q0, we recursively \nprocess the child  nodes of the node (Line 10), otherwise, we \nrecursively process the child nodes with their marks adding 1 \n(Line 12). In each iteration, if the input node is a leaf node, \nthe linked record list is merged into the results R after length \nÔ¨Åltering and position Ô¨Åltering (Line 1-3).\nLength Filter. As mentioned before, the string length varies \nin a dataset. But after MinCompact, the sketch strings have the \nsame length. The candidates may have a large length difference \nthat exceeds the threshold with the query. To address the issue, \nwe attach the original string length to each record in the recordAlgorithm 2: TrieSearch\nInput: Query sketch string q0, a node of trie node,\u000b,\nmark ^\u000b\nOutput: String SetR\n1ifnode is the leafnode then\n2Rn node:RecordList after LengthFilter and\nPositionFilter;\n3R R[R n\n4else\n5 foreachchild2node do\n6 if^\u000b>\u000b then\n7 continues;\n8 else\n9 ifq0[i] ==node:character then\n10 TrieSearch(q0[i+ 1];child;\u000b; ^\u000b)\n11 else\n12 TrieSearch(q0[i+ 1];child;\u000b; ^\u000b+ 1)\n13ReturnR;\nlists to do the Ô¨Åltering. When we look up a  record, the attached \nlength is compared with query length. If the length difference \nis larger than the given threshold, the record can be pruned \nsince its original string can not be a result.\nPosition Filter. Dissimilar strings may produce similar \nsketch strings by coincidence. Pruning these candidates can \nfurther improve the query efÔ¨Åciency. T o alleviate the problem, \nthe position of each pivot in the original string is attached \nto each record in the list. Then, if a record contains a same \npivot with the query, positions of the pivot in their original \nstrings are compared. If the position difference is larger than \nthe threshold, which is not a feasible alignment, the pivot can \nbe considered as different. In this way, we can reduce the \nimproper candidates whose sketches happen to be similar to \nthe query.\nExample 3. Consider the trie of sketch strings in Fig. 3. \nSuppose the query sketch string q0=\\aba \" and \u000b = 1. We \ntraverse all the nodes in the tree from root r to the leaf \nnodes. When we meet the nodes at depth 1, node a equals \nto q0[0]=\\a\" while node b does not, then the mark of node \na keeps 0 while the mark of node b adds 1, and the marks \nare passed to their child nodes, respectively. When we meet \nthe node a at depth 2, since it does not equal to q0[1]=\\b\", \nits mark becomes 2 which is larger than \u000b, then its successor \nnodes can be pruned. Similarly, the successor nodes of d at \ndepth 2 can be pruned. Once we meet a leaf node and its mark \ndo not exceed \u000b, the records linked to it is merged into the \nresults after two filtering.\nCost Analysis. The space cost of the trie-based index \nis smaller than O(LN) since the common prefixes reduces \na certain amount of storage. On the other hand, the trie-\nbased index needs to express various relationships between\n\nc1 c2 c3 ‚Ä¶ cL\na\nb\nc\n‚Ä¶\n‚Ä¶\nx\ny\nzrecord list\nrecord lista\nb\nc\n‚Ä¶\n‚Ä¶\nx\ny\nza\nb\nc\n‚Ä¶\n‚Ä¶\nx\ny\nz\n‚Ä¶‚Ä¶‚Ä¶‚Ä¶ ‚Ä¶s‚ÄôÔºö\nDict\nŒ£record list\nrecord listrecord list\nrecord listFig. 4. Multi-level Inverted Index\ncharacters, so its implementation is more complicated, which \nincurs additional space costs. Suppose the average number of \nbranches of the nodes is \u001b (\u001b \u0014 j\u0006j, where \u0006 is the alphabet of \nall strings), the time cost of searching the trie is at least O(\u001b\u000b) \nsince we need to traverse all the nodes within depth \u000b before \nany pruning. In the worst case, the time cost is O (\u001bL).\nB. Multi-level Inverted Index\nAlthough the trie-based index is smaller compared to the \nexisting tree-based indexes [24], [28], its space consumption \nis still non-negligible, especially when the size of dataset is \nlarge. To further reduce the space consumption and improve \nthe search efficiency, we propose a simple and small index \nstructure called multi-level inverted index (minIL) along with \na learned index based length filter.\nIndex Structure. The multi-level inverted index consists \nof L levels of inverted indexes corresponding to the sketch \nstring length L. For each position j of the sketch strings, \nthere is one level inverted index. Each level inverted index \nconsists of record lists of characters c 2 \u0006. Each record list \nof a character c consists of the sketch strings that contain \ncharacter c at position j. Fig. 4 illustrates the structure of the \nindex. And the Index is built as Algorithm 3 shown. We Ô¨Årst \ninitialize the index with L levels. Secondly, each string si 2 S \nis transformed into s0\ni by the MinCompact. Thirdly, for each \ncharacter c at position j in s0\ni, we add s0\ni into the record list \nof c in the j-th level of the inverted index, denoted as Ij (c). \nFinally, after all strings are processed, the index is set up.\nSearching algorithm. The algorithm for threshold based \nsimilarity search on the multi-level inverted index is described \nin Algorithm 4. Given a set of strings S, a query string q \nand a similarity threshold k, the algorithm returns all strings \nsi 2 S such that ED(si; q) \u0014 k. The search method can be \nused for different thresholds with different accuracy at query \ntime. First, we use Algorithm 1 and Algorithm 3 to process \nall input strings and generate the index I. Then we construct \nq0 for q. Afterwards, for each pivot q0[i] at position i, we \nscan the list of Ii(q0[i]). After the length Ô¨Åltering ( Line 5) \nand position Ô¨Åltering ( Line 6 ), f or e ach s i i n t he l ist, i f i t is \nnot in the hashmap Maphsi; fi, which contains records and \ntheir frequencies, it is inserted into the map, otherwise, its \nfrequency plus one (Lines 7-9). After all positions of q0 areAlgorithm 3: Building Index\nInput: String SetS,L,\u0006\nOutput: IndxeI\n1InitialIwithLlevels and each level has \u0006record\nlists. foreachsi2S do\n2s0\ni MinCompact (s);\n3 foreachc2s0\nido\n4cis at position jofs0\ni;\n5Ij(c)[s0\ni;\n6ReturnI;\nAlgorithm 4: Threshold Based Similarity Search\nInput: A query string q, IndexI, thresholdk\nOutput: String SetR\n1Choose\u000baccording to the accuracy requirement;\n2q0 MinCompact (q);\n3fori\u0014Ldo\n4 foreachs0\ni2Ii(q0[i])do\n5LengthFliter (si);\n6PositionFliter (si);\n7 ifsi2Maphsi;fithen\n8 f f+ 1;\n9 Addhsi;1iintoMap;\n10foreachsi2Map do\n11 ifL\u0000f\u0014\u000bthen\n12 ifED(si;q)\u0014kthen\n13R R[si;\n14ReturnR;\nprocessed, we obtain the map contains strings whose sketch\nstrings have intersection with q0. We check the strings whose\nfrequency satisÔ¨Åes L\u0000f\u0014\u000bto determine the candidates.\nFinally, we verify the candidates and return the results (Lines\n12-14).\nj\u0006jCost Analysis. Different from the existing indexes that store \nvarious substrings with a large redundancy, the multi-level \ninverted index reserves the sketch strings with only a small \nredundancy, and does not use extra structure compared to the \ntrie-based index. Although the space cost is still O(LN), the \ncost in practice is smaller than that of the trie-based index. \nThe search algorithm scans L record lists and each record list \nhas a length N at average. Therefore, the time cost of the\nsearch method excluding the verification phase is O( LN\nj\u0006j).\nRemark. Our proposed method can achieve the perfect\naccuracy (> 0:99) by adjusting \u000b. What‚Äôs more, \u000bis data\nindependent to achieve the perfect accuracy: it depends on the\nthreshold factor t=k\nnand the parameter l. The selection\nof\u000bis further illustrated in the experimental study. It is\nworth noting that even \u000bis not selected appropriately, a high\naccuracy can be achieved by repeating the MinCompact with\n\nA Sorted\nRecoredListModelkey = |ùëû|\n[ùëû‚àíùëò,ùëû+ùëò]ùëû+errFig. 5. Searching Strings by A Learned Index\ndifferent minhash families. In this way, multiple sketch strings\nare produced for each string, which results in larger index size.\nIt is usually not necessary in practice, as a single MinCompact\nis already good enough. Moreover, the multi-level inverted\nindex can be scanned in parallel without any modiÔ¨Åcation.\nC. Improvements with Learned Index Technique\nWe use a naive way that traverses the record list to do the \nlength Ô¨Åltering. It is inefÔ¨Åcient when the size of  record list is \nlarge. Actually, we only need to retrieve the strings whose \nlengths are within the range of [jqj \u0000 k; jqj + k] with the \nthreshold k. To quickly  locate positions of strings within the \nrange, many technologies can be applied after sorting strings \nby length. Binary search or B-tree is a common option. A \nrecently proposed novel learned index structure which is much \nmore efÔ¨Åcient t han b inary s earch o r B -tree i s a vailable here. \nLearned indexes [9], [11], such as RMI, use machine learn-\ning techniques to model the cumulative distribution function \n(CDF) of a sorted array. We apply the learned index technique \nto the length filter, called learned length filter, by training a \nmodel for each record list separately to replace the plain  length \nfilter when building the index. Then the models are utilized to \nspeed up searching the sorted record lists. For example, Fig. 5 \nshows the process of searching strings within the length range [j\nqj \u0000 k; jqj + k] using the learned length filter. We take the \nlength jqj as the input key of the learned model, the model \noutputs key location in the sorted record list within O(1) time. \nAlthough the model of learned index always has a search error \nerr, using the length jqj as the key can avoid the error as long as \nthe key location falls in the search range, i.e., jqj + err 2 [jqj \u0000 \nk; jqj + k]. With an acceptable accuracy of the model, it \nhappens with high probability. Next, we retrieve both sides  \nfrom the location jqj + err to traverse the search range \nsequentially.\nCompared with traditional index structures, the learned \nindex has significant high efficiency. The performance of \nour search method benefits from utilizing learned index: The \ntime cost of searching a single record list is reduced to \nO(2k), thus the time cost of searching the candidates is \nreduced to O(2kL).\nV. OPTIMIZATION FOR STRING SHIFT ISSUE\nIn the analysis of MinCompact in Sec. III, we make an \noptimistic assumption that the characters need to be edited\n‚Ä¶\n|ùëû|Coverage\nof |ùëû|\nùúå|ùëû|\n|ùëûùë£1|2ùúå|ùëû|Coverage \nof |ùëûùë£1|‚Ä¶\n‚Ä¶\nùëòFig. 6. Analysis of Filling A Query String\nare uniformly distributed in the string. If the string is long \nand random, the distribution of characters need to be edited \nsatisfies the assumption with high probability. However, in \npractise, there exist exceptions to the assumption, i.e., the \nextreme string shift issue that the shifts are all concentrated \ntogether at the beginning or at the end of the string. The \nextreme string shift issue  may be caused by human or system \nerrors are uncommon but inevitable. For example, in text \nstrings, an article loses  a sentence at the beginning, or in \ngene strings, the last segment of a gene sequence is missing. \nAlthough MinCompact has the ability to tolerate string shift, \nit still suffers from the extreme cases.\nWhen MinCompact has a small parameter \", the extreme \nstring shift cases could be problematic: At the Ô¨Årst recursion, \nthe characters in fetching intervals between similar strings \nmay be totally different, which decreases the probability of \nproducing a same pivot. Afterwards, without the same pivot at \nthe Ô¨Årst recursion, the substrings are not aligned, the characters \nin fetching intervals at the next recursion may also be different. \nThis is a chain reaction that eventually leads to dissimilar \nsketch strings. Recall the discussion in Section III, we propose \nto use a slightly larger \" at the Ô¨Årst r ecursion t o d eal with \nthe issue. However, the optimization cannot completely solve \nthe problem. In experiments, we use a synthetic dataset that \nonly contains strings that have extreme string shifts to test the \neffectiveness of the optimization in Sect. VI. The optimization \nhelps to achieve about 40% accuracy on the extreme data \nwhich is still not acceptable.\nA. Improvements on Query Processing\nWe propose another optimization to improve the accuracy\non extreme string shift cases. Inspired by [15], we come up\nwith an idea of making the query string be aligned with shifted\nstrings by truncating or Ô¨Ålling the query string at the beginning\nor the end to reduce or even eliminate string shifts. Since\nstrings have various lengths, we apply different alignment\nschemes on the query to cover different string shifts. For\ncandidate strings that are shorter than the query, we truncate\nseveral characters at the beginning or end of the query. For\ncandidate strings that are longer than the query, we Ô¨Åll the\nquery with several place-holders at the beginning or the end.\nWe obtain multiple variants of the query through the above\noperations. The question is: how many characters/placeholders\n\nshould be truncated/Ô¨Ålled, in another words, how many vari-\nants of the query should be acquired. Suppose the string shift\ntolerance size of a query string qusing MinCompact is \u001a\u0001jqj.\nTherefore, the query string can cover shifted strings with\nlengths in the range of [jqj\u0000\u001a\u0001jqj;jqj+\u001a\u0001jqj]. Similarly, a\nvariantqviof the query can cover shifted strings with lengths\nin the range of [jqvij\u0000\u001a\u0001jqj;jqvij+\u001a\u0001jqj](for simplicity,\nsuppose the string shift tolerance size of qviis still\u001a\u0001jqj).\nThus, each variant qvican extend the shift coverage of q\nby2\u001a\u0001jqjat most. Consider the situation in Fig. 6 that\nwe need to cover all the shifted strings with lengths in the\nrange of [jqj;jqj+k], wherekis the threshold. Then, we\nneed at least mvariants each of which covers a subrange\n[jqj+ (2i\u00001)\u001a\u0001jqj;jqj+ (2i+ 1)\u001a\u0001jqj];(i= 1;2;::;m )to\nsatisfy (2m+ 1)\u0001\u001a\u0001jqj\u0015k. Accordingly, the total Ô¨Ålling\nsize is 2i\u0001\u001a\u0001jqj;(i= 1;2;:::;m ). Since\u001ais an uncertain\nvalue, we approximate it withk\n(2m+1)jqj. Therefore, the Ô¨Ålling\nsize is2i\u0001k\n2m+1. The total number of the variants is 4m(truncat-\ning/Ô¨Ålling strings at the beginning/end). In practice, m= 1 is\nalways enough to cover all the string shift possibilities when\nkis small. When m= 1, we Ô¨Åll the query string with2\n3k\nplaceholders for longer strings, and similarly, we truncate2\n3k\ncharacters of the query for shorter strings.\nAligning query string to extreme string shift cases produces\nmultiple variants that leads to time increase. Even though m=\n1is enough in the most cases, there are four variants that need\nto be processed. Fortunately, the search ranges in the record\nlists of the variants are half the length of the original query and\nthe candidates can be readily found using the learned length\nÔ¨Ålter. For the variants of longer strings, we only retrieve the\nstrings with a length in the range of (jqj;jqj+k]. While for\nthe variants of shorter strings, we only retrieve the strings with\na length in the range of [jqj\u0000k;jqj). Take searching strings\nwithin the range of (jqj;jqj+k]as an example. We use a\nmedian2jqj+k\n2of the search range as the input key of the\nlearned length Ô¨Ålter to avoid the model error. As long as the\nlocation of key falls in the search range, i.e.,2jqj+k\n2+err2\n(jqj;jqj+k], we can traverse the records within the range\nsequentially to Ô¨Ånd the candidates. BeneÔ¨Åting from the high\nperformance of learned length Ô¨Ålter, although the time cost\nincreases due to the multiple variants, it is still acceptable.\nVI. E XPERIMENTAL STUDY\nWe report on extensive experiments with real-world datasets\nthat offer insight into the performance of our proposed al-\ngorithms. In particular, we aims to answer the following\nquestions:\nQ1 :How to choose parameters that affect the performance of\nminIL ?\nQ2 :How much does minIL outperform the competitors under\ndefault settings?\nQ3 :How much does minIL reduce the space consumption\ncompared with the competitors?\nQ4 :How does the optimizations work under string shift?TABLE IV\nSTATISTICS OF DATASETS\nDataset Cardinality avg-len max-lenj\u0006jq-gram\nDBLP 863053 104.8 632 27 1\nREADS 1500000 136.7 177 5 3\nUNIREF 400000 445 35213 27 1\nTREC 233435 1217.1 3947 27 1\nA. Experiment Setup\nAll the algorithms are implemented in C++ compiled using\n64-bit addressing. All experiments are run on a Window 10\nmachine with an Intel 3.4Hz CPU and 32GB memory. We\nrelease our source code on Github1.\nDatasets. We conduct the experiments on four real-world\ndatasets:\n\u000fDBLP2: A dataset of DBLP publication information\nincluding authors, title and key words of papers.\n\u000fREADS3: A dataset contains short DNA sequencing\nreads, which was used in the edit similarity joins and\nsearch competition.\n\u000fUNIREF4: A dataset of protein sequences obtained from\nthe website of UniProt project.\n\u000fTREC5: A dataset of publication information including\nthe authors, title, and abstract of papers in 270 medical\njournals.\nThe datasets vary in cardinality, average length, maximum\nlength, and the dictionary size j\u0006j. Table IV shows the dataset\nstatistics. The average length of strings in DBLP and READS\nis small, while their cardinalities are large. The average string\nlength in UNIREF and TREC is much larger, especially the\naverage string length in TREC, which is over 1000. The max\nlength of UNIREF is quite large, it reaches 35213.\nAlgorithms. We compare our proposed method with the\nfollowing competing algorithms:\n\u000fMinSearch6[27]. It uses a local hash minima method to\nextract common substrings between similar strings and\nstores the substrings in a hash table.\n\u000fBed-tree7[28]. It uses several word ordering strategies\ntogether with the B+-tree structure to perform the search.\n\u000fHS-tree8[24]. It uses a hierarchical segment tree that\nrecursively stores the substrings of the half of the string\nthat contained in the parent node.\n\u000fminIL+trie . The proposed method using the trie-based\nindex.\n1https://github.com/yangzhong901/minIL\n2https://dblp.uni-trier.de/\n3https://www2.informatik.hu-berlin.de/ \u0018leser/searchjoincompetition2013/\n4http://trec.nist.gov/data/t9 Ô¨Åltering.html\n5http://Ô¨Åmi.ua.ac.be/data/\n6https://github.com/kedayuge/Search\n7https://github.com/ZhangZhenjie/bed-tree\n8https://github.com/TsinghuaDatabaseGroup/Similarity-Search-and-Join/\ntree/master/hstree\n\nTABLE V\nDEFAULT PARAMETER SETTINGS\nparameters values\nl 2, 3, 4 , 5, 6\n\r 0.3, 0.4, 0.5 , 0.6, 0.7\nt 0.03, 0.06, 0.09, 0.12, 0.15\nTABLE VI\nA B RIEF SELECTION OF \u000b\nl=3 l=4 l=5\nt\u000b accurac yt\u000b accurac yt\u000b accurac y\n0.03 2 0.999 0.03 2 0.990 0.03 4 0.998\n0.06 2 0.994 0.06 4 0.998 0.06 5 0.991\n0.09 3 0.998 0.09 4 0.992 0.09 7 0.995\n... ... ... ... ... ... ... ... ...\n\u000fminIL. The proposed method using the multi-level \nin-verted index.\nWe download the source codes of these algorithms and recom-\npile under our environment settings. For parameter settings\nof algorithms, we always choose the recommended parameter\ncombinations from the papers. MinSearch has one parameter\n\u000busing a small value (e.g., \u000b= 3). Bed-tree has several\nparameters to choose. HS-tree has no input parameter.\nWe compare the above algorithms on all datasets. However,\nHS-tree is not applicable on UNIREF and TREC, since it\ntakes too much memory usage that exceeds our computer‚Äôs\nlimit. Therefore, we do not display its results on UNIREF and\nTREC. As mentioned in Sec. I, most existing methods suffer\nfrom the massive memory usage and may be inapplicable on\nlarge datasets, where HS-tree is a representative one.\nB. Self Evaluation (Q1)\nWe report the performance of minIL under different param-\neter settings, and explore the effectiveness of the parameters.\nParameters Settings. We now present the parameter set-\ntings. Two main parameters land\"are considered, while\nthe parameter \u000bis spontaneously determined by the given\nthresholdk, string length nand the parameter lto achieve a\nperfect accuracy. For ease of presentation, we use a threshold\nfactort=k\nninstead of the threshold kin the experiments\n2(2l\u00001)to keep \u000b consistent for different query lengths under a same \naccuracy. A small l is required to avoid running out of the \nlength of the string in the recursion. A proper selection of \" \nis required to balance between the computation cost and the \naccuracy.\nWe employ a heuristic method  to tune the parameters l and \". \nThe main  idea is that we first set a large l according to the \naverage length of string (For example, the average string length \nin DBLP is about 100, we set l = 4, and the average string \nlength in READS is about 500, we set l = 5) and then vary \" to \ncheck whether l is feasible. If not, we decrease l and repeat the \nabove procedure. In addition, since the equation of computing \" \nis complicated when l is given, we set \" =  \r                                        andTABLE VII\nPERFORMANCE OVERVIEW WITH DEFAULT SETTINGS\nDataset Algorithm Memory Query\nUsage (GB) Times (s)\nDBLP minIL 0.52 0.003\nminIL+trie 1.5 0.006\nMinSearch 1.7 0.011\nBed-tree 4.8 0.110\nHS-tree 7.8 0.007\nREADS minIL 1.1 0.006\nminIL+trie 6.6 0.371\nMinSearch 4.3 0.389\nBed-tree 4.8 3.208\nHS-tree 4.4 7.007\nUNIREF minIL 0.84 0.006\nminIL+trie 2.2 0.297\nMinSearch 3.6 0.019\nBed-tree 4.8 1.028\nHS-tree - -\nTREC minIL 1.2 0.016\nminIL+trie 1.9 0.339\nMinSearch 5.2 1.477\nBed-tree 5.1 39\nHS-tree - -\nvary the value of \r 2 (0; 1) to simplify the tuning in practice. \nThe reason we set \" in this way is that MinCompact produces \npivots from 2l \u00001 different intervals, and the average length of\n2l\u00001. So \" needs to satisfy that 2\"n  < ln\n2 \u00001the interval is n , i.e.,\n\" <1\n2(2l\u00001). In the experiments, l and \r  are always feasible\nwhen we set l \u0014 6 and \r \u0014 0:5. The settings are shown in Table \nV. The default values of l are 4, 4, 5 and 5 on DBLP, READS, \nUNIREF and TREC, respectively. The default value of \r is 0.5 \non all datasets, and the default value of t is 0.15. To achieve a \nperfect accuracy (> 0:99), \u000b is determined by the cumulative \nprobability P\u000b, that is P0 + P1 + ::: + P\u000b > 0:99. Note that \u000b \nis data independent. Table VI shows a brief selection of \u000b with \ndifferent threshold factor t and parameter\nl. For example, when l =  3 and t =  0:09, we select \u000b  = 3 to \nachieve an accuracy P  = 0:998 >  0:99.\nEffectiveness of l. We conduct the experiments by varying\nlto Ô¨Ånd the best lfor each dataset and to explore the effect\noflon query time. Table VIII shows the query time of minIL\nwith different lwhent= 0:15 over four datasets. Since the\nquery time of minIL is insensitive to t, we avoid to display\nthe\nresults of the other values of t. We observe that, as the\naverage string length of DBLP and READS is small, the value\noflcan not be larger than 4and5, respectively. On DBLP\ndataset, with the increase of l, the running time drops rapidly.\nOn READS and UNIREF datasets, the results are similar to the\nresults on DBLP. With the increase of l, the running time Ô¨Årst\ndrops and then keeps stable with a slight increase. On TREC\n\nTABLE VIII\nQUERY TIME WITH DIFFERENTl\nDatasetl= 2l= 3l= 4l= 5l= 6\nDBLP 28ms 21ms 3ms - -\nREADS 26ms 23ms 6ms 6ms -\nUNIREF 22ms 13ms 6ms 6ms 7ms\nTREC 16ms 17ms 17ms 16ms 16ms\ndataset, the running time has a different trend, the times almost\nremains the same when lchanges. The results of running time\non the Ô¨Årst three datasets is reasonable. The query time is\nmainly determined by the veriÔ¨Åcation phase, where the time\nof searching on the index takes a small part. The smaller l\nis, the larger the distortion of the compacted string is, which\nmay result in more candidates to be veriÔ¨Åed. Meanwhile, the\nincrease of index size, when lincreases, has only a little effect\non the query time. The result of running time on TREC is\ndifferent. We infer that the number of candidates changes little\nwhenlchanges, so the running time is stable.\nEffectiveness of \"and\u000b.We conduct the experiments\nby varying\"and\u000bto explore the effects on the number of\ncandidates.\"is controlled by \ras mentioned in Sec. VI-B,\nand we set\r= 0.3, 0.4, 0.5, 0.6 and 0.7, respectively. Fig. 7\nillustrates the comparison on the number of candidates when\nvarying\rand\u000bover UNIREF and TREC datasets. Figs. 7 (a)\nand (b) show the distributions of the numbers of candidates\nwith different \u000b. The y-coordinate represents the numbers of\nsketch strings found in the index when \u000bequals to a certain\nvalue. Figs. 7 (c) and (d) show the cumulative numbers of\ncandidates. The value of the y-coordinate represents the total\nnumbers of sketch strings found in the index when \u000bis no\nlarger than a certain value. For example, when \u000b= 6 and\n\r= 0:5 in (c), the cumulative number is the summation\nof all candidates when \r= 0:5and\u000b\u00146in(a). We\nobserve that the distributions of the numbers of candidates\nin (a) and (b) approximate a normal distribution. When \r\nvaries, the position of the peak shifts. The plots in (c) and\n(d) are the cumulative distributions of plots in (a) and (b).\nThe cumulative number is the number of candidates that need\nto be veriÔ¨Åed, so the cumulative number directly impacts\nthe algorithm efÔ¨Åciency. We observe in (c) and (d), when \u000b\nincreases, the cumulative number Ô¨Årst goes up smoothly, then\nit increases rapidly approaching the maximum value, and at\nlast it reaches the maximum value slowly. In addition, the\nsmaller\ris, the later the curve goes up rapidly. According to\nthe above observations, we prefer to choose a small \"and\n\rwhen\u000bis chosen. However, an extremely small \"may\nlimit the selection of land exacerbate the string shift issue.\nTherefore, there is a trade-off to choose a proper \".\nC. Comparison On Query Time (Q2)\nWe report on the query performance of minIL+trie and\nminIL under the default settings and the comparison of query\ntime with competing algorithms.Fig. 8 reports the average query time of the algorithms as\na function of the threshold factor twheret=k\njqj2(0;1).\nIt is clear that our minIL performs the best, and Bed-tree \nalways performs the worst. Although HS-tree achieves the \nbest performance on DBLP when t is small, its average query \ntime increases signiÔ¨Åcantly a nd  e xceeds t he q uery t ime of \nminIL when t increases, and its performance is even worse \nthan Bed-tree on READS when t increases. MinSearch and \nour proposed methods perform better than Bed-tree on all \ndatasets and better than HS-tree on READS over an order \nof magnitude, while minIL always performs the best. The \nperformance of minIL+trie and MinSearch are similar on \nREADS and TREC, and minIL+trie performs better than \nMinSearch on DBLP while the opposite is true on UNIREF.\nIn addition, minIL is insensitive to the threshold. When t \nincreases, the average query time of minIL does not increase \nobviously. Table VII presents the results of query time when \nt = 0:15. It shows that minIL can speed up by at least \n3.6, 36.7, and 2.3 times over the competitors. In summary, \nminIL and MinSearch have the high efÔ¨Åciency a nd stability \nover all datasets, and HS-tree is more applicable on small \ndatasets with short  strings. Bed-tree has a relatively stable \nperformance but a low efficiency.\nIt is worth noting that although minIL+trie offers no im-\nprovements compared with minIL on the query time in most \nexperimental results, it provides better query performance in \nsome cases. Specifically, the time cost of minIL+trie can be \nsmaller than that of minIL, i.e., O(\u001bd) < O(LN=j\u0006j), espe-\ncially when N is large and d is small, where d (\u000b < d < L) is \nthe average depth of the searching on the tree. In these cases, \nthe trie-based index enables to outperform minIL. For example, \nin Fig. 8 on DBLP when t is small, minIL+trie performs better \nthan minIL.\nD. Comparison On Memory Usage (Q3)\nTable VII shows the comparison of memory usage of \nthe algorithms. The memory usage of minIL is related to \nparameters l and \" that are set to the default values. The \nobservation is that our minIL is clearly the best on all datasets \nthat has the smallest memory usage. The memory usages of the \nother algorithms are 3.2-15 times larger than that of minIL. For \nexample on DBLP, the memory usages are 0.52GB, 1.5GB, \n1.7GB, 4.8GB, and 7.8GB for all algorithms, respectively. \nAs mentioned before, the memory usages on UNIREF and \nTREC of HS-tree exceed our computer‚Äôs limit, which is \nlarger than 32GB. In this case, it is over 30 times larger \nthan the memory usage of minIL. We can observe that the \nmemory usage of minIL+trie is low on most datasets, but it \nis the largest on READS. The reason for the difference is that \nthe dictionary size of READS is much larger than the other \ndatasets. It is well known that a large dictionary size has a \nsignificant negative impact of a trie-based index. The results \nalso indicate that minIL+trie is more suitable for datasets with \nsmall dictionary size. But minIL does not have such limitation. \nWe can draw the conclusion that minIL is a simple and small \nindex method  for string similarity search with edit distance.\n\n0 3 6 912 15 18 21 24 27 30\nalpha\n(a)012345Numbers1e2 UNIREF\n0.3\n0.4\n0.5\n0.6\n0.7\n0 3 6 912 15 18 21 24 27 30\nalpha\n(b)01234567Numbers1e3 TREC\n0.3\n0.4\n0.5\n0.6\n0.7\n0 3 6 912 15 18 21 24 27 30\nalpha\n(c)0.00.51.01.52.02.53.0Cumulative Numbers1e3 UNIREF\n0.3\n0.4\n0.5\n0.6\n0.7\n0 3 6 912 15 18 21 24 27 30\nalpha\n(d)01234Cumulative Numbers1e4 TREC\n0.3\n0.4\n0.5\n0.6\n0.7Fig. 7. Number of Candidates with Different \"\n0.03 0.06 0.09 0.12 0.15\nt103\n102\n101\nAverage Time (s)\nDBLP\nBed-tree\nMinSerach\nHS-tree\nminIL\nminIL+trie\n0.03 0.06 0.09 0.12 0.15\nt103\n102\n101\n100101Average Time (s)\nREADS\nBed-tree\nMinSerach\nHS-tree\nminIL\nminIL+trie\n0.03 0.06 0.09 0.12 0.15\nt102\n101\n100Average Time (s)\nUNIREF\nBed-tree\nMinSerach\nminIL\nminIL+trie\n0.03 0.06 0.09 0.12 0.15\nt102\n101\n100101Average Time (s)\nTREC\nBed-tree\nMinSerach\nminIL\nminIL+trie\nFig. 8. Average Query Time with Different t\nIt has a signiÔ¨Åcant advantage in handling large datasets with\nlong strings.\nE. Evaluation of the Optimizations for String Shift (Q4)\nWe use a synthetic dataset to evaluate the effectiveness\nof the proposed optimizations for string shift. The synthetic\ndataset only contains strings with string shift at the beginning\nor at the end of the query string. We generate the data as\nfollows: 1) First, we randomly generate a query string with\nlength of 1200, 2) Given a shift length factor \u0011, we Ô¨Åll or\ntruncate the query string at the beginning or at the end of it\nwith ~scharacters, where ~sis a random value in [0;\u0011jqj], 3)\nWe generate 100K strings according to step 2) for different \u0011,\nrespectively. We set \u0011= 0:05; 0:1;0:15; 0:2.\nFig. 9 presents the average accuracy of the algorithms\nwith different shift lengths. NoOpt is the proposed method\nminIL without any optimization. Opt1 isminIL with the\nÔ¨Årst optimization described in Sec. III, and we use 2\"at\nthe Ô¨Årst recursion. Opt2 isminIL with both two proposed\noptimizations (the second optimization is introduced in Sec.\nV). We setm= 1 throughout the experiments. The accuracy\nis deÔ¨Åned as ratio of the number of candidate strings to\nthe dataset cardinality. From the results, we observe that the\noriginal method has difÔ¨Åculty in dealing with the extreme\nstring shift, its accuracy is always less than 0.1. But with the\nÔ¨Årst optimization, the accuracy of the method improves up to\n0.7 when shift length is 0:05jqj, and then it decreases quickly\nwith the increase of the shift length. The second optimization\nhas a signiÔ¨Åcant effect. It helps the method achieve a perfect\naccuracy when shift length is small, and keeps a high accuracy\nwhen shift length increases. When the accuracy of Opt2 falls\nto a low level, for example when the shift length is 0:2jqj,\nit indicates that the variants of the query of the optimization\n0.05*|q| 0.1*|q| 0.15*|q| 0.2*|q|\nShift Length0.00.20.40.60.81.0AccuracyAverage Accuracy\nNoOpt\nOpt1\nOpt2Fig. 9. Average Accuracy with Different Shift Length\ncannot cover all string shift possibilities. In this case, we only\nneeds to increase mto solve the problem.\nVII. R ELATED WORK\nIn this section we review the related work on similarity\nsearch and join problem.\nSimilarity Search. Similarity search with edit distance has\nbeen studied extensively in the literature [7], [12], [13], [15],\n[16], [19], [24], [27], [28]. Many of existing studies are de-\nvised to address the threshold-based similarity search problem,\nwhile the others aim to support the top-k similarity search.\nFor threshold-based similarity search, Li et al. [12] improved\nthe count Ô¨Ålter [16] based method and designed several list-\nmerge algorithms to tackle the problem. Li et al. [13] proposed\nvariable length grams based method which advisably chooses\nhigh-quality grams of different lengths to address the prob-\nlem. Qin et al. [15] proposed asymmetric signature schemes\nnamed QChunk and developed several dynamic programming-\n\nbased candidate pruning methods. Wang et al. [19] proposed\nan adaptive preÔ¨Åx Ô¨Åltering framework and a cost model to\njudiciously select an appropriate preÔ¨Åx for each object to\nspeed up the performance. Deng et al. [7] proposed a pivotal\npreÔ¨Åx Ô¨Ålter based method which signiÔ¨Åcantly reduces the\nunnecessary signatures. Zhang et al. [27] proposed a local hash\nminima method to extract common substrings between similar\nstrings. For top- ksimilarity search, Yang et al. [23] utilized\nan adaptive q-gram selection and several efÔ¨Åcient strategies\nto explore the problem. Deng et al. [8] used a trie index to\nshare common preÔ¨Åxes and proposed a range-based algorithm\nby grouping speciÔ¨Åc entries to avoid duplicated computations.\nWang et al. [21] designed a novel Ô¨Ålter-and-reÔ¨Åne pipeline\napproach that used long but approximate n-gram matches\nfor candidates pruning. Zhang et al. [28] developed Bed-tree\nutilizing B+-tree to index strings and Yu et al. [24] devised\na uniÔ¨Åed framework using hierarchical segment tree (HS-tree)\nto support both threshold and top- ksimilarity search.\nMost existing methods have two limitations. First, many\nalgorithms using q-gram based signatures have poor pruning\npower, since the value qis typically very small to avoid\nmissing results, and a small qhas limited pruning power.\nSecond, most existing algorithms do not perform well on long\nstrings compared with short strings. Long strings may cause\nmassive redundancy indexes that reduce the search efÔ¨Åciency,\nand signature-based methods cannot capture long signatures .\nSimilarity Join. Similarity join, i.e., given two string col-\nlection to Ô¨Ånd all similar string pairs, is a closely related\nproblem. The problem has been studied extensively as well\n[14], [15], [18]‚Äì[20], [22], [24]‚Äì[26]. Yu et al. [24] provided\na comprehensive survey of similarity join. Wandelt et al. [17]\nreported some state-of-the-art methods of string similarity\nsearch and join. Bayardo et al. [2] Ô¨Årst proposed the preÔ¨Åx\nÔ¨Ålter-based method for Ô¨Ånding similar pairs of vectors. Xiao et\nal. [22] proposed the Ed-join with perspective of investigating\nmismatching pairs to improve the preÔ¨Åx Ô¨Ålter. Ciaccia et al.\n[6] proposed M-tree that organizes and searches large data\nsets from a generic metric space. Arasu et al. [1] devised\nenumeration-based algorithm to produce exact answers with\nprecise performance guarantees. Wang et al. [18] utilized the\ntrie structure to efÔ¨Åciently Ô¨Ånd the similar string pairs based\non subtrie pruning to support similarity join of short strings.\nLi et al. [14] proposed Pass-Join that partitions a string into a\nset of segments with efÔ¨Åcient substring selection. Adapt [19],\nthe adaptive preÔ¨Åx Ô¨Åltering framework, and QChunk [15] can\nalso be applied to similarity join problem. Wang et al. [20]\nproposed VChunk extracting non-overlapping substrings from\nstrings with a class of new chunking schemes. Zhang et al. [25]\nproposed EmbedJoin which integrated the CGK-embedding\n[4] and LSH techniques [10] to handle the join problem. Zhang\net al. [26] proposed string partition based local hash minima\nmethod to achieve a perfect accuracy of the join results.\nVIII. C ONCLUSION\nIn this paper, we study the threshold-based string simi-\nlarity search problem with the edit distance. We present aMinCompact algorithm to construct sketch representations\nfor strings and propose a small and simple index, minIL ,\nto store and search the sketch strings. BeneÔ¨Åting from the\nsketch representations and the learned index technique, minIL\noutperforms the existing methods on large datasets with long\nstrings and substantially reduces the space consumption. We\nconsider that minIL has an advantage for solving the threshold-\nbased similarity query with edit distance. It works well on\ndifferent datasets and it is applied to each string independently\nwhile implicitly aligning the strings. In addition, although\nminIL is an approximate method, it enable to achieve a perfect\naccuracy by adjusting parameters.\nIn the future work, we plan to study how to apply the\ntechnique of minIL for other important and relevant problems,\nsuch as the similarity join and top- ksimilarity search.\nREFERENCES\n[1] Arvind Arasu, Venkatesh Ganti, and Raghav Kaushik. EfÔ¨Åcient exact\nset-similarity joins. In VLDB , pages 918‚Äì929. ACM, 2006.\n[2] Roberto J. Bayardo, Yiming Ma, and Ramakrishnan Srikant. Scaling up\nall pairs similarity search. In WWW , pages 131‚Äì140. ACM, 2007.\n[3] Andrei Z. Broder, Moses Charikar, Alan M. Frieze, and Michael\nMitzenmacher. Min-wise independent permutations. J. Comput. Syst.\nSci., 60(3):630‚Äì659, 2000.\n[4] Diptarka Chakraborty, Elazar Goldenberg, and Michal Kouck ¬¥y. Stream-\ning algorithms for embedding and computing edit distance in the low\ndistance regime. In STOC , pages 712‚Äì725. ACM, 2016.\n[5] Moses Charikar, OÔ¨År Geri, Michael P. Kim, and William Kuszmaul. On\nestimating edit distance: Alignment, dimension reduction, and embed-\ndings. In ICALP , volume 107 of LIPIcs , pages 34:1‚Äì34:14. Schloss\nDagstuhl - Leibniz-Zentrum f ¬®ur Informatik, 2018.\n[6] Paolo Ciaccia, Marco Patella, and Pavel Zezula. M-tree: An efÔ¨Åcient\naccess method for similarity search in metric spaces. In VLDB , pages\n426‚Äì435. Morgan Kaufmann, 1997.\n[7] Dong Deng, Guoliang Li, and Jianhua Feng. A pivotal preÔ¨Åx based\nÔ¨Åltering algorithm for string similarity search. In SIGMOD Conference ,\npages 673‚Äì684. ACM, 2014.\n[8] Dong Deng, Guoliang Li, Jianhua Feng, and Wen-Syan Li. Top-k string\nsimilarity search with edit-distance constraints. In ICDE , pages 925‚Äì\n936. IEEE Computer Society, 2013.\n[9] Paolo Ferragina and Giorgio Vinciguerra. The pgm-index: a fully-\ndynamic compressed learned index with provable worst-case bounds.\nProc. VLDB Endow. , 13(8):1162‚Äì1175, 2020.\n[10] Aristides Gionis, Piotr Indyk, and Rajeev Motwani. Similarity search\nin high dimensions via hashing. In VLDB , pages 518‚Äì529. Morgan\nKaufmann, 1999.\n[11] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Poly-\nzotis. The case for learned index structures. In SIGMOD Conference ,\npages 489‚Äì504. ACM, 2018.\n[12] Chen Li, Jiaheng Lu, and Yiming Lu. EfÔ¨Åcient merging and Ô¨Åltering\nalgorithms for approximate string searches. In ICDE , pages 257‚Äì266.\nIEEE Computer Society, 2008.\n[13] Chen Li, Bin Wang, and Xiaochun Yang. VGRAM: improving perfor-\nmance of approximate queries on string collections using variable-length\ngrams. In VLDB , pages 303‚Äì314. ACM, 2007.\n[14] Guoliang Li, Dong Deng, Jiannan Wang, and Jianhua Feng. PASS-JOIN:\nA partition-based method for similarity joins. Proc. VLDB Endow. ,\n5(3):253‚Äì264, 2011.\n[15] Jianbin Qin, Wei Wang, Yifei Lu, Chuan Xiao, and Xuemin Lin.\nEfÔ¨Åcient exact edit similarity query processing with the asymmetric\nsignature scheme. In SIGMOD Conference , pages 1033‚Äì1044. ACM,\n2011.\n[16] Sunita Sarawagi and Alok Kirpal. EfÔ¨Åcient set joins on similarity\npredicates. In SIGMOD Conference , pages 743‚Äì754. ACM, 2004.\n[17] Sebastian Wandelt, Dong Deng, Stefan Gerdjikov, Shashwat Mishra,\nPetar Mitankin, Manish Patil, Enrico Siragusa, Alexander Tiskin, Wei\nWang, Jiaying Wang, and Ulf Leser. State-of-the-art in string similarity\nsearch and join. SIGMOD Rec. , 43(1):64‚Äì76, 2014.\n\n[18] Jiannan Wang, Guoliang Li, and Jianhua Feng. Trie-join: EfÔ¨Åcient trie-\nbased string similarity joins with edit-distance constraints. Proc. VLDB\nEndow. , 3(1):1219‚Äì1230, 2010.\n[19] Jiannan Wang, Guoliang Li, and Jianhua Feng. Can we beat the preÔ¨Åx\nÔ¨Åltering?: an adaptive framework for similarity join and search. In\nSIGMOD Conference , pages 85‚Äì96. ACM, 2012.\n[20] Wei Wang, Jianbin Qin, Chuan Xiao, Xuemin Lin, and Heng Tao Shen.\nVchunkjoin: An efÔ¨Åcient algorithm for edit similarity joins. IEEE Trans.\nKnowl. Data Eng. , 25(8):1916‚Äì1929, 2013.\n[21] Xiaoli Wang, Xiaofeng Ding, Anthony K. H. Tung, and Zhenjie Zhang.\nEfÔ¨Åcient and effective KNN sequence search with approximate n-grams.\nProc. VLDB Endow. , 7(1):1‚Äì12, 2013.\n[22] Chuan Xiao, Wei Wang, and Xuemin Lin. Ed-join: an efÔ¨Åcient algorithm\nfor similarity joins with edit distance constraints. Proc. VLDB Endow. ,\n1(1):933‚Äì944, 2008.\n[23] Zhenglu Yang, Jianjun Yu, and Masaru Kitsuregawa. Fast algorithms\nfor top-k approximate string matching. In AAAI . AAAI Press, 2010.\n[24] Minghe Yu, Jin Wang, Guoliang Li, Yong Zhang, Dong Deng, and\nJianhua Feng. A uniÔ¨Åed framework for string similarity search with\nedit-distance constraint. VLDB J. , 26(2):249‚Äì274, 2017.\n[25] Haoyu Zhang and Qin Zhang. Embedjoin: EfÔ¨Åcient edit similarity joins\nvia embeddings. In KDD , pages 585‚Äì594. ACM, 2017.\n[26] Haoyu Zhang and Qin Zhang. Minjoin: EfÔ¨Åcient edit similarity joins\nvia local hash minima. In KDD , pages 1093‚Äì1103. ACM, 2019.\n[27] Haoyu Zhang and Qin Zhang. Minsearch: An efÔ¨Åcient algorithm for\nsimilarity search under edit distance. In KDD , pages 566‚Äì576. ACM,\n2020.\n[28] Zhenjie Zhang, Marios Hadjieleftheriou, Beng Chin Ooi, and Divesh\nSrivastava. Bed-tree: an all-purpose index structure for string similarity\nsearch based on edit distance. In SIGMOD Conference , pages 915‚Äì926.\nACM, 2010.",
  "textLength": 63944
}