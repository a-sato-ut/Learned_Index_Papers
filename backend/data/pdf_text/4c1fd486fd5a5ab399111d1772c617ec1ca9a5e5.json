{
  "paperId": "4c1fd486fd5a5ab399111d1772c617ec1ca9a5e5",
  "title": "Towards Practical Learned Indexing",
  "pdfPath": "4c1fd486fd5a5ab399111d1772c617ec1ca9a5e5.pdf",
  "text": "Towards Practical Learned Indexing (Extended Abstracts)\nMihail Stoian\nTUM\nmihail.stoian@tum.deAndreas Kipf\nMIT CSAIL\nkipf@mit.eduRyan Marcus\nMIT CSAIL, Intel Labs\nryanmarcus@mit.eduTim Kraska\nMIT CSAIL\nkraska@mit.edu\nABSTRACT\nLatest research proposes to replace existing index structures\nwith learned models. However, current learned indexes tend\nto have many hyperparameters, often do not provide any\nerror guarantees, and are expensive to build. We introduce\nPractical Learned Index (PLEX). PLEX only has a single\nhyperparameter \u000f(maximum prediction error) and o\u000bers a\nbetter trade-o\u000b between build and lookup time than state-\nof-the-art approaches. Similar to RadixSpline, PLEX con-\nsists of a spline and a (multi-level) radix layer. It \frst builds\na spline satisfying the given \u000fand then performs an ad-hoc\nanalysis of the distribution of spline points to quickly tune\nthe radix layer.\nAIDB Workshop Reference Format:\nMihail Stoian, Andreas Kipf, Ryan Marcus, Tim Kraska. Towards\nPractical Learned Indexing. AIDB 2021.\n1. INTRODUCTION\nWe introduce Practical Learned Index (PLEX). Compared\nto existing learned indexes, PLEX only has a single hyper-\nparameter\u000f(maximum prediction error) and is hence easy\nto use. PLEX builds upon RadixSpline [7] and CHT [2], and\nuses a spline layer to ensure an \u000ferror bound and a radix\nlayer that is inspired by CHT.\nRadixSpline. RadixSpline (RS) [7] consists of a linear spline\nmodel that approximates the CDF of the data within an\nerror bound and a radix table that indexes the computed\nspline points. The main feature of RS is that it can be built\nwith a constant amount of work per new element, which\nallows a single-pass build phase. However, when the radix\ntable is not able to properly index the spline keys (i.e., in\nthe presence of outliers), RS can have poor performance.\nPLEX addresses two problems of RS: (i) the radix layer is\nhard to parametrize and (ii) the radix layer can be a\u000bected\nby outliers. Despite the fact that PLEX needs to perform an\nadditional pass over the spline points array, PLEX manages\nto retain the high build performance of RS.\nThis article is published under a Creative Commons Attribution License\n(http://creativecommons.org/licenses/by/3.0/), which permits distribution\nand reproduction in any medium as well allowing derivative works, pro-\nvided that you attribute the original work to the author(s) and AIDB 2021.\n3rd International Workshop on Applied AI for Database Systems and Appli-\ncations (AIDBâ€™21), August 20, 2021, Copenhagen, Denmark.Hist-Tree. Hist-Tree (HT) [2] approximates the data dis-\ntributed as a histogram, instead of using functions, with\na fast radix tree-based traversal method to \fnd the right\nhistogram bucket. As such, HT has similarities with a tra-\nditional radix tree, as its indexing approach also uses the\nbinary representation of the keys, and with a learned index\nas it tries to approximate the data distribution rather us-\ning the traditional key-comparison present in B-tree traver-\nsals. HT recursively splits the data into bins until a certain\nthreshold for the number of elements is reached. Apart from\nallowing updates, Hist-Tree also comes in a compact version\n(CHT), optimized for read-only workloads. CHT is essen-\ntially a lookup table which stores the nodes of the initial\nHT and their pre\fx sums. CHT can either be built from\na HT, by running a depth-\frst pre-order tree traversal, or,\nas we have implemented it in this work, directly from the\ndata itself. The latter is more advantageous for our case, as\nkeys can be processed in chunks. While CHT did not have\nan auto-tuner, we now introduce one as part of this work\n(auto-tuning CHT is part of our auto-tuning process).\nOther Related Work. The \frst learned index, which paved\nthe way for the development of new alternatives of index\nstructures, is the recursive model index (RMI) [8]. RMIs use\nlearned models, arranged in a hierarchical structure, which\nare trained via supervised learning techniques on the cumu-\nlative distribution function (CDF) of the underlying data.\nSince then, a large suite of learned index structures have\nbeen proposed. Few of them support writes: PGM-index, a\nmulti-level structure, where each level represents an error-\nbounded piecewise linear regression [5] or ALEX [3], which\ncombines insights from RMI with proven storage and index-\ning techniques. With regard to multi-dimensional data, there\nexist several other recent approaches, including Flood [12]\nand Tsunami [4].\n2. PLEX\nPLEX is a combination of CHT [2] and RS [7]. We build\na spline on the underlying data, and then index the spline\npoints in a CHT. The resulting index structure features fast\nbuild times, error-bounded lookups, and is easy to use as it\nhas only one hyperparameter, the maximum error \u000f.\nRS employs a radix table for indexing its spline points.\nHowever, when the keys of the dataset cannot be easily in-\ndexed by a radix table, i.e., the longest common pre\fxes of\ntheir binary representation are large, RS can have a decrease\nin performance. PLEX addresses this issue by replacing the\nradix table with a radix tree, represented by CHT. Like RS,\n1arXiv:2108.05117v2  [cs.DB]  6 Nov 2021\n\nPLEX is built \\bottom-up\", by \frst constructing an error-\nbounded spline, which is then indexed in CHT.\nBuild Spline. The core part of PLEX is the linear spline\nmodelS, which approximates the position of each key k\nwithin\u000fpositions,\u000f >0. Formally, if p\u0003is the position of\nkin the CDF and ~ pis the approximated position computed\nby the spline, then j~p\u0000p\u0003j\u0014\u000f. In other words, the spline\nmodel always predicts the correct location of the data within\na maximum error of \u000f.\nThe error-bounded spline model is de\fned as a set of con-\nnected linear spline points, which are picked from the set of\nCDF points. An optimal spline, i.e., the one with the fewest\nnumber of spline points, can be computed via dynamic pro-\ngramming in O(N2), whereNis the CDF size. Since this\napproach does not scale for large datasets, we use instead\na greedy algorithm, which does not guarantee optimality\nanymore, but can be implemented in O(N) [13].\nA lookup consists of \frst determining in which spline seg-\nment\u001bthe keykis located, i.e., between which spline nodes\nthe key lies in, and then performing a linear interpolation\nin the respective segment. For more details on the error-\nbounded spline algorithm, we refer the reader to [13].\nBuild CHT. Once the spline points have been selected,\nthey can be indexed in CHT, using our new implementation,\nwhich iteratively builds each level of the tree by analyzing\nchunks of keys. This is di\u000berent from the proposed bulk-\nloading in [2], as our method directly builds CHT instead of\n\frst building a sparse tree.\nCHT has two hyperparameters: the number of radix bits\nrof each tree node, i.e., the fanout of the tree equals 2r,\nand the error \u000ewithin the index approximates the positions\nof the keys. Formally, if q\u0003is the position of a spline point\nand ~qis its estimated position computed by CHT, then q\u00032\nf~q;:::; ~q+\u000e\u00001g. This is a generalization of the radix table\nof RS, since setting \u000e=1leads to a CHT with a single node\n(a radix table). Notably, a radix table does not have a global\nbounded error (it must instead use the position of the next\npre\fx to obtain an upper bound on ~ q). This requires us to\ndevelop separate cost models for each of them (cf. Section 3).\nLookup. A lookup for key kstarts by searching the position\nof the spline segment \u001b. This routine is done in two steps:\nFirst, a lookup in CHT returns an approximated position ~ q.\nNext, a binary search is performed in the range f~q;:::; ~q+\u000e\u0000\n1gto \fnd the exact position of \u001b. Subsequently, we perform\na linear interpolation between the two spline points of \u001bto\nobtain an estimated CDF position ~ pof the key. Finally, we\nperform a binary search within the error bounds ~ p\u0006\u000fto\n\fnd the \frst occurrence of k.\n3. AUTO TUNING\nOne main drawback of (learned) index structures is the\nchoice of hyperparameters, as they have to be manually\ntuned in order to \fnd the best lookup time under certain\nconstraints (e.g., space).\nWe introduce cost models that approximate the lookup\ntimes and accurately compute the space consumption for\nboth radix table and CHT without building the actual data\nstructures.\nRadix Table. A radix table with parameter rsplits the\ninput data into 2rbuckets based on the \frst rmost signi\f-\ncant bits (pre\fx) of the keys. In RS, the input data for the\nradix table are the keys of the spline points. Therefore, alookup for key kconsists of \frst \fnding out in which radix\nbucketbkthe key is located, and then performing a local\nsearch on the spline nodes within the bucket to \fnd the ex-\nact spline segment. If the local search is implemented as a\nbinary search, then the number of steps equals dlog2(jbkj)e,\nwherejbkjis the number of spline points within bucket bk.\nAssume that only positive lookups are performed, i.e., the\nlookup key lies within the stored data D. Then the average\nlookup time \u0015rcan be estimated as\n\u0015r=1\njDjX\nk2Ddlog2(jbkj)e: (1)\nThis cost model has the advantage that it can be com-\nputed for all rwhile building the spline model, without\nstoring the actual radix tables. It also allows us to detect\nthe outlier problem of RS: Consider, for example, r= 1.\nThe optimal \u00151is achieved when the radix table splits the\nset of spline points in two equal-sized buckets, i.e., \u00151=\ndlog2(jSj\n2)e=dlog2(jSj)e\u00001. Note that we increased the\nnumber of bits and the cost decreased by \u00150\u0000\u00151= 1\nunit (r= 0 corresponds to a simple binary search, i.e.,\n\u00150=dlog2(jSj)e). This is not always the case: when all\nspline keys have the same value for the most-signi\fcant bit,\nthen the radix table is not able to split the initial bucket\nand we have \u00151=\u00150. In general, \u0015r\u0000\u0015r+12[0;1] tells\nus whether it is worth increasing the number of radix bits\nby one. Since we must update the model for each radix ta-\nble of parameter ras each spline node is computed, the time\ncomplexity is O(r+jSj), wherer+is the maximum rallowed.\nFinally, the memory consumption of a radix table with\nparameterrisO(2r), as we only need to store a value per\nbucket, namely the position of the \frst spline point with\nthat pre\fx.\nCHT. The same reasoning can be applied for CHT. The\naim is to consider a large set of CHTs with di\u000berent con\fg-\nurations (r;\u000e) and estimate their average lookup time and\nmemory consumption.\nA lookup for key kconsists of \frst \fnding out in which\nnodevkof CHT the key is located, i.e., the node which\ncontains the bin corresponding to kwith size\u0014\u000e, and then\nperforming a local search on the spline nodes within that\nbin. If the local search is implemented as a binary search,\nthen the number of search steps equals depth (vk)+dlog2(\u000e)e.\nAs the termdlog2(\u000e)eis the same for all lookup keys, we only\nhave to compute the average depth of the leaf nodes.\nIn the cost model for radix table (Eq. 1), the cost of each\nbucket is weighted by the number of data keys falling into\nthat bucket. This is more di\u000ecult for CHT, as we would\nhave to examine the original data multiple times, namely,\nas many times as the number of levels, to collect such statis-\ntics. Instead, we only calculate the average tree depth given\nlookups from the set of spline keys. Note that this is a simpli-\n\fcation that does not guarantee that we can still accurately\nmodel the average lookup time for the data itself. This was\nnot a problem for the radix table, where there is only one\nradix level to consider, and this one level can be covered\nwhen building the spline. Thus, the average lookup time\ncan be estimated as\n\u0015(r;\u000e)=dlog2(\u000e)e+1\njSjX\nk2Sdepth (vk): (2)\n2\n\nSpline keys\n00000\n10101\n20110\n30111\n41000\n51010\n61011\n7111112345670123\npositionlcp-lengthLCP -Histogram\nCHT (r= 1;\u000e= 2)\n[0,3] [4,7]\n[0,0] [1,3]\n[1,1] [2,3][4,6] [7,7]\n[4,4] [5,6]CHT (r= 2;\u000e= 2)\n[0,0] [1,3] [4,6] [7,7]\n;[1,1] [2,2] [3,3] [4,4];[5,5] [6,6]\nFigure 1: The lcp-histogram and depiction of strat-\negy for computing the average tree depths of a large\nset of trees with di\u000berent con\fgurations (r;\u000e). Lcp-\nlengthp= 1is used by the CHT with r= 1, but not\nalso by the one with r= 2, while lcp-length p= 2is\nused by both CHTs.\nTo compute the average tree depths for di\u000berent pairs\n(r;\u000e), we make use of the longest common pre\fx (lcp) of\nany two adjacent spline keys. To this end, we build the lcp-\nhistogram whose values are de\fned as lcpi:=lcp(keysi;\nkeysi\u00001);8i2[jSj\u00001] (cf. Fig. 1).\nThe idea is to traverse each lcp-length pof the histogram\nand \flter out the positions whose values are smaller than\np. The surviving positions form contiguous sequences rep-\nresenting exactly the intervals covered by bins in particular\nCHTs. If the length of a contiguous sequence exceeds the\nvalue of\u000efor a particular CHT, i.e., the corresponding bin\nof that interval is not a \fnal one, then that bin increases\nthe search path by one for each key kpassing through it.\nTo illustrate this method, consider again Fig. 1: At lcp-\nlengthp= 1, the surviving positions, after we \flter out the\npositions whose values are less than 1, form the contiguous\nsequences 1 ;2;3 and 5;6;7. Sincelcpalso considers the pre-\nvious key, the sequences actually de\fne the intervals [0 ;3]\nand [4;7] from the set of spline keys, which can be seen in\nthe \frst node of CHT( r= 1;\u000e= 2). Filtering out further\nin the same way at p= 2, we again get two contiguous se-\nquences, 2;3 and 5;6, representing the intervals [1 ;3] and\n[4;6], respectively. These de\fne the only two bins in both\nCHT(r= 1;\u000e= 2) and CHT( r= 2;\u000e= 2) that have outgo-\ning pointers, as the length of both intervals exceeds \u000e= 2.\nIn the same \fgure we see that lcp-length p= 1 is used by\nthe CHT with r= 1, i.e., the intervals are present in the\nbins of that particular CHT, but not also by the one with\nr= 2, whilep= 2 is used by both. Formally, a lcp-length pAlgorithm 1 Computation of average lookup times for mul-\ntiple pairs ( r;\u000e).\nInput: upper-bounds r+;\u000e+2Nonrand\u000e, respectively\nOutput:\u0015(r;\u000e);81\u0014r\u0014r+;1\u0014\u000e\u0014\u000e+\n1:intervals f[1;jSj\u00001]g\n2:for all lcp-lengthpdo\n3:for allIinintervals do\n4: for all maximal subinterval J\u0012I,\ns.t.lcpj\u0015p;8j2Jdo\n5: for allrin 1;:::;r+s.t.p\u0011r0do\n6: depthr(min(jJj\u00001;\u000e+)) +=jJj\n7: end for\n8: end for\n9:end for\n10:intervals newly found subintervals\n11:end for\n12:for allrin 1;:::;r+do\n13: for all\u000ein\u000e+\u00001;:::; 1do\n14:depthr(\u000e) +=depthr(\u000e+ 1)\n15: end for\n16: for all\u000ein 1;:::;\u000e+do\n17:\u0015(r;\u000e) dlog2(\u000e)e+depth r(\u000e)\njSj, acc. Eq. 2\n18: end for\n19:end for\nisused by a particular CHT( r;\u000e) ifp\u0011r0.\nThe method for computing \u0015(r;\u000e)for di\u000berent pairs ( r;\u000e)\nis given in Alg. 1. The algorithm receives as input the upper-\nboundsr+and\u000e+onrand\u000e, respectively, and outputs\naverage lookup times for all possible pairs. In Line 1, we\ninitialize the list of intervals with the entire range of keys,\ni.e., [1;jSj\u00001]. Then we iterate over all lcp-lengths pand split\nthe current intervals at positions with values < p(Line 4).\nEach new subinterval Jis consumed by updating depthr\nfor allrthat usethe current lcp-length p, i.e., that satisfy\np\u0011r0. In the end, depthr(\u000e) will store exactly the termX\nk2Sdepth (vk) from Eq. 2.\nAs mentioned earlier, if the length of the interval exceeds\nthe value of \u000efor a particular CHT, i.e., the corresponding\nbin of that interval is not a \fnal one, then that bin increases\nthe search path by one for each key kthat passes through\nit. The maximum \u000efor which this occurs is jJj\u00001. However,\nit would be expensive to update all \u000e\u0014jJj\u00001. Therefore,\nwe employ su\u000ex sums and only update depthr(jJj\u00001) in\nLine 6, while completing the update step for the others in\nLine 14. The algorithm \fnishes by computing \u0015(r;\u000e)accord-\ning to Eq. 2. Since we have at most 2pintervals for each\nlcp-lengthp, the algorithm takes O(r+(jSj+d+)) time.\nAs mentioned in [2], CHT can be stored as a \rat array\n(there is no need for pointers), by representing each node\nas 2rcells. Thus, the memory equals the number of nodes\ntimes 2r. The number of nodes can be computed exactly\nusing the above strategy, since each surviving interval rep-\nresents exactly a node. For instance, CHT( r= 1;\u000e= 2) from\nour example has four nodes apart from the root node, rep-\nresented by the aforementioned intervals [0 ;3], [4;7], [1;3],\nand [4;6], respectively.\nPLEX. The auto-tuning of PLEX relies on the optimization\nof both the radix table and CHT. After \u0015rand\u0015(r;\u000e)have\nbeen computed, we can choose the best average lookup time\n3\n\nFigure 2: Size versus build time on the four real-world datasets from SOSD.\nFigure 3: Size versus lookup time on the four real-world datasets from SOSD.\nunder the constraint that the space consumption does not\nexceed the space requirement of the spline model, i.e., in\nthe worst case the \fnal index structure is twice the size of\nthe spline model. This way, PLEX requires only a single\nhyperparameter \u000f.\n4. EVALUATION\nSetup. Experiments are conducted on a m5zn.metal AWS\nmachine with 192 GiB of RAM and 48 vCPUs (4.5 GHz turbo).\nDatasets. We evaluate PLEX using the SOSD benchmark\n[6, 10]. We use four 64-bit datasets, each of them contain-\ning 200 M key/value pairs (3.2 GiB): amzn (book popularity\ndata), face (Facebook user IDs), osm (composite cell IDs\nfrom Open Street Map) and wiki (timestamps of Wikipedia\nedits). See [10] for details on these datasets.\nBuild Time. We compare indexes regarding build time,\nas this highlights how expensive the learning process for\neach dataset is. We therefore consider the build time / size\ntrade-o\u000b o\u000bered by RMI, CHT, PGM, RS, and PLEX, re-\nspectively, for each dataset, as shown in Fig. 2. RS achieves\nthe lowest build times, due to its single-pass build phase.\nNote that the build time of PLEX already includes the auto-\ntuning time, unlike RS, CHT, or RMI [11], which were tuned\no\u000fine via an expensive grid search. Our current implemen-\ntation of CHT does not support key duplicates, which is the\ncase for the wiki dataset. PLEX does not have this prob-\nlem, as the spline keys are unique.\nProbe Time. We further compare with classical index struc-\ntures, namely ART [9] and BTree [1]. In the following, we\nconsider the performance / size trade-o\u000b o\u000bered by each in-\ndex structure for each dataset, as shown in Fig. 3. A \frst ob-servation is that PLEX successfully solves the outlier prob-\nlem of RS on the face dataset, while preserving the perfor-\nmance of RS on the other datasets. As pointed out in [10],\ntheosm dataset is di\u000ecult to learn, as evidenced by the\nbehavior of RMI on this dataset. However, that is exactly\nwhere the radix approach of RS and PLEX is bene\fcial. An\ninteresting fact to observe for both amzn andface datasets\nis the constant o\u000bset of \u001925ns between PLEX and RMI.\nIt should also be noted that for almost all datasets, PGM,\nART, and BTree eventually lose performance as the index\nsize is increased, because the cost of navigating the index\nstructure no longer makes up for the time required to bi-\nnary search the underlying data.\nAuto Tuning. We have empirically veri\fed that our auto-\ntuning strategy succeeds in \fnding optimal con\fgurations\nfor PLEX, which have been found by grid-search on the\nhyperparameter triple ( \u000f;r;\u000e ), with\u000f;\u000e2f21;:::; 210gand\nr2f1;:::; 10g. The auto-tuner correctly decides to fall back\non the radix table as its subindex on all datasets except\ntheface dataset (where it successfully detects the outlier\nproblem and decides to use CHT). In some cases, it even\noutperforms the manually-tuned PLEX because it explored\nmore options for rand\u000e.\n5. CONCLUSIONS\nWe have introduced PLEX, an auto-tuned learned index\nwhich o\u000bers a better trade-o\u000b between build and lookup\ntime than state-of-the-art approaches. PLEX is easy to use,\nas its only hyperparameter is the maximum prediction error.\nIn future work, we plan to extend PLEX to support e\u000ecient\nupdates by using a Fenwick tree on each tree node.\n4\n\nAcknowledgments. This research is supported by Google,\nIntel, and Microsoft as part of DSAIL at MIT, and NSF IIS\n1900933. This research was also sponsored by the United\nStates Air Force Research Laboratory and the United States\nAir Force Arti\fcial Intelligence Accelerator and was accom-\nplished under Cooperative Agreement Number FA8750-19-\n2-1000. The views and conclusions contained in this docu-\nment are those of the authors and should not be interpreted\nas representing the o\u000ecial policies, either expressed or im-\nplied, of the United States Air Force or the U.S. Govern-\nment. The U.S. Government is authorized to reproduce and\ndistribute reprints for Government purposes notwithstand-\ning any copyright notation herein.\n6. REFERENCES\n[1] R. Bayer and E. M. Mccreight. Organization and\nmaintenance of large ordered indexes. Acta Inf. ,\n1(3):173{189, Sept. 1972.\n[2] A. Crotty. Hist-Tree: Those who ignore it are doomed\nto learn. In 11th Conference on Innovative Data\nSystems Research, CIDR 2021, Virtual Event,\nJanuary 11-15, 2021, Online Proceedings .\nwww.cidrdb.org, 2021.\n[3] J. Ding, U. F. Minhas, J. Yu, C. Wang, J. Do, Y. Li,\nH. Zhang, B. Chandramouli, J. Gehrke, D. Kossmann,\nD. B. Lomet, and T. Kraska. ALEX: an updatable\nadaptive learned index. In D. Maier, R. Pottinger,\nA. Doan, W. Tan, A. Alawini, and H. Q. Ngo, editors,\nProceedings of the 2020 International Conference on\nManagement of Data, SIGMOD Conference 2020,\nonline conference [Portland, OR, USA], June 14-19,\n2020, pages 969{984. ACM, 2020.\n[4] J. Ding, V. Nathan, M. Alizadeh, and T. Kraska.\nTsunami: A learned multi-dimensional index for\ncorrelated data and skewed workloads. Proc. VLDB\nEndow. , 14(2):74{86, 2020.\n[5] P. Ferragina and G. Vinciguerra. The PGM-index: a\nfully-dynamic compressed learned index with provable\nworst-case bounds. Proc. VLDB Endow. ,\n13(8):1162{1175, 2020.\n[6] A. Kipf, R. Marcus, A. van Renen, M. Stoian,\nA. Kemper, T. Kraska, and T. Neumann. SOSD: A\nbenchmark for learned indexes. NeurIPS Workshop on\nMachine Learning for Systems , 2019.\n[7] A. Kipf, R. Marcus, A. van Renen, M. Stoian,\nA. Kemper, T. Kraska, and T. Neumann.RadixSpline: a single-pass learned index. In\nR. Bordawekar, O. Shmueli, N. Tatbul, and T. K. Ho,\neditors, Proceedings of the Third International\nWorkshop on Exploiting Arti\fcial Intelligence\nTechniques for Data Management, aiDM@SIGMOD\n2020, Portland, Oregon, USA, June 19, 2020 , pages\n5:1{5:5. ACM, 2020.\n[8] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and\nN. Polyzotis. The case for learned index structures. In\nG. Das, C. M. Jermaine, and P. A. Bernstein, editors,\nProceedings of the 2018 International Conference on\nManagement of Data, SIGMOD Conference 2018,\nHouston, TX, USA, June 10-15, 2018 , pages 489{504.\nACM, 2018.\n[9] V. Leis, A. Kemper, and T. Neumann. The Adaptive\nRadix Tree: ARTful indexing for main-memory\ndatabases. In Proceedings of the 2013 IEEE\nInternational Conference on Data Engineering (ICDE\n2013) , ICDE '13, page 38{49, USA, 2013. IEEE\nComputer Society.\n[10] R. Marcus, A. Kipf, A. van Renen, M. Stoian,\nS. Misra, A. Kemper, T. Neumann, and T. Kraska.\nBenchmarking learned indexes. Proc. VLDB Endow. ,\n14(1):1{13, 2020.\n[11] R. Marcus, E. Zhang, and T. Kraska. CDFShop:\nExploring and optimizing learned index structures. In\nProceedings of the 2020 ACM SIGMOD International\nConference on Management of Data , SIGMOD '20,\npage 2789{2792, New York, NY, USA, 2020.\nAssociation for Computing Machinery.\n[12] V. Nathan, J. Ding, M. Alizadeh, and T. Kraska.\nLearning multi-dimensional indexes. In D. Maier,\nR. Pottinger, A. Doan, W. Tan, A. Alawini, and H. Q.\nNgo, editors, Proceedings of the 2020 International\nConference on Management of Data, SIGMOD\nConference 2020, online conference [Portland, OR,\nUSA], June 14-19, 2020 , pages 985{1000. ACM, 2020.\n[13] T. Neumann and S. Michel. Smooth interpolating\nhistograms with error guarantees. In W. A. Gray,\nK. G. Je\u000bery, and J. Shao, editors, Sharing Data,\nInformation and Knowledge, 25th British National\nConference on Databases, BNCOD 25 , volume 5071 of\nLecture Notes in Computer Science , pages 126{138,\nCardi\u000b, UK, July 2008. Springer.\n5",
  "textLength": 23908
}