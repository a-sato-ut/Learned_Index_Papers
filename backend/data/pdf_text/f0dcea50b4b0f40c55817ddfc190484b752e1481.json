{
  "paperId": "f0dcea50b4b0f40c55817ddfc190484b752e1481",
  "title": "Network Representation Learning: Consolidation and Renewed Bearing",
  "pdfPath": "f0dcea50b4b0f40c55817ddfc190484b752e1481.pdf",
  "text": "Network Representation Learning: Consolidation and\nRenewed Bearing\nSaket Gurukar\u00031, Priyesh Vijayan*2, 4, Aakash Srinivasan*2, Goonmeet Bajaj1, Chen Cai1, Moniba Keymanesh1, Saravana\nKumar1, Pranav Maneriker1, Anasua Mitra3, Vedang Patel1, Balaraman Ravindran2, 4, and Srinivasan Parthasarathy1\n1Computer Science and Engineering, The Ohio State University\n2Department of Computer Science and Engineering, IIT Madras\n3Department of Computer Science and Engineering, IIT Guwahati\n4Robert Bosch Centre for Data Sciences and AI, IIT Madras\nABSTRACT\nGraphs are a natural abstraction for many problems where\nnodes represent entities and edges represent a relationship\nacross entities. The abstraction can be explicit (e.g., trans-\nportation networks, social networks, foreign key relation-\nships) or implicit (e.g., nearest neighbor problems). An\nimportant area of research that has emerged over the last\ndecade is the use of graphs as a vehicle for non-linear di-\nmensionality reduction in a manner akin to previous ef-\nforts based on manifold learning with uses for downstream\ndatabase processing (e.g., entity resolution and link predic-\ntion, outlier analysis), machine learning and visualization.\nIn this systematic yet comprehensive experimental survey,\nwe benchmark several popular network representation learn-\ning methods operating on two key tasks: link prediction and\nnode classi\fcation.\nWe examine the performance of 12 unsupervised embed-\nding methods on 15 datasets. To the best of our knowledge,\nthe scale of our study { both in terms of the number of\nmethods and number of datasets { is the largest to date.\nOur benchmarking study in as far as possible uses the orig-\ninal codes provided by the original authors.\nOur results reveal several key insights about work-to-date\nin this space. First, we \fnd that certain baseline methods\n(task-speci\fc heuristics, as well as classic manifold meth-\nods) that have often been dismissed or are not considered\nby previous e\u000borts can compete on certain types of datasets\nif they are tuned appropriately. Second, we \fnd that recent\nmethods based on matrix factorization o\u000ber a small but rel-\natively consistent advantage over alternative methods (e.g.,\nrandom-walk based methods) from a qualitative standpoint.\n\u0003Equal contribution. Rest of the authors are listed in the\nalphabetical order of their last namesSpeci\fcally, we \fnd that MNMF, a community preserving\nembedding method, is the most competitive method for the\nlink prediction task. While NetMF is the most competitive\nbaseline for node classi\fcation. Third, no single method\ncompletely outperforms other embedding methods on both\nnode classi\fcation and link prediction tasks. We also present\nseveral drill-down analysis that reveals settings under which\ncertain algorithms perform well (e.g., the role of neighbor-\nhood context on performance; dataset characteristics that\nin\ruence performance) { guiding the end-user.\n1. INTRODUCTION\nGraphs are e\u000bective in multiple disparate domains to model,\nquery and mine relational data. Examples abound ranging\nfrom the use of nearest neighbor graphs in database systems\n[18, 63] and machine learning [9, 43] to the analysis of bio-\nlogical networks [24, 7] and from social network analysis [22,\n62] to the analysis of transportation networks[13]. ML-\nenhanced data structures and algorithms such as learned in-\ndexes [31] have recently shown promising results in database\nsystems. An active area in ML research { network repre-\nsentation learning { has potential in multiple applications\nrelated to the downstream database processing tasks such\nas outlier analysis [63, 34], entity resolution [15, 19], link\nprediction [35, 47] and visualization [17, 50]. However, a\nplethora of new network representation learning methods\nhas been proposed recently [11, 25]. Given the wide range\nof methods proposed, it is often tough for a practitioner\nto determine or understand which of these methods they\nshould consider adopting for a particular task on a particu-\nlar dataset. Part of the challenge is the lack of a standard\nevaluation benchmark and a thorough independent under-\nstanding of the strengths and weaknesses of each method\nfor the particular task on hand. The challenges are daunt-\ning and can be summarized as follows:\nLack of Standard Assessment Protocol: First, there\nis no standard to evaluate the quality of generated embed-\ndings. The e\u000ecacy among embedding methods is often eval-\nuated based on downstream machine learning tasks. As a re-\nsult, the superiority of one embedding method over another\nhinges on the performance in a downstream machine learn-\ning task. With the lack of a standard evaluation protocol forarXiv:1905.00987v2  [cs.LG]  15 Jun 2019\n\nthese downstream tasks, the results reported by di\u000berent re-\nsearch articles are often inconsistent. As a speci\fc example,\nNode2vec [21] report the node classi\fcation performance of\nDeepwalk on Blogcatalog dataset { for multi-label classi\f-\ncation with a train-test split of 50:50 { as 21.1% Macro-f1,\nwhereas the Deepwalk paper [47] reports Deepwalks' perfor-\nmance as 27.3% Macro-f1.\nTuning Comparative Strawman: Second, a new method\nalmost always compares its performance against a subset of\nother methods, and on a subset of tasks and datasets pre-\nviously evaluated. In many cases, while great care is taken\nto tune the new method (via careful hyper-parameter tun-\ning) { the same care is often not taken when evaluating\nbaselines. For example, in our experiments on Blogcata-\nlog, we \fnd that with a train-test split of 50:50 the Lapla-\ncian Eigenmaps method [6] without Grid-Search achieves a\nMacro-f1 score of 3.9% (similar to what was reported in [20,\n21]). However, with tuning the hyper-parameters of logistic\nregression, we \fnd that the Laplacian Eigenmaps method\nachieves a Macro-F1 of 29.2% . Importantly, while logis-\ntic regression is commonly used to evaluate the quality of\nnode embeddings in such methods, Grid-Search over logis-\ntic regression parameters is rarely conducted or reported.\nAdditionally, reported results are rarely averaged over mul-\ntiple shu\u000fes to reduce any bias or patterns in the training\ndata1. In short, a lack of consistency in evaluation inhibits\nour understanding of the scienti\fc advances in this arena,\ndiscussed next.\nStandard Benchmark: Third, there is no agreed list of\ndatasets that are used consistently in the literature. A\nnew embedding method evaluates their method on selected\ndatasets with a suitable node classi\fcation/link prediction\nsetup. For instance, few methods report node classi\fca-\ntion performance for the baselines with the train-test split of\n10:90 while few methods report the same with the train-test\nsplit of 50:50. As a result, the comparison across embedding\nmethods is often unclear. Additionally, there are no clear\nguidelines on whether the proposed embedding methodology\nfavors a certain type of dataset characteristic (e.g., sparsity).\nTask Speci\fc Baselines: Fourth, for many tasks such as\nnode classi\fcation and link prediction there is a rich pre-\nexisting literature [8, 36] focused on such tasks (that do not\nexplicitly rely on node embedding methodology as a prepro-\ncessing step). Few, if any, of the prior art in network repre-\nsentation learning consider such baselines { often such meth-\nods compare performance on downstream ML tasks against\nother node embedding methods. In our experiments, we \fnd\nthat a curated feature vector based on heuristics can achieve\na similar competitive AUROC score on many of the datasets\nfor the link prediction task.\nTo summarize, there is a clear and pressing need for a\ncomprehensive and careful benchmarking of such methods\nwhich is the focus of this study. To address the aforemen-\ntioned issues in the network embedding literature, we per-\nform an experimental study of 12 promising network embed-\ndings methods on 15 diverse datasets. The selected embed-\nding methods are unsupervised techniques to generate the\nnode embeddings of a graph. Our goal is to perform a uni-\nform, principled comparison of these methods on a variety of\n1This is our observation based on the evaluation scripts pub-\nlically shared by multiple authors.di\u000berent datasets and across two key tasks { link prediction\nand node classi\fcation.\nSpeci\fc \fndings of our experimental survey that we wish\nto highlight are that:\n1. For the link-prediction task we \fnd that MNMF [59], a\ncommunity preserving embedding method, o\u000bers a com-\npelling advantage when it scales to a particular dataset.\nOther more scalable alternatives such as Verse and LINE\nalso perform well on most datasets. The heuristic ap-\nproach we present for link prediction competes excep-\ntionally well on all types of datasets surveyed for this\ntask.\n2. For the node classi\fcation task, NetMF [49] when it scales\nto the particular dataset, o\u000bers a small but consistent\nperformance advantage. We \fnd that for node-classi\fcation\ntask, the task-speci\fc heuristic methodology we compare\nwith works well when operating on datasets with fewer la-\nbels { in such scenarios, it competes well with a majority\nof the methods surveyed, whereas, some recent methods\nproposed fare much worse.\n3. We also drill down to study the impact of context embed-\ndings on the link prediction and node classi\fcation tasks\n(and \fnd some methods impervious to the use of context\n{ where for others it helps signi\fcantly). We also exam-\nine two common ways in which link prediction strategies\nare evaluated (explicitly through a classi\fer, or implicitly\nthrough vector dot-product ranking). We \fnd that there\nis a clear separation in performance when using these al-\nternative strategies.\n2. NOTATIONS AND DEFINITIONS\nWe denote the input graph as G= (V;E) whereVand\nEdenote the set of nodes and edges of the graph, G. The\nnotations used in this work are listed in Table 1. In this\nstudy, we consider both directed as well as undirected graphs\nalong with weighted as well as unweighted graphs. We eval-\nuate the embedding methods on non-attributed, homoge-\nneous graphs.\nDefinition 2.1. Network Embedding: Given a Graph, G=\n(V;E)and an embedding dimension, dwhered\u001cjVj, the\ngoal of a network embedding method is to learn a d-dimensional\nrepresentation of the graph, Gsuch that similarity in graph\nspace approximates closeness in d-dimensional space.\n3. NETWORK EMBEDDING METHODS\nIn this section, we give a summary of the network em-\nbedding methods evaluated in our work. Herein, for each\nmodels along with their description, we also provide addi-\ntional experimental details for reproducibility.\n1.Laplacian Eigenmaps [6] : Laplacian Eigenmaps gen-\nerates ad-dimensional embedding of the graph us-\ning the smallest deigenvectors of Laplacian matrix\nL=D\u0000A.\nminimize\nUtrace (UTLU)\nsubject to UTDU=I\nUis generated embedding matrix RjVj\u0003d. The above\nequation can be reduced to simple minimization of\n\nSymbol Meaning\nG Input graph\nV Nodes\nE Edges\nn Number of nodes, jVj\nA Adjacency matrix. A2Rn\u0003n\nD Degree Matrix of Graph.\nDi;i=Pn\nj=0Ai;jandDi;j= 0 wherei6=j\nI Identity Matrix\n\b(u) Node embedding of node u\n (u) Context embedding of node u\nU;V Node and context embedding matrix of size Rn\u0003d\nvol(G)Sum of weights of all edges\nS Graph Similarity matrix\n\u001b(x) a non-linear function such as Sigmoid function\n\u0015 Number of negative samples\nPD\u00001A\nLrwLrw=I\u0000D\u00001A\nLsymLsym=I\u0000D\u00001=2AD\u00001=2\nTable 1: The notations symbols used in this study and the\nterms associated with those symbols.\nL2 distance for adjacent nodes - \u0006 i;jjju(i)\u0000u(j)jj2Aij.\nLaplacian Eigenmaps levers the \frst order information\nfor generating the embeddings.\nReproducibility notes : We search for following hy-\nperparameters: Embedding dimension = [64, 128, 256].\nOn the datasets with >1M nodes, Laplacian Eigen-\nmaps did not scale for embedding dimension 128, 256.\n2.DeepWalk [47] : DeepWalk is a random walk based\nnetwork embedding method which uses truncated ran-\ndom walks and levers local information from these gen-\nerated walks to learn similar latent representations.\nDeepWalk draws inspiration from Skip-gram model in\nWord2vec [39] by treating random walks as sequences\nand optimizing following objective function:\nminimize \b\u0000logPr(fvi\u0000w;\u0001\u0001\u0001;vi+wgnvij\b(vi))\n(1)\nwhereviis target node while fvi\u0000w;\u0001\u0001\u0001;vi+wgnviare\nthe context nodes. \b( vi) denotes the embedding of\nthe nodevi. Since the objective function is expensive\nto compute for large graphs, it is approximated using\nHierarchical Softmax[40].\nReproducibility notes : We search for following hy-\nperparameters: Walk length = [5, 20, 40], Number of\nwalks = [20, 40, 80], Window size = [2, 4, 10], Embed-\nding dimension = [64, 128, 256]. In the case of directed\ngraphs, we observe lower performance in node classi-\n\fcation and link prediction task. In order to have a\nfair comparison with other methods, we treat directed\ngraphs as undirected for DeepWalk.\n3.Node2Vec [21] : Node2Vec is a biased random walk\nbased network embedding method which allows the\nrandom walk to be more \rexible in exploring the graph\nneighborhoods. The \rexibility of the random walk is\nachieved by interpolating between Breadth-\frst traver-\nsal and Depth-\frst traversal. The objective function is\nagain based on Skip-gram model -based on Word2vec[39], and since the objective function is expensive to\ncompute for large graphs, it is approximated by nega-\ntive sampling [39].\nReproducibility notes : We search for following hy-\nperparameters: Walk length = [10, 20, 40], Number of\nwalks = 80. Window Size = 10, pandq= [0.25, 1,\n2, 4], Embedding dimension = [64, 128, 256]. In case\nof directed graphs, we observe lower performance in\nnode classi\fcation and link prediciton task. In order\nto have a fair comparison with other methods, we treat\ndirected graphs as undirected for Node2Vec.\n4.GraRep [12] : GraRep is a matrix factorization based\nnetwork embedding method which captures the global\nstructural information of the graph while learning node\nembeddings. The authors observe that the existing\nSkip-gram based models project all the k-step rela-\ntional information into a common subspace and then,\nargue the importance of preserving di\u000berent k-step re-\nlational information in separate subspaces. The loss\nfunction to preserve the k-step relationship between\nnodewandcis proposed as:\nLk(u;v) =Ak\nu;v\u0001log\u001b(\b(u)\u0001\b(v))\n+\u0015\njVjX\nv02V;v06=vAk\nu;v0\u0001log\u001b(\u0000\b(u)\u0001\b(v0))\n(2)\nwherev0refers to the negative node at k-th step for\nnodeu(see Table 1 for additional notation, e.g. \u0015).\nThe above loss function in closed form results in log-\ntransformed, probabilistic adjacency matrix which is\nfactorized with SVD for generating each k-step repre-\nsentation. The \fnal node representation is generated\nby concatenation of all the k-step representations.\nReproducibility notes : We search for following hy-\nperparameters: kfrom 1 to 6, Embedding dimension\n= [64, 128, 256]. On the datasets with >2M edges,\ndue to scalability issue, we searched for kfrom 1 to 2\nand Embedding dimension = [64, 128].\n5.NetMF [49] : NetMF is a matrix factorization based\nnetwork embedding method. NetMF presents theo-\nretical proofs for their claim that Skip-gram models\nwith negative sampling are implicitly approximating\nand factorizing appropriate matrices constructed with\nthe help of graph Laplacians. The objective matrix\nbased for NetMF on small context window T is given\nby (see Table 1 for notation):\nlog \nvol(G) \n1\nTTX\nr=1\u0000\nD\u00001A\u0001r!\nD\u00001!\n\u0000logb(3)\nwhere vol(G) refers to sum of all edge weights and b\ncorresponds to number of negative samples in skip-\ngram model. NetMF factorizes the above closed form\nDeepWalk matrix with SVD in order to generate node\nembedding and provides two algorithms for small con-\ntext window and large context window.\nReproducibility notes : We search for following hy-\nperparameters: T= [1, 10], Negative samples \u0015= [1,\n2, 3], Rank Hfor large context window = [128, 256,\n512], Embedding dimension = [64, 128, 256].\n\n6.M-NMF [59] : M-NMF is a matrix factorization based\nnetwork embedding method which generates node em-\nbeddings that preserves the microscopic information in\nform of \frst-order and second-order proximities among\nnodes and the generated embeddings also preserve meso-\nscopic information in form of community structure.\nThe objective function for M-NMF is given as\nO= min\nU;V C;H\u00150kS\u0000VUTk2+\u000b\u0003kH\u0000UCTk2\nF\n\u0000\ftr(HTBH) +\u0010kHHT\u0000Ik2\nF(4)\nwhereHis the binary community membership matrix,\nCis the latent representations of communities and B\nis the modularity matrix obtained from the adjacency\nmatrix,A(see Table 1 for rest of the notations). Over-\nall, M-NMF discovers communities through modular-\nity constraints. The node embeddings generated with\nthe help of microscopic information and community\nembeddings are then, jointly optimized by assuming\nconsensus relationship between both node and com-\nmunity embeddings.\nReproducibility notes : We search for following hy-\nperparameters: \u000b= [0.1, 1.0, 10.0], \f= [0.1, 1.0, 10.0],\nEmbedding dimension = [64, 128, 256].\n7.HOPE [44] : HOPE is a matrix factorization based\nnetwork embedding method which generates node em-\nbeddings that preserve asymmetric transitivity of nodes\nin directed graphs. If there exists a directed edge from\nnodeutowandwtov, then { due to asymmetric\ntransitivity property { an edge from utovis more\nlikely to form than edge from vtou. The objective\nfunction of HOPE is given as follows\nminkS\u0000Us(Ut)Tk2\n2 (5)\nwhereUsandUtare the source and target embed-\ndings. In order to preserve asymmetric transitivity of\nnodes, the proximity matrix Sis constructed using a\nsimilarity metric which respects the directionality of\nedges. The node embeddings are generated by factor-\nizing the proximity matrix with generalized SVD [46].\nReproducibility notes : We search for following hy-\nperparameters: The decay parameter \f= 0.5/\u000b, where\n\u000bis spectral radius of the graph. Embedding dimen-\nsion = [64, 128, 256].\n8.LINE [54] : LINE is an optimization-based network\nembedding method which optimizes an objective func-\ntion that preserves both \frst and second order prox-\nimity among nodes in the embedding space. The ob-\njective function for \frst-order proximity is given as:\nO1=\u0000X\n(u;v)2EAu;vlog\u001b (\b(u):\b(v)) (6)\nThe objective function to preserve the second order\nproximity is given as:\nO2=\u0000X\n(u;v)2EAu;vlogexp(\b(u): (v))P\nv02V;v06=vexp(\b(u): (v0))\n(7)\nwhere (u) represents context embedding of node u\n(see Table 1 for rest of the notations). The \frst-orderproximity corresponds to local proximity between nodes\nbased on the presence of edges in the graph while the\nsecond-order proximity corresponds to global proxim-\nity between nodes based on shared neighborhoods of\nthose nodes in the graph. Since the objective function\nis expensive to compute for large graphs, it is approx-\nimated by negative sampling [39].\nReproducibility notes : We search for following hy-\nperparameters: Number of samples = 10 billion, Em-\nbedding dimension 2[64, 128, 256]. In the case of\ndirected graphs, as suggested by the authors of LINE,\nwe evaluate only second-order proximity.\n9.Verse [55] : Verse is an optimization-based network\nembedding method which optimizes an objective func-\ntion that minimizes the Kullback-Leibler (KL) diver-\ngence from the given similarity distribution in graph\nspace to similarity distribution in embedding space\n(E). The objective function is given as follows:\nX\nv2VKL(simG(v;:)ksimE(v;:)) (8)\nThe similarity distribution in graph space could be\nconstructed with help of Personalized PageRank[45],\nSimRank[27], or Adjacency matrix[55]. Since the ob-\njective function is expensive to compute for large graphs,\nit is approximated by Noise Constrastive Estimation\n[23].\nReproducibility notes : We search for following hy-\nperparameters: PageRank damping factor \u000b= [0.7,\n0.85, 0.9], Negative samples = [3, 10], Embedding di-\nmension = [64, 128, 256].\n10.SDNE [58] : SDNE is a deep autoencoder based net-\nwork embedding method which optimizes an objective\nfunction that preserves both \frst and second order\nproximity among nodes in the embedding space. The\nobjective function of SDNE is given below\nLjoint =\u000bL1st+L2nd+\u0017Lreg (9)\nwhereL1standL2ndare loss functions to preserve\n\frst-order and second-order proximities respectively,\nwhileLregis the regularizer term. The authors pro-\npose a semi-supervised deep model to minimize the\nmentioned objective function. The deep model con-\nsists of two components: supervised and unsupervised.\nThe supervised component attempts to preserve the\n\frst-order proximity while the unsupervised compo-\nnent attempts to preserve the second-order proximity\nby minimizing reconstruction loss of nodes.\nReproducibility notes : We search for following hy-\nperparameters: \u000b= [1e-05, 0.2], Penalty coe\u000ecient \f\n= [5, 10], Embedding dimension = [64, 128, 256].\n11.VAG [28] : VAG is a graph autoencoder based net-\nwork embedding method which minimizes the recon-\nstruction loss of the adjacency matrix. The reconstruc-\ntion matrix is generated as ^A=\u001b(ZZT) whereZis\nnode embeddings generated with Graph Convolutional\nNetworks (GCN) [29] as Z=GCN (X;A) withXas\nnode features (see Table 1 for additional notation). In\nthe case of unattributed graphs, the node feature ma-\ntrix is the identity matrix.\n\nReproducibility notes : We search for following hy-\nperparameters: Epochs = [50, 100], Embedding di-\nmension = [64, 128, 256].\n12.Watch Your Step [2] : Watch Your Step (WYS)\naddresses the sensitivity issue of hyper-parameters in\nthe random walk based embedding methods. WYS\nsolves the sensitivity issue with the attention mecha-\nnism on the expected random walk matrix. The at-\ntention mechanism guides the random walk to focus\non short or long term dependencies pertinent to the\ninput graph. The objective function of WYS is given\nas\nmin\nL;R;q\fkqk2\n2\u0000kE[D;q]\u000elog(\u001b(L\u0003RT))\n\u0000 1[A= 0]\u000elog(1\u0000\u001b(L\u0003RT))k1(10)\nwhereqis attention parameter vector, LandRare\nnode embeddings, E[D;q] is expectation on the ran-\ndom walk matrix (see Table 1 for rest of the notations).\nReproducibility notes : We search for following hy-\nperparameters: Learning rate = [0.05, 0.1, 0.2, 0.5,\n1.0], Number of Hops = 5, Embedding dimension =\n[64, 128, 256].\n4. DATASETS\nWe select datasets from multiple domains, Table 2 de-\nscribes empirical properties of datasets. The selected datasets\nsupport both multi-label and multi-class classi\fcation. Di-\nrected as well as undirected datasets were selected in order\nto evaluate the embeddings methods on the link-prediction\ntask e\u000eciently. Further, datasets with and without edge\nweights are also included, thereby, providing us with a com-\nprehensive set of possibilities to evaluate the methods. We\nsummarize the datasets below:\n\u000fWeb: The WebKB datasets2[14] consist of classi\fed\nwebpages (nodes) and hyperlinks between them (edges).\nHere, labels are the categories of the webpages.\n\u000fMedical: The PPI dataset [10] represents a subgraph\nof protein interactions in humans. Labels represent\nbiological states corresponding to hallmark gene sets.\n\u000fNatural Language: The Wikipedia dataset [38] is a\ndump of Wikipedia with nodes as words, edges corre-\nsponding to the co-occurrence matrix and labels cor-\nresponding to Part-of-Speech (POS) tags.\n\u000fSocial: The Blogcatalog dataset and Flickr dataset [60]\nrepresent social networks. Blogcatalog and Flickr both\nrepresent bloggers and their friendships. YouTube dataset\n[21] represents users and their friendships. Labels for\nBlogcatalog, Flickr, and YouTube correspond to the\ngroups to which each user belongs. The Epinions dataset\n[51] represents user annotated trust relationships, where\nusers annotate which other users they trust. These are\nused to determine the reviews shown to a user.\n\u000fCitation: The DBLP, CoCit, and Pubmed datasets\nrepresent citation networks. DBLP (Co-Author) rep-\nresents a subset of papers in DBLP3from closely re-\nlated \felds. CoCit (Microsoft) [1] corresponds to a co-\ncitation subgraph of the Microsoft Academic Graph.\n2http://linqs.cs.umd.edu/projects/projects/lbc/\n3https://dblp.uni-trier.de/Dataset #Nodes #Edges #Labels (C/L) D W\nWebKB (Texas) 186 464 4 C F T\nWebKB (Cornell) 195 478 5 C F T\nWebKB (Washington) 230 596 5 C F T\nWebKB (Wisconsin) 265 724 5 C F T\nPPI 3,890 38,739 50 L F F\nWikipedia 4,777 92,517 40 L F T\nBlogcatalog 10,312 333,983 39 L F F\nDBLP (Co-Author) 18,721 122,245 3 C F T\nCoCit (Microsoft) 44,034 195,361 15 C F F\nWiki-Vote 7,115 103,689 - - T F\nPubmed 19,717 44,338 3 C T F\np2p-Gnutella 62,586 147,892 - - T F\nFlickr 80,513 5,899,882 195 L F F\nEpinions 75,879 508,837 - - T F\nYouTube 1,134,890 2,987,624 47 C F F\nTable 2: Dataset Properties. \\C\"/\\L\" refers to Multi Class\nor vs Multi Label(\\L\"). \\D\" refers to Directed and \\W\"\nrefers to weighted.\nFinally, Pubmed corresponds to a subset of diabetes-\nrelated publications on Pubmed4. Labels in DBLP\ncorrespond to the sub-\feld of the paper. In CoCit,\nthey correspond to the conference of the paper, and in\nPubmed correspond to the types of diabetes.\n\u000fDigital: The p2p-Gnutella dataset [52] represents con-\nnections between hosts on a peer-to-peer \fle sharing\nnetwork. This dataset has no node labels.\n\u000fVoting: The Wiki-Vote dataset [32] is constructed from\nvoting data in multiple elections for Wikipedia admin-\nistratorship. Users are nodes, and (directed) edge ( i,\nj) represents a vote from user ito userj. This dataset\nalso has no node labels.\n5. EXPERIMENTAL SETUP\nIn this section, we elaborate on the experimental setup\nfor link prediction and node classi\fcation tasks employed to\nevaluate the quality of embeddings generated by di\u000berent\nmethods. We present two heuristics baselines for both the\ntasks and de\fne the metrics used for comparing the embed-\nding methods.\n5.1 Link Prediction\nPrediction of ties is an essential task in multiple domains\nwhere the relational information is costlier to obtain such\nas drug-target interactions [16], protein-protein interactions\n[4], or when the environment is partially observable. The\nproblem of prediction of tie/link between two nodes iandj\nis often evaluated in one of two ways. The \frst is to treat the\nproblem as a binary classi\fcation problem. The second is\nto use the dot product on the embedding space as a scoring\nfunction to evaluate the strength of the tie.\nThe edge features for binary classi\fcation consists of node\nembeddings of nodes iandj, where two node embeddings\nare aggregated with a binary function. In our study, we ex-\nperimented with three binary functions on node embeddings:\nConcatenation, Hadamard, and L2 distance. We used logis-\ntic regression as our base classi\fer for the prediction of the\nlink. The parameters of the logistic regression are tuned us-\ning GridSearchCV with 5-fold cross validation with scoring\nmetric as `roc auc'. We evaluate the link prediction per-\nformance with metrics: Area Under the Receiver Operating\n4https://www.ncbi.nlm.nih.gov/pubmed/\n\nCharacteristics (AUROC) and Area Under Precision-Recall\ncurve (AUPR). An alternative evaluation strategy is to pre-\ndict the presence of link ( i;j) based on dot product value of\nnode embeddings of nodes iandj. We study the impact of\nboth the evaluation strategies in Section 6.1.\nConstruction of the train and test sets : The method\nof construction of train and test sets for link prediction task\nis crucial for comparison of embedding methods. The train\nand test split consists of 80% and 20% of the edges respec-\ntively and are constructed in the following order:\n1. Self-loops are removed.\n2. We randomly select 20% of all edges as positive test\nedges and add them in the test set.\n3. Positive test edges are removed from the graph. We\n\fnd the largest weakly connected component formed\nwith the non-removed edges. The edges of the con-\nnected component form positive train edges.\n4. We sample negative edges from the largest weakly con-\nnected component and add the sampled negative edges\nto both the training set and test set. The number of\nnegative edges is equal to the number of positive edges\nin both training and test sets.\n5. For directed graphs, we form \\directed negative test\nedges\" which satisfy the following constraint: ( j;i)62\nEbut (i;j)2EwhereErefers to edges in the largest\nweakly connected component. We add the directed\nnegative test edges ( j;i) edges to our test set. The\nnumber of \\directed negative test edges\" is around\n10% of negative test edges in the test set.\n6. Nodes present in the test set, but not present in the\ntraining set, are deleted from the test set.\nIn case of large datasets ( >5M edges), we reduce our train-\ning set. We consider 10% of both randomly selected positive\nand negative train edges for learning the binary classi\fer.\nThe learned model is evaluated on the test set. The above\nsteps are repeated for 5 folds of a train:test splits of 80:20%\nand we report the average AUROC and AUPR scores across\n5 folds.\n5.2 Node classiﬁcation\nIn network embedding literature, node classi\fcation is the\nmost popular way of comparing the quality of embeddings\ngenerated by di\u000berent embedding methods. The generated\nnode embeddings are treated as node features, and node\nlabels are treated as ground truth. The classi\fcation task\nperformed in our experiments is either Multi-label or Multi-\nclass classi\fcation. The details on the classi\fcation task\nperformed on each dataset are provided in Table 2. We select\nLogistic Regression as our classi\fer. The hyperparameters\nof the logistic regression are tuned using GridSearchCV with\n5-fold cross validation with scoring metric as `f1 micro'. We\nsplit the dataset with 50:50 train-test splits. The learned\nmodel is evaluated on the test set, and we report the results\naveraged over 10 shu\u000fes of train-test sets. The model does\nnot have access to test instances while training.\nWe note that a majority of the e\u000borts in the literature\ndo not tune the hyper-parameters of Logistic Regression.\nDefault hyper-parameters are not always the best hyper-\nparameters for Logistic Regression. For instance, with de-\nfault hyper-parameters of LR classi\fer, the Macro-f1 per-\nformance of Laplacian eigenmaps on Blogcatalog dataset\nis 3.9% for the train-test split of 50:50. However, tuningthe hyper-parameters results in signi\fcant improvement of\nMacro-f1 score to 29.2%.\nThe choice of a \\linear\" classi\fer to evaluate the quality of\nembeddings is not a hard constraint in the node classi\fcation\ntask. In this work, we also test the idea of leveraging a\n\\non-linear\" classi\fer for the node classi\fcation task and use\nEigenPro [37] classi\fer for the same task. On large datasets,\nEigenPro provides a signi\fcant performance boost over the\nstate-of-the-art kernel methods with faster convergence rates\n[37]. In the experiments, we see a bene\ft to this approach,\nup to 15% improvement in Micro-f1 scores with non-linear\nclassi\fer compared to the linear classi\fer.\n5.3 Heuristics\nNext, we present heuristics baseline for both link predic-\ntion and node classi\fcation tasks. The purpose of de\fning\nheuristics baseline is to assess the di\u000eculty of performing\na particular task on a particular dataset and also to com-\npare the performance of sophisticated network embedding\nmethods over simple heuristics.\n5.3.1 Link Prediction Heuristics\nIn the link prediction literature, there exist multiple sim-\nilarity based metrics [36] which can predict a score for link\nformation between two nodes. Examples of such metrics\ninclude Jaccard Index [26, 57], Adamic Adar [3]. These\nsimilarity-based metrics often base their predictions on the\nneighborhood overlap between the nodes. We combine the\nsimilarity-based metrics to form a curated feature vector\nof an edge [53]. The binary classi\fer in the link predic-\ntion task is then trained on the generated edge embeddings.\nOur selected similarity-based metrics are Common Neigh-\nbors (CN), Adamic Adar (AA) [3], Jaccard Index (JA) [26],\nResource Allocation Index (RA) [64] and Preferential At-\ntachment Index (PA) [5]. The similarity-based metrics CN,\nJA, and PA captures \frst-order proximity between nodes,\nwhile the metrics AA and RA capture second-order proxim-\nity between nodes. We found this heuristic based model to\nbe highly competitive as compared to the embedding meth-\nods on multiple datasets.\n5.3.2 Node Classiﬁcation Heuristics\nNodes in the graph can be characterized/represented by\ntheir properties. We combine the node properties to form a\nfeature vector/embedding of a node. The classi\fer in node\nclassi\fcation task is then trained on the generated node em-\nbeddings. The node properties capture information such as\nnodes' neighborhood, in\ruence on other nodes, structural\nproperties. We select following node properties : Degree,\nPageRank [45], Clustering Coe\u000ecient, Hub and Author-\nity scores [30], Average Neighbor Degree, and Eccentricity\n[42]. We treat the graph as undirected while computing the\nnode properties. As the magnitude of each node property\nvaries with another, we perform column-wise normalization\nwith RobustScaler available from Scikit-learn. We will show\nin the experiments Section 6.2 that the node classi\fcation\nheuristics baseline is competitive with most of the embed-\nding methods on datasets with fewer labels.\n5.4 Comparison Measures\nIn this section, we present two measures for comparing\nthe performance of embedding methods in the downstream\nmachine learning task.\n\nMean Rank : We compute the rank of all the embedding\nmethods on each dataset based on selected performance met-\nric and report the average rank of an embedding method\nacross all datasets as the Mean Rank of the embedding\nmethod. Let Re;dbe the rank of embedding method eon\ndatasetdwithDbeing the set of datasets then mean rank\nof embedding method MReis given by\nMRe=P\nd2DRe;d\njDj(11)\nMean Penalty [56] : We de\fne penalty of an embed-\nding method eon a dataset das di\u000berence between best\nscore achieved by any embedding method on dataset dand\nthe score achieved by embedding method eon same dataset\nd. Score is the selected performance metric for a particular\ndownstream ML task. Let Ebe the set of embedding meth-\nods andSe;dbe the score achieved by embedding method e\non same dataset d, then the Mean Penalty MPeis given by\nMPe=P\nd2Dmax(fSe0;dg)\u0000Se;d\njDj;e02E (12)\nFor a model, lower values for Mean Rank andMean Penalty\nsuggest better performance. We compare the embedding\nmethods with Mean Rank , and Mean Penalty measures on\nthe datasets where all the embedding methods complete ex-\necution. Though the measures do not consider the dataset\nsize or missing values, the measures are simple and intuitive.\n6. EXPERIMENTAL RESULTS\nIn this section, we report the performance of network em-\nbedding methods on link prediction task and node classi-\n\fcation task. We tune both the parameters of embedding\nmethods and the parameters of classi\fers in link prediction\nand node classi\fcation task. Whenever possible, we rely on\nthe authors' code implementation of the embedding method.\nAll the methods which do not complete execution on large\ndatasets are executed on a modern machine with 500 GB\nRAM and 28 cores. All the evaluation scripts are executed\nin the same virtual python environment.5\n6.1 Link Prediction\nThe link prediction performance of 12 embedding methods\nmeasured in terms of AUROC and AUPR on 15 datasets is\nshown in Figure 1 and Figure 2. The Overall (or aggregate)\nperformance of an embedding method on all the datasets is\nalso shown at the end of the horizontal bar of each method\nin Figure 1 and Figure 2. We represent Overall score as the\nsum of scores (AUROC or AUPR) of the method on all the\ndatasets. The Mean Rank and Mean Penalty of embedding\nmethods { on datasets for which all methods run to comple-\ntion on our system { is shown in Figure 3. We also provide\nthe tabulated results in Tables 3 and 4. As mentioned in\nSection 3, we tune the hyper-parameters of each embedding\nmethod and report the best average AUROC scores and av-\nerage AUPR scores across 5 folds. In the case of WebKB\ndatasets, we evaluate the methods on embedding dimensions\n64, 128. We perform the link prediction task with both nor-\nmalized and unnormalized embeddings and report the best\nperformance.\n5The evaluation scripts and datasets are available at\nhttps://github.com/PriyeshV/NRL Benchmark.We make the following observations:\n\u000fE\u000bectiveness of MNMF for Link Prediction : We\nobserve that MNMF achieves the highest overall link pre-\ndiction performance in terms of best average AUROC and\nAUPR scores as compared to other methods. The compet-\nitive performance of MNMF on link prediction could be\ncredited to the community information imbibed into the\nnode embeddings generation. The Mean Rank and Mean\nPenalty is lowest for MNMF which also suggest MNMF\nas a competitive baseline for Link Prediction. MNMF\nachieves the \frst rank for 7 out of 15 datasets. The small\nvalue of Mean Penalty suggests that even when MNMF\nis not the top-ranked method for a particular dataset,\nMNMF's performance is closest to that of the top-ranked\nmethod on that dataset. However, MNMF does not com-\npletely outperform other methods on all the datasets. For\ninstance, on the Wiki-Vote and Pubmed dataset, WYS\nachieves the best average AUROC scores while on Mi-\ncrosoft dataset, GraRep achieves the best average AU-\nROC score. In Figure 1b and Figure 2b, we see that\namong the more scalable methods, LINE achieves the\nhighest overall link prediction performance followed by\nDeepWalk in terms of both AUROC and AUPR scores.\nNote that MNMF did not scale for the datasets with\n\u00155M edges on a modern machine with 500 GB RAM and\n28 cores. However, the scalability issue of non-negative\nmatrix factorization based methods can be addressed by\nadopting modern ideas [41, 33] (outside the scope of this\nstudy).\n\u000fPerformance of Heuristic Baseline: We observe that\nthe Link Prediction Heuristics baseline { described in sec-\ntion 5.3.1 { is both e\u000ecient and e\u000bective. We see that\nLink Prediction Heuristics baselines' overall performance\nis better than that of Laplacian Eigenmaps and SDNE and\ncompetitive to that of Node2vec, HOPE, Verse, LINE.\nThe Mean Penalty of Link Prediction Heuristics is also\nclose to other embedding methods. On the largest dataset\nYouTube, Link Prediction Heuristics achieve an AUROC\nof 96.2% which is close to the best performing Verse with\nAUROC of 97.6%. As compared to the most competi-\ntive baseline MNMF, the heuristics baseline outperforms\nMNMF on Wikipedia, Blogcatalog datasets. We also ob-\nserve that the heuristics baseline performance is competi-\ntive against several methods on the directed datasets too\neven though the chosen similarity-based metrics in heuris-\ntics baseline treat the underlying graph as undirected.\nFeature study on the Heuristic Baseline: We study the\nimportance of the individual feature in the heuristics by\nanalyzing the impact of the feature removal on link pre-\ndiction. The results are reported in Figure 4. The blue\nline on the top of the columns in Figure 4 corresponds to\nAUROC scores achieved with the proposed link predic-\ntion heuristic. In the feature study on the link prediction\nheuristics, we see that the removal of preferential attach-\nment (PA) feature results in consistent drop in AUROC\nscores. We \fnd that the removal of PA feature results in\nstatistically signi\fcant drop at signi\fcance level of 0.05\nwith paired t-test. The removal of rest of the features\nboth in link prediction heuristics did not result in signi\f-\ncant drop in the downstream performance.\n\n77.7\n73.1\n79.7\n83.0\n81.7\n78.2\n78.7\n78.7\n82.4\n80.9\n96.0\n78.6\n83.081.5\n77.3\n79.2\n82.0\n87.0\n77.5\n84.4\n79.9\n80.2\n81.2\n96.7\n74.8\n84.475.3\n70.1\n75.3\n75.0\n82.2\n72.9\n78.1\n72.8\n76.7\n75.5\n97.5\n73.9\n79.279.4\n71.4\n80.7\n78.0\n88.2\n72.5\n84.3\n75.5\n76.9\n82.3\n98.9\n73.9\n84.590.9\n78.2\n89.1\n88.3\n89.6\n87.8\n90.0\n88.4\n89.3\n87.3\n96.9\n87.4\n91.591.6\n77.9\n90.9\n90.9\n91.3\n91.2\n92.3\n90.4\n50.0\n91.4\n88.4\n89.5\n92.391.5\n83.5\n97.4\n97.6\n94.9\n96.6\n88.4\n97.8\n96.6\n95.5\n92.2\n94.3\n98.295.2\n77.4\n94.3\n95.0\n97.3\n95.2\n96.2\n95.3\n95.6\n95.1\n94.0\n94.8\n96.095.6\n93.3\n96.0\n95.4\n97.9\n94.3\n97.1\n89.6\n50.0\n95.9\n99.4\n94.1\n96.887.7\n89.6\n89.1\n89.3\n96.6\n92.7\n77.7\n90.1\n88.9\n89.8\n94.3\n93.6\n97.0866.3\n791.8\n871.6\n874.6\n906.5\n859.0\n867.2\n858.5\n786.6\n874.7\n954.2\n855.0\n903.0Heuristics\nLapEig\nDeepWalk\nNode2Vec\nVERSE\nLINE\nGraRep\nHOPE\nSDNE\nNetMF\nMNMF\nVAG\nWYS\n0.0 250.0 500.0 750.0 1000.0W-Texas\nW-Cornell\nW-Washington\nW-Wisconsin\nPPI\nWikipedia\nWiki-Vote\nBlogCatalog\nDBLP\nPubmed(a) Smaller datasets: All methods complete execution.\n89.5\n95.6\n97.6\n97.3\n83.7\n97.2\n97.9\n94.5\n91.9\n96.9\n96.6\n96.2\n0.083.8\n69.9\n88.2\n88.3\n77.6\n91.2\n71.8\n88.6\n83.9\n87.5\n92.3\n0.0\n0.092.4\n93.0\n95.8\n94.7\n72.6\n95.2\n95.5\n96.5\n93.0\n97.2\n0.0\n0.0\n0.092.2\n90.9\n93.3\n93.4\n91.9\n91.6\n93.7\n92.7\n92.7\n92.8\n0.0\n0.0\n0.096.2\n96.0\n93.6\n91.4\n97.6\n96.5\n91.4\n92.4\n0.0\n0.0\n0.0\n0.0\n0.0454.1\n445.3\n468.5\n465.2\n423.5\n471.7\n450.3\n464.6\n361.5\n374.4\n188.8\n96.2\n0.0Heuristics\nLapEig\nDeepWalk\nNode2Vec\nVERSE\nLINE\nGraRep\nHOPE\nSDNE\nNetMF\nMNMF\nVAG\nWYS\n0.0 100.0 200.0 300.0 400.0 500.0Microsoft\nP2P\nFlickr\nEpinions\nYoutube (b) Larger datasets: Not all methods complete execution.\nFigure 1: Link Prediction performance measured with AUROC . Figure 1a shows the best average AUROC of all methods\non datasets where all methods' results are available. Not all methods are scalable on the datasets shown in Figure 1b. If a\nmethod runs out of memory or faults on a particular dataset, we represent its performance as 0.0 in the above plot.\n81.8\n78.0\n81.9\n85.0\n85.0\n82.2\n82.1\n82.9\n85.5\n83.6\n96.0\n80.6\n85.281.9\n78.9\n79.8\n81.0\n87.8\n76.7\n86.2\n79.8\n79.5\n82.0\n96.7\n75.6\n86.680.3\n75.0\n76.5\n78.0\n86.5\n75.5\n82.9\n77.2\n81.5\n80.4\n97.7\n78.7\n83.482.3\n74.8\n81.6\n79.0\n90.7\n76.4\n87.4\n78.8\n80.7\n85.2\n98.5\n78.1\n86.791.4\n80.7\n90.4\n89.5\n90.7\n88.1\n90.8\n89.2\n90.2\n87.9\n96.5\n88.1\n92.293.0\n76.0\n92.5\n92.3\n92.8\n92.8\n93.1\n91.8\n75.0\n92.8\n89.9\n91.4\n93.587.9\n82.1\n96.9\n97.2\n94.8\n95.3\n84.0\n96.8\n96.3\n93.8\n87.6\n94.3\n97.495.1\n77.5\n94.3\n94.8\n97.9\n95.1\n96.0\n95.0\n95.5\n94.8\n93.6\n94.6\n96.196.7\n93.8\n96.8\n96.1\n98.2\n95.6\n97.4\n90.9\n75.0\n96.7\n99.2\n95.2\n97.385.0\n85.0\n81.5\n82.3\n96.8\n90.3\n74.1\n91.4\n90.5\n86.1\n90.3\n95.2\n96.9875.4\n801.6\n872.1\n875.3\n921.3\n867.9\n874.1\n873.7\n849.7\n883.1\n946.1\n871.9\n915.3Heuristics\nLapEig\nDeepWalk\nNode2Vec\nVERSE\nLINE\nGraRep\nHOPE\nSDNE\nNetMF\nMNMF\nVAG\nWYS\n0.0 250.0 500.0 750.0 1000.0W-Texas\nW-Cornell\nW-Washington\nW-Wisconsin\nPPI\nWikipedia\nWiki-Vote\nBlogCatalog\nDBLP\nPubmed\n(a) Smaller datasets: All methods complete execution.\n91.9\n95.5\n97.9\n97.5\n76.4\n97.7\n97.9\n95.3\n93.5\n97.1\n95.2\n96.4\n0.079.3\n68.1\n84.3\n84.5\n68.3\n88.6\n71.3\n85.8\n80.8\n84.0\n89.6\n0.0\n0.092.5\n95.0\n96.1\n95.0\n70.9\n95.5\n95.7\n96.7\n94.0\n97.6\n0.0\n0.0\n0.089.2\n89.5\n91.7\n91.9\n88.6\n88.8\n93.0\n91.5\n91.6\n91.8\n0.0\n0.0\n0.096.7\n96.7\n95.0\n93.0\n98.2\n97.0\n92.2\n94.0\n0.0\n0.0\n0.0\n0.0\n0.0449.6\n444.7\n465.0\n461.9\n402.5\n467.6\n450.1\n463.4\n359.9\n370.5\n184.7\n96.4\n0.0Heuristics\nLapEig\nDeepWalk\nNode2Vec\nVERSE\nLINE\nGraRep\nHOPE\nSDNE\nNetMF\nMNMF\nVAG\nWYS\n0.0 100.0 200.0 300.0 400.0 500.0Microsoft\nP2P\nFlickr\nEpinions\nYoutube (b) Larger datasets: Not all methods complete execution.\nFigure 2: Link Prediction performance measured with AUPR . Figure 1a shows the best average AUPR of all methods on\ndatasets where all methods' results are available. Not all methods are complete execution on the datasets shown in Figure\n1b. If a method runs out of memory or faults on a particular dataset, we represent its performance as 0.0 in the above plot.\n7.50\n11.00\n6.43\n6.14\n4.57\n7.36\n5.43\n7.50\n7.93\n6.64\n5.07\n10.07\n5.368.84\n14.83\n7.66\n7.62\n6.73\n8.29\n9.26\n8.94\n15.31\n7.85\n1.44\n10.62\n6.71Heuristics\nLapEig\nDeepWalk\nNode2Vec\nVERSE\nLINE\nGraRep\nHOPE\nSDNE\nNetMF\nMNMF\nVAG\nWYS\n0.00 10.00 20.00 30.00Mean Rank Mean Penalty\nFigure 3: Mean Rank and Mean Penalty for link prediction\nwith selected performance metric as AUROC.\n\u000fImpact of Evaluation Strategy: As described in sec-\ntion 5.1, the presence of a link between two nodes can\nbe predicted with either the Logistic Regression classi\fer\n(treating the embeddings as features) or the dot product\nbetween the node embeddings. We compare the perfor-\nmance of both evaluation strategies on each embedding\nmethod over all datasets using the di\u000berences in the av-\nerage AUROC scores. A positive di\u000berence implies link\nAUROC\n020406080100\nW-Texas W-Cornell \nW-Washington W-WisconsinPPI\nWikipedia Wiki-Vote\nBlogCatalogDBLP\nPubmed MicrosoftP2PFlickr\nEpinions YoutubeNo Adamic Adar No Common Neighbors No Jaccard Index\nNo Preferential attachment Index No Resource Allocation Index All featuresFigure 4: Feature study on the link prediction for link pre-\ndiction heuristics.\nprediction performance with classi\fer is better than that\nof the dot product. The results are presented as box-plot\nin Figure 5. Paired t-test suggests the positive di\u000berence\nis statistically signi\fcant for all methods, except for Verse\nand WYS, with a signi\fcance level of 0.05. Hence, the use\nof classi\fer over dot product provides signi\fcant predic-\n\nWYSVAGMNMFNetMFSDNEHOPEGraRepLINEVERSEN2VDWLapEig\n0 10 20 30 40Embedding methodsFigure 5: The box-plot represents distribution of the dif-\nferences between computed AUROC score with classi\fer\nand computed AUROC score with dot product on all the\ndatasets. The di\u000berence is statistically signi\fcant (paired\nt-test) for all methods, except for Verse and WYS, with sig-\nni\fcance level 0.05.\nLapEig\nDeepWalkNode2VecVERSELINE\nGraRepHOPESDNENetMF MNMFVAGWYSW-Texas\nW-Cornell\nW-Washington\nW-Wisconsin\nPPI\nWikipedia\nWiki-Vote\nBlogCatalog\nDBLP\nPubmed\nMicrosoft\nP2P\nFlickr\nEpinions\nYoutubeN/AHadmardL2Concat\nFigure 6: The heatmap shows which binary function {\nHadamard, Concatenation and L2 { resulted in best aver-\nage AUROC score for an embedding method on a particular\ndataset.\ntive performance gain on the task of link prediction. We\nalso investigate the changes in the ranking of embedding\nmethods based on overall average AUROC scores when\npredictions are performed with classi\fer rather than dot\nproduct. The methods were ranked based on overall aver-\nage AUROC scores and we considered only those datasets\non which all methods complete execution. We observed\nthat the rank of NetMF in the ranking generated with\ndot product was 10 while its rank in the ranking gen-\nerated with classi\fer improved to 3. Since the best link\nprediction performance for the majority of the embedding\nmethods was achieved with classi\fer, we believe the supe-\nriority of the embedding methods based on link prediction\ntask should be asserted by leveraging the classi\fer.\nAs mentioned in section 5.1, we lever binary functions:\nHadamard, Concatenation, and L2 to generate the edge\nembedding. In \fgure 6, we present which binary function\nachieved the best average AUROC score for an embed-\nding method on a particular dataset. We see that the\nbinary function Hadamard resulted in achieving a maxi-\nmum number of best average AUROC scores. However,\nthere is no single winner in terms of choice of binary func-\ntions.\nWYSMNMFNetMFHOPEGraRepLINEN2VDW\n−5 0 5 10Embedding methodsFigure 7: The box-plot represents the distribution of the\ndi\u000berences between computed AUROC score with node +\ncontext embeddings and computed AUROC score with only\nnode embeddings on directed datasets. The di\u000berence is sta-\ntistically signi\fcant (paired t-test) for methods LINE, WYS,\nGraRep and MNMF with signi\fcance level 0.05.\n\u000fImpact of context embeddings: We study the im-\npact of context embeddings on directed datasets for the\nlink prediction task. We consider only those embedding\nmethods which generate both node and context embed-\ndings for this study. We compare the impact of using\nnode + context embeddings over using only node embed-\ndings with the help of di\u000berences in AUROC scores. The\nresults are detailed in Figure 7. A positive di\u000berence im-\nplies the use of context embeddings helps in link predic-\ntion. We see that levering node + context embeddings\nimprove the link prediction performance of LINE, HOPE,\nand WYS. For MNMF, use of context embeddings does\nnot improve the link prediction performance as in MNMF\nthe community information { crucial for link prediction {\nis incorporated in the node embeddings. In the case of\nGraRep, we \fnd that the node embeddings encapsulate\nhigh-order information and, hence, levering context does\nnot help improve the performance. We \fnd that the re-\nsults of DeepWalk and Node2Vec on directed datasets are\nsigni\fcantly lower, so in order to have a fair comparison\nwith other embedding methods, we treated the directed\ndatasets as undirected for DeepWalk and Node2Vec. The\nmedian of the box plot of DeepWalk and Node2Vec is close\nto zero due to this treatment.\n\u000fRobustness of embedding methods: In link predic-\ntion, we compute the average AUROC score and average\nAUROC standard error of an embedding method over 5\nfolds of a selected dataset. The computed average AU-\nROC standard error corresponds to the robustness of that\nembedding method on the selected dataset { as larger val-\nues of standard error corresponds to large variance in AU-\nROC scores across 5 folds. In Figure 8, we report the dis-\ntribution of average AUROC standard error of each em-\nbedding method over all datasets. We observed large vari-\nance in average AUROC standard error scores over We-\nbKB datasets and show the results for WebKB datasets\nin Figure 8a while we show the results for other datasets\nin Figure 8b. Interestingly, even on WebKB datasets, the\nvariance in average AUROC standard error scores is low\nfor MNMF method. From Figure 8b, we observe that the\n\nWYSVAGMNMFNetMFSDNEHOPEGraRepLINEVERSEN2VDWLapEig\n0 1 2 3 4 5Embedding methods(a) WebKB datasets\nWYSVAGMNMFNetMFSDNEHOPEGraRepLINEVERSEN2VDWLapEig\n0.0 0.5 1.0 1.5 2.0Embedding methods (b) All datasets except WebKB datasets\nFigure 8: The box-plot represents the distribution of the standard error of AUROC scores computed on all the datasets for\nthe link prediction task.\nWYSVAGMNMFNetMFSDNEHOPEGraRepLINEVERSEN2VDWLapEig\n−2.5 0.0 2.5 5.0Embedding methods\nFigure 9: The box-plot represents the distribution of the\ndi\u000berences in AUROC scores between 128 dimensional em-\nbedding and 64 dimensional embedding for each method on\nall datasets\nmedian of box-plots of majority of the methods is closer\nto zero.\n\u000fImpact of embedding dimension: We study the im-\npact of embedding size for all the embedding methods\non the link prediction. Speci\fcally, we compare the per-\nformances of 64 dimensional embedding with 128 dimen-\nsional embedding. The improvement { quanti\fed in terms\nof performance di\u000berence { obtained with 128 dimensional\nembedding over 64 dimensional embedding is reported in\nFigure 9. The box-plot represents the distribution of dif-\nferences in AUROC scores between 128 dimensional em-\nbedding and 64 dimensional embedding for each method\non all datasets. In link prediction, we observe a statis-\ntically signi\fcant improvement at signi\fcance level 0.05\nwith the 128 dimensional embedding for Laplacian Eigmaps,\nGraREP, HOPE, NetMF and MNMF methods.\n\u000fImpact of embedding normalization: We study the\nimpact of L2 normalization of the embeddings on the\nlink prediction performance. The comparison results are\nshown in Figure 10 where Figure 10a and Figure 10bshows the comparison results when link prediction is per-\nformed through classi\fer and dot-product, respectively.\nThe box plot represents the distribution of di\u000berences of\nAUROC between normalized and unnormalized embed-\ndings on link prediction task. The positive di\u000berence im-\nplies L2 normalization results in better downstream per-\nformance. When link prediction is performed through\nclassi\fer, the negative di\u000berence is statistically signi\f-\ncant for VERSE and GraREP at signi\fcance level of 0.05\nwith paired t-test. However, surprisingly the di\u000berence in\nperformance with respect to normalization of embeddings\nis not statistically signi\fcantly for rest of the methods\nfor link prediction. When link prediction is performed\nthrough dot-product, the normalization of embedding re-\nsults in statistically signi\fcant improvement for Node2vec\nand Verse, while not performing normalization of em-\nbedding results in statistically signi\fcant improvement\nHOPE, NetMF and WYS.\n6.2 Node Classiﬁcation\nThe node classi\fcation performance of 12 embedding meth-\nods measured in terms of Micro-f1 scores on 15 datasets with\ntrain-test split of 50:50 is reported in Figure 11 and Figure\n12. The overall performance of an embedding method on\nall the datasets is shown at the end of the horizontal bar\nof each method in Figure 11 and Figure 12 and represents\nthe sum of scores (Micro-f1) of the method on the datasets.\nThe Mean Rank and Mean Penalty of embedding methods\n{ on the datasets for which all methods run to completion\non our system { is shown in Figure 13. We also report\ntheMean Rank and Mean Penalty of embedding methods\n{ on datasets with few labels { in Figure 14. We tune the\nhyper-parameters of each embedding method { mentioned in\nsection 3 { and report the best Micro-f1 score. In the case\nof WebKB datasets, we evaluate the methods on generated\nembedding with dimensions 64, 128. We perform the node\nclassi\fcation with both normalized and unnormalized em-\nbeddings and report the best performance. We also provide\ntabulated results in Tables 5 and 6. We make the following\nobservations.\n\u000fE\u000bectiveness of NetMF for node classi\fcation: We\nobserve that NetMF achieves the highest overall node clas-\n\nWYSVAGMNMFNetMFSDNEHOPEGraRepLINEVERSEN2VDWLapEig\n−10 0 10Embedding methods(a) Prediction of link through classi\fer\nWYSVAGMNMFNetMFSDNEHOPEGraRepLINEVERSEN2VDWLapEig\n−20 0 20 40 60Embedding methods (b) Prediction of link through dot-product\nFigure 10: The box-plot represents the distribution of the di\u000berences in AUROC scores between L2 normalized embeddings\nand unnormalized embeddings\n61.83\n54.62\n55.05\n57.20\n54.52\n61.83\n56.13\n59.14\n58.00\n67.10\n57.96\n54.80\n60.6542.14\n30.92\n40.51\n34.29\n35.41\n44.08\n40.92\n41.94\n48.50\n48.06\n36.12\n40.30\n41.8465.04\n43.30\n56.00\n58.43\n51.48\n65.30\n46.70\n62.78\n60.50\n61.13\n60.43\n59.00\n65.3051.65\n41.50\n52.26\n45.56\n41.73\n52.86\n50.83\n51.50\n51.30\n56.69\n53.91\n48.10\n53.1637.35\n37.06\n35.86\n35.55\n37.21\n36.98\n35.69\n35.98\n37.40\n36.57\n36.19\n36.20\n0.0057.84\n81.67\n81.46\n81.10\n63.00\n64.36\n79.09\n74.72\n67.70\n79.96\n77.05\n63.50\n73.62315.85\n289.08\n321.14\n312.14\n283.34\n325.42\n309.35\n326.06\n323.40\n349.51\n321.67\n301.90\n294.56Heuristics\nLapEig\nDeepWalk\nNode2Vec\nVERSE\nLINE\nGraRep\nHOPE\nSDNE\nNetMF\nMNMF\nVAG\nWYS\n0.00 100.00 200.00 300.00W - Texas W - Cornell W - Washington W - Wisconsin DBLP (Co-Author) Pubmed\n(a) Datasets with few labels\n10.82\n22.34\n21.39\n21.02\n19.65\n19.88\n20.39\n18.79\n17.40\n21.31\n18.62\n19.20\n22.5641.91\n46.27\n49.96\n51.42\n43.77\n56.34\n58.82\n57.88\n52.40\n58.40\n48.10\n41.10\n44.3917.05\n42.13\n41.46\n41.73\n35.50\n38.56\n41.33\n34.43\n29.50\n41.66\n21.64\n17.10\n38.9325.04\n43.00\n46.29\n46.73\n45.99\n46.49\n46.58\n44.60\n38.10\n43.46\n44.67\n43.50\n0.0019.08\n33.97\n35.56\n35.14\n30.05\n33.38\n10.54\n28.68\n30.80\n34.17\n0.00\n0.00\n0.0024.39\n0.00\n40.73\n40.25\n38.54\n40.33\n38.00\n38.71\n0.00\n0.00138.30\n187.71\n235.39\n236.29\n213.51\n234.97\n215.66\n223.08\n168.20\n199.00\n133.02\n120.90\n105.88Heuristics\nLapEig\nDeepWalk\nNode2Vec\nVERSE\nLINE\nGraRep\nHOPE\nSDNE\nNetMF\nMNMF\nVAG\nWYS\n0.00 50.00 100.00 150.00 200.00 250.00PPI Wikipedia Blogcatalog CoCit (microsoft) Flickr Youtube (b) Datasets with more labels\nFigure 11: The node classi\fcation performance measured with Micro-f1 on train-test split of 50:50 with Logistic Regression.\nFor each method, the number at the end of bar represent the summation of the Micro-f1 values across the datasets.\nsi\fcation performance in terms of best Micro-f1 scores us-\ning both linear and non-linear classi\fers. From Figure 13,\nwe see that the Mean Rank and Mean Penalty is lowest\nfor NetMF which suggest NetMF as the strongest over-\nall method for node classi\fcation. NetMF achieves low\nMean Rank suggesting NetMF is among the top-ranked\nmethods on the evaluated datasets. The smallest value\nofMean Penalty suggests that even when NetMF is not\nthe top-ranked method for a particular dataset, NetMF's\nperformance is closest to that of the top-ranked method\nfor that dataset. However, it does not entirely outperform\nother methods on all the datasets. LINE, DeepWalk, and\nNode2Vec are also competitive baselines for the task of\nnode classi\fcation as their overall performance is clos-\nest to that of NetMF. The performance of GraRep on\ndatasets with more labels is comparable with other meth-\nods when we exclude the Flickr dataset. However, the re-\nported results for GraRep on the Flickr dataset are with\nembeddings of dimension 64. Embedding dimensions 128\nand 256 for GraRep resulted in memory errors on a mod-\nern machine with 500GB RAM and 28 cores. Note that\nNetMF did not scale for the YouTube dataset. While\nscalability is currently outside the scope of our study, the\nscalability of such methods is under active development\n(we refer the interested reader elsewhere [48, 33]).\u000fLaplacian Eigenmaps Performance: We observe that\nthe Laplacian Eigenmaps method achieves competitive\nMicro-f1 scores on several datasets. For instance, on Blog-\ncatalog dataset with 39 labels, Laplacian Eigenmaps method\nachieves the best Micro-f1 score of 42.1% while on the\nPubmed dataset, the Laplacian Eigenmaps methods out-\nperform all other embedding methods with 81.7% Micro-\nf1. With a non-linear classi\fer, Laplacian Eigenmaps\nachieves the second-best performance on the PPI dataset\nwith 23.8% Micro-f1. We observe from Figure 13 that\ntheMean Penalty of Laplacian Eigenmaps is also clos-\nest to other embedding methods, namely, Verse, MNMF,\nVAG. On the PPI and Flickr datasets, Laplacian Eigen-\nmaps baselines' Micro-f1 is close to the best Micro-f1. The\nobserved results for Laplacian Eigenmaps on evaluated\ndatasets are better than the reported results [20, 21] for\nboth node classi\fcation and link prediction tasks. This\nimprovement in the performance of Laplacian Eigenmaps\nis due to the hyper-parameter tuning of parameters of lo-\ngistic regression classi\fer.\n\u000fNode Classi\fcation Heuristic: We observe from Fig-\nure 11a and Figure 12a that the node classi\fcation heuris-\ntic baseline is competitive against other embedding meth-\nods on datasets with fewer labels (up to 5 labels) as its\n\n55.91\n51.51\n57.96\n56.77\n54.30\n68.06\n58.71\n56.45\n64.10\n63.01\n56.88\n57.70\n58.8242.86\n36.84\n43.67\n43.16\n40.82\n42.65\n36.43\n39.80\n46.10\n48.88\n42.55\n41.70\n43.5766.09\n46.70\n57.13\n61.22\n57.04\n61.30\n52.96\n59.57\n60.70\n59.13\n62.00\n56.50\n63.9151.88\n46.24\n56.69\n55.64\n53.53\n51.05\n52.86\n56.39\n53.80\n62.93\n58.20\n49.90\n56.0236.89\n37.35\n36.77\n37.04\n37.12\n37.04\n36.48\n36.54\n37.50\n37.09\n36.76\n36.10\n0.0057.12\n81.34\n83.03\n82.52\n78.00\n77.90\n80.77\n79.33\n71.50\n81.05\n78.59\n68.10\n79.70310.74\n299.97\n335.25\n336.36\n320.82\n338.02\n318.20\n328.07\n333.70\n352.09\n334.98\n310.00\n302.02Heuristics\nLapEig\nDeepWalk\nNode2Vec\nVERSE\nLINE\nGraRep\nHOPE\nSDNE\nNetMF\nMNMF\nVAG\nWYS\n0.00 100.00 200.00 300.00W - Texas W - Cornell W - Washington W - Wisconsin DBLP (Co-Author) Pubmed(a) Datasets with few labels\n10.73\n23.81\n23.65\n23.56\n21.15\n20.01\n24.27\n21.86\n20.50\n23.85\n22.45\n22.30\n22.7840.56\n47.10\n50.55\n49.17\n44.26\n54.06\n58.70\n56.41\n53.00\n57.46\n47.36\n40.10\n44.7316.40\n43.81\n41.67\n41.81\n35.86\n38.76\n43.97\n37.30\n29.90\n43.92\n23.17\n19.40\n41.2228.81\n43.63\n47.55\n48.24\n47.60\n47.09\n47.53\n44.05\n39.70\n44.56\n46.08\n45.60\n0.0020.15\n33.85\n36.35\n34.21\n32.31\n34.46\n16.02\n34.95\n31.70\n37.01\n0.00\n0.00\n0.0026.74\n0.00\n43.60\n43.00\n42.45\n42.61\n40.00\n41.00\n0.00\n0.00\n0.00\n0.00\n0.00143.39\n192.20\n243.37\n239.98\n223.64\n237.00\n230.49\n235.56\n174.80\n206.79\n139.06\n127.40\n108.73Heuristics\nLapEig\nDeepWalk\nNode2Vec\nVERSE\nLINE\nGraRep\nHOPE\nSDNE\nNetMF\nMNMF\nVAG\nWYS\n0.00 50.00 100.00 150.00 200.00PPI Wikipedia Blogcatalog CoCit (microsoft) Flickr YouTube (b) Datasets with more labels\nFigure 12: The node classi\fcation performance measured with Micro-f1 on train-test split of 50:50 with non-linear classi\fer.\n8.25\n8.00\n6.13\n7.00\n10.75\n4.88\n6.75\n6.25\n7.13\n2.75\n7.75\n10.38\n5.0011.81\n10.00\n5.58\n6.50\n12.21\n4.94\n6.07\n5.20\n7.18\n1.06\n8.62\n12.46\n5.29Heuristics\nLapEig\nDeepWalk\nNode2Vec\nVERSE\nLINE\nGraRep\nHOPE\nSDNE\nNetMF\nMNMF\nVAG\nWYS\n0.00 5.00 10.00 15.00 20.00 25.00Mean Rank Mean Penalty\nFigure 13: Mean Rank and Mean Penalty { on all datasets\nwhere all methods \fnish execution { for node classi\fcation\nwith selected performance metric as Micro-f1 and Logistic\nRegression classi\fer.\noverall score is better than many of the methods . This ob-\nservation can also be veri\fed from Figure 14 as both Mean\nRank and Mean Penalty of node classi\fcation heuristics\nbaseline is better than many of the methods . However,\nas the number of labels in the datasets increases ( >5 la-\nbels), we observe that the Micro-f1 scores of node heuris-\ntics baseline decrease drastically. The decrease in overall\nperformance re\rects that the node heuristics features lack\nthe discriminative power to classify multiple labels.\nFeature study on the Heuristics baseline: We study the\nimportance of the individual feature in the node classi\f-\ncation heuristics by analyzing the impact of the feature re-\nmoval on the node classi\fcation performance. The results\nfor node classi\fcation heuristic with logistic regression\nand EigenPro are reported in Figure 15a and Figure 15b,\nrespectively. The blue line on the top of the columns in the\n\fgures corresponds to Micro-f1 scores achieved with the\nproposed node classi\fcation heuristics. The removal of\nindividual feature in node classi\fcation heuristics did not\nresult in signi\fcant drop in the downstream performance.\nHowever, we see that the node classi\fcation heuristics'\nclassi\fcation performance with both Logistic Regression\nand EigenPro is better than the ones achieved through the\nremoval of individual features on most of the datasets.\n\u000fContext embeddings can improve performance: We\nsee from Figure 16 that levering both node and context\n5.6\n10.4\n7\n8.6\n11.8\n4\n8.4\n5.6\n6\n2.6\n6.2\n9.8\n4.48.18\n13.46\n6.78\n8.54\n14.64\n6.16\n9.14\n5.86\n6.66\n1.26\n6.76\n10.72\n4.96Few labelsHeuristics\nLapEig\nDeepWalk\nNode2Vec\nVERSE\nLINE\nGraRep\nHOPE\nSDNE\nNetMF\nMNMF\nVAG\nWYS\n0 10 20 30Mean Rank Mean PenaltyFigure 14: The Mean Rank and Mean Penalty { on datasets\nwith few labels { for node classi\fcation with selected perfor-\nmance metric as Micro-f1 and Logistic Regression classi\fer.\nof Skip-gram based models results in signi\fcant improve-\nment (up to 25%) for most of the methods. On Pubmed\ndataset, we observe that the node classi\fcation perfor-\nmance of embedding methods like LINE (2ndorder), HOPE,\nand WYS was signi\fcantly lower than that of other meth-\nods. The Micro-f1 scores of the embedding methods are\nshown in Fig. 16. We found that the Pubmed dataset\nconsists of around 80% sink nodes. As a result, when\nthe embedding methods based on Skip-gram model gener-\nate the node embeddings, the sink nodes are always con-\nsidered as \\context\" nodes and are never considered as\n\\source\" nodes. Hence, the quality of node embeddings\nof sink nodes is of lower quality. In order to have a fair\ncomparison, we concatenate both the node and context\nembeddings of the methods (whenever possible) and eval-\nuate the performance on the concatenated embeddings.\n\u000fImpact of nonlinear classi\fer: We study the impact\nof the nonlinear classi\fer on the node classi\fcation per-\nformance. The comparison results are shown in box plot\n17. The box plot represents the distribution of di\u000berences\nof Micro-f1 scores computed with the non-linear (Eigen-\nPro [37]) and linear classi\fers (Logistic Regression). The\npositive di\u000berence implies that the results with non-linear\nclassi\fer are better than linear classi\fer. For Verse, we see\na 15% absolute increase with the use of nonlinear classi-\n\fer on the PubMed dataset. The positive di\u000berence is\nstatistically signi\fcant (with paired t-test) for methods\n\nMicro-f1\n0.00.20.40.60.8\nW-Texas W-Cornell \nW-Washington W-WisconsinPPI\nWikipedia\nBlogCatalogDBLP\nPubmed MicrosoftFlickr\nYoutubeNo Degree No PageRankScore No NodeClustCf No HubScore No NodeEcc\nNo AverageNeighborDegree No AuthScore All features(a) Linear classi\fer : Logistic Regression\nMicro-f1\n020406080\nW-Texas W-Cornell \nW-Washington W-WisconsinPPI\nWikipedia\nBlogCatalogDBLP\nPubmed MicrosoftFlickr\nYoutubeNo Degree No PageRankScore No NodeClustCf No HubScore No NodeEcc\nNo AverageNeighborDegree No AuthScore All features (b) Nonlinear classi\fer : EigenPro\nFigure 15: Feature study on the node classi\fcation for node classi\fcation heuristics.\nMicro-F1\n0.000.200.400.600.80\nLINE \n(2nd)NetMF GraRep HOPE MNMF WYSNode + Context embedding Node embedding\nFigure 16: Node Classi\fcation on Directed Dataset\n(PubMed) with/without concatenation of Node embeddings\nand Context Embeddings (128 dimensions).\nDeepWalk, Verse, SDNE, GraRep and MNMF with sig-\nni\fcance level 0.05. It is worth pointing out that on the\nsmaller datasets this gain is less evident while on, the\nlarger datasets (more training data) the bene\fts of using\na nonlinear classi\fer are much clearer.\n\u000fImpact of embedding dimension: We study the im-\npact of embedding size for all the embedding methods on\nthe node classi\fcation task. Speci\fcally, we compare the\nperformances of 64 dimensional embedding with 128 di-\nmensional embedding. The improvement { quanti\fed in\nterms of performance di\u000berence { obtained with 128 di-\nmensional embedding over 64 dimensional embedding is\nreported in Figure 18. The box-plot represents the dis-\ntribution of di\u000berences in Micro-f1 scores between 128 di-\nmensional embedding and 64 dimensional embedding for\neach method on all datasets. In node classi\fcation with\nlinear classi\fer, none of the evaluated methods obtained a\nstatistically signi\fcant di\u000berence at signi\fcance level 0.05.\nWhile in node classi\fcation with non-linear classi\fer, the\nembedding method HOPE obtained a statistically signif-\nicant positive di\u000berence { at signi\fcance level 0.05 { with\n128 embedding dimension.\n\u000fImpact of embedding normalization: We study the\nimpact of L2 normalization the embeddings for the node\nclassi\fcation task. The comparison results are shown in\nFigure 19. The box plot represents the distribution of\nWYSVAGMNMFNetMFSDNEHOPEGraRepLINEVERSENode2VecDeepWalkLapEigHeuristics\n−5 0 5 10 15\nDifference between NonLinear and Linear classifierEmbedding methodsFigure 17: Box-plot represents the distribution of di\u000berences\nbetween non-linear and linear classi\fer on all the datasets.\ndi\u000berences of Micro-f1 scores for embedding methods be-\ntween normalized and unnormalized embeddings on node\nclassi\fcation task. The positive di\u000berence implies L2 nor-\nmalization results in better downstream performance. In\nnode classi\fcation with linear classi\fer, the positive dif-\nference is statistically signi\fcantly for NetMF while the\nnegative di\u000berence is statistically signi\fcantly for Deep-\nWalk at signi\fcance level 0.05 with paired t-test. While in\nnode classi\fcation with non-linear classi\fer, the positive\ndi\u000berence is statistically signi\fcantly for NetMF method\nat signi\fcance level 0.05 with paired t-test.\n\u000fNode classi\fcation performance on 10:90 train:\ntest split: We report the node classi\fcation performance\nof all methods on all the evaluated datasets with 10:90\ntrain:test split in with logistic regression classi\fer in Fig-\nure 20 and non-linear classi\fer EigenPro in Figure 21. The\nMean Rank and Mean Penalty of embedding methods on\nthe datasets for which all methods run to completion on\nour system is shown in Figure 22a. We also report the\nMean Rank and Mean Penalty of embedding methods on\ndatasets with few labels in Figure 22b. The observa-\ntions we reported with train:test 50:50 split also seem to\nhold with train:test 10:90 split. Speci\fcally, we observe\nNetMF is the most competitive method for node classi-\n\fcation while Laplacian Eigenmaps method outperforms\n\nWYSVAGMNMFNetMFSDNEHOPEGraRepLINEVERSEN2VDWLapEig\n−5 0 5Embedding methods(a) Linear classi\fer : Logistic Regression\nWYSVAGMNMFNetMFSDNEHOPEGraRepLINEVERSEN2VDWLapEig\n−10 −5 0 5Embedding methods (b) Nonlinear classi\fer : EigenPro\nFigure 18: The box-plot represents the distribution of the di\u000berences in Micro-f1 scores between 128 dimensional embedding\nand 64 dimensional embedding for each method on all datasets\nWYSVAGMNMFNetMFSDNEHOPEGraRepLINEVERSEN2VDWLapEig\n−5 0 5 10Embedding methods\n(a) Linear classi\fer : Logistic Regression\nWYSVAGMNMFNetMFSDNEHOPEGraRepLINEVERSEN2VDWLapEig\n−10 0 10 20 30 40Embedding methods (b) Nonlinear classi\fer : EigenPro\nFigure 19: Impact of embedding normalization on node classi\fcation performance.\nmultiple existing methods on multiple datasets (Blogcat-\nalog, Co-author datasets). Embedding methods such as\nDeepWalk and LINE also perform well on most datasets.\n7. DISCUSSIONS AND CONCLUSIONS\nNetwork representational learning has attracted lot of at-\ntention in past few years. An interested reader can refer to\nthe survey of network embedding methods [11, 25, 61]. The\nsurveys focus on categorization of the embedding methods\nbased either encoder-decoder framework [25] or novel tax-\nonomy [11, 61] but does not provide experimental compari-\nson of the embedding methods. There does exist one other\nexperimental survey of network embedding methods [20].\nHowever there are key di\u000berences. First, we present a sys-\ntematic study on a larger set of embedding methods, includ-\ning several more recent ideas, and on many more datasets\n(15 vs 7). Speci\fcally, we evaluate 12 embedding meth-\nods + 2 e\u000ecient heuristics on 15 datasets. Second, there\nare several key di\u000berences in terms of results reported and\nreproducability. In our work we carefully tune all hyper-\nparameters of each method as well as the logistic classi\fer\n(and include information in our reproducability notes). As\na concrete example of where such careful tuning can make\na di\u000berence consider that on Blogcatalog with a train-testsplit of 50:50, Goyal et al, achieve Macro-f1 score of 3.9%\nwhile with tuning the hyper-parameters of logistic regression\nwe achieve a Macro-f1 score of 29.2%. Third, our analysis\nreveals several important insights on the role of context, role\nof di\u000berent link prediction evaluation strategies (dot prod-\nuct vs classi\fer), impact of non-linear classi\fers and many\nothers. All of these provide useful insights for end-users\nas well as guidance for future research and evaluation in\nnetwork representational learning and downstream applica-\ntions. Fourth, we also provide a comparison against simple\nbut e\u000bective task-speci\fc baseline heuristics which will serve\nas useful strawman methods for future work in these areas.\nTo conclude, we identify several issues in the current lit-\nerature: lack of standard assessment protocol, use of default\nparameters for baselines, lack of standard benchmark, igno-\nrance of task-speci\fc baselines. Additionally, we make the\nfollowing observations:\n\u000fMNMF and NetMF are the most e\u000bective baselines for the\nlink prediction and node classi\fcation task respectively.\n\u000fNo one method completely outperform the other methods\non both link prediction and node classi\fcation tasks.\n\u000fIf one considers Laplacian Eigenmaps as a baseline, the\nclassi\fer parameters should be tuned appropriately.\n\n52.32\n50.54\n50.18\n53.57\n49.88\n57.44\n54.11\n53.93\n55.80\n57.86\n54.76\n54.30\n54.9633.92\n38.58\n39.15\n38.47\n40.57\n39.60\n39.83\n40.68\n43.20\n41.70\n40.91\n38.70\n42.2358.36\n41.30\n45.41\n48.99\n45.65\n54.35\n42.75\n50.43\n50.70\n46.86\n51.79\n49.20\n57.1740.92\n43.68\n45.23\n44.85\n42.59\n44.73\n45.36\n45.86\n46.40\n49.83\n46.36\n44.50\n47.1436.40\n36.95\n34.73\n34.86\n34.51\n34.65\n34.61\n34.34\n36.30\n35.59\n34.67\n35.30\n0.0056.61\n79.53\n80.12\n79.66\n61.00\n62.75\n76.95\n71.67\n66.30\n77.26\n70.83\n62.20\n71.12278.53\n290.58\n294.82\n300.39\n274.20\n293.52\n293.61\n296.91\n298.70\n309.11\n299.32\n284.20\n272.62Heuristics\nLapEig\nDeepWalk\nNode2Vec\nVERSE\nLINE\nGraRep\nHOPE\nSDNE\nNetMF\nMNMF\nVAG\nWYS\n0.00 100.00 200.00 300.00W - Texas W - Cornell W - Washington W - Wisconsin DBLP (Co-Author) Pubmed(a) Datasets with few labels\n9.13\n17.45\n13.56\n13.27\n11.83\n10.98\n15.95\n13.47\n12.50\n14.66\n13.58\n15.70\n17.4641.17\n41.01\n43.77\n45.44\n37.37\n50.61\n52.04\n49.67\n47.10\n51.90\n43.54\n40.70\n38.2214.03\n37.39\n35.40\n34.94\n27.92\n31.04\n35.51\n27.29\n24.90\n36.83\n17.81\n16.90\n32.5824.75\n42.35\n43.51\n44.78\n44.04\n44.65\n43.52\n40.66\n36.30\n42.11\n41.46\n42.10\n0.0019.43\n28.36\n29.67\n28.14\n23.34\n26.92\n5.59\n27.03\n27.90\n28.78\n0.00\n0.00\n0.0024.69\n0.00\n34.84\n27.77\n31.52\n30.76\n32.00\n31.22\n0.00\n0.00\n0.00\n0.00\n0.00133.20\n166.56\n200.75\n194.32\n176.03\n194.96\n184.60\n189.35\n148.70\n174.29\n116.39\n115.40\n88.26Heuristics\nLapEig\nDeepWalk\nNode2Vec\nVERSE\nLINE\nGraRep\nHOPE\nSDNE\nNetMF\nMNMF\nVAG\nWYS\n0.00 50.00 100.00 150.00 200.00PPI Wikipedia Blogcatalog CoCit (microsoft) Flickr YouTube (b) Datasets with more labels\nFigure 20: The node classi\fcation performance measured with Micro-f1 on train-test split of 10:90 with Logistic Regression.\nFor each method, the number at the end of bar represent the summation of the Micro-f1 values across the datasets.\n46.55\n53.15\n53.39\n54.11\n54.88\n62.08\n58.93\n57.50\n59.70\n59.82\n55.12\n51.30\n59.1129.32\n39.77\n42.27\n42.05\n40.45\n42.73\n41.08\n40.68\n41.10\n42.33\n40.34\n39.90\n42.2254.59\n36.43\n53.24\n57.29\n48.65\n57.92\n44.49\n57.63\n50.50\n53.77\n56.14\n46.00\n58.7033.10\n44.60\n43.89\n42.59\n43.97\n43.22\n45.27\n45.40\n47.70\n45.31\n44.69\n48.20\n44.1836.64\n37.27\n35.45\n35.98\n34.94\n35.27\n35.31\n35.89\n37.10\n36.55\n35.81\n36.00\n0.0056.27\n79.42\n80.29\n79.80\n68.00\n62.15\n77.40\n75.16\n68.20\n78.07\n72.51\n63.40\n72.72256.47\n290.65\n308.54\n311.82\n290.90\n303.37\n302.48\n312.26\n304.30\n315.86\n304.60\n284.80\n276.92Heuristics\nLapEig\nDeepWalk\nNode2Vec\nVERSE\nLINE\nGraRep\nHOPE\nSDNE\nNetMF\nMNMF\nVAG\nWYS\n0.00 100.00 200.00 300.00W - Texas W - Cornell W - Washington W - Wisconsin DBLP (Co-Author) Pubmed\n(a) Datasets with few labels\n7.35\n17.68\n17.38\n18.36\n15.37\n14.31\n19.22\n15.31\n15.80\n18.17\n16.14\n15.80\n18.1741.37\n43.59\n45.71\n47.13\n41.75\n47.00\n52.73\n45.23\n48.10\n51.12\n45.62\n40.40\n42.6116.38\n38.75\n37.78\n37.69\n30.45\n33.15\n39.15\n30.49\n26.50\n39.14\n22.37\n17.30\n35.2927.23\n41.07\n44.89\n45.69\n44.48\n44.82\n44.57\n41.22\n37.00\n42.86\n42.16\n43.40\n0.0018.47\n30.43\n30.92\n29.90\n27.00\n28.81\n15.85\n30.24\n30.20\n30.91\n0.00\n0.00\n0.0023.28\n0.00\n39.83\n32.48\n37.91\n35.82\n35.00\n35.24\n0.00\n0.00\n0.00\n0.00\n0.00134.07\n171.51\n216.52\n211.25\n196.96\n203.90\n206.52\n197.73\n157.60\n182.20\n126.28\n116.90\n96.07Heuristics\nLapEig\nDeepWalk\nNode2Vec\nVERSE\nLINE\nGraRep\nHOPE\nSDNE\nNetMF\nMNMF\nVAG\nWYS\n0.00 50.00 100.00 150.00 200.00PPI Wikipedia Blogcatalog CoCit (microsoft) Flickr YouTube (b) Datasets with more labels\nFigure 21: The node classi\fcation performance measured with Micro-f1 on train-test split of 10:90 with non-linear classi\fer.\n10.63\n7.75\n7.25\n7.38\n10.63\n6.75\n5.75\n3.38\n5.50\n6.38\n6.25\n8.88\n4.5011.22\n5.85\n5.43\n4.63\n9.93\n5.59\n6.17\n2.42\n4.22\n5.41\n7.08\n9.26\n4.42Heuristics\nLapEig\nDeepWalk\nNode2Vec\nVERSE\nLINE\nSDNE\nNetMF\nGraRep\nHOPE\nMNMF\nVAG\nWYSRank Penalty\n(a) On all datasets where all methods \fnish execution\n10.00\n9.80\n8.00\n7.80\n10.60\n6.40\n4.20\n3.60\n7.40\n6.00\n5.00\n8.80\n3.409.45\n7.15\n5.86\n4.77\n9.93\n6.10\n5.39\n3.17\n6.07\n5.36\n4.94\n8.09\n3.35Heuristics\nLapEig\nDeepWalk\nNode2Vec\nVERSE\nLINE\nSDNE\nNetMF\nGraRep\nHOPE\nMNMF\nVAG\nWYSRank Penalty (b) On datasets with few labels\nFigure 22: The Mean Rank and Mean Penalty { on datasets with few labels { for node classi\fcation with selected performance\nmetric as Micro-f1 on train-test split of 10:90 and logistic regression as the classi\fer.\n\u000fThe Link Prediction Heuristic we present is simple, e\u000e-\ncient to compute and o\u000bers competitive performance. The\nNode Classi\fcaton Heuristic is also simple and e\u000ecient to\ncompute and is e\u000bective on data-sets with fewer labels.\n\u000fFor both tasks, some methods are impervious to the use\nof context whereas for other methods context helps sig-\nni\fcantly.\u000fWhile comparing embeddings methods through link pre-\ndiction task, the superiority of the embedding methods\nshould be asserted by leveraging the classi\fer.\nWe hope the insights put forward in this study are help-\nful to the community and encourage the comparison of novel\nembedding methods with the task-speci\fc competitive meth-\nods and proposed task-speci\fc heuristics.\n\nTable 3: Link Prediction performance measured with AUROC. The \\-\" represents that the method did not scale on the\nparticular dataset.\nDatasets Heuristics LapEig DeepWalk Node2Vec Verse LINE GraRep HOPE SDNE NetMF MNMF VAG WYS\nW-Texas 77.7 73.1 79.7 83 81.7 78.2 78.7 78.7 82.4 80.9 96.0 78.6 83.0\nW-Cornell 81.5 77.3 79.2 82 87 77.5 84.4 79.9 80.2 81.2 96.7 74.8 84.4\nW-Washington 75.3 70.1 75.3 75 82.2 72.9 78.1 72.8 76.7 75.5 97.5 73.9 79.2\nW-Wisconsin 79.4 71.4 80.7 78 88.2 72.5 84.3 75.5 76.9 82.3 98.9 73.9 84.5\nPPI 90.9 78.2 89.1 88.3 89.6 87.8 90 88.4 89.3 87.3 96.9 87.4 91.5\nWikipedia 91.6 77.9 90.9 90.9 91.3 91.2 92.3 90.4 50 91.4 88.4 89.5 92.3\nWiki-Vote 91.5 83.5 97.4 97.6 94.9 96.6 88.4 97.8 96.6 95.5 92.2 94.3 98.2\nBlogCatalog 95.2 77.4 94.3 95 97.3 95.2 96.2 95.3 95.6 95.1 94 94.8 96.0\nDBLP (Co-Author) 95.6 93.3 96 95.4 97.9 94.3 97.1 89.6 50 95.9 99.4 94.1 96.8\nPubmed 87.7 89.6 89.1 89.3 96.6 92.7 77.7 90.1 88.9 89.8 94.3 93.6 97.0\nCoCit (microsoft) 89.5 95.6 97.6 97.3 83.7 97.2 97.9 94.5 91.9 96.9 96.6 96.2 -\nP2P 83.8 69.9 88.2 88.3 77.6 91.2 71.8 88.6 83.9 87.5 92.3 - -\nFlickr 92.4 93 95.8 94.7 72.6 95.2 95.5 96.5 93 97.2 - - -\nEpinions 92.2 90.9 93.3 93.4 91.9 91.6 93.7 92.7 92.7 92.8 - - -\nYoutube 96.2 96 93.6 91.4 97.6 96.5 91.4 92.4 - - - - -\nTable 4: Link Prediction performance measured with AUPR. The \\-\" represents that the method did not scale on the\nparticular dataset.\nDatasets Heuristics LapEig DeepWalk Node2Vec Verse LINE GraRep HOPE SDNE NetMF MNMF VAG WYS\nW-Texas 81.8 78 81.9 85 85 82.2 82.1 82.9 85.5 83.6 96.0 80.6 85.2\nW-Cornell 81.9 78.9 79.8 81 87.8 76.7 86.2 79.8 79.5 82 96.7 75.6 86.6\nW-Washington 80.3 75 76.5 78 86.5 75.5 82.9 77.2 81.5 80.4 97.7 78.7 83.4\nW-Wisconsin 82.3 74.8 81.6 79 90.7 76.4 87.4 78.8 80.7 85.2 98.5 78.1 86.7\nPPI 91.4 80.7 90.4 89.5 90.7 88.1 90.8 89.2 90.2 87.9 96.5 88.1 92.2\nWikipedia 93 76 92.5 92.3 92.8 92.8 93.1 91.8 75 92.8 89.9 91.4 93.5\nWiki-Vote 87.9 82.1 96.9 97.2 94.8 95.3 84 96.8 96.3 93.8 87.6 94.3 97.4\nBlogCatalog 95.1 77.5 94.3 94.8 97.9 95.1 96 95 95.5 94.8 93.6 94.6 96.1\nDBLP (Co-author) 96.7 93.8 96.8 96.1 98.2 95.6 97.4 90.9 75 96.7 99.2 95.2 97.3\nPubmed 85 85 81.5 82.3 96.8 90.3 74.1 91.4 90.5 86.1 90.3 95.2 96.9\nCocit (microsoft) 91.9 95.5 97.9 97.5 76.4 97.7 97.9 95.3 93.5 97.1 95.2 96.4 -\nP2P 79.3 68.1 84.3 84.5 68.3 88.6 71.3 85.8 80.8 84 89.6 - -\nFlickr 92.5 95 96.1 95 70.9 95.5 95.7 96.7 94 97.6 - - -\nEpinions 89.2 89.5 91.7 91.9 88.6 88.8 93.0 91.5 91.6 91.8 - - -\nYoutube 96.7 96.7 95 93 98.2 97 92.2 94 - - - - -\nTable 5: Node Classi\fcation performance measured in terms of Micro-f1 with train-test split of 50:50 with Logistic Regression.\nThe \\-\" represents that the method did not scale on the particular dataset.\nDatasets Heuristics LapEig DeepWalk Node2Vec Verse LINE GraRep HOPE SDNE NetMF MNMF VAG WYS\nW - Texas 61.8 54.6 55.1 57.2 54.5 61.8 56.1 59.1 58.0 67.1 58.0 54.8 60.6\nW - Cornell 42.1 30.9 40.5 34.3 35.4 44.1 40.9 41.9 48.5 48.1 36.1 40.3 41.8\nW - Washington 65.0 43.3 56.0 58.4 51.5 65.3 46.7 62.8 60.5 61.1 60.4 59.0 65.3\nW - Wisconsin 51.7 41.5 52.3 45.6 41.7 52.9 50.8 51.5 51.3 56.7 53.9 48.1 53.2\nPPI 10.8 22.3 21.4 21.0 19.7 19.9 20.4 18.8 17.4 21.3 18.6 19.2 22.6\nWikipedia 41.9 46.3 50.0 51.4 43.8 56.3 58.8 57.9 52.4 58.4 48.1 41.1 44.4\nBlogcatalog 17.1 42.1 41.5 41.7 35.5 38.6 41.3 34.4 29.5 41.7 21.6 17.1 38.9\nDBLP (Co-Author) 37.3 37.1 35.9 35.6 37.2 37.0 35.7 36.0 37.4 36.6 36.2 36.2 -\nPubmed 57.8 81.7 81.5 81.1 63.0 64.4 79.1 74.7 67.7 80.0 77.1 63.5 73.6\nCoCit (microsoft) 25.0 43.0 46.3 46.7 46.0 46.5 46.6 44.6 38.1 43.5 44.7 43.5 -\nFlickr 19.1 34.0 35.6 35.1 30.1 33.4 10.5 28.7 30.8 34.2 - - -\nYoutube 24.4 - 40.7 40.3 38.5 40.3 38.0 38.7 - - - - -\nTable 6: Node Classi\fcation performance measured in terms of Macro-f1 with train-test split of 50:50 with Logistic Regression.\nThe \\-\" represents that the method did not scale on the particular dataset.\nDatasets Heuristics LapEig DeepWalk Node2Vec Verse LINE GraRep HOPE SDNE NetMF MNMF VAG WYS\nW -Texas 42.1 18.1 26.9 22.7 18.1 36.4 25.0 34.4 39.5 49.9 23.8 27.8 40.3\nW -Cornell 22.2 21.3 28.1 23.2 22.4 25.0 27.7 28.7 13.1 32.8 23.5 20.6 26.5\nW -Washington 32.2 22.2 24.3 27.3 23.1 30.6 28.7 30.2 29.1 29.7 28.6 26.3 31.1\nW -Wisconsin 24.7 29.0 31.9 23.9 21.9 27.8 34.7 28.3 26.1 34.8 33.8 25.5 33.9\nPPI 6.0 17.9 18.1 18.0 16.5 16.9 17.4 15.9 15.2 17.5 15.9 13.1 17.9\nWikipedia 5.5 10.4 11.9 12.9 8.2 18.2 18.3 20.1 14.1 18.4 11.0 3.8 10.1\nBlogcatalog 3.1 29.2 27.3 27.9 22.1 23.6 28.9 20.8 14.8 28.8 8.2 3.1 26.3\nDBLP (Co-Author) 18.1 20.1 30.0 29.4 20.6 19.2 30.5 28.6 21.1 30.0 26.6 27.8 -\nPubmed 48.9 80.2 80.1 79.8 58.0 61.3 77.6 73.0 63.3 78.4 75.4 60.6 71.6\nCoCit (microsoft) 12.6 27.3 34.3 34.2 33.3 33.8 34.8 32.8 27.8 34.0 30.4 29.2 -\nFlickr 1.7 20.4 21.2 20.7 17.6 18.2 0.9 11.4 14.9 20.2 - - -\nYoutube 9.3 - 34.7 34.0 32.1 33.1 30.0 30.8 - - - - -\n\n8. REFERENCES\n[1] Microsoft Academic Graph - KDD cup, 2016.\nhttps://kddcup2016.azurewebsites.net/Data .\n[2] S. Abu-El-Haija, B. Perozzi, R. Al-Rfou, and\nA. Alemi. Watch your step: Learning node\nembeddings via graph attention. In Neural\nInformation Processing Systems , 2018.\n[3] L. A. Adamic and E. Adar. Friends and neighbors on\nthe web. Social networks , 25(3):211{230, 2003.\n[4] E. M. Airoldi, D. M. Blei, S. E. Fienberg, E. P. Xing,\nand T. Jaakkola. Mixed membership stochastic block\nmodels for relational data with application to\nprotein-protein interactions. In In Proceedings of the\nInternational Biometrics Society Annual Meeting ,\n2006.\n[5] A.-L. Barab\u0013 asi and R. Albert. Emergence of scaling in\nrandom networks. science , 286(5439):509{512, 1999.\n[6] M. Belkin and P. Niyogi. Laplacian eigenmaps for\ndimensionality reduction and data representation.\nNeural computation , 15(6):1373{1396, 2003.\n[7] A. R. Benson, D. F. Gleich, and J. Leskovec.\nHigher-order organization of complex networks.\nScience , 353(6295):163{166, 2016.\n[8] S. Bhagat, G. Cormode, and S. Muthukrishnan. Node\nclassi\fcation in social networks. In Social network\ndata analytics , pages 115{148. Springer, 2011.\n[9] D. K. Bhattacharyya and J. K. Kalita. Network\nanomaly detection: A machine learning perspective .\nChapman and Hall/CRC, 2013.\n[10] B.-J. Breitkreutz, C. Stark, T. Reguly, L. Boucher,\nA. Breitkreutz, M. Livstone, R. Oughtred, D. H.\nLackner, J. B ahler, V. Wood, et al. The biogrid\ninteraction database. Nucleic acids research ,\n36(suppl 1):D637{D640, 2007.\n[11] H. Cai, V. W. Zheng, and K. C.-C. Chang. A\ncomprehensive survey of graph embedding: Problems,\ntechniques, and applications. IEEE Transactions on\nKnowledge and Data Engineering , 30(9):1616{1637,\n2018.\n[12] S. Cao, W. Lu, and Q. Xu. Grarep: Learning graph\nrepresentations with global structural information. In\nProceedings of the 24th ACM International on\nConference on Information and Knowledge\nManagement , pages 891{900. ACM, 2015.\n[13] E. K. Cetinkaya, M. J. Alenazi, A. M. Peck, J. P.\nRohrer, and J. P. Sterbenz. Multilevel resilience\nanalysis of transportation and communication\nnetworks. Telecommunication Systems , 60(4):515{537,\n2015.\n[14] S. Chakrabarti, B. Dom, and P. Indyk. Enhanced\nhypertext categorization using hyperlinks. In ACM\nSIGMOD Record , pages 307{318. ACM, 1998.\n[15] W. W. Cohen and J. Richman. Learning to match and\ncluster large high-dimensional data sets for data\nintegration. In Proceedings of the eighth ACM\nSIGKDD international conference on Knowledge\ndiscovery and data mining , pages 475{480. ACM,\n2002.\n[16] G. Crichton, Y. Guo, S. Pyysalo, and A. Korhonen.\nNeural networks for link prediction in realistic\nbiomedical graphs: a multi-dimensional evaluation ofgraph embedding-based approaches. BMC\nBioinformatics , 19(1):176, May 2018.\n[17] R. W. Eckardt III, R. G. Wolf Jr, A. Shapiro, K. G.\nRivette, and M. F. Blaxill. Method and apparatus for\nselecting, analyzing, and visualizing related database\nrecords as a network, Mar. 2 2010. US Patent\n7,672,950.\n[18] D. Eppstein, M. S. Paterson, and F. F. Yao. On\nnearest-neighbor graphs. Discrete & Computational\nGeometry , 17(3):263{282, 1997.\n[19] L. Getoor and A. Machanavajjhala. Entity resolution:\ntheory, practice & open challenges. Proceedings of the\nVLDB Endowment , 5(12):2018{2019, 2012.\n[20] P. Goyal and E. Ferrara. Graph embedding\ntechniques, applications, and performance: A survey.\nKnowledge-Based Systems , 151:78{94, 2018.\n[21] A. Grover and J. Leskovec. node2vec: Scalable feature\nlearning for networks. In Proceedings of the 22nd ACM\nSIGKDD international conference on Knowledge\ndiscovery and data mining , pages 855{864. ACM,\n2016.\n[22] Y. Gu, Y. Sun, and J. Gao. The co-evolution model\nfor social network evolving and opinion migration. In\nProceedings of the 23rd ACM SIGKDD international\nconference on knowledge discovery and data mining ,\npages 175{184. ACM, 2017.\n[23] M. Gutmann and A. Hyv arinen. Noise-contrastive\nestimation: A new estimation principle for\nunnormalized statistical models. In Proceedings of the\nThirteenth International Conference on Arti\fcial\nIntelligence and Statistics , pages 297{304, 2010.\n[24] W. Hamilton, Z. Ying, and J. Leskovec. Inductive\nrepresentation learning on large graphs. In Advances\nin Neural Information Processing Systems , pages\n1024{1034, 2017.\n[25] W. L. Hamilton, R. Ying, and J. Leskovec.\nRepresentation learning on graphs: Methods and\napplications. arXiv preprint arXiv:1709.05584 , 2017.\n[26] P. Jaccard. \u0013Etude comparative de la distribution\n\rorale dans une portion des alpes et des jura. Bull Soc\nVaudoise Sci Nat , 37:547{579, 1901.\n[27] G. Jeh and J. Widom. Simrank: a measure of\nstructural-context similarity. In Proceedings of the\neighth ACM SIGKDD international conference on\nKnowledge discovery and data mining , pages 538{543.\nACM, 2002.\n[28] T. N. Kipf and M. Welling. Variational graph\nauto-encoders. NIPS Workshop on Bayesian Deep\nLearning , 2016.\n[29] T. N. Kipf and M. Welling. Semi-supervised\nclassi\fcation with graph convolutional networks. In\nInternational Conference on Learning Representations\n(ICLR) , 2017.\n[30] J. M. Kleinberg. Authoritative sources in a\nhyperlinked environment. Journal of the ACM\n(JACM) , 46(5):604{632, 1999.\n[31] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and\nN. Polyzotis. The case for learned index structures. In\nProceedings of the 2018 International Conference on\nManagement of Data , pages 489{504. ACM, 2018.\n[32] J. Leskovec, D. Huttenlocher, and J. Kleinberg.\n\nSigned networks in social media. In Proceedings of the\nSIGCHI conference on human factors in computing\nsystems , pages 1361{1370. ACM, 2010.\n[33] J. Liang, S. Gurukar, and S. Parthasarathy. Mile: A\nmulti-level framework for scalable graph embedding.\narXiv preprint arXiv:1802.09612 , 2018.\n[34] J. Liang, P. Jacobs, J. Sun, and S. Parthasarathy.\nSemi-supervised embedding in attributed networks\nwith outliers. In Proceedings of the 2018 SIAM\nInternational Conference on Data Mining , pages\n153{161. SIAM, 2018.\n[35] D. Liben-Nowell and J. Kleinberg. The link-prediction\nproblem for social networks. Journal of the American\nsociety for information science and technology ,\n58(7):1019{1031, 2007.\n[36] L. L u and T. Zhou. Link prediction in complex\nnetworks: A survey. Physica A: statistical mechanics\nand its applications , 390(6):1150{1170, 2011.\n[37] S. Ma and M. Belkin. Diving into the shallows: a\ncomputational perspective on large-scale shallow\nlearning. In Advances in Neural Information\nProcessing Systems , pages 3778{3787, 2017.\n[38] M. Mahoney. Large text compression benchmark.\nURL: http://www. mattmahoney. net/text/text. html ,\n2011.\n[39] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and\nJ. Dean. Distributed representations of words and\nphrases and their compositionality. In Proceedings of\nthe 26th International Conference on Neural\nInformation Processing Systems - Volume 2 , NIPS'13,\npages 3111{3119, USA, 2013. Curran Associates Inc.\n[40] A. Mnih and G. E. Hinton. A scalable hierarchical\ndistributed language model. In Advances in neural\ninformation processing systems , pages 1081{1088,\n2009.\n[41] G. E. Moon, A. Sukumaran-Rajam, S. Parthasarathy,\nand P. Sadayappan. Pl-nmf: Parallel\nlocality-optimized non-negative matrix factorization.\narXiv preprint arXiv:1904.07935 , 2019.\n[42] M. Newman. Networks: an introduction . Oxford\nuniversity press, 2010.\n[43] M. Nickel, K. Murphy, V. Tresp, and E. Gabrilovich.\nA review of relational machine learning for knowledge\ngraphs. Proceedings of the IEEE , 104(1):11{33, 2016.\n[44] M. Ou, P. Cui, J. Pei, Z. Zhang, and W. Zhu.\nAsymmetric transitivity preserving graph embedding.\nInProceedings of the 22nd ACM SIGKDD\ninternational conference on Knowledge discovery and\ndata mining , pages 1105{1114. ACM, 2016.\n[45] L. Page, S. Brin, R. Motwani, and T. Winograd. The\npagerank citation ranking: Bringing order to the web.\nTechnical report, Stanford InfoLab, 1999.\n[46] C. C. Paige and M. A. Saunders. Towards a\ngeneralized singular value decomposition. SIAM\nJournal on Numerical Analysis , 18(3):398{405, 1981.\n[47] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk:\nOnline learning of social representations. In\nProceedings of the 20th ACM SIGKDD international\nconference on Knowledge discovery and data mining ,\npages 701{710. ACM, 2014.\n[48] J. Qiu, Y. Dong, H. Ma, J. Li, C. Wang, and K. Wang.Netsmf: Large-scale network embedding as sparse\nmatrix factorization. Proceedings of the 2019 World\nWide Web Conference on World Wide Web , 2019.\n[49] J. Qiu, Y. Dong, H. Ma, J. Li, K. Wang, and J. Tang.\nNetwork embedding as matrix factorization: Unifying\ndeepwalk, line, pte, and node2vec. In Proceedings of\nthe Eleventh ACM International Conference on Web\nSearch and Data Mining , pages 459{467. ACM, 2018.\n[50] L. F. Ribeiro, P. H. Saverese, and D. R. Figueiredo.\nstruc2vec: Learning node representations from\nstructural identity. In Proceedings of the 23rd ACM\nSIGKDD International Conference on Knowledge\nDiscovery and Data Mining , pages 385{394. ACM,\n2017.\n[51] M. Richardson, R. Agrawal, and P. Domingos. Trust\nmanagement for the semantic web. In International\nsemantic Web conference , pages 351{368. Springer,\n2003.\n[52] M. Ripeanu and I. Foster. Mapping the gnutella\nnetwork: Macroscopic properties of large-scale\npeer-to-peer systems. In international workshop on\npeer-to-peer systems , pages 85{93. Springer, 2002.\n[53] A. Sinha, R. Cazabet, and R. Vaudaine. Systematic\nbiases in link prediction: Comparing heuristic and\ngraph embedding based methods. In L. M. Aiello,\nC. Cheri\f, H. Cheri\f, R. Lambiotte, P. Li\u0013 o, and L. M.\nRocha, editors, Complex Networks and Their\nApplications VII , pages 81{93, Cham, 2019. Springer\nInternational Publishing.\n[54] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and\nQ. Mei. Line: Large-scale information network\nembedding. In Proceedings of the 24th International\nConference on World Wide Web , pages 1067{1077.\nInternational World Wide Web Conferences Steering\nCommittee, 2015.\n[55] A. Tsitsulin, D. Mottin, P. Karras, and E. M uller.\nVerse: Versatile graph embeddings from similarity\nmeasures. In Proceedings of the 2018 World Wide Web\nConference on World Wide Web , pages 539{548.\nInternational World Wide Web Conferences Steering\nCommittee, 2018.\n[56] P. Vijayan, Y. Chandak, M. M. Khapra, and\nB. Ravindran. Fusion graph convolutional networks.\nMining and Learning with Graphs (MLG), KDD , 2018.\n[57] C. Wang, V. Satuluri, and S. Parthasarathy. Local\nprobabilistic models for link prediction. In Proceedings\nof the 2007 Seventh IEEE International Conference on\nData Mining , ICDM '07, pages 322{331, Washington,\nDC, USA, 2007. IEEE Computer Society.\n[58] D. Wang, P. Cui, and W. Zhu. Structural deep\nnetwork embedding. In Proceedings of the 22nd ACM\nSIGKDD international conference on Knowledge\ndiscovery and data mining , pages 1225{1234. ACM,\n2016.\n[59] X. Wang, P. Cui, J. Wang, J. Pei, W. Zhu, and\nS. Yang. Community preserving network embedding.\nInAAAI , pages 203{209, 2017.\n[60] R. Zafarani and H. Liu. Social computing data\nrepository at asu, 2009.\n[61] D. Zhang, J. Yin, X. Zhu, and C. Zhang. Network\nrepresentation learning: A survey. IEEE transactions\non Big Data , 2018.\n\n[62] F. Zhang, W. Zhang, Y. Zhang, L. Qin, and X. Lin.\nOlak: an e\u000ecient algorithm to prevent unraveling in\nsocial networks. Proceedings of the VLDB Endowment ,\n10(6):649{660, 2017.\n[63] M. Zhao and V. Saligrama. Anomaly detection with\nscore functions based on nearest neighbor graphs. In\nAdvances in neural information processing systems ,\npages 2250{2258, 2009.\n[64] T. Zhou, L. L u, and Y.-C. Zhang. Predicting missing\nlinks via local information. The European Physical\nJournal B , 71(4):623{630, 2009.",
  "textLength": 92099
}