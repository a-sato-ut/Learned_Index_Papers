{
  "paperId": "d327539c0a90a245b36c690ae81f29397706910c",
  "title": "Sig2Model: A Boosting-Driven Model for Updatable Learned Indexes",
  "pdfPath": "d327539c0a90a245b36c690ae81f29397706910c.pdf",
  "text": "SIG2MODEL: A BOOSTING-DRIVENMODEL FORUPDATABLE\nLEARNEDINDEXES\nAlireza Heidari\nHuawei Technologies Ltd\nVancouver, Canada\nalireza.heidarikhazaei@huawei.comAmirhossein Ahmadi\nHuawei Technologies Ltd\nVancouver, Canada\namirhossein.ahmadi1@huawei.com\nWei Zhang\nHuawei Technologies Ltd\nVancouver, Canada\nwei.zhang6@huawei.comYing Xiong\nHuawei Technologies Ltd\nVancouver, Canada\nying.xiong2@huawei.com\nABSTRACT\nLearned Indexes (LIs) represent a paradigm shift from traditional index structures by employing\nmachine learning models to approximate the Cumulative Distribution Function (CDF) of sorted\ndata. While LIs achieve remarkable efficiency for static datasets, their performance degrades under\ndynamic updates: maintaining the CDF invariant (PF(k) = 1 ) requires global model retraining,\nwhich blocks queries and limits the Queries-per-second (QPS) metric. Current approaches fail to\naddress these retraining costs effectively, rendering them unsuitable for real-world workloads with\nfrequent updates. In this paper, we presentSig2Model,an efficient and adaptive learned index\nthat minimizes retraining costthrough three key techniques:(1) A Sigmoid boosting approximation\ntechnique that dynamically adjusts the index model by approximating update-induced shifts in data\ndistribution with localized sigmoid functions that preserves the model’s ϵ-bounded error guarantees\nwhile deferring full retraining.(2) Proactive update trainingvia Gaussian Mixture Models (GMMs)\nthat identifies high-update-probability regions for strategic placeholder allocation that speeds up\nupdates coming on these slots.(3) A neural joint optimization frameworkthat continuously refining\nboth the sigmoid ensemble and GMM parameters via gradient-based learning. We rigorously evaluate\nSig2Model against state-of-the-art updatable LIs on real-world and synthetic workloads, and show\nthat Sig2Modelreduces retraining cost by 20× while it shows up to 3×higher QPS and 1000×\nlower memory usage.\n1 Introduction\nContext.Learned Indexes (LIs) [ 14,7,43,33,63,38,25,36,42,32] represent a paradigm shift in database indexing\nby replacing traditional pointer-based structures (e.g., B-trees) with machine learning models that directly approximate\nthecumulative distribution function (CDF)of sorted data. At their core, LIs treat the indexing problem as a modeling\ntask: given a sorted dataset, they learn a mapping of keys to their positions by fitting the CDF F(k) , which describes the\nprobability that a key ≤kexists in the dataset. This approach enablessingle-step position predictionsduring queries,\nbypassing theO(logn)traversals of B-trees [17].\nThe Recursive Model Index (RMI) [ 40] exemplifies this approach through a hierarchical model ensemble, where\nhigher levels narrow the search range and the leaf models predict exact positions. Practical implementations like\nALEX [ 14], DobLIX [ 33], LISA [ 43] optimize this further usingpiecewise linear regression, partitioning the key space\ninto segments modeled by: pos=a×k+b±E , where a, bare learned parameters and Ebounds the prediction error,\nensuring correctness via a final localized search ( ϵ-bounded error), and achieving 2–10× faster lookups than B-trees for\nstatic data [17].arXiv:2509.20781v1  [cs.LG]  25 Sep 2025\n\nSig2Model: A Boosting-Driven Model for Updatable Learned Indexes\nFigure 1:(a) Retrain cost on three updatable LIs. Number of retrain occurrences and average retrain duration\nare shown by bar plots and×markers, respectively. (b) Impact of update on an LI modelM i(.)\nHowever, a fundamental limitation of LIs stems from their inherent assumption of static data distributions. Since CDF\nmust maintainPF(k) = 1 , any update to the key domain (insertions/deletions) necessitates non-local adjustments\nto the entire model. This requirement makes it particularly challenging to preserve model accuracy in dynamic\nworkloads [56].\nPrevious studies [ 20,77,14,19,47,63,18,71,33] attempt to address this problem. Aside from methods such\nas DobLIX [ 33], which implement LI in read-only structures and thus avoid update issues, other approaches have\nlimitations because they ignore the training cost of LI models [ 69,21,32]. Figure 1(a) quantifies the retraining overhead\nof three state-of-the-art updatable LIs. The results reveal two key insights: (1) LIPP [ 70] and DILI [ 45] exhibit\nfrequent retraining (approximately once every 500 updates), and (2) while ALEX [ 14] shows fewer retraining events,\neach retraining incurs significantly higher latency. These excessive retraining overheads render current LI systems\nimpractical for update-intensive workloads, common in real-world applications, as each retraining operation blocks\nquery processing and severely degrades system QPS.\nMotivation.Figure 1(b) illustrates how a single update affects the key space of an LI model. This model, denoted\nasMi1, with a non-zero error E, maps a range across a key space. Incoming updates, viewed as a random variable,\nimpact this range in four distinct ways: ( u1) shifts all elements uniformly without changing the range size; ( u2) expands\nthe range by shiftingM i(k)to the right; (u 3) enlarges the range on the right side; (u 4) does not alter the range size.\nEach update induces a step-wise displacement in the model’s prediction space. We reformulate retraining as a distri-\nbutional prediction problem, where sigmoid functions approximate these discrete shifts smoothly. The differentiable\nproperties of the sigmoid enable gradual adaptation of the model, deferring full retraining of CDF. For a single update, the\nsigmoid σ0(k, A, ω u, u)will be added to the model Mi(k). When incremental updates exceed the capacity of a single sig-\nmoid, we introduce aSigmaSigmoidensemble to capture cumulative effects: M′\ni(k) =M i(k) +PN\ni=1σ(k, A i, ωi, ϕi)\n, where Ndynamically grows with new update patterns. This boosting approach amortizes retraining costs by (1)\npreserving existing model parameters and (2) isolating adjustments to affected regions via localized sigmoid terms (See\nAppendix C for a detailed motivational example).\nApproach.We propose Sigma-Sigmoid Model ing, (Sig2Model),an efficient updatable learned index that minimizes\nretraining through adaptive sigmoid approximation and proactive workload modeling.Our approach introduces three\nkey techniques:First, Sig2Model employs asigmoid boosting techniquethat dynamically adjusts the index model to\nincoming updates. By approximating the step-wise patterns of updates with localized sigmoid functions, each acting as a\nweak learner, the system incrementally corrects model errors while maintaining ϵ-bounded error guarantee. This allows\ncontinuous model adaptation without immediate retraining.Second, we develop aGaussian Mixture Model (GMM)\ncomponent that predicts update patterns, enabling strategic insertion of placeholders in high-probability update regions.\nThis anticipatory mechanism significantly reduces future retraining needs by pre-allocating space in frequently modified\nindex segments for future updates.Third, Sig2Model integrates these components through aunified neural architecture\nthat jointly optimizes both the sigmoid ensemble parameters and GMM distributions via gradient-based learning. The\nsystem processes updates in batched operations through a dedicated buffer, with a control module monitoring model\nerror bounds to trigger retraining only when necessary. During retraining phases, the system simultaneously refines\nboth the sigmoid approximations and placeholder allocations based on observed update patterns.\nSig2Model adapts an LI model2to support efficient updates using the above three techniques. The end-to-end workflow\nof Sig2Model for update and lookup operations is shown in Figure 2. For updates, the system first checks whether the\nincoming update uexists in the current index domain Dτ. If a pre-allocated placeholder is available, uis inserted directly.\nOtherwise, it is staged in the Update Buffer. This buffer serves as an efficient batching mechanism, accumulating updates\nuntil a threshold (ρ) is reached, at which point neural network training is triggered to optimize the SigmaSigmoid and\nGMM parameters. When the number of active sigmoids reach system capacity Nduring this process, a full retraining\nis initiated. For lookups, queries first probe the buffer for key k. On a miss, the inference module applies the learned\n1The full table of notations is provided in Appendix A.\n2Our implementation builds upon RadixSpline [38].\n2\n\nSig2Model: A Boosting-Driven Model for Updatable Learned Indexes\nBuﬀer(a) Update\n(b) Lookup<latexit sha1_base64=\"xjx8+W4NUiq0eMgd25q7Mz58Vzs=\">AAADIHicbVJNbxMxEHWWrxK+WjhyWREhcaiiJKpKj5V64VQFibRFSRTZ3tmsFXtt2WOaaJVfwRUu/BpuiCP8GrzJHnCakVb7/GYsz5s3zEjhsNf700ru3X/w8NHB4/aTp8+evzg8ennltLccRlxLbW8YdSBFCSMUKOHGWKCKSbhmi4s6f/0FrBO6/IQrA1NF56XIBacYqM+Xl7NqMnRiPTvs9Lq9TaR3Qb8BHdLEcHaUtCaZ5l5BiVxS58ZnBqcVtSi4hHV74h0Yyhd0DuMAS6rATatNw+v0bWCyNNc2fCWmG/b/GxVVzq0UC5WKYuF2czW5Lzf2mJ9NK1Eaj1Dy7UO5lynqtFafZsICR7kKgHIrQq8pL6ilHMOMoleY1gukzEVKquVWQMTNLTWF4MuYVV6isPo2Zp1nnJp69Ou9miIy+MaCTSoepvUSsuPab1d6xcBCFsYu5zrIKdQgHn2FhYVwsBi07NhSmXlupEa3t5XjxoL6j7DEuIix0FU7LE1/d0XugqtBt3/aPf140jkfNOtzQF6TN+Qd6ZP35Jx8IEMyIpwo8pV8I9+TH8nP5Ffye1uatJo7r0gUyd9/6JMJvg==</latexit>NN \n<latexit sha1_base64=\"GLK9e7Cyn5y56G64SfkZreS06Co=\">AAADH3icbVJNaxsxEJW3H0ndr6Q99rLUFHoIxjYlzTHQS0/BhTox2MZI8uyusLQS0qixWfwnem0v/TW9lV7zb6q191A5Hlj26c0IzZs3zEjhsNe7ayUPHj56fHT8pP302fMXL09OX1077S2HEddS2zGjDqQoYYQCJYyNBaqYhBu2/FTnb76BdUKXX3FtYKZoXopMcIqBGl9dzavpUGzmJ51et7eN9D7oN6BDmhjOT5PWdKG5V1Ail9S5yYXBWUUtCi5h0556B4byJc1hEmBJFbhZte13k74LzCLNtA1fiemW/f9GRZVza8VCpaJYuP1cTR7KTTxmF7NKlMYjlHz3UOZlijqtxacLYYGjXAdAuRWh15QX1FKOYUTRK0zrJVLmIiXVaicg4nJLTSH4KmaVlyisvo1Z5xmnpp785qCmiAy2seCSiodpvYTFWW23K71iYGERxi5zHeQUahCPvsLCQjhYDFr2bKlMnhmp0R1s5ayxoP4jrDAuYix01Q5L099fkfvgetDtn3fPv3zoXA6a9Tkmb8hb8p70yUdyST6TIRkRTiT5Tn6Qn8mv5HfyJ/m7K01azZ3XJIrk7h+IfglB</latexit>NN⇧Buﬀer<latexit sha1_base64=\"0AeS0RLlnGE7KpKKajX1lnofl6Y=\">AAADKXicbVI9b9swEKXVj6Tul9OOXYQaBToEhm0UacYAWToFKVAnAWzDIKmTRJgUCfKY2hD0T7K2S39Nt7Zr/0gpW0Pl+ABBx3dH8N57x4wUDofD353owcNHjw8On3SfPnv+4mXv6NWV095ymHAttb1h1IEUBUxQoIQbY4EqJuGaLc/r+vUtWCd08QXXBuaKZoVIBacYoEWvd3GxKGeKYs6pLM+ratHrDwfDTcT3k1GT9EkTl4ujqDNLNPcKCuSSOjc9NTgvqUXBJVTdmXdgKF/SDKYhLagCNy83k1fxu4Akcapt+AqMN+j/N0qqnFsrFjrrGd1urQb31aYe09N5KQrjEQq+fSj1MkYd1zLEibDAUa5DQrkVYdaY59RSjkGs1itM6yVS5lpMytWWQAvLLDW54Ks2qrxEYfXXNuo849TUHlR7ObXAYCALfqm2mNZLSI5r413hFQMLSZBdZjrQydW4LX2JuYVwsBi47NhSmiw1UqPbO8pxY0H9R1hhu4mxMFU3LM1od0XuJ1fjwehkcPL5Q/9s3KzPIXlD3pL3ZEQ+kjPyiVySCeHkltyRb+R79CP6Gf2K/mxbo05z5zVpRfT3H4/EDR4=</latexit>NNCEmbedding<latexit sha1_base64=\"C8NnFCN4+zOssujBPSYcXQS1Fgg=\">AAADO3icbVI9jxMxEHWWryN85aCkWV2ERHGKktPpSHkSDWWQyN1JSYhs72TXir227DEkWqXn19BCww+hpkO09HiTLXAuI633+c1YM29mmJHCYb//s5XcuXvv/oOjh+1Hj588fdY5fn7ltLccxlxLbW8YdSBFCWMUKOHGWKCKSbhmy7e1//oTWCd0+QHXBmaK5qVYCE4xUPPOCaN8aaw2NN8x1RSMEzKg6UhsPoZj3un2e/2tpbfBoAFd0thofpy0ppnmXkGJXFLnJkODs4paFFzCpj31DkxIS3OYBFhSBW5WbbVs0leBydKFtuErMd2y/7+oqHJurViIVBQLt++ryUO+icfFcFaJ0niEku8SLbxMUad1Y9JMWOAo1wFQbkWoNeUFtZRjaF+UhWm9RMpcpKRa7QREXG6pKQRfxazyEoXVn2PWecapqWewOagpIsNIWZiXiptpvYTstF4FV3rFwEIW2i5zHeQU6ixufYWFhXCxGLTsjaUy+cJIje5gKafNCOo/wgrjIMZCVe2wNIP9FbkNrs56g4vexfvz7uWwWZ8j8pKckNdkQN6QS/KOjMiYcPKFfCXfyPfkR/Ir+Z382YUmrebNCxJZ8vcf+3sVPQ==</latexit>backpropagation⇧✏⇧\n<latexit sha1_base64=\"RP7J53DQmelJjM+6onICqR96hyg=\">AAADPXicbVK9jhMxEHaWvyP85aCkWRGQKE5RcjodKU+ioQwSuTspCZHtnd1YsdeWPYZEq7wAT0MLDc/BA9AhWlq8yRY4l5HW+/mbsWa+mWFGCof9/s9Wcuv2nbv3ju63Hzx89PhJ5/jppdPechhzLbW9ZtSBFCWMUaCEa2OBKibhii3f1v6rT2Cd0OUHXBuYKVqUIhecYqDmnZeM8qWx2tBix1RTME7IgKYjJzYf63Pe6fZ7/a2lN8GgAV3S2Gh+nLSmmeZeQYlcUucmQ4OziloUXMKmPfUOTEhMC5gEWFIFblZt1WzSV4HJ0lzb8JWYbtn/X1RUObdWLEQqigu376vJQ76Jx3w4q0RpPELJd4lyL1PUad2aNBMWOMp1AJRbEWpN+YJayjE0MMrCtF4iZS5SUq12AiKusNQsBF/FrPIShdWfY9Z5xqmpp7A5qCkiw1BZmJiKm2m9hOykXgZXesXAQhbaLgsd5CzUadz6ChcWwsVi0LI3lsoUuZEa3cFSTpoR1H+EFcZBjIWq2mFpBvsrchNcnvYG573z92fdi2GzPkfkOXlBXpMBeUMuyDsyImPCyRfylXwj35Mfya/kd/JnF5q0mjfPSGTJ33/StBY3</latexit>backpropagation ✏ Feed Forward\n<latexit sha1_base64=\"Wirj7eU4nIu73dHyat5P1/r0QVk=\">AAADHXicbVJNbxMxEHWWrxK+WjhyWREhFSmKkqoqVU+VuHAsEkmD0qiyvbMbE3tt2WOaaJX/wBUu/BpuiCvi3+BN9oDTjLTa5zdjed68YUYKh/3+31Zy5+69+w/2HrYfPX7y9Nn+wfOR095yGHIttR0z6kCKEoYoUMLYWKCKSbhk83d1/vILWCd0+RGXBqaKFqXIBacYqNHhuPvpTft6v9Pv9deR3gaDBnRIExfXB0nrKtPcKyiRS+rc5NTgtKIWBZewal95B4byOS1gEmBJFbhpte52lb4OTJbm2oavxHTN/n+josq5pWKhUlGcue1cTe7KTTzmp9NKlMYjlHzzUO5lijqtpaeZsMBRLgOg3IrQa8pn1FKOYUDRK0zrOVLmIiXVYiMg4gpLzUzwRcwqL1FYfROzzjNOTT331U5NERlMY8EjFQ/TeglZtzbblV4xsJCFsctCBzkzdRSPvsKZhXCwGLRs2VKZIjdSo9vZSrexoP4jLDAuYix0VS/NYHtFboPRUW9w0jv5cNw5P2vWZ4+8JK/IIRmQt+ScvCcXZEg4+Uy+km/ke/Ij+Zn8Sn5vSpNWc+cFiSL58w9Rygdl</latexit>(X, Y)\n<latexit sha1_base64=\"gGlbYfYi6XschyfbJe+RU/zWYQs=\">AAADN3icbVJNbxMxEHWWrxI+mpYjEloRVRQpirIVKj1WggMXpCCRtlKyrGzvbGLFXq/sMU20yo1fwxUu/BRO3BBX/gHeZA9smpEsP7+x5XnzhhVSWBwMfraCW7fv3L23d7/94OGjx/udg8MLq53hMOJaanPFqAUpchihQAlXhQGqmIRLNn9T5S8/g7FC5x9xWUCs6DQXmeAUPZV0nr39NEHqxu9fJPLYJSUotupNhiKp2Jdx0ukO+oN1hDdBVIMuqWOYHAStSaq5U5Ajl9Ta8VmBcUkNCi5h1Z44CwXlczqFsYc5VWDjcq1jFR55Jg0zbfzKMVyz/78oqbJ26SsMjxTFmd3OVeSu3NhhdhaXIi8cQs43H2VOhqjDqilhKgxwlEsPKDfC1xryGTWUo29d4xem9Rwpsw0l5WIjoMFNDS1mgi+arHIShdHXTdY6xmlRObLaqalBejuZd081m2mchLRXjYHNnWJgIPVtl1Pt5czUSbP1Jc4M+INBr2XLlrKYZoXUaHeW0qstqHaEBTYvMearavuhibZH5Ca4OOlHp/3TD6+651E9PnvkKXlOjklEXpNz8o4MyYhw8oV8Jd/I9+BH8Cv4HfzZXA1a9ZsnpBHB338cOxIb</latexit>D⌧[M0l(uemb,⇧⌧)]\n<latexit sha1_base64=\"gGlbYfYi6XschyfbJe+RU/zWYQs=\">AAADN3icbVJNbxMxEHWWrxI+mpYjEloRVRQpirIVKj1WggMXpCCRtlKyrGzvbGLFXq/sMU20yo1fwxUu/BRO3BBX/gHeZA9smpEsP7+x5XnzhhVSWBwMfraCW7fv3L23d7/94OGjx/udg8MLq53hMOJaanPFqAUpchihQAlXhQGqmIRLNn9T5S8/g7FC5x9xWUCs6DQXmeAUPZV0nr39NEHqxu9fJPLYJSUotupNhiKp2Jdx0ukO+oN1hDdBVIMuqWOYHAStSaq5U5Ajl9Ta8VmBcUkNCi5h1Z44CwXlczqFsYc5VWDjcq1jFR55Jg0zbfzKMVyz/78oqbJ26SsMjxTFmd3OVeSu3NhhdhaXIi8cQs43H2VOhqjDqilhKgxwlEsPKDfC1xryGTWUo29d4xem9Rwpsw0l5WIjoMFNDS1mgi+arHIShdHXTdY6xmlRObLaqalBejuZd081m2mchLRXjYHNnWJgIPVtl1Pt5czUSbP1Jc4M+INBr2XLlrKYZoXUaHeW0qstqHaEBTYvMearavuhibZH5Ca4OOlHp/3TD6+651E9PnvkKXlOjklEXpNz8o4MyYhw8oV8Jd/I9+BH8Cv4HfzZXA1a9ZsnpBHB338cOxIb</latexit>D⌧[M0l(uemb,⇧⌧)]Is Empty?True\n<latexit sha1_base64=\"TtOBs7FKiWBwrnbnxMB0SX50AvA=\">AAADIHicbVJNaxsxEJW3H0ndr6Q99rLUFFIIxg4lCT0FeukxhTpJcUyQtLO7wtJKSKPEZvGv6LW55Nf0Vnpsf0219h4qxwPLPr0ZoXnzhhkpHA4GfzrJg4ePHm9tP+k+ffb8xcud3VdnTnvLYcS11PaCUQdSVDBCgRIujAWqmIRzNv3U5M+vwTqhq684NzBRtKhELjjFQH0bmYwi7Pn3Vzu9QX+wjPQ+GLagR9o4vdpNOpeZ5l5BhVxS58bHBic1tSi4hEX30jswlE9pAeMAK6rATeplw4v0XWCyNNc2fBWmS/b/GzVVzs0VC5WKYunWcw25KTf2mB9PalEZj1Dx1UO5lynqtFGfZsICRzkPgHIrQq8pL6mlHMOMoleY1lOkzEVK6tlKQMQVlppS8FnMKi9RWH0Ts84zTk0z+sVGTREZfGPBJhUP03oJ2X7jt6u8YmAhC2OXhQ5ySnUQj77G0kI4WAxa1mypTZEbqdFtbGW/taD5I8wwLmIsdNUNSzNcX5H74OygPzzsH3750Dv52K7PNnlD3pI9MiRH5IR8JqdkRDhR5Dv5QW6Tu+Rn8iv5vSpNOu2d1ySK5O8/DioJdA==</latexit>Update(u)\nNotFoundEmbedding<latexit sha1_base64=\"EojdSLFjU60Woldl4lqTtB23P9g=\">AAADHnicbVJNaxsxEJW3H0ndr6Q99rLUBHoIxg4h9THQS48p1HHAMUbSzq7FSishjVqbxT8i1/bSX9Nb6bX9N9Xae6gcDwiN3kjovTfDjBQOB4O/neTBw0ePDw6fdJ8+e/7i5dHxq2unveUw5lpqe8OoAykqGKNACTfGAlVMwoSVH5r65AtYJ3T1GVcGZooWlcgFpxigSTmvQbH1/Kg36A82kd5Phm3SI21czY+Tzm2muVdQIZfUuenI4KymFgWXsO7eegeG8pIWMA1pRRW4Wb2hu05PApKlubZhVZhu0P9f1FQ5twq00hNFceF2aw24rzb1mI9mtaiMR6j49qPcyxR12mhPM2GBo1yFhHIrAteUL6ilHIND0S9M6xIpc5GSerkVEGGFpWYh+DJGlZcorP4ao84zTk1j/HqvpggMXWOhSSo203oJ2WnTbVd5xcBCFmyXhQ5yFuostr7GhYVwsBi07LSlNkVupEa3l8pp24JmR1hifImxwKobhma4OyL3k+uz/vCif/HpvHc5asfnkLwhb8k7MiTvySX5SK7ImHBSkjvyjXxPfiQ/k1/J7+3VpNO+eU2iSP78AzJeCSs=</latexit>kemb\n<latexit sha1_base64=\"pDPvyUViZVc82XELOi+NKCF8+2Q=\">AAADIHicbVJNaxsxEJW3X6n7lbTHXpaaQgrB2CGkoadALj30kEKdpDgmSNrZXWFpJaRRa7P4V+SaXPpreis9tr+mWnsPleOBZUdvRmjee8OMFA4Hgz+d5N79Bw8fbT3uPnn67PmL7Z2XZ057y2HEtdT2glEHUlQwQoESLowFqpiEczY9aern38A6oasvODcwUbSoRC44xQB9/aT11Jvd6bur7d6gP1hGejcZtkmPtHF6tZN0LjPNvYIKuaTOjY8MTmpqUXAJi+6ld2Aon9ICxiGtqAI3qZcDL9K3AcnSXNvwVZgu0f9v1FQ5N1csdCqKpVuvNeCm2thjfjSpRWU8QsVXD+VepqjThn2aCQsc5TwklFsRZk15SS3lGDSKXmFBGaTMRUzq2YpAhBWWmlLwWYwqL1FY/T1GnWecmkb6xUZOERh8Y8EmFYtpvYRsr/HbVV4xsJAF2WWhA51S7cfS11haCAeLgcuaLbUpciM1uo2j7LUWNH+EGcZNjIWpumFphusrcjc52+8PD/uHnw96xx/a9dkir8kbskuG5D05Jh/JKRkRThS5JjfkNvmR/Ex+Jb9XrUmnvfOKRJH8/QcxjgmB</latexit>Lookup(k)NN Training\n<latexit sha1_base64=\"GkvXtU39m3QZbUDziirnt/G8tRg=\">AAADKnicbVJNbxMxEHWWrxK+EjhyWREhcaiipKpKxakIDhyLRNpKSYhs7+yuFXtt2WOaaJX8E65w4ddwq7jyQ/Ame2DTjLTa5zdjeea9YUYKh4PBTSu6c/fe/QcHD9uPHj95+qzTfX7htLccRlxLba8YdSBFASMUKOHKWKCKSbhk8w9V/vIbWCd08QWXBqaKZoVIBacYqFmn+z5J1n6Nev3xazlB6lezTm/QH2wivg2GNeiROs5n3ag1STT3Cgrkkjo3PjU4LalFwSWs2hPvwFA+pxmMAyyoAjctN62v4teBSeJU2/AVGG/Y/2+UVDm3VCxUKoq5281V5L7c2GN6Oi1FYTxCwbcPpV7GqONKhzgRFjjKZQCUWxF6jXlOLeUY1Gq8wrSeI2WuMUm52A7Q4DJLTS74oskqL1FYfd1knWecmsqE1d6ZGmRwkAXDVFNM6yUkh5XzrvCKgYUkyC4zHcbJ1VFT+hJzC+FgMcyyY0tpstRIjW5vK4e1BdUfYYHNIsZCV+2wNMPdFbkNLo76w5P+yefj3tm7en0OyEvyirwhQ/KWnJFP5JyMCCfX5Dv5QX5Gv6Lf0U30Z1sateo7L0gjor//ALDxDec=</latexit>Add u to D⌧<latexit sha1_base64=\"C20z1hrjs6tXZYReuCGbfZSPKFY=\">AAADQnicbVLNbhMxEHaWvxL+UjhyWREhgVRFSQVthYRUwYVjkUhbKZtGtnd214q9Xtnjkmi7z8DTcIULL8ErcENcOeBN9sCmGcny+JuxZr5vhhVSWBwOf3aCGzdv3b6zc7d77/6Dh496u49PrXaGw5hrqc05oxakyGGMAiWcFwaoYhLO2Px9HT+7BGOFzj/hsoCpomkuEsEpemjWexlpH2aGciijzBb1/ToEVVUX5dW7iwipu3obmUxXs15/OBiuLLzujBqnTxo7me0GnSjW3CnIkUtq7eSowGlJDQouoepGzoKvN6cpTLybUwV2Wq4oVeFzj8Rhoo0/OYYr9P8fJVXWLhXzmYpiZjdjNbgtNnGYHE1LkRcOIefrQomTIeqw1ieMhQGOcukdyo3wvYY8o14f9Cq2qjCt50iZbTEpF2sCLSw1tMgEX7RR5SQKoz+3UesYp0U9nGorpxZYj84PUrXFNE5CvFdvhM2dYmAg9rLLVHs6mdpvS19iZsA/DHouG2MpizQppEa7tZW9ZgT1jbDAdhJjvquuX5rR5opcd073B6ODwcHHV/3jN8367JCn5Bl5QUbkkByTD+SEjAknX8hX8o18D34Ev4LfwZ91atBp/jwhLQv+/gOdRRgu</latexit>|B⌧|=⇢z}|{<latexit sha1_base64=\"uf9c293tAVaUuL+dTHD2kYRdjQ0=\">AAADK3icbVJNj9MwEHXD11K+usuRS0SFxKGq2tVq6XElLhyLRHdXaqrKdiaJVTu27DG0ivpXuMKFX8MJxJX/gdPmQLodKcrzm7E8894wI4XD0ehXJ7p3/8HDRyePu0+ePnv+ond6du20txxmXEttbxl1IEUJMxQo4dZYoIpJuGGr93X+5jNYJ3T5CTcGFormpcgEpxioZe8smYplgtQPkqnbo2WvPxqOdhHfBeMG9EkT0+Vp1ElSzb2CErmkzs0nBhcVtSi4hG038Q4M5SuawzzAkipwi2rX+zZ+E5g0zrQNX4nxjv3/RkWVcxvFQqWiWLjDXE0ey809ZpNFJUrjEUq+fyjzMkYd10LEqbDAUW4CoNyK0GvMC2opxyBX6xWm9Qopc61JqvV+gBaXW2oKwddtVnmJwuovbdZ5xqmpXdgenalFBgtZcEy1xbReQjqorXelVwwspEF2meswTqHO29JXWFgIB4thlgNbKpNnRmp0R1sZNBbUf4Q1tosYC111w9KMD1fkLrg+H44vh5cfL/pXk2Z9Tsgr8pq8JWPyjlyRD2RKZoSTNflKvpHv0Y/oZ/Q7+rMvjTrNnZekFdHffxUDDac=</latexit>⇧⌧, ⌧\n…\n<latexit sha1_base64=\"xnyzZIU4fivXOAdpLodOkhhAiqQ=\">AAADGnicbVJNbxMxEHWWrxK+WjhyWREhcaiibFWVHCtx4VgEaSulUWR7Z3et2GvLHkOiVX4CV7jwa7ghrlz4N3iTPeA0I632+c1YnjdvmJHC4Wj0t5fcuXvv/oODh/1Hj588fXZ49PzSaW85TLiW2l4z6kCKGiYoUMK1sUAVk3DFFu/a/NVnsE7o+hOuDMwULWtRCE4xUB/zeTY/HIyGo02kt0HWgQHp4mJ+lPRucs29ghq5pM5NxwZnDbUouIR1/8Y7MJQvaAnTAGuqwM2aTavr9HVg8rTQNnw1phv2/xsNVc6tFAuVimLldnMtuS839ViMZ42ojUeo+fahwssUddrqTnNhgaNcBUC5FaHXlFfUUo5hOtErTOsFUuYiJc1yKyDiSktNJfgyZpWXKKz+ErPOM05NO/T1Xk0RGRxjwSAVD9N6Cflx67SrvWJgIQ9jl6UOcip1Eo++wcpCOFgMWnZsaUxZGKnR7W3luLOg/SMsMS5iLHTVD0uT7a7IbXB5MszOhmcfTgfn4259DshL8oq8IRl5S87Je3JBJoSTknwl38j35EfyM/mV/N6WJr3uzgsSRfLnHySjBwE=</latexit>d1<latexit sha1_base64=\"zV0WUFO5WCb6hvaku61/NOR1gBw=\">AAADGnicbVJNbxMxEHWWj5bw1cKRy4oIiUMVJVFVcqzUC8ciSFspjSLbO7trxV5b9hgSrfITuMKFX8MNceXCv8Gb7AGnGWm1z2/G8rx5w4wUDgeDv53k3v0HDw8OH3UfP3n67PnR8Ysrp73lMOFaanvDqAMpKpigQAk3xgJVTMI1W1w0+evPYJ3Q1SdcGZgpWlQiF5xioD5m89H8qDfoDzaR3gXDFvRIG5fz46Rzm2nuFVTIJXVuOjY4q6lFwSWsu7fegaF8QQuYBlhRBW5Wb1pdp28Ck6W5tuGrMN2w/9+oqXJupVioVBRLt5tryH25qcd8PKtFZTxCxbcP5V6mqNNGd5oJCxzlKgDKrQi9pryklnIM04leYVovkDIXKamXWwERV1hqSsGXMau8RGH1l5h1nnFqmqGv92qKyOAYCwapeJjWS8hOGqdd5RUDC1kYuyx0kFOqUTz6GksL4WAxaNmxpTZFbqRGt7eVk9aC5o+wxLiIsdBVNyzNcHdF7oKrUX941j/7cNo7H7frc0hekdfkLRmSd+ScvCeXZEI4KchX8o18T34kP5Nfye9tadJp77wkUSR//gEnVwcC</latexit>d2\n<latexit sha1_base64=\"rOvIbMK2YJjgUNjbKhu2Ik76m0A=\">AAADI3icbVJNaxsxEJW3X6n7lbTHXpaaQArB2CGkoadAL70UUqiTgG2MpJ3dFZZWQhq1Nov/Rq/tpb+mt9JLD/0v1dp7qBwPCI3eSOi9N8OMFA4Hgz+d5M7de/cf7D3sPnr85Omz/YPnV057y2HEtdT2hlEHUlQwQoESbowFqpiEazZ/19SvP4N1QlefcGlgqmhRiVxwigGafJjJo/msBsVWr2f7vUF/sI70djJskx5p43J2kHQmmeZeQYVcUufG5wanNbUouIRVd+IdGMrntIBxSCuqwE3rNedVehiQLM21DavCdI3+/6Kmyrll4JUeKoql26414K7a2GN+Pq1FZTxCxTcf5V6mqNPGgDQTFjjKZUgotyJwTXlJLeUYbIp+YVrPkTIXKakXGwERVlhqSsEXMaq8RGH1lxh1nnFqGvdXOzVFYGgdC51SsZnWS8iOm5a7yisGFrJguyx0kFOqk9j6GksL4WAxaNlqS22K3EiNbieV47YFzY6wwPgSY4FVNwzNcHtEbidXJ/3hWf/s42nv4m07PnvkJXlFjsiQvCEX5D25JCPCiSFfyTfyPfmR/Ex+Jb83V5NO++YFiSL5+w/QwwrI</latexit>Ml(kemb)Inference Module\n<latexit sha1_base64=\"IOuRGB0mUT6XDeDrXe2twBi54wU=\">AAADH3icbVJNbxMxEHWWrxK+WjhyWREhcaiipKpKxakSF45BIm2kJIps7+yuFXtt2WNItMqf4AoXfg03xLX/Bm+yB5xmpNU+vxnL8+YNM1I4HAxuO8m9+w8ePjp63H3y9NnzF8cnL6+d9pbDmGup7YRRB1JUMEaBEibGAlVMwg1bfmzyN1/BOqGrL7g2MFe0qEQuOMVATWYjsZgh9Yvj3qA/2EZ6Fwxb0CNtjBYnSWeWae4VVMgldW56aXBeU4uCS9h0Z96BoXxJC5gGWFEFbl5v+92kbwOTpbm24asw3bL/36ipcm6tWKhUFEu3n2vIQ7mpx/xyXovKeISK7x7KvUxRp434NBMWOMp1AJRbEXpNeUkt5RhGFL3CtF4iZS5SUq92AiKusNSUgq9iVnmJwupvMes849Q0k98c1BSRwTYWXFLxMK2XkJ02drvKKwYWsjB2Weggp1Rn8ehrLC2Eg8WgZc+W2hS5kRrdwVZOWwuaP8IK4yLGQlfdsDTD/RW5C67P+sOL/sXn897Vh3Z9jshr8oa8I0PynlyRT2RExoQTSb6TH+Rn8iv5nfxJ/u5Kk0575xWJIrn9B7v+CVs=</latexit>⇧⌧<latexit sha1_base64=\"N0tJg96VWzPmzsKGLdMYnKgJRfE=\">AAADhHicbVLfb9MwEHYaGKP86uCRF4tqohVR1Q4YExJoEy+8TCoS3SY1VWS7bmrVjiP7DK2i/pE88pfwitOGH+l6kuXLd3fx3XcfzaWw0O//DBrhnbsH9w7vNx88fPT4Sevo6ZXVzjA+Ylpqc0OJ5VJkfAQCJL/JDSeKSn5NF5/K+PU3bqzQ2VdY5XyiSJqJmWAEPJS0FpcvE9lZJAVXdB3FQ5HEQFwXf8CX//AufoVj61RSdC6iWCuekijO56Ibiwz/qVn7FJEq8vdn+CLCVTLeZCetdr/X3xi+7Qwqp40qGyZHjSCeauYUz4BJYu34LIdJQQwIJvm6GTvLc8IWJOVj72ZEcTspNqSs8bFHpnimjT8Z4A36f0VBlLUr3yg+VgTmdjdWgvtiYwezs0khstwBz9j2oZmTGDQuGcZTYTgDufIOYUb4XjGbE0MY+D3UXqFaL4BQW5ukWG4HqGGpIZ5BtqyjykkQRn+vo9ZRRvJyveu9M9VArw3qpaDqZBon+TQqNWUzpyg3fOppl6n248zVSZ36AuaG+w8DfpadtRR5OsulBru3lahaQXkDX0I9iVLfVdOLZrArkdvO1UlvcNo7/fKmff6+ks8heo5eoA4aoHfoHH1GQzRCDP1AvwIUBOFBGIWvw7fb1EZQ1TxDNQs//gYGeSfG</latexit>M0l(kemb,⇧⌧)=Ml(kemb)+X(A,!,\u0000)2⇧⌧\u0000(kemb, A,!,\u0000)\n<latexit sha1_base64=\"HJOUhl2MeZGq3bR+mj/990VPS+M=\">AAADHnicbVJNbxMxEHWWrxK+WjhyWREhcaiipEKl6qkSHDgWiTSVklDZ3tldK/bassdtolV+BFe48Gu4Ia7wb/Ame8BpRlrt85uxPG/eMCOFw8Hgbye5c/fe/Qd7D7uPHj95+mz/4PmF095yGHEttb1k1IEUFYxQoIRLY4EqJmHM5u+b/PgarBO6+oxLAzNFi0rkglMM1PjDlylSf3q13xv0B+tIb4NhC3qkjfOrg6QzzTT3Cirkkjo3OTE4q6lFwSWsulPvwFA+pwVMAqyoAjer1+2u0teBydJc2/BVmK7Z/2/UVDm3VCxUKoql28415K7cxGN+MqtFZTxCxTcP5V6mqNNGe5oJCxzlMgDKrQi9pryklnIME4peYVrPkTIXKakXGwERV1hqSsEXMau8RGH1Tcw6zzg1zeBXOzVFZHCNBZNUPEzrJWSHjduu8oqBhSyMXRY6yCnVUTz6GksL4WAxaNmypTZFbqRGt7OVw9aC5o+wwLiIsdBVNyzNcHtFboOLo/7wuH/86W3v7LRdnz3ykrwib8iQvCNn5CM5JyPCyZx8Jd/I9+RH8jP5lfzelCad9s4LEkXy5x/4Bwi5</latexit>D⌧:<latexit sha1_base64=\"cOl1LbTwAJhp+suLlQjdCWOTIu4=\">AAADIXicbVJNbxMxEHWWj5bw1cKRy4oIiUMVJVVVcqzUC8cikbYiiSLbO7trxV+yx5BolX/BFS78Gm6IG+LP4E32wKYZabXPb8byvHnDrBQeB4M/neTe/QcPDw4fdR8/efrs+dHxi2tvguMw5kYad8uoByk0jFGghFvrgCom4YYtLuv8zWdwXhj9EVcWZooWWuSCU4zUp2xe+fkUaVjPj3qD/mAT6V0wbECPNHE1P04608zwoEAjl9T7ycjirKIOBZew7k6DB0v5ghYwiVBTBX5WbTpep28ik6W5cfHTmG7Y/29UVHm/UixWKoql383V5L7cJGA+mlVC24Cg+fahPMgUTVrLTzPhgKNcRUC5E7HXlJfUUY5xSK1XmDELpMy3lFTLrYAWVzhqS8GXbVYFicKZL23WB8aprWe/3qupRUbjWPRJtYfpgoTspDbc66AYOMji2GVhopxSnbZHX2HpIB4cRi07tlS2yK006Pe2ctJYUP8RltguYix21Y1LM9xdkbvg+rQ/PO+ffzjrXYya9Tkkr8hr8pYMyTtyQd6TKzImnGjylXwj35Mfyc/kV/J7W5p0mjsvSSuSv/8AB5kKhg==</latexit>ds⌧\n<latexit sha1_base64=\"jJt7hlzXT841/x9W0WbOhn86aF4=\">AAADHnicbVJNbxMxEHWWrxI+2sKRy4oIiUOJkqoqPVZCSByLRJpKSRTZ3tldK/basseQaJUfwRUu/BpuiCv8G7zJHnCakVb7/GYsz5s3zEjhcDD420nu3L13/8HBw+6jx0+eHh4dP7t22lsOI66ltjeMOpCighEKlHBjLFDFJIzZ4l2TH38G64SuPuHKwEzRohK54BQDNX7zfj5F6udHvUF/sIn0Nhi2oEfauJofJ51pprlXUCGX1LnJhcFZTS0KLmHdnXoHhvIFLWASYEUVuFm9aXedvgpMlubahq/CdMP+f6OmyrmVYqFSUSzdbq4h9+UmHvOLWS0q4xEqvn0o9zJFnTba00xY4ChXAVBuReg15SW1lGOYUPQK03qBlLlISb3cCoi4wlJTCr6MWeUlCqu/xKzzjFPTDH69V1NEBtdYMEnFw7ReQnbSuO0qrxhYyMLYZaGDnFKdxqOvsbQQDhaDlh1balPkRmp0e1s5aS1o/ghLjIsYC111w9IMd1fkNrg+7Q/P++cfz3qXZ+36HJAX5CV5TYbkLbkkH8gVGRFOFuQr+Ua+Jz+Sn8mv5Pe2NOm0d56TKJI//wDXTAio</latexit>\u0000E⌧<latexit sha1_base64=\"jctqXXNR/PAPvhnlAeMFzwvYiJk=\">AAADHnicbVJNaxsxEJW3X6n7kaQ99rLUFAoNxg4hzTFQCj2mUMcB2xhJO7srLK2ENGptFv+IXttLf01vpdf231Rr76FyPLDs05sRmjdvmJHC4WDwt5PcuXvv/oODh91Hj588PTw6fnbttLccRlxLbW8YdSBFBSMUKOHGWKCKSRizxbsmP/4M1gldfcKVgZmiRSVywSkGavzm/XyK1M+PeoP+YBPpbTBsQY+0cTU/TjrTTHOvoEIuqXOTC4OzmloUXMK6O/UODOULWsAkwIoqcLN60+46fRWYLM21DV+F6Yb9/0ZNlXMrxUKloli63VxD7stNPOYXs1pUxiNUfPtQ7mWKOm20p5mwwFGuAqDcitBryktqKccwoegVpvUCKXORknq5FRBxhaWmFHwZs8pLFFZ/iVnnGaemGfx6r6aIDK6xYJKKh2m9hOykcdtVXjGwkIWxy0IHOaU6jUdfY2khHCwGLTu21KbIjdTo9rZy0lrQ/BGWGBcxFrrqhqUZ7q7IbXB92h+e988/nvUuz9r1OSAvyEvymgzJW3JJPpArMiKcLMhX8o18T34kP5Nfye9tadJp7zwnUSR//gHR2Aim</latexit>+E⌧\n<latexit sha1_base64=\"9bnyhIxMhCDcR/Ma3XegD5XxMeo=\">AAADL3icbVJNj9MwEHXD11I+tgsSFy4RFdIiVVG7QkuPK3HhwKFIdHeltqpsx0ms2rFlj6FVyJ/hChd+DeKCuPIvcNoccLcjRRm/Gcvz3huiBbcwHP7qRLdu37l77+h+98HDR4+PeydPLq1yhrIpVUKZa4ItE7xkU+Ag2LU2DEsi2BVZvW3qV5+YsVyVH2Gj2ULivOQZpxg8tOw9m0sMBcWiel8vq/mE16fJIHm17PWHyXAb8c1k1CZ91MZkeRJ15qmiTrISqMDWzsYaFhU2wKlgdXfuLNOYrnDOZj4tsWR2UW3nr+OXHknjTBn/lRBv0f9vVFhau5HEdzbT2v1aAx6qzRxk40XFS+2AlXT3UOZEDCpuxIhTbhgFsfEJpob7WWNaYIMpeMmCV4hSK8DEBkyq9Y5AgOUG64LTdYhKJ4Ab9TlErSMU68aJ+iCnAPQ2Eu+aDMU0TrB00NhvSycJMyz1sotceTqFPAulr6AwzB8MeC57tlQ6z7RQYA+OMmgtaP7A1hA2EeKn6vqlGe2vyM3k8iwZnSfnH173L8bt+hyh5+gFOkUj9AZdoHdogqaIoi/oK/qGvkc/op/R7+jPrjXqtHeeoiCiv/8ATvYOuw==</latexit>L⇧(., .)\n<latexit sha1_base64=\"7SsVXn1k+P0ZUIS173P+mhji+gY=\">AAADLnicbVLPixMxFE7HX2v91VU8eRkswgpLaRdZe1zw4sFDBbu70JaSZDIzockkJC/aEuaP8aoX/xrBg3j1zzDTzsF0+2CYl++9kPd93yNacAvD4a9Ocuv2nbv3ju53Hzx89PhJ7/jppVXOUDalSihzTbBlgldsChwEu9aGYUkEuyKrd0396jMzlqvqE2w0W0hcVDznFEOAlr3nc4mhpFj4D/XSzyeW1yeD18tefzgYbiO9mYzapI/amCyPk848U9RJVgEV2NrZWMPCYwOcClZ3584yjekKF2wW0gpLZhd+O36dvgpIlubKhK+CdIv+f8Njae1GktDZDGv3aw14qDZzkI8XnlfaAavo7qHciRRU2miRZtwwCmITEkwND7OmtMQGUwiKRa8QpVaAiY2Y+PWOQIQVBuuS03WMSieAG/UlRq0jFOvGiPogpwgMLpJgmozFNE6w7LRx31ZOEmZYFmQXhQp0SnkWS++hNCwcDAQue7Z4XeRaKLAHRzltLWj+wNYQNxESpuqGpRntr8jN5PJsMDofnH98078Yt+tzhF6gl+gEjdBbdIHeowmaIoo8+oq+oe/Jj+Rn8jv5s2tNOu2dZyiK5O8/ZXAOyg==</latexit>L (.)Data PreparationFullSignal(T/F)Signal(T/F)RetrainModuleRetrainModuleFalse<latexit sha1_base64=\"UQSqoeE9Ow7UXqnrt0C3m2OnnP4=\">AAADI3icbVJNbxMxEHWWrxK+WjhyWRFV4lBFSYVKxamCC8dWIm2lJES2d3bXir227DEkWuVvcIULv4Yb4tID/wVvsgecZiTL4ze2/N6bYUYKh4PBTSe5c/fe/Qd7D7uPHj95+mz/4Pml095yGHEttb1m1IEUFYxQoIRrY4EqJuGKzT809asvYJ3Q1SdcGpgqWlQiF5xigCbvZzUotvo8Qepn+71Bf7CO9HYybJMeaeN8dpB0JpnmXkGFXFLnxqcGpzW1KLiEVXfiHRjK57SAcUgrqsBN6zXnVXoYkCzNtQ2rwnSN/v+ipsq5ZeCWHiqKpduuNeCu2thjfjqtRWU8QsU3H+VepqjTxoA0ExY4ymVIKLcicE15SS3lGGyKfmFaz5EyFympFxsBEVZYakrBFzGqvERh9dcYdZ5xahr3Vzs1RWBoHQudUrGZ1kvIjpqWu8orBhayYLssdJBTquPY+hpLC+FgMWjZakttitxIjW4nlaO2Bc2OsMD4EmOBVTcMzXB7RG4nl8f94Un/5OJN7+xdOz575CV5RV6TIXlLzshHck5GhBNDvpHv5EfyM/mV/E7+bK4mnfbNCxJF8vcfBuILOg==</latexit>B⌧emb<latexit sha1_base64=\"Wirj7eU4nIu73dHyat5P1/r0QVk=\">AAADHXicbVJNbxMxEHWWrxK+WjhyWREhFSmKkqoqVU+VuHAsEkmD0qiyvbMbE3tt2WOaaJX/wBUu/BpuiCvi3+BN9oDTjLTa5zdjed68YUYKh/3+31Zy5+69+w/2HrYfPX7y9Nn+wfOR095yGHIttR0z6kCKEoYoUMLYWKCKSbhk83d1/vILWCd0+RGXBqaKFqXIBacYqNHhuPvpTft6v9Pv9deR3gaDBnRIExfXB0nrKtPcKyiRS+rc5NTgtKIWBZewal95B4byOS1gEmBJFbhpte52lb4OTJbm2oavxHTN/n+josq5pWKhUlGcue1cTe7KTTzmp9NKlMYjlHzzUO5lijqtpaeZsMBRLgOg3IrQa8pn1FKOYUDRK0zrOVLmIiXVYiMg4gpLzUzwRcwqL1FYfROzzjNOTT331U5NERlMY8EjFQ/TeglZtzbblV4xsJCFsctCBzkzdRSPvsKZhXCwGLRs2VKZIjdSo9vZSrexoP4jLDAuYix0VS/NYHtFboPRUW9w0jv5cNw5P2vWZ4+8JK/IIRmQt+ScvCcXZEg4+Uy+km/ke/Ij+Zn8Sn5vSpNWc+cFiSL58w9Rygdl</latexit>(X, Y)\nFigure 2:Sig2Model Overview.\nSigmaSigmoid adjustments to the base model to perform a final search within the LI’s error bound ±E. Ifknot found\nin theErange, Sig2Model performs an exhaustive search and triggers the retraining signal.\nContributions.The main contributions of this paper are as follows:(1) Index Model Approximation ( Π): We propose\nSig2Model, a novel method that leverages sigmoid functions as weak approximators, similar to boosting techniques,\nto dynamically update the LI model. This approach significantly reduces the need for retraining, offering an efficient\nand adaptive solution.(2) Probabilistic Update Workload Prediction ( Ψ): We use GMM in Sig2Model to predict\nhigh-density regions in the key distribution, allowing strategic placeholder placement and postponing retraining.(3)\nNeural Joint Optimization: We propose two neural networks architecture ( NN Π,NN Ψ) connected via a shared layer\n(NNc) that continuously fine-tunes both ΠandΨin a background process.(4) Comprehensive Experimental Evaluation:\nWe rigorously evaluate the performance of Sig2Model through extensive experiments, and show over 20 ×reduce in\nretraining time and an increase of up to 3×in QPS compared to the state-of-the-art LI solutions.\n2 Index Model Approximation\nIn this section, we present theSigmaSigmoid Boostingapproach for capturing model updates efficiently. We assume\nthat updates originate from an unknown distribution, denoted as Dupdate . The bias introduced by updates, which can\nbe modeled as a step function, is approximated using smooth, differentiable sigmoid functions (see Motivation in\nSection 1). This approximation avoids abrupt changes and delays retraining by gradually adjusting the model.\nFor a model Mat stage τ, if dataset Dτ(with sτkeys) receives an update ubetween keys kjandkj+1, the index\nmodel can be adjusted without full retraining using Dτ∪ {u} . If a single sigmoid fails to capture an update, additional\nsigmoids can be introduced. The combined effect of these sigmoids, termed the SigmaSigmoid function, adjusts the\nmodel and postpones retraining.\nThe resulting SigmaSigmoid-based model,M′\nl(k,Π), is defined as:\nM′\nl(k,Π) =M l(k) +NX\ni=1σ(k, A i, ωi, ϕi)\n| {z }\nAdjustments to capture updates(1)\nΠ ={(A i, ωi, ϕi)}i∈N parameterizes Nsigmoids, where Aicontrols amplitude, ωicontrols step steepness, and ϕi\ncenters the sigmoid around update locations. The system capacity Nmay differ from the number of updates |U|, often\nwith|U| ≥ N . This model raises theoretical questions and defines system limits, discussed in Appendix G. Ideally,\nround(M l(kj+1)) =round(M l(kj)) + 1, requiring a new modelM′\nlsuch that:\n1≤\f\fM′(kj+1,Π)−M′(kj,Π)\f\f≤2. (2)\nIt is important to note that Equation 1 has a trivial solution for Nupdates or less, even without training, by setting\nϕi=uiandAi= 1for every i∈[1 :N] . However, this approach does not fully leverage the maximum capacity of\nSigmaSigmoid modeling.\n2.1 Optimization Objectives\nGiven a dataset Dand update set U(collected from buffer Bof size ρ), we adjust each key’s index based on updates\nx∈U where x < k . The updated model M⋆(·)is obtained by retraining on D∪U . Our goal is to find parameters Π\nforM′(·,Π)that minimize:\nL(Π) =1\n|D∪U|P\nk∈D∪U\f\fM′(k,Π)−M⋆(k)\f\f+γ\nNf(|Π|) (3)\nwheref(·)is monotonically increasing, and|Π| ≤ N. The optimization problem is:\n3\n\nSig2Model: A Boosting-Driven Model for Updatable Learned Indexes\n(a)(b)(c)Shared Layer...Fully ConnectedFully Connected...\n<latexit sha1_base64=\"TSH9ZS8KRPQQxewXGEFPUyZlmd4=\">AAADIHicbVJNbxMxEHWWrxK+WjhyWREhcaiipKpKjpV64VQFibRFSRTZ3tmNFXtt2WOaaJVfwRUu/BpuiCP8GrzJHnCakVb7/GYsz5s3zEjhsNf700ru3X/w8NHB4/aTp8+evzg8ennltLccRlxLbW8YdSBFCSMUKOHGWKCKSbhmi4s6f/0FrBO6/IQrA1NFi1LkglMM1OfLy1k1GTqxnh12et3eJtK7oN+ADmliODtKWpNMc6+gRC6pc+OBwWlFLQouYd2eeAeG8gUtYBxgSRW4abVpeJ2+DUyW5tqGr8R0w/5/o6LKuZVioVJRnLvdXE3uy4095oNpJUrjEUq+fSj3MkWd1urTTFjgKFcBUG5F6DXlc2opxzCj6BWm9QIpc5GSarkVEHGFpWYu+DJmlZcorL6NWecZp6Ye/XqvpogMvrFgk4qHab2E7Lj225VeMbCQhbHLQgc5c3USj77CuYVwsBi07NhSmSI3UqPb28pxY0H9R1hiXMRY6Kodlqa/uyJ3wdVJt3/WPft42jkfNOtzQF6TN+Qd6ZP35Jx8IEMyIpwo8pV8I9+TH8nP5Ffye1uatJo7r0gUyd9/6mEJxA==</latexit>NN \n<latexit sha1_base64=\"TSH9ZS8KRPQQxewXGEFPUyZlmd4=\">AAADIHicbVJNbxMxEHWWrxK+WjhyWREhcaiipKpKjpV64VQFibRFSRTZ3tmNFXtt2WOaaJVfwRUu/BpuiCP8GrzJHnCakVb7/GYsz5s3zEjhsNf700ru3X/w8NHB4/aTp8+evzg8ennltLccRlxLbW8YdSBFCSMUKOHGWKCKSbhmi4s6f/0FrBO6/IQrA1NFi1LkglMM1OfLy1k1GTqxnh12et3eJtK7oN+ADmliODtKWpNMc6+gRC6pc+OBwWlFLQouYd2eeAeG8gUtYBxgSRW4abVpeJ2+DUyW5tqGr8R0w/5/o6LKuZVioVJRnLvdXE3uy4095oNpJUrjEUq+fSj3MkWd1urTTFjgKFcBUG5F6DXlc2opxzCj6BWm9QIpc5GSarkVEHGFpWYu+DJmlZcorL6NWecZp6Ye/XqvpogMvrFgk4qHab2E7Lj225VeMbCQhbHLQgc5c3USj77CuYVwsBi07NhSmSI3UqPb28pxY0H9R1hiXMRY6Kodlqa/uyJ3wdVJt3/WPft42jkfNOtzQF6TN+Qd6ZP35Jx8IEMyIpwo8pV8I9+TH8nP5Ffye1uatJo7r0gUyd9/6mEJxA==</latexit>NN  \n <latexit sha1_base64=\"DKf+JBo9C3cWAEcXIz/+u/nt+X8=\">AAADHXicbVJNbxMxEHWWj5bw1cKRy4oIiUMVJVVVcqzUC8cgkbRSEkW2M7trYq8tewyJVvkPXOHCr+GGuCL+Dd5kDzjNSKt9fjOW580bZqRw2Ov9bSX37j94eHT8qP34ydNnz09OX4yd9pbDiGup7S2jDqQoYYQCJdwaC1QxCTdseV3nbz6DdUKXH3FtYKZoXopMcIqBGk+HTsxxftLpdXvbSO+CfgM6pInh/DRpTReaewUlckmdmwwMzipqUXAJm/bUOzCUL2kOkwBLqsDNqm23m/RNYBZppm34Sky37P83KqqcWysWKhXFwu3navJQbuIxG8wqURqPUPLdQ5mXKeq0lp4uhAWOch0A5VaEXlNeUEs5hgFFrzCtl0iZi5RUq52AiMstNYXgq5hVXqKw+kvMOs84NfXcNwc1RWQwjQWPVDxM6yUszmqzXekVAwuLMHaZ6yCnUOfx6CssLISDxaBlz5bK5JmRGt3BVs4aC+o/wgrjIsZCV+2wNP39FbkLxufd/mX38sNF52rQrM8xeUVek7ekT96RK/KeDMmIcPKJfCXfyPfkR/Iz+ZX83pUmrebOSxJF8ucfZggIhg==</latexit> t...\n<latexit sha1_base64=\"LkC4DuCS9dukgIwT9MHoBY2ehW8=\">AAADGHicbVJNaxsxEJW3H0ndryQ99rLUFHoIxg4l9THQSyGXBOok4JggaWd3haWVkEaNzeJf0Gtzya/prfTaW/9NtfYeKscDyz69GaF584YZKRwOBn87yaPHT57u7D7rPn/x8tXrvf2DC6e95TDmWmp7xagDKSoYo0AJV8YCVUzCJZt9bvKX38A6oauvuDAwVbSoRC44xUCdn97s9Qb9wSrSh2DYgh5p4+xmP+lcZ5p7BRVySZ2bjAxOa2pRcAnL7rV3YCif0QImAVZUgZvWq0aX6fvAZGmubfgqTFfs/zdqqpxbKBYqFcXSbeYacltu4jEfTWtRGY9Q8fVDuZcp6rRRnWbCAke5CIByK0KvKS+ppRzDbKJXmNYzpMxFSur5WkDEFZaaUvB5zCovUVh9G7POM05NM/LlVk0RGfxiwR4VD9N6Cdlh47OrvGJgIQtjl4UOckp1FI++xtJCOFgMWjZsqU2RG6nRbW3lsLWg+SPMMS5iLHTVDUsz3FyRh+DiqD887h+ff+ydjNr12SVvyTvygQzJJ3JCvpAzMiacAPlOfpC75D75mfxKfq9Lk0575w2JIvnzDw5gBkQ=</latexit>K<latexit sha1_base64=\"M0IelVEmDew1HQ3BKC6L7LhsF6g=\">AAADGXicbVJNbxMxEHWWrxK+WjhyWREhcaiipEUlx0pcKnEpiLSV0qiyvbO7Vuy1ZY8h0Sr/gGt76a/hhrhy4t/gTfaA04y02uc3Y3nevGFGCoeDwd9Ocu/+g4ePdh53nzx99vzF7t7LM6e95TDmWmp7wagDKSoYo0AJF8YCVUzCOZt9bPLn38A6oauvuDAwVbSoRC44xUB9Ofx0tdsb9AerSO+CYQt6pI3Tq72kc5lp7hVUyCV1bjIyOK2pRcElLLuX3oGhfEYLmARYUQVuWq86XaZvA5OlubbhqzBdsf/fqKlybqFYqFQUS7eZa8htuYnHfDStRWU8QsXXD+VepqjTRnaaCQsc5SIAyq0Ivaa8pJZyDMOJXmFaz5AyFymp52sBEVdYakrB5zGrvERh9feYdZ5xapqZL7dqishgGAv+qHiY1kvI9hujXeUVAwtZGLssdJBTqoN49DWWFsLBYtCyYUttitxIjW5rK/utBc0fYY5xEWOhq25YmuHmitwFZwf94VH/6PP73vGoXZ8d8pq8Ie/IkHwgx+SEnJIx4SQnP8g1uUluk5/Jr+T3ujTptHdekSiSP/8AvoMGgQ==</latexit>3K...\n<latexit sha1_base64=\"ZDt7StdqzdXtISGloTE+ukCRyXo=\">AAADHHicbVJNj9MwEHXD11K+duHIJaJC4rCqmhVaelyJC8dForsrtVVlO5PEqh1b9hhaRf0NXOHCr+GGuCLxb3DaHHC3I0V5fjOW580bZqRwOBr97SV37t67/+DoYf/R4ydPnx2fPL9y2lsOE66ltjeMOpCihgkKlHBjLFDFJFyz5fs2f/0ZrBO6/oRrA3NFy1oUglMM1GSm/CJbHA9Gw9E20tsg68CAdHG5OEl6s1xzr6BGLqlz07HBeUMtCi5h0595B4byJS1hGmBNFbh5s212k74OTJ4W2oavxnTL/n+jocq5tWKhUlGs3H6uJQ/lph6L8bwRtfEINd89VHiZok5b5WkuLHCU6wAotyL0mvKKWsoxzCd6hWm9RMpcpKRZ7QREXGmpqQRfxazyEoXVX2LWecapace+OagpIoNnLFik4mFaLyE/bb12tVcMLORh7LLUQU6lzuLRN1hZCAeLQcueLY0pCyM1uoOtnHYWtH+EFcZFjIWu+mFpsv0VuQ2uzobZ+fD849vBxbhbnyPykrwib0hG3pEL8oFckgnhRJCv5Bv5nvxIfia/kt+70qTX3XlBokj+/APBFAfv</latexit>µ1\n<latexit sha1_base64=\"ZKqSm1CtZG6wq2+Ug6omPc1r38U=\">AAADHHicbVJNj9MwEHXD11K+duHIJaJC4rCqmhVaelyJC8dForsrtVVlO5PEqh1b9hhaRf0NXOHCr+GGuCLxb3DaHHC3I0V5fjOW580bZqRwOBr97SV37t67/+DoYf/R4ydPnx2fPL9y2lsOE66ltjeMOpCihgkKlHBjLFDFJFyz5fs2f/0ZrBO6/oRrA3NFy1oUglMM1GRmxCJbHA9Gw9E20tsg68CAdHG5OEl6s1xzr6BGLqlz07HBeUMtCi5h0595B4byJS1hGmBNFbh5s212k74OTJ4W2oavxnTL/n+jocq5tWKhUlGs3H6uJQ/lph6L8bwRtfEINd89VHiZok5b5WkuLHCU6wAotyL0mvKKWsoxzCd6hWm9RMpcpKRZ7QREXGmpqQRfxazyEoXVX2LWecapace+OagpIoNnLFik4mFaLyE/bb12tVcMLORh7LLUQU6lzuLRN1hZCAeLQcueLY0pCyM1uoOtnHYWtH+EFcZFjIWu+mFpsv0VuQ2uzobZ+fD849vBxbhbnyPykrwib0hG3pEL8oFckgnhRJCv5Bv5nvxIfia/kt+70qTX3XlBokj+/AOosQfm</latexit>⇡1\n<latexit sha1_base64=\"rhF0DS3KT6iSYqQc3A27OOFylsw=\">AAADH3icbVJNbxMxEHWWrxK+WjhyWREhcaiipKpKjpW4cCyCtJGSKLK9s7tW7LVljyHRKn+CK1z4NdwQ1/4bvMkecJqRVvv8ZizPmzfMSOFwMLjtJPfuP3j46Ohx98nTZ89fHJ+8vHbaWw5jrqW2E0YdSFHBGAVKmBgLVDEJN2z5ocnffAXrhK6+4NrAXNGiErngFAM1mX0WhaKL4eK4N+gPtpHeBcMW9EgbV4uTpDPLNPcKKuSSOjcdGZzX1KLgEjbdmXdgKF/SAqYBVlSBm9fbfjfp28Bkaa5t+CpMt+z/N2qqnFsrFioVxdLt5xryUG7qMR/Na1EZj1Dx3UO5lynqtBGfZsICR7kOgHIrQq8pL6mlHMOIoleY1kukzEVK6tVOQMQVlppS8FXMKi9RWP0tZp1nnJpm8puDmiIy2MaCSyoepvUSstPGbld5xcBCFsYuCx3klOosHn2NpYVwsBi07NlSmyI3UqM72Mppa0HzR1hhXMRY6Koblma4vyJ3wfVZf3jRv/h03rsctetzRF6TN+QdGZL35JJ8JFdkTDiR5Dv5QX4mv5LfyZ/k76406bR3XpEoktt/Fr0JHA==</latexit>⌃1\n<latexit sha1_base64=\"EggJ0r/BhNCsXNFvsShCgcFnQ7s=\">AAADHHicbVJNaxsxEJW3H0ndr6Q99rLUFHoIxg4h9TGQS6GXFOokYBsjaWd3haWVkEatzeLf0Gt76a/prfRa6L+p1t5D5Xhg2ac3IzRv3jAjhcPB4G8nuXf/wcODw0fdx0+ePnt+dPzi2mlvOYy5ltreMupAigrGKFDCrbFAFZNwwxaXTf7mM1gndPUJVwZmihaVyAWnGKjx1Ij5h/lRb9AfbCK9C4Yt6JE2rubHSWeaae4VVMgldW4yMjirqUXBJay7U+/AUL6gBUwCrKgCN6s3za7TN4HJ0lzb8FWYbtj/b9RUObdSLFQqiqXbzTXkvtzEYz6a1aIyHqHi24dyL1PUaaM8zYQFjnIVAOVWhF5TXlJLOYb5RK8wrRdImYuU1MutgIgrLDWl4MuYVV6isPpLzDrPODXN2Nd7NUVk8IwFi1Q8TOslZCeN167yioGFLIxdFjrIKdVpPPoaSwvhYDFo2bGlNkVupEa3t5WT1oLmj7DEuIix0FU3LM1wd0XuguvT/vC8f/7xrHcxatfnkLwir8lbMiTvyAV5T67ImHAiyFfyjXxPfiQ/k1/J721p0mnvvCRRJH/+Ae75CAA=</latexit>⇡K\n<latexit sha1_base64=\"JbE5ZH1rZqaAdTTzYdUIO9QWqmA=\">AAADHHicbVJNaxsxEJW3H0ndr6Q99rLUFHoIxg4h9TGQS6GXFOokYBsjaWd3haWVkEatzeLf0Gt76a/prfRa6L+p1t5D5Xhg2ac3IzRv3jAjhcPB4G8nuXf/wcODw0fdx0+ePnt+dPzi2mlvOYy5ltreMupAigrGKFDCrbFAFZNwwxaXTf7mM1gndPUJVwZmihaVyAWnGKjxVPn5h/lRb9AfbCK9C4Yt6JE2rubHSWeaae4VVMgldW4yMjirqUXBJay7U+/AUL6gBUwCrKgCN6s3za7TN4HJ0lzb8FWYbtj/b9RUObdSLFQqiqXbzTXkvtzEYz6a1aIyHqHi24dyL1PUaaM8zYQFjnIVAOVWhF5TXlJLOYb5RK8wrRdImYuU1MutgIgrLDWl4MuYVV6isPpLzDrPODXN2Nd7NUVk8IwFi1Q8TOslZCeN167yioGFLIxdFjrIKdVpPPoaSwvhYDFo2bGlNkVupEa3t5WT1oLmj7DEuIix0FU3LM1wd0XuguvT/vC8f/7xrHcxatfnkLwir8lbMiTvyAV5T67ImHAiyFfyjXxPfiQ/k1/J721p0mnvvCRRJH/+AQdrCAk=</latexit>µK\n<latexit sha1_base64=\"uqLTJLhUvdIu57gQwi3OiLG2iow=\">AAADH3icbVJNj9MwEHXD11K+duHIJaJC4rCq2hVaelyJCxKXRdDdSm1V2c4ksWrHlj2GVlH/BFe48Gu4Ia77b3DaHHC3I0V5fjOW580bZqRwOBjcdJI7d+/df3D0sPvo8ZOnz45Pnl857S2HMddS2wmjDqSoYIwCJUyMBaqYhGu2fN/kr7+CdUJXX3BtYK5oUYlccIqBmsw+i0LRxcfFcW/QH2wjvQ2GLeiRNi4XJ0lnlmnuFVTIJXVuOjI4r6lFwSVsujPvwFC+pAVMA6yoAjevt/1u0teBydJc2/BVmG7Z/2/UVDm3VixUKoql28815KHc1GM+mteiMh6h4ruHci9T1GkjPs2EBY5yHQDlVoReU15SSzmGEUWvMK2XSJmLlNSrnYCIKyw1peCrmFVeorD6W8w6zzg1zeQ3BzVFZLCNBZdUPEzrJWSnjd2u8oqBhSyMXRY6yCnVWTz6GksL4WAxaNmzpTZFbqRGd7CV09aC5o+wwriIsdBVNyzNcH9FboOrs/7wvH/+6W3vYtSuzxF5SV6RN2RI3pEL8oFckjHhRJLv5Af5mfxKfid/kr+70qTT3nlBokhu/gFdBQk2</latexit>⌃K...Fully Connected...<latexit sha1_base64=\"t5sLxHQK6dM2GNYExUg5+Dwf89g=\">AAADGnicbVJNj9MwEHXD11K+duHIJaJC4rCqmhVaelzEheMi6O5K3aqynUli1Y4tewytov4ErnDh13BDXLnwb3DaHHC3I0V5fjOW580bZqRwOBr97SW3bt+5e+/gfv/Bw0ePnxwePb1w2lsOE66ltleMOpCihgkKlHBlLFDFJFyyxbs2f/kZrBO6/oQrAzNFy1oUglMM1Me382x+OBgNR5tIb4KsAwPSxfn8KOld55p7BTVySZ2bjg3OGmpRcAnr/rV3YChf0BKmAdZUgZs1m1bX6cvA5GmhbfhqTDfs/zcaqpxbKRYqFcXK7eZacl9u6rEYzxpRG49Q8+1DhZcp6rTVnebCAke5CoByK0KvKa+opRzDdKJXmNYLpMxFSprlVkDElZaaSvBlzCovUVj9JWadZ5yadujrvZoiMjjGgkEqHqb1EvLj1mlXe8XAQh7GLksd5FTqJB59g5WFcLAYtOzY0piyMFKj29vKcWdB+0dYYlzEWOiqH5Ym212Rm+DiZJidDk8/vB6cjbv1OSDPyQvyimTkDTkj78k5mRBOSvKVfCPfkx/Jz+RX8ntbmvS6O89IFMmff8WyBt4=</latexit>A1\n<latexit sha1_base64=\"uH5XoANvWZ0LuC9rqJWgdGDKx20=\">AAADH3icbVJNaxsxEJW3X6n7lbTHXpaaQg/B2KGkPgZ66TGBOjHYxkja2V1haSWkUWuz+E/02l76a3oruebfVGvvoXI8sOzTmxGaN2+YkcLhYHDXSR48fPT4ydHT7rPnL16+Oj55fe20txzGXEttJ4w6kKKCMQqUMDEWqGISbtjyc5O/+QbWCV19xbWBuaJFJXLBKQZqMtMKCroYLo57g/5gG+l9MGxBj7RxuThJOrNMc6+gQi6pc9ORwXlNLQouYdOdeQeG8iUtYBpgRRW4eb3td5O+D0yW5tqGr8J0y/5/o6bKubVioVJRLN1+riEP5aYe89G8FpXxCBXfPZR7maJOG/FpJixwlOsAKLci9JryklrKMYwoeoVpvUTKXKSkXu0ERFxhqSkFX8Ws8hKF1d9j1nnGqWkmvzmoKSKDbSy4pOJhWi8hO23sdpVXDCxkYeyy0EFOqc7i0ddYWggHi0HLni21KXIjNbqDrZy2FjR/hBXGRYyFrrphaYb7K3IfXJ/1h+f986uPvYtRuz5H5C15Rz6QIflELsgXcknGhBNJfpCf5FfyO/mT/E1ud6VJp73zhkSR3P0DWD8JNA==</latexit>!1\n<latexit sha1_base64=\"fWuRTbCSlxWtxm2bmOrtGxpOoe0=\">AAADHXicbVJNj9MwEHXD11K+duHIJaJC4rCq2tVq6XElLhwXiXZXaqvKdiaJqR1b9hhaRf0PXOHCr+GGuCL+DU6bA+52pCjjN2N53nvDjBQOB4O/neTO3Xv3Hxw97D56/OTps+OT5xOnveUw5lpqe8OoAykqGKNACTfGAlVMwjVbvmvq15/BOqGrj7g2MFe0qEQuOMUATWamFIvh4rg36A+2kd5Ohm3SI21cLU6SzizT3CuokEvq3HRkcF5Ti4JL2HRn3oGhfEkLmIa0ogrcvN5Ou0lfByRLc23DV2G6Rf+/UVPl3Fqx0Kkolm6/1oCHalOP+Whei8p4hIrvHsq9TFGnDfU0ExY4ynVIKLcizJryklrKMQgUvcK0XiJlLmJSr3YEIqywNEjIVzGqvERh9ZcYdZ5xahrdNwc5RWAwjQWPVCym9RKy08ZsV3nFwEIWZJeFDnRKdRZLX2NpIRwsBi57ttSmyI3U6A6Octpa0PwRVhg3MRam6oalGe6vyO1kctYfXvQvPpz3Lkft+hyRl+QVeUOG5C25JO/JFRkTTj6Rr+Qb+Z78SH4mv5Lfu9ak0955QaJI/vwD6gAIWA==</latexit>\u00001...\n<latexit sha1_base64=\"AxecuOAKPL00fkbKmbvDFZzB59w=\">AAADH3icbVJNaxsxEJW3H0ndr6Q99rLUFHoIxg4l9THQS0/BhTox2MZI2tm1sLQS0qixWfwnem0v/TW9lV7zb6q191A5Hlj26c0IzZs3zEjhsNe7ayUPHj56fHT8pP302fMXL09OX1077S2HEddS2zGjDqQoYYQCJYyNBaqYhBu2/FTnb76BdUKXX3FtYKZoUYpccIqBGl9dzavpUGzmJ51et7eN9D7oN6BDmhjOT5PWNNPcKyiRS+rcZGBwVlGLgkvYtKfegaF8SQuYBFhSBW5WbfvdpO8Ck6W5tuErMd2y/9+oqHJurVioVBQXbj9Xk4dyE4/5YFaJ0niEku8eyr1MUae1+DQTFjjKdQCUWxF6TfmCWsoxjCh6hWm9RMpcpKRa7QREXGGpWQi+ilnlJQqrb2PWecapqSe/OagpIoNtLLik4mFaLyE7q+12pVcMLGRh7LLQQc5Cncejr3BhIRwsBi17tlSmyI3U6A62ctZYUP8RVhgXMRa6aoel6e+vyH1wfd7tX3QvvnzoXA6a9Tkmb8hb8p70yUdyST6TIRkRTiT5Tn6Qn8mv5HfyJ/m7K01azZ3XJIrk7h+KTAlH</latexit>NN⇧\n<latexit sha1_base64=\"AxecuOAKPL00fkbKmbvDFZzB59w=\">AAADH3icbVJNaxsxEJW3H0ndr6Q99rLUFHoIxg4l9THQS0/BhTox2MZI2tm1sLQS0qixWfwnem0v/TW9lV7zb6q191A5Hlj26c0IzZs3zEjhsNe7ayUPHj56fHT8pP302fMXL09OX1077S2HEddS2zGjDqQoYYQCJYyNBaqYhBu2/FTnb76BdUKXX3FtYKZoUYpccIqBGl9dzavpUGzmJ51et7eN9D7oN6BDmhjOT5PWNNPcKyiRS+rcZGBwVlGLgkvYtKfegaF8SQuYBFhSBW5WbfvdpO8Ck6W5tuErMd2y/9+oqHJurVioVBQXbj9Xk4dyE4/5YFaJ0niEku8eyr1MUae1+DQTFjjKdQCUWxF6TfmCWsoxjCh6hWm9RMpcpKRa7QREXGGpWQi+ilnlJQqrb2PWecapqSe/OagpIoNtLLik4mFaLyE7q+12pVcMLGRh7LLQQc5Cncejr3BhIRwsBi17tlSmyI3U6A62ctZYUP8RVhgXMRa6aoel6e+vyH1wfd7tX3QvvnzoXA6a9Tkmb8hb8p70yUdyST6TIRkRTiT5Tn6Qn8mv5HfyJ/m7K01azZ3XJIrk7h+KTAlH</latexit>NN⇧Shared LayerFully Connected\n ...<latexit sha1_base64=\"cUMeJLMAlWlEgj2dDiaSzfx4G8A=\">AAADHHicbVJNbxMxEHWWj5bw1cKRy4oIiUMVJRUqOVbiwjFIpK2URJHtnd21Yq8te0wTrfIbuMKFX8MNcUXi3+BN9oDTjLTa5zdjed68YUYKh4PB305y7/6Dh0fHj7qPnzx99vzk9MWV095ymHAttb1h1IEUFUxQoIQbY4EqJuGaLT80+esvYJ3Q1WdcG5grWlQiF5xioCazsVjg4qQ36A+2kd4Fwxb0SBvjxWnSmWWaewUVckmdm44MzmtqUXAJm+7MOzCUL2kB0wArqsDN622zm/RNYLI01zZ8FaZb9v8bNVXOrRULlYpi6fZzDXkoN/WYj+a1qIxHqPjuodzLFHXaKE8zYYGjXAdAuRWh15SX1FKOYT7RK0zrJVLmIiX1aicg4gpLTSn4KmaVlyisvo1Z5xmnphn75qCmiAyesWCRiodpvYTsrPHaVV4xsJCFsctCBzmlOo9HX2NpIRwsBi17ttSmyI3U6A62ctZa0PwRVhgXMRa66oalGe6vyF1wdd4fXvQvPr3rXY7a9Tkmr8hr8pYMyXtyST6SMZkQTgT5Sr6R78mP5GfyK/m9K0067Z2XJIrkzz8G/AgJ</latexit>⇧t\n<latexit sha1_base64=\"pMc1D2ilH8dqhxeD45CWe88qtjw=\">AAADInicbVJNaxsxEJU3/UjdryQ99rLUFHoIxg4h9THQS08lhToJ2CZI2tldYWklpFFis/hn9Npe+mt6Kz0V+mOqtfdQOR5Y9unNCM2bN8xI4XAw+NNJ9h48fPR4/0n36bPnL14eHB5dOu0thzHXUttrRh1IUcEYBUq4NhaoYhKu2PxDk7+6BeuErr7g0sBM0aISueAUAzWZKoolp7L+tLo56A36g3Wk98GwBT3SxsXNYdKZZpp7BRVySZ2bjAzOampRcAmr7tQ7MJTPaQGTACuqwM3qdcur9G1gsjTXNnwVpmv2/xs1Vc4tFQuVTYtuO9eQu3ITj/loVovKeISKbx7KvUxRp43+NBMWOMplAJRbEXpNeUkt5RimFL3CtJ4jZS5SUi82AiKusNSUgi9iVnmJwuq7mHWecWqa4a92aorI4BwLRql4mNZLyI4bx13lFQMLWRi7LHSQU6qTePQ1lhbCwWLQsmVLbYrcSI1uZyvHrQXNH2GBcRFjoatuWJrh9orcB5cn/eFZ/+zzae981K7PPnlN3pB3ZEjek3PykVyQMeFEk6/kG/me/Eh+Jr+S35vSpNPeeUWiSP7+A/a2Ctk=</latexit>N<latexit sha1_base64=\"/utZYs6cGhWspGlIzplnPJQxJqY=\">AAADI3icbVJNbxMxEHW2fJTw1ZYjlxUREocqSgoqOVbi0hMqEmkrJVFle2d3rdhryx5DolX+Bld64ddwQ1w48F/wJnvAaUZa7fObsTzz3jAjhcPB4E8n2bt3/8HD/Ufdx0+ePnt+cHh06bS3HMZcS22vGXUgRQVjFCjh2ligikm4YvMPTf7qC1gndPUZlwZmihaVyAWnGKjp26miWHIq64+rm4PeoD9YR3oXDFvQI21c3BwmnWmmuVdQIZfUucnI4KymFgWXsOpOvQND+ZwWMAmwogrcrF73vEpfByZLc23DV2G6Zv+/UVPl3FKxUNm06LZzDbkrN/GYj2a1qIxHqPjmodzLFHXaCJBmwgJHuQyAcitCrykvqaUcg0zRK0zrOVLmoknqxWaAiCssNaXgi5hVXqKw+mvMOs84NY36q50zRWSwjgWnVCym9RKy48ZyV3nFwEIWZJeFDuOU6iSWvsbSQjhYDLNs2VKbIjdSo9vZynFrQfNHWGBcxFjoqhuWZri9InfB5Ul/eNo//fSudzZq12efvCSvyBsyJO/JGTknF2RMODHkG/lObpMfyc/kV/J7U5p02jsvSBTJ33+o+gsW</latexit>3N<latexit sha1_base64=\"3DQz2n/MeeBRsEYgwMXw3q4FF+c=\">AAADKHicbVJNbxMxEHWWj5bw0RSOXFZESByqKKmqkmMRF06oSKStlESR7Z3dtWKvV/aYJlrtL+EKF34NN9QrvwRvsgecZqTVjt+M5XnvDSulsDgc3nWiBw8fPT44fNJ9+uz5i6Pe8csrq53hMOFaanPDqAUpCpigQAk3pQGqmIRrtvzY1K+/gbFCF19xXcJc0awQqeAUPbToHX1YVDNFMedUVp/retHrDwfDTcT3k1Gb9Ekbl4vjqDNLNHcKCuSSWjsdlzivqEHBJdTdmbNQUr6kGUx9WlAFdl5tBq/jtx5J4lQb/xUYb9D/b1RUWbtWzHc2M9rdWgPuq00dpuN5JYrSIRR8+1DqZIw6blSIE2GAo1z7hHIj/Kwxz6mhHL1WwStM6yVSZgMm1WpLIMAyQ8tc8FWIKidRGH0botYxTsvGgnovpwD0/jFvlwrFNE5CctL4bgunGBhIvOwy055Ork5D6SvMDfiDQc9lx5aqzNJSarR7RzlpLWj+CCsMmxjzU3X90ox2V+R+cnU6GJ0Pzr+c9S/G7focktfkDXlHRuQ9uSCfyCWZEE4c+U5+kJ/Rr+h39Ce627ZGnfbOKxJE9PcfjcIMyg==</latexit>AN\n<latexit sha1_base64=\"CnGdQBNDTHHSKXwWRrrc7+sZRww=\">AAADLXicbVJNj9MwEHXD11K+usCNS0SFxGFVtavV0uNKXDihRaK7K7VVZTuTxKodW/YEWqL8F65w4ddwQFrtlb+B0+aAux0pyvjNWJ733jAjhcPh8E8nunP33v0HBw+7jx4/efqsd/j8wunScphwLbW9YtSBFAVMUKCEK2OBKibhki3fN/XLL2Cd0MVnXBuYK5oVIhWcoocWvZczrSCji2qmKOacyupjXS96/eFguIn4djJqkz5p43xxGHVmiealggK5pM5NxwbnFbUouIS6OysdGMqXNIOpTwuqwM2rzfR1/MYjSZxq678C4w36/42KKufWivnOZka3W2vAfbVpiel4XonClAgF3z6UljJGHTdSxImwwFGufUK5FX7WmOfUUo5esOAVpvUSKXMBk2q1JRBgmaUmF3wVoqqUKKz+GqKuZJyaxod6L6cA9CYy75kKxbSlhOSoMd8VpWJgIfGyy0x7Ork6DqWvMLfgDxY9lx1bKpOlRmp0e0c5ai1o/ggrDJsY81N1/dKMdlfkdnJxPBidDk4/nfTPxu36HJBX5DV5S0bkHTkjH8g5mRBOvpHv5Af5Gf2KfkfX0c22Neq0d16QIKK//wA7Eg8g</latexit>!N\n<latexit sha1_base64=\"hG4XJkP84bZxhhsXDoqaVLtirAE=\">AAADK3icbVJNj9MwEHXD11K+usuRS0SFxGFVtSu09LgSF05okejuSm1V2c4ktWrHlj2GVlH+Cle48Gs4gbjyP3DaHHC3I0V5fjOWZ94bZqRwOBz+6iR37t67/+DoYffR4ydPn/WOT66c9pbDhGup7Q2jDqQoYYICJdwYC1QxCdds9a7JX38G64QuP+HGwFzRohS54BQDteidzMxSLKqZorjkVFYf6nrR6w8Hw22kt8GoBX3SxuXiOOnMMs29ghK5pM5NxwbnFbUouIS6O/MODOUrWsA0wJIqcPNq23udvgpMlubahq/EdMv+f6OiyrmNYqGy6dHt5xryUG7qMR/PK1Eaj1Dy3UO5lynqtBEizYQFjnITAOVWhF5TvqSWcgxyRa8wrVdImYsmqda7ASKusDTIydcxq7xEYfWXmHWecWoaF+qDM0VksJAFx1QspvUSstPGeld6xcBCFmSXhQ7jLNVZLH2FSwvhYDHMsmdLZYrcSI3uYCunrQXNH2GNcRFjoatuWJrR/orcBldng9H54Pzjm/7FuF2fI/KCvCSvyYi8JRfkPbkkE8LJmnwl38j35EfyM/md/NmVJp32znMSRfL3H8MFDkQ=</latexit>\u0000N\nEmbedding Model<latexit sha1_base64=\"TSH9ZS8KRPQQxewXGEFPUyZlmd4=\">AAADIHicbVJNbxMxEHWWrxK+WjhyWREhcaiipKpKjpV64VQFibRFSRTZ3tmNFXtt2WOaaJVfwRUu/BpuiCP8GrzJHnCakVb7/GYsz5s3zEjhsNf700ru3X/w8NHB4/aTp8+evzg8ennltLccRlxLbW8YdSBFCSMUKOHGWKCKSbhmi4s6f/0FrBO6/IQrA1NFi1LkglMM1OfLy1k1GTqxnh12et3eJtK7oN+ADmliODtKWpNMc6+gRC6pc+OBwWlFLQouYd2eeAeG8gUtYBxgSRW4abVpeJ2+DUyW5tqGr8R0w/5/o6LKuZVioVJRnLvdXE3uy4095oNpJUrjEUq+fSj3MkWd1urTTFjgKFcBUG5F6DXlc2opxzCj6BWm9QIpc5GSarkVEHGFpWYu+DJmlZcorL6NWecZp6Ye/XqvpogMvrFgk4qHab2E7Lj225VeMbCQhbHLQgc5c3USj77CuYVwsBi07NhSmSI3UqPb28pxY0H9R1hiXMRY6Kodlqa/uyJ3wdVJt3/WPft42jkfNOtzQF6TN+Qd6ZP35Jx8IEMyIpwo8pV8I9+TH8nP5Ffye1uatJo7r0gUyd9/6mEJxA==</latexit>NN \n Highway Neural Network(2 layers)Nonlinear Activation(ReLu)Neural Network(3 layers)Dropout Layer\nFully ConnectedReplay Buﬀer\nEWC Regularization<latexit sha1_base64=\"AxecuOAKPL00fkbKmbvDFZzB59w=\">AAADH3icbVJNaxsxEJW3H0ndr6Q99rLUFHoIxg4l9THQS0/BhTox2MZI2tm1sLQS0qixWfwnem0v/TW9lV7zb6q191A5Hlj26c0IzZs3zEjhsNe7ayUPHj56fHT8pP302fMXL09OX1077S2HEddS2zGjDqQoYYQCJYyNBaqYhBu2/FTnb76BdUKXX3FtYKZoUYpccIqBGl9dzavpUGzmJ51et7eN9D7oN6BDmhjOT5PWNNPcKyiRS+rcZGBwVlGLgkvYtKfegaF8SQuYBFhSBW5WbfvdpO8Ck6W5tuErMd2y/9+oqHJurVioVBQXbj9Xk4dyE4/5YFaJ0niEku8eyr1MUae1+DQTFjjKdQCUWxF6TfmCWsoxjCh6hWm9RMpcpKRa7QREXGGpWQi+ilnlJQqrb2PWecapqSe/OagpIoNtLLik4mFaLyE7q+12pVcMLGRh7LLQQc5Cncejr3BhIRwsBi17tlSmyI3U6A62ctZYUP8RVhgXMRa6aoel6e+vyH1wfd7tX3QvvnzoXA6a9Tkmb8hb8p70yUdyST6TIRkRTiT5Tn6Qn8mv5HfyJ/m7K01azZ3XJIrk7h+KTAlH</latexit>NN⇧\nFigure 3:Neural Networks Architectures. The multi-layer neural network NNC(c) processes sequential data\nbatches for continuous learning. It employs highway networks, ReLU activations, and dropout layers to extract\npatterns and prevent overfitting. To address catastrophic forgetting [ 35], two strategies are used:Replay\nBuffer[ 13] andElastic Weight Consolidation[ 39]. The network outputs are fed into two subnets, NN Π(a) and\nNN Ψ(b), each containing specific tasks. We explainNN Carchitecture in Appendix D.\narg min\nM′(·,Π)∈ML(Π)\ns.t.E u∼D update\u0002\n|M′(u,Π)−M⋆(u)|\u0003\n≤α,\nP\u0002\n|M′(k,Π)−M′(u,Π)|< E Π\u0003\n≤β∀u∼ D update , k∈D∪U.(4)\nSolving Equation 4 is challenging for arbitrary hypothesis spaces M. We use sigmoids as base models, transforming the\noriginal index function M. In this equation, αbounds prediction bias, and βspecifies the allowable error level. When\nβ= 0 , the problem becomes NP-Hard, requiring a complete search in Mor even infeasible. EΠpresent the measure of\nconfusion in LI models’ prediction can be viewed as the variance of the index estimator for incoming u∼ D update . An\nunbiased estimator ( α= 0 ) is preferred over high variance, as variance only expands the last-mile search range. In\naddition, the target key in the lower variance has a higher likelihood of presence in the center of the predicted range.\nTo ensure a non-empty feasible solution space for the optimization problem described in Equation 4, it is necessary\nto show that maintaining unbiasedness ( α≈0 ) leads to increased variance, thereby bias-variance analysis indicates\nthatEΠ∝1\nα. Additionally, the second constraint in Equation 4 accounts for the most probable incoming updates to\noptimize intervals effectively, and is the relaxed derived as a modification of Equation 2, as shown in Theorem 1 (proof\nin Appendeix F):\nTheorem 1 IfDupdate is bounded, Equation 2 satisfies I P\u0002\n|M′(k,Π)−M′(u,Π)|< E Π\u0003\n≤β for all u∼\nDupdate , k∈D∪U.\n2.2 Neural Unit for Fine-Tuning\nThe parameter set Π ={(A i, ωi, ϕi)}i∈N, where Nis a hyperparameter, requires dynamic fine-tuning to adjust the LI\nmodel M. We implement this through a neural network NN Πwith two fully-connected input networks (having Nand\n3Nparameters respectively) connected to a shared layerNN Cas shown in Figure 2.\nTo minimize last-mile search errors, NN Πis optimized to near-overfit conditions using mean squared error (MSE) as\nthe loss function:MSE(Xτ, Yτ) =1\n|X|P\n(Xτ\ni,Yτ\ni)∈(Xτ,Yτ)(ˆIi−Yi)2, where ˆIiis computed via Equation 1.\nThe cost function incorporates a regularization term to minimize sigmoid usage for new buffer entriesBτ:\nLΠ(Xτ, Yτ) =MSE(Xτ, Yτ) +γ\nNPN\nj=1ρAj\ndexp\u0010\n1−Aj\nd\u0011\n(5)\nwhereρis the buffer size,dis the normalization constant, andγis an experimentally-tuned hyperparameter.\nFrom Equation 3, we derive |Π| ≈PI(A,.,.)∈Π,A̸=0 andf(x) =x\ndexp(1−x\nd)(which is monotonic). This regularization\ndesign preferentially penalizes smaller amplitudes ( Aj), pushing them toward zero while allowing larger amplitudes\nto cover more examples, consistent with the monotonicity of the index function. The optimization process solves\nEquation 4 using the prepared data from Section 4.1.\n3 Update Workload Training\nIn this section, we introduce Sig2Model’supdate workload training, which is calledNullifiercomponent. Sig2Model\ntrains a probabilistic model on the incoming update distribution, and whenever a retraining signal is raised, it uses this\ntrained distribution to put the update placeholder to improve system performance.\n4\n\nSig2Model: A Boosting-Driven Model for Updatable Learned Indexes\n<latexit sha1_base64=\"GKd+8FvckoQjuNquKZJEy/xv99A=\">AAADH3icbVJNbxMxEHWWrxK+WjhyWREhcaiiTYVKjxVcOBaJtJGSUNne2V0r9tqyx5BolT/BFS78Gm6Ia/8N3mQPOM1Iq31+M5bnzRtmpHCYZTe95M7de/cfHDzsP3r85Omzw6Pnl057y2HMtdR2wqgDKWoYo0AJE2OBKibhii0+tPmrr2Cd0PVnXBmYK1rWohCcYqAm7780M6R+fX04yIbZJtLbYNSBAeni4voo6c1yzb2CGrmkzk3PDM4balFwCev+zDswlC9oCdMAa6rAzZtNv+v0dWDytNA2fDWmG/b/Gw1Vzq0UC5WKYuV2cy25Lzf1WJzNG1Ebj1Dz7UOFlynqtBWf5sICR7kKgHIrQq8pr6ilHMOIoleY1gukzEVKmuVWQMSVlppK8GXMKi9RWP0tZp1nnJp28uu9miIy2MaCSyoepvUS8uPWbld7xcBCHsYuSx3kVOokHn2DlYVwsBi07NjSmLIwUqPb28pxZ0H7R1hiXMRY6Koflma0uyK3weXJcHQ6PP30dnCedetzQF6SV+QNGZF35Jx8JBdkTDiR5Dv5QX4mv5LfyZ/k77Y06XV3XpAokpt/Gn0JdQ==</latexit>B⌧<latexit sha1_base64=\"xbilWRWbXJtAXWKJug0dbufyX80=\">AAADK3icbVJNaxsxEJW3X6n75aTHXpaaQgrG2KGkOQZ6aC8BF+okYBsjaWd3haWVkEaJzeK/0mt76a/pqaXX/o9q7T10HQ8s+/RmhGbeG2akcDgY/GpF9+4/ePjo4HH7ydNnz190Do8unfaWw5hrqe01ow6kKGCMAiVcGwtUMQlXbPGhyl/dgHVCF19wZWCmaFaIVHCKgZp3jj5eXBwvetORE/NyitSv38473UF/sIn4LhjWoEvqGM0Po9Y00dwrKJBL6tzkzOCspBYFl7BuT70DQ/mCZjAJsKAK3Kzc9L6O3wQmiVNtw1dgvGH/v1FS5dxKsVCpKOZuN1eR+3ITj+nZrBSF8QgF3z6Uehmjjish4kRY4ChXAVBuReg15jm1lGOQq/EK03qBlLnGJOVyO0CDyyw1ueDLJqu8RGH1bZN1nnFqKhfWe2dqkMFCFhxTTTGtl5D0Kutd4RUDC0mQXWY6jJOrk6b0JeYWwsFimGXHltJkqZEa3d5WerUF1R9hic0ixkJX7bA0w90VuQsuT/rD0/7p53fd80G9PgfkFXlNjsmQvCfn5BMZkTHhZEm+km/ke/Qj+hn9jv5sS6NWfeclaUT09x+kOA0a</latexit>GMM(k, ⌧)\n<latexit sha1_base64=\"DnY4Tuh6HFdXn2ZgViJtqgZiRs4=\">AAADGnicbVJNbxMxEHWWj5bw1cKRy4oIiUMVbaqq9FipF45FkLZSGkW2d3bXir227DEkWuUncIULv4Yb4sqFf4M32QNOM9Jqn9+M5XnzhhkpHGbZ315y7/6Dh3v7j/qPnzx99vzg8MWV095yGHMttb1h1IEUNYxRoIQbY4EqJuGazS/a/PVnsE7o+hMuDUwVLWtRCE4xUB/ns9HsYJANs3Wkd8GoAwPSxeXsMOnd5pp7BTVySZ2bnBmcNtSi4BJW/VvvwFA+pyVMAqypAjdt1q2u0jeBydNC2/DVmK7Z/280VDm3VCxUKoqV28615K7cxGNxNm1EbTxCzTcPFV6mqNNWd5oLCxzlMgDKrQi9pryilnIM04leYVrPkTIXKWkWGwERV1pqKsEXMau8RGH1l5h1nnFq2qGvdmqKyOAYCwapeJjWS8iPWqdd7RUDC3kYuyx1kFOp43j0DVYWwsFi0LJlS2PKwkiNbmcrR50F7R9hgXERY6Grflia0faK3AVXx8PR6fD0w8ngPOvWZ5+8Iq/JWzIi78g5eU8uyZhwUpKv5Bv5nvxIfia/kt+b0qTX3XlJokj+/AM1NQcA</latexit>k1<latexit sha1_base64=\"9kzHHZWncuz9ngu9eIIBoxnc5Ew=\">AAADIXicbVJNbxMxEHWWrxK+WjhyWREhcaiiTVWVHitx4Vgk0lYkUWR7Z3et+GNljyHRKv+CK1z4NdwQN8SfwZvsAacZabXPb8byvHnDaikcZtmfXnLn7r37Dw4e9h89fvL02eHR8ytnvOUw5kYae8OoAyk0jFGghJvaAlVMwjVbvGvz15/BOmH0R1zVMFO01KIQnGKgPi3mjZtPkfr1/HCQDbNNpLfBqAMD0sXl/CjpTXPDvQKNXFLnJuc1zhpqUXAJ6/7UO6gpX9ASJgFqqsDNmk3H6/R1YPK0MDZ8GtMN+/+NhirnVoqFSkWxcru5ltyXm3gszmeN0LVH0Hz7UOFliiZt5ae5sMBRrgKg3IrQa8orainHMKToFWbMAilzkZJmuRUQcaWldSX4MmaVlyis+RKzzjNO63b2672aIjIYx4JPKh6m9RLy49Zwp71iYCEPY5elCXIqdRKPvsHKQjhYDFp2bGnqsqilQbe3lePOgvaPsMS4iLHQVT8szWh3RW6Dq5Ph6Gx49uF0cJF163NAXpJX5A0Zkbfkgrwnl2RMONHkK/lGvic/kp/Jr+T3tjTpdXdekCiSv/8AGFwKhQ==</latexit>ks⌧\n<latexit sha1_base64=\"Nv89go/jU0fFUMda5Znj5jVqU9Y=\">AAADRXicbVJNj9MwEHXD11I+tgtHLhEV0iJVVVoty15WWokDHItEd1dqqsp2nMSqHUf2BFpZ4Ufwa7jChd/Aj+CGuILT5oC7HSnK+M1YnvfekFJwA1H0sxPcun3n7r2D+90HDx89PuwdPbk0qtKUTakSSl8TbJjgBZsCB8GuS82wJIJdkeWbpn71kWnDVfEB1iWbS5wVPOUUg4MWvUH8FkuJj1+9PB9FURSnGlP7uY3axhJDTrGwk7o+Hy96/WgYbSK8mYzapI/amCyOgk6cKFpJVgAV2JjZWQlzizVwKljdjSvDSkyXOGMzlxZYMjO3G1Z1+MIhSZgq7b4Cwg36/w2LpTFrSVxnM6XZrTXgvtqsgvRsbnlRVsAKun0orUQIKmwkChOuGQWxdgmmmrtZQ5pjJww4Ib1XiFJLwMR4TOxqS8DDMo3LnNOVj8pKANfqk4+ailBcNv7Uezl5oDOXOC+lL6auBEsGzVKYopKEaZY42UWmHJ1cjn3pLeSauYMGx2XHFltmaSkUmL2jDFoLmj+wFfhNhLipum5pRrsrcjO5HA9Hp8PT9yf9i5N2fQ7QM/QcHaMReo0u0Ds0QVNE0Rf0FX1D34Mfwa/gd/Bn2xp02jtPkRfB339PJxhc</latexit>\u0000(5) = 1000P=2\n<latexit sha1_base64=\"89xYy43aAraSnBp0+uEybERyo2A=\">AAADOXicbVJNbxMxEHWXrxK+WhAnLhYREocq2q2q0kulIi5ckIpE2khpiGyvd9eKvbbsMU20yv/gCn+DX8KRG+LKH8Cb7AGnGWm1z2/G8sx7Q40UDtL0505y6/adu/d27/cePHz0+Mne/tMLp71lfMi01HZEieNS1HwIAiQfGcuJopJf0tm7Nn/5hVsndP0JFoZPFClrUQhGIFCf82nz4e1oiU9xlqbpdK+fDtJV4Jsg60AfdXE+3U+eX+WaecVrYJI4Nz4xMGmIBcEkX/auvOOGsBkp+TjAmijuJs2q6yV+FZgcF9qGrwa8Yv+/0RDl3ELRUKkIVG4z15LbcmMPxcmkEbXxwGu2fqjwEoPGrQQ4F5YzkIsACLMi9IpZRSxhEISKXqFaz4BQF03SzNcDRFxpiakEm8es8hKE1dcx6zxlxLT6L7fOFJHBPBq8UrGY1kueH7Smu9oryi3Pg+yy1GGcSh3G0jdQWR4OFsIsG7Y0piyM1OC2tnLQWdD+gc8hLqJUbRemFxYp21ybm+DicJAdD44/HvXPjrqV2kUv0Ev0GmXoDTpD79E5GiKGLPqKvqHvyY/kV/I7+bMuTXa6O89QFMnff7sHEtw=</latexit>dMAX= 100012\n    Retrain ModuleControl Unit3Signal\n45<latexit sha1_base64=\"qbZT95xZvHpESbMArB1TV21YNHA=\">AAADPHicbVJNbxMxEHWXrxI+moI4cVkRIRWpipKqKjlWggMnVCTSVkpCZHtnN1bstWWPIdEqv4Qr/A3+B3duiCtnvMkecJqRVvv8ZizPvDfMSOGw1/u5l9y6fefuvf37rQcPHz0+aB8+uXTaWw5DrqW214w6kKKEIQqUcG0sUMUkXLH5mzp/9RmsE7r8iEsDE0WLUuSCUwzUtH3w3ksp8uXR209jpP7VtN3pdXvrSG+CfgM6pImL6WHybJxp7hWUyCV1bjQwOKmoRcElrFpj78BQPqcFjAIsqQI3qdaNr9KXgcnSXNvwlZiu2f9vVFQ5t1QsVCqKM7edq8lduZHHfDCpRGk8Qsk3D+VepqjTWoU0ExY4ymUAlFsRek35jFrKMWgVvcK0niNlLpqkWmwGiLjCUjMTfBGzyksUVn+JWecZp6a2YLVzpogM/rFgl4rFtF5Cdlz77kqvGFjIguyy0GGcmTqJpa9wZiEcLIZZtmypTJEbqdHtbOW4saD+IywwLmJM7RamFRapv702N8HlSbd/1j37cNo5HzQrtU+ekxfkiPTJa3JO3pELMiScePKVfCPfkx/Jr+R38mdTmuw1d56SKJK//wAIfxS4</latexit>Nullify(D⌧)\n<latexit sha1_base64=\"iG8B4yKJ7xPct/cy/W+WlqdnKmY=\">AAADXnicbVLBbhMxEHWy0JZAaQoXJC4WERKHKkpaaIsEUiVA4lgk0lZKlsh2vLtW7LVljyHRKvkhvoYjcOFT8CaLxKYZyfLbNzPreTNDjRQOer2fjWZ05+7O7t691v0H+w8P2oePrpz2lvEB01LbG0oclyLnAxAg+Y2xnCgq+TWdviv911+5dULnn2FueKxImotEMAKBGrc/vP8yAuLxWzyiIh0u1zaaaHDLf4ZPlstXJXi9xV2mxeN2p9ftrQzfBv0KdFBll+PDZiP8hHnFc2CSODc8NxAXxIJgki9aI++4IWxKUj4MMCeKu7hYyV3g54GZ4ETbcHLAK/b/jIIo5+aKhkhFIHObvpLc5ht6SM7jQuTGA8/Z+qHESwwal73DE2E5AzkPgDArQq2YZcQSBqHDtVeo1lMg1NWUFLO1gBqXWmIywWZ1VnkJwupvddZ5yogpB7fYqqlGhqnTMGRVb6b1kk+Oym1xuVeUWz4JbZepDnIydVxvfQGZ5eHDQtCyMZbCpImRYQ+2lnJUjaC8gc+gHkRpqKoVlqa/uSK3wdVxt3/aPf30snPxplqfPfQUPUMvUB+doQv0EV2iAWLoO/qBfqHfzT/RTrQfHaxDm40q5zGqWfTkL3znIhg=</latexit>D⌧=⇥...35 9...⇤\n<latexit sha1_base64=\"X/M7CRH430v7tXsmgij2+AaFNTE=\">AAADhXichVPNb9MwFHcbYKN8dXDkElEhDWlU7RhlBxCT4MBhmoZEt0lNqGzHSazacWQ/Q6so/SO58Z9wxElzIF0lnhT55ffe8/v6meSCGxiNfne63p279/b27/cePHz0+En/4OmVUVZTNqVKKH1DsGGCZ2wKHAS7yTXDkgh2TRafKvv1D6YNV9k3WOUslDjJeMwpBgfN++LCCsHj1eHn7wFg+8r/4AeEJ7MgUmDWb9brANgSDC3qs85XaBaVxcX0/Lws/29/u66vqi4N5/3BaDiqxb+tjBtlgBq5nB90Oy6aWskyoAIbMzvNISywBk4FK3uBNSzHdIETNnNqhiUzYVFXUfovHRL5sdLuy8Cv0X8jCiyNWUniPCWG1GzbKnCXbWYhPg0LnuUWWEY3iWIrfFB+NWI/4ppRECunYKq5q9WnKdaYgltEKwtRagGYmFYnxXLTQAtLNM5TTpdtVFoBXKufbdRYQnFe7bfc2VMLdOQgjguyPUxtBYuOKlKZzErC3Erd2EWiXDupPG6PvoBUM/ejwfWytZYiT+JcOALsLOWoWUF1VvxpOxHiquo50oy3KXJbuToejifDydeTwdn7hj776Dl6gQ7RGL1DZ+gLukRTRNEv9KfTcS9qz3vtnXiTjWu308Q8Qy3xPv4Fizou3g==</latexit>Nullify(D⌧)=⇥...3NULLNULL5...⇤\nRetrain ModuleControl UnitInferenceModuleNN Training<latexit sha1_base64=\"BScN8sz2AjDmtt+KWlcgGC/5B8o=\">AAADHnicbVJNbxMxEHWWj5bw1cKRy4oICYkqSipUcqzEhQtSkUhTKY0i2zu7a8VeW/aYJlrlR3CFC7+GG+IK/wZvsgecZqTVPr8Zy/PmDTNSOBwM/naSO3fv3T84fNB9+Ojxk6dHx88unfaWw5hrqe0Vow6kqGCMAiVcGQtUMQkTtnjf5CdfwDqhq8+4MjBTtKhELjjFQE0+zmv5ZrieH/UG/cEm0ttg2IIeaeNifpx0rjPNvYIKuaTOTUcGZzW1KLiEdffaOzCUL2gB0wArqsDN6k276/RVYLI01zZ8FaYb9v8bNVXOrRQLlYpi6XZzDbkvN/WYj2a1qIxHqPj2odzLFHXaaE8zYYGjXAVAuRWh15SX1FKOYULRK0zrBVLmIiX1cisg4gpLTSn4MmaVlyisvolZ5xmnphn8eq+miAyusWCSiodpvYTspHHbVV4xsJCFsctCBzmlOo1HX2NpIRwsBi07ttSmyI3U6Pa2ctJa0PwRlhgXMRa66oalGe6uyG1wedofnvXPPr3tnY/a9TkkL8hL8poMyTtyTj6QCzImnCzIV/KNfE9+JD+TX8nvbWnSae88J1Ekf/4Bu+QIoQ==</latexit>Ml+1\n<latexit sha1_base64=\"BScN8sz2AjDmtt+KWlcgGC/5B8o=\">AAADHnicbVJNbxMxEHWWj5bw1cKRy4oICYkqSipUcqzEhQtSkUhTKY0i2zu7a8VeW/aYJlrlR3CFC7+GG+IK/wZvsgecZqTVPr8Zy/PmDTNSOBwM/naSO3fv3T84fNB9+Ojxk6dHx88unfaWw5hrqe0Vow6kqGCMAiVcGQtUMQkTtnjf5CdfwDqhq8+4MjBTtKhELjjFQE0+zmv5ZrieH/UG/cEm0ttg2IIeaeNifpx0rjPNvYIKuaTOTUcGZzW1KLiEdffaOzCUL2gB0wArqsDN6k276/RVYLI01zZ8FaYb9v8bNVXOrRQLlYpi6XZzDbkvN/WYj2a1qIxHqPj2odzLFHXaaE8zYYGjXAVAuRWh15SX1FKOYULRK0zrBVLmIiX1cisg4gpLTSn4MmaVlyisvolZ5xmnphn8eq+miAyusWCSiodpvYTspHHbVV4xsJCFsctCBzmlOo1HX2NpIRwsBi07ttSmyI3U6Pa2ctJa0PwRlhgXMRa66oalGe6uyG1wedofnvXPPr3tnY/a9TkkL8hL8poMyTtyTj6QCzImnCzIV/KNfE9+JD+TX8nvbWnSae88J1Ekf/4Bu+QIoQ==</latexit>Ml+1LookupUpdateSignalSignalBuﬀer B12<latexit sha1_base64=\"uf9c293tAVaUuL+dTHD2kYRdjQ0=\">AAADK3icbVJNj9MwEHXD11K+usuRS0SFxKGq2tVq6XElLhyLRHdXaqrKdiaJVTu27DG0ivpXuMKFX8MJxJX/gdPmQLodKcrzm7E8894wI4XD0ehXJ7p3/8HDRyePu0+ePnv+ond6du20txxmXEttbxl1IEUJMxQo4dZYoIpJuGGr93X+5jNYJ3T5CTcGFormpcgEpxioZe8smYplgtQPkqnbo2WvPxqOdhHfBeMG9EkT0+Vp1ElSzb2CErmkzs0nBhcVtSi4hG038Q4M5SuawzzAkipwi2rX+zZ+E5g0zrQNX4nxjv3/RkWVcxvFQqWiWLjDXE0ey809ZpNFJUrjEUq+fyjzMkYd10LEqbDAUW4CoNyK0GvMC2opxyBX6xWm9Qopc61JqvV+gBaXW2oKwddtVnmJwuovbdZ5xqmpXdgenalFBgtZcEy1xbReQjqorXelVwwspEF2meswTqHO29JXWFgIB4thlgNbKpNnRmp0R1sZNBbUf4Q1tosYC111w9KMD1fkLrg+H44vh5cfL/pXk2Z9Tsgr8pq8JWPyjlyRD2RKZoSTNflKvpHv0Y/oZ/Q7+rMvjTrNnZekFdHffxUDDac=</latexit>⇧⌧, ⌧<latexit sha1_base64=\"csdPuHWhMTRxvPCWo31MJf5KVLA=\">AAADHXicbVJNbxMxEHWWrxK+WjhyWREhcaiipKpKjhVcOBaJpJWSUNne2V0Te23ZY0i0yn/gChd+DTfEFfFv8CZ7wGlGWu3zm7E8b94wI4XDweBvJ7l1+87dewf3uw8ePnr85PDo6cRpbzmMuZbaXjHqQIoKxihQwpWxQBWTcMkWb5v85WewTujqA64MzBUtKpELTjFQkzcfZ0j99WFv0B9sIr0Jhi3okTYuro+SzizT3CuokEvq3HRkcF5Ti4JLWHdn3oGhfEELmAZYUQVuXm+6XacvA5OlubbhqzDdsP/fqKlybqVYqFQUS7eba8h9uanHfDSvRWU8QsW3D+VepqjTRnqaCQsc5SoAyq0Ivaa8pJZyDAOKXmFaL5AyFympl1sBEVdYakrBlzGrvERh9ZeYdZ5xapq5r/dqishgGgseqXiY1kvIjhuzXeUVAwtZGLssdJBTqpN49DWWFsLBYtCyY0ttitxIjW5vK8etBc0fYYlxEWOhq25YmuHuitwEk5P+8Kx/9v60dz5q1+eAPCcvyCsyJK/JOXlHLsiYcPKJfCXfyPfkR/Iz+ZX83pYmnfbOMxJF8ucfLM0IcQ==</latexit>B⌧\n<latexit sha1_base64=\"qbZT95xZvHpESbMArB1TV21YNHA=\">AAADPHicbVJNbxMxEHWXrxI+moI4cVkRIRWpipKqKjlWggMnVCTSVkpCZHtnN1bstWWPIdEqv4Qr/A3+B3duiCtnvMkecJqRVvv8ZizPvDfMSOGw1/u5l9y6fefuvf37rQcPHz0+aB8+uXTaWw5DrqW214w6kKKEIQqUcG0sUMUkXLH5mzp/9RmsE7r8iEsDE0WLUuSCUwzUtH3w3ksp8uXR209jpP7VtN3pdXvrSG+CfgM6pImL6WHybJxp7hWUyCV1bjQwOKmoRcElrFpj78BQPqcFjAIsqQI3qdaNr9KXgcnSXNvwlZiu2f9vVFQ5t1QsVCqKM7edq8lduZHHfDCpRGk8Qsk3D+VepqjTWoU0ExY4ymUAlFsRek35jFrKMWgVvcK0niNlLpqkWmwGiLjCUjMTfBGzyksUVn+JWecZp6a2YLVzpogM/rFgl4rFtF5Cdlz77kqvGFjIguyy0GGcmTqJpa9wZiEcLIZZtmypTJEbqdHtbOW4saD+IywwLmJM7RamFRapv702N8HlSbd/1j37cNo5HzQrtU+ekxfkiPTJa3JO3pELMiScePKVfCPfkx/Jr+R38mdTmuw1d56SKJK//wAIfxS4</latexit>Nullify(D⌧)\nFigure 4:(a) Steps for using the estimated update workload to produce placeholders. (b) Retrain Policy\nNullifer manages data with a workload Dkeys by creating space for updates based on their distribution. It operates on\ninput data Dτ= [k 1, k2, . . . , k sτ]drawn from Dkeys, with a maximum gap dMAX , and extends Dτusing the update\ndistributionD update (see Figure 2(a)). The gap size between keysk iandk j(i < j)is calculated as\nGS(k i, kj) =\u0018\ndMAX .Rkj\nkiDupdate (x)dx\nP:=Rksτ\nk1Dupdate (x)dx\u0019\n≈dMAX\u0018Rkj\nkiGMM(x,Ψ τ)dx\nP\u0019\n(6)\nTo approximate Dupdate using updates U, a Gaussian Mixture Model (GMM) [ 79] is used. The nullifier creates gaps in\nDτby iterating over successive elements and applying Equation 6. For j=i+ 1 , this simplifies to Γ(k) =GS(k −, k).\nNullifer produces NULL values for each k∈Dτ, and the updated index for kis calculated as GS(k 1, k)×Index(k) ,\nwithΓ(k) vacant spaces before and Γ(k+)after. Figure 4(a) illustrates the procedure, initiated by the Control Unit\nand relayed to the Retrain module, for constructing a new index model. This occurs after the index ( Y) is revised by\nintegrating placeholders among the data by utilizing the trained GMM.\n3.1 Training Incoming Workload\nThe workload updates, Dupdate distribution, is modeled as a GMM (Section B.3), Dupdate ∼GMM(k,Ψ t), where\nΨt={(π i, µi, σi)}K\ni=1: weights πi, means µi, and standard deviations σi. The GMM parameters generated by a neural\nnetwork. Training minimizes a loss function balancing model accuracy and simplicity by penalizing unnecessary\ncomponents.\nThe initial GMM parameters are established using a greedy approach (see Algorithm 2 in Appendix E). Starting with the\ndata set D0, it groups data into distributions by iteratively forming candidate Gaussians using the two smallest elements\nand adding points that meet a confidence threshold δgiven as a hyperparameter. Once no more points fit, the cluster is\nfinalized, and the process repeats until all data is assigned. This yields initial parameters Ψ0={(π i, µi, σi)}K\ni=1and an\ninitialK, which may overestimate the true number of components, but provides a strong starting point.\n3.2 Neural Unit for Fine-Tuning.\nFigure 2(b) shows the neural architecture NN Ψthat aims to fine-tune ψparameters. The model consists of a single\nfully connected hidden layer with Kneurons and leads to an output layer with 3Kneurons. These layers are set up\npost-initialization, with the hidden layer being entirely linked to the shared layer NNC. The neural network, initialized\nwithΨ0from Algorithm 2, directly predicts the GMM parameters Ψ ={(π i, µi, σi)}K\ni=1. During training, the loss is\ncomputed for each batch of input data, gradients are calculated, and network weights are updated via gradient descent.\nThe process continues until the loss converges or GMM parameter changes are negligible. The regularization term\nensures components with small weights are suppressed, simplifying the model without explicit pruning.\nThe GMM parameters are trained using the following loss function:\nLΨ(Xτ) =−P\nx∈Xτlog\u0012PK\ni=1πiN\u0000\nx|µ i, σi\u0001\u0013\n+c1\nKPK\ni=1πν\ni, (7)\nThe first term ensures accurate data modeling Xτ, and the second term penalizes small weights πito reduce unnecessary\ncomponents. The regularization strength is controlled by c >0 , and 0< ν≤1 adjusts the penalty’s scaling. This\nencourages sparsity, which reduces the number of active components.\n5\n\nSig2Model: A Boosting-Driven Model for Updatable Learned Indexes\n4 Sig2Model Training\nThis section describes the core technical implementation of Sig2Model for the neural network training and model\nretraining. We first explain the data preparation phase, and then delve into the training processes.\n4.1 Data Preparation\nThe data preparation phase combines buffer indices with original data indices to provide the neural network\nAlgorithm 1Training Procedure forSig2ModelNeural Networks\n1:Input:Buffer Bτ, Representation X, Labels Y, Thresholds\nϵΠ,ϵΨ, Maximum Iterations MaxIter , Neural Network NN=\n{NN C, NN Π, NN Ψ}, Sampling Fraction λ, Replay Buffer Size κ\n2:Output:Fine-TunedNN\n3:Initialization:iterations←0, L Π←2ϵ Π, LΨ←2ϵ Ψ\n4:Bτ\nidx←ReindexRB(Bτ, RBτ−1)▷ReindexRBτ−1using\nAlg. 3 (Appendix E)\n5:repeat\n6:Π,Ψ←FeedForward(X∪RBτ−1)using updated weights\n7:L Π← L Π(X∪Bτ, Y∪RBτ−1.I)▷Using Equation 5\n8:ifL Π≤ϵΠandL Ψ≤ϵΨthen\n9:break▷Desired error thresholds achieved\n10:end if\n11:ifL Π> ϵΠthen▷ backpropagationΠ\nϵΠin Fig 2(b)\n12:Perform backpropagation onNN Π\n13:Π,Ψ←FeedForward(X∪RBτ−1)\n14:end if\n15:L Ψ← L Ψ(X∪Bτ)▷Using Equation 7\n16:ifL Ψ> ϵΨthen▷ backpropagationΨ\nϵΨin Fig 2(b)\n17:Perform backpropagation onNN Ψ\n18:end if\n19:iterations←iterations+ 1\n20:untiliterations≥MaxIter\n21:RBτ←UpdateRB(Bτ\nidx, RBτ−1, λ, κ)▷UpdateRBusing\nAlg. 4 (Appendix E)with both embedding representations and up-\ndated positional contexts. This integration\nenables the network to learn patterns based\non data features and their positional relation-\nships.\nBuffer and Embedding.When the buffer,\nimplemented using a B-tree, reaches its capac-\nityρ, nodes are traversed to sort the data and\nproduce Bτ, where τrepresents the buffer’s\noverflow count (stage). Using the current\nmodel M′\nl, the positions of the data points\nare determined. If a position in Dτis already\noccupied, the data is stored in the buffer. For\nsimplicity, we assume Dτis fully occupied,\nalthough in practice the training focuses on\nunaccommodated data.\nConstructing Representation.Inputs Xτ\nare derived from the embeddings of current\ndataDτ,Dτ\nemb(Dτ\nembis the representation\nofDτ). First, we obtain IBτ\nembwhich is the\ncurrent buffer indexes from M′\nl(.,Π τ). Then\nwe select the element of Dτ\nembthat the in-\ndexIBτ\nembrefers to, thus Xτ=Dτ\nemb[IBτ\nemb].\nThus, Xτcorresponds to the embedding of\ndata points in Dτare occupied for buffer el-\nements.\nGenerating Labels.Labels Yτaccount for\nindex changes after updates. For each Xτ\nj, the model output index IBτ\nemb[j]is corrected as Yτ\nj=IBτ\nemb[j] +j . Since\nBτis sorted, the target index is preceded byjnew update indices, ensuring accurate model training.\n4.2 Training Process\nThe training process operates in two phases:(1) updating neural networks for incoming updates and SigmaSigmoid\nmodeling, (2) evolving the model from MltoMl+1. These phases improve system representation through the integration\nofNsigmoid functions within the neural network while maintaining prior data buffers viaPN\ni=1σ(., A i, ωi, ϕi). The\nsystem triggers updates to the original index model when performance metrics indicate degradation.\nNeural Network Training Process.This section describes the neural network training procedure (Figure 2(b)). Two\ncost functions, LΠandLΨ, are computed based on the outputs of NN ΠandNN Ψ. As these operate in different\ndimensions of spaces, a specialized training strategy (Algorithm 1) is used. Each model minimizes its cost independently,\niterating until bothL Π≤ϵΠandL Ψ≤ϵΨare met or the maximum iterations are reached.\nThe training process starts with afeed forwardstep using data from the current buffer Bτ(Section 4.1). Errors are\ncalculated and backpropagation is performed only if LΠ> ϵΠorLΨ> ϵΨ. Priority is given to NN Πif its error\nexceeds the threshold. The weights are updated, and the cycle continues with NN Ψ, using a batch size equal to the\nbuffer size ρ. Training stops when both errors are below their respective thresholds or when the iteration limit is reached.\nBoth cost functions have closed-form derivatives for efficient optimization.\nLearned Index Model Retraining.This section explains theRetrain Modulefor the underlying LI model, shown\nin Figure 4(b), focusing on retraining signals, data preparation, and system re-initialization after retraining for new\nupdates.\n6\n\nSig2Model: A Boosting-Driven Model for Updatable Learned Indexes\nTable 1:QPS comparisons with state-of-the-art methods. The numbers are in106.QPS (MQPS).\nWorkload DatasetS2M S2M−Ψ S2M−B B+Tree Alex LIPP DILI Average Max\nS M7 S M7 S M7 S M S M\nRead-OnlyWiki 5.45.4 NA NA NA NA 2.0 4.1 5.2 5.4 50.5% 48.8% 2.68x 2.68x\nLogn 6.36.2 NA NA NA NA 1.9 6.2 6.0 6.3 56.1% 55.4% 3.18x 3.18x\nFB 4.54.5 NA NA NA NA 2.0 2.3 4.3 4.5 55.5% 51.6% 2.23x 2.23x\nRead-HeavyWiki 4.35.4 4.1 5.1 3.9 4.9 1.5 2.2 3.9 4.1 69.3% 75.8% 2.86x 3.41x\nLogn 4.15.1 3.9 4.9 3.8 4.7 1.5 3.2 2.9 4.0 61.2% 70.1% 2.71x 3.38x\nFB 2.63.3 2.5 3.1 2.4 3.0 1.3 1.1 2.3 2.5 62.8% 72.4% 2.28x 2.91x\nWrite-HeavyWiki 3.64.5 3.3 4.2 3.1 4.0 1.3 1.9 2.8 3.1 73.8% 80.2% 2.76x 3.45x\nLogn 3.94.8 3.7 4.6 3.4 4.3 1.3 3.2 2.9 3.6 76.2% 83.5% 3.00x 3.70x\nFB 3.94.7 3.6 4.4 3.3 4.1 1.3 OOM 2.1 3.5 82.2% 88.4% 3.00x 3.61x\nWrite-OnlyWiki 2.92.9 2.7 2.8 2.4 2.4 1.3 1.4 2.2 2.4 76.8% 76.8% 2.23x 2.23x\nLogn 3.13.2 3.0 3.0 2.6 2.6 1.2 2.8 2.2 2.5 58.4% 62.8% 2.58x 2.66x\nFB 2.52.5 2.2 2.2 2.0 2.0 1.2 OOM 1.7 2.1 68.1% 68.1% 2.04x 2.04x\nRetrain Signals.Signals for retraining come fromNN Trainingand theInference Module(Figure 2(b,c)). During\ntraining, signals arise when backpropagation reaches its limit ( iterations≥MaxIter in Algorithm 1). During\ninference, signals occur when out-of-range searches show that M′\nlstruggles to maintain accuracy, typically when the\nsystem reaches maximum capacity (Section G.2), and the target not be found in theϵ-bounded range.\nData Preprocessing.We sort the data for training a new LI model. The output of NN Ψgenerates a GMM optimized\nfor the update workload distribution ( Dupdate (k)∼GMM(k,Ψ τ)). After the buffer reaches capacity, Bτis merged\nwithDτto create Dτ+1. Using Dupdate , Equation 6 introduces gaps between data points. Training data for Ml+1are\nformed by pairing the entries in Dτ+1with their new indices. This ensures that the model smoothly handles anticipated\ngaps without need for modifications.\nNeural Networks Re-Initialize.Upon receiving update signals (Figure 4), the Control Unit initiates training for\nthe new model Ml+1while executing a systematic re-initialization protocol. The SigmaSigmoid parameters\nSequentialZipfianHotSpot\nExponentialUniformLatest\n(a) Request Distributions01234MQPS$Sig2Mod B+Tree Alex LIPP DILI\nA B C D E F\n(b) YCSB Workloads012345MQPSSig2Mod B+Tree Alex LIPP DILI\nWIKI LOGN FB\n(c) Datasets1041061081010Index size\n(MBytes)Sig2Model B+Tree Alex LIPP DILI\nFigure 5:Evaluation results. (a) QPS on request\ndistributions. (b) YCSB workloads. (c) Index mem-\nory sizes.undergo complete reset to preserve the update distribution\nDupdate , with distinct handling procedures for each neural net-\nwork component: NN Ψremains unchanged, continuously\nadapting to incoming data streams. For NN Π, the system\nneutralizes sigmoids from the previous model Mlthrough four\ncoordinated operations: (1) complete purging of the replay\nbuffer, (2) zero-initialization of weights connecting to the out-\nput neurons {Aj}N\nj=1, (3) recalibration of sigmoid parameters\n{ϕj}N\nj=1using either Dupdate or uniform random sampling from\nthe key space [ 29], and (4) uniform weight adjustment setting\nωj= 1for all j∈[1,N] . Uniformly modifying weights to set\nωj= 1|N\nj=1. The neural network NNCremains unchanged, pre-\nserving the feedforward paths from NNCtoNN Ψto maintain\nthe GMM while resetting SigmaSigmoid for future updates.\n5 Experimental Evaluation\nWe conduct a comprehensive evaluation of Sig2Model against\nstate-of-the-art learned indexes (DILI, LIPP, and Alex).\nSig2Model shows significant improvements across three key\nmetrics: Up to 3×higher QPS, 20× lower training cost, and\n1000× reduced memory usage compared to baseline methods.\nThese improvements are particularly significant in update-intensive scenarios that highlight Sig2Model’s architec-\ntural advantages. Complete experimental configurations and additional results are available in Appendices H and I\nrespectively.\nQPS Comparison.Table 1 presents a detailed QPS comparison between Sig2Model (S2M), three baseline methods,\nand two ablated variants: S2M −Ψ (without placeholder training) and S2M −B (without buffer component). We\nevaluate both single-threaded and multi-threaded configurations of Sig2Model variants. We provide the details on\nmulti-threading in Appendix I.3. Sig2Model shows superior QPS scaling as update rates increase, achieving an 82%\naverage improvement (88%on the multi-threaded version) over baseline methods.\nIn read-only workloads, Sig2Model matches the performance of its underlying RadixSpline implementation while\noutperforming B+Tree by 15-20% and achieving comparable results to DILI. The ablated variants S2M −Ψand S2M −B\n7\n\nSig2Model: A Boosting-Driven Model for Updatable Learned Indexes\nTable 2:Components ablation study\nComponents NoneBΨ ΠB+Ψ Π+ΨB+ΠFull\nMQPS 0.03 1.4 2.0 2.1 2.3 2.9 3.0 3.6\nare excluded from these tests, as they specifically optimize update handling rather than read performance. For read-\nheavy workloads with 10% updates, Sig2Model achieves 2.7× higher QPS on the Logn dataset, with multi-threading\nproviding 3.4× speedup. As update rates increase, Sig2Model maintains a consistent 60% average QPS advantage over\ncompeting methods. Write-heavy workload tests reveal particularly strong performance, with Sig2Model processing 4.7\nMQPS on the Facebook dataset, significantly outperforming B+Tree ( 1.3M), LIPP ( 2.1M), and DILI ( 3.5M). Alex also\nfails in several experiments due to its known memory constraints [ 73]. In write-only scenarios, Sig2Model achieves\n67.7-74.5% higher QPS than baselines through its optimized update handling mechanisms.\nVarious Request Distribution.Figure 5(a) demonstrates Sig2Model’s consistent performance across six different\nrequest distributions for write-heavy workloads using the Wiki dataset [ 11]. The system maintains QPS improvements\nof2.61× over B+Tree, 1.78× over Alex, 1.15× over LIPP, and 1.54× over DILI, showing a robust adaptation to\nvarying access patterns.\nYCSB Benchmark Results.Figure 5(b) shows Sig2Model achieves consistently superior QPS across all six YCSB\nworkloads [ 8]. For read-intensive workloads (YCSB B, C, and D), the system delivers 2.1-4.1MQPS, outperforming\nbaseline methods by 1.0-2.8× . In balanced workloads (YCSB A and F), Sig2Model maintains strong performance\nat3.3-3.8MQPS while sustaining a 55.1% average QPS advantage. The system particularly excels in scan-heavy\noperations (YCSB E), where its efficient range query processing yields 55% higher QPS than DILI and significantly\noutperforms other baselines that suffer from substantial re-traversal overhead.\nIndex Memory Size.Figure 5(c) shows the memory usage of the Sig2Model and the baselines, including memory for\nfine-tuning neural networks in the Sig2Model. Sig2Model uses up to 1000× less memory than DILI, due to its efficient\nplaceholder placement and minimized buffer additions. In contrast, DILI and LIPP consume more memory due to new\nleaf nodes and empty slots created during conflicts. Alex’s in-place placeholder strategy is less efficient as it does not\nconsider the incoming workload distribution, leading to slightly higher memory usage than Sig2Model.\nCore Components Ablation Study.We analyze the individual contributions of the three core com-\nponents of Sig2Model on the Wiki dataset under write-heavy workloads: (1) buffer management (B),\n(2) index approximation network using SigmaSigmoid boosting ( Π), and (3) update workload train-\ning using GMM ( Ψ). Table 2 presents QPS measurements for all combinations of components.\n10 20 30 40 50\n(a) Epochs00.20.40.40.60.81Loss\nLoss\nLoss\n00.25 0.5 0.75 11.25 1.5 1.75\n(b) Normalized Required EpochsBaselineReplay BufferHighway NNReLUDropoutNeural NetworkEWC\n2 4 6 8 10 12 14 16 18 20\n(c) Update Round01020304050Epochs\nRequired Epochs\nDistribution\nshift\nFigure 6:(a) Loss curves for NN ΠandNN Ψ. (b)\nAblation study on NNc. (c) Impact of distribution\nshift on required epochs.The baseline RadixSpline with full retraining (None) achieves\nonly 0.03 MQPS. Individual components show varying effec-\ntiveness: Ψ(2.0 MQPS, comparable to Alex), Π(2.1MQPS),\nandB( 1.4MQPS). The combinations of two components\nshow synergistic effects, withB+ Πachieving 3.0MQPS and\nΠ+Ψreaching 2.9MQPS (surpassing LIPP). The complete\nSig2Model configuration (B+ Π+Ψ) delivers optimal perfor-\nmance at 3.6MQPS, consistent with our full system results in\nTable 1.\nTraining Loss Curves.Figure 6(a) shows the loss curves\nforNN ΠandNN Ψduring ten rounds of fine-tuning on the\nWiki dataset using the read-heavy workload. Both networks\ntrain smoothly and converge to ϵΠandϵΨ. Initially, NN Π\nrequires about 50epochs to converge, but this decreases to 29\nepochs in later updates due to memory retained from NNC.\nSimilarly, NN Ψrequires fewer epochs in subsequent updates,\nas the update distributions remain consistent, and the GMM\nparameters ( Ψ) do not require significant changes. After four\nupdates with stable workload, the initial loss for NN Ψfalls\nbelowϵ Ψ, eliminating the need for further iterations.\nNNCAblation Study.Figure 6(b) analyze the contributions\nofNNCcomponents to training efficiency by measuring the\nincrease in epochs required to reach ϵΠandϵΨwhen compo-\nnents are removed. Removing the reply buffer, highway NN, ReLU, Dropout, and EWC increase the epochs by 1.42× ,\n1.31× ,1.24× ,1.21× , and 1.46× , respectively. Simplifying fully connected layers from 3 to 1 results in a 1.73×\nincrease in epochs.\n8\n\nSig2Model: A Boosting-Driven Model for Updatable Learned Indexes\nRetrain Cost Analysis.Figure 7 shows retraining costs for Sig2Model and baselines under a write-\nonly workload on the Wiki dataset. Sig2Model achieves up to 2.20× reduction in the total number\n50 100 200 400 800\nNumber of Updates (Million)100101102T otal Retrain\nTime (s) 103105\nRetrain TimesSig2Mod Alex LIPP DILI\nFigure 7:Retrain costof retrainings and a 20.58× decrease in the total duration of\nretraining compared to the baselines.\nLimitations.(1) Distribution Shift. Since the parameters in\nSigmaSigmoids and GMM are trained on an initial data distri-\nbution, any shift in the distribution requires additional training\ntime for the neural network to adapt. For example, as shown\nin Figure 6(c), the number of training epochs spikes during the\n11th round when the data distribution changes from Zipfian to Exponential.(2) Fixed SigmaSigmoid Capacity ( N).\nIn our experiments, we use a constant value for Nbased on hyperparameter sensitivity test [ 1](See Appendix I.4).\nHowever, this value can be dynamically adjusted based on the observed data distribution and workload to improve\nfurther improve the performance.\n6 Conclusion\nSig2Model presents a mathematically rigorous framework for efficient learned index updates, directly addressing the\ncritical retraining bottleneck through innovative model adaptation techniques. While index updates remain inherently\nnon-local operations, our approach guarantees bounded sub-optimality. By employing a boosting methodology,\nSig2Model demonstrates that an ensemble of weak approximators can progressively converge toward an optimal update\npolicy—eliminating the need for expensive full retraining cycles. This fundamental advancement not only maintains\nindex effectiveness during updates but also opens new research directions for sustainable learned index architectures.\nOur work establishes a foundation for future systems that can adapt dynamically to workload changes while preserving\ntheoretical guarantees.\nReferences\n[1]Jacob Adkins, Michael Bowling, and Adam White. A method for evaluating hyperparameter sensitivity in\nreinforcement learning. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang\n(eds.),Advances in Neural Information Processing Systems, volume 37, pp. 124820–124842. Curran Asso-\nciates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/\ne1cadf5f02cc524b59c208728c73f91c-Paper-Conference.pdf.\n[2]Pierre Baldi and Peter J Sadowski. Understanding dropout.Advances in neural information processing systems,\n26, 2013.\n[3]Sumita Barahmand and Shahram Ghandeharizadeh. D-zipfian: a decentralized implementation of zipfian. In\nProceedings of the Sixth International Workshop on Testing Database Systems, pp. 1–6, 2013.\n[4]Rudolf Bayer and Edward McCreight. Organization and maintenance of large ordered indices. InProceedings of\nthe 1970 ACM SIGFIDET (Now SIGMOD) Workshop on Data Description, Access and Control, pp. 107–141,\n1970.\n[5] Timo Bingmann. Stx b+ tree c++, 2024. URLhttps://github.com/bingmann/stx-btree/.\n[6]Bertram C Brookes. The derivation and application of the bradford-zipf distribution.Journal of documentation,\n24(4):247–265, 1968.\n[7]Subarna Chatterjee, Mark F Pekala, Lev Kruglyak, and Stratos Idreos. Limousine: Blending learned and classical\nindexes to self-design larger-than-memory cloud storage engines.Proceedings of the ACM on Management of\nData, 2(1):1–28, 2024.\n[8]Brian F. Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell Sears. Benchmarking cloud\nserving systems with ycsb. InACM Symposium on Cloud Computing, SoCC ’10, pp. 143–154, New York, NY ,\nUSA, 2010. ACM. ISBN 978-1-4503-0036-0. doi: 10.1145/1807128.1807152. URL http://doi.acm.org/\n10.1145/1807128.1807152.\n[9]Yifan Dai, Yien Xu, Aishwarya Ganesan, Ramnatthan Alagappan, Brian Kroth, Andrea Arpaci-Dusseau, and\nRemzi Arpaci-Dusseau. From {WiscKey }to bourbon: A learned index for {Log-Structured }merge trees. In14th\nUSENIX Symposium on Operating Systems Design and Implementation (OSDI 20), pp. 155–171, 2020.\n[10] FB Dataset. doi, 2019. URLhttps://doi.org/10.7910/DVN/JGVF9A/Y54SI9.\n[11] WikiTS Dataset. doi, 2019. URLhttps://doi.org/10.7910/DVN/JGVF9A/SVN8PI.\n[12] Angjela Davitkova, Evica Milchevski, and Sebastian Michel. The ml-index: A multidimensional, learned index\nfor point, range, and nearest-neighbor queries. InEDBT, pp. 407–410, 2020.\n9\n\nSig2Model: A Boosting-Driven Model for Updatable Learned Indexes\n[13] Shirli Di-Castro, Shie Mannor, and Dotan Di Castro. Analysis of stochastic processes through replay buffers. In\nInternational Conference on Machine Learning, pp. 5039–5060. PMLR, 2022.\n[14] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li, Hantian Zhang, Badrish Chan-\ndramouli, Johannes Gehrke, Donald Kossmann, et al. Alex: an updatable adaptive learned index. SIGMOD,\n2020.\n[15] Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. Tsunami: A learned multi-dimensional index\nfor correlated data and skewed workloads.arXiv, 2020.\n[16] Ben Eysenbach, Russ R Salakhutdinov, and Sergey Levine. Search on the replay buffer: Bridging planning and\nreinforcement learning.Advances in neural information processing systems, 32, 2019.\n[17] Paolo Ferragina and Giorgio Vinciguerra. Learned data structures. InRecent Trends in Learning From Data:\nTutorials from the INNS Big Data and Deep Learning Conference (INNSBDDL2019). Springer, 2020.\n[18] Paolo Ferragina and Giorgio Vinciguerra. The pgm-index: a fully-dynamic compressed learned index with\nprovable worst-case bounds.VLDB, 13(8), 2020.\n[19] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim Kraska. Fiting-tree: A data-aware\nindex structure. InSIGMOD, 2019.\n[20] Jiake Ge and et al. Sali: A scalable adaptive learned index framework based on probability models.SIGMOD,\n2023. doi: 10.1145/3626752.\n[21] Jiake Ge, Boyu Shi, Yanfeng Chai, Yuanhui Luo, Yunda Guo, Yinxuan He, and Yunpeng Chai. Cutting learned\nindex into pieces: An in-depth inquiry into updatable learned indexes. In2023 IEEE 39th International Conference\non Data Engineering (ICDE), pp. 315–327, 2023. doi: 10.1109/ICDE55515.2023.00031.\n[22] Goetz Graefe. Modern b-tree techniques.Found. Trends Databases, 3(4):203–402, April 2011. ISSN 1931-7883.\ndoi: 10.1561/1900000028. URLhttps://doi.org/10.1561/1900000028.\n[23] Leo J Guibas and Robert Sedgewick. A dichromatic framework for balanced trees. In19th Annual Symposium on\nFoundations of Computer Science (sfcs 1978), pp. 8–21. IEEE, 1978.\n[24] Ali Hadian and Thomas Heinis. Shift-table: A low-latency learned index for range queries using model correction.\narXiv preprint arXiv:2101.10457, 2021.\n[25] Alireza Heidari and Wei Zhang. Filter-centric vector indexing: Geometric transformation for efficient filtered\nvector search. InProceedings of the Eighth International Workshop on Exploiting Artificial Intelligence Techniques\nfor Data Management, aiDM ’25, New York, NY , USA, 2025. Association for Computing Machinery. ISBN 979-\n8-4007-1920-2/2025/06. doi: 10.1145/3735403.3735996. URL https://doi.org/10.1145/3735403.\n3735996.\n[26] Alireza Heidari, Ihab F Ilyas, and Theodoros Rekatsinas. Approximate inference in structured instances with\nnoisy categorical observations–supplementary material.\n[27] Alireza Heidari, Joshua McGrath, Ihab F Ilyas, and Theodoros Rekatsinas. Holodetect: Few-shot learning for\nerror detection. InProceedings of the 2019 International Conference on Management of Data, pp. 829–846, 2019.\n[28] Alireza Heidari, Ihab F Ilyas, and Theodoros Rekatsinas. Approximate inference in structured instances with\nnoisy categorical observations. InUncertainty in Artificial Intelligence, pp. 412–421. PMLR, 2020.\n[29] Alireza Heidari, Shrinu Kushagra, and Ihab F Ilyas. On sampling from data with duplicate records.arXiv preprint\narXiv:2008.10549, 2020.\n[30] Alireza Heidari, George Michalopoulos, Shrinu Kushagra, Ihab F Ilyas, and Theodoros Rekatsinas. Record fusion:\nA learning approach.arXiv preprint arXiv:2006.10208, 2020.\n[31] Alireza Heidari, George Michalopoulos, Ihab F Ilyas, and Theodoros Rekatsinas. Record fusion via inference and\ndata augmentation.ACM/JMS Journal of Data Science, 1(1):1–23, 2024.\n[32] Alireza Heidari, Amirhossein Ahmadi, and Wei Zhang. Uplif: An updatable self-tuning learned index framework.\nIn Richard Chbeir, Sergio Ilarri, Yannis Manolopoulos, Peter Z. Revesz, Jorge Bernardino, and Carson K. Leung\n(eds.),Database Engineered Applications, pp. 345–362, Cham, 2025. Springer Nature Switzerland. ISBN\n978-3-031-83472-1.\n[33] Alireza Heidari, Amirhossein Ahmadi, and Wei Zhang. Doblix: A dual-objective learned index for log-structured\nmerge trees.Proc. VLDB Endow., 18(11):3965–3978, September 2025. ISSN 2150-8097. doi: 10.14778/3749646.\n3749667. URLhttps://doi.org/10.14778/3749646.3749667.\n[34] Alireza Heidari, Wei Zhang, and Ying Xiong. Fusedann: Convexified hybrid ann via attribute-vector fusion, 2025.\nURLhttps://arxiv.org/abs/2509.19767.\n10\n\nSig2Model: A Boosting-Driven Model for Updatable Learned Indexes\n[35] Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. Measuring catastrophic\nforgetting in neural networks. InProceedings of the AAAI conference on artificial intelligence, volume 32, 2018.\n[36] Minsu Kim, Jinwoo Hwang, Guseul Heo, Seiyeon Cho, Divya Mahajan, and Jongse Park. Accelerating string-key\nlearned index structures via memoization-based incremental training.arXiv preprint arXiv:2403.11472, 2024.\n[37] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper, Tim Kraska, and Thomas\nNeumann. Sosd: A benchmark for learned indexes.NeurIPS Workshop on Machine Learning for Systems, 2019.\n[38] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper, Tim Kraska, and Thomas\nNeumann. Radixspline: a single-pass learned index. Ininternational workshop on exploiting artificial intelligence\ntechniques for data management, 2020.\n[39] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran\nMilan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in\nneural networks.Proceedings of the national academy of sciences, 114(13):3521–3526, 2017.\n[40] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned index structures.\nInSIGMOD, 2018.\n[41] Hai Lan, Zhifeng Bao, J Shane Culpepper, and Renata Borovica-Gajic. Updatable learned indexes meet disk-\nresident dbms - from evaluations to design choices.ACM on Management of Data, 2023. doi: 10.1145/3589284.\n[42] Hai Lan, Zhifeng Bao, J Shane Culpepper, Renata Borovica-Gajic, and Yu Dong. A fully on-disk updatable\nlearned index. In40th IEEE International Conference on Data Engineering (ICDE). IEEE, 2024.\n[43] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. Lisa: A learned index structure for spatial data. In\nSIGMOD, 2020.\n[44] Pengfei Li, Yu Hua, Jingnan Jia, and Pengfei Zuo. Finedex: A fine-grained learned index scheme for scalable and\nconcurrent memory systems.VLDB, 2021. doi: 10.14778/3489496.3489512.\n[45] Pengfei Li, Hua Lu, Rong Zhu, Bolin Ding, Long Yang, and Gang Pan. Dili: A distribution-driven learned index.\nVLDB, 2023. doi: 10.14778/3598581.3598593.\n[46] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.Advances in\nneural information processing systems, 30, 2017.\n[47] Liang Liang, Guang Yang, Ali Hadian, Luis Alberto Croquevielle, and Thomas Heinis. : A memory-efficient\nsliding window learned index.Proc. ACM Manag. Data, 2(1), March 2024. doi: 10.1145/3639296. URL\nhttps://doi.org/10.1145/3639296.\n[48] Liyang Liu, Zhanghui Kuang, Yimin Chen, Jing-Hao Xue, Wenming Yang, and Wayne Zhang. Incdet: In defense\nof elastic weight consolidation for incremental object detection.IEEE transactions on neural networks and\nlearning systems, 32(6):2306–2319, 2020.\n[49] Ester Livshits, Alireza Heidari, Ihab F Ilyas, and Benny Kimelfeld. Approximate denial constraints.arXiv preprint\narXiv:2005.08540, 2020.\n[50] Chaohong Ma, Xiaohui Yu, Yifan Li, Xiaofeng Meng, and Aishan Maoliniyazi. Film: A fully learned index\nfor larger-than-memory databases. 16(3), 2022. ISSN 2150-8097. doi: 10.14778/3570690.3570704. URL\nhttps://doi.org/10.14778/3570690.3570704.\n[51] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and\nMartin Riedmiller. Playing atari with deep reinforcement learning.arXiv, 2013.\n[52] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. Learning multi-dimensional indexes. In\nProceedings of the 2020 ACM SIGMOD international conference on management of data, pp. 985–1000, 2020.\n[53] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,\nZeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep\nlearning library.Advances in neural information processing systems, 32, 2019.\n[54] Jianzhong Qi and et al. Effectively learning spatial indices.VLDB, 13(12), 2020.\n[55] Antônio H Ribeiro, Koen Tiels, Luis A Aguirre, and Thomas Schön. Beyond exploding and vanishing gradients:\nanalysing rnn training using attractors and smoothness. InInternational conference on artificial intelligence and\nstatistics, pp. 2370–2380. PMLR, 2020.\n[56] Ibrahim Sabek and Tim Kraska. The case for learned in-memory joins. 2023. ISSN 2150-8097. doi: 10.14778/\n3587136.3587148. URLhttps://doi.org/10.14778/3587136.3587148.\n11\n\nSig2Model: A Boosting-Driven Model for Updatable Learned Indexes\n[57] Yufan Sheng, Xin Cao, Yixiang Fang, Kaiqi Zhao, Jianzhong Qi, Gao Cong, and Wenjie Zhang. Wisk: A\nworkload-aware learned index for spatial keyword queries. 1(2), 2023. doi: 10.1145/3589332. URL https:\n//doi.org/10.1145/3589332.\n[58] Jiachen Shi, Gao Cong, and Xiao-Li Li. Learned index benefits: Machine learning based index performance\nestimation. 15(13), 2022. ISSN 2150-8097. doi: 10.14778/3565838.3565848. URL https://doi.org/10.\n14778/3565838.3565848.\n[59] Jiachen Shi, Gao Cong, and Xiao-Li Li. Learned index benefits: Machine learning based index performance\nestimation.Proc. VLDB Endow., 15(13):3950–3962, September 2022. ISSN 2150-8097. doi: 10.14778/3565838.\n3565848. URLhttps://doi.org/10.14778/3565838.3565848.\n[60] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple\nway to prevent neural networks from overfitting.The journal of machine learning research, 15(1):1929–1958,\n2014.\n[61] Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. Highway networks.arXiv preprint\narXiv:1505.00387, 2015.\n[62] Zhaoyan Sun, Xuanhe Zhou, and Guoliang Li. Learned index: A comprehensive experimental evaluation.VLDB,\n2023. doi: 10.14778/3594512.3594528.\n[63] Chuzhe Tang and et al. Xindex: a scalable learned index for multicore data storage. InSIGPLAN, 2020.\n[64] Haixin Wang, Xiaoyi Fu, Jianliang Xu, and Hua Lu. Learned index for spatial queries. InInternational Conference\non Mobile Data Management (MDM). IEEE, 2019.\n[65] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of continual learning: theory,\nmethod and application.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.\n[66] Yifan Wang, Haodi Ma, and Daisy Zhe Wang. Lider: An efficient high-dimensional learned index for large-scale\ndense passage retrieval.Proc. VLDB Endow., 16(2), oct 2022. ISSN 2150-8097. doi: 10.14778/3565816.3565819.\nURLhttps://doi.org/10.14778/3565816.3565819.\n[67] Youyun Wang, Chuzhe Tang, Zhaoguo Wang, and Haibo Chen. Sindex: a scalable learned index for string keys.\nInSIGOPS, 2020.\n[68] Christopher JCH Watkins and Peter Dayan. Q-learning.Machine learning.\n[69] Chaichon Wongkham, Baotong Lu, Chris Liu, Zhicong Zhong, Eric Lo, and Tianzheng Wang. Are updatable\nlearned indexes ready?VLDB, 2022. doi: 10.14778/3551793.3551848.\n[70] Jiacheng Wu, Yong Zhang, Shimin Chen, Jin Wang, Yu Chen, and Chunxia Xing. Updatable learned index with\nprecise positions.VLDB, 2021. doi: 10.14778/3457390.3457393.\n[71] Peizhi Wu and Zachary G Ives. Modeling shifting workloads for learned database systems.Proceedings of the\nACM on Management of Data, 2(1):1–27, 2024.\n[72] Shangyu Wu and et al. 15(10), 2022. ISSN 2150-8097. doi: 10.14778/3547305.3547322. URL https:\n//doi.org/10.14778/3547305.3547322.\n[73] Rui Yang, Evgenios M Kornaropoulos, and Yue Cheng. Algorithmic complexity attacks on dynamic learned\nindexes.\n[74] Yue Yang and Jianwen Zhu. Write skew and zipf distribution: Evidence and implications.ACM transactions on\nStorage (TOS), 12(4):1–19, 2016.\n[75] Zongheng Yang, Badrish Chandramouli, Chi Wang, Johannes Gehrke, Yinan Li, Umar Farooq Minhas, Per-Åke\nLarson, Donald Kossmann, and Rajeev Acharya. Qd-tree: Learning data layouts for big data analytics. In\nSIGMOD, 2020.\n[76] Jiaoyi Zhang and Yihan Gao. Carmi: A cache-aware learned index with a cost-based construction algorithm.\nVLDB, 2022. doi: 10.14778/3551793.3551823.\n[77] Shunkang Zhang, Ji Qi, Xin Yao, and André Brinkmann. Hyper: A high-performance and memory-efficient\nlearned index via hybrid construction.Proc. ACM Manag. Data, 2(3), May 2024. doi: 10.1145/3654948. URL\nhttps://doi.org/10.1145/3654948.\n[78] Songnian Zhang, Suprio Ray, Rongxing Lu, and Yandong Zheng. Sprig: A learned spatial index for range and\nknn queries. In17th International Symposium on Spatial and Temporal Databases, 2021.\n[79] Bingchen Zhao, Xin Wen, and Kai Han. Learning semi-supervised gaussian mixture models for generalized\ncategory discovery. InProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp.\n16623–16633, October 2023.\n12\n\nSig2Model: A Boosting-Driven Model for Updatable Learned Indexes\nAppendix Contents\nA Table of Notations 14\nB Preliminaries 14\nB.1 Learned Index (LI) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nB.2 Model Adjustment with Sigmoids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\nB.3 Gaussian Mixture Model for Distribution Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\nC Motivational Example 15\nD Canonical Neural Network (NN C) 16\nE Algorithms 17\nF Proof of Theorem 18\nG Theoretical Analysis 18\nG.1ωAnalysis with Single Update . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nG.2 Neural Network Learning Feasibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nG.3 Complexity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nH Experimental Settings 20\nI Detailed Evaluations Results 20\nI.1 CPU vs. GPU Training. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nI.2 GMM Impact Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nI.3 Sig2Model Parallelization on Inference Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nI.4 Sensivity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nI.5 Tail Latency Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nI.6 Analysis of Ensemble Size Impact on Lookup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n13\n\nSig2Model: A Boosting-Driven Model for Updatable Learned Indexes\nA Table of Notations\nTable 3: Notations\nDτSorted keys afterτthupdate with sizes τ\nDτ\nemb Embedded keys ofDτwith vectors of sizen\nBτThe single buffer for entire index structure at stageτ\nρ The size of the buffer\nRBτThe neural network replay buffer at stageτ\nκ The size ofRBτ\nXτ\ni ithinput to the neural network at stageτ\nYτ\ni Corresponding label ofXτ\ni\nA Amplitude of Sigmoid function\nω Slope of Sigmoid function\nϕ Center of Sigmoid function\nσ(x, A, ω, ϕ) Parametrized Sigmoid function\nN Maximum number of Sigmoids\nDupdate The distribution of incoming updates\nU Set of new updates drawn fromD update\nMl(.) The LI model afterlthretrain with maximum errorE\nM′\nl(.,Π τ) The adjusted model ofM l(.)with parameterΠ τat stageτ\nEl The maximum estimation error ofM l(.)\nλ Sampling fraction\nΠ Set of sigma sigmoid parameters\nΨ Set of GMM parameters\nK Number of GMM’s kernel\nMThe hypothesis space of Sigma-Sigmoid based models, all\nconfigurations ofNsigmoids, with parameters defined byΠ.\nΓ(k) Determine size gap before givenk\nG Maximum gap between continuous keys\nd Minimum possible distance between keys\nα Model prediction bias factor\nβ Interference factor\nδ Confidence level of initial clustering\nEΠ Prediction confusion parameter\nϵΠ/ϵΨ Error threshold for Sigma-Sigmoid/Incoming updates model\ns Size of data\nB Preliminaries\nB.1 Learned Index (LI)\nThe learned indexes [ 14,7,43,63,38,36,42] aim to improve the efficiency of data retrieval in database systems\nby using machine learning models that map keys to their locations. The traditional learned index employs ensemble\nlearning and hierarchical model organization. Starting from the root node and progressing downward, the model\npredicts the subsequent layers to use for a query key kbased on F(k)×s , where sis the number of keys, and Fis the\ncumulative distribution function (CDF) that estimates the probability p(x≤k) . Given the overhead of training and\ninference in complex models, most learned indexes utilize piecewise linear models to fit the CDF. Querying involves\npredicting the key’s position using pos=a×k+b with a maximum error e, where aandbare learned parameters,\nandeis for the final search to locate the target key.\nUpdatable Learned Index.Learned indexes require a fixed record distribution, making updates difficult (Section 1).\nSolutions for up-datable learned indexes include:(i) delta buffer, (ii) in place, (iii) hybrid structures[ 62,20].Delta\nbuffermethods (e.g., LIPP) use buffers to postpone updates, but merging occurs when buffers overflow.In-place\napproaches (e.g., Alex) reserve placeholders for updates but may cause inefficient searches when offsets fill up.Hybrid\nmethods balance efficiency and speed by combining buffers and placeholders. DILI, a hybrid solution, uses a tree\nstructure for level-wise lookups, but updates increase the tree height over time.\n14\n\nSig2Model: A Boosting-Driven Model for Updatable Learned Indexes\n−10 −5 5 100.511.52\nxσ(x)\nσ(x−2)\nσ(2x)\n2σ(x)\nFigure 8: Sigmoid functionσ(x)transformations.\nB.2 Model Adjustment with Sigmoids\nAs discussed in Section 1, sigmoid functions enable smooth model adjustments by treating small updates as gradual\nchanges, reducing the frequency of retraining. The sigmoid function σ(x) =1\n1+e−xcreates a \"S\"-shaped curve,\ncommonly used in machine learning.\nWhen combined with another function, for example, f(x) +σ(x−ϕ) , it introduces a smooth step-like transition near\nϕ. This property makes sigmoids ideal for approximating stepwise behaviors.\nThe generalized sigmoid, σ(x, A, ω, ϕ) =A\n1+e−ω(x−ϕ) , adds flexibility to control amplitude, slope, and center, enabling\nbroader behavior modeling in learned indexes.\nB.3 Gaussian Mixture Model for Distribution Modeling\nAGaussian Mixture Model(GMM) assumes data are generated from a mixture of Gaussian distributions, each defined\nby a mean and variance. GMMs are flexible and well-suited for modeling complex, multimodal key distributions in\nreal-world workloads. The GMM is mathematically expressed as\nGMM(x,Ψ) =p(x) =KX\ni=1πiN(x|µ i,Σi)(8)\nwhere Kis the number of Gaussian components (kernels) and Ψ ={(π i, µi, σi)}K\ni=1represents the GMM parameters,\nπiis the weight of the i-th component, and N(x|µ i,Σi)is the Gaussian distribution with mean µiand covariance Σi\n(PK\ni=1πi= 1).\nIn learned indexes, GMMs predict update distributions. Each Gaussian component represents a key cluster, enabling the\nprecise placement of placeholders for future updates.\nC Motivational Example\nConsider the LI model M0(k) = 2.5k+ 1.5 with a prediction error of E= 0.25 in the keys domain D0=\n{−0.2,0.3,0.59,0.91} . The predicted indices for the elements in D0yield ID0=M 0(D0) ={1,2.25,2.975,3.775} .\nConsequently, for each I∈I D0, the range of search positions is given as I±E . For example, for the key k= 0.59\nwhere the prediction is M0(0.59) = 2.975 , the search range is 2.975±0.25 , resulting in the interval [2.725,3.225] .\nConsidering that positions are integers, we only search for positions3and4.\nConsider an incoming update u1with a key value of u1= 0.46 . This update alters the domain, D1=\n{−0.2,0.3,0.46,0.59,0.91} , while the index set ID1={1,2.25,2.65,2.975,3.775} . The update does not affect\nthe indices for −0.2 and0.3, so the model M0remains applicable. However, for the elements 2.975 and3.775 , the\nindices are outdated. In essence, the predictions for elements ≤u1remain unchanged, while elements in the domain\nthat are ≥u1are impacted. Furthermore, it is evident that elements > u1experience an exact effect of 1, implying\nthat incrementing their previous prediction by one aligns their indices with the new index. This insight suggests\nthat by implementing a step-like adjustment to M0, we can avoid training a new model M1across D1. By adding\n1\n1+e−48.3(k−0.47) toM0, a new model is generated that produces adjusted outputs M′\n0(k) =M 0(k) +\u0010\n1\n1+e−48.3(k−0.47)\u0011\n.\nThen, IM′\n0(D1)={1,2.25,3.03,3.971,4.775} which using Egives us the correct index for all the keys in D1. Given\nthe second update u2= 0.14 ,D2={−0.2,0.14,0.3,0.46,0.59,0.91} , you might choose to apply another step\nfunction, a second sigmoid, expressed as M′′\n0(k) =M′\n0(k) +\u0010\n1\n1+e−98.7(k−0.15)\u0011\n, or you can fully utilize the capacity\nof the initial sigmoid applied in M′\n0, formulated as M′′′\n0(k) =M 0(k) +\u0010\n2\n1+e−12.6(x−0.33)\u0011\n. In line with theOakum\nrazor principle, we choose M′′′\n0instead of M′′\n0because it is simpler and needs less memory. However, the minimal\nmemory usage isn’t always feasible, as it relies on the interval between (i.e., distribution) updates. Consequently, the\n15\n\nSig2Model: A Boosting-Driven Model for Updatable Learned Indexes\n−0.2 0.14 0.30.460.59 0.91123456\nkIndexM0(k)\nM′\n0(k)\nM′′\n0(k)\nM′′′\n0(k)\nFigure 9: Adjusting the LI model upon receiving updates by employing the sigmoid as a step function.\nsuggested model should account for these intervals when determining the count of step functions (i.e., sigmoids). These\napproximations are shown in Figure 9. This example demonstrates that by employing an appropriate step function, we\ncan modify the LI model without the need to retrain completely on new data. In subsequent sections, we expand this\nconcept and establish a formal learning framework to train the step function.\nD Canonical Neural Network (NN C)\nThis section describes the shared weights of NNC, a core component of the neural networks linked to system parameters\nand update workload distribution. NNCprocesses embedded data to build knowledge memory and representations for\nNN ΠandNN Ψ. The next section details the architecture of NNC, followed by an explanation of data preparation for\nits input and the training process for these networks during initialization and mid-development.\nThe multi-layer neural network NNCprocesses sequential data batches for continuous learning. It employs highway\nnetworks, ReLU activations, and dropout layers to extract patterns and prevent overfitting. To address catastrophic\nforgetting [ 35], two strategies are used:Replay Buffer[ 13] andElastic Weight Consolidation (EWC)[ 39]. The network\noutputs feed into two subnets,NN ΨandNN Π, each handling specific tasks. Key components ofNN Cinclude:\nReplay Buffer ( RB).The replay buffer stores observed data ( D0,B1,B2,. . .,Bτ−1) as 4-ary tuples: key, representa-\ntion, global index, and age. Algorithm 3 updates global indexes for RBτ−1upon new data Bτ, ensuring kMIN and\nkMAX are preserved the updated . Algorithm 4 refreshes the replay buffer after neural networks by adding new data or\nreplacing the oldest entries probabilistically when capacityκis reached.\nHighway Neural Network (2 layers).Input data (new and replayed) is processed through a 2-layer highway network\nto mitigate vanishing gradients, similar to residual connections [ 61]. This approach preserves essential features while\nlearning new ones [65].\nNonlinear Activation (ReLU).ReLU activation introduces non-linearity essential for capturing complex relationships\nand avoids vanishing gradients during backpropagation [46, 55].\nDropout Layer.Dropout randomly disables neurons during training to prevent overfitting and improve generaliza-\ntion [60, 2].\nNeural Network (3 layers).A 3-layer network improves the understanding of complex data relationships, helping\ngeneralization between tasks.\nEWC Regularization.Elastic Weight Consolidation (EWC) [ 48] protects critical parameters during training, mitigating\ncatastrophic forgetting.\nFully Connected to Subnets.The final layer connects to subnets NN ΨandNN Π, each handling specific tasks,\nimproving performance in multi-task scenarios.\n16\n\nSig2Model: A Boosting-Driven Model for Updatable Learned Indexes\nE Algorithms\nAlgorithm 2Greedy Initialization of GMM Parameters\n1:Input:DataD0with Sizes 0, Confidence Levelδ\n2:Output:GMM ParametersΨ 0, Number of KernelsK\n3:Ψ 0← ∅,K←0\n4:while|D0| ≥2do\n5:Select the two smallest elements:k 1, k2∈D0\n6:T← {k 1, k2},D0←D0\\ {k 1, k2}\n7:Construct a GaussianN T=N(avg(T),samplevar(T))\n8:foreach elementk∈D0do\n9:ifP(k∼N T)> δthen\n10:T←T∪ {k}\n11:UpdateN T←N(avg(T),samplevar(T))\n12:D0←D0\\ {k}\n13:else\n14:Ψ 0←Ψ 0∪\u0012\n|T|\ns0,avg(T),samplevar(T)\u0013\n15:K←K+ 1\n16:end if\n17:end for\n18:end while\n19:if|D0| ̸= 0then\n20:Add remaining elements toT\n21:Update the last normal distribution and its coefficient\n22:end if\nAlgorithm 3ReindexRB()ReIndex Replay Buffer Indexes\n1:Input:Updates BufferBτ, Replay BufferRBτ−1={(k 1, r1, I1, a1), . . . ,(k κ, rκ, Iκ, aκ)}\n2:Output:Reindexed Replay BufferRBτ−1, Indexed BufferBτ\nidx\n3:cntr←1, Bτ\nidx← ∅\n4:fori= 1toκdo\n5:whilecntr≤ |Bτ|andBτ[cntr].key < k ido\n6:Add((Bτ[cntr].key, Bτ[cntr] emb, Ii+cntr,0))toBτ\nidx\n7:cntr←cntr+ 1\n8:end while\n9:I i←Ii+cntr\n10:a i←a i+ 1▷Increment age excludingk MIN andk MAX\n11:end for\n12:whilecntr≤ρdo\n13:Add((Bτ[cntr].key, Bτ[cntr] emb, Im+cntr,0))toBτ\nidx\n14:cntr←cntr+ 1\n15:end while\nAlgorithm 4UpdateRB()Update Replay Buffer fromτ−1toτ\n1:Input:Indexed BufferBτ\nidx, Replay BufferRBτ−1, Sampling Fractionλ, Replay Buffer Sizeκ\n2:Output:New Replay BufferRBτ\n3:foreach entryeinBτ\nidxwith probabilityλdo\n4:if|RBτ−1|< κthen\n5:AddetoRBτ−1\n6:else\n7:Replace oldest element inRBτ−1withe\n8:end if\n9:end for\n10:RBτ←SortByKey(RBτ−1)\n17\n\nSig2Model: A Boosting-Driven Model for Updatable Learned Indexes\nF Proof of Theorem\nWe provide the proof for Theorem 1 in this section.\nProof 1 Step 1 (Discrete Spacing).By hypothesis, for any two consecutive keys ki< k i+1, the gap satisfies\n1≤\f\fM′(ki+1,Π)−M′(ki,Π)\f\f≤2. Hence, if we list keys kin ascending order, each key kcan have only a\nsmall number of “neighboring keys” k±for which |M′(k,Π)−M′(k±,Π)|<2 . In particular, for EΠchosen in\nthe range [1,2) , only the same key kor its one or two immediate neighbors in the sorted order of keys can satisfy\n|M′(k,Π)−M′(k±,Π)|< E Π.\nStep 2 (Choose EΠ≥1).We pick EΠ≥1. Because each key khas at most O(1) neighboring keys whose model\noutputs lie within EΠ, the event\f\fM′(k,Π)−M′(u,Π)\f\f< E Πcan only occur if uis either kitself or one of those few\nneighbors. Let Neighbor(k, E Π)denote this (small) set of possible neighbors of k. Then\f\fM′(k,Π)−M′(u,Π)\f\f<\nEΠ=⇒u∈ {k} ∪Neighbor(k, E Π).\nStep 3 (Bound the Probability).Since the set {k} ∪Neighbor(k, E Π)is small and fixed for each k, the probability that\na randomu∼ D update lands in that set is bounded above by some function of its measure or cardinality. Specifically,\nPh\f\fM′(k,Π)−M′(u,Π)\f\f< E Πi\n≤Ph\nu∈ {k} ∪Neighbor(k, E Π)i\n.\nUnder assumption bounded Dupdate , there exist a finite βand we can make this probability ≤β (e.g., βcan be chosen\nbased on density).\nThus, there is anE Π≥1such thatP\u0002\f\fM′(k,Π)−M′(u,Π)\f\f< E Π\u0003\n≤β,completing the proof.\nG Theoretical Analysis\nThis section addresses two theoretical aspects of Sig2Model: (1) how individual updates affect ωin the sigmoid\napproximation and (2) the neural network’s feasibility in achieving optimal sigmoid-based approximation. We define\nϵas the maximum error within M’s domain caused by the sigmoids’ gradual transition from 0to their maximum A.\nLastly, we analyze the time complexity of Sig2Model’s main algorithms and update process.\nG.1ωAnalysis with Single Update\nThis section analyzes the behavior of ωfor a constant A≥1 and an update upositioned at the center of the sigmoid.\nAssuming Doriginates from a uniform domain and is large enough to reflect this distribution, we study ωfor a specific\nerror levelϵ. For an updateubetweenk iandk i+1:\narg minω: max{M(k i+1)−M′(ki+1) + 1, M′(ki)−M(k i)} ≤ϵ(9)\nTheorem 2 For the adjustment model M′and error ϵ >0 , and a random update minD < u <maxD , we have,\nE[ω]≤2(|D|−1)\nmaxD−minDln\u0000A−ϵ\nϵ\u0001\n.\nProof 2 Letki< u < k i+1,i∈(1 :n−1) , and define das the minimum distance from uto its neighboring elements:\nθ= min{u−k i, ki+1−u}, k= arg min\nki,ki+1{u−k i, ki+1−u}.\nIn a uniform distribution, the distance between two elements is also uniformly distributed. If X∼uniform(a, b) ,\nthen for x, x′∼X , the random variable Y=|x−x′|follows Y∼uniform(0, b−a) . Thus, the distribution of θis\nuniform(0,maxD−minD). For|D|elements:\nE[quantile] =maxD−minD\n|D|−1. (10)\nSinceufalls into one of the|D| −1quantile:\nθ∼uniform\u0000\n0,E[quantile]\u0001\n.(11)\nAssume u= 0 andk=k i(left side closest). Using the sigmoid symmetry, the same analysis applies for k=k i+1.\nThen,M′(θ) =M(θ) +A\n1+eωθ. From inequality 9:\nA\n1 +eωθ≤ϵ→1 +eωθ≥A\nϵ→eωθ≥A\nϵ−1→\nln\u0012A\nϵ−1\u0013\n≥ωθ→ω≤1\nθln\u0012A\nϵ−1\u0013\n.\n18\n\nSig2Model: A Boosting-Driven Model for Updatable Learned Indexes\nUsing E[θ] from Eq. 11: E[ω]≤2\nE[quantile]ln\u0000A−ϵ\nϵ\u0001\n.Substituting Eq. 10 gives the result. This conclusion applies\nsymmetrically to the right side (k=k i+1).\nThis shows the system numerically bounded and parameters change are monotone as the system receives updates.\nG.2 Neural Network Learning Feasibility\nThis section provides a theoretical framework to show that the proposed neural network operates within a feasible\nsolution space for optimal solutions. We derive a feasibility condition and prove that a parameter configuration satisfying\nit always exists. Even in the worst case, where each sigmoid covers a single update, the sigma-sigmoid modifications\nensure the total error near the update is limited toϵ.\nThe minimum distance drepresents the densest region, where updates affect the center with distance dfrom it. Assuming\nthe closest key is at the left boundary,−d,−2d,−3d, . . ., the effect ofσ(0, A,1, ϕ)on the index prediction is:\n|U|−1P\ni=0A\n1+eω.d.i≤ϵ. (12)\nNote that the size update is the same as buffer size|U|=ρ.\nLemma 1Given Equation 12 withω >0,d >0, andϵ >0, the upper bound for|U|is:\nρ=|U| ≤2\nωdln\u0012\nAeωdϵ\nA−A\nϵ\u0013\n. (13)\nProof 3 Letf(x) =1\n1+eωdx. Using the Euler-Maclaurin formula, we approximate the sum:|U|−1P\ni=0f(i)≈\nR|U|−1\n0f(x)dx+1\n2[f(0) +f(|U| −1)] . Focusing on the integral:R|U|−1\n0f(x)dx=1\nωd[ln(1 +eωd(|U|−1))−ln(2)] .\nThen, given|U|−1P\ni=0f(i)≤ϵ, we derive:|U| ≤1\nωdln(2eωdϵ−1) + 1≈2\nωdln\u0000Aeωdϵ\nA−A\nϵ\u0001\n, so this completes the proof.\nTheorem 3For anyρ=|U| ≥2,d >0, andϵ >0, there exists a configuration ofωandAsatisfying Equation 13.\nProof 4A=ϵ simplifies the inequality |U| ≤2\nωdln\u0000\neωd−1\u0001\n. The function f(ω) =2\nωdln(eωd−1) is continuous\nforω >0and diverges asω→0+. Thus, for any finite|U| ≥2, there exists anω >0satisfying the inequality.\nTheorem 3 shows that the system can achieve a parameter configuration that reduces the approximation error, regardless\nof the update(buffer) size. If the system fails after extensive iterations ( MaxIter in Algorithm 1), retraining (Section 4.2)\nbecomes necessary, not due to the capacity of the Sigma-Sigmoid model.\nG.3 Complexity Analysis\nUpdates Time Complexity.As shown in Figure 10, Sig2Model employs a multi-stage approach to handle updates\nwhile minimizing retraining frequency. The system first attempts to insert new updates into available placeholders using\nGaussian Mixture Model (GMM) allocation, which has a constant time complexity of O(1) . When no placeholders\nremain, updates are stored in a B+tree buffer with logarithmic time complexity O(logρ) , where ρrepresents the buffer’s\nmaximum capacity. Once the buffer reaches capacity ( ρupdates), Sig2Model performs incremental integration of the\nbuffered updates into the model using SigmaSigmoid boosting. This operation has a time complexity of O(s+ρlogρ) ,\nwhere sdenotes the current size of the index array. Finally, when the number of active SigmaSigmoids reaches the\nsystem’s capacity N, Sig2Model initiates a full retraining of the RadixSpline model with time complexity O(NlogN) ,\nwhereN=s+ρrepresents the total data size (existing data plus buffered updates).\nDelayPlaceholderRetrainModel AdjustmentBuﬀer<latexit sha1_base64=\"iXrpbbPX+tr2szjJ+xNpFxhPZ6M=\">AAADMHicbVJNbxMxEHWWrxK+Ujhw4LIiQkpFFSUVasutEhduFIm0lZIosr2zu1bstWXP0kSr/BqucOHXwAlx5VfgTRYJpxlptc9vxvK8ecOMFA4Hg5+t6NbtO3fv7d1vP3j46PGTzv7TC6dLy2HEtdT2ilEHUhQwQoESrowFqpiESzZ/V+cvP4N1QhefcGlgqmhWiFRwip6adZ5/6PXc64nN9cFE6uwfPph1uoP+YB3xTTBsQJc0cT7bj1qTRPNSQYFcUufGpwanFbUouIRVe1I6MJTPaQZjDwuqwE2rtYBV/MozSZxq678C4zX7/42KKueWivlKRTF327ma3JUbl5ieTitRmBKh4JuH0lLGqON6GnEiLHCUSw8ot8L3GvOcWsrRzyx4hWk9R8pcoKRabAQEXGapyQVfhKwqJQqrr0PWlYxTU1ux2qkpIL2PzNumwmHaUkJyWPvvilIxsJD4sctMezm5OgpHX2FuwR8sei1btlQmS43U6Ha2cthYUP8RFhgWMea7avulGW6vyE1wcdQfHvePP77pnr1t1mePvCAvSY8MyQk5I+/JORkRTlbkC/lKvkXfox/Rr+j3pjRqNXeekSCiP38BemQObQ==</latexit>O((s+⇢) log(s+⇢))\n<latexit sha1_base64=\"LH56BHCt0aJ9oOB9SiR6R4o1N0o=\">AAADKnicbVJNbxMxEHWWrxK+EjhyWREhFVFFSYVKuVXiwo0ikbZSEkW2M7trxV5b9pgmWuWncIULv4ZbxZUfgjfZA04z0spv34zlefOGGSkcDgY3reTO3Xv3Hxw8bD96/OTps073+YXT3nIYcS21vWLUgRQljFCghCtjgSom4ZItPtb5y29gndDlV1wZmCqalyITnGKgZp3u50P3dmILPZE6r883s05v0B9sIr0Nhg3okSbOZ92kNZlr7hWUyCV1bnxqcFpRi4JLWLcn3oGhfEFzGAdYUgVuWm1aX6evAzNPM23DV2K6Yf+/UVHl3EqxUKkoFm43V5P7cmOP2em0EqXxCCXfPpR5maJO6zmkc2GBo1wFQLkVodeUF9RSjmFa0StM6wVS5iIl1XIrIOJyS00h+DJmlZcorL6OWecZp6Y2Yb1XU0QGB1kwTMXDtF7C/Kh23pVeMbAwD2OXuQ5yCnUcj77CwkL4sRi07NhSmTwzUqPb28pRY0F9IiwxLmIsdNUOSzPcXZHb4OK4Pzzpn3x51zv70KzPAXlJXpFDMiTvyRn5RM7JiHByTb6TH+Rn8iv5ndwkf7alSau584JEkfz9BxIdDPE=</latexit>O(s+⇢log⇢)\n<latexit sha1_base64=\"BIw3vhJKyQ76I89eJh0FGye5qt4=\">AAADInicbVJNaxsxEJW3X6n7lbTHXpaaQgrB2KGk6S2QS29NoU4CtgmSdnZXWFoJadTaLP4ZvSaX/JreSk+F/phq7T1UjgeWfXozQvPmDTNSOBwM/nSSe/cfPHy087j75Omz5y92916eO+0thxHXUttLRh1IUcEIBUq4NBaoYhIu2Oy0yV98A+uErr7iwsBU0aISueAUAzX+vD+RupjYUr+72u0N+oNVpHfBsAU90sbZ1V7SmWSaewUVckmdGx8bnNbUouASlt2Jd2Aon9ECxgFWVIGb1quWl+nbwGRprm34KkxX7P83aqqcWygWKhXF0m3mGnJbbuwxP57WojIeoeLrh3IvU9Rpoz/NhAWOchEA5VaEXlNeUks5hilFrzCtZ0iZi5TU87WAiCssNaXg85hVXqKw+nvMOs84Nc3wl1s1RWRwjgWjVDxM6yVkB43jrvKKgYUsjF0WOsgp1WE8+hpLC+FgMWjZsKU2RW6kRre1lYPWguaPMMe4iLHQVTcszXBzRe6C88P+8Kh/9OV97+Rjuz475DV5Q/bJkHwgJ+QTOSMjwokmP8g1uUluk5/Jr+T3ujTptHdekSiSv/8AVkgKQQ==</latexit>O(log⇢)\n<latexit sha1_base64=\"C0DoeJff0gdVJhbaFIYPmqD9L7c=\">AAADG3icbVJNbxMxEHWWrxK+WjhyWREhFamKshUq5VaJCzeKRNpKaVTZ3tldK/bassdtolX+Ale48Gu4Ia4c+Dd4kz3gNCOt9vnNWJ43b5iRwuFo9LeX3Ll77/6DnYf9R4+fPH22u/f8zGlvOYy5ltpeMOpAihrGKFDChbFAFZNwzmYf2vz5NVgndP0FFwamipa1KASn2FKf9rM3V7uD0XC0ivQ2yDowIF2cXu0lvctcc6+gRi6pc5Njg9OGWhRcwrJ/6R0Yyme0hEmANVXgps2q12X6OjB5WmgbvhrTFfv/jYYq5xaKhUpFsXKbuZbclpt4LI6njaiNR6j5+qHCyxR12gpPc2GBo1wEQLkVodeUV9RSjmE80StM6xlS5iIlzXwtIOJKS00l+DxmlZcorL6JWecZp6ad+nKrpogMlrHgkIqHab2E/KC12tVeMbCQh7HLUgc5lTqMR99gZSEcLAYtG7Y0piyM1Oi2tnLQWdD+EeYYFzEWuuqHpck2V+Q2ODscZkfDo89vByfvu/XZIS/JK7JPMvKOnJCP5JSMCScV+Uq+ke/Jj+Rn8iv5vS5Net2dFySK5M8/7KQG6Q==</latexit>O(1)Sig2ModelUpdateCost\nFigure 10:Approaches to delaying retrain categorized by insertion cost.sis the size of data andρis buffer size.\n19\n\nSig2Model: A Boosting-Driven Model for Updatable Learned Indexes\nAlgorithms Time Complexity.The time complexity of the algorithms in Sig2Model is as follows: (1) Lookup:\nO(Model+N+ log(E τ+ϵ)) , as it requires inference using the learned index model, followed by Πparameters with\nNsigmoids, and a final search of the data. (2) GMM clustering (Algorithm 2): O(|D0|), which requires one pass over\nthe initial data. (3) Reindexing reply buffer (Algorithm 3): O(κ+ρ) , requiring a pass over both the buffer and the\ncurrent reply buffer. (4) Updating the reply buffer (Algorithm 4) from rb(t−1) torb(t) :O(ρ) , requiring a single pass\nover the buffer size.(4) Construction Cost: The adjustment model requires a memory complexity of O(N) , if we have\nNN Ψmodule complexity change to O(N+K) . Additional overheads arise from tasks such as generating the initial\nindex model,M 0, trained on the datasetD0. These overheads depend on the primary index modeling; in our case, we\nused RadixSpline [ 38]. Additionally, the process of training a neural network on D0incurs a computational complexity\nofO(MaxIter× |D0|).\nH Experimental Settings\nEnvironment.Sig2Model is implemented in C++17 and compiled with GCC 9.0.1. We use PyTorch for the neural\nnetwork implementation [ 53]. The evaluation is performed on an Ubuntu 20.04 machine with an AMD Ryzen\nThreadRipper Pro 5995WX (64-core, 2.7GHz) and 256GB DDR4 RAM, and a data-centered GPU with 40GB vRAM.\nDatasets.Sig2Model is assessed using three SOSD benchmark datasets [ 37]:(1) FB [ 10]:200M Facebook user\nIDs,(2) Wiki [ 11]:190M unique integer timestamps from Wikipedia logs,(3) Logn:200M values sampled from a\nlog-normal distribution ( µ= 0 ,σ= 1 ). Key-value pairs are pre-sorted by key before Sig2Model initialization. All\nexperiments use 8-byte keys from the datasets with randomly generated 8-byte values.\nBaselines.We compare Sig2Model against:(1) B+Tree:A standard STX B+Tree [ 5],(2) Alex [ 14]:An in-place\nlearned index,(3) LIPP [ 41]:A delta-buffer learned index,(4) DILI [ 45]:A hybrid index combining in-place and\ndelta-buffer methods. Open-source implementations are used for comparisons.\nWorkloads.QPS is measured on four workloads:(1) Read-Only:100% reads,(2) Read-Heavy:90% reads, 10% writes,\n(3) Write-Heavy:50% reads, 50% writes,(4) Write-Only:100% writes. Read/write scenarios interleave operations\n(e.g., 19 reads per write in read-heavy). The keys are randomly selected using a Zipfian distribution [3].\nMetrics.Evaluation metrics include:QPS:Average operations per second,Latency:99th percentile operation latency,\nIndex size:Combined size of the index and neural network model.\nSystem Parameters.System parameters are tuned by sensitivity analysis: buffer size ( ρ= 1000 ), replay buffer size\n(κ= 500 ), sigmoid capacity ( N= 20 ), RadixSpline error range ( 128) [38], confidence level ( δ= 0.95 ), regularization\nparameters ( ν= 0.5 ,c=γ= 1 ),MaxIter= 100 ,ϵΠandϵΨ(Algorithm 1) both set to 0.01, sampling fraction\n(λ= 0.1 , Algorithm 4), and regularization coefficients in Equations 5 and 7 set to 1. The value dis empirically\ndetermined for the initial data (D0) of each dataset.D0size (s 0) is50%of the respective dataset size.\nI Detailed Evaluations Results\nI.1 CPU vs. GPU Training.\nGPU training significantly outperforms CPU training, especially as the number of sigmoids Nincreases. Due to parallel\nprocessing, GPUs scale more efficiently, widening the performance gap at higher N. Table 4 shows this trend—while\nGPU time grows moderately, CPU time increases steeply, making GPUs essential for larger model capacities.\nTable 4: Training Time Comparison: CPU vs. GPU (in milliseconds)\nN CPU (ms) GPU (ms)\n1 15 17\n5 241 28\n10 399 56\n20 955 108\n50 4705 174\nI.2 GMM Impact Analysis.\nTable 5 compares Sig2Model GMM (GMM-based placeholder placement) with Sig2Model rand (random placement).\nSig2Model GMM has 32% higher update latency and 19.5% more memory usage on average, as it strategically places\nplaceholders based on predicted update distributions. In contrast, Sig2Model rand randomly places slots, leading to\nmany unused placeholders. The lowest increase in latency and memory usage is observed for the Logn dataset due to its\ncomplex distribution.\n20\n\nSig2Model: A Boosting-Driven Model for Updatable Learned Indexes\nTable 5: Normalized update latency and memory usage of Sig2Model rand overSig2Model GMM on the write-heavy\nworkload over different datasets.\nDataset Latency Memory\nWiki 43.2% 34.6%\nLogn 16.3% 4.2%\nFB 36.7% 19.9%\nI.3 Sig2Model Parallelization on Inference Module\nSince the Sigma-Sigmoid model boosted by multiple weak sigmoid learners, its computations can be parallelized across\nmultiple threads, improving performance by distributing the workload. Ideally, the number of threads can be increased\ntoN+ 1; however, there is a trade-off between the benefits of parallelization and I/O contention.\nTo evaluate the impact of thread parallelization, we varied the number of threads to process Sigmoid components in the\nSig2Model. Results, averaged over 5runs (Figure 11), show overhead dropping sharply to 1.0% at7threads. Beyond\nthis, performance degrades slightly due to result aggregation costs.\n# ThreadsM′Overhead (%)\nbaseline(M)02467.2\n5.2\n3.4\n1.6\n1.01.44.45.1\n2467 17 12 20 =N\nFigure 11: Overhead M′compared to Mas a function of the number of threads. The overhead drops sharply until 7\nthreads, then degrades slowly due to thread coordination costs.\nAt zero threads means single thread for all, the overhead is 7.2% , decreasing rapidly with more threads. However, after\n7threads, degradation begins, reaching 5.1% at20threads (equal to N). This highlights that while threading improves\nperformance, excessive synchronization can negate its benefits.\nI.4 Sensivity Analysis\nWe performed extensive sensitivity analysis on key hyperparameters. We run the experiments on the full version of\nSig2Model on the Wiki Dataset with Read-heavy workload, and generalized these results for all our experiments.\nTable 6: Sensivity Analysis Results\nParameter (Tested Values) Value 1 Value 2 Value 3 Value 4\nSigmoid SizeN 3.2 (-25.2%) 3.9 (-8.6%) 4.3 (opt) 4.1 (-2.9%)\n(5, 10, 20, 40)\nBuffer Sizeρ 4.0 (-5.4%) 4.1 (-2.6%) 4.3 (opt) 4.2 (-0.8%)\n(250, 500, 1000, 2000)\nReplay Bufferκ 4.2 (-1.2%) 4.3 (opt) 4.2 (-0.4%) 4.2 (-0.9%)\n(250, 500, 1000, 2000)\nRegularization Coeff. 4.1 (-2.9%) 4.2 (-0.6%) 4.2 (-0.2%) 4.3 (opt)\n(0, 0.5, 0.75, 1)\nError Thresholdsϵ 4.3 (opt) 4.2 (-0.8%) 4.1 (-2.3%) 4.0 (-5.0%)\n(0.1, 0.2, 0.4, 0.8)\nI.5 Tail Latency Analysis\nWe provide comprehensive p99 latency measurements relative to median latency across different ensemble sizes ( N)\nusing our optimal 7-thread configuration on the Wikipedia read-heavy workload.\nTable 7: p99 Latency Reduction over DifferentN\nEnsemble SizeN 5 10 20 40\np99 Latency Reduction -38.2% -44.6% -60.4% -87.2%\n21\n\nSig2Model: A Boosting-Driven Model for Updatable Learned Indexes\nWe also run the same experiment on the other baselines (Table below). Our analysis reveals that while all learned index\nsystems exhibit some tail latency degradation due to retraining, Sig2Model demonstrates significantly better behavior\n(60.4% reduction vs 92-127% for baselines).\nTable 8: p99 Latency Reduction over Different Baselines\nBaseline ALEX LIPP DILI Sig2Model (N= 20)\np99 Latency Reduction -92.0% -108.1% -127.7% -60.4%\nThis improvement stems from two key factors: (1) LIPP and DILI require frequent retraining (approximately every 500\nupdates), (2) While ALEX retrains less frequently, each retraining event incurs substantially higher latency. Our design\nachieves better tail latency by distributing the retraining overhead more evenly through the neural joint optimization\nframework.\nI.6 Analysis of Ensemble Size Impact on Lookup\nThe larger ensemble sizes ( N) linearly increase inference time. We address this concern through both empirical analysis\nand architectural optimizations. First, our sensitivity analysis on the Wiki dataset (read-heavy workload) reveals that\nQuery-per-second (QPS) metric improves with larger Nup to 20, as the benefits of deferred retraining outweigh the\nlatency costs. At N= 40 , we observe a 2.9% QPS drop (see Table 9), confirming that excessively large ensembles can\nnegatively impact performance.\nTable 9: QPS performance for different ensemble sizesN\nEnsemble SizeN 5 10 20 (optimal) 40\nPerformance (MQPS) 3.2 (-25.2%) 3.9 (-8.6%) 4.3 4.1 (-2.9%)\nThe choice of N= 20 represents a careful balance between update agility and lookup performance. While larger\nensembles could theoretically provide greater update capacity, our experiments confirm that N= 20 delivers optimal\nthroughput for read-heavy workloads while still offering substantial improvements in update efficiency compared to\ntraditional learned indexes.\nSecond, the parallelizable nature of the SigmaSigmoid architecture effectively compensates for the increased computa-\ntion. As detailed in Appendix I.3, distributing the workload across just 7 threads reduces the parallelization overhead\nto a negligible 1.0% for N= 20 . This demonstrates that with proper implementation, the latency impact becomes\npractically insignificant for reasonable ensemble sizes.\n22",
  "textLength": 126204
}