{
  "paperId": "109e2c97c2aaeee86d1bfb348e801243d56efa7c",
  "title": "Learned Lock-free Search Data Structures",
  "pdfPath": "109e2c97c2aaeee86d1bfb348e801243d56efa7c.pdf",
  "text": "LEARNED LOCK-FREE SEARCH DATA STRUCTURES\nGaurav Bhardwaj\nIndian Institute of Technology\nHyderabad.\ncs19resch11003@iith.ac.inBapi Chatterjee\nIndraprastha Institute of Information Technology\nDelhi.\nbapi@iiitd.ac.in\nAbhinav Sharma\nIndraprastha Institute of Information Technology\nDelhi.\nabhinav19006@iiitd.ac.inSathya Peri\nIndian Institute of Technology\nHyderabad.\nsathya p@cse.iith.ac.in\nSiddharth Nayak\nIndraprastha Institute of Information Technology\nDelhi.\nsiddharth22128@iiitd.ac.in\nABSTRACT\nNon-blocking search data structures offer scalability with a progress guarantee on high-performance\nmulti-core architectures. In the recent past, ”learned queries” have gained remarkable attention. It\nrefers to predicting the rank of a key computed by machine learning models trained to infer the\ncumulative distribution function of an ordered dataset. A line of works exhibits the superiority of\nlearned queries over classical query algorithms. Yet, to our knowledge, no existing non-blocking\nsearch data structure employs them. In this paper, we introduce Kanva , a framework for learned\nnon-blocking search. Kanva has an intuitive yet non-trivial design: traverse down a shallow hier-\narchy of lightweight linear models to reach the ”non-blocking bins,” which are dynamic ordered\nsearch structures. The proposed approach significantly outperforms the current state-of-the-art –\nnon-blocking interpolation search trees and elimination (a,b) trees – in many workload and data\ndistributions. Kanva is provably linearizable.\nKeywords lock-free, concurrent data structures, learned index, non-blocking\n1 Introduction\nDynamic hierarchical search data structures are the primary methods for predecessor and range queries. Today’s\nmulticore processors provide a natural hardware platform for the scalability of such algorithms with adaptability to\nstreaming settings. The shared locks are the first approach to correctly translate a sequential data structure to a con-\ncurrent shared-memory system. However, irrespective of granularity, locks are prone to pitfalls such as deadlock,\nconveying, etc. By contrast, the lock-free (non-blocking) progress ensures that some non-faulty threads finitely com-\nplete their operations. While a progress guarantee is desirable, consistency of concurrent operations is a necessity.\nThe most popular consistency framework is linearizability [23], i.e., every concurrent operation takes effect at an\natomic step between its invocation and return. In this work, we focus on the lock-free linearizable implementation of\nhierarchical search data structures, oft-represented by search trees, for predecessor queries.\nStarting from Ellen et al. [17] in PODC 2010, several lock-free binary search trees (BSTs) were proposed over the\nlast decade: Howley and Jones [24], Natarajan and Mittal [31], Chatterjee et al. [14], Brown et al. [8]. Arbel-Raviv et\nal. [4], Natarajan et al. [32], and Brown et al. [11] further improved the results. While BSTs remained popular, during\nthe same period, other lock-free hierarchical search structures were also proposed: k-ary search tree [9], B+tree [6]\nand variants such as BW-tree [41], and skip-lists with batch updates [27]. Further, the lock-free interpolation searcharXiv:2308.11205v1  [cs.DC]  22 Aug 2023\n\nLearned Lock-free Search Data Structures\ntree (C-IST) by Brown et al. [10] presented impressive empirical performance along with good theoretical guarantees\n(see Section 7). Later, elimination (a, b)-tree [38], which is a lock-based variant of classical B+tree with variable\n(between aandb) number of keys in nodes, claimed outperforming the then existing concurrent search trees. These\nworks generally consider membership queries with concurrent updates in the data structures. The range queries mostly\nreceived generic approaches in the literature for inclusion into concurrent linked lists and BSTs: [13], [3], [33], [42],\n[37].\nIncidentally, the last decade also witnessed an unprecedented all-pervasive proliferation of machine learning (ML)\ntechniques, influencing query algorithms. Kraska et al. [28] proposed a new perspective on search queries: obtain\nthe approximate rank of a key using ML models trained to predict the cumulative distribution function of data. The\napproximation error would work as a tunable hyperparameter to define bounded proximity for the key’s location.\nKraska et al. [28] named the algorithm recursive model index (RMI) , and the general approach a learned index . RMI\ndid not allow dynamic updates. The experiments of [28] showed that RMI performed up to 3x better than B+ trees for\nrange queries.\nSubsequent works on learned indexes incorporated dynamic updates, for example, ALEX [16] and PGM-index [18],\nthat use a hierarchy of learned linear regression models to predict the approximate key-rank. ALEX keeps gaps in the\nsorted data array for dynamic updates and splits them when needed. On the other hand, PGM-index uses a strategy\nsimilar to log-structured merge-trees (LSM-tree) [34] wherein an insert causing overflow at one data structure level is\npushed to another level with a bigger storage capacity. A similar approach was also adopted by Radix-spline [26].\nDynamic updates in learned indexes naturally motivated exploring their concurrent design. As the first approach,\nXIndex [39] used locks on RMI of [28] with augmented buffers for each key segment to ingest dynamic updates. With\nsuch a design, a query searches for a key in a segment and the associated buffer; while the former uses a learned model,\nthe latter employs the traditional comparison-based search. When the buffer is full, it is merged with its segment under\na barrier. Thereon, the RMI is retrained.\nThe model retraining under barrier-based synchronization of [39] was improved upon by FINEdex [29]. They proposed\na heuristically built shallower learned index fitted with low-depth B-trees, called bins, for ingesting dynamic updates.\nOn reaching a storage threshold, the bins are replaced by local ML models under fine-grained locks. This allows two\ndifferent segments to receive updates and model retraining concurrently (which they seem to have claimed as non-\nblocking). However, at the data structure level, the progress guarantee is still blocking. The concurrent operations in\nFINEdex are not linearizable. Empirically, FINEdex outperforms XIndex across various workloads.\nBoth XIndex and FINEdex support range queries concurrent with updates and membership queries. Though XIndex\ndiscusses limited linearizability of update and membership operations, it is not clear if these operations are linearizable\nin FINEdex. It is important to note here that range queries in both XIndex [39] and FINEdex [29] are not linearizable .\nThis paper introduces Kanva , a lock-free learned index structure that supports concurrent linearizable membership,\nrange and update operations. More specifically,\n1. we present a formal notion of search data structures (Section 2) to review the existing concurrent and learned\nindexes under a common framework. (Section 7).\n2. we describe the design of Kanva, which integrates learned queries and lock-free synchronization techniques to\nenable fast linearizable reads, updates, range search, and model retraining without any buffer, lock, or barrier. The\ncore of our linearizable range search is a non-trivial versioning approach for model arrays. (Section 3).\n3. unlike FINEdex, our local model training is hyperparameter free; this is enabled by a clever use of exponential\nsearch for the last mile exploration. (Section 3).\n4. We extensively evaluate the presented algorithm. Kanva outperforms state-of-the-art lock-free C-IST [10] by up\nto two orders of magnitudes across the workload. It also significantly outperforms the elimination (a, b)tree.\nConcerning learned rage queries, Kanva significantly outperforms FINEdex while offering linearizability. The\ncode is available at https://anonymous.4open.science/r/Kanva-CD83 (Section 6).\n5. We discuss why the future of lock-free search structures should be learned. (Section 8).\n2 Preliminaries\nWe consider a Machine Learning Model , often called a model for brevity, representing the learning outcome of\nan ML algorithm on a dataset. Formally, a model is a set of parameters x, which is implemented as a vector in\nd-dimensional real space: x∈Rd.\nWe consider an Abstract Data Type (ADT) Athat is a set of operations {INSERT (K, V ), D ELETE (K),\nSEARCH (K), RANGE (K, r)}on a key-value store S= (K,V), where Kis a partially ordered set, also called the\n2\n\nLearned Lock-free Search Data Structures\ndataset andVis a set of objects such that there is a bijection between KandV. More specifically, (i) An I NSERT (K, V )\ninserts the key KtoKand the associated value VtoVand returns true if K /∈ K otherwise if K∈ K andV /∈ V,\nit inserts VtoVand returns true, and if K∈ K ∧ V∈ V it returns false without any updates in either KorV, (ii) A\nDELETE (K)deletes the key KfromKand its corresponding value V∈ V ifK∈ K; ifK /∈ K, it returns false\nwithout any modifications to either KorV, (iii) A S EARCH (K)searches the key KinKand return the corresponding\nV∈ V ifK∈ K; otherwise, it returns null, and (iv) A R ANGE (K, r)returns the set VR={Vi} ⊂ V corresponding\ntoKR={Ki} ⊂ K such that K≤Ki≤K+r; if no such Kiexists, it returns null.\nWe employ a Search Data Structure Dto implement the ADT A. For efficient implementation of ADT operations\noverS,Dis equipped with an indexI. A specific data point R∈ K is designated as the root ofD.A Hierarchical\nIndex is a hierarchy of arithmetic or logical expressions to compute the query path from the root R∈ K to the\npotential location of a query key K∈ K. Formally, it can be expressed as a finite ordered set E={ei}r\ni=1, where\neacheiisan arithmetic or logical expression . Update operations – I NSERT (K, V )and D ELETE (K)modify Ein a\ndynamic setting.\n90 250\n37 243\n13 17 25 47 53 91 99Index\nData Nodes\n(a)\nModel 1.1\nModel 2.1 Model 2.2  Model 2.3  \n13\nModel 3.1 Model 3.2  Model 3.3  Model 3.4  \n 13 17 25 47 53 91 99 125 112 103 76 31 61 109 136 148Index\nData Nodes\n (b)Figure 1: (a) Traditional Index (b) Learned Index\nA Comparison-based Index , often referred to as traditional orclassical index , primarily uses logical operations\n<,≤,=,≥, >, to determine the query path. Each eicomprises one or more logical operations along with some\ndata. For example, in a binary search tree, each internal node, as a representative ei, will contain a key xalong with\nlogical operations <,≥.\nRoot\nInternal Node Internal Node Bin\nBin Bin Bin Bin Internal Node\nBin Bin Bin BinBin\n(a)\nBn+1Keys\nBinsVersionsModelInternal NodeRoot\n (b)\nModels\nBn+1Keys\nBinsV ersions (c)\nTwo-Level Bin20 40 60 80\nLinked List\n60\nLinked List\n20 40 60 80Linked ListArray\nFullOne-Level Bin\nFull\nInternal Node (d)\nFigure 2: Kanva Design structure: (a) Shallow Hierarchy (b) An internal node (c) The root node (d) The lifecycle of a\nBin\nA Learned Index uses arithmetic expressions that are machine learning models to compute the query path. In essence,\nthe models E={ei}r\ni=1together approximate the cumulative distribution function (CDF) Fof the set K. If˜Fis an\napproximation to F, then the potential position of a key K∈ K will belong to the interval\n[˜F(K)× |K| − ϵ,˜F(K)× |K| +ϵ],\n3\n\nLearned Lock-free Search Data Structures\nwhere ϵis a tunable hyperparameter related to the approximation error. An important difference between a comparison-\nbased index and a learned index, as shown in Figure 1, is that a query path from root R∈ K to a key K∈ K is unique\nin the former, whereas in the latter, in some designs, for example in RMI [28], there can be multiple query paths\ndepending on the inference by different models in the hierarchy.\nAlgorithms Learned UpdatesData\nSortedConcurrent Insert/Remove/SearchRange QueriesWrite Retrain Lock-free Linearizable\nC-IS-Tree [10] ✗ ✗ Not present\nLF(a, b )-Tree [7] ✗ ✗ Not present\nElim. (a, b )-Tree [38] ✗ ✗ ✗ Not present\nRMI [28] ✗ ✗ Sequential\nFITing Tree [19] ✗ ✗ Sequential\nALEX [16] ✗ ✗ Sequential\nPGM-Index [18] ✗ ✗ Sequential\nXIndex [39] ✗ ✗ Blocking Not Lin-\nearizable\nFINEdex [29] ✗\n✗Blocking Not Lin-\nearizable\nKanva (ours) Lock-free Lin-\nearizable\nTable 1: Concurrent and Learned Indexes\nThe Shared Memory System considered in this work supports atomic read ,write , and CAS (compare-and-swap)\nprimitives and comprises a finite set of threads {ti}p\ni=1.\nA linearizable Lock-free Data Structure implements the ADT operations in a shared memory system such that in\nevery concurrent execution of operations, the effect of each of them will be visible to a user between their invocation\nand response to ensure linearizability. Furthermore, in such executions, at least one non-faulty operation will finish in\na finite number of steps, irrespective of the behavior of other concurrent operations, to ensure lock-freedom .\nA linearizable Learned Lock-free Data Structure is an updatable ordered set of expressions E={ei}r\ni=1, where\nsome or all of eis are machine learning models. As implemented on a shared memory system, the ADT operations in\nsuch a data structure are linearizable and lock-free.\nTable 1 summarizes relevant concurrent, learned, and concurrent & learned indexes.\n3 Kanva Data Structure Design\n3.1 Layout and Operations basics\nStructurally, Kanva capitalizes on FINEdex [29] to implement a linearizable lock-free dynamic key-value store. Kanva\nimplements an ADT that we presented in the last section. Our approach for linearizable range search requires pairing\nthe values with their update timestamps drawing from [42]. For this purpose, we associate a list of objects packing\na value and its corresponding update timestamp to a key. We refer to this object as vValue or versioned-value. The\ndesign and evolution of Kanva is shown in Figure 2.\nModels: We use linear regression models in Kanva nodes to approximate the ranks of the keys as\nrank =a×key+b+ϵ.\nWe apply a simple lock-free linear model fitting approach as described in Section 4. Given a dataset as a sorted array\nof keys, models essentially approximate the CDF of key segments of the dataset. Thus, a set of models approximate\nthe CDF of the dataset in a piecewise linear fashion. A model is represented as a packet of parameters {a, b, ϵ}.\nHierarchical structure of Kanva as shown in Figure 2(a) is shallow and unbalanced. It comprises internal nodes and\nBins. Both internal nodes and bins contain multiple key-value pairs and are updatable.\nInternal nodes consist of a sorted array of keys and an array of vValue pointers, see Figure 2(b). An internal node\nother than the root is equipped with a single model. Internal nodes are also updatable: inserting a key-value pair\nwith a new value updates the latest vValue object associated with the key, and a deleted key can be re-inserted with a\nnew associated value. Each internal node other than the root is roughly equal in size, which is determined by the size\nthreshold of a bin, which we specify below. An internal node containing mkeys has m+ 1pointers to bins associated\nwith it following the order of the keys. The bin pointers are null until a key-value pair is stored.\n4\n\nLearned Lock-free Search Data Structures\nBins of Kanva ingest the insertion and deletion of key-value pairs. A bin is structurally different from an external node\nof a classic B +tree. Therefore, we named them so. We initialize a bin as a sorted linked list called a one-level bin .\nAs the number of key-value pairs in a one-level bin reaches its size threshold, it is replaced with a two-level bin . A\ntwo-level bin has an array for indexing and multiple one-level bin pointers, which can further store key-value pairs.\nAs the number of keys in a two-level bin reaches its size threshold, it is replaced with an internal node containing its\nkey-value pairs. The lifecycle of a bin is shown in Figure 2(d).\nThe root of Kanva is a unique internal node that contains the entire dataset at initialization; see Figure 2(c). It\nincludes an array of models for a piecewise linear fitting to the entirety of the initially given dataset. Using a model\nfitting scheme described in Section 4, we obtain key segments with an identical error-bound ϵfor linear regression\nmodels as mentioned above. Initially, n+ 1bin pointers are available in the root to ingest the updates, where nis the\nsize of the initially given dataset.\nTraversals in Kanva are powered by the ML models. With multiple key segments in the root, we start by searching\na query key over the set of endpoints, often applying a binary search. Having searched the appropriate segment, we\nsearch the key in the segment enabled by its associated model. Thus, the approximate location of the key is fixed.\nAfter that, a binary search in a bounded vicinity, determined by the regression error ϵ, is performed to find the key’s\nexact (possible) location. We perform an exponential search in an internal node other than the root to find the precise\nlocation. Exponential search does not require computing ϵ, which is beneficial in a lock-free setting, as we describe in\nSection 4. An operation only travels to the next level if the query key is not found at an internal node. Thus, a shallow\nhierarchy substantially reduces the lookup cost in Kanva. A linear search is performed in a one-level bin, whereas, in\na two-level bin, a linear search follows a binary search to determine the appropriate one-level bin.\n3.2 Data types\nc l a s s Node { }\nc l a s s Model {\nf l o a t p a r a m e t e r s [ ] ;\n}\nc l a s s MNode i n h e r i t s Node {\nModel models [ ] ;\nkt y p e keys [ ] ;\nvValue *v e r s i o n s [ ] ;\nNode *c h i l d r e n [ ] ;\n}\nc l a s s Bin i n h e r i t s Node {\ni n t s i z e ;\nbo ol i s O n e L e v e l ;\n}\nc l a s s OLB i n h e r i t s Bin {\nKNode *head ;\n}c l a s s TLB i n h e r i t s Bin {\nkt y p e keys [ ] ;\nKNode *c h i l d r e n [ ] ;\n}\nc l a s s KNode {\nkt y p e i tem ;\nvValue *v e r s i o n ;\nKNode *n e x t ;\n}\nc l a s s vValue {\nvt y p e v a l ;\ni n t t s ;\nvValue *v n e x t ;\n}\nMNode r o o t = BuildKanva ( D a t a s e t ) ;\nA t o m i c I n t Timestamp = 0 ;\nAlgorithm 1: Kanva: Data Structure Objects\nThe objects to implement Kanva are described in Algorithm 1. Both internal nodes and bins in Kanva are instances\nof class Node . An internal node instantiates the class MNode (Modelled Node). A model is represented by a set of\nparameters as implemented by the class Model . An MNode object encapsulates a Model array models[] fitted to\nthe keys that it contains. The associated list of vValue objects is linked by their head pointers stored in the array\nversions .\nThe class Bin maintains its size threshold and a boolean to indicate if it is a two-level bin. A Bin class is further\ninherited by the classes TLB andOLB of which instances implement two-level bins and one-level bins, respectively.\nThe keys and associated links to vValue objects are encapsulated by class KNode . The vValue objects instantiate the\nclass vValue .\nAn instance of Kanva contains an atomic integer Timestamp that helps in versioning the updates to enable linearizable\nrange search as we discuss in Section 3.4.\n3.3 Lock-free I NSERT , DELETE , and S EARCH\nSEEK method implements traversals in Kanva as described above, which every operation calls at the invocation; see\nAlgorithm 2. S EEK calls the method searchInMNode to determine the index of the key array where the key could\nbe potentially located. If the key is not located in the MNode ,searchInMNode returns the index of the children\narray, where seek should be directed to following the sorted order. As a requirement, S EEK returns a packet of MNode ,\n5\n\nLearned Lock-free Search Data Structures\nthe index of a child pointer or the index of the key if located, and an enumeration – {FOUND ,NFOUND ,MAYBE}– of the\nstatus of locating the key.\n1: SEEK(key, node)\n2: ix←searchInMNode (node, key );\n3: if(node.keys [ix] =key)then\n4: return (node, ix, FOUND)\n5: else\n6: childNode ←node.children [ix+ 1]\n7: if(childNode =null )then\n8: return (node, ix, NFOUND)\n9: else if (type(childNode ) =Bin)then\n10: return (node, ix, MAYBE)\n11: else\n12: return SEEK(key, childNode )\nAlgorithm 2: Traversals in Kanva.\n13: INSERT (KEY,VALUE )\n14: retry:\n15: (node, ix, status )←SEEK(key, Root )\n16: if(status =FOUND )then\n17: return WRITE VALUE (node, ix, VALUE )\n18: else if (status =NFOUND )then\n19: newBin ←new Bin( KEY,VALUE )\n20: ifnode.children [ix].CAS(null, newBin )then\n21: return True\n22: else goto retry\n23: else\n24: bin←node.children [ix]\n25: if(bin.size ≥threshold )then\n26: HELP MAKEMODEL(node, ix, bin )\n27: goto retry\n28: else\n29: res←bin.insertBin (KEY,VALUE )\n30: if(res=underMakeModel )then\n31: HELP MAKEMODEL(node, ix, bin )\n32: goto retry\n33: else return res\n34: DELETE (KEY)\n35: retry:\n36: (node, ix, status )←SEEK(key, Root )\n37: if(status =FOUND )then\n38: if(READ VALUE (node.versions[ix] ̸=null ))then\n39: return WRITE VALUE (node, ix, null )\n40: elsereturn false\n41: else if (status =NFOUND )then\n42: return false\n43: else\n44: bin←node.children [ix]\n45: res←bin.deleteBin (key)\n46: if(res=underMakeModel )then47: HELP MAKEMODEL(node, ix, bin )\n48: goto retry\n49: else return res\n50: SEARCH (key)\n51: (node, ix, status )←SEEK(key, Root )\n52: if(status =FOUND )then\n53: return READ VALUE (node.versions [ix])\n54: else if (status =NFOUND )then\n55: return null;\n56: else\n57: bin←node.children [ix]\n58: binNode ←searchBin (bin, key )\n59: if(binNode.key =key)then\n60: return READ VALUE (binNode.version )\n61: else\n62: return null;\n63: INIT TS(vValue∗ver)\n64: ifver.ts =−1then\n65: CAS(ver.ts, −1, timestamp )\n66: READ VALUE (vValue∗ver)\n67: INITTS(ver)\n68: return ver.val\n69: WRITE VALUE (node, ix, val)\n70: while (true) do\n71: currV head ←node.versions [ix]\n72: INITTS(currV head )\n73: if(currV head.val =val)then return false\n74: newV er ←newvV alue (val)\n75: newver.vnext =currV head\n76: if(CAS (node.versions [ix], currV head, newver ))then\n77: INITTS(newver)\n78: return true\nAlgorithm 3: I NSERT , DELETE , and S EARCH operations.\nINSERT is given in lines 13 to 33 in Algorithm 3. It performs traversal at line 15 to find the node where the insert\ncan happen. If the key is found in an MNode , in that case, it attempts to update the corresponding value using an\natomic compare-and-swap ( CAS) in the method WRITE VALUE , which repeatedly attempts to update the value until a\nCASsucceeds.\nIf the key doesn’t exist in an MNode , the operation will try inserting it in an appropriate Bin. If the Bin doesn’t exist,\na new Bin is created at line 19 and atomically inserted using a CAS at line 21. If the CAS operation fails at line\n21, then some other thread must have added the Bin, and therefore the operation is retried. If the Bin exists and\nhas reached the threshold or is already undergoing transformation to a MNode , it will engage in helping via method\nHELP MAKEMODEL at lines 26 or 31. After any such possible help, insertion is attempted in a bin using method\ninsertBin , whose result is returned finally.\nDELETE is given in lines 34 to 49. Kanva doesn’t perform the physical deletion of a key. Instead, it marks the key\nas deleted by updating its value to null. In that way, a D ELETE operations works exactly like an I NSERT operation to\nupdate the corresponding value of a present key. If the key is not found it returns False .\n6\n\nLearned Lock-free Search Data Structures\n79: Method HELP MAKEMODEL (node, ix, bin)\n80: binKeys, binV ersions, model ←bin. MAKE MODEL()\n81: newNode ←new MNode( binKeys, binV ersions, model )\n82: node.children [ix].CAS(bin, newNode )\n83: return\n84: Method MAKE MODEL\n85: Keys ,V ersions := Collect all the keys and versions after freezing each node one by one.\n86: Model :=makeModel (Keys, V ersions )\n87: return Keys, V ersions, Model\nAlgorithm 4: HELP MAKEMODEL and MAKE MODEL\nSEARCH operations, given in lines 50 to 58 in Algorithm 3, similarly starts with calling Seek in line 51. In case\nSEEK returns FOUND orMAYBE , and the fact that for the deleted keys, the corresponding values are null, it is straight-\nforward for a S EARCH operation to implement its ADT definition. If S EEK returns NFOUND , SEARCH returns null.\nHelping is performed in the method HELP MAKEMODEL as given in Algorithm 4. Essentially, helping is required\nwhen a bin reaches its threshold. For both OLBandTLB, we start with freezing its KNode s. For this purpose, we adopt\nthe popular bit-stealing technique [20] to use one of the unused bits of their next pointers. On observing a frozen bin,\ninsertBin anddeleteBin operations result into indicating that a I NSERT and D ELETE operation need to engage in\nhelping. Once an OLBis frozen, a TLBreplaces it appropriately. For a frozen TLB, a key array is created of the keys\ncollected from all its OLBs. After that, we perform a lock-free model training on this key array as described in Section\n4 using the method MAKE MODEL . The created model and key array make a new MNode ; see line 81.\n3.4 Range Search\n88: RANGE SEARCH (key, range, result)\n89: ts←READ TIMESTAMP ()\n90: result = null\n91: SCAN(root, key, range, result, ts )\n92: return result\n93: SCAN(node, key, range, result, ts)\n94: ix←node.find idx(key)\n95: while ((ix≤node.keys.size )V(range > 0))do\n96: if(node.keys [ix]≥key)then\n97: val=read(node.versions [ix], ts)\n98: if(val̸=null) then\n99: result.push back(node.keys [ix], val )\n100: range ←range −1\n101: node←node.children [ix]\n102: if(type(node) =MNode )then\n103: SCAN(node, key, range, result, ts )104: else if (type(node) =Bin)then\n105: node.scanBin (key, range, result, ts )\n106: ix+ +\n107: return\n108: READ TIMESTAMP ()\n109: ts←Timestamp\n110: CAS(Timestamp , ts, ts + 1)\n111: return ts\n112: READ VALUE (vValue∗ver, ts)\n113: INITTS(node)\n114: while (verVver.ts > ts )do\n115: ver←ver.vnext\n116: if(ver)then (return ver.val )\n117: return tombstone\nAlgorithm 5: Pseudocode of R ANGE SEARCH\nThe linearizable range search in Kanva, as given in Algorithm 5, uses the versioning approach of Wei et al. [42]. The\nINSERT and D ELETE methods discussed above developed the ground for R ANGE . As a key-value pair is inserted,\ndeleted, or updated, the value gets its corresponding timestamp. With every key, the head of the associated vValue\npointer points to the vValue object with the highest timestamp; see method WRITE VALUE in Algorithm 3.\nSee line 89: after reading the atomic integer Timestamp , the method READ TIMESTAMP atomically increments it by\n1. A range search will use this timestamp to collect the key-value pairs that have timestamps at most the one that\nit started with. Accordingly, its linearization point is decided as we discuss in Section 5. R ANGE SEARCH calls the\nmethod S CAN to recursively collect the target key-value pairs based on the timestamp. It is pertinent to underscore\nhere the approach of fine-grained versioning of key-value pairs, even in the ‘fat’ internal nodes in Kanva is beneficial\nto the implemented linearizable range search, which is not possible in lock-free ISTree [10], wherein they replace the\ninternal nodes using double-compare-single-swap (DCSS).\n4 Lock-free Asynchronous Model Fitting\nIn this section, we describe the choice of a model and its training algorithm. The experimental setup and other platform\nconfigurations are described in the next section, which are used for the experimental analysis that we present here.\n7\n\nLearned Lock-free Search Data Structures\nLock-free regression: We explored several options for computing a regression model. Firstly, as different threads\ncollect the keys, they synchronize at the same array of entities (Px,Py,Px2,Py2,Pxy, where xrepresents\na key and yrepresents its rank in the array keys[] ) for computing the regression co-efficient in a lock-free manner.\nThis can be done via CAS as well as via fetch-and-add ( FAA) primitives. We also explored the celebrated approach\nof Hogwild! [36] for training a regression model via gradient updates; however, it is essentially a multi-pass scheme\nand performs poorly if only one pass over the dataset is applied. Finally, we allocated different memory for selected\nkeys and regression coefficient by different threads, and we selected one of the resultant ones after its completion in a\nsingle-step lock-free synchronization. The regression error and corresponding latency results are given in Table 2.\nRegression Error Time Taken\nThreads 8 16 32 64 128 8 16 32 64 128\nSchemeFAA 22 35 30 29 40 3961 5525 5759 7842 5100\nCAS 22 33 26 25 30 9135 8984 12327 16968 3800\nLock-Based 22 33 26 25 30 1488 1871 2364 3192 4164\nHogwild 1529 1467 1653 1368 1627 3851 5876 6121 7561 5800\nLF 22 33 26 25 30 514 600 183 277 207\nTable 2: Different Lock-free Regression Fitting\nWorkloads books fb osmc normal log normal uniform sparse\nNumber of Data 200M 200M 200M 200M 200M 200M\nNo. of Models\nor NodesLPA 330,571 1,572,094 3,139,126 1,603 2,244 71,161\nPGM 263,414 1,081,848 680,521 1,385 1,942 53,258\nLF(a, b)-Tree 27821943 67184623 15559343 113989225 39144243 18505423\nC-IS-Tree 18963332 19592860 20094599 20033152 19563103 20666490\nTable 3: Cardinality of Hierarchy\nClearly, the last one works best to select our threshold, which we experimentally traded off against the overall perfor-\nmance. Note that our linear model fitting approach is one pass, resulting in non-identical ϵacross the nodes.\nBin Structure Selection: For overall performance of the data structure, we also explored the choices of bins with\ndifferent index choices; we considered (a) non-blocking skip-lists [22] (b) non-blocking two-level bins as described in\nSection 3 (c) a variant of our introduced two-level bin, where we have linked lists replaced with arrays in the hope of\ngetting advantage of better cache behaviour. For (c), we needed to reallocate memory for an array of size 32 for every\nupdate.\nIndex Selection: Figure 3 shows the comparative throughput for different combinations of model fitting algorithms\nand non-blocking bin structures. We considered two different model fitting algorithms PGM [18] and PLA of [43],\nfor our lock-free approach to build on. PLA of [43] is a heuristic approach wherein a regression model is created\nusing a number of keys. Suppose the residual error is found to be more than that of the predetermined bound of the\napproximation error. In that case, the regression model computation is reattempted over a smaller set of keys. While\nPGM is a single-pass scheme, the latter is a multi-pass approach. Still, we found that in a concurrent setting, a directly\ncomputed regression model works better than a geometric linear model fitting approach of [18].\nOur proposed two-level non-blocking bins with a lock-free model fitting algorithm offer the maximum throughput\nas shown in Figure 3. This can be understood as memory reallocation cost overshadowing marginal cache locality\noptimization. Furthermore, it also establishes the superiority of the chosen model fitting algorithm. We recorded the\ncardinality of the hierarchy, i.e. the total number of comparison-based or model-based nodes created in each method.\nThe numbers are presented in Table 3. It can be seen that the concurrent learned indexes have a much smaller hierarchy\nthan concurrent classical ones.\n5 Correctness of Algorithm\nWe start with observing some invariants maintained by the presented algorithm.\nInvariant 1 There is a unique path from the root to a key.\nAs the traversal scheme uses models whose parameters are immutable, we can use induction to argue that any update\nto Kanva does not invalidate Invariant 1. As soon as the threshold of a Bin is reached, every update operation will\n8\n\nLearned Lock-free Search Data Structures\n8 16 32 64 128\nThreads0255075100125150175Throughput (MOPS)\n(a)\n8 16 32 64 128\nThreads010203040Throughput (MOPS) (b)\nFigure 3: a) Throughput with read heavy workload with different combinations on uniform dataset b) Throughput with\nupdate heavy Workload with different combinations on uniform dataset\neither freeze its KNode s or will help at that. Once the KNode s are frozen, they become immutable, and any I NSERT or\nDELETE operation necessarily engages in helping before reattempting its steps. In the helping phase, all KNode s of a\nTLBare collected, and a model is trained. Only after a frozen OLBis replaced by a TLB, or a frozen TLBis replaced by\na new MNode , the search of a key inserted or deleted will return as defined. This ensures that Invariant 2 is maintained.\nInvariant 2 (a) No key-value pair is lost due to the transformation of a bin in its lifecycle.\n(b) A SEARCH operation always returns the latest value paired with a key inserted in Kanva. Once a key is deleted,\naSEARCH always returns null.\nThe recursive call of S CAN operation starting from the root ensures that it collects all the target key-value pairs. At\nthe same time, a range search is oblivious to an ongoing transformation of a Binobject, which is only replaced using\nan atomic CAS. This ensures Invariant 3.\nInvariant 3 A range search returns every key-value pair defined by its query key and range size.\nLinearizability: Having observed the maintenance of the above invariants, we discuss the linearization points\n(LPs). The LP of a successful I NSERT is at the atomic CASto either insert a new Bin, a new KNode , or a new vValue\nobject. Similarly, for a successful D ELETE operation, it is the CASto introduce a vValue object with null value. For\nan unsuccessful I NSERT , it is at the atomic read step of the vValue object pointer that contains a matching value. For\nan unsuccessful D ELETE operation LP is at the invocation if the key was not present in the data structure or immedi-\nately after the concurrent D ELETE operation that would have removed the key. The LP of an unsuccessful S EARCH\noperation is determined similar to an unsuccessful D ELETE operation. A successful search operation linearizes at the\natomic read step of the vValue pointer containing the latest value associated with the query key. A R ANGE SEARCH\noperation linearizes on atomically reading the global Timestamp . The linearizability of ADT operations is proved by\nordering them in an arbitrary concurrent execution by their linearization points (LPs).\nLock freedom: We already observed that on a CASfailure, an I NSERT or D ELETE operation either retries or helps.\nThe read operations do not either obstruct any operation or engage in helping. Evidently, there is no loop in their\nexecution path. This establishes that at least one non-faulty thread will complete its operation in a finite number of\nsteps in a concurrent execution, proving lock freedom.\n6 Evaluation\nPlatform Configuration: We conducted the experiments on a system with an AMD EPYC 7452 with 2 NUMA\nprocessing units packing 32 physical cores each thus with a total of 64 cores, with a minimum clock speed of 1.5 GHz\n9\n\nLearned Lock-free Search Data Structures\nand a maximum clock speed of 2.5 GHz. There are two logical threads for each core, and each has a private 32KB L1\ndata cache and L1 instruction cache. Every pair of cores shares a 512KB L2 cache and a 10MB L3 cache. The system\nhas 256GB RAM and a 2TB hard disk. The machine runs Ubuntu 18.04.6 LTS. Our implementation is based on C++;\nthe code was compiled using g++ 11.1.0 with -std=c++17 and linked the pthread and atomic libraries.\nExperiments Setup: In each case, we take a dataset of 200M keys. We prefill the data structure with 10M keys\nrandomly selected from the dataset. Then we take a random permutation of the dataset to perform ADT operations\nINSERT , DELETE , and S EARCH . Each experiment is performed by warming up the system, and then steady results are\ntaken.\n8 16 32 64 128\nNumber of Threads020406080100120140160Throughput (MOPS)\n(a)\n8 16 32 64 128\nThreads010203040Throughput (MOPS) (b)\n1 2 3 4 5 6\nDataset050100150200Throughput (MOPS) (c)\n1 2 3 4 5 6\nDataset010203040Throughput (MOPS) (d)\nA B C\nHotspot Ratio010203040506070Throughput (MOPS)\n(e)\nA B C\nHotspot Ratio020406080Throughput (MOPS) (f)\n8 16 32 64 128\nNumber of Threads050100150200250300350400LLC Misses per Operation (g)\n8 16 32 64 128\nThreads0100200300400500LLC Misses per Operation (h)\nFigure 4: The performance of Kanva when compared to different datastrucutre. (a) read heavy workload with uniform\ndataset (b) update heavy Workload with uniform dataset (c) read Heavy workload with different dataset (1: uniform, 2:\nnormal, 3: Amazon, 4: Facebook, 5: OSM(Open Street Map), 6: Wikipedia) (d) update heavy workload with different\ndataset (e) YCSB workload with uniform distribution (f) YCSB workload with ziphian distribution (g) last-level cache\nmisses (lower is better) on read heavy workload, (h) last level cache misses on update heavy workload\nAlgorithms: We compared Kanva with state-of-the-art comparison-based and learned indexes: (a) Lock-free (a, b)-\ntree (LF (a, b)-Tree) [7], (b) Elimination (a, b)-tree (Elim. (a, b)-Tree) [38], (c) lock-free interpolation search tree\n(C-IS-Tree) [10], and (d) FINEdex [29]. We did not include XIndex [39], as FINEdex already reported 1.3x speed up\nover them across the workloads and data distributions.\nMemory Management has a significant overhead on the performance of lock-free data structures, which lock-\nbased ones – Elim. (a, b)-Tree and FINEdex– escape. For a fair comparison, we include two variants of Kanva, one\nwith memory management – Kanva (MR), and another without that. We used the epoch-based memory management\nscheme DEBRA [12] as used by LF (a, b)-Tree and C-IS-Tree.\nDatasets and Distributions We used the datasets and distributions used in [25] and [38] to evaluate Kanva vis-\na-vis its competitors. Each dataset includes 200 million 64-bit unsigned integers, out of which 10 million are initially\nused to populate the data structure; the operations after initialization use the complete dataset. More specifically,\nwe used (1) Amazon [1], (2) Facebook [40], (3) Wikipedia [2], and (4) OSM [35], in addition to synthetic datasets\ngenerated using uniform and lognormal distributions. Amazon is the book sale popularity data; Facebook represents\nthe unsampled version of the Facebook user ID dataset; Wikipedia is the article edit timestamps; OSM is uniformly\nsampled OpenStreetMap locations represented as Google S2 CellIds. Finally, we also used Yahoo Cloud Service\n10\n\nLearned Lock-free Search Data Structures\nBenchmark [15] (YCSB) workloads with uniform as well as Zipfian distribution. Notice that, such a selection of\ndatasets ensures critical testing of the proposed scheme under real-life and skewed data settings.\n6.1 Read-heavy workload\nHaving initially populated the data structure, concurrent threads perform ADT operations for 10 seconds out of which\n95% are S EARCH , 3% are I NSERT , and 2% are D ELETE . We record the throughput in million operations per second\nvs. the number of threads for each of the algorithms.\nWe examine the scalability with number of threads: 8, 16, 32, 64 and 128, on uniformly distributed data as plotted in\nFigure 4 (a). Across the methods, throughput increases as we increase the number of threads from 8 to 64. The impact\nof hyperthreading (128 threads) is observed on each of them. Throughout Kanva outperforms its competitors by hand-\nsome margins, in particular, it offers approximately two orders of magnitude better performance over the compared\nlock-free schemes – LF (a, b)-Tree and C-IS-Tree, even after discounting for memory management overhead. Exam-\nining the methods without memory management, Kanva outperforms Elim. (a, b)-Tree by 10x for 8 and 16 threads\nand up to 4x for 64 threads. Owing to its lock-free progress, Kanva significantly outperforms FINEdex as contention\nincreases with the number of threads.\nKanva maintains an excellent lead over its competitors across the datasets as we see in Figure 4 (b). In this set of\nplotted results we used 64 threads, however, the comparative performance is unchanged with other contention levels\nas well. It offers an average of around 1.25x higher throughput in comparison to its nearest competitor FINEdex.\nTo understand these results, we note that the comparison-based structures use the traditional search technique to\ntraverse a hierarchy of height O(log(n)), by contrast, FINEdex and Kanva, not only use O(1)arithmetic operations\nfor a model inference at each level but also traverse a much shallower hierarchy. As FINEdex uses fine-grained locks\non bins for every query, Kanva outperforms it for its lock-free scheme for every query. Though C-IS-Tree employs\ninterpolation search in internal ’fat’ nodes, it still performs poorly because the cost of rebalancing is too high even\nwith a small number of updates in the workload.\n6.2 Update-heavy Workload\nThis set of workloads is constructed in the same way as the read-heavy case except that a 10 seconds run of operations\ncomprises of 50% I NSERT , 20% D ELETE and 30% S EARCH . The performance measurement method remains as it\nwas earlier.\nPerformance scalability with threads on uniform dataset is plotted in Figure 4 (c). We note that there is no degra-\ndation in performance even after hyperthreading kicks in. Irrespective of the number of threads, Kanva significantly\noutperforms its competitors for identical memory management.\nFigure 4 (d) demonstrate a comparative performance of Kanva with its competitors using 64 threads. Compared to\nread-heavy workload, the conventional data structures C-IS-Tree and LF (a, b)-Tree catch up with Kanva, which still\noffers a throughput as good as twice that of LF (a, b)-Tree and four times that of C-IS-Tree across the datasets. Further-\nmore, without memory management, it outperforms FINEdex with a decent margin in all types of data distributions.\nWe can understand this performance as the following. Conventional data structure performed well despite traversing\ndown the tree because the leaf node size, where the dataset updates are ingested, is much smaller compared to Kanva’s\nbin. The bin size in Kanva should be big enough to train the model over it to avoid the effort of retraining repeatedly.\nEven in this case, Kanva owes its lead by an average 1.17x higher throughput over FINEdex to its lock-free design.\n6.3 YCSB Workload\nWe benchmarked Kanva and its competitors on the standard YCSB workloads – which are of three levels A: update-\nheavy, B: read-heavy, and C: read-only – with uniform and Zipfian distributions. The results are plotted in Figures 4\n(e) and (f).\nWith uniform distribution, it surpassed FINEdex in update-heavy workload by 1.13x, whereas Elim. (a, b)-Tree out-\nperformed FINEdex by 1.08x. In read-heavy workload, Kanva outperformed Elim. (a, b)-Tree by 1.34x. In read-only\nworkload, FINEdex performed as good as Kanva. Kanva outperformed Elim. (a, b)-Tree by 1.37x in read-only work-\nload.\nSimilarly, Kanva outperformed its competitors with Zipfian distribution. The performance of FINEdex dropped by\n4.8x in Zipfian distribution compared to the uniform distribution due to its locks. Similarly, in read-heavy workload,\n11\n\nLearned Lock-free Search Data Structures\n520 40 60 80100\nHotspot Ratio0255075100125150175Throughput (MOPS)\n(a)\n520 40 60 80100\nHotspot Ratio051015202530Throughput (MOPS) (b)\nFigure 5: a) Throughput with read heavy workload with different hostpot Ratio b) Throughput with update heavy\nworkload with different hostpot Ratio\nElim. (a, b)-Tree outperformed FINEdex by 1.19x, whereas Kanva outperformed Elim. (a, b)-Tree by 1.14x. In\nread-only workload, Kanva outperformed FINEdex by 1.07x.\nIt is interesting to observe that with better concurrency management, such as with an elimination scheme, even a\ncomparison-based index, such as Elim. (a, b)-Tree, can significantly outperform an efficient learned index, such as\nFINEdex, over an adverse workload, which is the case with YCSB zipfian update-heavy workload. In any case, Kanva\noffers better performance compared to competitors across the workload for identical memory reclamation overheads.\nClearly, Kanva brings together the best of both worlds, i.e. lock-free synchronization and learned queries.\n6.4 LLC Misses\nWe also compared the LLC misses of the data structures on uniform distribution for both read-heavy and update-heavy\nworkloads in Figure 4 (g) and (h), respectively. LLC misses of different data structures corroborate their respective\nthroughput. The big fat nodes of Kanva make it more cache efficient than its conventional competitors, where the\ndepth is high to reach the leaf node to perform the operations. C-IS-Tree, which again uses the fat nodes at the internal\nlevel with less depth of leaf node from the root node, still receives the highest number of LLC misses due to its\ncomplicated rebalancing mechanism. On the other hand, Kanva gets an advantage here as it does not need to perform\nany rebalancing. Overall, Kanva and FINEdex demonstrate the least cache misses as compared to others due to their\ndesign.\n6.5 Throughput with Skewed Contention\nTo capture the effect of skewness in query-key distribution , we also checked the performance of the considered methods\nwith different hotspot ratios, which indicates how the range of the available dataset in a data structure relates to the\nrange of the query keys. Essentially, a low hotspot ratio indicates high contention among operations as taking place at\na narrow portion of the data structure. The results are plotted in Figures 5 (a) and (b). The conventional comparison-\nbased structures demonstrate a lesser effect of skewness, whereas FINEdex performs very poorly even with a hotspot\nratio of 0.8, because of its locking technique on nodes even for read operations. The lack of a balancing mechanism\nin Kanva results in the observation of comparative performance thrashing here; however, it can still outperform its\ncompetitors in most settings.\n6.6 Throughput with Range Search\nFor consistent and inconsistent range queries, we compared the throughput of Kanva with FINEdex in Figure 6(a) for\nread-heavy workload and 6(b) for the update-heavy workload. In FINEdex, fine-grained locks are used for inconsistent\nrange search while coarse-grained locks are used for consistent range search. FINEdex consistent range queries do\nnot scales due to their coarse-grained locks, and Kanva consistent and inconsistent range queries outperform it by a\nmagnitude of degree 2. Kanva also outperforms FINEdex in inconsistent range query with a significant margin.\n12\n\nLearned Lock-free Search Data Structures\n8 16 32 64 128\nThreads051015202530Throughput (MOPS)\n(a)\n8 16 32 64 128\nThreads0246810121416Throughput (MOPS) (b)\nFigure 6: a) Throughput of read-heavy workload with Range Search b) Throughput of update-heavy workload with\nRange Search\n7 Related Concurrent and Learned Indexes\nB+Tree and variants are classical indexes where each ei∈ Eis based on comparison operations with Bdata points,\nstored in “fat” nodes. Some of the lock-free designs are lock-free B+-tree [6], Open BW-Tree [41], and BZ-Tree [5].\n(a, b)-Tree derives from B+-trees by relaxing the size of the fat nodes to vary between aandb, whose lock-free\nimplementation was introduced by [7]. Recently, Elimination (a,b) tree [38], a lock-based variant of [7], which also\nuses elimination [21] of concurrent I NSERT and D ELETE operations with the same key, demonstrably outperformed\nseveral other classical concurrent indexes.\nLock-free interpolation search tree C-IST [10] uses the interpolation search [30] to choose the child node for indexing.\nThey are our nearest classical comparison-based index counterpart. Given a dataset of size n, the root node is of degree√ncovering the entire dataset and dividing it into√nbuckets. The child node of a bucket covering√nkeys will have\n4√ndegree. This rule-based division continues for O(log log n )levels, ensuring expected amortized O(log log n +p)\nsteps for data structure operations on smooth key distributions, where nis the dataset size at the invocation, and p\nis the maximum number of threads. The data exists at leaf nodes. Interpolation search makes the indexing faster for\nfavourable key distributions; however, rebalancing is costly in C-IST. The fat nodes are array-based and are reallocated\nand replaced using double-compare-single-swap (DCSS) for every update; however, the operations display good cache\nbehaviour. By contrast, we never reallocate MNode s and use only system native single-word CASprimitives.\n[41] and [5] presented experimental results for range search queries. However, their implementations are not lin-\nearizable . Others [6], [7], [38], [10] do not have any implementation of range search, though [38] and [10] mention\nthat it can be implemented following [11].\nXIndex [39] was the first learned index to support concurrent updates. Similar to classical B+-trees, the dataset is\nstored in leaf nodes. The index uses the RMI [28]. Each leaf node contains an associated model for search. In addition,\nthey contain buffers for ingesting updates. A query in a leaf node has to explore both the data and the buffer whereof\nsearching in the buffer uses a comparison-based scheme. It uses background threads to merge the data and the buffer\nunder barriers to maintain consistency, which blocks the update operation in the buffer.\nFINEdex [29], which Kanva builds on, addressed the drawbacks of XIndex. Firstly, in the leaf nodes, they used\n”bins” as in Kanva, that enabled both cache-optimized read and lock-based concurrent updates, scrapping the need to\nexplore the data array and buffer separately. They used a heuristic approach [43] to train the linear models with fixed\nerror bound. FINEdex outperformed XIndex across the workloads and data distributions. As mentioned in Section 1,\nboth [39] and [29] support only non-linearizable range search.\n13\n\nLearned Lock-free Search Data Structures\n8 Conclusion\nThis paper presented the first lock-free learned search data structure , which we named Kanva. Our algorithm is\nprovably linearizable. Experimentally, Kanva outperforms the existing lock-free and lock-based concurrent search\nmethods by a good margin. In linearizable concurrent data structures, the closest competitor to the presented algo-\nrithm is Elimination- (a, b)-tree, which recently demonstrated superior performance to all existing concurrent search\nalgorithms. With this relative comparison, the proposed algorithm can be placed as outperforming its existing com-\npetitors.\nSeveral instances exist in the literature on concurrent data structures where a carefully optimized lock-based scheme\nsignificantly outperforms the lock-free approach of translating a sequential data structure to a concurrent setting. A\ngood example is the comparison between Elim. (a, b)-Tree and LF (a, b)-Tree. Thus, it is refreshing to see a lock-free\ndata structure outperforming every other existing concurrent data structure of the same class.\nOur experiments provided several insights. For example, a lock-based algorithm puts a fat node-wide lock to update a\ncell of the node array. Under this lock, it can shift elements without reallocating memory unless absolutely needed, for\nexample, to maintain the node size threshold. By contrast, a lock-free scheme finds it challenging to synchronize with-\nout reallocating the node most of the time and atomically replacing it using a DCSS primitive or CAS primitive, with\nthe former having some advantage in a few cases. There is a clear trade-off between the node size and synchronization\noptions with the growing contention. Though Kanva and FINEdex have a similar index structure, Kanva outperformed\nFINEdex in both low and high contention cases. We obtained this sweet spot of node size by random search.\nIt is important to highlight that our experiments with skewed query-key distributions (small hotspot ratio) also revealed\nthat an efficient synchronization method, such as one in Elim. (a, b)-Tree, can enable even a comparison-based index\nto outperform a learned index, such as FINEdex, as soon as the overhead of contention dominates the efficiency gained\nin query complexity. Nevertheless, our proposed method Kanva sailed through this adverse examination as it combined\nefficient learned query and better progress guarantees of lock-free synchronization. This finding is worth noticing for\nfuture research in lock-free data structures, or for that matter, the same in concurrent learned indexes.\nOur technique is the first to allow lock-free range searches over learned data structures. We underscore that Kanva\noutperformed FINEdex by a wide margin, which demonstrated to have outperformed its competitors.\nReferences\n[1] Amazon sales rank data for print and kindle books. https://www.kaggle.com/datasets/ucffool/\namazon-sales-rank-data-for-print-and-kindle-books/ , 18th August, 2022.\n[2] Wikimedia. https://dumps.wikimedia.org/ , 18th August, 2022.\n[3] M. Arbel-Raviv and T. Brown. Harnessing epoch-based reclamation for efficient range queries. ACM SIGPLAN\nNotices , 53(1):14–27, 2018.\n[4] M. Arbel-Raviv, T. Brown, and A. Morrison. Getting to the root of concurrent binary search tree performance.\nInUSENIX , pages 295–306, 2018.\n[5] J. Arulraj, J. Levandoski, U. F. Minhas, and Per-Ake Larson. Bztree: A high-performance latch-free range index\nfor non-volatile memory. VLDB Endowment , 11(5):553–565, 2018.\n[6] Anastasia Braginsky and Erez Petrank. A lock-free b+ tree. In 24th annual ACM SPAA , pages 58–67, 2012.\n[7] Trevor Brown. Techniques for constructing efficient lock-free data structures. CoRR , abs/1712.05406, 2017.\nURL: http://arxiv.org/abs/1712.05406 ,arXiv:1712.05406 .\n[8] Trevor Brown, Faith Ellen, and Eric Ruppert. A general technique for non-blocking trees. In 19th ACM SIGPLAN\nPPOPP , pages 329–342, 2014.\n[9] Trevor Brown and Joanna Helga. Non-blocking k-ary search trees. In OPODIS , pages 207–221. Springer, 2011.\n[10] Trevor Brown, Aleksandar Prokopec, and Dan Alistarh. Non-blocking interpolation search trees with doubly-\nlogarithmic running time. In 25th ACM PPOPP , pages 276–291, 2020.\n[11] Trevor Brown, William Sigouin, and Dan Alistarh. Pathcas: an efficient middle ground for concurrent search\ndata structures. In 27th ACM SIGPLAN PPOPP , pages 385–399, 2022.\n[12] Trevor Alexander Brown. Reclaiming memory for lock-free data structures: There has to be a better way. In\nProceedings of the 2015 ACM Symposium on Principles of Distributed Computing , pages 261–270, 2015.\n[13] Bapi Chatterjee. Lock-free linearizable 1-dimensional range queries. In Proceedings of the 18th International\nConference on Distributed Computing and Networking , pages 1–10, 2017.\n14\n\nLearned Lock-free Search Data Structures\n[14] Bapi Chatterjee, Nhan Nguyen, and Philippas Tsigas. Efficient lock-free binary search trees. In 2014 ACM\nPODC , pages 322–331, 2014.\n[15] Brian F Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell Sears. Benchmarking cloud\nserving systems with ycsb. In 1st ACM SOCC , pages 143–154, 2010.\n[16] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li, Hantian Zhang, Badrish Chan-\ndramouli, Johannes Gehrke, Donald Kossmann, et al. Alex: an updatable adaptive learned index. In 2020 ACM\nSIGMOD , pages 969–984, 2020.\n[17] F. Ellen, P. Fatourou, E. Ruppert, and F. van Breugel. Non-blocking binary search trees. In 29th ACM SIGACT-\nSIGOPS , pages 131–140, 2010.\n[18] Paolo Ferragina and Giorgio Vinciguerra. The pgm-index: a fully-dynamic compressed learned index with\nprovable worst-case bounds. Proceedings of the VLDB Endowment , 13(8):1162–1175, 2020.\n[19] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim Kraska. Fiting-tree: A data-\naware index structure. In 2019 ACM SIGMOD , pages 1189–1206, 2019.\n[20] Timothy L. Harris. A pragmatic implementation of non-blocking linked-lists. In DISC 2021, Lisbon, Portugal,\nOctober 3-5, 2001, Proceedings , pages 300–314, 2001.\n[21] Danny Hendler, Nir Shavit, and Lena Yerushalmi. A scalable lock-free stack algorithm. In Proceedings of the\nsixteenth annual ACM symposium on Parallelism in algorithms and architectures , pages 206–215, 2004.\n[22] M. Herlihy, Y . Lev, V . Luchangco, and N. Shavit. A provably correct scalable concurrent skip list. In OPODIS ,\npage 103, 2006.\n[23] M. P. Herlihy and J. M. Wing. Linearizability: A correctness condition for concurrent objects. ACM TOPLAS ,\n12(3):463–492, 1990.\n[24] S. V . Howley and J. Jones. A non-blocking internal binary search tree. In 24th annual ACM SPAA , pages\n161–171, 2012.\n[25] A. Kipf, R. Marcus, A. van Renen, M. Stoian, A. Kemper, T. Kraska, and T. Neumann. Sosd: A benchmark for\nlearned indexes. arXiv preprint arXiv:1911.13014 , 2019.\n[26] A. Kipf, R. Marcus, A. van Renen, M. Stoian, A. Kemper, T. Kraska, and T. Neumann. Radixspline: a single-\npass learned index. In Proceedings of the Third International Workshop on Exploiting Artificial Intelligence\nTechniques for Data Management , pages 1–5, 2020.\n[27] Tadeusz Kobus, Maciej Kokoci ´nski, and Paweł T Wojciechowski. Jiffy: a lock-free skip list with batch updates\nand snapshots. In 27th ACM PPOPP , pages 400–415, 2022.\n[28] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned index structures.\nInACM SIGMOD , pages 489–504, 2018.\n[29] Pengfei Li, Yu Hua, Jingnan Jia, and Pengfei Zuo. Finedex: a fine-grained learned index scheme for scalable and\nconcurrent memory systems. Proceedings of the VLDB Endowment , 15(2):321–334, 2021.\n[30] Kurt Mehlhorn and Athanasios Tsakalidis. Dynamic interpolation search. JACM , 40(3):621–634, 1993.\n[31] Aravind Natarajan and Neeraj Mittal. Fast concurrent lock-free binary search trees. In 19th ACM SIGPLAN\nPPOPP , pages 317–328, 2014.\n[32] Aravind Natarajan, Arunmoezhi Ramachandran, and Neeraj Mittal. Feast: a lightweight lock-free concurrent\nbinary search tree. ACM TOPC , 7(2):1–64, 2020.\n[33] Jacob Nelson, Ahmed Hassan, and Roberto Palmieri. Bundled references: an abstraction for highly-concurrent\nlinearizable range queries. In Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of\nParallel Programming , pages 448–450, 2021.\n[34] Patrick O’Neil, Edward Cheng, Dieter Gawlick, and Elizabeth O’Neil. The log-structured merge-tree (lsm-tree).\nActa Informatica , 33(4):351–385, 1996.\n[35] Varun Pandey, Andreas Kipf, Thomas Neumann, and Alfons Kemper. How good are modern spatial analytics\nsystems? Proceedings of the VLDB Endowment , 11(11):1661–1673, 2018.\n[36] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild!: A lock-free approach to parallelizing\nstochastic gradient descent. Advances in neural information processing systems , 24, 2011.\n[37] Gali Sheffi, Pedro Ramalhete, and Erez Petrank. Eemarq: Efficient lock-free range queries with memory recla-\nmation. arXiv preprint arXiv:2210.17086 , 2022.\n15\n\nLearned Lock-free Search Data Structures\n[38] Anubhav Srivastava and Trevor Brown. Elimination (a, b)-trees with fast, durable updates. In 27th ACM PPOPP ,\npages 416–430, 2022.\n[39] Chuzhe Tang, Youyun Wang, Zhiyuan Dong, Gansen Hu, Zhaoguo Wang, Minjie Wang, and Haibo Chen. Xin-\ndex: a scalable learned index for multicore data storage. In 25th PPOPP , pages 308–320, 2020.\n[40] Peter Van Sandt, Yannis Chronis, and Jignesh M Patel. Efficiently searching in-memory sorted arrays: Revenge\nof the interpolation search? In 2019 SIGMOD , pages 36–53, 2019.\n[41] Z. Wang, A. Pavlo, H. Lim, V . Leis, H. Zhang, M. Kaminsky, and D. G. Andersen. Building a bw-tree takes\nmore than just buzz words. In ACM SIGMOD , pages 473–488, 2018.\n[42] Yuanhao Wei, Naama Ben-David, Guy E Blelloch, Panagiota Fatourou, Eric Ruppert, and Yihan Sun. Constant-\ntime snapshots with applications to concurrent data structures. In PPoPP , pages 31–46, 2021.\n[43] Q. Xie, C. Pang, X. Zhou, X. Zhang, and K. Deng. Maximum error-bounded piecewise linear representation for\nonline stream approximation. The VLDB journal , 23(6):915–937, 2014.\n16",
  "textLength": 59731
}