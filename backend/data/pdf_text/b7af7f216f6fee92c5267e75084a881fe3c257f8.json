{
  "paperId": "b7af7f216f6fee92c5267e75084a881fe3c257f8",
  "title": "LotusFilter: Fast Diverse Nearest Neighbor Search via a Learned Cutoff Table",
  "pdfPath": "b7af7f216f6fee92c5267e75084a881fe3c257f8.pdf",
  "text": "arXiv:2506.04790v1  [cs.CV]  5 Jun 2025LotusFilter: Fast Diverse Nearest Neighbor Search via a Learned Cutoff Table\nYusuke Matsui\nThe University of Tokyo\nmatsui@hal.t.u-tokyo.ac.jp\nAbstract\nApproximate nearest neighbor search (ANNS) is an essen-\ntial building block for applications like RAG but can some-\ntimes yield results that are overly similar to each other. In\ncertain scenarios, search results should be similar to the\nquery and yet diverse. We propose LotusFilter, a post-\nprocessing module to diversify ANNS results. We precom-\npute a cutoff table summarizing vectors that are close to\neach other. During the filtering, LotusFilter greedily looks\nup the table to delete redundant vectors from the candidates.\nWe demonstrated that the LotusFilter operates fast (0.02\n[ms/query]) in settings resembling real-world RAG appli-\ncations, utilizing features such as OpenAI embeddings. Our\ncode is publicly available at https://github.com/\nmatsui528/lotf .\n1. Introduction\nAn approximate nearest neighbor search (ANNS) algo-\nrithm, which finds the closest vector to a query from\ndatabase vectors [8, 29, 31], is a crucial building block\nfor various applications, including image retrieval and in-\nformation recommendation. Recently, ANNS has become\nan essential component of Retrieval Augmented Genera-\ntion (RAG) approaches, which integrate external informa-\ntion into Large Language Models [5].\nThe essential problem with ANNS is the lack of diver-\nsity. For example, consider the case of image retrieval us-\ning ANNS. Suppose the query is an image of a cat, and the\ndatabase contains numerous images of the same cat. In that\ncase, the search results might end up being almost uniform,\nclosely resembling the query. However, users might prefer\nmore diverse results that differ from one another.\nDiverse nearest neighbor search (DNNS) [15, 41, 50]\nis a classical approach to achieving diverse search results\nbut often suffers from slow performance. Existing DNNS\nmethods first obtain Scandidates (search step) and then se-\nlectK(< S)results to ensure diversity (filter step). This\napproach is slow for three reasons. First, integrating mod-\nern ANN methods is often challenging. Second, selecting\nð’’ð’™13\nð’™28ð’™25(a) Usual ANNS\nð’’ðœ€\nð’™25ð’™61\nð’™19ð’™ð‘–âˆ’ð’™ð‘—22â‰¥ðœ€ (b) DNNS with the LotusFilter\nFigure 1. (a) Usual ANNS. The search results are close to the\nquery qbut similar to each other. (b) DNNS with the proposed\nLotusFilter. The obtained vectors are at leastâˆšÎµapart from each\nother. The results are diverse despite being close to the query.\nKitems from Scandidates is a subset selection problem,\nwhich is NP-hard. Lastly, existing methods require access\nto the original vectors during filtering, which often involves\nslow disk access if the vectors are not stored in memory.\nWe propose a fast search result diversification approach\ncalled LotusFilter , which involves precomputing a cutoff ta-\nble and using it to filter search results. Diverse outputs are\nensured by removing vectors too close to each other. The\ndata structure and algorithm are both simple and highly ef-\nficient (Fig. 1), with the following contributions:\nâ€¢ As LotusFilter is designed to operate as a pure post-\nprocessing module, one can employ the latest ANNS\nmethod as a black-box backbone. This design provides\na significant advantage over existing DNNS methods.\nâ€¢ We introduce a strategy to train the hyperparameter, elim-\ninating the need for complex parameter tuning.\nâ€¢ LotusFilter demonstrates exceptional efficiency for large-\nscale datasets, processing queries in only 0.02 [ms/query]\nfor9Ã—1051536 -dimensional vectors.\n2. Related work\n2.1. Approximate nearest neighbor search\nApproximate nearest neighbor search (ANNS) has been\nextensively studied across various fields [29, 31]. Since\n\naround 2010, inverted indices [4, 7, 13, 24, 30] and graph-\nbased indices [20, 28, 34, 36, 46, 48] have become the\nstandard, achieving search times under a millisecond for\ndatasets of approximately 106items. These modern ANNS\nmethods are significantly faster than earlier approaches, im-\nproving search efficiency by orders of magnitude.\n2.2. Diverse nearest neighbor search\nThe field of recommendation systems has explored di-\nverse nearest neighbor search (DNNS), especially during\nthe 2000s [9, 15, 41, 50]. Several approaches propose ded-\nicated data structures as solutions [16, 39], indicating that\nmodern ANNS methods have not been fully incorporated\ninto DNNS. Hirata et al. stand out as the only ones to use\nmodern ANNS for diverse inner product search [22].\nMost existing DNNS methods load Sinitial search re-\nsults (the original D-dimensional vectors) and calculate all\npossible combinations even if approximate. This approach\nincurs a diversification cost of at least O(DS2). In contrast,\nour LotusFilter avoids loading the original vectors or per-\nforming pairwise computations, instead scanning Sitems\ndirectly. This design reduces the complexity to O(S), mak-\ning it significantly faster than traditional approaches.\n2.3. Learned data structure\nLearned data structures [17, 26] focus on enhancing clas-\nsical data structures by integrating machine learning tech-\nniques. This approach has been successfully applied to\nwell-known data structures such as B-trees [10, 18, 19, 49],\nKD-trees [12, 21, 33], and Bloom Filters [27, 32, 42, 47].\nOur proposed method aligns with this trend by constructing\na data structure that incorporates data distribution through\nlearned hyperparameters for thresholding, similar to [10].\n3. Preliminaries\nLet us describe our problem setting. Considering that we\nhaveN D -dimensional database vectors {xn}N\nn=1, where\nxnâˆˆRD. Given a query vector qâˆˆRD, our task is to\nretrieve Kvectors that are similar to qyet diverse, i.e., dis-\nsimilar to each other. We represent the obtained results as a\nset of identifiers, K âŠ† { 1, . . . , N }, where |K|=K.\nThe search consists of two steps. First, we run ANNS\nand obtain S(â‰¥K)vectors close to q. These initial search\nresults are denoted as S âŠ† { 1, . . . , N }, where |S|=S. The\nsecond step is diversifying the search results by selecting a\nsubset K(âŠ† S)from the candidate set S. This procedure\nis formulated as a subset selection problem. The objective\nhere is to minimize the evaluation function f: 2Sâ†’R.\nargmin\nKâŠ†S,|K|=Kf(K). (1)\nHere, fevaluates how good Kis, regarding both â€œproximityto the queryâ€ and â€œdiversityâ€, formulated as follows.\nf(K) =1âˆ’Î»\nKX\nkâˆˆKâˆ¥qâˆ’xkâˆ¥2\n2âˆ’Î»min\ni,jâˆˆK, iÌ¸=jâˆ¥xiâˆ’xjâˆ¥2\n2.\n(2)\nThe first term is the objective function of the nearest neigh-\nbor search itself, which indicates how close qis to the se-\nlected vectors. The second term is a measure of the diver-\nsity. Following [3, 22], we define it as the closest distance\namong the selected vectors. Here Î»âˆˆ[0,1]is a parameter\nthat adjusts the two terms. If Î»= 0, the problem is a near-\nest neighbor search. If Î»= 1, the equation becomes the\nMAX-MIN diversification problem [40] that evaluates the\ndiversity of the set without considering a query. This for-\nmulation is similar to the one used in [9, 22, 39] and others.\nLet us show the computational complexity of Eq. (1) is\nO(T+\u0000S\nK\u0001\nDK2), indicating that it is slow. First, since itâ€™s\nnot easy to represent the cost of ANNS, we denote ANNSâ€™s\ncost as O(T), where Tis a conceptual variable govern-\ning the behavior of ANNS. The first term in Eq. (2) takes\nO(DK), and the second term takes O(DK2)for a naive\npairwise comparison. When calculating Eq. (1) naively, it\nrequires\u0000S\nK\u0001\ncomputations for subset enumeration. There-\nfore, the total cost is O(T+\u0000S\nK\u0001\nDK2).\nThere are three main reasons why this operation is slow.\nFirst, it depends on D, making it slow for high-dimensional\nvectors since it requires maintaining and scanning original\nvectors. Second, the second term calculates all pairs of ele-\nments in K(costing O(K2)), which becomes slow for large\nK. Lastly, subset enumeration,\u0000S\nK\u0001\n, is unacceptably slow.\nIn the next section, we propose an approximate and efficient\nsolution with a complexity of O(T+S+KL), where Lis\ntypically less than 100 for N= 9Ã—105.\n4. LotusFilter Algorithm\nIn this section, we introduce the algorithm of the proposed\nLotusFilter. The basic idea is to pre-tabulate the neighbor-\ning points for each xnand then greedily prune candidates\nby looking up this table during the filtering step.\nAlthough LotusFilter is extremely simple, it is unclear\nwhether the filtering works efficiently. Therefore, we in-\ntroduce a data structure called OrderedSet to achieve fast\nfiltering with a theoretical guarantee.\n4.1. Preprocessing\nAlgorithm 1 illustrates a preprocessing step. The inputs\nconsist of database vectors {xn}N\nn=1and the threshold for\nthe squared distance, ÎµâˆˆR. InL1, we first construct I, the\nindex for ANNS. Any ANNS methods, such as HNSW [28]\nfor faiss [14], can be used here.\nNext, we construct a cutoff table in L2-3 . For each xn,\nwe collect the set of IDs whose squared distance from xnis\n\nAlgorithm 1: BUILD\nInput: {xn}N\nn=1âŠ†RD, ÎµâˆˆR\n1I â† BUILD INDEX ({xn}N\nn=1) # ANNS\n2fornâˆˆ {1, . . . , N }do\n3Lnâ† {iâˆˆ {1, . . . , N } | âˆ¥xnâˆ’xiâˆ¥2\n2< Îµ, n Ì¸=i}\n4return I,{Ln}N\nn=1\nAlgorithm 2: SEARCH AND FILTER\nInput: qâˆˆRD, S, K (â‰¤S),I,{Ln}N\nn=1\n1S â† I .SEARCH (q, S) #S âŠ† { 1, . . . , N }\n2K â†âˆ…\n3while|K|< K do # At most Ktimes\n4 kâ†POP(S) #O(L)\n5K â† K âˆª { k} #O(1)\n6S â† S \\ L k #O(L)\n7return K #K âŠ† S where |K|=K\nless than Îµ. The collected IDs are stored as Ln. We refer to\nthese{Ln}N\nn=1as a cutoff table (an array of integer arrays).\nWe perform a range search for each xnto create the cut-\noff table. Assuming that the cost of the range search is also\nO(T), the total cost becomes O(NT). As demonstrated\nlater in Tab. 2, the runtime for N= 9Ã—105is approxi-\nmately one minute at most.\n4.2. Search and Filtering\nThe search and filtering process is our core contribution and\ndescribed in Algorithm 2 and Fig. 2. The inputs are a query\nqâˆˆRD, the number of initial search results S(â‰¤N), the\nnumber of final results K(â‰¤S), the ANNS index I, and\nthe cutoff table {Ln}N\nn=1.\nAs the search step, we first run ANNS in L1(Fig. 2a) to\nobtain the candidate set S âŠ† { 1, . . . , N }. InL2, we prepare\nan empty integer set Kto store the final results.\nThe filtering step is described in L3-6 where IDs are\nadded to the set Kuntil its size reaches K. InL4, we pop\nthe ID kfromS, where xkis closest to the query, and add\nit toKinL5. Here, L6is crucial: for the current focus\nk, the IDs of vectors close to xkare stored in Lk. Thus,\nby removing LkfromS, we can eliminate vectors similar\ntoxk(Fig. 2b). Repeating this step (Fig. 2c) ensures that\nelements in Kare at leastâˆšÎµapart from each other.1\nHere, the accuracy of the top-1 result (Recall@1) after\nfiltering remains equal to that of the initial search results.\nThis is because the top-1 result from the initial search is\nalways included in KinL4during the first iteration.\n1The filtering step involves removing elements within a circle centered\non a vector (i.e., eliminating points inside the green circle in Figs. 2b\nand 2c). This process evokes the imagery of lotus leaves, which inspired\nus to name the proposed method â€œLotusFilterâ€.Note that the proposed approach is faster than existing\nmethods for the following intuitive reasons:\nâ€¢ The filtering step processes candidates sequentially\n(O(S)) in a fast, greedy manner. Many existing meth-\nods determine similar items in Sby calculating distances\non the fly, requiring O(DS2)for all pairs, even when ap-\nproximated. In contrast, our approach precomputes dis-\ntances, eliminating on-the-fly calculations and avoiding\npairwise computations altogether.\nâ€¢ The filtering step does not require the original vectors,\nmaking it a pure post-processing step for any ANNS mod-\nules. In contrast, many existing methods depend on re-\ntaining the original vectors and computing distances dur-\ning the search. Therefore, they cannot be considered pure\npost-processing, especially since modern ANNS methods\noften use compressed versions of the original vectors.\nIn Sec. 5, we discuss the computational complexity in detail\nand demonstrate that it is O(T+S+KL).\n4.3. Memory consumption\nWith L=1\nNPN\nn=1|Ln|being the average length of Ln,\nthe memory consumption of the LotusFilter is 64LN [bit]\nwith the naive implementation using 64 bit integers. It is\nbecause, from Algorithm 2, the LotusFilter requires only a\ncutoff table {Ln}N\nn=1as an auxiliary data structure.\nThis result demonstrates that the memory consumption\nof our proposed LotusFilter can be accurately estimated in\nadvance. We will later show in Tab. 1 that, for N= 9Ã—105,\nthe memory consumption is 1.14Ã—109[bit]= 136 [MiB].\n4.4. Theoretical guarantees on diversity\nFor the results obtained by Algorithm 2, the diversity term\n(second term) of the objective function Eq. (2) is bounded\nbyâˆ’Îµas follows. We construct the final results of Algo-\nrithm 2, K, by adding an element one by one in L4. For\neach loop, given a new kinL4, all items whose squared\ndistance to kis less than Îµmust be contained in Lk. Such\nclose items are removed from the candidates SinL6. Thus,\nfor all i, jâˆˆ K where iÌ¸=j,âˆ¥xiâˆ’xjâˆ¥2\n2â‰¥Îµholds, resulting\ninâˆ’mini,jâˆˆK,iÌ¸=jâˆ¥xiâˆ’xjâˆ¥2\n2â‰¤ âˆ’Îµ.\nThis result shows that the proposed LotusFilter can al-\nways ensure diversity, where we can adjust the degree of\ndiversity using the parameter Îµ.\n4.5. Safeguard against over-pruning\nFiltering can sometimes prune too many candidates from\nS. To address this issue, a safeguard mode is available as\nan option. Specifically, if LkinL6is large and |S|drops to\nzero, no further elements can be popped. If this occurs, K\nreturned by Algorithm 2 may have fewer elements than K.\nWith the safeguard mode activated, the process will ter-\nminate immediately when excessive pruning happens in L6.\nThe remaining elements in Swill be added to K. This\n\n12\n3\n6\n45\n14\n810\n9 1213\n711ð’’ð‘˜â„’ð‘˜\n14, 8\n2\n311, 14\n41, 8\n510\n6\n712\n81, 4\n9\n10 5\n11 3, 14\n12 7\n13\n14 3, 11ð’®=5,10,3,14,7,11(a) Initial search result\n912\n3\n6\n414\n81213\n711510\nð’’ð‘˜â„’ð‘˜\n14, 8\n2\n311, 14\n41, 8\n510\n6\n712\n81, 4\n9\n10 5\n11 3, 14\n12 7\n13\n14 3, 11ð’®=5,10,3,14,7,11\nð’¦={5}Pop (b) Accept the 1stcandidate. Cutoff.\n910\n12\n6\n481213\n75ð‘˜â„’ð‘˜\n14, 8\n2\n311, 14\n41, 8\n510\n6\n712\n81, 4\n9\n10 5\n11 3, 14\n12 7\n13\n14 3, 11 3\n1411ð’’ð’®=3,14,7,11\nð’¦={5,3}Pop (c) Accept the 2ndcandidate. Cutoff.\nFigure 2. Overview of the proposed LotusFilter ( D= 2, N = 14, S= 6, K = 2)\nsafeguard ensures that the final result meets the condition\n|K|=K. In this scenario and only in this scenario, the\ntheoretical result discussed in Sec. 4.4 does not hold.\n5. Complexity Analysis\nWe prove that the computational complexity of Algorithm 2\nisO(T+S+KL)on average. This is fast because just\naccessing the used variables requires the same cost.\nThe filtering step of our LotusFilter ( L3-L6 in Algo-\nrithm 2) is quite simple, but it is unclear whether it can\nbe executed efficiently. Specifically, for S,L4requires a\npop operation, and L6removes an element. These two op-\nerations cannot be efficiently implemented with basic data\nstructures like arrays, sets, or priority queues.\nTo address this, we introduce a data structure called Or-\nderedSet. While OrderedSet has a higher memory con-\nsumption, it combines the properties of both a set and an\narray. We demonstrate that by using OrderedSet, the opera-\ntions in the while loop at L3can be run in O(L).\n5.1. Main result\nProposition 5.1. The computational complexity of the\nsearch and filter algorithm in Algorithm 2 is O(T+S+KL)\non average using the OrderedSet data structure for S.\nProof. InL1, the search takes O(T), and the initialization\nofStakesO(S). The loop in L3is executed at most K\ntimes. Here, the cost inside the loop is O(L). That is, P OP\nonStakesO(L)inL4. Adding an element to a set takes\nO(1)inL5. The Ltimes deletion for SinL6takesO(L).\nIn total, the computational cost is O(T+S+KL).\nTo achieve the above, we introduce the data structure\ncalled OrderedSet to represent S. An OrderedSet satisfies\nO(S)for initialization, O(L)for P OP, andO(1)for the\ndeletion of a single item.\n5.2. OrderedSet\nOrderedSet, as its name suggests, is a data structure repre-\nsenting a set while maintaining the order of the input ar-ray. OrderedSet combines the best aspects of arrays and\nsets at the expense of memory consumption. See the swift-\ncollections package2in the Swift language for the reference\nimplementation. We have found that this data structure im-\nplements the P OPoperation in O(L).\nFor a detailed discussion of the implementation, here-\nafter, we consider the input to OrderedSet as an array v=\n[v[1], v[2], . . . , v [V]]withVelements (i.e., the input to Sin\nL1of Algorithm 2 is an array of integers).\nInitialization: We show that the initialization of Ordered-\nSet takes O(V). OrderedSet takes an array vof length V\nand converts it into a set (hash table) V:\nV â† SET(v). (3)\nThis construction takes O(V). Then, a counter câˆˆ\n{1, . . . , V }indicating the head position is prepared and ini-\ntialized to câ†1. The OrderedSet is a simple data structure\nthat holds v,V, and c. OrderedSet has high memory con-\nsumption because it retains both the original array vand its\nset representation V. An element in Vmust be accessed and\ndeleted in constant time on average. We utilize a fast open-\naddressing hash table boost::unordered flat set\nin our implementation3. InL1of Algorithm 2, this initial-\nization takes O(S).\nRemove: The operation to remove an element afrom Or-\nderedSet is implemented as follows with an average time\ncomplexity of O(1):\nV â† V \\ { a}. (4)\nIn other words, the element is deleted only from V. As the\nelement in vremains, the deletion is considered shallow. In\nL6of Algorithm 2, the Lremovals result in an O(L)cost.\n2https : / / swiftpackageindex . com / apple /\nswift - collections / 1 . 1 . 0 / documentation /\norderedcollections/orderedset\n3https://www.boost.org/doc/libs/master/libs/\nunordered/doc/html/unordered/intro.html\n\nPop: Finally, the P OPoperation, which removes the first\nelement, is realized in O(âˆ†)as follows:\nâ€¢ Step 1: Repeat câ†c+ 1untilv[c]âˆˆ V\nâ€¢ Step 2: V â† V \\ { v[c]}\nâ€¢ Step 3: Return v[c]\nâ€¢ Step 4: câ†c+ 1\nStep 1 moves the counter until a valid element is found.\nHere, the previous head (or subsequent) elements might\nhave been removed after the last call to P OP. In such cases,\nthe counter must move along the array until it finds a valid\nelement. Let âˆ†be the number of such moves; this counter\nupdate takes O(âˆ†). In Step 2, the element is removed in\nO(1)on average. In Step 3, the removed element is re-\nturned, completing the P OPoperation. Step 4 updates the\ncounter position accordingly.\nThus, the total time complexity is O(âˆ†). Here, âˆ†repre-\nsents the â€œnumber of consecutively removed elements from\nthe previous head position since the last call to P OPâ€. In our\nproblem setting, between two calls to P OP, at most Lele-\nments can be removed (refer to L6in Algorithm 2). Thus,\nâˆ†â‰¤L. (5)\nTherefore, the P OPoperation is O(L)in Algorithm 2.\nUsing other data structures, achieving both P OPand R E-\nMOVE operations efficiently is challenging. With an array,\nPOPcan be accomplished in O(âˆ†) in the same way. How-\never, removing a specific element requires a linear search,\nwhich incurs a cost of O(V). On the other hand, if we use\na set (hash table), deletion can be done in O(1), but P OP\ncannot be implemented. Please refer to the supplemental\nmaterial for a more detailed comparison of data structures.\n6. Training\nThe proposed method intuitively realizes diverse search by\nremoving similar items from the search results, but it is un-\nclear how it contributes explicitly to the objective function\nEq. (1). Here, by learning the threshold Îµin advance, we\nensure that our LotusFilter effectively reduces Eq. (1).\nFirst, letâ€™s confirm the parameters used in our approach;\nÎ», S, K, andÎµ. Here, Î»is set by the user to balance the pri-\nority between search and diversification. Kis the number\nof final search results and must also be set by the user. S\ngoverns the accuracy and speed of the initial search. Setting\nSis not straightforward, but it can be determined based on\nruntime requirements, such as setting S= 3K. The param-\neterÎµis less intuitive; a larger Îµincreases the cutoff table\nsizeL, impacting both results and runtime. The user should\nsetÎµminimizing f, but this setting is not straightforward.\nTo find the optimal Îµ, we rewrite the equations as fol-\nlows. First, since Sis the search result of q, we can write\nS= NN( q, S). Here, we explicitly express the solution fâˆ—\nof Eq. (1) as a function of Îµandqas follows.fâˆ—(Îµ,q) = argmin\nKâŠ†NN(q, S),|K|=Kf(K). (6)\nWe would like to find Îµthat minimizes the above. Since q\nis a query data provided during the search phase, we cannot\nknow it beforehand. Therefore, we prepare training query\ndataQtrainâŠ‚RDin the training phase. This training query\ndata can usually be easily prepared using a portion of the\ndatabase vectors. Assuming that this training query data is\ndrawn from a distribution similar to the test query data, we\nsolve the following.\nÎµâˆ—= argmin\nÎµE\nqâˆˆQtrain[fâˆ—(Îµ,q)]. (7)\nThis problem is a nonlinear optimization for a single\nvariable without available gradients. One could apply a\nblack-box optimization [1] to solve this problem, but we use\na more straightforward approach, bracketing [25], which re-\ncursively narrows the range of the variable. See the supple-\nmentary material for details. This simple method achieves\nsufficient accuracy as shown later in Fig. 4.\n7. Evaluation\nIn this section, we evaluate the proposed LotusFil-\nter. All experiments were conducted on an AWS EC2\nc7i.8xlarge instance (3.2GHz Intel Xeon CPU, 32 vir-\ntual cores, 64GiB memory). We ran preprocessing us-\ning multiple threads while the search was executed using\na single thread. For ANNS, we used HNSW [28] from\nthe faiss library [14]. The parameters of HNSW were\nefConstruction=40 ,efSearch=16 , and M=256 .\nLotusFilter is implemented in C++17 and called from\nPython using nanobind [23]. Our code is publicly available\nathttps://github.com/matsui528/lotf .\nWe utilized the following datasets:\nâ€¢ OpenAI Dataset [35, 45]: This dataset comprises 1536-\ndimensional text features extracted from WikiText us-\ning OpenAIâ€™s text embedding model. It consists of\n900,000 base vectors and 100,000 query vectors. We use\nthis dataset for evaluation, considering that the proposed\nmethod is intended for application in RAG systems.\nâ€¢ MS MARCO Dataset [6]: This dataset includes Bing\nsearch logs. We extracted passages from the v1.1 vali-\ndation set, deriving 768-dimensional BERT features [11],\nresulting in 38,438 base vectors and 1,000 query vectors.\nWe used this dataset to illustrate redundant texts.\nâ€¢ Revisited Paris Dataset [37]: This image dataset fea-\ntures landmarks in Paris, utilizing 2048-dimensional R-\nGeM [38] features with 6,322 base and 70 query vectors.\nIt serves as an example of data with many similar images.\nWe used the first 1,000 vectors from base vectors for hyper-\nparameter training ( Qtrain in Eq. (7)).\n\nCost function ( â†“) Runtime [ms/query] ( â†“) Memory overhead [bit] ( â†“)\nFiltering Search Diversification Final ( f) Search Filter Total {xn}N\nn=1 {Ln}K\nk=1\nNone (Search only) 0.331 âˆ’0.107 0 .200 0 .855 - 0.855 - -\nClustering 0.384 âˆ’0.152 0 .223 0 .941 6 .94 7 .88 4 .42Ã—1010-\nGMM [40] 0.403 âˆ’0.351 0 .177 0.977 13 .4 14 .4 4 .42Ã—1010-\nLotusFilter (Proposed) 0.358 âˆ’0.266 0.171 1.00 0 .02 1 .03 - 1.14Ã—109\nTable 1. Comparison with existing methods for the OpenAI dataset. The parameters are Î»= 0.3, K = 100 , S= 500 , Îµâˆ—= 0.277,and\nL= 19.8. The search step is with HNSW [28]. Bold and underlined scores represent the best and second-best results, respectively.\n7.1. Comparison with existing methods\nExisting methods We compare our methods with existing\nmethods in Tab. 1. The existing methods are the ANNS\nalone (i.e., HNSW only), clustering, and the GMM [3, 40].\nâ€¢ ANNS alone (no filtering): An initial search is performed\nto obtain Kresults. We directly use them as the output.\nâ€¢ Clustering: After obtaining the initial search result S, we\ncluster the vectors {xs}sâˆˆSintoKgroups using k-means\nclustering. The nearest neighbors of each centroid form\nthe final result K. Clustering serves as a straightforward\napproach to diversifying the initial search results with the\nrunning cost of O(DKS ). To perform clustering, we re-\nquire the original vectors {xn}N\nn=1.\nâ€¢ GMM: GMM is a representative approach for extracting\na diverse subset from a set. After obtaining the initial\nsearch result S, we iteratively add elements to Kaccord-\ning to jâˆ—= arg max jâˆˆS\\K\u0000\nminiâˆˆKâˆ¥xiâˆ’xjâˆ¥2\n2\u0001\n, updat-\ningKasK â† Kâˆª{ jâˆ—}in each step. This GMM approach\nproduces the most diverse results from the set S. With a\nbit of refinement, GMM can be computed in O(DKS ).\nLike k-means clustering, GMM also requires access to the\noriginal vectors {xn}N\nn=1.\nWe consider the scenario of obtaining Susing modern\nANNS methods like HNSW, followed by diversification.\nSince no existing methods can be directly compared in this\ncontext, we use simple clustering and GMM as baselines.\nWell-known DNNS methods, like Maximal Marginal\nRelevance (MMR) [9], are excluded from comparison due\nto their inability to directly utilize ANNS, resulting in slow\nperformance. Directly solving Eq. (1) is also excluded be-\ncause of its high computational cost. Note that MMR can\nbe applied to Srather than the entire database vectors. This\napproach is similar to the GMM described above and can be\nconsidered an extension that takes the distance to the query\ninto account. Although it has a similar runtime as GMM, its\nscore was lower, so we reported the GMM score.\nIn the â€œCost functionâ€ of Tab. 1, the â€œSearchâ€ refers to\nthe first term in Eq. (2), and the â€œDiversificationâ€ refers to\nthe second term. The threshold Îµis the value obtained from\nEq. (7). The runtime is the average of three trials.Results From Tab. 1, we observe the following results:\nâ€¢ In the case of NN search only, it is obviously the fastest;\nhowever, the results are the least diverse (with a diversifi-\ncation term of âˆ’0.107).\nâ€¢ Clustering is simple but not promising. The final score is\nthe worst ( f= 0.223), and it takes 10 times longer than\nsearch-only ( 7.88[ms/query]).\nâ€¢ GMM achieves the most diverse results ( âˆ’0.351), attain-\ning the second-highest final performance ( f= 0.177).\nHowever, GMM is slow ( 14.4[ms/query]), requiring ap-\nproximately 17 times the runtime of search-only.\nâ€¢ The proposed LotusFilter achieves the highest perfor-\nmance ( f= 0.171). It is also sufficiently fast ( 1.03\n[ms/query]), with the filtering step taking only 0.02\n[ms/query]. As a result, it requires only about 1.2 times\nthe runtime of search-only.\nâ€¢ Clustering and GMM consume 40 times more memory\nthan LotusFilter. Clustering and GMM require the orig-\ninal vectors, costing 32ND [bits] using 32-bit floating-\npoints, which becomes especially large for datasets with\na high D. In contrast, the memory cost of the proposed\nmethod is 64LNusing 64-bit integers.\nThe proposed method is an effective filtering approach re-\ngarding performance, runtime, and memory efficiency, es-\npecially for high-dimensional vectors. For low-dimensional\nvectors, simpler baselines may be more effective. Please\nsee the supplemental material for details.\n7.2. Impact of the number of initial search results\nWhen searching, users are often interested in knowing how\nto set S, the size of the initial search result. We evaluated\nthis behavior for the OpenAI dataset in Fig. 3. Here, Î»=\n0.3, andÎµis determined by solving Eq. (7) for each point.\nTaking more candidates in the initial search (larger S)\nresults in the following:\nâ€¢ Overall performance improves (lower f), as having more\ncandidates is likely to lead to better solutions.\nâ€¢ On the other hand, the runtime gradually increases. Thus,\nthere is a clear trade-off in Sâ€™s choice.\n\n102103\nS0.180.200.22f\n0.951 [ms/query]\n1.0111.0950.894 [ms/query]\n0.948\n1.020K= 200\nK= 100Figure 3. Fix K, vary S\n0.2 0.3 0.4 0.5\nÎµ0.330.34f\nFrom test query\nÎµâˆ—by Eq. 7 Figure 4. Evaluate Îµâˆ—by Eq. (7)Runtime [s]\nN Î» Îµâˆ—L Train Build\n9Ã—1030.3 0.39 8.7 96 0.16\n0.5 0.42 19.6 99 0.17\n9Ã—1040.3 0.33 10.1 176 3.8\n0.5 0.36 23.5 177 3.9\n9Ã—1050.3 0.27 18.4 1020 54\n0.5 0.29 29.3 1087 54\nTable 2. Train and build\n7.3. Effectiveness of training\nWe investigated how hyperparameter tuning in the training\nphase affects final performance using the OpenAI dataset.\nWhile simple, we found that the proposed training proce-\ndure achieves sufficiently good performance.\nThe training of Îµas described in Sec. 6 is shown in Fig. 4\n(Î»= 0.3, K= 100 , S= 500 ). Here, the blue dots repre-\nsent the actual calculation of fusing various Îµvalues with\nthe test queries. The goal is to obtain Îµthat achieves the\nminimum value of this curve in advance using training data.\nThe red line represents the Îµâˆ—obtained from the training\nqueries via Eq. (7). Although not perfect, we can obtain\na reasonable solution. These results demonstrate that the\nproposed data structure can perform well by learning the\nparameters in advance using training data.\n7.4. Preprocessing time\nTab. 2 shows the training and construction details (building\nthe cutoff table) with K= 100 andS= 500 for the OpenAI\ndataset. Here, we vary the number of database vectors N.\nFor each condition, Îµis obtained by solving Eq. (7). The\ninsights obtained are as follows:\nâ€¢ AsNincreases, the time for training and construction in-\ncreases, and Lalso becomes larger, whereas Îµâˆ—decreases.\nâ€¢ As Î»increases, Îµâˆ—andLincrease, and training and con-\nstruction times slightly increase.\nâ€¢Lis at most 30 within the scope of this experiment.\nâ€¢ Training and construction each take a maximum of ap-\nproximately 1,100 seconds and 1 minute, respectively.\nThis runtime is sufficiently fast but could potentially be\nfurther accelerated using specialized hardware like GPUs.\n7.5. Qualitative evaluation for texts\nThis section reports qualitative results using the MS\nMARCO dataset (Tab. 3). This dataset contains many short,\nredundant passages, as anticipated for real-world use cases\nof RAG. We qualitatively compare the results of the NNS\nand the proposed DNNS on such a redundant dataset. The\nparameters are K= 10 ,S= 50 ,Î»= 0.3, andÎµâˆ—= 18.5.Query : â€œTonsillitis is a throat infection that occurs on the tonsil.â€\nResults by nearest neighbor search\n1: â€œTonsillitis refers to the inflammation of the pharyngeal tonsils and\nis the primary cause of sore throats.â€\n2: â€œStrep throat is a bacterial infection in the throat and the tonsils.â€\n3: â€œStrep throat is a bacterial infection of the throat and tonsils.â€\n4: â€œStrep throat is a bacterial infection of the throat and tonsils.â€\n5: â€œMastoiditis is an infection of the spaces within the mastoid bone.â€\nResults by diverse nearest neighbor search (proposed)\n1: â€œTonsillitis refers to the inflammation of the pharyngeal tonsils and\nis the primary cause of sore throats.â€œ\n2: â€œStrep throat is a bacterial infection in the throat and the tonsils.â€œ\n3: â€œMastoiditis is an infection of the spaces within the mastoid bone.â€œ\n4: â€œTonsillitis (enlarged red tonsils) is caused by a bacterial (usually\nstrep) or viral infection.â€œ\n5: â€œSpongiotic dermatitis is a usually uncomfortable dermatological\ncondition which most often affects the skin of the chest, abdomen,\nand buttocks.â€œ\nTable 3. Qualitative evaluation on text data using MS MARCO.\nSimple NNS results displayed nearly identical second,\nthird, and fourth-ranked results (highlighted in red), while\nthe proposed LotusFilter eliminates this redundancy. This\ntendency to retrieve similar data from the scattered dataset\nis common if we run NNS. Eliminating such redundant re-\nsults is essential for real-world RAG systems. See the sup-\nplemental material for more examples.\nThe proposed LotusFilter is effective because it obtains\ndiverse results at the data structure level. While engineer-\ning solutions can achieve diverse searches, such solutions\nare complex and often lack runtime guarantees. In contrast,\nLotusFilter is a simple post-processing module with com-\nputational guarantees. This simplicity makes it an advan-\ntageous building block for complex systems, especially in\napplications like RAG.\n7.6. Qualitative evaluation for images\nThis section reports qualitative evaluations of images. Here,\nwe consider an image retrieval task using image features ex-\ntracted from the Revisited Paris dataset (Fig. 5). The param-\neters are set to K= 10 ,S= 100 ,Î»= 0.5, andÎµâˆ—= 1.14.\n\nNN\nSearch\nDiverse \nSearch \n(Proposed)Query1st 2nd                               3rd                                      4th                                        5th \n1st 2nd                             3rd                              4th                                        5th \n1st 2nd                               3rd                                 4th                                 5th \n1st 2nd                                             3rd                            4th                  5th NN\nSearch\nDiverse \nSearch \n(Proposed)QuerySimilar Similar Similar SimilarSimilarSimilar\nSimilarFigure 5. Qualitative evaluation on image data using Revisited Paris.\nIn the first example, a windmill image is used as a query\nto find similar images in the dataset. The NNS results are\nshown in the upper row, while the proposed diverse search\nresults are in the lower row. The NNS retrieves images close\nto the query, but the first, second, and fifth images show\nwindmills from similar angles, with the third and fourth im-\nages differing only in sky color. In a recommendation sys-\ntem, such nearly identical results would be undesirable. The\nproposed diverse search, however, provides more varied re-\nsults related to the query.\nIn the second example, the query image is a photograph\nof the Pompidou Center taken from a specific direction. In\nthis case, all the images retrieved by the NNS have almost\nidentical compositions. However, the proposed approach\ncan retrieve images captured from various angles.\nIt is important to note that the proposed LotusFilter is\nsimply a post-processing module, which can be easily re-\nmoved. For example, if the diverse search results are less\nappealing, simply deactivating LotusFilter would yield the\nstandard search results. Achieving diverse search through\nengineering alone would make it more difficult to switch\nbetween results in this way.\n7.7. Limitations and future works\nThe limitations and future works are as follows:\nâ€¢ LotusFilter involves preprocessing steps. Specifically, we\noptimize Îµfor parameter tuning, and a cutoff table needs\nto be constructed in advance.\nâ€¢ During Îµlearning, Kneeds to be determined in advance.In practical applications, there are many cases where K\nneeds to be varied. If Kis changed during the search, it\nis uncertain whether Îµâˆ—is optimal.\nâ€¢ A theoretical bound has been established for the diver-\nsification term in the cost function; however, there is no\ntheoretical guarantee for the total cost.\nâ€¢ Unlike ANNS alone, LotusFilter requires additional\nmemory for a cutoff table. Although the memory usage is\npredictable at 64LN [bits], it can be considerable, espe-\ncially for large values of N.\nâ€¢ When Dis small, more straightforward methods (such as\nGMM) may be the better option.\nâ€¢ The proposed method determines a global threshold Îµ.\nSuch a single threshold may not work well for challeng-\ning datasets.\nâ€¢ The end-to-end evaluation of the RAG system is planned\nfor future work. Currently, the accuracy is only assessed\nby Eq. (2), and the overall performance within the RAG\nsystem remains unmeasured. A key future direction is\nemploying LLM-as-a-judge to evaluate search result di-\nversity comprehensively.\n8. Conclusions\nWe introduced the LotusFilter, a fast post-processing mod-\nule for DNNS. The method entails creating and using a cut-\noff table for pruning. Our experiments showed that this ap-\nproach achieves diverse searches in a similar time frame to\nthe most recent ANNS.\n\nAcknowledgement\nWe thank Daichi Amagata and Hiroyuki Deguchi for re-\nviewing this paper, and we appreciate Naoki Yoshinaga for\nproviding the inspiration for this work.\nReferences\n[1] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru\nOhta, and Masanori Koyama. Optuna: A next-generation hy-\nperparameter optimization framework. In Proc. ACM KDD ,\n2019. 5\n[2] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru\nOhta, and Masanori Koyama. Optuna: A next-generation\nhyperparameter optimization framework. In Proceedings of\nthe 25th ACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining , 2019. 1\n[3] Daichi Amagata. Diversity maximization in the presence of\noutliers. In Proc. AAAI , 2023. 2, 6\n[4] Fabien Andr Â´e, Anne-Marie Kermarrec, and Nicolas Le\nScouarnec. Quicker adc: Unlocking the hidden potential of\nproduct quantization with simd. IEEE TPAMI , 43(5):1666â€“\n1677, 2021. 2\n[5] Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen.\nAcl2023 tutorial on retrieval-based language models and ap-\nplications, 2023. 1\n[6] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jian-\nfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNa-\nmara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song,\nAlina Stoica, Saurabh Tiwary, and Tong Wang. Ms marco:\nA human generated machine reading comprehension dataset.\narXiv , 1611.09268, 2016. 5\n[7] Dmitry Baranchuk, Artem Babenko, and Yury Malkov. Re-\nvisiting the inverted indices for billion-scale approximate\nnearest neighbors. In Proc. ECCV , 2018. 2\n[8] Sebastian Bruch. Foundations of Vector Retrieval . Springer,\n2024. 1\n[9] Jaime Carbonell and Jade Goldstein. The use of mmr,\ndiversity-based reranking for reordering documents and pro-\nducing summaries. In Proc. SIGIR , 1998. 2, 6\n[10] Daoyuan Chen, Wuchao Li, Yaliang Li, Bolin Ding, Kai\nZeng, Defu Lian, and Jingren Zhou. Learned index with dy-\nnamic Ïµ. InProc. ICLR , 2023. 2\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proc. NAACL-HLT ,\n2019. 5\n[12] Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim\nKraska. Tsunami: A learned multi-dimensional index for\ncorrelated data and skewed workloads. In Proc. VLDB , 2020.\n2\n[13] Matthijs Douze, Alexandre Sablayrolles, and Herv Â´e JÂ´egou.\nLink and code: Fast indexing with graphs and compact re-\ngression codes. In Proc. IEEE CVPR , 2018. 2\n[14] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff\nJohnson, Gergely Szilvasy, Pierre-Emmanuel Mazar Â´e, Maria\nLomeli, Lucas Hosseini, and Herv Â´e JÂ´egou. The faiss library.\narXiv , 2401.08281, 2024. 2, 5[15] Marina Drosou and Evaggelia Pitoura. Search result diversi-\nfication. In Proc. SIGMOD , 2010. 1, 2\n[16] Marina Drosou and Evaggelia Pitoura. Disc diversity: Result\ndiversification based on dissimilarity and coverage. In Proc.\nVLDB , 2012. 2\n[17] Paolo Ferragina and Giorgio Vinciguerra. Learned Data\nStructures . Springer International Publishing, 2020. 2\n[18] Paolo Ferragina and Giorgio Vinciguerra. The pgmindex:\na fully dynamic compressed learned index with provable\nworst-case bounds. In Proc. VLDB , 2020. 2\n[19] Paolo Ferragina, Fabrizio Lillo, and Giorgio Vinciguerra.\nWhy are learned indexes so effective? In Proc. ICML , 2020.\n2\n[20] Cong Fu, Chao Xiang, Changxu Wang, and Deng Cai. Fast\napproximate nearest neighbor search with the navigating\nspreading-out graph. In Proc. VLDB , 2019. 2\n[21] Fuma Hidaka and Yusuke Matsui. Flexflood: Efficiently up-\ndatable learned multi-dimensional index. In Proc. NeurIPS\nWorkshop on ML for Systems , 2024. 2\n[22] Kohei Hirata, Daichi Amagata, Sumio Fujita, and Takahiro\nHara. Solving diversity-aware maximum inner product\nsearch efficiently and effectively. In Proc. RecSys , 2022. 2\n[23] Wenzel Jakob. nanobind: tiny and efficient c++/python bind-\nings, 2022. https://github.com/wjakob/nanobind. 5\n[24] Herv Â´e JÂ´egou, Matthijis Douze, and Cordelia Schmid. Prod-\nuct quantization for nearest neighbor search. IEEE TPAMI ,\n33(1):117â€“128, 2011. 2\n[25] Mykel J. Kochenderfer and Tim A. Wheeler. Algorithms for\nOptimization . The MIT Press, 2019. 5, 1\n[26] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and\nNeoklis Polyzotis. The case for learned index structures. In\nProc. SIGMOD , 2018. 2\n[27] Qiyu Liu, Libin Zheng, Yanyan Shen, and Lei Chen. Stable\nlearned bloom filters for data streams. In Proc. VLDB , 2020.\n2\n[28] Yury A. Malkov and Dmitry A. Yashunin. Efficient and ro-\nbust approximate nearest neighbor search using hierarchical\nnavigable small world graphs. IEEE TPAMI , 42(4):824â€“836,\n2020. 2, 5, 6, 3\n[29] Yusuke Matsui, Takuma Yamaguchi, and Zheng Wang.\nCvpr2020 tutorial on image retrieval in the wild, 2020. 1\n[30] Yusuke Matsui, Yoshiki Imaizumi, Naoya Miyamoto, and\nNaoki Yoshifuji. Arm 4-bit pq: Simd-based acceleration for\napproximate nearest neighbor search on arm. In Proc. IEEE\nICASSP , 2022. 2\n[31] Yusuke Matsui, Martin Aum Â¨uller, and Han Xiao. Cvpr2023\ntutorial on neural search in action, 2023. 1\n[32] Michael Mitzenmacher. A model for learned bloom filters,\nand optimizing by sandwiching. In Proc. NeurIPS , 2018. 2\n[33] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim\nKraska. Learning multi-dimensional indexes. In Proc. SIG-\nMOD , 2020. 2\n[34] Yutaro Oguri and Yusuke Matsui. General and practical tun-\ning method for off-the-shelf graph-based index: Sisap index-\ning challenge report by team utokyo. In Proc. SISAP , 2023.\n2\n\n[35] Yutaro Oguri and Yusuke Matsui. Theoretical and empiri-\ncal analysis of adaptive entry point selection for graph-based\napproximate nearest neighbor search. arXiv , 2402.04713,\n2024. 5\n[36] Naoki Ono and Yusuke Matsui. Relative nn-descent: A\nfast index construction for graph-based approximate nearest\nneighbor search. In Proc. MM , 2023. 2\n[37] Filip Radenovi Â´c, Ahmet Iscen, Giorgos Tolias, Yannis\nAvrithis, and Ond Ë‡rej Chum. Revisiting oxford and paris:\nLarge-scale image retrieval benchmarking. In Proc. IEEE\nCVPR , 2018. 5\n[38] Filip Radenovi Â´c, Giorgos Tolias, and Ond Ë‡rej Chum. Fine-\ntuning cnn image retrieval with no human annotation. IEEE\nTPAMI , 41(7):1655â€“1668, 2018. 5\n[39] Vidyadhar Rao, Prateek Jain, and C.V . Jawahar. Diverse yet\nefficient retrieval using locality sensitive hashing. In Proc.\nICMR , 2016. 2\n[40] Sekharipuram S. Ravi, Daniel J. Rosenkrantz, and Giri Ku-\nmar Tayi. Heuristic and special case algorithms for disper-\nsion problems. Operations Research , 542(2):299â€“310, 1994.\n2, 6, 3\n[41] Rodrygo L. T. Santos, Craig Macdonald, and Iadh Ounis.\nSearch result diversification. Foundations and Trends in In-\nformation Retrieval , 9(1):1â€“90, 2015. 1, 2\n[42] Atsuki Sato and Yusuke Matsui. Fast partitioned learned\nbloom filter. In Proc. NeurIPS , 2023. 2\n[43] Xuan Shan, Chuanjie Liu, Yiqian Xia, Qi Chen, Yusi\nZhang, Kaize Ding, Yaobo Liang, Angen Luo, and Yuxiang\nLuo. Glow: Global weighted self-attention network for web\nsearch. In Proc. IEEE Big Data , 2021. 2\n[44] Harsha Vardhan Simhadri, George Williams, Martin\nAum Â¨uller, Matthijs Douze, Artem Babenko, Dmitry\nBaranchuk, Qi Chen, Lucas Hosseini, Ravishankar Krish-\nnaswamny, Gopal Srinivasa, Suhas Jayaram Subramanya,\nand Jingdong Wang. Results of the neuripsâ€™21 challenge on\nbillion-scale approximate nearest neighbor search. In Proc.\nPMLR , 2022. 2\n[45] Harsha Vardhan Simhadri, Martin Aum Â¨uller, Amir Ing-\nber, Matthijs Douze, George Williams, Magdalen Dobson\nManohar, Dmitry Baranchuk, Edo Liberty, Frank Liu, Ben\nLandrum, Mazin Karjikar, Laxman Dhulipala, Meng Chen,\nYue Chen, Rui Ma, Kai Zhang, Yuzheng Cai, Jiayang Shi,\nYizhuo Chen, Weiguo Zheng, Zihao Wan, Jie Yin, and Ben\nHuang. Results of the big ann: Neuripsâ€™23 competition.\narXiv , 2409.17424, 2024. 5\n[46] Suhas Jayaram Subramanya, Fnu Devvrit, Harsha Vardhan\nSimhadri, Ravishankar Krishnawamy, and Rohan Kadekodi.\nDiskann: Fast accurate billion-point nearest neighbor search\non a single node. In Proc. NeurIPS , 2019. 2\n[47] Kapil Vaidya, Eric Knorr, Michael Mitzenmacher, and Tim\nKraska. Partitioned learned bloom filters. In Proc. ICLR ,\n2021. 2\n[48] Mengzhao Wang, Xiaoliang Xu, Qiang Yue, and Yuxiang\nWang. A comprehensive survey and experimental compari-\nson of graph-based approximate nearest neighbor search. In\nProc. VLDB , 2021. 2[49] Jiacheng Wu, Yong Zhang, Shimin Chen, Jin Wang, Yu\nChen, and Chunxiao Xing. Updatable learned index with\nprecise positions. In Proc. VLDB , 2021. 2\n[50] Kaiping Zheng, Hongzhi Wang, Zhixin Qi, Jianzhong Li,\nand Hong Gao. A survey of query result diversification.\nKnowledge and Information Systems , 51:1â€“36, 2017. 1, 2\n\nLotusFilter: Fast Diverse Nearest Neighbor Search via a Learned Cutoff Table\nSupplementary Material\nA. Selection of data structures\nWe introduce alternative data structures for Sand demon-\nstrate that the proposed OrderedSet is superior. As intro-\nduced in Sec. 5.2, an input array v= [v[1], v[2], . . . , v [V]]\ncontaining Velements is given. The goal is to realize a data\nstructure that efficiently performs the following operations:\nâ€¢ POP: Retrieve and remove the foremost element while\npreserving the order of the input array.\nâ€¢ R EMOVE : Given an element as input, delete it from the\ndata structure.\nThe average computational complexity of these opera-\ntions for various data structures, including arrays, sets, pri-\nority queues, lists, and their combinations, are summarized\nin Tab. A.\nArray When using an array directly, the P OPoperation\nfollows the same procedure as OrderedSet. However, ele-\nment removal incurs a cost of O(V). This removal is im-\nplemented by performing a linear search and marking the\nelement with a tombstone. Due to the inefficiency of this\nremoval process, arrays are not a viable option.\nSet If we convert the input array into a set (e.g.,\nstd::unordered set in C++ or set in Python), el-\nement removal can be achieved in O(1). However, since\nthe set does not maintain element order, we cannot perform\nthe P OPoperation, making this approach unsuitable.\nList Consider converting the input array into a list (e.g., a\ndoubly linked list such as std::list in C++). The first\nposition in the list is always accessible, and removal from\nthis position is straightforward, so P OPcan be executed in\nO(1). However, for R EMOVE , a linear search is required to\nlocate the element, resulting in a cost of O(V). Hence, this\napproach is slow.\nPriority queue A priority queue is a commonly used data\nstructure for implementing P OP. C++ STL has a standard\nimplementation such as std::priority queue . If the\ninput array is converted into a priority queue, the P OPop-\neration can be performed in O(logV). However, priority\nqueues are not well-suited for removing a specified element,\nas this operation requires a costly full traversal in a naive\nimplementation. Thus, priority queues are inefficient for\nthis purpose.List + dictionary Combining a list with a dictionary\n(hash table) achieves both P OPand R EMOVE operations in\nO(1), making it the fastest from a computational complex-\nity perspective. The two data structures, a list and a dic-\ntionary, are created in the construction step. First, the input\narray is converted into a list to maintain order. Next, the dic-\ntionary is created with a key corresponding to an element in\nthe array, and a value is a pointer pointing to a correspond-\ning node in the list.\nDuring removal, the element is removed from the dic-\ntionary, and the corresponding node in the list is also re-\nmoved. This node removal is possible since we know its\naddress from the dictionary. This operation ensures the list\nmaintains the order of remaining elements. For P OP, the\nfirst element in the list is extracted and removed, and the\ncorresponding element in the dictionary is also removed.\nWhile the list + dictionary combination achieves the best\ncomplexity, its constant factors are significant. Construct-\ning two data structures during initialization is costly. R E-\nMOVE must also remove elements from both data structures.\nFurthermore, in our target problem (Algorithm 2), the cost\nof set deletions within the for-loop ( L6) is already O(L).\nThus, even though P OPisO(1), it does not improve the\noverall computational complexity. Considering these fac-\ntors, we opted for OrderedSet.\nOrderedSet As summarized in Tab. A, our OrderedSet\nintroduced in Sec. 5.2 combines the advantages of arrays\nand hash tables. During initialization, only a set is con-\nstructed. The only operation required for removals is dele-\ntion from the set, resulting in smaller constant factors than\nother methods.\nB. Details of training\nIn Algorithm A, we describe our training approach for Îµ\n(Eq. (7)) in detail. The input to the algorithm is the train-\ning query vectors QtrainâŠ‚RD, which can be prepared by\nusing a part of the database vectors. The output is Îµâˆ—that\nminimizes the evaluation function fâˆ—(Îµ,q)defined in Eq.\n(6). Since this problem is a non-linear single-variable opti-\nmization, we can apply black-box optimization [2], but we\nuse a more straightforward approach, bracketing [25].\nAlgorithm A requires several hyperparameters:\nâ€¢ÎµmaxâˆˆR: The maximum range of Îµ. This value can be\nestimated by calculating the inter-data distances for sam-\npled vectors.\nâ€¢WâˆˆR: The number of search range divisions. We will\ndiscuss this in detail later.\n\nMethod P OP() R EMOVE (a) Constant factor Overall complexity of Algorithm 2\nArray O(âˆ†) O(V) O(T+KLS )\nSet (hash table) - O(1) N/A\nList O(1) O(V) O(T+KLS )\nPriority queue O(logV)O(V) O(T+KLS )\nList + dictionary (hash table) O(1) O(1) Large O(T+S+KL)\nOrderedSet: array + set (hash table) O(âˆ†) O(1) O(T+S+KL)\nTable A. The average computational complexity to achieve operations on S\nAlgorithm A: Training for Îµ\nInput: QtrainâŠ‚RD\nHyper params: ÎµmaxâˆˆR,WâˆˆR,I,\nÎ»âˆˆ[0,1],S,K\nOutput: Îµâˆ—âˆˆR\n1Îµleftâ†0 # Lower bound\n2Îµrightâ†Îµmax # Upper bound\n3râ†Îµrightâˆ’Îµleft # Search range\n4Îµâˆ—â† âˆž\n5fâˆ—â† âˆž\n6repeat 5 times do\n# Sampling Wcandidates at\nequal intervals from the search\nrange\n7E â†n\nÎµleft+iÎµleftâˆ’Îµright\nW|iâˆˆ {0, . . . , W }o\n# Evaluate all candidates and\nfind the best one\n8 forÎµâˆˆ Edo\n9 fâ†E\nqâˆˆQtrain[fâˆ—(Îµ,q)]\n10 iff < fâˆ—then\n11 Îµâˆ—â†Îµ\n12 fâˆ—â†f\n13 râ†r/2 # Shrink the range\n# Update the bounds\n14 Îµleftâ†max( Îµâˆ—âˆ’r,0)\n15 Îµrightâ†min(Îµâˆ—+r, Îµmax)\n16return Îµâˆ—\nâ€¢ LotusFilter parameters: These include I,Î»âˆˆ[0,1],S,\nandK. Notably, this training algorithm fixes Î»,S, and\nK, and optimizes Îµunder these conditions.\nFirst, the search range for the variable Îµis initialized in\nL1andL2. Specifically, we consider the range [Îµleft, Îµright]\nand examine the variable within this range Îµâˆˆ[Îµleft, Îµright].\nThe size of this range is recorded in L3. The optimization\nloop is executed in L6, where we decided to perform five\niterations. In L7,Wcandidates are sampled at equal inter-\nvals from the current search range of the variable. We eval-uate all candidates in L8-12 , selecting the best one. Subse-\nquently, the search range is narrowed in L13-15 . Here, the\nsize of the search range is gradually reduced in L13. The\nsearch range for the next iteration is determined by center-\ning around the current optimal value Îµâˆ—and extending rin\nboth directions ( L14-15 ).\nThe parameter Wis not a simple constant but is dynam-\nically scheduled. Wis set to 10 for the first four iterations\nto enable coarse exploration over a wide range. In the final\niteration, Wis increased to 100 to allow fine-tuned adjust-\nments after the search range has been adequately narrowed.\nThe proposed training method adopts a strategy similar\nto beam search, permitting some breadth in the candidate\npool while greedily narrowing the range recursively. This\napproach avoids the complex advanced machine learning\nalgorithms, making it simple and fast (as shown in Table\n2, the maximum training time observed in our experiments\nwas less than approximately 1100 seconds on CPUs). As il-\nlustrated in Fig. 4, this training approach successfully iden-\ntifies an almost optimal parameter.\nC. Experiments on memory-efficient datasets\nWe present the experimental results on a memory-efficient\ndataset and demonstrate that simple baselines can be vi-\nable choices. Here, we use the Microsoft SpaceV 1M\ndataset [44]. This dataset consists of web documents rep-\nresented by features extracted using the Microsoft SpaceV\nSuperion model [43]. While the original dataset contains\nN= 109vectors, we used the first 107vectors for our ex-\nperiments. We utilized the first 103entries from the query\nset for query data. The dimensionality of the vectors is 100,\nwhich is relatively low-dimensional, and each element is\nrepresented as an 8-bit integer. Therefore, compared to fea-\ntures like those from CLIP, which are represented in float\nand often exceed 1000 dimensions, this dataset is signifi-\ncantly more memory-efficient.\nTab. B shows the results. While the overall trends are\nsimilar to those observed with the OpenAI dataset in Ta-\nble 1, there are key differences:\nâ€¢ LotusFilter remains faster than Clustering and GMM, but\nthe runtime advantage is minor. This result is because Lo-\n\nCost function ( â†“) Runtime [ms/query] ( â†“) Memory overhead [bit] ( â†“)\nFiltering Search Diversification Final ( f) Search Filter Total {xn}N\nn=1{Ln}K\nk=1\nNone (Search only) 10197 âˆ’778 6904 0 .241 - 0.241 - -\nClustering 11384 âˆ’2049 7354 0 .309 0 .372 0 .681 8 Ã—109-\nGMM [40] 12054 âˆ’9525 5580 0.310 0 .367 0 .677 8 Ã—109-\nLotusFilter (Proposed) 10648 âˆ’5592 5776 0.310 0 .016 0 .326 - 3.7Ã—1010\nTable B. Comparison with existing methods for the MS SpaceV 1M dataset. The parameters are Î»= 0.3, K = 100 , S= 300 , Îµâˆ—= 5869 ,\nandL= 58.3. The search step is with HNSW [28]. Bold and underlined scores represent the best and second-best results, respectively.\ntusFilterâ€™s performance does not depend on D, whereas\nClustering and GMM are D-dependent, and thus their\nperformance improves relatively as Ddecreases.\nâ€¢ Memory usage is higher for LotusFilter. This is due to the\ndataset being represented as memory-efficient 8-bit inte-\ngers, causing the cutoff table of LotusFilter to consume\nmore memory in comparison.\nFrom the above, simple methods, particularly GMM, are\nalso suitable for memory-efficient datasets.\nD. Additional results of qualitative evaluation\non texts\nIn Tab. C, we present additional results of a diverse search\non text data as conducted in Sec 7.5. Here, we introduce the\nresults for three queries as follows.\nFor the first query, â€œThis condition...â€, three identical\nresults appear at the first three results. When considering\nRAG, it is typical for the information source to contain re-\ndundant data like this. Removing such redundant data be-\nforehand can sometimes be challenging. For instance, if the\ndata sources are continuously updated, it may be impossible\nto check for redundancy every time new data is added. The\nproposed LotusFilter helps eliminate such duplicate data as\na simple post-processing. LotusFilter does not require mod-\nifying the data source or the nearest neighbor search algo-\nrithm.\nFor the second query, â€œPsyllium...â€, the first and second\nresults, as well as the third and fourth results, are almost\nidentical. This result illustrates that multiple types of redun-\ndant results can often emerge during the search. Without\nusing the proposed LotusFilter, removing such redundant\nresults during post-processing is not straightforward.\nFor the third query, â€œIn the United...â€, while there is no\nperfect match, similar but not identical sentences are filtered\nout. We can achieve it because LotusFilter identifies redun-\ndancies based on similarity in the feature space. As shown,\nLotusFilter can effectively eliminate similar results that can-\nnot necessarily be detected through exact string matching.\n\nQuery : â€œThis condition is usually caused by bacteria entering the bloodstream and infecting the heart.â€\nResults by nearest neighbor search\n1: â€œIt is a common symptom of coronary heart disease, which occurs when vessels that carry blood to the heart become narrowed and blocked\ndue to atherosclerosis.â€\n2: â€œIt is a common symptom of coronary heart disease, which occurs when vessels that carry blood to the heart become narrowed and blocked\ndue to atherosclerosis.â€\n3: â€œIt is a common symptom of coronary heart disease, which occurs when vessels that carry blood to the heart become narrowed and blocked\ndue to atherosclerosis.â€\n4: â€œCardiovascular disease is the result of the build-up of plaques in the blood vessels and heart.â€\n5: â€œThe most common cause of myocarditis is infection of the heart muscle by a virus.â€\nResults by diverse nearest neighbor search (proposed)\n1: â€œIt is a common symptom of coronary heart disease, which occurs when vessels that carry blood to the heart become narrowed and blocked\ndue to atherosclerosis.â€\n2: â€œCardiovascular disease is the result of the build-up of plaques in the blood vessels and heart.â€\n3: â€œThe most common cause of myocarditis is infection of the heart muscle by a virus.â€\n4: â€œThe disease results from an attack by the bodyâ€™s own immune system, causing inflammation in the walls of arteries.â€\n5: â€œThe disease disrupts the flow of blood around the body, posing serious cardiovascular complications.â€\nQuery : â€œPsyllium fiber comes from the outer coating, or husk of the psyllium plantâ€™s seeds.â€\nResults by nearest neighbor search\n1: â€œPsyllium is a form of fiber made from the Plantago ovata plant, specifically from the husks of the plantâ€™s seed.â€\n2: â€œPsyllium is a form of fiber made from the Plantago ovata plant, specifically from the husks of the plantâ€™s seed.â€\n3: â€œPsyllium husk is a common, high-fiber laxative made from the seeds of a shrub.â€\n4: â€œPsyllium seed husks, also known as ispaghula, isabgol, or psyllium, are portions of the seeds of the plant Plantago ovata, (genus Plantago), a\nnative of India and Pakistan.â€\n5: â€œPsyllium seed husks, also known as ispaghula, isabgol, or psyllium, are portions of the seeds of the plant Plantago ovata, (genus Plantago), a\nnative of India and Pakistan.â€\nResults by diverse nearest neighbor search (proposed)\n1: â€œPsyllium is a form of fiber made from the Plantago ovata plant, specifically from the husks of the plantâ€™s seed.â€\n2: â€œPsyllium husk is a common, high-fiber laxative made from the seeds of a shrub.â€\n3: â€œFlaxseed oil comes from the seeds of the flax plant (Linum usitatissimum, L.).â€\n4: â€œThe active ingredients are the seed husks of the psyllium plant.â€\n5: â€œSisal fibre is derived from the leaves of the plant.â€\nQuery : â€œIn the United States there are grizzly bears in reserves in Montana, Idaho, Wyoming and Washington.â€\nResults by nearest neighbor search\n1: â€œIn the United States there are grizzly bears in reserves in Montana, Idaho, Wyoming and Washington.â€\n2: â€œIn North America, grizzly bears are found in western Canada, Alaska, Wyoming, Montana, Idaho and a potentially a small population in\nWashington.â€\n3: â€œIn the United States black bears are common in the east, along the west coast, in the Rocky Mountains and parts of Alaska.â€\n4: â€œMajor populations of Canadian lynx, Lynx canadensis, are found throughout Canada, in western Montana, and in nearby parts of Idaho and\nWashington.â€\n5: â€œMajor populations of Canadian lynx, Lynx canadensis, are found throughout Canada, in western Montana, and in nearby parts of Idaho and\nWashington.â€\nResults by diverse nearest neighbor search (proposed)\n1: â€œIn the United States there are grizzly bears in reserves in Montana, Idaho, Wyoming and Washington.â€\n2: â€œIn the United States black bears are common in the east, along the west coast, in the Rocky Mountains and parts of Alaska.â€\n3: â€œMajor populations of Canadian lynx, Lynx canadensis, are found throughout Canada, in western Montana, and in nearby parts of Idaho and\nWashington.â€\n4: â€œToday, gray wolves have populations in Alaska, northern Michigan, northern Wisconsin, western Montana, northern Idaho, northeast Oregon\nand the Yellowstone area of Wyoming.â€\n5: â€œThere are an estimated 7,000 to 11,200 gray wolves in Alaska, 3,700 in the Great Lakes region and 1,675 in the Northern Rockies.â€\nTable C. Additional qualitative evaluation on text data using MS MARCO.",
  "textLength": 59715
}