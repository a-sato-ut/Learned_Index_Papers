{
  "paperId": "7a63ba6f585200fa60b9ad981aaea9d4790a6229",
  "title": "Barbarians at the Gate: How AI is Upending Systems Research",
  "pdfPath": "7a63ba6f585200fa60b9ad981aaea9d4790a6229.pdf",
  "text": "Barbarians at the Gate:\nHow AI is Upending Systems Research\nAudrey Cheng *, Shu Liu,*Melissa Pan*, Zhifei Li, Bowen Wang, Alexander Krentsel,\nTian Xia, Mert Cemri, Jongseok Park, Shuo Yang, Jeff Chen, Lakshya Agrawal,\nAditya Desai, Jiarong Xing, Koushik Sen, Matei Zaharia, Ion Stoica\nUC Berkeley\nOctober 13, 2025\nABSTRACT\nArtificial Intelligence (AI) is starting to transform the research process as we know\nit by automating the discovery of new solutions. Given a task, the typical AI-\ndriven approach is (i) to generate a set of diverse solutions, and then (ii) to verify\nthese solutions and select one that solves the problem. Crucially, this approach\nassumes the existence of a reliable verifier, i.e., one that can accurately determine\nwhether a solution solves the given problem. We argue that systems research, long\nfocused on designing and evaluating new performance-oriented algorithms, is par-\nticularly well-suited for AI-driven solution discovery. This is because system per-\nformance problems naturally admit reliable verifiers: solutions are typically im-\nplemented in real systems or simulators, and verification reduces to running these\nsoftware artifacts against predefined workloads and measuring performance. We\nterm this approach as AI-Driven Research for Systems (ADRS), which iteratively\ngenerates, evaluates, and refines solutions. Using OpenEvolve, an existing open-\nsource ADRS instance, we present case studies across diverse domains, includ-\ning multi-region cloud scheduling, load balancing for Mixture-of-Experts infer-\nence, LLM-based SQL queries, and transaction scheduling. In multiple instances,\nADRS discovers algorithms that outperform state-of-the-art human designs (e.g.,\nachieving up to 5.0×runtime improvements or 26% cost reductions). We distill\nbest practices for guiding algorithm evolution, from prompt design to evaluator\nconstruction, for existing frameworks. We then discuss the broader implications\nfor the systems community: as AI assumes a central role in algorithm design,\nwe argue that human researchers will increasingly focus on problem formulation\nand strategic guidance. Our results highlight both the disruptive potential and the\nurgent need to adapt systems research practices in the age of AI.\n1 INTRODUCTION\nOne of the most ambitious goals of artificial intelligence (AI) is to revolutionize scientific discovery\nby automating algorithm design, experiment execution, and even the research process itself. While\nthe realization of this goal will likely be uneven—with certain domains being transformed earlier\nand more profoundly than others—AI-driven approaches Novikov et al.; Sharma (2025) have already\nreached a level of capability where they can meaningfully contribute to computer systems research.\nThis raises fundamental questions about how we should conduct research.\nA significant portion of systems research—spanning networking, databases, and distributed\nsystems—is dedicated to enhancing performance. This is typically achieved through the meticu-\nlous, human-driven design of new algorithms for tasks such as routing Boukerche et al. (2011);\n*Equal contribution, ordered alphabetically.\n1arXiv:2510.06189v3  [cs.AI]  10 Oct 2025\n\nSirika & Mahajan (2016); A. et al. (2023), scheduling Wu et al. (2024); Cheng et al. (2024), and\nresource management Gao et al. (2024); Khan et al. (2020). Crucially, the novelty and efficacy of\nthese algorithms are often the primary metrics for a publishable paper.\nThe central thesis of this paper is that a new class of AI-driven approaches, which we term AI-Driven\nResearch for Systems (ADRS), is beginning to show promising results in automated algorithm dis-\ncovery, and will ultimately prompt a re-evaluation of the traditional role of systems researchers. To\nsubstantiate this claim, we follow previous work in presenting several case studies using existing\nADRS frameworks (e.g., OpenEvolve Sharma (2025)) to generate algorithms that match or even\nexceed the performance of state-of-the-art, human-designed solutions for a range of computer sys-\ntems research problems Wu et al. (2024); AI (2024). For example, in a load-balancing problem for a\nMixture-of-Experts (MoE) model, OpenEvolve discovered an algorithm to rebalance experts across\nGPUs that is 5.0×faster than the best-known baseline. In a job scheduling problem aimed at reduc-\ning costs by using spot instances across multiple cloud regions, OpenEvolve generated a solution\nthat achieved roughly 30% greater savings than an expert-developed baseline. By using powerful\nLLMs such as GPT-4o, o3, and Gemini 2.5 Pro, most of our case studies produced solutions that\nmatched or surpassed the state-of-the-art within a few hours, at a cost of only a few dollars to tens\nof dollars (see Table 1). We note that these results were obtained by different students without ex-\ntensive ablation studies. Therefore, the reported results should be viewed as a lower bound on the\ncapabilities of the ADRS frameworks. We expect stronger results as researchers gain more expe-\nrience with these frameworks and as both the frameworks and their underlying models continue to\nimprove.\nOne of the main reasons system performance problems are a particularly good fit for AI-driven re-\nsearch is that their solutions can be verified relatively easily. The typical pattern for solving a\nproblem using AI is to generate a diverse set of solutions and then verify which solutions actually\nsolve the problem, if any Trinh et al. (2024); Silver et al. (2017). Thus, verification accuracy is\nkey to the success of an AI-driven approach in a given problem domain. In general, verification is\nchallenging. For example, it can be difficult to verify that an AI-generated program or an answer to\na trivia question is correct. Fortunately, this is not the case for system performance problems. In\nsuch cases, solutions—such as new algorithms and protocols—are typically integrated into artifacts\nlike networks, databases, or operating systems. These solutions are then evaluated by running the\nsystem in which they are implemented against representative workloads and inspecting the resulting\nperformance metrics. For instance, to evaluate a new routing protocol, we implement it in the routers\nof a network system and measure outputs like throughput, delays, loss rate for various workloads. In\nanother example, to evaluate a new CPU scheduler, we can implement it in an operating system, and\nthen measure the response times for mixes of interactive and batch applications. Moreover, to avoid\nthe high overhead of relying on real systems for evaluations, researchers often develop simulators\nthat capture the salient features of real systems, enabling rapid and low-cost iteration. These simu-\nlators allow researchers to evaluate solutions quickly before deploying them in live systems, making\nsystems research particularly well-suited for AI-driven exploration.\nIn the broader context of AI-driven research, our focus is deliberately narrow. Not only do we\nrestrict our scope to the systems domain, but in this context, we focus only on the task of solution\ndiscovery, while largely ignoring other aspects in the research process, like problem formulation,\nliterature survey, or paper writing. Narrowing the focus has several advantages. First, as discussed,\nit reduces the risk of hallucinations, since solution correctness can be grounded in empirical system\nperformance. Second, it allows us to go deeper and develop stronger solutions within one domain,\nrather than spreading efforts across many. At the same time, if successful, the systems area is\nimpactful enough to accelerate progress in other computer science areas by providing faster, more\nefficient infrastructures.\nFinally, given this impending shift towards AI-driven research, we discuss its consequences for the\nsystems community. As AI takes on the role of algorithm discovery and optimization, the emphasis\nfor human researchers will likely pivot to problem formulation, high-level ideation, and strategic\ndirection. In this new model, the researcher acts as an advisor to powerful AI research assistants:\ndefining meaningful problems, proposing creative starting points, and distilling insights from gener-\nated solutions. This approach can create a powerful virtuous cycle: the same AI-driven methodolo-\ngies can be applied to improve the AI systems themselves, leading to a compounding acceleration\nof the pace of discovery.\n2\n\nThis paper serves as a call to action for the systems community to proactively consider these changes,\nadapt our skills, and guide the co-evolution of human and AI in research. To facilitate this transition,\nwe provide best practices for leveraging these technologies, outline their benefits, and highlight\nseveral key open problems. Ultimately, leveraging AI will enable researchers to dedicate their time\nto the most creative and fulfilling aspects of their work.\n2 RELATEDWORK\nADRS builds on a long line of research that combines large-scale search with machine learning to\ntackle complex problems.\nPre-LLM AI for System Optimizations.Prior to LLMs, machine learning had already been widely\napplied to systems. In databases, learned models were used for query optimization Marcus et al.\n(2019), cardinality estimation Yang et al. (2022; 2020), learned indexes Kraska et al. (2018), and\nautomated system tuning Aken et al. (2017). Reinforcement learning (RL) and other learning-based\ntechniques have advanced core networking problems, including congestion control Jay et al. (2018),\npacket classification Liang et al. (2019), and topology modeling Rusek et al. (2020). More broadly,\nRL has been applied in systems for scheduling over data processing workloads Mao et al. (2019),\nphysical device placement Mirhoseini et al. (2017), and video streaming Du et al. (2020).\nAutomated discovery with learned approaches.A series of advances in AI have demonstrated the\npower of automated discovery in increasingly complex scientific and computational domains. Early\nsuccesses include AlphaGo Silver et al. (2017) and AlphaZero Silver et al. (2018), which demon-\nstrated how search and RL can master games, and AlphaFold Jumper et al. (2021), which achieved\nbreakthroughs in protein structure prediction. AlphaDev Mankowitz et al. (2023) extended these\nideas to discover efficient low-level algorithms, while AlphaChip Goldie & Mirhoseini (2024) uses\nRL to generate production chip layouts. Big Sleep Walker (2025) applies AI agents to detect secu-\nrity vulnerabilities in software. More recently, benchmarks, such as AlgoTune Press et al. (2025),\nhave been developed to evaluate LLM abilities to optimize programs for different domains.\nLLM-based coding assistants.LLMs have transformed code generation to accelerate the research\nprocess. Coding assistants like GitHub Copilot GitHub (2021), Cursor Inc. (2024), and Codex Ope-\nnAI (2025), and Claude Code Anthropic (2025) help researchers rapidly prototype ideas, build sim-\nulators, and implement baselines. These tools enable high-level algorithmic ideas to be rapidly\ntranslated into working implementations. Recent work also explores using LLMs for performance-\ncritical code Hong et al. (2025), such as GPU kernel code generation and optimization Ouyang et al.\n(2025), further illustrating their potential as building blocks for ADRS.\nLLM-driven research.Beyond coding assistance, recent work leverages LLMs to automate larger\nparts of the research process. Frameworks such as AlphaEvolve Novikov et al.; Nadga et al. (2025)\nand OpenEvolve Sharma (2025) use MAP elites algorithm and island-based population models to\nevolve and discover new algorithms; GEPA Agrawal et al. (2025) employs reflective prompt evo-\nlution for better LLM generation; and LLM4AD Liu et al. (2024a), which provides a unified plat-\nform for algorithm design. ShinkaEvolve Lange et al. (2025) emphasizes sample-efficient evolu-\ntion, EvoPrompt Guo et al. (2023) focuses on using genetic algorithms to optimize prompts, Meta-\nMuse Ma et al. (2025) presents a self-reflective framework for algorithm generation, and Policy-\nSmith Dwivedula et al. (2025) selects top-performing solutions in designing algorithms.\nEnd-to-end research automation attempts are also emerging. MLGym Nathani et al. (2025) offers\nstandardized benchmarks for AI research agents, while Code Researcher Singh et al. (2025) explores\nLLM-based agents for large-scale software engineering tasks. Recent work on self-evolving AI\nagents systematically optimizes their internal components Fang et al. (2025). There is growing\ninterest in applying these approaches to broader systems problems Liang et al. (2025); Zhou (2024).\nOur work focuses on automated algorithm discovery for the systems domain, where strong evalua-\ntors enable reliable verification – a critical requirement for productive automation.\n3\n\n3 WHYAI-DRIVENRESEARCH FORSYSTEMS?\nIn this paper, we advocate an AI-driven approach to systems performance problems. While perfor-\nmance optimization is not the only focus of systems research, it remains a central one—a brief sur-\nvey of top systems, networking, and database venues (NSDI, OSDI, SIGMOD, SOSP, and VLDB)\nshows that over one-third of published papers feature performance optimization algorithms as their\ncore contribution. The main reason we believe an AI-driven approach is particularly well suited to\nsuch problems is that it is relatively straightforward to develop robust and cost-effective verification\nprocesses to evaluate candidate solutions.\nFirst, it is easy to verify whether a given solution improves a system’s performance. Such solutions\ntypically introduce new techniques or algorithms that are implemented directly within the systems\nthey aim to optimize. Verification then reduces to running these systems under representative work-\nloads and checking whether they outperform the baselines on the relevant performance metrics.\nSecond, the solutions for systems performance problems typically preserve the correctness of the\noriginal solution, or, at least, it is relatively easy to verify if they do. For instance, it is easy to\nverify whether a load-balancing algorithm schedules all assigned tasks or whether a network router\nforwards all the packets it receives.\nThird, the portions of system code that usually must undergo evolution is often relatively small,\ne.g., the core logic of a scheduler, load balancer, or resource allocator. This makes the generated\ncode easier for humans to interpret which can further help with verifying its correctness. In all\nof our case studies, we were able to readily understand the generated solutions and identify their\nmain ideas and techniques (see Section 5). As these tools become more powerful, we expect their\nscope of modification to grow and extend across multiple components, e.g., when designing complex\ndistributed protocols. Maintaining code interpretability in such cases is an important topic for future\nresearch.\nFinally, systems researchers often use simulators to develop and evaluate solutions before deploying\nthem in real systems. Simulator-based verification is relatively cheap. Thus, even if the search pro-\ncess is inefficient and produces more candidate solutions than is strictly necessary, their evaluation\nremains practical. For example, each of our case studies required only a few hours and cost no more\nthan several tens of dollars. That said, building simulators for complex systems (e.g., operating sys-\ntems, databases) that are not only inexpensive but also faithful is far from trivial, and it remains a\ntopic for future research (see Section 7.2.1).\n4 USINGAITOACCELERATESYSTEMSRESEARCH\nThis section provides an overview of the systems research process and then introduces the AI-Driven\nResearch for Systems (ADRS) approach which accelerates this process through automatic solution\ndiscovery and evaluation.\n4.1 SYSTEMSPERFORMANCERESEARCHPROCESS\nThe typical research process employed by systems researchers can take many weeks or months.\nBroadly speaking, it consists of five stages (Figure 1):\n•Problem Formulation:Define the problem to solve, such as improving system through-\nput or latency. The entire research process is organized around solving this problem and\ncommunicating the results.\n•Evaluation Framework:Develop a framework to implement and evaluate potential solu-\ntions. This framework can be the system itself or a simulator that approximates the system’s\nbehavior. Even when a system is available, a simulator may be built to accelerate develop-\nment. In addition, researchers collect or use existing workloads (traces) or benchmarks to\ndrive the evaluation.\n•Solution:Develop a solution (e.g., algorithm) to the problem, such as a new scheduling,\nresource allocation, or search algorithm.\n•Evaluation:Implement the solution in the simulator or system, evaluate its performance\nusing the selected workloads, and compare the results against the baseline(s). If the solution\n4\n\n000Solution \nDesign new \nsolution to solve \nthe problem Paper \nWrite-Up \nCommunicate \nﬁndings in \nresearch paper Evaluation \nParametric evaluation \n(formula, cost model, \netc.) in an emulator, \nsimulator, or real system Scientist \nDevelop the system or a \nsimulator, integrate \nworkload traces, implement \nbaselines Evaluation \nFramework \nExperimentation \nloop Problem \nFormulation \nLiterature review, \nproblem setup: \nhypothesis, objective, \nconstraints, workloads \n-Figure 1: The five stages of the systems research process. In this paper, we show how AI can\nautomate theSolutionandEvaluationstages (grey area).\ndoes not show improvements, researchers return to theSolutionstage to refine the approach\nor develop alternatives.\n•Paper Write-Up:Once a solution achieves the desired results, document the findings for\npublication.\nProb Formulation\n21.5%\nEval Framework22.9%\nSolution (Alg Design)21.5%Evaluation 20.1%Writing\n14.0%Avg. Time Distribution Across Research Process\nFigure 2: Time spent in various stages of the systems research process in systems, based on a survey\nof 31 PhD students.Algorithm Design(21.5%) andEvaluation(20.1%) together account for over\n40% of total effort, highlighting a significant opportunity for leveraging AI to accelerate this process.\nFigure 2 shows the approximate time spent in each of these stages, based on survey data from over\n30 systems graduate students from a US university.\n4.2 AI-DRIVENRESEARCH FORSYSTEMS(ADRS)\n00000 Solution \nGenerator  \nLLM ensemble to \ngenerate solutions Evaluator \nTests solutions and \nassigns scores & \nfeedback \nconfigs (e.g., LLMs) \nScientist \nProblem \nFormulation \nPaper \nWrite-Up \nEvaluation \nFramework \nSolution \nSelector \nSelect promising \nsolutions to reﬁne Prompt \nGenerator \nCreates context-rich \nprompts Storage \nStores solutions \nscores & \nfeedbacks Inner loop \nAI-Driven Research for System (ADRS) Observations \n(e.g., solution, feedback) Outer loop Evaluator + \nInitial Solution  Problem \nstatement \nFigure 3: The AI-driven Research for Systems (ADRS) architecture shown in the context of the sys-\ntems research process (see in Figure 1). ADRS (grey area) automates theSolutionandEvaluation\nstages.\nAs Large Language Models (LLMs) have progressed from simple text completion to sophisticated\nreasoning and tool use, new architectures have been developed to enhance the reliability and scope\n5\n\nof tasks to which they can be applied. In this paper, we focus on how LLMs can be used to design,\nimplement, and evaluate new solutions (e.g., algorithms) to solve systems research problems. We\ncall this approach the AI-Driven Research for Systems (ADRS) and depict it in Figure 3. Basically,\nADRS implements the two iterative stages of the systems research process shown in Figure 1:So-\nlutionandEvaluation. Together, these two stages account for about 40% of the time spent (based\non a survey of over 30 systems researchers, Figure 2) in producing new results.\nAt its core, ADRS implements a loop that, in each iteration, creates or refines prompts for LLMs\nto generate new solutions or improve existing ones, and then evaluates these solutions in either a\nreal system or a simulator. This iterative process continues until a desired solution is discovered, a\nresource budget is exhausted, or a researcher decides to stop it. ADRS consists of five components.\n•Prompt Generator: Creates the prompt used to generate the solution. This prompt con-\nsists of the problem statement and context provided by the researcher, which might include\nsystem or simulator code. The prompt may also include previous solutions and their eval-\nuations (provided by theSolution Selector) to further refine the prompt.\n•Solution Generator: Feeds the prompt fromPrompt Generatorto one or more LLMs to\ngenerate a new solution or refine an existing one. This is typically done by directly updating\nthe code in the simulator or the real system.\n•Evaluator: Takes the solution fromSolution Generatorand runs it against a predefined\nset of workloads (traces). It then scores the solution based on the run’s performance and\ncan use an LLM to provide qualitative feedback. If the score is high enough, the loop\nterminates.\n•Storage: Stores the solutions, their outputs, scores, and the feedback provided by theEval-\nuatorcomponent.\n•Solution Selector: Chooses a subset of solutions fromStoreand provides them toPrompt\nGeneratorto refine the prompt to generate new and improved solutions.\nTogether, these components form an internal automated feedback loop that enables ADRS to itera-\ntively refine solutions for a given problem. This can be paired with an outer feedback loop in which\na human can observe the generated solutions and provide high-level guidance that is incorporated\ninto future prompts.\nIn most cases, ADRS relies on a simulator rather than the real system for solution implementation\nand evaluation. There are two main reasons for this. First, the codebase of the real system is often too\nlarge to fit within the context window of current LLMs. Second, running evaluations in a simulator\ncan be orders of magnitude faster than on the real system, significantly accelerating the evolution\nprocess.\nAs mentioned earlier, ADRS implements theSolutionandEvaluationstages of the research process\ndepicted in Figure 1: theSolutionstage is implemented by thePrompt GeneratorandSolution\nGeneratorcomponents, while theEvaluationstage is implemented by theEvaluatorcomponent.\nAs discussed, theStoreandSolution Selectorare auxiliary components that support thePrompt\nGenerator. Regarding the other stages in the research process, ADRS does not have any impact: it\nmakes it neither harder nor easier for researchers to decide what problem to work on, building the\nevaluation framework, document their findings and write the paper. As a result, ADRS advances the\nPareto frontier of the research process.\n4.3 ADRS EXAMPLES\nADRS is not entirely a new concept. It aims to capture the architecture of recently developed systems\ndesigned for similar problems that allow for strong verification. Next, we provide several examples\nof such systems.\nAlphaEvolve and OpenEvolve.AlphaEvolve Novikov et al. is a system developed by Google\nDeepMind that uses artificial intelligence to automatically discover and design novel, high-\nperformance computer science algorithms. Similarly to ADRS, AlphaEvolve takes as input a user-\nprovided problem definition, an initial reference solution, and a programmatic evaluation function\nthat quantitatively scores any proposed solution. AlphaEvolve uses an evolution algorithm to facili-\ntate the inner feedback loop, and it does not include an outer loop to take in human feedback during\nthe evolution process (though such loop can be easily added). The evolution algorithm uses a com-\n6\n\nbination of the Multi-dimensional Archive of Phenotypic (MAP) elites algorithm Mouret & Clune\n(2015) and island-based population models Tanese (1989) that the database not only contains the\nbest-performing programs but also a wide variety of high-quality solutions with different attributes.\nOpenEvolve Sharma (2025) is an open-source implementation of the main concepts presented in\nAlphaEvolve. OpenEvolve provides a controller that orchestrates an asynchronous pipeline between\nan LLM ensemble for code generation, a pool of evaluators for scoring, and a program database\nfor storing and sampling candidate solutions. While it contains the core key features, it does not\nimplement all functionality described in AlphaEvolve (e.g., meta prompt evolution) and provides\nsome additional functionality (e.g., evolution tree visualization). Due to its availability as open\nsource and flexibility, this is the main framework we have used in our study so far.\nGEPA.GEPA (Genetic-Pareto) Agrawal et al. (2025) is another recent ADRS-like framework that\nuses a different evolution process and reflection with natural language feedback to improve prompts.\nThe evolution algorithm automates the exploration–exploitation tradeoff by iteratively mutating\nprompts with reflective updates, merging candidates, and filtering through Pareto frontiers to pre-\nserve diverse high-performing solutions.\nLLM4AD.LLM4AD Liu et al. (2024a) is a general-purpose platform that integrates search, LLM\ngeneration, and evaluation in a unifed framework. It provides modular components from problem\nspecification, solution generation, evaluation, and integrates a broad set of search strategies with\na standard interface, including evolutionary algorithms (e.g., NSGA-II, MOEA/D), neighborhood\nsearch, and random sampling. It also offers unified evaluation sandboxes, logging and profiling\ntools, and support for over 20 tasks. LLM4AD can be viewed as an end-to-end testbed that enables\nresearchers to benchmark ADRS pipelines across diverse domains.\nCoding Assistants.Coding assistants such as Cursor or Windsurf can also be seen as ADRS in-\nstances. These assistants have context over entire codebases, enabling them to perform complex\ncode modifications. These coding assistants can be instructed to explore the solution space to evolve\nalgorithms, use tools to verify them, and can orchestrate the entire feedback loop via natural lan-\nguage prompts. In addition, these tools facilitate a human-driven outer feedback loop by providing\nan easy way for the researcher to interactively provide guidance in the search.\n5 EVALUATION ANDCASESTUDIES\nTo evaluate the ADRS approach, we investigate 11 system tasks across several sub-domains, in-\ncluding networking, databases, and core systems. We summarize our findings in Table 1. Each\ncase study follows a common schema that captures the problem setup, environment, evolutionary\nprocess, model usage, and final outcome. We show this schema in Table 2.\nNext, we present four representative case studies to highlight insights that we hope will be useful to\nother researchers. These insights cover both the limitations of current frameworks and best practices\nfor using them, which are detailed further in Section 6. We used OpenEvolve as the primary ADRS\nfor our case studies.\nThe four case studies cover distributed systems, databases, and LLM systems.\n•Optimizing Spot Instance Savings under Deadlines:Given a job with a deadline, the solu-\ntion aims to maximize the use of cheaper spot instances in a public cloud without violat-\ning the deadline. ADRS improves the SOTA result by up to 16% for a single region and\nachieves 48% improvements over a strong baseline in a multiple-region setting.\n•Optimizing Expert Placement in MoE Inference:The solution seeks to balance the load\nacross GPUs by mapping the expert replicas across GPUs. ADRS provides a fivefold im-\nprovement in the time it takes to rebalance experts compared with the best-known propri-\netary implementation.\n•Optimizing LLM Inference for SQL Queries:The solution to this problem reorders rows\nand columns in a table to maximize the hit rate in a KV cache when performing LLM\n1Reported “time” reflects the automated solution-iteration phase (model–evaluator loop). It excludes re-\nsearcher effort related to problem specification (e.g., refining prompts, evaluators, or experimental setup), which\ntypically ranges from a few hours to a few days—still orders of magnitude less than the time required to man-\nually develop a solution. Because this effort is difficult to measure consistently, we do not report it here.\n7\n\nTask & SOTA Publication Objective Result vs. SOTA / Baseline Time / Cost\nTelemetry Repair\nKrentsel et al. (2024)\n[HotNets ‘24]Repair buggy network telemetry\ncounters and calibration.+9% better counter repair score, +30% higher\nconfidence calibration score than published solu-\ntion.8h (300 iters),\n≤$10\nAdaptive Weight\nCompression\n[In-Progress Work]Assign bitrate per column to min-\nimize bits/elem while preserving\naccuracy.Similar bits/elem, 14.2% worse PPL. 12h (200 iters),\n≤$20\nCloudcast\nWooders et al. (2024)\n[NSDI ‘24]Optimize multi-cloud data trans-\nfer cost.Matches SOTA cost. 1h (100 iters),\n≤$5\nExpert Parallelism\nLoad Balancer\n[In-Progress Work]Balance expert-parallel load\nacross GPUs.Same balanceness, 2×faster runtime vs. internal\nimplementation.5h (300 iters),\n≤$10\nGlobal Model Placement\nYu et al. (2025)\n[arXiv]Optimize cost for model-to-GPU\nplacement.18.5% cheaper than published solution. 40m (70 iters),\n≤$5\nLLM-SQL\nLiu et al. (2024b)\n[MLSys ‘25]Reorder tabular data to improve\nprefix hit rate.Comparable hit rate, 3.9×faster runtime. 1h (100 iters),\n≤$7\nTransaction Scheduling\nCheng et al. (2024)\n[VLDB ‘24]Minimize makespan in transac-\ntion scheduling.20% better than greedy (offline).<2h (100 iters),\n≤$20\nCan’t Be Late\nWu et al. (2024)\n[NSDI ‘24]Schedule deadline-driven jobs on\nsingle-region spot instances.Up to 16% (average 7%) higher cost savings vs.\nSOTA.5h (400 iters),\n≤$20\nCan’t Be Late\nMulti-Region Extension\n[In-Progress Work]Schedule deadline-driven jobs on\nmulti-region spot instances.26% lower cost vs. single-region baseline. 1h (100 iters),\n≤$5\nSparse Attention Design\nDesai et al. (2025)\n[NeurIPS ‘25]Balance attention sparsity and ac-\ncuracy.7% average error and density improvement vs.\nSOTA.4h (100 iterations),\n≤$15\nMulti-Agent System\nOptimization\n[ICLR ‘24]Improve multi-agent collabora-\ntion using MAST taxonomy and\nrubric-based feedback.7% improvement on ProgramDev.<2h (100 iters),\n≤$15\nTable 1: Summary of project task objectives and it’s corresponding SOTA publication, performance\nimprovements relative to SOTA and baseline solutions, and overall time/cost efficiency. Most tasks\nachieve near-SOTA performance within hours at modest cost, demonstrating the practicality of the\nADRS approach.1\ninference. ADRS achieves a similar hit rate to SOTA, while reducing the running time of\nthe reordering algorithm by 3×.\n•Optimizing Transaction Scheduling:The solution aims to reorder transactions to minimize\nconflicts and hence improve the makespan and throughput. ADRS “rediscovers” the SOTA\nsolution for the online case and improves a strong baseline by 34% for the offline case, for\nwhich we are not aware of any published solution.\nBefore proceeding, we make one important point. The case studies we present here were carried out\nby different students in parallel during the summer of 2025. As a result, they use (sometimes widely)\ndifferent configuration parameters. This makes direct comparison difficult and highlights the need\nfor systematic ablation studies to identify which configurations work best for different problems—an\nimportant direction for future work (see Section 7.2.2). Consequently, the case studies presented\nhere should be viewed as a lower bound on the capabilities of the existing ADRS framework. As\nwe gain deeper understanding of how to use these frameworks effectively and as the frameworks\nthemselves evolve, we expect to see even more impressive results.\n5.1 CASESTUDY# 1: OPTIMIZINGSPOTINSTANCESAVINGS UNDERDEADLINES\nOur first case study focus on reducing the cost of deadline-driven jobs by exploiting cheaper but\nunreliable spot instances in the cloud. Spot instances are typically 60% to 90% cheaper than on-\n8\n\nDimension Key Components\nPrompt Generator (Prob-\nlem Setup)Problem description: problem domains – computer systems (networking,\ndistributed systems, databases, MLSys, etc.); problem description – perfor-\nmance optimization, e.g., find the most cost-effective transfer graph\nOptimization objective: e.g., latency, throughput, cost, algorithm runtime\nConstraints: e.g., latency SLOs\nSolution GeneratorLLM type: what model used for solution generation, this includes reasoning\nvs. non-reasoning, tool-use vs. non-tool-use, LLM ensemble\nNumber of iterations: number of rounds to iterate the solution\nEvaluatorEnvironment and test data: testbed environment such as CPU simulator,\ndatabase, GPU environment; test data and traces\nInitial Program: the initial program to start with, use public source\n(GitHub/paper) if available, otherwise use simple algorithm\nAdditional Baselines: additional baselines for comparison if any\nEvaluator Feedback: execution score, more advanced if any (e.g., error\nmessages, human- or agent-in-the-loop feedback)\nSolution SelectorSelection algorithm: e.g., greedy, random, or island algorithm\nHow We Analyze ResultPerformance: compare evolved result vs. initial program and SOTA algo-\nrithm\nCost-benefit: compute budget, LLM calls, simulation time (survey: was the\nquality gain worth the cost?)\nOther metrics: robustness, generalization\nHow We Analyze Evolu-\ntion ProcessSearch trajectory: from initial program to checkpoints to final outputs (ex-\namples of key transitions, what features are added)\nCommon patterns: where models get stuck, recurring failures\nFeedback granularity and utility: scores, constraint violations, trace-level\nlogs; which kinds of feedback resolve which failure patterns\nTable 2: Expanded schema for problem formulation and evaluation of AI-driven algorithm discov-\nery. Each row corresponds to an element in the setup.\ndemand instances, but they are not always available and can be preempted at any time. Each pre-\nemption incurs a changeover delay, representing the setup time on a new instance. The challenge is\nto minimize the cost by using spot instances as much as possible without violating job deadlines.\nWe use OpenEvolve to evolve algorithms for this setting. It discovers an algorithm that improves the\ncost savings of the best published solution by an average of 7%, with per-workload improvements of\nup to 16.7%. We also use OpenEvolve to develop an algorithm on an expanded multi-region setting,\nwhere no prior policy has been published. In this setting, OpenEvolve discovers an algorithm that\noutperforms a hand-tuned baseline by 26%.\n5.1.1 SINGLEREGION: CAN’TBELATE[NSDI ‘24]\nFirst, we discuss optimizing cost savings for a single region. This problem was explored in aNSDI\n‘24outstanding paper Wu et al. (2024), which introduced the current state-of-the-art policy.\nProblem setup.The task is to minimize the cost of running a deadline-aware job on a single node\nby using spot instances in one cloud region, while ensuring the job still meets its deadline.\nObjective and constraints.We evaluate the average cost savings across a range of workload traces.\nWe require that all deadlines be met for an algorithm to be considered valid.\n9\n\nFigure 4: Side-by-side comparison of the initial Uniform Progress policy and the evolved adaptive\nstrategy. Key innovations in the evolved policy are highlighted.\n1defUniformProgress(has_spot, state, env, task):\n2# Calculate uniform progress rate\n3progress_rate = task.duration / task.deadline\n4expected = env.elapsed_time *progress_rate\n5actual = task.progress_made\n6\n7# Simple decision: behind schedule?\n8ifactual < expected:\n9# Must use any available resource\n10ifhas_spot:\n11returnClusterType.SPOT\n12else:\n13returnClusterType.ON_DEMAND\n14\n15# Hysteresis for on-demand\n16ifstate == ClusterType.ON_DEMAND:\n17buffer = expected + 2 *task.overhead\n18ifactual < buffer:\n19returnClusterType.ON_DEMAND\n20\n21# Default: use spot if available\n22ifhas_spot:\n23returnClusterType.SPOT\n24else:\n25returnClusterType.NONE# Wait\n(a) Before: Uniform Progress Policy1defAdaptiveStrategy(has_spot, state, env, task):\n2# Track recent spot availability patterns\n3 self.window.append(has_spot)\n4 alpha = window_avg(self.window)\n5 streak = longest_run(self.window)\n6 tail = trailing_run(self.window)\n7\n8 params = get_params(classify_situation(alpha,\nstreak, tail)),→\n9\n10# Compare remaining time to safety margin\n11need = ticks_needed(task, env)\n12slack = ticks_remaining(env)\n13safety = safety_margin(task, params)\n14\n15ifneed >= slack:\n16lock_on_demand()\n17returnClusterType.ON_DEMAND\n18\n19ifis_locked():\n20ifcan_unlock(has_spot, tail, params):\n21unlock()\n22else:\n23returnClusterType.ON_DEMAND\n24 elifslack <= need + params.lock_margin:\n25 lock_on_demand()\n26 returnClusterType.ON_DEMAND\n27\n28 ifrebuilding_buffer()and not\nbuffer_recovered(slack, safety, params):,→\n29 returnClusterType.ON_DEMAND\n30\n31 ifsafety >= slack:\n32 returnClusterType.SPOTifhas_spotelse\nClusterType.ON_DEMAND,→\n33\n34ifstate == ClusterType.ON_DEMAND:\n35ifsafe_to_switch(has_spot, streak, tail,\nparams):,→\n36returnClusterType.SPOT\n37returnClusterType.ON_DEMAND\n38\n39 ifhas_spotandlooks_stable(streak, tail,\nparams):,→\n40 returnClusterType.SPOT\n41elifslack > safety + params.wait_margin:\n42returnClusterType.NONE# Wait for\nstable spot,→\n43else:\n44start_rebuilding(params.dwell)\n45returnClusterType.ON_DEMAND\n(b) After: Evolved Adaptive Policy\nSolution generator and selector.We run OpenEvolve’s default island evolution with 4 islands for\n400 iterations, which takes 5 hours total and costs less than $20. We use a two-model ensemble:\n20% GPT-5 for reasoning-driven exploration and 80% Gemini 2.5 Pro to enhance diversity.\nEvaluator.We use the simulator from the published paper and use configurations covering different\njob fractions, changeover delays, regions, and accelerator types. For each configuration, we sample\n30% of the traces as a feedback subset used during OpenEvolve search to reduce overfitting to\nspecific traces. We report final results on the full evaluation set.\nInitial program and baselines.The initial program is thegreedy policyfrom Wu et al. (2024),\nwhich uses spot instances until preemption risks missing a deadline. We compare against theUni-\nform Progressalgorithm, the paper’s state-of-the-art solution, which tracks expected progress and\nswitches between spot and on-demand instances based on whether it is ahead or behind schedule.\nEvaluator Feedback.The evaluator checks syntax and interface compliance and tests valid solutions\non sampled traces. It reports average cost savings over the Uniform Progress baseline and per-\nconfiguration statistics (mean, deviation, count). Trace features such as availability and average spot\nduration are also included to provide richer context.\nOpenEvolve results.The best policy is developed in iteration 389, achieving 7% higher average\ncost savings than Uniform Progress (and 23.8% over the greedy policy) while meeting all deadlines.\nPer-trace improvements reach up to 16.7% compared to Uniform Progress.\n10\n\nAs shown in Figure 4b, the evolved policy fundamentally differs from Uniform Progress shown in\nFigure 4a. While Uniform Progress follows a fixed formula to maintain steady progress, the evolved\npolicy adaptively learns from recent spot availability patterns based on a sliding window (lines 3).\nIt classifies situations as stable, moderate, or unstable and adjusts action accordingly (lines 4-8).\nWhen spots are stable, it risks more to save cost; when unstable, it becomes conservative. The\npolicy compares remaining slack time to safety margins that expand or shrink based on recent spot\nstability (lines 24-26). When slack becomes tight or spots look unreliable, it temporarily switches\nto on-demand instances to rebuild the slack buffer, preventing last-minute scrambles (lines 28-29).\nWhen slack recovers and patterns look favorable again, it returns to using spots. The safety margins\nscale dynamically with the cost of recent preemption (lines 31-32), creating larger buffers when\nrestart costs are high.\nThe main limitation of Uniform Progress is its inflexibility when behind schedule: it must use every\navailable spot instance, regardless of how short-lived it might be. This leads to a “changeover trap,”\nwhere frequent, brief spot allocations cause repeated switches with little real progress. The evolved\npolicy avoids this by making spot use selective (lines 39–40): it waits when spots appear unstable and\nsufficient slack remains, rather than wasting effort on likely short-lived opportunities. This selective\nwaiting lets it skip unreliable spots while still exploiting stable ones when conditions improve.\nEvolution process.The search runs for 400 iterations, gradually refining when to use spot instances.\nIn early iterations (1–90), the policy learns to track recent spot availability using a sliding window\nand recognize when spots are plentiful versus scarce. By iteration 180, it introduces adaptive safety\nmargins adjustment based on spot stability. Around iteration 240, it adds finer-grained situation de-\ntection, using hysteretic transitions between stable and unstable regimes to avoid frequent switching.\nBy iteration 350, it learns that safety margins should adapt dynamically through chance-constrained\nrisk lines, expanding when recent preemption are frequent and shrinking when spots have been sta-\nble. The final strategy (iteration 389) integrates all these mechanisms with lean parameter tuning\nand introduces selective waiting: skipping unreliable spots and waiting for stable ones.\n5.1.2 MULTI-REGIONCAN’TBELATE\nProblem setup.The original Uniform Progress assumes one region with uniform spot prices. In\npractice, spot prices and availability differ across regions (Mao et al. (2025)), so a policy must\ndecide when to switch spot and on-demand, which region to use, and when to migrate jobs. We use\nOpenEvolve to explore this multi-region space and derive an better policy.\nObjective and constraints.We evaluate the total cost in a multi-region setup, accounting for spot\nand on-demand instances and migration costs. A policy is valid only if all job deadlines are met.\nEvaluator and other config.For the multi-region setting, we extend the Uniform Progress simulator\nand evaluate cost savings on 106 traces. As no baseline exists, we adopt a Uniform Progress variant\nthat first assigns spot instances locally and, if none are available, move to other regions in a round-\nrobin manner. We run OpenEvolve for 100 iterations with Gemini 2.5 Pro, using the same island\nsetup as before. The evaluator then reports both overall and per-trace scores, similar to Section 5.1.1.\nOpenEvolve results.The final policy achieves 26% cost savings, on average, compare to the multi-\nregion Uniform Progress baseline. It balances cost efficiency with deadline guarantees using a sim-\nple principle: when a job is not urgent (i.e., not at risk of missing its deadline), it explores additional\nregions to seek lower-cost spot capacity; if a job is urgent, it prioritizes immediate progress, select-\ning spot instances when available or falling back to on-demand. This adaptive logic enables op-\nportunistic exploration under slack conditions while ensuring reliability when deadlines are at risk,\neffectively managing the trade-off between exploration and guaranteed progress in a multi-region\nenvironment. In addition, the policy leverages a dynamic view of regional capacity to opportunisti-\ncally migrate when conditions are favorable.\nEvolution process.The search process demonstrates iterative improvement of the deadline monitor-\ning mechanisms from multi-region scheduling policies. Initial strategies implement basic progress\ntracking by comparing task completion against elapsed time. The system discovers key insights\nthrough failure analysis. At iteration 7, the system introduces region caching and urgency calcula-\ntion. Iteration 5-12 attempts with aggressive cost reduction initially show promise, but ultimately\nfail when accumulated delays cannot be recovered within deadline constraints. These failures guide\nthe search toward more balanced approaches. The final evolved strategy at iteration 63 implements\n11\n\na two-stage urgency detection system. Rather than applying uniform resource allocation rules, it\ncombines schedule-based progress monitoring with direct deadline pressure analysis. This design\nenables adaptive behavior: immediate allocation of on-demand instances when deadlines are at risk,\nwhile maintaining intelligent region exploration when deadline permits. The insight is the separation\nof deadline assessment from resource provisioning decisions, enabling adaptive region selection.\n5.2 CASESTUDY# 2: OPTIMIZINGEXPERTPLACEMENT INMOE INFERENCE\nIn this section, we discuss the problem of designing efficient algorithms to balance computa-\ntional load during inference across multiple GPUs in a Mixture-of-Experts (MoE) architecture.\nOpenEvolve discovers an algorithm implementation that runs 5.0×faster than the state-of-the-art\nfrontier-lab reference implementation while achieving similar load balancing.\nProblem setup.The basic Expert Parallelism Load Balancing (EPLB) algorithm runs in three\nstages: (i) distribute expert groups across nodes to balance the load, (ii) create replicas for hot\n(popular) experts, and (iii) assign these replicas to GPUs to further minimize the imbalance. The\nproblem is then: given a query workload, an MoE model and a set of GPUs, determine the number of\nreplicas for each experts and then map these replicas on GPUs such that to minimize the imbalance.\nObjective and constraints.Our optimization goal is twofold: to minimize load imbalance (i.e., the\nratio of average to maximum tokens generated per GPU) and to reduce the running time of the\nalgorithm used to re-balance experts when the load changes.\nSolution generator and selector.We run OpenEvolve with five islands, and we use a combination\nof 80% Gemini 2.5 Flash and 20% Gemini 2.5 Flash Lite. We cap the evolution at 300 iterations.\nThe total evolution takes roughly five hours and costs less than $10.\nEvaluator.Our simulator models a distributed GPU inference engine for MoE models. The simu-\nlator is implemented in PyTorch and consists of 168 lines of code. Our evaluation trace models the\nload changes over the ShareGPT and GSM8K datasets .\nInitial program and baselines.The initial program as the open-source EPLB implementation from\nDeepSeek (2024). This solution performs expert placement using a greedy bin-packing, i.e., it sorts\nexperts by their load in descending order and assigns each to the least-loaded GPU that has capacity\nleft. However, despite its simplicity, this solution is slow as it is written an Python and uses a for-\nloop to performs linear search for finding the best-fit GPU choice. On average, it takes about 540\nms to re-balance the experts and achieves an imbalance factor of 0.66.\nAs a baseline, we include a non-public reference implementation from a frontier lab to which we\nhad access. It introduces a clever heuristic to replace the loop: instead of explicit bin packing, it\nreshapes and transposes tensors representing expert indices, using PyTorch’s fast tensor operations to\neffectively stagger expert assignments via a zigzag (or “snake”) pattern between heavily and lightly\nloaded GPUs. The main idea is to alternate expert placement in a way that naturally interleaves high-\nload and low-load experts across GPU slots. This heuristic avoids explicit iteration and reduces the\nrebalancing algorithm runtime from 540 ms to 19.6 ms while achieving the same imbalance factor.\nEvaluator feedback.The evaluator’s ouput metrics are: (a) the imbalance factor and (b) the time\nit takes to rearrange the expert replicas when the load changes over our test datasets. Since\nOpenEvolve require us to provide a single metric, we compute this metric as the equally weighted\naverage of the load imbalance factor and the rebalance algorithm runtime.\nOpenEvolve results.The new algorithm generated by OpenEvolve independently discovers the\nstaggered placement technique, learning to use tensor reshaping and reversal to evenly distribute ex-\npert load. Notably, our baseline implementation is not publicly available, making it highly unlikely\nthat the Gemini models used in our experiments were exposed to this implementation during train-\ning. Impressively, OpenEvolve also introduces subtle enhancements, including improved ordering\nlogic and more adaptive reshaping strategies. The resulting algorithm matches the imbalance factors\nof the baselines while reducing runtime to just 3.7 ms, yielding a 5.0×speedup over the internal\nreference implementation.\nEvolution process.OpenEvolve’s evolution trajectory can be characterized by two major steps\nin improving runtime: first, replacing Python for-loops with PyTorch tensor operations, and sec-\nond, discovering the zigzag placement pattern (Figure 5b). Interestingly, the initial introduction of\n12\n\nFigure 5: Side-by-side comparison of the initial greedy policy and final evolved heuristic. Key\ninnovations in the evolved policy are highlighted.\n1defInitialStrategy(...):\n2...\n3foriteminsorted(items, reverse=True):\n4# Greedily choose least-loaded pack\n5# Note that these are plain for-loops\n6available_packs = filter_nonfull(packs)\n7min_loaded_pack = min(available_packs)\n8min_loaded_pack.add(item)\n9...\n(a) Before: Initial Program1defEvolvedStrategy(...):\n2...\n3indices = torch.arange(num_items)\n4block_id = indices // len(num_packs)\n5idx_in_block = indices % len(num_packs)\n6is_even_block = block_id % 2 == 0\n7\n8# Heuristics here: no loop at all!\n9# The \"snake\" pattern:\n10# For items in even blocks,\n11# assign them to each pack while\n12# for odd blocks, in a reversed order\n13 packs_for_sorted_items = torch.where(\n14 is_even_block,\n15 idx_in_block,\n16 num_packs - 1 - idx_in_block\n17 )\n18\n19# Now we have assigned a pack for each item\n20update_packs(packs_for_sorted_items)\n21...\n(b) After: Evolved Policy\nthe zigzag pattern did not yield immediate gains—the imbalance factor sometimes worsened, and\nrearrangement costs fluctuated. The breakthrough came later, when OpenEvolve learned to system-\natically reuse the zigzag partitioning heuristic across multiple stages of EPLB, rather than only in the\ninitial group distribution. At this point, performance improved substantially, reducing rearrangement\ntime by orders of magnitude.\nThe expert replication stage, by contrast, remained the most unstable throughout evolution. The sys-\ntem oscillated between strategies such as copying the least-used experts, overloading popular ones,\nor attempting proportional spreads. These experiments rarely improved the score, and ultimately\nthe intuitive rule of replicating only overloaded experts prevailed. Consequently, many iterations\nwere unproductive, with the main speed improvements coming from global reorganization logic that\nexploited PyTorch’s batched operations and the zigzag layout.\nIn summary, OpenEvolve independently rediscovered and fully exploited a tensorized zigzag parti-\ntioning scheme, yielding an evolved EPLB algorithm that achieves a 5.0×speedup without worsen-\ning the imbalance factor.\n5.3 CASESTUDY#3: OPTIMIZINGLLM INFERENCE ONSQL QUERIES[MLSYS‘25]\nThis research problem Liu et al. (2024b) arises in relational analytics, where SQL queries invoke\nLLMs over entire tables, with each row triggering a separate LLM inference operation. At scale,\nthis is prohibitively expensive. The state-of-the-art solution mitigates cost by reordering rows and\nfields to maximize prefix KV cache reuse. Using OpenEvolve, we evolve such a reordering policy,\nachieving similar hit rates while delivering a 3×runtime speedup.\nProblem setup.To minimize inference time and cost, we aim to maximize the prefix cache hit rate\n(PHR) by reordering both rows and fields in the table before performing inference. This problem is\ncombinatorial: for a table withnrows andmfields, there aren!×(m!n)possible orderings, making\na naive brute-force search infeasible. Thus, the goal is to design a reordering algorithm that achieves\nhigh PHR while keeping its runtime small relative to the overall inference time.\nObjective and constraints.Our objective is to maximize the prefix hit rate (PHR) while keeping the\nruntime of the reordering algorithm low. Since PHR measures the fraction of token prefixes shared\nacross consecutive rows, and serves as a proxy for inference cost and latency.\nSolution generator and selector.We configure OpenEvolve with three islands, and we use an LLM\nensemble of 80% OpenAI o3 and 20% Gemini 2.5 Pro, and we run it for 100 iterations. The entire\nevolution takes about one hour and costs less than $7.\nEvaluator.We leverage the publicly available simulator from the paper that measures prefix cache\nhit rate (PHR) given dataset table. The simulator is written in Python (200 LOC) and evaluates a\n13\n\nFigure 6: Side-by-side comparison of the greedy recursive grouping (QuickGreedy) and the evolved\nprefix-aware reordering policy. Key innovations in the evolved algorithm are highlighted.\n1defQuickGreedy(df):\n2# 1. Compute value counts for all cells\n3counts = Counter(df.stack())\n4val_len = {v: len(str(v)) **2forvincounts}\n5\n6# 2. Pick value maximizing lenˆ2 *(count-1)\n7v_star = argmax_v [ val_len[v] *(counts[v]-1) ]\n8ifv_staris None:returnfixed_reorder(df)\n9\n10# 3. Split rows with/without v_star\n11G = rowswithv_star\n12R = rows without v_star\n13\n14# 4. Reorder columns in G (v_star front, deps\nafter),→\n15forrowinG:\n16cols_with_val = [cforcindf.columnsif\nrow[c]==v_star],→\n17reordered = cols_with_val + (others)\n18row = row[reordered]\n19\n20# 5. Recurse on G remainder and R\n21G = QuickGreedy(G remainder)\n22R = QuickGreedy(R)\n23returnconcat(G,R)\n(a) QuickGreedy baseline.1defEvolvedPolicy(df):\n2# 1. Cache value lengths once\n3counts = Counter(df.stack())\n4val_len = {v: len(str(v)) **2forvincounts}\n5\n6 # 2. Larger base cutoff→fewer recursions\n7 BASE = 4000\n8\n9# 3. Find max group value using cached counts\n10v_star = argmax_v [ val_len[v] *(counts[v]-1) ]\n11ifv_staris None:returnfixed_reorder(df)\n12\n13 # 4. Recurse only if |df| > BASE\n14 iflen(df) > BASE:\n15 top, bottom = split(df)\n16returnconcat(EvolvedPolicy(top),\n17EvolvedPolicy(bottom))\n18\n19# 5. Lightweight per-row heuristic\n20# Columns equal to previous row first,\n21# sorted by squared length, then others\n22prev =None\n23 forrowindf:\n24 ifprevis None: order = df.columns\n25 else:\n26 matches = [cforcindf.columnsif\nrow[c]==prev[c]],→\n27 matches.sort(key=lambdac:\nval_len[row[c]],,→\n28 reverse=True)\n29 order = matches + [cforcindf.columns\nifcnot inmatches],→\n30 prev = row\n31 row = row[order]\n32\n33returndf\n(b) Evolved prefix-aware policy.\nbenchmark of representative LLM queries across five recommendation datasets (e.g., movies Pang\n& Lee (2005), beer McAuley et al. (2012), BIRD Li et al. (2024), PDMX Long et al. (2024), prod-\nucts He & McAuley (2016)).\nInitial program and baselines.As initial program, we use the greedy recursive group algorithm\n(GGR) Liu et al. (2024b), a heuristic that recursively groups rows by common field values and re-\norders fields using schema statistics with early stopping. This approach approximates the optimal\nreordering algorithm while running more efficiently. For comparison, we also include a simple base-\nline: the table in its original ordering, i.e., the default row/field order of the input table. This program\nis implemented in Pandas McKinney (2010), an open-source Python library for data manipulation\nand analysis.\nEvaluator feedback.The evaluator reports four key metrics. First, it reports a combine score= 0.5×\nPHR+0.5×1\n1+runtime, defined as the equally weighted average of PHR and algorithm runtime across\ndatasets, where higher PHR and lower runtime yield a higher score. To preserve query semantics,\nthe reordering algorithm must not alter which rows or fields are included, only their order. Second,\nthe evaluator records a binary flag indicating whether the candidate program executes end-to-end.\nThird, it reports the detailed prefix hit rate for each dataset. Finally, it measures the total runtime of\nthe policy. The combined score serves as the optimization objective during evolution.\nOpenEvolve results.The new program generated by OpenEvolve achieves a similar average PHR\ncompared to the GGR algorithm, while reducing runtime by 3×, thereby yielding a higher combined\nscore. In contrast, the greedy baseline spends most of its time recomputing value counts and recur-\nsively traversing the table, which results in high overhead. The evolved solution introduces several\noptimizations. First, instead of recomputing full value counts at each recursion, it maintains a lazily\nupdated global frequency map, eliminating redundant data traversals. Second, it replaces slow Pan-\ndas lookups with a direct attribute-mapping method, reducing the core loop from costly Pandas calls\nto straightforwardO(N rows×N cols)Python operations. Finally, it applies a local heuristic for per-\nrow ordering: instead of globally sorting the entire table, it reorders fields by maximizing continuity\nwith the preceding row while weighting by squared value length. These features are highlighted and\ndescribed in Figure 6.\n14\n\nEvolution process.The search begins with the published GGR algorithm, which achieves a good\nPrefix Hit Rate (PHR) but suffers from repeated counter operations and deep recursion. Early in the\nsearch, by iteration 32, OpenEvolve discovers a faster heuristic that orders columns by (frequency×\nsquared length) instead of using recursive splitting. This change eliminates redundant value counting\nand improves runtime, though at the cost of some grouping accuracy.\nLater, by iteration 72, the heuristic is refined. It now uses normalized weights (frequency ratio×\nsquared length) and limits multi-key sorting to only the most informative columns, which improves\nhit rates while maintaining efficiency.\nBy iteration 97, the final program strikes an optimal balance between speed and accuracy. It raises\nthe recursion base threshold, reuses cached counts and string lengths, reintroduces selective recur-\nsion for rows, and incorporates a NumPy-based reordering to replace costly pandas lookups.\nOverall, the evolution progresses from early runtime improvements to mid-stage refinements that\nrecover accuracy, culminating in a final design that integrates both for the best overall score.\n5.4 CASESTUDY#4: OPTIMIZINGTRANSACTIONSCHEDULING[VLDB ‘24]\nThis research problem Cheng et al. (2024) aims to find efficient schedules to reduce conflicts for\ntransactional workloads. Transaction processing systems can significantly improve throughput by\ndetermining and executing transactions in an order that minimizes overall execution time. We apply\nOpenEvolve to this problem and find that the framework is unable to find a more successful algo-\nrithm than the existing state-of-the-art policy when constrained to the online setting as in the original\nproblem. However, OpenEvolve is able to find an algorithm that provides better schedules without\nonline constraints, demonstrating that it can be useful for rapidly exploring different variations of\nproblems.\nProblem setup.Conflicts on shared data cause performance bottlenecks in many transactional\nworkloads Cheng et al. (2022). One approach to minimize conflicts is to carefully schedule the\ntransactions. The problem we aim to solve is: given a set of transactions, find a schedule that\nminimizes the conflicts and improves the throughput.\nSolution generator and selector.We configure OpenEvolve to run with three islands and use a\ntwo-model ensemble of 80% Gemini 2.5 Pro and 20% OpenAI o3 for both problem settings. We\nrun for 100 iterations, which takes less than two hours and costs less than $20.\nObjective and constraints.Maximizing throughput in this setting is equivalent to minimizing the\nschedulemakespan, i.e., the total time to execute all transactions. We consider both the online and\nthe offline settings for this problem. In the online setting, we assume that the transaction order is\nfixed once the schedule is determined (i.e., committed transactions cannot be rollbacked). We con-\nstrain the scheduling algorithm to O(n) runtime (wherenis the number of transactions to be sched-\nuled) and assume the read/write operations are not known apriori (only hot keys can be predicted).\nWe also consider the offline scheduling problem, which is relevant to deterministic databases that\nschedule batches of transactions, a setting with no previously known results.\nInitial program and baselines.We use the state-of-the-art algorithm, Shortest Makespan First\n(SMF), which greedily chooses transactions to schedule, as our initial program. We also compare\nagainst a number of transactional scheduling algorithms as well as a simple baseline that picks the\ntransactions at random.\nEvaluator.We use the Python simulator from the SMF paper Cheng et al. (2024), which assumes that\neach operation takes one unit of time. The simulator calculates the makespan of a given transaction\nschedule and also provides statistical bounds on the makespan of the schedule for a given workload.\nWe measure total makespan over five traces from the OLTP benchmarks used in the original paper\n(Epinions, SmallBank, TPC-C, TAOBench, YCSB) with 500 transactions each. We use the random-\nscheme baseline as the initial program.\nOpenEvolve results.In the online setting, the best discovered policy is SMF. We note that\nOpenEvolve is able to rediscover this algorithm from a random scheduling baseline. It is likely that\nthis is a case of contamination, i.e., the model was trained upon the SMF paper. While this result is\nnot as interesting, it shows these frameworks can reproduce state-of-the-art solutions. Furthermore,\nit is not unexpected that OpenEvolve is unable to find a better solution in this case: transaction\n15\n\nscheduling is a complex optimization problem, where we need to consider groups of operations,\ndependencies across operations, and correctness constraints (e.g., serializability). Prior work found\nthat more involved heuristics, such as hill climbing, simulated annealing, etc., did not perform better\ndue to the complexity of the problem constraints Cheng et al. (2024). Instead, leveraging the cost of\nconflicts, as SMF does, provides a generalizable solution that works across different workloads.\nAt a high level, the intuition behind SMF is to minimize the cost of conflicts as the schedule is\nconstructed by placing transactions with high conflict costs far apart. The incrementalmakespan\nincrease when a given transaction is added to the schedule accounts for the cost of all potential\nconflicts that an unscheduled transaction has with the current ordering. Concretely, SMF starts the\nschedule with a random transaction and at each iteration (Figure 7a, lines 2-3), finds the transaction\nthat increases makespan the least amongkrandomly sampled unscheduled requests (lines 5-25) and\nappend this transaction to the schedule (lines 28-35). For tie-breaking, a transaction is randomly\nchosen. It has a linear runtime ofO(n×k)run time, wherenis the number of transactions to be\nscheduled andkis a constant representing the sample size.\nIn the offline setting, OpenEvolve discovers a novel algorithm than reduces makespan by 34% com-\npared to SMF. This result reduces concerns about contamination since it is not a previously known\nsolution. The final algorithm involves three parts. First, it computes simple features per transaction\n(number of writes, length) and builds a strong initial sequence by sorting transactions lexicograph-\nically by (fewest writes, then shortest), which tends to reduce conflicts (Figure 7b, lines 6-13).\nSecond, it then runs the full greedy algorithm in which each transaction is tried in every position\n(lines 16–33) It then tries to optimize for any local minima due by performing a pair-swap hill climb\nfor a fixed number of iterations. Finally, it tries a few random schedules as a safety net (lines 49–55).\nThis algorithm extends the greedy intuition of SMF and hasO(n2)runtime. This result indicates\nthat scheduling based on cost of conflicts is the right approach to reduce makespan. Furthermore,\nOpenEvolve is able to quickly develop a variation of the algorithm for altered problem constraints\n(e.g., no runtime or reordering constraints), showing it can assist researchers in developing solutions\nfor different problem settings (that may otherwise require manual algorithm re-design).\nEvolution ProcessFor the online setting, OpenEvolve shifts from random schedules and length-\nonly heuristics toward increasingly conflict-aware solutions. As the framework recognized that con-\ntention leads to higher makespan, it explores different heuristics, such as write-count bucketing and\ngreedily constructing schedules to minimize the contentions. Some common pitfalls in the search\nprocess were over-reliance on transaction length (length correlates poorly with contention) and full\ngreedy selection that violated the O(n) constraint. The search often showed early over-confidence\nin single heuristics (length-first, then write-first), converging quickly but missing gains from other\ntechniques. The best program generalized the evolution’s heuristic insights (account for the cost of\nconflicts) into the state-of-the-art policy.\nFor the offline setting, the evolution also shifts from random sampling to simple heuristics (e.g.,\nshortest-first, fewest-writes). The search eventually centered in two directions: (i) greedily ap-\npending transactions to minimize makespan at each iteration and (ii) metaheuristics that broadened\nexploration using simulated annealing (SA) and neighborhood swaps. The search often got stuck\nwhen swap-only hill climbs plateaued quickly in local minima when starting from random sched-\nules. The framework later discovered that pairing this technique with greedy construction led to\nbetter schedules. In particular, the reasoning models demonstrated a better understanding of how to\nescape local minima by using problem-relevant signals (e.g., key frequency) and considering larger\nneighborhoods (e.g., insert into every possible position). The best solution combined the successful\ntechniques that emerged during evolution: a cheap yet informative heuristic start (sorting transac-\ntions based on writes/length), a powerful greedy construction method, and a lightweight swap hill\nclimb plus a few random restarts for diversification.\n6 EARLYBESTPRACTICES\nAlthough existing ADRS frameworks have demonstrated potential in our case studies, these systems\nare still in their infancy. They exhibit several significant limitations, such as the size and complexity\nof the code they can analyze, and are prone to failures. Table 3 provides a taxonomy of the most\ncommon failures we have encountered, which fall into three main categories:\n16\n\nFigure 7: Side-by-side comparison of the evolved constant-time greedy policy (SMF) and the offline\nevolved policy. Key differences in the offline policy are highlighted.\n1# --- 1. Candidate sampling ---------- #\n2SAMPLE_SIZE = 8\n3rng = np.random.default_rng()\n4\n5whileremaining:\n6# --- 2. Draw candidates --------------- #\n7# Pick a constant-sized random subset\n8iflen(remaining) <= SAMPLE_SIZE:\n9candidates = remaining\n10else:\n11candidates = rng.choice(remaining,\nsize=SAMPLE_SIZE, replace=False),→\n12\n13# --- 3. Evaluate incremental cost --- #\n14best_cand =None\n15best_extra = math.inf\n16best_start_end = (0, 0)\n17\n18forcandincandidates:\n19extra_cost, t_start, t_end =\ncompute_incremental_cost(,→\n20self.txns[cand], key_map, txn_id,\ntotal_cost,→\n21)\n22ifextra_cost < best_extra:\n23best_extra = extra_cost\n24best_cand = cand\n25best_start_end = (t_start, t_end)\n26\n27# --- 4. Commit update global state ----- #\n28schedule.append(best_cand)\n29remaining.remove(best_cand)\n30\n31apply_txn_to_keymap(\n32self.txns[best_cand], key_map, txn_id,\nbest_start_end[0],→\n33)\n34total_cost += best_extra\n35txn_id += 1\n36\n37returntotal_cost, schedule\n(a) SMF / greedy policy.1defget_best_schedule(self, num_seqs: int = 10):\n2n = len(self.txns)\n3idxs = list(range(n))\n4\n5# --- 0. Pre-compute transaction statistics --- #\n6 write_cnt = [\n7 sum(1foropinself.txns[t]ifop[0] == \"w\")\nfortinidxs,→\n8 ]\n9\n10# - 1. Sorted seed sequence (by #writes, length) -\n#,→\n11 base_seq = sorted(idxs, key=lambdat:\n(write_cnt[t], len(self.txns[t]))),→\n12 best_seq = base_seq\n13 best_cost = self.get_opt_seq_cost(best_seq)\n14\n15# --- 2. Greedy insertion ------- #\n16improved =True\n17whileimproved:\n18improved =False\n19 foriinrange(n):\n20txn = best_seq.pop(i)\n21insert_best_pos = 0\n22insert_best_cost = float(\"inf\")\n23forposinrange(n):\n24candidate = best_seq[:pos] + [txn] +\nbest_seq[pos:],→\n25cost = self.get_opt_seq_cost(candidate)\n26ifcost < insert_best_cost:\n27insert_best_cost = cost\n28insert_best_pos = pos\n29best_seq.insert(insert_best_pos, txn)\n30ifinsert_best_cost < best_cost:\n31best_cost = insert_best_cost\n32improved =True\n33break\n34\n35# --- 3. Pair-swap hill climb -------- #\n36 max_iters = max(100, num_seqs *50)\n37 for_inrange(max_iters):\n38 i, j = random.sample(range(n), 2)\n39 ifi == j:\n40 continue\n41 new_seq = best_seq.copy()\n42 new_seq[i], new_seq[j] = new_seq[j], new_seq[i]\n43 new_cost = self.get_opt_seq_cost(new_seq)\n44 ifnew_cost < best_cost:\n45 best_cost = new_cost\n46 best_seq = new_seq\n47\n48# --- 4. Random restarts (keep best) ----------- #\n49 for_inrange(num_seqs):\n50 seq = idxs[:]\n51 random.shuffle(seq)\n52 cost = self.get_opt_seq_cost(seq)\n53 ifcost < best_cost:\n54 best_cost = cost\n55 best_seq = seq\n56\n57returnbest_cost, best_seq\n(b) Offline evolved policy.\n•Runtime Errors.The generated code fails to run due to compilation errors or exceeding the\nexperiment’s resource budget.\n•Search Failures.The evolutionary process stalls, such as getting stuck in a local optimum\nor alternating between poor solutions.\n•Algorithm Failures.The final solution runs, but is flawed or does not improve the baseline.\nThis includes solutions that ignore problem constraints, exploit loopholes in the evaluator,\nor rely on shallow tweaks.\nWe discuss these failures in more detail in Appendix B. Based on our experience, next, we present\nthe best-practices to avoid these failures and alleviate other limitations. We group these gudelines\nby the main components of ADRS, as depicted in Figure 3.\n17\n\nTable 3: Common failure patterns in ADRS pipelines (distribution estimated from 420 LLM-judged\ntraces).\nCategory Failure Type Description\nRuntime\nErrorsSyntax & Interface ErrorsCandidate solution fails to compile or integrate with evaluator.\nBudget ExhaustionCandidate exceeds resource limits (e.g., context window, API\nquotas, timeouts).\nSearch\nFailuresPremature ConvergenceSearch settles on a local optimal solution too early.\nStuck-in-the-LoopSearch repeats similar solutions without meaningful progress.\nMutation DriftSearch produces contradicting or random edits to the solution.\nAlgorithm\nFailuresMisaligned ObjectivesSolutions ignore key constraints (e.g., latency SLOs).\nSub-Optimal OptimizationsShallow changes (e.g., API calls) instead of substantive algo-\nrithmic improvement.\nOverfittingHard-coded / narrow solutions underperform on unseen traces.\nReward HackingSolution exploits loopholes in the evaluator rather than solving\nintended problem.\n6.1 PROMPTGENERATOR\nA clear and well-scoped problem formulation is the foundation of effective algorithm evolution.\nProvide structured specifications.Many execution and algorithm failures trace back to missing\ncontext, such as critical details about the problem or absent code API documentation. A well-\ndesigned prompt should be as specific and structured as possible, clearly defining three key areas:\n•The problem:what is the core task to solve.\n•The evaluation criteria:how a solution will be evaluated, including optimization goals and\ncorrectness constraints.\n•The context:any necessary information, such as required APIs.\nWe recommend drafting prompts with external LLMs (e.g., ChatGPT, Gemini, etc.) to craft a struc-\nture before launching evolution.\nProvide a suitable base program.The choice of base program strongly shapes the trajectory of\nalgorithm evolution. Buggy or weak baselines waste iterations on trivial fixes (budget exhaustion),\nwhile a strong, clean baseline can accelerate progress toward meaningful improvements. For exam-\nple, in the LLM-SQL case study, the published baseline already achieved near state-of-the-art prefix\nhit rate; the main bottleneck was runtime, so∼100 iterations sufficed to evolve a solution that was\n3×faster without loss in PHR.\nConversely, overly strong baselines that encode near-SOTA solutions or rely on high-level APIs\ncan limit the search to shallowmicro-optimizations. In the Can’t-be-Late problem (Section 5.1.1),\nevolution from a simple greedy baseline produced better results than starting from the stronger\nUniform Progress policy, which restricted exploration. We recommend seeding evolution with clean,\nminimal, high-quality baselines, e.g., using coding assistants such as Claude Code.\nProvide suitable solution hints.While a detailed problem specification is always beneficial, the\nvalue of providing solution hints – specific suggestions for how the system should approach the\nproblem – is more nuanced. Too much guidance can riskpremature convergenceand prevent the\ndiscovery of novel solutions, while too little can make the search inefficient (i.e.,stuck-in-the-loop).\nFor example, in the EPLB problem, hints could have prevented wasted iterations on “extreme”\nreplication strategies. However, in the transaction scheduling use case, hints about batching biased\nthe search toward sub-optimal designs, whereas leaving it unconstrained led to a 30% faster greedy\npolicy in OpenEvolve.\nWe find that providing intermediate human feedback as hints is especially effective when the search\ngets stuck in the loop. In summary, we recommend trying several prompts with different levels of\nhint specificity and inject relevant hints as how the evolution progresses.\n18\n\nChoose a suitable level of abstraction.We recommend exposing only the level of abstraction that\nmatches your goal. Allowing full access to high-level external library APIs can sometimes lead\ntosub-optimal optimizations: e.g., trivial speedups from replacing custom operators with PyTorch\nprimitives, while blocking deeper innovation. To encourage algorithmic advances, restrict API ac-\ncess to help the system explore new strategies rather than rely on pre-built solutions. On the other\nhand, when the goal is execution efficiency, providing optimized library access is appropriate. In\npractice, tuning this boundary between enabling useful shortcuts and enforcing genuine problem-\nsolving is critical to avoid micro-optimizations.\n6.2 SOLUTIONGENERATOR\nUse model ensembles.Search failures often arise when the solution generator either over-explores\nor over-exploits. We recommend using an ensemble of models to balance exploration and exploita-\ntion. On one hand, reasoning models, such as o3, encourage exploration by generating novel solu-\ntions. In our ablation study for the ‘Can’t Be Late” problem (Section 5.1), o3 provided more creative\nideas that led to higher-performing solutions. However, these models are typically slower and more\nexpensive. On the other hand, non-reasoning models are effective at refining existing solutions more\nefficiently. This enables OpenEvolve to iterate faster and perform many more iterations at the same\nbudget.\nTherefore, we currently recommend using two different models for efficient exploration. We find\nout that using more than two models often introduces conflicting ideas, leading to instability and\nmutation driftto the candidate solutions.\n6.3 EVALUATOR\nPrevent overfitting.Evaluating against narrow workloads lead to algorithm failures likeoverfitting,\nwhere the solutions either hard-code behaviors or overfit to specific traces. For example, our ablation\nstudy in the Can’t-be-Late use case (Appendix A) shows that limited spot scheduling evaluation to a\nsingle availability pattern led to worse performance on workloads that are not part of the evaluation\nset. Broader and more diverse test sets that cover edge cases help mitigate this issue.\nPrevent reward hacking.Reward hacking occurs when solutions exploit evaluator loopholes rather\nthan solving the intended problem. For example, when we evolve code for detecting failures in a\nmulti-agent system, some candidates bypassed one stage of the evaluation pipeline, achieving high\nscores without performing the full task. To avoid this, evaluators should combine multiple evaluation\nsignals (e.g., correctness, efficiency, robustness) and add adversarial tests. This makes it harder for\nADRS frameworks to ”game” the evaluation and also reduces failures ofmisaligned objectives,\nensuring candidates satisfy all constraints.\n6.4 SOLUTIONSELECTOR\nSearch outcomes also depend heavily on how candidates are retained between iterations, which is\ndetermined by the solution selector.\nBalance exploration-exploitation.Greedy selectors converge quickly but riskpremature conver-\ngence, while overly random ones waste resources and causestuck-in-the-loopfailures. Effective\nselectors maintain diversity while still advancing higher-quality candidates. For example, AlphaE-\nvolve blends MAP-Elites with island-based evolution to strike this balance. In practice, the explo-\nration–exploitation ratio is a tunable parameter (e.g., in OpenEvolve), and careful adjustment is key\nto avoiding search failures.\n7 LIMITATIONS ANDOPENCHALLENGES\nIn this section, we discuss the limitations of ADRS—specifically, the types of problems for which\nthe ADRS approach is well suited and those for which it is not—and then outline several open\nchallenges aimed at improving ADRS so that it can successfully address an increasingly broader\nrange of problems.\n19\n\n7.1 WHICHPROBLEMSAREBESTSUITED FORADRS,ANDWHICHARENOT?\nBased on our early experience, ADRS works best for problems that require localized changes, are\nfast to evaluate, and easily verifiable. It is less effective for problems that require changes across\nmany systems, rely on weak verifiers, or involve costly evaluations. Understanding these boundaries\nwill help researchers apply ADRS where it is most likely to succeed and adapt systems to better align\nwith its strengths. In summary, ADRS is well suited for systems performance problems that exhibit\nthe following properties:\n•Isolated changes:Since existing LLMs are more reliable when modifying small amounts of\ncode, ADRS is a better fit for problems that require improving policies or algorithms that\nare isolated from the rest of the system. Examples include schedulers, cache managers,\nload balancers, and resource allocators. In contrast, existing ADRS systems do not work\nwell for optimizing policies or protocols that are distributed across large systems, such\nas consensus protocols that involve a complex interplay between components (e.g. state\nmanagement, network communication and failure detection logic Lamport (2001); Ongaro\n& Ousterhout (2014)).\n•Reliable evaluations:Given two solutions, it should be easy to determine which one per-\nforms better and whether the two are semantically equivalent, i.e., they produce the same\noutputs. One example is improving a load balancer: it is straightforward to determine which\nsolution is better by measuring the imbalance factor, and any two solutions are trivially se-\nmantically equivalent if every query is routed to a replica of the same service. However, in\nother cases, proving semantic equivalence can be difficult or even infeasible. For instance,\narbitrarily modifying a query plan and verifying that it is equivalent to the original plan\nis, in general, impossible Abiteboul et al. (1995), though certain transformations provably\npreserve semantics, such as reordering joins or reordering columns or rows in a table.\n•Efficient evaluations:The evaluation must be both fast and cost-effective. Using ADRS\nto discover new solutions may require hundreds or even thousands of iterations. If each\nevaluation takes many hours to complete and costs hundreds of dollars, the overall process\ncan take months or even years and amount to hundreds of thousands of dollars. Some\nexamples are GPU-intensive workloads, such as weight compression, which can take more\nthan 12 hours per evolution cycle (see Section 5), or large-scale distributed training across\nthousands of GPUs.\nFinally, we do not expect ADRS to be as effective for problems that can be cast as optimization prob-\nlems and solved directly using existing solvers, such as integer linear programming (ILP) solvers.\n7.2 OPENCHALLENGES\nAs ADRS is emerging as a promising approach to accelerate systems research, two natural questions\narise: what should we build to better support ADRS and how should we improve ADRS itself?\n7.2.1 SUPPORTINGADRS: BUILDBETTEREVALUATORS\nA recurring lesson from our case studies is that ADRS is only as good as the evaluators guiding\nit. Successful discovery requiresevaluatorsthat provide accurate, fast, and detailed feedback to the\nsearch process. We distill below the key properties that should guide the design of evaluators for\nADRS:\n•Fidelity:Evaluators must capture the salient behaviors of the system that are relevant to the\nproblem being solved (e.g., flow-level or packet-level fidelity in networking). The desired\nlevel of fidelity depends not only on the system under study, but also on the type of solution\nthat ADRS might explore.\n•Generality:Evaluators must support diverse workloads and traces to provide reliable feed-\nback signals and prevent overfitting to a single configuration or dataset.\n•Speed and reliability:As discussed above, reliable and efficient evaluators are key to the\nsuccessful application of ADRS. Building testbeds that support rapid forking and rollback\n(e.g., via lightweight VMs, container snapshots, or database cloning) can drastically shorten\nfeedback cycles and improve ADRS scalability Liu et al. (2025).\nNext, we suggest two approaches to improve the efficiency of the evaluation framework.\n20\n\nProblem-specific simulators.Simulators offer fast, low-cost evaluation but are hard to design, as\nthey must balance fidelity and simplicity for effective LLM reasoning. A practical solution is to build\ndomain-specific simulators that capture only behaviors relevant to the task: for instance, modeling\nCPU scheduling without full operating system details. Tools like OpenEvolve can iteratively refine\nthese simulators until their metrics (e.g., latency, throughput) align with real systems. Though costly\nto build, such simulators can be reused across related problems, amortizing development effort.\nCascading evaluators.Fast and accurate evaluation can be achieved by cascading evaluators: from\nfast, coarse-grained cost models to slower but high-fidelity ones. For example, one might begin with\na simple cost model (e.g., for database queries Siddiqui et al. (2020)), then progress to simulators of\nincreasing granularity, followed by emulators, and finally tests on the real system. In networking,\nfor instance, session-level simulators offer coarser evaluation compared to packet-level simulators.\nThis mirrors standard research practice: prototype quickly, then validate precisely.\n7.2.2 IMPROVINGADRS\nTo improve the reach of ADRS, we need advances across the following key components.\nPrompt generator.Existing ADRS frameworks often rely on relatively simple prompts that provide\nonly a problem description. Just as human researchers ground their ideas in prior work, ADRS\nframeworks should incorporate retrieval techniques Packer et al. (2023) to draw from a broader\nbody of knowledge (e.g., academic literature, documentation, and related examples) to better guide\ntheir search. As discussed in Section 6, providing richer solution hints in a prompt improves search\nefficiency, while fewer hints may encourage broader exploration. Prompt evolution Novikov et al.;\nAgrawal et al. (2025), where the LLM refines its own context, can help manage this trade-off.\nSolution generator.A strong solution generator should act like a autonomous developer, being\nable to navigate, reason over, and modify a system codebase rather than isolated code snippets. For\nexample, modern AI workloads often rely on workload analysis and cross-layer optimizations and to\nreduce hardware costs Chung et al. (2024). Similarly, distributed systems that optimize communica-\ntion protocols may require coordinated changes to both the sender and the receiver logic Ongaro &\nOusterhout (2014); Ng et al. (2023). Achieving this demands more agentic coding abilities from the\nsolution generator: understanding dependencies, invoking analysis tools, and reasoning coherently\nacross non-contiguous code modules.\nBeyond a single model, ADRS frameworks should also support ensembles of specialized agents that\ncollaborate to produce high-quality solutions. Future work should focus on developing tools that\nautomatically optimize the composition based on the given problem, much like forming a research\nteam with complementary skills.\nEvaluator.Some of the existing ADRS frameworks like OpenEvolve require researchers to formal-\nize intuitive trade-offs into numerical weights, which can be difficult in practice. For example, in\nour EPLB case study, we struggled to decide exactly how to weigh the importance of load balance\nagainst the re-balancing algorithm runtime. Future systems could insteadlearnuser preferences\nautomatically. Some possible approaches are:\n•Preference Learning.ADRS could present researchers with pairs of solutions (e.g., “So-\nlution A is faster but less fair; Solution B is fairer but slower”), and the researcher selects\nthe one they prefer. By observing these choices, ADRS can infer the underlying objective\nfunction to optimize.\n•Inverse Reinforcement Learning (IRL).If a researcher can provide a few examples of\n“good” solutions (e.g., those with particular levels of fairness and performance), ADRS\ncan work backward to infer the reward function most likely to generate such solutions.\nSolution selector.The process by which ADRS frameworks discover new solutions remains a\nblack box. Current evolutionary searches are monolithic, producing only a single final program,\noften mixing a good idea with poor implementation and thus penalizing promising concepts. Prior\nwork Tang et al. (2025) suggests separating ideation from code generation to address this issue.\nThe evolutionary search is also inefficient, frequently looping over failed heuristics or repeated errors\n(Appendix B). More flexible frameworks are needed to allow finer-grained feedback, letting humans\nor LLMs lock in working code, boost diversity to escape local optima, or roll back to prior versions\nwhen evolution stalls.\n21\n\nOverall framework.Finally, we discuss open challenges related to the overall ADRS approach.\nHyperparameter tuning.Balancing exploration and exploitation in LLMs remains difficult and often\nrelies on trial and error. Future work should focus on automating this tuning process. A meta-\nlearning layer, for instance, could allow the system to learn and adjust as evolution progresses,\nmaking the entire framework more reliable and accessible.\nHuman-ADRS interaction.The optimal balance between synchronous (interactive assistants like\nCursor) and asynchronous (autonomous frameworks like OpenEvolve) user interface remains an\nopen question. A key challenge remains for ADRS is to define when human guidance adds values\nversus when ADRS should act autonomously.\n8 HOWCANADRS IMPACT THERESEARCHPROCESS?\nDespite its limitations (see Section 7.1), ADRS can already help researchers in two key ways:\n•Accelerating discovery:ADRS tools automate tedious tasks such as implementation and,\nto some extent, debugging. This frees researchers to focus on problem selection, system\narchitecture and design. Even imperfect solutions offer value by revealing new directions.\nIn our telemetry repair case study, AI-generated insights shaped a better human-designed\nalgorithm.\n•Achieve better-than-human results:ADRS tools can explore the solution space more thor-\noughly than humans. Where a human might stop after a breakthrough, AI can continue\ntesting variations for incremental gains. These small improvements compound. In our\nEPLB case study, OpenEvolve produced an algorithm that surpassed the state-of-the-art by\nexploiting missed optimizations.\nAs such, ADRS has the potential to reshape systems research as we know it.\nOne way to think about ADRS is as providing researchers with a virtually unbounded number of\n“assistants” that follow directions reliably. Much like junior researchers, ADRS frameworks are\nmost effective when given clear problem specifications and well-defined goals.\nAs a result, ADRS adds another layer to an already established research hierarchy. In academia,\nfor example, a faculty member typically advises Ph.D. students or postdocs, who in turn mentor\nundergraduate students. Each of these roles can now leverage ADRS tools to accelerate their work\nand broaden the scope of what they can accomplish.\nIn this context, two natural questions arise:\nHow would the role of a researcher change?As ADRS tools begin to solve an increasing num-\nber of problems autonomously, researchers will have more time to focus on higher-leverage activi-\nties—selecting which problems to pursue and formulating them precisely. If successful, ADRS has\nthe potential to elevate everyone in the research hierarchy—faculty and students alike—and make\nthem far more productive.\nWill ADRS lead to fewer researchers?We believe the opposite is true. If anything, ADRS will\nexpand the research community by enabling individuals who may not be expert problem solvers to\ncontribute meaningfully. The result will be a faster rate of progress—solving more problems, faster\nand better. And, as history has shown, the supply of open research problems is virtually unbounded.\nFor example, in the context of AI systems alone, workloads are becoming more complex (e.g.,\ntest-time compute, online reinforcement learning), hardware is growing more heterogeneous (e.g.,\nspecialized accelerators, new networking technologies), and performance and scalability demands\nare higher than ever (e.g., multi-modal training and inference).\n9 CONCLUSION\nAs AI systems take over algorithm discovery, the role of the human researcher will change. Much\nlike an academic advisor guides a student, the researcher of the future will act as a guide for these\nAI systems. The researchers’ responsibilities will continue to be defining the problem, steering the\nresearch process, and critically evaluating the results.\n22\n\nOne of the most profound implication is the potential for avirtuous cycle. We can use an ADRS to\nimprove itself. As recent work has shown, models can learn to refine their own reasoning, debug\ntheir code, and discover more effective strategies. As these AI agents rapidly improve themselves,\nthey will compound the pace of scientific discovery.\nTo summarize, in this paper, we have illustrated the potential of ADRS in systems research. Our case\nstudies show that the ADRS approach can already outperform human baselines on key performance\nproblems. While still very early, we call on the systems community to embrace these tools, not just\nas accelerators of research, but as subjects of research. Improving ADRS–making them efficient,\nscalable, and reliable–is itself a systems challenge. As such, we believe that system builders are\nuniquely positioned to shape the future of AI-driven discovery.\nACKNOWLEDGMENTS\nWe thank Aurojit Panda, Tianyin Xu, Asankhaya Sharma, Rishabh Iyer, and Dave Patterson for\ntheir valuable feedback and insightful discussions. This research was supported by gifts from Ac-\ncenture, AMD, Anyscale, Broadcom Inc., Google, IBM, Intel, Intesa Sanpaolo, Lambda, Mibura\nInc, Samsung SDS, and SAP.\nREFERENCES\nSuryanarayan Menon A., Sanjay J Prakash, Vinayak Naveen, Roshan Aji Cherian, Ron Regi\nZacharia, Suryapriya S., and Josna Vr. A survey on routing algorithms and techniques used to\nimprove network performance in software-defined networking. In2023 2nd International Con-\nference on Computational Systems and Communication (ICCSC), pp. 1–6, 2023.\nSerge Abiteboul, Richard Hull, and Victor Vianu.Foundations of Databases. Addison-Wesley,\n1995. ISBN 0-201-53771-0.\nLakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong,\nArnav Singhvi, Herumb Shandilya, Michael J Ryan, Meng Jiang, et al. Gepa: Reflective prompt\nevolution can outperform reinforcement learning.arXiv preprint arXiv:2507.19457, 2025.\nDeepSeek AI. Expert parallelism load balancer (eplb).https://github.com/deepseek-\nai/eplb, 2024.\nDana Van Aken, Andrew Pavlo, Geoffrey J. Gordon, and Bohan Zhang. Automatic database man-\nagement system tuning through large-scale machine learning. InProceedings of the 2017 ACM\nSIGMOD International Conference on Management of Data (SIGMOD ’17), pp. 1009–1024,\nChicago, IL, USA, 2017. ACM. doi: 10.1145/3035918.3064029.\nAnthropic. Claude code: Agentic code assistant.https://www.anthropic.com/, 2025. Ac-\ncessed: 2025-09-30.\nA. Boukerche, B. Turgut, N. Aydin, M.Z. Ahmad, L. B ¨ol¨oni, and D. Turgut. Routing protocols in\nad hoc networks: a survey.Computer Networks, 55(13):3032–3080, September 2011.\nMert Cemri, Melissa Z Pan, Shuyi Yang, Lakshya A Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt\nKeutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, et al. Why do multi-agent llm\nsystems fail?arXiv preprint arXiv:2503.13657, 2025.\nAudrey Cheng, Xiao Shi, Aaron Kabcenell, Shilpa Lawande, Hamza Qadeer, Jason Chan, Harrison\nTin, Ryan Zhao, Peter Bailis, Mahesh Balakrishnan, Nathan Bronson, Natacha Crooks, and Ion\nStoica. Taobench: An end-to-end benchmark for social network workloads.Proc. VLDB Endow.,\n15(9):1965–1977, may 2022. ISSN 2150-8097.\nAudrey Cheng, Aaron Kabcenell, Jason Chan, Xiao Shi, Peter Bailis, Natacha Crooks, and Ion\nStoica. Towards optimal transaction scheduling.Proceedings of the VLDB Endowment, 17(11):\n2694–2707, 2024.\nJae-Won Chung, Nishil Talati, and Mosharaf Chowdhury. Toward cross-layer energy optimizations\nin ai systems, 2024. arXiv:2404.06675v2.\n23\n\nAditya Desai, Kumar Krishna Agrawal, Shuo Yang, Alejandro Cuadron, Luis Gaspar Schroeder,\nMatei Zaharia, Joseph E. Gonzalez, and Ion Stoica. vattention: Verified sparse attention, 2025.\nURLhttps://arxiv.org/abs/2510.05688.\nKuntai Du, Ahsan Pervaiz, Xin Yuan, Aakanksha Chowdhery, Qizheng Zhang, Henry Hoffmann,\nand Junchen Jiang. Server-driven video streaming for deep learning inference. InProceedings of\nthe Annual Conference of the ACM Special Interest Group on Data Communication on the Appli-\ncations, Technologies, Architectures, and Protocols for Computer Communication (SIGCOMM\n’20), pp. 557–570, Virtual Event, USA, 2020. ACM. doi: 10.1145/3387514.3405887.\nRohit Dwivedula, Divyanshu Saxena, Aditya Akella, Swarat Chaudhuri, and Daehyeok Kim. Man-\nmade heuristics are dead. long live code generators! InProceedings of the 24th ACM Workshop\non Hot Topics in Networks (HotNets ’25), 2025.\nJinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu,\nSiwei Liu, Zihao Li, et al. A comprehensive survey of self-evolving ai agents: A new paradigm\nbridging foundation models and lifelong agentic systems.arXiv preprint arXiv:2508.07407, 2025.\nChuanchao Gao, Niraj Kumar, and Arvind Easwaran. Energy-efficient real-time job mapping and\nresource management in mobile-edge computing. In2024 IEEE Real-Time Systems Symposium\n(RTSS), pp. 15–28. IEEE, December 2024. doi: 10.1109/rtss62706.2024.00012. URLhttp:\n//dx.doi.org/10.1109/RTSS62706.2024.00012.\nGitHub. Github copilot.https://github.com/features/copilot, 2021. Accessed:\n2025-09-30.\nAnna Goldie and Azalia Mirhoseini. How alphachip transformed computer chip design.\nhttps://deepmind.google/discover/blog/how-alphachip-transformed-\ncomputer-chip-design/, September 2024. DeepMind Blog.\nQingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian,\nand Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful\nprompt optimizers.arXiv preprint arXiv:2309.08532, 2023.\nRuining He and Julian McAuley. Ups and downs: Modeling the visual evolution of fashion trends\nwith one-class collaborative filtering. InProceedings of the 25th International Conference on\nWorld Wide Web, WWW ’16, pp. 507–517, Republic and Canton of Geneva, CHE, 2016. In-\nternational World Wide Web Conferences Steering Committee. ISBN 9781450341431. doi:\n10.1145/2872427.2883037. URLhttps://doi.org/10.1145/2872427.2883037.\nCharles Hong, Sahil Bhatia, Alvin Cheung, and Yakun Sophia Shao. Autocomp: Llm-driven code\noptimization for tensor accelerators, 2025. URLhttps://arxiv.org/abs/2505.18574.\nSirui Hong, Mingchen Zhuge, Jiaqi Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang,\nZili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin\nWu, and J ¨urgen Schmidhuber. Metagpt: Meta programming for a multi-agent collaborative frame-\nwork, 2024. URLhttps://arxiv.org/abs/2308.00352.\nCursor Inc. Cursor: Ai coding assistant.https://www.cursor.com/, 2024. Accessed: 2025-\n09-30.\nNathan Jay, Noga H Rotman, P Godfrey, Michael Schapira, and Aviv Tamar. Internet congestion\ncontrol via deep reinforcement learning.arXiv preprint arXiv:1810.03259, 2018.\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,\nKathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ´ıdek, Anna Potapenko, et al. Highly accurate\nprotein structure prediction with alphafold.nature, 596(7873):583–589, 2021.\nKamil Khan, Sudeep Pasricha, and Ryan Gary Kim. A survey of resource management for\nprocessing-in-memory and near-memory processing architectures.Journal of Low Power Elec-\ntronics and Applications, 10(4):30, September 2020. doi: 10.3390/jlpea10040030. URL\nhttps://doi.org/10.3390/jlpea10040030.\n24\n\nTim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures, 2018. URLhttps://arxiv.org/abs/1712.01208.\nAlexander Krentsel, Rishabh Iyer, Isaac Keslassy, Sylvia Ratnasamy, Anees Shaikh, and Rob Shakir.\nThe case for validating inputs in software-defined wans. InProceedings of the 23rd ACM Work-\nshop on Hot Topics in Networks, pp. 246–254, 2024.\nLeslie Lamport. Paxos made simple.ACM SIGACT News (Distributed Computing Column) 32, 4\n(Whole Number 121, December 2001), pp. 51–58, 2001.\nRobert Tjarko Lange, Yuki Imajuku, and Edoardo Cetin. Shinkaevolve: Towards open-ended and\nsample-efficient program evolution, 2025. URLhttps://arxiv.org/abs/2509.19349.\nJinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin,\nRuiying Geng, Nan Huo, et al. Can llm already serve as a database interface? a big bench for\nlarge-scale database grounded text-to-sqls.Advances in Neural Information Processing Systems,\n36, 2024.\nChieh-Jan Mike Liang, Haoran Qiu, Francis Y . Yan, Tianyin Xu, and Lidong Zhou. The next horizon\nof system intelligence.https://www.microsoft.com/en-us/research/blog/the-\nnext-horizon-of-system-intelligence/, September 2025. Accessed: 2025-09-29.\nEric Liang, Hang Zhu, Xin Jin, and Ion Stoica. Neural packet classification, 2019. URLhttps:\n//arxiv.org/abs/1902.10319.\nFei Liu, Rui Zhang, Zhuoliang Xie, Rui Sun, Kai Li, Xi Lin, Zhenkun Wang, Zhichao Lu, and\nQingfu Zhang. Llm4ad: A platform for algorithm design with large language model.arXiv\npreprint arXiv:2412.17287, 2024a.\nShu Liu, Asim Biswal, Amog Kamsetty, Audrey Cheng, Luis Gaspar Schroeder, Liana Patel, Shiyi\nCao, Xiangxi Mo, Ion Stoica, Joseph E Gonzalez, et al. Optimizing llm queries in relational data\nanalytics workloads.arXiv preprint arXiv:2403.05821, 2024b.\nShu Liu, Soujanya Ponnapalli, Shreya Shankar, Sepanta Zeighami, Alan Zhu, Shubham Agarwal,\nRuiqi Chen, Samion Suwito, Shuo Yuan, Ion Stoica, et al. Supporting our ai overlords: Redesign-\ning data systems to be agent-first.arXiv preprint arXiv:2509.00997, 2025.\nPhillip Long, Zachary Novack, Taylor Berg-Kirkpatrick, and Julian McAuley. Pdmx: A large-\nscale public domain musicxml dataset for symbolic music processing, 2024. URLhttps://\narxiv.org/abs/2409.10831.\nRuiying Ma, Chieh-Jan Mike Liang, Yanjie Gao, and Francis Y . Yan. Algorithm generation via\ncreative ideation, 2025. URLhttps://arxiv.org/abs/2510.03851.\nDaniel J Mankowitz, Andrea Michi, Anton Zhernov, Marco Gelmi, Marco Selvi, Cosmin Paduraru,\nEdouard Leurent, Shariq Iqbal, Jean-Baptiste Lespiau, Alex Ahern, et al. Faster sorting algorithms\ndiscovered using deep reinforcement learning.Nature, 618(7964):257–263, 2023.\nHongzi Mao, Malte Schwarzkopf, Shaileshh Bojja Venkatakrishnan, Zili Meng, and Mohammad\nAlizadeh. Learning scheduling algorithms for data processing clusters. InProceedings of the\n2019 ACM SIGCOMM Conference, pp. 270–288, Beijing, China, 2019. ACM. doi: 10.1145/\n3341302.3342080. URLhttps://doi.org/10.1145/3341302.3342080.\nZiming Mao, Tian Xia, Zhanghao Wu, Wei-Lin Chiang, Tyler Griggs, Romil Bhardwaj, Zongheng\nYang, Scott Shenker, and Ion Stoica. Skyserve: Serving ai models across regions and clouds\nwith spot instances. InProceedings of the Twentieth European Conference on Computer Systems,\nEuroSys ’25, pp. 159–175, New York, NY , USA, 2025. Association for Computing Machinery.\nISBN 9798400711961. doi: 10.1145/3689031.3717459. URLhttps://doi.org/10.1145/\n3689031.3717459.\nRyan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh, Tim Kraska, Olga\nPapaemmanouil, and Nesime Tatbul. Neo: a learned query optimizer.Proceedings of the VLDB\nEndowment, 12(11):1705–1718, July 2019. ISSN 2150-8097. doi: 10.14778/3342263.3342644.\nURLhttp://dx.doi.org/10.14778/3342263.3342644.\n25\n\nJulian McAuley, Jure Leskovec, and Dan Jurafsky. Learning attitudes and attributes from multi-\naspect reviews, 2012. URLhttps://arxiv.org/abs/1210.3926.\nWes McKinney. Data structures for statistical computing in python. InProceedings of the 9th Python\nin Science Conference (SciPy 2010), pp. 51–56. SciPy, 2010.\nAzalia Mirhoseini, Hieu Pham, Quoc V . Le, Benoit Steiner, Rasmus Larsen, Yuefeng Zhou,\nNaveen Kumar, Mohammad Norouzi, Samy Bengio, and Jeff Dean. Device placement op-\ntimization with reinforcement learning. InProceedings of the 34th International Conference\non Machine Learning (ICML 2017), volume 70, pp. 2430–2439, Sydney, Australia, 2017.\nPMLR. doi: 10.5555/3305890.3305932. URLhttp://proceedings.mlr.press/v70/\nmirhoseini17a.html.\nJean-Baptiste Mouret and Jeff Clune. Illuminating search spaces by mapping elites.arXiv preprint\narXiv:1504.04909, 2015. Also published/extended in later venues.\nAnsh Nadga, Abhradeep Thakurta, and Prabhakar Raghavan. Ai as a research partner: Advancing\ntheoretical computer science with alphaevolve.https://research.google/blog/ai-\nas-a-research-partner-advancing-theoretical-computer-science-\nwith-alphaevolve/, September 30 2025. Google DeepMind Research Blog.\nDeepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vincent\nMoens, Amar Budhiraja, Despoina Magka, Vladislav V orotilov, Gaurav Chaurasia, et al. Ml-\ngym: A new framework and benchmark for advancing ai research agents.arXiv preprint\narXiv:2502.14499, 2025.\nHarald Ng, Seif Haridi, and Paris Carbone. Omni-paxos: Breaking the barriers of partial connec-\ntivity. InProceedings of the Eighteenth European Conference on Computer Systems, EuroSys\n’23, pp. 314–330, New York, NY , USA, 2023. Association for Computing Machinery. ISBN\n9781450394871.\nAlexander Novikov, Ng ˆan Vu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt\nWagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, et al.\nAlphaevolve: A coding agent for scientific and algorithmic discovery, 2025.URL: https://arxiv.\norg/abs/2506.13131.\nDiego Ongaro and John Ousterhout. In search of an understandable consensus algorithm. InPro-\nceedings of the 2014 USENIX Annual Technical Conference (USENIX ATC ’14), pp. 305–320,\nPhiladelphia, PA, 2014. USENIX Association.\nOpenAI. Openai codex.https://openai.com/index/introducing-codex/, 2025. Ac-\ncessed: 2025-09-30.\nAnne Ouyang, Simon Guo, Simran Arora, Alex L Zhang, William Hu, Christopher R ´e, and Azalia\nMirhoseini. Kernelbench: Can llms write efficient gpu kernels?arXiv preprint arXiv:2502.10517,\n2025.\nCharles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, and Joseph E.\nGonzalez. Memgpt: Towards LLMs as operating systems.arXiv preprint arXiv:2310.08560,\n2023. Version v2, 12 Feb 2024.\nBo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization\nwith respect to rating scales. InProceedings of the ACL, 2005.\nOri Press, Brandon Amos, Haoyu Zhao, Yikai Wu, Samuel K Ainsworth, Dominik Krupke, Patrick\nKidger, Touqir Sajed, Bartolomeo Stellato, Jisun Park, et al. Algotune: Can language models\nspeed up general-purpose numerical programs?arXiv preprint arXiv:2507.15887, 2025.\nChen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize\nChen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Chat-\ndev: Communicative agents for software development, 2024. URLhttps://arxiv.org/\nabs/2307.07924.\n26\n\nKrzysztof Rusek, Jose Suarez-Varela, Paul Almasan, Pere Barlet-Ros, and Albert Cabellos-\nAparicio. Routenet: Leveraging graph neural networks for network modeling and optimization\nin sdn.IEEE Journal on Selected Areas in Communications, 38(10):2260–2270, October 2020.\nISSN 1558-0008. doi: 10.1109/jsac.2020.3000405. URLhttp://dx.doi.org/10.1109/\nJSAC.2020.3000405.\nAsankhaya Sharma. Openevolve: an open-source evolutionary coding agent, 2025. URLhttps:\n//github.com/codelion/openevolve.\nTarique Siddiqui, Alekh Jindal, Shi Qiao, Hiren Patel, and Wangchao Le. Cost models for big\ndata query processing: Learning, retrofitting, and our findings. InProceedings of the 2020 ACM\nSIGMOD International Conference on Management of Data (SIGMOD ’20), pp. 15–29, Portland,\nOR, USA, 2020. ACM. doi: 10.1145/3318464.3380584. URLhttps://doi.org/10.1145/\n3318464.3380584.\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,\nThomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go\nwithout human knowledge.nature, 550(7676):354–359, 2017.\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur\nGuez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap,\nKaren Simonyan, and Demis Hassabis. A general reinforcement learning algorithm that mas-\nters chess, shogi, and go through self-play.Science, 362(6419):1140–1144, 2018. doi:\n10.1126/science.aar6404.\nRamneet Singh, Sathvik Joel, Abhav Mehrotra, Nalin Wadhwa, Ramakrishna B Bairi, Aditya\nKanade, and Nagarajan Natarajan. Code researcher: Deep research agent for large systems code\nand commit history.arXiv preprint arXiv:2506.11060, 2025.\nShewaye Sirika and Smita Mahajan. Survey on dynamic routing protocols.International Jour-\nnal of Engineering Research & Technology (IJERT), 5(1), January 2016. doi: 10.17577/\nIJERTV5IS010028. Paper ID: IJERTV5IS010028.\nReiko Tanese.Distributed Genetic Algorithms for Function Optimization. PhD thesis, University of\nMichigan, 1989. Ph.D. thesis.\nJiabin Tang, Lianghao Xia, Zhonghang Li, and Chao Huang. Ai-researcher: Autonomous scientific\ninnovation.arXiv preprint arXiv:2505.18705, 2025.\nTrieu H. Trinh, Yuhuai Wu, Quoc V . Le, He He, and Thang Luong. Solving olympiad geometry\nwithout human demonstrations.Nature, 625(7995):476–482, 2024. doi: 10.1038/s41586-023-\n06747-5.\nKent Walker. A summer of security: empowering cyber defenders with ai, July 2025.\nURLhttps://blog.google/technology/safety-security/cybersecurity-\nupdates-summer-2025/. Accessed: 2025-10-02.\nSarah Wooders, Shu Liu, Paras Jain, Xiangxi Mo, Joseph E Gonzalez, Vincent Liu, and Ion Stoica.\nCloudcast:{High-Throughput},{Cost-Aware}overlay multicast in the cloud. In21st USENIX\nSymposium on Networked Systems Design and Implementation (NSDI 24), pp. 281–296, 2024.\nZhanghao Wu, Wei-Lin Chiang, Ziming Mao, Zongheng Yang, Eric Friedman, Scott Shenker, and\nIon Stoica. Can’t be late: optimizing spot instance savings under deadlines. In21st USENIX\nSymposium on Networked Systems Design and Implementation (NSDI 24), pp. 185–203, 2024.\nZongheng Yang, Amog Kamsetty, Sifei Luan, Eric Liang, Yan Duan, Xi Chen, and Ion Stoica.\nNeurocard: one cardinality estimator for all tables.Proc. VLDB Endow., 14(1):61–73, Septem-\nber 2020. ISSN 2150-8097. doi: 10.14778/3421424.3421432. URLhttps://doi.org/\n10.14778/3421424.3421432.\nZongheng Yang, Wei-Lin Chiang, Sifei Luan, Gautam Mittal, Michael Luo, and Ion Stoica.\nBalsa: Learning a query optimizer without expert demonstrations. InProceedings of the 2022\nInternational Conference on Management of Data, SIGMOD/PODS ’22, pp. 931–944, New\nYork, NY , USA, 2022. Association for Computing Machinery. ISBN 9781450392495. doi:\n10.1145/3514221.3517885. URLhttps://doi.org/10.1145/3514221.3517885.\n27\n\nShan Yu, Jiarong Xing, Yifan Qiao, Mingyuan Ma, Yangmin Li, Yang Wang, Shuo Yang, Zhiqiang\nXie, Shiyi Cao, Ke Bao, et al. Prism: Unleashing gpu sharing for cost-efficient multi-llm serving.\narXiv preprint arXiv:2505.04021, 2025.\nLidong Zhou. A match made in silicon: The co-evolution of systems and ai. InNeurIPS 2024 Invited\nTalks, 2024. URLhttps://neurips.cc/virtual/2024/invited-talk/101132.\nAccessed: 2025-10-07.\n28\n\nA ABLATIONSTUDIES\nWe have conducted a few ablation studies on the Can’t Be Late case study and provide the results as\nfollows.\nA.1 GEPARESULTS\nIn addition to OpenEvolve, we apply GEPA to evolve the program generation for the Can’t Be Late\nuse case. We build a custom adapter that wraps two stages of evaluation, similar to OpenEvolve:\nstage 1 validates syntax and simulator compliance, while stage 2 runs full simulations on our test\nworkloads to compute cost savings. GEPA uses this score as the fitness value and attaches diagnostic\nfeedback for failed runs. Feedback is then converted into a reflective dataset which guides a model\n(we use OpenAI o3 in this setup) to rewrite the candidate program. We initialize the process with\nthe greedy baseline, and set our evaluation metric as the average cost savings across single-region\ntraces.\nWe give the greedy program as the base program, and use Claude Opus to refine the base program\nentirely. The feedback function given to GEPA is the score evaluated on the downstream single-\nregion traces. Our final result achieved 4% improvement over the greedy baseline at iteration 68,\ncapping the search at 200 iterations.\nA.2 CHANGINGTRAINSETCOVERAGE\n0204060Cost savings (%)1xK80 (0.95) 1xV100 (0.68) 1xV100 (0.60) 8xK80 (0.59)\n0.6 0.8\nJob Fraction0204060Cost savings (%)8xK80 (0.55)\n0.6 0.8\nJob Fraction8xV100 (0.34)\n0.6 0.8\nJob Fraction8xV100 (0.33)\n0.6 0.8\nJob Fraction1xK80 (0.25)\nGreedy Uniform Progress OpenEvolve Solution\n(a) Training with 3% of the available training set.\n0204060Cost savings (%)1xK80 (0.95) 1xV100 (0.68) 1xV100 (0.60) 8xK80 (0.59)\n0.6 0.8\nJob Fraction0204060Cost savings (%)8xK80 (0.55)\n0.6 0.8\nJob Fraction8xV100 (0.34)\n0.6 0.8\nJob Fraction8xV100 (0.33)\n0.6 0.8\nJob Fraction1xK80 (0.25)\nGreedy Uniform Progress OpenEvolve Solution (b) Training with the full training set.\nFigure 8: Impact of training set coverage on policy evolution. We split data into 30% training and\n70% testing. The left panel shows results when using only 3% of the training data, while the right\npanel uses the entire training set.\n29\n\nB FAILURETAXONOMY\nADRSes demonstrate exciting capabilities in advancing systems research in our case study; however,\nthey currently face challenges that require careful setup. To mitigate these challenges, we first\ncharacterize the common failure modes ADRSes exhibit. In the next section (see Section 6), we\noutline best practices and guidelines to address these limitations.\nAs shown in Table 3, we group failures into three broad categories—execution errors, search (evolu-\ntion) failures, and algorithm failures, and report their frequency in our evaluation. Detailed descrip-\ntions of each failure type are provided in Section 6.\nExecution Errors.Among one third of the failures come from immediate execution failures. The\nmost common cases aresyntax and interface errors, where the generated solution code fails to\ncompile or connect with the evaluator due to missing imports, type mismatches, or misusing required\ninterfaces. Other fails due tobudget exhaustion, where a candidate consumes excessive memory,\nruns into timeouts, or exceeds model API quotas.\nSearch Failures.Roughly half of the failiures happen when the solution executes but search fails\nto make progress.Premature convergencehappens when the search fo solution settles on a local\noptimum too early, such as sticking with a steiner tree strategy in multi-region transfer while ignoring\nopportunities to make use of data partitions.Stuck-in-the-looprefers to generating near-duplicate\nsolutions without improvement, like iterations that only rename variables or add logging.Mutation\ndriftoccurs when edits are contradictory or random. For example, oscillating between BFS tree and\ngraph-based method in the multi-region network transfer task, slowing or preventing convergence.\nAlgorithm Failures.These are failures where candidate solutions run but fail to advance algo-\nrithms. Common patterns includemisaligned objectives, where the solution ignore constraints (e.g.,\nthroughput gains that break latency SLOs).Suboptimal optimizationsalso appear frequently as the\nevolved solution makes shallow tweaks in APIs instead of coming up with algorithmic innovation.\nOverfittinghard-codes behavior to evaluation traces (e.g., spot scheduling policies collapsing on\nunseen spot patterns).Reward hackingexploits loopholes in the evaluator. For example, in our\nmulti-agent runs, some candidates bypass a required stage in algorithm, causing the evaluator to\nassign higher scores.\nC CASESTUDIES\nWe now present detailed evolution results from the case studies detailed in Table 1 as follows.\nC.1 ADAPTIVE WEIGHT COMPRESSION\nThis task studies adaptive weight quantization with variable per-column bitrate. The objective is to\nassign high vs. low bitrates across columns to minimize overall storage cost (measured in average\nbits per element across all elements in the weight tensor, lower is better) while preserving accuracy\n(measured by Wikitext-2 perplexity, or PPL, where lower indicates higher accuracy). The base\nprogram is a hand-written heuristic that applies dynamic bitrate encoding, where each column of\nthe weight matrix is quantized at a bitrate chosen according to its importance score. An initial\nhand-tuned calculation and mapping from importance score to column bitrate provided reasonable\ncompression performance of 2.64 bits/elem and PPL of 22.9 on a Llama3.2-1B, but left room for\nimprovement.\nTo guide evolution, the evaluator combines normalized scores from Wikitext PPL (0.5), PTB PPL\n(0.2), and bitrate (0.3), with solutions exceeding 2.5 bits/elem and PPl of 30 receiving zero score,\nto hit a compression ratio under 2.5 with the highest accuracy. We use Gemini-2.5-pro to generate\ncandidates, running 200 iterations in about 12 hours with less than 20 USD. The evolution takes\nmore than 10 hours due to the evaluator time, i.e., each evaluation needs to fully quantize the entire\nLlama3.2-1B model before computing perplexity and bitrate. The evolved program achieves 2.49\nbits/elem with PPL of 22.84, hitting a lower bitrate compared to the best sweep configuration (2.5\nbits/elem) while maintaining accuracy.\nThe improved solution from the evolution simply tunes hyperparameter values of the hand-crafted\nmapping between importance scores and bitrates, without any algorithmic improvements on column\n30\n\nimportance finding or mapping strategy. As such, it offers only marginal gains over a parameter\nsweep and remains more of an automated hyperparameter tuner than a novel quantization strategy.\nFuture work could explore structural changes to better prompting (e.g., explicitly listing the meth-\nods and parameters to explore) or evolutionary settings (e.g., exploration ratio) to move beyond\nincremental improvements.\nC.2 NETWORKTELEMETRYREPAIR\nThis task identified by HotNets’24 Krentsel et al. (2024) studies repair of faulty router telemetry\nsignals, which can become buggy due to router faults or collection infrastructure errors. Such in-\nconsistencies (for example, counters on the two ends of a link not matching) can cause the network\ncontrollers to make incorrect decisions. The objective is to detect and repair faulty telemetry to\nproduce a self-consistent view of network state.\nWe use OpenEvolve to evolve repair strategies, running 300 iterations (about 8 hours) with a mix of\nreasoning (GPT-o3, 80%) and non-reasoning (GPT-4o, 20%) models, supplemented by contextual\nhints from the HotNets’24 paper. The evolved program introduces structured repair logic, including\naveraging nearby counters to reduce noise, and separating repair and confidence estimation into\ndistinct steps. It achieves a repair score of 95% and confidence calibration of 95%, outperforming\nthe HotNets’24 solution (86% repair, 65% confidence).\nC.3 CLOUDCAST\nThe Cloudcast problem, published in NSDI ’24 Wooders et al. (2024), studies cost-aware multicast\nacross multi-region and multi-cloud topologies. The objective is to construct an overlay connecting\na source, multiple destinations, and optional waypoints so as to minimize egress cost.\nOur initial program is a direct replication strategy: the source sends data independently to each\ndestination. While simple, this approach often incurs high egress costs when destinations are located\nin distant or expensive regions. To guide evolution, the evaluator tests candidate algorithms across\n10 multi-region, multi-cloud configurations and assigns a total score based on the overall egress cost\nof the scheduled paths. We use OpenEvolve with a mix of o3 and Gemini-2.5-pro models, running\n100 iterations in about one hour, costing less than $10 for the entire run.\nThe evolved solution successfully rediscovers the Steiner tree strategy as the state-of-the-art, achiev-\ning an average cost reduction of 31.1% compared to direct replication. This solution constructs a\ncost-efficient multicast tree by introducing intermediate waypoints. For example, data may be repli-\ncated once at a cheaper waypoint region and then forwarded to multiple destinations, reducing the\ntotal egress cost.\nC.4 GLOBAL MODEL PLACEMENT\nThe research problem Yu et al. (2025) focus on the challenge of multi-LLM serving on shared GPUs\nunder bursty, heterogeneous workloads. The key metric is the KV pressure ratio (KVPR), defined\nas the SLO-weighted request rate divided by available KV cache memory. The optimization goal\nis to minimize the maximum KVPR across GPUs, thereby reducing contention and improving SLO\ncompliance.\nThe base program is the algorithm from the paper Yu et al. (2025): models are placed sequentially\nonto the first GPU with sufficient remaining memory, without considering long-term balance. While\nfeasible, this approach often overloads some GPUs while leaving others underutilized. We use\nOpenEvolve to evolve improved placement strategies, guided by a scoring function that combines\nexecution correctness with the KVPR objective. The evolution runs with GPT-4o (70%) and GPT-\no3 (30%) as the model ensemble, converging in about 70 iterations (roughly 40 minutes). The best\nevolved program achieves an 18.5% higher score compared to the state-of-the-art reported in the\noriginal paper.\nThe evolved strategy mirrors the SOTA algorithm from the paper and assignment used in PRISM,\nbut crucially adds a local improvement stage. After the initial placement, it repeatedly tests whether\nmoving or swapping models between GPUs reduces the maximum KVPR, applies such refinements,\nand stops once it finds a move that can reduce the current maximum KVPR.\n31\n\nC.5 SPARSE ATTENTION DESIGN\nThe research problem Desai et al. (2025) studies efficient sparse attention mechanisms that ap-\nproximate dense attention while balancing accuracy and compute efficiency. Sparse attention re-\nduces computation by restricting each query to attend to only a subset of keys, but selecting\nthis subset (i.e., the active indices) introduces a trade-off between density (fraction of active in-\ndices) and relative error (approximation quality). The objective is therefore to evolve sparse at-\ntention masks that minimize a combined loss of density and relative error, formulated as Score=\n−(Density+Relative Error).\nC.6 MULTI-AGENTSYSTEMOPTIMIZATION\nThe research problem extends the work of Cemri et al. (2025) (NeurIPS’25), which studies di-\nagnosing and repairing failures in multi-agent LLM systems (MAS), and proposes MAST as the\ntaxonomy of MAS failure modes. Such systems (e.g., MetaGPT Hong et al. (2024), ChatDev Qian\net al. (2024)) often suffer from breakdowns in coordination, memory, or communication that reduce\ntask success. The challenge is how to improve multi-agent systems, evolving more robust agent\narchitectures, prompts, and inter-agent communication patterns to increase reliability and perfor-\nmance.\nThe base program is a direct adaptation of MetaGPT Hong et al. (2024), assembled into a minimal\nPython implementation (roughly 400 LOC). This initial version defines fixed agent roles, commu-\nnication protocols, and system prompts, but frequently encounters the failure modes identified in\nthe MAST. To guide evolution, the evaluator uses the MAST annotator, which assigns a score of\n1/(1 +total FM occurrence), penalizing common coordination and memory failures.\nWe ran three evolution configurations with different mutation scopes: (1) modifying agent defini-\ntions and communication schemes, (2) introducing new verification and communication flows using\nGPT-5, and (3) allowing changes to the number and types of agents, i.e., the system topology. The\nevolved solutions introduced innovations such as improved context management (v1) and verifica-\ntion/communication flows (v2), though removing verification in v3 degraded performance. Overall,\ndownstream program development success improved from 40% in the base program to 47% (v1)\nand 53% (v2) on the ProgramDev-v1 benchmark, before dropping to 30% in v3. The fast that\nverification agent was removed in v3 was an example of reward hacking (since we penalize the\nverification failures, the evolution algorithm got rid of the whole verification when it could) and an\nexample of the importance of carefully tuning which parts of the initial code the evolution algorithm\nis allowed to change. These results show that OpenEvolve can automatically discover MAS design\nrefinements that improve robustness, though careful control over mutation scope is critical to avoid\nreward hacking.\nC.7 GPU CACHEALGORITHMOPTIMIZATION\nThis task studies optimizing cache algorithms for GPU execution. While CPU versions of the algo-\nrithm run correctly, directly adapting them to GPUs often introduces significant overhead and poor\nperformance.\nThe base program was an initial GPU implementation generated by GPT-o3. This version was\nextremely slow and failed some correctness tests, making it unsuitable for practical use. To guide\nevolution, the evaluator balanced two objectives: correctness (passing functional tests) and runtime\nspeed.\nWe ran 120 iterations of OpenEvolve using Gemini-2.5-flash, with a total runtime of about 40 min-\nutes and cost of roughly $20. The evolved program uncovered practical optimizations such as wrap-\nping key kernels withtorch.jit, which noticeably improved execution speed. However, despite\nthese gains, the final code remained slower than desired and continued to fail certain edge-case tests.\n32",
  "textLength": 123988
}