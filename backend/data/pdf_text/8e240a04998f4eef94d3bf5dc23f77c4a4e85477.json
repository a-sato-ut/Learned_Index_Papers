{
  "paperId": "8e240a04998f4eef94d3bf5dc23f77c4a4e85477",
  "title": "Evaluating Persistent Memory Range Indexes: Part Two",
  "pdfPath": "8e240a04998f4eef94d3bf5dc23f77c4a4e85477.pdf",
  "text": "Evaluating Persistent Memory Range Indexes: Part Two\n[Extended Version]\nYuliang He\nSimon Fraser University\ngeorgeh@sfu.caDuo Lu\nSimon Fraser University\nluduol@sfu.caKaisong Huang\nSimon Fraser University\nkha85@sfu.caTianzheng Wang\nSimon Fraser University\ntzwang@sfu.ca\nABSTRACT\nScalable persistent memory (PM) has opened up new opportunities\nfor building indexes that operate and persist data directly on the\nmemory bus, potentially enabling instant recovery, low latency\nand high throughput. When real PM hardware (Intel Optane Per-\nsistent Memory) first became available, previous work evaluated\nPM indexes proposed in the pre-Optane era. Since then, newer\nindexes based on real PM have appeared, but it is unclear how they\ncompare to each other and to previous proposals, and what further\nchallenges remain.\nThis paper addresses these issues by analyzing and experimen-\ntally evaluating state-of-the-art PM range indexes built for real PM.\nWe find that newer designs inherited past techniques with new\nimprovements, but do not necessarily outperform pre-Optane era\nproposals. Moreover, PM indexes are often also very competitive or\neven outperform indexes tailored for DRAM, highlighting the poten-\ntial of using a unified design for both PM and DRAM. Functionality-\nwise, these indexes still lack good support for variable-length keys\nand handling NUMA effect. Based on our findings, we distill new\ndesign principles and highlight future directions.\n1 INTRODUCTION\nThe recently commercialized persistent memory (PM) devices, rep-\nresented by Intel Optane Persistent Memory (Optane PMem) [ 16],\ndeliver persistence, high capacity, lower cost and fast speed on the\nmemory bus. There have been many PM-inspired (re)designs in\ndata-intensive systems [ 3,4,9,45,55,63,76]. In particular, much\nexciting progress has been made on devising single-level persistent\nOLTP indexes that directly operate and store data on PM without\ninvolving the storage stack, even before real PM devices became\navailable [ 2,11,12,27,40,56,64,68,71,73]. As we have shown in\nthe past [ 44], although these pre-Optane proposals do not perform\nas expected (e.g., much slower) on real PMem devices due to inaccu-\nrate assumptions and emulation, several building blocks (unsorted\nnodes, fingerprinting, hybrid DRAM-PM structures) have proved\nuseful for devising new indexes on PM.\nThe availability of real PM devices has further enabled a new\nbreed of PM-based indexes [ 13,34,46,48,49,77], which are tailor-\nmade for Intel Optane PMem. Although they appear/claim to per-\nform better than pre-Optane proposals on real PM, it remains un-\nclear (1) how they compare against each other , as these new indexes\nappeared roughly concurrently and/or were proposed by different\nresearch communities (e.g., VLDB/SIGMOD vs. SOSP/OSDI), (2) how\nthey are different from (or similar to) the pre-Optane proposals (i.e.,\nThis document is an extended version of ‚ÄúEvaluating Persistent Memory Range Indexes:\nPart Two‚Äù which will appear in Proceedings of the VLDB Endowment (PVLDB), Vol.\n15, Issue 11 and will be presented at the 48th International Conference on Very Large\nData Bases (VLDB 2022).what ‚Äúlegacy‚Äù has pre-Optane proposals left for the new breed?),\nand (3) what further challenges and opportunities remain in this area .\nThe goal of this paper is to answer these questions, which is the\nkey to pushing actual adoption of these new indexes and PM-based\nsystems in general. We do so by conducting a comprehensive eval-\nuation of five state-of-the-art PM indexes, including DPTree [ 77],\nùúáTree [ 13], LB+-Tree [ 46], ROART [ 49] and PACTree [ 34]. We also\ninclude as a reference FPTree [ 56], a representative and arguably\nthe best-performing design from the pre-Optane era [44].\nThis work can be seen as a sequel to the ‚Äúfirst episode‚Äù of our\npast work which benchmarked and distilled the aforementioned\nuseful building blocks from pre-Optane era PM indexes [ 44], but is\nnot a mere repeat of what was done before with newer indexes. On\nthe one hand, we follow the similar methodology and focus on rep-\nresentative range indexes. On the other hand, in this paper we give\na snapshot of the latest state of this area and highlight new findings,\nobservations and perspectives that were often omitted in the past,\nespecially variable-length key support, NUMA-awareness and new\npotential impact of existing PM indexes on future work. We sum-\nmarize our findings below. 1The key optimization target remains\nto be reducing read and (more often) write operations on PM to\npreserve PM bandwidth (and thus achieving higher performance).\n2As we predicted in past work, several pre-Optane techniques are\neffective on real PM hardware and widely adopted by new propos-\nals, including fingerprinting [ 56], unsorted nodes and leveraging\nDRAM. 3Although modern PM-enabled servers are often multi-\nsocket, most state-of-the-art PM indexes are still only optimized\nfor single-socket machines and consciously avoided NUMA effect\nin their experimental evaluations; handling NUMA effect remains\nan unsolved problem. 4PM programming infrastructure (e.g., PM\nallocators and runtime) is still far from ideal in many cases and\nrequires further improvements. 5Finally and perhaps most pro-\nfoundly, for the first time, we observe that techniques employed by\nPM indexes can also be useful in devising high-performance volatile\nin-memory indexes. Surprisingly, when running on DRAM without\nthe extra fences and cacheline flushes, under certain workloads a\nPM index can match or even outperform well-tuned indexes specif-\nically designed for DRAM, such as HOT [ 6] and Masstree [ 50]. This\nhighlights the potential of unifying persistent and volatile indexing\nto simplify the design and implementation of future systems.\nThe remaining sections expand on more details and insights. We\ngive the necessary background on PM in Section 2, followed by a re-\nview of pre-Optane PM range indexes in Section 3. Sections 4‚Äì5 then\nsurvey and analyze state-of-the-art PM range indexes. Sections 6‚Äì7\npresent our experimental results and observations. We present an\noutlook of future indexes in Section 8. Section 9 covers related work\nand Section 10 concludes this paper. We have open-sourced our\ncode and results at https://github.com/sfu-dis/pibench-ep2 .arXiv:2201.13047v2  [cs.DB]  27 Jul 2022\n\nYuliang He, Duo Lu, Kaisong Huang, and Tianzheng Wang\n2 PERSISTENT MEMORY\nWe briefly review the properties of PM (in particular Intel Optane\nPMem1) and its implications on software; readers already familiar\nwith the literature may skim and fast forward to Section 3.\nCollectively, ‚ÄúPM‚Äù refers to a class of devices that offer byte-\naddressability (like DRAM), high capacity and persistence (like\nSSDs) on the memory bus. They can be built using various materi-\nals and techniques, such as PCM [ 70], STT-RAM [ 24] and memris-\ntor [62]. However, only the 3D XPoint [ 17] based Optane PMem so\nfar has delivered high capacity promised by the PM vision. Thus,\nwe target Optane PMem in this paper. Optane PMem‚Äôs performance\ngenerally fall between DRAM and SSDs.2The first generation (100\nseries) exhibits‚àº300ns latency and‚àº5‚Äì40GB/s bandwidth depend-\ning on the access pattern, and with sequential/read accesses being\nfaster than random/write accesses. The latest 200 series further\ngives‚àº30% higher bandwidth [31].\nAlthough PM offers persistence, it is still behind multiple lev-\nels of CPU caches. Software normally goes through CPU caches\nto access data stored on PM using load andstore instructions,\nand explicitly issues cacheline flush instructions (e.g., CLWB and\nCLFLUSHOPT ) and fences to ensure data is correctly persisted [ 30].\nHowever, PM-capable platforms use asynchronous DRAM refresh\n(ADR) to guarantee data flushed from the CPU caches will first land\non the CPU‚Äôs write buffer, which is power failure protected. As a\nresult, a PM write is considered complete once the data is forced to\nthe ADR domain, without necessarily having arrived at physical\nPM media. More recent platforms further feature enhanced ADR\n(eADR) which also protects the CPU cache, effectively providing\ndurable CPU caches and potentially sparing the need of cacheline\nflush instructions (although fences are still needed for correct order-\ning). eADR is only available for the more recent 200-series Optane\nPMem and Skylake platforms [ 30]. Our experiments are based on\nthe 100-series PMem and Cascade Lake platform without eADR\n(detailed setups in Section 6). However, we do not expect our con-\nclusions to change based on recent results obtained from evaluating\nthe impact of eADR [ 22]. We leave it as future work to document\neADR‚Äôs detailed behavior and impact on PM range indexes.\nOptane PMem can operate under the Memory, App Direct, or\nDual modes [ 30]. The Memory mode uses the system‚Äôs DRAM as a\nhardware-controlled cache and presents bigger but volatile memory.\nThe App Direct mode enables persistence, thus allowing building\npersistent indexes. The Dual mode combines both by allowing part\nof PM to be configured for the Memory or App Direct mode. Same\nas other work, we focus on the App Direct mode as it gives software\nthe flexibility to use DRAM and PM as needed.\n3 PREVIOUSLY ON PM RANGE INDEXES\nIndexing has received much attention even before real PM was\navailable [ 2,11,12,27,40,56,64,68,73]. This section reviews pre-\nOptane designs to set the stage for our new evaluations.\n3.1 Early Assumptions in the Pre-Optane Era\nDue to the lack of real devices, early proposals had to ‚Äúguesstimate‚Äù\nthe properties of PM based on prototypes and simulations [ 39,70,\n1Also known as Optane DC Persistent Memory Module (DCPMM) [28].\n2Exceptions apply under certain workloads and hardware configurations [26].75].3Common assumptions included (1) limited write endurance,\n(2) 3‚Äì5√óhigher latency but similar bandwidth to DRAM‚Äôs [ 58,67],\n(3) writes slower than reads, (4) 8-byte atomic PM writes [ 15], and\n(5) volatile CPU caches and reorderings by the CPU. Out of these,\nthe assumption on bandwidth turned out to be inaccurate: Optane\nPMem‚Äôs lower-than-DRAM bandwidth is a major factor that limits\nperformance [ 44]. The speed gap between sequential and random\naccesses was also largely left out. Endurance so far has not been a\nmajor issue for Optane PMem which warrants virtually unlimited\nendurance during the usual replacement cycle of 3‚Äì5 years [ 28,51].\nNevertheless, these assumptions rightfully suggested that classic\nin-memory indexes would not guarantee correctness (customized\nrecovery protocols are necessary), nor perform well on PM. Con-\ncurrency control must be carefully considered given PM‚Äôs higher\nlatency. These led to the development of numerous PM indexes\neven before actual PM hardware was available. The key is reduc-\ning PM read/write operations and avoiding unnecessary cacheline\nflushes and fences to both improve performance and reduce wear.\n3.2 Presumed Designs and Building Blocks\nTo reduce PM accesses, several building blocks have been proposed\naround re-designing the tree architecture, node structure and con-\ncurrency control. Now we discuss them in the context of previously\nevaluated PM indexes (wBTree [ 12], BzTree [ 2], FPTree [ 56] and\nNV-Tree [ 73]). Table 1 lists their main design choices. The first\nthree dimensions (architecture, node structure and concurrency) re-\nceived the most attention in the past; in this work, we also consider\nvariable-length keys, PM management and NUMA-awareness.\nTree Architecture: PM-Only ‚ÜíDRAM-PM Hybrid. Most\npre-Optane proposals adapt in-memory B+-trees. Some (e.g., wB-\nTree and BzTree) place the entire tree on PM to allow instant recov-\nery. But doing so leads to much slower traversal speed compared to\nDRAM indexes. A key contribution by FPTree and NV-Tree was to\nleverage the fact that B+-tree‚Äôs inner nodes only guide search traffic\nand are reconstructible using leaf nodes. This allows improved tra-\nversal speed by loosening the consistency requirements for inner\nnodes, by omitting flushes/fences [ 73] and/or placing all the inner\nnodes in DRAM [ 56]. Upon restart, the inner nodes can be rebuilt\nusing B+-tree bulk loading algorithms. The downside is recovery\ntime can scale with data size, sacrificing instant recovery.\nNode Structure: Sorted ‚ÜíUnsorted. Traditional B+-trees\nkeep keys sorted for fast binary search. The drawback of inheriting\nthis design on PM is insertions may shift keys, causing excessive PM\nreads and writes, while having the risk of incomplete updates upon\nhardware failure. Moreover, binary search becomes less (or not\nat all) beneficial with small nodes commonly used by in-memory\nB+-trees. A popular solution is to keep nodes unsorted and use\na linear scan to retrieve the target key. Most of the surveyed pre-\nOptane indexes adopted this technique. To mitigate the impact of\nlinear search, BzTree periodically consolidates nodes to become\nsorted. FPTree proposed fingerprinting which maintains one-byte\nhashes of the keys in the node and a lookup starts by checking\nthe fingerprints. Only the keys with matched fingerprints will be\nfurther examined. This greatly reduces PM accesses, especially for\n3Various materials can be used to build PM, yet they may perform differently. This par-\ntially forced past work to make general assumptions for potentially wider applicability.\n\nEvaluating Persistent Memory Range Indexes: Part Two [Extended Version]\nTable 1: Design choices of pre-Optane PM index proposals (wBTree, BzTree, FPTree and NV-Tree) [44]. Common building\nblocks such as using DRAM to store reconstructible data (e.g., inner nodes), using unsorted nodes and fingerprinting have\nproved to be useful on real Optane PMem. The impact of variable-length keys, PM allocator and NUMA effect were largely\nunconsidered.\nArchitecture Node Structure ConcurrencyVar.\nKeysPM AllocatorNUMA-\nAware\nwBTree [12] B+-tree; PM-only Unsorted; indirection Single-threaded Pointer Emulation/PMDK No\nBzTree [2] B+-tree; PM-only Partially unsorted leafLock-free with Persis-\ntent MwCAS [68]Inlined Emulation/PMDK No\nFPTree [56]B+-tree; DRAM (inner) +\nPM (leaf)Unsorted leaf; fingerprintsHTM (inner) + lock-\ning (leaf)Pointer Customized/PMDK No\nNV-Tree [73]B+-tree; PM-only or op-\ntionally DRAM+PMUnsorted leaf; inconsistent\ninner nodesLocking Pointer Emulation/PMDK No\nnegative search where the key does not exist. Another approach is\nto keep an indirection array [ 12] that stores sorted index positions\nof keys, allowing binary search using the indirection array.\nConcurrency Control: Pessimistic ‚ÜíOptimistic. PM indexes\noften prefer lightweight concurrency control over pessimistic lock\ncoupling [ 59]. FPTree uses separate strategies for inner and leaf\nnodes: For the former it leverages hardware transactional memory\n(HTM) to reduce traversal costs, and for the latter it uses tradi-\ntional locking because inserting into PM-resident leaf nodes may\ninvolve cacheline flushes, which in turn will abort HTM transac-\ntions. BzTree uses lock-free multi-word compare-and-swap (PMw-\nCAS) [ 68] that can atomically modify multiple 8-byte words. With-\nout pessimistic locking, FPTree scales to high core count but ex-\nhibits high tail latency and low throughput under contention, due\nto HTM‚Äôs inherent limitations [ 44]. FPTree delegates the detailed\nHTM-locking interactions to Intel TBB which uses a fast-path that\nuses HTM and a slow-path that serves as a fallback when HTM\nabort rate becomes higher than a threshold (10 by default).4\n3.3 Functionality and PM Management\nNow we turn to the remaining three dimensions under considera-\ntion in Table 1: support for variable-length keys, NUMA awareness\nand PM programming infrastructure support.\nMost pre-Optane proposals [ 12,56,73] focused on handling fixed-\nlength, 8-byte keys, so were past evaluation efforts. Variable-length\nkeys are usually supported using pointers to keys stored in the (per-\nsistent) heap. Some proposals differentiate pointers from inlined\nvalues by designating a special ‚Äútype‚Äù bit in the 8-byte key area,\neffectively limiting the maximum length of keys to be 63 bits. Oth-\ners, e.g., FPTree, require compile-time customization by specifying\nwhether keys are 8-byte integers or pointers to support full 64-bit\nkeys. Pointers are used in case both types are required. As a result,\naccessing shorter keys ‚â§8 bytes is also prone to cache misses caused\nby pointer chasing. Out of the four indexes, only BzTree provides\ninlined support for variable-length keys with slotted pages.\nNone of the surveyed pre-Optane indexes handle NUMA effect.\nMost of them also do not have a well thought-out design for manag-\ning PM. FPTree uses PMDK [ 29], the current de facto standard PM\n4Details at https://github.com/oneapi-src/oneTBB/blob/v2021.5.0/src/tbb/rtm_mutex.\ncpp#L33 . Our evaluation (Section 6) sets the threshold to 256 for better performance.library, to avoid issues such as PM leaks. For better performance,\nFPTree has to use customized slabs (large chunks allocated from\nPMDK allocator) to amortize PM allocation cost. Still, our previous\nwork [ 44] has identified PM allocation in all these indexes as a main\nbottleneck that should be removed by future designs.\n4 STATE-OF-THE-ART PM RANGE INDEXES\nWe survey five representatives of recent PM indexes optimized for\nOptane PMem: LB+-Tree [ 46], DPTree [ 77],ùúáTree [ 13], ROART [ 49]\nand PACTree [ 34]. They can be categorized as B+-tree based, trie\nbased and hybrid which makes use of both B+-trees and tries.5\n4.1 B+-Tree based: LB+-Tree andùúáTree\nWe survey two representative B+-tree based PM range indexes,\nLB+-Tree [ 46] andùúáTree [ 13], which represent designs that mainly\noptimize for high throughput and low tail latency, respectively.\nLB+-Tree. As evaluated by previous work [ 33,44,72], Optane\nPMem uses 256-byte granularity internally: accesses smaller than\n256 bytes will still incur 256-byte of traffic to/from the physical\nmedia. It then becomes important to (1) coordinate PM accesses\nin 256-byte granularity and (2) reduce unnecessary PM accesses\n(to save PM bandwidth) [ 33,72]. To reach these goals, as shown\nin Tables 1 and 2, LB+-Tree starts with several useful techniques\nproposed by FPTree: fingerprinting, DRAM+PM architecture and\noptimistic concurrency with HTM and locking. Compared to FP-\nTree, it uses hand-rolled RTM transactions instead of TBB. We\nhighlight the impact of this design decision later in Section 6.\nOn top of these existing techniques, LB+-Tree proposes new ones\nto further optimize PM accesses. First, node sizes are set to align\nwith and be multiples of 256 bytes for better CPU cache and PM\nutilization. Second, to minimize PM writes during inserts, LB+-Tree\nproposes entry moving to bound the number of cacheline writes\nper insert. As Figure 1(a) shows, a 256-byte leaf node is divided\ninto four 64-byte cachelines. Upon insert, LB+-Tree first attempts\nto insert the record into Line 0 if an empty slot is available in it.\nSince the header is also in Line 0 , only one cacheline write to PM is\nneeded to persist both the record and header. Otherwise, LB+-Tree\n5In addition to traditional indexes, learned indexes [ 36] are also being adapted for\nPM [ 47]. However, they are still in very early stage. We thus leave it as future work to\nevaluate them to avoid pre-mature conclusions.\n\nYuliang He, Duo Lu, Kaisong Huang, and Tianzheng Wang\nmoves data from Line 0 to another line where the new entry is\ninserted. This proactively spares empty slots in the first cacheline,\nwhich will reduce flush operations incurred by future inserts.\nWrite-ahead logging (WAL) is widely used for crash recovery [ 29],\nbut incurs additional PM writes. LB+-Tree disposes of WAL with\nlogless node splits by storing two sibling pointers in each leaf\nnode and uses an alt bit in node header to indicate the valid pointer.\nUpon split, a new leaf node is first allocated and tracked by the\nunused pointer, followed by a redistribution of entries to the last\ntwo cachelines of the new node. Then the alt bit and bitmap in\nthe original node are updated using an 8-byte atomic PM write to\nensure the tree is always in a consistent state even across failures.\nùúáTree. Different from most PM indexes, ùúáTree mainly optimizes\nfor tail latency caused by PM‚Äôs high latency. It places B+-tree inner\nnodes in DRAM, but redesigns leaf nodes to use both DRAM and PM.\nAs shown in Figure 1(b), leaf nodes in ùúáTree consist of two layers:\nan array layer and a list layer. The former consists of traditional\nB+-tree nodes which store pointers to list layer nodes; the latter is a\nPM-resident singly-linked list where each node stores a key-value\npair.ùúáTree uses optimistic locking [ 43] for the DRAM-resident\nB+-tree and manages the PM-resident linked list in a lock-free\nmanner. To insert a record, the thread first traverses the in-DRAM\nB+-tree optimistically without holding any locks, and then inserts\na node representing the key-value pair in the linked list using the\ncompare-and-swap ( CAS) instruction [ 20,23]. It then acquires the\nlock in the corresponding leaf node in the B+-tree to insert the\nkey, which may trigger splits that will in turn acquire locks from\nthe bottom up in DRAM. This way, B+-tree structural modification\noperations (SMOs) and actual key-value inserts/removals (which\nincur PM flushes in the list layer) are decoupled, and multiple\nthreads inserting into the same leaf node could proceed in parallel\nin the list layer. This could allow more concurrency and removes\nflushes from the critical path, thus reducing tail latency.\n4.2 Trie based: ROART and PACTree\nNewer designs have also explored tries on PM. They are mostly PM-\nonly (instead of DRAM-PM hybrids) and based on ART [ 42]. We sur-\nvey two representative proposals, ROART [49] and PACTree [34].\nROART. Based on ART, ROART optimizes for range scans,\nwhich are known to be a weak point of tries [ 49]. Figure 1(c) shows\nthe overall architecture of ROART. It is purely on PM but selectively\npersists metadata entries and reconstructs the inconsistent ones\nupon recovery (similar to NV-Tree‚Äôs selective consistency [ 73]).\nTo optimize scan, ROART compacts small sub-trees ( <64 entries)\ninto leaf arrays that store pointers to records. This reduces pointer\nchasing overhead during scan and makes the index shallower. The\ndownside is splits become expensive, as all the keys in a leaf array\nare divided into subsets based on the first differentiating byte, fol-\nlowed by a node allocation for each subset. This requires more PM\nwrites and fences, and puts higher pressure on the PM allocator.\nTo this end, ROART reduces the number of fences by relaxing the\norder of split steps and using a depth field to detect and resolve\ninconsistency. It also proposes delayed check memory management\n(DCMM) to cope with the high PM allocation demand. DCMM per-\nforms fast allocations using thread-local pools but delays garbage\ncollection by traversing the entire index at a later time, potentially\nH09134812116351471210S0S1headerH09134812116351471210line0line1line2line3H1‚Ä¶1st256Bleafnodem-th256BleafnodeDRAM256B inner nodes...‚Ä¶S0PMmx256BleafnodessiblingpointersDRAMarray layerlist layerkeyB+-tree(a)LB+-Tree...ptrkv...nextPM\ndata nodeversionptr011664(b)¬µTreeinner nodes\nbitmapptr64 pointer slotsfingerprintaddress64160ARTPM‚Ä¶\n.........leaf array...kv(c)ROARTB+-tree\n...\n(d)PACTreePMART\n(e)DPTreeDRAM\nPMsearch layer......data layeranchor key  deleted  next ptr  prev ptr  bitmapfingerprint arrayversion lock (not persisted)kvarraypermutation array version (not persisted)permutation array (not persisted)\nfront buffer treemiddle buffer treebase tree(read-only)(read-only)query\ntailqueryinsert/delete/update...B+-treeB+-treetrieleaflayerparallelmergelogheadertable...partitioned......headvalidinvalidappendmeta0meta1kv1kvn0globalversioncrash-consistentreconstructible...querydeletedFigure 1: Architecture of five state-of-the-art PM indexes.\nincreasing PM usage. Since ROART is fully in PM, it supports true\ninstant recovery, and natively supports variable-length keys with-\nout extra pointer chasing using its trie-based design.\nPACTree. PACTree is a PM-only two-layer persistent trie with\nB+-tree styled leaf nodes. As shown in Figure 1(d), PACTree consists\nof a search layer and a data layer. The search layer is a durable trie\nbased on concurrent ART that uses read-optimized write exclusion\n(ROWEX) [ 43]. The data layer is a doubly-linked list of B+-tree leaf\nnodes, each of which contains 64 key-value pairs and an anchor key\nto indicate the smallest key in the node. PACTree stores fingerprints\nand indirection arrays in leaf nodes to facilitate search and scan, but\nthey are not persisted to reduce PM writes. Upon split, the target\nleaf node in the data layer is first locked, and then a log entry is\nwritten to a per-thread SMO log in PM. The thread then splits the\nleaf node and commits without modifying inner nodes in the search\nlayer. A background thread will then finish the remaining SMO in\nthe search layer. This allows worker threads to commit early right\nafter modifying leaf nodes. However, it also creates inconsistencies\nbetween the search and data layers. Thus, query threads may need\n\nEvaluating Persistent Memory Range Indexes: Part Two [Extended Version]\nTable 2: Main design choices of state-of-the-art PM range indexes (LB+-Tree, DPTree, ùúáTree, ROART and PACTree). They\nbase on Intel Optane PMem and inherit designs from pre-Optane proposals, by using DRAM (sometimes more aggressively),\nlightweight concurrency control and unsorted nodes. Some also advocate customized PM allocators to reduce PM accesses.\nVariable-length keys and NUMA-awareness are still less considered.\nArchitecture Node Structure ConcurrencyVar.\nKeysPM AllocatorNUMA-\nAware\nLB+-Tree [46]B+-tree; DRAM (inner) +\nPM (leaf)Unsorted leaf; fingerprints;\nextra metadataHTM (inner) + lock-\ning (leaf)Pointer Customized/PMDK No\nùúáTree [13]B+-tree; DRAM (B+-tree)\n+ PM (linked list)SortedLocking (array layer)\n+ lock-free (list layer)Pointer PMDK No\nDPTree [77]Hybrid; DRAM (B+-tree,\ntrie inner) + PM (trie leaf)Unsorted leaf; fingerprints;\nindirection; extra metadataOptimistic lock [ 43] +\nasync. updatesPointer PMDK No\nROART [49] Trie; PM-onlyB+-tree like unsorted leaf;\nfingerprintsROWEX [43] Inlined Customized/PMDK No\nPACTree [34]Trie; PM-only or option-\nally DRAM+PMUnsorted leaf; fingerprints;\nindirectionOptimistic lock [ 43] +\nasync. updateInlined Customized/PMDK Yes\nto perform a ‚Äúlast-mile‚Äù search after arriving at the data layer to find\nthe correct leaf node using anchor keys. PACTree is the only index\nthat mitigates NUMA effect. It uses separate pools for the search\nlayer, data layer and logs in each NUMA node. It also advocates\nthe use of snooping-based CPU coherence protocols (instead of the\ndefault directory-based protocol on most platforms) to avoid poor\nperformance when PM accesses cross NUMA boundaries.\n4.3 Hybrid: DPTree\nAs shown in Figure 1(e), DPTree is a PM-DRAM hybrid index that\ncombines up to two B+-trees (a front and a middle buffer tree) in\nDRAM, a trie ( base tree) that places inner nodes in DRAM and\nleaf nodes in PM. To search for a key, DPTree first visits the front\nbuffer tree. If the target key is not found, the middle buffer tree (if\nexists) will be further searched. If the key does not exist in the buffer\ntrees, the base tree will have to be searched. To reduce unnecessary\ntraversals, DPTree maintains a bloom filter per buffer tree. For\nrange queries, DPTree has to search and merge results from all the\ntrees. When the size ratio between the front buffer tree and base\ntree reaches a pre-defined threshold, DPTree creates a new front\nbuffer tree and turns the previous front buffer tree into a middle\nbuffer tree. Tree merge operations are triggered when the size ratio\nbetween the front buffer tree and the base tree reaches a pre-defined\nthreshold, and are performed by background threads. Using the\nversion number and extra set of metadata, DPTree ensures changes\nare invisible to concurrent queries when a merge is in progress.\nAfter merging, the middle buffer tree is destroyed and the inner\nnodes of the base tree (ART) are rebuilt. Then the global version\nbit is flipped to expose changes to incoming requests. DPTree also\nuses selective metadata persistence with reconstructible metadata\n(e.g., record count and fingerprints) in DRAM.\n5 ANALYZING THE STATE-OF-THE-ART\nWith the high-level designs laid out, now we analyze the new PM\nindexes in detail and distill common building blocks which can\nbe useful for future PM indexes. We analyze them from the six\ndimensions in Table 2, followed by empirical evaluation in Section 6.5.1 Index Architecture\nNew PM indexes often inherit the PM+DRAM architecture with\nnew optimizations, and consider tries and hybrid structures.\n(More Extensive) Use of DRAM. New PM indexes based on\nB+-tree and hybrid structures (LB+-Tree, DPTree and ùúáTree) con-\ntinue to use DRAM to store part of the index. We also observe more\naggressive use of DRAM, e.g., DPTree and ùúáTree place entire tree\nstructures in DRAM to get more performance gains. The tradeoffs\nare (1) longer recovery time, (2) more complex programming and\n(3) higher DRAM consumption which we quantify in Section 6.\nBeyond B+-Trees and Monolithic Indexes. New PM indexes\nalso adapt tries (PACTree and ROART), but they default to pure\nPM designs, potentially leading to suboptimal performance but\npreserving instant recovery. Notably, PACTree stores keys in both\ninner and leaf nodes, but it is possible to place inner nodes in\nDRAM to improve performance. We expect to see more B+-tree\nbased PM indexes utilize DRAM for faster access/write speed, as PM\nservers will still feature DRAM in the foreseeable future.6As a major\ndeparture from just adapting one type of data structure, DPTree\ncombines B+-trees and tries. When it comes to node structure, new\nindexes are also introducing new designs that no longer use pure\ntrie or B+-tree nodes, which we highlight next.\n5.2 Node Structure\nNew PM indexes base their designs on Optane PMem with node\nalignment of 256 bytes to reduce unnecessary PM accesses. Several\npre-Optane designs‚Äîfingerprinting, unsorted (leaf) nodes and se-\nlective consistency for metadata‚Äîcontinue to be used by new PM\nindexes. But they are further optimized with new techniques.\nFingerprinting on Steroids. As PM accesses are slow, finger-\nprinting becomes the most popular approach used by four out of the\nfive new PM indexes. Meanwhile, new techniques are introduced\nto better store and use fingerprints. LB+-Tree uses SIMD instruc-\ntions to compare up to 64 one-byte fingerprints in one instruction.\n6DRAM must be present for PMem to work, even if the software does not need it [ 26].\n\nYuliang He, Duo Lu, Kaisong Huang, and Tianzheng Wang\nROART embeds a two-byte fingerprint inside pointers to key-value\npairs, minimizing pointer chasing at the leaf level.\nExtra and Selectively Persisted Metadata. LB+-Tree and DP-\nTree both use an extra set of metadata per leaf node to avoid logging\n(thus reducing PM writes). For LB+-Tree, this allows it to achieve\nlogless split. DPTree uses the extra metadata to track PM allocations\nand hide incomplete changes during tree merge operations.\nNot all metadata entries have to be persisted when modified. For\ninstance, version locks in PACTree leaf nodes are only meaningful\nat runtime; fingerprints and indirection arrays can be rebuilt during\nrecovery (DPTree and PACTree) or on demand at runtime by query\nthreads (ROART). Keeping them volatile can significantly reduce\nPM writes and improve performance, at the cost of slower recovery.\nHybrid Leaf Nodes. All the surveyed PM indexes that adopt trie\n(DPTree, ROART and PACTree) use B+-tree like leaf nodes where a\nnode stores multiple records to reduce insert overhead, as the leaf\nnode is only split when it is full. This design also reduces pressure on\nthe PM allocator, as trie-based indexes typically incur more frequent\nallocations with varying node sizes compared to B+-trees. Scan\nperformance is also improved with less pointer chasing. Different\nfrom other proposals, ùúáTree introduces a linked list layer in PM,\nmaking its ‚Äúleaf node‚Äù logical. Although this design decouples SMOs\nand data movement to potentially enable more parallelism, it adds\nmore overhead to scans due to more pointer chasing.\n5.3 Concurrency Control\nAll the surveyed new PM indexes use optimistic concurrency con-\ntrol. They optimize traversals using lock-free read or HTM for inner\nnodes; locks are only acquired at the leaf level and/or as needed in\ninner nodes to reduce PM writes. Further, they tend to use back-\nground threads for SMOs (e.g., PACTree and DPTree). The benefit\nof offloading SMOs to the background is potentially lower latency\nfor index operations. However, it can be tricky to determine the\nappropriate number of background threads. Also, with a given CPU\nbudget (e.g., in the cloud), the machine may not have enough re-\nsources to spare for the background threads, which may then fall\nbehind the foreground threads and affect the overall progress.\n5.4 Functionality and PM Management\nAs Table 2 lists, among the new PM indexes, only trie-based ROART\nand PACTree natively support variable-length keys; the others\nfollow pre-Optane proposals to use pointers to keys. With real\nhardware and libraries like PMDK, all the indexes have taken into\naccount PM management issues (e.g., avoiding persistent leaks and\noptimizing allocation performance). However, many need a cus-\ntomized allocator for performance reasons. Finally, only PACTree\nis designed to mitigate NUMA effect. Other indexes would have to\nuse general-purpose approaches [ 66] that can be applied on any\nPM index, but they come with limitations (e.g., focus on certain\nworkloads). Such facts indicate that in terms of functionality, new\nPM indexes have been mainly sticking with the status quo.\n6 EVALUATING THE STATE-OF-THE-ART\nNow we empirically evaluate new PM indexes and compare them\nwith FPTree, the best-performing pre-Optane PM range index.6.1 Experimental Setup\nWe run experiments on a 40-core (80-hyperthread) server equipped\nwith two Intel Xeon Gold 6242R CPUs clocked at 3.10 GHz with\n36MB of cache. The server is fully populated with 12 √ó32GB DRAM\nDIMMs (384GB in total) and 12 √ó128GB Optane PMem DIMMs\n(1.5TB in total) for maximum bandwidth. Both DRAM and PMem\nrun at 2666MT/s.7The server runs Arch Linux with kernel 5.14.9.\nAll the code is compiled using GCC 11.1 with all the optimizations.\nUnless otherwise specified, we use PMDK [ 29]/jemalloc [19] for\nPM/DRAM allocations.\nBenchmarking Framework. We use PiBench [ 44], a unified\nframework for benchmarking PM indexes, to stress test the in-\ndexes. PiBench generates and issues synthetic workloads of given\ndistributions that consist of user-specified operations (lookup/in-\nsert/update/delete/scan). It requires each index implement a set of\ncommon interfaces, by extending an abstract C++ class. We create a\nwrapper for each index that uses PiBench‚Äôs interfaces to invoke the\nindex‚Äôs internal operations. The wrapper is compiled as a shared\nlibrary and loaded into PiBench‚Äôs address space at runtime.\nMetrics and Workloads. We measure throughput (operations\nper second) and latency at various thread counts. Using PiBench,\nwe collect statistics such as cache misses and bandwidth to aid anal-\nysis. We test both fixed-length (8-byte) integer and variable-length\nkeys, following the setups used by previous work [ 44]. For point\nqueries we use keys chosen randomly under uniform or skewed\ndistributions; for range scans, we uniform randomly choose a start\nkeyùêæand scan 100 records following ùêæ. We prefill each index with\n100 million key-value pairs, after which we start to run individual\nand mixes of index operations. The results across runs vary little\nand so we report the average of three 10-second runs.\n6.2 Index Implementations and Parameters\nFor all the indexes (except FPTree which we had to implement), we\nuse the original authors‚Äô code obtained from their public reposito-\nries. We use parameters that lead to the best performance (described\nbelow) and make necessary changes to each index for correctness,\nfunctionality and fairness.\nLB+-Tree . (1) The original insert and delete functions do not\nguarantee persistence; we followed the authors‚Äô suggestions to\nfix them.8(2) We implemented range scan (similar to FPTree) and\nupdate with locking in leaf nodes. (3) We found the PMDK allocator\ncan provide sufficient performance after tuning PMDK parameters\nand allocating 256-byte aligned leaf nodes, so we also use PMDK\nfor LB+-Tree. The inner/leaf nodes contain 15/14 entries (256-byte).\nùúáTree . Instead of using chunk-based allocation, the original\ncode in fact uses PMDK‚Äôs POBJ_ZALLOC . We changed it to use\npmemobj_alloc for much better performance. We were not able to\nverify the correctness of multi-threaded inserts (with keys missing\nafter successful inserts), so we only include ùúáTree in single-threaded\nexperiments. Inner/leaf node sizes are both set to 29 entries.\nROART . The open-source code of ROART supports both PMDK\nand its customized DCMM allocator [ 49]; we present numbers under\nboth allocators. Leaf node size is set to 64 entries.\n7DRAM has to be clocked down from 3200MT/s to 2666MT/s for PMem to work [ 28].\n8Details at https://github.com/schencoding/lbtree/issues/2 .\n\nEvaluating Persistent Memory Range Indexes: Part Two [Extended Version]\nPACTree . We use PACTree‚Äôs own NUMA-aware PM allocation.\nFor fair comparison, we pin the background and worker threads to\nthe same CPU cores so that all the indexes use the same amount of\nCPU cores. Node size is set to 64 entries.\nDPTree . The original code misses PM allocator support, so we\nported it to use PMDK. We follow the original paper to use an equal\nnumber of worker and merge threads. For fair comparison, we also\npin the merge and worker threads to the same cores (similar to\nPACTree‚Äôs setup). Inner and leaf nodes contain 31 entries for buffer\ntrees; the base tree uses 256-entry leaf nodes.\nFPTree . Since the original implementation is proprietary, we\nimplemented FPTree by strictly following the paper.9We have\nverified that our implementation performs similarly to the original\nauthor‚Äôs binary does. We follow past work‚Äôs recommendations to\nset inner/leaf node sizes to 128/64 entries [44, 56].\n6.3 Single-threaded Performance\nWe begin with single-threaded experiments to avoid concurrency\ncomplicating our analysis. We run lookup/insert/update/scan opera-\ntions under the uniform random distribution and report throughput.\nPoint Queries. LB+-Tree, DPTree and ùúáTree perform similarly\nfor lookups in Figure 2(a). They are up to ‚àº2√ófaster than FPTree,\nROART and PACTree, which also perform similarly. Overall, DP-\nTree performs the best under single-threaded lookups, largely be-\ncause of its extensive use of DRAM: if the search key is in the\nDRAM-resident buffer tree, the entire query can finish without ever\naccessing PM. If the base tree needs to be visited, only the search\nin leaf node will incur PM access, which is mitigated by binary\nsearch. The other two faster indexes (LB+-Tree andùúáTree) also\nbenefit from placing inner nodes and the array leaf layer in DRAM.\nLB+-Tree also extensively uses SIMD instructions and prefetching,\nwhich as shown in Figure 3(a) drastically reduces cache misses\nand our factor analysis showed that prefetching alone improves\nperformance by‚àº10%. For inserts, B+-tree based FPTree/LB+-Tree\nand hybrid DPTree are up to ‚àº2√ófaster than trie-based ROART and\nPACTree in Figure 2(b). ùúáTree‚Äôs use of linked lists in the leaf level\ncancels out some of B+-tree‚Äôs advantages due to high cache miss\nrates in Figures 3(b‚Äìc). The performance of PM allocators is critical\nfor ROART as for each update it needs to allocate a new PM block,\nand using its own DCMM can double the throughput for updates\nin Figure 2(c). Compared to lookups, updates incur additional PM\nwrites to update the payload, but will not trigger SMOs compared\nto inserts. As expected, the update performance of all indexes falls\nbetween their lookup and insert performance, but follows the trend\nof insert performance more closely because (1) PM exhibits higher\nwrite latency, and (2) similar to inserts, leaf-level locks can only\nbe released after the new value is flushed, adding delays (although\nbeing a constant amount of overhead under a single thread).\nRange Scans. As Figure 2(d) shows, range scan performance\ndepends largely on the cost of scanning within and across leaf nodes,\ni.e., whether the nodes are sorted and they are big or small. Although\nDPTree needs to search multiple trees and combine results, it is\nstill the fastest. DPTree‚Äôs leaf nodes use indirection arrays, so the\nresult can be returned directly without sorting, as oppose to trees\nthat use unsorted nodes, e.g., FPTree. LB+-Tree inherited a lot from\n9Code available at https://github.com/sfu-dis/fptree .\n(a) Lookup00.30.60.91.21.5Throughput (Mop/s)(b) Insert00.30.60.91.21.5\n(c) Update00.30.60.91.21.5\n(d) Scan00.10.20.3FPTree\nLB+-TreeROART-PMDK\nROART-DCMMDPTree\nPACTree¬µTreeFigure 2: Single-threaded throughput (uniform distribution).\nOverall, DPTree and LB+-Tree perform the best. FPTree can\nbe very competitive to (or even better than) newer indexes.\nFPTree, but is‚àº30% slower than FPTree for scans, because its leaf\nnodes are smaller (14 entries). To scan for the same number of\nrecords, compared to FPTree which uses 64-entry nodes, more leaf\nnodes have to be visited by LB+-Tree, causing more cache misses\nin Figure 3(d). This result highlights the tradeoff between hardware\nconsciousness and optimization goals: using small nodes allows\nLB+-Tree to perform well in point queries, but can penalize scans.\nAll the tested indexes except ROART first copy the scan results\nto an array and then optionally sort them before they are returned\nto the user. ROART, however, first returns an array of leaf point-\ners without copying. Moreover, to support variable-length keys,\nit stores in leaf nodes pointers to keys. This mandates the sorting\npass to dereference pointers to keys, causing extra cache misses.\nWe note that as an optimization, if key size is known, one may\nchange ROART to copy keys first, which in our tests can double the\nperformance at the cost of supporting variable-length keys. Finally,\nùúáTree exhibits low scan performance because every record is stored\nin a linked list node, traversing them results in many cache misses.\nSummary. The most effective technique to achieve high perfor-\nmance remains leveraging DRAM, which newer PM indexes adopt\nmore aggressively, by putting more components or even complete\ntrees in DRAM. This is at the cost of higher memory consumption,\nmore complex recovery protocol and higher cost of ownership. Im-\nportantly, FPTree remains very competitive. PACTree and ROART\nare only marginally faster than FPTree for lookups. In Figure 2(b),\nPACTree, ROART and ùúáTree are even slower than FPTree. Only\nDPTree and PACTree perform better than FPTree for scans.\n6.4 Multi-threaded Experiments\nNow we measure index throughput with different thread counts\nunder uniform and skewed distributions. We start with one socket\nand expand to NUMA with two sockets in Section 6.7.\nIndividual Operations. Figures 4(a‚Äìd) show the throughput\nof lookup/insert/update/scan under the uniform distribution; the\nshaded areas (over 20 threads) indicate numbers obtained when\nhyperthreads are also used. All indexes scale well for pure lookups\nin Figure 4(a), with DPTree and LB+-Tree achieving higher raw\nthroughput than others. This result aligns with that obtained in\nSection 6.3 under a single thread. DPTree uses optimistic lock cou-\npling for its buffer/base trees, and readers can traverse without\nincurring PM accesses. Its bloom filter also helps avoid unnecessary\nlookups in the buffer trees. LB+-Tree uses HTM which without\nwrite operations exhibits little/no aborts.\n\nYuliang He, Duo Lu, Kaisong Huang, and Tianzheng Wang\n(a) Lookup(1t)02468L3 Cache Miss/op (b) Insert(1t)036912\n(c) Update(1t)02468\n(d) Scan(1t)0306090120\n(e) Lookup(20t)02468\n(f) Insert(20t)01234\n(g) Update(20t)02468\n(h) Scan(20t)020406080FPTree LB+-Tree ROART-PMDK ROART-DCMM DPTree PACTree ¬µTree\nFigure 3: Last-level cache misses per operation under a single (a‚Äìd) and 20 threads (e‚Äìh).\n110203040\n# of threads\n(a) Uniform Lookup012243648Throughput (Mop/s)\n110203040\n# of threads\n(b) Uniform Insert05101520\n110203040\n# of threads\n(c) Uniform Update05101520\n110203040\n# of threads\n(d) Uniform Scan02468\n110203040\n# of threads\n(e) Skewed Lookup014284256\n110203040\n# of threads\n(f) Skewed Update05101520\n110203040\n# of threads\n(g) Skewed Scan02468\nFPTree LB+-Tree ROART-PMDK ROART-DCMM DPTree PACTree\nFigure 4: Throughput under uniform (a‚Äìd) and skewed (e‚Äìg, self-similar with 80% accesses on 20% of keys) distributions.\nFor inserts, although LB+-Tree does not perform the best un-\nder a single thread, it scales the best under multiple threads, by\nbeing 1.55√ó/1.96√ó/3.07√ó/2.23√ófaster than ROART-DCMM/ROART-\nPMDK/DPTree/PACTree. Although LB+-Tree inherits many designs\nfrom FPTree, its logless split, new node layout and inner node locks\nto avoid re-traversals during split further make it 2.09 √ófaster than\nFPTree. DPTree‚Äôs performance stops scaling beyond 10 threads,\nmainly due to its 7-phase merge: each time a merge occurs, records\nin the middle buffer tree will be moved into the base tree, causing\nthe base tree‚Äôs inner nodes to be rebuilt. This in turn incurs high\ngarbage collection costs. ROART-DCMM achieves 1.27 √óhigher\nperformance using its customized PM allocator compared to us-\ning PMDK. Not leveraging DRAM also contributes to its lower\nperformance compared to others that do leverage DRAM.\nFor updates, LB+-Tree outperforms others in Figure 4(c), thanks\nto its fast traversal and node layout design. DPTree is also very com-\npetitive, as updates are served in-place without triggering merge\noperations. Similar to the single-threaded results, for all indexes,\nupdates behave more similarly to inserts (than lookups) as leaf-level\nlocks must be retained until the new value is flushed.\nAs shown in Figure 4(d), under multiple threads the relative\nmerits of different indexes on range scan are similar to the single-\nthreaded results. The only exception and best performing index is\nPACTree. It scales to 40 threads, thanks to the combination of (1) its\nleaf node design that inlines key-value pairs and leverages PM‚Äôs fast\nsequential read, (2) indirection that avoids sorting, and (3) optimistic\nconcurrency that incurs no PM writes for reads. The other trie-based\nROART performs the worst although it specifically optimizes for\nscan since it does not use DRAM and incurs more cache misses\n(Section 6.3); under high core counts, cache misses are further\nexacerbated in Figure 3(h). For B+-tree variants, LB+-Tree performs\nmuch worse for scans due to its use of small nodes (more cache\nmisses). In contrast to the single-threaded results, FPTree performs\npoorly: using larger leaf reduces cache misses, but increases lock\n1 10 20 30 40\n(a) Read Heavy09182736Throughput (Mop/s)\n1 10 20 30 40\n(b) Balanced06121824\n1 10 20 30 40\n(c) Write Heavy05101520\nFPTree\nLB+-TreeROART-PMDK\nROART-DCMMDPTree\nPACTreeFigure 5: Throughput of mixed workloads (lookups + in-\nserts) under uniform distribution.\ncontention on leaf nodes. DPTree takes no locks for scans (OLC), but\nneeds to visit multiple indexes and merge results, which contributes\nto its lower performance than PACTree.\nSkewed Accesses. Under the self-similar distribution where\n80% of accesses are focused on 20% of all the keys [ 21], lookups\nas shown in Figure 4(e) exhibit a similar trend to uniform distribu-\ntion but with higher raw throughput because of better CPU cache\nutilization (the working set is smaller). For updates in Figure 4(f),\nDPTree remains scalable as updates are all performed in the DRAM\nbuffer tree. FPTree shows unstable and low throughput for updates\ndue to frequent HTM aborts and more PM writes caused by SMOs\nduring update: updates to the same record are appended without\ndeduplication, so the node can become full and get split during\nan update. No index scales under update workloads with hyper-\nthreading. Under contention, locking takes over to become the main\nbottleneck in FPTree, despite the working set is smaller.\nLike FPTree, LB+-Tree also uses locking for leaf nodes, but scales\nunder scan because of their different ways of using HTM: FPTree\ndelegates HTM and locking to TBB (Section 3.2) which has a global\nfallback path after a pre-defined number (256) of aborts of HTM\ntransactions, whereas LB+-Tree directly uses HTM instructions\n\nEvaluating Persistent Memory Range Indexes: Part Two [Extended Version]\nmin 50% 90% 99%99.9%99.99%99.999%\n(a) Lookup (1t)0246810Latency ( ¬µs)\nmin 50% 90% 99%99.9%99.99%99.999%\n(b) Insert (1t)0510152025\nmin 50% 90% 99%99.9%99.99%99.999%\n(c) Scan (1t)01938577695\nmin 50% 90% 99%99.9%99.99%99.999%\n(d) Lookup (20t)0246810\nmin 50% 90% 99%99.9%99.99%99.999%\n(e) Insert (20t)03691215\nmin 50% 90% 99%99.9%99.99%99.999%\n(f) Scan (20t)01530456075\nFPTree LB+-Tree ROART ROART-DCMM DPTree PACTree ¬µTree\nFigure 6: Tail latency of PM indexes under uniform distribution and a single thread (a‚Äìc) and 20 threads (d‚Äìf).\n(e.g., xbegin /xend ). Although scan is read-only, the first leaf node\nlock (in PM) is acquired inside the HTM transaction at the end of tra-\nversal. This incurs much contention under skewed workloads and\ntriggers HTM aborts, leading FPTree to use the slow fallback path\nprotected by a global mutex. DPTree‚Äôs optimistic locks allow high\nscan throughput under skewed accesses as no locks are acquired.\nPACTree scales with the best performance under skewed accesses\nfor scans, although the gain diminishes with more hyperthreads.\nROART scales slightly worse in skewed update: for each update it\ncreates a new leaf and replaces the original leaf pointer inside the\nleaf array, which becomes more expensive under contention.\nMixed Workloads. We test mixed workloads with different\nread/insert ratios: read heavy (90% lookups + 10% inserts), balanced\n(50% lookups + 50% inserts) and write heavy (10% lookups + 90%\ninserts). As Figure 5 shows, LB+-Tree exhibits the best performance\nand scalability. DPTree scales worse with more inserts as tree merge\nbecomes a major overhead. Finally, FPTree again remains very\ncompetitive with ROART, PACTree and DPTree.\n6.5 Tail Latency\nWe measure tail latency using the same approach from previous\nwork to strike a balance between overhead and accuracy [ 44]. In\ndetail, we sample 10% of all the operations during each run under\nuniform distribution to rule out the impact of CPU caches. Figure 6\nshows the tail latency at varying percentiles under one thread\n(a‚Äìc) and 20 threads (d‚Äìf). As expected, we observe no obvious\ndifferences between one and 20 threads for lookups.\nFor inserts, lookups and scans in Figures 6(a‚Äìc), ùúáTree shows\nconsistently higher latency (and skyrockets at 99.99% for inserts),\nalthough its key design goal is to reduce tail latency (Section 4.1).\nWe observe the reason is in its use of PM-resident linked lists with\nper-record nodes. To handle an insert, ùúáTree uses‚àº2900 cycles\nto allocate a list node and ‚àº2200 cycles to complete a CASand\ncacheline flush to insert the allocated node into the linked list. In\ncontrast, LB+-Tree only needs around 80 cycles to insert a key into\na leaf node. Moreover, the use of linked lists in the leaf layer causes\nmany cache misses during scans: as shown in Figure 3(d), ùúáTree\nexhibits the highest cache miss ratio. ROART-DCMM exhibits lower\nlatency than ROART-PMDK for inserts, thanks to the better DCMM\nallocator: in the worst case, a split in ROART could allocate 63 leaf\narrays and one inner node. ROART has relatively higher latency\nfor scans in Figures 6(c) and 6(f), due to pointer chasing at the leaf\nlevel to dereference pointers to keys. The other trie-based PACTree\n110 20 30 40\n# of threads\n(a) Lookup06121824Throughput (Mop/s)\n110 20 30 40\n# of threads\n(b) Lookup06121824\n110 20 30 40\n# of threads\n(c) Lookup06121824\n110 20 30 40\n# of threads\n(d) Lookup02468\n110 20 30 40\n# of threads\n(e) insert\nsynthetic036912\n110 20 30 40\n# of threads\n(f) Insert\nnames036912\n110 20 30 40\n# of threads\n(g) Insert\nwiki036912\n110 20 30 40\n# of threads\n(h) Insert\nuk-200502468\nFPTree LB+-Tree ROART-PMDK ROART-DCMM DPTreeFigure 7: Throughput under variable-length keys using syn-\nthetic and real-world datasets [61].\ndirectly stores keys in leaf nodes, hence exhibiting lower latency.\nPACTree also shows relatively low latency for all operations in\nmost cases (except inserts under 20 threads) because SMOs are\noffloaded to background threads. Under 20 threads, the background\nthreads start to fall behind, requiring worker threads to traverse\nextra nodes to reach the correct leaf node, thus increasing latency.\n6.6 Support for Variable-Length Keys\nNow evaluate variable-length key support. We first run the same\nexperiment as Section 6.4 with 8-byte keys, but force the indexes\nto use their variable-length key support. This may limit the depth\nof trie-based indexes (thus giving them advantages), but allows us\nto reason about the efficiency of the variable-length key support\nby comparing with the fixed-length key experiments; we use three\nreal-world datasets later. For most indexes, including FPTree, LB+-\nTree and DPTree this means allocating the key in the heap and\nstoring a pointer to the key in the index. ROART uses its own\nnative support to store keys in index nodes themselves. All indexes\nare plotted except PACTree.10As shown in Figures 7(a) and 7(e),\nROART performs the best in all cases. This is exactly opposite to\nthe case using fixed-length keys (cf. Figure 4). In particular, cache\n10PACTree assumes null-terminated strings. This is incompatible with PiBench which\nmay generate keys with \\0, which will be wrongly treated as short keys.\n\nYuliang He, Duo Lu, Kaisong Huang, and Tianzheng Wang\nmisses caused by pointer chasing (to access keys) dominate the\nperformance of FPTree, LB+-Tree and DPTree.\nWe further test the indexes with three representative real-world\ndatasets [ 61]: Reddit usernames ( names ) [52], Wikipedia ( wiki ) [69]\nand URLs ( uk-2005 ) [38]. They respectively consist of string keys\nof up to 25, 256 and 2029 bytes; for space limitation we omit more\ndetails about the datasets which can be found in the Appendix A. As\nshown in Figures 7(b‚Äìd) and 7(f‚Äìh), the gaps between ROART and\nFPTree/LB+-Tree shrink, although ROART remains advantageous.\nWith longer keys, trie-based indexes will build more inner nodes to\nform deeper traversal paths, while the depth of B+-tree variants is\nnot affected as they store pointers to keys. Among all the indexes,\nDPTree performs much worse with longer keys. We found a main\nreason is that most lookups (94% according to our profiling results)\nneeded to traverse the base tree and search leaf nodes. Since DPTree\nmaintains multiple indexes, it also requires more complex traversal\nlogic that incurs extensive key comparisons (on average 34 memcmp\ncalls vs. 22 in FPTree), further lowering its performance.\nOverall, these results highlight the need to enhance variable-\nlength key support in future PM indexes, for example by combining\nthe best of tries and B+-trees without tradeoffs.\n6.7 Impact of NUMA Effect\nNow we extend our experiments to use both NUMA nodes on the\nserver. Except for PACTree, we allocate PM and DRAM from the\nfirst socket. All the threads are pinned, and we first use all the 40\nphysical cores across two sockets, before using hyperthreading\nbeyond 40 threads. This allows us to stress the indexes with inter-\nsocket traffic and contrast their behavior with and without NUMA\neffect. For PACTree, we include two variants which respectively\nenable and disable its NUMA-aware per-node PM pools.\nAs shown in Figure 8, NUMA effect has major impact on all\nindexes‚Äô throughput, and no index scales well beyond one socket\nfor all operations. Although not specifically designed for NUMA,\nLB+-Tree achieves the best scalability for lookups, and the highest\nthroughput for inserts and updates (with a dropping trend beyond\none socket). We attribute the reason to its frugal use of cacheline\nflushes and careful node layout designs. Both reduce PM accesses\nand cross-socket traffic. For lookups, FPTree also does not collapse,\nwhereas the performance of other indexes fluctuate and/or drop\nbeyond 20 threads. This implies HTM is robust to NUMA effect for\nread-only workloads, thanks to its lightweight conflict detection\nmechanism that piggybacks on the coherence protocol. Moreover,\nHTM can use the extra last-level cache in the second NUMA node\nto track reads [ 8,57]. Other approaches (OLC, locking, ROWEX)\nare unable to take good advantage of the coherence protocol like\nHTM, contributing to more severe NUMA effect.\nPACTree is the only PM index that takes NUMA effect into ac-\ncount by (1) using separate PM pools for each NUMA node and\n(2) leveraging snooping coherence protocol. With separate PM\npools ( PACTree-NUMA ), PACTree maintains performance beyond\none socket without collapsing, but still does not scale as expected.\nThe main culprit is the directory-based coherence protocol that in-\ncurs additional PM accesses to update the directory. Thus, PACTree\nadvocates using snooping protocols for PM, which broadcasts co-\nherence traffic across all cores, instead of using a directory to record\n11020304080\n# of threads\n(a) Lookup010203040Throughput (Mop/s)\n11020304080\n# of threads\n(b) Insert0481216\n11020304080\n# of threads\n(c) Update05101520\n11020304080\n# of threads\n(d) Scan0246\nFPTree\nLB+-TreeROART-PMDK\nROART-DCMMDPTree\nPACTreePACTree-NUMAFigure 8: Impact of NUMA effect for PM indexes. No index\nscales well under all operations due to additional PM ac-\ncesses by the directory-based CPU coherence protocol.\nTable 3: PM and DRAM consumption (GB) after loading 100\nmillion records with 8-byte keys and 8-byte values.\nFPTree LB+-Tree ROART DPTree PACTree ùúáTree\nDRAM 0.14 0.34 0.14 1.14 0.18 2.63\nPM 2.69 2.54 18.92 4.79 3.2 3.2\ncacheline status, hence does not incur additional PM accesses. How-\never, most platforms default to a directory-based protocol because\nsnooping may not scale to high core counts. Since our server does\nnot allow changing coherence protocols, we were unable to verify\nthe performance of PM indexes using snooping protocols; we leave\nit as future work. As we have noted in Section 4.2, requiring a cer-\ntain coherence protocol may inflict issues with other applications\nand limit the applicability of the index.\n6.8 PM and DRAM Space Consumption\nNew PM indexes are using DRAM more extensively to achieve\nhigh performance. The downsides are (1) more complex recovery\nprotocols and (2) higher DRAM space consumption (hence higher\ncost of ownership). Table 3 lists the amount of DRAM and PM\nused by each index after loading 100 million records of 8-byte key\nand values (1.6GB). DPTree and ùúáTree practically store complete\ntrees in DRAM, resulting in up to ‚àº18√óhigher DRAM consumption\nwhen compared with FPTree and LB+-Tree. PACTree uses a similar\namount of PM to ùúáTree‚Äôs and is among the most frugal in using\nDRAM (second to FPTree) because of its packed design.\nSurprisingly, ROART uses 18.92GB to index 1.6GB of data, while\nothers need 2.5‚Äì4.8GB. The reason is it requires cacheline-aligned\nnodes and overprovisions leaf arrays: for 100 million records, it\nallocates space for around one billion records (17 million leaf arrays)\noccupying 10.89GB of PM as each leaf array is 640-byte. However,\nmost space (reserved for leaf pointers) are unused. Such overpro-\nvisioning is due to ROART‚Äôs split mechanism. In our experiment,\nkeys are uniform randomly generated, so the 64 records in a full leaf\narray are usually very different despite they share a common prefix.\nThen a split operation could allocate 63 leaf arrays in the worst\ncase, leaving each new leaf array with only few records, leading to\nan average occupancy of ‚àº9% and a waste of over 9GB of PM.\n\nEvaluating Persistent Memory Range Indexes: Part Two [Extended Version]\nFPTree\nLB+-Tree\nROART-PMDK\nROART-DCMM\nDPTree\nPACTree\n(a) Lookup00.20.40.60.81.0KB per operation\nFPTree\nLB+-Tree\nROART-PMDK\nROART-DCMM\nDPTree\nPACTree\n(b) Insert00.20.40.60.81.0\nFPTree\nLB+-Tree\nROART-PMDK\nROART-DCMM\nDPTree\nPACTree\n(c) Scan03691215DRAM Reads DRAM Writes PM Reads PM Writes\nFigure 9: Bandwidth consumption per operation under 20\nthreads with 100 million records of 8-byte keys and values.\n6.9 Bandwidth Utilization and Requirements\nPrevious evaluation [ 44] has shown that PM bandwidth is scarce.\nSince a database system also uses various other components, it is\ndesirable to keep the bandwidth consumption low for indexes. This\nhas been the main focus of newer indexes. We observe that the peak\nusage across all indexes and operations does not reach the limit\n(‚àº10GB/s/‚àº40GB/s for random write/sequential read). This shows\nthe effectiveness of the bandwidth saving techniques proposed by\nthe surveyed indexes. Due to space limitation, we omit the details\non total bandwidth utilization and focus on the bandwidth used per\noperation, which is more indicative on how frugal (or not) an index\nuses PM bandwidth. Figure 9 shows the results with 20 threads\nwhen running lookups, inserts and scans under 8-byte keys and\n8-byte values under the uniform distribution. Overall, LB+-Tree\nexhibits the lowest number of bytes per operation, thanks to its\nnode layout and logless design. ROART exhibits the highest PM\nreads as it overprovisions node space. In contrast, PACTree, which is\nalso trie-based, has a similar bandwidth requirement to LB+-Tree‚Äôs,\nbecause of its layout designed to reduce unnecessary PM accesses.\n7 OBSERVATIONS AND INSIGHTS\nIn this section, we summarize the major findings based on our\nexperimental results and analysis of the surveyed PM indexes.\n1. The rule of thumb remains reducing PM accesses, which\nwas first set in the pre-Optane era. Almost all the design choices\n(both pre-Optane and new ones) center around this goal, due to\nPM‚Äôs lower bandwidth and higher latency. If the properties of future\nPM hardware changes, the principles may be revisited.\n2. Some building blocks from the pre-Optane era continue\nto work well and are further optimized by new PM indexes.\nMost indexes use DRAM to accelerate traversal; some (e.g., DPTree)\neven place entire trees in DRAM. However, tries typically cannot\ntake the full advantage of DRAM to store reconstructible data. Fin-\ngerprints are also widely used and enhanced by placing them in the\nspare bits of pointers and accessing them using SIMD instructions.\n3. Using extra metadata and selective persistence of meta-\ndata can further accelerate performance. The main reason is\nthese approaches can avoid using WAL, which may incur additional\nPM writes and complicate code logic.\n4. Newly Proposed ‚â†Better. Pre-Optane FPTree is still very\ncompetitive and sometimes can even outperform newer indexes.\nAlthoughùúáTree optimizes for tail latency, it exhibits the highestlatency in many cases. Such results call for careful benchmarking\nand comprehensive evaluations.\n5. All the new PM indexes are tailor-made for one product\n(Intel Optane PMem), which can be a double-edged sword.\nWhile this can deliver high performance, as exemplified by LB+-\nTree which performs the best in most cases, it could pose challenges\nwhen the PM hardware landscape becomes more diverse.\n6. Support for NUMA-awareness, efficient PM management\nand variable-length keys remains inadequate. There have been\ninitial attempts (e.g., using pointers for variable-length keys), but\nthey are usually ad hoc or partial solutions with practical limitations\n(e.g., requiring a specific coherence protocol).\n7. There is no clear ‚Äúwinner‚Äù index architecture, but the\nchoice may affect how (efficiently) functionality can be sup-\nported. For example, B+-tree (trie) variants perform well for fixed-\nlength (variable-length) keys. But it remains to be explored whether\nit is easier to add efficient variable-length key support in LB+-Tree\nor to optimize PACTree to match LB+-Tree‚Äôs performance.\n8. Linked lists with small nodes are a bad fit for PM in-\ndexes, and cache misses in general should be minimized or\nhidden. Accessing and scanning through a linked list of individual\nrecords incur many cache misses which can dominate the perfor-\nmance and lead to high latency (e.g., in ùúáTree), canceling out the\npositive effects brought by other optimizations.\n9. HTM can perform well under NUMA for read-only work-\nloads, but is challenging to handle contention and debug. In\nparticular, the programmability issue is further complicated with\nother system-level infrastructure: we observed extremely high abort\nrates under glibc version 2.33 which does not use the right instruc-\ntions that can work with HTM in memcpy .11The bug was fixed in\nglibc 2.34 [ 54] which is used in our experiments. It is noticeable\nthat the best performing LB+-Tree is based on HTM; it therefore\nremains to be seen in future work whether other approaches can\novercome these issues while maintaining high performance.\n8 ON THE NEXT PM AND DRAM INDEXES\nWe give the outlook of the PM indexing space and describe promis-\ning future directions for PM and DRAM indexing.\n8.1 Future PM Indexes\nWe identify three promising areas of future work for PM indexes.\n1. Efficient Support for Full Functionality. As we have dis-\ncussed previously, variable-length keys and NUMA-awareness re-\nmain open problems for future PM indexes. Importantly, it is desir-\nable to maintain the high performance obtained by existing designs\nwhile better supporting full functionality.\n2. Wider Applicability/Less Tailor-Made. There are various\nways to realize PM, by using new materials (e.g., memristor [ 62],\nSTT-RAM [ 24] and Intel 3D XPoint which PMem is based upon)\nor NVDIMMs which combine flash and DRAM [ 1,65]. However,\nmost (if not all) indexes aiming for real PM are tailor-made for Intel\nOptane PMem; yet certain properties like 256-byte alignment may\neven change across generations of the same product, and designs\nbased on them may not work well on NVDIMMs, diminishing their\napplicability. Although some of the hardware efforts are in their\n11Details at https://sourceware.org/bugzilla/show_bug.cgi?id=28033 .\n\nYuliang He, Duo Lu, Kaisong Huang, and Tianzheng Wang\n110 20 30 40\n# of threads\n(a) Uniform Lookup020406080Throughput (Mop/s)\n110 20 30 40\n# of threads\n(b) Uniform Insert020406080\n110 20 30 40\n# of threads\n(c) Uniform Update020406080\n110 20 30 40\n# of threads\n(d) Uniform Scan0246\n110 20 30 40\n# of threads\n(e) Skewed Lookup020406080\n110 20 30 40\n# of threads\n(f) Skewed Update010203040\n110 20 30 40\n# of threads\n(g) Skewed Scan0246\nFPTree LB+-Tree ROART HOT Masstree\nFigure 10: Throughput of PM and DRAM indexes running on DRAM. Without the extra fences, cacheline flushes and PM\nmanagement code, PM-tailored indexes at least match the performance of HOT and Masstree. LB+-Tree performs even better\nthan HOT and Masstree on certain insert workloads (b), and FPTree tops scan performance under uniform distribution (d).\nearly stage, we argue it is important to consider applicability of\nfuture designs on different PM devices.\n3. Real-World Adoption and Cost-Effectiveness. Although\nthere have been numerous PM index proposals, we are yet to see\nmajor adoption in real systems and applications. Part of the reason is\nthe low cost effectiveness of PM-based servers as identified by other\nwork [ 26], especially when compared to modern SSDs which can\ndeliver high bandwidth and microsecond-level latency. Therefore,\non the hardware side, we hope future work to lower the per GB cost\nof PM servers. On the software side, PM indexes and data structures\nin general should focus more on cost/performance.\n8.2 Unifying PM and DRAM Indexing\nIn a similar vein to the point on wider applicability, we observe tech-\nniques proposed for PM indexes can also be effective for DRAM.\nWe conduct preliminary experiments to compare the surveyed\nstate-of-the-art PM indexes (with the extra cacheline flushes and\nfences removed) and two representative DRAM-optimized volatile\nindexes (HOT [ 6] and Masstree [ 50]). Figure 10 shows the through-\nput obtained by running the same workload as Section 6.4, but\npurely on DRAM. Under both uniform and skewed distributions,\nPM indexes perform competitively with DRAM-optimized indexes.\nIn certain cases, PM indexes perform even better than HOT and\nMasstree which are specifically optimized for DRAM, e.g., LB+-Tree\nfor inserts in Figure 10(b) and FPTree (despite being a pre-Optane\nproposal) in Figure 10(d) for scans. Although it remains to be seen\nhow the optimizations for PM and DRAM indexes compare, and\nhow PM techniques may be used by DRAM indexes (and vice versa),\nour results indicate it is promising to devise future indexes that\nwould work on both volatile and non-volatile memory. This could\ngreatly simplify implementation and widen the applicability of\ntechniques proposed by both types of indexes.\n9 RELATED WORK\nOur work is most related to performance studies for PM devices\nand data structures, PM indexes and PM management issues.\nPerformance Studies. Early work [ 33,72] characterized the\nperformance of Optane PMem, exposing a set of properties different\nfrom what were previously assumed by emulations. Gugnani et.\nal [22] exposed more properties of Optane PMem under various\nscenarios, e.g., eADR and NUMA, along with case studies. Beyond\nrange indexes, Hu et. al [ 25] evaluated PM-optimized hash indexeson Optane PMem. Koutsoukos et. al [ 35] analyzed the performance\nof PM-enhanced database engines under TPC-C and TPC-H, and\ncame up with guidelines of tuning the system for best performance.\nPM Indexes. In addition to adapting specific indexes, general-\npurpose approaches such as RECIPE [ 41], NAP [ 66] and TIPS [ 37]\npresent principled methods for converting DRAM indexes into PM\nindexes. It is interesting future work to evaluate these approaches.\nSome early efforts have adapted learned indexes [ 36] for PM. Chen\net. al [ 10] observe that the bigger nodes used by learned indexes\ncan cause excessive PM accesses. APEX [ 47] transforms the DRAM-\nbased updatable ALEX [ 18] with concurrency and instant recovery\non PM. Hash tables are also being re-designed for PM. CCEH [53]\nis a failure-atomic variant of extendible hashing that reduces di-\nrectory management overhead. Dash [ 48] integrates a set of useful\ntechniques to adapt extendible and linear hashing for PM; the key\ninsight is that both PM reads and writes should be minimized.\nClevel [ 14] is a lock-free version of level hashing [ 78] that performs\nasynchronous resizing in the background.\nPM Libraries. PMDK [ 29] is the de facto standard, but may not\nbe the optimal solution: ROART, DPTree and PACTree all devise\ntheir own approaches. Designing better PM libraries remains an\nopen area, as seen by many recent alternatives [5, 7, 32, 60, 74].\n10 CONCLUSION\nWe conducted a comprehensive evaluation of representative PM\nrange indexes proposed based on real Intel Optane PMem. These\nnew indexes inherited many useful designs from pre-Optane PM\nindexes and proposed new building blocks that can be useful for\nbuilding future PM indexes. Based on our evaluation, we gave a list\nof observations, insights and future directions. We found the new\nindexes do not necessarily outperform the pre-Optane proposals,\nand efficient designs for variable-length keys, PM management and\nNUMA awareness are still lacking. We also discovered for the first\ntime that a PM range index can match or even outperform DRAM-\noptimized indexes, highlighting the potential of unifying PM and\nDRAM indexing to save design and implementation efforts.\nACKNOWLEDGMENTS\nWe thank the anonymous reviewers and associate editor for their\nconstructive feedback. This work is partially supported by an NSERC\nDiscovery Grant, Canada Foundation for Innovation John R. Evans\nLeaders Fund and the B.C. Knowledge Development Fund.\n\nEvaluating Persistent Memory Range Indexes: Part Two [Extended Version]\nREFERENCES\n[1]AgigaTech. 2022. AGIGARAM NVDIMM-N. Retrieved May 15, 2022 from\nhttp://agigatech.com/products/agigaram-nvdimms/ .\n[2]Joy Arulraj, Justin J. Levandoski, Umar Farooq Minhas, and Per-√Öke Larson. 2018.\nBzTree: A High-Performance Latch-free Range Index for Non-Volatile Memory.\nPVLDB 11, 5 (2018), 553‚Äì565.\n[3] Joy Arulraj, Andrew Pavlo, and Subramanya R. Dulloor. 2015. Let‚Äôs Talk About\nStorage & Recovery Methods for Non-Volatile Memory Database Systems. In\nProceedings of the 2015 ACM SIGMOD International Conference on Management of\nData (SIGMOD ‚Äô15) . 707‚Äì722.\n[4]Lawrence Benson, Hendrik Makait, and Tilmann Rabl. 2021. Viper: An Efficient\nHybrid PMem-DRAM Key-Value Store. Proc. VLDB Endow. 14, 9 (may 2021),\n1544‚Äì1556.\n[5]Kumud Bhandari, Dhruva R. Chakrabarti, and Hans-J. Boehm. 2016. Makalu:\nFast Recoverable Allocation of Non-Volatile Memory. In Proceedings of the 2016\nACM SIGPLAN International Conference on Object-Oriented Programming, Systems,\nLanguages, and Applications . 677‚Äì694.\n[6]Robert Binna, Eva Zangerle, Martin Pichl, G√ºnther Specht, and Viktor Leis. 2018.\nHOT: A Height Optimized Trie Index for Main-Memory Database Systems. In\nProceedings of the 2018 International Conference on Management of Data (SIGMOD\n‚Äô18). 521‚Äì534.\n[7]Wentao Cai, Haosen Wen, H. Alan Beadle, Chris Kjellqvist, Mohammad Hedayati,\nand Michael L. Scott. 2020. Understanding and Optimizing Persistent Memory\nAllocation. In Proceedings of the 2020 ACM SIGPLAN International Symposium on\nMemory Management (ISMM 2020) . 60‚Äì73.\n[8]Zixian Cai, Stephen M. Blackburn, and Michael D. Bond. 2021. Understanding\nand Utilizing Hardware Transactional Memory Capacity. In Proceedings of the\n2021 ACM SIGPLAN International Symposium on Memory Management (ISMM\n2021) . 1‚Äì14.\n[9]Cheng Chen, Jun Yang, Mian Lu, Taize Wang, Zhao Zheng, Yuqiang Chen,\nWenyuan Dai, Bingsheng He, Weng-Fai Wong, Guoan Wu, Yuping Zhao, and\nAndy Rudoff. 2021. Optimizing In-Memory Database Engine for AI-Powered\non-Line Decision Augmentation Using Persistent Memory. PVLDB 14, 5 (2021),\n799‚Äì812.\n[10] Leying Chen and Shimin Chen. 2021. How Does Updatable Learned Index Perform\non Non-Volatile Main Memory?. In 2021 IEEE 37th International Conference on\nData Engineering Workshops (ICDEW) . 66‚Äì71.\n[11] Shimin Chen, Phillip B. Gibbons, and Suman Nath. 2011. Rethinking Database\nAlgorithms for Phase Change Memory. In 5th Biennial Conference on Innovative\nData Systems Research, CIDR 2011, Asilomar, CA, USA, January 9-12, 2011, Online\nProceedings .\n[12] Shimin Chen and Qin Jin. 2015. Persistent B+-Trees in Non-Volatile Main Memory.\nPVLDB 8, 7 (2015), 786‚Äì797.\n[13] Youmin Chen, Youyou Lu, Kedong Fang, Qing Wang, and Jiwu Shu. 2020. uTree:\na Persistent B+-Tree with Low Tail Latency. PVLDB 13, 11 (2020), 2634‚Äì2648.\n[14] Zhangyu Chen, Yu Hua, Bo Ding, and Pengfei Zuo. 2020. Lock-free Concur-\nrent Level Hashing for Persistent Memory. In 2020 USENIX Annual Technical\nConference (USENIX ATC 20) . 799‚Äì812.\n[15] Jeremy Condit, Edmund B. Nightingale, Christopher Frost, Engin Ipek, Ben-\njamin Lee, Doug Burger, and Derrick Coetzee. 2009. Better I/O through Byte-\nAddressable, Persistent Memory. In Proceedings of the ACM SIGOPS 22nd Sympo-\nsium on Operating Systems Principles (SOSP ‚Äô09) . 133‚Äì146.\n[16] Intel Corporation. 2021. Intel Optane Persistent Memory (PMem). https:\n//www.intel.ca/content/www/ca/en/architecture-and-technology/optane-dc-\npersistent-memory.html\n[17] Rob Crooke and Mark Durcan. 2015. A Revolutionary Breakthrough in Memory\nTechnology. 3D XPoint Launch Keynote (2015).\n[18] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned Index.\nInProceedings of the 2020 ACM SIGMOD International Conference on Management\nof Data (SIGMOD ‚Äô20) . 969‚Äì984.\n[19] Jason Evans. 2006. A Scalable Concurrent malloc (3) Implementation for FreeBSD.\nInProceedings of the BSDCan Conference .\n[20] Keir Fraser. 2004. Practical lock-freedom . Technical Report UCAM-CL-TR-579.\nUniversity of Cambridge, Computer Laboratory. https://www.cl.cam.ac.uk/\ntechreports/UCAM-CL-TR-579.pdf\n[21] Jim Gray, Prakash Sundaresan, Susanne Englert, Ken Baclawski, and Peter J.\nWeinberger. 1994. Quickly Generating Billion-Record Synthetic Databases. In\nProceedings of the 1994 ACM SIGMOD International Conference on Management of\nData (SIGMOD ‚Äô94) . 243‚Äì252.\n[22] Shashank Gugnani, Arjun Kashyap, and Xiaoyi Lu. 2020. Understanding the\nIdiosyncrasies of Real Persistent Memory. PVLDB 14, 4 (2020), 626‚Äì639.\n[23] Timothy L. Harris. 2001. A Pragmatic Implementation of Non-Blocking Linked-\nLists. In Proceedings of the 15th International Conference on Distributed Computing\n(DISC ‚Äô01) . Springer-Verlag, Berlin, Heidelberg, 300‚Äì314.[24] M. Hosomi, H. Yamagishi, T. Yamamoto, K. Bessho, Y. Higo, K. Yamane, H. Yamada,\nM. Shoji, H. Hachino, C. Fukumoto, H. Nagao, and H. Kano. 2005. A novel\nnonvolatile memory with spin torque transfer magnetization switching: spin-\nram. IEEE International Electron Devices Meeting (IEDM) (2005), 459‚Äì462.\n[25] Daokun Hu, Zhiwen Chen, Jianbing Wu, Jianhua Sun, and Hao Chen. 2021.\nPersistent Memory Hash Indexes: An Experimental Evaluation. PVLDB 14 (2021),\n785‚Äì798.\n[26] Kaisong Huang, Darien Imai, Tianzheng Wang, and Dong Xie. 2022. SSDs Striking\nBack: The Storage Jungle and Its Implications on Persistent Indexes. In 12th\nAnnual Conference on Innovative Data Systems Research, CIDR 2022, Chaminade,\nCA, USA, January 9-12, 2022, Online Proceedings .\n[27] Deukyeon Hwang, Wook-Hee Kim, Youjip Won, and Beomseok Nam. 2018. En-\ndurable transient inconsistency in byte-addressable persistent B+-tree. In 16th\nUSENIX Conference on File and Storage Technologies (FAST 18) . 187‚Äì200.\n[28] Intel. 2021. Brief: Intel ¬ÆOptane ‚Ñ¢Persistent Memory ‚Äì The Challenge of Keeping\nUp with Data. https://www.intel.ca/content/www/ca/en/products/docs/memory-\nstorage/optane-persistent-memory/optane-dc-persistent-memory-brief.html\n[29] Intel. 2021. Persistent Memory Development Kit. (2021). http://pmem.io/pmdk/ .\n[30] Intel Corporation. 2021. Intel 64 and IA-32 Architectures Software Developer‚Äôs\nManual. (2021). https://software.intel.com/content/www/us/en/develop/articles/\nintel-sdm.html\n[31] Intel Corporation. 2021. Optane DCPMM 200 Series Product Brief. Retrieved\nAugust 17, 2021 from https://www.intel.com/content/dam/www/public/us/en/\ndocuments/product-briefs/optane-persistent-memory-200-series-brief.pdf .\n[32] Keita Iwabuchi, Karim Youssef, Kaushik Velusamy, Maya Gokhale, and Roger\nPearce. 2022. Metall: A persistent memory allocator for data-centric analytics.\nParallel Comput. 111 (2022).\n[33] Joseph Izraelevitz, Jian Yang, Lu Zhang, Juno Kim, Xiao Liu, Amirsaman\nMemaripour, Yun Joon Soh, Zixuan Wang, Yi Xu, Subramanya R. Dulloor, Jishen\nZhao, and Steven Swanson. 2019. Basic Performance Measurements of the Intel\nOptane DC Persistent Memory Module. arXiv:1903.05714 [cs.DC]\n[34] Wook-Hee Kim, R. Madhava Krishnan, Xinwei Fu, Sanidhya Kashyap, and Chang-\nwoo Min. 2021. PACTree: A High Performance Persistent Range Index Using\nPAC Guidelines. In Proceedings of the ACM SIGOPS 28th Symposium on Operating\nSystems Principles . 424‚Äì439.\n[35] Dimitrios Koutsoukos, Raghav Bhartia, Ana Klimovic, and Gustavo Alonso. 2021.\nHow to use Persistent Memory in your Database. CoRR abs/2112.00425 (2021).\narXiv:2112.00425 https://arxiv.org/abs/2112.00425\n[36] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In Proceedings of the 2018 International\nConference on Management of Data (SIGMOD ‚Äô18) . 489‚Äì504.\n[37] R. Madhava Krishnan, Wook-Hee Kim, Xinwei Fu, Sumit Kumar Monga, Hee Won\nLee, Minsung Jang, Ajit Mathew, and Changwoo Min. 2021. TIPS: Making Volatile\nIndex Structures Persistent with DRAM-NVMM Tiering. In 2021 USENIX Annual\nTechnical Conference (USENIX ATC 21) . 773‚Äì787.\n[38] Laboratory for Web Algorithms. 2022. UK Domain from 2005. http://data.law.di.\nunimi.it/webdata/uk-2005/uk-2005.urls.gz\n[39] Benjamin C. Lee, Engin Ipek, Onur Mutlu, and Doug Burger. 2009. Architecting\nPhase Change Memory as a Scalable Dram Alternative. In Proceedings of the 36th\nAnnual International Symposium on Computer Architecture (ISCA ‚Äô09) . 2‚Äì13.\n[40] Se Kwon Lee, K. Hyun Lim, Hyunsub Song, Beomseok Nam, and Sam H. Noh.\n2017. WORT: Write Optimal Radix Tree for Persistent Memory Storage Systems.\nIn15th USENIX Conference on File and Storage Technologies (FAST 17) . 257‚Äì270.\n[41] Se Kwon Lee, Jayashree Mohan, Sanidhya Kashyap, Taesoo Kim, and Vijay Chi-\ndambaram. 2019. RECIPE: Converting Concurrent DRAM Indexes to Persistent-\nMemory Indexes. In Proceedings of the 27th ACM Symposium on Operating Systems\nPrinciples (SOSP ‚Äô19) . 462‚Äì477.\n[42] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. The Adaptive Radix\nTree: ARTful Indexing for Main-Memory Databases. In Proceedings of the 2013\nIEEE International Conference on Data Engineering (ICDE ‚Äô13) . 38‚Äì49.\n[43] Viktor Leis, Florian Scheibner, Alfons Kemper, and Thomas Neumann. 2016.\nThe ART of Practical Synchronization. In Proceedings of the 12th International\nWorkshop on Data Management on New Hardware (DaMoN ‚Äô16) . Article 3, 8 pages.\n[44] Lucas Lersch, Xiangpeng Hao, Ismail Oukid, Tianzheng Wang, and Thomas\nWillhalm. 2019. Evaluating Persistent Memory Range Indexes. PVLDB 13, 4\n(2019), 574‚Äì587.\n[45] Gang Liu, Leying Chen, and Shimin Chen. 2021. Zen: A High-Throughput Log-\nFree OLTP Engine for Non-Volatile Main Memory. PVLDB 14, 5 (2021), 835‚Äì848.\n[46] Jihang Liu, Shimin Chen, and Lujun Wang. 2020. LB+Trees: Optimizing Persistent\nIndex Performance on 3DXPoint Memory. PVLDB 13, 7 (2020), 1078‚Äì1090.\n[47] Baotong Lu, Jialin Ding, Eric Lo, Umar Farooq Minhas, and Tianzheng Wang.\n2021. APEX: A High-Performance Learned Index on Persistent Memory. PVLDB\n15, 3 (2021), 597‚Äì610.\n[48] Baotong Lu, Xiangpeng Hao, Tianzheng Wang, and Eric Lo. 2020. Dash: Scalable\nHashing on Persistent Memory. PVLDB 13, 8 (2020), 1147‚Äì1161.\n[49] Shaonan Ma, Kang Chen, Shimin Chen, Mengxing Liu, Jianglang Zhu, Hongbo\nKang, and Yongwei Wu. 2021. ROART: Range-query Optimized Persistent ART.\nIn19th USENIX Conference on File and Storage Technologies (FAST 21) . 1‚Äì16.\n\nYuliang He, Duo Lu, Kaisong Huang, and Tianzheng Wang\n[50] Yandong Mao, Eddie Kohler, and Robert Tappan Morris. 2012. Cache craftiness\nfor fast multicore key-value storage. In Proceedings of the 7th ACM european\nconference on Computer Systems . 183‚Äì196.\n[51] Chris Mellor. 2019. Is Optane DIMM endurance good enough? Quick answer. . . Yes,\nIntel has delivered. https://blocksandfiles.com/2019/04/04/enduring-optane-\ndimm-question-is-its-endurance-good-enough-yes-intel-has-delivered/\n[52] Colin Morris. 2017. Reddit Usernames. https://www.kaggle.com/datasets/\ncolinmorris/reddit-usernames\n[53] Moohyeon Nam, Hokeun Cha, Young ri Choi, Sam H. Noh, and Beomseok Nam.\n2019. Write-Optimized Dynamic Hashing for Persistent Memory. In 17th USENIX\nConference on File and Storage Technologies (FAST 19) . 31‚Äì44.\n[54] Carlos O‚ÄôDonell. 2021. The GNU C Library version 2.34 is now available ‚Äì [28033]\nlibc: Need to check RTM_ALWAYS_ABORT for RTM. https://sourceware.org/\npipermail/libc-alpha/2021-August/129718.html .\n[55] Ismail Oukid, Daniel Booss, Wolfgang Lehner, Peter Bumbulis, and Thomas\nWillhalm. 2014. SOFORT: A Hybrid SCM-DRAM Storage Engine for Fast Data\nRecovery. In Proceedings of the Tenth International Workshop on Data Management\non New Hardware (DaMoN ‚Äô14) . Article 8, 7 pages.\n[56] Ismail Oukid, Johan Lasperas, Anisoara Nica, Thomas Willhalm, and Wolfgang\nLehner. 2016. FPTree: A Hybrid SCM-DRAM Persistent and Concurrent B-Tree\nfor Storage Class Memory. In Proceedings of the 2016 International Conference on\nManagement of Data, SIGMOD . 371‚Äì386.\n[57] Jinsu Park and Woongki Baek. 2018. Quantifying the Performance and Energy-\nEfficiency Impact of Hardware Transactional Memory on Scientific Applications\non Large-Scale NUMA Systems. In 2018 IEEE International Parallel and Distributed\nProcessing Symposium (IPDPS) . 804‚Äì813.\n[58] Steven Pelley, Thomas F. Wenisch, Brian T. Gold, and Bill Bridge. 2013. Storage\nManagement in the NVRAM Era. PVLDB 7, 2 (2013), 121‚Äì132.\n[59] Raghu Ramakrishnan and Johannes Gehrke. 2003. Database Management Systems\n(3 ed.).\n[60] David Schwalb, Tim Berning, Martin Faust, Markus Dreseler, and Hasso Plattner.\n2015. nvm malloc: Memory Allocation for NVRAM.. In ADMS@VLDB . 61‚Äì72.\n[61] Benjamin Spector, Andreas Kipf, Kapil Vaidya, Chi Wang, Umar Farooq Minhas,\nand Tim Kraska. 2021. Bounding the Last Mile: Efficient Learned String Indexing\n(Extended Abstracts). In 3rd International Workshop on Applied AI for Database\nSystems and Applications, AIDB Workshops .\n[62] D. B. Strukov, G. S. Snider, D. R. Stewart, and R. S. Williams. 2008. The missing\nmemristor found. Nature 453, 7191 (2008), 80‚Äì83.\n[63] Alexander van Renen, Viktor Leis, Alfons Kemper, Thomas Neumann, Takushi\nHashida, Kazuichi Oe, Yoshiyasu Doi, Lilian Harada, and Mitsuru Sato. 2018.\nManaging Non-Volatile Memory in Database Systems. In Proceedings of the 2018\nInternational Conference on Management of Data (SIGMOD ‚Äô18) . 1541‚Äì1555.\n[64] Shivaram Venkataraman, Niraj Tolia, Parthasarathy Ranganathan, and Roy H.\nCampbell. 2011. Consistent and Durable Data Structures for Non-Volatile Byte-\nAddressable Memory. In Proceedings of the 9th USENIX Conference on File and\nStroage Technologies (FAST‚Äô11) .\n[65] Viking Technology. 2017. DDR4 NVDIMM. Retrieved August 17, 2021 from\nhttp://www.vikingtechnology.com .\n[66] Qing Wang, Youyou Lu, Junru Li, and Jiwu Shu. 2021. Nap: A Black-Box Approach\nto NUMA-Aware Persistent Memory Indexes. In 15th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI 21) . 93‚Äì111.\n[67] Tianzheng Wang and Ryan Johnson. 2014. Scalable Logging through Emerging\nNon-Volatile Memory. PVLDB 7, 10 (2014), 865‚Äì876.\n[68] Tianzheng Wang, Justin Levandoski, and Per-√Öke Larson. 2018. Easy Lock-Free\nIndexing in Non-Volatile Memory. In 2018 IEEE 34th International Conference on\nData Engineering (ICDE) . 461‚Äì472.\n[69] Wikimedia Dump Service. 2022. Wikipedia Dump 20220420. https://dumps.\nwikimedia.org/enwiki/20220420/enwiki-20220420-all-titles.gz\n[70] H. S P Wong, S. Raoux, SangBum Kim, Jiale Liang, John P. Reifenberg, B. Rajen-\ndran, Mehdi Asheghi, and Kenneth E. Goodson. 2010. Phase Change Memory.\nProc. IEEE 98, 12 (2010), 2201‚Äì2227.\n[71] Fei Xia, Dejun Jiang, Jin Xiong, and Ninghui Sun. 2017. HiKV: A Hybrid Index\nKey-value Store for DRAM-NVM Memory Systems. In Proceedings of the 2017\nUSENIX Annual Technical Conference (USENIX ATC ‚Äô17) . 349‚Äì362.\n[72] Jian Yang, Juno Kim, Morteza Hoseinzadeh, Joseph Izraelevitz, and Steven Swan-\nson. 2020. An Empirical Guide to the Behavior and Use of Scalable Persistent\nMemory. In Proceedings of the 18th USENIX Conference on File and Storage Tech-\nnologies (FAST‚Äô20) . 169‚Äì182.\n[73] Jun Yang, Qingsong Wei, Cheng Chen, Chundong Wang, Khai Leong Yong, and\nBingsheng He. 2015. NV-Tree: reducing consistency cost for NVM-based single\nlevel systems. In 13th USENIX Conference on File and Storage Technologies (FAST\n15). 167‚Äì181.\n[74] Lu Zhang and Steven Swanson. 2019. Pangolin: A Fault-Tolerant Persistent\nMemory Programming Library. In 2019 USENIX Annual Technical Conference\n(USENIX ATC 19) . 897‚Äì912.\n[75] Ping Zhou, Bo Zhao, Jun Yang, and Youtao Zhang. 2009. A Durable and Energy\nEfficient Main Memory Using Phase Change Memory Technology. In Proceedingsof the 36th Annual International Symposium on Computer Architecture (ISCA ‚Äô09) .\n14‚Äì23.\n[76] Xinjing Zhou, Joy Arulraj, Andrew Pavlo, and David Cohen. 2021. Spitfire: A\nThree-Tier Buffer Manager for Volatile and Non-Volatile Memory. In Proceedings\nof the 2021 International Conference on Management of Data (SIGMOD/PODS ‚Äô21) .\n2195‚Äì2207.\n[77] Xinjing Zhou, Lidan Shou, Ke Chen, Wei Hu, and Gang Chen. 2019. DPTree:\nDifferential Indexing for Persistent Memory. PVLDB 13, 4 (2019), 421‚Äì434.\n[78] Pengfei Zuo, Yu Hua, and Jie Wu. 2018. Write-Optimized and High-Performance\nHashing Index Scheme for Persistent Memory. In 13th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI 18) . 461‚Äì476.\nAPPENDIX\nA DATASETS USED AND CLARIFICATION\nHere we present the details of all three datasets used in the variable-\nlength key experiments. The names dataset contains 25 million\nunique Reddit usernames of length up to 25 characters. wiki con-\ntains 55 million unique article titles of length up to 256 characters.\nuk-2005 contains 40 million unique URLs of up to 2029 charac-\nters. The key distributions and statistics of the three datasets are\nshown in Figure 11, covering a wide range of different key lengths\nin reality. As a trie-based index, ROART already supports variable\nlength keys. So no modification is required. For LB+-Tree, FPTree\nand DPTree, we modified their lookup and insert operations to\nsupport variable-length keys. The parameters of the related opera-\ntions are unchanged but we allocate the variable-length keys from\nvolatile/persistent heap and store the key pointers in inner/leaf\nnodes. Key comparison is performed using memcmp over the two\nkey pointers. We also allocate each key by the maximum length re-\ngardless of its actual length, to ease implementation. This approach\ndoes results in wasted memory especially in the case of skewed\ndatasets.\nB LONG-RUNNING WORKLOADS\nAlthough PM‚Äôs bandwidth exhausts quickly with increasing number\nof writers [ 44], its performance tends to be stable across the short-\nlived benchmarks. To simulate the impact of long-running systems\nand high space utilization on PM performance, we conducted two\nstress tests using LB+-Tree.\nA pool of 485GB is first allocated from PM, which is the max-\nimum available PM space that can be allocated on the first node\nof our machine. We then populated the entire reserved space with\n8-byte keys/values generated from a uniform distribution, and ran\na read-only workload for 20 minutes with 40 threads. The lookup\nthroughput averaged by minutes is shown in Figure 12(a). As de-\npicted, the number of per-second read operations completed is very\nstable across the entire experiment phase.\nWe further conducted another insert experiment, where 20 bil-\nlion uniformly distributed 8-byte key/values are inserted with 40\nthreads. The experiment kept running until the PM space is ex-\nhausted (it took around 20 minutes). As shown by Figure 12(b), the\ninsert throughput was also stable with minor gradual degradation\n(‚àº12%) as we insert more keys. This is expected because the index\ngets larger and deeper with more keys inserted, thus resulting in\nslower traversal and SMOs.\nOverall, PMem demonstrates robustness and stable performance\nthroughout the long-running read/write intensive benchmarks\nwhile under high space utilization. Different from traditional SSDs\n\nEvaluating Persistent Memory Range Indexes: Part Two [Extended Version]\nFigure 11: Key length distributions of three real-world variable-length key datasets.\n1234567891011121314151617181920\nMinutes\n(a) Lookup1618202224Throughput (Mop/s)\n1234567891011121314151617181920\nMinutes\n(b) Insert1012141618\nFigure 12: LB+-Tree performance over time.\n11020304080\n# of threads\n(a) Lookup0306090120Throughput (Mop/s)\n11020304080\n# of threads\n(b) Insert018365472\n11020304080\n# of threads\n(c) Update024487296\n11020304080\n# of threads\n(d) Scan0369\nFPTree LB+-Tree ROART HOT Masstree\nFigure 13: Impact of NUMA effect for indexes on DRAM.that are known to perform worse as approaching full space utiliza-\ntion, PMem has shown no obvious hardware/software performance\ndegradation (except due to the characteristics of the workload itself,\ne.g., inserts) in our experiments. However, we acknowledge that\nthese tests are preliminary and further investigation with even\nlonger experiments would be needed to thoroughly explore this\ntopic in future work.\nC NUMA EXPERIMENTS ON DRAM\nThis set of experiments repeats our earlier NUMA experiments in\nthe main text but were done using DRAM to explore the impact of\nNUMA effect. As shown by Figure 8, most PM indexes are not scal-\nable and their throughput can drop drastically beyond one socket.\nHowever, it is not the case on DRAM. As Figure 13 shows, when\nrunning on DRAM, FPTree, LB+-Tree and ROART are still scal-\nable across two NUMA nodes. Thus, the main PM-specific concern\nlies in the impact of PM‚Äôs higher latency/lower bandwidth on the\ninterconnect traffic and cache coherence protocol [34].",
  "textLength": 92577
}