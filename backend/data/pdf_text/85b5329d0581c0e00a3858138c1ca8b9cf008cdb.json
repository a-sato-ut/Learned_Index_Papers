{
  "paperId": "85b5329d0581c0e00a3858138c1ca8b9cf008cdb",
  "title": "Flexible Operator Embeddings via Deep Learning",
  "pdfPath": "85b5329d0581c0e00a3858138c1ca8b9cf008cdb.pdf",
  "text": "Flexible Operator Embeddings via Deep Learning\nRyan Marcus\nBrandeis University\nryan@cs.brandeis.eduOlga Papaemmanouil\nBrandeis University\nolga@cs.brandeis.edu\nABSTRACT\nIntegrating machine learning techniques into the internals of\ndatabase systems requires signi\fcant feature engineering, a\nhuman e\u000bort-intensive process to determine the best way to\nrepresent relevant pieces of information for a data manage-\nment task. In addition to being labor intensive, the process\nof hand-engineering features must generally be repeated for\neach task, and may make assumptions about the underly-\ning database that are not universally true. We introduce\na deep learning technique for automatically transforming\nquery operators into dense, information-rich feature vectors,\ncalled operator embeddings. These learned embeddings are\ncustom-tailored to the underlying database, enabling o\u000b-\nthe-shelf machine learning techniques to achieve good per-\nformance on multiple data management tasks. Experimen-\ntally, we show that our \rexible operator embeddings out-\nperform existing feature engineering techniques using both\nsynthetic and real-world datasets.\nPVLDB Reference Format:\nRyan Marcus, Olga Papaemmanouil. Flexible Operator Embed-\ndings via Deep Learning. PVLDB , 12(xxx): xxxx-yyyy, 2019.\nDOI: https://doi.org/10.14778/xxxxxxx.xxxxxxx\n1. INTRODUCTION\nAs database management systems and their applications\ngrow increasingly complex, researchers have turned to ma-\nchine learning to solve numerous data management prob-\nlems, such as query admission control [60], query schedul-\ning [41], cluster sizing [34,39,46], index selection [49], cardi-\nnality estimation [26,33], and query optimization [29,37,45].\nOne of the most critical steps of integrating machine learn-\ning techniques into data management systems is the process\noffeature engineering . Generally, feature engineering in-\nvolves human experts deciding (1) if a particular piece of\ninformation is relevant to a task (too many features may\nincrease inference time or decrease the e\u000ecacy of some ma-\nchine learning models), (2) how to combine available domain\ninformation into useful features (many models desirable fortheir fast inference time are unable to learn arbitrary com-\nbinations of features, e.g. linear regression), and (3) how\ntoencode these features into usable input (most machine\nlearning algorithms require vector inputs).\nIn addition to being a di\u000ecult task to undertake, feature\nengineering has many disadvantages speci\fc to data man-\nagement tasks:\n\u000fTime/cost intensive : feature engineering requires\nexperts in machine learning and database systems to\nspend a long time testing and experimenting with dif-\nferent combinations of features. For example, there\nwere 18 engineered features in [2] for latency predic-\ntion, and 41 engineered features in [25] used for database\nintrusion detection.\n\u000fNon-task transferable : generally, features engineered\nby hand for a speci\fc task will not provide good per-\nformance on a di\u000berent task { the process must be\nrepeated for each task (e.g. the features developed\nfor query admission control in [60] cannot be used for\nlatency prediction [2], nor for database intrusion de-\ntection [25], and vice versa).\n\u000fNon-data transferable : features engineered for a\nparticular task may work well for one dataset (e.g.\nTPC-H), but those same features may fail for another\ndataset (e.g. a real-world dashboard system).\nIn order to ease these burdens, this work introduces \rex-\nible operator embeddings : a general technique to perform\nmulti-purpose, database-tailored feature engineering with\nminimal human e\u000bort. As query operators lie at the heart\nof numerous complex tasks, including but not limited to re-\nsource consumption prediction [32], query optimization [55],\nquery performance prediction [2], and query scheduling [60],\nour embeddings o\u000ber feature engineering at the operator\nlevel. Here, an operator embedding is a mapping from a\nquery operator to a low-dimensional feature (vector) space\nthat captures rich, useful information about the operator.\nBy leveraging deep learning techniques, these embeddings\nact as vectorized representations of an operator that (a) can\nbe generated and tailored to a speci\fc database, (b) can pro-\nvide useful features for a variety of data management tasks,\nand (c) provide usable input to numerous o\u000b-the-shelf ma-\nchine learning algorithms.\nOur technique for learning representations of query op-\nerators is based on the intuition that the behavior of an\noperator is context sensitive : a join of two large relations\nwill behave di\u000berently than a join of two small relations\n1arXiv:1901.09090v2  [cs.DB]  31 Jan 2019\n\n(e.g. greater memory usage, possible bu\u000ber spilling, etc.).\nWe capture this intuition by training a deep neural network\nwith a specialized, hour-glass architecture to predict the con-\ntext of a given query operator: in our case, the operator's\nchildren. The training set for this neural network is a set of\nquery plans (i.e., trees of query operators). Once trained,\nwe use the internal representation of a particular operator\nlearned by the neural network as that operator's embed-\nding. Intuitively, training the neural network to predict an\noperator's children ensures that the internal representation\nlearned by the neural network carries a high amount of se-\nmantic information about the operator. For example, if the\nnetwork can predict that a join operator with a particular\npredicate normally has two very large inputs, then the inter-\nnal representation learned by the network is likely to carry\nsemantic information relevant to whether or not that join\noperator will use a signi\fcant amount of memory.\nOne unique characteristic of our operator embeddings is\nthat they produce features that can be fed to traditional, o\u000b-\nthe-shelf machine learning models with fast inference time.\nFurthermore, they can be useful for integrating machine\nlearning techniques into a variety of data management tasks\nsuch as query admission, query outlier detection, and detect-\ning cardinality estimation errors. Finally, once learned by\nthe neural network, our \rexible operator embeddings are sig-\nni\fcantly information rich. This implies that, when learned\nembeddings are used as input to a traditional machine learn-\ning model, only a relatively small amount of training data\nis required to learn a particular task. Overall, we argue\nthat learned embeddings represent a valuable addition to the\nDBMS designer's toolbox, enabling accurate, light-weight,\ncustom-tailored models for a wide variety of tasks that can\nbe generated with minimal human e\u000bort.\nThe contributions of this paper are:\n\u000fWe present \rexible operator embeddings , a novel tech-\nnique to automatically map query operators to useful\nfeatures tailored to a particular database, alleviating\nthe need for costly human feature engineering.\n\u000fWe demonstrate that our embeddings can be e\u000bec-\ntively combined with simple, low-latency, and o\u000b-the-\nshelf machine learning models.\n\u000fWe demonstrate the e\u000ecacy and \rexibility of our cus-\ntom embeddings across a variety of datasets and tasks.\nThis remainder of this paper is organized as follows. In\nSection 2, we give an intuitive overview of why e\u000bectively\nrepresenting query operators as vectors is di\u000ecult, and how\ndeep learning can be used to learn e\u000bective embeddings.\nSection 3 describes our framework for learning and applying\ndatabase-tailored operator embeddings. Section 4 describes\nhow to train a custom operator embedding and apply it\nto a particular task. We present experimental results in\nSection 5, demonstrating the e\u000ecacy of our approach across\na variety of workloads and tasks. Related work is presented\nin Section 6, and concluding remarks are given in Section 7.\n2. CONTEXT-A WARE EMBEDDINGS\nMachine learning is not magic, and like all algorithms,\nobeys the maxim of \\garbage in, garbage out.\" If one pro-\nvides a meaningless or nearly-meaningless representation of\nthe domain, a machine learning algorithm will show poor\nking [0, 0, 0, 1, 0, 0, 0]\nqueen [1, 0, 0, 0, 0, 0, 0]\nbanana [0, 0, 1, 0, 0, 0, 0]\napple [0, 0, 0, 0, 0, 0, 1]A INLJg B [1, 0, 0, 1, 0, 0, 0]\nA MJh B [0, 1, 0, 0, 1, 0, 0]\nσp(A) [0, 0, 1, 0, 0, 1, 0]\nσq(A) [0, 0, 1, 0, 0, 0, 1]\nOperator type Attrib.Predicate(a) One-hot encodings of words (left) and query oper-\nators (right)\nking\nqueen\napple\nbananaA INLJg B\nA MJh B\nσp(A)\nσq(A)\n(b) Example embedded space word vectors (left) and\noperator embeddings (right)\nFigure 1: Encodings and embeddings for words and query\noperators\nperformance [12]. If one wants a machine learning model\nto generalize well to unseen data, one needs to provide a\ngood representation of the data. Here, \\good\" inputs to\na machine learning algorithm means inputs that are gener-\nally dense (i.e., every dimension is more-or-less continuous\nand meaningful) and information-rich (i.e., the distance be-\ntween points is meaningful). Unfortunately, coming up with\na dense, information-rich representation of query operators\nis often not easy. Next, we motivate our context-aware op-\nerator embeddings by discussing alternative approaches and\ntheir drawbacks when used for data management tasks.\nOne-hot encodings A commonly used information encod-\ning approach (particularly in the domain of natural language\nprocessing, NLP) is using an one-hot vector, i.e., a vector\nwhere there is a single high (generally 1) value and all the\nother values are low values (generally 0). For example, in\nNLP, where the main units of analysis are words, a given\nword can be represented with a high value in a speci\fc po-\nsition in a vector with the size of the vocabulary. Figure 1a\nshows an example of how the words king, queen, apple, and\nbanana might be represented as part of an eight-word vocab-\nulary. Here, the word \\king\" is represented by a high value\nin the fourth position of the vector, and the word \\queen\"\nis represented by a high value in the \frst position.\nThe right side of Figure 1a shows how a similar one-hot\nencoding strategy can be used to encode query operators.\nHere, four operators { an index-nested loop (natural) join\non an attribute g, a merge (natural) join on attribute h, and\ntwo selection operators, with predicates pandq{ are repre-\nsented with a straightforward \\combined\" one-hot encoding\nwhich captures information about the operator type (in the\n\frst three dimensions), join attribute (in the next two di-\nmensions), and predicate (in the last two dimensions). For\nexample, the appearance of nested loop join is captured by\nthe value of 1 in the \frst position, a merge-join is represented\nby the value of 1 in the second position, and the selection\noperator is captured in the third position. Similarly, the join\nattributes \\g\" and\\h\" are captured by a value of 1 in the\nfourth and \ffth positions, respectively.\n2\n\nFor both English words and query operators, it is immedi-\nately clear that this representation is (1) not dense, as each\ndimension is used minimally, and (2) not information-rich,\nas all three operator types (and all four English words) are\nequidistant from each other in the embedded space. For ex-\nample, the distance between the \frst three operators (the\nnested loop join, the merge join, and the \frst selection op-\nerator) are all equidistant. As a result, this representation\nencodes very little semantic information. While not partic-\nularly useful (indeed, both database-related and NLP mod-\nels trained directly on one-hot vectors perform poorly), this\nsort of encoding is very easy to come up with, and requires\nalmost no human e\u000bort.\nInformation-rich embeddings Ideally, our vectorized rep-\nresentations would contain signi\fcant semantic information.\nIn the NLP case, what is desired is the representation de-\npicted in the left side of Figure 1b: \\king\" and \\queen\" are\nneighbors, as are \\apple\" and \\banana.\" For query opera-\ntors, we want the vectors representing the merge join and\nindex nested loop join operators to be closer to each other\nthan to the selections, as depicted in the right side of Fig-\nure 1b, since both merge join and index nested loop join\nimplement the same logical operator (a join). However, un-\nlike one-hot encodings, it is not trivial to construct such an\nembedding. Thus, we ask a natural question: can we au-\ntomatically transform the one-hot, sparse, easily-engineered,\ninformation-anemic representation into a dense, information-\nrich representation useful for machine learning algorithms?\nContext-aware representations In order to facilitate learn-\ning a dense, information-rich representation from an easy-\nto-construct one-hot encoding, the notion of context is of-\nten leveraged [36, 42, 43, 51]. For instance, word vectors , in-\nvented to transform words into vectors, is a way to take\nadvantage of the context that a word appears in to repre-\nsent that word as a vector. For example, in the sentence\n\\Long live the !\", we except to see a word like \\king\"\nor \\queen\", as opposed to a word like \\apples\" or \\bananas.\"\nHowever, in the sentence \\ are a tasty fruit.\", the con-\nverse is true: one would expect \\apples\" or \\bananas\" in-\nstead of \\king\" or \\queen.\"\nIn a query processing context, if we know that the child\nof an unknown unary operator is a scan operator reading\nphone number data, we know it is unlikely that the unknown\noperator is an average aggregate (as the numerical average\nof a set of phone numbers is nonsensical). If we know that\nboth the children of an unknown binary operator are sort\noperators, we know the operator is more likely to be a merge\njoin than a hash join (a query optimizer would have little\nreason to sort the input to a hash join operator). Hence, in\nthe next paragraphs, we demonstrate how we can build these\ncontext-aware embeddings via deep neural network (DNNs).\n2.1 Context-aware Learning via DNNs\nContext and deep neural neworks (DNNs) can be lever-\naged to learn a semantically-rich, low-dimensional vector\nspace from sparse, one-hot vectors [42]. At a high level, our\napproach works by training a neural network to predict the\ncontext (children) of an operator. In practice, this neural\nnetwork maps one-hot representations of query operators to\none-hot representations of their children. Once trained, the\ninternal representation learned by the neural network is used\nas an operator embedding.\nLearned \nembedding\nPrediction layerOne-hot encoded context vector\nHidden layersInput layerOne-hot encoded input vectorFigure 2: Hourglass embedding neural network\nTo facilitate the discussion in the next paragraphs we \frst\nprovide necessary background on neural networks.\nNeural networks Deep neural networks are composed of\nmultiple layers of nodes, in which each node takes input\nfrom the previous layer, transforms it via a di\u000berentiable\nfunction and a set of weights , and passes the transformed\ndata to the next layer. Thus, the output of each node can\nbe thought of as a feature : a piece of information derived\nfrom the input data. As you advance into the subsequent\nlayers of the neural net, nodes begin to represent more com-\nplex features, since each node aggregates and recombines\nfeatures from the previous layer. Because each node applies\na di\u000berentiable transformation, and thus the entire network\ncan be viewed as applying a single complex di\u000berentiable\ntransformation, neural networks can be trained using gra-\ndient descent [52]. This training is done by de\fning a loss\nfunction , a function that measures the prediction accuracy\nof the neural network on the desired task, and then using\ngradient descent to modify the parameters (i.e., weights) of\nthe neural network to minimize this loss function. Unlike\nmost traditional machine-learning algorithms, deep neural\nnetworks learn complex features automatically , without hu-\nman intervention. Readers unfamiliar with deep neural net-\nworks may wish to see [54] for an overview.\nHour-glass DNN architecture Next, we discuss how deep\nneural networks can be used to learn context-aware operator\nembeddings. Our framework takes advantage of contextual\nrelationships between query operators. The critical compo-\nnent is a deep neural network architecture that is trained to\npredict the context of a particular entity. For example, in\nthe domain of query processing, the neural network, given\na merge join operator, would be trained to predict the two\nsorted inputs (details in Section 4).\nIntuitively, this process works by \frst training a neural\nnetwork to map a simple, one-hot encoding of a query op-\nerator to some contextual information about that operator:\nfor example, the network would be trained to map a one-hot\nrepresentation of merge join operator to the one-hot repre-\nsentation of the merge join's two (sorted) inputs. Here, there\nis no separate process needed apart from training the neural\nnetwork for this auxiliary task. Once the network is trained,\neach hidden layer outputs combinations of its input features\nthat are useful for this predictive task. Hence, these outputs\ncan be seen an embedding of the (sparse) input vectors of\nthe neural network.\nTo ensure that the \fnal hidden layer of the network o\u000bers\nan output (an embedding) that is dense and information-\n3\n\nrich, the architecture of this neural network is designed with\naninformation bottleneck . We use an \\hour-glass\" shaped\nnetwork, depicted in Figure 2. Here, one-hot vectors of op-\nerator information are fed into an input layer and then pro-\njected down into lower and lower dimensional spaces through\na series of hidden layers. Then, a very low dimensional layer\nis used to represent the learned embedding. At the end of the\nnetwork, a \fnal prediction layer is responsible for mapping\nthe low-dimensional learned embedding into a prediction of\nthe context of the input operator's feature vector.\nDuring training, the neural network strives to identify\ncomplex transformations of its input that improve the ac-\ncuracy of its output prediction. Implicitly, this forces the\nneural network to learn a representation (an embedding) of\nthe input one-hot vectors that can be used to predict the cor-\nrect operator context at the \fnal layer. After the network\nis trained, the prediction layer is \\cut o\u000b\", resulting in a\nnetwork which takes in an one-hot encoded, sparse vector\nand outputs a lower-dimensional embedding. Because this\nlow-dimension learned embedding was trained to be useful\nfor predicting the context of the operator, we know it is\ninformation-rich. The low-dimension layer representing the\nlearned embedding serves as an information bottleneck, forc-\ning the learned representation to be dense.\nOf course, precisely predicting the context of a query oper-\nator is an impossible task. After all, the children of a merge\njoin operator will not always be sort operators. Since ulti-\nmately we will cut o\u000b the \fnal prediction layer, we do not\ncare much about its accuracy, as long as its output some-\nwhat matches the distribution of potential contexts. For\nexample, when fed a merge join operator, the network pre-\ndicts a higher likelihood that the children of the merge join\noperator were sort operators than hash operators. Once\ntrained to have this property, the resulting network (with\nthe prediction layer cut o\u000b), serves as a mapping from a\nsparse, information-anemic, easy-to-engineer representation\nto a dense, information-rich representation.\nThe rest of this paper explains how we employ this learned\nembedding framework to automate and custom-tailor the\nfeature engineering process to a particular database, and\nhow the output of our feature engineering process can be\nuseful for a number of data management tasks.\n3. LEARNING FRAMEWORK OVERVIEW\nLearning database-speci\fc operator embeddings is the \frst\nstep towards automating the feature engineering process.\nOnce operator embeddings are obtained, an o\u000b-the-shelf ma-\nchine learning algorithm can utilize them as an input for a\nspeci\fc task. Next, we provide a brief overview of our frame-\nwork before describing its technical details in Section 4.\nOur proposed framework operates in three phases, de-\npicted in Figure 3. In the \frst embedding learning step, an\noperator embedding is learned by training the \\hour-glass\"\nneural network. This training uses previously-observed query\nplans (i.e., operator trees) from a particular database, allow-\ning the network to learn to predict an operator's children\n(i.e., context). In the task-based model training phase, the\nlearned embeddings are used to train an o\u000b-the-shelf ma-\nchine learning model for a speci\fc task (i.e., cardinality error\nestimation). Speci\fcally, the trained cut-o\u000b neural network\nis used to map the training set for this task into the dense\nembedded vector space and these dense vectors are fed into\na traditional machine learning model to train the model for\nSample \nWorkloadEmbedding\nLearningTask-Based\nModel Training\nOp\nOp\nOpEmbedded \nVectors (labeled)Traditional \nML ModelRuntime\nPredictions\nTraditional \nML ModelHourglass-shaped DNN\nEmbedded \nVectors \n(to be labeled)\nUnlabeled\nOperatorsOp\nOp\nOpOp\nOp\nOp\nPredicted LabelsLabeled\nOperatorsFigure 3: Three phase learning framework\nthe particular task. Finally, at runtime, observed query op-\nerators are embedded through the cut-o\u000b neural network\nand the embedded output is sent through the traditional\nmachine learning model, resulting in a prediction for the\nparticular task and for the observed operator.\nEmbedding learning This \frst phase focuses on learning\nthe operator embedding for a particular database workload.\nHere, we assume the availability of a long history of executed\nqueries from the database, which we call a sample workload.\nThis sample workload can be extracted from database logs\nor through other means. Many DBMSs automatically store\nsuch logs for auditing and debugging purposes. The only\ninformation needed from these queries are their query plans,\nwhich allows one to extract information about each plan's\nquery operators, e.g., the type of operator, the expected\ncost of the operator according to the optimizer's cost model,\netc.1However, these queries need no additional annotation\nor tagging, i.e. they do not have to be hand-labeled with\nany particular piece of information.\nIn this phase, we train a deep neural network with an\nhour-glass architecture (orange in Figure 3). The \frst layer\ntakes as input any available piece of information about a\nquery operator in the training set. The subsequent, hidden\nlayers of the network slowly decrease in size until the penulti-\nmate layer of the network, called the embedding layer . After\nthe embedding layer, an additional prediction layer is added,\nresponsible for taking the information from the embedding\nlayer and predicting the input operator's children. The em-\nbedding layer, the smallest layer of the network, serves as an\ninformation bottleneck, forcing the neural network to learn\na compact representation of the input operators.\nGiven the trained neural network, the last step of the em-\nbedding learning stage is to \\cut o\u000b\" the \fnal prediction\nlayer of the network. This results in a network that maps\nan input operator to a low-dimensional vector, i.e. the em-\nbedding layer becomes the output layer of the network.\nTask-based model training During the model training\nphase, depicted in the second row of Figure 3, the learned\nembeddings are used to train an o\u000b-the-shelf machine learn-\ning model from a small training set. For example, suppose\none is interested in training a machine learning model to pre-\n1This information is made available in many DBMSes via\nEXPLAIN queries.\n4\n\ndict cardinality estimation errors, as are common in queries\nwith many joins [31]. One would then collect a small train-\ning set consisting of query operators and labels specifying\nwhether or not the estimated cardinality of the operator was\ntoo high or too low (these labels are represented with green\nand purple in Figure 3). These collected operators are then\nencoded into one-hot sparse vectors. These sparse vectors\nare ran through the hour-glass neural network, generating\none dense vector for each operator. These generated vec-\ntors, which can be thought of as embedded versions of the\nlabeled operators, are dense and low-dimensional. These\nvectors, and their corresponding labels, are then used to\ntrain an o\u000b-the-shelf machine learning model (e.g., logistic\nregression [21], random forest [10]) to predict whether the\ncandinality estimation of a particular operator will be too\nhigh or too low.\nRuntime Predictions At runtime, the specialized model\ntrained in the previous step is used on new, unlabeled op-\nerators. First, these previously-unseen operators are sent\nthrough the cut-o\u000b neural network. This produces an em-\nbedded dense vector which can be classi\fed by the tradi-\ntional machine learning model trained in the previous stage.\nFor example, the specialized model may classify the new\noperator as having an under or over estimated cardinality.\nThe \\Runtime Prediction\" stage in Figure 3 shows an exam-\nple of this process, where an initially unlabeled (uncolored)\noperator is fed through the embedding network to produce\nan embedded vector, which is subsequently classi\fed by the\ntraditional ML model. The result of this classi\fcation can\nthen be utilized by the DBMS, e.g. if the specialized model\npredicts that a particular operator's cardinality has been un-\nderestimated, the DBMS could perform sampling, or replace\nthe particular operator with one less sensitive to cardinality\noverestimations (e.g. replace a loop join with a hash join).\n4. OPERATOR EMBEDDINGS TRAINING\nIn this section, we give a technical description of the em-\nbedding learning, task-based model training, and runtime\npredictions phases of the framework we discussed above.\n4.1 Embedding Learning\nThe \frst phase of our framework involves the training of\nthe embedding network. During this step, we train a neural\nnetwork to emit operator embeddings using a large history of\npreviously-executed queries. Let us assume a sample query\nworkloadWfrom the target database. We treat each query\noperatorx2Was a large, sparse vector containing infor-\nmation about xsuch as the operator type, join algorithm,\nindex used, table scanned, column predicates, etc. Categor-\nical variables are one-hot encoded, and vector entries cor-\nresponding to properties that a certain operator does not\nhave are set to zero, e.g. for table scan operators, the vector\nentry for \\join type\" is set to zero.\nWe train our embedding network to predict the children,\nC1(x) andC2(x), of a given query operator x2W. If a\nquery operator has no children, we de\fne both C1(x) and\nC2(x) as the zero vector, denoted ~0, and if a query operator\nhas only one child, we de\fne C2(x) to be~0 andC1(x) to\nbe that child. Query operators with more than two children\nare uncommon, and can either be ignored or truncated (e.g.,\nignore the 3rd child onward).\nGiven a sample workload W, we next construct an hour-\nglass shaped neural network, as shown in Figure 4, which\nTscnIscnMJ\nMJTscn\nIscnInput LayerHiddenLayers\nEmbedding \nLayerOutput Layer\nOutput Layer EncoderFigure 4: Training the embedding\nwe train to learn a mapping from each parent to its chil-\ndren. The initial input layer for the embedding network is\nas large as the sparse encoding of each query operator. The\nsubsequent layers, known as hidden layers, map the output\nof the previous layer into smaller and smaller layers, cor-\nresponding with lower and lower dimensional vectors. The\npenultimate layer, the embedding layer , is the smallest layer\nin the network. Collectively, these layers are referred to as\ntheencoder . Immediately following the embedding layer are\nthe output layers, which attempt to map the dense, compact\nrepresentation outputted by the embedding layer to the pre-\ndiction targets, i.e. the children of the input operator.\nWe refer to the encoder as a function Enc\u0012(\u0001) parame-\nterized by the network weights \u0012, and we refer to P1\n\u0012(\u0001) and\nP2\n\u0012(\u0001) as the functions representing the output layers for pre-\ndicting the \frst and second child of the input operator from\nthe output of the encoder, respectively. Thus, the embed-\nding network is trained to by adjusting the weights \u0012to\nminimize the network's prediction error:\nmin\n\u0012X\nx2WE(P1\n\u0012(Enc\u0012(x));C1(x)) +E(P2\n\u0012(Enc\u0012(x));C2(x))\nwhereE(\u0001;\u0001) represents an error criteria. For vector ele-\nments representing scalar variables (such as cardinality and\nresource usage estimates), we use mean square error, e.g.\nE(x;y) = (x\u0000y)2. For vector elements representing categor-\nical variables (such as join algorithm type, or hash function\nchoice), we use cross entropy loss [30].\nBy minimzing this loss function via stochastic gradient de-\nscent [52], we train the neural network to accurately predict\nthe contextual information (children) of each input operator.\nIt is important to note that the network will never achieve\nvery high accuracy { in fact, it is quite likely that the same\noperator has di\u000berent children in di\u000berent queries within the\nsample workload, thus making perfect accuracy impossible.\nWhen this is the case, the best the network can do is match\nthe distribution of the operator's children, e.g. to predict\nthe average of the correct outputs (as this will minimize the\nloss function), which still requires that the narrow learned\nembedding contain information about the input operator.\nThe point is not for the embedding network to achieve a\nhigh accuracy, but for the network to learn a representation\nof each query operator that carries a semantic information.\nIntuitively, the learned representation is information-rich\nand dense: the narrow embedding layer forces the neural\nnetwork to make its prediction of the operator's children\nbased on a compact vector, so the network must densely\n5\n\nEmbedded VectorsLabeled \nTraining DataTask-based \nModel \nEncoderFigure 5: Task-based model\nencode information in the embedding layer in order to make\naccurate predictions, and thus minimize the prediction error.\nSince the network can make accurate predictions about the\ncontext of a query operator given the dense encoding, the\ndense encoding must be information-rich.\nExample Figure 4 shows an example of how the embedding\nnetwork is trained for a single operator: a merge join (MJ)\nwith two children, a table scan (Tscn) and an index scan\n(Iscn). The network is trained so that when the sparse,\nvectorized representation of the merge join operator is fed\ninto the input layer, the encoder (which contains a sequence\nof layers with decreasing sizes) maps the sparse vector into a\ndense, semantically rich output, which is \fnally fed into the\noutput layers to predict the merge join's children, the table\nscan and the index scan. Because the \fnal output layers\nmust make their prediction of the input operator's children\nbased only on the small vector produced by the encoder,\nthe output of the encoder must contain semantic contextual\ninformation about the input operator.\n4.2 Task-based Model Training\nAfter a good encoder Enc\u0012has been trained, a user can\nidentify a task (e.g. cardinality estimation error prediction)\nand build a small training set of labeled operators. For\nthe cardinality estimation error prediction task, these labels\nwould indicate whether or not an operator's cardinality was\nunder or over estimated. We denote each operator in the\ntraining set at x2T, with the label of xbeingLabel (x).\nUsing the embedding network, the user can transform each\nlabeled query operator into a labeled vector, e.g. for any\nx2T, we compute Enc\u0012(x). These input vectors Enc\u0012(x)\nand their labels Label (x) can be used as a training set for a\ntraditional, o\u000b-the-shelf ML model such as a logistic regres-\nsion or a random forest.\nWe denote this o\u000b-the-shelf model with parameters \u001eas\nM\u001e, and note that, in general, Mwill be trained to minimize\nthe model's classi\fcation error:\nmin\n\u001eX\nx2TE(M\u001e(Enc\u0012(x));Label (x)) (1)\nVirtually any machine learning model can be used in place\nofM\u001e, and the corresponding learning algorithm can be used\nto \fnd a good value for \u001e(e.g. random forest tree induc-\ntion for building a random forest model [10], or gradient\ndescent for \fnding coe\u000ecients for a logistic regression [21]).\nBecause the vectors produced by a well-trained encoder are\ninformation-rich and dense, operators with interesting prop-\nerties (such as those with cardinality estimation errors) may\nbe somewhat linearly separable, allowing for fast, simple\nmodels like logistic regression to be used with reasonable\naccuracy. We demonstrate this experimentally in Section 5.Training set size One extra advantage of using the embed-\nded operator vectors as input to traditional machine learn-\ning models is that it reduces the amount of data required to\ntrain a e\u000bective model compared to training a model directly\non labeled sparse operator vectors. While many deployed\ndatabase systems have a large log of executed queries avail-\nable, acquiring labeled query operators, i.e. query operators\nthat have been tagged (possibly by hand) with the addi-\ntional piece of information one wishes to predict, is generally\nmore di\u000ecult than analyzing logs. For example, for the car-\ndinality estimation error prediction task, acquiring a large\nnumber of query plans from logs is straight-forward, but\ndetermining whether the optimizer under or over-estimated\nthe cardinality for each operator requires re-executing the\nquery and recording the estimated and actual cardinalities.\nClearly, acquiring this information by re-executing the entire\nquery log is untenable.\nBy training the embedding network to predict the context\nof a query operator using the large supply of easily-acquired\nunlabeled data, we ensure that the embedded operator vec-\ntors contain information about patterns in the query work-\nload. Using these embedded vectors to train a traditional\nmachine learning model removes the need for the tradi-\ntional model to re-learn workload-level information, and can\nthereby achieve strong performance without a large supply\nof labeled training data.\nExample An example of task-speci\fc model training for\nthe cardinality error estimation task is depicted in Figure 5.\nHere, a small set of training data is represented by squares\nin the left side of the \fgure. The color corresponds to the\nprediction target: whether or not the cardinality estimate\nfor the operator was too high or too low. The training set\nis then fed through the learned embedding. The scatterplot\non the right side of Figure 5 depicts the resulting dense,\ninformation-rich vectors (in this case, of dimension 2). A\nresulting classi\fer can be trained to \fnd a simple linear\nboundary (e.g., logistic regression, perceptron, SVM) which\nnearly-perfectly separates the two classes.\n4.3 Runtime Prediction\nAt runtime, an unlabeled operator xcan be ran through\nthe encoder Enc\u0012(x), then through the traditional ML model,\nM\u001e(Enc\u0012(x)), in order to produce a predicted label for x.\nThe resulting classi\fcation decision can be used by the DBMS.\nFor example, for the cardinality error estimation task, if the\nmodelM\u001epredicts that an operator x's cardinality has been\nunderestimated, the DBMS could perform additional sam-\npling, refuse to run the query, or ask the user for con\frma-\ntion. In a more advanced setting, the model's prediction\ncould be combined with a rule-based system to avoid catas-\ntrophic query plans, e.g. if the model predicts that a loop\njoin operator has an underestimated cardinality, replace that\nloop join with a hash join.\nWe note that because this inference is happening during\nquery processing, inference time matters. As a result, users\nmay wish to select a model with su\u000ecently fast inference\ntime for their application: if model inference time is too\nhigh, the net e\u000bect on the system may be negative, even if\nthe model provided accurate predictions. We experimentally\nanalyze the inference time required by the encoder, and the\nencoder combined with various o\u000b-the-shelf machine learn-\ning models, in Section 5.3.\n6\n\n5. EXPERIMENTAL ANALYSIS\nHere, we present an experimental analysis of the operator\nembedding framework. First, we analyze the embeddings\nthemselves, investigating the properties of the learned vec-\ntor space that operators are embedded into. This analysis\nallow us to build an intuition for why the learned embedding\napproach o\u000bers e\u000bective feature vectors for use with tradi-\ntional machine learning algorithms. Then, we measure the\ne\u000bectiveness and e\u000eciency of \rexible operator embeddings\nfor several data management tasks.\nNeural network setup Unless otherwise stated, our hour-\nglass embedding network generates embeddings of size 32,\nmeaning that query operators are mapped into 32 \roating-\npoint numbers. This neural network has six hidden layers\n(of size 256, 256, 128, 128, 64, and 64 nodes). Layer nor-\nmalization [7] (a technique to stabilize the training of neural\nnetworks) and ReLUs [18] (an activation function) are used\nafter all layers except for the output layer. The network\nwas trained using stochastic gradient descent [52] over 100\nepochs (passes over the training data). The embeddings\nwere trained using a GeForce GTX TITAN GPU and the\nPyTorch [48] deep learning toolkit.\nDatabase setup Unless otherwise stated, all queries are\nexecuted using PostgreSQL 10.5 [1], running on Linux kernel\n4.18. PostgreSQL was ran from inside a virtual machine\nwith 8 GB of RAM, a con\fgured bu\u000ber size of 6GB, and\ntwo virtualized CPU cores. The underlying machine has\n64GB of RAM and a Intel Xeon E5-2640 v4.\nInput vectors setup Queries are initially encoded into a\nsparse representation based on features we extracted from\nthe output of PostgreSQL's EXPLAIN functionality. The num-\nber of features (i.e., the size of the input sparse vectors)\ndepends on the dataset, as di\u000berent datasets have di\u000berent\nnumbers of relations, attributes, etc. and thus have di\u000berent\nsized sparse input vectors. These features vary between 280\nand 478, and contain information such as the optimizer's es-\ntimated cost (for all operators) and the expected number of\nhash buckets required (for hashing operators). Details about\nthe initial sparse encoding can be found in Appendix A.\nDataset We conducted our experiments over one synthetic\nand two real-world datasets, which were provided by a large\ncorporation on the condition of anonymity:\n\u000fTPC-DS Workload : this workload includes 21,688\nTPC-DS query instances generated from the 70 TPC-\nDS templates that are compatible with PostgreSQL\nwithout modi\fcation. These queries were executed on\na TPC-DS dataset with scale factor of 10, resulting\nin a total database size of 25GB. We have made ex-\necution traces of these queries publicly available2for\nreplication and analysis.\n\u000fOnline Workload : we also used a real-world, online\nworkload extracted from execution logs of large corpo-\nration. The dataset contains 8,000 analytic (OLAP)\nqueries sent by 4 di\u000berent users in a 48-hour period.\nThe total database size is 5TB, and the average query\nreads approximately 300GB of data.\n\u000fBatch Workload : \fnally we used a real-world, batch\nworkload executed weekly at a large corporation for\nreport generation. The dataset contains 1,500 analytic\n2http://cs.brandeis.edu/~rcmarcus/tpcds.tar.xz(OLAP) queries sent by 98 di\u000berent users. The total\ndatabase size is 2.5TB, and the average query reads\napproximately 350GB of data.\n5.1 Analysis of Operator Embeddings\nFirst, we explore and analyze the embeddings learned by\nthe hourglass neural network. This is the output of the \frst\nphase of our framework. Here, we trained the neural network\non the query plans we collected from the TPC-DS queries.\nWe then ran all the query operators present in those query\nplans through the trained embedding network, resulting in\none 32-dimensional vector for each operator. We refer to\nthese as the embedded operators .\nVisualizing embedded operators To visualize the em-\nbedded operators, we used the t-Distributed Stochastic Neigh-\nbor Embedding (t-SNE) [63] technique. The t-SNE tech-\nnique projects high dimensional vector spaces (in our case,\nthe 32-dimensional embedding space), into lower dimensional\nspaces (2D, for visualization), while striving to keep data\nthat is clustered together in the high dimensional space clus-\ntered together in the low dimensional space as well.\nFigure 6a shows the t-SNE of the 32-dimensional TPC-\nDS operator vectors. Note that the X and Y values of a\nparticular point have no meaning on their own { the t-SNE\nonly attempts to preserve clusters in the higher dimensional\nspace. On its own, Figure 6a does not give much insight\ninto the shape of the learned embedding. However, several\nclusters and shapes appear, which we analyze next.\nGroupings in the t-SNE To con\frm that the learned em-\nbedding carries information about query operator semantics,\nwe next color each embedded operator based on the accu-\nracy of PostgreSQL's cardinality estimate. In Figure 6b,\noperators for which the optimizer correctly estimated the\ncardinality within a factor of two are colored gray (Cor-\nrect). When the cardinality is overestimated by at least a\nfactor of two, the operator is colored blue (Over). When the\ncardinality is underestimated by at least a factor of two, the\noperator is colored red (Under).\nWe make two observations about Figure 6b.\n1. Even in the 2D space used for visualization, there are\nmany apparent clusters of cardinality under and over\nestimations. This demonstrates that the operator em-\nbedding { which was trained with no knowledge of car-\ndinality estimation errors { still learned an embedding\nthat preserves semantic information about the query\noperators. In other words, by learning an embedding\nuseful for predicting the context (children) of an oper-\nator, the neural network learned to embed query op-\nerators into a vector space with semantic meaning.\n2. The fact that operators with cardinality estimation er-\nrors are clustered together indicates that a machine\nlearning model should be able to learn underlying pat-\nterns in the embedded data (e.g., the clusters) and\nmake useful predictions. We investigate this directly\nin Section 5.2.2.\nWe also colored the t-SNE plot by operator latency, as\nshown in Figure 6c. Here, we color each query operator\nbased on the percentile of its latency, so that the fastest\nquery operator, the 0th percentile, is colored with dark blue\nand the slowest query operator, the 100th percentile, is col-\nored with yellow. When comparing Figure 6b and Figure 6c\n7\n\n(a) Embedding under t-SNE\n (b) Cardinality errors\n (c) Query latency\nFigure 6: t-SNE plots of a learned embedding for TPC-DS (best viewed in color)\nwe can observe, unsurprisingly, that many of the slowest\nquery operators correspond with cardinality underestima-\ntions, possibly resulting in spills to disk or suboptimal join\norderings. Figure 6c demonstrates similar behavior to the\nprevious plot. Long-running query operators tend to be\nneighbors with other long-running query operators, form-\ning clusters in the 2D visualization. These clusters demon-\nstrate that the learned embeddings carry semantic informa-\ntion (e.g., information related to operator latency), which\ncan be taken advantage of by a machine learning model.\n5.2 Operator Embeddings Effectiveness\nNext, we evaluated the task-based model training com-\nponent of our framework. Here, we trained a number of\nmachine learning models using our learned operator embed-\ndings for a number of di\u000berent data management tasks, and\nwe evaluate the e\u000bectiveness of this models. These models\nare trained using a labeled set of a query operators for a\nparticular task. To show the \rexibility of our approach, we\nmeasure the performance of the learned embeddings across\nthree di\u000berent tasks:\n1.Query admission : the model is trained to predict\nwhether a particular query contains an operator that\nwill take an extraordinary amount of time to execute.\nThe decision is used to admit or reject the query.\n2.Cardinality boosting : the model is trained to pre-\ndict whether the cardinality estimate of a particular\noperator's output is too high, too low, or correct.\n3.User identi\fcation : the model is trained to identify\nthe user that submitted a particular query. One appli-\ncation of such a model is to test for outlier queries.\nFor each task, we used a number of o\u000b-the-shelf machine\nlearning models: (1) logistic regression, (2) random for-\nest [10] ( RF) with 100 trees, (3) k-nearest neighbors [11]\n(kNN ) con\fgured to account for 6 neighbors using weighted\ndistance (the best value found after an extensive hyper-\nparamter search), and (4) support vector machines [20] ( SVM ).\nIn order to demonstrate that task-based models can be trained\nwith relatively little training data, each model is evaluated\nusing 5-fold cross validation, in which one-\ffth of the data\nis used for training and four-\ffths are used for testing at\na time (the \fnal number reported is thus the median of 5\nruns). For the TPC-DS dataset, cross validation folds are\nchosen based on query templates, so that the query tem-\nplates in each training set are distinct from queries in eachtesting set. For the online workload dataset, training and\ntesting sets are chosen so that all queries in the training set\nprecede all queries in the testing set (i.e., the training set\nrepresents the \\past\", and the test set represents the \\fu-\nture\"). For the batch workload dataset, folds are chosen\nusing uniform random sampling without replacement.\nWe compare the performance of each trained machine\nlearning model when trained using a number of di\u000berent\ninput feature vectors. The feature engineering techniques\nwe used for extracting these vectors are the following:\n1.Sparse : this is the raw, unreduced sparse encoding\ntaken directly from PostgreSQL's EXPLAIN functional-\nity (see Appendix A for a listing).\n2.Neural : this is the automatically-learned operator\nembeddings generated using the \\hourglass\" neural\nnetwork approach presented here.\n3.PCA : here we use feature vectors from the 32 lead-\ning principal components (vectors that explain a high\npercentage of the variance in the data) of the origi-\nnal sparse vectors. These components are found by\nperforming (automated) principal components analy-\nsis [23] on the sparse input vectors.\n4.FA: this is an automatic feature engineering process\nthat uses feature agglomeration [50], a technique simi-\nlar to hierarchical clustering. This technique builds up\nfeatures by combining redundant (measured by corre-\nlation) features in the sparse input vectors together\nuntil the desired number of features (32) is reached.\n5.2.1 Query Admission Task\nFor the query admission task, we trained various machine\nlearning models to predict whether or not any operator in\na query plan would fall above or below the 95th-percentile\nfor latency. In other words, the model tries to predict if any\noperator in a given query plan will take longer than 95th% of\npreviously seen operators or not. To do this, we applied the\ntrained model on each operator in an incoming query, and\nif the model predicts that anyoperator in the query would\nexceed the 95th percentile threshold, the query is \ragged.\nDBMSes may wish to reject such \ragged queries, or prompt\nthe user for con\frmation before utilizing a large amount of\n(potentially shared) resources to execute them [60,66,68].\nFigure 7 shows the average 5-fold cross validation accu-\nracy for each model on all three datasets. The red line at\n0:75 represents the prior (i.e., since 75% of queries do not\n8\n\n 0.5 0.6 0.7 0.8 0.9 1\nLogistic RF kNN SVMAccuracy (2 class)\nModelNeural\nFAPCA\nSparsePrior(a) TPC-DS\n 0.5 0.6 0.7 0.8 0.9 1\nLogistic RF kNN SVMAccuracy (2 class)\nModelNeural\nFAPCA\nSparsePrior (b) Online Workload\n 0.5 0.6 0.7 0.8 0.9 1\nLogistic RF kNN SVMAccuracy (2 class)\nModelNeural\nFAPCA\nSparsePrior (c) Batch Workload\nFigure 7: Query admission prediction accuracy for di\u000berent models and feature vectors. The models predict if a query does\nor does not contain an operator with latency above the 95th percentile.\nhave an operator exceeding the 95th latency percentile, a\nmodel that always guessed this most common class would\nachieve an accuracy of 75%).\nFor all three datasets, the specialized models trained using\nthe Neural input (the learned operator embeddings) outper-\nformed models trained with the other input vectors. While\nthe combination of the operator embeddings (Neural) and\nlogistic regression saw the highest performance and outper-\nformed other combinations across all three datasets by 10-\n15%, the operator embeddings (Neural) also produced the\nmost e\u000bective models compared with any other feature engi-\nneering approach. The only exception is the SVM model on\nthe online workload, where the model trained on the Neural\ninputs was within 5% of the most e\u000bective model.\nSurprising performance of logistic regression For the\nquery admission task (and the other tasks analyzed next),\nthe combination of the learned embeddings with a logistic\nregression classi\fer is surprisingly e\u000bective (97% accuracy\nfor TPC-DS). This is due to the fact that the logistic re-\ngression model has a very similar mathematical form to the\ncross-entropy and mean-squared error loss functions used\nto train the neural network. Training a logistic regression\nmodel on the embedded data is equivalent to re-training\nthe last layer of the embedding network for a di\u000berent pre-\ndiction target, a technique called knowledge distillation or\ntransfer learning, which has been shown to be extremely ef-\nfective [8,69]. This also explains the large gap (15% - 18%)\nbetween the performance of logistic regression when using\nthe Neural featurization and with the other featurizations.\n5.2.2 Cardinality Boosting Task\nNext, we trained various machine learning models to pre-\ndict whether or not the PostgreSQL query optimizer's car-\ndinality estimate would be correct, too high (by at least a\nfactor of two), or too low (by at least a factor of two). We\ncall this task cardinality boosting because of the similarities\nto \\boosting\" techniques in machine learning [53], i.e. using\none model to predict the errors of another model. Providing\ngood cardinality estimates is extremely important for query\noptimization [31], and many recent works have applied ma-\nchine learning to this problem [26,33,45].\nFigure 8 shows the performance of the trained machine\nlearning models for the cardinality boosting task across each\ndataset with each feature engineering technique. Again, the\ncombination of the Neural featurization and logistic regres-\nsion produced the most e\u000bective model for all three datasets,\nachieving over 93% accuracy on the TPC-DS dataset. Whilethe gap between the Neural featurization and the other fea-\nturizations appears signi\fcantly higher for the cardinality\nboosting task than the query admission task, this is at-\ntributable to the change in the prior (i.e., 75% for the query\nadmission task and strictly less than 50% for the cardinality\nboasting task).\nThe strong performance of the Neural featurization is well-\nexplained by the t-SNE plot in Figure 6b. Since we know\nthe learned embedding is clustering operators with similar\ncardinality estimation errors together, it is not surprising\nthat machine learning models can \fnd separation bound-\naries/cluster centers within the data.\n5.2.3 User Identiﬁcation Task\nThe last task we evaluate is user identi\fcation. In this\ntask, the model's goal is to predict the user who submit-\nted the query containing a particular operator. While the\nDBMS generally knows the user submitting a query, such\na model is useful for determining when a user-submitted\nquery does not match the queries usually submitted by that\nuser, a common learning task in database intrusion detec-\ntion [6,19,25,35] or query outlier detection [56].\nFor the TPC-DS data, we use the query template as the\n\\user\" for a each query (and therefore we use random cross-\nvalidation folds). For the online and batch datasets, user\ninformation was provided by their respective corporations.\nThe online dataset contains queries submitted from 4 users,\nwhereas the batch dataset contains queries submitted by 98\nusers. In all cases, the number of queries submitted by each\nuser is approximately equal. Figure 9 shows the results.\nThe TPC-DS and batch workloads show an extremely\nlarge gap between the logistic regression performance us-\ning the Neural featurization and the other featurizations,\nagain because of the signi\fcantly lower prior. Overall, the\nNeural featurization outperforms all other baselines, demon-\nstrating that the learned embedding contains rich, semantic\ninformation about each query operator.\nTwo exception, however, are notable: \frst, for the TPC-\nDS dataset, the kNN model with the Sparse featurization\noutperforms the Neural featurization by approximately 1%\n(Figure 9a). This is due to certain operators { such as max\naggregates over speci\fc columns { appearing in only one\nquery template. Such an operator will have a very close\nneighbor in the sparse vector space, allowing the kNN model\nto easily classify it. This advantage, however, does not ex-\ntend to real-world data (e.g. Figures 9b and 9c), where such\nuniquely identifying operators do not exist.\n9\n\n 0.4 0.5 0.6 0.7 0.8 0.9 1\nLogistic RF kNN SVMAccuracy (3 class)\nModelNeural\nFAPCA\nSparsePrior(a) TPC-DS\n 0.4 0.5 0.6 0.7 0.8 0.9 1\nLogistic RF kNN SVMAccuracy (3 class)\nModelNeural\nFAPCA\nSparsePrior (b) Online Workload\n 0.4 0.5 0.6 0.7 0.8 0.9 1\nLogistic RF kNN SVMAccuracy (3 class)\nModelNeural\nFAPCA\nSparsePrior (c) Batch Workload\nFigure 8: Cardinality over/under estimation prediction accuracy for di\u000berent models and feature vectors. The models predict\nif the optimizer's cardinality estimation is within a factor of two, over by a factor of two, or under by a factor of two.\n 0 0.2 0.4 0.6 0.8 1\nLogistic RF kNN SVMAccuracy (70 class)\nModelNeural\nFAPCA\nSparsePrior\n(a) TPC-DS\n 0 0.2 0.4 0.6 0.8 1\nLogistic RF kNN SVMAccuracy (4 class)\nModelNeural\nFAPCA\nSparsePrior (b) Online Workload\n 0 0.1 0.2 0.3 0.4 0.5\nLogistic RF kNN SVMAccuracy (98 class)\nModelNeural\nFAPCA\nSparsePrior (c) Batch Workload (note y-axis scale)\nFigure 9: Query user identi\fcation prediction accuracy for di\u000berent models and feature vectors. The specialized model tries\nto predict the user that sent a particular query (number of classes varies by dataset).\nSecond, the random forest algorithm exhibits surprisingly\ngood performance using sparse inputs for the batch work-\nload (Figure 9c) { signi\fcantly better than the Neural fea-\nturization and random forest, although not as good as the\nNeural featurization and the logistic regression. The reason\nthe Sparse encoding works so well with the random forest\nmodel is due to the speci\fcs of the batch dataset: while most\nusers access every table, almost all users can be uniquely\nidenti\fed by the setof tables they accessed. The random\nforest algorithm, which builds a tree of rules based on dis-\ncrete splitting points, is especially well-suited to identifying\nthe table usage patterns of each user in the one-hot encod-\ning. We note, however, that the logistic regression combined\nwith the Neural feature vectors still outperformed all ran-\ndom forest models on this task.\n5.2.4 Impact of Embedding Size\nUp until this point, we have only used learned embeddings\nwith embedding layers of size 32, i.e. query operators are\nmapped into a vector space with 32-dimensions. Here, we\nevaluate changing this hyperparameter. Figure 10 shows\nhow the performance of various models change for the TPC-\nDS dataset when the size of the embedding is set to 64, 32,\n16, and 8 dimensions. We observed similar results for both\nthe online and batch workloads, but these plots are omitted\ndue to space constraints.\nGenerally, the performance of the size-64 embeddings and\nthe size-32 embeddings are nearly identical, indicating that\nadding additional dimensions beyond 32 to the embedding\nspace does not cause the embedding network to learn a bet-\nter compressed representation. On the other hand, the per-formance of all models drops signi\fcantly between the size-\n32 and size-16 embeddings, and again between the size-16\nembeddings and size-8 embeddings (best depicted in Fig-\nure 10c). This suggests that when the embedding size is\nsmaller than 32, the \\information bottleneck\" (see Section 4)\nbecomes too narrow, and the neural network is unable to\nlearn a su\u000eciently rich description of each query operator\nin such a small vector.\nThe ideal embedding size is hard to predict ahead of time,\nand although our experiments show that an embedding size\nof 32 or 64 provides good results on a number of datasets,\nwe suggest that users test several con\fgurations. Doing so\ncan be done in an automatic manner by training multiple\nembedding sizes, and then selecting the one that results in\nthe best cross-validated model performance.\n5.3 Runtime Efﬁciency\nAfter an embedding has been trained and a subsequent\nmachine learning model has been trained for a particular\ntask, the DBMS applies that model at runtime. Because the\nmodel is being applied at runtime, inference time matters, as\nwe do not want to unnecessarily slow down query processing.\nNext, we discuss the inference time when these models are\ntrained using our learned operator embeddings. We also\nperformed a Pareto analysis of the e\u000eciency (inference time)\nvs. the e\u000bectiveness (accuracy) of these models.\nInference time Figure 11a shows the amount of time it\ntakes to perform model inference on a single operator for\neach machine learning model we used and feature engineer-\ning technique. Note the log scale on the y-axis. The Sparse\nfeaturization has exceptionally high inference time, espe-\n10\n\n 0.5 0.6 0.7 0.8 0.9 1\nLogistic RF kNN SVMAccuracy (2 class)\nModel64 units\n32 units16 units\n8 unitsPrior(a) Query Admission\n 0.4 0.5 0.6 0.7 0.8 0.9 1\nLogistic RF kNN SVMAccuracy (3 class)\nModel64 units\n32 units16 units\n8 unitsPrior (b) Cardinality Boosting\n 0 0.2 0.4 0.6 0.8 1\nLogistic RF kNN SVMAccuracy (70 class)\nModel64 units\n32 units16 units\n8 unitsPrior (c) User Identi\fcation\nFigure 10: Accuracy of various models on di\u000berent tasks for the TPC-DS dataset, varying the size of the embedding layer.\n 0.01 0.1 1 10 100 1000\nLogistic RF kNN SVMInference time (ms)\nModelNeural\nFA\nPCA\nSparse\n(a) Inference time\n 0.7 0.75 0.8 0.85 0.9 0.95 1\n 0.1  1  10  100Accuracy (3 class)\nInference time (ms)Neural\nFA\nPCA\nSparse (b) Cardinality boosting, TPC-DS\n 0 2 4 6 8 10 12\nTPC-DS Online BatchTraining Time (m)\nDatasetGPU\nCPU (c) Training time\nFigure 11: Analysis of inference time, training time, and accuracy\ncially for kNN and SVM models, because these models re-\nquire measuring the distance between the input vector and\na large number of other points (for kNN, this could poten-\ntially be every point in the training set; for SVM, this could\nbe a large number of support vectors).\nThe PCA and FA featurizations have slightly lower infer-\nence time than the Neural featurization. This is because\nrunning the sparse feature vector through the multi-layer\ndeep neural network takes slightly longer than the simple\ndot product computations required for PCA and FA. How-\never, the di\u000berence is minimal ( <1ms). It is important\nto note that operator embeddings (and other dimensional-\nity reduction techniques) lead to faster inference time than\nthe original sparse vector encoding , because the embedding\nprocess is often asymptotically cheaper than the inference\nprocess. This is especially true for models such as SVM and\nkNN, which scale poorly with dimensionality { for exam-\nple, inference time using Sparse featurization with an SVM\nmodel (130ms) is almost an order of magnitude more than\nusing the Neural featurization (18ms).\nPareto analysis Of course, inference time is not the only\nimportant factor when considering a runtime model: ac-\ncuracy is obviously important as well. Here, operator em-\nbeddings' natural synergy with logistic regression (see Sec-\ntion 5.2.1) give the Neural featurization a massive advan-\ntage, as illustrated in the Pareto plot in Figure 11b.\nFigure 11b shows all the models trained for the cardinal-\nity boosting task for the TPC-DS dataset, plotted based\non their inference time (x-axis, log scale) and their testing\naccuracy (y-axis). The Pareto front, the models for which\nno other model is both faster andmore accurate, contains\nonly the logistic regression classi\fers using the Neural, FA,\nand PCA featurizations. While the logistic regression clas-si\fers using the FA and PCA featurizations have slightly\nfaster inference time (the Neural featurization still produces\nan inference time under 0.1ms), the accuracy of the model\nproduced by the Neural classi\fer is substantially higher: ap-\nproximately 94% versus 73%. Therefore, with the excep-\ntion of the two logistic regression models with signi\fcantly\nlower accuracy, we conclude that the models produced by the\nlearned operator embeddings are Pareto dominate in terms\nof inference time and accuracy.\n5.4 Training Time\nFinally, we analyze the time required to train the hourglass-\nshaped neural network used to represent an embedding. We\ncompare the training time required for both a CPU (Intel\nXeon E5-2640 v4) and a GPU (GeForce GTX TITAN). The\nresults are shown in Figure 11. The time to train the net-\nwork is a function of several parameters, the most notable\nbeing the size of the training set. Since our TPC-DS work-\nload has the most queries (21,688), it has a longer training\ntime than the online (8,000 queries) or batch (1,500 queries)\nworkloads. For TPC-DS, the largest workload, training time\non a CPU required 10 minutes, whereas on a GPU the train-\ning time dropped to 2.8 minutes.\nSince the learned embedding does not need to be fre-\nquently retrained (as demonstrated by the consistent model\nperformance on the online workload), we conclude that the\ntraining overhead of the proposed learned embeddings is man-\nageable even for systems without a GPU , although we note\nthat a GPU greatly accelerates training. Of course, training\ntime can be reduced by sampling from the available training\nset, and we leave studying the e\u000bects of such a strategy on\nmodel performance to future work.\n11\n\n6. RELATED WORK\nMachine learning in DBMSes A recent groundswell of\nwork has focused on applying machine learning to data man-\nagement problems. Recent works in intrusion detection [6,\n19], index structures [28], SLA management [15, 34, 39, 41,\n46, 47, 58], entity matching [44], physical design [38, 49],\nand latency prediction [2, 13, 14, 17, 32, 64{66] have all em-\nployed machine learning techniques. SageDB [59] proposes\nbuilding an entire database system around machine learning\nconcepts, including integrating machine learning techniques\ninto join processing, sorting, and indexing. With little ex-\nception, each of these works have included hand-engineered\nfeatures derived for each particular task, a arduous process.\nNotably, recent work has applied reinforcement learning\nto various problems in data management systems, includ-\ning join order enumeration [29, 37, 40], cardinality estima-\ntion [45], and adaptive query processing [24, 61, 62]. Re-\ncent work [26,33] has also used more traditional supervised\nlearning approaches, using a specialized neural network ar-\nchitecture, to perform join cardinality estimation. Predom-\ninately, these techniques are custom-tailored to a problem\nat hand, and while several of the systems mentioned utilize\ndeep learning (and thus learn useful features automatically\nfor their speci\fc task), they do not generally decrease the\nburden of applying machine learning to new problems in\ndata management.\nFeature engineering Recent works related to feature en-\ngineering, such as ZOMBIE [4], Brainwash [3], and Ring-\ntail [5] take a \\human-in-the-loop\" approach to feature en-\ngineering, assisting data scientists in selecting good features.\nThese techniques all seek to automate or shorten the pro-\ncess of evaluating the utility of a particular feature (e.g.,\noptimizing model testing), a time-consuming task in feature\nengineering. In contrast, our technique takes a completely\nautomatic approach, custom-tailoring a featurization to a\nspeci\fc database without any user interaction.\nAutomatic machine learning As machine learning grows\nmore popular and complex, recent research has also focused\non automating the entire machine learning pipeline [9,27,67].\nThese systems are generally designed for use by data science\npractitioners applying machine learning to external prob-\nlems, and assist data scientists with model selection, hyper-\nparameter tuning, etc. In contrast, our approach focuses on\nautomatically generating features for machine learning ap-\nplications within the DBMS and with generating features in\nanentirely automated fashion.\nLearned embeddings This work is not the \frst to ap-\nply learned embeddings to database systems. In [44], the\nauthors show how embeddings learned on the rows of a ta-\nble (as opposed to this work, which learns an embedding of\nquery operators) can be used for entity matching. In prelim-\ninary work [16], the authors present Termite, a system that\nhelps users navigate and explore heterogeneous databases\nby building a multi-faceted embedding on structured and\nunstructured data. In [56], the authors present a method\nfor embedding the text of SQL queries (the actual query as\nwritten by the user) using recurrent neural networks, and\nshow that the features learned are useful for various textual\ntasks (such as detecting syntax errors). Most researching\ninto embeddings have focused on natural language process-\ning [42, 57] or computer vision [22]. In contrast, our work\ndemonstrates the power of deep learning as a feature engi-\nneering tool for operator-level data management tasks.Feature PostgreSQL Ops Encoding Description\nPlan Width All Numeric Optimizer's estimate of output row width\nPlan Rows All Numeric Optimizer's cardinality estimate\nPlan Bu\u000bers All Numeric Optimizer's estimate of operator memory requirements\nEstimated I/Os All Numeric Optimizer's estimate of the number of I/Os performed\nTotal Cost All Numeric Optimizer's cost estimate for this operator, plus the subtree\nJoin Type Joins One-hot One of: semi, inner, anti, full\nParent Relationship Joins One-hot When the child of a join. One of: inner, outer, subquery\nHash Buckets Hash Numeric # hash buckets for hashing\nHash Algorithm Hash One-hot Hashing algorithm used\nSort Key Sort One-hot Key for sort operator\nSort Method Sort One-hat Sorting algorithm, e.g. \\quicksort\", \\top-N\", \\external sort\"\nRelation Name All Scans One-hot Base relation of the leaf\nAttribute Mins All Scans Numeric Vector of minimum values for relevant attributes\nAttribute Medians All Scans Numeric Vector of median values for relevant attributes\nAttribute Maxs All Scans Numeric Vector of maximum values for relevant attributes\nIndex Name Index Scans One-hot Name of index\nScan Direction Index Scans Boolean Direction to read the index (forward or backwards)\nStrategy Aggregate One-hot One of: plain, sorted, hashed\nPartial Mode Aggregate Boolean Eligible to participate in parallel aggregation\nOperator Aggregate One-hot The aggregation to perform, e.g. max, min, avg\nTable 1: Features used for naive encoding\n7. CONCLUSIONS\nWe have presented \rexible operator embeddings, an auto-\nmatic technique to custom-tailor a multi-purpose featuriza-\ntion to a particular database. Utilizing deep learning, our\ntechnique learns semantically rich, information dense em-\nbeddings of query operators automatically, and with min-\nimal human interaction. We have shown that our tech-\nnique produces features that can be utilized by simple, well-\nstudied, o\u000b-the-shelf machine learning models such as lo-\ngistic regression, and that the resulting trained models are\nboth accurate and fast.\nMoving forward, we plan to investigate additional applica-\ntions of operator embeddings, especially in parallel databases.\nWe are also considering new ways to integrate the embed-\nding training process into modern query optimizers, and if\nthere is any way the training process could exploit recorded\npartial execution information about past queries.\nAPPENDIX\nA. ENCODING FROM POSTGRESQL\nHere, we describe the naive, sparse encoding we derive\nfrom the PostgreSQL EXPLAIN output.\nTable 1 describes the values used for our naive encod-\ning. The \frst column lists the name of the quantity. The\nsecond column describes which PostgreSQL operators use a\nparticular type of input. The third column describes how\nthe particular value is encoded into an input suitable for a\nneural network. The encoding strategies are:\n\u000fNumeric : the value is encoded as a numeric value,\noccupying a single vector entry.\n\u000fBoolean : the value is encoded as either a zero or a\none, occupying a single vector entry.\n\u000fOne-hot : the value is categorical, and is encoded as a\none-hot vector, e.g. a vector with a single \\1\" element\nwhere the rest of the elements are \\0\", occupying a\nnumber of vector entries.\nA particular operator is encoded using all of its applica-\nble values, and using zeros for all inapplicable values. For\nexample, a join operator will be encoded as a vector that\nhas zeros for the \\Strategy,\" \\Partial Mode,\" and \\Oper-\nator\" entries, as these apply only to aggregate operators.\nAn aggregate operator, on the other hand, would have zeros\nfor the \\Join Type\" (for example) entry. See Section 2 for\nadditional details on the sparse encoding.\n12\n\nB. REFERENCES\n[1] PostgreSQL database, http://www.postgresql.org/.\n[2] M. Akdere et al. Learning-based query performance\nmodeling and prediction. In ICDE '12 .\n[3] M. Anderson et al. Brainwash: A Data System for\nFeature Engineering. In CIDR '13 .\n[4] M. R. Anderson et al. Input selection for fast feature\nengineering.\n[5] D. Antenucci et al. Ringtail: A Generalized\nNowcasting System. VLDB '13 .\n[6] Asmaa Sallam et al. DBSAFE|An Anomaly\nDetection System to Protect Databases From\nEx\fltration Attempts. Systems '17 .\n[7] J. L. Ba et al. Layer Normalization. arXiv '16 .\n[8] Y. Bengio. Deep Learning of Representations for\nUnsupervised and Transfer Learning. In ICML WUTL\n'12.\n[9] C. Binnig et al. Towards Interactive Curation &\nAutomatic Tuning of ML Pipelines. In DEEM '18 .\n[10] L. Breiman. Random Forests. Machine Learning '01 .\n[11] T. Cover et al. Nearest neighbor pattern classi\fcation.\nInformation Theory '67 .\n[12] P. Domingos. A Few Useful Things to Know About\nMachine Learning. Comm. ACM '12 .\n[13] J. Duggan et al. Contender: A Resource Modeling\nApproach for Concurrent Query Performance\nPrediction. In EDBT '14 .\n[14] J. Duggan et al. Performance Prediction for\nConcurrent Database Workloads. In SIGMOD '11 .\n[15] A. J. Elmore et al. Characterizing Tenant Behavior for\nPlacement and Crisis Mitigation in Multitenant\nDBMSs. In SIGMOD '13 .\n[16] R. C. Fernandez et al. Termite: A System for\nTunneling Through Heterogeneous Data. In Preprint,\n2019.\n[17] A. Ganapathi et al. Predicting Multiple Metrics for\nQueries: Better Decisions Enabled by Machine\nLearning. In ICDE '09 .\n[18] X. Glorot et al. Deep Sparse Recti\fer Neural\nNetworks. In PMLR '11 .\n[19] H. Grushka - Cohen et al. CyberRank: Knowledge\nElicitation for Risk Assessment of Database Security.\nInCIKM '16 .\n[20] M. A. Hearst et al. Support vector machines. ISA '98 .\n[21] D. W. Hosmer Jr et al. Applied Logistic Regression .\n2013.\n[22] Y. Jia et al. Ca\u000be: Convolutional Architecture for\nFast Feature Embedding. In MM '14 .\n[23] I. Jolli\u000be. Principal Component Analysis. In IESS '11 .\n[24] T. Kaftan et al. Cuttle\fsh: A Lightweight Primitive\nfor Adaptive Query Processing. arXiv '18 .\n[25] L. Khan et al. A new intrusion detection system using\nsupport vector machines and hierarchical clustering.\nVLDB '07 .\n[26] A. Kipf et al. Learned Cardinalities: Estimating\nCorrelated Joins with Deep Learning. In CIDR '19 .\n[27] T. Kraska. Northstar: An Interactive Data Science\nSystem. VLDB '18 .\n[28] T. Kraska et al. The Case for Learned Index\nStructures. In SIGMOD '18 .[29] S. Krishnan et al. Learning to Optimize Join Queries\nWith Deep Reinforcement Learning. arXiv '18 .\n[30] Y. LeCun et al. Deep learning. Nature '15 .\n[31] V. Leis et al. How Good Are Query Optimizers,\nReally? VLDB '15 .\n[32] J. Li et al. Robust estimation of resource consumption\nfor SQL queries using statistical techniques. VLDB\n'12.\n[33] H. Liu et al. Cardinality Estimation Using Neural\nNetworks. In CASCON '15 .\n[34] K. Lolos et al. Elastic management of cloud\napplications using adaptive reinforcement learning. In\nBig Data '17 .\n[35] Lorenzo Bossi et al. A System for Pro\fling and\nMonitoring Database Access Patterns by Application\nPrograms for Anomaly Detection. ToSE '14 .\n[36] A. L. Maas et al. Learning word vectors for sentiment\nanalysis. In H:T '11 .\n[37] R. Marcus et al. Deep Reinforcement Learning for\nJoin Order Enumeration. In aiDM '18 .\n[38] R. Marcus et al. NashDB: An Economic Approach to\nFragmentation, Replication and Provisioning for\nElastic Databases. In SIGMOD '18 .\n[39] R. Marcus et al. Releasing Cloud Databases from the\nChains of Performance Prediction Models. In CIDR\n'17.\n[40] R. Marcus et al. Towards a Hands-Free Query\nOptimizer through Deep Learning. In CIDR '19 .\n[41] R. Marcus et al. WiSeDB: A Learning-based Workload\nManagement Advisor for Cloud Databases. VLDB '16 .\n[42] T. Mikolov et al. E\u000ecient Estimation of Word\nRepresentations in Vector Space. arXiv '13 .\n[43] T. Mikolov et al. Linguistic Regularities in Continuous\nSpace Word Representations. In HLT '13 .\n[44] S. Mudgal et al. Deep Learning for Entity Matching:\nA Design Space Exploration. In SIGMOD '18 .\n[45] J. Ortiz et al. Learning State Representations for\nQuery Optimization with Deep Reinforcement\nLearning. In DEEM '18 .\n[46] J. Ortiz et al. PerfEnforce Demonstration: Data\nAnalytics with Performance Guarantees. In SIGMOD\n'16.\n[47] J. Ortiz et al. SLAOrchestrator: Reducing the Cost of\nPerformance SLAs for Cloud Data Analytics. In\nUSENIX ATX'18 .\n[48] A. Paszke et al. Automatic di\u000berentiation in PyTorch.\nInNIPS-W '17 .\n[49] A. Pavlo et al. Self-Driving Database Management\nSystems. In CIDR '17 .\n[50] F. Pedregosa et al. Scikit-learn: Machine Learning in\nPython. JMLR '11 .\n[51] M. Peters et al. Deep Contextualized Word\nRepresentations. In NAACL '18 .\n[52] S. Ruder. An overview of gradient descent\noptimization algorithms. arXiv '16 .\n[53] R. E. Schapire. The Strength of Weak Learnability.\nMachine Learning '90 .\n[54] J. Schmidhuber. Deep learning in neural networks: An\noverview. NN '15 .\n[55] P. G. Selinger et al. Access Path Selection in a\nRelational Database Management System. In\n13\n\nSIGMOD '89 .\n[56] Shrainik Jain et al. Database-Agnostic Workload\nManagement. In CIDR '19 .\n[57] D. Snyder et al. Deep neural network-based speaker\nembeddings for end-to-end speaker veri\fcation. In\nSLT '16 .\n[58] R. Taft et al. STeP: Scalable Tenant Placement for\nManaging Database-as-a-Service Deployments. In\nSoCC '16 .\n[59] Tim Kraska et al. SageDB: A Learned Database\nSystem. In CIDR '19 .\n[60] S. Tozer et al. Q-Cop: Avoiding bad query mixes to\nminimize client timeouts under heavy loads. In ICDE\n'10.\n[61] I. Trummer et al. SkinnerDB: Regret-bounded Query\nEvaluation via Reinforcement Learning. VLDB '18 .\n[62] K. Tzoumas et al. A Reinforcement Learning\nApproach for Adaptive Query Processing. In Technical\nReport, 08 .\n[63] L. van der Maaten et al. Visualizing Data using\nt-SNE. JMLR '08 .\n[64] S. Venkataraman et al. Ernest: E\u000ecient performance\nprediction for large-scale advanced analytics. In NSDI\n'16.\n[65] W. Wu et al. Predicting Query Execution Time: Are\nOptimizer Cost Models Really Unusable? In ICDE\n'13.\n[66] W. Wu et al. Towards Predicting Query Execution\nTime for Concurrent and Dynamic Database\nWorkloads. VLDB '13 .\n[67] D. Xin et al. Helix: Accelerating Human-in-the-loop\nMachine Learning. VLDB '18 .\n[68] P. Xiong et al. ActiveSLA: A Pro\ft-oriented\nAdmission Control Framework for\nDatabase-as-a-Service Providers. In SoCC '11 .\n[69] J. Yosinski et al. How Transferable Are Features in\nDeep Neural Networks? In NIPS '14 .\n14",
  "textLength": 75885
}