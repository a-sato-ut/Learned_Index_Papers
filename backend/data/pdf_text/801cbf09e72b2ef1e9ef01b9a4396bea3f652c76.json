{
  "paperId": "801cbf09e72b2ef1e9ef01b9a4396bea3f652c76",
  "title": "BS-tree: A gapped data-parallel B-tree",
  "pdfPath": "801cbf09e72b2ef1e9ef01b9a4396bea3f652c76.pdf",
  "text": "Bğ‘†-tree: A gapped data-parallel B-tree\nDimitrios Tsitsigkos\nArchimedes, Athena RC, Athens, Greece\ndtsitsigkos@athenarc.grAchilleas Michalopoulos\nUniversity of Ioannina & Archimedes, Athena RC\namichalopoulos@cse.uoi.gr\nNikos Mamoulis\nUniversity of Ioannina & Archimedes, Athena RC\nnikos@cs.uoi.grManolis Terrovitis\nAthena RC, Athens, Greece\nmter@athenarc.gr\nABSTRACT\nWe propose Bğ‘†-tree, an in-memory implementation of the B+-tree\nthat adopts the structure of the disk-based index (i.e., a balanced,\nmultiway tree), setting the node size to a memory block that can be\nprocessed fast and in parallel using SIMD instructions. A novel fea-\nture of the Bğ‘†-tree is that it enables gaps (unused positions) within\nnodes by duplicating key values. This allows (i) branchless SIMD\nsearch within each node, and (ii) branchless update operations in\nnodes without key shifting. We implement a frame of reference\n(FOR) compression mechanism, which allows nodes to have vary-\ning capacities, and can greatly decrease the memory footprint of\nBğ‘†-tree. We compare our approach to existing main-memory in-\ndices and learned indices under different workloads of queries and\nupdates and demonstrate its robustness and superiority compared\nto previous work in single- and multi-threaded processing.\n1 INTRODUCTION\nThe B+-tree is the dominant indexing method for DBMSs, due to its\nlow and guaranteed cost for (equality and range) query processing\nand updates. It was originally proposed as a disk-based index, where\nthe objective is to minimize the I/O cost of operations. As memories\nbecome larger and cheaper, main-memory and hardware-specific\nimplementations of the B+-tree [ 14,16,25,32,33,38,40,44,52,53,\n58,66], as well as alternative access methods for in-memory data [ 10,\n13,15,36,42,48,50,67,69] have been proposed. The optimization\nobjective in all these methods is minimizing the computational\ncost and cache misses during search. More recently, learned indices\n[21,24,26,35,37,45,64,65,70,71], which replace the inner nodes\nof the B+-tree by ML models have been suggested as a way for\nreducing the memory footprint of indexing and accelerating search\nat the same time.\nIn this paper, we propose Bğ‘†-tree, a B+-tree for main memory\ndata, which is optimized for modern commodity hardware and data\nparallelism. Bğ‘†-tree adopts the structure of the disk-based B+-tree\n(i.e., a balanced, multiway tree), setting the node size to a memory\nblock that can be processed in parallel. At the heart of our proposal\nlies a data-parallel successor operator ( ğ‘ ğ‘¢ğ‘ğ‘), implemented using\nSIMD, which is applied at each tree level for branching during\nsearch and updates and for locating the search key position at\nthe leaf level. To facilitate fast updates, without affecting SIMD-\nbased search, we propose a novel implementation for gaps (unused\npositions) by duplicating keys. The main idea is that we write in\neach unused slot the next used key value in the node or a global\nMAXKEY value if all subsequent slots are unused. Our Bğ‘†-tree\nconstruction algorithm initializes sparse leaf nodes with intentionalgaps in them, in order to (i) delay possible splits and (ii) reduce data\nshifting at insertions. Splitting also adds gaps proactively. Finally,\nwe apply a frame-of-reference (FOR) based compression method\nthat allows nodes that use fixed-size memory blocks to have varying\ncapacities , which saves space and increases data parallelism. The\ncomputational cost of search and update operations in Bğ‘†-tree is\nğ‘‚(logğ‘“ğ‘›), where ğ‘“is the capacity of the nodes, assuming that ğ‘“is\nselected such that each node can be processed by a (small) constant\nnumber of SIMD instructions.\nNovelty and contributions. There already exist several SIMD-\nbased implementations of B-trees and k-ary search [ 28,33,38,56,\n58,66]. In addition, updatable learned indices [ 21] use gaps to facil-\nitate fast updates. Finally, key compression in B-trees has also been\nstudied in previous work [ 14]. To our knowledge, our proposed\nBğ‘†-tree is the first B+-tree implementation that gracefully combines\nall these features, achieving at the same time minimal storage and\nhigh throughput. In particular, the use of duplicate key vales in\nunused gaps/slots allows (i) branchless, data-parallel SIMD search\nat each node, and (ii) efficient key insertions and deletions with\nlimited shifting of keys within each node. In addition, our compres-\nsion scheme allows for direct application of data-parallel search\non compressed nodes. To our knowledge, using duplicate keys in\ngaps within each node for efficient data-parallel search and updates\nat the same time is novel and has not been supported by previous\nB-tree implementations [ 28]. We implement a version of optimistic\nlock coupling in Bğ‘†-tree for concurrency control and extensively\ncompare Bğ‘†-tree with open-source single- and multi-threaded im-\nplementations of state-of-the-art non-learned and learned indices\non widely used real datasets, to find that Bğ‘†-tree and its compressed\nversion consistently prevail in different workloads of reads and up-\ndates, typically achieving 1.5x-2x higher throughput than the best\ncompetitor.\nOutline Section 2 presents related work. The Bğ‘†-tree is described\nin Section 3 and its updates and construction in Section 4. Section 5\ndescribes Bğ‘†-tree compression. Implementation details and concur-\nrency control are discussed in Section 6 and 7, respectively. Section\n8 includes our experimental evaluation. We conclude in Section 9.\n2 RELATED WORK\n2.1 B-tree\nThe B+-tree is considered the de-facto access method for relational\ndata, having substantial advantages over hash-based indexing with\nrespect to construction cost, support of range queries, sorted data\naccess, and concurrency control [ 23,27,28]. As memory sizes grow,arXiv:2505.01180v1  [cs.DB]  2 May 2025\n\nDimitrios Tsitsigkos, Achilleas Michalopoulos, Nikos Mamoulis, and Manolis Terrovitis\nthe interest has shifted to in-memory access methods [ 68]. Rao\nand Ross [ 52] were the first to consider the impact of cache misses\nin memory-based data structures; they proposed Cache-Sensitive\nSearch Trees (CSS-trees), in which every node has the same size as\nthe cache-line of the machine and does not need to keep pointers\nfor the links between nodes, but offsets that can be calculated by\narithmentic operations. Rao and Ross [ 53] also proposed the Cache\nSensitive B+-tree (CSB+tree), which achieves cache performance\nclose to CSS-Trees, while having the advantages of a B+-tree. Chen\net al. [ 18,19] showed how prefetching can significantly improve the\nperformance of index structures by reducing memory access latency.\nThe pkB-tree [ 14] is an in-memory variant of the B-tree that uses\npartial-keys (fixed-size parts of keys), which reduce cache misses\nand improve search performance. Zhou and Ross [ 73] investigated\nbuffering techniques, based on fixed-size or variable-sized buffers,\nfor memory index structures, aiming to avoid cache thrashing and\nto improve the performance of bulk lookup in relation to a sequence\nof single lookups. Graefe and Larson [ 29] surveyed techniques that\nimprove the perfomance of B+-tree by exploting CPU caches.\n2.2 (Data) parallelism in B-trees\nThe advent of SIMD instructions and GPUs opened new perspec-\ntives for in-memory index structures. In an early work, Zhou and\nRoss [ 72] explored the use of SIMD to parallelize key database\noperations (such as scans, joins, and filtering), minimizing branch\nmispredictions. Schlegel et al. [ 56] present methods for SIMD-based\nk-ary search (find which out of k partitions contains a search key).\nFAST [ 33] optimizes k-way tree search by leveraging architecture-\nspecific features such as cache locality, SIMD parallelism on CPUs,\nand massive parallelism on GPUs. [ 25] introduced a â€œbraidedâ€ B+-\ntree structure optimized for parallel searches on GPUs, enabling\nlock-free traversal using additional pointers. Kaczmarski [ 32] pro-\nposed a bottom-up B+-tree construction and maintenance technique\nusing CPU and GPU for bulk-loading and updates. Bw-Tree [ 44]\nis a highly scalable and latch-free B+-tree variant optimized for\nmodern hardware platforms, including multi-core processors and\nflash storage. Hybrid B+-tree [ 58] leverages both CPU and GPU\nresources to optimize in-memory indexing, by dynamically balanc-\ning the workload between the CPU and the GPU. Yan et al. [ 66]\nproposed a B+-tree tailored for GPU and SIMD architectures. This\nstructure decouples the â€œkey regionâ€, which contains keys of the\nB+-tree with the â€œchild regionâ€, which is organized as a prefix-sum\narray and stores only each nodeâ€™s first child index in the key region.\nKwon et al. [ 38] proposed DB+-tree, a B+-tree with partial keys, that\nutilizes SIMD and other sequential instructions for fast branching.\nPALM [ 57] is a parallel latch-free variant of the B+-tree, which is\noptimized for multi-core processors, enabling concurrent search\nand update operations. Other works explore the implementation of\nB-trees on flash memory [ 8,11,30,31,46,51,60,63], non-volatile\nmemory [20, 47] and hardware transactional memory [59].\n2.3 Other in-memory access methods\nBesides B-trees, other data structures have also been used for in-\nmemory indexing, especially trie-based ones, such as the HAT-trie\n[9,10], the generalized prefix tree (trie) [ 15], KISS-TREE [ 36], andMasstree [ 48]. Leis et al. [ 42] proposed a fast and space-efficient in-\nmemory trie called ART, which dynamically adjusts its node sizes\nproviding a compact and cache-efficient representation. ART uses\nlazy expansion and path compression to improve space utilization\nand search performance. Leis et al. proposed two synchronization\nprotocols for ART in [ 43], which have good scalability despite rely-\ning on locks: optimistic lock coupling and the read-optimized write\nexclusion (ROWEX) protocol. Height Optimized Trie (HOT) [ 13]\nis an in-memory trie-based index that reduces tree height through\npath compression and node merging. SuRF (Succinct Range Filter)\n[69] leverages succinct tries to provide a space-efficient solution\nfor range query filtering.\n2.4 Learned Indexing\nThe advent of fast and accurate machine learning techniques in-\nspired the design of a new type of index structure, called learned\nindex [37]. The main idea is to learn a cumulative distribution func-\ntion (CDF) of the keys and define a Recursive Model Index (RMI)\nthat replaces the inner nodes of a B+-tree by a hierarchy of models\nthat can predict very fast the position of the search key. FITing tree\n[26] and PGM-index [ 24] build upon RMI with a focus on improv-\ning model performance. with provable worst-case bounds on query\ntime and space usage. The RadixSpline (RS) [ 35] learned index can\nbe constructed in a single traversal of sorted data. ALEX [ 21] is an\nupdatable learned index structure, based on RMI. ALEX utilizes a\ngapped array layout that gracefully distributes extra space between\nelements based on the modelâ€™s predictions, enabling faster updates\nand lookups by exponential search. Other updatable learned indices\ninclude CARMI [ 70], NFL [ 65], LIPP [ 64], and DILI [ 45], and Hy-\nper [ 71]. Refs. [ 34,49,62] provide comprehensive evaluations on\nupdatable learned indices and traditional indices including many\nimportant findings, based on tests on several real-world datasets.\n3 THE Bğ‘†-TREE\nWe propose Bğ‘†-tree, an in-memory implementation of the B+-tree,\nwhich supports fast searches and updates by exploiting data paral-\nlelism (i.e., SIMD instructions). We first present the data structure\nin Section 3.1. Then, Section 3.2 describes the implementation of\nthe successor operator applied to each node for branching and key\nlocation during search and updates. Finally, Section 3.3 presents\nthe algorithms for equality and range search.\n3.1 The structure\nBğ‘†-tree follows the structure of the B+-tree. Each internal node of\nthe tree fits up to ğ‘references to nodes at the lower level and up\ntoğ‘âˆ’1keys. Leaf nodes contain rid-key pairs, where a record-id\n(rid) is the address (potentially on the disk) of the record that has\nthe corresponding key value. We assume that keys are unique (if\nnot, rid is replaced by a pointer to a block that keeps the ridâ€™s of\nall records having the corresponding search key). The storage of\nthe ridâ€™s is decoupled from the storage of the keys, i.e., they are\nstored in two different arrays, such that the rid array is accessed\nonly if necessary (i.e., only if the key is found and we need access\nto the corresponding record). Similarly, the storage of keys in an\ninternal node is decoupled from the storage of children addresses,\nto facilitate fast search, as we explain later. Each leaf node hosts\n\nBğ‘†-tree: A gapped data-parallel B-tree\nthe address of the next leaf in the total key order; Leaf chaining\nefficiently supports range queries, where query results are in con-\nsecutive tree leaves. For the Bğ‘†-tree nodes we use a value of ğ‘that\nfacilitates fast and parallel search, as we will explain in Sections\n5 and 6. For the efficient handling of updates, we allow gaps (i.e.,\nunused slots) in nodes, similarly to previous work [ 21,45,64], as\nwill be discussed in Section 4.\nFigure 1 shows an example of a Bğ‘†-tree, where each node holds\nup to ğ‘âˆ’1=4keys. Each non-leaf node is shown as an array\nofğ‘node pointers (bottom) and ğ‘âˆ’1keys (top) that work as\nseparators. All keys in the subtree pointed by the ğ‘–-th pointer are\nstrictly smaller than the ğ‘–-th key and greater than or equal to the\n(ğ‘–âˆ’1)-th key (if ğ‘–>0). Any unused key slots at the end of each node\ncarry a special MAXKEY value (denoted by âˆin the figure), which\nis larger than the maximum possible value in the key domain. For\nexample, if keys are unsigned 64-bit integers, MAXKEY =264âˆ’1\nand key values range in [0,264âˆ’2].\nBasic Structure52104112031âˆâ€¦258âˆ1113141861758196164âˆ232245258âˆ258260261âˆâ€¦\nFigure 1: Example of Bğ‘†-tree\n3.2 Search within a Bğ‘†-tree node\nWe now elaborate on the implementation of branching at each node\nof the Bğ‘†-tree, i.e., selecting the next node to visit. Traditionally,\nat each visited node, starting from the root, finding the smallest\nkey which is strictly greater that the query key ğ‘˜is done either by\nbinary search or by linearly scanning the entries until we find the\nfirst key greater than ğ‘˜. Both these approaches incur significant\nCPU cost due to branch mispredictions.\nWe use an efficient implementation of the successor operator\napplied to each node along the search path, which does not involve\nsearch decisions. We denote by ğ‘ ğ‘¢ğ‘ğ‘ >the operator that finds the\nsmallest key position which is strictly greater than the query key ğ‘˜\n(used in non-leaf nodes) and by ğ‘ ğ‘¢ğ‘ğ‘â‰¥the finding of the smallest\nkey position which is greater than or equal to ğ‘˜(used in leaf nodes).\nFor example, in Figure 1, ğ‘ ğ‘¢ğ‘ğ‘ >124(ğ‘Ÿğ‘œğ‘œğ‘¡)=2, as the smallest key\nwhich is greater than 124 is at position 2. In general, ğ‘ ğ‘¢ğ‘ğ‘ >is the\nmost frequently applied operation during search, as we apply it in\neach non-leaf node along the path from the root to the leaf that\nincludes the (first) search result.\nOur approach exploits data parallelism (i.e., SIMD instructions)\nand does not include if statements or while statements with an\nuncertain number of loops. Specifically, let ğ‘£be a node and ğ‘˜be the\nsearch key. Then, ğ‘ ğ‘¢ğ‘ğ‘ >ğ‘˜(ğ‘£)=|{ğ‘¥:ğ‘¥âˆˆğ‘£.ğ‘˜ğ‘’ğ‘¦ğ‘ âˆ§ğ‘˜â‰¥ğ‘¥}|, where\n|ğ‘†|denotes cardinality of ğ‘†. Based on this, ğ‘ ğ‘¢ğ‘ğ‘ >ğ‘˜(ğ‘£), for uint64\nkeys can be implemented by code Snippet 1. The corresponding\nSIMD-fied code (AVX 512) is Snippet 2, where Line 6 loads the\nnode keys vector, Line 7 creates a comparison mask which has 1 at\nkey positions where the search key is greater than the node key,\nand Line 8 counts the 1â€™s in the mask. As the code snippets do\nnot include if-statements, they do not incur branch mispredictions.CAPACITY is the (fixed) maximum capacity of the node, so the\nnumber of iterations of the for-loop is hardwired; all these favoring\ndata parallelism.\nSnippet 1: Counting search\n1int succG(Node *v, uint64 skey) {\n2 int count = 0;\n3 for(int i=0; i<CAPACITY; i++)\n4 count += (skey >= v->keys[i]);\n5 return count;\n6}\nSnippet 2: SIMD-based counting search (AVX 512)\n1int succG_SIMD(Node *v, uint64 skey) {\n2 int count = 0;\n3 __mmask8 cmp_mask = 0;\n4 __512i vec,Vskey = _mm512_set1_epi64(skey);\n5 for(int i = 0; i < CAPACITY; i += 8) {\n6 vec = _mm512_loadu_epi64((__512i*)(v->keys+i));\n7 cmp_mask = _mm512_cmpge_epu64_mask(Vskey, vec);\n8 count += _mm_popcnt_u32((uint32_t) cmp_mask);\n9 }\n10 return count;\n11}\nSimilarly, ğ‘ ğ‘¢ğ‘ğ‘â‰¥ğ‘˜(ğ‘£)=|{ğ‘¥:ğ‘¥âˆˆğ‘£.ğ‘˜ğ‘’ğ‘¦ğ‘ âˆ§ğ‘˜>ğ‘¥}|and the same\ncode snippets can be used, by replacing comparison >=by>and\n_mm512_cmpge_epu64_mask by_mm512_cmpgt_epu64_mask . This\napproach (with slightly different implementation) has also been\nsuggested for SIMD-based k-way search in [56, 58, 72].\nThe experiments of Figure 2 illustrate the efficiency of data-\nparallel ğ‘ ğ‘¢ğ‘ğ‘ >compared to traditional implementations of branch-\ning in multiway trees, on sorted arrays of 64-bit, 32-bit, and 16-bit\nunsigned integers. The arrays simulate key arrays in a full Bğ‘†-tree\nnode, with values drawn randomly from the corresponding uint do-\nmain. We performed random successor (i.e., branching) operations\nto the array and measured the throughput (in millions operations\nper second) of four implementations1ofğ‘ ğ‘¢ğ‘ğ‘ >:\nâ€¢Binary: use of (non-recursive) binary search\nâ€¢Linear: scan data from the beginning until successor is\nfound\nâ€¢Counting: count-based in a for-loop (Snippet 1)\nâ€¢SIMD-based: count-based using SIMD (Snippet 2)\nWe tested various sizes of the array, modeling different key-array\nsizes in a Bğ‘†-tree node. We used array sizes that are multiples of 8,\nwhich allows us to take full advantage of SIMD-parallelism.\nBinary search and linear scan perform similarly, with linear\nsearch being superior on small arrays and binary search prevailing\non larger arrays, as expected. Counting search (Snippet 1) is much\nfaster than binary/linear scan, due to the absence of branch instruc-\ntions and due to autovectorization optimizations at the assembly\nlevel, by the -O3 -march=native compilation flag.\nObserve the excellent performance of SIMD-based search (Snip-\npet 2) for all array and key sizes. Compared to black-box -O3com-\npilation, custom vectorization offers significant advantages and\nachieves the theoretically optimal performance. For example, for\n1See Section 8 for our experimental setup.\n\nDimitrios Tsitsigkos, Achilleas Michalopoulos, Nikos Mamoulis, and Manolis Terrovitis\n02004006008001000\n8163264128256Throughput (Mop/s)Array SizeData Space: [0, 264-1]Binary LinearCounting SIMD-based \n02004006008001000\n163264128256512Throughput (Mop/s)Array SizeData Space: [0, 232 -1]Binary LinearCounting SIMD-based \n02004006008001000\n32641282565121024Throughput (Mop/s)Array SizeData Space: [0, 216 -1]Binary LinearCounting SIMD-based \n(a)uint64 (b)uint32 (c)uint16\nFigure 2: Comparison on successor search techniques in small uint arrays\n64-bit keys and key-array capacity 16, it achieves 7x performance\nimprovement over binary search, which is even higher than the\ntheoretically expected 4x ( log1616vs.log216). For uint16 keys,\nSnippet 2 is more than 2x faster than autovectorized Snippet 1.\n3.3 Bğ‘†-tree search\nAlgorithms 3 and 4 show how the Bğ‘†-tree is searched for (i) equality\nqueries and (ii) range queries. For equality, we traverse the tree\nby applying a ğ‘ ğ‘¢ğ‘ğ‘ >ğ‘˜(ğ‘£)operation at each non-leaf node ğ‘£. At the\nreached leaf ğ‘£we apply a ğ‘ ğ‘¢ğ‘ğ‘â‰¥ğ‘˜(ğ‘£)operation to find the first posi-\ntionğ‘Ÿin the leaf having a key greater than or equal to ğ‘˜. Ifğ‘£.ğ‘˜ğ‘’ğ‘¦ğ‘ [ğ‘Ÿ]\nequals ğ‘˜, then the record at position ğ‘Ÿis returned; otherwise, ğ‘˜does\nnot exist. Equality search requires one ğ‘ ğ‘¢ğ‘ğ‘ >orğ‘ ğ‘¢ğ‘ğ‘â‰¥operation\nper node along the search path.\nFor range queries, assume that we are looking for all keys ğ‘¥, such\nthatğ‘˜1â‰¤ğ‘¥â‰¤ğ‘˜2. We traverse the tree using ğ‘ ğ‘¢ğ‘ğ‘ >ğ‘˜1operations to\nfind the first leaf that may contain a query result. In that leaf, we\napply a ğ‘ ğ‘¢ğ‘ğ‘â‰¥ğ‘˜1operation to locate the position ğ‘Ÿ1of the first qual-\nifying key. To find the end position ğ‘Ÿ2of the query results, starting\nfrom the current leaf ğ‘£, we search for the leaf which includes the\nfirst key greater than ğ‘˜2, by performing one ğ‘ ğ‘¢ğ‘ğ‘ >ğ‘˜2operation per\nleaf. Hence, range searches require one ğ‘ ğ‘¢ğ‘ğ‘ >operation per node\nalong the search path in search for ğ‘˜1plus one ğ‘ ğ‘¢ğ‘ğ‘ >operation for\neach leaf that includes range query results. For large query ranges\nwhose results appear in numerous leaves, we apply an alternative\nimplementation of range queries, where one equality search is used\nto locate ğ‘Ÿ1and another equality search is then applied to locate ğ‘Ÿ2.\nThis is expected to be faster than Algorithm 4 if the height of the\ntree is smaller compared to the number of leaves that include the\nquery results.\nAs an example, consider searching for key 13in the tree of Figure\n1. Ağ‘ ğ‘¢ğ‘ğ‘ >13operation on the root will give position 0, as there are\n0 keys smaller than or equal to 13, so the first pointer of the root\nwill be followed. Then, the ğ‘ ğ‘¢ğ‘ğ‘ >13operation on the visited node\nwill return 1, which means that we then visit the 2nd leaf, where\nğ‘ ğ‘¢ğ‘ğ‘â‰¥13is applied that returns 1, i.e., the position of 13 in the leaf.\nA range search for keys in [13,17]first locates 13and then finds\nthe upper bound 18in the same leaf after applying ğ‘ ğ‘¢ğ‘ğ‘ >17.\n4 GAPS AND UPDATES\nThe main novelty of Bğ‘†-tree is the implementation of gaps in nodes\nusing duplicated keys , which facilitates efficient updates and allowsALGORITHM 3: Equality Search\nInput : search keyğ‘˜, Bğ‘†-tree root node ğ‘£\nOutput : record-id corresponding to key ğ‘˜\n1whileğ‘›is non leaf do\n2ğ‘£â†node pointed by entry at position ğ‘£[ğ‘ ğ‘¢ğ‘ğ‘>ğ‘˜(ğ‘£)]\n3ğ‘Ÿâ†ğ‘ ğ‘¢ğ‘ğ‘â‰¥ğ‘˜(ğ‘£) âŠ²leaf node ;\n4ifğ‘£.ğ‘˜ğ‘’ğ‘¦ğ‘ [ğ‘Ÿ]==ğ‘˜then\n5 return record-id inğ‘£at positonğ‘Ÿ\n6elseâŠ²ğ‘˜does not exist\n7 return NULL\nALGORITHM 4: Range Search\nInput : search keysğ‘˜1,ğ‘˜2, Bğ‘†-tree root node ğ‘£\nOutput : record-ids of keys ğ‘¥, whereğ‘˜1â‰¤ğ‘¥â‰¤ğ‘˜2\n1whileğ‘›is non leaf do\n2ğ‘£â†node pointed by entry at position ğ‘£[ğ‘ ğ‘¢ğ‘ğ‘>ğ‘˜1(ğ‘£)]\n3ğ‘Ÿ1â†ğ‘ ğ‘¢ğ‘ğ‘â‰¥ğ‘˜1(ğ‘£)];\n4while(ğ‘Ÿ2â†ğ‘£.ğ‘˜ğ‘’ğ‘¦ğ‘ [ğ‘ ğ‘¢ğ‘ğ‘>ğ‘˜2(ğ‘£)])==Ndo\n5ğ‘£â†ğ‘›ğ‘’ğ‘¥ğ‘¡ğ‘™ğ‘’ğ‘ğ‘“(ğ‘£) âŠ²next toğ‘£leaf\n6 ifğ‘£== NULL then\n7 break âŠ²last leaf reached\n8return record-ids of keys from position ğ‘Ÿ1to positionğ‘Ÿ2(excl.)\nthe use of Snippet 2 for search at the same time. Specifically, while\nSIMD-based k-way search on packed arrays has also been suggested\nbefore [ 56,58,72], to our knowledge it has never been applied on\nB+-tree nodes with unused slots. As discussed in Sec. 3.1, unused\nkey slots at the end of each node are filled with MAXKEY values;\nhence, uniqueness is not a requirement for unused key positions.\nIn addition, Bğ‘†-tree does not require the used key slots to be con-\ntinuous at the beginning of the node. This means that â€˜gapsâ€™ with\nunused key slots are allowed in a node.\nIn Bğ‘†-tree, we enforce that the key value in a gap (unused slot) is\nthe same as the first subsequent non-gap key . By managing auxiliary\ninformation at each node, i.e., (i) the slot use (number of used slots)\nand (ii) a bitmap indicating used slots, we can efficiently track\nunused slots; see Figure 3 for an example. In the rest of this section,\n\nBğ‘†-tree: A gapped data-parallel B-tree\nwe will show how deletions and insertions are handled in the Bğ‘†-\ntree. We will explain how our approaches minimize the overhead\nof node modifications while maintaining high search performance.\n522474753565667677892104104123âˆâˆKeys Array:r5r22r47r47r53r56r56r67r67r78r92r104r104r123--References:Basic structure of leaf node mAuxiliary information for leaf node mSlot use:   10Bitmap:1101101011101100nextLeaf:   m+1\nFigure 3: Bğ‘†-tree leaf node structure\n522474753676767677892104104123âˆâˆKeys Array:References:After deletion of 56Slot use:   9Bitmap:1101100011101100nextLeaf:   m+1r5r22r47r47r53r67r67r67r67r78r92r104r104r123--\n522474753676767677892104104123âˆâˆKeys Array:References:After insertion of 52\nSlot use:   10Bitmap:1101110011101100nextLeaf:   m+1522474752536767677892104104123âˆâˆKeys Array:References:r5r22r47r47r53r67r67r67r67r78r92r104r104r123--r5r22r47r47r52r53r67r67r67r78r92r104r104r123--\nFigure 4: Updates to Bğ‘†-tree leaf node\n4.1 Deletions\nTo delete a key, we first locate its position ğ‘–in a leaf node, using\nthe equality search algorithm (discussed in Section 3.3). Then, we\ncopy the key value from position ğ‘–+1to position ğ‘–and propagate\nit backwards to previous gap positions in the node. If ğ‘–is the last\nposition in the leaf, we set ğ‘£.ğ‘˜ğ‘’ğ‘¦ğ‘ [ğ‘–]=MAXKEY.\nOne subtle point to note is that ğ‘ ğ‘¢ğ‘ğ‘â‰¥may not give us the real\nposition of the key ğ‘˜to be deleted but may give us the first of\na sequence of gaps that have key value equal to ğ‘˜. For example,\nfor deleting ğ‘˜=56in Figure 3, we apply ğ‘ ğ‘¢ğ‘ğ‘â‰¥(56)which gives\nus position 5. Then, we find the range of all positions having 56\n(i.e., [5,6]) and copy into them the next value (i.e., 67). Finding\nall the positions can be done very fast using bitwise operations.\nFigure 4 (top) shows the leaf node of Figure 3 after deleting key 56.\nAlgorithm 5 is a pseudocode for deletions to the Bğ‘†-tree.\nAs in previous work [ 53,64], we do not take action for nodes\nwith fewer than 50% occupied slots, as we anticipate insertions to be\nmore frequent than deletions, so node merges or key redistributions\nare not expected to pay-off. If the last entry is deleted from a node,\nthe node is marked as empty and the corresponding separator entry\nat its parent is â€˜deletedâ€™ by copying the next key into it.ALGORITHM 5: Deletion in Bğ‘†-tree\nInput : keyğ‘˜, Bğ‘†-tree root node ğ‘£\n1find leafğ‘£and positionğ‘Ÿby running lines 1-3 of Alg 3\n2ifğ‘£.ğ‘˜ğ‘’ğ‘¦ğ‘ [ğ‘Ÿ]â‰ ğ‘˜then\n3 return FAIL\n4ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘â†ğ‘£.ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘\n5ifğ‘Ÿ==ğ‘âˆ’1then âŠ²last key in node\n6ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘â†ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘âŠ•0x0001\n7ğ‘Ÿğ‘’ğ‘ğ‘™ğ‘–ğ‘ğ‘ğ‘ ğ‘‚ğ‘“ğ¾ğ‘’ğ‘¦â†_ğ‘¡ğ‘§ğ‘ğ‘›ğ‘¡(ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘)\n8 forğ‘–â†0toğ‘Ÿğ‘’ğ‘ğ‘™ğ‘–ğ‘ğ‘ğ‘ ğ‘‚ğ‘“ğ¾ğ‘’ğ‘¦âˆ’1do âŠ²propagate backwards\n9ğ‘£.ğ‘˜ğ‘’ğ‘¦ğ‘ [ğ‘Ÿâˆ’ğ‘–]â†ğ‘€ğ´ğ‘‹ğ¾ğ¸ğ‘Œ\n10else âŠ²ğ‘Ÿis not the last position in the leaf\n11ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘â†ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘âŠ•(0x8000â‰ªğ‘Ÿ)\n12ğ‘Ÿğ‘’ğ‘ğ‘™ğ‘–ğ‘ğ‘ğ‘ ğ‘‚ğ‘“ğ¾ğ‘’ğ‘¦â†_ğ‘™ğ‘§ğ‘ğ‘›ğ‘¡(ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘)\n13ğ‘›ğ‘’ğ‘¥ğ‘¡ğ‘‰ğ‘ğ‘™ğ‘–ğ‘‘ğ¾ğ‘’ğ‘¦â†ğ‘£.ğ‘˜ğ‘’ğ‘¦ğ‘ [ğ‘Ÿ+ğ‘Ÿğ‘’ğ‘ğ‘™ğ‘–ğ‘ğ‘ğ‘ ğ‘‚ğ‘“ğ¾ğ‘’ğ‘¦+1]\n14 forğ‘–â†0toğ‘Ÿğ‘’ğ‘ğ‘™ğ‘–ğ‘ğ‘ğ‘ ğ‘‚ğ‘“ğ¾ğ‘’ğ‘¦ do âŠ²propagate backwards\n15ğ‘£.ğ‘˜ğ‘’ğ‘¦ğ‘ [ğ‘Ÿ+ğ‘–]â†ğ‘›ğ‘’ğ‘¥ğ‘¡ğ‘‰ğ‘ğ‘™ğ‘–ğ‘‘ğ¾ğ‘’ğ‘¦\n16ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘â†ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘âŠ•(0ğ‘¥8000â‰«(ğ‘Ÿ+ğ‘Ÿğ‘’ğ‘ğ‘™ğ‘–ğ‘ğ‘ğ‘ ğ‘‚ğ‘“ğ¾ğ‘’ğ‘¦))\n17ğ‘£.ğ‘ ğ‘™ğ‘œğ‘¡ğ‘¢ğ‘ ğ‘’â†ğ‘£.ğ‘ ğ‘™ğ‘œğ‘¡ğ‘¢ğ‘ ğ‘’âˆ’1\n18ğ‘£.ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘â†ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘\n19return SUCCESS\n4.2 Insertions\nInserting a new key ğ‘˜to the Bğ‘†-tree entails searching for the leaf\nnode and the position in it to place it. Search is conducted by\napplying ğ‘ ğ‘¢ğ‘ğ‘ >ğ‘˜operations starting from the root and following the\ncorresponding pointers. When we reach the leaf ğ‘£where in ğ‘˜should\nbe inserted, we apply a ğ‘ ğ‘¢ğ‘ğ‘â‰¥ğ‘˜operation which finds the proper\nslot in ğ‘£to insert ğ‘˜. Then, we verify whether the slot ğ‘–returned by\nğ‘ ğ‘¢ğ‘ğ‘â‰¥ğ‘˜is occupied by another key. This is done by a simple test. If\nğ‘£.ğ‘˜ğ‘’ğ‘¦ğ‘ [ğ‘–]=ğ‘£.ğ‘˜ğ‘’ğ‘¦ğ‘ [ğ‘–+1], then we are sure that position ğ‘–is free,\nso we place ğ‘˜there and finish. For example, assume that we want\nto insert key 55 to the leaf node of Figure 3. We apply a ğ‘ ğ‘¢ğ‘ğ‘â‰¥ğ‘˜\noperation to the leaf, which will give us position 5. Since the next\nposition (6) has the same key, position 5 corresponds to a free slot\n(gap), hence, we have directly put the inserted key 55 there. On\nthe other hand, if the key at position ğ‘–is different compared to the\nkey at position ğ‘–+1, this means that the position is occupied. In\nthis case, we first find the first position ğ‘—after ğ‘–, which is unused\n(i.e., a gap), and right-shift all keys (and the corresponding record\npointers) from position ğ‘–to position ğ‘—âˆ’1, to make space, so that\nkeyğ‘˜can be inserted at position ğ‘–. If there is no free position after\nğ‘–, then we move one position to the left (left-shift) all keys and\nrecord ids from position ğ‘–until the first free position to the left of ğ‘–.\nFigure 4 (bottom) shows an example of inserting key 52. As the slot\nwhere 52 should go is occupied by 53 and it is not a gap (the key\nfollowing 53 is not equal to 53), we search for the next gap, which\nis the position next to 53, right-shift 53 there and make room for\nthe new key 52. Algorithm 6 describes Bğ‘†-tree insertion.\nIn case leaf ğ‘£is full, then we conduct a split ofğ‘£and introduce\na new leaf node. The existing keys in ğ‘£together with ğ‘˜are split\nin half and distributed between the two leaves. Instead of placing\nthe distributed keys to the first half of each of the two leaves, we\ninterleave each key with a gap to facilitate fast insertion of future\nkeys. Proactive gapping is described in the next subsection.\n\nDimitrios Tsitsigkos, Achilleas Michalopoulos, Nikos Mamoulis, and Manolis Terrovitis\nALGORITHM 6: Insertion in Bğ‘†-tree\nInput : keyğ‘˜, Bğ‘†-tree root node ğ‘£\n1compute leaf ğ‘£and positionğ‘Ÿby running lines 1-3 of Alg 3\n2ifğ‘£.ğ‘ ğ‘™ğ‘œğ‘¡ğ‘¢ğ‘ ğ‘’ <ğ‘then\n3ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘â† Â¬ğ‘£.ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘\n4 ifğ‘£.ğ‘˜ğ‘’ğ‘¦ğ‘ [ğ‘Ÿ]==ğ‘£.ğ‘˜ğ‘’ğ‘¦ğ‘ [ğ‘Ÿ+1]then âŠ²ğ‘Ÿis an empty slot\n5ğ‘£.ğ‘˜ğ‘’ğ‘¦ğ‘ [ğ‘Ÿ]â†ğ‘˜\n6ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘â†ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘âŠ•(0ğ‘¥8000â‰«ğ‘Ÿ)\n7 else\n8ğ‘˜ğ‘’ğ‘¦ğ‘ ğ¹ğ‘œğ‘Ÿğ‘†â„ğ‘–ğ‘“ğ‘¡â†_ğ‘™ğ‘§ğ‘ğ‘›ğ‘¡(ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘â‰ªğ‘Ÿ)\n9 ifğ‘˜ğ‘’ğ‘¦ğ‘ ğ¹ğ‘œğ‘Ÿğ‘†â„ğ‘–ğ‘“ğ‘¡ <ğ‘then âŠ²empty slot to the right\n10 shift rightğ‘˜ğ‘’ğ‘¦ğ‘ ğ¹ğ‘œğ‘Ÿğ‘†â„ğ‘–ğ‘“ğ‘¡ keys of node ğ‘£\n11 ğ‘£.ğ‘˜ğ‘’ğ‘¦ğ‘ [ğ‘Ÿ]â†ğ‘˜\n12 ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘â†ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘âŠ•(0ğ‘¥8000â‰«(ğ‘Ÿ+ğ‘˜ğ‘’ğ‘¦ğ‘ ğ¹ğ‘œğ‘Ÿğ‘†â„ğ‘–ğ‘“ğ‘¡))\n13 else âŠ²empty slot to the left\n14 ğ‘˜ğ‘’ğ‘¦ğ‘ ğ¹ğ‘œğ‘Ÿğ‘†â„ğ‘–ğ‘“ğ‘¡â†_ğ‘¡ğ‘§ğ‘ğ‘›ğ‘¡(ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘â‰«(ğ‘âˆ’ğ‘Ÿâˆ’1))âˆ’ 1\n15 shift leftğ‘˜ğ‘’ğ‘¦ğ‘ ğ¹ğ‘œğ‘Ÿğ‘†â„ğ‘–ğ‘“ğ‘¡ keys of node ğ‘£\n16 ğ‘£.ğ‘˜ğ‘’ğ‘¦ğ‘ [ğ‘Ÿâˆ’1]â†ğ‘˜\n17 ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘â†ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘âŠ•(0ğ‘¥8000â‰«(ğ‘Ÿâˆ’(ğ‘˜ğ‘’ğ‘¦ğ‘ ğ¹ğ‘œğ‘Ÿğ‘†â„ğ‘–ğ‘“ğ‘¡+1)))\n18ğ‘£.ğ‘ ğ‘™ğ‘œğ‘¡ğ‘¢ğ‘ ğ‘’â†ğ‘ ğ‘™ğ‘œğ‘¡ğ‘¢ğ‘ ğ‘’+1\n19ğ‘£.ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘â† Â¬ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘\n20else\n21 split leaf node ğ‘£\n22return\n4.3 Tree building\nWe now describe the algorithm for building a Bğ‘†-tree from a set of\nkeys (bulk loading). Like typical B+-tree construction algorithms,\nwe first sort the keys to construct the leaf level of the index. To\nfacilitate fast future insertions, we do not pack the nodes with\nkeys, that is we leave free space to accomodate future insertions.\nSpecifically, if ğ‘is the capacity of a leaf node, we construct all leaf\nnodes by adding to them the keys in sorted order; each leaf node\ntakes ğ›¼Â·ğ‘(key, record-id) pairs, where ğ›¼ranges from 0.5(half-full\nnodes) to 1(full nodes). We typically set ğ›¼=0.75for tree leaves.\nFor each leaf, instead of placing all keys at the beginning of the\nleaf and leaving(1âˆ’ğ›¼)ğ‘consecutive empty slots at the end of the\nnode, we spread the entries in the leaf by placing one gap (empty\nslot) after every1\n1âˆ’ğ‘âˆ’1entries.2For each leaf node (except the first\none) a separator key , equal to the first key of the leaf, is added to an\narray. For each separator key, a node pointer to the previous leaf is\nassociated to the separator. Finally, a node pointer to the last leaf\nis introduced at the end of the array (without a key value). After\nconstructing the leaves, the (already sorted) array of separator keys\nis used to construct the next level of Bğ‘†-tree (above the leaves),\nrecursively. We increase ğ›¼as we go up, since we anticipate much\nfewer insertions (and node overflows) at higher levels.\n5 KEY COMPRESSION\nBğ‘†-tree, as it has been discussed so far, stores the exact keys in its\nnodes. Previous work on key compression for B+-tree [ 14] uses\nfixed-size partial keys. One issue with partial keys is the overhead of\ndecompression which may compromise performance. For Bğ‘†-tree,\nwe opt for the frame-of-reference (FOR) compression approach,\nwhich has minimal decompression overhead.\n2Gaps between consecutive key values (for integer keys) are not introduced.Specifically, for each node ğ‘£, we store in the nodeâ€™s auxiliary\ninformation (see Figure 3) the first key ğ‘£.ğ‘˜0of the node and replace\ntheğ‘£â€™s key array (of size ğ‘) by an array where each original key ğ‘˜\nis replaced by the difference ğ‘˜âˆ’ğ‘˜0. This allows us to potentially\ndouble or quadruple the size of the array if the differences occupy\nmuch less space than the original keys. If ğ‘=16and the original\narray stores 64 bits, it may potentially be replaced by an array of\nğ‘=3232-bit differences or ğ‘=6416-bit differences. Since the\nkeys in a node are ordered, we expect the differences to be small,\nespecially in leaf nodes, so the space savings due to the reduction\nin the number of nodes are expected to be significant. To achieve\noptimal performance of our data-parallel ğ‘ ğ‘¢ğ‘ğ‘ >implementation,\nwe set the key array size to 1024 bits, so ğ‘can be 16, 32, or 64. As\nwe have seen in Fig. 2, the cost of our SIMD-based ğ‘ ğ‘¢ğ‘ğ‘ >(Snippet\n2) on ğ‘=64uint16 keys is the same as that on ğ‘=16uint64\nkeys. This means that, after compression, the height of the tree can\ndecrease and search can be accelerated.\nTree construction Our goal is to construct the tree in one pass over\nthe sorted keys and to result in leaf nodes having 75% occupancy\n(except when we are dealing with regions of sequential key values),\nwhile achieving the best possible compression. For this, we begin\nby checking whether the leaf can be filled with 16-bit differences\nfor the keys. If this is not feasible, we reattempt the process by\nchecking if half of the keys can be stored as 32-bit differences. If\nthis attempt also fails, we conclude by storing the exact 64-bit keys.\nSearch To apply ğ‘ ğ‘¢ğ‘ğ‘ >ğ‘˜at a node ğ‘£we first compute ğ‘˜â€²=ğ‘˜âˆ’ğ‘£.ğ‘˜0,\nwhere ğ‘£.ğ‘˜0is the first key value of ğ‘£, stored explicitly in ğ‘£â€™s meta-\ndata (this is the only decompression overhead). Then, we apply\nğ‘ ğ‘¢ğ‘ğ‘ >ğ‘˜â€²to the node to find the position of the node pointer to\nfollow. The same procedure is applied at the leaf nodes for ğ‘ ğ‘¢ğ‘ğ‘â‰¥ğ‘˜;\nthe position of ğ‘˜â€²=ğ‘˜âˆ’ğ‘£.ğ‘˜0corresponds to the position of ğ‘˜, or if\nğ‘ ğ‘¢ğ‘ğ‘â‰¥ğ‘˜â€²returns NULL, ğ‘˜does not exist.\nInsert We can directly use the Bğ‘†-tree insertion algorithm to insert\na new key ğ‘˜, by first running the search algorithm discussed above\nto find the leaf ğ‘£and the position in ğ‘£whereto insert ğ‘˜and then store\nthe difference ğ‘˜âˆ’ğ‘£.ğ‘˜0there. The new nodes after a splitting a node\nğ‘£can be of the same type as ğ‘£, or they can be further compressed\nas they include fewer entries than ğ‘£with the first and the last one\nhaving smaller differences.\nDelete Deletion is not affected by key compression. When the key\nto be deleted is found (represented exactly or by its difference to\nğ‘˜0) at the corresponding leaf node, we simply copy into it the value\nof the next key, or MAXKEY if the deleted key is the last one in the\nleaf node. In compressed nodes, MAXKEY is the maximum value\nthat can be represented using all available bits. If the first key of a\nnode is deleted, we do not change ğ‘˜0(as it is not stored in a slot of\nthe array) and keep in the slots the differences to ğ‘˜0.\n6 IMPLEMENTATION DETAILS\nThis section presents some implementation details of the Bğ‘†-tree\nthat have a significant impact in its performance in practice.\nNode size and structure. As in previous work [ 13,21,38,42,44,\n56,58,64,66,67,69,71], we aim at indexing large keys, each being\na 64-bit unsigned integer. To take full advantage of our SIMD ğ‘ ğ‘¢ğ‘ğ‘ >\nimplementation, each node stores a maximum of ğ‘=16entries;\n\nBğ‘†-tree: A gapped data-parallel B-tree\nbased on this, we allocate 16Ã—64=1024 bits for the keys of each\nnode. Hence, the keys of each node (internal or leaf) fill two cache\nlines (each cache line can store 64 bytes). This means that for ğ‘ ğ‘¢ğ‘ğ‘ >,\nwe perform 2 SIMD instructions per node by loading the keys at\n2 registers of 512 bits (8 keys at each register). 1024 bits are also\nallocated for keys in the compressed CBğ‘†-tree nodes; a compressed\nkey array may have 32 32-bit or 64 16-bit entries.\nMemory management. To store the Bğ‘†-tree in memory, we uti-\nlize two main structures: one to store the inner nodes and another\nfor the leaf nodes. Each inner node consists of two arrays with 16\nentries. The first array holds 64-bit keys, while the second contains\n32-bit references to nodes. 32 bits are sufficient for the references\nbecause they are in fact offsets to fixed-length slots in memory ar-\nrays allocated for nodes (one for inner nodes and one for leaf nodes).\nThe auxiliary data for each node are put in a separate dedicated\narray aligned with the node arrays. Hence, each inner node has a\nsize of 192 bytes, which fits into 3 cache lines. Our tested Bğ‘†-tree\nimplementation only has keys and no record-ids in its leaves, so a\nleaf node contains a single array of 16 64-bit keys, with each leaf\nnode occupying 128 bytes, fitting into 2 cache lines. The inner nodes\nare stored in a contiguous array, aligned to Transparent Huge Pages\n(2 MB) for efficiency. The leaf nodes are also stored in a contiguous\narray, but their alignment depends on the array size. If the array is\nsmaller than 3 GB, we align it using huge pages. Otherwise, it is\naligned per cache line (64 bytes). Alignment plays a crucial role in\noptimizing both cache efficiency and the use of SIMD operations,\nmaking it a key factor in the performance of Bğ‘†-tree. By aligning\ndata to cache lines, we minimize cache misses and ensure that the\nCPU can retrieve entire nodes in a single memory access, signif-\nicantly speeding up operations. By aligning inner nodes to huge\npages (2 MB), we reduce translation lookaside buffer (TLB) misses.\nWe also make use of __builtin_prefetch, a compiler intrinsic that\nallows us to pre-load data into the cache before it is needed, reduc-\ning latency. By combining cache-line and SIMD-friendly alignment,\nalong with appropriate use of __builtin_prefetch, Bğ‘†-tree allows\nfor SIMD acceleration, and reduces memory access latency, leading\nto significantly better overall performance.\nCompress or not? The compressed version of Bğ‘†-tree with variable-\ncapacity nodes (Section 5) may reduce the memory footprint of\nthe index and improve its performance, but also comes with the\noverhead of explicitly keeping the first key of a node, which does\nnot pay off for nodes having 64-bit differences that cannot be com-\npressed.3Hence, we employ a decision mechanism for choosing\nbetween the construction of a Bğ‘†-tree or a compressed Bğ‘†-tree,\nbased on the input data. Before bulk-loading the tree, we virtually\nsplit the sorted keys input into segments of 13 keys each, subtract\nthe smallest key from the largest key in each bucket, and calculate\nthe number of leading zeros. After performing these calculations\nfor all segments, we take the average number of leading zeros. If\nthis average is greater or equal to 32 bits, we conclude that the\ndataset can benefit from a compact Bğ‘†-tree compression, and we\ngo ahead with its construction. Otherwise, we create a standard\n(uncompressed) Bğ‘†-tree. The selection of 13 keys is not arbitrary,\n3As we want all leaf nodes to have the same fixed size (for alignment purposes), we do\nnot allow the same Bğ‘†-tree to have both uncompressed and compressed leaves.as we put 25% gaps at each leaf, and the 13th key serves as the sep-\narator for the node. We found out that compression is not effective\nfor inner nodes, so our final compressed Bğ‘†-tree implementation\nhas uncompressed inner nodes and compressed leaves.\n7 CONCURRENCY CONTROL\nA wide range of concurrency control techniques has been proposed\nfor the B+-tree and other indices, including lock coupling [12, 27],\nright-sibling pointers [ 39], fine-grained locking with lock coupling\nand logical removals [ 17], Bw-treeâ€™s lock-free mechanism [ 44,61]\nand Read-Optimized Write Exclusion (ROWEX) [ 43]. For B-trees,\nLeis et al. [ 41,43] proposed an Optimistic Lock Coupling (OLC)\ntechnique, which is easy to implement and highly efficient. In OLC,\nwhen a thread wants to read or modify a node, it first acquires an\noptimistic read lock, allowing it to traverse the tree while maintain-\ning a local copy of the nodeâ€™s state. If the thread intends to perform\nan update, it checks whether the node has been modified by another\nthread since it was read. If not, it applies the changes and commits\nthem atomically. In the event of a conflict (i.e., if another thread\nhas modified the node), the thread rolls back its changes and retries\nthe operation from the root of the tree.\nOur current implementation of Bğ‘†-tree employs the OLC mech-\nanism [ 41,43] with a slight modification in how node splits are\nhandled. In the original OLC, when a thread takes on a write task, it\nsplits the first full node (inner or leaf) it encounters during traversal.\nAfter completing the split, the thread restarts its traversal until it\nfinds a path where every node has at least one empty slot. This\nefficiently reorganizes node contents, ensuring that insertions trig-\ngering splits at multiple tree levels are managed without requiring\na global lock. The use of sparse nodes in Bğ‘†-tree allows for a more\nflexible procedure. Specifically, when a split occurs at a leaf node,\nthe restart process is only necessary if the separator must be stored\nin the first node of the couple, which has no available free slot. This\nrelaxation reduces unnecessary re-traversals.\n8 EXPERIMENTS\nWe experimentally compare Bğ‘†-tree to alternative main-memory\nindices (learned and non-learned). As in previous work [ 13,42,62],\nwe have built and compared indices for key data only; record ids or\nreferences are not stored, but the objective of each index is to locate\nthe position(s) of the searched key(s). The implementation of all\nmethods is in C++ and compiled with gcc(v13) using the flags -O3\nand-march=native . The experiments were conducted on a system\nwith an 11th Gen Intel Â®Coreâ„¢i7-11700K processor with 8 cores,\nrunning at 3.60 GHz, 128 GB of RAM, having AVX 512 support.\nThe operating system used was Ubuntu 22.04. We extended the\ncodebase of GRE [62] to include Bğ‘†-tree.\n8.1 Setup\nDatasets . We ran our tests on standard benchmarking real datasets\nused in previous work [ 34,49,62]; each on consists of unsigned\n64-bit integer keys. In Amazon BOOKS [ 34,49], each key represents\nthe popularity of a specific book. In FB [ 34,49,55], each key is a\nFacebook user-id. OSM [ 34,49] contains unique integer-encoded\nlocations from OpenStreetMap. GENOME [ 54,62] includes loci\npairs from human chromosomes. PLANET [ 2,62], a planet-wide\n\nDimitrios Tsitsigkos, Achilleas Michalopoulos, Nikos Mamoulis, and Manolis Terrovitis\ncollection of integer-encoded geographic locations compiled by\nOpenStreetMap. According to [ 62,71], OSM, FB, GENOME, and\nPLANET are complex real-world datasets that can pose challenges\nfor learned indices. In contrast, the key distribution of BOOKS is\neasy to learn. We did not conduct experiments using synthetic\ndatasets with common distributions, as, according to [ 49], it would\nbe trivial for a learned index to model such distributions. Although\nwe do not include experiments with non-integer keys, Bğ‘†-tree can\nbe used for floats/doubles, by changing the SIMD intrinsics in Snip-\npet 2, and for strings, after being dictionary encoded (see [22]).\nCompetitors . We compare our proposed Bğ‘†-tree and its com-\npressed version, denoted by CBğ‘†-tree , with five updatable learned\nand non-learned indices, for which the code was publicly available\nby the authors. We did not compare to methods found inferior in\nprevious comparative studies [ 13,62] and to those with proprietary\nor unavailable code (e.g., [38, 71]).\nNon-learned Indices . STX library [ 1] is a fully optimized C++\nimplementation of a main-memory B+-tree. We use the set-based\nimplementation from STX, which does not store values in the leaf\nnodes. For its construction, we used its fast bulk-loading method.\nWe used the default block size of STX (256 bytes), so each leaf node\nholds 32 keys ( 32Ã—8=256bytes). We used two versions of the\nSTX tree: the first is the original code, referred to as B+-tree , while\nthe second version creates 25% empty space at the end of each leaf\nnode, denoted by Sparse B+-tree (for fairness, as our Bğ‘†-tree also\nproactively introduces 25% of gaps in leaves).\nWe also compare to SIMD-based implementations of HOT [ 3,13]\nand ART [ 7,42]. These tries do not support bulk-loading. However,\nwe found that pre-sorting the data improves the construction time\nfor both and results in more efficient structures. The HOT code\nrelease does not support range queries, so we implemented them\nourselves. The ART code also lacks range query support, but we\nwere unable to implement it due to the trieâ€™s complex structure.\nHOT cannot handle keys greater than 263âˆ’1, so we removed values\nexceeding this limit from certain datasets.\nLearned Indices .ALEX [5,21] and LIPP [6,64] are the state-of-\nthe-art updatable learned indices for key-value pairs [ 62]. To use\nthem, for each dataset we used as value of each key the key itself.\nALEX and LIPP both support bulk-loading. Neither uses SIMD\nintrinsics during search. The reason is their large leaves (in the\norder of KB), where SIMD-based search is not effective (see Fig. 2).\nALEX uses exponential search in leaves, while LIPP eliminates the\nlast-mile search entirely by ensuring that predicted positions are\nexact for each key in the index.\nNote that all aforementioned Bğ‘†-tree competitors do not utilize\nhuge pages, as this would not bring a significant impact on them.\nIn the implementations of these methods, each node occupies a\nrandom location in memory and connects to its child nodes via\npointers. This design results in the retrieval of a significantly larger\nnumber of memory pages into the TLB cache, thereby diminishing\nthe advantages that huge pages would otherwise offer.\nWorkloads . We used several different workloads used to measure\nperformance. First, we randomly selected 150 million entries from\neach dataset for the construction phase. where we sorted the data\nand applied bulk-loading (except for HOT and ART, which, however,Table 1: Construction time (for 150 million keys)\nConstruction Time (sec)\nIndices / Datasets BOOKS OSM FB GENOME PLANET\nBğ‘†-tree 0.33 0.33 0.33 0.33 0.33\nCBğ‘†-tree 0.35 0.32 0.18 0.20 0.18\nB+-tree 0.39 0.39 0.39 0.39 0.39\nSparse B+-tree 0.50 0.50 0.50 0.50 0.50\nHOT 15.61 16.65 16.56 16.23 15.33\nART 5.65 6.11 6.62 6.42 6.19\nALEX 25.43 41.60 45.46 30.74 30.06\nLIPP 9.58 9.31 6.98 7.01 7.05\nTable 2: Memory footprint (for 150 million keys)\nMemory Footprint (GB)\nIndices / Datasets BOOKS OSM FB GENOME PLANET\nBğ‘†-tree 1.84 1.84 1.84 1.84 1.84\nCBğ‘†-tree 2.03 1.75 0.55 0.80 0.51\nB+-tree 1.41 1.41 1.41 1.41 1.41\nSparse B+-tree 1.88 1.88 1.88 1.88 1.88\nHOT 1.78 1.79 1.83 1.92 1.71\nART 7.23 7.36 7.42 7.38 7.85\nALEX 2.73 2.77 2.77 2.73 2.73\nLIPP 13.51 14.69 10.89 11.66 11.62\nboth benefit from sorting). For our workloads, we used 50 million\nkeys, that are selected randomly (i.e., queries and updates hit a\nrandom region of the space). Our workloads are:\nâ€¢Read-Only (Workload A) : 100% reads (equality searches).\nâ€¢Write-Only (Workload B) : 100% writes (insertions).\nâ€¢Read-Write (Workload C) : 50% reads, 50% writes.\nâ€¢Range-Write (Workload D) : 95% range searches, 5% writes.\nâ€¢Mixed (Workload E) : 60% reads, 35% writes, 5% deletions.\n8.2 Construction Cost and Memory Footprint\nFirst, we compare all tested methods with respect to their construc-\ntion cost and memory footprint for 150M keys. Since all indices\nrequire the data to be sorted (or benefit from sorting), we exclude\nsorting from the construction cost. Table 1 presents the construction\ntimes, while Table 2 shows the memory footprints. The construction\ntime of our Bğ‘†-tree also includes the decision-making mechanism\n(roughly takes 0.03 sec) on whether we will construct a Bğ‘†-tree or\na CBğ‘†-tree (see Section 6). To calculate the memory usage of each\nmethod, we utilize the C function getrusage4. Since all learned\nindices essentially store 64-bit values together with the keys, we re-\nport half of their measured memory requirements, to approximate\nthe memory required just for the keys and their inner structure.\nAs expected, non-learned indices (except from the CBğ‘†-tree)\nhave a stable construction time and memory footprint. On the\nother hand, the build cost of learned indices is high and is affected\nby the data distribution; their bottleneck is in training the models\nthat predict key positions. The Sparse B+-tree is larger than B+-tree,\nso it is costlier to build and has a larger memory footprint. Our\nBğ‘†-tree (and its compressed CBğ‘†-tree version) is faster to build\ncompared to B+-tree and Sparse B+-tree mainly due to our better\n4https://man7.org/linux/man-pages/man2/getrusage.2.html\n\nBğ‘†-tree: A gapped data-parallel B-tree\nmemory management (static pre-allocation vs. dynamic allocation)\nand because we use offset addressing instead of memory pointers.\nCBğ‘†-tree has the smallest memory footprint than all methods\nfor FB, GENOME, and PLANET because of its high compression\neffectiveness, which also has a positive impact to the construction\ntime. On the other hand, CBğ‘†-tree occupies more space than Bğ‘†-tree\non BOOKS because the distribution of keys there does not provide\nmany compression opportunities; in this case, most leaf nodes store\n64-bit differences and also need to explicitly store the 64-bit key\nof the first key, which renders the size of the index even larger\nthan that of Bğ‘†-tree. Note that for BOOKS and OSM, our decision\nmechanism (see Section 6) chooses to construct a Bğ‘†-tree while for\nFB, GENOME, and PLANET it decides to construct a CBğ‘†-tree.\nHOT is very expensive to build compared to Bğ‘†-tree and B+-tree,\nbecause HOT requires keys to be inserted one at a time, lacking\na bulk-loading mechanism. HOT uses slightly less memory than\nBğ‘†-tree; however, our CBğ‘†-tree has a significantly smaller memory\nfootprint than HOT in three out of the five datasets. ART is faster\nto build compared to HOT (but occupies more space), due to its\nsimpler insertion process, however, it is also slow compared to\nB-trees for the same reason as HOT.\nNote that all versions of B+-tree and Bğ‘†-tree have smaller mem-\nory footprints compared to the learned indices. LIPP is faster to\nconstruct compared to ALEX, because it does not readjust the mod-\nels when conflicts occur (two or more keys are mapped to the same\nposition). LIPPâ€™s downside is its large memory footprint, caused by\ncreating new nodes during conflicts, whereas node reorganization\nin ALEX results in a smaller memory footprint.\nIn conclusion, the Bğ‘†-tree has low construction cost, with the\nB+-tree exhibiting comparable performance alongside a very small\nmemory footprint. Additionally, the CBğ‘†-tree achieves the fastest\nconstruction time and consumes from 56% to 94% less memory than\nall methods in FB, GENOME, and PLANET. Our results align with\nthe findings of Wongkham et al. [ 62], which conclude that memory\nefficiency is not a distinct advantage of updatable learned indices.\n8.3 Single-Threaded Throughput\nNext, we evaluate the throughput of all methods (single-threaded)\non the five workloads described in Section 8.1. Figure 5 presents\nthe throughput (millions of operations per second) of all methods\nfor Workload A (read-only). Bğ‘†-tree and CBğ‘†-tree outperform all\ncompetitors across the board except for BOOKS, where ALEX is\nmarginally faster than CBğ‘†-tree and has the same throughput as Bğ‘†-\ntree. The excellent performance of ALEX on BOOKS is due to the\nsmooth key distribution there, which is easy for ALEX to learn. On\naverage, our methods have a significant performance gap compared\nto the nearest competitor. Specifically, Bğ‘†-tree is roughly 2.5x faster\nthan HOT on OSM, 1.5x faster than LIPP on FB and GENOME and\n2.2x faster than ART on FB and GENOME. CBğ‘†-tree is about 7%\nslower than ALEX on BOOKS, but much faster than previous work\non all other datasets and even faster than Bğ‘†-tree on FB, GENOME,\nand PLANET, while having a much smaller memory footprint. CBğ‘†-\ntree exploits the highly compressible keys of FB, GENOME, and\nPLANET to drastically reduce the capacity of leaf nodes and the\noverall space required for the index. This increases the likelihoodthat multiple searches hit the same leaves, exploiting the memory\ncache, as we will also show in the next set of experiments.\nFor Workload B (write-only), as Figure 6 shows, Bğ‘†-tree outper-\nforms all methods, while CBğ‘†-tree loses to ALEX only on BOOKS.\nART achieves relatively good performance in this workload because\nof its lazy expansion and path compression. LIPP is more robust\nthan ALEX due to its accurate prediction mechanism, which reduces\nthe need for frequent node splits or rebalancing due to the more\nappropriate placement of keys during its construction. Observe that\nthe Sparse B+-tree is faster than the B+-tree because it requires\nmuch fewer splits. CBğ‘†-tree has competitive write performance\ncompared to Bğ‘†-tree.\nOn Workload C (Figure 7), the performance of all methods stands\nbetween that of Workload A (read-only) and Workload B (write-\nonly), which is expected. For the results of Workload D (range-\nwrite), see Figure 8. The results are similar compared to Workload\nA (read-only) since Workload D is read-heavy. Range queries re-\ntrieve 153 keys on average. LIPP performs much worse than ALEX,\nas LIPPâ€™s structure is not optimized for range scans. On the other\nhand, ALEX has large nodes and facilitates jumps to sibling nodes.\nHOT is not optimized for range scans, so it performs poorly. On the\ncompressible datasets (FB, GENOME, PLANET), CBğ‘†-tree outper-\nforms Bğ‘†-tree as its compressed leaves have larger capacities and\nfewer leaves need to be scanned per query. Finally, Figure 9 shows\nthe results using Workload E (read-write-delete). The results are\nsimilar to those for Workloads A and B. Bğ‘†-tree and CBğ‘†-tree out-\nperform all competitors across all datasets except for BOOKS, where\nCBğ‘†-tree is slightly inferior to ALEX. Deletions do not impose an\noverhead to Bğ‘†-tree and all other methods.\n8.4 Performance Counters\nBesides throughput, we also compared all methods with respect to\nvarious performance counters, including instructions executed, cy-\ncles, mispredicted branches, and misses in L1 and TLB (Translation\nLookaside Buffer) misses. As representative datasets, we selected\nBOOKS and FB.5We chose to measure Workload C (reads - writes),\nas it is update-heavy and the unpredictable nature of writes can lead\nto numerous splits, stressing the indices. To calculate these metrics,\nwe utilized Leisâ€™s perf_event code [ 4]. The average performance\nmeasures per operation for all methods are presented in Table 3\nand Table 4 for BOOKS and FB, respectively.\nBğ‘†-tree needs the smallest number of instructions and cycles\non BOOKS, with CBğ‘†-tree being a close runner up. For FB, CBğ‘†-\ntree needs less cycles, which explains its superiority to Bğ‘†-tree.\nOur methods are simple and efficient, benefiting from the use of\nSIMD instructions and alignment, which reduce the number of\ncycles and instructions required for each task. They have the fewest\nmispredicted branches, which is expected due to their branchless\nsearch; mispredicted branches in Bğ‘†-tree arise from the insertions.\nRegarding L1, on average, our algorithms incur 16 cache misses,\nwhich is consistent with our expectation. Specifically, the height of\nour trees is 6 and their fanout is 16; we encounter 2 cache misses per\ntree level and 2 cache misses at the leaf level ( 2Ã—6+2=14misses),\nwith the remaining 2 misses caused by insertions. For BOOKS, ART\n5Recall that ALEX presents competitive behavior on BOOKS, whereas on FB Bğ‘†-tree\nand CBğ‘†-tree have a large performance gap to other methods.\n\nDimitrios Tsitsigkos, Achilleas Michalopoulos, Nikos Mamoulis, and Manolis Terrovitis\n024681012Throughput (Mop/s)\nIndices02468101214Throughput (Mop/s)\nIndices012345678910Throughput (Mop/s)\nIndices0246810Throughput (Mop/s)\nIndices024681012Throughput (Mop/s)\nIndicesBOOKSBs-tree B+-tree Sparse B+-tree HOT ART ALEX LIPP\nOSM FB GENOME PLANETCBs-tree\nFigure 5: Workload A : Read Only (100%)\n0123456Throughput (Mop/s)\nIndices0123456Throughput (Mop/s)\nIndices0123456Throughput (Mop/s)\nIndices0123456Throughput (Mop/s)\nIndices0123456Throughput (Mop/s)\nIndicesBOOKSBs-tree B+-tree Sparse B+-tree HOT ART ALEX LIPP\nOSM FB GENOME PLANETCBs-tree\nFigure 6: Workload B : Write Only (100%)\n0123456Throughput (Mop/s)\nIndices01234567Throughput (Mop/s)\nIndices0123456Throughput (Mop/s)\nIndices0123456Throughput (Mop/s)\nIndices0123456Throughput (Mop/s)\nIndicesBOOKSBs-tree B+-tree Sparse B+-tree HOT ART ALEX LIPP\nOSM FB GENOME PLANETCBs-tree\nFigure 7: Workload C : Read (50%) - Write (50%)\n00.511.522.53Throughput (Mop/s)\nIndices00.511.522.533.5Throughput (Mop/s)\nIndices00.511.522.53Throughput (Mop/s)\nIndices00.511.522.533.5Throughput (Mop/s)\nIndices00.511.522.53Throughput (Mop/s)\nIndicesBOOKSBs-tree B+-tree Sparse B+-tree HOT ART ALEX LIPP\nOSM FB GENOME PLANETCBs-treeART: Not Supported\nART: Not Supported\nART: Not Supported\nART: Not Supported\nART: Not Supported\nFigure 8: Workload D : Range (95%) - Write (5%)\nachieves the best performance in terms of L1 cache misses, though\nour algorithms are close in comparison. Lastly, in terms of TLB\nmisses, our algorithms exhibit outstanding performance relative to\nour competitors. This can be attributed to our use of huge pages (see\nSec. 6). Overall, the performance counters show that our Bğ‘†-tree\nand CBğ‘†-tree are fully optimized and cache-efficient via the use of\nSIMD instructions, huge pages, and branchless code.Table 3: Performance counters for BOOKS, Workload C\nWorkload C - Dataset: BOOKS\nIndices / Events Instr. Cycles Misp. Branches L1 Misses TLB Misses\nBğ‘†-tree 220.18 884.16 1.03 16.41 0.61\nCBğ‘†-tree 277.87 997.57 1.03 16.33 0.05\nB+-tree 656.84 2806.93 11.49 33.49 4.44\nSparse B+-tree 565.33 2408.02 10.61 28.29 3.84\nHOT 898.99 2585.71 3.44 31.78 4.75\nART 435.56 1443.29 2.12 14.12 3.06\nALEX 612.57 1165.58 5.23 21.03 2.22\nLIPP 300.47 1379.23 1.95 18.66 5.02\n\nBğ‘†-tree: A gapped data-parallel B-tree\n01234567Throughput (Mop/s)\nIndices01234567Throughput (Mop/s)\nIndices01234567Throughput (Mop/s)\nIndices012345678Throughput (Mop/s)\nIndices01234567Throughput (Mop/s)\nIndicesBOOKSBs-tree B+-tree Sparse B+-tree HOT ART ALEX LIPP\nOSM FB GENOME PLANETCBs-tree\nFigure 9: Workload E : Read (60%) - Write (35%) - Deletions (5%)\n051015202530354045\n2 4 6 8Throughput (Mop/s)\n# of coresBOOKSBs-treeOLC B+-treeOLC HOT -ROWEX ART-OLC ALEX + LIPP+\n051015202530354045\n2 4 6 8Throughput (Mop/s)\n# of coresOSM\n051015202530354045\n2 4 6 8Throughput (Mop/s)\n# of coresFB\n051015202530\n2 4 6 8Throughput (Mop/s)\n# of coresGENOME\n051015202530\n2 4 6 8Throughput (Mop/s)\n# of coresPLANET\nFigure 10: Workload A : Read Only (100%) - OLC\n024681012141618\n2 4 6 8Throughput (Mop/s)\n# of coresBOOKSBs-treeOLC B+-treeOLC HOT -ROWEX ART-OLC ALEX + LIPP+\n024681012141618\n2 4 6 8Throughput (Mop/s)\n# of coresOSM\n024681012141618\n2 4 6 8Throughput (Mop/s)\n# of coresFB\n024681012\n2 4 6 8Throughput (Mop/s)\n# of coresGENOME\n02468101214\n2 4 6 8Throughput (Mop/s)\n# of coresPLANET\nFigure 11: Workload B : Write Only (100%) - OLC\n0246810121416\n2 4 6 8Throughput (Mop/s)\n# of coresBOOKSBs-treeOLC B+-treeOLC HOT -ROWEX ART-OLC ALEX + LIPP+\n0246810121416\n2 4 6 8Throughput (Mop/s)\n# of coresOSM\n0246810121416\n2 4 6 8Throughput (Mop/s)\n# of coresFB\n0246810121416\n2 4 6 8Throughput (Mop/s)\n# of coresGENOME\n0246810121416\n2 4 6 8Throughput (Mop/s)\n# of coresPLANET\nFigure 12: Workload C : Read (50%) - Write (50%) - OLC\n8.5 Multi-Threaded Workloads\nNext, we present multi-threaded experiments, where workloads\nare executed by multiple threads simultaneously. For concurrency\ncontrol, our Bğ‘†-tree uses OLC as described in Section 7, ART and B+-\ntree use OLC, HOT uses ROWEX, and ALEX and LIPP use optimistic\nlocking. We applied Workload A, Workload B, and Workload C,\nwhich are the most representative ones. Figure 10, presents the\nthroughput (millions of operations per second) of all methods for\nWorkload A (read-only). Observe that the access methods exhibit\nthe same behavior as in single-threaded processing. Our Bğ‘†-treeoutperforms all competitors, with ALEX being competitive only on\nBOOKS. For the write-only Workload B (Figure 11), ART achieves\nthe best performance, while our Bğ‘†-tree is competitive in some\ncases. ARTâ€™s strong performance is largely due to OLC, which is\nwell-suited to its structure. For the mixed read-write Workload\nC (Figure 12), our Bğ‘†-tree outperforms all competitors across all\ndatasets. All methods scale well with the number of cores.\n\nDimitrios Tsitsigkos, Achilleas Michalopoulos, Nikos Mamoulis, and Manolis Terrovitis\n0246810121416\n510 15 20 25 30 40 50Execution Time (sec)\n# of insertions ( â¨‰106)BOOKS\n0246810121416\n510 15 20 25 30 40 50Execution Time (sec)\n# of insertions ( â¨‰106)OSM\n0246810121416\n510 15 20 25 30 40 50Execution Time (sec)\n# of insertions ( â¨‰106)FB\n02468101214161820\n510 15 20 25 30 40 50Execution Time (sec)\n# of insertions ( â¨‰106)GENOME\n02468101214161820\n510 15 20 25 30 40 50Execution Time (sec)\n# of insertions ( â¨‰106)PLANETSIMD + Sparse Nodes Linear + Sparse Nodes\nFigure 13: Impact of data parallelism to the Insertion cost of Bğ‘†-tree\nTable 4: Performance counters for FB, Workload C\nWorkload C - Dataset: FB\nIndices / Events Instr. Cycles Misp. Branches L1 Misses TLB Misses\nBğ‘†-tree 220.51 877.78 1.03 16.43 0.59\nCBğ‘†-tree 269.33 784.75 1.09 14.29 0.00\nB+-tree 655.42 2807.87 11.54 33.45 4.41\nSparse B+-tree 566.01 2395.46 10.62 28.37 3.85\nHOT 962.58 2816.29 4.08 33.47 5.06\nART 603.69 1804.94 2.33 14.47 3.49\nALEX 1049.11 2634.38 6.97 40.31 4.72\nLIPP 276.74 1350.58 1.49 18.62 5.11\n8.6 Impact of Bğ‘†-tree design\nIn the final set of experiments, we assess the impact of the design\nchoices in Bğ‘†-tree. Figure 13 shows the impact of our data-parallel\nimplementation using gaps that copy the key values from the next\nused slot, on the cost of writes. We compare our Bğ‘†-tree implemen-\ntation to an implementation of Bğ‘†-tree, where nodes have gaps\nsimply supported by a bitmap (as in [ 21]). In this implementation,\nbranching in each node is done by linear scan using the bitmap to\nignore unused slots.6The figure shows that our SIMD implementa-\ntion with gaps reduces the cost of insertions (and node searches,\nwhich are part of the insertion procedure) by 30%-35%.\n024681012\nBOOKSOSMFBGENOMEPLANETThroughput (Mop/s)DatasetsNHP + CountingHP + CountingNHP + SIMDHP + SIMD\nFigure 14: Effect of Bğ‘†-tree design (Workload A)\nWe also analyze the effectiveness of the Bğ‘†-tree features, with the\nmost powerful ones being SIMD-based branching (Snippet 2) and\ntransparent huge pages. We implemented four versions of our struc-\nture: one without any optimizations (NHP + Counting), one with\nonly huge pages enabled (HP + Counting), another with only SIMD\ninstructions enabled (NHP + SIMD), and finally, a fully optimized\nversion that combines both huge pages and SIMD instructions (HP\n+ SIMD). Figure 14 shows the throughput of the four versions of\n6As nodes are small, linear scan is better than binary search. Besides, binary and\nexponential search require values in gaps that obey the total order [21].Bğ‘†-tree for Workload A (reads). We observe that both optimizations\n(HP and SIMD) boost Bğ‘†-tree.\n8.7 Summary of Experimental Findings\nIn summary, Bğ‘†-tree and CBğ‘†-tree exhibit excellent and robust per-\nformance for different workloads and different datasets of varying\ndistribution, being superior than all competitors in most cases, in\nsingle- and multi-threaded processing. They also have the lowest\nconstruction cost and memory footprint. Note that our decision\nmechanism, which imposes a small overhead in the construction\n(up to 10% of the construction cost) decides automatically and cor-\nrectly which of Bğ‘†-tree or CBğ‘†-tree to build for a given dataset.\nBğ‘†-tree outperforms trie-based indices like HOT and ART, except\nfor multithreaded write-heavy workloads, where ART is superior.\nStill, we have not implemented yet Bğ‘†-tree for string keys, where\ntrie-based structures are known to perform best. Regarding up-\ndatable learned indices, our study shows that they are typically\noutperformed by optimized non-learned indices like our Bğ‘†-tree.\n9 CONCLUSIONS\nWe proposed Bğ‘†-tree, a main-memory B+-tree that uses a simple\nand intuitive data-parallel implementation for branching at each\nlevel during search and updates. Bğ‘†-tree is based on a novel im-\nplementation of gaps in nodes by duplicating existing keys, which\ndoes not affect SIMD-based branchless search in nodes. We use\nFOR compression mechanism for nodes that include keys with\nsmall differences. Our experimental evaluation demonstrates the\nsuperiority of Bğ‘†-tree compared to open-source state-of-the-art\nnon-learned and learned indices, with respect to construction time,\nmemory footprint, and throughput for various workloads that in-\nclude queries and updates in single- and multi-threaded processing.\nInspired by [ 58], in the future, we plan to implement Bğ‘†-tree in a\nhybrid setup, where the top (and infrequently updated levels) are\nhandled by the GPU that allows much higher data parallelism and\nthe lower (frequently updated) levels are handled by the CPU. In\naddition, we will study the support of string keys in Bğ‘†-tree (one\nsolution is to use Binary, ASCII or Base64 encoding [21]).\nAVAILABILITY\nThe code of the Bğ‘†-tree is publicly available at\ngithub.com/dTsitsigkos/BsTree-A-gapped-data-parallel-B-tree\n\nBğ‘†-tree: A gapped data-parallel B-tree\nACKNOWLEDGEMENTS\nWork supported by project MIS 5154714 of the National Recov-\nery and Resilience Plan Greece 2.0, funded by the EU under the\nNextGenerationEU Program.\nREFERENCES\n[1] 2013. STX B+Tree code . https://github.com/bingmann/stx-btree/tree/master\n[2]2017. Google Cloud. OpenStreetMap. https://console.cloud.google.com/\nmarketplace/details/openstreetmap/geo-openstreetmap?project=practice-\nbigtable.\n[3] 2018. HOT code . https://github.com/speedskater/hot\n[4] 2018. PerfEvent code . https://github.com/viktorleis/perfevent?tab=readme-ov-\nfile\n[5] 2020. ALEX code . https://github.com/microsoft/ALEX\n[6] 2021. LIPP code . https://github.com/Jiacheng-WU/lipp\n[7]2022. ART code . https://github.com/pohchaichon/ARTSynchronized/tree/\n808372ded6b8c5a6d3a1741090510b79042f2aa7\n[8] Devesh Agrawal, Deepak Ganesan, Ramesh K. Sitaraman, Yanlei Diao, and Shashi\nSingh. 2009. Lazy-Adaptive Tree: An Optimized Index Structure for Flash Devices.\nProc. VLDB Endow. 2, 1 (2009), 361â€“372. https://doi.org/10.14778/1687627.1687669\n[9]Nikolas Askitis and Ranjan Sinha. 2007. HAT-Trie: A Cache-Conscious Trie-\nBased Data Structure For Strings. In Computer Science 2007. Proceedings of the\nThirtieth Australasian Computer Science Conference (ACSC2007). Ballarat, Victoria,\nAustralia, January 30 - February 2, 2007. Proceedings (CRPIT, Vol. 62) , Gillian Dob-\nbie (Ed.). Australian Computer Society, 97â€“105. http://crpit.scem.westernsydney.\nedu.au/abstracts/CRPITV62Askitis.html\n[10] Nikolas Askitis and Ranjan Sinha. 2010. Engineering scalable, cache and space\nefficient tries for strings. VLDB J. 19, 5 (2010), 633â€“660. https://doi.org/10.1007/\nS00778-010-0183-9\n[11] Manos Athanassoulis and Anastasia Ailamaki. 2014. BF-Tree: Approximate Tree\nIndexing. Proc. VLDB Endow. 7, 14 (2014), 1881â€“1892. https://doi.org/10.14778/\n2733085.2733094\n[12] Rudolf Bayer and Mario Schkolnick. 1977. Concurrency of Operations on B-Trees.\nActa Informatica 9 (1977), 1â€“21. https://doi.org/10.1007/BF00263762\n[13] Robert Binna, Eva Zangerle, Martin Pichl, GÃ¼nther Specht, and Viktor Leis. 2018.\nHOT: A Height Optimized Trie Index for Main-Memory Database Systems. In\nProceedings of the 2018 International Conference on Management of Data, SIGMOD\nConference 2018, Houston, TX, USA, June 10-15, 2018 , Gautam Das, Christopher M.\nJermaine, and Philip A. Bernstein (Eds.). ACM, 521â€“534. https://doi.org/10.1145/\n3183713.3196896\n[14] Philip Bohannon, Peter McIlroy, and Rajeev Rastogi. 2001. Main-Memory Index\nStructures with Fixed-Size Partial Keys. In Proceedings of the 2001 ACM SIGMOD\ninternational conference on Management of data, Santa Barbara, CA, USA, May\n21-24, 2001 , Sharad Mehrotra and Timos K. Sellis (Eds.). ACM, 163â€“174.\n[15] Matthias BÃ¶hm, Benjamin Schlegel, Peter Benjamin Volk, Ulrike Fischer, Dirk\nHabich, and Wolfgang Lehner. 2011. Efficient In-Memory Indexing with Gen-\neralized Prefix Trees. In Datenbanksysteme fÃ¼r Business, Technologie und Web\n(BTW), 14. Fachtagung des GI-Fachbereichs \"Datenbanken und Informationssys-\nteme\" (DBIS), 2.-4.3.2011 in Kaiserslautern, Germany (LNI, Vol. P-180) , Theo HÃ¤rder,\nWolfgang Lehner, Bernhard Mitschang, Harald SchÃ¶ning, and Holger Schwarz\n(Eds.). GI, 227â€“246. https://dl.gi.de/handle/20.500.12116/19581\n[16] Joan Boyar and Kim S. Larsen. 1992. Efficient Rebalancing of Chromatic Search\nTrees. In Algorithm Theory - SWAT â€™92, Third Scandinavian Workshop on Algorithm\nTheory, Helsinki, Finland, July 8-10, 1992, Proceedings (Lecture Notes in Computer\nScience, Vol. 621) , Otto Nurmi and Esko Ukkonen (Eds.). Springer, 151â€“164. https:\n//doi.org/10.1007/3-540-55706-7_14\n[17] Nathan Grasso Bronson, Jared Casper, Hassan Chafi, and Kunle Olukotun. 2010.\nA practical concurrent binary search tree. In Proceedings of the 15th ACM SIG-\nPLAN Symposium on Principles and Practice of Parallel Programming, PPOPP\n2010, Bangalore, India, January 9-14, 2010 , R. Govindarajan, David A. Padua, and\nMary W. Hall (Eds.). ACM, 257â€“268. https://doi.org/10.1145/1693453.1693488\n[18] Shimin Chen, Phillip B. Gibbons, and Todd C. Mowry. 2001. Improving Index\nPerformance through Prefetching. In Proceedings of the 2001 ACM SIGMOD\ninternational conference on Management of data, Santa Barbara, CA, USA, May\n21-24, 2001 , Sharad Mehrotra and Timos K. Sellis (Eds.). ACM, 235â€“246. https:\n//doi.org/10.1145/375663.375688\n[19] Shimin Chen, Phillip B. Gibbons, Todd C. Mowry, and Gary Valentin. 2002.\nFractal prefetching B Â±Trees: optimizing both cache and disk performance. In\nProceedings of the 2002 ACM SIGMOD International Conference on Management of\nData, Madison, Wisconsin, USA, June 3-6, 2002 , Michael J. Franklin, Bongki Moon,\nand Anastassia Ailamaki (Eds.). ACM, 157â€“168. https://doi.org/10.1145/564691.\n564710\n[20] Shimin Chen and Qin Jin. 2015. Persistent B+-Trees in Non-Volatile Main Memory.\nProc. VLDB Endow. 8, 7 (2015), 786â€“797. https://doi.org/10.14778/2752939.2752947[21] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid B. Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned\nIndex. In Proceedings of the 2020 International Conference on Management of\nData, SIGMOD Conference 2020, online conference [Portland, OR, USA], June 14-19,\n2020, David Maier, Rachel Pottinger, AnHai Doan, Wang-Chiew Tan, Abdussalam\nAlawini, and Hung Q. Ngo (Eds.). ACM, 969â€“984. https://doi.org/10.1145/3318464.\n3389711\n[22] Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020.\nTsunami: A Learned Multi-dimensional Index for Correlated Data and Skewed\nWorkloads. Proc. VLDB Endow. 14, 2 (2020), 74â€“86.\n[23] Ramez Elmasri and Shamkant B. Navathe. 1989. Fundamentals of Database\nSystems . Benjamin/Cummings.\n[24] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds. Proc. VLDB Endow.\n13, 8 (2020), 1162â€“1175. https://doi.org/10.14778/3389133.3389135\n[25] Jordan Fix, Andrew Wilkes, and Kevin Skadron. 2011. Accelerating braided b+\ntree searches on a gpu with cuda. In 2nd Workshop on Applications for Multi and\nMany Core Processors: Analysis, Implementation, and Performance (A4MMC), in\nconjunction with ISCA . Citeseer.\n[26] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim\nKraska. 2019. FITing-Tree: A Data-aware Index Structure. In Proceedings of\nthe 2019 International Conference on Management of Data, SIGMOD Conference\n2019, Amsterdam, The Netherlands, June 30 - July 5, 2019 , Peter A. Boncz, Stefan\nManegold, Anastasia Ailamaki, Amol Deshpande, and Tim Kraska (Eds.). ACM,\n1189â€“1206. https://doi.org/10.1145/3299869.3319860\n[27] Goetz Graefe. 2011. Modern B-Tree Techniques. Found. Trends Databases 3, 4\n(2011), 203â€“402.\n[28] Goetz Graefe. 2024. More Modern B-Tree Techniques. Found. Trends Databases\n13, 3 (2024), 169â€“249.\n[29] Goetz Graefe and Per-Ã…ke Larson. 2001. B-Tree Indexes and CPU Caches. In\nProceedings of the 17th International Conference on Data Engineering, April 2-6,\n2001, Heidelberg, Germany , Dimitrios Georgakopoulos and Alexander Buchmann\n(Eds.). IEEE Computer Society, 349â€“358. https://doi.org/10.1109/ICDE.2001.\n914847\n[30] Peiquan Jin, Chengcheng Yang, Christian S. Jensen, Puyuan Yang, and Lihua Yue.\n2016. Read/write-optimized tree indexing for solid-state drives. VLDB J. 25, 5\n(2016), 695â€“717. https://doi.org/10.1007/S00778-015-0406-1\n[31] Martin V. JÃ¸rgensen, RenÃ© Bech Rasmussen, Simonas Saltenis, and Carsten\nSchjÃ¸nning. 2011. FB-tree: a B+-tree for flash-based SSDs. In 15th International\nDatabase Engineering and Applications Symposium (IDEAS 2011), September 21 -\n27, 2011, Lisbon, Portugal , Bipin C. Desai, Isabel F. Cruz, and Jorge Bernardino\n(Eds.). ACM, 34â€“42. https://doi.org/10.1145/2076623.2076629\n[32] Krzysztof Kaczmarski. 2012. B+-Tree Optimized for GPGPU. In On the Move to\nMeaningful Internet Systems: OTM 2012, Confederated International Conferences:\nCoopIS, DOA-SVI, and ODBASE 2012, Rome, Italy, September 10-14, 2012. Proceed-\nings, Part II (Lecture Notes in Computer Science, Vol. 7566) , Robert Meersman,\nHervÃ© Panetto, Tharam S. Dillon, Stefanie Rinderle-Ma, Peter Dadam, Xiaofang\nZhou, Siani Pearson, Alois Ferscha, Sonia Bergamaschi, and Isabel F. Cruz (Eds.).\nSpringer, 843â€“854. https://doi.org/10.1007/978-3-642-33615-7_27\n[33] Changkyu Kim, Jatin Chhugani, Nadathur Satish, Eric Sedlar, Anthony D.\nNguyen, Tim Kaldewey, Victor W. Lee, Scott A. Brandt, and Pradeep Dubey.\n2010. FAST: fast architecture sensitive tree search on modern CPUs and\nGPUs. In Proceedings of the ACM SIGMOD International Conference on Man-\nagement of Data, SIGMOD 2010, Indianapolis, Indiana, USA, June 6-10, 2010 ,\nAhmed K. Elmagarmid and Divyakant Agrawal (Eds.). ACM, 339â€“350. https:\n//doi.org/10.1145/1807167.1807206\n[34] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2019. SOSD: A Benchmark for Learned\nIndexes. NeurIPS Workshop on Machine Learning for Systems (2019).\n[35] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass learned\nindex. In Proceedings of the Third International Workshop on Exploiting Artificial\nIntelligence Techniques for Data Management, aiDM@SIGMOD 2020, Portland,\nOregon, USA, June 19, 2020 , Rajesh Bordawekar, Oded Shmueli, Nesime Tatbul,\nand Tin Kam Ho (Eds.). ACM, 5:1â€“5:5. https://doi.org/10.1145/3401071.3401659\n[36] Thomas Kissinger, Benjamin Schlegel, Dirk Habich, and Wolfgang Lehner. 2012.\nKISS-Tree : smart latch-free in-memory indexing on modern architectures. In Pro-\nceedings of the Eighth International Workshop on Data Management on New Hard-\nware, DaMoN 2012, Scottsdale, AZ, USA, May 21, 2012 , Shimin Chen and Stavros\nHarizopoulos (Eds.). ACM, 16â€“23. https://doi.org/10.1145/2236584.2236587\n[37] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In Proceedings of the 2018 International\nConference on Management of Data, SIGMOD Conference 2018, Houston, TX, USA,\nJune 10-15, 2018 , Gautam Das, Christopher M. Jermaine, and Philip A. Bernstein\n(Eds.). ACM, 489â€“504. https://doi.org/10.1145/3183713.3196909\n\nDimitrios Tsitsigkos, Achilleas Michalopoulos, Nikos Mamoulis, and Manolis Terrovitis\n[38] Yongsik Kwon, Seonho Lee, Yehyun Nam, Joong Chae Na, Kunsoo Park, Sang K.\nCha, and Bongki Moon. 2023. DB+-tree: A new variant of B+-tree for main-\nmemory database systems. Inf. Syst. 119 (2023), 102287.\n[39] Philip L. Lehman and S. Bing Yao. 1981. Efficient Locking for Concurrent Op-\nerations on B-Trees. ACM Trans. Database Syst. 6, 4 (1981), 650â€“670. https:\n//doi.org/10.1145/319628.319663\n[40] Tobin J. Lehman and Michael J. Carey. 1986. A Study of Index Structures for\nMain Memory Database Management Systems. In VLDBâ€™86 Twelfth International\nConference on Very Large Data Bases, August 25-28, 1986, Kyoto, Japan, Proceedings ,\nWesley W. Chu, Georges Gardarin, Setsuo Ohsuga, and Yahiko Kambayashi (Eds.).\nMorgan Kaufmann, 294â€“303. http://www.vldb.org/conf/1986/P294.PDF\n[41] Viktor Leis, Michael Haubenschild, and Thomas Neumann. 2019. Optimistic Lock\nCoupling: A Scalable and Efficient General-Purpose Synchronization Method.\nIEEE Data Eng. Bull. 42, 1 (2019), 73â€“84. http://sites.computer.org/debull/A19mar/\np73.pdf\n[42] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. The adaptive radix\ntree: ARTful indexing for main-memory databases. In 29th IEEE International\nConference on Data Engineering, ICDE 2013, Brisbane, Australia, April 8-12, 2013 ,\nChristian S. Jensen, Christopher M. Jermaine, and Xiaofang Zhou (Eds.). IEEE\nComputer Society, 38â€“49. https://doi.org/10.1109/ICDE.2013.6544812\n[43] Viktor Leis, Florian Scheibner, Alfons Kemper, and Thomas Neumann. 2016.\nThe ART of practical synchronization. In Proceedings of the 12th International\nWorkshop on Data Management on New Hardware, DaMoN 2016, San Francisco,\nCA, USA, June 27, 2016 . ACM, 3:1â€“3:8. https://doi.org/10.1145/2933349.2933352\n[44] Justin J. Levandoski, David B. Lomet, and Sudipta Sengupta. 2013. The Bw-Tree:\nA B-tree for new hardware platforms. In 29th IEEE International Conference on\nData Engineering, ICDE 2013, Brisbane, Australia, April 8-12, 2013 , Christian S.\nJensen, Christopher M. Jermaine, and Xiaofang Zhou (Eds.). IEEE Computer\nSociety, 302â€“313. https://doi.org/10.1109/ICDE.2013.6544834\n[45] Pengfei Li, Hua Lu, Rong Zhu, Bolin Ding, Long Yang, and Gang Pan. 2023. DILI:\nA Distribution-Driven Learned Index. Proc. VLDB Endow. 16, 9 (2023), 2212â€“2224.\nhttps://doi.org/10.14778/3598581.3598593\n[46] Yinan Li, Bingsheng He, Jun Yang, Qiong Luo, and Ke Yi. 2010. Tree Indexing on\nSolid State Drives. Proc. VLDB Endow. 3, 1 (2010), 1195â€“1206. https://doi.org/10.\n14778/1920841.1920990\n[47] Jihang Liu, Shimin Chen, and Lujun Wang. 2020. LB+-Trees: Optimizing Persis-\ntent Index Performance on 3DXPoint Memory. Proc. VLDB Endow. 13, 7 (2020),\n1078â€“1090. https://doi.org/10.14778/3384345.3384355\n[48] Yandong Mao, Eddie Kohler, and Robert Tappan Morris. 2012. Cache craftiness\nfor fast multicore key-value storage. In European Conference on Computer Systems,\nProceedings of the Seventh EuroSys Conference 2012, EuroSys â€™12, Bern, Switzerland,\nApril 10-13, 2012 , Pascal Felber, Frank Bellosa, and Herbert Bos (Eds.). ACM,\n183â€“196. https://doi.org/10.1145/2168836.2168855\n[49] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra,\nAlfons Kemper, Thomas Neumann, and Tim Kraska. 2020. Benchmarking Learned\nIndexes. Proc. VLDB Endow. 14, 1 (2020), 1â€“13. https://doi.org/10.14778/3421424.\n3421425\n[50] Donald R. Morrison. 1968. PATRICIA - Practical Algorithm To Retrieve In-\nformation Coded in Alphanumeric. J. ACM 15, 4 (1968), 514â€“534. https:\n//doi.org/10.1145/321479.321481\n[51] Gap-Joo Na, Sang-Won Lee, and Bongki Moon. 2012. Dynamic In-Page Logging\nfor B+-tree Index. IEEE Transactions on Knowledge and Data Engineering 24, 7\n(2012), 1231â€“1243. https://doi.org/10.1109/TKDE.2011.32\n[52] Jun Rao and Kenneth A. Ross. 1999. Cache Conscious Indexing for Decision-\nSupport in Main Memory. In VLDBâ€™99, Proceedings of 25th International Conference\non Very Large Data Bases, September 7-10, 1999, Edinburgh, Scotland, UK , Mal-\ncolm P. Atkinson, Maria E. Orlowska, Patrick Valduriez, Stanley B. Zdonik, and\nMichael L. Brodie (Eds.). Morgan Kaufmann, 78â€“89.\n[53] Jun Rao and Kenneth A. Ross. 2000. Making B+-Trees Cache Conscious in\nMain Memory. In Proceedings of the 2000 ACM SIGMOD International Conference\non Management of Data, May 16-18, 2000, Dallas, Texas, USA , Weidong Chen,\nJeffrey F. Naughton, and Philip A. Bernstein (Eds.). ACM, 475â€“486.\n[54] Suhas SP Rao, Miriam H Huntley, Neva C Durand, Elena K Stamenova, Ivan D\nBochkov, James T Robinson, Adrian L Sanborn, Ido Machol, Arina D Omer, Eric S\nLander, et al .2014. A 3D map of the human genome at kilobase resolution reveals\nprinciples of chromatin looping. Cell159, 7 (2014), 1665â€“1680.\n[55] Peter Van Sandt, Yannis Chronis, and Jignesh M. Patel. 2019. Efficiently Searching\nIn-Memory Sorted Arrays: Revenge of the Interpolation Search?. In Proceedings\nof the 2019 International Conference on Management of Data, SIGMOD Conference\n2019, Amsterdam, The Netherlands, June 30 - July 5, 2019 , Peter A. Boncz, Stefan\nManegold, Anastasia Ailamaki, Amol Deshpande, and Tim Kraska (Eds.). ACM,\n36â€“53. https://doi.org/10.1145/3299869.3300075\n[56] Benjamin Schlegel, Rainer Gemulla, and Wolfgang Lehner. 2009. k-ary search\non modern processors. In Proceedings of the Fifth International Workshop on\nData Management on New Hardware, DaMoN 2009, Providence, Rhode Island,\nUSA, June 28, 2009 , Peter A. Boncz and Kenneth A. Ross (Eds.). ACM, 52â€“60.\nhttps://doi.org/10.1145/1565694.1565705[57] Jason Sewall, Jatin Chhugani, Changkyu Kim, Nadathur Satish, and Pradeep\nDubey. 2011. PALM: Parallel Architecture-Friendly Latch-Free Modifications to\nB+ Trees on Many-Core Processors. Proc. VLDB Endow. 4, 11 (2011), 795â€“806.\nhttp://www.vldb.org/pvldb/vol4/p795-sewall.pdf\n[58] Amirhesam Shahvarani and Hans-Arno Jacobsen. 2016. A Hybrid B+-tree as\nSolution for In-Memory Indexing on CPU-GPU Heterogeneous Computing\nPlatforms. In Proceedings of the 2016 International Conference on Management\nof Data, SIGMOD Conference 2016, San Francisco, CA, USA, June 26 - July 01,\n2016, Fatma Ã–zcan, Georgia Koutrika, and Sam Madden (Eds.). ACM, 1523â€“1538.\nhttps://doi.org/10.1145/2882903.2882918\n[59] Dimitrios Siakavaras, Panagiotis Billis, Konstantinos Nikas, Georgios I. Goumas,\nand Nectarios Koziris. 2020. Efficient Concurrent Range Queries in B+-trees\nusing RCU-HTM. In SPAA â€™20: 32nd ACM Symposium on Parallelism in Algorithms\nand Architectures, Virtual Event, USA, July 15-17, 2020 , Christian Scheideler and\nMichael Spear (Eds.). ACM, 571â€“573. https://doi.org/10.1145/3350755.3400237\n[60] Li Wang, Zining Zhang, Bingsheng He, and Zhenjie Zhang. 2020. PA-Tree: Polled-\nMode Asynchronous B+ Tree for NVMe. In 36th IEEE International Conference on\nData Engineering, ICDE 2020, Dallas, TX, USA, April 20-24, 2020 . IEEE, 553â€“564.\nhttps://doi.org/10.1109/ICDE48307.2020.00054\n[61] Ziqi Wang, Andrew Pavlo, Hyeontaek Lim, Viktor Leis, Huanchen Zhang,\nMichael Kaminsky, and David G. Andersen. 2018. Building a Bw-Tree Takes\nMore Than Just Buzz Words. In Proceedings of the 2018 International Conference\non Management of Data, SIGMOD Conference 2018, Houston, TX, USA, June 10-15,\n2018, Gautam Das, Christopher M. Jermaine, and Philip A. Bernstein (Eds.). ACM,\n473â€“488. https://doi.org/10.1145/3183713.3196895\n[62] Chaichon Wongkham, Baotong Lu, Chris Liu, Zhicong Zhong, Eric Lo, and\nTianzheng Wang. 2022. Are Updatable Learned Indexes Ready? Proc. VLDB\nEndow. 15, 11 (2022), 3004â€“3017. https://doi.org/10.14778/3551793.3551848\n[63] Chin-Hsien Wu, Tei-Wei Kuo, and Li-Ping Chang. 2007. An efficient B-tree layer\nimplementation for flash-memory storage systems. ACM Trans. Embed. Comput.\nSyst. 6, 3 (2007), 19. https://doi.org/10.1145/1275986.1275991\n[64] Jiacheng Wu, Yong Zhang, Shimin Chen, Yu Chen, Jin Wang, and Chunxiao Xing.\n2021. Updatable Learned Index with Precise Positions. Proc. VLDB Endow. 14, 8\n(2021), 1276â€“1288. https://doi.org/10.14778/3457390.3457393\n[65] Shangyu Wu, Yufei Cui, Jinghuan Yu, Xuan Sun, Tei-Wei Kuo, and Chun Jason\nXue. 2022. NFL: Robust Learned Index via Distribution Transformation. Proc.\nVLDB Endow. 15, 10 (2022), 2188â€“2200. https://doi.org/10.14778/3547305.3547322\n[66] Zhaofeng Yan, Yuzhe Lin, Lu Peng, and Weihua Zhang. 2019. Harmonia: a high\nthroughput B+tree for GPUs. In Proceedings of the 24th ACM SIGPLAN Symposium\non Principles and Practice of Parallel Programming, PPoPP 2019, Washington, DC,\nUSA, February 16-20, 2019 , Jeffrey K. Hollingsworth and Idit Keidar (Eds.). ACM,\n133â€“144. https://doi.org/10.1145/3293883.3295704\n[67] Huanchen Zhang, David G. Andersen, Andrew Pavlo, Michael Kaminsky, Lin\nMa, and Rui Shen. 2016. Reducing the Storage Overhead of Main-Memory\nOLTP Databases with Hybrid Indexes. In Proceedings of the 2016 International\nConference on Management of Data, SIGMOD Conference 2016, San Francisco, CA,\nUSA, June 26 - July 01, 2016 , Fatma Ã–zcan, Georgia Koutrika, and Sam Madden\n(Eds.). ACM, 1567â€“1581. https://doi.org/10.1145/2882903.2915222\n[68] Hao Zhang, Gang Chen, Beng Chin Ooi, Kian-Lee Tan, and Meihui Zhang. 2015.\nIn-Memory Big Data Management and Processing: A Survey. IEEE Trans. Knowl.\nData Eng. 27, 7 (2015), 1920â€“1948. https://doi.org/10.1109/TKDE.2015.2427795\n[69] Huanchen Zhang, Hyeontaek Lim, Viktor Leis, David G. Andersen, Michael\nKaminsky, Kimberly Keeton, and Andrew Pavlo. 2018. SuRF: Practical Range\nQuery Filtering with Fast Succinct Tries. In Proceedings of the 2018 International\nConference on Management of Data, SIGMOD Conference 2018, Houston, TX, USA,\nJune 10-15, 2018 , Gautam Das, Christopher M. Jermaine, and Philip A. Bernstein\n(Eds.). ACM, 323â€“336. https://doi.org/10.1145/3183713.3196931\n[70] Jiaoyi Zhang and Yihan Gao. 2022. CARMI: A Cache-Aware Learned Index with a\nCost-based Construction Algorithm. Proc. VLDB Endow. 15, 11 (2022), 2679â€“2691.\nhttps://doi.org/10.14778/3551793.3551823\n[71] Shunkang Zhang, Ji Qi, Xin Yao, and AndrÃ© Brinkmann. 2024. Hyper: A High-\nPerformance and Memory-Efficient Learned Index via Hybrid Construction. Proc.\nACM Manag. Data 2, 3 (2024), 145. https://doi.org/10.1145/3654948\n[72] Jingren Zhou and Kenneth A. Ross. 2002. Implementing database operations\nusing SIMD instructions. In Proceedings of the 2002 ACM SIGMOD International\nConference on Management of Data, Madison, Wisconsin, USA, June 3-6, 2002 ,\nMichael J. Franklin, Bongki Moon, and Anastassia Ailamaki (Eds.). ACM, 145â€“156.\nhttps://doi.org/10.1145/564691.564709\n[73] Jingren Zhou and Kenneth A. Ross. 2003. Buffering Accesses to Memory-Resident\nIndex Structures. In Proceedings of 29th International Conference on Very Large\nData Bases, VLDB 2003, Berlin, Germany, September 9-12, 2003 , Johann Christoph\nFreytag, Peter C. Lockemann, Serge Abiteboul, Michael J. Carey, Patricia G.\nSelinger, and Andreas Heuer (Eds.). Morgan Kaufmann, 405â€“416. https://doi.\norg/10.1016/B978-012722442-8/50043-4",
  "textLength": 85211
}