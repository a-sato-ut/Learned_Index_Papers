{
  "paperId": "22481f4a3ce9ae3bfe59f2ee07ca61c875bd18ff",
  "title": "The Case for Learned Spatial Indexes",
  "pdfPath": "22481f4a3ce9ae3bfe59f2ee07ca61c875bd18ff.pdf",
  "text": "The Case for Learned Spatial Indexes\nVarun Pandey\nTUM\npandey@in.tum.deAlexander van Renen\nTUM\nrenen@in.tum.deAndreas Kipf\nMIT CSAIL\nkipf@mit.edu\nIbrahim Sabek\nMIT CSAIL\nsabek@mit.eduJialin Ding\nMIT CSAIL\njialind@mit.eduAlfons Kemper\nTUM\nkemper@in.tum.de\nABSTRACT\nSpatial data is ubiquitous. Massive amounts of data are\ngenerated every day from billions of GPS-enabled devices\nsuch as cell phones, cars, sensors, and various consumer-\nbased applications such as Uber, Tinder, location-tagged\nposts in Facebook, Twitter, Instagram, etc. This exponen-\ntial growth in spatial data has led the research community\nto focus on building systems and applications that can pro-\ncess spatial data e\u000eciently. In the meantime, recent re-\nsearch has introduced learned index structures. In this work,\nwe use techniques proposed from a state-of-the art learned\nmulti-dimensional index structure (namely, Flood) and ap-\nply them to \fve classical multi-dimensional indexes to be\nable to answer spatial range queries. By tuning each par-\ntitioning technique for optimal performance, we show that\n(i) machine learned search within a partition is faster by\n11.79% to 39.51% than binary search when using \fltering\non one dimension, (ii) the bottleneck for tree structures is\nindex lookup, which could potentially be improved by lin-\nearizing the indexed partitions (iii) \fltering on one dimen-\nsion and re\fning using machine learned indexes is 1.23x to\n1.83x times faster than closest competitor which \flters on\ntwo dimensions, and (iv) learned indexes can have a signif-\nicant impact on the performance of low selectivity queries\nwhile being less e\u000bective under higher selectivities.\n1. INTRODUCTION\nWith the increase in the amount of spatial data available\ntoday, the database community has devoted substantial at-\ntention to spatial data management. For e.g., NYC Taxi\nRides open dataset [30] consists of pickup and drop-o\u000b lo-\ncations of more than 2.7 billion rides taken in the city since\n2009. This represents more than 650,000 taxi rides every day\nin one of the most densely populated cities in the world, but\nis only a sample of the location data that is captured by\nmany applications today. Uber, a popular ride hailing ser-\nvice available via a mobile application, operates on a global\nThis article is published under a Creative Commons Attribution License\n(http://creativecommons.org/licenses/by/3.0/), which permits distribution\nand reproduction in any medium as well allowing derivative works, pro-\nvided that you attribute the original work to the author(s) and AIDB 2020.\n2nd International Workshop on Applied AI for Database Systems and Ap-\nplications (AIDB’20), August 31, 2020, Tokyo, Japan.\nML BS0.00.20.40.60.81.01.21.4\nGain: 25%Gain: 25%\nindex and\nreﬁnement\ndominatesindex and\nreﬁnement\ndominates\nLow SelectivityLow SelectivityML BS050100150200\nscan\ndominatesscan\ndominates\nHigh SelectivityHigh SelectivityQuery time [ µs] Query time [ µs]Index Time Reﬁnement ScanFigure 1: Machine Learning vs. Binary Search. For\nlow selectivity (0.00001%), the index and re\fnement\nphases dominate, while for high selectivity (0.1%),\nthe scan phase dominates (parameters are tuned to\nfavor Binary Search).\nscale and completed 10 billion rides in 2018 [46]. The un-\nprecedented rate of generation of location data has led to a\nconsiderable amount of research e\u000borts that have been fo-\ncused on, systems that scale out [1, 2, 8, 9, 14, 39, 40, 41,\n48, 50, 51], databases [12, 26, 27, 31, 33], improving spatial\nquery processing [11, 18, 19, 20, 35, 42, 43, 44, 53, 34], or\nleveraging modern hardware and compiling techniques [6, 7,\n37, 36, 38, 52], to handle the increasing demands of appli-\ncations today.\nRecently, Kraska et al. [23] proposed the idea of replacing\ntraditional database indexes with learned models that pre-\ndict the location of a key in a sorted dataset, and showed\nthat learned models are generally faster than binary search.\nKester et al. [16] showed that index scans are preferable\nover optimized sequential scans in main-memory analytical\nengines if a query selects a narrow portion of the data.\nIn this paper, we build on top of these recent research re-\nsults, and provide a thorough study for the e\u000bect of apply-\ning ideas from learned index structures (e.g., Flood [28]) to\nclassical multi-dimensional indexes. In particular, we focus\non \fve core spatial partitioning techniques, namely Fixed-\ngrid [4], Adaptive-grid [29], Kd-tree [3], Quadtree [10] and\nSTR [24]. Typically, query processing on top of these par-\ntitioning techniques include three phases; index lookup, re-\n\fnement, and scanning (Details of these phases are in Sec-\ntion 2.1). We propose to replace the typical search tech-\nniques used in the re\fnement phase (e.g., binary search)\n1arXiv:2008.10349v1  [cs.DB]  24 Aug 2020\n\n(a) Fixed grid\n (b) Adaptive grid\n (c) k-d tree\n (d) Quadtree\n (e) STRtree\nFigure 2: An illustration of the di\u000berent partitioning techniques\nwith learned models (e.g., RadixSpline [21]).\nInterestingly, we found that, by using a learned model as\nthe search technique, we can gain a considerable speedup\nin the query run-time, especially for low selectivity range\nqueries (Similar to the observation from Kester et al. [16]).\nFigure 1 shows the average running time of a range query\nusing Adaptive-grid on a Tweets dataset, which consists of\n83 million records (Section 3.1), with and without learning.\nIt can be seen that for a low selectivity query (which se-\nlects 0.00001% of the data, i.e., 8 records) the index and\nre\fnement times dominate the lookup, while for a high se-\nlectivity query (which selects 0.1% of the data, i.e., 83 thou-\nsand records) the scan time dominates. Another interest-\ning \fnding from our study is that 1-dimensional grid par-\ntitioning techniques (e.g., Fixed-grid) can bene\ft from the\nlearned models more than 2-dimensional techniques (e.g.,\nQuadtree). Our study will assist researchers and practition-\ners in understanding the performance of di\u000berent spatial in-\ndexing techniques when combined with learned models.\n2. APPROACH\nIn this section, we \frst explain how a range query pro-\ncessing has been implemented. Then, we describe the spa-\ntial partitioning techniques that we have implemented in\nour work. We conclude the section by describing the search\ntechniques used within the individual partitions.\n2.1 Range Query Processing\nA given range query has a lower bound and an upper\nbound in both dimensions. The task is to materialize all\nthe points that lie within the bounds of the query. Query\nprocessing works in three phases:\n\u000fIndex Lookup : In index lookup, we intersect a given\nrange query using the grid directories (or trees) to \fnd\nthe partitions the query intersects with.\n\u000fRe\fnement : Once the partitions intersected have\nbeen determined from the index lookup phase, we use a\nsearch technique (Section 2.3) to \fnd the lower bound\nof the query on the sorted dimension within the par-\ntition. There can be various cases on how a query\nintersects with the partition, and we only consult the\nsearch technique when it is actually needed to \fnd the\nlower bound of the given query on the sorted dimen-\nsion.. For example, a partition could be fully inside\nthe range query, and in such a case we simply copy\nall the points in the partition rather than use a search\ntechnique.\n\u000fScan : Once the lower bound in the sorted dimension\nhas been determined in re\fnement, we scan the parti-\ntion to \fnd the qualifying points on both dimensions.We stop as soon as we reach the upper bound of the\nquery on the sorted dimension, or we reach the end of\nthe partition.\n2.2 Partitioning Techniques\nSpatially partitioning a dataset into partitions (or cells),\nsuch that the objects within the partitions are also close in\nspace, is known as spatial partitioning. Spatial partition-\ning techniques can be classi\fed into space partitioning tech-\nniques (partitions the embedded space) or data partitioning\ntechniques (partitions the data space). In this paper, we em-\nploy Fixed-grid [4], Adaptive-grid [29], and Quadtree [10] as\nspace partitioning techniques; and Sort-Tile-Recursive [24]\nand K-d tree [3] as data partitioning techniques. Figure 2 il-\nlustrates these techniques on a sample of the Tweets dataset\nused in our experiments (details are in Section 3.1), where\nsample points and partition boundaries are shown as dots\nand grid axes respectively.\n2.2.1 Fixed and Adaptive Grid\nThe grid (or cell) methods were primarily designed to op-\ntimize retrieval of records from disk and generally they share\na similar structure. The grid family imposes a d-dimensional\ngrid on the d-attribute space. Every cell in the grid corre-\nsponds to one data page (or bucket) and the data points that\nfall within a particular cell boundary resides in the data page\nof that cell. Every cell thus has to store a pointer to the data\npage it indexes. This mapping of grid cells to data pages is\nknown as the grid directory. The Fixed-grid [4] method re-\nquires that the grid subdivision lines to be equidistant. The\nGrid File [29], or the Adaptive-grid, on the other hand re-\nlaxes this restriction. Since the grid subdivision lines are\nnot equidistant in the case of Grid File, it introduces an\nauxiliary data structure called linear scales, which are a set\nof d-dimensional arrays and de\fne the partition boundaries\nof the d-dimensions. Flood [28] is a state-oftheart learned\nmulti-dimensional index for d-dimensional data, which par-\ntitions the data using a grid over d-1 dimensions and uses the\nlast dimension as the sort dimension. In our implementa-\ntion, the grid partitioning techniques use a similar approach\nwhere the space is divided in one dimension and the other\ndimension is used as the sort dimension.\n2.2.2 Quadtree\nQuadtree [10] along with its many variants is a tree data\nstructure that also partitions the space like the k-d tree. The\nterm quadtree is generally referred to the two-dimensional\nvariant, but the basic idea can easily be generalized to d\ndimensions. Like the k-d tree, the quadtree decomposes the\nspace using rectilinear hyperplanes. The important distinc-\ntion is that quadtree is not a binary tree, and the interior\nnodes in the tree have 2dchildren for d-dimensions. For d =\n2, each interior node has four children, each corresponding to\n2\n\na rectangle. The search space is recursively decomposed into\nfour quadrants until the number of objects in each quadrant\nis less than a prede\fned threshold (usually the page size).\nQuadtrees are generally not balanced as the tree goes deeper\nfor the areas with higher densities.\n2.2.3 K-d tree\nK-d tree [3] is a binary search tree that recursively subdi-\nvides the space into equal subspaces by means of rectilinear\n(or iso-oriented) hyperplanes. The subdivision alternates\nbetween the k dimensions to be indexed. The splitting hy-\nperplanes at every level are known as the discriminators.\nFor k = 2, for example, the splitting hyperplanes are al-\nternately perpendicular to the x-axis and the y-axis, and\nare called the x-discriminator and the y-discriminator re-\nspectively. The original K-d tree partitioned the space into\nequal partitions, for example if the input space consists of\nGPS co-ordinate system (-90.0, -180 to 90, 180) the space\nwould be divided into equal halves (-45, -90 to 45, 90). K-\nd trees are thus not balanced if the data is skewed (most\nof which might only lie in one partition). K-d tree can be\nmade data-aware by selecting a median point from the data\nand dividing the data into two halves. This ensures that\nboth partitions in the binary tree are balanced. We have\nimplemented the data-aware k-d tree in our work.\n2.2.4 Sort-Tile-Recursive (STR) packed R-tree\nSort-Tile-Recursive [24] is a packing algorithm to \fll R-\ntree [13] and aims to maximize space utilization. The main\nidea behind STR packing is to tile the data space into S\u0002\nSgrid. For example, consider the number of points in a\ndataset to be PandNbe the capacity of a node. The data\nspace can then be divided into S\u0002Sgrid where S=p\nP=N.\nThe points are \frst sorted on the x-dimension (in case of\nrectangles, the x-dimension of the centroid) and then di-\nvided into Svertical slices . Within each vertical slice, the\npoints are sorted on the y-dimension, and packed into nodes\nby grouping them into runs of length Nthus forming Shor-\nizontal slices. The process then continues recursively. Pack-\ning the R-tree in this way packs all the nodes completely,\nexcept the last node which may have fewer than Nelements.\n2.3 Search Within Partition\nThe learned index structures require the underlying data\nto be sorted. In multi-dimensions, there is no inherent sort\norder over all dimensions. Thus, after partitioning the data,\na sort ordering on some dimension is required for the learned\nindexes to work. To achieve that, within each partition we\nsort the data using one dimension. Since spatial data con-\nsists of two dimensions (in two-dimensional space), either of\nthe two dimensions can be selected as the sort dimension.\nOnce the data within the partition has been sorted, either\na learned index or binary search (hereby search technique)\ncan be used on the sorted dimension. In all our experiments,\nwe have sorted on the longitude value of the location.\nWe use a RadixSpline [21, 22] over the sorted dimension\nwhich consists of two components: 1) a set of spline points,\nand 2) a radix table to quickly determine the spline points\nto examine for a lookup key (in our case the dimension over\nwhich the data is sorted). At lookup time, \frst the radix\ntable is consulted to determine the range of spline points to\nexamine. In the next step, these spline points are searched\nover to determine the spline points surrounding the lookupkey. In the last step, linear interpolation is used to predict\nthe position of the lookup key in the sorted array. Unlike the\nRMI [23], the RadixSpline only requires one pass over the\ndata to build the index, while retaining competitive lookup\ntimes. The RadixSpline and the RMI, at the time of writing,\nonly work on integer values, and we adapted the open-source\nimplementation of RadixSpline to work with \roating-point\nvalues (spatial datasets generally contain \roating point val-\nues). In our implementation, we have set the spline error to\n32 in all experiments.\nIt is important to make a distinction between how we use\nRadixSpline and binary search for re\fnement. In case of bi-\nnary search, we do a lookup for the lower bound of the query\non the sorted dimension. As learned indexes come with an\nerror, usually a local search is done to \fnd the lookup point\n(in our case the query lower bound). For range scans, as we\ndo, there can be two cases. The \frst case is that the esti-\nmated value from the spline is lower than the actual lower\nbound on the sorted dimension. In this case, we scan up\nuntil we reach the lower bound on the sorted dimension. In\nthe second case, the estimated value is higher than the ac-\ntual lower bound, hence, we \frst scan down to the lower\nbound, materialize all the points in our way until we reach\nthis bound, and after that we scan up until the query upper\nbound (or the partition end). In case the estimated value is\nlower than the upper bound of the query (i.e. the estimated\nvalue is within both query bounds), the second case incurs\nzero cost for local search as we can scan in both directions\nuntil we reach the query bounds within the partition.\n3. EVALUATION\nAll experiments were run single threaded on an Ubuntu\n18.04 machine with an Intel Xeon E5-2660 v2 CPU (2.20 GHz,\n10 cores, 3.00 GHz turbo)1and 256 GB DDR3 RAM. We use\nthenumactl command to bind the thread and memory to\none node to avoid NUMA e\u000bects. CPU scaling was also dis-\nabled during benchmarking using the cpupower command.\n3.1 Datasets\nFor evaluation, we used three datasets, the New York\nCity Taxi Rides dataset [30] (NYC Taxi Rides), geo-tagged\ntweets in the New York City area (NYC Tweets), and Open\nStreets Maps (OSM). NYC Taxi Rides contains 305 million\ntaxi rides from the years 2014 and 2015. NYC Tweets data\nwas collected using Twitter's Developer API [45] and con-\ntains 83 million tweets. The OSM dataset has been taken\nfrom [32] and contains 200M records from the All Nodes\n(Points) dataset. Figure 3 shows the spatial distribution of\nthe three datasets. We further generated two types of query\nworkloads for each of the three datasets: skewed queries\n(which follows the distribution of the underlying data) and\nuniform queries. For each type of query workload, we gener-\nated six di\u000berent workloads ranging from 0.00001% to 1.0%\nselectivity. For example, in the case of Taxi Rides dataset\n(305M records), these queries would materialize 30 records\nto 3 million records. These query workloads consist of one\nmillion queries each. To generate skewed queries, we select\na record from the data, and expand its boundaries (using\n1CPU: https://ark.intel.com/content/www/us/en/ark/\nproducts/75272/intel-xeon-processor-e5-2660-v2-\n25m-cache-2-20-ghz.html\n3\n\n(a) Twitter\n (b) Taxi Trips\n (c) OSM\nFigure 3: Datasets: (a) Tweets are spread across\nNew York, (b) NYC Taxi trips are clustered in cen-\ntral New York, and (c) All Nodes dataset from OSM.\na random ratio in both dimensions) until the selectivity re-\nquirement of the query is met. For uniform queries, we\ngenerated points uniformly in the embedding space of the\ndataset and expand the boundaries similarly until the selec-\ntivity requirement of the query is met. The query selectivity\nand the type of query are mostly application dependent. For\nexample, consider the application Google Maps, and a user\nissues a query to \fnd the popular pizzeria near the user.\nThe expected output for this query should be a handful of\nrecords, i.e. a low selectivity query (a list of 20-30 restau-\nrants near the user). On the other hand a query on an an-\nalytical system, would materialize many more records (e.g.\n\fnd average cost of all taxi rides originating in Manhattan).\n3.2 Tuning Partitioning Techniques\nRecent work in learned multi-dimensional and spatial in-\ndexes have focused on learning from the data and the query\nworkload. The essential idea behind learning from both\ndata and query workload is that a particular usecase can\nbe instance-optimized. To study this e\u000bect, we conducted\nmultiple experiments on the three datasets by varying the\nsizes of the partitions, tuning them on two workloads with\ndi\u000berent selectivities (to cover a broad spectrum we tune\nthe indexes on queries with low and high selectivity) for\nboth skewed and uniform queries.\nFigure 4 shows the e\u000bect of tuning when the indexes are\ntuned for the lowest selectivity workload for the two query\ntypes. It can be seen in the \fgure that it is essential to tune\nthe grid partitioning techniques for a particular workload.\nFirstly, they are susceptible to the size of the partition. As\nthe size of the partition increases, we notice an improvement\nin the performance until a particular partition size is reached\nwhich corresponds to the optimal performance. After this\npoint, increasing the size of the partitions only degrades\nperformance. It can be seen that, usually, for grid (single-\ndimension) partitioning techniques the partition sizes are\nmuch larger compared to partitioning techniques which \fl-\nter on both dimensions (only Quadtree is shown in the \fgure\nbut the same holds for the other partitioning techniques we\nhave covered in this work, we do not show the other trees\nbecause the curve is similar for them). Due to the large par-\ntition sizes in grid partitioning techniques, we notice a large\nincrease in performance while using a learned index com-\npared to binary search. This is especially evident for skewed\nqueries (which follow the underlying data distribution). We\nencountered a speedup from 11.79% up to 39.51% compared\nto binary search. Even when we tuned a learned index to a\npartition size which corresponds to the optimal performance\nfor binary search, we found that in multiple cases learned in-dex frequently outperformed binary search. Learned indexes\ndo not help much for partitioning techniques which \flter on\nboth dimensions, instead the performance of Quadtree (and\nSTRtree) dropped in many cases, see Table 1. The reason\nis that the optimal partition sizes for these techniques is\nvery low (less than 1,000 points per partition for most con-\n\fgurations). The re\fnement cost for learned indexes is an\noverhead in such cases. K-d tree on the other hand, contains\nmore points per partition (from 1200 to 7400) for the opti-\nmal con\fguration for Taxi Trips and OSM datasets and thus\nlearned indexes perform faster by 2.43% to 9.17% than bi-\nnary search. For Twitter dataset, the optimal con\fguration\ncontains less than 1200 points per partition, and we observed\na similar drop in performance using learned indexes.\nFigure 5 shows the e\u000bect of number of cells and number of\npoints that are scanned in each partition on query runtime\nfor Fixed-grid on Taxi Trips dataset for lowest selectivity.\nAs the number of points per partitions increases (i.e. fewer\nnumber of partitions), the number of cells decreases. At the\nsame time, the number of points that need to be scanned\nfor the query increases. The point where these curves meet\nis the optimal con\fguration for the workload which corre-\nsponds to the lowest query runtime. For tree structures,\nthe e\u000bect is di\u000berent. Figure 6 shows that the structures\nthat \flter on both dimensions do most of the pruning in the\nindex lookup. The dominating cost in these structures is\nthe number of points scanned within the partition and the\nquery runtime is directly proportional to this number. To\nminimize the number of points scanned, they do most of the\npruning during index lookup which require more partitions\n(i.e. less number of points per partition), but then they pay\nmore for index lookup.\n3.3 Range Query\nFigure 7 shows the query runtime for all learned index\nstructures. It can be seen that Fixed-grid along with Adaptive-\ngrid performs (1D schemes) perform the best for all the\ncases except uniform queries on Taxi and OSM datasets.\nFor skewed queries, Fixed-grid is 1.23 \u0002to 1.83 \u0002faster than\nthe closest competitor, Quadtree (2D), across all datasets\nand selectivity. The slight di\u000berence in performance be-\ntween Fixed-grid and Adaptive-grid comes from the index\nlookup. For Adaptive-grid, we use binary search on the\nlinear scales to \fnd the \frst partition the query intersects\nwith. For Fixed-grid, the index lookup is almost negligible\nas only an o\u000bset computation is needed to \fnd \frst inter-\nsecting partition. It can also be seen in the \fgure that the\nQuadtree is signi\fcantly better for uniform queries in case of\nTaxi Rides dataset (1.37 \u0002) and OSM dataset (2.68 \u0002) than\nthe closest competitor Fixed-grid. There are two reasons\nfor this, \frstly the Quadtree intersects with fewer number\nof partitions than the other index structures, see Table 2.\nSecondly, for uniform queries, the Quadtree is more likely to\ntraverse the sparse and low-depth region of the index. This\nis in conformance with an earlier research [17], where the au-\nthors report similar \fndings while comparing the Quadtree\nto the R*-tree and the Pyramid-Technique.\n3.4 Indexing Costs\nFigure 8 shows that Fixed-grid and Adaptive-grid are\nfaster to build than the tree based indexes. Fixed-grid is\n2.11\u0002, 2.05\u0002, and 1.90 \u0002faster to build than closest com-\npetitor STRtree. Quadtree is the slowest to build because it\n4\n\n246810skewed queries\nTweets (83M)\n2.55.07.510.012.515.0\nTaxi Rides (305M)\n2.55.07.510.012.5\nOSM (200M)\n1021031041051060204060uniform queries\n102103104105106050100150200\n1021031041051060200400600\nAverage number of points per partition (log)Average query time [ µs]ml-ﬁxed-grid bs-ﬁxed-grid ml-adaptive-grid bs-adaptive-grid ml-quadtree bs-quadtreeFigure 4: Con\fguration Experiments - ML vs. BS for low selectivity (0.00001%).\nTaxi Trips (Skewed Queries) Taxi Trips (Uniform Queries)\nFixed Adaptive Quadtree Fixed Adaptive Quadtree\nSelectivity (%) ML BS ML BS ML BS ML BS ML BS ML BS\n0.00001 1.78 2.35 1.86 2.40 2.77 2.51 2.02 2.58 81.4 10.54 1.48 1.31\n0.0001 4.54 5.82 4.67 6.12 6.12 5.82 5.85 6.91 228.1 27.69 3.69 3.42\n0.001 14.97 18.83 15.32 19.49 20.84 19.47 22.87 24.34 708.8 87.49 13.59 12.98\n0.01 90.13 97.04 89.48 95.96 117.01 104.37 141.24 151.47 2634.4 309.62 98.85 112.77\n0.1 678.12 698.39 675.14 696.49 922.67 793.96 988.35 922.96 9609.9 1174.79 891.24 1101.95\n1.0 8333.94 8408.15 8301.56 8399.69 10678.04 9512.29 8843.71 8753.68 8574.84 8836.28 10647.97 12377.14\nTable 1: Total query runtime (in microseconds) for both RadixSpline (ML) and binary search (BS) for Taxi\nRides dataset on skewed and uniform query workloads (parameters are tuned for selectivity 0.00001%).\n103104105106\nAverage number of points per partition23456789Average query time [ µs]\nQuery runtime\nNumber of cells\nScanned points\n0123456\nNumber of cells\u0002105\n0500100015002000250030003500\nScanned Points\nFigure 5: E\u000bect of number of cells and number of\npoints scanned for Fixed-grid on Taxi Trip dataset\nfor skewed queries (0.00001% selectivity).\nTaxi Rides OSM\nPartitioning Skewed Uniform Skewed Uniform\nFixed 1.97 7.98 1.72 23.73\nAdaptive 1.74 31.57 1.51 24.80\nk-d tree 1.70 21.62 1.56 30.95\nQuadtree 1.79 2.12 1.37 7.96\nSTR 2.60 47.03 1.90 11.05\nTable 2: Average number of partitions intersected\nfor each partitioning scheme for selectivity 0.00001%\non Taxi Rides and OSM datasets.\n102103104105\nAverage number of points per partition345678Average query time [ µs]\n0:00:51:01:52:02:5\nNumber of cells\u0002106\n050010001500200025003000\nScanned points\n0:60:81:01:21:41:6Index Time\nQuery runtime\nNumber of cells\nScanned points\nIndex TimeFigure 6: E\u000bect of number of cells and number of\npoints scanned for Quadtree on Taxi Trip dataset\nfor skewed queries (0.00001% selectivity).\ngenerates a large number of cells for optimal con\fguration.\nNot all partitions in Quadtree contain an equal number of\npoints as it divides space rather than data, thus leading to an\nimbalanced number of points per partition. Fixed-grid and\nAdaptive grid do not generate large number of partitions,\nas the partitions are quite large for optimal con\fguration.\nThey are lower in size for similar reasons. The index size in\nFigure 8 also includes the size of data being indexed.\n4. RELATED WORK\nRecent work by Kraska et al. [23] proposed the idea of\nreplacing traditional database indexes with learned mod-\nels that predict the location of one-dimensional keys in a\n5\n\n100101102skewed queries\nTweets (83M)\n Taxi Rides (305M)\n OSM (200M)\n1e-05 0.0001 0.001 0.01100101102uniform queries\n1e-05 0.0001 0.001 0.01\n 1e-05 0.0001 0.001 0.01\nQuery Selectivity (in percent)Average query time [ µs]ml-\fxed-grid ml-adaptive-grid ml-kdtree ml-quadtree ml-strtreeFigure 7: Total query runtime with parameters tuned on selectivity 0.00001%.\nTweets\n(83M)Taxi\n(305M)OSM\n(200M)101102Index Build Time [s]\nTweets\n(83M)Taxi\n(305M)OSM\n(200M)01234567Index Size [GBs]ml-ﬁxed-grid ml-adaptive-grid ml-kdtree ml-quadtree ml-strtree\nFigure 8: Index build times and sizes for the three\ndatasets.\ndataset. Since then, there has been a corpus of work on\nextending the ideas of the learned index to spatial and multi-\ndimensional data. Flood [28] is an in-memory read-optimized\nmulti-dimensional index that organizes the physical layout\nofd-dimensional data by dividing each dimension into some\nnumber of partitions, which forms a grid over d-dimensional\nspace and adapts to the data and query workload. Learning\nhas also been applied to the challenge of reducing I/O cost\nfor disk-based multi-dimensional indexes. Qd-tree [49] uses\nreinforcement learning to construct a partitioning strategy\nthat minimizes the number of disk-based blocks accessed by\na query. LISA [25] is a disk-based learned spatial index that\nachieves low storage consumption and I/O cost while sup-\nporting range queries, nearest neighbor queries, and inser-\ntions and deletions. The ZM-index [47] combines the stan-\ndard Z-order space-\flling curve with the RMI from [23] by\nmapping multi-dimensional values into a single-dimensional\nspace, which is then learnable using models. The ML-index [5]\ncombines the ideas of iDistance [15] and the RMI to supportrange and KNN queries.\n5. CONCLUSIONS AND FUTURE WORK\nIn this work, we implemented techniques proposed in a\nstate-of-the-art multi-dimensional index, namely, Flood [28],\nwhich indexes points using a variant of the Grid-\fle and ap-\nplied them to \fve classical spatial indexes. We have shown\nthat replacing binary search with learned indexes within\neach partition can improve overall query runtime by 11.79%\nto 39.51%. As expected, the e\u000bect of using a more e\u000ecient\nsearch within a partition is more pronounced for queries\nwith low selectivity. With increasing selectivity, the e\u000bect\nof a fast search diminishes. Likewise, the e\u000bect of using\na learned index is larger for (1D) grid partitioning tech-\nniques (e.g., Fixed-grid) than for (2D) tree structures (e.g.,\nQuadtree). The reason is that the partitions (cells) are less\nrepresentative of the points they contain in the 1D case than\nin the 2D case. Hence, 1D partitioning requires more re\fne-\nment within each cell.\nIn contrary, \fnding the qualifying partitions is more ef-\n\fcient with 1D than with 2D partitioning, thus contribut-\ning to lower overall query runtime (1.23x to 1.83x times\nfaster). Currently, we are using textbook implementations\nfor Quadtree and K-d tree. Future work could study re-\nplacing these tree structures with learned counterparts. For\nexample, we could linearize Quadtree cells (e.g., using a\nHilbert or Z-order curve) and store the resulting cell identi-\n\fers in a learned index.\nSo far we have only studied the case where indexes and\ndata \ft into RAM. For on-disk use cases, performance will\nlikely be dominated by I/O and the search within partitions\nwill be of less importance. We expect partition sizes to be\nperformance-optimal when aligned with the physical page\nsize. To reduce I/O, it will be crucial for partitions to not\ncontain any unnecessary points. Hence, we expect 2D par-\ntitioning to be the method of choice in this case. We refer\nto LISA [25] for further discussions on this topic.\n6\n\n6. REFERENCES\n[1] A. Aji, F. Wang, H. Vo, R. Lee, Q. Liu, X. Zhang, and\nJ. H. Saltz. Hadoop-gis: A high performance spatial\ndata warehousing system over mapreduce. PVLDB ,\n6(11):1009{1020, 2013.\n[2] K. Amemiya and A. Nakao. Layer-integrated edge\ndistributed data store for real-time and stateful\nservices. In NOMS 2020 - IEEE/IFIP Network\nOperations and Management Symposium, Budapest,\nHungary, April 20-24, 2020 , pages 1{9. IEEE, 2020.\n[3] J. L. Bentley. Multidimensional binary search trees\nused for associative searching. Commun. ACM ,\n18(9):509{517, 1975.\n[4] J. L. Bentley and J. H. Friedman. Data structures for\nrange searching. ACM Comput. Surv. , 11(4):397{409,\n1979.\n[5] A. Davitkova, E. Milchevski, and S. Michel. The\nML-Index: A Multidimensional, Learned Index for\nPoint, Range, and Nearest-Neighbor Queries. In 2020\nConference on Extending Database Technology\n(EDBT) , 2020.\n[6] H. Doraiswamy and J. Freire. A gpu-friendly\ngeometric data model and algebra for spatial queries.\nInProceedings of the 2020 International Conference\non Management of Data, SIGMOD Conference 2020,\nonline conference [Portland, OR, USA], June 14-19,\n2020, pages 1875{1885. ACM, 2020.\n[7] H. Doraiswamy and J. Freire. A gpu-friendly\ngeometric data model and algebra for spatial queries:\nExtended version. CoRR , abs/2004.03630, 2020.\n[8] A. Eldawy and M. F. Mokbel. Spatialhadoop: A\nmapreduce framework for spatial data. In 31st IEEE\nInternational Conference on Data Engineering, ICDE\n2015, Seoul, South Korea, April 13-17, 2015 , pages\n1352{1363, 2015.\n[9] A. Eldawy, I. Sabek, M. Elganainy, A. Bakeer,\nA. Abdelmotaleb, and M. F. Mokbel. Sphinx:\nEmpowering impala for e\u000ecient execution of SQL\nqueries on big spatial data. In Advances in Spatial and\nTemporal Databases - 15th International Symposium,\nSSTD 2017, Arlington, VA, USA, August 21-23, 2017,\nProceedings , pages 65{83, 2017.\n[10] R. A. Finkel and J. L. Bentley. Quad trees: A data\nstructure for retrieval on composite keys. Acta Inf. ,\n4:1{9, 1974.\n[11] F. Garc\u0013 \u0010a-Garc\u0013 \u0010a, A. Corral, L. Iribarne, and\nM. Vassilakopoulos. Improving distance-join query\nprocessing with voronoi-diagram based partitioning in\nspatialhadoop. Future Gener. Comput. Syst. ,\n111:723{740, 2020.\n[12] D. Gomes. MemSQL Live: Nikita Shamgunov on the\nData Engineering Podcast , 2019. https:\n//www.memsql.com/blog/memsql-live-nikita-\nshamgunov-on-the-data-engineering-podcast/ .\n[13] A. Guttman. R-trees: A dynamic index structure for\nspatial searching. In SIGMOD'84, Proceedings of\nAnnual Meeting, Boston, Massachusetts, USA, June\n18-21, 1984 , pages 47{57. ACM Press, 1984.\n[14] S. Hagedorn, P. G otze, and K. Sattler. The STARK\nframework for spatio-temporal data analytics on\nspark. In Datenbanksysteme f ur Business, Technologie\nund Web (BTW 2017), 17. Fachtagung desGI-Fachbereichs ,,Datenbanken und\nInformationssysteme\" (DBIS), 6.-10. M arz 2017,\nStuttgart, Germany, Proceedings , pages 123{142, 2017.\n[15] H. V. Jagadish, B. C. Ooi, K.-L. Tan, C. Yu, and\nR. Zhang. Idistance: An adaptive b+-tree based\nindexing method for nearest neighbor search. ACM\nTrans. Database Syst. , 30(2):364397, June 2005.\n[16] M. S. Kester, M. Athanassoulis, and S. Idreos. Access\npath selection in main-memory optimized data\nsystems: Should I scan or should I probe? In\nProceedings of the 2017 ACM International\nConference on Management of Data, SIGMOD\nConference 2017, Chicago, IL, USA, May 14-19, 2017 ,\npages 715{730. ACM, 2017.\n[17] Y. J. Kim and J. M. Patel. Rethinking choices for\nmulti-dimensional point indexing: Making the case for\nthe often ignored quadtree. In CIDR 2007, Third\nBiennial Conference on Innovative Data Systems\nResearch, Asilomar, CA, USA, January 7-10, 2007,\nOnline Proceedings , pages 281{291. www.cidrdb.org,\n2007.\n[18] A. Kipf, H. Lang, V. Pandey, R. A. Persa, C. Anneser,\nE. T. Zacharatou, H. Doraiswamy, P. A. Boncz,\nT. Neumann, and A. Kemper. Adaptive main-memory\nindexing for high-performance point-polygon joins. In\nProceedings of the 23nd International Conference on\nExtending Database Technology, EDBT 2020,\nCopenhagen, Denmark, March 30 - April 02, 2020 ,\npages 347{358. OpenProceedings.org, 2020.\n[19] A. Kipf, H. Lang, V. Pandey, R. A. Persa, P. A.\nBoncz, T. Neumann, and A. Kemper. Adaptive\ngeospatial joins for modern hardware. CoRR ,\nabs/1802.09488, 2018.\n[20] A. Kipf, H. Lang, V. Pandey, R. A. Persa, P. A.\nBoncz, T. Neumann, and A. Kemper. Approximate\ngeospatial joins with precision guarantees. In 34th\nIEEE International Conference on Data Engineering,\nICDE 2018, Paris, France, April 16-19, 2018 , pages\n1360{1363, 2018.\n[21] A. Kipf, R. Marcus, A. van Renen, M. Stoian,\nA. Kemper, T. Kraska, and T. Neumann. Radixspline:\nA single-pass learned index. CoRR , abs/2004.14541,\n2020.\n[22] A. Kipf, R. Marcus, A. van Renen, M. Stoian,\nA. Kemper, T. Kraska, and T. Neumann. Radixspline:\na single-pass learned index. In Proceedings of the\nThird International Workshop on Exploiting Arti\fcial\nIntelligence Techniques for Data Management,\naiDM@SIGMOD 2020, Portland, Oregon, USA, June\n19, 2020 , pages 5:1{5:5. ACM, 2020.\n[23] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and\nN. Polyzotis. The case for learned index structures. In\nProceedings of the 2018 International Conference on\nManagement of Data, SIGMOD Conference 2018,\nHouston, TX, USA, June 10-15, 2018 , pages 489{504.\nACM, 2018.\n[24] S. T. Leutenegger, J. M. Edgington, and M. A. L\u0013 opez.\nSTR: A simple and e\u000ecient algorithm for r-tree\npacking. In Proceedings of the Thirteenth International\nConference on Data Engineering, April 7-11, 1997,\nBirmingham, UK , pages 497{506. IEEE Computer\nSociety, 1997.\n7\n\n[25] P. Li, H. Lu, Q. Zheng, L. Yang, and G. Pan. LISA: A\nLearned Index Structure for Spatial Data. In\nProceedings of the 2020 International Conference on\nManagement of Data , SIGMOD 20, New York, NY,\nUSA, 2020. Association for Computing Machinery.\n[26] A. Makris, K. Tserpes, G. Spiliopoulos, and\nD. Anagnostopoulos. Performance evaluation of\nmongodb and postgresql for spatio-temporal data. In\nProceedings of the Workshops of the EDBT/ICDT\n2019 Joint Conference, EDBT/ICDT 2019, Lisbon,\nPortugal, March 26, 2019 , volume 2322 of CEUR\nWorkshop Proceedings . CEUR-WS.org, 2019.\n[27] MongoDB Releases - New Geo Features in MongoDB\n2.4, 2013. https://www.mongodb.com/blog/post/\nnew-geo-features-in-mongodb-24/ .\n[28] V. Nathan, J. Ding, M. Alizadeh, and T. Kraska.\nLearning multi-dimensional indexes. In Proceedings of\nthe 2020 International Conference on Management of\nData , SIGMOD 20, New York, NY, USA, 2020.\nAssociation for Computing Machinery.\n[29] J. Nievergelt, H. Hinterberger, and K. C. Sevcik. The\ngrid \fle: An adaptable, symmetric multikey \fle\nstructure. ACM Trans. Database Syst. , 9(1):38{71,\n1984.\n[30] NYC Taxi and Limousine Commission (TLC) - TLC\nTrip Record Data , 2019. https://www1.nyc.gov/\nsite/tlc/about/tlc-trip-record-data.page .\n[31] Oracle Spatial and Graph Spatial Features , 2019.\nhttps://www.oracle.com/technetwork/database/\noptions/spatialandgraph/overview/\nspatialfeatures-1902020.html/ .\n[32] V. Pandey, A. Kipf, T. Neumann, and A. Kemper.\nHow good are modern spatial analytics systems?\nProc. VLDB Endow. , 11(11):1661{1673, 2018.\n[33] V. Pandey, A. Kipf, D. Vorona, T. M uhlbauer,\nT. Neumann, and A. Kemper. High-performance\ngeospatial analytics in hyperspace. In Proceedings of\nthe 2016 International Conference on Management of\nData, SIGMOD Conference 2016, San Francisco, CA,\nUSA, June 26 - July 01, 2016 , pages 2145{2148, 2016.\n[34] K. Richly. Optimized spatio-temporal data structures\nfor hybrid transactional and analytical workloads on\ncolumnar in-memory databases. In Proceedings of the\nVLDB 2019 PhD Workshop, co-located with the 45th\nInternational Conference on Very Large Databases\n(VLDB 2019), Los Angeles, California, USA, August\n26-30, 2019 , volume 2399 of CEUR Workshop\nProceedings . CEUR-WS.org, 2019.\n[35] D. Sidlauskas, S. Chester, E. T. Zacharatou, and\nA. Ailamaki. Improving spatial data processing by\nclipping minimum bounding boxes. In 34th IEEE\nInternational Conference on Data Engineering, ICDE\n2018, Paris, France, April 16-19, 2018 , pages\n425{436. IEEE Computer Society, 2018.\n[36] R. Y. Tahboub, G. M. Essertel, and T. Rompf. How\nto architect a query compiler, revisited. In Proceedings\nof the 2018 International Conference on Management\nof Data, SIGMOD Conference 2018, Houston, TX,\nUSA, June 10-15, 2018 , pages 307{322. ACM, 2018.\n[37] R. Y. Tahboub and T. Rompf. On supporting\ncompilation in spatial query engines: (vision paper).\nInProceedings of the 24th ACM SIGSPATIALInternational Conference on Advances in Geographic\nInformation Systems, GIS 2016, Burlingame,\nCalifornia, USA, October 31 - November 3, 2016 ,\npages 9:1{9:4. ACM, 2016.\n[38] R. Y. Tahboub and T. Rompf. Architecting a query\ncompiler for spatial workloads. In Proceedings of the\n2020 International Conference on Management of\nData, SIGMOD Conference 2020, online conference\n[Portland, OR, USA], June 14-19, 2020 , pages\n2103{2118. ACM, 2020.\n[39] M. Tang, Y. Yu, Q. M. Malluhi, M. Ouzzani, and\nW. G. Aref. Locationspark: A distributed in-memory\ndata management system for big spatial data.\nPVLDB , 9(13):1565{1568, 2016.\n[40] K. Theocharidis, J. Liagouris, N. Mamoulis, P. Bouros,\nand M. Terrovitis. SRX: e\u000ecient management of\nspatial RDF data. VLDB J. , 28(5):703{733, 2019.\n[41] T. Toliopoulos, N. Nikolaidis, A. Michailidou,\nA. Seitaridis, A. Gounaris, N. Bassiliades,\nA. Georgiadis, and F. Liotopoulos. Developing a\nreal-time tra\u000ec reporting and forecasting back-end\nsystem. In Research Challenges in Information\nScience - 14th International Conference, RCIS 2020,\nLimassol, Cyprus, September 23-25, 2020,\nProceedings , volume 385 of Lecture Notes in Business\nInformation Processing , pages 58{75. Springer, 2020.\n[42] D. Tsitsigkos, P. Bouros, N. Mamoulis, and\nM. Terrovitis. Parallel in-memory evaluation of spatial\njoins. CoRR , abs/1908.11740, 2019.\n[43] D. Tsitsigkos, P. Bouros, N. Mamoulis, and\nM. Terrovitis. Parallel in-memory evaluation of spatial\njoins. In Proceedings of the 27th ACM SIGSPATIAL\nInternational Conference on Advances in Geographic\nInformation Systems, SIGSPATIAL 2019, Chicago,\nIL, USA, November 5-8, 2019 , pages 516{519. ACM,\n2019.\n[44] D. Tsitsigkos, K. Lampropoulos, P. Bouros,\nN. Mamoulis, and M. Terrovitis. A two-level spatial\nin-memory index. CoRR , abs/2005.08600, 2020.\n[45] Tutorials: Filtering Tweets by location , 2020.\nhttps://developer.twitter.com/en/docs/\ntutorials/filtering-tweets-by-location .\n[46] Uber. Uber newsroom: 10 billion, 2018.\nhttps://www.uber.com/newsroom/10-billion/ .\n[47] H. Wang, X. Fu, J. Xu, and H. Lu. Learned index for\nspatial queries. In 2019 20th IEEE International\nConference on Mobile Data Management (MDM) ,\npages 569{574, 2019.\n[48] D. Xie, F. Li, B. Yao, G. Li, L. Zhou, and M. Guo.\nSimba: E\u000ecient in-memory spatial analytics. In\nProceedings of the 2016 International Conference on\nManagement of Data, SIGMOD Conference 2016, San\nFrancisco, CA, USA, June 26 - July 01, 2016 , pages\n1071{1085, 2016.\n[49] Z. Yang, B. Chandramouli, C. Wang, J. Gehrke, Y. Li,\nU. F. Minhas, P.-A. Larson, D. Kossmann, and\nR. Acharya. Qd-tree: Learning Data Layouts for Big\nData Analytics. In Proceedings of the 2020\nInternational Conference on Management of Data ,\nSIGMOD 20, New York, NY, USA, 2020. Association\nfor Computing Machinery.\n[50] S. You, J. Zhang, and L. Gruenwald. Large-scale\n8\n\nspatial join query processing in cloud. In 31st IEEE\nInternational Conference on Data Engineering\nWorkshops, ICDE Workshops 2015, Seoul, South\nKorea, April 13-17, 2015 , pages 34{41, 2015.\n[51] J. Yu, J. Wu, and M. Sarwat. Geospark: a cluster\ncomputing framework for processing large-scale spatial\ndata. In Proceedings of the 23rd SIGSPATIAL\nInternational Conference on Advances in Geographic\nInformation Systems, Bellevue, WA, USA, November\n3-6, 2015 , pages 70:1{70:4, 2015.\n[52] E. T. Zacharatou, H. Doraiswamy, A. Ailamaki, C. T.Silva, and J. Freire. GPU rasterization for real-time\nspatial aggregation over arbitrary polygons. PVLDB ,\n11(3):352{365, 2017.\n[53] E. T. Zacharatou, D. Sidlauskas, F. Tauheed,\nT. Heinis, and A. Ailamaki. E\u000ecient bundled spatial\nrange queries. In Proceedings of the 27th ACM\nSIGSPATIAL International Conference on Advances\nin Geographic Information Systems, SIGSPATIAL\n2019, Chicago, IL, USA, November 5-8, 2019 , pages\n139{148. ACM, 2019.\n9",
  "textLength": 42524
}