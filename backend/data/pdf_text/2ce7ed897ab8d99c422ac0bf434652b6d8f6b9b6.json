{
  "paperId": "2ce7ed897ab8d99c422ac0bf434652b6d8f6b9b6",
  "title": "Towards Parallel Learned Sorting",
  "pdfPath": "2ce7ed897ab8d99c422ac0bf434652b6d8f6b9b6.pdf",
  "text": "Towards Parallel Learned Sorting\nby\nIvan Carvalho\nA THESIS SUBMITTED IN PARTIAL FULFILLMENT OF\nTHE REQUIREMENTS FOR THE DEGREE OF\nB.SC. COMPUTER SCIENCE HONOURS\nin\nIrving K. Barber Faculty of Science\n(Computer Science)\nTHE UNIVERSITY OF BRITISH COLUMBIA\n(Okanagan)\nApril 2022\n©Ivan Carvalho, 2022arXiv:2208.06902v1  [cs.DS]  14 Aug 2022\n\nAbstract\nWe introduce a new sorting algorithm that is the combination of ML-\nenhanced sorting with the In-place Super Scalar Sample Sort (IPS4o). The\nmain contribution of our work is to achieve parallel ML-enhanced sorting,\nas previous algorithms were limited to sequential implementations. We in-\ntroduce the In-Place Parallel Learned Sort (IPLS) algorithm and compare\nit extensively against other sorting approaches. IPLS combines the IPS4o\nframework with linear models trained using the Fastest Minimum Con\rict\nDegree algorithm to partition data. The experimental results do not crown\nIPLS as the fastest algorithm. However, they do show that IPLS is com-\npetitive among its peers and that using the IPS4o framework is a promising\napproach towards parallel learned sorting.\nii\n\nTable of Contents\nAbstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ii\nTable of Contents . . . . . . . . . . . . . . . . . . . . . . . . . . . iii\nList of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iv\nAcknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . vi\nChapter 1: Introduction . . . . . . . . . . . . . . . . . . . . . . . 1\nChapter 2: Background . . . . . . . . . . . . . . . . . . . . . . . 2\n2.1 In-Place Parallel Super-Scalar Sample Sort . . . . . . . . . . 3\n2.2 Learned Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2.3 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\nChapter 3: Machine Learning Models for Sorting . . . . . . . 7\n3.1 Recursive Model Index . . . . . . . . . . . . . . . . . . . . . . 9\n3.2 Decision Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3.3 Linear Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\nChapter 4: Proposed Algorithm . . . . . . . . . . . . . . . . . . 16\n4.1 Worked Example of Proposed Algorithm . . . . . . . . . . . . 16\nChapter 5: Experimental Evaluation . . . . . . . . . . . . . . . 18\n5.1 Sequential Results . . . . . . . . . . . . . . . . . . . . . . . . 19\n5.2 Parallel Results . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n5.3 Scalability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nChapter 6: Conclusion and Future Work . . . . . . . . . . . . . 26\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\niii\n\nList of Figures\nFigure 2.1 Ideal case of ML-enhanced with a perfect model. . . . 4\nFigure 3.1 Generic RMI with multiple levels. . . . . . . . . . . . 9\nFigure 3.2 RMI used in practice. The number of levels is \fxed\nto two and the optimal model types are chosen via\ntraining with heuristics. . . . . . . . . . . . . . . . . . 10\nFigure 5.1 Comparison of the sorting rate of the algorithms for\nreal-world datasets under a sequential setting. IS2Ra\ntakes the lead in this scenario. Higher rates are better. 20\nFigure 5.2 Comparison of the sorting rate of the algorithms for\nsynthetic datasets under a sequential setting. Learned\nSort takes the lead in this scenario. Higher rates are\nbetter. . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nFigure 5.3 Comparison of the sorting rate of the algorithms for\nsynthetic with an increased number of duplicates un-\nder a sequential setting. Learned Sort takes the lead\nin this scenario, but struggles with the Root Dups\ndataset. Higher rates are better. . . . . . . . . . . . . 21\nFigure 5.4 Comparison of the sorting rate of the algorithms for\nreal-world datasets under a parallel setting. IPS4o\ntakes the lead in this scenario. Higher rates are better. 22\nFigure 5.5 Comparison of the sorting rate of the algorithms for\nsynthetic datasets under a parallel setting. IPS4o\ntakes the lead in this scenario. Higher rates are better. 22\nFigure 5.6 Comparison of the sorting rate of the algorithms for\nsynthetic datasets with an increased number of du-\nplicates under a parallel setting. IPS4o takes the lead\nin this scenario. Higher rates are better. . . . . . . . 23\niv\n\nLIST OF FIGURES\nFigure 5.7 Comparison of the sorting rates for the Normal dataset\nas the input size grows under a sequential setting.\nHigher rates are better. . . . . . . . . . . . . . . . . . 24\nFigure 5.8 Comparison of the sorting rates for the Normal dataset\nas the input size grows under a parallel setting. Higher\nrates are better. . . . . . . . . . . . . . . . . . . . . . 25\nv\n\nAcknowledgements\nI would like to thank my supervisor, Dr. Ramon Lawrence, for all the\nsupport while completing this work. Thanks for motivating me to do re-\nsearch in the \feld of databases and Learned Indexes.\nI would also like to thank my friends Matthew Currie, Sean Roarty, and\nKathryn Lecha. Thanks for listening to incomplete parts of this thesis, often\nwhen the details were unclear and things went wrong.\nvi\n\nChapter 1\nIntroduction\nSorting algorithms are among the most fundamental algorithms in Com-\nputer Science and are critical in database applications. The performance of\nsorting can impact common operations such as index building, data aggrega-\ntion, and in-memory joins. Asymptotically optimal comparison-based sort-\ning algorithms such as Quicksort have been known since the 1960s [Hoa62].\nDespite that, researchers keep pushing the boundaries of sorting performance\nby proposing algorithms that outperform Quicksort.\nTwo boundaries are relevant for this work. The \frst boundary is that of\nCPU-based parallel sorting, with In-Place Parallel Super-Scalar Sample Sort\n(IPS4o) as the state-of-the-art in the \feld [AWFS22]. The second boundary\nis that of Machine Learning (ML) enhanced sorting, which proposes training\nsimple ML models to predict the position of the data to sort items [KVc+20].\nOur contribution is the combination of ML-enhanced sorting with IPS4o\nand extensive experimental evaluation.\nIn this thesis, we introduce the In-Place Parallel Learned Sort (IPLS)\nalgorithm. IPLS builds upon the algorithmic framework from IPS4o, replac-\ning the decision tree with the linear model introduced in [WZC+21]. We are\nthe \frst work to interpret IPS4o as a ML-enhanced algorithm, which has two\nconsequences. The \frst consequence is that models from the \feld of Learned\nIndexes [KBC+18] can be used to create variants of IPS4o. The second con-\nsequence is that IPS4o provides a framework to e\u000eciently parallelize ML-\nenhanced sorting algorithms, addressing the shortcomings of earlier works\nthat only implemented serial routines.\nWe evaluate the performance of IPLS against other sorting algorithms\nin a benchmark adapted from the Learned Sort 2.0 paper [KVK21]. The\nextensive results do not crown IPLS as the fastest algorithm. However, they\ndo show that IPLS is competitive among its peers and that using the IPS4o\nframework is a promising approach towards parallel learned sorting.\n1\n\nChapter 2\nBackground\nThere is a multitude of sorting algorithms that leverage creative tech-\nniques to enhance performance. Despite the sheer number of algorithms,\nthey can be grouped into categories due to their similarities. We focus on\ntwo categorizations that are relevant to the two algorithms we will review:\ncomparison-based vs distribution-based; and in-place vs non-in-place.\nComparison-based algorithms such as Quicksort rely on comparing pairs\nof elements to sort the data. This class of algorithms has a lower bound\nof \n(nlogn) comparisons to sort the data. Distribution-based algorithms\non the other hand are not subject to that lower bound. Radix Sort has a\ncomplexity ofO(wn), wherewis the width of the key being sorted. How-\never, we note that the performance of sorting algorithms goes beyond just\nasymptotic complexity. The algorithms we will review exploit the constants\nof the implementations with techniques such as instruction parallelism, loop\nunrolling, SIMD instructions and other clever tricks to be faster. There is\nno clear winner between the two classes. We will verify that Radix Sort does\nperform better than comparison-based sorts in sequential settings. However,\nif a parallel setting is taken into account, we verify the opposite. The fastest\nalgorithm for the sequential case is distribution-based, while the fastest al-\ngorithm for the parallel case is comparison-based.\nIn-place algorithms use a constant amount of memory to sort the array,\nwhile non-in-place algorithms use an amount that depends on the size nof\nthe input. This a\u000bects the performance, especially for large inputs. As n\ngrows, allocating memory becomes more expensive hence using a constant\namount of memory becomes advantageous. Moreover, in-place algorithms\nhave di\u000berent memory access patterns than non-in-place algorithms. If im-\nplemented carefully, in-place algorithms can leverage cache-friendly memory\naccess patterns that boost performance. We note that the two algorithms\nwe review are in-place sorting algorithms.\n2\n\n2.1. In-Place Parallel Super-Scalar Sample Sort\n2.1 In-Place Parallel Super-Scalar Sample Sort\nIPS4o [AWFS22] is an enhanced in-place and parallel version of the Super\nScalar Sample Sort algorithm [SW04]. Sample Sort itself is a generalization\nof Quicksort to multiple pivots. It uses kpivots instead of just one to\nperform the partitioning and runs in O(nlogn).\nPartitioning works in three steps. The \frst step is to sample \u000bk\u00001\nrandom elements from the array, where \u000bis an oversampling factor that can\nbe tweaked. The second step is to sort the sampled elements and pick k\nequidistant keys. The third step is to predict a bucket Bifor each key xi\nand movexito the allocated region of Biin the array. After every element\nhas been moved into its bucket, each bucket is an independent sorting sub-\nproblem that is handled recursively.\nThe di\u000berence between IPS4o and previous variants of Sample Sort is\nthat the partitioning is done in-place. The previous versions allocated O(n)\nmemory for an auxiliary array that stores a copy of the elements. IPS4o\ninstead uses bu\u000bers of size bfor each bucket. It allocates O(kb) total memory,\nand when a bu\u000ber is full it \rushes back to the array.\nOn a \frst pass, IPS4o predicts a bucket for each key and creates O(n=b)\nblocks of size b. Afterwards, IPS4o permutes the blocks such that each region\nBiis contiguous in the array using a routine similar to defragmentation.\nThe bene\fts of the in-place partitioning routine are twofold. The im-\nmediate bene\ft is that the required memory to execute the algorithm is\nlower. The second bene\ft is the cache-friendly pattern of the memory ac-\ncesses.bcan be chosen carefully such that the bu\u000bers \ft into the cache,\nwhich reduces cache misses. In contrast, the previous versions of Sample\nSort accessed scattered memory positions which is slower.\nIn addition to being in-place, IPS4o is also parallelizable. The authors\nof IPS4o implemented an e\u000ecient task scheduler to coordinate the parallel\ntasks with the goal of always trying to use most of the hardware available.\nBoth the prediction step and the block permutation step in the partition-\ning phase can be executed in parallel using tasks. Once the recursive sub-\nproblem sizes become small, sorting the sub-problem sequentially also be-\ncomes a task. The end result of parallelizing all steps of the algorithm is an\nalgorithm that outperforms Quicksort by a large margin in multi-threaded\nsettings.\nAnother relevant contribution from the authors of IPS4o is that they\nreuse the code from the in-place partitioning and task scheduler to imple-\nment Radix Sort as well. The In-place Parallel Super Scalar Radix Sort\n(IPS2Ra) builds on top of the IPS4o and uses the most-signi\fcant digits to\n3\n\n2.2. Learned Sort\npartition the data instead of using sampled pivots. The benchmarks for the\nauthors indicate that IPS2Ra consistently beats IPS4o in sequential settings\nand is among the fastest Radix Sort implementations available.\n2.2 Learned Sort\nLearned Sort is a distribution-based sorting algorithm that introduces\nthe paradigm of ML-enhanced sorting [KVc+20]. The main idea is that if\nthere exists a model Fthat predicts the sorted position of a key x, we could\nsort the array in a single pass by moving each element to its correct location\nwithA[F(x)] =x.\n18 39 47 25 27 12 5 1 9 3\nF(x)\n39\nFigure 2.1: Ideal case of ML-enhanced with a perfect model.\nThe \frst challenge with this idea is that Fis unknown at the start.\nInstead of using the exact model, we sample some data from Aand train\na model on those samples. The second challenge with the idea is picking\nthe appropriate model that can predict the sorted position of an element.\nOne possible solution is to use Empirical Cumulative Distribute Functions\n(eCDF). The eCDF yields the probability P(A\u0014x) that an element is\nsmaller than x, hence for an array with Nelements we predict that the\nposition will be pos=F(x) =bN\u0001P(A\u0014x)c.\nLearned Sort relies on the Recursive Model Index (RMI) architecture\nintroduced in [KBC+18] to calculate the eCDF. RMIs were originally pro-\nposed for static index lookups in in-memory databases. They were originally\ntrained on the sorted array containing all data to be indexed, and performed\nlook-ups for keys xby checking if xwas in the range [ F(x)\u0000\";F(x) +\"] of\nthe array where \"is the error of the RMI. Learned Sort does not have all\nthe data sorted in advance, as the input by de\fnition needs to be sorted.\n4\n\n2.2. Learned Sort\nThe algorithm overcomes this issue by sorting the sampled data using other\nalgorithms and training the RMI on the sorted sample. There is a trade-o\u000b\nbetween sample size, the accuracy of the eCDF, and the training time. If\na sample size is too small, it trains faster but yields worse estimates for\nthe eCDF. If the training size is too big, the eCDF estimate is better but\nsorting the samples dominates the run-time. The authors of Learned Sort\nfound that sampling 1% worked well in practice.\nAdditional details need to be handled to make ML-enhanced sorting work\nin practice. The eCDF is an estimate that is not guaranteed to be monotonic,\ntherefore there might exist elements aandbsuch thata < b butF(a)>\nF(b), and the implementation needs to correct those errors. Moreover, there\ncan be collisions with elements having F(a) =F(b), hence the algorithm also\nneeds to account for those. The collision problem is exacerbated when there\nare many duplicates because in that case it is guaranteed that all duplicates\nwill collide at F(x).\nLearned Sort 2.0 [KVK21] accomplishes the task of applying ML-enhanced\nsorting in practice. The algorithm consists of three routines: Partitioning,\nModel-Based Counting Sort, and Correction with Insertion Sort. The par-\ntitioning routine is in-place and uses a strategy similar to the partitioning\nintroduced in IPS4o. The main di\u000berences are that the eCDF is used to\nassign keys to buckets, that the number of buckets kis higher compared to\nIPS4o, and that partitioning is executed only twice instead of recursively.\nTo handle duplicates, Learned Sort 2.0 performs a homogeneity check after\npartitioning: if all elements within a bucket are equal, the bucket is left as is\nbecause a sequence of identical elements is already sorted. The base case is a\nModel-Based Counting Sort that uses the eCDF to predict the \fnal position\nof the keys in the buckets. Lastly, Insertion Sort is executed to correct the\npossible mistakes from the eCDF and guarantee that the output is sorted.\nBecause the sequence is almost sorted, Insertion Sort is cheap to execute in\npractice.\nLearned Sort 2.0 outperforms Quicksort and is competitive with state-\nof-the-art algorithms such as IPS4o. This fact is remarkable, given that\nML-enhanced sorting is a novel approach competing against implementa-\ntions that have been extensively tuned. There is room for algorithmic inno-\nvation in ML-enhanced sorting, hence future work might make it even more\ncompetitive with other approaches.\nThere are limitations to using Learned Sort. An obvious limitation is\nthat the current implementations focus on numeric types and cannot sort\nstrings. The most substantial limitation of Learned Sort currently is that it\ndoes not support parallelization. This can limit its use in practice because\n5\n\n2.3. Related Work\nparallel algorithms such as IPS4o can outperform Learned Sort by using all\nthe hardware available.\n2.3 Related Work\nML-enhanced sorting is closely related to the \feld of Learned Indexes\n[KBC+18]. Learned Indexes come from an emerging \feld using machine\nlearning to create highly e\u000ecient index data structures that outperform\ntraditional data structures such as B-Trees.\nEarly work on learned indexes focused mainly on key lookup and pri-\nmarily did not support updates. Their models relied heavily on modeling\nthe eCDF to predict the correct location of a key on a sorted array, al-\nbeit with di\u000berent strategies. RMIs count on a two-layer model where each\nmodel at the second layer can be seen as an expert on a speci\fc chunk of\nthe eCDF. RadixSpline [KMvR+20] approximates the eCDF using piece-\nwise linear functions and a radix table. The linear functions of RadixSpline\nguarantee a maximum error of \"for the eCDF approximation, hence to \fnd\na key we use the radix pre\fx of the element to use the correct function to\napproximate the eCDF. The Piecewise Geometric Model Index [FV20] also\nuses the idea of piecewise linear functions with bounded error to approx-\nimate the eCDF, although it opts for a multi-level structure instead of a\nradix table.\nLater work on learned indexes also incorporated indexes that supported\nupdates such as ALEX [DMY+20] and LIPP [WZC+21]. Both ALEX and\nLIPP store the data in a tree structure and use models to navigate the tree.\nDue to the tree structure, those indexes are also concerned with keeping\na small tree depth. To achieve small depths, they aim to minimize the\nmaximum number of elements in a tree node. Hence, newer models from\nlearned indexes also try to achieve a balanced partitioning.\nOther works that are related to this thesis are the Learned In-Memory\nJoins [SK21], particularly Learned Sort-Join. Learned Sort-Join pro\fts from\nthe eCDF modeling approach discussed earlier to speed up both the parti-\ntioning and the chunked-join phases of the algorithm.\n6\n\nChapter 3\nMachine Learning Models for\nSorting\nML-enhanced sorting algorithms leverage statistical properties of the\ndistribution to be sorted to achieve faster execution times. In general, ML-\nenhanced sorting is characterized by four phases:\nSample Data!Sort Sample!Train Model!Predict on Keys\nBefore we review each phase, it is important to introduce the concept of\ncomputing budget for ML-enhanced sorting. ML-enhanced sorting competes\nagainst traditional sorting algorithms like Quicksort. Hence, the cost of\nexecuting all four phases from ML-enhanced sorting must be less than or\nequal to the cost of executing Quicksort. This is a narrow budget because\nQuicksort itself is an e\u000ecient algorithm. The consequence of the computing\nbudget is that some choices that in theory lead to better models need to\nbe ruled out because they would cause the algorithm to be slower than\nQuicksort.\nThe \frst phase is sampling data. This phase is required because every\nmodel needs data to train on. The size of the sample is highly dependent\non the algorithm and the model used. More samples lead to more accurate\nmodels, however, there is a cost to sampling more data as we will explain in\nthe next phase.\nThe second phase is sorting the sample using another sorting algorithm.\nThe models used in sorting rely on the relative order of the elements from\nthe sample to learn about the data distribution. Hence, the sample needs\nto be sorted which is often done using a third algorithm. The cost of the\nsecond phase will always be higher than the cost of the \frst phase. For\nSsamples, the \frst phase runs in O(S) time while the second phase will\ngenerally run inO(SlogS).\nThe second phase is also relevant for implementation purposes because\nit implies that ML-enhanced algorithms cannot exist on their own. ML-\nenhanced algorithms can recursively call themselves to sort the sample if the\n7\n\nChapter 3. Machine Learning Models for Sorting\nsample size is large. However, a traditional sorting algorithm will still be at\nthe base. The fact is not necessarily an issue, because many popular sorting\nimplementations use a third algorithm such as Insertion Sort to handle base\ncases.\nThe third phase is training the model. This phase takes at least \n( S)\nwork, but its cost is very dependent on the chosen model. ML-enhanced\nsorting favors models that can be trained quickly and that do not require\nlots of parameters. Often, there is no budget to do a complete search to\n\fnd the optimal parameters for a model hence the search space is reduced.\nAn even more radical approach is to set some parameters to constants that\nwork well in the general case, such as in Learned Sort.\nMoreover, we can classify the models we can train into two types. Par-\ntitioning models are trained to predict the bucket Biof an element. The\noutput of prediction-based models is an integer in f0;1;2;:::k\u00001gwhere\nkis the number of buckets. This type of model aims to minimize metrics\nrelated to the number of elements per bucket in order to achieve a balanced\npartitioning. A \frst metric is to minimize the average number of elements\nper bucket,Pk\u00001\ni=0(jBij\u0000N\nk)2. Another possible metric is to minimize the\nmaximum number of elements in a bucket, min(max 0\u0014i<k(jBij)). We point\nout that the choice of the metric impacts the cost of training the model: for\nexample, it is cheaper to use the second metric to train linear models.\nIn contrast, eCDF models are trained to predict P(A\u0014x), the propor-\ntion of elements that are smaller than a given element in the distribution.\nThis happens because with P(A\u0014x), we can scale the output of the eCDF\nto \fnd the exact position of xin the array. The outputs of this type of\nmodel are real numbers in [0 ;1). eCDF models generally aim to minimize\nthe mean-squared error of their predictions,1\nNPN\u00001\ni=0(F(xi)\u0000P(A\u0014xi))2.\nWe highlight that any eCDF model can be converted to a partitioning model\nofkbuckets by scaling using Bi=bk\u0001F(xi)c.\nThe last phase is predicting the position or bucket of a key using the\ntrained model. For partition-based models, this is the phase that e\u000bectively\ncreates independent sorting subproblems that can be handled recursively.\nFor eCDF-based models, this phase is the one that attempts to sort the\narray by putting the elements in their correct positions.\nThe cost of the predicting phase is again dependent on the chosen model,\njust like in the training phase. However, we need to take into account that\nthe training phase handles Ssamples while the predicting phase handles\nall theNelements of the array. In that sense, slow predicting times are\nampli\fed by the fact that they apply to many elements. ML-enhanced\nsorting favors again simpler models such as those with O(1) predicting time\n8\n\n3.1. Recursive Model Index\nper element.\n3.1 Recursive Model Index\nThe Recursive Model Index (RMI) is an architecture based on a hierarchy\nof models that predicts the eCDF. It emerged as an alternative for B-Trees\nwhen doing look-ups on in-memory static arrays. Because RMIs estimate\nthe eCDF, they can also be used for sorting in algorithms that rely on eCDF\nsuch as Learned Sort.\nA RMI consists of Llevels, and each level ihasMimodels that recur-\nsively select the model in the next level of the RMI. The output F(x) of an\nRMI is a value in [0 ;1) that is an approximation for P(A\u0014x).\nFigure 3.1: Generic RMI with multiple levels.\nWe denote the k-th model of level iasf(k)\ni(x) and the ensemble of the\nlevel asfi(x) withfi(x)2[0;1). Hence,i-th level of a RMI can be described\nmathematically using recursion:\nfi(x) =f(bMifi\u00001(x)c)\ni (x)\nThe base case is at the \frst level with f1(x) =f(1)\n1(x), and the eCDF is\ngiven byF(x) =fL(x). The recursive strategy of the RMI is particularly\nsuccessful at avoiding the Last Mile Problem. The Last Mile Problem is the\nchallenge to reduce the magnitude of the error of the eCDF approximation.\nFor example, reducing the error rate from 10\u00003to 10\u00004might require a\nmuch more complex model than reducing the error rate from 10\u00001to 10\u00002.\n9\n\n3.1. Recursive Model Index\nRMIs mitigate the e\u000bect of the last mile problem by \\splitting\" the data\namong the models. Each model can be seen as an expert on a speci\fc part\nof the eCDF, which helps produce reasonable eCDF estimates with simpler\nmodels.\nOne observation from the de\fnition of RMIs is that the architecture is\nextremely \rexible. This is advantageous because, in theory, RMIs can model\nthe eCDF accurately. The disadvantage is that training an optimal RMI can\nbe a challenge. There are many parameters to optimize such as the number\nof levels, the number of models for each level, and the types of models for\neachf(k)\ni.\nIn practice, RMIs used in Learned Indexes and Sorting are simpler\nthan the originally proposed idea and are \fxed to L= 2 levels: F(x) =\nf(bM2f(1)\n1(x)c)\n2 (x).\nFigure 3.2: RMI used in practice. The number of levels is \fxed to two and\nthe optimal model types are chosen via training with heuristics.\nDespite having a \fxed number of levels, \fnding the optimal RMI remains\na non-trivial task. We still need to choose the model type of the \frst layer,\nthe second layer, and the number of models. Because the parameters are not\nindependent, heuristics such as the ones introduced in CDFSHOP [MZK20]\nare needed to \fnd the best RMI.\nML-enhanced sorting cannot a\u000bord the cost to \fnd the optimal RMI,\nbecause optimizing the RMI would cost more than sorting the array with\nother algorithms. Fortunately, an optimal RMI is not needed to achieve good\nresults. The authors of Learned Sort use a linear spline on the \frst layer with\na \fxed size of linear functions on the second layer. The second layer functions\nare not trained using linear regression and instead use linear interpolation\nto \fnd the coe\u000ecients. These choices yield a sub-optimal RMI, but the\nmodel is still e\u000bective because the error in eCDF predictions is balanced\nby the quicker training time of a constrained RMI. An independent review\n10\n\n3.2. Decision Trees\nof the RMI found that some data distributions may lead to many empty\nsegments in the second layer [MD21], which could impact the e\u000eciency of\nsorting because empty segments waste the predictive power of the RMI.\nNevertheless, RMIs work well in practice and Learned Sort has competitive\nresults.\n3.2 Decision Trees\nDecision Trees are non-parametric machine learning models that use\ncomparisons to make predictions. They are built on top of O(k) training\nelements and have a height of O(logk). For Sorting applications, Decision\nTrees predict the partition bucket Bifor eachxiwith the property that if\nxi<xj, thenBi\u0014Bj.\nTraining a Decision Tree is straightforward using a divide-and-conquer\nstrategy. We can assume that the sampled data is sorted from the previous\nsteps, which just requires converting the sorted array to a Binary Tree. At\neach step, we select the middle element as the splitter for that node of the\ndecision tree and continue to build the tree recursively by dividing the array\nin half.\nNaive implementations of the decision tree use conditionals to predict\nthe bucket of an element, which can cause branch mispredictions. Opti-\nmized implementations such as the one from IPS4o use a branchless version\nof the decision tree combined with loop unrolling. The splitters for the tree\nare stored in an array using a binary tree structure, hence the child nodes\nofajarea2janda2j+1. Thus, the next node can be decided using just\ncomparisons with j= 2j+ (x > a j). The branchless optimization is rele-\nvant because it can be combined with loop unrolling, e\u000bectively turning the\nprediction costO(1) for small k.\nDecision trees can also bene\ft from instruction-level parallelism. IPS4o\nbatches the elements in groups of 8 and applies loop unrolling again when\npredicting for the batch. This can increase the throughput of the decision\ntree because modern CPUs can execute more than one instruction per clock\ncycle.\nOne advantage of decision trees is that they are robust models against\ndistributions with lots of duplicates. IPS4o leverages the concept of equality\nbuckets in the decision tree. If during sampling we notice that there are\nmany duplicates of an element si, we can create a bucket Bijust for the\nelementsx=si. This is extremely convenient because after the partitioning\nthe equality bucket Biwill already be sorted as all the elements on it are\n11\n\n3.3. Linear Models\nidentical. Decision trees, therefore, contrast with the other models that\noften struggle to handle duplicates.\nThe disadvantages of decision trees come from the same points that make\nit a straightforward, robust model. To enhance a decision tree, we need to\nincreasekand the sample size as there are no other parameters to tweak.\nHowever, doing so increases both the training time and the evaluation time.\nIn addition, large values of kmay lead to the splitters not \ftting into the\ncache, which can harm performance. Thus, decision trees are e\u000ecient when\nthe number of buckets is moderate, but ine\u000ecient if the number of buckets\nis high.\n3.3 Linear Models\nLinear models are among the most fundamental machine learning models\nand they can also be applied to sorting. Evaluating a linear model is trivial\nand can be done in O(1), which is a desirable property for sorting as the\ncomputing budget for prediction time is low. Another desirable property of\nlinear models is monotonicity, because if xi< xj, thenF(xi)\u0014F(xj) (or\nthe mirrored equivalent if the slope is negative).\nWe focus on linear models for partitioning because linear models for\neCDF are dominated by RMIs. Each RMI can potentially have a linear\nmodel atf(k)\ni(x) and it is very likely that a two-layer RMI performs better\nthan a single linear model.\nThe models for partitioning are described by three parameters: the num-\nber of buckets k, the slope aand the constant term b. Mathematically, the\nmodel is:\nF(x) =8\n><\n>:0; ifba\u0001x+bc<0\nk\u00001; ifba\u0001x+bc\u0015k\nba\u0001x+bc;otherwise\nThe equation is adjusted using the \roor function because the output\nmust always be an integer. The result needs to satisfy 0 \u0014a\u0001x+b < k ,\nhence if the output is negative or too large we map it to zero and k\u00001\nrespectively.\nSome choices must be made when training a linear model because there\nare multiple metrics to optimize when training a linear model. Ideally, we\naim to haveN\nkelements per bucket if the goal is to have balanced partition-\ning. Hence, one possible metric is minimizingPk\u00001\ni=0(jBij\u0000N\nk)2. Minimizing\n12\n\n3.3. Linear Models\nthat speci\fc metric is a challenge, hence we analyse two alternatives.\nThe \frst option is minimizing the mean-squared error of the predicted\nbucket using a traditional linear regression. This is a valid option and the\nregression can be trained in O(S) forSsamples. If we minimize the sum of\nthe squared distances between each element and its target bucket, we expect\nthat most elements will indeed fall into their bucket. However, this strategy\nmay lead to a bucket having too many elements. Having too many elements\non a bucket implies doing more recursive partition steps.\nThe second option is to minimize the maximum number of nodes in a\nbucket. The Fastest Minimum Con\rict Degree (FMCD) is an algorithm in-\ntroduced in [WZC+21] that trains a linear model with exactly that property.\nIt was originally proposed as the model for the Updatable Learned Index\nwith Precise Positions (LIPP), a novel learned index that outperforms RMIs.\nFMCD can train a linear model from Ssample data points in O(S) time\nand guarantees that at mostS\n3elements will be in the same bucket if there\nare no duplicate data points. If the sample is representative, we also expect\nthat at mostN\n3keys will be in the bucket with the most elements after\napplying the model to every element.\ntemplate<typename Iterator>\nstd::tuple<double, double, std::size_t> FMCD(\nIterator begin, Iterator end, std::size_t K\n) {\nstd::size_t i = 0;\nstd::size_t D = 1;\nstd::size_t N = static_cast<std::size_t>(end - begin);\nlong double u_d = (\nstatic_cast<long double>(begin[N - 1 - D])\n- static_cast<long double>(begin[D])\n)/static_cast<long double>(K - 2);\nwhile(i <= N - 1 - D){\nwhile(\ni + D < N\n&& static_cast<long double>(begin[i + D] - begin[i]) >= u_d\n){\ni++;\n}\n13\n\n3.3. Linear Models\nif(i + D >= N) { break; }\nD += 1;\nif(D*3 > N) {\nbreak;\n}\nu_d = (\nstatic_cast<long double>(\nbegin[N - 1 - D])\n- static_cast<long double>(begin[D])\n)/static_cast<long double>(K - 2);\n}\nlong double A = 1/u_d;\nlong double B = (K - A * (\nstatic_cast<long double>(\nbegin[N - 1 - D])\n+ static_cast<long double>(begin[D])\n)) / static_cast<long double>(2);\nreturn std::make_tuple(\nstatic_cast<double>(A), static_cast<double>(B), D\n);\n}\nListing 1: Implementation of FMCD in C++\nThe linear model trained by FMCD is remarkable because it is suitable\nfor sorting even when the input distribution is non-linear. TheN\n3bound\nyields that on average O(logN) recursive partition steps will be performed\nno matter what input is given. If the input is modeled by a linear model,\nthen it is likely that fewer steps will be required because the constant of the\nlogarithm will be smaller.\nIn addition, we highlight that using large values for kis feasible using\nlinear models. The prediction time is independent of k, therefore the cost\nfor increasing konly a\u000bects the training step. This makes linear models\n14\n\n3.3. Linear Models\ngood candidates for parallel sorting on a large number of cores, as parallel\nsorting can bene\ft from a large kto match the number of cores p.\n15\n\nChapter 4\nProposed Algorithm\nWe propose the In-Place Parallel Learned Sort (IPLS) algorithm. IPLS\nextends IPS4o and has the goal to show that the code from IPS4o can be\nused to implement a parallel version of ML-enhanced sorting. IPLS keeps\nmost of the details of in-place partitioning and parallelism from IPS4o, but\nchanges the model and the base case for the algorithm.\nIPLS partitions the data in k= 256 buckets using linear models. It\nsamples\u000bk\u00001 random elements from the array with \u000b= 0:2 logN(kand\n\u000bwere kept the same as in IPS4o). Then, it trains a linear model on the\nsamples using the FMCD algorithm discussed earlier. The linear model F(x)\nis then used to predict the bucket for each element.\nForn\u0014212, IPLS uses SkaSort [Ska16] as the base case. SkaSort is one\nof the fastest Radix Sort implementations for inputs of that size. The choice\nof using SkaSort was inspired by IPS2Ra, which also uses SkaSort as a base\ncase.\n4.1 Worked Example of Proposed Algorithm\nTo illustrate the proposed algorithm, we give an example on a small\ninput of size n= 20 . We tweak some parameters because the algorithm\nwould generally not be executed for a small input. Assume that there are\nk= 4 buckets, that the bu\u000ber size for \rushing the bucket to the array is 2\nand that the base case is executed for inputs smaller than 10.\nInitially, we have an unordered array:\n18 39 33 28 25 34 11 27 47 248 50 10 636 13 912 22 29\nWe sample some data and obtain the coe\u000ecients a= 0:15 andb=\n\u00001:55 for our linear model. The algorithm then predicts the bucket of each\nkey. For example, for the key 28, we predict that it is in bucket 2 because\nb0:15\u000128\u00001:55c=b2:65c= 2.\n16\n\n4.1. Worked Example of Proposed Algorithm\nWe visually represent each bucket with colors: the zeroth bucket is rep-\nresented in pink, the \frst in yellow, the second one in green, and the third\none in cyan. We recall that the buckets are zero-indexed.\n39 33 28 25 34 47 11 248 50 10 613 918 22 27 29 12 36\nAfter, we apply a pass to make the buckets contiguous:\n11 210 613 912 18 22 28 25 27 29 39 33 34 47 48 50 36\nThe contiguous buckets may now be considered independent subprob-\nlems. The algorithm then opts to sort each bucket individually using the\nbase case given that the inputs are tiny:\n26910 11 12 13 18 22 25 27 28 29 33 34 36 39 47 48 50\nThe algorithm terminates after executing the base case. The output is\nan ordered array.\n17\n\nChapter 5\nExperimental Evaluation\nWe evaluate the performance of IPLS compared to other sorting algo-\nrithms on a similar benchmark presented in the Learned Sort 2.0 paper\n[KVK21]. We executed the benchmarks on the m5zn.metal instance from\nAWS for reproducibility. The instance runs an Intel ®Xeon®Platinum\n8252C CPU @ 3.80GHz with 48 cores, 768KB of L1 cache, 24MB of L2\ncache, and 192 GB of RAM.\nWe compare IPLS against four algorithms: IPS4o, IPS2Ra, Learned Sort,\nandstd::sort from C++ STL [ISO20]. The implementations were written\nin C++ and compiled with GCC 11 with the -O3 \rag.\nThe benchmark includes sequential and parallel settings. We refer to\nthe sequential versions of the algorithms as ILS, IS4o, and IS2Ra. We also\ndrop Learned Sort from the parallel benchmark because there is no parallel\nimplementation of Learned Sort.\nThe datasets used in the benchmark are split into real-world data and\nsynthetic data. The real-world datasets contain 64-bit unsigned integer ele-\nments to be sorted. The synthetic datasets contain 64-bit double \roating-\npoint elements. We note that IPS2Ra used the extractor from SkaSort that\nmaps double to integers in order to sort the synthetic datasets.\nAn overview of the datasets is:\nReal-World Datasets\n\u0000OSM/Cell IDs (N = 2\u0001108): Uniformly sampled location IDs from\nOpenStreetMaps [MKvR+20].\n\u0000Wiki/Edit (N = 2\u0001108): The edit timestamps from Wikipedia\narticles [MKvR+20].\n\u0000FB/IDs (N = 2\u0001108): The IDs from Facebook users sampled in a\nrandom walk of the network graph [MKvR+20].\n\u0000Books/Sales (N = 2\u0001108): Book popularity data from Amazon\n[MKvR+20].\n18\n\n5.1. Sequential Results\n\u0000NYC/Pickup (N = 108): The yellow taxi trip pick-up time stamps\n[KVK21].\nSynthetic Datasets\n\u0000Uniform (N = 108): Generated by std::uniform real distribution\n[ISO20] with a= 0 andb=N.\n\u0000Normal (N = 108): Generated by std::normal distribution [ISO20]\nwith\u0016= 0 and\u001b= 1.\n\u0000Log-Normal (N = 108): Generated by std::lognormal distribution\n[ISO20] with \u0016= 0 and\u001b= 0:5.\n\u0000Mix Gauss (N = 108): Generated by a random additive distribution\nof \fve Gaussian distributions [KVK21].\n\u0000Exponential (N = 108): Generated by std::exponential distribution\n[ISO20] with \u0015= 2.\n\u0000Chi-Squared (N = 108): Generated by std::chi squared distribution\n[ISO20] with k= 4.\n\u0000Root Dups (N = 108): Generated by A[i] =imodp\nNas proposed\nin [EW16].\n\u0000Two Dups (N = 108): Generated by A[i] =i2+N=2 modNas\nproposed in [EW16].\n\u0000Zipf (N = 108): Generated by a Zip\fan distribution with szipf= 0:75\n[KVK21].\n5.1 Sequential Results\nWe analyse the sorting rate of the algorithms. The sorting rate is mea-\nsured on keys per second and indicates the throughput of each algorithm.\nA higher sorting rate indicates that an algorithm is more performant.\nIS2Ra is the most e\u000ecient for sorting real-world datasets. IS4o comes\nin second and ILS comes in third place. Learned Sort comes fourth and\nstd::sort comes last. Our results disagree with those from [KVK21] that\nclaim Learned Sort is faster than IS4o in real-world datasets. We also note\nthat ILS beats Learned Sort in all of the real-world datasets.\n19\n\n5.1. Sequential Results\nOSM/Cell IDs Wiki/Edit FB/IDs Books/Sales NYC/Pickup010M/s20M/s30M/s40M/s50M/s60M/sSorting Rate (keys/sec)Performance of Sequential Sorting Algorithms in Real-World Datasets\nAlgorithms\nILS\nIS2ra\nIS4o\nLearnedSort\nStdSort\nFigure 5.1: Comparison of the sorting rate of the algorithms for real-world\ndatasets under a sequential setting. IS2Ra takes the lead in this scenario.\nHigher rates are better.\nUniform Normal Log Normal Mix Gauss Exponential010M/s20M/s30M/s40M/s50M/sSorting Rate (keys/sec)Performance of Sequential Sorting Algorithms in Synthetic Datasets\nAlgorithms\nILS\nIS2ra\nIS4o\nLearnedSort\nStdSort\nFigure 5.2: Comparison of the sorting rate of the algorithms for synthetic\ndatasets under a sequential setting. Learned Sort takes the lead in this\nscenario. Higher rates are better.\nLearned Sort, however, takes the lead on synthetic datasets. IS2Ra comes\nsecond. ILS and IS4o tie for third place. std::sort comes last one more\ntime. We can interpret the results as RMIs being able to model the distri-\n20\n\n5.2. Parallel Results\nbutions due to their \rexibility. Compared to real-world data, distributions\nlike the normal and exponential distribution contain less noise and are easier\nto model. Hence, Learned Sort achieves a favorable case which puts it \frst\nin performance. ILS does not get the same performance boost on synthetic\ndata and presents similar sorting rates that are comparable to the ones when\nsorting real-world data.\nChi Squared Root Dups Two Dups Zipf020M/s40M/s60M/s80M/sSorting Rate (keys/sec)Performance of Sequential Sorting Algorithms in Synthetic Datasets\nAlgorithms\nILS\nIS2ra\nIS4o\nLearnedSort\nStdSort\nFigure 5.3: Comparison of the sorting rate of the algorithms for synthetic\nwith an increased number of duplicates under a sequential setting. Learned\nSort takes the lead in this scenario, but struggles with the Root Dups\ndataset. Higher rates are better.\nThe trend continues and Learned Sort leads in 4 out of the 5 datasets\nwith duplicates. However, Learned Sort struggles with the Root Dups\ndataset being beaten even by std::sort . IS4o by contrast performs best on\nthe Root Dups dataset because of the equality buckets of its decision tree.\nILS remains tied with IS4o in all datasets except for Root Dups because of\nthe optimizations IS4o has to handle duplicates.\n5.2 Parallel Results\nIn the parallel setting, the fastest algorithms di\u000ber from the sequential\nsetting. IPS4o is the fastest, with IPLS in second, IPS2Ra in third, and\nthe parallel version of std::sort in last. In contrast to the sequential\nbenchmark, that order was kept the same for both real-world and synthetic\ndatasets.\n21\n\n5.2. Parallel Results\nOSM/Cell IDs Wiki/Edit FB/IDs Books/Sales NYC/Pickup0500M/s1B/s1.5B/s2B/s2.5B/sSorting Rate (keys/sec)Performance of Parallel Sorting Algorithms in Real-World Datasets\nAlgorithms\nIPLS\nIPS2ra\nIPS4o\nParallelStdSort\nFigure 5.4: Comparison of the sorting rate of the algorithms for real-world\ndatasets under a parallel setting. IPS4o takes the lead in this scenario.\nHigher rates are better.\nUniform Normal Log Normal Mix Gauss Exponential0500M/s1B/s1.5B/s2B/s2.5B/sSorting Rate (keys/sec)Performance of Parallel Sorting Algorithms in Synthetic Datasets\nAlgorithms\nIPLS\nIPS2ra\nIPS4o\nParallelStdSort\nFigure 5.5: Comparison of the sorting rate of the algorithms for synthetic\ndatasets under a parallel setting. IPS4o takes the lead in this scenario.\nHigher rates are better.\nWe may interpret the results from the parallel benchmark as to which\nalgorithm uses the hardware available the most. IPS4o uses a decision tree\nto partition the data, which creates many subproblems of a balanced size.\n22\n\n5.3. Scalability\nThis favours the performance of IPS4o because it manages to keep every\nthread of the CPU busy always doing work.\nIPLS on the other hand uses a linear model to partition the data. The\nlinear model also creates subproblems, but some distributions might trigger\nthe worst case of the partitioning where there is a bucket with N=3 of the\nelements. In this sense, IPLS generally manages to use all the threads\navailable but in some cases, it might take more recursive steps until it uses\nall of them.\nBy contrast, IPS2Ra does not manage all the hardware because its par-\ntitions are not balanced. There are no bounds on the number of elements\nthat have the same radix pre\fx and go in the same bucket. Hence, IPS2Ra\nmay end with threads waiting for work, hurting its sorting rate compared\nto IPS4o and IPLS which always keep threads busy.\nChi Squared Root Dups Two Dups Zipf0500M/s1B/s1.5B/s2B/s2.5B/s3B/s3.5B/sSorting Rate (keys/sec)Performance of Parallel Sorting Algorithms in Synthetic Datasets\nAlgorithms\nIPLS\nIPS2ra\nIPS4o\nParallelStdSort\nFigure 5.6: Comparison of the sorting rate of the algorithms for synthetic\ndatasets with an increased number of duplicates under a parallel setting.\nIPS4o takes the lead in this scenario. Higher rates are better.\n5.3 Scalability\nWe also compare how the sorting of the algorithms scales as the number\nof elements nincreases. We test both sequential and parallel versions on the\nNormal dataset with inputs ranging from 10k to 1B elements.\nOn the sequential setting, Learned Sort, IS4o, and ILS have close sorting\nrates until the input hits a size of 10M elements. The mark of 10M elements\n23\n\n5.3. Scalability\n10K 100K 1M 10M 100M 1B\nNumber of elements (log scale)10M/s25M/s50M/s100M/s250M/s500M/s1B/s2.5B/s5B/sSorting Rate (keys/sec) (log scale)Scalability on Normal Dataset: Number of Keys vs Sorting Rate\nAlgorithms\nILS\nIS2ra\nIS4o\nLearnedSort\nStdSort\nFigure 5.7: Comparison of the sorting rates for the Normal dataset as the\ninput size grows under a sequential setting. Higher rates are better.\nis the point where the number of elements stops \ftting into the cache. All\nalgorithms except IS2Ra have their sorting rates signi\fcantly reduced at\nthat point. Learned Sort takes the lead at that point and IS2Ra climbs to\nsecond. IS4o and ILS remain tied albeit the former performs slightly faster\nat 1B elements.\nOverall, ILS scales well in the sequential setting. Its sorting rate is almost\nidentical to IS4o, which is very competitive because IS4o is a state-of-the-art\nsorting algorithm.\nOn the parallel setting, the sorting rates at the beginning are impacted\nby the overhead of using multiple threads. We note that IPS4o and IPLS\nare slower than their sequential implementations at 1M elements, which puts\nthe parallel implementation of std::sort at the top. However, we point\nout that the single-threaded versions of IPS4o and IPLS are 2.5x faster than\nthe parallel std::sort for the range of 1M to 10M elements.\n24\n\n5.3. Scalability\n1M 10M 100M 1B\nNumber of elements (log scale)100M/s250M/s500M/s1B/s2.5B/s5B/sSorting Rate (keys/sec) (log scale)Scalability on Normal Dataset: Number of Keys vs Sorting Rate\nAlgorithms\nIPLS\nIPS2ra\nIPS4o\nParallelStdSort\nFigure 5.8: Comparison of the sorting rates for the Normal dataset as the\ninput size grows under a parallel setting. Higher rates are better.\nAs the input size grows, so does the sorting rate of IPLS and IPS4o\nuntil they hit 100M elements. We interpret the growth as a sign that larger\ninputs make it simpler for the algorithms to utilize all the hardware resources\navailable. The bottleneck during this range is not the partitioning but the\nscheduling of the parallel tasks.\nAfter 100M elements are reached, the sorting rate starts to degrade. The\ncache becomes the limiting factor one more time and the gap between IPLS\nand IPS2Ra shrinks, but IPLS always stays ahead of IPS2Ra.\nOverall, IPLS also scales well in the parallel setting. Its sorting rate in\nthe 1M to 10M could be improved by limiting the number of threads being\nused, as the overhead of using all threads makes the algorithm slower than in\nthe sequential setting. For the 10M to 1B range, IPLS does not win against\nIPS4o but still beats IPS2Ra and parallel std::sort .\n25\n\nChapter 6\nConclusion and Future Work\nWe have shown that the IPS4o provides a framework to implement par-\nallel ML-enhanced sorting. Through the framework, we were able to achieve\nparallel learned sorting overcoming the limitations of earlier work that could\nnot run in parallel.\nThe choice of linear models trained with FMCD did not crown our pro-\nposed algorithm as the fastest sorting algorithm available. However, it re-\nmained competitive against its peers. Remarkably, using a simple linear\nmodel to partition the data can achieve such sorting rates.\nIn this sense, our approach is promising because it lets us focus on im-\nproving the models used to sort and let the IPS4o framework handle imple-\nmentation details such as in-place partitioning and parallelism.\nFuture work to improve ML-enhanced sorting is an invitation to experi-\nment with modeling. There is a gap in modeling the eCDF distribution of\nstrings, as current models focus on numeric types. Moreover, there is room\nto incorporate ideas and models from the \feld of Learned Indexes into ML-\nenhanced sorting. Progress on Learned Indexes will bene\ft ML-enhanced\nsorting and vice-versa.\n26\n\nBibliography\n[AWFS22] Michael Axtmann, Sascha Witt, Daniel Ferizovic, and Peter\nSanders. Engineering in-place (shared-memory) sorting algo-\nrithms. ACM Trans. Parallel Comput. , 9(1), Jan 2022. !\npages 1, 3\n[DMY+20] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaey-\noung Do, Yinan Li, Hantian Zhang, Badrish Chandramouli,\nJohannes Gehrke, Donald Kossmann, David Lomet, and Tim\nKraska. ALEX: An updatable adaptive learned index. In Pro-\nceedings of the 2020 ACM SIGMOD International Conference\non Management of Data . ACM, Jun 2020. !pages 6\n[EW16] Stefan Edelkamp and Armin Wei\u0019. BlockQuicksort:\nHow Branch Mispredictions don't a\u000bect Quicksort.\nhttps://arxiv.org/abs/1604.06697 , 2016.!pages 19\n[FV20] Paolo Ferragina and Giorgio Vinciguerra. The PGM-index: a\nfully-dynamic compressed learned index with provable worst-\ncase bounds. PVLDB , 13(8):1162{1175, 2020. !pages 6\n[Hoa62] C. A. R. Hoare. Quicksort. The Computer Journal , 5(1):10{16,\n01 1962.!pages 1\n[ISO20] ISO. ISO/IEC 14882:2020 | Programming languages |\nC++ . International Organization for Standardization, Geneva,\nSwitzerland, 6th edition, Dec 2020. !pages 18, 19\n[KBC+18] Tim Kraska, Alex Beutel, Ed H Chi, Je\u000brey Dean, and Neoklis\nPolyzotis. The case for learned index structures. In Proceed-\nings of the 2018 SIGMOD International Conference on Man-\nagement of Data , pages 489{504, 2018. !pages 1, 4, 6\n[KMvR+20] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail\nStoian, Alfons Kemper, Tim Kraska, and Thomas Neumann.\nRadixspline: a single-pass learned index. In Proceedings of the\n27\n\nBibliography\nThird International Workshop on Exploiting Arti\fcial Intelli-\ngence Techniques for Data Management , pages 1{5, 2020. !\npages 6\n[KVc+20] Ani Kristo, Kapil Vaidya, Ugur C \u0018etintemel, Sanchit Misra, and\nTim Kraska. The case for a learned sorting algorithm. In Pro-\nceedings of the 2020 ACM SIGMOD International Conference\non Management of Data , SIGMOD '20, page 1001{1016, New\nYork, NY, USA, 2020. Association for Computing Machinery.\n!pages 1, 4\n[KVK21] Ani Kristo, Kapil Vaidya, and Tim Kraska. Defeating dupli-\ncates: A re-design of the learnedsort algorithm. arXiv preprint\narXiv:2107.03290 , 2021.!pages 1, 5, 18, 19\n[MD21] Marcel Maltry and Jens Dittrich. A critical analysis of recur-\nsive model indexes. arXiv preprint arXiv:2106.16166 , 2021.!\npages 11\n[MKvR+20] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail\nStoian, Sanchit Misra, Alfons Kemper, Thomas Neumann, and\nTim Kraska. Benchmarking learned indexes. Proc. VLDB En-\ndow., 14(1):1{13, sep 2020. !pages 18\n[MZK20] Ryan Marcus, Emily Zhang, and Tim Kraska. Cdfshop: Ex-\nploring and optimizing learned index structures. In Proceedings\nof the 2020 ACM SIGMOD International Conference on Man-\nagement of Data , SIGMOD '20, page 2789{2792, New York,\nNY, USA, 2020. Association for Computing Machinery. !\npages 10\n[SK21] Ibrahim Sabek and Tim Kraska. The case for learned in-\nmemory joins. arXiv preprint arXiv:2111.08824 , 2021.!pages\n6\n[Ska16] Malte Skarupke. I Wrote a Faster Sorting Algorithm , Dec 2016.\n[link].!pages 16\n[SW04] Peter Sanders and Sebastian Winkel. Super scalar sample sort.\nIn Susanne Albers and Tomasz Radzik, editors, Algorithms {\nESA 2004 , pages 784{796, Berlin, Heidelberg, 2004. Springer\nBerlin Heidelberg. !pages 3\n28\n\nBibliography\n[WZC+21] Jiacheng Wu, Yong Zhang, Shimin Chen, Jin Wang, Yu Chen,\nand Chunxiao Xing. Updatable learned index with precise po-\nsitions. Proc. VLDB Endow. , 14(8):1276{1288, Apr 2021. !\npages 1, 6, 13\n29",
  "textLength": 51840
}