{
  "paperId": "0453366b166796ff205856aaa229f0af74ee91be",
  "title": "Learning Multi-Dimensional Indexes",
  "pdfPath": "0453366b166796ff205856aaa229f0af74ee91be.pdf",
  "text": "Learning Multi-dimensional Indexes\nVikram Nathan∗, Jialin Ding∗, Mohammad Alizadeh, Tim Kraska\n{vikramn,jialind,alizadeh,kraska}@mit.edu\nMassachusetts Institute of Technology\nABSTRACT\nScanning and filtering over multi-dimensional tables are key\noperations in modern analytical database engines. To opti-\nmize the performance of these operations, databases often\ncreate clustered indexes over a single dimension or multi-\ndimensional indexes such as R-Trees, or use complex sort\norders (e.g., Z-ordering). However, these schemes are often\nhard to tune and their performance is inconsistent across dif-\nferent datasets and queries. In this paper, we introduce Flood,\na multi-dimensional in-memory read-optimized index that\nautomatically adapts itself to a particular dataset and work-\nload by jointly optimizing the index structure and data storage\nlayout. Flood achieves up to three orders of magnitude faster\nperformance for range scans with predicates than state-of-the-\nart multi-dimensional indexes or sort orders on real-world\ndatasets and workloads. Our work serves as a building block\ntowards an end-to-end learned database system.\n1 INTRODUCTION\nScanning and filtering are the foundation of any analytical\ndatabase engine, and several advances over the past several\nyears specifically target database scan and filter performance.\nMost importantly, column stores [ 7] have been proposed to\ndelay or entirely avoid accessing columns (i.e., attributes)\nwhich are not relevant to a query. Similarly, there exist many\ntechniques to skip over records that do not match a query filter.\nFor example, transactional database systems create a clustered\nB-Tree index on a single attribute, while column stores often\nsort the data by a single attribute. The idea behind both is\nthe same: if the data is organized according to an attribute\nthat is present in the query filter, the execution engine can\neither traverse the B-Tree or use binary search, respectively, to\nquicklynarrowitssearchtotherelevantrangeinthatattribute.\nWe refer to both approaches as clustered column indexes.\nIf data has to be filtered by more than one attribute, sec-\nondary indexes can be used. Unfortunately, their large storage\noverhead and the latency incurred by chasing pointers make\nthem viable only for a rather narrow use case, namely when\nthe predicate on the indexed attribute has a very high selectiv-\nity; in most other cases, scanning the entire table can be faster\nand more space efficient [ 6]. An alternative approach is to use\nmulti-dimensional indexes to organize the data; these may\n∗Equal contribution.be tree-based data structures (e.g., k-d trees, R-Trees, or oc-\ntrees) or a specialized sort order over multiple attributes (e.g.,\na space-filling curve like Z-ordering or hand-picked hierar-\nchical sort). Indeed, many state-of-the-art analytical database\nsystems use multi-dimensional indexes or sort-orders to im-\nprove the scan performance of queries with predicates over\nseveral columns. For example, both Redshift [ 1] and Spark-\nSQL [ 4] use Z-ordering to lay out the data; Vertica can define\na sort-order over multiple columns (e.g., first age, then date),\nwhile IBM Informix, along with other spatial database sys-\ntems, uses an R-Tree [15].\nHowever, multidimensional indexes still have significant\ndrawbacks. First, these techniques are extremely hard to tune.\nFor example, Vertica’s ability to sort hierarchically on mul-\ntiple attributes requires an admin to carefully pick the sort\norder. The admin must therefore know which columns are\naccessed together, and their selectivity, to make an informed\ndecision. Second, there is no single approach (even if tuned\ncorrectly) that dominates all others. As our experiments will\nshow, the best multidimensional index varies depending on\nthe data distribution and query workload. Third, most existing\ntechniques cannot be fully tailored for a specific data distribu-\ntion and query workload. While all of them provide tunable\nparameters (e.g., page size), they do not allow finer-grained\ncustomization for a specific dataset and filter access pattern.\nTo address these shortcomings, we propose Flood, the first\nlearned multi-dimensional in-memory index. Flood’s goal is\nto locate records matching a query filter faster than existing\nindexes, by automatically co-optimizing the data layout and\nindex structure for a particular data and query distribution.\nCentral to Flood are two key ideas. First, Flood uses a sample\nquery filter workload to learn how often certain dimensions\nare used, which ones are used together, and which are more\nselective than others. Based on this information, Flood au-\ntomatically customizes the entire layout to optimize query\nperformance on the given workload. Second, Flood uses em-\npirical CDF models to project the multi-dimensional and po-\ntentially skewed data distribution into a more uniform space.\nThis “flattening” step helps limit the number of points that\nare searched and is key to achieving good performance.\nFlood’s learning-based approach to layout optimization\nis what distinguishes it from other multi-dimensional index\n1arXiv:1912.01668v1  [cs.DB]  3 Dec 2019\n\nSIGMOD’20, June 14-19, 2020, Portland, OR, USA Vikram Nathan∗, Jialin Ding∗, Mohammad Alizadeh, Tim Kraska\nstructures. It allows Flood to target its performance to a par-\nticular query workload, avoid the superlinear growth in in-\ndex size that plagues some indexes even with uniformly dis-\ntributed data [ 9], and locate relevant records quickly without\nthe high traversal times incurred by k-d trees and hyperoc-\ntrees, especially for larger range scans.\nWhile Flood’s techniques are general and may potentially\nbenefit a wide range of systems, from OLTP in-memory trans-\naction processing systems to disk-based data warehouses, this\npaper focuses on improving multi-dimensional index perfor-\nmance (i.e., reducing unnecessary scan and filter overhead)\nfor an in-memory column store. In-memory stores are increas-\ningly popular due to lower RAM prices [ 20] and the increasing\namount of main memory which can be put into a single ma-\nchine [ 8,19]. In addition, Flood is optimized for reads (i.e.,\nquery speed) at the expense of writes (i.e., incremental index\nupdates), making it most suitable for rather static analytical\nworkloads, though our experiments show that adjusting to a\nnew query workload is relatively fast. We envision that Flood\ncould serve as the building block for a multi-dimensional\nin-memory key-value store or be integrated into commer-\ncial in-memory (offline) analytics accelerators like Oracle’s\nDatabase In-Memory (DBIM) [34].\nThe ability to self-optimize allows Flood to outperform al-\nternative state-of-the-art techniques by up to three orders of\nmagnitude, while often having a significantly smaller storage\noverhead. More importantly though, Flood achieves optimal-\nity across the board : it has better, or at least on-par, perfor-\nmance compared to the next-fastest indexing technique on\nall our datasets and workloads. For example, on a real sales\ndataset, Flood achieves a boost of 3×over a tuned clustered\ncolumn index and 72×over Amazon Redshift’s Z-encoding\nmethod. On a different workload derived from TPC-H, Flood\nis61×faster than the clustered column index but only 3×\nfaster than the Z-encoding.\nWe make the following contributions:\n(1)We design and implement Flood, the first learned multi-\ndimensional index, on an in-memory column store. Flood\ntargets its layout for a particular workload by learning\nfrom a sample filter predicate distribution.\n(2)We evaluate a wide range of multi-dimensional indexes on\none synthetic and three real-world datasets, including one\nwith a workload from an actual sales database at a major\nanalytical database company. Our evaluation shows that\nFlood outperforms all other index structures.\n(3)We show that Flood achieves query speedups on different\nfilter predicates and data sizes, and its index creation time\nis competitive with existing multi-dimensional indexes.2 RELATED WORK\nThere is a rich corpus of work dedicated to multi-dimensional\nindexes, and many commercial database systems have turned\nto multi-dimensional indexing schemes. For example, Ama-\nzon Redshift organizes points by Z-order [ 29], which maps\nmulti-dimensional points onto a single dimension for sort-\ning [ 1,33,46]. With spatial dimensions, SQL Server allows\nZ-ordering [ 27], and IBM Informix uses an R-Tree [ 15]. Other\nmulti-dimensional indexes include K-d trees, octrees, R∗trees,\nUB trees (which also make use of the Z-order), among many\nothers (see [ 31,40] for a survey). Flood’s underlying index\nstructure is perhaps most similar to Grid Files [ 30], which\nhas many variants [ 13,14,41]. However, Grid Files do not\nautomatically adjust to the query workload, yielding poorer\nperformance (§7). In fact, Grid Files tend to have superlinear\ngrowth in index size even for uniformly distributed data [9].\nFlood also differs from other adaptive indexing techniques\nsuch as database cracking [ 16,17,37]. The main goal of crack-\ning is to build a query-adaptive incremental index by par-\ntitioning the data incrementally with each observed query.\nHowever, cracking produces only single dimensional clus-\ntered indexes, and does not jointly optimize the layout over\nmultiple attributes. This limits its usefulness on queries with\nmulti-dimensional filters. Furthermore, cracking does not\ntake the data distribution into account and adapts only to\nqueries; on the other hand, Flood adapts to both the queries\nandthe underlying data.\nArguably most relevant to this work is automatic index se-\nlection [ 3,25,44]. However, these approaches mainly focus on\ncreating secondary indexes, whereas Flood optimizes the stor-\nage and index itself for a given workload and data distribution.\nFor aggregation queries, data cubes [ 11] are an alternative\nto indexes. However, data cubes alone are insufficient for\nqueries over arbitrary filter ranges, and they cannot support\narbitrary actions over the queried records (e.g., returning the\nrecords themselves).\nFinally, learned models have been used to replace/enhance\ntraditional B-trees [ 5,10,23] and secondary indexes [ 21,45].\nSelf-designing systems use learned cost models to synthe-\nsize the optimal algorithms for a data structure, resulting in\na continuum of possible designs that form a “periodic table”\nof data structures [ 18]. Flood extends these works in two\nways. First, Flood learns models for indexing multiple dimen-\nsions. Since there is no natural sort order for points in many\ndimensions, Flood requires a design tailored specifically to\nmulti-dimensional data. Second, prior work focused solely on\nconstructing models of the data, without taking queries into\naccount. Flood optimizes its layout by learning from the query\nworkload as well. Also unlike [ 18], Flood embeds models into\nthe data structure itself.\n2\n\nLearning Multi-dimensional Indexes SIGMOD’20, June 14-19, 2020, Portland, OR, USA\nOptimize \nLayout Queries \nDataset \nPreprocess \nDataset Execution \nEngine \nFind Intersecting \nCells \nEstimate Physical \nIndex and Rectify \nScan and Filter \nOptimal Layout \nCDFs \nData sorted \nby cell and \nsort attribute \nOffline Online Query \nResult \nFigure 1: Flood’s system architecture.\nSageDB[ 22]proposedtheideaofalearnedmulti-dimensional\nindex but did not describe any details.\n3 INDEX OVERVIEW\nFlood is a multi-dimensional clustered index that speeds up\nthe processing of relational queries that select a range over\none or more attributes. For example:\nSELECT SUM(R.X)\nFROM MyTable\nWHERE (a≤R.Y≤b) AND (c≤R.Z≤d)\nNote that equality predicates of the form R.Z == f can\nbe rewritten as f≤R.Z≤f. Typical selections generally\nalso include disjunctions (i.e. ORclauses). However, these can\nbe decomposed into multiple queries over disjoint attribute\nranges; hence our focus on ANDs.\nFlood consists of two parts: (1) an offline preprocessing\nstep that chooses an optimal layout, creating an index based\non that layout, and (2) an online component responsible for\nexecuting queries as they arrive (see Fig. 1).\nAt a high level, Flood is a variant of a basic grid index that\ndivides d-dimensional data space into a d-dimensional grid of\ncontiguous cells, so that data in each cell is stored together. We\ndescribe Flood’s grid layout and online operation in §3.1 and\n§3.2. We then discuss Flood’s central idea: how to automat-\nically optimize the grid layout’s parameters for a particular\nquery workload (§4). The rest of this paper uses the terms\nattribute anddimension interchangeably, as well as the terms\nrecord andpoint .\n3.1 Data Layout\nConsider an index on ddimensions. Unlike the single dimen-\nsional case, points in multiple dimensions have no natural sort\norder. Our first goal is then to impose an ordering over the data.\nWe first rank the dattributes. The details of how to choose\na ranking are discussed in §4, but for the purposes of illus-\ntration, we assume it is given. Next, we use the first d−1\ndimensions in the ordering to overlay a (d−1)-dimensional\ngrid on the data, where the ith dimension in the ordering is\ndivided into ciequally spaced columns between its minimum\nand maximum values. Every point maps to a particular cell\nin this grid, i.e. a tuple with d−1attributes. In particular, if\nMiandmiare the maximum and minimum values of the data\nCell 1 Cell 2 Cell 3 Cell 4 Cell 5 \nAttribute 1 Attribute 2 Figure 2: A basic layout in 2D, with dimension order\n(x, y) and c0=5. Points are bucketed into columns\nalong x and then sorted by their y-values, creating the\nseriliaziation order indicated by the arrows.\nalong the ith dimension, then define the dimension’s range\nasri=Mi−mi+1. Then the cell for point p=(p1,...,pd)is:\ncell(p)=\u0012\u0016p1−m1\nr1·c1\u0017\n,...,\u0016pd−1−md−1\nrd−1·cd−1\u0017\u0013\nNote that the cell is determined only by the first d−1dimen-\nsions; the dth dimension, the sort dimension , will be used to\norder points within a cell.\nFlood orders the points using a depth-first traversal of the\ncells along the dimension ordering, i.e. cells are sorted by the\nfirst value in the tuple, then the second, etc. Within each cell,\npoints are sorted by their value in the dth dimension. Fig. 2\nillustrates the sort order for a dataset with two attributes.\nFlood then sorts the data by this traversal. In other words,\npoints in cell 0 (sorted by their sort dimension) come first,\nfollowed by cell 1, etc. Ties are broken arbitrarily.\n3.2 Basic Operation\nFlood receives as input a filter predicate consisting of ranges\nover one or more attributes, joined by AND s. The intersection\nof these ranges defines a hyper-rectangle, and Flood’s goal\nis to find and process exactly the points within this hyper-\nrectangle (e.g., by aggregating them). At a high level, Flood\nexecutes the following workflow (Fig. 3):\n(1)Projection : Identify the cells in the grid layout that inter-\nsect with the predicate’s hyper-rectangle. For each such\ncell, identify the range of positions in storage, i.e. the phys-\nical index range , that contains that cell’s points (§3.2.1).\n(2)Refinement: If applicable, take advantage of the or-\ndering of points within each cell to shorten (or re-\nfine)eachphysicalindexrangethatmustbescanned\n(§3.2.2).\n(3)Scan : For each refined physical index range, scan and\nprocess the records that match the filter.\n3.2.1 Projection. In order to determine which points match\na filter, Flood first determines which cells contain the match-\ning points. Since the query defines a “hyper-rectangle” in the\n3\n\nSIGMOD’20, June 14-19, 2020, Portland, OR, USA Vikram Nathan∗, Jialin Ding∗, Mohammad Alizadeh, Tim Kraska\n(1a) Projection finds 4 \nintersecting cells \n(cells 1-4) (1b) Identify physical \nindex range of third cell. \n(Repeat for other cells.) (2) Refine the \nphysical index \nrange (3) Scan and \nFilter qSqe\n= matched query filter \n= not matched query filter \n= start and end \nof cell’s physical \nindex range \n= start and end \nof refined physical \nindex range \nCell 1 Cell 2 Cell 4 Cell 5 Cell 3 \nFigure 3: Basic flow of Flood’s operation\n(d−1)-dimensional grid, computing intersections is straight-\nforward. Suppose that each filter in the query is a range of\nthe form[qs\ni,qe\ni]for each indexed dimension i. If an indexed\ndimension is not present in the query, we simply take the start\nand end points of the range to be −∞and+∞, respectively.\nConversely, if the query includes a dimension not in the index,\nthat filter is ignored at this stage of query processing.\nThe “lower-left” corner of the hyper-rectangle is qs=\n(qs\n0, ... ,qs\nd−1)and likewise for the “upper-right” corner qe.\nBoth are shown in Fig. 3. Then, we define the set of intersect-\ning cells as{Ci|cell(qs)i≤Ci≤cell(qe)i}. Flood keeps a cell\ntable which records the physical index of the first point in\neach cell. Knowing the intersecting cells then easily translates\nto a set of physical index ranges to scan.\n3.2.2 Refinement. When the query includes a filter over the\nsort dimension, Flood uses the fact that points in each cell are\nordered by the sort dimension to further refine the physical\nindex ranges to scan. In particular, suppose the query includes\na filter over the sort dimension R.Sof the form a≤R.S≤b.\nFor each cell, Flood finds the physical indices of both the first\npoint I1having R.S≥aand the last point I2such that R.S≤b.\nThis narrows the physical index range for that cell down to\n[I1,I2]. The simplest way to find [I1,I2]is by performing binary\nsearch within Con the values in the sort dimension. This is\npossible only because the points in Care stored contiguously\nin sorted order by the sort dimension. We discuss a faster way\nto refine, using models, in §5.2. If the query does not filter\nover the sort dimension, Flood skips the refinement step.\n4 OPTIMIZING THE GRID\nFlood’s grid layout has several parameters that can be tuned,\nnamely the number of columns allocated to each of the d−1\ndimensions that form the grid, and which dimension to use as\nthe sort dimension. Adjusting these parameters is the key way\nin which Flood optimizes performance on a given query work-\nload. We found that the ordering of the d−1grid dimensions\ndid not significantly impact performance.\nAdding more columns in each dimension allows Flood to\nscan a rectangle that more tightly bounds the true query filter,\nwhich reduces the number of points that must be scanned\nAttribute 1Attribute 2\nAttribute 1Attribute 2Figure4:Doublingthenumberofcolumnscanincrease\nthe number of visited cells but decreases the number\nof scanned points that don’t match the filter (light red).\n(Fig. 4). However, adding more columns also increases the\nnumber of sub-ranges, which incurs extra cost for projection\nand refinement. Striking the right balance requires choosing a\nlayout with an optimal number of columns in each dimension.\nFlood can also select the sort dimension. The sort dimen-\nsion is special because it will incur no scan overhead; given\na query, Flood finds the precise sub-ranges to scan in the re-\nfinement step, so that the values in the sort dimension for\nscanned points are guaranteed to lie in the desired range. On\nthe other hand, the grid dimensions do incur scan overhead\nbecause a certain column might only lie partially within the\nquery rectangle. Therefore, the choice of sort dimension can\nhave a significant impact on performance.\nIt is hard to select the optimal number of columns in each\ndimension because it depends on many interacting factors,\nincluding the frequency of queries filtering on that dimen-\nsion, the average and variance of filter selectivities on that\ndimension, and correlations with other dimensions in both\nthe data and query workload. The optimal sort dimension is\nalso hard to select for similar reasons. Therefore, we optimize\nlayout parameters using a cost model based approach. We\nfirst describe the cost model, then present the procedure that\nFlood uses to optimize the layout.\n4.1 Cost Model\nDefine a layout over ddimensions as L=(O,{ci}0≤i<d−1),\nwhere Ois an ordering of the ddimensions, in which the\ndth dimension is the sort dimension and {ci}0≤i<d−1is the\nnumber of columns in the remaining d−1grid dimensions.\nGiven a dataset Dand a layout L, we model the query time\nof any query qas a sum of three parts, which correspond to\nthe steps of the query flow from §3.2. Each part consists of\nsome measurable statistic N, which is multiplied by a variable\nweight wwhich is a function of the dataset D, query q, and\nlayout L, to produce an estimate of time taken on that step:\n(1)Projection contributes wpNcto the query time, where Nc\nis the number of cells that fall within the query rectangle,\nandwpis the average time to perform projection on a sin-\ngle cell. The weight wpis not constant across all datasets,\nqueries, and layouts. For example, it is faster to identify\na block of cells along a single grid dimension, which are\n4\n\nLearning Multi-dimensional Indexes SIGMOD’20, June 14-19, 2020, Portland, OR, USA\nadjacent on linear storage media, than a hypercube of cells\nalong multiple grid dimensions which are non-adjacent.\n(2)Refinement contributes wrNcto the query time, where\nwris the average time to perform refinement on a cell. If\nthe query qdoes not filter on the sort dimension, refine-\nment is skipped and wris zero. Also, wris lower if the\ncell is smaller, because the piecewise linear CDF for that\ncell (explained in §5.2) is likely less complex and makes\npredictions more quickly.\n(3)Scan contributes wsNsto the query time, where Nsis the\nnumber of scanned points, and wsis the average time to\nperform each scan. The weight wsdepends on the num-\nber of dimensions filtered (fewer dimensions means fewer\nlookups for each scanned point), the run length of the scan\n(longer runs have better locality), and how many scans\nfall within exact sub-ranges (explained in §7.1).\nPutting everything together, our model for query time is:\nTime(D,q,L)=wpNc+wrNc+wsNs (1)\nGiven a dataset Dand a workload of queries {qi}, we find\nthe layout Lthat minimizes the average of Eq. 1 for all q∈{qi}.\n4.1.1 Calibrating the Cost Model Weights. Since the four\nweight parameters w={wp,wr,ws}vary based on the data,\nquery and layout, Flood uses models to predict w. The features\nof these weight models are statistics that can be measured\nwhen running the query on a dataset with a certain layout.\nThese statistics include N={Nc,Ns}, the total number of\ncells, the average, median, and tail quantiles of the sizes of\nthe filterable cells, the number of dimensions filtered by the\nquery, the average number of visited points in each cell, and\nthe number of points visited in exact sub-ranges.\nAs we show in §7.7, the weight models are accurate across\ndifferent datasets and query workloads. In particular, when\nnew data arrives or the query distribution changes, Flood\nneeds only to evaluate the existing models, instead of training\nnew ones. Flood therefore trains the weight models once to\ncalibrate to the underlying hardware. To produce training ex-\namples, Flood uses an arbitrary dataset and query workload,\nwhich can be synthetic. Flood generates random layouts by\nrandomly selecting an ordering of the ddimensions, then ran-\ndomly selecting the number of columns in the grid dimensions\nto achieve a random target number of total cells. Flood then\nruns the query workload on each layout, and measures the\nweights wand aforementioned statistics for each query. Each\nquery for each random layout will produce a single training\nexample. In our evaluation, we found that 10 random layouts\nproduces a sufficient number of training examples to create\naccurate models. Flood then trains a random forest regression\nmodel to predict the weights based on the statistics.\nOne natural question to ask is whether a single random\nforest model can be trained to predict query time, instead of\n100103106109\nNum scanned points102\n101\n100101ws\n100102104106\nAvg scan run length102\n101\n100101wsFigure 5: ws(and by extension, scan time) is not con-\nstant and is difficult to model analytically because of\nits non-linear dependence on related features.\nfactoring the query time as weighted linear terms and training\na model for each weight. However, a single model is inade-\nquate because we want to accurately predict query times\nacross a range of magnitudes; a model for query time would\noptimize for accuracy of slow queries at the detriment of fast\nqueries. On the other hand, the weights span a relatively nar-\nrow range (e.g., the average time to scan a point will not vary\nacross orders of magnitude), so are more amenable to our goal.\n4.1.2 Why Use Machine Learning? We model the cost using\nmachine learning because query time is a function of many\ninterdependent variables with potentially non-linear relation-\nships that are difficult to model analytically. For instance, on\n10k training examples, Fig. 5 shows not only that the empirical\naverage time to scan a point ( ws) is not constant, but also that\nits dependence on two related features (number of scanned\npoints and average scan run length, which affects locality) is\nnon-linear and does not follow an obvious pattern.\nIndeed, we found that query time predicted using a simple\nanalytical model that replaces the weight parameters of Eq. 1\nwith fine-tuned constants has on average 9 ×larger difference\nfrom the true query time than our machine-learning based\ncost model. Furthermore, predicting the weight parameters\nusing a linear regression model that uses the same input fea-\ntures as our random forest produces query time predictions\nwith 4×larger difference from the true query time, which\nconfirms that the features are interdependent and/or have\nnon-linear relation with query time.\n4.2 Optimizing the Layout\nGiven a calibrated cost model, Flood optimizes its layout for\na specific dataset and query workload as follows (pseudocode\nis provided in Appendix B):\n(1)Sample the dataset and query workload, then flatten the\ndata sample and workload sample using RMIs trained on\neach dimension.\n(2)Iteratively select each of the ddimensions to be the sort\ndimension. Order the remaining d−1dimensions that\nform the grid by the average selectivity on that dimension\nacross all queries in the workload. This gives us O.\n5\n\nSIGMOD’20, June 14-19, 2020, Portland, OR, USA Vikram Nathan∗, Jialin Ding∗, Mohammad Alizadeh, Tim Kraska\n(3)For each of these dpossible orderings, run a gradient\ndescent search algorithm to find the optimal number of\ncolumns{ci}0≤i<d−1for the d−1grid dimensions. The\nobjective function is Eq. 1. For each call to the cost model,\nFlood computes the statistics N={Nc,Ns}and the in-\nput features of the weight models using the data sample\ninstead of the full dataset D.\n(4)Select the layout with the lowest objective function cost\namongst the dlayouts.\nOptimizing the layout is efficient (§7.7) because each itera-\ntion of gradient descent does not require building the layout,\nsorting the dataset, or running the query. Instead, statistics\nare either estimated using a sample of Dor computed exactly\nfrom the query rectangle and layout parameters.\n5 LEARNING FROM THE DATA\nThe simple index presented in §3 does not consider or adapt\nto the underlying distribution of the data. Here, we present\ntwo ways that Flood learns its layout from the data. First,\nFlood uses a model of each attribute to better determine column\nspacing. Second, Flood accelerates refinement within each cell\nusing a model of the underlying data.\n5.1 Flattening\nThe index in §3 spaces columns equally, but this type of layout\nis inefficient when indexing highly skewed data: some grid\ncells will have a large number of points, causing Flood to scan\ntoo many superfluous points.\nIf we were to have an accurate model of each attribute’s dis-\ntribution, i.e. its CDF, we could choose columns such that for\neach attribute, each column is responsible for approximately\nthe same number of points. In practice, Flood models each\nattribute using a Recursive Model Index (RMI), a hierarchy\nof models, e.g. linear models in our case, that is quick to eval-\nuate [ 23]. The input to the model is the attribute value v; the\noutput is the fraction of points with values ≤v. At query time,\nsuppose that we would like to split the kth dimension into n\ncolumns. A point with value vin the kth dimension will be\nplaced into column ⌊CDF(v)·n⌋. Since evaluating the RMI is\nefficient, we can efficiently determine the columns in each\ndimension that the query intersects.\nFig. 6 shows example 2-D data and the result of applying\nthis transformation to Attribute 1. The column boundaries\nare no longer equally spaced across the range of Attribute 1’s\nvalues. Instead, they are equally spaced in terms of the CDF\nof Attribute 1. This means that each of the four columns has\naround 1/4of the points. Since the number of points in each\ncolumn is evened out, we call this a flattened layout. Flood\napplies this flattening transformation for each grid dimension.\nSkewness is abundantly present in real-world data; on two of\nFigure 6: By flattening, each of the four columns in a\ndimension will contain a fourth of the points.\nthe datasets used in our evaluation (§7), flattening provides\na performance boost of 20−30×over a non-flattened layout.\nNote that while flattening may assign an equal number of\npoints to each column of a single attribute, it does not guar-\nantee that each cell in the final grid has a similar number of\npoints. In particular, if two attributes are correlated, flattening\neach attribute independently will not yield uniformly sized\ncells. This may lead to some cells incurring a high scan over-\nhead. In practice, we found that modeling single attributes,\ni.e. assuming each dimension is independent, was sufficient.\nIf necessary, adding more columns per dimension can further\nreduce the per-cell scan overhead. Flood’s layout training pro-\ncedure (§4) will choose the number of columns per dimension\nto trade off scan overhead with projection and refinement\ncost, mitigating the effect of non-uniform cell sizes. Addition-\nally, if the correlation results in some cells being empty, those\ncells can be easily pruned using the cell table, incurring very\nlittle overhead. However, recent work [ 21,45] suggests that\nit might be possible to further reduce the index size by taking\nadvantage of the correlation. Exploring such techniques for\nmulti-dimensional clustered indexes remains future work.\n5.2 Faster Refinement\nThe simple index from §3.2.2 uses binary search over the sort\ndimension to refine the physical index range of each cell. In\npractice, since we may have to refine in every cell, binary\nsearch is too slow. Instead, Flood builds a CDF model over the\nsort dimension values for each cell. Flood uses a cell’s model\nto estimate the endpoints of the refined physical index range,\nand then corrects any misprediction through a local search.\nWe want a model that can achieve a low average absolute\nerror, while keeping the maximum error bounded to a reason-\nable value, in order for local search to be fast. Unfortunately, it\nis difficult to build an RMI with a target error bound. Instead,\nthe model Flood uses is a piecewise linear model (PLM).\nA PLM models a CDF by partitioning a sorted list of values\nVinto slices, each of which is modeled by a linear segment. Let\nP(v)be the predicted index of value v∈V, determined by the\nsegment responsible for the slice containing v, and let D(v)\n6\n\nLearning Multi-dimensional Indexes SIGMOD’20, June 14-19, 2020, Portland, OR, USA\nbe the index of the first occurrence of v. We require that the\nlinear segments serve as a lower bound on the true CDF values,\ni.e.P(v)≤D(v), with the property that for every segment, the\naverage absolute error is less than a given threshold δ:\n1\n|V|Õ\nv∈V|D(v)−P(v)|≤δ\nThe lower bound property allows us to turn this condition into:\n1\n|V|Í\nv∈VD(v)−P(v)≤δ, which is much easier to achieve.\nFlood uses a greedy algorithm to partition Vinto slices:\nfor eachv∈Vin increasing order, it adds (v,D(v))to the\nsegment for the current slice. If the segment’s average error\nover the values in current slice exceeds δ, it begins a new slice.\nThe model records the smallest vin each slice and forms a\ncache-optimized B-Tree over those values. At inference time,\nFlood uses the B-Tree to find the appropriate segment and\nthus P(v). The parameter δencodes a tradeoff between size\nand speed (lower δis faster): see §7.8 for experiments tuning\nδand a comparison of the PLM to other methods.\n6 DISCUSSION\nTuning of traditional indexes. Existing multi-dimensional\nindexes strive for fewer hyperparameters, typically only a\npage size, to lower the overhead of tuning. However, fewer\nparameters also restricts the search space over which Flood\ncan optimize its layout, limiting the speedups it can achieve\nover existing indexes. Indeed, Fig. 8 demonstrates that sim-\nply tuning page size does not offer substantial performance\nimprovement. In contrast, Flood’s grid layout intentionally\noffers a larger number of parameters over which to optimize,\nall of which can be automatically tuned by Flood’s learning\nprocedure. As a result, Flood can customize the layout for a\nparticular query workload better than existing indexes.\nAlternatives to grids. Flood is a learning-enhanced version\nof a basic grid index, but many alternative techniques to di-\nvide a multi-dimensional space exist (R-Tree, k-d tree, Z-order,\netc.). We decided to use a grid structure for several reasons.\nFirst, it has a small space overhead: other multi-dimensional\nindexes use between 10MB and 1GB, but Flood’s grid uses\nless than 1kB (§7), leaving ample room to add per-cell mod-\nels. Second, the grid has low lookup latency, since it avoids\npointer chasing. On the TPC-H dataset in §7, Flood with flat-\ntening takes 0.46ms to identify relevant grid cells (excluding\nrefinement), while the k-d tree and hyperoctree take 8.9ms\n(20×) and 1.8ms ( 4×) to identify matching pages, respectively.\nThis trend is consistent across the datasets we evaluate on.\nZ-order based indexes have low lookup times but expose no\nobvious parameters that can be tuned for the query workload.\nNote that our flattening approach is necessary to keep scan\ntimes low by making sure the grid is not highly imbalanced.Nearest Neighbor Queries. Tree-based indexes that are\nused for geospatial data, such as k-d trees and R-trees, support\nk−nearest neighbor (kNN) queries. For example, a k-d tree\nlocates the page with the query point and checks adjacent\npages until all kneighbors are found. Flood can easily locate\nadjacent cells in its grid layout, allowing a similar kNN algo-\nrithm. However, since this paper does not focus on geospatial\nanalytics, we exclude kNN queries from our evaluation.\nMulti-dimensional CDFs. In §5.1, we mentioned that corre-\nlated dimensions can yield non-uniform data after flattening.\nTo address this issue, for each pair of correlated dimensions,\none could train a 2-dimensional joint CDF, or train a condi-\ntional CDF that creates a 1-D model for attribute A within each\ncolumn of attribute B. However, it is difficult to ensure that\na multi-dimensional RMI model gives monotonic predictions\nalong each dimension, which is a necessary property for par-\ntitioning points into columns; and conditional CDFs did not\nsignificantly improve performance in our benchmarks, but did\nsignificantly increase index size. Therefore, Flood does not use\nmulti-dimensional CDFs. Efficiently modeling correlations be-\ntween more dimensions is an active area of research [ 35,43].\n7 EVALUATION\nWe first describe the experimental setup and then present\nthe results of an in-depth experimental study that compares\nFlood with several other indexing methods on a variety of\ndatasets and workloads. Overall, this evaluation shows that:\n(1)Flood achieves optimality across the board : it is faster than,\nor on par with, every other index on the tested workloads.\nHowever, the next best index changes depending on the\ndataset. On our datasets, Flood is up to 187 ×faster than\na single-dimensional clustered column index, up to 62 ×\nfaster than a Grid File, up to 72 ×faster than a Z-order index,\nup to 250×faster than an UB-tree, up to 43 ×faster than a\nhyperoctree, and up to 48 ×faster than a k-d tree or R-tree.\n(2)Flood’s index can take up to 50×less space than the next\nfastest index.\n(3)Even though we did not optimize Flood for dynamic work-\nloads, Flood can train its layout and reorganize the records\nquickly for a new query distribution, typically in under\na minute for a 300 million record dataset.\n(4)Flood’s performance over baseline indexes improves with\nlarger datasets and higher selectivity queries.\n7.1 Implementation\nWe implement Flood in C++ on a custom column store that\nuses block-delta compression: in each column, the data is di-\nvided into consecutive blocks of 128 values, and each value\nis encoded as the delta to the minimum value in its block. Our\nencoding scheme allows constant-time element access and is\nable to compress the datasets used in our evaluation by 77%.\n7\n\nSIGMOD’20, June 14-19, 2020, Portland, OR, USA Vikram Nathan∗, Jialin Ding∗, Mohammad Alizadeh, Tim Kraska\nOur implementation uses 64-bit integer-valued attributes.\nAny string values are dictionary encoded prior to evaluation.\nFloating point values are typically limited to a fixed number\nof decimal points (e.g., 2 for price values). We scale all values\nby the smallest power of 10 that converts them to integers.\nOur column store implementation has two other optimiza-\ntions to improve scanning times:\n(1)If the range of data being scanned is exact , i.e., we are guar-\nanteed ahead of time that all elements within the range\nmatch the query filter, we skip checking each value against\nthe query filter. For common aggregations, e.g. COUNT , this\nremoves unnecessary accesses to the underlying data.\n(2)Similar to the idea of [ 24], our implementation allows in-\ndexes to speed up common aggregations like SUMby includ-\ning a column in which the ith value is the cumulative aggre-\ngation of all elements up to index i. In the case of an exact\nrange, the final aggregation result is simply the difference\nbetween the cumulative aggregations at the range end-\npoints. Note that this is not a data cube as we can support\narbitrary ranges instead of only pre-aggregated ranges.\nThese additions are meant to demonstrate that Flood can take\nadvantage of features that existing indexes enjoy. We show\nin §7.5 that these optimizations are not required for Flood: its\nperformance benefits are due primarily to the optimality of the\nlayout and not the details of the underlying implementation.\nOur random forest regression uses Python’s Scipy library [ 38].\nTo demonstrate that our column store implementation is\ncomparable to the existing state of the art, we benchmark\nour column store with MonetDB, an open-source column\nstore [ 28], by executing a query workload with full scans. Both\nMonetDB and our implementation were run single-threaded,\nwith identical bit widths for each attribute, and without com-\npression. Note that MonetDB does not support compression\non numerical columns. Averaged over 150 aggregation queries\non the TPC-H dataset (§7.3), our scan times are within 5% of\nMonetDB, showing that our column store implementation is\non par with existing systems.\n7.2 Baselines\nWe compare Flood to several other approaches, each imple-\nmented on the same column store and using the same opti-\nmizations where applicable:\n(1)Full Scan : Every point is visited, but only the columns\npresent in the query filter are accessed.\n(2)Clustered Single-Dimensional Index : Points are sorted by\nthe most selective dimension in the query workload, and\nwelearnaB-TreeoverthissortedcolumnusinganRMI[ 23].\nIf a query filter contains this dimension, we locate the end-\npoints using the RMI. Otherwise, we perform a full scan.\nSince a clustered index spends a vast majority of its time\nscanning instead of indexing, an RMI-based approach per-\nforms comparably to a standard B-tree: their total querytimes are within 1% of each other. Therefore, we show\nresults only for the RMI-based index.\n(3)Grid Files [30] index points by assigning them to cells in\na grid, similar to Flood. However, unlike Flood, Grid File\ncolumns are determined incrementally and do not opti-\nmize for a query workload. Additionally, points in multiple\nadjacent cells may be stored together in the same bucket\nand are not sorted. We found that the Grid Files algorithm\nin [30] requires a long time to construct on heavily skewed\ndata, so we omit results when it took over an hour.\n(4)TheZ-Order Index is a multidimensional index that orders\npoints by their Z-value [9]; contiguous chunks are grouped\ninto pages. Given a query, the index finds the smallest and\nlargest Z-value contained in the query rectangle and iter-\nates through each page with Z-values in this range.\n(5)TheUB-tree [36] also indexes points using their Z-values.\nA query finds the range of points to scan in the same man-\nner as the Z-Order Index. The UB-tree has the ability to\nskip forward to the next Z-value contained in the query\nrectangle, which avoids unnecessary scans.\n(6)TheHyperoctree [26] recursively subdivides space equally\nintohyperoctants(the d-dimensionalanalogto2-dimensional\nquadrants), until the number of points in each leaf is below\na predefined but tunable page size.\n(7)Thek-d tree recursively partitions space using the median\nvalue along each dimension, until the number of points\nin each leaf falls below the page size. The dimensions are\nselected in a round robin fashion, in order of selectivity.\n(8)TheR∗-Tree is a read-optimized variant of the R-Tree that\nis bulk loaded to optimize for read query performance. We\nbenchmark the R∗-Tree implementation from libspatialin-\ndex [ 12]. On larger datasets, the R∗-Tree was prone to out-\nof-memory errors and was not included in benchmarks.\nWhile some techniques are also entirely read-optimized like\nFlood (e.g., the Z-Order and R∗-Tree), others are inherently\nmore write-friendly (e.g., UB-tree); we still include them for\nthe sake of comparison while optimizing them for reads as\nmuch as possible (e.g., using dense cache-aligned pages). Ad-\nditional implementation details can be found in Appendix A.\nOur primary goal is to evaluate the performance of our\nmultidimensional index as a fundamental building block for\nimproving range request with predicates (e.g., filters) over one\nor more attributes. We therefore do not evaluate against other\nfull-fledged database systems or queries with joins, group-bys,\nor other complex query operators. While the impact of our\nmulti-dimensional clustered index on a full query workload\nfor an in-memory column-store database system would be\ninteresting, it requires major changes to any available open-\nsource column-store and is beyond the scope of this paper.\nHowever, it should be noted that that even in its current form,\nFlood could be directly used as a component to build useful\nservices, such as a multi-dimensional key-value store.\n8\n\nLearning Multi-dimensional Indexes SIGMOD’20, June 14-19, 2020, Portland, OR, USA\nsales tpc-h osm perfmon\nrecords 30M 300M 105M 230M\nqueries 1000 700 1000 800\ndimensions 6 7 6 6\nsize (GB) 1.44 16.8 5.04 11\nTable 1: Dataset and query characteristics.\nFor a fair comparison, all benchmarks are implemented\nusing a single thread without SIMD instructions. We excluded\nmultiple threads mainly because our baselines were not op-\ntimized for it. We discuss how Flood could take advantage of\nparallelism and concurrency in §8. All experiments are run\non an Ubuntu Linux machine with an Intel Core i9 3.6GHz\nCPU and 64GB RAM.\n7.3 Datasets\nWe evaluate indexes on three real-world and one synthetic\ndataset, summarized in Tab. 1. Queries are either real work-\nloads or synthesized for each dataset, and include a mix of\nrange filters and equality filters. The Sales dataset is a 6-\nattribute dataset and corresponding query workload drawn\ndirectly from a sales database at a commercial technology com-\npany. It was donated to us by a large corporation on the condi-\ntion of anonymity. The dataset consists of 30 million records,\nwith an anonymizing transformation applied to each dimen-\nsion. Each query in this workload was submitted by an analyst\nas part of report generation and analysis at the corporation.\nOur second real-world dataset, OSM , consists of all 105 mil-\nlion records catalogued by the OpenStreetMap [ 32] project in\nthe US Northeast. All elements contain 6 attributes, including\nan ID and timestamp, and 90% of the records contain GPS\ncoordinates. Our queries answer relevant analytics questions,\nsuch as “How many nodes were added to the database in a par-\nticular time interval?” and “How many buildings are in a given\nlat-lon rectangle?” Queries use between 1 and 3 dimensions,\nwith range filters on timestamp, latitude, and longtitude, and\nequality filters on type of record and landmark category. Each\nquery is scaled so that the average selectivity is 0.1%±0.013% .\nThe performance monitoring dataset Perfmon contains\nlogs of all machines managed by a major US university over\nthe course of a year. Our queries include filters over time,\nmachine name, CPU usage, memory usage, swap usage, and\nload average. The data in each dimension is non-uniform and\noften highly skewed. The original dataset has 23M records,\nbut we use a scaled dataset with 230M records.\nOur last dataset is TPC-H [42]. For our evaluation, we use\nonly the fact table, lineitem , with 300M records (scale factor\n50) and create queries by using filters commonly found in the\nTPC-H query workload, with filter ranges scaled so that the\naverage query selectivity is 0.1%. Our queries include filtersover ship date, receipt date, quantity, discount, order key, and\nsupplier key, and either perform a SUMorCOUNT aggregation.\nFor each dataset, we generate a train and test query work-\nload from the same distribution. Flood’s layout is optimized\non the training set, and we only report results on the test set.\n7.4 Results\nOverall Performance. We first benchmark how well Flood\ncan optimize for a query workload compared to baseline in-\ndexes that are also optimized for the same query workload.\nFig. 7 shows the query time for each optimized index on each\ndataset. Flood uses the layout learned using the algorithm in\n§4, while we tuned the baseline approaches as much as possi-\nble per workload (e.g., ordered dimensions by selectivity and\ntuned the page sizes). This represents the best case scenario\nfor the other indexes: that the database administrator had the\ntime and ability to tune the index parameters.\nOn three of the datasets, Flood achieves between 2.4×and\n3.3×speedup on query time compared to the next closest\nindex, and is always at least on-par, thus achieving the best\nperformance across-the-board . However, the next best system\nchanges across datasets. Thus, depending on the dataset and\nworkload, Flood can outperform each baseline by orders of\nmagnitude. For example, on the real-world sales dataset, Flood\nis at least 43×faster than each multi-dimensional index, but\nonly 3×faster than a clustered index. However, on the TPC-H\ndataset, Flood is 187 ×faster than a clustered index.\nOn every dataset, Fig. 8 shows that Flood beats the Pareto\nfrontier set by the other multi-dimensional indexes. In par-\nticular, even though Flood’s performance on OSM is on par\nwith the hyperoctree, its index size is more than 20×smaller.\nThe hyperoctree thus has to spend much more memory for\nits performance than Flood. Flood’s space overhead comes\npartially from the grid layout metadata, but mostly (over 95%)\nfrom the models of the sort attribute it maintains per cell.\nDifferent Workload Characteristics. In practical settings,\nit is unlikely that a database administrator will be able to man-\nually tune the index for every workload change. The ability\nof Flood to automatically configure its index for the current\nquery workload is thus a significant advantage. We measure\nthis advantage by tuning all indexes for the workloads in Fig. 7,\nand then changing the query workload characteristics to:\n(1)Single record filters, i.e. point lookups, using one or two\nID attributes, as commonly found in OLTP systems.\n(2)An OLAP workload, similar to the ones in Fig. 7, that\nanswer reasonable business questions about the underly-\ning dataset. Some types of queries occur more often than\nothers, skewing the workload.\n(3)An OLAP workload where each query type is equally\nlikely.\n9\n\nSIGMOD’20, June 14-19, 2020, Portland, OR, USA Vikram Nathan∗, Jialin Ding∗, Mohammad Alizadeh, Tim Kraska\nFull Scan Clustered R* Tree Z Order UB tree Hyperoctree K-d tree Grid File Flood101\n100101102Avg query time (ms)92.8\n0.4635.4\n10.938.1\n6.46 7.34 7.99\n0.13Sales: Query Time\nFull ScanClusteredZ OrderUB treeHyperoctreeK-d treeGrid FileFlood101102103Avg query time (ms)1.62k\n663\n34.875.3\n29.656.261.5\n12TPC-H: Query Time\nFull ScanClusteredR* TreeZ OrderUB treeHyperoctreeK-d treeFlood100101102Avg query time (ms)406\n206\n58.4\n5.5267.6\n1.072.84\n1.04OSM: Query Time\nFull Scan Clustered Z Order UB tree Hyperoctree K-d tree Flood101102103Avg query time (ms)843\n144\n9.66204\n41.7\n14.1\n3.17Perfmon: Query Time\nFigure 7: Query speed of Flood on all datasets. Flood’s index is trained automatically, while every other index\nis manually optimized for each workload to achieve the best performance. We excluded the R-tree for cases for\nwhich it ran out of memory. Note the log scale.\nFlood Z Order UB tree Hyperoctree K-d tree Grid File R* Tree Clustered Full Scan\n100kB 1MB 10MB 100MB 1GB\nIndex size101\n100101102Average query time (ms)\nSales: Query Time\n100kB 1MB 10MB 100MB 1GB 10GB\nIndex size101102103Average query time (ms)\nTPC-H: Query Time\n100kB 1MB 10MB 100MB 1GB 10GB\nIndex size100101102Average query time (ms)\nOSM: Query Time\n10kB 100kB 1MB 10MB 100MB 1GB 10GB\nIndex size101102103Average query time (ms)\nPerfmon: Query Time\nFigure 8: Flood (blue) sees faster performance with a smaller index, pushing the pareto frontier. Note the log scale.\nFlood Z Order UB tree Hyperoctree K-d tree Grid File\nFD MD OO O Ou O1 O2 ST0.011100Query Time (ms)TPC-H Predicates: Representative Workloads (Log Scale)\nFD OO O Ou O1 ST0.011100Query Time (ms)OSM: Representative Workloads (Log Scale)\nFigure 9: Flood and other indexes on workloads that have: fewer dimensions than the index (FD), as many\ndimensions as the index (MD), a skewed OLAP workload (O), a uniform OLAP workload (Ou), an OLTP workload\nover a single primary key (i.e., point lookups) (O1) and two keys (O2), a mixed OLTP +OLAP workload (OO), and\na single query type (ST). Note the log scale.\n(4)An equal split of workloads (1) and (2), i.e., combined OLTP\nand OLAP queries.\n(5)A workload with a single type of query, using the same\ndimensions with the same selectivities.\n(6)A workload with fewer dimensions (a strict subset) than\nindexed by the baseline indexes.\nFig. 9 shows the potential advantages Flood can achieve\nover more static alternatives. Flood consistently beats other in-\ndexes, though the magnitude of improvement depends on the\ndataset and query workload. For example, on TPC-H, Flood\nachieves a speedup of more than 20×on half the workloads,\nwhile on OSM, the median improvement is 2.2×.\nDynamic Query Workload Changes. Here, we demon-\nstrate how the performance of Flood varies over several ran-\ndom workloads, when the administrator does not tune theother indexes. We created 30 random workloads for the TPC-\nH dataset. Each workload runs for one hour and consists of at\nmost 10 distinct query types, and each query type in turn con-\nsists of up to 6 dimensions, both chosen uniformly at random.\nThe selectivities of each dimension are chosen randomly, with\nthe constraint that all queries have an average total selectivity\nof around 0.1% and are more selective on key attributes.\nFig. 10 shows the results over time with Flood being the\nonly index that changes from one hour to the next (all others\nwere kept fixed and tuned for the workload in Fig. 7). At the\nstart of each hour, a new query workload is introduced, and\nwe trigger Flood’s retraining. During the retraining phase,\nwhich we assume happens on a separate instance, Flood runs\nthe new queries on its old layout, causing brief performance\ndegradation and producing a spike at the start of each hour.\nIt only switches to the new, more performant layout once\n10\n\nLearning Multi-dimensional Indexes SIGMOD’20, June 14-19, 2020, Portland, OR, USA\nFlood Z Order UB tree Hyperoctree K-d tree Grid File\n0 5 10 15 20 25 30\nTime (hours)1101001000Query Time (ms)TPC-H Random Queries (Log Scale)\nFigure 10: Flood vs. other indexes on 30 random query workloads, each for one hour. At the start of each hour,\nFlood’s performance degrades, since it is not trained for the new workload; however, it recovers in 5 minutes on\naverage once the layout is re-learned, and beats the next best index by 5×at the median. Note the log scale.\nsales tpc-h osm perfmon\nSO TPS ST IT TT SO TPS ST IT TT SO TPS ST IT TT SO TPS ST IT TT\nFull Scan 644 3.09 92.6 0 92.8 965 5.27 1580 0 1620 1090 3.83 403 0 406 990 3.52 833 0 843\nClustered 3.18 3.09 0.462 6.76e-4 0.463 447 4.71 655 7.15e-4 662 478 4.50 207 8.92e-4 208 186 3.32 144 1.20e-3 144\nZ Order 57.9 4.00 10.9 0.0161 10.9 14.9 7.63 34.80 0.0267 34.8 6.85 8.37 5.5 0.0164 5.52 9.08 4.42 9.64 0.0146 9.66\nUB tree 55.7 14.5 38.0 0.0175 38.1 15.3 16.1 75.2 0.0284 75.3 22.5 31.3 67.5 0.0171 67.6 38.8 21.9 204 0.0120 204\nHyperoctree 38.8 3.34 6.11 0.353 6.46 20.8 4.38 27.8 1.77 29.6 2.36 3.59 0.812 0.253 1.07 33.8 3.47 28.2 13.4 41.7\nK-d tree 38.2 3.40 6.13 1.21 7.34 36.4 4.26 47.3 8.85 56.2 6.60 3.51 2.22 0.611 2.84 15.7 3.07 11.6 2.51 14.1\nGrid File 37.4 4.53 7.99 0.0594 7.99 36.9 5.28 59.5 1.88 61.5 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A\nFlood 1.82 1.26 0.108 0.0182 0.128 5.90 5.53 9.96 2.02 12.0 3.13 2.39 0.717 0.328 1.05 4.26 2.77 2.84 0.327 3.17\nTable 2: Performance breakdown: scan overhead (SO), i.e. the ratio between points scanned and result size; average\ntime spent scanning per scanned point, in nanoseconds (TPS); average time spent scanning, in milliseconds (ST); av-\nerage time spent indexing (for Flood this includes projection and refinement), in milliseconds (IT); total query time,\nin milliseconds (TT). SO ×TPS is proportional to ST, and ST+IT+ ϵ=TT (a small fraction of query time is spent neither\nscanning nor indexing). R∗-tree omitted because instrumentation for collecting statistics was inadequate in [12].\nSimple Grid +Sort Dim +Flattening +Learning0.00.20.40.6Avg query time (ms)Sales: Query Time\nSimple Grid +Sort Dim +Flattening +Learning100101102Avg query time (ms)TPC-H: Query Time\nSimple Grid +Sort Dim +Flattening +Learning100101102Avg query time (ms)OSM: Query Time\nSimple Grid +Sort Dim +Flattening +Learning100101102Avg query time (ms)Perfmon: Query Time\nFigure 11: Flattening and learning help Flood achieve low query times but is workload dependent.\nretraining is finished. Flood outperforms all other indexes,\nshowing a median improvement of more than 5×over the clos-\nest competitor, with 30% of queries achieving more than a 10×\nspeedup. The results suggest that Flood is able to generalize\nwell by adapting to new and unforeseen workloads.\nFig. 10 also highlights the importance of learning from the\nquery workload. When transitioning to the next query work-\nload, Flood’s performance often worsens, since the current lay-\nout is usually not suitable for the new workload. Re-learning\nthe layout based on the new workload lowers query time back\nlower than other indexes. Learning a layout is therefore (a)\neffective at adapting to new query workloads and (b) crucial\nto Flood’s performance improvement over other indexes. We\nleave the detection of workload changes to future work (§8).\nWhile the results are encouraging, it is also important to\nconsider the time it takes to adjust to a new query workload.\nFlood takes at most around 1 minute to adapt to a new queryworkload, but it more than makes up for this adjustment pe-\nriod through improved performance on the subsequent work-\nload. We evaluate index creation time in further detail in §7.7.\nPerformance Breakdown. Where does Flood’s advantage\noverbaselineindexescomefrom?Welookatthe scan overhead ,\nthe ratio of total points scanned by the index to points match-\ning the query. The scan overhead is implementation agnostic:\nit relies neither on the machine nor on the implementation of\nthe underlying column store. A high scan overhead suggests\nthat the index wastes time scanning unnecessary points. Since\nall indexes spend the vast majority of their time scanning, the\nscan overhead is a good proxy for overall query performance.\nTab. 2 shows that Flood achieves the lowest scan overhead\n(SO) on three out of four datasets , which confirms that Flood’s\noptimized layout is able to better isolate records that match a\nquery filter. Additionally, Flood usually spends less time per\nscanned point (TPS) because Flood avoids accessing the sort\n11\n\nSIGMOD’20, June 14-19, 2020, Portland, OR, USA Vikram Nathan∗, Jialin Ding∗, Mohammad Alizadeh, Tim Kraska\nFlood\nZ OrderUB tree\nHyperoctreeK-d tree\nGrid FileClustered\nFull Scan\n106107108\nDataset Size (num records)101\n100101102103Avg query time (ms)\nTPC-H: Varying Dataset Size\n105\n104\n103\n102\n101\nQuery Selectivity101102103Avg query time (ms)\nTPC-H: Varying Query Selectivity\nFigure 12: Flood’s performance scales both with\ndataset size and query selectivity. The dashed blue line\ndepicts what linear scaling would like like.\ndimension. As a result, Flood consistently achieves the lowest\nscan time (ST), which is proportional to the product of SO\nand TPS. This more than makes up for Flood’s higher index\ntime (IT), which includes the time to project and refine. In-\ndexes based on Z-order incur the cost of computing Z-values\nand thus have a higher time per scanned point. Tree-based\nindexes have the highest index time due to the overhead of\ntree traversal.\nWhich of Flood’s components is responsible for its perfor-\nmance? Fig. 11 shows the incremental benefit of (1) sorting the\nlast indexed dimension instead of creating a d-dimensional\nhistogram, (2) flattening the data instead of using columns of\nfixed width, and (3) adapting to the query workload using the\ntraining procedure from §4. The baseline system is a “Simple\nGrid” on all ddimensions, with the number of columns in\neach dimension proportional to that dimension’s selectivity.\nSorting by the last dimension offers marginal benefits: it al-\nlows more columns to be allocated to the first d−1dimensions,\nincreasing the resolution of the index along those dimensions\nwithout increasing the total number of cells. The biggest im-\nprovementscomefromflatteningandlearningthelayoutfrom\nthe queries. However, the effect of each varies across datasets.\nFlattening benefits OSM and Perfmon since both datasets have\nheavily skewed attributes. Since Sales and TPC-H data are\nfairly uniform, using a non-flattened layout performs equally\nwell. Finally, learning from queries provides major perfor-\nmance gains on all datasets, corroborating results from Fig. 10.\n7.5 Scalability\nDataset Size. To show how Flood scales with dataset size,\nwe sample records from the TPCH dataset to create smaller\ndatasets. We train and evaluate these smaller datasets with\nthe same train and test workloads as the full dataset. Fig. 12a\nshows that the query time of Flood grows sub-linearly. As\nthe number of records grows, the layout learned by Flood\nuses more columns in each dimension, which results in more\ncells. The extra overhead incurred by processing more cells\nis outweighed by the benefit of lowering scan overhead.\nFlood\nZ OrderUB tree\nHyperoctreeK-d tree\nGrid FileClustered\nFull Scan\n5 10 15\nNumber of dimensions101\n100101102103Query Time (ms)\nSynthetic: Scaling Dimensions\n5 10 15\nNumber of dimensions100101102103Query Time Ratio\nSynthetic: Scaling DimensionsFigure 13: (a): Query time as number of dataset dimen-\nsions varies. (b): Ratio of the index’s query time to the\ntime for a full scan.\nQuery Selectivity. To show how Flood performs at different\nquery selectivities, we scale the filter ranges of the queries\nin the original TPC-H workloads up and down equally in\neach dimension in order to achieve between 0.001% and 10%\nselectivity. Fig. 12b shows that Flood performs well at all se-\nlectivities. The performance benefit of Flood is less apparent\nat 10% selectivity because all indexes are able to incur lower\nscan overhead when more points fall into the query rectangle.\nNumber of Dimensions. To show how Flood scales with di-\nmensions, we create synthetic d-dimensional datasets (d≤18)\nwith 100 million records whose values in each dimension are\ndistributed uniformly at random. For each dataset, we create\na query workload of 1000 queries. The number of dimensions\nfiltered in the queries varies uniformly from 1 to d. If a query\nhas a filter on kdimensions, they are the first kdimensions\nin the dataset. For each query, the filter selectivity along each\ndimension is the same and is set so that the overall selectivity\nis 0.1%. For example, for a 2-dimensional dataset, 500 queries\nwill select 0.1% of the domain of dimension 1, and 500 queries\nwill select around 3.2% of the domains of dimensions 1 and 2.\nFig. 13a shows that Flood continues to outperform the base-\nline indexes at higher dimensions. Note that the clustered\nindex’s relative performance also improves, since the base-\nline indexes spend resources on dimensions which are not\nfrequently filtered on. By contrast, Flood learns which dimen-\nsions to prioritize; for example, on the higher-dimensional\ndatasets, Flood chooses not to include the least frequently\nfiltered dimensions in the index. Yet, Flood is also impacted\nby the curse of dimensionality (Fig. 13b), which depicts the\nspeedup of each index compared to a full scan. However,\nFlood can dampen the effect of the curse through its self-\noptimization, degrading more slowly than other indexes.\n7.6 The Cost Model\nFindingtheOptimum. Choosing an optimallayout requires\nbalancing two competing factors: reducing the latency of lo-\ncating both the relevant cells and physical index ranges within\neach cell (index time), and reducing the scan time by lowering\n12\n\nLearning Multi-dimensional Indexes SIGMOD’20, June 14-19, 2020, Portland, OR, USA\n105106107\nNumber of cells01020Time per query (ms)\nQuery time\nScan timeIndex time\nLearned optimum\n105106107\nNumber of cells51015Time (ns) or Number\nScan overhead Time per scan\nFigure 14: TPC-H: Adding cells reduces scan overhead\nbut incurs a higher indexing cost and worse locality.\nLayout learned for\nsales tpc-h osm perfmonModels\ntrained\nonsales 0.128 10.8 (-8%) 0.975 (-7%) 3.49 (+17%)\ntpch 0.132 (+3%) 11.7 0.986 (-6%) 3.18 (+6%)\nosm 0.134 (+5%) 11.7 (+0%) 1.05 3.14 (+5%)\nperfmon 0.137 (+7%) 11.6 (-1%) 0.964 (-8%) 2.99\nTable 3: Query time (ms) when layouts are learned\nusing cost models trained on different examples.\nscan overhead. Fig. 14a illustrates this trade-off as the size of\nthe grid changes (we fix a layout and then scale the columns in\neach dimension proportionally): as the number of cells grows,\nscan time decreases because scan overhead decreases, but\nindex time increases because there are more cells to process.\nFlood’s cost model must be able to find the appropriate\ntrade-off between scan time and index time. Indeed, Fig. 14a\nshows that Flood finds the number of cells that minimizes the\ntotal query time (red star). Note that we show the cost surface\nalong only a single degree of freedom for visual clarity.\nRobustnessofthemodel. As§4.1.1describes,ourcostmodel\nneeds to learn the weights {wp,wl,wr,ws}, here referred to as\ncalibration. This calibration should happen once per dataset\nand machine. On our server, it took around 10 minutes, most\nof which was spent generating training examples.\nHowever, maybe surprisingly, the weights are quite robust\nto the data itself, so the cost model does not need to be re-\ntrained for every dataset. To show this, we trained our cost\nmodel on each of our four datasets, used each model to learn\nlayouts for all four datasets, and then ran all 16 layouts on the\ncorresponding query workloads. Tab. 3 shows that, no matter\nwhich dataset is used to learn the layout, the query times from\nthe resulting layouts are similar, often with less than a 10% dif-\nference between them. Therefore, Flood can use the same cost\nmodel regardless of changes to the dataset or query workload.\nThis makes calibration a one-time cost of 10 minutes.\n7.7 Index Creation\nTab. 4 shows the time to create each index. We separate index\ncreation time for Flood into learning time, which is the time\ntaken to learn the layout (§4.2); and loading time, which is the\ntime to build the primary index. The reported learning timessales tpc-h osm perfmon\nFlood Learning 10.3 33.4 44.5 33.3\nFlood Loading 4.12 29.6 8.03 22.0\nFlood Total 14.4 63.0 52.5 55.3\nClustered 2.11 16.2 4.85 11.6\nZ Order 7.82 86.7 24.9 72.6\nUB tree 8.28 81.9 26.0 69.5\nHyperoctree 2.47 42.2 31.4 54.8\nK-d tree 8.45 140 36.9 250\nGrid File 10.6 121 N/A N/A\nR* tree 259 N/A 1340 N/A\nTable 4: Index Creation Time in Seconds\nuse sampling of the dataset and query workload, described\nnext. The total index creation time of Flood is competitive\nwith the creation time of the baseline indexes.\nSampling records. Optimizing the layout using the entire\ndataset and query workload can take prohibitively long and\ndoes not scale well to larger datasets and workloads. How-\never, Flood can reduce learning time without a significant\nperformance loss by sampling the data. Fig. 15 shows that\neven when estimating features with a sample of only 0.01–1%,\nFlood maintains low query times. This is because the main\npurpose of the sample is to estimate the number of records\nscanned per query. Since our query selectivities are around\n0.1% or higher, a sample of 1% records is sufficiently accurate.\nYet, this alone is not sufficient to match the creation time of the\nhyperoctree, the fastest of our multi-dimensional baselines.\nSampling queries. Sampling the query workload can reduce\nFlood’s creation time even further. Here we conservatively use\na data sample size of 100k records and vary the query sample\nsize. As Fig. 16 shows, Flood maintains low query times when\nusing only 5% of queries. This is because the query workloads\ncontain limited number of query types. Since queries within\neach type have similar characteristics with respect to selec-\ntivity and which dimensions are filtered, Flood only requires\na few queries of each type to learn a good layout. However,\nthe variance in performance increases as the query work-\nload sample size decreases. With both optimizations (data and\nquery samples), Flood achieves a learning time on par with the\nhyperoctree creation time without sacrificing performance.\n7.8 Per-cell Models\nIn §5.2, we discuss CDF models to accelerate the location of\nphysical indexes along the sort dimension. Since these CDFs\nare evaluated twice for each visited cell (beginning and end of\nthe range), small speedups in lookup time may be noticeable\non overall query time. Fig. 17a benchmarks the lookup time,\nincluding inference and an exponential search rectification\nphase, of three options we considered: the piecewise-linear\nmodel(PLM,ourapproach),thelearnedB-tree[ 23],andbinary\n13\n\nSIGMOD’20, June 14-19, 2020, Portland, OR, USA Vikram Nathan∗, Jialin Ding∗, Mohammad Alizadeh, Tim Kraska\nQuery time Learning time Hyperoctree creation time\n103105107\nDataset sample size0.00.10.20.30.4Query Time (ms)Sales\n050010001500\nLearning time (s)\n103104105106107\nDataset sample size0510Query Time (ms)TPC-H\n0200400600800\nLearning time (s)\n103105107\nDataset sample size0.00.51.0Query Time (ms)OSM\n0200400\nLearning time (s)\n103104105106107\nDataset sample size01234Query Time (ms)Perfmon\n050010001500\nLearning time (s)\nFigure 15: Learning time and resulting query time when sampling the dataset over several trials. One standard\ndeviation from the mean is shaded. For comparison, we show the index creation time for the hyperoctree.\nQuery time Learning time Hyperoctree creation time\n101102103\nQuery workload sample size0.000.050.100.15Query Time (ms)Sales\n050100150\nLearning time (s)\n101102\nQuery workload sample size020406080Query Time (ms)TPC-H\n0100200300\nLearning time (s)\n101102103\nQuery workload sample size01020Query Time (ms)OSM\n050100150200\nLearning time (s)\n101102\nQuery workload sample size0510Query Time (ms)Perfmon\n050100150200\nLearning time (s)\nFigure 16: Learning time and resulting query time when sampling the queries over several trials. One standard\ndeviation from the mean is shaded. For comparison, we show the index creation time for the hyperoctree.\n30k 6M 105M 500k 10M0100200300400500600Avg Lookup Time (ns)\nOSM StaggeredPLM\nRMI\nBinary\n0 200 400 600 800\nSize (kB)200225250275300325350375400Lookup time (ns)\n=50\nFigure 17: (a) A comparison of three per-cell CDF\nmodels on two 1-D datasets. (b) The size-speed tradeoff\nfor the PLM, with our configuration marked.\nsearch. We used real data (timestamps from the OSM dataset)\nand synthetic staggered uniform data (data is uniform over\nidentically sized but disjoint intervals), with query points sam-\npled from the dataset. The PLM and RMI perform comparably,\nand both beat binary search by up to 4×on these datasets. We\nuse the PLM since it requires only a single tuning parame-\nterδ, which encodes the tradeoff between accuracy and size\n(Fig. 17b). By contrast, the learned B-tree [ 23] requires exten-\nsive tuning of the number of experts per layer. The choice of δ\ndependsonhowmuchtheadministratorprioritizesspeedover\nspace: Fig. 17b shows that δ=50strikes a reasonable balance.\n8 FUTURE WORK\nShifting workloads. Flood can quickly adapt to workload\nchanges but cannot detect when the query distribution has\nchanged sufficiently to merit a new layout. To do this, Flood\ncould periodically evaluate the cost (§4) of the current layouton queries over a recent time window. If the cost exceeds a\nthreshold, Flood can replace the layout.\nAdditionally, Flood is completely rebuilt for each new work-\nload. However, Flood could also be incrementally adjusted,\ne.g. by coalescing adjacent columns or splitting a column, or\nby incorporating aspects of incremental layout creation from\ndatabase cracking [16], to avoid rebuilding the index.\nInsertions. Flood currently only supports read-only work-\nloads. To support insertions, each cell could maintain gaps,\nsimilar to the fill factor of a B+Tree. It could also maintain a\ndelta index [ 39] in which updates are buffered and periodically\nmerged into the data store, similar to Bigtable [2].\nConcurrency and parallelism. Flood is currently single-\nthreaded, but it can be extended to take advantage of con-\ncurrency and parallelism. Different cells can be refined and\nscanned simultaneously; within a cell, records can be scanned\nin parallel, allowing Flood to benefit from multithreading.\nAdditionally, since Flood stores each column in the column\nstore as a dense array, it can also take advantage of SIMD.\n9 CONCLUSION\nDespite the shift of OLAP workloads towards in-memory\ndatabases, state-of-the-art systems have failed to take advan-\ntage of multi-dimensional indexes to accelerate their queries.\nMany instead opt for simple 1-D clustered indexes with bulky\nsecondary indexes that waste space. We design a new multi-\ndimensional index Flood with two properties. First, it serves\nas the primary index and is used as the storage order for\nunderlying data. Second, it is jointly optimized using both\n14\n\nLearning Multi-dimensional Indexes SIGMOD’20, June 14-19, 2020, Portland, OR, USA\nthe underlying data and query workloads. Our approach out-\nperforms existing clustered indexes by 30−400×. Likewise,\nlearning the index layout from the query workload allows\nFlood to beat optimally tuned spatial indexes, while using a\nfraction of the space. Our results suggest that learned primary\nmulti-dimensional indexes offer a significant performance im-\nprovement over state-of-the-art approaches and can serve as\nuseful building blocks in larger in-memory database systems.\nREFERENCES\n[1]Amazon AWS. 2016. Amazon Redshift Engineering’s Ad-\nvanced Table Design Playbook: Compound and Interleaved Sort\nKeys. https://aws.amazon.com/blogs/big-data/amazon-redshift-\nengineerings-advanced-table-design-playbook-compound-and-\ninterleaved-sort-keys/.\n[2]Fay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C. Hsieh,\nDeborah A. Wallach, Mike Burrows, Tushar Chandra, Andrew Fikes,\nand Robert E. Gruber. 2008. Bigtable: A Distributed Storage System\nfor Structured Data. ACM Trans. Comput. Syst. 26, 2, Article 4 (June\n2008), 26 pages. https://doi.org/10.1145/1365815.1365816\n[3]Surajit Chaudhuri and Vivek Narasayya. 1997. An Efficient, Cost-\nDriven Index Selection Tool for Microsoft SQL Server. In Proceedings\nof the VLDB Endowment . VLDB Endowment.\n[4]Databricks Engineering Blog. [n.d.]. Processing Petabytes of Data\nin Seconds with Databricks Delta. https://databricks.com/blog/2018/\n07/31/processing-petabytes-of-data-in-seconds-with-databricks-\ndelta.html.\n[5]Jialin Ding, Umar Farooq Minhas, Hantian Zhang, Yinan Li, Chi Wang,\nBadrish Chandramouli, Johannes Gehrke, Donald Kossmann, and\nDavid Lomet. 2019. ALEX: An Updatable Adaptive Learned Index.\narXiv:arXiv:1905.08898\n[6]Andrew Lamb et al. 2012. The Vertica Analytic Database: C-Store 7\nYears Later. In Proceedings of the VLDB Endowment . VLDB Endowment.\n[7]Mike Stonebraker et al. 2005. C-Store: A Column-oriented DBMS. In\nProceedings of the 31st VLDB Conference . VLDB Endowment.\n[8]Exasol. [n.d.]. The World’s Fastest In-Memory Analytic Database.\nhttps://www.exasol.com/en/community/resources/resource/worlds-\nfastest-analytic-database/.\n[9]Volker Gaede and Oliver Günther. 1998. Multidimensional access\nmethods. ACM Computing Surveys (CSUR) 30 (1998), 170–231. Issue 2.\n[10] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo\nFonseca, and Tim Kraska. 2018. A-Tree: A Bounded Approximate\nIndex Structure. CoRR abs/1801.10207 (2018). arXiv:1801.10207\nhttp://arxiv.org/abs/1801.10207\n[11] Jim Gray, Surajit Chaudhuri, Adam Bosworth, Andrew Layman, Don\nReichart, Murali Venkatrao, Frank Pellow, and Hamid Pirahesh. 1997.\nData Cube: A Relational Aggregation Operator Generalizing Group-By,\nCross-Tab, and Sub-Totals. Data Min. Knowl. Discov. 1, 1 (Jan. 1997),\n29–53. https://doi.org/10.1023/A:1009726021843\n[12] Marios Hadjieleftheriou. 2014. libspatialindex. https:\n//libspatialindex.org/.\n[13] Klaus Hinrichs. 1985. Implementation of the Grid File: De-\nsign Concepts and Experience. BIT 25, 4 (Dec. 1985), 569–592.\nhttps://doi.org/10.1007/BF01936137\n[14] Andreas Hutflesz, Hans-Werner Six, and Peter Widmayer. 1988.\nTwin Grid Files: Space Optimizing Access Schemes. In Proceedings\nof the 1988 ACM SIGMOD International Conference on Manage-\nment of Data (SIGMOD ’88) . ACM, New York, NY, USA, 183–190.\nhttps://doi.org/10.1145/50202.50222[15] IBM. [n.d.]. The Spatial Index. https://www.ibm.com/support/\nknowledgecenter/SSGU8G_12.1.0/com.ibm.spatial.doc/ids_spat_024.\nhtm.\n[16] Stratos Idreos, Martin L. Kersten, and Stefan Manegold. 2007. Database\nCracking. Conference on Innovative Data Systems Research (CIDR).\n[17] Stratos Idreos, Martin L. Kersten, and Stefan Manegold. 2009. Self-\norganizing Tuple Reconstruction in Column-stores. In SIGMOD . ACM.\n[18] Stratos Idreos, Konstantinos Zoumpatianos, Brian Hentschel, Michael S.\nKester, and Demi Guo. 2018. The Data Calculator: Data Structure Design\nand Cost Synthesis From First Principles, and Learned Cost Models.\nInACM SIGMOD International Conference on Management of Data .\n[19] Intel Corporation. 2014. Scaling Data Capacity for SAP HANA with\nFujitsu PRIMERGY/PRIMEQUEST Servers . Technical Report.\n[20] Irfan Khan. 2012. Falling RAM prices drive in-memory database surge.\nhttps://www.itworld.com/article/2718428/falling-ram-prices-drive-\nin-memory-database-surge.html.\n[21] Hideaki Kimura, George Huo, Alexander Rasin, Samuel Madden, and\nStanley B. Zdonik. 2009. Correlation Maps: A Compressed Access\nMethod for Exploiting Soft Functional Dependencies. Proc. VLDB Endow.\n2, 1 (Aug. 2009), 1222–1233. https://doi.org/10.14778/1687627.1687765\n[22] Tim Kraska, Mohammad Alizadeh, Alex Beutel, Ed H. Chi, Ani Kristo,\nGuillaume Leclerc, Samuel Madden, Hongzi Mao, and Vikram Nathan.\n2019. SageDB: A Learned Database System. In CIDR 2019, 9th Biennial\nConference on Innovative Data Systems Research, Asilomar, CA, USA,\nJanuary 13-16, 2019, Online Proceedings .\n[23] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis.\n2018. The Case for Learned Index Structures. In Proceedings of the 2018\nInternational Conference on Management of Data (SIGMOD) . ACM.\n[24] Iosif Lazaridis and Sharad Mehrotra. 2001. Progressive Approxi-\nmate Aggregate Queries with a Multi-resolution Tree Structure. In\nProceedings of the 2001 ACM SIGMOD International Conference on\nManagement of Data (SIGMOD ’01) . ACM, New York, NY, USA, 401–412.\nhttps://doi.org/10.1145/375663.375718\n[25] Lin Ma, Dana Van Aken, Amed Hefny, Gustavo Mezerhane, Andrew\nPavlo, and Geoffrey J. Gordon. 2018. Query-based Workload Forecasting\nfor Self-Driving Database Management Systems. In SIGMOD . ACM.\n[26] Donald Meagher. 1980. Octree Encoding: A New Technique for the\nRepresentation, Manipulation and Display of Arbitrary 3-D Objects by\nComputer . Technical Report.\n[27] Microsoft SQL Server. 2016. Spatial Indexes Overview. https:\n//docs.microsoft.com/en-us/sql/relational-databases/spatial/spatial-\nindexes-overview?view=sql-server-2017.\n[28] MonetDB. 2018. monetdb. https://www.monetdb.org/.\n[29] G. M. Morton. 1966. A computer Oriented Geodetic Data Base; and a\nNew Technique in File Sequencing (PDF) . Technical Report. IBM.\n[30] J. Nievergelt, Hans Hinterberger, and Kenneth C. Sevcik. 1984.\nThe Grid File: An Adaptable, Symmetric Multikey File Struc-\nture. ACM Trans. Database Syst. 9, 1 (March 1984), 38–71.\nhttps://doi.org/10.1145/348.318586\n[31] Beng Chin Ooi, Ron Sacks-davis, and Jiawei Han. 2019. Indexing in\nSpatial Databases.\n[32] OpenStreetMap contributors. 2019. US Northeast dump obtained from\nhttps://download.geofabrik.de/. https://www.openstreetmap.org.\n[33] Oracle Database Data Warehousing Guide. 2017. Attribute Clustering.\nhttps://docs.oracle.com/database/121/DWHSG/attcluster.htm.\n[34] Oracle, Inc. [n.d.]. Oracle Database In-Memory. https:\n//www.oracle.com/database/technologies/in-memory.html.\n[35] Yongjoo Park, Shucheng Zhong, and Barzan Mozafari. 2019. QuickSel:\nQuick Selectivity Learning with Mixture Models. In SIGMOD . ACM.\n[36] Frank Ramsak1, Volker Markl, Robert Fenk, Martin Zirkel, Klaus\nElhardt, and Rudolf Bayer. 2000. Integrating the UB-Tree into a\nDatabase System Kernel . In Proceedings of the 26th International\n15\n\nSIGMOD’20, June 14-19, 2020, Portland, OR, USA Vikram Nathan∗, Jialin Ding∗, Mohammad Alizadeh, Tim Kraska\nConference on Very Large Databases . VLDB Endowment.\n[37] Felix Martin Schuhknecht, Alekh Jindal, and Jens Dittrich. 2013. The\nUncracked Pieces in Database Cracking. Proc. VLDB Endow. 7, 2 (Oct.\n2013), 97–108. https://doi.org/10.14778/2732228.2732229\n[38] Scipy.org. [n.d.]. scipy.optimize.basinhopping. https:\n//docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.optimize.\nbasinhopping.html.\n[39] Dennis G. Severance and Guy M. Lohman. 1976. Differ-\nential Files: Their Application to the Maintenance of Large\nDatabases. ACM Trans. Database Syst. 1, 3 (Sept. 1976), 256–267.\nhttps://doi.org/10.1145/320473.320484\n[40] Hari Singh and Seema Bawa. 2017. A Survey of Traditional and\nMapReduceBased Spatial Query Processing Approaches. SIGMOD Rec.\n46, 2 (Sept. 2017), 18–29. https://doi.org/10.1145/3137586.3137590\n[41] Markku Tamminen. 1982. The extendible cell method for closest point\nproblems. BIT22 (01 1982), 27–41. https://doi.org/10.1007/BF01934393\n[42] TPC. 2019. TPC-H. http://www.tpc.org/tpch/.\n[43] Kostas Tzoumas, Amol Deshpande, and Christian S. Jensen. 2011.\nLightweight Graphical Models for Selectivity Estimation Without\nIndependence Assumptions. In Proceedings of the VLDB Endowment .\nVLDB Endowment.\n[44] Gary Valentin, Michael Zuliani, Daniel C. Zilio, Guy Lohman, and\nAlan Skelley. 2000. DB2 Advisor: An Optimizer Smart Enough to\nRecommend its own Indexes. In Proceedings of the 16th International\nConference on Data Engineering . IEEE.\n[45] Yingjun Wu, Jia Yu, Yuanyuan Tian, Richard Sidle, and Ronald Barber.\n2019. Designing Succinct Secondary Indexing Mechanism by Exploiting\nColumn Correlations. In Proceedings of the 2019 International Conference\non Management of Data, SIGMOD Conference 2019, Amsterdam, The\nNetherlands, June 30 - July 5, 2019. 1223–1240.\n[46] Zack Slayton. 2017. Z-Order Indexing for Multifaceted Queries in\nAmazon DynamoDB. https://aws.amazon.com/blogs/database/z-order-\nindexing-for-multifaceted-queries-in-amazon-dynamodb-part-1/.\nA INDEX IMPLEMENTATION DETAILS\nThis section provides additional details for the indexes de-\nscribed in §7.2 that we implemented ourselves. When query-\ning an index, the user provides two arguments: (1) the start\nand end value of the filter range in each dimension (set to\nnegative and positive infinity if the dimension is not filtered\nin the query), and (2) a Visitor object which will accumulate\nthe statistic of the aggregation. All of our experiments are per-\nformed on aggregation queries. Indexes only scan the columns\nfor dimensions that appear in the query filter. The baseline\nindexes we implemented:\n•Clustered Single-Dimensional Index: We use an\nRMI with three layers, where all models are linear mod-\nels. Models in the non-leaf layers are linear spline mod-\nels to ensure that the models accessed in the following\nlayer are monotonic; the models in the leaf layer are\nlinear regressions. The numbers of experts in each layer\nare1,√n,andn, respectively, with ntuned to minimize\nquery time on the target workload.\n•Grid File: The d-dimensional space is divided into\nblocks by a grid (each block is one grid cell). Multiple ad-\njacent blocks constitute a bucket . All points in a bucketare stored contiguously and not sorted: if a record in\na bucket needs to be accessed, the entire bucket must\nbe scanned. The grid is built incrementally, starting\nwith a single block that contains the entire space. Each\npoint is added to its corresponding bucket; once the\nnumber of points in a bucket hits a user-defined page\nsize, that bucket is split to form a new bucket by either\n(1) splitting points along an existing block boundary, if\nit exists in any dimension, or (2) adding a grid column\n(and therefore more blocks) that divides the existing\nbucket at its midpoint along a particular dimension.\nThe dimension along which the block is split is cycled\nthrough in a round robin fashion. The page size is tuned\nto minimize query time on the target workload. When\na query arrives, the grid file scans all the buckets that\nintersect the query rectangle.\n•Z-Order Index: We use 64-bit Z-order values. When\nindexing ddimensions, we compute the Z-order value\nfor a point by taking the first ⌊64/d⌋bits of each dimen-\nsion’s value and interleaving them, ordered by selec-\ntivity (e.g., the most selective dimension’s LSB is the\nZ-order value’s LSB). We order points by their Z-order\nvalue and group contiguous chunks into pages. For each\npage, we store the min and max value in each dimension\nfor points in the page. Given a query, the index finds\nthe smallest and largest Z-order value contained in the\nquery rectangle (conceptually the bottom-left and top-\nright vertices of the query rectangle), uses binary search\nto find the physical indexes that correspond to those\nZ-order values, and iterates through every page that\nfalls between those physical indexes. The points in a\npage are only scanned if the metadata min/max values\nindicate that it is possible for points in the page to match\nthe query filter (i.e., we only scan a page if the rectangle\nformed by the page’s min/max values intersects with\nthe query rectangle).\n•UB-tree: Z-order values are computed in the same way\nas the Z-Order Index. We order points by their Z-order\nvalue and group contiguous chunks into pages. For each\npage, we store the minimum Z-order value contained in\nthatpage.Givenaquery,theindexfindsthesmallestand\nlargest Z-order value contained in the query rectangle\n(conceptually the bottom-left and top-right vertices of\nthe query rectangle), uses binary search to find the phys-\nical indexes that correspond to those Z-order values,\nand iterates through every physical index in this range.\nIf we reach a Z-order value that is outside the query rec-\ntangle (the Z-order curve might enter and exit the query\nrectangle many times), we compute the next Z-order\nvalue that falls within the query rectangle. We then\n“skip ahead” to the page that contains this Z-order value,\nby comparing with each page’s minimum Z-order value.\n16\n\nLearning Multi-dimensional Indexes SIGMOD’20, June 14-19, 2020, Portland, OR, USA\n•Hyperoctree: We recursively split into d-dimensional\nhyperoctants until each page has below the page size\nnumber of points. Points within a page are stored con-\ntiguously,andpagesareorderedbyanin-ordertraversal\nof the tree index. Each node in the hyperoctree contains\nan array of points to 2dchild nodes, the min and max\nvalue in each dimension for points in the page, and\nthe start and end physical index for points in the page.\nGiven a query, the index finds all pages that intersect\nwith the query rectangle, uses the node’s metadata to\nidentify the physical index range for each page, and\nscans all physical index ranges.\n•K-d tree: We recursively partition space using the me-\ndian value along each dimension, until the number of\npoints in each page has below the page size number of\npoints. The dimensions are used for partitioning in around robin fashion, in order of decreasing selectivity.\nIf the remaining points all have the same value in a\nparticular dimension, that dimension is no longer used\nfor further partitioning. Points within a page are stored\ncontiguously, and pages are ordered by an in-order tra-\nversal of the tree index. Each node in the k-d tree con-\ntains the pointers to its two children, the dimension that\nis split on, the split value, and the start and end physical\nindex for points in the page. Given a query, the index\nfinds all pages that intersect with the query rectangle,\nuses the node’s metadata to identify the physical index\nrange for each page, and scans all physical index ranges.\nB OPTIMIZATION PSEUDOCODE\nAlgorithm1providespseudocodefortheprocedureofoptimiz-\ning the layout using a calibrated cost model, described in §4.2.\n17\n\nSIGMOD’20, June 14-19, 2020, Portland, OR, USA Vikram Nathan∗, Jialin Ding∗, Mohammad Alizadeh, Tim Kraska\nAlgorithm 1 Layout Optimization\n1:Inputs: d-dimensional dataset D, query workload Q={qi}, cost model T:(D,q,L)→query time\n2:Output: layout L=(O,C), where Ois the order of dimensions and C={ci}0≤i<d−1is the number columns in each grid\ndimension\n3:procedure FindOptimalLayout (D,Q,T)\n4:bD= Sample( D)\n5:bQ= Sample( Q)\n6: /* RMIs trained on each dimension of bDare used to flatten the data and query workload samples\n7: by replacing each value vin the i-th dimension of a point or query with CDF i(v)*/\n8:bD,bQ= Flatten( bD,bQ)\n9: dims = /* dimensions ordered by decreasing average selectivity of q∈bQonbD*/\n10: best_cost =∞\n11: best_L = null\n12: fori in 0:d do\n13: O = {dims[0:i], dims[i+1,:], dims[i]} /* use i-th dimension as sort dimension */\n14: /* search for minimum cost T(bD,q,(O,C))averaged over q∈bQ, assuming fixed order O, by varying C */\n15: C, cost = GradientDescent(T, O, bD,bQ) /* returns lowest found cost and the C that achieves it */\n16: L = (O, C)\n17: ifcost < best_cost then\n18: best_cost = cost\n19: best_L = L\n20: end if\n21: end for\n22: return best_L\n23:end procedure\n18",
  "textLength": 87748
}