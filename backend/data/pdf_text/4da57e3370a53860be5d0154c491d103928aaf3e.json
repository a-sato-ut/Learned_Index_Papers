{
  "paperId": "4da57e3370a53860be5d0154c491d103928aaf3e",
  "title": "Cardinality Estimation of an SQL Query Using Recursive Neural Networks",
  "pdfPath": "4da57e3370a53860be5d0154c491d103928aaf3e.pdf",
  "text": "Mathematical Problems of Computer Science 54, 41{52, 2020.\nUDC 004.65, 004.8\nCardinality Estimation of an SQL Query Using\nRecursive Neural Networks\nDavit S. Karamyan\nInformation Systems\nYerevan State University, Armenia\ne-mail: dkaramyan@krisp.ai\nAbstract\nTo learn complex interactions between predicates and accurately estimate the cardi-\nnality of an SQL query, we develop a novel framework based on recursive tree-structured\nneural networks, which take into account the natural properties of logical expressions:\ncompositionality and n-ary associativity. The proposed architecture is an extension\nof MSCN (multi-set convolutional network) for queries containing both conjunction\nand disjunction operators. The goal is to represent an arbitrary logical expression in a\ncontinuous vector space by combining sub-expression vectors according to the operator\ntype. We compared the proposed approach with the histogram-based approach on the\nreal-world dataset and showed that our approach signi\fcantly outperforms histograms\nwith a large margin.\nKeywords: Recursive neural networks, Representation learning, Cardinality esti-\nmation, Compositionality, Complex logical expressions.\n1. Introduction\nCardinality estimation is a key unit in query optimization. To choose the best execution plan,\nthe query optimizer should precisely estimate the cardinality of an SQL query the number of\nrows in the table selected by the query without actual execution. Existing query optimizers\nin today's database management systems still select poor execution strategies. Their main\ndrawback is that query optimizers make simplifying assumptions (e.g., column independence)\nabout the underlying distribution of the relational table. These simplifying assumptions are\nhelping to factorize the joint distribution into some low-dimensional representation using\nper column statistics, which are cheap to construct and store. When these assumptions do\nnot hold, cardinality estimation errors occur, and the problem gets more complicated as the\nnumber of columns grows, leading to sub-optimal plan selections.\nRecently, a number of machine and deep learning techniques have been successfully ap-\nplied in many RDBMS (Relational Database Management System) applications, including\n41\n\n42 Cardinality Estimation of an SQL Query Using Recursive Neural Networks\nquery optimization [1] - [6], indexing [7]. Attempts are being made to replace programmed\nheuristics with learned models. The advantage of these methods concluded in their ability\nto learn latent patterns of the dataset that are hard to \fnd using rule-based heuristics.\nThis work was originally inspired by the recent work done by Kipf et al., 2018 [1]. The\nauthors have developed a new deep learning approach (a multi-set convolutional network) to\ncardinality estimation. They show that deep learning models can learn complex interactions\nbetween predicates and even can capture join-crossing correlations.\nNevertheless, there remain challenging problems with complex queries, which can contain\nan arbitrary number of predicates connected by one of the Boolean operators (conjunction or\ndisjunction). We use a recursive neural model [8] to approach this challenge by representing\na logical expression with a dense vector which can be considered as a non-linear composition\nof sub-expression vectors.\nAnother challenge is related to the evaluation metric: mean q-error [9], which is the ratio\nbetween an estimate and the true cardinality. The relative factor can be the same for big\nand small cardinalities. For example, if the true and predicted cardinalities are equal to 10\nand 5, the q-error will be 2. The same q-error will be obtained if the true and predicted\ncardinalities are equal to 10000 and 5000. It will be better to distinguish between mistakes\nmade on big cardinalities mistakes made on small cardinalities. That is why we introduce two\nnovel evaluation metrics for cardinality estimation: mean absolute interval error ( MAIR )\nand interval accuracy ( IA), which take into account not only the relative factor but also the\nabsolute di\u000berence.\nThe major contributions of our work are as the followings:1\n\u000fWe present a novel deep learning framework for cardinality estimation, which can\nhandle complex queries.\n\u000fWe introduce new evaluation metrics as well as a new objective function for cardinality\nestimation to distinguish between big and small mistakes.\n\u000fWe compare the proposed model with a histogram-based approach and show the su-\nperiority of the proposed model.\n2. Related Work\nIvanov et al. [10] used query execution statistics of the previously executed queries to train\nfamous machine learning models (e.g., KNN). Despite the simplicity, their approach does\nnot take into account the semantic similarity between clauses, i.e., predicates are atomic\nobjects.\nKipf et al. introduced [1] the multi-set convolutional network for cardinality estimation.\nThey used set convolution to represent a set of tables, a set of joins and a set of predicates\nwith \fxed-length vectors, which are then fed as input to a regression module. They have\n1https://github.com/naymaraq/SQL-Cardinality-Estimation.gitThe rest of this paper is organized as follows: A literature review on cardinality estimation\nis presented in Section 2. This is followed by the introduction of the proposed model and\nits essential aspects. Experimental evaluations and training details are presented in Section\n4. This is followed by discussion and conclusions.\n\nD. Karamyan 43\nshown that it can learn (join-crossing) correlations pretty well. However, there are still\nopen questions on dealing with complex predicates, which may include both conjunction\nand disjunction.\nAnother supervised method was proposed by Wang et al. [11], where an attempt was\nmade to build a consistent and interpretable cardinality estimator by introducing a mono-\ntonic regression model w.r.t the query threshold.\nTo capture the multivariate distributions of relational tables, Yang et al. [6] proposed an\nunsupervised approach based on the deep autoregressive model combined with Monte Carlo\nsampling. The model approximates conditional densities, which are then used to evaluate\nthe joint distribution.\nIn [12], the authors focused on more general queries, which may include DISTINCT,\nAND, OR, and NOT operators. They described a recursive algorithm to extend any cardi-\nnality estimation method that only handles conjunctive queries to one that works for more\ngeneral queries, without changing the method itself. However, the proposed method has\nexponential complexity and it is tractable for queries, which have relatively small number of\npredicates.\nMarcus et al. [13] presented \rexible operator embeddings, to automatically map query\noperators to useful features tailored to a particular database which can be combined with\nmachine learning models.\n3. Proposed Approach\nIn this chapter, we will describe the key aspects of the proposed approach and explain how\nto use natural properties of logical expressions (compositionality and n-ary associativity)\nand inject these inductive properties into deep learning architecture, which will allow them\nto learn composable, permutations invariant functions. We will introduce the parsing al-\ngorithm (Permutation Invariant Parse Tree) to achieve compositionality as well as chosen\ndeep learning architecture. Also, we brie\ry remind how to represent queries with multi-hot\nvectors as it was done in previous works [1, 5, 12].\n3.1. Query Representation\nFrom now on, we focus only on SQL queries, which have the following form:\nselect count(*)\nfrom table\nwherelogicalexpression\nwherelogicalexpression consists of an arbitrary number of predicates, which may be con-\nnected with one of the Boolean operators (&& ;jj) and may be grouped using parentheses.\nPredicates are tuples with the form ( c;op;v ), whereccorresponds to column name, opcan\nbe any comparison operator: op2f<;\u0014;=;6=;\u0015;>g,vis taken from the column domain.\nLike the related works, we also represent queries with multi-hot vectors, according to\nwhich every predicate of the form ( c;op;v ) is encoded with multiple one-hot vectors one for\neach part of the predicate. More formally, the feature vector of predicate pis a concatenation\nof one-hot vectors of its parts: xp= [xc;xop;xv]. Here we assume that xvis a one-hot vector,\nbut in general, it could also be a numeric value. These vectors are used as leaf nodes and\nserve as inputs for TreeRNN (see Section 3.3.).\n\n44 Cardinality Estimation of an SQL Query Using Recursive Neural Networks\nIn addition, we will restrict our attention to single-table queries, i.e., in from \feld, there\nis always one table. That is why we do not need to encode tables and join attributes. It is\nworth noting that the proposed architecture can be easily extended for queries containing\njoins and multiple-relations, as was done in [1].\n(a) Expression tree. (b) Permutation invariant parse tree.\nFig. 1. Expression and permutation invariant trees for ( x1&&x2)&&( x3&&x4):\n3.2. Parsing\nIn this section, we would like to study the structure of functions operating on logical expres-\nsions. These functions take any valid logical expression as an input, and the output response\nrange is a continuous space R, as in the case of regression. A function facting on logical\nexpressions needs to be invariant with respect to a permissible permutation of predicates. In\nother words, the permutation invariability of function fcan be de\fned recursively as follows:\nifxandyare sub-expressions, then\n\u000ff(xjjy) =f(yjjx)\n\u000ff(x&&y) =f(y&&x)\nThus, to construct the proper structure of function f, we extend the traditional expression\ntree2to an alternative and equivalent one: Permutation Invariant Parse Tree, where every\nnode might have an arbitrary number of children. For example, a permutation invariant\nparse tree for ( x1&&x2)&&(x3&&x4) will look like just as the example shown in Fig. 1b.\nAll the children of the internal node form a set, and the permutation of this set should not\nlead to any changes in the \fnal output.\nThe algorithm of construction of such a tree is pretty straightforward: in the \frst step,\nthe expression tree is being constructed from a pre\fx/post\fx form of the logical expression.\nIn the second step, we traverse from leaves to root by merging two internal nodes if they\nboth have the same operator.\n2An expression tree is a binary tree, in which each internal node corresponds to the operator. Each leaf\nnode corresponds to an operand.\n\nD. Karamyan 45\nFig. 2. The architecture of the model: During the forward propagation, the corresponding\nsubnetwork adjusted to each internal node according to the operator (&& ;jj).\nThe activation of the root node acts as an input for the regression module.\n3.3. Model Architecture\nDue to the compositional nature of logical expression, we choose a tree-structured recursive\nneural network (TreeRNN) model [8], which has been successful in a number of NLP tasks,\nincluding sentiment analysis [14, 15], paraphrase detection [16], natural language inference\n[17], etc. Their success highly depends on their ability to capture semantic compositionality.\nIn NLP, compositionality is the ability to express sentences of arbitrary length by combining\nphrases and words. Besides their ability to express compositionality with \fxed-length repre-\nsentations, tree-structured models have justi\fed themselves to learn logical deduction from\nreasonably-sized training sets. It is shown that these models can learn to identify logical\nrelationships such as entailment and contradiction[18]. Also, TreeRNN models have been\nused to \fnd the equivalence of arbitrary symbolic and boolean expressions by forcing the\nmodel to cluster its output to one location per equivalence class[19].\nGiven a tree described in Section 3.2., let C(j) denote the set of children of node jand\nN(j) denote the number of children of node j. The following equations describe the forward\npropagation of the model:\n~hj=1\nN(j)X\nk2C(j)\u001e(hk);\n\u001e(hk) =(\n\u001e&&(hk);iftype(j) = &&\n\u001ejj(hk);iftype(j) =jj;\ns=R(~hroot): (1)\n\n46 Cardinality Estimation of an SQL Query Using Recursive Neural Networks\nwhere in (1), ~hrootis a vector representation of root node, Ris a shallow neural network that\nestimates the \fnal cardinality score.\nIn Fig. 2, we illustrate the architecture of the proposed TreeNN model on a particular\ninput query: ( x1jjx2jjx3)&&x4&&(x5jjx6). During the forward propagation, the model uses\na di\u000berent type of subnetwork according to the operator ( \u001e&&or\u001ejj). These two types\nof subnetworks are just shallow multilayer perceptrons (MLP) with the linear activation\nfunctions for the hidden layers and the hyperbolic tangent activation functions for the output\nlayer. The outputs from MLP are averaged by mean pooling layer, making the activations of\neach tree node invariant to the corresponding child nodes permutations. The motivation of\nchoosing mean pooling comes from Deep Sets [20], where Zaheer et al. introduce a network\narchitecture, which can operate on sets. In particular, they prove a theorem, which claims\nthat any function f(X) operating on a set Xis a valid set function, i.e., invariant to the\npermutation of instances in X, if it can be decomposed in the form \u001a(P\nx2X\u001e(x)), for suitable\ntransformations \u001eand\u001a. Finally, the activations of the root node ( ~hroot) act as an input for\nanother multilayer perceptron ( R), which is responsible for cardinality estimation.\n4. Experiments\n4.1. Data Collection\nFirst, we generate random queries based on the database schema and the values in it. Next,\nthe true cardinalities are obtained by executing queries on the whole database. As we have\nmentioned before, we only examine single-table queries, i.e., a training sample consists of a\nlogical expression de\fned in where \feld and the true cardinality. An example of a logical\nexpression is shown in Fig. 3.\nTo generate predicates, we uniformly select a column from the database schema, then\nselect the corresponding operation from f<;\u0014;=;6=;\u0015;>gand, \fnally, select a value from\nthe column values.\nWe perform parallel query execution in Hadoop infrastructure, which gives us the fastest\nway to collect the training data.\nFig. 3. An example of generated logical expression .\n4.2. Evaluation Metrics\nThe traditional evaluation metric for cardinality estimation is the mean q-error [20], which\nshows how many times the predicted and the true cardinalities di\u000ber from each other. The\ndrawback of q-error is that it scores big and small cardinalities in the same way. For instance,\nif the true and predicted cardinalities are equal to 10 and 5, respectively, the q-error will be\n\nD. Karamyan 47\n2. We will get the same q-error if the true and predicted cardinalities are equal to 10000\nand 5000. Hence, it would be preferable to score these cases by considering also the absolute\ndi\u000berence. That is why we introduce two novel metrics for cardinality estimation: Mean\nAbsolute Interval Error (MAIR) and Interval Accuracy (IA) . To de\fne these metrics, we\n\frst split the output range into Kconsecutive intervals. Let G(x) denote the index of the\ninterval, which contains x. Based on these intervals, MAIR andIAare calculated as follows:\nMAIR =1\nNX\nqjG(y)\u0000G(^y)j;\nIA=P\nq1G(y)=G(^y)\nN;\nwhereNis the number of samples, yand ^yare the true and predicted cardinalities. The\nhigher the value of K, i.e., the smaller the intervals, the more reliable these metrics are.\nBesides, we also report other evaluation metrics such as RMSE (root mean squared error),\nMAE (mean absolute error), MQE (mean q-error).\n4.3. Training Details\n4.3.1. Training Data\nThe training was conducted on 100 Kgenerated queries, from which 10 Kqueries were used\nas a development set, and another 10 Kwere used as a \fnal evaluation (test) set. We also\nremoved 0-cardinality examples from the development and test sets to have a fair evaluation.\nThe log-normalization was used to normalize the outputs.\nAs each query has its unique parse tree, we cannot easily organize mini-batch training.\nSo, in all our experiments, the training was conducted with a mini-batch size equal to 1,\nwhich leads to slow training.\n4.3.2. Hyperparameter Search\nHyperparameter search was performed on the development set to choose the dimensionality\nof node vectors hjand the dimensionality of layers in regression network R. Because of slow\ntraining, hyperparameter search was performed on the \frst 20 epochs using early stopping.\nThe best result was achieved for dhj= 32 anddlR= 80.\n4.3.3. Loss Function and Optimization\nTo overcome the drawback mentioned in Section 4.2., we construct the loss function, which\nconsists of two parts: the relative factor and the squared di\u000berence. The \frst part is respon-\nsible for optimizing the relative di\u000berence, i.e., the q-error, and the second part is responsible\nfor di\u000berentiating big and small mistakes.\nLq(y;^y) =\u000b\n2(y\n^y+^y\ny) + (1\u0000\u000b)(y\u0000^y)2;\nL=1\nNX\nqLq(y;^y): (2)\n\n48 Cardinality Estimation of an SQL Query Using Recursive Neural Networks\n(a) Convergence of loss function with (b) MAIR and IA changes with\nthe number of epochs. the number of epochs.\nFig. 4. Model performance with the number of epochs.\nThe \fnal training objective is to minimize the loss function written in (2). The loss is\na convex combination of relative and squared di\u000berence terms. We set \u000bequal to 0:99 to\nconcentrate the central part of the loss on the relative part, plus an additional penalization\nscore when huge mistakes were made.\nWe used Adam optimizer [21] with \f1= 0:9;\f2= 0:99 and\u000f= 10\u00008. We set the learning\nrate equal to 10\u00004. The parameters, which achieve the smallest absolute interval error on\nthe development set will be chosen for \fnal evaluation.\nFig. 4a shows how the training and validation losses have decreased over epochs. It\ntakes 30-35 epochs to converge. On the other hand, Fig. 4b shows how MAIR andIAhave\nchanged over epochs. Both metrics are evaluated on the development set. It can be seen from\nFig. 4 that minimizing the loss function leads to the smaller mean absolute interval error\nand the larger interval accuracy, which means that the TreeRNN is generalized successfully\non the development set.\n4.4. Results\n4.4.1. Histogram\nThe histogram approach makes a simplifying assumption about column independence. The\nindependence assumption is helping to factorize the joint distribution. In particular, one can\nderive inference rules based on that assumption:\nP(A^B) =P(A)P(B);\nP(:A) = 1\u0000P(A);\nP(A_B) = 1\u0000(1\u0000P(A))(1\u0000P(B));\nwhereAandBare predicates, Pis the probability of a single predicate.\n\nD. Karamyan 49\n4.4.2. Quantitative Analysis\nThe performance of the proposed permutation invariant TreeRNN is compared with the\nbaseline model. The appropriate performance metrics are given in Table 1. To calculate\nMAIR andIA, we divide the output range into 15 intervals: 0 <50<500<1000<\n2500<5000< ::: < 250000<400000<600000. The expression is considered short if it\nis a combination of at most \fve predicates. The expression is considered middle if it is a\ncombination of at least six but not more than 12 predicates. And, \fnally, the expression is\nconsidered long if it is a combination of at least 13 predicates.\nOne can see from Table 1 that the proposed model is superior to the baseline model\nwith a clear margin. Due to their simple architecture, histograms are fast and accurate\nwhen dealing with short expressions. Long expressions are better learned with the TreeRNN\nmodel, as it takes into account the natural properties of logical expressions and can learn\nmore complex interactions between predicates.\nTable 1: Comparison of Histogram against the proposed TreeRNN. The integer in parenthe-\nses indicates the number of evaluated samples corresponding to each type.\nQuery TypeTreeRNN Histogram\nMQE MAIR IA MAE RMSE MQE MAIR IA MAE RMSE\nShort (313) 1.616 0.294 0.719 7811 14927 1.36 0.3 0.715 6077 11294\nMiddle (1353) 1.718 0.363 0.66 3661 9400 2.685 0.5 0.59 4846 13121\nLong (4093) 1.768 0.2 0.81 594 2652 3.833 0.393 0.649 1006 4625\nOverall (5759) 1.747 0.243 0.77 1707 6154 3.43 0.413 0.64 2184 7911\n4.4.3. Monotonic Test\nThe ideal cardinality estimation system must be not only accurate but also consistent and\ninterpretable. To explain it, let us look at the example: if query Ais a subset of B, then the\ncardinality estimator should give more scores to Brather than A.\nWe make a simple experiment to understand the behaviour of the proposed model. We\nrandomly remove one or two predicates from logical expressions in the test set and see how\nthe output changes. If removed predicates are connected to others with && ( jj) operator,\nthe output must be increased (decreased). We report the accuracy of successful outcomes.\nTable 2 shows the results. Although we did not even demand that TreeRNN must be\nmonotonous, it performs very well, especially for removed predicates, which are connected\nto others with && operator. Histograms, on the other side, are designed to be monotonous.\n\n50 Cardinality Estimation of an SQL Query Using Recursive Neural Networks\nTable 2: Monotonic test result.\n#RemovedTreeRNN Histogram\n&&jj &&jj\n1 92% 67% 100% 100%\n2 94% 78% 100% 100%\n5. Conclusion\nWe have presented a novel cardinality estimation system using recursive tree-structured\nneural networks. The proposed TreeRNN model is capable of taking complex tree-structured\ndata and is able to learn underlying patterns of relational tables. Experimental evaluations\nshow that our method outperforms the well-known histogram method with a clear margin.\nThis is achieved by injecting inductive biases into the model architecture, which allows us\nto learn composable and permutations invariant functions.\nReferences\n[1] A. Kipf, T. Kipf, B. Radke, V. Leis, P. Boncz and A. Kemper, Learned Cardinalities:\nEstimating Correlated Joins with Deep Learning , arXiv preprint arXiv:1809.00677,\n2018.\n[2] S. Krishnan, Z. Yang, K. Goldberg, J. Hellerstein and I. Stoica, Learning to Optimize\nLoin Queries with Deep Reinforcement Learning , arXiv preprint arXiv:1808.03196,\n2018.\n[3] R. Marcus and O. Papaemmanouil, \\Deep reinforcement learning for join order enu-\nmeration\", In Proceedings of the First International Workshop on Exploiting Arti\fcial\nIntelligence Techniques for Data Management , pp. 1-4, 2018.\n[4] J. Ortiz, M. Balazinska, J. Gehrke, and S. S. Keerthi, \\Learning state representations\nfor query optimization with deep reinforcement learning\", In Proceedings of the Second\nWorkshop on Data Management for End-To-End Machine Learning , pp. 1-4, 2018.\n[5] J. Ortiz, M. Balazinska, J. Gehrke, and Sathiya S, An empirical analysis of deep\nlearning for cardinality estimation , arXiv preprint arXiv:1905.06425, 2019.\n[6] Z. Yang, E. Liang, A. Kamsetty, C. Wu, Y. Duan, X. Chen, P. Abbeel, J. M. Hellerstein,\nS. Krishnan, and I. Stoica, Deep unsupervised cardinality estimation , arXiv preprint\narXiv:1905.04278, 2019.\n[7] T. Kraska, A. Beutel, E. H. Chi, J. Dean and N. Polyzotis, \\The case for learned index\nstructures\", In Proceedings of the 2018 International Conference on Management of\nData , pp. 489-504, 2018.\n[8] C. Goller and A. Kuchler, \\Learning task-dependent distributed representations by\nbackpropagation through structure\", In Proceedings of International Conference on\nNeural Networks (ICNN96) , IEEE,vol. 1, pp. 347-352, 1996.\n[9] G. Moerkotte, T. Neumann, and G. Steidl, \\Preventing bad plans by bounding the\nimpact of cardinality estimation errors\", Proceed- ings of the VLDB Endowment , vol.\n2, no. 1, pp. 982-993, 2009.\n\nD. Karamyan 51\n[10] O. Ivanov and S. Bartunov, Adaptive cardinality estimation , arXiv preprint\narXiv:1711.08330, 2017.\n[11] Y.Wang, C.Xiao, J.Qin, X.Cao, Y.Sun, W.Wang, and M.Onizuka, \\Monotonic cardi-\nnality estimation of similarity selection: A deep learning approach\", In Proceedings\nof the 2020 ACM SIGMOD International Conference on Management of Data , pp.\n1197-1212, 2020.\n[12] R. Hayek and O. Shmueli, Nn-based transformation of any sql cardinality estimator for\nhandling distinct, and, or and not , arXiv preprint arXiv:2004.07009, 2020.\n[13] R.Marcusand O.Papaemmanouil, Flexible operator embeddings via deep learning , arXiv\npreprint arXiv:1901.09090, 2019.\n[14] O. Irsoy and C. Cardie, \\Deep recursive neural networks for compositionality in lan-\nguage\", In Advances in Neural Information Processing Systems , pp. 2096-2104, 2014.\n[15] R. Socher, J. Pennington, E. H. Huang, A. Y. Ng and C. D. Manning, \\Semi-supervised\nrecursive autoencoders for predicting sentiment distributions, In Proceedings of the\n2011 conference on empirical methods in natural language processing , pp. 151-161,\n2011.\n[16] R. Socher, E. H. Huang, J. Pennin, C. D. Manning and A. Y. Ng, \\Dynamic pooling\nand unfolding recursive autoencoders for paraphrase detection\", In Advances in Neural\nInformation Processing Systems , pp. 801-809, 2011.\n[17] I. Dagan, O. Glickman and B. Magnini, \\The pascal recognising textual entailment\nchallenge\", in Machine Learning Challenges Workshop , Springer, pp. 177-190, 2005.\n[18] S. Bowman, C. Potts and C. D. Manning, \\Recursive neural networks can learn logical\nsemantics\", In Proceedings of the 3rd workshop on continuous vector space models and\ntheir compositionalit y, pp. 12-21, 2015.\n[19] M. Allamanis, P. Chanthirasegaran, P. Kohli and C. Sutton, \\Learning continuous se-\nmantic representations of symbolic expressions\", In International Conference on Ma-\nchine Learning , pp. 80-88, 2017.\n[20] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov and A. J.\nSmola, \\Deep sets\", In Advances in Neural Information Processing Systems , pp. 3391-\n3401, 2017.\n[21] D.P.Kingma and J.Ba, Adam:A method for stochastic optimization , arXiv preprint\narXiv:1412.6980, 2014.\nSubmitted 10.06.2020, accepted 04.11.2020.\n\n5 2 Cardinality Estimation of an SQL query using Recursive Neural Networks\nÐ³ñóÙ³Ý Ñ½áñáõÃÛ³Ý Ùáï³ñÏáõÙÁ é»ÏáõñëÇí Ý»ÛñáÝ³ÛÇÝ\nó³Ýó»ñÇ ÙÇçáóáí\n¸³íÇÃ ê. ø³ñ³ÙÛ³Ý\nºñ¨³ÝÇ å»ï³Ï³Ý Ñ³Ù³Éë³ñ³Ý\ne-mail: davkar98@gmail.com\n²Ù÷á÷áõÙ\näñ»¹ÇÏ³ïÝ»ñÇ  ÙÇç¨ µ³ñ¹ Ï³å»ñÁ ëáíáñ»Éáõ  ¨ ¾ë-øÛáõ-¾É  Ñ³ñóáõÙÝ»ñÇ  Ñ½áñáõÃÛáõÝÁ  \n×ß·ñÇï ·Ý³Ñ³ï»Éáõ  Ñ³Ù³ñ ³é³ç³ñÏíáõÙ  ¿ Ýáñ Ùáï»óáõÙ,  áñÁ ÑÇÙÝí³Í  ¿ \né»ÏáõñëÇí  Ý»ÛñáÝ³ÛÇÝ  ó³Ýó»ñÇ  íñ³, áñáÝù Ñ³ßíÇ »Ý ³éÝáõÙ ïñ³Ù³µ³Ý³Ï³Ý  \n³ñï³Ñ³ÛïáõÃÛáõÝÝ»ñÇ  µÝ³Ï³Ý Ñ³ïÏáõÃÛáõÝÝ»ñÁ,  µ³Õ³¹ñ³Ï³ÝáõÃÛáõÝÁ  ¨ ³ëáóÇ³-\nïÇíáõÃÛáõÝÁ:  ²é³ç³ñÏíáÕ  ×³ñï³ñ³å»ïáõÃÛáõÝÁ  MSCN-Ç (Multi-Set  Convolutional  Net-\nwork) ÁÝ¹É³ÛÝáõÙÝ  ¿ ¹Ç½áõÝÏóÇ³  ¨ ÏáÝÛáõÏóÇ³  ûå»ñ³ïáñÝ»ñ  å³ñáõÝ³ÏáÕ  Ñ³ñóáõÙÝ»ñÇ  \nÑ³Ù³ñ: Üå³ï³ÏÝ  ¿ Ý»ñÏ³Û³óÝ»É  Ï³Ù³Û³Ï³Ý  ïñ³Ù³µ³Ý³Ï³Ý  ³ñï³Ñ³ÛïáõÃÛáõÝ  \n³ÝÁÝ¹Ñ³ï  í»Ïïáñ³ÛÇÝ  ï³ñ³ÍáõÃÛ³Ý  Ù»ç` Ñ³Ù³ï»Õ»Éáí  »ÝÃ³³ñï³Ñ³ÛïáõÃÛáõÝ»ñÇ  \ní»ÏïáñÝ»ñÁ ` Áëï ûå»ñ³ïáñÇ  ï»ë³ÏÇ:  ²é³ç³ñÏíáÕ  Ùáï»óáõÙÁ  Ñ³Ù»Ù³ïí»É  ¿ \nÑÇëïá·ñ³Ù³ÛÇÝ  Ùáï»óÙ³Ý  Ñ»ï ¨ óáõÛó ¿ ïñí»É, áñ ³ÛÝ ½·³ÉÇáñ»Ý  ·»ñ³½³ÝóáõÙ  ¿ \nÑÇëïá·ñ³Ù³ÛÇÝ  Ùáï»óÙ³ÝÁ\nÎöåíêà ìîùíîñòè SQL-çàïðîñà ñ ïîìîùüþ ðåêóðñèâíûõ\níåéðîííûõ ñåòåé\nÄàâèä Ñ. Êàðàìÿí\nÅðåâàíñêèé ãîñóäàðñòâåííûé óíèâåðñèòåò\ne-mail: davkar98@gmail.com\nÀííîòàöèÿ\n×òîáû èçó÷èòü ñëîæíûå âçàèìîäåéñòâèÿ ìåæäó ïðåäèêàòàìè è òî÷íî îöåíèòü\nìîùíîñòü SQL-çàïðîñà, áûëà ñêîíñòðóèðîâàíà íîâàÿ ñòðóêòóðà, îñíîâàííàÿ íà\nðåêóðñèâíûõ íåéðîííûõ ñåòÿõ ñ äðåâîâèäíîé àðõèòåêòóðîé, êîòîðûå ó÷èòûâàþò\nåñòåñòâåííûå ñâîéñòâà ëîãè÷åñêèõ âûðàæåíèé: êîìïîçèöèîííîñòü è n-àðíóþ\nàññîöèàòèâíîñòü. Ïðåäëàãàåìàÿ àðõèòåêòóðà ÿâëÿåòñÿ ðàñøèðåíèåì MSCN(Multi-\nSet Convolutional Network) äëÿ çàïðîñîâ, ñîäåðæàùèõ îïåðàòîðû êîíúþíêöèè è\näèçúþíêöèè. Öåëü ñîñòîèò â òîì, ÷òîáû ïðåäñòàâèòü ïðîèçâîëüíîå ëîãè÷åñêîå\nâûðàæåíèå â íåïðåðûâíîì âåêòîðíîì ïðîñòðàíñòâå ïóòåì îáúåäèíåíèÿ\nâåêòîðîâ ïîäâûðàæåíèé â ñîîòâåòñòâèè ñ òèïîì îïåðàòîðà. Ïðåäëàãàåìûé\nïîäõîä áûë ñðàâíåí ñ ïîäõîäîì, îñíîâàííûì íà ãèñòîãðàììàõ, íà ðåàëüíîì\níàáîðå äàííûõ è áûëî ïîê àçàíî, ÷òî ïðåäëîæåííûé ïîäõîä çíà÷èòåëüíî\nïðåâîñõîäèò ãèñòîãðàììû.\nÊëþ÷åâûå ñëîâà: ðåêóðñèâíûå íåéðîííûå ñåòè, ðåïðåçåíòàòèâíîå îáó÷åíèå,\nìîùíîñòü SQL-çàïðîñîâ, êîìïîçèöèîííîñòü, ñëîæíûå ëîãè÷åñêèå âûðàæåíèÿ.´³Ý³ÉÇ  µ³é»ñ `   é»ÏáõñëÇí  Ý»ÛñáÝ³ÛÇÝ  ó³Ýó»ñ, Ý»ñÏ³Û³óáõóã³Ï³Ý  áõëáõóáõÙ,  Ñ³ñó-\nÙ³Ý Ñ½áñáõÃÛáõÝ,  µ³Õ³¹ñ³Ï³ÝáõÃÛáõÝ,  µ³ñ¹ ïñ³Ù³µ³Ý³Ï³Ý  ³ñï³Ñ³ÛïáõÃÛáõÝÝ»ñ:",
  "textLength": 28039
}