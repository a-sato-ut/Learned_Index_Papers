{
  "paperId": "c1289b5babbc8eb2a50df198ee7bdf66bb7abc59",
  "title": "Neural networks as building blocks for the design of efficient learned indexes",
  "pdfPath": "c1289b5babbc8eb2a50df198ee7bdf66bb7abc59.pdf",
  "text": "S.I.: TECHNOLOGIES OF THE 4TH INDUSTRIAL REVOLUTION WITH\nAPPLICATIONS\nNeural networks as building blocks for the design of efficient learned\nindexes\nDomenico Amato1•Giosue ´Lo Bosco1•Raffaele Giancarlo1\nReceived: 31 January 2023 / Accepted: 28 June 2023 / Published online: 21 July 2023\n/C211The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2023, corrected publication 2023\nAbstract\nThe new area of Learned Data Structures consists of mixing Machine Learning techniques with those speciﬁc to Data\nStructures, with the purpose to achieve time/space gains in the performance of those latter. The perceived paradigm shift in\ncomputer architectures, that would favor the employment of graphics/tensor units over traditional central processing units,is one of the driving forces behind this new area. The advent of the corresponding branch-free programming paradigm\nwould then favor the adoption of Neural Networks as the fundamental units of Classic Data Structures. This is the case of\nLearned Bloom Filters. The equally important ﬁeld of Learned Indexes does not appear to make use of Neural Networks atall. In this paper, we offer a comparative experimental investigation regarding the potential uses of Neural Networks as a\nfundamental building block of Learned Indexes. Our results provide a solid and much-needed evaluation of the role Neural\nNetworks can play in Learned Indexing. Based on our ﬁndings, we highlight the need for the creation of highly specialisedNeural Networks customised to Learned Indexes. Because of the methodological signiﬁcance of our ﬁndings and appli-\ncation of Learned Indexes in strategic domains, such as Computer Networks and Databases, care has been taken to make\nthe presentation of our results accessible to the general audience of scientists and engineers working in Neural Networksand with no background about Learned Indexing.\nKeywords Information retrieval /C1Machine learning /C1Neural networks /C1Learned indexes\n1 Introduction\nLearned Data Structures are a new area of research that\ncombines Machine Learning (ML) techniques with thoseinherent to Data Structures, with the goal of enhancing thespeed and space efﬁciency of traditional Data Structures. It\nwas started by Kraska et al. [ 27], has expanded quickly\n[16], and has recently been extended to include Learned\nAlgorithms [ 35]. Its impact on the design of modern\nInformation Systems is expected to be quite signiﬁcant\n[28].\n1.1 The motivation for learned data structures:\ncomputer architectures\nDue to their learning ability, Neural Networks (NNs) [ 38]\nare without a doubt the ML models perceived to have thegreatest potential in the ﬁeld of Learned Data Structures\n[27]. However, they demand excessive computational\npower. This computational bottleneck has essentially beeneliminated for their successful use in many application\nareas [ 29], thanks to the adoption of highly engineered\ndevelopment platforms such as TensorFlow [ 1], and the\nintroduction of graphics processing units (GPU) and tensorAn extended abstract related to this paper has been presented\nat the 23rd International Conference EAAAI/EANN. The\nConference proceedings are the Series Communications in\nComputer and Information Science by Springer 2022, Vol.1600. The full reference is [ 5].\n&Giosue ´Lo Bosco\ngiosue.lobosco@unipa.it\nDomenico Amato\ndomenico.amato01@unipa.it\nRaffaele Giancarlo\nraffaele.giancarlo@unipa.it\n1Dipartimento di Matematica e Informatica, Universita ´degli\nStudi di Palermo, Via Archiraﬁ 34, 90123 Palermo, Italy\n123Neural Computing and Applications (XXXX) 35:21399–21414\nhttps://doi.org/10.1007/s00521-023-08841-1 (0123456789().,-volV) (0123456789(). ,- volV)\n\nprocessing units (TPU) architectures in commercial com-\nputers [ 40,43]. The main advantage of these novel archi-\ntectures is their excellent ability to parallelize algebraic\ntasks performed by NNs. Further gains are foreseen sincecurrent studies argue that the power of the GPU can\nincrease by 1000x in the next few years. On the other hand,\nthose performance gains are not anticipated for traditionalCentral Processing Units (CPUs), due to Moore’s Law\nlimitations [ 36]. Therefore, a programming paradigm that\nwould promote straight-line mathematical operations,\nwhich can be easily pipelined on GPUs, appears much\nmore promising than the one that uses branches of the if-then-else type. Due to these factors and to the fact that\nclassic Data Structures implementations use branch\ninstructions, the use of ML models, like NNs, to reduce thebranchy part of code in Data Structures implementations\nmay lead to versions of those latter that can take full\nadvantage of the new cutting-edge architectures. Such anachievement would have major effects on many strategic\ndomains, e.g., Computer Networks and Databases. Unfor-\ntunately, even though the reason for creating Learned DataStructures based on NNs is strong, how to achieve such an\nadvantage and a quantiﬁcation of how much would that be\nare at an early stage of an investigation, or not addressed atall, as we discuss in more detail in the following.\n1.2 The role of NNs: from motivation to design\nand implementation of learned data\nstructures\n•Learned Bloom Filters. Since the very beginning of\nthe ﬁeld of Learned Data Structure, NNs have been\nactively employed in the design and implementation ofLearned Bloom Filters [ 27]. Note that a Bloom Filter[ 9]\nis a Data Structure to solve the so-called Approximate\nMembership Problem for a given set A/C26U, where Uis\na universe of elements. This means deciding whether a\nquery element x2Ualso belongs to A, with a certain\nFalse Positive Rate /C15and zero False Negatives. Given a\nFalse Positive Rate, query time and space occupancy\nare crucial factors in determining how well a Bloom\nFilter performs. These variables are closely related toeach other, as clearly stated in [ 11]. Convolutional and\nRecurrent NNs have been adopted in the most recent\nversions of Learned Bloom Filters [ 14,27,34,42]. The\nreader who is interested in this particular Learned Data\nStructure can ﬁnd an experimental comparative study in\n[19].\n•Learned Indexes. They have been developed to\naddress the so-called Predecessor Search Problem .\nLetting now Abe a sorted table and given a query\nelement x2U, the Predecessor Search Problem entails\nlocating the A[j] such that A½j/C138/C20x\\A½jþ1/C138. In Sect. 2,a primer of the Learned Indexes methodology is\nprovided, designed for a general audience due to thepotential impact that this area may have on Machine\nLearning and Data Structures. As stated earlier,\nalthough the use of NNs to take advantage of novelcomputer architectures is one of the main motivations\nfor this new area, unexpectedly and to the best of our\nknowledge, none of the Learned Indexes proposed sofar, e.g., [ 4,6,7,16,17,27\n,31] to mention a few, use\nNNs. Our assessment is in agreement with what was\nreported in [ 30]. Even worse, no evaluation is available\nregarding the real role that NNs can have in the design\nand implementation of Learned Indexes.\nAlthough Dynamic Learned Indexes have been\nsuccessfully designed, e.g., ALEX [ 15], PGM-Index\n[17] and SageDb [ 26], here we concentrate on the static\nversion of the Predecessor Search Problem. Indeed, asevident in what follows, our ﬁnding in this speciﬁc\ncontext role out the use of NNs in the dynamic case.\n•Additional Learned Data Structures. Both Learned\nRank and Select Data Structures [ 10] and Learned Hash\nFunctions [ 27] do not employ NNs. Once more, no\nanalysis is available that justiﬁes such a choice.\n1.3 Our findings: the atomic case of neural\nnetworks in the design of learned indexes\nOur original contribution to the development of Learned\nData Structures is the ﬁrst evaluation of the suitability of\nNNs as core elements of Learned Indexes, keeping in view\nthe State of the Art described in Sect. 1.2. Atomic Learned\nIndexes (see Sect. 3.1) are taken into consideration in order\nto provide a clear comparative assessment of the potential\nusefulness of NNs in the aforementioned domain. They arethe most basic models that come to mind. The justiﬁcation\nfor their choice is that in the case NNs do not provide any\nsigniﬁcant advancement with respect to very simpleLearned Indexes, taking into consideration the results in\n[4,7], even with the beneﬁt of GPU processing, NNs have\nvery little to offer to Learned Indexing. Basically, in thispaper, we provide the following contributions.\n•The ﬁrst Learned Index design solely based on NN\nmodels. Because they provide an excellent balance\nbetween time effectiveness, space occupancy, and\nlearning capacity [ 8], we opt for Feed Forward NNs.\nWe refer to this Learned Index as an Atomic Learned\nIndex because it lacks any additional ML\nsubcomponents.\n•A thorough experimental investigation on the perfor-\nmance of the Atomic Learned Index for both CPU and\nGPU processing.21400 Neural Computing and Applications (XXXX) 35:21399–21414\n123\n\n•This Atomic Learned Index is thoroughly compared to\nanalogous Atomic Learned Indexes that merely uselinear regression[ 18] as the learned component. These\nmodels have been analysed and utilised in [ 4,7]. More\ncomplicated models include them as building blocks,e.g, [ 4,7,16,17,27,31].\n•For completeness, and in order to highlight how\ndelicate is to choose a Learned Index to effectivelyprocess a given table, we also compare the Atomic\nLearned Indexes with the ones that are considered State\nof the Art.\nOur ﬁndings unequivocally show the need to design NNs\nspeciﬁcally tailored for Learned Indexing, as opposed to\nLearned Bloom Filters. A work like this, which paves the\nroad for the creation of NNs more tailored for LearnedIndexing, is signiﬁcant from a methodological perspective.\nIn order to make this area accessible to researchers and\nengineers working on NNs, care is taken in providing anintuitive and easy-to-follow introduction to this area, that\nthen ‘‘moves on‘‘ to account in full for the State of the Art.\nThe software used for the experiments carried out in thispaper can be found at [ 21].\n1.4 Structure of the paper\nThe structure of this article is as follows. A high level\npresentation of the main ideas sustaining Learned Indexingis provided in Sect. 2. Atomic Learned Indexes (see\nSect. 3.1) and the primary Hierarchical Learned Indexes\n(see Sect. 3.2) are both covered in Sect. 3, which provides\nan overview of Learned Indexes that have been actually\nproposed in the Literature. We describe the adopted\nexperimental methodology in Sect. 4. Experiments and\nresults are reported in Sect. 5. We present conclusions and\nfuture study directions in Sect. 6.2 Classic and learned solutions\nfor searching and indexing in a sorted set\nConsider a sorted table Awith nkeys drawn from a uni-\nverse U. As already stated in the Introduction, the Prede-\ncessor Search Problem, also known as Sorted Table Search,states that for a given query element x, return the A[j] such\nthatA½j/C138/C20x\\A½jþ1/C138. In the sections that follow, we detail\nboth the traditional solutions to this problem and themethod suggested by Kraska et al. [ 27] for turning it into a\nlearning-prediction problem.\n2.1 Traditional options: sorted table search\ntechniques\nBinary and Interpolation Searches are well-known algo-\nrithmic solutions to the Predecessor Search Problem. The\nﬁrst is optimal in a worst-case time setting under various\ncomputational models [ 2,13,25,39], while the second has\nan excellent average case time performance fortables drawn uniformly and at random from the Universe\nU[33,39]. Moreover, they both feature loops with a very\nsmall number of instructions, which makes them extremelyquick, even in practice. For the purposes of this paper, we\nuse two C ?? implementations of Binary Search: a text-\nbook one, which we refer to as Standard (abbreviated asBS), and an implementation of the Uniform variant (ab-\nbreviated as US), which was developed as part of a study\nby Khoung and Morin [ 22], following an approach sug-\ngested by Knuth [ 25]. Additionally, the Algorithms 1 and 2\nprovide the pseudo code for those two routines, respec-\ntively. Recalling that we are interested in Learned SortedTable Search, as it will be evident in the remainder of this\npaper, we could have considered also Interpolation Search\nbut, due to its poor performance in the Learned setting [ 3],\nwe have excluded it for the experiments conducted in this\npaper.Neural Computing and Applications (XXXX) 35:21399–21414 21401\n123\n\n2.2 Learned indexing: the essential ingredients\nAs already indicated, Kraska et al. [ 27] developed a novel\nstrategy that converts the Predecessor Search Problem into\na learning-prediction one. Referring to Fig. 1, the potential\nlocation of a query element in the table, in terms of aninterval to search into, is returned by the model that was\nlearned from the data. Binary Search is then used in that\npossibly smaller interval with respect to the entire table, inorder to ﬁnalize the search. In other words, the model is\nused as an oracle to predict the query element position: the\nmore accurate the oracle, the smaller the interval it returns.In addition to that, the oracle must return its prediction very\nquickly, possibly in constant time.\nWe now describe the simplest method for creating a\nmodel for Ausing Linear Regression and Mean Square\nError Minimization [ 18], with the help of an example. In\nlight of Fig. 2and Table Ain the caption, a Learned Index\ncan be trained in the way described below.\n•Computation of the Cumulative Distribution Func-\ntion (CDF for short) of a Sorted Table . Regarding\nFig. 2a, we may plot the elements of Aon a graph,\nwhere the ordinates correspond to the ranks of theelements, and the abscissa displays the corresponding\nvalues of the elements. The outcome of this plot\nresembles a Cumulative Distribution Function CDF that\nunderlines the table, as pointed out by Marcus et al [ 31].\nThe speciﬁc procedure illustrated here can be applied to\nany sorted table to obtain the mentioned discrete curve,which in the Literature is referred to as CDF .\n•Select a Model for the CDF . The discrete CDF must\nnow be converted into a continuous curve. Fitting astraight line with the equation FðxÞ¼axþbto the\nCDF is the simplest way to accomplish this. To get\naandbin this case, we apply Linear Regression with\nMean Square Error Minimization. This process is\ndepicted in Fig. 2b, where the resulting aandbare,respectively, 0.01 and 0.85 when the array Aspeciﬁed\nin the ﬁgure legend is used as input.\n•Model Error Correction . Since Fis a rough estimate\nof the ranks of the elements in the table, using it to\npredict the rank of an element may result in an error e.\nFor instance, in Fig. 2c, we apply the model to the\nelement 398 and get a predicted rank of 4.68 instead of\nthe actual rank of 7. This means that the error of the\nmodel Fðx\nÞ¼0:01/C3xþ0:85 on this element is\ne¼j7/C0b4:68cj ¼ 3. We must thus correct this error\nbefore we can use the equation Fto predict where an\nelement xlies in the table. It is natural to correct such an\napproximate estimate by taking into account the\nmaximum error: the largest gap /C15between the actual\nrank of each element in the table and its rank aspredicted by the model. Then, the search interval of an\nelement xis speciﬁed to be ½FðxÞ/C0 /C15;FðxÞþ /C15/C138. In the\nexample discussed in Fig. 2c, the resulting /C15is equal to\n3.\n3 Learned indexes\nIn Sect. 2.2, we presented the essential ingredients of a\nLearned Index, also providing an example. However, those\nData Structures are quite successful and key to the futuredevelopment of Information Retrieval [ 28]. It comes as no\nsurprise that many proposals for Learned Indexes have\nappeared in the Literature (see again [ 28]). In agreement\nwith the State of the Art and in order to best appreciate the\nresults of this paper, we provide synoptic details about the\ndeﬁnition and functionality of a Learned Index. In partic-ular, we can distinguish two types of indexes:\n•Atomic Learned Indexes . They are the simplest type\nof Learned Indexes, i.e., as described in Sect. 3.1, they21402 Neural Computing and Applications (XXXX) 35:21399–21414\n123\n\nconsist of a single machine learning model capable of\nlearning particular features of the data.\n•Hierarchical Indexes . As described in Sect. 3.2, they\nuse Atomic Learned Indexes as nodes of a tree-like\nstructure to achieve better performance.\n3.1 Atomic learned indexes\nA model for Learned Indexes can be a linear function thatmodels the CDF of the data, as described in Sect. 2.2.W e\nidentify a model as Atomic if it consists of a closed-form\nformula or a straightforward algorithm that estimates the\nCDF on a speciﬁc point (see Fig. 1). That is, it lacks any\nlearned sub-component from the data.\n3.1.1 Atomic learned indexes characterized by analytic\nsolutions to regression problems\nRegression is a technique to estimate a given function G:\nR\nm!Rusing a particular function model ~G. Predictors\nand outcomes, respectively, are the terms used to describethe independent variables x2Rmand the dependent vari-\nable y2R. By minimising a given error function calcu-\nlated using a sample set of predictor-outcome\nmeasurements, the parameters of ~Gcan be estimated. The\nMean Square Error is the error function that is most fre-\nquently adopted, and there are various approaches to carryout the minimization task. Here, we adhere to the proce-\ndure described in [ 20], which speciﬁcally provides poly-\nnomial closed-form equations to solve the minimization ofmean square error. When a geometric linear form is taken\nas a model for ~G, it is referred to as Linear Regression.\nSimple Linear Regression (SLR) is used when m¼1 and\nMultiple Linear Regression (MLR) in all other circum-\nstances. The objective is to characterise the linear function\nmodel ~GðxÞ¼^wx\nTþ^bby estimating the parameters ^w2\nRmand ^b2Rusing a given training set of npredictor-\noutcome couples ðxi;yiÞ, where xi2Rmandyi2R. The\ndesign matrix Zcan be deﬁned as a matrix of size\nn/C2ðmþ1Þ, where Ziis the i-th row of ZandZi¼½xi;1/C138.\nAdditionally, ydenotes a vector of size n, whose j-th\ncomponent is indicated as yj. The estimation is carried out\nby a Mean Square Error Minimization as follows:\nMSEðw;bÞ¼1\nn½w;b/C138ZT/C0y/C13/C13/C13/C132\n2ð1Þ\nTaking into consideration that MSE is a convex quadratic\nfunction on ½w;b/C138, it has a unique minimum that can be\nobtained by setting to zero its gradient rw;b, whose value is\n½^w;^b/C138¼yZðZTZÞ/C01ð2Þ\nThe Simple Linear Regression case is characterized by a\npolynomial with degree g¼1. The general case of Poly-\nnomial Regression, which adopts a polynomial with degreeg[1, is a special case of Multiple Linear Regression,\nwhereFig. 1 A General Paradigm of Learned Searching in a Sorted Set [ 31].\nThe model is trained on the data in the table. Then, given a query\nelement, it is used to predict an interval in the table of a reduced sizewhere to search for the query (included in brackets in the ﬁgure). Abinary search in the reduced interval is then applied to ﬁnd the\nlocation of the query element in the table\nFig. 2 The Linear Regression Method for Learning a Simple Model.\nAssume that Ais [47, 105, 140, 289, 316, 358, 386, 398, 819, 939].\naThe CDF of A is shown. In the diagram, the rank and value of each\nelement in the table are indicated by their yand xcoordinates,\nrespectively. bBy using linear regression, the values aandbof theequation FðxÞ¼axþbare determined. cThe highest error epsilon\none can ﬁnd using Fisepsilon ¼3, i.e. the largest difference between\na rank of a value in the table and its predicted rank from Fafter\nrounding. For the provided query element x, the interval to search\ninside is given by I¼½FðxÞ/C0epsilon ;FðxÞþepsilon /C138Neural Computing and Applications (XXXX) 35:21399–21414 21403\n123\n\n~GðzÞ¼wzTþb ð3Þ\nwis of size gandz¼½x; ::;xg/C01;xg/C1382Rgis the predictor\nvector for MLR . In this study, we employ linear, quadratic\nand cubic regression models to estimate the function\nFprovided by the underlying CDF . The relevant models\nare speciﬁcally preﬁxed with L, Q, orC, respectively.\n3.1.2 Atomic learned indexes based on neural networksThe function G:R\nm!Rcan be also learned by a NN. We\nconcentrate on Multi-Layer Perceptrons (MLPs) Networksfollowing the paper by Kraska et al [ 27]. MLPs are feed-\nforward networks and their general strategy entails an\niterative training phase where the ~Gapproximation is\nimproved at each iteration. After starting with an initial\napproximation ~G\n0, at each iteration i, an effort is made to\nreduce an error function Eso that Eð~Gi/C01Þ/C21Eð~GiÞ.A\ntraining set Tof examples is used for the minimization of\nE. The process can continue for a ﬁxed number of steps or\nit can be terminated when, given a tolerance d,\njEð~Gi/C01Þ/C0Eð~GiÞj /C20d. We list the fundamental compo-\nnents that deﬁne the employed type of NN in the following.\n1.TOPOLOGY OF THE ARCHITECTURE .\n(a) The standard Perceptron [ 8] with relu activation\nfunction represents the atomic element of our\nNN.\n(b) H, the number of Hidden Layers.\n(c) nhi, the Number of Perceptrons for each hidden\nlayer hi.\n(d) Fully Connected NN, i.e., each Perceptron of\nlayer hiis connected with each Perceptron of the\nnext layer hiþ1.\n2.THE LEARNING ALGORITHM .\n(a) E, the error function that measures how close is\n~GtoG.\n(b) Starting from ~G0, a gradient descent iterative\nprocess that at each step, approximates better and\nbetter Gby reducing E. The parameters of each\nlayer are changed via a backwards and forwardpass. A learning rate characterizes the model,\ni.e., a multiplicative constant of the gradient\nerror.\n3.THE TRAINING SCHEME .\n(a) B, the size of a batch, i.e., the subset of elements\nto extract from the training set T. At each\nextraction of the batch, the model parameters are\nupdated.(b) The number of epochs ne, i.e., the number of\ntimes the training set Tis presented to the NN for\nthe minimization of E.\nA suitable training set is used by the learning algorithm of a\nNN, to carry out the iterative gradient descent process. The\ntraining data for the case of indexing, which is our goal, arein the form of scalar integers. It is necessary to represent a\nscalar integer xwith a vector representation x!to do a\nregression using a NN. As proposed by Kraska et al. for the\nsame indexing problem [ 27],x!is a string holding the\n64-bit binary representation of x.\n3.2 Learned indexes using hierarchical\nstructures\nThe Atomic Learned Indexes have been utilised as the\nbasis for more complex Learned Indexes with a hierar-\nchical structure. Among the many proposals available, forthis research, we consider the three indexes that appear to\nperform better than the others in the Literature [ 24,31]. It\nis to be pointed out that, although all of the Atomic Indexesmentioned in Sect. 3.1can be used as building boxes for\nthe more complex indexes described next, to the best of our\nknowledge, and as already pointed out in the Introduction,none uses NNs.\n3.2.1 The recursive model indexHistorically, the ﬁrst Learned Index proposal is the\nRecursive Model Index, or the RMI [27]. It is a Hierar-\nchical Index with a tree-like structure, as seen in Fig. 4a.\nIts nodes are all Atomic Learned Indexes, just like those\nmentioned in Sect. 3.1. A prediction at each level indicates\nthe model of the following level to be used for the next\nprediction, given a key to search for. The last level is\nachieved after continuing this process from the root.Finally, leaves provide a small interval for searching,\nwhich is given in input to the Binary Search. In order to\nobtain an RMI for a given Sorted Table, many hyperpa-\nrameters must be speciﬁed, such as the number of levels,\nthe number of nodes for each level, and the model toemploy for each node. Which hyperparameters setting is\nbest depends on the real context in which the speciﬁc index\nis employed, and its identiﬁcation via efﬁcient algorithmsis an open problem. A partial solution to this is provided by\nthe software platform CDF-Shop [32]: for a given dataset,\nit returns up to ten RMI s, that are identiﬁed via Combi-\nnatorial Optimization heuristics.21404 Neural Computing and Applications (XXXX) 35:21399–21414\n123\n\n3.2.2 The piece-wise geometric model\nThe Piece-wise Geometric Model Index (abbreviated as\nPGM )[17] is a Learned Index that uses a piece-wise linear\napproximation algorithm [ 12] to estimate the CDF of the\ndata, combined with a bottom-up procedure. The prediction\nerror at each stage of its hierarchy is controlled by a user-\ndeﬁned approximation parameter. An example of the PGM\nis depicted in Fig. 4b and it is obtained as follows. The\npiece-wise linear approximation of the CDF provides three\nsegments. The CDF of each segment is approximated by a\nlinear model. By choosing the lowest values in each of the\nthree segments, a new table is created. This new table is\nagain divided into segments as in the ﬁrst stage. The pro-cess is iterated until the resulting table consists of only one\nsegment. The search for an element starts at the root of the\nPGM where the next model to query is selected until one\nof the segments is reached at the bottom. Then, Binary\nSearch is used to ﬁnalize the query step.\n3.2.3 The radix spline index\nAnother example of a bottom-up method to Learned\nIndexing is the Radix Spline Index ( RSfor short) [ 23],\nwhich, unlike the PGM , estimates the distribution through\na spline curve [ 37]. As seen in Fig. 4c, a spline is con-\nstructed to roughly represent the CDF of the table, and the\nradix table is then utilised to determine which spline points\nshould be used to narrow the search interval. A maximumapproximation error is ensured for the RSusing the user-\ndeﬁned value.\n3.3 Prediction accuracy of a model\nThe approximation error is crucial in minimizing the size\nof the interval to search into, as it is clearly shown in\nFig. 2. The part of the table where the last search must be\ndone gets smaller while the error decreases. The percentageof the table that is no longer taken into account for\nsearching after a prediction is the reduction factor (RF),\nwhich we use in this study to describe the accuracy in theprediction of a model. Because of the variability across the\nmodels to establish the search interval, and in order to\nplace all models on par, we estimate empirically the RFof\na model. In particular, we utilize a batch of queries and the\nmodel to decide how long the interval ( Iin Fig. 2) should\nbe for each query. This allows us to simply calculate thereduction factor for that query. The reduction factor of the\nmodel for the speciﬁed table is then determined by aver-\naging these reduction factors across the full set of queries.\nNote that the machine learning problem related to a learned\nindex prediction does not involve any generalization error,since the problem is formulated in such a way that thepredictions are always computed on the same dataset used\nfor the training. As a consequence, the RF is always\nintended for the training.\n4 Experimental procedure\n4.1 Hardware\nA workstation with an Intel Core i7-8700 3.2GHz CPU and\nan Nvidia Titan V GPU has been used for the experiments.\nA total of 32 Gbytes of DDR4 serve as the system memory.\nAdditionally, the GPU is equipped with 12 Gbytes of\nDDR5 RAM on its own, and it uses the CUDA parallel\ncomputing framework. A PCIe 3 bus with a 32Gbyte/sbandwidth connects the CPU and GPU. Ubuntu LTS 20.04\nis used as the operating system.\n4.2 Datasets\nWe need to recall from Sect. 3.1.2 that an NN used in this\nresearch on a given integer dataset, requires that the 64 bits\nrepresentation of each integer is transformed into a binary\nvector with 64 entries. Therefore, after training, the NNsize is considerably larger than the original dataset used for\ntraining, having for this latter the possibility of packing\neach element in one memory word. Now, there exist largeand real datasets that are de facto standards in terms of\nbenchmarking of Learned Indexes [ 31]. However, they\nhave a dimension in the Gigabytes and therefore, they aretoo large to be used in conjunction with NNs as Atomic\nLearned Indexes. This brings to light a limitation: NNs can\nbe used only on relatively small datasets, i.e., size in theMbs.\nGiven all of the above, in order to benchmark NNs for\nthis study, we have resorted to a choice of datasets thathave also appeared in the Learned Indexing Literature at\nthe early stages of its development, e.g., [ 17,27]. In par-\nticular, as detailed next, we use real and artiﬁcial datasets.The ﬁrst type has a size that NNs can work with, while the\nsize of the second type can be determined so that NNs have\nno space problem.\n1.Unicollects data sampled from a Uniform distribution,\ndeﬁned as\nUðx;a;bÞ¼(\n1\nb/C0aifx2½a;b/C138\n0 otherwiseð4Þ\nwhere a¼1e b¼2r/C01/C01, where ris the CPU\ninteger precision. It contains 1.05e ?06 integers and\nhas a size of 1.10e ?04 Kb.Neural Computing and Applications (XXXX) 35:21399–21414 21405\n123\n\n2.Logn collects data sampled from a Log-normal distri-\nbution, deﬁned as\nLðx;l;rÞ¼e/C0ðlnx/C0lÞ2\n2r2\nxﬃﬃﬃ\n2p\nprð5Þ\nwhere l¼0 is the means and r¼1 is the variance of\nthe distribution. It contains 1.05e ?06 integers and has\na size of 1.05e ?04 Kb.1.Real-wl collects timestamps of /C24715 M requests\nperformed by a web server. It contains 3.16e ?07\nintegers and has a size of 3.48e ?05 Kb.\n2.Real-iot collects timestamps of /C2426 M events\nrecorded by IoT sensors. It contains 1.52e ?07 integers\nand has a size of 1.67e ?05 Kb.\nThe CDFs of each of those datasets are reported in Fig. 5.\nAs evident from that ﬁgure, the logn dataset has a CDF that\nmay be challenging to learn, as stated in [ 27], while the\nother datasets follow a uniform distribution and their CDF\ncan be considered easy to learn.Fig. 4 Examples of different hierarchical learned indexes. aARMI\nexample with two layers and a branching factor b. The top box\nindicates that a linear function is used to choose the lower models.\nRegarding the leaf boxes, each one identiﬁes which of the AtomicLearned Indexes is applied to the relevant part of the table wheremaking a prediction. bAn illustration of a PGM index. The table is\nbroken into three segments at the bottom. In this manner, a new\ntable is built, and the procedure is repeated. cARSillustration. Thebins at the top are where the elements fall according to their top three\ndigits. A linear spline with appropriately selected spline points that\napproximates the CDF of the data is shown at the bottom. Each\nbucket points to a spline point, and as a consequence when a queryelement falls into a bucket (let’s say six), the search interval isrestricted by the spline points pointed by that bucket and the one\nbefore it (ﬁve in our example)Fig. 3 The neural network architectures used in this research. We use the notation: a NN0 for no hidden layers; b NN1 for one hidden layer; and\nc NN2 for two hidden layers. The number of input neurons is 64, each layer has 256 units21406 Neural Computing and Applications (XXXX) 35:21399–21414\n123\n\nFor each of the aforementioned tables, the query dataset\nis equally divided across elements that are present in the\ntable and those that are absent. Its size is equal to 50% of\nthe reference dataset. The query datasets for all experi-ments are not sorted.\n4.3 Atomic learned indexes and binary search\nWe use the standard Binary Search strategy, and in addi-tion to it, Uniform Binary Search [ 22,25] for the last\nsearch step. They are the routines BSandUSof Sect. 2.1,\nrespectively. In terms of Atomic Learned Indexes, weemploy L, Q, andC. As for NNs, and as mentioned\nalready, we follow the initial proposal by Kraska et al. [ 27],\nand use only Multi-layer Perceptron Networks with anincreasing complexity of architecture in terms of hidden\nlayers, as detailed below. Speciﬁcally, NN0 denotes an NN\nwith no hidden layers (see Fig. 3a),NN1 one with one\nhidden layer (Fig. 3b), and NN2 one with two hidden\nlayers (Fig. 3c). Each layer has 256 units. For the sake of\nclarity, we anticipate that, as is evident from the resultsdiscussed in Sect. 5.2.2 , no further studies are needed\nregarding the investigation of network hyperparameters or\nmore complex architectures such as the Convolutional(CNN), as even the simplest networks considered here turn\nout not to be competitive in terms of query time with the\nother models investigated. Two Atomic Learned Indexesare provided by each of the models mentioned above, one\nfor each Binary Search routine utilized in the ﬁnal search\nstage. However, US may be streamlined to prevent\n‘‘branchy’’ instructions in its implementation, according to\nKhuong and Morin [ 22], yielding better performance with\nrespect to BS. For this reason, it performs better than BSin\nour experiments and, for conciseness, we report only the\nexperiments regarding US. Given an input table, all Atomic\nIndexes are built according to the procedures outlined inSect. 3.1.\n4.4 Hierarchical learned indexes\nAs for Hierarchical Indexes, we consider RMI ,PGM and\nRS. We use the Search on Sorted Data (SOSD for short)\n[31] platform for model training of the PGM and the RS,\nfor a given table. However, as already mentioned, for the\ntraining of the RMI , we use CDF-Shop . As for query\nprocessing, for each Hierarchical Index, each batch of\nqueries is executed within SOSD . It should be noted that\nthe main version of the SOSD platform only offers a\nUniform Binary Search implementation as the ﬁnal stage of\nthe model that utilises the standard C ?? lower_bound\nprocedure. A version of this platform suitably modiﬁed forthis research is described in [ 6].5 Experiments and findings\nThe datasets outlined in Sect. 4.1have been considered. To\nuse them as input for the NNs, both training and query\ndatasets are modiﬁed as described at the end of Sect. 3.1.2 .\nIn regard to both training and query times, we report the\naverage per element. That is, for query processing, we take\nthe total time to process a batch with a given Learned Indexand then divide that time by the number of items in the\nbatch. An analogous procedure is followed for training on\nan input dataset. This method of collecting timing results isin agreement with the Literature since it assures a reliable\nmeasure of time performance [ 31]. We report results\nregarding both training and querying as follows.\n•Training . A comparison between GPU and CPU\ntraining is performed and reported in Sect. 5.1.I n\nparticular, we use the highly engineered Tensorﬂow\nplatform with GPU support to train NNs Atomic\nIndexes. As far as L,QandCare concerned, we use\na CPU implementation for their training.\n•Batch Query Processing . We perform four different\nkinds of experiments in order to study the competitive-ness of NNs Atomic Indexes with respect to the otherTable 1 Training time and reduction factor for atomic learned\nindexes\nUni Logn\nTT (s) RF (%) TT (s) RF (%)\nNN0 2.55e /C004 94.08 1.39e /C004 54.40\nNN1 4.18e /C004 99.89 3.79e /C004 94.21\nNN2 4.49e /C004 99.87 8.60e /C004 97.14\nL 8.20e /C008 99.94 5.61e /C008 77.10\nQ 1.27e /C007 99.98 1.02e /C007 90.69\nC 1.84e /C007 99.97 1.74e /C007 95.76\nReal-wl Real-iot\nTT (s) RF (%) TT (s) RF (%)\nNN0 2.50e /C004 99.99 1.28e /C004 89.90\nNN1 2.31e /C004 99.88% 4.20e /C004 98.54\nNN2 2.33e /C004 99.80 3.57e /C004 97.31\nL 5.82e /C008 99.99 7.70e /C008 96.48\nQ 1.14e /C007 99.99 1.25e /C007 99.10\nThe training time for each element, indicated in seconds (column TT\n(s)), and the percentage of table reduction (column RF (%) ), as\ndeﬁned in Sect. 3.3, are displayed for each dataset, indicated in the\nﬁrst row of the tables, and each model, indicated in the ﬁrst column.L,QandCare the linear, quadratic and cubic atomic models while\nNN0, NN1, NN2 indicate NN models with 0, 1 and 2 hidden layers\nrespectivelyNeural Computing and Applications (XXXX) 35:21399–21414 21407\n123\n\nLearned Indexing models considered in this research,\nand also in regard to the various hardware and softwaresolutions that we have available.\n–TensorFlow . In order to do query searches with the\nLearned Indexes based on NNs, we have carried outquery experiments using Tensorﬂow with GPU\nsupport. Because of the overhead of uploading\nTensorﬂow to the GPU, results are quite poor andhence not reported. This is consistent with the\noutcomes mentioned in [ 27]. For this reason,\nTensorﬂow was exclusively utilized to train NNs.\n–GPU and NNs vs Parallel Binary Search .W e\nemploy two C ?? CUDA implementations, one of\nthe Learned Index corresponding to NN0 , and the\nother relative to a parallel version of BS. Sec-\ntion 5.2.1 shows a report of the ﬁndings highlightingthat using the GPU is not advantageous, also in\ncomparison with the baseline BSimplemented in\nCUDA. As a result, no additional GPU experiments\nhave been carried out. We point out that here we use\nBSdue to the technical difﬁculties a C ?? CUDA\nimplementation of USimposes, de facto making it\nanalogous to BS.\n–CPU only .\nAtomic Learned Indexes . We have carried out\nall of the experiments using the Atomic Learned\nIndexes considered for this research. For the sakeof clarity and as anticipated, because the results\nonBSwould contribute very little to the discus-\nsion, we only provide the results using US.\nSection 5.2.2 reports and discusses them.\nTable 2 Training time and\nreduction factor of neuralnetworks for different sizes ofthe logn training setSampling percentage NN0 NN1 NN2\nTT (s) RF (%) TT (s) RF (%) TT (s) RF (%)\n25% 4.77e /C004 15,76% 4.07e /C003 63,42 4.44e /C003 83,71\n50% 2.6e /C004 17,09% 2.43e /C003 64,23 3,57e-03 83,94\n75% 3.13e /C004 16,52% 2.42e /C003 63.90 5,06e-03 82.61\n100% 1.39e /C004 54.40% 3.79e /C004 94.21 8.60e /C004 97.14\nThe ﬁrst column indicates the sample size, while the remaining part of the table is organized as Table 1Fig. 5 Datasets CDF. For each\ndataset used in this paper, wereport in the x-axis the value of\neach element in the dataset and\nin the y-axis, its cumulative\nprobability computed as in [ 27]21408 Neural Computing and Applications (XXXX) 35:21399–21414\n123\n\nInsights into the Learning the Complex CDF of\na Sorted Table. The Hierarchical Learned\nIndexes, i.e., RMI ,PGM andRS, are the most\ncompetitive in terms of query time. Therefore, itis natural to ask what is the gap that NNs have in\nperformance with respect to the most advanced\nLearned Indexes. As a complement to thisassessment, we also show that the choice of a\nLearned Index for a given Sorted Table depends\ncritically on the complexity of the CDF that needs\nto be learned. Section 5.2.3 reports and discusses\nthe ﬁndings of this set of experiments.\n5.1 Training: GPU versus CPU\nThe training times for the experiments outlined at the\nbeginning of this section are reported in Table 1, along\nwith the corresponding RF, which was calculated as\nexplained in Sect. 3.3. The training time for Atomic\nLearned Indexes L,QandCis equal to the time required to\nsolve Eq. ( 2). The stochastic gradient descent learningapproach is employed for NN models, with a momentum\nparameter of 0.9 and a learning rate of 0.1. A number ofepochs equal to 2000 are used with a batch size of 64. All\nthose values have been set via trial and error, observing\nthat increasing batch size and number of epochs does notlead to improvements. As shown by the ﬁndings listed in\nTable 1, the size of the employed NNs affects the reduction\nfactor and training time. In particular, the more layers, thebetter the reduction factor and the higher the training time.\nMoreover, NNs are not competitive with the L,Qand\nCAtomic Learned Indexes, both in training time and RF.\nIn regard to training time, this lack of performance holds\neven with GPU support and the use of the highly-engi-\nneered Tensorﬂow platform. In fact, for each dataset, thetraining time for NNs is four orders of magnitude longer\nthan that for non-NN Atomic Learned Indexes with a\nsimilar RF, although those latter use a CPU and therefore\nthey do not beneﬁt from parallelism. Since the training of\nNN models is performed via Tensorﬂow, it is not possible,\nto the best of our knowledge, to proﬁle the CPU-GPU I/Otime, i.e., the time to input the data and get the trained NN,\nand compare it with the training phase that is performed in\nparallel within the GPU. Despite such a shortcoming and inFig. 6 Training Curve on a Sample of Size 25% of the logn dataset. Each ﬁgure reports the MSE (ordinate) over the epochs (abscissa) for the\ngiven training sample. NN0 ,NN1 andNN2 are reported from left to right\nTable 3 A comparison of\nprediction accuracy on the logn\ndataset: atomic learned indexes\nversus hierarchical onesIndexes Logn (%)\nNN2 97.14\nC 95.76RMI 99.99PGM 99.99RS 99.99\nFor conciseness, only the most\ncomplex Atomic Learned\nIndexes are reported in the ﬁrst\ntwo rows of the table ( NN2, C ).\nThe hierarchical models areRMI, PGM, RS . The values in\nthe table are the RFof each of\nthe considered Indexes on thelogn datasetTable 4 Query time on GPUs\nMethods Copy (s) Op. (s) Search (s) Query (s)\nNN0-BS 3.27e /C008 4.20e /C009 1.84e /C009 3.27e /C008\nBS 2.55e /C009 - 1.89e /C009 4.44e /C009\nBinary Search using NN0 as the prediction step is referred to as NN0-\nBS, whereas GPU-based parallel Binary Search without a prior pre-\ndiction is indicated as BS. We report various time results, in seconds\nand per element (averaged over an entire batch of queries), as follows.The time for CPU-GPU copy operations and vice versa (column Copy\n(s)); the time for math operations (column Op. (s) ), the time to search\ninto the interval with BS(column Search (s) ), and the overall time to\nﬁnish the query process (column Query (s) )Neural Computing and Applications (XXXX) 35:21399–21414 21409\n123\n\nview of the results reported in Sect. 5.2.1 , it is reasonable\nto justify such a result with the fact that the I/O timedominated the GPU training time, to the point of making\nthe use of parallelism detrimental. Given the order of\nmagnitude difference in training time between NN andother Atomic Models training, such a picture is unlikely tochange even if the I/O bandwidth CPU/GPU increases by\nconstant factors.\nRegarding the RFperformance, the support for NN use\nwas motivated by Kraska et al. with their ability to learn‘‘complex patterns‘‘ in the data that could not be captured\nby Atomic Models based on regression. This may be the\ncase, as indicated by comparing the RFperformance of the\nNNs on the logn dataset with that of the other Atomic\nLearned Indexes.\nAs far as NNs are concerned, it is of interest to assess\nhowTTandRFvary with respect to the size of the training\nset. We have experimented only with the ‘‘most difﬁcult’’of the datasets, i.e., logn. Speciﬁcally, for each of NNO ,\nNN1 andNN2 , different sizes of the training set have been\nconsidered, ranging from 25 %to 100 %. The results are\nreported in Table 2. It is clear that the RFs increase as the\nsize of the training set and the complexity of the networks\nincrease, with a corresponding increase in TT. It is worth\nrecalling from Sect. 3.3that the machine learning problem\nrelated to the learned index prediction does not involve any\ngeneralization error, so the RFon validation or test are not\nprovided. For completeness, Figs. 6-9depict the MSE of\nthe three models with respect to the learning epochs, on\ndifferent training set sizes.\nGiven all of the above regarding NNs, it seems to be\nmore proﬁtable, in terms of prediction accuracy, to resort to\nthe Hierarchical Learned Indexes that divide the CDF to belearned into pieces, possibly obtaining a set of ‘‘simpler‘‘\ncurves to learn. Such a fact is illustrated in Table 3for the\nlogn dataset, where the Hierarchical Learned Indexes have\na nearly perfect prediction. Note that, in this table, we\nreport only the reduction factor of the NN2 model because\nit has shown the best results in terms of training time andreduction factor in regard to the experiments conducted on\nall the datasets, reported in Table 1.\n5.2 Query\n5.2.1 GPU and NNs versus parallel binary search\nAssuming as a baseline a simple parallel implementation of\nBinary Search, i.e., BS, in order to determine whether there\nis actually a beneﬁt to using the GPU for queries in con-\njunction with NNs, we conduct an experiment by com-paring the former classic and simple parallel routine with\nNN0 on the simplest of the datasets to learn, i.e., uni. Such\na choice provides an advantage to the NN. As alreadymentioned, we could not use the Uniform version of Binary\nSearch, i.e., US, on the GPU because it was difﬁcult to\nimport within CUDA. The results are reported in Table 4.\nThey clearly show that on GPUs, the usage of NNs on this\narchitecture is superﬂuous because a traditional parallelTable 5 Query time of NN atomic learned indexes on CPU for each\nof the considered datasets\nDataset US NN0-US NN1 NN2\nUni 2.81e /C007 1.31e /C007 1.56e /C006 5.16e /C006\nLogn 2.08e /C007 1.92e /C007 1.69e /C006 5.24e /C006\nReal-wl 3.38e /C007 4.59e /C007 Space Error Space Error\nReal-iot 3.07e /C007 4.76e /C007 1.90e /C006 1.94e /C005\nThe datasets are indicated in the ﬁrst column. The time taken by\nUniform Binary Search alone is indicated in the column named US,\nwhile its version using NN0 as the prediction step is indicated by the\ncolumn NN0-US . The other columns refer to the time taken by NN1\nandNN2 only for the interval prediction. The time is in seconds and is\nper query element (averaged over an entire batch of queries). Thelabel Space Error indicates the case when the queries are too big to ﬁt\nin the main memory\nTable 6 Query time of non-NN atomic learned indexes on CPU for\neach of the considered datasets\nDataset US L-US Q-US C-US\nUni 2.81e /C007 9.42e /C008 8.11e /C008 9.39e /C008\nLogn 2.08e /C007 1.60e /C007 1.59e /C007 1.54e /C007\nReal-wl 3.38e /C007 5e05e-08 2.12e /C07 1.80e /C07\nReal-iot 3.07e /C007 8.32e /C008 1.99e /C07 2.57e /C07\nResults with Linear, Quadratic and Cubic models are reported and\nindicated in columns L-US ,Q-US ,C-US respectively. The ﬁrst two\ncolumns are the same as in Table 5. Every time in the Table is rep-\nresented in seconds and is per query element (averaged over an entire\nbatch of queries)\nTable 7 Query time of hierarchical indexes on CPU for each of the\nconsidered datasets\nDataset US RMI PGM RS\nUni 2.81e /C007 1.5e /C007 1.62e /C007 1.66e /C007\nLogn 2.08e /C007 1.45e /C007 1.59e /C007 1.66e /C007\nReal-wl 3.38e /C007 1.11e /C007 1.26e /C007 1.59e /C07\nReal-iot 3.07e /C007 1.36e /C007 1.45e /C007 1.68e /C07\nThe columns RMI ,PGM andRSreport the results with the con-\nsidered Hierarchical Learned Indexes. The ﬁrst two columns are the\nsame as in Table 5. Every time in the Table is represented in seconds\nand is per query element (averaged over an entire batch of queries)21410 Neural Computing and Applications (XXXX) 35:21399–21414\n123\n\nBinary Search on the GPU is by itself faster than its\nLearned counterparts. It is also of interest to point out that\ncopy operations from CPU to GPU and vice versa cancel\nout the one order of magnitude speedup of maths opera-tions. The lectures to be learned from this experiment are\nrather subtle. Indeed, it is the case that GPU code execution\nfavours straight-line math operations (and therefore NNs)with respect to if-then-else constructs (Binary Search).\nHowever, transfer from the Main Memory to the GPU may\nplay a key role in cancelling eventual time gains. More-\nover, recalling that the joint usage of GPU and NNs was\nonce again one of the motivations to resort to LearnedIndexing approaches, we ﬁnd that when one is willing to\nuse the GPU, a simple parallel implementation of standard\nBinary Search is enough. That is, the Learned Indexingframework is certainly of success, but not with the com-\nponents initially envisaged. In particular, GPU usage.\n5.2.2 CPU only: atomic learned indexes\nWe take Uniform Binary Search USas a baseline to\ncompare against the Atomic Learned Indexes. Such a\nchoice is motivated by the fact that results reported in [ 22]\nindicate that USis usually faster than BSon modern\ncomputer architectures. It results convenient to report the\nexperiments regarding NNs separately (Table 5) with\nrespect to the ones regarding the other Atomic Indexes(Table 6). From those results, we have that only NN0 is\ncompetitive on the artiﬁcial datasets with respect to the\nbaseline. As for the other two NNs, either they are not\ncompetitive even for prediction time only, or they run outof space. This latter fact is due to the coding of the query\ndata that must be used for its use in conjunction with the\nNNs and that causes an expansion of the query set size, (seeSect. 3.1.2 ). Therefore, NNs more complex than NN0 are\nslow in time and costly in space. As for the remaining\nAtomic Indexes, they all report a gain in time with respect\nto the baseline. In conclusion, NNs are not very competi-\ntive as Atomic Learned Indexes.\n5.2.3 CPU only: insights into the complexity of learning\nthe CDF of a sorted table\nTable 7contains the ﬁndings of the query experiments\nperformed with the use of the Hierarchical Learned Indexeson the datasets considered for the Atomic Indexes. A\ncomparison with Table 5shows that NNs are not com-\npetitive with Hierarchical Learned Indexes unless the CDFof the input Table is particularly easy to learn (see the CDF\nofuniin Fig. 5). In this case, only NN0 is worth consid-\neration. It is also of interest to compare the HierarchicalIndexes with the remaining Atomic ones, whose results are\nreported in Table 6. With the exclusion of the somewhat\ndifﬁcult-to-learn table logn, the Atomic LIndex is muchFig. 7 Training curve on a sample of size 50% of logn dataset. The legend is as in Fig. 6\nFig. 8 Training curve on a sample of size 75% of the logn dataset. The legend is as in Fig. 6Neural Computing and Applications (XXXX) 35:21399–21414 21411\n123\n\nbetter than the Hierarchical Learned Indexes. This is due to\nthe regularity of the CDFs to be learned, not requiringcomplicated models.\nIn order to better illustrate the above point, we have\nperformed an additional experiment with the use of theOpen Street Map ( osm for short) dataset. It is the most\ndifﬁcult in terms of the CDF, among the real benchmark\ndatasets used in [ 31] and that, as already pointed out, are\ntoo large for the NNs. This dataset comprises 200 M\n64-bits integers representing the cell IDs of embedded\nlocations in Open Street Map. Its CDF is depicted inFig. 10. The query time results of the Hierarchical LearnedIndexes and of the Regression-based ones are reported in\nTable 8. The QIndex has been excluded for brevity since\nit performs as the other two Atomic ones. As evident from\nthose results, the Hierarchical Learned Indexes are nowcompetitive with respect to the Atomic ones.\nThe lesson to be drawn here is that NNs are far from\nbeing competitive with Hierarchical Indexes which, in turn,are competitive with respect to Atomic Learned Indexes\nonly on Sorted Tables with a complex CDF curve to learn.\n6 Conclusions\nA perceived paradigm shift is one of the motivations for theintroduction of the Learned Indexes. Despite that, the use\nof a GPU architecture for Learned Indexes based on NNsseems not to be appropriate when we use generic NNs as\nwe have done here. It is to be pointed out that certainly, the\nuse of a GPU accelerates the performance of math opera-tions, but the data transfer between CPU and GPU is a\nbottleneck in the case of NNs: not only data but also the\nsize of the model matters. When we consider CPU only,NN models are not competitive with very simple models\nbased on Linear Regression. This research clearly points to\nthe need to design NN architectures specialized forLearned Indexing, as opposed to what happens for Bloom\nFilters where generic NN models guarantee good perfor-\nmance to their Learned versions. In particular, those newNN models must be competitive with the Atomic Learned\nIndexes based on Linear Regression, which are widely usedFig. 9 Training curve on the full logn dataset. The legend is as in Fig. 6\nFig. 10 OSM Dataset CDF. The legend is as in Fig. 5\nTable 8 Query time of\nhierarchical indexes on difﬁcult\ndatasets and a comparison with\natomic indexesDataset US L C RMI PGM RS\nOSM 6.85e /C007 6.67e /C007 5.49e /C007 2.75e /C007 1.72e /C007 1.71e /C007\nLogn 2.08e /C007 1.60e /C007 1.54e /C007 1.45e /C007 1.59e /C007 1.66e /C007\nThe Table reports query times of the search alone (column US), of Atomic Indexes (columns LandC,\nrespectively), and of Hierarchical Learned Indexes (columns RMI ,PGM andRS, respectively) on two\n‘‘difﬁcult’’ datasets, one real ( osm) and one synthetic ( logn). Every time in the Table is represented in\nseconds and is per query element (averaged over an entire batch of queries)21412 Neural Computing and Applications (XXXX) 35:21399–21414\n123\n\nas building blocks of more complex models [ 4,7,31]. It is\nto be remarked that we have considered the static case\nonly, i.e., no insertions or deletions are allowed in the table.\nThe dynamic case has also been considered in the litera-ture, i.e., [ 15,17]. However, for that setting, no NN solu-\ntion is available. In conclusion, this study provides solid\ngrounds and valuable indications for the future develop-ment of Learned Data Structures, which would include a\npervasive presence of NNs.\nAcknowledgements This research is funded in part by the MIUR\nProject of National Relevance 2017WR7SHH ‘‘Multicriteria Data\nStructures and Algorithms: from compressed to learned indexes, and\nbeyond’’. We also acknowledge an NVIDIA Higher Education andResearch Grant (donation of a Titan V GPU). Raffaele Giancarlo isalso partially supported by INDAM- GNCS Project 2023 ‘‘Approcci\ncomputazionali per il supporto alle decisioni nella Medicina di Pre-\ncisione’’. Giosue `Lo Bosco is also supported by the University of\nPalermo FFR (Fondo Finalizzato alla ricerca di Ateneo) year 2023.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and thesource, provide a link to the Creative Commons licence, and indicate\nif changes were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unlessindicated otherwise in a credit line to the material. If material is notincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyrightholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/ .\nFunding Open access funding provided by Universita `degli Studi di\nPalermo within the CRUI-CARE Agreement.\nData Availability The data that support the ﬁndings of this study are\navailable on request to the corresponding author.\nReferences\n1. Abadi M (2015) Tensorﬂow: large-scale machine learning on\nheterogeneous distributed systems. http://download.tensorﬂow.\norg/paper/whitepaper2015.pdf\n2. Aho AV, Hopcroft JE, Ullman JD (1974) The design and analysis\nof computer algorithms\n3. Amato D (2022) A tour of learned static sorted sets dictionaries:\nfrom speciﬁc to generic with an experimental performance\nanalysis. Ph.D. thesis\n4. Amato D, Lo Bosco G, Giancarlo R (2021) Learned sorted\ntable search and static indexes in small model space. In: AIxIA\n2021—advances in artiﬁcial intelligence: 20th internationalconference of the Italian association for artiﬁcial intelligence,virtual event, December 1–3, 2021, Revised Selected Papers.\nSpringer, Berlin, Heidelberg, pp 462–477\n5. Amato D, Lo Bosco G, Giancarlo R (2022) On the suitability of\nneural networks as building blocks for the design of efﬁcientlearned indexes. In: Iliadis L, Jayne C, Tefas A, Pimenidis E (eds)Engineering applications of neural networks. Springer, Cham,\npp 115–127\n6. Amato D, Lo Bosco G, Giancarlo R (2023) Standard versus\nuniform binary search and their variants in learned static index-\ning: the case of the searching on sorted data benchmarking\nsoftware platform. Softw Pract Exp 53(2):318–346\n7. Amato D, Giancarlo R, Lo Bosco G (2023) Learned sorted\ntable search and static indexes in small-space data models. Data\n8(3)\n8. Bishop CM (1995) Neural networks for pattern recognition.\nOxford University Press, New York\n9. Bloom BH (1970) Space/time trade-offs in hash coding with\nallowable errors. Commun ACM 13(7):422–426\n10. Boffa A, Ferragina P, Vinciguerra G (2021) A ‘‘learned’’\napproach to quicken and compress rank/select dictionaries. In:Proceedings of the SIAM symposium on algorithm engineering\nand experiments (ALENEX)\n11. Broder A, Mitzenmacher M (2003) Network applications of\nbloom ﬁlters: a survey. Internet Math 1(4):485–509\n12. Chen DZ, Wang H (2009) Approximating points by a piecewise\nlinear function. Algorithmica 66:682–713\n13. Cormen TH, Leiserson CE, Rivest RL, Stein C (2009) Intro-\nduction to algorithms. The MIT Press, New York\n14. Dai Z, Shrivastava A (202) Adaptive learned bloom ﬁlter (Ada-\nBF): efﬁcient utilization of the classiﬁer with application to real-time information ﬁltering on the web. In: Larochelle H, RanzatoM, Hadsell R, Balcan MF, Lin H (eds) Advances in neural\ninformation processing systems, vol 33. Curran Associates, Inc.,\npp 11700–11710\n15. Ding J, Minhas UF, Yu J, Wang C, Do J, Li Y, Zhang H,\nChandramouli B, Gehrke J, Kossmann D, Lomet D, Kraska T\n(2020) Alex: an updatable adaptive learned index. In: Proceed-ings of the 2020 ACM SIGMOD international conference onmanagement of data, SIGMOD ’20, New York. Association for\nComputing Machinery, pp 969–984\n16. Ferragina P, Vinciguerra G (2020) Learned data structures. In:\nRecent trends in learning from data. Springer, pp 5–41\n17. Ferragina P, Vinciguerra G (2020) The PGM-index: a fully-dy-\nnamic compressed learned index with provable worst-case\nbounds. PVLDB 13(8):1162–1175\n18. Freedman D (2005) Statistical models: theory and practice.\nCambridge University Press, Cambridge\n19. Fumagalli G, Raimondi D, Giancarlo R, Malchiodi D, Frasca M\n(2022) On the choice of general purpose classiﬁers in learnedbloom ﬁlters: an initial analysis within basic ﬁlters. In: Pro-\nceedings of the 11th international conference on pattern recog-\nnition applications and methods (ICPRAM), pp 675–682\n20. Goodfellow I, Bengio Y, Courville A (2016) Deep learning. The\nMIT Press, New York\n21.http://tinyurl.com/bench-atomic-learned-indexes . Last accessed\n06, Feb 2023\n22. Khuong PV, Morin P (2017) Array layouts for comparison-based\nsearching. J Exp Algorithmics 22:1.3:1-1.3:39\n23. Kipf A, Marcus R, van Renen A, Stoian M, Kemper A, Kraska T,\nNeumann T (2020) Radixspline: a single-pass learned index. In:Proceedings of the third international workshop on exploiting\nartiﬁcial intelligence techniques for data management, aiDM ’20.\nAssociation for Computing Machinery, pp 1–5\n24. Kipf A, Marcus R, van Renen A, Stoian M, Kemper A, Kraska T,\nNeumann T. SOSD Leaderboard. https://learnedsystems.github.\nio/SOSDLeaderboard/leaderboard/\n25. Knuth DE (1976) The art of computer programming, vol 3\n(Sorting and Searching)\n26. Kraska T, Alizadeh M, Beutel A, Chi EH, Ding J, Kristo A,\nLeclerc G, Madden S, Mao H, Nathan V (2021) Sagedb: a learned\ndatabase systemNeural Computing and Applications (XXXX) 35:21399–21414 21413\n123\n\n27. Kraska T, Beutel A, Chi EH, Dean J, Polyzotis N (2018) The case\nfor learned index structures. In: Proceedings of the 2018 inter-\nnational conference on management of data. ACM, pp 489–504\n28. Kraska T (2021) Towards instance-optimized data systems. Proc.\nVLDB Endow. 14(12):3222–3232\n29. LeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature\n521(7553):436\n30. Maltry M, Dittrich J (2022) A critical analysis of recursive model\nindexes. Proc VLDB Endow 15(5):1079–1091\n31. Marcus R, Kipf A, van Renen A, Stoian M, Misra S, Kemper A,\nNeumann T, Kraska T (2020) Benchmarking learned indexes.Proc VLDB Endow 14(1):1–13\n32. Marcus R, Zhang E, Kraska T (2020) CDFShop: exploring and\noptimizing learned index structures. In: Proceedings of the 2020ACM SIGMOD international conference on management of data,SIGMOD’20, pp 2789–2792\n33. Mehlhorn K, Tsakalidis A (1991) Data structures. In: Handbook\nof theoretical computer science, vol. A: algorithms and com-plexity. MIT Press, Cambridge, pp 302–341\n34. Mitzenmacher M (2018) A model for learned bloom ﬁlters and\noptimizing by sandwiching. In: Bengio S, Wallach H, LarochelleH, Grauman K, Cesa-Bianchi N, Garnett R (eds) Advances inneural information processing systems, vol 31. Curran Associ-\nates, Inc\n35. Mitzenmacher M, Vassilvitskii S (2020) Algorithms with pre-\ndictions. CoRR: abs/2006.0912336. Moore GE (1965) Cramming more components onto integrated\ncircuits. Electronics 38:8\n37. Neumann T, Michel S (2008) Smooth interpolating histograms\nwith error guarantees\n38. Ohn I, Kim Y (2019) Smooth function approximation by deep\nneural networks with general activation functions. Entropy21(7):627\n39. Peterson WW (1957) Addressing for random-access storage. IBM\nJ Res Dev 1(2):130–146\n40. Sato K, Young C, Patterson D (2017) An in-depth look at Goo-\ngle’s ﬁrst tensor processing unit. https://cloud.google.com/blog/\nproducts/ai-machine-learning/an-in-depth-look-at-googles-ﬁrst-\ntensor-processing-unit-tpu\n41. Schulz L, Broneske D, Saake G (2018) An eight-dimensional\nsystematic evaluation of optimized search algorithms on modernprocessors. Proc VLDB Endow 11:1550–1562\n42. Vaidya K, Knorr E, Kraska T, Mitzenmacher M (2020) Parti-\ntioned learned bloom ﬁlter. ArXiv: abs/2006.03176\n43. Wang B (2017) Moore’s law is dead but GPU will get 1000x\nfaster by 2025. https://www.nextbigfuture.com/2017/06/moore-\nlaw-is-dead-but-gpu-will-get-1000x-faster-by-2025.html\nPublisher’s Note Springer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.21414 Neural Computing and Applications (XXXX) 35:21399–21414\n123",
  "textLength": 61792
}