{
  "paperId": "e44262fec6b4997b29a8207f4d580a63d31545c5",
  "title": "Designing Succinct Secondary Indexing Mechanism by Exploiting Column Correlations",
  "pdfPath": "e44262fec6b4997b29a8207f4d580a63d31545c5.pdf",
  "text": "Designing Succinct Secondary Indexing Mechanism\nby Exploiting Column Correlations\n(Extended Version)\nYingjun Wu\nIBM Research - Almaden\nyingjun.wu@ibm.comJia Yu∗\nArizona State University\njiayu2@asu.eduYuanyuan Tian\nIBM Research - Almaden\nytian@us.ibm.com\nRichard Sidle\nIBM\nricsidle@ca.ibm.comRonald Barber\nIBM Research - Almaden\nrjbarber@us.ibm.com\nABSTRACT\nDatabase administrators construct secondary indexes on data\ntables to accelerate query processing in relational database\nmanagement systems (RDBMSs). These indexes are built\non top of the most frequently queried columns according\nto the data statistics. Unfortunately, maintaining multiple\nsecondary indexes in the same database can be extremely\nspace consuming, causing significant performance degrada-\ntion due to the potential exhaustion of memory space. In\nthis paper, we demonstrate that there exist many opportu-\nnities to exploit column correlations for accelerating data\naccess. We propose Hermit , a succinct secondary indexing\nmechanism for modern RDBMSs. Hermit judiciously lever-\nages the rich soft functional dependencies hidden among\ncolumns to prune out redundant structures for indexed key\naccess. Instead of building a complete index that stores ev-\nery single entry in the key columns, Hermit navigates any\nincoming key access queries to an existing index built on the\ncorrelated columns. This is achieved through the Tiered Re-\ngression Search Tree ( TRS-Tree ), a succinct, ML-enhanced\ndata structure that performs fast curve fitting to adaptively\nand dynamically capture both column correlations and out-\nliers. We have developed Hermit in two different RDBMSs,\n∗Work done during an internship at IBM Research - Almaden.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear\nthis notice and the full citation on the first page. Copyrights for components\nof this work owned by others than ACM must be honored. Abstracting with\ncredit is permitted. To copy otherwise, or republish, to post on servers or to\nredistribute to lists, requires prior specific permission and/or a fee. Request\npermissions from permissions@acm.org.\nSIGMOD’19, June 2019, Amsterdam, The Netherlands\n©2019 Association for Computing Machinery.\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnnand our extensive experimental study confirmed that Her-\nmitcan significantly reduce space consumption with limited\nperformance overhead in terms of query response time and\nindex maintenance time, especially when supporting com-\nplex range queries.\nACM Reference Format:\nYingjun Wu, Jia Yu, Yuanyuan Tian, Richard Sidle, and Ronald Bar-\nber. 2019. Designing Succinct Secondary Indexing Mechanism by\nExploiting Column Correlations: (Extended Version). In Proceed-\nings of ACM Conference (SIGMOD’19). ACM, New York, NY, USA,\n19 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n1 INTRODUCTION\nModern relational database management systems (RDBMSs)\nsupport fast secondary indexes that help accelerate query\nprocessing in both transactional and analytical workloads.\nThese indexes, created either by database administrators or\nautomatically by query optimizers, are built on top of the\nmost frequently queried columns, hence providing an effi-\ncient way to retrieve data tuples via these columns. However,\nmanaging multiple secondary indexes in the database can\nconsume large amounts of storage space, potentially causing\nsevere performance degradation due to the exhaustion of\nmemory space. This problem is not uncommon especially\nin the context of modern main-memory RDBMSs, where\nmemory space is a scarce resource.\nConfronting this problem, researchers in the database com-\nmunity have proposed various practical solutions to limit the\nspace usage for index maintenance. From the database tun-\ning perspective, some of the research works have introduced\nsmart performance tuning advisors that can automatically\nselect the most beneficial secondary indexes given a fixed\nspace budget [ 4,9,35]. While satisfying the space constraints,\nthese techniques essentially limit the number of secondary\nindexes built on the tables, consequently causing poor perfor-\nmance for queries that lookup the unindexed columns. From\nthe structure design perspective, a group of researchers hasarXiv:1903.11203v2  [cs.DB]  1 Apr 2019\n\ndeveloped space-efficient index structures that consume less\nstorage space compared to conventional indexes [ 13]. These\nworks either store only a subset of the column entries [ 34]\nor use compression techniques to reduce space consump-\ntion [ 39]. However, such solutions save limited amount of\nspace and can cause high overhead for lookup operations.\nWe attempt to address this problem in a third way. The\nmain observation that sparks our idea is that many columns\nin the data tables exhibit correlation relations, or soft func-\ntional dependencies , where the values of a column can be\nestimated by that of another column with approximation. Ex-\nploiting such a relation can greatly reduce the memory con-\nsumption caused by secondary index maintenance. Specif-\nically, if we want to create an index on a column Mthat is\nhighly correlated with another column Nwhere an index has\nbeen built, we can simply construct a succinct, ML-enhanced\ndata structure, called Tiered Regression Search Tree , orTRS-\nTree , to capture the correlation between MandN.TRS-Tree\nexploits multiple simple statistical regression processes to fit\nthe curve of the hidden correlation function. Different from\nexisting machine learning-based indexing solutions, TRS-\nTree efficiently handles inserts, deletes, and updates, and\nsupports on-demand structure reorganization to re-optimize\nthe index efficiency at system runtime. To perform a lookup\nquery on M, the RDBMS retrieves a lookup range on Nfrom\nthe newly constructed TRS-Tree and fetches the targeted\ntuples using N’s index. We call this mechanism Hermit .\nHermit achieves competitive performance when support-\ning range queries, which are prevalent for secondary key\ncolumn accesses. It also presents a tradeoff between com-\nputation and space consumption. While avoiding building a\ncomplete index structure remarkably reduces space consump-\ntion, Hermit requires any incoming query to go through an\nadditional hop before retrieving the targeted tuples. How-\never, as our experiments will show, this overhead does not\nsubstantially affect performance in practice, and it also brings\nin huge benefits when storage space is valuable and scarce,\nsuch as in main-memory RDBMSs.\nWe are not the first to exploit column correlations in\nRDMBSs. Several previous works have proposed correlation-\nbased optimizations for query processing, database design,\nand data access. Different from these existing works, our\nwork advocates a succinct ML-enhanced tree-based structure,\ncalled TRS-Tree , that adaptively and dynamically capture\nboth complex correlations and outliers. Using TRS-Tree , we\nfurther show how Hermit exploits multiple correlations in\nboth main-memory and disk-based RDBMSs.\nThis paper is organized as follows. Section 2 provides tech-\nnical background. Section 3 gives an overview of Hermit ’s\nmethodology. Section 4 presents the detailed design of Her-\nmit’sTRS-Tree structure, and Section 5 shows how Hermit\nleverages TRS-Tree to perform tuple retrieval. Section 6provides detailed discussion on several issues. We report\nextensive experiment results in Section 7. Section 8 reviews\nrelated works and Section 9 concludes. We provide extended\ndiscussion on several issues in Appendix.\n2 BACKGROUND\nIn this section, we provide some technical background about\nindex structures and column correlations in the context of\nRDBMSs.\n2.1 Index Structures\nModern RDBMSs use secondary index structures to improve\nthe speed of data retrieval at the cost of additional writes\nand space consumption. A secondary index is created ei-\nther by a database administrator or automatically by a query\noptimizer, and is built on top of one or more frequently ac-\ncessed columns. An index can be considered as a copy of the\ncorresponding key columns organized in a format that pro-\nvides fast mapping to the tuple identifiers, in terms of either\ntuple locations or primary keys [ 37]. Maintaining multiple\nsecondary indexes on the same database can be expensive,\nespecially in the context of main-memory RDBMSs. This is\nconfirmed by a recent study [ 39] which showed that index\nmaintenance can consume around 55% of the total memory\nspace. Confronting this problem, researchers have proposed\nvarious space-efficient index structures to reduce space con-\nsumption. In general, these works share two basic ideas:\n(1) using classic compression techniques such as Huffman\nencoding or dictionary encoding to reduce the index node\nsize [ 13]; (2) storing only a subset of entries from the indexed\ncolumns to reduce the number of leaf nodes [ 34]. Despite the\nlimited reduction in memory consumption, these techniques\ncan incur high overhead when processing lookup operations.\n2.2 Column Correlations\nA conventional RDBMS allows database administrators to\nset integrity constraints using SQL statements to express\nthe functional dependencies among data columns. These ex-\nplicitly declared functional dependencies can be leveraged\nby query optimizers to provide a more accurate cost estima-\ntion during the query rewriting phase. In addition to these\n“hard” functional dependencies, modern query optimizers\nalso attempt to explore “soft” functional dependencies to\ngenerate better query plans using the column correlation\nrelations. Column correlations capture approximate depen-\ndencies, meaning that the value of a column can determine\nthat of another approximately. Following the definition in\nexisting works, we define a column correlation relation as\na triple(M,N,Fn), where MandNare data columns in the\ntable exhibiting correlations, and Fnis acorrelation func-\ntion specifying how N’s values can be estimated from M.\n\nBesides simple algebraic computation [ 7] (e.g., +,−,×,/)\nand linear functions [ 11,15] (e.g., N=βM+α±ϵ), a cor-\nrelation function N=Fn(M)can be of any possible form.\nThis will allow us to capture the correlations in many mod-\nern database applications, such as environment monitoring\n(oxygen v.s. carbon dioxide), stock market (Dow-Jones v.s.\nS&P 500), and healthcare (glucagon v.s. insulin). Existing\nRDBMSs have already exploited this kind of data character-\nistics to address system efficiency problems, including data\ncompression, query rewriting, and database tuning [ 19–21].\nIn the following sections, we will show how we can leverage\ncorrelations to accelerate data access.\n3 OVERVIEW\nThe correlation hidden among different columns in an RDBMS\nindicates a high similarity of their corresponding index struc-\ntures. Observing that, we developed a succinct, yet fast sec-\nondary indexing mechanism, namely Hermit , which exploits\nthe column correlations to answer queries.\nTo index a specified column M,Hermit requires two com-\nponents: a succinct data structure called Tiered Regression\nSearch Tree (abbr., TRS-Tree ) on the target column M, and\na pre-existing complete index called host index on the host\ncolumn N.TRS-Tree models the correlation between Mand\nN: it leverages a tiered regression method to perform hierar-\nchical curve fitting over the correlation function Fnfrom M\ntoN, and uses a tree structure to index a set of regression\nfunctions each of which represents an approximate linear\nmapping from a value range of Mto that of N.\nTo process a query, Hermit runs a three-phase searching\nalgorithm: (1) TRS-Tree search; (2) host index search; and\n(3) base table validation. Specifically, Hermit uses the query\npredicate to search the TRS-Tree in order to retrieve the\nrange mapping from MtoN. It then leverages the host index\nto find a set of candidate tuple identifiers. We note that this\ncandidate set is approximate, and it contains false positives\nthat fail to satisfy the original predicates. Hermit removes\nthose false positives by directly validating the corresponding\nvalues on the base table.\nHermit also works for multi-column secondary indexes.\nSuppose that two columns AandMon a table are queried\ntogether frequently, so an index on (A,M)is desirable. Her-\nmitcan utilize a host index on (A,N)and the correlation\nbetween MandN, to answer queries on AandM.\nIn the case where multi-column correlation exists (e.g., (W,\nX)->(Y,Z), although rarely detected by RDBMSs), Hermit\ncan concatenate multiple keys and build TRS-Tree on them.\nWe now use a running example to demonstrate how Her-\nmitworks. Let us consider a data table STOCK_HISTORY record-\ning U.S. stock market trading histories with four different\ncolumns: TIME (i.e., trading date), DJ(i.e., Dow Jones), SP(i.e.,\nTIMEDJSPVOLINDEXTABLE(a) Conventional index\nTIMEDJSPVOLINDEXTABLE (b)Hermit\nFigure 1: A comparison between data retrieval via conven-\ntional secondary indexes and Hermit . (double triangles de-\nnotes conventional secondary index; small single triangle\ndenotes the proposed TRS-Tree structure)\nS&P 500), and VOL(i.e., total trading volume). The database\nadministrator has already created an index on ( TIME ,DJ).\nNow she decides to create another index on ( TIME ,SP) due\nto the frequent occurrence of the queries like:\nSELECT *FROM STOCK_HISTORY\nWHERE (TIME BETWEEN ?AND ?)AND (SP BETWEEN ?AND ?)\nOn receiving the index creation statement, the RDBMS\nadopting Hermit first checks whether any column correla-\ntion involving TIME orSPhas been detected via any correla-\ntion discovery algorithms. If observing that the values in SP\nare highly correlated with those in DJand that there is an\nexisting index on ( TIME ,DJ), the RDBMS then constructs a\nTRS-Tree to model the correlation mapping from SPtoDJ.\nGiven the query ranges (Tmin ,Tmax)on column TIME and\n(Smin ,Smax)on column SP,Hermit first inputs the SPrange\n(Smin ,Smax)to the constructed TRS-Tree to fetch the cor-\nresponding range(Dmin ,Dmax)onDJ. Then it searches the\nhost index on ( TIME ,DJ), with TIME range(Tmin ,Tmax)and\nDJrange(Dmin ,Dmax), to find all the satisfying tuples. To\nfilter out false positives, the RDBMS reads the SPvalues from\nthe base table and validates the correctness of the result.\nFigure 1 shows how Hermit is different from conventional\nsecondary indexing mechanisms when retrieving tuples in\nthe running example. Unlike existing indexing techniques\nthat provide direct accesses to the tuple identifiers, Hermit\nrequires two-hop accesses. While potentially causing higher\naccess overhead for point queries, Hermit can achieve very\ncompetitive performance for range queries that are highly\ncommon for secondary indexes (as we will demonstrate in\nthe experiments). And, of course, Hermit can significantly\nreduce the space consumption for index maintenance.\nHermit can support insert, delete, and update operations\nwith correctness guarantees. Due to its approximate char-\nacteristics, Hermit works best for range queries, which are\nquite common for secondary key columns, especially in data\nanalytics. Furthermore, Hermit is extremely beneficial for\nmain-memory RDBMSs, where memory space is scarce.\n\nABCDFELAYER 0LAYER 1LAYER 2LAYER 30ABCD2565127681024EFFigure 2: TRS-Tree data structure on a target column with\nvalue range from 0 to 1024. The node fanout is set to 4. The\nboxes represent the leaf nodes and the circles represent the\ninternal nodes. The ruler bar shows how TRS-Tree parti-\ntions the range of the target column.\n4TRS-TREE\nTRS-Tree is a succinct tree structure that models data cor-\nrelation between a target column Mand a host column N\nwithin the same data table of a database. It leverages a tiered\nregression method to adaptively anddynamically perform\nthe curve fitting over the correlation function N=Fn(M). To\nbe precise, TRS-Tree decomposes the complex curve-fitting\nproblem into multiple simpler sub-problems and uses linear\nregression method to accurately address these sub-problems.\nTRS-Tree is adaptive, in the sense that it constructs its inter-\nnal structures based on the correlation complexity; it is also\ndynamic, meaning that it reorganizes its internals at runtime\nto ensure the best efficiency.\nIn the following section, we first discuss TRS-Tree ’s inter-\nnal structure, and then demonstrate its construction, lookup,\nmaintenance, parameter setting, and optimization.\n4.1 Internal Structure\nTRS-Tree is ak-ary tree structure that maps the values in\nthe target column Mto that in the host column N. Its con-\nstruction algorithm recursively divides M’s value range into\nkuniform sub-ranges until every entry pair (m,n)from M\nandNcovered by the corresponding sub-range can be well\nestimated by a simple linear regression-based data mapping.\nAs a tree-based data structure, TRS-Tree uses leaf nodes to\nmaintain the detailed data mappings, with its internal nodes\nproviding fast navigation to these leaf nodes. Figure 2 shows\naTRS-Tree structure constructed on a target column whose\nvalue range is from 0 to 1024.\nLeaf node. A leaf node in TRS-Tree is associated with a\nsub-range rof the target column M. We define that a range\nrhas two elements: a lower bound lband an upper bound\nub. Given a set of column entries Mrfrom Mcovered byr(i.e.,∀m∈Mr,r.lb≤m≤r.ub), the leaf node attempts\nto provide an approximate linear mapping from Mrto its\ncorresponding set of column entries Nrin the host column\nN. Such a mapping is represented using a linear function\nn=βm+α±ϵ, where mandnrepresent column values\nfrom MrandNr,βandαrespectively denote the function’s\nslope and intercept, and ϵdenotes the confidence interval.\nTRS-Tree computesβandαusing the standard linear\nregression formula [3] listed below:\nα=Nr−βMrβ=cov(Mr,Nr)\nvar(Mr)\nwhere NrandMrrespectively denote the average values\nof elements in NrandMr,var(Mr)is the variance of ele-\nments in Mr, and cov(Mr,Nr)presents the covariance of the\ncorresponding elements in MrandNr. Based on the above\nequations, both αandβcan be computed with one scan of\nthe data in MrandNr.\nPlease note that, instead of adopting gradient descent to\niteratively converge to local optimal, we directly compute\nthe solution using ordinary least square method, which is\ncomputationally easy and and works well for univariate\ncases.\nDifferent from the slope and intercept, the confidence in-\ntervalϵcan be computed based on a user-defined parameter,\ncalled error _bound , as will be elaborated in Section 4.5.\nThe function n=βm+α±ϵcaptures an approximate linear\ncorrelation between columns MandNunder the sub-range\nrinM. Given an minMr, it bounds the corresponding nto\nbe in the range(βm+α−ϵ,βm+α+ϵ). However, not all the\nentry pairs(m,n)from MrandNrare necessarily covered\nby the computed linear function. We call these entry pairs\nasoutliers . The leaf node maintains all these outliers in an\noutlier buffer , which is implemented as a hash table mapping\nfrom mto the corresponding tuple’s identifier, which can be\neither a primary key or a tuple location, as we will elaborate\nin Section 5.\nInternal node. An internal node in TRS-Tree functions\nas a navigator that routes the queries to their targeted leaf\nnodes. Similar to the leaf nodes, each internal node is asso-\nciated to a range in the target column M. However, instead\nof maintaining any mapping to the host column, an internal\nnode only maintains a fixed number of pointers pointing to\nits child nodes, each of which can be either a leaf node or\nanother internal node. To perform a lookup, an internal node\ncan easily navigate the query to the corresponding child\nnode whose range covers the input value.\n4.2 Construction\nAn RDBMS can efficiently construct a TRS-Tree upon the\nuser’s request. Algorithm 1 details the construction steps.\nThe construction algorithm takes as input the base table T,\n\nAlgorithm 1: Index construction in TRS-Tree\nData: base table T, target column ID cidM, host column ID\ncidN, value range R\nResult: TRS-Tree ’s root node root\n1Node root( R);\n2TmpTable fullTmpTable = ProjectTable (T,cidM,cidN);\n3FIFOQueue queue;\n4queue. Push (Pair(root, fullTmpTable));\n5while queue. IsNotEmpty ()do\n6 Pair pair = queue. Pop();\n7 Node node = pair. GetKey ();\n8 TmpTable tmpTable = pair. GetValue ();\n9 Compute (tmpTable, node);\n10 if!Validate (tmpTable, node) then\n11 Node[] subNodes = SplitNode (node, node _fanout );\n12 TmpTable[] subTables = SplitTable (tmpTable,\nsubNodes);\n13 foreach iin(0 to node _fanout−1)do\n14 queue. Push (Pair(subNodes[i], subTables[i]));\n15 DeleteTmpTable (tmpTable);\n16return root;\n17Function Validate( tmpTable, node )\n18 foreach entry intmpTable do\n19 ifentry.host <node. GetHostRange (entry.target)\nthen\n20 node.outliers. Add(entry.target, entry.tid);\n21 ifnode.outliers. Size () >\noutlier _ratio *tmpTable. Size ()then\n22 return false;\n23 return true;\nthe target column ID cidM, the host column ID cidN, and\nthe target column’s full range R. The range Rcontains the\nminimum and maximum values in the target column and\ncan be easily obtained from the RDBMS’s optimizer statistics.\nThe algorithm also requires a set of TRS-Tree ’s pre-defined\nparameters for computation.\nThis construction algorithm utilizes a FIFO (first-in-first-\nout) queue to build the TRS-Tree in a top-down fashion.\nEach element in the FIFO queue is a pair that contains a TRS-\nTree node and the node’s corresponding temporary table.\nThe temporary table for a TRS-Tree node is a sub-table of T,\nwhich selects rows with target column in the node’s range,\nand projects only the target and host columns along with\neach tuple’s identifier.\nTRS-Tree ’s construction starts by creating a root node\nwith its range set to the whole range R, and pushing the\nroot node along with its projected temporary table into the\nFIFO queue. It then does the following steps iteratively until\nthe FIFO queue is empty: (1) retrieve the ( tmpTable ,node )\npair from the FIFO queue; (2) compute node ’sβ,α, andϵanddetermine whether the generated linear mapping can well\ncover its corresponding entry pairs; (3) if (2) returns false,\nthen split node andtmpTable respectively into multiple child\nnodes and sub-tables, then push the corresponding pairs of\nchild node and sub-table back to the FIFO queue.\nTheCompute function scans a node’s temporary table to\ncompute the parameters for the linear function. The Validate\nfunction scans the temporary table again, and validates whether\neach pair of target and host column values can be covered by\nthe linear function. Any unqualified entry is inserted to the\nnode’s outlier buffer. A node’s linear function is determined\nto be not good enough if the outlier buffer exceeds a pre-\ndefined outlier _ratio , which is a ratio of the outlier buffer\nsize to the total number of tuples covered by the node. In\nthis case, step (3) is triggered, which drops all the generated\ncontent in node and splits it into a fixed number (equals to\nthe pre-defined node _fanout parameter) of equal-range child\nnodes. The users can limit the maximum depth of the tree\nstructure by setting the parameter max_heiдht.\nThe user-defined parameters for TRS-Tree can directly\ncontrol the confidence interval of the linear functions as well\nas the outlier buffer size, consequently affecting the perfor-\nmance. We discuss the parameters in Section 4.5. We further\nelaborate several optimization strategies in Appendix D.\nWe now perform a complexity analysis for Algorithm 1 .\nTRS-Tree uses Compute function to scan the tuples covered\nby every tree node to derive a linear regression model. If\nthe generated TRS-Tree is always a balanced full tree, then\nrunning linear regressions for all the tree nodes at the same\nheight takes a full scan of all tuples. A TRS-Tree with height\nequal to hwill perform hfull scans in total. As his bounded by\nthe parameter max_heiдht, we can conclude that the average\nand worst case complexities are O(N).\n4.3 Lookup\nTRS-Tree allows users to perform both point and range\nlookups on the target column Mto get the corresponding\nresults on the host column N. Instead of returning results\nthat exactly match the query predicates, TRS-Tree ’ lookup\nalgorithm returns approximate results. Hermit will perform\nadditional lookups on the host indexes and further validate\nthe results and generate exact matchings.\nAlgorithm 2 lists the details of TRS-Tree ’s lookup algo-\nrithm. The algorithm takes as input TRS-Tree ’s root node\nroot and a query predicate Pon the target column M. It gen-\nerates as output a set of value ranges RSon the host column\nNas well as a set of tuple identifiers IS. Without losing gen-\nerality, we consider Pto be a value range on Mwith two\nelements: lower bound lband upper bound ub. A point query\npredicate has its lower bound equals to its higher bound. The\nlookup starts from root and runs a breadth-first search using\n\nAlgorithm 2: Index lookup in TRS-Tree\nData: root node root, predicate P\nResult: range set RS, tuple identity set IS\n1Set<Range> RS;\n2Set<TupleID> IS;\n3FIFOQueue queue;\n4queue. Push (root);\n5while queue. IsNotEmpty ()do\n6 node = queue. Pop();\n7 ifnode. IsLeaf ()then\n8 Range r = Intersect (node.range, P);\n9 RS.Add(node. GetHostRange (r));\n10 IS.Add(node.outliers. Lookup (r));\n11 else\n12 foreach child innode.children do\n13 ifchild. IsOverlapping (P)then\n14 queue. Push (child);\n15RS = Union (RS);\n16return RSandIS;\na FIFO queue. The TRS-Tree iterates every single node in the\nqueue and performs a lookup if the node is a leaf node. On\nconfronting an internal node, TRS-Tree retrieves its child\nnodes and checks whether each child’s range overlaps with\nP. Any overlapping child node will be pushed to the FIFO\nqueue. The lookup algorithm continues iterating until the\nqueue is empty.\nTRS-Tree performs a lookup on a leaf node by taking the\nfollowing steps. First, it computes an intersection between\nthe query predicate Pand the node’s value range. The inter-\nsection result is a value range r. Using this range, the node\ncan then use its linear function to compute the estimated\nrange on Nthat covers the exact matching. The estimated\nrange will be either (β×r.lb+α−ϵ,β×r.ub+α+ϵ)or\n(β×r.ub+α−ϵ,β×r.lb+α+ϵ), depending on the sign\n(positive or negative) of the slope β. Not all the matchings\nare covered by the linear function. Hence, the leaf node\nfurther retrieves a set of tuple identifiers from its outlier\nbuffer. These identifiers can be used to directly fetch the cor-\nresponding tuples from the RDBMS without looking up the\nhost index. Before terminating the algorithm, TRS-Tree com-\nputes a union among all the elements in RS. This is because\nthe returned ranges generated by different leaf nodes can\noverlap. Computing the union can help reduce the number\nof elements in RS.\n4.4 Maintenance\nAt system runtime, TRS-Tree can dynamically support all\nof the commonly used database operations, including inser-\ntions, deletions, and updates. This makes TRS-Tree a drastic\ndeparture from existing machine learning-based solutions,Algorithm 3: Index insertion and deletion in TRS-Tree\nData: root node root, target column value m, host column\nvalue n, tuple ID tid\n1Function Insert( root,m,n,tid)\n2 Node node = Traverse (root);\n3 ifn<node. GetHostRange (m)then\n4 node.outliers. Add(m,tid);\n5Function Delete( root,m,n,tid)\n6 Node node = Traverse (root);\n7 node.outliers. Remove (m,tid);\n8Function Traverse( node, m)\n9 ifnode. IsLeaf ()then\n10 return node;\n11 else\n12 foreach child innode.children do\n13 ifm∈child.range then\n14 return Traverse( child );\nwhich rely on a long-running training phase to reconstruct\nthe index structures from scratch. TRS-Tree also reorganizes\ntheTRS-Tree structure at runtime to ensure the best query\nperformance. Algorithm 3 demonstrates how TRS-Tree pro-\ncesses insertions and deletions.\nInsertion. Tuple insertions in TRS-Tree are performed\nswiftly with little runtime change to its internal structures.\nGiven the to-be-inserted tuple’s target column value m, host\ncolumn value n, and tuple identifier tid,TRS-Tree starts\nthe insertion by locating the leaf node containing the range\ncovering m. After that, TRS-Tree checks whether the node’s\ncorresponding range of the host column can cover n(using\nthe leaf node’s linear function). If not, then TRS-Tree inserts\nthis tuple’s mandtidinto the outlier buffer. Otherwise, the\ninsertion algorithm directly terminates. The outlier buffer\nsize of certain leaf nodes may grow to be too large, conse-\nquently degrading TRS-Tree ’s lookup performance. In this\ncase, TRS-Tree invokes structure reorganization to further\nsplit these nodes, as we shall discuss shortly.\nDeletion. The tuple deletion algorithm in TRS-Tree shares\na similar routine as the insertion. However, TRS-Tree does\nnot perform any computation after locating the leaf node.\nInstead, it directly checks the outlier buffer and removes the\ncorresponding entry if exists. Frequent tuple deletion from\nthe index can cause suboptimal space utilization problem,\nmeaning that TRS-Tree can potentially use less number of\nleaf nodes to accurately capture the column correlations.\nTRS-Tree also relies on the structure reorganization to han-\ndle this issue.\nReorganization. As we have mentioned above, TRS-Tree\nreorganizes its internal structure on demand to optimize the\nindex efficiency in terms of both lookup performance and\n\nspace utilization. TRS-Tree detects reorganization opportu-\nnities based on two criteria. First, the outlier buffer size of a\ncertain leaf node reaches a threshold ratio compared to the\ntotal number of tuples covered in the corresponding range;\nsecond, the number of deleted tuples covered by the leaf\nnode’s corresponding range reaches a threshold compared\nto the total number of tuples. For the first case, TRS-Tree\ndirectly splits the leaf node into multiple equal-range child\nnodes, as described in Algorithm 1. For the second case, TRS-\nTree checks the node’s neighbors to determine whether\nmerging is beneficial.\nTRS-Tree uses a dedicated background thread to execute\nthe reorganization procedure, but it offloads the detection of\ncandidate nodes to each insert/delete operation. Specifically,\nTRS-Tree maintains a FIFO queue to record nodes where\nmerge or split can be made. Once an insert operation finishes\nits procedure and detects that the outlier buffer size of its\nvisited leaf node has reached the threshold, it then adds\nthe pointer to the leaf node into the FIFO queue. Delete\noperations proceed in a similar manner, but they add into\nthe FIFO queue the pointer to the parent of the visited leaf\nnode. Every entry in the queue is attached with a flag to\nidentify whether this node is a candidate for split or merge.\nTo perform structure reorganization, the background thread\nscans the target column to obtain all the tuples that fall into\nthe affected value range. It then computes the linear func-\ntions and populates the outlier buffers before installing the\nnew node(s) into the tree structure. To reduce the latency\nof the reorganization procedure, TRS-Tree periodically per-\nforms batch structure reorganization, meaning that the back-\nground thread can reorganize several candidate nodes in one\nscan. On confronting drastic workload change, TRS-Tree\ncan reorganize entire subtree at once (as we shall see in\nSection 7.7 ).TRS-Tree also supports online structure reor-\nganization, which enables concurrent lookup/insert/delete\noperations with little interference. TRS-Tree ensures the\nstructure consistency by leveraging a very simple yet effi-\ncient synchronization protocol. Due to the space limit, we\nprovide more details in Appendix B.\n4.5 Parameters\nTRS-Tree requires the users to pre-define some parame-\nters before the index construction, including node _fanout ,\nmax_heiдht, and outlier _ratio . There is another important\nparameter, called error _bound , which is used to determine\nthe confidence interval ϵof each leaf node.\nTheerror _bound parameter represents the expected num-\nber of host column Nvalues covered by the range returned\nfrom searching a TRS-Tree node for a point query on the\ntarget column M. So, by setting this parameter, TRS-Tree\nroughly measures the number of false positives for a pointquery. For a given leaf node with range ron column M, its\nlinear function returns an estimated range (β×r.lb+α−\nϵ,β×r.ub+α+ϵ)on column N(We assume slope βto\nbe positive. Our discussion be easily generalized to cases\nwith negative slopes.). For any point m∈ron column M,\nthe linear function returns a range (β×m+α−ϵ,β×m+\nα+ϵ)on column N. Let nbe the number tuples covered\nby the leaf node. Assuming that values on column Nare\nuniformly distributed, then the expected number of values\nofNfor a point query (i.e. error _bound ) can be estimated\naserror _bound =2ϵ\nβ(r.ub−r.lb)+2ϵ×n≈2ϵ\nβ(r.ub−r.lb)×n. So,\nnow given a desired error _bound parameter value, we can\nderiveϵ≈β(r.ub−r.lb)×error _bound\n2n.\nIn theory, error _bound should be set carefully, since larger\nerror _bound generates larger ϵ, which subsequently results\nin larger returned ranges for the upcoming host index lookup.\nToo small error _bound can also cause performance degrada-\ntion, since more tuples covered by the corresponding range\nmay be identified as outliers, consequently causing node\nsplitting, yielding much deeper tree structure. Fortunately,\nin practice, the configuration of error _bound does not impact\nthe performance too much. This is because database users are\nmore likely to issue range queries on secondary key columns,\nin which case the amount of false positives brought by large\nϵis negligible compared to the amount of the tuples covered\nby the range query predicate. Hence, Hermit adopting TRS-\nTree can enjoy a very competitive end-to-end performance\ncompared to conventional complete-tree indexes, even with\nlarger error _bound .\n5HERMIT\nTRS-Tree lookup returns only approximate results. To obtain\nthe real matching for the input queries, Hermit further needs\nto remove all the false positive results. In this section, we\nfirst discuss tuple identifier schemes in existing RDBMSs, and\nshow how Hermit can work with these different schemes\nand generate accurate query results.\n5.1 Tuple Identifiers\nA secondary index built on a certain (set of) column(s) pro-\nvides a mapping from the columns’ key values to the corre-\nsponding tuples’ identifiers. Tuple identifiers can be imple-\nmented in two different ways depending on the RDBMS’s\nperformance requirement [ 14,37].Hermit ’s indexing mech-\nanism is general enough to work with both schemes.\nAn RDBMS adopting logical pointers stores the primary\nkey of the corresponding tuple in each secondary index’s\nleaf nodes. The rationale behind logical pointers is that any\nupdate to tuple locations will not influence the secondary\nindexes. However, the drawback of this mechanism is that the\nRDBMS has to perform an additional lookup on the primary\n\nTRS-TreeHost IndexPrimary IndexBase Table1. Lookup2. Range lookups3. Point lookups(Optional)4. ValidationsFigure 3: The workflow of Hermit ’s lookup mechanism.\nindex every time a secondary index lookup happens. Popular\nRDBMSs like MySQL adopt this mechanism.\nAn RDBMS adopting the other identifier mechanism, called\nphysical pointers , directly stores tuple locations (in the for-\nmat of \"blockID+offset\") in each secondary index’s leaf nodes.\nWhile avoiding traversing the primary index during a sec-\nondary index lookup, an RDBMS using physical pointers has\nto update each index’s corresponding leaf node once any tu-\nple location is changed. Several DBMSs such as PostgreSQL\nemploy this scheme.\n5.2 Lookup in Hermit\nHermit can generate accurate lookup results for both tuple\nidentifier schemes. Figure 3 shows the entire workflow of\nHermit ’s lookup mechanism. We list the key steps as follows:\nStep 1. TRS-Tree lookup – This step performs a lookup on\nTRS-Tree as described in Section 4.3. The results are a set of\nranges on the host column and a set of tuple identifiers.\nStep 2. Host index lookup – This step performs lookups\non the host index with the returned host column ranges as\ninputs. The result is a set of tuple identifiers, which is further\nunioned with the set of identifiers returned from Step 1.\nStep 3. Primary index lookup (optional) – This step occurs\nonly if the RDBMS adopts logical pointers as tuple identifiers.\nIt looks up the primary index with the returned set of tuple\nidentifiers as inputs. The result is a set of tuple locations.\nStep 4. Base table validation – This step fetches the actual\ntuples using tuple locations and validates whether each tu-\nple satisfies the input predicates. This step returns all the\nqualified results to the input query.\nPlease note that a primary index can also serve as the host\nindex, in which case the lookup procedure shall be the same.\nCompared to conventional secondary index methods, Her-\nmitcan bring in additional overhead due to the extra host\nindex lookup phase as well as the base table validation phase.\nThe overhead is exacerbated when using logical pointers\nas the tuple identifier scheme, as it involves unnecessary\nprimary index lookups for unqualified matchings. However,\nwe argue that such overhead is insignificant when perform-\ning range queries, which are prevalent for secondary in-\ndexes. This is because the number of false positives for range\nqueries is quite small when compared to that of the qualifiedtuples. Moreover, as a TRS-Tree greatly reduces the space\nconsumption, it brings huge benefit to modern main-memory\nRDBMSs, where memory space is precious.\n6 DISCUSSION\nTradeoff between space and computation. Compared to\nconventional indexing mechanisms, TRS-Tree achieves great\nspace saving by sacrificing access performance. While the\nactual space used by TRS-Tree is dependent on the cor-\nrelation quality (i.e., how correlate the two columns are),\nwe can indeed strike a balance by tuning the error _bound\nparameter. Let us consider a scenario with max_heiдhtset\nto 1, where TRS-Tree becomes a single-node, single-layer\nstructure. Now we tune the error _bound parameter and an-\nalyze how TRS-Tree behaves. In an extreme case where\nerror _bound is set to 0, Hermit shall identify every single\ndata that cannot be perfectly covered by the generated lin-\near function as outlier. In this case, Hermit can consume\nmore memory but achieves optimal lookup performance.\nThe increase of error _bound can effectively drop the mem-\nory consumption in the expense of reducing lookup per-\nformance. This is because TRS-Tree enlarges the returned\nbound for the lookups, and consequently introducing more\nfalse positives. However, as we shall see in our experiments,\nTRS-Tree ’s performance is actually not quite sensitive to\nthe value of error _bound parameter, as long as it is set to\nsmall enough. The key reason is that TRS-Tree navigates\nlookups based on simple linear function computation, which\nis much cheaper than chasing pointers and retrieving every\nsingle elements from the standard index structure.\nFault tolerance. The RDBMS must periodically persist\nHermit ’sTRS-Tree into the underlying storage for fault\ntolerance. Depending on the scenario, TRS-Tree can function\neither like a disk-based index and persist leaf nodes on disks\nor like a pure in-memory index which relies on write-ahead\nlogging and checkpointing for persistence. We leave the\ndetailed discussion as a future work.\nDue to the space limit, please refer to Appendix D for\ndetailed discussion on correlation recovery, optimization,\ncomplex machine learning models, and several other issues.\n7 EVALUATION\n7.1 Experiment Setups\nWe implemented Hermit in two different RDBMSs: DBMS-X ,\na main-memory RDBMS prototype built internally in IBM\n– Almaden, and PostgreSQL [ 2], a disk-based RDBMS that\nis widely used in backing modern database applications. We\nperformed all the experiments on a commodity machine\nrunning Ubuntu 16.04 with one 6-core Intel i7-8700 processor\nclocked at 3.20 GHz. The machine has a 16 GB DRAM and\none PCIe attached 1 TB NVMe SSD.\n\nHERMIT Baseline\n1.0% 2.5% 5.0% 7.5% 10%\nSelectivity024487296120Throughput (K ops)(a) Logical Pointer\n1.0% 2.5% 5.0% 7.5% 10%\nSelectivity030060090012001500Throughput (K ops) (b) Physical Pointer\nFigure 4: Range lookup throughput with different selectivi-\nties ( Stock ).\n25 50 75 100\nNumber of indexes0326496128160Memory (MB)\n(a) Memory Consumption\nHERMIT Baseline\nIndexing method0.00.20.40.60.81.0Space breakdownTable Existing Indexes New Indexes (b) Space Breakdown\nFigure 5: Memory consumption with different numbers of\nindexes ( Stock ).\nWe use three different applications to evaluate Hermit :\nSynthetic ,Stock , and Sensor . We provide the detailed\ndescriptions of these applications in Appendix A.\nThroughout this section, we compare two mechanisms:\n⋄Hermit : the correlation-based secondary indexing mech-\nanism proposed in this paper.\n⋄Baseline : the standard B+-tree-based secondary index-\ning mechanism used in conventional RDBMSs.\nThe B+-tree in DBMS-X is fully maintained in memory. It\nis highly optimized for modern CPUs with many advanced\ntechniques such as cache-conscious layout and SIMD instruc-\ntions (for numerical keys) applied. The node size is set to 256\nbytes. PostgreSQL instead implements a page-based B+-tree\nbacked by buffer pool. In our experiments, we have reconfig-\nured the buffer pool size to ensure that the B+-tree is fully\ncached in memory.\nWithout any explicit declaration, we set TRS-Tree ’s pa-\nrameters, including node _fanout ,max_heiдht,outlier _ratio ,\nanderror _bound , to 8, 10, 0.1, and 2, respectively. TRS-Tree\nachieves a good space-computation tradeoff with this con-\nfiguration, as we shall see later.\n7.2 Real-World Applications\nIn this subsection, we evaluate Hermit ’s performance using\nreal-world applications in DBMS-X .\nThe first experiment uses the Stock application to eval-\nuate Hermit ’s range query performance. It is simple for\nHermit ’sTRS-Tree to model the correlations between a\nstock’s daily highest and lowest prices, as they form a near-\nlinear correlation. One thing worth noticing is that there\n1.0% 2.5% 5.0% 7.5% 10%\nSelectivity0.000.040.080.120.160.20Throughput (M ops)(a) Logical Pointer\n1.0% 2.5% 5.0% 7.5% 10%\nSelectivity0.00.30.60.91.21.5Throughput (M ops) (b) Physical Pointer\nFigure 6: Range lookup throughput with different selectivi-\nties ( Sensor ).\n1 2 3 4\nNumber of tuples (millions)0.00.81.62.43.24.0Memory (GB)\n(a) Memory Consumption\nHERMIT Baseline\nIndexing method0.00.20.40.60.81.0Space breakdownTable Existing Indexes New Indexes (b) Space Breakdown\nFigure 7: Memory consumption with different numbers of\ntuples ( Sensor ).\ndoes not exist any strict bound between the two prices, and\nstock price can increase/drop by over 50% in a single day\n(see PG&E stock ( NYSE: PCG )).Hermit shall identify and\nmaintain these readings as outliers. Figure 4 shows the range\nlookup throughput with different selectivities. As we can\nsee,Hermit yields a very competitive performance to the\nbaseline solution, which requires building a complete sec-\nondary index on every single column. While Hermit suffers\nslightly from the overhead caused by false positive removal,\nits influence to the overall throughput is reduced with the\nincrease of the selectivity.\nWe then measure Hermit ’s memory consumption by chang-\ning the number of stocks stored in the table. Figure 5a shows\nthe result. When setting the number of indexes to 25, it means\nwe store 25 stocks in the table, as we build one index for each\nstock’s lowest price column. The result indicates that Her-\nmit’sTRS-Tree takes little memory space, and the RDBMS\nadopting Hermit consumes only half of the memory com-\npared to adopting the baseline solution, which creates one\nindex for each column. In fact, Hermit in this case spends a\ngreat fraction of memory for storing outliers, and this guaran-\ntees a small false positive ratio, as we shall see later. Figure 5b\nprovides a space breakdown to confirm this finding.\nNext, we test Hermit ’s performance using the Sensor\napplication. Supporting fast data retrieval in this applica-\ntion is challenging, as each sensor reading column has a\nnon-linear correlation with the average reading columns.\nFigure 6 shows the throughput with different range lookup\nselectivities. When setting the selectivity to 1.0%, Hermit\nyields a throughput that is around 22% lower than the base-\nline solution. However, the performance gap diminishes with\n\n0.01% 0.025% 0.05% 0.075% 0.1%\nSelectivity0.00.30.60.91.21.5Throughput (K ops)(a) Logical Pointer\n0.01% 0.025% 0.05% 0.075% 0.1%\nSelectivity048121620Throughput (K ops) (b) Physical Pointer\nFigure 8: Range lookup throughput with different selectivi-\nties ( Synthetic –Linear ).\n0.01% 0.025% 0.05% 0.075% 0.1%\nSelectivity0.00.30.60.91.21.5Throughput (K ops)\n(a) Logical Pointer\n0.01% 0.025% 0.05% 0.075% 0.1%\nSelectivity048121620Throughput (K ops) (b) Physical Pointer\nFigure 9: Range lookup throughput with different selectivi-\nties ( Synthetic –Sigmoid ).\nthe growth of the selectivity. This is because Hermit gener-\nates approximate results, and the time ratio of filtering out\nfalse positives decreases with the increase of the result size.\nWe then use the same application to measure Hermit ’s\nmemory consumption. As Figure 7a shows, the space con-\nsumed by the baseline solution grows much faster than that\nconsumed by Hermit . According to the analytics in Fig-\nure 7b, the baseline solution spends most of the memory\nmaintaining newly created secondary indexes. In contrast,\nHermit ’sTRS-Tree takes much less space, and most of the\nspace is used for storing outliers.\n7.3 Lookup\nIn this subsection, we evaluate Hermit ’s range and point\nlookup performance using the Synthetic application, with\nboth Linear andSigmoid correlation functions. We perform\nall the experiments in DBMS-X .\nWe use both Hermit and the baseline method to index\ncolumn colC. Modeling Linear correlation function is triv-\nial using Hermit ’TRS-Tree structure. However, it can be\nchallenging to model Sigmoid correlation function, which is\npolynomial.\nFigure 8 depicts the performance of Hermit and the base-\nline mechanism with Linear correlation function. We set the\nnumber of tuples to 20 million and measure the throughput\nchanges of the range lookup queries with different query se-\nlectivities. We also adopt different tuple identifier methods to\nshow their impacts on the performance. The result indicates\nthatHermit ’s performance is very close to that achieved by\nthe baseline. Using logical pointers, Hermit and the baseline\nrespectively proceed 1.19 and 1.27 K operations per second\n(K ops) with selectivity set to 0.01%. This gap is reduced with\nTRS-Tree Host Index Primary Index Base Table\n0.01% 0.025% 0.05% 0.075% 0.1%\nSelectivity0.00.20.40.60.81.0Performance breakdown(a) Logical Pointer\n0.01% 0.025% 0.05% 0.075% 0.1%\nSelectivity0.00.20.40.60.81.0Performance breakdown (b) Physical Pointer\nFigure 10: Hermit ’s range lookup performance breakdown\nwith different selectivities ( Synthetic –Sigmoid ).\nSecondary Index Primary Index Base Table\n0.01% 0.025% 0.05% 0.075% 0.1%\nSelectivity0.00.20.40.60.81.0Performance breakdown\n(a) Logical Pointer\n0.01% 0.025% 0.05% 0.075% 0.1%\nSelectivity0.00.20.40.60.81.0Performance breakdown (b) Physical Pointer\nFigure 11: Baseline’s range lookup performance breakdown\nwith different selectivities ( Synthetic –Sigmoid ).\nthe increase of selectivity. The experiments with physical\npointers also demonstrate the same results. One of the rea-\nsons is that Hermit ’sTRS-Tree only needs to use a single\nleaf node to model the correlation function, yielding optimal\nperformance. We then use Sigmoid function to test whether\nTRS-Tree can efficiently model complex correlations. The\nresults in Figure 9 show that the performance gap is little\nchanged. Observing this, we decide to perform a detailed\nanalysis to understand where the time goes.\nFigure 10 and Figure 11 respectively show the performance\nbreakdown of Hermit and the baseline method. The time\nincludes both CPU and memory IO. Recall that Hermit per-\nforms lookups through the following steps: TRS-Tree lookup,\nhost index lookup, primary index lookup (optional), and base\ntable validation. In contrast, the baseline method only needs\nto perform secondary index lookup, primary index lookup\n(optional), and base table access. With logical pointers as\ntuple identifiers, both methods spend over 90% of their time\non the primary index lookup. This is inevitable because an\nRDBMS using logical pointers does not directly expose the\ntuple location to any index other than the primary one. When\nidentifying tuples using physical pointers, the major bottle-\nneck of both methods shifted to the base table access. While\none-time tuple retrieval from the base table using its tuple\nlocation seems to be trivial, the total number of the fetches\nis actually equivalent to the total number of returned tuples,\nwhich can be expensive in range queries.\nDespite the high efficiency for range queries, Hermit suf-\nfers from some performance degradation on point lookups,\n\n1 5 10 15 20\nNumber of tuples (millions)02404807209601200Throughput (K ops)(a) Logical Pointer\n1 5 10 15 20\nNumber of tuples (millions)06001200180024003000Throughput (K ops) (b) Physical Pointer\nFigure 12: Point lookup throughput with different numbers\nof tuples ( Synthetic –Linear ).\n1 5 10 15 20\nNumber of tuples (millions)02404807209601200Throughput (K ops)\n(a) Logical Pointer\n1 5 10 15 20\nNumber of tuples (millions)06001200180024003000Throughput (K ops) (b) Physical Pointer\nFigure 13: Point lookup throughput with different numbers\nof tuples ( Synthetic –Sigmoid ).\ndue to its introduction of false positives. We now measure the\npoint lookup throughput by increasing the number of tuples\nin the database. Figure 12 shows the result with Linear cor-\nrelation function. Using logical pointers for tuple identifiers,\nHermit ’s throughput is 35% lower than that achieved by the\nbaseline method, when the number of tuples is set to 20 mil-\nlions. This is because Hermit ’sTRS-Tree lookup results in\nnot only multiple unnecessary lookups on host and primary\nindexes, but also additional validation phase on the base ta-\nble. We also observe that such a performance gap is reduced\nto 15% when switching the identifier method to physical\npointers. The key reason is that the absence of expensive\nprimary index lookups helped reduce Hermit ’s performance\noverhead. Figure 13 shows the point lookup performance\nwhen using Sigmoid correlation function. Hermit ’s perfor-\nmance degrades with more tuples. The key reason is that\nthe increasing tuple count makes correlation function more\ndifficult to model, hence Hermit ’sTRS-Tree can generate\nmore false positives for its subsequent processes, eventually\ndegrading the performance.\nWe further perform a performance breakdown to better\nunderstand the point queries. Figure 14 and Figure 15 show\nthe results. There are two points worth noticing. First, using\nlogical pointers, Hermit spends an increasing amount of\ntime on primary index lookup when the tuple count increases.\nAs explained above, this is because the larger tuple count\nindicates more complex correlation relations, and Hermit\nhas to waste more time on retrieving unqualified tuples from\nthe primary index. Second, compared to the baseline method,\nHermit spends a larger portion of time on the base table.\nThis is because Hermit has to validate every single tuple\nfetched from the base table to filter out false positives.\nTRS-Tree Host Index Primary Index Base Table\n1 5 10 15 20\nNumber of tuples (millions)0.00.20.40.60.81.0Performance breakdown(a) Logical Pointer\n1 5 10 15 20\nNumber of tuples (millions)0.00.20.40.60.81.0Performance breakdown (b) Physical Pointer\nFigure 14: Hermit ’s point lookup performance breakdown\nwith different tuple counts ( Synthetic –Sigmoid ).\nSecondary Index Primary Index Base Table\n1 5 10 15 20\nNumber of tuples (millions)0.00.20.40.60.81.0Performance breakdown\n(a) Logical Pointer\n1 5 10 15 20\nNumber of tuples (millions)0.00.20.40.60.81.0Performance breakdown (b) Physical Pointer\nFigure 15: Baseline’s point lookup performance breakdown\nwith different tuple counts ( Synthetic –Sigmoid ).\nHermit ’s performance can be affected by the correlation\nquality as well as the user-defined parameters. Now we\ncontrol the percentages of the injected noise as well as the\nvalue of error _bound to see how Hermit behaves. Figure 16\nshows the range lookup (selectivity set to 0.01%) throughput\nwith different percentages of injected noise and different\nerror _bound values. We use logical pointers in the experi-\nment. We observe that Hermit ’s performance drops drasti-\ncally with the increase of error _bound . This is because larger\nerror _bound indicates more false positives, and Hermit has\nto perform redundant secondary index lookups and rely on\nthe validation phase to remove unqualified tuples. This is\nconfirmed by Figure 17 , which shows that the false positive\nrate reaches up to 80% when error _bound is set to 10,000. An\ninteresting finding is that Hermit ’s performance remains\nstable with the increase of noise percentage. The key reason\nis that Hermit is capable to capture any outlier that fall be-\nyond its generated linear function, and it can effectively find\nthese outliers from the corresponding outlier buffers.\nFigure 18 further shows how noisy data and error _bound\nvalues affect Hermit ’s memory consumption. Our first find-\ning is that the memory consumption grows linearly with\nthe increase of noise percentage. This is because Hermit\nstores noisy data in outlier buffers, as explained above. Our\nsecond finding is that the memory consumption declines\nby increasing the error _bound values. This is because larger\nerror _bound covers more data, and hence Hermit ’sTRS-Tree\ncan construct less nodes to capture the correlations. One\nthing worth mentioning is the memory spent for capturing\n\n0.0% noise\n 2.5%\n 5.0%\n 7.5%\n 10%\n1 10 100 1000 10000\nerror_bound value0.00.30.60.91.21.5Throughput (K ops)(a)Linear Correlation\n1 10 100 1000 10000\nerror_bound value0.00.30.60.91.21.5Throughput (K ops) (b)Sigmoid Correlation\nFigure 16: Range lookup throughput with different percent-\nages of injected noises and error _bound values ( Synthetic ).\n1 10 100 1000 10000\nerror_bound value0.00.20.40.60.81.0False positive ratio\n(a)Linear Correlation\n1 10 100 1000 10000\nerror_bound value0.00.20.40.60.81.0False positive ratio (b)Sigmoid Correlation\nFigure 17: Range lookup false positive ratio with differ-\nent percentages of injected noises and error _bound values\n(Synthetic ).\n1 10 100 1000 10000\nerror_bound value04080120160200Memory (MB)\n(a)Linear Correlation\n1 10 100 1000 10000\nerror_bound value04080120160200Memory (MB) (b)Sigmoid Correlation\nFigure 18: Memory consumption with different percentages\nof injected noises and error _bound values ( Synthetic ).\ntheLinear andSigmoid correlations tend to be the same\n(close to 120 MB) when error _bound is set to 10,000. This\nmatches our expectation, as Hermit ’sTRS-Tree spent most\nof the space for storing outliers, and only little space is used\nfor model correlations.\nWe observe that memory usage for capturing Sigmoid\ndrops a lot when changing error _bound from 1 to 10, but we\ndo not really see a noticeable decline in throughput. This\nis because TRS-Tree can identify too many data as outliers\nwith error _bound set to 1. With the increase of error _bound ,\nit efficiently captures correlations using linear regression\nmodels. It also navigates lookups using computation rather\nthan chasing pointers, hence achieving good performance.\n7.4 Space Consumption\nHermit trades performance for space efficiency. Its goal is\nto greatly reduce the storage space while achieving “good\nenough” tuple retrieval speed. In the last subsection, we\nshowed that Hermit yields competitive performance to the\nconventional secondary index mechanisms when support-\ning database operations, especially range queries. Now we\n1 5 10 15 20\nNumber of tuples (millions)10-1100101102103Memory (MB)(a)Linear Correlation\n1 5 10 15 20\nNumber of tuples (millions)10-1100101102103Memory (MB) (b)Sigmoid Correlation\nFigure 19: Index memory consumption with different num-\nbers of tuples ( Synthetic ).\n1 2 4 8 10\nNumber of indexes0246810Memory (GB)\n(a) Memory Consumption\nHERMIT Baseline\nIndexing method0.00.20.40.60.81.0Space breakdownTable Existing Indexes New Indexes (b) Space Breakdown\nFigure 20: Total memory consumption with different num-\nbers of tuples ( Synthetic –linear ).\nmeasure Hermit ’s space efficiency using the Synthetic\napplication. We still run all the experiments in DBMS-X .\nThe first experiment measures the amount of memory\nused respectively by TRS-Tree and conventional secondary\nindex on colC. Figure 19 shows that, compared to the baseline\nsolution, Hermit takes little space to index the column. An\nextreme case is to use Hermit ’sTRS-Tree for capturing\nLinear correlation function. In this case, TRS-Tree only\nneeds to use a constant amount of memory (a few bytes)\nto record the linear function’s parameters. When modeling\nSigmoid function, TRS-Tree consumes more memory (less\nthan 10 MB) as the number of tuples increases, because TRS-\nTree needs to construct more leaf nodes to better fit the\ncorrelation curve. However, Hermit ’s memory consumption\nis still negligible compared to the baseline solution, which\ntakes close to 400 MB.\nNext, we measure the overall memory consumption caused\nbyHermit . Other than the existing indexes on colAandcolB,\nwe add some additional columns and build one index on\neach of them. All these newly added columns are correlated\ntocolB. Figure 20a shows that, when adopting the baseline\nsolution, the amount of memory consumption grows near\nlinearly with the increasing number of newly added indexes.\nSpecifically, the database used up to 8.5 GB memory when\nsupporting 10 secondary indexes. In contrast, when adopting\nHermit , the database only consumes 2.4 GB memory, which\nis a significant gain in memory utilization compared to the\nbaseline solution. Figure 20b further depicts the memory\nusage breakdown when the number of indexes is set to 10.\nThe space consumed by Hermit ’sTRS-Tree is negligible\ncompared to that used by the base table and the primary\nindex. However, when adopting the baseline solution, the\n\n1 2 4 6 8\nNumber of threads0.00.61.21.82.43.0Elapsed time (s)(a)Linear Correlation\n1 2 4 6 8\nNumber of threads0.01.22.43.64.86.0Elapsed time (s) (b)Sigmoid Correlation\nFigure 21: Index construction time with different numbers\nof threads ( Synthetic ).\n1 2 4 8 10\nNumber of indexes0.00.51.01.52.02.5Throughput (M ops)\n(a) Insertion Throughput\nHERMIT Baseline\nIndexing method0.00.20.40.60.81.0Performance breakdownTable Existing Indexes New Indexes (b) Performance Breakdown\nFigure 22: Index insertion throughput with different num-\nbers of indexes ( Synthetic –Linear ).\ndatabase application has to use over 70% of the memory to\nmaintain the secondary indexes. This result further confirms\nHermit ’s space efficiency.\n7.5 Construction\nThe construction of Hermit ’sTRS-Tree is different from\nthat of the conventional B+-tree-like index structures. In this\nexperiment, we measure the time for constructing TRS-Tree\ninDBMS-X with different numbers of threads. We compare\nthe results with that obtained by constructing the B+-tree\nindex. The B+-tree is built using single-thread bulk loading,\nas it currently does not support multithreading mode. We\nleave the comparison with concurrent B+-tree construction\nas a future work. The results in Figure 21 contain two inter-\nesting findings. First, TRS-Tree needs more time to finish\nthe index construction when confronting complex correla-\ntion functions, such as Sigmoid . This is because TRS-Tree\nneeds to perform multiple rounds of computations to cal-\nculate the leaf nodes’ linear functions. Second, TRS-Tree ’s\nconstruction time drops near linearly with the increase of\nthreads. This is because TRS-Tree constructs its internal\nstructures using a top-down mechanism, hence it can be\neasily parallelized.\n7.6 Insertion\nDifferent from existing machine learning based index struc-\ntures that require expensive retraining in face of data changes,\nHermit can dynamically support operations like insertion,\ndeletion, and updates at runtime. In this experiment, we use\nDBMS-X to compare Hermit ’s insertion performance with\nconventional secondary indexes. Figure 22a depicts the over-\nall insertion throughputs with different numbers of indexes.\n0 5 10 15 20 25 30\nTime (s)0.00.40.81.21.62.0Throughput (K ops)(a) Lookup Throughput\n0 5 10 15 20 25 30\nTime (s)0.000.240.480.720.961.20Memory (GB) (b) Memory Consumption\nFigure 23: Index reorg. performance ( Synthetic -Sigmoid ).\nPlease note that we take into account the time for updating\nthe primary index and the base table. These results are ob-\ntained with Linear correlation function and logical pointers,\nand we observed the same trend with other configuration\ncombinations. As the result shows, when setting the num-\nber of indexes to 10, Hermit can process 1.7 million insert\noperations per second, which is 2.6 times higher than that\nachieved by the conventional secondary index scheme. The\nmajor reason is that Hermit ’sTRS-Tree only needs to up-\ndate the leaf nodes’ outlier buffers when necessary, which\nis pretty lightweight. Figure 22b further explains the result.\nUsing the baseline mechanism, the database application has\nto spend over 80% of the time for inserting tuples into the\nsecondary indexes. This demonstrates the inefficiency of the\nconventional indexing mechanism for supporting inserts.\n7.7 Maintenance\nHermit ’sTRS-Tree can support online structure reorgani-\nzation to re-optimize its efficiency. In this experiment, we\nshow how Hermit ’s range lookup throughput and memory\nconsumption changes during the process of index reorgani-\nzation. We first create TRS-Tree on a table with 10 K tuples,\nand then insert another 19,990 K tuples to the table, yielding\n20 million tuples in total. After that, we trigger structure reor-\nganization every 5 seconds, each time reorganizing 1/4 of the\nstructure (given our default node _fanout =8, the reorganiza-\ntion procedure reorganizes 2 first-level subtrees each time).\nNote that this is an artificial scenario for testing purpose only.\nIn real life scenarios, TRS-Tree can adjust its reorganization\nfrequency based on the update rates, and the reorganization\nprocess would happen in parallel with updates. During the\ntest, each partial reorganization takes around 2 seconds to\nfinish. Figure 23 shows a 30-second trace of range lookup\nthroughput (selectivity = 0.01%) and memory consumption.\nAs we can see, the range lookup throughput remains stable\nduring the reorganization. In general, reorganization reduces\nthe sizes of outlier buffers, resulting in less number of direct\npointer chasing during query processing. At the same time,\nit also produces more tree nodes, hence contributing to more\nprecise characterization of the correlation. These two factors\nbalance out during the process. The memory consumption\ndrops significantly thanks to the structure reorganization.\n\n1.0% 2.5% 5.0% 7.5% 10%\nSelectivity020406080100Throughput (ops)(a) Lookup Throughput\n1.0% 2.5% 5.0% 7.5% 10%\nSelectivity0.00.20.40.60.81.0Performance breakdownTRS-Tree Index Validation (b) Performance Breakdown\nFigure 24: Range lookup performance in PostgreSQL.\nHowever, we also observed instant spike during the start of\neach reorganization. This is because the background thread\nneeds to perform table scan and materialize corresponding\ndata in order to compute linear function.\n7.8 Disk-Based RDBMSs\nNow we integrate Hermit into a popular disk-based RDBMS,\nnamely PostgreSQL. We use the Sensor application to com-\npareHermit ’s lookup performance with that of PostgreSQL’s\nsecondary indexing mechanism (which is denoted as the base-\nline solution). Please note that PostgreSQL adopts physical\npointers for tuple identifiers. We implemented a PostgreSQL\nclient using libpqxx [ 1] to issue queries. We still keep Her-\nmit’sTRS-Tree in memory.\nFigure 24a shows Hermit ’s range lookup performance in\nPostgreSQL. Similar to what we observed before, the perfor-\nmance gap drops with the increase of the selectivity. When\nsetting the selectivity to 1.0%, Hermit is over 30% slower\nthan the baseline solution. The major reason is that fetch-\ning data from secondary storage is more expensive than\nfetching from main memory. Figure 24b further depicts the\nbreakdown. Not surprisingly, TRS-Tree lookup is negligible\ncompared to host index lookup in PostgreSQL. Validating\nfalse positives also take times, as our implementation ma-\nterializes and then iterates the result set of the host index\nlookup. One may optimize the performance by pushing the\nfilter operator down to the index scan.\n8 RELATED WORK\nTree index structures. B+-Tree [ 10] is the textbook index\nfor disk-oriented DBMSs and its structure is well-designed\nto reduce random disk accesses. With the decrease of main\nmemory prices, researchers and practitioners have devel-\noped memory-friendly indexes that can efficiently leverage\nthe larger main memory and fast random access speed. Some\npioneering works include T-tree [ 27] and cache-conscious\nindexes [ 32]. All these indexes use the hierarchical tree struc-\nture to return accurate query results in a timely manner.\nHowever, these solutions can lead to high memory consump-\ntions, causing high pressure to main-memory RDBMSs.Succinct index structures. Sparse indexes such as col-\numn imprints [ 33] and Hippo [ 38] only store pointers to\ndisk pages (or column blocks) in parent tables and value\nranges in each page (or column block) to reduce the space\noverhead. Column Sketch [16] indexes tables on a values-by-\nvalue basis but compresses the values into a lossy map. The\ntradeoff is that these structures can introduce false positives\nin query time. BF-Tree [ 5] is an approximate index designed\nfor ordered or partitioned data. While generating unqual-\nified results, it can largely reduce the space consumption\nby only recording approximate tuple locations. The learned\nindex [ 24] improves space efficiency by exploiting data dis-\ntribution using machine learning techniques. It yields good\nperformance but requires a long training phase to generate\nthe data structure. Zhang et al. [ 41] proposed a new range\nquery filtering mechanism for log-structured merge trees.\nStonebraker [ 34] introduced the partial index that stores\nonly a subset of entries from the indexed columns to reduce\nthe number of leaf nodes. Idreos et al. [ 17,18] developed a\nseries of techniques called database cracking to adaptively\ngenerate indexes based on the query workload. Specifically,\npartial sideways cracking [17] introduces an index called\npartial maps which consists of several self-organized chunks.\nThese chunks can be automatically dropped or re-created\naccording to the remaining storage space such that the max-\nimum available space is exploited. Athanassoulis et al. [ 6]\nlater proposed the RUM conjecture to capture the relations\namong read, update, and memory overhead.\nCompression techniques [ 12,42] drop redundant data in-\nformation to save storage space. However, these techniques\nrequire extra time for compressing data ahead of time and de-\ncompressing data at query time. This compromises the query\nperformance and index maintenance speed. In addition, they\nstill store the pointers for tuples such that the amount of\nsaved memory is limited.\nSecondary index selection. Several works have also dis-\ncussed how to select secondary indexes given a fixed amount\nof space budget. A group of researchers at Microsoft pro-\nposed a mechanism that analyzes a workload of SQL queries\nand suggests suitable indexes [ 9]. They further presented an\nend-to-end solution to address the problem of selecting ma-\nterialized views and indexes [ 4]. Researchers at IBM modeled\nthe index selection problem as a variant of the knapspack\nproblem, and introduced an index recommendation mech-\nanism into the DB2. Most recently, Pavlo et al. [ 31] investi-\ngated this problem using a machine learning based approach.\nColumn correlations. BHUNT [ 7] automatically discov-\ners algebraic constraints between pairs of columns. By re-\nlaxing the dependency, CORDS [ 19] uses sampling to dis-\ncover correlations and soft functional dependencies between\ncolumns. In addition, CORDS recommends groups of columns\non which to maintain certain simple joint statistics. Researchers\n\non data cleansing also put lots of efforts on detecting func-\ntional dependencies including soft dependency and approxi-\nmate dependency [ 8,25]. CORADD [ 21] proposes a correlation-\naware database designer to recommend the best set of ma-\nterialized views and indexes for given database size con-\nstraints. Correlation Maps [ 20] (CM) is a data structure that\nexpresses the mapping between correlated attributes for ac-\ncelerating unindexed column access. While sharing a similar\nidea of leveraging column correlations to save space, Hermit\ndoes not require using clustered columns; more importantly,\nits ML-enhanced TRS-Tree structure can adaptively and\ndynamically model both complex correlations and outliers,\nhence yielding better performance in many cases.\nCardinality estimation. Cardinality estimation plays a\ncrucial role in RDBMS query optimizers. Column correlation\nis the most common reason that encumbers the estimation.\nSample views [ 26] and PSALM [ 40] use sampling methods to\ndetect the column correlation. Recent projects [ 28,29] start\ntreating column semantics as a black box and use machine\nlearning models to learn cardinalities from query feedbacks.\nKipf et al. [22] opt to use deep learning techniques to learn\ncardinalities for join queries.\n9 CONCLUSIONS\nWe have introduced Hermit , a new secondary indexing\nmechanism that exploits column correlations to reduce index\nspace consumption. Hermit utilizes TRS-Tree , a succinct,\nML-enhanced tree structure to adaptively and dynamically\ncapture complex correlations and outliers. Our extensive ex-\nperimental study has confirmed Hermit ’s effectiveness in\nboth main-memory and disk-based RDBMSs.\n\nREFERENCES\n[1] libpqxx. http://pqxx.org/development/libpqxx/.\n[2] PostgreSQL. http://www.postgresql.org.\n[3]Simple linear regression. https://en.wikipedia.org/wiki/Simple_linear_\nregression.\n[4]S. Agrawal, S. Chaudhuri, and V. R. Narasayya. Automated Selection\nof Materialized Views and Indexes for SQL Databases. In VLDB , 2000.\n[5]M. Athanassoulis and A. Ailamaki. BF-Tree: Approximate Tree Index-\ning. PVLDB , 7(14), 2014.\n[6]M. Athanassoulis, M. S. Kester, L. M. Maas, R. Stoica, S. Idreos, A. Aila-\nmaki, and M. Callaghan. Designing Access Methods: The RUM Con-\njecture. In EDBT , 2016.\n[7]P. G. Brown and P. J. Hass. BHUNT: Automatic Discovery of Fuzzy\nAlgebraic Constraints in Relational Data. In VLDB , 2003.\n[8]L. Caruccio, V. Deufemia, and G. Polese. Relaxed Functional Depen-\ndencies - A Survey of Approaches. IEEE TKDE , 28(1), 2016.\n[9]S. Chaudhuri and V. R. Narasayya. An Efficient, Cost-Driven Index\nSelection Tool for Microsoft SQL Server. In VLDB , 1997.\n[10] D. Comer. The Ubiquitous B-Tree. CSUR , 11(2), 1979.\n[11] P. Godfrey, J. Gryz, and C. Zuzarte. Exploiting Constraint-Like Data\nCharacterizations in Query Optimization. In SIGMOD , 2001.\n[12] J. Goldstein, R. Ramakrishnan, and U. Shaft. Compressing Relations\nand Indexes. In ICDE , 1998.\n[13] G. Graefe et al. Modern B-Tree Techniques. Foundations and Trends ®\nin Databases , 3(4):203–402, 2011.\n[14] G. Graefe, H. Volos, H. Kimura, H. A. Kuno, J. Tucek, M. Lillibridge,\nand A. C. Veitch. In-Memory Performance for Big Data. PVLDB , 8(1),\n2014.\n[15] J. Gryz, B. Schiefer, J. Zheng, and C. Zuzarte. Discovery and Application\nof Check Constraints in DB2. In ICDE , 2001.\n[16] B. Hentschel, M. S. Kester, and S. Idreos. Column Sketches: A Scan\nAccelerator for Rapid and Robust Predicate Evaluation. In SIGMOD ,\n2018.\n[17] S. Idreos, M. L. Kersten, and S. Manegold. Self-Organizing Tuple\nReconstruction in Column-Stores. In SIGMOD , 2009.\n[18] S. Idreos, S. Manegold, H. Kuno, and G. Graefe. Merging What’s\nCracked, Cracking What’s Merged: Adaptive Indexing in Main-\nMemory Column-Store. PVLDB , 4(9), 2011.\n[19] I. F. Ilyas, V. Markl, P. Haas, P. Brown, and A. Aboulnaga. CORDS: Au-\ntomatic Discovery of Correlations and Soft Functional Dependencies.\nInSIGMOD , 2004.\n[20] H. Kimura, G. Huo, A. Rasin, S. Madden, and S. B. Zdonik. Correlation\nMaps: A Compressed Access Method for Exploiting Soft Functional\nDependencies. VLDB , 2(1), 2009.\n[21] H. Kimura, G. Huo, A. Rasin, S. Madden, and S. B. Zdonik. CORADD:\nCorrelation Aware Database Designer for Materialized Views and\nIndexes. PVLDB , 3(1-2), 2010.\n[22] A. Kipf, T. Kipf, B. Radke, V. Leis, P. A. Boncz, and A. Kemper. Learned\nCardinalities: Estimating Correlated Joins with Deep Learning. In\nCIDR , 2019.\n[23] J. Kivinen and H. Mannila. Approximate Inference of Functional\nDependencies from Relations. Theoretical Computer Science , 149(1),\n1995.\n[24] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The Case for\nLearned Index Structures. In SIGMOD , 2018.\n[25] S. Kruse and F. Naumann. Efficient Discovery of Approximate Depen-\ndencies. PVLDB , 11(7), 2018.\n[26] P. Larson, W. Lehner, J. Zhou, and P. Zabback. Cardinality Estimation\nUsing Sample Views with Quality Assurance. In SIGMOD , 2007.\n[27] T. J. Lehman and M. J. Carey. A Study of Index Structures for Main\nMemory Database Management Systems. In VLDB , 1986.[28] H. Liu, M. Xu, Z. Yu, V. Corvinelli, and C. Zuzarte. Cardinality Estima-\ntion Using Neural Networks. In CASCON , 2015.\n[29] T. Malik, R. C. Burns, and N. V. Chawla. A Black-Box Approach to\nQuery Cardinality Estimation. In CIDR , 2007.\n[30] T. Papenbrock, J. Ehrlich, J. Marten, T. Neubert, J. Rudolph, M. Schön-\nberg, J. Zwiener, and F. Naumann. Functional Dependency Discovery:\nAn Experimental Evaluation of Seven Algorithms. PVLDB , 8(10), 2015.\n[31] A. Pavlo, G. Angulo, J. Arulraj, H. Lin, J. Lin, L. Ma, P. Menon, T. C.\nMowry, M. Perron, I. Quah, et al. Self-Driving Database Management\nSystems. In CIDR , 2017.\n[32] J. Rao and K. A. Ross. Making B+-Trees Cache Conscious in Main\nMemory. In SIGMOD , 2000.\n[33] L. Sidirourgos and M. L. Kersten. Column Imprints: A Secondary Index\nStructure. In SIGMOD , 2013.\n[34] M. Stonebraker. The Case for Partial Indexes. Sigmod Record , 18(4),\n1989.\n[35] G. Valentin, M. Zuliani, D. C. Zilio, G. Lohman, and A. Skelley. DB2\nAdvisor: An Optimizer Smart Enough to Recommend Its Own Indexes.\nInICDE , 2000.\n[36] A. D. Well and J. L. Myers. Research design & statistical analysis .\nPsychology Press, 2003.\n[37] Y. Wu, J. Arulraj, J. Lin, R. Xian, and A. Pavlo. An Empirical Evaluation\nof In-Memory Multi-Version Concurrency Control. PVLDB , 10(7), 2017.\n[38] J. Yu and M. Sarwat. Two Birds, One Stone: A Fast, yet Lightweight,\nIndexing Scheme for Modern Database Systems. PVLDB , 10(4), 2016.\n[39] H. Zhang, D. G. Andersen, A. Pavlo, M. Kaminsky, L. Ma, and R. Shen.\nReducing the Storage Overhead of Main-Memory OLTP Databases\nwith Hybrid Indexes. In SIGMOD , 2016.\n[40] H. Zhang, I. F. Ilyas, and K. Salem. PSALM: Cardinality Estimation in\nthe Presence of Fine-Grained Access Controls. In ICDE , 2009.\n[41] H. Zhang, H. Lim, V. Leis, D. G. Andersen, M. Kaminsky, K. Keeton,\nand A. Pavlo. SuRF: Practical Range Query Filtering with Fast Succinct\nTries. In SIGMOD , 2018.\n[42] M. Zukowski, S. Héman, N. Nes, and P. A. Boncz. Super-Scalar RAM-\nCPU Cache Compression. In ICDE , 2006.\nA APPLICATIONS IN THE EXPERIMENTS\nSynthetic : The synthetic data contains one single table with\nfour 8-byte numeric columns, namely colA,colB,colC, and colD.\nColumns colBandcolCare correlated, as colB’s values are generated\nby a certain correlation function from colC, i.e. colB=Fn(colC).\nWe use two types of correlation functions: Linear function and\nSigmoid function. We also inject uniformly distributed noisy data to\ncol_B. By default, we inject 1% noises (percent =abnormal tuples\ncardinalit y).\nWe have already built a primary index on colAand a secondary\nindex on colB. The application frequently queries on colCto retrieve\nvalues on colD. Our experiments build indexes on colC.\nStock : This application records the market price of 100 stocks\nin the U.S. stock market over the last 60 years. We store over 15,000\nrows containing datetime and daily highest and lowest prices of\nthese 100 stocks in a wide table (201 columns in total). We set the\nentries to NULL if certain readings are not available. Each pair of\nthe highest and lowest price columns forms a simple near-linear\ncorrelation. We build a primary index on the datetime column,\nand a set of secondary indexes on each lowest price column. The\napplication continuously issues queries to those unindexed highest\nprice columns. The queries are like: “during which time periods do\nStock X’s highest price fall between YandZ?”. Our experiments build\nindexes on all the unindexed columns and evaluate the performance.\n\nSensor : This application monitors chemical gas concentration\nusing 16 sensors. We store 4,208,260 rows containing the timestamp,\nthe 16 sensor readings, and the reading average in a single table (18\ncolumns in total). These 16 sensor reading columns and the average\nreading column form a non-linear correlation. We have constructed\none index on the average reading column. The application continu-\nously queries one of those 16 unindexed sensor reading columns.\nThe queries are like: “during which time period do the readings in\nSensor Xfall between YandZ?”. Our experiments build indexes on\nall the unindexed columns and evaluate the performance.\nB MAINTENANCE\nTRS-Tree can easily support concurrent insertions. As a single\ninsert/delete/update operation only affects at most one leaf node,\nTRS-Tree can easily guarantee the structure consistency by using\nconcurrent hash tables to implement the leaf nodes’ outlier buffers.\nTRS-Tree also enables online structure reorganization at run-\ntime without incurring much overhead to any concurrent opera-\ntions. Unlike conventional concurrent tree-based structures, TRS-Tree\ndoes not implement latch coupling, which can be overly complicated\nand expensive for TRS-Tree . Instead, it adopts a coarse-grained\nlatching protocol to maximize concurrency. The intuition behind\nthis decision is that insert/delete/update operations in TRS-Tree\nnever trigger cascading node modifications, and the reorganization\nhappens infrequently and can be processed with low latency.\nTRS-Tree uses a flag to identify the reorganization phase. A\ndedicated background thread starts the reorganization by setting\nthe flag to true. When observing this flag, any concurrent insert /\ndelete / update operations append their modifications to a temporal\nbuffer to avoid phantom problems. The background thread then\nscans and retrieves all corresponding entry pairs and subsequently\ncreates the new tree nodes. Before installing these nodes to TRS-\nTree , the background thread further holds a coarse-grained latch\non the entire tree and applies all the changes in a temporal buffer.\nThe latch is released once the new nodes are installed.\nC COMPARISON\nCompare with Correlation Maps (CM). TRS-Tree inHermit is\na ML-enhanced tree index. CM [ 20] adopts a map-like structure\nwhich stores the bucket mappings between correlated columns.\nBoth CM and HERMIT leverage column correlations to save space.\nBut we find that these two proposals are drastically different.\n•TRS-Tree captures correlations using tiered curve fitting and\nhandles outliers. This makes it robust to noisy data which is preva-\nlent in real-world applications. In contrast, CM does not include\nany scheme to handle outliers, and hence its performance can drop\nwhen confronting sparsely distributed noisy data.\n•TRS-Tree adaptively constructs its internal structures and auto-\nmatically decides the partition granularity. It dynamically maintains\nits internal structures, and performs reorganization in the presence\nof large amounts of insert/delete/update operations. In comparison,\nCM relies on its tuning advisor to decide the granularity for its\nsingle-layer buckets by building multiple histograms beforehand.\nIt is unknown how CM adapts to dynamic workload where the\nunderlying data drastically change over time.\n•Hermit is a general secondary indexing mechanism and can\nexploit multiple correlations on the same table. In contrast, CM canonly exploit correlations when there is a clustered index on the\nhost column. At most one clustered index can exist in a table.\nWe also empirically compare Hermit with CM in Appendix E.\nCompare with BF-Tree. BF-Tree [ 5] is an approximate index\nthat exploits implicit ordering and clustering in the underlying data\nto reduce storage overhead. It adopts the same tree structure with\nB+ Tree but stores a set of Bloom Filters in its leaf nodes. Those\nBloom Filters record the approximate physical locations of values.\nAlthough both BF-Tree and Hermit tend to reduce index size by\nintroducing false positives, they act very differently.\n•BF-Tree exploits implicit ordering and clustering in the under-\nlying data to reduce the index size. Hermit leverages the correlation\nbetween the host column and the indexed column to shrink its size\non the indexed column.\n•BF-Tree requires that the underlying data should be ordered or\nat least have some clusterings. Hermit does not have any specific\nrequirement for the data distribution. It works for any data order.\n•BF-Tree stores Bloom Filters and disk page ranges in its leaf\nnodes. For every key lookup on the indexed column, these Bloom\nFilters may return \"true\" for some non-existing keys and thus result\nin page scans on the false positive disk pages. The TRS-Tree in\nHermit stores ML models (linear regression in our paper) in leaf\nnodes. This range may include some false positive values which\nneed to be pruned later.\nD MORE DISCUSSIONS\nD.1 Correlation Discovery\nHermit fully relies on the underlying RDBMS or users to per-\nform correlation discovery. There has been a flurry of systems that\naddressed the problem of correlation (or functional dependency)\ndiscovery in different ways. In the past two decades, researchers\nextensively studied how to automatically find all functional depen-\ndencies (including those among composite columns) in a database.\nTo accelerate the discovery, they [ 8,19,23,25,30] opt to leverage\nspecific rules to prune columns or compute approximate coefficients\nbased on samples.\nIn practice, most commercial RDBMSs still largely rely on “hu-\nman knowledge” to discover possibly correlated columns because\nof the huge search space of column combinations. A database ad-\nministrator (DBA) can identify candidate columns that exhibit se-\nmantic relationships, then evaluate the correlation using different\ncorrelation coefficients, including Pearson coefficient andSpearman\ncoefficient [36]. Once the coefficient reaches a certain predefined\nthreshold, the DBA can recommend this correlation information to\nthe database optimization module.\nNow let us discuss correlation types Hermit may capture. Fig-\nure 25 shows three different functions: (1) linear (e.g., y=x) (2)\nmonotonic (e.g., y=siдmoid(x)) (3) non-monotonic (e.g., y=\nsin(x)). We do not consider noisy data at this moment. TRS-Tree\ncan perfectly index the correlations in both (1) and (2) since it di-\nrectly navigates a key lookup to a single value on the indexed host\ncolumn. A DBA can easily capture these two correlations using\nPearson coefficient and Spearman coefficient, respectively (coeffi-\ncient = 1). However, Hermit cannot yield good performance for\nnon-monotonic correlation like sinfunction because a single value\n\n−4 −2 0 2 4\nX−5−3−1135Y(a)y=x\n−4 −2 0 2 4\nX0.00.20.40.60.81.0Y (b)y=siдmoid(x)\n−10 −5 0 5 10\nX−1.0−0.6−0.20.20.61.0Y (c)y=sin(x)\nFigure 25: Three different correlation functions.\n1991 1995 1999 2003 2007 2011\nY ear03000600090001200015000Dow-Jones S&P 500\n(a) Stocks over year\n400 620 840 1060 1280 1500\nS&P 50003000600090001200015000Dow-Jones (b) Correlation function\nFigure 26: Dow-Jones and S&P 500 values. In (b), Green dots\nfalling beyond red lines are identified as outliers.\non the host column can be mapped to many values on the target\ncolumn. Consequently, TRS-Tree can generate many false positives,\nresulting in low performance. The DBA can detect a non-monotonic\ncorrelation using Spearman coefficient (coefficient = 0).\nHermit captures correlations and handles outliers. This makes\nHermit applicable to cases where nice difference bounding does not\nexist. Figure 26a shows the value trace of Dow-Jones and S&P 500\nduring the years 1991 to 2011. While the two indices are correlated\nin most years, we observe some major shifts in certain months or\nyears. Hermit handles them by directly identifying and maintaining\nthem in outlier buffers, and hence eliminate some false positives.\nD.2 Optimization\nSampling-based outlier estimation. During the index construc-\ntion, a TRS-Tree needs to compute the linear function parameters,\nnamely slope βand intercept α, of the leaf nodes using the standard\nlinear regression formula (see Section 4.1). Sometimes the default\nconstruction algorithm shown in Algorithm 1 unnecessarily com-\nputes these parameters even if the corresponding node later splits\ndue to too many outliers. We adopt a sampling-based strategy to\navoid this problem. Before performing the parameter computation,\nour optimization algorithm randomly samples the data (by default\n5%) covered by the range and runs the simple linear regression on\nthem. Within the sample set, if the number of outliers has already\nexceeded the pre-defined threshold, then the construction directly\npartitions the range. This helps us make the decision quickly.\nMulti-threaded index construction. Algorithm 1 shows how\nwe construct TRS-Tree using a single thread. Observing the popu-\nlarity of massively parallel processors, we can now leverage multi-\nthreading to speed up the construction algorithm. Different from\nB-tree, Hermit constructs its internal and leaf nodes following\na top-down scheme. This means that we can parallelize the tree\nconstruction without confronting any synchronization points. As-\nsuming that the index fanout is set to k, we can easily parallelize the\nsplitting and computation of TRS-Tree ’s every single node using kthreads. These threads proceed independently without incurring\nany inter-thread communication.\nD.3 Complex Machine Learning Models\nTable 1: Training time for different ML models\nNumber of tuples 1 K 10 K 100 K\nLinear regression 0.42 ms 0.81 ms 3.2 ms\nSVR (RBF) 0.09 s 4.5 s > 60 s\nSVR (linear) 0.28 s 29 s > 60 s\nSVR (polynomial) 0.29 s 24 s > 60 s\nHermit performs a linear regression in each TRS-Tree node\nwhich costs a scan of corresponding tuples. Actually, TRS-Tree ’s\nstructure is also flexible enough to adopt more complex models such\nas Support Vector Regression (SVR) and neural networks. However,\nalthough these models may yield less false positives, training these\nmodels takes tremendous time (orders of magnitude slower than\nlinear regression) if the table size increases significantly.\nTo prove that, we run a set of machine learning models on differ-\nent scales of data and report the training time in Table 1. As depicted\nin the table, training linear regression models only takes several\nmilliseconds while training SVR models with different kernels in-\ncluding RBF, linear and polynomial is at least 200 times slower.\nHaving said that, we believe that researchers and practitioners still\ncan easily extend Hermit to incorporate other models and fit in\ntheir specific scenarios.\nIn addition, a recent paper featuring learned indexes [ 24] dis-\ncusses the cases of using complex machine learning models such as\nneural networks and multivariate regression models to predict lo-\ncations of keys. As opposed to learned indexes, Hermit models the\ncorrelation between two columns and leverages the curve-fitting\ntechnique to adaptively create simple yet customized ML models\nfor different regions ( TRS-Tree tree nodes).\nD.4 Hermit on Secondary Storage\nAlthough the storage overhead of an index may not seem too ex-\npensive on traditional Hard Disk Drives (HDDs), the dollar cost\nincreases dramatically when the index is deployed on modern stor-\nage devices (e.g., Solid State Drives and Non-Volatile Memory) be-\ncause they are still more than an order of magnitude expensive than\nHDDs. This is also an important issue when deploying RDBMSs\non the cloud, where the customers are charged on a pay-as-you-go\nbasis. Hermit is a generic indexing mechanism designed for both\nmain-memory and disk-based RDBMSs. Deploying it on precious\nsecondary storage such as SSD can save considerable storage bud-\nget and still exhibit comparable lookup performance as opposed to\nB+Tree. Our experiment in Section 7.8 also confirmed this claim.\nD.5 Complex SQL Queries\nAs a replacement of classic B+-tree-based secondary indexes, Her-\nmitis general enough and can be used in any complex queries when-\never a classic secondary index is used. Even for complex queries\nlike join, RDBMSs can still use Hermit to execute local predicate,\nconsequently accelerating the processing of the entire query plan.\n\nHERMIT\n Baseline\n CM-16\n CM-64\n CM-256\n CM-1024\n CM-4096\n0.0% 2.5% 5.0% 7.5% 10%\nPercentage of injected noises0.00.30.60.91.21.5Throughput (K ops)(a) Host bucket size = 24\n0.0% 2.5% 5.0% 7.5% 10%\nPercentage of injected noises0.00.30.60.91.21.5Throughput (K ops)(b) Host bucket size = 26\n0.0% 2.5% 5.0% 7.5% 10%\nPercentage of injected noises0.00.30.60.91.21.5Throughput (K ops)(c) Host bucket size = 28\n0.0% 2.5% 5.0% 7.5% 10%\nPercentage of injected noises0.00.30.60.91.21.5Throughput (K ops)(d) Host bucket size = 210\n0.0% 2.5% 5.0% 7.5% 10%\nPercentage of injected noises0.00.30.60.91.21.5Throughput (K ops)(e) Host bucket size = 212\nFigure 27: Range lookup throughput with different percentage of injected noises ( Synthetic -Linear ).\n0.0% 2.5% 5.0% 7.5% 10%\nPercentage of injected noises0100200300400500Memory (MB)\n(a) Host bucket size = 24\n0.0% 2.5% 5.0% 7.5% 10%\nPercentage of injected noises0100200300400500Memory (MB)(b) Host bucket size = 26\n0.0% 2.5% 5.0% 7.5% 10%\nPercentage of injected noises0100200300400500Memory (MB)(c) Host bucket size = 28\n0.0% 2.5% 5.0% 7.5% 10%\nPercentage of injected noises0100200300400500Memory (MB)(d) Host bucket size = 210\n0.0% 2.5% 5.0% 7.5% 10%\nPercentage of injected noises0100200300400500Memory (MB)(e) Host bucket size = 212\nFigure 28: Memory consumption with different percentage of injected noises ( Synthetic -Linear ).\n0.0% 2.5% 5.0% 7.5% 10%\nPercentage of injected noises0.00.30.60.91.21.5Throughput (K ops)\n(a) Host bucket size = 16\n0.0% 2.5% 5.0% 7.5% 10%\nPercentage of injected noises0.00.30.60.91.21.5Throughput (K ops) (b) Host bucket size = 26\n0.0% 2.5% 5.0% 7.5% 10%\nPercentage of injected noises0.00.30.60.91.21.5Throughput (K ops)(c) Host bucket size = 28\n0.0% 2.5% 5.0% 7.5% 10%\nPercentage of injected noises0.00.30.60.91.21.5Throughput (K ops)(d) Host bucket size = 210\n0.0% 2.5% 5.0% 7.5% 10%\nPercentage of injected noises0.00.30.60.91.21.5Throughput (K ops)(e) Host bucket size = 212\nFigure 29: Range lookup throughput with different percentage of injected noises ( Synthetic -Sigmoid ).\n0.0% 2.5% 5.0% 7.5% 10%\nPercentage of injected noises0100200300400500Memory (MB)\n(a) Host bucket size = 16\n0.0% 2.5% 5.0% 7.5% 10%\nPercentage of injected noises0100200300400500Memory (MB) (b) Host bucket size = 26\n0.0% 2.5% 5.0% 7.5% 10%\nPercentage of injected noises0100200300400500Memory (MB)(c) Host bucket size = 28\n0.0% 2.5% 5.0% 7.5% 10%\nPercentage of injected noises0100200300400500Memory (MB)(d) Host bucket size = 210\n0.0% 2.5% 5.0% 7.5% 10%\nPercentage of injected noises0100200300400500Memory (MB)(e) Host bucket size = 212\nFigure 30: Memory consumption with different percentage of injected noises ( Synthetic -Sigmoid ).\nE EXPERIMENTS\nWe compare Hermit with an existing solution, namely Correlation\nMaps (CM). We implemented CM faithfully based on its original\npaper. Instead of implementing CM’s tuning advisor, we performed\nparameter sweeping and tuned the bucket size in both target and\nhost columns to evaluate the performance. Throughout this section,\nwe use host bucket size to refer to the bucket size in host column.\nWe use CM-X to denote CM with the bucket size in target column\nset to X (e.g., CM-16 means the bucket size in target column is 16).\nFigure 27 and Figure 28 show the range lookup throughput (se-\nlectivity=0.01%) and memory consumption of Hermit , CM, and\nthe B+-tree-based baseline solution using Synthetic -Linear . We\nchange the percentage of injected noises from 0% to 10%. Since CM\nwas designed for disk-based RDBMSs, [ 20] showed that CM usually\nperforms better with a smaller bucket size. Now CM is adapted\nto in-memory databases, this does not always hold true any more.\nThe host index look up and base table access are much faster now\nin memory, thus the overhead of accessing the CM structure itself\nplays a bigger role in the overall performance. A smaller bucket\nsize means more buckets in CM and accordingly more overhead for\naccessing the CM structure. We also observed a compute-storagetradeoff in CM. While CM’s lookup throughput drops with the in-\ncrease of bucket size, its memory consumption is actually reduced.\nAnother key observation in these figures is that CM’s performance\ncan drop significantly with the increase of percentage of injected\nnoises. This is because CM has to maintain mappings among buck-\nets for every single entry pairs in the target and host columns. Even\nwith small amount of sparsely distributed outliers, CM in the ex-\ntreme case may have to scan the entire table to remove outliers (just\nconsider the case where the outliers are scattered to every single\nbucket in the target column). Compared to CM, Hermit can sustain\na high throughput even when the noise percentage is increased to\n10%. This is because CM can identify and maintain outliers in leaf\nnodes’ outlier buffers. The tradeoff is that its memory consumption\ncan increase. Overall, Hermit and CM both can reduce memory\nconsumption compared to B+-tree. However, Hermit can achieve\nmuch better performance in the presence of outliers, and it saves\nmuch more memory as the correlation information is well captured\nby the tiny linear regression models.\nFigure 29 and Figure 30 further show the experiment results with\nSynthetic -Sigmoid . We also observed similar results in this set\nof experiments. The only difference is that Hermit needs to spend\nmore memory to capture correlations.",
  "textLength": 94627
}