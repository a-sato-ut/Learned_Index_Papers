{
  "paperId": "557cbb9809d587951fd42cc2a30bb400772f4953",
  "title": "Learning-Augmented Priority Queues",
  "pdfPath": "557cbb9809d587951fd42cc2a30bb400772f4953.pdf",
  "text": "Learning-Augmented Priority Queues\nZiyad Benomar\nENSAE, Ecole polytechnique,\nFairplay joint team, Paris\nziyad.benomar@ensae.frChristian Coester\nDepartment of Computer Science\nUniversity of Oxford\nchristian.coester@cs.ox.ac.uk\nAbstract\nPriority queues are one of the most fundamental and widely used data structures in computer science.\nTheir primary objective is to efficiently support the insertion of new elements with assigned priorities\nand the extraction of the highest priority element. In this study, we investigate the design of priority\nqueues within the learning-augmented framework, where algorithms use potentially inaccurate predic-\ntions to enhance their worst-case performance. We examine three prediction models spanning different\nuse cases, and we show how the predictions can be leveraged to enhance the performance of priority\nqueue operations. Moreover, we demonstrate the optimality of our solution and discuss some possible\napplications.\n1 Introduction\nPriority queues are an essential abstract data type in computer science [Jaiswal, 1968; Brodal, 2013] whose\nobjective is to enable the swift insertion of new elements and access or deletion of the highest priority\nelement. Other operations can also be needed depending on the use case, such as changing the priority of\nan item or merging two priority queues. Their applications span a wide range of problems within computer\nscience and beyond. They play a crucial role in sorting [Williams, 1964; Thorup, 2007], in various graph\nalgorithms such as Dijkstra’s shortest path algorithm [Chen et al., 2007] or computing minimum spanning\ntrees [Chazelle, 2000], in operating systems for scheduling and load balancing [Sharma et al., 2022], in\nnetworking protocols for managing data transmission packets [Moon et al., 2000], in discrete simulations for\nefficient event processing based on occurrence time [Goh and Thng, 2004], and in implementing hierarchical\nclustering algorithms [Day and Edelsbrunner, 1984; Olson, 1995].\nVarious data structures can be used to implement priority queues, each offering distinct advantages and\ntradeoffs [Brodal, 2013]. However, it is established that a priority queue with nelements cannot guarantee\no(logn) time for all the required operations [Brodal and Okasaki, 1996]. This limitation can be surpassed\nwithin the learning augmented framework [Mitzenmacher and Vassilvitskii, 2022], where the algorithms can\nbenefit from machine-learned or expert advice to improve their worst-case performance. We propose in this\nwork learning-augmented implementations of priority queues in three different prediction models, detailed\nin the next section.\n1.1 Problem definition\nA priority queue is a dynamic data structure where each element xis assigned a key ufrom a totally ordered\nuniverse ( U, <), determining its priority. The standard operations of priority queues are:\n(i) FindMin(): returns the element with the smallest key without removing it,\n(ii) ExtractMin(): removes and returns the element with the smallest key,\n(iii) Insert( x, u): adds a new element xto the priority queue with key u,\n(iv) DecreaseKey( x, v): decreases the key of an element xtov.\n1arXiv:2406.04793v2  [cs.DS]  17 Nov 2024\n\nThe elements of a priority queue can be accessed via their keys in O(1) time using a HashMap. Hence,\nthe focus is on establishing efficient algorithms for key storage and organization, facilitating the execution\nof priority queue operations. For any key u∈ Uand subset V ⊂ U , we denote by r(u,V) the rank of uinV,\ndefined as the number of keys in Vthat are smaller than or equal to u,\nr(u,V) = #{v∈ V:v≤u}. (1)\nThe difficulty lies in designing data structures offering adequate tradeoffs between the complexities of the\noperations listed above. This paper explores how using predictions can allow us to overcome the limitations\nof traditional priority queues. We examine three types of predictions.\nDirty comparisons In the first prediction model, comparing two keys ( u, v)∈ U2is slow or costly.\nHowever, the algorithm can query a prediction of the comparison ( u < v ). This prediction serves as a rapid\nor inexpensive, but possibly inaccurate, method of comparing elements of U, termed a dirty comparison and\ndenoted by ( ub< v). Conversely, the true outcome of ( u < v ) is referred to as a clean comparison. For all\nu∈ UandV ⊆ U , we denote by η(u,V) the number of inaccurate dirty comparisons between uand elements\nofV,\nη(u,V) = #{v∈ V : 1(ub< v)̸= 1(u < v )}. (2)\nThis prediction model was introduced in [Bai and Coester, 2023] for sorting, but it has a broader significance\nin comparison-based problems, such as search [Borgstrom and Kosaraju, 1993; Nowak, 2009; Tschopp et al.,\n2011], ranking [Wauthier et al., 2013; Shah and Wainwright, 2018; Heckel et al., 2018; El Ferchichi et al.], and\nthe design of comparison-based ML algorithms [Haghiri et al., 2017, 2018; Ghoshdastidar et al., 2019; Perrot\net al., 2020; Meister and Nietert, 2021]. Particularly, it has great theoretical importance for priority queues,\nwhich have been extensively studied in the comparison-based framework [Gonnet and Munro, 1986; Brodal\nand Okasaki, 1996; Edelkamp and Wegener, 2000]. Comparison-based models are often used, for example,\nwhen the preferences are determined by human subjects. Assigning numerical scores in these cases is inexact\nand prone to errors, while pairwise comparisons are absolute and more robust [David, 1963]. Within such\na setting, dirty comparisons can be obtained by a binary classifier, and used to minimize human inference\nyielding clean comparisons, which are time-consuming and might incur additional costs.\nPointer predictions In this second model, upon the addition of a new key uto the priority queue Q, the\nalgorithm receives a prediction [Pred( u,Q)∈ Qof the predecessor of u, which is the largest key belonging to\nQand smaller than u. Before uis inserted, the ranks of uand its true predecessor in Qare equal, hence we\ndefine the prediction error as\n⃗ η(u,Q) =|r(u,Q)−r([Pred( u,Q),Q)|. (3)\nIn priority queue implementations, a HashMap preserves a pointer from each inserted key to its corresponding\nposition in the priority queue. Consequently, [Pred( u,Q) provides direct access to the predicted predecessor’s\nposition. For example, this prediction model finds applications in scenarios where concurrent machines have\naccess to the priority queue [Sundell and Tsigas, 2005; Shavit and Lotan, 2000; Lind´ en and Jonsson, 2013].\nEach machine can estimate, within the elements it has previously inserted, which one precedes the next\nelement it intends to insert. However, this estimation might not be accurate, as other concurrent machines\nmay have inserted additional elements.\nRank predictions The last setting assumes that the priority queue is used in a process where a finite\nnumber Nof distinct keys will be inserted, i.e., the priority queue is not used indefinitely, but Nis unknown\nto the algorithm. Upon the insertion of any new key ui, the algorithm receives a prediction bR(ui) of the\nrank of uiamong all the Nkeys{uj}j∈[N]. Denoting by R(ui) =r(ui,{uj}j∈[N]) the true rank of ui, the\nprediction error of uiis\nη∆(ui) =|R(ui)−bR(ui)|. (4)\nThe same prediction model was explored in [McCauley et al., 2024] for the online list labeling problem.\nBai and Coester [2023] investigate a similar model for the sorting problem, but in an offline setting where\ntheNelements to sort and the predictions are accessible to the algorithm from the start.\n2\n\nNote that Ncounts the distinct keys that are added either with Insert or DecreaseKey operations. An\narbitrarily large number of DecreaseKey operations thus can make Narbitrarily large although the total\nnumber of insertions is reduced. However, with a lazy implementation of DecreaseKey, we can assume\nwithout loss of generality that Nis at most quadratic in the total number of insertions. Indeed, it is possible\nto omit executing the DecreaseKey operations when queried, and only store the new elements’ keys to update.\nThen, at the first ExtractMin operation, all the element’s keys are updated by executing for each one only\nthe last queried DecreaseKey operation involving it. Denoting by kthe total number of insertions, there\nare at most kExtractMin operations, and for each of them, there are at most kDecreaseKey operations\nexecuted. The total number of effectively executed DecreaseKey operations is therefore O(k2). In particular,\nthis implies that a complexity of O(logN) is also logarithmic in the total number of insertions.\n1.2 Our results\nWe first investigate augmenting binary heaps with predictions. A binary heap Qwith nelements necessitates\nO(logn) time but only O(log log n) comparisons for insertion [Gonnet and Munro, 1986]. To leverage dirty\ncomparisons, we first design a randomized binary search algorithm to find the position of an element uin a\nsorted list L. We prove that it terminates using O(log(# L)) dirty comparisons and O(logη(u, L)) clean com-\nparisons in expectation. Subsequently, we use this result to establish an insertion algorithm in binary heaps\nusing O(log log n) dirty comparisons and reducing the number of clean comparisons to O(log log η(u,Q)) in\nexpectation. However, ExtractMin still mandates O(logn) clean comparisons. In the two other prediction\nmodels, binary heaps and other heap implementations of priority queues appear unsuitable, as the positions\nof the keys are not determined solely by their ranks.\nConsequently, in Section 3, we shift to using skip lists. We devise randomized insertion algorithms requir-\ning, in expectation, only O(log⃗ η(u,Q)) time and comparisons in the pointer prediction model, O(logn) time\nandO(logη(u,Q)) clean comparisons in the dirty comparison model, and O(log log N+log max i∈[N]η∆(ui))\namortized time and O(log max i∈[N]η∆(ui)) comparisons in the rank prediction model, where we use in the\nlatter an auxiliary van Emde Boas (vEB) tree [van Emde Boas et al., 1976]. Across the three prediction\nmodels, FindMin and ExtractMin only necessitate O(1) time, and the complexity of DecreaseKey aligns with\nthat of insertion. Finally, we prove in Theorem 4.1 the optimality of our data structure. Table 1.2 sum-\nmarizes the complexities of our learning-augmented priority queue (LAPQ) in the three prediction models\ncompared to standard priority queue implementations.\nPriority queues FindMin ExtractMin Insert DecreaseKey\nBinary Heap O(1) O(logn) O(log log n) O(logn)\nFibonacci Heap (amortized) O(1) O(logn) O(1)\nSkip List (average) O(1) O(1) O(logn)\nLAPQ with dirty comparisons (average) O(1) O(1) O(logη(u,Q))\nLAPQ with pointer predictions (average) O(1) O(1) O(log⃗ η(u,Q))\nLAPQ with rank predictions (average) O(1) O(1) O(log max i∈[n]η∆(ui))\nTable 1: Number of comparisons per operation used by different priority queues.\nOur learning-augmented data structure enables additional operations beyond those of priority queues,\nsuch as the maximum priority queue operations FindMax, ExtractMax, and IncreaseKey with analogous\ncomplexities, and removing an arbitrary key ufrom the priority queue, finding its predecessor or successor\nin expected O(1) time.\nFurthermore, we show in Section 5.1 that it can be used for sorting, yielding the same guarantees as the\nlearning-augmented sorting algorithms presented in [Bai and Coester, 2023] for the positional predictions\nmodel with displacement error , and for the dirty comparison model. In the second model, our priority queue\noffers even stronger guarantees, as it maintains the elements sorted at any time even if the insertion order\nis adversarial, while the algorithm of [Bai and Coester, 2023] requires a random insertion order to achieve a\nsorted list by the end within the complexity guarantees. We also show how the learning-augmented priority\nqueue can be used to accelerate Dijkstra’s algorithm.\nFinally, in Section 6, we compare the performance of our priority queue using predictions with binary\n3\n\nand Fibonacci heaps when used for sorting and for Dijkstra’s algorithm on both real-world city maps and\nsynthetic graphs. The experimental results confirm our theoretical findings, showing that adequately using\npredictions significantly reduces the complexity of priority queue operations.\n1.3 Related work\nLearning-augmented algorithms Learning-augmented algorithms, introduced in [Lykouris and Vassil-\nvtiskii, 2018; Purohit et al., 2018], have captured increasing interest over the last years, as they allow breaking\nlongstanding limitations in many algorithm design problems. Assuming that the decision-maker is provided\nwith potentially incorrect predictions regarding unknown parameters of the problem, learning-augmented\nalgorithms must be capable of leveraging these predictions if they are accurate (consistency), while keeping\nthe worst-case performance without advice even if the predictions are arbitrarily bad or adversarial (robust-\nness). Many fundamental online problems were studied in this setting, such as ski rental [Gollapudi and\nPanigrahi, 2019; Anand et al., 2020; Bamas et al., 2020; Diakonikolas et al., 2021; Antoniadis et al., 2021;\nMaghakian et al., 2023; Shin et al., 2023], scheduling [Purohit et al., 2018; Merlis et al., 2023; Lassota et al.,\n2023; Benomar and Perchet, 2024b], matching [Antoniadis et al., 2020; Dinitz et al., 2021; Chen et al., 2022],\nand caching [Lykouris and Vassilvtiskii, 2018; Chlkedowski et al., 2021; Bansal et al., 2022; Antoniadis et al.,\n2023b,a; Christianson et al., 2023]. Data structures can also be improved within this framework. However,\nthis remains underexplored compared to online algorithms. The seminal paper by Kraska et al. [2018] shows\nhow predictions can be used to optimize space usage. Another study by [Lin et al., 2022] demonstrates that\nthe runtime of binary search trees can be enhanced by incorporating predictions of item access frequency.\nRecent papers have extended this prediction model to other data structures, such as dictionaries [Zeynali\net al., 2024] and skip lists [Fu et al., 2024]. The prediction models we study deviate from the latter, and are\nmore related to those considered respectively in [Bai and Coester, 2023] for sorting, and [McCauley et al.,\n2024] for online list labeling. An overview of the growing body of work on learning-augmented algorithms\n(also known as algorithms with predictions) is maintained at [Lindermayr and Megow, 2022].\nDirty comparisons The dirty and clean comparison model is also related to the prediction querying\nmodel, gaining a growing interest in the study of learning-augmented algorithms, where the decision-maker\ndecides when to query predictions and for which items [Im et al., 2022; Benomar and Perchet, 2024a; Sadek\nand Elias, 2024]. In particular, having free-weak and costly-strong oracles has been studied in [Silwal et al.,\n2023] for correlation clustering, in [Bateni et al., 2023] for clustering and computing minimum spanning trees\nin a metric space, and in [Eberle et al., 2024] for matroid optimization. Another related setting involves the\nalgorithm observing partial information online, which it can then use to decide whether to query a costly\nhint about the current item. This has been explored in contexts such as online linear optimization [Bhaskara\net al., 2021] and the multicolor secretary problem [Benomar et al., 2024].\nPriority queues Binary heaps, introduced by Williams [1964], are one of the first efficient implementations\nof priority queues. They allow FindMin in constant time, and the other operations in O(logn) time, where n\nis the number of items in the queue. Shortly after, other heap-based implementations with similar guarantees\nwere designed, such as leftist heaps [Crane, 1972] and randomized meldable priority queues [Gambin and\nMalinowski, 1998]. A new idea was introduced in [Vuillemin, 1978] with Binomial heaps, where instead of\nhaving a single tree storing the items, a binomial heap is a collection of Θ(log n) trees with exponentially\ngrowing sizes, all satisfying the heap property. They allow insertion in constant amortized time, and O(logn)\ntime for ExtractMin and DecreaseKey. A breakthrough came with Fibonacci heaps [Fredman and Tarjan,\n1987], which allow all the operations in constant amortized time, except for ExtractMin, which takes O(logn)\ntime. It was shown later, in works such as [Brodal, 1996; Brodal et al., 2012], that the same guarantees\ncan be achieved in the worst-case, not only in amortized time. Although they offer very good theoretical\nguarantees, Fibonacci heaps are known to be slow in practice [Larkin et al., 2014; Lewis, 2023], and other\nimplementations with weaker theoretical guarantees such as binary heaps are often preferred. We refer the\ninterested reader to the detailed survey on priority queues by Brodal [2013].\nSkip lists A skip list [Pugh, 1990] is a probabilistic data structure, based on classical linked lists, having\nshortcut pointers allowing fast access between non-adjacent elements. Due to the simplicity of their imple-\n4\n\nmentation and their strong performance, skip lists have many applications [Hu et al., 2003; Ge and Zdonik,\n2008; Basin et al., 2017]. In particular, they can be used to implement priority queues [R¨ onngren and Ayani,\n1997], guaranteeing in expectation a constant time for FindMin and ExtractMin, and O(logn) time for Insert\nand DecreaseKey. They show particularly good performance compared to other implementations in the case\nof concurrent priority queues, where multiple users or machines can make requests to the priority queue\n[Shavit and Lotan, 2000; Lind´ en and Jonsson, 2013; Zhang and Dechev, 2015].\n2 Heap priority queues\nA common implementation of priority queues uses binary heaps, enabling all operations in O(logn) time.\nBinary heaps maintain a balanced binary tree structure, where all depth levels are fully filled, except possibly\nfor the last one, to which we refer in all this section as the leaf level . Moreover, it satisfies the heap property ,\ni.e., any key in the tree is smaller than all its children. To maintain these two structure properties, when a\nnew element is added, it is first inserted in the leftmost empty position in the leaf level, and then repeatedly\nswapped with its parent until the heap property is restored.\n2.1 Insertion in the comparison-based model\nInsertion in a binary heap can be accomplished using only O(log log n) comparisons, albeit O(logn) time,\nby doing a binary search of the new element’s position along the path from the leftmost empty position in\nthe leaf level to the root, which is a sorted list of size O(logn). To improve the insertion complexity with\ndirty comparisons, we first tackle the search problem in this setting.\n2.1.1 Search with dirty comparisons\nConsider a sorted list L= (v1, . . . , v k) and a target u, the position of uinLcan be found using binary search\nwith O(logk) comparisons. Extending ideas from Bai and Coester [2023] and Lykouris and Vassilvtiskii\n[2018], this complexity can be reduced using dirty comparisons. Indeed, we can obtain an estimated position\nbr(u, L) through a binary search with dirty comparisons, followed by an exponential search with clean com-\nparisons, starting from br(u, L) to find the exact position r(u, L). However, the positions of inaccurate dirty\ncomparisons can be adversarially chosen to compromise the algorithm. This can be addressed by introduc-\ning randomness to the dirty search phase. We refer to the randomized binary search as the algorithm that\nproceeds similarly to the binary search, but whenever the search is reduced to an array {vi, . . . , v j}, instead\nof comparing uto the pivot vmwith index m=i+⌊j−i\n2⌋, it compares uto a pivot with an index chosen\nuniformly at random in the range {i+⌈j−i\n4⌉, . . . , j − ⌈j−i\n4⌉}.\nLemma 2.1. The randomized binary search in a list of size kterminates after O(logk)comparisons almost\nsurely.\nProof. Let us denote by Stthe size of the sub-list to which the search is reduced after tcomparisons, and\nT= min {t≥1 :St= 1}. For convenience, we consider that St= 1 for all t > T . It holds that S0=n, and\nfor all t≥1, ifSt=j−i+ 1≥2 then\nSt+1≤max( j−ℓ, ℓ−i)≤j−i− ⌈j−i\n4⌉ ≤3\n4(j−i)≤3\n4St.\nOn the other hand, if St= 1 then St+1= 1. We deduce that St+1≤max(1 ,3\n4St) with probability 1, hence\nSt≤max(1 ,(3/4)tk) for all t≥1. Therefore, it holds almost surely that\nT≤ ⌈log4/3k⌉.\nWe now use the previous Lemma to establish the runtime and the comparison complexity of the dirty/clean\nsearch algorithm we described in a sorted list.\n5\n\nTheorem 2.2. A dirty randomized binary search followed by a clean exponential search finds the target’s\nposition using O(logk)dirty comparisons and, in expectation, O(logk)time and O(logη(u, L))clean com-\nparisons.\nProof. The algorithm runs the randomized binary search (RBS) with dirty comparisons to obtain an es-\ntimated position br(u, L) ofuinL, then conducts a clean exponential search starting from br(u, L) to find\nthe exact position r(u, L). Lemma 2.1 guarantees that the dirty RBS uses O(logk) comparisons, while the\nclean exponential search terminates in expectation after O(log|r(u, L)−br(u, L)|) comparisons. Therefore,\nfor demonstrating Theorem 2.2, it suffices to prove that E[log|r(u, L)−br(u, L)|] =O(logη(u, L)).\nTo simplify the expressions, we write ηinstead of η(u, L) in the rest of the proof. Let ( i0, j0) = (1 , n), and\n(it, jt) the indices delimiting the sub-list of Lto which the RBS is reduced after tsteps for all t≥1. Denoting\nbyt∗+ 1 the first step where the outcome of a dirty comparison queried by the algorithm is inaccurate,\nit holds that br(u, L), r(u, L)∈ {it∗, jt∗}. Indeed, the first t∗dirty comparisons are all accurate, thus the\nestimated and the true positions of uinLare both in {it∗, jt∗}. Therefore, |r(u, L)−br(u, L)| ≤St∗−1,\nwhere St=jt−it+ 1 is the size of the sub-list delimited by indices ( it, jt), which yields\nE\u0002\nlog|r(u, L)−br(u, L)|\u0003\n≤E[log(St∗)]. (5)\nWe focus in the remainder on bounding E[log(St∗)]. For this, we use a proof scheme similar to that of\nLemmas A.4 and A.5 in Bai and Coester [2023].\nE[log(St∗)]≤E[⌈log(St∗)⌉]\n≤∞X\nm≥1mPr(⌈log(St∗)⌉=m)\n≤log(η+ 1) +∞X\nm=⌈log(η+1)⌉mPr(⌈log(St∗)⌉=m), (6)\nand for all m≥1, we have that\nPr(⌈log(St∗)⌉=m) = Pr( St∗∈(2m−1,2m])\n=∞X\nt=1Pr(St∈(2m−1,2m], t=t∗)\n=∞X\nt=1Pr(St∈(2m−1,2m]) Pr( t=t∗|St∈(2m−1,2m]).\nIn the sum above, for any t≥1, the probability that t=t∗is the probability that the next sampled pivot\nindex ℓtis in the set F(u, L) ={v∈L: 1(ub< v)̸= 1(u < v )}, which has a cardinal η. Thus\nPr(t=t∗|St) = Pr( ℓt∈ F(u, L)|St) =#(F ∩ { it, . . . , j t})\nSt≤η\nSt.\nIt follows that\nPr(⌈log(St∗)⌉=m)≤η\n2m−1∞X\nt=1Pr(St∈(2m−1,2m])\n≤η\n2m−1E[#{t≥1 :St∈(2m−1,2m]}]\n≤3η\n2m−1,\nwhere the last inequality is an immediate consequence of the identity St+1≤3\n4St+1proved in Lemma 2.1,\nwhich shows in particular that St+3< St/2, i.e. the after at most 3 steps, the size of the search sub-list is\n6\n\ndivided by two, hence ( St)tfalls into the interval (2m−1,2m] at most three times. Substituting into (6) gives\nE[logSt∗]≤log(η+ 1) + 3 η·∞X\nm=⌈log(η+1)⌉m\n2m−1\n= log( η+ 1) + 3 η·O\u0012logη\nη\u0013\n=O(logη).\nTherefore, by (5), the expected number of comparisons used during the clean exponential search is at most\nO(E[log|r(u, L)−br(u, L)|]) =O(logη), which concludes the proof.\n2.1.2 Randomized insertion in a binary heap\nIn a binary heap Q, any new element uis always inserted along the path of size O(logn) from the root\nto the leftmost empty position in the leaf level. If all the inaccurate dirty comparisons are chosen along\nthis path, then the insertion would require in expectation O(logη(u,Q)) clean comparisons by Theorem 2.2.\nThis complexity can be reduced further by randomizing the choice of the root-leaf path where uis inserted,\nas explained in Algorithm 1.\nAlgorithm 1: Randomized insertion in binary heap\nInput: Binary heap Qwith randomly filled positions in the leaf level, new key u\n1L←keys path from a uniformly random empty position in the leaf level to the root;\n2br(u, L)←outcome of a dirty randomized binary search of uinL;\n3r(u, L)←outcome of the clean exponential search of uinLstarting from index br(u, L);\n4Insert uin the chosen leaf empty position, then swap it with its parents until position r(u, L);\nTheorem 2.3. The insertion algorithm 1 terminates in O(logn)time, using O(log log n)dirty comparisons\nandO(log log η(u,Q))clean comparisons in expectation.\nProof. Consider a binary heap Qcontaining nelements, with positions randomly filled in the leaf level.\nEven with this randomization, Qis still balanced, and the length of any path from an empty position in the\nleaf level to the root has a size O(logn). Denoting by Lthe random insertion path chosen in Algorithm 1,\nconstructing Land storing it as an array requires O(logn) time and space. By Theorem 2.2, the randomized\nsearch in Algorithm 1 uses O(log log n) dirty comparisons, and the exponential search uses O(E[logη(u, L)])\nclean comparisons in expectation. Inserting uand then swapping it up until its position does not use any\ncomparison and requires a O(logn) time. Therefore, to prove the theorem, it suffices to demonstrate that\nE[logη(u, L)] =O(log log η(u,Q)).\nWe assume that the key uto be inserted can be chosen by an oblivious adversary, unaware of the\nrandomization outcome. This means that the internal state of Qremains private at all times. Let F(u,Q) =\n{v∈ Q: 1(ub< v)̸= 1(u < v )}the set of keys in Qwhose dirty comparison with uis inaccurate, which has\na cardinal of η(u,Q). Enumerating the binary heap levels starting from the root, each level ℓexcept for the\nlast one, denoted ℓmax, contains exactly 2ℓ−1elements vℓ\n1, . . . , vℓ\n2ℓ−1. A key vℓ\niin level ℓhas a probability\n1/2ℓ−1of belonging to the path L. Denoting by ξℓ\ni= 1(vℓ\ni∈ F(u,Q)) for all ℓ∈[ℓmax−1] and i∈[2ℓ−1],\nthe expected number of keys in F(u,Q) belonging to Lis\nEL[η(u, L)] =ℓmax−1X\nℓ=12ℓ−1X\ni=1ξℓ\ni\n2ℓ−1=ℓmaxX\nℓ=11\n2ℓ−1\n2ℓ−1X\ni=1ξℓ\ni\n.\nGiven thatPℓmax\nℓ=1P2ℓ−1\ni=1ξℓ\ni=η(u,Q)≤2⌈log(η(u,Q)+1)⌉−1, the expression above is maximized under this\nconstraint for the instance ¯ξℓ\ni= 1(ℓ≤ ⌈log(η(u,Q) + 1)⌉). Therefore,\nEL[η(u, L)]≤X\nℓ1\n2ℓ−1\n2ℓ−1X\ni=1¯ξℓ\ni\n=⌈log(η(u,Q) + 1)⌉.\n7\n\nFinally, Jensen’s inequality and the concavity of log yield\nE[logη(u, L)]≤logE[η(u, L)] =O(log log η(u,Q)),\nwhich gives the result.\n2.2 Limitations\nThe previous theorem demonstrates that accurate predictions can reduce the number of clean comparisons for\ninsertion in a binary heap. However, for the ExtractMin operation, when the minimum key, which is the root\nof the tree, is deleted, its two children are compared and the smallest is placed in the root position, and this\nprocess repeats recursively, with each new empty position filled by comparing both of its children, requiring\nnecessarily O(logn) clean comparisons in total to ensure the heap priority remains intact. Improving the\nefficiency of ExtractMin using dirty comparisons would therefore require bringing major modifications to the\nbinary heap’s structure.\nSimilar difficulties arise when attempting to enhance ExtractMin using dirty comparisons or the other\nprediction models in different heap implementations, such as Binomial or Fibonacci heaps. Consequently,\nwe explore in the next section another priority queue implementation, using skip lists, which allows for an\neasier and more efficient exploitation of the predictions.\n3 Skip lists\nA priority queue can be implemented naively by maintaining a dynamic sorted linked list of keys. This\nguarantees constant time for ExtractMin, but O(n) time for insertion. Skip lists (see Figure 1) offer a\nsolution to this inefficiency, by maintaining multiple levels of linked lists, with higher levels containing fewer\nelements and acting as shortcuts to lower levels, facilitating faster search and insertion in expected O(logn)\ntime. In all subsequent discussions concerning linked lists or skip lists, it is assumed that they are doubly\nlinked, having both predecessor and successor pointers between elements.\nHEADNIL\nv1v2v3v4v5v6v7v8v9\nFigure 1: A skip list with keys v1< . . . < v 9∈ U.\nThe first level in a skip list is an ordinary linked list containing all the elements, which we denote by\nv1, . . . , v n. Every higher level is constructed by including each element from the previous level independently\nwith probability p, typically set to 1 /2. For any key viin the skip list, we define its height h(vi) as the\nnumber of levels where it appears, which is an independent geometric random variable with parameter p.\nA number 2 h(vi) of pointers are associated with vi, giving access to the previous and next element in each\nlevel ℓ∈[h(vi)], denoted respectively by Prev( vi, ℓ) and Next( vi, ℓ). Using a HashMap, these pointers can\nbe accessed in O(1) time via the key value vi. For convenience, we consider that the skip list contains two\nadditional keys v0=−∞andvn+1=∞, corresponding respectively to the head and the NIL value. Both\nhave a height equal to the maximum height in the queue h(v0) =h(vn+1) = max i∈[n]h(vi).\nSince the expected height of keys in the skip list is 1 /p, deleting any key only requires a constant time\nin expectation, by updating its associated pointers, along with those of its predecessors and successors in\nthe levels where it appears. In particular, FindMin and ExtractMin take O(1) time, and DecreaseKey can\nbe performed by deleting the element and reinserting it with the new key, yielding the same complexity as\n8\n\ninsertion. Furthermore, by the same arguments, inserting a new key unext to a given key viin the skip list\ncan be done in expected constant time.\nTherefore, implementing efficient Insert and DecreaseKey operations for skip lists with predictions is\nreduced to designing efficient search algorithms to find the predecessor of a target key uin the skip list, i.e.,\nthe largest key viin the skip list that is smaller than u. In all the following, we denote by Qa skip list\ncontaining nkeys v1≤. . .≤vn∈ U, and u∈ Uthe target key.\nThe keys of the priority queue can be considered pairwise distinct by grouping elements with the same\nkey together in a collection, for example, a HashSet. This collection can be accessible in O(1) time via the\nkey using a HashMap. When a new item xwith priority uis to be inserted, if uis already in the priority\nqueue, then xis added to the corresponding collection in O(1) time. With such implementation, when an\nExtractMin operation is called, if multiple elements correspond to the minimum key, then the algorithm can,\nfor example, retrieve an arbitrary one of them, or the first inserted one, depending on the use case. In the\nfollowing, we present separately for each model an insertion algorithm leveraging the predictions.\nMaximum of i.i.d. geometric random variables Before presenting the insertion algorithms in the\nthree prediction models, we present an upper bound from [Eisenberg, 2008] on the expected maximum of\ni.i.d. geometric random variables with parameter p. This Lemma will be useful in the analysis of our\nalgorithms, as the heights of elements in the skip list are i.i.d. geometric random variables.\nLemma 3.1 ([Eisenberg, 2008]) .IfX1, . . . , X mare i.i.d. random variables following a geometric random\ndistribution with parameter p, then, denoting by q= 1−p, it holds that\nE[max\ni∈[m]Xi]≤1 +1\nlog(1 /q)mX\nk=11\nk=O(log1/qm).\n3.1 Pointer prediction\nGiven a pointer prediction vj=[Pred( u,Q), we describe below an algorithm for finding the true predecessor\nofustarting from the position of the key vj, then inserting u. We assume in the algorithm that vj≤u. If\nvj> u, then the algorithm can be easily adapted by reversing the search direction.\nAlgorithm 2: ExpSearchInsertion( Q, vj, u)\nInput: Skip list Q, source vj∈ Q, and new key u∈ U\n1w←vj; ▷Bottom-Up search\n2while Next( w, h(w))≤udo\n3 w←Next( w, h(w));\n4ℓ←h(w); ▷Top-Down search\n5while ℓ >0do\n6 while Next( w, ℓ)≤udo\n7 w←Next( w, ℓ);\n8 ℓ←ℓ−1;\n9Insert unext to w;\nAlgorithm 2 is inspired by the classical exponential search in arrays, but is adapted to leverage the skip\nlist structure. The first phase consists of a bottom-up search, expanding the size of the search interval by\nmoving to upper levels until finding a key w∈ Q satisfying w≤u < Next( w, h(w)). The second phase\nconducts a top-down search from level h(w) downward, refining the search until locating the position of\nu. It is worth noting that the classical search algorithm in skip lists, denoted by Search( Q, u), corresponds\nprecisely to the top-down search, starting from the head of the skip list instead of w.\nTheorem 3.2. Augmented with pointer predictions, a skip list allows FindMin andExtractMin in expected\nO(1)time, and Insert( u)in expected O(log⃗ η(u,Q))time using Algorithm 2.\n9\n\nProof of Theorem 3.2. The key wfound at the end of the algorithm is the processor of uinQ. Inserting\nunext to wonly requires expected O(1) time. Thus, we demonstrate in the following that Algorithm 2,\nstarting from a key vj∈ Q, finds the predecessor of uinO(log|r(vj)−r(u)|) expected time, where r(v)\ndenotes the rank r(v,Q) ofvinQ. In particular, for vj=[Pred( u,Q), we obtain the claim of the theorem.\nWe assume in the proof that vj< u, i.e. the exponential search goes from left to right. Let h∗(vj, u) be\nthe maximum height of all elements in Qbetween uandvj\nh∗(vj, u) = max {h(v) :v∈ Qsuch that vj≤v≤u}.\nThe number of elements between vjandu, with vjincluded, is |r(u)−r(vj)|+ 1, and the heights of all the\nelements in Qare independent geometric random variables with parameter p, thus Lemma 3.1 gives that\nE[h∗(vj, u)] =O(log|r(u)−r(vj)|).\nThe key w∗found at the end of the Bottom-Up search is the last element, going from vjtou, having a\nheight of h∗(vj, u). Indeed, in the Bottom-Up search, whenever the algorithm reaches a new key, it moves to\nthe maximum level to which the key belongs, the height of w∗is therefore necessarily the maximum height\nof all the keys between vjandw∗, i.e. h(w∗) =h∗(vj, w∗). Since the Bottom-Up search stops at key w∗,\nthen Next( w∗, h(w∗))> u, which means that there is no key in Qbetween w∗anduhaving a height more\nthan h(w∗)−1.\nThe number of comparisons made in this phase is therefore at most the number of comparisons needed to\nreach level h∗(vj, u) + 1 starting from vjusing the Bottom-Up search. We consider the hypothetical setting\nwhere the skip list is infinite to the right, the expected number of comparisons to reach level h∗(vj, u) + 1,\nin this case, is an upper bound on the expected number of comparisons needed in the Bottom-Up phase of\nAlgorithm 2, as the algorithm also terminates if the end of the skip-list is reached. Let T(ℓ) be the expected\nnumber of comparisons made in the bottom-up search to reach level ℓin an infinite skip list. After each\ncomparison made in the bottom-up search, it is possible to go at least one level up with probability p, while\nthe algorithm can only move horizontally to the right with probability 1 −p. This induces the inequality\nT(ℓ)≤1 +pT(ℓ−1) + (1 −p)T(ℓ),\nwhich yields\nT(ℓ)≤1\np+T(ℓ−1).\nGiven that T(1) = 0, we have for ℓ≥1 that T(ℓ)≤ℓ−1\np, and we deduce that the expected number of\ncomparisons made by the algorithm during the Bottom-Up search is at mostE[h∗(vj,u)]\np=O(log|r(vj)−r(u)|).\nIn the Top-Down search described in the second phase, the path traversed by the algorithm is exactly the\ninverse of the Bottom-Up search from the predecessor of utow∗. The same arguments as the analysis of the\nfirst phase give that the Top-Down search terminates after O(log|r(vj)−r(u)|) comparisons in expectation,\nwhich concludes the proof.\n3.2 Dirty comparisons\nWe devise in this section a search algorithm using dirty and clean comparisons. Algorithm 3 first estimates\nthe position of uwith a dirty Top-Down search starting from the head, then performs a clean exponential\nsearch starting from the estimated position to find the true position.\nAlgorithm 3: Insertion with dirty and clean comparisons\nInput: Skip list Q, new key u∈ U\n1ˆw←Search( Q, u) with dirty comparisons;\n2ExpSearchInsertion( Q,ˆw, u) with clean comparisons;\nThe dirty search concludes within O(logn) steps, and Theorem 3.2 guarantees that the exponential search\nterminates within O(log|r( ˆw,Q)−r(u,Q)|) steps. Combining these results and relating the distance between\nuand ˆwinQto the prediction error η(u,Q), we derive the following theorem.\n10\n\nTheorem 3.3. Augmented with dirty comparisons, a skip list allows FindMin andExtractMin inO(1)\nexpected time, and Insert( u)with Algorithm 3 in O(logn)expected time, using O(logn)dirty comparisons\nandO(logη(u,Q))clean comparisons in expectation.\nProof. LetF(u,Q) the set of keys in Qwhose dirty comparisons with uare inaccurate\nF(u,Q) ={v∈ Q: 1(ub< v)̸= 1(u < v )}.\nThe prediction error η(u,Q) defined in (2) is the cardinal of F(u,Q). Let\nh∗(F(u,Q)) = max {h(v) :v∈ F(u,Q)}\nbe the maximal height of elements in F(u,Q). The search algorithm Search( Q, u) with dirty comparisons\nstarts from the highest level at the head of the skip list, and then goes down the different levels until finding\nthe predicted position ˆ wofu. This is the classical search algorithm in skip lists, and it is known to require\nO(logn) comparisons to terminate. This can also be deduced from the analysis of the exponential search\ndescribed in Algorithm 2, as it corresponds to the Top-Down search starting from the head of the skip list.\nBefore level h∗(F(u,Q)) is reached in Search( Q, u), all the dirty comparisons are accurate. Denot-\ning by v′the last key in Qvisited in a level higher than h∗(F(u,Q)) during this search, and v′′=\nNext(Q, v′, h∗(F(u,Q))), it holds that both keys ˆ wandware between v′andv′′, and there is no key\ninQbetween v′andv′′with height more than h∗(F(u,Q))−1.\nIn particular, the maximal key height between ˆ wandwis at most h∗(F(u,Q))−1. We showed in the\nproof of Theorem 3.2 that the number of comparisons and runtime of ExpSearchInsertion( Q, vj, u) is linear\nwith the maximal height of keys in Qthat are between vjandu. Using this result with ˆ winstead of vjgives\nthat ExpSearchInsertion( Q,ˆw, u) finds the position of uusing O(h∗(F(u,Q))) clean comparisons. Finally,\nsince h∗(F(u,Q)) is the maximum of a number η(u,Q) of i.i.d. geometric random variables with parameter\np, Lemma 3.1 gives that\nE[h∗(F(u,Q))] = O(logη(u,Q)),\nwhich proves the theorem.\n3.3 Rank predictions\nIn the rank prediction model, each Insert( u) request is accompanied by a prediction bR(u) of the rank of\nuamong all the distinct keys already in, or to be inserted into the priority queue. If the predictions are\naccurate and the total number Nof distinct keys to be inserted is known, the problem reduces to designing\na priority queue with integer keys in [ N], taking as keys the ranks ( Ri)i∈[N]. This problem can be addressed\nusing a van Emde Boas (vEB) tree over [ N] [van Emde Boas et al., 1976], guaranteeing O(log log N) runtime\nfor all the priority queue operations. In the case of imperfect rank predictions, we will also use an auxiliary\nvEB tree to accelerate the runtime of the priority queue.\n3.3.1 Van Emde Boas (vEB) trees\nA vEB tree over an interval {i+ 1, . . . , i +m}has a root with√mchildren, each being the root of a smaller\nvEB tree over a sub-interval {i+k√m+ 1, . . . , i + (k+ 1)√m}for some k∈ {0, . . . ,√m−1}. The tree\nleaves are either empty or contain elements with the corresponding key, stored together in a collection, and\ninternal nodes carry binary information indicating whether or not the subtree they root contains at least\none element. Denoting by H(m) the height of a vEB tree of size m, it holds that H(m) = 1 + H(√m),\nwhich yields that H(m) =O(log log m), enabling efficient implementation of the operations listed below in\nO(log log m) time:\n•Insert( x, k): insert a new element xwith key k∈[m] in the tree,\n•Delete( x, k): Delete the element/key pair ( x, k),\n•Predecessor( k): return the element in the tree with the largest key smaller than or equal to k,\n•Successor( k): return the element in the tree with the smallest key larger than or equal to k,\n11\n\n•ExtractMin(): removes and returns the element with the smallest key.\nOther operations such as FindMin, FindMax, or Lookup( k) are supported in O(1) time. These runtimes,\nhowever, require knowing the maximal key value mfrom the beginning, as it is used for constructing Tm.\n3.3.2 Dynamic size vEB trees\nIf the maximal key value ¯Ris unknown, we argue that the operations listed above can be supported in\namortized O(log log ¯R) time. Given a vEB tree Tmof size m, if a new key k∈ {m+ 1, . . . , 2m}is to be\ninserted, we construct an empty vEB tree T2mof size 2 minO(m) time, then repeatedly extract the elements\nwith minimal key from Tmand insert them in T2mwith the same key. Each ExtractMin operation in Tm\nand insertion in T2mrequires O(log log m) time. Therefore, constructing T2mand inserting all the elements\nfromTmtakes O(mlog log m) time.\nThis observation can be used to define a vEB with dynamic size. First, we construct a vEB tree with an\ninitial constant size R0. If at some point the size of the vEB tree is m≥R0and a new key k > m is to be\ninserted, then we iterate the size doubling process described before until the size of the vEB tree is at least\nk. At any time step, denoting by ¯Rthe maximal key value inserted in the vEB tree, and letting i≥1 such\nthat 2i−1R0≤¯R <2iR0, the size of the tree has been doubled up to this step itimes to cover all the keys.\nThe total time for resizing the vEB tree is at most proportional to\ni−1X\nj=02jR0log log(2jR0)≤\u0010i−1X\nj=02j\u0011\nR0log log ¯R\n≤2iR0log log ¯R\n≤2¯Rlog log ¯R .\nTherefore, if Nis the total number of elements inserted into the vEB tree and ¯Rthe maximum key value,\nwe can neglect the cost of resizing by considering that each insertion requires an amortized time of O((1 +\n¯R\nN) log log ¯R). The runtime of all the other operations is O(log log ¯R). In particular, if ¯R=O(N), then all\nthe operations run in O(log log N) amortized time.\n3.3.3 Insertion with rank predictions\nConsider the setting where Nis unknown and all the predicted ranks, revealed online, satisfy R(ui) =O(N).\nThe priority queue we consider is a skip list Qwith an auxiliary dynamic size vEB tree T. For insertions,\nwe use Algorithm 4. For ExtractMin, we first extract the minimum uminfromQin expected O(1) time,\nthen we delete it from the corresponding position bR(umin) inTinO(log log N) time. Deleting an arbitrary\nkeyufromQcan be done in the same way, by removing it from Qin expected O(1) time and then deleting\nit from the position bR(u) inTinO(log log N) time. Thus, as in the other prediction models, DecreaseKey\ncan be implemented by deleting the element and reinserting it with the new key, which requires the same\ncomplexity as insertion, with an additional O(log log N) term.\nAlgorithm 4: Insertion with rank prediction\nInput: Skip list Q, dynamic size vEB tree T, new element ui∈ U, prediction bR(ui)\n1Insert uiinTwith key bR(ui);\n2ˆw←predecessor of uiinT;\n3ExpSearchInsertion( Q,ˆw, ui);\nWhenever a new key uiis to be added, Algorithm 4 inserts it first in Tat position bR(ui), gets its\npredecessor ˆ winT, i.e., the element in Twith the largest predicted rank smaller than or equal to bR(ui),\nthen uses ˆ was a pointer prediction to find the position of uiinQ. If the predecessor is not unique, the\nalgorithm chooses an arbitrary one. We prove the following theorem, giving the runtime and comparison\ncomplexities achieved by this priority queue.\n12\n\nTheorem 3.4. IfbR(ui) =O(N)for all i∈[N], then there is a data structure allowing FindMin and\nExtractMin inO(1)amortized time, and Insert inO(log log N+ log max i∈[N]η∆(ui))amortized time using\nO(log max i∈[N]η∆(ui))comparisons in expectation.\nIn contrast to other prediction models, the complexity of inserting uiis not impacted only by η∆(ui),\nbut by the maximum error over all keys {uj}j∈[N]. This occurs because the exponential search conducted in\nAlgorithm 4 starts from the key ˆ w∈ Q, whose error also affects insertion performance. A similar behavior\nis observed in the online list labeling problem [McCauley et al., 2024], where the bounds provided by the\nauthors also depend on the maximum prediction error for insertion.\nWith perfect predictions, the number of comparisons for insertion becomes constant, and its runtime\nO(log log N). It is not clear if the runtime of all the priority queue operations can be reduced to O(1) with\nperfect predictions. Indeed, the problem in that case is reduced to a priority queue with all the keys in\n[N]. The best-known solution to this problem is a randomized priority queue, proposed by Thorup [2007],\nsupporting all operations in O(√log log N) time. However, in our approach, we use vEB trees beyond the\nclassical priority queue operations, as we also require fast access to the predecessor of any element. A data\nstructure supporting all these operations solves the dynamic predecessor problem, for which vEB trees are\noptimal [P˘ atra¸ scu and Thorup, 2006]. Reducing the runtime of insertion below O(log log N) would therefore\nrequire omitting the use of predecessor queries.\n3.3.4 Proof of Theorem 3.4\nWe denote by u1, . . . , u Nthe distinct keys that will be inserted in the priority queue. For all t∈[N], we\ndenote by Qt,Ttthe set of keys in the skip list and the set of integer keys in the dynamic vEB tree right\nafter the insertion of ut. Note that, due to the eventual ExtractMin operations, the sizes of QtandTtcan\nbe smaller than t. Following the notation in (1), for all i, t∈[N], we denote by r(ui,Qt) the rank of ui\ninQt, and r(bR(ui),Tt) the rank of bR(ui) inTt. The following lemma shows that the absolute difference\nbetween the two previous quantities for any given i, tis at most twice the maximal rank prediction error.\nLemma 3.5. For any subset I⊂[N], it holds for all i∈Ithat\n|r(ui,{uj}j∈I)−r(bR(ui),{bR(uj)}j∈I)| ≤2 max\nj∈[N]η∆(uj).\nProof. Let\n∆∗= max\nI⊂[N]\u0012\nmax\ni∈I|r(ui,{uj}j∈I)−r(bR(ui),{bR(uj)}j∈I)|\u0013\n, (7)\nand let I⊂[N] for which this maximum is reached. We assume without loss of generality that I= [m]\nfor some m∈[N]. For all s∈[N], let\n˜Qs={uj}j∈[s],˜Ts={bR(uj)}j∈[s],∆s= max\ni∈[s]|r(ui,˜Qs)−r(bR(ui),˜Ts)|.\nTo simplify the expressions, we denote by rs\ni=r(ui,˜Qs) andbrs\ni=r(bR(ui),˜Ts) for all ( s, i)∈[N]2. We will\nprove that ∆s= ∆∗for all s∈ {m, . . . , N }. By definition of ∆ ∗, it holds that ∆ ∗≥∆sfor all s, it remains\nto prove the other inequality. It is true for s=mby definition of Iandm. Now let s∈ {m, . . . , N −1}and\nassume that ∆s= ∆∗, i.e. there exists i≤ssuch that |rs\ni−brs\ni|= ∆∗. Assume for example that brs\ni=rs\ni+∆∗.\n•Ifus+1< ui, then rs+1\ns+1< rs+1\niandrs+1\ni=rs\ni+ 1. By definition of ∆ ∗, it holds that\nbrs+1\ns+1≤rs+1\ns+1+ ∆∗≤rs+1\ni−1 + ∆ ∗=rs\ni+ ∆∗=brs\ni.\nThis implies necessarily that bR(us+1)≤bR(ui), and therefore\nbrs+1\ni=brs\ni+ 1 = rs\ni+ 1 + ∆ ∗=rs+1\ni+ ∆∗,\nwhich gives that ∆s+1≥ |brs+1\ni−rs+1\ni|= ∆∗.\n13\n\n•Ifus+1> ui, then rs+1\ni=rs\niandbrs+1\ni≥brs\ni, thus ∆s+1≥brs+1\ni−rs+1\ni≥brs\ni−rs\ni= ∆∗.\n•Ifus+1=uithen rs+1\ns+1=rs+1\ni=rs\ni+ 1. On the other hand, if bR(us+1)≤bR(ui) then brs+1\ni=brs\ni+ 1,\notherwise brs+1\ns+1≥brs\ni+ 1. In both cases, it holds that\n∆s+1≥max(brs+1\ni−rs+1\ni,brs+1\ns+1−rs+1\ns+1)\n≥(brs\ni+ 1)−(rs\ni+ 1) = ∆ ∗.\nThe same proof can be used for the case where rs\ni=brs\ni+ ∆∗. Therefore, we have for all s∈ {m, . . . , N }that\n∆∗= ∆s. In particular, for s=N, observing that r(ui,˜QN) =R(ui) for all i∈[N], we obtain\n∆∗= max\ni∈[N]|R(ui)−r(bR(ui),˜TN)|. (8)\nLet us denote by η∆\nmax= max j∈[N]η∆(uj) the maximum rank prediction error. We will prove in the\nfollowing that\n∀i∈[N] :|R(ui)−r(bR(ui),˜TN)| ≤2η∆\nmax.\nWith the assumption that the keys {ui}i∈[N]are pairwise distinct, the ranks ( R(ui))i∈[N]form a permutation\nof [N].\nGiven that |R(uk)−bR(uk)| ≤η∆\nmaxfor all k∈[N], it holds for any i, j∈[N] that\nbR(uj)≤bR(ui) =⇒R(uj)−η∆\nmax≤R(ui) +η∆\nmax\n=⇒R(uj)≤R(ui) + 2η∆\nmax,\nhence, given that TN={bR(uj)}j∈[N]and by definition (1) of the rank r(bR(ui),TN), we have\nr(bR(ui),TN) = #{j∈[N] :bR(uj)≤bR(ui)}\n≤#{j∈[N] :R(uj)≤R(ui) + 2η∆\nmax}\n= #{k∈[N] :k≤R(ui) + 2η∆\nmax} (9)\n= min( N , R (ui) + 2η∆\nmax)\n≤R(ui) + 2η∆\nmax, (10)\nwhere Equation 9 holds because ( R(ui))i∈[N]is a permutation of [ N]. Similarly, we have for all i, j∈[N]\nthat\nR(uj)≤R(ui)−2η∆\nmax=⇒R(uj) +η∆\nmax≤R(ui)−η∆\nmax\n=⇒bR(uj)≤bR(ui),\nand it follows for all i∈[N] that\nr(bR(ui),TN) = #{j∈[N] :bR(uj)≤bR(ui)}\n≥#{j∈[N] :R(uj)≤R(ui)−2η∆\nmax}\n= #{k∈[N] :k≤R(ui)−2η∆\nmax}\n= max(0 , R(ui)−2η∆\nmax)\n=R(ui)−2η∆\nmax. (11)\nFrom (10) and (11), we deduce that\n∀i∈[N] :|R(ui)−r(bR(ui),TN)| ≤2η∆\nmax,\nCombining this with (8) and (7) yields the wanted result.\n14\n\nProof of the theorem Using the previous lemma and the complexity of ExpSearchInsertion proved in\nTheorem 3.2, we prove Theorem 3.4.\nProof. When a key uiis to be inserted, it is first inserted in the dynamic vEB tree Tiwith integer key bR(ui),\nand its predecessor ˆ winTiis retrieved. These first operations require O(log log N) time. The position of ui\ninQiis then obtained via an exponential search starting from ˆ w, which requires O(log|r(ui,Qi)−r( ˆw,Qi)|)\nexpected time by Theorem 3.2. Finally, inserting uin the found position takes expected O(1) time.\nFor any newly inserted element, by accounting for the potential future deletion runtime via ExtractMin\nat the moment of insertion, we can consider that all ExtractMin operations require constant amortized time,\nand that an additional O(log log N) time is needed for insertions. Therefore, to prove Theorem 3.4, we only\nneed to show that log |r(ui,Qi)−r( ˆw,Qi)|=O(log max j∈[N]η∆(ui)). Since ˆ wis the predecessor of uiinTi,\nit holds that r(bR(ui),Ti)∈ {r(bR( ˆw),Ti), r(bR( ˆw,Ti) + 1}, the first case occurs if bR(ui) =bR( ˆw), and the\nsecond if bR(ui)>bR( ˆw). Using this observation and Lemma 3.5, it follows that\n|r(ui,Qi)−r( ˆw,Qi)|\n≤ |r(ui,Qi)−r(bR(ui),Ti)|+|r(bR(ui),Ti)−r(bR( ˆw),Ti)|+|r( ˆw,Qi)−r(bR( ˆw),Ti)|\n≤4 max\nj∈[N]η∆(uj) + 1 ,\nand it follows that log |r(ui,Qi)−r( ˆw,Qi)|=O(log max j∈[N]η∆(ui)).\n4 Lower bounds\nAs explained earlier, ExtractMin requires only O(1) expected time in skip lists. Furthermore, we presented\ninsertion algorithms for the three prediction models and provided upper bounds on their complexities. The\nfollowing theorem establishes lower bounds on the complexities of ExtractMin and Insert for any priority\nqueue augmented with any of the three prediction types.\nTheorem 4.1. For each of the three prediction models, the following lower bounds hold.\n(i) Dirty comparisons: no data structure Qcan support ExtractMin with O(1)clean comparisons and\nInsert( u)with o(logη(u,Q))clean comparisons in expectation.\n(ii) Pointer predictions: no data structure Qcan support ExtractMin withO(1)comparisons and Insert( u)\nwith o(log⃗ η(u,Q))comparisons in expectation.\n(iii) Rank predictions: no data structure Qcan support ExtractMin with O(1)comparisons and Insert( ui)\nwith o(log max i∈[N]η∆(ui))comparisons in expectation, for all i∈[N].\nThese lower bounds with Theorems 3.3 and 3.2 prove the tightness of our priority queue in the dirty\ncomparison and the pointer prediction models. In the rank prediction model, the comparison complexities\nproved in Theorem 3.4 are optimal, whereas the runtimes are only optimal up to an additional O(log log N)\nterm. In particular, they are optimal if the maximal error is at least Ω(log N).\nThe key argument for proving the lower bounds is a reduction from the design of priority queues to\nsorting. Indeed, a priority queue can be used for sorting a sequence A= (ai)i∈[n]∈ Un, by first inserting\nall the elements in the priority queue, then repeatedly extracting the minimum until the priority queue is\nempty. In settings with dirty comparisons or positional predictions, the number of comparisons required by\nthis sorting algorithm is constrained by the impossibility result demonstrated in Theorem 1.5 of Bai and\nCoester [2023]. We use this impossibility result to prove the lower bounds stated in Theorem 4.1.\n4.1 Impossibility result for sorting with predictions\nWe begin by summarizing the setting and the impossibility result demonstrated in Theorem 1.5 of [Bai and\nCoester, 2023] for sorting with predictions.\n15\n\nPositional predictions In the positional prediction model, the objective is to sort a sequence A= (ai)i∈[n],\ngiven a prediction bRi∈[n] ofRi=r(ai, A) for all i∈[n]. This model differs from our rank prediction model\nin that the sequence Aand the predictions bR= (bRi)i∈[n]are given offline to the algorithm. Two different\nerror measures are considered in [Bai and Coester, 2023], but we restrict ourselves to the displacement error\nη∆\ni=|r(ai, A)−bRi|, which is the same as our rank prediction error 4. In all the following, consider that\nbRis a fixed permutation of [ n], i.e. the predicted ranks are pairwise distinct. For all ξ≥1, consider the\nfollowing set of instances\ncand( ˆR, nξ ) ={A∈ Un:nX\ni=1log(η∆\ni+ 2)≤nξ}.\nBai and Coester [2023] prove that, for 1 ≤ξ≤O(logn), no algorithm can sort every instance from\ncand( ˆR, nξ ) with o(nξ) comparisons. However, in their proof, they demonstrate a stronger result: no\nalgorithm can sort every instance from In(bR, ξ) with o(nξ) comparisons, where In(bR, ξ) is the subset of\ncand( ˆR, nξ ) defined by\nIn(bR, ξ) ={A∈ Un: max\ni∈[n]log(η∆\ni+ 2)≤ξ}. (12)\nDirty comparisons in the dirty comparison model, the authors prove an analogous result by a reduction\nto the positional prediction model. More precisely, any permutation bRon [n] defines a unique dirty order\nb<onAgiven by ( aib< aj) = (bRi<bRi), and max i∈[n]ηi≤max i∈[n]η∆\ni, where ηi=η(ai, A) = #{j∈[n] :\n(aib< aj)̸= (ai< aj)}. We deduce that\nIn(bR, ξ)⊂ {A∈ Un: max\ni∈[n]log(ηi+ 2)≤ξ}. (13)\nHence, given the dirty order b<, there is no algorithm that can sort every instance with max i∈[n]log(ηi+2)≤\nξino(nξ) time. In the following, we use these lower bounds on sorting to prove our Theorem 4.1.\n4.2 Proof of the Theorem\nFor any permutation πof [n] and i∈[n], we denote by Aπ\ni={aπ(j)}j∈[i]. This first elementary Lemma uses\nideas from the proof of Theorem 1.3 in Bai and Coester [2023].\nLemma 4.2. A permutation πof[n]satisfying bRπ(1)≤. . .≤bRπ(n)can be constructed in O(n)time, and it\nholds for all i∈ {2, . . . , n }that\n|r(aπ(i), Aπ\ni−1)−r(aπ(i−1), Aπ\ni−1)| ≤η∆\nπ(i−1)+η∆\nπ(i)+bRπ(i)−bRπ(i−1).\nProof. The elements of Acan be sorted in non-decreasing order of their predicted ranks bRiwithin O(n)\ntime using a bucket sort. Hence, we obtain the permutation π. For all i∈ {2, . . . , n },|r(aπ(i), Aπ\ni−1)−\nr(aπ(i−1), Aπ\ni−1)|is the number of elements in Aπ\ni−1whose ranks are between those of aπ(i)andaπ(i−1).\nThis is at most the number of elements in Awhose ranks are between those of aπ(i)andaπ(i−1), which is\n|Rπ(i)−Rπ(i−1)|. We deduce that\n|r(aπ(i), Aπ\ni)−r(aπ(i−1), Aπ\ni)| ≤ |Rπ(i)−Rπ(i−1)|\n≤ |Rπ(i)−bRπ(i)|+|Rπ(i−1)−bRπ(i−1)|+|bRπ(i)−bRπ(i−1)|\n=η∆\nπ(i−1)+η∆\nπ(i)+bRπ(i)−bRπ(i−1),\nwhere we used the triangle inequality and that bRπ(i)≥bRπ(i−1).\nWe move now to the proof of our lower bound in the pointer prediction model, by reducing the problem\nof sorting with positional predictions to the design of a learning-augmented priority queue with pointer\npredictions.\n16\n\n4.2.1 Lower bound in the pointer prediction model\nProof. Assume that there exists an implementation of a priority queue Qaugmented with pointer predic-\ntions, supporting ExtractMin with O(1) comparisons and the insertion of any new key uwith o(log⃗ η(u,Q))\ncomparisons. This means that, regardless of the history of operations made on Q, the number of comparisons\nused by ExtractMin is at most a constant C, and for any ξ≥1, inserting a key usuch that log( ⃗ η(u,Q)+2)≤ξ\nrequires at most ε(ξ)ξcomparisons, where ε(·) is a positive function satisfying lim ξ→∞ε(ξ) = 0. We will\nshow that this contradicts the impossibility result on sorting with positional predictions.\nLetω(1)≤ξ≤O(logn),bRa fixed permutation of [ n], and Aan arbitrary instance from the set In(bR, ξ)\ndefined in (12). Let πbe the permutation satisfying the property of Lemma 4.2, and consider the sorting\nalgorithm which inserts the elements of AinQin the order given by π, then extracts the minimum repeatedly\nuntilQis emptied. Let us denote by Qithe state of the Qafter iinsertions. Upon the insertion of aπ(i),\nthe algorithm uses [Pred( aπ(i),Qi−1) =aπ(i−1)as a pointer prediction. By (3), the error of this prediction is\n⃗ η(aπ(i),Qi−1) =|r(aπ(i), Aπ\ni−1)−r(aπ(i−1), Aπ\ni−1)|,\nwhere Aπ\ni−1={aπ(j)}j<i, and by Lemma 4.2, we have that\n⃗ η(aπ(i),Qi−1)≤η∆\nπ(i−1)+η∆\nπ(i)+bRπ(i)−bRπ(i−1). (14)\nbRis a permutation of [ n], hence bRπ(i)=bRπ(i−1)+ 1 by definition of π. Moreover, A∈ In(ξ), thus\nmax(log( η∆\nπ(i−1)+ 2),log(η∆\nπ(i)+ 2))≤ξ, and it follows that\nlog(⃗ η(aπ(i),Qi−1) + 2) ≤log(η∆\nπ(i−1)+η∆\nπ(i)+ 3)\n≤log(η∆\nπ(i−1)+ 2) + log( η∆\nπ(i)+ 2)\n≤2ξ ,\nwhere the second inequality is a consequence of log( α+β)≤log(α) + log( β) for all α, β≥2.\nTherefore, all the insertions into Qrequire at most (2 ξ·ε(2ξ)) comparisons, and the total number of\ncomparisons Tused to sort Ais at most\nT≤n(2ξ·ε(2ξ) +C)\n=\u0012\n2ε(2ξ) +C\nξ\u0013\nnξ\n=o(nξ),\nbecause ξ=w(1), i.e. lim n→∞ξ=∞, and lim ξ→∞(2ε(2ξ) +C/ξ) = 0. This means that any instance\ninIn(ξ) can be sorted using o(nξ) comparisons, which contradicts the lower bound on sorting algorithms\naugmented with positional predictions.\n4.2.2 Rank prediction and dirty comparisons model\nIn the rank prediction model, the total number of inserted keys is N=n. Denote by η∆\ni=η∆(ai, A) for\nalli∈[n]. If there is a data structure Qnot satisfying the lower bound of Theorem 4.1 for positional\npredictions, then we can use it for sorting A, and similarly to the proof for pointer predictions, if bRis a\npermutation of [ n], using Qfor sorting any instance A∈ In(ξ) with ω(1)≤ξ≤O(logn) requires at most\no(nξ) comparisons, which contradicts the lower bound on learning-augmented sorting algorithms.\nThe same arguments, combined with (13), give the result also for the dirty comparison model.\n5 Applications\n5.1 Sorting algorithm\nOur learning-augmented priority queue can be used for sorting a sequence A= (a1, . . . , a n), by first inserting\nall the elements, then repeatedly extracting the minimum until the priority queue is empty. We compare\nbelow the performance of this sorting algorithm to those of [Bai and Coester, 2023].\n17\n\nDirty comparison model Denoting by ηi=η(ai, A), Bai and Coester [2023] prove a sorting algorithm\nusing O(nlogn) time, O(nlogn) dirty comparisons, and O(P\nilog(ηi+ 2)) clean comparisons. Theorem 3.3\nyields the same guarantees with our learning-augmented priority queue. Moreover, our learning-augmented\npriority queue is a skip list, maintaining elements in sorted order at any time, even if the elements are\nrevealed online and the insertion order is adversarial, while in [Bai and Coester, 2023], it is crucial that the\ninsertion order is chosen uniformly at random.\nPositional predictions In their second prediction model, they assume that the algorithm is given offline\naccess to predictions {bR(ai)}i∈[n]of the relative ranks {R(ai)}i∈[n]of the nelements to sort, and they study\ntwo different error measures. The rank prediction error η∆\ni=|R(ai)−bR(ai)|matches their definition of\ndisplacement error , for which they prove a sorting algorithm in O(P\nilog(η∆\ni+ 2)) time. The same bound\ncan be deduced using our results in the pointer prediction model. Indeed, by Lemma 4.2, a permutation π\nof [n] satisfying bR(aπ(1))≤. . .≤bR(aπ(n)) can be constructed in O(n) time, and Inequality (14) shows that\ninserting the elements of Ainto the priority queue in the order given by π, then taking each inserted element\naπ(i)as pointer prediction for the following one aπ(i+1), yields a pointer prediction error of\n⃗ η(aπ(i),Qi−1)≤η∆\nπ(i−1)+η∆\nπ(i)+bRπ(i)−bRπ(i−1)\nfor all i∈ {2, . . . , n }. By Theorem 3.2, the runtime for inserting aπ(i)using this pointer prediction is\nO(log⃗ η(aπ(i),Qi−1)). The expected total time for inserting all the elements into the priority queue is therefore\nat most proportional to\nnX\ni=2log(⃗ η(aπ(i),Qi−1) + 2) ≤nX\ni=2log(η∆\nπ(i−1)+η∆\nπ(i)+bRπ(i)−bRπ(i−1)+ 2)\n≤nX\ni=2\u0010\nlog(η∆\nπ(i−1)+ 2) + log( η∆\nπ(i)+ 2) + log( bRπ(i)−bRπ(i−1)+ 2))\u0011\n≤2nX\ni=1log(η∆\nπ(i)+ 2) +nX\ni=2(bRπ(i)−bRπ(i−1)) +n\n≤2nX\ni=1log(η∆\nπ(i)+ 2) + bRπ(2)+n\n=O\nX\ni∈[n]log(η∆\ni+ 2)\n.\nThe second inequality follows from log( α+β+γ)≤logα+log β+log γfor all α, β, γ ≥2, and the subsequent\ninequality from log( α+ 1)≤αfor all α≥0. Finally, to complete the sorting algorithm, the minimum is\nrepeatedly extracted from the priority queue until is it empty, and each ExtractMin only takes expected\nO(1) time in the skip list.\nOnline rank predictions Ifnis unknown to the algorithm, and the elements a1, . . . , a nalong with their\npredicted ranks are revealed online, possibly in an adversarial order, then by Theorem 3.4, the total runtime of\nour priority queue for maintaining all the inserted elements sorted at any time is O(nlog log n+nlog max iη∆\ni),\nand the number of comparisons used is O(nlog max iη∆\ni). No analogous result is demonstrated in [Bai and\nCoester, 2023].\n5.2 Dijkstra’s shortest path algorithm\nConsider a run of Dijkstra’s algorithm on a directed positively weighted graph Gwith nnodes and medges.\nThe elements inserted into the priority queue are the nodes of the graph, and the corresponding keys are\ntheir tentative distances to the source, which are updated over time. During the algorithm’s execution,\nat most m+ 1 distinct keys {di}i∈[m+1]are inserted into the priority queue. Given online predictions\n18\n\n(bR(di))i∈[m+1]of their relative ranks ( R(di))i∈[m+1], the total runtime using our priority queue augmented\nwith rank predictions is\nO\u0000\nmlog log n+mlog max\ni∈[m+1]|R(di)−bR(di)|\u0001\n.\nIn contrast, the shortest path algorithm of Lattanzi et al. [2023] (which also works for negative edges) has a\nlinear dependence on a similar error measure. Even with arbitrary error, our guarantee is never worse than\ntheO(mlogn) runtime with binary heaps. Using Fibonacci heaps results in an O(nlogn+m) runtime, which\nis surpassed by our learning-augmented priority queue in the case of sparse graphs where m=o(nlogn\nlog log n) if\npredictions are of high quality. However, it is known that Fibonacci heaps perform poorly in practice, even\ncompared to binary heaps, as supported by our experiments.\n6 Experiments\nIn this section, we empirically evaluate the performance of our learning-augmented priority queue (LAPQ) by\ncomparing it with Binary and Fibonacci heaps. We use two standard benchmarks for this evaluation: sorting\nand Dijkstra’s algorithm. We also compare our results with those of Bai and Coester [2023] for sorting. For\nDijkstra’s algorithm, we assess performance on both real city maps and synthetic random graphs. In all the\nexperiments, each data point represents the average result from 30 independent runs. The code used for\nconducting the experiments is available at github.com/Ziyad-Benomar/Learning-augmented-priority-queues.\n6.1 Sorting\nWe compare sorting using our LAPQ with the algorithms of Bai and Coester [2023] under their same\nexperimental settings. Given a sequence A= (a1, . . . , a n), we evaluate the complexity of sorting it with\npredictions in the class and the decay setting. In the first, Ais divided into cclasses (( tk−1, tk])k∈[c], where\n0 =t0≤t1≤. . .≤tc=nare uniformly random thresholds. The predicted rank of any item aiwith\ntk≤i < t k+1is sampled uniformly at random within ( tk, tk+1]. In the decay setting, the ranking is initially\naccurate but degrades over time. Each time step, one item’s predicted position is perturbed by 1, either left\nor right, uniformly at random.\nIn both settings, we test the LAPQ with the three prediction models. First, assuming that the rank\npredictions are given offline, we use pointer predictions as explained in Section 5.1. In the second case,\nthe elements to insert along with their predicted ranks are revealed online in a uniformly random order.\nFinally, in both the class and the decay settings, we test the dirty comparison setting with the dirty order\n(aib< aj) = (bR(ai)<bR(aj)) induced by the predicted ranks.\n0.0 0.2 0.4 0.6 0.8 1.0\n(#classes)/ n100101(#comparisons)/nn= 1000\n0.0 0.2 0.4 0.6 0.8 1.0\n(#classes)/ n100101n= 10000\n0.0 0.2 0.4 0.6 0.8 1.0\n(#classes)/ n100101n= 100000\nLAPQ ofﬂine predictions\nLAPQ online predictions\nLAPQ dirty comparisons\nDouble-Hoover sort\nDisplacement sort\nBinary heap\nFibonacci heap\nFigure 2: Sorting with rank predictions in the class setting, for n∈ {1000,10000 ,100000 }.\nFigures 2 and 3 show the obtained results respectively in the class and the decay setting for n∈\n{103,104,105}. In the class setting with offline predictions, the LAPQ slightly outperforms the Double-\nHoover and Displacement sort algorithms of Bai and Coester [2023], which were shown to outperform\nclassical sorting algorithms. In the decay setting, the LAPQ matches the performance of the Displacement\nsort, but is slightly outperformed by the Double-Hoover sort. With online predictions, although the problem\nis harder, LAPQ’s performance remains comparable to the previous algorithms. In both settings, the LAPQ\n19\n\n0.0 0.2 0.4 0.6 0.8 1.0\n(#timesteps)/ (n√n)246810(#comparisons)/nn= 1000\n0.0 0.2 0.4 0.6 0.8 1.0\n(#timesteps)/ (n√n)2.55.07.510.012.5n= 10000\n0.0 0.2 0.4 0.6 0.8 1.0\n(#timesteps)/ (n√n)51015n= 100000\nLAPQ ofﬂine predictions\nLAPQ online predictions\nLAPQ dirty comparisons\nDouble-Hoover sort\nDisplacement sort\nBinary heapFigure 3: Sorting with rank predictions in the decay setting, for n∈ {1000,10000 ,100000 }.\nwith offline predictions, online predictions, and dirty comparisons all yield better performance than binary\nor Fibonacci heaps, even with predictions that are not highly accurate.\n6.2 Dijkstra’s algorithm\nConsider a graph G= (V, E) with nnodes and medges, and a source node s∈V. In the first predictions\nsetting, we pick a random node ˆ sand run Dijkstra’s algorithm with ˆ sas the source, memorizing all the\nkeys ˆD= (ˆd1, . . . , ˆdm) inserted into the priority queue. In subsequent runs of the algorithm with different\nsources, when a key diis to be inserted, we augment the insertion with the rank prediction bR(di) =r(di,ˆD).\nWe call these key rank predictions . This model aims at exploiting the topology and uniformity of city maps.\nAs computing shortest paths from any source necessitates traversing all graph edges, keys inserted into the\npriority queue—partial sums of edge lengths—are likely to exhibit some degree of similarity even if the\nalgorithm is executed from different sources. Notably, this prediction model offers an explicit method for\ncomputing predictions, readily applicable in real-world scenarios.\nIn the second setting, we consider rank predictions of the nodes in G, ordered by their distances to s. As\nDijkstra’s algorithm explores a new node x∈V, it receives a prediction br(x) of its rank. The node xis then\ninserted with a key di, to which we assign the prediction bR(di) =br(x). Unlike the previous experimental\nsettings, we initially have predictions of the nodes’ ranks, which we extend to predictions of the keys’ ranks.\nSimilarly to the sorting experiments, we consider class anddecay perturbations of the node ranks.\nIn the context of searching the shortest path, rank predictions in the class setting can be derived from\nsubdividing the city into multiple smaller areas. Each class corresponds to a specific area, facilitating the\nordering of areas from closest to furthest relative to the source. However, comparing the distances from the\nsource to the nodes in the same class might be inaccurate. On the other hand, the decay setting simulates\nmodifications to shortest paths, such as rural works or new route constructions, by adding or removing edges\nfrom the graph. These alterations may affect the ranks of a limited number of nodes.\nWe present below experiment results obtained with different city maps having different numbers of nodes\nand edges. All the maps were obtained using the Python library Osmnx Boeing [2017]. Figures 4 and 5\nrespectively illustrate the results in the class and the decay settings with node rank predictions . In both\nfigures, for each city, we present the numbers of comparisons used for the same task by a binary and Fibonacci\nheap, and the number of comparisons used when the priority queue is augmented with key rank predictions .\nIn both settings, the performance of the LAPQ substantially improves with the quality of the predictions,\nand notably, key rank predictions yield almost the same performance as perfect node rank predictions ,\naffirming our intuition on the similarity between the keys inserted in runs of Dijkstra’s algorithm starting\nfrom different sources.\nSimulations on Poisson Voronoi Tesselations We also evaluate the performance of Dijkstra’s algo-\nrithm with our LAPQ on synthetic random graphs.\nWe present the results in the class and the decay settings respectively in Figures 7 and 8. Similar to\nprevious experiments with Dijkstra on city maps, these figures illustrate how the number of comparisons\n20\n\n0.0 0.2 0.4 0.6 0.8 1.0\n(#classes)/ n345678910(#comparisons)/nBrussels: n=1380,m=2580\n0.0 0.2 0.4 0.6 0.8 1.0\n(#classes)/ n46810121416Paris: n=9559,m=18400\n0.0 0.2 0.4 0.6 0.8 1.0\n(#classes)/ n68101214161820New York: n=55326,m=139547\n0.0 0.2 0.4 0.6 0.8 1.0\n(#classes)/ n5.07.510.012.515.017.520.0London: n=128924,m=300612\nLAPQ node rank predictions\nLAPQ node dirty comparisons\nLAPQ key rank predctions\nBinary heap\nFibonacci heapFigure 4: Dijkstra’s algorithm on city maps with class predictions\n0 5 10 15 20\n(#timesteps)/ n345678(#comparisons)/nBrussels: n=1380,m=2580\n0 5 10 15 20\n(#timesteps)/ n34567891011Paris: n=9559,m=18400\n0 5 10 15 20\n(#timesteps)/ n68101214New York: n=55326,m=139547\n0 5 10 15 20\n(#timesteps)/ n468101214London: n=128924,m=300612\nLAPQ node rank predictions\nLAPQ node dirty comparisons\nLAPQ key rank predctions\nBinary heap\nFibonacci heap\nFigure 5: Dijkstra’s algorithm on city maps with decay predictions\nFor this, we use Poisson Voronoi Tessellations (PVT), which are a\ncommonly used random graph model for street systems Gloaguen et al.\n[2006]; Gloaguen and Cali [2018]; Benomar et al. [2022]; Benomar et al..\nPVTs are random graphs created by sampling an integer Nfrom\na Poisson distribution with parameter n≥1. Subsequently, N\npoints, termed ”seeds,” are uniformly chosen at random within a two-\ndimensional region I, typically [0 ,1]2. A Voronoi diagram is then gen-\nerated based on these seeds. This process results in a planar graph\nwhere edges represent the boundaries between the cells of the Voronoi\ndiagram, and the nodes are their intersections. For I= [0,1]2, the\nexpected number of nodes in this construction is n. Figure 6 provides\na visualization of a PVT with n= 100.\n Figure 6: PVT with n= 100\ndecreases when the LAPQ is augmented with node rank predictions or with the corresponding dirty com-\nparator. We compare them with the number of comparisons induced by using a binary or Fibonacci heap,\nas well as with the number of comparisons of the LAPQ augmented with key rank predictions .\nThe same observations regarding the performance improvement can be made, as in the previous ex-\nperiments with city maps. However, in PVT tessellations, the performance of the LAPQ with key rank\npredictions surpasses even that of perfect node rank predictions. This is due to the PVTs having a more\nuniform structure across space.\nReferences\nKeerti Anand, Rong Ge, and Debmalya Panigrahi. Customizing ml predictions for online algorithms. In\nInternational Conference on Machine Learning , pages 303–313. PMLR, 2020.\nAntonios Antoniadis, Themis Gouleakis, Pieter Kleer, and Pavel Kolev. Secretary and online matching\n21\n\n0.0 0.2 0.4 0.6 0.8 1.0\n(#classes)/ n681012(#comparisons)/nPVT: n=1000\n0.0 0.2 0.4 0.6 0.8 1.0\n(#classes)/ n6810121416PVT: n=5000\n0.0 0.2 0.4 0.6 0.8 1.0\n(#classes)/ n68101214161820PVT: n=10000\nLAPQ online predictions\nLAPQ dirty comparisons\nLAPQ key rank predctions\nBinary heap\nFibonacci heapFigure 7: Dijkstra’s algorithm on Poisson Voronoi Tesselation with class predictions\n0 5 10 15 20\n(#timesteps)/ n567891011(#comparisons)/nPVT: n=1000\n0 5 10 15 20\n(#timesteps)/ n678910111213PVT: n=5000\n0 5 10 15 20\n(#timesteps)/ n67891011121314PVT: n=10000\nLAPQ online predictions\nLAPQ dirty comparisons\nLAPQ key rank predctions\nBinary heap\nFibonacci heap\nFigure 8: Dijkstra’s algorithm on Poisson Voronoi Tesselation with decay predictions\nproblems with machine learned advice. Advances in Neural Information Processing Systems , 33:7933–\n7944, 2020.\nAntonios Antoniadis, Christian Coester, Marek Eli´ as, Adam Polak, and Bertrand Simon. Learning-\naugmented dynamic power management with multiple states via new ski rental bounds. Advances in\nNeural Information Processing Systems , 34:16714–16726, 2021.\nAntonios Antoniadis, Joan Boyar, Marek Eli´ as, Lene Monrad Favrholdt, Ruben Hoeksma, Kim S Larsen,\nAdam Polak, and Bertrand Simon. Paging with succinct predictions. In International Conference on\nMachine Learning , pages 952–968. PMLR, 2023a.\nAntonios Antoniadis, Christian Coester, Marek Eli´ aˇ s, Adam Polak, and Bertrand Simon. Online metric\nalgorithms with untrusted predictions. ACM Transactions on Algorithms , 19(2):1–34, 2023b.\nXingjian Bai and Christian Coester. Sorting with predictions. Advances in Neural Information Processing\nSystems , 36, 2023.\nEtienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for learning augmented\nalgorithms. Advances in Neural Information Processing Systems , 33:20083–20094, 2020.\nNikhil Bansal, Christian Coester, Ravi Kumar, Manish Purohit, and Erik Vee. Learning-augmented weighted\npaging. In Proceedings of the 2022 ACM-SIAM Symposium on Discrete Algorithms, SODA , pages 67–89.\nSIAM, 2022.\n22\n\nDmitry Basin, Edward Bortnikov, Anastasia Braginsky, Guy Golan-Gueta, Eshcar Hillel, Idit Keidar, and\nMoshe Sulamy. Kiwi: A key-value map for scalable real-time analytics. In Proceedings of the 22Nd ACM\nSIGPLAN Symposium on Principles and Practice of Parallel Programming , pages 357–369, 2017.\nMohammadHossein Bateni, Prathamesh Dharangutte, Rajesh Jayaram, and Chen Wang. Metric clustering\nand mst with strong and weak distance oracles. arXiv preprint arXiv:2310.15863 , 2023.\nZiyad Benomar and Vianney Perchet. Advice querying under budget constraint for online algorithms.\nAdvances in Neural Information Processing Systems , 36, 2024a.\nZiyad Benomar and Vianney Perchet. Non-clairvoyant scheduling with partial predictions. arXiv preprint\narXiv:2405.01013 , 2024b.\nZiyad Benomar, Chaima Ghribi, Elie Cali, Alexander Hinsen, Benedikt Jahnel, and Jean-Philippe Wary.\nMulti-agent simulations for virus propagation in D2D 5G+ networks .\nZiyad Benomar, Chaima Ghribi, Elie Cali, Alexander Hinsen, and Benedikt Jahnel. Agent-based modeling\nand simulation for malware spreading in d2d networks. AAMAS ’22, page 91–99, Richland, SC, 2022.\nInternational Foundation for Autonomous Agents and Multiagent Systems. ISBN 9781450392136.\nZiyad Benomar, Evgenii Chzhen, Nicolas Schreuder, and Vianney Perchet. Addressing bias in online selection\nwith limited budget of comparisons. In The Thirty-eighth Annual Conference on Neural Information\nProcessing Systems , 2024. URL https://openreview.net/forum?id=BdGFgKrlHl .\nAditya Bhaskara, Ashok Cutkosky, Ravi Kumar, and Manish Purohit. Logarithmic regret from sublinear\nhints. Advances in Neural Information Processing Systems , 34:28222–28232, 2021.\nGeoff Boeing. Osmnx: New methods for acquiring, constructing, analyzing, and visualizing complex street\nnetworks. Computers, environment and urban systems , 65:126–139, 2017.\nRyan S Borgstrom and S Rao Kosaraju. Comparison-based search in the presence of errors. In Proceedings\nof the twenty-fifth annual ACM symposium on Theory of computing , pages 130–136, 1993.\nGerth Stølting Brodal. Worst-case efficient priority queues. In Proceedings of the Seventh Annual ACM-\nSIAM Symposium on Discrete Algorithms , SODA ’96, page 52–58, USA, 1996. Society for Industrial and\nApplied Mathematics. ISBN 0898713668.\nGerth Stølting Brodal. A survey on priority queues. In Space-Efficient Data Structures, Streams, and\nAlgorithms: Papers in Honor of J. Ian Munro on the Occasion of His 66th Birthday , pages 150–163.\nSpringer, 2013.\nGerth Stølting Brodal and Chris Okasaki. Optimal purely functional priority queues. Journal of Functional\nProgramming , 6(6):839–857, 1996.\nGerth Stølting Brodal, George Lagogiannis, and Robert E Tarjan. Strict fibonacci heaps. In Proceedings of\nthe forty-fourth annual ACM symposium on Theory of computing , pages 1177–1184, 2012.\nBernard Chazelle. A minimum spanning tree algorithm with inverse-ackermann type complexity. Journal of\nthe ACM (JACM) , 47(6):1028–1047, 2000.\nJustin Chen, Sandeep Silwal, Ali Vakilian, and Fred Zhang. Faster fundamental graph algorithms via learned\npredictions. In International Conference on Machine Learning , pages 3583–3602. PMLR, 2022.\nMo Chen, Rezaul Alam Chowdhury, Vijaya Ramachandran, David Lan Roche, and Lingling Tong. Priority\nqueues and dijkstra’s algorithm. 2007.\nJakub Chlkedowski, Adam Polak, Bartosz Szabucki, and Konrad Tomasz .Zolna. Robust learning-augmented\ncaching: An experimental study. In International Conference on Machine Learning , pages 1920–1930.\nPMLR, 2021.\n23\n\nNicolas Christianson, Junxuan Shen, and Adam Wierman. Optimal robustness-consistency tradeoffs for\nlearning-augmented metrical task systems. In International Conference on Artificial Intelligence and\nStatistics , pages 9377–9399. PMLR, 2023.\nClark Allan Crane. Linear lists and priority queues as balanced binary trees . Stanford University, 1972.\nHerbert Aron David. The method of paired comparisons , volume 12. London, 1963.\nWilliam HE Day and Herbert Edelsbrunner. Efficient algorithms for agglomerative hierarchical clustering\nmethods. Journal of classification , 1(1):7–24, 1984.\nIlias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Ali Vakilian, and Nikos Zarifis. Learning online\nalgorithms with distributional advice. In International Conference on Machine Learning , pages 2687–\n2696. PMLR, 2021.\nMichael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Faster matchings\nvia learned duals. Advances in neural information processing systems , 34:10393–10406, 2021.\nFranziska Eberle, Felix Hommelsheim, Alexander Lindermayr, Zhenwei Liu, Nicole Megow, and Jens Schl¨ oter.\nAccelerating matroid optimization through fast imprecise oracles. arXiv preprint arXiv:2402.02774 , 2024.\nStefan Edelkamp and Ingo Wegener. On the performance of weak-heapsort. In Annual Symposium on\nTheoretical Aspects of Computer Science , pages 254–266. Springer, 2000.\nBennett Eisenberg. On the expectation of the maximum of iid geometric random variables. Statistics &\nProbability Letters , 78(2):135–143, 2008.\nHafedh El Ferchichi, Matthieu Lerasle, and Vianney Perchet. Active ranking and matchmaking, with perfect\nmatchings. In Forty-first International Conference on Machine Learning .\nMichael L Fredman and Robert Endre Tarjan. Fibonacci heaps and their uses in improved network opti-\nmization algorithms. Journal of the ACM (JACM) , 34(3):596–615, 1987.\nChunkai Fu, Jung Hoon Seo, and Samson Zhou. Learning-augmented skip lists. arXiv preprint\narXiv:2402.10457 , 2024.\nAnna Gambin and Adam Malinowski. Randomized meldable priority queues. In International Conference\non Current Trends in Theory and Practice of Computer Science , pages 344–349. Springer, 1998.\nTingjian Ge and Stan Zdonik. A skip-list approach for efficiently processing forecasting queries. Proceedings\nof the VLDB Endowment , 1(1):984–995, 2008.\nDebarghya Ghoshdastidar, Micha¨ el Perrot, and Ulrike von Luxburg. Foundations of comparison-based hier-\narchical clustering. Advances in neural information processing systems , 32, 2019.\nCatherine Gloaguen and Elie Cali. Cost estimation of a fixed network deployment over an urban territory.\nAnnals of Telecommunications , 73:367–380, 2018.\nCatherine Gloaguen, Frank Fleischer, Hendrik Schmidt, and Volker Schmidt. Fitting of stochastic telecom-\nmunication network models via distance measures and monte–carlo tests. Telecommunication Systems , 31:\n353–377, 2006.\nRick Siow Mong Goh and Ian Li-Jin Thng. Dsplay: An efficient dynamic priority queue structure for discrete\nevent simulation. In Proceedings of the SimTecT Simulation Technology and Training Conference . Citeseer,\n2004.\nSreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with expert advice. In\nInternational Conference on Machine Learning , pages 2319–2327. PMLR, 2019.\nGaston H Gonnet and J Ian Munro. Heaps on heaps. SIAM Journal on Computing , 15(4):964–971, 1986.\n24\n\nSiavash Haghiri, Debarghya Ghoshdastidar, and Ulrike von Luxburg. Comparison-based nearest neighbor\nsearch. In Artificial Intelligence and Statistics , pages 851–859. PMLR, 2017.\nSiavash Haghiri, Damien Garreau, and Ulrike Luxburg. Comparison-based random forests. In International\nConference on Machine Learning , pages 1871–1880. PMLR, 2018.\nReinhard Heckel, Max Simchowitz, Kannan Ramchandran, and Martin Wainwright. Approximate ranking\nfrom pairwise comparisons. In International Conference on Artificial Intelligence and Statistics , pages\n1057–1066. PMLR, 2018.\nYih-Chun Hu, Adrian Perrig, and David B Johnson. Efficient security mechanisms for routing protocolsa.\nInNdss. Citeseer, 2003.\nSungjin Im, Ravi Kumar, Aditya Petety, and Manish Purohit. Parsimonious learning-augmented caching.\nInInternational Conference on Machine Learning , pages 9588–9601. PMLR, 2022.\nNarendra Kumar Jaiswal. Priority queues , volume 50. Academic press New York, 1968.\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned index\nstructures. In Proceedings of the 2018 international conference on management of data , pages 489–504,\n2018.\nDaniel H Larkin, Siddhartha Sen, and Robert E Tarjan. A back-to-basics empirical study of priority queues.\nIn2014 Proceedings of the Sixteenth Workshop on Algorithm Engineering and Experiments (ALENEX) ,\npages 61–72. SIAM, 2014.\nAlexandra Anna Lassota, Alexander Lindermayr, Nicole Megow, and Jens Schl¨ oter. Minimalistic predictions\nto schedule jobs with online precedence constraints. In International Conference on Machine Learning ,\npages 18563–18583. PMLR, 2023.\nSilvio Lattanzi, Ola Svensson, and Sergei Vassilvitskii. Speeding up bellman ford via minimum violation\npermutations. In International Conference on Machine Learning, ICML 2023 , 2023.\nRhyd Lewis. A comparison of dijkstra’s algorithm using fibonacci heaps, binary heaps, and self-balancing\nbinary trees. arXiv preprint arXiv:2303.10034 , 2023.\nHonghao Lin, Tian Luo, and David Woodruff. Learning augmented binary search trees. In International\nConference on Machine Learning , pages 13431–13440. PMLR, 2022.\nJonatan Lind´ en and Bengt Jonsson. A skiplist-based concurrent priority queue with minimal memory\ncontention. In Principles of Distributed Systems: 17th International Conference, OPODIS 2013, Nice,\nFrance, December 16-18, 2013. Proceedings 17 , pages 206–220. Springer, 2013.\nAlexander Lindermayr and Nicole Megow. Algorithms with predictions. https://\nalgorithms-with-predictions.github.io , 2022.\nThodoris Lykouris and Sergei Vassilvtiskii. Competitive caching with machine learned advice. In Interna-\ntional Conference on Machine Learning , pages 3296–3305. PMLR, 2018.\nJessica Maghakian, Russell Lee, Mohammad Hajiesmaili, Jian Li, Ramesh Sitaraman, and Zhenhua Liu. Ap-\nplied online algorithms with heterogeneous predictors. In International Conference on Machine Learning ,\npages 23484–23497. PMLR, 2023.\nSamuel McCauley, Ben Moseley, Aidin Niaparast, and Shikha Singh. Online list labeling with predictions.\nAdvances in Neural Information Processing Systems , 36, 2024.\nMichela Meister and Sloan Nietert. Learning with comparison feedback: Online estimation of sample statis-\ntics. In Algorithmic Learning Theory , pages 983–1001. PMLR, 2021.\n25\n\nNadav Merlis, Hugo Richard, Flore Sentenac, Corentin Odic, Mathieu Molina, and Vianney Perchet. On\npreemption and learning in stochastic scheduling. In International Conference on Machine Learning , pages\n24478–24516. PMLR, 2023.\nMichael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. Communications of the ACM ,\n65(7):33–35, 2022.\nSung-Whan Moon, Jennifer Rexford, and Kang G Shin. Scalable hardware priority queue architectures for\nhigh-speed packet switches. IEEE Transactions on computers , 49(11):1215–1227, 2000.\nRobert Nowak. Noisy generalized binary search. Advances in neural information processing systems , 22,\n2009.\nClark F Olson. Parallel algorithms for hierarchical clustering. Parallel computing , 21(8):1313–1325, 1995.\nMihai P˘ atra¸ scu and Mikkel Thorup. Time-space trade-offs for predecessor search. In Proceedings of the\nthirty-eighth annual ACM symposium on Theory of computing , pages 232–240, 2006.\nMicha¨ el Perrot, Pascal Esser, and Debarghya Ghoshdastidar. Near-optimal comparison based clustering.\nAdvances in Neural Information Processing Systems , 33:19388–19399, 2020.\nWilliam Pugh. Skip lists: a probabilistic alternative to balanced trees. Communications of the ACM , 33(6):\n668–676, 1990.\nManish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml predictions. Advances\nin Neural Information Processing Systems , 31, 2018.\nRobert R¨ onngren and Rassul Ayani. A comparative study of parallel and sequential priority queue algorithms.\nACM Transactions on Modeling and Computer Simulation (TOMACS) , 7(2):157–209, 1997.\nKarim Ahmed Abdel Sadek and Marek Elias. Algorithms for caching and MTS with reduced number of\npredictions. In The Twelfth International Conference on Learning Representations , 2024. URL https:\n//openreview.net/forum?id=QuIiLSktO4 .\nNihar B Shah and Martin J Wainwright. Simple, robust and optimal ranking from pairwise comparisons.\nJournal of machine learning research , 18(199):1–38, 2018.\nAvinash Sharma, Dankan Gowda, Anil Sharma, S Kumaraswamy, MR Arun, et al. Priority queueing model-\nbased iot middleware for load balancing. In 2022 6th International Conference on Intelligent Computing\nand Control Systems (ICICCS) , pages 425–430. IEEE, 2022.\nNir Shavit and Itay Lotan. Skiplist-based concurrent priority queues. In Proceedings 14th International\nParallel and Distributed Processing Symposium. IPDPS 2000 , pages 263–268. IEEE, 2000.\nYongho Shin, Changyeol Lee, Gukryeol Lee, and Hyung-Chan An. Improved learning-augmented algo-\nrithms for the multi-option ski rental problem via best-possible competitive analysis. arXiv preprint\narXiv:2302.06832 , 2023.\nSandeep Silwal, Sara Ahmadian, Andrew Nystrom, Andrew McCallum, Deepak Ramachandran, and\nSeyed Mehran Kazemi. Kwikbucks: Correlation clustering with cheap-weak and expensive-strong sig-\nnals. In The Eleventh International Conference on Learning Representations , 2023. URL https:\n//openreview.net/forum?id=p0JSSa1AuV .\nH˚ akan Sundell and Philippas Tsigas. Fast and lock-free concurrent priority queues for multi-thread systems.\nJournal of Parallel and Distributed Computing , 65(5):609–627, 2005.\nMikkel Thorup. Equivalence between priority queues and sorting. Journal of the ACM (JACM) , 54(6):28–es,\n2007.\nDominique Tschopp, Suhas Diggavi, Payam Delgosha, and Soheil Mohajer. Randomized algorithms for\ncomparison-based search. Advances in Neural Information Processing Systems , 24, 2011.\n26\n\nPeter van Emde Boas, Robert Kaas, and Erik Zijlstra. Design and implementation of an efficient priority\nqueue. Mathematical systems theory , 10(1):99–127, 1976.\nJean Vuillemin. A data structure for manipulating priority queues. Communications of the ACM , 21(4):\n309–315, 1978.\nFabian Wauthier, Michael Jordan, and Nebojsa Jojic. Efficient ranking from pairwise comparisons. In\nInternational Conference on Machine Learning , pages 109–117. PMLR, 2013.\nJ. W. J. Williams. Algorithm 232: Heapsort. Communications of the ACM , 7(6):347–348, 1964.\nAli Zeynali, Shahin Kamali, and Mohammad Hajiesmaili. Robust learning-augmented dictionaries. arXiv\npreprint arXiv:2402.09687 , 2024.\nDeli Zhang and Damian Dechev. A lock-free priority queue design based on multi-dimensional linked lists.\nIEEE Transactions on Parallel and Distributed Systems , 27(3):613–626, 2015.\n27",
  "textLength": 88138
}