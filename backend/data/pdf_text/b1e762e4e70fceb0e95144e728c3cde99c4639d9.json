{
  "paperId": "b1e762e4e70fceb0e95144e728c3cde99c4639d9",
  "title": "Uncertainty-aware Cardinality Estimation by Neural Network Gaussian Process",
  "pdfPath": "b1e762e4e70fceb0e95144e728c3cde99c4639d9.pdf",
  "text": "Uncertainty-aware Cardinality Estimation\nby Neural Network Gaussian Process\nKangfei Zhao\nThe Chinese University of Hong Kong\n{kfzhao}@se.cuhk.edu.hkJeffrey Xu Yu\nThe Chinese University of Hong Kong\nyu@se.cuhk.edu.hk\nZongyan He\nRenmin University of China\nhzy9819@ruc.edu.cnHao Zhang\nThe Chinese University of Hong Kong\nhzhang@se.cuhk.edu.hk\nAbstract\nDeep Learning (DL) has achieved great success in many real appli-\ncations. Despite its success, there are some main problems when\ndeploying advanced DL models in database systems, such as hyper-\nparameters tuning, the risk of overfitting, and lack of prediction\nuncertainty. In this paper, we study cardinality estimation for SQL\nqueries with a focus on uncertainty, which we believe is important\nin database systems when dealing with a large number of user\nqueries on various applications. With uncertainty ensured, instead\nof trusting an estimator learned as it is, a query optimizer can ex-\nplore other options when the estimator learned has a large variance,\nand it also becomes possible to update the estimator to improve\nits prediction in areas with high uncertainty. The approach we\nexplore is different from the direction of deploying sophisticated\nDL models in database systems to build cardinality estimators. We\nemploy Bayesian deep learning (BDL), which serves as a bridge\nbetween Bayesian inference and deep learning. The prediction dis-\ntribution by BDL provides principled uncertainty calibration for the\nprediction. In addition, when the network width of a BDL model\ngoes to infinity, the model performs equivalent to Gaussian Process\n(GP). This special class of BDL, known as Neural Network Gauss-\nian Process (NNGP), inherits the advantages of Bayesian approach\nwhile keeping universal approximation of neural network, and can\nutilize a much larger model space to model distribution-free data\nas a nonparametric model. We show that our uncertainty-aware\nNNGP estimator achieves high accuracy, can be built very fast, and\nis robust to query workload shift, in our extensive performance\nstudies by comparing with the existing approaches.\n1 Introduction\nThe learning approaches are shifting from the traditional ML (Ma-\nchine Learning) models (e.g., KDE, GBDT) to DL (Deep Learning)\nmodels (e.g., NN, MSCN, VAE, MADE, Transformer, SPN, LSTM).\nThe shifting is motivated by the powerful approximation capability\nof neural networks in end-to-end applications and the high effi-\nciency of DL frameworks. Compared with traditional ML, DL has\nachieved a great improvement on estimation accuracy, and more\nand more advanced DL architectures are devised to improve the\nperformance of the learning tasks with deeper layers to pursue\nmore powerful modeling capability. In database systems, the DL\nmodels have been extensively studied for query optimization [ 45],\nindex recommendation [ 9], view materialization [ 41], and cardi-\nnality estimation [ 13,14,24,31,32,56,66,67]. Despite the successof DL approaches, the complex DL approaches incur some main\nproblems in general.\nThe first is hyper-parameters, on which the performance of DL\nmodels highly relies, including the training hyper-parameters and\nnetwork architecture configurations. Note that onerous effort of\nparameter tuning must be paid to pursuing satisfying performance,\nwhich is labor-intensive. Although autoML tools [ 30] have been\ndeveloped to avoid human-in-the-loop, deploying such tools in\ndatabase systems needs great effort as searching a suitable hyper-\nparameter combination needs a huge number of trials with high\ncomputation cost. It is worth mentioning that a well-tuned model\nfor one database is difficult to transfer to other databases, which\nmeans that the retraining/tuning needs repeating when the under-\nlying data changes significantly.\nThe second is a high risk of overfitting that DL models are ex-\nposed to. Subject to a particular family of function designed, general\nDL models are indexed by a large number of parameters, fully fitted\nby the training data. It is assumed testing data is from the same un-\nderlying distribution of the training data, otherwise the prediction\nmay have a large variance. Many regularization techniques can be\nused, but they can only alleviate the problem to some degree. Col-\nlecting more data to enhance training helps to reduce the variance.\nBut this means a higher cost for acquiring the data/ground truth\nand training.\nThe third is prediction belief, which is an important issue we\nfocus on in this work. The issue is that DL models cannot capture\nand convey their prediction belief, namely, how probably their\nprediction is accurate or how much is the prediction uncertainty.\nThe uncertainty comes from different sources, for example, the\nnoise of the training data, the dissimilarity of the test data from\nthe training data, the mismatching of the model class to the data\nto be described. For classification tasks, DL models predict the\nprobability distribution that one data point is associated with the\ncandidate classes by a softmax function, which is shown to be over-\nconfident on the most likely class [ 22]. For regression tasks (e.g.,\ncardinality estimation), DL models can only output a scalar value\nwithout any uncertainty measurements of the prediction, such as\nvariance and confidential interval. It is highly desirable to avoid\nsituations where we have no choices but trust the DL predictions\nbeing made in database systems. In other words, as a database\nsystem to support a large number of users in various applications,\nwhere user queries may be different and databases will be updated\nfrom time to time, what database systems require is not only anarXiv:2107.08706v1  [cs.DB]  19 Jul 2021\n\nTable 1: ML/DL Approaches for AQP and Cardinality Estimation\nApproachSupported QueriesLearning Strategy Model Model UpdateJoin Selection Aggregate Group By\nDBEst [42] precomp. join num., cate. cnt,sum,avg,etc. âœ“ supervised & unsupervised KDE & GBDT âœ—\nDeepDB [25] precomp. join num., cate. cnt,sum,avg âœ“ unsupervised SPN âœ“\nThirumuruganathan et. al. [58] âœ— num., cate. cnt,sum,avg âœ“ unsupervised VAE âœ—\nKiefer et al. [31] Equi-join num., cate. cnt âœ— unsupervised KDE âœ“\nKipf et al. [32] PK/FK join num., cate. cnt âœ— supervised MSCN âœ—\nDutt et al. [14] âœ— num., cate. cnt âœ— supervised NN, GBDT âœ—\nDutt et al. [13] PK/FK join num., cate. cnt âœ— supervised GBDT âœ“\nSun et al. [56] PK/FK join num., cate., str. cnt âœ— supervised Tree LSTM âœ“\nHasan et al. [24] âœ— num., cate. cnt âœ— unsupervised MADE âœ—\nNaru [67] precomp. join num., cate. cnt âœ— unsupervised MADE, Transformer âœ“\nNeuroCard [66] Full outer join num., cate., cnt âœ— unsupervised MADE, Transformer âœ“\naccurate model prediction, but also an indication of how much the\npredictions can be trusted regarding the learned model.\nIn this paper, we study cardinality estimation for SQL queries\nwith selection, projection and join. We focus on uncertainty. The\napproach we take can also address the hyper-parameter and risk\nof overfitting. Table 1 summarizes the recent ML/DL approaches\nstudied for AQP (Approximate Query Processing) (the top three)\nand cardinality estimation (the bottom eight). These approaches are\ncategorized into supervised learning approaches and unsupervised\nlearning approaches. The supervised learning approaches are query-\ndriven, which learn a function that maps the query features to its\ncardinality. The unsupervised learning approaches are data-driven,\nwhich learn the joint probability distribution of the underlying rela-\ntional data. The supervised learning approaches are easy to deploy\nwith relatively low training and prediction overhead, however, they\nlack robustness to shifting query workloads. On the contrary, the\nunsupervised learning approaches are robust to different query\nworkloads. But, building a model consumes many resources as\nlarge volumes of data need to be fed into the model multiple times.\nFor example, for a single relation within 100MB, the unsupervised\nlearning estimators DeepDB [25] and NeuroCard [66] take hundreds\nand thousands of seconds to train a model on a 56-core CPU, which\nis 2-3 orders slower than the lightweight supervised learning esti-\nmators [ 14]. Furthermore, the unsupervised estimators suffer from\nunderestimation for range queries as the prediction is conducted\nby integration over the learned distribution regarding the query\ncondition. In this work, we study cardinality estimation based on\na supervised approach with an uncertainty guarantee. Consider\ncardinality estimation for an SQL query. If a DL-based estimator\nprovides an estimate without uncertainty, the optimizer used can\nonly take it as it is. If a DL-based estimator provides an additional\nprobability distribution with mean, say ğœ‡, and standard deviation,\nsay2ğœ‡, as its predictive cardinality. Then, the query optimizer can\ntake it as inaccurate since the coefficient of variation is up to 2, and\nthe optimizer can either explore other options or use the built-in\nestimator for query planning. It is important that the system is\naware that the estimator may be outdated and need retraining, if\nthe learned estimator delivers a low confidence level for a large\nfraction of arrival queries. Till now, all the DL-based cardinality\napproaches cannot provide the prediction uncertainty.\nIn this work, different from the direction of deploying sophisti-\ncated DL models in database systems, we employ Bayesian deep\nlearning (BDL) to build cardinality estimators, where BDL serves\nas a bridge between Bayesian inference and deep learning [ 61]. In\nML\nâ€¢KDE\nâ€¢GBDT\nâ€¢SPNDL\nâ€¢DNN\nâ€¢LSTM\nâ€¢MSCN\nâ€¢Transformer\nâ€¢MADE\nâ€¢VAEBDL GP\nNNGPFigure 1: From ML/DL estimators To NNGP (GP & BDL)\na nutshell, BDL imposes a prior distribution on the neural network\nweights, and the derived models are ensembles of neural networks\nin a particular function space, which output a distribution as the\nprediction. The prediction distribution provides principled uncer-\ntainty calibration for the prediction. This uncertainty helps the\nmodel to improve itself explicitly, by collecting the information of\ntesting data with higher uncertainty and retraining/fine-tuning the\nmodel. In addition, the prediction distribution by BDL is robust to\nthe overfitting problem. The parameters and hyper-parameters of\nBDL models are much fewer (i.e., the statistics of the prior distribu-\ntion and the prior type, respectively), in versus to the parameters\nand hyper-parameters that need in DL models (i.e., the weights of\nthe neural networks and the network architecture configurations,\nnumber of layers, number of hidden units in each layer), and in\nDL training configurations (i.e., optimization algorithm, learning\nrate, batch size, epochs, etc.). When the network width of the BDL\nmodel goes to infinity , the model performs equivalent to Gauss-\nian Process (GP) [ 48], named Neural Network Gaussian Process\n(NNGP). Exact Bayesian inference can be used to train this special\nGP as a lightweight cardinality estimator, while offering a more\npowerful generalization capability than a finite wide neural net-\nwork. NNGP keeps the flexible modeling capability of deep learning,\nwhile offering robustness and interpretability by Bayesian infer-\nence. Fig. 1 delineates the extraordinary standpoint of our NNGP\nestimator among the existing ML/DL-based estimators in Table 1,\nfor cardinality estimation. Here, KDE and GBDT are classical ML\nmodels while SPN is a new type of probabilistic graphical model\nwith deeper layers. DL models are associated with different neural\nnetwork architectures. A BDL model is a special kind of DL model\nin which the neural network parameters are probability distribu-\ntions. We explore a new model, named NNGP, which is the BDL\nmodel with infinite wide hidden layers, and can be built using GP.\n\nContribution. The main contributions of this paper are summa-\nrized as follows. âŠWe employ an advanced BDL model, named\nNeural Network Gaussian Process (NNGP), to build cardinality es-\ntimators for relational database systems. Our NNGP estimator can\nsupport range queries on single relations and equi/theta-join on\nmultiple relations for a database. To the best of our knowledge, this\nis the first exploration of BDL approaches for database applications.\nâ‹As the first DL-based cardinality estimator which supports prin-\ncipled uncertainty estimation, we investigate the uncertainty that\nNNGP captures, compared with existing uncertainty quantification\napproaches for DL. âŒWe conduct extensive experimental studies\nto verify the effectiveness and efficiency of NNGP estimator via\ncomparison with recent ML/DL based estimators. The NNGP es-\ntimator distinguishes from existing estimators by its swift model\nconstruction and prediction, robustness to workload shifts, and\nappealing accuracy.\nRoadmap. Â§2 gives the problem statement. In Â§3, we give an\noverview of the NNGP estimator, and introduce Bayesian learning,\nGP and NNGP in Â§4. Then, we elaborate on the uncertainty calibra-\ntion of NNGP in Â§5. Â§6 reports the experimental results. Finally, we\nreview the related works in Â§7 and conclude the paper in Â§8.\n2 Problem Statement\nA relational database consists of a set of relations, {ğ‘…1,ğ‘…2,Â·Â·Â·,ğ‘…ğ‘},\nwhere a relation ğ‘…ğ‘–hasğ‘‘ğ‘–attributes such as ğ‘…ğ‘–=(ğ´1,Â·Â·Â·ğ´ğ‘‘ğ‘–). Here,\nan attribute ğ´ğ‘—is either a numerical attribute in a domain with\ngiven range[ğ‘šğ‘–ğ‘›ğ‘—,ğ‘šğ‘ğ‘¥ğ‘—]or a categorical attribute with a discrete\nfinite domain{ğ‘1,ğ‘2,Â·Â·Â·ğ‘ğ‘šğ‘—}.\nWe study cardinality estimation for the select-project-join SQL\nqueries with conjunctive conditions. A selection on an attribute is\neither a range filter (i.e., [ğ‘™ğ‘ğ‘—,ğ‘¢ğ‘ğ‘—], denoting the condition ğ‘™ğ‘ğ‘—â‰¤\nğ´ğ‘—â‰¤ğ‘¢ğ‘ğ‘—), if the attribute is numerical, or INfilter (i.e.,ğ´ğ‘—INğ¶,ğ¶âŠ‚\n{ğ‘1,ğ‘2,Â·Â·Â·ğ‘ğ‘šğ‘—}, denoting the condition âˆƒğ‘ğ‘˜âˆˆğ¶,ğ´ğ‘—=ğ‘ğ‘˜), if the\nattribute is categorical. A projection can be on any attributes. A join\ncan be equi/theta-join. For joining over numerical attributes, the\njoin condition can be {<,â‰¤,=,>,â‰¥,â‰ }. For joining over categorical\nattributes, the join condition is either =orâ‰ , as the categorical\ndomain is order-free. The primary-foreign key join, or PK/FK-join,\nis treated as a special equi-join with the extra constraints. We also\nsupport self-joins, which are conducted by renaming the same\nrelations. An example is given below.\nğœ(100â‰¤ğ‘…1.ğ´1â‰¤200)âˆ§(ğ‘…2.ğ´2IN{15,20,25})(ğ‘…1Z\nğ‘…1.ğ´3â‰¤ğ‘…2.ğ´3ğ‘…2)\nwhereğœis the select operator and Zis a join operator. It is worth\nmentioning that we support general select-project-join SQL queries.\nAll the existing learned estimators do not support joins other than\nPK/FK joins. For example, [ 66] does not support cyclic join queries\nand selection conditions on join attributes and [ 25] does not support\nmultiple selection conditions on join attributes.\nThe cardinality of an SQL query ğ‘is the number of resulting tu-\nples, denoted as ğ‘(ğ‘). To learn a cardinality estimation, like the exist-\ning work, we require a set of joinable attribute pairs {(ğ‘…1.ğ´ğ‘–,ğ‘…2.ğ´ğ‘—),\nÂ·Â·Â·}, in addition to a set of relations {ğ‘…1,Â·Â·Â·ğ‘…ğ‘}. For example, two\nrelations{ğ‘…1,ğ‘…2}with joinable attribute pairs {(ğ‘…1.ğ´3,ğ‘…2.ğ´3)}.\nThe Problem Statement : Given a set of relations and a set of\njoinable attribute pairs, learn a model M:ğ‘â†¦â†’Rfrom a trainingquery set{(ğ‘1,ğ‘(ğ‘1)),(ğ‘2,ğ‘(ğ‘2)),Â·Â·Â·}, whereğ‘ğ‘–is an SQL select-\nproject-join query and ğ‘(ğ‘ğ‘–)is its actual cardinality, to predict the\ncardinality for unseen queries.\nWe use q-error to evaluate the accuracy of the estimated value.\nq-error(ğ‘)=ğ‘šğ‘ğ‘¥\u001ağ‘(ğ‘)\nË†ğ‘(ğ‘),Ë†ğ‘(ğ‘)\nğ‘(ğ‘)\u001b\n(1)\nIntuitively, q-error quantifies the factor by which the estimated\ncount ( Ë†ğ‘(ğ‘ğ‘–)) differs from the true count ( ğ‘(ğ‘ğ‘–)). It is symmetrical\nand relative so that it provides the statistical stability for true counts\nof various magnitudes. Here, we assume ğ‘(ğ‘)â‰¥1andË†ğ‘(ğ‘)â‰¥1.\nWe learn an uncertainty-aware model, and we do not make any\nassumptions on data and query, e.g., the distribution of attribute\nvalues, the independence of attributes and relations, the distribution\nof the selection/join conditions of the queries, etc.\nQuery Encoding : Following the existing work, a select-project-\njoin query we support can be encoded by a fixed length vector. The\nencoding consists of two parts: the selection conditions and the join\nconditions. The two parts are encoded separately and concatenated\nas follows.\nğ‘…1selectionsz          }|          {\n0.25 0.5|    {z    }\nğ‘…1.ğ´1Â·Â·Â·ğ‘…2selectionsz           }|           {\n1 1 0 1 0|     {z     }\nğ‘…2.ğ´2Â·Â·Â·joinsz                 }|                 {\n1 1 0|{z}\n(ğ‘…1.ğ´3,ğ‘…2.ğ´3)0 0 0\nFor selection condition, the encoding of all the attributes in all\nthe relations of the schema are concatenated by a fixed order (e.g.,\nlexicographical order). In a similar manner, for the join conditions,\nthe encoding of all the join pairs are concatenated.\nThe selection conditions are specified on numerical/categorical\nattributes. For a range filter, ğ‘™ğ‘ğ‘—â‰¤ğ´ğ‘—â‰¤ğ‘¢ğ‘ğ‘—, on a numerical at-\ntributeğ´ğ‘—, we normalize ğ‘™ğ‘ğ‘—andğ‘¢ğ‘ğ‘—to[0,1]by mapping[ğ‘™ğ‘ğ‘—,ğ‘¢ğ‘ğ‘—]\nto[ğ‘™ğ‘ğ‘—âˆ’ğ‘šğ‘–ğ‘› ğ‘—\nğ‘šğ‘ğ‘¥ ğ‘—âˆ’ğ‘šğ‘–ğ‘› ğ‘—,ğ‘¢ğ‘ğ‘—âˆ’ğ‘šğ‘–ğ‘› ğ‘—\nğ‘šğ‘ğ‘¥ ğ‘—âˆ’ğ‘šğ‘–ğ‘› ğ‘—], where[ğ‘šğ‘–ğ‘›ğ‘—,ğ‘šğ‘ğ‘¥ğ‘—]is the domain of\nthe attribute ğ´ğ‘—. Thus, the representation is the interval of two real\nvalues. For an INfilter,ğ´ğ‘—INğ¶, on a categorical attribute ğ´ğ‘—, where\nğ¶is a subset of the attribute ğ´ğ‘—â€™s domain{ğ‘1,ğ‘2,...ğ‘ğ‘š}, a straight-\nforward encoding is to build an ğ‘š-dim bitmap where its ğ‘˜-th bit\nis 1 ifğ‘ğ‘˜âˆˆğ¶, otherwise 0. This binary representation is effective\nfor attributes with small domain, however, it is difficult to scale\non a large domain where the predicate vector is high-dimensional\nand sparse. Therefore, for a large domain, we adopt the factorized\nbitmap [ 66], i.e., slicing the whole bitmap to chunks, and converting\neach chunk into corresponding base-10 integer. Finally, the selec-\ntion condition is represented losslessly by âŒˆğ‘š/ğ‘ âŒ‰integers, where ğ‘ \nis the length of the chunk.\nRegarding join conditions, for each joinable attribute pair (ğ´ğ‘–,ğ´ğ‘—),\nwe use a 3-bit bit-map to encode the join condition on this pair,\ncorresponding to the 3 comparison operators, <,=,>, respectively,\nwhere â€˜1â€™ denotes there is a comparative condition on (ğ´ğ‘–,ğ´ğ‘—). For\nexample,ğ´ğ‘–<ğ´ğ‘—,ğ´ğ‘–â‰¥ğ´ğ‘—, andğ´ğ‘–â‰ ğ´ğ‘—are encoded as â€˜100â€™, â€˜011â€™\nand â€˜101â€™, respectively. The bit-map â€˜000â€™ denotes that the query is\nfree of join condition on the pair.\n3 An Overview\nWe present an NNGP overview for cardinality estimator that learns\na DL model using GP. We discuss NNGP from the viewpoints of\n\nx1x2\nh1h2h3h4\ny\nx1x2\nh1h2h3h4\n......\ny\n0.20.05âˆ’0.1âˆ’0.25\n0.550.40.250.1\n0.20.550.91.25Figure 2: Neural Network (left) vs. NNGP (right)\nstandard neural network and GP, where NNGP exhibits the approx-\nimation capability of neural network and can be solved by exact\nBayesian inference as a regular GP. Such NNGP properties enable\nour cardinality estimator to be robust, lightweight, and uncertainty-\naware, compared with the existing DL-based estimators. We show\nthe differences between a neural network and NNGP in Fig. 2.\nOn the left, Fig. 2 shows a standard fully-connected neural net-\nwork, which is the building block of all the DL-based estimators\nin Table 1. The hidden layer is a weighted linear transformation\nwith nonlinearity that transforms input representation to output\nin a layer by layer fashion. Given an empirical loss function, the\nparameters (i.e., the weights of the linear transformation) fit to\ngiven training data (i.e., the vectorized relational data or SQL query\nregarding cardinality estimation) by forward-backward propaga-\ntion algorithm. As a parametric model, the prediction on new input\nis determined by the learned parameters. Theoretically, neural net-\nwork is able to approximate any given continuous function [ 26,27],\nand achieve an arbitrary small approximation error with infinite\nwide hidden layers.\nOn the right, Fig 2 shows NNGP, which is a special class of\nBayesian DL model, whose hidden layer has an infinite number of\nneurons. In statistical learning, Bayesian inference is a principled\nway to describe prediction belief. Bayesian DL is derived from de-\nveloping Bayesian inference on modern DL [ 61]. Specifically, prior\ndistributions are imposed on the parameters of the neural network.\nIn other words, given a set of training data, the posterior distribu-\ntion of parameters is inferred by Bayes rule. And the prediction of\na new input is a probabilistic distribution computed by Bayesian\nmodel average that ensembles all the models in the parameters\nspace weighted by the posterior of the parameters.1NNGP, as a\nspecial type of Bayesian DL, is equivalent to GP in the sense that\nany finite collection of outputs is a Gaussian distribution, and its\noutput is a summation of an infinite number of i.i.d. random vari-\nables implied by Central Limit Theorem [ 48]. The infinite hidden\nneurons are composed of the set of GP basis functions, leading to\na parametric, non-stationary GP kernel. Compared with regular\nGP with stationary kernels, this DL-based kernel is more flexible\nto adapt to the underlying data by exploiting the representation\nlearning ability of DL. Our testing in Â§6 shows that without DL,\na simple GP estimator fails to achieve an approaching or better\nperformance, compared with the DL cardinality estimators.\n1In general, exact analytical posterior distribution is intractable, thereby approximate\ninference such as variational inference, Markov chain Monte Carlo are adopted.From the perspective of Bayesian DL, NNGP inherits the advan-\ntages of Bayesian approach while keeping the universal approxi-\nmation of neural network. With the parameter prior, NNGP con-\nverts from neural network parameter learning to the priorâ€™s hyper-\nparameter learning so that it overcomes the over-parametrization of\nneural network. Instead of betting on one parameter configuration,\nNNGP ensembles an infinite number of plausible neural network\nmodels in the space of a given architecture and prior family, thereby\nprovides a robust approximation. The predictive distribution nat-\nurally conveys the prediction uncertainty regarding the modelâ€™s\nposterior and indicates out-of-distribution testing points.\nFrom the perspective of GP, NNGP is a nonparametric model\nwith the infinite number of basis functions. Here, nonparametrics\ndoes not mean the model is parameter-free, but in the sense that\nthe model can not be specified in a finite number of parameters. As\na nonparametric model, NNGP utilizes a much larger model space\nto model distribution-free data. In other words, it does not assume\ndata to be modeled is i.i.d. or generated from a specified distribution\nas what DL models require [ 37]. The property of distribution-free\npromotes deploying database-oriented learning tasks (e.g., cardinal-\nity estimation), since real data and queries in DBMS are too large\nand diverse. It is difficult to assume that they are subject to some\ndistribution where both data and queries may evolve over time.\nTherefore, compared to general DL models, NNGP does not require\nlarge volume training samples to approximate an i.i.d. assump-\ntion. In addition, the learning paradigm of NNGP converts from\nparameter learning for DL to kernel learning for GP, which avoids\nthe approximation inference of Bayesian DL by manipulating the\ninfinite wide neural network implicitly. More concretely, NNGP\nunder certain neural network configuration (e.g., ReLU nonlinear-\nity) has an analytical kernel function, which means training and\nprediction of this special type of DL model can be solved by pure\nstatistical method in closed-form (i.e., exact Bayesian inference) as\nfor a regular GP. In our experiments of Â§6, we verify training an\nNNGP estimator on the fly only consumes several seconds, up to\n1-2 orders faster than corresponding DL estimators.\nWe show NNGP, as a kernel method, from the point of neural\nnetwork enhanced kernel function in Fig. 3. In Fig. 3, we visualize\ntwo kernel matrices of NNGP estimator by taking vectorized SQL\nqueries as the input, which are created by 500training queries\nand500test queries. The queries for Fig. 3(a) are on a single rela-\ntion forest , with 2âˆ¼10selection conditions, while the queries for\nFig. 3(b) are join queries over 0âˆ¼5relations in TPC-DS. The kernel\nmatrices are ordered by the number of selection/join conditions for\nthe training query and the coefficient of variation of the predictive\ndistribution for the test query. In Fig. 3, the lighter the color, the\nlarger the inner product of the infinite hidden representations of\na train-test query pair, which indicates the larger the similarity\nbetween the pair. There are two key observations. First, the uncer-\ntainty of the prediction of a test query is highly correlated with its\nsimilarity to the training queries. The lower the uncertainty, the\nlarger the similarity to all the 500 training queries. This observation\nsupports our intuition. Second, a query with more join conditions\ntends to be more dissimilar to other queries. The NNGP kernel pro-\nvides a simple yet effective mechanism to compare input similarity\nregarding transformed infinite feature space. The knowledge of\n\n2 4 6 8 10\n# of predicates (Train)low highCoefficient of Variation (Test)(a)forest\n0 1 2 3 4\n# of joins (Train)low highCoefficient of Variation (Test) (b)TPC-DS\nFigure 3: Test-Train Kernels of NNGP\nthe training data is persisted in the kernel matrix to smooth the\nprediction, with similarity measures between training/testing data.\nWe discuss NNGP with the existing DL approaches for cardi-\nnality estimation following a recent experimental study in [ 62]\nthat analyzes and compares 5 learned cardinality estimators over\nsingle relation. In [ 62], the authors identify a set of behavior log-\nics, as the inductive biases the learned estimators are expected to\ncapture, namely, consistency, monotonicity, stability, and fidelity.\nAll the query-driven DL estimators only preserve the stability as\nthey model the estimation as a regression task. The data-driven\nDL estimator NeuroCard supports fidelity but cannot satisfy others,\nwhereas the data-driven DL estimator DeepDB satisfies all the logics\nsince it relies on hierarchical density factorization. Our NNGP esti-\nmator is a supervised learning based estimator. Thereby NNGP only\nsupports stability like all the other query-driven regressors. How-\never, the NNGP estimator distinguishes from the existing learned\nestimators from its small tuning cost and swift model construction,\nwhich are promising for fast adaption in dynamic environments.\nNNGP supports Bayesian based uncertainty quantification regard-\ning the acquired knowledge, and kernel learning enables kernel\nbased feature understanding and selection [6, 55].\nFinally, the way we use GP is different from GP used for database\nconfiguration tuning [ 2,12]. First, in [ 2,12], GP is used to model\nthe concerned performance given a set of input configurations, and\nBayesian optimization is used to search for a better one among\na set of configurations. In our approach, the GP regressor is the\ncardinality estimator. Second, the GP used in [ 2,12] is equipped\nwith stationary kernel functions. For cardinality estimation with a\nlarger feature space (i.e., the query space and a higher requirement\nof accurate prediction), stationary kernel functions are not flexible\nin terms of adapting to the input data. We use non-stationary kernel\nfunctions that belong to a particular type of neural network.\n4 NNGP: from NN to GP\nIn this section, we discuss Bayesian learning, and its difference from\nstandard learning, introduce GP and present NNGP.\n4.1 Bayesian Learning\nIn standard parametric ML/DL, a model to learn is a function of\ninputğ‘¥,ğ‘“(ğ‘¥,ğ‘¤), parameterized by ğ‘¤. Learning is to optimize a\nspecified loss function L(ğ‘“(ğ‘¥,ğ‘¤),ğ‘¦)to fit the parameter ğ‘¤to train-\ning data(ğ‘¥,ğ‘¦)in a training set(ğ‘‹,ğ‘Œ). In contrast to learning by\noptimization , Bayesian learning is to learn by marginalization . The\nparameterğ‘¤is assumed to be a random variable drawn from ağ‘“(ğ‘¥)=ğ’˜ğ‘¥+ğ‘ğ‘“(ğ‘¥)=ğ’˜ğ‘¥+ğ‘\nğ’˜âˆ¼N( 0,ğœ2ğ‘¤),ğ‘âˆ¼N( 0,ğœ2\nğ‘)\n-1.5-1.0-0.500.51.0\nx1.61.82.02.22.4f(x)\n(a) Linear Regression\n-1.5-1.0-0.500.51.0\nx1.61.82.02.22.4f(x) (b) Bayesian Linear Regression\nFigure 4: LR vs. BLR\nprior distribution ğ‘(ğ‘¤). Given the observed training data (ğ‘‹,ğ‘Œ),\nthe posterior distribution of ğ‘¤can be inferred by Bayes rule (Eq. (2)).\nğ‘(ğ‘¤|ğ‘Œ,ğ‘‹)=ğ‘(ğ‘Œ|ğ‘‹,ğ‘¤)ğ‘(ğ‘¤)\nğ‘(ğ‘Œ|ğ‘‹)=ğ‘(ğ‘Œ|ğ‘‹,ğ‘¤)ğ‘(ğ‘¤)âˆ«\nğ‘(ğ‘Œ|ğ‘‹,ğ‘¤)ğ‘(ğ‘¤)ğ‘‘ğ‘¤(2)\nTo infer the target value for testing data ğ‘‹âˆ—, the predictive probabil-\nityğ‘(ğ‘Œâˆ—|ğ‘‹âˆ—,ğ‘Œ,ğ‘‹)is computed by applying the probabilistic sum\nand product rules, assuming the training and testing are conditional\nindependent regarding ğ‘¤.\nğ‘(ğ‘Œâˆ—|ğ‘‹âˆ—,ğ‘Œ,ğ‘‹)=âˆ«\nğ‘(ğ‘Œâˆ—|ğ‘‹âˆ—,ğ‘¤)ğ‘(ğ‘¤|ğ‘Œ,ğ‘‹)ğ‘‘ğ‘¤ (3)\nThe predictive distribution of Eq. (3) represents Bayesian model\naverage [64]. That is, instead of relying on a single prediction of one\nmodel with a single configuration of parameters, Eq. (3) ensembles\nall the models with all possible configurations of the parameters ğ‘¤,\nweighted by the posterior of the parameters, ğ‘(ğ‘¤|ğ‘Œ,ğ‘‹), from Eq. (2),\nby marginalization of ğ‘¤. Therefore, the predictive distribution does\nnot depend on any specific parameter configuration. In contrast,\nclassical training of parametric model aims to find one configuration\nË†ğ‘¤that maximizes the likelihood of the observed data or minimizes\nthe empirical loss in equivalence. In other words, the posterior\ndistribution ğ‘(ğ‘¤|ğ‘Œ,ğ‘‹)=1forğ‘¤=Ë†ğ‘¤and0otherwise, leading to\nthe model inference as Eq. (4).\nğ‘(ğ‘Œâˆ—|ğ‘‹âˆ—,ğ‘Œ,ğ‘‹)=ğ‘(ğ‘Œâˆ—|ğ‘‹âˆ—,Ë†ğ‘¤),Ë†ğ‘¤=arg max\nğ‘¤ğ‘(ğ‘Œ|ğ‘‹,ğ‘¤)(4)\nComparing the inference of Eq. (3) and Eq. (4), if the weights poste-\nriorğ‘(ğ‘¤|ğ‘Œ,ğ‘‹)has a flat distribution and ğ‘(ğ‘Œâˆ—|ğ‘‹âˆ—,Ë†ğ‘¤)varies signif-\nicantly where the posterior has mass, the discrepancy of prediction\nby Bayesian model average and classical approach tends to be large.\nOne well-known reason is the observed data is insufficient or devi-\nating from the features of the test data, where the observed data\ncannot well infer the weight posterior in principle. The Bayesian\nlearning calibrates this kind of uncertainty, a.k.a., epistemic uncer-\ntainty .\nExample 4.1: A simple example of Linear Regression (LR) and\nBayesian Linear Regression (BLR) is shown in Fig. 4 to illustrate the\ndifferences between standard learning and Bayesian learning. In LR\n(Fig. 4(a)), a linear function ğ‘“(ğ‘¥)=ğ‘¤ğ‘¥+ğ‘is used to fit the training\ndata points, minimizing the square error. The parameters ğ‘¤andğ‘\ncan be solved analytically by least squares or gradient descent. In\nBLR (Fig. 4(b)), it does not aim to solve a deterministic value for\nğ‘¤,ğ‘, instead the posterior distributions of ğ‘¤,ğ‘are computed, given\nthe training data, assuming the priors of ğ‘¤,ğ‘ are simple Gaussian\ndistributions with zero means. The model is no longer a linear func-\ntion but a random variable determined by its parameter posterior,\n\nwhich forms a function space with infinite linear functions. The\ncolor lines in Fig. 4(b) indicate some samples in the function space.\nThe derived predictive distribution (Eq. (3)) of BLR is a Gaussian\ndistribution, thereby the shaded area in Fig. 4(b) delineates the\n95%-confidential interval. We can observe that in the range with\nfew training data, i.e., ğ‘¥âˆˆ[âˆ’1.5,âˆ’0.5], the predictive uncertainty\ntends to be large, reflect to the fanout of the sampled functions and\nwider confidential interval. BLR only has two hyper-parameters to\nbe set, which are the variances ğœ2ğ‘¤,ğœ2\nğ‘of the parameter prior. The\nhyper-parameters can be tuned by Bayesian model selection [ 53].\n4.2 Gaussian Process\nGiven a set of ğ‘training data points ğ‘‹={ğ‘¥1,Â·Â·Â·,ğ‘¥ğ‘}and a model\nğ‘“(ğ‘¥), GP is used in modeling the joint distribution of the modelâ€™s\npredictions ğ‘“(ğ‘‹)={ğ‘“(ğ‘¥1),Â·Â·Â·,ğ‘“(ğ‘¥ğ‘)}. Consider a simple linear\nmodel in Eq. (5), with fixed basis function ğœ™(Â·)={ğœ™1,Â·Â·Â·,ğœ™ğ‘š},\n(e.g., polynomial, radial basis function), which is a projection from\noriginal input to a feature space.\nğ‘“(ğ‘‹)=ğ‘šâˆ‘ï¸\nğ‘—=1ğ‘¤ğ‘—ğœ™ğ‘—(ğ‘‹)=ğš½ğ‘¤ (5)\nIn Eq. (5), ğš½âˆˆRğ‘Ã—ğ‘šis the design matrix where ğš½ğ‘›ğ‘—=ğœ™ğ‘—(ğ‘¥ğ‘›),\ni.e., the value of the ğ‘—-th basis function for the ğ‘›-th data point.\nSuppose a zero mean Gaussian prior is put on the weights ğ‘¤, where\nIâˆˆRğ‘Ã—ğ‘is the Identity matrix.\nğ‘¤âˆ¼N( 0,ğœ2\nğ‘¤I) (6)\nSinceğ‘“(ğ‘‹)is a linear transformation of ğ‘¤,ğ‘“(ğ‘‹)is also a Gaussian\ndistribution as Eq. (7), where ğœ–âˆ¼N( 0,ğœ2ğœ–I)is an extra noise on\nğ‘“(ğ‘‹).\nğ‘“(ğ‘‹)âˆ¼N( 0,ğœ2\nğ‘¤ğš½ğš½ğ‘‡+ğœ2\nğœ–I) (7)\nHere, Eq. (7) shows the function ğ‘“(ğ‘¥), as a random variable, is\naGaussian process by definition such that for any finite selec-\ntion of points ğ‘‹,ğ‘“(ğ‘‹)is a joint Gaussian distribution [ 43]. The\ncovariance matrix Kğ‘‹,ğ‘‹=ğœ2ğ‘¤ğš½ğš½ğ‘‡+ğœ2ğœ–I, a.k.a., the kernel ma-\ntrix, models the similarity of data points. Specifically, the entry\nKğ‘–ğ‘˜=ğœ2ğ‘¤âŸ¨ğœ™(ğ‘¥ğ‘–),ğœ™(ğ‘¥ğ‘˜)âŸ©+ğœ2ğœ–Iğ‘–ğ‘˜measures the similarly of points\nğ‘¥ğ‘–,ğ‘¥ğ‘˜under basis function ğœ™(Â·). A simplest example is that ğœ™(Â·)is\nthe identity function, i.e., ğœ™(ğ‘¥)=ğ‘¥, its model is equivalent to BLR.\nTo make prediction for testing data points ğ‘‹âˆ—={ğ‘¥âˆ—\n1,Â·Â·Â·,ğ‘¥âˆ—\nğ‘€},\nwe need to compute the conditional distribution ğ‘(ğ‘“(ğ‘‹âˆ—)|ğ‘“(ğ‘‹))\nas the prediction. It is also proved to be a Gaussian distribution as\nEq. (8), where Kğ‘‹,ğ‘‹âˆˆRğ‘Ã—ğ‘,Kğ‘‹,ğ‘‹âˆ—âˆˆRğ‘Ã—ğ‘€,Kğ‘‹âˆ—,ğ‘‹âˆ—âˆˆRğ‘€Ã—ğ‘€\nare theğ‘‹,ğ‘‹ kernel,ğ‘‹,ğ‘‹âˆ—kernel andğ‘‹âˆ—,ğ‘‹âˆ—kernel matrices, re-\nspectively.\nğ‘“(ğ‘‹âˆ—)|ğ‘“(ğ‘‹)âˆ¼N(ğœ‡,ğ¶) (8)\nğœ‡=Kğ‘‡\nğ‘‹,ğ‘‹âˆ—Kâˆ’1\nğ‘‹,ğ‘‹ğ‘“(ğ‘‹) (9)\nğ¶=Kğ‘‹âˆ—,ğ‘‹âˆ—âˆ’Kğ‘‡\nğ‘‹,ğ‘‹âˆ—Kâˆ’1\nğ‘‹,ğ‘‹Kğ‘‹,ğ‘‹âˆ— (10)\nGiven the ground truth of the training data ğ‘‹, denoted as ğ‘Œ, and\nthe prediction target for ğ‘‹âˆ—isğ‘Œâˆ—, the expectation of the prediction\nisE[ğ‘Œâˆ—]=Kğ‘‡\nğ‘‹,ğ‘‹âˆ—Kâˆ’1\nğ‘‹,ğ‘‹ğ‘Œ, derived from Eq. (9). Assume a matrix\nof functions â„(ğ‘‹âˆ—)=Kğ‘‡\nğ‘‹,ğ‘‹âˆ—Kâˆ’1\nğ‘‹,ğ‘‹, we have E[ğ‘“(ğ‘‹âˆ—)]=â„(ğ‘‹âˆ—)ğ‘Œ,\nindicating GP regression is a weighted linear smoother over theobserved target value ğ‘Œ. The weight function â„is determined by the\ntrain-train and train-test kernels. Meanwhile, the diagonal element\nof matrixğ¶in Eq. (9) measures the variance of the prediction.\nWith this predictive Gaussian distribution, we can easily compute\ntheğ›¿-confidential interval of ğ‘“(ğ‘¥âˆ—)as[ğœ‡ğ‘¥âˆ—âˆ’ğ‘ğ›¿ğ‘‘ğ‘–ğ‘ğ‘”(ğ¶)ğ‘¥âˆ—,ğœ‡ğ‘¥âˆ—+\nğ‘ğ›¿ğ‘‘ğ‘–ğ‘ğ‘”(ğ¶)ğ‘¥âˆ—], whereğ‘ğ›¿is theğ›¿-quantile ofN(0,1). Intuitively, the\nexpectation E[ğ‘¦âˆ—]should be treated as the explicit prediction Ë†ğ‘¦. In\npractical applications, as it aims to minimize the predictive loss\ngiven an empirical loss function, the prediction is to find Ë†ğ‘¦that\nminimize the expected loss by averaging the empirical loss Lexp\nw.r.t. the predictive distribution as Eq. (11).\nLexp=âˆ«\nLemp(ğ‘¦âˆ—,Ë†ğ‘¦)ğ‘(ğ‘¦âˆ—|ğ‘¥âˆ—,ğ‘Œ,ğ‘‹)ğ‘‘ğ‘¦âˆ—(11)\n4.3 Neural Network Gaussian Process\nGP is a stochastic process with a fixed basis function. If the basis\nis fixed, the model is linear w.r.t. the parameters, and the kernel\nfunction Kas well as the predictive distribution (Eq. (8)) are an-\nalytically tractable. The limitation of a fixed basis function is its\nincapability of adapting to the training data. In general, a model\nwith an adaptive basis function (e.g., neural networks) can be a po-\ntential extension, but it is much difficult to treat it analytically [ 53].\nTo address it, the authors in [ 48] show that there is a special case\nwhere the neural network has an infinite number of hidden units.\nWith such findings, some complex neural network architectures\nwith infinite wide hidden layers are proved to be GP such as con-\nvolutional neural network [18, 49], recurrent neural network [65],\nattention [28] and graph neural network [29].\nWe discuss the foundation, the infinite wide multilayer percep-\ntrons which we use for cardinality estimation. We explain it using a\nsingle hidden layer feed forward neural network, ğ‘“(ğ‘¥), which takes\nğ‘‘-dim vector ğ‘¥=[ğ‘¥ğ‘—]ğ‘‘as input, and predicts a scalar value ğ‘¦. Here,\nğœ(Â·)is the nonlinear activation function, ğ‘0=[ğ‘0\nğ‘—]ğ‘š,ğ‘âˆˆRis the\nbias term, and both ğ‘¤0=[ğ‘¤0\nğ‘–ğ‘—]ğ‘šÃ—ğ‘‘andğ‘¤=[ğ‘¤ğ‘–]ğ‘šare the weights\nof the hidden layer and the output layer, respectively. Eq. (12)-(13)\nshow the computation on each neuron of the hidden and output\nlayers, where â„ğ‘—in Eq. (12) is the post-activation of the ğ‘—-th hidden\nunit.\nâ„ğ‘–(ğ‘¥)=ğœ(ğ‘‘âˆ‘ï¸\nğ‘—=1ğ‘¤0\nğ‘–ğ‘—ğ‘¥ğ‘—+ğ‘0\nğ‘—) (12)\nğ‘“(ğ‘¥)=ğ‘šâˆ‘ï¸\nğ‘–=1ğ‘¤ğ‘–â„ğ‘–(ğ‘¥)+ğ‘ (13)\nIn classical machine learning, it optimizes the parameters ğ‘¤and\nğ‘¤0directly, under a specified loss function as an objective, by back\npropagation algorithm. Note that, even though the neural network\nis nonlinear, it can be regarded as a linear combination of a collec-\ntion of parametric basis functions {â„1(ğ‘¥)Â·Â·Â·,â„ğ‘š(ğ‘¥)}in Eq. (13) [ 3].\nThe basis functions are parametrized by the weight ğ‘¤0and will be\ntrained to adapt to the training data. Under the assumption that,\nfor each layer, the weight and bias element parameters have i.i.d.\nprior densities, we have\nğ‘¤0\nğ‘–ğ‘—âˆ¼D( 0,ğœ2\nğ‘¤/ğ‘‘),ğ‘0\nğ‘—âˆ¼D( 0,ğœ2\nğ‘) (14)\nğ‘¤ğ‘–âˆ¼D( 0,ğœ2\nğ‘¤/ğ‘š),ğ‘âˆ¼D( 0,ğœ2\nğ‘)\n\nwhere the prior distribution Dcan be non-Gaussian. Because the\nweight and bias parameters are subject to be i.i.d., and have zero\nmean, the hidden units â„ğ‘–(ğ‘¥)are i.i.d. bounded random variables.\nFollowing the Central Limit Theorem that, for ğ‘ši.i.d. random vari-\nables with bounded mean and variance, the summation of them is\na Gaussian distribution when ğ‘šâ†’âˆ . Thus, we have ğ‘“(ğ‘¥)as an\napproximate Gaussian distribution when the width of output layer\nğ‘šis large, as given in Eq. (15).\nğ‘“(ğ‘¥)âˆ¼N( 0,ğœ2\nğ‘+ğœ2\nğ‘¤E[â„ğ‘–(ğ‘¥)2]) (15)\nLikewise, following the multi-dimensional Central Limit Theorem,\nany finite collection ğ‘“(ğ‘‹)={ğ‘“(ğ‘¥1),Â·Â·Â·,ğ‘“(ğ‘¥ğ‘)}have a joint mul-\ntivariate Gaussian distribution, which is exactly a GP.\nğ‘“(ğ‘‹)âˆ¼N( 0,K) (16)\nK=ğœ2\nğ‘¤E[ğš½ğš½ğ‘‡]+ğœ2\nğ‘I (17)\nThis GP is the Neural Network Gaussian Process (NNGP). This\nreveals priors over infinite wide neural network leads to an equiv-\nalence to GP. In the kernel function Kof Eq. (17), ğš½is the design\nmatrix given the parametric basis function {â„1(ğ‘¥)Â·Â·Â·,â„ğ‘š(ğ‘¥)}. The\ndifference between the NNGP kernel (Eq. (17)) and the standard\nliner modelâ€™s kernel (Eq. (7)) is that the NNGP kernel needs to com-\npute the expectation of the product of the design matrix w.r.t. the\nprior distribution of the parameters, which is used to define the\nbasis function. This enables NNGP to be not only a model ensemble\nin the space of the linear parameters ğ‘¤but also in the space of\nthe basis function parameters ğ‘¤0. By recursively applying Central\nLimit Theorem, the kernel of deep neural network is induced [38]\nin Eq. (18), where ğ‘“ğ‘™\nğ‘—is the pre-activation of the ğ‘—-th hidden unit in\ntheğ‘™-th layer and ğš½ğ‘™is the design matrix defined by the 0-th toğ‘™-th\nlayers of the neural network. The base case kernel K0in Eq. (19) is\nequivalent to the kernel of BLR.\nKğ‘™=E[ğ‘“ğ‘™\nğ‘—(ğ‘‹)ğ‘“ğ‘™\nğ‘—(ğ‘‹)]\n=ğœ2\nğ‘¤Eğ‘“ğ‘™âˆ’1\nğ‘—âˆ¼N( 0,Kğ‘™âˆ’1)[ğš½ğ‘™âˆ’1(ğš½ğ‘™âˆ’1)ğ‘‡]+ğœ2\nğ‘I (18)\nK0=ğœ2\nğ‘¤ğ‘‹ğ‘‹ğ‘‡+ğœ2\nğ‘I (19)\nThe NNGP kernel can be computed analytically under certain acti-\nvation functions ğœ(Â·), (e.g., the rectified linear function ReLU [8],\nthe error function Erf[63]). To conduct inference, as the model is a\nstandard GP, we can get the exact solution by Eq. (8).\nWe discuss the complexity of NNGP. As a standard GP, exact\nprediction needs to compute the inverse of the kernel matrix in\nğ‘‚(ğ‘3), whereğ‘is the number of training data points. Note that\nthe kernel matrix is in closed-form and the inversion only needs to\ncompute once in advance. For a new test data point, inference takes\nvector-matrix multiplication in ğ‘‚(ğ‘2). Quadratic time complex-\nity w.r.t. training data is an obstacle to deploying GP model. But,\nwith the acceleration of parallel and GPU architecture, the training\nand inference time is reasonable, as confirmed in our extensive\nexperimental studies. Note that GP is usually data-efficient.\nModel design for cardinality estimation : To test query set ğ‘„,\nwe use the mean-squared-error (MSE) as the empirical loss functionof NNGP in Eq. (11), which is shown in Eq. (20)-(21).\nLemp(ğ‘„)=1\n|ğ‘„|âˆ‘ï¸\nğ‘âˆˆğ‘„Lemp(ğ‘(ğ‘),bğ‘(ğ‘)) (20)\nLemp(ğ‘(ğ‘),bğ‘(ğ‘))=|logğ‘(ğ‘)âˆ’logbğ‘(ğ‘)|2=log2ğ‘(ğ‘)\nbğ‘(ğ‘)(21)\nTo achieve an average low relative error, the target ğ‘(ğ‘)and predic-\ntionbğ‘(ğ‘)are transformed to logarithmic scale. It is worth noting\nthat minimizing the average of logğ‘(ğ‘)\nbğ‘(ğ‘)(Eq. (20)) is equivalent\nto minimize the geometric mean of q-error , and minimizing the\nsquared-error (Eq. (21)) further imposes higher weights on larger\nq-error over the average due to the square [ 14]. When the empirical\nloss is squared loss, the prediction that minimizes the expected loss\nof Eq. (11) is the mean of the predictive distribution, i.e., Eq. (9).\n5 Calibration of Uncertainty\nIn this section, we investigate the uncertainty calibration for car-\ndinality estimation by NNGP in comparison with two existing DL\napproaches for uncertainty calibration that are applicable for regres-\nsion tasks, namely, Deep Ensemble and Bayesian Neural Network.\nWe introduce them below.\nDeep Ensemble is a uniformly-weighted mixture model where each\nmixture is a deep neural network [ 36]. Each neural network treats\none data point as a sample from a Gaussian distribution, in predict-\ning the mean and variance via the final layer of the neural network.\nTraining one neural network ğ‘¤is to minimize its negative Gaussian\nlog-likelihood in Eq. (22), where ğœ‡ğ‘¤(ğ‘¥)andğœ2ğ‘¤(ğ‘¥)are its predictive\nmean and variance, respectively.\nâˆ’logğ‘ğ‘¤(ğ‘¦|ğ‘¥)=logğœ2ğ‘¤(ğ‘¥)\n2+(ğ‘¦âˆ’ğœ‡ğ‘¤(ğ‘¥))2\n2ğœ2ğ‘¤(ğ‘¥)(22)\nThe ensemble prediction is approximated as a Gaussian distribution\nwhose mean and variance are respectively the mean and variance\nofğ‘€neural networks. The idea of Deep Ensemble is simple whereas\nit needs to maintain ğ‘€neural networks explicitly.\nBayesian Neural Network (BNN) , as the BDL model, quantifies the\nuncertainty for neural network by defining a prior belief ğ‘(ğ‘¤)on\nits parameterization. The prediction is a distribution marginalizing\nover the posterior distribution ğ‘(ğ‘¤|ğ‘Œ,ğ‘‹)as shown in Eq. (3). The\ncomputation of this marginalization is approximated by variational\ninference where ğ‘(ğ‘¤)is a tractable variational distribution. Gal et\nal. in [ 16,17] propose an efficient inference that relates a Bernoulli\nvariational distribution to BNN via dropout training of the neural\nnetwork. In [ 16,17], inference is done by training with a dropout\nbefore weight layers and by performing dropout at test time, and\nthe output distribution is approximated by ğ‘‡Monte Carlo forward\npasses with stochastic parameter ğ‘¤ğ‘¡(Eq. (23)).\nğ‘(ğ‘¦âˆ—|ğ‘¥âˆ—,ğ‘Œ,ğ‘‹)â‰ˆâˆ«\nğ‘(ğ‘¦âˆ—|ğ‘¥âˆ—,ğ‘¤)ğ‘(ğ‘¤)ğ‘‘ğ‘¤â‰ˆ1\nğ‘‡ğ‘‡âˆ‘ï¸\nğ‘¡=1ğ‘(ğ‘¦âˆ—|ğ‘¥âˆ—,ğ‘¤ğ‘¡)\n(23)\nWe have implemented above two techniques for the lightweight\nneural network estimator [ 14], as DeepEns andBNN-MCD respec-\ntively. The estimator is a two-layer multilayer perceptron with 512\nhidden units. The DeepEns ensembles 5 estimators and BNN-MCD\n\n10âˆ’210âˆ’1100101102103\nq-error0.20.40.60.81.01.21.4Coefficient of Variation(a)forest NNGP\n10âˆ’710âˆ’410âˆ’1102105108\nq-error0.40.60.81.0Coefficient of Variation (b)forest DeepEns\n10âˆ’2100102104\nq-error0.050.100.150.200.250.30Coefficient of Variation (c)forest BNN-MCD\n10âˆ’2100102\nq-error0.20.40.60.81.0Coefficient of Variation\n(d)TPC-DSNNGP\n10âˆ’1310âˆ’610110810151022\nq-error0.250.500.751.001.251.501.75Coefficient of Variation (e)TPC-DSDeepEns\n10âˆ’1101103\nq-error0.0250.0500.0750.1000.1250.1500.175Coefficient of Variation (f)TPC-DSBNN-MCD\nFigure 5: Visualization of Estimation Uncertainty\nuses a dropout probability of 0.5 and 2,000 forward passes for the\nprediction.\nFig. 5 visualizes the uncertainty and the q-error for thousands\nof queries on forest andTPC-DS, where the points denote testing\nqueries. The uncertainty is represented by coefficient of variation,\nwhich is the standard deviation normalized by the mean, to quantify\nthe dispersion of the predictive distribution. Ideally, the uncertainty\nshould be highly correlated with the prediction error. That is, the\nlarger the q-error , the larger the uncertainty. NNGP estimator shows\nsuch behavior. For the queries with larger q-error , their coefficient\nof variation of NNGP estimator is large in Fig. 5(a) and Fig. 5(d), where\nthe scatter plots are in a V-shape. For DeepEns andBNN-MCD , this V-\nshape distribution is not obvious. The flat bottom in the scatter plots\nindicates the estimators are still over-confident on some queries\nwhose q-error is far away from 1.0. Furthermore, we also observe\nthatDeepEns still has a relative worse performance w.r.t. prediction\naccuracy, compared with NNGP andBNN-MCD . And BNN-MCD faces\nthe risk of unstable training as the variational inference aims to\noptimize a lower bound of the original training loss. All the three\nestimators make some conservative predictions in the sense that the\nactual q-error is small whereas the uncertainty is large. It reflects\nthe fact that all of them are possible to process queries out-of-\ndistribution, via different smoothing techniques on their prediction.\nIn terms of efficiency, DeepEns needs to train and maintain mul-\ntiple neural networks explicitly, and BNN-MCD needs to conduct a\nlarge number of forward passes in the testing phase. It is impractical\nto deploy these two techniques on high-end DL estimators, (e.g.,\nthe Tree LSTM estimator [ 56]) in a DBMS. In Â§6, we show that the\ntraining and prediction of NNGP are faster than wide neural network,\nthus is faster than the DeepEns andBNN-MCD counterparts.\n6 Experimental Studies\nIn this section, we give the test setting (Â§6.1), and report the exten-\nsive experiments in the following facets: â‘ Compare the accuracy\nof NNGP estimator with state-of-the-art ML/DL estimators (Â§6.2). â‘¡\nTest the training and prediction efficiency of NNGP estimator and\nverify it is a lightweight DL-based estimator (Â§6.3). â‘£Validate the\nrobustness of NNGP estimation regarding small number of training\nqueries and unbalanced workloads (Â§6.4) â‘¢Study the application\nof the uncertainty that NNGP provides in active learning, which\nall the DL-based estimators do not support (Â§6.5).6.1 Experimental Setup\nBaseline Approaches. To comprehensively evaluate the effec-\ntiveness and efficiency of NNGP estimator, we compare it with\n8 estimators including 5 query-driven learned estimators, 2 data-\ndriven learned estimators and 1 traditional estimator as follows.\nâŠNeural Network ( NN) [14] is the standard fully-connected neu-\nral network with ReLU activation. â‹Gradient Boosting Decision\nTree ( GBDT ) [13,14] is the ensembling decision regression trees\nby gradient boosting. âŒMulti-set Convolutional Neural Network\n(MSCN ) [32] firstly embeds the table set, join set and predicate set by\n3 separate multilayer perceptrons as the set convolutions, respec-\ntively, and then concatenate the 3 embeddings to a long vector and\nfeed it into a final output network. âTree LSTM ( TLSTM ) [56] is\na high-end DL model originally designed for cost and cardinality\nestimation, by taking tree-structured query plans as its input. It\nis composed of three stacked layers, the embedding layer, repre-\nsentation layer and estimation layer. The embedding layer embeds\noperations (join and table scan), predicates, metadata of leaf nodes\nof the plan tree into vector representations. The embedding layer\naggregates the representation of each node on the tree in a bottom-\nup fashion by Tree-Structured LSTM [ 57]. The estimation layer\nis a fully-connected neural network with sigmoid activations that\nfinally outputs the predictions. A detailed formulation of the model\ncan be found in [ 56].âDeepDB [25] (DeepDB ) is an unsupervised\nlearning, data-driven estimator that uses Relational Sum-Product\nNetwork (RSPN) [ 46] to model the joint distribution of a relation.\nTo support join queries, an ensemble of RSPNs or a joint RSPN is\nbuilt, and the choice is based on independent test on the relations.\nSQL queries are complied into probabilistic queries on the RSPN. â\nNaru [67]/NeuroCard [66] (NeuroCard ) factorizes the joint distribu-\ntion into conditional distributions and adopts deep autoregressive\nmodels such as MADE [ 19] or Transformer [ 60] to approximate the\njoint distribution. NeuroCard further extends Naru to support full\nouter joins. To predict the cardinalities, progressive sampling is con-\nducted over the density model. âPostgreSQL estimator ( Postgres )\nis a build-in statistical estimator. Estimated cardinality is obtained\nfrom the EXPLAIN command of PostgreSQL .â‘Gaussian Process\nwith radial basis kernel function ( GP-RBF ) is compared as a typical\nGP baseline. All the above learned estimators only support PK/FK\njoins.\nImplementation and Settings. The NNGP estimator is built on\nNeural Tangents [50], which is based on Google JAX [4]. The empir-\nical zero-mean Gaussian prior is imposed on the weights of neural\nnetworks and ReLU is used as the nonlinear activation, leading\nto a closed-form kernel function for Bayesian inference. There is\nno extra cost paid for NNGP hyper-parameters tuning. GBDT esti-\nmator is implemented by XGBoost [7] and a tree ensemble con-\ntains 32 trees. GP-RBF is implemented by the exact GP regressor\nofscikit-learn . For NeuroCard , we follow all the hyper-parameter\nconfigurations in its implementation based on MADE as the deep\nautoregressive model. NN,MSCN , and TLSTM estimators are imple-\nmented by PyTorch 1.6 [ 1]. We use the Adam optimizer with a\ndecaying learning rate to train these models. For different datasets,\nthe main hyper-parameters for training are tuned in their empirical\nrange: learning rate âˆˆ[10âˆ’3,10âˆ’4], epochsâˆˆ{50,80,100}, mini-\nbatch sizeâˆˆ{16,32,64}, L2 penalty of Adam âˆˆ[10âˆ’3,10âˆ’5]. For NN,\n\nTable 2: Query Sets\nType Dataset # Queries # Select/Join Cond. Range of ğ‘(ğ‘)\nSingle Rel. forest 18,000 {2, Â·Â·Â·, 10}[100,106]\nSingle Rel. higgs 12,000 {2, Â·Â·Â·, 7}[100,108]\nJoin TPC-H 16,000 {0, 1, 2, 3} [100,107]\nJoin TPC-DS 15,000 {0, 1, 2, 3, 4} [100,107]\nGBDT ,GP-RBF andNNGP , their encodings of input are same, as the\nintroduction in Â§2. Other models have their own input encoding\ndue to their specific model design. Particularly, as TLSTM takes a\ntree-structured plan as input, we generate a left-deep tree for each\njoin query following a total order of the relations. Since we only\nperform one task of cardinality estimation, one-hot physical oper-\nator encoding is simplified to the corresponding one-hot logical\noperator encoding. Both training and prediction are performed on\na Linux server with 32 Intel(R) Xeon(R) Silver 4215 CPUs and 128G\nRAM.\nDatasets. We use 4 relational datasets, 2 for single relation range\nqueries, 2 for multi-relation join queries. forest [11] originally con-\ntains 54 attributes of forest cover type. Following [ 14,21], we use\nthe first 10 numerical attributes. The relation has about 581K of\nrows. higgs [11] is a physical dataset contains 7 high-level kinematic\nattributes of particles, collected by detectors and post-processed\nby scientific functions. The relation has 11M rows. TPC-H(1 GB).\nWe use the relations supplier ,orders ,part andlineitem . There are 3\nPK/FK join conditions. TPC-DS(2 GB). We use the relations store ,\nitem,customer andpromotion ,store-sales , where store-sales is the\nfactual relation and others are the dimensional relations. There are\n5 PK/FK join conditions and the schema has a cycle.\nQueries. We construct large query workloads in the following way.\nFor single relation forest andhiggs , we generate query sets with\nthe number of select conditions varying from 2 to ğ·whereğ·is the\nnumber of attributes, and generate 2,000 queries for each subset.\nTo generate a query of ğ‘‘selection conditions, first we uniformly\nsampleğ‘‘attributes from all the ğ·attributes, then uniformly sample\neach attribute by the data-centric distribution following [ 14]. For\njoin queries, we test and report the existing baselines supported\nquery type, i.e., PK/FK join without selection conditions on the\njoin attributes. We generate query sets with the number of PK/FK\njoins, t, varying from 0 to |ğ‘‡|âˆ’1, where|ğ‘‡|is the number of\ninvolved relations. To generate a query of ğ‘¡(ğ‘¡>0)joins, firstly a\nstarting relation is uniformly sampled, then the query is constructed\nby traversing from the starting relation over the join graph in ğ‘¡\nsteps. Here, for each relation of the sampled join query, additional\nselection conditions are drawn independently. For each ğ‘¡, 4,000 and\n3,000 queries are generated for TPC-HandTPC-DS, respectively.\nWe only preserve unique queries with nonzero cardinality. Table 2\nsummarizes the 4 corresponding query sets.\n6.2 Accuracy\nWe investigate the estimation accuracy of NNGP estimator. For\nthe query-centric approaches, i.e., NNGP ,NN,GBDT ,MSCN ,TLSTM and\nGP-RBF , we split the queries into 60% for training, 20% for validation\nand 20% for testing. The split is conducted by stratified sampling\non the subsets with different numbers of selection/join conditions.\nFig. 6 presents the statistical distribution of q-error ofNNGP com-\npared with the 8 baselines. In general, NNGP ,MSCN andDeepDB are\nforest higgs TPC-H TPC-DS1102104\nDatasetElapsed Time (ms)NNGP NN GBDT MSCN TLSTM\nDeepDB NeuroCard Postgres GP-RBF\n2-3 4-5 6-7 8-9 1010âˆ’810âˆ’41104108\n# of Selection Conditionsq-error(a)forest\n2-3 4-5 6-710âˆ’1010âˆ’511051010\n# of Selection Conditionsq-error\n(b)higgs\n0 1 2 310âˆ’610âˆ’31103106\n# of Joinsq-error\n(c)TPC-H\n0 1 2 3 410âˆ’1010âˆ’511051010\n# of Joinsq-error\n(d)TPC-DS\nFigure 6: Query Evaluation Accuracy\nthe top three best performed estimators. There is no an overall best\nestimator that consistently outperforms others under all the test cir-\ncumstances. Due to the infinite wide hidden layer, the performance\nofNNGP consistently surpasses the finite wide NN. The performance\nadvantage of NNGP is mainly reflected in queries of single relation\n(Fig. 6(a), 6(b)), and the 25%-75% quantile of q-error is within 1.5. For\nthe join queries (Fig. 6(c), 6(d)), we speculate the join encoding and\nthe multilayer perceptrons based kernel of NNGP are simple to model\nthe complex join queries. In contrast, the performance advantage\nofMSCN is reflected in the join queries rather than queries on single\nrelation. Due to satisfaction the behavior logics, DeepDB achieves a\npromising 25%-75% quantile of q-error . However, we observe that\nthere are many over/underestimated queries for forest (Fig. 6(a))\nandTPC-DS(Fig. 6(d)), as the model would factorize intertwined\nattributes under the conditional independent assumption.\nWe discuss other baseline approaches. Although TLSTM is a\nhigh-end DL estimator with complex layers, its performance is\nnot promising for a sole cardinality estimation task. We specu-\nlateTLSTM is mainly applicable for real cost estimation tasks when\nmore features about the physical operators and meta-data of the\n\nforest higgs TPC-H TPC-DS10âˆ’410âˆ’21102104\nDatasetElapsed Time (ms)NNGP NN GBDT MSCN\nTLSTM DeepDB NeuroCardFigure 7: Query Evaluation Latency (ms)\nphysical database are fed into the model. Its original implemen-\ntation learns TLSTM by multi-task learning for the physical cost\nand the cardinality. However, the tree-structured plan directly in-\nfluences the physical cost but not the cardinality. The simple GP\nbaseline, GP-RBF , underfits the training queries, and therefore it can-\nnot be used as a cardinality estimator. It indicates the representation\nlearning provided by DL is critical for a complicated learning task.\nPostgres uses single column statistics and assumes the columns\nare independent. This conventional statistical approach performs\nwell on the benchmark TPC-Hbut leads to large estimation bias on\nother datasets, particularly for complex queries.\n6.3 Efficiency\nWe compare the prediction time of NNGP estimators with the base-\nline approaches. Fig. 7 presents the average elapsed time of the\nwhole test query set on our 32-core CPU. In general, the light-\nweight GBDT estimator achieves the fastest prediction and our NNGP\nestimator is the second best. XGBoost is a well-optimized learning\nsystem for GBDT . The inference of NNGP is a linear combination of\nthe ground-truth cardinalities, where the weights are the closed-\nform kernels of Eq. (18). The prediction of DeepDB is 2-3 orders\nof magnitude slower than that of supervised learning approaches.\nDeepDB needs to perform a bottom-up pass on the tree-structured\ndeep sum-product network. Estimation on NeuroCard is quite slow\nas one query needs thousands of sampling on the model to perform\na Monte Carlo integration. We noticed that in its original paper [ 66],\nthe GPU executed prediction is up to 2 orders of magnitude faster\nthan our CPU execution. That means GPU acceleration is necessary\nto employ NeuroCard as an estimator available for DBMS. However,\nNeuroCard also consumes large GPU memory. Training on forest\nis out of memory on a 16GB Nvidia V100. In terms of easy use for\nDBMS, lightweight estimators like our NNGP have a great advantage.\nForNNGP ,NNandGBDT , the prediction time for queries with different\nnumber of selection/join conditions are equal, as the input vectors\nare of equal length. For other estimators, the more complex the\nquery, the longer the prediction time. The prediction time of DeepDB\nandNeuroCard is also related to the database schema. In general,\ncomplex schema incurs a larger model and longer prediction time.\nScalability of Training . We test the scalability of NNGP estimator\nin the facets of training time and memory usage, comparing with the\ntop-2 lightweight models NNandGBDT . As the the number of queries\nincreases linearly, Fig. 8(a), 8(c), 8(e) and 8(g) show the training time\non CPUs, where training an NNtakes tens to hundreds seconds while\ntraining GBDT orNNGP only takes less than 3 seconds. Although the\ntraining complexity of NNGP isğ‘‚(ğ‘3), the exact Bayesian inference\nis boosted by highly paralleled basic linear algebra operations. The\n2.6 5.2 7.8 10.401234\n# of Training Queries (K)Max Memory Usage (GB)NNGP NN GBDT\n2.6 5.2 7.8 10.40.11.010100\n# of Training Queries (K)Training Time (s)(a)forest\n2.6 5.2 7.8 10.401234\n# of Training Queries (K)Max Memory Usage (GB) (b)forest\n2.4 4.8 7.2 9.60.11.010100\n# of Training Queries (K)Training Time (s)\n(c)higgs\n2.4 4.8 7.2 9.6012345\n# of Training Queries (K)Max Memory Usage (GB) (d)higgs\n3.2 6.5 9.6 12.80.11.010100\n# of Training Queries (K)Training Time (s)\n(e)TPC-H\n3.2 6.5 9.6 12.80246\n# of Training Queries (K)Max Memory Usage (GB) (f)TPC-H\n3 6 9 120.11.010100\n# of Training Queries (K)Training Time (s)\n(g)TPC-DS\n3 6 9 1201234\n# of Training Queries (K)Max Memory Usage (GB) (h)TPC-DS\nFigure 8: Training Time & Memory Usage\ntime complexity of NNisğ‘‚(ğ‘‡ğ‘ğ‘‘2â„), whereğ‘‡,ğ‘‘, andâ„are the\nnumber of iterations/epochs, dimension of input and the number\nof neurons in the hidden layer. Extra overhead is involved in the\nforward/backward propagation. Fig. 8(b), 8(d), 8(f) and 8(h) show the\npeak memory usage monitored in the training phase. Compared to\nNNandGBDT ,NNGP needs more memory to persist the kernel matrix,\nwhich is quadratic to the number of training queries. Fortunately,\nas we will show in Â§6.4, as a nonparametric model, NNGP already\nachieves satisfying and robust performance under a small volume\nof training data. It is worth noting that all the other estimators are\nmore time and memory consuming than the two-layer NN. Given\nthe same epoch, training of MSCN is roughly constant time slower\nthan that of NNand training of TLSTM is even much slower than\nthat of MSCN andNN. For the data-driven estimators, DeepDB and\nNeuroCard , the resources consumed are directly determined by the\nscale of the input relations. Among all the learned estimators, the\nmost consuming is NeuroCard , where training on the large dataset\nhiggs is failed within 72 hours.\n6.4 Robustness to Workload Shifts\nTo study the robustness of NNGP estimator, we train different models\nindependently over various training workloads of forest andTPC-H,\nand test the models on split fixed test query sets, i.e., 20% of the\n\nTable 3: mean-squared-error (MSE) on test queries\nMSE Estimator Origin Iter. 1 Iter. 2 Iter. 3\nforestNNGP 6.27 5.79 5.65 5.50\nDeepEns 45.43 38.00 35.26 33.50\nBNN-MCD 36.18 33.63 34.87 34.80\nTPC-HNNGP 5.30 5.13 5.02 4.95\nDeepEns 14.14 11.29 10.87 10.49\nBNN-MCD 16.78 11.45 11.35 11.53\nwhole query sets. The test queries are evenly distributed on the\nnumber of selection conditions for forest or the number of join\nconditions for TPC-H. To generate various training workloads, first,\nwe control the total number of training queries to 1,000,2,000,\n4,000,8,000, respectively, where the queries with different numbers\nof selection/join conditions are evenly distributed, and the result\nis shown in Fig. 9(a) and 9(c). Second, the total number of training\nqueries is fixed to 40% of the whole queries, and the fraction of\nqueries with less/more selection or join conditions is set within\n{2:8, 4:6, 6:4, 8:2}. forest queries with 2-5 selection conditions are\nregarded as less conditions queries while others are more conditions.\nTPC-Hqueries with 0-1 joins are regarded as less conditions queries\nwhile others are more conditions. The testing results on the fixed\n20% test queries are shown in Fig. 9(b) and 9(d).\nIn general, the key observation is that the NNGP estimator per-\nforms supremely robust on various workloads, regardless of the\nnumber of training queries and the fraction of different queries. In\nFig. 9(a) and 9(c), even though there are only 1,000training queries,\ntheq-error statistics of NNGP varies within one order of magnitude\ncompared with the model with 8,000training queries, and even\nbetter than NNwith 8,000training queries. In contrast, due to over-\nfitting, the q-error of its counterpart NNis degenerated drastically\nin the scenario of fewer training queries, which is up to 1010. As\nthe fraction of queries with less/more selection or join conditions\nshifts in Fig. 9(b) or Fig. 9(d), respectively, the performance of NNGP\nalso oscillates slightly. As a nonparametric model, GBDT is relatively\nmore robust than NNbut has a larger q-error . We notice that GBDT\nunderfits on forest , whose prediction has a large bias on the test\nqueries and even on the training queries. In the experiments, we\nalso observe that the performance of NNestimator is not stable dur-\ning multiple runs, which is influenced by parameter initialization\nand stochastic optimization.\n6.5 Uncertainty for Active Learning\nFinally, we investigate leveraging the predictive uncertainty to\nimprove the model explicitly by active learning. The key step of\nactive learning is to select a set of informative test data to enrich\nthe original training data and update the model [ 54]. The predictive\nuncertainty, as the modelâ€™s belief on its prediction, is an efficient\nand effective selection criterion. The corresponding active learning\nalgorithm, a.k.a., uncertainty sampling [ 39], is to sample from the\nregion of the data which has the most uncertainty regarding the\ncurrent model, request the ground truth for these test data and\nretrain/update the model by original and added data. This process\nis repeated for several iterations under a specified stop condition,\ne.g., a given iteration number or sampling budget. As the existing\nDL-based estimators do not deliver the predictive uncertainty, we\nforest higgs TPC-H TPC-DS1102104\nDatasetElapsed Time (ms)NNGP NN GBDT MSCN TLSTM DeepDB NeuroCard Postgres GP-RBF\n2-34-56-78-92-34-56-78-92-34-56-78-92-34-56-78-910âˆ’1010âˆ’5110510101000 2000 4000 8000\n# of Selection Conditions (Test)q-error(a) Varying the number of training queries for forest\n2-34-56-78-92-34-56-78-92-34-56-78-92-34-56-78-910âˆ’810âˆ’411041082 : 8 4 : 6 6 : 4 8 : 2\n# of Selection Conditions (Test)q-error\n(b) Varying the fraction of predicate size for forest\n012301230123012310âˆ’1010âˆ’5110510101000 2000 4000 8000\n# of Joins (Test)q-error\n(c) Varying the number of training queries for TPC-H\n012301230123012310âˆ’810âˆ’411041082 : 8 4 : 6 6 : 4 8 : 2\n# of Joins (Test)q-error\n(d) Varying the fraction of join size for TPC-H\nFigure 9: Robustness to Various Training Workloads\nused the two uncertainty-aware DL-based estimators introduced in\nÂ§5,DeepEns andBNN-MCD , as the baselines.\nTo conduct active learning, the entire queries are split into 40%\nfor training an original base estimator, 20% for testing and 40% as a\nselection pool. The base DeepEns (5NNestimators for the ensem-\nble) and BNN-MCD are trained by 50 epochs. We apply 3 iterations\nof uncertainty sampling where each iteration 1,000 queries are\ndrawn without replacement regarding the coefficient of variation.\nDeepEns andBNN-MCD are updated on their base model, and NNGP\nis retrained from scratch. Table 3 presents the mean-squared-error\n(MSE) (Eq. (21)) for the base model and those after the 3 iterations,\non the test query set. NNGP and the two DL baselines are able to\nimprove themselves introspectively via uncertainty-based active\nlearning. Among them NNGP achieves the lowest test MSE. The MSE\nofNNGP andDeepEns are consistently reduced in the 3 iterations\n\nforest higgs TPC-H TPC-DS1102104\nDatasetElapsed Time (ms)NNGP DeepEns BNN-MCD NN GBDT MSCN TLSTM DeepDB NeuroCard GP-RBF\n2-34-56-78-92-34-56-78-92-34-56-78-92-34-56-78-910âˆ’1010âˆ’511051010Origin Iter. 1 Iter. 2 Iter. 3\n# of Selection Conditions (Test)q-error(a)forest\n012301230123012310âˆ’810âˆ’41104108Origin Iter. 1 Iter. 2 Iter. 3\n# of Joins (Test)q-error\n(b)TPC-H\nFigure 10: q-error of 3 Iterations of Active Learning\nalthough the MSE of NNGP is already small. A close observation on\nthe prediction accuracy of different forest andTPC-Hqueries is\nshown in Fig. 10. Due to the neural network nature, the performance\nofBNN-MCD andDeepEns improves significantly as more queries\nare incorporated into the training set. As the test MSE of origin\nNNGP is small, its q-error reduction is not obvious compared with\nBNN-MCD andDeepEns . But we still can observe an improvement\non the queries with 2-3 joins in Fig. 10(b). The model updating has\nmarginal effect for all the three estimators.\n7 Related Work\nClassical Cardinality Estimators. Cardinality/Selectively esti-\nmation of relational queries is studied over decades in the database\narea. Early approaches propose multi-dimensional histograms to\nrepresent the joint probability distribution [ 21,51,52], where the\nassumption of attribute value independence is not necessary to be\nheld. For join queries, join sampling algorithms [ 5,23,40,69] are\ndesigned, which can be used to approximate cardinality and query\nresults. These approaches face the risk of sampling failure in the in-\ntermediate join step, when the data distribution is complex. [ 20,59]\nuse Bayesian Network to estimate cardinality by modeling the joint\nprobability distribution of all the attributes. Structural learning is\nneeded to explore a well-structured DAG to represent potential\nconditional independence of attribute. Note Bayesian Network is\ndifferent from Bayesian neural network. The former is a proba-\nbilistic graphical model based on the conditional independence\nassumption whereas the latter is the Bayesian DL model.\nML/DL for Cardinality Estimation and AQP. ML/DL models\nare exploited to perform cardinality estimation and AQP for RDBMS .\nWe briefly review ML/DL approaches in Table 1. DBEst [42] builds\nkernel density estimation (KDE) models and tree-based regressors\nto conduct AQP. DeepDB [25] adopts Sum-Product Networks (SPNs)\nto learn the joint probability distribution of attributes. An SQL query\nis complied to a product of expectations or probability queries onthe SPNs, where the product is based on independence of the at-\ntributes. [ 58] uses deep generative model, e.g., Variational Autoen-\ncoder (VAE) to model the joint probability distribution of attributes,\nwhich only support analytical aggregate query on a single table.\n[31] estimates multivariate probability distributions of a relation\nto perform cardinality estimation by KDE. Multiple joins can be\nestimated by building the KDE estimator on pre-computed join\nresult or an estimation formula that leverages samples from the\nmultiple relations as well as the models. Deep autoregressive model,\ne.g., Masked Autoencoder (MADE), is also adopted to learn the joint\nprobability distribution [ 24,67], which decomposes the joint distri-\nbution to a product of conditional distributions. Kipf et al. propose\na multi-set convolutional neural network (MSCN) to express query\nfeatures using sets [ 32]. Anshuman et al. [ 14] use MLP and tree-\nbased regressor to express multiple attributes range queries. Sun et\nal. [56] extract the features of physical query plan by Tree LSTM\nto estimate the query execution cost as well as the cardinality.\nML/DL for Databases. In recent years, with the development of\nML/DL techniques, ML/DL models are exploited to support multiple\ndatabase applications. Various types of ML/DL models serve as a\ncost estimator of algorithms or query plans, which are to support\napplications of data partitioning [ 15], index recommendation [ 9]\nand concurrency control [ 70]. [10,33,44,47] propose learned in-\ndex structures, which learn a cumulative distribution function of\nthe underlying data. [ 34,44,45] design end-to-end learning-based\nquery optimizers where [ 34,45] optimize the binary join order\nand [ 44] generates the physical plans. Their approaches reformu-\nlate the dynamical programming problem of query optimization to\nMarkov Decision Process and adopt different reinforcement learn-\ning (RL) algorithms to learn neural network models as the optimizer.\n[2,12,35,68] adopt ML/DL to tune the database configurations,\nwhere GP-based Bayesian optimization and RL algorithms are used\nrespectively to conduct an online tuning in [ 2,12] and [ 35,68].\nIt is worth mentioning that this paper is the first exploration of\nBayesian DL in the database area.\n8 Conclusion\nIn this paper, we explore a new simple yet effective NNGP estimator\nto estimate cardinality of SQL queries. We compare it with 7 baseline\nestimators over 4 relational datasets. In terms of accuracy, NNGP\nis one of the top-3 estimators, and performs best in many cases.\nIn terms of efficiency of training, NNGP is 1-2 orders faster than\nthe baseline estimators except GBDT. GBDT is marginally more\nefficient than NNGP but has a low prediction accuracy. In terms\nof uncertainty, NNGP can consistently improve its accuracy by\nuncertainty sampling via active learning, whereas BNN cannot do\nso. And it achieves a much smaller prediction error comparing to\nDeep Ensemble and BNN. In addition, NNGP is supremely robust on\nvarious workloads, and can be learned with much fewer training\nqueries. Our source code is public available in https://github.com/\nKangfei/NNGP.\nAcknowledgement\nWe thank Zongheng Yang, the author of NeuroCard [66] for his\nhelp in testing the estimator.\n12\n\nReferences\n[1] Pytorch. https://github.com/pytorch/pytorch.\n[2]D. V. Aken, A. Pavlo, G. J. Gordon, and B. Zhang. Automatic database management\nsystem tuning through large-scale machine learning. In Proc. SIGMODâ€™17 , pages\n1009â€“1024, 2017.\n[3]C. M. Bishop. Pattern recognition and machine learning, 5th Edition . Information\nscience and statistics. Springer, 2007.\n[4]J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Nec-\nula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: compos-\nable transformations of Python+NumPy programs, 2018.\n[5] W. Cai, M. Balazinska, and D. Suciu. Pessimistic cardinality estimation: Tighter\nupper bounds for intermediate join cardinalities. In Proc. SIGMODâ€™19 , pages\n18â€“35, 2019.\n[6]J. Chen, M. Stern, M. J. Wainwright, and M. I. Jordan. Kernel feature selection via\nconditional covariance minimization. In Proc. NIPSâ€™19 , pages 6946â€“6955, 2017.\n[7]T. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. In Proc.\nSIGKDDâ€™16 , pages 785â€“794, 2016.\n[8]Y. Cho and L. K. Saul. Kernel methods for deep learning. In Proc. NIPSâ€™09 , pages\n342â€“350, 2009.\n[9]B. Ding, S. Das, R. Marcus, W. Wu, S. Chaudhuri, and V. R. Narasayya. AI meets\nAI: leveraging query executions to improve index recommendations. In Proc.\nSIGMODâ€™19 , pages 1241â€“1258, 2019.\n[10] J. Ding, U. F. Minhas, J. Yu, C. Wang, J. Do, Y. Li, H. Zhang, B. Chandramouli,\nJ. Gehrke, D. Kossmann, D. B. Lomet, and T. Kraska. ALEX: an updatable adaptive\nlearned index. In Proc. SIGMODâ€™20 , pages 969â€“984, 2020.\n[11] D. Dua and C. Graff. UCI machine learning repository, 2017.\n[12] S. Duan, V. Thummala, and S. Babu. Tuning database configuration parameters\nwith ituned. Proc. VLDB Endow. , 2(1):1246â€“1257.\n[13] A. Dutt, C. Wang, V. Narasayya, and S. Chaudhuri. Efficiently approximating\nselectivity functions using low overhead regression models. Proc. VLDB Endow. ,\n13(12):2215â€“2228, 2020.\n[14] A. Dutt, C. Wang, A. Nazi, S. Kandula, V. R. Narasayya, and S. Chaudhuri. Se-\nlectivity estimation for range predicates using lightweight models. Proc. VLDB ,\n12(9):1044â€“1057, 2019.\n[15] W. Fan, R. Jin, M. Liu, P. Lu, X. Luo, R. Xu, Q. Yin, W. Yu, and J. Zhou. Application\ndriven graph partitioning. In Proc. SIGMODâ€™20 , pages 1765â€“1779, 2020.\n[16] Y. Gal and Z. Ghahramani. Bayesian convolutional neural networks with bernoulli\napproximate variational inference. CoRR , abs/1506.02158, 2015.\n[17] Y. Gal and Z. Ghahramani. Dropout as a bayesian approximation: Representing\nmodel uncertainty in deep learning. In Proc. ICMLâ€™16 , volume 48, pages 1050â€“1059,\n2016.\n[18] A. Garriga-Alonso, C. E. Rasmussen, and L. Aitchison. Deep convolutional\nnetworks as shallow gaussian processes. In Proc. ICLRâ€™19 , 2019.\n[19] M. Germain, K. Gregor, I. Murray, and H. Larochelle. MADE: masked autoencoder\nfor distribution estimation. In Proc. ICMLâ€™15 , volume 37, pages 881â€“889, 2015.\n[20] L. Getoor, B. Taskar, and D. Koller. Selectivity estimation using probabilistic\nmodels. In Proc. SIGMODâ€™01 , pages 461â€“472, 2001.\n[21] D. Gunopulos, G. Kollios, V. J. Tsotras, and C. Domeniconi. Selectivity estimators\nfor multidimensional range queries over real attributes. VLDB J. , 14(2):137â€“154,\n2005.\n[22] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural\nnetworks. In Proc. ICMLâ€™17 , volume 70, pages 1321â€“1330. PMLR, 2017.\n[23] P. J. Haas and J. M. Hellerstein. Ripple joins for online aggregation. ACM SIGMOD\nRecord , 28(2):287â€“298, 1999.\n[24] S. Hasan, S. Thirumuruganathan, J. Augustine, N. Koudas, and G. Das. Deep\nlearning models for selectivity estimation of multi-attribute queries. In Proc.\nSIGMODâ€™20 , pages 1035â€“1050, 2020.\n[25] B. Hilprecht, A. Schmidt, M. Kulessa, A. Molina, K. Kersting, and C. Binnig.\nDeepdb: Learn from data, not from queries! Proc. VLDB , 13(7):992â€“1005, 2020.\n[26] K. Hornik. Approximation capabilities of multilayer feedforward networks.\nNeural Networks , 4(2):251â€“257, 1991.\n[27] K. Hornik, M. B. Stinchcombe, and H. White. Universal approximation of an\nunknown mapping and its derivatives using multilayer feedforward networks.\nNeural Networks , 3(5):551â€“560, 1990.\n[28] J. Hron, Y. Bahri, J. Sohl-Dickstein, and R. Novak. Infinite attention: NNGP and\nNTK for deep attention networks. In Proc. ICMLâ€™20 , volume 119, pages 4376â€“4386.\nPMLR, 2020.\n[29] J. Hu, J. Shen, B. Yang, and L. Shao. Infinitely wide graph convolutional networks:\nSemi-supervised learning via gaussian processes. CoRR , abs/2002.12168, 2020.\n[30] F. Hutter, L. Kotthoff, and J. Vanschoren. Automated machine learning: methods,\nsystems, challenges . Springer Nature, 2019.\n[31] M. Kiefer, M. Heimel, S. BreÃŸ, and V. Markl. Estimating join selectivities using\nbandwidth-optimized kernel density models. Proc. VLDB , 10(13):2085â€“2096, 2017.\n[32] A. Kipf, T. Kipf, B. Radke, V. Leis, P. A. Boncz, and A. Kemper. Learned cardinalities:\nEstimating correlated joins with deep learning. In Proc. CIDRâ€™19 , 2019.\n[33] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned\nindex structures. In Proc. SIGMODâ€™18 , pages 489â€“504, 2018.[34] S. Krishnan, Z. Yang, K. Goldberg, J. M. Hellerstein, and I. Stoica. Learning to\noptimize join queries with deep reinforcement learning. CoRR , abs/1808.03196,\n2018.\n[35] M. Kunjir and S. Babu. Black or white? how to develop an autotuner for memory-\nbased analytics. In Proc. SIGMODâ€™20 , pages 1667â€“1683, 2020.\n[36] B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive\nuncertainty estimation using deep ensembles. In Proc. NIPSâ€™17 , pages 6402â€“6413,\n2017.\n[37] H. K. Lee. Bayesian nonparametrics via neural networks . SIAM, 2004.\n[38] J. Lee, Y. Bahri, R. Novak, S. S. Schoenholz, J. Pennington, and J. Sohl-Dickstein.\nDeep neural networks as gaussian processes. In Proc. ICLRâ€™18 , 2018.\n[39] D. D. Lewis and J. Catlett. Heterogeneous uncertainty sampling for supervised\nlearning. In Proc. ICMLâ€™94 , pages 148â€“156, 1994.\n[40] F. Li, B. Wu, K. Yi, and Z. Zhao. Wander join: Online aggregation via random\nwalks. In Proc. SIGMODâ€™16 , pages 615â€“629, 2016.\n[41] X. Liang, A. J. Elmore, and S. Krishnan. Opportunistic view materialization with\ndeep reinforcement learning. CoRR , abs/1903.01363, 2019.\n[42] Q. Ma and P. Triantafillou. Dbest: Revisiting approximate query processing\nengines with machine learning models. In Proc. SIGMODâ€™19 , pages 1553â€“1570,\n2019.\n[43] D. J. MacKay. Introduction to gaussian processes. NATO ASI series F computer\nand systems sciences , 168:133â€“166, 1998.\n[44] R. Marcus, E. Zhang, and T. Kraska. Cdfshop: Exploring and optimizing learned\nindex structures. In Proc. SIGMODâ€™20 , pages 2789â€“2792, 2020.\n[45] R. C. Marcus, P. Negi, H. Mao, C. Zhang, M. Alizadeh, T. Kraska, O. Papaem-\nmanouil, and N. Tatbul. Neo: A learned query optimizer. Proc. VLDB Endow. ,\n12(11):1705â€“1718, 2019.\n[46] A. Nath and P. M. Domingos. Learning relational sum-product networks. In Proc.\nAAAIâ€™15 , pages 2878â€“2886, 2015.\n[47] V. Nathan, J. Ding, M. Alizadeh, and T. Kraska. Learning multi-dimensional\nindexes. In Proc. SIGMODâ€™20 , pages 985â€“1000, 2020.\n[48] R. M. Neal. Priors for infinite networks. In Bayesian Learning for Neural Networks ,\npages 29â€“53. Springer, 1996.\n[49] R. Novak, L. Xiao, Y. Bahri, J. Lee, G. Yang, J. Hron, D. A. Abolafia, J. Pennington,\nand J. Sohl-Dickstein. Bayesian deep convolutional networks with many channels\nare gaussian processes. In Proc. ICLRâ€™19 , 2019.\n[50] R. Novak, L. Xiao, J. Hron, J. Lee, A. A. Alemi, J. Sohl-Dickstein, and S. S. Schoen-\nholz. Neural tangents: Fast and easy infinite neural networks in python. In Proc.\nICLRâ€™20 , 2020.\n[51] V. Poosala and Y. E. Ioannidis. Selectivity estimation without the attribute value\nindependence assumption. In Proc. VLDBâ€™97 , pages 486â€“495, 1997.\n[52] V. Poosala, Y. E. Ioannidis, P. J. Haas, and E. J. Shekita. Improved histograms for\nselectivity estimation of range predicates. In Proc. SIGMOD , pages 294â€“305, 1996.\n[53] C. E. Rasmussen and C. K. I. Williams. Gaussian processes for machine learning .\nMIT Press, 2006.\n[54] B. Settles. Active Learning . Synthesis Lectures on Artificial Intelligence and\nMachine Learning. Morgan & Claypool Publishers, 2012.\n[55] L. Song, A. J. Smola, A. Gretton, J. Bedo, and K. M. Borgwardt. Feature selection\nvia dependence maximization. J. Mach. Learn. Res. , 13:1393â€“1434, 2012.\n[56] J. Sun and G. Li. An end-to-end learning-based cost estimator. Proc. VLDB ,\n13(3):307â€“319, 2019.\n[57] K. S. Tai, R. Socher, and C. D. Manning. Improved semantic representations\nfrom tree-structured long short-term memory networks. In Proc. ACLâ€™15 , pages\n1556â€“1566, 2015.\n[58] S. Thirumuruganathan, S. Hasan, N. Koudas, and G. Das. Approximate query\nprocessing for data exploration using deep generative models. In Proc. ICDEâ€™20 ,\npages 1309â€“1320, 2020.\n[59] K. Tzoumas, A. Deshpande, and C. S. Jensen. Lightweight graphical models for\nselectivity estimation without independence assumptions. Proc. VLDB Endow. ,\n4(11):852â€“863, 2011.\n[60] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser,\nand I. Polosukhin. Attention is all you need. In Proc. NeurIPSâ€™17 , pages 5998â€“6008,\n2017.\n[61] H. Wang and D. Yeung. Towards bayesian deep learning: A framework and some\nexisting methods. IEEE Trans. Knowl. Data Eng. , 28(12):3395â€“3408, 2016.\n[62] X. Wang, C. Qu, W. Wu, J. Wang, and Q. Zhou. Are we ready for learned\ncardinality estimation? CoRR , abs/2012.06743, 2020.\n[63] C. K. I. Williams. Computation with infinite neural networks. Neural Comput. ,\n10(5):1203â€“1216, 1998.\n[64] A. G. Wilson and P. Izmailov. Bayesian deep learning and a probabilistic perspec-\ntive of generalization. In Proc. NeurIPSâ€™20 , 2020.\n[65] G. Yang. Wide feedforward or recurrent neural networks of any architecture are\ngaussian processes. In Proc. NeurIPSâ€™19 , pages 9947â€“9960, 2019.\n[66] Z. Yang, A. Kamsetty, S. Luan, E. Liang, Y. Duan, P. Chen, and I. Stoica. Neurocard:\nOne cardinality estimator for all tables. Proc. VLDB Endow. , 14(1):61â€“73, 2020.\n[67] Z. Yang, E. Liang, A. Kamsetty, C. Wu, Y. Duan, P. Chen, P. Abbeel, J. M. Hellerstein,\nS. Krishnan, and I. Stoica. Deep unsupervised cardinality estimation. Proc. VLDB ,\n13(3):279â€“292, 2019.\n13\n\n[68] J. Zhang, Y. Liu, K. Zhou, G. Li, Z. Xiao, B. Cheng, J. Xing, Y. Wang, T. Cheng,\nL. Liu, M. Ran, and Z. Li. An end-to-end automatic cloud database tuning system\nusing deep reinforcement learning. In Proc. SIGMODâ€™19 , pages 415â€“432, 2019.[69] Z. Zhao, R. Christensen, F. Li, X. Hu, and K. Yi. Random sampling over joins\nrevisited. In Proc. SIGMODâ€™18 , pages 1525â€“1539, 2018.\n[70] X. Zhou, J. Sun, G. Li, and J. Feng. Query performance prediction for concurrent\nqueries using graph embedding. Proc. VLDB Endow. , 13(9):1416â€“1428, 2020.",
  "textLength": 80383
}