{
  "paperId": "aa293f214db7f0d830e26543a4af6c69e8088c2f",
  "title": "WISK: A Workload-aware Learned Index for Spatial Keyword Queries",
  "pdfPath": "aa293f214db7f0d830e26543a4af6c69e8088c2f.pdf",
  "text": "187WISK: A Workload-aware Learned Index for Spatial\nKeyword Queries\nYUFAN SHENG, University of New South Wales, Australia\nXIN CAOâˆ—,University of New South Wales, Australia\nYIXIANG FANG, The Chinese University of Hong Kong, Shenzhen, China\nKAIQI ZHAO, The University of Auckland, New Zealand\nJIANZHONG QI, The University of Melbourne, Australia\nGAO CONG, Nanyang Technological University, Singapore\nWENJIE ZHANG, University of New South Wales, Australia\nSpatial objects often come with textual information, such as Points of Interest (POIs) with their descriptions,\nwhich are referred to as geo-textual data. To retrieve such data, spatial keyword queries that take into account\nboth spatial proximity and textual relevance have been extensively studied. Existing indexes designed for\nspatial keyword queries are mostly built based on the geo-textual data without considering the distribution of\nqueries already received. However, previous studies have shown that utilizing the known query distribution\ncan improve the index structure for future query processing. In this paper, we propose WISK, a learned index\nfor spatial keyword queries, which self-adapts for optimizing querying costs given a query workload. One\nkey challenge is how to utilize both structured spatial attributes and unstructured textual information during\nlearning the index. We first divide the data objects into partitions, aiming to minimize the processing costs of\nthe given query workload. We prove the NP-hardness of the partitioning problem and propose a machine\nlearning model to find the optimal partitions. Then, to achieve more pruning power, we build a hierarchical\nstructure based on the generated partitions in a bottom-up manner with a reinforcement learning-based\napproach. We conduct extensive experiments on real-world datasets and query workloads with various\ndistributions, and the results show that WISK outperforms all competitors, achieving up to 8 Ã—speedup in\nquerying time with comparable storage overhead.\nCCS Concepts: â€¢Information systems â†’Location based services ;Query optimization .\nAdditional Key Words and Phrases: learned index, geo-textual data, spatial keyword query\nACM Reference Format:\nYufan Sheng, Xin Cao, Yixiang Fang, Kaiqi Zhao, Jianzhong Qi, Gao Cong, and Wenjie Zhang. 2023. WISK:\nA Workload-aware Learned Index for Spatial Keyword Queries. Proc. ACM Manag. Data 1, 2, Article 187\n(June 2023), 28 pages. https://doi.org/10.1145/3589332\nâˆ—Xin Cao is the corresponding author.\nAuthorsâ€™ addresses: Yufan Sheng, University of New South Wales, Sydney, Australia, yufan.sheng@unsw.edu.au; Xin Cao,\nUniversity of New South Wales, Sydney, Australia, xin.cao@unsw.edu.au; Yixiang Fang, The Chinese University of Hong\nKong, Shenzhen, Shenzhen, China, fangyixiang@cuhk.edu.cn; Kaiqi Zhao, The University of Auckland, Auckland, New\nZealand, kaiqi.zhao@auckland.ac.nz; Jianzhong Qi, The University of Melbourne, Melbourne, Australia, jianzhong.qi@\nunimelb.edu.au; Gao Cong, Nanyang Technological University, Singapore, Singapore, gaocong@ntu.edu.sg; Wenjie Zhang,\nUniversity of New South Wales, Sydney, Australia, wenjie.zhang@unsw.edu.au.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\nÂ©2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n2836-6573/2023/6-ART187 $15.00\nhttps://doi.org/10.1145/3589332\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.arXiv:2302.14287v2  [cs.DB]  14 Apr 2023\n\n187:2 Yufan Sheng et al.\nKeywords: and \nO4{Hotel, Pool}\nO1 {Park, Pool}\nO3\n{\nPizza\n, Food}\nO2\n{BBQ, Food}\n(a) Data-driven index\nO1{Park, Pool}O4{Hotel, Pool}\nO3\n{Pizza, Food}\nO2\n{BBQ, Food} (b) Query-aware index\nFig. 1. Data-driven indexes vs query-aware indexes\n1 INTRODUCTION\nThe worldwide mobile internet use has already surpassed desktop use since late 20161. In such a\nmobile internet, massive geo-textual data with spatial and textual attributes are generated, e.g.,\nPoints of Interest (POIs) in Google Maps are associated with geo-locations and descriptive texts.\nManaging and retrieving geo-textual data at scale has attracted much attention. In recent years, the\nspatial keyword queries [7,12,13,15,16,20,26,58,74] have been extensively studied, which\ntake a location and a set of keywords as arguments and return objects based on different definitions\nof spatial proximity and textual relevance. Spatial keyword indexes are developed to process\nsuch queries efficiently by incorporating the techniques of indexing spatial objects and documents.\nHowever, state-of-the-art spatial keyword indexes still have several drawbacks. First, as shown\nin previous studies [ 10,12,41], no existing index can work efficiently for all data distributions,\nand there is no single approach that dominates all others. Second, traditional indexes may have\nsome parameters to be set with fixed values across the entire input data space. For example, CDIR-\ntree [ 15] needs a parameter to balance the importance of spatial proximity and textual relevancy.\nSince the query and data distributions vary in different areas, it is hard to select a single parameter\nvalue that fits all distributions. Third, no indexes have considered utilizing the query workload.\nPrevious works [ 6,55] have shown that certain data regions could be much more heavily queried.\nFor spatial keyword queries, both the query keyword distribution and query location distribution\ncan be various over different spatial regions. Thus, utilizing the known query distribution could\nfurther optimize the index structure for future query processing [ 18,45]. For example, as shown in\nFigure 1, a spatial keyword query workload includes ğ‘1andğ‘2with the same query region (red\nrectangle) and different keywords. Existing data-driven indexes (Figure 1a) store objects with close\nspatial distances and large textual similarities into a partition (enclosed by blue rectangles), and\nboth queries have to check two partitions containing four objects. If the four objects are grouped\nin a different way to adapt for the query keywords, both queries only need to check two objects\nin a single partition. Query-aware index can be expected to achieve better performance on future\nqueries following similar known query distributions.\nMotivated by these observations, in this work, we propose a novel Workload-aware learned\nIndex for Spatial Keyword queries (WISK). The objective is to learn an index structure using both\nthe spatial and textual information such that the processing cost for the known query workload\nusing this index is minimized. We focus on the spatial keyword range query workload.\nThere exist some learned indexes for spatial query processing, which can also be classified\ninto data-driven indexes (such as ZM [ 62], LISA [ 38], and RSMI [ 51]) and query-aware indexes\n(such as Flood [ 45] and Tsunami [ 18]). These learned spatial index structures are not suitable for\nprocessing spatial keyword queries directly, because they only use spatial attributes for index\nlearning. Concurrent with our work, a learned index has been proposed for spatial keyword\n1Mobile vs. Desktop Internet Usage: https://www.broadbandsearch.net/blog/mobile-desktop-internet-usage-statistics/\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\nWISK: A Workload-aware Learned Index for Spatial Keyword Queries 187:3\nO5{Food}\nO6 {Food}\nO2{Food} O1{Food}\nO3{Store}O4{Store}O8\n{Food}O7{Store }{Store}\n{Food}\n(a) Learned spatial index\nO5{Food}\nO6{Food}\nO2{Food} O1{Food}\nO3{Store}O4{Store}O8\n{Food}O7{Store }{Store}\n{Food} (b) WISK\nFig. 2. Learning without and with textual information\nquerying [ 19]. It uses spatial attributes for index learning first and then creates the textual index,\nand thus it cannot well learn the spatial and textual correlations for building the index.\nThe key challenge of learning a spatial keyword index is how to capture the data and query\ndistribution considering both the structured spatial information and unstructured textual attributes\nduring index learning, and make use of the distribution captured to partition the data objects such\nthat more irrelevant partitions can be filtered out during query processing, thus improving the\nquerying efficiency. One simple method is to learn a spatial index first, which does not consider\nkeywords, and then build an inverted file to manage the textual information within each partition\nof the index. Our experiments show that this has poor performance. The reason could be briefly\nexplained in Figure 2, with a dataset containing 8 objects and a workload with 3 queries. The learned\nspatial indexes put fewer objects into the left partition which contains more queries to reduce costs.\nHowever, such partitions are worse than that in Figure 2b, when objects without query keywords\ncan be ignored during query processing using inverted files. Figure 2a has 3 query-relevant objects\n(Store ) in the left partition and 4 query-relevant objects ( Food ) in the right partition, and thus the\nnumber of checks required is 2Ã—3+1Ã—4=10. But Figure 2b only needs 2Ã—3+1Ã—2=8checks.\nIt can be observed that the query cost largely depends on how the objects are partitioned. We first\nformulate a problem of finding ğ‘˜partitions with the optimal cost for a given query workload. We\nshow the NP-hardness of this problem by a reduction from the MaxSkip partitioning problem [ 56,73].\nTo learn good partitions, we design a cost estimation method considering both spatial and textual\ninformation of the query and data, based on trained models that can approximate the Cumulative\nDistribution Function (CDF) of geo-textual data. Then, we propose a heuristic algorithm and use\nStochastic Gradient Descent (SGD) [53] to generate the ğ‘˜partitions.\nIf there is only one-level of partitions, we need to check many partitions irrelevant to the query.\nHence, we further group the partitions into a hierarchy as do most indexes. A simple way is to adopt\nthe method in CDIR-tree [ 15] for building the tree in a bottom-up manner. However, it is hard to\nselect the weights of the spatial proximity and textual relevance between partitions. We show that\nit might even have worse query time in experiments. Instead, we propose to pack the nodes level\nby level, and model the one-level node packing problem as a sequential decision-making process.\nIn particular, we develop a reinforcement learning [ 29] algorithm to find the optimal packing for\neach level and build the index in a bottom-up manner by considering the query workload.\nIn summary, we make the following main contributions:\nâ€¢We propose a query-aware learned index named WISK considering spatial and textual\nattributes simultaneously.\nâ€¢To generate the leaf nodes of WISK, we define an optimal partitioning problem and show its\nNP-hardness. We propose a heuristic algorithm to solve the problem using machine learning\ntechniques.\nâ€¢To build the hierarchy of WISK, we propose to pack the nodes level by level in a bottom-up\nmanner, and we treat the node packing as a sequential decision-making process. We develop\na solution based on reinforcement learning.\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\n187:4 Yufan Sheng et al.\nâ€¢We perform a comprehensive empirical study using real-world datasets and synthetic query\nworkloads with various distributions. The results show that WISK outperforms the state-of-\nthe-art spatial keyword indexes consistently in terms of efficiency, achieving up to 8 Ã—times\nspeedup while having a comparable index size.\n2 PRELIMINARIES\n2.1 Problem Statement\nWe consider a geo-textual dataset ğ·where each data object, i.e., a geo-textual object ğ‘œâˆˆğ·, has a\npoint location denoted as ğ‘œ.ğ‘™ğ‘œğ‘ and a text description denoted as ğ‘œ.ğ‘˜ğ‘¤ğ‘  , which is available from a\nrange of source [ 14]. For ease of discussion, we assume two-dimensional coordinates in Euclidean\nspace to represent ğ‘œ.ğ‘™ğ‘œğ‘ , although our proposed techniques can generalize to multi-dimensional\nspaces easily. The text description ğ‘œ.ğ‘˜ğ‘¤ğ‘  is represented as a set of keywords, e.g., tags indicating\nthe functionality of a POI. We aim to process spatial keyword range queries overğ·.\nDefinition 1 ( Spatial Keyword Range (SKR) Query ).An SKR query q is represented by a\npair(ğ‘.ğ‘ğ‘Ÿğ‘’ğ‘,ğ‘.ğ‘˜ğ‘’ğ‘¦ğ‘ )where q.area and q.keys denote a spatial region and a set of keywords, respectively.\nThe result of q, ğ‘(ğ·)={ğ‘œâˆˆğ·|ğ‘œ.ğ‘™ğ‘œğ‘ğ‘–ğ‘›ğ‘.ğ‘ğ‘Ÿğ‘’ğ‘,ğ‘œ.ğ‘˜ğ‘¤ğ‘  âˆ©ğ‘.ğ‘˜ğ‘’ğ‘¦ğ‘  â‰ âˆ…}, is a subset of D that includes\nall objects within the query region containing at least one query keyword.\nHere, we use a rectangular query region. Our techniques can be easily extended to handle other\nshapes (e.g., circles) by an extra filtering after querying with the bounding rectangle.\nProblem. Our goal is to learn an index structure that can efficiently process SKR queries utilizing\nthe distributions of the geo-textual data and the given query workload.\n2.2 Reinforcement Learning\nReinforcement learning (RL) [ 29] is a machine learning technique where an agent learns from\nfeedback obtained from trial-and-error interactions with an environment. It has been shown to be\neffective for sequential decision-making problems [54, 78].\nRL formulation is based on the Markov Decision Process (MDP) [ 50]. An MDP has four compo-\nnents: a set of states S, a set of actions A, transition probabilities ğ‘ƒ, and rewards ğ‘…. At some state\nğ‘ âˆˆğ‘†, an agent may take an action ğ‘âˆˆğ´. As a result, there is a probability ğ‘ƒğ‘(ğ‘ ,ğ‘ â€²)that the agent\ntransits to state ğ‘ â€², and a reward ğ‘…ğ‘(ğ‘ ,ğ‘ â€²)is received from such an action and state transition. The\ngoal of the agent is to learn a policy function ğœ‹:ğ´Ã—ğ‘†â†’[0,1], i.e., the probability of taking action\nğ‘at stateğ‘ , such that the cumulative reward of state transitions is maximized. Figure 3 shows the\nbasic workflow of RL. The environment connects to the agent via perception and action and it offers\nthe agent the possible action choices based on the current state of the agent. The agent learns its\npolicy based on rewards accumulated from interactions with the environment. Its learning process\nstops when a terminal state is reached.\nAgent EnvironmentState\nActionReward Policy\nFig. 3. The typical RL learning framework\nQ-learning [ 67] is a commonly used value-based policy learning algorithm, which learns the\nvalue of an action given a state. It learns a policy that maximizes the value of a so-called Q-function,\nğ‘„(ğ‘ ,ğ‘), i.e., the overall expected reward when an agent plays following the policy [ 43]. State-of-\nthe-art RL models such as Deep-Q-Network (DQN) [ 44] use a deep neural network ğ‘„(ğ‘ ,ğ‘;ğœƒ)with\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\nWISK: A Workload-aware Learned Index for Spatial Keyword Queries 187:5\nparameters ğœƒto estimate the value of the Q-function ğ‘„(ğ‘ ,ğ‘). Onceğ‘„(ğ‘ ,ğ‘;ğœƒ)is trained, it can be\nused for decision-making for future events.\n3 INDEX OVERVIEW\nWISK consists of two parts: (1) learn an optimal data layout for the given query workload, and (2)\ncreate an index based on that layout.\nQuery processing on an index that partitions objects into clusters typically involves two costly\noperations: filtering and verification. Irrelevant partitions are filtered out, and objects in the\nremaining partitions are verified. Thus, in WISK we first aim to learn an optimal partition of the\ngeo-textual objects, such that for the given SKR query workload we can achieve the minimum\nquery processing costs computed using both the filtering and verification costs. However, given\na large dataset, the number of possible partitions of the objects is extremely huge, and it is hard\nto learn an optimal partition. We propose to simplify the problem: we divide the 2D space into\ndisjoint partitions to obtain an optimal spatial layout. We will give the detail in Section 4.\nIf there is only one layer of the index, we need to check all partitions to see if they are relevant\nto the query, leading to a high filtering cost. We can organize the partitions obtained in the first\nstep into a tree structure to build the final index such that the query processing cost can be further\noptimized. This is a type of combinatorial optimization task. We propose to pack the partitions\nlevel by level, and view the one-level packing problem as a sequential decision-making process,\nwhich can solved by reinforcement learning. We will present the detail of this step in Section 5.\nThe framework of WISK is shown in Figure 4. A leaf node contains a number of objects, a\nminimum bounding rectangle (MBR) of the objects, and an inverted file to index the objects in this\nnode. A non-leaf node contains pointers to its child nodes, an MBR for all child nodes, and a bitmap\nto index keywords appearing in its sub-tree. Here, the bitmap is used due to its much smaller size.\nJoint spatial \nand textual \nlearningGeo-textual\nData\nQuery \nWorkloadBottom Clusters\nRL-based\npacking\nHierarchical Indexâ€¦ â€¦ â€¦ â€¦â€¦ â€¦â€¦\nâ€¦\nâ€¦\nFig. 4. The WISK framework\nIndex construction. Algorithm 1 summarizes the construction process of WISK, which consists\nof two main steps. Step 1 (lines 1 and 2) is to construct the bottom clusters. A high-quality partition\nof bottom clusters should result in a low query cost given a query workload. We train machine\nlearning models to approximate the Cumulative Distribution Function (CDF) of geo-textual objects.\nThen, we define the cost estimation function based on the learned CDFs and make it differentiable,\nsuch that we can use stochastic gradient descent (SGD) to learn the optimal partitions. Step 2\n(lines 3 and 4) is to construct the hierarchy of WISK by a bottom-up packing of the bottom clusters.\nOur goal is to minimize the filtering cost when an SKQ query is processed by this hierarchy. We\nmodel the packing process in one level as an MDP and apply RL to solve the problem. Based on the\ngiven query workload, we design a reward to measure the reduction of the filtering cost given a\npacking decision, and we train a model to predict the reward, which can be used to guide packing\nthe bottom clusters level by level. The RL approach can also be used to group objects into bottom\nclusters. However, due to a large number of objects, this will require huge amounts of states during\nthe RL procedure, and both the training time and the performance would be unacceptable [60].\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\n187:6 Yufan Sheng et al.\nAlgorithm 1: WISK Construction\nInput: Q, the workload; D, the dataset; C, the cost function\nOutput: I, a learned WISK index\n1KMâ†MLModel( D) ; /* Learn CDF models of objects */\n2Gâ†SGDPartition( Q,C,KM) ; /* G minimizes C */\n3RMâ†RLTrain( G,Q) ; /* Learn a model RM based on G, Q */\n4Iâ†Packing( RM,G) ; /* Group G by using RM */\n5return I;\nQuery processing. Our query algorithm is similar to those using traditional spatial keyword\nindexes. Given a SKR query ğ‘, it traverses WISK in a breath-first manner starting from the root. A\nqueueğ‘„is used to manage the nodes visited. For every non-leaf node visited, only the child nodes\nwhose MBRs overlap with ğ‘.ğ‘ğ‘Ÿğ‘’ğ‘ and contain some query keywords are added to ğ‘„for future\nverification. When a leaf node (i.e., the bottom cluster) is reached, we use its inverted file to fetch\nthe query-relevant objects and return those in the query region.\n4 PARTITIONING OPTIMIZATION\nA core problem in WISK construction is to form partitions (i.e., bottom clusters) for cost minimization\nover the query workload. In this section, we model the query cost, define an optimal partition\nproblem, show the NP-hardness of the problem, and present a heuristic algorithm for the problem.\n4.1 Cost Model\nWe model the time cost ğ¶(ğ‘ğ‘–)to process an SKR query ğ‘ğ‘–over a set of bottom clusters ğºas a linear\ncombination of (1) the cost to scan all bottom clusters to find a subset ğºğ‘–âŠ‚ğºthat overlap with\nğ‘ğ‘–.ğ‘ğ‘Ÿğ‘’ğ‘ and containing at least one keyword in ğ‘ğ‘–.ğ‘˜ğ‘¤ğ‘ , and (2) the cost to examine the inverted\nfile in each cluster ğ‘âˆˆğºğ‘–and find the objects in ğ‘ğ‘–.ğ‘ğ‘Ÿğ‘’ğ‘ and contain at least one query keyword.\nEq. 1 formalizes the cost, where |ğº|denotes the total number of clusters, andÃ\nğ‘âˆˆğºğ‘–|ğ‘‚ğ‘|denotes\nthe number of objects in ğºğ‘–that contains at least one query keyword. In particular, ğ‘¤1measures\nthe time cost for checking (1) if the MBR of a cluster intersects with the query region, and (2) if the\ncluster contains some query keywords by scanning the textual index of the cluster. Both checks are\nindependent of the cluster size. Meanwhile, ğ‘¤2measures the time cost to perform the same checks\nbut at the object level. Following recent studies [ 18,77], we use fixed values for these parameters.\nğ¶(ğ‘ğ‘–)=ğ‘¤1|ğº|+ğ‘¤2âˆ‘ï¸\nğ‘âˆˆğºğ‘–|ğ‘‚ğ‘| (1)\nExample 4.1: Figure 5 illustrates this cost function. Suppose that the red and green points represent\nobjects that contain keywords ğ‘˜1andğ‘˜2respectively. There are two queries, and ğ‘1.ğ‘˜ğ‘¤ğ‘ ={ğ‘˜1}\nandğ‘2.ğ‘˜ğ‘¤ğ‘ ={ğ‘˜2}. If all objects are in a cluster (i.e., no partitioning, Figure 5a), according to Eq. 1,\nthe two queries incur a cost of 2(ğ‘¤1+4ğ‘¤2)=2ğ‘¤1+8ğ‘¤2. This is because there is only one cluster,\nand each query needs to check four objects containing the query keywords (i.e., four red points for\nğ‘˜1and four green points for ğ‘˜2). If the space is split forming two clusters of five and three points\neach (Figure 5b), the cost of ğ‘2andğ‘1will become 2ğ‘¤1+2ğ‘¤2(checking two clusters and two green\n(a) All objects in a cluster\n (b) Objects in two clusters\nFig. 5. Partitioning the space increases the number of clusters, which leads to a larger cluster scanning cost,\nbut also potentially a lower object scanning cost.\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\nWISK: A Workload-aware Learned Index for Spatial Keyword Queries 187:7\npoints) and 2ğ‘¤1+4ğ‘¤2(checking two clusters and four red points), which sum up to 4ğ‘¤1+6ğ‘¤2. The\npartitioning may lead to an overall lower query cost if ğ‘¤2dominates the cost.\n4.2 The Optimal Partitioning Problem\nWe formulate an optimal partition problem to find a set of clusters that minimize the query cost\nover a given set of queries.\nProblem 1 ( Optimal Partitioning ).Given a dataset D ={ğ‘œ1,ğ‘œ1,...,ğ‘œğ‘›}and a query workload\nW ={ğ‘1,ğ‘2,...,ğ‘ğ‘š}, we aim to find an optimal partition, i.e., a set of k clusters ğº={ğ‘1,ğ‘2,...,ğ‘ğ‘˜}\nwhere (1) each object belongs to exactly one cluster, i.e.,Ã\nğ‘ğ‘–âˆˆğºğ‘ğ‘–=ğ·, andâˆ€ğ‘ğ‘–,ğ‘ğ‘—âˆˆğºğ‘ğ‘–âˆ©ğ‘ğ‘—=âˆ…, and (2)\nthe total cost,Ã\nğ‘ğ‘–âˆˆğ‘Šğ¶(ğ‘ğ‘–), is minimized, where ğ¶(ğ‘ğ‘–)(Eq. 1) is the cost of ğ‘ğ‘–.\n4.2.1 Problem Analysis. We proceed to show that the optimal partitioning problem is NP-hard\nby reducing from the MaxSkip partitioning problem, which has been shown to be NP-hard [ 56,73].\nTheorem 4.1. Problem 1 is NP-hard.\nProof. We first briefly introduce the MaxSkip partitioning problem, which arises from big data\nanalytics systems. Let Qbe a collection of queries. Consider a set of partitions ğ‘ƒ={ğ‘1,ğ‘2,...,ğ‘ğ‘˜}\nwhere each partition is a collection of tuples, and the size of each partition is larger than a minimum\nsize boundğ‘. A big data analytics system can prune a partition ğ‘ğ‘–if none of the tuples in this\npartition satisfies a query ğ‘âˆˆğ‘„when processing ğ‘. A cost function (Eq. 2) can thus be defined on\neach partition, which denotes the number of tuples that can be skipped for processing all queries\ninğ‘„, if such a partition is formed. Here, |ğ‘ğ‘–|denotes the number of tuples in partition ğ‘ğ‘–, andğ‘„ğ‘–\ndenotes the set of queries that can be processed without accessing ğ‘ğ‘–.\nğ¶ğ‘œğ‘ ğ‘¡(ğ‘ğ‘–)=|ğ‘„ğ‘–||ğ‘ğ‘–|,ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ğ‘„ ğ‘–âŠ†ğ‘„ (2)\nThe MaxSkip partitioning problem aims to find the optimal partitions ğ‘ƒğ‘œğ‘ğ‘¡maximizing the total\nnumber of tuples that can be skipped when executing Q, i.e.,ğ‘ƒğ‘œğ‘ğ‘¡=arg maxğ‘ƒÃ\nğ‘ğ‘–âˆˆğ‘ƒğ¶ğ‘œğ‘ ğ‘¡(ğ‘ğ‘–).\nWe map one instance of the MaxSkip partitioning problem to an instance of our optimal parti-\ntioning problem as below: for each query ğ‘ğ‘¤inğ‘„, we create a keyword ğ‘‘ğ‘¤and form a SKR query\nğ‘={ğ‘.ğ¿,ğ‘.ğ‘‘ğ‘¤}whereğ‘.ğ¿is the MBR of the entire space. For each tuple ğ‘¡ğ‘š, we create a geo-textual\nobjectğ‘œğ‘šsuch that its location is in ğ¿, and its keywords correspond to the queries it can satisfy in\nğ‘„.\nGiven this mapping, in the MaxSkip partitioning problem, for a partition ğ‘ğ‘–âˆˆğ‘ƒ, if a set of queries\nğ‘„ğ‘–âŠ†ğ‘„can be skipped when processing ğ‘ğ‘–, we can get a cluster ğ‘ğ‘–in our problem and a set of SKR\nqueriesğ‘…ğ‘–âˆˆğ‘Šthat are irrelevant to ğ‘ğ‘–(since no geo-textual objects in ğ‘ğ‘–contains a keyword in ğ‘…ğ‘–).\nHence, if we could find an optimal partition that maximize the total number of tuples when running\nqueries inğ‘Š, it is equivalent that we can find an optimal partitioning method that minimizes the\ncost in our problem. Since the mapping is of linear time, we complete the proof. â–¡\n4.3 A Heuristic Partition Algorithm\nAs pointed out by Christoforaki et al. [ 13], a query region is usually much smaller than the data\nspace such that many data objects are not queried by the workload ğ‘Š. Hence, to fully utilize\nthe query workload for partitioning, a data based partitioning method is not suitable to solve\nour problem. Instead, we employ a space-disjoint partitioning approach and propose a heuristic\npartition algorithm.\nOur index aims to learn splitting the spatial data space along different dimensions and coordinate\nvalues. Our partition algorithm starts by initializing one single partition that covers the full data\nspace (which corresponds to a cluster that contains the full dataset). At this point, each query\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\n187:8 Yufan Sheng et al.\ncontributes the same ğ‘¤1+|ğ·|Â·ğ‘¤2cost to the overall cost of the query workload ğ‘Š. Then, we find\na split dimension ğ‘‘ğ‘ and a split value ğ‘£ğ‘ that yield the largest reduction in the query cost. We use\nthe resulting ğ‘‘ğ‘ andğ‘£ğ‘ to split the data space into two sub-spaces and update the total query cost.\nFor each sub-space, we repeat the splitting process recursively until the total query cost cannot be\nreduced or some pre-defined conditions, e.g., a minimum number of queries intersecting with the\nsub-space, are met. When the algorithm terminates, we use the MBR of the data objects in each\nresultant sub-space as a bottom cluster in WISK.\n4.3.1 Learning the Split Dimension and Value .A naÃ¯ve method to find the value to make a\nsplit uses a brute-force search. Let ğ‘‰ğ‘‘(ğ‘‘âˆˆ{ğ‘¥,ğ‘¦}) be a sorted list of distinct object coordinate values\nalong dimension ğ‘‘in the current (sub-)space to be partitioned. Except for the first and the last\nvalues, every value in ğ‘‰ğ‘‘can be used to split the space into two sub-spaces. Examining all |ğ‘‰ğ‘‘|\nvalues takesO\u0000ğ‘“Â·(|ğ‘‰ğ‘¥|+|ğ‘‰ğ‘¦|)\u0001whereğ‘“denotes the time cost to split on a value and run queries\nbased on such a splitting This approach becomes impractical for large datasets with a large value\nof|ğ‘‰ğ‘¥|+|ğ‘‰ğ‘¦|.\nMotivated by the recent success of machine learning in solving complex problems [ 4,49], we\npropose a learning-based method to predict the query costs given a split dimension and a split\nvalue, such that the optimal split can be approximated by minimizing the predicted query cost\nwith high efficiency. At the core of the query cost prediction problem of a split is to (1) predict the\nnumber of resultant sub-spaces overlapping with the query, and (2) predict the number of objects\nthat contain any of the query keywords and reside in the resultant sub-spaces.\nTo address the first prediction problem, we use the indicator function [ 33] to denote whether a\nsub-space overlaps with the query region. For example, let [ğ‘ğ‘¥ğ‘,ğ‘ğ‘¥ğ‘¢]be the xrange of query qand\nğ‘ğ‘¥be a split value along dimension x. The indicator functions ğŸ™(ğ‘ğ‘¥â‰¥ğ‘ğ‘¥ğ‘)andğŸ™(ğ‘ğ‘¥<ğ‘ğ‘¥ğ‘¢)are\nused to decide whether ğ‘intersects with the resultant left and right sub-spaces, respectively. If a\nsub-space has an indicator function value of 1, we need to further predict the number of query result\nobjects within the sub-space. Otherwise, we can ignore the sub-space when computing the query\ncost. The indicator function is not differentiable, and machine learning methods such as gradient\ndescent cannot be applied to solve a split value optimization problem formulated by such functions.\nAs such, we use the sigmoid function [ 24],ğœ(ğ›½ğ‘¥)withğ›½=3, to approximate the indicator function\nas does in prior work [8, 9], e.g., ğŸ™(ğ‘ğ‘¥â‰¥ğ‘ğ‘¥ğ‘)=ğŸ™(ğ‘ğ‘¥âˆ’ğ‘ğ‘¥ğ‘â‰¥0)â‰ˆğœ(3(ğ‘ğ‘¥âˆ’ğ‘ğ‘¥ğ‘)).\nTo address the second prediction problem, we follow the idea in recent studies [ 36,37,45] that\nlearn the Cumulative Distribution Function (CDF) to estimate the density of objects in a data space.\nOur goal is to learn the joint CDF ğ¹ğ‘‹,ğ‘Œ(ğ‘¥,ğ‘¦)of two variables XandY, corresponding to the spatial\ncoordinates in two dimensions. The learned CDF can quickly estimate the number of objects in\na rectangular region, i.e., a sub-space. To accelerate the CDF learning, we assume that XandY\nare independent, following a previous study [ 45]. Thus, we can decompose the joint CDF into the\nproduct of two marginal CDFs, ğ¹ğ‘‹(ğ‘¥)andğ¹ğ‘Œ(ğ‘¦), as shown in Eq. 3.\nğ¹ğ‘‹,ğ‘Œ(ğ‘¥,ğ‘¦)=ğ‘ƒ(ğ‘‹â‰¤ğ‘¥,ğ‘Œâ‰¤ğ‘¦)=ğ¹ğ‘‹(ğ‘¥)ğ¹ğ‘Œ(ğ‘¦) (3)\nFor ease of presentation, we use F(x) and F(y) to denote the marginal CDFs of XandYin the rest\nof the paper, respectively.\nLemma 4.2. Given a two-dimensional object (ğ‘¥,ğ‘¦)and a rectangular region [(ğ‘¥ğ‘,ğ‘¦ğ‘),(ğ‘¥ğ‘¢,ğ‘¦ğ‘¢)]\nwhere(ğ‘¥ğ‘,ğ‘¦ğ‘)and(ğ‘¥ğ‘¢,ğ‘¦ğ‘¢)denote the bottom-left and the upper-right points of the rectangular region,\nrespectively, the probability of an object residing in the area is:\nğ‘ƒ(ğ‘¥ğ‘â‰¤ğ‘¥â‰¤ğ‘¥ğ‘¢,ğ‘¦ğ‘â‰¤ğ‘¦â‰¤ğ‘¦ğ‘¢)=\u0000ğ¹(ğ‘¥ğ‘¢)âˆ’ğ¹(ğ‘¥ğ‘)\u0001\u0000ğ¹(ğ‘¦ğ‘¢)âˆ’ğ¹(ğ‘¦ğ‘)\u0001\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\nWISK: A Workload-aware Learned Index for Spatial Keyword Queries 187:9\nProof. According to the definition of CDF, we have ğ‘ƒ(ğ‘¥ğ‘â‰¤ğ‘¥â‰¤ğ‘¥ğ‘¢,ğ‘¦ğ‘â‰¤ğ‘¦â‰¤ğ‘¦ğ‘¢)=ğ¹(ğ‘¥ğ‘¢,ğ‘¦ğ‘¢)âˆ’\nğ¹(ğ‘¥ğ‘,ğ‘¦ğ‘¢)âˆ’ğ¹(ğ‘¥ğ‘¢,ğ‘¦ğ‘)+ğ¹(ğ‘¥ğ‘,ğ‘¦ğ‘). Due to the independence assumption, we can decompose each\njoint CDF based on Eq. 3, and obtain the equation in Lemma 4.2. â–¡\nThe CDF in Eq. 3 only estimates the spatial density of objects without considering the keyword\ndistribution. To solve this issue, we learn the marginal CDFs, i.e., ğ¹ğ‘˜(ğ‘¥)andğ¹ğ‘˜(ğ‘¦), for each keyword\nğ‘˜. The choice of CDF models will be detailed in Section 6.\nWith the CDF models and the sigmoid functions, we formulate the cost for processing a query ğ‘\nwith region[(ğ‘¥ğ‘,ğ‘¦ğ‘),(ğ‘¥ğ‘¢,ğ‘¦ğ‘¢)]after splitting on dimensions ğ‘¥orğ‘¦in Eq. 4.\nğ¿ğ‘(ğ‘¥)=ğœ\u00003(ğ‘¥âˆ’ğ‘¥ğ‘)\u0001|ğ‘‚1|+ğœ\u00003(ğ‘¥ğ‘¢âˆ’ğ‘¥)\u0001|ğ‘‚2|\nğ¿ğ‘(ğ‘¦)=ğœ\u00003(ğ‘¦âˆ’ğ‘¦ğ‘)\u0001|ğ‘‚1|+ğœ\u00003(ğ‘¦ğ‘¢âˆ’ğ‘¦)\u0001|ğ‘‚2|(4)\nwhere|ğ‘‚1|and|ğ‘‚2|denote the number of objects containing the query keywords in the two resulting\nsub-spaces, respectively, which are estimated through the learned keyword-based marginal CDF\nmodels. The sigmoid functions (e.g., ğœ\u00003(ğ‘¥âˆ’ğ‘¥ğ‘)\u0001andğœ\u00003(ğ‘¥ğ‘¢âˆ’ğ‘¥)\u0001) predicts whether the query\nintersects the two resultant sub-spaces, respectively. We apply stochastic gradient descent (SGD) to\nminimizeğ¿ğ‘(ğ‘¥)andğ¿ğ‘(ğ‘¦)using the query workload as the training data.\n4.3.2 Bottom Cluster Generation .When splitting a data space, there are both profit and loss\nin the query costs. The profit is gained by the reduced number of objects to be checked while the\nloss reflects an increased number of sub-spaces to be checked. In Example 4.1, the profit and loss\nare equal to 2ğ‘¤2and2ğ‘¤1, respectively. The difference between the profit and the loss determines\nwhether a split is needed, and where the split should be made.\nAlgorithm 2 summarizes our bottom cluster generation algorithm. The algorithm takes the query\nworkloadğ‘Šand the data space ğ‘†enclosing all geo-textual objects as the input, and it aims to\nreturn a set of clusters that minimize the cost of executing all the queries in ğ‘Š. The algorithm\nmaintains a priority queue ğ‘„of sub-spaces to the examined, which are prioritized by their numbers\nof intersecting queries. At the start, ğ‘„contains only the input data space ğ‘†(lines 1 and 2). Then,\nwe iterate through the sub-spaces in ğ‘„. Let the current sub-space to be split be ğ‘ . We set the initial\nobject checking the cost of ğ‘ to be|ğ‘‚ğ‘ |Â·|ğ‘Šğ‘ |Â·ğ‘¤2where|ğ‘‚ğ‘ |and|ğ‘Šğ‘ |denote the number of objects\ninğ‘ and the number of queries intersecting with ğ‘ , respectively (lines 5 and 6). Then, we find the\noptimal split along both ğ‘¥- andğ‘¦-dimensions, respectively (lines 7 and 8), and we use the one with\na smaller object checking cost as our candidate split (line 9). If the reduction in the object checking\ncost fromğ¶ğ‘ outweighs the increase in cluster checking cost, i.e., ğ‘¤1Â·|ğ‘Š|(every split adds a cluster\nto be checked against |ğ‘Š|queries), we execute the split and enqueue the resultant sub-spaces (lines\n10 to 13). Otherwise, ğ‘ is finalized, and we generate the MBR for the data objects in ğ‘ and use it as a\nbottom cluster (lines 14 to 16). The process terminates when ğ‘„becomes empty (line 4).\nWhen finding the optimal splitting value along a dimension (lines 18 to 24), we apply SGD [ 53]\nto minimize Eq. 4 (line 21). Here, we use a map structure ğ‘œğ‘ğ‘¡to record the new object checking\ncost, the dimension, and the value of a learned optimal split.\nThe time complexity of each iteration in Algorithm 2 is O\u0000â„Â·(ğ¸ğ‘¥+ğ¸ğ‘¦)\u0001whereâ„andğ¸ğ‘‘,ğ‘‘âˆˆ{ğ‘¥,ğ‘¦}\ndenote the time complexity of SGD per iteration and the number of epoches respectively. Recall\nthat the time complexity of the brute-force algorithm is O\u0000ğ‘“Â·(|ğ‘‰ğ‘¥|+|ğ‘‰ğ‘¦|)\u0001. We note that ğ‘“is larger\nthanâ„because our heuristic algorithm does not need to run a split to calculate a query cost (while\nthe brute-force algorithm does). ğ¸ğ‘¥andğ¸ğ‘¦depend on the algorithm configurations, such as the\nlearning rate and the number of model parameters. They are usually much smaller than ğ‘‰ğ‘¥andğ‘‰ğ‘¦,\nrespectively. Therefore, the time complexity of our heuristic algorithm is lower than that of the\nbrute-force algorithm.\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\n187:10 Yufan Sheng et al.\nAlgorithm 2: Bottom Clusters Generation\nInput: W, the query workload; S, the data space\nOutput: G, the set of clusters\n1Qâ†NewPriorityQueue();\n2Q.Enqueue( S);\n3Gâ†âˆ… ;\n4while Q is not empty do\n5ğ‘ â†Q.Dequeue();\n6ğ¶ğ‘ â†InitializeObjectCheckingCost( ğ‘ );\n7ğ‘œğ‘ğ‘¡ğ‘¥â†FindOptimalPartition( ğ‘ , x);\n8ğ‘œğ‘ğ‘¡ğ‘¦â†FindOptimalPartition( ğ‘ , y);\n9ğ‘ğ‘’ğ‘ ğ‘¡â†ğ‘œğ‘ğ‘¡ğ‘¥ifğ‘œğ‘ğ‘¡ğ‘¥.costâ‰¤ğ‘œğ‘ğ‘¡ğ‘¦.cost elseğ‘œğ‘ğ‘¡ğ‘¦;\n10 ifğ¶ğ‘ âˆ’ğ‘¤2Â·ğ‘ğ‘’ğ‘ ğ‘¡.ğ‘ğ‘œğ‘ ğ‘¡ >ğ‘¤1Â·|ğ‘Š|then\n11ğ‘ 1,ğ‘ 2â†GenerateSubSpace( ğ‘ğ‘’ğ‘ ğ‘¡ .dim,ğ‘ğ‘’ğ‘ ğ‘¡ .val);\n12 Q.Enqueue(ğ‘ 1);\n13 Q.Enqueue(ğ‘ 2);\n14 else\n15 câ†GenerateMBR( ğ‘ );\n16 G.add( c);\n17returnğº;\n18Function FindOptimalPartition( ğ‘ ,ğ‘‘):\n19ğ‘œğ‘ğ‘¡.ğ‘‘ğ‘–ğ‘šâ†d; /* a map structure to record optimal split result */\n20ğ‘ğ‘œğ‘ ğ‘¡,ğ‘£ğ‘ğ‘™â†SGDLearn( s.queries) ; /* SGDLearn() returns the optimal cost and split value */\n21ğ‘œğ‘ğ‘¡.ğ‘ğ‘œğ‘ ğ‘¡â†ğ‘ğ‘œğ‘ ğ‘¡ ;\n22ğ‘œğ‘ğ‘¡.ğ‘£ğ‘ğ‘™â†ğ‘£ğ‘ğ‘™;\n23 returnğ‘œğ‘ğ‘¡;\n5 BOTTOM-UP PACKING\nThe bottom clusters generated from Section 4 can be used as a flat and coarse-grained index. To\nfurther improve the pruning power of our index, we build a hierarchical structure over the clusters.\n5.1 Design Considerations\nAs shown in Section 3, when executing a query with a hierarchical index, we traverse all qualified\nnodes until reaching the leaf nodes. An internal node and its descendants can be pruned if it does\nnot intersect with the query or include any query keyword. We build our hierarchical index level\nby level, i.e., recursively packing the clusters to maximize the reduction in the pruning cost at each\nlevel. Here we omit the object checking costs as they are only triggered on the bottom clusters.\n5.1.1 Optimization Goal .The query time spent on node pruning can directly reflect the pruning\ncapability of a hierarchical index. Measuring the query time, however, needs to run all queries\nin the query workload on an existing index, which is not suitable to be used as an optimization\nmetric of our bottom-up packing problem. We observe that the pruning time cost is proportional to\nthe number of accessed nodes for the workload such that it can be used to evaluate the pruning\ncapability. To adopt this criterion, we associate each bottom cluster ğ‘ğ‘–with a query label set denoted\nbyğ‘ğ‘–.ğ‘™. If a cluster ğ‘ğ‘–intersects with a training query ğ‘ğ‘—and its textual document includes any\nkeyword of ğ‘ğ‘—, we addğ‘ğ‘—to the query label set of this cluster, that is, ğ‘ğ‘–.ğ‘™={ğ‘ğ‘—}. During packing,\nthe labels of a node in an upper level (an â€œupper nodeâ€ for short hereafter) can be easily generated\nby merging all labels of its sub-tree.\n5.1.2 Bottom-up Packing Problem .Next, we define the bottom-up packing problem to mini-\nmize the number of accessed nodes.\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\nWISK: A Workload-aware Learned Index for Spatial Keyword Queries 187:11\nYZ X\nZ10 0 0 0 0 0 1 1 0 0 1 0 1 0\nFig. 6. An example of the state representation\nProblem 2 ( Bottom-up Packing ).Given a query workload W and the set of bottom clusters ğº,\nthe bottom-up packing process aims to generate a hierarchical index ğ¼that minimizes the number of\naccessed nodes to process the queries in ğ‘Š.\nGiven the leaf nodes, i.e., bottom clusters, we can build a hierarchical index using techniques from\ntraditional indexes such as the CDIR-Tree. However, those techniques only consider the underlying\ndata distribution, which might lead to worse performance as shown in the later experiment.\nTo address these issues, and motivated by the strong performance of query-aware structures\nlearned by reinforcement learning (RL), we propose an RL-based algorithm to learn a packing. We\nconstruct our index level by level with a bottom-up packing process, and we model the packing\nproblem at each level as a sequential decision-making process, i.e., a Markov decision process,\nwhich makes it solvable by RL. To pack each level, the nodes from a lower level to be packed\n(â€œbottom nodesâ€ hereafter) are processed sequentially, and we find an upper node to host each\nbottom node until there are no more bottom nodes. After the packing process of one level stops,\nthe non-empty upper nodes become the new bottom nodes to be packed for the next level.\n5.2 Packing with Reinforcement Learning\nWe propose an RL-based packing algorithm following the idea of the Deep-Q-Network (DQN) [ 44]\nto learn the optimal policy (i.e., a packing strategy) for solving the packing problem (Problem 2).\nTo form a tree structure, we require that the number of upper nodes does not exceed that of the\nbottom nodes.\nThere are two main challenges in our packing problem.\n(1)To use a neural network to estimate an expected reward (e.g., the reduction in the number of\nnode accesses), the states (e.g., the relation of two levels resulting from a packing decision)\nneed to be represented by a fixed-length vector. However, there are many different possibilities\nof bottom nodes, and it is challenging to generate such a vector to encode the current packing\nof bottom nodes effectively.\n(2)Every time a node is added to the structure, it may lead to a reduced reward (i.e., more node\naccesses). However, it is necessary to add nodes to the structure continuously such that the\nstructure can be built up. How to adapt the cost model for this case is another challenge.\nTo address these challenges, we formulate an MDP process for our packing problem as follows:\nStates. A state needs to capture the status of a (partially packed) level in an index structure. As\nmentioned above, the number of bottom nodes bounds that of the upper nodes. Hence, we initialize\nğ‘empty upper nodes given ğ‘bottom nodes. Consider ğ‘šqueries are used in the learning process.\nEach of the ğ‘upper nodes to be constructed takes an (ğ‘š+1)-dimensional vector representation.\nThe firstğ‘šdimensions denote whether the node is labeled by each of the ğ‘šqueries, and the last\ndimension is a count on the number of bottom nodes to be connected to this node. The ğ‘upper\nnodes together form an (ğ‘š+1)Â·ğ‘-dimensional vector. We further append ğ‘šdimensions to the\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\n187:12 Yufan Sheng et al.\nY Z X Bottom nodes\nX+\nY+1\n2 3\nZ+1\n2 3\nZ+Z+Y+\nZ\n+1\n2 3\nZ\n+Z\n+Y+\nZ\n+1\n2 3\nZ+Z\n+\n12 3 1 2 3\n 12 3\n 1\n2 3\n 1 2 3\n 1 2 3 12 3\n 1\n2 3\n 12 3\nFig. 7. An example of MDP formulation for Problem 2\nvector to represent the query label of the next bottom node to be connected to (i.e., packed into)\none of the upper nodes. Overall, these form an\u0000(ğ‘š+1)Â·ğ‘+ğ‘š\u0001-dimensional vector representing a\nstate. Figure 6 shows an example, assuming ğ‘š=3queries and ğ‘=3bottom nodes ( ğ‘‹,ğ‘Œ, andğ‘).\nThe circles denote upper nodes, and the colors denote different query labels.\nActions. An action adds a bottom node to an upper node. To make the action space and the state\nrepresentation consistent, we define the action space ğ´={1,2,...,ğ‘}, where action ğ‘=ğ‘–denotes\npacking the next bottom node into the ğ‘–-th upper node.\nTransition. Given a state and an action, the agent transits to a new state by packing a bottom\nnode into the chosen upper node and moving on to the next bottom node. The agent reaches a\nterminal state when there are no more bottom nodes to be packed.\nReward. A larger reward represents a packing with better quality. Since we aim to reduce the\nnumber of node accesses when executing the query workload, the reward signal should reflect the\nexpected number of node accesses before and after taking an action.\nWe propose to use the average number of node accesses per query to formulate the rewards\nsince the total number of node accesses grows monotonically as more bottom nodes are added to\nthe consideration, which will lead to constant negative rewards.\nğ‘Ÿ=ğ‘ğ‘âˆ’ğ‘â€²\nğ‘ (5)\nThe reward function is formulated as Eq. 5, where ğ‘ğ‘(ğ‘â€²\nğ‘) denotes the average number of node\naccesses before (after) action ğ‘is taken. The agent chooses the action that maximizes the reward\nduring exploitation. Additionally, we observe a positive correlation between the sum of rewards\nand the reduction in the average number of node accesses after packing all bottom nodes. Let ğ‘âˆ—\nğ‘\nbe the average number of node accesses of packing the last bottom node. As ğ‘ğ‘in each iteration is\nidentical toğ‘â€²\nğ‘of the last iteration, the sum of rewards after packing all ğ‘bottom nodes is equal to\n1âˆ’ğ‘âˆ—\nğ‘, and it is positively correlated to ğ‘+1âˆ’ğ‘âˆ—\nğ‘. Note that the number of node accesses is equal\ntoğ‘+1before creating the upper nodes. Thus, if the sum of the rewards at a level is not larger\nthanâˆ’ğ‘, the bottom-up packing process will be terminated.\nExample 5.2: Figure 7 presents an example of the MDP for the bottom-up packing problem. Same\nas in Figure 6, the colors represent different query labels. Here, we only show state transitions\nwith nonzero probabilities, and we have omitted the rewards to avoid clutter. The ellipse nodes\nand edges represent states and actions, respectively. Since there are 3 bottom nodes (rectangle),\nwe initialize 3 upper nodes (circle), and the bottom nodes are to be packed sequentially. When no\nincoming bottom node is to be inserted at one level, i.e. the leaf node in Figure 7, we reach the\nterminal states at this level and move to the upper level.\n5.3 Training\nRecall that Q-learning is a commonly used RL algorithm as introduced in Section 2.2. We train a deep\nQ-network (DQN) [ 44] to project the high-dimensional state and action spaces to low-dimension\nspaces using neural networks and efficiently predict the value of the Q-function ğ‘„(ğ‘ ,ğ‘). In our\nmodel, we adopt the deep Q-learning with a technique known as experience replay where we\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\nWISK: A Workload-aware Learned Index for Spatial Keyword Queries 187:13\nAlgorithm 3: DQN Learning for Node Packing\nInput:ğº, the bottom nodes with query labels; ğ‘€, replay memory; ğ¸, the number of epochs\nOutput:ğ‘„(ğ‘ ,ğ‘;ğœƒ), action-value function\n1Initializeğ‘„(ğ‘ ,ğ‘;ğœƒ),Ë†ğ‘„(ğ‘ â€²,ğ‘â€²,ğœƒâˆ’);\n2forepochâˆˆ[1,ğ¸]do\n3ğºğ‘¢â†NewList();ğ‘€â†NewList();\n4 forğ‘ğ‘–âˆˆGdo\n5 Updateğ‘ usingğºğ‘¢andğ‘ğ‘–;\n6 Compute the average number of node accesses ğ‘ğ‘according to ğºğ‘¢.ğ‘™;\n7 Chooseğ‘by theğœ–-greedy method;\n8 Packğ‘ğ‘–intoğºğ‘¢[ğ‘] and generate the new state ğ‘ â€²;\n9 Computeğ‘â€²ğ‘according to the new ğºğ‘¢.ğ‘™;\n10 Compute reward ğ‘Ÿbased on Eq. 5;\n11 Store transition(ğ‘ ,ğ‘,ğ‘Ÿ,ğ‘ â€²)intoğ‘€;\n12 Draw a batch of samples from ğ‘€and perform a gradient step based on Eq. 6;\n13 Update Ë†ğ‘„(;ğœƒâˆ’)withğ‘„(;ğœƒ)softly based on Eq. 7 after every Csteps;\n14returnğ‘„(ğ‘ ,ğ‘;ğœƒ);\nstore the agentâ€™s experience ğ‘’ğ‘¡=(ğ‘ ğ‘¡,ğ‘ğ‘¡,ğ‘Ÿğ‘¡,ğ‘ ğ‘¡+1)at each time-step ğ‘¡. We implement two networks, a\npolicy network ğ‘„and a target network Ë†ğ‘„separately, which has been shown to be more stable than\nusing only one network as done in the standard Q-learning [44].\nGiven a batch of transitions (ğ‘ ,ğ‘,ğ‘Ÿ,ğ‘ â€²), the policy network parameters ğœƒare updated with a\ngradient descent step by minimizing the mean square error (MSE) loss as shown in Eq. 6, where\nğ›¾âˆˆ(0,1)denotes a discount factor determining the importance of future rewards, and ğœƒâˆ’are the\nparameters of the target network.\nğ¿(ğœƒ)=âˆ‘ï¸\nğ‘ ,ğ‘,ğ‘Ÿ,ğ‘ â€²\u0010\nğ‘Ÿ+ğ›¾max\nğ‘â€²Ë†ğ‘„(ğ‘ â€²,ğ‘â€²;ğœƒâˆ’)âˆ’ğ‘„(ğ‘ ,ğ‘;ğœƒ)\u00112\n(6)\nNote that the target network parameters ğœƒâˆ’are only synchronized with the policy network\nparametersğœƒevery Tsteps and are held fixed between weight updates. However, directly copying\nthe weights has been shown to be unstable due to noise and outliers. Inspired by prior works\n[34,39], we apply the soft update (Eq. 7) to the target network. The weights of the target network\nare updated by interpolating between the weights of the target network and those of the policy\nnetwork through a fixed ratio ğœ=0.001[39].\nğœƒâˆ’=ğœğœƒ+(1âˆ’ğœ)ğœƒâˆ’,ğœâ‰ª1 (7)\nWe present the learning process in Algorithm 3. We first initialize the policy network and the\ntarget network with the same random parameters (line 1). In each epoch, we reset the replay\nmemoryğ‘€and the set of upper nodes ğºğ‘¢(line 3). Then, the learning process sequential packs the\nbottom nodes to the upper nodes (lines 4 to 13). For every incoming bottom node ğ‘ğ‘–, we generate\nthe state by combining ğºğ‘¢andğ‘ğ‘–(line 5). We compute the average number of node accesses based\non the query labels of the current upper nodes (line 6). To balance between RL exploration and\nexploitation, we use the ğœ–-greedy algorithm [ 57] to choose a random action with probability ğœ–\n(i.e., exploration) or the action that maximizes the action-value function of the policy network\n(i.e., exploitation) (line 7). After ğ‘ğ‘–is packed, we update the state representation and compute the\naverage value again (lines 8 and 9). Then, we compute the reward and store this transition in the\nreplay memory (lines 10 and 11). To train the DQN, we draw a batch of transitions to train the\npolicy network (line 12) and periodically copy the policy network parameters to the target network\n(line 13). Finally, we use the learned action-value function ğ‘„(ğ‘ ,ğ‘;ğœƒ)to pack the nodes.\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\n187:14 Yufan Sheng et al.\n6 DESIGN OPTIMIZATIONS\nChoice of CDF models. For our heuristic partition algorithm, the number of objects with each\nkeyword is approximated by a model that learns the corresponding CDF. Prior works [ 36,38] have\nused the neural network (NN) to learn the CDF. However, learning an NN for each query keyword\nmay lead to a large number of NNs to be learned and hence high preparation costs.\nWe observe that the query time of WISK is more sensitive to high-frequency keywords. To\ndecrease preparation costs, we divide keywords into three classes based on their frequency: low\n(â‰¤0.001â€°), medium ( 0.001âˆ’0.1â€°), and high (â‰¥0.1â€°). Previous studies [ 64,72] have shown that\nmore resources should be allocated to records with more frequent elements to get better prediction\naccuracy. When calculating the query cost, low-frequency keywords are ignored as they have little\nimpact on the query time. We adopt Gaussian functions to approximate the data distribution of\neach medium-frequency keyword and learn an NN to approximate the CDF of each rest keyword.\nOur empirical results show that such a strategy balances the preparation costs and the query time.\nCorrelation between keywords. In Section 4, we consider each keyword independently when\napproximating|ğ‘‚ğ‘ |(ğ‘ âˆˆ{1,2}) in Eq. 4. This independence assumption impacts the performance of\nthe heuristic partitioning algorithm when a query has more than one keyword, e.g., if an object\ncontainsğ‘˜query keywords of a query, this object will be counted ğ‘˜times when predicting the\nnumber of objects in a sub-space, leading to inaccurate query cost prediction.\nTo solve this issue, we exploit frequent itemset mining to discover all frequent keyword sets and\nextract associations among the given set of keywords [ 1,2,25,59]. We apply a classic algorithm,\nFP-Tree [ 25], to find frequent keyword sets from the underlying data. Then, we learn a CDF model\nof objects containing all keywords in one frequent keyword set and use the learned model to predict\nthe number of objects with the set of query keywords more accurately.\nAction mask in RL. When the packing of a level starts, the upper nodes are all empty. To choose\nthe upper node to insert for the first bottom node, we observe that actions ğ‘=ğ‘–(ğ‘–>1) are all\nequivalent to ğ‘=1. We call these actions duplicated actions . Duplicated actions exist when more\nthan one upper nodes are empty. As observed in Figure 7, the actions of adding the bottom node\nto any of these empty upper nodes are equivalent. Such duplicated actions make the exploration\ninefficient, leading to slow convergence [23].\nThus, motivated by a prior study [ 76], another use of the environment is to generate an action\nmask based on the current state to hide the duplicated actions from the agent. In the example above,\nbefore inserting the first bottom node, the action mask generated by the environment makes the\nagent only chooses action ğ‘=1.\nTraining time acceleration. WISK has two steps: finding the bottom clusters and packing the\nbottom clusters through RL. To reduce the training time of WISK, we design acceleration techniques\nfor both steps. The first technique is to use sampled training queries, following a previous work [ 45].\nWe use stratified sampling [ 5] to obtain query samples that can better represent the distribution\nof the original workload. The second technique groups the bottom clusters using a clustering\nalgorithm to reduce the number of bottom clusters to be packed in the bottom-up packing step. We\nutilize the spectral clustering [ 46] with the coordinates of the bottom left and top right points of\neach bottom cluster as features.\n7 EXPERIMENTS\n7.1 Implementation and Setup\nImplementation. The learning process of CDF NN models, Algorithm 2, and Algorithm 3 are\nimplemented with PyTorch [ 47]. The performance evaluation of all index structures is implemented\nin C++ and compiled using GCC 9.3 with -O3 flag. In the process of generating the bottom clusters,\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\nWISK: A Workload-aware Learned Index for Spatial Keyword Queries 187:15\nwe empirically set 0.1 and 1 to the weights of stage 1 and stage 2, i.e., ğ‘¤1andğ‘¤2, respectively.\nThe CDF network consists of 4 layers, and each hidden layer has 16 units. We use ReLU as the\nactivation function of the hidden layer. The output of the CDF is activated by a sigmoid function.\nWhen packing the bottom clusters, we follow the original implementation of DQN [ 44]. The neural\nnetwork consists of 3 layers, and each hidden layer has 64 units. We set the capacity of experience\nreplay to 256, and the discount factor is set to 0.99. For ğœ–-greedy algorithm, the initial value of\nğœ–is set to 1, and the value decreases with more learning steps, which balances exploration and\nexploitation well.\nEnvironment. We run single-threaded experiments in the main memory on an Ubuntu machine\nwith Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz, 128GB RAM, and a 500 GB SSD disk. Besides,\nwe train our CDF models on an Ubuntu machine with Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz,\n256GB RAM, and RTX 2080 Ti GPU.\nBaselines. We compare WISK with four SOTA conventional indexes, i.e., CDIR-Tree [15],SFC-\nQuad [13],ST2I [27], and ST2D [58]. We implement these indexes using the default parameter\nvalues reported in their original papers. Note that ST2D is only evaluated on FSby setting the\nsimilarity threshold to 0 since it is only suitable for the case that containing a few distinct keywords\n(a few hundreds) because of the textual clustering.\nWe also integrate a learned spatial index with a textual index loosely, following traditional spatial\nkeyword indexes. This results in a learned spatial-first index (SFI) and a textual-first index (TFI).\nSFI attaches an inverted file for keywords indexing to each leaf node of a learned spatial index,\nwhile TFI uses an inverted file as its top-level index and creates a learned spatial index for the\nobjects containing the same keyword. It has been shown that textual-first indexes outperform their\nspatial-first counterparts [ 61,79]. Therefore, we only report results for TFI in our experiments.\nLISA [ 38] is used as the learned spatial index since it returns the exact results. We further extend\na learned multi-dimensional index, i.e., Flood [ 45]. We build an inverted file for each grid cell in\nFlood and also improve its cost function for building the grid index by incorporating the textual\ninformation, utilizing our CDF models on the geo-textual data, following the method presented in\nSection 4. We denote this index by Flood-T . It splits the data along only one dimension in the 2D\ngeographical space, which limits its capability to capture the complex data distribution. We also\ncompare with LSTI [19], the latest index to support spatial keyword queries. This method maps\nthe data into one dimension using a Z-order curve based on the spatial coordinates and builds a\nRadixSpline index [ 32] using the mapped values. Then, an inverted file is created for each spline\npoint by scanning the dataset again.\n7.2 Datasets and Workloads\nWe use three real-world datasets in Table 1. The FSdataset [ 71] consists of global-scale check-in\nrecords of Foursquare (https://foursquare.com/) from Apr. 2012 to Jan. 2014. A check-in data has a\nspatial location and its category. The SPdataset includes recreational and sports areas extracted\nfrom OpenStreetMap (https://www.openstreetmap.org). We use the center of each area and the\noriginal description as the spatial location and keywords, respectively. The BPD dataset contains\nglobal POIs published by the SLIPO project [ 48] (http://slipo.eu/). The OSM dataset contains 100M\nPOIs extracted from OpenStreetMap, which is published in UCR STAR [ 22]. Each POI has a point\nlocation, and its keywords include all related information such as street and category. As there\nis no public real-world query workload for the geo-textual datasets, we generate the queries by\nfollowing previous works [ 10,23,28,63,65]. Specifically, to generate a query, we first sample\nan object in the dataset, and then generate a bounding rectangular area with the location of this\nobject being its center. Inspired by previous works [ 23,63], we use four methods to generate the\ncenters: (i) UNI, where centers are uniformly sampled from the dataset. (ii) LAP , where centers\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\n187:16 Yufan Sheng et al.\nTable 1. Dataset Statistics\nProperty FS SP BPD OSM\nNumber of data objects 3M 4M 25M 100M\nNumber of distinct keywords 462 1M 24M 447M\nTotal number of keywords 6M 11M 116M 478M\nTable 2. Parameters and their settings\nParameter Setting\nQuery distribution UNI LAP GAU MIX\nQuery region size (%) 0.005 0.01 0.05 0.1 0.5 1\nNumber of query keywords 1 357 9\nare sampled from the Laplace distribution [ 35]. We set the location and scale parameters, i.e., ğœ‡and\nğ‘, to|ğ·|/2and|ğ·|/10respectively, where ğ·is the object set. (iii) GAU , where centers are sampled\nfrom a Gaussian distribution ( ğœ‡=|ğ·|/2,ğœ=100). (iv) MIX , composing of the centers generated\nfrom the (i) and (ii) in equal proportions. Finally, we associate keywords for the queries following\nprior works [ 10,65]. If the number of query keywords is less than the number of keywords of the\ncenter, we choose the query keywords from the sampled object. Otherwise, we randomly choose\nthe remaining keywords from the global keyword set.\nTo evaluate the performance of indexes in different scenarios, we generate query sets with\ndifferent numbers of keywords and query sizes. Table 2 summarizes parameters, where default\nvalues are in bold and underlined. We generate 2000 queries under each setting, in which 1000\nqueries are utilized to test the performance of all the indexes, and others are used to train learned\nindexes.\n7.3 Query Time Evaluation\nTo evaluate the query time, we execute testing queries 100 times and report the average cost of the\nqueries in each query set.\n7.3.1 Effect of query distribution. In this experiment, we fix other settings except for the query\ndistribution and show the results on all datasets in Figure 8. Clearly, conventional indexes (SFC-\nQuad, ST2I, and CDIR-Tree) perform worse on the skewed workload since they do not use the query\ncharacteristics when constructing the index. For the learned indexes, TFI performs even worse\nthan the conventional indexes since it only loosely combines a learned spatial index with a textual\nindex. Flood-T shows a slight fluctuation in its performance since it learns from the underlying data\nand the query workload simultaneously, but in the geo-textual scenario, it only splits along one\ndimension, making it incompatible with the skewed workload. Our WISK improves the partitioning\nand adopts RL to build a tree, so it is less sensitive to this alteration.\nU NILAPGAUMIX101102103Query time (Âµs)\n(a) FS\nU NILAPGAUMIX101102103Query time (Âµs) (b) SP\nU NILAPGAUMIX101102103Query time (Âµs) (c) BPD\nU NILAPGAUMIX102103104Query time (Âµs) (d) OSM\nFig. 8. Varying the distribution of the query workload\n7.3.2 Effect of query region size. We show the performance of all indexes, by varying the query\nregion size varies from 0.005% to1%of the whole region in Figure 9. Again, WISK performs the best\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\nWISK: A Workload-aware Learned Index for Spatial Keyword Queries 187:17\n  CDIR-Tree   SFC-Quad   ST2I   TFI\n  LSTI   Flood-T   WISK   ST2D\n0 .0050.010.050.10.51101102103Query time (Âµs)\n(a) FS\n0 .0050.010.050.10.51102103104Query time (Âµs) (b) SP\n0 .0050.010.050.10.51102103104Query time (Âµs) (c) BPD\n0 .0050.010.050.10.51102103104105Query time (Âµs) (d) OSM\nFig. 9. Varying the query region size\n  CDIR-Tree   SFC-Quad   ST2I   TFI\n  LSTI   Flood-T   WISK   ST2D\n1 3 5 7 9 102103Query time (Âµs)\n(a) FS\n1 3 5 7 9 102103Query time (Âµs) (b) SP\n1 3 5 7 9 102103Query time (Âµs) (c) BPD\n1 3 5 7 9 103104Runtime (Âµs) (d) OSM\nFig. 10. Varying no. of query keywords\non four datasets. Besides, Flood-T performs slightly worse than ST2I on SP, even though it optimizes\nits layout by learning from the data and query workload. It is because it only splits the whole region\nalong one dimension. Thus, we improve this process to generate the leaf nodes of WISK and also\npack the bottom clusters into a hierarchical structure. The two techniques simultaneously result in\nthe superiority of WISK over the other indexes.\n7.3.3 Effect of number of query keywords. We evaluate the query sets with different numbers\nof keywords. Figure 10 shows that the query time of all indexes grows with the query keyword\nset size. The reason is that with the increase in the number of query keywords, more candidates\nneed to be verified after the filtering step. Besides, WISK consistently outperforms other baseline\nindexes, and its cost grows much slower than those of others, e.g., the increased time of WISK on\nBPD is around 100 ğœ‡s while those of Flood-T and ST2I are both over 250 ğœ‡s. Hence, compared to\nother indexes, WISK is less sensitive to the number of query keywords.\n7.3.4 Scalability. We generate five sub-datasets of OSM containing from 1 to 100 million objects\nand run experiments on these sub-datasets. We choose ST2I, LSTI, and Flood-T as our baselines.\nAs shown in Figure 11, the query processing time increases with the size of the dataset, but WISK\nperforms more stable.\n1M5M10M50M100M0900180027003600N\no. of objectsQuery time (Âµs) ST2I \nLSTI \nFlood-T \nWISK\nFig. 11. Comparison of performance\nvarying dataset size (num records)\n00.20.40.60.8150100150200250L\nAP ratioQuery time (Âµs) ST2I \nFlood-T \nWISKFig. 12. Comparison of performance\nwhen changing query distribution\n7.3.5 Robustness. We evaluate the performance of ST2I, Flood-T, and WISK when the query\ndistribution changes on FS. We initially train Flood-T and WISK based on the query workload with\nUNI distribution. Then, we keep the index consistent and adjust the ratio of queries with LAP\ndistribution from 0.2 to 1.0 in the testing query set. As shown in Figure 12, the performance of\nquery-aware indexes becomes worse when query distribution is more different from the training\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\n187:18 Yufan Sheng et al.\none. However, it can be seen that WISK is more robust than Flood-T due to its improved partitioning\nalgorithm and the bottom-up packing process. Additionally, the query time of ST2I also increases\nsince it ignores the query knowledge when building the index, but the fluctuation is less than the\none of Flood-T.\n7.4 Index Size & Construction\n7.4.1 Index Sizes. Table 3 reports the index sizes. Overall, WISK costs less space than conventional\nindexes but is comparable to that of the best adapted learned indexes. In particular, the size of\nCDIR-Tree is larger than those of the others since each of its nodes has an inverted file. For query\nefficiency, we have not compressed SFC-Quad, which leads to a larger size. ST2I has the smallest\nsize among the conventional indexes. Among the learned indexes, the sizes of TFI are the largest,\nas it uses inverted files. WISK takes more space than Flood-T on a small dataset since the number\nof its bottom clusters is similar to the number of the columns of Flood-T, but we build a hierarchical\nindex. However, on larger datasets, WISK needs less space cost, since Flood-T splits more columns\nfor better performance and builds inverted files for them.\nTable 3. Index structure size\nIndex FS SP BPD OSM\nCDIR-Tree 2002MB 3571MB 33.15GB 108.45GB\nSFC-Quad 1406MB 2568MB 15.65GB 58.71GB\nST2I 761MB 1554MB 15.18GB 56.05GB\nTFI 573MB 1423MB 8.86GB 32.05GB\nLSTI 642MB 1073MB 8.85GB 8.09GB\nFlood-T 400MB 937MB 7.15GB 27.94GB\nWISK 483MB 980MB 7.02GB 25.78GB\n7.4.2 Index Construction Time. We compare the efficiency of index construction algorithms\nand report the results in Table 4. It takes the minimum time to build SFC-Quad and ST2I on small\ndatasets. However, the time cost of ST2I significantly increases when the dataset becomes larger,\nsince ST2I is built based on the set of converted points, and its time cost is positively correlated to\nthe total number of keywords. CDIR-Tree takes the highest time cost because it inserts the objects\nsequentially.\nTable 4. Index construction time\nIndex FS SP BPD OSM\nCDIR-Tree 391 sec 490 sec 56.17 min 196.87 min\nSFC-Quad 20 sec 30 sec 3.35 min 9.18 min\nST2I 19 sec 29 sec 6.55 min 26.23 min\nTFI 125 sec 283 sec 33.75 min 143.07 min\nLSTI 23 sec 32 sec 4.16 min 16.32 min\nFlood-T 188 sec 974 sec 19.66 min 25.97 min\nWISK 353 sec 1216 sec 55.28 min 65.37 min\nWISK\n(Accelerated)131 sec 547 sec 12.18 min 17.43 min\nFor the learned indexes, we report the training time. LSTI takes the least time to build because it\nonly needs to scan the whole dataset twice. The time cost of TFI increases significantly when there\nare more different keywords. For Flood-T and WISK, we report the average time as the time costs\nof query-aware learned indexes usually increase with more query keywords.\nWe designed two training time acceleration techniques as presented in Section 6. We report\nthe training and query times of WISK with different sampling ratios in Figure 13a. The result of\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\nWISK: A Workload-aware Learned Index for Spatial Keyword Queries 187:19\n  Training time   Query time\n2040608010003006009001200S\nample ratioTraining time (sec)0\n60120180240 \nQuery time (Âµs)\n(a) Sampling ratio (%)\n20406080100050100150200C\nlustering ratioTraining time (sec)2\n0406080100 \nQuery time (Âµs) (b) Clustering ratio (%)\nFig. 13. Training time and resulting query time on SP\neach sampling ratio is an average of 10 runs. While the training time decreases by 72%, we do not\nobserve a large drop in query performance with a sample of only 30% of the full query workload.\nWe also observe that the standard deviation (represented by the width of the bands) of the training\nand query times of WISK is consistently small for all sampling ratios. This demonstrates that WISK\nhas a stable performance using stratified sampling. We vary the clustering ratio, i.e., the number of\ngroups obtained over the number of bottom clusters, to balance the training and querying time.\nFigure 13b shows that even when the number of bottom clusters decreases by 80%, the query time\nof WISK still only changes slightly. We set the sampling ratio to 30% and the clustering ratio to 20%\nand generate the Accelerated WISK . As shown in Table 4, WISK has longer training times than\nthe other learned indexes, but the acceleration techniques can reduce index training time up to 4\ntimes while the query time is only affected marginally.\n7.5 Index Update\n7.5.1 Dynamic Query Workload Changes. To update the index when query distribution\nchanges, we can retrain WISK periodically following the former study [ 45]. We generate six\nworkloads for FS. For each workload, we randomly select the query region size and the number of\nquery keywords, and the query distribution adopts the default settings ( MIX ) and we randomly\nselect the proportions of UNI andLAP . Each workload runs for 30 minutes and consists of 100\nqueries. As Figure 14 shows, at the start of each 30-minute period, i.e., a new query workload starts,\nretraining WISK is triggered, which happens in a separate thread and does not interrupt the query\nprocessing. While the index is being rebuilt, WISK runs the new queries on its old layout, which\nexplains the jumps in the figure. The retraining lasts about 3 minutes, and then WISK switches to\nthe new layout adapted to the new query workload. Thus, the query time drops back again.\nTo capture minor changes in query distribution when retraining, we propose to apply incremental\nupdates to the original index, in parallel to the retraining process. We locate the bottom clusters\nthat are affected by the new queries, re-partition these clusters if the query costs can be reduced\nusing new queries, and then insert the new clusters back into the non-leaf nodes they previously\nbelonged to. The incremental updates may also help reduce the query times, which explains the\nmultiple drops (e.g., at 00:30 and 01:30).\nFigure 14 also indicates the necessity of learning from the query workload. When a new workload\narrives, the performance of WISK drops due to its outdated layout. Re-learning the layouts based on\nthe new workload mitigates the impact of the changing query distribution. The other two indexes\ndo not utilize the queries during construction, which leads to much worse performance than WISK\n(e.g. at 01:00, 02:00, and 02:30) on the skewed query workloads (i.e. high proportion of LAP).\n7.5.2 Data Insertion. WISK also handles data insertion well. Given a new object o, we can\ntraverse WISK to find the bottom cluster where ofalls. Next, we update the inverted file or the\nbitmap of the affected nodes to obtain an updated index. This simple process, however, cannot\nguarantee an optimal layout because the bottom clusters might need to be split after the insertion.\nThus, we buffer the inserted objects and retrain our index when the buffer is full.\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\n187:20 Yufan Sheng et al.\n00:0000:3001:0001:3002:0002:3003:00120240360480T\nimeQuery time (Âµs) ST2I \nLSTI \nWISK\nFig. 14. Impact of dynamic workload changes\nWe set the buffer size at 100,000 (around 20MB) and run experiments to examine the impact of\ndata insertions. We randomly select 500,0000 objects from FSfor the insertions. We insert 100,000\nobjects every 30 minutes. Figure 15 shows the performance of ST2I, LSTI, and WISK. We compare\nwith WISK using the simple insertion process without retraining. It can be seen that the query\ntime of all indexes increases when more objects are inserted. Between the two WISK variants, we\nsee that the query time of WISK without retraining increases faster with more insertions, thus\nverifying the importance of retraining in improving the query time of WISK under dynamic data\nsettings. We also observe that the retraining process takes only 1 to 2 minutes each time, since\nonly the affected bottom clusters need to be split, and the RL-based packing can inherit knowledge\nfrom the previous training process, i.e., the unaffected bottom clusters are initially packed into the\nprevious corresponding upper nodes.\n00:000 0:300 1:000 1:300 2:000 2:3090180270360T\nimeQuery time (Âµs) \nST2I WISK (w/o retrain) \nLSTI WISK\nFig. 15. Impact of data insertion\n7.6 Ablation Study\n7.6.1 RL-based Packing. We conduct an experiment to compare the cost at the leaf level and that\nat the non-leaf level. Figure 16 shows that the time at the leaf level dominates the query processing\ntime, which occupies around 90% of the total cost, verifying the way to define cost function is\nreasonable. In Figure 17, we observe that packing our bottom clusters by directly using CDIR-Tree\nconstruction method might affect the query time because it may pack some leaf nodes intersecting\nwith various queries.\n13 5 7 9 060120180240N\no. of objectsQuery time (Âµs) Non-Leaf \nLeaf\nFig. 16. Comparison of processing time\nUNILAPGAUMIX050100150Q\nuery distributionQuery time (Âµs) Flat Hier (IR) \nHier (RL)Fig. 17. Comparison of packing meth-\nods\nWe evaluate the effectiveness of the bottom-up construction process. As shown in Figure 18a,\nthe improvement of the different number of keywords is similar. This is because the number of\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\nWISK: A Workload-aware Learned Index for Spatial Keyword Queries 187:21\nquery keywords has little effect on the number of bottom clusters. Thus, the improvement is stable\nusing this RL-based grouping algorithm.\n Flat  Hier\n13 5 7 9 060120180240300N\no. of keywordsQuery time (Âµs)\n(a) Varying no. of keywords\n0.0050.010.050.10.510420840126016802100Q\nuery region sizeQuery time (Âµs) (b) Varying query region size\nFig. 18. Comparison of index layouts on BPD\nIt can be seen from Figure 18b that the improvement of hierarchical indexes becomes more\nsignificant for queries of larger region sizes. This is because queries of larger region size correspond\nto a wider space covered by these queries and more bottom nodes such that the bottom-up packing\nprocess can reduce more filtering cost. However, we also observe that the improvement becomes\nstable as the region size continues to get larger, as the query region covers most of the data space.\n7.6.2 CDF Model. When generating bottom clusters, we use CDF models to estimate the number\nof objects sharing the same keywords inside a region. To reduce the parameters, we propose to use\nGaussian functions and NNs for keywords with different frequencies. In Figure 19a, we compare\nour method with the settings in which only Gaussian models or NN models are used. Although\nthe Gaussian-only method has the least training time, its estimation results are inaccurate, leading\nto much worse query time. In contrast, the NN-only method achieves the best query time, but it\nneeds much more training time. In comparison, the proposed mixed method achieves similar query\nperformance as the NN-only method without significantly increasing the training time.\nGaussianN NM ix06001200180024003000C\nDF ModelTraining time (sec) Training \nRunning0\n306090120150Q\nuery time (Âµs)\n(a) Model selection\n1000200030000.10.20.30.40.50\n 1D  2D (32 units) \n2D  2D (3 layers)R\noundsMSE (b) 1D vs. 2D model\nFig. 19. Effect of different CDF settings\nTo speed up pre-processing, we assume that the two spatial dimensions are independent following\nthe existing work [45]. We next study the impact of such an assumption.\nWe observe that keywords with higher frequency have a stronger impact on the query time, and\nthus they need a more accurate CDF estimation. We run experiments with a randomly selected\nhigh-frequency keyword on FS. We train two marginal (1D) models and a joint (2D) model for\nthe selected keyword on the whole dataset. The 1D models and the 2D model all employ a neural\nnetwork with 2 hidden layers and 16 hidden units. We also compare with two variants of the 2D\nmodel, one uses more hidden units (32 units) and the other uses more layers (3 layers). We randomly\nsample 1,000 rectangular query regions within the data space as testing data. For each query region,\nwe compute the proportion of objects that fall within it as the ground truth. The product of the\ntwo 1D models and the output of the 2D models are used as the estimation results corresponding\nto 1D and 2D models, respectively. For every 50 training rounds, we calculate the mean squared\nerror (MSE) between the estimations and the ground truth. Figure 19b shows that the 1D model\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\n187:22 Yufan Sheng et al.\nconverges much faster while its final loss is comparable to those of the 2D models. These results\njustify the use of 1D CDFs in our method.\n7.6.3 Frequent Itemset (FI) .The FI mining extracts the frequent interrelation among keywords.\nIn this experiment, the minimum support is set to 0.01â€°and the maximum size of target itemsets is\nequal to the number of query keywords. Figure 20 shows the effect of FI on the index construction\nby query efficiency on FSandBPD . We see that the FI mining improves the performance of WISK\nconsistently when there is more than one query keyword. Without the FI mining component, we\nlearn models of each keyword separately where redundancies occur when there is more than one\nkeyword. We also observe that this adaptation is more beneficial given more query keywords. In\ngeneral, more query keywords lead to a higher possibility of resulting in redundancy since the\nprobability of an object including more than one query keyword increases.\n With FI  Without FI\n35 7 9 020406080100N\no. of keywordsQuery time (Âµs)\n(a) FS\n35 7 9 070140210280350N\no. of keywordsQuery time (Âµs) (b) BPD\nFig. 20. Effect of the frequent itemset\nBesides, the performance improvement on BPD is more obvious than that on FS. This is because\nthe number of distinct keywords on FSis much less, making the number of frequent itemsets\nless than others. Additionally, the improvement becomes consistent when the number of query\nkeywords is larger than a threshold. The reason is that each object includes finite keywords, and\nthus we cannot generate frequent itemsets with more keywords.\n7.6.4 Action Mask. When using the RL framework, the environment applies the action mask to\nreduce the action space. Here, we evaluate its effectiveness in two aspects. We use the SmoothL1Loss\nwith the sum reduction as the loss function in our RL framework. Figure 21a shows that the RL\nframework with the action mask can speed up the model convergence and reach a smaller loss. In\nFigure 21b, we sum the total rewards and the number of bottom nodes in each epoch to show the\nreduction in the average number of accessed nodes. The result shows that the pruning capability is\nalways better when applying the action mask.\n  Masked   No mask\n1503004506007509000510152025R\noundsLoss\n(a) Loss\n20406080100150200250300350400E\npochsReduction (b) Filtering capacity\nFig. 21. Comparison of model convergence and reward\nthere are some fluctuations in the results as the RL agent learns from the feedback through\ntrial-and-error interactions with the environment. RL balances the trade-off between exploration\nand exploitation, which reduces the fluctuation with more training epochs. These results confirm\nthat the action mask helps to decrease the number of training epochs, leading to less training time.\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\nWISK: A Workload-aware Learned Index for Spatial Keyword Queries 187:23\n7.7 Parameter Sensitivity Study\nWe further evaluate the sensitivity of WISKâ€™s training and query times to our key parameters. We\nshow the results of varying the numbers of hidden units and layers in the neural networks in Figure\n22a. Using more hidden units significantly increases the training time but only slightly improves the\nquery time. Increasing the number of hidden layers shows a similar effect. Additionally, increasing\nthe size of the structure requires larger memory space. Thus, we set the default hidden units and\nlayers to 16 and 2.\n Training time  Query time\n163 26 49001200150018002100N\no. of hidden unitsTraining time (sec)2\n0406080Q\nuery time (Âµs)\n(a) Varying no. of hidden units\n23 4 9001200150018002100N\no. of hidden layersTraining time (sec)2\n0406080Q\nuery time (Âµs) (b) Varying no. of hidden layers\nFig. 22. Comparison of training time and query time\nWe also vary the capacity of experience replay and the discount factor. The results show that\nthey have minor impacts on the query time and the RL convergence rate. We omit the details due\nto space limits. The performance of WISK is less sensitive to these hyper-parameters, and we can\nfollow the existing work [44] to set them.\n8 RELATED WORK\nTraditional geo-textual indexes. In recent years, due to the popularity of SKR queries and their\napplicability in practical scenarios, a series of geo-textual indexes [ 11,13,26,31,58,61,79] have\nbeen proposed to support efficient SKR query processing.\nThe general idea of geo-textual indexes is to combine spatial and textual indexes to exploit their\npruning capabilities based on the spatial and textual attributes of the data. Early works only loosely\ncombine both types of indexes. For example, Vaid et al. [ 61] propose the first grid-based geo-textual\nindexes, i.e., the spatial primary index (ST) and the text primary index (TS), which are spatial-first\nand textual-first integrations, respectively. Parallel to this work, the R*-tree-inverted file (R*-IF)\nand the inverted file-R*-tree (IF-R*) [ 79] combine the inverted file with the R*-tree [ 3]. The loose\nintegrated indexes has been shown to result in unsatisfactory query time in both the follow-up\nstudy [10] and our experiments.\nLater works combine spatial and textual indexes more tightly such that they use both types of\nattributes of the data for search space pruning in parallel. For example, each grid cell in Spatial-\nKeyword Inverted File (SKIF) [ 31] is presented by an inverted list, and it works in the rectangular\nobject scenario. Hariharan et al. [ 26] propose the Keyword-R*-tree (KR*-tree). Each node of this\nindex is associated with the set of keywords that appear in the sub-tree rooted at this node. Thus,\neach tree node can prune the search space with both a spatial region and a set of keywords at the\nsame time.\nThe indexes above focused on SKR queries. There are also geo-textual indexes [ 15,16,27,52,68,\n75], such as IR-Tree, developed for other types of spatial keyword queries. Some of these can be\nadapted to answer SKR queries. However, since they are not tailored for SKR queries, their query\ntime is usually worse [10].\nLearned indexes. Kraska et al. [ 36] propose the recursive model index (RMI), which leverages\nmachine learning models to replace a traditional index over one-dimensional search keys. The\nmotivation is that an index can be seen as a function mapping a search key to the storage position\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\n187:24 Yufan Sheng et al.\nof the corresponding record. Several follow-up studies propose learned indexes for one-dimensional\ndata [17, 21, 69]. More details can be found in a benchmark study [42].\nTo handle multi-dimensional data, the Z-order model [ 62] extends RMI by utilizing a Z-order\ncurve to map multi-dimensional search keys into one-dimensional keys. Since this index might\nlead to large and uneven gaps between the mapped keys of adjacent objects, RSMI [ 51] proposes a\nrank space-based technique, and it further proposes a hierarchical learned partitioning strategy for\nindex learning over large spatial datasets. A parallel work, LISA [ 38], designs a grid-based index\nthat supports data updates. The RLR-tree [ 23] uses machine learning techniques to build a better\nR-tree without the need to change the structure or query processing algorithms of the R-tree.\nAlthough these indexes have shown performance gains by exploiting the data distribution, they\nhave ignored the query workload in index construction. Several studies [ 18,40,45] take into account\nthe query workload and propose to automatically optimize the index structure for a given data and\nquery distribution.\nReinforcement learning. RL is often utilized in sequence generation applications, such as game\nplaying [ 54], machine translation [ 30], and bin packing [ 78]. Recently, it has been adapted to solve\ndatabase optimization problems, such as query optimization [ 76], index tuning [ 70], and trajectory\nsimplification [ 66]. However, these problems are quite different from ours by definitions, and so are\ntheir state and reward formulations. Thus, our RL formulation requires new designs in the states\nand reward functions.\n9 CONCLUSIONS AND FUTURE WORK\nWe proposed a hierarchical index named WISK for SKR queries, which is jointly optimized for a\ngiven dataset and a query workload. WISK is built in two stages. First, a partitioning algorithm finds\nthe data clusters that minimize the time cost of executing the query workload. Then, an RL-based\nalgorithm packs the data clusters into a hierarchical index in a bottom-up manner for more efficient\npruning at query time. Learning from the query workload enables WISK to significantly outperform\ntraditional SKR indexes. Experimental results on real-world datasets show that WISK yields strong\nquery performance over various workloads, achieving up to 8 Ã—times speedups with comparable\nstorage overhead.\nThere are several directions for future work. First, we intend to better answer Boolean ğ‘˜NN\nqueries and to support more types of spatial keyword queries. Second, even WISK can adapt to\nworkload changes by model retraining, a data-driven learned index, which is compatible with\nfrequent workload shifts and data changes, is an interesting direction to explore.\nREFERENCES\n[1]Rakesh Agrawal, Tomasz ImieliÅ„ski, and Arun Swami. 1993. Mining association rules between sets of items in large\ndatabases. In Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data . 207â€“216.\n[2]Rakesh Agrawal, Heikki Mannila, Ramakrishnan Srikant, Hannu Toivonen, A Inkeri Verkamo, et al .1996. Fast discovery\nof association rules. Vol. 12. AAAI/MIT Press Menlo Park, CA, 307â€“328.\n[3]Norbert Beckmann, Hans-Peter Kriegel, Ralf Schneider, and Bernhard Seeger. 1990. The R*-tree: An efficient and\nrobust access method for points and rectangles. In Proceedings of the 1990 ACM SIGMOD International Conference on\nManagement of Data . 322â€“331.\n[4]Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. 2021. Machine learning for combinatorial optimization: a\nmethodological tour dâ€™horizon. European Journal of Operational Research 290, 2 (2021), 405â€“421.\n[5] Zdravko Botev and Ad Ridder. 2017. Variance reduction. Wiley statsRef: Statistics reference online (2017), 1â€“6.\n[6]Nicolas Bruno, Surajit Chaudhuri, and Luis Gravano. 2001. STHoles: A multidimensional workload-aware histogram.\nInProceedings of the 2001 ACM SIGMOD International Conference on Management of Data . 211â€“222.\n[7]Xin Cao, Gao Cong, Christian S Jensen, and Beng Chin Ooi. 2011. Collective spatial keyword querying. In Proceedings\nof the 2011 ACM SIGMOD International Conference on Management of Data . 373â€“384.\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\nWISK: A Workload-aware Learned Index for Spatial Keyword Queries 187:25\n[8]Yankai Cao and Victor M Zavala. 2020. A sigmoidal approximation for chance-constrained nonlinear programs. arXiv\npreprint arXiv:2004.02402 (2020).\n[9]Huanhuan Chen, Peter TiÅˆo, and Xin Yao. 2013. Efficient probabilistic classification vector machine with incremental\nbasis function selection. IEEE Transactions on Neural Networks and Learning Systems 25, 2 (2013), 356â€“369.\n[10] Lisi Chen, Gao Cong, Christian S Jensen, and Dingming Wu. 2013. Spatial keyword query processing: An experimental\nevaluation. Proceedings of the VLDB Endowment 6, 3 (2013), 217â€“228.\n[11] Yen-Yu Chen, Torsten Suel, and Alexander Markowetz. 2006. Efficient query processing in geographic web search\nengines. In Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data . 277â€“288.\n[12] Zhida Chen, Lisi Chen, Gao Cong, and Christian S Jensen. 2021. Location-and keyword-based querying of geo-textual\ndata: a survey. The VLDB Journal 30, 4 (2021), 603â€“640.\n[13] Maria Christoforaki, Jinru He, Constantinos Dimopoulos, Alexander Markowetz, and Torsten Suel. 2011. Text vs.\nspace: efficient geo-search query processing. In Proceedings of the 20th ACM International Conference on Information\nand Knowledge Management . 423â€“432.\n[14] Gao Cong, Kaiyu Feng, and Kaiqi Zhao. 2016. Querying and mining geo-textual data for exploration: Challenges and\nopportunities. In 2016 IEEE 32nd International Conference on Data Engineering Workshops (ICDEW) . IEEE, 165â€“168.\n[15] Gao Cong, Christian S Jensen, and Dingming Wu. 2009. Efficient retrieval of the top-k most relevant spatial web\nobjects. Proceedings of the VLDB Endowment 2, 1 (2009), 337â€“348.\n[16] Ian De Felipe, Vagelis Hristidis, and Naphtali Rishe. 2008. Keyword search on spatial databases. In 2008 IEEE 24th\nInternational Conference on Data Engineering (ICDE) . IEEE, 656â€“665.\n[17] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li, Hantian Zhang, Badrish Chandramouli,\nJohannes Gehrke, Donald Kossmann, et al .2020. ALEX: an updatable adaptive learned index. In Proceedings of the 2020\nACM SIGMOD International Conference on Management of Data . 969â€“984.\n[18] Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020. Tsunami: a learned multi-dimensional index\nfor correlated data and skewed workloads. Proceedings of the VLDB Endowment 14, 2 (2020), 74â€“86.\n[19] Xiaofeng Ding, Yinting Zheng, Zuan Wang, Kim-Kwang Raymond Choo, and Hai Jin. 2022. A learned spatial textual\nindex for efficient keyword queries. Journal of Intelligent Information Systems (2022), 1â€“25.\n[20] Yixiang Fang, Reynold Cheng, Gao Cong, Nikos Mamoulis, and Yun Li. 2018. On spatial pattern matching. In 2018\nIEEE 34th International Conference on Data Engineering (ICDE) . IEEE, 293â€“304.\n[21] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic compressed learned index with\nprovable worst-case bounds. Proceedings of the VLDB Endowment 13, 8 (2020), 1162â€“1175.\n[22] Saheli Ghosh, Tin Vu, Mehrad Amin Eskandari, and Ahmed Eldawy. 2019. UCR-STAR: The UCR spatio-temporal\nactive repository. SIGSPATIAL Special 11, 2 (2019), 34â€“40.\n[23] Tu Gu, Kaiyu Feng, Gao Cong, Cheng Long, Zheng Wang, and Sheng Wang. 2023. The RLR-Tree: A Reinforcement\nLearning Based R-Tree for Spatial Data. Proceedings of the ACM on Management of Data 1, 1 (2023).\n[24] Jun Han and Claudio Moraga. 1995. The influence of the sigmoid function parameters on the speed of backpropagation\nlearning. In International workshop on Artificial neural networks . Springer, 195â€“201.\n[25] Jiawei Han, Jian Pei, and Yiwen Yin. 2000. Mining frequent patterns without candidate generation. ACM SIGMOD\nRecord 29, 2, 1â€“12.\n[26] Ramaswamy Hariharan, Bijit Hore, Chen Li, and Sharad Mehrotra. 2007. Processing spatial-keyword (SK) queries in\ngeographic information retrieval (GIR) systems. In 19th International Conference on Scientific and Statistical Database\nManagement (SSDBM 2007) . IEEE, 16â€“16.\n[27] Tuan-Anh Hoang-Vu, Huy T Vo, and Juliana Freire. 2016. A unified index for spatio-temporal keyword queries. In\nProceedings of the 25th ACM International on Conference on Information and Knowledge Management . 135â€“144.\n[28] Xiao Hu, Yuxi Liu, Haibo Xiu, Pankaj K. Agarwal, Debmalya Panigrahi, Sudeepa Roy, and Jun Yang. 2022. Selectivity\nFunctions of Range Queries are Learnable. In Proceedings of the 2022 ACM SIGMOD International Conference on\nManagement of Data , Zachary Ives, Angela Bonifati, and Amr El Abbadi (Eds.). ACM, 959â€“972.\n[29] Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. 1996. Reinforcement learning: A survey. Journal of\nArtificial Intelligence Research 4 (1996), 237â€“285.\n[30] Xiaomian Kang, Yang Zhao, Jiajun Zhang, and Chengqing Zong. 2020. Dynamic Context Selection for Document-level\nNeural Machine Translation via Reinforcement Learning. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP) . 2242â€“2254.\n[31] Ali Khodaei, Cyrus Shahabi, and Chen Li. 2010. Hybrid indexing and seamless ranking of spatial and textual features\nof web documents. In International Conference on Database and Expert Systems Applications . Springer, 450â€“466.\n[32] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper, Tim Kraska, and Thomas Neumann.\n2020. RadixSpline: a single-pass learned index. In Proceedings of the Third International Workshop on Exploiting Artificial\nIntelligence Techniques for Data Management . 1â€“5.\n[33] Stephen Cole Kleene. 1952. Introduction to metamathematics. (1952).\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\n187:26 Yufan Sheng et al.\n[34] Taisuke Kobayashi and Wendyam Eric Lionel Ilboudo. 2021. T-soft update of target network for deep reinforcement\nlearning. Neural Networks 136 (2021), 63â€“71.\n[35] Samuel Kotz, Tomasz Kozubowski, and Krzysztof PodgÃ³rski. 2001. The Laplace distribution and generalizations: a revisit\nwith applications to communications, economics, engineering, and finance . Springer Science & Business Media.\n[36] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018. The case for learned index structures. In\nProceedings of the 2018 ACM SIGMOD International Conference on Management of Data . 489â€“504.\n[37] Guoliang Li, Xuanhe Zhou, and Lei Cao. 2021. AI meets database: AI4DB and DB4AI. In Proceedings of the 2021 ACM\nSIGMOD International Conference on Management of Data . 2859â€“2866.\n[38] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. 2020. LISA: A learned index structure for spatial data. In\nProceedings of the 2020 ACM SIGMOD International Conference on Management of Data . 2119â€“2133.\n[39] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan\nWierstra. 2015. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971 .\n[40] Lin Ma, Dana Van Aken, Ahmed Hefny, Gustavo Mezerhane, Andrew Pavlo, and Geoffrey J Gordon. 2018. Query-based\nworkload forecasting for self-driving database Management systems. In Proceedings of the 2018 International Conference\non Management of Data . 631â€“645.\n[41] Ahmed R. Mahmood and Walid G. Aref. 2019. Scalable Processing of Spatial-Keyword Queries . Morgan & Claypool\nPublishers.\n[42] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra, Alfons Kemper, Thomas Neumann,\nand Tim Kraska. 2020. Benchmarking learned indexes. Proceedings of the VLDB Endowment 14, 1 (2020), 1â€“13.\n[43] Francisco S Melo. 2001. Convergence of Q-learning: A simple proof. Institute Of Systems and Robotics, Tech. Rep (2001),\n1â€“4.\n[44] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves,\nMartin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al .2015. Human-level control through deep reinforcement\nlearning. Nature 518, 7540 (2015), 529â€“533.\n[45] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learning multi-dimensional indexes. In\nProceedings of the 2020 ACM SIGMOD International Conference on Management of Data . 985â€“1000.\n[46] Andrew Ng, Michael Jordan, and Yair Weiss. 2001. On spectral clustering: Analysis and an algorithm. Advances in\nneural information processing systems 14 (2001).\n[47] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin,\nNatalia Gimelshein, Luca Antiga, et al .2019. Pytorch: An imperative style, high-performance deep learning library.\nAdvances in Neural Information Processing Systems 32.\n[48] Kostas Patroumpas, Dimitrios Skoutas, Georgios Mandilaras, Giorgos Giannopoulos, and Spiros Athanasiou. 2019.\nExposing points of interest as linked geospatial data. In Proceedings of the 16th International Symposium on Spatial and\nTemporal Databases . 21â€“30.\n[49] Marcelo Prates, Pedro HC Avelar, Henrique Lemos, Luis C Lamb, and Moshe Y Vardi. 2019. Learning to solve np-\ncomplete problems: A graph neural network for decision tsp. In Proceedings of the AAAI Conference on Artificial\nIntelligence , Vol. 33. 4731â€“4738.\n[50] Martin L Puterman. 2014. Markov decision processes: discrete stochastic dynamic programming. (2014).\n[51] Jianzhong Qi, Guanli Liu, Christian S Jensen, and Lars Kulik. 2020. Effectively learning spatial indices. Proceedings of\nthe VLDB Endowment 13, 12 (2020), 2341â€“2354.\n[52] Joao B Rocha-Junior, Orestis Gkorgkas, Simon Jonassen, and Kjetil NÃ¸rvÃ¥g. 2011. Efficient processing of top-k spatial\nkeyword queries. In International Symposium on Spatial and Temporal Databases . Springer, 205â€“222.\n[53] David Saad. 1998. Online algorithms and stochastic approximations. Vol. 5. Cambridge Univ. Press Cambridge, UK, 6.\n[54] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser,\nIoannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al .2016. Mastering the game of Go with deep neural\nnetworks and tree search. Nature 529, 7587 (2016), 484â€“489.\n[55] Gleb Skobeltsyn, Toan Luu, Ivana Podnar Zarko, Martin Rajman, and Karl Aberer. 2007. Web text retrieval with a p2p\nquery-driven index. In Proceedings of the 30th annual international ACM SIGIR conference on Research and Development\nin Information Retrieval . 679â€“686.\n[56] Liwen Sun, Michael J Franklin, Sanjay Krishnan, and Reynold S Xin. 2014. Fine-grained partitioning for aggressive\ndata skipping. In Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data . 1115â€“1126.\n[57] Richard S Sutton and Andrew G Barto. 1999. Reinforcement learning: An introduction. Robotica 17, 2 (1999), 229â€“235.\n[58] Panagiotis Tampakis, Dimitris Spyrellis, Christos Doulkeridis, Nikos Pelekis, Christos Kalyvas, and Akrivi Vlachou.\n2021. A Novel Indexing Method for Spatial-Keyword Range Queries. In 17th International Symposium on Spatial and\nTemporal Databases . 54â€“63.\n[59] Hannu Toivonen. 2010. Frequent Itemset. In Encyclopedia of Machine Learning , Claude Sammut and Geoffrey I. Webb\n(Eds.). Springer, 418.\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\nWISK: A Workload-aware Learned Index for Spatial Keyword Queries 187:27\n[60] William TB Uther and Manuela M Veloso. 1998. Tree based discretization for continuous state space reinforcement\nlearning. Aaai/iaai 98 (1998), 769â€“774.\n[61] Subodh Vaid, Christopher B Jones, Hideo Joho, and Mark Sanderson. 2005. Spatio-textual indexing for geographical\nsearch on the web. In International Symposium on Spatial and Temporal Databases . Springer, 218â€“235.\n[62] Haixin Wang, Xiaoyi Fu, Jianliang Xu, and Hua Lu. 2019. Learned index for spatial queries. In 2019 20th IEEE\nInternational Conference on Mobile Data Management (MDM) . IEEE, 569â€“574.\n[63] Xiaoying Wang, Changbo Qu, Weiyuan Wu, Jiannan Wang, and Qingqing Zhou. 2021. Are We Ready For Learned\nCardinality Estimation? Proc. VLDB Endow. 14, 9 (2021), 1640â€“1654.\n[64] Xiaoyang Wang, Ying Zhang, Wenjie Zhang, Xuemin Lin, and Wei Wang. 2014. Selectivity estimation on streaming\nspatio-textual data using local correlations. Proceedings of the VLDB Endowment 8, 2 (2014), 101â€“112.\n[65] Xiaoyang Wang, Ying Zhang, Wenjie Zhang, Xuemin Lin, and Wei Wang. 2014. Selectivity estimation on streaming\nspatio-textual data using local correlations. Proceedings of the VLDB Endowment 8, 2 (2014), 101â€“112.\n[66] Zheng Wang, Cheng Long, and Gao Cong. 2021. Trajectory simplification with reinforcement learning. In 2021 IEEE\n37th International Conference on Data Engineering (ICDE) . IEEE, 684â€“695.\n[67] Christopher JCH Watkins and Peter Dayan. 1992. Q-learning. Machine Learning 8, 3 (1992), 279â€“292.\n[68] Dingming Wu, Man Lung Yiu, Gao Cong, and Christian S Jensen. 2011. Joint top-k spatial keyword query processing.\nIEEE Transactions on Knowledge and Data Engineering 24, 10 (2011), 1889â€“1903.\n[69] Jiacheng Wu, Yong Zhang, Shimin Chen, Jin Wang, Yu Chen, and Chunxiao Xing. 2021. Updatable learned index with\nprecise positions. Proceedings of the VLDB Endowment 14, 8 (2021), 1276â€“1288.\n[70] Wentao Wu, Chi Wang, Tarique Siddiqui, Junxiong Wang, Vivek Narasayya, Surajit Chaudhuri, and Philip A Bernstein.\n2022. Budget-aware Index Tuning with Reinforcement Learning. In Proceedings of the 2022 ACM SIGMOD International\nConference on Management of Data . 1528â€“1541.\n[71] Dingqi Yang, Bingqing Qu, Jie Yang, and Philippe Cudre-Mauroux. 2019. Revisiting user mobility and social relationships\nin lbsns: a hypergraph embedding approach. In Proceedings of the 30th International Conference on World Wide Web .\n2147â€“2157.\n[72] Yang Yang, Ying Zhang, Wenjie Zhang, and Zengfeng Huang. 2019. Gb-kmv: An augmented kmv sketch for approximate\ncontainment similarity search. In 2019 IEEE 35th International Conference on Data Engineering (ICDE) . IEEE, 458â€“469.\n[73] Zongheng Yang, Badrish Chandramouli, Chi Wang, Johannes Gehrke, Yinan Li, Umar Farooq Minhas, Per-Ã…ke Larson,\nDonald Kossmann, and Rajeev Acharya. 2020. Qd-tree: Learning data layouts for big data analytics. In Proceedings of\nthe 2020 ACM SIGMOD International Conference on Management of Data . 193â€“208.\n[74] Chengyuan Zhang, Ying Zhang, Wenjie Zhang, and Xuemin Lin. 2013. Inverted linear quadtree: Efficient top k spatial\nkeyword search. In 2013 IEEE 29th International Conference on Data Engineering (ICDE) . IEEE, 901â€“912.\n[75] Dongxiang Zhang, Kian-Lee Tan, and Anthony KH Tung. 2013. Scalable top-k spatial keyword search. In Proceedings\nof the 16th International Conference on Extending Database Technology . 359â€“370.\n[76] Lixi Zhang, Chengliang Chai, Xuanhe Zhou, and Guoliang Li. 2022. LearnedSQLGen: Constraint-aware SQL Generation\nusing Reinforcement Learning. In Proceedings of the 2020 ACM SIGMOD International Conference on Management of\nData . 945â€“958.\n[77] Songnian Zhang, Suprio Ray, Rongxing Lu, and Yandong Zheng. 2021. SPRIG: A Learned Spatial Index for Range and\nkNN Queries. In 17th International Symposium on Spatial and Temporal Databases . 96â€“105.\n[78] Hang Zhao, Qijin She, Chenyang Zhu, Yin Yang, and Kai Xu. 2021. Online 3D bin packing with constrained deep\nreinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 35. 741â€“749.\n[79] Yinghua Zhou, Xing Xie, Chuang Wang, Yuchang Gong, and Wei-Ying Ma. 2005. Hybrid index structures for location-\nbased web search. In Proceedings of the 14th ACM International Conference on Information and knowledge Management .\n155â€“162.\nAKNN QUERY SUPPORT\nWISK can also support the Boolean kNN (Bk) query without any modification to the index layout.\nA Bkquery on geo-textual data is formed by a set of keywords ğœ“, a spatial query point o, and the\nresult size k. It aims to retrieve kobjects, each of which covers at least one keyword in ğœ“and is\ntop-kclosest to o. To process B ğ‘˜queries, we follow existing works [ 15,68] by using a best-first\nsearch. Here, we compare WISK with two SOTA indexes, WBIR-Tree [ 68] and LSTI, and we use\nthe index layout generated under the default setting. As shown in Figure 23a, the query times\nof WBIR-Tree and our WISK grow with the number of query keywords, which is expected. LSTI\nshows an opposite trend, as it needs to scan more spline points with fewer keywords. We note that\nWISK shows comparable performance with the best baseline results. Figure 23b further shows the\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.\n\n187:28 Yufan Sheng et al.\nquery times when the result size ğ‘˜is varied. WISK and WBIR-Tree show stable performance as ğ‘˜\nincreases, while LSTI degrades rapidly. WISK achieves the best performance when ğ‘˜>15.\n  WBIR-Tree   LSTI  WISK\n13 5 7 9 0100200300N\no. of keywordsQuery time (Âµs)\n(a) Varying no. of query keywords\n51 02 05 0050100150T\nop-kQuery time (Âµs) (b) Varying ğ‘˜\nFig. 23. kNN query time\nNote that these experiments only aim to show that WISK is applicable to the B kquery as well.\nOur work mainly focuses on SKR queries. Optimizing WISK or designing an optimized learned\nindex for other types of queries remains our future work.\nReceived October 2022; revised January 2023; accepted February 2023\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 187. Publication date: June 2023.",
  "textLength": 101474
}