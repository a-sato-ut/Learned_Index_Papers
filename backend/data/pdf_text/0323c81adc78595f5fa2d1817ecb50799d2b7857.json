{
  "paperId": "0323c81adc78595f5fa2d1817ecb50799d2b7857",
  "title": "Intelligent Path-Selection-Aided Decoding of Polar Codes",
  "pdfPath": "0323c81adc78595f5fa2d1817ecb50799d2b7857.pdf",
  "text": "Citation: Cui, H.; Niu, K.; Zhong, S.\nIntelligent Path-Selection-Aided\nDecoding of Polar Codes. Entropy\n2023 ,25, 200. https://doi.org/\n10.3390/e25020200\nAcademic Editor: Erdem Koyuncu\nReceived: 21 October 2022\nRevised: 6 January 2023\nAccepted: 17 January 2023\nPublished: 19 January 2023\nCopyright: © 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nentropy\nArticle\nIntelligent Path-Selection-Aided Decoding of Polar Codes\nHongji Cui\n , Kai Niu * and Shunfu Zhong\nKey Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and\nTelecommunications, Beijing 100876, China\n*Correspondence: niukai@bupt.edu.cn\nAbstract: CRC-aided successive cancellation list (CA-SCL) decoding is a powerful algorithm that\ndramatically improves the error performance of polar codes. Path selection is a major issue that\naffects the decoding latency of SCL decoders. Generally, path selection is implemented using a metric\nsorter, which causes its latency to increase as the list grows. In this paper, intelligent path selection\n(IPS) is proposed as an alternative to the traditional metric sorter. First, we found that in the path\nselection, only the most reliable paths need to be selected, and it is not necessary to completely sort\nall paths. Second, based on a neural network model, an intelligent path selection scheme is proposed,\nincluding a fully connected network construction, a threshold and a post-processing unit. Simulation\nresults show that the proposed path-selection method can achieve comparable performance gain\nto the existing methods under SCL/CA-SCL decoding. Compared with the conventional methods,\nIPS has lower latency for medium and large list sizes. For the proposed hardware structure, IPS’s\ntime complexity is O(klog2(L))where kis the number of hidden layers of the network and Lis the\nlist size.\nKeywords: polar codes; successive cancellation list; path selection; sort; neural network\n1. Introduction\nPolar codes [ 1] were the ﬁrst channel coding method to achieve channel capacity\nwith a low-complexity encoder and decoder. The CA-SCL [ 2] decoding algorithm was\nproposed to improve the performance of polar codes compared to LDPC codes under ﬁnite\ncode length.\nSCL [ 3] and CA-SCL are the most popular decoding algorithms for polar codes.\nHowever, there are two challenges in improving the throughput of SCL decoding: (1) Data\ndependence of successive cancellation in the decoding processes; the decoder can only\ndecode bit by bit. (2) Path selection: usually, a metric sorter is used for path selection.\nIn this paper, we focus on path selection. Path selection selects the best Lcandidate\npaths from all 2Lcandidate paths. Traditionally, the path metric (PM) is used for metric\nsorting, and Lpaths with the smallest path metric are selected as the surviving paths. In\nsoftware simulation, metric sorting can be done using methods such as bubble sorting\nand quick sorting. In hardware implementation, it is necessary to consider the balance of\nhardware resources and latency.\nThe LLR-based path metric and pruned radix-2L sorting were proposed in [ 4]. The\nLLR-based PM has some good properties that can reduce the complexity of sorting. If L\nPMs are completely sorted, the 2LPMs after path extension are partially ordered, and there\nis no need to sort all the 2LPMs. Based on this property, pruned bitonic sorting and bubble\nsorting [5] were proposed, both of which can reduce hardware complexity.\nFocusing on reducing hardware complexity and latency, double thresholding sort-\ning [ 6], odd-even sorting [ 7], pairwise metric sorting [ 8], hybrid bucket sorting [ 9], and\nother methods have been proposed, one after another. It is worth noting that the research\nproposes a non-sorting direct selection strategy that uses bitwise comparison of PMs to\nselect surviving paths and lowers resource complexity and latency [ 10]. However, all of\nEntropy 2023 ,25, 200. https://doi.org/10.3390/e25020200 https://www.mdpi.com/journal/entropy\n\nEntropy 2023 ,25, 200 2 of 16\nthe above methods come at a cost. Low resource consumption and delays will result in\nperformance loss. When the list becomes larger ( L>32), resource consumption and delay\nwill increase sharply, which limits the throughput of the CA-SCL decoder. At the same\ntime, for the polarization-adjusted convolutional (PAC) codes, the authors of [ 11] proposed\na local sorting method that picks only the best path at each state node, thereby greatly\nreducing the sorting latency.\nPermutation entropy [ 12] was proposed in 2002 to analyze the complexity of time\nseries. Certainly, permutation entropy can be used to measure the chaotic degree of the PM\nsequence. By comparing the change in permutation entropy before and after extension, we\ncan get a new measure of the complexity of path selection.\nIn recent years, neural networks have been applied to channel coding and decoding.\nIn this paper, we exploit a neural network to perform path sorting and achieve a high-\nthroughput SCL/CA-SCL decoder. The ﬁrst neural sorting network was proposed in\n1997 [ 13]. It requires the time complexity of O(1), but the latency is still huge for a CA-SCL\ndecoder. In 2020, Tim Kraska proposed an ML-enhanced sorting algorithm called Learned\nSort [ 14], which is based on learned index structures [ 15]. The core of the algorithm is to\ntrain the cumulative distribution function (CDF) model Fover a small sample of data Ato\npredict the position of test data x; the test set size is M:pos=FA(x)\u0001M=P(A\u0014x)\u0001M. In fact,\nit is impossible to train a perfect CDF model, so positional collisions will inevitably occur.\nModel establishment and collision handling methods will affect network performance\nand speed. This kind of neural network is usually used for processing big data, such as\ndatabases.\nInspired by neural network sorting methods, we have designed intelligent path selec-\ntion (IPS) to replace the traditional path sorter. IPS is different from traditional sorters or\nneural sorting networks. Actually, IPS is not a sorter but a binary classiﬁer. There is no need\nto sort from 1 to 2 Lbecause all paths have been divided into good paths and bad ones.\nHere, we give an example. For an unordered array A= [8, 10, 15, 24, 19, 4, 30, 43 ],\nthe perfect CDF for its full sorting is a staircase function, and Figure 1a is one of them.\nHowever, for the path selection of SCL, it only needs to be a step function, and one of the\npossibilities is Figure 1b. The key to the problem is the position of the step. For a one-\ndimensional problem, it is an equivalent problem. If a suitable step position is obtained,\nthe CDF is obtained. On the contrary, getting a CDF means that the suitable step position is\nknown. For the path selection problem, a natural step position is the median. Knowing the\nmedian makes it easy to divide the data into two parts, larger and smaller. However, in\nSCL decoding, the arrays dynamically change, which is why we need a neural network to\nhandle the problem. The neural network will give us a CDF or median that can adapt to\ndynamic arrays. In this study, we chose to train with a target output similar to the CDF.\n0 20 4000.20.40.60.81\n0 20 4000.20.40.60.81\n(a) ( b)\nFigure 1. (a) Fully sorted CDF; ( b) CDF of path selection of SCL.\n\nEntropy 2023 ,25, 200 3 of 16\nIn this way, the system’s complexity will be signiﬁcantly reduced. Next, we designed\na simple neural network for training: a universal neural network that can be adapted\nto different code lengths and code rates. Finally, we designed a threshold and a path\nselection matching strategy to match the network’s output and the SCL/CA-SCL decoder.\nThe hardware structure of the fully connected neural network is designed to have highly\nparallel [16] performance to maintain overall lower latency, and the pipelined structure [17]\nreduces resource consumption and improves the utilization of hardware resources. The\nsimulation results show that compared with the traditional sorting SCL/CA-SCL algorithm,\nIPS has little performance loss and low complexity. To conclude, the innovations of the\nproposed IPS are as follows:\n• We propose a framework that uses permutation entropy to measure the complexity of\npath selection. By comparing different path extension methods, the best path extension\nmethod can be determined. Treating path selection as a binary classiﬁcation problem\nbrings us new solutions.\n• We propose an intelligent path-selection method consisting of a neural network, a\nthreshold, and a matching strategy, which reduces the latency of path selection. The\nsimpler the network, the less the hardware resource complexity and latency.\nThe remainder of paper is as follows. Section 2 gives a short introduction to polar\ncodes and LLR-based SCL/CA-SCL. Section 3 introduces the permutation entropy and the\ndifferences between various path extension schemes. Section 4 shows the design of IPS,\nthe neural network, and the matching strategy. Section 5 provides the simulation results,\nand the conclusions are in Section 6.\n2. Preliminaries\n2.1. Polar Codes\nA polar codeP(N,K)of length Nwith Kinformation bits is constructed by applying\na linear transformation to the message word uN\n1=fu1,u2, . . . , uNgas\nxN\n1=uN\n1F\nn,F=\u00141 0\n1 1\u0015\n, (1)\nwhere xN\n1=fx1,x2,. . .,xNgis the codeword, F\nnis the n-th Kronecker power of the polar-\nizing matrix F, and n=log2N. The message word uN\n1contains a setAofKinformation\nbits and a setFofN\u0000Kfrozen bits. In this paper, frozen bits are selected according to\nthe 5GNR standard. Binary phase-shift keying (BPSK) modulation and an additive white\nGaussian noise (AWGN) channel model are considered.\nyN\n1= (1\u00002xN\n1) +z, (2)\nwhere 1is an all-one vector with size N, and z2RNis the AWGN noise vector with\nvariance s2and a zero mean.\nIn addition, for an (N,KI)CRC-polar concatenated code, the inner code is an (N,K)\npolar code, and the outer code is a (K,KI)CRC code. The CRC-concatenated polar code\nrate is R=KI/N, and the set of information bits is A,jAj=K. The message word\nbKI\n1=fb1,b2, . . . , bKIgis encoded as the CRC encoded codeword cK\n1:\ncK\n1=bKI\n1Gc, (3)\nwhere Gcis the generator matrix generated by the CRC polynomial g(x).KP=K\u0000KIis\nthe length of the CRC check bit.\nInsert the CRC codeword cK\n1into the information sequence uN\n1according to the infor-\nmation bits setA, and obtain the codeword xN\n1after polar encoding.\n\nEntropy 2023 ,25, 200 4 of 16\nIn the log-likelihood ratio (LLR) domain, the LLR vector fL(i)\n1,i=1, 2,. . .,Ngof the\ntransmitted codeword is\nL(i)\n1=ln\u0012W(yij0)\nW(yij1)\u0013\n=2yi\ns2(4)\n2.2. LLR-Based Successive Cancellation List Decoding\nThe SC decoding algorithm can be regarded as a greedy search on the decoding tree.\nIn each decision of the information bit, only the one with the larger posterior probability is\nselected. Obviously, once a bit error occurs in the decoding process, the decoding of the\ncodeword fails. The SCL decoding algorithm is an enhanced version of the SC algorithm\nthat includes a list of candidate paths of size L. In other words, the SCL decoding algorithm\nis a breadth-ﬁrst algorithm on the decoding tree.\nThe SCL decoding algorithm can be divided into three stages. (1) Extend the candidate\npath until the candidate path size is L. (2) Extend the candidate paths to 2L; sort by 2Lpath\nmetrics; select the most reliable Lcandidate paths. (3) The last information bit outputs the\nmost reliable candidate path. In this paper, we discuss how to extend the candidate paths\nand how to select the most reliable Lcandidate paths.\nIn the implementation of the high-throughput CA-SCL decoder, LLR-based and ap-\nproximate calculations are usually used to reduce complexity. LLRs are deﬁned as\nL(i)\nN(yN\n1,ˆui\u00001\n1) =ln \nW(i)\nN(yN\n1,ˆui\u00001\n1j0)\nW(i)\nN(yN\n1,ˆui\u00001\n1j1)!\n. (5)\nThe estimation ˆuiof information bits uiis deﬁned as\nˆui=(0 i/2A,\nd(L(i)\nN)i2A.(6)\nwhere d(x) =1\n2(1\u0000sign(x)).\nThe LLR update rule is\nL(2i\u00001)\nN(yN\n1,ˆu2i\u00002\n1) = f(L(i)\nN/2(yN/2\n1,ˆu2i\u00002\n1,o\bˆu2i\u00002\n1,e), L(i)\nN/2(yN\nN/2+1,ˆu2i\u00002\n1,e)),\nL(2i)\nN(yN\n1,ˆu2i\u00001\n1) =g(L(i)\nN/2(yN/2\n1,ˆu2i\u00002\n1,o\bˆu2i\u00002\n1,e), L(i)\nN/2(yN\nN/2+1,ˆu2i\u00002\n1,e),ˆu2i\u00001),(7)\nwhere the gfunction and the approximate ffunction are\nf(a,b) =sign(a)\u0001sign(b)\u0001minfjaj,jbjg,\ng(a,b,us) = (\u00001)us\u0001a+b.(8)\nThe path metric (PM) for the i-th bit of path lis deﬁned as\nPM(i)\nl=8\n<\n:PM(i\u00001)\nlifˆui[l] =d(L(i)\nN[l]),\nPM(i\u00001)\nl+jL(i)\nN[l]jotherwise.(9)\n3. Path Selection and Permutation Entropy\nIn this section, we analyze the process of path selection in the SCL algorithm and\nestablish a relationship with the permutation entropy.\n3.1. Rethinking Path Selection\nAmong the Lcandidate paths maintained by the SCL algorithm, each path corresponds\nto a PM to represent the reliability of the path. In this paper, the smaller the PM, the more\nreliable the candidate path.\n\nEntropy 2023 ,25, 200 5 of 16\nAssume that the PM metric of the (i\u00001)-th bit isfPM(i\u00001)\nl,l=1, 2, . . .,L,i2Ag ,\nAfter path extension, the PM extension (PME) values are fPME(i\u00001)\nl,l=1, 2,. . ., 2L,i2Ag .\nTo ﬁnd the most reliable Lpaths, the PME values are usually sorted to get fPME0(i\u00001)\nl,\nl=1, 2, . . ., 2L,i2Ag , which satisﬁes PME0(i\u00001)\n1\u0014PME0(i\u00001)\n2\u0014. . .\u0014PME0(i\u00001)\n2L. Keep\ntheLsmallest PME values as the new metric values: fPM(i)\nl=PME0(i\u00001)\nl,l=1, 2, . . .,L,\ni2Ag .\nFor software simulations, we generally do not care how sorting is done. However,\nhardware resource consumption and delay are crucial aspects that must be taken into\naccount while designing hardware.\nFor the expansion from fPM(i\u00001)\nl,l=1, 2,. . .,L,i2Ag tofPME(i\u00001)\nl,l=1, 2,. . ., 2L,\ni2Ag , we discuss the following extension schemes.\nExtension Scheme 1: Extend path lto2l\u00001and 2l, where the hard decision of 2l\u00001\nis zero and the hard decision of 2 lis one.\nPME(i\u00001)\n2l\u00001=8\n<\n:PM(i\u00001)\nlifd(L(i)\nN[l]) = 0,\nPM(i\u00001)\nl+jL(i)\nN[l]jotherwise.(10)\nPME(i\u00001)\n2l=8\n<\n:PM(i\u00001)\nlifd(L(i)\nN[l]) = 1,\nPM(i\u00001)\nl+jL(i)\nN[l]jotherwise.(11)\nIn this way, it is easy to ﬁnd the expanded original path after sorting.\nExtension Scheme 2: Extend path lto2l\u00001and 2l, where the PM of 2l\u00001is smaller\nthan that of 2 l.\nPME(i\u00001)\n2l\u00001=PM(i\u00001)\nl,\nPME(i\u00001)\n2l=PM(i\u00001)\nl+jL(i)\nN[l]j.(12)\nThis is easy to extend, because 2 l\u00001 always remains the same.\nExtension Scheme 3: Extend path ltoland l+L, where the PM of lis smaller than\nl+L.\nPME(i\u00001)\nl=PM(i\u00001)\nl,\nPME(i\u00001)\nl+L=PM(i\u00001)\nl+jL(i)\nN[l]j.(13)\nIn this case, the entire PME remains unchanged for l\u0014L.\nIn hardware implementation, schemes 2 and 3 are both considered. Due to the\npotential size relationship ( PME(i\u00001)\n2l\u00001<PME(i\u00001)\n2lorPME(i\u00001)\nl<PME(i\u00001)\nl+L), the number of\ncomparisons will be reduced.\nFurthermore, not all PMEs require complete sorting. Partial path sorting simpliﬁes\nthe sorting operation and only needs to meet the condition of PME0(i\u00001)\n1\u0014PME0(i\u00001)\n2\u0014. . .\n\u0014PME0(i\u00001)\nL<PME0(i\u00001)\nm ,m=L+1,L+2,. . ., 2L. This means that the number of\ncomparisons can theoretically be reduced further.\nIn the next subsection, we explore a more idealized way of path selection by analyzing\nthe permutation entropy.\n3.2. Analysis Based on Permutation Entropy\nPermutation entropy is used to describe the chaotic degree of a time series, which is\ncalculated by the entropy based on the permutation patterns. A permutation pattern is\ndeﬁned as the order relationship among values of a time series.\n\nEntropy 2023 ,25, 200 6 of 16\n3.2.1. Deﬁnition of Permutation Entropy\nWe use permutation entropy to deﬁne the chaotic degree of a sequence fxtgt=1,...,T.\na vector composed of the n-th subsequent values is constructed:\ns7!(xs+1,xs+2, . . . , xx+n). (14)\nnis the order of permutation entropy. The permutation can be deﬁned as: p= (r0r1. . .rn\u00001),\nwhich satisﬁes\nxs+r0\u0014xs+r1\u0014. . .\u0014xs+rn\u00001(15)\nObviously, there are a total of n!permutation patterns p. For each p, we determine\nthe relative frequency (# means number):\np(p) =#fsj0\u0014s\u0014T\u0000n,(xs+1, . . . , xs+n)has type pg\nT\u0000n+1(16)\nDeﬁnition 1. The permutation entropy is deﬁned as (n \u00152):\nPE(n) =\u0000n!\nå\ni=1p(pi)logp(pi). (17)\nNoting that PE2[0, log n!], a normalized permutation entropy can be deﬁned as:\nPEnorm(n) =\u00001\nlogn!n!\nå\ni=1p(pi)logp(pi). (18)\n3.2.2. Permutation Entropy in Path Selection\nIn the SCL decoder, the size of the PM value is Land the size of the PM value after\nextension is 2 L.\nWe assume that the original sequence fPM(i\u00001)\nl,l=1, 2,. . .,L,i2Ag before extension\nis unordered (order-3 permutation entropy for analysis):\nh0=supPE(3) =log(3!) =log(6). (19)\nThefPME(i\u00001)\nl,l=1, 1, . . ., 2L,i2Ag after Extension Scheme 1 is obviously un-\nordered:\nh1=supPE(3) =log(3!) =log(6). (20)\nHowever, after Extension Scheme 2 , the situation becomes different. Permutation\np= (210)never appears. The maximum permutation entropy becomes\nh2=supPE(3) =log(5). (21)\nExtension Scheme 3 is similar to Extension Scheme 1 but more ordered than\nExtension Scheme 1 :\nh3=h1. (22)\nFigure 2 shows the PME permutation entropy under the same codeword and noise\nconditions. The horizontal axis has the information bits (only if fully extended to Lpaths),\nand the vertical axis has the order-3 permutation entropy. The logarithm is in base 2. From\nthis result, we can ﬁnd many interesting conclusions. (1) As in the previous analysis,\nin most cases, PE(3)ES1<PE(3)ES3<PE(3)ES2. (2) Some speciﬁc information bits\nhave high permutation entropy, and some speciﬁc information bits have low permutation\nentropy. This means that, for a speciﬁc set of information bits, sorting algorithms with\ndifferent complexities can be used to reduce the overall complexity of the algorithm.\n\nEntropy 2023 ,25, 200 7 of 16\n0 10 20 30 40 50\n/uni0000002c/uni00000051/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000045/uni0000004c/uni00000057/uni000000560.40.50.60.70.80.91.0/uni00000033/uni00000048/uni00000055/uni00000050/uni00000058/uni00000057/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000048/uni00000051/uni00000057/uni00000055/uni00000052/uni00000053/uni0000005c/uni00000028/uni00000036/uni00000014\n/uni00000028/uni00000036/uni00000015\n/uni00000028/uni00000036/uni00000016\nFigure 2. Permutation entropy of PME values in P(64, 32 )L=16SCL decoder. ES1/2/3: Extension\nScheme 1/2/3 .\n3.2.3. Ideal-Path-Selection Method\nThe traditional sorting method, whether it involves complete sorting or a partial\nsorting, will get PME0(i\u00001)\n1\u0014PME0(i\u00001)\n2\u0014. . .\u0014PME0(i\u00001)\nL<PME0(i\u00001)\nm ,m=L+1,\nL+2,. . ., 2L, which is PM(i)\n1\u0014PM(i)\n2\u0014. . .\u0014PM(i)\nL,i2A . The new PM values are\ncompletely ordered:\nPE(n)(i)\nTrad=0. (23)\nHowever, the paths in the list do not actually need to be sorted. Due to the existence\nof noise, the PM value cannot fully represent the reliability of the path. Every surviving\npath has the potential to be the correct path. This can be reﬂected in the CA-SCL algorithm,\nwhich selects the path that passes the CRC check instead of the path with the smallest\nPM. Thus, it is not necessary to sort the surviving path every time, and this leads to the\nfollowing corollary.\nCorollary 1. The ideal path selection can be viewed as a binary classiﬁcation, where all paths are\nclassiﬁed as more reliable and less reliable. Assuming that the fPME(i\u00001)\nl,l=1, 2,. . ., 2L,i2Ag\ngoes through the ideal path selection, then it should satisfy\nPME0(i\u00001)\nm<PME0(i\u00001)\nn ,81\u0014m\u0014L,L+1\u0014n\u00142L. (24)\nKeep the Lsmallest PME values as the new metric value fPM(i)\nl=PME0(i\u00001)\nl,l=1, 2, . . .,L,\ni2Ag ; the permutation entropy of PM should satisfy\nPE(n)(i)\nIdeal\u00150. (25)\nThe ideal path selection can be represented using the system model of Figure 3, where\nthe input isfPME(i\u00001)\nl,l=1, 2, . . ., 2L,i2Ag and the output is fd(i\u00001)\nl,l=1, 2, . . ., 2L,\nd2f0, 1gg.d(i\u00001)\nlis the label of the PME; d(i\u00001)\nl=1means this is a more reliable path and\nneeds to be kept. On the contrary, d(i\u00001)\nl=0 means discard it.\nBefore the extension, the path metric has the same complexity, PE(n)(i\u00001). After\ndifferent path selections, different permutation entropies PE(n)(i)are obtained. Obviously,\nthe larger PE(n)(i), the lower the complexity of path selection.\n\nEntropy 2023 ,25, 200 8 of 16\n( 1) ( 1) ( 1)\n1 2 2 (PME ,PME ,...,PME )i i i\nL f− − −\n( 1) ( 1) ( 1)\n1 2 2 PME ,PME ,...,PMEi i i\nL− − −\n( 1) ( 1) ( 1)\n1 2 2, ,..., , {0,1 }i i i\nL d d d d− − −Ideal Path Selection\nFigure 3. Ideal-path-selection system model.\nFinding such a function is almost impossible, but luckily, we can approximate this\nfunction using neural network methods. In the next section, we go into the details of the\ndesign and use of the neural network.\n4. Intelligent-Path-Selection-Aided Decoding Algorithm\nIn this section, we design a general path-selection neural network. We describe\nthe overall IPS structure in the ﬁrst subsection and explain each detail in the following\nsubsections.\n4.1. Intelligent-Path-Selection-Aided Decoder’s Structure\nThe intelligent path selection input is 2LPMEs, and the output is LPMs. We designed\nthe following intelligent path-selection architecture to accomplish these functions.\nAs shown in Figure 4, the input PMEs can be recalculated by the network to obtain\nthe new path reliability metrics fo(i\u00001)\nl,l=1, 2, . . ., 2L,o2(0, 1)g. The larger o(i\u00001)\nlis,\nthe more reliable the path. Next, a threshold divides the paths into good paths ( d(i\u00001)\nl=1)\nand bad paths ( d(i\u00001)\nl=0). Up till now, we have completed the binary classiﬁcation process\nand picked out the most reliable paths. However, there is the small drawback that the\nnumber of the most reliable paths is not always equal to L, which sometimes results in\nwasted resources. Thus, we designed a post-processing unit such that the number of IPS\noutputs is always L.\n !\"#$%&%'&(')!*+$,'-*!&\n.(\"&/.$(#%\"\"!*0'-*!&\n ! !  ! ! .+&1'2%3%#&!(*'4%&5($6' ! !  ! ! \n7*&%33!0%*&'.+&1'2%3%#&!(* .8 9:; .8 9:< \n.8 9:9 \n.8 9:= .8 ;:< \n.8 ;:; .8 <:< \nL !\"#\"$$%$ \n&' (#)*%++,-. ')#%+ \nFigure 4. IPS-aided decoder’s structure.\n\nEntropy 2023 ,25, 200 9 of 16\nIt is worth noting that training does not need to be done online. After the network has\nbeen trained, the parameters are put into the SCL/CA-SCL decoder. Therefore, the com-\nplexity of the training does not affect the latency of the decoding. However, complex\nnetworks are not conducive to hardware implementation. Thus, for the network structure,\nthe simpler the better.\n4.2. Path-Selection Neural Network\n4.2.1. Data Preparation\nFor input vectorfPME l,l=1, 2,. . ., 2Lg,2LPME values are sorted PME01<PME02<\n. . .<PME02L. IfPME l\u0014PME0L, label vector is deﬁned as dl=1; else, dl=0. Thus, the\nsum of label vector d=fdl,l=1, 2, . . . , 2 LgisL.\n4.2.2. Neural Network Model\nWe use a simple neural network as the basic component of IPS (IPS-NN), including a\nnormalization layer, two linear layers, and a sigmoid layer. Each layer of the network has\n2Lneurons. Figure 5 shows the IPS neural network model.\nThe binary cross entropy (BCE) is considered as the loss function:\nBCE=\u00001\n2L2L\nå\nl=1[dllog(ol) + ( 1\u0000dl)log(1\u0000ol)], (26)\nwhere olisl-th output of the IPS-NN.\nThe IPS threshold is a simple switch structure used to convert the network’s output\ndiscrete value ointo a binary value ˆd.\nˆdl=(\n1ol>T,\n0ol\u0014T,l=1, 2, . . . , 2 L, (27)\nwhere Tis the threshold value. Obviously, ˆdl=1means that the metric value of this path is\nsmall, and it is a potential successful decoding path (good path); otherwise, it is a bad path.\n˖\nĂĂ ⾎㓿ݳ \nĂĂ ĂĂ ĂĂ \n䳀㯿ቲ 䰘䲀 ĂĂ ĂĂ \n䗃ࠪቲ 䗃ޕቲ \nĂĂ \nĂĂ 2L  !\"#$%&' \nĂĂ ĂĂ ĂĂ \n()**\"& +,-\"$ ˖\n. +)&\",$ /,-\"$ 0123 \n45$\"'5%/* ĂĂ ĂĂ \n2)67%)* !%$7 \n+,-\"$ \n0123!\"#$,/ !\"89%$: \nĂĂ ; 1<= \n. 1<= \n> 1<= \n. 1<= L o\n o\n o\n Lo d\n d\n d\n Ld\nFigure 5. IPS neural network model.\n4.2.3. IPS-NN Conﬁguration and Results\nWe trained IPS-NN with N=64,R=1/2, L=16. The detailed hyperparameters for\ntraining are shown in Table 1.\n\nEntropy 2023 ,25, 200 10 of 16\nTable 1. Hyperparameters used for training the IPS-NN.\nParameter Value\nOptimizer SGD\nLearning rate 0.01\nTraining SNR ( Eb/N0) 1dB, 100,000 frames\ntraining sets:test sets 3:1\nbatch size 64\nAccuracy 0.9836425\nDeﬁnition 2. The accuracy of the neural network is deﬁned as:\nAcc=å\nS2L\nå\nl=01fˆdl=dlg\n2L\u0001jSj, (28)\nSis test data set andjSjis the size of the test data set. ˆdis the IPS output vector. 1f\u0001gis an indicator\nfunction that takes value one if the argument is true and zero otherwise. We consider each element\nin the vector as the standard of accuracy, rather than the entire vector. Even if the output vector\nis not exactly equal to the label, as long as the correct path is included, this vector is valid for the\nSCL algorithm.\nThe IPS-NN network was implemented using the Pytorch framework. For the test set,\nthe IPS-NN achieved an accuracy of 98.3%. However, this accuracy rate cannot completely\ndetermine the performance of the entire decoder. What is important is the performance of\nthe entire CA-SCL decoder after replacing metric sorting with IPS.\n4.3. Post-Processing Unit\nIn the previous section, we proposed IPS-NN and IPS-Threshold. It is worth noting\nthat the output value of IPS is ˆd. We expect the sum of ˆdto be L, but in fact, the sum of\nˆdis related to the network output and threshold. We need to perform post-processing\noperations on the network’s output.\nFigure 6 shows four sets of PME values selected in the decoding process with N=256,\nR=1/2,L=16. The size of the shape represents the order the input PME value. After\nIPS-NN, four sets of output values in the range (0, 1)are obtained.\n15 20 25 30 35 40 45\nIPS-NN input: PME Value00.20.40.60.81IPS-NN output: o\nThreshold = 0.5Threshold = 0.8\nThreshold = 0.2\nFigure 6. The output of the four sets of PME through the IPS-NN.\n\nEntropy 2023 ,25, 200 11 of 16\nWe can observe two phenomena: (1) The same PME value has different outputs in\ndifferent sets, which shows that the IPS-NN can well adapt to the PME value of the entire\nset. In decoding the different bits, the PM value continues to increase, and the network is\napplicable. (2) The number of IPS output ˆdl=1is related to the threshold value; the larger\nthe threshold, the smaller the output ˆdl=1.\nWe denote the sum of ˆdlasW. In order to solve the effect of the threshold on the output\nresults, there are two solutions: (1) Using a variable threshold, use the dichotomy method\nto ﬁnd the threshold value of when W6=L. (2) Add a compensation strategy to make\nthe decoder output Lpaths when W6=L. Method 1 has good performance but increases\nthe complexity of each decoding stage. Method 2 has some performance losses, but it\nis not sensitive to the decoding threshold and can have a larger threshold range. In this\npaper, we use method 2 and propose a simple matching strategy to make the decoder\nwork smoothly. The matching strategy is intended to be compatible with conventional\nSCL/CA-SCL decoders and does not need to provide additional performance. Our network\nchooses the best paths, even if they are smaller than the size of the list. However, there is\nonly one correct path, and it is most likely in the path we have chosen.\nStrategy 1. (Discard Matching Strategy) : When W6=L, a simple discarding matching\nstrategy is adopted. If W>L, discard some good path; if W<L, discard some decoding\npath. A detailed description of the discard matching strategy is given in Algorithm 1.\nIfW>L, we simply discard the good paths larger than L. IfW<L, we supplement the\nunselected paths at the front of the list with the candidate paths. Obviously, the performance\nof this strategy is related to the extension scheme. We can set a larger threshold to ensure\nW<Lin most cases, and the performance of the matching strategy is determined by the\nextension scheme.\nIn addition to using the discard matching strategy, different strategies, such as random\nselection, can also be used. These strategies will affect the performance of the decoder to a\ncertain extent.\nAlgorithm 1 Discard matching strategy.\nInput: Current 2 Lextended pathfPME(i\u00001)\nl,l=1, 2, . . . , 2 Lg, IPS output ˆd.\nOutput: Lsurvival paths PM fPMi\nl,l=1, 2, . . . , Lg\n1:cnt_pm=1\n2:fors=1, 2, . . . , 2 Ldo\n3: ifˆdl=1 and cnt_pm\u0014Lthen\n4: PMi\ncnt_pm=PME(i\u00001)\ns\n5: cnt_pm=cnt_pm+1\n6: end if\n7:end for\n8:ifW<Lthen\n9: fors=1, 2, . . . , 2 Ldo\n10: ifˆdl=0 and cnt_pm\u0014Lthen\n11: PMi\ncnt_pm=PME(i\u00001)\ns\n12: cnt_pm=cnt_pm+1\n13: end if\n14: end for\n15:end if\n4.4. Hardware Design for IPS-NN\nIn this subsection, we provide a basic parallel NN structure for more convenient\nrepresentation of latency. In the simulation, the norm layer was not necessary, and its\nremoval does not affect performance. As for the sigmoid function, it can be implemented\nusing ROM as a look-up table. The key to the latency is the design of the hidden layer.\n\nEntropy 2023 ,25, 200 12 of 16\nThe design of the hidden layers is based on two basic principles. (1) Parallelism:\nthe nodes of the same layer depend only on the previous layer and can be computed\nsimultaneously. (2) Pipelined operation, as each hidden layer depends only on the output\nof the previous layer; therefore, hardware resources can be reused to achieve pipelined\noperation. Therefore, we only need to design for one hidden layer node.\nThe output of the l-th node at the t-th hidden layer is\not,l=fa(2L\nå\nm=1(wt,m\u0001ot\u00001,m) +bt,l), (29)\nwhere fa()is the activation function, which can be implemented using a look-up table. One\nparallel structure of this hidden nodes is as Figure 7.\nIn the structure, multiplication can be performed in parallel, and the time consumed\nby addition is log2(2L). Thus, the time consumption for a single hidden layer is\nDhidden =1+log2(2L) =2+log2(L). (30)\nIt is worth noting that, unlike the compare-and-swap (CAS) used for the traditional\nsorter, the NN implementation relies on adders and multipliers. With the same structure,\nthe data-bit width also affects the throughput of the hardware.\n\u0011\u0011\u0011 \u0011\u0011\u0011 ,1 tw\n1,1 to-\n,2 tw\n1,2 to-\n1,3 to-,3 tw\n,4 tw\n1,4 to-\n,2 1 t L w-\n1,2 1 t L o- - \n,2 t L w\n1,2 t L o-,tlb\nFigure 7. Structure of the l-th node of the t-th layer.\n5. Simulation Results\nWe put the IPS trained in Section 4 into the SCL/CA-SCL decoder. Note that the L\nPMs of the SCL algorithm using IPS are unordered. Hence, it is necessary to sort from the\nlast bit in order to output the path with the smallest PM. CA-SCL does not require any\nadditional operations.\n5.1. IPS and Extension Scheme Performance Analysis\nThe training data for IPS were generated by P(64, 32 )L=16following Extension\nScheme 2 .Twas the threshold value. Given the codelengths N=64and K=32, CRC\nlength KP=6. Figure 8 shows the block-error-ratio (BLER) performance for different\nextension schemes and thresholds. With the same threshold value T=0.9,Extension\nScheme 1 has very poor performance. The reason is that the original sequence has a large\nsorting entropy, so IPS cannot classify well, and the matching strategy does not make\n\nEntropy 2023 ,25, 200 13 of 16\nup for this deﬁciency. Extension Scheme 3 has the best performance, mainly due to the\nmatching strategy. Performance with different thresholds varies, and larger thresholds\nperform better.\n1 1.5 2 2.5 3 3.5 4 4.5 510-410-310-210-1100BLER\nIPS ES1 T=0.9\nIPS ES2 T=0.9\nIPS ES3 T=0.9\nIPS ES3 T=0.7\nIPS ES3 T=0.5\nFigure 8. Same network, different extension scheme BLER comparison.\n5.2. SCL/CA-SCL Performance Comparison\nFigures 9 and 10 give the BLER performance comparisons for various coding and\ndecoding schemes. CA-SCL uses 6-bit CRC with codelength N=64, and its generator\npolynomial is g(x) = x6+x4+1. CA-SCL uses 24-bit CRC with codelength N=512,\nand its generator polynomial is g(x) = x24+x23+x21+x20+x17+x15+x13+x12+\nx8+x4+x2+x+1. As the ﬁgures show, the curves of SCL and IPS overlap. Under\nthis conﬁguration, IPS can also achieve the performance very close to that of CA-SCL.\nAdditionally, for IPS decoding, the network was trained by P(64, 32 )L=16. The network\ntrained with the same P(64, 32 )L=16used for all different code lengths and code rates.\nTraining for different code lengths and rates can further improve the performance of IPS.\n1 1.5 2 2.5 3 3.5 4 4.5 510-510-410-310-210-1100BLERSCL N64K16\nIPS N64K16\nSCL N64K32\nIPS N64K32\nSCL N64K48\nIPS N64K48\nCA-SCL N64K32C6\nIPS N64K32C6\nFigure 9. BLER performance comparisons for SCL/CA-SCL and IPS with block length N=64and\ndifferent code rates.\n\nEntropy 2023 ,25, 200 14 of 16\n1 1.5 2 2.5 3 3.5 410-610-510-410-310-210-1100BLER\nSCL N512K256\nIPS N512K256\nCA-SCL N512K256C24\nIPS N512K256C24\nFigure 10. BLER performance comparisons for SCL/CA-SCL and IPS with block length N=512and\nR=1/2.\n5.3. Latency of Decoding\nTo evaluate the latency of IPS, we compare the theoretical delay with that of the\nstate-of-the-art hybrid sorter (HS) [ 18] in the SCL algorithm and that of the local sorter [ 11]\nin the list Viterbi algorithm (LVA). The decoding delay of SCL/CA-SCL is divided into\ntwo parts. The ﬁrst part is the decoding delay of SC. For unoptimized IPS and HS, this\npart of the delay is the same, 2(N\u00001). The second part is path selection. In the LVA\nalgorithm, the same path selection is required. Therefore, we only compare the delay of a\nsingle path-selection operation.\nThe delay of the HS is\nTHS=1\n2log2(L)\u0001(1+log2(L)). (31)\nThe IPS delay with khidden layers is\nTIPS=k\u0001(2+log2(L)). (32)\nThe delay of the LVA local sorter is dependent on dividing the total list of size Linto\n2msmall lists ( L0=L/2m):\nTLVA=1\n2\u0012\n1+log2L\n2m\u0013\u0012\n2+log2L\n2m\u0013\n\u0000log2L\n2m, (33)\nthe last term of the formula is due to the fact that the local sorter does not sort with a metric.\nThis means that the local sorter just pick the best path at each state node on the trellis.\nAs shown in Figure 11, compared to HS, IPS has lower latency for large lists, but the\nincrease in speed is much lower than for HS. A local sorter is a special algorithm that only\nsorts at each state nodes on the trellis of the Viterbi algorithm, which results in a large list\nbeing split into smaller lists, reducing complexity. Meanwhile, a similar idea is used in IPS:\nsorting between surviving paths is not necessary, so the local sorter has lower latency than\na small-list sorter.\n\nEntropy 2023 ,25, 200 15 of 16\n3 4 5 6 7 8 9 10510152025303540455055delay (cycle)HS\nk=2 IPS\nm=2 LVA\nm=4 LVA\nFigure 11. Single path-selection operation latency comparison.\n6. Conclusions\nIn this paper, we proposed a new path-selection method. We ﬁrst analyzed the\npermutation entropy of the path metric. With the help of neural networks, we proposed\nan approximation scheme for ideal path selection named IPS. Compared with traditional\nsolutions, IPS showed little performance loss and lower theoretical latency. We believe the\nproposed path-selection method is helpful for building a low-latency SCL decoder.\nAuthor Contributions: Conceptualization, K.N. and H.C.; methodology, K.N. and H.C.; software,\nH.C. and S.Z.; writing—review and editing, H.C.; visualization, H.C.; supervision, K.N.; project\nadministration, K.N.; funding acquisition, K.N. All authors have read and agreed to the published\nversion of the manuscript.\nFunding: This research was funded by the National Natural Science Foundation of China under\nGrant 92067202, Grant 62071058, Grant 62001049.\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability Statement: Not applicable.\nConﬂicts of Interest: The authors declare no conﬂict of interest.\nAbbreviations\nThe following abbreviations are used in this manuscript:\nSCL Successive Cancellation List\nCA-SCL CRC-Aided Successive Cancellation List\nIPS Intelligent Path Selection\nPM/PME Path Metric/Path Metric Extension\nReferences\n1. Arikan, E. Channel polarization: A method for constructing capacity-achieving codes for symmetric binary-input memoryless\nchannels. IEEE Trans. Inf. Theory 2009 ,55, 3051–3073. [CrossRef]\n2. Niu, K.; Chen, K. CRC-aided decoding of polar codes. IEEE Commun. Lett. 2012 ,16, 1668–1671. [CrossRef]\n3. Tal, I.; Vardy, A. List decoding of polar codes. IEEE Trans. Inf. Theory 2015 ,61, 2213–2226. [CrossRef]\n\nEntropy 2023 ,25, 200 16 of 16\n4. Balatsoukas-Stimming, A.; Parizi, M.B.; Burg, A. LLR-based successive cancellation list decoding of polar codes. IEEE Trans.\nSignal Process. 2015 ,63, 5165–5179. [CrossRef]\n5. Balatsoukas-Stimming, A.; Parizi, M.B.; Burg, A. On metric sorting for successive cancellation list decoding of polar codes. In\nProceedings of the 2015 IEEE International Symposium on Circuits and Systems (ISCAS), Lisbon, Portugal, 24–27 May 2015;\npp. 1993–1996.\n6. Fan, Y.; Xia, C.; Chen, J.; Tsui, C.Y.; Jin, J.; Shen, H.; Li, B. A low-latency list successive-cancellation decoding implementation for\npolar codes. IEEE J. Sel. Areas Commun. 2015 ,34, 303–317. [CrossRef]\n7. Kong, B.Y.; Yoo, H.; Park, I.C. Efﬁcient sorting architecture for successive-cancellation-list decoding of polar codes. IEEE Trans.\nCircuits Syst. II Express Briefs 2016 ,63, 673–677. [CrossRef]\n8. Li, H. Enhanced metric sorting for successive cancellation list decoding of polar codes. IEEE Commun. Lett. 2018 ,22, 664–667.\n[CrossRef]\n9. Wang, J.; Hu, Z.; An, N.; Ye, D. Hybrid Bucket Sorting Method for Successive Cancellation List Decoding of Polar Codes. IEEE\nCommun. Lett. 2019 ,23, 1757–1760. [CrossRef]\n10. Shi, S.; Han, B.; Gao, J.L.; Wang, Y.J. Enhanced successive cancellation list decoding of polar codes. IEEE Commun. Lett. 2017 ,\n21, 1233–1236. [CrossRef]\n11. Rowshan, M.; Viterbo, E. List Viterbi decoding of PAC codes. IEEE Trans. Veh. Technol. 2021 ,70, 2428–2435. [CrossRef]\n12. Bandt, C.; Pompe, B. Permutation entropy: A natural complexity measure for time series. Phys. Rev. Lett. 2002 ,88, 174102.\n[CrossRef] [PubMed]\n13. Lin, S.S.; Hsu, S.H. A low-cost neural sorting network with O (1) time complexity. Neurocomputing 1997 ,14, 289–299. [CrossRef]\n14. Kristo, A.; Vaidya, K.; Çetintemel, U.; Misra, S.; Kraska, T. The case for a learned sorting algorithm. In Proceedings of the 2020\nACM SIGMOD International Conference on Management of Data, Portland, OR, USA, 14–19 June 2020 ; pp. 1001–1016.\n15. Kraska, T.; Beutel, A.; Chi, E.H.; Dean, J.; Polyzotis, N. The case for learned index structures. In Proceedings of the 2018\nInternational Conference on Management of Data, Amsterdam, The Netherlands, 30 June–5 July 2018; pp. 489–504.\n16. Zhai, X.; Ali, A.A.S.; Amira, A.; Bensaali, F. MLP neural network based gas classiﬁcation system on Zynq SoC. IEEE Access 2016 ,\n4, 8138–8146. [CrossRef]\n17. Antonyus, P .D.A.; Barros, E.N.D.S. A high performance full pipelined arquitecture of MLP neural networks in FPGA. In\nProceedings of the 2010 17th IEEE International Conference on Electronics, Circuits and Systems, Athens, Greece, 12–15 December\n2010; pp. 742–745.\n18. Wang, K.; Li, L.; Han, F.; Feng, F.; Lin, J.; Fu, Y.; Sha, J. Optimized sorting network for successive cancellation list decoding of\npolar codes. IEICE Electron. Express 2017 ,14, 20170735. [CrossRef]\nDisclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.",
  "textLength": 39241
}