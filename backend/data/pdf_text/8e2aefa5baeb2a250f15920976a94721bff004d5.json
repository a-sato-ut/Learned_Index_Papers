{
  "paperId": "8e2aefa5baeb2a250f15920976a94721bff004d5",
  "title": "Deploying a Steered Query Optimizer in Production at Microsoft",
  "pdfPath": "8e2aefa5baeb2a250f15920976a94721bff004d5.pdf",
  "text": "Deploying a Steered Query Optimizer in Production at Microsoft\nWangda Zhang, Matteo Interlandi, Paul Mineiro, Shi Qiao, Nasim Ghazanfari\nKarlen Lie, Marc Friedman, Rafah Hosn, Hiren Patel, Alekh Jindal\nMicrosoft\nqo-advisor@microsoft.com\nABSTRACT\nModern analytical workloads are highly heterogeneous and mas-\nsively complex, making generic query optimizers untenable for\nmany customers and scenarios. As a result, it is important to spe-\ncialize these optimizers to instances of the workloads. In this paper,\nwe continue a recent line of work in steering a query optimizer to-\nwards better plans for a given workload, and make major strides in\npushing previous research ideas to production deployment. Along\nthe way we solve several operational challenges including, making\nsteering actions more manageable, keeping the costs of steering\nwithin budget, and avoiding unexpected performance regressions\nin production. Our resulting system, QO-Advisor , essentially ex-\nternalizes the query planner to a massive offline pipeline for better\nexploration and specialization. We discuss various aspects of our\ndesign and show detailed results over production SCOPE workloads\nat Microsoft, where the system is currently enabled by default.\nCCS CONCEPTS\n‚Ä¢Information systems ‚ÜíQuery optimization .\nKEYWORDS\nquery optimization, SCOPE, machine learning, contextual bandit\nACM Reference Format:\nWangda Zhang, Matteo Interlandi, Paul Mineiro, Shi Qiao, Nasim Ghaz-\nanfari, Karlen Lie, Marc Friedman, Rafah Hosn, Hiren Patel, Alekh Jindal.\n2022. Deploying a Steered Query Optimizer in Production at Microsoft. In\nProceedings of the 2022 International Conference on Management of Data\n(SIGMOD ‚Äô22), June 12‚Äì17, 2022, Philadelphia, PA, USA. ACM, New York, NY,\nUSA, 14 pages. https://doi.org/10.1145/3514221.3526052\n1 INTRODUCTION\nGeneral purpose query optimization is hitting the end of the road\nin modern cloud-based data processing systems. These systems\nwitness a wide variety of highly complex applications that are very\nhard to optimize globally [ 18], and yet these systems come with\ngeneric query optimizers that were built for all users, scenarios,\nand scales. Or worse, they are tuned for generic benchmarks like\nTPC-H or TPC-DS that often do not represent the real customer\nworkloads [ 35]. As a result, these general purpose query optimizers\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGMOD ‚Äô22, June 12‚Äì17, 2022, Philadelphia, PA, USA\n¬©2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-9249-5/22/06. . . $15.00\nhttps://doi.org/10.1145/3514221.3526052are often far from optimal in their plan choices for a given customer\nand a given workload [ 25]. Therefore, there is a need to specialize\nthe cloud-based query optimizers to the needs of specific users\nand applications at hand, and thus optimizing the workloads more\neffectively, also sometimes referred to as instance optimization [20].\nRecent work on instance optimization have proposed to use\nmachine learning to learn from a given user workload different\ncomponents of a query optimizer that indirectly lead to better query\nplan choices. More ambitiously, Neo [ 27] proposed to replace the\nentire query optimizer with a learned one and producing the query\nplans directly. Given that replacing an entire query optimizer is not\npossible in a real system, the follow-up work, BAO [ 26], proposed\nto steer the optimizer towards better plan choices by providing\nrule hints to navigate the search space better for each query. While\nBAO considered synthetic workloads on PostgreSQL, our previous\nwork [ 29] grounded the ideas in BAO to real-world workloads in\nindustry strength cloud data processing system, SCOPE [ 42], illus-\ntrating both the challenges and the opportunities in an enterprise\nworkload setting. In this paper, we continue this line of work and\ntake the above ideas all the way to production deployment at Mi-\ncrosoft. Along the way, we address three sets of challenges to make\nsuch a query optimizer operational, as discussed below.\nFirst, prior approaches to steering the query optimizer consider\nmultiple rule hints (as many as 250 rules in the SCOPE query opti-\nmizer) at the same time to help the optimizer navigating the search\nspace. The problem, however, is that it is hard to understand or\nexplain what really was wrong and what helped in making the\nbetter choice. Thus, neither the system developers can learn about\nthe quality of their query optimizer code bases, nor the service\noperators explain the impact of the change in case of performance\nregression, nor the users can build confidence on the robustness\nof the steering approach. Second, the experimentation costs in-\nvolved in prior steering approach were non-trivial. This is because\nof random sampling in the space of steering hints which when\nexecuted in a large scale processing system like SCOPE leads to\nsignificant pre-production costs, making it difficult to maintain\nover time. And third, steering the query optimizer to newer unseen\npoints in the search space could lead to performance regression,\nfor newer queries and over time. This is because the estimated\nquery costs do not necessarily lead to better plans due to inaccurate\ncost models [ 29]. These performance regressions are not acceptable\nfor critical workloads in a running system since users expect a\nconsistent system behavior with predictable performance.\nWe present QO-Advisor to overcome the above deployment chal-\nlenges and to steer the optimizer in an explainable, cost-effective,\nand safe manner. Our key ideas include breaking down the steering\nprocess into smaller incremental steps that are easily explainable\nand reversible. We introduce a novel model pipeline configurationarXiv:2210.13625v1  [cs.DB]  24 Oct 2022\n\nwhere a contextual bandit model is used to significantly cut down\nthe pre-production experimentation costs, followed by a cloud vari-\nability model determining performance improvement or regression.\nEssentially, QO-Advisor opens up the core of a query optimizer, i.e.,\nthe query planner, which was historically seen as a black box, and\nexternalizes it for better specialization over a customer workload.\nConsequently, to the best of our knowledge, QO-Advisor is the first\nproduction deployment of steering a query optimizer.\nContributions and Organization. We make the following major\ncontributions in this paper.\n(1)We present a background to the line of work on steering\nquery optimizer, discussing the current state of the art, the\nproductization challenges, and our design principles for\nQO-Advisor . (Section 2)\n(2)We describe the learning principles that we implemented\ninQO-Advisor to overcome the productization challenges.\nThese include using a contextual bandit model to recom-\nmend, and a validation model used to accept or reject sug-\ngested modification to the query plans. (Section 3)\n(3)We discuss several operationalization aspects, including how\nwe integrate with the Azure Personalizer [ 1] service, how\nwe deal with plans having non-tree format, and how we\nfeaturize such plans. (Section 4)\n(4)We show experimental results from production workloads\nand demonstrate the overall impact that customers can ex-\npect, as well as present an in depth analysis of the factors\ncontributing to it. (Section 5)\n(5)We discuss the lessons we learned over time as we imple-\nmented the ideas firstly presented in BAO from a research\nprototype to a production-ready system. (Section 6)\n2 STEERING QO: ONE YEAR LATER\nIn this Section, we describe our journey in the steering query opti-\nmizer research over the last year. We first give some background\non both the problem that QO-Advisor tries to address, and the\nrelevant system details from the SCOPE processing engine at Mi-\ncrosoft (Section 2.1). Then, we discuss the various challenges we\nencountered while taking our initial steering optimizer implemen-\ntation to production deployment (Section 2.2). Finally, we introduce\nQO-Advisor to overcome these challenges (Section 2.3), discuss the\ndesign principles we followed when implementing the QO-Advisor\n(Section 2.4), and provide an overview of the end-to-end pipeline\ninvolved in the steering process (Section 2.5).\n2.1 Background\nWe start with a quick summary of the SCOPE [ 33] data processing\nsystem at Microsoft, followed by a brief overview of the prior at-\ntempt to apply steering optimizer ideas to SCOPE [ 29].QO-Advisor\nbuilds upon that work and extends it for production readiness.\nSCOPE. SCOPE is a large scale distributed data processing sys-\ntem [ 8]. It powers production workloads from a wide range of\nMicrosoft products, processing petabytes of data on hundreds of\nthousands of machines every day. SCOPE uses a SQL-like scripting\nlanguage that is compiled into Direct Acyclic Graphs (DAGs) of\noperators. SCOPE scripts, commonly referred to as jobs, are com-\nposed as a data flow of one or more SQL statements that are stitchedtogether into a single DAG by the SCOPE compiler. The SCOPE opti-\nmizer is implemented similarly to the traditional cascades-style [ 13]\nquery optimizer. As such, it uses a set of rules to transform a log-\nical query plan in a top-down fashion, but it also makes all the\ndecisions required to produce a distributed plan. Examples of such\ndecisions are determining how to partition the inputs, or selecting\nthe optimal amount of parallelism given the number of containers\navailable for the job. The number of concurrent containers used\nby each job is referred to as number of tokens in SCOPE, while the\ntotal amount of tokens used for executing a job is called vertices .\nThe SCOPE optimizer estimates the cost of a plan using a combi-\nnation of data statistics and other heuristics tuned over the years.\nOnce executed, several metrics are logged by the SCOPE runtime.\nCommon metrics of interests are job latency ,vertices count , and\nPNhours (i.e., the sum of the total CPU and I/O time over all ver-\ntices). Finally, SCOPE provides an A/B testing infrastructure (called\nFlighting Service ) which allows to re-run jobs in a pre-production\nenvironment using different engine configurations and compare\nthe performance of those configurations with the default one.\nThe importance of recurring jobs. More than 60% of SCOPE jobs\narerecurring , i.e., periodically arriving template-scripts with differ-\nent input cardinalities and filter predicates [ 17,19] , but same set\nof operators. While each execution of a template can have different\nmetrics such as input cardinalities and selectivities, recurring jobs\nare overall interesting because we can use historical information\non previous executions to improve future occurrences. Because of\nthis, in the remainder of the paper we will focus on recurring jobs.\nWe will touch upon in Section 8 how we are planning to remove\nsuch limitation in future versions of QO-Advisor .\nSCOPE rule-based optimizer. There are 256 rules in the SCOPE\noptimizer. SCOPE rules are divided into 4 categories [ 29]:required\n(which must always be enabled to get valid plans), off-by-default\n(which are disabled by default because experimental or very sensi-\ntive to estimates), on-by-default , and implementation rules (mapping\nlogical operators into physical ones). The default optimizer rule\nconfiguration may return sub-optimal plans for certain job instances\nor workloads, whereas an instance-optimized rule configuration\ncould return better instance-optimal plans. This, for example, can\nbe achieved by turning on an off-by-default rule, or conversely by\nturning off an on-by-default or implementation rule (and therefore\nrestricting a part of the optimizer search space). SCOPE users are\naware of this, and in fact over the years have attempted to use opti-\nmizer hints to override the default query optimizer behavior. This,\nhowever, is a manual task which requires both deep knowledge\nof SCOPE internals, as well as tedious experimentation through\ntrials and errors. Despite this, on a daily basis we see that up to\n9% of jobs have hints overriding the default configuration of the\noptimizer. Applying state-of-the-art techniques such as BAO [ 26]\nis non-trivial, and it will not work because the configuration space\nis just too big: 2256in SCOPE vs just 48considered in BAO.\nTowards instance-optimized rule configurations. To overcome\nthe above problem, in [ 29] we have explored a first step towards\nautomating this manual experimentation task. We analyzed large\nproduction workloads from SCOPE and introduced two new con-\ncepts to apply the steering ideas in such a system:\n\nRule Signature. For each plan, we extended the SCOPE optimizer\nto return a bit vector specifying which rules directly con-\ntributed to it. For example, if only the first and the second\nrule were used during the optimization, then the rule signa-\nture will be 1100000000.\nJob Span. Given a job, we compute a set containing all rules which,\nif enabled or disabled, can affect the final query plan. Intu-\nitively, the span of a job contains all the rules that can lead\nto some change in the final optimized plan. The job span al-\nlows to narrow down the exploration of rule configurations\nby skipping the unworthy ones. The job span is computed\nheuristically, as described in [29].\nGiven the rule signature for a job, and its span, in [ 29] we showed\nthat it is possible to generate instance-optimized rule configurations\nreturning better estimated cost and runtime metrics (up to 90%\nlatency improvement) than using the default rule configuration.\nThe heuristic used in [ 29] for the configuration search is as follows:\n(1)For each job, randomly sample 1000 configurations over that\njob‚Äôs span following a uniform distribution;\n(2)Recompile all generated samples and confront the new esti-\nmated cost returned by the optimizer against the ones from\nthe default rule configuration;\n(3)For all configurations with better cost estimates, pick the 10\nmore promising one, and flight them against the default con-\nfiguration for validating that better cost estimates translate\ninto better runtime metrics;\n(4)Among all the flighted configurations, pick the one with the\nbest runtime metrics improving over the default configura-\ntion (if it exists), and apply such configuration to the next\noccurrence of the recurring job.\nIn this paper, we will close the loop, and describe how we are able\nto achieve fully automated instance-optimized rule configurations\nof SCOPE jobs with QO-Advisor .\n2.2 Productization Challenges\nOver the past one year, we have been trying to deploy in produc-\ntion the heuristics of [ 29] for steering the SCOPE query optimizer.\nHowever, while doing this, we faced several challenges.\nDifficult to debug. Instance-optimized rule configurations are gen-\nerated using uniform sampling. This provides no real insight\non why a configuration is better than another. Consequently,\nwhen an IcM (Incident Management ticket) is raised (e.g.,\nbecause of some unexpected performance regressions) de-\nbugging is almost impossible.\nExpensive to maintain. The amount of resources required to\nmaintain this feature in production is not trivial. In fact, for\neach recurring job we recompile 1000 different rule configu-\nrations, as well as flight (re-run in pre-production environ-\nment) 10of them, which can quickly become very expensive.\nPerformance regressions are hard to catch upfront. There is\nno principled way of detecting regression or bad configu-\nrations upfront. The only guard we have are the estimated\ncosts from the SCOPE optimizer after recompilation (whose\nreliability is well known to be lacking [ 37]) and flighting\n(which does not consider the variance inherent in the clus-\nters, unless we repeat each flighting run several times).2.3 Introduction of QO-Advisor\nWe now give an overview of the QO-Advisor , which addresses the\nabove challenges in SCOPE-like big data query optimizers. Simi-\nlar to index recommendations by an index advisor [ 3,10,11], the\ngoal of QO-Advisor is to recommend better search paths, via rule\nhints, for a query plan. The difference, however, is that while index\nrecommendations are applied offline and appropriate indexes are\ncreated a priori, the search space recommendations are applied on-\nline during the query optimization process itself. Therefore, to keep\nthis process lightweight, QO-Advisor does all the heavy lifting of\ngenerating the appropriate rule hints for different recurring job\ntemplates in an offline pipeline. The idea is to leverage the massive\npast telemetry to explore better query plans in an offline loop, and\nintegrate the recommendations to steer the optimizer to those plans\nin the future. Figure 1 shows how the QO-Advisor pipeline sits next\nto the SCOPE engine and helps steer its search space.\nQO-Advisor opens up a core query optimizer component, namely\nthe query planner, to be explored and improved upon externally\nvia the use of past telemetry and offline compute resources. This\nis a major shift from traditional query planning and allows one\nto think differently. For example, we can consider more expensive\nand more exhaustive search algorithms which one cannot afford\nduring query optimization. We can observe the actual execution\ncosts seen in the past as compared to relying purely on the esti-\nmated costs. We can do more validation offline to anticipate any\nunexpected query plan behavior and even improve upon it before\nactually deploying it to future queries. To top it all, QO-Advisor\nintegrates back seamlessly with the query optimization of future\nqueries in an automated loop, without having users to worry about\nor even notice the externalized query planning component. Thus,\nQO-Advisor introduces a new way to think about query planning\nthat helps scale query optimizers to the complexity and challenges\nthat we see in modern cloud workloads [18].\n2.4 Design Principles\nAs mentioned above, the QO-Advisor is a pipeline of tasks that\nis recurrently triggered every day. QO-Advisor pipeline takes as\ninput historical metadata for a given day, and return a list of job\ntemplate identifiers and rule hint pairs. These pairs are consumed\nby the SCOPE optimizer such that, every time a job matching one\nof the template identifier is found, the provided rule hint is used at\ncompile time to steer the query optimizer.\nSingle rule flip. The first major difference compared to previous\napproaches is that QO-Advisor does not provide full rule configu-\nrations, but it only amends the default SCOPE rule configuration\nby turning on or off a single rule at a time. We made this design de-\ncision for several reasons: (1) the search space is much smaller with\nsingle rule flips and therefore easier to manage and less expensive\nto maintain; (2) in case of performance regression, one single rule\nchange is far more easier to manage/revert compared to arbitrary\nchanges to the rule configurations; (3) this simpler approach is eas-\nier to control in production settings, therefore better for building\nconfidence with the product teams.\nFrom naive to informed experimental design. Uniform sam-\npling over the configuration space generated by the job span is\nexpensive to maintain because it requires the generation of several\n\nFigure 1: Overview of the QO-Advisor pipeline and its inte-\ngration with SCOPE. The pipeline is composed of five tasks\n(Feature Generation, Recommendation, Recompilation, Val-\nidation and Hint Generation) and it uses three services:\nAzure Personalizer, SCOPE Flighting, and Stats and Insight\nService.\nconfigurations, the majority of which are discarded. In QO-Advisor\nwe provide a more principled approach whereby we train a con-\ntextual bandit model over features extracted from the historical\nruns, as detailed in Sections 3 and 4. This allows us to decrease the\nnumber of required re-compilations, as well as flighting runs.\nLearning over estimates rather than runtime metrics. Learn-\ning over query execution times, as in BAO, is hard at SCOPE scale.\nSCOPE daily executes 100,000s of jobs spanning 100,000s of ma-\nchines, whereby: (1) re-executing even a small fraction of them can\nbe prohibitively expensive; and (2) testing configurations without\nany (although imprecise) guardrail introduces the chance of execut-\ning plans with orders of magnitude worse runtime metrics, which,\nat SCOPE scales, means a large waste of resources. Therefore we\ndecide to learn rule configurations over the estimated costs output\nby the SCOPE optimizer. While this approach is not ideal in terms\nof accuracy (since we are learning to improve estimated costs rather\nthan real ones), still it allows us to run at SCOPE scale.\nModels pipeline for avoiding regression. Our learned model\nsuggests new rule configurations by predicting the eventual im-\nprovement on estimated costs. The question on how to avoid regres-\nsions is still open, since (predicted or actual) estimated costs can be\noff compared to the real execution runtime, as previously suggested\nin Section 2.2. To address this, in QO-Advisor we decide to resort\nto a model pipeline whereby a validation model is applied after the\ncontextual bandit model. The goal of the second model is to make\nsure that the selected rule configuration will not create performance\nregressions after execution. To gather runtime measurements, we\nuse flighting. However we flight only a small number of jobs, and\nunder a tight budget. The flighting budget is proportional to how\nstrictly we want to avoid performance regressions.\n2.5 Pipeline Overview\nFigure 1 provides a sketch of the QO-Advisor pipeline and its inte-\ngration with SCOPE. The QO-Advisor pipeline is executed offline\nand daily. It takes as input historical information on job execution\nfor a specified date, and produces a list of pairs (job template, hints)\nwhich are loaded into SCOPE‚Äôs optimizer through the Stats and\nInsight Service (SIS) [ 16]. From the historical runs metadata, the\npipeline first generates a set of features, and then feeds them into aRecommendation task which uses Contextual Bandit [ 24] to sug-\ngest up to one rule hint for each job. These rule hints are then\npassed to the SCOPE optimizer for a Recompilation run. Jobs are\nrecompiled such that: (1) we can catch compilation errors due to the\nnew rule settings upfront; and (2) we can get a new estimated cost\nfor the plan. Jobs with worse estimates are pruned, while for the\nremaining jobs we run an A/B test through the Flighting Service.\nThe output of flighting is then fed into a Validation task that applies\na linear regression model. This provides a guard over performance\nregressions, given that it is crucial for production workloads [ 4].\nThe job templates (and related hints) that pass validation, are then\nwritten into a file by the Hint Generation task, and loaded into SIS,\nwhich, in turn, loads them into the SCOPE optimizer such that the\ngenerated hint is applied to the next occurrence of the job template.\nNext we describe QO-Advisor ‚Äôs training tasks in the follow-\ning section. Thereafter, in Section 4, we will provide insights on\nQO-Advisor ‚Äôs operationlization and integration with SCOPE.\n3 LEARNING PRINCIPLES\nIn this section we describe how QO-Advisor learns to steer the\nSCOPE optimizer. Below we first give a short background to con-\ntextual bandits and then describe our formulation for steering the\noptimizer search space.\n3.1 Introduction to Contextual Bandits\nContextual bandits (CBs) are an extension of supervised learning\nwhere the feedback is limited to the actions made by the learn-\ning algorithm [ 24]. For optimization problems where enumeration\nis impractical, CB is a useful abstraction because they limit the\nevaluation burden to the actions made by the learning algorithm.\nSpecifically, in our problem setting, the burden of computing the\nquality of all plausible rule configurations implies supervised learn-\ning has a very high overhead, but CB learning has low overhead as\nonly the rule configuration selected by the learning algorithm will\nneed to be evaluated.\nCBs achieve statistical guarantees similar to those of supervised\nlearning by constructing an experimental design and then reducing\nit to supervised learning [ 2,12]. However, the use of supervised\nlearning as a subroutine implies CB learning inherits some of the\nissues of supervised learning. In particular, query plan representa-\ntions can have a large impact on performance. We discuss issues of\nrepresentation later in this section.\nThe experimental designs induced by CB algorithms are ran-\ndomized; informally, good actions become increasingly more likely\nunder the experiment design as more data accumulates, but other\nactions still have some likelihood despite their poor historical per-\nformance. Specifically to our setting, a CB algorithm will initially\nchoose rule configurations uniformly-at-random, but as historical\ndata accumulates it will play (what the supervised learning model\nindicates is) the best rules configuration with increasing frequency.\nAlthough the guarantees of CB learning have the same dependence\non data volume as supervised learning, the effective number of data\npoints is proportional, in the worst case, to the number of possible\nactions per example. This motivates our strategy for limiting the\nnumber of actions per example discussed later in this section.\n\nThese aspects of CB learning (limited evaluation requirements\nand informed randomization) are the basis for the improvements\nover our previous approach [29].\n3.2 Contextual Bandit Formulation\nIn CB, the learning algorithm repeatedly receives a context and\naction-set pair(ùë•,{ùëéùëñ}), it chooses an action ùëéùëñ, and then receives\nrewardùëü. To formulate query optimization as a CB problem, we\nmust specify the context, the actions, and the rewards.\nRecommendation. The Recommendation task consumes informa-\ntion available after the default compilation and optionally recom-\nmends an alternative compilation, as shown in Figure 1. For this\ncomponent we use the fractional reduction in (optimizer) estimated\ncost as the reward. Rewards with extreme dynamic ranges can cause\nproblems for existing learning algorithms, so we use clipping to\nmitigate the effects of outliers when training.\nThe specification of the action set {ùëéùëñ}leverages the concepts\nof rule signature and job span introduced in Section 2.1. The main\nstatistical issue is to limit the number of actions per example in\norder to control the amount of data required for learning to con-\nverge. With ùêµpossible bits in the rule signature, there are na√Øvely\n2ùêµpossible actions corresponding to each rule signature. This is\nintractably large so we instead only consider the rule signatures\nwhich differ in the job span. If there are ùëÜbits in the job span for a\nquery, then there are 2ùëÜpossible actions corresponding to possible\nsettings of the span bits. For this work, we limited the action space\nto single bit deviations from the default query plan (both to reduce\ndata requirements for learning and for the reasons in Section 2.4).\nThus the number of possible actions is (1+ùëÜ), corresponding to\neither changing nothing (1) or flipping a single bit in the span ( ùëÜ).\nEmpirically, ùëÜis on average 10 but with a long tail distribution [ 29],\nso having actions scale linearly with ùëÜensures tractability.\nFor the context, any information which is known after the ini-\ntial default compilation can in principle be utilized. We initially\nattempted to compute features directly from the logical query plan\nvia properties of the DAG, but this was not effective. Instead we\nfound representing the logical query by the job span itself to be\nmore effective. In other words, the complete set of bit positions in\nthe job span provides valuable and concise information about which\nbits can be flipped to improve the result, especially when interacted\nto create second and third order co-occurrence indicators. Beyond\nthese features, we found representing some properties of the in-\nput data streams (e.g., row count) provided marginal improvement.\nFurther details are provided in Section 4.2.\nValidation. The Validation component consumes information avail-\nable after a single flighting run of the query plan and decides\nwhether to accept or reject the modified plan. Because (1) there are\nonly two actions, and (2) the reward of reject is known (relative\nchange is 0), we treat this a supervised learning problem and utilize\na model trained on a fixed dataset. Further details are in Section 4.3.\n4 OPERATIONALIZATION\nIn this section we will provide insights on the QO-Advisor imple-\nmentation. We have implemented the QO-Advisor pipeline entirely\nin Python. Additionally, the QO-Advisor interface with externalcomponents, namely SCOPE, Flighting Service, SIS and Azure Per-\nsonalizer [1], use a mix of Python and Powershell commands.\nWe run the QO-Advisor pipeline daily from a set of Windows\nmachines. QO-Advisor is currently deployed for some of SCOPE\nworkloads running on multiple SCOPE clusters, where it is en-\nabled by default, i.e., every job submitted within those workloads\ngets search space recommendations produced by the QO-Advisor\npipeline. We are currently investigating further integration with\nautomation services such as Azure Data Factory (ADF) [5].\nQO-Advisor takes as input a denormalized view of the workload\naggregating all compile time as well as execution information of\njobs run on a given date [ 16]. This view file is generated automat-\nically by an ADF pipeline and is shared across different learned\ncomponents in SCOPE [ 16,17,36,37]. It contains information such\njob identifier, name, a description of the job plan, as well as op-\ntimizer (e.g., estimated cardinalities, estimated cost) and runtime\nstatistics (e.g., latency, PNhours, actual row counts). We defer to\nSection 4.1 below for a detailed description on the features avail-\nable from the above view file, as well as how they are used by our\nmachine learning models in QO-Advisor . This denormalized view\nfor a given date is typically available on COSMOS [ 33] (the big data\nplatform at Microsoft) within 3 days. This means that QO-Advisor\nis triggered over jobs executing not earlier than 3 days from the\ncurrent date.\nQO-Advisor pipeline is composed of five tasks, as depicted in\nFigure 1, namely Feature Generation, rule Recommendation with\nRecompilation, plan Validation, and the final Hint Generation. Be-\nlow, we provide implementation details for each of these tasks.\n4.1 Feature Generation\nIn the first step of the QO-Advisor pipeline, starting from the de-\nnormalized workload view, we run a sequence of SCOPE jobs to\n(1) generate the span for all jobs, (2) return all features necessary\nfor the training of the downstream models.\nJob span generation. We use the same algorithm illustrated in [ 29]\nwithout any further modification. Briefly, the span algorithm imple-\nments an heuristics search whereby only new rules having effect\non the final plan can be discovered. Specifically, for each job we\nstart from the original rule configuration, and we turn on all the\noff-by-default rules, while we turn off all the on-by-default and\nimplementation rules that appear in the original rule signature. We\nthen pass this new rule configuration to the SCOPE optimizer for a\nrecompilation pass. Since some rules that were previously used (and\non) are now off, and some other rules that instead were previously\noff are now available to the optimizer, after recompilation we may\nhave a rule signature with new used rules. We then again turn off\nall the newly used (on-by-default, off-by-default or implementation\nrules) and run the new configuration through the optimizer. This\nprocess is repeated until we reach a fix-point (i.e., no new rule is\nadded to the signature, or the recompilation fails). All jobs that have\nan empty span (i.e., the heuristics cannot find any modification to\nthe default configuration) are not further considered.\nFeature aggregation. An interesting aspect of SCOPE is that each\njob executes a script which can contain one or more queries. There-\nfore, for each job, the optimized plan is a DAG of operators (instead\nof a single tree, as is common for relational database plans), with\n\nTable 1: Features generated for each job, how they are aggre-\ngated, and their source. For each feature we also highlight\nwhether it is a job-level (J) or a query-level (Q) feature. Note\nthat for job-level features we always use minas aggregate\nfunction, since all queries within the same job have the same\nvalue for those features.\nFeature Aggregation Source Level\nNormalized Job Name min Job Metadata J\nRule Signature min Optimizer J\nLatency min Runtime Statistics J\nEstimated Cost min Optimizer J\nQuery Template min Job Metadata Q\nTotal Number of Vertices min Runtime Statistics J\nEstimated Cardinalities sum Optimizer Q\nBytes Read sum Runtime Statistics Q\nMaximum Memory Used min Runtime Statistics J\nAverage Memory Used min Runtime Statistics J\nAverage Row Length avg Optimizer Q\nRow Count sum Optimizer Q\nPNHours min Runtime Statistics J\none or more output nodes, one for each resulting dataset. Each\noutput node can be seen as the root of a tree, whereby the SCOPE\noptimizer and runtime generates some statistics (features) per tree,\nwhile other statistics are generated per script. Examples of these\nper-tree features are row counts, estimated cardinalities and av-\nerage row length. Examples of job-level statistics are PNhours,\nlatency and total number of used containers. Another interesting\naspect of having DAGs instead of trees is that each job could be part\nof different templates, since template information are generated\nper-query tree and not per-job script. Finally, since QO-Advisor\nexecutes at the job granularity (and similarly hints can be provided\nto the SCOPE optimizer at the job level) we found that there is a\ndisconnect between how the features are generated by SCOPE and\nthe format that is needed for QO-Advisor to operate properly.\nTo solve the above challenge, from the raw features and plans\ncontained in the view file, we transform DAGs into trees by gener-\nating a super root node . Super root nodes aggregate all the features\nand information contained in the sub-query trees composing a job.\nFeatures are aggregated in two different ways: (1) following their\nsemantics, e.g., we do sum over estimated cardinalities, and avg\nover average row length; (2) for plan-level features as well as for\ntemplate-level features, we just get the min. Table 1 summarizes all\nthe features generated at this step of the pipeline, and the related\naggregate function. After this step, all features are job level and\nproperly aggregated. We then attach the span information and feed\nthe result to the Recommend task, which we describe next.\n4.2 Rule Recommendation\nAs formulated in Section 3.2, we use a CB model to recommend rule\nflips (i.e., turn off an on-rule or turn on an off-rule) for a specific\njob. More specifically, each action flips a rule in the job span. We\nfeaturize the actions using the rules id and rules category informa-\ntion as introduced in Section 2.1. The context includes the features\nof Table 1 generated from the previous step of the QO-Advisor\npipeline, plus the job span. The reward is the relative change inthe optimizer‚Äôs estimated cost, i.e. the ratio between estimated cost\nin the default setting over the estimated cost after re-compilation\nwith a recommended rule flip. The contextual bandit optimization\nis to maximize the reward: if a rule flip leads to a higher reward, the\nestimated cost after recompilation becomes smaller, so that a plan\nwith lower estimated cost is found. We clip the range of the ratio\nso that extreme values do not overly skew the recommendation\nmodel. Currently we heuristically clip any range greater than 2.0,\ni.e., we clip any plan that is more than 2 √óthe baseline. The clipped\ndelta is then fed back to the contextual bandit model as the reward.\nFor CB learning we use an off-policy learning approach [ 40],\nwhere we gather reward information using the uniform-at-random\npolicy, but for the subsequent steps we act using the learned contex-\ntual bandit policy. This accelerates learning by inducing a maximally\ninformative training dataset, at the cost of doubling the number of\nrule configurations test compilations. Since job recompilations are\nrelatively inexpensive this is an acceptable trade-off.\nIn our implementation, we use the Azure Personalizer service [ 1]\nto generate rule recommendations. Azure Personalizer uses the con-\ntextual bandit approach introduced in Section 3.1 to predict and\nlearn under a specific problem scenario. Azure Personalizer allows\nfor better development in our scenario compared to ad-hoc solution,\nthanks to three main advantages: (1) Azure Personalizer automat-\nically handles all aspects of model management, fault-tolerance\nand high availability; (2) it logs with high fidelity so that we can\ncounter-factually evaluate policies; and (3) it adheres to all Azure\ncompliance and security settings.\n4.3 Validation\nIn the previous rule Recommendation task, we evaluated the rule\nflips (actions) generated by the CB model. However, we found that\nusing estimated costs alone could sometimes lead to performance\nregressions. This is mainly due to the variability inherent in the\ncloud computing system which makes cost estimation incredibly\nhard [ 37]. Interestingly though, we found that the variability prob-\nlem is more severe for the job latency metric and lesser so for the\nPNhours metric. While we will further discuss this finding in Sec-\ntion 5.1, where we run A/A tests to evaluate the variance in SCOPE\nclusters, in summary we found that since PNhours is computed as\nthe sum between CPU time and I/O time, the variablity of I/O time\nacross A/A runs is bounded as data read and data written remain\nconstant. From the above observations we made two conclusions:\n(1)The variance in the cluster is so high that it makes hard to\ninfer improvements in the runtime reliably and programmat-\nically, without having to execute each job several times.\n(2)The total amount of data read and data written are good\npredictors for whether a rule flip introduces improvement or\nnot. Intuitively, if with the new configuration a job read and\nwrite less data, this will likely translate into better runtime.\nIn order to gather information about data read, data written and\nother indicators, we use the SCOPE Flighting Service to test the\nrule flips in a pre-production environment. Next we describe how\nwe use the flight results to validate the recommended actions that\nare output from the contextual bandit model.\nFlighting. Given a list of jobs and their related recommended\nactions, we use the estimated costs (over the recompiled plans\n\nwith the rule configurations embedding the flips suggested by the\nmodel) to heuristically select which jobs to flight. Fighting jobs is\nin fact not just expensive, but it is the larger source of resource\nand time consumption for QO-Advisor . Because of this, we have\na limited budget of machines that can be concurrently used for\nflighting, and we set thresholds on (1) the maximum flighting time\nfor each job (24 hours); (2) the total time budget for flighting; and\n(3) the delta between the estimated cost from recompilation, and\nthe default one. Additionally, we do not flight all jobs that reach\nthe Recompilation step, but instead we flight one representative\njob per template (picked randomly). The intuition here is that jobs\nwith the same template are composed by the same queries, and\ntherefore it is not necessary to flight all of them because they have\nthe same plans and eventually the same rule flip. Finally, we flight\njobs with lower estimated costs first, such that if we finish the total\ntime budget, we are still able to learn and provide some suggestion\neven if we are not able to complete the flighting process in full.\nGiven our limited budget on the number of jobs that can be\nconcurrently flighted, we interact with the SCOPE Flighting Service\nthrough a job queue of fixed size. For each job being flighted, the\nservice can return several different outcomes: (1) failure (e.g., the job\ninformation or the input data expired); (2) timeout; (3) filtered (e.g.,\nif the job belongs to certain classes of jobs that are not supported\nby the Flighting Service); (4) success. Only the jobs/templates that\nare flighted with success are passed to the next step in the pipeline.\nWhile flighting introduces some overhead cost for QO-Advisor , this\ncost is one-time for each job, and amortized over time. The cost is\nalso bound by the available budget, which can be tuned based on\nhow aggressive we want to be in production. In Section 8 we will\ntouch on how we are planning to improve the flighting process for\nincreasing the throughput.\nValidation model. The flighting results go over a Validation step\nfor catching possible regressions before running the new rule con-\nfiguration in production, as suggested in Section 3. In this step we\nrun a linear regression model that learns the PNhours delta given\nthe per job total DataRead andDataWritten features returned by\nflighting. Again, the intuition here is that if a job reads and writes\nless data, then it is likely that the PNhours will be reduced (despite\nthe variance inherent in the cluster). In order to train this super-\nvised validation model, we flight a random subset of the jobs over a\nperiod of 14 days to gather a data set of flighting results. The data\npoints are indexed by their timestamps, so that we can split the\ndataset by date to generate a training set (e.g., data in week0) and\na testing set (data in week1), in order to test whether the trained\nmodel can generalize to other dates temporally.\nThe output of the Validation model is then compared against a\npre-determined safety threshold such that only when the PNhours\ndelta is below this threshold we are actually confident with running\nthe chosen action in production, without causing significant regres-\nsions. The threshold can be increased or decreased based on how\nmuch aggressive we want be. At the moment, for the workloads\nwe are currently running in production, this threshold is set to\n-0.1, meaning that, for a particular job, if the expected reduction in\nPNhours is at least 10%, the job passes the validation.\nFigure 2: Recurring job stability: latency improvements over\nrecurring jobs for week0 cannot always be repeated in\nweek1.\n4.4 Hint Generation\nIn this final step, we gather the validated (job templates, new rule\nconfiguration) pairs and then explode them by applying the same\nconfiguration to all jobs belonging to the same template. The output\nis saved to a file in the Stats and Insight Service (SIS) pre-defined\nformat, and this file is uploaded to the SIS. SIS makes deploying\nmodels and configurations in SCOPE easier as it manages version-\ning and validates the format before installing them in the SCOPE\noptimizer [ 16]. In our case, the SIS picks the file containing jobs and\nrelated rule configurations, and applies them as hints to the SCOPE\noptimizer every time a new instance of the same job template is\nsubmitted in the future.\n5 EXPERIMENTS\nIn this Section, we present several results and insights from one of\nthe production SCOPE workloads, in a pre-production environment.\nBased on these results, the QO-Advisor was enabled by default for\nthis workload beginning November 2021. Throughout this section,\nwe focus on answering the following questions that made the above\ndeployment possible:\n(1)Which runtime metrics are best suited for measuring suc-\ncess? (Section 5.1)\n(2)Can we reliably use compile-time-only information to pre-\ndict runtime outcomes? (Section 5.2)\n(3)Can we reliably predict future runtime outcomes from a\nsingle flight run? (Section 5.3)\n(4)Do we improve over default query plans in aggregate? (Sec-\ntion 5.4)\n(5)What is the impact on query performance in the worst case?\n(Section 5.5)\n(6)Is our biased search using contextual bandit more effective\nthan a uniformly-at-random baseline? (Section 5.6)\n5.1 Metrics\nWhich runtime metrics are best suited for measuring success?\n\nFigure 3: Variance of latency plotted over jobs with different\nexecution time (normalized). Red line is 5% variance. The\nmajority of jobs have high-variance.When we first tested QO-Advisor in pre-production over recur-\nring jobs, we found that even if we can find savings in latency (or\nPNhours) by A/B testing against the default configuration, when\nwe run the same recurring job again on a different week, the sav-\nings cannot always be repeated, as shown in Figure 2. For example,\nfor a job that we found in week0 with 25% reduction in latency\n(ùë•=‚àí0.25), the same recurring jobs running in week1 does not\nhave any latency reduction, and in fact the latency became larger.\nOverall, we found that more than 40% of the jobs will regress when\nre-run one week apart.\nAfter further investigation we found that the above result is\ndue to the variance inherent in the large-scale distributed data\nprocessing systems like SCOPE. Figure 3 shows an A/A test for\nthe same jobs of Figure 2, where we run each of them 10 times\nusing the default rule configurations. We plot the job variance over\nthe normalized job execution time. The red line marks 5% variance\nthat is typically expected by the product teams. As we can see\nfrom the Figure, more than 90% of the jobs have more than 5%\nvariance in latency, with few jobs having over 100% variance. This\nvariance creates problems for both learning and evaluation. For\nlearning, datasets with such high variance contains too much noise.\nFor evaluation, we cannot rely on a single A/B test to determine\nthe performance improvements, and we cannot afford to always\nrun it multiple times, and then take the mean for example. While\nthis variance is intrinsic in distributed processing over a cluster of\nmachines, it is exacerbated by how SCOPE manages resources [ 7].\nA similar conclusion can be drawn for PNhours, as shown in\nFigures 4 and 5. Simply relying on PNhours found previously in\nweek0 will lead to more than 40% regression. The variance here\nlooks more contained however, with less than 50% of jobs having a\nvariance greater than 5%. Since the PNhours metric appears to have\nless variance, and there is no clear signal between estimated cost\nand job latency (as we shall explain next), in the current deployment\nofQO-Advisor we decided to focus on optimizing for PNhours.\n5.2 Estimated Cost vs. Real Performance\nCan we reliably use compile-time-only information to predict run-\ntime outcomes?\nFigure 4: Recurring job stability: savings in PNhours over re-\ncurring jobs for week0 cannot always be repeated in week1.\nFigure 5: Variance of PNhours plotted over jobs with differ-\nent normalized execution time. The red line is 5% variance.\nCompared to the latency plot of Figure 3, PNhours is more\nstable, with more than 50% jobs incurring <5% variance.\nAlthough our initial design did not include a validation compo-\nnent, the results in this section convinced us that some run-time\ninformation was necessary for reliable improvement. Specifically,\nwe tested the rule flips leading to lower estimated costs in an A/B\ntesting using the Flighting Service.\nFigure 6 plots the estimated cost delta (relative difference be-\ntween the estimated cost output of the recompilation with the new\nhints minus the original estimated cost output of the default rule\nconfiguration), and the delta of the latency of 950 jobs run over\n5 days on one of the SCOPE clusters. Our expectation was that\nas the estimated cost delta decreases (i.e., the cost decreases), the\nlatency delta would decrease as well. However as we can see from\nthe figure, there is no real correlation between improvements in\nthe estimated costs and latency. For example, the left-side of the\nfigure contains the jobs with large improvements on the estimated\ncosts, but over 40% of them show actually performance regression.\n\nFigure 6: Delta (new value / old value - 1) in estimated cost\nversus latency. Improvement in estimated cost after recom-\npile do not often translate in improvement in latency.\nNevertheless, estimated cost information are still important for\nthe success of the approach. For instance, we ran an experiment\nwhere we disabled any use of the estimated cost coming from the\nSCOPE optimizer. Specifically, in the Recommendation step we\ndisabled all filters based on estimated costs, and we randomly flip\none rule in the span, instead of using the CB model. What we found\nis that, after three days, QO-Advisor was not able to complete\nflighting (a task that usually is completed within half a day). This\nis because, without using estimated costs to heuristically (1) pick\nrules, and (2) filter which jobs to flight, plans leading to job latencies\norders of magnitude worse than the baseline can be introduced into\nthe pipeline, eventually making fighting impractical.\n5.3 Validation Model Performance\nCan we reliably predict future runtime outcomes from a single\nruntime outcome?\nAs discussed in Section 4.3, we use a Validation step to verify\nthat the recommended rule flip can indeed improve PNhours, with-\nout causing significant regression on other metrics. Since there is\nstill some variance in the PNhours metric, we cannot trust a single\nrun of the A/B testing for recurring jobs (Figure 4), and, instead,\nwe have to build a model to predict future PNhours delta.\nThrough experimentation we found that in addition to the PN-\nhours metric itself, DataRead and DataWritten deltas are good\nindicators of how PNhours delta will change. Figure 7 shows that\nthere is a correlation between DataRead delta and PNhours delta in\nthe historical data we gathered. The dotted blue line demonstrates\nthe trend by a one-dimensional polynomial fit. This result suggests\nthat if we have seen less data read in the A/B testing, then it is likely\nthat the PNhours will also be reduced. Similar observations can be\nfound in Figure 8 for DataWritten. These results corroborate the\nintuition in Section 4.3 that by reducing I/O during job executions,\nwe can reduce the PNhours and achieve better performance.\nUsing the Validation model described in Section 4.3, we can\ncompute the predicted delta of PN hours. Given a certain threshold\n(e.g.,ùëëùëíùëôùë°ùëé <‚àí0.1), we select the jobs with predicted PNhours\nFigure 7: Correlation between DataRead and PNHours. Blue\ndotted line highlights the trend.\nFigure 8: Correlation between DataWritten and PNHours.\nBlue dotted line highlights the trend.\nFigure 9: Predicted PNhours delta vs. Actual PNhours delta.\ndelta qualified for this threshold, and run them in A/B testing to\nvalidate our prediction and check the results on other performance\nmetrics. To compensate for the variance in PNhours metric, we\nusually use a threshold smaller than zero, so that we can avoid\nsome regressions introduced by the variability in the cluster, as\ndiscussed in Section 5.1. Figure 9 demonstrates the accuracy of the\nValidation model (trained on historical data) on 150 jobs in one\n\nday of the test data. For the jobs we predicted with PNhours delta\nsmaller than -0.1, 85% of them have their actual PNhours deltas\nsmaller than -0.1, and 91% of the jobs have their actual PNhours\ndeltas smaller than 0.0 as indicated by the red line.\n5.4 Aggregate Performance\nDo we improve over default query plans in aggregate?\nAs introduced in Section 2.1, metrics of interests in our exper-\niments include PNhours, latency, and vertices count. As just dis-\ncussed in the Section 5.1, in QO-Advisor our primary focus is opti-\nmizing PNhours, which measures the execution cost of a SCOPE job\nin terms of total resources. But our goal is also to avoid significant\nregressions on job latency and vertices count as well.\nTable 2 presents the aggregate pre-production results for one\nSCOPE workload. This workload contains 70 jobs on a single day\nmatching the hints generated by the QO-Advisor pipeline. Typi-\ncally, we expect that around 5% of the unique jobs of the workloads\nwith QO-Advisor enabled can find matches in the hints recommen-\ndation. In aggregate, comparing to the default plans generated by\nthe SCOPE optimizer, on this particular workload we observed\n14.3% savings in total PNhours, 8.9% savings in total job latency,\nand 52.8% savings in vertices count, after using the recommended\nhints for these jobs. Note that we set the threshold of predicted PN\ndelta to -0.1, and the PNhours reductions is roughly in this range.\nTable 2: Pre-production results. % of reduction for\nQO-Advisor compared to the default rules configuration.\nMetric %Reduction\nPNhours -14.3%\nLatency -8.9%\nVertices -52.8%\n5.5 Distribution of Performance Metrics\nWhat is the impact on query performance in the worst case?\nIn this section we drill down on the aggregated results presented\nin the previous Section. Figures 10, 11, and 12 present the per-\nformance changes for the 70 jobs of Section 5.4 on three metrics\n(PNhours, latency, and vertices, respectively) compared with the\ndefault query plan. Note that in each figure, the jobs are ordered\nby the change in that specific metric, and a delta greater than 0.0\nindicates a regression.\nIn Figure 10, we see that over 80% of the jobs have smaller PN-\nhours, indicating savings in resource usage. In the best case on the\nleft part of the figure, the PNhours of a job is reduced by almost 50%.\nConversely, in the worst case, the PNhours of a job is increased by\nat most 15%‚Äîa considerable improvement over the 100% worst case\nregression of our previous work that uses randomly chosen rule\nflips without any validation [29].\nRegarding job latency, in Figure 11 we also observe that about\n80% of jobs have reduced their latency, by a maximum of over 90%.\nWe also see that 20% of jobs have latency increased, but compared\nwith the results in Figure 6 (using estimated cost alone) or Figure 4\n(using recurring jobs without validation), the regression rate is\nFigure 10: Pre-production results drill down: PNhours delta.\nIn the best case, QO-Advisor improves PNhours over the de-\nfault rule configuration by approximately 50%. In the worst\ncase PNhours increases by 15%.\nFigure 11: Pre-production results drill down: latency delta.\nIn the best case QO-Advisor is able to improve latency by 90%.\nIn the worst case, QO-Advisor introduces a regression of\nabout 45%. The larger regression compared to PNhours (Fig-\nure 10) is because QO-Advisor is tuned over PNhours.\nFigure 12: Pre-production results drill down: vertices delta.\nIn this case QO-Advisor introduces a regression of 10% in the\nworst case, while in the best case it can improve vertices uti-\nlization by more than 60%.\nreduced from more than 40% to 20%. However, since in the Vali-\ndation step we primarily focused on optimizing for PNhours, the\nregression on latency is less contained.\n\nTable 3: Comparison between random and CB rule flips.\nNumber of jobs Random Random % CB CB%\nLower cost 377 10.6% 1226 34.5%\nEqual cost 1257 35.4% 1141 32.1%\nHigher cost 1280 36.0% 693 19.5%\nRecompile failures 638 18.0% 492 13.9%\nFinally, for vertices count (Figure 12), we see that only two jobs\nused 10% more vertices. This effect appears to be explainable by\nour approach of optimizing PNhours and reducing the data that\nis read and written during job execution, since the I/O reduction\nmight be a natural result of fewer vertices used for job execution\nwhich consequentially requires less data transmission.\nSo given the small fraction of regressions and the contained\nimpact of the regressed jobs, we consider that QO-Advisor is work-\ning well on production workloads and overall we can reduce the\nPNhours, job latency and vertices for the recurring jobs matching\nour recommendations.\n5.6 Biased Randomization\nIs our biased search using contextual bandit more effective than a\nuniformly-at-random baseline?\nConceivably, the Validation step of the QO-Advisor pipeline might\nbe strong enough to ensure aggregate improvement, even if the\nbiased randomization in the Recommendation component provides\nno improvement. Therefore, we evaluated the effectiveness of the\nrule Recommendation step (Section 4.2) using CB against a baseline\nthat chooses a rule uniformly at random in the action set to flip.\nFor this experiment, we check how often these two approaches can\nfind better query plans in terms of optimizer estimated costs, which\nwe use as the reward to train the reinforcement learning policy.\nFor the workload tier we are currently running QO-Advisor on,\nfrom one single day after feature generation (Section 4.1) around\n66% of the jobs have non-empty job span (i.e. the action set). For\nthese jobs, the random baseline flips one rule out of the span at\nuniform random, while our approach chooses one rule with the\nguidance of the contextual bandit policy. Note that for the contex-\ntual bandit policy, the selected rule flip does not necessarily lead\nto a better plan than the default rule configuration. Thus we al-\nways recompile with the CB‚Äôs selected rule flip and short-circuit\nsubsequent processing if there is no estimated cost improvement.\nFor the same set of jobs in the workload, we recompile with the\nrule flips to check whether the estimated costs of the jobs become\nlower, higher, or remain the same compared to the default rule\nconfigurations. Table 3 shows the number of jobs in each category\nof the cost changes. We observe that using CB increases the number\nof jobs with lower costs by 3 √ó, and reduces the number of jobs\nwith higher costs by almost 2 √ó. It also leads to fewer recompilation\nfailures. Overall, the total estimated cost of this particular workload\nis reduced from 1.7ùëí11using random rule flips to 1.0ùëí09using the\nCB rule flips‚Äîan improvement of over 100 √ó.\n6 LESSONS LEARNED\nFrom research to practice: trade-offs. Applying academic re-\nsearch ideas into production always involves trade-off, and goingfrom the BAO‚Äôs ideas to QO-Advisor was no exception. For example,\nalready in [ 29] we realized that generating new rules configura-\ntions using runtime information at SCOPE scale requires a large\namount of resources. Not just because SCOPE workloads are orders\nof magnitude larger than the one considered in [ 26], but also be-\ncause SCOPE workloads are more complex, and only focusing on\nfew rules (as in BAO) would not work for all use-cases. In our initial\ninvestigation, in fact, we found that there is no subset of rules that\ncaptures all the benefits, whereas all the rules need to be consid-\nered to properly optimize SCOPE workloads. In QO-Advisor we\ntherefore decided to focus on all the 256 rules available in SCOPE,\nand to learn over estimated costs (which can be quickly returned\nby the SCOPE optimizer) rather than actual runtime metrics.\nSimplicity first. Similarly, we found that suggesting arbitrary com-\nplex rule configurations such as in [ 29] was not ideal for our product\npartners because in case of regression it would be almost impossi-\nble to debug. We therefore decided to stage the problem with only\none rule difference from the default rule configuration in the first\nversion. This allowed us to build trust with the product team where\nthis feature works, and also explain better which rules are really\nmoving the needle, something the team could further investigate\nand corroborate based on their domain knowledge.\nDo not reinvent the wheel. In the first prototype of QO-Advisor ,\nwe used Vowpal Wabbit [ 34] for the contextual bandit model. How-\never, as we hardened the system, we realized that maintaining the\nstate over pipeline runs in a reliable way is non-trivial, especially\nif, for explainability, we want to trace how the model evolves and\nlearns over time. Instead of building our own infrastructure, we de-\ncided to integrate QO-Advisor with Azure Personalizer [ 1], which\nmanages the log and state, as well as provides all the enterprise-\ngrade features we needed for our scenario.\nRegressions. We learned that performance regressions are impor-\ntant to catch upfront. At SCOPE scale, if even a 1% of the jobs\nintroduce regression, the number of customer incidents could eas-\nily overwhelm the product team [ 15]. At the same time, we learned\nthat SCOPE clusters have non-negligible variability in performance,\nmaking it practically impossible to effectively catching regression\nwithout extensive A/A and A/B testing. Our trade-off here is to try\nto catch regression in a best effort way, with the limited budged\nallowed for flighting, and by using a machine learning model to val-\nidate the output of flighting before uploading the actions for online\nproduction consumption. In general, live online experimentation is\nexpensive, difficult, or even not allowed to be done in SCOPE-like\nproduction systems that run critical customer workloads. We use\ncounter-factual evaluations where we can rely on past telemetry\noffline to improve learning parameters and to tune the model.\nThe surprising effectiveness of span features. Interestingly, we\nfound that complex featurizations of query plans, as suggested in\nprevious works (e.g., [ 26,27]) were mostly ineffective, whereas the\nuse of the complete job span as context features was critical to our\nsuccess. In particular second and third order coocurrence indicator\nfeatures over the job span. Intuitively, the complete job span con-\ncisely represents the query plan by identifying the set of controls\nwhich affect the optimizer output on this instance . This strategy is\nplausibly reusable in other scenarios where a learning system is\n\nattempting to steer the result of an optimization subroutine in an\ninstance-dependent fashion.\n7 RELATED WORK\nSteering query optimizers. BAO [ 26] generates 48rule configura-\ntions in PostgreSQL that affect the optimizer behaviour for choosing\nscan operators, join operators, and join orders. BAO treats each rule\nconfiguration as an arm in a multi-armed bandit problem and given\na new query, it learns to choose one of the 48arms. BAO models\na reinforcement learning problem in which it sees a sequence of\nqueries, and over time learns to make the correct decisions. BAO\nwas evaluated on a custom dataset with queries ranging from a few\nto several hundred seconds. Also, a core component of BAO is a\ntree convolutional neural network which learns a cost model for\ntree structured PostgreSQL query plans by executing the plans.\nIn our recent work [ 29], we extended the steering optimizer\nresearch by considering how an approach similar to BAO can be\nported over real-world workloads in industry strength cloud data\nprocessing system, SCOPE [ 8]. We show that indeed it is possible to\nimprove SCOPE plans by steering its query optimizer, although this\nrequires substantial experimentation and resources since SCOPE op-\ntimizer rule set is quite large. Another problem that we faced is that,\nwhile plans can be improved, a large portion of the explored plans\nactually introduce regressions. In this work we address both these\nconcerns, and show, through production results, that is possible to\nsteer SCOPE optimizer towards better plans, without necessarily\nintroduce too much regressions.\nLearning-based optimizations in SCOPE. Apart from the scale\nand complexity, one of the other major challenges in SCOPE is effi-\nciency [ 30]. As a result, the Peregrine infrastructure has been built\nover the recent years to introduce workload optimizations in SCOPE\nto optimize multiple jobs and reduce the overall costs [ 16]. Subse-\nquently, several efforts have built on top of Peregrine to introduce\nlearned components in SCOPE. Examples include learned models\nto automatically choose the number of concurrent containers a job\nshould use [ 6,32,36], learned cardinality estimation tailored to the\npast SCOPE workloads [ 41], a learning-based approach for cost\nmodels [37], and even learning-based checkpointing [43].\nInstance-optimized database components. Several recent works\nfocus on improving database components [ 9,21,22,31,39] or gen-\nerating efficient query plans [ 23,27,28] by exploiting learning over\nactual runtimes or the output of an optimizer‚Äôs cost model. While in-\nstance optimization is very appealing for modern cloud customers,\nthere are concerns about its end to end integration, impact on actual\nperformance, and also potential performance regressions [4, 11].\nContextual Bandit, Reinforcement Learning and their appli-\ncations. Applications and systems constantly face decisions that re-\nquire picking from a set of actions based on contextual information.\nReinforcement-based learning algorithms such as contextual ban-\ndits [ 24] can be very effective in these settings. The Azure Person-\nalizer service [ 1] implements several contextual bandits algorithms\nand is deployed to many content recommendation applications such\nas MSN news recommendation and XBox website personalization.\nFor systems, deployed instances of contextual bandit approaches\ninclude Azure Compute that wants to contextually optimize thewaiting time with non-responsive virtual machines in a cloud ser-\nvice, and tuning parameters of components in Skype [14].\n8 LIMITATIONS AND FUTURE WORK\nIn its current version, QO-Advisor has several limitations that we\nplan to improve as we build deployment experience and confidence\nin the pipeline. The first limitation is that QO-Advisor currently\nsuggests only one single rule flip per job. While the simplicity of\nthis solution helps with debugging and with building trust with\nthe product partners, it limits how many plans can be improved. In\nfuture work we will propose multiple rule flips, e.g., by utilizing\ntechniques from combinatorial contextual bandits or short-horizon\nepisodic reinforcement learning [38].\nAnother limitation is the inefficiency of the Validation stage\nof the pipeline, which requires comparing the runtime metrics of\nthe proposed rule change to the default configuration. Even for\nrecurrent queries, where this additional query execution cost can\nbe amortized over the lifetime of the query, the overhead limits\nthe absolute number of proposed rule changes we can accept per\nday due to capacity constraints. In future work we will attempt to\noptimistically accept proposed query plans and detect regressions\nfrom subsequent runtime metrics.\nOther limitations include: (1) we are currently using heuristics\nto pick which jobs to flight, and (2) each pipeline is run on a given\ndate independently from the others. We are exploring how to make\nQO-Advisor stateful such that jobs already explored on a previous\ndate can be skipped, when unmodified, in successive dates. In the\nlong term, we are planning to make flighting part of the model,\nsuch that we can chose which job to flight based on what part of the\nspace the model thinks is better to explore. Eventually, we would\nlike to go beyond recurrent jobs and provide hints for any SCOPE\njobs. This will require a tighter integration between the SCOPE\noptimizer and Azure Personalizer, in an online learning setting\nassisted by an offline pipeline for exploring new configurations.\nFinally, QO-Advisor is just one of many learned components that\nare currently being explored in production to improve SCOPE per-\nformance. It is not clear how these learned components synergize\nwith each other, and this is an exciting area requiring further explo-\nration. For instance, the quality of query plans is directly related to\nthe quality of the estimated costs and cardinalities, but these esti-\nmates can be off by a large margin in practice. QO-Advisor avoids\nthis pitfall by learning how to directly steer the query optimizer\ntowards better plans. Will better learned cardinalities improve the\nplans generated by QO-Advisor , and vice-versa, in a virtuous cycle?\nOr as we add more learned components, will the improvements\ncoming from each feature compound with each other?\n9 CONCLUSION\nWe provide an update on our journey towards making steering\nquery optimizers a reality in production for SCOPE users. We in-\ntroduce QO-Advisor to address several shortcomings of previous\napproaches in a novel way. Specifically, we use a pipeline of models\nto (1) generate new rule configurations 1-edit distance from the\ndefault configuration; and (2) validate A/B testing runs to catch pos-\nsible regressions before production deployment. As of November\n\n2021, QO-Advisor pipeline runs offline periodically (once a day) and\nis currently deployed in production over different SCOPE clusters.\nACKNOWLEDGMENTS\nWe would like to thank Carlo Curino, John Langford, Raghu Ra-\nmakrishnan, and Siddhartha Sen for their insightful feedback, as\nwell as GT Ni for the work during early stages of the development.\nREFERENCES\n[1]Alekh Agarwal, Sarah Bird, Markus Cozowicz, Luong Hoang, John Langford,\nStephen Lee, Jiaji Li, Dan Melamed, Gal Oshri, Oswaldo Ribas, et al .2016. Making\ncontextual decisions with low technical debt. arXiv preprint arXiv:1606.03966\n(2016).\n[2]Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert\nSchapire. 2014. Taming the monster: A fast and simple algorithm for contextual\nbandits. In International Conference on Machine Learning . PMLR, 1638‚Äì1646.\n[3]Sanjay Agrawal, Surajit Chaudhuri, Lubor Kollar, Arun Marathe, Vivek\nNarasayya, and Manoj Syamala. 2005. Database tuning advisor for microsoft sql\nserver 2005. In Proceedings of the 2005 ACM SIGMOD international conference on\nManagement of data . 930‚Äì932.\n[4]Remmelt Ammerlaan, Gilbert Antonius, Marc Friedman, HM Sajjad Hossain,\nAlekh Jindal, Peter Orenberg, Hiren Patel, Shi Qiao, Vijay Ramani, Lucas Rosen-\nblatt, et al .2021. PerfGuard: deploying ML-for-systems without performance\nregressions, almost! Proceedings of the VLDB Endowment 14, 13 (2021), 3362‚Äì3375.\n[5]Microsoft Azure. [n.d.]. Azure Data Factory. https://azure.microsoft.com/en-\nus/services/data-factory/.\n[6]Malay Bag, Alekh Jindal, and Hiren Patel. 2020. Towards plan-aware resource\nallocation in serverless query processing. In 12th USENIX Workshop on Hot Topics\nin Cloud Computing (HotCloud 20) .\n[7]Eric Boutin, Jaliya Ekanayake, Wei Lin, Bing Shi, Jingren Zhou, Zhengping Qian,\nMing Wu, and Lidong Zhou. 2014. Apollo: Scalable and Coordinated Scheduling\nfor Cloud-Scale Computing. In 11th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 14) . 285‚Äì300.\n[8]Ronnie Chaiken, Bob Jenkins, Per-√Öke Larson, Bill Ramsey, Darren Shakib, Simon\nWeaver, and Jingren Zhou. 2008. Scope: easy and efficient parallel processing of\nmassive data sets. Proceedings of the VLDB Endowment 1, 2 (2008), 1265‚Äì1276.\n[9]Carlo Curino, Neha Godwal, Brian Kroth, Sergiy Kuryata, Greg Lapinski, Siqi Liu,\nSlava Oks, Olga Poppe, Adam Smiechowski, Ed Thayer, et al .2020. MLOS: An\nInfrastructure for Automated Software Performance Engineering. In Proceedings\nof the Fourth International Workshop on Data Management for End-to-End Machine\nLearning . 1‚Äì5.\n[10] Debabrata Dash, Neoklis Polyzotis, and Anastasia Ailamaki. 2011. CoPhy: A\nScalable, Portable, and Interactive Index Advisor for Large Workloads. Proceedings\nof the VLDB Endowment 4, 6 (2011).\n[11] Bailu Ding, Sudipto Das, Ryan Marcus, Wentao Wu, Surajit Chaudhuri, and\nVivek R Narasayya. 2019. Ai meets ai: Leveraging query executions to improve\nindex recommendations. In Proceedings of the 2019 International Conference on\nManagement of Data . 1241‚Äì1258.\n[12] Dylan J Foster, Claudio Gentile, Mehryar Mohri, and Julian Zimmert. 2020. Adapt-\ning to misspecification in contextual bandits. Advances in Neural Information\nProcessing Systems 33 (2020), 11478‚Äì11489.\n[13] Goetz Graefe. 1995. The cascades framework for query optimization. IEEE Data\nEng. Bull. 18, 3 (1995), 19‚Äì29.\n[14] Jayant Gupchup, Ashkan Aazami, Yaran Fan, Senja Filipi, Tom Finley, Scott Inglis,\nMarcus Asteborg, Luke Caroll, Rajan Chari, Markus Cozowicz, Vishak Gopal,\nVinod Prakash, Sasikanth Bendapudi, Jack Gerrits, Eric Lau, Huazhou Liu, Marco\nRossi, Dima Slobodianyk, Dmitri Birjukov, Matty Cooper, Nilesh Javar, Dmitriy\nPerednya, Sriram Srinivasan, John Langford, Ross Cutler, and Johannes Gehrke.\n2020. Resonance: Replacing Software Constants with Context-Aware Models in\nReal-time Communication. In NeurIPS 2020 .\n[15] Alekh Jindal and Matteo Interlandi. 2021. Machine Learning for Cloud Data\nSystems: the Promise, the Progress, and the Path Forward. Proc. VLDB Endow. 14,\n12 (2021), 3202‚Äì3205. http://www.vldb.org/pvldb/vol14/p3202-jindal.pdf\n[16] Alekh Jindal, Hiren Patel, Abhishek Roy, Shi Qiao, Zhicheng Yin, Rathijit Sen,\nand Subru Krishnan. 2019. Peregrine: Workload optimization for cloud query\nengines. In Proceedings of the ACM Symposium on Cloud Computing . 416‚Äì427.\n[17] Alekh Jindal, Shi Qiao, Hiren Patel, Zhicheng Yin, Jieming Di, Malay Bag, Marc\nFriedman, Yifung Lin, Konstantinos Karanasos, and Sriram Rao. 2018. Com-\nputation reuse in analytics job service at microsoft. In Proceedings of the 2018\nInternational Conference on Management of Data . 191‚Äì203.\n[18] Alekh Jindal, Shi Qiao, Rathijit Sen, and Hiren Patel. 2021. Microlearner: A\nfine-grained Learning Optimizer for Big Data Workloads at Microsoft. In 2021\nIEEE 37th International Conference on Data Engineering (ICDE) . IEEE, 2423‚Äì2434.[19] Sangeetha Abdu Jyothi, Carlo Curino, Ishai Menache, Shravan Matthur Narayana-\nmurthy, Alexey Tumanov, Jonathan Yaniv, Ruslan Mavlyutov, Inigo Goiri, Subru\nKrishnan, Janardhan Kulkarni, et al .2016. Morpheus: Towards Automated\n{SLOs}for Enterprise Clusters. In 12th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 16) . 117‚Äì134.\n[20] Tim Kraska. 2021. Towards instance-optimized data systems. Proceedings of the\nVLDB Endowment 14, 12 (2021).\n[21] Tim Kraska, Mohammad Alizadeh, Alex Beutel, H Chi, Jialin Ding, Ani Kristo,\nGuillaume Leclerc, Samuel Madden, Hongzi Mao, and Vikram Nathan. 2019.\nSageDB: A Learned Database System. In CIDR .\n[22] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings of the 2018 international\nconference on management of data . 489‚Äì504.\n[23] Sanjay Krishnan, Zongheng Yang, Ken Goldberg, Joseph Hellerstein, and Ion\nStoica. 2018. Learning to optimize join queries with deep reinforcement learning.\narXiv preprint arXiv:1808.03196 (2018).\n[24] John Langford and Tong Zhang. 2007. The epoch-greedy algorithm for multi-\narmed bandits with side information. Advances in neural information processing\nsystems 20 (2007).\n[25] Viktor Leis, Andrey Gubichev, Atanas Mirchev, Peter Boncz, Alfons Kemper, and\nThomas Neumann. 2015. How good are query optimizers, really? Proceedings of\nthe VLDB Endowment 9, 3 (2015), 204‚Äì215.\n[26] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Nesime Tatbul, Mohammad Al-\nizadeh, and Tim Kraska. 2021. Bao: Making learned query optimization practical.\nInProceedings of the 2021 International Conference on Management of Data . 1275‚Äì\n1288.\n[27] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh,\nTim Kraska, Olga Papaemmanouil, and Nesime Tatbul. 2019. Neo: a learned\nquery optimizer. Proceedings of the VLDB Endowment 12, 11 (2019), 1705‚Äì1718.\n[28] Ryan Marcus and Olga Papaemmanouil. 2018. Deep reinforcement learning for\njoin order enumeration. In Proceedings of the First International Workshop on\nExploiting Artificial Intelligence Techniques for Data Management . 1‚Äì4.\n[29] Parimarjan Negi, Matteo Interlandi, Ryan Marcus, Mohammad Alizadeh, Tim\nKraska, Marc Friedman, and Alekh Jindal. 2021. Steering query optimizers: A\npractical take on big data workloads. In Proceedings of the 2021 International\nConference on Management of Data . 2557‚Äì2569.\n[30] Hiren Patel, Alekh Jindal, and Clemens Szyperski. 2019. Big Data Processing at\nMicrosoft: Hyper Scale, Massive Complexity, and Minimal Cost. In Proceedings of\nthe ACM Symposium on Cloud Computing . 490‚Äì490.\n[31] Andrew Pavlo, Gustavo Angulo, Joy Arulraj, Haibin Lin, Jiexi Lin, Lin Ma,\nPrashanth Menon, Todd C Mowry, Matthew Perron, Ian Quah, et al .2017. Self-\nDriving Database Management Systems.. In CIDR , Vol. 4. 1.\n[32] Anish Pimpley, Shuo Li, Anubha Srivastava, Vishal Rohra, Yi Zhu, Soundararajan\nSrinivasan, Alekh Jindal, Hiren Patel, Shi Qiao, and Rathijit Sen. 2021. Optimal\nresource allocation for serverless queries. arXiv preprint arXiv:2107.08594 (2021).\n[33] Conor Power, Hiren Patel, Alekh Jindal, Jyoti Leeka, Bob Jenkins, Michael Rys, Ed\nTriou, Dexin Zhu, Lucky Katahanas, Chakrapani Bhat Talapady, et al .2021. The\ncosmos big data platform at Microsoft: over a decade of progress and a decade to\nlook forward. Proceedings of the VLDB Endowment 14, 12 (2021), 3148‚Äì3161.\n[34] Yahoo! Research. [n.d.]. Vowpal Wabbit. https://vowpalwabbit.org/research.html.\n[35] Abhishek Roy, Alekh Jindal, Priyanka Gomatam, Xiating Ouyang, Ashit Gosalia,\nNishkam Ravi, Swinky Mann, and Prakhar Jain. 2021. SparkCruise: workload\noptimization in managed spark clusters at Microsoft. Proceedings of the VLDB\nEndowment 14, 12 (2021), 3122‚Äì3134.\n[36] Rathijit Sen, Alekh Jindal, Hiren Patel, and Shi Qiao. 2020. Autotoken: Predicting\npeak parallelism for big data analytics at microsoft. Proceedings of the VLDB\nEndowment 13, 12 (2020), 3326‚Äì3339.\n[37] Tarique Siddiqui, Alekh Jindal, Shi Qiao, Hiren Patel, and Wangchao Le. 2020.\nCost models for big data query processing: Learning, retrofitting, and our findings.\nInProceedings of the 2020 ACM SIGMOD International Conference on Management\nof Data . 99‚Äì113.\n[38] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-\nduction . MIT press.\n[39] Dana Van Aken, Andrew Pavlo, Geoffrey J Gordon, and Bohan Zhang. 2017.\nAutomatic database management system tuning through large-scale machine\nlearning. In Proceedings of the 2017 ACM international conference on management\nof data . 1009‚Äì1024.\n[40] Yu-Xiang Wang, Alekh Agarwal, and Miroslav Dudƒ±k. 2017. Optimal and adaptive\noff-policy evaluation in contextual bandits. In International Conference on Machine\nLearning . PMLR, 3589‚Äì3597.\n[41] Chenggang Wu, Alekh Jindal, Saeed Amizadeh, Hiren Patel, Wangchao Le, Shi\nQiao, and Sriram Rao. 2018. Towards a learning optimizer for shared clouds.\nProceedings of the VLDB Endowment 12, 3 (2018), 210‚Äì222.\n[42] Ming-Chuan Wu, Jingren Zhou, Nicolas Bruno, Yu Zhang, and Jon Fowler. 2012.\nScope playback: self-validation in the cloud. In Proceedings of the Fifth Interna-\ntional Workshop on Testing Database Systems . 1‚Äì6.\n[43] Yiwen Zhu, Matteo Interlandi, Abhishek Roy, Krishnadhan Das, Hiren Patel,\nMalay Bag, Hitesh Sharma, and Alekh Jindal. 2021. Phoebe: a learning-based\n\ncheckpoint optimizer. Proceedings of the VLDB Endowment 14, 11 (2021), 2505‚Äì\n2518.",
  "textLength": 78937
}