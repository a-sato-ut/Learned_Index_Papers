{
  "paperId": "5ba52bbe1101939c490a06cc0cf316a09000834e",
  "title": "Neo: A Learned Query Optimizer",
  "pdfPath": "5ba52bbe1101939c490a06cc0cf316a09000834e.pdf",
  "text": "Neo: A Learned Query Optimizer\nRyan Marcus1, Parimarjan Negi2, Hongzi Mao2, Chi Zhang1,\nMohammad Alizadeh2, Tim Kraska2, Olga Papaemmanouil1, Nesime Tatbul23\n1Brandeis University2MIT3Intel Labs\nABSTRACT\nQuery optimization is one of the most challenging problems\nin database systems. Despite the progress made over the\npast decades, query optimizers remain extremely complex\ncomponents that require a great deal of hand-tuning for\nspeci\fc workloads and datasets. Motivated by this short-\ncoming and inspired by recent advances in applying machine\nlearning to data management challenges, we introduce Neo\n(Neural Optimizer ), a novel learning-based query optimizer\nthat relies on deep neural networks to generate query exe-\ncutions plans. Neo bootstraps its query optimization model\nfrom existing optimizers and continues to learn from incom-\ning queries, building upon its successes and learning from its\nfailures. Furthermore, Neo naturally adapts to underlying\ndata patterns and is robust to estimation errors. Experimen-\ntal results demonstrate that Neo, even when bootstrapped\nfrom a simple optimizer like PostgreSQL, can learn a model\nthat o\u000bers similar performance to state-of-the-art commer-\ncial optimizers, and in some cases even surpass them.\n1. INTRODUCTION\nIn the face of a never-ending deluge of machine learning\nsuccess stories, every database researcher has likely won-\ndered if it is possible to learn a query optimizer. Query op-\ntimizers are key to achieving good performance in database\nsystems, and can speed up the execution time of queries by\norders of magnitude. However, building a good optimizer\ntoday takes thousands of person-engineering-hours, and is\nan art only a few experts fully master. Even worse, query\noptimizers need to be tediously maintained, especially as the\nsystem's execution and storage engines evolve. As a result,\nnone of the freely available open-source query optimizers\ncome close to the performance of the commercial optimizers\no\u000bered by IBM, Oracle, or Microsoft.\nDue to the heuristic-based nature of query optimization,\nthere have been many attempts to improve query optimizers\nthrough learning over the last several decades. For exam-\nple, almost two decades ago, Leo, DB2's LEarning Opti-\nmizer, was proposed [54]. Leo learns from its mistakes by\nadjusting its cardinality estimations over time. However,\nLeo still requires a human-engineered cost model, a hand-\npicked search strategy, and a lot of developer-tuned heuris-\ntics, which take years to develop and perfect. Furthermore,\nLeo only learns better cardinality estimations, but not new\noptimization strategies (e.g., how to account for uncertainty\nin cardinality estimates, operator selection, etc.).\nMore recently, the database community has started to ex-\nplore how neural networks can be used to improve query op-timizers [35,60]. For example, DQ [22] and ReJOIN [34] use\nreinforcement learning combined with a human-engineered\ncost model to automatically learn search strategies to ex-\nplore the space of possible join orderings. While these papers\nshow that learned search strategies can outperform conven-\ntional heuristics on the provided cost model, they do not\nshow a consistent or signi\fcant impact on actual query per-\nformance. Moreover, they still rely on the optimizer's heuris-\ntics for cardinality estimation, physical operator selection,\nand estimating the cost of candidate execution plan.\nOther approaches demonstrate how machine learning can\nbe used to achieve better cardinality estimates [20, 43, 44].\nHowever, none demonstrate that their improved cardinality\nestimations actually lead to better query plans. It is rel-\natively easy to improve the average error of a cardinality\nestimation, but much harder to improve estimations for the\ncases that actually improve query plans [26]. Furthermore,\ncardinality estimation is only one component of an opti-\nmizer. Unlike join order selection, identifying join operators\n(e.g., hash join, merge join) and selecting indexes cannot be\n(entirely) reduced to cardinality estimation. Finally, Skin-\nnerDB showed that adaptive query processing strategies can\nbene\ft from reinforcement learning, but requires a special-\nized query execution engine, and cannot bene\ft from oper-\nator pipelining or other advanced parallelism models [56].\nIn other words, none of the recent machine-learning-based\napproaches come close to learning an entire optimizer, nor\ndo they show how their techniques can achieve state-of-the-\nart performance (to the best of our knowledge, none of these\napproaches compare with a commercial optimizer). Showing\nthat an entire optimizer can be learned remains an impor-\ntant milestone and has far reaching implications. Most\nimportantly, if a learned query optimizer could achieve per-\nformance comparable to commercial systems after a short\namount of training, the amount of human development time\nto create a new query optimizer will be signi\fcantly reduced.\nThis, in turn, will make good optimizers available to a much\nbroader range of systems, and could improve the perfor-\nmance of thousands of applications that use open-source\ndatabases today. Furthermore, it could change the way\nquery optimizers are built, replacing an expensive stable of\nheuristics with a more holistic optimization problem. This\nshould result in better maintainability, as well as lead to op-\ntimizers that will truly learn from their mistakes and adjust\ntheir entire strategy for a particular customer instance to\nachieve instance optimality [21].\nIn this work, we take a signi\fcant step towards the mile-\nstone of building an end-to-end learned optimizer with state-\n1arXiv:1904.03711v1  [cs.DB]  7 Apr 2019\n\nof-the-art performance. To the best of our knowledge, this\nwork is the \frst to show that an entire query optimizer can be\nlearned. Our learned optimizer is able to achieve similar per-\nformance to state-of-the-art commercial optimizers, e.g., Or-\nacle and Microsoft, and sometimes even surpass them. This\nrequired overcoming several key challenges, from capturing\nquery semantics as vectors, processing tree-based query plan\nstructures, designing a search strategy, incorporating physi-\ncal operator and index selection, replacing human-engineered\ncost models with a neural network, adopting reinforcement\nlearning techniques to continuously improve, and signi\f-\ncantly shorting the training time for a given database. All\nthese techniques were integrated into the \frst end-to-end\nlearned query optimizer, called Neo(Neural Optimizer ).\nNeo learns to make decisions about join ordering, physical\noperator selection, and index selection. However, we have\nnot yet reached the milestone of learning these tasks from\nscratch. First, Neo still requires a-priori knowledge about all\npossible query rewrite rules (this guarantees semantic cor-\nrectness and the number of rules are usually small). Second,\nwe restrict Neo to project-select-equijoin-aggregate-queries\n(though, our framework is general and can easily be ex-\ntended). Third, our optimizer does not yet generalize from\none database to another, as our features are speci\fc to a set\nof tables | however, Neo does generalize to unseen queries,\nwhich can contain any number of known tables. Fourth, Neo\nrequires a traditional (weaker) query optimizer to bootstrap\nits learning process. As proposed in [35], we use the tradi-\ntional query optimizer as a source of expert demonstration,\nwhich Neo uses to bootstrap its initial policy. This tech-\nnique, referred to as \\learning from demonstration\" [9, 16,\n49, 50] signi\fcantly speeds up the learning process, reduc-\ning training time from days/weeks to just a few hours. The\nquery optimizer used to bootstrap Neo can be much weaker\nin performance and, after an initial training period, Neo sur-\npasses its performance and it is no longer needed. For this\nwork, we use the PostgreSQL optimizer, but any traditional\n(open source) optimizer can be used.\nOur results show that Neo outperforms commercial op-\ntimizers on their own query execution engines, even when\nit is boostrapped using the PostgreSQL optimizer. Inter-\nestingly, Neo learns to automatically adjust to changes in\nthe accuracy of cardinality predictions (e.g., it picks more\nrobust plans if the cardinality estimates are less precise).\nFurther, it can be tuned depending on the customer pref-\nerences (e.g., increase worst-case performance vs. relative\nperformance) | adjustments which are hard to achieve with\ntraditional techniques.\nWe argue that Neo represents a step forward in building\nan entirely learned optimizer. Moreover, Neo can already\nbe used, in its current form, to improve the performance\nof thousands of applications which rely on PostgreSQL and\nother open-source database systems (e.g. SQLite). We hope\nthat Neo inspires many other database researchers to exper-\niment with combining query optimizers and learned systems\nin new ways, similar to how AlexNet [23] changed the way\nimage classi\fers were built.\nIn summary, we make the following contributions:\n\u000fWe propose Neo | an end-to-end learning approach\nto query optimization, including join ordering, index\nselection, and physical operator selection.\nNeo\nExpertise\nRuntim\neQ’QQQSample\nWorkloadExpert\nOptimizer\nExecuted Plans\nFeaturizer\nPlan Search\nDatabase Execution Engine\nValue Model\nPrediction\nSelected planExperience\nLatencyUser Query\nrow vectorsFigure 1: Neo system model\n\u000fWe show that, after training for a dataset and repre-\nsentative sample query workload, Neo is able to gener-\nalize even over queries it has not encountered before.\n\u000fWe evaluate di\u000berent feature engineering techniques\nand propose a new technique, which implicitly repre-\nsents correlations within the dataset.\n\u000fWe show that, after a short amount of training, Neo\nis able to achieve performance comparable to Oracle's\nand Microsoft's query optimizers on their respective\ndatabase systems.\nThe remainder of the paper is organized as follows: Sec-\ntion 2 provides an overview of Neo's learning framework and\nsystem model. Section 3 describes how queries and query\nplans are represented by Neo. Section 4 explains Neo's value\nnetwork, the core learned component of our system. Sec-\ntion 5 describes row vector embeddings, an optional learned\nrepresentation of the underlying database that helps Neo\nunderstand correlation within the user's data. We present\nan experimental evaluation of Neo in Section 6, discuss re-\nlated works in Section 7, and o\u000ber concluding remarks in\nSection 8.\n2. LEARNING FRAMEWORK OVERVIEW\nWhat makes Neo unique that it is the very \frst end-to-end\nquery optimizer. As shown in Table 1, it replaces every com-\nponent of a traditional Selinger-style [52] query optimizers\nthrough machine learning models: (i) the query representa-\ntion is through features rather than an object-based query\noperator tree (e.g., Volcano-style [15] iterator tree); (ii) the\ncost model is a deep neural network (DNN) model as op-\nposed to hand-crafted equations; (iii) the search strategy is\na DNN-guided learned best-\frst search strategy instead of\nplan space enumeration or dynamic programming; (iv) car-\ndinality estimation is based on either histograms or a learned\nvector embedding scheme, combined with a learned model,\ninstead of hand-tuned histogram-based cardinality estima-\ntion model. Finally, (v) Neo uses reinforcement learning\nand learning from demonstration to integrate these into an\nend-to-end query optimizer rather than relying on human\nengineering. While we describe the di\u000berent components in\nthe individual sections as outlined in Table 1, the following\nprovides a general overview of how Neo learns, as depicted\nin Figure 1.\nExpertise Collection The \frst phase, labeled Expertise ,\ninitially generates experience from a traditional query opti-\nmizer, as proposed in [35]. Neo assumes the existence of an\n2\n\nTraditional Optimizer Neural Optimizer (Neo)\nCreation Human developers Demonstration, reinforcement learning (Section 2)\nQuery Representation Operator tree Feature encoding (Section 3)\nCost Model Hand-crafted model Learned DNN model (Section 4)\nPlan Space Enumeration Heuristics, dynamic programming DNN-guided search strategy (Section 4.2)\nCardinality Estimation Histograms, hand-crafted models Histograms, learned embeddings (Section 5)\nTable 1: Traditional cost-based query optimizer vs. Neo\napplication-provided Sample Workload consisting of queries\nrepresentative of the application's total workload. Addition-\nally, we assume Neo has access to a simple, traditional rule-\nor cost-based Expert Optimizer (e.g., Selinger [52], Post-\ngreSQL [1]). This simple optimizer is treated as a black box,\nand is only used to create query execution plans (QEPs) for\neach query in the sample workload. These QEPs, along with\ntheir latencies, are added to Neo's Experience (i.e., a set of\nplan/latency pairs), which will be used as a starting point in\nthe next model training phase. Note that the Expert Opti-\nmizer can be unrelated to the underlying Database Execution\nEngine .\nModel Building Given the collected experience, Neo builds\nan initial Value Model . The value model is a deep neural\nnetwork with an architecture designed to predict the \fnal\nexecution time of a given partial or complete plan for a given\nquery. We train the value network using the collected expe-\nrience in a supervised fashion. This process involves trans-\nforming each user-submitted query into features (through\ntheFeaturizer module) useful for a machine learning model.\nThese features contain both query-level information (e.g.,\nthe join graph, predicated attributes, etc.) and plan-level\ninformation (e.g., selected join order, access paths, etc.).\nNeo can work with a number of di\u000berent featurization tech-\nniques, ranging from simple one-hot encodings (Section 3.2)\nto more complex embeddings (Section 5). Neo's value net-\nwork uses tree convolution [40] to process the tree-structured\nQEPs (Section 4.1).\nPlan Search Once query-level information has been en-\ncoded, Neo uses the value model to search over the space of\nQEPs (i.e., selection of join orderings, join operators, and\nindexes) and discover the plan with the minimum predicted\nexecution time (i.e., value). Since the space of all execution\nplans for a particular query is far too large to exhaustively\nsearch, Neo performs a best-\frst search of the space, using\nthe value model as a heuristic. A complete plan created\nby Neo, which includes a join ordering, join operators (e.g.\nhash, merge, loop), and access paths (e.g., index scan, ta-\nble scan) is sent to the underlying execution engine, which\nis responsible for applying semantically-valid query rewrite\nrules (e.g., inserting necessary hash and sort operations) and\nexecuting the \fnal plan. This ensures that every execution\nplan generated by Neo computes the correct result. The\nplan search is discussed in detail in Section 4.2.\nModel Re\fnement As new queries are optimized through\nNeo, the model is iteratively improved and custom-tailored\nto the underlying database and execution engine. This is\nachieved by incorporating newly collected experience regard-\ning each query's QEP and performance. Speci\fcally, once a\nQEP is chosen for a particular query, it is sent to the un-\nderlying execution engine, which processes the query and\nreturns the result to the user. Additionally, Neo records the\fnal execution latency of the QEP, adding the plan/latency\npair to its Experience . Then, Neo retrains the value model\nbased on this experience, iteratively improving its estimates.\nDiscussion This process { featurizing, searching, and re-\n\fning { is repeated for each query sent by the user. Neo's\narchitecture is designed to create a corrective feedback loop:\nwhen Neo's learned cost model guides Neo to a query plan\nthat Neo predicts will perform well, but then the result-\ning latency is high, Neo's cost model learns to predict a\nhigher cost for the poorly-performing plan. Thus, Neo is\nless likely to choose plans with similar properties to the\npoorly-performing plan in the future. As a result, Neo's\ncost model becomes more accurate, e\u000bectively learning from\nits mistakes.\nNeo's architecture, of using a learned cost model to guide\na search through a large and complex space, is inspired by\nAlphaGo [53], a reinforcement learning system developed to\nplay the game of Go. At a high level, for each move in a\ngame of Go, AlphaGo uses a neural network to evaluate the\ndesirability of each potential move, and uses a search routine\nto \fnd a sequence of moves that is most likely to lead to a\nwinning position. Similarly, Neo uses a neural network to\nevaluate the desirability of partial query plans, and uses a\nsearch function to \fnd a complete query plan that is likely\nto lead to lower latency.\nBoth AlphaGo and Neo additionally bootstrap their cost\nmodels from experts. AlphaGo bootstraps from a dataset\nof Go games played by expert humans, and Neo bootstraps\nfrom a dataset of query execution plans built by a traditional\nquery optimizer (which was designed by human experts).\nThe reason for this bootstrapping is because of reinforce-\nment learning's inherent sample ine\u000eciency [16, 49]: with-\nout bootstrapping, reinforcement learning algorithms like\nNeo or AlphaGo may require millions of iterations [38] be-\nfore even becoming competitive with human experts. Boot-\nstrapping from an expert source (i.e., learning from demon-\nstration) intuitively mirrors how young children acquire lan-\nguage and learn to walk by imitating nearby adults (ex-\nperts), and has been shown to drastically reduce the num-\nber of iterations required to learn a good policy [16,50]. De-\ncreasing sample ine\u000eciency is especially critical for database\nmanagement systems: each iteration requires a query execu-\ntion, and users are unlikely to be willing to execute millions\nof queries before achieving performance on-par with current\nquery optimizers. Worse yet, executing a poor query execu-\ntion plan takes longer than executing a good execution plan,\nso the initial iterations would take an infeasible amount of\ntime to complete [35].\nThus, Neo can be viewed as a learning-from-demonstration\nreinforcement learning system similar to AlphaGo { there\nare, however, many di\u000berences between AlphaGo and Neo.\nFirst, because of the grid-like nature of the Go board, Al-\n3\n\nABB\nDMJLJ\n(scan)(scan)(index)C\n(unspecified)Figure 2: Partial query plan\nphaGo can trivially represent the board as an image and\nuse image convolution, possibly the most well-studied and\nhighly-optimized neural network primitive [24, 28], in order\nto predict the desirability of a board state. On the other\nhand, query execution plans have a tree structure, and can-\nnot be trivially represented as images, nor can image convo-\nlution be easily applied. Second, in Go, the board represents\nall the information relevant to a particular move, and can be\nrepresented using less than a kilobyte of storage. In query\noptimization, the data in the user's database is highly rel-\nevant to the performance of query execution plans, and is\ngenerally much larger than a kilobyte (it is not possible to\nsimply feed a user's entire database into a neural network).\nThird, AlphaGo has a single, unambiguous goal: defeat its\nopponent and reach a winning game state. Neo, on the other\nhand, needs to take the user's preferences into account, e.g.\nshould Neo optimize for average-case or worst-cast latency?\nThe remainder of this paper describes our solutions to\nthese problems in detail, starting with the notation and en-\ncoding of the query plans.\n3. QUERY FEATURIZATION\nIn this section, we describe how query plans are repre-\nsented as vectors, starting with some necessary notation.\n3.1 Notation\nFor a given query q, we de\fne the set of base relations\nused inqasR(q). A partial execution plan Pfor a query\nq(denotedQ(P) =q) is a forest of trees representing an\nexecution plan that is still being built. Each internal (non-\nleaf) tree node is a join operator ./i2J, whereJis the set\nof possible join operators (e.g., hash join ./H, merge join\n./M, loop join./L) and each leaf tree node is either a table\nscan, an index scan, or an unspeci\fed scan over a relation\nr2R(q), denoted T(r),I(r), andU(r) respectively.1An\nunspeci\fed scan is a scan that has not been assigned as\neither a table or an index scan yet. For example, the partial\nquery execution plan depicted in Figure 2 is denoted as:\n[(T(D)./MT(A))./LI(C)];[U(B)]\nHere, the type of scan for Bis still unspeci\fed, as is the\njoin that will eventually link Bwith the rest of the plan, but\nthe plan speci\fes a table scan of table DandA, which feed\ninto a merge join, whose result will then be joined using a\nloop join with C.\nAcomplete execution plan is a plan with only a single\ntree and with no unspeci\fed scans; all decisions on how the\nplan should be executed have been made. We say that one\nexecution plan Piis asubplan of another execution plan Pj,\n1Neo can trivially handle additional scan types, e.g., bitmap\nscans.\n  A B C D E\nA 0 0 1 1 0\nB 0 0 1 0 0\nC 0 1 0 0 0\nD 1 1 0 0 0\nE 0 0 0 0 0\nJoin Graph\nA.1 A.2 … B.1 B.2 … E.1 E.2\n 0   1  …  1   0  …  0   0\nColumn PredicatesA\nBC\nDA.2 < 5\nB.1 = ‘h’SELECT * FROM A, B, C, D WHERE\nA.3=C.3 AND A.4=D.4 AND C.5=B.5\nAND A.2<5 AND B.1=‘h’;\n 0 1 1 0 1 0 0 0 0 0 0 1 … 1 0 … 0 0\nQuery-level EncodingFigure 3: Query-level encoding\nwrittenPi\u001aPj, ifPjcould be constructed from Piby (1)\nreplacing unspeci\fed scans with index or table scans, and\n(2) combining subtrees in Piwith a join operator.\n3.2 Encodings\nIn order to train a neural network to predict the \fnal\nlatency of partial or complete query execution plans (QEPs),\nwe require two encodings: \frst, a query encoding , which\nencodes information regarding the query, but is independent\nof the query plan. For example, the involved tables and\npredicates fall into this category. Second, we require a plan\nencoding , which represents the partial execution plan.\nQuery Encoding The representation of query-dependent\nbut plan-independent information in Neo is similar to the\nrepresentations used in previous work [22, 34], and consists\nof two components. The \frst component encodes the joins\nperformed by the query, which can be represented as an ad-\njacency matrix of the join graph, e.g. in Figure 3, the 1 in\nthe \frst row, third column corresponds to the join predicate\nconnecting A and C. Both the row and column correspond-\ning to the relation Eare empty, because Eis not involved\nin the example query. Here, we assume that at most one\nforeign key between each relation exists. However, the rep-\nresentation can easily be extended to include more than one\npotential foreign key (e.g., by using the index of the rele-\nvant key instead of the constant value \\1\", or by adding ad-\nditional columns for each foreign key). Furthermore, since\nthis matrix is symmetrical, we choose only to encode the\nupper triangular portion, colored in red in Figure 3. Note\nthat the join graph does not specify the order of joins.\nThe second component of the query encoding is the col-\numn predicate vector. In Neo, we currently support three\nincreasingly powerful variants, with varying levels of pre-\ncomputation requirements:\n1.1-Hot (the existence of a predicate): is a simple \\one-\nhot\" encoding of which attributes are involved in a\nquery predicate. The length of the one-hot encoding\nvector is the number of attributes over all database\ntables. For example, Figure 3 shows the \\one-hot\"\nencoded vector with the positions for attribute A:2 and\nB:1 set to 1, since both attributes are used as part of\npredicate. Note that join predicates are not considered\nhere. Thus, the learning agent only knows whether\nan attribute is present in a predicate or not. While\nnaive, the 1-Hot representation can be built without\nany access to the underlying database.\n4\n\nABB\nDMJLJ\n(scan)(scan)(index)[0 1 1 0 0 0 0 1 1 0]\n[1 0 1 0 0 0 0 0 1 0] C[0 0 0 0 0 0 0 1 0 0]\n[0 0 0 0 0 0 0 0 1 0][0 0 0 0 0 1 0 0 0 0]Merge\nLoop\nABCD\nindex\ntable[0 0 1 0 0 0 0 0 0 0]LJ\n(index)[0 1 1 0 0 1 0 1 1 0]\nMerge\nLoop\nABCDMerge\nLoop\nABCD\nMerge\nLoop\nABCD\nMerge\nLoop\nABCD\nMerge\nLoop\nABCDMerge\nLoop\nABCDFigure 4: Plan-level encoding\n2.Histogram (the selectivity of a predicate): is a simple\nextension of the previous one-hot encoding which re-\nplaces the indication of a predicate's existence with the\npredicted selectivity of that predicate (e.g., A:2 could\nbe 0:2, if we predict a selectivity of 20%). For predict-\ning selectivity, we use an o\u000b-the-shelf histogram ap-\nproach with uniformity assumptions, as used by Post-\ngreSQL and other open-source systems.\n3.R-Vector (the semantics of a predicate): is the most\nadvanced encoding scheme, where we use row vectors .\nWe designed row vectors based on a natural language\nprocessing (NLP) model mirroring word vectors [36].\nIn this case, each entry in the column predicate vec-\ntor contains semantically relevant information related\nto the predicate. This encoding requires building a\nmodel over the data in the database, and is the most\nexpensive option. We discuss row vectors in Section 5.\nThe more powerful the encoding, the more degrees of free-\ndom the model has to learn complex relationships. How-\never, this does not necessarily mean that the model cannot\nlearn more complex relationships with a simpler encoding.\nFor example, even though Histogram does not encode any-\nthing about correlations between tables, the model might\nstill learn about them and accordingly correct the cardinal-\nity estimations internally, e.g. from repeated observation of\nquery latencies. However, with the R-Vector encoding, we\nmake Neo's job easier by providing a semantically-enhanced\nrepresentation of the query predicate.\nPlan Encoding The second encoding we require is to rep-\nresent the partial or complete query execution plan. While\nprior works [22,34] have \rattened the tree structure of each\npartial execution plan, our encoding preserves the inherent\ntree structure of execution plans . We transform each node\nof the partial execution plan into a vector, creating a tree of\nvectors, as shown in Figure 4. While the number of vectors\n(i.e., number of tree nodes) can increase, and the structure\nof the tree itself may change (e.g., left deep or bushy), every\nvector has the same number of columns.\nThese vectors are created as follows: each node is trans-\nformed into a vector of size jJj+2jRj, wherejJjis the number\nof di\u000berent join operators, and jRjis the number of relations.\nThe \frstjJjentries of each vector encode the join type (e.g.,\nin Figure 4, the root node uses a loop join), and the next\n2jRjentries encode which relations are being used, and what\ntype of scan (table, index, or unspeci\fed) is being used. For\nleaf nodes, this subvector is a one-hot encoding, unless the\nleaf represents an unspeci\fed scan, in which case it is treatedas though it were both an index scan and a table scan (a 1\nis placed in both the \\table\" and \\index\" columns). For any\nother node, these entries are the union of the corresponding\nchildren nodes. For example, the bottom-most loop join op-\nerator in Figure 4 has ones in the position corresponding to\ntable scans over D and A, and an index scan over C.\nNote that this representation can contain two partial query\nplans (i.e., several roots) which have yet to be joined, e.g.\nto represent the following partial plan:\nP= [(T(D)./MT(A))./LI(C)];[U(B)]\nWhen encoded, the U(B) root node would be encoded as:\n[0000110000]\nIntuitively, partial execution plans are built \\bottom up\",\nand partial execution plans with multiple roots represent\nsubplans that have yet to be joined together with a join op-\nerator. The purpose of these encodings is merely to provide\na representation of execution plans to Neo's value network,\ndescribed next.\n4. VALUE NETWORK\nIn this section, we present the Neo value network , a deep\nneural network model which is trained to approximate the\nbest-possible query latency that a partial execution plan Pi\ncould produce (in other words, the best-possible query la-\ntency achievable by a complete execution plan Pfsuch that\nPiis a subplan of Pf). Since knowing the best-possible com-\nplete execution plan for a query ahead of time is impossible\n(if it were possible, the need for a query optimizer would be\nmoot), we approximate the best-possible query latency with\nthe best query latency seen so far by the system .\nFormally, let Neo's experience Ebe a set of complete\nquery execution plans Pf2Ewith known latency, denoted\nL(Pf). We train a neural network model Mto approximate,\nfor allPithat are a subplan of any Pf2E:\nM(Pi)\u0019minfC(Pf)jPi\u001aPf^Pf2Eg\nwhereC(Pf) is the cost of a complete plan. The user can\nchange the cost function to alter the behavior of Neo. For\nexample, if the user is concerned only with minimizing total\nquery latency across the workload, the cost could be de\fned\nas the latency, i.e.,\nC(Pf) =L(Pf):\nHowever, if instead the user prefers to ensure that every\nqueryqin a workload performs better than a particular\nbaseline, the cost function can be de\fned as\nC(Pf) =L(Pf)=Base (Pf);\nwhereBase (Pf) is latency of plan Pfwith that baseline.\nRegardless of how the cost function is de\fned, Neo will at-\ntempt to minimize it over time. We experimentally evaluate\nboth of these cost functions in Section 6.4.4.\nThe model is trained by minimizing a loss function [51].\nWe use a simple L2 loss function:\n(M(Pi)\u0000minfC(Pf)jPi\u001aPf^Pf2Eg)2:\nNetwork Architecture The architecture of the Neo value\nnetwork model is shown in Figure 5.2We designed the\n2We omit activation functions, present between each layer,\nfrom our diagram and our discussion.\n5\n\nQuery-level Encoding\n1 x 64Fully Connected Layer\n1 x 128Fully Connected Layer\n1 x 64Fully Connected Layer\n1 x 321 x 20\n1 x 201 x 20\n1 x 201 x 20Plan-level Encoding\nConcatenation1 x 52\n1 x 521 x 52\n1 x 521 x 52Augmented Tree\nTree Convolution1x512 1x256 1x128Fully Connected Layer\n1 x 128Fully Connected Layer\n1 x 64Fully Connected Layer\n1 x 32Fully Connected Layer\n1 x 1Dynam\nic Pooling\n1 x 128Figure 5: Value network architecture\nmodel's architecture to create an inductive bias [33] suit-\nable for query optimization: the structure of the neural net-\nwork itself is designed to re\rect an intuitive understanding\nof what causes query plans to be fast or slow. Intuitively,\nhumans studying query plans learn to recognize suboptimal\nor good plans by pattern matching: a merge join on top of\na hash join with a shared join key is likely inducing a re-\ndundant sort or hash step; a loop join on top of two hash\njoins is likely to be highly sensitive to cardinality estimation\nerrors; a hash join using a fact table as the \\build\" relation\nis likely to incur spills; a series of merge joins that do not\nrequire re-sorting is likely to perform well, etc. Our insight\nis that all of these patterns can be recognized by analyzing\nsubtrees of a query execution plan. Our model architecture\nis essentially a large bank of these patterns that are learned\nautomatically, from the data itself , by taking advantage of\na technique called tree convolution [40] (discussed in Sec-\ntion 4.1).\nAs shown in Figure 5, when a partial query plan is evalu-\nated by the model, the query-level encoding is fed through\na number of fully-connected layers, each decreasing in size.\nThe vector outputted by the third fully connected layer is\nconcatenated with the plan-level encoding, i.e., each tree\nnode (the same vector is added to all tree nodes). This is a\nstandard technique [53] known as \\spatial replication\" [63]\nfor combining data that has a \fxed size (the query-level en-\ncoding) and data that is dynamically sized (the plan-level\nencoding). Once each tree node vector has been augmented,\nthe forest of trees is sent through several tree convolution\nlayers [40], an operation that maps trees to trees. After-\nwards, a dynamic pooling operation [40] is applied, \ratten-\ning the tree structure into a single vector. Several additional\nfully connected layers are used to map this vector into a\nsingle value, used as the model's cost prediction for the in-\nputted execution plan. A formal description of the value\nnetwork model is given in Appendix A.\n4.1 Tree Convolution\nCommon neural network models, like fully-connected neu-\nral networks or convolution neural networks, take as input\ntensors with a \fxed structure, such as a vector or an image.\nIn our problem, the features embedded in each execution\nplan are structured as nodes in a query plan tree (e.g., Fig-\nure 4). To process these features, we use tree convolution\nmethods [40], an adaption of traditional image convolution\nfor tree-structured data.\nTree convolution is a natural \ft for this problem. Similar\nto the convolution transformation for images, tree convolu-\ntion slides a set of shared \flters over each part of the query\ntree locally. Intuitively, these \flters can capture a wide va-\nriety of local parent-children relations. For example, \flters\ncan look for hash joins on top of merge joins, or a join oftwo relations when a particular predicate is present. The\noutput of these \flters provides signals utilized by the \fnal\nlayers of the value network; \flter outputs could signify rele-\nvant factors such as when the children of a join operator are\nsorted on the key (in which case merge join is likely a good\nchoice), or a \flter might estimate if the right-side relation\nof a join will have low cardinality (indicating that an index\nmay be useful). We provide two concrete examples later in\nthis section.\nOperationally, since each node on the query tree has ex-\nactly two child nodes,3each \flter consists of three weight\nvectors,ep;el;er. Each \flter is applied to each local \\trian-\ngle\" formed by the vector xpof a node and two of its left\nand right child, xlandxrto produce a new tree node x0\np:\nx0\np=\u001b(ep\fxp+el\fxl+er\fxr):\nHere,\u001b(\u0001) is a non-linear transformation (e.g., ReLU [14]), \f\nis a dot product, and x0\npis the output of the \flter. Each \fl-\nter thus combines information from the local neighborhood\nof a tree node (its children). The same \flter is \\slid\" across\neach tree in a execution plan, allowing a \flter to be applied\nto execution plans with arbitrarily sized trees. A set of \fl-\nters can be applied to a tree in order to produce another\ntree with the same structure, but with potentially di\u000berent\nsized vectors representing each node. In a large neural net-\nwork, such as those in our experimental evaluation, typically\nhundreds of \flters are applied.\nSince the output of a tree convolution is also a tree with\nthe same shape as the input (but with di\u000berent sized vec-\ntor representing each node), multiple layers of tree convolu-\ntion \flters can be sequentially applied to an execution plan.\nThe \frst layer of tree convolution \flters will access the aug-\nmented execution plan tree, and each \flter will \\see\" each\nparent/left child/right child triangle of the original tree. The\namount of information seen by a particular \flter is called the\n\flter's receptive \feld [30]. The second layer of convolution\n\flters will be applied to the output of the \frst, and thus\neach \flter in this second layer will see information derived\nfrom a node nin the original augmented tree, n's children,\nandn's grandchildren. Thus, each tree convolution layer\nhas a larger receptive \feld than the last. As a result, the\n\frst tree convolution layer will learn simple features (e.g.,\nrecognizing a merge join on top of a merge join), whereas\nthe last tree convolution layer will learn complex features\n(e.g., recognizing a left-deep chain of merge joins).\nWe present two concrete examples in Figure 6 that show\nhow the \frst layer of tree convolution can detect interesting\npatterns in query execution plans. In Example 1 of Fig-\nure 6a, we show two execution plans that di\u000ber only in the\n3We attach nodes with all zeros to each leaf node.\n6\n\n(a) Query trees(b) Features on each node(c) Tree convfilters(d) OutputMerge\tjoinCAB[1,0,1,1,0][0,0,0,0,1][0,0,1,0,0][0,0,0,1,0]Tree\tConvFilterel[1,-1,0,0,0]er[1,-1,0,0,0]ep[1,-1,0,0,0]1000Merge\tjoinMerge\tjoinCABHash\tjoin[1,0,1,1,1][1,0,1,1,0][0,0,0,0,1][0,0,1,0,0][0,0,0,1,0][0,1,1,1,1]210000\nMerge\tjoinCAB[1,0,1,1,0][0,0,0,0,1][0,0,1,0,0][0,0,0,1,0]Tree\tConvFilter3-101Merge\tjoinMerge\tjoinBACMerge\tjoin[1,0,1,1,1][1,0,1,0,1][0,0,0,1,0][0,0,1,0,0][0,0,0,0,1][1,0,1,1,1]-1-11002Example 1Example 2el[0,0,0,-1,0]er[-1,-1,-1,1,-1]ep[1,-1,0,1,-1]Figure 6: Tree convolution examples\ntopmost join operator (a merge join and hash join). As de-\npicted in the the top portion of Figure 6b, the join type\n(hash or merge) is encoded in the \frst two bits of the fea-\nture vector in each node. Now, if a tree convolution \flter\n(Figure 6c top) is comprised of three weight vectors with\nf1;\u00001gin the \frst two positions and zeros for the rest, it\nwill serve as a \\detector\" for query plans with two merge\njoins in a row. This can be seen in Figure 6d (top): the root\nnode of the plan with two merge joins in a row receives an\noutput of 2 from this \flter, whereas the root node of the plan\nwith a hash join on top of a merge join receives an output of\n0. Subsequent tree convolution layers can use this informa-\ntion to form more complex predicates, e.g. to detect three\nmerge joins in a row (a pipelined query execution plan), or\na mixture of merge joins and hash joins (which may require\nfrequent re-hashing or re-sorting). In Example 2 of Figure 6,\nsuppose A and B are physically sorted on the same key, and\nare thus optimally joined together with a merge join opera-\ntor, but that C is not physically sorted. The tree convolution\n\flter shown in Figure 6(c, bottom) serves as a detector for\nquery plans that join A and B with a merge join, behavior\nthat is likely desirable. The top weights recognize the merge\njoin (f1, -1gfor the \frst two entries) and the right weights\nprefer table B over all other tables. The result of this convo-\nlution (Figure 6d bottom) shows its highest output for the\nmerge join of A and B (in the \frst plan), and a negative\noutput for the merge join of A and C (in the second plan).\nIn practice, \flter weights are learned over time in an end-\nto-end fashion. By using tree convolution layers in a neu-\nral network, performing gradient descent on the weights of\neach \flter will cause \flters that correlate with latency (e.g.,\nhelpful features) to be rewarded (remain stable), and \flters\nwith no clear relationship to latency to be penalized (pushed\ntowards more useful values). This creates a corrective feed-\nback loop, resulting in the development of \flterbanks that\ngenerate useful features [28].\n4.2 DNN-Guided Plan Search\nThe value network predicts the quality (cost) of an ex-\necution plan, but it does not directly give an execution\nplan. Following several recent works in reinforcement learn-\ning [2,53], we combine the value network with a search tech-\nnique to generate query execution plans, resulting in a value\niteration technique [5] (discussed at the end of the section).\nGiven a trained value network and an incoming query q,\nNeo performs a search of the plan space for a given query.\nIntuitively, this search mirrors the search process used by\ntraditional database optimizers, with the trained value net-work taking on the role of the database cost model. Un-\nlike these traditional systems, the value network does not\npredict the cost of a subplan, but rather the best possible\nlatency achievable from an execution plan that includes a\ngiven subplan. This di\u000berence allows us to perform a best-\n\frst search [10] to \fnd an execution plan with low expected\ncost. Essentially, this amounts to repeatedly exploring the\ncandidate with the best predicated cost until a halting con-\ndition occurs.\nThe search process for query qstarts by initializing an\nempty min heap to store partial execution plans. This min\nheap is ordered by the value network's estimation of a partial\nplan's cost. Then, a partial execution plan with an unspeci-\n\fed scan for each relation in R(q) is added to the heap. For\nexample, if R(q) =fA;B;C;Dg, then the heap is initialized\nwithP0:\nP0= [U(A)];[U(B)];[U(C)];[U(D)];\nwhereU(r) is the unspeci\fed scan for the relation r2R(q).\nAt each search iteration, the subplan Piat the top of the\nmin heap is removed. We enumerate all of Pi's children,\nChildren (Pi), scoring them using the value network and\nadding them to the min heap. Intuitively, the children of Pi\nare all the plans creatable by specifying a scan in Pior by\njoining two trees of Piwith a join operator. Formally, we\nde\fneChildren (Pi) as the empty set if Piis a complete plan,\nand otherwise as all possible subplans Pjsuch thatPi\u001aPj\nand such that PjandPidi\u000ber by either (1) changing an\nunspeci\fed scan to a table or index scan, or (2) merging\ntwo trees using a join operator. Once each child is scored\nand added to the min heap, another search iteration begins,\nresulting in the next most promising node being removed\nfrom the heap and explored.\nWhile one could terminate this search process as soon as\na leaf node (a complete execution plan) is found, this search\nprocedure can easily be transformed into an anytime search\nalgorithm [64], i.e. an algorithm that continues to \fnd better\nand better results until a \fxed time cuto\u000b. In this variant,\nthe search process continues exploring the most promising\nnodes from the heap until a time threshold is reached, at\nwhich point the most promising complete execution plan is\nreturned. This gives the user control over the tradeo\u000b be-\ntween planning time and execution time. Users could even\nselect a di\u000berent time cuto\u000b for di\u000berent queries depend-\ning on their needs. In the event that the time threshold\nis reached before a complete execution plan is found, Neo's\nsearch procedure enters a \\hurry up\" mode [55], and greed-\nily explores the most promising children of the last node ex-\n7\n\nplored until a leaf node is reached. The cuto\u000b time should be\ntuned on a per-application bases, but we \fnd that a value of\n250ms is su\u000ecient for a wide variety of workloads (see Sec-\ntion 6.5), a value that is acceptable for many applications.\nFrom a reinforcement learning point of view, the combi-\nnation of the value network with a search procedure is a\nvalue iteration technique [5]. Value iteration techniques cy-\ncle between two steps: estimating the value function, and\nthen using that value function to improve the policy. We\nestimate the value function via supervised training of a neu-\nral network, and we use that value function to improve a\npolicy via a search technique. Q learning [61] and its deep\nneural network variants [38], which have been recently used\nfor query optimization [22], are also value iteration methods:\nthe value estimation step is similar, but they use that value\nfunction to select actions greedily (i.e., without a search).\nThis approach is equivalent to Neo's \\hurry up\" mode. As\nour experiments show, combining value estimation with a\nsearch procedure leads to a system that is less sensitive to\nnoise or inaccuracies in the value estimation model, result-\ning in signi\fcantly better query plans. This improvement\nhas been observed in other \felds as well [2,53].\n5. ROW VECTOR EMBEDDINGS\nNeo can represent query predicates in a number of ways,\nincluding a simple one-hot encoding ( 1-Hot ) or a histogram-\nbased representation ( Histogram ), as described in Section 3.2.\nHere, we motivate and describe row vectors , Neo's most ad-\nvanced option for representing query predicates ( R-Vector ).\nCardinality estimation is one of the most important prob-\nlems in query optimization today [25,29]. Estimating cardi-\nnalities is directly related to estimating selectivities of query\npredicates { whether these predicates involve a single ta-\nble or joins across multiple tables. The more columns or\njoins are involved, the harder the problem becomes. Mod-\nern database systems make several simplifying assumptions\nabout these correlations, such as uniformity, independence,\nand/or the principle of inclusion [26]. These assumptions\noften do not hold in real-world workloads, causing orders\nof magnitude increases in observed query latencies [25]. In\nNeo, we take a di\u000berent approach: instead of making sim-\nplifying assumptions about data distributions and attempt-\ning to directly estimate predicate cardinality, we build a\nsemantically-rich, vectorized representation of query predi-\ncates that can serve as an input to Neo's value model, en-\nabling the network to learn generalizable data correlations.\nWhile Neo supports several di\u000berent encodings for query\npredicates, here we present row vectors , a new technique\nbased on the popular word2vec [36] algorithm. Intuitively,\nwe build a vectorized representation of each query predicate\nbased on data in the database itself . These vectors are mean-\ningless on their own, but the distances between these vectors\nwill have semantic meaning . Neo's value network can take\nthese row vectors are inputs, and use them to identify cor-\nrelations within the data and predicates with syntactically-\ndistinct but semantically-similar values (e.g. Genre is \\ac-\ntion\" andGenre is \\adventure\").\n5.1 R-Vector Featurization\nThe basic idea behind our approach is to capture contex-\ntual cues among values that appear in a database. To give\na high-level example from the IMDB movie dataset [25], if\na keyword \\marvel-comics\" shows up in a query predicate,then we wish to be able to predict what else in the database\nwould be relevant for this query (e.g., other Marvel movies).\nWord vectors To generate row vectors, we use word2vec\n| a natural language processing technique for embedding\ncontextual information about collections of words [36]. In\nthe word2vec model, each sentence in a large body of text\nis represented as a collection of words that share a con-\ntext, where similar words often appear in similar contexts.\nThese words are mapped to a vector space, where the angle\nand distance between vectors re\rect the similarity between\nwords. For example, the words \\king\" and \\queen\" will\nhave similar vector representations, as they frequently ap-\npear in similar contexts (e.g. \\Long live the...\"), whereas\nwords like \\oligarch\" and \\headphones\" will have dissimi-\nlar vector representations, as they are unlikely to appear in\nsimilar contexts.\nWe see a natural parallel between sentences in a document\nand rows in a database: values of correlated columns tend to\nappear together in a database row. In fact, word2vec-based\nembeddings have recently been applied to other database\nproblems, such as semantic querying [7], entity matching [41],\nand data discovery [12]. In Neo, we use an o\u000b-the-shelf\nword2vec implementation [48] to build an embedding of each\nvalue in the database. We then utilize these embeddings to\nencode correlations across columns.4\nWe explored two variants of this featurization scheme. In\nthe \frst approach, we treat each row in every table of the\ndatabase as a training sentence. This approach captures\ncorrelations within a table. To better capture cross-table\ncorrelations, in our second approach, we augment the set of\ntraining sentences by partially denormalizing the database.\nConcretely, we join large fact tables with smaller tables\nwhich share a foreign key, and each resulting row becomes\na training sentence. Denormalization enables learning cor-\nrelations such as \\Actors born in Paris are more likely to\nplay in French movies\". While a Paris-born actor's name\nand birthplace may be stored only in the names table, the\ncast info table captures information relating this actor to\nmany French movies. By joining these two tables together,\nwe can make these relationships | such as the actor's name,\nbirthplace, and all the French movies they have appeared\nin | explicit to the word2vec training algorithm. Concep-\ntually, denormalizing the entire database would provide the\nbest embedding quality, but at the expense of signi\fcantly\nincreasing the size and number of training examples (a com-\npletely denormalized database may have trillions of rows),\nand hence the word2vec training time. A fully denormalized\ndatabase is often unfeasible to materialize. In Section 6.3.2,\nwe present the details of the row vector training performance\nin practice. Our word2vec training process is open source,\nand available on GitHub.5\nFigure 7 presents a visual example to show how row vec-\ntors capture semantic correlations between database tables.\nWe use t-SNE [58] to project embedded vectors of actor\nnames from their original 100-dimensional space into two-\ndimensional space for plotting. The t-SNE algorithm \fnds\nlow dimensional embeddings of high dimensional spaces that\nattempts to maintain the distance between points: points\n4Predicates with comparison operators, e.g. INand LIKE,\ncan lead to multiple matches. In this case, we take the mean\nof all the matched word vectors as the embedding input.\n5https://github.com/parimarjan/db-embedding-tools\n8\n\n(a) Birthplace of each actor\n (b) Top actors in each genre\nFigure 7: t-SNE projection of actor names embedded in the\nword2vec model. Column correlations across multiple IMDB\ntables show up as semantically meaningful clusters.\nthat are close together in the two-dimensional space are close\ntogether in the high dimensional space as well, and points\nthat are far apart in the low dimensional space are far apart\nin the high dimensional space as well.\nAs shown, various semantic groups (e.g., Chinese actors,\nsci-\f movie actors) are clustered together (and are thus also\nclose together in the original high-dimensional space), even\nwhen these semantic relationships span multiple tables. In-\ntuitively, this provides helpful signals to estimate query la-\ntency given similar predicates: as many of the clusters in\nFigure 7 are linearly separable, their boundaries can be\nlearned by machine learning algorithms. In other words,\nsince predicates with similar semantic values (e.g., two Amer-\nican actors) are likely to have similar correlations (e.g., be in\nAmerican \flms), representing the semantic value of a query\npredicate allows the value network to recognize similar pred-\nicates as similar. In Section 6.4.1, we \fnd that row vectors\nconsistently improves Neo's performance compared to other\nfeaturizations.\nRow vector construction In our implementation, the fea-\nture vectors for query predicates are constructed as follows.\nFor every distinct value in the underlying database, we gen-\nerate vectors which are a concatenation of the following:\n1. One-hot encoding of the comparison operators (e.g.\nequal or not equal to)\n2. Number of matched words\n3. Column embedding generated by word2vec (100 val-\nues, in our experiments)\n4. Number of times the given value is seen in the training\nThe concatenated vectors replace the \\1\"s or \\0\"s in the\ncolumn predicate vector of 1-Hot representation of the query-\nlevel information (see Section 3). For columns without a\npredicate, zeros are added so that the vector remains the\nsame size regardless of the number of predicates.\n5.2 Analysis\nHere, we analyze the embedding space learned by our\nrow vector approach on the IMDB dataset. We use the be-\nlow SQL query from the IMDB database to illustrate how\nlearned row vectors can be useful for tasks like cardinality\nestimation and query optimization.\nThis query counts the number of movies with genre \\ro-\nmance\" and containing the keyword \\love\". It spans \fveSELECT count (*)\nFROM title as t,\nmovie_keyword as mk ,\nkeyword as k,\ninfo_type as it ,\nmovie_info as mi\nWHERE it.id = 3\nAND it.id = mi. info_type_id\nAND mi. movie_id = t.id\nAND mk. keyword_id = k.id\nAND mk. movie_id = t.id\nAND k. keyword ILIKE '% love % '\nAND mi. info ILIKE '% romance % '\nFigure 8: Example query with correlations\nKeyword Genre Similarity Cardinality\nlove romance 0.24 11128\nlove action 0.16 2157\nlove horror 0.09 1542\n\fght action 0.28 12177\n\fght romance 0.21 3592\n\fght horror 0.05 1104\nTable 2: Similarity vs. Cardinality. In this case, correlated\nkeywords and genres, as shown in the SQL query in Figure 8,\nalso have higher similarity and higher cardinality.\ntables in the IMDB dataset. As input to word2vec training,\nwe partially denormalized these tables by joining title,\nkeyword info, keyword andtitle, movie info, info type.\nIt is important to note that, after this denormalization,\nkeywords and genres do not appear in the same row, but\nkeywords-titles as well as titles-genres do appear in separate\nrows.\nIn Table 2, we compare the cosine similarity (higher value\nindicating higher similarity) between the vectors for key-\nwords and genres to their true cardinalities in the dataset.\nAs shown, highly correlated keywords and genres (e.g., \\love\"\nand \\romance\") have higher cardinalities. As a result, this\nembedding provides a representative feature that can some-\nwhat substitute a precise cardinality estimation: a model\nbuilt using these vectors as input can learn to understand\nthe correlations within the underlying table.\nPostgreSQL, with its uniformity and independence as-\nsumptions, always estimates the cardinalities for the \fnal\njoined result to be close to 1, and therefore prefers to use\nnested loop joins for this query. In reality, the real cardinal-\nities vary wildly, as shown in Table 2. For this query, Neo\ndecided to use hash joins instead of nested loop joins, and\nas a result, was able to execute this query 60% faster than\nPostgreSQL.\nThis simple example provides a clear indication that row\nvector embeddings can capture meaningful relationships in\nthe data beyond histograms and one-hot encodings. This,\nin turn, provides Neo with useful information in the pres-\nence of highly correlated columns and values. An additional\nadvantage of our row embedding approach is that, by learn-\ning semantic relationships in a given database, Neo can gain\nuseful information even about column predicates that it has\nnever seen before in its training set (e.g., infer similar car-\ndinality using similar correlation between two attributes).\nWhile we can observe useful correlations in the row vectors\nbuilt for the IMDB dataset, language models like word2vec\nare notoriously di\u000ecult to interpret [45]. To the best of\n9\n\nour knowledge, there are no formal methods to ensure that\na word2vec model { on either natural language or database\nrows { will always produce helpful features. Developing such\nformal analysis is an active area of research in machine learn-\ning [31, 37]. Thus, while we have no evidence that our row\nvector embedding technique will work on every imaginable\ndatabase, we argue that our analysis on the IMDB database\n(a database with signi\fcant correlations / violations of uni-\nformity assumptions) provides early evidence that row vec-\ntors may also be useful in other similar applications with\nsemantically rich datasets. We plan to pursue this as part\nof our future work.\n6. EXPERIMENTS\nWe evaluated Neo's performance using both synthetic and\nreal-world datasets to answer the following questions: (1)\nhow does the performance of Neo compare to commercial,\nhigh-quality optimizers, (2) how well does the optimizer gen-\neralize to new queries, (3) how long is the optimizer execu-\ntion and training time, (4) how do the di\u000berent encoding\nstrategies impact the prediction quality, (5) how do other\nparameters (e.g., search time or loss function) impact the\noverall performance, and \fnally, (6) how robust is Neo to\nestimation errors.\n6.1 Setup\nWe evaluate Neo across a number of di\u000berent database\nsystems, using three di\u000berent benchmarks:\n1.JOB: the join order benchmark [25], with a set of queries\nover the Internet Movie Data Base (IMDB) consisting\nof complex predicates, designed to test query optimiz-\ners.\n2.TPC-H : the standard TPC-H benchmark [46], using a\nscale factor of 10.\n3.Corp: a 2TB dataset together with 8,000 unique queries\nfrom an internal dashboard application, provided by a\nlarge corporation (on the condition of anonymity).\nUnless otherwise stated, all experiments are conducted by\nrandomly placing 80% of the available queries into a training\nset, and using the other 20% of the available queries as a\ntesting set. In the case of TPC-H, we generated 80 training\nand 20 test queries based on the benchmark query templates\nwithout reusing templates between training and test queries.\nWe present results as the median performance from \ffty\nrandomly initialized neural networks. The Adam [19] opti-\nmizer is used for network training, as well as layer normal-\nization [3] to stabilize neural network training. The \\leaky\"\nvariant of recti\fed linear units [14] are used as activation\nfunctions. We use a search time cuto\u000b of 250ms. The net-\nwork architecture follows Figure 5, except the size of the\nplan-level encoding is dependent on the featurization cho-\nsen (e.g. 1-Hot orHistogram ).\n6.2 Overall Performance\nTo evaluate Neo's overall performance, we compared the\nmean execution time of the query plans generated by Neo\non two open-source (PostgreSQL 11, SQLite 3.27.1), and\ntwo commercial (Oracle 12c, Microsoft SQL Server 2017 for\nLinux) database systems, with the execution time of the\nplans generated by each system's native optimizer, for each\n 0 0.2 0.4 0.6 0.8 1 1.2\nPostgreSQL SQLite SQL Server OracleRelative performanceJOB\nTPC-HCorporationFigure 9: Relative query performance to plans created by\nthe native optimizer (lower is better) for di\u000berent workloads\nof our three workloads. Due to the license terms [47] of\nMicrosoft SQL Server and Oracle, we can only show perfor-\nmance in relative terms.\nFor initial experience collection for Neo, we always used\nthe PostgreSQL optimizer as the expert. That is, for every\nquery in the training set, we used the PostgreSQL optimizer\nto generate an initial query plan. We then measured the\nexecution time of this plan on the target execution engine\n(e.g., MS SQL Server) by forcing the target system, through\nquery hints, to obey the proposed query plan. Next, we\ndirectly begin training: Neo encodes the execution plan for\neach query in the training set, these plans are executed on\nthe native system, and the encoded vectors along with the\nresulting run times are added to Neo's experience.\nFigure 9 shows the relative performance of Neo after 100\ntraining iterations on each test workload, using the R-Vector\nencoding over the holdout dataset (lower is better). For ex-\nample, with PostgreSQL and the JOBworkload, Neo pro-\nduces queries that take only 60% of average execution time\nthan the ones created by the original PostgreSQL optimizer.\nSince the PostgreSQL optimizer is used to gather initial ex-\npertise for Neo, this demonstrates Neo's ability to improve\nupon an existing open-source optimizer.\nMoreover, for MS SQL Server and the JOBandCorp work-\nloads, the query plans produced by Neo are also 10% faster\nthan the plans created by the commercial optimizers on\ntheir native platforms. Importantly, both commercial opti-\nmizers, which include a multi-phase search procedure and\na dynamically-tuned cost model with hundreds of inputs\n[13,42], are expected to be substantially more advanced than\nPostgreSQL's optimizer. Yet, by bootstrapping only with\nPostgreSQL's optimizer, Neo is able to eventually outper-\nform or match the performance of these commercial opti-\nmizers on their own platforms. Note that the faster execu-\ntion times are solely based on better query plans without\nrun-time modi\fcations of the system. The only exception\nwhere Neo does not outperform the two commercial systems\nis for the TPC-H workload. We suspect that both MS SQL\nServer and Oracle were overtuned towards TPC-H, as it is\none of the most common benchmarks.\nOverall, this experiment demonstrates that Neo is able\nto create plans, which are as good as, and sometimes\neven better than, open-source optimizers and their\nsigni\fcantly superior commercial counterparts. How-\never, Figure 9 only compares the median performance of Neo\nafter the 100th training episode. This naturally raises the\n10\n\nfollowing questions: (1) how does the performance compare\nwith a fewer number of training episodes and how long does\nit take to train the model to a su\u000ecient quality (answered\nin the next subsection), and (2) how robust is the optimizer\nto various imputations (answered in Section 6.4).\n6.3 Training Time\nTo analyze the convergence time, we measured the per-\nformance after every training iteration, for a total of 100\ncomplete iterations. We \frst report the learning curves in\ntraining intervals to make the di\u000berent systems compara-\nble (e.g., a training episode with MS SQL Server might run\nmuch faster than PostgreSQL). Afterwards, we report the\nwall-clock time to train the models on the di\u000berent systems.\nFinally, we answer the question of how much our bootstrap-\nping method helped with the training time.\n6.3.1 Learning Curves\nWe measured the relative performance of Neo on our en-\ntire test suite with respect to the native optimizer (i.e., a\nperformance of 1 is equivalent to the engine's optimizer),\nfor every episode (a full pass over the set of training queries,\ni.e., retraining the network from the experience, choosing a\nplan for each training query, executing that plan, and adding\nthe result to Neo's experience) of the 100 complete training\nepisodes of the optimizer. We plot the median value as a\nsolid line, and the minimum and maximum values using the\nshaded region. For all DBMSes except for PostgreSQL, we\nadditionally plot the relative performance of the plans gen-\nerated by the PostgreSQL optimizer when executed on the\ntarget engine.\nConvergence Each \fgure demonstrates a similar behav-\nior: after the \frst training iteration, Neo's performance is\npoor (e.g., nearly 2.5 times worse than the native optimizer).\nThen, for several iterations, the performance of Neo sharply\nimproves, until it levels o\u000b (converges). We analyze the\nconvergence time speci\fcally in Section 6.3.2. Here, we note\nthat Neo is able to improve on the PostgreSQL optimizer in\nas few as 9 training iterations (i.e., the number of training\niterations until the median run crosses the line representing\nPostgreSQL). It is not surprising that matching the perfor-\nmance of a commercial optimizer like MS SQL Server or\nOracle requires signi\fcantly more training iterations than\nfor SQLite, as commercial systems are much more sophisti-\ncated.\nVariance The variance between the di\u000berent training it-\nerations is small for all workloads, except for the TPC-H\ndataset. We hypothesize that, with uniform data distribu-\ntions in TPC-H, the R-Vector embedding is not as useful,\nand thus it takes the model longer to adjust accordingly.\nThis behavior is not present in the other two non-synthetic\ndatasets.\n6.3.2 Wall-Clock Time\nSo far, we analyzed how long it took Neo to become com-\npetitive in terms of training iterations ; next, we analyze the\ntime it takes for Neo to become competitive in terms of wall-\nclock time (real time). We analyzed how long it took for Neo\nto learn a policy that was on-par with (1) the query execu-\ntion plans produced by PostgreSQL, but executed on the\ntarget execution engine, and (2) the query plans produced\nby the native optimizer and executed on the same execution\nengine. Figure 11 shows the time (in minutes) that it tookfor Neo to reach these two milestones (the left and right bar\ncharts represent milestone (1) and (2), respectively), split\ninto time spent training the neural network and time spent\nexecuting queries. Note that the query execution step is\nparallelized, executing queries on di\u000berent nodes simultane-\nously.\nUnsurprisingly, it takes longer for Neo to become compet-\nitive with the more advanced, commercial optimizers. How-\never, for every engine, learning a policy that outperforms\nthe PostgreSQL optimizer consistently takes less than two\nhours. Furthermore, Neo was able to match or exceed the\nperformance of every optimizer within half a day . Note that\nthis time does not include the time for training the query\nencoding, which in the case of the 1-Hot and Histogram\nare negligible. However, this takes longer for R-Vector (see\nSection 6.6).\n6.3.3 Is Demonstration Even Necessary?\nSince gathering demonstration data introduces additional\ncomplexity to the system, it is natural to ask if demon-\nstration data is necessary at all. Is it possible to learn a\ngood policy starting from zero knowledge? While previous\nwork [34] showed that an o\u000b-the-shelf deep reinforcement\nlearning technique can learn to \fnd query plans that min-\nimize a cost model without demonstration data, learning a\npolicy based on query latency (i.e., end to end) poses ad-\nditional di\u000eculties: a bad plan can take hours to execute.\nUnfortunately, randomly chosen query plans behave excep-\ntionally poorly. Leis et al. showed that randomly sampled\njoin orderings can result in a 100x to 1000x increase in query\nexecution times for JOBqueries, compared to a reasonable\nplan [25], potentially increasing the training time of Neo by\na similar factor [35].\nWe attempted to work around this problem by selecting an\nad-hoc query timeout t(e.g., 5 minutes), and terminating\nquery executions when their latencies exceed t. However,\nthis technique destroys a good amount of the signal that\nNeo uses to learn: join patterns resulting in a latency of 7\nminutes get the same reward as join patterns resulting in a\nlatency of 1 week, and thus Neo cannot learn that the join\npatterns in the 7-minute plan are an improvement over the\n1-week plan. As a result, even after training for over three\nweeks, we did not achieve the plan quality that we achieve\nwhen bootstrapping the system with the PostgreSQL opti-\nmizer.\n6.4 Robustness\nFor all experiments thus far, Neo was always evaluated\nover the test dataset, never the training dataset. This clearly\ndemonstrates that Neo does generalize to new queries. In\nthis subsection, we study this further by also testing Neo's\nperformance for the di\u000berent featurization techniques, over\nentirely new queries (i.e., queries invented speci\fcally to ex-\nhibit novel behavior), and measuring the sensitivity of cho-\nsen query plans to cardinality estimation errors.\n6.4.1 Featurization\nFigure 12 shows the performance of Neo across all four\nDBMSes for the JOBdataset, varying the featurization used.\nHere, we examine both the regular R-Vector encoding and\na variant of it built without any joins for denormalization\n(see Section 5). As expected, the 1-Hot encoding consis-\ntently performs the worst, as the 1-Hot encoding contains\n11\n\nPostgreSQL SQLite MS SQL Server OracleJOB\n 0 0.5 1 1.5 2 2.5\n 0  20  40  60  80  100Normalized Latency\nIterationsPostgres\nNeo (R-Vectors)\n 0 0.5 1 1.5 2 2.5\n 0  20  40  60  80  100Normalized Latency\nIterationsSQLite\nPostgreSQL on SQLite\nNeo (Row Vectors)\n 0 0.5 1 1.5 2 2.5\n 0  20  40  60  80  100Normalized Latency\nIterationsSQL Srv\nPostgreSQL on SQL Srv\nNeo (Row Vectors)\n 0 0.5 1 1.5 2 2.5\n 0  20  40  60  80  100Normalized Latency\nIterationsOracle\nPostgreSQL on Oracle\nNeo (Row Vectors) TPC-H\n 0 0.5 1 1.5 2 2.5\n 0  20  40  60  80  100Normalized Latency\nIterations\n 0 0.5 1 1.5 2 2.5\n 0  20  40  60  80  100Normalized Latency\nIterations\n 0 0.5 1 1.5 2 2.5\n 0  20  40  60  80  100Normalized Latency\nIterations\n 0 0.5 1 1.5 2 2.5\n 0  20  40  60  80  100Normalized Latency\nIterations Corp\n 0 0.5 1 1.5 2 2.5\n 0  20  40  60  80  100Normalized Latency\nIterations\n 0 0.5 1 1.5 2 2.5\n 0  20  40  60  80  100Normalized Latency\nIterations\n 0 0.5 1 1.5 2 2.5\n 0  20  40  60  80  100Normalized Latency\nIterations\n 0 0.5 1 1.5 2 2.5\n 0  20  40  60  80  100Normalized Latency\nIterations\nFigure 10: Learning curves with variance. Shaded area spans minimum to maximum across \ffty runs with di\u000berent random\nseeds. For a plot with all four featurization techniques, please visit: http://rm.cab/l/lc.pdf\n 0 100 200 300 400 500\nPostgreSQL SQLite SQL Server OraclePostgreSQL\nNative\nPostgreSQL\nNative\nPostgreSQL\nNativeTime (m)Neural network time\nQuery execution time\nFigure 11: Training time, in minutes, for Neo to match the\nperformance of PostgreSQL and each native optimizer.\n 0.5 0.6 0.7 0.8 0.9 1 1.1 1.2\nPostgreSQL SQLite SQL Server OracleRelative performanceR-Vectors\nR-Vectors (no joins)Histograms\n1-Hot\nFigure 12: Neo's performance using each featurization.no information about predicate cardinality. The Histogram\nencoding, while making naive uniformity assumptions, pro-\nvides enough information about predicate cardinality to im-\nprove Neo's performance. In each case, the R-Vector en-\ncoding variants produce the best overall performance, with\nthe \\no joins\" variant lagging slightly behind. This is be-\ncause the R-Vector encoding contains signi\fcantly more se-\nmantic information about the underlying database than the\nnaive histograms (see Section 5). The improved performance\nofR-Vector compared to the other encoding techniques\ndemonstrates the bene\fts of tailoring the feature represen-\ntation used to the underlying data.\n6.4.2 On Entirely New Queries\nPrevious experiments demonstrated Neo's ability to gen-\neralize to queries in a randomly-selected, held-out test set\ndrawn from the same workload as the training set. While\nthis shows that Neo can handle previously-unseen predi-\ncates and modi\fcations to join graphs, it does not neces-\nsarily demonstrate that Neo will be able to generalize to\na completely new query. To test Neo's behavior on new\nqueries, we created a set of 24 additional queries6, which we\ncallExt-JOB , that are semantically distinct from the original\nJOBworkload (no shared predicates or join graphs).\nAfter Neo had trained for 100 episodes on the JOBqueries,\nwe evaluated the relative performance of Neo on the Ext-JOB\nqueries. Figure 13 shows the results: the full height of each\nbar represents the performance of Neo on the unseen queries\nrelative to every other system. First, we note that with the\nR-Vector featurization, the execution plans chosen for the\n6https://git.io/extended_job\n12\n\n 0.5 0.6 0.7 0.8 0.9 1 1.1 1.2 1.3\nPostgreSQL SQLite SQL Server OracleRelative performanceR-Vectors\nR-Vectors (no joins)Histograms\n1-HotFigure 13: Neo's performance on entirely new queries\n(Ext-JOB ), full bar height. Neo's performance after 5 it-\nerations with Ext-JOB queries, solid bar height.\nentirely-unseen queries in the Ext-JOB dataset still outper-\nformed or matched the native optimizer. Second, the larger\ngap between the R-Vector featurizations and the Histogram\nand 1-Hot featurizations demonstrates that row vectors are\nan e\u000bective way of capturing information about query pred-\nicates that generalizes to entirely new queries.\nLearning new queries Since Neo is able to progressively\nlearn from each query execution, we also evaluated the per-\nformance of Neo on the Ext-JOB queries after just 5 addi-\ntional training episodes. The solid bars in Figure 13 show\nthe results. Once Neo has seen each new query a hand-\nful of times, Neo's performance increases quickly, having\nlearned how to handle the new complexities introduced by\nthe previously-unseen queries. Thus, while the performance\nof Neo initially degrades when confronted with new queries,\nNeo quickly adapts its policy to suit these new queries . This\nshowcases the potential for a deep-learning powered query\noptimizer to keep up with changes in real-world query work-\nloads.\n6.4.3 Cardinality Estimates\nThe strong relationship between cardinality estimation\nand query optimization is well-studied [4, 39]. However,\nquery optimizers must take into account that most cardi-\nnality estimation methods tend to become signi\fcantly less\naccurate when the number of joins increases [25]. While\ndeep neural networks are generally regraded as black boxes,\nhere we show that Neo is capable of learning when to trust\ncardinality estimates and when to ignore them.\nTo measure the robustness of Neo to cardinality estima-\ntion errors, we trained two Neo models, with an additional\nfeature at each tree node. The \frst model received the Post-\ngreSQL optimizer's cardinality estimation, and the second\nmodel received the true cardinality. We then plotted a his-\ntogram of both model's outputs across the JOBworkload\nwhen the number of joins was \u00143 and>3, introducing\narti\fcial error to the additional features.\nFor example, Figure 14a shows the histogram of value\nnetwork predictions for all states with at most 3 joins. When\nthe error is increased from zero orders of magnitude to two\nand \fve orders of magnitude, the variance of the distribution\nincreases: in other words, when the number of joins is at\nmost 3, Neo learns a model that varies with the PostgreSQL\ncardinality estimate. However, in Figure 14b, we see thatthe distribution of network outputs hardly changes at all\nwhen the number of joins is greater than 3: in other words,\nwhen the number of joins is greater than 3, Neo learns to\nignore the PostgreSQL cardinality estimates all together.\nOn the other hand, Figure 14c and Figure 14d show that\nwhen Neo's value model is trained with true cardinalities as\ninputs, Neo learns a model that varies its prediction with the\ncardinality regardless of the number of joins. In other words,\nwhen provided with true cardinalities, Neo learns to rely\non the cardinality information irrespective of the number of\njoins. Thus, we conclude that Neo is able to learn which\ninput features are reliable, even when the reliability of those\nfeatures varies with the number of joins.\n6.4.4 Per Query Performance\nFinally, we analyzed the per-query performance of Neo (as\nopposed to the workload performance). The absolute per-\nformance improvement (or regression) in seconds for each\nquery of the JOBworkload between the Neo and PostgreSQL\nplans are shown in Figure 15, in purple. As it can be seen,\nNeo is able to signi\fcantly improve the execution time of\nmany queries up to 40 seconds, but also worsens the exe-\ncution time of a few of queries e.g., query 24a becomes 8.5\nseconds slower.\nHowever, in contrast to a traditional optimizer, in Neo we\ncan easily change the optimization goal. So far, we always\naimed to optimize the total workload cost, i.e., the total la-\ntency across all queries. However, we can also change the\noptimization goal to optimize for the relative improvement\nper query (green bars in Figure 15). This implicitly pe-\nnalizes changes in the query performance from the baseline\n(e.g., PostgreSQL). When trained with this cost function,\nthe total workload time is still accelerated (by 289 seconds,\nas opposed to nearly 500 seconds), andall but one query7\nsees improved performance from the PostgreSQL baseline.\nThus, we conclude that Neo responds to di\u000berent optimiza-\ntion goals, allowing it to be customized for di\u000berent scenarios\nand for the user's needs.\nIt is possible that Neo's loss function could be further\ncustomized to weigh queries di\u000berently depending on their\nimportance to the user, i.e. query priority. It may also\nbe possible to build an optimizer that is directly aware of\nservice-level agreements (SLAs). We leave such investiga-\ntions to future work.\n6.5 Search\nNeo uses the trained value network to search for query\nplans until a \fxed-time cuto\u000b (see Section 4.2). Figure 16\nshows how the performance of a query with a particular\nnumber of joins (selected randomly from the JOBdataset, ex-\necuted on PostgreSQL) varies as the search time is changed\n(previous experiments used a \fxed cuto\u000b of 250ms). Note\nthat the x-axis skips some values, e.g. the JOBdataset has no\nqueries with 13 joins. Here, query performance is given rel-\native to the best observed performance. For example, when\nthe number of joins is 10, Neo found the best-observed plan\nwhenever the cuto\u000b time was greater than 120ms. We also\ntested signi\fcantly extending the search time (to 5 minutes),\nand found that such an extension did not change query per-\nformance regardless of the number of joins in the query (up\nto 17 in the JOBdataset).\n7Query 29b regresses by 43 milliseconds.\n13\n\n 0 0.5 1 1.5 2\n-2 -1.5 -1 -0.5  0  0.5  1  1.5  2Normalized Frequency\nValue Network OutputError = 0\nError = 2\nError = 5(a) PostgreSQL, \u00143 joins\n 0 0.5 1 1.5 2\n-2 -1.5 -1 -0.5  0  0.5  1  1.5  2Normalized Frequency\nValue Network OutputError = 0\nError = 2\nError = 5 (b) PostgreSQL, >3 joins\n 0 0.5 1 1.5 2\n-2 -1.5 -1 -0.5  0  0.5  1  1.5  2Normalized Frequency\nValue Network OutputError = 0\nError = 2\nError = 5 (c) True cardinality, \u00143 joins\n 0 0.5 1 1.5 2\n-2 -1.5 -1 -0.5  0  0.5  1  1.5  2Normalized Frequency\nValue Network OutputError = 0\nError = 2\nError = 5 (d) True cardinality, >3 joins\nFigure 14: Robustness to cardinality estimation errors\n-50-40-30-20-10 0 10 20\nQuery\n16b\n17d\n17a\n17c\n17f\n6d*\n17e\n25c\n18a\n20a\n8a\n18c\n6f\n17b\n30c\n25a*\n7a*\n16c\n19d\n16d\n7c*\n26c\n20b\n26a*\n22d\n22c\n12c\n31c\n14c\n30b\n30a\n10a*\n31a\n7b*\n25b\n31b\n6b\n13c\n23c\n13a\n13b\n22b\n13d\n20c*\n2d\n2c\n26b\n14b\n16a\n11c*\n14a\n2a\n11d\n2b\n22a\n21a\n11b\n11a*\n3b\n3a\n28b\n9d*\n21b\n21c\n4c\n29a\n10c\n6a\n15d\n27c\n12a\n5b\n6c\n8b\n6e\n1d\n5a\n5c\n1b\n15b*\n8d\n9a*\n8c\n1c\n9b*\n32a\n3c\n1a\n19b\n10b\n33b\n27b*\n15c\n19a\n12b*\n4a\n24b*\n32b*\n9c*\n4b\n27a\n19c\n28c\n33c\n29c\n33a\n29b\n23a\n15a\n23b\n28a\n18b\n24aDiﬀerence from PostgreSQL (s)\nQueryWorkload cost\nRelative cost\nFigure 15: Workload cost vs. relative cost for JOBqueries between Neo and PostgreSQL (lower is better)\n 50 100 150 200 250\n4 5 6 7 8 910 11 12 14 17Search Time (ms)\nNumber of Joins\n 1 1.5 2 2.5 3 3.5 4 4.5\nFigure 16: Search time vs. performance, grouped by number\nof joins\nThe relationship between the number of joins and sensi-\ntivity to search time is unsurprising: queries with more joins\nhave a larger search space, and thus require more time to\noptimize. While 250ms to optimize a query with 17 joins\nis acceptable in many scenarios, other options [59] may be\nmore desirable when this is not the case.\n6.6 Row vector training time\nHere, we analyze the time it takes to build the R-Vector\nrepresentation. Our implementation uses the open source\ngensim package [48], with no additional optimizations. Fig-\nure 17 shows the time it takes to train row vectors on each\ndataset, for both the \\joins\" (partially denormalized) and\n\\no joins\" (normalized) variants, as described in Section 5.\nThe time to train a row embedding model is proportional to\nthe size of the dataset. For the JOBdataset (approximately\n4GB), the \\no joins\" variant trains in less than 10 minutes,\nwhereas the \\no joins\" variant for the Corp dataset (approxi-\nmately 2TB) requires nearly two hours to train. The \\joins\"\n(partially denormalized) variant takes signi\fcantly longer to\n 1 10 100 1000 10000\nJOB TPC-H CorpTime to build (m)\nDatasetJoins\nNo joinsFigure 17: Row vector training time\ntrain, e.g. three hours ( JOB) to a full day (27 hours, Corp).\nBuilding either variant of row vectors may be prohibitive\nin some cases. However, experimentally we found that, com-\npared to Histogram , the \\joins\" variant on average resulted\nin 5% faster query processing times and that the \\no joins\"\nvariant on average resulted in 3% faster query processing\ntimes (e.g., Figure 9). Depending on the multiprocessing\nlevel, query arrival rate, etc., row vectors may \\pay for\nthemselves\" very quickly: for example, the training time\nfor building the \\joins\" variant on the Corp dataset is \\paid\nfor\" after 540 hours of query processing, since the row vec-\ntors speed up query processing by 5% and require 27 hours\nto train. As the corporation constantly executes 8 queries\nsimultaneously, this amounts to just three days. The \\no\njoins\" variant (improves performance by 3%, takes 217 min-\nutes to train) is \\paid for\" after just 15 hours.\nWe do not analyze the behavior of row vectors on a chang-\ning database. It is possible that, depending on the database,\nrow vectors quickly become \\stale\" (the data distribution\nshifts quickly), or remain relevant for long periods of time\n(the data distribution shifts slowly). New techniques [11,62]\n14\n\nsuggest that retraining word vector models when the under-\nlying data has changed can be done quickly, but we leave\ninvestigating these methods to future work.\n7. RELATED WORK\nThe relational query optimization problem has been around\nfor more than forty years and is one of the most stud-\nied problems in database management systems [8, 52]. Yet,\nquery optimization is still an unsolved problem [29], espe-\ncially due to the di\u000eculty of accurately estimating cardi-\nnalities [25,26]. IBM DB2's LEO optimizer was the \frst to\nintroduce the idea of a query optimizer that learns from its\nmistakes [54]. In follow-up work, CORDS proactively dis-\ncovered correlations between any two columns using data\nsamples in advance of query execution [17].\nRecent progress in machine learning has led to new ideas\nfor learning-based approaches, especially deep learning [60],\nto optimizing query run time. For example, recent work\n[18, 57] showed how to exploit reinforcement learning for\nEddies-style, \fne-grained adaptive query processing. More\nrecently, Trummer et al. have proposed the SkinnerDB sys-\ntem, based on the idea of using regret bound as a quality\nmeasure while using reinforcement learning for dynamically\nimproving the execution of an individual query in an adap-\ntive query processing system [56]. Ortiz et al. analyzed how\nstate representations a\u000bect query optimization when using\nreinforcement learning [43]. QuickSel o\u000bered using query-\ndriven mixture models as an alternative to using histograms\nand samples for adaptive selectivity learning [44]. Kipf et\nal. and Liu et al. proposed a deep learning approach to\ncardinality estimation, speci\fcally designed to capture join-\ncrossing correlations and 0-tuple situations (i.e., empty base\ntable samples) [20,27]. The closest work to ours is DQ [22],\nwhich proposed a learning based approach exclusively for\njoin ordering, and only for a given cost model. The key con-\ntribution of our paper over all of these previous approaches\nis that it provides an end-to-end, continuously learning so-\nlution to the database query optimization problem. Our\nsolution does not rely on any hand-crafted cost model or\ndata distribution assumptions.\nThis paper builds on recent progress from our own team.\nReJOIN [34] proposed a deep reinforcement learning ap-\nproach for join order enumeration [34], which was general-\nized into a broader vision for designing an end-to-end learning-\nbased query optimizer in [35]. Decima [32] proposed a rein-\nforcement learning-based scheduler, which processes query\nplans via a graph neural network to learn workload-speci\fc\nscheduling policies that minimize query latency. SageDB [21]\nlaid out a vision towards building a new type of data process-\ning system which will replace every component of a database\nsystem, including the query optimizer, with learned compo-\nnents, thereby gaining the capability to best specialize itself\nfor every use case. This paper is one of the \frst steps to\nrealizing this overall vision.\n8. CONCLUSIONS\nThis paper presents Neo, the \frst end-to-end learning op-\ntimizer that generates highly e\u000ecient query execution plans\nusing deep neural networks. Neo iteratively improves its per-\nformance through a combination of reinforcement learning\nand a search strategy. On four database systems and threequery datasets, Neo consistently outperforms or matches ex-\nisting commercial query optimizers (e.g., Oracle's and Mi-\ncrosoft's) which have been tuned over decades.\nIn the future, we plan to investigate various methods for\ngeneralizing a learned model to unseen schemas (using e.g.\ntransfer learning [6]). We also intend to further optimize\nour row vector encoding technique. Finally, we are inter-\nested in measuring the performance of Neo when bootstrap-\nping from both more primitive and advanced commercial\noptimizers. Using a commercial database system as an ini-\ntial expert might provide substantially faster convergence,\nor even a better \fnal result (although this would introduce\na dependency on a complex, hand-engineered optimizer, de-\nfeating a major bene\ft of Neo). Alternatively, using a sim-\nple, Selinger-style [52] optimizer may prove e\u000bective, allevi-\nating the need for even the complexities of the PostgreSQL\noptimizer.\n9. REFERENCES\n[1] PostgreSQL database, http://www.postgresql.org/.\n[2] T. Anthony, Z. Tian, and D. Barber. Thinking Fast\nand Slow with Deep Learning and Tree Search. In\nAdvances in Neural Information Processing Systems\n30, NIPS '17, pages 5366{5376, 2017.\n[3] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer\nNormalization. arXiv:1607.06450 [cs, stat] , July 2016.\n[4] B. Babcock and S. Chaudhuri. Towards a Robust\nQuery Optimizer: A Principled and Practical\nApproach. In Proceedings of the 2005 ACM SIGMOD\nInternational Conference on Management of Data ,\nSIGMOD '05, pages 119{130, New York, NY, USA,\n2005. ACM.\n[5] R. Bellman. A Markovian Decision Process. Indiana\nUniversity Mathematics Journal , 6(4):679{684, 1957.\n[6] Y. Bengio. Deep Learning of Representations for\nUnsupervised and Transfer Learning. In Proceedings of\nICML Workshop on Unsupervised and Transfer\nLearning , ICML WUTL '12, pages 17{36, June 2012.\n[7] R. Bordawekar and O. Shmueli. Using Word\nEmbedding to Enable Semantic Queries in Relational\nDatabases. In Proceedings of the 1st Workshop on\nData Management for End-to-End Machine Learning\n(DEEM) , DEEM '17, pages 5:1{5:4, 2017.\n[8] S. Chaudhuri. An Overview of Query Optimization in\nRelational Systems. In ACM SIGMOD Symposium on\nPrinciples of Database Systems , SIGMOD '98, pages\n34{43, 1998.\n[9] G. V. de la Cruz Jr, Y. Du, and M. E. Taylor.\nPre-training Neural Networks with Human\nDemonstrations for Deep Reinforcement Learning.\narXiv:1709.04083 [cs] , Sept. 2017.\n[10] R. Dechter and J. Pearl. Generalized Best-\frst Search\nStrategies and the Optimality of A*. J. ACM ,\n32(3):505{536, July 1985.\n[11] M. Faruqui, J. Dodge, S. K. Jauhar, C. Dyer, E. H.\nHovy, and N. A. Smith. Retro\ftting Word Vectors to\nSemantic Lexicons. In The 2015 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies , NAACL '15, pages 1606{1615, 2015.\n15\n\n[12] R. C. Fernandez and S. Madden. Termite: A System\nfor Tunneling Through Heterogeneous Data. Preprint,\n2019, 2019.\n[13] L. Giakoumakis and C. A. Galindo-Legaria. Testing\nSQL Server's Query Optimizer: Challenges,\nTechniques and Experiences. IEEE Data Eng. Bull. ,\n31:36{43, 2008.\n[14] X. Glorot, A. Bordes, and Y. Bengio. Deep Sparse\nRecti\fer Neural Networks. In G. Gordon, D. Dunson,\nand M. Dud\u0013 \u0010k, editors, Proceedings of the Fourteenth\nInternational Conference on Arti\fcial Intelligence and\nStatistics , volume 15 of PMLR '11 , pages 315{323,\nFort Lauderdale, FL, USA, Apr. 2011. PMLR.\n[15] G. Graefe and W. J. McKenna. The Volcano\nOptimizer Generator: Extensibility and E\u000ecient\nSearch. In Proceedings of the Ninth International\nConference on Data Engineering , ICDE '93, pages\n209{218, Washington, DC, USA, 1993. IEEE\nComputer Society.\n[16] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot,\nT. Schaul, B. Piot, D. Horgan, J. Quan,\nA. Sendonaris, G. Dulac-Arnold, I. Osband,\nJ. Agapiou, J. Z. Leibo, and A. Gruslys. Deep\nQ-learning from Demonstrations. In Thirty-Second\nAAAI Conference on Arti\fcal Intelligence , AAAI '18,\nNew Orleans, Apr. 2017. IEEE.\n[17] I. F. Ilyas, V. Markl, P. Haas, P. Brown, and\nA. Aboulnaga. CORDS: Automatic Discovery of\nCorrelations and Soft Functional Dependencies. In\nACM SIGMOD International Conference on\nManagement of Data , SIGMOD '04, pages 647{658,\n2004.\n[18] T. Kaftan, M. Balazinska, A. Cheung, and J. Gehrke.\nCuttle\fsh: A Lightweight Primitive for Adaptive\nQuery Processing. arXiv preprint , Feb. 2018.\n[19] D. P. Kingma and J. Ba. Adam: A Method for\nStochastic Optimization. In 3rd International\nConference for Learning Representations , ICLR '15,\nSan Diego, CA, 2015.\n[20] A. Kipf, T. Kipf, B. Radke, V. Leis, P. Boncz, and\nA. Kemper. Learned Cardinalities: Estimating\nCorrelated Joins with Deep Learning. In 9th Biennial\nConference on Innovative Data Systems Research ,\nCIDR '19, 2019.\n[21] T. Kraska, M. Alizadeh, A. Beutel, Ed Chi, Ani\nKristo, Guillaume Leclerc, Samuel Madden, Hongzi\nMao, and Vikram Nathan. SageDB: A Learned\nDatabase System. In 9th Biennial Conference on\nInnovative Data Systems Research , CIDR '19, 2019.\n[22] S. Krishnan, Z. Yang, K. Goldberg, J. Hellerstein, and\nI. Stoica. Learning to Optimize Join Queries With\nDeep Reinforcement Learning. arXiv:1808.03196 [cs] ,\nAug. 2018.\n[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImageNet Classi\fcation with Deep Convolutional\nNeural Networks. In Proceedings of the 25th\nInternational Conference on Neural Information\nProcessing Systems - Volume 1 , NIPS '12, pages\n1097{1105, USA, 2012. Curran Associates Inc.\n[24] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning.\nNature , 521(7553):436{444, May 2015.\n[25] V. Leis, A. Gubichev, A. Mirchev, P. Boncz,A. Kemper, and T. Neumann. How Good Are Query\nOptimizers, Really? Proc. VLDB Endow. ,\n9(3):204{215, Nov. 2015.\n[26] V. Leis, B. Radke, A. Gubichev, A. Mirchev,\nP. Boncz, A. Kemper, and T. Neumann. Query\noptimization through the looking glass, and what we\nfound running the Join Order Benchmark. The VLDB\nJournal , pages 1{26, Sept. 2017.\n[27] H. Liu, M. Xu, Z. Yu, V. Corvinelli, and C. Zuzarte.\nCardinality Estimation Using Neural Networks. In\nProceedings of the 25th Annual International\nConference on Computer Science and Software\nEngineering , CASCON '15, pages 53{59, Riverton,\nNJ, USA, 2015. IBM Corp.\n[28] W. Liu, Z. Wang, X. Liu, N. Zeng, Y. Liu, and F. E.\nAlsaadi. A survey of deep neural network architectures\nand their applications. Neurocomputing , 234:11{26,\nApr. 2017.\n[29] G. Lohman. Is Query Optimization a `\"Solved\"\nProblem? In ACM SIGMOD Blog , ACM Blog '14,\n2014.\n[30] J. Long, E. Shelhamer, and T. Darrell. Fully\nConvolutional Networks for Semantic Segmentation.\nInThe IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) , CVPR '15, June 2015.\n[31] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y.\nNg, and C. Potts. Learning word vectors for sentiment\nanalysis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics:\nHuman Language Technologies , volume 1 of H:T '11 ,\npages 142{150. Association for Computational\nLinguistics, June 2011.\n[32] H. Mao, M. Schwarzkopf, S. B. Venkatakrishnan,\nZ. Meng, and M. Alizadeh. Learning scheduling\nalgorithms for data processing clusters. arXiv preprint\narXiv:1810.01963 , 2018.\n[33] G. Marcus. Innateness, AlphaZero, and Arti\fcial\nIntelligence. arXiv:1801.05667 [cs] , Jan. 2018.\n[34] R. Marcus and O. Papaemmanouil. Deep\nReinforcement Learning for Join Order Enumeration.\nInFirst International Workshop on Exploiting\nArti\fcial Intelligence Techniques for Data\nManagement , aiDM '18, Houston, TX, June 2018.\n[35] R. Marcus and O. Papaemmanouil. Towards a\nHands-Free Query Optimizer through Deep Learning.\nIn9th Biennial Conference on Innovative Data\nSystems Research , CIDR '19, 2019.\n[36] T. Mikolov, K. Chen, G. Corrado, and J. Dean.\nE\u000ecient Estimation of Word Representations in\nVector Space. arXiv:1301.3781 [cs] , Jan. 2013.\n[37] T. Mikolov, W.-t. Yih, and G. Zweig. Linguistic\nRegularities in Continuous Space Word\nRepresentations. In Proceedings of the 51st Annual\nMeeting of the Association for Computational\nLinguistics: Human Language Technologies , HLT '13,\n2013.\n[38] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu,\nJ. Veness, M. G. Bellemare, A. Graves, M. Riedmiller,\nA. K. Fidjeland, and G. Ostrovski. Human-level\ncontrol through deep reinforcement learning. Nature ,\n518(7540):529{533, 2015.\n[39] G. Moerkotte, T. Neumann, and G. Steidl. Preventing\n16\n\nBad Plans by Bounding the Impact of Cardinality\nEstimation Errors. Proc. VLDB Endow. , 2(1):982{993,\nAug. 2009.\n[40] L. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin.\nConvolutional Neural Networks over Tree Structures\nfor Programming Language Processing. In Proceedings\nof the Thirtieth AAAI Conference on Arti\fcial\nIntelligence , AAAI '16, pages 1287{1293, Phoenix,\nArizona, 2016. AAAI Press.\n[41] S. Mudgal, H. Li, T. Rekatsinas, A. Doan, Y. Park,\nG. Krishnan, R. Deep, E. Arcaute, and\nV. Raghavendra. Deep Learning for Entity Matching:\nA Design Space Exploration. In Proceedings of the\n2018 International Conference on Management of\nData , SIGMOD '18, pages 19{34, New York, NY,\nUSA, 2018. ACM.\n[42] B. Nevarez. Inside the SQL Server Query Optimizer .\nRed Gate books, Mar. 2011.\n[43] J. Ortiz, M. Balazinska, J. Gehrke, and S. S. Keerthi.\nLearning State Representations for Query\nOptimization with Deep Reinforcement Learning. In\n2nd Workshop on Data Managmeent for End-to-End\nMachine Learning , DEEM '18, 2018.\n[44] Y. Park, S. Zhong, and B. Mozafari. QuickSel: Quick\nSelectivity Learning with Mixture Models.\narXiv:1812.10568 [cs] , Dec. 2018.\n[45] M. Peters, M. Neumann, M. Iyyer, M. Gardner,\nC. Clark, K. Lee, and L. Zettlemoyer. Deep\nContextualized Word Representations. In Proceedings\nof the 2018 Conference of the North American\nChapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1\n(Long Papers) , NAACL '18, pages 2227{2237, New\nOrleans, Louisiana, 2018. Association for\nComputational Linguistics.\n[46] M. Poess and C. Floyd. New TPC Benchmarks for\nDecision Support and Web Commerce. SIGMOD\nRecords , 29(4):64{71, Dec. 2000.\n[47] A. G. Read. DeWitt clauses: Can we protect\npurchasers without hurting Microsoft. Rev. Litig. ,\n25:387, 2006.\n[48] R. \u0014Reh\u0017 u\u0014 rek and P. Sojka. Software Framework for\nTopic Modelling with Large Corpora. In Proceedings\nof the LREC 2010 Workshop on New Challenges for\nNLP Frameworks , LREC '10, pages 45{50. ELRA,\nMay 2010.\n[49] S. Schaal. Learning from Demonstration. In\nProceedings of the 9th International Conference on\nNeural Information Processing Systems , NIPS'96,\npages 1040{1046, Cambridge, MA, USA, 1996. MIT\nPress.\n[50] M. Schaarschmidt, A. Kuhnle, B. Ellis, K. Fricke,\nF. Gessert, and E. Yoneki. LIFT: Reinforcement\nLearning in Computer Systems by Learning From\nDemonstrations. arXiv:1808.07903 [cs, stat] , Aug.\n2018.\n[51] J. Schmidhuber. Deep learning in neural networks: An\noverview. Neural Networks , 61:85{117, Jan. 2015.\n[52] P. G. Selinger, M. M. Astrahan, D. D. Chamberlin,\nR. A. Lorie, and T. G. Price. Access Path Selection in\na Relational Database Management System. In\nJ. Mylopolous and M. Brodie, editors, SIGMOD '89 ,SIGMOD '89, pages 511{522, San Francisco (CA),\n1989. Morgan Kaufmann.\n[53] D. Silver, A. Huang, C. J. Maddison, A. Guez,\nL. Sifre, G. van den Driessche, J. Schrittwieser,\nI. Antonoglou, V. Panneershelvam, M. Lanctot,\nS. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner,\nI. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu,\nT. Graepel, and D. Hassabis. Mastering the game of\nGo with deep neural networks and tree search. Nature ,\n529(7587):484{489, Jan. 2016.\n[54] M. Stillger, G. M. Lohman, V. Markl, and M. Kandil.\nLEO - DB2's LEarning Optimizer. In Proceedings of\nthe 27th International Conference on Very Large Data\nBases , VLDB '01, pages 19{28, San Francisco, CA,\nUSA, 2001. Morgan Kaufmann Publishers Inc.\n[55] N. Tran, A. Lamb, L. Shrinivas, S. Bodagala, and\nJ. Dave. The Vertica Query Optimizer: The case for\nspecialized query optimizers. In 2014 IEEE 30th\nInternational Conference on Data Engineering , ICDE\n'14, pages 1108{1119, Mar. 2014.\n[56] I. Trummer, S. Moseley, D. Maram, S. Jo, and\nJ. Antonakakis. SkinnerDB: Regret-bounded Query\nEvaluation via Reinforcement Learning. Proc. VLDB\nEndow. , 11(12):2074{2077, Aug. 2018.\n[57] K. Tzoumas, T. Sellis, and C. Jensen. A\nReinforcement Learning Approach for Adaptive Query\nProcessing. Technical Report, 08, June 2008.\n[58] L. van der Maaten and G. Hinton. Visualizing Data\nusing t-SNE. Journal of Machine Learning Research ,\n9(Nov):2579{2605, 2008.\n[59] F. Waas and A. Pellenkoft. Join Order Selection\n(Good Enough Is Easy). In Advances in Databases ,\nBNCD '00, pages 51{67. Springer, Berlin, Heidelberg,\nJuly 2000.\n[60] W. Wang, M. Zhang, G. Chen, H. V. Jagadish, B. C.\nOoi, and K.-L. Tan. Database Meets Deep Learning:\nChallenges and Opportunities. SIGMOD Rec. ,\n45(2):17{22, Sept. 2016.\n[61] C. J. Watkins and P. Dayan. Q-learning. Machine\nlearning , 8(3-4):279{292, 1992.\n[62] L. Yu, J. Wang, K. R. Lai, and X. Zhang. Re\fning\nWord Embeddings Using Intensity Scores for\nSentiment Analysis. IEEE/ACM Transactions on\nAudio, Speech, and Language Processing ,\n26(3):671{681, Mar. 2018.\n[63] J.-Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A. A.\nEfros, O. Wang, and E. Shechtman. Toward\nMultimodal Image-to-Image Translation. In I. Guyon,\nU. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, editors, Advances\nin Neural Information Processing Systems , NIPS '17,\npages 465{476. Curran Associates, Inc., 2017.\n[64] S. Zilberstein. Using Anytime Algorithms in Intelligent\nSystems. AI Magazine , 17(3):73{73, Mar. 1996.\nAPPENDIX\nA. NEURAL NETWORK MODEL\nIn this appendix, we present a formal speci\fcation of Neo's\nneural network model (the value network). An intuitive de-\nscription is provided in Section 4.\n17\n\nLet the query-level information vector for an execution\nplanPfor a query Q(P) beV(Q(P)). LetF(P) be the set\nof root nodes in the (forest) P. We de\fne L(x) andR(x)\nas the left and right children of a node, respectively. Let\nV(x) be the vectorized representation of the tree node x.\nWe denote all r2F(P) as the tuple ( r;L(r);R(r)).\nThe query-level information V(Q(P)) is initially passed\nthrough a set of fully connected layers (see Figure 5) of\nmonotonically decreasing size. After the \fnal fully con-\nnected layer, the resulting vector ~ gis combined with each\ntree node to form an augmented forest F0(P). Intuitively,\nthis augmented forest is created by appending ~ gto each tree\nnode. Formally, we de\fne A(r) as the augmenting function\nfor the root of a tree:\nA(r) = (V(r)_~ g;A (L(r));A(R(r)))\nwhere_is the vector concatenation operator. Then:\nF0(P) =fA(r)jr2F(P)g\nWe refer to each entry of an augmented tree node's vector\nas a channel . Next, we de\fne tree convolution, an operation\nthat maps a tree with cinchannels to a tree with coutchan-\nnels. The augmented forest is passed through a number of\ntree convolution layers. Details about tree convolution can\nbe found in [40]. Here, we will provide a mathematical spec-\ni\fcation. Let a \flterbank be a matrix of size 3 \u0002cin\u0002cout.\nWe thus de\fne the convolution of a root node rof a tree with\ncinchannels with a \flterbank f, resulting in an structurally\nisomorphic tree with coutchannels:\n(r\u0003f) = (V(r)_L(r)_R(r)\u0002f;L(r)\u0003f;R(r)\u0003f)\nWe de\fne the convolution of a forest of trees with a \flter-\nbank as the convolution of each tree in the forest with the\n\flterbank. The output of the three consecutive tree convo-\nlution layers in the value network, with \flterbanks f1,f2,\nandf3, and thus be denoted as:\nT= ((F0(P)\u0003f1)\u0003f2)\u0003f3\nLetfinal outbe the number of channels in T, the output\nof the consecutive tree convolution layers. Next, we apply\na dynamic pooling layer [40]. This layer takes the element-\nwise maximum of each channel, \rattening the forest into a\nsingle vector Wof sizefinal out. Dynamic pooling can be\nthought of as stacking each tree node's vectorized represen-\ntation into a tall matrix of size n\u0002final out, wherenis the\ntotal number of tree nodes, and then taking the maximum\nvalue in each matrix column.\nOnce produced, Tis passed through a \fnal set of fully\nconnected layers, until the \fnal layer of the network pro-\nduces a singular output. This singular output is used to\npredict the value of a particular state.\n18",
  "textLength": 99034
}