{
  "paperId": "2d8e6622ea8cd867d0f9cf2af0830d9721d581fe",
  "title": "NeuralCubes: Deep Representations for Visual Data Exploration",
  "pdfPath": "2d8e6622ea8cd867d0f9cf2af0830d9721d581fe.pdf",
  "text": "NeuralCubes: Deep Representations for Visual Data Exploration\nZhe Wang, Dylan Cashman, Mingwei Li, Jixian Li, Matthew Berger,\nJoshua A. Levine, Remco Chang, and Carlos Scheidegger\nFig. 1. NeuralCubes is a system that removes the need for a visualization to connect to a database during runtime. Instead,\nNeuralCubes approximate the query results of a database with a trained neural network model. (Top): At the ofﬂine training stage,\nNeuralCubes use a speciﬁc user model (types and frequency of different queries they will make) and application model to generate\nquery-result pairs. Then a neural network model is trained on these pairs so that it learns how to predict a result for a future new query.\nAt runtime, the trained model answers all queries issued by the application. (Bottom): The example shown in the lower part of the\nﬁgure is a data visualization system for YellowCab taxi data [43]. The orange lines and the heatmap on the right are plotted using\npredictions by the trained model. For the purpose of evaluation, we also plot the ground truth by blue lines and by the heatmap on the\nleft. Note that the query results generated from NeuralCubes closely match the ground truth. However, trained NeuralCubes is small in\nmemory footprint. In the example above, the NeuralCubes requires only 798KB whereas the raw Y ellowCab taxi dataset used in the\nexample is 1.8GB.\nAbstract — Visual exploration of large multidimensional datasets has seen tremendous progress in recent years, allowing users to\nexpress rich data queries that produce informative visual summaries, all in real time. Techniques based on data cubes are some of the\nmost promising approaches. However, these techniques usually require a large memory footprint for large datasets. To tackle this\nproblem, we present NeuralCubes: neural networks that predict results for aggregate queries, similar to data cubes. NeuralCubes\nlearns a function that takes as input a given query, for instance, a geographic region and temporal interval, and outputs the result of the\nquery. The learned function serves as a real-time, low-memory approximator for aggregation queries. NeuralCubes models are small\nenough to be sent to the client side (e.g. the web browser for a web-based application) for evaluation, enabling data exploration of large\ndatasets without database/network connection. We demonstrate the effectiveness of NeuralCubes through extensive experiments on a\nvariety of datasets and discuss how NeuralCubes opens up opportunities for new types of visualization and interaction.\nIndex Terms —Visual data exploration, neural networks, multiple coordinated views\n1 I NTRODUCTION\n1arXiv:1808.08983v3  [cs.DB]  10 Jul 2019\n\nInteractive visual exploration is becoming increasingly essential for\nmaking sense of large multidimensional datasets. It is not uncommon\nfor datasets to have billions of data items that contain a variety of\nattributes of geographic, temporal, and categorical nature. Due to its\nsize and complexity, querying such large data in real-time is often not\nfeasible as it will result in unreasonable amount of latency. Instead, ef-\nﬁcient data structures are used by visualizations in lieu of querying raw\ndata in databases in real-time. These data structures are pre-computed\nand optimized around queries that are frequently used by the visualiza-\ntion, such as performing summary (aggregation) of the data [24, 35] ,\nranking [30], and applying multivariate statistics [45].\nHowever, while these data structures are effective, they can still be\nprohibitively large as data size increases. Worse, when data complexity\nincreases (i.e. in terms of the number of dimensions in the data),\nthe sizes of many of these data structures grow exponentially. As a\nresult, these data structures are often stored on a server. Only the sub-\nparts of the data structures are fetched in real-time based on the users\nexploration.\nIn this paper, we introduce NeuralCubes, a technique that generates\nextremely small data structures that can support interactively explo-\nration of multi-dimensional data. Unlike existing data structures that\nrely on performing and storing summary statistics, NeuralCubes is a\ntrained deep neural network that can respond to queries about the data\nin real time. We design a compact neural network architecture yet\naccurate enough for data visualization. Due to its extremely small foot-\nprint, NeuralCubes can be stored in client memory, thereby eliminating\nthe need for a visualization system to fetch data or sub-parts of a data\nstructure from a server in real-time. Instead, with NeuralCubes, all\nqueries can be computed on the client in real time.\nThe inspiration behind our design of NeuralCubes is that we consider\nquerying a database as a function that maps from a given input query to\nproduce an aggregation result. Assuming that there are latent patterns\nin the data (i.e. that the data is not purely random), these functions\ncan be efﬁciently learned using the latest advances in deep learning.\nIn particular, we observe that typical visualizations (such as the one\nshown in Fig. 1) generate a limited number of query templates and\nexpect a ﬁxed number of numeric values in response. For example, in\nthe example shown in Fig. 1, the query to the database will be based\non four sets of ﬁlters (geographic region, month of year, day of week,\nand time of day). In response, the visualization anticipates a set of\nnumeric values to populate the geographic heatmap and the three line\ncharts. Given this clearly deﬁned inputs and outputs of the function,\ndeep neural networks have been shown to be robust and efﬁcient in\nlearning the mapping.\nNeuralCubes has a off-line training stage and a real-time running\nstage. To train NeuralCubes, we ﬁrst need an application model and a\nuser model to generate training set. An application model can be seen\nas the “data schema” of an application. It contains information of how\nmany attributes are used and what’s the format of each attribute. A user\nmodel derives from types of queries and the frequency of them that\nusers perform when using an application. With these two models, we\ncan easy generate query-result pairs as training set for NeuralCubes. We\nuse many-hot encoding to represent the input query and feed it to neural\nnetworks that try to predict the result of that query. After training, the\nlearned neural network model can answer any queries issued from the\nsame application. Since the model is very small, NeuralCubes model\ncan be evaluated in real-time on any modern CPU/GPU.\nWe evaluate NeuralCubes on a variety of datasets including\nBrightKite social network check-ins [7], Flights dataset [34], Yellow-\nCab taxi dataset [43], and SPLOM dataset [17]. We quantitatively\n• Zhe Wang, Mingwei Li, Jixian Li, Josh Levine and Carlos Scheidegger are\nwith the University of Arizona. emails: fzhew, jixianli, mwli, josh,\ncscheid g@email.arizona.edu.\n• Matthew Berger is with Vanderbilt University. email:\nmatthew.berger@vanderbilt.edu.\n• Dylan Cashman and Remco Chang are with Tufts University. email:\nfdylancashman, remco g@tufts.edu.analyze the accuracy of NeuralCubes and how neural network size,\ntraining set size, raw data size, and attribute resolution affect prediction.\nWe report experimental results in Section 6.\nWe summarize our contributions as follows:\n•we show that neural networks can learn to answer aggregate\nqueries efﬁciently and effectively, that they generalize across\nheterogeneous attribute types (such as geographic, temporal, and\ncategorical data), and present a method to convert the schemata\nneeded to describe visual exploration systems into an appropriate\ndeep neural network architecture;\n•we use these neural networks that learn the structure of aggrega-\ntion queries to provide the user 2D projections that enable the\nintuitive exploration of data queries; and\n•we conduct extensive experiments on a variety of datasets that\nproves the effectiveness of our approach.\n2 R ELATED WORK\nOur work proceeds from recent work in two mostly disparate ﬁelds;\ndata management and neural networks. In data management, we discuss\narchitectures, data structures, and algorithms that exploit access patterns\nto offer better performance. In neural networks, we review some of\nthe recent applications of neural networks to novel domains, as well as\nrelevant work on the interpretability of deep networks.\n2.1 Data management\nThe importance of data management technology in the context of in-\nteractive data exploration has been recognized for over 30 years, with\nthe work of MacKinlay, Stolte, and collaborators in Polaris, APT, and\nShow Me being central contributions to the ﬁeld [27, 28, 39] . Since\nthen, researchers in both data management and visualization have ex-\ntended the capabilities of data exploration systems (both visual and\notherwise) in a number of ways. Scalable, low-latency systems now\nexist for both the exact and approximate querying situation [1, 47] . Re-\ncently, Wu et al. have proposed that data management systems should\nbe designed speciﬁcally with visualization in mind [46]. NeuralCubes,\nas we will later discuss, provides evidence that machine learning tech-\nniques should also be designed with visualization in mind, and that\nsuch design enables novel visual data exploration tools.\nWhile we developed NeuralCubes to leveraging machine learning\ntechnology for providing richer information during data exploration\nitself, we are clearly not the ﬁrst to propose to use machine learning\ntechniques in the context of data management. Notably, ML has been\nrecently used to enable predictive interaction : if a system can accurately\npredict the future behavior of the user, there are ample opportunities\nfor performance gains (and speciﬁcally for hiding latency) [2, 6].\nGray et al.’s breakthrough idea of organizing aggregation queries in\nthe appropriate lattice — the now-ubiquitous data cube — spawned an\nentire subﬁeld of advances in algorithms and data structures [12, 13, 38] .\nThis work has gained renewed interest in the context of inter-\nactive exploration, where additional information (such as screen\nresolution, visualization encoding, and query prediction) can be\nleveraged [16, 24, 26, 35] . Since every query in NeuralCubes is exe-\ncuted by a ﬁxed-size network , it also provides low latency in aggre-\ngation queries. But because we design NeuralCubes speciﬁcally so\nthat networks learn the interaction between query inputs and results, it\nprovides additional information about the dataset that can itself be used\nin interactive exploration.\n2.2 Deep Neural Networks\nOur approach is inspired by the recent success of applying deep neural\nnetworks to a variety of domains, including image recognition [23], ma-\nchine translation [41], and speech recognition [14]. These techniques\nare solely focused on prediction, and our method is similar, in that we\nare focused on training deep networks for the purposes of query pre-\ndiction. Yet, we differ in that prediction is not the only goal, rather we\nwant to perform learning in a manner that provides the user a fast and\n2\n\nFig. 2. Comparison of systems using traditional database, Datacubes\nand NeuralCubes.\nlow-memory-cost way to visually explore data. The query prediction\ntask at hand can be viewed as a means to realize these goals.\nOur approach to training neural networks for data exploration can be\nviewed as a form of making deep networks more interpretable. In the vi-\nsual analytics (V A) literature there has been much recent work devoted\nto the interpretability of deep networks, namely with respect to the\ntraining process, the learned features of the network, the input domain,\nand the network’s output space. Liu et al. [25] visualize convolutional\nneural networks (CNNs) by clustering activations within each layer\ngiven a set of inputs, and visualize prominent ﬂows between clustered\nactivations. Other V A approaches to interpreting CNNs have considered\nvisualizing correlations between classes during the training process [4],\nand visualizing per-layer convolution ﬁlters and their relationships, in\norder to understand ﬁlters that are important to training [36]. Visual-\nizing and understanding recurrent neural networks (RNNs) has also\nreceived much attention, through understanding the training process [5],\nas well as understanding hidden states dynamics [40] . All of these\napproaches seek to provide interpretability for deep neural networks\nthat were never designed to be interpretable. In contrast, our approach\ndirectly builds interpretability into the network, such that the user can\ntake advantage of different aspects of the learned network to help their\nexploration.\nIn this context, our method for learning features of aggregation\nqueries can be viewed as a form of unsupervised learning, where treat-\ning query prediction as pretext, the features that we learn along the way\ncan be used for other purposes – in our case exploratory purposes. This\nis similar to recent techniques in computer vision that learn features\nusing different forms of self supervision, for instance learning to predict\nspatial context [9, 32] , temporal context [44], and perhaps more perti-\nnent to our work, learning to count visual primitives in a scene [33].\nThese techniques solve certain types of relevant visual tasks that do\nnot require human supervision, but then extract the learned features for\nsupervised learning. Our approach is similar: our training data does not\nrequire human intervention, since it is built from existing data cubes\ntechniques, yet the features that we learn from this task can be used to\nhelp with visual data exploration.\nWe also note that there is some very recent work that seeks to\ncombine databases with neural networks. Kraska et al. [22] make the\nconnection between indexing, such as b-trees or hashes, and models,\nand show that such indexing schemes can be learned using neural\nnetworks. Mitzenmacher [31] consider similar learning techniques for\nBloom ﬁlters. These methods are concerned with using neural networks\nto speed up computation and minimize memory storage. Although we\ndemonstrate that our method can attain these beneﬁts, the primary focus\nof our method is in using a neural network as an integral component to\nvisual exploration, i.e. NeuralCubes is not trying to predict anyqueries\nthat a database can answer.3 N EURAL CUBES : R EPLACING A DATABASE WITH A\nLEARNED NEURAL NETWORK\nWe introduce our approach by comparing it with visualization systems\nthat utilizing traditional database and those using advanced preprocess-\ning techniques. Fig. 2 is an overview of these different approaches.\nFirst, we brieﬂy discuss what data queries does a interactive visu-\nalization system need. Suppose we are given a set of records , each\nrecord contains a set of attributes , and each attribute has a certain type,\nfor instance continuous, categorical, geographic, and temporal, that\ncharacterizes the set of values it may take on. Database queries may\nreturn a single record, or multiple records, and in the case of the latter\nit is often of interest to summarize the set of records by performing\nanaggregation , for instance count ,average , ormax , depending on\nthe attribute type. Within a visualization system, the set of attributes,\ntheir types, and the class of aggregations determine the sorts of queries\none may issue that serve as the backbone for visual interaction. For\ninstance, we may perform a group-by query for a given attribute that\nwill return, for each of its values, the result of a speciﬁed aggregation,\ne.g.count . If the attribute type is categorical or temporal, then we\ncan visualize this result as a histogram, whereas if the attribute type is\ngeographic, we may plot the result as a heatmap over a spatial region.\nNaively, a visualization system can always issue SQL queries to the\ndatabase to get the needed information. As summarized in Fig. 3, this\napproach does support “cold start”, which means no extra processing is\nneeded when the data changes (e.g. adding new records). Besides, there\nis no need for extra memory other than the data set itself. The results\nwill always be accurate. However, this approach obviously won’t scale\nwhen the dataset gets larger, which stops it from being practically used\nin interactive visualization systems for large dataset.\nMany database techniques have been proposed with visualizations\nin mind. For example, datacubes based proposals [24, 35, 45] are built\nwith the goal of answering aggregational queries in real-time. In par-\nticular, they are focused on linked views, such that selection of an\nattribute in one view updates the visualization of other views, where\nview updates are based on a set of queries made to the database. Linked\nviews enable the user to engage with the visualization in an iterative\nprocess: the user spots a trend in one view, makes a selection based on\nthe trend (i.e. ﬁlter on a geographic region), they are presented with\nupdated views in the remaining attributes, and their interaction process\nrepeats. However, these techniques usually requires a large memory\nfootprint and long preprocessing time. While some datacubes based\ntechnique can provide absolute correct answer at a given resolution,\nmany sampling based techniques can not guarantee controllable error.\nThe basic idea behind NeuralCubes is the use of neural networks to\nlearn the process of performing database aggregation queries. It is thus\nuseful to think about the neural network as a function that approximates\na database aggregation query, where the input is a data query in the\nform of a set of attribute ranges, and the output is the aggregation of the\ndata returned from the given input query. The fundamental difference\nof NeuralCubes with existing techniques is that NeuralCubes is trying\nto solve the problem in a pure machine learning perspective. Thus\nNeuralCubes has two modes: ofﬂine training and real-time querying.\nDuring ofﬂine training, we generate query-result pairs and train neural\nnetworks to learn to predict results for queries. Then at real-time\nquerying stage, new queries will be evaluated by the trained model to\nget a predicted result.\nLike most machine learning techniques, the predicted query results\nfrom NeuralCubes don’t have a strict error bound. However, we can\npractically control the error by using neural networks with enough\ncapacity and feeding in a large enough training set. Furthermore,\nNeuralCubes is designed for visualization. The absolute error is visually\nneglectable as long as the overall trend and distribution reﬂects the truth.\nUser can always issue SQL queries to the database to get the accurate\nnumbers.\nWe summarize the trade-offs of different approaches discussed above\nin Fig. 3.\n3\n\nFig. 3. Comparison of supported features of different approaches.\nFig. 4. The corresponding input-out pairs for one state of the UI.\n3.1 What’s the input of NeuralCubes?\nWe ﬁrst deﬁne the concept of state of a data visualization system. We\nassume that the underlying database schema has a total of dattributes,\nwhere we denote each attribute by ai;1\u0014i\u0014d, and we represent the\nrange selection operation for a given attribute adbyr(ad). For instance,\nif an attribute was hour-of-day, then the range operation on this attribute\nwould return a set of hours. At any time, there must be a range selected\non each attribute. (No selection on a attribute means select the full range\nof this attribute.) The set of these ranges of different attributes deter-\nmine what the plots of each attribute look like. We call this set of ranges\nastate of the visualization system, denoted as S=r(a1);r(a2); :::r(ad).\nThe corresponding query results are DB(S)2R. Our objective is to\ntrain a neural network f(S)2Rto best approximate DB, given training\ndata D= ((S1;DB(S1));(S2;DB(S2)); :::(Sn;DB(Sn))), where Siis a\nstate, i.e. Si= (ri(a1);ri(a2); :::ri(ad)), and DB(Sn)is the aggrega-\ntional result from the database. For example, the state shown in Fig. 4\nisS=f[3;5];[1;5]g.\nThen we determine the representation of the range selection that is\nfed into the network. This is nontrivial due to the different types of\nattributes, e.g. geographic, temporal, categorical, as well as the types of\nselections that can be performed on attributes, e.g. spatially contiguous\nselections in geographic coordinates. To address these challenges in a\nuniﬁed manner, we use many-hot encodings for attribute selections, as\nshown in Fig. 4. Many-hot encodings are generalizations of one-hot en-\ncodings, commonly used as a way to uniquely represent words in neural\nlanguage models [3], categorical inputs for generative models [10], as\nwell as geographic coordinates for image recognition [42].\nMore speciﬁcally, for a given attribute aiwe assume that it may\nbe discretized into m(ai)many values. For certain attributes, this\nassumption is natural: categorical data, temporal data such as hour-of-\nday or day-of-week, while for continuously-valued data we uniformly\nFig. 5. We highlight the general structure of our neural network. For each\nattribute, we ﬁrst learn a feature embedding (orange), and then use the\nembedding for two purposes: we concatenate the embeddings to predict\naggregations (red), as well as learn a 2D projection (brown).\ndiscretize the data space into bins. For the attribute’s selection r(ai),\nwe then associate a binary vector r(ai)2f0;1gm(ai)such that r(ai)j=\n1if the value at index jbelongs to the selection, and 0otherwise.\nThis permits arbitrary types of selections for categorical and temporal\ndata. Spatial data, speciﬁcally 2D geographic regions, is slightly more\ncomplicated: one option is to represent each discretized cell as a single\ndimension in r, but this would result in a large number of inputs for even\nsmall spatial resolutions. We simplify this problem by restrict selections\non 2D regions to be rectangular. Thus to represent such a region we\nassociate a pair of vectors rx(ai)andry(ai)to for the selected x and y\nintervals, respectively, of the rectangle and then concatenate these two\nvectors to form the input. In practice, non-rectangular selections can be\napproximated by issuing multiple rectangular selections.\nWhen generating training set, the ground truth results for the queries\nare obtained from making actual database queries. At runtime, new\nqueries will be encoded in the same way as in training stage. Then it\nwill be fed into the trained model to get a predicted result.\n3.2 Generating Training Data: Modeling Application and\nUser Interaction\n3.2.1 Collecting vs. Generating Training Set\nMachine learning models can be seen as a function of the data that\nthey are trained on [21]. It is of integral importance that the training\nset reﬂects the goal of the network. In many applications of neural\nnetworks such as image classiﬁers, training data sets must be gathered\nfrom the real world, and manually labeled by a large number of human\nworkers [8]. In contrast, since NeuralCubes are used to approximate\ndatabase queries, a training set can instead be generated by executing\nqueries against a dataset to form ground truth.\n3.2.2 Sampling in Input Space vs. User Query Space\nWhen generating a training set, it is important to be careful how the\nqueries are generated. The space of potential queries is exponential in\nthe size of the range of values that can be queried over. But within an\ninformation visualization, some queries are much more likely, and thus\nmuch more important for the neural network to predict accurately. By\nchoosing a sampling strategy that mirrors the types of queries that will\nbe called by the visualization, we can focus the learning problem on\nthe relevant data distribution.\nAs discussed in section 3.1, we should randomly generate a state\nof the UI and then turn it into corresponding queries. Thus, to gener-\nate queries, we ﬁrst generate a range selection for each attribute, e.g.\ncontiguous ranges for temporal or spatial attributes. Then we perform\nagroup by query on one of the attribute with the constrains (range\nselections) on other attributes, resulting in a batch of query-result pairs\n4\n\nfor the current attribute. We do the same thing for every attribute, thus\ngiving us all query-result pairs of a state .\nIn order to sample a range selection over an attribute, there are two\nstrategies we can apply:\n1.We can uniformly sample a lower bound of the range from all\npossible values of the attribute and then uniformly choose a valid\nupper bound. For example, to generate a range selection for\nmonth (represented by integers from 1to12), we randomly choose\na lower bound, say 9. Then the all possible (exclusive) upper\nbounds are 10,11,12and13. So we just randomly choose one\nfrom them, say 11. Finally this range selection will be [9;11).\n2.We can uniformly sample the length of the range from all valid\nlengths and then uniformly choose a start and end position for\nthat range. Using month as an example again. The possible length\nof ranges we can make for month are from 1to12. We randomly\nchoose a length, say 3. Then, for a length-three range, the possible\ninclusive starting points/lower bounds are 1;2;3;4;5;6;7;8;9;10.\nSo we randomly choose one from them, say 2. Finally this range\nselection will be [2;5).\nThe difference between these two strategies is that strategy 1 gen-\nerates more short-length ranges while strategy 2 generate more long-\nlength ranges. In our experiments, we found people are likely to make\nqueries that has more long-length ranges. An extreme example is the\nmost frequently issued query - the default view for a data visualization\ndashboard, for which we select full lengths at every attribute. So we\napply strategy 2 for all the use cases shown in Section 6.\nWhile it may seem artiﬁcial to carefully sample a training set to\nmake the network ﬁt a certain kind of input, it’s important to remember\nthat NeuralCubes is designed, ﬁrst and foremost, with visualization in\nmind. Thus, even if we aren’t necessarily learning over the full data\ndistribution of queries, so long as our sampling resembles the manner\nin which users perform selection, then a user’s interaction with the\nnetwork should remain meaningful.\n3.3 A Neural Network Architecture for Data Queries\nOur neural network is composed of a sequence of layers , where a layer\nis deﬁned as the application of an afﬁne function, followed by applying\nan elementwise nonlinear function. In this paper we exclusively use\nfully connected layers. A fully connected layer at index iis parameter-\nized by a weight matrix Wi2Rb\u0002aand bias vector bi2Rb, where it is\nassumed the previous layer is a vector of length a, and the output of the\nlayer produces a vector of length b. We denote the afﬁne function at\nlayer ibygi:Ra!Rb, the nonlinearity by hi:Rb!Rb, and thus the\nfunction fis:f=hn\u000egn\u000ehn\u00001\u000egn\u00001: : :h1\u000eg1. Our objective then\nis to ﬁnd the sets of parameters, namely the weight matrices and biases,\nfor each layer that result in fbeing a good approximator of DB.\n3.3.1 Predicting Aggregations\nOur general strategy for predicting aggregations is to learn an embed-\nding for each type of attribute selection, followed by concatenating\nthe attribute embeddings, and then predicting the aggregation query\nfrom the concatenation. The intuition behind this architecture is to\nﬁrst learn attribute-speciﬁc features that are predictive of aggregation\nqueries , providing us a more informative representation than the in-\nput many-hot encodings, and then to combine these features to learn\ntheir relationships in predicting the aggregation. We use this general\narchitecture for all of the datasets in the paper, shown in Fig. 5, but\ntailor the architectures based on the given dataset, which we defer to\nSection 6. All networks, nonetheless, share the following steps to form\nthe network f:\n1.Learning Attribute Embeddings. For a given set of attribute\nselections represented as binary vectors, we ﬁrst transform each\nof them separately into their own feature embedding. Namely, for\nattribute ai, let fi:Rm(ai)!Rdirepresent a series of layers that\ntransforms the attribute selection to a di-dimensional embedding\nspace.2.Attribute Embedding Concatenation. We then\nconcatenate the embeddings into a single vector\nˆf= [ f1(r(a1));f2(r(a2)); :::fd(r(ad))], where ˆf2Rˆd,\nˆd=åd\ni=1di.\n3.Aggregation Query Prediction. Given the concatenated embed-\nding ˆf, we then feed it through a series of layers, where the last\nlayer outputs a single value, corresponding to the aggregation\nquery. Multiple fully connected layers are used in order to learn\nthe relationship between the attributes, so as to make better pre-\ndictions.\nPrediction Loss. Given the neural network f, we can now optimize\nover its set of parameters to best predict database queries DB. For this\npurpose, we deﬁne a loss function for prediction that combines an L1\nloss and a mean-squared loss for a given query Q:\nLpred=l1jf(Q)\u0000DB(Q)j+l2(f(Q)\u0000DB(Q))2; (1)\nwhere l1andl2weight the contributions of the L1 and mean-squared\nlosses, respectively. The intuition behind this loss is to learn the general\ntrend in the data, captured by the L2 loss, but in order for the training\nto not be overwhelmed by aggregations that result in very large values,\nthe L1 loss provides a form of robustness.\n3.3.2 Autoencoder: Reconstruction as Regularization\nDeep neural networks are easy to overﬁt because they have large amount\nof parameters. To avoid that, we need add some regularization in train-\ning. Reconstruction as regularization has been used in many recent\nwork [20, 37] . We think this approach aligns well with our main goal:\nwe’d like the model to learn the underlying data distribution than memo-\nrize the correlation between the noise in the input and the corresponding\noutput. Also, the reconstruction step can provide opportunities for new\ntypes of visualization, which we’ll discuss more in section 4.3.\nWe achieve this by deﬁning an autoencoder [15] for each attribute\nquery ai. More speciﬁcally, we learn a projection to 2D through a\nseries of layers, starting from the input layer, going to 2D, denoted as\nanencoder byei:Rdi!R2. We also want to project back: reconstruct\nthe original query (via its binary representation) from its 2D position,\nor adecoder di:R2!Rm(ai). Then the ﬁrst few layers in the encoder\nwill be shared with the regressor as shown in Fig. 5.\nAutoencoder Loss. Since we represent attribute selections as binary-\nvalued, a suitable loss function for measuring the quality of our autoen-\ncoder is the binary cross-entropy loss:\nLae=\u0000m(ai)\nå\nj=1\u0000\nr(ai)jlog(di(ei(zi))j)+(1\u0000r(ai)j)log(1\u0000di(ei(zi))j)\u0001\n;\n(2)\nwhere zi=fi(r(ai))is the feature embedding of the query selection.\nNote that this loss is deﬁned for each attribute, in order to learn attribute-\nspeciﬁc autoencoders.\n3.3.3 Combining Together\nWe combine the prediction loss and the autoencoder loss to learn a\nfunction that can both predict queries as well as learn 2D projections of\nattributes:\nL=Lpred+l3Lae; (3)\nwhere l3is a weight giving importance to the autoencoder, relative to\nthe weights on the prediction loss. One can view this objective as a type\nof multi-task autoencoder [11]: we want to learn an embedding, and a\n2D projection, that enables self-reconstruction, while simultaneously\nlearning to predict query aggregations. Importantly, this permits us to\ncontextualize attribute selections with respect to the aggregation task.\nThe prediction task can be viewed as a form of supervision for the 2D\nprojection task, thus attribute selections that result in similar predictions\nwill have similar feature embeddings, as well as similar 2D projections.\n5\n\nFig. 6. NeuralCubes can be used in a similar fashion to traditional data\ncubes techniques, allowing us to plot histograms and heatmaps with\nrespect to various attribute selections.\n4 U SING NEURAL CUBES FOR VISUAL EXPLORATION\nIn this section, we describe how NeuralCubes can be used to build\ninteractive data visualization systems.\n4.1 Plotting Histograms and Heatmaps with NeuralCubes\nIn traditional data cubes techniques, queries are typically made in order\nto plot histograms (for 1D attributes) and heatmaps (for 2D attributes).\nThis is typically realized through group by queries, where selections\nare made for all but one attribute, and then for the held-out attribute, a\nsingle query is made to gather aggregations for each of its values, i.e.\nSELECT COUNT(*) FROM BrightkiteTable\nGROUP_BY dayofweek\nNeuralCubes can enable the same type of visual exploration. More\nspeciﬁcally, we perform a group by query through our many-hot input\nencoding, placing a 1 on the attribute value that we would like to query,\nand a 0 for all other attribute values. Furthermore, we can take advan-\ntage of GPU data-parallelism in neural network implementations, and\nperform this operation in a single mini-batch, providing a signiﬁcant\nspeed-up through GPU acceleration. Our interface allows the user to\nperform arbitrary range selections for a given attribute, and enables in-\nteractive updates of histograms/heatmaps over the remaining attributes,\nsee Fig. 6 for an illustration.\n4.2 Evaluating at Client Side\nAnother advantage of NeuralCubes is that the trained model is small\nenough to be sent to client side for evaluation. In comparison, other\nOLAP datacubes based techniques requires network connections with a\nbackend server for interaction. This advantage of NeuralCubes can be\nbeneﬁcial to both system users and service providers. First, users can\nexpect better experience when making queries. Being able to evaluate\nat client side, NeuralCubes can eliminate network latency, which is\nusually a bottleneck. Secondly, service providers can expect much\nlower cost because the same server can provide service to much more\nusers since Query Per Second (QPS) will be signiﬁcantly lower than\nother client-server systems.\n4.3 Visualizing Attribute Latent Spaces\nAlthough the network can replicate the types of queries perform with\ndata cubes techniques, we can also use different structures that the net-\nwork learned to enable new forms of visual exploration. In particular,\nwe allow the user to explore the space of attribute selections through\neach attribute’s learned 2D projection, as discussed in Section 3.3.2.\nTo enable this, we ﬁrst generate all possible ranges selections for a\nFig. 7. We show how the user can jointly interact with traditional his-\ntogram visualizations and latent space visualizations learned by Neu-\nralCubes. On the left we initially show an histogram of counts of social\nmedia check-ins dataset of Austin for attribute Month . Upon selecting\nmonth March (1), the latent space on the right updates, to reﬂect this\nselection (2) (colored by red circular stroke with larger width) and also\nhighlight (colored by red circular stroke with smaller width) all possible\nselections with the same length of range (which is 1in this case). By\nperforming a selection in the latent space (3), the user can explore the\nfrequency of attribute selections (4) that belong to the selected latent\nspace subset.\ngiven attribute, and use the autoencoder to create an overview of their\ndistribution in the 2D space, where we visually encode attributes in a\nscatterplot. Each point in the latent space view represent a selection\nof this attribute, where the radius of the point is proportional to its\naggregation value, and the color of the point represents the range of the\nselection, namely the number of values selected in the attribute. Im-\nportantly, we ensure that the latent space and the histograms/heatmaps\ndiscussed previously in Sec. 4.1 are linked , so that interactions in one\nview update the other view, see Fig. 7 for an example.\n5 I MPLEMENTATION\nSoftware NeuralCubes is written in Python 3.6 with PyTorch ver-\nsion 0.4. It is implemented in the way that the neural network archi-\ntecture is dynamically created given a JSON conﬁguration ﬁle. When\nchanging a dataset or updating the neural networks, users only need to\nprovide a new JSON conﬁguration. For a given trained model, we uti-\nlize it in a Flask http backend server, providing RESTful web services.\nTo test the functionality of evaluating at client side, we converted the\nmodel from PyTorch format to TensorFlow format. Then we use Ten-\nsorFlow.js in client side for model evaluation. We have implemented a\nweb user interface, NeuralCubes Viewer, for interaction, implemented\nin Javascript using React and React-Vis. In order to make the training\nset generation faster, we performed ground truth aggregation queries in\nC++.\nHardware All the models are trained on a machine with an 8 core\nIntel i7-7700K 4.20GHz CPU, 32GB main memory, and a Nvidia GTX\n1080 Ti GPU with 12GB video memory.\nDatasets A summary of datasets and training/testing statistics of\nall the case studies is provided in Fig. 8. We evaluated our method on\nheld-out test datasets, generated in the same manner as training data.\nTesting error is computed as the average L1 norm difference between\nthe predictions and ground truth, scaled by the inverse of the mean of\nthe ground truth set, in order to be commensurable across datasets.\nArchitectures While the architectures we used to train the models\nwere all quite similar, there are some distinctions between them that\nwe describe here for the sake of reproducibility. The table in Fig. 9\ndescribes the architectures used in each of the trained models we discuss\nin this paper.\n6\n\nDataSet Raw Data Size # States M. Size RAE\nB.K. NYC 79k (3.1MB) 10k 703KB 4.25%\nB.K Austin 22k (0.8MB) 10k 703KB 3.88%\nFlights Count 5m (204MB) 60k 1.2MB 3.11%\nFlights Delay 5m (204MB) 60k 1.2MB 6.58%\nYellow Cab 12m (1.8GB) 30k 798KB 0.97%\nSPLOM 100k (3.9MB) 10k 135KB 2.64%\nFig. 8. Summary of training results for the experiments reported in this\npaper. # States means the number of states used in the training set. M.\nSize represents the saved ﬁle size of trained models. RAE represents\nRelative Absolute Error, which is calculated as RAE =åijˆyi\u0000yij\nåijyi\u0000¯yj\u0002100% ,\nwhere ˆyiis the model’s prediction for query i,yiis the ground truth for\nquery i, and ¯yis the average value of yiin the testing set.\nMini-Batch When training neural networks, a mini-batch usually is\nconstructed by randomly choosing a certain number of training samples.\nHowever, in practice, we found training is not very stable using this\napproach for our setup. One of the possible reasons could be the fact\nthat the type of queries are unbalanced in the training set. For example,\nfor an attribute with 10 dimensions, there will be 10 queries needed to\nplot a line chart for it. For an attribute with 5 dimensions, there will only\nbe 5 queries. Since our training set is generated by selecting the same\nnumber of random ranges for each attribute, the number of queries for\ndifferent attributes will be different. Speciﬁcally, attributes with higher\ndimensionality will have more sample queries than attributes with\nlower dimensionality. People have proposed solutions like stratiﬁed\nsampling [48] to solve this problem. In this paper, we take advantage\nof being able to control how we generate the training set and propose\nastate based mini-batch construction strategy. At training time, a\nmini-batch will be composed by the corresponding queries of several\nrandomly selected states .\nTraining Although we need to tune parameters for each different\nmodel, there are some empirical best practice that we found stay stable\nin our experiments. When choosing the right weights l1;l2;l3, it\nusually works when setting l1to be 1. Next, set l2so that l2L1is one\nor two order of magnitude larger than l1Lae. Then set l3so that l3L2\nis at least two order of magnitude smaller than l2L1. Sometimes setting\nl3to be 0:0achieves better performance. When choosing the optimizer\nfor training, we found if we only use mini-batch gradient descent(GD),\nit’s hard for NeuralCubes to converge. To solve this problem, we used\na similar approach as described in [18], which simply use Adam [19]\nfor the ﬁrst 10 to 20 epochs of training then switch to mini-batch\ngradient descent. This technique works well for all the training in our\nexperiments.\n6 E XPERIMENTAL EVALUATION\nWe have explored our method in the context of a variety of datasets,\nwhere in this section we highlight quantitative results of our method\nin predicting aggregations, and qualitative results of our proposed\nvisualization techniques. We note through all results, we show our\npredictions as orange curves, and the ground truth aggregations in blue\ncurves, in order to consistently compare our method with the actual\ndatabase queries.\n6.1 Brightkite Social Media Check-ins\nHere we study Brightkite [7]social media check-ins to assess the capa-\nbility of NeuralCubes to learn the count aggregation. Fig. 10 shows\nexamples of how our technique can be treated analogous to a data cubes\nsystem, capable of plotting histograms and heatmaps from aggregation\nqueries. We also use this dataset to evaluate training stability and to\ntest the learned latent space contains meaningful information or not.\n6.1.1 Training Details\nWe choose two separate metropolitan areas and train separate networks\non each: New York City and Austin. We choose month, day of week,Input Autoencoder Regressor\nBrightKite\nMonth (12) [8,4, 2, 4, 8]\n[220]Day of Week (7) [8,4, 2, 4, 8]\nHour (24) [12,6, 2, 6, 12]\nGeospatial (40) [400, 128, 2, 128, 400]\nFlights (count)\nMonth (12) [120, 20, 2, 20, 120]\n[256, 128]Day of Week (7) [70,20, 2, 20, 70]\nHour (24) [240, 20, 2, 20, 240]\nGeospatial (40) [400, 128, 20, 2, 20, 128, 400]\nCarrier (10) [100, 20, 2, 20, 100]\nDelayBin (14) [140, 32, 2, 32, 140]\nFlights (delay)\nMonth (12) [120, 20, 2, 20, 120]\n[256, 128,\n64]Day of Week (7) [70,20, 2, 20, 70]\nHour (24) [240, 20, 2, 20, 240]\nGeospatial (40) [400, 128, 20, 2, 20, 128, 400]\nCarrier (10) [100, 20, 2, 20, 100]\nDelayBin (14) [140, 32, 2, 32, 140]\nYellow Cab\nMonth (12) [120, 20, 2, 20, 120]\n[220]Day of Week (7) [70,20, 2, 20, 70]\nHour (24) [240, 20, 2, 20, 240]\nGeospatial (40) [400, 128, 20, 2, 20, 128, 400]\nSPLOM\na0 (#bin) [16,8, 2, 8, 16]\n[120, 60]a1 (#bin) [16,8, 2, 8, 16]\na2 (#bin) [16,8, 2, 8, 16]\na3 (#bin) [16,8, 2, 8, 16]\na3 (#bin) [16,8, 2, 8, 16]\nFig. 9. Architectures for the neural networks used in the experiments\nwith NeuralCubes. The numbers in bold represent attribute embedding\nlayers as described in Fig. 5.\nhour, and geospatial information (longitude and latitude) as input dimen-\nsions, following common practice in the study of urban activity [24, 29] .\nThe longitude and latitude are encoded as 20+20=40bins, following\nthe strategy described in Fig. 4. The weights for L1 loss and L2 loss\nfor regressor and BCE loss for autoencoder are 20, 0.001, and 1 respec-\ntively. Each model is trained for 1000 epochs and each epoch takes 6\nseconds to train.\n6.1.2 Results\nThe predictions are good for both cities under the same neural network\nconﬁguration. This shows the generalizability of NeuralCubes.\nTraining Stability We also evaluated the training stability on\nBrightKite NYC dataset. Speciﬁcally, we independently run 50 training\non BrightKite NYC dataset with exactly the same conﬁgurations. Then\nwe record the testing error for each model for each epoch. The results\nare shown in Fig. 11. We can see that NeuralCubes always converge to\nthe same optimal.\nComparing Two Cities by Latent Space The latent space plots\nmay convey information that could not be directly perceived on his-\ntograms of counts. Fig. 12 shows an example. Firstly, the latent space\nforhour of both cities forms a loop for ranges with same length. This\nsuggests that NeuralCubes learned the fact that hour indeed has a re-\npeated circular pattern. However, the “circle” in Austin’s latent space\nhas a large opening. This could be caused by the difference of lifestyles\nof the two cities: New York City never sleeps, while Austin goes to bed\nat night.\n6.2 Flight Dataset\nWe use a dataset collected by the Bureau of Transportation Statistics\nconsisting of ﬂight delay information in the year 2008 [34]. For this\n7\n\nFig. 10. We show how NeuralCubes learned that there are more check-ins on University of Texas at Austin campus during daytime on weekdays.\nFig. 11. Testing error for 50 independent training on BrightKite NYC\ndataset with the same conﬁguration. Light green lines represent each\nindividual training. Solid green line represent the mean testing error.\nDarker green area represent standard deviation from mean. We can\nnotice some variance between different runs. However, all of them\neventually converge to relatively the same optimal.\ndataset, our ﬁrst experiment uses the total ﬂight counts as the aggre-\ngation operation. Since this dataset contains an attribute delay time ,\nwhich is also a meaningful attribute in which to aggregate, our second\nexperiment builds a model to predict the average delay time. Our goal\nwith this model is to check the extent to which NeuralCubes can learn\nnon-monotonic aggregations.\n6.2.1 Training Details\nCount Predictions For the count aggregation we ﬁlter ﬂights to\nbe within the contiguous United States, and restrict entries to only\nthe 10 most used airlines in the dataset, giving us a total of 5,092,321\nentries after removing entries containing missing data. We note that\nthis dataset has more entries and attributes than the Brightkite dataset,\nincluding a numeric variable (Delay Time). To encode the numeric\nvariable in our many-hot encoding, we bin the delays in 15 minute\nincrements. The weights for L1 loss, L2 loss and BCE loss are 1, 1e-7,\nand 1, respectively. Each model is trained for 500 epochs and each\nepoch takes 160 seconds to train.\nAverage Predictions We follow a similar training setup to count ,\nwith a couple exceptions. Since generating training samples to predict\naverage delay time itself is very time consuming, we dropped longitude\nand latitude columns in the raw data and discarded entries whose\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n0\n1,000\n2,000\n3,000\n4,000\n5,000\n6,000\n1 2 3 4\n123\n4\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n500\n1,000\n1,500\n02 3 4 1\n12\n34Fig. 12. In this ﬁgure, we compare the diurnal patterns present in the\nlatent space for “hour” in Austin and NYC. The top half shows the queries\nand latent space for NYC, while the bottom half shows them for Austin.\nWe highlight the sequence of queries in the latent space for sliding\nranges throughout the day, and notice that the activity in New Y ork City is\nmore circular in nature. We ﬁnd this pattern always exists within multiple\nindependently trained models; though they may have different views (e.g.\nrotated, stretched) of the latent space due to randomness in the training.\ndelay time is smaller than \u000060minutes or larger than 140minutes. The\nweights for L1 loss, L2 loss and BCE loss are 10, 10, and 1, respectively.\nEach model is trained for 500 epochs and each epoch takes 160 seconds\nto train.\n6.2.2 Results\nFig. 8 shows the quantitative results for the two types of aggregation\nqueries. Overall, we ﬁnd the errors to be competitive, if not lower, than\nBrightkite, showing that our method is capable of handling different\ntypes of attributes, as well as different forms of aggregation. Further-\nmore, for this dataset note that the size is quite large – 204MB, while\nthe size of our trained networks only occupy 1.2MB for both tasks,\ndemonstrating the signiﬁcant compression capabilities of our network.\n6.3 YellowCab Taxi Dataset\nWe use NYC YellowCab Taxi trip records of year 2015 from NYC Taxi\nand Limousine Commission (TLC) [43] to study the learning capacity\nof NeuralCubes in a series of controlled settings.\n8\n\nRaw Data Size # States Model Size Testing RAE\n12k (1.8MB) 30k 798KB 3.70%\n120k (18MB) 30k 798KB 2.04%\n1.2m (180MB) 30k 798KB 1.35%\n12m (1.8GB) 30k 798KB 0.97%\nFig. 13. Y ellowcab dataset with different raw data size.\nModel Size Training RAE Testing RAE\n113KB 5.06% 5.18%\n220KB 3.93% 4.03%\n798KB 3.59% 3.70%\n1.7MB 2.98% 3.10%\nFig. 14. Performance evaluation of different model size (e.g. different\nnumber of hidden layers) on the same dataset. In this experiment, we\ntested four different models on the Y ellow Cab 12k dataset.\n6.3.1 Training Details\nWe choose month, day of week, hour, and pickup location (longitude\nand latitude) as input dimensions. The longitude and latitude are en-\ncoded as 20+20=40bins. We created four different datasets under\nthis same schema by sampling 1k records per month, 10k records per\nmonth, 100k records per month and 1 million records per month re-\nspectively from the original dataset. We refer to these four datasets as\nYC-1K, YC-10K, YC-100K, and YC-1M respectively. (We performed\ndata cleaning and ﬁltering after sampling. So the actual number of\nrecords of each month of a dataset will be slightly less than the sampled\nnumber.) As described in Fig. 9, we use the same network conﬁguration\nfor the four datasets. The weights for L1 loss, L2 loss and autoencoder\nloss are 100.0, 0.0 and 1.0 for YC-1K, YC-10K and YC-100K. For\nYC-1M, the weights for L1 loss, L2 loss and autoencoder loss are 10.0,\n0.0 and 1.0. Each model is trained for 1000 epochs and each epoch\ntakes 15 seconds to train.\n6.3.2 Results\nRaw Data Size We training results for YC-1K, YC-10k, YC-100k\nand YC-1M are in Fig. 13. We notice a fact that the testing error\nbecomes smaller when the raw data size increase. We think this is due\nto the fact that when more data are available, there will be less noise.\nThe data distributions that NeuralCubes need to learn will be more\nsmooth, requiring less learning capacity. Since we are using the same\nneural network conﬁguration, we can expect lower error on dataset that\ndoesn’t require large learning capacity.\nModel Size Another experiment we did is using neural networks\nwith different sizes for the same dataset. We start from a small neural\nnetwork and then gradually increase the number of hidden layers and\nneurons to see if the prediction improves. The results are in Fig. 14.\nNot surprisingly, with larger neural networks, the error reduces.\nTraining Set Size Another factor that can inﬂuence the training is\nthe size of training set. In this experiment, we tested the same model\nbut with different number of training sets. Results can be found in\nFig. 15. We can see a signiﬁcant accuracy improvement when using\n30k training states than 15k training states. However, the improvement\nfrom using 30k to 60k is very small. The reason is that the capacity of\nthe neural network may be not enough for 60k training states.\n6.4 SPLOM Dataset\nLast, we use the synthetic SPLOM dataset of Kandel et al. [17] to\nvalidate whether NeuralCubes can learn how to predict aggregational\nvalues under a controlled setting. Since all the attributes of SPLOM\ndataset are real values, it also provides us an opportunity to study the\nbehavior of NeuralCubes when bin size increases.\nTraining Details Following the procedure described in [17], we\ngenerated 100 ;000entries of ﬁve-attribute records, and divide each\nattribute into a prescribed number of bins. We trained ﬁve different\nNeuralCubes using 10, 20, 30, 40, and 50 bins respectively. TheTraining States Training RAE Testing RAE\n15k 6.74% 6.85%\n30k 3.59% 3.70%\n60k 3.39% 3.47%\nFig. 15. Performance evaluation of different training set size on the same\ndataset. In this experiment, we tested three different models on the\nY ellow Cab 12k dataset with different number of states in training sets.\nBin Size # States Model Size Testing RAE\n10 10k 109KB 1.02%\n20 10k 116KB 1.85%\n30 10k 122KB 2.25%\n40 10k 129KB 2.09%\n50 10k 135KB 2.64%\nFig. 16. SPLOM dataset with different bin size.\nweights for L1 loss, L2 loss and autoencoder loss are 1.0, 0.0 and 1.0,\nrespectively.\nResults A detailed quantitative evaluation of training NeuralCubes\nfor SPLOM dataset can be found in the table of Fig. 16. As suggested\nin the table, when we increase the number of bins, the network requires\nmore neurons, and thus a higher capacity, to learn well. We note that\nthe increase in testing error is to be expected, for several reasons. The\nﬁrst reason is that when the bins are reﬁned, few records fall into the\nsame bin. So the variance within each bin is larger, making it more\ndifﬁcult for NeuralCubes to learn the underlying distribution. The\nsecond reason is that when the number of bins increase, the space of\npossible different queries grows exponentially. Yet, we didn’t increase\nthe training set.\n7 D ISCUSSION\nThe main limitation in NeuralCubes is the current dependence of the\nnetwork architecture on the dataset and schema complexity. While we\nwere able to successfully train these networks with a certain amount\nof experimentation, the process is more cumbersome than we would\nlike. More automated methods to choose among different network\narchitectures are still a current topic of research in machine learning,\nand beyond the scope of the current paper.\nAnother limitation is that the output of NeuralCubes are approxima-\ntions. However, in a visual data exploration system, it is the overall\ntrend and distribution that people mostly want to get. Whenever the user\nneeds the absolute correct answer, they can always query the database\nfor that. The value of NeuralCubes is that it provide a tool for them to\nquickly ﬁnd what question they want to ask to the data. Further more,\nNeuralCubes is designed for visualization. The ﬁnal results users get\nare heatmap, line charts, histograms, etc. As long as the error is small,\nusers can hardly tell the difference visually.\nOne notable feature of NeuralCubes is that the query training set\nis dependent on the affordances provided by the visual exploration\nsystem. This is an advantage in terms of machine learning, because\nthe additional information available allows us to simplify the problem\nof training a network capable of answering anyquery. At the same\ntime, the fact that we have full control over the training set of queries\nis sometimes a disadvantage, because a poorly-generated training set\ncan cause the training procedure to fail. Our proof-of-concept system\nshows the advantages that a neural network provides in the context of\na visual exploration system, but a thorough study on how to generate\nappropriate training sets remains a topic for future work.\n8 C ONCLUSION AND FUTURE WORK\nWe believe that the main value of NeuralCubes lies in its ability to\nlearn high-level features of the dataset from which powerful visual data\nexploration techniques become possible. While the accuracy of the\napproximated results seems well within what is to be expected of a\nneural network, we warn against expecting that NeuralCubes would be\nable to learn minute details of the dataset. Liu et al. [26] distinguished\n9\n\nbetween approximate, sampling-based systems and exact aggregation\nsystems. We would consider NeuralCubes to be approximate as well,\nbut approximate in its aggregations ; it trades exact accuracy of the\nanswer for higher-level knowledge about the queries to be answered.\nWe remain enthusiastic about the future of connecting interactive\ninformation visualization and machine learning. We are particularly\ninterested in solving the above two problems by leveraging recent work\nin user modeling and predictive interaction [2]. The better we can\npredict how people (through a particular UI) make queries against a\nDB in a visual exploration scenario, the more effectively we will be\nable to train NeuralCubes’s networks.\nBecause the process of aggregating queries is a differentiable func-\ntion, this opens up several opportunities, such as query sensitivity and\nthe discovery of queries that lead to user-prescribed aggregations. We\nare excited to explore these research avenues as part of future work.\nACKNOWLEDGMENTS\nThis work is supported in part by the National Science Founda-\ntion (NSF) under grant numbers IIS-1452977, IIS-1513651 and IIS-\n1815238; and by the Defense Advanced Research Projects Agency\n(DARPA) under agreement numbers FA8750-17-2-0107 and FA8750-\n19-C-0002; and by the U.S. Department of Energy, Ofﬁce of Science,\nOfﬁce of Advanced Scientiﬁc Computing Research, under Award Num-\nber(s) DE-SC-0019039.\nREFERENCES\n[1]S. Agarwal, B. Mozafari, A. Panda, H. Milner, S. Madden, and I. Stoica.\nBlinkdb: queries with bounded errors and bounded response times on\nvery large data. In Proceedings of the 8th ACM European Conference on\nComputer Systems , pp. 29–42. ACM, 2013.\n[2]L. Battle, R. Chang, and M. Stonebraker. Dynamic prefetching of data\ntiles for interactive visualization. In Proceedings of the 2016 International\nConference on Management of Data , pp. 1363–1375. ACM, 2016.\n[3]Y . Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic\nlanguage model. Journal of machine learning research , 3(Feb):1137–1155,\n2003.\n[4]A. Bilal, A. Jourabloo, M. Ye, X. Liu, and L. Ren. Do convolutional neural\nnetworks learn class hierarchy? IEEE transactions on visualization and\ncomputer graphics , 24(1):152–162, 2018.\n[5]D. Cashman, G. Patterson, A. Mosca, and R. Chang. Rnnbow: Visualizing\nlearning via backpropagation gradients in recurrent neural networks. In\nWorkshop on Visual Analytics for Deep Learning (VADL) , 2017.\n[6]S.-M. Chan, L. Xiao, J. Gerth, and P. Hanrahan. Maintaining interactivity\nwhile exploring massive time series. In Visual Analytics Science and\nTechnology, 2008. VAST’08. IEEE Symposium on , pp. 59–66. IEEE, 2008.\n[7]E. Cho, S. A. Myers, and J. Leskovec. Friendship and mobility: user\nmovement in location-based social networks. In Proceedings of the 17th\nACM SIGKDD international conference on Knowledge discovery and data\nmining , pp. 1082–1090. ACM, 2011.\n[8]J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A\nlarge-scale hierarchical image database. In Computer Vision and Pattern\nRecognition, 2009. CVPR 2009. IEEE Conference on , pp. 248–255. IEEE,\n2009.\n[9]C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual representation\nlearning by context prediction. In The IEEE International Conference on\nComputer Vision (ICCV) , December 2015.\n[10] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to generate\nchairs with convolutional neural networks. In Computer Vision and Pattern\nRecognition (CVPR), 2015 IEEE Conference on , pp. 1538–1546. IEEE,\n2015.\n[11] M. Ghifary, W. Bastiaan Kleijn, M. Zhang, and D. Balduzzi. Domain\ngeneralization for object recognition with multi-task autoencoders. In\nProceedings of the IEEE international conference on computer vision , pp.\n2551–2559, 2015.\n[12] J. Gray, S. Chaudhuri, A. Bosworth, A. Layman, D. Reichart, M. Venka-\ntrao, F. Pellow, and H. Pirahesh. Data cube: A relational aggregation\noperator generalizing group-by, cross-tab, and sub-totals. Data mining\nand knowledge discovery , 1(1):29–53, 1997.\n[13] V . Harinarayan, A. Rajaraman, and J. D. Ullman. Implementing data cubes\nefﬁciently. In Acm Sigmod Record , vol. 25, pp. 205–216. ACM, 1996.[14] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior,\nV . Vanhoucke, P. Nguyen, T. N. Sainath, et al. Deep neural networks\nfor acoustic modeling in speech recognition: The shared views of four\nresearch groups. IEEE Signal Processing Magazine , 29(6):82–97, 2012.\n[15] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of\ndata with neural networks. Science , 313(5786):504–507, 2006.\n[16] N. Kamat, P. Jayachandran, K. Tunga, and A. Nandi. Distributed and\ninteractive cube exploration. In Data Engineering (ICDE), 2014 IEEE\n30th International Conference on , pp. 472–483. IEEE, 2014.\n[17] S. Kandel, R. Parikh, A. Paepcke, J. M. Hellerstein, and J. Heer. Proﬁler:\nIntegrated statistical analysis and visualization for data quality assessment.\nInProceedings of the International Working Conference on Advanced\nVisual Interfaces , pp. 547–554. ACM, 2012.\n[18] N. S. Keskar and R. Socher. Improving generalization performance by\nswitching from adam to sgd. arXiv preprint arXiv:1712.07628 , 2017.\n[19] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization.\narXiv preprint arXiv:1412.6980 , 2014.\n[20] D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling. Semi-\nsupervised learning with deep generative models. In Advances in neural\ninformation processing systems , pp. 3581–3589, 2014.\n[21] P. W. Koh and P. Liang. Understanding black-box predictions via inﬂuence\nfunctions. arXiv preprint arXiv:1703.04730 , 2017.\n[22] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for\nlearned index structures. arXiv preprint arXiv:1712.01208 , 2017.\n[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation\nwith deep convolutional neural networks. In F. Pereira, C. J. C. Burges,\nL. Bottou, and K. Q. Weinberger, eds., Advances in Neural Information\nProcessing Systems 25 , pp. 1097–1105. Curran Associates, Inc., 2012.\n[24] L. Lins, J. T. Klosowski, and C. Scheidegger. Nanocubes for real-time\nexploration of spatiotemporal datasets. IEEE Transactions on Visualization\nand Computer Graphics , 19(12):2456–2465, 2013.\n[25] M. Liu, J. Shi, Z. Li, C. Li, J. Zhu, and S. Liu. Towards better analysis of\ndeep convolutional neural networks. IEEE transactions on visualization\nand computer graphics , 23(1):91–100, 2017.\n[26] Z. Liu, B. Jiang, and J. Heer. immens: Real-time visual querying of big\ndata. In Computer Graphics Forum , vol. 32, pp. 421–430. Wiley Online\nLibrary, 2013.\n[27] J. Mackinlay. Automating the design of graphical presentations of re-\nlational information. ACM Transactions On Graphics , 5(2):110–141,\n1986.\n[28] J. Mackinlay, P. Hanrahan, and C. Stolte. Show me: Automatic presenta-\ntion for visual analysis. IEEE transactions on visualization and computer\ngraphics , 13(6), 2007.\n[29] F. Miranda, H. Doraiswamy, M. Lage, K. Zhao, B. Gon c ¸alves, L. Wilson,\nM. Hsieh, and C. T. Silva. Urban pulse: Capturing the rhythm of cities.\nIEEE transactions on visualization and computer graphics , 23(1):791–800,\n2017.\n[30] F. Miranda, L. Lins, J. T. Klosowski, and C. T. Silva. Topkube: a rank-\naware data cube for real-time exploration of spatiotemporal data. IEEE\ntransactions on visualization and computer graphics , 24(3):1394–1407,\n2018.\n[31] M. Mitzenmacher. A model for learned bloom ﬁlters and related structures.\narXiv preprint arXiv:1802.00884 , 2018.\n[32] M. Noroozi and P. Favaro. Unsupervised learning of visual representations\nby solving jigsaw puzzles. In European Conference on Computer Vision ,\npp. 69–84. Springer, 2016.\n[33] M. Noroozi, H. Pirsiavash, and P. Favaro. Representation learning by\nlearning to count. In The IEEE International Conference on Computer\nVision (ICCV) , Oct 2017.\n[34] B. of Transportation Statistics. On-time performance.\nhttp://www.transtats.bts.gov/Fields.asp?Table ID=236. Accessed:\n2018-03-29.\n[35] C. A. Pahins, S. A. Stephens, C. Scheidegger, and J. L. Comba. Hashed-\ncubes: Simple, low memory, real-time visual exploration of big data.\nIEEE transactions on visualization and computer graphics , 23(1):671–\n680, 2017.\n[36] N. Pezzotti, T. H ¨ollt, J. Van Gemert, B. P. Lelieveldt, E. Eisemann, and\nA. Vilanova. Deepeyes: Progressive visual analytics for designing deep\nneural networks. IEEE transactions on visualization and computer graph-\nics, 24(1):98–108, 2018.\n[37] S. Sabour, N. Frosst, and G. E. Hinton. Dynamic routing between capsules.\nInAdvances in neural information processing systems , pp. 3856–3866,\n2017.\n10\n\n[38] Y . Sismanis, A. Deligiannakis, N. Roussopoulos, and Y . Kotidis. Dwarf:\nShrinking the petacube. In Proceedings of the 2002 ACM SIGMOD\ninternational conference on Management of data , pp. 464–475. ACM,\n2002.\n[39] C. Stolte, D. Tang, and P. Hanrahan. Polaris: A system for query, anal-\nysis, and visualization of multidimensional relational databases. IEEE\nTransactions on Visualization and Computer Graphics , 8(1):52–65, 2002.\n[40] H. Strobelt, S. Gehrmann, H. Pﬁster, and A. M. Rush. Lstmvis: A tool\nfor visual analysis of hidden state dynamics in recurrent neural networks.\nIEEE transactions on visualization and computer graphics , 24(1):667–676,\n2018.\n[41] I. Sutskever, O. Vinyals, and Q. V . Le. Sequence to sequence learning with\nneural networks. In Advances in neural information processing systems ,\npp. 3104–3112, 2014.\n[42] K. Tang, M. Paluri, L. Fei-Fei, R. Fergus, and L. Bourdev. Improving\nimage classiﬁcation with location context. In Proceedings of the IEEE\ninternational conference on computer vision , pp. 1008–1016, 2015.\n[43] N. Taxi and L. Commission. Yellowcab taxi trip records.\nhttp://www.nyc.gov/html/tlc/html/about/trip record data.shtml. Accessed:\n2018-09-14.\n[44] X. Wang and A. Gupta. Unsupervised learning of visual representations\nusing videos. In The IEEE International Conference on Computer Vision\n(ICCV) , December 2015.\n[45] Z. Wang, N. Ferreira, Y . Wei, A. S. Bhaskar, and C. Scheidegger. Gaussian\ncubes: Real-time modeling for visual exploration of large multidimen-\nsional datasets. IEEE transactions on visualization and computer graphics ,\n23(1):681–690, 2017.\n[46] E. Wu, L. Battle, and S. R. Madden. The case for data visualization\nmanagement systems: vision paper. Proceedings of the VLDB Endowment ,\n7(10):903–906, 2014.\n[47] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and I. Stoica.\nSpark: Cluster computing with working sets. 2010.\n[48] P. Zhao and T. Zhang. Accelerating minibatch stochastic gradient descent\nusing stratiﬁed sampling. arXiv preprint arXiv:1405.3080 , 2014.\n11",
  "textLength": 65288
}