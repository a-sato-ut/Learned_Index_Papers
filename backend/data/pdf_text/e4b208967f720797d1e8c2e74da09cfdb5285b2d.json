{
  "paperId": "e4b208967f720797d1e8c2e74da09cfdb5285b2d",
  "title": "On the Complexity of Algorithms with Predictions for Dynamic Graph Problems",
  "pdfPath": "e4b208967f720797d1e8c2e74da09cfdb5285b2d.pdf",
  "text": "On the Complexity of Algorithms with Predictions for Dynamic\nGraph Problems\nMonika Henzinger∗Barna Saha†Martin P. Seybold Christopher Ye\nAbstract\nAlgorithms with predictions is a new research direction that incorporates machine learn-\ning predictions into algorithm design. So far a plethora of works published in recent years\nhave incorporated the power of predictions to improve on worst-case optimal bounds for\nonline problems. In this paper, we initiate the study of complexity of dynamic data struc-\ntures with predictions, including dynamic graph algorithms. Unlike in online algorithms,\nthe main goal in dynamic data structures is to maintain the solution efficiently with every\nupdate.\nMotivated by prior work in online algorithms, we investigate three natural models of\npredictions: (1) ε-accurate predictions where each predicted request matches the true request\nwith probability at least ε, (2) list-accurate predictions where a true request comes from a\nlist of possible requests, and (3) bounded delay predictions where the true requests are some\n(unknown) permutations of the predicted requests. For ε-accurate predictions, we show that\nlower bounds from the non-prediction setting of a problem carry over, up to a 1 −εfactor.\nThen we give general reductions among the prediction models for a problem, showing that\nlower bounds for bounded delay imply lower bounds for list-accurate predictions, which\nimply lower bounds for ε-accurate predictions.\nFurther, we identify two broad problem classes based on lower bounds due to the Online\nMatrix Vector (OMv) conjecture. Specifically, we show that dynamic problems that are\nlocally correctable have strong conditional lower bounds for list-accurate predictions that\nare equivalent to the non-prediction setting, unless list-accurate predictions are perfect.\nMoreover, we show that dynamic problems that are locally reducible have a smooth transition\nin the running time, for the online and the offline setting with bounded delay predictions.\nWe categorize problems with known OMv lower bounds accordingly and give several upper\nbounds in the delay model that show that our lower bounds are almost tight, including\nproblems in dynamic graphs.\nWe note that concurrent work by v.d.Brand et al. [arXiv:2307.09961] and Liu and\nSrinivas [arXiv:2307.08890] independently study dynamic graph algorithms with predictions,\nbut their work is mostly focused on showing upper bounds.\n∗This project has received funding from the European Research Council (ERC)\nunder the European Union’s Horizon 2020 research and innovation programme (Grant agree-\nment No. 101019564 “The Design of Modern Fully Dynamic Data Structures (MoDynStruct)”\nand from the Austrian Science Fund (FWF) project Z 422-N, and project “Fast Algorithms for\na Reactive Network Layer (ReactNet)”, P 33775-N, with additional funding from the netidee SCIENCE Stiftung ,\n2020–2024.\n†University of California Berkeley. This work is supported partly by NSF 1652303, 1909046, and HDR\nTRIPODS 1934846 grants, and an Alfred P. Sloan Fellowship. This work was also supported by the Simons NTT\nresearch fellowship.\n1arXiv:2307.16771v2  [cs.DS]  10 Sep 2023\n\nContents\n1 Introduction 1\n1.1 Contribution and Paper Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n2 Preliminaries and OMv with Predictions 5\n2.1 The Online Matrix Vector Problem . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2 Upper and Lower Bounds for OMv with Predictions . . . . . . . . . . . . . . . . 6\n3 Dynamic Prediction Models with General Lower Bounds 7\n3.1 ε-Accurate Predictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n3.2 List Accurate Predictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n3.3 Bounded Delay Predictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n4 Extensions of the OMv Conjecture 11\n4.1 Sparse OMv Conjecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n5 Locally Correctable Problems: Lower Bounds for List-accurate Predictions 13\n5.1 Preliminaries: Edge Updates in Dynamic Graphs . . . . . . . . . . . . . . . . . . 13\n5.2 An OuMv Reduction for the # s-△Problem . . . . . . . . . . . . . . . . . . . . . 14\n5.3 Locally Correctable Dynamic Problems . . . . . . . . . . . . . . . . . . . . . . . 16\n6 Locally Reducible Problems: Lower Bounds for Delay Predictions 18\n6.1 Locally Reducible Dynamic Problems . . . . . . . . . . . . . . . . . . . . . . . . 18\n6.2 The # s-△Problem is Locally Reducible . . . . . . . . . . . . . . . . . . . . . . . 27\n7 Further Locally Reducible and Locally Correctable Problems 28\n7.1 Locally Correctable Fully-Dynamic Problems . . . . . . . . . . . . . . . . . . . . 31\n7.2 Locally Correctable Partially-Dynamic Problems . . . . . . . . . . . . . . . . . . 31\n8 Dynamic Algorithms with Bounded Delay Predictions 32\n8.1 Bounded Delay Predictions with Outliers . . . . . . . . . . . . . . . . . . . . . . 33\n8.2 Two Algorithms for the # s-△Problem . . . . . . . . . . . . . . . . . . . . . . . 34\n8.3 Subgraph Connectivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n8.3.1 Generalization to dAgnostic Algorithm . . . . . . . . . . . . . . . . . . . 43\n8.4 Transitive Closure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n8.5 Shortest Paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n8.6 Erickson’s Maximum Value Problem . . . . . . . . . . . . . . . . . . . . . . . . . 48\nA Lower Bounds for OMv with Predictions 59\nB Concurrent Work 60\n2\n\n1 Introduction\nModern Machine Learning predictions models are surprisingly accurate in practice and exploit-\ning their, seemingly ever improving, accuracy is a novel direction in theory. Algorithms with\npredictions have access to an oracle that provide a hint for solving the problem at hand, that\nis based on learning from some distribution. Bounds for handling an input object that are\nsensitive to prediction quality can improve substantially on worst-case optimal bounds. An\nalgorithm with prediction is called robust if the algorithm does not perform worse than the\nbest known algorithm that does not use predictions, even if the predictions contain errors. For\nalgorithm design and analysis, prediction oracles are assumed to have a bounded error measure\nor a bounded accuracy probability . Analysis that is sensitive to the bounded error assumption\nrequires a meaningful notion of distance between predicted and actual inputs. Depending on\nthe problem, some measures are better suited to establish upper bounds than others (see, e.g.,\nthe survey of Mitzenmacher and Vassilvitskii [MV21]). Moreover, in general, it is not possible\nto know the exact error apriori. In contrast, predictions with bounded accuracy probability do\nnot assume bounds on a particular error measure and instead assume that every prediction is\ncorrect with certain probability [GPSS22].\nIn online problems1, both bounded-error (e.g. [APT22]) and bounded-accuracy\n(e.g. [GPSS22]) predictions have been studied extensively. There are a wide range of prob-\nlems where predictions allow to improve quality over worst-case optimal competitive ratios,\nsuch as counting sketches [HIKV19, AIV19, EIN+21], bloom filters [KBC+18], caching/paging\n[Roh20, LV21, BCK+22, IKPP22, ABE+23], ski rental [PSK18, BMS20, ACE+21, SLLA23],\ncorrelation clustering [SAN+23], among many others.2The standard assumption is that the\nalgorithm is given access to the predictions for the whole sequence of operations (or requests)\nbefore the algorithm has to produce its first output, i.e, during preprocessing. Dependent on\nthe correctness of the information provided by the prediction, ideally algorithms with prediction\nshould provide a smooth transition between the online and offline problems.\nIn this paper, we initiate the study of complexity of dynamic data structures and algorithms\nwith predictions. Unlike in online algorithms, the main goal in the dynamic setting is to\nmaintain the solution efficiently with every update. To the best of our knowledge, investigating\nthe potential of algorithms with prediction in the dynamic setting has only just started with\nour present work, and the independent, concurrent work of van den Brand, Forster, Nazari, and\nPolak [vdBFNP23] and Liu and Srinivas [LS23]. Like in online algorithms, we aim for: 1) The\nalgorithm should be consistent , achieving the performance of an optimal offline algorithm when\nthe prediction quality is high. 2) The algorithm should be robust , matching the performance of\nan (online) dynamic algorithm regardless of prediction quality. 3) The algorithm’s performance\nshould degrade gracefully between the two extremes as prediction quality deteriorates. Given\nthe predictions, we can allow polynomial preprocessing time. When the actual updates arrive,\nthe dynamic data structure must process them fast provided the available information from\npreprocessing.\nLet us consider as an example the Online Matrix Vector (OMv) problem which has been\ninstrumental in developing lower bounds for dynamic data structures [HKNS15]. In this prob-\nlem, given a Boolean matrix Mof size n×n, and an online sequence of nvectors ⃗ v1, . . . ,⃗ v n,\none needs to report M⃗ vibefore seeing any ⃗ vtwith t > i. While the OMv conjecture states that\nthe total time needed to process these nvectors cannot be sub-cubic, if these vectors are given\napriori as part of predictions, then one could have preprocessed them in O(nω) time using fast\nmatrix multiplication where ω <2.373, and output the results in O(n) time per vector. This\nis often called the offline-online gap. Of course, it is unrealistic to assume that predictions are\n1We use online to denote problems where the input consists of a sequence of operations, which can modify\nthe input or ask a query about the input, and offline to denote that all input is given at once.\n2See for example, https://algorithms-with-predictions.github.io or the survey [MV21].\n1\n\ncompletely accurate. But this already showcases ample room for potential improvements in\ndynamic data structures due to algorithms with predictions.\nFor dynamic graph problems, a line of work initiated by [HKNS15] establishes conditional\nlower bounds on the time trade-offs between updates and queries for a large number of dynamic\nproblems, based on the OMv conjecture (see e.g., [Dah16, GHP17, BHG+21, HPS21, HPS22]).\nHowever, these reductions typically consist of update sequences that present pathological and\nrepetitive behaviour, e.g. repeatedly requesting certain updates, asking a query, and then\nreverting the updates again. Algorithms with predictions might have a tremendous potential\nfor improving the running time bounds on such sequences. Thus, in this work, the central\nquestion that we investigate is:\nCan predictions lead to provably faster dynamic graph algorithms?\n1.1 Contribution and Paper Outline\nWarm Up: OMv with Predictions Given the central importance of the OMv Problem,\nwe start by investigating if algorithms with predictions can bypass the bounds of the OMv\nConjecture (Section 2.2). Recall that in this problem an n×nBoolean matrix Mis given initially\nand can be preprocessed arbitrarily in polynomial time. This is called a round . Then a sequence\nofnBoolean vectors v1, v2, . . .arrives and the Boolean product Mvineeds to be output before\nseeing the next vector vi+1. Let us call the predicted vectors as (ˆ v1, . . . , ˆvn). A natural measure\nto quantify prediction errors could be the maximum ℓ1distance or Hamming distance between\nˆviandvifori= 1,2, . . . , n . In fact, we consider an even more general notion of error called\nExtended Hamming Distance which is always upper bounded by Hamming distance, and show a\nsmooth transition in complexity across the offline-online gap for OMv that uses predictions with\nbounded Extended Hamming distance (Theorem 2.2). Moreover, our lower bound (Theorem 2.3)\nshows that the algorithm is essentially optimal under the OMv conjecture.\nOur study then turns to analyzing conditions that allow or prevent obtaining similar pos-\nitive results for more general dynamic data structures and graph problems with appropriate\nprediction models.\nPrediction Models for Dynamic Problems and General Lower Bounds In Section 3,\nwe propose and analyze three quality measures for prediction accuracy that are suitable for\ndynamic problems. These are (1) ε-accurate predictions , where each predicted request matches\nthe true request with probability at least ε, (2) list-accurate predictions , where the prediction\nfor each time step is a list of possible requests, and (3) bounded-delay predictions , where the\ntrue requests are some (unknown) permutations of the predicted requests.\nTheε-accurate predictions [GPSS22] and bounded-delay predictions [PSK18, Roh20, LV21,\nAGS22, LM22] have already been studied in the online algorithms literature . The list-accurate\npredictions are similar to multi-prediction model [AGKP22, DIL+22, ACE+23] where the best\nprediction from a list needs to be selected at every step. It can also be seen as a generalization of\nε-accurate predictions in a sense by providing a list of possibilities out of which one is the correct\nupdate. For algorithms with ε-accurate predictions , where ε∈(0,1), we show that any lower\nbound on the time complexity from the non-prediction setting carries over, reduced by a factor\nof 1−ε(see Proposition 3.1). Then we give general reductions among the notions for a problem,\nshowing that lower bounds for bounded delay imply lower bounds for list-accurate predictions\n(Corollary 3.6), which imply lower bounds for ε-accurate predictions (Corollary 3.3). We believe\nthat our ‘Alternating Parallel Simulation’ technique to show such reductions (Lemma 3.5) is of\ninterest for analyzing further prediction models. In particular, this provides a natural hierarchy\nin the power of prediction models.\n2\n\nLocally Correctable Problems: Hardness for List-Accurate Predictions In Section 5,\nwe introduce a class of locally correctable dynamic problems (defined informally below), and\nshow that OuMv lower bounds for these problems continue to hold for any algorithm with list\naccurate predictions, unless the predictions are perfect (i.e. the list of each time step has size\n1). The OuMv problem is a slight generalization of the OMv problem where we are given two\nsets of nvectors ⃗ u1, ⃗ u2, . . . , ⃗ u nand⃗ v1,⃗ v2, . . . ,⃗ v nalong with the matrix Mofn×ndimensions.\nThe products u⊤\niMvineeds to be computed before seeing ⃗ uj,⃗ vjwith j=i+ 1, . . . , n . The\nOuMv conjecture excludes algorithms with total work that is subcubic in n, and follows from\nthe OMv conjecture (see Theorem 2.1).\nTheorem 1.1 (Informal, cf. Theorem 5.3) .Letε > 0be constant. Suppose Pis a locally\ncorrectable problem due to an OuMv reduction that uses u(n)many updates and q(n)many\nqueries. Then there is no algorithm solving Pwith 2-list accurate predictions that has update\ntime U(n)and query time Q(n)satisfying\nn\u0010\nu(n)U(n) +q(n)Q(n)\u0011\n= Ω( n3−ε),\nif the OuMv conjecture is true.\nOur lower bound follows from a reduction from OuMv such that the set of request sequences\nadmit efficiently computable 2-list accurate predictions. The basic idea is that for this class of\nproblems a generic “universal request sequence” can be efficiently created (without knowledge\nof the exact reduction sequence arising in the hardness reduction). Now any dynamic algorithm\n(without prediction) can efficiently construct this universal request sequence for itself in the\npreprocessing phase and then execute an algorithm with prediction using this universal request\nsequence as prediction. Thus, no efficient dynamic algorithm with predictions can exist unless\nthe OuMv conjecture is false.\nRoughly speaking, a locally correctable problem Psatisfies the following three properties:\n1) any OuMv instance can be simulated by choosing some subsequence of a universal request\nsequence, containing updates needed for answering the query as well as updates that will turn\nout not to be useful for answering the queries, called “junk” updates 2) any Pinstance can be\naugmented efficiently into an instance containing both useful as well as useless updates, and 3)\nthe answer to any query in the augmented instance can be corrected efficiently to answer the\ncorresponding query in the original instance. We can then construct the following reduction\nfrom an OuMv instance. Typically, an OuMv based reduction encodes the round’s query into\nthe problem instance using some subsequence of possible updates to modify the data of Ponly\nwhere necessary. Instead, our reductions perform the specified update where necessary and\notherwise insert a “junk” update. As a result, obtaining a 2-list accurate prediction is simple:\nThe list contains the update itself and an arbitrary “junk” update.\nLocally Reducible Problems: Hardness for Bounded Delay Predictions In Section 6,\nwe introduce a class of locally reducible dynamic problems, where proving lower bounds against\nalgorithms with bounded delay predictions is possible. As with list accurate predictions, our\nlower bounds rely on constructing an OuMv-based reduction such that a bounded delay pre-\ndiction for the resulting sequence can be constructed efficiently. Roughly speaking, a locally\nreducible problem satisfies two properties: 1) any OuMv instance can be simulated by choosing\nsome subsequence of a universal request sequence and 2) each update, if repeated often enough,\nsayordtimes, leaves the dynamic data structure unchanged.3Inspired by algebra, we call\nsuch operations cyclic . As before, a typical OuMv reduction proceeds by choosing a subset of\npossible updates in the problem instance in order to encode a round’s query. Our prediction\nthen simply predicts that in each query vector, every possible update will be necessary. Denote\n3Alternatively, we could model this by giving each update operation a corresponding “undo” operation.\n3\n\neach set of updates required to encode one query round a block.Pis (u, q)-locally reducible if\neach block contains u(n) updates and q(n) queries. Since each block consists of some subset of\nthe universal update set, an update will not occur before its predicted block, but it is possible\nan update occurs after its predicted block. To ensure that a request does not occur too long\nafter its expected block (say a request has not occurred in ordblocks but it was contained\nin the universal request sequence already ordtimes), we perform the update ordtimes, using\nproperty 2) to show that the underlying data structure does not change. It follows that an\nupdate cannot occur more than ordblocks after its predicted block. Since this sequence has\nsmall delay relative to the predicted sequence, the universal request sequence is a prediction\nwith small delay. Our lower bound then follows, as any dynamic algorithm can construct this\nprediction during the preprocessing phase.\nTheorem 1.2 (Informal, cf. Theorem 6.2 and Theorem 6.8) .Letε >0be constant. Suppose P\nis(u, q)-locally reducible from OuMv. Then there is no algorithm solving PwithO(u(n)+q(n))\ndelayed predictions with update time U(n)and query time Q(n)satisfying\nn\u0010\nu(n)U(n) +q(n)Q(n)\u0011\n=O(n3−ε),\nif the OuMv conjecture is true.\nUsing similar techniques, we show that the lower bound degrades gracefully as the delay\nerror of the prediction decreases (Theorem 6.9).\nExamples of Locally Correctable and Locally Reducible Problems In Section 7, we\nuse our frameworks to provide lower bounds against algorithms with predictions for the following\nproblems: Subgraph Connectivity [FI00, Dua10, CPR11, KKM13, AW14, HKNS15],\nReachability [AW14, HKNS15], Shortest Path [ES81, DHZ00, RZ11, RZ12], Dis-\ntance Spanners/Emulators [BHG+21], Maximum Matching [GP13, BGS18, Sol16,\nDah16, KPP16], Maximum Flow [Mad11, HKNS15, Dah16], Triangle Detection\n[HKNS15], Densest Subgraph [HKNS15], d-Failure Connectivity [DP10, KPP16],\nVertex Color Distance Oracle [HLWY11, Che12, LOP+15, EFW21], Weighted\nDiameter [FHW12, HKNS15], Strong Connectivity [AW14, HKNS15], Electrical\nFlows [GHP17], Erickson’s Maximum Value Problem [Pat10, HKNS15], Langer-\nman’s Zero Prefix Sum Problem [Pat10, HKNS15] . All turn out to be Locally Reducible\nProblems. Additionally, all are Locally Correctable Problems as well, with exception of Erick-\nson’s Maximum Value Problem.\nDynamic Algorithms with Predictions In Section 8, we give several algorithms with\nbounded delay predictions for the problems # s-△, Subgraph Connectivity, Transitive Closure,\nAll Pairs Shortest Path, and Erickson’s Maximum Value Problem. Some of them can even\nhandle outliers, which are updates that were not at all in the predicted set. See Table 1. These\nalgorithms with predictions are optimal (up to lower order terms) with respect to the prediction\nquality. That is, matching our conditional lower bounds for d-delayed in either the update or\nquery time. Moreover, none of these algorithms need to know the prediction quality (the error\nparameter dapriori).\nTo design our algorithms, we show that the difference between the state of the predicted data\nand the actual data scales with the delay error of the prediction. Furthermore, this difference can\nbe maintained efficiently. In the preprocessing phase, we compute the predicted data structures,\nextracting useful intermediate values we require from the predicted data structure. Now, given\nthe online request sequence up to some time step t, we show that by making small changes to the\npredicted data structure (on the order of the predictions delay) we can recover the result of the\nquery on the actual data structure from precomputed values on the predicted data structure.\n4\n\nProblem Upper Bounds Lower Bounds\nUpdate Query Reference Update Query Reference\n#s-△ d+k 1 Thm. 8.5 d1−εd2−εThm. 6.9\n#s-△ 1 (d+k)2Thm. 8.5 d1−εd2−εThm. 6.9\nSubgraph Connectivity 1 d2Thm. 8.14 d1−εd2−εThm. 6.9\nTransitive Closure 1 d2Thm. 8.18 d1−εd2−εThm. 6.9\nAll Pairs Shortest Path 1 d2Thm. 8.21 d1−εd2−εThm. 6.9\nErickson’s Problem d+k 1 Thm. 8.22 d1−εd2−εThm. 6.9\nErickson’s Problem 1 (d+k)2Thm. 8.22 d1−εd2−εThm. 6.9\nTable 1: List of algorithms with predictions with dbounded delay. Each algorithm has poly-\nnomial preprocessing time. A running time involving kstates that the algorithm can handle\npredictions that are ddelayed with koutliers. The lower bounds state that no algorithm with\npolynomial preprocessing time exists for any constant ε >0 that attains the stated update and\nquery times simultaneously , unless the OMv-conjecture fails.\nConcurrent Work Independent work of van den Brand, Forster, Nazari, and Polak\n[vdBFNP23] and Liu and Srinivas [LS23] jointly initiate the study of dynamic graph algorithms\nwith predictions, focusing on upper bounds. [vdBFNP23] gives (among many other things)\npartially dynamic algorithms with bounded delay predictions for transitive closure and all pairs\nshortest path (cf. Theorem 8.18 and 8.21), and show that these algorithms are optimal with\na lower bound giving the same result as Theorem 6.8 (cf. Appendix B). [LS23] considers the\nprediction model where a deletion time is predicted for every inserted edge, which is different\nfrom our proposed models. To the best of our knowledge, the above summarizes any overlapping\ncontribution with [vdBFNP23] and [LS23].\n2 Preliminaries and OMv with Predictions\nWe define dynamic data structures in general.\nDefinition 1 (Dynamic Data Structure) .LetPbe a dynamic problem. For any instance xof\nP, letX(x)be the set of possible updates onxand let Q(x)be the set of possible queries .\nWhen the instance is clear, we omit the subscripts and write X,Q. In the pre-processing step,\nthe algorithm receives as input an initial data structure x0. At each time step t, the algorithm\nreceives some request ρt∈ X ∪Q . When given a query ρt∈ Q, the algorithm must answer the\nquery correctly on the current structure xt, where xtis obtained by applying request sequence\n(ρ1, ρ2, . . . , ρ t−1)to the initial set x0. The query must be answered before the following request\nρt+1is revealed.\nIf the updates Xdo not allow element deletions or do not allow element insertions, the data\nstructure is called partially dynamic , otherwise it is fully dynamic .\nFor a given request sequence ρ, letρ[a,b]= (ρa, . . . , ρ b) denote the sub-sequence between the\na-th and b-th time step. Let ρ≤t=ρ[1,t]denote the prefix of the first trequests in ρand let ρ<t\ndenote ρ≤t−1.\n2.1 The Online Matrix Vector Problem\nOur hardness results are built on the OMv Conjecture of [HKNS15]. Recall that, in Boolean\nMatrix-Vector multiplication the arithmetic plus operation is replaced by logical-OR ∨and the\narithmetic multiplication operation is replaced by logical-AND ∧(see, e.g., [Wil07, Section 3]).\n5\n\nDefinition 2 (OMv and OuMv [HKNS15, Def. 2.6]) .An algorithm for the OMv (resp. OuMv)\nproblem is given parameter nas its input. Next, it is given a Boolean matrix Mof size n×n\nthat can be preprocessed in time p(n). This is followed by n-rounds of processing online input\nvectors.\nAnOMv algorithm is given an online sequence of nvectors ⃗ v1, . . . ,⃗ v n, one vector after the\nother, and the task is to report each result of Boolean Matrix-Vector multiplication Mˆvtbefore\nˆvt+1arrives.\nAnOuMv algorithm is given an online sequence of nvector pairs (⃗ u1,⃗ v1), . . . , (⃗ un,⃗ vn),\none pair after the other, and the task is to report each result of Boolean Vector-Matrix-Vector\nmultiplication ⃗ u⊤\ntM⃗ vtbefore (⃗ ut+1,⃗ vt+1)arrives.\nWe call the processing of each individual input vector a round . Clearly, every OMv round\ncan be solved in O(n2) time, yielding a trivial O(n3) algorithm. The OMv conjecture claims\nthat this is basically optimal (cf. [HKNS15, Conj. 1.1]):\nConjecture 2.1. For any constant ε >0, there is no algorithm with O(n3−ε)total time that\nsolves OMv with an error probability of at most 1/3.\nThis conjecture leads to the following result for the OuMv problem (cf. [HKNS15,\nThm. 2.7].). We simply call this the OuMv conjecture even though it is just a consequence\nof the OMv conjecture and nota different conjecture.\nTheorem 2.1. For any constant ε >0, the OMv conjecture implies that there is no algorithm\nwith preprocessing time polynomial in nand total time for all requests of O(n3−ε)that solves\nOuMv with an error probability of at most 1/3.\n2.2 Upper and Lower Bounds for OMv with Predictions\nWe begin by discussing how predictions affect the complexity of the Online Matrix-Vector\nProblem, leading to bounds that have a smooth transition across the offline-online gap in terms\nof the prediction quality. We also give conditional lower bounds, showing that this result is\noptimal.\nThe extended Hamming distance EH(s, t) of two bit-strings s, t∈ {0,1}nis defined as\nfollows. Let ℓ≥1 be the largest index with s1=s2=. . .=sℓandt1=t2=. . .=tℓ, then\nEH(s1. . . s n, t1. . . t n) := ( s1+t1mod 2) + EH(sℓ+1. . . s n, tℓ+1. . . t n).\nSince each block-difference is only counted once, the EH-distance is at most the Hamming\ndistance, where the latter is equal to the L1-distance on {0,1}n. The works of Lingas et\nal. [Lin02, GJL+21] introduced the extended Hamming distance to study Boolean matrix-vector\nmultiplication and showed that, after ˜O(n2) pre-processing, each OMv round can be solved in\n˜O(n+mst(M)) time. Here mst(M) =O(n2) is the weight of a (geometric) Minimum Spanning\nTree of the row-vectors in M. That is, each row-vector in Mis interpreted as one point in\nthen-dimensional Hamming space {0,1}nand the distance between any two row-vectors is the\nextended Hamming distance. Our prediction-based algorithm in this section is due to a small\nadaptation of the online O(logn)-approximate Minimum Spanning Tree heuristic in [GJL+21,\nSection 3.2]. Our algorithm shows however that bounded-error predictions allow to bypass query\nbounds that are sensitive to the, potentially quadratic, weight mst(M). That is predication-\nbased algorithms yield a smooth transition between the O(n3) online and O(nω) offline bound.\nTheorem 2.2 (OMv with Predictions) .LetM∈ {0,1}n×nand∆∈[0, n]. Given predictions\n(ˆv1, . . . , ˆvn)that have EH(ˆvi,⃗ vi)≤∆for each online input ⃗ vi∈ {0,1}n, each arithmetic product\nM⃗ vican be computed in time Q(n,∆) = O(n(1 + ∆)) , after preprocessing of Mand(ˆv1, . . .ˆvn)\ninO(nω)time.\nIn particular, each Boolean-result vector can be computed in O(n(1 + ∆)) time.\n6\n\nRecall that bounds that are sensitive to EH-distance are stronger than bounds sensitive\ntoL1. We remark that the following proof can be extended to rectangular matrices Mwith\nnon-binary entries (e.g. small integers or real values).\nProof. We first describe the preprocessing of the algorithm. Compute the matrix (ˆ y1, . . . , ˆyn) =\nM(ˆv1, . . . , ˆvn) using fast matrix multiplication. Compute a Prefix-Sum array Difor each row\niofM. That is, Diallows to obtain, for an block-range [ a, b] of column indices, the valueP\nk∈[a,b]Mi,k=Di(b)−Di(a−1) in O(1) time. Since each data structure Dican be computed\ninO(n) time, preprocessing takes O(n2+nω) time.\nTo compute the arithmetic result ⃗ y=M⃗ vfor the online input ⃗ v∈ {0,1}nof some OMv\nround, we determine in O(n) time the blocks b1, . . . , b ∆of index-ranges that contribute to\nthe extended Hamming distance EH(ˆv,⃗ v). Next, we initialize the result vector ⃗ ywith the\nprecomputed vector ˆ y. Then we iterate, over each row j∈[n] and each block bi= [b−\ni, b+\ni], and\nadd to the j-th entry in ⃗ ythe value ( ⃗ vb+\ni−ˆvb+\ni)P\nk∈biMi,k, where ( ⃗ vb+\ni−ˆvb+\ni)∈ {− 1,+1}since\n⃗ vand ˆvare bit-vectors. This requires time O(1 + ∆).\nNote that, after preprocessing, our algorithm with prediction requires time O(n(1 + ∆))\nper round. As we show next, this upper bound is also nearly the best possible that can be\nachieved for algorithms with predictions that have extended Hamming distance at most ∆,\nunless the OMv-conjecture is false. Specifically, we show a conditional lower bound stating that\nno algorithm can achieve query time ˜˜o(n(1+∆)) with polynomial improvement (see Definition 8).\nTheorem 2.3. Letε∈(0,1)be a constant and ∆∈[1, nε]. There is no algorithm with\npredictions of EH-distance at most ∆for the OMv problem with amortized time Q(n,∆) = ˜˜o(n∆)\nper round, if the OMv-conjecture is true.\n(See Appendix A for the proof.)\n3 Dynamic Prediction Models with General Lower Bounds\nThe first question to ask is how to define algorithms with predictions in the dynamic setting.\nIn this section, we generalize definitions of algorithms with predictions from the static setting\nand the corresponding error-measures to the dynamic setting and show that some definitions\nof predictions are so weak that almost the same lower bounds on the time complexity can be\nshown as in the setting without prediction.\n3.1 ε-Accurate Predictions\nWe begin with the simplest, very general formulation that lead to improved algorithms (see\ne.g. [GPSS22]). Informally, a predicted request sequence ˆ ρisε-accurate if each predicted request\nmatches the respective online request with probability at least ε.\nDefinition 3 (ε-Accurate Predictions) .Letε∈[0,1]. Consider a dynamic problem with update\nsetXand query set Q. LetDbe a distribution over sequences of Trequests, i.e. over (X ∪Q )T,\nandˆρ= (ˆρ1,ˆρ2, . . . , ˆρT)be a sequence of Tpredicted requests.\nThen ˆρis an ε-accurate prediction for D, if each ˆρtinˆρhasPr\nρ∼D[ρt= ˆρt]≥ε.\nThis is a natural model of prediction as accuracy is one of the most common metrics to\nevaluate the performance of a machine learning model. However, we show that it is too weak\nof a notion to design efficient dynamic algorithms with prediction. The following proposition\nshows that for any constant ε <1, a dynamic problem that is hard in the online setting remains\nhard even if an ε-accurate prediction is available in advance. For example, even if a dynamic\nalgorithm has a prediction that is correct for 99 .9% of future requests, known lower bounds for\nthe online problem still hold.\n7\n\nProposition 3.1 (Request Amplification) .Letε∈(0,1)be a constant and fa non-constant,\nnon-decreasing function in n. Suppose there is a dynamic problem Pwith query set Q ̸=∅\nsuch that any algorithm processing Trequests on instances of size nrequires worst-case time\nΩ(Tf(n)). Then there exists a distribution Dof request sequences from Psuch that any algo-\nrithm Awith ε-accurate predictions for Drequires amortized time Ω((1−ε)f(n))per request.\nProof. Letρbe a worst-case request sequence of length Tfor the algorithm and let q∗∈ Qbe\nan arbitrary fixed query. We give a randomized reduction that maps ρto an augmented request\nsequence ρ′as follows: Let a=⌈1/(1−ε)⌉. Consider the set of all request sequences of length\n(a+ 1)Tthat contain ρas a sub-sequence and aTmany copies of query q∗. The augmented\nrequest sequence ρ′is drawn uniformly at random from this set. Note that for every t∈[1, T],\nthet-th position consists of the query q∗with probabilityaT\n(a+1)T≥1/(1−ε)\n1/(1−ε)+1=1\n2−ε≥ε. Here\nthe first inequality is due tox\nx+1having a positive derivative and the second inequality is true\nas 0≥2ε−ε2−1 =−(ε−1)2.\nBy assumption, any algorithm correctly processing ρrequires Ω( Tf(n)) time. As the se-\nquence ρ′contains the sequence ρas a sub-sequence, and the added queries do not change\nthe underlying data of the problem instance, any algorithm correctly processing the request\nsequence ρ′requires time Ω( Tf(n)).\nNow, consider an algorithm Awith the prediction ˆ ρ= (q∗, . . . , q∗) of length ( a+ 1)T.\nBy construction, this is an ε-accurate prediction. Since any dynamic algorithm that has to\nanswer ρcan itself generate ˆ ρin time O(aT) and use it as an (arguably useless) prediction for\npreprocessing. Thus, the total time for such a dynamic algorithm is O(aT+Tp), where Tpis\nthe total time for the algorithm with an ε-accurate prediction to process ρ′, but this must be\nΩ(Tf(n)). It follows that the amortized time of A, even given an ε-accurate prediction, must\nbe at least Ω\u0010\nTf(n)−aT\naT\u0011\n= Ω((1 −ε)f(n)) per request.\nThus, for any constant ε <1, the amortized time per request is at least Ω ( f(n)). This\nshows that ε-accurate predictions are not particularly powerful for dynamic problems with\nknown, conditional or unconditional, lower bounds. This motivates the search for alternative\nstronger models of prediction under which it may be possible to harness the power of efficient\noffline algorithms.\nOne shortcoming of ε-accurate predictions are their generality. Regardless of the prediction,\nthe support of the distribution Dcan be every possible request sequence of length T, if we assign\nsmall enough probability to request sequences that do not match well with the prediction.\nWe thus investigate also more restrictive models that restrict the possible input sequences\nS ⊂(X ∪ Q )Tfor a given prediction.\n3.2 List Accurate Predictions\nNext we investigate a deterministic model for predictions that does not require a prediction to\nexactly specify the t-th request, but only to reveal ‘some information’ about it. If each request is\nrepresented by a bit-string of O(log|X ∪Q| ) bits, the following prediction model can be thought\nof as revealing a subset of these bits, i.e. the prediction ˆ ρtfor step tis a set, of size at most L,\nof possible requests such that the t-th request ρt∈ˆρt.\nDefinition 4 (L-List Accurate Predictions) .In a dynamic problem with update set Xand query\nsetQ, letS ⊆(X ∪ Q )Tbe a set of sequences with Trequests.\nA sequence of Tsets ˆρ= (ˆρ1,ˆρ2, . . . , ˆρT), where each ˆρt⊆ X ∪ Q , is called an L-list\naccurate prediction for S, if each set ˆρtcontains at most Lelements and we have for each\nsequence ρ∈ Sand all t∈[T]thatρt∈ˆρt.\nClearly, having an L-list accurate prediction with L= 1 is a perfect prediction. Also note\nthat there is always a |X ∪ Q| -list accurate prediction for all inputs, i.e. S= (X ∪ Q )T. In\n8\n\nthe OMv-problem for example, we have that the queries are from Q={0,1}nand having an\nL-list accurate prediction for preprocessing allows to solve each OMv round in O(n) time, after\nspending O(Lnω) time for preprocessing.\nLemma 3.2. Given an L-list accurate prediction ˆρ= (ˆρ1, . . . , ˆρT)forS, one can compute in\nO(P\nt|ˆρt|) =O(LT)time an1\nL-accurate prediction ˆρ′for the uniform distribution on S.\nProof. Given an L-list accurate prediction ˆ ρ, we can choose an element from the t-th set ˆ ρ′\nt∈ˆρt\nuniformly at random in time O(|ˆρt|), for t= 1, t= 2, and so forth. Thus, computing ˆ ρ′takes\nO(LT) time. Since ˆ ρis an L-list accurate prediction for S, we have that Pr[ ρt= ˆρ′\nt]≥1/Lfor\nallρ∈ S, t∈[T].\nThe lemma’s reduction, from L-list accurate to ε-accurate algorithms, shows that lower\nbounds against algorithms with L-list accurate predictions also yield lower bounds against\nalgorithms with ε-accurate predictions for a problem.\nCorollary 3.3. Suppose there is a dynamic problem Psuch that any algorithm with L-list\naccurate prediction requires worst-case total time Ω(Tf(n, L))for the request sequences in some\nS ⊆(X ∪Q )Tandf(n, L) = Ω( L), then any algorithm with ε-accurate prediction for the uniform\ndistribution on Srequires g(n, ε) = Ω( f(n, L))amortized time per request, for any ε≥1/L.\nThough this shows that L-list predictions are a stronger notion than ε-accurate predictions,\nwill show in Section 5.2 that for a wide range of problems, there called locally correctable\nproblems, strong lower bounds (similar to Proposition 3.1) hold: Any algorithm using L-list\naccurate predictions is subject to the same conditional lower bounds as a prediction-less online\nalgorithm, unless the list accurate predictions are perfect, i.e. L= 1.\nWe thus seek to investigate even more powerful prediction models in the following.\n3.3 Bounded Delay Predictions\nUnlike ε-accurate and L-list accurate predictions that aimed at predicting every individual time\nstep t∈[T], we further investigate predictions that know all Trequests in advance, though\nthe actual order of a request sequence may have various ‘small deviations’ from the predicted\nsequence of requests. That is, we will measure prediction accuracy by a notion of closeness for\npermutations. We consider a permutation π∈Perm( T) as a bijective map π: [T]→[T] on the\nintegers [ T], e.g. π(1) is the first element of the permutation.\nDefinition 5. Letd≥0andπ, σ∈Perm( T). We call πandσ d-close , denoted |π−σ|∞≤d,\nif\f\fπ−1(t)−σ−1(t)\f\f≤dfor all t∈[T]. Further, |π−σ|1=P\nt|π−1(t)−σ−1(t)|is called the\ntotal-distance ofπandσ.\nTo simplify exposition, we overload the notation of a permutation π∈Perm( T) to yield a\nreordering of a request sequence of length T.\nDefinition 6. For a request sequence ρ∈(X ∪ Q )Tand π∈Perm( T), let π(ρ) =\u0000\nρπ(1), . . . , ρ π(T)\u0001\nbe the request sequence obtained by reordering the requests inρaccording\ntoπ.\nNext we formalize what it means for predicted request sequence ˆ ρ∈(X ∪ Q )Tto be a\nbounded delay prediction for a set of input request sequences.\nDefinition 7 (Bounded Delay Predictions) .LetS ⊆(X ∪ Q )Tbe a set of request sequences of\nlength Tandˆρ= (ˆρ1,ˆρ2, . . . , ˆρT)a given sequence of Tpredicted requests.\nThen ˆρhas at most ddelay for S, called d-delayed forS, if for all ρ∈ S, there exists some\npermutation πwith π(ρ) = ˆρ, and πisd-close |π−id|∞≤dto the identity permutation id.\nFurther, ˆρhas at most dtotal-delay for S, called d-total-delayed forS, if for all ρ∈ S,\nthere exists some permutation πwithπ(ρ) = ˆρ, and πhas at most dtotal-distance |π−id|1≤d\nto the identity permutation id.\n9\n\nClearly, every d-delayed prediction for Shas at most dTtotal-delay for S. Next, we show\nthat bounded delay predictions are a stronger notion than list-accurate predictions, which were\na stronger notion than ε-accurate predictions (see Lemma 3.2).\nLemma 3.4. Given an integer d≥0and a request sequence ˆρ∈(X ∪ Q )T, one can compute\ninΘ((d+ 1)T)time a (2d+ 1)-list accurate prediction ˆρ′= (ˆρ′\n1, . . . , ˆρ′\nT)for all request sequence\nsetsS ⊆(X ∪ Q )Tfor which ˆρisd-delayed.\nProof. Since request ˆ ρtcan appear in a sequence ρ∈ S, with delay at most d, only in ρ[t−d,t+d],\nwe can compute a list-prediction ˆ ρ′from ˆ ρfor a given dby taking as list prediction for the t-th\nrequest the list ˆ ρ′\nt:={ˆρτ:τ∈[t−d, t+d]}.\nGiven d≥0 and ˆ ρ= (ˆρ1, . . . , ˆρT), this takes Θ(( d+ 1)T) time.\nThough the lemma requires that integer d≥0 is given as input, as opposed to the reduction\nin Lemma 3.2, we can still reduce, from bounded delay to L-list accurate algorithms, using an\n‘Alternating Parallel Simulation’ that allows to search for a 2-approximation of a minimum d\nvalue for an online input ρ, in a way that is efficient in the amortized sense.\nLemma 3.5 (Alternating Parallel Simulation) .Letρ,ˆρ∈(X ∪ Q )Tandd∗∈[0, T]be minimal\nsuch that prediction ˆρhas delay at most d∗for the request sequence ρ. Suppose there is an\nalgorithm A′that solves, given an L-list accurate prediction, request sequences of length Tof a\ndynamic problem Pin time O(Tf′(n, L))after at most P′(n, L)preprocessing time. If LT+\nP′(n, L) =O(Tf′(n, L)), then there is an algorithm Athat solves ρ, given the delay prediction\nˆρ, inO(f′(n,4d∗+ 1) log T)amortized time per request, after O(T)time for preprocessing of ˆρ.\nNote that the LT+P′(n, L) =O(Tf′(n, L)) condition on A′is very mild, i.e. its prepro-\ncessing of a list prediction of size O(LT) takes not more time than solving a request sequence\nfor which the prediction is L-list accurate.\nProof. We consider O(logT) values to find an approximation dwith d∗∈[d/2, d], i.e. d∈\n{0,1,2,4, . . .}. The algorithm Afor problem Pwill spawn copies of algorithm A′and selectively\npause/resume their computation. To spawn A′\ndfor a dvalue, we first compute a list prediction of\nsizeL= (2d+1) using Lemma 3.4. With this list-prediction, the copy A′\ndstarts its preprocessing\nand its computation for all online requests ( ρ1, ρ2, . . .) that arrived thus far. For each copy, we\ntrack the total time Tdspent thus far as Achooses to pause/resume individual copies. (Note\nthat a copy A′\nimay be paused/resumed several times while still in its preprocessing phase.)\nWe define for each copy A′\nditsprogress , which is the number of completed requests from the\nonline sequence ( ρ1, ρ2, . . .) divided by Td, i.e. total execution time (including preprocessing and\ncomputation) spend thus far.\nInitially, algorithm Aspawns only one copy of A′\n0ford= 0. To avoid that the runtime\nΘ(Td) of Lemma 3.4 dominates overall execution for d≫d∗, algorithm Adelays starting the\ncomputation of the list-prediction for the next larger value duntil at least one of the spawned\ncopies {A′\ni}has spent a total execution time Tiof at least Tdthus far. If this threshold is\nmet, then Apauses the parallel simulation of all spawned copies, executes the computation\nof Lemma 3.4, and resumes the parallel simulation of all spawned copies (including the new\ncopy) afterwards. Further, the parallel simulation of algorithm Apauses/resumes any one of\nthe spawned copies {A′\ni}, if total execution time is not within a constant factor of the total\nexecution time of the fastest progressing copy , i.e. the copy with maximal progress. Note that\nthe fastest progressing copy is not paused by the parallel simulation that Aperforms, and that\nall spawned copies (are allowed to) spent at least as much total execution time as the fastest\nprogressing copy. To answer the t-th online request ρtin case it is a query, Asimply takes the\nresult from the fastest progressing copy. This completes the description of algorithm A.\nFrom the O(logT) values, let d′be the value that minimizes the total execution time of A′\non the request sequence ( ρ1, . . . , ρ T). (Note that d′is the fastest progressing copy when at the\n10\n\nlast online request ρT.) We will show next that the total execution time of Ato finish all online\nrequests in ρis, amortized over the Trequests, at most\nO\u0012Tf′(n,2d′+ 1) + P′(n,2d′+ 1) + Td′\nTlogT\u0013\n=O(f′(n,4d∗+ 1) log T).\nClearly, the number of spawned copies is O(logT) at all times. Since none of the copies spends\nmore than a constant of the total execution time of the fastest progressing copy, the sum of the\ntotal execution times of all spawned copiesP\niTiis bounded within a O(logT)-factor of the\ntime of the copy that uses the value d′, which has Td′=O(Tf′(n,2d′+ 1) + P′(n,2d′+ 1)). It\nremains to argue for the runtime cost due to executing Lemma 3.4. Since the size of the input\nlist prediction is upper bounded by the total execution time for solving with this list prediction,\ni.e. (2 d′+ 1)T=O(Tf′(n,2d′+ 1)), algorithm Amust spawn the copy that has value d′.\nFurther, since any copy with a specific dvalue is only spawned if Td < max iTi, we have that\nthe cost Θ( dT) is negligible in the amortized sense. Finally, f′(n,2d′+ 1) = O(f′(n,4d∗+ 1)\nsince d∗∈[d′/2, d′].\nNote that the ‘alternating parallel simulation’ technique to show the reduction in the pre-\nvious lemma is quite general, though we only use it to reduce from algorithms with bounded\ndelay to algorithms with list-prediction (i.e. taking Lemma 3.4). The reduction in the previous\nlemma immediately yields the following general, lower bounds.\nCorollary 3.6. Suppose there is a dynamic problem Psuch that any algorithm with d-delayed\nprediction processes Trequests on instances of size nrequires time Ω(Tf(n, d)). Then any\nalgorithm with L-list predictions for Prequires amortized time ˜Ω(f(n,(d−1)/4))per request.\nClearly, there are sets of request sequences that do not admit delay predictions with small d.\nIn Section 6, we will however show that for many problems with OMv-based lower bounds, it\nis possible to construct sets of request sequences Sadmitting bounded predictions while being\nsimultaneously powerful enough to express an arbitrary OMv instance. Concretely, for the class\nof locally reducible dynamic problems (Definitions 14 and 15) we will show lower bounds for\nbounded delay predictions (even with no outliers) in Section 6.1.\n4 Extensions of the OMv Conjecture\nBefore discussing our lower bounds against general dynamic problems, we revisit generalizations\nand extensions of the OMv conjecture. The OMv and OuMv conjectures generalize to non-\nsquare dimensions (i.e. Definition 2.1 and 2.6 in [HKNS15]). To state this, we need to introduce\nthe˜˜o-notation for multivariate functions (cf. [HKNS15, Definition 1.2]).\nDefinition 8 (polynomially lower ˜˜o-notation) .Forf:N3→Nand any constants c1, c2, c3≥0,\nwe write f(n1, n2, n3) =˜˜o(nc1\n1nc2\n2nc3\n3)if and only if there exist constants ε, N, C > 0such that\nf(n1, n2, n3)≤C(nc1−ε\n1nc2\n2nc3\n3+nc1\n1nc2−ε\n2nc3\n3+nc1\n1nc2\n2nc3−ε\n3)for all n1, n2, n3> N.\nWe use the analogous definition for functions with one or two parameters.\nRecall that the standard ˜Oand ˜o-notation suppresses factors that are polylogarithmic in the\nproblem size.\nDefinition 9 (Rectangular γ-OMv and γ-OuMv) .Letγ >0be a fixed constant. An algorithm\nfor the γ-OMv (resp. γ-OuMv) problem is given parameters n2, n3as its input. Next, it is given\na Boolean matrix Mof size n1×n2that can be preprocessed, where n1:=⌊nγ\n2⌋. This is followed\nbyn3-rounds of processing online input vectors.\nAγ-OMv algorithm is given an online sequence of n3vectors ⃗ v1, . . . ,⃗ v n3, one vector after\nthe other, and the task is to report each result of Boolean Matrix-Vector multiplication M⃗ vt\nbefore ⃗ vt+1arrives.\n11\n\nAγ-OuMv algorithm is given an online sequence of n3vector pairs (⃗ u1,⃗ v1), . . . , (⃗ un3,⃗ vn3),\none pair after the other, and the task is to report each result of Boolean Vector-Matrix-Vector\nmultiplication (⃗ ut)⊤M⃗ vtbefore (⃗ ut+1,⃗ vt+1)arrives.\nTheγ-uMv problem is the special case of γ-OuMv with n3= 1.\nClearly, the OMv and OuMv problems are the special cases of γ-OMv and γ-OuMv with\nγ= 1 and n1=n2=n3=n. The OMv conjecture implies an analogous lower bound for the\nγ-OuMv problem (cf. Theorem 2.2 and 2.7 in [HKNS15]).\nTheorem 4.1 (Hardness of γ-OMv and γ-OuMv) .For any constant γ >0, the OMv conjecture\nimplies that there is no algorithm for γ-OMv with parameters n2, n3that has preprocessing time\nP(n2) =poly(n2), total running time for all requests of ˜˜o(n1n2n3), where n1=⌊nγ\n2⌋, and error\nprobability at most 1/3.\nFor any constant γ, the OuMv conjecture implies that there is no algorithm for γ-OuMv\nwith parameters n2, n3that has preprocessing time P(n2) =poly(n2), total running time for all\nrequests of ˜˜o(n1n2n3), and error probability at most 1/3.\nIt is possible to solve the OMv problem faster than Θ( n3). Green Larsen and\nWilliams [LW17] gave a non-combinatorial OMv algorithm that runs in O(n3/2Ω(√logn)) time.\nWilliams [Wil07] gave a combinatorial algorithm that, after O(n2+ε) preprocessing, solves any\nOMv round in O(n2/log2n) time. Chakraborty, Kamma and Larsen [CKL18] settled the cell\nprobe complexity, showing that any data structure storing r∈(n, n2) bits must have a query\ntime t, i.e. the number of reads from memory cells, with r·t= Ω( n3) and that this lower bound\nis tight, by giving an algorithm with r=t=˜O(n3/2) cell probes.\n4.1 Sparse OMv Conjecture\nWe show in this section that the difficulty of the OMv and OuMv problem “degrades gracefully”\nwith increased sparsity of query vectors. For any integer n, let [ n] denote the set {1,2, . . . , n }.\nThesupport of a vector ⃗ v∈Rnis the set of indices where ⃗ vis non-zero, i.e.\nsupp( ⃗ v) ={i∈[n] :⃗ v[i]̸= 0}, (1)\nand the restriction ⃗ v|Sof⃗ vto an index-subset Sis the vector from Rnthat has in the\nk-th component\n(⃗ v|S) [k] =(\n⃗ v[k]k∈S\n0 k̸∈S. (2)\nNext we define the problem variants that have sparse input vectors, with respect to fixed\nsets of indices.\nDefinition 10 (Sparse S-γ-OMv and S-γ-OuMv) .TheS-γ-OMv problem differs from the γ-\nOMv problem only by having an additional input S2⊆[n2]of size |S2| ≤nt\n2for some t∈(0,1],\nwhich is given during the preprocessing phase. In the online phase, each of the n3query vectors\n⃗ vimust fulfill support supp( ⃗ vi)⊆S2.\nTheS-γ-OuMv problem differs from the γ-OuMv problem only by having an additional input\nS1⊆[n1]andS2⊆[n2]of size |S1| ≤nt\n1and|S2| ≤nt\n2for some t∈(0,1], which are given\nduring the preprocessing phase. In the online phase, each of the n3pairs of query vectors (⃗ vi, ⃗ ui)\nhas support supp( ⃗ ui)⊆S1andsupp( ⃗ vi)⊆S2.\nClearly, each S-γ-OMv query round can be answered in O(nγ+t\n2) time and each S-γ-OuMv\nquery round can be answered in O(nγt+t\n2) time.\n12\n\nConjecture 4.1 (S-γ-OMv and S-γ-OuMv) .Letn2, n3, t, γ be parameters for the S-γ-OMv\nandS-γ-OuMv problem.\nThere is no ˜˜o(nγ+t\n2n3)-time algorithm that solves the S-γ-OMv problem with an error prob-\nability of at most 1/3after processing in time polynomial in n2.\nThere is no ˜˜o(nγt+t\n2n3)-time algorithm that solves the S-γ-OuMv problem with an error\nprobability of at most 1/3after processing in time polynomial in n2.\nFor the S-OMv and S-OuMv problems setting n1=n2=n3=n, the above conjecture\nis equivalent to saying there is no ˜˜o(n2+t) algorithm for the S-OMv problem and no ˜˜o(n1+2t)\nalgorithm for the S-OuMv problem.\nTheorem 4.2. Conjecture 4.1 for S-γ-OMv ( S-γ-OuMv) is true if Conjecture 2.1 for γ-OMv\n(γ-OuMv) is true.\nProof. Consider an γ-OMv instance with the parameters n1, n2, n3, matrix Mand vectors {⃗ vk}.\nRecall that n1=⌊nγ\n2⌋.\nFor contradiction, suppose there exists a t∈(0,1) where there is an algorithm solving S-γ-\nOMv instances in time ˜˜o(n1nt\n2n3). Partition the set [ n2] into ℓ=O(n1−t\n2) sets, where ℓ−1 have\nsize⌊nt\n2⌋and one has size at most ⌊nt\n2⌋. Label each set S1, S2, . . . , S ℓ. Then, for each query ⃗ vk,\nconstruct ℓrestricted vectors ⃗ vk,i:=⃗ vk|Siby taking only the non-zero entries of indices in Si.\nSince we can answer the γ-OMv query ⃗ vkby summing the ℓrestricted results, i.e.\nM⃗ vk=ℓX\ni=1M⃗ vk,i,\nwe can solve the entire γ-OMv instance in time O(n2n3) +ℓ·˜˜o(n1nt\n2n3) =˜˜o(n1n2n3), since\n(1) solving with the assumed algorithm takes ℓ·˜˜o(n1nt\n2n3) time and (2) computing those sums\ntakes O(n2) time in each round and O(n2n3) =˜˜o(n1n2n3) since γ >0. This contradicts the\nhardness of γ-OMv under the OMv conjecture.\nConsider now an γ-OuMv instance. Partition [ n1] into ℓ1=O(n1−t\n1) sets where ℓ1−1 have\nsize⌊nt\n1⌋and one has size at most ⌊nt\n1⌋. Partition [ n2] into ℓ2=O(n1−t\n2) sets where ℓ2−1 have\nsize⌊nt\n2⌋and one has size at most ⌊nt\n2⌋. Analogously, we construct ℓ1many vectors ⃗ uk,i:=⃗ uk|S′\niandℓ2many vectors ⃗ vk,j:=⃗ vk|Sjgiven the OuMv vectors ( ⃗ uk,⃗ vk). Then, we can compute,\n⃗ u⊤\nkM⃗ vk=X\ni,j⃗ u⊤\nk,iM⃗ vk,j,\nthus solving the γ-OuMv instance in time O(ℓ1ℓ2n3) +ℓ1ℓ2˜˜o(nt\n1nt\n2n3) =˜˜o(n1n2n3), contra-\ndicting the hardness of γ-OuMv under the OuMv conjecture.\n5 Locally Correctable Problems: Lower Bounds for List-\naccurate Predictions\nWe show in this section that certain problems, which we formally define in Definition 12, allow\nfor remarkably strong lower bounds, in contrast to our general reductions in Section 3.\n5.1 Preliminaries: Edge Updates in Dynamic Graphs\nWe will primarily focus on dynamic graphs with edge updates. Formally, a special case of\nDefinition 1 is the classic edge update model for problems in dynamic n-vertex graphs.\nDefinition 11 (Edge-Updates and Queries in Dynamic Graphs) .LetPbe a dynamic graph\nproblem. Let Vbe a set of nvertices. Let X(V) ={(u, v)∈V×Vs.t.u̸=v}denote the set\n13\n\nof possible edge flip updates. In an undirected graph, Xcontains all unordered pairs of vertices,\nwhile in a directed graph Xcontains all ordered pairs. Q(V)denotes the set of queries that\nare possible for P. When the underlying graph is clear, we omit Vand write X,Q. In the\npre-processing step, the algorithm receives as input an initial graph G0on vertices V. At each\ntime step t, the algorithm receives some request ρt∈ X ∪ Q . When given a query ρt∈ Q,\nthe algorithm must answer the query correctly on the current graph Gt, obtained by applying\nrequest sequence (ρ1, ρ2, . . . , ρ t−1)to the initial graph G0. The query must be answered before\nthe following request ρt+1is revealed.\nIn the Maximum Matching problem for example, the query set Qconsists of a single element\nq, resembling ‘What is the size of a maximum matching in the current graph?’ . In above’s edge\nupdate model of dynamic graphs, a sequence of Trequests (updates or queries) arrive in an\nonline manner, one request after the other. To study the potential of algorithms with predictions\nfor the offline-online gap of dynamic problems, we assume that the algorithm is given in advance,\ni.e. for pre-processing, some form of prediction for the Trequests in the online phase.\nFor a dynamic graph Gonnvertices and an update sequence ρ= (ρ1, ρ2, . . . , ρ T), we denote\nwith G0= (V, E 0) the initial graph and with Gt(ρ) = ( V, E t(ρ)) the graph after applying the\nt-th request of ρ, i.e. Et(ρ) is the edge set after applying all updates in the first trequests to\nE0. When the request sequence is clear, we omit ρand write Gt= (V, E t).\nWe also use the notion of an edge flip : An edge flip of edge einserts eif it is currently not\nin the graph and removes it otherwise.\n5.2 An OuMv Reduction for the #s-△Problem\nWe now motivate our definition of locally correctable problems by the example of a simple,\nconditional lower bound construction for the # s-△problem.\nIn the # s-△problem, each query asks to report the number of triangles in a dynamic n\nvertex graph that contain a fixed vertex s(cf. [HKNS15]). As a warm up, we give a lower\nbound in for the online setting (without predictions).\nTheorem 5.1. There is no algorithm solving the #s-△problem in dynamic nvertex graphs\nwith update time U(n), query time Q(n), and pre-processing time P(n) =poly(n), satisfying\nn2U(n) +nQ(n) =˜˜o(n3),\nif the OuMv conjecture (Conjecture 2.1) is true.\nProof. We design a sequence of # s-△updates such that any algorithm correctly answering all\nqueries solves the OuMv problem.\ns\nu1\nu2\nu3\nu4\nv1\nv2\nv3\nv4\n⃗ v=<1,0,0,1>\n⃗ u=<1,0,1,0>\nM=\n1 1 0 0\n1 0 1 0\n0 1 0 1\n0 1 1 0\nFigure 1: A small example of the OuMv lower bound construction for the # s-△problem. The\ndiagram shows that a 1 in the matrix or in vectors corresponds to an edge existing in the graph.\n14\n\nSetup Phase. Consider an 1-OuMv instance with parameters n1, n2, n3=n. We construct\na graph G0on 2n+ 1 vertices {u1, . . . , u n} ∪ {v1, . . . , v n} ∪ {s}.G0contains no edges, beside\nedges of form {ui, vj} ∈Eif and only if M[i][j] = 1.\nDynamic Phase. In each round k, we are given vectors ⃗ uk,⃗ vk. We use at most 2 nupdates\nto ensure that {s, ui} ∈Eif and only if ⃗ uk[i] = 1 and {s, vj} ∈Eif and only if ⃗ vk[j] = 1. For\nany edge {u, v}, letχ(u, v) be the indicator for whether {u, v} ∈E. To answer the query of the\nOuMv round, we observe that\n⃗ u⊤\nkM⃗ vk=X\ni,j∈[n]⃗ uk[i]·M[i][j]·⃗ vk[j] =X\ni,j∈[n]χ(ui, s)·χ(ui, wj)·χ(wj, s) = # s-△.\nThus, the number of triangles containing sis #s-△>0 if and only if the OuMv round must\nreport the 1-bit. Overall, this takes nqueries and O(n2) updates to answer the OuMv rounds.\nThus, we cannot have for the # s-△problem that\nn2U(n) +nQ(n) =˜˜o(n3),\nif Conjecture 2.1 is true.\nNext, we show a lower bound against algorithms with list-accurate predictions. Unless the\npredictions are perfect, i.e. 20-list accurate, any algorithm solving the # s-△problem is still\nsubject to the same OuMv-based lower bound of Theorem 5.1.\nTheorem 5.2. Suppose there is an algorithm Athat, given 21-list accurate predictions, solves\nthe#s-△problem in polynomial preprocessing time P(n), update time U(n), and query time\nQ(n). Then, U(n)andQ(n)cannot satisfy\nn2U(n) +nQ(n) =˜˜o(n3),\nif the OuMv conjecture (Conjecture 2.1) is true.\nProof. We will use the structure of the request sequences from the previous proof to show that\nthere is onegeneric 2-list accurate prediction that is suitable for all OuMv instances.\nConsider an OuMv instance of size nwith matrix Mand query vectors {(⃗ uk,⃗ vk)}. We will\nreduce to a # s-△instance with 2 n+ 2 vertices {u1, . . . , u n} ∪ { v1, . . . , v n} ∪ { s, t}. Unlike in\nthe last proof, there are twospecial vertices, sandt, in this graph.\nSetup Phase. We choose the initial graph to only contain edges that encode the matrix M,\nthat is edge {ui, vi}is present if and only if M[i][j] = 1.\nDynamic Phase. For the k-th OuMv query ( ⃗ uk,⃗ vk), we proceed as follows. For each index\ni∈[n], we ensure that {s, ui}is an edge if and only if ⃗ uk[i] = 1, and that {s, vi}is an edge if\nand only if ⃗ vk[i] = 1. To do so, we perform exactly nflips to satisfy the condition for ⃗ u, followed\nby exactly nflips to satisfy the condition for ⃗ v. Whenever the condition is already satisfied, we\nflip{t, ui}instead of {s, ui}and{t, vi}instead of {s, vi}, so that the flip has no influence on\nthe number of s-△’s in graph G. For the queries we again have that,\n⃗ u⊤\nkM⃗ vk=X\ni,j∈[n]⃗ uk[i]·M[i][j]·⃗ vk[j] =X\ni,j∈[n]χ(ui, s)·χ(ui, vj)·χ(wj, s) = # s-△,\nwhere χ(u, v) is the indicator variable for the existence of edge {u, v}. The # s-△queries again\nexactly answer the OuMv rounds, proving the correctness of our reduction.\n15\n\nFor each OuMv round, this requires O(n) updates. Thus, requiring O(n2) edge flips over\nallnrounds. By the OuMv conjecture, any dynamic algorithm, with polynomial preprocessing\ntime, correctly answering all queries on this # s-△request sequence cannot satisfy\nn2U(n) +nQ(n) =˜˜o(n3),\nas desired.\nTo show the desired lower bound, we show that there is one, generic 2-list accurate prediction\nthat is suitable for all online OuMv request sequences: For each OuMv round k∈[n], we use the\nprediction {{s, ui},{t, ui}}for each i∈[n], followed by {(s, vi),(t, vi)}for each i∈[n], followed\nby one query. By our above discussion, this prediction is a 2-list accurate prediction, regardless\nof the OuMv request sequence, and can be constructed in O(n2) time during preprocessing.\nTherefore, no algorithm solving the # s-△problem with 2-list accurate predictions can have\nupdate time U(n) and query time Q(n) that satisfy\nn2U(n) +nQ(n) =˜˜o(n3),\nif the OuMv conjecture (Conjecture 2.1) is true.\n5.3 Locally Correctable Dynamic Problems\nNext, we formalize this approach of OuMv lower bounds for algorithms with list accurate\npredictions in our definition of locally correctable problems.\nIn the previous proof, we observed that every pair of query vectors can be simulated by tak-\ning a subsequence of a universal update request sequence ( s, u1), . . . , (s, un),(s, v1), . . . , (s, vn),\nfollowed by one query, where we only flip the edges necessary to ensure the edges ( s, ui) correctly\nencode ⃗ uand the edges ( s, vi) correctly encode ⃗ v. However, in the general reduction (Theo-\nrem 5.1), the actual subsequence depends heavily on the specific instance. By augmenting the\ngraph with one dummy vertex tthat remains non-adjacent to s, we restricted the number of\nedge flips needed at any one time step, without affecting the query result, to a list of twoupdate\nrequests. That is flipping ( t, ui) or ( s, ui) for the bits in ⃗ uand flipping ( t, vi) or ( s, vi) for the\nbits in ⃗ v. In our # s-△example, the query computation in the augmented instance immediately\nyields the correct answer for the non-augmented instance, without further computations needed\nforcorrecting the query results.\nNext, we formally define the locally correctable problems and then prove the generalization\nof the technique in our reduction below.\nDefinition 12 (Locally Correctable Dynamic Problem) .LetPbe a dynamic problem.\nSuppose there is no algorithm for Pwith update time U(n)and query time Q(n)satisfying\nn3\u0010\nu(n1, n2)U(n) +q(n1, n2)Q(n)\u0011\n=˜˜o(n1n2n3)\nif the OuMv conjecture is true, where n1, n2, n3are integers with n1=⌊nγ\n2⌋for some constant\nγ > 0, functions u, q:N×N→N, and n=n(n1, n2)is the size of the Pinstance in the\nreduction.\nThen, Pislocally correctable if there exists a universal sequence ¯ρ= ¯ρ(n1, n2)of re-\nquests from X ∪Q , an augmentation function ffor pre-processing, and a correction function g,\nsatisfying:\n1. The sequence ¯ρcan be partitioned into n3subsequences ¯ρ=B1◦. . .◦Bn3, where block Bk\ncontains u(n1, n2)updates and q(n1, n2)queries.\n2. For any γ-OuMv instance with n1×n2matrix Mand query vector pairs {(⃗ uk,⃗ vk)}n3\nk=1the\nreduction constructs initial data D0of problem Pand a request sequence ρsatisfying:\n16\n\n(a)ρis the concatenation of request sequences B′\n1◦B′\n2◦. . .◦B′\nn3, where each B′\nkis a\nsubsequence of Bk.\n(b) For each γ-OuMv request (⃗ uk,⃗ vk), the result bit ⃗ u⊤\nkM⃗ vkcan be computed in\nO(q(n1, n2))time based on the answers given to the queries in B′\n1◦B′\n2◦. . .◦B′\nk.\n3.Augmentability: f(D0)is an instance of size O(n)andX(D0)⊂ X(f(D0)).\n4.Correctability: There is a non-empty subset X∗⊆ X(f(D0))\\ X(D0)such that, for\nall time steps tand queries q, the function gyields g(q(Y), Y) =q(Dt), where Dtis the\ndata structure after request sequence ρ≤t, and Yis the result of request sequence ρ≤twith\narbitrary requests from X∗inserted, applied to f(D0).\n5.fis computable in polynomial time and gis computable in ˜O(1)time.\nThe request sequence ¯ ρis universal in the sense that it does not depend on any specific γ-\nOuMv instance. However, the sequence ¯ ρdepends on the dynamic problem Pand the reduction\nfrom γ-OuMv to P. Specifically, ¯ ρconsists of all updates that might be necessary in the\nreduction from γ-OuMv to encode a vector update ( ⃗ uk,⃗ vk) into a Pinstance.\nThe following theorem shows that all problems P, that have a reduction from OuMv satis-\nfying Definition 12, 2-list accurate predictions offer no improvement over a dynamic algorithm\nwith no predictions.\nTheorem 5.3. Suppose Pis a dynamic locally correctable problem. Then there is no algorithm\nsolving Pwith2-list accurate predictions with update time U(n)and query time Q(n)satisfying\nn3\u0010\nu(n1, n2)U(n) +q(n1, n2)Q(n)\u0011\n=˜˜o(n1n2n3)\nif the OuMv conjecture is true.\nProof. We begin by constructing a reduction from γ-OuMv to Pthat admits efficiently com-\nputable 2-list accurate predictions. If an efficient algorithm with predictions exists, then we can\ndesign a dynamic algorithm without predictions as follows. First, we compute the efficiently\ncomputable predictions, and then run the algorithm with predictions as a sub-routine, violating\nthe lower bound based on the OuMv conjecture.\nConsider a γ-OuMv instance with n1×n2matrix Mand vector updates {(⃗ uk,⃗ vk)}n3\nk=1. By\nassumption, there is an initial data structure D0and request sequence ρ′=B′\n1◦B′\n2◦. . .◦B′\nn3\nsuch that each γ-OuMv request ( ⃗ uk,⃗ vk) can be computed in O(q(n1, n2)) time given the answers\nto the queries in B′\n1◦B′\n2◦. . .◦B′\nk. We construct a new request sequence ρ∗=B∗\n1◦B∗\n2◦. . .◦B∗\nn3on\nthe augmented initial data structure f(D0). Fix an arbitrary update x∗∈ X∗. For each update\ninBk,B∗\nkwill contain ρtifρt∈B′\nkandx∗otherwise. Consider a query at time step t. Let Yt\nbe the current state of the data structure, that is f(D0) with request sequence ρ∗\n≤tapplied. By\nassumption, g(q(Yt)) = q(Dt′) where Dt′isD0with ρ′\n≤t′applied and ρ′\n≤t′⊂ρ′is the longest\nprefix such that ρ′\n≤t′⊆ρ∗\n≤t. Then, given the query computations after B∗\n1◦B∗\n2◦. . .◦B∗\nk, we\ncan compute ⃗ u⊤\nkM⃗ vkin˜O(q(n1, n2)) time as gis efficiently computable and B′\n1◦B′\n2◦. . .◦B′\nk⊆\nB∗\n1◦B∗\n2◦. . .◦B∗\nkcontains all the queries required to compute ⃗ uT\nkM⃗ vk.\nNext, we claim that there is an efficient prediction for the above reduction. That is, consider\nthe prediction ˆ ρ={(ρt, x∗)}t∈[T]. Since ρ∗contains either ρtorx∗at the t-th position, this is\na 2-list accurate prediction. Furthermore ˆ ρis efficiently computable.\nTherefore, suppose there is an efficient algorithm with 2-list accurate predictions. Then,\ngiven a γ-OuMv instance, we compute f(D0) and ˆ ρin the preprocessing phase in polynomial\ntime, providing this as the initial input to the algorithm with predictions. Then, we compute\neach vector update using the appropriate query computations from ρ∗, therefore obtaining a\ndynamic algorithm for the γ-OuMv instance. Thus, the update and query times must not satisfy\n17\n\nn3\u0010\nu(n1, n2)U(n) +q(n1, n2)Q(n)\u0011\n=˜˜o(n1n2n3),\nif the OuMv conjecture is true.\n6 Locally Reducible Problems: Lower Bounds for Delay Pre-\ndictions\nIn this section, we will provide a framework for proving trade-off conditional lower bounds\nagainst algorithms with bounded delay predictions given a conditional lower bound against\nonline algorithms (without predictions). To do so, we introduce the notion of locally reducible\ndynamic problems (Definitions 14 and 15) and show that for this large class of problems, the\nOuMv-based lower bounds carry through to the setting of algorithms with d-delayed predictions.\nFor each problem, we show that there is a delay threshold (roughly the number of updates\nand queries used to process one round in the OuMv problem) below which predictions offer\nno benefit over a generic online algorithm. The basic idea is that, when allowed sufficient\ndelay, every OuMv request sequence can be generated by simply reordering one generic request\nsequence. Thus, even with prediction, a locally reducible dynamic problem is still powerful\nenough to solve any γ-OuMv instance, and is, thus, still difficult to compute. This implies\nthat a prediction algorithm does not only need to know what operations will happen, but also\nwhen the operations will happen. We also show that the lower bound degrades gracefully as\nthe prediction quality surpasses this threshold.\n6.1 Locally Reducible Dynamic Problems\nWe now define the class of locally reducible dynamic problems. Then we show in Theorems 6.2\nand 6.8 that for any locally reducible problem, OuMv-based lower bounds extend to dynamic\nalgorithms with bounded delay predictions. For a multi-set S, let set( S) denote the set of\nelements that occur in Sat least once. For two request sequences ρ1, ρ2, letρ1◦ρ2denote the\nconcatenation of the two request sequences. We define also the notion of a cyclic update.\nDefinition 13. Consider a dynamic problem Pwith updates Xand queries Q. An update\nx∈ X hascyclic order ord(x)if for any request sequence ρ, inserting or removing exactly\nord(x)identical copies of xinto the sequence ρbetween indices i0, i1does not change the results\nof any queries that are not part of ρbetween the indices i0andi1. We say xiscyclic if\n1≤ord(x) =O(1).\nFor example, consider an edge insertion and an edge deletion to not be two separate oper-\nations, but instead consider it one operation called (edge) flipping (which inserts the edge if it\nexists and deletes it if it does not exist). Now note that edge flipping has cyclic order 2. In\norder to place strong bounds on the positions of each individual update and query, we will insert\nredundant updates so that the positions of requests are more predictable. For example, if at the\ncurrent time, an edge was predicted to be flipped 2 more times than it has already been flipped,\nwe can flip the edge twice without changing the dynamic graph to correct this prediction error.\nHaving updates of small cyclic order therefore allow us to insert redundant updates without\nsignificantly blowing up the size of the problem instance and therefore weakening our lower\nbounds.\nIn a bit more detail, consider a typical OMv-based lower bound. Let Pbe a dynamic problem\nfor which there is an OuMv lower bound. First, for some fixed γ, there is a generic reduction\nfrom any arbitrary γ-OuMv instance with arbitrary parameters n1, n2, n3to an instance of Pof\nsizen=n(n1, n2). In this reduction, each of the n3rounds of the γ-OuMv instance is simulated\nseparately, i.e. for each round there are u(n1, n2) updates and q(n1, n2) queries for the problem\n18\n\ninstance P. To simulate a single OuMv round, we choose some subset of necessary updates\nfrom a universal request block to correctly encode the γ-OuMv instance into the Pinstance. To\nconstruct our prediction, we predict for each block that the whole universal request set occurs.\nSince the request sequence is a subset of the universal set, an update cannot occur in a block\nbefore it is predicted to. However, this prediction could very well predict a request to occur\nin a block long after it is predicted to. For example, in the reduction of Theorem 5.1 if there\nis an index i0such that ⃗ uk[i0] = 0 for all k, then the edge ( s, ui0) will never be flipped in the\noriginal request sequence. To solve this, we add (after the query for the vector update ( ⃗ uk,⃗ vk)\nhas arrived and before the next vector update arrives) ord( x) copies of update xwhenever\nan update xhas occurred in fewer than k−ord(x) request blocks when simulating the vector\nupdate ( ⃗ uk,⃗ vk). These redundant requests do not change the result of any query computation\nand ensure that a request cannot occur more than ord( x) blocks later than it is predicted to.\nWe now give separate definitions for fully dynamic and partially dynamic problems, begin-\nning with the fully dynamic setting.\nDefinition 14 (Fully Dynamic Locally Reducible) .LetPbe a fully dynamic problem with\nupdate set Xand query set Q. Suppose there is no algorithm for Pwith update time U(n)and\nquery time Q(n)satisfying\nn3\u0010\nu(n1, n2)U(n) +q(n1, n2)Q(n)\u0011\n=˜˜o(n1n2n3),\nif the OuMv conjecture is true, where n1, n2, n3are integers satisfying n1=⌊nγ\n2⌋for some\nconstant γ >0, functions u, q:N×N→N, and n=n(n1, n2)is the size of the Pinstance in\nthe reduction.\nPis(u, q)-locally reducible from γ-OuMv if there exists a universal request sequence\n¯ρ= ¯ρ(n1, n2)of requests satisfying the following properties:\n1. The universal request sequence ¯ρconsists of n3identical copies of the sequence B=\nB(n1, n2), indexed B1, B2, . . . , B n3. The request sequence Bcontains u(n1, n2)updates\nandq(n1, n2)queries.\n2. For any γ-OuMv instance with n1×n2matrix Mand vector updates {(⃗ uk,⃗ vk)}n3\nk=1the\nreduction constructs an initial data structure D0and request sequence ρsatisfying:\n(a)ρis the concatenation of request sequences B′\n1◦B′\n2◦. . .◦B′\nn3where B′\nk=πk(B′′\nk)is\nan ordering of B′′\nkfor some B′′\nk⊆B.\n(b) Each γ-OuMv request (⃗ uk,⃗ vk),⃗ u⊤\nkM⃗ vkcan be computed in O(q(n1, n2))time based\non the answers given to the queries in B′\n1◦B′\n2◦. . .◦B′\nk.\n3. Every update x∈¯ρin the universal sequence ¯ρis cyclic.\nRemark 6.1. Our proof assumes that each update has some small finite cyclic order. For\nexample, the edge flip operation has order 2. Alternatively, we can view an update in a fully\ndynamic algorithm to have an inverse operation. For example, removal and insertion of the\nsame edge are inverse operations. The proof of Theorem 6.2 for locally reducible fully dynamic\nproblems follows in this case as well. Whenever we insert two edge flips in the proof of Theorem\n6.2, we can insert an operation and its inverse operation counterpart. Both of these sequences\nof two updates have the desired effect of leaving the underlying data structure unmodified.\nLet us compare the Definitions 12 and 14. In both cases, the universal request sequence ¯ ρ\ndepends only on the reduction from γ-OuMv to the dynamic problem P. It is universal in the\nsense that ¯ ρisindependent of any specific γ-OuMv instance.\nIn Definition 14, each block of the request sequence ρdoes not have to respect the order of\n¯ρ. Each block of the request sequence in the reduction to a locally correctable problem must be\n19\n\na subsequence of the corresponding block in the universal request sequence. In the reduction to\na locally reducible problem, we may instead arbitrarily permute a subsequence of a block of the\nuniversal request sequence. To see why this is the case, observe that a list accurate prediction\nimposes the constraint that a certain update can occur only at O(1) time steps in each block\n(since only 2 updates can occur at a given time step). Instead, bounded delay predictions allow\nthe update to be placed at any point within a range of the predicted update. We are therefore\nfree to order the subset of block of requests without being forced to adhere to the original order\nin the universal sequence.\nFurthermore, instead of requiring that any instance can be efficiently augmented to a larger\ninstance with an efficient “correction” function to the query computations (Conditions 3, 4, and\n5), we now require that each update is cyclic (Condition 3). We now give the definition for\npartially dynamic locally reducible problems.\nDefinition 15 (Partially Dynamic Locally Reducible) .Letγ >0be a constant and n1, n2, n3\nbe integers satisfying n1=⌊nγ\n2⌋. Let u, q:N×N→N. Let Pbe an incremental (resp.\ndecremental) dynamic problem with update set Xand query set Q.\nSuppose there is no algorithm for Pwith update time U(n)and query time Q(n)satisfying,\nn3\u0010\nu(n1, n2)U(n) +q(n1, n2)Q(n)\u0011\n=˜˜o(n1n2n3)\nif the OuMv conjecture is true, where n=n(n1, n2)is the size of the Pinstance in the reduction.\nPis(u, q)-locally reducible from γ-OuMv if there exists a universal request sequence ¯ρ=\n¯ρ(n1, n2)of requests (where only queries can occur more than once) satisfying the following\nproperties:\n1. The universal request sequence ¯ρconsists of n3subsequences {Bk}n3\nk=1where request block\nBkcontains u(n1, n2)updates and q(n1, n2)queries.\n2. For any γ-OuMv instance with n1×n2matrix Mand vector updates {(⃗ uk,⃗ vk)}n3\nk=1the\nreduction constructs an initial data structure D0and request sequence ρsatisfying:\n(a)ρis the concatenation of request sequences B′\n1◦B′\n2◦. . .◦B′\nn3where B′\nk=πk(Bk)is\nan ordering of Bk.\n(b) Each γ-OuMv request (⃗ uk,⃗ vk),⃗ u⊤\nkM⃗ vkcan be computed in O(q(n1, n2))time based\non the answers given to the queries in B′\n1◦B′\n2◦. . .◦B′\nk.\nWe are now ready to present our main lower bound result.\nTheorem 6.2. Letγ >0be a constant and u, qbe functions. Suppose Pis a fully dynamic\nproblem that is (u, q)-locally reducible from γ-OuMv and let C= max x∈Xord(x).\nThen there is no algorithm solving Pwith (1 +C)(u(n1, n2) +q(n1, n2))delayed predictions\nwith update time U(n)and query time Q(n)satisfying\nn3\u0010\nu(n1, n2)U(n) +q(n1, n2)Q(n)\u0011\n=˜˜o(n1n2n3)\nif the OuMv conjecture is true.\nWe begin by defining some useful notation, denoting the position in which the k-th instance\nof a request occurs.\nDefinition 16. LetXdenote the set of updates and Qthe set of queries. Let ρ∈(X ∪ Q )T\nbe a sequence of requests. For a given request a∈ X ∪ Q andk∈N, define pos(a, k, ρ )to be\nthe position in ρof the k-th occurrence of a. Ifadoes not occur ktimes in ρ,pos(a, k, ρ ) =⊥.\nWhen the underlying request sequence is clear, we omit the sequence and write pos(a, k).\n20\n\nWe now prove Theorem 6.2.\nProof. The key ingredient for the lower bound will be a reduction from a γ-OuMv instance\nto the dynamic problem P. However, we will require the reduction to construct the online\nrequest sequence in such a way that we can efficiently compute a very simple prediction with\n(1 +C)(u(n1, n2) +q(n1, n2)) delay. Then, if an efficient algorithm Awith (1 + C)(u(n1, n2) +\nq(n1, n2)) bounded delay predictions exists, we can solve the γ-OuMv problem by constructing\nthe prediction and running the algorithm Aas a sub-routine, violating the γ-OuMv lower bound.\nPreliminaries. Since our reduction will be constructed by modifying an existing reduction, we\nbegin by describing the existing reduction given by Condition 2. Consider a γ-OuMv instance\n(M,U) consisting of a n1×n2matrix Mand a length- n3sequence of vector updates U=\n{(⃗ uk,⃗ vk)}n3\nk=1. We use ρ′(U) =B′\n1◦B′\n2◦. . .◦B′\nn3to denote the request sequence given by the\nreduction such that each γ-OuMv request ( ⃗ uk,⃗ vk) can be computed in O(q(n1, n2)) time given\nthe answers to the queries in B′\n1◦B′\n2◦. . .◦B′\nk.\nWe now describe the universal request sequence ¯ ρ. In the given γ-OuMv reduction, each\nvector update ( ⃗ uk,⃗ vk) is encoded into the data structure using some set of updates. For any\nrequest x∈ X ∪ Q , define M(x) to be the maximum number of times an update x∈ X occurs\nin a single block B′\nk⊂ρ′(U) over all kand all possible vector update sequences U(not just the\nworst-case one). The set Bthen contains M(x) copies of xfor all requests x∈ X ∪Q . Note that\nifM(x) = 0, Bdoes not contain any copy of M(x). Additionally, we give an arbitrary, fixed\norder to the requests in B, so that Bis a sequence. The universal request sequence ¯ ρconsists of\nn3copies of the sequence B. Define the predicted requested sequence ˆ ρ= ¯ρ=B1◦B2◦. . .◦Bn3\nto be the universal request sequence.\nNote that for a specific γ-OuMv instance ( M,U), we may not need every request in Bkto\nencode the vector update ( ⃗ uk,⃗ vk) (with k∈[n3]) given the state of the data structure after the\nblock computing the previous vector update ( ⃗ uk−1,⃗ vk−1). However, by our definition of M(x),\nit is possible to encode the vector update using some subset B′\nk⊆Bk. This is precisely the\nblock of requests in the request sequence ρ′(U).\nNotation. During the proof, we will focus on three request sequences. We use ρ′to denote\nthe request sequence that is generated by the reduction of a worst-case γ-OuMv instance to P.\nWe denote by ˆ ρthe predicted request sequence constructed from the universal sequence ¯ ρas\ndiscussed above. Note that it can be constructed without knowledge of ρ′. In this proof we will\nmodify ρ′into a request sequence ρ∗encoding the same instance, with the additional property\nthatπ(ρ∗) = ˆρfor some permutation |π−id| ≤d. Finally, we denote by ρan arbitrary request\nsequence for Ppartitioned into n3blocks. For any 1 ≤k≤n3letρ(k)denote the first kblocks\nofρand for a request x∈ X ∪ Q , letN(x, k, ρ ) denote the number of times that xoccurs in\nρ(k).\nOur proof will proceed in three parts. In Part 1, we describe how to modify ρ′intoρ∗for\nanyγ-OuMv instance. In Part 2, we show that π(ρ∗) = ˆρfor some permutation |π−id| ≤d. In\nPart 3, we complete the proof by showing how an algorithm with bounded delay can be given\nˆρas prediction and can be used to answer any γ-OuMv instance.\nPart 1: Constructing ρ∗from ρ′\nWe begin by describing the construction of ρ∗. In Lemma 6.3, we will argue that ρ∗constructed\nfrom ρ′correctly encodes the γ-OuMv instance, while satisfying certain properties that we will\nuse in Part 2 to show that π(ρ∗) = ˆρfor some permutation πthat is d-close to the identity\npermutation. We construct ρ∗sequentially, appending requests to the end of ρ∗. Recall that we\nhave constructed the universal request sequence ¯ ρby imposing an arbitrary order onto Band\nconcatenating n3copies of B. Whenever we append a request x, we always append the copy\n21\n\nofxthat occurs earliest in the universal sequence ¯ ρout of all requests of ¯ ρthat we have not\nalready added to ρ∗.\nWe proceed by induction on k. For k= 1, we create B∗\n1in three steps.\n1. We begin by initializing B∗\n1toB′\n1.\n2. Then, for every update x∈ Xsatisfying N(x,1, ρ′)≤M(x)−ord(x), we appendj\nM(x)−N(x,1,ρ′)\nord(x)k\n·ord(x) copies of xto the end of B∗\n1. Recall that we always append the\ncopy of an update that occurs earliest in ¯ ρfirst.\n3. Finally, for every query q∈ Qsuch that N(q,1, ρ′)< M(q), we append M(q)−N(q,1, ρ′)\ncopies of qto the end of B∗\n1. Recall that we always append the copy of a query that occurs\nearliest in ¯ ρfirst.\nDenote this augmented sequence by ρ∗\n(1)=B∗\n1. For k >1 we extend ρ∗\n(k−1)toρ∗\n(k)also in\nthree steps. For i∈ {1,2,3}, letρ∗\n(k),idenote the sequence ρ∗after step iwhen extending ρ∗\n(k−1)\ntoρ∗\n(k). Note ρ∗\n(k)=ρ∗\n(k),3.\n1. We begin by concatenating B′\nkto obtain ρ∗\n(k),1←B∗\n1◦B∗\n2◦. . .◦B∗\nk−1◦B′\nk=ρ∗\n(k−1)◦B′\nk.\nWe emphasize that we always append the copy of an update that occurs earliest in ¯ ρfirst.\nIn particular, in this step we may in fact append a copy of xfrom Bjforj < k rather\nthan from Bk.\n2. Then, for every update x∈ X such that N\u0010\nx, k, ρ∗\n(k),1\u0011\n≤kM(x)−ord(x), we append$\nkM(x)−N\u0010\nx,k,ρ∗\n(k),1\u0011\nord(x)%\n·ord(x) copies of xto the end of ρ∗\n(k),1, emphasizing that we always\nappend the copy of an update that occurs earliest in ¯ ρfirst.\n3. For every query q∈ Qsuch that N\u0010\nq, k, ρ∗\n(k),2\u0011\n< kM (q), append kM(q)−N\u0010\nq, k, ρ∗\n(k),2\u0011\ncopies of qto the end of ρ∗\n(k),2, emphasizing that we always append the copy of a query\nthat occurs earliest in ¯ ρfirst.\nFinally, after the final block B′\nn3, we add in all remaining unused requests from the universal\nrequest sequence ¯ ρ. Thus ρ∗contains exactly all requests of ˆ ρ.\nWe now claim that ρ∗computes the same γ-OuMv instance as ρ′, while satisfying certain\nadditional properties we will use in Part 2.\nLemma 6.3. The constructed sequence ρ∗satisfies the following properties.\n1. For all x∈ X and1≤k≤n3,kM(x)−ord(x)< N(x, k, ρ∗)≤kM(x).\n2. For all q∈ Q and1≤k≤n3,N(q, k, ρ∗) =kM(q).\n3. Each γ-OuMv request (⃗ uk,⃗ vk),⃗ u⊤\nkM⃗ vkcan be answered in O(q(n1, n2))time given the\nanswers to the queries in the request sequence ρ∗\n(k)=B∗\n1◦B∗\n2◦. . .◦B∗\nk.\nProof. We proceed by induction on k. Let k= 1. We begin by verifying Condition 1. After\nStep 1, we have N(x,1, ρ′)≤M(x) copies of xinB∗\n1, asB′\n1⊆B1. In Step 2, we append\n(M(x)−N(x,1, ρ′))−ord(x)<\u0016k·M(x)−N(x,1, ρ′)\nord(x)\u0017\nord(x)≤M(x)−N(x,1, ρ′)\ncopies of xto the end of B∗\n1so that,\nM(x)−ord(x)< N(x,1, ρ∗)≤M(x)\n22\n\nSince we only append queries in Step 3, Condition 1 is satisfied. We satisfy Condition 2 with a\nsimular argument, since we have N(q,1, ρ′)≤M(q) copies of qinB∗\n1after Step 1 as B′\n1⊆B1,\nand we append M(q)−N(q,1, ρ′) copies of qin Step 3, therefore obtaining M(q) copies of qin\nB∗\n1.\nTo verify Condition 3, observe that the appended queries occur after B′\n1, so that the answers\nto the queries in B′\n1are the same in ρ′\n(1)andρ∗\n(1). Therefore, by Condition 2 of the Theorem,\nwe compute ⃗ uT\nkM⃗ vkinO(q(n1, n2)) time given the answers to the queries in B′\n1.\nNow, let k >1. We begin with Condition 1. By the inductive hypothesis, N(x, k−1, ρ∗)≤\n(k−1)M(x). In Step 1, we append B′\nk⊂Bkwhich contains at most M(x) copies of the update\nxso that N\u0010\nx, k, ρ∗\n(k),1\u0011\n≤kM(x). In Step 2, we append\n\u0010\nk·M(x)−N\u0010\nx, k, ρ∗\n(k),1\u0011\u0011\n−ord(x)<M(x)−N\u0010\nx, k, ρ∗\n(k),1\u0011\nord(x)ord(x)\n≤\u0010\nk·M(x)−N\u0010\nx, k, ρ∗\n(k),1\u0011\u0011\ncopies of xtoρ∗\n(k),1. Since we append only queries in Step 3,\nN(x, k, ρ∗) =N\u0010\nx, k, ρ∗\n(k),3\u0011\n=N\u0010\nx, k, ρ∗\n(k),2\u0011\n∈[kM(x)−ord(x) + 1, kM(x)]\nFollowing a similar argument, we verify Condition 2 and note that,\nN(q, k, ρ∗) =N\u0010\nq, k, ρ∗\n(k),3\u0011\n=kM(q)\nFinally, we verify Condition 3. For all k,B∗\nk\\B′\nkcontains l·ord(x) copies of each update\nx∈ Xfor some l≥0, as all updates in B∗\nk\\B′\nkare appended in Step 2. Furthermore, queries do\nnot modify the underlying data structure and therefore do not affect the answers given to other\nqueries. Then, the answers to the queries in B′\nkare the same in the request sequences ρ′, ρ∗.\nTherefore, we may compute ⃗ uT\nkM⃗ vkusing the answers from the queries in B′\nkinO(q(n1, n2))\ntime.\nPart 2: Showing that π(ρ∗) = ˆρfor some π∈Perm( T)such that |π−id| ≤d\nWe show that ρ∗can be obtained by re-ordering the predicted sequence ˆ ρ, for some permutation\nπthat is d-close to the identity. In particular, this will show that ˆ ρis a prediction with bounded\ndelay dfor request sequence set Sconsisting of all request sequences ρ∗which can be produced\nin Step 1.\nRecall that ˆ ρis the universal request sequence ¯ ρobtained by concatenating n3copies of B.\nWe will prove that ˆ ρhas ( C+ 1)( u(n1, n2) +q(n1, n2)) delay with the following steps. First,\nin Lemma 6.4, we show that any request occurs at most O(1) blocks away from its predicted\nposition. Then, in Lemma 6.5, we bound the size of each block. Combining, we show in Lemma\n6.6 that we obtain an upper bound on the delay of prediction ˆ ρ.\nLemma 6.4. Let1≤k≤n3.\nIfx∈Bkis an update, then x∈B∗\nifork≤i≤k+l\nord(x)\nM(x)m\n−1.\nIfq∈Bkis a query, then q∈B∗\nk.\nProof. Consider an update x∈ X. Since we insert copies of xin the order that they occur in\nthe universal sequence ¯ ρduring the construction of ρ∗, the j-th copy of xin ˆρis the j-th copy\nofxinρ∗, for all 1 ≤j≤n3M(x).\nFirst, we show that x∈B∗\nifork≤i≤k+ ord( x)−1. Suppose for contradiction i≤k−1.\nSince x∈Bkit is at least the (( k−1)M(x) + 1)-th occurrence of xinρ∗. Then, N(x, i, ρ∗)≥\n23\n\n(k−1)M(x) + 1≥iM(x) + 1, contradicting Condition 1 of Lemma 6.3. Otherwise, suppose\ni≥k+l\nord(x)\nM(x)m\n. Since x∈Bkit is at most the ( kM(x))-th occurrence of xinρ∗. Then,\nN\u0012\nx, k+\u0018ord(x)\nM(x)\u0019\n−1, ρ∗\u0013\n≤kM(x)−1\n≤kM(x)−M(x)\n=kM(x) + ord( x)−M(x)−ord(x)\n≤\u0012\nk+\u0018ord(x)\nM(x)\u0019\n−1\u0013\nM(x)−ord(x)\ncontradicting Condition 1 of Lemma 6.3.\nNow, we show q∈B∗\nk. Since from Condition 2 of Lemma 6.3, N(q, k, ρ∗) =kM(q) for all k,\nthere are exactly M(q) copies of qin each block B∗\nk. By definition, there are also exactly M(q)\ncopies of qin each block Bkof ˆρ. Since the copies of qare in the same order in ρ∗as in ˆ ρ, we\nhave q∈B∗\nk.\nLemma 6.5. Letρ∗be a request sequence as constructed in Part 1. Let C= max x∈Bord(x).\nFor all 1≤k≤n3,\nk(u(n1, n2) +q(n1, n2))−C·u(n1, n2)<\f\f\fρ∗\n(k)\f\f\f≤k(u(n1, n2) +q(n1, n2))\nProof. Fix an update x∈B. By Conditions 1 and 2 of Lemma 6.3, ρ∗\n(k)contains at least\nkM(x)−ord(x) + 1 and at most kM(x) copies of x. If we fix a query q∈B,ρ∗\n(k)contains\nexactly kM(q) copies of q. Summing over all unique updates and queries in B, we obtain\n|ρ∗\n(k)| ≤X\nx∈BkM(x) +X\nq∈BkM(q)≤k|B|=k(u(n1, n2) +q(n1, n2))\nand\n|ρ∗\n(k)|>X\nx∈B(kM(x)−ord(x)) +X\nq∈BkM(q)≥k(u(n1, n2) +q(n1, n2))−C·u(n1, n2)\nLemma 6.6. Letρ∗be a request sequence as constructed in Part 1. Let C= max x∈Bord(x).\nThen, there exists permutation πthat is (1 + C)(u(n1, n2) +q(n1, n2))close to the identity\npermutation and π(ρ∗) = ˆρ.\nProof. It suffices to show that for all requests x∈Band 1 ≤j≤n3M(x), the j-th copy of\nxdoes not occur at an index more than (1 + C)(u(n1, n2) +q(n1, n2)) away from the index\nwhere the j-th copy of xoccurs in ˆ ρ. Recall that pos(x, j, ρ ) denotes the index in the request\nsequence ρwhere the j-th copy of request xoccurs. Our goal is then to bound the error\n|pos(x, j, ρ∗)−pos(x, j,ˆρ)|.\nConsider an update x∈B. Let 1 ≤j≤n3M(x). The j-th copy of xin ˆρoccurs in the\n⌈j/M(x)⌉-th block of ˆ ρ. Then,\npos(x, j, ρ∗)>\f\f\fρ∗\n(⌈j/M(x)⌉−1)\f\f\f≥(⌈j/M(x)⌉ −1) (u(n1, n2) +q(n1, n2))−C·u(n1, n2)\nwhere the first inequality follows from Lemma 6.4 and the second from Lemma 6.5. Similarly,\n24\n\npos(x, j, ρ∗)≤\f\f\fρ∗\n(⌈j/M(x)⌉+⌈ord(x)/M(x)⌉−1)\f\f\f\n≤\u0012\u0018j\nM(x)\u0019\n+\u0018ord(x)\nM(x)\u0019\n−1\u0013\n(u(n1, n2) +q(n1, n2))\n≤\u0012\u0018j\nM(x)\u0019\n+\u0018C\nM(x)\u0019\n−1\u0013\n(u(n1, n2) +q(n1, n2))\nSince ˆ ρ=B1◦B2◦. . .◦Bn3,\npos(x, j,ˆρ)>\f\f\fρ∗\n(⌈j/M(x)⌉−1)\f\f\f≥(⌈j/M(x)⌉ −1) (u(n1, n2) +q(n1, n2))\npos(x, j,ˆρ)≤\f\f\fρ∗\n(⌈j/M(x)⌉)\f\f\f≤(⌈j/M(x)⌉) (u(n1, n2) +q(n1, n2))\nas the j-th copy of xmust occur in the ⌈j/M(x)⌉-th block of ˆ ρand each block Bkhas size\nu(n1, n2) +q(n1, n2). Combining our inequalities, we obtain,\n|pos(x, j,ˆρ)−pos(x, j, ρ∗)| ≤(C+ 1)( u(n1, n2) +q(n1, n2))\nConsider now a query q∈B. In both sequences ˆ ρ, ρ∗, the j-th copy of qoccurs in the\n⌈j/M(x)⌉-th block. Again, we begin by bounding pos(q, j, ρ∗). Then,\npos(q, j, ρ∗)>\f\f\fρ∗\n(⌈j/M(x)⌉−1)\f\f\f≥(⌈j/M(x)⌉ −1) (u(n1, n2) +q(n1, n2))−C·u(n1, n2)\nand\npos(x, j, ρ∗)≤\f\f\fρ∗\n(⌈j/M(x)⌉)\f\f\f≤(⌈j/M(x)⌉) (u(n1, n2) +q(n1, n2))\nwhere in both equations, the first inequality follows from Lemma 6.4 and the second from\nLemma 6.5. On the predicted request sequence ˆ ρ, we get the same bounds as for updates,\n(⌈j/M(q)⌉ −1) (u(n1, n2) +q(n1, n2))< pos (q, j,ˆρ)≤(⌈j/M(q)⌉) (u(n1, n2) +q(n1, n2))\nCombining our inequalities again,\n|pos(q, j,ˆρ)−pos(q, j, ρ∗)| ≤(C+ 1)( u(n1, n2) +q(n1, n2))\ncompleting the proof of the Lemma.\nPart 3: Proof of Theorem 6.2 for Fully Dynamic Locally Reducible Problems\nWe now complete the proof of Theorem 6.2 for fully dynamic problems. Suppose there exists\nan algorithm Awith (1 + C)(u(n1, n2) +q(n1, n2)) bounded delay predictions solving Pwith\npolynomial preprocessing time, update time U(n) and query time Q(n).\nWe design an algorithm B(that works without prediction) for the γ-OuMv problem. Let\n(M,U) be a worst-case γ-OuMv instance. In the preprocessing step of B, we compute the\nuniversal sequence ¯ ρ= ¯ρ(n1, n2) in polynomial time, construct the predicted request sequence\nˆρ= ¯ρ, and give ˆ ρas input to A. Note we do not need to see the matrix Mnor the request\nsequence Uto construct ˆ ρ. Recall that matrix Mis given to Bduring preprocessing. It gives\nMtoA, which builds the initial data structure D0. This completes the preprocessing phase.\nNext, given a vector update ( ⃗ uk,⃗ vk),Bconstructs the sequence B∗\nkand asks Ato perform\nthis sequence of requests. Areturns the correct answers to the requests in B∗\nktoBas, by\n25\n\nLemma 6.6, ˆ ρis a (1 + C)(u(n1, n2), q(n1, n2)) delayed prediction for ρ′=ρ′(U), and Ais a\ncorrect algorithm when given (1+ C)(u(n1, n2), q(n1, n2)) delayed predictions. Thus, by Lemma\n6.3,Bcan correctly answer ⃗ uT\nkM⃗ vkinO(q(n1, n2)) time given the answers to the queries in B′\nk.\nLet us analyze the complexity of B. In the preprocessing phase, Bconstructs ˆ ρandD0,\nrequiring only polynomial time. For each vector update, Bcomputes B∗\nkinO(u(n1, n2) +\nq(n1, n2)) (Lemma 6.7), asking Ato perform the updates in B∗\nk. Since Bsolves γ-OuMv, the\nOuMv conjecture states that Acannot satisfy,\nn3\u0010\nu(n1, n2)U(n) +q(n1, n2)Q(n)\u0011\n=˜˜o(n1n2n3)\nWe conclude the proof by proving Lemma 6.7.\nLemma 6.7. For all 1≤k≤n3,B∗\nkcan be constructed in O(u(n1, n2) +q(n1, n2))time.\nProof. Since B∗\nj\\Bjcontains a multiple of ord( x) copies of every update x, the state of the data\nstructure after B∗\n1◦. . . B∗\nk−1is the same as the state of the data structure after B′\n1◦. . . B′\nk−1.\nThen, given vector update ( ⃗ uk,⃗ vk), construct B′\nkas promised by the γ-OuMv reduction. To\ncompute ρ∗\n(k),2, ρ∗\n(k),3, we can keep count of the number of copies of xinserted into ρ∗so far.\nComputing the number of copies of xto append to ρ∗and then appending these requests\ntoρ∗requires time O(|B∗\nk|). From Lemma 6.5, we can conclude that |B∗\nk|=O(u(n1, n2) +\nq(n1, n2)).\nThis concludes our proof of Theorem 6.2.\nNext, we prove an analogous result for partially dynamic problems with a simpler argument.\nWe note that this lower bound gives a similar result to Theorem 1.3 in the independent work\nof [vdBFNP23].\nTheorem 6.8. Letγ >0be a constant and u, qbe functions. Suppose Pis a partially dynamic\nproblem that is (u, q)-locally reducible from γ-OuMv.\nThen there is no algorithm solving Pwith (u(n1, n2) +q(n1, n2))delayed predictions with\nupdate time U(n)and query time Q(n)satisfying\nn3\u0010\nu(n1, n2)U(n) +q(n1, n2)Q(n)\u0011\n=˜˜o(n1n2n3)\nif the OuMv conjecture is true.\nProof. For partially dynamic problems, we do not need to modify the reduction from γ-OuMv.\nIn particular, we set ρ∗=ρ′.\nWe again claim that the prediction ˆ ρ= ¯ρhas small bounded delay. Note that ˆ ρhas bounded\ndelay at most u(n1, n2) +q(n1, n2), since for any request xthej-th occurrence of xoccurs in\nthe same block in both the predicted sequence ˆ ρand the actual sequence ρ∗. Furthermore, the\nsizes of the blocks in ρ∗,ˆρare identical, both of size |Bk|=u(n1, n2) +q(n1, n2). Therefore,\nsuppose for contradiction that there is an algorithm Awith update time U(n) and query time\nQ(n) satisfying,\nn3\u0010\nu(n1, n2)U(n) +q(n1, n2)Q(n)\u0011\n=˜˜o(n1n2n3)\ngiven u(n1, n2) +q(n1, n2) bounded delay predictions. Then, we again have a pure dynamic\nalgorithm Bforγ-OuMv that constructs the prediction ˆ ρin the preprocessing phase, providing\nthis as input to A. Then, given a vector update, BasksAto compute the request sequence\nB′\nk, computing ⃗ uT\nkM⃗ vkgiven the answers to the queries in B′\nk. Following a similar argument to\nthe fully dynamic case, Bcomputes γ-OuMv in total time ˜˜o(n1n2n3), contradicting the OuMv\nconjecture.\n26\n\nAbove, we have established that algorithms with predictions with delay (1 + C)(u(n1, n2) +\nq(n1, n2)) with C= max x∈Bord(x) cannot be more efficient than algorithms with no predictions\nat all. In the following, we show that for smaller delay the conditional lower bounds based on\nthe OuMv conjecture degrade gracefully with the quality of the predictions.\nTheorem 6.9. Suppose Pis(u, q)-locally reducible from γ-OuMv with n=n(n1, n2)a non-\ndecreasing function. Let t∈(0,1)be a constant. Let d1=⌊dγ\n2⌋where d2=⌊nt\n2⌋.\nThen there is no algorithm solving Pon instances of size n=n(n1, n2)with (1 +\nC)(u(d1, d2) +q(d1, d2))delayed predictions with update time U(n)and query time Q(n)satis-\nfying\nn3\u0010\nu(d1, d2)U(n) +q(d1, d2)Q(n)\u0011\n=˜˜o(d1d2n3) =˜˜o(nt\n1nt\n2n3)\nif the OuMv conjecture is true.\nIn particular, as the guaranteed prediction quality increases (as tdecreases towards 0), the\nlower bound weakens.\nProof. Consider a S-γ-OuMv instance with n1×n2matrix Mand vector requests {(⃗ uk,⃗ vk)}n3\nk=1.\nSuppose further that there are subsets S1⊆[n1] of size d1andS2⊆[n2] of size d2and\nsupp( ⃗ uk)⊂S1and supp( ⃗ vk)⊂S2for all k.\nNow, observe that,\n⃗ u⊤\nkM⃗ vk=X\ni,j⃗ uk[i]M[i][j]⃗ vk[j] =X\ni∈S1,j∈S2⃗ uk[i]M[i][j]⃗ vk[j]\nso only the values M[i][j] where i∈S1, j∈S2influence the final product. Consider then the\nγ-OuMv instance with d1×d2matrix M[S1][S2] and vector updates {(⃗ uk[S1],⃗ vk[S2])}n3\nk=1. In\nparticular, Since Pis (u, q)-locally reducible from γ-OuMv, from Theorems 6.2 and 6.8 there is\nno algorithm solving Pwith (1 + C)(u(d1, d2) +q(d1, d2)) delayed predictions with update time\nU(d) and query time Q(d) satisfying,\nn3(u(d1, d2)U(d) +q(d1, d2)Q(d)) = ˜˜o(d1d2n3)\nwhere d=n(d1, d2) is the size of the Pinstance in the reduction. Since n=n(n1, n2)≥\nn(d1, d2) =d, the lower bound also holds for any algorithm solving instances of size n. To\nconclude, note that d1= Θ( dγ\n2) = Θ( nγt\n2) = Θ( nt\n1) and d2= Θ( nt\n1).\n6.2 The #s-△Problem is Locally Reducible\nKeeping the reduction from Theorem 5.1 in mind, it is now easy to show that any algorithm\nwith O(n) delayed predictions is subject to the same conditional lower bound under the OMv\nConjecture.\nTheorem 6.10. There is no algorithm solving the #s-△problem with O(n)delayed predictions\nwith update time U(n)and query time Q(n)satisfying\nn2U(n) +nQ(n) =˜˜o(n3)\nif the OuMv conjecture is true.\nProof. We claim the # s-△problem is a fully dynamic locally-reducible problem. Set parameters\nn1=n2=n3=n. We claim that # s-△is (n1+n2,1)-locally reducible where u(n1, n2) =n1+n2\nandq(n1, n2) = 1. It is easily verified that the reduction of Theorem 5.1 from γ-OuMv to # s-△\ninstances of size n(n1, n2) = 1 + n1+n2= 2n+ 1 satisfies the required conditions, with\nBk={(s, ui)}n\ni=1∪ {(s, wi)}n\ni=1∪ {q}. Finally, we note that each update (edge flip) has cyclic\norder 2.\n27\n\nIn Theorem 8.5, we will show that for any d=O(n1−ε), there is an algorithm with d\ndelayed predictions overcoming the lower bound below. In particular, we will show there is\nan update optimized algorithm with constant update time and O(d2) query time, as well as a\nquery optimized algorithm with O(d) update time and constant query time. In particular, when\nthe prediction quality is better than the linear threshold, there are algorithms with predictions\nthat bypass OuMv-based lower bounds. Next, we show that Theorem 8.5 is almost tight. This\nfollows immediately from Theorem 6.9 and the above observation that # s-△is locally reducible.\nTheorem 6.11. Lett∈(0,1)be a constant and d=⌊nt⌋. There is no algorithm solving\nthe#s-△problem with O(d)-delayed predictions with update time U(n)and query time Q(n)\nsatisfying\nndU(n) +nQ(n) =˜˜o(nd2)\nif the OuMv conjecture is true.\nIn particular, either update time is not ˜˜o(d) or query time is not ˜˜o(d2). Thus, Theorem 6.11\nand Theorem 8.5 are (almost) tight.\n7 Further Locally Reducible and Locally Correctable Problems\nWe now give a list of examples of Locally Reducible Problems , noting that this list includes almost\nall instances of dynamic problems that have OMv/OuMv-based lower bounds. All referenced\nlower bounds are conditional on the OMv Conjecture unless otherwise stated. Throughout this\nsection, let δ∈(0,1) be a constant. Unless otherwise specified, all graphs are unweighted and\nundirected with nvertices and medges.\nSubgraph Connectivity In the subgraph connectivity problem, the algorithm is given a\nfixed graph Gand a subset S⊂V. Each update adds (resp. removes) a vertex vto (resp.\nfrom) S. Let G[S] denote the subgraph of Ginduced by S. In the ( s, t) subgraph connectivity\nproblem, each query asks for a fixed pair of vertices ( s, t) ifs, tare connected in G[S]. The single\nsource subgraph connectivity problem asks if a fixed source sis connected to any vertex vin\nG[S]. The all pairs subgraph connectivity problems asks for any pair u, vif they are connected\ninG[S]. [HKNS15] give a local reduction showing that no partially dynamic algorithm has worst\ncase update time ˜˜o(n) and query time ˜˜o(n2). [HKNS15] also give a local reduction1−δ\nδ-uMv to\npartially dynamic single source subgraph connectivity showing that no algorithm has update\ntime ˜˜o(mδ) and query time ˜˜o(m1−δ). All of the above bounds hold for fully dynamic algorithm\nwith amortized update and query time.\nReachability In reachability problems, the algorithm is given an initial directed graph G.\nEach update adds or removes an edge in G. In the ( s, t) reachability problem, each query asks\nif for a fixed pair of vertices ( s, t) iftis reachable from s. The single source reachability problem\nasks if any vertex vis reachable from a fixed source s. The all pairs reachability (also known as\nTransitive Closure or TC) problem asks for any pair u, vifvis reachable from u. A similar local\nreduction for the reachability problem shows similar bounds as in the subgraph connectivity\nproblem.\nShortest Path In the shortest path problem, the algorithm is given an initial graph G. Let\nδG(u, v) denote the distance between vertices u, vin the graph G, abbreviated δ(u, v) when the\nunderlying graph is clear. Each update adds or removes an edge in G. The ( s, t)-shortest path\n((s, t)-SP) problem asks for a fixed pair ( s, t) the distance δ(s, t). The single source shortest path\n(SSSP) problem asks for a fixed source sand any vertex vthe distance δ(s, v). The all pairs\n28\n\nshortest path (APSP) problem asks for any pair ( u, v) the distance δ(u, v). For any number\nα≥1, an α-approximate algorithm is required to return a distance estimate ˆd(u, v) satisfying\nδ(u, v)≤ˆδ(u, v)≤α·δ(u, v).\nA local reduction of [HKNS15] shows that no partially dynamic\u00005\n3−ε\u0001\n-approximation al-\ngorithm for ( s, t)-shortest path can have worst case update time ˜˜o(n) and query time ˜˜o(n2).\nFurthermore, [HPS21] show a similar local reduction if updates are restricted to only changing\nthe weights of edges in the graph. While updates in the model as defined are not cyclic, the\nreduction essentially toggles edge weights between the set {1,3}, which is a cyclic request.\n[HKNS15] also show that no partially dynamic (3 −ε)-approximation algorithm can have\nworst case update time ˜˜o(n1/2) and query time ˜˜o(n), a lower bound that is extended to con-\nstant degree graphs by [HPS22]. Furthermore, the same lower bound holds for expander and\npower law graphs in the fully dynamic case [HPS22], while no algorithm can have update time\n˜˜o(n(1+t)/2) and query time ˜˜o(n1+t) on graphs with maximum degree nt. No partially dynamic\n(2−ε)-approximation algorithm for SSSP can have worst case update time ˜˜o(mδ) and query\ntime ˜˜o(m1−δ). The above bounds for partially dynamic algorithms hold also for fully dynamic\nalgorithms with amortized complexity.\n[HKNS15] give a local reduction showing that there is no partially dynamic ( s, t)-shortest\npath algorithm with total update time ˜˜o(m3/2) and query time ˜˜o(m) or (2 −ε)-approximate\nAPSP algorithm with total update time ˜˜o(n2−δ\n1−δ) and query time ˜˜o(nδ\n1−δ) for δ≤1\n2.\nOn planar graphs, [AD16] show that no fully dynamic APSP algorithm on weighted graphs\ncan have amortized update time and query time ˜˜o(n1/2). On unweighted graphs, no fully\ndynamic algorithm can have amortized update time u(n) and query time q(n) satisfying\nmax( q(n)2u(n), q(n)u(n)2) =˜˜o(n)\nin particular showing that update and query time cannot both be ˜˜o(n1/3).\nDistance Spanners and Emulators Given an undirected, unweighted graph G, a subgraph\nH⊂Gis an ( α, β) spanner if for every pair of vertices x, y∈V,δG(x, y)≤δH(x, y)≤\nα δG(x, y) +β. A weighted graph H′is an ( α, β)-emulator if it fulfills the above constraint, but\nis not necessarily a subgraph of G. In particular, every spanner is an emulator but not vice\nversa. In the dynamic spanner/emulator problem, the algorithm is given an initial graph G0\nand each update inserts/removes an edge and asked to maintain at each time step an ( α, β)\nspanner of the dynamic graph. [BHG+21] show that there is no partially dynamic algorithm\nmaintaining a (1 , no(1))-emulator (and therefore spanner) with ˜˜o(m) edges, arbitrary polynomial\npreprocessing time, and total update time ˜˜o(mn).\nBipartite Maximum Cardinality Matching The algorithm is given an initial bipartite\ngraph G= (A, B). Each update inserts/removes an edge with one endpoint in Aand another\ninB. Each query asks for the cardinality of the maximum matching in G. [Dah16] gives a local\nreduction showing that no partially dynamic algorithm (even only on bipartite graphs) can have\namortized update time ˜˜o(n) and query time ˜˜o(n2), improving upon the ˜˜o(m1/2) update time\nand˜˜o(m) query time lower bound of [HKNS15]. [HPS22] extend these results to show that even\nfor constant degree graphs, no partially dynamic algorithm can have amortized update time\n˜˜o(n1/2) and query time ˜˜o(n). [HPS22] give the same bounds for fully dynamic algorithms on\n(not necessarily bipartite) expander graphs and power law graphs. On graphs with maximum\ndegree nt, no fully dynamic algorithm can have amortized update time ˜˜o(n(1+t)/2) and query\ntime ˜˜o(n1+t). [HPS21] also give a lower bound when updates are restricted to only change\nweights. As with the case of shortest path, each update toggles the weight of an edge between\n{1,2}, which is cyclic.\n29\n\nMaximum (s, t)Flow The algorithm is given an initial graph with edge insertions and dele-\ntions as updates. Each query asks to compute the maximum flow from a fixed source sto a fixed\nsinkt. Combining the lower bound for maximum matching of [Dah16] and standard reductions\nfrom bipartite matching to directed flows, we can also argue that there is no partially dynamic\nalgorithm for maximum ( s, t) flow on unweighted directed graphs or weighted undirected graphs\nwith amortized update and query time ˜˜o(n).\nTriangle Detection and Counting In the triangle detection/counting problem, there is\nagain a dynamic graph undergoing edge insertions/deletions. Each query asks for the number\nof triangles in the graph G(or if any triangle exists). The # s-△problems asks for the number\nof triangles containing the vertex s(or if any triangle exists). [HKNS15] show that no partially\ndynamic algorithm can have worst case update time ˜˜o(n) and query time ˜˜o(n2), and the same\nbound holds for the amortized update time and query time of fully dynamic algorithms.\nDensest Subgraph The algorithm is given an initial graph G. Each update inserts/removes\nan edge. For any subset S⊂V, letE(S) denote the number of edges in the induced subgraph\nG[S]. The density of the subgraph G[S] is given by |E(S)|/|S|. Each query asks for the density\nof the densest subgraph. [HKNS15] show that there is no partially dynamic algorithm with\nupdate time ˜˜o(n1/3) and query time ˜˜o(n2/3). On constant degree graphs, expander graphs, and\npower law graphs, [HPS22] show that there are no fully dynamic algorithms with update time\n˜˜o(n1/4) and query time ˜˜o(n1/2).\nd-Failure Connectivity The algorithm is given a fixed graph G0. Each update restores the\noriginal graph G0and removes any dvertices from the original fixed graph G0. Each query asks\nifs, tare connected, for any pair s, t∈V. For any constant δ∈(0,1\n2] and n, m =O(n1/(1−δ)),\n[HKNS15] show that no algorithm has update time ˜˜o(nd) and query time ˜˜o(d).\nVertex Color Distance Oracle The algorithm is given a fixed graph G0. Each update\nchanges the color of a vertex. Each query asks for a given vertex sand color c, what is the\nshortest distance from sto any vertex of color c. [HKNS15] give a local reduction from1−δ\nδ-uMv\nto partially (3 −ε)-dynamic vertex color distance oracles showing that no algorithm has update\ntime ˜˜o(mδ) and query time ˜˜o(m1−δ). The result holds also for fully dynamic algorithm with\namortized update and query time.\nWeighted Diameter The algorithm is given an initial graph G. Each update inserts/removes\nan edge in G. Each query asks for the diameter of the graph. [HKNS15] show that there is no\n(2−ε)-approximate algorithm for {0,1}-weighted diameter with update time ˜˜o(√n) and query\ntime ˜˜o(n). While the updates in the original problem (setting an edge to a weight) are not\ncyclic, we can modify the updates to cycle between the following states for each vertex pair\n(u, v): no edge, weight 0 edge, weight 1 edge. Each original update requires at most 2 cyclic\nupdates, so that the lower bound still holds.\nStrong Connectivity The algorithm is given an initial graph G. Each update in-\nserts/removes an edge in G. Each query asks if the graph is strongly connected. A similar\nlocal reduction for strong connectivity shows similar bounds as in the subgraph connectivity\nproblem.\nElectrical Flows Given a ( s, t)-flow f:E→R, itsenergy is defined asP\ner(e)f(e)2where\nr(e) is the resistance of an edge (see e.g. [GHP17]). Generally, r(e) = 1 /w(e) is the inverse\nof the weight of an edge. The ( s, t)-electrical flow is the flow minimizing energy among all\n30\n\n(s, t)-flows with unit value. In the subgraph electrical flows problem, there is a fixed graph with\na dynamic set of vertices undergoing insertions and deletions. Each update either activates\n(inserts into the set) a vertex, or deactives (removes from the set) a vertex. Each query asks\nfor the ( s, t) electrical flow on the subgraph induced by the set of active vertices. [GHP17] give\na local reduction from 1-OuMv to the subgraph electrical flows problem.\nErickson’s Maximum Value Problem The algorithm is given an initial matrix of size\nn×n. Each update increments all the values in a given row or column. Each query asks for\nthe maximum value in the matrix. [HKNS15] show that there is no algorithm with update time\n˜˜o(n) and query time ˜˜o(n2).\nLangerman’s Zero Prefix Sum Problem The algorithm is given an initial array Aofn\nintegers. Each update sets A[i] =xfor any i, x. Each query asks if there exists ksuch thatPk\ni=1A[i] = 0. [HKNS15] show that there is no algorithm with update time ˜˜o(√n) and query\ntime ˜˜o(√n). As with weighted diamater, the original problem does not have cyclic updates.\nHowever, the reduction requires only a constant number of weights to be assigned to each array\nentry, and therefore we can replace each update with a constant number of cycles through the\nconstant number of values.\n7.1 Locally Correctable Fully-Dynamic Problems\nWe first note that for many fully dynamic graph problems listed in Section 7 are locally cor-\nrectable if we define f(G) to add two isolated vertices w0, w1to the graph G. Then, X∗consists\nof the single update ( w0, w1). For problems such as subgraph connectivity, reachability, shortest\npath, maximum flow, triangle detection, densest subgraph, d-failure connectivity, vertex color\ndistance oracle, and electrical flows, it suffices to consider gas the identity function, as the\nanswer to the query depends only on the connected component in the original reduction. For\nmaximum matching, we can define g(m) =m−χ(w0, w1) to subtract 1 from the matching if\nand only if ( w0, w1) is an edge. This is computable in constant time.\nFor diameter and strong connectivity, we will require a slightly more careful reduction, as\nf(G) as defined above always has infinite diameter and is never strongly connected. For strong\nconnectivity, we use the reduction from ( s, t) reachability to strong connectivity of [AW14].\nGiven a graph G,f(G) vertices w0, w1and adds directed edges ( t, v) and ( v, s) for v /∈ {s, t}.\nThen, we claim that regardless of ( w0, w1)∈E,tis reachable from sif and only if f(G) is\nstrongly connected. If tis reachable from sinGvia path P, then every pair of vertices u, vhas\nthe path ( u, s)◦P◦(t, v). Iff(G) is strongly connected, there is a path from stot, which cannot\nuse any of the additional edges, so that this path must be in G, and therefore tis reachable in\nGfrom s.\nFor weighted diameter, we consider the reduction of [HKNS15]. The reduction f(G) picks a\nvertex aand adds two vertices w0, w1each connected to awith weight 0 edges. X∗consists of\nturning on and off an edge of weight 0 between w0, w1. Note that the diameter of Gis exactly\nthat of f(G), as{a, w 0, w1}can be treated as a single vertex regardless of the presence of edge\n(w0, w1), and it suffices to set gto the identity function.\nConsidering a non-graph problem, for Langerman’s Zero Prefix Sum, we can easily define\nf(x) to extend the array by one entry and set X∗to be the update that sets the extra entry to\n0. Clearly this does not affect the sum of any subarray, so it suffices to take gas the identity.\n7.2 Locally Correctable Partially-Dynamic Problems\nBelow, we briefly discuss a few examples of locally correctable partially dynamic problems. For\npartially dynamic problems, we will require small modifications to the known reductions to\nensure that the problem is locally correctable.\n31\n\nShortest Path [HKNS15] give a local reduction from ( s, t)-shortest path to 1-OuMv. We\nrecall their reduction and argue that ( s, t)-shortest path is in fact a locally correctable problem.\nWe describe the reduction given by [HKNS15] for the decremental setting, noting that a similar\nargument holds in the incremental setting. Consider a 1-OuMv instance with n×nmatrix M\nand vector updates {(⃗ uk,⃗ vk)}n\nk=1. Construct a bipartite graph GMwith vertex sets L, R of size\nnand edges ( li, rj) if and only if M[i][j] = 1. Then, construct paths P, Q ofnvertices each and\nconnect all edges ( pk, li) and ( qk, rj). We define s=p1andt=q1to be the special vertices.\nThis defines the initial graph G0.\nIn the original reduction, with each vector update ( ⃗ uk,⃗ vk), we disconnect ( pk, li) if⃗ uk[i] = 0\nand ( qk, rj) if⃗ vk[j] = 0. After querying for the ( s, t) shortest path, we disconnect any remaining\nedges ( pk, li) and ( qk, rj), satisfying Conditions 1 and 2. [HKNS15] show that δ(s, t)≤2t+ 1 if\n⃗ uT\nkM⃗ vk= 1 and δ(s, t)≥2t+ 2 otherwise.\nSince we have to remove any remaining edges after the query step, we define each block of\nthe universal request sequence to first disconnect all edges ( pk, li),(qk, rj), query for the shortest\n(s, t)-path, and then disconnect all edges ( pk, li),(qk, rj) again. We need to remove all edges\na second time to ensure that we can disconnect the remaining edges even when the request\nsequence is required to be a subsequence of the universal request sequence.\nMaximum Matching [Dah16] gives a local reduction from bipartite maximum matching to\nOuMv. We will consider the incremental setting, as in [Dah16]. Consider a 1-OuMv instance\nwith n×nmatrix Mand vector updates {(⃗ uk,⃗ vk)}n\nk=1. [Dah16] constructs a graph with vertex\nsetsS, A, B, C, D, T each a bipartite graph on 2 nnodes with nnodes on the left and right\nside each. Each A, B, C, D consists of a perfect matching, connecting all edges ( xl\ni, xr\ni) for all\nx∈ {a, b, c, d }andi∈[n]. Furthermore, connect ( br\ni, cl\nj) if and only if M[i][j] = 1. This\nconstitutes the graph G0.\nGiven a vector update ⃗ uk,⃗ vk, [Dah16] connects ( ar\nk, bl\ni) if⃗ uk[i] = 1 and ( cr\nj, dl\nk) if⃗ vk[j] = 1.\nNext, add the edges ( sr\nk, al\nk) and ( dr\nk, tl\nk) and query for a maximum matching. Finally, add the\nedges ( sl\nk, sr\nk) and ( tl\nk, tr\nk) and any remaining edges ( ar\nk, bl\ni) and ( cr\nj, dl\nk).\nSince we have to add remaining edges after the query, we define each block of the universal\nrequest sequence to 1) add all edges ( ar\nk, bl\ni),(cr\nj, dl\nk), 2) add edges ( sr\nk, al\nk),(dr\nk, tl\nk), 3) query for\nthe maximum matching, 4) add edges ( sl\nk, sr\nk),(tl\nk, tr\nk) and finally, 5) add all edges ( ar\nk, bl\ni),(cr\nj, dl\nk)\nagain.\nSSSP, APSP, and Transitive Closure [HKNS15] give also local reductions from single\nsource shortest path and (2 −ε)-approximate all pairs shortest path to 1-OuMv. Following\na similar approach to above, we copy all updates in the universal request sequence after the\nquery, allowing the remaining updates to be computed while maintaining that the true request\nsequence is a subsequence of the universal request sequence.\nDistance Spanners and Emulators The reduction of [BHG+21] from OuMv to dynamic\ndistance spanners and emulators can be augmented to a graph with a single additional edge\ndisconnected from the remainder of the graph. Since each vector update is encoded into the\ndynamic graph by choosing a subsequence of some universal request sequence, distance spanners\nand emulators are a locally correctable problem.\n8 Dynamic Algorithms with Bounded Delay Predictions\nIn this section, we give several dynamic algorithms with predictions to overcome conditional\nlower bounds under the OMv Conjecture. While the overall computation time (including pre-\nprocessing time) may not be less than a prediction-less dynamic algorithm, we can maintain a\n32\n\ndata structure with more efficient updates and queries, whilst performing the expensive compu-\ntations in the preprocessing phase. We believe that this is justified since the precomputation can\nbe done for a single prediction and then reused for any request sequence in the future for which\nthe prediction is valid. If the predictions are perfect, then we can simply handle updates and\nqueries in constant time by returning the precomputed answers. If predictions are reasonably\naccurate, our algorithms will make small, thereby efficient, adjustments to the precomputed\nanswers.\nWe will use the following basic fact about predictions with bounded delay in our dynamic\nalgorithms with predictions.\nLemma 8.1. Ifˆρhas at most d-delay for some set S ⊆(X ∪Q )T, then we have for each ρ∈ S\nthatˆρ≤t−d⊆ρ≤t⊆ˆρ≤t+dholds for all t∈[T].\nProof. Letρ∈ Sbe any request sequence. Since ˆ ρis ad-delayed prediction for S, there is a\npermutation π∈Perm( T) such that πisd-close to the identity permutation idand ˆρ=π(ρ).\nConsider for a time step jthe request ρj∈ρ. Since πand idared-close, we have that\nd≥\f\f\fπ−1\u0000\nπ(j)\u0001\n−id−1\u0000\nπ(j)\u0001\f\f\f=|j−π(j)|,\nwhich yields for π(j) that j−d≤π(j)≤j+d. In particular, for all ρj∈ρ≤t, we have\nπ(j)≤j+d≤t+dand therefore ρj∈ˆρ≤t+d. To show the other inclusion, consider some\nρπ(j)∈ˆρ≤t−d. Then, π(j)≤t−dimplies that j≤π(j) +d≤tand therefore ρj∈ρ≤t.\n8.1 Bounded Delay Predictions with Outliers\nThe bounded delay model requires that the prediction ˆ ρand each sequence ρ∈ S have the\nsame length and contain the same requests. We will design algorithms with predictions that\nin fact will be able to handle small discrepancies between the set of predicted requests and\nactual requests. In this section, we define a weaker notion of prediction by allowing that the\nset of elements, of predicted sequence ˆ ρand request sequence ρ∈ S′, may differ up to a small\nnumber k≥0 ofoutlier elements, resulting in the following, more general prediction model (i.e.\nS′⊇ S).\nOur definition will use the following notation.\nDefinition 17. Letρbe a request sequence of length TandI⊂[T]a subset. The complement\nofIis denoted by IC= [T]\\I. Then, the subsequence ρI= (ρi)i∈Iis the subsequence defined\nby taking the elements at index i∈Iof the request sequence ρ.\nFor any i∈I, letp(i, I) =|{1≤j≤i|j∈I}|denote the index in Iwhere ioccurs.\nNote that ρ[T]=ρand, for any request ρt∈ρ, we have p(t, I)≤p(t,[T]) =tift∈I.\nDefinition 18 (Bounded Delay Predictions with Outliers) .LetS ⊆(X ∪Q )Tbe a set of request\nsequences of length T, and ˆρ= (ˆρ1,ˆρ2, . . . , ˆρT)a given sequence of Tpredicted requests.\nThen ˆρhas at most ddelay and at most koutliers for S, called d-delayed with koutliers\nforS, if there exist for any ρ∈ S\n(1) two sub-sequences I,ˆI⊂[T]that both have a length T′≥T−k, and\n(2) a π∈Perm( T′)such that π(ρI) = ˆρˆIandπisd-close to the identity permutation.\nRelaxing (2) to at most d-total-delay is called d-total-delayed with koutliers prediction.\nOf course, a prediction sequence ˆ ρisd-delayed if and only if ˆ ρisd-delayed with k= 0\noutliers. Next, we give a generalization of Lemma 8.1. At any time step, the symmetric\ndifference between the predicted and actual request sequences is linear in the delay and number\nof outliers.\n33\n\nLemma 8.2. Letˆρbe a prediction that is d-delayed with koutliers for S ⊆ (X ∪ Q )T. For\nρ∈ Sandt∈[T], letDt=ρ≤t∆ˆρ≤tdenote the symmetric difference between the set of the first\ntpredicted requests and the set of the first trequest that occurred in ρ(including multiplicities).\nThen, the symmetric difference contains at most |Dt| ≤4k+ 2delements.\nProof. Letρ∈ Sbe any request sequence. Since ˆ ρis ad-delayed prediction with koutliers,\nthere are two sub-sequences I,ˆI⊂[T] of length T′≥T−kand a permutation π∈Perm( T′)\nsuch that πisd-close to the identity permutation idandπ(ρI) = ˆρˆI. Let ρICand ˆρˆICbe the\nsub-sequence of at most koutlier requests in ρand ˆρ, respectively.\nConsider a time step t∈Iwith request ρt. Let t′=p(t, I)≤t. Since πand idared-close,\nwe have from d≥ |π−1(π(t′))−id−1(π(t′))|=|t′−π(t′)|that\nt′−d≤π(t′)≤t′+d .\nFrom t′∈[t−k, t], we that the index π(t′)∈[t−(k+d), t+d]. Since, π(ρI) = ˆρˆIandt∈I, we\nnote that ρt∈ρis shuffled to index π(t′) in ˆρˆI. As there are at most koutliers, ρtis shuffled\nto index at most π(t′) +k≤t+ (k+d) in ˆρ. Since outliers can only increase the index that ρt\nis shuffled to in ˆ ρ, we can also conclude that ρtis shuffled to index at least π(t′). We are now\nready to bound |Dt|.\nFirst, we bound ρ≤t\\ˆρ≤t. Consider a request at index j≤t. At most kindices in [ t] can be\ninIC. Suppose j∈I∩[t] and let j′=p(j, I) so that j−k≤j′≤j. From our above argument,\nthe request ρjis shuffled to at most index j+ (k+d)≤t+ (k+d) in ˆρ. Therefore, at most\n(k+d) requests in I∩[t] can be shuffled to index larger than t. Combined with the at most k\nrequests in IC, we can, thus, bound ρ≤t\\ˆρ≤tto have size (2 k+d).\nNext, we bound ˆ ρ≤t\\ρ≤t. Consider an index ˆj≤t. Suppose ˆj∈ˆI∩[t], noting that most k\nindices in [ t] can be in ˆIC. Then, denote by ˆj′=p(ˆj,ˆI) the position of ˆjinˆIand let j′be such\nthat ˆj′=π(j′) for some j′=p(j, I) where jis the index such that j′=p(j, I). Since ρand ˆρ\nhave at most koutliers, we can upper bound,\nj≤j′+k≤ˆj′+ (k+d)≤ˆj+ (k+d)≤t+ (k+d)\nSince we want to bound ˆ ρ≤t\\ρ≤t, we consider only the indices j≥t+ 1, of which there are\nat most ( k+d) indices satisfying t+ 1≤j≤t+ (k+d). Combined with the at most kindices\ninˆIC, we can bound ˆ ρ≤t\\ρ≤tto have size (2 k+d).\nFurthermore, the difference set can be maintained efficiently.\nLemma 8.3. For all t∈[T], the symmetric difference Dt−1can be updated to Dtin˜O(1)time.\nThe update bound can be made O(1)expected time.\nProof. Letρtbe the actual update and ˆ ρtbe the predicted update. If ρt= ˆρt, then Dtrequires\nno modification, so we may assume ρt̸= ˆρt. Clearly Dt−1andDtdiffer by at most two elements\n(specifically ρt,ˆρt) and we can easily update Dtin˜O(1) time.\n8.2 Two Algorithms for the #s-△Problem\nOur first example of algorithms with predictions is the # s-△problem. In the # s-△problem,\nthe algorithm is required to maintain the number of triangles that contain a fixed vertex s. In\nSection 6.2, we showed conditional lower bounds for the # s-△problem given bounded delay\npredictions. We now prove that our lower bound is almost optimal.\nAs a warm-up, we recall how to solve the # s-△problem in the standard online setting.\n34\n\nWarmup: #s-△Algorithm Without Predictions\nFor completeness, we start by revisiting a well-known algorithm for the # s-△problem. In our\nalgorithms with bounded delay predictions for the # s-△problem (Theorem 8.5), we use the\nonline algorithm (without predictions) as a sub-routine in the preprocessing phase to compute\nthe counts under the predicted update sequence.\nTheorem 8.4. There is an algorithm which solves the #s-△problem with O(n2)preprocessing\ntime, O(n)time per update, and O(1)time per query.\nThere is also an algorithm which solves the #s-△problem with O(n2)preprocessing time,\nO(1)time per update, and O(n2)time per query.\nProof. First, for any graph G, we can count the number of # s-△’s in the graph GinO(n2) time\nby considering all n2triples of vertices ( s, a, b ) for a, b∈V. Note that this gives automatically\nthe second update-optimized algorithm. In the pre-processing step, we use O(n2) time to save\nthe adjacency matrix of the initial graph. At each update, we update the adjacency matrix in\nconstant time, and compute # s-△from scratch in O(n2) time at each query.\nWe now describe the first query-optimized algorithm. In the preprocessing step, we save the\nadjacency structure of the initial graph Gand compute the initial number of # s-△’s in O(n2)\ntime. Store this count in a variable c. Consider now an update.\nCase 1: Any update of an edge not adjacent to s\nLet the update be to the edge ( u, v) (either inserting or deleting the edge). Note that\nedge ( u, v) can only participate in the # s-△of three vertices ( s, u, v ). Therefore, we need to\nincrement (resp. decrement) cif and only if ( s, u),(s, v) are both edges in the graph G. This\nrequires O(1) time.\nCase 2: Any update of an edge adjacent to s\nLet the update be to the edge ( s, u). (s, u) participates in the at most n−2 triangles ( s, u, v )\nwhere ( s, v),(u, v)∈E. We can count the number of such v, denoted cuinO(n) time. It suffices\nto increment cbycuif (s, u) is an insertion, and decrement cbycuif (s, u) is a deletion.\nSince the adjacency structure can be updated in O(1) time, any update can be processed in\nO(n) time while any query can be answered in O(1) time by returning c.\nNext, we explore the # s-△problem under our various prediction models, presenting tight\nupper bounds.\nAn Algorithm for Bounded Delay Predictions with Outliers\nWe begin with an upper bound for the # s-△problem taking advantage of predictions with\nbounded delay and outliers. This immediately implies an algorithm given predictions with\nbounded delay.\nTheorem 8.5. LetTbe polynomial in n. Let ˆρ= (ˆρ1, . . . , ˆρT)be ad-delayed prediction with k\noutliers for the #s-△problem (Definition 18).\nThere is an algorithm solving the #s-△problem given ˆρwith polynomial preprocessing time\nP(n), update time U(n) =O(d+k), and query time Q(n) =O(1).\nThere is an algorithm solving the #s-△problem given ˆρwith polynomial preprocessing time\nP(n), update time U(n) =O(1), and query time Q(n) =O((d+k)2).\nObserve that if the number of outliers is small (i.e. k=O(d)), then we have a query-\noptimized algorithm that has U(n) =O(d) and Q(n) =O(1) and a update-optimized algorithm\nthat has U(n) =O(1) and Q(n) =O(d2). We now introduce a definition that will be useful for\nthe # s-△problem.\n35\n\nDefinition 19. LetGbe a dynamic graph with update sequence ρ. For a given vertex u∈V\nand timestep τ, thesensitivity of vertex uat time τwith respect to sequence ρ, is\ns#s-△(u, τ, ρ ) :=\f\f\f{v∈V\\ {s, u}:{u, v},{v, s} ∈Eτ(ρ)}\f\f\f.\nThat is, if the edge {s, u}is flipped in the τ-th update, then the number of #s-△’s in graph\nGchanges by the value s#s-△(u, τ, ρ ). For convenience, the sensitivity may be denoted s(u, τ, ρ )\nors(u, τ)when the update sequence ρis clear.\nThe Query-Optimized Algorithm\nIn the query-optimized algorithm, we maintain the following data structures:\n1.cthe count of # s-△in the current graph G. We answer each query in O(1) time by\nreturning c.\n2. The current graph G= (V, E t) where Et=Et(ρ).\n3. The predicted graph ˆG= (V, E t(ˆρ)).\n4.Dt=ρ≤t∆ˆρ≤t=Et(ρ)∆Et(ˆρ) the symmetric difference of the predicted and actual update\nsequence (including multiplicity), or alternatively the symmetric difference of the edge sets\nEt(ρ), Et(ˆρ). Observe that the second equality follows from the fact that the initial graph\nG0is known.\n5.VDt={v∈Vs.t.vis incident to some edge in Dt}the set of vertices containing all\nendpoints of edges in Dt.\n6. The predicted sensitivities s(v, t,ˆρ) for all vertices vand time steps t. For each vertex\nv, this is stored as a sequence of tuples S(v) = ( t, m) where for each twith s(v, t,ˆρ)̸=\ns(v, t−1,ˆρ), there is an entry ( t, s(v, t,ˆρ)). The list S(v) is stored in increasing order of\nt. For a given v∈V, t∈[T], we can easily compute s(v, t,ˆρ) by searching for the largest\nkeyt0such that t0≤tinS(v) in˜O(1) time.\nFirst, we bound the size of the above data structures.\nLemma 8.6. At any time step t,|Dt|,|VDt|=O(d+k). The predicted sensitivities require\nspace O(nT).\nProof. The bound on Dtfollows as a consequence of Lemma 8.2. Since each edge has two\nendpoints, we can bound the size of VDtbyO(d+k).\nTo bound the size of each sequence of predicted sensitivities, it suffices to note that there\narenvertices each with at most Tdistinct time steps in which the sensitivity may change.\nNext, we claim that we can efficiently update each data structure.\nLemma 8.7. LetDtandVDtdenote the data structures D, V Dat time step trespectively. Then,\nDt, VDtcan be updated from Dt−1, VDt−1in˜O(1)time.\nProof. Letebe the given update and ˆ ebe the predicted update. If e= ˆe, then D(and therefore\nVD) require no modification, so we may assume e̸= ˆe. Clearly Dt, Dt−1differ by at most 2\nelements (specifically e,ˆe) and VDt, VDt−1differ by at most 4 elements (their endpoints). We\ncan easily update these data structures in ˜O(1) time.\n36\n\nWe are now ready to present the query-optimized algorithm.\nAlgorithm 1 UPreprocess #s-△(G0,ˆρ)\nInput: Initial graph G0andddelayed predictions ˆ ρwith koutliers\n1Compute initial sensitivities s(u,0)←#{v /∈ {s, u}s.t. ( u, v),(v, s)∈E0}for all u̸=s\n2Initialize S(v)← {(0, s(v,0))}for all v∈Vandv̸=s.\n3Compute c←#s-△(G0)\n4Initialize D← ∅,VD← ∅\n5fort= 1toTdo\n6 ifˆρt= (s, u)then\n7 forv̸=uand(u, v)∈Et(ˆρ)do\n8 S(v)←S(v)◦(t, s(v, t−1,ˆρ) +e) where e=(\n+1 if ( s, u) inserted\n−1 o/w\n9 ifˆρt= (u, v)then\n10 if(s, v)∈Et(ˆρ)then\n11 S(u)←S(u)◦(t, s(u, t−1,ˆρ) +e) where e=(\n+1 if ( u, v) inserted\n−1 o/w\n12 if(s, u)∈Et(ˆρ)then\n13 S(v)←S(v)◦(t, s(v, t−1,ˆρ) +e) where e=(\n+1 if ( u, v) inserted\n−1 o/w\nAlgorithm 2 UUpdate #s-△(Gt−1,(ut, vt),ˆρ, ρ≤t)\nInput: Current graph Gt−1, current update ( ut, vt),d-delayed predictions ˆ ρwith koutliers,\nand update history ρ<t.\n14Update G(resp. ˆG) by flipping edge ( ut, vt) (resp. ˆ ρt)\n15Update D←ρ≤t∆ˆρ≤tandVDaccording to Lemma 8.7\n16ifs /∈(ut, vt)then\n17 return c←c+ewhere e=\n\n+1 ( s, ut),(s, vt),(ut, vt)∈Et\n−1 (s, ut),(s, vt)∈Et,(ut, vt)/∈Et\n0 o/w\n18if{s, x}={ut, vt}then\n19 serr(x)←0\n20 forv∈VD\\ {s, x}do\n21 serr(x)←serr(x) +ewhere e=\n\n+1 ( x, v),(s, v)∈Etand ( x, v),(s, v)/∈Et(ˆρ)\n−1 (x, v),(s, v)∈Et(ˆρ) and ( x, v),(s, v)/∈Et\n0 o/w\n22 return c←c+b(s(x, t,ˆρ) + s err(x)) where b= +1 if ( s, x) inserted and −1 otherwise.\nGiven a query, we return cinO(1) time.\nWe now provide the proof for the query-optimized algorithm.\nUpdate Correctness\nProof. In each query, we return cin constant time. It therefore suffices to show that after each\nrequest, ccontains the correct value # s-△(Gt). We then proceed by induction on t. Clearly in\nthe base case c= #s-△(G0) after Algorithm 1.\n37\n\nWe now prove the inductive case. As in Theorem 8.4, if the update edge ( ut, vt) does not\ninclude s, we increment cif the triangle ( s, ut, vt) is added, decrement cif (s, ut, vt) is deleted,\nand leave cunchanged otherwise. If the edge update is of the form ( s, x), it suffices to show\nserr(x) =s(x, t, ρ )−s(x, t,ˆρ). Recall that s(x, t, ρ ) is the number of vertices v /∈ {s, x}such\nthat ( s, v),(v, x)∈Et(ρ). Thus, unless v∈VD, the 2 edge path ( s, v, x ) cannot exist in only\none of Et(ρ) and Et(ˆρ). Therefore, it suffices to check only vertices v∈VD\\ {s, x}. For each\nsuch vertex, we increment (resp. decrement) s err(x) if the path ( s, v, x ) exists in Et(ρ) but not\nEt(ˆρ) (resp. Et(ˆρ) but not Et(ρ)).\nUpdate Time\nProof. The preprocessing algorithm computes S(v) for all v∈VinO(nT) time. Initializing c\nrequires O(n2) time (Theorem 8.4) and initializing D, V Drequires O(1) time. Overall, this is\nO(nT+n2) = poly( n) time.\nThe update algorithm updates G,ˆG, D, V Din˜O(1) time. If s /∈(ut, vt),cis updated and\nreturned in O(1) time. Otherwise, s err(x) is computed in O(d+k) time and cis again updated\nand returned in O(1) time.\nThe query algorithm of returning crequires O(1) time.\nIn the update-optimized algorithm, we maintain the following data structures:\n1.{ˆct}t∈[T]where ˆ ct= #s-△(V, E t(ˆρ)) where ˆ ctis the number of s-triangles in the predicted\ngraph Et(ˆρ).\n2. The current graph G= (V, E t) where Et=Et(ρ).\n3. The predicted graph ˆG= (V, E t(ˆρ)).\n4. The predicted sensitivities s(v, t,ˆρ) for all vertices vand time steps t. We store this in\nthe data structure S(v) as in the query-optimized algorithm.\nAlgorithm 3 QPreprocessing #s-△(G0,ˆρ)\nInput: Initial graph G,d-delayed predictions ˆ ρwith koutliers.\n23Compute initial sensitivities s(u,0)←#{v /∈ {s, u}s.t. ( u, v),(v, s)∈E0}for all u̸=s\n24Initialize S(v)← {(0, s(v,0))}for all v∈Vandv̸=s.\n25Compute ˆ c0←#s-△(G0).fort= 1toTdo\n26 Compute ˆ ct←#s-△(V, E t(ˆρ)) using O(n) update algorithm of Theorem 8.4\n27 ifˆρt= (s, u)then\n28 forv̸=uand(u, v)∈Et(ˆρ)do\n29 S(v)←S(v)◦(t, s(v, t−1,ˆρ) +e) where e=(\n+1 if ( s, u) inserted\n−1 o/w\n30 ifˆρt= (u, v)then\n31 if(s, v)∈Et(ˆρ)then\n32 S(u)←S(u)◦(t, s(u, t−1,ˆρ) +e) where e=(\n+1 if ( u, v) inserted\n−1 o/w\n33 if(s, u)∈Et(ˆρ)then\n34 S(v)←S(v)◦(t, s(v, t−1,ˆρ) +e) where e=(\n+1 if ( u, v) inserted\n−1 o/w\nGiven an update, we update G←(V, E t) and ˆG←(V, E t(ˆρ)) in O(1) time.\n38\n\nAlgorithm 4 QQuery #s-△(Gt,ˆρ, ρ≤t)\nInput : Current graph Gt,d-delayed predictions ˆ ρtwith koutliers.\nOutput: c= #s-△(Gt)\n35Initialize c err←0\n36LetVD\\ {s}={v1, v2, . . . , v m}form=O(d+k)\n37forviwith 1≤i≤mdo\n38 Initialize s err(vi)←0\n39 forvjwith i̸=jdo\n40 cerr←cerr+ewhere e=\n\n+1\n2(s, vj),(vj, vi),(vi, s)∈Etand ( s, vj),(vj, vi),(vi, s)/∈Et(ˆρ)\n−1\n2(s, vj),(vj, vi),(vi, s)∈Et(ˆρ) and ( s, vj),(vj, vi),(vi, s)/∈Et\n0 o/w\n41 serr(vi)←serr(vi) +ewhere e=(\n−1 (s, vj),(vj, vi)∈Et(ˆρ)\n0 o/w\n42 cerr←cerr+b(s(vi, t,ˆρ) + s err(vi)) where b=\n\n+1 ( s, vi)∈Etand ( s, vi)/∈Et(ˆρ)\n−1 (s, vi)∈Et(ˆρ) and ( s, vi)/∈Et\n0 o/w\n43return c←ˆct+ cerr\nThe Update-Optimized Algorithm\nWe now give the proof of the second algorithm, completing the proof of Theorem 8.5.\nQuery Correctness Our algorithm counts all the s-△’s the appear in exactly one of Et, Et(ˆρ).\nConsider an s-△with vertex set {s, a, b}. Ifa, b /∈VD, then {s, a, b}is a triangle in Etif and\nonly if it is a triangle in Et(ˆρ). In Line 40, we count all triangles with a, b∈VD. In Line 42,\nwe count all triangles with a∈VD, b /∈VD, where we adjust the pre-computed sensitivity in\nLine 41 to avoid double counting. We first prove an intermediate lemma showing that Line 41\ncorrectly removes the s-△’s with only one (non- s) vertex in VD.\nLemma 8.8. In Line 42,\ns(vi, t,ˆρ)−serr(vi)\nis the number of vertices u /∈VD∪ {s, vi}such that the two edge path (s, u, v i)∈Et. This is\nalso the number of vertices u /∈VDsuch that the two edge path (s, u, v i)is in Et(ˆρ).\nProof. First, since u /∈VD, the edge set incident to uis identical in Et, Et(ˆρ). Thus, the number\nof vertices u /∈VDsuch that ( s, u, v i) is a path is the same regardless of the choice of edge set\nEt, Et(ˆρ).\nRecall that s(vi, t,ˆρ) is the number of vertices u /∈ {s, vi}such that the path ( s, u, v i)\nis in Et(ˆρ). In Line 42, s err(vi) =−mwhere mis the number of vertices vj∈VDsuch\nthat ( s, vj, vi)∈Et(ˆρ). In particular, s(vi, t,ˆρ)−serr(vi) is exactly the number of vertices\nu /∈VD∪ {s, vi}such that ( s, u, v i) is in Et(ˆρ), proving the claim.\nWe now prove the correctness of the algorithm.\nProof. It suffices to show c err= #s-△(Et)−#s-△(Et(ˆρ)). By analyzing Algorithm 4, we see\nthat in Line 40, c erraccumulates +1 for every triangle ( s, a, b ) inEtand not in Et(ˆρ) and −1\nfor every triangle in Et(ˆρ) and not Etifa, b∈VD. Since we iterate over both orderings of pairs\n(i, j), we accumulate1\n2in each iteration.\nIn Line 42 c errincreases (resp. decreases) by ( s(vi, t,ˆρ)−serr(vi)) if ( s, vi)∈Et\\Et(ˆρ) (resp.\n(s, vi)∈Et(ˆρ)\\Et). By Lemma 8.8, this is the number of two edge paths ( s, u, v i) inEtand\n39\n\nEt(ˆρ) with u /∈VD. Therefore, if ( s, vi)∈Et\\Et(ˆρ) (resp. ( s, vi)∈Et(ˆρ)\\Et), this is precisely\nthe number of s-△’s with vertex set {s, u, v i}with u /∈VDinEtbut not Et(ˆρ) (resp. Et(ˆρ) but\nnotEt). Otherwise, if ( s, vi) is in both or none of Et, Et(ˆρ), the number of s-△’s with vertex\nset{s, u, v i}with u /∈VDis the same in both Et, Et(ˆρ).\nFinally, if a, b /∈VD, then {s, a, b}is a triangle in Etif and only if it is a triangle Et(ˆρ) so\nthat the number of s-△’s in Et, Et(ˆρ) with vertex set {s, a, b}is identical when a, b /∈VD.\nWe have thus shown that c err= #s-△(Et)−#s-△(Et(ˆρ)), concluding the proof.\nQuery Time\nProof. Algorithm 3 requires O(n2+nT) time, following similar arguments to Theorem 8.4 and\nthe analysis of Algorithm 1.\nWe now examine Algorithm 4. Within each loop, updating c err,serrrequires O(1) time by\nexamining the adjacency structures of G,ˆG. From Lemma 8.6, we have |VD|=O(d+k) so that\nAlgorithm 4 requires time O((d+k)2).\nWe have shown that given ddelayed predictions with koutliers for d+k=O(n1−ε), it is\npossible to design an algorithm that beats the conditional lower bounds for algorithms without\npredictions. However, since the OMv Conjecture holds against any algorithm with polynomial\npreprocessing time, we can therefore conclude it is hard to make good predictions for the # s-△\nproblem. Using Theorem 8.5 we claim that no polynomial time prediction algorithm can yield\ntruly sub-linear delay.\nProposition 8.9. Under the OuMv conjecture, no polynomial time algorithm can output ˆρthat\nis addelayed prediction with koutliers for the #s-△problem if d+k=O(n1−ε)forε >0.\nProof. Suppose such a polynomial time algorithm exists. Then, we run this algorithm in the\npreprocessing step. Using Theorem 8.5, we have an algorithm with polynomial preprocessing\ntime, update time U(n) =O(d+k), and query time Q(n) =O(1) , contradicting the OuMv\nconjecture.\n8.3 Subgraph Connectivity\nRecall that in subgraph connectivity problem, the algorithm is given a fixed graph Gand a\nsubset of vertices S. Each update adds or removes a vertex from the set Sand each query\nasks for some pair of vertices u, v∈S(in variants of the problem one of both of u, vcan be a\nfixed vertex) whether u, vare connected on the subgraph G[S] induced by S. We give an upper\nbound for the subgraph connectivity problem using predictions with bounded delay.\nFrom Delay Aware to Delay Agnostic Algorithms. Generally, we do not expect an\nalgorithm to be aware of the quality that the prediction will have for the online request sequence.\nIndeed, consistency and robustness should both hold without the algorithm being given the\nquality of its prediction in addition to the prediction itself. However, in the setting of dynamic\nalgorithms Lemmas 8.2 and 8.3 imply that with no extra cost, an algorithm can compute (up\nto a constant factor) the quality of the predictions it has seen so far. Using this observation,\nwe can design algorithms that are not only given a prediction ˆ ρ, but a guarantee dthat ˆ ρ\nisddelayed. Then, taking in the current guess for the delay parameter, we can choose an\nappropriate parameter for the delay-aware algorithm to handle the request of the current time\nstep. We exhibit this transformation for the subgraph connectivity problem.\n40\n\nWarmup: Update-Optimized Algorithm with parameter dknown\nWe begin with an algorithm that is not only given a ddelayed prediction ˆ ρ, but dsuch that ˆ ρis\nguaranteed to be at most ddelayed. In Section 8.3.1, we give a transformation to a dagnostic\nalgorithm.\nLemma 8.10. LetTby polynomial in n. Let ˆρ= (ˆρ1,ˆρ2, . . . , ˆρT)be ad-delayed prediction for\nthe subgraph connectivity problem.\nThere is an algorithm solving the subgraph connectivity problem given dandˆρwith polyno-\nmial preprocessing time P(n), update time U(n) =˜O(1)and query time Q(n) =O(d2).\nWe show that there exists an algorithm with polynomial preprocessing time that allows the\nupdate and query time to overcome the conditional lower bound imposed on purely dynamic\nalgorithms.\nWe begin with some useful definitions. Let Tbe an integer and ρ= (ρ1, ρ2, . . . , ρ T) be a\nsequence of requests on some graph Gwith initial vertex set S0. For any time t, letStdenote\nthe set of vertices in Safter the first trequests. Let 1 ≤t1< t2≤Tbe two time steps. Consider\nthe interval [ t1, t2]⊂[T]. A node vispermanent in ρfrom t1tot2ifv∈Stfor all t∈[t1, t2]\nandvis not part of any query between time steps t1andt2. A node visactive in ρfrom\nt1tot2if any request ρtchanges the membership of vinStor queries vfort1≤t≤t2. Note\nthat for any t∈[t1, t2], every vertex in St(which includes every queried vertex) must either be\npermanent or active.\nWe will also require the dynamic connectivity algorithm of Kapron, King, and Mountjoy,\ncomputing all pairs connectivity with worst case polylogarithmic update and query time.\nTheorem 8.11 ([KKM13]) .There is an algorithm supporting the following operations in ˜O(1)\ntime:\n1.UPDATE (u, v): Insert or remove edge (u, v)\n2.QUERY (a, b): Answer if a, bare connected in the graph G\nThe algorithm is always correct if a, bare connected and correct with high probability when a, b\nare not connected.\nNote that applying the above algorithm ntimes given a vertex insertion/deletion gives a\nconditionally optimal algorithm for the pure online case, as discussed in [HKNS15].\nIn both algorithms, we maintain the following data structures.\n1.St, the current set of vertices in S.\n2. For all t∈Tandu, v∈At, letC(u, v, t ) denote whether u, vare connected in the graph\nˆH(u, v, t ) =G[Pt∪ {u, v}], the subgraph induced by Pt∪ {u, v}, where Ptis the set of\npermanent vertices in ˆ ρfrom t−dtot+dandAtis the set of active vertices in ˆ ρfrom\nt−dtot+d. This is precomputed in the preprocessing phase and only queried in the\ndynamic phase.\n3.Qt=St\\Pt, the set of vertices in Stthat are not permanent in ˆ ρfrom t−dtot+d. To\nmaintain Qtefficiently, note that Ptcan change by at most 1 vertex with each update and\n2 vertices with each query. In particular, a vertex is added to Ptwhen it is inserted into S\nat time step t−dand will remain in Swithout being queried until time step t+d, while\na vertex is removed from Ptonly when it is removed from Sor queried at time step t+d.\nWe can therefore maintain Ptas a sequence of insertions and deletions and maintain Qt\ninO(1) time.\n41\n\nThe set Qtrepresents the possibly unexpected vertices in the set Stdue to the error of the\nprediction. We claim that the size of this set of vertices depends on the quality of the prediction.\nLemma 8.12. Suppose ˆρis ad-delayed prediction. For t∈[T], letPtbe the set of permanent\nvertices in ˆρfrom t−dtot+d. Let Stbe the set of vertices in Safter the first ttrue request\nin the sequence ρ. Then, St=Pt∪Qtwhere |Qt| ≤4d+ 2.\nFurthermore, in each time step St, Qt, Pteach change by at most three vertices.\nProof. First, we claim Pt⊂St. Let v∈Pt. Consider the true update sequence ( ρ1, . . . , ρ t).\nSince ˆ ρisd-delayed, we have\n(ˆρ1, . . . , ˆρt−d)⊂(ρ1, . . . , ρ t)⊂(ˆρ1, . . . , ˆρt+d).\nSince v∈Pt,v∈Safter ˆ ρ≤t−dand is not modified in the sub-sequence ˆ ρ[t−d,t+d], so that v∈S\nafter ρ≤tandv∈St.\nNext, we show that |Qt| ≤5d. Consider a vertex w∈St. Ifw /∈Pt, then there is some update\nin ˆρ[t−d,t+d]that inserts wintoSt, or some query involving win ˆρ[t−d,t+d]. Note that there are\nat most 2 d+ 1 such requests, and each can include at most 2 vertices, so that |Qt| ≤4d+ 2.\nBy definition, Stchanges by one vertex in each time step. Above we have argued that Pt\nchanges by at most 2 vertices in each time step. Combining, Qtchanges by at most 3 vertices\nin each time step, and so can be maintained in ˜O(1) time.\nIn the update-optimized algorithm, given a query pair ( u, v), we will query for connectivity\nin the graph with vertex set Qt∪ {u, v}, with edges encoding pairs of vertices with a path\nbetween them. Since the size of this graph is small, we are able to compute connectivity queries\nefficiently.\nLemma 8.13. Lett∈[T]be a time step. Let u, vbe two vertices in St=Pt∪Qt. Let Ht\ndenote the graph with vertex set Qt∪ {u, v}and(a, b)∈E(Ht)if either (a, b)∈E(Gt)is an\nedge in the original graph, or C(a, b, t ) = 1 , that is there is a path from a, bwith all internal\nvertices in Pt. Then, u, vare connected in G[St]if and only if u, vare connected in Ht.\nProof. Suppose u, vare connected in Ht. It suffices to show that every edge in Hthas endpoints\nthat are connected in G[St]. An edge in Htis either an edge in the original graph or a path\nwith all internal vertices in Pt. In the former case, note that every vertex in Htis inSt, so that\nthe edge is present in G[St]. In the latter case, again note that all vertices in the path are in\nSt, and every edge is in the original graph.\nSuppose u, vare connected in G[St]. Consider one path between u, vand consider the\nsubsequence of vertices on this path consisting of vertices in Qt∪ {u, v}. This path exists in\nHtas any vertex not in this subsequence is necessarily in Pt, a case which is covered by the\nadditional edges inserted.\nTherefore, to check connectivity in G[St], it suffices to check connectivity in the smaller\ngraph Ht. We now present our algorithm with ˜O(1) update time and O(d2) query time.\nAlgorithm 5 PromisePreprocessingSubConn (G, S 0,ˆρ, d)\nInput: Fixed graph Gwith initial vertex set S0.d-delayed predictions ˆ ρ.\n44fort= 1toTdo\n45 Compute Pt={vs.t.vis permanent in ˆ ρfrom max(1 , t−d) to min( T, t+d)}\n46 Compute At={vs.t.vis active in ˆ ρfrom max(1 , t−d) to min( T, t+d)}\n47 LetˆHtbe the subgraph G[Pt] induced by Pt\n48 foru, v∈Atdo\n49 Run DFS on ˆH(u, v, t )←G[Pt∪ {u, v}] and store in C(u, v, t ) a 1 if u, vare connected\ninˆH(u, v, t ) and 0 otherwise.\n50return {Pt}t,{At}t,{C(u, v, t )}u,v,t\n42\n\nWe preprocess the input as described by Algorithm 5. Given an update, we maintain the\nsetsSt, Qtin˜O(1) time. To maintain Qtgiven St, note that the sets of permanent vertices {Pt}\ncan be maintained as a sequence of insertions and deletions, and we can process these insertions\nand deletions in ˜O(1) time. In the query step, we will construct the graph Htas described in\nLemma 8.13 and compute connectivity in this graph.\nAlgorithm 6 PromiseQuerySubConn (G, S t,ˆρ, ρ≤t, d)\nInput : Fixed graph Gwith current vertex set St,d-delayed predictions ˆ ρ.\nOutput: YES if u, vare connected in G[St], NO otherwise\n51Construct Htwith vertex set Qt∪{u, v}and edge set G[Qt∪{u, v}] and additional edges ( a, b)\nwhere C(a, b, t ) = 1.\n52Run DFS from uinHtand return YES u, vare connected and NO otherwise.\nCorrectness follows immediately from Lemma 8.13. We now conclude the proof by analyzing\nthe time complexity of our algorithm. In the preprocessing step, for each t, we can compute\nPt, At∈O(d) =O(n) time, and construct ˆHtinO(d2) =O(n2) time. Using Theorem 8.11, we\ncan maintain ˆHtwith O(n) updates and compute C(u, v, t ) with O(d2n) updates and O(d2) =\nO(n2) queries. Overall, this requires ˜O(Tn3) time in the preprocessing phase. For each query,\nwe construct a graph on O(d) vertices and compute a DFS in O(d2) time. Since Qt⊂At, the\nadditional edges can each be added in constant time.\n8.3.1 Generalization to dAgnostic Algorithm\nWe now design an algorithm that is not given din the preprocessing phase.\nFor each d= 2d′where 1 ≤d′≤ ⌈logn⌉, we maintain the set St\\Pt,dwhere Pt,ddenotes\nthe set of permanent vertices in ˆ ρfrom t−dtot+d. As before, this set can be maintained in\n˜O(1) time. Since there are O(logn) such sets, we can complete an update in ˜O(1) time.\nWe also need to check if Pt,d⊆St. We claim that this can be maintained for all din˜O(1)\ntime. Recall that Pt,dchanges by at most 2 vertices in every time step (and these vertices must\nbe involved at the t+drequest) and Stchanges by at most 1 vertex in every time step. Then,\nat each time step, we can simply query for membership in Stfor any vertex added into Pt,dand\nquery for membership in Pt,dfor any vertex removed from St, requiring only ˜O(1) time. Again,\nfor all O(logn) values of d, this requires ˜O(1) time.\nFordsatisfying Pt,d⊆St, let Qt,d=St\\Pt,dand let d∗be the smallest d∗such that\n|Qt,d| ≤4d+ 2. If d∗does not exist, then ˆ ρis not a d-delayed prediction for d≤nand we can\nafford to answer the query using the full graph G[St] inO(n2) time. Otherwise, ˆ ρisd∗delayed\nbut notd∗\n2delayed. If ˆ ρwhered∗\n2delayed, then |Qt,d∗/2| ≤2d∗+ 2 by Lemma 8.12. Since ˆ ρis\nd∗delayed, we can construct Hton the vertex set Qt,d∗of size O(d∗) and follow the algorithm\nof Lemma 8.10. We now state the final theorem, providing the algorithm and proof.\nTheorem 8.14. LetTby polynomial in n. Let ˆρ= (ˆρ1,ˆρ2, . . . , ˆρT)be a d-delayed prediction\nfor the subgraph connectivity problem.\nThere is an algorithm solving the subgraph connectivity problem given ˆρwith polynomial\npreprocessing time P(n), update time U(n) =˜O(1)and query time Q(n) =O(d2).\nAlgorithm 7 PreprocessingSubConn (G, S 0,ˆρ)\nInput: Fixed graph Gwith initial vertex set S0.d-delayed predictions ˆ ρ.\n53ford= 2d′where d′= 1to⌈logn⌉do\n54 {Pt,d},{At,d}, Cd←PromisePreprocessingSubConn (G, S 0,ˆρ, d)\n43\n\nWe preprocess the input as described by Algorithm 7. Given an update, we maintain the\nsetsSt, Qt,dfor all dwhere Pt,d⊂StinO(logn) time, since there are O(logn) values for dand\nwe can maintain each in constant time. In the query step, we first compute a valid value of d\nand construct the graph Htas before.\nAlgorithm 8 QuerySubConn (G, S t,ˆρ, ρ≤t)\nInput : Fixed graph Gwith current vertex set St,d-delayed predictions ˆ ρ.\nOutput: YES if u, vare connected in G[St], NO otherwise\n55Letd∗←min{ds.t.Pt,d⊂Stand|Q(t, d)| ≤4d+ 2}\n56ifd∗=∞then\n57 Run DFS on G[St] and return YES if u, vare connected and NO otherwise.\n58else\n59 return PromiseQuerySubConn (G, S t,ˆρ, ρ≤t, d∗)\nWe now prove Theorem 8.14.\nProof. First we show correctness. By Lemma 8.12, d∗≤2d. Since Pt,d∗⊆St, we apply Lemma\n8.13 and observe that u, vare connected in Htif and only if u, vare connected in G[St]. When\nd∗=∞, we compute connectivity on the graph G[St] which is trivially correct.\nTo analyze the preprocessing time, we note that Algorithm 5 required O(Tn3) time. Since\nwe run this algorithm for O(logn) values of d, then the preprocessing algorithm required\nO(Tn3logn) time.\nTo analyze Algorithm 8, note that finding d∗requires O(logn) time, as we can iterate over\nthe values of |Q(t, d)|. In fact, d∗can be maintained in the updating step by noting the size of\nQt,dafter the set is updated. Given that d∗≤2d, we construct the graph Htand run DFS in\nO(d2) time, as desired. If d∗=∞, then we have d= Ω( n), and we can run DFS on the full\ngraph G[St] inO(m) =O(n2) =O(d2) time.\n8.4 Transitive Closure\nFollowing a similar approach as the subgraph connectivity problem, we obtain a (conditionally)\noptimal algorithm for the all pairs reachability (transitive closure) problem with constant update\ntime and query time O(d2). For simplicity, we again begin with an algorithm that is given the\nprediction delay das additional input, and use a similar transformation to design an algorithm\nwith only prediction ˆ ρas input.\nLetGtdenote the graph at time step twith edge set Et. Given a request sequence ρand time\nsteps t1, t2, an edge ( u, v) ispermanent in ρfrom t1tot2if (u, v)∈Etfor all t∈[t1, t2]. A\nvertex uisactive in ρfrom t1tot2ifuis incident to any edge update in the request sequence\nρ[t1,t2]or if uis part of any query in the request sequence ρ[t1,t2]. We maintain the following\ndata structures at each time step t.\n1.Gt, the current graph G. This is maintained in O(1) time by updating one bit in the\nadjacency matrix.\n2. For all t∈Tandu, v∈At, let C(u, v, t ) denote vis reachable from uin the graph\nˆH(t) = (V, P t), where Atis the set of active vertices in ˆ ρfrom t−dtot+dandPtis the\nset of permanent edges in ˆ ρfrom t−dtot+d. This is precomputed in the preprocessing\nphase and only queried in the dynamic phase.\n3.Ft=Et\\Pt, the set of edges that are not permanent in ˆ ρfrom t−dtot+d. This can\nbe maintained in ˜O(1) time as Etchanges only by the edge specified in ρt, while Ptcan\nadd one edge from the update ˆ ρt−dand lose one edge from the update ˆ ρt+d. Furthermore,\n44\n\nPtcan in fact be computed in the preprocessing phase, so that maintaining Ftcan be\naccomplished by maintaining Etand checking which O(1) edges are inserted and removed\nfrom Pt.\nIn our algorithm, we will maintain the edge set Ft. Let V(Ft) denote all vertices incident\nto at least one edge in Ft. On a given query ( u, v), we construct the graph Htwith vertex set\nV(Ft)∪{u, v}with all edges in the induced subgraph augmented by an edge for every pair ( a, b)\nfor which bis reachable from ausing only edges in Pt. On this graph we run DFS to check\nfor reachability. To guarantee the performance of the algorithm, we require that Ftis not too\nlarge.\nLemma 8.15. Pt⊂Etand|Ft| ≤2d+ 1\nProof. By Lemma 8.1, if ˆ ρis ad-delayed prediction, then for all t, ˆρ≤t−d⊂ρ≤t⊂ˆρ≤t+d. If\ne∈Pt, then e∈Et′(ˆρ) for all t′∈[t−d, t+d] so that no update in ˆ ρ[t−d,t+d]flips the edge e.\nTherefore, e∈Et.\nFor the second claim, we observe that Ft⊂Atand|At| ≤2d+ 1 since there are 2 d+ 1\nupdates between t−dandt+dand each update can involve only one edge.\nTo guarantee correctness, we require that the graph Htaccurately encodes reachability\nrelationships between vertices.\nLemma 8.16. Letu, v∈V(Ht). Then, vis reachable from uinHtif and only if vis reachable\nfrom uinGt.\nProof. Suppose vis reachable from uinHtwith the path ( u, w 1, w2, . . . , w l−1, v). Each edge\nis either an edge in the induced subgraph and therefore an edge in Gtor an auxiliary edge\n(wi, wi+1) added for wi+1reachable from wiin using edges in Pt⊂Et. Therefore, there is a\npath from utovinGtas well.\nSuppose vis reachable from uinGtwith a path ( u, w 1, w2, . . . , w l−1, v). Consider the\nsubsequence {wij}of vertices in V(Ht), noting that u, vare in this subsequence. It suffices to\nshow that for all j, the edge ( wij, wij+1) is in Ht. Suppose not, then ij+1> ij+1, otherwise the\nedge is in the induced subgraph. Consider the subpath from wijtowij+1. Every edge is in Et\nbut not the induced subgraph on V(Ft)∪{u, v}, as at least one endpoint is not in V(Ft)∪{u, v}.\nThen, since each edge is not in Ft, this edge must be in Pt, so that C(wij, wij+1, t) is true and\nthe edge exists in Ht.\nEquipped with these tools, we state our update-optimized algorithm and the resulting the-\norem.\nAlgorithm 9 PromisePreprocessingTC (G0,ˆρ, d)\nInput: Initial graph G0.d-delayed predictions ˆ ρ.\n60fort= 1toTdo\n61 Compute Pt={es.t.eis permanent in ˆ ρfrom max(1 , t−d) to min( T, t+d)}\n62 Compute At={vs.t.vis active in ˆ ρfrom max(1 , t−d) to min( T, t+d)}\n63 LetˆHt= (V, P t) be the subgraph with edge set Pt.\n64 foru, v∈Atdo\n65 Run DFS on ˆHt←(V, P t) and store in C(u, v, t ) a 1 if vis reachable from uinˆHtand\n0 otherwise.\nWe preprocess the input as described by Algorithm 9. Given an update, we maintain Gt, Ft\ninO(1) time, as discussed above. In the query step, we will construct the graph Htand compute\nconnectivity in this graph.\n45\n\nAlgorithm 10 PromiseQueryTC (Gt,ˆρ, ρ≤t, d)\nInput : Current graph Gt,d-delayed predictions ˆ ρ.\nOutput: YES if vis reachable from uinGt, NO otherwise\n66Construct Htwith vertex set V(Ft)∪{u, v}and edge set G[V(Ft)∪{u, v}] and additional edges\n(a, b) where C(a, b, t ) = 1.\n67Run DFS from uinHtand return YES if vis reachable from uand NO otherwise.\nLemma 8.17. LetTby polynomial in n. Let ˆρ= (ˆρ1,ˆρ2, . . . , ˆρT)be ad-delayed prediction for\nthe transitive closure problem.\nThere is an algorithm solving the transitive closure problem given ˆρanddwith polynomial\npreprocessing time P(n), update time U(n) =˜O(1)and query time Q(n) =O(d2).\nProof. Correctness of the algorithm follows from Lemma 8.16. To examine the running time,\nin the preprocessing step we can compute At, Ptin˜O(T) time for all t∈[T] by computing the\nchange from Pt−1(resp. At−1) toPt(resp. At) in ˜O(1) time. Computing reachability from\n|At|=O(d) sources requires O(n2d) time for each t, so that Algorithm 9 requires time O(Tn2d).\nAlgorithm 10 constructs a graph on O(d) vertices (Lemma 8.15) and runs DFS, which\nrequires at most O(d2) time.\nWe use a similar transformation as in the subgraph connectivity problem to obtain an\nalgorithm that does not require das input. In the preprocessing algorithm, we run Algorithm\n9 for O(logn) values of d= 2d′where 1 ≤d′≤ ⌈logn⌉, saving the data structures {Pt,d}{At,d}\nand{Cd(u, v, t )}. In the update step, we maintain in ˜O(1) time the dynamic edge set Etas\nwell as the sets Ft,d=Et\\Pt,dfor all dsuch that Pt,d⊂Et. In each update, we also maintain\nthe minimum d∗such that Pt,d⊂Etand|Ft|=|Et\\Pt,d| ≤2d+ 1. Given a query, we return\nPromiseQueryTC (Gt,ˆρ, ρ≤t,d∗), which is correct by Lemma 8.16 and Pt,d∗⊂Et.\nFurthermore, we run Algorithm 9 O(logn) times in the preprocessing phase, so that the\npreprocessing time remains polynomial. By Lemma 8.15, d∗≤2dwhere dis the delay of the\nprediction ˆ ρ, so that each query requires O(d2) time. If d∗=∞, then d= Ω( n) and we can\nrun DFS on the full graph GtinO(m) =O(n2) =O(d2) time. The above discussion yields the\nfollowing theorem.\nTheorem 8.18. LetTby polynomial in n. Let ˆρ= (ˆρ1,ˆρ2, . . . , ˆρT)be a d-delayed prediction\nfor the transitive closure problem.\nThere is an algorithm solving the transitive closure problem given ˆρwith polynomial prepro-\ncessing time P(n), update time U(n) =˜O(1)and query time Q(n) =O(d2).\n8.5 Shortest Paths\nWith a slightly more careful analysis, we can obtain a similar result for all pairs shortest\npaths. Keeping the same definitions as with transitive closure, we maintain the following data\nstructures at each time step t.\n1.Gt, the current graph Gmaintained in O(1) time.\n2. For all t∈Tandu, v∈At, let D(u, v, t ) denote the distance from utovin the graph\nˆHt= (V, P t).\n3.Ft=Et\\Pt, the set of edges in Gtthat are not permanent in ˆ ρfrom t−dtot+d,\nmaintained in O(1) time.\nOur algorithm again maintains the set Ft. By an identical argument to Lemma 8.15, we\nhave Et=Pt∪Ftwhere |Ft| ≤2d+ 1. At each query ( u, v), construct the graph Htwith vertex\n46\n\nsetV(Ft)∪ {u, v}and edge set consisting of the induced subgraph Gt[V(Ft)∪ {u, v}] and edge\n(a, b) with weight D(a, b, t ). If ( a, b) is already an edge in the induced subgraph, set the weight\nof (a, b) to be the minimum of the edge weight and the distance D(a, b, t ). We now require the\nfollowing lemma for correctness.\nLemma 8.19. Lettbe a time step. Let (u, v)be the query at ρt. Let Htbe the graph with\nvertex set V(Ht) =V(Ft)∪ {u, v}and edge set G[V(Ht)]and edge (a, b)with weight D(a, b, t )\nfor all pairs of vertices a, bwhere D(a, b, t )<∞is finite.\nThen, δG(u, v) =δHt(u, v).\nProof. First, we argue that δGt(u, v)≤δHt(u, v). This follows as every edge in Htis either\nan edge in Gtor an edge with weight equivalent to a path in Gt. Then, it suffices to verify\nthat δHt(u, v)≤δGt(u, v). Consider a shortest path P(u, v) = ( u, w 1, . . . , w l−1, v) inGt. We\ntake the subsequence PH= (wij) of vertices wij∈V(Ht). It suffices to show that PHis a\npath in Htwith weight at most δGt(u, v), the total weight of path P(u, v). We consider two\ncases. If ij+1=ij+ 1 then the edge ( wij, wij+1)∈E(Ht) as an edge in the induced subgraph.\nOtherwise, the intermediate vertices are not in V(Ht) and therefore not in V(Ft). Since at\nleast one endpoint of each edge is not in Ft, all the edges in Etbetween wij, wij+1are in Pt. In\nparticular, wt( wij, wij+1) =D(a, b, t ) is at most the weight of the subpath in Gt, proving the\ndesired statement.\nFollowing a similar approach as transitive closure, we obtain the following theorem.\nLemma 8.20. LetTby polynomial in n. Let ˆρ= (ˆρ1,ˆρ2, . . . , ˆρT)be ad-delayed prediction for\nthe all pairs shortest path problem on weighted, directed graphs.\nThere is an algorithm solving the all pairs shortest path problem on weighted digraphs given\nˆρanddwith polynomial preprocessing time P(n), update time U(n) = ˜O(1)and query time\nQ(n) =O(d2).\nThe algorithm and proof of correctness are given below.\nAlgorithm 11 PromisePreprocessingAPSP (G0,ˆρ, d)\nInput: Initial graph G0.d-delayed predictions ˆ ρ.\n68fort= 1toTdo\n69 Compute Pt={es.t.eis permanent in ˆ ρfrom max(1 , t−d) to min( T, t+d)}\n70 Compute At={vs.t.vis active in ˆ ρfrom max(1 , t−d) to min( T, t+d)}\n71 LetˆHt= (V, P t) be the subgraph with edge set Pt.\n72 foru, v∈Atdo\n73 Run Dijkstra’s on ˆHt←(V, P t) and store in D(u, v, t )←δˆHt(u, v)\nWe preprocess the input as described by Algorithm 11. Given an update, we maintain Gt, Ft\nin˜O(1) time. In the query step, we will construct the graph Htand compute the shortest path\ndistance in this graph.\nAlgorithm 12 PromiseQueryAPSP (Gt,ˆρ, ρ≤t, d)\nInput : Current graph Gt,d-delayed predictions ˆ ρ.\nOutput: Distance from utovinGt.\n74Construct Htwith vertex set V(Ft)∪{u, v}and edge set G[V(Ft)∪{u, v}] and additional edges\n(a, b) with weight D(a, b, t ) where D(a, b, t )<∞is finite.\n75Run Dijkstra’s from uinHtand return δHt(u, v)\n47\n\nProof. Correctness follows from Lemma 8.19. The preprocessing algorithm requires O(Tn2d)\nas each invocation of Dijkstra’s algorithm requires O(n2) time. The query algorithm requires\nO(d2) time as we invoke Dijkstra’s algorithm on a graph with dvertices.\nAgain, we apply the same transformation as with transitive closure and subgraph connec-\ntivity to design an algorithm that requires input ˆ ρonly. In the preprocessing algorithm, we run\nAlgorithm 11 for O(logn) values of d= 2d′where 1 ≤d′≤ ⌈logn⌉, saving the data structures\n{Pt,d}{At,d}and{Dd(u, v, t )}. In the update step, we maintain in O(1) time the dynamic edge\nsetEtas well as the sets Ft,d=Et\\Pt,dfor all dsuch that Pt,d⊂Et. In each update, we also\nmaintain the minimum d∗such that Pt,d⊂Etand|Ft|=|Et\\Pt,d| ≤2d+ 1. Given a query, we\nreturn PromiseQueryAPSP (Gt,ˆρ, ρ≤t,d∗), which is correct by Lemma 8.19 and Pt,d∗⊂Et.\nFurthermore, we run Algorithm 11 O(logn) times in the preprocessing phase, so that the\npreprocessing time remains polynomial. Since |Ft| ≤2d+ 1,d∗≤2dwhere dis the delay of the\nprediction ˆ ρ, so that each query requires O(d2) time. If d∗=∞, then d= Ω( n) and we can run\nDijkstra on the full graph GtinO(m) =O(n2) =O(d2) time. The above discussion yields the\nfollowing theorem.\nTheorem 8.21. LetTby polynomial in n. Let ˆρ= (ˆρ1,ˆρ2, . . . , ˆρT)be a d-delayed prediction\nfor the all pairs shortest path problem on weighted digraphs.\nThere is an algorithm solving the all pairs shortest path problem on weighted digraphs given\nˆρwith polynomial preprocessing time P(n), update time U(n) =˜O(1)and query time Q(n) =\nO(d2).\n8.6 Erickson’s Maximum Value Problem\nNext, we showcase the optimality of our lower bounds for a non-graph problem. Recall that in\nErickson’s Problem, the algorithm is given an initial matrix M0. Each update increments either\na row or a column by 1. Each query asks for the maximum value in the matrix Mt.\nTheorem 8.22. LetTby polynomial in n. Let ˆρ= (ˆρ1,ˆρ2, . . . , ˆρT)be a d-delayed prediction\nfor Erickson’s Problem.\nThere is an algorithm solving Erickson’s problem given ˆρwith polynomial preprocessing time\nP(n), update time U(n) =˜O(d)and query time Q(n) =˜O(1).\nThere is an algorithm solving Erickson’s problem given ˆρwith polynomial preprocessing time\nP(n), update time U(n) =˜O(1)and query time Q(n) =˜O(d2).\nThe trivial algorithm simply maintains the n×ndynamic matrix, processing an update\ninO(n) time and storing the maximum value, which is returned in O(1) time. Alternatively,\nwe can store an increment in O(1) time (e.g. maintaining an array recording the number of\nincrements for each row and column) and construct the matrix, computing the maximum in\nO(n2) time.\nWe now give some useful definitions that will help us design efficient algorithms with pre-\ndiction. Consider an initial matrix M0and request sequence ρ. Let Mtthe current state of the\nmatrix after trequests. For i, j∈[n], let r(ρ, t, i), c(ρ, t, j ) be functions denoting the number of\ntimes row iand column jhave been incremented at time step t, so that value of entry ( i, j) at\ntime step tisMt[i][j] =M0[i][j] +r(ρ, t, i) +c(ρ, t, j ). There are at most drows and dcolumns\nwhere the number of predicted increments does not match the number of actual increments, a\nset of errors which we can maintain efficiently. For a given time step t, letER(t)⊂[n] denote\nthe set of rows where r(ρ, t, i)̸=r(ˆρ, t, i) and EC(t)⊂[n] denote the set of columns where\nc(ρ, t, j )̸=c(ˆρ, t, j ).\nAs with subgraph connectivity, given a predicted request sequence ρand time steps t1, t2, a\nrowiispermanent in ρfrom t1tot2ifiis not incremented in ρ[t1,t2]. A row iisactive in ρ\nfrom t1tot2ifiis incremented in ρ[t1,t2]. Permanent and active columns are defined similarly.\n48\n\nLemma 8.23. Letˆρbe a prediction that is ddelayed with koutliers. For all t∈[T],\n|ER(t)|,|EC(t)|=O(d+k). Given ER(t−1), EC(t−1), the sets ER(t), EC(t)can be maintained\nin˜O(1)time per request. It is possible to also maintain r(ρ, t, i)−r(ˆρ, t, i)for all i∈ER(t)and\nc(ρ, t, j )−c(ˆρ, t, j )for all j∈EC(t).\nProof. The bound on |ER(t)|,|EC(t)|follow Lemma 8.2 as ER(t)∪EC(t)⊂ρ≤t∆ˆρ≤t, the\nsymmetric difference between the request sequences, which was shown to be of size at most\nO(d+k).\nIt remains to show that the sets can be maintained efficiently dynamically. Suppose we have\nr(ρ, t−1, i)−r(ˆρ, t−1, i) for all iandc(ρ, t−1, j)−c(ˆρ, t−1, j) for all j. Given the predicted\nrequest ˆ ρtand true request ρt, we update at most 2 values in ˜O(1) time, leaving the remaining\ndifference values as they remain unchanged. If an updated difference becomes 0, we remove this\nindex from ERorECas appropriate. In an updated difference becomes non-zero, we add this\nindex to ERorECas appropriate. This can be done in ˜O(1) time.\nNow, if we let ˆMtdenote the matrix under the predicted request sequence ˆ ρafter the t-th\ntime step, Mt[i][j] = ˆMt[i][j] ifi /∈ER(t) and j /∈EC(t), so that the maximum entry among\nthese i, jremains the same. For a fixed i∈ER(t) and among all j /∈EC(t), then the relative\ndifference between entries in the i-th row are the same in Mtand ˆMt, even if the absolute values\nare different. Therefore, if we maintain a data structure (for example a heap) that keeps the\npredicted maximum values for the i-th row, then we need to only correct up to O(d) entries\nto find the new maximum of the i-th row. Only if both i, j∈ER(t), EC(t) do we need to\ncompletely recalculate the maximum, but there are at most O(d2) such entries.\nQuery-Optimized ˜O(d+k)Update Algorithm We will in fact show a query-optimized\nalgorithm that requires ddelay with koutlier predictions. By choosing k= 0, this gives the\nquery-optimized algorithm of Theorem 8.22. In our algorithm, we maintain the following data\nstructures:\n1.IR, IC: [n]→N, two size nlist storing how many times each row and column is incre-\nmented. This can be maintained in O(1) time per update.\n2.ER, EC, the error sets described above. For each i, j∈[n], we also maintain r(ρ, t, i)−\nr(ˆρ, t, i) and c(ρ, t, j )−c(ˆρ, t, j ). This can be maintained in ˜O(1) time by Lemma 8.23.\n3. For each row iand time step t, a binary heap HR(i, t) with column indices as keys and\nˆMt[i][j] as the values. For each column jand time step t, a binary heap HC(j, t) with row\nindices as keys ˆMt[i][j] as the values. This is pre-computed in the preprocessing phase.\nDuring the t-th request, there may be some modifications to HR(i, t) and HC(j, t). After\nthet-th request, the heaps can be discarded.\n4. The current maximum value cand indices i∗, j∗such that Mt[i∗][j∗] =c.\nWe now describe our algorithm.\nAlgorithm 13 UPreprocessingErickson (M0,ˆρ)\nInput: Initial Matrix M0.d-delayed with koutliers predictions ˆ ρ.\n76IR[i], IC[j]←0 for all i, j∈[n]\n77ER, EC← ∅\n78c←max i,jM0[i][j] and i∗, j∗such that M0[i∗][j∗] =c.\n79fort= 1toTdo\n80 Compute ˆMtfrom ˆMt−1by incrementing the row or column specified by ˆ yt\n81 Construct binary heaps HR(i, t) for all i∈[n] and HC(j, t) for all j∈[n].\n49\n\nFor the update algorithm, we assume without loss of generality that the request ρtincrements\nrowi. An analogous algorithm exists if ρtincrements column j. Let itdenote the row index\nincremented at time t.\nAlgorithm 14 UUpdateErickson (Mt, it,ˆρ, ρ<t)\nInput : Current matrix Mt, update it,ddelayed predictions ˆ ρ, and request history ρ≤t\n82Update IR[it]←IR[it] + 1\n83Update ERaccording to Lemma 8.23\n84ifit=i∗then\n85 c←c+ 1,i∗←i∗andj∗←j∗\n86else\n87 forj∈ECdoUpdate key jofHR(it, t) with value ˆMt[it][j] + (c(ρ, t, j )−c(ˆρ, t, j ));\n88 j0, c0←max( HR(it, t)) maximum value with index j0and value c0\n89 c1←c0+ (r(ρ, t, i t)−r(ˆρ, t, i t))\n90 ifc1> cthen\n91 c←c1,i∗←itandj∗←j0\n92 else\n93 c←c,i∗←i∗andj∗←j∗\nOn a given query, we simply return cinO(1) time. In the following Lemma, we claim that\nAlgorithm 14 maintains the correctness of the data structures defined above.\nLemma 8.24. After Algorithm 14, the data structures IR, IC, ER, EC, c, i∗, j∗contain the de-\nsired values.\nProof. We assume that the data structures are maintained correctly up to the t−1 time step.\nBy definition, ICdoes not change since we have a row update and only IR[it] increments as this\nis the row updated. ER, ECare maintained according to Lemma 8.23. If it=i∗, then since\nMt−1[i∗][j∗] was the maximum value of Mt−1,|Mt−Mt−1|∞= 1 and Mt[i∗][j∗] =Mt−1[i∗][j∗]+1,\nthen c+1 is the maximum value and this is achieved by i∗, j∗. Therefore, we can assume it̸=i∗\nin the following.\nWe begin by noting the following equalities which hold for all i, j∈[n].\nMt[i][j] =M0[i][j] +r(ρ, t, i) +c(ρ, t, j )\nˆMt[i][j] =M0[i][j] +r(ˆρ, t, i) +c(ˆρ, t, j )\nMt[i][j] =ˆMt[i][j] + (r(ρ, t, i)−r(ˆρ, t, i)) + ( c(ρ, t, i)−c(ˆρ, t, j ))\nAfter the preprocessing step, we have that HR(it, t) contains the values ˆM[it][j] for all\nj∈[n]. After Line 87 then, we have HR(it, t) contains for all j,\nˆMt[it][j] + (c(ρ, t, j )−c(ˆρ, t, j )) =Mt[i][j]−(r(ρ, t, i t)−r(ˆρ, t, i t))\nIn particular, since the error term does not depend on j,\nj0= arg max\njMt[it][j]−(r(ρ, t, i t)−r(ˆρ, t, i t)) = arg max\njMt[it][j]\nso that after Line 89, c1←max jMt[it][j] and this maximum is attained at j0. Since\nMt−1[i][j] =Mt[i][j] for all i̸=it, we have that the maximum value is either corMt[it][j0].\nComparing the two and updating c, i∗, j∗accordingly completes the proof of the Lemma.\nGiven the above lemma, correctness easily follows as we handle queries by returning c.\n50\n\nLemma 8.25. Algorithm 13 requires O(Tn2)time. Algorithm 14 requires O((d+k) logn)time.\nProof. To examine Algorithm 13, note that IR, IC, ER, ECcan be computed in O(n) time while\nc, i∗, j∗can be computed in O(n2) time. Then, for each time step t, we compute ˆMtinO(n)\ntime following the trivial algorithm, while each heap construction requires O(n) time. Since we\nconstruct nheaps, this requires O(n2) time overall.\nExamining Algorithm 14, updating IRandERrequire ˜O(1) time. If it=i∗, the entire\nalgorithm requires ˜O(1) time. Otherwise, we update O(d) key values in the heap HR(it, t),\nrequiring O((d+k) logn) time. Extracting j0, c0and the remaining steps can be completed in\nO(logn) time.\nUpdate-Optimized ˜O(d2)Query Algorithm To design an update-optimized algorithm,\nwe observe that given ddelayed predictions ˆ ρ, for all time steps t,\nˆρ≤t−d⊆ρ≤t⊆ˆρ≤t+d\nby Lemma 8.1.\nAgain, for all 1 ≤d′≤ ⌈logn⌉, letd= 2d′. We will maintain the following data structures.\n1.IR, IC, ER, EC,{HR(i, t)}i,t,{HC(j, t)}j,tas in the query-optimized algorithm.\n2.D(−)\nR,d, ann-dimensional array containing r(ρ, t, i)−r(ˆρ, t−d, i) for all i∈[n],d= 2d′\n3.D(+)\nR,d, ann-dimensional array containing r(ˆρ, t+d, i)−r(ρ, t, i) for all i∈[n],d= 2d′\n4.D(−)\nC,d, ann-dimensional array containing c(ρ, t, j )−c(ˆρ, t−d, j) for all j∈[n],d= 2d′\n5.D(+)\nC,d, ann-dimensional array containing c(ˆρ, t+d, j)−c(ρ, t, j ) for all j∈[n],d= 2d′\n6.BR,d={i∈[n] s.t. D(−)\nR,d[i], D(+)\nR,d[i]≥0}andBC,d={j∈[n] s.t. D(−)\nC,d[j], D(+)\nC,d[j]≥0}\nfor all d= 2d′\n7.B(>)\nR,d={i∈[n] s.t. D(−)\nR,d[i]>0}andB(>)\nC,d={j∈[n] s.t. D(−)\nC,d[j]>0}for all d= 2d′\nIn the preprocessing step, initialize IR, IC, ER, ECand compute the binary heaps in the\nupdate-optimized algorithm, as well as a partial maximum ˆ pt, consisting of the maximum entry\nwhere both the row and column indices are permanent in ˆ ρfrom t−dtot+d. We also initialize\nthe data structures D(−)\nR,d, D(+)\nR,d, D(−)\nC,d, D(+)\nC,d, BR,d, BC,d, B(>)\nR,d, B(>)\nC,dto their initial values. We do\nthis for all values of d.\n51\n\nAlgorithm 15 QPreprocessingErickson (M0,ˆρ)\nInput: Initial Matrix M0.d-delayed predictions ˆ ρ.\n94IR[i], IC[j]←0 for all i, j∈[n]\n95ER, EC← ∅\n96ford= 2d′with d′= 1to⌈logn⌉do\n97 D(−)\nR,d, D(−)\nC,d←⃗0\n98 D(+)\nR,d[i]←r(ˆρ, d, i ) for all i∈[n]\n99 D(+)\nR,d[j]←c(ˆρ, d, j ) for all j∈[n]\n100 BR,d, BC,d←[n]\n101 B(>)\nR,d, B(>)\nC,d← ∅\n102fort= 1toTdo\n103 Compute ˆMtfrom ˆMt−1by incrementing the row or column specified by ˆ yt\n104 Construct binary heaps HR(i, t) for all i∈[n] and HC(j, t) for all j∈[n]\n105 ford= 2d′with d′= 1to⌈logn⌉do\n106 PR(t, d)← {is.t. row ipermanent in ˆ ρfrom t−dtot+d}\n107 PC(t, d)← {js.t. column jpermanent in ˆ ρfrom t−dtot+d}\n108 Compute ˆ pt,d←max{ˆMt[i][j] s.t. i∈PR(t, d), j∈PC(t, d)}\nConsider now an update. Our first step is to maintain d∗, the smallest dsatisfying ˆ ρ≤t−d⊂\nρ≤t⊂ˆρ≤t+d.\nFirst, we describe how to maintain the data structures initialized in Algorithm 15. Note\nIR, IC, ER, ECcan be maintained in ˜O(1) time by Lemma 8.25. Fix a single d. The arrays\nD(−)\nR,d, D(+)\nR,d, D(−)\nC,d, D(+)\nC,dcan be maintained in O(1) time by maintaining the appropriate array\nentries. Membership in BR,d, BC,dcan only change when an array entry crosses between 0 ,−1.\nWhen this occurs (say D(−)\nR,d[i] = 0) we check the corresponding entry in its associated array (in\nthis case D(+)\nR,d[i]) and update membership in BR,das appropriate. Using, for example a hash\ntable, BR,d, BC,dcan be maintained in O(1) time. A similar argument maintains B(>)\nR,d, B(>)\nC,din\nO(1) time. Thus, we maintain the above data structures for all din˜O(1) time.\nFurthermore, let d∗be the smallest value dsuch that |BR,d|=|BC,d|=n. Whenever\nˆρ≤t−d⊆ρ≤t⊆ˆρ≤t+dholds, we have |BR,d|=|BC,d|=n, so that we may bound d∗≤2d, if\nthe prediction ˆ ρisddelayed. Furthermore, d∗can be maintained with no additional cost per\nupdate by checking the size of BR,d, BC,dafter updating the hash tables.\nGiven a query, we use the following algorithm.\n52\n\nAlgorithm 16 QQueryErickson (Mt,ˆρ, ρ≤t, d∗)\nInput : Matrix Mt,ddelayed predictions ˆ ρ, request history ρ≤t, empirical delay d∗\nOutput: max i,jMt[i][j]\n109ifd∗=∞then\n110 Construct matrix Mt[i][j]←M0[i][j] +IR(i) +IC(j)\n111 return max{Mt[i][j]}\n112c←ˆpt,d∗\n113fori̸∈PR(t, d∗)do\n114 forj̸∈PC(t, d∗)do\n115 Update key jofHR(i, t) with value ˆMt[i][j] + (c(y, t, j )−c(ˆy, t, j ))\n116 Update key iofHC(j, t) with value ˆMt[i][j] + (r(y, t, i)−r(ˆy, t, i))\n117fori̸∈PR(t, d∗)doc←max( c,(r(y, t, i)−r(ˆy, t, i)) + max( HR(i, t))) ;\n118forj̸∈PC(t, d∗)doc←max( c,(c(y, t, j )−c(ˆy, t, j )) + max( HC(j, t))) ;\n119return c\nWe now prove that Algorithm 16 returns the correct value c.\nProof. First, we argue that for all ( i, j)∈PR(t, d∗)×PC(t, d∗), the predicted matrix ˆMt[i][j] =\nMt[i][j]. This follows as no update in ˆ ρ[t−d∗,t+d∗]increments the i-th row or the j-th column.\nTherefore,\nˆpt,d∗= max {Mt[i][j] s.t. i∈PR(t, d∗), j∈PC(t, d∗)}\nandcis set to this value in Line 112.\nNext, we argue that after Line 116, each heap HR(i, t) and HC(j, t) contain the correct\nmaximum index. In particular, we claim that the maximum index jinHR(i, t) is exactly\narg max jMt[i][j]. After preprocessing, each entry HR(i, t) is\nˆMt[i][j] =Mt[i][j]−(r(ρ, t, i)−r(ˆρ, t, i))−(c(ρ, t, j )−c(ˆρ, t, j ))\nFor all j∈PC(t, d∗), note c(ρ, t, j ) =c(ˆρ, t, j ). Then, after Line 116, we have,\nˆMt[i][j] =Mt[i][j]−(r(ρ, t, i)−r(ˆρ, t, i))\nand the claim follows as the error does not depend on j. In particular, in Line 117, we update\ncwith maximum of max jMt[i][j] for all i̸∈PR(t, d∗). Following a similar argument, Line 118\nupdates cwith the maximum of max iMt[i][j] for all j̸∈PC(t, d∗). Note that this covers all\ni, j∈[n]×[n], proving the correctness of the algorithm.\nWe now analyze the time complexity of the above algorithms.\nProof. The preprocessing algorithm requires preprocessing time O(Tn2logn) as constructing\nthe matrices ˆMtand heaps requires O(n2) time. For each n, maintaining PR(t, d), PC(t, d)\nrequires O(1) time while computing ˆ pt,drequires O(n2) time. We conclude by noting there are\nlognvalues of d.\nAbove, we have argued that the update algorithm requires ˜O(1) time.\nConsider now the query algorithm. We claim |[n]\\PR(t, d∗)|,|[n]\\PC(t, d∗)|=O(d). This\nfollows as d∗≤2dand there are at most O(d∗) =O(d) active rows and columns from t−2dto\nt+ 2d. Therefore, computing the double for loop in Line 116 requires O(d2logn) =˜O(d2) time.\nIn Lines 117 and 118 we extract the maximum from O(d) heaps in O(dlogn) =˜O(d) time.\n53\n\nReferences\n[ABE+23] Antonios Antoniadis, Joan Boyar, Marek Elias, Lene Monrad Favrholdt, Ruben\nHoeksma, Kim S. Larsen, Adam Polak, and Bertrand Simon. Paging with Suc-\ncinct Predictions. In Proc. 40th International Conference on Machine Learning\n(ICML’23) , p. 952–968, 2023.\n[ACE+21] Antonios Antoniadis, Christian Coester, Marek Eli´ as, Adam Polak, and Bertrand\nSimon. Learning-Augmented Dynamic Power Management with Multiple States\nvia New Ski Rental Bounds. In Proc. Neural Information Processing Systems\n(NeurIPS’21) , p. 16714–16726, 2021.\n[ACE+23] Antonios Antoniadis, Christian Coester, Marek Eli´ as, Adam Polak, and Bertrand\nSimon. Mixing Predictions for Online Metric Algorithms. In Proc. International\nConference on Machine Learning (ICML’23) , p. 969–983, 2023.\n[AD16] Amir Abboud and Søren Dahlgaard. Popular Conjectures as a Barrier for Dynamic\nPlanar Graph Algorithms. In Proc. 57th Symposium on Foundations of Computer\nScience (FOCS’16) , p. 477–486. IEEE Computer Society, 2016.\n[AGKP22] Keerti Anand, Rong Ge, Amit Kumar, and Debmalya Panigrahi. Online Algo-\nrithms with Multiple Predictions. In Proc. International Conference on Machine\nLearning, (ICML’22) , p. 582–598, 2022.\n[AGS22] Antonios Antoniadis, Peyman Jabbarzade Ganje, and Golnoosh Shahkarami. A\nNovel Prediction Setup for Online Speed-Scaling. In Proc. 18th Scandinavian\nSymposium and Workshops on Algorithm Theory (SWAT’22) , p. 9:1–9:20, 2022.\n[AIV19] Anders Aamand, Piotr Indyk, and Ali Vakilian. (Learned) Frequency Estimation\nAlgorithms under Zipfian Distribution. CoRR , abs/1908.05198, 2019.\n[APT22] Yossi Azar, Debmalya Panigrahi, and Noam Touitou. Online Graph Algorithms\nwith Predictions. In Proc. Symposium on Discrete Algorithms (SODA’22) , p.\n35–66, 2022.\n[AW14] Amir Abboud and Virginia Vassilevska Williams. Popular Conjectures Imply\nStrong Lower Bounds for Dynamic Problems. In Proc. 55th Symposium on Foun-\ndations of Computer Science (FOCS’14) , p. 434–443, 2014.\n[BCK+22] Nikhil Bansal, Christian Coester, Ravi Kumar, Manish Purohit, and Erik Vee.\nLearning-Augmented Weighted Paging. In Proc. Symposium on Discrete Algo-\nrithms (SODA’22) , p. 67–89, 2022.\n[BGS18] Surender Baswana, Manoj Gupta, and Sandeep Sen. Fully Dynamic Maximal\nMatching in O(log n) Update Time (Corrected Version). SIAM J. Comput. ,\n47(3):617–650, 2018.\n[BHG+21] Thiago Bergamaschi, Monika Henzinger, Maximilian Probst Gutenberg, Vir-\nginia Vassilevska Williams, and Nicole Wein. New Techniques and Fine-Grained\nHardness for Dynamic Near-Additive Spanners. In D´ aniel Marx, editor, Proceed-\nings of the 2021 ACM-SIAM Symposium on Discrete Algorithms, SODA 2021,\nVirtual Conference, January 10 - 13, 2021 , p. 1836–1855. SIAM, 2021.\n[BMS20] ´Etienne Bamas, Andreas Maggiori, and Ola Svensson. The Primal-Dual method\nfor Learning Augmented Algorithms. In Proc. Neural Information Processing\nSystems (NeurIPS’20) , 2020.\n54\n\n[Che12] Shiri Chechik. Improved Distance Oracles and Spanners for Vertex-Labeled\nGraphs. In Proc. 20th Annual European Symposium (ESA’12) , p. 325–336, 2012.\n[CKL18] Diptarka Chakraborty, Lior Kamma, and Kasper Green Larsen. Tight cell probe\nbounds for succinct Boolean matrix-vector multiplication. In Proc. of the 50th\nSymposium on Theory of Computing (STOC’18) , p. 1297–1306, 2018.\n[CPR11] Timothy M. Chan, Mihai P˘ atra¸ scu, and Liam Roditty. Dynamic Connectivity:\nConnecting to Networks and Geometry. SIAM J. Comput. , 40(2):333–349, 2011.\n[Dah16] Søren Dahlgaard. On the Hardness of Partially Dynamic Graph Problems and\nConnections to Diameter. In Proc. 43rd International Colloquium on Automata,\nLanguages, and Programming (ICALP’16) , p. 48:1–48:14, 2016.\n[DHZ00] Dorit Dor, Shay Halperin, and Uri Zwick. All-Pairs Almost Shortest Paths. SIAM\nJ. Comput. , 29(5):1740–1759, 2000.\n[DIL+22] Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei\nVassilvitskii. Algorithms with Prediction Portfolios. In NeurIPS , 2022.\n[DP10] Ran Duan and Seth Pettie. Connectivity oracles for failure prone graphs. In Proc.\nof the 42nd ACM Symposium on Theory of Computing, STOC 2010, Cambridge,\nMassachusetts, USA, 5-8 June 2010 , p. 465–474. ACM, 2010.\n[Dua10] Ran Duan. New Data Structures for Subgraph Connectivity. In Automata, Lan-\nguages and Programming, 37th International Colloquium, ICALP 2010, Bordeaux,\nFrance, July 6-10, 2010, Proc., Part I , volume 6198 of Lecture Notes in Computer\nScience , p. 201–212. Springer, 2010.\n[EFW21] Jacob Evald, Viktor Fredslund-Hansen, and Christian Wulff-Nilsen. Near-Optimal\nDistance Oracles for Vertex-Labeled Planar Graphs. In Proc. 32nd International\nSymposium on Algorithms and Computation (ISAAC’21) , p. 23:1–23:14, 2021.\n[EIN+21] Talya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal,\nand Tal Wagner. Learning-based Support Estimation in Sublinear Time. In 9th\nInternational Conference on Learning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021 . OpenReview.net, 2021.\n[ES81] Shimon Even and Yossi Shiloach. An On-Line Edge-Deletion Problem. J. ACM ,\n28(1):1–4, 1981.\n[FHW12] Silvio Frischknecht, Stephan Holzer, and Roger Wattenhofer. Networks cannot\ncompute their diameter in sublinear time. In Proc. of the Twenty-Third Annual\nACM-SIAM Symposium on Discrete Algorithms, SODA 2012, Kyoto, Japan, Jan-\nuary 17-19, 2012 , p. 1150–1162. SIAM, 2012.\n[FI00] Daniele Frigioni and Giuseppe F. Italiano. Dynamically Switching Vertices in\nPlanar Graphs. Algorithmica , 28(1):76–103, 2000.\n[GHP17] Gramoz Goranci, Monika Henzinger, and Pan Peng. The Power of Vertex Sparsi-\nfiers in Dynamic Graph Algorithms. In 25th Annual European Symposium on Al-\ngorithms, ESA 2017, September 4-6, 2017, Vienna, Austria , volume 87 of LIPIcs ,\np. 45:1–45:14. Schloss Dagstuhl - Leibniz-Zentrum f¨ ur Informatik, 2017.\n[GJL+21] Leszek Gasieniec, Jesper Jansson, Christos Levcopoulos, Andrzej Lingas, and Mia\nPersson. Pushing the Online Boolean Matrix-vector Multiplication conjecture off-\nline and identifying its easy cases. J. Comput. Syst. Sci. , 118:108–118, 2021.\n55\n\n[GP13] Manoj Gupta and Richard Peng. Fully dynamic (1+ e)-approximate matchings.\nIn2013 IEEE 54th Annual Symposium on Foundations of Computer Science , p.\n548–557. IEEE, 2013.\n[GPSS22] Anupam Gupta, Debmalya Panigrahi, Bernardo Subercaseaux, and Kevin Sun.\nAugmenting Online Algorithms with ε-Accurate Predictions. In Advances in Neu-\nral Information Processing Systems , volume 35, p. 2115–2127, 2022.\n[HIKV19] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-Based Fre-\nquency Estimation Algorithms. In 7th International Conference on Learning Rep-\nresentations, ICLR . OpenReview.net, 2019.\n[HKNS15] Monika Henzinger, Sebastian Krinninger, Danupon Nanongkai, and Thatchaphol\nSaranurak. Unifying and Strengthening Hardness for Dynamic Problems via the\nOnline Matrix-Vector Multiplication Conjecture. In Proc. of the Forty-Seventh\nAnnual ACM on Symposium on Theory of Computing, STOC , p. 21–30. ACM,\n2015.\n[HLWY11] Danny Hermelin, Avivit Levy, Oren Weimann, and Raphael Yuster. Distance\nOracles for Vertex-Labeled Graphs. In Proc. 38th International Colloquium, Au-\ntomata, Languages and Programming (ICALP’11) , p. 490–501, 2011.\n[HPS21] Monika Henzinger, Ami Paz, and Stefan Schmid. On the Complexity of Weight-\nDynamic Network Algorithms. In IFIP Networking Conference, IFIP Networking\n2021, Espoo and Helsinki, Finland, June 21-24, 2021 , p. 1–9. IEEE, 2021.\n[HPS22] Monika Henzinger, Ami Paz, and A. R. Sricharan. Fine-Grained Complexity Lower\nBounds for Families of Dynamic Graphs. In Proc. 30th European Symposium on\nAlgorithms (ESA’22) , volume 244 of LIPIcs , p. 65:1–65:14. Schloss Dagstuhl -\nLeibniz-Zentrum f¨ ur Informatik, 2022.\n[IKPP22] Sungjin Im, Ravi Kumar, Aditya Petety, and Manish Purohit. Parsimonious\nLearning-Augmented Caching. In Proc. of the 39th International Conference on\nMachine Learning , volume 162 of Proc. of Machine Learning Research , p. 9588–\n9601. PMLR, 17–23 Jul 2022.\n[KBC+18] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. The\nCase for Learned Index Structures. In Proc. of the 2018 International Conference\non Management of Data, SIGMOD Conference 2018, Houston, TX, USA, June\n10-15, 2018 , p. 489–504. ACM, 2018.\n[KKM13] Bruce M. Kapron, Valerie King, and Ben Mountjoy. Dynamic graph connectivity\nin polylogarithmic worst case time. In Proc. of the Twenty-Fourth Annual ACM-\nSIAM Symposium on Discrete Algorithms, SODA , p. 1131–1142. SIAM, 2013.\n[KPP16] Tsvi Kopelowitz, Seth Pettie, and Ely Porat. Higher Lower Bounds from the 3SUM\nConjecture. In Proc. of the Twenty-Seventh Annual ACM-SIAM Symposium on\nDiscrete Algorithms, SODA 2016, Arlington, VA, USA, January 10-12, 2016 , p.\n1272–1287. SIAM, 2016.\n[Lin02] Andrzej Lingas. A Geometric Approach to Boolean Matrix Multiplication. In\nProc. 13th International Symposium Algorithms and Computation (ISAAC’02) ,\np. 501–510, 2002.\n[LM22] Alexander Lindermayr and Nicole Megow. Non-Clairvoyant Scheduling with Pre-\ndictions Revisited. CoRR , abs/2202.10199, 2022.\n56\n\n[LOP+15] Jakub Lacki, Jakub Ocwieja, Marcin Pilipczuk, Piotr Sankowski, and Anna Zych.\nThe Power of Dynamic Distance Oracles: Efficient Dynamic Algorithms for the\nSteiner Tree. In Proc. 47th Symposium on Theory of Computing (STOC’15) , p.\n11–20, 2015.\n[LS23] Quanquan C. Liu and Vaidehi Srinivas. The Predicted-Deletion Dynamic Model:\nTaking Advantage of ML Predictions, for Free. CoRR , abs/2307.08890, 2023.\n[LV21] Thodoris Lykouris and Sergei Vassilvitskii. Competitive Caching with Machine\nLearned Advice. J. ACM , 68(4):24:1–24:25, 2021.\n[LW17] Kasper Green Larsen and R. Ryan Williams. Faster Online Matrix-Vector Multi-\nplication. In Proc. 28th Symposium on Discrete Algorithms (SODA’17) , p. 2182–\n2189, 2017.\n[Mad11] Aleksander Madry. From graphs to matrices, and back: new techniques for graph\nalgorithms . PhD thesis, Massachusetts Institute of Technology, 2011.\n[MV21] Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with Predictions , p.\n646–662. 2021.\n[Pat10] Mihai Patrascu. Towards polynomial lower bounds for dynamic problems. In Proc.\nof the 42nd ACM Symposium on Theory of Computing, STOC 2010, Cambridge,\nMassachusetts, USA, 5-8 June 2010 , p. 603–610. ACM, 2010.\n[PSK18] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving Online Algorithms\nvia ML Predictions. In Advances in Neural Information Processing Systems 31:\nAnnual Conference on Neural Information Processing Systems 2018, NeurIPS\n2018, December 3-8, 2018, Montr´ eal, Canada , p. 9684–9693, 2018.\n[Roh20] Dhruv Rohatgi. Near-Optimal Bounds for Online Caching with Machine Learned\nAdvice. In Proc. of the 2020 ACM-SIAM Symposium on Discrete Algorithms,\nSODA 2020, Salt Lake City, UT, USA, January 5-8, 2020 , p. 1834–1845. SIAM,\n2020.\n[RZ11] Liam Roditty and Uri Zwick. On Dynamic Shortest Paths Problems. Algorithmica ,\n61(2):389–401, 2011.\n[RZ12] Liam Roditty and Uri Zwick. Dynamic Approximate All-Pairs Shortest Paths in\nUndirected Graphs. SIAM J. Comput. , 41(3):670–683, 2012.\n[SAN+23] Sandeep Silwal, Sara Ahmadian, Andrew Nystrom, Andrew McCallum, Deepak\nRamachandran, and Seyed Mehran Kazemi. KwikBucks: Correlation Clustering\nwith Cheap-Weak and Expensive-Strong Signals. In The Eleventh International\nConference on Learning Representations ICLR , 2023.\n[SLLA23] Yongho Shin, Changyeol Lee, Gukryeol Lee, and Hyung-Chan An. Improved\nLearning-Augmented Algorithms for the Multi-Option Ski Rental Problem via\nBest-Possible Competitive Analysis. In Proc. of the 40th International Conference\non Machine Learning , p. 31539–31561. PMLR, 2023.\n[Sol16] Shay Solomon. Fully Dynamic Maximal Matching in Constant Update Time. In\nIEEE 57th Annual Symposium on Foundations of Computer Science, FOCS , p.\n325–334. IEEE Computer Society, 2016.\n[vdBFNP23] Jan van den Brand, Sebastian Forster, Yasamin Nazari, and Adam Polak. On\nDynamic Graph Algorithms with Predictions. CoRR , abs/2307.09961, 2023.\n57\n\n[Wil07] Ryan Williams. Matrix-vector multiplication in sub-quadratic time: (some prepro-\ncessing required). In Proc. 18th Symposium on Discrete Algorithms (SODA’07) ,\np. 995–1001, 2007.\n58\n\nA Lower Bounds for OMv with Predictions\nWe give the proofs for lower bounds against algorithms with prediction solving the OMv prob-\nlem.\nTheorem 2.3. Letε∈(0,1)be a constant and ∆∈[1, nε]. There is no algorithm with\npredictions of EH-distance at most ∆for the OMv problem with amortized time Q(n,∆) = ˜˜o(n∆)\nper round, if the OMv-conjecture is true.\nProof. Consider an S-OMv instance Mof size n×nwith nvectors ⃗ v1, . . . ,⃗ v nsuch that\nsupp( ⃗ vi)⊂Sfor some index-set S⊂[n] of size |S| ≤∆. Observe that the n-dimensional\nvector ⃗0 has L1-distance, and, thus, EH-distance, at most ∆ to ⃗ vifor every i.\nNow consider an algorithm with prediction Athat takes amortized time Q(n,∆) per round,\nafter polynomial preprocessing time. Computing M⃗ vifor all iwith 1 ≤i≤nsolves the original\nS-OMv problem in time n·Q(n,∆). Note that the prediction ( ⃗0, . . . ,⃗0) has EH distance at\nmost ∆ and can be passed to Afor the preprocessing, after which the dynamic algorithm runs\nin time O(nQ(n,∆)).\nAccording to the Theorem 4.2, there is no dynamic algorithm running in ˜˜o(n2∆) time. This\nimplies that here is no algorithm with amortized time Q(n,∆) = ˜˜o(n∆) per round, if the OMv\nconjecture (Conjecture 2.1) is true.\nAlternatively, we can consider a prediction model which reveals some information about\neach round. Let Sbe a set of vector sequences. We will say a prediction sequence (ˆ v1, . . . , ˆvn)\nisb-bit accurate for Sif each ˆ vk∈ {0,1,∗}nhas at least bindices with ˆ vk[i]∈ {0,1}and\nfor all ( ⃗ v1, . . . ,⃗ v n)∈ S, we have ⃗ vk[i] = ˆvk[i] for all i, k∈[n] where ˆ vk[i]̸=∗. Intuitively, ˆ vk\nspecifies at least bbits of vector ⃗ vk. Trivially, there is an algorithm with O(nω) preprocessing\ntime computing each product M⃗ vkinQ(n, b) =O(n(n−b)) time. To see this, define the vectors\n(⃗ v′\n1, . . . ,⃗ v′\nn) as follows.\n⃗ v′\nk[i] =(\n1 ˆvk[i] = 1\n0 o/w\nand in the preprocessing step compute M⃗ v′\nkfor all k∈[n] using fast matrix multiplication, which\nrequires O(nω) time. Given a vector update, define ¯ vk=⃗ vk−⃗ v′\nkso that |supp(¯ vk)| ≤n−b.\nThen, compute M¯vkinO(n(n−b)) time and compute M⃗ vk=M⃗ v′\nk+M¯vk. We show that this\nis almost optimal.\nTheorem A.1. Letb∈[n]. There is no algorithm with b-bit accurate predictions for the OMv\nproblem with amortized time Q(n, b) =˜˜o(n(n−b))per round, if the OMv-conjecture is true.\nProof. Consider an S-OMv instance Mof size n×nwith nvectors ⃗ v1, . . . ,⃗ v nsuch that\nsupp( ⃗ vi)⊂Sfor some index-set S⊂[n] of size |S| ≤n−b. Observe that the prediction\n(ˆv, . . . , ˆv) isb-bit accurate by defining ˆ vas follows.\nˆv[i] =(\n0i /∈S\n∗o/w\nNow consider an algorithm with prediction Athat takes amortized time Q(n, b) per round,\nafter polynomial preprocessing time. Computing M⃗ vkfor all kwith 1 ≤k≤nsolves the\noriginal S-OMv problem in time n·Q(n, b). Note that the prediction (ˆ v, . . . , ˆv) isb-bit accurate\nand can be passed to Afor the preprocessing, after which the dynamic algorithm runs in time\nO(nQ(n,∆)).\nAccording to the Theorem 4.2, there is no dynamic algorithm running in ˜˜o(n2(n−b)) time.\nThis implies that here is no algorithm with amortized time Q(n, b) =˜˜o(n(n−b)) per round, if\nthe OMv conjecture (Conjecture 2.1) is true.\n59\n\nB Concurrent Work\nIn an independent work, van den Brand, Forster, Nazari, and Polak [vdBFNP23] design a\nvariety of dynamic graph algorithms with predictions, considering both edge and vertex updates.\n[vdBFNP23] considers three prediction models: 1) the bounded delay model, 2) a ℓ1variant of\nthe bounded delay model, and 3) a fully dynamic prediction model where the predictions are\nnot given during the preprocessing time, but instead a vertex’s deletion time is predicted upon\nthe insertion of that vertex. [LS23] applies the third model to edge updates, giving a generic\nframework for designing robust fully dynamic algorithms that achieve the worst-case update\ntime of partially dynamic algorithms when the predicted deletion times are accurate.\n[vdBFNP23] design several algorithms with prediction relying on fast matrix multiplication.\nWe describe a few that are related to our work below.\n1. A partially dynamic algorithm for dynamic transitive closure under edge updates with d\nbounded delay predictions. We obtain the same update ( ˜O(1)) and query time ˜O(d2))\n(Theorem 8.18) with a combinatorial fully dynamic algorithm, with worse preprocessing\ntime.\n2. A partially dynamic algorithm for (1 + ε) approximate all pairs shortest path under edge\nupdates with bounded delay predictions. We obtain the same update ( ˜O(1)) and query\ntime ˜O(d2)) (Theorem 8.21) for exact APSP with a combinatorial fully dynamic algorithm,\nwith worse preprocessing time.\n3. A fully dynamic algorithm for triangle detection and single source reachability (among\nother problems) under vertex updates with bounded delay predictions. They obtain con-\nstant query time and update time ˜O(nω−1+nmin(di, n)) where didenotes the delay of\nthei-th vertex update. For # s-△, we obtain the following two algorithms (both with\nworse preprocessing time than the algorithm of [vdBFNP23]).\n(a) An update optimized algorithm with constant update time and O((d+k)2) query\ntime.\n(b) A query optimized algorithm with constant query time and update time O(d+k).\nNote that our algorithms additionally support predictions that are ddelayed with kout-\nliers. However, the performance of our algorithms depend on the largest delay of any\nupdate, while the performance of [vdBFNP23] depends only on the delay of the current\nupdate.\n[vdBFNP23] also give a lower bound for partially dynamic algorithms with prediction for the\ntransitive closure and approximate all pairs shortest path problems. We give a similar result for\nlocally reducible problems in the partially dynamic setting (Definition 15 and Theorem 6.8).\n60",
  "textLength": 192361
}