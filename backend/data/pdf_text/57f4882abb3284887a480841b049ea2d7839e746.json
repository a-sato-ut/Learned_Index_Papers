{
  "paperId": "57f4882abb3284887a480841b049ea2d7839e746",
  "title": "Lazy B-Trees",
  "pdfPath": "57f4882abb3284887a480841b049ea2d7839e746.pdf",
  "text": "arXiv:2507.00277v1  [cs.DS]  30 Jun 2025Lazy B-Trees\nCasper Moldrup Rysgaard /envel⌢pe\nAarhus University, Denmark\nSebastian Wild /envel⌢pe\nPhilipps-University Marburg, Germany, and University of Liverpool, UK\nAbstract\nLazy search trees (Sandlund & Wild FOCS 2020, Sandlund & Zhang SODA 2022) are sorted\ndictionaries whose update and query performance smoothly interpolates between that of efficient\npriority queues and binary search trees – automatically, depending on actual use; no adjustments\nare necessary to the data structure to realize the cost savings. In this paper, we design lazy B-trees ,\na variant of lazy search trees suitable for external memory that generalizes the speedup of B-trees\nover binary search trees wrt. input/output operations to the same smooth interpolation regime.\nA key technical difficulty to overcome is the lack of a (fully satisfactory) external variant of\nbiasedsearch trees, on which lazy search trees crucially rely. We give a construction for a subset of\nperformance guarantees sufficient to realize external-memory lazy search trees, which we deem of\nindependent interest.\nAs one special case, lazy B-trees can be used as an external-memory priority queue, in which\ncase they are competitive with some tailor-made heaps; indeed, they offer faster decrease-key and\ninsert operations than known data structures.\n2012 ACM Subject Classification Theory of computation →Data structures design and analysis\nKeywords and phrases B-tree, lazy search trees, lazy updates, external memory, deferred data\nstructures, database cracking\nFunding Casper Moldrup Rysgaard : Independent Research Fund Denmark, grant 9131-00113B.\nSebastian Wild : EPSRC grant EP/X039447/1.\n1 Introduction\nWeintroducethe lazy B-tree datastructure, whichbringstheadaptiveperformanceguarantees\nof lazy search trees to external memory.\nThe binary search tree (BST) is a fundamental data structure, taught in every computer\nscience degree and widespread in practical use. Wherever rank-based operations are needed,\ne.g., finding a kth smallest element in a dynamic set or determining the rank of an element\nin the set, i.e., its position in the sorted order of the current elements, augmented BSTs\nare the folklore solution: binary search trees using one of the known schemes to “balance”\nthem, i.e., guarantee O(logN)height for a set of size N, where we additionally store the\nsubtree size in each node. On, say, AVL-trees, all operations of a sorted dictionary, i.e., rank,\nselect, membership, predecessor, successor, minimum, and maximum, as well as insert, delete,\nchange-key, split, and merge can all be supported in O(logN)worst case time, where Nis\nthe current size of the set.\nFrom the theory of efficient priority queues, it is well known that much more efficient\nimplementations are possible if not all of the above operations have to be supported: When\nonly minimum, insert, delete, decrease-key, and meld are allowed, all can be supported to\nrun in constant time, except for O(logN)delete. Lazy search trees [ 44,46] (LSTs) show that\nwe can get the same result with full support for all sorted-dictionary operations, so long as\nthey are not used. When they do get used, lazy search trees give the optimal guarantees of\nany comparison-based data structure for the given query ranks, gracefully degrading from\n\n2 Lazy B-Trees\n∆1\n∆1∆2\n∆2∆3\n∆3∆4\n∆4∆5\n∆5∆6\n∆6∆7\n∆7∆8\n∆8\nr0 r1 r2r3r4 r5 r6r7 r8\n∆2\nI2,1I2,2I2,3 I2,4 I2,5I2,6I2,7I2,8\nFigure 1 Schematic view of the original lazy search tree data structure [44].\npriority-queue performance to BST performance, as more and more queries are asked. They\ntherefore serve as a versatile drop-in replacement for both priority queues and BSTs.1\nThe arguably most impactful application of sorted dictionaries across time is the database\nindex. Here, economizing on accesses to slow secondary storage can outweigh any other\ncosts, which gave rise to external-memory data structures; in particular B-trees and their\nnumerousvariants. Theyachievea log2(B)-factorspeedupoverBSTsintermsofinput/output\noperations (I/Os), for Bthe block size of transfers to the external memory, for scanning-\ndominated operations such as a range query, even a factor Bspeedup can be achieved.\nMuch ingenuity has been devoted to (a) write-optimized variants of B-trees [ 3,15], often\nwith batched queries, and (b), adaptive indices [ 31], or more recently learned indices [ 36] that\ntry to improve query performance via adapting to data at hand. Deferred data structuring –\nknown as database cracking in the systems community – is a special case of (b) that, like\nlazy search trees, refines a sorted order (of an index) successively upon queries. Yet in the\npast, these two approaches often remained incompatible, if not having outright conflicting\ngoals, making either insertions or queries fast. Lazy search trees are a principled solution\nto get both (as detailed in Section 1.2); unfortunately, standard lazy search trees are not\ncompetitive with B-trees in terms of their I/O efficiency, thus losing their adaptive advantages\nwhen data resides on secondary storage.\n1We point out that a related line of work on “deferred data structures” also adapts to the queried\nranks in the way LSTs do, but all data structures prior to LSTs had Ω(logN)insertion time (with\nthe assumption that any insertion is preceded by a query for said element). These data structures are\ntherefore inherently unable to function as an efficient priority queue. Further related work is discussed\nin Section 1.3.\n\nC. M. Rysgaard, S. Wild. 3\nIn this paper, we present lazy B-trees , an I/O-efficient variant of the original lazy search\ntrees [44].2Lazy search trees consist of three layers (see Figure 1). The topmost layer consists\nof a biased search tree of gaps(defined in Section 1.2), weighted by their size. The second\nlayer consists, for each gap ∆i, of a balanced binary search tree of O(log|∆i|)intervals Ii,j.\nIntervals are separated by splitter elements (pivots as in Quicksort), but are unsorted within.\nThe third layer represents intervals as a simple unordered list.\nA key obstacle to external-memory variants of lazy search trees – both for the original\nversion [ 44] and for optimal lazy search trees [ 46] – is the topmost layer. As discussed in\nSection 1.3, none of the existing data structures fully solves the problem of how to design a\ngeneral-purpose external-memory biased search tree – and we likewise leave this as an open\nproblem for future work. However, we show that for a slightly restricted set of performance\nguarantees sufficient for lazy search trees, a (to our knowledge) novel construction based on\npartitioning elements by weightwith doubly-exponentially increasing bucket sizes provides\nan I/O-efficient solution. This forms the basis of our lazy B-trees, which finally bring the\nadaptive versatility of lazy search trees to external memory.\n1.1 The External-Memory Model\nIn this paper we study the sorted dictionary problem in a hierarchical-memory model, where\nwe have an unbounded external memory and an internal memory of capacity Melements,\nand where data is transferred between the internal and external memory in blocks of B\nconsecutive elements. A block transfer is called an I/O(input/output operation). The I/O\ncost of an algorithm is the number of I/Os it performs. Aggarwal and Vitter [ 2] introduced\nthis as the external-memory model (and proved, e.g., lower bounds for sorting in the model).\n1.2 Lazy Search Trees\nIn this section we briefly describe lazy search trees [ 44]. Lazy search trees support all\noperations of a dynamic sorted dictionary (details in [ 44]). We consider the following\noperations (sufficient to base a full implementation on):\nConstruct (S):Constructs the data structure from the elements of set S.3\nInsert (e):Adds element eto the set.\nDelete (ptr):Deletes the element at pointer ptrfrom the set.\nChangeKey (ptr, e′):Changes the element eat pointer ptrtoe′.\nQueryElement (e):Locates the predecessor e′to element ein the set, and returns rank\nrofe′, and a pointer to e′.\nQueryRank (r):Locates the element ewith rank rin the set, and returns a pointer to e.\nLazy search trees maintain the elements of the set partitioned into G“gaps”,∆1, . . . , ∆G.\nAll elements in ∆iare weakly smaller than all elements in ∆i+1, but the gaps are otherwise\ntreated as unsorted bags. Boundaries between gaps (and new gaps) are introduced only\nvia a query; initially we have a single gap ∆1. Every query is associated with its query\nrank r[45, §4], i.e., the rank of its result, and splits an existing gap into two new gaps,\nsuch that its query rank is the largest rank in the left one of these gaps. After queries with\n2The follow-up work [ 46] replaces the second and third layer using selectable priority queues ; we discuss\nwhy these are challenging to make external in Section 1.4.\n3Construct and iterative insertion have the same complexity in internal memory, but in external\nmemory, the bulk operation can be supported more efficiently.\n\n4 Lazy B-Trees\nTable 1 Overview of results on Search Trees and Priority Queues for both the internal- and\nexternal-memory model. For the internal model the displayed time is the number of RAM operations\nperformed, while for the external model the displayed time is the number of I/Os performed.\nAmortized times are marked by “am.” and results doing batched updates and queries are marked by\n“batched”. For priority queues, query is only for the minimum. All results use linear space.\nInsert Query\nInternal-Memory\nBalanced BST [1, 26] O(logN) O(logN)\nPriority Queue [14, 19] O(1) O(logN)\nLazy Search Tree [44] O/parenleftbig\nlogN\n|∆i|+ log log N/parenrightbig\nO(logN+xlogc)am.\nOptimal LST [46] O/parenleftbig\nlogN\n|∆i|/parenrightbig\nO(logN+xlogc)am.\nExternal-Memory\nB-tree [8] O(logBN) O(logBN)\nBε-tree [15] O/parenleftbig1\nεB1−εlogBN/parenrightbig\nam. O/parenleftbig1\nεlogBN/parenrightbig\nam.\nBuffer Tree [3, 4] O/parenleftbig1\nBlogM/BN\nB/parenrightbig\nbatched O/parenleftbig1\nBlogM/BN\nB/parenrightbig\nbatched\nI/O-eff. heap [37] O/parenleftbig1\nBlog2N\nB/parenrightbig\nam. O/parenleftbig1\nBlog2N\nB/parenrightbig\nam.\nx-treap heap [28] O/parenleftbig1\nBlogM/BN\nB/parenrightbig\nam. O/parenleftbigMε\nBlog2\nM/BN\nB/parenrightbig\nam.\nThis paper, LST\n(Theorem 1 )O/parenleftbig\nlogBN\n|∆i|+ logBlogB|∆i|/parenrightbigO/parenleftbig\nlogBmin{N, q}+1\nBlog2|∆i|\n+ logBlogB|∆i|+1\nBxlog2c/parenrightbig\nam.\nThis paper, PQ\n(Corollary 2)O(logBlogBN)O/parenleftbig1\nBlog2N+ logBlogBN/parenrightbig\nam.\nranks r1< r2<···< rq, we thus obtain the gaps ∆1, . . . , ∆q+1where|∆i|=ri−ri−1upon\nsetting r0= 0andrq+1=n.\nLazy search trees support insertions landing in gap ∆iinO(log(N/|∆i|) + log log|∆i|)\n(worst-case) time and queries in O(xlogc+ log n)amortized time, where the query splits\na gap into new gaps of sizes xandcxfor some c≥1. Deletions are supported in O(logn)\n(worst-case) time. To achieve these times, gaps are further partitioned into intervals, with an\namortized splitting and merging scheme (see Section 3 for details).\n1.3 Related Work\nWe survey key prior work in the following; for a quick overview with selected results and\ncomparison with our new results, see also Table 1.\n(External) Search trees. Balanced binary search trees (BSTs) exist in many varieties,\nwith AVL trees [ 1] and red-black trees [ 26] the most widely known. When augmented with\nsubtree sizes, they support all sorted dictionary operations in O(logN)worst-case time.\nSimpler variants can achieve the same via randomization [ 47,39] or amortization [ 48]. In\nexternal memory, B-trees [ 8], often in the leaf-oriented flavor as B+-trees and augmented\nwith subtree sizes, are the benchmark. They support all sorted-dictionary operations in\nO(logBN)I/Os. By batching queries, buffer trees [ 3,4] substantially reduce the cost to\namortizedO/parenleftbig1\nBlogM/BN\nB/parenrightbig\nI/Os, but are mostly useful in an offline setting due to the long\ndelay from query to answer. Bε-trees [15] avoid this with a smaller buffer of operations per\nnode to achieve amortized O/parenleftbig1\nεB1−εlogBN/parenrightbig\nI/Os for updates and O/parenleftbig1\nεlogBN/parenrightbig\nI/Os for\nqueries (with immediate answers), where ε∈(0,1]is a parameter.\n\nC. M. Rysgaard, S. Wild. 5\nDynamic Optimality. The dynamic-optimality conjecture for Splay trees [ 48] resp. the\nGreedyBST [ 38,41,21] algorithm posits that these methods provide an instance-optimal\nbinary-search-tree algorithm for any (long) sequence of searches over a static set of keys\nin a binary search tree. While still open for Splay trees and GreedyBST, the dynamic\noptimality of data structures has been settled in some other models: it holds for a variant\nof skip lists [ 13], multi-way branching search trees, and B-trees [ 13]; it has been refuted for\ntournament heaps [ 42]. As in lazy search trees, queries clustered in time or space allow a\nsequence to be served faster. Unlike in lazy search trees, insertions can remain expensive\neven when no queries ever happen close to them. A more detailed technical discussion of\nsimilarities and vital differences compared to lazy search trees appears in [44, 45].\n(External) Priority Queues. When only minimum-queries are allowed, a sorted dictionary\nbecomes a priority queue (PQ) (a.k.a. “heap”). In internal memory, all operations except\ndelete-min can then be supported in O(1)amortized [ 25] or even worst-case time [ 14,19].\nIn external memory, developing efficient PQs has a long history. For batch use, buffer-tree\nPQs [4] and the I/O-efficient heap of [ 37] support koperations of insert and delete-min in\nO/parenleftbigk\nBlogM/BN\nM/parenrightbig\nI/Os, with Ndenoting the maximum number of stored keys over the k\noperations. The same cost per operation can be achieved in a worst-case sense [ 16] (i.e., B\nsubsequent insert/delete-min operations cost O/parenleftbig\nlogM/BN\nM/parenrightbig\nI/Os).\nNone of these external-memory PQs supports decrease-key. The I/O-efficient tournament\ntrees of [ 37] support a sequence of insert, delete-min, and decrease-key with O/parenleftbig1\nBlog2N\nB/parenrightbig\nI/Os per operation. A further log log Nfactor can be shaved off [ 32] (using randomization),\nbut that, surprisingly, is optimal [ 23]. A different trade-off is possible: insert and decrease-key\nare possible inO/parenleftbig1\nBlogM/BN\nB/parenrightbig\namortized I/Os at the expense of driving up the cost for\ndelete/delete-min to O/parenleftbigMε\nBlog2\nM/BN\nB/parenrightbig\n[28].\n(External) Biased Search Trees. Biased search trees [ 9] maintain a sorted set, where each\nelement ehas a (dynamic) weight w(e), such that operations in the tree spend O/parenleftbig\nlogW\nw(e)/parenrightbig\ntime, for Wthe total weight, W=/summationtext\new(e). Biased search trees can be built from height-\nbalanced trees ( (2, b)-globally-biased trees [ 9]) or weight-balanced trees (by representing\nhigh-weight elements several times in the tree [ 40]), and Splay trees automatically achieve\nthe desired time in an amortized sense [48, 40].\nNone of the original designs are well suited for external memory. Feigenbaum and\nTarjan [24] extended biased search trees to (a, b)-trees for that purpose. However, during\nthe maintenance of (a, b)-trees, some internal nodes may have a degree much smaller than\na(at least 2), which means that instead of requiring O(N/B )blocks to store Nweighted\nelements, they require O(N)blocks in the worst case.4The authors in [ 24] indeed leave it\nas an open problem to find a space-efficient version of biased (a, b)-trees. Another attempt\nat an external biased search tree data structure is based on deterministic skip lists [ 5]. Yet\nagain, the space usage seems to be Ω(N)blocks of memory.5\n4In particular, in [ 24], they distinguish internal nodes between minor and major, minor being the nodes\nthat have degree < aor have a small rank. All external nodes are major.\n5Unfortunately, it remains rather unclear from the description in the article exactly which parts of\nthe skiplist “towers” of pointers of an element are materialized. Unlike in the unweighted case, an\ninput could have large and small weight alternating, with half the elements of height ≈logbW. Fully\nmaterializing the towers would incur Ω(nlogbW)space; otherwise this seems to require a sophisticated\nscheme to materialize towers on demand, e.g., upon insertions, and we are not aware of a solution.\n\n6 Lazy B-Trees\nTo our knowledge, no data structure is known that achieves O/parenleftbig\nlogBW\nw(e)/parenrightbig\nI/Os for a\ndynamic weighted set in external memory while using O(N/B )blocks of memory.\nDeferred Data Structures & Database Cracking. Deferred data structuring refers to the\nidea of successively establishing a query-driven structure on a dataset. This is what lazy\nsearch trees do upon queries. While the term is used more generally, it was initially proposed\nin [35] on the example of a (sorted) dictionary. For an offline sequence of qqueries on a given\n(static) set of Nelements, their data structure uses O(Nlogq+qlogN)time. In [ 20], this\nis extended to allow update operations in the same time (where qnow counts all operations).\nThe refined complexity taking query ranksinto account was originally considered for the\n(offline) multiple selection problem: when qranks r1<···< rqare sought, leaving gaps\n∆1, . . . , ∆q+1,Θ/parenleftbig/summationtextq+1\ni=1|∆i|log(N/∆i)/parenrightbig\ncomparisons are necessary and sufficient [ 22,33]. For\nmultiple selection in external memory, Θ/parenleftbig/summationtextq+1\ni=1|∆i|\nBlogM/BN\n|∆i|/parenrightbig\nI/Os are necessary and\nsufficient [7, 17], even cache-obliviously [17, 18].\nClosest to lazy search trees is the work on online dynamic multiple selection by Barbay\net al. [6,7], where online refers to the query ranks arriving one by one. As pointed out\nin [44], the crucial difference between all these works and lazy search trees is that the analysis\nof dynamic multiple selection assumes that every insertion is preceded by a query for the\nelement, which implies that insertions must take Ω(logN)time. (They assume a nonempty\nset of elements to initialize the data structure with, for which no pre-insertion queries are\nperformed.) Barbay et al. also consider online dynamic multiple selection in external memory.\nBy maintaining a B-tree of the pivots for partitioning, they can support updates – again,\nimplicitly preceded by a query – at a cost of Θ(logBN)I/Os each.\nIn the context of adaptive indexing of databases, deferred data structuring is known\nunder the name of database cracking [30, 29, 27]. While the focus of research is on systems\nengineering, e.g., on the partitioning method [ 43], some theoretical analyses of devised\nalgorithms have also appeared [ 50,49]. These consider the worst case for qqueries on N\nelements similar to the original works on deferred data structures.\n1.4 Contribution\nOur main contribution, the lazy B-tree data structure, is summarized in Theorem 1 below.\n▶Theorem 1 (Lazy B-Trees) .There exists a data structure over an ordered set, that supports\nConstruct (S)in worst-caseO(|S|/B)I/Os,\nInsertin worst-caseO/parenleftbig\nlogBN\n|∆i|+ logBlogB|∆i|/parenrightbig\nI/Os,\nDelete in amortizedO/parenleftbig\nlogBN\n|∆i|+1\nBlog2|∆i|+ logBlogB|∆i|/parenrightbig\nI/Os,\nChangeKey in worst-caseO/parenleftbig\nlogBlogB|∆i|/parenrightbig\nI/Os if the element is moved towards the\nnearest queried element but not past it, and amortized O/parenleftbig1\nBlog2|∆i|+ logBlogB|∆i|/parenrightbig\nI/Os otherwise, and\nQueryElement andQueryRank in amortized\nO/parenleftbig\nlogBmin{N, q}+1\nBlog2|∆i|+ logBlogB|∆i|+1\nBxlog2c/parenrightbig\nI/Os.\nHere Ndenotes the size of the current set, |∆i|denotes the size of the manipulated gap, q\ndenotes the number of performed queries, and xandcxforc≥1are the resulting sizes of\nthe two gaps produced by a query. The space usage is O(N/B )blocks.\nFrom the above theorem, the following corollary can be derived, which states the perfor-\nmance of lazy B-trees when used as a priority queue.\n\nC. M. Rysgaard, S. Wild. 7\n▶Corollary 2 (Lazy B-Trees as external PQ) .A lazy B-tree may be used as a priority queue,\nto support, within O(N/B )blocks of space, the operations\nConstruct (S)in worst-caseO(|S|/B)I/Os,\nInsertin worst-caseO(logBlogBN)I/Os,\nDelete in amortizedO/parenleftbig1\nBlog2N+ logBlogBN/parenrightbig\nI/Os,\nDecreaseKey in worst-caseO(logBlogBN)I/Os, and\nMinimum in amortizedO/parenleftbig1\nBlog2N+ logBlogBN/parenrightbig\nI/Os.\nThe running times of lazy B-trees when used as a priority queue are not competitive with\nheaps targeting sorting complexity (such as buffer trees [ 3,4]); however these data structures\ndo not (and cannot [ 23]) support decrease-key efficiently. By contrast, for very large N, lazy\nB-trees offer exponentially faster decrease-key and insert than previously known external\npriority queues, while only moderately slowing down delete-min queries.\nOur key technical contribution is a novel technique for partially supporting external\nbiased search tree performance, formally stated in Theorem 3 below. In the language of\na biased search tree, it supports searches (by value or rank) as well as incrementing or\ndecrementing6a weight w(e)by1inO(logB(W/w (e)))I/Os for an element eor weight\nw(e); inserting or deleting an element, however, takes O(logBN)I/Os irrespective of weight,\nwhere Nis the number of elements currently stored. Unlike previous approaches, the space\nusage is theO(N/B )blocks throughout. A second technical contribution is the streamlined\npotential-function-based analysis of the interval data structure of lazy search trees.\nWe mention two, as yet insurmountable, shortcomings of lazy B-trees. The first one is\nthelog log Nterm we inherit from the original Lazy search trees [ 44]. This cost term is in\naddition to the multiple-selection lower bound and thus not necessary. Indeed, it was in\ninternal memory subsequently removed [ 46], using an entirely different representation of gaps,\nwhich fundamentally relies on soft-heap-based selection on a priority queue [ 34]. The route to\nan external memory version of this construction is currently obstructed by two road blocks.\nFirst, we need an external-memory soft heap; the only known result in this direction [ 11]\nonly gives performance guarantees when N=O/parenleftbig\nB(M/B )M/2(B+√\nM/B )/parenrightbig\nand hence seems\nnot to represent a solution for the general problem. Second, the selection algorithm from [ 46]\nrequires further properties of the priority queue implementation, in particular a bound on\nthe fanout; it is not clear how to combine this with known external priority queues.\nThe second shortcoming is that – unlike for comparisons – we do not get close to the\nI/O-lower bound for multiple-selection with lazy B-trees. Doing so seems to require a way\nof buffering as in buffer trees, to replace our fanout of Bby a fanout of M/B. This again\nseems hard to achieve since an insertion in the lazy search tree uses a queryon the gap data\nstructure, and a queryon the lazy search tree entails an insertion into the gap data structure\n(plus a re-weighting operation).\nOutline. The remainder of the paper is structured as follows. Section 2 describes the\ngap data structure (our partial external biased search tree) and key innovation. Section 3\nsketches the changes needed to turn the interval data structure from [ 44] into an I/O-efficient\ndata structure; the full details, including our streamlined potential function, appear in\nAppendix A. In Section 4, we then show how to assemble the pieces into a proof of our main\nresult, Theorem 1. We conclude in Section 5 with some open problems. To be self-contained,\nwe include proofs of some technical lemmas used in the analysis in Appendix B.\n6General weight changes are possible in that time with w(e)the minimum of the old and new weight.\n\n8 Lazy B-Trees\n2The Gap Structure: A New Restricted External Biased Search Tree\nIn this section we present a structure on the gaps, which allows for the following operations.\nLetNdenote the total number of elements over all gaps and Gdenote the number of gaps.\nNote that G≤N, as no gaps are empty. We let ∆idenote the ith gap when sorted by\nelement, and|∆i|denote the number of elements contained in gap ∆i.\nGapByElement (e)Locates the gap ∆icontaining element e.\nGapByRank (r)Locates the gap ∆icontaining the element of rank r.\nGapIncrement (∆i)/GapDecrement (∆i)Changes the weight of gap ∆iby1.\nGapSplit (∆i,∆′\ni,∆′\ni+1)Splits gap ∆iinto the two non-overlapping gaps ∆′\niand∆′\ni+1,\ns.t. the elements of ∆iare equal to the elements of ∆′\niand∆′\ni+1.\nFor the operations, we obtain I/O costs, as described in the theorem below.\n▶Theorem 3 (Gap Data Structure) .There exists a data structure on a weighted ordered set,\nthat supports GapByElement ,GapByRank ,GapIncrement andGapDecrement in\nO/parenleftbig\nlogBN\n|∆i|/parenrightbig\nI/Os, and GapSplit inO(logBG)I/Os. Here Ndenotes the total size of all\ngaps,|∆i|denotes the size of the touched gap and Gdenotes the total number of gaps. The\nspace usage isO(G/B )blocks.\n▶Remark 4 (Comparison with biased search trees). A possible solution would be an external-\nmemory version of biased search trees, but as discussed in the introduction, no fully satisfac-\ntory such data structure is known. Instead of supporting all operations of biased trees in full\ngenerality, we here opt for a solution solving (just) the case at hand. The solution falls short\nof a general biased search trees, as the insertion or deletion costs are not a function of the\nweightof the affected gap, but the total number Gof gaps in the structure. Moreover, we\nonly describe how to change the weight of gaps by 1; however, general weight changes could\nbe supported, with the size of the change entering the I/O cost, matching what we expect\nfrom a general biased search tree.\nNote that the gaps in the structure are non-overlapping, and that their union covers\nthe whole element range. The query for an element contained in some gap is therefore a\npredecessor query on the left side of the gaps, however, as their union covers the entire\nelement range, the queries on the gap structure behave like exact queries : we can detect\nwhether we have found the correct gap and can then terminate the search. (By definition,\nthere are no unsuccessful searches in our scenario, either.)\nForconsistencywiththenotationofbiasedsearchtrees, wewriteinthefollowing wi=|∆i|\nandW=/summationtext\niwi. Note that W=N. Consider a conceptual list, where the gaps are sorted\ndecreasingly by weight, and let gap ∆ibe located at some index ℓin this list. This conceptual\nlist is illustrated in Figure 2, and Figure 3 gives a two-dimensional view of the list, which\nconsiders both the gap weight as well as the gaps ordered by element. As the total weight\nbefore index ℓin the conceptual list is at most Wand the weight of each gap before index ℓ\nis at least wi, then it must hold that ℓ≤W\nwi. If we were to search for a gap of known\nweight, it can therefore be found with an exponential search [ 10] in timeO(logℓ)=O/parenleftbig\nlogW\nwi/parenrightbig\n.\nHowever, searches are based on element values (and, e.g., for insertions, without knowing\nthe target gap’s weight), so this alone would not work.\n2.1 Buckets by Weight and Searching by Element\nInstead, the gaps are split into buckets bj, s.t. the weight of all gaps in bucket bjis greater\nthan or equal to the weight of all gaps in bucket bj+1. We further impose the invariant\n\nC. M. Rysgaard, S. Wild. 9\n0 1 2 3 4 5 6 7 8 . . . ℓ. . .∆4∆9∆7∆2∆8∆6∆3∆5∆1 ∆ib0 b1 b2 early late buckets\nFigure 2 The conceptual list of the gaps. The gaps are sorted by decreasing weight, with the\nheaviest gap (largest weight), ∆4at index 0. Gap ∆iwith weight wiis stored at index ℓ≤W/w iin\nthe list. The gaps are split into buckets b0, b1, b2, . . .of doubly exponential size.\nelement value\nsmall largegap weight\nlight heavy∆4\n∆9\n∆7\n∆2\n∆8\n∆6\n∆3\n∆5\n∆1\nearly\nb0\nb1\nb2late\nFigure 3 Two dimensional view of the first gaps of Figure 2 represented by rectangles, with\nthe width of the rectangle denoting the weight (size) of the gap. The horizontal axis sorts gaps by\nelement value; the vertical axis by weight. Dashed lines show bucket boundaries.\nthat the size of (number of gaps in) bucket bjisexactly B2jfor all but the last bucket,\nwhich may be of any (smaller) size. Each bucket is a B-tree containing the gaps in the\nbucket, sorted by element value. The time to search for a given element in bucket bjis\nthereforeO(logB(|bj|))=O/parenleftbig\n2j/parenrightbig\nI/Os. For consistent wording, we will in the following\nalways use smaller/larger to refer to the order by element values, lighter/heavier for gaps of\nsmaller/larger weight/size, and earlier/later for buckets of smaller/larger index. Note that\nearlier buckets contain fewer, but heavier gaps.\nGapByRank .A search for a gap proceeds by searching in buckets b0, b1, b2, . . .until\nthe desired gap is found in some bucket bk. To search in all buckets up until bkrequires/summationtextk\nj=0O/parenleftbig\n2j/parenrightbig\n=O/parenleftbig\n2k/parenrightbig\nI/Os, which is therefore up to constant factors the same cost as\nsearching in only bucket bk. Consider some gap ∆i, which has the ℓth heaviest weight, i.e., it\nis located at index ℓ, when sorting all gaps decreasingly by weight. By the invariant, the\nbuckets are sorted decreasingly by the weight of their internal elements. Let the bucket\ncontaining ∆ibebk. It must then hold that the sizes of the earlier buckets does not allow\nfor∆ito be included, but that bucket bkdoes. Therefore,\nk−1/summationdisplay\nj=0|bj|< ℓ≤k/summationdisplay\nj=0|bj|.\nAs|bj|=B2j, the sums are asymptotically equal to the last term of the sum up to constant\nfactors (Lemma 10 in Appendix B). It then holds that ℓ=O/parenleftbig\nB2k/parenrightbig\nandℓ= Ω/parenleftbig\nB2k−1/parenrightbig\n=\nΩ/parenleftbig\n(B2k)1/2/parenrightbig\n. Thus logB(ℓ) = Θ (logB|bk|), and conversely 2k= Θ(logBℓ). The gap ∆ican\nthus be found using O/parenleftbig\nlogBW\nwi/parenrightbig\nI/Os, concluding the GapByElement operation.\n\n10 Lazy B-Trees\n2.2 Updating the Weights\nBoth explicit weight changes in GapIncrement /GapDecrement as well as the GapSplit\noperation require changes to the weights of gaps. Here, we have to maintain the sorting of\nbuckets by weight.\nGapIncrement /GapDecrement .When performing GapIncrement on gap ∆i,\nthe weight is increased by 1. In the conceptual list, this may result in the gap moving to an\nearlier index, with the order of the other gaps the same. This move is made in the conceptual\nlist (Figure 2) by swapping ∆iwith its neighbor. As the buckets are ordered by element\nvalue, swapping with a neighbor (by weight) in the same bucket does not change the bucket\nstructure. When swapping with a neighbor in an earlier bucket, however, the bucket structure\nmust change to reflect this. Following the conceptual list (Figure 2), this gap is a lightest\n(minimum-weight) gap in bucket bk−1. This may then result in ∆imoving out of its current\nbucket bk, into some earlier bucket. As the gap moves to an earlier bucket, some other gap\nmust take its place in bucket bk. Following the conceptual list, it holds that this gap is a\nlightest (minimum-weight) gap in bucket bk−1, and so on for the remaining bucket moves.\nWhen performing GapDecrement , the gap may move to a later bucket, where the\nheaviest gap in bucket bk+1is moved to bucket bk.\nIn both cases, a lightest gap in one bucket is swapped with a heaviest gap in the\nneighboring (later) bucket. To find lightest or heaviest gaps in a bucket, we augment the\nnodes of the B-tree of each bucket with the values of the lightest and heaviest weights in the\nsubtree for each of its children. This allows for finding a lightest or heaviest weight gap in\nbucket bjefficiently, usingO(logB|bj|)I/Os. Further, this augmentation can be maintained\nunder insertions and deletions.\nUpon a GapIncrement orGapDecrement operation, the desired gap ∆ican be located\nin the structure using O/parenleftbig\nlogBW\nwi/parenrightbig\nI/Os, and the weight is adjusted. We must then perform\nswaps between the buckets, until the invariant that buckets are sorted decreasingly by weight\nholds. This swapping may be performed on all buckets up to the last touched bucket. For\nGapIncrement theswappingonlyappliestoearlierbucketsastheweightof ∆iincreases, and\nthe height of the tree in the latest bucket is O/parenleftbig\nlogBW\nwi/parenrightbig\n. ForGapDecrement the swapping\nis performed into later buckets, but only up to the final landing place for weight wi−1. If\nwi≥2, the height of the tree in the last touched bucket is O/parenleftbig\nlogBW\nwi−1/parenrightbig\n=O/parenleftbig\n1 +logBW\nwi/parenrightbig\n.\nIfwi= 1, gap ∆iis to be deleted. The gap is swapped until it is located in the last bucket,\nfrom where it is removed. We have O/parenleftbig\nlogBW\nwi/parenrightbig\n=O/parenleftbig\nlogBW/parenrightbig\n, aswi= 1, whereas the height\nof the last bucket is O(logBG)=O(logBW), asG≤W. Therefore both GapIncrement\nandGapDecrement can be performed using O/parenleftbig\nlogBW\nwi/parenrightbig\nI/Os.\nGapSplit .When GapSplit is performed on gap ∆i, the gap is removed and replaced\nby two new gaps ∆′\niand∆′\ni+1, s.t.|∆i|=|∆′\ni|+|∆′\ni+1|. In the conceptual list, the new gaps\nmust reside at later indexes than ∆i. As the sizes of the buckets are fixed, and the number\nof gaps increases, some gap must move to the last bucket. Similarly to GapIncrement\nandGapDecrement , the order can be maintained by performing swaps of gaps between\nconsecutive buckets: First, we replace ∆i(in its spot) by ∆′\ni; if this violates the ordering of\nweights between buckets, we swap ∆′\niwith the later neighboring bucket, until the correct\nbucket is reached. Then we insert ∆′\ni+1into the last bucket and swap it with its earlier\nneighboring bucket until it, too, has reached its bucket. Both processes touch at most all\nlogBlog2Gbuckets and spend a total of O(logBG)I/Os.\n\nC. M. Rysgaard, S. Wild. 11\nelement value\nsmall largegap weight\nlight heavy b0\nb1\nb2\nb3 early late∆11∆13\n∆18\n∆21\nFigure 4 Illustration of “holes”. The figure shows the two-dimensional view of gaps described in\nFigure 3 (gap widths not to scale). The holebetween ∆11and∆21in bucket b1, marked by the\ndotted line, contains the total weight of all gaps marked in gray. These are precisely the gaps that\nfall both (a) between ∆11and∆21by element value and (b) in later buckets by (lighter) weight.\nNote specifically that ∆13and∆18arenotcounted in the hole, as they reside in an earlier bucket.\n2.3 Supporting Rank Queries\nThe final operation to support is GapByRank . Let the global rank of a gap ∆i,r(∆i) =\n|∆1|+···+|∆i−1|, denote the rank of the smallest element located in the gap, i.e., the\nnumber of elements residing in gaps of smaller elements. The local rank of a gap ∆iin bucket\nbj,rj(∆i), denotes the rank of the gap among elements located in bucket bjonly.\nComputing the rank of a gap. To determine the global rank of a gap ∆i, the total weight\nof all smaller gaps must be computed. This is equivalent to the sum of the local ranks of ∆i\nover all buckets: r(∆i) =/summationtext\njrj(∆i). First we augment the B-trees of the buckets, such that\neach node contains the total weight of all gaps in the subtree. This allows us to compute the\nlocal rank rj(∆i)of a gap ∆iinside a bucket bjusingO(logB|bj|)I/Os. The augmented\nvalues can be maintained under insertions and deletions in the tree within the stated I/O\ncost. When searching for the gap ∆i, the local rank rj(∆i)of all earlier buckets bjup to the\nbucket bk, which contains ∆i, can then be computed using O/parenleftbig\nlogBW\nwi/parenrightbig\nI/Os in total.\nIt then remains to compute the total size of gaps smaller than ∆iin all buckets after bk,\ni.e., the sum of the local ranks rℓ(∆i)for all later buckets bℓ,ℓ > k, to compute in total the\nglobal rank r(∆i). As these buckets are far bigger, we cannot afford to query them. Note\nthat any gaps in these later buckets must fall between two consecutive gaps in bk, as the gaps\nare non overlapping in element-value space. Denote the space between two gaps in a bucket\nas ahole. We then further augment the B-tree to contain in each hole of bucket bjthe total\nsize of gaps of laterbuckets contained in that hole, and let each node contain the total size of\nall holes in the subtree. See Figure 4 for an illustration of which gaps contributes to a hole.\nThis then allows computing the global rank r(∆i)of gap ∆i, by first computing the local\nrank rj(∆i)in all earlier buckets bj, i.e., for all j≤k, and then adding the total size of all\nsmaller holes in bk. The smaller holes in bkexactly sum to the local ranks rℓ(∆i)for all later\n\n12 Lazy B-Trees\nbuckets bℓforℓ > k. As this in total computes/summationtext\njrj(∆i), then by definition, we obtain the\nglobal rank r(∆i)of the gap ∆i.\nGapByRank .For a GapByRank operation, some rank ris given, and the gap ∆iis to\nbe found, s.t. ∆icontains the element of rank r, i.e., the global rank r(∆i)≤r < r (∆i+1).\nThe procedure is as follows. First, the location of the queried rank ris computed in the\nfirst bucket. If this location is contained in a gap ∆iof the bucket, then ∆imust be the\ncorrect gap, which is then returned. Otherwise, the location of rmust be in a hole. As the\nsizes of the gaps contained in the first bucket is not reflected in the augmentation of later\nbuckets, the value of ris updated to reflect the missing gaps of the first bucket: we subtract\nfrom tthe local rank r0(∆i)of the gap ∆iimmediately after the hole containing the queried\nrank r. This adjusts the queried rank rto be relative to elements in gaps of later buckets.\nPut differently, the initial queried rank ris the queried global rank, which is the sum local\nranks; we now remove the first local rank again. This step is recursively applied to later\nbuckets, until the correct gap ∆iis found in some bucket bk, which contains the element of\nthe initially queried global rank r. As the correct gap ∆imust be found in the bucket bk\ncontaining it, the GapByRank operation usesO/parenleftbig\nlogBW\nwi/parenrightbig\nI/Os to locate it.\nMaintaining hole sizes. The augmentation storing the sizes of all holes in a subtree can be\nupdated efficiently upon insertions or deletions of gaps in the tree, or upon updating the size\nof a hole. However, computing the sizes of the holes upon updates in the tree is not trivial.\nIf a gap is removed from a bucket, the holes to the left resp. right of that gap are merged\ninto a single hole, with a size equal to the sum of the previous holes. If the removed gap\nis moved to a later bucket, the hole must now also include the size of the removed gap. A\nnewly inserted gap must, by the non-overlapping nature of gaps, land within a hole of the\nbucket, which is now split in two. If the global rank of the inserted gap is known, then the\nsizes of the resulting two holes can be computed, s.t. the global rank is preserved.\nIn the operations on the gap structure, GapByElement orGapByRank do not\nchange the sizes of gaps, so the augmentation does not change. Upon a GapIncrement or\nGapDecrement operation, the size of some gap ∆ichanges. This change in size can be\napplied to all holes containing ∆iin earlier buckets using O/parenleftbig\nlogBW\nwi/parenrightbig\nI/Os in total. Then\nswaps are performed between neighboring buckets until the invariant that buckets are sorted\nby weight, holds again. During these swaps, the sizes of gaps do not change, which allows for\ncomputing the global rank of the gaps being moved, and update the augmentation without\nany overhead in the asymptotic number of I/Os performed. The total number of I/Os to\nperform a GapIncrement orGapDecrement operation does not increase asymptotically,\nand therefore remains O/parenleftbig\nlogBW\nwi/parenrightbig\n.\nWhen a GapSplit is performed, a single gap ∆iis split. As the elements in the two new\ngaps ∆′\niand∆′\ni+1remain the same as those of the old gap ∆i, there cannot be another gap\nbetween them. The value in all smaller holes therefore remains correct. Moving ∆′\ni+1to\nthe last bucket then only needs updating the value of the holes of the intermediate buckets,\nwhich touches at most all logBlog2Gbuckets spending O(logBG)I/Os in total. Swaps are\nthen performed, where updating the augmentation does not change the asymptotic number\nof I/Os performed.\nSpace Usage. To bound the space usage, note that the augmentation of the B-trees at\nmost increase the node size by a constant factor. Since the space usage of a B-tree is linear\n\nC. M. Rysgaard, S. Wild. 13\nin the number of stored elements, and since each gap is contained in a single bucket, the\nspace usage isO(G/B )blocks in total. This concludes the proof of Theorem 3.\n3 The Interval Structure\nA gap ∆icontains all elements in the range the gap covers. Note that by Theorem 1, a\nquery on the gap for an element must be fasterfor elements closerto the border of a gap;\ninformation-theoretically speaking, such queries reveal less information about the data. It is\ntherefore not sufficient to store the elements in the gap in a single list.\nExternal memory interval data structure. We follow the construction of Sandlund and\nWild [44], in their design of the interval structure. In this section we present the construction,\nand argue on the number of I/Os this construction uses. The main difference from the\noriginal construction is in the speed-up provided by the external-memory model; namely that\nscanning a list is faster by a factor B, and that B-trees allows for more efficient searching.\nThese differences are also what allows for slightly improving the overall I/O cost of the\noriginal construction, when moving it to the external-model. Due to space constraints, the\nfull analysis can be found in Appendix A.\nWe allow for the following operations:\nIntervalsInsert (e):Inserts element einto the structure.\nIntervalsDelete (ptr):Deletes the element eat pointer ptrfrom the structure.\nIntervalsChange (ptr, e′):Changes the element eat pointer ptrto the element e′.\nIntervalSplit (e)orIntervalSplit (r):Splits the set of intervals into two sets of\nintervals at element eor rank r.\nA gap has a “sidedness” , which denotes the number of sides the gap has had a query.\nDenote a side as a queried side , if that rank has been queried before (cf. [ 44]). If there have\nbeen no queries yet, the (single) gap is a 0-sided gap. When a query in a 0-sided gap occurs,\ntwo new gaps are created which are both 1-sided. Note that “ 1-sided” does not specify which\nside was queried – left or right. When queries have touched both sides, the gap is 2-sided.\nWe obtain the following I/O costs for the above operations.\n▶Theorem 5 (Interval Data Structure) .There exists a data structure on an ordered set,\nmaintaining a set of intervals, supporting\nIntervalsInsert in worst-caseO(logBlogB|∆i|)I/Os,\nIntervalsDelete in amortizedO/parenleftbig1\nBlog2|∆i|+ logBlogB|∆i|/parenrightbig\nI/Os,\nIntervalsChange in worst-caseO(logBlogB|∆i|)I/Os, if the element is moved towards\nthe nearest queried side or amortized O/parenleftbig1\nBlog2|∆i|+ logBlogB|∆i|/parenrightbig\nI/Os otherwise, and\nIntervalSplit in amortizedO/parenleftbig1\nBlog2|∆i|+ logBlogB|∆i|+1\nBxlog2c/parenrightbig\nI/Os.\nHere|∆i|denotes the number of elements contained in all intervals, and xandcxforc≥1\nare the resulting sizes of the two sets created by a split. The space usage is O(|∆i|/B)blocks.\nIntervals in external memory. Let the gap ∆icontain multiple non-overlapping intervals\nIi,j, which contain the elements located in the gap. The elements of the intervals are sorted\nbetween intervals, but not within an interval. Intervals therefore span a range of elements\nwith known endpoints. Each such interval is a blocked-linked-list containing the elements\nof the interval. Additionally, we store an augmented B-tree over the intervals, allowing for\nefficiently locating the interval containing a given element (using O(logB(#intervals ))I/Os).\n\n14 Lazy B-Trees\nThe B-tree is augmented to hold in each node the total sizes of all intervals in the subtrees\nof the node, which allows for efficient rank queries.\nBy packing all intervals into a single blocked-linked-list, and noting that there can be no\nmore intervals than there are elements, the space usage of the intervals and the augmented\nB-tree over the intervals is O(|∆i|/B)blocks.\nIntuition of interval maintenance of [ 44].Intuitively, there is a trade-off in maintaining\nintervals: having many small intervals reduces future query costs since these are typically\ndominated by the linear cost of splitting one interval; but it increases the time to search\nfor the correct interval upon insertions (and other operations). Lazy search trees handle\nthis trade-off as follows. To bound the worst case insertion costs, we enforce a hard limit of\nO(log2|∆i|)on the number of intervals in a gap ∆i, implemented via a merging rule. To\namortize occasional high costs for queries, we accrue potential for any intervals that have\ngrown “too large” relative to their proximity to a gap boundary.\nGiven the logarithmic number of intervals, the best case for queries would be to have\ninterval sizes grow exponentially towards the middle. This makes processing of intervals close\nto the boundary (i.e., close to previous queries) cheap; for intervals close to the middle, we\ncan afford query costs linear in ∆i. It can be shown that for exponentially growing intervals,\nthe increase of the lower bound from any query allows us to precisely pay for the incurred\nsplitting cost. However, the folklore rule of having each interval, say, 2–4 times bigger than\nthe previous interval seems too rigid to maintain. Upon queries, it triggers too many changes.\nMerging & Potential. We therefore allow intervals to grow bigger than they are supposed\nto be, but charge them for doing so in the potential. By enforcing the merging rule when\nthe number of elements in the gap decreases, we maintain the invariant that the number\nof intervals isO(log2|∆i|), which allows for locating an interval using O(logBlog2|∆i|)=\nO(logBlogB|∆i|)I/Os. Enforcing the merging rule is achieved by scanning the intervals\nusing the B-tree, and it can be shown that this operation uses amortized O/parenleftbig1\nBlog2|∆i|/parenrightbig\nI/Os. Merging intervals takes O(1)I/O, but by adding extra potential to intervals, this step\nof the merge can be amortized away.\nUpon this, we can show that IntervalsInsert usesO(logBlogB|∆i|)I/Os, and that\nIntervalsDelete uses amortizedO/parenleftbig1\nBlog2|∆i|+ logBlogB|∆i|/parenrightbig\nI/Os. When performing\nIntervalsChange , moving the element from one interval to another uses O(logBlogB|∆i|)\nI/Os. However, the potential function causes an increase of O/parenleftbig1\nBlog2|∆i|/parenrightbig\nI/Os in the\namortized cost when an element is moved away from the closest queried side of the gap.\nWhenaqueryoccurs, aninterval Ii,jissplitaroundsomeelementorrank. Theintervalcan\nbe located using the B-tree over intervals on either element or rank in O(logBlogB|∆i|)I/Os.\nForsplittingaroundanelement, theintervalisscannedandpartitionedusing O(|Ii,j|/B)I/Os.\nForsplittingaroundarank, deterministicselection[ 12]isapplied, whichuses O(|Ii,j|/B)I/Os\n(see, e.g., [ 17,18]). In both cases the number of I/Os grows with the interval size. Analyzing\nthe change in the potential function upon this split, we can show that IntervalSplit uses\namortizedO/parenleftbig1\nBlog2|∆i|+ logBlogB|∆i|+1\nBxlog2c/parenrightbig\nI/Os.\nThis (together with the details from Appendix A) concludes the proof of Theorem 5.\n4 Lazy B-Trees\nIn this section, we combine the gap structure of Section 2 and the interval structure of\nSection 3, to achieve an external-memory lazy search-tree. Recall our goal, Theorem 1.\n\nC. M. Rysgaard, S. Wild. 15\n▶Theorem 1 (Lazy B-Trees) .There exists a data structure over an ordered set, that supports\nConstruct (S)in worst-caseO(|S|/B)I/Os,\nInsertin worst-caseO/parenleftbig\nlogBN\n|∆i|+ logBlogB|∆i|/parenrightbig\nI/Os,\nDelete in amortizedO/parenleftbig\nlogBN\n|∆i|+1\nBlog2|∆i|+ logBlogB|∆i|/parenrightbig\nI/Os,\nChangeKey in worst-caseO/parenleftbig\nlogBlogB|∆i|/parenrightbig\nI/Os if the element is moved towards the\nnearest queried element but not past it, and amortized O/parenleftbig1\nBlog2|∆i|+ logBlogB|∆i|/parenrightbig\nI/Os otherwise, and\nQueryElement andQueryRank in amortized\nO/parenleftbig\nlogBmin{N, q}+1\nBlog2|∆i|+ logBlogB|∆i|+1\nBxlog2c/parenrightbig\nI/Os.\nHere Ndenotes the size of the current set, |∆i|denotes the size of the manipulated gap, q\ndenotes the number of performed queries, and xandcxforc≥1are the resulting sizes of\nthe two gaps produced by a query. The space usage is O(N/B )blocks.\nWe use the I/O bounds shown in Theorems 3 and 5 to bound the cost of the combined\noperations. These operations are performed as follows.\nAConstruct (S)operation is performed by creating a single gap over all elements in S,\nand in the gap create a single interval with all elements. This can be done by a single scan\nofS, and assuming that Sis represented compactly/contiguously, this uses O(|S|/B)I/Os.\nTo perform an Insert (e)operation, GapByElement is performed to find the gap ∆i\ncontaining element e. Next, IntervalsInsert is performed to insert einto the interval\nstructure of gap ∆i. Finally, GapIncrement is performed on ∆i, as the size has increased.\nIn total this uses worst-case O/parenleftbig\nlogBN\n|∆i|+ logBlogB|∆i|/parenrightbig\nI/Os.\nSimilarly, upon a Delete (ptr)operation, IntervalsDelete is performed using ptr, to\nremove the element eat the pointer location. Next GapDecrement is performed on the\ngap∆i. In total this uses amortized O/parenleftbig\nlogBN\n|∆i|+1\nBlog2|∆i|+ logBlogB|∆i|/parenrightbig\nI/Os.\nAChangeKey operation may only change an element, s.t. it remains within the same\ngap (otherwise we need to use Delete andInsert). This operation therefore does not need\nto perform operations on the gap structure, but only on the interval structure of the relevant\ngap. The operation is therefore performed directly using the IntervalsChange operation.\nTheQueryElement andQueryRank operations are performed in similar fashions.\nFirst, GapByElement orGapByRank is performed to find the relevant gap ∆icon-\ntaining the queried element. Next IntervalSplit is performed on the interval struc-\nture, which yields two new gaps ∆′\niand ∆′\ni+1, which is updated into the gap struc-\nture using GapSplit . Note that the number of gaps is bounded by the number of el-\nements N, but also by the number of queries qperformed, as only queries introduce\nnew gaps. In total, the QueryElement andQueryRank operations uses amortized\nO/parenleftbig\nlogBmin{N, q}+1\nBlog2|∆i|+ logBlogB|∆i|+1\nBxlog2c/parenrightbig\nI/Os.\nThe space usage of the gap structure is O(G/B )=O(N/B )blocks. For gap ∆i, the space\nusage of the interval structure is O(|∆i|/B)blocks. If|∆j|=O(B), for some ∆j, we cannot\nsimply sum over all the substructures. By instead representing all such “lightweight” gaps\nin a single blocked-linked-list, we obtain that the space usage over the combined structure\nisO(N/B )blocks. The gaps are located in the combined list using the gap structure, and\nupdates or queries to the elements of the gap may then be performed using O(1)I/Os,\nallowing the stated time bounds to hold. Similarly, an interval structure may be created for\na gap, when it contains Ω(B)elements usingO(1)I/Os. The stated time bounds therefore\nstill applies to the lightweight gaps.\nThis concludes the proof of Theorem 1.\n\n16 Lazy B-Trees\nPriority Queue Case. When a lazy B-tree is used a priority queue, the queries are only\nperformed on the element of smallest rank, and this element is deleted before a new query is\nperformed. If this behavior is maintained, we first note that the structure only ever has a\nsingle 1-sided gap of size N, with the queried side to the left. This allows for Insertto be\nperformed in worst-case O(logBlogBN)I/Os, as the search time in the gap structure reduces\ntoO(1). Similarly, Delete is performed using amortized O/parenleftbig1\nBlog2N+ logBlogBN/parenrightbig\nI/Os.\nTo perform the DecreaseKey operation, a ChangeKey operation is performed. As the\nqueried side is to the left, this must move the element towards the closest queried side,\nleading to a DecreaseKey operation being performed using worst-case O(logBlogBN)\nI/Os. Finally, Minimum is performed as a QueryRank (r)with r= 1. As this must\nsplit the gap at x= 1andcx=N−1, the operation is performed using amortized\nO/parenleftbig1\nBlog2N+ logBlogBN/parenrightbig\nI/Os. This concludes the proof of Corollary 2.\n5 Open Problems\nThe internal lazy search trees [ 44] discuss further two operations on the structure; Split\nandMerge, which allows for splitting or merging disjoint lazy search tree structures around\nsome query element or rank. These operations are supported as efficient as queries in the\ninternal-model. In the external-memory case, the gap structure designed in this paper,\nhowever, does not easily allow for splitting or merging the set of gaps around an element\nor rank, as the gaps are stored by element in disjoint buckets, where all buckets must be\nupdated upon executing these extra operations. By sorting and scanning the gaps, the\noperations may be supported, but not as efficiently as a simple query, but instead in sorting\ntime relative to the number of gaps. It remains an open problem to design a gap structure,\nwhich allows for efficiently supporting SplitandMerge.\nIn the external-memory model, bulk operations are generally faster, as scanning consecu-\ntive elements saves a factor BI/Os. One such operation is RangeQuery , where a range\nof elements may be queried and reported at once. In a B-tree, this operation is supported\ninO(logBN+k/N)I/Os, when kelements are reported. The lazy B-tree designed in this\npaper allows for efficiently reporting the elements of a range in unsorted order, by querying\nthe endpoints of the range, and then reporting all elements of the gaps between the end-\npoints. Note that for the priority-queue case, this allows reporting the ksmallest elements in\nO/parenleftbig1\nBklog2N\nk+1\nBlog2N+ logBlogBN/parenrightbig\nI/Os. If the elements must be reported in sorted\norder, sorting may be applied to the reported elements. However, this effectively queries\nall elements of the range, and should therefore be reflected into the lazy B-tree. As this\nintroduces many gaps of small size, the I/O cost increases over simply sorting the elements\nof the range. It remains an open problem on how to efficiently support sorted RangeQuery\noperations, while maintaining the properties of lazy B-trees.\nIn the internal-memory model, an optimal version of lazy search trees was constructed [ 46],\nwhich gets rid of the added log log Nterm on all operations. It remains an open problem to\nsimilarly get rid of the added logBlogBNterm for the external-memory model.\nImprovements on external-memory efficient search trees and priority queues use buffering\nof updates to move more data in a single I/O, thus improving the I/O cost of operations.\nThe first hurdle to overcome in order to create buffered lazy B-trees is creating buffered\nbiased trees used for the gap structure. By buffering updates to the gaps, it must then hold\nthat the weights are not correctly updated, which imposes problems on searching; both by\nelement and rank. It remains an open problem to overcome this first hurdle as a step towards\nbuffering lazy B-trees.\n\nC. M. Rysgaard, S. Wild. 17\nReferences\n1Georgy M. Adelson-Velsky and Evgenii M. Landis. An algorithm for the organization of\ninformation. Proceedings of the USSR Academy of Sciences (in Russian) , 146:263–266, 1962.\n2Alok Aggarwal and Jeffrey S. Vitter. The input/output complexity of sorting and related\nproblems. Communications of the ACM , 31(9):1116––1127, September 1988. doi:10.1145/\n48529.48535 .\n3Lars Arge. The buffer tree: A new technique for optimal I/O-algorithms. In Workshop\non Algorithms and Data Structures (WADS) , page 334–345. Springer, 1995. doi:10.1007/\n3-540-60220-8_74 .\n4Lars Arge. The buffer tree: A technique for designing batched external data structures.\nAlgorithmica , 37(1):1–24, 2003. doi:10.1007/S00453-003-1021-X .\n5Amitabha Bagchi, Adam L Buchsbaum, and Michael T Goodrich. Biased skip lists. Algorith-\nmica, 42:31–48, 2005.\n6Jérémy Barbay, Ankur Gupta, Srinivasa Rao Satti, and Jon Sorenson. Dynamic online\nmultiselection in internal and external memory. In WALCOM: Algorithms and Computation ,\npages 199–209. Springer, 2015. doi:10.1007/978-3-319-15612-5_18 .\n7Jérémy Barbay, Ankur Gupta, Srinivasa Rao Satti, and Jon Sorenson. Near-optimal online\nmultiselection in internal and external memory. Journal of Discrete Algorithms , 36:3–17, 2016.\nWALCOM 2015.\n8Rudolf Bayer and Edward M. McCreight. Organization and maintenance of large ordered\nindices.Acta Informatica , 1:173–189, 1972. doi:10.1007/BF00288683 .\n9Samuel W. Bent, Daniel D. Sleator, and Robert E. Tarjan. Biased search trees. SIAM Journal\non Computing , 14(3):545–568, 1985.\n10Jon Louis Bentley and Andrew Chi-Chih Yao. An almost optimal algorithm for unbounded\nsearching. Information Processing Letters , 5(3):82–87, 1976. doi:10.1016/0020-0190(76)\n90071-5.\n11Alka Bhushan and Sajith Gopalan. External Memory Soft Heap, and Hard Heap, a Meld-\nable Priority Queue , page 360–371. Springer Berlin Heidelberg, 2012. doi:10.1007/\n978-3-642-32241-9_31 .\n12Manuel Blum, Robert W. Floyd, Vaughan R. Pratt, Ronald L. Rivest, and Robert Endre\nTarjan. Time bounds for selection. J. Comput. Syst. Sci. , 7(4):448–461, 1973. doi:10.1016/\nS0022-0000(73)80033-9 .\n13Prosenjit Bose, Karim Douïeb, and Stefan Langerman. Dynamic optimality for skip lists and\nB-trees. In Symposium on Discrete Algorithms (SODA) , pages 1106–1114. SIAM, 2008.\n14Gerth Stølting Brodal. Worst-case efficient priority queues. In Symposium on Discrete\nAlgorithms (SODA) , 1996.\n15Gerth Stølting Brodal and Rolf Fagerberg. Lower bounds for external memory dictionaries. In\nProceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms , pages\n546––554. Society for Industrial and Applied Mathematics, 2003.\n16Gerth Stølting Brodal and Jyrki Katajainen. Worst-case efficient external-memory priority\nqueues. In Stefan Arnborg and Lars Ivansson, editors, Algorithm Theory — SWAT’98 , pages\n107–118, Berlin, Heidelberg, 1998. Springer Berlin Heidelberg. doi:10.1007/BFb0054359 .\n17Gerth Stølting Brodal and Sebastian Wild. Funnelselect: Cache-oblivious multiple selection. In\nIngeLiGørtz, MartinFarach-Colton, SimonJ.Puglisi, andGrzegorzHerman, editors, European\nSymposium on Algorithms (ESA) , pages 25:1–25:17, 2023. doi:10.4230/LIPIcs.ESA.2023.25 .\n18Gerth Stølting Brodal and Sebastian Wild. Deterministic cache-oblivious funnelselect. In\nScandinavian Symposium on Algorithm Theory (SWAT) , 2024. arXiv:2402.17631 ,doi:\n10.4230/LIPIcs.SWAT.2024.17 .\n19Gerth Stølting Brodal, George Lagogiannis, and Robert E. Tarjan. Strict fibonacci heaps. In\nSymposium on Theory of Computing (STOC) , STOC’12, page 1177–1184. ACM, May 2012.\ndoi:10.1145/2213977.2214082 .\n\n18 Lazy B-Trees\n20Yu-Tai Ching, Kurt Mehlhorn, and Michiel H.M. Smid. Dynamic deferred data structuring.\nInformation Processing Letters , 35(1):37 – 40, 1990.\n21Erik D. Demaine, Dion Harmon, John Iacono, Daniel Kane, and Mihai Pˇ atraşcu. The geometry\nof binary search trees. In Symposium on Discrete Algorithms SODA 2009 , pages 496–505.\nSIAM, 2009.\n22David Dobkin and J. Ian Munro. Optimal time minimal space selection algorithms. Journal\nof the Association for Computing Machinery , 28(3):454–461, 1981.\n23Kasper Eenberg, Kasper Green Larsen, and Huacheng Yu. Decreasekeys are expensive for\nexternal memory priority queues. In Symposium on Theory of Computing (STOC) , page\n1081–1093. ACM, June 2017. doi:10.1145/3055399.3055437 .\n24Joan Feigenbaum and Robert E Tarjan. Two new kinds of biased search trees. Bell System\nTechnical Journal , 62(10):3139–3158, 1983.\n25Michael L. Fredman and Robert Endre Tarjan. Fibonacci heaps and their uses in improved\nnetwork optimization algorithms. Journal of the ACM , 34(3):596–615, July 1987. doi:\n10.1145/28869.28874 .\n26Leonidas J. Guibas and Robert Sedgewick. A dichromatic framework for balanced trees. In\n19th Annual Symposium on Foundations of Computer Science, Ann Arbor, Michigan, USA,\n16-18 October 1978 , pages 8–21. IEEE Computer Society, 1978. doi:10.1109/SFCS.1978.3 .\n27Felix Halim, Stratos Idreos, Panagiotis Karras, and Roland H. C. Yap. Stochastic database\ncracking: towards robust adaptive indexing in main-memory column-stores. Proceedings of the\nVLDB Endowment , 5(6):502–513, February 2012. doi:10.14778/2168651.2168652 .\n28John Iacono, Riko Jacob, and Konstantinos Tsakalidis. External memory priority queues with\ndecrease-key and applications to graph algorithms. In European Symposium on Algorithms\n(ESA). Schloss Dagstuhl – Leibniz-Zentrum für Informatik, 2019. doi:10.4230/LIPICS.ESA.\n2019.60.\n29Stratos Idreos. Database Cracking: Towards Auto-tuning Database Kernels . PhD thesis, 2010.\n30Stratos Idreos, Martin L. Kersten, and Stefan Manegold. Database cracking. In Conference\non Innovative Data Systems Research (CIDR) , 2007.\n31Stratos Idreos, Stefan Manegold, and Goetz Graefe. Adaptive indexing in modern database\nkernels. In International Conference on Extending Database Technology (EDBT) , EDBT ’12,\npage 566–569. ACM, March 2012. doi:10.1145/2247596.2247667 .\n32Shunhua Jiang and Kasper Green Larsen. A faster external memory priority queue with\ndecreasekeys. In Symposium on Discrete Algorithms (SODA) , page 1331–1343. Society for\nIndustrial and Applied Mathematics, January 2019. doi:10.1137/1.9781611975482.81 .\n33Kanela Kaligosi, Kurt Mehlhorn, J. Ian Munro, and Peter Sanders. Towards optimal multiple\nselection. In International Colloquium on Automata, Languages and Programming (ICALP) ,\npages 103–114. Springer, 2005. doi:10.1007/11523468_9 .\n34Haim Kaplan, László Kozma, Or Zamir, and Uri Zwick. Selection from heaps, row-sorted\nmatrices, and x+yusing soft heaps. In Symposium on Simplicity in Algorithms (SOSA) .\nSchloss Dagstuhl – Leibniz-Zentrum für Informatik, 2019. doi:10.4230/OASICS.SOSA.2019.5 .\n35Richard M. Karp, Rajeev Motwani, and Prabhakar Raghavan. Deferred data structuring.\nSIAM Journal on Computing , 17(5):883–902, 1988.\n36Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In International Conference on Management of Data , SIGMOD/PODS ’18,\npage 489–504. ACM, May 2018. doi:10.1145/3183713.3196909 .\n37Vijay Kumar and Eric J. Schwabe. Improved algorithms and data structures for solving graph\nproblems in external memory. In Proceedings of the Eighth IEEE Symposium on Parallel\nand Distributed Processing, SPDP 1996, New Orleans, Louisiana, USA, October 23-26, 1996 ,\npages 169–176. IEEE Computer Society, 1996. doi:10.1109/SPDP.1996.570330 .\n38Joan M. Lucas. Canonical forms for competitive binary search tree algorithms. Technical\nReport DCS-TR-250, Rutgers University, 1988.\n\nC. M. Rysgaard, S. Wild. 19\n39Conrado Martínez and Salvador Roura. Randomized binary search trees. J. ACM, 45(2):288–\n323, 1998. doi:10.1145/274787.274812 .\n40Kurt Mehlhorn. Data Structures and Efficient Algorithms, Volume 1: Sorting and Searching .\nSpringer, 1984.\n41J. Ian Munro. On the competitiveness of linear search. In ESA 2000 , pages 338–345. Springer,\n2000. doi:10.1007/3-540-45253-2_31 .\n42J. Ian Munro, Richard Peng, Sebastian Wild, and Lingyi Zhang. Dynamic optimality re-\nfuted – for tournament heaps. working paper, 2019. URL: https://www.wild-inter.net/\npublications/unordered-dynamic-optimality ,arXiv:1908.00563 .\n43Holger Pirk, Eleni Petraki, Stratos Idreos, Stefan Manegold, and Martin Kersten. Database\ncracking: fancy scan, not poor man’s sort! In International Workshop on Data Management\non New Hardware (DaMoN) , SIGMOD/PODS’14, page 1–8. ACM, June 2014. doi:10.1145/\n2619228.2619232 .\n44Bryce Sandlund and Sebastian Wild. Lazy search trees. In Sandy Irani, editor, 61st IEEE\nAnnual Symposium on Foundations of Computer Science, FOCS 2020, Durham, NC, USA,\nNovember 16-19, 2020 , pages 704–715. IEEE, 2020. doi:10.1109/FOCS46700.2020.00071 .\n45Bryce Sandlund and Sebastian Wild. Lazy search trees, 2020. arXiv:2010.08840 .\n46Bryce Sandlund and Lingyi Zhang. Selectable heaps and optimal lazy search trees. In\nProceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA) ,\npages 1962–1975, 2022. doi:10.1137/1.9781611977073.78 .\n47Raimund Seidel and Cecilia R. Aragon. Randomized search trees. Algorithmica , 16(4/5):464–\n497, 1996. doi:10.1007/BF01940876 .\n48Daniel Dominic Sleator and Robert Endre Tarjan. Self-adjusting binary search trees. Journal\nof the ACM , 32(3):652–686, 1985. doi:10.1145/3828.3835 .\n49Yufei Tao. Maximizing the optimality streak of deferred data structuring (a.k.a. database\ncracking). In International Conference on Database Theory (ICDT) . Schloss Dagstuhl –\nLeibniz-Zentrum für Informatik, 2025. doi:10.4230/LIPICS.ICDT.2025.10 .\n50Fatemeh Zardbani, Peyman Afshani, and Panagiotis Karras. Revisiting the theory and practice\nof database cracking. In International Conference on Extending Database Technology (EDBT) .\nOpenProceedings.org, 2020. doi:10.5441/002/EDBT.2020.46 .\n\n20 Lazy B-Trees\nA The Interval Structure\nIn this appendix, we give the full description and detailed analysis of the interval data\nstructure outlined in Section 3. We follow the construction of Sandlund and Wild [ 44], in\ntheir design of the interval structure, with some improvements for external memory. To give\na self-contained analysis of the number of I/Os used by all operations, we reproduce the\nanalysis from [44, 45] and spell out all cases explicitly.\nA gap ∆icontains all elements in the range the gap covers. Recall that a query on the\ngap for an element must be faster for elements closer to the border of a gap. We allow for\nthe following operations:\nIntervalsInsert (e):Inserts element einto the structure.\nIntervalsDelete (ptr):Deletes the element eat pointer ptrfrom the structure.\nIntervalsChange (ptr, e′):Changes the element eat pointer ptrto the element e′.\nIntervalSplit (e)orIntervalSplit (r):Splits the set of intervals into two sets of\nintervals at element eor rank r.\nA gap has a “sidesness” , which denotes the number of sides the gap has had a query.\nDenote a side as a queried side , if that rank has been queried before (cf. [ 44]). If there have\nbeen no queries yet, the (single) gap is a 0-sided gap. When a query in a 0-sided gap occurs,\ntwo new gaps are created which are both 1-sided. Note that both gaps which have a single\nquery on either the left or right side is called 1-sided. When queries have touched both sides,\nthe gap is 2-sided.\nWe obtain the following I/O costs for the above operations.\n▶Theorem 6. There exists a data structure on an ordered set, that maintains a set of\nintervals, supporting\nIntervalsInsert in worst-caseO(logBlogB|∆i|)I/Os,\nIntervalsDelete in amortizedO/parenleftbig1\nBlog2|∆i|+ logBlogB|∆i|/parenrightbig\nI/Os,\nIntervalsChange in worst-caseO(logBlogB|∆i|)I/Os, if the element is moved towards\nthe nearest queried side or amortized O/parenleftbig1\nBlog2|∆i|+ logBlogB|∆i|/parenrightbig\nI/Os otherwise, and\nIntervalSplit in amortizedO/parenleftbig1\nBlog2|∆i|+ logBlogB|∆i|+1\nBxlog2c/parenrightbig\nI/Os.\nHere|∆i|denotes the number of elements contained in all intervals, and xandcxforc≥1\nare the resulting sizes of the two sets produced by a split. The space usage is O(|∆i|/B)blocks.\nThe structure is as follows. Let the gap ∆icontain multiple non-overlapping intervals\nIi,j, which contains the elements located in the gap. The elements of the intervals are sorted\nbetween intervals, but not within an interval. Intervals therefore span a range of elements\nwith known endpoints. Each such interval is a blocked-linked-list containing the elements\nof the interval. Additionally, we store an augmented B-tree over the intervals, allowing for\nefficiently locating the interval containing a given element (using O(logB(#intervals ))I/Os).\nThe B-tree is augmented to hold in each node the total size of all intervals in the subtree of\nthe node, which allows for efficient rank queries.\nBy packing all intervals into a single blocked-linked-list, and noting that there can be no\nmore intervals than there are elements, the space usage of the intervals and the augmented\nB-tree over the intervals is O(|∆i|/B)blocks.\nA.1 Invariants and Potential\nIntuitively, there is a trade-off in maintaining intervals: having many small intervals reduces\nfuture query costs (typically dominated by the linear cost of splitting one interval), but\n\nC. M. Rysgaard, S. Wild. 21\nincreases the time to search for the correct interval upon insertions (and other operations).\nLazy search trees handle this trade-off as follows. To bound the worst case insertion costs,\nwe enforce a hard limit number on the number of intervals via a merging rule. To amortize\noccasional high costs for queries, we accrue potential ( Φi,jbelow) for any intervals that are\n“too large” relative to their proximity of a gap boundary.\nMoreover, for queries we would like to have interval sizes grow exponentially towards\nthe middle, as this allows for efficiently manipulating the intervals closest to the edge. This\nfolklore rule seems too rigid to maintain upon queries, though, as many changes can be\nnecessary.\nA.1.1 Merging Intervals\nWe maintain the following invariant on the number of intervals:\nInvariant (#Int): For a gap ∆i, the number of intervals Ii,jis at most 4 log2|∆i|+ 2.\nThis allows for locating an interval using O(logBlog2|∆i|)=O(logBlogB|∆i|)I/Os.\nThis I/O cost also applies to recomputing the sizes of subtrees when updating an interval.\nTo bound the I/O cost of the operations, we define the following for the analysis. Let o(Ii,j),\nthe “outside (elements) of Ii,j”, denote the number of elements between interval Ii,jand a\nqueriedside of the gap; if both sides have been queries, o(Ii,j)is the towards the closestside.\nWe denote an interval to be leftif the closest edge is to the left and rightotherwise. Note\nfor a 0-sided gap, the outside is always 0. For a 1-sided gap with a query on the left (resp.\nright), all intervals are left (resp. right).\nWhen an insertion occurs in the gap, the interval containing the inserted element is\nlocated, and the inserted element is appended to the list of the interval. This increases |∆i|\nby one, but does not create any new intervals, so (#Int) remains satisfied.\nWhen a deletion occurs, |∆i|decreases, which may invalidate (#Int). We therefore need a\nway to reduce the number of intervals, which we do by simply merging two adjacent intervals.\nRecall that elements within intervals are not sorted, so merging two intervals effectively just\nmeans “forgetting” about the pivot element currently separating them; here, we need to\nconcatenate the list of elements. The rule in lazy search trees is as follows, using the outside\nas a measure for how close to the middle an interval is:\nMerge Rule (M): If|Ii,j|+|Ii,j+1|<o(Ii,j), merge Ii,jandIi,j+1.\nThis rule only applies between intervals of the same type; a left interval is never merged\nwith a right interval.\nThat is, an interval Ii,jis merged with its inner neighbor, if they combined contain fewer\nelements than outside of the intervals; they are therefore intuitively “too small” for where\nthey reside in the gap. Note that we do nottreat this rule as an invariant; we do not require\nit to be true at all times, but only use it to reduce the number of intervals when explicitly\ninvoked in operation IntervalsMerge .\nSuppose that IntervalsMerge has just merged all interval pairs that satisfy (M). We\nthen obtain bounds on the number of intervals as stated in the following two lemmas.\n▶Lemma 7 (The Merge rule on one side yields logarithmic internals) .Let a set of Nelements\npartitioned in intervals that satisfy rule (M). Let further Ibe one of the intervals and k=o(I)\nbe its number of outside elements. Then there are at most max{1,2log2N\nk+ 2}intervals\ninside of Iand on the same side as I.\n\n22 Lazy B-Trees\nProof.The proof goes by induction in N. When N= 1, there can be at most one interval.\nIfN < k, there can be at most one interval, as the merge rule would otherwise be violated.\nIn both cases, the claim is satisfied.\nForN≥k, we may assume the induction hypothesis on all smaller values of N. If\nthere are less than three intervals, the bound holds from the additive constant. Otherwise,\nleta,bandcbe the size of the first three intervals in order. By the merge rule, it holds\nthat a+b≥k. By definition o(Ic)=k+a+b≥2k. The number of intervals is then\nthe two intervals aandb, and the number of intervals from c, which by induction is\n≤2 + 2 log2N−a−b\no(Ic)+ 2≤2 log2N\n2k+ 4 = 2 log2N\nk+ 2. This concludes the proof. ◀\n▶Lemma 8 ((M) implies (#Int)) .Let a gap ∆iadhere to rule (M) on intervals. Then there\nare at most 4 log2|∆i|+ 2intervals.\nProof.The proof goes by case analysis on the sidesness of ∆i. If the gap is 0-sided, there\nmust be one interval. If the gap is 1-sided, the first interval has size at least 1, and there\nmust then be at most 2log∆i+ 2remaining intervals by Lemma 7. In both cases the bound\nholds.\nOtherwise, the gap must be 2-sided, and the merge rule holds from both left and right.\nThe outermost intervals have size at least 1. Let LandRdenote the number of elements in\nleft and right intervals respectively. By Lemma 7, the number of intervals is at most\n2+(2 log2L+2)+(2 log2R+2) = 6+2 log2(L·R)≤6+2 log2∆2\ni\n4= 4 log2∆i+2.◀\nA.1.2 Potential\nWhen a query occurs, an interval Ii,jis split around some element or rank. The interval can\nbe located using the B-tree over intervals on either element or rank in O(logBlogB|∆i|)I/Os.\nForsplittingaroundanelement, theintervalisscannedandpartitionedusing O(|Ii,j|/B)I/Os.\nFor splitting around a rank, deterministic selection [ 12] is applied, which uses O(|Ii,j|/B)\nI/Os (see, e.g., [17, 18]). In both cases the number of I/Os grows with the interval size.\nWe therefore need a potential function, which balances the time to split an interval with\nhow close to the border it is, which is defined using the outside of an interval. For 1-sided\ngaps, the outside however only measures toward one side of the gap, and we therefore need\nfurther potential for the 1-sided intervals. Let N01be the number of elements contained in\ntotal in 0- and 1-sided gaps. Further, we need potential to efficiently merge intervals in a\nscan.\nWe let the potential be\nΦ =1\nBN01+/summationdisplay\ni,jΦi,j\nand let\nΦi,j= 1 +1\nBmax{|Ii,j|−o(Ii,j),0}.\nUsing this potential function, we can analyze the amortized I/O cost of applying the\nmerge rule on all intervals, which we denote as the IntervalsMerge operation. This\noperation uses that scanning all keys of a B-tree can be done efficiently.\n▶Lemma 9 (Merging runtime) .Let the number of intervals contained in the gap ∆ibek.\nThen IntervalsMerge uses amortizedO(k/B)I/Os.\n\nC. M. Rysgaard, S. Wild. 23\nProof.When IntervalsMerge is performed, first o(Ii,j)is computed for each Ii,jin\nO(k/B)I/Os, by scanning the B-tree. The B-tree may then be scanned again, to perform\nthe merges on the intervals, and finally a new B-tree over the resulting intervals can be built.\nAs the number of intervals can only decrease, the B-tree can be build using O(k/B)I/Os.\nLetIi,jandIi,j+1be a pair of intervals which are merged during the operation. It must\nhold that|Ii,j|+|Ii,j+1|<o(Ii,j). The merge is performed by following the pointers to the\nintervals and concatenating the lists, using O(1)I/O. Let Ii,j′be the resulting interval of\nthe merge. The potential then decreases by Φi,jandΦi,j+1, and increases by Φi,j′. From\nthe merge rule and the definition of the outside, it follows that\nΦi,j = 1 +1\nBmax{|Ii,j|−o(Ii,j),0} = 1\nΦi,j+1= 1 +1\nBmax{|Ii,j+1|−(o(Ii,j) +|Ii,j|),0}= 1\nΦi,j′= 1 +1\nBmax{(|Ii,j|+|Ii,j+1|)−o(Ii,j),0}= 1.\nThe difference in potential is therefore ∆Φ =−1, which covers the I/Os performed by the\nmerge, which concludes the proof. ◀\nA.2 Updating Elements of the Intervals\nUpon an IntervalsInsert operation, a new element eis added to the gap ∆i. The interval\nIi,jcontaining ecan be located using the B-tree, and the size of the interval incremented by 1\nusingO(logBlogB|∆i|)I/Os. As previously discussed, the intervals need not be merged, as\nthe invariant on the interval count is satisfied. In the potential function Φ, the outside of all\ninner intervals grows by 1, which does not increase the value of Φ. The size of Ii,jgrows by\n1, and N01grows by at most 1, which in total increases Φby at most2\nB.IntervalsInsert\ncan therefore be performed in both amortized and worst-case O(logBlogB|∆i|)I/Os.\nWhen an IntervalsDelete operation occurs, a pointed to element is removed from\nsome interval Ii,j. The size is then decremented by 1, and the B-tree must be updated\nto reflect so. If this decreases the size to 0, the interval is removed, which in total uses\nO(logBlogB|∆i|)I/Os. In contrast to the IntervalsInsert operation, upon removing an\nelement, the outside of all inner intervals decreases, which makes the potential function Φ\ngrow by at most1\nBfor each such interval, of which there are O(log2|∆i|). In addition, the\nnumber of elements decreases without the number of intervals decreasing, which may leading\nto breaking the invariant on the bound on the number of intervals. Note that merging a\nsingle interval is enough to ensure that the number of intervals stays bounded, and even so,\nonly after linearly many deletes, the merge of a single interval needs to occur. However, as\nthe potential increases by O/parenleftbig1\nBlog2|∆i|/parenrightbig\n, and the IntervalsMerge uses the same amount\nof amortized I/Os, it is efficient enough to run IntervalsMerge after every delete. In total,\nIntervalsDelete uses amortizedO/parenleftbig1\nBlog2|∆i|+ logBlogB|∆i|/parenrightbig\nI/Os.\nWhen IntervalsChange (ptr, e′)in performed, the element eat pointer ptris changed\nto element e′. This new element must then be placed in the correct interval, which can be\ndone using the B-tree over intervals using O(logBlogB|∆i|)I/Os. This then changes the\nsize of at most two intervals, with one increasing and the other decreasing, which increases\nthe potential by at most1\nB. This may also change the outside size of intervals. If the\nelement is moved towards the closest queried side, then the outside may increase, which does\nnot increase the potential. Otherwise, the outside may grow by one, for each interval the\nelement is moved past, which increases the potential by at most O/parenleftbig1\nBlog2|∆i|/parenrightbig\n. In total,\n\n24 Lazy B-Trees\nQuery point\nx cx\nFigure 5 The query splits the gap in two, where some side contains xelements and the other\ncontains cxelements, for some c≥1.\nInterval before query:\nIntervals after query:Ii,j\nQuery point\nℓ/2 ℓ/4 ℓ/4 r/4 r/4 r/2Left side, size ℓ Right side, size r\nFigure 6 A query in interval Ii,jsplits the interval into six new intervals.\nIntervalsChange uses worst-caseO(logBlogB|∆i|)I/Os, if the element is moved towards\nthe closest queried side, and otherwise amortized O/parenleftbig1\nBlog2|∆i|+ logBlogB|∆i|/parenrightbig\nI/Os.\nA.3 Splitting the Intervals\nWhen a query occurs in gap ∆i, the gap is split at the query element into two new gaps ∆′\ni\nand∆′\ni+1. Let the resulting gap sizes be xandcxfor some c≥1(see Figure 5). We assume\nhere and throughout that the left resulting gap has size x; the other case is symmetric.\nUpon a IntervalSplit operation, the interval Ii,jcontaining the query element must be\nsplit, and the remaining intervals distributed into the two new gaps. First, IntervalsMerge\nis applied on gap ∆i, to ensure structure on the intervals. Then interval Ii,jis split around\nthe query element into a left and right side of elements with respectively smaller and larger\nelements. Let ℓbe the number of elements on the left side. Using deterministic selection, the\nleft side is partitioned into three intervals of sizes ℓ/2,ℓ/4andℓ/4, such that the sizes are\ndoubling from the query point and outwards. The right side is similarly split, resulting in six\nnew intervals covering the elements of Ii,j. See Figure 6 for an illustration of this split. From\nthese new intervals, along with the remaining intervals of gap ∆i, two new gaps ∆′\niand∆′\ni+1\nare created, containing the intervals to the left and right side of the query point respectively.\nTo finally ensure invariant (#Int) on the two new gaps, we lastly apply IntervalsMerge\non them both, which concludes the steps of the IntervalSplit operation.\nWe next analyze the amortized I/O cost of IntervalSplit . The interval Ii,jcan be\nlocated using the B-tree in O(logBlogB|∆i|)I/Os. Splitting the interval is done using a con-\nstant number of scans and deterministic selections on the interval, which uses O/parenleftbig1\nB|Ii,j|/parenrightbig\nI/Os\nin total. When performing IntervalsMerge on∆i,∆′\ni, and ∆′\ni+1, then by Lemma 9, the\ninvariant on the number of intervals (#Int), and that splitting interval Ii,jcreatesO(1)new\nintervals, it holds that these merges uses amortized O/parenleftbig1\nBlog2|∆i|/parenrightbig\nI/Os in total. Note that\n\nC. M. Rysgaard, S. Wild. 25\nthese costs is independent on where Ii,jresides in the structure, and the sidesness of gap ∆i.\nThe change in potential however depends heavily on these properties. We shall therefore use\ncase analysis for the change in potential. These cases are on if the split interval is on the\nleft or right, and on the sidesness of the gap ∆i. We only analyze the difference in potential\nunder the splitting of interval Ii,j, as the IntervalsMerge operation already is amortized.\nA.3.1 Gap ∆iis2-sided.\nThe split must result in two new 2-sided gaps. Therefore, N01is unchanged, and can be\ndisregarded for the potential difference. As the gap is 2-sided, o(Ii,j)is the smaller distance\nto the two sides of the gap. It holds that Ii,joverlaps the query point, and therefore the\ndistance to the left side (in ∆i) is at most x, and the distance to the right side (in ∆i) is at\nmost cx. Therefore, o(Ii,j)≤x.\nThere are three types of intervals: the intervals resulting from splitting Ii,j, the intervals\nto the left of Ii,jin∆i, and the intervals to the right of Ii,jin∆i. Let us first consider the\npotential on the intervals resulting from splitting Ii,j. We denote this change in potential as\n∆ΦI. The potential on Ii,jisΦi,j, which is removed. Then six new intervals are introduced.\nConsider the intervals created on the left side of the query point, and let the total number of\nelements in these three intervals be ℓ. The rightmost one of these intervals is next to a query\npoint in the new gap, and thus its outside is 0. The potential on this interval is therefore\n1\nBℓ/4. The middle interval has the right interval as its outside, and as they are both of size\nℓ/4, there is no potential on this interval. The leftmost interval may have either side as its\noutside; in the worst-case, it has a potential of1\nBℓ/2. This holds symmetrically for the new\nintervals on the right. As the size of the left and right in total is |Ii,j|, the total worst-case\npotential on the new intervals is3\n41\nB|Ii,j|. In total, the change in potential on the intervals\ncreated from Ii,jis\n∆ΦI≤5 The number of intervals created vs removed\n−1\nBmax{|Ii,j|−o(Ii,j),0}Removing Ii,j\n+3\n41\nB|Ii,j| Introducing new intervals\nTo simplify this, we distinguish cases on the size of Ii,j, and compare it to the I/O\ncost of splitting the interval. If |Ii,j|≤x, then ∆Φ≤5 +3\n41\nBx=O/parenleftbig1\nBx/parenrightbig\n. Otherwise,\n∆Φ≤5 +1\nBx−1\n41\nB|Ii,j|. The amortized cost of the split, including only the potential on\nthe intervals of Ii,j, is thereforeO/parenleftbig1\nBx/parenrightbig\nI/Os. The total amortized cost must also cover the\nremaining intervals.\nFor the intervals to the left of Ii,j, let the change in potential be denoted as ∆ΦL. In\ntotal there are at most xelements located on the left side. As there is no new intervals\nintroduced, the potential function on the left intervals is therefore bounded by the number\nof elements, and thus ∆ΦL≤1\nBx. For the change in potential for the intervals on the right\nofIi,j, denoted ∆ΦR, we must further distinguish whether Ii,jis a left or right interval.\nCase 1: Ii,jis a left interval. We partition the intervals on the right into three categories:\nthe intervals that were a left interval in ∆iand remain a left interval in ∆′\ni+1(LL), the\nintervals that were right intervals in ∆iand become left intervals in ∆′\ni+1(RL), and the\nintervals that were right intervals in ∆iand remain right intervals in ∆′\ni+1(RR). Note that\nthere cannot be any intervals that were left and became right, as the number of elements\n\n26 Lazy B-Trees\nIi,j LL RL RR\nQuery pointx cx∆i\n∆′\ni ∆′\ni+1\nFigure 7 The categories of intervals upon splitting a 2-sided gap, with Ii,jbeing a left interval.\nto the left decreases; if this was the side with the fewer elements before, it must remain so.\nThese categories are illustrated on Figure 7.\nFirst note that the number of intervals and their sizes do not change from ∆ito∆′\ni+1.\nThe only change is in the size of the outsides. For the intervals in RR, their outside does not\nchange, and therefore their potential does not change.\nNote that IntervalsMerge was performed before the split, and therefore rule (M) is\nsatisfied on the intervals in ∆i. Also note that splitting another interval does not alter the\noutside values of other intervals. Therefore, for the intervals in LL,xelements are removed\nfrom their outside in ∆′\ni+1. We must then bound the number of intervals in LL, to bound\nthe change in the potential. As the outside of the leftmost interval in LLhas an outside of\nat least xin∆i, and there are at most cxelements in LL, as they are all contained in ∆′\ni+1,\nthen by Lemma 7, the number of intervals in LLis at most 2log2cx\nx+ 2 =O(log2c). The\npotential increase of LLis therefore bounded by O/parenleftbig1\nBxlog2c/parenrightbig\n.\nFor the intervals in RL, as they were a right interval before in ∆i, but become a left\ninterval in ∆′\ni+1due to xelements being removed from the left, their outside can decrease\nby at most x, when moved to ∆′\ni+1. Consider all but the rightmost interval of RL. As these\nare left intervals of ∆′\ni+1without the last left interval, it must hold that they are contained\nentirely in the left half of the cxelements of ∆′\ni+1. As they are right intervals in ∆i, then\ntheir old outside is to the right in ∆i, and therefore the outside of the last considered interval\nis in ∆iat least the right half of the cxelements. By Lemma 7, the number of intervals in\nRL, including the rightmost interval, is bounded by 2log2cx/2\ncx/2+ 2 + 1 = 3 . The potential\nincrease of RLis therefore bounded by3\nBx.\nIn total, ∆ΦR=O/parenleftbig1\nBxlog2c/parenrightbig\n.\nCase 2: Ii,jis a right interval. As there are more elements to the left of Ii,jthan to the\nright in ∆i, there are at most xelements remaining to the right of Ii,j. As there are no new\nintervals introduced, the potential function on the right intervals is therefore bounded by the\nnumber of elements, and thus ∆ΦR≤1\nBx.\nA.3.2 Gap ∆iis0-sided.\nAs there have been no queries to the left or right of the gap, then there can only be a single\ninterval in ∆i, which is Ii,j. The bottom of Figure 6 therefore illustrates this case. Upon\nsplitting this interval, two 1-sided gaps are created. All elements are moved into these gaps\nfrom ∆i, and therefore N01is unchanged, and can be disregarded for ∆Φ. For both gaps,\nthe queried side is at the query point. Therefore, the outside of each interval in ∆′\niand∆′\ni+1\n\nC. M. Rysgaard, S. Wild. 27\nIi,j LL\nQuery pointx cx∆i\n∆′\ni ∆′\ni+1\nFigure 8 The categories of intervals upon splitting a 1-sided gap, with left queried side.\nIi,j RL RR\nQuery pointx cx∆i\n∆′\ni ∆′\ni+1\nFigure 9 The categories of intervals upon splitting a 1-sided gap, with right queried side.\nis towards the middle. Following the analysis of ∆ΦIin the 2-sided case, it holds that the\nnew potential is1\n41\nB|Ii,j|, as the outside of the outermost intervals is towards the middle,\nand therefore ∆Φ = 5−3\n41\nB|Ii,j|.\nGap∆iis1-sided.\nAs before, we assume for the current query, that the left resulting new gap has size x. If in\n∆i, the queried side was the left side, then gap ∆′\nibecomes a 2-sided gap, and ∆′\ni+1remains\na1-sided gap, with the queried side on the left. This case is illustrated on Figure 8. The\noutside of Ii,jin∆iremains bounded by x, and the analysis on the change in potential of\nIi,jremains the same as in the 2-sided gap analysis, which bounds the potential increase by\nO/parenleftbig1\nBx/parenrightbig\n. For the intervals on the left, the potential may be at most1\nBx, as in the 2-sided\ncase. However, N01decreases by x, which bounds the increase in potential on the left to at\nmost 0. For the intervals on the right, it must hold that they were all left intervals in ∆iand\nthat they remain left intervals in ∆′\ni+1. Following the argument for LLfrom the 2-sided gap\ncase above, the increase in potential is bounded by O/parenleftbig1\nBxlog2c/parenrightbig\n.\nThe case when the queried side in ∆iis to the right is illustrated on Figure 9. In this\ncase, the outside of Ii,jis no longer bounded by x, but by cx. However, all elements on the\nright leave the 1-sided gap ∆i, and enter the 2-sided gap ∆′\ni+1; therefore N01decreases by\ncx, and so the potential decreases by1\nBcx. The intervals on the right must have been right\nintervals in ∆i, and may be both left or right intervals in ∆′\ni+1. Using the same analysis\nas in the 2-sided case for the RLandRRintervals, it holds that the potential on the right\ninterval increase by at most3\nBx. The intervals to the left spans at most xelements, and the\n\n28 Lazy B-Trees\npotential on these are at most1\nBx, as in the 2-sided case. In total\n∆Φ≤5 The number of intervals created vs removed\n−1\nBmax{|Ii,j|−o(Ii,j),0}Removing Ii,j\n+3\n41\nB|Ii,j| Introducing new intervals\n−1\nBcx Elements removed from 1-sided gap\n+3\nBx+1\nBx Potential change on the remaining intervals\nRecall that o(Ii,j)≤cx. We then case analysis on the size of Ii,j. If|Ii,j|≤cx, then\n∆Φ≤5−1\n41\nBcx+4\nBx. As splitting the interval uses O/parenleftbig1\nB|Ii,j|/parenrightbig\nI/Os, the amortized cost of\nthis case isO/parenleftbig1\nBx/parenrightbig\nI/Os. Otherwise, if |Ii,j|> cx, then ∆Φ≤5−1\n41\nB|Ii,j|+4\nBx, and the\namortized cost of this case is therefore also O/parenleftbig1\nBx/parenrightbig\nI/Os.\nA.3.3 Total I/O Cost\nIn total, the increase in potential on the intervals, and therefore also the amortized I/O cost\nof the split, is bounded by O/parenleftbig1\nBxlog2c/parenrightbig\n. The IntervalSplit operation, which includes\nsearching for the query point and the IntervalsMerge operations, therefore uses amortized\nO/parenleftbig1\nBlog2|∆i|+ logBlogB|∆i|+1\nBxlog2c/parenrightbig\nI/Os in total.\nThis concludes the proof of Theorem 6.\nB Supporting Lemmas\n▶Lemma 10 (Bounding the Sum of Double Exponential Function) .Leta, b≥2. Then\nn/summationdisplay\ni=0abi= Θ/parenleftig\nabn/parenrightig\n.\nProof.The proof proceeds by showing the lower and upper bound separately. For the lower\nbound, as all terms of the sum is positive, then the sum is bounded below by the last term\nof the sum. To show the upper bound, it is shown by induction in nthat\nn/summationdisplay\ni=0abi≤2abn.\nForn= 0the inequality holds, as there is only one term which is equal to a.\nn/summationdisplay\ni=0abi=0/summationdisplay\ni=0abi=ab0=a≤2a= 2ab0= 2abn.\nFor the induction step, let n=n′+ 1, and let the inequality hold by induction for n′. Then\nn/summationdisplay\ni=0abi=\nn′/summationdisplay\ni=0abi\n+abn′+1≤2abn′\n+/parenleftig\nabn′/parenrightigb\nFrom here, the goal bound is twice that of the last term in the sum. It can therefore be\nshown by proving that the second term bounds the first. For simplicity, let k=abn′\n. Then it\nholds that\n2k≤kb⇐⇒ logk(2k)≤b⇐⇒ logk(2) + 1≤b .\n\nC. M. Rysgaard, S. Wild. 29\nAsa, b≥2andn′≥0, then logk(2)≤1, and the above inequality holds. It therefore holds\nthat\n2abn′\n+/parenleftig\nabn′/parenrightigb\n≤2/parenleftig\nabn′/parenrightigb\n= 2abn,\nConcluding the proof. ◀",
  "textLength": 92050
}