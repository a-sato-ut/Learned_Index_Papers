{
  "paperId": "849696f5195be4f5f63f1b7ef8bbb0f9f19ce8b4",
  "title": "LeCo: Lightweight Compression via Learning Serial Correlations",
  "pdfPath": "849696f5195be4f5f63f1b7ef8bbb0f9f19ce8b4.pdf",
  "text": "LeCo: Lightweight Compression via Learning Serial Correlations\nYihao Liu\nTsinghua University\nliuyihao21@mails.tsinghua.edu.cnXinyu Zeng\nTsinghua University\nzeng-xy21@mails.tsinghua.edu.cnHuanchen Zhang\nTsinghua University\nhuanchen@tsinghua.edu.cn\nABSTRACT\nLightweight data compression is a key technique that allows col-\numn stores to exhibit superior performance for analytical queries.\nDespite a comprehensive study on dictionary-based encodings to\napproach Shannonâ€™s entropy, few prior works have systematically\nexploited the serial correlation in a column for compression. In this\npaper, we propose LeCo (i.e., Learned Compression), a framework\nthat uses machine learning to remove the serial redundancy in a\nvalue sequence automatically to achieve an outstanding compres-\nsion ratio and decompression performance simultaneously. LeCo\npresents a general approach to this end, making existing (ad-hoc)\nalgorithms such as Frame-of-Reference (FOR), Delta Encoding, and\nRun-Length Encoding (RLE) special cases under our framework.\nOur microbenchmark with three synthetic and eight real-world\ndata sets shows that a prototype of LeCo achieves a Pareto improve-\nment on both compression ratio and random access speed over\nthe existing solutions. When integrating LeCo into widely-used\napplications, we observe up to 5.2Ã—speed up in a data analytical\nquery in the Arrow columnar execution engine, and a 16%increase\nin RocksDB â€™s throughput.\nACM Reference Format:\nYihao Liu, Xinyu Zeng, and Huanchen Zhang . 2023. LeCo: Lightweight\nCompression via Learning Serial Correlations. In Proceedings of ACM SIG-\nMOD International Conference on Management of Data (SIGMODâ€™24). ACM,\nNew York, NY, USA, 16 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n1 INTRODUCTION\nAlmost all major database vendors today have adopted a column-\noriented design for processing analytical queries [ 31,33,44,53,\n61,74,76,94]. One of the key benefits of storing values of the\nsame attribute consecutively is that the system can apply a vari-\nety of lightweight compression algorithms to the columns to save\nspace and disk/network bandwidth [ 28,29,104]. These algorithms,\nsuch as Run-Length Encoding (RLE) [ 29] and Dictionary Encod-\ning, typically involve a single-pass decompression process (hence,\nlightweight) to minimize the CPU overhead. A few of them (e.g.,\nFrame-of-Reference or FOR [ 59,117]) allow random access to the\nindividual values. This is a much-preferred feature because it allows\nthe DBMS to avoid full-block decompression for highly selective\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nSIGMODâ€™24, June 11â€“16, 2024, Santiago, Chile\nÂ©2023 Association for Computing Machinery.\nACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnnqueries, which are increasingly common, especially in hybrid trans-\nactional/analytical processing (HTAP) [ 17,62,69,78,90,93] and\nreal-time analytics [13, 76].\nThere are two categories of lightweight compression algorithms\nthat exploit different sources of redundancy in a value sequence. The\nfirst are dictionary-based algorithms, including those that encode\nsubstring patterns (e.g., FSST [ 36], HOPE [ 114]). These algorithms\nleverage the uneven probability distribution of the values and have\na compression ratio limited by Shannonâ€™s Entropy [ 98]. On the\nother hand, integer compression algorithms such as Run-Length\nEncoding (RLE) [ 29], FOR, and Delta Encoding [ 29,80] exploit the\nserial correlation between the values in a sequence: the value of\nthe current position may depend on its preceding values.\nHowever, RLE, FOR, and Delta Encoding are ad-hoc solutions\nmodeling the simplest serial patterns. For example, Delta adopts\na model of a basic step function, while RLE only works with con-\nsecutive repetitions (elaborated in Section 2). Consequently, we\nhave missed many opportunities to leverage more sophisticated\npatterns such as the piecewise linearity shown in Figure 1 for better\ncompression in a column store. Prior studies in time-series data\nstorage [ 50,51,64,72,87,108] have proposed to learn the series\ndistribution and minimize the model sizes to achieve a lossy com-\npression. These techniques, however, are not applicable to a general\nanalytical system. To the best of our knowledge, none of the exist-\ning column stores apply machine learning to improve the efficiency\nof their lightweight lossless compression systematically.\nWe, thus, propose a framework called LeCo (i.e., Learned Com-\npression) to automatically learn serial patterns from a sequence and\nuse the models for compression. Our key insight is that if we can\nfit such serial patterns with lightweight machine-learning models,\nwe only need to store the prediction error for each value to achieve\na lossless compression. Our framework addresses two subproblems.\nThe first is that given a subsequence of values, how to best fit the\ndata using one model? This is a classic regression problem. However,\ninstead of minimizing the sum of the squared errors, we minimize\nthe maximum error because we store the deltas (i.e., prediction\nerrors) in a fixed-length array to support fast random access during\nquery processing. LeCo also includes a Hyperparameter-Advisor to\nselect the regressor type (e.g., linear vs. higher-order) that would\nproduce the best compression ratios.\nThe second subproblem is data partitioning: given the type(s) of\nthe regression model, how to partition the sequence to minimize\nthe overall compression ratio? Proactive partitioning is critical to\nachieving high-prediction accuracy in the regression tasks above\nbecause real-world data sets typically have uneven distributions\n[70,115]. The partition schemes introduced by lossy time-series\ncompression are not efficient to apply. They only target minimiz-\ning the total size of the model parameters rather than striking a\nbalance between the model size and the delta array size. Our eval-\nuation (Section 4.8) shows that the state-of-the-art partitioning\n1arXiv:2306.15374v3  [cs.DB]  23 Nov 2023\n\nSIGMODâ€™24, June 11â€“16, 2024, Santiago, Chile Liu et al.\npositionvalue\npartition\n   #1partition\n    #2\nFigure 1: A Motivating Ex-\nample. â€“ On movieid data set.\n0 10 20 30 40 50\nCompression Ratio(%)050100150200250300350Random Access(ns)FORElias-FanoDelta\nLeCoLeCo-varFigure 2: Performance-space\ntrade-offs.\nalgorithms [ 40,72] are still suboptimal for general lossless column\ncompression.\nIn the lossless case, however, having smaller partitions might be\nbeneficial for reducing the local max errors, but it increases the\noverall model (and metadata) size. Because optimal partitioning\nis an NP-hard problem, we developed different heuristic-based\nalgorithms for different regression models to obtain approximate\nsolutions in a reasonable amount of time. Another design trade-off\nis between fixed-length and variable-length partitions. Variable-\nlength partitions produce a higher compression ratio but are slower\nin random access.\nWe implemented a prototype of LeCo to show the benefit of using\nmachine learning to compress columnar data losslessly. For each\npartition, we store a pre-trained regression model along with an\narray of fixed-length deltas. Decompressing a value only involves a\nmodel inference plus a random access to the delta array. LeCo is\nhighly extensible with built-in support for various model types and\nfor both fixed-length and variable-length partition schemes.\nWe compared LeCo against state-of-the-art lightweight compres-\nsion algorithms including FOR, Elias-Fano, and Delta Encoding us-\ning a microbenchmark consisting of both synthetic and real-world\ndata sets. As illustrated in Figure 21, LeCo achieves a Pareto im-\nprovement over these algorithms. Compared to FOR and Elias-Fano,\nLeCo improves the compression ratio by up to 91%while retain-\ning a comparable decompression and random access performance.\nCompared to Delta Encoding, LeCo is an order-of-magnitude faster\nin random access with a competitive or better compression ratio.\nWe further integrated LeCo into two widely-used applications to\nstudy its benefit on end-to-end system performance. We first report\nLeCoâ€™s performance on a columnar execution engine, using Apache\nArrow [ 4] and Parquet [ 6] as the building blocks. Enabling LeCo in\nthis system speeds up a multi-column filter-groupby-aggregation\nquery by up to 5.2Ã—and accelerates single-column bitmap aggrega-\ntion query up to 11.8Ã—with a 60.5%reduction in memory footprint.\nWe also use LeCo to compress the index blocks in RocksDB [ 14,48]\nand observed a 16%improvement in RocksDBâ€™s throughput com-\npared to its default configuration.\nThe paper makes four primary contributions. First, we identify\nthat exploiting the serial correlation between values has a great\npotential for efficient column compression. Second, we make the\ncase for applying machine learning to lightweight lossless column\ncompression. Third, we propose the Learned Compression (LeCo)\nframework and implement a prototype that achieves a Pareto im-\nprovement on compression ratio and random access speed over\n1Figure 2 is based on the weighted average result of twelve data sets in Section 4.3.existing algorithms. Finally, we integrate LeCo into a columnar exe-\ncution engine and a key-value store and show that it helps improve\nthe systemsâ€™ performance and space efficiency simultaneously.\n2 THE CASE FOR LEARNED COMPRESSION\nThe performance of persistent storage devices has improved by or-\nders of magnitude over the last decade [ 109]. Modern NVMe SSDs\ncan achieve 7GB/s read throughput and over 500,000 IOPS [ 16].\nThe speed of processors, on the other hand, remains stagnant as\nMooreâ€™s Law fades [ 56]. Such a hardware trend is gradually shifting\nthe bottleneck of a data processing system from storage to compu-\ntation. Hence, pursuing a better compression ratio is no longer the\ndominating goal when developing a data compression algorithm.\nMany applications today prefer lightweight compression schemes\nbecause decompressing the data is often on the critical path of\nquery execution. Meanwhile, an analytical workload today is often\nmixed with OLTP-like queries featuring small range scans or even\npoint accesses [ 12,89]. To handle such a wide range of selectivity, it\nis attractive for a data warehouse to adopt compression algorithms\nthat can support fast random access to the original data without\ndecompressing the entire block.\nDictionary encoding is perhaps the most widely-used compres-\nsion scheme in database management systems (DBMSs). Nonethe-\nless, for a sequence where the values are mostly unique, dictionary\nencoding does not bring compression because it assumes indepen-\ndence between the values, and its compression ratio is bounded\nby Shannonâ€™s Entropy [ 98]. Shannonâ€™s Entropy, however, is not\nthe lower bound for compressing an existing sequence2. In many\nreal-world columns, values often exhibit strong serial correlations\n(e.g., sorted or clustered) where the value at a particular position is\ndependent on the values preceding it. Unfortunately, to the best of\nour knowledge, there is no general solution proposed that can sys-\ntematically leverage such positional redundancy for compression.\nWe argue that a learned approach is a natural fit. Extracting se-\nrial correlation is essentially a regression task. Once the regression\nmodel captures the â€œcommon patternâ€ of the sequence, we can use\nfewer bits to represent the remaining delta for each value. This\nModel + Delta framework (a.k.a., LeCo) is fundamental for exploit-\ning serial patterns in a sequence to achieve lossless compression.\nFor example, Boffa et al. attempted to use linear models for storing\nrank &select dictionaries specifically [ 35]. In fact, the widely-used\nFOR, RLE, and Delta Encoding (Delta) can be considered special\ncases under our framework as well.\nFOR divides an integer sequence into frames, and for each value\nğ‘£ğ‘–in a frame, it is encoded as ğ‘£ğ‘–âˆ’ğ‘£ğ‘šğ‘–ğ‘›whereğ‘£ğ‘šğ‘–ğ‘›is the minimum\nvalue of that frame. From a LeCo â€™s point of view, the regression\nfunction for each frame in FOR is a horizontal line. Although such\na naive model is fast to train and inference, it is usually suboptimal\nin terms of compression ratio. RLE can be considered a special\ncase of FOR, where the values in a frame must be identical. Delta\nEncoding achieves compression by only storing the difference be-\ntween neighboring values. Specifically, for an integer sequence\nğ‘£1,ğ‘£2,...,ğ‘£ ğ‘›, Delta encodes the values as ğ‘£1,ğ‘£2âˆ’ğ‘£1,ğ‘£3âˆ’ğ‘£2,ğ‘£ğ‘›âˆ’ğ‘£ğ‘›âˆ’1.\n2The lower bound is known as the Kolmogorov Complexity. It is the length of the\nshortest program that can produce the original data [ 81]. Kolmogorov Complexity is\nincomputable.\n2\n\nLeCo: Lightweight Compression via Learning Serial Correlations SIGMODâ€™24, June 11â€“16, 2024, Santiago, Chile\nSimilar to FOR, it uses the horizontal-line function as the model,\nbut each partition/frame in Delta only contains one item. The ad-\nvantage of Delta is that the models can be derived from recovering\nthe previous values rather than stored explicitly. The downside,\nhowever, is that accessing any particular value requires a sequential\ndecompression of the entire sequence.\nLeCo helps bridge the gap between data compression and data\nmining. Discovering and extracting patterns are classic data mining\ntasks. Interestingly, these tasks often benefit from preprocessing the\ndata set with entropy compression tools to reduce â€œnoiseâ€ for a more\naccurate prediction [ 101]. As discussed above, these data mining\nalgorithms can inversely boost compression efficiency by extracting\nthe serial patterns through the LeCo framework. The theoretical\nfoundation of this relationship is previously discussed in [ 52]. No-\ntice that although we focus on regression in this paper, other data\nmining techniques, such as anomaly detection, also reveal serial pat-\nterns that can improve compression efficiency [ 30,38]. The beauty\nof LeCo is that it aligns the goal of sequence compression with that\nof serial pattern extraction. LeCo is an extensible framework: it\nprovides a convenient channel to bring related advances in data\nmining to the improvement of sequence compression.\nAlthough designed to solve different problems, LeCo is related to\nthe recent learned indexes [ 47,55,73] in that they both use machine\nlearning (e.g., regression) to model data distributions. A learned\nindex tries to fit the cumulative distribution function (CDF) of a\nsequence and uses that to predict the quantile (i.e., position) of\nan input value. Inversely, LeCo takes the position in the sequence\nas input and tries to predict the actual value. LeCoâ€™s approach is\nconsistent with the mapping direction (i.e., position â†’value) in\nclassic pattern recognition tasks in data mining.\nMoreover, LeCo mainly targets immutable columnar formats\nsuch as Arrow [ 4] and Parquet [ 6]. Updating the content requires a\ncomplete reconstruction of the files on which LeCo can piggyback\nits model retraining. Unlike indexes where incremental updates are\nthe norm, the retraining overhead introduced by LeCo is amortized\nbecause the files in an analytical system typically follow the pattern\nof â€œcompress once and access many timesâ€.\nWe next present the LeCo framework in detail, followed by\nan extensive microbenchmark evaluation in Section 4. We then\nintegrate LeCo into two real-world applications and demonstrate\ntheir end-to-end performance in Section 5.\n3 THE LECO FRAMEWORK\nLet us first define the learned compression problem that the LeCo\nframework targets. Given a data sequence Â®ğ‘£[0,ğ‘›)=(ğ‘£0,...,ğ‘£ ğ‘›âˆ’1), let\nğ‘ƒ0=Â®ğ‘£[ğ‘˜0=0,ğ‘˜1),ğ‘ƒ1=Â®ğ‘£[ğ‘˜1,ğ‘˜2),...,ğ‘ƒ ğ‘šâˆ’1=Â®ğ‘£[ğ‘˜ğ‘šâˆ’1,ğ‘˜ğ‘š=ğ‘›)be a partition\nassignmentPwithğ‘šnon-overlap segments where each partition\nğ‘—has a modelFğ‘—. Letğ›¿ğ‘–=ğ‘£ğ‘–âˆ’Fğ‘—(ğ‘–), whereFğ‘—(ğ‘–)is the model\nprediction at position ğ‘–, forğ‘£ğ‘–âˆˆğ‘ƒğ‘—. The goal of learned compression\nis to find a partition assignment Pand the associated models F\nsuch that the model size plus the delta-array size are minimized:\nğ‘šâˆ’1âˆ‘ï¸\nğ‘—=0(âˆ¥Fğ‘—âˆ¥+(ğ‘˜ğ‘—+1âˆ’ğ‘˜ğ‘—)(ğ‘˜ğ‘—+1âˆ’1max\nğ‘–=ğ‘˜ğ‘—âŒˆlog2ğ›¿ğ‘–âŒ‰))\nwhereâˆ¥Fğ‘—âˆ¥denotes the model size of Fğ‘—, and maxâŒˆlog2ğ›¿ğ‘–âŒ‰is the\nnumber of bits required to represent the largest ğ›¿ğ‘–in the partition.\nUncompressed \nData\nCompressed \nDataEncoderPartitionerRegressorModel Partition BoundariesModel ParametersDecoderPoint/Range Request\nDecompressed Value(s)Distribution FeaturesHyper-parameter AdvisorFigure 3: The LeCo Framework â€“ An overview of the modules\nand their interactions with each other.\nAs shown in Figure 3, LeCo consists of five modules: Regressor ,\nPartitioner ,Hyperparameter-Advisor ,Encoder , and Decoder .\nThe Hyper-parameter Advisor trains a Regressor Selector model\noffline. Given an uncompressed sequence of values at runtime, it\nextracts features from it for model inference and outputs the recom-\nmended Regressor type as well as advises on partitioning strategy.\nThen, LeCo enters the model learning phase, where the Regressor\nand the Partitioner work together to produce a set of regression\nmodels with associated partition boundaries. The Encoder receives\nthe model parameters as well as the original sequence and then\ngenerates a compact representation of the â€œModel + Deltaâ€ (i.e.,\nthe compressed sequence) based on a pre-configured format. The\ncompressed sequence is self-explanatory: all the metadata needed\nfor decoding is embedded in the format. When a user issues a query\nby sending one or a range of positions, the Decoder reads the model\nof the relevant partition along with the corresponding locations in\nthe delta array to recover the requested values.\nA design goal of LeCo is to make the framework extensible.\nWe first decouple model learning (i.e., the logical value encoding)\nfrom the physical storage layout because applying common storage-\nlevel optimizations such as bit-packing and null-suppression to a\ndelta sequence is orthogonal to the modeling algorithms. We also\ndivide the model learning task into two separate modules. The\nRegressor focuses on best fitting the data in a single partition, while\na Partitioner determines how to split the data set into subsequences\nto achieve a desirable performance and compression ratio.\nSuch a modular design facilitates integrating future advances in\nserial pattern detection and compressed storage format into LeCo.\nIt also allows us to reason the performance-space trade-off for each\ncomponent independently. We next describe our prototype and the\ndesign decisions made for each module (Section 3.1 to Section 3.3),\nfollowed by the extension to handling string data in Section 3.4.\n3.1 Regressor\nThe Regressor takes in a sequence of values ğ‘£0,ğ‘£1,...,ğ‘£ ğ‘›âˆ’1and out-\nputs a single model that â€œbest fitsâ€ the sequence. LeCo supports\nthe linear combination of various model types, including constant,\nlinear, polynomial, and more sophisticated models, such as expo-\nnential and logarithm. Given a model F(ğ‘–)=Ã\nğ‘—(ğœƒğ‘—Â·Mğ‘—(ğ‘–))where\nMğ‘—denotes different model terms with ğœƒğ‘—as its linear combination\nweight andğ‘–represents the position in the sequence, classic regres-\nsion methods minimize the sum of the squared errorsÃ\nğ‘–(ğ‘£ğ‘–âˆ’F(ğ‘–))2\n(i.e., theğ‘™2norm of deltas), which has a closed-form solution. If\nLeCo stores deltas in variable lengths, this solution would pro-\nduce a delta sequence with minimal size. As we discussed before,\n3\n\nSIGMODâ€™24, June 11â€“16, 2024, Santiago, Chile Liu et al.\nreal databases usually avoid variable-length values because of the\nparsing overhead during query execution.\nLeCo, therefore, stores each value in the delta array in fixed\nlength. Specifically, LeCo adopts the bit-packing technique. Suppose\nthe maximum absolute value in the delta array is ğ›¿ğ‘šğ‘ğ‘¥ğ‘ğ‘ğ‘  , then each\ndelta occupies a fixed ğœ™=âŒˆğ‘™ğ‘œğ‘”2(ğ›¿ğ‘šğ‘ğ‘¥ğ‘ğ‘ğ‘ )âŒ‰bits. The storage size of\nthe delta array is thus determined by ğœ™rather than the expected\nvalue of the deltas, and our regression objective becomes:\nminimize ğœ™\nsubject toâŒˆlog2(|F(ğ‘–)âˆ’vğ‘–|)âŒ‰â‰¤ğœ™,ğ‘–=0,...,ğ‘›âˆ’1\nğœ™â‰¥0\nThe constrained optimization problem above can be transformed\ninto a linear programming problem with 2ğ‘›+1constraints where\nwe can get an approximated optimal solution in ğ‘‚(ğ‘›)time [97].\nWe introduce a Regressor Selector (RS) in the Hyperparameter-\nAdvisor to automatically choose the regressor type (e.g., linear vs.\nhigher-order) for a given sequence partition. RS takes in features\ncollected from a single pass of the input data and then feeds them\nto its classification model (e.g., Classification and Regression Tree\nor CART). The model is trained offline using the same features from\nthe training data sets. We briefly introduce the main features used\nin the current RS implementation below.\nLog-scale data range. Data range gives an upper bound of\nthe size of the delta array. A smaller data range prefers simpler\nmodels because the model parameters would take a significant\nportion of the compressed output.\nDeviation of the ğ‘˜th-order deltas. Given a data sequence\nğ‘£0,...,ğ‘£ ğ‘›âˆ’1, we define the first-order delta sequence as ğ‘‘0\n1=ğ‘£1âˆ’\nğ‘£0,ğ‘‘1\n1=ğ‘£2âˆ’ğ‘£1,...,ğ‘‘1\nğ‘›âˆ’2=ğ‘£ğ‘›âˆ’1âˆ’ğ‘£ğ‘›âˆ’2. Then, theğ‘˜th-order delta\nsequence is{ğ‘‘ğ‘˜\n0,ğ‘‘ğ‘˜\n1,...,ğ‘‘ğ‘˜\nğ‘›âˆ’ğ‘˜âˆ’1}, whereğ‘‘ğ‘˜\nğ‘–âˆ’1=ğ‘‘ğ‘˜âˆ’1\nğ‘–âˆ’ğ‘‘ğ‘˜âˆ’1\nğ‘–âˆ’1. Let\nğ‘‘ğ‘˜ğ‘šğ‘ğ‘¥,ğ‘‘ğ‘˜\nğ‘šğ‘–ğ‘›, andğ‘‘ğ‘˜ğ‘ğ‘£ğ‘”be the maximum, minimum, and average delta\nvalues, respectively. We then compute the normalized deviation of\ntheğ‘˜th-order deltas asÃ\nğ‘–âˆˆ[0,ğ‘›âˆ’ğ‘˜)(ğ‘‘ğ‘˜\nğ‘–âˆ’ğ‘‘ğ‘˜\nğ‘ğ‘£ğ‘”)\n(ğ‘›âˆ’ğ‘˜)(ğ‘‘ğ‘˜ğ‘šğ‘ğ‘¥âˆ’ğ‘‘ğ‘˜\nğ‘šğ‘–ğ‘¥). We use this metric to\ndetermine the maximum degree of polynomial needed to fit the data.\nThe intuition is that the ğ‘˜th-order delta sequence of a ğ‘˜th-degree\npolynomial is constant (i.e., with minimum deviation).\nSubrange trend and divergence. We first split the data into\nfixed-length subblocks {Â®ğ‘£[ğ‘–Â·ğ‘ ,(ğ‘–+1)Â·ğ‘ )}ğ‘–, each containing ğ‘ records\nwith a data range (i.e., subrange) of ğ‘Ÿğ‘–. We define the subrange ratio\n(SR) between adjacent subblocks asğ‘Ÿğ‘–\nğ‘Ÿğ‘–âˆ’1. The metric â€œsubrange\ntrendâ€Tis the average SR across all subblocks, while â€œsubrange\ndivergenceâ€Dis the difference between the maximum SR and\nminimum SR. These two metrics provide a rough sketch of the\nvalue-sequence distribution: Tdepicts how fast the values increase\non average, andDindicates how stable the increasing-trend is.\n3.2 Partitioner\nGiven a Regressor, the Partitioner divides the input sequence\nÂ®ğ‘£[0,ğ‘›)=ğ‘£0,ğ‘£1,...,ğ‘£ ğ‘›âˆ’1intoğ‘šconsecutive subsequences (i.e., par-\ntitions)Â®ğ‘£[0,ğ‘˜1),Â®ğ‘£[ğ‘˜1,ğ‘˜2),...,Â®ğ‘£[ğ‘˜ğ‘šâˆ’1,ğ‘˜ğ‘š)where a regression model is\ntrained on each partition. The goal of the Partitioner is to minimize\nthe overall size of the compressed sequences.\nAlthough partitioning increases the number of models to store,\nit is more likely for the Regressor to produce a smaller delta array\npositionvalue\npartition\n   #1partition\n    #2Figure 4: Fixed-length Parti-\ntioning Example.\n102103104105106107\nBlock size0204060Compression Ratio(%)\nbooksale\nnormalFigure 5: ompression Ratio\nTrend. â€“ Sweeping block size.\nwhen fitting a shorter subsequence. Thus, we require the Partitioner\nto balance between the model storage overhead and the general\nmodel fitting quality. We can find an optimal partition arrangement\nby computing the compressed size of each possible subsequence\nthrough dynamic programming [ 99]. Such an exhaustive search,\nhowever, is forbiddingly expensive with time complexity of ğ‘‚(ğ‘›3)\nand space complexity of ğ‘‚(ğ‘›2).\nWe next propose two practical partitioning schemes developed\nin LeCo that make different trade-offs between compression ratio\nand compression/decompression performance.\n3.2.1 Fixed-Length Partitioning. The most common strategy is\nsplitting the sequence into fixed-length partitions. This partitioning\nscheme is easy to implement and is friendly to random accesses.\nBecause each partition contains a fixed number of items, given\na position, an application can quickly locate the target partition\nwithout the need for a binary search in the metadata. The downside,\nhowever, is that fixed-length partitioning is not flexible enough to\nhelp the Regressor capture the desired patterns. For example, as\nshown in Figure 4, if we divide the Movie ID data set into fixed-\nlength partitions, the Regressor would fail to leverage the piecewise\nlinearity in certain ranges. To find an optimal partition size:\n(1)Sample <1%of the data randomly, consisting of subsequences\nof lengthğ‘, whereğ‘is the maximum partition length in the\nsearch space (e.g., ğ‘=10ğ‘˜).\n(2)Search the (fixed) partition size between 1andğ‘that produces\nthe lowest compression ratio on the samples. Because the com-\npression ratio typically has a â€œU-shapeâ€ as we vary the partition\nsize (illustrated in Figure 5), we first perform an exponential\nsearch to go past the global minimum. Then, we search back\nwith smaller steps to approach the optimal partition size.\n(3)Stop the search process once the compression ratio converges\n(with <0.01%decline between adjacent iterations).\n3.2.2 Variable-Length Partitioning. Below, we propose a greedy\nalgorithm for variable-length partitioning for an arbitrary Regres-\nsor discussed in Section 3.1 to approximate the optimal solution\nobtained by the dynamic programming approach.\nOur greedy algorithm includes two phases: split andmerge . In\nthe split phase, the algorithm groups consecutive data points into\nsmall partitions where the Regressor can predict with small errors.\nWe impose strict constraints to limit the maximum prediction er-\nror produced by the Regressor for each partition. Because of our\naggressive guarantee of prediction errors, the algorithm tends to\ngenerate an excessive number of partitions in the split phase, where\nthe cumulative model size could dominate the final compressed\nsize. To compensate for the over-splitting, the algorithm enters the\n4\n\nLeCo: Lightweight Compression via Learning Serial Correlations SIGMODâ€™24, June 11â€“16, 2024, Santiago, Chile\nmerge phase where adjacent partitions are merged if such an action\ncan reduce the final compressed size.\nSpecifically, in the split phase, we first pick a few starting parti-\ntions. A starting partition contains at least a minimum number of\nconsecutive values for the Regressor to function meaningfully (e.g.,\nthree for a linear Regressor). Then, we examine the adjacent data\npoint to determine whether to include this point into the partition.\nThe intuition is that if the space cost of incorporating this data\npoint is less than a pre-defined threshold, the point is added to the\npartition; otherwise, a new partition is created.\nThe splitting threshold is related to the model size ğ‘†ğ‘€of the\nRegressor. Suppose the current partition spans from position ğ‘–to\nğ‘—âˆ’1:Â®ğ‘£[ğ‘–,ğ‘—). LetÎ”(Â®ğ‘£)be a function that takes in a value sequence\nand outputs the number of bits required to represent the maximum\nabsolute prediction error from the Regressor (i.e., âŒˆğ‘™ğ‘œğ‘”2(ğ›¿ğ‘šğ‘ğ‘¥ğ‘ğ‘ğ‘ )âŒ‰).\nThen, the space cost of adding the next data point ğ‘£ğ‘—is\nğ¶=(ğ‘—+1âˆ’ğ‘–)Â·Î”(Â®ğ‘£[ğ‘–,ğ‘—+1))âˆ’(ğ‘—âˆ’ğ‘–)Â·Î”(Â®ğ‘£[ğ‘–,ğ‘—))\nWe compare ğ¶againstğœğ‘†ğ‘€, whereğœis a pre-defined coefficient\nbetween 0and1to reflect the â€œaggressivenessâ€ of the split phase: a\nsmallerğœleads to more fine-grained partitions with more accurate\nmodels. Ifğ¶â‰¤ğœğ‘†ğ‘€,ğ‘£ğ‘—is included to the current partition Â®ğ‘£[ğ‘–,ğ‘—).\nOtherwise, we create a new partition with ğ‘£ğ‘—as the first value.\nIn the merge phase, we scan through the list of partitions\nÂ®ğ‘£[0,ğ‘˜1),Â®ğ‘£[ğ‘˜1,ğ‘˜2),...,Â®ğ‘£[ğ‘˜ğ‘šâˆ’1,ğ‘˜ğ‘š)produced in the split phase and merge\nthe adjacent ones if the size of the merged partition is smaller\nthan the total size of the individual ones. Suppose the algorithm\nproceeds at partition Â®ğ‘£[ğ‘˜ğ‘–âˆ’1,ğ‘˜ğ‘–). At each step, we try to merge the\npartition to its right neighbor Â®ğ‘£[ğ‘˜ğ‘–,ğ‘˜ğ‘–+1). We run the Regressor on\nthe merged partition Â®ğ‘£[ğ‘˜ğ‘–âˆ’1,ğ‘˜ğ‘–+1)and compare its size ğ‘†ğ‘€+(ğ‘˜ğ‘–+1âˆ’\nğ‘˜ğ‘–âˆ’1)Â·Î”(Â®ğ‘£[ğ‘˜ğ‘–âˆ’1,ğ‘˜ğ‘–+1))to the combined size of the original partitions\n2ğ‘†ğ‘€+(ğ‘˜ğ‘–âˆ’ğ‘˜ğ‘–âˆ’1)Â·Î”(Â®ğ‘£[ğ‘˜ğ‘–âˆ’1,ğ‘˜ğ‘–))+(ğ‘˜ğ‘–+1âˆ’ğ‘˜ğ‘–)Â·Î”(Â®ğ‘£[ğ‘˜ğ‘–,ğ‘˜ğ‘–+1)). We accept\nthis merge if it results in a size reduction. We iterate the partition\nlist multiple times until no qualified merge exists.\nWe summarize our vari-length partitioning algorithm as follows:\n[Init Phase] Scan all data point once. Pick a few â€œgoodâ€ initial\npositions to form the starting partitions.\n[Split Phase] Scan the starting partition set once.\nâ€¢Try â€œgrowingâ€ each starting partition by adding adjacent points.\nâ€¢Calculate the inclusion cost and approve the inclusion if it is be-\nlow the predefined threshold related to the model size. Otherwise,\nstart a new partition with a single point.\nâ€¢Stops after each point belongs to a partition.\n[Merge Phase] Scan the partition sets multiple times.\nâ€¢Merge a partition to its right neighbor if the combined one\nachieves a lower compression ratio.\nâ€¢Stops when no merge can reduce the total space.\nWe next discuss two critical aspects that largely determine the\nefficiency of the above split-merge algorithm.\nComputing Î”(Â®ğ‘£[ğ‘–,ğ‘—))Efficiently. The computational complex-\nity ofÎ”(Â®ğ‘£[ğ‘–,ğ‘—))dominates the overall algorithm complexity because\nthe function is invoked at every data point inclusion in the split\nphase. For a general ğ‘˜-degree polynomial modelÃ\nğ‘–âˆˆ[0,ğ‘˜]ğœƒğ‘–Â·ğ‘¥ğ‘–,\nwe can use the method introduced in [ 97] to compute Î”(Â®ğ‘£[ğ‘–,ğ‘—))\nstarting partition0303132294911912411812240132400223684446\nmerge03031322949119124118122\nâœ…split03031322949119124118122\nâœ…\nâœ…\n11-320705-6430\nrequired bitsdelta bitsFigure 6: Variable-length Partitioning on Delta Encoding â€“\nValue 29is successfully included into segment {30,31,32}in the\nsplit phase because its inclusion cost ğ¶[1,5)=6is less than the\npre-defined threshold ğœğ‘†ğ‘€=0.5Â·32=16. In the merge phase,\nthe attempt to merge segment {30,31,32,29}and{49}succeeds\nbecause the space consumption of the segment formed is smaller\nthan the summation of the two original segments.\nin linear time. To further speed up the process for the linear Re-\ngressor (which is most commonly used), we propose a much sim-\npler metric eÎ”(Â®ğ‘£[ğ‘–,ğ‘—))=log2(maxğ‘—âˆ’1\nğ‘˜=ğ‘–+1(ğ‘‘ğ‘˜)âˆ’minğ‘—âˆ’1\nğ‘˜=ğ‘–+1(ğ‘‘ğ‘˜)), where\nğ‘‘ğ‘˜=ğ‘£ğ‘˜âˆ’ğ‘£ğ‘˜âˆ’1to approximate the functionality of Î”(Â®ğ‘£[ğ‘–,ğ‘—))The in-\ntuition is that the proposed metric eÎ”(Â®ğ‘£[ğ‘–,ğ‘—))indicates the difficulty\nof the linear regression task and has a positive correlation to max\nbit-width measure Î”(Â®ğ‘£[ğ‘–,ğ‘—)).\nAs discussed in Section 2, Delta Encoding is considered a specific\ndesign point under the LeCo framework. The model in each Delta\npartition is an implicit step function, and only the first value in the\npartition is explicitly stored as the model. The prediction errors (i.e.,\ntheğ›¿â€²ğ‘ ) of Delta Encoding are the differences between each pair\nof the adjacent values. Therefore, Î”(Â®ğ‘£[ğ‘–,ğ‘—))=âŒˆlog2(maxğ‘—âˆ’1\nğ‘˜=ğ‘–+1ğ‘‘ğ‘˜)âŒ‰,\nwhereğ‘‘ğ‘˜=ğ‘£ğ‘˜âˆ’ğ‘£ğ‘˜âˆ’1. After adding the next data point ğ‘£ğ‘—to this par-\ntition, we can directly compute Î”(Â®ğ‘£[0,ğ‘—+1))=max{Î”(Â®ğ‘£[0,ğ‘—)),ğ‘‘ğ‘—}.\nSelecting Good Starting Positions. Because the algorithms\nused in both the split and merge phases are greedy, the quality\nof the algorithmsâ€™ starting partitions can significantly impact the\npartition results, especially for the split phase. Suppose we start\nat a â€œbumpyâ€ region Â®ğ‘£[ğ‘–,ğ‘—)during splitting. Because Î”(Â®ğ‘£[ğ‘–,ğ‘—))of\nthis partition is already large, there is a high probability that it\nstays the same when including an extra data point in the partition\n(i.e.,Î”(Â®ğ‘£[ğ‘–,ğ‘—+1))=Î”(Â®ğ‘£[ğ‘–,ğ‘—))). Therefore, the space cost of adding\nthis point becomes a constant ğ¶=Î”(Â®ğ‘£[ğ‘–,ğ‘—)). As long as ğ¶â‰¤ğœğ‘†ğ‘€,\nthis â€œbadâ€ partition would keep absorbing data points, which is\ndestructive to the overall compression.\nFor a general polynomial model of degreeğ‘˜, we select segments\nwhere the(ğ‘˜+1)th-order deltas (refer to the definition in Section 3.1)\nare minimized as the positions to initiate the partitioning algorithm.\nThe intuition is that the discrete (ğ‘˜+1)th-order deltas approximate\nthe(ğ‘˜+1)th-order derivatives of a continuous function of degree\nğ‘˜. If a segment has small (ğ‘˜+1)th-order deltas, the underlying\nfunction to be learned is less likely to contain terms with a degree\nmuch higher than ğ‘˜.\nFor Delta Encoding, a good starting partition is when the differ-\nences between the neighboring values are small (i.e., a small model\n5\n\nSIGMODâ€™24, June 11â€“16, 2024, Santiago, Chile Liu et al.\n......\nRange decoding error correction \n......\nDelta array     Header\nFigure 7: LeCoâ€™s Storage Format for One Partition\nprediction error) and when the neighboring points form roughly an\narithmetic progression (i.e., the partition has the potential to grow\nlarger). We, therefore, compute the bit-width for each delta in the\nsequence first (â€œrequired bitsâ€ in Figure 6). We then compute the\nsecond-order â€œdelta bitsâ€ based on those â€œrequired bitsâ€ and pick\nthe positions with the minimum value (the yellow-boxed zeros in\nFigure 6) as the initial partitions. The required bits are used as the\ntie-breaker to determine the partition growth precedence.\nTo summarize, we compared the split-merge partitioning algo-\nrithm with the linear Regressor against the optimal partitioning\nobtained via dynamic programming on real-world data sets intro-\nduced in Section 4.1 and found that our greedy algorithm imposes\nless than 3%overhead on the final compressed size.\n3.2.3 Partitioning Strategy Advising. Compared to fixed-length par-\ntitions, variable-length partitions could produce a higher compres-\nsion ratio with a cost of slower random access and compression\nspeed. The choice of the partitioning strategies depends largely\non the applicationâ€™s needs. To facilitate estimating the trade-offs,\nour Hyperparameter-Advisor provides two scores to indicate the\npotential space benefit of adopting the variable-length strategy.\nThe two scores are inspired by the definitions of â€œlocal hardnessâ€\n(Hğ‘™) and â€œglobal hardnessâ€ ( Hğ‘”) of a data set introduced in [ 107].\nHğ‘™captures the local unevenness in the values distribution, while\nHğ‘”depicts the degree of variation of the distribution at a global\nscale. Intuitively, if the data set is locally hard (i.e., Hğ‘™is high),\nno Regressor would fit the data well regardless of the partitioning\nstrategy. On the other hand, if the data set is locally easy but globally\nhard (i.e.,Hğ‘”is high), applying variable-length partitioning could\nimprove the compression ratio significantly because it is able to\ncatch the â€œsharp turnsâ€ in the global trend of the value distribution.\nSimilar to [ 107], we computeHğ‘™by running the piece-wise linear\napproximation (PLA) algorithm with a small error bound (e.g., ğœ–=7)\non the data set and count the number of segments generated. The\ncount is then divided by the data set size to normalize the Hğ‘™score.\nForHğ‘”, we run the same PLA algorithm with a much larger error\nbound (e.g., ğœ–=4096). Instead of counting the number of segments,\nwe use the the average gap3between adjacent segments and the\nvariance of the segment lengths to estimate the â€œglobal hardnessâ€ of\nthe value distribution. Hğ‘”is the summation of these two numbers,\nwith each normalized.\n3.3 Encoder and Decoder\nTheEncoder is responsible for generating the final compressed\nsequences. The input to the Encoder is a list of value partitions\nproduced by the Partitioner, where each partition is associated with\na model. The Encoder computes the delta for each value through\nmodel inference and then stores it in the delta array.\nThe storage format is shown in Figure 7. There is a header and\nadelta array for each partition. In the header, we first store the\nmodel parameters. For the default linear Regressor, the parameters\n3first value of the latter segment - last value of the former segment\nindex\n079104158a a aa d ba e aa g c  \ncharacter set: a-z 26-base\nFigure 8: LeCo String Compression â€“ An example including\nalgorithm optimizations and storage format modifications.\nare two 64-bit floating-point numbers: intercept ğœƒ0and slopeğœƒ1.\nBecause we bit pack the delta array according to the maximum delta,\nwe must record the bit-length ğ‘for an array item in the header.\nFor fixed-length partitions, the Encoder stores the partition size\nğ¿in the metadata. If the partitions are variable-length, the Encoder\nkeeps the start index (in the overall sequence) for each partition\nso that a random access can quickly locate the target partition. We\nuse ALEX [ 47] (a learned index) to record those start positions to\nspeed up the binary search.\nTo decompress a value given a position ğ‘–, theDecoder first deter-\nmines which partition contains the requested value. If the partitions\nare fixed-length, the value is located in the âŒŠğ‘–\nğ¿âŒ‹th partition. Other-\nwise, the Decoder conducts a â€œlower-boundâ€ search in the metadata\nto find the partition with the largest start index â‰¤ğ‘–.\nAfter identifying the partition, the Decoder reads the model\nparameters from the partition header and then performs a model\ninference using ğ‘–â€²=ğ‘–âˆ’start_index to get a predicted value Ë†ğ‘£.\nThen, the Decoder fetches the corresponding ğ›¿ğ‘–â€²in the delta array\nby accessing from the ( ğ‘Â·ğ‘–â€²)th bit to the(ğ‘Â·(ğ‘–â€²+1)âˆ’1)th bit. Finally,\nthe Decoder returns the decompressed value âŒŠË†ğ‘£âŒ‹+ğ›¿ğ‘–â€². Decoding a\nvalue involves at most two memory accesses, one for fetching the\nmodel (often cached) and the other for fetching the delta.\nThe basic algorithm for range decompression is to invoke the\nabove decoding process for each position in the range. Because of\nthe sequential access pattern, most cache misses are eliminated. For\nthe default linear regression, the Decoder performs two floating-\npoint calculations for model inference (one multiplication and one\naddition) and an integer addition for delta correction.\nWe carry out an optimization to increase the range decompres-\nsion throughput by 10âˆ’20%. For position ğ‘–, the model prediction is\nË†ğ‘£ğ‘–=ğœƒ0+ğœƒ1Â·ğ‘–. We can obtain Ë†ğ‘£ğ‘–by computing Ë†ğ‘£ğ‘–âˆ’1+ğœƒ1, thus saving\nthe floating-point multiplication. However, because of the limited\nprecision in the floating-point representation, the ğœƒ1-accumulation\nresult at certain position ğ‘–is incorrect (i.e.,âŒŠğœƒ0+Ãğ‘–\n1ğœƒ1âŒ‹+ğ›¿ğ‘–â‰ \nâŒŠğœƒ0+ğœƒ1Â·ğ‘–âŒ‹+ğ›¿ğ‘–). Therefore, we append an extra list to the delta\narray to correct the deviation at those positions.\n3.4 Extension to Handling Strings\nThe (integer-based) algorithms discussed so far can already benefit\na subset of the string columns in a relational table where the values\nare dictionary-encoded. In this section, we extend our support to\nmostly unique string values under the LeCo framework. The idea\nis to create an order-preserving mapping between the strings and\nlarge integers so that they can be fed to the Regressor.\nGiven a partition of string values, we first extract their common\nprefix (marked in dashed box in Figure 8) and store it separately in\n6\n\nLeCo: Lightweight Compression via Learning Serial Correlations SIGMODâ€™24, June 11â€“16, 2024, Santiago, Chile\nthe partition header . Then, we shrink the size of the character set\nif possible. Because many string data sets refer to a portion of the\nASCII table, we can use a smaller base to perform the string-integer\nmapping. For example, we adopt 26-based integers in Figure 8 with\nonly lower-case letters presenting.\nNotice that for an arbitrary ğ‘€-based mapping, the computation\nrequired to recover each character from the integer is expensive.\nGiven the mapped integer ğ‘£, it requires an integer modulo ğ‘£%ğ‘€\nto decode the current character and an integer division ğ‘£/ğ‘€to\nprepare for decoding the next one. Both operations take tens of\nCPU cycles. To speed up decoding, we set ğ‘€to its closest power\nof two ( 2ğ‘š) so that the modulo becomes a left-shift followed by\na bit-wise AND ( ğ‘£&((1<<ğ‘š)âˆ’1)), and the division becomes a\nright-shift (ğ‘£>>ğ‘š). For example, for strings that only consist of\nlower-case characters, we set ğ‘€=32.\nLeCo requires strings to be fixed-length. For a column of\nvarchar(3) , we pad every string to 3 bytes (padding bytes marked\nwith orange â€œaâ€ in Figure 8). An interesting observation is that we\ncan leverage the flexibility in choosing the padding characters to\nminimize the stored deltas. Suppose the string at position ğ‘–isğ‘ ğ‘–, and\nthe smallest/largest valid string after padding is ğ‘ ğ‘šğ‘–ğ‘›\nğ‘–/ğ‘ ğ‘šğ‘ğ‘¥\nğ‘–(i.e., pad\neach bit position with the smallest/largest character in the character\nset). We then choose the padding adaptively based on the predicted\nvalue Ë†ğ‘ ğ‘–from the Regressor to minimize the absolute value of the\nprediction error. If Ë†ğ‘ ğ‘–<ğ‘ ğ‘šğ‘–ğ‘›\nğ‘–, we adopt the minimum padding and\nstoreğ›¿ğ‘–=ğ‘ ğ‘šğ‘–ğ‘›\nğ‘–âˆ’Ë†ğ‘ ğ‘–in the delta array; if Ë†ğ‘ ğ‘–>ğ‘ ğ‘šğ‘ğ‘¥\nğ‘–, we use the\nmaximum padding and produce ğ›¿ğ‘–=ğ‘ ğ‘šğ‘ğ‘¥\nğ‘–âˆ’Ë†ğ‘ ğ‘–; ifğ‘ ğ‘šğ‘–ğ‘›\nğ‘–â‰¤Ë†ğ‘ ğ‘–â‰¤ğ‘ ğ‘šğ‘ğ‘¥\nğ‘–,\nwe choose Ë†ğ‘ ğ‘–as the padded string directly and obtain ğ›¿ğ‘–=0.\nThe lower part of Figure 8 shows the updated storage format\nto accommodate varchars. Additionally, the header includes the\nmaximum padding length (without prefix) along with the common\nprefix of the partition. We also record the length of each varchar\nvalue in the delta array (the slot before each delta value) to mark the\nboundary of the valid bytes from padded bytes in order to decode\ncorrectly. These lengths can be omitted for fixed-length strings.\n4 MICROBENCHMARK EVALUATION\nWe evaluate LeCo in two steps. In this section, we compare LeCo\nagainst state-of-the-art lightweight compression schemes through a\nset of microbenchmarks. We analyze LeCoâ€™s gains and trade-offs in\ncompression ratio, random access speed, and range decompression\nthroughput. In Section 5, we integrate LeCo into two widely-used\napplications to show the end-to-end performance.\n4.1 Compression Schemes and Data Sets\nThe baseline compression schemes under evaluation are Elias-\nFano [ 88,103], Frame-of-Reference (FOR) [ 59,117], Delta Encod-\ning (Delta) [ 29], and rANS [ 49]. FOR and Delta are introduced in\nSection 2. rANS is a variant of arithmetic encoding [ 106] with a\ndecoding speed similar to Huffman [ 63]. Elias-Fano is an encoding\nmechanism to compress a sorted list of integers. Suppose the list\nhasğ‘›integers, with ğ‘šbeing the difference between the maximum\nand minimum value of the sequence. Elias-Fano stores the lower\nâŒˆğ‘™ğ‘œğ‘”2(ğ‘š\nğ‘›)âŒ‰bits for each value explicitly with bit packing. For the\nremaining higher bits, Elias-Fano uses unary coding to record thenumber of appearances for each possible higher-bit value. For ex-\nample, the binary sequence 00000, 00011, 01101, 10000, 10010, 10011,\n11010, 11101 is encoded as â€œ00 11 01 00 10 11 10 01â€ for the lower\nbits and â€œ110 0 0 10 1110 0 10 10â€ for the higher bits. Elias-Fano is\nquasi-succinct [ 103] in that it only requires (2+âŒˆğ‘™ğ‘œğ‘”2(ğ‘š\nğ‘›)âŒ‰)bits\nper element.\nWe evaluate LeCo and the baseline solutions extensively on\nthirteen integer data sets:\nâ€“linear, normal : synthetic data sets with 200M 32-bit sorted\nintegers following a clean linear (or normal) distribution.\nâ€“poisson : 87M 64-bit timestamps following a Poisson distribution\nthat models events collected by distributed sensors [113].\nâ€“ml: 14M 64-bit sorted timestamps from the UCI-ML data set [ 18].\nâ€“booksale, facebook, wiki, osm : each with 200M 32-bit or 64-bit\nsorted integers from the SOSD benchmark [70].\nâ€“movieid : 20M 32-bit â€œlikedâ€ movie IDs from MovieLens [9].\nâ€“house_price : 100K 32-bit sorted integers representing the dis-\ntribution of house prices in the US [10].\nâ€“planet : 200M 64-bit sorted planet ID from OpenStreetMap [ 42].\nâ€“libio : 200M 64-bit sorted repository ID from libraries.io [85].\nâ€“medicare : (used in Section 4.5) 1.5 billion augmented 64-bit inte-\ngers (without order) exported from the public BI benchmark [ 24].\nseven additional non-linear data sets (used in Section 4.4):\nâ€“cosmos : 100M 32-bit data simulating a cosmic ray signal4.\nâ€“polylog : 10M 64-bit synthetic data of a biological population\ngrowth curve5.\nâ€“exp, poly : 200M 64-bit synthetic data, each block follows the\nexponential or polynomial distribution of different parameters.\nâ€“site, weight, adult : 250k, 25k and 30k sorted 32-bit in-\nteger column exported from the websites_train_sessions,\nweights_heights, and adult_train data sets in mlcourse.ai [23].\nnine tabular data sets, each sorted by its primary key column:\nâ€“lineitem, partsupp, orders : TPC-H [ 27] tables, scale factor = 1.\nâ€“inventory, catalog_sales, date_dim : from TPC-DS [ 26], sf = 1.\nâ€“geo, stock, course_info : real-world tables extracted from geon-\names [20], GRXEUR price [21] and Udemy course [22].\nand three string data sets:\nâ€“email : 30K email addresses (host reversed) with an average\nstring length of 15 bytes [2].\nâ€“hex: 100K sorted hexadecimal strings (up to 8 bytes) [36].\nâ€“word : 222K English words with an average length of 9 bytes [ 3].\nFigure 9a visualizes the eighteen integer data sets where notice-\nable unevenness is observed frequently in real-world data sets.\n4.2 Experiment Setup\nWe run the microbenchmark on a machine with Intel Â®XeonÂ®(Ice\nLake) Platinum 8369B CPU @ 2.70GHz and 32GB DRAM. The\nthree baselines are labeled as Elias-Fano ,FOR, and Delta-fix .\nDelta-var represents our improved version of Delta Encoding\nthat uses the variable-length Partitioner in LeCo. LeCo-fix and\n4We use(sinğ‘¥+10\n60ğœ‹+1\n10sin3(ğ‘¥+10)\n60ğœ‹)Ã—106+N( 0,100)to construct it.\n5Constructed by concatenating the polynomial and logarithm distribution, in turn,\nevery 500 records.\n7\n\nSIGMODâ€™24, June 11â€“16, 2024, Santiago, Chile Liu et al.\n0 2e+080 4.3e+09linear\n0 2e+080 4.3e+09normal\n0 2e+080 5.3e+08libio\n0 2e+081.2e+09wiki\n0 2e+080 4.3e+09booksale\n0 1.4e+071.5e+12ml\n0 1e+030 8.2e+04movieid\n0 8.7e+070 5e+16poisson\n0 1e+050 6e+07house_price\n0 2e+080 9.2e+09planet\n0 2e+080 1.8e+19facebook\n0 2e+080 1.4e+19osm\n0 3e+040 2e+09Poly\n0 2e+031 1.7e+15Exp\n0 1e+070 3.1e+09polylog\n0 2.5e+050 3.5e+04Site\n0.0 2.5\n1e46.028 7.5151e6Weight\n0 3.3e+040 1.5e+06Adult\n(a) Data Distribution Plot. â€“ The first row presents the nine data sets classified as â€œlocal easyâ€.\n0.0 0.2 0.4 0.6 0.8 1.0\nLocal Hardness0.30.40.50.60.70.80.91.0Global Hardnesshouse_price\nwikilibiobooksale\nplanetml\nfacebook normal\nlinearpoisson\nmovieid\nosm (b) Dataset Hardness.\nFigure 9: Data Distribution with Hardness evaluation.\nLeCo-var arelinear-Regressor LeCo prototypes that adopt fixed-\nlength and variable-length partitioning, respectively. The corre-\nsponding LeCo variants with polynomial Regressor are labeled\nLeCo-Poly-fix andLeCo-Poly-var .\nFor all the fixed-length partitioning methods, the partition size\nis obtained through a quick sampling-based parameter search de-\nscribed in Section 3.2.1. For Delta-var, LeCo-var, and LeCo-Poly-var,\nwe set the split-parameter ğœto be small (in the range [0,0.15]) in\nfavor of the compression ratio over the compression throughput.\nGiven a data set, an algorithm under test first compresses\nthe whole data set and reports the compression ratio (i.e., com-\npressed_size / uncompressed_size ) and compression throughput.\nThen the algorithm performs ğ‘uniformly-random accesses ( ğ‘\nis the size of the data set) and reports the average latency. Finally,\nthe algorithm decodes the entire data set and measures the decom-\npression throughput. All experiments run on a single thread in the\nmain memory. We repeat each experiment three times and report\nthe average result for each measurement.\n4.3 Integer Benchmark\nFigure 10 shows the experiment results for compression ratio, ran-\ndom access latency, and decompression throughput on the twelve\ninteger data sets. Elias-Fano does not apply to poisson andmovieid\nbecause these two data sets are not fully-sorted.\nOverall, LeCo achieves a Pareto improvement over the existing\nalgorithms. Compared to Elias-Fano and FOR, the LeCo variants\nobtain a significantly better compression ratio while retaining a\ncomparable decompression and random access speed. When com-\npared to Delta Encoding, LeCo remains competitive in the compres-\nsion ratio while outperforming the Delta variants by an order of\nmagnitude in random access.\n4.3.1 Compression Ratio .As shown in the first row of Figure 10,\nthe compression ratios from the LeCo variants are strictly better\nthan the corresponding ones from FOR. This is because FOR is\na special case of LeCo: the output of its Regressor is fixed to a\nhorizontal line (refer to Section 2).\nWe further plot the local hardness Hğ‘™and the global hardness\nHğ‘”(defined in Section 3.2.3) of the different data sets in Figure 9b.\nThe horizontal/vertical dashed line marks the average global/local\nhardness among the data sets. we observe that LeCoâ€™s compression-\nratio advantage over FOR is larger on locally-easy data sets ( 40.9%\nimprovement on average) than the three locally-hard data sets ( 9.3%\nimprovement on average). This is because local unevenness in the\ndistribution makes it difficult for a regression algorithm to fit well.LeCo also compresses better than Elias-Fano across (almost) all\ndata sets. Although Elias-Fano is proved to be quasi-succinct, it\nfails to leverage the embedded serial correlation between the values\nfor further compression. rANS remains the worst, which indicates\nthat the redundancy embedded in an integer sequence often comes\nmore from the serial correlation rather than the entropy.\nCompared to Delta Encoding, LeCo shows a remarkable im-\nprovement in compression ratio for â€œsmoothâ€ (synthetic) data sets:\nlinear ,normal , and poisson . For the remaining (real-world) data\nsets, however, LeCo remains competitive. This is because many\nreal-world data sets exhibit local unevenness, as shown in Figure 9a.\nThe degree of such irregularity is often at the same level as the\ndifference between adjacent values.\nAnother observation is that variable-length partitioning is effec-\ntive in reducing the compression ratio on real-world data sets that\nhave rapid slope changes or irregular value gaps (e.g., movieid ,\nhouse_price ). Our variable-length partitioning algorithm pro-\nposed in Section 3.2 is able to detect those situations and create\npartitions accordingly to avoid oversized partitions caused by un-\nfriendly patterns to the Regressor. We also notice that LeCo-var\nachieves an additional 28.2%compression compared to LeCo-fix on\nthe four locally-easy and globally-hard data sets, while the improve-\nment drops to <10%for the remaining data sets6. This indicates\nthat the two metrics used for the partitioning strategy advising\n(refer to Section 3.2.3) is effective in identifying data sets that can\npotentially benefit from variable-length partitions.\n4.3.2 Random Access .The second row of Figure 10 presents the\naverage latency of decoding a single value in memory for each\ncompression scheme. The random access speed of LeCo-fix is com-\nparable to that of FOR because they both require only two memory\naccesses per operation. FOR is often considered the lower bound of\nthe random access latency for lightweight compression because it\ninvolves minimal computation (i.e., an integer addition). Compared\nto FOR, LeCo-fix requires an additional floating-point multiplica-\ntion. This overhead, however, is mostly offset by a better cache hit\nratio because LeCo-fix produces a smaller compressed sequence.\nLeCo-var is slower because it has to first search the metadata to\ndetermine the corresponding partition for a given position. This\nindex search takes an extra 35âˆ’90ns depending on the total number\nof partitions. The Delta variants are an order of magnitude slower\nthan the others in most data sets because they must decompress\nthe entire partition sequentially to perform a random access.\n6Except for the ideal cases in linear andnormal\n8\n\nLeCo: Lightweight Compression via Learning Serial Correlations SIGMODâ€™24, June 11â€“16, 2024, Santiago, Chile\n0204060Compression Ratio(%)77.6 74.1 72.1 84.8 87.1rANS FOR Elias-Fano Delta Delta-var Leco Leco-var model size\n0100200Random Access(ns)5e+05 5e+05 9e+05 1e+05 5e+05 1e+06 1e+05 7e+05 5e+05 5e+05 2e+05 1e+06\n40 395743 51 5848\n16 2443\n10651657 1643 334 377 347 344 394 285 5371884 1857 530 1329 641 525 549 753 378 1053 625\n28 297255 567767\n19 2663\n1161\nlinear normal libio wiki booksale planet facebook ml movieid poisson house_price osm0.01.02.03.04.0Decode TPS(GB/s)4.14 7.77.42\nFigure 10: Compression Microbenchmark â€“ Measurement of seven compression schemes on twelve integer data sets from three aspects:\nCompression Ratio, Random Access Latency, and Full Decompression Throughput. We break down the compression ratio into model size\n(marked with the cross pattern) and delta size in the first row. The dashed lines split these data sets into four groups in the order of locally\neasy - globally easy, locally hard - globally easy, locally easy - globally hard, and locally hard - globally hard according to Figure 9b.\nFOR Elias-Fano Delta-fix Delta-var LeCo-fix LeCo-var\n0.81Â±0.28 0.58Â±0.17 1.04Â±0.14 0.04Â±0.01 0.78Â±0.11 0.02Â±0.01\nTable 1: Compression Throughput (GB/s).\nmovieid poly cosmos exp polylog site weight adult020406080CPR(%)FOR\nLeCooptimal\nrecommend\nFigure 11: Regressor Selection Result.\n4.3.3 Full Decompression .The third row in Figure 10 shows\nthe throughput of each compression algorithm for decompressing\nan entire data set. In general, LeCo-fix is 14%âˆ’34%7slower than its\nfastest competitor FOR because LeCo-fix involves an extra floating-\npoint operation upon decoding each record. Delta-var and LeCo-var\nperform exceptionally well on house_price . The reason is that\npart of the data set contains sequences of repetitive values. LeCoâ€™s\nPartitioner would detect them and put them into the same segment,\nmaking the decompression task trivial for these partitions.\n4.3.4 Compression throughput .Table 1 shows the compression\nthroughput for each algorithm weighted averaged across all the\ntwelve data sets with error bars. LeCo-fix has a similar compres-\nsion speed to the baselines because our linear Regressor has a low\ncomputational overhead. Algorithms that adopt variable-length\npartitioning (i.e., Delta-var and LeCo-var), however, are an order of\nmagnitude slower because the Partitioner needs to perform multiple\nscans through the data set and invokes the Regressor (or an approx-\nimate function) frequently along the way. Such a classic trade-off\nbetween compression ratio and throughput is often beneficial to\napplications that do not allow in-place updates.\nrANS FOR LeCo\n-fixLeCo\n-varLeCo-\n Poly-fixLeCo-\nPoly-varsin 2sin 2sin-freq020406080100CPR(%)82.2\n61.454.6 50.542.3 41.836.725.8 21.1Figure 12: Compression ratio on cosmos .\n4.4 Cases for higher-order models\nAlthough linear models perform sufficiently well in the above inte-\nger benchmark8, there are cases where higher-order models shine.\nBecause our setting is mostly read-only, it is usually worthwhile\nto spend more computation to compress the data once and then\nbenefit from long-term space and query efficiency.\nWe first verify the effectiveness of our Regressor Selector in the\nHyperparameter Advisor (refer to Section 3.2.3). In this experiment,\nwe consider the following six Regressor types: constant (FOR), lin-\near, polynomial up to a degree of three, exponential, and logarithm.\nWe create synthetic data sets (with random noise) for each Regres-\nsor type and extract the features introduced in Section 3.2.3 to train\nthe classification model (i.e., CART) offline.\nWe compare the compression ratios obtained by using our rec-\nommended Regressor per partition (labeled recommend ) to those ob-\ntained by FOR, LeCo-fix, and the optimal (i.e., exhaustively search\nin the candidate Regressor types and pick the one with the best\ncompression ratio). Figure 11 shows the results. Note that none\nof the eight tested data sets were used for training. We observe\nthatrecommend achieves a compression ratio close to the optimal ,\nwith up to 64.7%improvement over LeCo-fix (with linear regres-\nsion only) on data sets that exhibit higher-order patterns. For data\nsets that are mostly linear (e.g. movieid ), the benefit of applying\nhigher-order models is limited, as expected.\nOne can even extend the LeCo framework to leverage domain\nknowledge easily. For example, the cosmos data set contains a\nmixture of two signals (i.e., sine function) with random noise. As\n7except for house_price where the enhancement of FOR over LeCo-fix is 49%\n8Many data sets in the integer benchmark come from the SOSD benchmark [ 70], which\nfavors linear models.\n9\n\nSIGMODâ€™24, June 11â€“16, 2024, Santiago, Chile Liu et al.\nshown in Figure 12, if we include a sine term in the Regressor\n(labeled sin), we are able to achieve a better compression ratio\n(36.7%) compared to the recommended polynomial model ( 42.3%).\nIf we include two sine terms (labeled 2sin ), we are able to extract an\nadditional 29.7%compression out of the LeCo framework compared\ntosin. If we further know the approximate frequencies of the\ntwo sine terms (labeled 2sin-freq ), LeCo produces an even better\ncompression ratio, as presented in Figure 12.\n4.5 Compressing Dictionaries\nBuilding dictionaries that preserve the key ordering is a common\ntechnique to achieve compression and speed up query process-\ning [ 34,86,114]. Reducing the memory footprint of such dictionar-\nies is an important use case of LeCo. In the following experiment,\nwe perform a hash join with the probe side being dictionary en-\ncoded. Specifically, we use the medicare dataset as the probe-side\ncolumn, and we pre-build a hash table of size 84MB in memory,\nwhich contains 50% of the unique values (i.e., 50% hash table hit\nratio during the join). The probe side first goes through a filter of\nselectivity of 1% and then probes the hash table for the join. The\nprobe-side values are encoded using an order-preserving dictionary\ncompressed by LeCo (i.e., LeCo-fix), FOR, and Raw (i.e., no com-\npression). We vary the memory budget from 3GB to 500MB and\nreport the throughput (defined as the raw data size of the probe\nside divided by the query execution time) of executing this query.\nFigure 14 shows that applying LeCo improves the throughput up\nto95.7Ã—compared to FOR when the memory budget for this query is\nlimited. This is because LeCo compresses the probe-side dictionary\nfrom 2.4GB to 5.5MB (cpr ratio = 0.23%) so that it constantly fits\nin memory. For comparison, the dictionary size compressed using\nFOR is still 400MB (cpr ratio = 17%). When the available memory is\nlimited, this larger dictionary causes a significant number of buffer\npool misses, thus hurting the overall query performance.\n4.6 Multi-Column Benchmark\nIn this section, we evaluate the effectiveness of LeCo on nine multi-\ncolumn tabular data sets9. As shown in Figure 13 (bottom right), we\ncompute the â€œsortednessâ€ of a table (in the range [0, 1]) by averaging\nthe sortedness of each column using the portion of inverse pairs [ 39]\nas the metric.\nFrom Figure 13 (the top row), we observe that LeCo achieves a\nbetter compression ratio than FOR in all nine tables. This is because\ncolumns in a table are often correlated [ 58,65,95]. Our â€œsorted-\nnessâ€ metric indicates that non-primary-key columns have different\ndegrees of correlation with the primary-key (i.e., sorting) column\nacross tables, thus partially inheriting the serial patterns. Tables\nwith high sortedness such as inventory anddata_dim are more\nlikely to achieve better compression ratios with the LeCo variants.\nThe bottom left of Figure 13 presents the compression ratios of\nthe TPC-H tables10with high-cardinality columns only (i.e., NDV\n> 10% #row). LeCoâ€™s has a more noticeable advantage over FOR on\ncolumns that are likely to select FOR as the compression method.\n9Elias-Fano is not included as a baseline because most columns are not strictly sorted.\n10Due to space limitations, we only present the results of TPC-H. Results of the other\nsix data sets can be found in our technique report at [25].4.7 String Benchmark\nWe compare LeCo (i.e., LeCo-fix) against the state-of-the-art light-\nweight string compression algorithm FSST [ 36] using three string\ndata sets email ,hexandwords . FSST adopts a dictionary-based\napproach by building a fine-grained static symbol table to map a\npartial string to a 1-byte code. Because each compressed string has\na variable length, FSST must store a byte-offset array to support\nrandom access. An optimization (not mentioned in the FSST paper)\nis to delta-encode this offset array to trade its random access speed\nfor a better compression ratio. To perform a fair comparison, we\ntested six different block sizes of the delta encoding: 0 (i.e., no delta\ncompression), 20, 40, 60, 80, and 100. For LeCo, we present two data\npoints with different character-set sizes.\nFigure 15 shows the random access latencies and compression\nratios for different algorithm configurations. Each LeCo point is\nmarked with the base value used to convert strings. We observed\nthat LeCoâ€™s string extension provides a higher random access speed\nwhile retaining a competitive compression ratio, compared to FSST\non both email andhexdata sets. The compression ratio of LeCo,\nhowever, is slightly worse than that of FSST on word . This is because\ndictionary-based algorithms are more suitable for human-readable\nstrings that contain repeating patterns such as common prefixes,\nroots, and suffixes, while learned compression is better at leveraging\nserial patterns between values.\n4.8 Partitioner Efficiency\nIn this section, we compare LeCoâ€™s default Partitioner (as described\nin Section 3.2) to state-of-the-art partitioning algorithms, includ-\ning the PLA algorithm adopted by time-series compression [ 87],\nas well as FITing tree [ 57], Sim-Piece introduced in [ 72], and the\nla_vector algorithm proposed in [ 35]. The angle-based PLA pre-\ndefines a fixed global prediction error bound ( ğœ–) and determines\nthe partition boundaries greedily in one pass. Sim-Piece adopts\nthe angle-based PLA as its partitioner and compactly stores linear\nmodels with the same intercept together to reduce the overall space.\nThey sacrifice model parameter precisions to create more segments\nwith the same intercept. On the other hand, la_vector translates\neach data point ğ‘£ğ‘–into a vertex ğ‘–, where the weight of edge (ğ‘–,ğ‘—)is\ndefined as the compression ratio of segment [ğ‘£ğ‘–,ğ‘£ğ‘—]. The optimal\npartitioning problem is thus converted into finding the shortest\npath in the above graph G. la_vector approximates GwithGâ€²with\nfewer edges and proofs that the best compression ratio achieved\nonGâ€²is at mostğ‘˜Â·ğ‘™larger than that on Gwhereğ‘˜is a constant\nandğ‘™is the shortest path length.\nWe integrated PLA, Sim-Piece, and la_vector into the LeCo frame-\nwork (with the linear Regressor denoted by LeCo-PLA, Sim-Piece\nand LeCo-la-vec, respectively) and repeated the experiments in Sec-\ntion 4.3 on four representative data sets. As shown in Figure 16, all\nthree candidate methods exhibit significantly worse compression\nratios compared to LeCo-var. The globally-fixed error bound in\nLeCo-PLA fails to adapt to data segments with rapidly changing\nslopes. We also found that LeCo-PLA is more sensitive to its hy-\nperparameter compared to LeCo-var, as shown in Figure 17 where\nwe sweep the hyperparameters for LeCo-PLA ( ğœ–) and LeCo-var ( ğœ)\non the books data set. The model compaction in Sim-Piece doesnâ€™t\ntake effect because, on mostly sorted data sets, the intercept of\n10\n\nLeCo: Lightweight Compression via Learning Serial Correlations SIGMODâ€™24, June 11â€“16, 2024, Santiago, Chile\nlineitem partsupp orders inventory catalog_sales date_dim geo stock course_info0204060Compression Ratio(%)5.1%\n15.1% 12.3%\n25.9%0.3%\n55.2%11.6%\n7.1%5.6% 5.1%\n20.5%11.8%\n31.1%17.0%\n78.9%28.8%\n14.0%15.7%FOR Delta-fix Delta-var LeCo-fix LeCo-var\nlineitem\n (2/16) partsupp\n (2/5)orders\n (2/9)inventory\n (0/4)catalog_sales\n (13/34)date_dim\n (5/28)geo\n (3/17)stock\n (5/6)course_info\n (1/6)0204060Compression Ratio(%)15.58%\n31.63%18.15%\n25.85%1.26%\n90.37%10.75%\n7.12%29.69%16.08%\n32.35%17.67%\n31.14%12.45%\n91.2%22.77%\n14.01%41.63%\nnamelineitempartsuppordersinventorycatalog_salesdata_dimgeostockcourse_infosize725MB114MB164MB226MB283MB10MB1.5GB9MB73MBsortedness0.240.320.510.810.070.780.450.980.19# total columns1659434281766# numeric columns84343316456\nFigure 13: Multiple Column â€“ Compression ratio of five methods on nine tabular data sets. The second row of the result only considers\ncolumns with cardinality â‰¥10%. We report the size in bytes, average sortedness (in the range [0,1]), total column number, and integer/nu-\nmerical column number of each table. We mark the enhancement ratio of LeCo variants over FOR above the bars.\n3G 2G 1G 800M 700M 600M 500M\nMemory Limit(B)0.00.51.01.52.02.53.0Throughput (GB/s)\n1.2x3.2x\n42.1x\n95.7x\n34.1x4.2x 2.2xLeCo\nFOR\nRaw\nFigure 14: Hash Probe TPS.\n0 50 100 150 200 250 300 350\nRandom Access(ns)20\n020406080Compression Ratio(%)\n7812864433226\nFSST_Email\nFSST_HEX\nFSST_WordsLeCo_Email\nLeCo_HEX\nLeCo_Words Figure 15: String Evaluation.\nnormal  house\npricebooksale movieid0102030405060Compression Ratio(%)LeCo-fix\nLeCo-PLA\nLeco-la-vec\nSim-Piece\nLeCo-var Figure 16: Partition efficiency.\n0.00 0.04 0.08 0.12 0.16 0.20\nLeCo-var hyper parameter0204060Compression Ratio(%)\nLeCo-var LeCo-PLA345678910111213LeCo-PLA hyper parameter\n Figure 17: Robustness test.\neach linear model is also increasing. The precision sacrifice in their\nimplementation results in an even worse compression ratio on\nhouse_price compared to LeCo-PLA. For LeCo-la-vec, although\nit finds the shortest path in the approximate â€œcompression-ratio\ngraphâ€, it overlooked the length of the shortest path, resulting in\nan excessive number of models that dominate the compressed size\non data sets such as movieid .\n5 SYSTEM EVALUATION\nTo show how LeCo can benefit real-world systems, we integrated\nLeCo into two system applications: (1) a columnar execution\nengine implemented using Arrow [ 4] and Parquet [ 6] and (2)\nRocksDB [ 14]. All experiments are conducted on a machine with\n4Ã—IntelÂ®XeonÂ®(Cascade Lake) Platinum 8269CY CPU @ 2.50GHz,\n32GB DRAM, and a local NVMe SSD of 447GB with 250k maximum\nread IOPS. We use Apache Arrow 8.0.0, Parquet version 2.6.0, and\nRocksDB Release version 6.26.1 in the following experiments.\n5.1 Integration to Arrow and Parquet\nWe first integrated LeCo (as well as FOR and Delta for comparison)\ninto Apache Arrow (the most widely-used columnar in-memory for-\nmat) and Apache Parquet (the most widely-used columnar storage\nformat), and built an execution engine prototype using their C++\nlibraries to demonstrate how LeCo can benefit query processing.\nParquet uses dictionary encoding as the default compression\nmethod. It falls back to plain encoding if the dictionary grows too\nlarge. We refer to this mechanism as Default . In the followingexperiments, we set Parquetâ€™s row group size to 10M rows and\ndisable block compression unless specified otherwise.\nThe primary component of the Arrow format is the Arrow Ar-\nray that represents a sequence of values of the same type. Except\nfor basic dictionary encoding, no compression is applied to Arrow\narrays to guarantee maximum query-processing performance. We\nre-implemented the Arrow Array structure using lightweight com-\npression methods (i.e., LeCo, FOR, and Delta) without changing its\ninterface. We use a consistent lightweight-compressed format for\nthe Arrow Array and Parquet Column Chunk so that no additional\ndecoding is required when scanning the data from disk to memory.\nThe Arrow Compute library implements various basic database\noperators (e.g., Take, Filter, GroupBy) on Arrow arrays as compute\nfunctions. Our execution engine uses these compute functions as\nbuilding blocks. The engine is implemented using late materializa-\ntion [ 37] where intermediate results are passed between operators\nas position bitmaps. We also push down the filters to the storage\nlayer (i.e., Parquet).\n5.1.1 Filter-Groupby-Aggregation. We create a query template of a\ntypical filter-groupby-aggregation as follows. Suppose we have 10k\nsensors recording measurements. The table Thas three columns:\n(1)ts, timestamps (in seconds, almost sorted) extracted from the\nml[18] data set, (2) id, 16-bit sensor IDs ranging from 1 to 10k, and\n(3)val, 64-bit-integer sensor readings. To vary the compressibility\nof the table, we generate two different data distributions for the id\nand the valcolumns: (1) random : both idandvalare randomly\n11\n\nSIGMODâ€™24, June 11â€“16, 2024, Santiago, Chile Liu et al.\n0.02.55.07.510.0Query Time(s)3.3x 3.2x 3.2x 3.2x 2.5x\n2.1x2.4x 2.5x 2.5x2.3x\n1.3x 1.3x 1.3x 1.3x1.2xrandom\n0.001 0.01 0.1 1 10\nSelectivity(%)0.02.55.07.510.0Query Time(s)5.2x 5.2x 5.2x 5.0x 3.5x\n2.7x 3.1x 3.2x 3.2x2.6x\n1.7x 1.7x 1.7x 1.7x1.4xcorrelatedDefault_groupby_CPU\nDefault_filter_CPU\nDefault_IO\nDelta_groupby_CPU\nDelta_filter_CPU\nDelta_IO\nFOR_groupby_CPU\nFOR_filter_CPU\nFOR_IO\nLeCo_groupby_CPU\nLeCo_filter_CPU\nLeCo_IO\nFigure 18: Filter Groupby Aggregation\ngenerated and are difficult to compress no matter which algorithm,\nand (2) correlated :ids are clustered in groups of 100, and vals\nare monotonically increasing across groups (but random within\na group). There are serial patterns in this setting for lightweight\ncompression algorithms to leverage.\nWe construct the following query that outputs the aver-\nage reading for each sensor within a given time range per\nday: SELECT AVG(val) FROM T WHERE ts\\_begin < ts \\% val\\_2 <\nts\\_end GROUP BY id . We adjust the time range (i.e., ts_end -\nts_begin ) to control the queryâ€™s selectivity. When executing this\nquery, our execution engine first pushes down the filter predicate to\nParquet, which outputs a bitmap representing the filtering results.\nThe engine then scans the idand the valcolumn from Parquet\ninto Arrow arrays and performs the groupby-aggregation. Both\ngroupby and aggregation only decode entries that are still valid\naccording to the filter-bitmap, which involves random accesses to\nthe corresponding Arrow arrays.\nWe generated four Parquet files with Default, Delta, FOR, and\nLeCo as the encoding algorithms (with a partition size of 10k en-\ntries). In the case of random distribution, the resulting file sizes are\n3.8GB, 1.3GB, 1.5GB, and 1.4GB, respectively. For the correlated\ndistribution, the corresponding file sizes are 3.8GB, 706MB, 1.2GB,\nand 785MB (with better compression ratios). We execute the above\nquery template and repeat each query instance three times with its\naverage execution time reported.\nAs shown in Figure 18, all three lightweight compression al-\ngorithms outperform the Default because of the significant I/O\nsavings proportional to the file size reduction. Compared to Delta,\nLeCo is much more CPU-efficient because Delta requires to decode\nthe entire partition to random-access particular entries during the\ngroupby-aggregation. Compared to FOR, LeCo mainly gains its\nadvantage through the I/O reduction due to a better compression\nratio. This I/O advantage becomes larger with a more compressible\ndata set (i.e., correlated ).\nInterestingly, LeCo is up to 10.5Ã—faster than FOR when perform-\ning the filter operation. Suppose that the model of a partition is\nğœƒ0+ğœƒ1Â·ğ‘–, and the bit-length of the delta array is ğ‘. For a less-than\npredicateğ‘£<ğ›¼, for example, once LeCo decodes the partition up\nto positionğ‘˜, whereğœƒ0+ğœƒ1Â·ğ‘˜âˆ’2ğ‘âˆ’1>ğ›¼(assumeğœƒ1â‰¥0), we\ncan safely skip the values in the rest of the sequence because they\nare guaranteed to be out of range. FOR cannot perform such a\ncomputation pruning because the tscolumn is not strictly sorted.\n5.1.2 Bitmap Aggregation. In this experiment, we zoom in on\nthe critical bitmap aggregation operation of the above end-to-end\nquery and further verify LeCoâ€™s performance and space benefits on\nfour different data sets introduced in Section 4.1: normal ,poisson ,\n0.001 0.01 0.1 1 100.00.51.0Query Time(s)normal\nDefault_CPU\nDefault_IO\nDelta_CPUDelta_IO\nFOR_CPU\nFOR_IOLeCo_CPU\nLeCo_IO\n0.001 0.01 0.1 1 100.00.51.0booksale\n0.001 0.01 0.1 1 10\nselectivity(%)0.00.51.01.52.0Query Time(s)poisson\n0.001 0.01 0.1 1 10\nselectivity(%)0.00.51.01.52.0mlFigure 19: Bitmap Aggregation.\nnormal books poisson ml0.00.51.01.5File Size (GB)1.3x\n2.7x\n5.8x1.4x\n1.0x1.1x1.5x\n1.0x\n1.1x\n7.6x1.1x\n1.2xDefault\nDefault-zstd\nFOR\nFOR-zstd\nLeCo\nLeCo-zstd\nFigure 20: Parquet With zstd Compression â€“ Numbers on bars\nindicating additional improvement introduced by zstd.\nbooksale , and ml11. For each data set, we create four Parquet files\nwith different lightweight compression algorithms (i.e., Default,\nDelta, FOR, and LeCo) enabled as above. The bitmaps used in the\nexperiments include ten set-bit clusters following a Zipf-like distri-\nbution with a varying ratio of â€œonesâ€ (to represent different filter\nselectivities). Data is scanned directly into Arrow arrays in a row-\ngroup granularity, where a row-group is skipped if the bits in the\ncorresponding area in the bitmap are all zeros. We then feed the\narrays and the bitmap to the Arrow Compute function to perform\nthe summation.\nAs shown in Figure 19, LeCo consistently outperforms Default\n(by up to 11.8Ã—), Delta (by up to 3.9Ã—), and FOR (by up to 5.0Ã—).\nLeCoâ€™s speedup comes from both the I/O reduction (due to a better\ncompression ratio) and the CPU saving (due to fast random access\nand better caching). Moreover, we found that LeCo consumes less\nmemory during the execution. The peak memory usage (for pro-\ncessing a Parquet row group) of LeCo is 60.5%,35.3%, and 10.0%less\ncompared to Default, FOR, and Delta, respectively on average. This\nis much preferred for systems with constrained memory budgets.\n5.1.3 Enabling Block Compression. People often enable block com-\npression on columnar storage formats such as Parquet and ORC [ 5]\nto further reduce the storage overhead. We repeat the Parquet load-\ning phase of the above experiments with zstd [ 19] enabled to show\nhow block compression algorithms affect the final file sizes.\nAs shown in Figure 20, the additional improvement introduced\nby zstd is marked above each bar. Applying zstd on top of the\nlightweight encoding schemes in Parquet can further reduce the\nfile sizes. The relative improvement of LeCo + zstd over LeCo is\nhigher than that in the case of FOR. This shows that LeCoâ€™s ability\nto remove serial redundancy is complementary to some degree to\nthe general-purpose block compression algorithms.\n11we scale mlto 200M rows while preserving its value distribution.\n12\n\nLeCo: Lightweight Compression via Learning Serial Correlations SIGMODâ€™24, June 11â€“16, 2024, Santiago, Chile\nDefault FOR LeCo0.00.51.01.52.02.53.0Query Time(s)CPU_w/o_zstd\nIO_w/o_zstd\nCPU_w/_zstd\nIO_w/_zstd\nFigure 21: Time breakdown\nof zstd on Parquet.\n0.100.110.120.130.140.15\n2 3 4 6 100.000.010.02\nBlock Cache size (GB)Million op/s\nBaseline_1\nBaseline_16Baseline_128\nLeCoFigure 22: RocksDB Seek\nQuery Throughput.\nThe decompression overhead of zstd, however, can be significant.\nWe perform the bitmap selection experiment with zstd turned on for\nParquet. Figure 21 shows an example result ( mldata set, selectivity\n= 0.01). We observe that the I/O savings from zstd are outweighed by\nits CPU overhead, leading to an increase in the overall query time.\nThe result confirms our motivation in Section 2 that heavyweight\ncompression algorithms are likely to cause CPU bottlenecks in\nmodern data processing systems.\n5.2 RocksDB Index Block Compression\nRocksDB is a key-value store based on log-structured merge trees.\nEach level consists of a sorted run of key-value pairs stored in a\nsequence of SSTables. Each SSTable is divided into multiple data\nblocks (4KB by default). RocksDB builds an index on top of the data\nblocks. For each pair of adjacent data blocks ğµğ‘–âˆ’1andğµğ‘–, an index\nentry is created where the key is the shortest string greater than\nthe last key in ğµğ‘–âˆ’1and smaller than the first key in ğµğ‘–. The value\nof the index entry is a â€œblock handleâ€ that records the byte offset\nand the size of ğµğ‘–. To locate a particular key ğ‘˜, RocksDB performs\na binary search in the index block and obtains the entry with the\nsmallest keyâ‰¥ğ‘˜. It then reads the associated â€œblock handleâ€ and\nfetches the corresponding data block that (potentially) contains ğ‘˜.\nRocksDB offers a native compression scheme for the index blocks.\nIt includes a hyper-parameter called â€œrestart intervalâ€ (RI) to make\ntrade-offs between the lookup performance and the index size. The\nvalue of RI determines the size of a compression unit in an index\nblock. Within each compression unit, RocksDB applies a variation\nof Delta Encoding to both the keys and values. For the index keys,\nsupposeğ‘˜ğ‘–âˆ’1proceedsğ‘˜ğ‘–in the compressed sequence. Then ğ‘˜ğ‘–\nis encoded as(ğ‘šğ‘–,ğ‘˜â€²\nğ‘–)whereğ‘šğ‘–denotes the length of the shared\nprefix between ğ‘˜ğ‘–âˆ’1andğ‘˜ğ‘–, andğ‘˜â€²\nğ‘–is the remaining suffix. For the\nâ€œblock handlesâ€, RocksDB simply stores the offset of each block in a\ndelta-encoded sequence.\nWe use LeCo to compress the keys and values separately in a\nRocksDB index block to shrink its size and to improve the lookup\nperformance at the same time. We adopt LeCo-fix for both key and\nvalue sequences. Because all internal keys in RocksDB are strings,\nwe use LeCo with the string extension to compress the keys.\nWe compare RocksDB with LeCo against12three baseline con-\nfigurations: Baseline_1 ,Baseline_16 , and Baseline_128 . The\nnumber at the end of each label denotes the value of the RI param-\neter (1 is RocksDBâ€™s default). We configured RocksDB according\nto the settings in its Performance Benchmark [ 15]13. We turned on\ndirect I/O to bypass the large OS page cache.\n12The fixed partition size are set to 64 entries for LeCo.\n13block_size = 4096B; pin_l0_filter_and_index_blocks_in _cache is enabled.In each experiment, we first load the RocksDB with 900 million\nrecord generated from the above RocksDB Performance Benchmark.\nEach record has a 20-byte key and a 400-byte value. The resulting\nRocksDB is around 110GB. LeCo, Baseline_1 ,Baseline_16 , and\nBaseline_128 achieve a compression ratio of 28.1%, 71.3%, 18.9%\nand 15.9%, respectively on the index blocks in RocksDB. We then\nperform 200M non-empty Seek queries using 64 threads. The query\nkeys are generated using YCSB [ 43] with a skewed configuration\nwhere 80%of the queries access 20%of the total keys. We repeat\neach experiment three times and report the average measurement.\nFigure 22 shows the system throughputs for LeCo, and the base-\nlines with a varying block cache size. RocksDB with LeCo consis-\ntently outperforms the three baseline configurations by up to 16%\ncompared to the second-best configuration. The reasons are two-\nfold. First, compared to Baseline_1 where no compression for the\nindex blocks are carried out (each compression unit only contains\none entry), LeCo produces smaller index blocks so that more data\nblocks can fit in the block cache to save I/Os. Such a performance\nimprovement is more recognizable with a smaller block cache.\nSecond, compared to Baseline_16 andBaseline_128 where\nthe index blocks are compressed using Delta Encoding. Although\nLeCo no longer exhibits an index-size advantage over these base-\nlines, it saves a significant amount of computations. Compared to\nBaseline_128 which need to decompress the entire 128-entry unit\nbefore it accesses a single entry, LeCo only requires two memory\nprobes to perform a random access in the index block.\nTo sum up, applying LeCo speeds up binary search in the index\nblocks. Such a small change improved the performance of a complex\nsystem (RocksDB) noticeably. We believe that other systems with\nsimilar â€œzone-mapâ€ structures can benefit from LeCo as well.\n6 RELATED WORK\nMany prior compression algorithms leverage repetitions in a data\nsequence. Null suppression omits the leading zeros in the bit rep-\nresentation of an integer and records the byte length of each\nvalue [ 1,29,92,96,100]. Dictionary [ 32,34,36,83,86,94,114]\nand entropy-based compression algorithms [ 63,106] build a bijec-\ntive map between the original values and the code words. Block\ncompression algorithms such as LZ77 [ 116], Gzip [ 7], Snappy [ 8],\nLZ4 [ 11], and zstd [ 19] achieve compression by replacing repeated\nbit patterns with shorter dictionary codes. These approaches, how-\never, miss the opportunity to exploit the serial correlation between\nvalues to achieve a compressed size beyond Shannonâ€™s Entropy.\nA pioneer work by Boffa et al. [ 35] proposed to use a similar linear\nmodel as in the PGM-Index [ 55] with a customized partitioning\nalgorithm (i.e., la_vector) to compress a specific data structure called\nthe rank&select dictionaries. Their approach represents a specific\ndesign point in the LeCo framework that is much more general and\nextensible in model types and partitioning algorithms. Also, LeCoâ€™s\ndefault variable-length partitioning algorithm is shown to be more\nefficient than la_vector for compressing columnar data.\nSemantic compression [ 58,65,66] aims to compress tabular data\nby exploiting correlations between columns using complex models\nlike Bayesian networks. LFR[ 111] and DFR[ 110] use linear model\nor Delta-like model to compress data without partitioning. Because\n13\n\nSIGMODâ€™24, June 11â€“16, 2024, Santiago, Chile Liu et al.\ntheir model parameters vary at each data point, they do not support\nquick random access.\nData partitioning plays an essential role in achieving a good\ncompression ratio for various algorithms. Several prior work [ 88,\n91] targeting inverted indexes proposed partitioning algorithms for\nspecific compression schemes like Elias-Fano [ 103] and VByte [ 102,\n105]. The partitioning algorithms introduced in Section 3.2 are\napplicable to an arbitrary linear combination of regression models.\nIn terms of storage format, FastPFOR [ 117] and NewPFD [ 112]\nstores outlier values separately in a different format to improve the\noverall storage and query efficiency.\nTime-series/IoT data compression field adopts a similar idea with\nLeCo of approximating data distribution with models, but they tar-\nget keeping the prediction error within a predetermined threshold\nand achieve lossy compression. Their optimization goal is to mini-\nmize the total space of model parameters. Partitioning algorithms\nfor linear models [ 51,87,108] and constant value models [ 77] are de-\nsigned to minimize the segment number. Sim-Piece[ 72] introduces\na more compact format to keep the output models. Eichinger et\nal. [50] consider utilizing higher order models but require additional\ncomputation effort in the approximation process.\nCodec selection is critical in improving data compression per-\nformances. A common practice is to define a feature set and use\nmachine learning classifiers for selection. Abadi et al .[29] empiri-\ncally analyzed the performance of different codecs and manually\nbuilt a decision tree for selection. While the features introduced by\nCodecDB [ 68] overlook the chance to utilize distribution patterns,\nin contrast to our Regressor Selector.\nBoth learned indexes and learned compression use regression to\nmodel data distributions. RMI [ 73] and RS [ 71] apply hierarchical\nmachine learning models to fit the CDFs, while PGM-Index [ 55],\nFITing-Tree [ 57], and CARMI [ 115] put more effort into the par-\ntitioning strategies to reduce model prediction errors. ALEX [ 47]\nand Finedex [ 82] proposed techniques such as a gapped array and\nnon-blocking retraining to improve the indexesâ€™ update efficiency.\nPrevious work [ 28,118] have shown that heavyweight compres-\nsion algorithms [ 7,8,63] designed for disk-oriented systems could\nincur notable computational overhead to the overall system per-\nformance. Algorithms such as FSST [ 36] and PIDS [ 67], therefore,\nemphasize low CPU usage besides a competitive compression ratio.\nOther related work reduces the computational overhead by enabling\ndirect query execution on compressed formats [29, 45, 68], includ-\ning filter and aggregation/join pushdowns [ 41,46,54,60,75,79,84].\n7 CONCLUSION\nThis paper introduces LeCo, a lightweight compression framework\nthat uses machine learning techniques to exploit serial correlation\nbetween the values in a column. We provide a complementary per-\nspective besides Shannonâ€™s entropy to the general data compression\nproblem. The LeCo framework bridges data mining and data com-\npression with a highly modular design. Both our micro-benchmark\nand system evaluation show that LeCo is able to achieve better\nstorage efficiency and faster query processing simultaneously.REFERENCES\n[1]2009. Google Varint. https://static.googleusercontent.com/media/research.\ngoogle.com/en//people/jeff/WSDM09-keynote.pdf.\n[2]2018. 300 Million Email Database. https://archive.org/details/\n300MillionEmailDatabase.\n[3]2020. English Word Dataset in HOPE. https://github.com/efficient/HOPE/blob/\nmaster/datasets/words.txt.\n[4] 2022. Apache Arrow. https://arrow.apache.org/.\n[5] 2022. Apache ORC. https://orc.apache.org/.\n[6] 2022. Apache Parquet. https://parquet.apache.org/.\n[7] 2022. GNU GZip. https://www.gnu.org/software/gzip/.\n[8] 2022. Google snappy. http://google.github.io/snappy/.\n[9]2022. Kaggle Movie ID dataset. https://www.kaggle.com/datasets/grouplens/\nmovielens-20m-dataset?select=rating.csv.\n[10] 2022. Kaggle USA Real Estate Dataset. https://www.kaggle.com/datasets/\nahmedshahriarsakib/usa-real-estate-dataset?select=realtor-dataset-100k.csv.\n[11] 2022. Lz4. https://github.com/lz4/lz4.\n[12] 2022. Personal communication, anonymized for review. .\n[13] 2022. Real-time Analytics for MySQL Database Service. https://www.oracle.\ncom/mysql/.\n[14] 2022. Rocksdb Github. https://github.com/facebook/rocksdb.\n[15] 2022. Rocksdb Performance Benchmarks. https://github.com/facebook/rocksdb/\nwiki/Performance-Benchmarks.\n[16] 2022. Samsung 980 PRO 4.0 NVMe SSD. https://www.samsung.com/us/\ncomputing/memory-storage/solid-state-drives/980-pro-pcie-4-0-nvme-ssd-\n1tb-mz-v8p1t0b-am/.\n[17] 2022. SingleStore. https://www.singlestore.com/.\n[18] 2022. UCI Machine Learning Repository: Timestamp in Bar Crawl: Detecting\nHeavy Drinking Data Set. https://archive.ics.uci.edu/ml/datasets/Bar+Crawl%\n3A+Detecting+Heavy+Drinking.\n[19] 2022. Zstandard. https://github.com/facebook/zstd.\n[20] 2023. GeoNames Data. https://www.geonames.org/export/.\n[21] 2023. HistData GRXEUR. https://www.histdata.com/.\n[22] 2023. Kaggle Udemy Courses. https://www.kaggle.com/datasets/hossaingh/\nudemy-courses.\n[23] 2023. mlcourse.ai. https://github.com/Yorko/mlcourse.ai/tree/main/data.\n[24] 2023. Public BI Benchmark. https://homepages.cwi.nl/~boncz/\nPublicBIbenchmark/.\n[25] 2023. Technical Report. https://gitfront.io/r/Leco2023/Hk2zGFeQUSVw/Learn-\nto-Compress/blob/Leco_sigmod2024_techreport.pdf.\n[26] 2023. TPC-DS Benchmark Standard Specification. https://www.tpc.org/tpcds/.\n[27] 2023. TPC-H Benchmark Standard Specification. https://www.tpc.org/tpch/.\n[28] Daniel Abadi, Peter Boncz, Stavros Harizopoulos Amiato, Stratos Idreos, and\nSamuel Madden. 2013. The design and implementation of modern column-oriented\ndatabase systems . Now Hanover, Mass.\n[29] Daniel Abadi, Samuel Madden, and Miguel Ferreira. 2006. Integrating compres-\nsion and execution in column-oriented database systems. In Proceedings of the\n2006 ACM SIGMOD international conference on Management of data . 671â€“682.\n[30] Shikha Agrawal and Jitendra Agrawal. 2015. Survey on anomaly detection using\ndata mining techniques. Procedia Computer Science 60 (2015), 708â€“713.\n[31] Ramakrishna Varadarajan Nga Tran Ben Vandiver Lyric Doshi Chuck Bear\nAndrew Lamb, Matt Fuller. 2012. The Vertica Analytic Database: C-Store 7 Years\nLater. Proceedings of the VLDB Endowment 5, 12 (2012), 1790â€“1801.\n[32] Gennady Antoshenkov, David Lomet, and James Murray. 1996. Order preserving\nstring compression. In Proceedings of the Twelfth International Conference on\nData Engineering . IEEE, 655â€“663.\n[33] Nikos Armenatzoglou, Sanuj Basu, Naga Bhanoori, Mengchu Cai, Naresh\nChainani, Kiran Chinta, Venkatraman Govindaraju, Todd J Green, Monish Gupta,\nSebastian Hillig, et al .2022. Amazon Redshift Re-invented. In Proceedings of the\n2022 ACM SIGMOD International Conference on Management of Data . 2205â€“2217.\n[34] Carsten Binnig, Stefan Hildenbrand, and Franz FÃ¤rber. 2009. Dictionary-based\norder-preserving string compression for main memory column stores. In Pro-\nceedings of the 2009 ACM SIGMOD International Conference on Management of\ndata. 283â€“296.\n[35] Antonio Boffa, Paolo Ferragina, and Giorgio Vinciguerra. 2021. A â€œLearnedâ€ Ap-\nproach to Quicken and Compress Rank/Select Dictionaries. In 2021 Proceedings\nof the Workshop on Algorithm Engineering and Experiments (ALENEX) . SIAM,\n46â€“59.\n[36] Peter Boncz, Thomas Neumann, and Viktor Leis. 2020. FSST: fast random\naccess string compression. Proceedings of the VLDB Endowment 13, 12 (2020),\n2649â€“2661.\n[37] Peter A. Boncz, Marcin Zukowski, and Niels Nes. 2005. MonetDB/X100: Hyper-\nPipelining Query Execution. In Second Biennial Conference on Innovative Data\nSystems Research, CIDR . 225â€“237.\n[38] Paul Boniol, John Paparrizos, Themis Palpanas, and Michael J Franklin. 2021.\nSAND: streaming subsequence anomaly detection. Proceedings of the VLDB\nEndowment 14, 10 (2021), 1717â€“1729.\n14\n\nLeCo: Lightweight Compression via Learning Serial Correlations SIGMODâ€™24, June 11â€“16, 2024, Santiago, Chile\n[39] C. G. Borroni. 2013. A new rank correlation measure. Statistical Papers 54, 2\n(2013), 255â€“270.\n[40] Scott H Cameron. 1966. Piece-wise linear approximations . Technical Report. IIT\nRESEARCH INST CHICAGO IL COMPUTER SCIENCES DIV.\n[41] Lemke Christian, Sattler Kai-Uwe, Faerber Franz, and Zeier Alexander. 2010.\nSpeeding up queries in column stores: a case for compression. DaWaK (2010)\n(2010), 117â€“129.\n[42] Google Cloud. 2017. OpenStreetMap(2017). https://console.cloud.google.com/\nmarketplace/details/openstreetmap/geo-openstreetmap.\n[43] Brian F Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell\nSears. 2010. Benchmarking cloud serving systems with YCSB. In Proceedings of\nthe 1st ACM symposium on Cloud computing . 143â€“154.\n[44] Benoit Dageville, Thierry Cruanes, Marcin Zukowski, Vadim Antonov, Artin\nAvanes, Jon Bock, Jonathan Claybaugh, Daniel Engovatov, Martin Hentschel,\nJiansheng Huang, et al .2016. The snowflake elastic data warehouse. In Proceed-\nings of the 2016 International Conference on Management of Data . 215â€“226.\n[45] Patrick Damme, Annett UngethÃ¼m, Johannes Pietrzyk, Alexander Krause, Dirk\nHabich, and Wolfgang Lehner. 2020. Morphstore: Analytical query engine with a\nholistic compression-enabled processing model. arXiv preprint arXiv:2004.09350\n(2020).\n[46] Dinesh Das, Jiaqi Yan, Mohamed Zait, Satyanarayana R Valluri, Nirav Vyas, Ra-\nmarajan Krishnamachari, Prashant Gaharwar, Jesse Kamp, and Niloy Mukherjee.\n2015. Query optimization in Oracle 12c database in-memory. Proceedings of the\nVLDB Endowment 8, 12 (2015), 1770â€“1781.\n[47] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid Lomet, and Tim Kraska. 2019. ALEX: An Updatable Adaptive Learned\nIndex. (2019). https://doi.org/10.1145/3318464.3389711 arXiv:arXiv:1905.08898\n[48] Siying Dong, Andrew Kryczka, Yanqin Jin, and Michael Stumm. 2021. RocksDB:\nevolution of development priorities in a key-value store serving large-scale\napplications. ACM Transactions on Storage (TOS) 17, 4 (2021), 1â€“32.\n[49] Jarek Duda. 2013. Asymmetric numeral systems: entropy coding combining\nspeed of Huffman coding with compression rate of arithmetic coding. arXiv\npreprint arXiv:1311.2540 (2013).\n[50] Frank Eichinger, Pavel Efros, Stamatis Karnouskos, and Klemens BÃ¶hm. 2015.\nA time-series compression technique and its application to the smart grid. The\nVLDB Journal (2015).\n[51] Hazem Elmeleegy, Ahmed Elmagarmid, Emmanuel Cecchet, Walid G Aref, and\nWilly Zwaenepoel. 2009. Online piece-wise linear approximation of numerical\nstreams with precision guarantees. (2009).\n[52] Christos Faloutsos and Vasileios Megalooikonomou. 2007. On data mining,\ncompression, and kolmogorov complexity. Data mining and knowledge discovery\n15, 1 (2007), 3â€“20.\n[53] Franz FÃ¤rber, Sang Kyun Cha, JÃ¼rgen Primsch, Christof BornhÃ¶vd, Stefan Sigg,\nand Wolfgang Lehner. 2012. SAP HANA database: data management for modern\nbusiness applications. ACM Sigmod Record 40, 4 (2012), 45â€“51.\n[54] Ziqiang Feng, Eric Lo, Ben Kao, and Wenjian Xu. 2015. Byteslice: Pushing\nthe envelop of main memory data processing with a new storage layout. In\nProceedings of the 2015 ACM SIGMOD International Conference on Management\nof Data . 31â€“46.\n[55] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds. Proceedings of the\nVLDB Endowment 13, 8 (2020), 1162â€“1175.\n[56] Kenneth Flamm. 2019. Measuring Mooreâ€™s law: evidence from price, cost, and\nquality indexes. In Measuring and Accounting for Innovation in the 21st Century .\nUniversity of Chicago Press.\n[57] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim\nKraska. 2019. Fiting-tree: A data-aware index structure. In Proceedings of the\n2019 International Conference on Management of Data . 1189â€“1206.\n[58] Yihan Gao and Aditya Parameswaran. 2016. Squish: Near-optimal compression\nfor archival of relational datasets. In Proceedings of the 22nd ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining . 1575â€“1584.\n[59] Jonathan Goldstein, Raghu Ramakrishnan, and Uri Shaft. 1998. Compressing\nrelations and indexes. In Proceedings 14th International Conference on Data\nEngineering . IEEE, 370â€“379.\n[60] Goetz Graefe and Leonard D Shapiro. 1990. Data compression and database\nperformance . University of Colorado, Boulder, Department of Computer Science.\n[61] Anurag Gupta, Deepak Agarwal, Derek Tan, Jakub Kulesza, Rahul Pathak,\nStefano Stefani, and Vidhya Srinivasan. 2015. Amazon redshift and the case for\nsimpler data warehouses. In Proceedings of the 2015 ACM SIGMOD international\nconference on management of data . 1917â€“1923.\n[62] Dongxu Huang, Qi Liu, Qiu Cui, Zhuhe Fang, Xiaoyu Ma, Fei Xu, Li Shen, Liu\nTang, Yuxing Zhou, Menglong Huang, et al .2020. TiDB: a Raft-based HTAP\ndatabase. Proceedings of the VLDB Endowment 13, 12 (2020), 3072â€“3084.\n[63] David A Huffman. 1952. A method for the construction of minimum-redundancy\ncodes. Proceedings of the IRE 40, 9 (1952), 1098â€“1101.\n[64] Nguyen Quoc Viet Hung, Hoyoung Jeung, and Karl Aberer. 2012. An evaluation\nof model-based approaches to sensor data compression. IEEE Transactions onKnowledge and Data Engineering 25, 11 (2012), 2434â€“2447.\n[65] Amir Ilkhechi, Andrew Crotty, Alex Galakatos, Yicong Mao, Grace Fan, Xiran\nShi, and Ugur Cetintemel. 2020. DeepSqueeze: deep semantic compression for\ntabular data. In Proceedings of the 2020 ACM SIGMOD international conference\non management of data . 1733â€“1746.\n[66] HV Jagadish, Jason Madar, and Raymond T Ng. 1999. Semantic compression\nand pattern extraction with fascicles. In VLDB , Vol. 99. 186â€“97.\n[67] Hao Jiang, Chunwei Liu, Qi Jin, John Paparrizos, and Aaron J Elmore. 2020. PIDS:\nattribute decomposition for improved compression and query performance in\ncolumnar storage. Proceedings of the VLDB Endowment 13, 6 (2020), 925â€“938.\n[68] Hao Jiang, Chunwei Liu, John Paparrizos, Andrew A Chien, Jihong Ma, and\nAaron J Elmore. 2021. Good to the Last Bit: Data-Driven Encoding with CodecDB.\nInProceedings of the 2021 International Conference on Management of Data . 843â€“\n856.\n[69] Alfons Kemper and Thomas Neumann. 2011. HyPer: A hybrid OLTP&OLAP\nmain memory database system based on virtual memory snapshots. In 2011\nIEEE 27th International Conference on Data Engineering . IEEE, 195â€“206.\n[70] A. Kipf, R Marcus, A Van Renen, M. Stoian, A. Kemper, T. Kraska, and T. Neu-\nmann. 2019. SOSD: A Benchmark for Learned Indexes. (2019).\n[71] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass learned\nindex. In Proceedings of the Third International Workshop on Exploiting Artificial\nIntelligence Techniques for Data Management . 1â€“5.\n[72] Xenophon Kitsios, Panagiotis Liakos, Katia Papakonstantinopoulou, and Yannis\nKotidis. [n.d.]. Sim-Piece: Highly Accurate Piecewise Linear Approximation\nthrough Similar Segment Merging. ([n. d.]).\n[73] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings of the 2018 international\nconference on management of data . 489â€“504.\n[74] Tirthankar Lahiri, Shasank Chavan, Maria Colgan, Dinesh Das, Amit Ganesh,\nMike Gleeson, Sanket Hase, Allison Holloway, Jesse Kamp, Teck-Hua Lee, et al .\n2015. Oracle database in-memory: A dual format in-memory database. In 2015\nIEEE 31st International Conference on Data Engineering . IEEE, 1253â€“1258.\n[75] Harald Lang, Tobias MÃ¼hlbauer, Florian Funke, Peter A Boncz, Thomas Neu-\nmann, and Alfons Kemper. 2016. Data blocks: Hybrid OLTP and OLAP on\ncompressed storage using both vectorization and compilation. In Proceedings of\nthe 2016 International Conference on Management of Data . 311â€“326.\n[76] Per-Ã…ke Larson, Adrian Birka, Eric N Hanson, Weiyun Huang, Michal\nNowakiewicz, and Vassilis Papadimos. 2015. Real-time analytical processing\nwith SQL server. Proceedings of the VLDB Endowment 8, 12 (2015), 1740â€“1751.\n[77] I. Lazaridis and S. Mehrotra. 2003. Capturing sensor-generated time series with\nquality guarantees. In Data Engineering, 2003. Proceedings. 19th International\nConference on .\n[78] Juchang Lee, SeungHyun Moon, Kyu Hwan Kim, Deok Hoe Kim, Sang Kyun\nCha, and Wook-Shin Han. 2017. Parallel replication across formats in SAP\nHANA for scaling out mixed OLTP/OLAP workloads. Proceedings of the VLDB\nEndowment 10, 12 (2017), 1598â€“1609.\n[79] Jae-Gil Lee, Gopi Attaluri, Ronald Barber, Naresh Chainani, Oliver Draese,\nFrederick Ho, Stratos Idreos, Min-Soo Kim, Sam Lightstone, Guy Lohman, et al .\n2014. Joins on encoded and partitioned data. Proceedings of the VLDB Endowment\n7, 13 (2014), 1355â€“1366.\n[80] Daniel Lemire and Leonid Boytsov. 2015. Decoding billions of integers per\nsecond through vectorization. Software: Practice and Experience 45, 1 (2015),\n1â€“29.\n[81] Ming Li, Paul VitÃ¡nyi, et al .2008. An introduction to Kolmogorov complexity and\nits applications . Vol. 3. Springer.\n[82] Pengfei Li, Yu Hua, Jingnan Jia, and Pengfei Zuo. 2021. FINEdex: a fine-grained\nlearned index scheme for scalable and concurrent memory systems. Proceedings\nof the VLDB Endowment 15, 2 (2021), 321â€“334.\n[83] Yinan Li, Craig Chasseur, and Jignesh M Patel. 2015. A padded encoding scheme\nto accelerate scans by leveraging skew. In Proceedings of the 2015 ACM SIGMOD\nInternational Conference on Management of Data . 1509â€“1524.\n[84] Yinan Li and Jignesh M Patel. 2013. Bitweaving: Fast scans for main memory data\nprocessing. In Proceedings of the 2013 ACM SIGMOD International Conference on\nManagement of Data . 289â€“300.\n[85] Libraries.io. 2017. Repository ID in Libraries.io. https://libraries.io/data.\n[86] Chunwei Liu, McKade Umbenhower, Hao Jiang, Pranav Subramaniam, Jihong\nMa, and Aaron J Elmore. 2019. Mostly order preserving dictionaries. In 2019\nIEEE 35th International Conference on Data Engineering (ICDE) . IEEE, 1214â€“1225.\n[87] Ge Luo, Ke Yi, Siu-Wing Cheng, Zhenguo Li, Wei Fan, Cheng He, and Yadong Mu.\n2015. Piecewise linear approximation of streaming time series data with max-\nerror guarantees. In 2015 IEEE 31st international conference on data engineering .\nIEEE, 173â€“184.\n[88] Giuseppe Ottaviano and Rossano Venturini. 2014. Partitioned elias-fano indexes.\nInProceedings of the 37th international ACM SIGIR conference on Research &\ndevelopment in information retrieval . 273â€“282.\n15\n\nSIGMODâ€™24, June 11â€“16, 2024, Santiago, Chile Liu et al.\n[89] Fatma Ã–zcan, Yuanyuan Tian, and Pinar TÃ¶zÃ¼n. 2017. Hybrid transactional/-\nanalytical processing: A survey. In Proceedings of the 2017 ACM International\nConference on Management of Data . 1771â€“1775.\n[90] Massimo Pezzini, Donald Feinberg, Nigel Rayner, and Roxane Edjlali. 2014. Hy-\nbrid transaction/analytical processing will foster opportunities for dramatic\nbusiness innovation. Gartner (2014, January 28) Available at https://www. gart-\nner. com/doc/2657815/hybrid-transactionanalyticalprocessing-foster-opportunities\n(2014), 4â€“20.\n[91] Giulio Ermanno Pibiri and Rossano Venturini. 2019. On optimally partitioning\nvariable-byte codes. IEEE Transactions on Knowledge and Data Engineering 32, 9\n(2019), 1812â€“1823.\n[92] J. Plaisance, N. Kurz, and D Lemire. 2016. Vectorized VByte Decoding. Comput-\nerence (2016).\n[93] Hasso Plattner. 2009. A common database approach for OLTP and OLAP\nusing an in-memory column database. In Proceedings of the 2009 ACM SIGMOD\nInternational Conference on Management of data . 1â€“2.\n[94] Vijayshankar Raman, Gopi Attaluri, Ronald Barber, Naresh Chainani, David\nKalmuk, Vincent KulandaiSamy, Jens Leenstra, Sam Lightstone, Shaorong Liu,\nGuy M Lohman, et al .2013. DB2 with BLU acceleration: So much more than just\na column store. Proceedings of the VLDB Endowment 6, 11 (2013), 1080â€“1091.\n[95] Vijayshankar Raman and Garret Swart. 2006. How to wring a table dry: Entropy\ncompression of relations and querying of compressed relations. In Proceedings\nof the 32nd international conference on Very large data bases . 858â€“869.\n[96] Benjamin Schlegel, Rainer Gemulla, and Wolfgang Lehner. 2010. Fast integer\ncompression using SIMD instructions. In Proceedings of the Sixth International\nWorkshop on Data Management on New Hardware . 34â€“40.\n[97] Raimund Seidel. 1991. Small-dimensional linear programming and convex hulls\nmade easy. Discrete & Computational Geometry 6 (1991), 423â€“434.\n[98] Claude Elwood Shannon. 1948. A mathematical theory of communication. The\nBell system technical journal 27, 3 (1948), 379â€“423.\n[99] Fabrizio Silvestri and Rossano Venturini. 2010. Vsencoding: efficient coding and\nfast decoding of integer lists via dynamic programming. In Proceedings of the\n19th ACM international conference on Information and knowledge management .\n1219â€“1228.\n[100] Alexander A Stepanov, Anil R Gangolli, Daniel E Rose, Ryan J Ernst, and\nParamjit S Oberoi. 2011. SIMD-based decoding of posting lists. In Proceed-\nings of the 20th ACM international conference on Information and knowledge\nmanagement . 317â€“326.\n[101] Phillip M Taylor, Nathan Griffiths, Zhou Xu, and Alexandros Mouzakitis. 2019.\nData mining and compression: where to apply it and what are the effects?. In\nProceedings of the 8th SIGKDD International Workshop on Urban Computing .\nACM.\n[102] Larry H Thiel and HS Heaps. 1972. Program design for retrospective searches\non large data bases. Information Storage and Retrieval 8, 1 (1972), 1â€“20.\n[103] Sebastiano Vigna. 2013. Quasi-succinct indices. In Proceedings of the sixth ACM\ninternational conference on Web search and data mining . 83â€“92.\n[104] Benjamin Welton, Dries Kimpe, Jason Cope, Christina M Patrick, Kamil Iskra,\nand Robert Ross. 2011. Improving i/o forwarding throughput with data com-\npression. In 2011 IEEE International Conference on Cluster Computing . IEEE,\n438â€“445.\n[105] Hugh E Williams and Justin Zobel. 1999. Compressing integers for fast file\naccess. Comput. J. 42, 3 (1999), 193â€“201.\n[106] Ian H Witten, Radford M Neal, and John G Cleary. 1987. Arithmetic coding for\ndata compression. Commun. ACM 30, 6 (1987), 520â€“540.\n[107] Chaichon Wongkham, Baotong Lu, Chris Liu, Zhicong Zhong, Eric Lo, and\nTianzheng Wang. 2022. Are updatable learned indexes ready? Proceedings of\nthe VLDB Endowment 15, 11 (2022), 3004â€“3017.\n[108] Xie, Qing, Zhang, Xiangliang, Zhou, Xiaofang, Deng, Ke, Pang, and Chaoyi.\n2014. Maximum error-bounded Piecewise Linear Representation for online\nstream approximation. VLDB journal: The international journal of very large\ndata bases (2014).\n[109] Qiumin Xu, Huzefa Siyamwala, Mrinmoy Ghosh, Tameesh Suri, Manu Awasthi,\nZvika Guz, Anahita Shayesteh, and Vijay Balakrishnan. 2015. Performance anal-\nysis of NVMe SSDs and their implication on real world databases. In Proceedings\nof the 8th ACM International Systems and Storage Conference . 1â€“11.\n[110] Ren Xuejun, Fang Dingyi, and Chen Xiaojiang. 2011. A Difference Fitting\nResiduals algorithm for lossless data compression in wireless sensor nodes. In\n2011 IEEE 3rd International Conference on Communication Software and Networks .\n481â€“485. https://doi.org/10.1109/ICCSN.2011.6013638\n[111] Ren Xuejun and Ren Zhongyuan. 2018. A Sensor Node Lossless Compression\nAlgorithm Based on Linear Fitting Residuals Coding. In Proceedings of the 10th\nInternational Conference on Computer Modeling and Simulation (ICCMS â€™18) .\nAssociation for Computing Machinery, New York, NY, USA, 62â€“66. https:\n//doi.org/10.1145/3177457.3177482\n[112] Hao Yan, Shuai Ding, and Torsten Suel. 2009. Inverted index compression and\nquery processing with optimized document ordering. In Proceedings of the 18th\ninternational conference on World wide web . 401â€“410.[113] Huanchen Zhang, Hyeontaek Lim, Viktor Leis, David G Andersen, Michael\nKaminsky, Kimberly Keeton, and Andrew Pavlo. 2018. Surf: Practical range\nquery filtering with fast succinct tries. In Proceedings of the 2018 International\nConference on Management of Data . 323â€“336.\n[114] Huanchen Zhang, Xiaoxuan Liu, David G Andersen, Michael Kaminsky, Kim-\nberly Keeton, and Andrew Pavlo. 2020. Order-preserving key compression for\nin-memory search trees. In Proceedings of the 2020 ACM SIGMOD International\nConference on Management of Data . 1601â€“1615.\n[115] Jiaoyi Zhang and Yihan Gao. 2022. CARMI: A Cache-Aware Learned Index with\na Cost-based Construction Algorithm. Proceedings of the VLDB Endowment 15,\n11 (2022), 2679 â€“ 2691.\n[116] J. Ziv and A. Lempel. 1977. A universal algorithm for data compression. IEEE\nTransactions on Information Theory 23, 3 (1977), 337â€“343.\n[117] Marcin Zukowski, Sandor Heman, Niels Nes, and Peter Boncz. 2006. Super-\nscalar RAM-CPU cache compression. In 22nd International Conference on Data\nEngineering (ICDEâ€™06) . IEEE, 59â€“59.\n[118] Marcin Zukowski, Mark Van de Wiel, and Peter Boncz. 2012. Vectorwise: A\nvectorized analytical DBMS. In 2012 IEEE 28th International Conference on Data\nEngineering . IEEE, 1349â€“1350.\n16",
  "textLength": 105460
}