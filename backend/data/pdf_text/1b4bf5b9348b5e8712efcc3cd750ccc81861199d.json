{
  "paperId": "1b4bf5b9348b5e8712efcc3cd750ccc81861199d",
  "title": "Parsimonious Learning-Augmented Caching",
  "pdfPath": "1b4bf5b9348b5e8712efcc3cd750ccc81861199d.pdf",
  "text": "Parsimonious Learning-Augmented Caching\nSungjin Im1Ravi Kumar2Aditya Petety1Manish Purohit2\nAbstract\nLearning-augmented algorithms‚Äîin which, traditional algorithms are augmented with machine-learned\npredictions‚Äîhave emerged as a framework to go beyond worst-case analysis. The overarching goal is to design\nalgorithms that perform near-optimally when the predictions are accurate yet retain certain worst-case guarantees\nirrespective of the accuracy of the predictions. This framework has been successfully applied to online problems\nsuch as caching where the predictions can be used to alleviate uncertainties.\nIn this paper we introduce and study the setting in which the learning-augmented algorithm can utilize the pre-\ndictions parsimoniously. We consider the caching problem‚Äîwhich has been extensively studied in the learning-\naugmented setting‚Äîand show that one can achieve quantitatively similar results but only using a sublinear\nnumber of predictions.\n1. Introduction\nLearning-augmented algorithms have recently emerged as a framework to strengthen traditional algorithms with machine\nlearned predictions. Traditional algorithm design obtains algorithms with formal guarantees for allinputs. Hence, they\nare often geared towards working well on worst-case inputs and not for typical, real-world instances. In contrast, machine\nlearning performs extremely well on typical instances but can occasionally fail on rare instances. The learning-augmented\nframework aims to design algorithms that can beneÔ¨Åt from the machine learning predictions while retaining worst-case\nguarantees.\nThis framework was initiated by Kraska et al. (2018), who demonstrated that indexed data structures can be improved using\nlearned predictions. Inspired by their work, Lykouris & Vassilvitskii (2018) studied the classic online caching problem\nand obtained an algorithm whose performance guarantee gracefully degrades as the prediction quality worsens but still\nremains robust regardless of the prediction quality. The learning-augmented framework has found applications in streaming\nalgorithms, data structures, and particularly for online algorithms where predictions can alleviate the uncertainties for\nunseen future inputs; see the survey (Mitzenmacher & Vassilvitskii, 2020).\nIn this paper we focus on an important yet largely overlooked aspect in previous works‚Äîthe cost of predictions . Predictions\nare typically obtained from an ML model, which can be computationally expensive; this makes it highly desirable to use\npredictions parsimoniously . In this work we study online caching in the learning-augmented framework in which hints are\nused sparingly.\nOnline caching. In online caching, a sequence of page requests arrive at a cache of size k. If the requested page is in\nthe cache, then it can be served at no extra cost, but otherwise a cache miss occurs to fetch the missing page into the\ncache. The goal of an online algorithm is to minimize the number of cache misses. A number of randomized algorithms\nare known to be \u0002(logk)-competitive (Achlioptas et al., 2000; Fiat et al., 1991), meaning that they incur O(logk)times\nmore cache misses than the ofÔ¨Çine optimal solution for all inputs, and this is the best possible. Belady‚Äôs furthest-in-future\nalgorithm (Belady, 1966) that always evicts the page whose next request is the furthest in the future is well-known to be\nthe optimal ofÔ¨Çine algorithm\n*Equal contribution1University of California, Merced, CA, USA.2Google Research, Mountain View, CA, USA. Correspondence to:\nSungjin Im<sim3@ucmerced.edu >, Manish Purohit <mpurohit@google.com >.arXiv:2202.04262v1  [cs.DS]  9 Feb 2022\n\nParsimonious Learning-Augmented Caching\nTo exploit predictions, Lykouris & Vassilvitskii (2018) proposed an algorithm that assumes the knowledge of the next\npredicted request time of allpages in the cache. For the `1-norm prediction error with respect to the actual arrival times,\nthey showed that the algorithm‚Äôs competitive ratio improves to O(1)as the error tends to 0 and remains O(logk)always.\nThese bounds have further been quantitatively improved recently by Rohatgi (2020); Wei (2020).\nOur contributions. We show that we can use signiÔ¨Åcantly fewer predictions for online caching to obtain results similar\nto the aforementioned work. More precisely, we allow our algorithm to querybpages in cache to learn their predicted\nnext arrival time for each cache miss. We show that we can obtain an O(logb+1k)-competitive ratio for good predictions\nwhile retaining the O(logk)-competitive ratio always (Theorem 11). Thus, as long as the cache miss rate is1\nk\u000ffor any\nconstant\u000f>0, we can obtain a constant O(log1\n\u000f)-competitive ratio using a sublinear number of queries in the number of\npage requests. We also show that our trade-off is near-optimal (Theorem 12). Our experiments show that even with very\nfew queries, e.g., making two queries per cache miss, we can signiÔ¨Åcantly improve the traditional online algorithms with\nthe worst case guarantees in practice. The experimental results also demonstrate that we can match (and even exceed) the\nperformance of prior learning-augmented algorithms but querying only \u001811% of the page requests.\nAs is typical for most caching algorithms, our algorithm is also based on the randomized marking algorithm. However,\ninstead of evicting a randomly chosen page per cache miss, the algorithm queries bunmarked pages in the cache and evicts\nthe one with the furthest predicted request time. At the high-level, if there are kunmarked pages, we can show that the\nevicted page is not requested before k=(b+ 1) pages in the cache in expectation, provided all the predictions are correct.\nUsing this we can formally show how to reduce the number of cache misses. While this idea is easy to state, the analysis\nis delicate as the prediction error is deÔ¨Åned only over the pages that were queried. To keep the competitive ratio O(logk),\nwe follow the technique of Lykouris & Vassilvitskii (2018) and switch to using the randomized marking strategy once we\ndetect that the algorithm has made too many mistakes. The lower bound is shown by an explicit but intricate construction;\nthe formal analysis is quite subtle.\nRelated work. Online caching has been extensively studied in the literature. For generalizations of caching, including\nthek-server problem, see (Koutsoupias & Papadimitriou, 1995; Bansal et al., 2015; Bubeck et al., 2018; Lee, 2018); see\nalso (Bansal et al., 2012; Adamaszek et al., 2012). The reader is referred to the book by Borodin & El-Yaniv (2005) for a\ngeneral overview of online algorithms.\nLearning-augmented algorithms largely fall in the rubric of ‚Äúbeyond worst-case algorithms‚Äù; see (Roughgarden, 2020) for\nan extensive survey of the Ô¨Åeld. They have recently been extensively explored particularly for online algorithms, including\nload balancing (Lattanzi et al., 2020; Li & Xian, 2021), rent-or-buy (Kumar et al., 2018), scheduling (Azar et al., 2021),\nonline set cover (Bamas et al., 2020), metrical task systems (Antoniadis et al., 2020), and many others. For online caching,\nits weighted version has been studied in (Jiang et al., 2020; Bansal et al., 2022).\nThe problem of learning-augmented algorithms with sub-linear number of queries was recently studied by Bhaskara et al.\n(2021), but in the regret setting for online linear optimization. Our paper studies an analogous question for caching, but in\nthe competitive ratio setting.\n2. Model\nLetUdenote a universe of pages and kbe the number of distinct pages that can be held in the cache at any time. In the\nclassical unweighted caching problem, a sequence \u0000 =hp1;p2;:::i, where each pi2U, of page requests arrives online\nand the algorithm is required to maintain a set of at most kpages in the cache at any time. At any time t, if the currently\nrequested page ptis not in the cache, then the algorithm incurs a cache miss and must fetch the requested page in the cache\n(possibly by evicting some other page). The objective of the online algorithm is to minimize the total number of cache\nmisses incurred.\nNote that an online algorithm has to choose the page to be evicted without knowing \u0000. We measure its performance by\ncomparing against Belady‚Äôs furthest-in-the-future ( FiF)algorithm (Belady, 1966), which is the optimal ofÔ¨Çine algorithm\nthat knows \u0000. Let cost \u0000(\u0001)denote the total number of cache misses of an algorithm for the request sequence \u0000and\nletOPT = cost \u0000(FiF)be the cost of the optimal ofÔ¨Çine solution. An online (randomized) algorithm Ais said to be\nc-competitive if for all request sequences \u0000, it holds that\nE[cost \u0000(A)]\u0014c\u0001OPT+b;\n\nParsimonious Learning-Augmented Caching\nwhereb\u00150is a constant independent of the length of \u0000, and the expectation is over the randomness (if any) of A. For\nbrevity, from now on we will work with a given \u0000and omit it from all the subsequent notation.\nIn the usual learning-augmented setting, at each time t, along with the requested page pt, the algorithm is presented with\na (possibly noisy) prediction \u001ct2Nfor the next time after tthat the page ptwill be requested again; hence the predicted\narrival time of the next request is available for every page in the cache. In the learning-augmented setting with queries , at\nany timetand for any page pthat is in the cache, the algorithm is allowed to query a possibly noisy (stochastic) oracle Q\nfor the time, after t, of the next request for p. Let\u001cp;t=Q(p;t)denote such a predicted arrival time of the next request\nto pagepafter timet; letap;t\u0015tdenote the actual arrival time of the next request to page p. LetQbe the set of queries\nmade toQ. We deÔ¨Åne the error of the oracle to be \u0011=P\n(p;t)2Qj\u001cp;t\u0000ap;tj.\nThe learning-augmented setting with queries generalizes many well-studied caching problems. On one hand, if the al-\ngorithm makes no queries to the oracle, then it is the standard caching problem and we can get a O(logk)-competitive\nsolution, say, with a randomized marking algorithm (see Section 3.1). On the other hand, if the oracle is error-free and the\nalgorithm queries it at every time step, Belady‚Äôs algorithm yields the optimal solution. In a recent work, Lykouris & Vassil-\nvitskii (2018); Wei (2020); Rohatgi (2020) designed a learning-augmented caching algorithm for noisy oracles, showing a\ntight trade-off between the error of the oracle and the competitive ratio of the algorithm; their algorithm, however, queries\nthe oracle at every time step. The question we ask in this paper is: can we get similar trade-offs but using much fewer\nqueries?\n3. Preliminaries\nA pair (p;t1)and(q;t2)of queries in Qis called an inversion if\u001cp;t1\u0015\u001cq;t2butap;t1<aq;t2, i.e., the next request of page\npis earlier than that of qalthough the predictions indicated otherwise. Let I=jf(p;t1);(q;t2)j\u001cp;t1\u0015\u001cq;t2butap;t1<\naq;t2gjbe the number of inversions. The following relates the number of inversions to the error.\nLemma 1 (Diaconis & Graham (1977); Rohatgi (2020)) .For any request sequence \u0000and any setQof queries,\n\u0011\u00151\n2I:\n3.1. Marking algorithms\nMarking algorithms are a class of caching algorithms that associate a ‚Äúmarking‚Äù bit with each page in the cache, and upon\na cache miss only evict an unmarked page from the cache. Formally, the algorithm Ô¨Årst divides the request sequence\ninto phases where a phase is a maximal contiguous sequence of requests to only kdistinct pages. At the beginning\nof each phase, all pages in the cache are unmarked. Pages that are requested during the phase get marked one by one\nand upon any cache miss, the algorithm only evicts some unmarked page. Once all the pages in the cache have been\nmarked, a new phase begins and the process repeats. It is well known that anymarking algorithm is O(k)-competitive\nand the randomized marking (Fiat et al., 1991) algorithm, which evicts an unmarked page chosen uniformly at random, is\nO(H(k))-competitive, where H(k) := 1 +1\n2+\u0001\u0001\u0001+1\nk= \u0002(logk).\nAlgorithm 1: A generic marking algorithm.\nforeach requested page pdo\nifpin cache then\n‚ÄúMark‚Äùp\nelse\nifall pages in cache are marked then\nUnmark all pages\nend\nEvict an unmarked page\nFetchpin cache and ‚Äúmark‚Äù it\nend\nend\nConsider any phase hand an arbitrary page pthat is requested in phase h. We say that page pisclean ifpwas not requested\nin the previous phase (i.e., phase h\u00001), and we say pisstale otherwise. Note that once kis known, the phases of the\n\nParsimonious Learning-Augmented Caching\nsequence‚Äîas well as clean and stale pages‚Äîare determined, independent of the algorithm. Let `hdenote the total number\nof distinct clean pages requested in phase h. The following result bounds the number of cache misses incurred by the\noptimal ofÔ¨Çine algorithm in terms of the number of distinct clean pages.\nLemma 2 ((Fiat et al., 1991)) .1\n2P\nh`h\u0014OPT\u0014P\nh`h.\n4. Warm-up: ModiÔ¨Åed marking algorithm\nWe Ô¨Årst show how the classic randomized marking algorithm (Fiat et al., 1991) can be modiÔ¨Åed to effectively use predic-\ntions but making fewer queries. For ease of exposition, we assume for now that the oracle is error-free; we extend the\nanalysis to handle noisy predictions in Section 4.1.\nWe consider the following modiÔ¨Åcation to the marking algorithm: whenever a page needs to be evicted, if there are at least\n\u000fkunmarked pages in the cache, then evict an unmarked page chosen uniformly at random; otherwise, query Q(p;t)for\nall unmarked pages pand evict the page whose next request appears furthest in the future (i.e., apply Belady‚Äôs method).\nWe remark that once we query all the remaining unmarked pages in a phase, we can simply reuse these predictions for\nany further cache misses and hence make at most \u000fkcache misses in any phase. Algorithm 2 describes this naive eviction\npolicy formally.\nAlgorithm 2: Naive eviction.\nFunction: Evict() :\nData:U\u0012U: Set of unmarked pages in cache\nResult:\u000b: Page to be evicted\nifjUj\u0015\u000fkthen\n\u000b Uniformly random page from U\nelse\nifwe have not already queried pages in Uin this phase then\nforeach pagepinUdo\nLet\u001cp Q (p;t)\n\u000b argmaxp2U\u001cp\nreturn\u000b;\nTheorem 3. For any\u000f >0, for any request sequence \u0000, there is an O(log(1=\u000f))-competitive algorithm for caching that\nmakes at most \u000fj\u0000jqueries.\nProof. Consider any phase hof the marking algorithm and `hbe the number of clean pages in that phase. Let p1;:::;pk\ndenote thekpages in cache at the beginning of the phase and further suppose that the pages are sorted in order of the\narrival time of the Ô¨Årst request to a page in this phase (breaking ties arbitrarily). In other words, pages p1, . . . ,pk\u0000`hare\nthek\u0000`hstale pages requested in this phase and further the Ô¨Årst request to page piis earlier than that of page pjfor any\ni<j .\nConsider any stale page piwhere 1\u0014i\u0014k\u0000\u000fk, and let`(i)\u0014`hbe the number of clean pages that have been requested\nbefore the Ô¨Årst request to page pi. When the Ô¨Årst request to page iarrives, there are exactly k\u0000i+ 1unmarked stale pages\nof which`(i)pages have been evicted from the cache uniformly at random. Hence, the algorithm incurs a cache miss for\npagepiwith probability`(i)\nk\u0000i+1\u0014`h\nk\u0000i+1.\nFinally, consider the Ô¨Årst request to page pk\u0000\u000fk. If the algorithm incurs any cache miss after this time, then it queries all\nthe\u000fkremaining unmarked pages and evicts the page whose next request is furthest in the future, i.e., it evicts a page from\nthe setfpk\u0000`h;:::;pkgthat is not requested in this phase. Thus, for any i > k\u0000\u000fk, pagepiincurs a cache miss only\nif it has already been evicted by the time page pk\u0000\u000fkis Ô¨Årst requested. Thus, any such page piincurs a cache miss with\nprobability at most minf1;`h\n\u000fkg.\nBy the linearity of expectation, summing over all pages pi, the expected number of cache misses incurred by the algorithm\nfor stale pages is at mostPk\u0000\u000fk\ni=1`h\nk\u0000i+1+Pk\u0000`h\ni=k\u0000\u000fk+1`h\n\u000fk\u0014`h+`h(Hk\u0000H\u000fk)\u0014O(`h(log1\n\u000f)). In addition, the algorithm\n\nParsimonious Learning-Augmented Caching\nalso incurs`hadditional cache misses for the clean pages. Hence, the total expected number of cache misses incurred in\nphasehisO(`hlog(1\n\u000f)). The desired competitive ratio now follows from Lemma 2.\nTo bound the total number of queries, we observe that the algorithm makes at most \u000fkqueries in each phase. Since each\nphase has at least krequests, any request sequence \u0000has at mostj\u0000j=kphases, and thus the total number of queries is at\nmost\u000fj\u0000j.\n4.1. Handling prediction errors\nLet us now consider the case where the oracle can give erroneous predictions.\nSince Algorithm 2 does not utilize predictions as long as there are at least \u000fkunmarked pages left in the cache, we only\nneed to reconsider the cache misses that occur after there are fewer than \u000fkunmarked pages left. Consider any page pifor\ni>k\u0000\u000fkand let ~tdenote the time when the algorithm queries all the remaining unmarked pages. Suppose the algorithm\nincurs a cache miss on page piand evicts page q= argmaxp2U\u001cp;~t. Now, if the predictions are correct, then qbelongs\nto the setfpk\u0000`h;:::;pkgof pages that are not requested in this phase. However, suppose the predictions are incorrect\nand pageqis requested in this phase, then the algorithm incurs an additional cache miss. However, in this case the pair\n(q;~t)and(pk;~t)of queries is an inversion and we can charge the additional cache miss incurred to this inversion. Since we\nonly incur at most one cache miss for a page, it can be easily veriÔ¨Åed that we charge at most one cache miss to a speciÔ¨Åc\ninversion. Let Ihbe the total number of inversions for queries made in phase h, then from the above discussion we have\nthat the expected number of cache misses incurred by the algorithm in phase his at mostO(`h(log1\n\u000f) +E[Ih]). Hence,\nthe total cost incurred over all phases is O((P\nh`h)(log1\n\u000f) +E[I])where the expectation is over the randomness in the\npages evicted by the algorithm. Using Lemma 1 and Lemma 2, we conclude that the total cost incurred by Algorithm 2 is\nat mostO(2 log(1=\u000f)OPT+E[\u0011])and obtain the following:\nTheorem 4. For any\u000f >0, there is an O(log(1=\u000f) +E[\u0011]=OPT)-competitive algorithm for caching that makes at most\n\u000fj\u0000jqueries.\nWhile this warm-up result is a proof of concept for parsimonious use of predictions, to achieve a constant competitive ratio,\nwe still need to make a linear number of queries in the request sequence length. To overcome this weakness we propose a\nnew algorithm in the following section that is more adaptive in deciding which pages to query.\n5. Adaptive query algorithm\nThe new algorithm queries for bunmarked pages uniformly at random per cache miss and evicts the one that is predicted\nto be requested the furthest in the future. We call this the adaptive query algorithm ( AdaptiveQuery -b); see Algorithm 3.\nHere,bis a parameter that governs a trade-off between the desired competitive ratio and the number of queries we are\nwilling to make per cache miss.\nAlgorithm 3: Adaptive query eviction.\nFunction: Evict() :\nData:U\u0012U: Set of unmarked pages in cache and an integer b>0\nResult:\u000b: Page to be evicted\nS Samplebpages fromUuniformly at random without replacement\nLet\u001cp Q (p;t)for allp2S\n\u000b argmaxp2S\u001cp\nreturn\u000b\nAs before we Ô¨Årst analyze the algorithm assuming the predictions are all correct. We will show the following trade-off in\nSection 5.1.\nTheorem 5. Under the assumption that the oracle is error-free, for any integer b >0, the adaptive query algorithm is\n2(logb+1k+ 3) -competitive and makes at most 2b(logb+1k+ 3)\u0001OPT queries in expectation.\nThis bound is shown to be nearly tight in Section 6; see Theorem 12. We then extend Theorem 5 in Section 5.2, so it can\nhandle error-prone predictions.\n\nParsimonious Learning-Augmented Caching\n5.1. Analysis\nIf we show that the adaptive query algorithm is c-competitive, then it immediately follows that the number of queries made\niscb\u0001OPT. Thus, we only need to establish the desired competitive ratio.\nConsider any Ô¨Åxed phase hof the marking algorithm and let f1;:::;f`hbe the clean pages requested in that phase. We\nconsider the following notion of eviction chains (Lykouris & Vassilvitskii, 2018) for the sake of analysis. An eviction\nchainCi=hqi;0:=fi;qi;1;:::;qi;Miiis a sequence of pages constructed as follows: qi;1is the stale page that is evicted\nby the algorithm when it serves the clean page fi; similarly for all j\u00151,qi;j+1is the stale page that gets evicted when\nthe algorithm serves the request to page qi;j. Eventually, a stale qi;Migets evicted that is not requested in the phase and\nthe sequence ends. We note that each eviction chain starts with a distinct clean page and ends with a stale page that is\nnot requested in the phase. Further, the `heviction chains are disjoint and each cache miss incurred by the algorithm is\nencoded in these chains. The ith eviction chain Cileads toMicache misses where Miis a random variable. Our goal is to\nbound the total number of cache misses, i.e., E[P`h\ni=1Mi].\nPage ranks. We Ô¨Årst order all clean pages and stale pages in the cache by the arrival time of the Ô¨Årst request to that page\nin this phase (the `hstale pages that are not requested in the phase appear last in the ordering, in an arbitrary order). For\neach stale page pevicted by the algorithm, we deÔ¨Åne its rankr(p)as the number of stale pages after page pin the above\nordering that have not yet been evicted (at the time pwas evicted). By construction of the eviction chains, page qi;jis\nevicted when page qi;j\u00001is requested and hence qi;jis afterqi;j\u00001in the ordering (since all pages before qi;j\u00001in the\nordering have already been marked). Hence, we always have r(qi;j)\u0014r(qi;j\u00001). Similarly, for each clean page fi, we\ndeÔ¨Åne its rank r(fi) =r(qi;0)to be the number of stale pages after fithat have not yet been evicted when fiwas requested.\nNote thatr(fi)\u0014k, for all 1\u0014i\u0014`h.\nWe Ô¨Årst show the following simple lemma that follows from order statistics of the uniform distribution. We defer its proof\nto the Supplementary Material.\nLemma 6. IfS=fs1;:::;sbgis a set sampled uniformly at random without replacement from f0;1;:::;rg, then\nE[mint2[b]st]\u0014r\nb+1.\nThe following two lemmas are used to bound the expected length of an eviction chain.\nLemma 7. Consider any eviction chain Ciand suppose it evicts page qi;j+1to service a request to page qi;j. Then we\nhaveE[r(qi;j+1)jr(qi;j)]\u0014r(qi;j)\nb+1.\nProof. When a cache miss occurs for page qi;j, note that all pages that appear before qi;j(when ordered by the arrival\ntime of their Ô¨Årst request in the phase) have already been marked. Thus, all the queried stale pages must appear after qi;j.\nSuppose there are r\u0014r(qi;j)unmarked stale pages left. When the predictions are all correct, Algorithm 3 randomly\nsamplesbpages from all unmarked stale pages and evicts qi;j+1as the one that is latest in the ordering. In other words,\nr(qi;j+1)is the minimum of buniform samples from f0;1;:::;r\u00001g. Thus from Lemma 6, we have E[r(qi;j+1)j\nr(qi;j)]\u0014r\u00001\nb+1\u0014r(qi;j)\nb+1.\nLemma 8. For every 1\u0014i\u0014`h, we have E[Mi]\u0014logb+1k+ 3whereMiis the length of the eviction chain beginning\nwith the clean page fi.\nProof. Note that an eviction chain ends when it evicts one of the `hstale pages that are not requested in the phase. Fix\na particular chain Ciand for brevity, let rj:=r(qi;j). Since we have r0\u0014k, using Lemma 7 and the law of iterated\nexpectation, we have E[rj]\u0014k\n(b+1)j. By Markov‚Äôs inequality, we have Pr[rj\u00151]\u0014k\n(b+1)j. Note that if Mi>j, then it\nmust be the case that rj\u00151. This is because if rj= 0, thenqi;jwill not be evicted and the chain Cimust have length j.\nLetc:=dlogb+1(k)e. We can now bound the expected length of the chain as follows.\nE[Mi] =c\u00001X\nj=0Pr[Mi\u0015j] +X\nj\u0015cPr[Mi\u0015j]\n=c+X\nj\u00150Pr[Mi>c+j] =c+X\nj\u00150Pr[rc+j\u00151]\n\nParsimonious Learning-Augmented Caching\n\u0014c+X\nj\u00150k\n(b+ 1)c+j=c+X\nj\u001501\n(b+ 1)j\n\u0014logb+1(k) + 3:\nProof of Theorem 5. Fix a phasehof the marking algorithm. Since every cache miss incurred by the algorithm is recorded\nin exactly one eviction chain, the expected total number of cache misses incurred by the algorithm in this phase is\nE[P`h\ni=1Mi]. Using Lemma 8, this is at most `h(logb+1k+ 3) . The desired competitive ratio now follows from Lemma\n2. Further, the algorithm makes at most bqueries for each cache miss it incurs and thus we have Theorem 5.\n5.2. Handling prediction errors\nIn this section we extend the analysis to allow for oracles that make erroneous predictions. In this scenario, since we evict\na page that is only predicted to arrive furthest in the future (and not actually be the one to arrive the latest), Lemma 7 fails.\nHowever, as we show below, in this case the oracle has a large error and we can bound the expected cost of the algorithm\nin terms of the prediction error.\nWe Ô¨Årst show the following technical statement that relates the rank of the page evicted by the algorithm and the rank of\nthe page that actually arrives the furthest in the future from among the sampled pages (while processing any cache miss).\nLemma 9. LetSbe any set of bpages and let a1<\u0001\u0001\u0001< abdenote their actual next arrival times and let h\u001c1;:::;\u001cbi\nbe the sequence of their predicted arrival times. Let \u0011S=Pb\ni=1jai\u0000\u001cijbe`1-error of the predictions for the set S. If\n^b= argmax\u000b\u001c\u000bis the page with the furthest predicted arrival time, then we have\nr(^b)\u0014r(b) +\u0011S:\nProof. We assume that ^b6=bsince otherwise the lemma is trivial. By deÔ¨Ånition of ^bwe have\u001c^b\u0015\u001cbanda^b< ab. For\nconvenience let ^pandpdenote the corresponding pages. Since the number of unmarked pages between ^pandp, when\nordered by the request time of their Ô¨Årst request, is at most ab\u0000a^b, by deÔ¨Ånition of rank we have\nr(^b)\u0014r(b) +ab\u0000a^b:\nWe now show \u0011S\u0015ab\u0000a^b, which will complete the proof. To show this, we observe that \u0011S\u0015ja^b\u0000\u001c^bj+jab\u0000\u001cbj. We\nconsider three cases.\nCase 1:\u001c^b\u0014a^b.In this case we have \u0011S\u0015jab\u0000\u001cbj= (ab\u0000a^b) + (a^b\u0000\u001cb)\u0015ab\u0000a^b.\nCase 2:ab\u0015\u001c^b>a^b.Since\u001cb\u0014\u001c^b, we have\u0011S\u0015ja^b\u0000\u001c^bj+jab\u0000\u001cbj=\u001c^b\u0000a^b+ab\u0000\u001cb\u0015ab\u0000a^b.\nCase 3:\u001c^b>ab.Here we haveja^b\u0000\u001c^bj=\u001c^b\u0000a^b\u0015ab\u0000a^b.\nLemma 9 lets us prove the following analog of Lemma 8.\nLemma 10. For every 1\u0014i\u0014`h, we have E[Mi]\u0014logb+1k+ 3 + 2 E[\u0011Si]whereMiis the length of the eviction chain\nbeginning with the clean page fiandSiis the set of pages queried when pages on path Piare evicted.\nProof. Fix a particular chain Ci. Letr(qi;j)be the number of stale unmarked pages left in the cache when the algorithm\nincurs a cache miss for page qi;jat some time t. In this case, we sample a set Sofbof those pages uniformly at random,\nand setqi;j+1= argmaxp2SQ(p;t). Letq\u0003\ni;j+1= argmaxp2Sap;tbe the sampled page that actually arrives furthest in the\nfuture. Then by Lemma 7, we have E[r(q\u0003\ni;j+1)jr(qi;j)]\u0014r(qi;j)\nb+1. Further, for any queried set Sof pages, by Lemma 9\nwe have,r(qi;j+1)\u0014r(q\u0003\ni;j+1) +\u0011Swhere\u0011Sis the`1-error of the predictions for the set S. Thus we obtain the following\nwhere\u0011i;j+1is deÔ¨Åned to be the prediction error of the oracle for pages queried while evicting page qi;j+1.\nE[r(qi;j+1)jr(qi;j)]\u0014r(qi;j)\nb+ 1+E[\u0011i;j+1]:\nNow, since we have r(qi;0)\u0014k, using the law of iterated expectation we have\nE[r(qi;j)]\u0014k\n(b+ 1)j+jX\nj0=1E[\u0011i;j0]\n(b+ 1)j\u0000j0:\n\nParsimonious Learning-Augmented Caching\nFinally, using Markov‚Äôs inequality, we have\nPr[Mi>j]\u0014Pr[r(qi;j)\u00151]\u0014k\n(b+ 1)j+jX\nj0=1E[\u0011i;j0]\n(b+ 1)j\u0000j0:\nAs earlier, let c=dlogb+1(k)e. We can now bound the expected length of the chain.\nE[Mi] =c+X\nj\u00150Pr[Mi>c+j]\n\u0014c+X\nj\u001500\n@k\n(b+ 1)c+j+c+jX\nj0=1E[\u0011i;j0]\n(b+ 1)c+j\u0000j01\nA\n\u0014c+X\nj\u001501\n(b+ 1)j+X\nj0\u00151E[\u0011i;j0]X\nj\u0015j01\n(b+ 1)j\u0000j0\n\u0014logb+1(k) + 3 + 2X\nj0\u00151E[\u0011i;j0]:\nFinally, since the algorithm makes a distinct set of queries when evicting any page, we haveP\nj0\u00151E[\u0011i;j0] =E[\u0011Si]and\nthe lemma follows.\n5.3. Adding worst-case guarantees\nIn this section we show how a simple modiÔ¨Åcation to the algorithm allows us to obtain an O(logk)-competitive ratio\neven when the prediction error is arbitrarily large. In order to obtain this worst-case guarantee, we make the following\nmodiÔ¨Åcation: when processing a cache miss for the jth page (qi;j) on chainCi, ifj >logk, then the algorithm switches to\nevict an unmarked stale page uniformly at random (as opposed to querying bpages and evicting the one with the furthest\npredicted arrival).\nTheorem 11. For any integer b>0, there is an O(minflogb+1k+E[\u0011]=OPT;logkg)-competitive algorithm for caching\nthat makes at most bqueries per cache miss.\nProof. When pages are evicted according to Algorithm 3, Lemma 10 shows that the expected length of any eviction chain is\nat mostO(logb+1k+E[\u0011Si]). We consider the modiÔ¨Åed algorithm that switches to evicting a uniformly random unmarked\npage once the chain length exceeds logk. Following the traditional analysis of the randomized marking algorithm (Fiat\net al., 1991; Lykouris & Vassilvitskii, 2018), we observe that once the algorithm switches to random evictions, the length\nof the chain increases by at most O(logk)in expectation. Consequently, the modiÔ¨Åed algorithm incurs at most O(1)\u0001\nminflogb+1k+ 3 + 2 E[\u0011Si];2 logkgcache misses in expectation on each eviction chain.\nSumming over all clean pages seen in the phase, the expected number of cache misses incurred in any phase his at most\nO(minflogb+1k+E[\u0011h];logkg), where\u0011his deÔ¨Åned to be the `1-error of all the queries made in this phase. The desired\ncompetitive ratio now follows from Lemma 2.\n6. Lower bound\nThe lower bound instance is fairly simple. Each phase starts with a request for a clean page that has never been requested\nbefore. Then, it is followed by requests for k\u00001stale pages that are chosen uniformly at random among the kstale pages.\nWe provide a formal description below.\nLower bound instance. The page requests proceed in phases. Let Phdenote the pages that are requested in phase hfor\nh2[H], whereHis a sufÔ¨Åciently large integer. We will have jPhj=kfor allh2[H], andjPh+1nPhj=jPhnPh+1j= 1\nfor allh2[H\u00001]. First,P1is an arbitrary set of kpages and there is one request for each page in P1. We now iteratively\nconstructPh+1fromPhas follows: Let fh+1be a clean page that has never been requested before. Let p1;:::;pkbe a\nuniformly random permutation of the set of pages in Ph. Then the request sequence for phase h+ 1isfh+1,hfh+1ikpk,\nhfh+1pkikpk\u00001, . . . ,hfh+1pk:::p 3ikp2in this order. Here, hSikimplieskrepetitions of the sequence S. Focusing on the\npage arriving after the repeated sequence, we will say that pages are requested in the order of fh+1;pk;:::;p 2.\n\nParsimonious Learning-Augmented Caching\nb: number of queries per cache missCompetitive Ratio\n1.52.02.53.03.5\nb=1 b=2 b=4 b=8 b=16 b=32 b=64 b=128ùõî = 20\nùõî = 15\nùõî = 10\nùõî = 5\nùõî = 2\nùõî = 0\nFigure 1. Average competitive ratio of AdaptiveQuery for different values of band the error parameter \u001b.\nThe proof of Theorem 12 requires care to impose constraints on the structure of candidate algorithms, and formally demon-\nstrate that a learning-augmented algorithm for caching can do no better than querying unmarked stale pages and always\nevict the one that arrives furthest in the future. Unlike in the analysis of the upper bound, the algorithm can make varying\nnumbers of queries per cache miss, even stochastically, which renders the analysis considerably more challenging. We\ndefer the full proof to the Supplementary Material.\nTheorem 12. For any integer c\u0014lnk, any (c+ 4)-competitive algorithm must make at least1\n12 ln(k+1)ck1=c\u0001OPTqueries\n(with no error).\n7. Experiments\nWe experimentally evaluate our algorithm on a real-world dataset and demonstrate the empirical dependendence of the\ncompetitive ratio on the number of queries made as well as on the prediction errors.\nInput dataset. We use the CitiBike dataset, closely following (Lykouris & Vassilvitskii, 2018). The dataset comes from\na publicly-available (cit) bike sharing platform operating in New York City. For each month of year the 2018, we construct\none instance where each page request corresponds to the starting point of a bike trip. We truncate each months data to the\nÔ¨Årst 25,000 events, and thus each input sequence length is 25,000. Finally we set the cache size k= 500 and obtain 7\nnon-trivial instances1. We use a bigger cache than (Lykouris & Vassilvitskii, 2018) to illustrate our algorithm‚Äôs trade-off\nbetween number of queries and the competitive ratio.\nPredictions. To demonstrate the empirical dependence of different algorithms on the prediction error, we generate the\nfollowing synthetic predictions. For each page pin the cache, its predicted next request time is set to its actual next request\ntime plus a noise, which is drawn i.i.d. from a lognormal distribution whose underlying normal distribution has mean 0\nand standard deviation \u001b. If the page is never requested in the future, we pretend its actual request time is the sequence\nlength plus 1, i.e., 25,001.\nWe also use a very simple prediction model to demonstrate the efÔ¨Åcacy of easy off-the-shelf predictors. For each page,\nwe compute the average time \u0016pelapsed between consecutive requests for that page. For any page pat timet, we set the\npredicted arrival time as Q(p;t) =~tp+\u0016pwhere ~tpis the last time before twhen pagepwas requested. We refer to these\npredictions as ‚ÄúMean Predictions‚Äù in Table 1.\nAlgorithms. We implement the following algorithms.\n‚Ä¢RandomMarker (Randomized Marking, Fiat et al. (1991)). Evicts a randomly chosen unmarked page; \u0002(logk)-\ncompetitive.\n‚Ä¢LRU (Least Recently Used). A widely used heuristic that evicts the least recently used page.\n‚Ä¢BlindOracle . Evicts the page with the latest predicted next request time.\n‚Ä¢LVMarker (Lykouris & Vassilvitskii, 2018). A learning-augmented marking algorithm that evicts the page with the\n1The other 5 sequences have less than 500 distinct pages and the caching problem is trival.\n\nParsimonious Learning-Augmented Caching\nTable 1. Average competitive ratio of algorithms for different error parameters. (Smaller values means better performance.)\nAlgorithmsMean Synthetic Predictions\nPredictions \u001b= 0\u001b= 2\u001b= 4\u001b= 6\nRandomMarker 3.14 3.14 3.14 3.14 3.14\nLRU 2.86 2.86 2.86 2.86 2.86\nBlindOracle 1.92 1.00 1.02 3.92 4.15\nLVMarker 2.49 1.77 1.81 2.94 3.11\nRohatgiMarker 2.54 1.77 1.83 3.15 3.29\nRobustOracle 4.29 1.80 1.83 4.48 4.51\nAdaptiveQuery -2 2.91 2.46 2.46 2.52 2.65\nAdaptiveQuery -4 2.71 2.07 2.07 2.20 2.49\nAdaptiveQuery -8 2.59 1.86 1.86 2.07 2.54\nfurthest predicted arrival until the length of the eviction chain is O(logk)and then switches to evicting a randomly\nchosen unmarked page.\n‚Ä¢RohatgiMarker (Rohatgi, 2020). Identical to LVMarker except the switch occurs after the chain length exceeds one.\n‚Ä¢RobustOracle (Wei, 2020). Uses the combiner (Fiat et al., 1994) to combine BlindOracle andRandomMarker .\n‚Ä¢AdaptiveQuery -b. Our algorithm (with worst-case guarantees) that is parameterized by b, the number of queries\nmade per cache miss.\nResults. Table 1 shows the competitive ratios of all the implemented algorithms averaged over the seven instances. We\nobserve that our AdaptiveQuery algorithm performs signiÔ¨Åcantly better that RandomMarker andLRU (that do not use\nany predictions), even while using very few predictions, e.g., making b= 2 queries on each cache miss. Since our\nalgorithm is equivalent to RandomMarker whenb= 1, this demonstrates than even minimal predictions can considerably\nhelp online algorithms.\nAt the same time, Table 1 also demonstrates that AdaptiveQuery performs as well as the three other learning-augmented\nalgorithms even for relatively small values of bwith both synthetic predictions as well as the simple mean predictions. In\nfact, for high prediction errors, the AdaptiveQuery algorithm is less affected by these errors and outperforms the other\nlearning-augmented algorithms. For comparison, with b= 8, theAdaptiveQuery algorithm uses only 2839 queries for\neach instance on average, it utilizes predictions for about 11% of requests in the sequence.\nWe also compare the dependence of our algorithm on the number of queries and the prediction error. Figure 1 shows the\ncompetitive ratio of AdaptiveQuery -bfor different values of band different error parameters. Unsurprisingly, we observe\nthat when predictions are perfect (or very good), the competitive ratio of the algorithm improves with the number of queries\nit is allowed to make. Surprisingly, however, when the prediction error is large, using more queries actually leads to a worse\ncompetitive ratio. This is because when predictions are highly erroneous, using many queries often leads to making poor\neviction decisions.\n8. Conclusions\nIn this paper we initiate the study of online algorithms augmented with parsimonious learned predictions. Both the theory\nand experimental results suggest that performance of online algorithms can be signiÔ¨Åcantly improved by judiciously using\njust a few predictions. Such an approach can make learning-augmented algorithms more practically appealing since obtain-\ning predictions is often computationally expensive. An interesting future direction is to further explore this parsimonious\nmodel for other online problems. For example, consider problems that involve predictions of locations, such as metric\ntask system and online matching (Antoniadis et al., 2020). It is conceivable that one can use less predictions by spatial\ninterpolation.\nReferences\nCiti bike system data. https://www.citibikenyc.com/system-data .\nAchlioptas, D., Chrobak, M., and Noga, J. Competitive analysis of randomized paging algorithms. TCS, 234(1-2):203‚Äì218,\n2000.\nAdamaszek, A., Czumaj, A., Englert, M., and R ¬®acke, H. An O(logk)-competitive algorithm for generalized caching. In\nSODA , pp. 1681‚Äì1689, 2012.\n\nParsimonious Learning-Augmented Caching\nAntoniadis, A., Coester, C., Elias, M., Polak, A., and Simon, B. Online metric algorithms with untrusted predictions. In\nICML , pp. 345‚Äì355, 2020.\nAzar, Y ., Leonardi, S., and Touitou, N. Flow time scheduling with uncertain processing time. In STOC , pp. 1070‚Äì1080,\n2021.\nBamas, ¬¥E., Maggiori, A., and Svensson, O. The primal-dual method for learning augmented algorithms. In NeurIPS , 2020.\nBansal, N., Buchbinder, N., and Naor, J. S. A primal-dual randomized algorithm for weighted paging. JACM , 59(4):19,\n2012.\nBansal, N., Buchbinder, N., Madry, A., and Naor, J. A polylogarithmic-competitive algorithm for the k-server problem.\nJACM , 62(5):1‚Äì49, 2015.\nBansal, N., Coester, C., Kumar, R., Purohit, M., and Vee, E. Scale-free allocation, amortized convexity, and myopic\nweighted paging. In SODA , 2022.\nBelady, L. A study of replacement algorithms for a virtual-storage computer. IBM Systems Journal , 5(2):78‚Äì101, 1966.\nBhaskara, A., Cutkosky, A., Kumar, R., and Purohit, M. Logarithmic regret from sublinear hints. In NeurIPS , 2021.\nBorodin, A. and El-Yaniv, R. Online Computation and Competitive Analysis . Cambridge University Press, 2005.\nBubeck, S., Cohen, M. B., Lee, Y . T., Lee, J. R., and Madry, A. k-server via multiscale entropic regularization. In STOC ,\npp. 3‚Äì16, 2018.\nDiaconis, P. and Graham, R. L. Spearman‚Äôs footrule as a measure of disarray. JRS Series B‚ÄìMethodological , 39:262‚Äì268,\n1977.\nFiat, A., Karp, R. M., Luby, M., McGeoch, L. A., Sleator, D. D., and Young, N. E. Competitive paging algorithms. J.\nAlgorithms , 12(4):685‚Äì699, 1991.\nFiat, A., Rabani, Y ., and Ravid, Y . Competitive k-server algorithms. JCSS , 48(3):410‚Äì428, 1994.\nJiang, Z., Panigrahi, D., and Sun, K. Online algorithms for weighted paging with predictions. In ICALP , pp. 69:1‚Äì69:18,\n2020.\nKoutsoupias, E. and Papadimitriou, C. H. On the k-server conjecture. JACM , 42(5):971‚Äì983, 1995.\nKraska, T., Beutel, A., Chi, E. H., Dean, J., and Polyzotis, N. The case for learned index structures. In SIGMOD , pp.\n489‚Äì504, 2018.\nKumar, R., Purohit, M., and Svitkina, Z. Improving online algorithms via ML predictions. In NIPS , pp. 9661‚Äì9670, 2018.\nLattanzi, S., Lavastida, T., Moseley, B., and Vassilvitskii, S. Online scheduling via learned weights. In SODA , pp. 1859‚Äì\n1877, 2020.\nLee, J. R. Fusible HSTs and the randomized k-server conjecture. In FOCS , pp. 438‚Äì449, 2018.\nLi, S. and Xian, J. Online unrelated machine load balancing with predictions revisited. In ICML , pp. 6523‚Äì6532, 2021.\nLykouris, T. and Vassilvitskii, S. Competitive caching with machine learned advice. In ICML , pp. 3296‚Äì3305, 2018.\nMitzenmacher, M. and Vassilvitskii, S. Algorithms with predictions. In Roughgarden, T. (ed.), Beyond the Worst-Case\nAnalysis of Algorithms , pp. 646‚Äì662. Cambridge University Press, 2020.\nRohatgi, D. Near-optimal bounds for online caching with machine learned advice. In SODA , pp. 1834‚Äì1845, 2020.\nRoughgarden, T. (ed.). Beyond the Worst-Case Analysis of Algorithms . Cambridge University Press, 2020.\nWei, A. Better and simpler learning-augmented online caching. In APPROX/RANDOM , 2020.\n\nParsimonious Learning-Augmented Caching\nSupplementary Material\nA. Proof of Lemma 6\nProof. We proceed assuming that replacement is allowed since it only increases E[mint2[b]st]. Then, the sampling process\ncan be simulated by sampling s0\n1;:::;s0\nb\u0018Unif(0;r)from the uniform distribution and taking the Ô¨Çoor of them. Thus, we\nhaveE[mint2[b]st]\u0014E[mint2[b]s0\nt].\nIt well known that if x1;:::;xbare uniformly sampled from [0;1], thenE[mint2[b]xt] =1\nb+1. (This can be easily veriÔ¨Åed\nby observing that Pr[mint2[b]xt> x] = (1\u0000x)band a simple calculus.) Since we can set s0\nt=r\u0001xt, we have,\nE[mint2[b]st]\u0014E[mint2[b]s0\nt] =r\nb+1.\nB. Lower bound\nWe repeat the lower bound instance here for clarity.\nLower bound instance. The page requests proceed in phases. Let Phdenote the pages that are requested in phase hfor\nh2[H], whereHis a sufÔ¨Åciently large integer. We will have jPhj=kfor allh2[H], andjPh+1nPhj=jPhnPh+1j= 1\nfor allh2[H\u00001]. First,P1is an arbitrary set of kpages and there is one request for each page in P1. We now iteratively\nconstructPh+1fromPhas follows: Let fh+1be a clean page that has never been requested before. Let p1;:::;pkbe a\nuniformly random permutation of the set of pages in Ph. Then the request sequence for phase h+ 1isfh+1,hfh+1ikpk,\nhfh+1pkikpk\u00001, . . . ,hfh+1pk:::p 3ikp2in this order. Here, hSikimplieskrepetitions of the sequence S. Focusing on the\npage arriving after the repeated sequence, we will say that pages are requested in the order of fh+1;pk;:::;p 2.\nWe Ô¨Årst observe that the optimum ofÔ¨Çine solution for such an instance always incurs a cache miss on the Ô¨Årst, clean page\nof each phase (except the Ô¨Årst phase) and evicts the unique page in Ph\u00001nPh. By construction, the optimum algorithm\nincurs no more cache misses in each phase. Finally, any algorithm must incur kcache misses for the Ô¨Årst phase and we\nhave the following claim.\nClaim 13. There is an ofÔ¨Çine solution that incurs exactly k+H\u00001cache misses in total.\nWe would like to lower bound the number of cache misses incurred by any c-competitive algorithm. Consider a Ô¨Åxed\noptimum online algorithm A.\nClaim 14. At the beginning of each phase h\u00152, we can assume without loss of generality that Ahas all pages in Ph\u00001\nin cache.\nProof. Assume that the universe of pages is inÔ¨Ånite. Then, knowing that we cannot guess the clean page that will be\nrequested in phase hand all the other requests are for the stale pages, the claim follows.\nThanks to Claim 14 and the repeated identical structure of the lower bound instance in every phase, we can assume without\nloss of generality that we use the same optimum online algorithm that we call Ain all phases except the Ô¨Årst. If c0is\nthe expected number of cache misses Aincurs in each phase h\u00152, forAto bec-competitive, it must be the case that\nk+c0(H\u00001)\nk+H\u00001\u0014cfrom Claim 13. Here, kin the numerator is the number of cache misses incurred by Ain the Ô¨Årst phase.\nThus, we must have c0=casH!1 .\nTherefore, we can focus on one phase and lower bound the expected number of queries made by Aassuming that it incurs\nat mostccache misses in expectation. Henceforth we drop indices referring to phases from the notation. We will say that\na page is marked in the phase if it has been requested in the phase. For simplicity, we will assume that Amakes at least\none query before each page eviction; this would have no effect on the asymptotic lower bound we aim to prove.\nLemma 15. In a phase that is not the Ô¨Årst, with a given limit c >0on the expected number of cache misses, there is an\nalgorithm that makes the minimum number of queries in expectation and simultaneously satisÔ¨Åes the following:\n(i) evicts a page only when it is forced to do so;\n(ii) never evicts marked pages and therefore it only needs to query unmarked pages;\n(iii) only queries pages just before a page eviction;\n(iv) evicts the queried page with furthest arrive time, if it makes any queries.\n\nParsimonious Learning-Augmented Caching\nWe Ô¨Årst prove (i)‚Äì(iii). After setting up additional notation that will be used throughout the analysis, we will prove (iv).\nProof of Lemma 15(i)‚Äì(iii). As argued in Fiat et al. (1991), we can assume wlog that the algorithm needs to make a change\n(evict a page) only when it is forced; such algorithms are called lazy. This implies (i).\nWe now show (ii). Suppose algorithm Aevicts a page pifor somei > j to servicepjin the request sub-sequence,\nhpk+1pk:::pj+1ikpj. Then, for every repetition of hpk+1pk:::pjiin the subsequence of hpk+1pk:::pjikpj\u00001, untilA\nfetchespiand evicts an unmarked page, it incurs another cache miss. If it makes a cache miss for every repetition, we\ncan makeAbetter or no worse by instead evicting an arbitrary unmarked page (such an algorithm incurs at most kcache\nmisses even without using randomization). Otherwise, that is, if Aever replaces with an unmarked page before pj\u00001is\nrequested, we could let Ahave done so earlier to reduce cache misses. Thus, we can assume that before we see pj\u00001, an\nunmarked page was evicted to service pj. This will incur no cache misses for the repetition hpk+1pk:::pjik. We have\nshown that we can assume wlog that Anever evicts marked pages; thus, we have (ii).\nNow (iii) follows as once a page gets marked it stays marked in the phase. Thus, by deferring the queries until being forced\nto evict a page,Acan only potentially avoid querying about pages that will be marked soon.\nFor the remaining analysis, we take the eviction chain view we used in the analysis of the adaptive query algorithm. As our\nanalysis will require careful conditioning and deconditioning, we slightly override the notation. From the above reasoning,\nparticularly from Lemma 15 (i)‚Äì(iii), we now have the following problem: We will see a request sequence for a clean\npagepk+1, andk\u00001stale pages, p\u0019(k),p\u0019(k\u00001), . . . ,p\u0019(2). Note thatp\u0019(1)is the dead page that is not requested. Here,\n\u0019(k);:::;\u0019 (1), denotes a random permutation of the kstale pages. Then, we consider the chain Cthat starts with pk+1\nand ends with p\u0019(1). Recall that an edge from qtoq0means that we evict page q0to service page q. Our goal is to lower\nbound the number of queries made under the requirement that the expected length (number of cache misses) MofCis at\nmostc. Pagep\u0019(j)is deÔ¨Åned to have rank j; this deÔ¨Ånition is slightly simpler than the one in Section 5.1 as we have only\none clean page, thus only one chain.\nWith this notation set up, we are now ready to prove Lemma 15(iv).\nProof of Lemma 15(iv). Our goal is to consider any algorithm Asatisfying (i)‚Äì(iii), and to construct another algorithm A0\nsatisfying (iv) as well, without increasing the number of cache misses but making no more queries.\nIn the execution of A, supposeith evicted page byAhas rankRiandAmakesqiqueries just before the eviction. Now A0\nmakes the same number qiof queries just before evicting ith page, but it instead evicts the one with smallest rank among\nthe queried pages, i.e., satisÔ¨Åes property (iv). By a simple induction on i, we can show that A0generates a sequence that\nstochastically dominates what Agenerates. More precisely, suppose Ahas madeqqueries, and the queried pages have\nranksS1<\u0001\u0001\u0001< Sq. Then, by deÔ¨Ånition, A0has also made qqueries (if there are not enough pages to query, then it\nis only better forA0), and letS0\n1<\u0001\u0001\u0001< S0\nqbe the rank of pages queried by A0. Then, we say that the sequence S0\nstochastically dominates sequence SifS0\nj\u0014Sjfor allj2[q]. Because the chain Cends once the last page of rank 1is\nevicted (we can assume we evict only a queried page under the assumption we query at least one page before each page\neviction),A0make no more cache misses than Ain expectation. Further, by construction, A0can only make less queries\nthanAin expectation.\nHenceforth, we consider an algorithm Athat satisÔ¨Åes Lemma 15(i)‚Äì(iv). Let R0:=k+1, be the rank of the clean page. Let\nRidenote the rank of the ith evicted page. As mentioned before, Ceventually ends with p\u0019(1). For notational convenience,\nonceRibecomes 1, we deÔ¨Åne all the subsequent Ri+1;Ri+2, . . . , to be 1.\nLetQidenote the number of queries Amakes when we witness the ith cache miss. For analysis, we will assume that we\ndo not make too many queries for each page eviction. This is because we can Ô¨Ånd the page p\u0019(1)without making many\nmore queries.\nLemma 16. Suppose we show that any algorithm that incurs at most ccache misses in expectation makes at least dqueries\nunder the assumption that Ri> Q2\ni+1for alli. Then, it implies the following lower bound: any algorithm that incurs at\nmostc+ 4cache misses in expectation makes at least d=5queries.\nProof. If the Ô¨Åxed algorithm considered to show the lower bound makes Qi+1queries such that Ri\u0014Q2\ni+1for the Ô¨Årst\ntime, then instead we let it makepRiqueries per cache miss until the phase ends. This is equivalent to a problem where\n\nParsimonious Learning-Augmented Caching\nthe cache size is Riand we makep\nRiqueries per cache miss. Thus, by Lemma 8, we know that the number of cache\nmisses is at most 5 in expectation, and we make at most 5p\nRi\u00145Qi+1queries in expectation. In summary, this change\nmakes at most 4 extra cache misses in expectation and increases the number of queries by a factor of 5.\nLet us Ô¨Åxiand letRi\u00001=randQi=q. If we choose qpoints uniformly at random from [0;r]and the expected minimum\nof the samples is well known to ber\nq+1. But, we want to know the expected value ofr\nRi+1, where the only randomness\ncomes from \u0019(1);:::;\u0019 (r\u00001), which we denote as \u0019([r\u00001])for short. Bounding this quantity needs more care.\nNow, our concern is to upper bound E\u0019([r\u00001])r\nS, whereS1;:::;Sqare sampled uniformly from [r\u00001]without replacement\nandS:= minj2[q]Sj. This corresponds to making queries about qpages among r\u00001unmarked ones and evicting the one\nwith the minimum rank, i.e., the furthest request time in the future. For ease of analysis, we pretend that samples S0\n1;:::;S0\nq\nare made from [1=r;1]and we want to upper bound E[1\nS0], whereS0:= minj2[q]S0\nj. Here, we relax the random selection\nby making the sampling domain continuous.\nWe Ô¨Årst show this relaxation does not change the expectation by much.\nLemma 17. If1\u0014q2<r, we have E\u0019(r\u00001)[r\nS]\u00146E[1\nS0].\nProof. Scale down S1;:::;Sqby a factor of r, so we can pretend that they are sampled from D=f1\nr;:::;r\u00001\nrgwithout\nreplacement. Now, we want to upper bound E[1\nS]. LetD:=\u0000D\nq\u0001\n.\nWe observe that\nE[1\nS] =E[1\n\u0016S0j\u0016S02D];\nwhere \u0016S0:=fbrS0\nic=rji2[q]gand\u0016S0= min \u0016S0. In other words, after ‚Äúrounding‚Äù down each S0\nito the nearest multiple\nof1=r, if they are all distinct, we keep them. This is an equivalent way of getting samples S1;:::;Sq.\nFurther, the rounding changes the expectation by a factor of at most 2. Therefore, we have,\nE[1\n\u0016S0j\u0016S02D]\u00142E[1\nS0j\u0016S02D]:\nWe would like to decondition on \u0016S02D.\nE[1\nS0j\u0016S02D]\u0001Pr[\u0016S02D]\u0014E[1\nS0]\nAs\u0016S0is a uniform sample with replacement, we have, Pr[\u0016S02D] =r(r\u00001)\u0001\u0001\u0001(r\u0000q+1)\nrq\u0015(1\u0000q=r)q\u0015(1\u0000q=(q2+1))q\u0015\n1=3, where the last inequality follows from a simple calculation. Combining the above equations yields the lemma.\nLemma 18. If1\u0014q2<r, we have E[1\nS0]\u00142qln(k+ 1) .\nProof. Observe that Pr[S0\u0015x] =\u0010\n1\u0000x\n1\u00001=r\u0011q\n. Thus, the pdf of S0isq(1\u0000x)q\u00001\n(1\u00001=r)qwherex2[1=r;1]. For brevity, we omit\nthe denominator in the following equations and bring it back at the end.\nE[1\nS0] =Z1\nx=1=r1\nxq(1\u0000x)q\u00001dx\n=Z1=q\nx=1=r1\nxq(1\u0000x)q\u00001dx+Z1\nx=1=q1\nxq(1\u0000x)q\u00001dx\n\u0014qZ1=q\nx=1=r1\nxdx+Z1\nx=1=qq\u0001q(1\u0000x)q\u00001dx\n\u0014qlnr=q+q\u0014qlnr\u0014qln(k+ 1):\nThus, by factoring in the denominator1\n(1\u00001=r)q\u00141\n(1\u00001=(q2+1))q\u00142, we obtain the lemma.\n\nParsimonious Learning-Augmented Caching\nCorollary 19. E[Ri\nRi+1]\u001412qln(k+ 1)E[Qi+1].\nProof. From Lemmas 17 and 18, we know E\u0019([r\u00001])[r\nS(r;q;\u0019 )]\u001412qln(k+ 1) . Here, the parameters r;q;\u0019 are used to\nmake the dependency of Sclear. As this holds for any Ri=randQi+1=q, by deconditioning, we have the desired\nresult.\nLemma 20. IfChas lengthMand for any integer c>0, we have E[Pc\ni=1QijM=c]\u00151\n12 ln(k+1)ck1=c:\nProof. Using the linearly of expectation and Corollary 19,\n12 ln(k+ 1)cX\ni=1E[Qijt=c]\n\u0015E[cX\ni=1Ri\u00001\nRijRc= 1;Rc\u00001>1]\n\u0015E[c(cY\ni=1Ri\u00001\nRi)1=cjRc= 1;Rc\u00001>1]\n= (k+ 1)1=c;\nwhere the last inequality follows from the AM‚ÄìGM inequality and the last equality follows from a telescoping product,\nR0=k+ 1, andRc= 1.\nBy Lemma 20, if algorithm Amakes at most ccache misses in expectation, then the number of queries it makes is lower\nbounded by the optimum objective of the following LP:\n1\n12 ln(k+ 1)minX\ni\u00151ik1=ixi\nX\ni\u00151ixi\u0014c (1)\nX\ni\u00151xi= 1\nxi\u001508i\u00151\nHere,xi:= Pr[M=i], i.e., the probability that the chain Chas lengthi, or equivalentlyAmakesicache misses. The last\ntwo constraints deÔ¨Åne a probability distribution over the values Mcan have and constraint (1) means that we can afford to\nmake at most ccache misses in expectation.\nLemma 21. Ifc\u0014lnk, then the above LP‚Äôs optimum objective is at least1\n12 ln(k+1)ck1=c.\nProof. Letf(y) :=yk1=y. By simple calculus, we have f0(y) =k1=y(1\u0000lnk\ny)andf00(y) =ln2k\ny2k1=y. Thus,f\ndecreases in yfory2[1;lnk]and is convex. Then, the LP objective is1\n12 ln(k+1)P\ni\u00151f(i)xi. By convexity, we haveP\ni\u00151f(i)xi\u0015f(P\ni\u00151ixi). Then, by constraint (1) and fbeing decreasing in y, we havef(P\ni\u00151ixi)\u0015f(c).\nTo summarize, we have shown that any c-competitive algorithm must make at least1\n12 ln(k+1)ck1=ccache misses, but under\nthe assumption stated in Lemma 16, i.e., Ri>Q2\ni+1for alli. Thus, by the lemma, we have the following.\nTheorem 22. For any integer c\u0014lnk, anyc+ 4-competitive algorithm must make at least1\n12 ln(k+1)ck1=c\u0001OPT queries.",
  "textLength": 53791
}