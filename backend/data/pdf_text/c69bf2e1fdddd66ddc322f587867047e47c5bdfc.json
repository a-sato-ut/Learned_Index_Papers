{
  "paperId": "c69bf2e1fdddd66ddc322f587867047e47c5bdfc",
  "title": "Honeycomb: Ordered Key-Value Store Acceleration on an FPGA-Based SmartNIC",
  "pdfPath": "c69bf2e1fdddd66ddc322f587867047e47c5bdfc.pdf",
  "text": "Honeycomb: ordered key-value store acceleration on an FPGA-based SmartNIC\nJunyi Liu\nMicrosoftAleksandar Dragojevi ´c\u0003\nCitadel SecuritiesShane Flemming\u0003\nAMDAntonios Katsarakis\u0003\nHuawei Research\nDario Korolija\u0003\nETH ZurichIgor Zablotchi\u0003\nMysten LabsHo-cheung Ng\u0003\nImperial College LondonAnuj Kalia\nMicrosoftMiguel Castro\nMicrosoft\nAbstract\nIn-memory ordered key-value stores are an important building\nblock in modern distributed applications. We present Hon-\neycomb, a hybrid software-hardware system for accelerat-\ning read-dominated workloads on ordered key-value stores\nthat provides linearizability for all operations including scans.\nHoneycomb stores a B-Tree in host memory, and executes\nSCAN and GET on an FPGA-based SmartNIC, and PUT,UP-\nDATE and DELETE on the CPU. This approach enables large\nstores and simpliﬁes the FPGA implementation but raises\nthe challenge of data access and synchronization across the\nslow PCIe bus. We describe how Honeycomb overcomes this\nchallenge with careful data structure design, caching, request\nparallelism with out-of-order request execution, wait-free read\noperations, and batching synchronization between the CPU\nand the FPGA. For read-heavy YCSB workloads, Honey-\ncomb improves the throughput of a state-of-the-art ordered\nkey-value store by at least 1:8\u0002. For scan-heavy workloads\ninspired by cloud storage, Honeycomb improves throughput\nby more than 2\u0002. The cost-performance, which is more im-\nportant for large-scale deployments, is improved by at least\n1:5\u0002on these workloads.\n1 Introduction\nIn-memory key-value stores are an important building block\nin modern distributed applications [7,9,14,42,50]. They store\ndata in servers that provide a simple interface ( PUT(K,V) ,\nV=GET(K),UPDATE (K,V) and DELETE (K)) to clients across\nthe network. Ordered key-value stores expand the set of sup-\nported applications by providing an efﬁcient SCAN operation\nto retrieve the key-value pairs whose keys are within a speci-\nﬁed range. For example, distributed ﬁle systems can use SCAN\nto map ranges of logical ﬁle offsets to the nodes storing the\ndata [9]. It is also used to query graph stores [7, 50] and the\npopular Redis [14] offers sorted sets. We describe Honey-\ncomb, a system that provides hardware acceleration for an\nin-memory ordered key-value store.\n*This work was done when afﬁliated with Microsoft\n†This work has been submitted to the IEEE for possible publication.\nCopyright may be transferred without notice, after which this version may\nno longer be accessible.There is a large body of research on improving the perfor-\nmance of ordered key-value stores, e.g., [12, 18, 29, 38, 41,\n52, 56]. Recent research has shown how to leverage modern\nnetworks to achieve high throughput and low latency with an\nRPC-based system [29]. Other research has explored using\none-sided RDMA reads to bypass the server CPU for GET\nand SCAN operations [12, 18, 41, 52, 56]. Since RDMA NICs\nonly provide simple one-sided reads of contiguous memory\nregions, these systems require at least two RDMA reads per\noperation when supporting variable-sized keys or values, and\nthey require client-side caching to avoid additional RDMAs\nwhen traversing the tree data structures stored by the servers.\nHoneycomb accelerates an ordered key-value store using an\nFPGA-based SmartNIC [10,45] attached to a host CPU. These\nSmartNICs are widely deployed in data centers [10,20 –22,45].\nThey enable effective CPU ofﬂoad by avoiding the function-\nality limitations of RDMA and the performance problems of\nSmartNICs based on general-purpose, low-power cores [8].\nPrevious work used a similar SmartNIC to accelerate an un-\nordered key-value store [35]. Honeycomb solves a harder\nproblem because ordered key-value stores use more complex\ndata structures whose operations perform O(log(n)) memory\naccesses instead of the O(1) accesses performed by hash table\noperations in unordered key-value stores.\nHoneycomb implements the ordered key-value store using\na B-Tree [13] that supports variable-sized keys and values\nthat are stored inline in B-Tree nodes to improve the per-\nformance of scans. It uses multiversion-concurrency control\n(MVCC [5]) to provide linearizability [25] for scans. This is\nimportant to provide strong consistency, e.g., in the distributed\nﬁle system example above, clients may fail to read previously\nwritten data when scans are not linearizable.\nHoneycomb stores the B-Tree in host memory. It acceler-\nates GET and SCAN operations by executing them on the\nFPGA, but PUT,UPDATE and DELETE operations are ex-\necuted by the host CPU. Since most workloads are read-\ndominated [4, 15, 17, 34, 50], there is less beneﬁt in accel-\nerating write operations and the cost of accelerating them in\nhardware is high because they may require complex splitting\nand merging of B-Tree nodes. This hybrid approach enables\nmuch larger stores than would be possible using the limited\nDRAM attached to the FPGA and it reduces the complexity of\n1arXiv:2303.14259v2  [cs.DC]  6 Apr 2023\n\nthe FPGA implementation. However, it brings the challenge\nof data access and synchronization across the slow PCIe bus.\nWe overcome this challenge with careful co-design of the\nsoftware running on the CPU and the FPGA hardware image.\nHoneycomb uses several techniques to reduce data accesses\nover PCIe. It caches B-Tree nodes on both FPGA SRAM\nand on-board DRAM. It also uses large B-Tree nodes with\nshortcuts , a list of sorted keys and offsets into the node that\ndivide it into segments of similar size. This allows requests\nto fetch only the relevant segments of each node they access,\nwhich reduces the number of bytes accessed to search for a\nkey and the total amount of memory required to cache interior\nnodes compared to simply using smaller nodes.\nTo use available off-chip FPGA bandwidth efﬁciently, Hon-\neycomb exploits request-level parallelism and avoids head of\nline blocking with out-of-order request execution. To use all\navailable off-chip bandwidth, Honeycomb also implements a\ndynamic load balancer that directs some requests that hit on\nthe cache to host memory if there is no bandwidth available\nto on-board DRAM and there is unused bandwidth over PCIe.\nHoneycomb reduces the cost of synchronization across\nPCIe by making GET and SCAN operations wait free [24]. It\nalso divides each leaf B-Tree node into a sorted block with\nsorted key-value pairs and a small log block that logs recent\nPUTand DELETE operations. The log block is merged into the\nsorted block when its size excedes a threshold. This ensures\nreads traverse mostly sorted items while avoiding sorting the\nnode and synchronizing the CPU and the FPGA on each write.\nA comparison with a state-of-the-art ordered key-value\nstore, which uses eRPC [29] and Masstree [38], shows\nthat Honeycomb improves both peak performance and cost-\nperformance, which is more important in large-scale data\ncenters [28], signiﬁcantly for scan-dominated workloads. For\nexample, in uniform workloads with inserts and short scans,\nHoneycomb improves the throughput by 1:2\u0002for workloads\nwith 50% reads and by more than 2:3\u0002for workloads with\nat least 80% reads. Most importantly, Honeycomb improves\nthroughput per watt of Termal Design Power (TDP), which is\na proxy for total cost of ownership [28], by more than 1:9\u0002\nfor workloads with at least 80% reads. We expect these gains\nto increase with future hardware because newer FPGAs have\nmore on-chip memory to cache B-Tree nodes and they are\nconnected to the host using PCIe Gen5 that has four times\nhigher bandwidth than the PCIe Gen3 used in our system.\nWe describe the architecture of Honeycomb in Section 2,\nthe algorithms and data structures used to implement the B-\nTree in Section 3, and how these are implemented in hardware\nin Sections 4 and 5. Section 6 presents the evaluation.\n2 Honeycomb architecture\nFigure 1 shows the architecture of Honeycomb: an FPGA-\nbased SmartNIC that connects a host server to the datacenter\nnetwork. The Honeycomb accelerator is built on the Catapult\nHost Machine FPGA\nEthernet\n(50 Gbps)CPU\n(put, update, \ndelete)\nOnboard\nDRAM (4 GB)\nOnboard\nDRAM (4 GB)\nHost Memory \n(64 GB)\nHost Memory \n(64 GB)\nB-Tree Accelerator\n(scan, get)PCIe\nDMA\nMemory Subsystem\nCache PageTable\nMemory Subsystem\nCache PageTable\nMemory Subsystem\nCache PageTableNetworking\nTransport RDMA\nNetworking\nTransport RDMA\nNetworking\nTransport RDMAFigure 1: Hardware system architecture.\nFPGA infrastructure [10, 45], which provides a shell with IO\nsupport. The core FPGA image has three subsystems: the\nB-Tree accelerator, networking, and memory.\nThe B-Tree is stored in host DRAM because it scales to\nmuch larger capacities than on-board DRAM in FPGAs. The\nB-Tree accelerator implements GET and SCAN operations and\nPUT and DELETE operations are executed by the CPU.\nThe networking subsystem implements LTL [10], a UDP-\nbased transport protocol similar to RoCE v2 [55] on top of\n50 Gbps Ethernet, and an RDMA engine to enable efﬁcient\nhost-to-FPGA communication. The RDMA engine supports\ncustom commands for GET and SCAN in addition to general\ncommands like READ ,WRITE ,SEND , and RECEIVE . These\nenable kernel bypass at the clients and at the servers for PUT,\nUPDATE and DELETE . They also enable the B-Tree accelera-\ntor to completely bypass the CPU for GET and SCAN requests\nby receiving these commands directly from the network sub-\nsystem, processing them, and sending back replies.\nThe memory subsystem interfaces with host memory over\nPCIe and with on-board DRAM. It maintains a cache and\na page table. The cache stores interior B-Tree nodes in on-\nboard DRAM and the root of the B-Tree in on-chip SRAM.\nThe page table simpliﬁes atomic updates to the B-Tree (Sec-\ntion 3.4) by adding indirection [34]. It is stored in on-board\nDRAM and maps logical identiﬁers (LIDs) for B-Tree nodes\nto their physical addresses in host memory.\nThe FPGA is connected over two PCIe Gen 3 \u00028to the\nhost. We measured a peak throughput of 13 GB/s and a latency\nof more than 1 µs (depending on load). For comparison, the\nbandwidth between host CPU and DRAM is up to 64 GB/s\nwith an order of magnitude lower latency, and CPU cores have\nlarge caches that improve performance further. Therefore,\nHoneycomb must address the challenge of data access and\nsynchronization across the PCIe bus to achieve performance\nand cost-performance gains relative to CPU-only systems.\nThe next sections describe how we overcame this challenge\nwith careful hardware-software co-design.\n2\n\nHeader\nShortcuts Sorted Log\nHeader\nShortcuts Sorted LogFigure 2: Honeycomb node layout.\n3 B-Tree\nHoneycomb implements a B+ tree [13], i.e., key-value pairs\n(also called items) are stored only in leaf nodes while interior\nnodes store keys and pointers to child nodes. Honeycomb\nguarantees linearizability [25] for all operations including\nscans.\n3.1 Node layout\nB-Tree nodes are ﬁxed-size, which is conﬁgured to 8 KB in\nthis paper. They are allocated in pinned host memory to allow\nthe FPGA to use physical memory addresses directly. B-Tree\nnodes do not store addresses of child or sibling nodes directly.\nInstead, they store 6-byte logical identiﬁers (LIDs) [34]. The\nmapping from a LID to the virtual and physical addresses of\na node is maintained in a page table.\nThe node layout is shown in Figure 2. The node header\nis stored in the ﬁrst 48 bytes. It contains ﬁelds specifying\nthe node type (interior or leaf), the number of bytes used,\na lock word, and a version number to synchronize accesses\nto the node. The header of an interior node stores the LID\nof the leftmost child. The header of a leaf node stores the\nLIDs of the siblings to improve scan performance. Leaf nodes\nstore keys and values in variable-size blobs. Each blob has\na 2-byte header that speciﬁes its size. Keys and values are\nstored inline in B-Tree nodes to improve performance. In the\ncurrent conﬁguration, the maximum key length is 460 bytes\nand values larger than 469 bytes are stored outside the node.\nLeaf nodes are split in two blocks: sorted and log. The\npointer to the boundary between them is stored in the node’s\nheader. The sorted block stores items in ascending lexico-\ngraphic order of keys and the log block stores a log of recent\nchanges. When the log block grows larger than a threshold\n(currently set to 512 bytes), it gets merged with the sorted\nblock. Therefore, read operations beneﬁt from accessing\nmostly sorted data and write operations avoid the overhead of\nsorting the node on every change. It also reduces synchroniza-\ntion across PCIe because it requires one update to the FPGA\npage table per-merge instead of one on every write operation.\nSince interior nodes change infrequently, the cost of storing\na log block and accessing it on reads outweighs the beneﬁts.\nTherefore, interior nodes do not have log blocks.\nEntries in the log block are either newly inserted or updated\nitems or delete markers. Each entry stores a 2-byte pointer to\nan item in the sorted block. For a newly inserted item, it pointsto the ﬁrst item with key greater than the item’s key. For other\nentries, it points to the deleted or updated item. In addition,\neach entry records the 1-byte index in the sorted order of log\nitems at the time of insertion. These ﬁelds are used to sort all\nnode items, when searching the leaf, without comparing keys\nand with O(1) cost per log block item (Section 4.3).\nTo optimize the search for a key in a node, we split the\nsorted block into segments of roughly equal size. We store\nthe keys at the boundaries of segments and the pointers to the\nboundaries in the shortcut block, which is stored immediately\nafter the node header. We currently reserve 48 bytes for the\nheader and 464 bytes for the shortcut block. The shortcut keys\nare selected during the merge of the sorted and log blocks.\nThey are stored only in the shortcut block, i.e., the start of\nthe segment for boundary key K contains the value associated\nwith K rather than another copy of K followed by the value.\nThe search for a key starts by traversing the shortcut block\nto identify the segment that contains the key. The search\nexamines only that segment and the log block, which reduces\nthe amount of data read both from host memory and the cache.\nSince Honeycomb’s bottleneck is FPGA off-chip bandwidth,\nthis optimization signiﬁcantly improves performance. For\nsmall keys with the current conﬁguration, a search reads at\nmost 1.5 KB of data from an 8 KB node, which is a 5\u0002\nperformance improvement relative to reading the whole node.\nAnalysis shows that using large nodes with shortcuts re-\nduces the number of bytes accessed to search for a key and\nthe total amount of memory required to cache interior nodes\ncompared to simply using smaller nodes. For the example of\na ﬁve-level tree with 8 KB nodes, 16-byte keys and values\nat 55% occupancy, searching the tree with this optimization\naccesses fewer than 75% of the bytes accessed searching a\nsimple tree with 512-byte nodes with a similar number of\nitems. It also requires approximately four times less space to\ncache all the interior nodes. The intuition is that the FPGA\ncan use the information in the header to fetch a sorted block\nsegment or a log block with a single DMA that reads only the\nbytes in use, whereas it would need to read the whole node or\nissue more dependent DMAs for the simple tree. Also using\nsmaller nodes increases overhead because it requires more\nstorage for pointers to children in interior nodes. Using log\nblocks reduces the impact of sorting the large node on writes.\n3.2 Synchronization\nGETand SCAN operations executed by the accelerator are wait-\nfree [24] – they never block or retry due to PUT orDELETE\noperations executed by the CPU. This reduces the overhead of\nsynchronization over PCIe and provides predictable latency.\nWe use multi-version concurrency control (MVCC) [5] to\nensure that read operations can always access a consistent\nsnapshot of data. Honeycomb maintains two shared 64-bit\nversion numbers: the global write version and the global read\nversion . A write operation performs an atomic fetch and add\n3\n\non the global write version to obtain its write version, which is\nused to version any items it creates. Writers release changes\nto readers in version order: a writer sets the global read ver-\nsion to its write version when it becomes the writer with the\nsmallest write version, and updates the copy of the global\nread version maintained by the accelerator over PCIe. We\ndelay responses to write requests until this update completes.\nWriters synchronize with each other using node locks, which\nare ignored by readers. Read operations read the global read\nversion maintained by the accelerator to get their read version.\nThey ignore items with versions greater than this value.\nAll items in the sorted block have the same version that is\nstored in the node header and is called the node version . The\nnode version is the write version of the write operation that\ncreated the version of the node. Nodes also store a pointer to\nthe state of the node as of the previous version. A reader that\nobserves a node version greater than its read version follows\nthe chain of old version pointers until it reaches a node with\na version less than or equal to its read version. The version\nof an item in the log block is stored in the item header. To\nreduce the size of these headers, we store a 5-byte version\ndelta from the node version instead the entire 8-byte version.\nIf a write would cause the delta version to wrap around, it\nmerges the sorted and log blocks. Readers ignore all items in\nthe log block with versions greater than their read version.\nTo ensure operations observe a consistent snapshot, Hon-\neycomb does not reclaim old versions of nodes while at least\none inﬂight operation can access a snapshot to which the old\nversion belongs. To garbage collect old versions that are not\naccessible anymore, Honeycomb uses an epoch-based mem-\nory manager [39]. Each CPU thread executes operations one\nat a time, assigns a sequence number to each inﬂight opera-\ntion, and exposes its current operation sequence number to the\nmemory manager. Similarly, the B-Tree accelerator assigns a\nsequence number to each operation and exposes the sequence\nnumbers of the newest and the oldest inﬂight operation to the\nmemory manager over PCIe. When a CPU thread makes a\nchange that removes old versions of nodes from the tree, it\nputs the old versions into a garbage collection list tagged with\na vector timestamp. This vector timestamp has entries for the\ncurrent operation sequence numbers of all CPU threads and\nthe newest operation sequence number on the accelerator.\nPeriodically, a thread scans the garbage collection list and\nreclaims all node versions that are not reachable by CPU\nthreads or the accelerator, i.e., node versions with a vector\ntimestamp where the entry for each CPU thread is smaller\nthan the current operation sequence number for the thread,\nand the entry for the accelerator is smaller than the current\noldest inﬂight operation on the accelerator. If a writer fails to\nallocate memory for a new node and the garbage collection\nlist is not empty, the operation is aborted and retried.\nThe page table is stored both in host DRAM and in FPGA\non-board DRAM. When a new node mapping is created or\nwhen a node’s mapping is changed, the CPU updates the tablein host memory and issues a PCIe command to the FPGA to\nupdate the copy of the table in FPGA on-board memory.\nHoneycomb can be conﬁgured with MVCC off by using\nzero as the version number for all nodes and log items, and by\nnot setting old version pointers. This improves write perfor-\nmance a little by avoiding updates to the global read version\non the FPGA. The atomic tree updates using the page table\nstill ensure linearizability for GET,PUT, and DELETE , but not\nforSCAN because sibling pointers are not updated atomically.\n3.3 Scan and get\nHoneycomb implements SCAN (Kl, Ku)that ﬁnds the largest\nkeyKsless than or equal to Kl, and returns a sorted list with\nall key value pairs with keys greater than or equal to Ksand\nless than or equal to Ku. We implement this version of SCAN\nto support mapping from a range of logical ﬁle offsets to\nthe nodes storing the data in distributed ﬁle systems [9]. In\nthis application, the key-value pairs represent variable-sized\nchunks of ﬁle data where the key is the logical offset of the\nﬁrst byte in the chunk and the value points to the servers\nstoring the data. Since the start of a logical ﬁle offset range\nin a client request can be in the middle of a chunk, the SCAN\nsemantics we implement are necessary for correctness.\nSCAN (Kl, Ku)traverses the tree from the root to a leaf.\nIt fetches the node header and shortcut block of each node\nit visits (currently stored in the ﬁrst 512 bytes). It follows\nold version pointers if needed to ﬁnd a node with version\nless than or equal to the operation’s read version. Then it\nsearches for the largest shortcut key less than or equal to Kl,\nand fetches the associated sorted block segment. When SCAN\nvisits an interior node, it searches the segment for the item\nwith the largest key less than or equal to Kl(which could be\nthe shortcut key), and obtains the LID of the next node to visit\nfrom the item.\nWhen SCAN reaches the leaf, it fetches the log block in\nparallel with searching the shortcuts. Then it searches both\nthe sorted and log blocks for the largest key Ksless than or\nequal to Kl. It scans forward from Ksuntil it reaches a key\ngreater than Kuor the end of the tree. It ignores any items with\nversions greater than the read version. If it ﬁnds one or more\nitems with the same key, it returns the value associated with\nthe item with the latest version unless that item is a delete.\nOtherwise, it ignores the item. SCAN uses sibling pointers to\nmove to the next leaf if needed.\nThe accelerator optimizes scan performance by determin-\ning a sorted order for the items in the log block of a leaf, and\nuses the back pointers from log block items to sorted block\nitems to determine a sorted order across all the items in the\nleaf (see Section 4.3). This enables efﬁcient scanning by over-\nlapping data access with key comparisons, generating results\nthat are already sorted, and avoiding key comparisons with\nall items in the log block.\nHoneycomb implements GET(K)asSCAN (K,K) and post-\n4\n\na) The tree before the insert into \nthe rightmost nodeb) Allocate a new buffer and merge into \nit. Insert the old pointer (dashed line) to \nthe old node.\nc) Update the page table to point to the new bufferInsertInsertMerge in\nnew buffer\nOld\nptrMerge in\nnew buffer\nOld\nptr\nOld\nptr GCOld\nptr GCFigure 3: Merging the sorted and log blocks.\nprocesses the result to return the value associated with K, or\nreturn not found if K is not in the result.\n3.4 Insert\nAPUT(K,V) operation inserts a new item if there is no item\nwith key Kin the tree. It traverses the tree as described for\nSCAN (K,K) without acquiring any locks, but it reads the\nlatest version of each node to implement linearizability. The\noperation locks the leaf node before modifying it to ensure\nthe state matches the one observed during the traversal.\nThe node header has a 32-bit lock word with a lock bit and\na 31-bit sequence number, which is incremented on writes to\nthe node. PUT tries to acquire the leaf’s lock using an atomic\ncompare-and-swap instruction that checks if the sequence\nnumber matches the one observed during the traversal. If the\ncompare-and-swap fails, the PUT operation restarts.\nFast-path insert: In the common case, the operation sim-\nply adds a new item to the log block: it copies the item to the\nlog block; acquires the write version with a fetch-and-add on\nthe global write version; sets the version of the item in the log\nblock; updates the node size; increments the node sequence\nnumber; and unlocks the node. The node size and the lock\nword are packed into a 64-bit word. So that the last three\nsteps can be performed with a single instruction. Concurrent\noperations either observe the node without the item or with\nthe item correctly written, and readers ignore the item until\nthe change is released (Section 3.2).\nSorted and log block merge: If the size of the log block\nexceeds a threshold (currently 512 bytes), the operation\nmerges the sorted and log blocks as shown in Figure 3. It\nallocates a new memory buffer for the node and sorts all the\nitems into a single sorted block in the new buffer. While\nsorting, it selects the shortcut keys. For each item, it decides\nwhether to put the key in the shortcut block based on the num-\nber of bytes copied so far, the remaining bytes to copy, and the\naverage size of keys and values. It attempts to maximize the\nnumber of shortcut keys while ensuring that segments have\nsimilar sizes, a minimum size (currently 256 bytes), and that\nNSplit NL NR\nSplit N into N L and N RCopy into new buffer and modify\nOld ptr\nOld ptr\nOld ptrNSwap\nNSplit NL NR\nSplit N into N L and N RCopy into new buffer and modify\nOld ptr\nOld ptr\nOld ptrNSwap\nGCNSwapOld ptr\nOld ptr\nOld ptrGC\nGCNSwapOld ptr\nOld ptr\nOld ptrGCa) Before split. The rightmost \nnode is getting split. The root of \nthe split is the root of the tree.b) Allocated two new nodes for the split and a \nnew buffer for swapping the root of the split.\nc) The mapping of the root of the split is updated \nand the split nodes are put into GC listSplitSplitSplitFigure 4: Splitting a node during insert.\nshortcut keys ﬁt in the shortcut block.\nThe operation sets the node version to the write version,\nand the old version pointer to the address of the old buffer\n(Figure 3b). Then it updates the LID mapping with the address\nof the new buffer in the page tables of both CPU and FPGA\n(Figure 3c). The parent node does not need to be changed\nbecause the LID of the child remains the same. The operation\nalso puts the old buffer in the garbage collection list and\nreleases the changes to make them visible to readers. This\nensures concurrent operations observe the change atomically.\nNode splitting: If there is not enough space in the leaf for\nthe merged block, the operation splits the leaf in two. This\nrequires inserting a new item in the parent node as shown\nin Figure 4. If the parent is full, the operation also splits the\nparent. Splitting can propagate to the root. The last interior\nnode that is updated but not split is called the root of the split .\nThe operation acquires locks on all interior nodes to split\nand on the root of the split. If locking fails due to a sequence\nnumber mismatch, it releases all acquired locks and restarts.\nThe operation also allocates two new nodes (new LID and\nmemory buffer) for each node that it splits and a memory\nbuffer (same LID) for the root of the split ( NL,NR, and Nswap\nin Figure 4b). Then it processes nodes from the leaf up split-\nting the items in each node evenly into the two newly allo-\ncated nodes. When it gets to the root of the split, the operation\ncopies the items into the new buffer and adds the LIDs of the\nnew child nodes and the key at the boundary between them.\nTo swap in the new subtree atomically, the operation up-\ndates the LID mapping for the root of the split in both CPU\nand FPGA page tables to point to the new buffer (Figure 4c). It\nalso locks the sibling leaves and updates their sibling pointers\nto point to the new leaves. This is sufﬁcient to ensure lineariz-\nability for all operations but scans, because the updates to the\nsibling leaves and the subtree swap are not atomic.\nLinearizable scans are important for applications like the\ndistributed ﬁle system [9]. Without them, a client Ccould\nwrite data Dto a ﬁle at a logical offset range R, and a later\n5\n\nRequest Management\nRequest\nPreprocess\nEpoch\nManagerKey Buffers\nLB Key\nBufferUB Key\nBufferB-Tree Accelerator\nLeaf -node Scan Engine\nInterior -node Search Engine\n Request\nInputResponse\nOutputMemory \nAccess\nGeneratorMemory \nAccess\nGeneratorBuffer\nSlot\nMSI MSI MSI MSIKey Search Array\nKey\nSearch\nUnitKey\nSearch\nUnitRange Scan Array\nRange\nScan\nUnitRange\nScan\nUnit\nSC Buffer L Buffer\nResult Buffer\nMSI\nAdapterMSI\nAdapterMSI\nAdapterMSI\nAdapterMSI\nAdapterMSI\nAdapterMSI\nAdapterMSI\nAdapterS BufferFigure 5: B-Tree accelerator architecture.\nKey Search UnitData\nBufferKey\nBufferKey\nAlignment\nKey\nComparison\nPipeline\nResult\nOutputResult\nGeneratorRequest\nInputRequest\nKeyBlock\nData Figure 6: Key seach unit\nread from a range containing Rcould fail to return D. This can\nhappen if the B-Tree node with the mapping for the servers\nthat store Dis split and the sibling pointers are not updated\nbefore the scan to ﬁnd the servers storing the data being read.\nTo provide linearizable scans, the operation sets the ver-\nsions in all newly allocated buffers to its write version, and\nsets the old version pointer in the new root of the split to\nthe address of the old buffer (Figure 4b). This ensures that\noperations with older read versions traverse the old subtree.\nIt also sets the old version pointers of the new leaves to the\naddress of the old leaf. This ensures that scans with older\nread versions, which may reach the new leaves using sibling\npointers, traverse the old leaf. The operation also puts the\nLIDs and memory buffers of all nodes that got split, and the\nold memory buffer of the root of the split in the garbage col-\nlection list. The operation then unlocks all nodes and releases\nthe changes to readers (Section 3.2). This ensures readers\nobserve all changes made during the split, or none of them.\nIf the tree root must be split, the operation grows the tree\nby allocating a LID and a memory buffer for the new root,\nand updating the root and tree height in the accelerator.\n3.5 Delete and update\nThe implementations of UPDATE and DELETE are similar to\nINSERT .UPDATE appends the updated key-value pair to the\nlog block, which points to the previous version of the item\nin the sorted or log block. For DELETE , a delete marker is\nappended in the same way to indicate that readers should\nignore the deleted item. The space for stale and deleted items\nis reclaimed when merging the sorted and log blocks. Nodes\nwhose occupancy drops below a threshold are merged and\nnodes that become empty are deleted. We use similar tech-\nniques to ensure atomicity as for splits but omit the details.\n4 B-Tree accelerator implementation\nThe B-Tree accelerator has three components (Figure 5): re-\nquest management, interior-node search engine, and leaf-node\nscan engine. Request management parses requests and assignsthem sequence numbers and read versions. The interior-node\nsearch engine is responsible for traversing the tree from the\nroot to a leaf. The leaf-node scan engine traverses the leaf or,\nforSCAN operations, leaves using sibling pointers.\n4.1 Request management\nThe time to complete requests is variable, e.g., due to different\nsize keys or cache misses. Therefore, the B-Tree accelera-\ntor exploits request-level parallelism and avoids head of line\nblocking by supporting out-of-order request execution. This\nmaximizes the utilization of compute resources, and on-board\nDRAM and PCIe bandwidth.\nThe request pre-process module extracts the lower (LB)\nand upper bound (UB) keys from the request and stores them\nin centralized key buffers. These buffers support multiple\nread ports to enable the interior-node search engine and the\nleaf-node scan engine to read multiple keys in parallel. When\na request completes, its key buffers are freed.\nThe request pre-process module also initializes request\nmetadata that includes: identiﬁers of the buffers holding the\nkeys, the LID of the node being traversed, the node’s level, the\nblock type being traversed (shortcut, sorted, or log), and the\nstart offset of the block/segment within the node. The request\nmetadata ﬂows over a wide data path through the accelerator.\nRequest management is also responsible for storing the\naccelerator’s copy of the 64-bit global read version, which\nis updated by the CPU over PCIe (Section 3.2). It reads the\nglobal read version to assign a read version to each incoming\nrequest. Requests are also assigned a 64-bit sequence num-\nber by incrementing a counter Snewmaintained by the epoch\nmanager. These are part of the request metadata.\nThe epoch manager also monitors request completion to\nkeep track of Sold, the sequence number of the oldest inﬂight\nrequest. It exposes SoldandSnewover PCIe to the memory\nmanager running on the CPU to enable reclaiming mem-\nory when it is no longer accessible by inﬂight requests (Sec-\ntion 3.2). Accesses to SoldandSnewover PCIe are infrequent.\n6\n\nc) Element 60 inserted a) Empty log blockb) Element 90 insertedSorted 9090606000\nSorted 906000\nd) Element 30 inserted e) Element 45 insertedSortedSorted Sorted 90900\nSorted 900\nSorted 90906060303045450001\nSorted 906030450001\nSorted 909060603030000\nSorted 906030000Figure 7: Example of order hints in the log block.\na) Processing item 90 b) Processing item 60\nd) Processing item 45 e) Final state of the indirection arrayc) Processing item 309090\n909060\n9060\n6060909030\n609030\n45\n30306060909045\n306090 3030454560609090 30456090 Figure 8: Sorting the log indirection array with order hints.\n4.2 Interior-node search engine\nThe interior-node search engine is responsible for travers-\ning interior nodes from the root as described in Section 3.3.\nIt uses a ring architecture to search node blocks iteratively\nwhile overlapping memory reads with compute (Figure 5).\nThe architecture exploits request-level parallelism by using\nmultiple memory subsystem interface (MSI) adapters and key\nsearch units (KSU). Additionally, each MSI adapter supports\nout-of-order data transfer to fully utilize off-chip bandwidth.\nFor each interior node visited by a GET orSCAN request,\nthe memory access generator ﬁrst generates a memory access\nrequest to one of the MSI adapters to fetch the node’s header\nand shortcut block. The adapter issues the request to the mem-\nory subsystem, waits for the response, and sends the response\nto one of the KSU. The KSU ﬁrst checks if the version in\nthe node header is greater than the request’s read version. If\nso, it sends the updated request metadata back to the memory\naccess generator to visit the old version of the node. If not, the\nKSU searches the shortcuts for the sorted block segment to\nfetch. Then sends the updated metadata back to the memory\naccess generator that generates a memory access request to\none of the MSI adapters to fetch the sorted block segment.\nWhen the adapter gets the segment, it is sent to one of the\nKSU. The KSU searches the segment for the LID of the next\nnode to visit. If the next node is a leaf, the request is passed on\nto the leaf-node scan engine. Otherwise, the request is passed\nto the memory access generator to continue with the child.\nThe architecture of a KSU is shown in Figure 6. It uses\nthe request input metadata to fetch the request’s lower bound\nkey. Then it processes the block data (header and shortcut, or\nsorted block segment) to ﬁnd the largest key that is smaller\nthan or equal to the request key. Since keys are variable size,\nKSU have to process keys sequentially. They process keys\nin a streaming fashion overlapping data transfer with key\ncomparisons. The key comparison pipeline implements com-\nparison logic equivalent to the C++ memcmp() function. It\ncompares key fragments stored in registers with conﬁgurable\nwidth (currently 16 bytes). Key alignment uses barrel shifters\nto stream keys into the comparison pipeline. The result gener-\nator outputs updated request metadata.\n4.3 Leaf-node scan engine\nThe leaf-node scan engine is responsible for traversing leaf\nnodes as described in Section 3.3. As shown in Figure 5, it also\nuses a ring architecture to scan leaf node blocks iteratively,and exploits request-level parallelism by using multiple MSI\nadapters and range scan units (RSU).\nUnlike the key search array, the range scan array includes\nseveral buffers: result buffer, SC buffer, S buffer , and L buffer .\nThe result buffer is used to accumulate partial results because\nthe leaf-node scan engine produces a sorted list of key-value\npairs spread across different blocks in one or more leaves. The\nSC buffer, S buffer, and L buffer are used to buffer data from\nshortcut, sorted, and log blocks in a leaf, respectively. This\nenables iteration over keys in the three blocks in a sorted order\nto generate sorted results. The range scan array also includes\ndifferent RSU variants to scan each type of block and ﬁnite\nstate machines (FSM) that coordinate scans across the three\nRSU variants. The buffers are separate from RSUs to enable\noverlapping of compute with data buffering. The buffers are\ndivided into request slots that also include the FSM state. The\nmemory access generator maintains a list of available slots. It\nassigns slots to requests or pushes back if no slot available.\nThe architecture of the RSU is similar to the KSU (Fig-\nure 6) but it has two parallel key comparison pipelines to\ncompare keys with both the lower and upper bound keys in\ntheSCAN operation. Additionally, the RSU variant responsi-\nble for log block processing must ﬁrst sort the log block to\nenable iterating over the key-value pairs in order.\nWe leverage hardware-software co-design to optimize log\nblock sorting. Sorting does not compare keys and costs only\nO(1)cycles per item. It uses 1-byte order hints stored in\nlog block items by inserts. Figure 7 illustrates an example\nsequence of insertions into the log block. For clarity, the\nnumber in the item represents its key. The number above the\nitem represents its order hint. The log block is initially empty.\nItem 90 is inserted ﬁrst and is assigned order hint 0. Next,\nitem 60 is inserted. Since it is the smallest item in the log\nblock, it is assigned order hint 0. The order hints in existing\nitems are not changed. Then, item 30 is inserted with order\nhint 0 since it is the smallest in the log block again. The ﬁnal\nitem is 45 and its order hint is 1.\nSorting creates an indirection array with offsets of items\nin the log block in ascending key order. Figure 8 illustrates\nsorting of the log block in Figure 7. It shows keys instead\nof item offsets for clarity. Items are processed in the order\nin which they are stored. When the item with order hint iis\nprocessed, its offset in the log block is inserted into position\niin the indirection array and all the elements at positions\nj\u0015iare shifted to the right. Since the indirection array is\nstored in a large shift register, insertion costs one cycle and\n7\n\nMemory Subsystem\nCache\nMeta\nData\nTableNode \nAddress\nTable\nPCIe\n(Host Memory)Onboard\nDRAM\nResponse\nOutput\nArrayRequest\nInput\nArrayLoad\nBalancerResponse\nCrossbar\nPage\nTableWrite\nBackRoot\nCacheFigure 9: Memory Subsystem\ncan be overlapped with fetching the next item. Any items with\nversion greater than the operation’s read version are ﬁltered\nout. After sorting, the RSU uses the indirection array to stream\nkeys to the key comparison pipelines in order.\n5 Memory subsystem\nAs shown in Figure 9, the memory subsystem includes the\ncache and the page table. It caches B-Tree nodes to improve\nperformance by reducing off-chip memory accesses, reducing\nmemory accesses over PCIe, and maximizing the use of avail-\nable off-chip bandwidth. The root node, which is accessed\nby all requests, is cached in on-chip memory to save off-\nchip bandwidth. Other interior nodes are cached in on-board\nDRAM to reduce PCIe accesses, which both improves request\nthroughput and latency. The load balancer directs some mem-\nory accesses over PCIe even when they hit in the cache to use\nall available off-chip memory bandwidth.\nThe memory subsystem supports multiple input/output in-\nterfaces for parallel access from the interior-node search and\nleaf-node scan engines. It also maintains a page table map-\nping node LIDs to node physical addresses. This is exposed\nover PCIe for write operations to replace subtrees atomically\nby changing mappings (Section 3.4). When the slow path for\na write operation changes a page table mapping, the cache\nentry for the node with that LID is invalidated. Leaf nodes\nare not cached to avoid the need for cache invalidations over\nPCIe for every write operation to the B-Tree.\nThe metadata table keeps track of cached B-Tree nodes in\non-board DRAM. It implements a four-way set associative\ncache indexed by LIDs. Each entry in a set records the node\nLID, the physical address, and a 32-bit occupancy map where\neach bit represents 256 bytes of node data (with 8 KB nodes).\nThe metadata table itself is stored in on-board DRAM. Honey-\ncomb uses a small on-chip metadata cache, which has capacity\nfor 1K entries in the current implementation, to minimize the\ntrafﬁc to on-board DRAM.\nOn cache misses, the cache module fetches an integral num-\nber of 256-byte chunks over PCIe and attempts to write themback to the cache. The write back module implements a lock-\ning scheme to avoid conﬂicts when the cache is invalidated on\npage table updates or during B-Tree root updates. The write\nback operation allocates space for a new node in the cache\nonly if the missing access was to the header and shortcut block\nof the node. The write back module ﬁlls other chunks in the\nallocated cache space when it reads sorted block segments for\nthe node. When interior nodes cannot ﬁt in the cache, we evict\na random node from the same set. We leave more complex\ncache replacement policies for future investigation.\nThe metadata table determines if incoming memory read\nrequests are hits or misses, and sends them to the node address\ntable (NAT). The NAT is responsible for ensuring each request\nhas a consistent view of each node it visits. For example,\nwithout the NAT, a request could read the shortcut block from\nthe cache and an inconsistent sorted block segment from host\nmemory after the page table entry for the node is updated.\nThe NAT maps the request sequence number to the physical\naddress of the node version ﬁrst accessed by the request. The\nNAT entry can be written by both the metadata table and\nthe page table depending on whether the ﬁrst access by the\nrequest to a node is a hit or a miss. When an access to a sorted\nblock hits in the cache, the physical address in the cache entry\nmust match the physical address recorded in the NAT for\nthe request. In case of a mismatch or cache miss, the sorted\nblock segment is always loaded from host memory using the\nphysical address from the NAT for the request.\nWith large caches, the accelerator can become bottlenecked\non accesses to on-board DRAM while underutilizing PCIe\nbandwidth. To use all available off-chip memory bandwidth,\nthe load balancer directs some memory accesses that hit in the\ncache to host memory over PCIe. It constantly monitors the\nnumber of inﬂight operations and the total number of bytes\nbeing read by those operations on both DRAM and PCIe\ninterfaces. Then balances load across the two interfaces.\n6 Evaluation\nThis section compares Honeycomb with eRPC-Masstree [29,\n38], a state-of-the-art ordered key-value store that supports\nvariable-size keys and values (we discuss why we chose this\nbaseline in Section 7). It also evaluates the impact of several\noptimizations on performance.\n6.1 Experimental setup\nExperiments ran one server on one machine and clients on\nanother. They ran on a single socket of a dual-socket machine\nbecause this provided the best cost-performance for both sys-\ntems. Each socket had a 10-core Intel Xeon E5-2660 v3 @\n2.89 GHz and four channels to 64 GB of DDR4-2133 DRAM.\nThe FPGA accelerator card had an Intel Arria 10 1150 FPGA\nwith two channels to 4 GB on-board DDR4-2133 DRAM,\ntwo PCIe Gen3 x8 channels, and 50-Gbps Ethernet. Each\n8\n\nA B C D E F\nYCSB Workloads02468101214Throughput (M reqs/sec)UniformeRPC-Masstree Honeycomb\nA B C D E F\nYCSB Workloads020406080Efficiency (K reqs/sec/W)UniformeRPC-Masstree Honeycomb\nA B C D E F\nYCSB Workloads02468101214Throughput (M reqs/sec)Zipfian/LatesteRPC-Masstree Honeycomb\nA B C D E F\nYCSB Workloads020406080Efficiency (K reqs/sec/W)Zipfian/LatesteRPC-Masstree HoneycombFigure 10: Comparison of throughput and efﬁciency for YCSB workloads using uniform and Zipﬁan/latest distributions.\nLogic Register Block RAM\nShell 14.0% 8.9% 14.6%\nNetworking 6.9% 2.4% 21.0%\nB-Tree accelerator 33.0% 11.5% 35.6%\nMemory subsystem 7.3% 2.9% 7.7%\nTotal 61.2% 25.7% 78.8%\nTable 1: FPGA resource usage of Honeycomb.\nA B C D E F\nRead\nOperationLOOKUP LOOKUP LOOKUP LOOKUPSCAN\n(1 to 100)LOOKUP\nWrite\nOperationUPDATE UPDATE - INSERT INSERTRD-MOD -\nWR\nRd-Wr\nRatio (%)50:50 95:5 100:0 95:5 95:550:50\n(66.6% LOOKUP )\nTable 2: YCSB workloads.\nmachine had a separate 40-Gbps Ethernet ConnectX-3 NIC.\nWe used the socket that is attached to both the FPGA and\nthe NIC. We ran 10 software threads, each pinned to a differ-\nent core on the socket. Honeycomb ran on Windows Server\n2016 Datacenter using the FPGA, and eRPC-Masstree ran on\nUbuntu 20.04 with DPDK 19.11.5 using the ConnectX-3 NIC.\nMachines were connected to a DELL Z9100-ON switch. The\ndifferent bandwidths did not affect the comparison because\neRPC-Masstree was never bottlenecked on the network.\nWe conﬁgured Honeycomb with MVCC by default even\nthough eRPC-Masstree does not provide linearizability for\nscans. The modular design of the Honeycomb accelerator en-\nables the user to choose conﬁgurations with different numbers\nof KSUs, RSUs, and MSIs. This is critical to achieving good\npower efﬁciency because it allows the user to trade off perfor-\nmance for power and hardware resource efﬁciency. We used a\nsimple analytic performance model and tuning experiments to\nselect the minimum number of these units to achieve a target\nSCAN operation throughput of approximately 10 Mops/s. We\nused 14 KSUs, 4 shortcut-RSUs, 5 log-RSUs, 5 sorted-RSUs,\nand 4 MSI adapters in all the experiments described in this pa-\nper. The accelerator is clocked at 220 MHz. The breakdown\nof FPGA resource usage is shown in Table 1. The FPGA\ndesign tool [26] reports a TDP of 34.9 W.\nWe ran each experiment three times and present the averageof the results. The range of the results was below 4% of the\naverage for all experiments.\n6.2 Workloads\nWe used two sets of workloads in the evaluation YCSB [1, 15]\nand a cloud-storage workload representative of the distributed\nﬁle system application [9] discussed throughout the paper. We\nran all six YCSB workloads (see Table 2) with both uniform\nand skewed access patterns. The read-modify-write operation\nin YCSB-F is a combination of a LOOKUP followed by an\nUPDATE . The cloud-storage workload is similar to YCSB-E\nbut uses shorter scans and we varied the percentage of scans\nfrom 50% to 100% to characterize the range of read-write\nratios for which Honeycomb is beneﬁcial. We also varied the\nnumber of key-value pairs returned by scans and the size of\nkeys and values. In all workloads, insert keys were generated\nrandomly with uniform distribution as in [52]. We used both\nuniform and Zipﬁan [15] ( q=0:99) distributions for lookup\nkeys and the start keys of scans.\nAll experiments used the uniform distribution, and 16-byte\nkeys and values unless speciﬁed otherwise. The store had 128\nmillion key-value pairs in the initial state for all experiments,\nwhich are stored in a 4-level tree in Honeycomb. We observed\nthat eRPC-Masstree consumes 3\u0002more memory than the\ntotal size of the key-value pairs to store the whole B-Tree,\nwhereas Honeycomb only consumes about 1 :44\u0002.\n6.3 Comparison with eRPC-Masstree\nCost-performance is the key metric to optimize in large scale\ndata centers. We use TDP as a proxy for total cost of own-\nership (TCO) as in [28]. We use a single-socket server for\nHoneycomb because adding another socket increases TDP\nwithout increasing the throughput of the hardware accelera-\ntor. We ran experiments with two sockets for eRPC-Masstree\nbut they resulted in worse cost-performance, e.g., it achieved\n1:34\u0002better cost-performance with one socket than with\ntwo for read-only 3-item scan workloads. Therefore, we also\npresent single socket results for eRPC-Masstree. We compute\nTDP by adding numbers for each component from published\ndocumentation. The server TDP is 127 W for eRPC-Masstree\n9\n\n50 60 70 80 90 95 100\nRead Percentage024681012Throughput (M reqs/sec)UniformeRPC-Masstree Honeycomb\n50 60 70 80 90 95 100\nRead Percentage010203040506070Efficiency (K reqs/sec/W)UniformeRPC-Masstree Honeycomb\n50 60 70 80 90 95 100\nRead Percentage024681012Throughput (M reqs/sec)ZipfianeRPC-Masstree Honeycomb\n50 60 70 80 90 95 100\nRead Percentage010203040506070Efficiency (K reqs/sec/W)ZipfianeRPC-Masstree HoneycombFigure 11: Comparison of throughput and efﬁciency for cloud-storage workloads with 50% to 100% reads using uniform and\nZipﬁan distributions. The read operation is SCAN with 3 to 4 items in the range.\n0 2 4 6 8 10 12\nThroughput (M reqs/sec)0510152025Median Latency (us)\neRPC-Masstree\nHoneycomb\nFigure 12: Latency-throughput.\n1246810 15 24\nNumber of items in scan range02468101214Throughput (M reqs/sec)\nHoneycomb\neRPC-Masstree Figure 13: Varying SCAN size.\n8 16 24 32 Mixed\nKey Size (Bytes)02468101214Throughput (M reqs/sec)eRPC-Masstree Honeycomb Figure 14: Varying key sizes.\n50 60 70 80 90\nRead Percentage024681012Throughput (M reqs/sec)MVCC-On MVCC-Off Figure 15: Impact of MVCC.\nand 157.9 W for Honeycomb. The TDP for Honeycomb is\nlarger because it uses a 40-W FPGA accelerator board instead\nof a 10-W ConnectX-3 NIC [40].\nBoth the throughput and the TDP should scale by a factor\nof two when using an additional CPU socket connected to\nan additional NIC (for eRPC-Masstree) or SmartNIC (for\nHoneycomb). Therefore, the performance per watt of TDP\nfor such a system would be similar to the results we present\nwith a single socket.\nYCSB throughput and efﬁciency: Figure 10 compares\nboth average throughput (Mreqs/s) and throughput per watt of\nTDP (Kreqs/s/W) of eRPC-Masstree and Honeycomb. Since\nHoneycomb is not optimized for write-heavy workloads, it\nis less efﬁcient running YCSB-A and F. For YCSB-B, C and\nD, Honeycomb improves throughput per watt by 1:5\u0002with\nboth uniform and Zipﬁan/latest distributions. It also improves\nthe throughput for these workloads by 1:9\u0002with uniform\nand1:8\u0002with Zipﬁan/latest distributions. Since Honeycomb\nis optimized for scans, it does particularly well in YCSB-E\nimproving throughout by up to 2:9\u0002and efﬁciency by up\n2:3\u0002. Honeycomb is bottlenecked on the network for large\nscans while eRPC-Masstree is always bottlenecked on the\nCPU.\nCloud storage throughput and efﬁciency: Figure 11\ncompares performance and efﬁciency of eRPC-Masstree and\nHoneycomb running the cloud-storage workload. In this ex-\nperiment, the boundary keys in Honeycomb SCAN (Kl, Ku)\nrequests were chosen to return exactly three items when exe-\ncuted on the inital key-value store. However, they can return\nbetween three and four items because of newly-inserted items.Since eRPC-Masstree provides a different SCAN (K,N) op-\neration that returns the Nitems following K, we selected a\nvalue of Nbetween three and four to ensure SCAN operations\nreturn the same average number of items in both systems. For\nhigh read percentages, Honeycomb also uses CPU cores to\nexecute SCAN operations. We conﬁgured eRPC-Masstree to\nenable any thread to execute SCAN s.\nThe results show that Honeycomb improves both perfor-\nmance and cost-performance signiﬁcantly for scan-mostly\nworkloads. Since Honeycomb accelerates only read opera-\ntions, the improvement grows with the read percentage. In\nthe worst case of 100% writes, Honeycomb achieves only\n58% of the throughput of eRPC-Masstree (47% of the cost-\nperformance) because there is no hardware acceleration and\nthe B-Tree is optimized for hybrid CPU-FPGA execution.\nHowever, even for workloads with 50% writes, Honeycomb\nachieves a similar throughput per watt as eRPC-Masstree and\nbetter throughput ( 1:2\u0002better with uniform distribution). For\nworkloads with at least 80% reads, Honeycomb improves\nthroughput per watt by 1:9\u0002with uniform and 1:6\u0002with Zip-\nﬁan distributions. It also improves the throughput for these\nworkloads by 2:3\u0002with uniform and 2:0\u0002with Zipﬁan dis-\ntributions.\nAs in YCSB workloads, the gains from acceleration are\nlower with the Zipﬁan distribution. Whereas eRPC-Masstree\ncan leverage better locality with CPU caching, the current\nimplementation of Honeycomb does not cache leaf nodes,\nwhich prevents it from caching leaves containing popular\nitems. We plan to explore leaf caching in the future.\nWith modern cloud storage server designs that leverage\n10\n\n0RT163264128256    NoLB\nCache Size (MB)0246810Throughput (M reqs/sec)Throughput\n020406080100\nHit Rate (%)\nCache Hit Rate(a) Performance and cache hit rate.\n RT 16 32 64 128 256 NoLB  \nCache Size (MB)02468101214Bandwidth (GB/sec)Host PCIe-RD\nCache DRAM-RD\nMetadata DRAM-RDPage T able DRAM-RD\nCache DRAM-WR\nMetadata DRAM-WR (b) FPGA memory bandwidth breakdown.\n RT 16 32 64 128 256 NoLB  \nCache Size (MB)051015202530354045IOPS (M ops/sec)Host PCIe-RD\nCache DRAM-RD\nMetadata DRAM-RDPage T able DRAM-RD\nCache DRAM-WR\nMetadata DRAM-WR (c) FPGA memory IOPS breakdown.\nFigure 16: Performance impact of caching and load balancing on Honeycomb (FPGA only) 1-item SCAN .\n0 256 512 768\nLeaf-node Log Block Size (Bytes)02468101214Throughput (M reqs/sec)Insert 1-Item Scan\nFigure 17: Performance impact of log block size.\nNVMe SSDs [44, 49] and fast networks to provide tens of\nmillions of IOPS per server, indexing metadata requires pow-\nerful CPUs that account for a large fraction of the overall\nTCO, e.g., CPUs, DRAM, and the NIC account for half the\nTDP in Open Compute Project’s Poseidon [31] storage server.\nTherefore, Honeycomb can signiﬁcantly increase overall per-\nformance per TCO for these storage servers, e.g., we estimate\nimprovements around 20% for server designs similar to Po-\nseidon. This is very signiﬁcant for large-scale data center\ndeployments.\nWe expect Honeycomb’s cost-performance gains to in-\ncrease with future hardware because newer FPGAs have more\non-chip memory to cache B-Tree nodes and use PCIe Gen5\nthat has 4\u0002the bandwidth of PCIe Gen3. Despite the band-\nwidth improvement, we expect PCIe to remain the bottleneck\nbecause we can use conﬁgurations with more KSUs, RSUs,\nand MSIs to increase parallelism. Therefore, the techniques\nproposed in this paper will continue to be important.\nEnd-to-end latency: Figure 12 shows throughput-latency\ncurves for median latency measured at the client in a read-only\nworkload where each SCAN returns exactly three items. Hon-\neycomb can provide better throughput than eRPC-Masstree’s\nthroughput at lower latency. However, eRPC-Masstree has\nlower latency at low load. This is mostly because Honeycomb\nmemory accesses over PCIe and to on-board DRAM havehigher latency than CPU memory accesses.\nScan size: Figure 13 shows the throughput of both systems\nfor a read-only workload when varying the number of items re-\nturned by SCAN s. The gains of acceleration increase with scan\nsize, for example, Honeycomb has 4:0\u0002better throughput and\n3:2\u0002better throughput per watt with 24-item scans. Since\neRPC-Masstree must follow pointers to each item in the scan\nrange, these random memory accesses become a bottleneck.\nHoneycomb can amortize PCIe accesses to a leaf node over\nmany items because it inlines variable-sized items in leaves.\nWith longer scans Honeycomb becomes network bound while\neRPC-Masstree remains CPU bound (as observed in YCSB-\nE).\nKey size: Figure 14 shows the throughput of both systems\nrunning 1-item SCAN on the initial store when the size of\nkeys and values increases (equal key and value sizes). We\nuse 1-item SCAN to better isolate the impact of increasing\nkey sizes on tree traversal. The performance of both systems\ndrops as key size increases. eRPC-Masstree has deeper trees\nto traverse with larger keys. The depth of the Honeycomb\ndoes not change but the accelerator must fetch larger sorted\nblock segments. We also compared both systems on a store\nwith a mix of key and value sizes chosen uniformly from\nmultiples of 8B less than or equal to 32B. These demonstrate\nthat Honeycomb performs well with variable-sized KV pairs.\n6.4 Impact of optimizations\nWe ran experiments to investigate the performance impact of\nusing MVCC, log blocks, the cache, and the load balancer.\nCost of MVCC: All experiments ran with MVCC to pro-\nvide linearizable scans. Since eRPC-Masstree does not pro-\nvide linearizable scans, we also ran experiments to evaluate\nthe impact of MVCC on throughput. Figure 15 shows that\nturning off MVCC improves Honeycomb performance on\ncloud-storage workloads by up to 14% when the workload\nis bottlenecked by INSERT s. The overhead on read-heavy\nworkloads is negligible.\n11\n\nLog block: Figure 17 shows throughput of 1-item scans\nin a read-only workload and of inserts in a write-only work-\nload with varying log block size. Using log blocks improves\ninsert performance because it avoids adding the new item\nto the sorted block, adjusting shortcuts, and updating the ac-\ncelerator page table on every operation. However, it reduces\nSCAN performance because it increases the amount of data\naccessed over PCIe. Using a 512-byte block achieves most of\nthe beneﬁt while decreasing SCAN throughput by only 8%.\nAccelerator cache and load balancer: Figure 16 shows\nthe impact of caching on 1-item SCAN throughput in a read-\nonly workload using only the FPGA (no CPU). Figure 16a\nshows request throughput and cache hit rate for interior node\naccesses. Caching the root node in on-chip memory (RT) im-\nproves performance by 30% over the no-cache case. Adding\nan on-board DRAM cache with 256 MB (which can hold\nall interior nodes) improves performance by 2:5\u0002. The load\nbalancer directs some cache hits to host memory over PCIe to\nmaximize off-chip bandwidth utilization. Removing the load\nbalancer (NoLB) decreases throughput by 13%.\nFigures 16b and 16c break down bandwidth and IOPS\nfor PCIe and on-board DRAM trafﬁc. For small cache sizes,\nthe bottleneck is PCIe bandwidth and there are writes to on-\nboard DRAM due to cache replacement. With a 256 MB\ncache (100% hit rate) and no load balancing (NoLB), on-\nboard DRAM bandwidth is the bottleneck while 4 GB/s of\nPCIe bandwidth is left unused. The load balancer shifts trafﬁc\nto PCIe to maximize off-chip bandwidth utilization increasing\nthroughput from 8.0 to 9.1 Mreqs/s. Page table and cache\nmetadata reads consume a small fraction of available on-\nboard DRAM bandwidth but a signiﬁcant fraction of IOPS.\nFor small cache sizes, there are no metadata reads from on-\nboard DRAM because all metadata ﬁts in the on-chip cache.\n7 Related work\nWe build on a large body of work on indices for in-memory or-\ndered key-value stores (e.g., [33,34,38,51,54]). A comparison\nstudy [51] showed that the Adaptive Radix Tree (ART) [33]\nperformed best followed by Masstree [38] except on scan-\ndominated workloads where a B+ tree [13] variant was better.\nWe chose Masstree [38] as a baseline because it has better\nscan performance than ART and it supports variable-size\nkeys unlike the B+ tree variant in [51]. Cuckoo Trie [54]\nexploits memory level parallelism to improve multi-core per-\nformance but it has worse scan performance than ART. None\nof these indices supports linearizable scans. Given our focus\non scan-dominated workloads, Honeycomb implements a B+\ntree variant with support for variable-size keys and MVCC.\nMany of the optimizations in recent work exploit the memory\nhierarchy of modern server-class CPUs and are not applicable\nto Honeycomb where the index is accessed over PCIe.\nOrdered key-value stores shard an index across servers\nand provide access to remote clients across the network. Re-cent research has shown how to leverage kernel-bypass using\nDPDK [27] to implement eRPC [29], a fast RPC mechanism.\neRPC and Masstree have been used to implement a high\nthroughput, low-latency, key-value store [29] that we use as\nour baseline. Other research has explored using one-sided\nRDMA reads to bypass the server CPU for GET and SCAN op-\nerations [12, 18, 41, 52, 56]. Since RDMA NICs only provide\nsimple reads of contiguous memory, these systems require at\nleast two RDMA reads per operation to support variable-sized\nkeys or values, and they use client-side caching to avoid addi-\ntional RDMA reads when traversing the index. XStore [52]\nuses a learned cache, a compact client-side cache design in-\nspired by the work on learned indices [16, 32], but it does not\nsupport variable-sized keys or linearizable SCAN s.\nCliqueMap [48] implements a hybrid in-memory key-value\ncaching system with both RPC and remote memory access\n(RMA). Like Honeycomb, it accelerates GET with RMA and\nexecutes SETvia RPC to save CPU cost and improve perfor-\nmance. However, this implementation is based on a hash table\nand does not provide support for efﬁcient SCAN s.\nThere are proposals to extend RDMA, e.g., [2,8], but these\nhave yet to be implemented in NIC hardware.\nSmartNICs avoid the functionality limitations of RDMA\nby providing programmable processing engines in the NIC.\nThere are two types: SoC-based with general purpose cores\nand accelerators for common functionality like compres-\nsion and encryption (see [37] for a survey); and FPGA-\nbased [10, 45]. Since the later offer the promise of better\nperformance and energy efﬁciency at the cost of being harder\nto program, Honeycomb leverages an FPGA-based SmartNIC\nbut implements complex split and merge of B-Tree nodes on\nthe host CPU.\nSmartNICs have been used as ofﬂoads for accelerating\nnetworking, e.g., [3, 6, 19 –21, 36, 43], AI inference [22],\ndistributed ﬁle systems [30], transactions [47], unordered\nkey-value stores [11, 19, 35, 46], and ordered key-value\nstores [23, 37, 53].\nKV-direct [35], which is the best unordered key-value store\nofﬂoad, stores the index in host memory and implements\nhash table reads and writes, which are much simpler than B-\nTree writes, in the FPGA. It achieves better performance than\nHoneycomb because this reduces synchronization across PCIe\nand hash table operations only require O(1) memory accesses,\nbut it does not provide SCAN s. The best ordered key-value\nstore ofﬂoad, HeteroKV [53], implements a B-Tree where the\nleaves are hash tables, which results in poor scan performance.\nIt cannot support large stores because it stores the B-Tree in\non-chip FPGA memory, and it supports only ﬁxed-size keys.\nHoneycomb provides support for large stores, variable-sized\nkeys and values, fast scans, and strong consistency.\n12\n\n8 Conclusion\nIn-memory ordered key-value stores are an important building\nblock in modern distributed applications. We presented Hon-\neycomb, a system that leverages an FPGA-based SmartNIC to\naccelerate these stores with a focus on scan-dominated work-\nloads. It stores a B-Tree in host memory, and executes SCAN\nand GET operations on the FPGA, and PUT,UPDATE , and\nDELETE operations on the CPU. This approach enables large\nstores and simpliﬁes the FPGA implementation but raises\nthe challenge of data access and synchronization across slow\nPCIe. We described how Honeycomb addresses this challenge\nby using large B-Tree nodes with shortcuts; caching in on-\nchip and on-board FPGA memory; exploiting request-level\nparallelism with out-of-order execution; making SCAN and\nGET operations wait free; and using a log in B-Tree nodes to\nbatch synchronization across PCIe. Honeycomb is evalutated\nagainst a state-of-the-art ordered key-value store using both\nYCSB and a cloud-storage-inspired workload. The compar-\nison shows that Honeycomb improves throughput by 1:9\u0002\nfor uniform read-dominated workloads in YCSB and by 2:3\u0002\nfor uniform cloud-storage-inspired workloads with more than\n80% SCAN operations. Most importantly, Honeycomb im-\nproves cost-performance, which is the key metric to optimize\nin large-scale data centers; it improves throughput per watt\nof TDP (a proxy for TCO) by more than 1:5\u0002and1:9\u0002for\nthese two set of workloads respectively.\nAcknowledgments\nWe thank Vadim Makhervaks, Lukasz Tomczyk, Prahasaran\nAsokan, and Ankit Agrawal for their help and discussions on\nexploring Honeycomb in cloud storage services. We thank\nAndrew Putnam for his support and help with onboarding\nCatapult FPGA platform. We thank Nikita Lazarev, David\nSidler, Mikhail Asiatici and Fabio Maschi who worked on\nrelated projects during their internships at Microsoft Research.\nWe thank the members of Cloud System Futures team in\nMicrsoft Research Cambridge for their help and feedback.\nReferences\n[1]Github reporsitory of YCSB. https://github.com/\nbrianfrankcooper/YCSB .\n[2]Marcos K Aguilera, Kimberly Keeton, Stanko No-\nvakovic, and Sharad Singhal. Designing far memory\ndata structures: Think outside the box. In Proceedings\nof the Workshop on Hot Topics in Operating Systems ,\npages 120–126, 2019.\n[3]Mina Tahmasbi Arashloo, Alexey Lavrov, Manya\nGhobadi, Jennifer Rexford, David Walker, and David\nWentzlaff. Enabling programmable transport protocolsin high-speed nics. In 17th USENIX Symposium on Net-\nworked Systems Design and Implementation (NSDI 20) ,\npages 93–109, 2020.\n[4]Berk Atikoglu, Yuehai Xu, Eitan Frachtenberg, Song\nJiang, and Mike Paleczny. Workload analysis of a large-\nscale key-value store. In Proceedings of the 12th ACM\nSIGMETRICS/PERFORMANCE joint international con-\nference on Measurement and Modeling of Computer\nSystems , pages 53–64, 2012.\n[5]Philip A Bernstein, Vassos Hadzilacos, and Nathan\nGoodman. Concurrency control and recovery in\ndatabase systems , volume 370. Addison-wesley Read-\ning, 1987.\n[6]Marco Spaziani Brunella, Giacomo Belocchi, Marco\nBonola, Salvatore Pontarelli, Giuseppe Siracusano,\nGiuseppe Bianchi, Aniello Cammarano, Alessandro\nPalumbo, Luca Petrucci, and Roberto Bifulco. hxdp:\nEfﬁcient software packet processing on fFPGAgnics.\nIn14thfUSENIXgSymposium on Operating Systems\nDesign and Implementation ( fOSDIg20), pages 973–\n990, 2020.\n[7]Chiranjeeb Buragohain, Knut Magne Risvik, Paul Brett,\nMiguel Castro, Wonhee Cho, Joshua Cowhig, Nikolas\nGloy, Karthik Kalyanaraman, Richendra Khanna, John\nPao, et al. A1: A distributed in-memory graph database.\nInProceedings of the 2020 ACM SIGMOD International\nConference on Management of Data , pages 329–344,\n2020.\n[8]Matthew Burke, Shannon Joyner, Adriana Szekeres, Ja-\ncob Nelson, Irene Zhang, and Dan RK Ports. Prism:\nRethinking the rdma interface for distributed systems.\nInProceedings of the ACM SIGOPS 28th Symposium on\nOperating Systems Principles CD-ROM , pages 228–242,\n2021.\n[9]Brad Calder, Ju Wang, Aaron Ogus, Niranjan Nilakan-\ntan, Arild Skjolsvold, Sam McKelvie, Yikang Xu, Shash-\nwat Srivastav, Jiesheng Wu, Huseyin Simitci, et al. Win-\ndows azure storage: a highly available cloud storage\nservice with strong consistency. In Proceedings of the\nTwenty-Third ACM Symposium on Operating Systems\nPrinciples , pages 143–157, 2011.\n[10] Adrian M. Caulﬁeld, Eric S. Chung, Andrew Put-\nnam, Hari Angepat, Jeremy Fowers, Michael Haselman,\nStephen Heil, Matt Humphrey, Puneet Kaur, Joo-Young\nKim, Daniel Lo, Todd Massengill, Kalin Ovtcharov,\nMichael Papamichael, Lisa Woods, Sitaram Lanka,\nDerek Chiou, and Doug Burger. A cloud-scale accelera-\ntion architecture. In The 49th Annual IEEE/ACM Inter-\nnational Symposium on Microarchitecture , MICRO-49,\nTaipei, Taiwan, 2016. IEEE Press.\n13\n\n[11] Sai Rahul Chalamalasetti, Kevin Lim, Mitch Wright,\nAlvin AuYoung, Parthasarathy Ranganathan, and Martin\nMargala. An fpga memcached appliance. In Proceed-\nings of the ACM/SIGDA international symposium on\nField programmable gate arrays , pages 245–254, 2013.\n[12] Yanzhe Chen, Xingda Wei, Jiaxin Shi, Rong Chen, and\nHaibo Chen. Fast and general distributed transactions\nusing rdma and htm. In Proceedings of the Eleventh\nEuropean Conference on Computer Systems , pages 1–\n17, 2016.\n[13] Douglas Comer. Ubiquitous b-tree. ACM Computing\nSurveys (CSUR) , 11(2):121–137, 1979.\n[14] Redis community. redis. https://redis.io/ .\n[15] Brian F Cooper, Adam Silberstein, Erwin Tam, Raghu\nRamakrishnan, and Russell Sears. Benchmarking cloud\nserving systems with ycsb. In Proceedings of the 1st\nACM symposium on Cloud computing , pages 143–154,\n2010.\n[16] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang,\nJaeyoung Do, Yinan Li, Hantian Zhang, Badrish Chan-\ndramouli, Johannes Gehrke, Donald Kossmann, et al.\nAlex: an updatable adaptive learned index. In Proceed-\nings of the 2020 ACM SIGMOD International Confer-\nence on Management of Data , pages 969–984, 2020.\n[17] Siying Dong, Mark Callaghan, Leonidas Galanis,\nDhruba Borthakur, Tony Savor, and Michael Strum. Op-\ntimizing space ampliﬁcation in rocksdb. In CIDR , vol-\nume 3, page 3, 2017.\n[18] Aleksandar Dragojevi ´c, Dushyanth Narayanan, Ed-\nmund B Nightingale, Matthew Renzelmann, Alex\nShamis, Anirudh Badam, and Miguel Castro. No com-\npromises: Distributed transactions with consistency,\navailability, and performance. In Proceedings of the\n25th symposium on operating systems principles , pages\n54–70, 2015.\n[19] Haggai Eran, Lior Zeno, Maroun Tork, Gabi Malka, and\nMark Silberstein.fNICAg: An infrastructure for inline\nacceleration of network applications. In 2019 USENIX\nAnnual Technical Conference (USENIX ATC 19) , pages\n345–362, 2019.\n[20] Daniel Firestone.fVFPg: A virtual switch platform for\nhostfSDNgin the public cloud. In 14thfUSENIXg\nSymposium on Networked Systems Design and Imple-\nmentation (fNSDIg17), pages 315–328, 2017.\n[21] Daniel Firestone, Andrew Putnam, Hari Angepat, Derek\nChiou, Adrian Caulﬁeld, Eric Chung, Matt Humphrey,\nKalin Ovtcharov, Jitu Padhye, Doug Burger, Dave\nMaltz, Albert Greenberg, Sambhrama Mundkur, AlirezaDabagh, Mike Andrewartha, Vivek Bhanu, Harish Ku-\nmar Chandrappa, Somesh Chaturmohta, Jack Lavier,\nNorman Lam, Fengfen Liu, Gautham Popuri, Shachar\nRaindel, Tejas Sapre, Mark Shaw, Gabriel Silva, Mad-\nhan Sivakumar, Nisheeth Srivastava, Anshuman Verma,\nQasim Zuhair, Deepak Bansal, Kushagra Vaid, and\nDavid A. Maltz. Azure accelerated networking: Smart-\nnics in the public cloud. In 15th USENIX Symposium on\nNetworked Systems Design and Implementation (NSDI) ,\nApril 2018.\n[22] Jeremy Fowers, Kalin Ovtcharov, Michael Papamichael,\nTodd Massengill, Ming Liu, Daniel Lo, Shlomi Alka-\nlay, Michael Haselman, Logan Adams, Mahdi Ghandi,\nStephen Heil, Prerak Patel, Adam Sapek, Gabriel Weisz,\nLisa Woods, Sitaram Lanka, Steve Reinhardt, Adrian\nCaulﬁeld, Eric Chung, and Doug Burger. A conﬁgurable\ncloud-scale dnn processor for real-time ai. In Proceed-\nings of the 45th International Symposium on Computer\nArchitecture, 2018 . ACM, June 2018.\n[23] Dennis Heinrich, Stefan Werner, Marc Stelzner, Christo-\npher Blochwitz, Thilo Pionteck, and Sven Groppe. Hy-\nbrid fpga approach for a b+ tree in a semantic web\ndatabase system. In 2015 10th International Symposium\non Reconﬁgurable Communication-centric Systems-on-\nChip (ReCoSoC) , pages 1–8. IEEE, 2015.\n[24] Maurice Herlihy. Wait-free synchronization. ACM\nTransactions on Programming Languages and Systems\n(TOPLAS) , 13(1):124–149, 1991.\n[25] Maurice P Herlihy and Jeannette M Wing. Linearizabil-\nity: A correctness condition for concurrent objects. ACM\nTransactions on Programming Languages and Systems\n(TOPLAS) , 12(3):463–492, 1990.\n[26] Intel. Intel Quartus Prime Pro Edition User Guide:\nPower Analysis and Optimization. https://www.\nintel.com/content/www/us/en/programmable/\ndocumentation/osq1513989409475.html , 2021.\n[27] DPDK Intel. Data plane development kit, 2014.\n[28] Norman P Jouppi, Doe Hyun Yoon, Matthew Ashcraft,\nMark Gottscho, Thomas B Jablin, George Kurian, James\nLaudon, Sheng Li, Peter Ma, Xiaoyu Ma, et al. Ten\nlessons from three generations shaped google’s tpuv4i:\nIndustrial product. In 2021 ACM/IEEE 48th Annual\nInternational Symposium on Computer Architecture\n(ISCA) , pages 1–14. IEEE, 2021.\n[29] Anuj Kalia, Michael Kaminsky, and David Andersen.\nDatacenter rpcs can be general and fast. In 16th\nfUSENIXgSymposium on Networked Systems Design\nand Implementation ( fNSDIg19), pages 1–16, 2019.\n14\n\n[30] Jongyul Kim, Insu Jang, Waleed Reda, Jaeseong Im,\nMarco Canini, Dejan Kosti ´c, Youngjin Kwon, Simon\nPeter, and Emmett Witchel. Linefs. In Proceedings\nof the ACM SIGOPS 28th Symposium on Operating\nSystems Principles CD-ROM . ACM, 2021.\n[31] Jungsoo Kim and Alan Chang. Po-\nseidon V1 E1.S SSD Storage System.\nhttps://www.opencompute.org/documents/\nposeidon-v1-reference-system-spec-pdf , 2022.\n[32] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and\nNeoklis Polyzotis. The case for learned index structures.\nInProceedings of the 2018 International Conference on\nManagement of Data , pages 489–504, 2018.\n[33] Viktor Leis, Alfons Kemper, and Thomas Neumann. The\nadaptive radix tree: Artful indexing for main-memory\ndatabases. In 2013 IEEE 29th International Conference\non Data Engineering (ICDE) , pages 38–49. IEEE, 2013.\n[34] Justin J. Levandoski, David B. Lomet, and Sudipta Sen-\ngupta. The bw-tree: A b-tree for new hardware plat-\nforms. In 2013 IEEE 29th International Conference on\nData Engineering (ICDE) , pages 302–313, 2013.\n[35] Bojie Li, Zhenyuan Ruan, Wencong Xiao, Yuanwei Lu,\nYongqiang Xiong, Andrew Putnam, Enhong Chen, and\nLintao Zhang. Kv-direct: High-performance in-memory\nkey-value store with programmable nic. In Proceedings\nof the 26th Symposium on Operating Systems Principles ,\npages 137–152, 2017.\n[36] Bojie Li, Kun Tan, Layong Luo, Yanqing Peng, Ren-\nqian Luo, Ningyi Xu, Yongqiang Xiong, Peng Cheng,\nand Enhong Chen. Clicknp: Highly ﬂexible and high\nperformance network processing with reconﬁgurable\nhardware. In Proceedings of the 2016 ACM SIGCOMM\nConference , pages 1–14, 2016.\n[37] Ming Liu, Tianyi Cui, Henry Schuh, Arvind Krishna-\nmurthy, Simon Peter, and Karan Gupta. Ofﬂoading dis-\ntributed applications onto smartnics using ipipe. In\nProceedings of the ACM Special Interest Group on Data\nCommunication , pages 318–333. 2019.\n[38] Yandong Mao, Eddie Kohler, and Robert Tappan Morris.\nCache craftiness for fast multicore key-value storage. In\nProceedings of the 7th ACM european conference on\nComputer Systems , pages 183–196, 2012.\n[39] Paul E McKenney and Jonathan Walpole. What is rcu,\nfundamentally? Linux Weekly News (LWN. net) , 2007.\n[40] Mellanox. ConnectX-3 VPI Single and\nDual QSFP Port Adapter Card User Manual.\nhttps://www.mellanox.com/related-docs/user_\nmanuals/ConnectX-3_VPI_Single_and_Dual_QSFP_Port_Adapter_Card_User_Manual.pdf ,\n2013.\n[41] Christopher Mitchell, Kate Montgomery, Lamont Nel-\nson, Siddhartha Sen, and Jinyang Li. Balancing\nfCPUgand network in the cell distributed b-tree\nstore. In 2016fUSENIXgAnnual Technical Confer-\nence (fUSENIXgfATCg16), pages 451–464, 2016.\n[42] Rajesh Nishtala, Hans Fugal, Steven Grimm, Marc\nKwiatkowski, Herman Lee, Harry C Li, Ryan McElroy,\nMike Paleczny, Daniel Peek, Paul Saab, et al. Scaling\nmemcache at facebook. In 10thfUSENIXgSympo-\nsium on Networked Systems Design and Implementation\n(fNSDIg13), pages 385–398, 2013.\n[43] Phitchaya Mangpo Phothilimthana, Ming Liu, Antoine\nKaufmann, Simon Peter, Rastislav Bodik, and Thomas\nAnderson. Floem: A programming system for nic-\naccelerated network applications. In 13thfUSENIXg\nSymposium on Operating Systems Design and Imple-\nmentation (fOSDIg18), pages 663–679, 2018.\n[44] Open Compute Project. Samsung PM9A3 NVMe PCIe\nSSD. https://www.opencompute.org/products/\n262/samsung-pm9a3-nvme-pcie-ssd .\n[45] Andrew Putnam, Adrian M. Caulﬁeld, Eric S. Chung,\nDerek Chiou, Kypros Constantinides, John Demme,\nHadi Esmaeilzadeh, Jeremy Fowers, Gopi Prashanth\nGopal, Jan Gray, Michael Haselman, Scott Hauck,\nStephen Heil, Amir Hormati, Joo-Young Kim, Sitaram\nLanka, James Larus, Eric Peterson, Simon Pope, Aaron\nSmith, Jason Thong, Phillip Yi Xiao, and Doug Burger.\nA reconﬁgurable fabric for accelerating large-scale data-\ncenter services. In Proceeding of the 41st Annual Inter-\nnational Symposium on Computer Architecuture , ISCA\n’14, page 13–24, Minneapolis, Minnesota, USA, 2014.\nIEEE Press.\n[46] Yuchen Ren, Jinyu Xie, Yunhui Qiu, Hankun Lv, Wenbo\nYin, Lingli Wang, Bowei Yu, Hua Chen, Xianjun He,\nZhijian Liao, et al. A low-latency multi-version key-\nvalue store using b-tree on an fpga-cpu platform.\nIn2019 29th International Conference on Field Pro-\ngrammable Logic and Applications (FPL) , pages 321–\n325. IEEE, 2019.\n[47] Henry N Schuh, Weihao Liang, Ming Liu, Jacob Nel-\nson, and Arvind Krishnamurthy. Xenic: Smartnic-\naccelerated distributed transactions. In Proceedings\nof the ACM SIGOPS 28th Symposium on Operating\nSystems Principles , pages 740–755, 2021.\n[48] Arjun Singhvi, Aditya Akella, Maggie Anderson, Rob\nCauble, Harshad Deshmukh, Dan Gibson, Milo M. K.\nMartin, Amanda Strominger, Thomas F. Wenisch, and\n15\n\nAmin Vahdat. Cliquemap: Productionizing an rma-\nbased distributed caching system. In Proceedings of the\n2021 ACM SIGCOMM 2021 Conference , SIGCOMM\n’21, page 93–105, New York, NY , USA, 2021. Associa-\ntion for Computing Machinery.\n[49] Ross Stenfort, Ta-Yu Wu, Lee Prewitt, Paul\nKaler, David Derosa, William Lynn, and Austin\nBolen. Datacenter NVMe SSD Speciﬁcation.\nhttps://www.opencompute.org/documents/\ndatacenter-nvme-ssd-specification-v2-0r21-pdf ,\n2021.\n[50] Venkateshwaran Venkataramani, Zach Amsden, Nathan\nBronson, George Cabrera III, Prasad Chakka, Peter Di-\nmov, Hui Ding, Jack Ferris, Anthony Giardullo, Jeremy\nHoon, et al. Tao: how facebook serves the social graph.\nInProceedings of the 2012 ACM SIGMOD International\nConference on Management of Data , pages 791–792,\n2012.\n[51] Ziqi Wang, Andrew Pavlo, Hyeontaek Lim, Viktor Leis,\nHuanchen Zhang, Michael Kaminsky, and David G An-\ndersen. Building a bw-tree takes more than just buzz\nwords. In Proceedings of the 2018 International Con-\nference on Management of Data , pages 473–488, 2018.\n[52] Xingda Wei, Rong Chen, and Haibo Chen. Fast rdma-\nbased ordered key-value store using remote learned\ncache. In 14thfUSENIXgSymposium on Operating Sys-\ntems Design and Implementation ( fOSDIg20), pages\n117–135, 2020.\n[53] Haichang Yang, Zhaoshi Li, Jiawei Wang, Shouyi Yin,\nShaojun Wei, and Leibo Liu. Heterokv: A scalable line-\nrate key-value store on heterogeneous cpu-fpga plat-\nforms. In 2021 Design, Automation & Test in Europe\nConference & Exhibition (DATE) , pages 834–837. IEEE,\n2021.\n[54] Adar Zeitak and Adam Morrison. Cuckoo trie: Exploit-\ning memory-level parallelism for efﬁcient dram index-\ning. In Proceedings of the ACM SIGOPS 28th Sympo-\nsium on Operating Systems Principles , pages 147–162,\n2021.\n[55] Yibo Zhu, Haggai Eran, Daniel Firestone, Chuanxiong\nGuo, Marina Lipshteyn, Yehonatan Liron, Jitendra Pad-\nhye, Shachar Raindel, Mohamad Haj Yahia, and Ming\nZhang. Congestion control for large-scale rdma deploy-\nments. ACM SIGCOMM Computer Communication\nReview , 45(4):523–536, 2015.\n[56] Tobias Ziegler, Sumukha Tumkur Vani, Carsten Bin-\nnig, Rodrigo Fonseca, and Tim Kraska. Designingdistributed tree-based index structures for fast rdma-\ncapable networks. In Proceedings of the 2019 Inter-\nnational Conference on Management of Data , pages\n741–758, 2019.\n16",
  "textLength": 78762
}