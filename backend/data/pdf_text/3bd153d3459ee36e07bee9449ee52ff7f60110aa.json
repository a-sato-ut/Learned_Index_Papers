{
  "paperId": "3bd153d3459ee36e07bee9449ee52ff7f60110aa",
  "title": "Learning Augmented Binary Search Trees",
  "pdfPath": "3bd153d3459ee36e07bee9449ee52ff7f60110aa.pdf",
  "text": "Learning-Augmented Binary Search Trees\nHonghao Lin∗†Tian Luo‡David P. Woodru\u000b§\nAbstract\nA treap is a classic randomized binary search tree data structure that is easy to implement\nand supports O(logn) expected time access. However, classic treaps do not take advantage\nof the input distribution or patterns in the input. Given recent advances in algorithms with\npredictions, we propose pairing treaps with machine advice to form a learning-augmented treap.\nWe are the \frst to propose a learning-augmented data structure that supports binary search tree\noperations such as range-query and successor functionalities. With the assumption that we have\naccess to advice from a frequency estimation oracle, we assign learned priorities to the nodes to\nbetter improve the treap's structure. We theoretically analyze the learning-augmented treap's\nperformance under various input distributions and show that under those circumstances, our\nlearning-augmented treap has stronger guarantees than classic treaps and other classic tree-based\ndata structures. Further, we experimentally evaluate our learned treap on synthetic datasets\nand demonstrate a performance advantage over other search tree data structures. We also\npresent experiments on real world datasets with known frequency estimation oracles and show\nimprovements as well.\n1 Introduction\nQuerying ordered elements is a fundamental problem in computer science. Classically, hash tables and\nvarious tree-based data structures such as Red-Black Trees, AVL Trees, and Treaps have been used\nto e\u000eciently solve this problem. While hash tables are very e\u000ecient and extensively used in practice,\ntrees are more memory-e\u000ecient and can dynamically resize with greater convenience. Additionally,\nsearch trees can o\u000ber extra functionality over hash tables in the form of successor/predecessor,\nminimum/maximum, order statistics, and range query capabilities. In practice, self-balancing binary\nsearch trees are used in routing tables, database indexing (B-Trees), the Linux kernel (Red-Black\nTrees), and various implementations of collections, sets, and dictionaries in standard libraries.\nClassic binary search tree data structures often support these functionalities in O(logn) time.\nHowever, most binary search tree implementations, such as Red-Black Trees and AVL Trees, do\nnot leverage patterns in the data to improve performance; instead, they provide a worst-case of\nO(logn) time per access. Splay Trees are able to implicitly take advantage of the underlying input\ndistribution without any information about the distribution as they are, up to a constant factor,\nstatically optimal and conjectured to be dynamically optimal [ ST85 ]. Unfortunately, each access\nis accompanied by a series of rotations that is proportional to the number of nodes visited during\n∗Equal Contribution.\n†Computer Science Department, Carnegie Mellon University. honghaol@andrew.cmu.edu.\n‡Computer Science Department, Carnegie Mellon University. tianl1@andrew.cmu.edu.\n§Computer Science Department, Carnegie Mellon University. dwoodruf@cs.cmu.edu. Honghao Lin and David\nWoodru\u000b would like to thank for partial support from the National Science Foundation (NSF) under Grant No.\nCCF-1815840.\n1arXiv:2206.12110v1  [cs.DS]  24 Jun 2022\n\nthe access, which increases the access time by a possibly large constant factor. On the other hand,\nif the underlying distribution is known, then one can generate a statically optimal tree in O(n2)\ntime [ Knu71 ] or an approximately statically optimal tree in O(nlogn) time [ Meh71 ]; however, these\nmethods do not allow for dynamic insertion and deletion operations.\nA natural idea that arises is to use patterns in data to improve the e\u000eciency of these data\nstructures. In recent years, the \feld of learning-augmented algorithms has blossomed (see [ MV20 ]\nfor a survey). Given a predictor that can output useful properties of the dataset, we can then\nleverage these predictions to optimize the performance of the algorithm based on the predicted\npatterns. In this paper, we present a learning-augmented binary search tree that is the \frst to\nsupport e\u000ecient range queries and order statistics.\nIn summary, we present the following contributions:\n•We introduce a learning-augmented Treap structure which exploits a rank prediction oracle to\ndecrease the number of comparisons needed to \fnd an element.\n•We analyze our learning-augmented Treap and provide theoretical guarantees for various\ndistributions. We further show that our learning-augmented Treap is robust under a reasonable\namount of noise from the oracle and that it performs no worse than a random Treap when the\noracle is inaccurate, for any input distribution D, up to an additive constant.\n•We experimentally evaluate the performance of our learning-augmented Treap over synthetic\ndistributions and real world datasets. On synthetic distributions, we show improvements of\nover 25% compared to the best classical binary search trees. On real world datasets, we show\nthat the performance is comparable to the best among popular classical binary search trees\nand show signi\fcant improvements over non-learned Treaps.\n1.1 Motivation for Learning-Augmented Treaps\nIn a binary search tree, more frequently accessed items should appear closer to the root to reduce\nthe average number of comparisons in order to retrieve the item. However, Red-Black Trees, AVL\nTrees, and non-learned Treaps do not take advantage of this property, while Splay Trees exploit this\nonly to the extent that more recent items are placed near the root.\nGiven an oracle that predicts the ranks of elements, it is natural to build a tree in which the\ntop-ranked items are near the root. These oracles are indeed realistic; for example, we can use\nfrequency estimators to approximately rank the elements. Hashing-based approaches, such as\nCount-Min [ CM05 ] and Count-Sketch [ CH08 ], have been shown to be e\u000ecient and e\u000bective in this\nregard. Further, recent advances in the learning-augmented space have spurred the development of\nlearning-augmented frequency estimators [ HIKV19 ]. In our experiments, we use the trained machine\nlearning oracle from Hsu et al. [HIKV19] as our frequency estimator.\nWith the availability of such a predictor, the motivation of augmenting a classic binary search\ntree data structure with a learned oracle is clear. Red-Black Trees and AVL Trees are uniquely\ndetermined by the insertion order of the elements and while it is feasible to insert the elements in such\nan order such that the top-ranked items are near the root, it is not clear how to support insertions\nand deletions to maintain this property. On the other hand, the order of insertions matters less for\na Splay Tree, especially over a long sequence of operations, as it is self-adjusting. Our attempts\nat producing a learning-augmented Splay Tree have been unfruitful; this was largely due to the\nhigh overhead associated with rotations and di\u000eculties in determining whether to splay an element.\n2\n\nInstead of a Splay Tree, statically optimal trees could also be built with a frequency estimation\noracle; however, these trees are unable to support insertions or deletions after initialization.\nTreaps are simpler to analyze and can naturally be adapted in the learning-augmented setting.\nIndeed, Treaps are uniquely determined by the priorities of each key (given that all priorities are\nunique) and elements with higher priority appear closer to the root of tree. In this paper, we suggest\nassigning learned priorities to the Treap instead of priorities being drawn from a distribution D;\nspeci\fcally, we assign the learned frequency as the priority. With this adjustment, we are able to\ne\u000eciently support insertions and deletions, among other tree operations, while improving access\ntime. We note that in the paper that introduced Treaps [ AS89 ], a modi\fcation was suggested where\nevery time an element was accessed, the new priority of the element was set to be the maximum of\nthe old priority and a new draw from D. We are the \frst to learn the priorities using a machine\nlearning oracle.\n1.2 Related Work\nThis paper follows the long line of research in the growing \feld of algorithms with predictions.\nLearning-augmented algorithms have been applied to a variety of online algorithms, such as the\nski rental problem and job scheduling [ PSK18 ]. Further, caches [ LV20 ], Bloom \flters [ Mit19 ],\nindex structures [ KBC+18], and Count-Min and Count-Sketch [ HIKV19 ] are among the many data\nstructures that have had a learning-augmented counterpart suggested.\nIn particular, [ KBC+18] suggests replacing B-Trees (or any other index structure) with trained\nmodels for querying databases. In their work, instead of traversing the B-Tree to access the position\nof a record, they use a neural net to directly point towards the record. Our work is di\u000berent from\ntheirs since it keeps the search tree and instead optimizes the structure of the tree for faster queries;\nthrough this, we are able to support common additional tree-based operations such as traversal,\norder statistics, merge/join, etc.\nOur work uses the frequency estimation oracle trained in Hsu et al. [ HIKV19 ] on a search query\ndataset (AOL) and an IP tra\u000ec dataset. Since then, other papers have used these predictions as\nthe basis of their learned structures [ JLL+20]. Furthermore,` [ DWM21 ] has shown an improved\noracle for the IP dataset, which shows signi\fcant improvements in accuracy.\n2 Preliminaries\nWe use [n] to denote the set f1;:::;ng; further, we identify the set of keys in our binary tree with\n[n]. For a sequence of mqueries, we let ei2[n] be theithmost frequent item with frequency fi,\nbreaking ties randomly. In our analysis, we also assume that the input distribution is the same\nthroughout the duration of the query sequence and that the frequency observed in the sequence of\nmqueries exactly re\rects its true distribution, as in element eioccurs with probability pi=fi\nm. We\nwill de\fne the rank of element eito beiand the ranking ordering to be e1;:::;en.\nTreaps: Treaps are binary search trees that also hold an additional \feld per node that stores\nthe priority of that node. For node i, we denote the priority of the node to be wi. At the end of any\noperation, in addition to the binary search tree order invariant, a Treap always satis\fes the heap\ninvariant, that is, if node xis an ancestor of node y, thenwx\u0015wy. Classically, the priorities of a\nTreap are drawn from a continuous distribution so as to not have any duplicate priorities.\nInsertion in a Treap is simple. We insert the new node by attaching it as a leaf in the appropriate\n3\n\nposition (i.e., satisfying the order invariant) in the Treap. Then, we repeatedly rotate the new node\nand its parent until the heap invariant is satis\fed. Deletion is achieved by rotating the node down\nuntil it is a leaf and detaching the node; we pick the child with the greater priority to perform the\nrotation.\nWe will refer to a Treap where priorities are assigned based on the rank or frequency of the\nelement as a learned Treap, while a classic random Treap will be referred to as a random Treap.\nThroughout the analysis, we make the assumption that the rank order of the keys is random, as\nin,e1;:::;enis a random permutation of [ n]. Section 3.6 shows how to remove this assumption;\nhowever, we keep this assumption for the \frst part of the analysis for ease. Furthermore, we will\nalso assume for convenience that the frequencies of the keys are unique, as in fi=fjif and only if\ni=j. To remove this assumption, we can break ties randomly. If elements xandyhave the same\nfrequency and the tie is broken in favour of x, we will say that xhas lower rank than y.\nZip\fan Distribution: In our analysis, we analyze the performance of a Treap under the Zip\fan\ndistribution. Speci\fcally, the Zip\fan distribution with parameter \u000bhas frequencies fi=m\ni\u000bHn;\u000b\nwhereHn;\u000b=Pn\ni=11\ni\u000bis thenthgeneralized harmonic number of order \u000b.\n3 Learning Augmented Treaps\nIn the following sections, we assume access to a perfect oracle and analyze the theoretical performance\nof our learned Treaps versus random Treaps. In Section 3.4, we discuss the robustness and\nperformance guarantees of our learned Treaps when we are given a noisy oracle. In Section 3.5,\nwe explore performance guarantees of our oracle if given a less powerful oracle. Finally, in Section\n3.6 we explore a modi\fed version of the learned Treap that removes the assumption that the rank\nordering is a random permutation.\n3.1 Treap Operations\n3.1.1 Treap Initialization\nGiven a predictor that outputs the frequency rank of an element, we assign a priority equal to\nthe frequency rank of the element and insert the element into the Treap. Similarly, if we had a\nfrequency estimation oracle instead of a rank-estimation oracle, we can insert the element into the\nTreap with priority equal to the frequency estimate.\n3.1.2 Access\nWe present the following theorem that bounds the expected depth of eiin a learned Treap.\nTheorem 3.1. The expected depth of eiin a learned Treap is 2Hi\u00001, whereHiis thei-th Harmonic\nnumber.\nProof. Consider the set of elements with higher priority, i.e., S=fekjk\u0014ig. Notice that only\nelements in Scan be ancestors of eiandeicannot be an ancestor of any element in S. Since\ne1;:::;enis a random permutation of [ n],e1;:::;eiis a random permutation of fe1;:::;eig.\nUnder these assumptions, the number of comparisons needed to access eiis equivalent to the\nnumber of comparisons needed to correctly insert a random element x2[i] in a sorted array of\n4\n\nelements [i]nfxgusing binary search, where pivots are chosen randomly. Here, the pivots are\nanalogous to the ancestors of ei.\nThis motivates the following recurrence for computing the expected depth of ei:\nT(i) =(\n1 i= 1\n1 +2\ni\u00001Pi\u00001\nk=1\u0000k\niT(k)\u0001\notherwise\nwhich simpli\fes to\nT(i) =(\n1 i= 1\n2\ni+T(i\u00001) otherwise\nThis recurrence evaluates to T(i) = 2Hi\u00001.\nTheorem 3.2. The expected depth of eiin a learned Treap is O(logi)with high probability.\nProof. Again, we analyze the depth of element eiby examining the number of comparisons needed\nto insert a random element x2[i] in a sorted array of [ i]nxuse random binary search. We employ\na similar technique to the classic high probability analysis of QuickSort.\nSuppose on iteration k, the size of the array being searched is Xk. With probability1\n2, the\nrandomized pivot is situated in the range [1\n4Xk;3\n4Xk]. In this case, Xk+1\u00143\n4Xk. Otherwise, if the\npivot does not land in this range, we know that Xk+1\u0014Xktrivially. We get the following:\nE[Xk]\u00141\n2Xk\u00001+3\n8Xk\u00001=7\n8Xk\u00001\nSinceX0\u0014i, we have\nE[Xk]\u0014\u00127\n8\u0013k\nX0\u0014\u00127\n8\u0013k\ni\nThe probability that the randomized binary search uses more than kiterations is exactly PrfXk\u00151g.\nUsing Markov's Inequality and setting k=clog 8\n7ifor some constant cgives\nPrfXk\u00151g\u0014E[Xk]\u00141\nic\u00001\nTherefore, setting c\u00152 implies that the expected depth of eiisO(logi) with high probability.\nTheorem 3.3. With constant probability, for every i,eihas depthO(logi). In other words, the\nentire tree is well-balanced.\nProof. Again, letXibe the depth of element ei. Notice that X1andX2are 1 and 2, respectively,\nwith probability 1. From Theorem 3.1, for i\u00153,Xi\u0014O(logi) with probability at most1\nic\u00001for\nsome constant c. Applying a union bound over elements Xifori\u00153 gives\nPrfn[\ni=3Xi\u0014O(logi)g\u0014nX\ni=31\nic\u00001\nForc= 3,Pn\ni=31\nic\u00001\u0014\u00192\n6\u00001:25\u00190:39.\n5\n\n3.1.3 Insertion/Deletion and Priority Updates\nCorollary 3.4. The expected time of an insertion, deletion or priority update is O(logn).\nProof. Suppose during the insertion process of a node x, we attach xto nodeyas a leaf. Then\nby Theorem 3.1, the depth of yisO(logn) in expectation. Similarly, suppose during the deletion\nprocess of node x, we detach node xwhen it is a child of node y. By Theorem 3.1, the time it takes\nisO(logn).\nSimilarly, a priority update takes at most O(logn) time.\n3.1.4 Other Operations\nOur learned Treap could also be optimized for other tree-based operations. Under these modi\fcations,\nthe following operations could be supported by a learned Treap with time similar to that of an\naccess on a learned Treap.\nRange Queries: Consider the simple operation of counting the number of elements between\nkeysxandy. This operation is commonly implemented by augmenting every node with a \feld that\nstores the size of the subtree rooted at the node. Counting the number of elements in the ranges\nreduces to traversing from the root to xand from the root to y, which is similar to the process of\naccessingxandy. Thus, the predictor can learn the frequency distribution of the boundaries of the\nrange query to optimize our Treap. Other such operations may include the standard RangeSum\noperation, which outputs the sum of the values stored in each key of the tree.\nSuccessor/Predecessor: On a query to the successor of key x, the output would be the\nsmallest key greater than x. Among the many ways to implement this functionality, a simple way is\nto keep a pointer that points to the successor/predecessor. When \fnding the successor of key x, we\nsimply access xin the Treap and use the stored pointer to access the successor. Our predictor can\nlearn the frequency distribution of successor queries to xand set the priority of xaccordingly in the\nlearned Treap.\nWhen supporting this operation, insertion and deletion become more complicated. When\ninserting element x, we must change the pointers of both the successor and predecessor of x\naccordingly; however, this requires at most a constant number of pointer changes.\n3.2 General Distributions\nIn this section, we analyze the expected cost per access of a learned Treap and a random Treap for\nan arbitrary frequency distribution D.\nLemma 3.5. The expected cost of a single access on a learned Treap isPn\ni=1pi(2Hi\u00001).\nProof. This follows immediately from Theorem 3.1.\nSince the expected cost of a single access is known to be at most O(logn) [AS89 ], we provide a\nlower bound on this expectation.\nTheorem 3.6. The expected cost of a single access on a random Treap is at least 2Hn+1\u00004for\nany frequency distribution.\n6\n\nProof. The expected depth of key iis well-known to be Hi+Hn\u0000i+1\u00002 [AS89 ]. LetXbe the\ndepth of an access and let Xijbe the depth of key iif it is thejth-ranked item, and 0 otherwise.\nE[X] =nX\ni=11\nnnX\nj=1E[Xij]\n=nX\ni=11\nnnX\nj=1pj(Hi+Hn\u0000i+1\u00002)\n=nX\ni=11\nn(Hi+Hn\u0000i+1\u00002)\n=2\nnnX\ni=1Hi\u00001\n=2\nn((n+ 1)Hn+1\u00002n)\n>2Hn+1\u00004\n3.3 Zip\fan Distributions\nIn this section, we analyze the expected cost of an access of a learned Treap where pi/1\ni\u000bfor a\nparameter\u000b.\nTheorem 3.7. The expected cost of a single access on a learned Treap isPn\ni=11\ni\u000bHn;\u000b(2Hi\u00001).\nProof. From Lemma 3.5, it is immediate that the expected cost isPn\ni=11\ni\u000bHn;\u000b(2Hi\u00001).\nLemma 3.8. The expected cost of an access for \u000b= 1is at mostHn.\nProof. For\u000b= 1, the expected cost isPn\ni=11\niHn(2Hi\u00001) by Theorem 3.7.\nConsider the sum C=Pn\ni=11\ni(Hi). Observe by expanding this summation that it evaluates\nto1\n2((Hn)2+Hn;2). The expected cost can then be expressed as2C\nHn\u00001 =Hn+Hn;2\n2Hn\u00001:This\napproaches Hn\u00001 asnincreases.\nCorollary 3.9. The expected cost of an access for a learned Treap on a Zip\fan distribution with\nparameter\u000b= 1is approximately a factor 2less than that of an access on a random Treap.\nLemma 3.10. The expected cost of an access for \u000b>1is constant.\nProof. For\u000b>1, the expected cost isPn\ni=11\ni\u000bHn;\u000b(2Hi\u00001) by Theorem 3.7.\nConsider the series ai=Hi\ni\"andbi=1\ni\u000b\u0000\"for some\">0. Observe the following properties:\n•SinceHn\u0014ln(n) + 1, lim n!1an= 0. Further,fangis monotonically decreasing for large n.\n•Since\u000b>1, there exists \"such that\u000b\u0000\">1 and thus,Pn\ni=1bi\u0014cfor some constant c.\nRecall Dirichlet's test: if fangis a monotonically decreasing sequence whose limit approaches 0\nandfbngis a sequence such thatP1\ni=1biis bounded by a constant, thenP1\ni=1aibiconverges as\nwell.\n7\n\nBy these two observations and using Dirichlet's test,Pn\ni=1anbn=Pn\ni=1Hn\nn\u000bconverges to a\nconstant. The expected cost here is2\nHn;\u000b(Pn\ni=1anbn)\u00001. Therefore, the expected cost is bounded\nfrom above by a constant.\nTheorem 3.11. The learned Treap is statically optimal in expectation for \u000b\u00151.\nProof. First, consider \u000b= 1. The Shannon entropy, H, of this distribution is an asymptotic lower\nbound for a statically optimal tree, namely, Mehlhorn shows that for any binary tree, the weighted\npath length must be at leastH\n3[Meh71 ]. The Shannon entropy for the Zip\fan distribution with\n\u000b= 1 is\nnX\ni=0\u0000pilog (pi) =nX\ni=01\niHnlog (iHn)\n=nX\ni=01\niHn(log (i) + log(Hn))\u0015nX\ni=01\niHnlog (i)\nClearly, this is within a constant factor of the expected cost of our learned Treap. Since the expected\ncost for the learned Treap is within a constant factor of the Shannon entropy, we are statically\noptimal up to a constant factor.\nFor\u000b > 1, the expected cost is constant; therefore, it is immediate that we are at most a\nconstant factor more than the statically optimal binary search tree.\n3.4 Noisy Oracles and Robustness to Errors\nIn this section, we will prove that given an accurate rank prediction oracle subject to a reasonable\namount of noise and error, our learned Treap's performance matches that of a perfect rank prediction\noracle up to an additive constant per access.\nGiven element i, letribe the real rank of iand let ^ribe the predicted rank of i. We will call an\noracle noisy if ^ ri\u0014\"r+\u000efor some constants \";\u000e\u00151.\nTheorem 3.12. Using predictions from a noisy oracle, the learned Treap's performance matches\nthat of a learned Treap with a perfect oracle up to an additive constant.\nProof. The expected cost of a single access on the learned Treap with a noisy oracle is at mostPn\ni=1pi(2H\"i+\u000e\u00001):\nThe di\u000berence between the expected cost of a learned Treap with a noisy oracle and a learned\nTreap with a perfect oracle isPn\ni=12pi(H\"i+\u000e\u0000Hi):\nUsing that ln(n)\u0014Hn\u0014ln(n) + 1 for the nthHarmonic number Hn, the di\u000berence is at mostPn\ni=12pi\u0000\n1 + ln\u0000\n\"+\u000e\ni\u0001\u0001\n\u0014Pn\ni=12pi(1 + ln (\"+\u000e))= 2(1 + ln (\"+\u000e))\u0014c;for some constant c.\nTherefore, under a noisy oracle, the learned Treap is at most an additive constant worse than a\nlearned Treap with a perfect oracle.\nWe remark that for frequency estimation oracles, it might be natural to consider an error bound\nof1\n\u0001fi\u0014^fi\u0014\u0001fiinstead; however, if the underlying distribution is Zip\fan, a frequency estimation\nerror bound of1\n\u0001fi\u0014^fi\u0014\u0001fiis equivalent to a rank estimation error bound of ^ ri2ri\u0006\u00012.\nWe will call an oracle inaccurate if there exist no constants \";\u000e\u00151 such that ^ri\u0014\"r+\u000e. Further,\nwe will de\fne the notion of an adversarial oracle as an oracle that outputs a rank ordering that is\n8\n\nadversarial; more speci\fcally, given a distribution Dwith a random rank ordering, a non-adversarial\noracle would output a random rank ordering that is not necessarily the same as the rank ordering\nofD.\nTheorem 3.13. A learned Treap based on an inaccurate but non-adversarial oracle has expected\nperformance no worse than that of a random Treap, up to a small additive constant.\nProof. Since the oracle is non-adversarial, the expected depth of any element is still bounded by\n2Hn\u00001 by Theorem 3.1. Therefore, the expected cost is at mostPn\ni=1pi(2Hn\u00001) = 2Hn\u00001:\n3.5 Oracles with Limited Capabilities\nIn certain circumstances, it may be impossible or inconvenient to obtain an oracle that predicts the\nfull rank ordering of the elements. Instead, it may be easier to obtain an oracle that predicts the\ntopkelements only.\nIn this case, we will assign the top kelements random positive real-valued priorities and the\nremaining elements will be assigned random negative real-valued priorities. Hence, the top k\nelements are ancestors of the remaining elements. Again, here, we will assume that the underlying\nrank ordering is a random permutation of [ n]. Further, suppose that the top kitems account for p\npercent of the queries.\nTheorem 3.14. With an oracle that predicts only the top kelements, the expected depth of an\naccess is at most 2(pHk+ (1\u0000p)Hn)\u00001.\nProof. For the top kelements, the expected depth is at most 2 Hk\u00001 and for the rest of the\nelements, the expected depth is at most 2 Hn\u00001. Therefore, the expected depth of an access is at\nmost 2(pHk+ (1\u0000p)Hn)\u00001.\nFor smallkand signi\fcant p, this results in a large constant factor reduction in expected access\ndepth. Similarly, if we were given an oracle that can only accurately predict the frequencies of the\ntopkitems, we can assign priorities of the top kitems to the frequency and assign random negative\nreal-valued priorities to the remaining n\u0000kitems.\n3.6 Removing Assumption of Random Rank Ordering\nIn real world datasets, it might not be the case that the rank ordering is a random permutation. For\nexample, in search queries, certain queries are lexicographically close to misspelled versions of the\nquery; however, misspelled versions of the query have a signi\fcantly reduced frequency compared to\nthe correctly spelled query. Furthermore, it may be the case that the oracle is adversarial. In this\ncase, we would want to remove the assumption that the rank ordering is a random permutation.\nOne natural idea is to map the identities of the elements to a random real number. For key\ni, we will use sito denote this random real. The idea is to use a random Treap (or any other\nself-balancing binary search tree) and a learned Treap together. The random Treap will use the\nactual identity of the element as the key and the learned Treap will use the random real as the key.\nFor each node in the learned Treap, we keep a pointer to the corresponding node in the random\nTreap. It immediately follows that the rank ordering on the keys of the learned Treap is equivalent\nto a random permutation. Furthermore, we keep a map that maps the identity of the element to its\ncorresponding random real. We show an example of this modi\fed learned treap in Figure 1.\n9\n\n5\n2\n1 36\n47s7\ns3\ns5s2s4\ns1s6\nFigure 1: An example of the learned Treap modi\fcation. White nodes form the learned Treap and\ngrey nodes form the random Treap. The red arrows are the pointers from nodes in the learned\nTreap to the corresponding node in the random Treap. One possible assignment of [ s1;:::;s 7] for\nthis Treap could be [1 ;5;3;6;2;7;4].\nWe describe each tree operation below:\nAccess: For an access operation to element i, we query siin the learned Treap and use the\npointer to access element iin the random Treap.\nInsertion: To insert element i, we generate siand storesiin our map. Then we insert iinto\nthe random Treap with a random priority and insert siinto the learned Treap with the learned\npriority. We set the pointer in the node containing sito point to i.\nDeletions: To delete element i, we delete ifrom the random Treap, sifrom the learned Treap,\nand remove iandsifrom the map.\nSuccessor/Predecessor: To support successor and predecessor functionalities, we apply the\nsame technique as described in Section 3.1.4 on the random Treap.\nUnfortunately, under this modi\fcation, there is no easy method of optimizing for range queries;\nhowever we note that this operation still takes at most O(logn) time in expectation because this is\nthe expected sum of depths of the two nodes that we access. The main issue arises from the fact\nthat range queries require access to the path from the root to the queried node on the random Treap;\nhowever, to remove the random rank ordering, we intentionally circumvent this path by traversing\nthe learned Treap instead. For all accesses and successor/predecessor operations, we increase the\ncost of an operation by at most an additive constant related to accessing the map and a constant\namount of pointer accesses. For insertions and deletions, we maintain the expected O(logn) bound\nsince every node has expected depth at most O(logn).\nIn practice, there might be a desire to avoid implementing a map; instead, using a hash function\nto implicitly store the map may be a more attractive alternative. We will show that using a 4-wise\nindependent hash function with range (0 ;1) would su\u000ece. We choose to implement the hash function\ninpoly(n) precision so that with high probability, there are no collisions and such a hash function\nrequiresO(logn) bits to store and only increases the cost of operations by at most an additive\nconstant.\nTheorem 3.15. Givensi=h(ei)wherehis drawn from a 4-wise universal hash family with range\n(0;1), the expected depth of siisO(logi).\nTo achieve this, the following observation is crucial.\nFact 3.16. Suppose that sjis an ancestor of siwherej < i . Then in the ordering of fsijx2\nf1;:::;j;igg,siandsjare adjacent.\n10\n\nProof of Theorem 3.15. Since the priorities of each key do not change, only elements in fe1;:::;ei\u00001g\ncan potentially be ancestors of ei. We proceed with an analysis similar to Knudsen and St ockel's\n([KS15]) analysis of quicksort under limited independence.\nFrom Lemma 4 of [ KS15 ], we have the following lemma: given hash function h:X\u0000 !(0;1)\ndrawn from a 4-universal hash family and disjoint sets A;B\u0012XwithjAj\u0014jBj, then\nE[jfa2Ajh(a)\u0014min\nb2Bh(b)gj] =O(1):\nSimilarly, E[jfa2Ajh(a)\u0015maxb2Bh(b)gj] =O(1).\nConsider the set Sj=fsjj1\u0014j\u0014i\u00001g. From Fact 3.16 we get that if sjis an ancestor of si\nfor somej <i , then for all j0<j,sj0<minfsi;sjgorsj0>maxfsi;sjg.\nFork= 1;2;:::;logi, de\fne\nBk= [2k\u00001] andAk=\u0010\n[2k]\\[i]\u0011\n=[2k\u00001]:\nSuppose that sjis an ancestor of sifor somej2Ak. Without loss of generality, we assume\nthatsj<si. Then we have that for each j02Bk,sj0<sjorsj0>si. Consider the hash function\nH:X\u0000 !(\u0000(1\u0000si);si) such that H(x) =h(x) ifh(x)<siandH(x) =h(x)\u00001 ifh(x)>si. Notice\nthatHis also a 4-wise independent hash function. This implies that H(j)>maxb2BkH(b). From\nthe lemma above, there are an expected O(1) such elements in Akand since there are only O(logi)\nvalues ofkfor whichAkis non-empty, it follows immediately by linearity of expectation that the\nexpected number of ancestors of siisO(logi) and thus, the expected depth of eiisO(logi).\n4 Experiments\nIn this section, we present experimental results that compare the performance of our learned Treap\nto classical self-balancing binary search tree data structures. Speci\fcally, we examined Red-Black\nTrees, AVL Trees, Splay Trees, B-Trees of order 3, and random Treaps. For binary search trees\nsensitive to insertion order, we insert all keys in a random order. For these experiments, we only\nconsider query operations and report the total number of comparisons made by each data structure.\nWe note that although the number of comparisons is not a precise measurement of actual runtime,\nwith the exception of Splay Trees, traversing the tree is extremely similar across all data structures,\nand for all data structures tested except B-Trees, the number of comparisons is exactly the access\ndepth. For Splay Trees, we can expect a constant factor more in actual runtime due to the rotations\ninvolved.\n4.1 Synthetic Datasets\nWe consider synthetic datasets where elements appear according to a Zip\fan distribution with\nparameter \u000b. As with section 3, we assume that the rank order of the elements is a random\npermutation. For each experiment, we consider a sequence of length 105.\nWe report experimental results where we vary n, the number of keys, for \u000b= 1 in Figure 2 and\nfor\u000b= 1:25 in Figure 3.\nNotice that for both \u000b= 1 and\u000b= 1:25, the learned Treap performs approximately 25% better\nthan Splay Trees and a bit over 30% better than AVL and Red-Black Trees in terms of the number\n11\n\nFigure 2: Total number of\ncomparisons of classical binary\nsearch tree data structures and\nthe learned Treap on the Zip-\n\fan Distribution with parame-\nter\u000b= 1\nFigure 3: Total number of\ncomparisons of classical binary\nsearch tree data structures and\nthe learned Treap on the Zip-\n\fan Distribution with parame-\nter\u000b= 1:25.\nFigure 4: Total number of\ncomparisons of Splay Tree and\nlearned Treaps for varying Zip-\n\fan parameter \u000b.\nof comparisons. For \u000b= 1, the factor-2 savings shown in Corollary 3.9 is exhibited and for \u000b= 1:25,\nwe can see that the cost of an access is constant, as shown in Lemma 3.10.\nIn Figure 4, we show the e\u000bects of varying \u000b; in this set of experiments, we \fx the number of\nkeys to be 104and only show results for the statically optimal trees, as in Splay Trees and learned\nTreaps. The learned Treap performs between approximately 27% \u000051% better than the Splay Tree.\nThe greatest improvement was at \u000b= 3 and the least improvement was observed when \u000b= 1.\n4.2 Real World Datasets\nIn this section, we used machine learning models trained by [ HIKV19 ] as our frequency estimation\noracle. We present 4 versions of our learned Treap. We consider the performance of our learned\nTreap with the trained frequency estimation oracle and with a perfect oracle; for both of these\ninstances, we also test the performance if we remapped the keys to a random permutation (i.e.,\nsimilar to the idea of Section 3.6). We call the remapped versions of the learned Treap \\shu\u000fed\".\nTo make the data more presentable, among classical binary search tree data structures, we only\nshow the results of Red-Black Trees and Treaps; we remark that the relative performance of all\nclassical binary search tree data structures in these datasets was similar to that in the synthetic\ndatasets.\n4.2.1 Internet Tra\u000ec Data\nVarious forms of self-balancing binary search trees and skip lists have been suggested to be used in\nrouting tables [ Skl91 ]. In this experiment, we measure the performance of the binary search trees if\nwe had to query every packet in the internet tra\u000ec logs.\nData: The internet tra\u000ec data was collected by CAIDA using a commercial backbone link (Tier\n1 ISP) [ CAI16 ]. Following [ HIKV19 ], we used the internet tra\u000ec recorded from Chicago outgoing\nto Seattle recorded on 2016-01-21 13:00-14:00 UTC. Each minute recorded approximately 30 million\nand 1 million unique \rows.\nModel: We used the prediction made by [ HIKV19 ]. In their paper, an RNN was used to encode\nthe source and destination IP addresses, ports, and protocol type, and a separate RNN was used to\n12\n\nFigure 5: Total number of comparisons of\nRed-Black Trees, random Treaps, and learned\nTreaps on the 20thtest minute\nFigure 6: Total number of comparisons of\nRed-Black Trees, random Treaps, and learned\nTreaps on the 50thday.\npredict the number of packets from the tra\u000ec \row based on the encoding. The \frst 7 minutes of\nthe dataset was used as training sets with the next 2 minutes used as the validation sets. The rest\nof the dataset was used for testing. See Hsu et al. [HIKV19] for details.\nResults: In Figure 5, we plot the performances of the various data structures. We consider\nthree variants of the dataset: a subset with the top 33% of the most frequent queries, a subset of\nthe top 50% of the most frequent queries, and the full dataset. We show the results on the 20thtest\nminute (2016-01-21 13:29 UTC).\nIn all cases, the shu\u000fed versions of the learned Treap perform signi\fcantly better than that of\nthe non-shu\u000fed versions, and the learned Treaps perform better than random Treaps. We note\nthat using the oracle from [ HIKV19 ], we are unable to beat Red-Black Trees; however, the shu\u000fed\nlearned Treap with the learned oracle is comparable and with a better oracle, it could be possible to\noutperform a Red-Black Tree.\n4.2.2 Search Query Data\nData: This dataset contains approximately 21 million queries on AOL collected over 90 days in\n2006. The distribution follows Zipf's Law (see Hsu et al. [HIKV19]).\nModel: Again, we use the predictions from [ HIKV19 ]. They use an RNN with LSTM cells to\nencode the queries character by character. The encoding is then fed into a fully connected layer to\npredict the frequency of each query. The \frst 5 days were used for training while the 6thday was\nused as the validation set.\nResults: As with the Internet tra\u000ec dataset, we show the performance of the learned Treaps, a\nRed-Black Tree, and a random Treap in Figure 6. For this dataset, we consider the top 1%, 2%,\nand 5% of the most frequent queries as our set of keys. We show the results for the 50thday.\nSimilar to the internet tra\u000ec dataset, the shu\u000fed version of the learned Treaps performed better\nand all learned Treaps performed better than the random Treap. For this dataset, the shu\u000fed\nlearned Treap with the frequency estimator from [ HIKV19 ] performed well and is comparable to the\nperformance of a Red-Black Tree. Furthermore, unlike the internet tra\u000ec dataset, the performance\n13\n\nof the learned Treaps with the machine learning model was close to that of the learned Treap with\na perfect oracle.\nFigure 7: Performance of learned Treap under oracles with di\u000berent errors\n4.3 Performance under Oracles with Di\u000berent Errors\nIn this section, we study the performance of the learned Treap under oracles with certain errors\non both synthetic and real-world data. In Figure 7 we show experimental results on synthetic and\nreal-world data that show a graceful degradation as error grows. Here the prediction, ^fi, of the\nfrequency satis\fes ^fi\u0014\u0001fi. We note that if the underlying distribution is Zip\fan, then the error\nbounds for the rank-estimation oracle are stronger than the bounds for a frequency estimation\noracle; if a given frequency estimation oracle has the error bound of1\n\u0001fi\u0014^fi\u0014\u0001fi, then under a\nZip\fan distribution with \u000b\u00151, then ^ri2ri\u0006\u00012.\n5 Conclusion\nWe introduced the concept of learning-augmented algorithms into the class of binary search tree\ndata structures that support additional operations beyond B-trees. The learned Treap is able to\nsupport various useful tree-based operations, such as range-queries, successor/predecessor, and order\nstatistic queries and can be optimized for such operations. We proved that the learned Treap is\nrobust under rank-estimation oracles with reasonable error and under modi\fcations, is no worse\nthan a random Treap regardless of the accuracy of the oracle and the underlying input distribution.\nFurther, we presented experimental evidence that suggests a learned Treap may be useful in practice.\nIn the future, it may be interesting to explore whether advanced tree data structures, such as van\nEmde Boas Trees or Biased Skip Lists, can also bene\ft from machine learning techniques.\nReferences\n[AS89] C. R. Aragon and R. G. Seidel. Randomized search trees. In Proceedings of the 30th\nAnnual Symposium on Foundations of Computer Science , SFCS '89, page 540{545, USA,\n1989. IEEE Computer Society.\n[CAI16] CAIDA. The caida ucsd anonymized internet traces - 2016, 2016.\n[CH08] Graham Cormode and Marios Hadjieleftheriou. Finding frequent items in data streams.\nProc. VLDB Endow. , 1:1530{1541, 2008.\n14\n\n[CM05] Graham Cormode and S. Muthukrishnan. An improved data stream summary: the\ncount-min sketch and its applications. Journal of Algorithms , 55(1):58{75, 2005.\n[DWM21] Elbert Du, Franklyn Wang, and Michael Mitzenmacher. Putting the \\learning\" into\nlearning-augmented algorithms for frequency estimation. In Marina Meila and Tong\nZhang, editors, Proceedings of the 38th International Conference on Machine Learning ,\nvolume 139 of Proceedings of Machine Learning Research , pages 2860{2869. PMLR,\n18{24 Jul 2021.\n[HIKV19] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency\nestimation algorithms. In International Conference on Learning Representations , 2019.\n[JLL+20]Tanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, and David P. Woodru\u000b. Learning-\naugmented data stream algorithms. In International Conference on Learning Represen-\ntations , 2020.\n[KBC+18]Tim Kraska, Alex Beutel, Ed H. Chi, Je\u000brey Dean, and Neoklis Polyzotis. The case\nfor learned index structures. In Proceedings of the 2018 International Conference\non Management of Data , SIGMOD '18, page 489{504, New York, NY, USA, 2018.\nAssociation for Computing Machinery.\n[Knu71] D. E. Knuth. Optimum binary search trees. Acta Inf. , 1(1):14{25, mar 1971.\n[KS15] Mathias B\u001ak Tejs Knudsen and Morten St ockel. Quicksort, largest bucket, and min-\nwise hashing with limited independence. In Nikhil Bansal and Irene Finocchi, editors,\nAlgorithms - ESA 2015 - 23rd Annual European Symposium, Patras, Greece, September\n14-16, 2015, Proceedings , volume 9294 of Lecture Notes in Computer Science , pages\n828{839. Springer, 2015.\n[LV20] Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned\nadvice, 2020.\n[Meh71] Kurt Mehlhorn. Nearly optimal binary search trees. Acta Informatica, v.5, 287-295\n(1975) , 5, 01 1971.\n[Mit19] Michael Mitzenmacher. A model for learned bloom \flters, and optimizing by sandwiching.\narXiv preprint arXiv:1901.00902 , 2019.\n[MV20] Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. arXiv\npreprint arXiv:2006.09123 , 2020.\n[PSK18] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml\npredictions. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems , volume 31.\nCurran Associates, Inc., 2018.\n[Skl91] Keith Sklower. A tree-based packet routing table for berkeley unix. In USENIX Winter ,\n1991.\n[ST85] Daniel Dominic Sleator and Robert Endre Tarjan. Self-adjusting binary search trees. J.\nACM , 32(3):652{686, jul 1985.\n15",
  "textLength": 41034
}