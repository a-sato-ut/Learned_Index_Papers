{
  "paperId": "ac08fa76038762f89a0f6c39ea38655422926408",
  "title": "Towards Building Autonomous Data Services on Azure",
  "pdfPath": "ac08fa76038762f89a0f6c39ea38655422926408.pdf",
  "text": "Towards Building Autonomous Data Services on Azure\nYiwen Zhu\nMicrosoft, USA\nyiwzh@microsoft.comYuanyuan Tian\nMicrosoft, USA\nyuanyuantian@microsoft.comJoyce Cahoon\nMicrosoft, USA\njcahoon@microsoft.comSubru Krishnan\nMicrosoft, USA\nsubru@microsoft.com\nAnkita Agarwal\nMicrosoft, USA\nankiagar@microsoft.comRana Alotaibi\nMicrosoft, USA\nranaalotaibi@microsoft.comJesús\nCamacho-Rodríguez\nMicrosoft, USA\njesusca@microsoft.comBibin Chundatt\nMicrosoft, India\nbibin.chundatt@microsoft.com\nAndrew Chung\nMicrosoft, China\nandchung@microsoft.comNiharika Dutta\nMicrosoft, USA\nnidutta@microsoft.comAndrew Fogarty\nMicrosoft, USA\nanfog@microsoft.comAnja Gruenheid\nMicrosoft, USA\nagruenheid@microsoft.com\nBrandon Haynes\nMicrosoft, USA\nbrhaynes@microsoft.comMatteo Interlandi\nMicrosoft, USA\nmainterl@microsoft.comMinu Iyer\nMicrosoft, USA\nminu.iyer@microsoft.comNick Jurgens\nMicrosoft, USA\nnicholas.jurgens@microsoft.com\nSumeet Khushalani\nMicrosoft, USA\nsukhusha@microsoft.comBrian Kroth\nMicrosoft, USA\nbpkroth@microsoft.comManoj Kumar\nMicrosoft, India\nmanok@microsoft.comJyoti Leeka\nMicrosoft, USA\nJyoti.Leeka@microsoft.com\nSergiy Matusevych\nMicrosoft, USA\nsergiym@microsoft.comMinni Mittal\nMicrosoft, India\nminni.mittal@microsoft.comAndreas Mueller\nMicrosoft, USA\namueller@microsoft.comKartheek Muthyala\nMicrosoft, USA\nkamuth@microsoft.com\nHarsha Nagulapalli\nMicrosoft, USA\nhanagula@microsoft.comYoonjae Park\nMicrosoft, USA\nyoonjae.park@microsoft.comHiren Patel\nMicrosoft, USA\nhirenp@microsoft.comAnna Pavlenko\nMicrosoft, USA\nanna.pavlenko@microsoft.com\nOlga Poppe\nMicrosoft, USA\nolpoppe@microsoft.comSanthosh Ravindran\nMicrosoft, USA\nsanthosh.ravindran@\nmicrosoft.comKarla Saur\nMicrosoft, USA\nkasaur@microsoft.comRathijit Sen\nMicrosoft, USA\nrathijit.sen@microsoft.com\nSteve Suh\nMicrosoft, USA\nstsuh@microsoft.comArijit Tarafdar\nMicrosoft, USA\narijitt@microsoft.comKunal Waghray\nMicrosoft, USA\nkunalwaghray@microsoft.comDemin Wang\nMicrosoft, USA\ndeminw@microsoft.com\nCarlo Curino\nMicrosoft, USA\nccurino@microsoft.comRaghu Ramakrishnan\nMicrosoft, USA\nraghu@microsoft.com\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGMOD-Companion ’23, June 18–23, 2023, Seattle, WA, USA\n©2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-9507-6/23/06. . . $15.00\nhttps://doi.org/10.1145/3555041.3589674ABSTRACT\nModern cloud has turned data services into easily accessible com-\nmodities. With just a few clicks, users are now able to access a\ncatalog of data processing systems for a wide range of tasks. How-\never, the cloud brings in both complexity and opportunity. While\ncloud users can quickly start an application by using various data\nservices, it can be difficult to configure and optimize these services\nto gain the most value from them. For cloud providers, managingarXiv:2405.01813v1  [cs.DC]  3 May 2024\n\nSIGMOD-Companion ’23, June 18–23, 2023, Seattle, WA, USA Yiwen Zhu et al.\nevery aspect of an ever-increasing set of data services, while meet-\ning customer SLAs and minimizing operational cost is becoming\nmore challenging. Cloud technology enables the collection of sig-\nnificant amounts of workload traces and system telemetry. With\nthe progress in data science (DS) and machine learning (ML), it is\nfeasible and desirable to utilize a data-driven, ML-based approach to\nautomate various aspects of data services, resulting in the creation\nofautonomous data services . This paper presents our perspectives\nand insights on creating autonomous data services on Azure. It also\ncovers the future endeavors we plan to undertake and unresolved\nissues that still need attention.\nCCS CONCEPTS\n•Information systems →Autonomous database administra-\ntion;•Computer systems organization →Cloud computing .\nKEYWORDS\ncloud, data service, autonomous data service, self-driving data ser-\nvice, artificial intelligence (AI), machine learning (ML), data science\n(DS), ML for system, cloud infrastructure, query engine\nACM Reference Format:\nYiwen Zhu, Yuanyuan Tian, Joyce Cahoon, Subru Krishnan, Ankita Agarwal,\nRana Alotaibi, Jesús Camacho-Rodríguez, Bibin Chundatt, Andrew Chung,\nNiharika Dutta, Andrew Fogarty, Anja Gruenheid, Brandon Haynes, Matteo\nInterlandi, Minu Iyer, Nick Jurgens, Sumeet Khushalani, Brian Kroth, Manoj\nKumar, Jyoti Leeka, Sergiy Matusevych, Minni Mittal, Andreas Mueller,\nKartheek Muthyala, Harsha Nagulapalli, Yoonjae Park, Hiren Patel, Anna\nPavlenko, Olga Poppe, Santhosh Ravindran, Karla Saur, Rathijit Sen, Steve\nSuh, Arijit Tarafdar, Kunal Waghray, Demin Wang, Carlo Curino, and Raghu\nRamakrishnan. 2023. Towards Building Autonomous Data Services on Azure.\nInCompanion of the 2023 International Conference on Management of Data\n(SIGMOD-Companion ’23), June 18–23, 2023, Seattle, WA, USA. ACM, Seattle,\nWA, USA, 8 pages. https://doi.org/10.1145/3555041.3589674\n1 INTRODUCTION\nModern cloud has made access to various data processing systems\neasier than ever. Today, Azure—Microsoft’s public cloud offering—\nprovides a large range of data services to customers, including\nSQL databases (e.g. SQL Server, PostgreSQL, and MySQL), NoSQL\ndatabases (e.g. CosmosDB), analytics (e.g. Synapse SQL and Synapse\nSpark), big data (e.g. Apache Kafka and Apache Storm), and BI (e.g.\nPowerBI Analysis Service). While cloud providers benefit from the\neconomy of scale, migration to cloud also brings with it complexity.\nAs the number and the complexity of data services continue to\ngrow, cloud providers are facing increasing difficulties in managing\nall aspects of a service, such as resource provisioning, scheduling,\nquery optimization, query execution and service tuning, while still\nsatisfying customer SLAs and reducing operational expenses. For\ncloud users, on the other hand, it is non-trivial to extract the maxi-\nmum benefit from these data services, with each service exposing\nmany configurations and performance knobs to tune. The recent\ntrend of serverless computing seeks to relieve users from the burden\nof choice. However, this product line simply transfers the problem\nfrom cloud users back to cloud providers. Automating data services\nthus is an integral part of the cloud to operate at scale.\nWhile the cloud brings with it complexity, it presents massive\nopportunities. We have never before had access to such detailedworkload traces and system telemetries, collected across millions of\nusers and applications. More instrumentation is continuously added\nto the cloud for better tracing and monitoring. The combination of\nthe recent advances in data science (DS) and machine learning (ML),\nsophisticated telemetry, and shortage of data experts make now an\nideal time for the development and adoption of autonomous data\nservices . Prior research on self-adaptive [ 17], self-tuning [ 7], and\nself-managing [ 37] databases has been ongoing for decades, but it\nis only with the advent of cloud technology that the practicality\nof autonomous data services has emerged. Oracle announced the\n“World’s First Self-Driving Database\" in 2017 [ 12] suggesting that\nML will replace DBA, followed by many efforts on autonomous\ndatabases from industry and academia [ 38,50]. Furthermore, there\nis a wealth of research focused on utilizing ML to improve or substi-\ntute various components of database engines, such as the cardinality\nestimator, cost model, query planner, and indexer [ 23–25]. We are\nwitnessing an explosion of DS/ML-for-Systems innovations applied\nin the area of autonomous databases.\nThe vision described in the paper is the distillation of multiple re-\nsearch efforts led by applied researchers and data scientists from the\nGray Systems Lab (GSL) [ 32], in close collaboration with engineers\nfrom various departments within Azure Data. The set of research\ninitiatives seek to enhance and automate different facets of Azure\nData services, which have yielded significant COGS (cost of goods\nsold) saving for Azure. In this paper, we present our perspectives\non the development of autonomous cloud data services, including\nthe challenges involved, the progress we have made, the lessons\nwe have learned, the future directions we are pursuing, and the\noutstanding questions that require further investigation.\n2 OUR VIEWPOINTS\nWe first present our viewpoints on building autonomous data ser-\nvices in the cloud and explain the rationals behind them.\nViewpoint 1: The economic scale that has driven the adoption of\ncloud technology has also necessitated the development of autonomous\ndata services. However, we contend that true autonomous data services\ncan only be achieved in the cloud, meaning that the cloud is a necessary\nprecondition for the attainment of autonomy in data services.\nGaining knowledge from past experiences, which may span mul-\ntiple users and applications, is a critical step towards achieving\nautonomy for data services. The cloud platform provides extensive\nvisibility into a vast array of system metrics and workloads from nu-\nmerous users and applications over time. Due to its expansive range\nof services and customer base, the cloud amortize the cost of ad-\nvanced quality-of-service (QoS) features, making it more financially\nviable to invest in ML-based solutions. Although a 1% improvement\nin on-premise systems for an individual customer may seem in-\nsignificant, when applied across millions of cloud users, it can have\na substantial impact. Additionally, as customer workloads continue\nto evolve, the learning process must adapt accordingly, which the\ncloud facilitates through the rapid deployment of updates, often\nwithout requiring end-user involvement.\nViewpoint 2: Autonomy spans all layers of data services: cloud\ninfrastructure layer, query engine layer, and service layer.\n\nTowards Building Autonomous Data Services on Azure SIGMOD-Companion ’23, June 18–23, 2023, Seattle, WA, USA\nDeveloping an autonomous data service on the cloud requires\nleveraging ML to improve or replace more than just the work of\nDBAs and individual engine components. To illustrate, let us con-\nsider the life cycle of a query on a serverless big data service. These\nservices offer a range of adjustable knobs that can impact system\nperformance. For example, Spark requires the user to specify the\nnumber of executors and resources (cores and memory) allocated\nto them. Since this is a serverless service, all such decisions must\nbe made automatically at the service layer . Prior to running the\nquery, the service must be operational. If this is the first time the\ncustomer has used the service, then VM or container resources\nmust be provisioned at the cloud infrastructure layer . This raises a\nnumber of questions, such as which VM or container SKU to select,\nwhat the software configuration should be, and whether proactive\nresource provisioning is necessary to meet SLA for the customer.\nIf so, what SKUs should be proactively provisioned? Once the ser-\nvice is running, the query must be optimized using an accurate\ncardinality, the correct cost model, and a reliable query planner.\nThe query is then executed efficiently, potentially utilizing indexes\nand materialized views recommended based on the workloads. It\nis evident that all layers of the cloud stack, including the cloud\ninfrastructure layer, query engine layer, and service layer, as well\nas the interactions among them, must be taken into consideration\nwhen creating autonomous data services. This level of complexity\ncan be daunting for many institutions.\nViewpoint 3: The objectives of autonomous data services are: im-\nproving ease of use, optimizing performance, reducing costs, and\nmaintaining data privacy.\nAutonomy is not the ultimate goal of cloud services, but rather a\nmeans to achieving simpler, faster, and more cost-effective services\nfor users, while also prioritizing data privacy. Simplicity or ease\nof use is an important aspect, whereby users should not have to\nworry about resource allocation, query optimization, or excessive\nconfiguration and tuning decisions to use a data service. Achieving\noptimized performance is a shared goal among cloud users and\nproviders. To meet this goal, the cloud infrastructure layer needs\nto offer fast and intelligent resource provisioning and scheduling,\nqueries must be optimized and executed efficiently, and services\nshould be appropriately tuned. Cost savings benefit both users\nand providers. Interestingly, as we will show later, many times\nperformance and cost saving can be achieved simultaneously, but\nsometimes they are at odds with each other and requires a trade-\noff between these two goals. Finally, preserving privacy must be a\nfundamental requirement when pursuing the other three objectives.\n3 THE VANTAGE POINT\nAs an applied research organization under Azure Data, GSL is\nsituated at the intersection of research and product development.\nOur viewpoints on building autonomous cloud data services are\ntherefore shaped by the rich research and our first-hand experience\nworking with various product groups in Azure Data.\nResearch in Autonomous Data Services. Our perspectives\nand progress in creating autonomous data services are built upon\na strong foundation of research from the academic community. In\nthe interest of space, we focus on the major trends and provide\nexamples of influential works, although a comprehensive surveyof research in this area is beyond the scope of this discussion. As\npointed out by [ 39], the work on autonomous data systems dates\nback more than four decades, with the development of self-adaptive\ndatabases [ 17]. In the early 2000s, projects such as Microsoft’s\nAutoAdmin [ 4] and IBM’s DB2 design advisor [ 54] marked the era\nof self-tuning databases [ 7]. Oracle subsequently introduced the\nself-managing database [ 37]. While these earlier works aimed to\nalleviate various administrative tasks for DBAs, such as memory\nallocation, index recommendations, and materialized views, they\ndid not utilize DS&ML techniques. However, the cloud and the\nadvancements in DS&ML technologies have accelerated progress\ntowards autonomous data services. In 2017, Oracle announced the\n“World’s First Self-Driving Database” [ 12]. In academia, several\nefforts have focused on autonomous or self-driving databases [ 38,\n50], which aim to automatically tune database configurations or\noptimize databases for predicted future workloads. In a related line\nof research, many studies have applied ML to improve database\nengine components, often referred to as learned components. These\ninclude learned indexes [ 24], learned cardinality estimation [ 23],\nlearned query optimizer [ 25], and learned checkpoint [ 52]. All of\nthese efforts occur either inside the database engine or on top of the\nengine in the service layer. The efforts in optimized infrastructure\nsupport [10, 11] for cloud data services are fewer in comparison.\nFirst-Hand Product Experience. Over the years, we have\nworked on a large number of ML-for-Systems projects, and suc-\ncessfully delivered many new or improved features in making the\ncorresponding Azure data services more autonomous. Horizontally,\nwe have worked on Cosmos [ 42] (an internal cloud data service\nin Microsoft), Azure SQL Database [ 30], Azure Synapse SQL [ 31],\nSynapse Spark [ 26], HDInsight [ 28], etc. Vertically, our work has\ntouched all three layers of data services. We have proposed novel\ntechniques as well as adapted existing state-of-the-art research\nideas to address practical concerns such as explainability, debugga-\nbility, and cost management. In Section 4, we will showcase some\nof these projects.\n4 CHALLENGES AND PROGRESS\nIn this section, we discuss the challenges in automating each layer\nof the cloud stack and report on the progress we have achieved.\n4.1 Cloud Infrastructure Layer\nThe cloud infrastructure manages all hardware and software re-\nsources for the life cycle of data services. Significant technical and\nresearch efforts have been made to enhance it, including resource\nprovisioning [ 16], job scheduling [ 3,15], container imaging [ 5], and\nautoscaling [ 13,48]. However, these components heavily depend on\nthe manual adjustments by experts in the field, with fixed parame-\nters dispersed throughout the code base in configuration files. With\nthe emergence of advanced analytical tools and abundant telemetry\ndata, new opportunities for automation arises. Our solutions in\nthis layer were built based on our findings on the predictability of\nsystem behaviors and user behaviors.\nModeling system behaviors based on domain knowledge\nand system metrics. Training models for autonomous data ser-\nvices requires a substantial amount of data from various system\ntunables. While existing observational data may suffice in scenarios\n\nSIGMOD-Companion ’23, June 18–23, 2023, Seattle, WA, USA Yiwen Zhu et al.\nE3_V17 -1\n SC2_Gen 4.1Task Execution Time (s)\nCPU Utilization\n(a) CPU vs Task Exec Time\nE3_V17 -1\n SC2_Gen 4.1# of Running Containers\nCPU Utilization (b) CPU vs Runinng Containers\nFigure 1: Models to predict machine behavior [53]\nwith inherent volatility [ 53], additional data is often necessary. How-\never, gathering such data through rounds of trials on the production\ninfrastructure is impractical due to potential service disruptions.\nAs a result, we must either devise ways to minimize the number\nof experiment runs or gather system metrics and use ML to “emu-\nlate” system dynamics. In both cases, domain knowledge is crucial\nto comprehend the causal links among different components and\nestablish trustworthy models of the complex system.\nAs an example, in [ 53], we employed multiple linear models\nto predict machine behavior, such as CPU utilization versus task\nexecution time or the number of running containers (see Figure 1).\nThese models were then integrated into an optimizer to balance\nworkloads by tuning Cosmos scheduler configurations, such as the\nmaximum running containers for each SKU. Similar methods were\nused to determine the hardware/software configuration, such as\nRAM/SSD size and the mapping of logical drives to physical me-\ndia, and to set power limits on Cosmos racks. For Azure Synapse\nSpark [ 26], we developed a simulator to mimic the cluster initial-\nization process and derived the optimal policy for sending requests,\nreducing its tail latency. As another example, by using ML to predict\nthe throughput and latency of benchmark workloads on VMs with\nvarious kernel parameters, developed on MLOS [ 9], we refined the\nparameters of the Azure VM that runs Redis workloads.\nModeling user behaviors for better trade-offs between\nquality of service (QoS) and cost. Cloud operators face a contin-\nuous challenge in managing resources, striking a balance between\nQoS, such as low latency, and operational costs. To fulfill customer\nSLAs, cloud operators often need to proactively provision resources,\nwhich can lead to additional expenses. This interdependence is il-\nlustrated by the Pareto curve in Figure 2. By utilizing ML, these\ntrade-offs can be measured, and the Pareto curve can be globally op-\ntimized. In [ 41], we demonstrated that 77% of Azure SQL Database\nServerless usage is predictable and used ML forecasts to pause/re-\nsume databases proactively. Another instance is proactive cluster\nprovisioning based on expected user cluster creation demand to\nreduce wait time for cluster initialization on Azure Synapse Spark,\noptimizing both COGS and performance.\n4.2 Query Engine Layer\nDespite the significant amount of research conducted on utilizing\nML techniques to enhance or replace parts of the query engine [ 23–\n25], there is still reluctance within the industry to apply these\nadvanced methods to actual production systems. This reluctance\ncan be attributed to several factors. Firstly, real production sys-\ntems are often more intricate than the academic prototypes used\nCost\nLatencyOptimizedFigure 2: Pareto curve depicting the trade-offs between the\nQoS (x-axis) and the cost (y-axis)\nin research papers. For instance, the start-of-the-art learned opti-\nmizer, Bao [ 25], which provides rule hints to steer the optimizer\ntowards better plans, only takes into account 48 rule configura-\ntions, whereas the SCOPE query engine [ 42] used in Cosmos has\n256 rules in the query optimizer, which leads to 2256rule configu-\nrations that need to be considered. Secondly, while sophisticated\nML algorithms have demonstrated superior performance over cur-\nrent engine components, production engineers prioritize the inter-\npretability and debuggability of the models, as every new feature\nintroduced may generate new incident tickets for on-call engineers\nto resolve. Thirdly, workload patterns change over time due to data\nor concept drift, and regression is a genuine concern. Finally, the\ncost of training, especially for deep neural networks, becomes as\nanother obstacle to their adoption.\nThis subsection outlines our efforts to automate various aspects\nof query engines in production environments and address the afore-\nmentioned challenges. The fundamental principle underlying our\nwork is to learn from the past to improve the future . Our work\nis based on the observation that in actual production workloads,\nqueries and jobs are often recurrent and similar . For instance,\nin SCOPE, over 60% of jobs are recurring (involving periodic runs of\nscripts with the same operations but different predicate values [ 51]),\nand nearly 40% of daily jobs share common subexpressions with\nat least one other job [ 22]. This highlights the potential benefits of\ninsights learned from the past workloads to improve the efficiency\nof future workloads.\nWorkload Analysis. To automate query engines, we start from\nworkload analysis [ 20]. There are several pieces of information that\nare crucial for learning: meta data, query logs, and run time statis-\ntics (such as execution time and actual cardinality). However, these\ndata sources are frequently dispersed in different locations. Conse-\nquently, our first step is to combine this information. To facilitate\nvarious applications of the workload data, queries or subexpressions\nof queries are categorized into templates based on their recurrence\nand similarity, and the dependencies of queries/jobs (where the\noutput of one job serves as the input of another) in pipelines are\ncaptured [ 20]. Furthermore, workloads evolve over time, and as\nsuch, we also learn the evolving nature of the historical workloads\nto forecast future workloads.\nQuery Optimization. The optimizer serves as the brain of a\nquery engine, and decades of research and development have been\ninvested in improving this component for any data engine on Azure.\nOur guiding principle is to minimize changes to the existing\n\nTowards Building Autonomous Data Services on Azure SIGMOD-Companion ’23, June 18–23, 2023, Seattle, WA, USA\noptimizer and supplement it with learned components . Specif-\nically, we externalize the learned components and add simple ex-\ntensions to the optimizer to leverage these external services. For\ncardinality estimation, we utilize the templates generated by work-\nload analysis and train per-template micromodels [ 49]. We reduce\nthe number of micromodels by retaining only those that would\nactually improve performance. Consequently, the optimizer can\nemploy more precise cardinalities for queries or subexpressions\nwith corresponding models while reverting to the default cardinali-\nties for others. We adopt the same micromodel approach for learned\ncost models [ 46] and introduce a meta ensemble model that cor-\nrects and combines predictions from individual models to increase\ncoverage. To enhance optimizer plans using rule hints, we have\nmade notable progress in applying state-of-the-art research ideas\nfrom Bao [ 25] to production settings. However, we had to make sig-\nnificant adjustments for the production system, including limiting\nsteering to small incremental steps for better interpretability and\ndebuggability, minimizing pre-production experimentation costs\nusing a contextual bandit model, and guarding against regression\nwith a validation model [35, 51].\nQuery Execution. In query engines of big data services like\nCosmos and Spark, a job is compiled into a Direct Acyclic Graph\n(DAG) of stages that are executed in parallel. In the case of Cosmos,\nwe have observed an increase in the job complexity over the years,\nwith some jobs containing thousands of stages [ 52]. During runtime,\nthese large jobs can lead to machine hotspots that run out of local\ntemporary storage space, longer restarting times in case of failures,\nand suboptimal performance due to compounding errors from poor\noptimizer estimates. In [ 52], we trained models to estimate the\nexecution time, output size, and start/end time of each stage taking\ninto account of the inter-stage dependency, then applied a linear\nprogramming algorithm to introduce checkpoint “cut(s)” of the\nquery DAG. With this checkpoint optimizer, we were able to free\nthe temporary storage on hotspots by more than 70%and restart\nfailed jobs 68%faster on average with minimal impact on Cosmos\nperformance.\nComputation Reuse. With the large portion of recurrent and\noverlapping queries observed in real production workloads, there\nis a great opportunity to reuse past computations for future queries.\nCloudViews [ 21,43] was developed to detect and reuse common\ncomputations on Cosmos and Spark. It relies on a lightweight subex-\npression hash, called a signature, for scalable materialized view\nselection and efficient view matching. Deployed on Cosmos, we\nhave observed 34%improvement on the accumulative job latency,\nand 37%reduced total processing time [ 21]. We have worked on im-\nprovements of CloudViews on several fronts, including extending\nthe reuse from the syntactically equivalent subexpressions detected\nby the signatures to semantically equivalent and contained subex-\npressions while still maintaining the efficiency and scalability of\nthe detection process, as well as enabling a query to partially take\nadvantage of a view with the remaining results computed on the\nbase tables.\nPipeline Optimization. Production workloads not only have\nmany recurrent queries, but also many recurrent query pipelines,\nwhere queries are interconnected by their outputs and inputs. For\nexample, 70% of daily SCOPE jobs have inter-job dependencies.\nWe analyzed the interdependency to facilitate job scheduling [ 8]and developed a pipeline optimizer to optimize these recurrent\npipelines [ 14], including collecting pipeline-aware statistics and\npushing common subexpressions across consumer jobs to their\nproducer job.\n4.3 Service Layer\nDS&ML solutions impact how customers engage with a system at\nthe service level. The primary goal of the autonomous cloud services\nis to automate as many customer-facing decisions and options\nas possible while also providing highly customizable solutions.\nDS&ML tools allow for the automation of various decisions by\nstudying customer and application profiles. We can develop models\nwith different levels of granularity: 1) a global model that is broad\nbut may not be precise, 2) a segment model that groups similar\ncustomers or applications and shares insights within the group,\nand 3) an individual model for each customer or application that\nrequires sufficient data observations.\nIndividual models are more accurate when there is enough\ndata. To automate the scheduling of backups for PostgreSQL and\nMySQL servers, we used ML models to forecast user load for each\nspecific server [ 40]. The system identifies low load windows with\n99% accuracy, and the solution has been deployed for tens of thou-\nsands of PostgreSQL and MySQL servers across all Azure regions.\nSegment models or global models are deployed jointly to\ntransfer learning across customers/applications. To automate\nthe SKU suggestion for migrating from on-premise SQL Server\nto the cloud, we proposed a profiling model that compares new\ncustomers to existing segments of Azure customers. This enables\nnew customers to benefit from the decisions made by customers\nwith similar characteristics. We achieved a recommendation accu-\nracy of over 95% by combining the segment-wise knowledge with\na per-customer price-performance curve that offers a customized\nrank of all SKU options [ 6]. Another example involves auto-tuning\nconfigurations for Spark, built on top of the resource usage predic-\ntor [45]. We use iterative tuning algorithms to replace the manual\nprocess for customers. We start with a global model trained using\ndata from multiple benchmark queries. While the global model may\nnot be highly accurate, it serves as a reasonable starting point and is\nfine-tuned for each application as more observational data becomes\navailable.\n5 LESSONS LEARNED\nGiven our experience developing and integrating DS&ML solutions\nat the cloud infrastructure, query engine and data service layers for\nvarious cloud services across Microsoft (e.g., SCOPE [ 42], Synapse\nDW [ 31] as well as Spark [ 26,28]), common patterns arose across\nour engagements. Here we list some key lessons that we believe un-\nderlie our production successes and the speed at which we generate\nvalue for our product partners.\nInsight 1: Simplicity rules.\nThe common pattern across all our engagements is that simple\nheuristics tend to overrule ML and simple ML models, like linear\nmodels and tree-based models, tend to overrule complex deep learn-\ning models. This is particularly true for new engagements, with\nteams that have yet to adopt ML within production. For example,\n\nSIGMOD-Companion ’23, June 18–23, 2023, Seattle, WA, USA Yiwen Zhu et al.\nin [40], for PostgreSQL or MySQL servers that follow a stable daily\nor a weekly pattern, a simple heuristic that predicts the load of a\nserver based on that of the previous day was already sufficient to\ngenerate 96% accuracy. There are many projects in which a linear\nregression was most appropriate [45, 53]. Simplicity helps with:\nCost.While there is growing consensus on the positive impact\nof ML in automating and optimizing cloud services, production\ndeployments have to evaluate trade-offs with the increase in COGS\nto enable it. Consequently, algorithms are selected, not only by\nperformance, but also by other factors, like training and inference\ncost, dependency on specialized hardware (GPU, FPGAs), etc..\nScalability .For production systems, we need the metrics to\nseamlessly scale in training time, re-train frequency and data/pa-\nrameter handling. Most sophisticated machine learning techniques\ndo not satisfy this fundamental requirement, e.g., reinforcement\nlearning requires substantial training data before outperforming\ntraditional approaches. Moreover, when inference is on the critical\npath (which impacts the customer-experienced latency), latency\nbecomes crucial for the design of the infrastructure which prunes\nthe solutions space considerably.\nManageability .Manageability is important in two dimensions\n— debuggability and upgrades/rollbacks. ML models are sometimes\nnotorious for their difficulty in debugging. In a production envi-\nronment, when encountering regression, a complex data lineage\nacross a multitude of systems and language is needed for a close\ninvestigation from data ingestion to model (deployed) inference[ 34].\nDebuggability needs to be well-supported with tracking/versioning\nthrough MLOps [2] for continuous integration.\nExplainability .For customer-facing solutions, the expectation\nis that the reasoning of a choice made under the hood by any\nalgorithm has a succinct and ideally intuitive rationale. In this\nsense, an explainable solution, which in turn translates to simplicity\nsuch as [ 6], is very much preferable, while also improving the\nmanageability as mentioned before.\nInsight 2: One size does not fit all.\nOne global (macro) model that functions reasonably well for all\nscenarios can typically be traded off against several specific (micro)\nmodels that are tailored for individual customers, as discussed\nin Section 4.3. Identifying and crafting a single global model is\ngenerally difficult, as data heterogeneity necessitates considerable\nfeature construction and model hyperparameter tuning for optimal\nperformance. Micro models, however, go against simplicity due to\nthe challenges in managing the large number of models. A happy\nmiddle ground can be achieved by identifying natural ways to\nstratify the data, and building micro models for each cluster as\ndone in the SKU recommendation framework [ 6] that recommends\nright-sized Azure SQL SKU to migrate on-premise databases.\nInsight 3: Feedback loop is indispensable.\nIt is universally accepted that all ML solutions undergo exten-\nsive testing before being deployed into production, including back-\ntesting, flighting [ 53] or A/B testing (potentially with a smaller\ngroup). The dynamic nature of cloud data services, however, ne-\ncessitates ongoing improvement of even \"well-tested\" solutions in\norder to maintain performance, which leads to requirements for(1) a thorough monitoring system to spot potential changes in real-\ntime, continually assess, and initiate fine-tuning of the model, and\n(2) a rollback mechanism that reacts fast and avoids regression.\n6 FUTURE DIRECTIONS\nIn this section, we discuss some of the future directions that we are\ncurrently pursuing while also highlighting the challenges.\nDirection 1: Reuse, reuse and reuse!\nDespite the differences between distinct data services on Azure,\nthey all face a set of similar issues. For example, at the infrastruc-\nture level, many services need efficient cluster provisioning and\nauto-scaling. At the engine level, many require improvement in\ncardinality estimation, query planning, and computation reuse. At\nthe service level, auto-tuning is highly sought after for many ser-\nvices. Working on similar issues with multiple Azure data services\nover time, we came to the realization that a common reusable\nsolution is highly desirable to efficiently leverage the similar\ntechnologies and software artifacts among multiple services .\nHowever, reality presents a lot of challenges to reusability. Dis-\ntinct services collect different service-specific telemetries and work-\nload traces, store them in different places (e.g., Kusto[ 27], SQL\nserver, etc.), and have different preferences on the infrastructure\nfor model deployment (e.g. AML[29], Synapse ML[33], etc.).\nSo, can we reach the holy grail of reusable ML solutions? Al-\nthough we don’t have a complete answer, we can perhaps try to\ntackle this problem at different granularities of reusability.\nFunction Level Reuse. In the finest granularity, we can reuse\npieces of code modules that implement specific functions, e.g., time\nseries analysis of OS performance counter data. Our proposal is to\ncreate a AlgorithmStore (analogous to a GitHub for models) , which is a\nproject gallery with predefined algorithm templates. The previously\ndeveloped algorithm can be discovered and adapted to address\nnew scenarios quickly. For this type of algorithm catalog, it is\nrequired to have: (1) an easy search interface to discover similar\npre-existing solutions; (2) good API design to support extensibility\nand customizations; (3) clean modularized functions; (4) significant\ncoverage of common use cases; (5) code quality to allow robust\nreuse; and (6) better documentation.\nComponent Level Reuse. At the component level, the ques-\ntion of reusability pertains to whether we can establish a shared\ninfrastructure that supports similar or related system components\nacross various data services. For example, can we develop a com-\nmon infrastructure that facilitates auto-scaling for all services or\nquery optimization for all data engines? This task becomes increas-\ningly difficult due to the aforementioned differences among distinct\nservices. Nevertheless, we have made some strides on this front.\nThe Peregrine workload optimization platform [ 20] represents a\ncommon infrastructure for a set of related engine problems, such as\ncardinality estimation, cost models, and computation reuse. It has\nbeen implemented for both Cosmos and Spark. Peregrine consists\nof an engine-agnostic workload representation, workload catego-\nrization based on patterns, and a workload feedback mechanism\nthat enables query engines to respond to workload feedback.\nSystem-for-ML Support Level Reuse. At the highest level\nof granularity, all ML-for-Systems projects require System-for-ML\nsupport, from data ingestion, featurization, model training and\n\nTowards Building Autonomous Data Services on Azure SIGMOD-Companion ’23, June 18–23, 2023, Seattle, WA, USA\ntuning, model deployment, to model tracking. In GSL, we have a\nlarge collection of System-for-ML projects towards building such\na common infrastructure. A summary and vision of our efforts in\nthis area is provided in [2].\nDirection 2: Standardization.\nStandardization is critical for developing reusable infrastructure\nacross data services. It begins with telemetry. In addition to struc-\nturing the collected telemetry similarly across platforms and data\nservices through initiatives such as OpenTelemetry [ 36], we are\nalso exploring the use of semantic information from telemetry to\nenhance reusability across platforms and services (e.g. CPU uti-\nlization metrics on Windows and Linux VMs possess the same\nmeaning even though they may have different names). At the query\nengine level, we require standardization for representing work-\nloads and query plans. We have made some initial efforts on an\nengine-agnostic workload representation as part of the Peregrine\nworkload optimization platform [ 20]. We are now exploring the use\nof cross-language query plan specification, such as Substrait [ 47],\nas a standard plan representation across our engines. To simplify\nthe reuse of models for deployment within a common infrastruc-\nture, we also adopt standard representations for ML models, such\nas ONNX [ 1]. Furthermore, we package an ML model (along with\nany additional required code and libraries) into a standard generic\ncontainer that can be efficiently reused across systems [ 44], making\nit portable across all of our model-serving capabilities at Microsoft.\nDirection 3: Optimization across components jointly.\nIn many projects, the primary focus is typically on optimizing a\nsingle component of the entire system since it is owned by a spe-\ncific product team. For example, VM provisioning is owned by the\ncluster service team, while cardinality estimates are owned by the\nquery optimizer team, and so on. However, sequentially optimiz-\ning each individual component is unlikely to yield optimal overall\nperformance. Conversely, for a complex cloud service, especially\nat scale, it is impractical to create a massive optimization problem\nthat simultaneously optimizes all components while accurately cap-\nturing interactions across different components. Ongoing efforts\ncontinue to jointly optimize a selection of components and syn-\nchronize the deployment of changes so that the observational data\nreflects the latest deployed configuration. This approach enables us\nto focus on optimizing related components that work together in a\ncoordinated manner. By improving the joint optimization of these\ncomponents, we can improve the overall system performance.\nDirection 4: Responsible AI (RAI)\nML cannot be applied without risks [ 19], e.g., over-indexing on\na particular customer or workload, and bias is an inherent problem\nthat we continually encounter. We introduce guardrails to protect\ncustomers from expensive solutions and from performance regres-\nsions, and we regularly check that our ML-driven decisions serve all\ncustomers fairly. We have a responsibility to ensure that customers,\nbig or small, do not get marginalized from autonomous decisions.\nAt Microsoft, we are operationalizing the Responsible AI (RAI)\nat scale to protect privacy and security, improve fairness, inclusive-\nness, reliability, safety, transparency, and accountability. For the\nML-related projects, we perform a comprehensive RAI assessmentwhich is for now a manual and prolonged process by domain ex-\nperts. Several automation tools were developed (e.g., [ 18]), however,\nad-hoc solutions are still required for many cases.\n7 CONCLUSION AND CALL TO ACTION\nWe are living in fascinating and rapidly evolving times where tech-\nnology is advancing at a breakneck pace. Cloud and AI are among\nthe most transformative technologies of our era. The intersection\nof these two revolutionary technologies can be witnessed in the\nprogress made towards autonomous data services on cloud. In this\npaper, we showcased some of our progress in automating data\nservices on Azure. However, challenges remain to be overcome\nas highlighted in the previous section. We believe that the data-\nbase community has a vital role to play in shaping the future of\ncloud data services. We welcome other researchers to join us in this\nexciting journey.\nACKNOWLEDGEMENTS\nWe thank past team members, interns, and MAIDAP collaborators\nfor their contribution to our progress.\nREFERENCES\n[1] 2017. Open Neural Network Exchange (ONNX). https://onnx.ai/.\n[2]Ashvin Agrawal, Rony Chatterjee, Carlo Curino, Avrilia Floratou, Neha Godwal,\nMatteo Interlandi, Alekh Jindal, Konstantinos Karanasos, Subru Krishnan, Brian\nKroth, Jyoti Leeka, Kwanghyun Park, Hiren Patel, Olga Poppe, Fotis Psallidas,\nRaghu Ramakrishnan, Abhishek Roy, Karla Saur, Rathijit Sen, Markus Weimer,\nTravis Wright, and Yiwen Zhu. 2020. Cloudy with high chance of DBMS: a\n10-year prediction for Enterprise-Grade ML. In CIDR .\n[3]Eric Boutin, Jaliya Ekanayake, Wei Lin, Bing Shi, Jingren Zhou, Zhengping Qian,\nMing Wu, and Lidong Zhou. 2014. Apollo: Scalable and Coordinated Scheduling\nfor{Cloud-Scale }Computing. In OSDI . 285–300.\n[4]Nicolas Bruno, Surajit Chaudhuri, Arnd Christian König, Vivek R. Narasayya,\nRavishankar Ramamurthy, and Manoj Syamala. 2011. AutoAdmin Project at\nMicrosoft Research: Lessons Learned. IEEE Data Eng. Bull. 34, 4 (2011), 12–19.\n[5]cachelot.io. [n.d.]. Memcached Performance Tuning. https://cachelot.io/blog/\n2015/04/20/Speed-up-your-application-by-fine-tuning-Memcached.html.\n[6]Joyce Cahoon, Wenjing Wang, Yiwen Zhu, Katherine Lin, Sean Liu, Ray-\nmond Truong, Neetu Singh, Chengcheng Wan, Alexandra M Ciortea, Sreraman\nNarasimhan, and Subru Krishnan. 2022. Doppler: Automated SKU Recommenda-\ntion in Migrating SQL Workloads to the Cloud. PVLDB 15, 12 (2022).\n[7]Surajit Chaudhuri and Vivek Narasayya. 2007. Self-Tuning Database Systems: A\nDecade of Progress. In VLDB ’07 . 3–14.\n[8]Andrew Chung, Subru Krishnan, Konstantinos Karanasos, Carlo Curino, and\nGregory R. Ganger. 2020. Unearthing inter-job dependencies for better cluster\nscheduling. In 14th USENIX Symposium on Operating Systems Design and Imple-\nmentation (OSDI 20) . USENIX Association, 1205–1223. https://www.usenix.org/\nconference/osdi20/presentation/chung\n[9]Carlo Curino, Neha Godwal, Brian Kroth, Sergiy Kuryata, Greg Lapinski, Siqi\nLiu, Slava Oks, Olga Poppe, Adam Smiechowski, Ed Thayer, et al .2020. MLOS:\nAn infrastructure for automated software performance engineering. In DEEM .\n1–5.\n[10] Sudipto Das, Divyakant Agrawal, and Amr El Abbadi. 2013. ElasTraS: An Elastic,\nScalable, and Self-Managing Transactional Database for the Cloud. TODS 38, 1,\nArticle 5 (apr 2013), 45 pages.\n[11] Sudipto Das, Feng Li, Vivek R. Narasayya, and Arnd Christian König. 2016.\nAutomated Demand-Driven Resource Scaling in Relational Database-as-a-Service.\nInSIGMOD . 1923–1934.\n[12] Edgar Haren. 2017. Oracle Revolutionizes Cloud with the World’s First Self-\nDriving Database. https://blogs.oracle.com/database/post/oracle-revolutionizes-\ncloud-with-the-worlds-first-self-driving-database.\n[13] Avrilia Floratou, Ashvin Agrawal, Bill Graham, Sriram Rao, and Karthik Ra-\nmasamy. 2017. Dhalion: self-regulating stream processing in heron. PVLDB 10,\n12 (2017), 1825–1836.\n[14] Sunny Gakhar, Joyce Cahoon, Wangchao Le, Xiangnan Li, Kaushik Ravichandran,\nHiren Patel, Marc Friedman, Brandon Haynes, Shi Qiao, Alekh Jindal, and Jyoti\nLeeka. 2022. Pipemizer: An Optimizer for Analytics Data Pipelines. PVLDB\n(2022).\n\nSIGMOD-Companion ’23, June 18–23, 2023, Seattle, WA, USA Yiwen Zhu et al.\n[15] Robert Grandl, Srikanth Kandula, Sriram Rao, Aditya Akella, and Janardhan\nKulkarni. 2016. {GRAPHENE }: Packing and {Dependency-Aware }Scheduling\nfor{Data-Parallel }Clusters. In OSDI . 81–97.\n[16] Ori Hadary, Luke Marshall, Ishai Menache, Abhisek Pan, Esaias E Greeff, David\nDion, Star Dorminey, Shailesh Joshi, Yang Chen, Mark Russinovich, et al .2020.\nProtean: {VM}allocation service at scale. In OSDI . 845–861.\n[17] Michael Hammer and Arvola Chan. 1976. Index Selection in a Self-Adaptive Data\nBase Management System. In SIGMOD . 1–8.\n[18] Kenneth Holstein, Jennifer Wortman Vaughan, Hal Daumé III, Miro Dudik, and\nHanna Wallach. 2019. Improving fairness in machine learning systems: What do\nindustry practitioners need?. In CHI. 1–16.\n[19] Alekh Jindal and Jyoti Leeka. 2022. Query Optimizer as a Service: An Idea Whose\nTime Has Come! SIGMOD Record (2022).\n[20] Alekh Jindal, Hiren Patel, Abhishek Roy, Shi Qiao, Zhicheng Yin, Rathijit Sen,\nand Subru Krishnan. 2019. Peregrine: Workload Optimization for Cloud Query\nEngines. In SoCC . 416–427.\n[21] Alekh Jindal, Shi Qiao, Hiren Patel, Abhishek Roy, Jyoti Leeka, and Brandon\nHaynes. 2021. Production Experiences from Computation Reuse at Microsoft.. In\nEDBT . 623–634.\n[22] Alekh Jindal, Shi Qiao, Hiren Patel, Zhicheng Yin, Jieming Di, Malay Bag, Marc\nFriedman, Yifung Lin, Konstantinos Karanasos, and Sriram Rao. 2018. Computa-\ntion Reuse in Analytics Job Service at Microsoft. In SIGMOD . 191–203.\n[23] Kyoungmin Kim, Jisung Jung, In Seo, Wook-Shin Han, Kangwoo Choi, and\nJaehyok Chong. 2022. Learned Cardinality Estimation: An In-Depth Study. In\nSIGMOD . 1214–1227.\n[24] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In SIGMOD , Gautam Das, Christopher M.\nJermaine, and Philip A. Bernstein (Eds.). 489–504.\n[25] Ryan C. Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh,\nTim Kraska, Olga Papaemmanouil, and Nesime Tatbul. 2019. Neo: A Learned\nQuery Optimizer. PVLDB 12, 11 (2019), 1705–1718.\n[26] Microsoft. [n.d.]. Apache Spark in Azure Synapse Analytics. https://docs.\nmicrosoft.com/en-us/azure/synapse-analytics/spark/apache-spark-overview.\n[27] Microsoft. [n.d.]. Azure Data Explorer. https://docs.microsoft.com/en-us/azure/\ndata-explorer/.\n[28] Microsoft. [n.d.]. Azure HDInsight. https://azure.microsoft.com/en-us/services/\nhdinsight/.\n[29] Microsoft. [n.d.]. Azure Machine Learning. https://azure.microsoft.com/en-\nus/services/machine-learning/.\n[30] Microsoft. [n.d.]. Azure SQL Database. https://azure.microsoft.com/en-us/\nproducts/azure-sql/database.\n[31] Microsoft. [n.d.]. Azure Synapse SQL architecture. https://docs.microsoft.com/en-\nus/azure/synapse-analytics/sql/overview-architecture.\n[32] Microsoft. [n.d.]. Gray Systems Lab. https://www.microsoft.com/en-us/research/\ngroup/gray-systems-lab/.\n[33] Microsoft. [n.d.]. SynapseML. https://microsoft.github.io/SynapseML/.\n[34] Mohammad Hossein Namaki, Avrilia Floratou, Fotios Psallidas, Subru Krishnan,\nAshvin Agrawal, Yinghui Wu, Yiwen Zhu, and Markus Weimer. 2020. Vamsa:\nAutomated Provenance Tracking in Data Science Scripts. In KDD . 1542–1551.\nhttps://doi.org/10.1145/3394486.3403205\n[35] Parimarjan Negi, Matteo Interlandi, Ryan Marcus, Mohammad Alizadeh, Tim\nKraska, Marc Friedman, and Alekh Jindal. 2021. Steering Query Optimizers: A\nPractical Take on Big Data Workloads. In SIGMOD . 2557–2569.\n[36] OpenTelemetry. [n.d.]. OpenTelemetry. https://substrait.io/.\n[37] Oracle. 2006. Oracle Database 10g Release 2: The Self-Managing Database . Techni-\ncal Report. Oracle.\n[38] Andrew Pavlo, Gustavo Angulo, Joy Arulraj, Haibin Lin, Jiexi Lin, Lin Ma,\nPrashanth Menon, Todd C. Mowry, Matthew Perron, Ian Quah, Siddharth San-\nturkar, Anthony Tomasic, Skye Toor, Dana Van Aken, Ziqi Wang, Yingjun Wu,Ran Xian, and Tieying Zhang. 2017. Self-Driving Database Management Systems.\nInCIDR .\n[39] Andrew Pavlo, Matthew Butrovich, Lin Ma, Prashanth Menon, Wan Shen Lim,\nDana Van Aken, and William Zhang. 2021. Make Your Database System Dream of\nElectric Sheep: Towards Self-Driving Operation. PVLDB 14, 12 (2021), 3211–3221.\n[40] Olga Poppe, Tayo Amuneke, Dalitso Banda, Aritra De, Ari Green, Manon Kno-\nertzer, Ehi Nosakhare, Karthik Rajendran, Deepak Shankargouda, Meina Wang,\nAlan Au, Carlo Curino, Qun Guo, Alekh Jindal, Ajay Kalhan, Morgan Oslake, So-\nnia Parchani, Vijay Ramani, Raj Sellappan, Saikat Sen, Sheetal Shrotri, Soundarara-\njan Srinivasan, Ping Xia, Shize Xu, Alicia Yang, and Yiwen Zhu. 2020. Seagull:\nAn Infrastructure for Load Prediction and Optimized Resource Allocation. In\nPVLDB . VLDB Endowment, 154–162.\n[41] Olga Poppe, Qun Guo, Willis Lang, Pankaj Arora, Morgan Oslake, Shize Xu, and\nAjay Kalhan. 2022. Moneyball: proactive auto-scaling in Microsoft Azure SQL\ndatabase serverless. PVLDB 15, 6 (2022), 1279–1287.\n[42] Conor Power, Hiren Patel, Alekh Jindal, Jyoti Leeka, Bob Jenkins, Michael Rys,\nEd Triou, Dexin Zhu, Lucky Katahanas, Chakrapani Bhat Talapady, Joshua Rowe,\nFan Zhang, Rich Draves, Marc Friedman, Ivan Santa Maria Filho, and Amrish\nKumar. 2021. The Cosmos Big Data Platform at Microsoft: Over a Decade of\nProgress and a Decade to Look Forward. PVLDB 14, 12 (2021).\n[43] Abhishek Roy, Alekh Jindal, Priyanka Gomatam, Xiating Ouyang, Ashit Gosalia,\nNishkam Ravi, Swinky Mann, and Prakhar Jain. 2021. SparkCruise: Workload\nOptimization in Managed Spark Clusters at Microsoft. PVLDB 14, 12 (2021),\n3122–3134.\n[44] Karla Saur, Tara Mirmira, Konstantinos Karanasos, and Jesús Camacho-Rodríguez.\n2022. Containerized Execution of UDFs: An Experimental Evaluation. PVLDB 15,\n11 (2022), 3158 – 3171. https://doi.org/doi:10.14778/3551793.3551860\n[45] Rathijit Sen, Alekh Jindal, Hiren Patel, and Shi Qiao. 2020. AutoToken: Predicting\npeak parallelism for Big Data analytics at Microsoft. PVLDB 13, 12 (2020), 3326–\n3339.\n[46] Tarique Siddiqui, Alekh Jindal, Shi Qiao, Hiren Patel, and Wangchao Le. 2020.\nCost Models for Big Data Query Processing: Learning, Retrofitting, and Our\nFindings. In SIGMOD . 99–113.\n[47] Substrait. [n.d.]. Substrait: Cross-Language Serialization for Relational Algebra.\nhttps://substrait.io/.\n[48] Muhammad Tirmazi, Adam Barker, Nan Deng, Md E Haque, Zhijing Gene Qin,\nSteven Hand, Mor Harchol-Balter, and John Wilkes. 2020. Borg: the next genera-\ntion. In EuroSys . 1–14.\n[49] Chenggang Wu, Alekh Jindal, Saeed Amizadeh, Hiren Patel, Wangchao Le, Shi\nQiao, and Sriram Rao. 2018. Towards a Learning Optimizer for Shared Clouds.\nPVLDB 12, 3 (nov 2018), 210–222.\n[50] Bohan Zhang, Dana Van Aken, Justin Wang, Tao Dai, Shuli Jiang, Jacky Lao,\nSiyuan Sheng, Andrew Pavlo, and Geoffrey J. Gordon. 2018. A Demonstration of\nthe OtterTune Automatic Database Management System Tuning Service. PVLDB\n11, 12 (2018), 1910–1913.\n[51] Wangda Zhang, Matteo Interlandi, Paul Mineiro, Shi Qiao, Nasim Ghazanfari,\nKarlen Lie, Marc Friedman, Rafah Hosn, Hiren Patel, and Alekh Jindal. 2022.\nDeploying a Steered Query Optimizer in Production at Microsoft. In SIGMOD .\n2299–2311.\n[52] Yiwen Zhu, Matteo Interlandi, Abhishek Roy, Krishnadhan Das, Hiren Patel,\nMalay Bag, Hitesh Sharma, and Alekh Jindal. 2021. Phoebe: A Learning-Based\nCheckpoint Optimizer. PVLDB 14, 11 (jul 2021), 2505–2518. https://doi.org/10.\n14778/3476249.3476298\n[53] Yiwen Zhu, Subru Krishnan, Konstantinos Karanasos, Isha Tarte, Conor Power,\nAbhishek Modi, Manoj Kumar, Deli Zhang, Kartheek Muthyala, Nick Jurgens,\net al.2021. KEA: Tuning an Exabyte-Scale Data Infrastructure. In SIGMOD .\n2667–2680.\n[54] Daniel C. Zilio, Jun Rao, Sam Lightstone, Guy Lohman, Adam Storm, Chris-\ntian Garcia-Arellano, and Scott Fadden. 2004. DB2 Design Advisor: Integrated\nAutomatic Physical Database Design. In VLDB ’04 . 1087–1097.",
  "textLength": 52301
}