{
  "paperId": "109b4bdd8807f9c9547940ac78df85bfbb5a3a00",
  "title": "Redbench: A Benchmark Reflecting Real Workloads",
  "pdfPath": "109b4bdd8807f9c9547940ac78df85bfbb5a3a00.pdf",
  "text": "arXiv:2506.12488v1  [cs.DB]  14 Jun 2025Redbench: A Benchmark Reflecting Real Workloads\nSkander Krid\nUniversity of Technology Nuremberg\nNuremberg, Germany\nskander.krid@utn.deMihail Stoian\nUniversity of Technology Nuremberg\nNuremberg, Germany\nmihail.stoian@utn.deAndreas Kipf\nUniversity of Technology Nuremberg\nNuremberg, Germany\nandreas.kipf@utn.de\nAbstract\nInstance-optimized components have made their way into produc-\ntion systems. To some extent, this adoption is due to the character-\nistics of customer workloads, which can be individually leveraged\nduring the model training phase. However, there is a gap between\nresearch and industry that impedes the development of realistic\nlearned components: the lack of suitable workloads. Existing ones,\nsuch as TPC-H and TPC-DS, and even more recent ones, such as\nDSB and CAB, fail to exhibit real workload patterns, particularly\ndistribution shifts. In this paper, we introduce Redbench, a collec-\ntion of 30 workloads that reflect query patterns observed in the real\nworld. The workloads were obtained by sampling queries from sup-\nport benchmarks and aligning them with workload characteristics\nobserved in Redset.\nCCS Concepts\n‚Ä¢Information systems ‚ÜíDatabase performance evaluation .\nKeywords\nbenchmarks, workload-aware optimizations, instance-optimized\nsystems, real workload characteristics\nACM Reference Format:\nSkander Krid, Mihail Stoian, and Andreas Kipf. 2025. Redbench: A Bench-\nmark Reflecting Real Workloads. In International Workshop on Exploiting\nArtificial Intelligence Techniques for Data Management (aiDM ‚Äô25), June\n22‚Äì27, 2025, Berlin, Germany. ACM, New York, NY, USA, 5 pages. https:\n//doi.org/10.1145/3735403.3735998\n1 Introduction\nIn one of his BTW 2025 talks, Goetz Graefe made a distinctive\nstatement: ‚ÄúWe, as a community, have very few ideas.‚Äù We feel\ncompelled to add: ‚Äúbut when that happens, that idea changes the\nway we build and optimize systems. ‚Äù A notable example is the shift\nfrom the traditional approach of optimizing databases for worst-\ncase scenarios to instance-optimized systems, i.e., components that\nadapt to the characteristics of the observed workloads [ 14]. This\nnew paradigm is well suited to production environments where\nuser workloads exhibit diverse patterns making one-size-fits-all\nworst-case optimizations suboptimal.\nMotivation. However, this naturally incurs a gap between research\nand industry: While production systems have the possibility to\nstress test their workload-driven optimizations, e.g., by performing\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\naiDM ‚Äô25, Berlin, Germany\n¬©2025 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-1920-2/2025/06\nhttps://doi.org/10.1145/3735403.3735998a workload replay, the research community is often constrained to\nusing benchmarks that do not accurately reflect query patterns seen\nin practice. In particular, workload distribution shifts determine\nthe difference between a robust and an impractical optimization.\nWithout efforts in this direction, the impact of upcoming learned\nor workload-driven optimizations will be limited.\nResearch ZIndustry. Fortunately, industry has been open to\nthis call and released in the last years statistics about production\nworkloads. For instance, Snowset [ 29] and Redset [ 27] capture cus-\ntomer query metadata from Snowflake and Redshift, respectively.\nTo this end, van Renen and Leis [ 28] analyzed Snowset and pro-\nposed the Cloud-Analytics Benchmark (CAB), which integrates the\nfindings found in Snowset. CAB does use TPC-H as its support\nbenchmark, yet it incorporates essential cloud-specific dimensions,\nsuch as elasticity and multi-tenancy. Since the release of Snowset,\nthere has been an increased interest in leveraging the repetitiveness\nof queries and table scan expressions for clustering [ 6], secondary\nindexing [ 22], query super-optimization [ 16], and resource alloca-\ntion [ 4,19]. Notably, repetitiveness is a key feature that workload-\ndriven optimizations rely on and is not well captured by any of the\nstandard benchmarks, as shown by van Renen et al. [ 27], nor by\nCAB, which randomly samples queries from the support bench-\nmark. DSB [ 5] aims to address this issue, particularly in TPC-DS.\nYet, the underlying workload statistics are generated synthetically.\nContribution. In light of recent advancements in the learned sys-\ntems community, we outline the characteristics a benchmark should\nfulfill to properly stress-test such proposals. Then, we introduce\nRedbench, a benchmark that maps a support benchmark to Redset\nby preserving these very characteristics that proved to be crucial in\nthe workload-driven optimizations of the last few years. Redbench‚Äôs\nworkloads for well-studied support benchmarks are available at:\nhttps://github.com/utndatasystems/redbench .\nStructure. The paper is structured as follows: We first provide\nan overview of recent learned and workload-driven optimizations,\nextracting the characteristics that they require from a benchmark to\nbe properly tested (Sec. 2). Then, we introduce Redbench‚Äôs sampling\nand mapping mechanisms (Sec. 3). Finally, we outline future work\nand plans for upcoming releases of Redbench (Sec. 4).\n2 Instance-Optimized Components\nReal-world workloads exhibit patterns that system components\noptimizing for the worst-case scenarios do not take advantage of.\nLearned and workload-aware components exploit this very fact by\nmodeling data and/or workload distribution during training and\ndata structure building.\n\naiDM ‚Äô25, June 22‚Äì27, 2025, Berlin, Germany Krid et al.\nTable 1: What learned and workload-aware optimizations require from a benchmark for proper validation.\nProblemInstance-Optimized\nComponentsWhat is needed TPC DSB Redbench\nCachingSemantic Caching [ 3]: Crys-\ntal [7], Cache Investment [ 12],\nPlan Caching [1, 2]query rep. ‚úó fixed ‚úì\nClustering (incl. Secondary Indexing)MDDL [6], table scan rep. ‚úó fixed v0.4\nPredicate Caching [22] workload drift ‚úó synthetic real\nQuery OptimizationLearned Cardinalities [ 8,11],\nNeo [ 18], Bao [ 17], Balsa [ 31]query rep. ‚úó fixed ‚úì\nLero [32], FlowLoss [20],\nSuper-Optimization [16]workload drift ‚úó synthetic real\nMaterialized Views [9, 23, 24]DynaMat [ 13], Hawc [ 21],\nCloudViews [10]query rep. ‚úó fixed ‚úì\nResource Allocation (incl. Query Stats Prediction) Intelligent Scaling [4, 19]timestamp-based\nquery rep.‚úó ‚úó v0.5\nIn recent years, our community has spent a significant amount\nof time optimizing these proposals. Even so, the benchmarks used\nfor evaluation have lagged behind, since they do not capture the\nquery patterns seen in production systems. To understand this gap,\nconsider Tab. 1, where we examine (1) the standard problems in\ndatabases, (2) existing learned/workload-aware components, (3)\nwhat these components require from a benchmark, and (4) whether\nexisting benchmarks actually meet these requirements. Before we\noutline the categorization made in column (3) of the same table, we\nbriefly review related work.\nExisting Benchmarks. TPC-benchmarks such as TPC-H [ 26] and\nTPC-DS [ 25] are widely used to test both research and production\nsystems, yet they operate on synthetic data. JOB [ 15], which aims to\nstress test the query optimizer, runs on the real-world IMDb dataset,\nyet its queries are handwritten. An extension of JOB is CEB [ 28],\nwhich provides more JOB-like queries on the same IMDb dataset.\nNeither of the aforementioned benchmarks accurately reflects the\ncharacteristics of production workloads. DSB [ 5] is a recent pro-\nposal that builds on TPC-DS, by providing more complex data\ndistributions and dynamic workloads. However, the query repeti-\ntion ratio is fixed, and the workload drifts are generated randomly\nusing a Gaussian distribution.\nClustering. In order to reduce I/O, data systems engineers spend a\nconsiderable amount of effort designing pruning strategies for data\nblocks [ 33]. This is not surprising, as queries spend around 50% of\ntheir time in scan and filter operators [ 28]. Notably, this is also the\ncase for read-only queries, where scan and filter operators account\nfor 44.5% of the total execution time. Traditionally, this pruning\neffect is enabled by clustering the data. However, this decision can\nbe rather arbitrary if the workload accesses multiple columns.\nA promising direction is to allow the system to analyze the\ncharacteristics of the workload and adapt accordingly. For example,\nMDDL [ 6] exploits the repetitiveness of table predicates to sort on\nmultiple columns at once. Notably, it can converge to the optimal\nnumber of data blocks, provided that all user predicates are captured\nwithin its auxiliary column. The challenge arises in the presence of\na workload drift, e.g., the set of table predicates changes completely,\nrendering the auxiliary column unusable. We plan to focus ontable scan repetitiveness as part of Redbench v0.4, particularly on\nmatching the findings in Schmidt et al. [22, Fig. 5].\nCaching. Workload drifts also limit the applicability of workload-\ndriven secondary indexes such as Predicate Caching [ 22], which\nmaintains a lightweight key-value cache of all table and semi-\njoin-induced predicates on base tables. The values in the cache\nare approximations of the row ranges that qualify for the filter.\nWorkload drifts affect the relevance of these cache entries. Hav-\ning a workload-aware benchmark opens the possibility for test-\ning predicate-relaxation techniques [ 30], e.g., rows qualifying for\no_orderdate <= ‚Äô2025-05-01‚Äô also qualify for later months.\nQuery Optimization. Given the repetitiveness of queries, a recent\nproposal by Marcus [ 16] envisions long optimization times instead\nof the traditional, fast optimization strategies that only consider\nstatistics. The intuition is that it is worth spending more time opti-\nmizing recurring queries, the overhead being amortized over the\nsubsequent runs. Evaluating such proposals requires realistic query\nrepetition patterns.\nView Maintenance. Materialized Views (MVs) are a complemen-\ntary approach to optimize query performance by pre-computing\nsubresults. An important decision around MVs is which views to\nchoose. The goal is to achieve the greatest performance benefit rel-\native to the associated maintenance and storage overhead. Leverag-\ning workload information‚Äîparticularly the repetitiveness of joins,\npredicates, and aggregations‚Äîcan simplify this decision.\nResource Allocation. In a serverless environment, a system must\nbe able to handle workload spikes [ 28]. As a result, accurately\npredicting the timestamps‚Äîpossibly even the queries‚Äîassociated\nwith peak periods is highly valuable. The same is true for resource\nallocation, where predicting query execution time and memory\nconsumption can further improve the decision on which resources\nto use during these bursts. Redbench v0.5 will include timestamps,\nallowing it to be integrated with CAB (Redbench can become one\nof CAB‚Äôs query pools).\nNext, we present a benchmark designed to bridge this gap by\ncapturing all key characteristics of real-world analytical workloads\ndiscussed so far.\n\nRedbench: A Benchmark Reflecting Real Workloads aiDM ‚Äô25, June 22‚Äì27, 2025, Berlin, Germany\n3 Redbench\nWe introduce Redbench, a repetition-aware benchmark based on\nRedset, the customer query metadata published by Redshift [ 27]\nspanning 3 months and 200 clusters. The idea behind Redbench\nis to sample users with different workload profiles from Redset,\nand recreate their workloads by sampling similar queries from a\nsupport benchmark. The sampling process of a single user workload\nis visualized in Fig. 1. In the following, we detail the individual steps.\n3.1 Definitions\nFor a Redset query, we define its scanset as the set of persistent ta-\nbles that it scans, and its hash as the combination of scanset , num-\nber of joins, number of table scans, and the feature_fingerprint\nfield, which is a proxy for query likeness in Redset.\nA key metric that we use to categorize a Redset user workload\nis the query repetition rate: the ratio of queries whose hash has\nalready occurred in the timeline. A user workload in Redset and\nits corresponding workload sampled by Redbench will have equal\nquery repetition rates. We describe in Sec. 3.5 how our workload\nsampling guarantees this.\n3.2 Prefiltering\nWe prefilter Redset queries as follows:\n‚Ä¢Currently, Redbench supports only SELECT -queries, which\naccount for 48.9% of Redshift‚Äôs fleet as shown in van Renen\net al. [27]. Support for updates is planned for v1.0.\n‚Ä¢We filter out queries that were answered from Redshift‚Äôs\nresult cache. A system with a result cache can answer such\nqueries by just scanning the cached output. Therefore, we\nbelieve that these queries are not worth optimizing.\n‚Ä¢During the workload sampling step, we will match Redset\nscansets with benchmark templates based on the number\nof joins. Thus, we eliminate Redset queries that do not con-\ntain any joins. Also since we choose the mapping based on\nthe normalized number of joins, we eliminate users whose\nminimum and maximum numbers of joins are equal.\n‚Ä¢For a consistent mapping, we want a scanset to uniquely\nidentify the number of joins. For most queries, assuming no\nself-joins or set operators are present, the number of joins is\nequal to the scanset size minus one. We eliminate all Redset\nqueries that do not adhere to this.\nBusiest Week. For each user, we identify the week with the highest\nnumber of queries and retain only the first ùêæqueries from that week;\nthe value of ùêæis user-specified and currently set to 1,000. A week\nstarts on Monday 8 am, and ends on Friday 5 pm.\n3.3 Support Benchmark\nRedbench is designed to be benchmark-agnostic, i.e., it does not\nmake any assumptions on the support benchmark. However, it is\ncrucial to make sure that the chosen benchmark has enough query\nvariability, namely query templates and query instances, so it can\naccurately replicate various Redset‚Äôs user workloads. A suitable\nsupport benchmark must also guarantee that query instances of\nthe same template must have the same number of joins. This is the\nw eek 110 queriesw eek 20 queriesw eek k\n2 43 queries3145 queriesR edset user...w eek n7 45 queries...quer y 1quer y 1000...\nbenchmark quer y 1benchmark quer y 1000...Suppor t BenchmarkGenerationsampled w orkloaduser quer y stat s\nquer y r epetition ratio: 85%quer y r epetition ratio: 85%\n...90%-100% buck et80%-90% buck et0%-10% buck etSQLSQLSQLSQL...Figure 1: Sampling and generation using a support bench-\nmark. Redbench‚Äôs workloads are partitioned into 10 buckets,\ncorresponding to the query repetition ratio in Redset [27].\ncase for most well-studied benchmarks, e.g., JOB [ 15], CEB [ 20],\nTPC-H [ 26], and TPC-DS [ 25]. As of the current version, we provide:\nredbench[imdb] .Redbench instantiation with the combination of\nthe IMDb-based Join Order Benchmark (JOB) and Cardinality Esti-\nmation Benchmark (CEB). Combining both benchmarks provides a\nwider variety of query templates and instances to sample from.\nredbench[tpc-ds] .Redbench instantiation with TPC-DS as the\nsupport benchmark. Given TPC-DS‚Äôs large number of templates, it\ncan be easily used as a support benchmark for Redbench.\n3.4 User Sampling\nWe divide users into 10 buckets based on their Redset‚Äôs query repe-\ntition rate: 0%-10%, 10%-20%, ..., 90%-100% (inclusive to the left).\nTo ensure more variety in Redbench‚Äôs workloads, we choose three\nusers from each repetition bucket, for a total of 30 users, as follows:\n(a)We rank users in each bucket in ascending order, once based\non the number of distinct number of joins values, and once\nbased on the number of distinct scansets across their queries.\n(b)We sum up both ranks for each user to get the user‚Äôs workload-\nvariability. A lower value means less diverse queries in terms\nof number of joins and scansets.\n(c)We select the users with the lowest, median, and highest\nworkload-variability values.\n3.5 Workload Sampling\nIn this step, we recreate each of the 30 user workloads from the\nprevious step using queries from the support benchmark. Given a\nworkload, we iterate over the queries in ascending order of their\narrival timestamps, and map them to a benchmark query:\n(a)If we have already encountered the query hash, reuse the\nsame benchmark query instance mapped to it.\n\naiDM ‚Äô25, June 22‚Äì27, 2025, Berlin, Germany Krid et al.\n(b)If we have already encountered the scanset, lookup the cor-\nresponding benchmark query template that we mapped to it\nand choose one of its unused query instances. We implement\na fallback strategy in case we have already used up all query\ninstances from the corresponding template (see below).\n(c)If we have not encountered the scanset before, then we need\nto map it to one of the unused templates of the support\nbenchmark. We only consider templates that produce a nor-\nmalized number of join closest to the normalized number of\njoins of the user query, and call these closest templates . Then,\nwe choose the template with the most number of query in-\nstances. It is possible that all templates have already been\nmapped. In that case, we rely on our fallback strategy.\nFallback Strategy. Reaching the fallback step means that the\nused support benchmark does not have enough query instances\nor templates to mimic the production workload. This happens for\naround 18.16% and 17.33% of all queries across the 30 workloads\ninredbench[imdb] andredbench[tpc-ds] , respectively. We pro-\nceed as follows: If at least one of the already mapped benchmark\nclosest templates has unused query instances, then randomly pick\none of those. Otherwise, randomly pick and reuse one of the query\ninstances from the closest templates.\n3.6 Workload Characteristics Preservation\nRedbench aims to retain key workload characteristics such as table\nscan repetitiveness and queries‚Äô relative complexity and timeline.\nNumber of Joins. Currently, Redbench preserves the number of\njoins from Redset in a relative sense. Specifically, the number of\njoins in each query is normalized during mapping, i.e., the minimum\nand maximum number of joins in the Redset workload are linearly\nmapped to the minimum and maximum number of joins in the\nsupport benchmark. As shown in Fig. 2, Redbench maintains the\nshape and structure of the join complexity curve over time despite\nthe shifted absolute numbers.\nScansets. Ideally, Redbench would achieve a one-to-one mapping\nbetween user-level tables in Redset and tables in the support bench-\nmark. In this scenario, each Redset query‚Äôs scanset would corre-\nspond uniquely to the scanset of a benchmark query, allowing\nprecise replication of table access patterns and, by extension, join\ngraph structures. However, two main limitations prevent this:\n(a)Insufficient table coverage in support benchmarks: 31% of\nall remaining Redset users after the prefiltering step query\nmore tables than the number of tables available in either\nIMDb or TPC-DS, making a bijection impossible.\n(b)Limited query template diversity: Even when table and tem-\nplate counts match, if the user‚Äôs workload includes scansets\nlike[T1] and[T2] , and the benchmark only includes [T1]\nand[T1, T2] , then no bijective mapping of user tables to\nbenchmark tables can preserve the scanset structure. There-\nfore, a suitable support benchmark should ideally include a\ntemplate for each possible scanset.\nInstead, Redbench offers a best-effort preservation of scansets,\nbounded by the richness of the available support benchmark. In\nfuture versions of Redbench, we plan to consider synthetically gen-\nerated queries as support benchmark. Assuming we can generate aquery for each possible scanset, Redbench would achieve a perfect\nscanset-to-scanset equivalence with Redset.\nFull Query Repetition. Another critical characteristic preserved\nby Redbench is full query repetition, i.e., the frequency and pattern\nwith which identical queries reoccur in a workload. By construction,\nRedbench precisely replicates the query repetition rate observed in\nRedset by mapping identical query hashes to the same benchmark\nquery instances. However, as mentioned earlier, we exclude Redset\nqueries that were answered from the result cache. Consequently,\nthe remaining repetitions typically represent SELECT queries in-\nterspersed with updates, a common workload pattern found in\nbusiness intelligence and dashboarding use cases [27].\nTimeline and Workload Peaks. Redbench currently preserves\nthe relative temporal order of queries and plays them back-to-back.\nWhile sufficient to evaluate order but time-independent optimiza-\ntions, discarding absolute timestamps is not suitable for real-time\nbenchmarking scenarios. In a future version, we intend to integrate\nRedbench with CAB so that workload peaks can be accurately rep-\nresented and used to evaluate workload-driven resource allocation\nstrategies. CAB preserves wall -clock timing and can drive realistic\nstress tests for elastic cloud systems.\n0.0 0.2 0.4 0.6 0.8 1.0\nQuery timeline0246810Cumulative average #joinsredbench[imdb] redset low- mid- high-variability\nFigure 2: How Redbench‚Äôs sampled workloads preserve Red-\nset‚Äôs relative join complexity over time for the query repeti-\ntion bucket 80%-90%.\n4 Conclusion\nRedbench is a novel benchmark that reflects the query patterns\npresent in production workloads. Being based on Redset [ 27], it\ninherits the natural properties that instance-optimized components\nrequire for a proper stress-test, such as query and table scan repeti-\ntiveness, and workload drifts. Redbench‚Äôs agnosticity to the support\nbenchmark makes it easy to instantiate it with an already exist-\ning one, provided it contains enough distinct query templates and\nquery instances, to make it conform to Redset‚Äôs statistics. Currently,\nRedbench has instantiations for TPC-DS [25] and IMDb [15, 20].\nFuture Work. We plan to provide Redbench‚Äôs instantiations of all\nwell-studied benchmarks as query pools for CAB [ 28]. Moreover, we\nplan to extend Redbench with updates so that instance-optimized\ncomponents can be evaluated under realistic conditions.\n\nRedbench: A Benchmark Reflecting Real Workloads aiDM ‚Äô25, June 22‚Äì27, 2025, Berlin, Germany\nReferences\n[1]G√ºnes Alu√ß, David DeHaan, and Ivan T. Bowman. 2012. Parametric Plan Caching\nUsing Density-Based Clustering. In ICDE . IEEE Computer Society, 402‚Äì413.\n[2]Tracy Boggiano and Grant Fritchey. 2019. What Is Query Store? Apress, Berkeley,\nCA, 1‚Äì29. doi:10.1007/978-1-4842-5004-4_1\n[3]Shaul Dar, Michael J. Franklin, Bj√∂rn √û√≥r J√≥nsson, Divesh Srivastava, and Michael\nTan. 1996. Semantic Data Caching and Replacement. In VLDB . Morgan Kaufmann,\n330‚Äì341.\n[4]Yanlei Diao, Dominik Horn, Andreas Kipf, Oleksandr Shchur, Ines Benito,\nWenjian Dong, Davide Pagano, Pascal Pfeil, Vikram Nathan, Balakrishnan\nNarayanaswamy, and Tim Kraska. 2024. Forecasting Algorithms for Intelligent\nResource Scaling: An Experimental Analysis. In SoCC . ACM, 126‚Äì143.\n[5]Bailu Ding, Surajit Chaudhuri, Johannes Gehrke, and Vivek R. Narasayya. 2021.\nDSB: A Decision Support Benchmark for Workload-Driven and Traditional Data-\nbase Systems. Proc. VLDB Endow. 14, 13 (2021), 3376‚Äì3388.\n[6]Jialin Ding, Matt Abrams, Sanghita Bandyopadhyay, Luciano Di Palma, Yanzhu Ji,\nDavide Pagano, Gopal Paliwal, Panos Parchas, Pascal Pfeil, Orestis Polychroniou,\nGaurav Saxena, Aamer Shah, Amina Voloder, Sherry Xiao, Davis Zhang, and Tim\nKraska. 2024. Automated Multidimensional Data Layouts in Amazon Redshift.\nInSIGMOD Conference Companion . ACM, 55‚Äì67.\n[7]Dominik Durner, Badrish Chandramouli, and Yinan Li. 2021. Crystal: A Unified\nCache Storage System for Analytical Databases. Proc. VLDB Endow. 14, 11 (2021),\n2432‚Äì2444.\n[8]Anshuman Dutt, Chi Wang, Vivek R. Narasayya, and Surajit Chaudhuri. 2020.\nEfficiently Approximating Selectivity Functions using Low Overhead Regression\nModels. Proc. VLDB Endow. 13, 11 (2020), 2215‚Äì2228.\n[9]Jonathan Goldstein and Per-√Öke Larson. 2001. Optimizing queries using ma-\nterialized views: a practical, scalable solution. SIGMOD Rec. 30, 2 (May 2001),\n331‚Äì342. doi:10.1145/376284.375706\n[10] Alekh Jindal, Shi Qiao, Hiren Patel, Zhicheng Yin, Jieming Di, Malay Bag, Marc T.\nFriedman, Yifung Lin, Konstantinos Karanasos, and Sriram Rao. 2018. Computa-\ntion Reuse in Analytics Job Service at Microsoft. In SIGMOD Conference . ACM,\n191‚Äì203.\n[11] Andreas Kipf, Thomas Kipf, Bernhard Radke, Viktor Leis, Peter A. Boncz, and\nAlfons Kemper. 2019. Learned Cardinalities: Estimating Correlated Joins with\nDeep Learning. In CIDR . www.cidrdb.org.\n[12] Donald Kossmann, Michael J. Franklin, and Gerhard Drasch. 2000. Cache in-\nvestment: integrating query optimization and distributed data placement. ACM\nTrans. Database Syst. 25, 4 (2000), 517‚Äì558.\n[13] Yannis Kotidis and Nick Roussopoulos. 1999. DynaMat: A Dynamic View Manage-\nment System for Data Warehouses. In SIGMOD Conference . ACM Press, 371‚Äì382.\n[14] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In SIGMOD Conference . ACM, 489‚Äì504.\n[15] Viktor Leis, Andrey Gubichev, Atanas Mirchev, Peter A. Boncz, Alfons Kemper,\nand Thomas Neumann. 2015. How Good Are Query Optimizers, Really? Proc.\nVLDB Endow. 9, 3 (2015), 204‚Äì215.\n[16] Ryan Marcus. 2023. Learned Query Superoptimization. In VLDB Workshops\n(CEUR Workshop Proceedings, Vol. 3462) . CEUR-WS.org.[17] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Nesime Tatbul, Mohammad Al-\nizadeh, and Tim Kraska. 2021. Bao: Making Learned Query Optimization Practical.\nInSIGMOD Conference . ACM, 1275‚Äì1288.\n[18] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh,\nTim Kraska, Olga Papaemmanouil, and Nesime Tatbul. 2019. Neo: A Learned\nQuery Optimizer. Proc. VLDB Endow. 12, 11 (2019), 1705‚Äì1718.\n[19] Vikram Nathan, Vikramank Y. Singh, Zhengchun Liu, Mohammad Rahman,\nAndreas Kipf, Dominik Horn, Davide Pagano, Gaurav Saxena, Balakrishnan\nNarayanaswamy, and Tim Kraska. 2024. Intelligent Scaling in Amazon Redshift.\nInSIGMOD Conference Companion . ACM, 269‚Äì279.\n[20] Parimarjan Negi, Ryan Marcus, Andreas Kipf, Hongzi Mao, Nesime Tatbul, Tim\nKraska, and Mohammad Alizadeh. 2021. Flow-Loss: Learning Cardinality Esti-\nmates That Matter. Proc. VLDB Endow. 14, 11 (2021), 2019‚Äì2032.\n[21] Luis Leopoldo Perez and Christopher M. Jermaine. 2014. History-aware query\noptimization with materialized intermediate views. In ICDE . IEEE Computer\nSociety, 520‚Äì531.\n[22] Tobias Schmidt, Andreas Kipf, Dominik Horn, Gaurav Saxena, and Tim Kraska.\n2024. Predicate Caching: Query-Driven Secondary Indexing for Cloud Data\nWarehouses. In SIGMOD Conference Companion . ACM, 347‚Äì359.\n[23] Amit Shukla and Jeff Naughton. 1999. Materialized view selection for multidimen-\nsional datasets . Ph. D. Dissertation. AAI9939712.\n[24] Divesh Srivastava, Shaul Dar, H. V. Jagadish, and Alon Y. Levy. 1996. Answering\nQueries with Aggregation Using Views. In VLDB . Morgan Kaufmann, 318‚Äì329.\n[25] Transaction Processing Performance Council. 2025. TPC-DS Benchmark. https:\n//www.tpc.org/tpcds/\n[26] Transaction Processing Performance Council. 2025. TPC-H Benchmark. https:\n//www.tpc.org/tpch/\n[27] Alexander van Renen, Dominik Horn, Pascal Pfeil, Kapil Vaidya, Wenjian Dong,\nMurali Narayanaswamy, Zhengchun Liu, Gaurav Saxena, Andreas Kipf, and Tim\nKraska. 2024. Why TPC Is Not Enough: An Analysis of the Amazon Redshift\nFleet. Proc. VLDB Endow. 17, 11 (2024), 3694‚Äì3706.\n[28] Alexander van Renen and Viktor Leis. 2023. Cloud Analytics Benchmark. Proc.\nVLDB Endow. 16, 6 (2023), 1413‚Äì1425.\n[29] Midhul Vuppalapati, Justin Miron, Rachit Agarwal, Dan Truong, Ashish Motivala,\nand Thierry Cruanes. 2020. Building An Elastic Query Engine on Disaggregated\nStorage. In NSDI . USENIX Association, 449‚Äì462.\n[30] Grisha Weintraub, Ehud Gudes, and Shlomi Dolev. 2024. Coverage-Based Caching\nin Cloud Data Lakes. In Proceedings of the 17th ACM International Systems and\nStorage Conference (Virtual, Israel) (SYSTOR ‚Äô24) . Association for Computing\nMachinery, New York, NY, USA, 193. doi:10.1145/3688351.3689165\n[31] Zongheng Yang, Wei-Lin Chiang, Sifei Luan, Gautam Mittal, Michael Luo, and Ion\nStoica. 2022. Balsa: Learning a Query Optimizer Without Expert Demonstrations.\nInSIGMOD Conference . ACM, 931‚Äì944.\n[32] Rong Zhu, Wei Chen, Bolin Ding, Xingguang Chen, Andreas Pfadler, Ziniu Wu,\nand Jingren Zhou. 2023. Lero: A Learning-to-Rank Query Optimizer. Proc. VLDB\nEndow. 16, 6 (2023), 1466‚Äì1479.\n[33] Andreas Zimmerer, Damien Dam, Jan Kossmann, Juliane Waack, Ismail Oukid,\nand Andreas Kipf. 2025. Pruning in Snowflake: Working Smarter, Not Harder.\nACM. doi:10.1145/3722212.3724447",
  "textLength": 28514
}