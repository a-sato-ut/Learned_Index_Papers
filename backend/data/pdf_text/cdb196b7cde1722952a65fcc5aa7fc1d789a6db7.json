{
  "paperId": "cdb196b7cde1722952a65fcc5aa7fc1d789a6db7",
  "title": "A Simple Yet High-Performing On-disk Learned Index: Can We Have Our Cake and Eat it Too?",
  "pdfPath": "cdb196b7cde1722952a65fcc5aa7fc1d789a6db7.pdf",
  "text": "A Simple Yet High-Performing On-disk Learned Index: Can We\nHave Our Cake and Eat it Too?\nHai Lanâˆ—, Zhifeng Baoâˆ—, J. Shane Culpepperâˆ—, Renata Borovica-Gajicâ˜…, Yu Dongâ€ \nâˆ—RMIT University,â˜…The University of Melbourne,â€ PingCAP\nABSTRACT\nWhile in-memory learned indexes have shown promising perfor-\nmance as compared to B+-tree, most widely used databases in real\napplications still rely on disk-based operations. Based on our ex-\nperiments, we observe that directly applying the existing learned\nindexes on disk suffers from several drawbacks and cannot outper-\nform a standard B+-tree in most cases. Therefore, in this work we\nmake the first attempt to show how the idea of learned index can\nbenefit the on-disk index by proposing AULID , a fully on-disk up-\ndatable learned index that can achieve state-of-the-art performance\nacross multiple workload types. The AULID approach combines the\nbenefits from both traditional indexing techniques and the learned\nindexes to reduce the I/O cost, the main overhead under disk setting.\nSpecifically, three aspects are taken into consideration in reducing\nI/O costs: (1) reduce the overhead in updating the index structure;\n(2) induce shorter paths from root to leaf node; (3) achieve better\nlocality to minimize the number of block reads required to complete\na scan. Five principles are proposed to guide the design of AULID\nwhich shows remarkable performance gains and meanwhile is easy\nto implement. Our evaluation shows that AULID has comparable\nstorage costs to a B+-tree and is much smaller than other learned\nindexes, and AULID is up to 2.11x,8.63x,1.72x,5.51x, and 8.02x\nmore efficient than FITing-tree, PGM, B+-tree, ALEX, and LIPP.\nACM Reference Format:\nHai Lanâˆ—, Zhifeng Baoâˆ—, J. Shane Culpepperâˆ—, Renata Borovica-Gajicâ˜…, Yu\nDongâ€ . 2023. A Simple Yet High-Performing On-disk Learned Index: Can\nWe Have Our Cake and Eat it Too?. In Proceedings of . ACM, New York, NY,\nUSA, 14 pages. -\n1 INTRODUCTION\nNowadays, most widely used database systems still rely on on-disk\nindexing techniques for (at least) two reasons. First, the total index\nsize may be larger than the main memory available â€“ a consequence\nof growing data sizes in real applications [ 1]. Also, multiple indexes\n(not just one index) might be built to optimize workload-specific\nperformance [ 40]; they are usually operationalized as a â€œsecondary\nindexâ€, where the leaf nodes should be included when calculating\nthe total storage requirements. Second, main memory is also a pre-\ncious resource for efficient query processing to store intermediate\nresults, e.g., a hash table in a hash join [ 9]. If most of the available\nmemory is used to hold the indexes, query performance can be\nsignificantly degraded. On the other hand, most existing learned\nThis paper is published under the Creative Commons Attribution 4.0 International\n(CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their\npersonal and corporate Web sites with the appropriate attribution.\n, ,\nÂ©2023 IW3C2 (International World Wide Web Conference Committee), published\nunder Creative Commons CC-BY 4.0 License.indexes are designed for main memory setting and try to reduce\nthe search/insert overhead via different approaches: (1) use model-\nbased search instead of binary search [ 4,35] (ğ‘‚(1)vs.ğ‘‚(logğ‘›)); (2)\nhave a smaller search range when employing a binary search [ 7,8];\n(3) reduce the tree height by modeling the data distribution [ 7,35];\n(4) support queries using cache-aware techniques [ 39]; (5) use a\ngapped array instead of a packed array [4, 35].\nWhen adapting the idea of learned indexes to a fully on-disk\nsetting, most of these techniques are no longer useful since I/O costs\nare the dominant bottleneck. For example, when issuing a lookup\nquery in a four-layer B+-tree, we find 93.6%of the total execution\ntime is spent on I/O operations. Hence, reducing the number of\nblock reads (and writes) is a critical performance factor.\nAn immediate question to ask then is how existing learned in-\ndexes perform on disk? To answer that, we implemented four state-\nof-the-art updatable learned indexes [ 4,7,8,35] on disk and com-\npared them against a standard B+-tree across six workload types\ncommonly encountered by a database. Figures 1(a)-(b) present the\nnormalized throughput on COVID andFB, which are a representa-\ntive of easy and hard dataset respectively, as per the dataset profiling\nin a recent experimental study on learned indexes [ 34]. We observe\nthat although these learned indexes exhibit different strengths and\nweaknesses depending on the workload type and dataset distribu-\ntion, none of them outperforms or achieves competitive performance\nto the B+-tree across all workload types on any dataset . This should\ncome as no surprise to database designers, given that most research\non learned indexes has focused on in-memory performance. The\nbenefit of learned indexes in main memory and the shortage of cur-\nrent learned index on disk motivate us to develop a high-performing\non-disk learned index. In the rest of this section, we will outline\nthe challenges when building on-disk learned indexes, our design\nprinciples to mitigate them, followed by our solutions that align\nwith these principles. Readers can refer to Figure 2 for an overview\nfrom challenges to our proposed design principles and solutions.\n1.1 Challenges in Building a Fully On-disk\nLearned Index\nWhen (re)implementing existing in-memory learned indexes on\ndisk, several critical challenges discussed next frequently arise. For\nthe reason of providing an intuitive illustration of these challenges,\nsome experimental comparison and analysis are highlighted in\nbetween.\nChallenge 1: a learned index cannot guarantee to reduce I/O costs\nwhen searching data on disk. Figure 1(c) shows the average number\nof inner nodes, inner blocks, and total blocks per query for Lookup-\nOnly and Scan-Only workloads on the FBdataset1. For LIPP, the\n1For a Scan-Only workload, we set the start key to the same key that was used in the\nLookup-Only workload, and then we scan forward 99keys. This ensures that ALEX,\n\n, , Hai Lanâˆ—, Zhifeng Baoâˆ—, J. Shane Culpepperâˆ—, Renata Borovica-Gajicâ˜…, Yu Dongâ€ \nFITing-treePGMALEXLIPPB+-tree#I. Nd356.71.8 (18.8)3#I. Blk33.96.5-3#L. Blk -L1.21.32.63.01#L. Blk -S21.74.124.01.5\nFITing-treePGMALEXLIPPB+-treeInner Nodes356.71.8 (18.8)3Inner Blocks33.96.5-3Leaf Blocks -Lookup1.21.32.63.01Leaf Blocks -Scan21.74.124.01.5\n# Inner Nodes# InnerBlocks# TotalBlocks (L)# TotalBlocks (S)FITing-tree534.25PGM63.95.25.6ALEX7.76.58.110.6LIPP1.8 (18.8)-324B+-tree4344.5# Inner Nodes# InnerBlocks# LeafBlocks (L)# LeafBlocks (S)FITing331.22PGM53.91.31.7ALEX6.76.52.64.1LIPP1.8 (18.8)-324B+-tree3311.5(c) Fetched NodesandBlocksforSearch on FB\n-L and S indicate lookup and scan, respectively.-We report total fetched nodes in LIPP and node count in scan is reported in the bracket (5throw).# Inner Nodes# InnerBlocks# TotalBlocks (L)# TotalBlocks (S)FITing-tree534.25PGM63.95.25.6ALEX7.76.58.110.6LIPP1.8 (18.8)-324B+-tree4344.5\nFigure 1: Throughput Comparison and Analysis. Each indexâ€™s throughput in (a)-(b) is normalized by the largest under the same\nworkload (higher is better), (c) is an analysis on the fetched blocks per query, and (d) is a latency breakdown per query.\ntotal number of fetched nodes is reported, and the number of nodes\nin the scan is highlighted in the bracket of the fifth row. When\ncombining the results for a Lookup-Only workload and a Scan-Only\nworkload in Figure 1(b), we observe that the performance rank is\ndirectly correlated with the number of fetched blocks. In contrast\nto in-memory indexes, reducing the search overhead for each step\ndoes not help on-disk indexes. Instead, reading or writing a block\nfrom/to disk is the main overhead. In a Lookup-Only workload,\namong all the learned indexes, only LIPP fetches fewer blocks than\na B+-tree and is more efficient than the B+-tree.\nWe also observe that, in contrast to the B+-tree, existing learned\nindexes have larger scan overheads, which, in practical terms,\nmeans fetching the next item becomes more expensive. For ex-\nample, to support a scan, LIPP traverses many nodes, which incurs\na higher I/O cost and leads to poorer performance.\nChallenge 2: large insertion overheads. Current indexing tech-\nniques support an insertion using four steps: (1) find a slot to hold\na new key-payload pair ( Search ); (2) do the insertion ( Insert ); (3)\ninduce an index structure modifications operation ( SMO ) if neces-\nsary; (4) update various statistics ( Stats ), such as the total number\nof lookups and insertions, which determine when to induce SMOs.\nAn SMO may create new nodes or re-construct an entire sub-tree\nduring the insertion, which is necessary for the index to allocate\nempty slots, and for a learned index to benefit from future model-\nbased operations. Figure 1(d) shows the average latency breakdown\nper query for a Write-Only workload on the FBdataset. We observe\nthat learned indexes have several shortcomings. ALEX and LIPP\nhave a large overhead when updating statistics and performing\nan SMO. The FITing-tree, PGM, and ALEX incur a large overhead\nfor the insertion. LIPP frequently induces an SMO to resolve con-\nflicts between two keys, while ALEX re-writes large nodes (the\nleaf node) in every SMO. Both lead to large SMO overheads. The\nshift operations, which are used to obtain an empty slot to store a\nnew key-payload pair in a FITing-tree, PGM, and ALEX, may span\nmultiple blocks on disk â€“ leading to more writes on disk.\n1.2 AULID â€“ Simple Is Better\nIn this paper, we show that the idea of learned index can benefit the\non-disk index design by proposing AULID ,anupdatable learned\nindex on disk. To address the above challenges, we propose five\ndesign principles for such an on-disk learned index in Section 3.1,\ndirected toward supporting lower tree heights and lower SMO\noverheads. Then, we design AULID to meet these principles by\nPGM, a FITing-tree, and a B+-tree fetch the same number of inner nodes and blocks\nneeded for a lookup and a scan.leveraging the idea of B+-tree and learned indexes in three ways\n(an overview from the challenges to our design principles and\nsolutions can be found in Figure 2):\n1Leaf Node Layout. To reduce the insertion overheads, instead of\nusing a learned model to search items for all layers, we use model-\nbased search only for inner nodes. This helps reduce the burden\non leaf nodes in maintaining the benefits of model-based search.\nSpecifically, we use a B+-tree styled layout for leaf nodes, which\nhas a low overhead when updating the index. Since the majority\nof SMOs are on leaf nodes, a lightweight SMO mechanism for leaf\nnodes, as achieved with AULID , reduces the insertion overheads\nsignificantly. A B+-tree styled leaf node design also benefits scan\noperations in fetching the next item.\n2Inner Node Layout. After building a B+-tree styled leaf node,\nthe path from the root node to a leaf node should be shorter than\nthat in a B+-tree. Otherwise, the learned index cannot outperform\na B+-tree for on-disk operations. The results in Figure 1(a)-(b) in-\nspired us to adopt the Fastest Minimum Conflict Degree (FMCD)\nalgorithm in LIPP [ 35] to reduce the tree depth. Although it is not\nthe best indexing method for most workload types, it has the small-\nest number of fetched blocks for a lookup, making it suitable for\non-disk indexes when attempting to reduce I/O costs. Moreover,\na lookup is often the first step in other operations. For example,\nin a scan operation to find the position of a start key and in an\ninsert operation to find a position to hold a new key-payload pair.\nThus, a better performance of the lookup operation should boost\nthe performance of other operations too. However, for certain work-\nload types and datasets, e.g., OSM (a hard dataset) [ 34], directly\napplying the aforementioned inner node layout still reveals several\nshortcomings â€“ a larger storage size and a lower throughput. To\novercome these shortcomings, we introduce two new inner node\ntypes and design processing algorithms upon the new layout. For\nexample, with our design, AULID is 1.18x more efficient on the\nLookup-Only workload while only taking 0.84x storage on OSM\n(as compared to a B+-tree).\n3Structural Modification Operations. With a B+-tree styled\nleaf node, AULID already manages to achieve a lighter overhead in\nmodifying the index structure due to a lower frequency in updating\nthe inner nodes and a lighter overhead in updating the leaf node.\nHowever, the tree height in some region could grow and even\nbecome larger than that of a standard B+-tree. In turn, AULID will\nhave a worse performance after lots of insertions. To avoid that\nfrom happening, we monitor the tree height of each branch and\ntrigger a re-construct process to bound their tree height if needed.\n\nA Simple Yet High-Performing On-disk Learned Index: Can We Have Our Cake and Eat it Too? , ,\nTo summarize, we make the following technical contributions:\nâ€¢To the best of our knowledge, AULID is the first approach to\nemploy the ideas of learned index to a fully on-disk setting\nto replace a traditional B+-tree2.\nâ€¢We propose five principles to guide the design of an on-disk\nupdatable learned index (Section 3.1), and carefully design\nthe indexing structure (Section 3.2 - 3.3), the query process-\ning algorithms (Section 4.1 - 4.3), and an SMO mechanism,\nto achieve efficient reads and writes (Section 4.4).\nâ€¢We implement AULID in C++, conduct comprehensive ex-\nperiments across a wide range of datasets and workloads\nand compare it against a B+-tree and on-disk implementa-\ntions of existing in-memory learned indexes. Our evaluation\nshows that AULID has competitive storage costs to a B+-\ntree and is much smaller than most other learned indexes.\nPerformance-wise, AULID achieves up to 2.11x,8.63x,1.72x,\n5.51x, and 8.02x larger throughput than FITing-tree, PGM,\nB+-tree, ALEX, and LIPP, respectively. We also conduct an\nin-depth evaluation on the benefits of the AULID design\n(Section 5).\n2 RELATED WORK\nGiven that our work focuses on developing a fully on-disk learned\nindex under the single-threaded setting, we start the literature\nreview on learned indexes outside main memory, followed by an\noverview of in-memory ones and a discussion on the difference in\nconcurrency support between in-memory case and on-disk case.\nLearned Indexes Outside Main Memory. The authors of [ 1,3]\nstudied how to use learned indexes on disk in a log-structured\nmerge tree (LSM) [ 23]. A learned model is constructed for each\nSorted Strings Table (SSTable), which is immutable after being cre-\nated. Modification operations (insert, update, delete) are supported\nin an LSM framework. Models are rebuilt and dropped during peri-\nodic compaction processes. The LSM framework supports efficient\nwrites at the cost of reads. In contrast, our work focuses on building\nan on-disk learned index with the hope of replacing the B+-tree.\nTwo most recent studies [ 19,38] focus on the larger than main\nmemory setting , where they pin part of the index in main memory\nand introduce different caching strategies. TreeLine [ 38] uses the\npartitioning algorithm proposed in PGM [ 7] to generate the leaf\nnodes and adopts a B+-tree to index them. The B+-tree in TreeLine\nis pinned in main memory, and a record-level caching strategy is\nused to cache frequently accessed items in main memory. FILM [ 19]\nbuilds a PGM index, stores it in main memory and uses one bit for\neach item to indicate the location of that item, i.e., in main memory\nor on disk. Moreover, FILM introduces a global chain and a local\nchain, to organize the segments at the last level and the items in\nthe segment based on their access time, respectively. In this way,\nFILM can quickly locate cold items. However, FILM is designed for\nappend-only insertions . Differently, our work focuses on storing the\nwhole index on disk rather than only leaf nodes.\nLu et al . [18] propose APEX, a learned index for persistent mem-\nory (PM) [ 33]. APEX is a variant of ALEX, with several tailored\ndesigns: (1) different node size settings used in APEX â€“ a larger\n2Two recent studies [ 1,3] on disk are built upon LSM tree [ 23] and suffer from poor\nread performance. Details are presented in Section 2.inner node and a smaller data node to reduce SMO overhead; (2)\na new probe-and-stash mechanism to resolve collisions without\nintroducing unnecessary nodesâ€™ access; (3) concurrency control and\nrecovery mechanisms are introduced to support simultaneous in-\nserts and instant recovery. Different from APEX, on-disk operations\nare our focus where I/O costs are the main overhead.\nLearned Indexes in Main Memory. Kraska et al . [14] was the\nfirst group to propose the idea of learned index, where a hierarchy\nof models, called RMI, was built to replace a B+ Tree for sorted\n1-d data. The new approach can achieve 3x performance boost\nand10x smaller index size. Given that RMI only supports lookup\nqueries, subsequent studies [ 4,7,8,35] address this limitation using\ntailored index structures and new mechanisms for index structure\nmodification. A FITing-tree [ 8] replaces the last layer of a B+tree\nwith model-based search, and supports insertions by introducing\nbuffers for each segment. PGM [ 7] uses a similar idea to the FITing-\ntree, but it leverages model-based search for every layer based\non an optimal partitioning algorithm [ 24]. PGM handles arbitrary\ninsertions in an LSM tree [ 23] manner. Although the FITing-tree\nand PGM leverage model-based search, additional binary search\noperations are needed. ALEX [ 4] inserts a key-payload pair using\nmodel prediction, and hence manages to have accurate predictions\nin the inner nodes without binary search. At each data node (leaf\nnode), ALEX uses a gapped array, where the key-payload pairs\nand empty slots are interleaved. The gapped array can reduce the\nfrequency of shifts for new insertions. Without an error bound for\nthe search range, ALEX uses an exponential search to find the target\nposition. Model prediction in LIPP [ 35] tends to be accurate in every\nlayer. LIPP has been shown to have better performance in practice\nthan other learned indexes in most settings [ 34]. However, LIPP\nrequires much more memory and is not efficient for a range query.\nWu et al . [36] use Normalizing Flows [ 28] to transform a dataset\nso that it can be easily modeled with linear models, and extend\nthe idea of LIPP by introducing a bucket node type. CARMI [ 39]\nis a cache-aware learned index which uses tailored partitioning\nalgorithms and a prefetching mechanism for the in-memory setting.\nTo verify the efficiency of learned indexes, several studies have\nconducted comprehensive experimental studies [ 21,34], detailed\ntheoretical analysis [6], and performance analysis [20].\nThere are also several studies that propose read-only learned\nindexes [ 11,13,30] or use the model to boost B+-treeâ€™s perfor-\nmance [ 10,17] for main memory operations. Several other learned\nindexes have also been proposed as secondary indexes [ 12,37], tai-\nlored for multi-dimensional data [ 2,5,22], spatial data [ 16,25â€“27],\nor string data [29, 32].\nLearned Index With Concurrency Support. Among these in-\nmemory learned indexes, the XIndex [ 31], FINEdex [ 15], ALEX+[ 34],\nand LIPP+[ 34] support concurrent operations. All of them use op-\ntimistic locking, which associates a versioning lock at each node.\nWith the node size being larger than a block at the leaf node, Tree-\nLine [ 38] proposes some locking strategies based on the node-level\nlock and block-level lock to support concurrent operations on the\nLSM data structure. Aligned with the latest study on the larger than\nmain memory case [ 19], we focus on the single-threaded setting in\nthis work. In a fully on-disk learned index with different sizes of\nnode and block, the node-level lock and the block-level lock need\n\n, , Hai Lanâˆ—, Zhifeng Baoâˆ—, J. Shane Culpepperâˆ—, Renata Borovica-Gajicâ˜…, Yu Dongâ€ \nChallenge 1I/O Cost for SearchChallenge 2Large Insert OverheadP1P3P2P4P5B+-tree styled leaf nodesLearned-based inner nodesPacked array & 2-layer B+-tree nodeTree adjustment based on the height\nFigure 2: An Overview of AULID : from Challenges to Design\nPrinciples and Solutions\nto be introduced at the same time. Without a tailored mechanism,\none cannot achieve a good scalability in a multi-core setting due to\nthe block-level lock. Although TreeLine [38] has introduced these\ntwo locks, its inner nodes are pinned in main memory, which is\neasier than the above case. We believe our work is an important\nfirst step and our findings will help the community in the future,\nwhen designing a fully on-disk concurrent learned index.\n3 AN OVERVIEW OF AULID\nIn this section, we first introduce the principles and highlights of\ntheAULID design â€“ addressing the challenges discussed in Sec-\ntion 1.2. Then, we present the AULID layout. Figure 2 illustrates\ntwo identified challenges and the associated design principles used\nto resolve them, as well as how these principles are reflected in our\nproposed solutions.\n3.1 Design Principles\nBased on the key properties arising from disk and learned indexes,\nwe propose a number of principles to guide the design of AULID :\nâ€¢P1. Reducing the Tree Height of the Index. Accessing each\nlevel in an index requires at least one disk access when an index\nis stored on disk. Reducing the tree height can reduce the number\nof disk access.\nâ€¢P2. Model-based Operations (Search and Insert). An index\nwith a reduced height usually has larger nodes in certain levels\nof the index. Model-based operations help AULID quickly find\nsearch keys in a specific part of the node, without the need to\naccess the entire node on disk.\nâ€¢P3. Lightweight Structure Modification Operations. Struc-\nture modification operations (SMOs) for the existing learned in-\ndexes incur a substantial amount of writes on disk. AULID should\nreduce the overhead of such SMO calls.\nâ€¢P4. Support Duplicate Index Keys. Duplicate (i.e. non-unique)\nindex keys are common in real systems. Typically, they can be\nsupported using a linked list in a main memory setting [ 35], but\nnot on disk, since it leads to additional disk reads.\nâ€¢P5. Better Scan Performance. Existing learned indexes have\ntheir own limitations when supporting scans on disk (see Fig-\nure 1(a)-(b)). AULID must provide a lightweight method to fetch\nthenext item efficiently.\n3.2 Design Highlights\nAULID uses a combination of existing and novel techniques to meet\nthe above principles and achieve high performance on disk. AULID\nconsists of inner nodes and leaf nodes, both of which are stored\nDATANODENULLâ€¦Mixed NodePacked Array NodeB+-tree NodeNode Type:Slot Type:Leaf NodesInner Nodesâ€¦â€¦MetanodeNode ANode BNode CNode DNode ENode FFigure 3: AULID Index Structure\non disk. Leaf nodes, where most SMOs occur, are organized in a\nB+-tree manner. A low update cost at leaf nodes reduces the SMO\noverhead ( P3). Moreover, AULID only uses the idea of learned index\nfor inner nodes to index the maximum key of each leaf node, which\nleads to less frequent SMOs in updating the inner nodes ( P3) and\na low tree height in inner part ( P1). Each leaf node is a packed\narray â€“ it stores pointers to its siblings and its size is equal to the\nblock size . Using the packed array and links to siblings, AULID\ncan support efficient scan operations ( P5). We optimize our inner\nnodes based on properties of the disk drive. Fast lookup time with\nthe learned model means that AULID can efficiently locate target\nleaf nodes ( P1, P2 ). To achieve robust performance on different\ndatasets (i.e., different distributions), we also introduce two new\nnode types for the inner nodes, a packed array and a two-layer\nB+-tree, with the purpose of reducing the number of SMOs for\nnon-leaf nodes ( P3). By proposing a tailored mechanism to handle\nduplicate keys inserted in inner nodes, AULID manages to store\nduplicate values with reduced on-disk costs ( P4). To maintain the\nperformance gains achieved from the learned model, AULID adjusts\nthe index structure based on the tree height and bounds the tree\nheight during insertions ( P1).\n3.3 Node Structure\nThe index structure of AULID is presented in Figure 3. Similar to\nexisting indexes, AULID is composed of two components: the inner\nnodes which store the route information to leaf nodes, and the leaf\nnodes which store the key-payload pairs.\n3.3.1 Metanode. Metanode in AULID stores (1) the physical ad-\ndress of the root node, (2) the linear model of the root node, and (3)\nthe physical address of the last leaf node, as well as the minimum\nand maximum keys of that node. We store the metanode in main\nmemory, which requires only 80bytes, a negligible main memory\noverhead.\n3.3.2 Inner Nodes. AULID has two node types in the inner part, a\nmixed node type and a packed array node type. And there are three\ntypes of slot in the inner part: NODE ,NULL , and DATA . The NODE\nslot stores the pointer to its child. The NULL slot is the empty slot\nand can be converted to NODE orDATA . The DATA slot stores the\nkey-payload pair. Each mixed node has a model to predict the slots\nfor a key search and can include three different slot types above.\nAULID stores the model in the parent node, combined with the\nphysical address. If we store the model at the starting address of a\n\nA Simple Yet High-Performing On-disk Learned Index: Can We Have Our Cake and Eat it Too? , ,\nmixed node, the large fanout for mixed nodes increases the chance\nthat the predicted position and the model are located in different\nblocks. Thus, two blocks must be fetched from disk for each level\nin the tree. In contrast, when storing the model in the parent node,\nAULID only fetches one block per level.\nTheNODE slot in AULID can be further divided into three types:\n(1) a pointer to the packed array of fixed size. That is, the first slot\nin Node A in Figure 3; (2) a small B+-tree node that contains at\nmost four child nodes â€“ the fourth slot in Node A; (3) a pointer to\nanother mixed node â€“ the sixth slot in Node A. For the first case,\nwe introduce four different packed array types, each with a fixed\nsize. The ğ‘–ğ‘¡â„packed array type can store 2ğ‘–+2items of DATA types,\nwhere ğ‘–ranges from 1to4. ADATA slot in the inner nodes stores\nthe physical address of a leaf node and the largest key it contains,\ni.e., the key-block pair. The second case is proposed to improve the\nperformance for scenarios where the number of keys to be inserted\ninto the same slot is greater than 64, but smaller than 1020 (which\nwill be explained later in Section 3.3.2). This indicates the conflict\ndegree for the region. If we create a new mixed nodeto hold these\nkeys, there can be key conflicts in the new node. This leads to a\nlarger tree height (more than two layers). Conversely, a two-layer\nB+-tree can be used to hold the nodes and help AULID to bound the\ntree height for the region. Therefore, AULID is able to bound the\nnumber of fetched blocks. Also, the design of the packed array and\nthe two-layer B+tree can support more newly inserted key-block\npairs (the routing knowledge to the leaf nodes) to be stored with\nlow overheads. Using the packed array and a two-layer B+-tree,\nAULID achieves a better empty slot ratio, and this translates to\nsmaller storage costs.\nThe B+-tree node contains only four child nodes for the follow-\ning reasons: (1) The conflict degrees in most of the test datasets\nwe have used (except for one)3is less than 1000 , which can be\nstored easily using a two-layer B+-tree. (2) A larger fanout requires\nmore metadata (pivot keys and physical addresses) in a slot, and\nit increases the total storage cost significantly. A key-block pair\noccupies 16bytes on disk in our implementation. Thus, a block\nwith 4KB can store 256pairs. The first item records the item count\nfor a two-layer B+-treeâ€™s leaf node. Four children can store at most\n1020 items.\n3.3.3 Leaf Nodes. The leaf nodes have the same structure as a\nstandard B+-tree. The DATA slot in the leaf node stores the key-\npayload pairs to be indexed. This layout design is based on the\nobservation that most SMOs happen on leaf nodes as new key-\npayload pairs are added. Learned indexes need to read all of the\nitems in a large leaf node and re-write them to disk to maintain the\nbenefits of their unique structure, which incurs large I/O costs on\ndisk. A lightweight SMO overhead for a leaf node design can help\nsignificantly reduce the number of SMO operations required (see\nthe experiments in Section 5.2.2).\nThis simple yet elegant design in the leaf nodes has many other\nbenefits. First, the link between siblings when using a packed stor-\nage layout requires no additional utility structures to perform ef-\nficiently when scan operations must locate the start of a query\nrange. Second, the storage costs of the inner nodes can be signifi-\ncantly reduced by only storing the largest keys. In our experiments,\n3We have also tested all of the datasets proposed in a recent benchmark paper [34].AULID has a similar storage size and bulkload time as a B+-tree on\ndisk, which is better than other learned indexes. Third, reducing\nthe items inserted into the inner nodes also decreases the SMO\nfrequency and the number of items that must be processed. Last,\nAULID is able to efficiently support duplicate index keys when\nusing a B+-tree styled leaf node.\n4AULID OPERATIONS\nFirst, we present how AULID supports each type of operation, and\nthen we discuss how structural modifications are supported.\n4.1 Bulkload\nAULID supports bulkload using two steps. In the first step, it creates\nleaf nodes to store the key-payload pairs using B+-tree styled leaf\nnodes. When building leaf nodes â€“ with the exception on the last\nleaf node â€“ AULID records the maximum key, and the physical\naddress for each leaf node, i.e., the key-block pairs to be indexed in\nthe inner nodes. For the final leaf node, AULID stores the minimum\nand maximum keys, as well as its physical address in a meta-node.\nThe second step builds the inner nodes for AULID over the key-\nblock pairs. We first use the Fastest Minimum Conflict Degree\n(FMCD) algorithm in LIPP to generate a linear model for a node.\nGiven the collection of keys to be indexed and the number of slots\nthat can be used, FMCD aims to generate a linear model under\nwhich the maximum number of keys inserted into the same slot is\nminimized, i.e., the smallest â€œconflict degreeâ€. Then, we insert the\nkey using the resulting model. If only one key is inserted into a\nslot, this slot is labeled as DATA and used to store the key-block\npair. Different from LIPP, AULID does not aggressively create a new\nnode if more than one item is mapped to the same slot. Instead, we\ndivide them into three cases depending on the size of the items that\nare mapped into the same slot: (1) If the size of the items mapped to\none slot is smaller than 64, a packed array is created. (2) If the size\nis greater than 64and less than 1020 (see explanation at the end of\nSection 3.3.2), a two-layer B+-tree is created with at most four child\nnodes. (3) Otherwise, a new mixed node is created to hold the keys.\n4.2 Lookup & Scan\n4.2.1 Lookup. Given a search key, we first check whether it be-\nlongs to the last leaf node by comparing it with the minimum key\nand the maximum key that are stored in the meta-node. The over-\nhead of this operation is negligible as the meta-node resides in main\nmemory. If the key belongs to the last leaf node, the leaf node is\nread from disk, and then a binary search is initiated. Otherwise, the\ninner nodes are searched to find the leaf node address where the\nsearch key should reside.\nWhen traversing from root node to leaf node, five different cases\nof model prediction can occur (assume a mixed node is the root\nnode):\nâ€¢DATA Slot: A leaf node is fetched based on the physical address\ncontained in it. If the key in the DATA slot is less than the search\nkey, then we fetch the successor.\nâ€¢NODE Slot for a Packed Array: The packed array content is\nretrieved from disk, and a DATA slot is located to hold the search\nkey. It is then processed in the same way as the DATA Slot case.\n\n, , Hai Lanâˆ—, Zhifeng Baoâˆ—, J. Shane Culpepperâˆ—, Renata Borovica-Gajicâ˜…, Yu Dongâ€ \nâ€¢NODE Slot for a B+-tree: Just as in a standard B+-tree, a child\nnode is found which holds the search key (if it exists), and then\nit is fetched and processed in the same way as the NODE Slot for\na Packed Array.\nâ€¢NODE Slot for another mixed Node: The model from the node is\nused to predict which node to access next, and the search process\nis repeated.\nâ€¢NULL Slot: Using the monotonic linear function from AULID\nand indexing the largest keys for each leaf node, to find the next\nDATA slot we must search forward. For example, given a search\nkey, suppose the predicted position is the 5ğ‘¡â„slot in Node A of\nFigure 3, which is a NULL slot. AULID will scan forward to find\nthenext DATA slot using Node C andNode F .\n4.2.2 Scan. Given a query range [ğ‘¢, ğ‘£], we first call a lookup oper-\nation to locate the leaf node where ğ‘¢should reside, and the position\nofğ‘¢in the node. Then, we scan forward until reaching the last key\nğ‘£. Using the links to sibling leaf nodes and the packed array, the\nnext item can be quickly accessed without any additional utility\nstructures, such as bitmaps in ALEX to differentiate empty slot, or\ntraversing many nodes in LIPP.\n4.2.3 Optimization for Reading. When storing only the largest key\nof each leaf node in the inner nodes, AULID could issue additional\nI/O requests for two reasons:\nâ€¢Issue 1 : When traversing the inner nodes, AULID may need to\nfind the predecessor (the left sibling of a target node) and extra\nI/O is needed to fetch that target.\nâ€¢Issue 2 : If the predicted location is a NULL slot, a scan operation\nis triggered to locate the next DATA slot. For example, if the\npredicted position is the 5ğ‘¡â„slot in Node A of Figure 3, AULID\nneeds to access Node C and Node F . Due to the large fanout\nof the inner nodes, Nodes A ,C, and Fare stored in different\nblocks, which could incur additional I/O costs. Note that with\na monotonic linear function in AULID , these two cases cannot\nhappen in a single lookup at the same time.\nTo address the first issue, if a DATA slot is found, instead of\nfetching the leaf node directly, we first check whether the key it\ncontains is smaller than the search key. If so, we scan forward to find\nthe next DATA slot. In contrast to a NULL slot, the scan operation is\ninitiated only on the currently fetched block. The DATA slot found\nbefore is used if no new DATA slots are found in the same block.\nOtherwise, scanning forwards reads at least one additional block.\nTo address the second issue, AULID fulfills the preceding NULL\nslots for one DATA slot until reaching the previous DATA node dur-\ning the bulkloading process. This operation has negligible overhead\nfor the bulkloading process while it only works with Read-Only\nworkloads. For any workload involving a write operation, inserting\na new key-block pair into the inner nodes will incur an update\nfor empty slots and hence leads to increased latency during an\ninsertion. The effectiveness of these two optimizations have also\nbeen verified in our evaluation at Section 5.4.\n4.3 Insert & Duplicate Index Keys Support\n4.3.1 Insert. The full insertion process is presented in Algorithm 1,\nand the key process is depicted in Figure 4. Given a new key-payload\npair(ğ‘˜, ğ‘), a lookup operation is first called to locate the leaf nodeAlgorithm 1: Insert(I,ğ‘˜,ğ‘)\nInput:I: the AULID index,ğ‘˜: the key,ğ‘: the payload, ğ‘‡: maximum\nslot count in the leaf node\n1accessed_nodes = [];\n2leaf_node =GetLeafNode (I,ğ‘˜);\n3ifleaf_node.sizeâ‰¥ğ‘‡then\n4 block_id ,kğ‘šğ‘ğ‘¥ =SplitNode (leaf_node); âŠ²max key and block id\nof left child;\n5ğ‘’=FindEntry (I, kğ‘šğ‘ğ‘¥,accessed_nodes ); âŠ²first non mixed\nNode entry;\n6 switchğ‘’.ğ‘¡ğ‘¦ğ‘ğ‘’ do\n7 case NULL do\n8 insert (k ğ‘šğ‘ğ‘¥,block_id ) intoğ‘’;\n9 case DATA do\n10 create a packed array PA;\n11 insert (k ğ‘šğ‘ğ‘¥,block_id ) and ( DATA .ğ‘˜,DATA .ğ‘£) into PA;\n12 updateğ‘’asNODE toğ‘ƒğ´;\n13 case B+-tree do\n14 ifğ‘’is full then\n15 ğ¾= (kğ‘šğ‘ğ‘¥,block_id )Ãitems inğ‘’;\n16 ğ‘ƒ=BuildMixedNode (ğ¾);âŠ²Build a mixed node;\n17 updateğ‘’asNODE toğ‘ƒ;\n18 else\n19 insert (k ğ‘šğ‘ğ‘¥,block_id ) intoğ‘’;\n20 case Packed Array do\n21 ifğ‘’is full then\n22 create a packed array or a B+-tree node ğ‘ƒ;\n23 insert (k ğ‘šğ‘ğ‘¥,block_id ) and items inğ‘’intoğ‘ƒ;\n24 updateğ‘’asNODE toğ‘ƒ;\n25 else\n26 insert (k ğ‘šğ‘ğ‘¥,block_id ) intoğ‘’;\n27 StatsUpdate (accessed_nodes );âŠ²Update statistics for SMOs;\n28 Adjust (T,accessed_nodes ); âŠ²See Algorithm 2;\n29else\n30 InsertLeaf (ğ‘˜,ğ‘,leaf_node );\nâ€¦ğ‘–!\"packed array(ğ‘–+1)!\"packed arrayâ€¦â€¦â€¦two-layer B+-treeâ€¦mixednode\nFigure 4: AULID â€™s Insertion Process\nthat should contain ğ‘˜and the position of ğ‘˜in the node (Line 2). If\nthe item count in this leaf node is less than a predefined threshold\nğ‘‡,(ğ‘˜, ğ‘)is added into this node (Line 30). Note that, if this is the\nlast leaf node, and (ğ‘˜, ğ‘)was the first or last item in the node, the\nminimum key or maximum key is updated in the meta-node. If the\nitem count exceeds the threshold, a splitting process is triggered\n(Line 4). Unlike a B+-tree, which keeps the smaller half of the items\nin the original node, AULID keeps the larger half of the items in\nthe original node. Otherwise, the address of the last key is updated\nin the original inner node, which requires extra writes. After a new\nleaf node is created, the links to the sibling nodes are also updated.\n\nA Simple Yet High-Performing On-disk Learned Index: Can We Have Our Cake and Eat it Too? , ,\nAlgorithm 2: Adjust(I,ğ‘)\nInput:I: the AULID index,ğ‘: accessed mixed nodes\n1foreachğ‘›âˆˆğ‘do\n2 ifğ‘›.sizeâ‰¥ğ›½Â·ğ‘›.init_sizeâˆ§ğ‘›.l3_itemâ‰¥ğ›¼Â·ğ‘›.sizethen\n3ğ¾= items (inner part only) in the subtree rooted as ğ‘›;\n4ğ‘ƒ=BuildMixedNode (ğ¾);\n5 update the pointer to ğ‘›in its parent to point to ğ‘ƒ;\n6 StatsUpdate (ancestors of P); âŠ²This step can be merged\nwith Line 27 in Algorithm 1 to reduce the write overhead;\nAfter the leaf node is generated, it is indexed (the largest key and\nphysical address) in the inner nodes (Lines 5-28). A lookup process\nis initiated to find the first non-mixed node slot to hold the new\nkey. Just as in the search process, there are five different cases: 1) If\nwe encounter a NULL slot, we insert the new key-block pair into it\nand the insertion is completed (Line 8). 2) If we encounter a NODE\nslot pointing to a mixed node, the model is fetched and the search\nprocess is repeated. 3) For a packed array, if full, a larger packed\narray type will be allocated. If the maximum supported packed\narray is already being used, it is converted into a two-layer B+-tree\n(Line 20-24). Otherwise, we insert the new key-payload pair into\nthe empty slot and complete the process (Line 26). 4) If the B+-tree\nis not full, the process proceeds as in a standard B+-tree (Line 19).\n5) Otherwise, it is converted into a mixed node (Lines 15-17).\nAfter completing the insertion, the statistics of the mixed nodes\n(accessed_nodes ) are updated in the access path to guide later SMOs\n(Line 27). AULID records the number of items in a third layer or\na deeper layer. Finally, we check if we need to initiate an SMO\noperation by calling the Adjust function (Line 28).\n4.3.2 Handling Duplicate Keys. Duplicate keys are common in\nreal databases. A linked list used in main memory is however not\nappropriate when on-disk, as it leads to additional I/O costs when\nfetching items from the list.\nUsing a B+-tree styled leaf node, AULID can efficiently store\nand search for duplicate keys in the leaf nodes. This process is the\nsame as a standard B+-tree. If a duplicate key must be inserted\ninto the inner nodes of AULID , one potential way is to directly\ninsert the duplicate key into the inner nodes. AULID can handle\nkey conflicts using the packed array/two-layer B+-tree proposed\nearlier. However, in this case, the maximum number of duplicate\nkeys that can be supported is 163,840for a block size of 4KB4.\nUsing the link between two sibling leaf nodes, another way is that\nwe can only store the first leaf nodeâ€™s address for the duplicate key.\nWith the larger half of items stored in the original block after the\nleaf node is split, the address (block number) stored in the original\nslot in the inner nodes needs to be updated. The write overhead is\nthe same as the last case while we can support an arbitrary number\nof duplicate keys.\n4.4 Structural Modification of Inner Nodes\nAs shown in Figure 4, in AULID , inserting new key-block pairs into\nthe packed array and the two-layer B+-tree will not increase the tree\n4A B+-tree node with 4children can store at most 640leaf node addresses and each\nleaf node can store 256items.height, i.e., not incur an additional I/O request. However, when a\ntwo-layer B+-tree node is converted into a new mixed node, certain\nregions may have a larger tree height. In our test datasets with up\nto800M key-payload pairs, a B+-tree has at most three layers in\nthe inner nodes. Therefore, the index structure must be carefully\nmodified to bound the height of the branches in AULID â€™s inner\nnodes to at most three layers. Otherwise, a B+-tree on disk will be\nthe best. Packed arrays and two-layer B+-tree nodes do not have an\nimpact on the tree height. Here, we focus on when to re-construct\nmixed nodes in AULID and how to perform a re-construction.\n4.4.1 When Should the Rebuilding Occur? To bound the tree height\nof the inner nodes and avoid aggressive node updates, we introduce\ntwo new constraints to determine when a mixed node (Line 2 in\nAlgorithm 2) should be reconstructed.\nCriterion 1 : the percentage of the items in a subtree rooted at node\nnin the third layer or a deeper layer ( l3_item ) is larger than ğ›¼. This\nguarantees that no more than ğ›¼leaf nodes can have a longer path\nthan a B+-tree on disk, with a high probability.\nCriterion 2 : The number of current items rooted at node nis larger\nthan ğ›½times the initial size.\nIn corner cases where a region has a high degree of conflict, a\nmixed node can have more than ğ›¼items, even when it is initially\nbeing created. To avoid reconstructing this node frequently, we\nadjust it after observing a sufficient number of new items.\nA smaller value for ğ›¼andğ›½leads to a more frequent node recon-\nstruction. By default, we set ğ›¼=0.05andğ›½=1.2to balance the\ntree height, i.e., lookup performance and SMO overheads. We have\nverified the impact of different settings for ğ›¼andğ›½in Section 5.4.3.\n4.4.2 How to Reconstruct a Node. If a mixed node meets both of\nthe above criteria, all key-payload pairs stored in the inner nodes\nrooted at that node are collected, and then the bulkload process is\ncalled again to build a new mixed node.\n4.5 Other Operations\nTo support a delete operation, AULID first locates the items to be\ndeleted at the leaf node, and then deletes it in the same manner as a\nstandard B+-tree. If no SMO is required (merging the sibling nodes),\nthe delete operation is finalized. In this case, even if we delete the\nlast key-payload pair in the leaf node, AULID still does not update\nthe inner nodes. If a merge is required, a delete operation in the\ninner nodes is required. If the key-block pair to be deleted is in a\nmixed node, this slot is marked as an empty slot. If it is contained\nin the packed array or a two-layer B+-tree node, it will be removed.\nAULID will convert the packed array or a two-layer B+-tree node\ninto a DATA slot if there is only one key-block pair remaining.\nThere are two types of update operations, updating the payload\nand updating the key. In the former, an in-place update is used5. For\nthe latter, a delete operation and an insert operation are initiated.\n5 EXPERIMENTS\nWe have conducted extensive experiments to answer the following\nquestions:\nQ1:How good is AULID as compared to other learned indexes and\na B+-tree when disk-resident?\n5Currently, AULID supports key-payload pairs of a fixed length.\n\n, , Hai Lanâˆ—, Zhifeng Baoâˆ—, J. Shane Culpepperâˆ—, Renata Borovica-Gajicâ˜…, Yu Dongâ€ \nQ2:How well does AULID scale to large datasets?\nQ3:Do the proposed index structure design and structural modifi-\ncation operation help improve the performance?\nWe start with the experimental setup as described in Section 5.1.\nThen, we present our answers to Q1-Q3 . To answer Q1, we compare\nAULID against five competitors across six different workload types\nand four different datasets. We demonstrate that AULID is superior\nin terms of throughput and storage cost in Section 5.2. To answer Q2,\nin Section 5.3 we use another four datasets with 800M keys to study\nthe performance of AULID on large datasets of varying hardness.\nNotably, most existing studies [ 4,19,35,38] use at most 200M key-\npayload pairs to study index performance. Finally, to answer Q3,\nin Section 5.4 we compare AULID to its variants, with and without\nthe proposed data structures and structure modification operations,\nin order to reveal the performance benefits of our proposed design\nchoices.\n5.1 Experimental Setup\n5.1.1 Baselines. We implement a standard B+-tree and four state-\nof-the-art (SOTA) updatable in-memory learned indexesâ€“PGM [ 7],\nFITing-tree [ 8], ALEX [ 4], and LIPP [ 35]â€“ all modified to work on\ndisk. To improve the FITing-Treeâ€™s performance and reduce the\nsegment count, we replace the greedy partitioning algorithm in\nthe FITing-Tree with a streaming algorithm [ 24] originally used in\nPGM. Additionally, to support arbitrary insertions, we support a\nDelta Insert Strategy [8] in the FITing-tree, which allocates a buffer\nfor each segment. LIPP and ALEX use their default settings. For\nPGM and FITing-tree, we set the error bound as 64, where they\nachieve good performance in most test cases. PGM supports the\ninsertion operation via the same mechanism as the studies [ 1,3],\nand hence PGM can also reflect their pros and cons.\n5.1.2 Datasets. The most recent experimental study [ 34] on memory-\nresident learned indexes introduced 11real datasets in their evalua-\ntion. Based on its profiling results, these datasets can be roughly\ndivided into four categories (see Figure 2 in [ 34]):C1: Globally\neasy and locally easy, C2: globally normal and locally normal, C3:\nglobally normal and locally hard, and C4: globally hard and locally\nnormal. We select one dataset from each of these categories for our\nexperiments: COVID (C1),PLANET (C2),GENOME (C3), and OSM\n(C4). Each dataset has 200M keys of type uint64 . The performance\nofAULID and LIPP correlates to the conflict degree (the maximum\nnumber of keys being inserted into the same slot) in one dataset\ndue to the usage of the FMCD algorithm. Consequently, datasets\nwith a greater conflict degree are more challenging for AULID and\nLIPP. A summary of the conflict degrees of the tested datasets is\npresented in Table 1.\nTable 1: Conflict Degree of Each Dataset\nDataset COVID PLANET GENOME OSM\nConflict Degree 27 22 585 4 ,106\nTo test the scalability of AULID on large datasets, we use OSM\nfrom [ 21], which contains 800Muint64 keys. The generator pro-\nposed in [ 34] is used to generate another three datasets, each of\nwhich has different levels of hardness (details presented in Sec-\ntion 5.3). All of the generated datasets contain 800Muint64 keys.\nFigure 5: Throughput on Lookup-Only Workload (W1).\nIt is worth highlighting that, as compared to the latest work on\nthe larger than the main memory setting [ 19,38] where at most\n30M keys are used, the number of keys in each dataset used in our\nexperiment is much larger.\nFor all datasets, we generate a uint64 payload for each key with\nkey plus 1as their value. The first four datasets require 2.98GB\nof storage space on disk, and the last four datasets (used for the\nscalability testing) require 11.92GB.\n5.1.3 Workloads. We compare AULID against all baselines across\nsix different workload types typically encountered in a database.\nW1 - Lookup-Only workload, where each index is built on 200M\nkey-payload pairs and the workload contains 20,000randomly sam-\npled search keys.\nW2 - Scan-Only workload, where the start key of the search range\nis set to the same key in the lookup-only workload, and the search\nrange is set to 100. The queries are issued on indexes prebuilt on\nthe full dataset.\nW3 - Write-Only workload, where the initial index is built with\n10M key-payload pairs that are randomly selected from a dataset,\nand then another 10M key-payload pairs are inserted.\nW4 - Read-Heavy workload includes 90%lookup queries and 10%\nwrite operations.\nW5 - Balanced workload consists of 50%lookup queries and 50%\nwrite operations.\nW6 - Write-Heavy workload includes 90%write operations and\n10%lookup queries.\nWe refer to W4-W6 as mixed workloads, with the only difference\nbetween them being the ratio between reads and writes. For mixed\nworkloads, the initial index is built over 10M key-payload pairs\nrandomly sampled from a dataset, and then lookup queries and\nwrite operations are issued ( 10M queries in total). The search keys\nin all mixed workloads are randomly sampled from the existing\nkeys of an index.\n5.1.4 Metrics & Environment. The primary metric we measure\nis throughput. We also report the number of fetched blocks, the\nstorage size of each index, and the tail latency. We conduct the\nexperiments on a SATA HDD using a Red Hat Enterprise Server 7.9\non an Intel Xeon CPU E5-2690 v3 @ 2.60GHz with 256GB memory\nand a 1TB HDD. The block size is 4KB in all experiments.\n5.2 Efficiency Comparisons on Disk\nIn this section, we compare AULID against four state-of-the-art\nlearned indexes, and a B+-tree on disk. AULID outperforms all five\nindexes on every dataset and workload tested.\n5.2.1 Lookup-Only Workload. Figure 5 shows the throughput and\nthe average number of fetched blocks per query, for each index.\nOverall, AULID is the most efficient indexing method. Specifically,\n\nA Simple Yet High-Performing On-disk Learned Index: Can We Have Our Cake and Eat it Too? , ,\nFigure 6: Throughput on Scan-Only Workload (W2).\nit achieves up to 1.68x,2.10x,1.62x,1.76x, and 1.55x higher through-\nput than the FITing-tree, PGM, B+-tree, ALEX, and LIPP, respec-\ntively. LIPP is the second most efficient index across the majority\nof datasets. The performance of each index is directly correlated\nto the number of fetched blocks where more fetched blocks lead\nto a lower throughput. The FITing-tree, PGM, and ALEX cannot\noutperform the B+-tree on disk even on the COVID dataset, which\nis considered to be an easy dataset as per [34]. The improvements\nfrom each search step attained in the main memory setting â€“ e.g.,\na smaller search range in FITing-tree and PGM, or model-based\nsearch in ALEX â€“ do not provide any tangible benefits for on-disk\noperations if they cannot reduce the number of fetched blocks.\nThe performance of the FITing-tree, PGM, and B+-tree is similar\nacross all datasets. ALEX however has the worst performance on\nOSM . The performance of AULID and LIPP vary across different\ndatasets; specifically, the performance is related to the conflict degree\nof a dataset, where a higher number of conflicts usually leads to a\ngreater tree height, and in turn more fetched blocks.\n5.2.2 Scan-Only Workload. Figure 6 summarizes the throughput\nand the average number of fetched blocks for the Scan-Only work-\nload. In terms of throughput, AULID outperforms FITing-tree, PGM,\nB+-tree, ALEX, and LIPP by up to 2.11x,2.44x,1.65x,3.04x,7.94x,\nrespectively. Just as in the Lookup-Only workload, the performance\nof the scans is determined by the number of fetched blocks.\nTo support a scan query, all indexes first initiate the search pro-\ncess for a lookup query to locate the start key in the search range,\nand then scan forward until reaching the end key. Consequently,\nbetter performance in Lookup-Only workloads yields better per-\nformance in Scan-Only workloads. Using the packed array in leaf\nnodes and links between siblings, the B+-tree and AULID reap the\nbenefits from efficient lookup queries, and are the two top perform-\ning algorithms. In contrast, LIPP does not gain any benefits from\nlookup queries. LIPP only has one node type, where key-payload\npairs, pointers to child nodes, and empty slots are all interleaved.\nThus, when fetching the next item, LIPP may have to traverse mul-\ntiple nodes. Since LIPP has a large fanout, there is a greater chance\nthat these nodes are in different blocks. Also, we observe that the\nperformance of ALEX decreases more quickly than the FITing-tree\nand PGM. This is because with a gapped array in the leaf node,\nALEX uses a bitmap to indicate whether a slot is occupied, and thus\nincurs additional I/O cost when fetching it.\n5.2.3 Write-Only and Mixed Workloads. Figure 7 shows the through-\nput for the workloads that include write operations. AULID is still\nthe best performer across all workloads and datasets. The superior-\nity of AULID is attributed to three reasons: (1) a lower latency to\nlocate where a new key-payload should be inserted, i.e., benefiting\nfrom the best lookup performance; (2) a lower SMO overhead on\nFigure 7: Throughput of Mixed Workloads (W3-W6).\nFigure 8: Comparison of Bulkload Time and Storage Usage.\nleaf nodes with the B+-tree styled leaf node design; and (3) a lower\nSMO overhead for the inner nodes, and fewer SMOs required. Based\non our design of the packed array and the two-layer B+-tree nodes,\nmost new key-block pairs can be stored without creating new mixed\nnodes. In the tested datasets, no dataset required AULID creation\nof new mixed nodes. Other learned indexes, however, require more\nSMOs, e.g., on the Write-Only workload, ALEX and LIPP require\n45,897and4.5M SMOs on GENOME , respectively.\nFrom the other learned indexes, PGM outperforms other ap-\nproaches on the Write-Only workload, but it performs worse when\nthe ratio of read queries increases. Better insertion support stem-\nming from the LSM tree [ 23] allows PGM to be competitive for write\noperations. However, since multiple files are maintained as static\nPGM indexes, PGM may access more than one file for a lookup\nquery, which increases the I/O cost. The performance gain from\na faster lookup time can benefit the workloads containing more\nreads, e.g., the FITing-tree and LIPP on the Read-Heavy workload.\nHowever, as the number of writes increases, the SMO overhead\nand the cost of updating statistics (for ALEX and LIPP) [ 34] can\noutweigh the benefits gained from faster lookups.\n5.2.4 Bulkloads. Figure 8 reports the bulkloading time and the\non-disk index size after bulkloading. When calculating index sizes,\ninstead of only reporting the inner node sizes, we report the total\nsize of the index file on disk. This ensures that the entire on-disk size\nis reported for a fair comparison in practice. In terms of bulkloading\ntime, AULID is similar to the B+-tree, and both of them are signifi-\ncantly smaller than the other indexes. AULID also achieves similar\nstorage cost to the B+-tree. The FITing-tree and LIPP have different\n\n, , Hai Lanâˆ—, Zhifeng Baoâˆ—, J. Shane Culpepperâˆ—, Renata Borovica-Gajicâ˜…, Yu Dongâ€ \nFigure 9: Storage Occupancy of Mixed Workloads (W3-W6).\nstorage sizes across different datasets. In the case of FITing-tree,\nharder datasets will create more leaf nodes (segments), and allocate\nadditional buffers on disk for later key-payload pair insertions. For\nLIPP, a dataset with a larger degree of conflict will result in more\nnodes being created on disk, and in turn occupy more space. ALEX\nand LIPP have larger bulkloading times than the other methods due\nto model training, and more on-disk writes.\n5.2.5 Index Size. Figure 9 presents the storage occupancy of all in-\ndexes after finishing workloads comprising writes ( W3-W6 ). Over-\nall,AULID achieves similar storage overheads to the B+-tree across\nall workload and dataset combinations. Among the rest of the com-\npetitors, PGM has the smallest storage size. This is attributed to\nthe LSM tree used in PGM to support arbitrary insertions, i.e. after\nan index has been merged, we can delete it from disk. For LIPP, a\ndataset with a higher degree of conflict usually has a larger storage\ncost due to the creation of additional nodes. The FITing-tree has a\nlarge space occupancy, regardless of the dataset or workload. For a\nhard dataset, re-segmenting a leaf node can generate many more\nleaf nodes compared to an easy one. This results in more buffers\nbeing created. For an easy dataset, a leaf node holds more items.\nThus, each SMO operation writes more blocks on disk.\n5.2.6 Tail Latency. To study the robustness of each index, in Fig-\nure 10 we report the p 99latency and standard deviation on the\nLookup-Only and Write-Only workloads. Overall, AULID has the\nsmallest p 99latency in the Lookup-Only workload. AULID , PGM,\nand B+-tree have similar p 99latencies for the Write-Only work-\nload â€“ all of which are better than the FITing-tree, ALEX, and LIPP.\nHowever, all learned indexes have a larger standard deviation than\nthe B+-tree across both workloads. Due to an unbalanced tree struc-\nture of LIPP and ALEX, accessing some regions may issue more\nI/O requests for the Lookup-Only workload. When indexing only\nthe largest key of each leaf node in the inner nodes, in a lookup,\nAULID may access more blocks to fetch the next DATA slot or read\nan extra block to locate the target leaf node as discussed in Sec-\ntion 4.2.3. PGM will periodically merge items into a larger index.\nHeavy SMOs for certain queries result in a larger latency in the\nWrite-Only workload, which in turn results in a larger variance.\nFigure 10: Tail Latency on Lookup-Only (W1) and Write-Only\n(W3) Workloads.\nFigure 11: Throughput Speedup on Large Datasets.\n5.3 Scalability Test\nIn this section, we study the performance of AULID on large scale\ndatasets using different workload types.\nSetting. Since existing learned indexes perform worse than the B+-\ntree overall, in this section, we compare the scalability of AULID\nagainst the B+-tree only. To test the performance of AULID on\ndatasets of different hardness, we include OSM800 [21], and three\nother datasets of size 800M generated using the method from [ 34].\nFor each, we set the local hardness and global hardness to 4x\nofCOVID ,PLANET , and GENOME and name them as COVID800 ,\nPLANET800 , and GENOME800 , respectively.\nFor the Lookup-Only and Scan-Only workloads, we issue 800,000\nqueries over the index built on the entire dataset. Search keys are\nrandomly sampled from the entire dataset. For the workloads that\ncontain writes, we build an initial index containing 150M keys\nsampled from a dataset and issue a total of 50M operations, where\nthe write ratio is the same as used for W3-W6 in Section 5.1.3.\n5.3.1 Performance Speedup. Figure 11 presents the throughput\nspeedup of AULID compared to the B+-tree, across the four large\ndatasets. AULID beats the B+-tree with up to 1.75x throughput\ngains on all tested workloads and datasets. AULID and B+-tree\nhave the same leaf node layout. Due to a carefully designed inner\nnode structure and an SMO mechanism to bound the tree height,\nAULID is more efficient when locating the target leaf node, and\nalso benefits scans and writes.\nThe superiority of AULID on large datasets also comes from the\nsmaller SMO overhead for write operations. When indexing the\nlargest key for each leaf node of the learned model, AULID reduces\n\nA Simple Yet High-Performing On-disk Learned Index: Can We Have Our Cake and Eat it Too? , ,\nFigure 12: Bulkload Time and Storage Usage Compared to\nB+-tree on Large Datasets.\nthe number of SMOs needed to reap the benefits of model-based\nsearch. Moreover, a packed array and a two-layer B+-tree hold\nthe new key-block pairs without increasing the tree height (See\nFigure 4), while incurring only small update overheads.\n5.3.2 Bulkload Time & Storage Usage. We report the bulkloading\ntime and index size for AULID and B+-tree in Figure 12. To build an\nindex for 800M key-payload pairs on disk, the B+-tree takes around\n20ğ‘ andAULID is competitive at 27ğ‘ . Both are much more efficient\nthan other learned indexes, even on small datasets.\n5.4 An In-depth Study of AULID Design\nIn this section, we study how the design of inner nodes meets our\nproposed design principles and address the two challenges afore-\nmentioned in Section 1.1. Typically, basic operations include the\nlookup and insertion, which in turn define the studied workload\ntypes ( W1, W3-W6 ) and are also the key step in W2. Specifically,\nwe first study the impact of the AULID design on these two opera-\ntions, and then investigate the impact of the adjustment strategy\nproposed in Section 4.4.\n5.4.1 Impact of Different Designs on Lookup-Only Workloads. To\nstudy the effectiveness of the proposed design, we compare AULID\nagainst LIPP-B+ â€“ an approach which directly adopts LIPP as the\ninner nodes, and organizes the leaf nodes in the same vein as B+-\ntree. We report the throughput of the Lookup-only Workload ( W1)\nin Table 2. Across all datasets, AULID outperforms LIPP-B+ and\nfetches fewer blocks.\nTable 2: Throughput Comparison of AULID and LIPP-B+ on\nLookup-Only Workload (W1).\nMetric Index COVID PLANET GENOME OSM\nThroughputLIPP-B+ 158,489 133,404 153,851 104,659\nAULID 164,897 141,543 163,825 123,749\nBlocksLIPP-B+ 2.15 2.72 2.25 3.30\nAULID 2.07 2.50 2.07 2.96\nThe performance of AULID is attributed to our read optimization\nstrategies (Section 4.2.3), packed array design , and two-layered B+-\ntree nodes (Section 3.3.2). The first helps reduce the number of\nblocks being fetched, and the latter two help reduce the tree height.\nBenefits of Read Optimizations. As discussed in Section 4.2.3, in\nAULID , there are two cases that may incur additional I/O costs: Case\n1- located in the predecessor of the target leaf node and require an\nadditional block being fetched; Case 2 - located in a NULL slot but\nneed to scan forward until the next DATA slot is found. For the first\ncase, AULID scans forward ( ScanFward ) to determine whether the\ncurrent block has another DATA slot. For the second case, AULIDTable 3: Extra Fetched Blocks under Different Optimizations\nDataset w/o Opt. Fulfill ScanFward Fulfill & ScanFward\nCOVID 26,107 18,337 9,266 277\nPLANET 59,711 52,619 30,090 21 ,027\nGENOME 47,229 40,727 9,456 710\nOSM 30,148 22,368 23,232 14 ,924\nfulfills ( Fulfill ) the empty slot with the next DATA slot during a\nbulkloading process.\nWith the Fulfill optimization, AULID avoids accessing extra\nblocks in Case 2 . Thus, all of the fetched extra blocks in the Fullfill\noptimization are stemming from Case 1 (the third column in Table 3).\nFrom Table 3, we observe that most additional fetched blocks are\nfrom Case 2 .\nWith the ScanFward optimization, AULID significantly reduces\nthe number of extra fetched blocks for COVID ,PLANET , and GENOME .\nWith the Fulfill optimization, AULID avoids fetching extra blocks\ninCase 1 .\nWhen enabling both operations, AULID can reduce the number\nof extra fetched blocks by at least 50%, particularly for COVID and\nGENOME . By default, we only enable the ScanFward optimization,\nwhich can reduce 0.08,0.15,0.18, and 0.03blocks per query for\nCOVID ,PLANET ,GENOME , and OSM , respectively. Thus, improve-\nments in AULID onCOVID andGENOME in Table 2 are produced\nbyScanFward optimization only.\nTable 4: Impact of Packed Array and Two-Layer B+-tree on\nthe Average DATA Slot Height and Storage.\nMetric Index COVID PLANET GENOME OSM OSM800\nAvg. HeightLIPP-B+ 1.00 1.60 1.01 2.29 2.28\nAULID 1.00 1.36 1.00 1.83 1.93\nStorage (GB)LIPP-B+ 4.27 4.47 4.28 5.11 19.51\nAULID 4.27 4.28 4.27 4.29 15.77\nBenefits of Data Structure Design. The design of the packed\narray and the two-layer B+-tree node in AULID helps further reduce\nthe number of fetched blocks for the PLANET andOSM datasets\ndue to the lower tree height (as compared to LIPP-B+).\nTable 4 reports the average node heights after a bulkload on\nCOVID andGENOME .AULID and LIPP-B+ both have the minimal\naverage node height, where most DATA slots are located in the first\nlevel. However, for datasets with a larger conflict degree ,AULID\nhas a smaller average node height. This is because LIPP-B+ creates\nmore nodes to eliminate the number of conflicts in each dataset\nwith a larger conflict degree , which leads to a greater height. A\nlower tree height can be achieved with the packed data structures\ndiscussed above, and thus AULID requires less storage space than\nLIPP-B+ on the hard datasets as shown in Table 4.\nTable 5: Throughput Comparison of AULID and LIPP-B+ on\nWrite-Only Workload (W3)\nMetric Index COVID PLANET GENOME OSM\nThroughputLIPP-B+ 111,017 100,430 109,631 76,865.4\nAULID 111,669 104,816 107,661 91,707.1\n5.4.2 Packed Array, Two-Layer B+-tree Nodes, and B+-tree Styled\nLeaf Nodes for Write-Only Workloads. Table 5 reports the through-\nput for the Write-Only workload ( W3). We observe that AULID\n\n, , Hai Lanâˆ—, Zhifeng Baoâˆ—, J. Shane Culpepperâˆ—, Renata Borovica-Gajicâ˜…, Yu Dongâ€ \nFigure 13: Latency Breakdown of Write-Only Workload (W3).\nFigure 14: Latency Breakdown of Append-Only Workload.\nFigure 15: Inner Nodesâ€™ Latency Breakdown of Append-Only\nWorkload.\nand LIPP-B+ have similar performance for COVID ,PLANET , and\nGENOME , but AULID outperforms LIPP-B+ by 1.19x on OSM .\nTo further understand the performance, we break down the\ninsertion process into three steps: (1) the search step ( Search ) to\nfind the leaf node that will hold the new key-payload pair, (2) the\ninsertion step on a leaf node ( Leaf ), and (3) the update step in the\ninner nodes ( Inner ). From Figure 13, we can see that the main\noverhead is brought by the first two steps. The overhead from\nindexing the new key-block pairs in the inner nodes ( Inner ) is\nnegligible. AULID and LIPP-B+ update the inner nodes when a\nleaf node is split. When compared against LIPP, both have fewer\nSMO operations. For example, on GENOME ,AULID and LIPP-B+\nonly require 49,038SMOs on the leaf nodes, where the leaf nodes\nmust be split; in contrast, LIPP requires 4.6M SMOs and most of\nthem are caused by creating LIPP nodes to eliminate conflicts. On\nOSM ,AULID has a more efficient Search step, which contributes to\nthe higher throughput, as compared to LIPP-B+. The design of the\npacked array and the two-layer B+-tree nodes in AULID consistently\nresults in shorter paths to leaf nodes, and hence smaller search cost.\nTable 6: Throughput Comparison of AULID and LIPP-B+ on\nAppend-Only Workload\nMetric Index COVID PLANET GENOME OSM\nThroughputLIPP-B+ 144,432 153,060 152,633 153,214\nAULID 182,732 188,015 187,886 184,051\nHot Region Insertions. Another potential problem with LIPP are\ninsertions in a hot region which occur in the inner nodes. This\nproduces a high number of conflicts and triggers additional SMOs.\nWe use here an Append-Only workload to analyze this case. The\nthroughput and latency breakdown for this case are shown in Ta-\nble 6 and Figure 14, respectively. To support new key-payload pairs,\nAULID and LIPP-B+ index the last leaf node in the meta-node. Thus,\nthey have similar performance on the Search andLeaf in Figure 6.\nAULID has a lower latency when updating the inner nodes ( Inner )\nacross all datasets.\nFigure 16: Throughput under Different Settings. The dashed\nline indicates the throughput of the corresponding workload\nwith the same color without any adjustments.\nTo study the overhead of the inner nodes, we further break down\nthe process of the inner nodes into five steps: (1) search to find a\nslot to hold the new key-block pair ( Search ), (2) create a new node\nor convert the node type ( Create ), (3) insert the key-block pair into\naDATA slot (Insert ), (4) adjust the tree structure ( Adjust ), and (5)\nupdate the statistics for later adjustment operations ( Update ).\nIn Figure 15, we observe that, except for the Insert ,AULID has\na lower latency than LIPP-B+. The packed array and two-layer\nB+-tree node design of AULID yield a shorter path to target slots\nholding the new key-block pairs, which contributes to lower search\nand update times. Moreover, conflicts in the Append-Only workload\nrequire more operations when creating LIPP nodes and adjusting\ntree structures in LIPP-B+. For example, AULID only requires 1,163\nand37SMOs in creating new nodes and adjusting the tree structure,\nwhile LIPP-B+ requires 38,837and1,625SMOs, respectively.\n5.4.3 Adjustment Study. Last, we study the effectiveness of index\nadjustments and provide an analysis on the parameters in AULID\nâ€“ğ›¼andğ›½presented in Algorithm 2. We use the OSM dataset, and\nbuild an initial index using 50M keys. Then, another 50M queries\nare issued with different write ratios, just as in W3-W6 . To study\nthe impact of ğ›¼andğ›½, we set ğ›½=1.07andğ›¼=0.0025 as the default.\nUsing small default values makes the corresponding condition true\n(Line 2 in Algorithm 2), and hence we can study the impact of other\nparameters in isolation. Figure 16 illustrates the throughput when\nusing different settings. We also report the throughput without ad-\njusting the index (the dashed lines). In Figure 16, we observe: (1) The\nindex adjustment in AULID significantly improves the throughput,\nespecially for workloads with write operations. (2) Larger values of\nğ›¼andğ›½usually result in worse performance, and the workloads\nwith more writes are more sensitive to these two parameters. If\nindex is not adjusted, certain regions in the inner nodes can result\nin longer paths to the leaf nodes. This incurs more reads to fetch\nleaf nodes and more writes to update statistics on disk. We find that\nthe default values of ğ›¼(0.05) and ğ›½(1.2) inAULID result in good\nperformance across all tested workloads.\n6 CONCLUSION\nIn this paper, we propose AULID , a novel simple yet efficient on-\ndisk learned index. In contrast to in-memory indexes, I/O cost is\nthe main bottleneck for disk-resident indexes. Toward that end, we\npropose five principles to build a learned index on disk, focusing\non reducing the I/O cost. We carefully craft the index structure,\npropose the query processing algorithms, and introduce an index\nadjustment mechanism to meet the proposed principles. AULID\noutperforms all of the baselines across a wide range of settings.\n\nA Simple Yet High-Performing On-disk Learned Index: Can We Have Our Cake and Eat it Too? , ,\nOur evaluation shows that AULID has competitive storage cost to\nthe B+-tree (the smallest of the alternatives), and achieves up to\n2.11x,8.63x,1.72x,5.51x, and 8.02x higher throughput than the\nFITing-tree, PGM, B+-tree, ALEX, and LIPP respectively.\n\n, , Hai Lanâˆ—, Zhifeng Baoâˆ—, J. Shane Culpepperâˆ—, Renata Borovica-Gajicâ˜…, Yu Dongâ€ \nREFERENCES\n[1]Hussam Abu-Libdeh, Deniz AltinbÃ¼ken, Alex Beutel, Ed H. Chi, Lyric Doshi, Tim\nKraska, Xiaozhou Li, Andy Ly, and Christopher Olston. 2020. Learned Indexes\nfor a Google-scale Disk-based Database. CoRR abs/2012.12501 (2020).\n[2]Andreas Behrend, Anton DignÃ¶s, Johann Gamper, Philip Schmiegelt, Hannes\nVoigt, Matthias Rottmann, and Karsten Kahl. 2019. Period Index: A Learned 2D\nHash Index for Range and Duration Queries. In SSTD . ACM, 100â€“109.\n[3]Yifan Dai, Yien Xu, Aishwarya Ganesan, Ramnatthan Alagappan, Brian Kroth,\nAndrea C. Arpaci-Dusseau, and Remzi H. Arpaci-Dusseau. 2020. From WiscKey\nto Bourbon: A Learned Index for Log-Structured Merge Trees. In OSDI . 155â€“171.\n[4]Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid B. Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned\nIndex. In SIGMO . 969â€“984.\n[5]Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020.\nTsunami: A Learned Multi-dimensional Index for Correlated Data and Skewed\nWorkloads. Proc. VLDB Endow. 14, 2 (2020), 74â€“86.\n[6]Paolo Ferragina, Fabrizio Lillo, and Giorgio Vinciguerra. 2020. Why Are Learned\nIndexes So Effective?. In ICML (Proceedings of Machine Learning Research, Vol. 119) .\nPMLR, 3123â€“3132.\n[7]Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds. Proc. VLDB Endow.\n13, 8 (2020), 1162â€“1175.\n[8]Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim\nKraska. 2019. FITing-Tree: A Data-aware Index Structure. In SIGMOD . 1189â€“1206.\n[9]Laura M. Haas, Michael J. Carey, Miron Livny, and Amit Shukla. 1997. Seeking\nthe Truth About ad hoc Join Costs. VLDB J. 6, 3 (1997), 241â€“256.\n[10] Ali Hadian and Thomas Heinis. 2019. Interpolation-friendly B-trees: Bridging the\nGap Between Algorithmic and Learned Indexes. In EDBT . OpenProceedings.org,\n710â€“713.\n[11] Ali Hadian and Thomas Heinis. 2021. Shift-Table: A Low-latency Learned Index\nfor Range Queries using Model Correction. In EDBT . OpenProceedings.org, 253â€“\n264.\n[12] Andreas Kipf, Dominik Horn, Pascal Pfeil, Ryan Marcus, and Tim Kraska. 2022.\nLSI: a learned secondary index structure. In aiDM@SIGMOD . ACM, 4:1â€“4:5.\n[13] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass learned\nindex. In aiDM@SIGMOD . ACM, 5:1â€“5:5.\n[14] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In SIGMOD . 489â€“504.\n[15] Pengfei Li, Yu Hua, Jingnan Jia, and Pengfei Zuo. 2021. FINEdex: A Fine-grained\nLearned Index Scheme for Scalable and Concurrent Memory Systems. Proc. VLDB\nEndow. 15, 2 (2021), 321â€“334.\n[16] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. 2020. LISA: A Learned\nIndex Structure for Spatial Data. In SIMGOD . ACM, 2119â€“2133.\n[17] Anisa Llaveshi, Utku Sirin, Anastasia Ailamaki, and Robert West. 2019. Acceler-\nating B+ tree search by using simple machine learning techniques. In Proceedings\nof the 1st International Workshop on Applied AI for Database Systems and Applica-\ntions .\n[18] Baotong Lu, Jialin Ding, Eric Lo, Umar Farooq Minhas, and Tianzheng Wang.\n2021. APEX: A High-Performance Learned Index on Persistent Memory. Proc.\nVLDB Endow. 15, 3 (2021), 597â€“610.\n[19] Chaohong Ma, Xiaohui Yu, Yifan Li, Xiaofeng Meng, and Aishan Maoliniyazi.\n2022. FILM: a Fully Learned Index for Larger-than-Memory Databases. Proc.\nVLDB Endow. 16, 3 (2022), 561â€“573.[20] Marcel Maltry and Jens Dittrich. 2022. A Critical Analysis of Recursive Model\nIndexes. Proc. VLDB Endow. 15, 5 (2022), 1079â€“1091.\n[21] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra,\nAlfons Kemper, Thomas Neumann, and Tim Kraska. 2020. Benchmarking Learned\nIndexes. Proc. VLDB Endow. 14, 1 (2020), 1â€“13.\n[22] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning Multi-Dimensional Indexes. In SIGMOD . ACM, 985â€“1000.\n[23] Patrick E. Oâ€™Neil, Edward Cheng, Dieter Gawlick, and Elizabeth J. Oâ€™Neil. 1996.\nThe Log-Structured Merge-Tree (LSM-Tree). Acta Informatica 33, 4 (1996), 351â€“\n385.\n[24] Joseph Oâ€™Rourke. 1981. An On-Line Algorithm for Fitting Straight Lines Between\nData Ranges. Commun. ACM 24, 9 (1981), 574â€“578.\n[25] Sachith Gopalakrishna Pai, Michael Mathioudakis, and Yanhao Wang. 2022. To-\nwards an Instance-Optimal Z-Index. In 4th International Workshop on Applied AI\nfor Database Systems and Applications (AIDB@ VLDB2022) .\n[26] Varun Pandey, Alexander van Renen, Andreas Kipf, Jialin Ding, Ibrahim Sabek,\nand Alfons Kemper. 2020. The Case for Learned Spatial Indexes. In AIDB@VLDB .\n[27] Jianzhong Qi, Guanli Liu, Christian S. Jensen, and Lars Kulik. 2020. Effectively\nLearning Spatial Indices. Proc. VLDB Endow. 13, 11 (2020), 2341â€“2354.\n[28] Danilo Jimenez Rezende and Shakir Mohamed. 2015. Variational Inference with\nNormalizing Flows. In ICML , Vol. 37. JMLR.org, 1530â€“1538.\n[29] Benjamin Spector, Andreas Kipf, Kapil Vaidya, Chi Wang, Umar Farooq Minhas,\nand Tim Kraska. 2021. Bounding the Last Mile: Efficient Learned String Indexing.\nCoRR abs/2111.14905 (2021).\n[30] Mihail Stoian, Andreas Kipf, Ryan Marcus, and Tim Kraska. 2021. PLEX: Towards\nPractical Learned Indexing. CoRR abs/2108.05117 (2021).\n[31] Chuzhe Tang, Youyun Wang, Zhiyuan Dong, Gansen Hu, Zhaoguo Wang, Minjie\nWang, and Haibo Chen. 2020. XIndex: a scalable learned index for multicore data\nstorage. In PPoPP . ACM, 308â€“320.\n[32] Youyun Wang, Chuzhe Tang, Zhaoguo Wang, and Haibo Chen. 2020. SIndex: a\nscalable learned index for string keys. In APSys â€™20: 11th ACM SIGOPS Asia-Pacific\nWorkshop on Systems, Tsukuba, Japan, August 24-25, 2020 . ACM, 17â€“24.\n[33] H.-S. Philip Wong, Simone Raoux, SangBum Kim, Jiale Liang, John P. Reifenberg,\nBipin Rajendran, Mehdi Asheghi, and Kenneth E. Goodson. 2010. Phase Change\nMemory. Proc. IEEE 98, 12 (2010), 2201â€“2227.\n[34] Chaichon Wongkham, Baotong Lu, Chris Liu, Zhicong Zhong, Eric Lo, and\nTianzheng Wang. 2022. Are Updatable Learned Indexes Ready? Proc. VLDB\nEndow. 15, 11 (2022), 3004â€“3017.\n[35] Jiacheng Wu, Yong Zhang, Shimin Chen, Yu Chen, Jin Wang, and Chunxiao Xing.\n2021. Updatable Learned Index with Precise Positions. Proc. VLDB Endow. 14, 8\n(2021), 1276â€“1288.\n[36] Shangyu Wu, Yufei Cui, Jinghuan Yu, Xuan Sun, Tei-Wei Kuo, and Chun Jason\nXue. 2022. NFL: Robust Learned Index via Distribution Transformation. Proc.\nVLDB Endow. 15, 10 (2022), 2188â€“2200.\n[37] Yingjun Wu, Jia Yu, Yuanyuan Tian, Richard Sidle, and Ronald Barber. 2019.\nDesigning Succinct Secondary Indexing Mechanism by Exploiting Column Cor-\nrelations. In SIGMOD . ACM, 1223â€“1240.\n[38] Geoffrey X. Yu, Markos Markakis, Andreas Kipf, Per-Ã…ke Larson, Umar Farooq\nMinhas, and Tim Kraska. 2022. TreeLine: An Update-in-Place Key-Value Store\nfor Modern Storage. Proc. VLDB Endow. 16, 1 (2022), 99â€“112.\n[39] Jiaoyi Zhang and Yihan Gao. 2022. CARMI: A Cache-Aware Learned Index with a\nCost-based Construction Algorithm. Proc. VLDB Endow. 15, 11 (2022), 2679â€“2691.\n[40] Xuanhe Zhou, Luyang Liu, Wenbo Li, Lianyuan Jin, Shifu Li, Tianqing Wang,\nand Jianhua Feng. 2022. AutoIndex: An Incremental Index Management System\nfor Dynamic Workloads. In ICDE . IEEE, 2196â€“2208.",
  "textLength": 77557
}