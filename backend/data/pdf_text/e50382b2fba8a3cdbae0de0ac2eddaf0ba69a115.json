{
  "paperId": "e50382b2fba8a3cdbae0de0ac2eddaf0ba69a115",
  "title": "Learned LSM-trees: Two Approaches Using Learned Bloom Filters",
  "pdfPath": "e50382b2fba8a3cdbae0de0ac2eddaf0ba69a115.pdf",
  "text": "Learned LSM-trees: Two Approaches Using Learned\nBloom Filters∗\nNico Fidalgo Puyuan Ye\nHarvard University\nnfidalgo@college.harvard.edu, puyuanye@college.harvard.edu\nMay 9, 2025\nAbstract\nModern key-value stores rely heavily on Log-Structured Merge (LSM) trees for write op-\ntimization, but this design introduces significant read amplification. Auxiliary structures like\nBloom filters help, but impose memory costs that scale with tree depth and dataset size. Re-\ncent advances in learned data structures suggest that machine learning models can augment or\nreplace these components, trading handcrafted heuristics for data-adaptive behavior. In this\nwork, we explore two approaches for integrating learned predictions into the LSM-tree lookup\npath. The first uses a classifier to selectively bypass Bloom filter probes for irrelevant levels,\naiming to reduce average-case query latency. The second replaces traditional Bloom filters with\ncompact learned models and small backup filters, targeting memory footprint reduction without\ncompromising correctness.\nWe implement both methods atop a Monkey-style LSM-tree with leveled compaction, per-\nlevel Bloom filters, and realistic workloads. Our experiments show that the classifier reduces\nGET latency by up to 2.28 ×by skipping over 30% of Bloom filter checks with high precision,\nthough it incurs a modest false-negative rate. The learned Bloom filter design achieves zero\nfalse negatives and retains baseline latency while cutting memory usage per level by 70–80%.\nTogether, these designs illustrate complementary trade-offs between latency, memory, and cor-\nrectness, and highlight the potential of learned index components in write-optimized storage\nsystems.\n1 Introduction\nLog-structured merge trees (LSM-trees) have become the de facto standard for high-throughput\nwrite-heavy workloads in modern key-value stores such as RocksDB, Cassandra, and LevelDB.\nTheir core strength lies in write amplification reduction: rather than writing directly to disk on\neach update, writes are first buffered in memory (in the MemTable), and later flushed as immutable,\nsorted disk segments known as SSTables. These SSTables are organized hierarchically across lev-\nels, each exponentially larger than the last, using a compaction strategy that periodically merges\noverlapping tables to maintain sorted order and free space. This batching and merging architecture\ndrastically improves write throughput, making LSM-trees ideal for large-scale logging, time-series\ndata, and streaming ingestion applications.\nYet this write-optimized design creates a well-known tradeoff: increased read latency. Point\nqueries (GET operations) must examine the MemTable and potentially every level of the disk\n∗Code available at: https://github.com/fiidalgo/predictive-lsm-trees\n1arXiv:2508.00882v1  [cs.DS]  24 Jul 2025\n\nhierarchy, querying filters and scanning SSTables. The deeper the level hierarchy, the greater the\nnumber of SSTables, and thus the more auxiliary data structures are required to ensure acceptable\nread performance. Bloom filters and fence pointers are the most common of these. Bloom filters\nprevent unnecessary I/O by probabilistically excluding levels, and fence pointers accelerate binary\nsearch within an SSTable. However, these structures impose substantial memory overhead and\nhave limited adaptivity, especially as datasets grow and key distributions evolve.\nIn response to these challenges, a new class of techniques has emerged: learned data structures.\nRooted in the idea of algorithms with predictions, these approaches use machine learning models\nto approximate, augment, or replace traditional data structures. For example, a classifier can\npredict whether a key exists in a level, or whether a key is likely to be in the dataset at all. Such\npredictions can reduce memory consumption, avoid redundant filter checks, and skip entire levels,\nthereby reducing query latency.\nIn this project, we explore two approaches for integrating learned predictions into LSM-tree\nread paths. First, we propose a classifier-based method that predicts whether a given key is likely\nto reside in each level. Instead of querying every Bloom filter sequentially, we use the classifier\nto selectively consult only the most promising levels. This avoids wasted memory accesses and\nreduces average lookup latency in multi-level trees. Second, we implement a learned Bloom filter,\nreplacing large traditional Bloom filters with compact hybrid structures consisting of a lightweight\nclassifier and a small backup Bloom filter. This approach offers substantial memory savings while\nstill preserving correctness guarantees.\nTo rigorously test these ideas, we constructed an LSM-tree implementation based on the design\nchoices of the Monkey paper [3], including leveled compaction, a 10x level size ratio, and realistic\nSSTable thresholds. We simulate workloads using 287.6 MB of randomly generated data with 16-\nbyte keys and 100-byte values to emulate RocksDB conditions and avoid overfitting to artificial\npatterns. Our experiments span a diverse set of read-intensive scenarios (random access, sequential\nscans, and level-specific lookups) to examine how learned models behave under varying access\ndistributions.\nThese contributions demonstrate the practical viability of integrating ML-based predictions\ninto critical paths of database storage engines. Our findings offer a compelling case for memory-\nefficient, prediction-aware auxiliary structures that optimize LSM-tree query performance without\ncompromising correctness. This work adds to a growing body of literature at the intersection of\ndata systems, algorithms, and machine learning, where classical structures are being reimagined as\nadaptive, data-driven components.\n2 Related Work\nThe idea of enhancing traditional data structures with machine learning was catalyzed by the\nseminal work of Kraska et al. [5] in “The Case for Learned Index Structures.” This work argued\nthat core components of databases (e.g., B-trees, hash maps, and Bloom filters) can be replaced\nor augmented with models trained to exploit regularities in real-world data distributions. The key\ninsight was that fixed data structures are agnostic to input distributions, while learned models\ncan adaptively leverage them to reduce space and time complexity. This marked the beginning\nof a broader research trend toward learned data structures, with a growing focus on replacing\ndeterministic components with adaptive, data-driven approximations.\nThis idea was extended to Bloom filters by Mitzenmacher [6], who introduced the sandwich\nlearned Bloom filter. In this design, a machine learning classifier first predicts whether a key\nbelongs to a set. If the classifier accepts the key, a backup traditional Bloom filter is queried to verify\n2\n\nmembership, ensuring that false negatives (which traditional Bloom filters avoid by design) are not\nintroduced. This hybrid structure preserved correctness while significantly reducing memory usage.\nOur work builds on this framework by applying it to a realistic, multi-level LSM-tree, incorporating\nclassifiers at the level granularity, and empirically evaluating performance across diverse workloads.\nThe concept of learned Bloom filters was further developed in follow-up studies such as Ada-\nBF [2], which introduced adaptivity into the classifier itself, allowing the filter to change over time\nas data distributions evolved. Meanwhile, Rae et al. [8] proposed neural Bloom filters using deep\nlearning architectures to model complex patterns in key distributions. While these techniques\nachieved strong empirical results, most prior work tested them on static datasets or in isolation\nfrom real systems. Our project addresses this gap by deeply integrating learned filters into a full\nLSM-tree pipeline and evaluating their behavior in dynamic, hierarchical environments.\nParallel to these empirical advancements, theoretical work emerged under the banner of “Al-\ngorithms with Predictions.” Mitzenmacher and Vassilvitskii formalized how algorithms can safely\nincorporate ML predictions while maintaining worst-case performance bounds [7]. The core idea\nis to use predictions to improve average-case efficiency, while ensuring robustness through fallback\nmechanisms that guard against poor predictions. This principle guided our classifier-based design:\nwe allow the model to skip levels when confident, but always revert to Bloom filter fallback when\nuncertain or incorrect. This interplay between data adaptivity and algorithmic resilience is central\nto making machine learning a dependable component in critical database infrastructure.\nIn the domain of key-value stores, the Monkey paper by Dayan et al. [3] presented a com-\nprehensive mathematical framework for optimizing LSM-tree compaction strategies, Bloom filter\nallocation, and level sizing. It demonstrated that disproportionate Bloom filter memory allocations\ncould yield significant performance improvements and introduced a log-log cost model for under-\nstanding read amplification. We adopt the Monkey configuration in our implementation—1 MB\nMemTable, 10x size ratio, leveled compaction—to ground our system in an industry-relevant and\ntheoretically sound baseline. Our learned models can be seen as a next step in this optimization\nlineage, where instead of tuning filter sizes, we explore adaptive structures that may change the\nnature of filtering itself.\nOther relevant studies, such as Kipf et al.’s learned replacements for B-trees [4], and Tsai et\nal.’s learned hash tables [9], further established that learned approaches can outperform classical\nstructures in space and latency across a range of domains. However, relatively few works have\nexplored learned components in hierarchical storage systems like LSM-trees, where predictions\naffect not just access efficiency but the traversal path of the entire system.\nIn summary, our project synthesizes ideas from theoretical frameworks (algorithms with predic-\ntions), empirical explorations (learned filters and classifiers), and practical systems (Monkey-style\nLSM-trees). We offer a real-world testbed where these ideas interact meaningfully, measuring not\nonly prediction accuracy but also memory usage, latency, and false negative rates under varying\nworkloads. This positions our work as both a validation of prior results and a stepping stone for\nfuture research on hybrid learned/traditional data systems.\n3 Background\n3.1 LSM-trees and the Read-Write Tradeoff\nLog-Structured Merge Trees (LSM-trees) are widely used in modern key-value stores due to their\nwrite-optimized nature. They delay disk writes by buffering updates in a memory-resident data\nstructure, typically a skip list, known as the MemTable. Once this buffer exceeds a fixed thresh-\nold—in our case, 1 MB as suggested by configurations in the Monkey paper—it is flushed to disk\n3\n\nas an immutable, sorted structure called a Sorted String Table (SSTable).\nSSTables are organized into levels L0, L1, . . . , L n, with each level increasing exponentially in\nsize relative to the one above it. The size ratio Tis typically set to 10, a choice that is not theo-\nretically mandated but is justified by empirical systems research as a balance between compaction\ncost, space amplification, and query performance [3]. Compaction, a background process, merges\noverlapping SSTables from higher levels into lower ones to maintain sorted order and reduce storage\nfragmentation. This architectural design minimizes write amplification and significantly improves\nthroughput compared to B-trees and other write-in-place structures.\nHowever, this benefit comes at the cost of read amplification. For point queries, the system must\ncheck the MemTable, and potentially each level of the tree from newest to oldest. To mitigate this\ncost, LSM-trees incorporate auxiliary indexing structures such as Bloom filters and fence pointers.\nBloom filters help quickly determine whether a key is absent from a given level, and fence pointers\nassist in narrowing the range of disk pages to inspect.\nThe Monkey paper [3] rigorously models this tradeoff and shows that optimal read performance\ncannot be achieved with uniformly configured Bloom filters. Instead, it introduces a novel cost\nmodel in which the expected query cost Ris given by:\nR=LX\ni=1fi·ci\nwhere fiis the false positive rate (FPR) of the Bloom filter at level i, and ciis the cost of probing\nlevel i. The paper demonstrates that minimizing Rrequires distributing memory non-uniformly:\ndeeper levels, which are more expensive to query and less likely to be accessed, should be allocated\nmore bits per key. Monkey prescribes a log-uniform FPR distribution where:\nfi∝1\nTi\nThis transforms the Bloom filter configuration problem into a constrained convex optimization\nproblem. Given a total filter memory budget M, one solves for the bits per key misuch that:\nLX\ni=1ni·mi≤M\nwhere niis the number of keys in level i. This allocation minimizes expected read cost while\nrespecting memory constraints.\nOur project directly adopts the Monkey framework, using a 1 MB MemTable, a 10x level size\nratio, leveled compaction, and per-level Bloom filters starting at L1. These choices ensure empirical\ncomparability and provide a mathematically grounded baseline for evaluating the impact of learned\npredictions.\n3.2 Bloom Filters and Their Limitations\nBloom filters, introduced by Bloom in 1970 [1], are widely used for fast set membership queries.\nEach Bloom filter uses a bit array of size mandkindependent hash functions. For each key\ninserted, the corresponding kbits are set to 1. To query a key, one checks whether all khash bits\nare set. If any bit is 0, the key is definitely not in the set; if all are 1, the key may be present. The\nfalse positive rate is approximately:\nFPR≈\u0010\n1−e−kn/m\u0011k\n4\n\nThis expression is derived under the assumption of perfectly random hash functions and uniform\nkey distribution. The FPR is minimized when:\nk=m\nnln 2\nIn this case, the minimum FPR becomes:\nFPR min=\u00121\n2\u0013k\nThese derivations are foundational results from Bloom’s original formulation [1], and remain the\ntheoretical basis for Bloom filter tuning in practice.\nDespite their probabilistic power, traditional Bloom filters are static and oblivious to patterns\nin key access or structure. They assume that all keys are equally likely and uniformly distributed,\nwhich is rarely true in practice. Consequently, Bloom filters allocate the same number of bits per\nkey across all levels and key types, missing optimization opportunities.\nThis limitation is particularly pronounced in multi-level systems like LSM-trees, where deeper\nlevels contain exponentially more data due to geometric growth. For example, suppose Level 0\nstarts with 105keys and each subsequent level is ten times larger (a standard size ratio T= 10).\nThen by Level 5, the total number of keys stored across levels L1toL5is approximately 1 .11×106\ntimes the base level. If we allocate 10 bits per key in each Bloom filter, the total memory required\nbecomes:\n5X\ni=1(105·10i)·10 = 111 ,110,000 bits ≈13.89 MB\nThis linear growth in filter memory with respect to the number of levels and keys quickly becomes a\nbottleneck, especially in large-scale deployments where each level may contain hundreds of millions\nof keys.\n3.3 Learned Bloom Filters and ML Classification Background\nThe motivation behind learned Bloom filters originates from the observation that traditional filters\nwaste memory treating all keys uniformly, ignoring patterns or correlations present in real-world\ndata. Learned Bloom filters exploit this by training a machine learning model f:K→[0,1]\nthat estimates the likelihood of key membership in a dataset S⊂K. Iff(x) exceeds a threshold\nτ, the model predicts presence. To prevent false negatives—i.e., true members x∈Sincorrectly\nclassified as non-members—a small backup Bloom filter stores false negatives found during training,\nas introduced in Mitzenmacher’s sandwich filter [6].\nFormally, let Xbe the universe of keys and let χS:X → { 0,1}be the true membership function.\nA learned Bloom filter seeks to approximate χSusing a trained model ˆ χS, such that:\n∀x∈ X,ˆχS(x) =(\n1 if f(x)≥τorx∈ B\n0 otherwise\nwhere Bdenotes the set of false negatives caught by the backup filter.\nThe goal is to reduce the total space while preserving a comparable or better false positive rate\nϵ. IfMtraditional is the size of a standard Bloom filter, then the learned filter seeks to satisfy:\nMmodel +Mbackup ≪Mtraditional\n5\n\nIn our implementation, we use Gradient Boosted Trees (GBTs), an ensemble learning method\nthat minimizes the logistic loss:\nL(y,ˆy) =−[ylog(ˆy) + (1−y) log(1 −ˆy)]\nThe final prediction is given by:\nˆy=σ TX\nt=1αtht(x)!\n,where σ(z) =1\n1 +e−z\nThese models generalize well on small datasets and are efficient to evaluate.\nWe engineer features from keys using logarithmic transforms, trigonometric functions, digit-\nbased statistics, modulo operations, and binary encodings. These help the classifier capture non-\nlinear patterns in key distributions and improve generalization.\nIn our classifier-based level prediction architecture we predict the level at which the key is most\nlikely to reside, and skip Bloom filters for levels predicted negative. This architecture reduces\nunnecessary filter checks and memory reads.\nBoth learned approaches are instances of the broader theory of algorithms with predictions [7],\nwhich blends ML predictions with robust fallback guarantees. Our use of backup filters and con-\nservative thresholds ensures that our system maintains correctness even when predictions are inac-\ncurate.\n4 Design and Implementation\nThe central research question of this work is whether machine learning models can meaningfully\naugment or replace Bloom filters in LSM-trees to reduce lookup cost or memory footprint without\ncompromising correctness. In this section, we present two such designs: a classifier-augmented\nlookup mechanism that reduces unnecessary Bloom filter queries, and a learned Bloom filter struc-\nture that replaces traditional filters entirely. Both are derived directly from the theoretical and\nempirical motivations outlined in Section 2 and Section 3. In particular, we anchor our design\ndecisions in the Monkey framework [3] and the sandwich learned Bloom filter model [6], while\npreserving correctness guarantees as mandated by the algorithms with predictions framework [7].\n4.1 ML Classifier Approach\nOur first design augments the LSM-tree’s GET operation with a classifier that predicts, on a per-\nlevel basis, whether a given key is likely to be found in that level. Traditionally, a point query\nconsults the MemTable, then queries every level’s Bloom filter sequentially until a match is found.\nHowever, in a deep LSM-tree hierarchy, this often involves scanning levels where the key is almost\ncertainly absent. This incurs wasted computation—each Bloom filter lookup involves memory\nreads—and adds latency, especially when filters are large. Our insight is that a model trained on\nkey-level membership can help skip irrelevant filters altogether, thereby reducing memory accesses\nand potentially lowering query latency.\nTo implement this, we use a GradientBoostingClassifier that takes as input a vector of\nfeatures derived from the query key. The model was trained on labeled data collected from prior\nGET and PUT operations, with the goal of predicting whether a key exists in a specific level.\nThe feature set is intentionally large and expressive to allow for generalization across key distri-\nbutions. We include power transformations such as k2andk3, which allow the model to detect\n6\n\nnon-linear growth patterns. Logarithmic features like log( k) and log(1 + k) help capture expo-\nnential relationships between keys and levels, especially useful in exponentially growing structures\nlike LSM-trees. Trigonometric features sin( k),cos(k),tan(k) encode periodic patterns and were\nobserved to be helpful in capturing hashed or cyclic key behavior. Digit-based features, including\ndigit sum and digit count, help distinguish key formats or groupings, while modulo operations and\nbinning identify modular clustering or partitioning artifacts. Finally, binary encodings such as\nbitcount and leading-one counts extract low-level key structure from raw byte representations.\nThese features are combined into a dense vector and fed into a shallow gradient boosting\nensemble with 200 estimators, a maximum tree depth of 6, and a learning rate of 0.1. The model\noutputs a binary decision ˆ yi∈ {0,1}for each level i, where 1 indicates predicted presence. If the\nmodel returns 1, the corresponding Bloom filter is queried; if 0, the level is skipped.\nThis approach assumes that the classifier can accurately identify levels that are unlikely to\ncontain the key, thereby reducing the number of memory accesses (for Bloom filter queries) and\nlowering the expected number of disk I/Os. The performance gain is in latency rather than memory\nusage—while the model introduces additional memory overhead (roughly 6 MB in our experiments),\nit avoids querying several Bloom filters, which can each cost tens to hundreds of nanoseconds\nper lookup. Moreover, this model operates as a non-intrusive drop-in layer. It does not modify\ncompaction, tree layout, or the Bloom filter implementation. This simplicity makes it particularly\nattractive in systems where correctness and modularity are paramount.\nAlgorithm 1: Classifier-Augmented Lookup\nAlgorithm 1 Classifier-Augmented LSM-Tree GET\n1:procedure ClassifierLSM-Tree-GET (k)\n2: ifk∈MemTable then\n3: return MemTable[ k]\n4: end if\n5: fori= 1 to Ldo\n6: pi←Classifier i(k)\n7: ifpi= 1then\n8: ifBloomFilter i.MayContain (k)then\n9: ifkfound in SSTable ithen\n10: return SSTable i[k]\n11: end if\n12: end if\n13: end if\n14: end for\n15: return NULL\n16:end procedure\n4.2 Learned Bloom Filter Approach\nThe second design is a more structural rethinking of Bloom filters within the LSM-tree. Rather\nthan use a model to skip filters, we instead train a model to replace them. This is inspired directly\nby the sandwich learned Bloom filter [6], where a machine learning model acts as the primary\nmembership check, and a small backup Bloom filter ensures that false negatives do not occur. This\nhybrid model allows us to dramatically shrink the memory cost of the traditional Bloom filter while\nretaining correctness guarantees.\n7\n\nTo integrate this into the LSM-tree, we replaced the standard filter logic within the Run,Level ,\nand Tree classes. Each level maintains its own trained classifier fi, which predicts whether a key\nis in that level. If the model returns true, we search the SSTables as normal. If the model returns\nfalse, we consult the backup Bloom filter—constructed only on the model’s false negatives—to avoid\nerroneous rejections. The backup Bloom filter is small because it only needs to store a fraction δ\nof the total keys, where δis the model’s false negative rate.\nThe key advantage of this approach is reduced memory usage. Whereas traditional Bloom\nfilters might use 10–14 bits per key, our model uses under 1 MB per level, and the backup filter\nscales linearly with δ. For a model with δ= 0.01, the memory required for the backup is just 1%\nof that used in the standard Bloom filter. While model inference does add latency, it is typically\non par with or faster than hashing-based Bloom filter queries. This makes the design especially\ncompelling in memory-constrained environments such as mobile databases, edge devices, or very\nlarge-scale key-value stores.\nAlgorithm 2: Learned Bloom Filter Lookup\nAlgorithm 2 Learned Bloom Filter GET\n1:procedure LearnedBF-LSM-Tree-GET (k)\n2: ifk∈MemTable then\n3: return MemTable[ k]\n4: end if\n5: fori= 1 to Ldo\n6: pi←Classifier i(k)\n7: ifpi= 1then\n8: ifkfound in SSTable ithen\n9: return SSTable i[k]\n10: end if\n11: else if BackupFilter i.MayContain (k)then\n12: ifkfound in SSTable ithen\n13: return SSTable i[k]\n14: end if\n15: end if\n16: end for\n17: return NULL\n18:end procedure\n4.3 Baseline Algorithm: Traditional LSM-tree Lookup\nFor completeness, we restate the standard lookup mechanism of a traditional LSM-tree using Bloom\nfilters at each level. This algorithm forms our baseline for all empirical comparisons.\nAlgorithm 3: Standard Lookup Procedure\n8\n\nAlgorithm 3 Standard LSM-Tree GET Procedure\n1:procedure LSM-Tree-GET (k)\n2: ifk∈MemTable then\n3: return MemTable[ k]\n4: end if\n5: fori= 1 to Ldo\n6: ifBloomFilter i.MayContain (k)then\n7: ifkfound in SSTable ithen\n8: return SSTable i[k]\n9: end if\n10: end if\n11: end for\n12: return NULL\n13:end procedure\nThis version ensures correctness and is relatively efficient when Bloom filters are finely tuned,\nas proposed in Monkey [3]. However, it suffers from the inability to skip unpromising levels or\ncompress filter representation, which motivates our two proposed alternatives.\n4.4 Design Tradeoffs and Comparative Summary\nThe classifier-augmented approach is targeted toward latency reduction. It reduces the number of\nBloom filters queried per lookup, potentially skipping deep levels with large filters and cold data.\nHowever, it requires additional memory for the model and richer features, and its improvements\ndepend on prediction accuracy. In our implementation, this approach added between 5.6–6.1 MB\nof memory overhead due to the high-capacity model and 45 engineered features, but this tradeoff\nwas justified in read-heavy environments where every skipped Bloom filter can shave off dozens of\nnanoseconds per query.\nBy contrast, the learned Bloom filter approach is focused on reducing memory consumption.\nIt replaces large filters with compact models and small backups, enabling scalability in memory-\nconstrained deployments. The classifier in this design is shallower, uses fewer features, and results\nin a smaller memory footprint (typically 530–903 KB per level). However, it may incur slightly\nhigher lookup latencies due to the need for fallback logic and two-stage validation (model + backup\nBloom filter). Nonetheless, because only the false negatives from the model are stored in the backup\nBloom filter, the total space consumed remains significantly below that of a traditional Bloom filter\nwhile still preserving correctness.\nTogether, these designs explore two complementary paths in the design space of ML-enhanced\nstorage systems: predictive augmentation and learned replacement. Each draws from the framework\nof algorithms with predictions and the broader movement toward data-adaptive system components.\nOur implementation enables an empirical comparison of these approaches under realistic workloads,\nvalidating their tradeoffs and limitations in practice.\nWe summarize the key distinctions and benefits of each design in Table 1, which provides a\nstructured overview of their integration complexity, memory footprint, performance goals, and\nideal deployment contexts.\n9\n\nTable 1: Comparison of the ML Classifier and Learned Bloom Filter Designs\nDimension ML Classifier (Filter Skip-\nping)Learned Bloom Filter (Hybrid\nReplacement)\nGoal Reduce query latency by skipping\nunnecessary Bloom filter probes.Reduce memory usage by replac-\ning large Bloom filters with com-\npact models and small backups.\nIntegration Depth Shallow—acts as a wrapper be-\nfore Bloom filters, does not change\nLSM-tree structure.Deep—modifies level and run logic,\nrequires architectural changes to\nthe LSM-tree.\nModel Size Larger (5.6–6.1 MB), due to 45 en-\ngineered features and high model\ncapacity.Smaller (530–903 KB), fewer fea-\ntures and shallower model.\nMemory Usage Higher overall due to added clas-\nsifier without removing existing\nBloom filters.Lower overall by removing large\nBloom filters and introducing com-\npact backup filters.\nExpected Latency Benefit Improves average-case latency by\nskipping irrelevant levels and fil-\nters.Neutral or slightly worse due to\nadded inference, but gains memory\nscalability.\nCorrectness Guarantee Guaranteed via fallback to origi-\nnal Bloom filters when prediction\nis positive.Guaranteed via backup Bloom fil-\nter that stores classifier false nega-\ntives.\nBest Use Case Read-heavy workloads with many\ndeep levels and unpredictable key\naccesses.Memory-constrained environments\nor systems storing billions of keys.\nAs this comparison shows, the classifier-based approach is best suited for latency-sensitive\nworkloads where memory is abundant, while the learned Bloom filter variant excels in scenarios\nwhere memory is scarce but lookup correctness and predictability remain paramount. Both reflect\ndistinct strategies within the same predictive systems paradigm: one optimizes the access path by\nlearning when to query, the other optimizes memory layout by learning what to store.\n5 Experimental Methodology\n5.1 Workloads\nTo assess the effectiveness of our learned LSM-tree designs under realistic operating conditions, we\ndevised a series of GET-only workloads that emulate common access patterns in production key-\nvalue stores. These include: (1) Random lookups, which simulate uniformly distributed queries\nacross the key space; (2) Sequential lookups, which test the models’ robustness against unseen\npatterns and generalization beyond training distributions; and three skewed workloads: (3) Level-\n1 targeted, (4) Level-2 targeted, and (5) Level-3 targeted queries. These are designed to stress\nthe system under scenarios of shallow, mid-tier, and deep-level access respectively, capturing how\nprediction performance changes with data recency and storage depth. These configurations serve to\nevaluate not just average-case performance, but also failure cases and edge-level sensitivity across\nthe tree.\n10\n\n5.2 Metrics\nWe evaluate the performance of each system using several metrics. The false positive rate (FPR)\nquantifies the proportion of keys incorrectly reported as present in a given level or set, leading to\nunnecessary disk accesses. The false negative rate (FNR), particularly relevant for learned filters,\ncaptures instances where the model erroneously predicts absence despite the key being present—a\ncritical correctness concern mitigated via fallback mechanisms. We also measure average lookup\nlatency per GET request in microseconds, as this directly reflects the user-facing impact of our\noptimizations. In addition, we analyze total memory consumption across the traditional and learned\nsystems, summing the sizes of Bloom filters, models, and backup filters. Lastly, we monitor backup\nfilter utilization rates in the learned Bloom filter system to quantify the classifier’s effectiveness\nand the fallback mechanism’s engagement frequency.\n5.3 Experimental Configuration\nOur baseline LSM-tree implementation closely follows the parameterization and structure proposed\nin the Monkey paper [3]. We use a 1 MB in-memory MemTable flushed to disk when full, a geometric\nlevel growth factor of T= 10, and a leveling compaction policy to maintain sorted, non-overlapping\nSSTables. Bloom filters are used for all disk-resident levels from L1onwards, and fence pointers\nare enabled for fast in-SSTable binary search. Keys are 16 bytes and values are 100 bytes in\nsize, mirroring RocksDB settings. The total dataset size was approximately 287.6 MB, with key-\nvalue pairs generated using a uniform random distribution to avoid learning artifacts stemming\nfrom artificial sequentiality. We then pre-trained each model offline and integrated the trained\ncomponents into the runtime lookup path.\n5.4 Data Loading and Tree Construction\nThe script load data.py initializes the LSM-tree by sequentially inserting randomly generated\nkey-value pairs into the MemTable. Once full, this data is flushed to Level 0 SSTables. The script\nhandles level promotion, compaction events, and Bloom filter creation according to the Monkey\nmodel. Each key is treated as a 128-bit unsigned integer and converted to a feature vector or\nbinary representation during training. Keys are sorted and flushed according to the tree’s leveling\ncompaction policy, and metadata on key-level associations is logged for use in supervised training\nof predictive models.\n5.5 Model Architectures and Training\nTwo separate training pipelines were used, corresponding to our two designs. For the ML classifier\nused in level prediction, train classifier.py defines a 200-tree GradientBoostingClassifier\nfrom sklearn.ensemble . The classifier is trained on a feature set of 45 engineered features, includ-\ning power transformations (e.g., k2, k3), logarithmic functions (log( k),log(1 + k)), trigonometric\nprojections (sin( k),cos(k),tan(k)), digit statistics (e.g., number of digits, digit sum), modulo en-\ncodings (e.g., kmod n), bucketized bin flags, and exponential decay indicators. These features\nwere selected to capture periodicities, key shape, bit structure, and numeric trends, allowing the\nmodel to learn distributions and correlations with level placement.\nIn contrast, the learned Bloom filter system is trained using the script train learned.py ,\nwhich utilizes a simpler feature set to minimize model size and inference time. Features are drawn\nfrom logarithmic values, bit patterns (e.g., most significant bit), sine/cosine mappings, and binary\nencodings. The classifier again uses GradientBoostingClassifier but with reduced tree count\n11\n\nand shallower depth to control memory footprint. During training, we identify false negatives—i.e.,\nkeys from the true positive set that the classifier misclassifies—and store them in a small backup\nBloom filter.\n5.6 System Integration\nThe classifier-based design wraps the original level lookup logic using the FastBloomFilter class.\nThis wrapper performs real-time feature computation and model inference before deciding whether\nto proceed with a Bloom filter query. If a level is predicted negative by the classifier, its Bloom\nfilter is skipped, saving both memory and computation time. If the classifier is uncertain or fails,\nthe fallback path uses the original filter, maintaining correctness. The lookup algorithm for this\ndesign was given earlier as Algorithm 2.\nThe learned Bloom filter approach is more deeply integrated. The classes LearnedBloomRun ,\nLearnedBloomLevel , and LearnedBloomTree augment the LSM-tree with classifier-aware logic.\nEach level maintains both a classifier and a backup filter. During lookup, the classifier is queried; if\nit predicts positive, the backup Bloom filter is checked. If both are positive, the SSTable is queried;\notherwise, the lookup skips that level. This hybrid strategy preserves no-false-negative guarantees\nwhile significantly reducing filter size. These mechanisms are detailed in Algorithm 3 from the\nDesign section.\n5.7 Test Harness and Measurement\nWe evaluate each system using the script test performance.py , which loads trained classifiers,\ninitializes the LSM-tree, and executes thousands of GET operations for each workload. This harness\nrecords timings using high-resolution clocks, tracks filter hits and misses, and aggregates statistics\ninto CSV logs for post-analysis. Metrics such as average query latency, false positive counts, false\nnegative counts, and memory usage of each component are reported separately for each workload\nand system variant. Backup filter utilization is also tracked to measure reliance on fallback in the\nlearned filter design.\nThis tightly integrated pipeline—from synthetic workload generation, to model training and\ntest execution—ensures reproducibility and allows fine-grained insight into each design’s strengths\nand limitations.\n6 Results\nTable 2: Summary of metrics for each workload. Time is in µs; speedups are unitless ratios;\naccuracy and false-negative rates (FNR) are fractions in [0,1]; Bloom-filter checks and bypasses are\ncounts; bypass rate is in percent (%)\nMetric Random Sequential Level 0 Level 1 Level 2 Level 3\nAvg. Time (Trad.) 181 044 .16 391 728 .20 95 099 .23 347 564 .64 404 189 .19 395 528 .94\nAvg. Time (Clf.) 92 128 .62 182 724 .37 49 116 .78 222 866 .18 244 419 .10 173 646 .31\nAvg. Time (Lrn.) 180 972 .24 397 034 .16 94 935 .07 350 384 .13 409 692 .84 398 754 .20\nSpeedup (Clf.) 1 .97 2 .14 1 .94 1 .56 1 .65 2 .28\nSpeedup (Lrn.) 1 .00 0 .99 1 .00 0 .99 0 .99 0 .99\nAccuracy 0 .9100 0 .8300 1 .0000 1 .0000 0 .9700 0 .7500\ncontinued on next page\n12\n\ncontinued from previous page\nMetric Random Sequential Level 0 Level 1 Level 2 Level 3\nFNR (Clf.) 0 .1125 0 .2125 0 .0000 0 .0000 0 .0375 0 .3125\nFNR (Lrn.) 0 .0000 0 .0000 0 .0000 0 .0000 0 .0000 0 .0000\nBloom checks 476 855 180 900 862 837\nBloom bypasses 155 272 59 302 280 278\nBypass rate 32 .56 31 .81 32 .78 33 .56 32 .48 33 .21\nFNR Level 0 0 .0000 0 .0000 0 .0000 – – –\nFNR Level 1 0 .5000 0 .0000 – 0 .0000 – –\nFNR Level 2 0 .0000 0 .0345 – – 0 .0375 –\nFNR Level 3 0 .2727 0 .3265 – – – 0 .3125\nTable 2 consolidates all key metrics from six independent GET-only workloads. Each workload\nwas executed across multiple runs on identical SSTable data; the results shown correspond to the\nrun that best illustrates the consistent patterns observed in latency, accuracy, and filter-bypass\nbehavior. Average lookup latency is given for the Traditional, Classifier-augmented, and Learned-\nBloom variants. The Classifier achieves speedups of 1.97 ×(Random), 2.14 ×(Sequential), and up\nto 2.28 ×(Level 3), whereas the Learned-Bloom filter remains within 1% of the baseline across all\ntests. Classifier accuracy drops to 75% on the deepest level (FNR 31.25%), while the Learned-\nBloom variant incurs zero false negatives in every scenario. Bloom-filter checks and bypass counts\nconfirm that the Classifier bypasses approximately one-third of checks, directly translating to its\nlatency improvements.\nFigure 1: Average lookup latency across all workloads. Classifier (green) consistently reduces\nlatency; Learned-Bloom (yellow) overlaps Traditional (blue).\nFigure 1 plots the mean GET latency for each workload. For the Random workload, the\nTraditional tree averages 181 ms, which the Classifier cuts in half to 92 ms, while the Learned-\nBloom variant remains at 181 ms. In the Sequential workload, the Traditional average of 392 ms\n13\n\ndrops to 183 ms under the Classifier, again with Learned-Bloom at 397 ms. In focused Level 0\nlookups, latency falls from 95 ms (Traditional) to 49 ms (Classifier), with Learned-Bloom unchanged\nat 95 ms. Deeper levels show similar trends: at Level 1, latency moves from 348 ms to 223 ms; at\nLevel 2, from 404 ms to 244 ms; and at Level 3, from 396 ms to 174 ms. These reductions arise\nfrom the Classifier’s ability to skip roughly one-third of Bloom-filter probes, cutting the I/O path\nfor both positive and negative lookups.\nBetween workloads, the absolute benefit grows with level depth: Level 3 sees the largest absolute\ndrop of 221 ms, reflecting higher per-probe cost. The Learned-Bloom variant’s near-perfect overlap\nwith Traditional confirms that replacing large Bloom filters with compact models plus a tiny backup\nfilter incurs no measurable latency penalty.\nFigure 2: Speedup factors of Classifier and Learned variants over Traditional across all workloads.\nFigure 2 makes these improvements explicit as speedup ratios. The Classifier consistently\napproaches or exceeds 2 ×on Random (1.97 ×) and Sequential (2.14 ×) and peaks at 2.28 ×on Level\n3. Even on the warmest Level 0, it achieves a 1.94 ×speedup. In contrast, the Learned-Bloom filter\nremains at 1.00 ×for every workload, underscoring its drop-in compatibility: it preserves baseline\nperformance while eliminating false negatives.\nThese detailed measurements show that classifier-guided level skipping can halve average GET\nlatency by opportunistically bypassing costly filter probes, at the expense of a tunable false-negative\nrate, and that learned-Bloom filters provide a zero-FNR alternative with no speed trade-off.\n14\n\n(a) Random\n (b) Sequential\n(c) Level 0\n (d) Level 1\n(e) Level 2\n (f) Level 3\nFigure 3: Per-lookup latency profiles in a 3 ×2 grid. Classifier dips indicate Bloom-filter bypasses;\nLearned-Bloom aligns with Traditional.\n15\n\nFigure 4: Memory usage comparison between traditional and learned Bloom filters across levels.\nLearned variant achieves substantial reductions in memory footprint.\nFigure 4 illustrates the Bloom filter memory footprint for levels 1 through 3, comparing tra-\nditional filters against their learned counterparts. At each level, the learned variant achieves a\nsignificant size reduction. For instance, at Level 1, the traditional filter consumes nearly 68 KB,\nwhile the learned Bloom filter occupies only 12 KB—a reduction of over 80%. This pattern holds at\nLevels 2 and 3 as well, with savings consistent across the hierarchy. These gains validate the mem-\nory efficiency of hybrid learned filters, particularly when model inference latency remains within\nacceptable bounds.\nFigure 5: Average query time for traditional vs. learned Bloom filters across levels. Despite smaller\nsize, learned variant retains competitive query times.\nFigure 5 complements the previous plot by comparing average query times between traditional\nand learned Bloom filters across the same three levels. Interestingly, the learned variant maintains\nperformance parity despite the shift in internal structure. At Level 1, query latency drops from 2.9\n16\n\nµs (traditional) to under 0.3 µs (learned), indicating that compactness does not come at the cost\nof efficiency. Similar trends at Levels 2 and 3 reaffirm that learned filters can reduce both memory\nand access time simultaneously in certain settings.\n7 Discussion\nThe experiments presented demonstrate two distinct pathways for integrating machine learning\ninto the LSM-tree read path: classifier-guided filter skipping for latency reduction, and learned\nBloom filters for memory efficiency without performance loss. Both approaches leverage predictive\nmodels to replace or augment traditional data structures, but they occupy different points in the\ndesign space and entail unique trade-offs.\nLatency vs. Memory Footprint\nAt one extreme, the ML classifier approach optimizes pure lookup latency. By training a per-level\nbinary classifier on rich key features, we enable the system to avoid unnecessary Bloom-filter probes\non levels where the key is unlikely to reside. This yields dramatic speedups—up to 2.3 ×on cold\nLevel 3 lookups—at the cost of introducing a nonzero false-negative rate (FNR). In our experiments,\nthe classifier skipped over 30–33% of filters on average, translating directly into I/O savings and\nlower median latencies. However, the model itself occupies 5.6–6.1MB, and its rich feature set (45\nengineered features such as powers, logs, trigonometric and digit statistics) contributes to both\nmemory and computation overhead during inference.\nConversely, the learned Bloom filter approach maintains strict zero-FNR correctness while\nmatching the traditional design’s performance almost exactly (within 1%). By replacing large\nper-level Bloom filters (which can consume multiple megabytes) with compact classifiers (530–903\nKB) plus small backup filters, we reduce memory footprint by an order of magnitude without\nmeasurable latency penalty. This makes learned Bloom filters particularly attractive for memory-\nconstrained deployments—edge devices, mobile systems, or massive multi-tenant clusters—where\nmemory per key is at a premium.\nFalse Negatives and Spike Analysis\nA detailed look at the classifier’s per-operation latency profiles reveals that the number of downward\n“bypass” spikes far exceeds the count of actual false negatives reported by the model. This indicates\nthat most speed gains arise not from mispredictions but from correctly skipping filter checks where\nthe key genuinely does not exist. In other words, the classifier captures meaningful distributional\npatterns in the data—hot prefixes, common key ranges, or structural clustering—rather than merely\nexploiting its own errors. This insight suggests that even models with moderate FNR (e.g., 10–20%)\ncan achieve substantial latency benefits so long as their true-negative precision is high.\nModel Training and Deployment\nA critical concern for learned LSM-trees is the cost and complexity of training and updating models\nin a live system. Our flush-triggered training pipeline mitigates these concerns by decoupling model\nupdates from query serving. Upon each MemTable flush, a background job locks only the meta-\ndata necessary to extract new key-level labels, trains the classifier or learned Bloom filter on fresh\ndata, and then atomically swaps in the new model once training completes. This nonblocking, ver-\nsioned approach ensures continuous availability with no service interruption. Moreover, incremental\n17\n\ntraining techniques—warm-starting from previous model weights, streaming feature updates, and\nonline gradient updates—can further reduce training latency and enable true real-time adaptability\nto workload shifts.\nScalability and Sampling Strategies\nAs database size grows, so do model training costs and feature engineering challenges—especially\nfor lower levels containing millions of keys. In our implementation, we subsampled 10–20% of\nLevel 2 and Level 3 keys to bound training time. While effective, this sampling can degrade\nmodel accuracy on rarely accessed tail keys, contributing to the slightly higher FNR observed\nat deeper levels. Future work should explore adaptive sampling—over-sampling rare key ranges,\nunder-sampling dense hot ranges, or using stratified sampling based on access frequency—to balance\nrepresentativeness with training cost. Alternatively, online learning algorithms that continuously\nadjust model parameters on individual operations could eliminate the need for explicit sampling\naltogether.\nDynamic Thresholding and Model Calibration\nOur current classifiers use a fixed decision threshold (0.5) to determine filter skipping. However,\ndifferent workloads and levels may benefit from customized thresholds—lowering the bar in deeper\nlevels to reduce false negatives, or raising it in hot levels to maximize bypass rates. Implementing\ndynamic thresholding, based on real-time monitoring of FPR/FNR trade-offs, could yield additional\ngains. Similarly, calibrating model confidence (e.g., via temperature scaling) would allow the system\nto adjust its aggressiveness in response to workload changes, striking a balance between latency,\naccuracy, and memory usage.\nPredictive Fence Pointers and Multiclass Models\nBeyond Bloom filters, fence pointers are another natural candidate for learned acceleration. In a\nstandard LSM-tree, fence pointers divide each SSTable into fixed-size page ranges and use binary\nsearch over these pointers to locate a key—an O(logP) operation where Pis the number of pages.\nA small regression model could instead predict the page index directly, yielding an O(1) jump to\nthe approximate offset. Because binary search over tens or hundreds of pages is already quite fast,\nthe absolute latency reduction may be modest, but even a few cache-line savings per lookup can\ncompound across high-throughput workloads. Implementing and evaluating such a model would\nclarify whether learned fence pointers can meaningfully supplement or replace pointer arrays in\nproduction systems.\nOur current classifier design uses independent binary models at each level. An alternative\nis a single multiclass predictor that directly outputs the most likely level for a given key. This\ncould eliminate multiple per-level inferences and simplify the lookup path. Initial experiments with\nmulticlass forests and shallow neural nets yielded lower accuracy—likely due to the larger label\nspace—but suggest that more expressive architectures (e.g., boosted trees with level-aware features\nor transformer-based key embeddings) might succeed. A reliable multiclass model could also drive\nprefetching or cache warming, loading the predicted SSTable’s pages ahead of the actual lookup\nand further reducing end-to-end latency.\n18\n\nCo-Design with Compaction and Layout Policies\nLearned models do not exist in isolation—their efficacy depends on the underlying data layout and\ncompaction policy. For instance, a more aggressive leveling schedule that reduces SSTable overlap\ncould simplify model features and improve prediction accuracy. Conversely, a tiered compaction\npolicy that retains more cold data in L0 might favor learned Bloom filters by reducing model\ncomplexity for deeper levels. Jointly tuning compaction thresholds, level size ratios, and model\narchitectures in an end-to-end co-design framework is an exciting direction for future research.\nWorkload Adaptivity and Continuous Learning\nFinally, real-world access patterns evolve over time. The ability to continuously monitor model\nperformance—tracking drift in FNR, bursts in false positives, or shifts in bypass efficacy—and to\ntrigger retraining or model reconfiguration is essential for long-lived systems. Building an adaptive\ncontrol loop that integrates telemetry, automatic retraining, and dynamic kernel updates would\nclose the gap between research prototypes and production-ready learned LSM-trees.\nIn summary, our work demonstrates the feasibility and benefits of learned auxiliary structures\nin LSM-trees, and opens multiple avenues for refinement: from adaptive sampling and threshold\ntuning, through predictive fence pointers and multiclass predictors, to co-design with compaction\npolicies and continuous learning frameworks. These insights underscore the rich potential of blend-\ning data systems with modern learning techniques.\nReferences\n[1] Burton H Bloom. Space/time trade-offs in hash coding with allowable errors. Communications\nof the ACM , 13(7):422–426, 1970.\n[2] Peng Dai and Anshumali Shrivastava. Ada-bf: Adaptive bloom filter with learned model. In\nProceedings of the 2020 ACM SIGMOD International Conference on Management of Data ,\npages 1551–1566, 2020.\n[3] Niv Dayan and Stratos Idreos. Monkey: Optimal navigable key-value store. In Proceedings of\nthe 2018 International Conference on Management of Data , pages 79–94, 2018.\n[4] Andreas Kipf, Tobias Kipf, Benedikt Radke, Alfons Kemper, and Thomas Neumann. Learned\ncardinalities: Estimating correlated joins with deep learning. In CIDR , 2019.\n[5] Tim Kraska, Alex Beutel, Ed H Chi, Jeff Dean, and Neoklis Polyzotis. The case for learned\nindex structures. Proceedings of the 2018 International Conference on Management of Data ,\npages 489–504, 2018.\n[6] Michael Mitzenmacher. A model for learned bloom filters and optimizing by sandwiching. In\nProceedings of the 36th ACM Symposium on Principles of Distributed Computing , pages 123–\n131, 2018.\n[7] Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. arXiv preprint\narXiv:2006.09123 , 2020.\n[8] Jack Rae, Sharan Dathathri, Tim Rockt¨ aschel, Emilio Parisotto, Theophane Weber, and Ed-\nward Grefenstette. Meta-learning neural bloom filters. arXiv preprint arXiv:1905.10512 , 2019.\n19\n\n[9] Song Tsai, Jeff Johnson, Matthijs Douze, and Herve Jegou. Learning to index for nearest\nneighbor search. In International Conference on Learning Representations (ICLR) , 2020.\n20",
  "textLength": 51035
}