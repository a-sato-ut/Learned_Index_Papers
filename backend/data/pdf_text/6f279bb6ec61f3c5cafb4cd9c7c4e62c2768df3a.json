{
  "paperId": "6f279bb6ec61f3c5cafb4cd9c7c4e62c2768df3a",
  "title": "Meta-Sketch: A Neural Data Structure for Estimating Item Frequencies of Data Streams",
  "pdfPath": "6f279bb6ec61f3c5cafb4cd9c7c4e62c2768df3a.pdf",
  "text": "Meta-Sketch: A Neural Data Structure\nfor Estimating Item Frequencies of Data Streams\nYukun Cao , Yuan Feng , Xike Xie*\nSchool of Computer Science and Technology, University of Science and Technology of China\nData Darkness Lab, MIRACLE Center, Suzhou Institute for Advanced Research, USTC\n{ykcho, yfung}@mail.ustc.edu.cn, xkxie@ustc.edu.cn\nAbstract\nTo estimate item frequencies of data streams with limited\nspace, sketches are widely used in real applications, including\nreal-time web analytics, network monitoring, and self-driving.\nSketches can be viewed as a model which maps the identiﬁer of\na stream item to the corresponding frequency domain. Starting\nfrom the premise, we envision a neural data structure, which\nwe term the meta-sketch, to go beyond the basic structure of\nconventional sketches. The meta-sketch learns basic sketching\nabilities from meta-tasks constituted with synthetic datasets\nfollowing Zipfdistributions in the pre-training phase, and can\nbe fast adapted to real (skewed) distributions in the adaption\nphase. Extensive experiments demonstrate the performance\ngains of the meta-sketch and offer insights into our proposals.\nIntroduction\nEstimating item frequency is a basic topic in data stream pro-\ncessing, which ﬁnds applications in the ﬁelds of networking,\ndatabases, and machine learning, such as real-time data ana-\nlyzing (Weller 2018; Zhu and Shasha 2002; Tinati et al. 2015;\nIrfan and Gordon 2019), network trafﬁc monitoring (Huang,\nLee, and Bao 2018; Madden and Franklin 2002; Wang et al.\n2013), natural language processing (Goyal, III, and Cormode\n2012) and search ranking (Dzogang et al. 2015). Towards\ninﬁnite data streams, a common class of solutions (Cormode\nand Muthukrishnan 2005; Charikar, Chen, and Farach-Colton\n2002; Estan and Varghese 2002; Roy, Khan, and Alonso 2016;\nZhou et al. 2018; Hsu et al. 2019) use a compact structure tak-\ning sublinear space for counting the number of occurrences\nof each stream item, called the sketch.\nUnder the prevalent evidence of skewed distributions in\ndata streams, basic sketches achieve the space compactness\nby hashing and approximately aggregating stream items. Ba-\nsic sketches, including CM-sketch (Cormode and Muthukr-\nishnan 2005), C-sketch (Charikar, Chen, and Farach-Colton\n2002) and CU-sketch (Estan and Varghese 2002), use a 2D\narray of counters as the core structure. Some varients (Li\net al. 2020; Zhong et al. 2021; Gao et al. 2022; Liu and Xie\n2021) broaden application scenarios based on basic sketches.\nTo optimize the sketching performance, several augmented\nsketches (Roy, Khan, and Alonso 2016; Zhou et al. 2018)\n*Xike Xie is the corresponding author.\nCopyright © 2023, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.were proposed, which attach ﬁlters to basic sketches, to cap-\nture the preliminary patterns of skewed distributions (e.g.,\nhigh/low-frequency items). By separately maintaining the\nﬁltered high/low-frequency items, augmented sketches strive\nto eliminate the estimation error incurred by hash collisions\nbetween the high- and low-frequency items. Further, learned\naugmented sketches (Hsu et al. 2019) improve the ﬁlters of\nthe augmented sketches by memorizing short-term high/low-\nfrequency items via a pre-trained neural network (NN in\nshort) classiﬁer. But it is not clear how the pre-trained NN\ncan be adapted to dynamic streaming scenarios, where the\ncorrespondence between items and frequencies varies. In\na nutshell, sketches are structures compactly summarizing\nstreams to count item frequencies with limited space budgets.\nFrom the retrospective analysis, an observation can be\ndrawn that the evolution of sketches conforms with the ex-\nploitation of data distributions. It is thus a natural evolution\nto consider a sketch that generally and automatically cap-\ntures more distribution patterns with limited space budget.\nIn this paper, we envision a novel neural sketch, called the\nmeta-sketch, with techniques of meta-learning and memory-\naugmented neural networks. The meta-sketch learns the\nsketching abilities from automatically generated meta-tasks.\nDepending on the types of meta-tasks, we study two versions\nof the meta-sketch, called basic andadvanced meta-sketches.\nThe basic meta-sketch implements the simulation of basic\nsketches, through the training process with basic meta-tasks\nfollowing Zipfdistributions, which are prevalent in the scenes\nof real data streams (Kolajo, Daramola, and Adebiyi 2019;\nZeng and Li 2014; Babcock et al. 2002; Cormode et al. 2012;\nPhridviRaja and GuruRao 2016). The advanced meta-sketch\nextends the basic version to fast adapt to the speciﬁc run-\ntime of stream processing, through the training with adaptive\nmeta-tasks, which are generated by online sampling of real\ndata streams. Our work follows a typical setting where the\ndistribution of item frequencies follows a skewed distribu-\ntion, but the correspondence between items and frequencies\nvaries. For example, in software-deﬁned networks (SDN),\nsketches are deployed to programmable switches to collect\nper-ﬂow statistics, where IP packets follow heavy-tailed dis-\ntributions (Tang, Huang, and Lee 2019; Hsu et al. 2019).\nIn distributed databases, it gives advances to collect statis-\ntics of data shards to optimize data placement and query\ncaching, where query phrases follow approximate Zipf dis-\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n6916\n\nStore\nQuery\nFigure 1: The Framework of the Meta-Sketch\ntributions (Hsu et al. 2019). Given that the item population\nfollows a speciﬁc distribution, the local distributions, i.e.,\nitem-frequency correspondences on shards or ﬂows, are dif-\nferent. Instead of retraining learned augmented sketches on\neach local distribution, the advanced-sketch can be quickly\nadapted to different local distributions once trained.\nAs a member of the neural data structure family (Kraska\net al. 2018; Rae, Bartunov, and Lillicrap 2019; Hsu et al.\n2019; Mitzenmacher 2018), the meta-sketch signiﬁcantly dif-\nfers from conventional sketches, in terms of the structure and\nworking mechanism. The meta-sketch utilizes NN’s powerful\nencoding/decoding capabilities to perceive data distributions\nand express and compress explicit or implicit information to\nretrieve item frequencies with better accuracies. Meanwhile,\nthe meta-sketch is differentiable to fully perceive frequency\npatterns for self-optimization.\nOur contributions are as follows. 1)We propose the meta-\nsketch, the ﬁrst neural data structure for the problem of item\nfrequency estimation, based on meta-learning. 2)The basic\nmeta-sketch acquires sketching abilities by learning from\nsynthetic datasets and outperforms basic sketches in real\ndatasets. The advanced meta-sketch automatically encom-\npasses the ability analogous to the auxiliary structures delib-\nerately devised in (learned) augmented sketches, yet yields\nbetter accuracies and robustness when adapted to dynamic\nscenes. 3)Through extensive empirical studies on real and\nsynthetic datasets, we evaluate our proposed meta-sketches\nand analyze the mechanism of major modules.\nMeta-Sketch Structure\nPreliminaries\nWe consider a standard data stream scenario (Cormode et al.\n2012). Suppose a stream SN:fe1;:::;eNgwithNitems and\nndistinct items. Each item ei2SNtakes a value from the\nitem domain X=fx1;:::;xngwherexi6=xj. The frequency\nfiis equal to the number of times that item xiappears inSN.\nTo leverage learning techniques for item frequency esti-\nmation, a naïve way is to train a NN model (e.g., MLP/L-\nSTM) that learns/memorizes the mapping relationship be-\ntween items and frequencies with multiple training iterations,\nsimilar to (Kraska et al. 2018; Hsu et al. 2019; Mitzenmacher\n2018). However, it violates the typical setting of stream pro-\ncessing where item observations are transient and are there-\nfore handled in one pass (Babcock et al. 2002). More, the\ncostly procedure has to be repeated from the scratch for a\nnew data stream. Inspired by the meta-bloom ﬁlter (Rae, Bar-\ntunov, and Lillicrap 2019), we consider a case of one-shot\nlearning (ﬁtting for one-pass stream processing) by using\nmeta-learning (Hospedales et al. 2020; Santoro et al. 2016)\nand memory-augmented networks (Graves, Wayne, and Dani-\nhelka 2014; Graves et al. 2016). Meta-learning employs sam-pled meta-tasks to learn the ability to solve a class of domain\ntasks rather than memorizing patterns for a speciﬁc task. The\nmemory-augmented networks incorporate external memories\ninto NN models, signiﬁcantly enhancing the potentials of\nNN models with more learnable parameters. Meanwhile, it\nperforms efﬁcient and explicit operations (i.e., reading and\nwriting) for external memories, allowing NN models to pro-\ncess information similarly to conventional data structures.\nThe framework of the meta-sketch consists of 4functional\nmodules, Embedding (FE),Sparse addressing (FSa),Com-\npressed storage matrix (M), and Decoding (Fdec), as shown\nin Figure 1. Like traditional sketches, the meta-sketch en-\ncodes and memorizes online stream items in one pass, and\nanswers queries by decoding corresponding item-frequency\ninformation from the structure.\nThus, we deﬁne 2operations, Store andQuery. Speciﬁcally,\ntheStore operation ﬁrst passes each incoming stream item\ntoFEfor the embedding representation, and then writes the\nembedding vector into M, according to the address derived\nbyFSa. When estimating the frequency of an item, the Query\noperation calculates the item’s address in MviaFSa, reads\nthe corresponding information vector from Mand decodes\nthe frequency byFdecfrom the retrieved information vector.\nModules\nEmbedding. The moduleFEhas two purposes: 1)perform-\ning representational transformation for an incoming item\neiand mapping it into a dense embedding vector zithat\nholds implicit features about item-frequency distributions\nand serves as the basis for identifying stream items; 2)decou-\npling the embedding vector zito obtain a reﬁned vector ri,\nwhich is used to derive the address for reading/writing on the\ncompressed storage matrix M.\nAccordingly,FEconsists of the embedding network gemb\nand the address network gadd. We assume that an item\nei2SNis numerically encoded for the unique identiﬁcation,\nfollowing the conventions of stream processing (Babcock\net al. 2002; Cormode et al. 2012). Thus, we have zi;ri \nFE(ei);wherezi gemb(ei)andri gadd(zi). Here,\nzi2Rlzis an embedding vector of dimension lz, and\nri2Rlris a reﬁned vector of dimension lr. The vector\nziserves multiple intents: 1)it makes a basis for deriving the\naddress of an item in FSa;2)it serves as the compressed vec-\ntor of an item written into M;3)it works as a partial input of\nFdecfor decoding the item frequency; 4)it also plays the role\nof perceiving/compressing patterns of a speciﬁc frequency\ndistribution, as discussed in analysis section. In addition, to\nenhance the addressing functionality and eliminate other in-\nterference factors, we decouple zito generate a reﬁned vector\nri, instead of using zidirectly for the addressing.\nSparse Addressing. The moduleFSaaims to derive the\naddressaifor storing the embedding vector ziinto the storage\nmatrix:ai FSa(ri). In terms of functionality, FSais\nanalogous to the hash functions of traditional sketches, except\nthatFSais parameterized and differentiable. Speciﬁcally, the\naddressing of the meta-sketch is done via a 3D addressing\nmatrixAof parameters to be learned and a sparse SoftMax\nfunction:ai SparseMax(rT\niA), whereA2Rd1\u0002lr\u0002d2.\n6917\n\nThen, the batch matrix multiplication of Aand the transpose\nofriresults in the addressing vector ai2Rd1\u00021\u0002d2.\nThe setting of d1andd2determines the size of address\nspace for storing the embedding vectors. Typical addressing\nmethods (Rae, Bartunov, and Lillicrap 2019; Graves et al.\n2016) use a 2D matrix ( lr\u0002d2) for recording the mapping\nof an embedding vector to a slot ( d2is the number of slots).\nIn contrast, we add one more dimension d1to simulate the\nmulti-hash setting of traditional sketches, in view of that a\n2D addressing matrix can reach a differentiable simulation of\na hash function (Rae, Bartunov, and Lillicrap 2019; Mitzen-\nmacher 2018). Matrix Asimulates multiple hash functions,\nyielding robust frequency decoding and the rationality of\nthe learning optimization. Note that each 2D slice A\u0003ofA\nis stacked from d2-unit vectors bi2Rlrby normalizing\nthe parameters of Aat each gradient update of the training\nprocess. Normalized Acan avoid overﬂowing when com-\npressing its size by reducing data precisions and enhancing\nthe interpretability (see analysis section).\nIn addition, we utilize sparse SoftMax (Martins and As-\ntudillo 2016; Laha et al. 2018) instead of SoftMax to nor-\nmalize the address ai. It brings the following beneﬁts by\nconstraining some bits of aito zero, which 1)promotes quick\nderivation during the back-propagation; 2)reduces the over-\nhead of storage matrix accessing by skipping the slots of M\ncorresponding to the “0” bits of ai;3)leads to de-noising\nwith the vector compression.\nCompressed Storage Matrix. We use a matrix M2\nRd1\u0002lz\u0002d21to store an embedding vector zi2Rlzin accor-\ndance to its address ai2Rd1\u00021\u0002d2. The functionality of M\nis similar to the 2D array of counters in traditional sketches,\nyet yields better storage compression. Traditional sketches\nstore item counts. Differently, Mstores embedding vectors,\nwhich have richer information compression capabilities, due\nto the diversity of value change on different bits.\nDecoding. Given a query item xi, the moduleFdec, con-\nsisting of one NN component gdec, decodes the information\ncorresponding to xi, to obtain the estimated frequency ^fi. The\nvector fed into gdecis the concatenation of vector fM\taig,\nvectorzi, and the current number of items (i.e., N) recorded\nin a counter, ^fi gdec(fM\taig;zi;N). The operator\t\nrefers to the reading operation for the storage matrix. The ba-\nsic form of\tgives the operation as M\tai=MaT\ni2(Graves,\nWayne, and Danihelka 2014; Graves et al. 2016). We con-\nsider two optimized forms of \t, inspired by the “count-min”\nmechanism of the CM-sketch. The ﬁrst one gives the mini-\nmum value of each row in MaT\ni, aiming to remove the noise\nof other items. The second one gives the minimum value of\neach row in MaT\ni\u000e1\nzi, a normalized form of MaT\ni. Here,\n\u000edenotes the Hadamard product, and zirequires broadcast\noperations to comply with its requirements. So, fM\taig\nrefers to the concatenation of vectors generated by the basic\nform and the two optimized forms.\nOperations\nOperation Store is performed by feeding an incoming item\n1In this paper, we control lr:lz\u00191 : 5 to compress A.\n2aT\nimeans transpose operation for dim 1 and d2Algorithm 1: Operations\n1Operation Store(e i,M):\n2zi;ri F E(ei);ai F Sa(ri);M M+ziai;\n3Operation Query(x i,M,N):\n4zi;ri F E(xi);ai F Sa(ri);\n5 return ^fi F dec(fM\taig;zi;N);\nAlgorithm 2: Training Framework\nData: Learnable parameters \u0012of Meta-Sketch, Sampler R;\n1whileinot reach max training steps do\n2 Sample a meta-task ti:fsi;qig\u0018Rand countN;\n3 fore(i)\nj2sidoStore(e(i)\nj,M);end\n4 forx(i)\nj;f(i)\nj2qido^f(i)\nj Query(x(i)\nj,M,N);\nL+=LossFun(f(i)\nj;^f(i)\nj);\n5 Backprop through: dL=d\u0012 and update parameters \u0012\n6 Normalize A and Clear M;\n7end\neitoFEandFSato obtain embedding vector ziand ad-\ndressai, and then additively writing zitoM, weighted by ai:\nM M+ziai. Here, other writing types (Graves, Wayne,\nand Danihelka 2014; Graves et al. 2016; Rae, Bartunov, and\nLillicrap 2019; Santoro et al. 2016) can also be employed, but\nsimple additive writing is more efﬁcient and allows to com-\npute gradients in parallel (Rae, Bartunov, and Lillicrap 2019).\nIn addition, additive writing also allows to deﬁne Delete\noperation for meta-sketch (see the supplement materials3).\nOperation Query estimates the frequency of a given query\nitemxi. First,ziandaiare obtained, similar to that of oper-\nation Store. Then, the vectors fM\taigare retrieved from\nMandNcan be easily obtained by a small counter. Finally,\nfM\taig,ziandNare jointly fed into gdecto get the es-\ntimated frequency ^fiofxias the returned value. The two\noperations are shown in Algorithm 1.\nMeta-Sketch Training\nTraining Framework\nThe meta-sketch employs an efﬁcient one-shot meta-training\nmethod (Vinyals et al. 2016). The training process thus con-\ntains two phases, pre-training andadaption phases. In the\npre-training phase, the meta-sketch learns an initial set of\nmodule parameters, including gemb,gadd,A, andgdec. The\npre-training goes ofﬂine across training units, i.e., basic meta-\ntasks, to acquire the ability of stream frequency estimation.\nThen, in the adaption phase, the pre-trained meta-sketch goes\nfast across a set of light-weighted training units, i.e., adaptive\nmeta-tasks, to quickly acquire the task-speciﬁc knowledge.\nThe training units, i.e., meta-tasks, are crucial for both\nphases. The training process of the meta-sketch on a sin-\ngle meta-task is equivalent to simulating storing and query-\ning a data stream instance while computing the error to\noptimize the learnable parameters. Thus, a meta-task ti\nconsists of a store set si(also called a support set) and\n3https://github.com/FFY0/meta-sketch/blob/main/Sup_ms.pdf\n6918\n\na query set qi. The store set sican be viewed as an in-\nstance of data streams, si:fe(i)\n1;:::;e(i)\nNig, whereNiis the\nnumber of stream items. The query set qican be represented\nby a set of items with paired frequencies in si, formally,\nqi:f(x(i)\n1:f(i)\n1);:::; (x(i)\nni:f(i)\nni)g, whereniis the number of\ndistinct items in si. In this work, we deﬁne two types of meta-\ntasks, basic andadaptive meta-tasks, corresponding to the\npre-training and adaption phases, respectively.\nThe two training phases, that are based on different types of\nmeta-tasks, follow the same training framework, as shown in\nAlgorithm 2, except for the sampler and initial parameters. To\nreduce both absolute and relative errors,i.e.AAE and ARE,4,\nwe devise an adaptive hybrid loss function (Kendall, Gal, and\nCipolla 2018) for the meta-sketch:1\n2\u001b2\n1(fi\u0000^fi)2+1\n2\u001b2\n2jfi\u0000\n^fij=fi+log\u001b 1\u001b2, where\u001b1and\u001b2are learned parameters.\nBasic Meta-Task Generation\nIn the pre-training phase, basic meta-tasks should make the\nmeta-sketch to simulate traditional sketches and preserve\ncertain generality without relying too much on the patterns\nof speciﬁc distributions. Therefore, we generate meta-tasks\nbased on the Zipf distribution, which is found to be prevalent\nin real scenes of data streams (Kolajo, Daramola, and Ade-\nbiyi 2019; Zeng and Li 2014; Babcock et al. 2002; Cormode\net al. 2012; PhridviRaja and GuruRao 2016). A meta-task is\nessentially a data stream instance with item size n, which can\nbe determined by the total number of items Nand the rela-\ntive frequency distribution p. We can generate meta-tasks by\npresupposing different n,\u0016fandp, where \u0016fis the frequency\nmean, since N=\u0016f\u0002n. Thus, basic meta-task generation is\nbased on a sampler R:fI;L;Pg, as follows.\nAnitem poolIis a subset of item domain X. If the item\ndomain is known a-priori, it can be directly taken as the\nitem pool. Otherwise, if the item domain is only partially\nknown or even unknown, the item pool can be constructed\nby sampling from historical records. Even if the item pool\ndoes not completely cover the item domain, the “missing”\nitem can still be identiﬁed, due to the homogeneity of the\ndomain-speciﬁc embedding space, given that the number of\ndistinct items is less than the item pool capacity jIj.\nAfrequency mean range Lis the range for the frequency\nmean \u0016f. One can get the value of \u0016fby statistics of sampled\nstream instances and extract the min and max \u0016fs to buildL.\nAdistribution pool Pconsists of many instances gener-\nated according to different parameters of relative frequency\ndistributions. In this paper, we consider a family of Zipfdistri-\nbutions (Adamic 2000) with varied parameter \u000b, as the base\nfor constructing P.\u000bcan be selected from a wide range to\nhave a good coverage of different distributions.\nNotice that the meta-tasks are for the meta-sketch to learn\nthe sketching ability, instead of spoon-feeding the meta-\nsketch to mechanically memorize the parameters of R. It\nmeans that the trained meta-sketch has the generalization\nability to handle the case not covered in R. The generation of\na meta-task tican be done based on sampler R, as follows.\n4AAE =1\nnPn\ni=1jfi\u0000^fij;ARE =1\nnPn\ni=1jfi\u0000^fij\nfi.We ﬁrst randomly sample a subset of niitems fromI, and\na frequency mean \u0016fi2L. Then, we sample a distribution\ninstancepi2Pand make the niitems’ frequencies conform\ntopiand\u0016fi. For example, the frequencies of niitems can be\nset asni\u0002\u0016fi\u0002pi, wherepi\u0018Zipf (\u000b)is a random variable.\nThe above steps are repeated until the siandqiare built.\nAdaptive Meta-Task Generation\nWhile processing real data streams, we can get the item set Ir\nand its distribution prby online sampling. Irandprare then\nused for generating the set of adaptive meta-tasks. For each\nadaptive meta-task, an item subset is sampled from Ir, and\nthe relative frequency corresponding to each item is sampled\nfrompr. The process is similar to the generation of basic\nmeta-tasks. The only difference from basic meta-task gener-\nation is that there is no distribution pool anymore because\nthe real data stream is unique. Also, we intentionally random-\nize the correspondence between an item and its real relative\nfrequency on the original data records. It is equivalent to\nconstructing meta-tasks where the item frequencies dynami-\ncally change. For example, the frequency of an item may ﬁrst\nincrease, then suddenly drop (Tang, Huang, and Lee 2019).\nWith adaptive meta-tasks, the meta-sketch learns to quickly\nadapt to the distribution pr, while being ﬂexible against the\nitem frequency change. The detailed algorithms of generating\nmeta-tasks are shown in supplement materials3.\nExperiments\nBasic Setup\nDataset. For fair comparison with all competitors, we choose\ntwo widely used real datasets in data stream ﬁeld. Word-query\n(Wq) is a streaming records of search queries, where each\nquery contains multiple words (e.g., “News today”) (Hsu\net al. 2019). IP-trace (It) consists of IP packets, where\neach packet is identiﬁed by a unique IP address pair (e.g.,\n192.168.1.1/12.13.41.4) (Tang, Huang, and Lee 2019). IP-\ntrace follows heavy-tailed distributions and the Word-query\nfollows Zipﬁan distributions. All items in the two datasets\nare numerically encoded, similar to (Hsu et al. 2019).\nBaseline. We hereby evaluate the basic and advanced meta-\nsketches (BMS and AMS). CM-sketch (CMS) and C-sketch\n(CS) are the basis for other sketch variants and the commonly\naccepted baselines. So we choose them as competitors to\nbasic MS (after the pre-training phase). We compare the\nadvanced MS (after the adaptation phase) with two variants of\nCM/C sketches, learned augmented sketch (LS) and cold ﬁlter\n(CF), which leverage auxiliary structures and both are the\nstate-of-the-art in their own category. According to the default\nsetting (Cormode and Muthukrishnan 2005; Charikar, Chen,\nand Farach-Colton 2002), the number of hash functions for all\nsketches is 3. We adopt two standard metrics for evaluating\nthe accuracies of frequency estimation, AAE and ARE4.\nParameters. We implement gemborgaddin MLP with\n2-layers of sizes 128and48, followed by batch normaliza-\ntion, andgdecin a MLP with 3-layers of 256with residual\nconnections. We use the relu function for layer connections.\nThe space budget Bis spent on storing M, the same as the\nsetting in neural data structures (Rae, Bartunov, and Lillicrap\n6919\n\nn 5K 10K 20K 40K\nB 9KB 11KB 13KB 15KB\nBMS\n(Wq)ARE 12.30 14.74 10.98 13.79\nAAE 31.54 38.54 40.63 53.67\nCS\n(Wq)ARE 32.94 57.97 98.01 162.43\nAAE 57.54 101.44 172.44 282.59\nCMS\n(Wq)ARE 21.34 48.33 111.82 239.11\nAAE 38.04 84.62 195.61 416.01\nBMS\n(It)ARE 3.00 1.51 2.97 1.13\nAAE 5.57 5.01 6.94 5.56\nCS\n(It)ARE 6.08 9.94 15.57 24.49\nAAE 10.42 16.82 26.46 41.91\nCMS\n(It)ARE 8.12 16.07 32.77 65.19\nAAE 13.67 27.39 55.29 110.65\nTable 1: Results of Basic Meta-Sketch (T r)\n5k 9k 13k 17k\nSpace_budget0102030405060AREBasic MS\nCMS\nCS\n(a) ARE w.r.t. B\n1000 2000 3000 4000 5000\nItem_size051015202530ARE\nBasic MS\nCMS\nCS (b) ARE w.r.t. n\nFigure 2: Basic Meta-Sketch w.r.t. Space Budget/Item Size\n2019). Other modules, like hashing libraries, are commonly\naccepted as reusable and amortizable resources for multi-\ndeployment of sketches (Rae, Bartunov, and Lillicrap 2019;\nTang, Huang, and Lee 2019). Note that due to space limita-\ntions, the details and methods of parameter settings of M(A),\nthe ablation experiments and some parameter discussions are\nshown in the supplement materials3.\nBasic Meta-Sketch\nSettings. For each dataset, we train the basic MSs under 4\nitem pools withf5K; 10K; 20K; 40Kgdifferent items, re-\nspectively. The meta-task samplers are with Zipfdistributions.\nWe build the distribution pools set with \u000b2[0:8;1:3]and set\nfrequency mean range L= [50; 500]. For basic meta-sketch\ntraining, the default maximum number of training steps \u001eis5\nmillion, the learning rate is 0:0001 , and theAdam optimizer\nis used. For evaluation, we consider two types of tasks, Tr\nandTs.Trare directly obtained by random sampling on two\nreal data streams with different values of n, i.e., the number\nof distinct items. Note that frequency distributions of Trare\nnot necessarily obey Zipfdistributions. Tsare the synthetic\ntasks, where the frequency follows the Zipfdistribution with\n\u000b2f0:5; 1:1;1:5g. To evaluate the generability and stability\nof basic MS, both Ts(0:5) andTs(1:5) ’s distributions are not\ncovered by the distribution pool of the meta-task samplers.\nPerformance. Table 1 shows the performance of all com-\npetitors based on real dataset Tr. It shows that the basic MS\noutperforms traditional basic sketches, i.e., CMS and CS, on\nall testing cases. For example, the results on IP-trace show\nthat, whenn=40 K,B=15KB, the ARE of basic MS is 1.13,\nwhile AREs of CMS and CS are 65.19 and 24.49, respectively.\nThe advantage of meta-sketch is signiﬁcant when testing on\nTswith different \u000bs, as shown in Table 2. Note that we use\nrandom choices to simulate the ideal hash functions for tradi-\n0%20%40%60%80%100%\nNew_item_ratio41220283644Metrics\nARE\nAAE(a) New Items\n5e+6 5e+5 5e+4 5e+3 5e+2\nTrue_mean5e+6\n5e+5\n5e+4\n5e+3\n5e+2Estimate\n5e+6 5e+5 5e+4 5e+3 5e+2\nTrue_mean26101418ARE\n (b) New Means\nFigure 3: Generalization\n0.00.20.40.60.81.0\nShuffle_ratio203040506070AAE\nAdvanced MS\nLCMS\nLCS\nFigure 4: LS/MS\n012345\nTrain_step1e50102030405060Norm\n|r|\n|z|\n012345\nTrain_step1e51591317Sparsity of a\n Figure 5:jrj,jzjw.r.t. Sparsity of a\ntional sketches (Hsu et al. 2019), so that CS and CMS have\nthe same result with the same \u000bin both datasets.\nWe show the trend of ARE w.r.t. the space budget, in\nFigure 2 (a) ( Tr,n=5 K, Wq). Compared to the dramatic per-\nformance degrading of traditional sketches, basic MS holds\nstable performance. We show that the trend of ARE w.r.t.\nthe number of distinct items in Figure 2 (b) ( Tr,B=9KB,\nWq). Compared to traditional sketches, the ARE of basic\nMS increases sub-linearly w.r.t. the value of n. The AAE has\nsimilar results, see the supplement materials3.\nGeneralization. We test the generality of basic MS to\nnew items that are not in the item pool of the meta-task\nsampler in Figure 3 (a). We make the experiments ( n=5K,\nB=9KB, Wq) by replacing some items in Trwith new items,\nand vary the fraction of new items to observe the trend of\nthe performance. It shows that the ARE/AAE moderately\nincreases w.r.t. the ratio of new items. The performance is\nacceptable considering the fact that the item domain is of-\nten stable in practical applications. We then test the gener-\nality of meta-sketches to varied frequency means that are\nnot in range Lof the meta-task sampler, as shown in Fig-\nure 3 (b). The experiment ( n=5K,B=9KB, Wq) is done\nby sampling a series of Tstasks with frequency means in\nf500; 5K;50K; 500K; 5000Kg. It shows that as the mean\nof the true frequencies increases, the estimated frequencies of\nmeta-sketch increase linearly, so that the ARE keeps stable.\nAdvanced Meta-Sketch\nSettings. The generation of adaptive meta-tasks is similar to\nthat of basic meta-tasks, except that each item pool reads real\nfrequency distributions for the adaption as described in the\nadaptive meta-task generation section. In the adaption phase,\nthe maximum number of training steps is 0:002\u0003\u001e.\nPerformance. Table 3 compares the performance of ad-\nvanced MS with traditional sketches and their variants, LS\nand CF, on real dataset Tr. We implement two LSs accord-\ning to (Hsu et al. 2019), learned CM-sketch (LCMS) and\nlearned C-sketch (LCS), following the default setting that\n(top 1%) high-frequency items are separately stored. For CF,\nwe follow the parameter setting in (Zhou et al. 2018), and use\nCF40, CF70, and CF90 for setting the ﬁlter percentages to\n6920\n\nn=5K B=9KB n=10K B=11KB n=20K B=13KB n=40K B=15KB\n0.5 1.1 1.5 0.5 1.1 1.5 0.5 1.1 1.5 0.5 1.1 1.5\nBMS\n(Wq)ARE 0.43 1.05 2.63 0.73 3.25 3.14 0.47 1.67 1.35 0.43 2.58 9.65\nAAE 24.70 17.72 8.93 31.24 27.02 9.41 27.29 22.19 9.20 25.04 26.95 19.87\nBMS\n(It)ARE 0.59 2.27 9.38 0.73 0.86 1.02 0.72 1.73 7.52 0.73 0.79 2.33\nAAE 26.45 21.49 14.73 38.33 19.32 7.95 35.48 22.28 15.74 39.57 21.75 14.06\nCSARE 1.98 6.72 10.99 2.70 12.12 16.90 3.73 20.80 27.46 5.17 37.96 43.76\nAAE 74.96 47.98 15.89 102.05 75.83 23.80 140.65 118.29 38.70 194.32 198.40 59.96\nCMSARE 4.96 7.52 5.47 9.27 15.85 9.44 17.29 32.70 16.38 32.24 66.35 27.89\nAAE 187.52 53.81 8.17 350.08 99.82 13.58 651.63 185.54 22.88 1213.38 347.32 38.18\nTable 2: Results of Basic Meta-Sketch (T s)\nn 5K 10K 20K 40K\nB 9KB 11KB 13KB 15KB\nAMS\n(Wq)ARE 3.05 2.83 4.06 5.20\nAAE 21.42 26.11 35.00 43.81\nCF 90\n(Wq)ARE 3.58 14.53 141.70 1127.11\nAAE 21.13 59.18 381.63 2217.28\nCF 70\n(Wq)ARE 7.95 29.02 139.87 541.37\nAAE 29.02 76.58 295.63 970.94\nCF 40\n(Wq)ARE 91.16 138.64 244.24 407.83\nAAE 174.86 252.22 421.85 693.47\nLCMS\n(Wq)ARE 20.52 48.69 111.85 266.50\nAAE 37.80 81.93 194.15 451.28\nLCS\n(Wq)ARE 25.53 40.84 67.21 104.54\nAAE 44.53 78.17 122.57 180.56\nAMS\n(It)ARE 0.87 0.89 1.38 2.29\nAAE 3.77 4.46 5.13 6.55\nCF 90\n(It)ARE 0.85 2.74 4.20 16.71\nAAE 1.32 3.01 7.71 31.20\nCF 70\n(It)ARE 1.51 3.10 8.95 46.79\nAAE 2.57 5.51 16.83 82.84\nCF 40\n(It)ARE 12.62 33.50 103.76 155.61\nAAE 24.16 60.79 175.14 279.72\nLCMS\n(It)ARE 8.34 17.09 35.22 77.79\nAAE 13.72 28.39 59.10 129.86\nLCS\n(It)ARE 5.20 7.80 11.33 17.12\nAAE 8.78 13.10 18.97 28.38\nTable 3: Results of Advanced Meta-Sketch\n40%, 70%, and 90%, respectively. It shows that the advanced\nMS achieves a better performance than LSs and CFs. Also,\nAAE/ARE of advanced MS increases more moderately w.r.t.\nthe number of distinct items n, compared to its competitors.\nNext, we compare the performance of the advanced MS\nand the LS under dynamic streaming scenarios, as shown in\nFigure 4. We select a set of Tr(n=5K,B=9KB, Wq), and\ngradually shufﬂe the correspondence between items and fre-\nquencies. It shows that the AAE of advanced MS only slightly\nﬂuctuates between 21:28 and21:68 . In contrast, AAEs of LC-\nS/LCMS starts above 37, and increase signiﬁcantly w.r.t. the\nincrease of the shufﬂe ratio. Actually, the classiﬁer of LS\ntends to incur more errors due to the gradual shift of high-\n/low-frequency items, resulting in an increased number of\nhash collisions, thus deteriorating the estimation accuracy.\nAnalysis\nThe meta-sketch is trained based on meta-tasks, consisting of\nvarious stream distributions. We expected that meta-sketch\ncan learn the ability to sketch item frequencies. Somehow, it\nis unavoidable that meta-sketch’s ability is limited by patterns\n1000 3000 5000\nItem_size5152535455565AAE\nAdvanced MS\nK-means MS\nRandom MS(a) AAE\nAdvanced K-means Random\nAddressing_matrices020406080100S.D. (b) Standard Deviations\nFigure 6: Three Addressing Matrices\n0.00.51.01.52.02.5\nTrain_step1e6812162024Sparsity\nlevel1\nlevel2\nlevel3\nlevel4\n(a) Zipf\n0.00.51.01.52.02.5\nTrain_step1e6812162024Sparsity\nlevel1\nlevel2\nlevel3\nlevel4 (b) Triangular\n0.00.51.01.52.02.5\nTrain_step1e6812162024Sparsity\nlevel1\nlevel2\nlevel3\nlevel4 (c) Uniform\nFigure 7: The Sparsity of Embedding Vectors\nof given meta-tasks. Thus, the two training phases beneﬁt\nthe balance of the trade-offs. In pre-training, we select rep-\nresentative Zipf distributions for basic meta-tasks, making\nthe meta-sketch adaptable to a wide range of data streams.\nIn adaptation, we sample meta-tasks from raw data streams\nto make the meta-sketch more specialized. Next, we analyze\nthe working mechanism of the modules of the meta-sketch\nas well as their roles in acquiring the two abilities.\nSparse Addressing. We take a 2D slice A\u0003(size islr\u0002d2)\nof theAto analyze the process of a reﬁned vector rgetting\naddressinga. SinceA\u0003is formed by stacking unit vectors\nbi, we haveSparseMax(rTA\u0003)=SparseMax(jr jc). Here,\nc=(cos\u0012 1;:::;cos\u0012d2)and\u0012iis the angle between randbi.\nWe then continue to transform the form to obtain address-\ninga Sparsegen(c; u;jrj\u00001\njrj)as described in (Laha et al.\n2018), where uis a component-wise transformation function\napplied onc, and we set u(c)=c.\nBased on the principle of Sparsegen (Laha et al. 2018),\njrjmainly affects the sparsity (i.e., the proportion of non-\nzero bits in the vector) of a, whilecdetermines the positions\nand values of non-sparse bits. The Figure 5 shows a strong\ncorrelation between the average jrjand the sparsity of a\nduring training from scratch ( n=5K,B=9KB, Wq, BMS).\nSince the embedding vector zdoes not directly participate\nin the addressing process, the average jzjremains stable.\nFurther, we observe that the sparsity of awill eventually\nconverge to around 1, which means that each item is generally\nstored in a slot corresponding to the reﬁned vector rand the\nunit vector in A\u0003with the maximum cosine similarity.\n6921\n\n1000 3000 5000\nItem_size816243240AAE\nAdvanced MS\nNo frozen g dec\nFrozen g dec(a) Item Size\n5e+6 5e+4 5e+2\nTrue_mean123456ARE\nAdvanced MS\nNo frozen g dec\nFrozen g dec (b) Frequency Mean\nFigure 8: Generality w.r.t. Decoding\nThus, the role of A\u0003is to map reﬁned vectors to the ad-\ndressing vectors. The d2unit vectors in A\u0003are the reference\nstandard for mapping, which is equivalent to the mutually ex-\nclusived2-divisions of the reﬁned vector space. Follow this\npoint, we build two matrices K\u0003andR\u0003of the same size as\nA\u0003. Thed2unit vectors in K\u0003come from the cluster centers\nof the sampled reﬁned vectors. To achieve mutually exclusive\ndivision, we perform K-means clustering with K=d2and\nCosine similarity criterion. Then, we normalize the resulting\nd2cluster centers and stack them as K\u0003. In contrast, the unit\nvectors inR\u0003are entirely randomly generated.\nFigure 6 (a) shows the results of replacing A\u0003on the\ntrained meta-sketch with K\u0003andR\u0003. The meta-sketch with\nR\u0003shows the worst performance, but the performance of\nmeta-sketch with K\u0003is close to original A\u0003. Furthermore,\nwe count the number of items mapped in every slot of A\u0003,\nK\u0003,R\u0003and show their standard deviation in Figure 6 (b).\nThe standard deviation of R\u0003is much higher than A\u0003andK\u0003,\nand a better meta-sketch tends to store items more evenly in\neach slot. Thus, the addressing module simulates the tradi-\ntional sketch mechanism. Its principal function is to store the\nembedding vectors of items as evenly as possible in multiple\nmemory slots, and an item is written to only one slot.\nEmbedding. The major source of conﬂicts in the meta-\nsketch is the stacking of different embedding vectors in a sin-\ngle slot. Thus, the sparsity of the embedding vector becomes\nan important indicator to determine the degree of conﬂicts.\nFigure 7 shows the relation between the sparsity of embed-\nding vectors and the stream distributions ( n=5K,B=9KB,\nWq, AMS). We select the meta-tasks under Zipf, Triangu-\nlar, and Uniform distributions with different skewness levels\n(see supplement materials3for detailed setup). The results\nshow that the sparsity of the embedding vector is positively\nproportional to the skewness of a distribution. Therefore, we\nspeculate that the meta-sketch memorizes the pattern infor-\nmation of the distribution being adapted by self-tuning the\nsparsity of embedding vectors.\nDecoding. The decoding module, as the deepest NNs in\nthe meta-sketch, integrates various information to predict the\nitem frequency and achieves generalization ability. To verify\nthis, we adapt the advanced MS ( n=5K,B=9KB, Wq) to a\nspecial adaptive meta-task. The meta-task was sampled from\nthe real data stream but with a ﬁxed item size ( 5000 ) and\nfrequency mean ( 250). Meanwhile, we do not change the\ncorrespondence between items and frequencies. Such meta-\ntask forces the meta-sketch to pay more attention to the ﬁxed\npatterns and thus limit its generalization.\nThus, we train the advanced MS with (or without) freez-\ning the decoding module parameters based on the above\n010203040\nUnstable_memory_slot\n0.00.51.0\n010203040\nStable_memory_slot\n0.00.51.0(a) Multiple Slots\n020406080100\nSort_of_frequency891011SparsityUnstable\nStable (b) A Single Slot\nFigure 9: Unstable Case vs. Stable Case\nmeta-task. Figure 8 (a) shows the performance changes of\nthe three models (advanced MS as baseline) on the evalu-\nation tasks ( Tr) of different item sizes. Without the frozen\ndecoding module, the meta-sketch loses generalization abil-\nity at extended item sizes other than 5000 . On the contrary,\nthe meta-sketch with the frozen decoding module still re-\ntains the generalization ability and further utilizes the data\nstream pattern compared to the advanced MS, achieving the\nbest performance. Similarly, as shown in Figure 8 (b), the\nmeta-sketch without the frozen decoding module also loses a\ncertain generalization ability in terms of frequency mean.\nActually, the above meta-task (termed as stable case) can\nbe viewed as a special case of an ordinary adaptive meta-task\n(termed as unstable case), and augmented sketches utilize\nfrequency patterns similar to the stable case. For example,\nthe learned augmented sketch memorizes (relatively) stable\ncorrespondence between items and frequencies, for ﬁltering\nhigh-frequency items. To understand the meta-sketch’s self-\noptimizing mechanism from the unstable case to the stable\ncase, we analyze the storage of high/low-frequency items\nbetween multiple slots and a single slot in the memory. In\nFigure 9 (a), we show density heat-maps of low-frequency\n(below the top 20% high frequencies) items, stored by meta-\nsketches of stable and unstable cases on a 2D slice of the\nM, where the x-axis is the index of slots. The two heat-maps\nshow that the meta-sketch under the stable case can store the\nlow-frequency items concentratedly in some slots to avoid\nconﬂicts with high-frequency items. Interestingly, the meta-\nsketch does not intentionally do this like augmented sketches.\nInstead, it is achieved by self-optimization during the train-\ning. Furthermore, Figure 9 (b) shows the relation between the\nsparsity of the embedding vector of items stored in a single\nslot and the frequency order, where the x-axis represents the\nfrequencies in the ascending order. We speculate that the\nmeta-sketch autonomously adjusts the sparsity of the embed-\nding vector within a single slot in the stable case, so that the\nhigh/low-frequency items are automatically separated.\nConclusion\nIn this paper, we propose a neural data structure: meta-sketch,\nfor estimating item frequencies in data streams. Unlike tra-\nditional sketches, the meta-sketch utilizes meta-learning and\nmemory-augmented neural networks. The meta-sketch is pre-\ntrained with Zipf distributions and can be fast adapted to\nspeciﬁc runtime streams. We study a series of techniques for\nconstructing the meta-sketch. Extensive empirical studies on\nreal datasets are done to evaluate our proposals. In the future,\nit is interesting to extend our proposal to other sketching tasks\nthat are supported by traditional sketches.\n6922\n\nAcknowledgements\nThis work is supported by NSFC (No.61772492, 62072428),\nthe CAS Pioneer Hundred Talents Program.\nReferences\nAdamic, L. A. 2000. Zipf, power-laws, and pareto-a ranking\ntutorial. Xerox Palo Alto Research Center, Palo Alto, CA,\nhttp://ginger. hpl. hp. com/shl/papers/ranking/ranking. html.\nBabcock, B.; Babu, S.; Datar, M.; Motwani, R.; and Widom,\nJ. 2002. Models and Issues in Data Stream Systems. In\nPODS, 1–16.\nCharikar, M.; Chen, K. C.; and Farach-Colton, M. 2002. Find-\ning Frequent Items in Data Streams. In ICALP, 693–703.\nCormode, G.; Garofalakis, M. N.; Haas, P. J.; and Jermaine,\nC. 2012. Synopses for Massive Data: Samples, Histograms,\nWavelets, Sketches. Found. Trends Databases, 4(1-3): 1–294.\nCormode, G.; and Muthukrishnan, S. 2005. An improved data\nstream summary: the count-min sketch and its applications.\nJ. Algorithms, 55(1): 58–75.\nDzogang, F.; Lansdall-Welfare, T.; Sudhahar, S.; and Cris-\ntianini, N. 2015. Scalable Preference Learning from Data\nStreams. In WWW 2015, Florence, Italy, May 18-22, 2015 -\nCompanion Volume, 885–890. ACM.\nEstan, C.; and Varghese, G. 2002. New directions in trafﬁc\nmeasurement and accounting. In SIGCOMM, 323–336.\nGao, R.; Xie, X.; Zou, K.; and Pedersen, T. B. 2022. Multi-\ndimensional Probabilistic Regression over Imprecise Data\nStreams. In WWW ’22: The ACM Web Conference 2022,\nVirtual Event, Lyon, France, April 25 - 29, 2022, 3317–3326.\nACM.\nGoyal, A.; III, H. D.; and Cormode, G. 2012. Sketch Al-\ngorithms for Estimating Point Queries in NLP. In EMNLP-\nCoNLL 2012, July 12-14, 2012, Jeju Island, Korea, 1093–\n1103. ACL.\nGraves, A.; Wayne, G.; and Danihelka, I. 2014. Neural turing\nmachines. arXiv preprint arXiv:1410.5401.\nGraves, A.; Wayne, G.; Reynolds, M.; Harley, T.; Danihelka,\nI.; Grabska-Barwi ´nska, A.; Colmenarejo, S. G.; Grefenstette,\nE.; Ramalho, T.; Agapiou, J.; et al. 2016. Hybrid comput-\ning using a neural network with dynamic external memory.\nNature, 538(7626): 471–476.\nHospedales, T. M.; Antoniou, A.; Micaelli, P.; and Storkey,\nA. J. 2020. Meta-Learning in Neural Networks: A Survey.\nCoRR, abs/2004.05439.\nHsu, C.-Y .; Indyk, P.; Katabi, D.; and Vakilian, A. 2019.\nLearning-Based Frequency Estimation Algorithms. In ICLR.\nHuang, Q.; Lee, P. P. C.; and Bao, Y . 2018. Sketchlearn:\nrelieving user burdens in approximate measurement with\nautomated statistical inference. In SIGCOMM, 576–590.\nIrfan, M. T.; and Gordon, T. 2019. The Power of Context in\nNetworks: Ideal Point Models with Social Interactions. In\nIJCAI, 6176–6180.\nKendall, A.; Gal, Y .; and Cipolla, R. 2018. Multi-task learn-\ning using uncertainty to weigh losses for scene geometry and\nsemantics. In CVPR, 7482–7491.Kolajo, T.; Daramola, O. J.; and Adebiyi, A. A. 2019. Big\ndata stream analysis: a systematic literature review. J. Big\nData, 6: 47.\nKraska, T.; Beutel, A.; Chi, E. H.; Dean, J.; and Polyzotis, N.\n2018. The case for learned index structures. In SIGMOD,\n489–504.\nLaha, A.; Chemmengath, S. A.; Agrawal, P.; Khapra, M.;\nSankaranarayanan, K.; and Ramaswamy, H. G. 2018. On\ncontrollable sparse alternatives to softmax. NIPS, 31.\nLi, J.; Li, Z.; Xu, Y .; Jiang, S.; Yang, T.; Cui, B.; Dai, Y .; and\nZhang, G. 2020. Wavingsketch: An unbiased and generic\nsketch for ﬁnding top-k items in data streams. In Proceed-\nings of the 26th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, 1574–1584.\nLiu, Y .; and Xie, X. 2021. XY-Sketch: on Sketching Data\nStreams at Web Scale. In WWW ’21: The Web Conference\n2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021,\n1169–1180. ACM / IW3C2.\nMadden, S.; and Franklin, M. J. 2002. Fjording the Stream:\nAn Architecture for Queries Over Streaming Sensor Data. In\nICDE, 555–566.\nMartins, A.; and Astudillo, R. 2016. From softmax to sparse-\nmax: A sparse model of attention and multi-label classiﬁca-\ntion. In ICML, 1614–1623. PMLR.\nMitzenmacher, M. 2018. A model for learned bloom ﬁlters\nand related structures. arXiv preprint arXiv:1802.00884.\nPhridviRaja, M. S. B.; and GuruRao, C. V . 2016. Data mining\n: past present and future - a typical survey on data streams.\nCoRR, abs/1605.01429.\nRae, J.; Bartunov, S.; and Lillicrap, T. 2019. Meta-learning\nneural bloom ﬁlters. In ICML, 5271–5280. PMLR.\nRoy, P.; Khan, A.; and Alonso, G. 2016. Augmented Sketch:\nFaster and More Accurate Stream Processing. In SIGMOD,\n1449–1463.\nSantoro, A.; Bartunov, S.; Botvinick, M.; Wierstra, D.; and\nLillicrap, T. 2016. Meta-learning with memory-augmented\nneural networks. In ICML, 1842–1850. PMLR.\nTang, L.; Huang, Q.; and Lee, P. P. C. 2019. MV-Sketch: A\nFast and Compact Invertible Sketch for Heavy Flow Detec-\ntion in Network Data Streams. In INFOCOM, 2026–2034.\nIEEE.\nTinati, R.; Wang, X.; Brown, I. C.; Tiropanis, T.; and Hall, W.\n2015. A Streaming Real-Time Web Observatory Architecture\nfor Monitoring the Health of Social Machines. In WWW,\n1149–1154.\nVinyals, O.; Blundell, C.; Lillicrap, T.; Kavukcuoglu, K.;\nand Wierstra, D. 2016. Matching Networks for One Shot\nLearning. In Lee, D. D.; Sugiyama, M.; von Luxburg, U.;\nGuyon, I.; and Garnett, R., eds., NIPS, 3630–3638.\nWang, L.; Luo, G.; Yi, K.; and Cormode, G. 2013. Quantiles\nover data streams: an experimental study. In SIGMOD.\nWeller, T. 2018. Compromised Account Detection Based on\nClickstream Data. In WWW, 819–823.\nZeng, X.-Q.; and Li, G.-Z. 2014. Incremental partial least\nsquares analysis of big streaming data. Pattern Recognition,\n47(11): 3726–3735.\n6923\n\nZhong, Z.; Yan, S.; Li, Z.; Tan, D.; Yang, T.; and Cui, B. 2021.\nBurstSketch: Finding bursts in data streams. In Proceedings\nof the 2021 International Conference on Management of\nData, 2375–2383.\nZhou, Y .; Yang, T.; Jiang, J.; Cui, B.; Yu, M.; Li, X.; and\nUhlig, S. 2018. Cold Filter: A Meta-Framework for Faster\nand More Accurate Stream Processing. In SIGMOD, 741–\n756.\nZhu, Y .; and Shasha, D. E. 2002. StatStream: Statistical\nMonitoring of Thousands of Data Streams in Real Time. In\nVLDB, 358–369.\n6924",
  "textLength": 45742
}