{
  "paperId": "9920d14a762231122035fbefab3bfd5983a5298a",
  "title": "Optimizing Context-Enhanced Relational Joins",
  "pdfPath": "9920d14a762231122035fbefab3bfd5983a5298a.pdf",
  "text": "Optimizing Context-Enhanced Relational Joins\nViktor Sanca\nEPFL\nviktor.sanca@epfl.chManos Chatzakis†\nEPFL\nemmanouil.chatzakis@epfl.chAnastasia Ailamaki\nEPFL\nanastasia.ailamaki@epfl.ch\nAbstract —Collecting data, extracting value, and combining\ninsights from relational and context-rich sources of many\nmodalities in data processing pipelines presents a challenge for\ntraditional relational DBMS. While relational operators allow\ndeclarative and optimizable query specification, they are limited\nto unsuitable data transformations for capturing or analyzing\ncontext. On the other hand, representation learning models\ncan map context-rich data into embeddings, allowing machine-\nautomated context processing but requiring imperative data\ntransformation integration with the analytical query.\nTo bridge this dichotomy, we present a context-enhanced rela-\ntional join and introduce an embedding operator composable\nwith relational operators. This enables hybrid relational and\ncontext-rich vector data processing, with algebraic equivalences\ncompatible with relational algebra and corresponding logical and\nphysical optimizations. We investigate model-operator interaction\nwith vector data processing and study the characteristics of the\nE-join operator. We demonstrate the hybrid context-enhanced\nrelational join operators with vector embeddings and evaluate\nit against a vector database approach. We show step-by-step\nthe impact of logical and physical optimizations, which result\nin orders of magnitude execution time improvement resulting in\ntensor join formulation and outline the performance tradeoffs\nand case of using scan-based processing against vector indexes.\nIndex Terms —analytics, vector embeddings, AI for database\nsystems, query optimization, hardware-conscious processing\nI. I NTRODUCTION\nRelational databases allow declarative query specification\nand abstractions for logical and physical query plan opti-\nmizations. These optimizations include operator reordering via\nalgebraic equivalences and heuristics and instantiating opera-\ntors for resource-efficient and hardware-conscious execution\non modern hardware. This allows ad-hoc query specification\nby abstracting out significant implementation details from\nthe user and end-to-end optimization. As the main goal of\nrelational analytical databases is to provide abstractions to\nlarge-scale processing and extraction of value from the data of\ninterest, relational databases are designed for data types where\na procedural way to process the data is possible to precisely\nspecify, such as aggregating numerical values or processing\nstrings with a well-specified pattern.\nStill, many data sources are unsuitable for processing in a\nrelational database and are typically only stored serialized in\nbinary formats. These include documents, text, images, and\nother data sources of increasing value, driven by the advent\nof the Internet and mobile devices and services such as social\n†Author contributed during an internship at DIAS lab, EPFL.\nUser\nIntermediate\nData/Results\nRDBMSŜ\nřĭྤ\nModel\nData\nExtractionData\nRefinementFig. 1: Problem: Model-RDBMS data analysis requires user\nexpertise, imperative tasks, and data movement specification.\nmedia. Such data has a lot of human-understandable contexts:\nthe contents and number of objects in an image, the semantics\nof a string despite alternative spellings, typos, or tenses, all of\nwhich make this task impractical if not impossible to specify\nin traditional relational data analytics.\nOn the other hand, advancements in artificial intelligence\nand machine learning have allowed increasingly complex\nmachine reasoning and performance in analyzing context-\nrich data such as images or text. Models such as BERT [1]\nand GPT [2] allow natural language processing, ResNet [3]\nobject localization and detection, often available as Foundation\nModels [4] that are trained on web-scale data and further\ncustomizable and re-trainable for the particular task. To use\nthose models, often based on Transformer architecture [5],\nanalysts would instantiate the particular model, input data, and\ncollect the output, using frameworks such as Tensorflow [6] or\nPytorch [7], often in an isolated, task-specific setting. With the\nproliferation of embedding-based analytics, vector databases\nhave recently gained traction, offering vector processing based\non index structures [8] with limited integration with traditional\nrelational analytics or available operations over data.\nWhile machine learning models can transform context-rich,\nmulti-modal data into embeddings, coordinating the models\nand data processing pipelines is manual and imperative. Sup-\npose an analyst wanted to analyze and extract insights from\nan RDBMS and use some data as input to the models as in\nFigure 1, which may be again input to an analytical query\nand models in a more complex analytical processing pipeline.\nCombining the data from social media feeds with user reviews,\ntransactions, and analytics in an online retailer case results in\ncomplex, hybrid data processing pipelines. The data of interest\nand value is both relational and vector-based, not fitting fully\nto cases outlined in TPC-H, TPC-DS, or SSB [9] benchmarks,\nor vector search-aimed ANN-Benchmarks [10].arXiv:2312.01476v2  [cs.DB]  13 Feb 2025\n\n0.135 0.654 ڮ0.345 0.848\n0.548 0.870 ڮ0.984 0.156\n0.498 0.148 ڮ0.165 0.958\nڭ\n0.318 0.844 ڮ0.283 0.418\n0.046 0.651 ڮ0.162 0.658\n0.156 0.598 ڮ0.968 0.411\nContext-Rich Data\nHuman Reasoning Embedding\nDomain MappingVector Feature Representation\nMachine ReasoningFig. 2: Enabler: models embed context-rich data into common\ntensor representation, enabling automated processing.\nWith two independent components, RDBMS with relational\ndata and Model with vector data, the user must be an expert\nto fine-tune the queries, potentially perform data integration,\ncorrectly deploy and scale the queries to the hardware, orches-\ntrate data movement, and specify the correct operator orders\nto prevent negatively impacting the performance. To keep the\nprinciple of data independence [11], hiding the implemen-\ntation details from the user, we propose building on top of\ndecades of research and engineering in query optimization and\nexecution engines allow hiding this complexity and is the key\nmotivation to extend and make relational algebra the basis of\nemerging methods in multi-modal and context-rich processing.\nTightly integrated, expressive, and optimizable, hybrid vector-\nrelational data management is part of our broader effort for\nthe next generation of context-rich analytical engines [12],\nenabled by embedding models that transform the context-rich\ndata into tensors as a common intermediate data representation\n(Figure 2). A separation of concerns is established: the model\nhandles modality, data context, and semantics; the analytical\nengine optimizes and processes context-free data and tensors\nvia defined operators.\nAs the data of interest can be a mix of relational and vector\ndata, this results in operators having different physical and\nlogical properties, which, naively built on top of relational\noperators, yield in suboptimal performance. In this work, we\ninvestigate the case of context-enhanced join operator, which\ntakes place over vector embeddings and relational data, and:\n•Motivate and propose the capabilities of a context-\nenhanced join operator in Section II, and introduce and\nformalize the relational operator extension in Section III,\n•Analyze the suitability of traditional join operator for the\ntask of vector data processing, and propose a cost model,\nlogical optimizations, and an alternative efficient tensor\nformulation for parallel execution of a join operator for\nprocessing neural embeddings in Section IV,\n•Evaluate the physical and hardware optimizations we pro-\npose in Section V, and benchmark the operator implemen-\ntation and characteristics in Section VI, showing the over\norders of magnitude impact on the execution time and the\nimportance of both logical and physical optimizations of\nvector-based join operations, and comparing the tradeoffs\nfor using a state-of-the-art index in a vector database.\n...Vector Data Contextual Dataʍ ɸʅ\nRelational DataڇEnriched Context Awareness\nModel-Relational Hybrid Analytics...Relational DataFig. 3: Context-enhanced, model-relational analytics.\nII. M OTIVATION AND PRELIMINARIES\nThere have been significant efforts to enable machine-\nautomated understanding of context-rich data. We first define\nkey concepts, starting from context-rich data , which contains\ninformation and metadata, often human-understandable but\nnot directly machine-understandable, such as word synonyms\nor understanding that two images are identical. The key\nidea behind neural embeddings is that the machine learning\nmodel (µ) learns how to transform the input data domain into\nhigh-dimensional vector space (Figure 2), where relationships\nbetween the data can be expressed using linear algebra ex-\npressions over vectors. Embedding models ( Eµ) take a human-\nunderstandable context-rich domain and map it into a machine-\noperable high dimensional vector space representation ( ten-\nsors), where embeddings are tensor representations of model\ninputs.\nEmbedding models enable data analysis over many data\nmodalities [1], [3], [13], [14], which was previously done\nmanually, and was difficult or impossible for applications such\nas e-commerce [15], search [16], Retrieval-Augmented Gen-\neration (RAG) [17], and recommender systems [18]. Vector\nindexes [8] with vector databases [19] provide a framework\nfor search and retrieval, with inherently similarity-based oper-\nations over vector embeddings. This contrasts with traditional\nrelational analytics, where expressions have exact semantics.\nUnifying both and investigating the tradeoffs in the hybrid\nspace in between is the topic of our study.\nA. Extended Functionality: Joins Over Contextual Data\nModel-driven embeddings transform data previously opaque\nto the relational data management system into context-free\nvectors (Figure 3). Separating concerns between the model-\ndriven context and uniform vector data representation enables\ndefining expressions over vectors and tensors , maintaining the\nrelational model principle of data independence [11]. Our ap-\nproach outlines a way to join context-rich data such as strings,\ndocuments, or images by defining a corresponding model and\nvector expression as first-class operations alongside relational\noperators, enabling novel logical and physical optimizations.\nSimilarity joins are an important class of operations in data\ncleaning and integration designed to tolerate the errors and in-\nconsistencies in the data and extend the traditional exact joins.\nThey have a long tradition and many real-world applications,\nsuch as entity resolution, duplicate detection, spell-checking,\n2\n\nand clustering [20]–[23], finding place in production platforms\nsuch as Microsoft SQL Server Integration Services [24], Infor-\nmatica [25], Knime [26], and Talend [27]. The proliferation of\nvector embeddings for contextual data further extends the need\nfor an efficient similarity join, expanding the aforementioned\nuse cases, where we next discuss some novel applications.\n1) Semantic-Based Similarity Operations\nInstead of having a human-in-the-loop or an expert sys-\ntem that performs dictionary-based or hard-coded rule-based\nsimilarity operations [20], [22], models automate similarity\noperations over many data types [1], [3], [13], including\nmulti-modal data [14]. A common tensor representation poses\nsimilarity joins simply as similarity expressions, such as cosine\ndistance, between the vector-embedded data. The models fine-\ntune the functionality and context. After embedding the data\nand providing operators and expressions over tensors such as\ncosine distance, model-independent operations can be com-\nbined with the rest of the relational query plan.\n2) Online Data Cleaning and Integration\nStrings or other context-rich data can be dirty or have rich\nsemantics. [20], [28] If we consider words or sentences, they\nmay have misspellings, alternative spellings, synonyms, or\ndifferent tenses that all have the same meaning. Specifying\nall the rules to unify context is error-prone and difficult.\nMeanwhile, embeddings can encompass such similarity using\nrepresentation learning. Therefore, such operators can process\nsuch data on the fly without prior cleaning and only the data\nof interest, relying on embeddings and specified similarity\nthresholds for data integration and potentially performing post-\nverification steps.\n3) Multi-Modal Data Processing\nThe data context is opaque to the execution engine, while\nthe model selection and parameters give context and transform\nthe data. By processing context-free tensors, relational engines\nprovide a common optimization framework for multi-modal\ndata ingested and transformed initially by models. A multi-\nmodal similarity join is useful in near-duplicate detection\nof unlabeled entries against another database, such as in\nmisinformation detection [29] or document tagging [30].\nMore generally, as a search query takes a single query as an\ninput, batching many search queries would be equivalent to\na join operation for better use of the available parallelism [31].\nB. Integrating Vector Embeddings With Relational Operators\nData management systems support and simplify data pro-\ncessing with research and systems contributions and features\nsuch as transactions and concurrency control [32], [33], auto-\ntuning [34], hardware-conscious implementations [35]–[39],\ncorresponding data structures [40], and query optimization\nwith declarative interfaces to abstract out the system com-\nplexity from the end-user. Instead of manual intermediate\norchestration and system integration to combine and analyze\nmulti-modal and context-rich data involving different systems,\ndata sources, and efficient operator reimplementation, we\ninvestigate how to extend traditional relational joins to support\nUser\nContext-Awareness \nvia Embeddings\nRDBMSŜ\nřĭ\u0003ྤ\nModel\nLogical + Physical \nOptimizationsExtended Relational \nOperators + AlgebraFig. 4: Goal: Hybrid vector-relational operations are declara-\ntive transformation primitives amenable to query optimization.\nmodel-driven context with minimal system intrusions and\nbuild on top of existing judiciously modified abstractions. In\nparticular, this means that the vectors are simply another data\ntype over which expressions and operations can be defined.\nThis makes index structures designed to store, maintain, and\nperform similarity search over tensors [8] compatible as phys-\nical access method options. Similarly, recent work has formu-\nlated traditional relational processing over tensors [41], where\ntensor processing platforms are used as analytical RDBMS to\nbenefit from existing implementation while transforming the\nrelational data and operations into tensor representation.\nWhile there has been prior work to integrate model infer-\nence and learning with analytical engines [42], [43], our goal\nis complementary as we focus on how we can extend the rela-\ntional model functionality with contextual data, as illustrated in\nFigure 4. Similarly, we expose co-optimization opportunities at\nlogical, physical, and implementation levels and fine-grained\nsystem interactions [12]. As vector databases are typically\nbased on index structures [19], in this work, we formulate\nour operator for both index-probes and scans, considering the\nrelational attribute-driven selectivity of analytical queries [31].\nC. Holistic Optimization\nSuppose the data of interest are strings and dates stored\nin an RDBMS. One can consider other context-rich formats\nstored as binary objects with other relational data. To allow\nsemantic similarity operations, such as matching strings that\nare synonyms, have misspellings, or different tenses, word\nembedding models transform strings into vectors, which are\nthen comparable using cosine distance. While RDBMS could\nexecute regex-like string expressions, mapping strings to em-\nbeddings allows capturing broader classes of similarity within\na model. Note that the model can be trained and adapted for\ndifferent datasets to adjust the notion of similarity, which the\nanalyst selects.\nWe are interested in joining two tables over strings, where\na condition over dates exists, making the queries selective\non both tables. In a declarative setting, query specification\nrequires embedding model information and the join condition\nexpression, and the selectivity information from the relational\ncolumn needs to propagate before the embeddings. Otherwise,\nthe whole interaction may result in the user eagerly mate-\n3\n\nrializing all the data as in Figure 1, performing expensive\nembedding, and only then filtering.\nPhysical optimization must address this interaction and\naccount for the tensor data format and the expressions suitable\nfor comparing high-dimensional data. For example, while\nan equi-join over tensors could be implemented as a hash-\njoin, more practical embedding comparisons, such as cosine\ndistance, require algorithms such as nested-loop join for pair-\nwise comparisons and consider the join, operation, and model\ndata structure access patterns in the algorithm and cost model.\nFor example, if the queries are selective and a vector index\nexists, should we select it or use an exhaustive scan for the\noperation [31].\nFinally, from a hardware-conscious perspective, using\nmany-dimensional vectors with relational operators designed\nand optimized for single-dimensional numerical data and ju-\ndicious use of caches and memory hierarchy demands novel\ntradeoffs. A 100-dimensional tensor embedding will change\nthe caching and execution patterns of traditional algorithms,\nand model embedding can incur computational or data access\ncosts at a critical path of execution. Designing hardware-\nconscious algorithms represents a direction driven by novel\nmodel-database interactions. We demonstrate join operator op-\ntimizations (Figure 4), step-by-step, from logical and physical\noptimization that enable efficient execution.\nTakeaway Neural embedding models transform the context-\nrich, multi-modal data into a common (per-model) tensor\nrepresentation space. From the perspective of declarative rela-\ntional processing, models provide separation between data se-\nmantics and context-less tensors. Relational operators perform\noperations such as cosine distance or vector transformations\nover tensors, amenable to query optimization. We propose\nnovel optimizations and analyze their performance, aware of\nthe new design space.\nIII. C ONTEXT -ENHANCED RELATIONAL JOINOPERATOR\nIn this section, we start with formalizing the proposal of a\nrelational operator extension to declaratively process context-\nrich data stored along traditional relational data, such as strings\nand text, that may be stored along with numerical or date\nattributes. We call this hybrid model-relational processing .\nThis enhancement stems from the fact that the contextual data\nmay need to be transformed and processed differently. How-\never, compatibility with relational algebra and optimizations\nfor processing purely relational data is required. Instead of\nusing separate systems and manually orchestrating the data\nmovement for processing using external programs or opaque\nUDFs, we propose a set of operations needed to express a\njoin based on embedding the original data that is amenable to\ntraditional query optimization.\nA. Neural Embeddings\nNeural embeddings and representation learning are rich\nand active research fields in machine learning. Images can\nbe embedded with models such as ResNet [3], audio withPANNss [13], and text with Bert [1], word2Vec [44], Fast-\nText [45], [46]. Foundation Models [4] offer an increasingly\nflexible way to specialize large models to a particular use case.\nIt is important to mention those models can be tuned, as they\nlearn representations from the training dataset through transfer\nlearning [47] (e.g., starting from one of the foundation models)\nor re-training. In this work, we focus on and experiment\nwith string embedding models. However, as embeddings are\ngenerally high-dimensional vectors, once in the embedding\ndomain, the processing of this data is model- and input-\ndata-type-agnostic, and the same principles and optimizations\nhold. Processing embedded data allows automating semantic\nsimilarity using cosine similarity (or another distance) between\nthe vectors. Using vectors necessitates interaction with linear\nalgebra; therefore, the equations below outline definitions\nof cosine similarity over vectors and matrices. We will use\nthem heavily in logical (Section IV) and physical (Section V)\noptimization phases.\ncos(θ) =A·B\n∥A∥∥B∥=nP\ni=1AiBi\ns\nnP\ni=1A2\nis\nnP\ni=1B2\ni\n(Cosine Similarity)\nB. Model-Operator Interaction\nIn our example, we focus on context awareness over strings\nso that common mistakes or semantically similar words are\nautomatically captured. Rather than imposing user to strictly\nspecify the rules for string similarity or clean the data ahead\nof time, we enable words such as ( barbecue ,barbecues ,\nbbq,barbicue ,grilling ) that have similar semantics,\nto automatically be used with relational operator predicates\nwithout prior user intervention. The user should only specify\nthe embedding model and a threshold distance parameter over\ncosine similarity calculation (Equation: Cosine Similarity).\nInstead of comparing two strings in their original domain, they\nare embedded. If the cosine similarity cos(θ)is larger than the\nspecified threshold, the two strings are similar and should be\nmatched. This avoids manual string processing and combining\ntechniques, such as Locality Sensitive Hashing, individually\nlimited to capturing only features such as misspellings.\nA context-aware operator is supplemented with an embed-\nding model ( µ). In this case, when an operator receives strings,\nit embeds them and then performs the requested processing\nin the vector domain. Models can be selected based on the\nanalyst’s needs, while often having desirable properties such as\nthe capability of training and adapting to the desired similarity\ncontext. This interaction opens up design and optimization\nchoices, such as how to mask or minimize the cost of embed-\nding/model and overlap it with operator execution. We capture\nthis interaction through relational algebra (Subsection III-C)\nand a cost model (Section IV) to allow holistic integration\nwith the remainder of the query plan.\n4\n\nC. Relational Operators and Algebra\nWe introduce the embedding operator ( E) using a model\n(µ), and relational algebra equivalences over selection ( σ) and\nθ-join ( ▷ ◁θ) operations, compatible with traditional relational\nalgebra definitions.\nSelection operation applies predicate θover input tuples and\nreturns only the tuples that satisfy the condition.\nσθ(R) ={t∈R, θ (t)} (Selection with predicate θ)\nTo change the domain of input data, we allow mapping\nthe input tuples (or a projection over the tuples for simple\nnotation) using a model ( µ) into vector space using embedding\n(E) operation.\nEµ(R) ={t∈R, t7→µ(t)}(Embedding with model µ)\nFor completeness and decoding of the embeddings and\nretrieving the context-rich data, an inverse operation E−1\nshould also be defined, which is the standard component of\nencoder-decoder architectures, and semantically correct only\nfor the same model µ. If the model does not have a decoder\nto recover the original data R, a lookup table mechanism can\nmaintain the object-embedding mapping via unique IDs.\nE−1\nµ(Eµ(R)) =R (Decoding with model µ)\nCombining embedding with selection allows the processing\nof tuples with a mixture of data formats. Some attributes\nmay have relational predicates, and some may require em-\nbedding and predicates using different metrics (such as cosine\ndistance). Predicate pushdown and operation reordering can\nhappen as soon as the attributes that predicates operate over\nare available.\nσE,µ,θ(R)⇔σθ(Eµ(R))⇔σθE(Eµ(σθR(R))))\n(E-Selection)\nJoin operation takes two relations and joins them over\nspecified attributes using specified predicate conditions ( θ-\njoin).\nR×S={(r, s), r∈R∧s∈S} (Cartesian Product)\nJoins are amenable to predicate pushdowns and reordering.\nR ▷ ◁ θS⇔σθ(R×S) (θ-Join Generalization)\nWe introduce embeddings to the generalized join definition\nand provide equivalences. Embeddings can be observed as a\nspecial projection operation that changes the domain.\nR ▷ ◁E,µ,θS⇔σE,µ,θ(R×S)\n⇔\nσθ(Eµ(R)× Eµ(S))⇔ E µ(R)▷ ◁θEµ(S)(E-θ-Join)\nTakeaway We formulate the context-enhanced operators by\nextending relational operators and algebra to allow declarative\nintegration of embedding models with relational engines and\noptimizers. A hybrid setting enables declarative and systematic\nlogical and physical optimizations, as depicted in the simple\nquery in Figure 5, providing semantic awareness using em-\nbeddings to separate concerns between models and engines.\nRʍʍ\nSɸ\nRɸڇɽ\nTaken\ndateImage \nBLOB\n01/02/23 [0101…]\n01/12/23 [0110…]05/12/23 [0111…]Taken>02/12/23Sim(ɸ(Image), ɸ(Example))>=0.9\nImage \n[0111…]Embed Image with model ʅ ɸʅ\nʅʅɸ(Image) \n[0.04 0.02 0.5 …]ɸ(Image) \n[0.01 0.03 0.02 …]\nOption 1: precomputed/cached \nvector embeddingsOption 2: online \nembeddingɸ-JoinFig. 5: Hybrid vector-relational query example, and the join\noperator which is the focus of the optimizations in this paper.\nIV. L OGICAL OPTIMIZATION\nStarting from the extended algebra and operators, we present\nthe logical optimization driven by model-operator interaction\nand tensors as a common intermediate data representation.\nIn contrast to traditional optimizations and relational operator\ncost models, the two factors are different. First, since models\nmay be on the critical path of execution, model embedding\ndata access or computation time must be considered in ad-\ndition to the relational operator’s data access and processing\ncost. Second, embeddings extend the notion of atomic data\ntypes prescribed by 1stnormal form [48], satisfying the\ncondition that an ”atomic data cannot be decomposed into\nsmaller pieces by the DBMS (excluding certain special func-\ntions)” [49]. Embeddings are not structured data but should\nbe observed and processed atomically by the DBMS.\nA. Cost Model\nWe outline the abstract cost model for the context-enhanced\nsimilarity selection and join operations. For joins, we investi-\ngate the first available strategy: nested-loop join (NLJ) [20].\nWe note that we focus on evaluating exact algorithms in\nour study. Since the distance we use is cosine-similar, hash-\nbased approaches would yield approximate solutions similar\nto locality-sensitive hashing. If we were to use equi-joins,\nit would be possible to use traditional hash-joins, but there\nwould be no benefit from using embeddings. Still, nested loop\njoins can be formulated with good cache-locality, an important\nperformance factor (Section VI) that does not incur random\naccess over high dimensional data as every vector needs to be\npair-wise compared using cosine distance.\nWe outline the abstract cost model for selection and join\nbelow, where RandSare relations, |R|is the cardinality of\nrelation R,Arepresents the data access cost, Mrepresents the\nmodel cost, and Cis the computation cost of the operation.\nAs the model, computation, and access cost vary relative to\nthe particular architecture and DBMS, the cost model should\nbe parametrized based on their mutually normalized relative\nperformance [31], [50].\nSelection is an operation where input data is scanned,\nembedded, and the condition is applied over every input tuple,\n5\n\nBatch/Matrix Operation, nݕݐ݅ݎ݈ܽ݅݉݅ݏ݁݊݅ݏ݋ܿ> ݈݀݋݄ݏ݁ݎ݄ݐ\n֞\ncosߠ=ܣȉܤ\nܣȉܤ>݈݀݋݄ݏ݁ݎ݄ݐVector embedding dimensionality, dBatch Size, \nm\ndm\nn0\n1\n201 234\n0\n1\n20 1 23 4\nA\nB\nm\nn0\n1\n20 1 23 4\n0,0 0,2 0,4\n1,2\n2,1 2,4>[ t h r e s h o l d ]ߠݏ݋ܿ\n{(0,0), (0,2), \n(0,4), (1,2),\n(2,1), (2,4)}\nBatch OffsetsResult Set1\n2\n1\n2Fig. 6: Matrix formulation of E-join allows scalable and cache-efficient execution over high-dimensional embeddings.\nwhere each tuple incurs access, model, and computation cost\n(when the tuples are only retrieved C= 0):\nCost (σE,µ,θ(R)) =|R| ·(A+M+C)(E-Selection Cost)\nBy naively extending the Nested-Loop Join (NLJ) operation,\nit would scan both relations and perform pair-wise condition\ncomparisons. In this implementation, model access would be\nperformed per-processed tuple without considering model-\nrelational interaction, which incurs quadratic model access\ncost. Considering that embedding models are computationally\nexpensive, the following cost model shows the suboptimality:\nCost (R ▷ ◁E,µ,θS) =|R|·|S|·(A+M+C)(E-NL Join Cost)\nInstead, by considering the characteristics of the nested-loop\njoin, we observe that tuple embedding needs to happen only\nonce per tuple from both relations. This can be performed as\na precursor to the join operation or as a lazy embedding and\ndata materialization strategy. By observing this behavior, the\njoin results in a linear model cost with prefetching:\nCost (R ▷ ◁E,µ,θS) =|R| · |S| ·(A+C) + (|R|+|S|)·M\n(E-NLJ Prefetch Optimization)\nThis optimization is significant, as the model cost can\nspan from random access to a lookup table (several times\nslower than sequential scan) to expensive computations over\ndeep neural networks (data transfer and computation). From\nanother perspective, if machine learning models are used as-\na-service and paid for per embedding [51], this cost model\nconversely results in monetary savings compared to the initial\nimplementation. Expressing embeddings as relational operator\nextensions allows logical optimization to occur in conjunc-\ntion with other operators in the hybrid relational-embedding\npipeline (selection pushdown, join reordering), such that the\ncardinality of the most costly part of the query plan will be\nreduced without explicit user intervention or knowledge about\nspecific interactions given a relational operator.\nB. Index Join Formulation\nCurrent vector databases rely on index structures [8], [52]\nthat accelerate vector search. They avoid exhaustive cross-\nproduct per-vector evaluation at the cost of approximate results\nand random access patterns. If an index exists on S, the index\nprobe cost is denoted as Iprobe, the join cost becomes:Scan Join Index Join\nAccuracy Exact Approximate\nFiltering Full Relational Vector Similarity & Pre-Filtering\nCost Compute & Scan Build & Compute & Probe\nFlexibility Any Expression Limited, Construction-Time Distance\nTABLE I: Index versus scan-based vector join operator.\nCost (R ▷ ◁E,µ,θS) =|R| ·Iprobe (S)·(A+C)\n(E-Index Join Cost)\nProbing the index removes the need for a full cross-product\nby pruning out the search space at construction time, reducing\nthe necessary computation. This assumes an index on the par-\nticular similarity expression exists, where the approximation\nis also an index build-time parameter, affecting the overall\nperformance [10]. Still, the per-tuple cost of probing the index\nis higher than scanning due to the data structure traversal and\nrandom accesses, aiming to be offset by lower overall cost.\nThis assumption works when all the data is searched/joined.\nHowever, analytical queries are typically selective (on rela-\ntional attributes), which is not directly compatible with a\nvector index. Rather, pre-filtering techniques are employed,\nwhere the result set excludes tuples based on the relational\ncondition on the fly while still incurring the traversal cost [19].\nOn the other hand, a scan-based approach can exclude tuples\nat a lower cost. We compare and evaluate our approach\nagainst the vector databases index approach (Subsection VI-E),\nextending the prior work on access path selection [31] and\nmotivating for a selectivity-driven decision. We outline the\nkey differences between the index and scan join in Table I.\nC. Tensor Join Formulation: Enhancing The Nested-Loop Join\nDtv=dX\ni=1RtiSiv\nS11. . .S1v\n.........\nSd1. . .Sdv\n→S\nR←\nR11. . .R1d\n.........\nRt1. . .Rtd\n\nD11. . .D1v\n.........\nDt1. . .Dtv\n→D=RS\n(Block Matrix Dot Product Decomposition [53])\n6\n\nAs the computation of context-enhanced operators happens\nover dense high-dimensional embedding vectors, following\nthe vector and matrix definitions of cosine distance in Sub-\nsection III-A, we present the tensor formulation of the dot\nproduct. It is important to highlight that cosine similarity is\nequivalent to the dot product with normalized input vectors.\nThe tensor formulation allows reasoning about the potential\ndecomposition of the problem for parallel and cache-efficient\nexecution beyond data parallelism, a basis for the physical\noptimization (Section V). This enables efficient and well-\nstudied matrix-based algorithms for linear algebra in addition\nto the traditional relational algorithms. We present the block-\nmatrix decomposition of the problem [53].\nGiven a (|R| ×dim)matrix Rwithtrow partitions and\ndcolumn partitions, and a (dim× |S|)matrixSwithdrow\npartitions and vcolumn partitions that are compatible with\nthe partitions of R, dot product D=RS can be formed\nblock-wise, yielding Das a (|R| × |S|)matrix with trow\npartitions and vcolumn partitions. We consider Sto be already\ntransposed if the initial data layout is as of R; in other words,\nmatrices RandSare compatible.\nIn particular, we partition the data along the tuple lines, not\nthe dimensions, as illustrated in Figure 6 1⃝. Transforming\nthe initial Nested-Loop Join into Tensor formulation enables\nthe application of linear algebra optimizations, in particular,\nmatrix multiplication algorithms, to achieve better cache uti-\nlization of high-dimensional data with formalized paralleliza-\ntion using block-matrix decomposition. This is compatible\nwith and extends recent research on formulating relational\noperators for tensor processing runtimes [54], as we explicitly\nconsider linear algebra domain optimizations in our Tensor\nJoin formulation.\nIn contrast to NLJ, a matrix block (several vectors) can\nremain in the cache and be reused over many operations,\nleading to better cache utilization. Block-matrix partitioning\nallows for defining the processing granularity, constraining the\nmemory footprint, and allowing fine control of the processing\ngranularity of cosine-distance-based similarity operations, all\nwhile reducing redundant data movement, resulting in a com-\nputationally and data access-optimized dense matrix operation.\nThe next step is to map back to corresponding tuple pairs\nthat satisfy the threshold condition, as in Figure 6 2⃝.\nMaintaining the starting offsets of input relation partitions\nis sufficient, so the result set constitutes a potentially sparse\nmatrix of pairs representing matrix batch offsets driven\nby the predicate selectivity. This result can be considered\nequivalent to late materialization, and while sparse, it is\nmore compact as tuples of offsets represent unique tensor\nidentifiers. This is increasingly important when using novel\nmemory hierarchies with fast but limited memory, such as\nhigh-bandwidth memory (HBM) [55].\nTakeaway Formulating the cost model and alternative\nequivalent execution plans using linear algebra allows tuning\nthe algorithms to the cost model and execution environment\nparameters, as high-dimensional vectors and model processingintroduce data access, caching, and processing overheads. This\nis a mandatory step that enables further logical and physical\noptimizations, different from the ones suitable for traditional\nrelational operators that process only single-dimensional data.\nV. P HYSICAL OPTIMIZATION\nModern data management systems are designed and opti-\nmized to efficiently utilize available hardware resources [35],\n[36], [39]. Equally, machine learning and linear algebra frame-\nworks are designed with physical optimizations to allow fast\nand efficient execution over vector data [6]–[8], [19].\nA. Data-Parallel Execution\nTo benefit from many core architectures, we outline the\nparallelization and hardware-conscious optimizations of the\njoin algorithm. In contrast to the traditional Nested-Loop Join\n(NLJ) that allows exact cosine-distance-based joins, high-\ndimensional embedding vectors take up more space in the\ncache hierarchy. Consider a 32KB L1 cache, and we oper-\nate over 4-byte values. Using a 100-dimensional embedding\nvector, the L1 cache can store only 80 values, in contrast to\n8000 for the single-dimensional data type. This necessitates\ncache-efficient implementation to benefit from the memory hi-\nerarchy. Computing cosine distance over vectors requires more\ncomputation cycles than simply performing the regular value-\nbased operation. Thus, judicious use of hardware resources is\nnecessary to speed up data access and computation.\n1) Data parallelization strategies\nNested-Loop Join can be parallelized by partitioning the\ninput relations and using a heuristic of keeping the smaller re-\nlation inside the inner loop to improve data and cache locality.\nWe propose using the matrix (tensor) formulation (Figure 6)\nusing linear algebra as an alternative to traditional NLJ. In\ncontrast to NLJ, matrix multiplication over dense vectors is\na linear algebra operation with a better cache locality [56],\n[57], improving the use of the memory hierarchy in the\npresence of high-dimensional data, and using efficient matrix\nmultiplication algorithms and linear-algebra frameworks.\n2) CPU Hardware Support\nTraditionally, CPUs benefit from the main memory access\nlocality. They are general-purpose compute units designed\nto process full-precision data types (e.g., 32-bit and 64-bit)\nthat can support SIMD, such as with Intel A VX instructions.\nRecent A VX-512 [58] instruction set has introduced hardware\nsupport for half-precision data types, which allows processing\nup to 32 16-bit floating point numbers in a SIMD register.\nAccelerating machine learning operations is becoming more\ncommonplace in modern and upcoming CPUs, such as 4th\ngeneration Intel Scalable Xeon Processors, which introduced\nspecialized instruction sets (AMX) and registers meant for\naccelerating vector and matrix computations [59], along with\na limited capacity High-Bandwidth memory which can speed-\nup memory-bound access patterns [55]. In general, specialized\ninstructions can accelerate the dense matrix computations.\nAt the same time, low-latency access to memory enables\noptimizing the sparse matrix processing when processing the\n7\n\nʍB\nResultʍthreshold\nappendʍABufferFig. 7: Matrix partitioning constrains the memory requirement.\nelements that satisfy the join predicate. As even the main\nmemory is often limited or expensive, we propose how to\nconstrain the memory requirements of the tensor join.\n3) SIMD Vectorization\nExecuting linear algebra operations such as cosine distance\nover dense vectors is compute-intensive and involves repeated\noperations over every vector embedding element. Since op-\nerations such as sum are repeated over every element of the\nlogical vector, it is a natural fit for using single-instruction\nmultiple-data instructions (SIMD). We use SIMD vectorization\nsupported by hardware to speed up the arithmetic operations\nusing fewer processing cycles using specialized registers and\ncompute units, in conjunction with data-parallel partitioning\nfor multi-core operator execution. We also show that physical\noptimizations do not come for free and can be underutilized\ndue to bad logical plans, emphasizing the additive performance\nproposed optimization steps.\nB. Constraining the Memory Requirements\nThe tensor join formulation (Figure 6) assumes a dot\nproduct operation between two matrices, a dense matrix linear\nalgebra operation. This will result in a large intermediate\nstate matrix of the same dimensions as the input relations.\nDespite joins being typically selective, which might reduce\nthe matrix size, as in Figure 7, the intermediate state might\nstill be too big to store and compute. Computing |R|x|S|\nfor two 100k inputs yields a 100k x 100k matrix that results\nin 40GB of FP32 values. While this matrix can be preserved\nto offset future computation, even for modest input relation\nsizes, this approach, in its naive formulation, does not scale\nwell concerning the memory requirements.\nTo resolve this issue, the previously presented matrix de-\ncomposition (Equation: Block Matrix Dot Product Decompo-\nsition [53]) enables partitioning the computation into batches\nand explicitly controlling the memory requirements based on\nthe desired intermediate matrix size. This trades off memory\nfor multiple invocations of the computation algorithm with\nsmaller matrices, effectively computing the large one while\npruning the intermediate sparse state after each sub-block-\nmatrix computation. We illustrate this in Figure 7, where\ntwo relations AandBare joined over vectors. While the\nrequired memory requirement can be selectivity driven by\nother pushed-down relational predicates ( σA,σB), this might\nnot fit the available buffer budget. Thus, based on the available\nBuffer size, the input data can be partitioned arbitrarily by\ndecomposing the input data along the vector tuple boundaries(not dimensions). The strict |A|x|B| memory requirement\nbecomes Buffer = |part(A)|x|part(B)| , at the cost\nof several invocations of the algorithm that might reduce the\noverall performance by frequent data movement and lower\ncache locality.\nTakeaway The physical operator design landscape encom-\npasses implementation and hardware device characteristics-\nbased decisions. Model-operator interactions only enrich and\nopen a new design space. High-dimensional data contributes\nto reduced capacity of the memory hierarchy in comparison to\ncommon atomic data types found in relational data processing\nand requires rethinking cache-local implementations. With the\nincrease in per-tuple compute cost, the strain is on both\nmemory and compute resources, which invites the use of\nspecialized hardware-conscious algorithms such as tensor join.\nVI. E VALUATION\nWe start by demonstrating the functionality of using models\nas a driver of context-enhanced relational operations through\nthe example of word embeddings. We then focus on the main\nperformance evaluation of the proposed logical and physical\noptimizations, showing that a holistic approach is necessary\nto obtain a performant join algorithm.\nSystem To conduct the in-depth study, we implement our\noperators in a standalone system in C++ and use Intel A VX\ninstructions for SIMD execution. Tensor formulation bench-\nmarks use Intel oneAPI Math Kernel Library for CPU-aware\nand efficient BLAS-based linear algebra operations. We imple-\nment our index-based formulation in a hardware-optimized,\nopen-source vector database Milvus [19]. We configure the\nsystem in standalone mode (version 2.3.9).\nHardware Setup We run the end-to-end and scalability\nexperiments on a two-socket Intel Xeon Gold 5118 CPU (2\nx 12-core, 48 threads) and 384GB of RAM. All experiments\nare with in-memory data; experiments with synthetic data use\nthe same random number generator seed for reproducibility.\nA. Enhancing Operator Context via Word Embeddings\nIn our study, we use the example of word embeddings\nthat transform input strings into high-dimensional vectors. We\nshow the context-awareness functionality that word embed-\ndings allow and note that embedding models can be fine-\ntuned and replaced to support different notions of similarity.\nThe intermediate data representation of an embedding is a\ncontext-free vector that operators process independently of the\nparticular model, on top of which we base our analysis. The\nproposed optimizations of our approach are independent by\ndesign and principled in approach due to the separation of\nconcerns between the model, which produces vectors, and the\noperator performing the join over context-free vectors.\nEmbedding Model We use FastText [45], [46] as a model\n(µ) for string embeddings, which has the desirable properties\nthat it can be trained and adapted to the context, it supports out\nof vocabulary word embedding and is resilient to misspellings.\nIt learns word representations by extending Word2Vec [44]\nwith subword information for better handling of rare words.\n8\n\nIt takes a string as an input and produces an embedding as\nan output. A context-aware operator is supplemented with an\nembedding model. In this case, when an operator receives\nstrings, it embeds them using FastText and then performs the\nrequested processing in the vector domain.\nDataset We train a 100-dimension embedding model over a\nsubset of Wikipedia dataset [60], cleaned of stopwords, using\na subset of 1M strings from the dataset to test the similarity\nusing the model. We show the nearest vectors to sample words\nas the strings are embedded into a high-dimensional vector\nspace. We then decode the vectors back into string and present\nsample semantic matches in Table II. The model has learned\nsemantics and context from the Wikipedia dataset. To fine-tune\nthe model, it is possible to specialize the embedding models\nwith other domain-specific datasets.\nTABLE II: Semantic Matching using FastText trained on\nWikipedia dataset, 100-D embeddings, sample words.\nWord Top-15 Model Matches\ndbms rdbms, nosql, dbmss, postgresql, rdbmss, sql, dbmses, sqlite,\ndataflow, ordbms, oodbms, couchdb, mysql, ldap, oltp\npostgres postgre, postgresql, openvt, dbms, rdbmss, sqlite, dbmss, odbc,\nbackend, rdbms, rdbmses, postgis, openvp, couchdb, mysql\nclothes dresses, clothing, garments, underwear, bedclothes, undergar-\nments, towels, underwears, scarves, shoes, nightgowns, cloth-\nings, bathrobes, underclothes\nModels enable automated semantic matching, and the\nstrings are not materialized or retrieved during operations in\nan intermediate step. The computation entirely happens on\nembedded data, and only positive matches are retrieved. It\nis possible to decode the embeddings, for example, based on\ntheir offset in the input relation and processing the embedding\nusing standard encoder-decoder model architecture.\nThis model aimed to detect synonyms, semantic and related\nmatches, and plural forms of the words without external user\nspecification. The only parameter in the case of a join with\ncosine distance would be a single threshold parameter. This\nallows relational operators normally operating over sample\nstrings (i.e., Word column in Table II) to perform matches\nwith strings on the right in the embedding domain without\nhumans in the loop or creating and specifying strict rules, in\ncontrast to current similarity join approaches [20], [23]. Such\nmodels can work by providing positive match examples that\ncould infer the correct cosine distance threshold parameter.\nB. NLJ Formulation: Logical Optimization\nWe extend the traditional relational join formulation by\nembedding vector processing and retrieval (Section IV). We\nevaluate the impact of logical optimization of vector prefetch-\ning and physical optimization using SIMD in Figure 8. This\nexperiment validates the cost difference between the naive join\nextension (Equation: E-NL Join Cost) and the one aware of\nthe vector retrieval (Equation: E-NLJ Prefetch Optimization).\nNot prefetching the embeddings incurs quadratic model access\ncosts validating the cost model, resulting in orders of magni-\ntude slower execution time and low benefit from hardware1k x 1k 10k x 1k 10k x 10k101103105\n389.8\n3,419\n36,242.6280.2\n3,023.6\n33,226.69\n62\n651.24\n35\n269.6\n|R|x|S|tuplesTime [ms] - log 10scaleNO-SIMD SIMD\nPrefetch NO-SIMD Prefetch SIMD\nFig. 8: The impact of logical and physical optimization on\nNLJ formulation. 100-D vectors, 48 threads.\n148121620242832 40 4805101520\n# of threadsTime [s]SIMD NO-SIMD\nFig. 9: Optimized NLJ scalability with correct logical opti-\nmization, 10k x 10k join input relations, 100-D vectors.\nacceleration. This demonstrates the importance of analyzing,\nexposing, and optimizing model-operator interactions, where\ndespite using the same hardware resources, including separate\nexperiments with and without SIMD, the main bottleneck is\nnot computational but access-pattern-related and algorithmic.\nWith the wrong holistic operator formulation, faster hardware\ncannot correct the suboptimal formulation, as may happen\nwith imperative operator specification by a non-expert user. In\nthis case, the optimal strategy of prefetching the embeddings\nfirst and then joining, despite having two separate tasks,\nallows faster execution time. SIMD instructions improve the\nexecution time 2x, indicating a computational bottleneck that\nadditional hardware instructions reduce, while this is not\npossible in the non-prefetch, sub-optimal formulation.\nTakeaway. Logical operator optimizations and task or-\nchestration are crucial to removing algorithmic bottlenecks.\nAllocating more resources cannot scale and is wasteful before\nresolving an algorithm’s logical costs and overheads. Using the\nimproved NLJ cost model formulation, execution time scales\nlinearly instead of quadratically, as in the non-optimized case.\nC. NLJ Formulation: Physical Optimization\nWe focus next on the physical optimizations and demon-\nstrate the scalability of CPU execution and physical and logical\noptimizations of NLJ formulation presented in Section V.\nFirst, we investigate the scalability (Figure 9). We enable hy-\nperthreading (24 physical, 48 logical cores), affinitize threads\nto cores (2 threads will run on 1 physical core, 4 on 2,\netc.), and run the NLJ formulation of 10k x 10k relation\n9\n\n10k x 10k 100k x 1k 1k x 100k1M x 1k 1k x 1M10k x 100k 100k x 10k100k x 100k10k x 1M 1M x 10k103104\n269.6264.4319.82,4103,957\n2,8542,786.428,31030,902.4\n22,593.6\n|R|x|S|tuplesTime [ms] - log 10scale|108||109||1010|operations\nFig. 10: Optimized NLJ formulation with varying input rela-\ntion sizes, 100-D vectors, 48 threads.\nsize input with 100-dimensional embeddings. The processor\nhas A VX-512 registers that can simultaneously fit 16 32-bit\nfloating-point values simultaneously. The average improve-\nment is 5.36x, indicating non-computational overheads during\nvectorization but improved execution time using available\nhardware intrinsics.\nNext, we evaluate the impact of different input relation sizes\n(in tuples) over 100-D, 32-bit embeddings over 48 threads and\ninvestigate the impact of physical and logical optimizations\nusing the NLJ formulation. In this experiment (Figure 10), we\ninvestigate the effects of input sizes, number of computations,\nand ordering of input relations of context-enhanced NLJ\nimplementation. First, the execution time scales linearly with\nthe number of computations/operations performed, according\nto the cost model (Equation: E-NLJ Prefetch Optimization).\nSecond, we validate that to achieve improved execution time\ndue to cache locality, a smaller relation should still be the inner\nloop, as in the traditional nested-loop-join. Despite more ex-\npensive per-vector computations, data access patterns still play\nan important role, impacting our experiment’s performance by\nup to∼35% (at 1010operations).\nTakeaway. Logical and physical optimizations of the NLJ\nformulation with vectors enable reducing the overheads by\norders of magnitude from the initial vector join extension. Still,\nthe approaches we proposed until now optimize for vector ex-\necution without explicitly considering the high dimensionality\nand similarity operations over individual tuples. The tensor\nformulation, which we will evaluate next, addresses this issue.\nD. Tensor Formulation: The Holistic Vector-Join Optimization\nWe proposed batching multiple vector tuples in a tensor join\nformulation using optimized matrix computation (Figure 6)\ninstead of individual vector operations in the NLJ. The key\nenabler and difference is that resulting matrix operations\nare highly optimized for the cache locality that the simple\nNLJ imposed in its formulation. We evaluate this physical\noptimization proposed in Section V, evaluating whether the14166425614166425614166425610−1100101\n25600 2560000 256000000 # FP32 Ops:Vector #FP32:\nTime per element [ns]Vectorize-NLJ Tensor\nFig. 11: Physical optimization. The tensor strategy (green)\npays off in larger inputs compared to NLJ (blue).\n14166425614166425614166425610−1100101\n25600 2560000 256000000 # FP32 Ops:Vector #FP32:\nTime per element [ns]Tensor-Fully-Batched Tensor-Non-Batched\nFig. 12: The impact of vector batching. Non-batched indicates\nthat one of the join inputs is processed one vector at a time.\ntensor formulation improves the per-vector-element processing\ntime. We compare two strategies, running the fully optimized\nNLJ against the Tensor formulation. For this, we vary two\nfactors: the total number of floating point numbers processed\n(#FP32 Ops) and how many floating point numbers represent\nan individual vector (vector dimensionality, Vector #FP32).\nFigure 11 summarizes the findings, where three data clusters\nare based on the number of operations, refined by individual\nvector size. In other words, for the 25600 case with dimen-\nsionality 1, there are 25600 /1tuples joined, equally balanced\nin two relations, indicatingp\n25600 /1 = 160 tuples per input\nrelation. Similarly, to obtain the number of tuples for the case\nof dimensionality 256,p\n25600 /256 = 10 . We use the per-\nFP32 breakdown as a unifying metric across the input size\nand dimensionality. First, we notice the benefit of vectorization\nwith increased vector size, where specialized hardware opera-\ntions improve the per-tuple performance. Second, pushing this\nboundary beyond per-tuple-vector but to a whole tuple-vector-\nbatch (Tensor), when sufficient computation can benefit from\nthe cache locality, significantly improves the execution time.\nThe Tensor approach is slower only in case there were a few\n(p\n25600 /64 = 20 andp\n25600 /256 = 10 ) tuples to join.\nBatching the vectors together in the tensor formulation is the\nkey to reducing unnecessary data movement. We demonstrate\nthe impact of batching in Figure 12, where the BLAS-matrix\noperations are used with one fully batched relation. At the\nsame time, the other is loaded vector-by-vector, repeated\nas many times as there are tuples. An alternative is where\nboth relations are fully batched. While inefficiencies are not\nnoticeable with very small input sizes, batching becomes\nincreasingly significant for scalability as the input grows.\nAs explained in Subsection V-B, batching too many vectors\nin large tensors simultaneously comes at a prohibitive memory\n10\n\n100k x 100k50k x 50k100k x 10k10k x 50k5k x 50k10k x 10k10k x 5k5k x 5k100101102\nMatrix Mini-Batch SizeRelative to No BatchRelative Slowdown Relative Decrease of Required RAM\nFig. 13: Batch size impact on memory requirements and\nexecution time. 100k x 100k, 100-D input (No Batch case).\n10k x 10k 100k x 10k 100k x 100k 1M x 100k 1M x 1M102104106\nTimeout: 40+ minutes35\n366.34\n4,303\n55,397.34\n4.52·105269.6\n2,854.8\n28,310\n2.5·105\n|R|x|S|tuplesTime [ms] - log 10scaleTensor NLJ\nFig. 14: Tensor join vs. NLJ formulation, 100-D, 48 threads.\ncost. We propose using mini-batches partitioned across tuple\nboundaries (Figure 7) that can still benefit from the improved\nlinear algebra algorithms and data locality. The impact of\nbatching is presented in Figure 13. We run the tensor join\nformulation over 100k x 100k, 100-D input using 48 threads.\nTheNo Batch case runs the join on the whole input si-\nmultaneously. At the same time, the experiment focuses on\nmemory footprint reduction and the computational price to pay\nwhen various mini-batches are used. While there is a negligible\nrelative slowdown due to some added data movement and\nrepeated operations, there is a significant benefit due to the\nreduction of the necessary memory.\nFinally, we compare the NLJ with the Tensor formulation\nend-to-end execution time in Figure 14. While the execution\ntime of both algorithms scales approximately linearly when\nincreasing the input relation size, the algorithm optimizations\nenabled by batching vectors into tensors opened linear algebra-\nbased execution optimizations with almost an order of magni-\ntude improvement across various input sizes.\nE. Scan vs Probe: Vector Indexes Meet Analytical Workloads\nWith an optimized scan-based Tensor join, fully amenable to\nrelational filtering, we compare this approach to a join operator\nimplemented using vector indexes commonly used by vector\ndatabases that support relational pre-filtering. We construct an\nHNSW index [52], taking the overall best-performing index0 10 20 30 40 50 60 70 80 90 100012\nselectivity %Time [s]Tensor Join Tensor Join (-filter cost)\nIndex Join (Lo) Index Join (Hi)\nFig. 15: Top-K=1 vector join condition (10k x 1M with filter)\n0 10 20 30 40 50 60 70 80 90 1000123\nselectivity %Time [s]Tensor Join Tensor Join (-filter cost)\nIndex Join (Lo) Index Join (Hi)\nFig. 16: Top-K=32 vector join condition (10k x 1M with filter)\nfrom ANN-Benchmark [10]. As indexes are approximate and\nsupport the defined build-time distance, we construct the\nindexes meant for cosine-distance filtering with two cases:\nhigher-recall/accuracy (Hi) and lower-recall (Lo).Hihas a\nmaximum degree of nodes on each graph layer M= 64 ,\nand search range parameter efConstruction = 512 .Lohas\nM= 32 andefConstruction = 256 , improving latency\nbut reducing accuracy. The system enables parallel probing,\nwhich we use to implement a join by batching the search\nvector queries. It is mandatory to specify the top-k values to\nretrieve in an index-based approach, limiting the flexibility of\nthe join operator. We limit the concurrent index probing to 10k,\nconstruct an index on 1Mvectors, and include one relational\nattribute column based on which we control the selectivity.\nWe compare the scan-based tensor join with pre-filtering in\nthe same setup, in-memory, and using 48 threads.\nIn case the index retrieves and joins only with the most\nsimilar value (Figure 15), which is the best case for an index-\nbased approach, the reduced scan and computation cost pays\noff at 20%−30% selectivity, where below this threshold\nthe scan-based approach manages to effectively filter out and\ncompute the join faster. Once we join with more than top-1\nsimilar tuples, in this case, top-32 (Figure 16), the index probe\nand traversal cost becomes more expensive, shifting the cross-\npoint to 80% for lower accuracy index, and impractical by\nbeing always slower for high-accuracy index. The scan-based\napproach is always exact, performing exhaustive comparisons.\nThe indexes are limited to build-time characteristics, and\nwhen a different expression, in this case (Figure 17), similarity\ncoefficient filter ( similarity > 0.9) is applied, the index-\nbased performance drops, being comparable to tensor join only\naround 5−10%selectivity despite still retrieving tuples based\non the top-k mechanism ( k= 32 ). On the other hand, Tensor-\n11\n\n0 10 20 30 40 50 60 70 80 90 100051015\nselectivity %Time [s]Tensor Join Index Join (Lo) Index Join (Hi)\nFig. 17: Range vector join condition (10k x 1M with filter)\njoin is flexible concerning expression processing and returns\nnot only top-k tuples in comparison but also all the matching\nand qualifying tuples, overall being faster in low and high\nselectivity ranges.\nTakeaway. Holistic optimization of the join algorithm with\nvector inputs is necessary to enable fast and efficient com-\nputation. We reduced model-operator overheads, tuned the\nindividual and batched vector computation, and designed and\nevaluated access pattern-aware operators that efficiently use\nunderlying hardware capabilities. We discussed access path\nselection [31] in vector data management and demonstrated\nthe trade-offs in scan versus index-based join implementation.\nVII. R ELATED WORK\nThis section outlines the related work and compares and\nplaces our approach in the rich design space of prior research.\nMachine Learning for Databases has been a research topic\nwhere DBMS’s structural components get enhanced using ML\ncommunity findings. Learned indexes [61] avoid data structure\ntraversal by learning the data distribution information and\noptimizing data access. From a systems perspective, using\ntensor processing frameworks for traditional relational pro-\ncessing has also recently been proposed [41]. Our approach is\nsimilar as we propose using ML embedding models to provide\ncontext to data traditionally opaque to relational DBMS, while\nbroadening the analysis and application scope of traditional\nanalytics with novel model-relational interactions compatible\nwith the relational model.\nDatabases for Machine Learning focus on applying\nor integrating machine learning components with systems.\nFrameworks such as Tensorflow [6] or Pytorch [7] are ef-\nficient, hardware-conscious dataflow engines. The prolifera-\ntion of model-embeddings fueled the development of vector-\nspecialized databases [19], focusing on retrieval and search\nwith efficient high-dimensional similarity indexes [8]. Still,\nsuch engines often lack a DBMS’s expressiveness, functional-\nity, and analytical operations for more complex data analysis,\nwhich would force users to write imperative code to integrate\ndifferent siloed system components, with suboptimal relational\nfiltering and access path selection. While systems that combine\nanalytics with training and deploying machine learning models\nexist [62], [63], to our best knowledge, they only provide\nusability wrappers around models and limited User-Defined\nFunction (UDF) capabilities [64] for implementing custom\noperators, making them an impractical platform for the end-\nto-end optimizations and detailed sensitivity analysis.Similarity Joins. We proposed a join operator that is\nfunctionally a similarity join, as processing embeddings in-\nherently entails similarity distance computation instead of\ncomparing exact values. Similarly, locality-sensitive hashing\ntechniques [65] exist for approximate joins. Traditional string\nsimilarity techniques require exact similarity specification us-\ning edit distance or q-grams as token-based string similar-\nity [28], requiring users to specify the similarity rules for\nfinding misspellings or token-based differences [20], [23].\nIn contrast, we propose using word embedding models\ncapable of identifying misspellings, different tenses, and se-\nmantic similarity based on training and fine-tuning training\ndataset and parameters [45], [46]. Through the separation of\nconcerns, from the perspective of DBMS, string similarity join\nhas a tensor-based input with cosine distance and threshold as\nparameters. At the same time, the embedding model handles\nthe string semantics and context and transforms the input into\ncontext-free embeddings for RDBMS to process. Furthermore,\nour approach extends the notion of similarity to other context-\nrich data formats and modalities for which embedding models\nexist [1], [3], [4], [13], [14], [44], [46].\nRepresentation Learning The significant body of work\nin representation learning is the key enabler of context-rich\nrelational operators. It allows for transforming the human-\ncentric, context-rich data representations into machine-centric\nformats amenable to automated processing. We combine ML-\nbased embedding models with relational operators and analyze\nend-to-end interactions, from logical to physical optimizations.\nSuch models allow masking and transforming contextual data\ninto embeddings as ubiquitous data representations that can be\nprocessed by extending RDBMS with tensor-based operators.\nA rich research area in machine learning drives embedding\nmodels that support other context-rich data formats beyond\nstrings, equally transforming the input into context-free em-\nbeddings, enabling multi-modality [14], processing images [3],\naudio [13], documents [1], [30]. Models trained on web-scale\ndata exist as Foundation Models [4], that can be re-trained and\nadapted for a specific task and dataset.\nVIII. C ONCLUSION\nData management systems support analysts with modern\ndata processing tools. As embedding models automate context-\nrich analysis, RDBMS needs to provide operators based on\nextended relational algebra and optimize their interactions.\nOperations over the embeddings are not exact but intrinsically\nfuzzy and necessitate similarity operations. We propose a\ncontext-enhanced join based on the key observation of the\nseparation of concerns between the context-providing em-\nbedding model and the analytical engine. We analyze the\nbehavior of the join operator and propose relational algebra\nextensions, as well as logical and physical optimizations for\nefficient execution. This includes the comparison of our scan-\nbased Tensor Join against the vector index-based alternative\nin a hybrid vector-relational setting, demonstrating the impact\nof optimizations and access path selection trade-offs on the\nexecution time.\n12\n\nREFERENCES\n[1] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-training\nof deep bidirectional transformers for language understanding,” in\nProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,\n2019, Volume 1 (Long and Short Papers) , J. Burstein, C. Doran, and\nT. Solorio, Eds. Association for Computational Linguistics, 2019, pp.\n4171–4186. [Online]. Available: https://doi.org/10.18653/v1/n19-1423\n[2] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-\nV oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler,\nJ. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,\nand D. Amodei, “Language models are few-shot learners,” in Advances\nin Neural Information Processing Systems 33: Annual Conference on\nNeural Information Processing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual , H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan,\nand H. Lin, Eds., 2020.\n[3] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in 2016 IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2016, Las Vegas, NV , USA, June\n27-30, 2016 . IEEE Computer Society, 2016, pp. 770–778. [Online].\nAvailable: https://doi.org/10.1109/CVPR.2016.90\n[4] R. Bommasani, D. A. Hudson, E. Adeli, R. B. Altman, S. Arora,\nS. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill,\nE. Brynjolfsson, S. Buch, D. Card, R. Castellon, N. S. Chatterji,\nA. S. Chen, K. Creel, J. Q. Davis, D. Demszky, C. Donahue,\nM. Doumbouya, E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh,\nL. Fei-Fei, C. Finn, T. Gale, L. Gillespie, K. Goel, N. D. Goodman,\nS. Grossman, N. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E.\nHo, J. Hong, K. Hsu, J. Huang, T. Icard, S. Jain, D. Jurafsky, P. Kalluri,\nS. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W. Koh, M. S.\nKrass, R. Krishna, R. Kuditipudi, and et al., “On the opportunities\nand risks of foundation models,” CoRR , vol. abs/2108.07258, 2021.\n[Online]. Available: https://arxiv.org/abs/2108.07258\n[5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\ninAdvances in Neural Information Processing Systems 30: Annual\nConference on Neural Information Processing Systems 2017, December\n4-9, 2017, Long Beach, CA, USA , I. Guyon, U. von Luxburg, S. Bengio,\nH. M. Wallach, R. Fergus, S. V . N. Vishwanathan, and R. Garnett, Eds.,\n2017, pp. 5998–6008. [Online]. Available: https://proceedings.neurips.\ncc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\n[6] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard et al. , “Tensorflow: a system for large-\nscale machine learning.” in Osdi , vol. 16, no. 2016. Savannah, GA,\nUSA, 2016, pp. 265–283.\n[7] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al. , “Pytorch: An\nimperative style, high-performance deep learning library,” Advances in\nneural information processing systems , vol. 32, 2019.\n[8] J. Johnson, M. Douze, and H. J ´egou, “Billion-scale similarity search\nwith gpus,” IEEE Trans. Big Data , vol. 7, no. 3, pp. 535–547, 2021.\n[Online]. Available: https://doi.org/10.1109/TBDATA.2019.2921572\n[9] P. E. O’Neil, E. J. O’Neil, and X. Chen, “The star schema benchmark\n(ssb),” Pat, vol. 200, no. 0, p. 50, 2007.\n[10] M. Aum ¨uller, E. Bernhardsson, and A. J. Faithfull, “Ann-benchmarks:\nA benchmarking tool for approximate nearest neighbor algorithms,”\nInf. Syst. , vol. 87, 2020. [Online]. Available: https://doi.org/10.1016/j.\nis.2019.02.006\n[11] E. F. Codd, “A relational model of data for large shared data banks,”\nCommun. ACM , vol. 13, no. 6, pp. 377–387, 1970. [Online]. Available:\nhttps://doi.org/10.1145/362384.362685\n[12] V . Sanca and A. Ailamaki, “Analytical engines with context-rich\nprocessing: Towards efficient next-generation analytics,” in 39th IEEE\nInternational Conference on Data Engineering, ICDE 2023, Anaheim,\nCA, USA, April 3-7, 2023 . IEEE, 2023, pp. 3699–3707. [Online].\nAvailable: https://doi.org/10.1109/ICDE55515.2023.00298\n[13] Q. Kong, Y . Cao, T. Iqbal, Y . Wang, W. Wang, and M. D.\nPlumbley, “Panns: Large-scale pretrained audio neural networks\nfor audio pattern recognition,” IEEE ACM Trans. Audio SpeechLang. Process. , vol. 28, pp. 2880–2894, 2020. [Online]. Available:\nhttps://doi.org/10.1109/TASLP.2020.3030497\n[14] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer\nlearning with a unified text-to-text transformer,” J. Mach. Learn.\nRes., vol. 21, pp. 140:1–140:67, 2020. [Online]. Available: http:\n//jmlr.org/papers/v21/20-074.html\n[15] J. Wang, P. Huang, H. Zhao, Z. Zhang, B. Zhao, and D. L. Lee,\n“Billion-scale commodity embedding for e-commerce recommendation\nin alibaba,” in Proceedings of the 24th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining , ser. KDD ’18.\nNew York, NY , USA: Association for Computing Machinery, 2018, p.\n839–848. [Online]. Available: https://doi.org/10.1145/3219819.3219869\n[16] J.-T. Huang, A. Sharma, S. Sun, L. Xia, D. Zhang, P. Pronin,\nJ. Padmanabhan, G. Ottaviano, and L. Yang, “Embedding-based\nretrieval in facebook search,” in Proceedings of the 26th ACM\nSIGKDD International Conference on Knowledge Discovery & Data\nMining , ser. KDD ’20. New York, NY , USA: Association for\nComputing Machinery, 2020, p. 2553–2561. [Online]. Available:\nhttps://doi.org/10.1145/3394486.3403305\n[17] P. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin,\nN. Goyal, H. K ¨uttler, M. Lewis, W. Yih, T. Rockt ¨aschel,\nS. Riedel, and D. Kiela, “Retrieval-augmented generation for\nknowledge-intensive NLP tasks,” in Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual ,\nH. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin,\nEds., 2020. [Online]. Available: https://proceedings.neurips.cc/paper/\n2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html\n[18] F. Zhang, N. J. Yuan, D. Lian, X. Xie, and W.-Y . Ma, “Collaborative\nknowledge base embedding for recommender systems,” in Proceedings\nof the 22nd ACM SIGKDD international conference on knowledge\ndiscovery and data mining , 2016, pp. 353–362.\n[19] J. Wang, X. Yi, R. Guo, H. Jin, P. Xu, S. Li, X. Wang, X. Guo, C. Li,\nX. Xu, K. Yu, Y . Yuan, Y . Zou, J. Long, Y . Cai, Z. Li, Z. Zhang, Y . Mo,\nJ. Gu, R. Jiang, Y . Wei, and C. Xie, “Milvus: A purpose-built vector\ndata management system,” in SIGMOD ’21: International Conference\non Management of Data, Virtual Event, China, June 20-25, 2021 , G. Li,\nZ. Li, S. Idreos, and D. Srivastava, Eds. ACM, 2021, pp. 2614–2627.\n[Online]. Available: https://doi.org/10.1145/3448016.3457550\n[20] M. Yu, G. Li, D. Deng, and J. Feng, “String similarity search and join:\na survey,” Frontiers Comput. Sci. , vol. 10, no. 3, pp. 399–417, 2016.\n[Online]. Available: https://doi.org/10.1007/s11704-015-5900-5\n[21] Y . Jiang, G. Li, J. Feng, and W. Li, “String similarity joins: An\nexperimental evaluation,” Proc. VLDB Endow. , vol. 7, no. 8, pp.\n625–636, 2014. [Online]. Available: http://www.vldb.org/pvldb/vol7/\np625-jiang.pdf\n[22] Z. Chen, Y . Wang, V . R. Narasayya, and S. Chaudhuri, “Customizable\nand scalable fuzzy join for big data,” Proc. VLDB Endow. ,\nvol. 12, no. 12, pp. 2106–2117, 2019. [Online]. Available: http:\n//www.vldb.org/pvldb/vol12/p2106-chen.pdf\n[23] Y . N. Silva, W. G. Aref, P. Larson, S. Pearson, and M. H. Ali,\n“Similarity queries: their conceptual evaluation, transformations, and\nprocessing,” VLDB J. , vol. 22, no. 3, pp. 395–420, 2013. [Online].\nAvailable: https://doi.org/10.1007/s00778-012-0296-4\n[24] Microsoft, “Fuzzy lookup transformation - sql server integration services\n(ssis),” https://learn.microsoft.com/en-us/sql/integration-services/\ndata-flow/transformations/fuzzy-lookup-transformation?view=\nsql-server-ver16, 2023, accessed: 2024-02-20.\n[25] Informatica, “Informatica data quality and observability,” https://www.\ninformatica.com/products/data-quality.html, 2024, accessed: 2024-02-\n20.\n[26] KNIME, “Knime,” https://www.knime.com/, 2024, accessed: 2024-02-\n20.\n[27] Talend, “Talend — a complete, scalable data management solution,”\nhttps://www.talend.com/, 2024, accessed: 2024-02-20.\n[28] L. Gravano, P. G. Ipeirotis, H. V . Jagadish, N. Koudas, S. Muthukrishnan,\nand D. Srivastava, “Approximate string joins in a database (almost)\nfor free,” in Proceedings of the 27th International Conference on Very\nLarge Data Bases , ser. VLDB ’01. San Francisco, CA, USA: Morgan\nKaufmann Publishers Inc., 2001, p. 491–500.\n[29] Meta AI. (2024) Using ai to detect covid-\n19 misinformation and exploitative content. Accessed:\n13\n\n2024-02-20. [Online]. Available: https://ai.meta.com/blog/\nusing-ai-to-detect-covid-19-misinformation-and-exploitative-content/\n[30] S. Chen, A. Soni, A. Pappu, and Y . Mehdad, “Doctag2vec: An\nembedding based multi-label learning approach for document tagging,”\ninProceedings of the 2nd Workshop on Representation Learning\nfor NLP , Rep4NLP@ACL 2017, Vancouver, Canada, August 3, 2017 ,\nP. Blunsom, A. Bordes, K. Cho, S. B. Cohen, C. Dyer, E. Grefenstette,\nK. M. Hermann, L. Rimell, J. Weston, and S. Yih, Eds. Association\nfor Computational Linguistics, 2017, pp. 111–120. [Online]. Available:\nhttps://doi.org/10.18653/v1/w17-2614\n[31] M. S. Kester, M. Athanassoulis, and S. Idreos, “Access path selection\nin main-memory optimized data systems: Should I scan or should I\nprobe?” in Proceedings of the 2017 ACM International Conference\non Management of Data, SIGMOD Conference 2017, Chicago, IL,\nUSA, May 14-19, 2017 , S. Salihoglu, W. Zhou, R. Chirkova, J. Yang,\nand D. Suciu, Eds. ACM, 2017, pp. 715–730. [Online]. Available:\nhttps://doi.org/10.1145/3035918.3064049\n[32] L. Zhang, M. Butrovich, T. Li, A. Pavlo, Y . Nannapaneni, J. Rollinson,\nH. Zhang, A. Balakumar, D. Biales, Z. Dong, E. J. Eppinger, J. E.\nGonzalez, W. S. Lim, J. Liu, L. Ma, P. Menon, S. Mukherjee,\nT. Nayak, A. Ngom, D. Niu, D. Patra, P. Raj, S. Wang, W. Wang,\nY . Yu, and W. Zhang, “Everything is a transaction: Unifying\nlogical concurrency control and physical data structure maintenance\nin database management systems,” in CIDR 2021, Conference\non Innovative Data Systems Research , 2021. [Online]. Available:\nhttps://db.cs.cmu.edu/papers/2021/cidr2021 paper06.pdf\n[33] A. Kemper and T. Neumann, “Hyper: A hybrid oltp&olap main memory\ndatabase system based on virtual memory snapshots,” in Proceedings\nof the 27th International Conference on Data Engineering, ICDE\n2011, April 11-16, 2011, Hannover, Germany , S. Abiteboul, K. B ¨ohm,\nC. Koch, and K. Tan, Eds. IEEE Computer Society, 2011, pp. 195–206.\n[Online]. Available: https://doi.org/10.1109/ICDE.2011.5767867\n[34] A. Pavlo, G. Angulo, J. Arulraj, H. Lin, J. Lin, L. Ma, P. Menon,\nT. Mowry, M. Perron, I. Quah, S. Santurkar, A. Tomasic, S. Toor,\nD. V . Aken, Z. Wang, Y . Wu, R. Xian, and T. Zhang, “Self-\ndriving database management systems,” in CIDR 2017, Conference\non Innovative Data Systems Research , 2017. [Online]. Available:\nhttps://db.cs.cmu.edu/papers/2017/p42-pavlo-cidr17.pdf\n[35] T. Neumann, “Efficiently compiling efficient query plans for modern\nhardware,” Proc. VLDB Endow. , vol. 4, no. 9, pp. 539–550, 2011.\n[Online]. Available: http://www.vldb.org/pvldb/vol4/p539-neumann.pdf\n[36] P. Chrysogelos, M. Karpathiotakis, R. Appuswamy, and A. Ailamaki,\n“Hetexchange: Encapsulating heterogeneous CPU-GPU parallelism in\nJIT compiled engines,” Proc. VLDB Endow. , vol. 12, no. 5, pp.\n544–556, 2019. [Online]. Available: http://www.vldb.org/pvldb/vol12/\np544-chrysogelos.pdf\n[37] T. Neumann and M. J. Freitag, “Umbra: A disk-based system with\nin-memory performance,” in 10th Conference on Innovative Data\nSystems Research, CIDR 2020, Amsterdam, The Netherlands, January\n12-15, 2020, Online Proceedings . www.cidrdb.org, 2020. [Online].\nAvailable: http://cidrdb.org/cidr2020/papers/p29-neumann-cidr20.pdf\n[38] T. Kersten, V . Leis, A. Kemper, T. Neumann, A. Pavlo, and P. A.\nBoncz, “Everything you always wanted to know about compiled\nand vectorized queries but were afraid to ask,” Proc. VLDB\nEndow. , vol. 11, no. 13, pp. 2209–2222, 2018. [Online]. Available:\nhttp://www.vldb.org/pvldb/vol11/p2209-kersten.pdf\n[39] M. Zukowski, M. van de Wiel, and P. A. Boncz, “Vectorwise: A\nvectorized analytical DBMS,” in IEEE 28th International Conference\non Data Engineering (ICDE 2012), Washington, DC, USA (Arlington,\nVirginia), 1-5 April, 2012 , A. Kementsietsidis and M. A. V . Salles, Eds.\nIEEE Computer Society, 2012, pp. 1349–1350. [Online]. Available:\nhttps://doi.org/10.1109/ICDE.2012.148\n[40] S. Idreos, K. Zoumpatianos, B. Hentschel, M. S. Kester, and D. Guo,\n“The data calculator: Data structure design and cost synthesis from\nfirst principles and learned cost models,” in Proceedings of the\n2018 International Conference on Management of Data, SIGMOD\nConference 2018, Houston, TX, USA, June 10-15, 2018 , G. Das,\nC. M. Jermaine, and P. A. Bernstein, Eds. ACM, 2018, pp. 535–550.\n[Online]. Available: https://doi.org/10.1145/3183713.3199671\n[41] A. Gandhi, Y . Asada, V . Fu, A. Gemawat, L. Zhang, R. Sen, C. Curino,\nJ. Camacho-Rodr ´ıguez, and M. Interlandi, “The tensor data platform:\nTowards an ai-centric database system,” 2023. [Online]. Available:\nhttps://www.cidrdb.org/cidr2023/papers/p68-gandhi.pdf[42] Q. Lin, S. Wu, J. Zhao, J. Dai, F. Li, and G. Chen, “A comparative\nstudy of in-database inference approaches,” in 38th IEEE International\nConference on Data Engineering, ICDE 2022, Kuala Lumpur, Malaysia,\nMay 9-12, 2022 . IEEE, 2022, pp. 1794–1807. [Online]. Available:\nhttps://doi.org/10.1109/ICDE53745.2022.00180\n[43] J. M. Hellerstein, C. R ´e, F. Schoppmann, D. Z. Wang, E. Fratkin,\nA. Gorajek, K. S. Ng, C. Welton, X. Feng, K. Li, and A. Kumar,\n“The madlib analytics library or MAD skills, the SQL,” Proc. VLDB\nEndow. , vol. 5, no. 12, pp. 1700–1711, 2012. [Online]. Available:\nhttp://vldb.org/pvldb/vol5/p1700 joehellerstein vldb2012.pdf\n[44] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of\nword representations in vector space,” in 1st International Conference\non Learning Representations, ICLR 2013, Scottsdale, Arizona, USA,\nMay 2-4, 2013, Workshop Track Proceedings , Y . Bengio and Y . LeCun,\nEds., 2013. [Online]. Available: http://arxiv.org/abs/1301.3781\n[45] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching\nword vectors with subword information,” Trans. Assoc. Comput.\nLinguistics , vol. 5, pp. 135–146, 2017. [Online]. Available: https:\n//doi.org/10.1162/tacl a00051\n[46] B. Edizel, A. Piktus, P. Bojanowski, R. Ferreira, E. Grave, and\nF. Silvestri, “Misspelling oblivious word embeddings,” in Proceedings\nof the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,\n2019, Volume 1 (Long and Short Papers) , J. Burstein, C. Doran, and\nT. Solorio, Eds. Association for Computational Linguistics, 2019, pp.\n3226–3234. [Online]. Available: https://doi.org/10.18653/v1/n19-1326\n[47] Y . Qi, D. S. Sachan, M. Felix, S. Padmanabhan, and G. Neubig, “When\nand why are pre-trained word embeddings useful for neural machine\ntranslation?” in Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT, New Orleans, Louisiana,\nUSA, June 1-6, 2018, Volume 2 (Short Papers) , M. A. Walker, H. Ji,\nand A. Stent, Eds. Association for Computational Linguistics, 2018,\npp. 529–535. [Online]. Available: https://doi.org/10.18653/v1/n18-2084\n[48] E. F. Codd, “Further normalization of the data base relational model,”\nResearch Report / RJ / IBM / San Jose, California , vol. RJ909, 1971.\n[49] ——, The relational model for database management: version 2 . USA:\nAddison-Wesley Longman Publishing Co., Inc., 1990.\n[50] B. Hilprecht and C. Binnig, “Zero-shot cost models for out-of-the-box\nlearned cost prediction,” Proc. VLDB Endow. , vol. 15, no. 11,\npp. 2361–2374, 2022. [Online]. Available: https://www.vldb.org/pvldb/\nvol15/p2361-hilprecht.pdf\n[51] V . Sanca and A. Ailamaki, “E-scan: Consuming contextual data\nwith model plugins,” in Joint Proceedings of Workshops at the 49th\nInternational Conference on Very Large Data Bases (VLDB 2023),\nVancouver, Canada, August 28 - September 1, 2023 , ser. CEUR\nWorkshop Proceedings, R. Bordawekar, C. Cappiello, V . Efthymiou,\nL. Ehrlinger, V . Gadepally, S. Galhotra, S. Geisler, S. Groppe,\nL. Gruenwald, A. Y . Halevy, H. Harmouch, O. Hassanzadeh, I. F. Ilyas,\nE. Jim ´enez-Ruiz, S. Krishnan, T. Lahiri, G. Li, J. Lu, W. Mauerer,\nU. F. Minhas, F. Naumann, M. T. ¨Ozsu, E. K. Rezig, K. Srinivas,\nM. Stonebraker, S. R. Valluri, M. Vidal, H. Wang, J. Wang, Y . Wu,\nX. Xue, M. Za ¨ıt, and K. Zeng, Eds., vol. 3462. CEUR-WS.org, 2023.\n[Online]. Available: https://ceur-ws.org/V ol-3462/CDMS11.pdf\n[52] Y . A. Malkov and D. A. Yashunin, “Efficient and robust approximate\nnearest neighbor search using hierarchical navigable small world\ngraphs,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 42, no. 4, p.\n824–836, apr 2020. [Online]. Available: https://doi.org/10.1109/TPAMI.\n2018.2889473\n[53] K. B. Petersen and M. S. Pedersen, “The matrix cookbook,” nov\n2012, version 20121115. [Online]. Available: http://www2.compute.dtu.\ndk/pubdb/pubs/3274-full.html\n[54] D. He, S. C. Nakandala, D. Banda, R. Sen, K. Saur, K. Park,\nC. Curino, J. Camacho-Rodr ´ıguez, K. Karanasos, and M. Interlandi,\n“Query processing on tensor computation runtimes,” Proc. VLDB\nEndow. , vol. 15, no. 11, pp. 2811–2825, 2022. [Online]. Available:\nhttps://www.vldb.org/pvldb/vol15/p2811-he.pdf\n[55] V . Sanca and A. Ailamaki, “Post-moore’s law fusion: High-bandwidth\nmemory, accelerators, and native half-precision processing for cpu-local\nanalytics,” in Joint Proceedings of Workshops at the 49th International\nConference on Very Large Data Bases (VLDB 2023), Vancouver,\nCanada, August 28 - September 1, 2023 , ser. CEUR Workshop\nProceedings, R. Bordawekar, C. Cappiello, V . Efthymiou, L. Ehrlinger,\n14\n\nV . Gadepally, S. Galhotra, S. Geisler, S. Groppe, L. Gruenwald, A. Y .\nHalevy, H. Harmouch, O. Hassanzadeh, I. F. Ilyas, E. Jim ´enez-Ruiz,\nS. Krishnan, T. Lahiri, G. Li, J. Lu, W. Mauerer, U. F. Minhas,\nF. Naumann, M. T. ¨Ozsu, E. K. Rezig, K. Srinivas, M. Stonebraker,\nS. R. Valluri, M. Vidal, H. Wang, J. Wang, Y . Wu, X. Xue, M. Za ¨ıt, and\nK. Zeng, Eds., vol. 3462. CEUR-WS.org, 2023. [Online]. Available:\nhttps://ceur-ws.org/V ol-3462/ADMS1.pdf\n[56] K. Goto and R. A. van de Geijn, “Anatomy of high-performance matrix\nmultiplication,” ACM Trans. Math. Softw. , vol. 34, no. 3, pp. 12:1–12:25,\n2008. [Online]. Available: https://doi.org/10.1145/1356052.1356053\n[57] T. M. Smith, R. A. van de Geijn, M. Smelyanskiy, J. R. Hammond,\nand F. G. V . Zee, “Anatomy of high-performance many-threaded\nmatrix multiplication,” in 2014 IEEE 28th International Parallel and\nDistributed Processing Symposium, Phoenix, AZ, USA, May 19-23,\n2014 . IEEE Computer Society, 2014, pp. 1049–1059. [Online].\nAvailable: https://doi.org/10.1109/IPDPS.2014.110\n[58] “Intel® avx-512 - fp16 instruction set for intel® xeon®\nprocessor based products technology guide.” [Online].\nAvailable: https://networkbuilders.intel.com/solutionslibrary/\nintel-avx-512-fp16-instruction-set-for-intel-xeon-processor-based-products-technology-guide\n[59] N. Nassif, A. O. Munch, C. L. Molnar, G. Pasdast, S. V . Lyer, Z. Yang,\nO. Mendoza, M. Huddart, S. Venkataraman, S. Kandula et al. , “Sapphire\nrapids: The next-generation intel xeon scalable processor,” in 2022 IEEE\nInternational Solid-State Circuits Conference (ISSCC) , vol. 65. IEEE,\n2022, pp. 44–46.\n[60] “Wikidata.” [Online]. Available: https://www.wikidata.org/\n[61] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” in Proceedings of the 2018 International\nConference on Management of Data, SIGMOD Conference 2018,\nHouston, TX, USA, June 10-15, 2018 , G. Das, C. M. Jermaine, and\nP. A. Bernstein, Eds. ACM, 2018, pp. 489–504. [Online]. Available:\nhttps://doi.org/10.1145/3183713.3196909\n[62] G. Cloud, “Introduction to bigquery ml,” https://cloud.google.com/\nbigquery/docs/bqml-introduction, 2023, accessed: 2024-02-20.\n[63] Microsoft, “Azure machine learning - ml as a service,” https://azure.\nmicrosoft.com/en-us/products/machine-learning, 2024, accessed: 2024-\n02-20.\n[64] G. Cloud, “User-defined functions in bigquery,” https://cloud.google.\ncom/bigquery/docs/user-defined-functions, 2024, accessed: 2024-03-01.\n[65] H. Zhang and Q. Zhang, “Minjoin: Efficient edit similarity joins\nvia local hash minima,” in Proceedings of the 25th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining ,\nser. KDD ’19. New York, NY , USA: Association for Computing\nMachinery, 2019, p. 1093–1103. [Online]. Available: https://doi.org/10.\n1145/3292500.3330853\n15",
  "textLength": 85911
}