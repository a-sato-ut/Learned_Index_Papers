{
  "paperId": "1ad0501d9f5849c3d5cc8c16195232d809025a36",
  "title": "TENSILE: A Tensor Granularity Dynamic GPU Memory Scheduling Method Toward Multiple Dynamic Workloads System",
  "pdfPath": "1ad0501d9f5849c3d5cc8c16195232d809025a36.pdf",
  "text": "JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nThis work has been submitted to the IEEE for possible\npublication. Copyright may be transferred without notice, after\nwhich this version may no longer be accessible.arXiv:2105.13336v5  [cs.DC]  6 Nov 2022\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\nTENSILE: A Tensor granularity dynamic GPU\nmemory scheduling method toward multiple\ndynamic workloads system\nKaixin Zhang, Hongzhi Wang, Han Hu, Songling Zou, Jiye Qiu, Tongxin Li, Zhishun Wang\nAbstract —Recently, deep learning has been an area of intense\nresearch. However, as a kind of computing-intensive task, deep\nlearning highly relies on the scale of GPU memory, which is\nusually prohibitive and scarce. Although some extensive works\nhave been proposed for dynamic GPU memory management, they\nare hard to apply to systems with multiple dynamic workloads,\nsuch as in-database machine learning systems.\nIn this paper, we demonstrated TENSILE, a method of\nmanaging GPU memory in tensor granularity to reduce the\nGPU memory peak, considering the multiple dynamic work-\nloads. TENSILE tackled the cold-starting and across-iteration\nscheduling problem existing in previous works. We implemented\nTENSILE on a deep learning framework built by ourselves and\nevaluated its performance. The experiment results show that\nTENSILE can save more GPU memory with less extra overhead\nthan prior works in single and multiple dynamic workloads\nscenarios.\nIndex Terms —GPU memory schedule, DB4AI\nI. I NTRODUCTION\nDEEP learning is widely used in many areas of data\nanalysis. With appropriate training, deep learning models\ncan achieve much higher performance than traditional ones [1].\nHowever, a signiﬁcant problem of deep learning is that it is\nusually costly for GPU memory, especially in the communities\nof image recognition and natural language processing, whose\nmodels like ResNet [2], BERT [3], and GPT-3 [4] have\nas many as billions of parameters. These parameters and\nthe corresponding feature maps cause massive GPU memory\nfootprint [5], which impedes their application. For example,\nsuch deep neural networks(DNN) could hardly be performed\nin a database since building a large GPU cluster is prohibitive\nfor the existing database systems. Thus, the increasing size of\ndeep learning models acquires that the GPU memory must be\nmanaged more efﬁciently.\nAdditionally, many scenarios of deep learning have multiple\ndynamic workloads, such as cloud database services [6],\nin-database machine learning [7], [8], cloud deep learning\nservices [9]–[11], and AI-driven DBMSes [12]–[14]. Although\nthe memory occupied by a single load is limited, the total\namount of running jobs is huge, so there needs to be enough\nGPU memory for each job. For example, prior work [11]\nmentioned that there are situations in that jobs cannot be\nco-located due to the GPU memory constraints in cluster\nscheduling problems. To tackle such problems under multiple\nHongzhi Wang is corresponding author.dynamic workload scenarios, we propose TENSILE to reduce\nthe total GPU memory peak.\nThe critical problem is how to reduce the GPU memory\npeak of each and entire computation process efﬁciently and up-\ndate the scheduling plan in time to ﬁt the dynamic workloads.\nAlthough there are already some efﬁcient methods [15]–[17] to\nreduce the GPU memory cost by swapping and recomputation,\nthe following three issues are still unsolved.\n\u000fMultiple dynamic workloads. To our best knowledge,\nmost existing approaches are designed to schedule GPU\nmemory for a single workload. When it comes to scenar-\nios with multiple dynamic workloads, these methods can\nhardly manage the GPU memory as well as they were\non a single job. Scheduling GPU memory with multiple\ndynamic workloads is challenging since the workloads\nare executed asynchronously, and such a mechanism will\ncause dynamic tensor access patterns. The core challenge\nis to update the scheduling plan with its ﬂuctuation. Also,\nchoosing the proper tensors that need to be scheduled is\ncritical to scheduling efﬁciency. Furthermore, the chal-\nlenge involves the design of the system architecture to\nsupport such kinds of scheduling algorithms.\n\u000fCold-starting. The most advanced method, such as Ca-\npuchin [15] which schedules in tensor granularity, needs\nto observe the tensor access pattern ﬁrst before generating\nthe scheduling plan. As shown in Fig. 1(b), such ’Passive\nMode’ will cause extra overhead by passively swapping\ntensors when the GPU memory is insufﬁcient. Also, the\nmultiple workload scenario causes the observation to be\ninaccurate. So the core challenge is to ﬁnd a proper way\nto initialize the tensor access pattern.\n\u000fAcross-iteration Scheduling. All prior methods only\nschedule within a single iteration. They ignore that\nscheduling across iterations is also necessary since some\ntensors evicted in the current iteration’s Opt phase are\nused as inputs in the next iterations F/B phase, as shown\nin Fig. 1, which leads to extra overhead. The core chal-\nlenge is to consider the swapping between two phases.\nIn this paper, we tackle these challenges and propose TEN-\nSILE, a run-time dynamic GPU memory scheduling method\nof tensor granularity designed to reduce the GPU memory\nfootprint peak towards multiple dynamic workloads.\nFor the ﬁrst problem of multi-workloads, we develop a\nscheduling method that can keep tracking the tensor access la-\ntency for updating the scheduling plan to support the multiple\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\nFig. 1. Tensor Access and Memory peak, weighti\n1means the weight 1’s value in the ithiteration, and the update TUA means the operator of updating the\nweights. (a) is the situation without scheduling; (b) is the scheduling that cannot across iterations, such scheduling will cause extra time overhead. It also\nshows the cold-starting problem of Capuchin; (c) is our method’s effect of reducing GPU memory peak\ndynamic workloads. By updating the scheduling plan in time,\nthe dynamic tensor access patterns caused by multi-workloads\nscenarios can be well adapted.\nTo solve the second problem, we use a light neural network\nmodel to predict the latency of each operator under current\nGPU usage. With this model, TENSILE can avoid the extra\noverhead caused by the ’Passive Mode’.\nAnd for the third problem, our method could schedule ten-\nsors across iterations. TENSILE can swap out the parameters\nat the Opt phase and prefetch them before they are used in the\nfollowing computation iteration. As the schematic Fig. 1(b, c)\nshows, such an approach can effectively reduce the total GPU\nmemory peak with less extra overhead of passively swapping\nand supporting multiple dynamic workloads.\nTENSILE is a tensor granularity scheduling approach for\nsupporting deep learning tasks based on tensor computation,\nsuch as the training of CNN [18] and GCN [19]. Additionally,\nwe can achieve higher performance with scheduling in tensor\ngranularity since tensor is the most fundamental component\nof deep learning algorithms. Also, beneﬁting from the tensor\ngranularity, TENSILE is not limited to the scope of application\nwith forward/backward propagation like most prior works do.\nIt can also handle any tensor computation tasks expressed with\ntensor operators in a static compute graph.\nOur contributions can be concluded as:\n\u000fInspired by existing GPU memory scheduling methods,\nwe proposed a scheduling method toward multiple dy-\nnamic workloads. To support such scheduling, we de-\nsigned a system to track the jobs’ running and manage\ntheir memory as a whole, rather than optimizing them\nseparately as previous works did. This method can also\nschedule the tensors across iterations and reduce the en-\ntire process’s GPU memory peak to increase the system’s\nmodel capacity.\n\u000fWe also proposed an innovative algorithm to generate\nthe swapping and recomputation plan based on tensoraccess sequence, which can support the multiple dynamic\nworkloads better than prior works. With this algorithm,\nwe could initialize the scheduling plan without measuring\nﬁrst and update the scheduling plan in time with the\nﬂuctuation of GPU usage.\n\u000fAccording to the experiment results, TENSILE can\nachieve a higher GPU memory saving rate with less per-\nformance loss, especially in multiple dynamic workload\nscenarios.\nII. R ELATED WORK\nIn this section, we introduce some prior works in GPU\nmemory management.\nNVIDIA is the ﬁrst organization to pay attention to the\nGPU management problem. It integrated the Uniﬁed Memory\ntechnology in CUDA61. This work allows programmers to\nuse the GPU and Host Memory as a uniﬁed space. Unlike\nTENSILE, this work is not specially designed to schedule\ndeep learning processes, which implies programmers need to\ndecide when and which tensor should be swapped with CUDA\nprogramming.\nIn the area of GPU memory scheduling for deep learning,\nMinsoo Rhu et al. proposed the vDNN [16], which is the\nﬁrst work focusing on scheduling the GPU memory footprint\nfor deep learning processes. The central part of vDNN is an\nalgorithm to decide which layer’s interim result should be\nreleased, evicted, or prefetched. Although it is a pioneering\nwork, some problems remain, such as delayed computation\nstart, high pinned memory requirements, and GPU memory\nfragmentation. Shriram S.B. et al. improved vDNN and solved\nthe three problems mentioned above [20]. By allowing to\noverlap of the tensor eviction and prefetching process with\nthe computation process, Shriram S.B. et al. signiﬁcantly\nimproved the vDNN’s performance. Linnan Wang et al. pro-\nposed SuperNeurons [17] which is the ﬁrst dynamic GPU\n1https://developer.nvidia.com/blog/uniﬁed-memory-in-cuda-6/\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\nmemory run-time management approach. It also introduced\nrecomputation into the scheduling. The authors believe the\nconvolution layer is the most valuable layer to be swapped and\ndesigned some approaches to schedule the convolution layers.\nAlthough this design helps it to achieve signiﬁcant efﬁciency\non convolutional networks, it also limits the compatibility\nfor more advanced network structures as transformer [21].\nDonglin Yang et al. proposed the ﬁrst method that supports a\nuniﬁed memory pool for non-linear networks [22]. The method\ncan construct the execution order for non-linear networks\nbased on graph analysis so that it could support non-linear\nnetworks better than previous works.\nAlthough all the methods above have signiﬁcant advantages,\nthey have a major shortcoming compared to TENSILE: all\nof them are designed to schedule GPU memory in layer\ngranularity. Such a mechanism makes them lack compatibility\nand unable to save GPU memory as much as tensor granularity\nmethods like TENSILE. This is the fundamental reason why\nthey can not schedule the newest deep learning models that\ncontain complex structures inside a ’layer’ (or as known as\n’block’), such as transformer-based networks [21] and the\nCapsule Network [23]. And the tensors inside the ’layer’ can\nnot be scheduled by these methods, and neither do the interim\ntensors in the optimizer, such as the 1stmoment vector and\nthe2ndmoment vector in the Adam optimizer [24]. These\nworks aim to improve the batch size in a single workload\nscenario, so only scheduling the tensors in the F/B phase is\nenough. However, we aim to reduce the memory footprint peak\nto make the system support more job training simultaneously\nwhen it comes to multiple workload scenarios. Moreover, such\nmethods are not good enough since the GPU memory peak\nwill be not only appear in the F/B phase but also in the Opt\nphase.\nIn 2019, another method was proposed [25], the ﬁrst ap-\nproach that can schedule GPU memory in tensor granularity.\nThis method proposed two orthogonal approaches to reduce\nthe memory cost from the system perspective. It reduces\nthe GPU memory peak by managing the memory pool and\nscheduling the swapping with a heuristic method. Beneﬁting\nfrom its tensor granularity, this method can support any deep\nlearning network. Xuan Peng et al. proposed Capuchin [15].\nCapuchin can observe the tensor access pattern and decide\nwhich tensor to be swapped and which to be recomputed\nwith a value-based approach. This method is more ﬂexible in\nsupporting various types of neural networks than those layer\ngranularity methods. However, it highly depends on observing\nthe tensor access pattern when the calculation is running for\nthe ﬁrst time since the tensor access pattern is needed to\ngenerate the scheduling plan. Such observation leads to passive\nswapping and PCIe channel occupancy when the GPU memory\nis insufﬁcient.\nThese two advanced works schedule GPU memory in tensor\ngranularity and solve compatibility problems. However, com-\npared to TENSILE, neither of these prior works can solve the\nthree problems we propose in Section I. We take Capuchin as\nan example to explain it.\nIn an environment with multiple workloads, inﬂuenced\nby multiple workloads, the GPU usage ﬂuctuates irregularly,\nFig. 2. Overhead Analysis of Capuchin. The ’ﬁrst iteration cost’ corresponds\nto the ’Passive Mode’ of Capuchin, and the ’wait for tensors ready’ corre-\nsponds to the effectiveness of swapping in. All the items except ’ﬁrst iteration\ncost’ are measured after the ﬁrst iteration.\nwhich causes the operators’ latency and tensor access pattern\nto keep changing, so the scheduling plan must be able to be\nkept updating iteratively. Capuchin makes a ﬁxed scheduling\nplan once at the ﬁrst epoch based on the observed tensor access\npattern in the ’Passive Mode’, which is not feasible because\nthe latency of the operator varies with the dynamic load of\nmultiple jobs. Especially when a job launches, the GPU usage\nwill change drastically and cause the initial scheduling plan\nto lose efﬁcacy. Capuchin has not enough ability to adjust\nthe plan to ﬁt the ﬂuctuation of the tensor access pattern in\nmultiple dynamic workload scenarios. This can be proved by\nFig. 2, the time cost of ’wait for tensors ready’ increases with\nthe number of workloads.\nThe ’Passive Mode’ also causes the cold-starting discussed\nproblem in section I. According to the ’ﬁrst iteration cost’\nof Fig. 2, this unexpected eviction causes the time cost of\nthe ﬁrst iteration increases since passive swap-out causes the\ncomputation to block.\nAlso, Capuchin and other works above only considered the\ntensor access in a single iteration. For example, the updated\nparameters’ accesses at the end of the iteration can not be\nswapped proactively. Thus, these parameters can only be\nswapped passively at the next iteration with an additional time\noverhead. As shown in Fig. 2, the passive eviction takes 32.1%\nof the overall time cost. Most of these evictions are caused by\nthe memory peak caused by the unexpected swap-in across\niterations.\nIII. O VERVIEW\nThis section deﬁnes the GPU memory scheduling problem\nand overviews our deep learning system built to support\nTENSILE.\nA. Preliminary\nFor the convenience of scheduling, we use a graph model\ncalled static compute graph [26] to describe the tensor calcula-\ntion jobs. Let Vbe the set of all basic operators to manipulate\ntensors,Ebe a set of tensors generated by operators in V, and\nSbe a set of scheduling events, whose elements are fSwap-\nout, Swap-in, Recompute, Release g. With these symbols, we\ndeﬁne the multiple dynamic workload scheduling problem.\nJob: A deep learning job jis denoted by a static compute\ngraph asGj(V;E). Note that the graph is a DAG (Direct\nAcyclic Graph) just like the one in TensorFlow [26], and the\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\noperators of the optimizer are also included in Gj(V;E). An\nexample of the graph is shown in Fig. 3(a), a convolution\noperator is represented as a node in the graph. Such a node\ntakes multiple tensors as the input and outputs other tensors.\nSuch a model could effectively represent the computation in\nmodern deep learning frameworks, such as TensorFlow [26].\nTensor Access: For a tensor tinE, which is generated\nby the operator \u0016and used by the operator \u001d, we deﬁne the\nexecution of \u0016as the Tensor Generating Access(TGA) of\ntand the execution of \u001dis the Tensor Using Access(TUA)\noft. Both tensor accesses are denoted as ai\nj, whereiis the\naccess index. Usually, if a given operator \u0016is not at the start\nor the end of a job, it corresponds to two kinds of tensor\naccesses, the TUA and the TGA . For example, as shown in\nFig. 3(b), the convolution operator takes tensors kernel, bias,\nandxgenerated by the previous operator as inputs and outputs\nthe feature map tensor y. The latency of a Tensor Access is\ndeﬁned as the corresponding operator’s execution time cost,\nwhich can be inferred from the compute graph or logged\nat run-time. The TUA is access of the input tensors of \u0016,\nand the TGA represents the accesses of the output tensors\nof\u0016. Additionally, since the operator placeholder generates\nthe input tensors of the whole compute graph, we regard the\nplaceholder as a TGA .\nWorkload: A workload Wjis denoted by a tensor access\nsequence<a1\nj,a2\nj;:::> , which is ordered by the topological\norder of job Gj(V;E). The topological order corresponds to\nthe execution order of operators in V. When a workload Wj\nis launched, the framework executes the operators of Wjin\ntopological order. A dynamic workload has tensor accesses\nwhose latency keeps changing at run time.\nScheduling Plan: We denote a scheduling plan for workload\nWjas a sequence Sjofsi;t\nj, such as< si0;t0\nj;si1;t1\nj;::: > .\nAnd for each event si;t\nj,jrepresents that it belongs to the\nscheduling plan of workload Wj,iis its trigger tensor access,\nwhich is the prior tensor access of the scheduled event, and t\nis the time interval between the execution end of the trigger\ntensor access and the start of the scheduling event, just as\nFig. 3(c) shows\nMemory Footprint: Let function Mem (\u0001)be the GPU\nmemory changing caused by tensor access aor scheduling\neventx, the memory footprint of a workload Wjat the end\nof operator k is\nMFk\nj=kX\ni=0Mem (ai\nj) +Mem (xi;t\nj) (1)\n, and the memory footprint peak of workload Wj(withm\noperators) is\nMPj= max\n0\u0014k\u0014mMFk\nj= max\n0\u0014k\u0014mkX\ni=0Mem (ai\nj) +Mem (xi;t\nj)\n(2)\nThe summation process in Equation 1 can be regarded as\na discrete GPU memory occupancy change process as the\nworkload’s tensor access sequence is executed. Assuming\nthere arenworkloads running asynchronously in the system,\ntheglobal memory footprint peak isMP =Pn\nj=0MPj.B. Problem Deﬁnition\nWith these symbols above, we deﬁne the problem here. Let\nMbe the GPU memory size, our multiple dynamic work-\nloads scheduling problem is how to generate a scheduling\nplanXjfor each workload Wjto minimize the global memory\nfootprint peak MP with as little extra overhead. So that the\nsystem with GPU memory Mcan run more neural networks\nsimultaneously or run these given workloads with larger batch\nsizes for higher efﬁciency. The optimization goal is formalized\nas Equation 3 shows.\nJ(X) = arg min\nfX0;:::X jgnX\nj=0max\n0\u0014i\u0014mkX\ni=0(Mem (ai\nj) +Mem (xi;t\nj))\n(3)\nNote that the interim results and the operators of the\noptimizer are also included in G, and can be scheduled like\nother tensors, which is different from prior works. Those\nworks are only designed to save GPU memory during the\nforward/backward propagation to achieve a larger batch size\nor deeper network. However, our method is designed toward\nmultiple dynamic workloads in the system. The method must\nreduce the entire job’s GPU memory peak to support multiple\nworkloads, including the peak caused by the optimizer. Oth-\nerwise, when two peaks caused by different jobs coincide, an\nOOM (Out Of Memory) exception will be triggered. Although\nthe exception can be handled by passive swapping just like the\nCapuchin [15] does, it will cause much extra overhead. This\nfeature is practical, especially when using an optimizer like\nAdam [24], which will use additional memory twice the size\nof the parameters.\nFig. 3. Demonstration of the Concepts, (a) is a simple compute graph of CNN;\n(b) is a tensor access sequence corresponding to (a); (c) is a scheduling plan\nof (b).\nC. Concepts Explanation\nTo facilitate understanding, we explain the basic concepts\nof TENSILE.\nTheTensor Access Sequence is the alias of workload. For\neach tensor, its Tensor Access Sequence begins with an TGA\nwhich generates the tensor and followed by a series of TUA\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\nwhich represents the tensor used by operators as input, as\nshown in Fig. 1.\nASwap Event transfers data between the GPU and the Host\nMemory. It has two kinds of types, Swap-Out Event andSwap-\nIn Event . As Fig. 3(c) shows, Swap-Out Event of a tensor will\ncopy the tensor from the GPU memory to the Host Memory\nand then releases it from the GPU memory as soon as it is\nnot occupied, which can be regarded as eviction. A Swap-In\nEvent will copy a tensor from the Host Memory to the GPU\nmemory, which is used to prefetch the tensor. It is necessary to\nemphasize that swap-in does not mean the tensor in the Host\nMemory will be released. On the contrary, it will remain in\nthe Host Memory until the last access of the tensor is ﬁnished,\nwhich could save much cost of swap-out.\nARecomputation Event regenerates the tensors which have\nalready been released from the GPU memory. For a Tensor\nAccessTi, we can release it after accessing and recompute it\nbefore the Tensor Access Tjthat takes the tensor ias input.\nARelease Event releases the corresponding tensor from the\nGPU memory as soon as it has not been used.\nWith these concepts, we can ofﬂoad tensors when not\nused and then prefetch or regenerate them by swapping or\nrecomputing them before their TUA.\nD. System Architecture\nSince existing approaches cannot schedule among multi-\nple dynamic workloads, we developed a system to support\nscheduling under such scenarios. The system can collect\nthe necessary information on all the jobs before and during\nrunning to support the scheduling algorithm. The information\nincludes the compute graph of jobs, the latency of each\noperator, and other run-time information such as the GPU\nutilization. The system will trigger the scheduling algorithm\nmultiple times during startup and running. When receiving\nthe newest scheduling plan, the system will apply it at the\nnext round of iteration. To achieve the functions above, our\nsystem contains four components: Global Controller, Memory\nScheduler, Executor, and Swap Executor , as shown in Fig. 4.\nGlobal Controller. The Global Controller is responsible\nfor launching new jobs’ processes and delivering information\nbetween the Executor and the Memory Scheduler . When the\nuser launches a new job, the Global Controller creates a new\nsub-process of the job’s Executor , and the threads for Swap\nExecutor . It is also responsible for communicating with the\nMemory Scheduler , and each job’s information is organized\nand sent to the Memory Scheduler . Also, the scheduling plan\nof each job is distributed to the corresponding Executor . With\nthe global controller, our system can support the scheduling\nalgorithm described in section IV .\nMemory Scheduler. TheMemory Scheduler takes the jobs’\ninformation as inputs, such as the compute graph and the\noperators’ latency tracked by the Executor . Such information is\nused to generate the Tensor Access Sequence and the schedul-\ning plan, which consists of multiple swapping, releasing, and\nrecomputation instructions. The detailed algorithm will be\ndescribed in section IV . Each instruction is described by a\ntuple (trigger; \u0001time), thetrigger is the previous operator’s\nFig. 4. System Architecture, after a job is launched, Executor uses the Latency\nPredictor to predict the operators’ latency and send the information to Memory\nScheduler. The Memory scheduler collects all jobs’ information and generates\na scheduling plan. After receiving the plan from Global Controller, Executor\ncall the Swap Executor to execute the plan.\nend time, and the \u0001time is the time interval after the trigger.\nTherefore, the corresponding Swap Event will be triggered in\n\u0001time after thetrigger , just as the Fig. 3(c) shows. These\ninstructions are grouped by job id and will be distributed to\nthe corresponding Executor by the Global Controller .\nSwap Executor . As Fig. 5 shows, the Swap Executor\ncontains an Execution Queue and an Execution Buffer. The\nExecution Queue is a thread with an event queue in it.\nIt receives the swap events triggered by a certain operator\n(trigger tensor) and pushes these swap events into the queue\nin chronological order. The Execution Queue pops up the swap\nevents to the Execution Buffer for implementing swapping\naccording to their time intervals.\nFig. 5. Swap Executor Architecture, Execution Queue tracks the delta time\nof Swap Event and call the Execution Buffer to swap tensors.\nExecutor. The Executor executes each operator node in\nthe graph by topological order. For each node, it checks\nwhether all input tensors are in the GPU memory before\nthe computation since the inaccuracy of the estimated Tensor\nAccess Sequence may lead to delays in swapping. If that\nhappens, the Executor will pause the computation and swap\nthe tensors into the GPU memory. When the computation\nis ﬁnished, certain tensors will be released if the Memory\nScheduler has set the operator as the release trigger for\nthese tensors. Before releasing, the Executor checks the Swap\nExecutor . The release and computation processes will be\nblocked if some Swap-Out Events have not been executed. By\nsuch a synchronous method, the execution order of swapping,\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\nreleasing, and computation can be kept. Otherwise, if a given\ntensor’s Swap-Out Event is executed later than its release, then\nthe data of the tensor will be lost and would cause exceptions.\nThe Executor also tracks every operator’s latency period-\nically and sends the information to the Memory Scheduler\nthrough the Global Controller for updating the scheduling\nplan. The Executor will check whether the scheduling plan\nhas been updated each time before a new round of calculation.\nDuring the execution of the compute graph, if the scheduling\nplan requires the current tensor access to trigger some Swap\nEvents , the Executor will send the Swap Event to the Execution\nQueue of the Swap Executor waiting for execution. At the end\nof an iteration, the Executor pauses to wait for the unﬁnished\nswap events of this iteration.\nThe system can collect information from each job and exe-\ncute the corresponding scheduling plan with these components.\nThe whole scheduling procedure has four steps:\n1) Collecting the new jobs information through the Global\nController .\n2) The Memory Scheduler generates the initial scheduling\nplan and distributes it to the corresponding job’s Executor\nthrough the Global Controller .\n3) The Swap Executor executes the scheduling plan during\ncomputing. Also, the Executor performs the recomputa-\ntion events at running.\n4) The Executor collects the time cost of each operator\nand reports to the Global Controller . When the current\noperators’ latency and the latency used to generate the\nprevious scheduling plan deviate more than the preset\nthreshold, the Global Controller will call the Memory\nScheduler to update the scheduling plan based on the\nnewest estimated Tensor Access Sequence .\nIV. M ETHODS\nIn this section, we detail our algorithms of TENSILE in\nthe order of actual execution, i.e., Tensor Access Sequence\nGeneration (subsection IV-A), GPU Memory Peak Analy-\nsis(subsection IV-B), Swap Event Schedulingsubsection IV-C),\nRecomputation Scheduling(subsection IV-D), and Scheduling\nPlan Updating(subsection IV-E).\nA. Tensor Access Sequence Generation\nWe ﬁrst introduce our tensor access sequence generation\nmethod since our method is based on GPU memory peak\nanalysis, which depends on tensor access sequence generation.\nWe use a learn-based latency prediction method to solve the\ncold-starting problem mentioned in Section I. In consideration\nthat there are two pieces of information required to infer the\nTensor Access Sequence , which are the Tensor Accesses’ order\nand the latency of each Tensor Access . We can deal with\nthis problem by diving into two sub-problems of inferring the\nTensor Access order and getting each Tensor Access’s latency.\nAs mentioned above, we execute the operators in the\ntopological order with the compute graph. Therefore, we can\nsolve the ﬁrst sub-problem and get the Tensor Accesses’ order\nstatically based on each operator’s inputs and outputs. It onlysolves the ﬁrst sub-problem, which is inferring the access\norder. The execution order is unrelated to the latency.\nHowever, accurately predicting the CUDA kernel’s latency\nis a challenging problem, especially with multiple dynamic\nworkloads.\nThe most straightforward way to solve the second sub-\nproblem is to run the job with ’Passive Mode’. However, it\nwill cause extra overhead since calculation and swapping can\nnot overlap, and the computation must wait for swapping. To\nmake the initial scheduling plan more efﬁcient, our solution\nuses pre-trained machine learning models to predict each GPU\noperator’s latency with the input data size and the current GPU\nutilization rates. When the system initializes, it will measure\neach GPU operator’s latency with different input data and GPU\nusage to generate the training data.\nSince the latency of a given operator depends on its\ninputs’ shape, the parameters such as strides of con-\nvolution operator, and the current GPU usage, the in-\nputs of our prediction model are denoted as inputs\u0011<\ndim0\nx1;:::;dimm\nx1;:::;dimk\nxn;para 1;:::;para s;gpuusage > ,\nwhere thex1toxndenotes the input tensors of the operator,\nthedimi\nxjdenotes the i-th dimension’s size of input xj, the\npara idenotes the i-th parameter of the operator, and the\ngpuusage denotes the current GPU’s utilization rate. We\nused a four layers MLP network as the prediction model\nsince the input is not complex, and a simple MLP model is\nenough to give a relatively precise prediction. According to our\nexperiment, the average MSE of every operator’s prediction\nresult is 4.269, which means that our method can give a\nrelatively precise prediction to most operators. A detailed\nevaluation of this method’s inﬂuence on the scheduling process\nwill be introduced in subsection V-B.\nAlthough the GPU’s utilization rate is a general representa-\ntion of the GPU usage condition, and the prediction results\nwill not be precisely accurate, we can keep tracking the\noperators’ latency and dynamically correct the predicted result\nwith the exponential moving weighted average method [27]\nduring the job’s run-time. The details will be introduced in\nsubsection IV-E.\nB. GPU Memory Peak Analysis\nTo reduce the entire system’s GPU memory peak, the\nscheduling algorithm has to know when the GPU memory\npeak appears and caused by which tensors so that it can try\nto schedule the Swap Event of these tensors. Thus, with the\ninformation inferred from the compute graph, we develop an\nalgorithm to analyze the GPU memory peak statistically.\nThe algorithm is a discrete simulation process of the\nEquation 2. According to the Equation 2, the GPU memory\nfootprint is changed only when these ﬁve situations happen.\n1)Iteration Beginning : At the beginning of a training itera-\ntion, these tensors will be in the GPU memory:\n\u000fInput data of the model\n\u000fParameters which are not swapped out from the last\niteration\n\u000fInitial space used by CUDA, cuDNN, and cuBLAS.\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\nThese tensors will be used to initialize the GPU memory\nfootprint.\n2)Tensor Generating Access : When a tensor is generated\nin the GPU memory via TGA , GPU memory footprint\nincreases. Note that when updating a parameter, we\nlogically treat the updated parameter as a new tensor,\nbut the new parameter will not cause an increase in GPU\nmemory footprint since it uses the memory address of the\nold parameter.\n3)Swap-In Event : ASwap-In Event causes GPU memory\nconsumption increase. Since we only concern with the\npeak of GPU memory footprint, the Swap-In Event’s\nﬁnishing time point can be regarded as the time point\nof the GPU memory footprint increases.\n4)Swap-Out Event : When a tensor is copied to Host Mem-\nory and released, the GPU memory footprint is reduced\nat the end of the swap-out event (or at the end of the\ncorresponding TUA , if theTUA ends later).\n5)Tensor Release : For a Tensor Access iwhich needs to\nrecompute or swap in, when the last Tensor Access i-1\nﬁnished, the tensor can be released from GPU memory,\nso that the GPU memory footprint will decrease.\nBased on the above discussions, we just need to sort the\nTensor Accesses and Swap Events by the time they trigger\nthe GPU memory footprint change. Then we can traverse\nthe job’s timeline to determine the GPU memory peak. We\nthen introduce the analysis algorithm in detail as shown in\nalgorithm 1.\nC. Swap Event Scheduling\nThe primary approach in our system to reduce the GPU\nmemory peak is by swapping tensors out to the Host Memory\nwhen they are not in use. It is obvious that swapping every\ntensor out of the GPU memory after their TGA , and swapping\nthem back right before their TUA can save the most GPU\nmemory. However, due to the data transfer occupying the PCIe\nchannels exclusively [15], there can only be one tensor being\nswapped simultaneously. Therefore, swapping tensors within\nand between tasks must be scheduled appropriately.\nThe scheduling process is a deformation of the task schedul-\ning problem with the next two difﬁculties:\n\u000fASwap Event cannot be executed without prerequisites.\nFor example, a Swap-In Event can be executed if the\ncorresponding Swap-Out Event can be performed. Oth-\nerwise, no data will be swapped into the GPU memory.\nAlso, to avoid interrupting task execution, the Swap-Out\nEvent can be executed only when the tensor’s Swap-\nIn Event can be scheduled successfully before the next\nTensor Access (we do not allow passively swapping in\nfor performance reasons).\n\u000fThe scheduling process must be performed considering\nglobal jobs’ information, and it needs to be updated\nfrequently due to the dynamic workloads. This fact deter-\nmines that we can not use a very complex algorithm to\ngenerate the scheduling plan. Otherwise, the generation\nof the scheduling plan will cause signiﬁcantly extra\noverheads and makes the scheduling plan can not be\nupdated in time under the ﬂuctuation of GPU utilization.Algorithm 1: GPU Memory Peak Analysis\nInput: TAS : tensor access sequence; SE: swap events\nOutput: MP : memory peak; MPTensor : tensors caused memory\npeak; LUA : last input access for each job before memory\npeak; MPTime : time of the memory peak\n1initialize the memory used and the ingputensors with the\ntensors which will be updated in the optimizing phase and the\ninputs/outputs of the deep learning model.;\n2foreachevent intime axis do\n3 time \u0000event:time ;\n4 Release the tensor in tensors toberelease if the present\ntime is after its end time;\n5 ifevent:type =output access andevent:tensor is not\nused to initialize the memory used and the\ningputensors then\n6 memory used \u0000memory used +event:size ;\n7 ingputensors:add (event:tensor );\n8 else if event:type =input access then\n9 ifevent:release =True then\n10 tensors tobereleased:append (event );\n11 else\n12 lastinput access \u0000event ;\n13 else if event:type =swap outorevent:type =swap in\nthen\n14 lastaccess \u0000getlastaccess (event; time axis);\n15 event:execute ref \u0000lastaccess ;\n16 event:execute time \u0000lastaccess:time ;\n17 ifevent:type =swap inthen\n18 memory used \u0000memory used +event:size ;\n19 ingputensors:add (event:tensor );\n20 else\n21 memory used \u0000memory used\u0000event:size ;\n22 ingputensors:remove (event:tensor );\n23 ifmemory used > memory peak then\n24 memory peak \u0000memory used ;\n25 MPTensor \u0000copy(ingputensors );\n26 LUA \u0000lastinput access ;\n27 MPTime \u0000time ;\n28return memory peak ,MPTensor ,LUA ,MPTime ;\nFacing the difﬁculties above, we design a greedy algorithm to\nbalance the performance and the time overhead. The main idea\nis to swap the largest available tensor whose access will cause\nthe GPU memory footprint peak. Since the scheduling for each\ntensor can change the GPU memory peak, we must analyze the\nGPU memory peak and generate the swap events iteratively. In\nparticular, the GPU memory peak will be analyzed ﬁrst based\non the Tensor Access Sequence , and choosing the largest tensor\nthat caused the peak to swap. This process is iterated several\ntimes until there are no tensors to be scheduled.\nWith such a greedy algorithm, each scheduled tensor can\nsigniﬁcantly reduce the GPU memory peak. Now we will\ndetail our algorithm.\nWe ﬁrstly choose the biggest MPT among all jobs as the\nmost valuable tensor to swap (candidate tensor). Note that each\njob has a max swapping rate limit. Considering the swapping\namong all jobs, the PCIe channel will be jammed if we swap\nthe tensors in each job as many as possible. Also, we can not\ncontrol the order of swap events among all jobs because jobs\nin our system are running asynchronously for performance\nreasons, and each job’s time cost of one training step is\nsigniﬁcantly different. If we use a synchronous way to execute\nthese jobs, then there must be extra synchronization overheads.\nFrom this perspective, to reduce the conﬂict opportunity, we\ndeﬁne the max swapping rate limit for each job’s tensor\nswapping as the rate of tensors swapped in a job to tensors\nswapped in the whole system. It can be regarded as a hyper-\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\nAlgorithm 2: Swap Scheduling Algorithm\nInput: TAS : tensor access sequence; SE: swapped events; MPT :\nthe tensors that cause the memory peak; MSR : the max\nswapping rate limit for each job, SON : swapped out tensors\nnumber for each job; TAT :TAS grouped by tensor and\nsorted by time; all the outputs of algorithm 1\nResult: Add new Swap Events toSE\n1Function scheduling swap( SE: list, tensor ):\n2 succeed ,have first access \u0000False ;\n3 Generate swap outevent andfeasible intervals ;\n4 foreachintervals infeasible intervals do\n5 ifintervals covers swap outevent then\n6 SE:add (swap outevent );\n7 succeed swap out \u0000True ;\n8 iftensor is an updated parameter then\n9 first access \u0000 the ﬁrst TUA of the\ncorresponding non-updated parameter;\n10 else\n11 first access \u0000 the ﬁrst TUA afteraccess ;\n12 have first access \u0000ﬁrst access ! =None ;\n13 Generate swap inevent from the first access ;\n14 iftryschedule swap in(first access )success\nthen\n15 SE:add (swap inevent );\n16 succeed \u0000True ;\n17 else\n18 SE:remove (swap outevent );\n19 swap outevent:latest time \u0000\nfirst access:end time ;\n20 break ;\n21 return succeed ,succeed swap out,have first access ;\n22foreachtensor inMPT do\n23 Infer the latest time from the peak analysis result;\n24 Infer the earliest time from the TGA of the swap-out event;\n25 iftensor has not been swapped out then\n26 iftensor is an updated parameter in optimizing phase then\n27 scheduling swap(SE,tensor );\n28 else if SON[tensor.job id]\u0014MSR[tensor.job id]and\nTAT[tensor].length >1then\n29 succeed \u0000False ;\n30 accesses \u0000TAT [tensor ];\n31 succeed swap out,have first access \u0000True ;\n32 while notsucceed and\nlatest time > earliest time and\nsucceed swap outandhave first access do\n33 succeed ,succeed swap out,\nhave first access \u0000 scheduling swap(SE,\ntensor );\n// Ascending sort by end time\n34 Try to swap-in the rest of accesses greedily if the\nabove scheduling succeeds;\nparameter. The max swapping rate can be considered the job’s\npriority. Because the tensors’ swapping can not always overlap\nwith computation with 100 percent odds due to the conﬂict on\nPCIe and the changing GPU utilization, swapping implies the\njob may run slower than usual. In this perspective, a lower\nswapping rate means higher priority.\nAfter choosing the candidate tensor, each Swap Event’s\nfeasible time intervals will be inferred, as shown in algorithm 2\nLine 4 and Lines 23-24, which is deﬁned as its earliesttime\nandlatesttime . There are three bases for inferring the\nfeasible time interval:\n\u000fThe latest time of the Swap-Out Event cannot be later\nthan the time the GPU memory peak appears since\nwe want to swap it to reduce the GPU memory peak.\nFurthermore, a Swap-Out Event must start after its TGA .\n\u000fASwap-In Event cannot be earlier than the corresponding\nSwap-Out Event or later than the TUA that uses it as\ninput. Therefore, our algorithm needs to decide the starttime of each event under the constraint of its feasible time\nintervals.\n\u000fThe Swap Event can not overlap with the corresponding\ntensor’s access.\nBased on the feasible time intervals, we choose the available\nintervals greedily. For a Swap-Out Event , it will start as early\nas possible, and for a Swap-In Event , it will start as late as\npossible so that we can save the GPU memory occupied by\nthe tensor for a longer time. If there are no available intervals,\nthen we will break the outer loop in Line 20 and try to swap\nthe following tensor.\nNext, we have to decide when to execute the Swap-Out\nEvent and the Swap-In Event . Since the tensors in the for-\nward/backward propagation phase and the optimizing phase\nhave different constraints, we will introduce them separately.\nThe tensors in the forward/backward propagation phase\nmust be swapped into the GPU before the ﬁrst TUA after\ntheSwap-Out Event . Otherwise, an exception will be triggered\ndue to missing necessary input tensors, and the system has to\npause the computation to wait for their swapping. As shown\nin algorithm 2 Lines 10-14, we look for the ﬁrst TUA after\ntheSwap-Out Event and generate the Swap-In Event and its\nfeasible intervals to see whether any of these intervals can\ncover the Swap-In Event . If they can not cover the event,\nwe will update the Swap-Out Event ’s earliest start time and\nregenerate the feasible intervals(Lines 18-19). Otherwise, for\nthe restTUA of the tensor, we try to swap them in as much as\npossible(Line 34). If a Swap Event is scheduled successfully,\nthe corresponding tensor can be released after the previous\nTUA is ﬁnished. TENSILE will also analyze the compute\ngraph and release the tensors after their last access(Activity\nAnalysis). The detailed algorithm of swapping scheduling is\nshown in algorithm 2.\nMost prior works ignore the tensors which need to be\nupdated in the Opt phase. This causes the ’Across-iteration\nScheduling’ problem. TENSILE solves this problem with the\nfollowing rules: if these tensors are swapped out at the opti-\nmizing phase, the corresponding tensors must be swapped in\nat the beginning of the next iteration. As shown in algorithm 2\nLines 8-9 and Line 27, it tries to swap out the updated tensors\nand then swaps in the corresponding tensor before the ﬁrst\nTUA . Fig. 1(c) shows this process more vividly.\nWith this algorithm, we can select the most valuable tensors\nto be appropriately scheduled among all jobs, thus reducing\nthe entire computation job’s GPU memory peak.\nD. Recomputation Scheduling\nIf the predicted GPU memory peak is still larger than the\nGPU memory after swapping scheduling, we need to use\nrecomputation to save more GPU memory. However, a major\nshortcoming of recomputation is that it will add extra TUA\nand change the Tensor Access Sequence . If the inputs of a\nrecomputation have been swapped out or released, then it\nneeds to be swapped in or recomputed, which will disrupt\nthe swapping plan just generated and cause much more time\noverhead. Therefore, we will only apply recomputation on\nthose tensor accesses which have never been released. To\nachieve the best GPU memory saving effect, we will select\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\nthose accesses that conform to the above conditions with the\nlargest recomputation value until the GPU memory is enough\nto run these jobs.\nTo measure a recomputation’s value, we used the metric\nproposed in Capuchin [15], known as MSPS. This metric can\nmeasure the memory saving and extra time cost at the same\ntime.\nMSPS\u0011Memory Saving\nRecomputation Time(4)\nE. Scheduling Plan Updating\nAs discussed in section I, each operator’s latency changes\nalong with the jobs’ running in the multiple dynamic work-\nloads scenarios. This leads to a changing Tensor Access\nSequence , so the initial scheduling plan based on the outdated\nTensor Access Sequence will be less efﬁcient. For example,\nthe hit rate of Swap-In Event will decrease and causes an\nincreasing extra overhead. To adapt to such kinds of dynamic\nworkloads, the Memory Scheduler needs to be able to update\nthe scheduling plan based on the operators’ latency collected\nby the Executor .\nTwo major problems should be solved.\n1) When should the scheduling plan needs to be updated?\n2) How to update the scheduling plan?\nSince the reason for updating the scheduling plan is that the\nlatency of operators has been signiﬁcantly changed compared\nto the time used to generate the last Tensor Access Sequence ,\nwe only need to propose a metric to measure this change.\nWe use the average latency rate of change, which is deﬁned\nasjs0\u0000sj\ns, wheres0is the newest sum of each operation’s\nlatency, and sis the last sum of each operation’s latency. If the\nchanging rate is larger than the updating threshold set before,\nthen the updating of the scheduling plan will be triggered.\nTo correct the Tensor Access Sequence and make the\nscheduling plan more accurate, we use the exponential\nweighted moving average algorithm [27] to update the la-\ntency of operators. By predicting and correcting the latency,\nTENSILE can achieve higher performance than Capuchin’s\n’Passive Mode’ since passively swapping tensors requires extra\noverhead.\nF . Discussion of supporting Dynamic Workload and the range\nof application\nWith algorithm 1, TENSILE can give a more effective initial\nscheduling plan than the ’Passive Mode.’ The system will keep\ntracing recent tensor accesses time to correct the Tensor Access\nSequence and update the scheduling plan. When a job is\nlaunched, the Tensor Access Sequences of other jobs are most\nlikely to ﬂuctuate greatly. In this situation, TENSILE updates\ntheTensor Access Sequence based on current GPU usage\nfor all jobs. To avoid extra overhead caused by scheduling\nplan changes, the system will apply the new plan before\ncomputing the next batch of data. The complete scheduling\nalgorithm is shown in algorithm 3. Firstly, we run an activity\nanalysis at the beginning of the scheduling to save more\nmemory. Every tensor will be released when its last TUA\nis ﬁnished. Then, the Memory Scheduler will iteratively runAlgorithm 3: Complete Scheduling Algorithm\nInput: TAS : tensor access sequence\n1initialization swap succeed \u0000True ,\nrecomputation succeed \u0000True ,iter \u00000;\n2run Activity Analysis;\n3while swap succeed =True or\nrecomputation succeed =True do\n4 ifthe average MP jof the jobs corresponding to the selected\ntensors in the past 3 iterations reduces less than 0.05% and\niter > 100 then\n5 break ;\n6 ifiter= 0 then\n7 Run theGPU Memory Peak Analysis algorithm for\neach job and merge their GPU memory footprint as MP ;\n8 ifswap succeed =True then\n9 swap succeed \u0000False ;\n10 RunSwap Scheduling to generate swap events;\n11 ifno swap events generated then\n12 swap succeed \u0000False ;\n13 else if MP\u0015Free GPU Memory then\n14 RunRecomputation Scheduling ;\n15 ifno recomputation events generated then\n16 recomputation succeed \u0000False ;\n17 iter=iter+ 1;\nthe process below until no swapping or recomputation is\nscheduled. At each iteration, the algorithm will start with\nrunning the GPU Memory Peak Analysis function for each\njob and merge the result as shown in Lines 6-7. Then, if at\nleast one swap event is scheduled successfully in the previous\niteration, it will keep trying to swap, as shown in Lines 8-12.\nOtherwise, if the GPU memory is insufﬁcient, it will try to\nschedule the recomputation events as shown in Lines 13-15.\nAlthough our method is designed to run on a single GPU, it\ncan also support data-parallel models for multiple GPUs in the\ncluster. In this situation, the scheduling processes are the same\nwith a single GPU situation on each node, with the premise\nthat its CPU has enough PCIe channels for the GPUs to use\nin a single node.\nV. E XPERIMENTS\nTo verify the effectiveness of the proposed approaches, we\nconduct extensive experiments in this section.\nA. Methodology\nExperiment Platform. To apply our scheduling algorithm,\nwe implement a naive deep learning framework with CUDA as\nour experiment platform. The framework is modiﬁed based on\nthe tinyﬂow open-source project2. We also reproduce vDNN\n[16], and Capuchin [15] coupling with our framework based\non their papers.\nBaselines. We choose two methods as baselines from layer-\ngranularity and tensor-granularity methods to compare with\nour method. The ﬁrst one is vDNN, which swaps out the\nappropriate feature maps and uses a static swap-in strategy\nfor prefetching. There are two types vDNNs, vDNN alland\nvDNN conv, the former tries to swap every feature map, and the\nlatter only swaps the feature maps of convolution layers. We\ntake the vDNN conv as one of our baselines since the authors\nof vDNN proved that vDNN conv has much higher perfor-\nmance than vDNN all[16]. The second baseline is Capuchin,\n2https://github.com/LB-Yu/tinyﬂow\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\nthe newest and the most representative dynamic scheduling\nmethod in tensor granularity for a single job. Although these\nbaselines are designed for single workload scenarios, we\ncan also create an instance for each workload in our\nexperiments and schedule each workload independently.\nAnd both the baselines and our method overlap data transfer\nand computation with cudaMemcpyAsync .\nMetrics. We introduce the metrics used to evaluate the\nperformance of TENSILE. We choose Memory Saving\nRate(MSR) ,Extra Overhead Rate(EOR) , and Cost-Beneﬁt\nRate(CBR) to measure the TENSILE’s performance and efﬁ-\nciency compared to the baselines. Their deﬁnitions are shown\nas follows:\nMSR =V MP\u0000EMP\nV MP;EOR =ETC\nV TC;CBR =MSR\nEOR.\nWhereVMP andEMP are the memory footprint peak of\nthe vanilla group (no scheduling) and the experimental group\n(TENSILE or baselines), VTC andETC are the time cost of\nthe vanilla group and the experimental group.\nThe three metrics we choose are the key metrics to evaluate\na GPU memory scheduling method. The Memory Saving\nRate (MSR) represents how much GPU memory a schedul-\ning method can save compared to the vanilla method (no\nscheduling), the higher MSR implies that the corresponding\nscheduling method can reduce more GPU memory footprint\npeak compared to the vanilla method. The EOR indicates how\nmany times the time cost will be caused by saving these\nproportions of GPU memory. The lower EOR implies that\nthe scheduling method has less extra overhead. The CBR is\nthe ratio of MSR and EOR, which indicates how much extra\noverhead it takes to save a piece of GPU memory. The higher\nCBR implies that the corresponding scheduling method is\nmore space-time efﬁcient than other methods. These metrics\nare common and fair for TENSILE and both baselines since we\nanalyzed and compared the MSR, EOR, and CBR of the whole\ncomputing process of TENSILE and both baselines in detail.\nSince TENSILE is designed for multiple dynamic workload\nscenarios, it is expected to have higher MSR andCBR and\nlessEOR compared to other baseline methods.\nStatement of our reproduction. It is necessary to empha-\nsize that the reproduction for Capuchin may not be exactly the\nsame as its authors’ implementation in the paper [15] in the\naspect of the system framework since there is no source code\nto refer to, and it is implemented in TensorFlow. For example,\nin Capuchin, they measure the GPU kernel latency through\nCUPTI. However, in our platform, every computation will be\nsynchronized between CPU and GPU, so we can measure the\noperators’ latency in the python codes. We want to compare\nthe scheduling algorithm that runs on the same platform rather\nthan whose platform implementation is more computationally\nefﬁcient. Therefore, the experiments will be fair since all\nthe methods’ scheduling algorithms are implemented exactly\nas their paper described and run on the same platform for\ncomparison.\nExperiment Platform. Our experiment is conducted on a\nserver that has Intel(R) Xeon(R) Silver 4210R CPU, 504GB\nHost Memory, and one RTX A6000 with 48GB memory. The\nsystem version is Ubuntu 18.04 with GCC 7.5.0, the CUDA\nversion is 11.3, the cuDNN is 8.2.0, and Python 3.10.Workloads. We choose the ﬁve neural networks in the\nevaluation of the Capuchin [15] to evaluate TENSILE, VGG-\n16 [28], InceptionV3 [29], InceptionV4 [30], ResNet-50 [2],\nand DenseNet [31], covered both linear network and non-linear\nnetwork. We will increase the workload number on one GPU\nand measure the GPU memory footprint and the extra time\noverhead for each method.\nExtra Setting. The Capuchin is designed to schedule only\nwhen the GPU memory is insufﬁcient. It will only save the\nGPU memory to the threshold that is just able to run the\nworkloads, which makes it difﬁcult to compare its performance\nwith TENSILE since TENSILE is a more active scheduling\nmethod that will save GPU memory as much as possible.\nTherefore, we set Capuchin’s GPU memory budget as the same\nas TENSILE. Then we can compare these two methods’ time\ncost and efﬁciency fairly.\nAlso, our method includes two phases, the cold start phase,\nand the dynamic updating phase. The former uses predicted\noperations’ latency to generate the schedule plan before the job\nlaunching, and the latter updates the scheduling plan with the\nrunning information collected from the framework. To analyze\nthese two phases separately, we denote the cold start phase\nbyTENSILE csand the updating phase by TENSILE up.\nTheTENSILE cs’s performance is measured once the job is\nlaunched, and the TENSILE up’s performance is measured\nonce the scheduling plan is updated. We also use TENSILE\nto denote the whole process of TENSILE.\nB. Performance of TENSILE’s Each Phase\nWe ﬁrst evaluate the performance of the cold start phase\nand the update phase of TENSILE by measuring the memory\nfootprint and time cost separately. The results are shown in\nTable I.\nTABLE I\nEACH PHASE ’SPERFORMANCE\nWorkloads Phase MSR EOR CBR\nVGG-16TENSILE 0.2670 1.6287 0.1639\nTENSILE cs 0.2670 6.0128 0.0444\nTENSILE up 0.3192 1.2475 0.2559\nInceptionV3TENSILE 0.4361 1.6468 0.2648\nTENSILE cs 0.4361 3.1128 0.1401\nTENSILE up 0.4788 1.4839 0.3227\nInceptionV4TENSILE 0.5154 1.4281 0.3609\nTENSILE cs 0.5154 2.6727 0.1929\nTENSILE up 0.5558 1.2898 0.4309\nResNet-50TENSILE 0.4258 1.5540 0.2740\nTENSILE cs 0.4258 3.5395 0.1203\nTENSILE up 0.4647 1.3813 0.3364\nDenseNetTENSILE 0.5135 1.1678 0.4397\nTENSILE cs 0.5135 2.2532 0.2279\nTENSILE up 0.5236 1.0664 0.4910\nThe MSR of TENSILE equals the metric of TENSILE cs,\nwhich proves that the cold start phase is the bottleneck of the\nwhole process of TENSILE. Also, compared to Capuchin’s\nperformance in Table III, TENSILE cscan achieve a similar\nperformance as Capuchin with the prediction method only.\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\nThis result proves the effectiveness of our prediction method.\nNote that the time cost of the cold start phase includes the\noverhead of the prediction of the MLP models, and this\nresult proves the effectiveness of the scheduling plan updating\nmethod. Furthermore, beneﬁting from the scheduling plan\nupdating method in subsection IV-E, the update phase’s MSR\nand EOR are signiﬁcantly higher than the cold start phase.\nSince the plan updating method takes the main ingredient, its\nhigh efﬁciency signiﬁcantly reduces the overhead of the entire\ncomputing process.\nWe also measure the ﬁrst iteration of TENSILE and Ca-\npuchin to compare the EOR of our MLP-based prediction\nmethod with the ’Passive Mode’ of Capuchin. The results are\nshown in Table II, which shows that TENSILE has a more\nefﬁcient method for the cold-starting situation. For the MLP\nlatency prediction models’ memory cost, our method contains\n40 models, and the size of their parameters is 321MB.\nTABLE II\nFIRST ITERATION TIMECOST\nWorkloads Method EOR Prediction Cost(s) Prediction EOR\nVGG-16TENSILE 1.2523 1.3549 0.2067\nCapuchin 5.8053 - -\nInceptionV3TENSILE 1.4744 1.4743 0.3699\nCapuchin 6.7529 - -\nInceptionV4TENSILE 1.3797 2.4991 0.2865\nCapuchin 10.2088 - -\nResNet-50TENSILE 1.3924 1.7823 0.3198\nCapuchin 9.6073 - -\nDenseNetTENSILE 1.3855 1.3855 0.3335\nCapuchin 10.0925 - -\nC. Single Workload Performance\nWe compare TENSILE’s performance with vDNN and\nCapuchin on a single workload. The batch size is set to 16.\nAnd for TENSILE, the max swapping rate limit is set to 100%\nfor each task since there is only one task in this experiment.\nThe result is shown in Table III.\nAnd the result also shows that our method can save 26.70%-\n51.54% memory than the vanilla method, which is much\nhigher than vDNN. This is foreseeable since TENSILE is a\ntensor granularity scheduling method. And the cost-beneﬁt rate\nof TENSILE is also much higher than vDNN and Capuchin,\nwhich proves the efﬁciency of TENSILE.\nThe high efﬁciency of TENSILE is due to the fact we\nput the extra time cost into priority consideration, which is\nembodied in that we do not allow the non-overlapping between\nswapping and computation logically. On the contrary, the other\nbaseline methods allow the computation to wait for swapping\nwhen scheduling. Also, compared to Capuchin, TENSILE can\nschedule tensors across iterations. This feature helps to avoid\nsome passive swap-in.\nThis result proves that although TENSILE is designed for\nmultiple jobs and dynamic workloads, it can still achieve the\nbest efﬁciency and memory saving rate in a single workload\nsituation.\nD. Scalability\nTo evaluate our method’s performance on multiple dynamic\nworkloads and test its scalability, we choose one to fourTABLE III\nSINGLE TASK PERFORMANCE\nWorkloads Methods MSR EOR CBR\nVGG-16vDNN 0.1109 1.7835 0.0622\nCapuchin 0.2558 1.8304 0.1398\nTENSILE 0.2670 1.6287 0.1639\nInceptionV3vDNN 0.1633 2.5184 0.0648\nCapuchin 0.4331 2.7745 0.1561\nTENSILE 0.4361 1.6468 0.2648\nInceptionV4vDNN 0.1903 2.5028 0.0760\nCapuchin 0.5129 3.2362 0.1585\nTENSILE 0.5154 1.4281 0.3609\nResNet-50vDNN 0.1733 2.2981 0.0754\nCapuchin 0.3859 2.2320 0.1729\nTENSILE 0.4258 1.5540 0.2740\nDenseNetvDNN 0.1381 2.6148 0.0528\nCapuchin 0.4860 3.4400 0.1413\nTENSILE 0.5135 1.1678 0.4397\nworkloads respectively and launch them simultaneously to\nsimulate a multiple dynamic workloads scenario. We repeat\neach test three times and report the average metric in Fig. 6.\nAs the Fig. 6 shows, in most of the workloads, TENSILE\nhas achieved the best MSR and CBR, which is solid proof of\nthe performance of TENSILE in multiple dynamic workloads\nsituations. The EOR and CBR of TENSILE csdo not outper-\nform Capuchin in VGG-16 since the VGG-16 has a massive\ntensor generated by a wide MLP layer, which causes it much\nharder to schedule without the overhead. TENSILE tackles this\nproblem by the updating method, and the whole TENSILE,\nincluding the updating phase, outperforms Capuchin. With\nthe increased workload, TENSILE’s MSR of InceptionV3,\nInceptionV4, ResNet-50, and DenseNet drops slowly. And in\nall experiments, TENSILE upsaves the most GPU memory.\nFurthermore, according to the EOR metric, Capuchin’s\noverhead dramatically increases as we analyze in the last\nparagraph of section II. The VDNN’s EOR reduces with the\nincreasing workload numbers since the VDNN is a layer-\ngranularity method, which can not schedule all tensors so it\nhas many free time intervals for swapping. This makes its extra\ntime cost less than the vanilla method’s time cost. And with\nincreasing workload numbers, the vanilla time cost rises faster\nthan the extra time cost. So the EOR gets lower. Although\nVDNN has a lower overhead in multiple workload scenarios,\nits MSR is meager compared to TENSILE and Capuchin.\nIn any case, the efﬁciency of TENSILE is still much higher\nthan both baselines. Moreover, the ratios of TENSILE’s CBR\nto vDNN/Capuchin’s CBR under the multi-workload envi-\nronment are higher than in the single-workload environment.\nThe conclusion is that TENSILE can run these workloads\nwith less (or at least the same) GPU memory and time\ncost than baselines. Especially compared to the Capuchin,\nwhose EOR is signiﬁcantly increasing when the number of\nworkloads increases, which implies most of their swapping\ntransfer can not overlap with the computation, TENSILES’s\nEOR is much more stable. This experiment strongly proves\nthat the performance of TENSILE is much better than vDNN\nand Capuchin in multiple dynamic workloads scenarios. To\nevaluate our method’s performance on multiple dynamic work-\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13\nloads and test its scalability, we choose one to four workloads\nrespectively and launch them simultaneously to simulate a\nmultiple dynamic workloads scenario. We repeat each test\nthree times and report the average metric in Fig. 6.\nFig. 6. Multiple Dynamic Workloads Performance. TENSILE outperforms\nboth baselines in MSR, EOR, and CBR. And with the increasing workload\nnumber, the ratio of TENSILEs CBR to Capuchins CBR keeps increasing in\nmost results. This phenomenon represents that the performance beneﬁts of\nTENSILE higher than single workload scenario.\nE. Mixed Neural Architectures\nTo measure the TENSILE’s performance in a mixed neural\narchitectures workloads scenario, we randomly launch the ﬁve\nabove workloads and measure the total GPU memory footprint\nand execution overhead. The ﬁve workloads are launched one\nby one in random order, and this procedure is repeated three\ntimes. We set the budget of each workload of Capuchin with\nthe MSR of TENSILE in Table IV.\nAs Table IV shows, TENSILE is comprehensively leading\nin EOR and CBR under different neural architectures. It can\nsave much more GPU memory with less extra cost in scenarios\nwith complex non-linear workloads, making it more suitable\nfor environments such as in-database machine learning.\nF . Overhead Analysis\nAlthough TENSILE outperforms two baselines, the over-\nheads during GPU memory scheduling can hardly be avoided\ncompletely.\nFor single workloads scenarios, the overheads are caused\nby two reasons. The ﬁrst reason is that when the memory is\nextremely insufﬁcient, the overhead of passive swap-in and\nrecomputation cannot be avoided completely. However, using\na more efﬁcient scheduling algorithm, TENSILE achieves less\noverhead than Capuchin. The second reason is the failed tensor\nprefetch caused by the ﬂuctuation of GPU operator latency and\nthe error of estimated operator latency.\nFor multiple workloads scenarios, the GPU utilization rate\nand the PCIe channel are all inﬂuenced by the asynchronous\nworkloads, which makes the prefetch much harder and makes\nmore passive swap-in and recomputation. TENSILE alleviates\nthis problem by choosing the most valuable tensors among all\nworkloads to schedule to achieve higher efﬁciency. However,\nit is not a perfect solution, and we will continue to ﬁnd better\nsolutions to this problem in the future.TABLE IV\nMIXED NEURAL ARCHITECTURES WORKLOADS PERFORMANCE\nMethods MSR EOR CBR\nvDNN 0.0580 1.6297 0.0356\nCapuchin 0.1150 7.6545 0.0150\nTENSILE 0.1565 1.1589 0.1350\nG. Inﬂuence of Batch Size\nWe also analyzed the batch size’s inﬂuence on our method\nby running those ﬁve workloads, respectively, with batch sizes\n4, 8, 16, 32, and 64. The results are shown in Fig. 7.\nThe MSR increases with the batch size for all workloads,\nand so does their CBR in general. This result shows that the\nTENSILE can save more GPU memory with less overhead\nwhen the batch size increases. This result proves that TEN-\nSILE is more effective in scheduling the interim results than\nthe parameters since the former is the only one inﬂuenced by\nthe batch size. The DenseNet with batch size 32 and 64 is an\nexception. We believe it is because there are too many tensors\nto swap. And when the batch size increases, the swap events\ncan more easily conﬂict with each other. And for VGG-16,\nit is the only workload whose EOR drops with the increase\nin batch size. This is because that VGG-16 has a wide MLP\nlayer that outputs a huge tensor. This tensor’s computation\ncost increases faster than its scheduling cost, so the EOR gets\nlower.\nFig. 7. Batch size’s inﬂuence\nVI. C ONCLUSIONS & F UTURE WORK\nWe propose TENSILE, a tensor granularity GPU memory\nscheduling method toward multiple dynamic workload sce-\nnarios. TENSILE has solved three major problems of prior\nworks, multiple dynamic workloads, cold-starting, and across-\niteration scheduling by designing a scheduling method and\ncorresponding system, a latency prediction method, and a\nscheduling algorithm. Furthermore, TENSILE is a generic\nscheduling method for computing tasks described as DAGs,\nespecially deep learning tasks. The experiment results show\nthat TENSILE can achieve much higher performance in such\nscenarios than vDNN and Capuchin. Even in single workload\nscenarios, TENSILE can still outperform baselines. These\nfeatures can help systems such as in-database machine learning\nrun more deep learning tasks with tiny extra overhead.\nACKNOWLEDGMENT\nThis paper was supported by NSFC grant (62232005).\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14\nREFERENCES\n[1] S. Dargan, M. Kumar, . Maruthi, M. R. Ayyagari, and . Kumar, “A\nsurvey of deep learning and its applications: A new paradigm to machine\nlearning,” 07 2019.\n[2] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” 2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , pp. 770–778, 2016.\n[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of\ndeep bidirectional transformers for language understanding,” in NAACL-\nHLT, 2019.\n[4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-\nV oss, G. Kr ¨uger, T. Henighan, R. Child, A. Ramesh, D. Ziegler,\nJ. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,\nand D. Amodei, “Language models are few-shot learners,” ArXiv , vol.\nabs/2005.14165, 2020.\n[5] D. Yang and D. Cheng, “Efﬁcient gpu memory management for\nnonlinear dnns,” in Proceedings of the 29th International Symposium on\nHigh-Performance Parallel and Distributed Computing , ser. HPDC ’20.\nNew York, NY , USA: Association for Computing Machinery, 2020, p.\n185C196. [Online]. Available: https://doi.org/10.1145/3369583.3392684\n[6] V . Narasayya, I. Menache, M. Singh, F. Li, M. Syamala, and S. Chaud-\nhuri, “Sharing buffer pool memory in multi-tenant relational database-\nas-a-service,” Proc. VLDB Endow. , vol. 8, pp. 726–737, 2015.\n[7] T. Li, J. Zhong, J. Liu, W. Wu, and C. Zhang, “Ease.ml: Towards multi-\ntenant resource sharing for machine learning workloads,” Proc. VLDB\nEndow. , vol. 11, pp. 607–620, 2018.\n[8] B. Karlas, J. Liu, W. Wu, and C. Zhang, “Ease.ml in action: Towards\nmulti-tenant declarative learning services,” Proc. VLDB Endow. , vol. 11,\npp. 2054–2057, 2018.\n[9] S. Boag, P. Dube, K. El Maghraoui, B. Herta, W. Hummer, K. R.\nJayaram, R. Khalaf, V . Muthusamy, M. Kalantar, and A. Verma, “De-\npendability in a multi-tenant multi-framework deep learning as-a-service\nplatform,” in 2018 48th Annual IEEE/IFIP International Conference on\nDependable Systems and Networks Workshops (DSN-W) , 2018, pp. 43–\n46.\n[10] W. Xiao, R. Bhardwaj, R. Ramjee, M. Sivathanu, N. Kwatra, Z. Han,\nP. Patel, X. Peng, H. Zhao, Q. Zhang, F. Yang, and L. Zhou, “Gandiva:\nIntrospective cluster scheduling for deep learning,” in Proceedings\nof the 13th USENIX Conference on Operating Systems Design and\nImplementation , ser. OSDI’18. USA: USENIX Association, 2018, p.\n595C610.\n[11] D. Narayanan, K. Santhanam, F. Kazhamiaka, A. Phanishayee, and\nM. A. Zaharia, “Heterogeneity-aware cluster scheduling policies for\ndeep learning workloads,” in OSDI , 2020.\n[12] R. Marcus, P. Negi, H. Mao, C. Zhang, M. Alizadeh, T. Kraska,\nO. Papaemmanouil, and N. Tatbul, “Neo: a learned query optimizer,”\nProceedings of the VLDB Endowment , vol. 12, pp. 1705–1718, 07 2019.\n[13] R. Marcus, P. Negi, H. Mao, N. Tatbul, M. Alizadeh, and T. Kraska,\n“Bao: Making learned query optimization practical,” 06 2021, pp. 1275–\n1288.\n[14] T. Kraska, A. Beutel, E. Chi, J. Dean, and N. Polyzotis, “The case for\nlearned index structures,” 05 2018, pp. 489–504.\n[15] X. Peng, X. Shi, H. Dai, H. Jin, W. Ma, Q. Xiong, F. Yang, and\nX. Qian, “Capuchin: Tensor-based gpu memory management for deep\nlearning,” Proceedings of the Twenty-Fifth International Conference\non Architectural Support for Programming Languages and Operating\nSystems , 2020.\n[16] M. Rhu, N. Gimelshein, J. Clemons, A. Zulﬁqar, and S. W. Keckler,\n“vdnn: Virtualized deep neural networks for scalable, memory-efﬁcient\nneural network design,” 2016 49th Annual IEEE/ACM International\nSymposium on Microarchitecture (MICRO) , pp. 1–13, 2016.\n[17] L. Wang, J. Ye, Y . Zhao, W. Wu, A. Li, S. Song, Z. Xu, and T. Kraska,\n“Superneurons: dynamic gpu memory management for training deep\nneural networks,” ACM SIGPLAN Notices , vol. 53, pp. 41–53, 02 2018.\n[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” Commun. ACM , vol. 60,\nno. 6, p. 84C90, May 2017. [Online]. Available: https://doi.org/10.\n1145/3065386\n[19] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph\nconvolutional networks,” CoRR , vol. abs/1609.02907, 2016. [Online].\nAvailable: http://arxiv.org/abs/1609.02907\n[20] B. ShriramS, A. Garg, and P. Kulkarni, “Dynamic memory management\nfor gpu-based training of deep neural networks,” 2019 IEEE Interna-tional Parallel and Distributed Processing Symposium (IPDPS) , pp.\n200–209, 2019.\n[21] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nu. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proceedings\nof the 31st International Conference on Neural Information Processing\nSystems , ser. NIPS’17. Red Hook, NY , USA: Curran Associates Inc.,\n2017, p. 6000C6010.\n[22] D. Yang and D. Cheng, “Efﬁcient gpu memory management for\nnonlinear dnns,” in Proceedings of the 29th International Symposium on\nHigh-Performance Parallel and Distributed Computing , ser. HPDC ’20.\nNew York, NY , USA: Association for Computing Machinery, 2020, p.\n185C196. [Online]. Available: https://doi.org/10.1145/3369583.3392684\n[23] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between\ncapsules,” in Proceedings of the 31st International Conference on Neural\nInformation Processing Systems , ser. NIPS’17. Red Hook, NY , USA:\nCurran Associates Inc., 2017, p. 3859C3869.\n[24] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nInternational Conference on Learning Representations , 12 2014.\n[25] J. Zhang, S. H. Yeung, Y . Shu, B. He, and W. Wang, “Efﬁcient memory\nmanagement for gpu-based deep learning systems,” arXiv preprint\narXiv:1903.06631 , 2019.\n[26] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard et al. , “Tensorﬂow: A system for large-\nscale machine learning,” in 12thfUSENIXgsymposium on operating\nsystems design and implementation ( fOSDIg16), 2016, pp. 265–283.\n[27] M. Perry, The Exponentially Weighted Moving Average , 06 2010.\n[28] K. Simonyan and A. Zisserman, “Very deep convolutional networks\nfor large-scale image recognition,” in 3rd International Conference on\nLearning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,\n2015, Conference Track Proceedings , Y . Bengio and Y . LeCun, Eds.,\n2015. [Online]. Available: http://arxiv.org/abs/1409.1556\n[29] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking\nthe inception architecture for computer vision,” 2016 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , pp. 2818–2826,\n2016.\n[30] C. Szegedy, S. Ioffe, V . Vanhoucke, and A. Alemi, “Inception-v4,\ninception-resnet and the impact of residual connections on learning,”\nAAAI Conference on Artiﬁcial Intelligence , 02 2016.\n[31] G. Huang, Z. Liu, and K. Q. Weinberger, “Densely connected\nconvolutional networks,” CoRR , vol. abs/1608.06993, 2016. [Online].\nAvailable: http://arxiv.org/abs/1608.06993",
  "textLength": 72221
}